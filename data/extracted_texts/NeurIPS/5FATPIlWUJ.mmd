# Robust Gaussian Processes via Relevance Pursuit

Sebastian Ament

Meta

ament@meta.com

&Elizabeth Santorella

Meta

santorella@meta.com

&David Eriksson

Meta

deriksson@meta.com

Ben Letham

Meta

bletham@meta.com

&Maximilian Balandat

Meta

balandat@meta.com

&Eytan Bakshy

Meta

ebakshy@meta.com

###### Abstract

Gaussian processes (GPs) are non-parametric probabilistic regression models that are popular due to their flexibility, data efficiency, and well-calibrated uncertainty estimates. However, standard GP models assume homoskedastic Gaussian noise, while many real-world applications are subject to non-Gaussian corruptions. Variants of GPs that are more robust to alternative noise models have been proposed, and entail significant trade-offs between accuracy and robustness, and between computational requirements and theoretical guarantees. In this work, we propose and study a GP model that achieves robustness against sparse outliers by inferring data-point-specific noise levels with a sequential selection procedure maximizing the log marginal likelihood that we refer to as _relevance pursuit_. We show, surprisingly, that the model can be parameterized such that the associated log marginal likelihood is _strongly concave_ in the data-point-specific noise variances, a property rarely found in either robust regression objectives or GP marginal likelihoods. This in turn implies the weak submodularity of the corresponding subset selection problem, and thereby proves approximation guarantees for the proposed algorithm. We compare the model's performance relative to other approaches on diverse regression and Bayesian optimization tasks, including the challenging but common setting of sparse corruptions of the labels within or close to the function range.

## 1 Introduction

Probabilistic models have long been a central part of machine learning, and Gaussian process (GP) models are a key workhorse for many important tasks , especially in the small-data regime. GPs are flexible, non-parametric predictive models known for their high data efficiency and well-calibrated uncertainty estimates, making them a popular choice for regression, uncertainty quantification, and downstream applications such as Bayesian optimization (BO) [8; 24; 26] and active learning [6; 55].

GPs flexibly model a distribution over functions, but assume a particular observation model. The standard formulation assumes i.i.d Gaussian observation noise, i.e., \(y()=f()+\), where \(f()\) is the true (latent) function value at a point \(\) and \((0,^{2})\), implying a homoskedastic Gaussian likelihood. While mathematically convenient, this assumption can be a limitation in practice, since noise distributions are often heavy-tailed or observations may be corrupted due to issues such as sensor failures, data processing errors, or software bugs. Using a standard GP model in such settings can result in poor predictive performance.

A number of _robust_ GP modeling approaches have been proposed to remedy this shortcoming, most of which fall into the following broad categories: data pre-processing (e.g., Winsorizing), modified likelihood functions (e.g., Student-\(t\)), and model-based data selection and down-weighting procedures.

These approaches offer different trade-offs between model accuracy, degree of robustness, broad applicability, computational requirements, and theoretical guarantees.

In this paper, we propose a simple yet effective implicit data-weighting approach that endows GPs with a high degree of robustness to challenging label corruptions. Our approach is flexible and can be used with arbitrary kernels, is efficient to compute, and yields provable approximation guarantees. Our main contributions are as follows:

1. We propose a modification to the standard GP model that introduces learnable data-point-specific noise variances.
2. We introduce a novel greedy sequential selection procedure for maximizing the model's marginal log-likelihood (MLL) that we refer to as _relevance pursuit_.
3. We prove that, under a particular parameterization, the MLL is strongly concave in the data-point-specific noise variances, and derive approximation guarantees for our algorithm.
4. We demonstrate that our approach, Robust Gaussian Processes via Relevance Pursuit (RRP), performs favorably compared to alternative methods across various benchmarks, including challenging settings of sparse label corruptions within the function's range, see e.g. Figure 1.

## 2 Preliminaries

We aim to model a function \(f:\) over some domain \(^{d}\). With a standard Gaussian noise model, for \(_{i}\) we obtain observations \(y_{i}=f(_{i})+_{i}\), where \(_{i}(0,^{2})\) are i.i.d. draws from a Gaussian random variable. \(\|\|\) denotes the Euclidean norm unless indicated otherwise.

### Gaussian Processes

A GP \(fP((),k_{}(,))\) is fully defined by its mean function \(:\) and covariance or kernel function \(k_{}:\), which is parameterized by \(\). Without loss of generality, we will assume that \( 0\). Suppose we have collected data \(=\{(_{i},y_{i})\}_{i=1}^{n}\) where \(:=\{_{i}\}_{i=1}^{n}\), \(:=\{y_{i}\}_{i=1}^{n}\). Let \(_{}_{++}^{n}\) denote the covariance matrix of the data set, i.e., \([_{}]_{ij}=k_{}(_{i}, _{j})+_{ij}^{2}\), where \(_{ij}\) is the Kronecker delta. The negative marginal log-likelihood (NMLL) \(\) is given by

\[-2():=-2 p(|,)= ^{}_{}^{-1}+ _{}+n 2.\] (1)

In the following, we will suppress the explicit dependence of the kernel matrix on \(\) for brevity of notation. For a comprehensive background on GPs, we refer to Rasmussen et al. .

### Noise Models

Additive, heavy-tailed noiseInstead of assuming the noise term \(_{i}\) in the observation model to be Gaussian, other noise models consider zero-mean perturbations drawn from distributions with heavier tails, such as the Student-\(t\), Laplace , or \(\)-Stable  distributions. These types of errors are common in applications such as finance, geophysics, and epidemiology . Robust regression models utilizing Student-\(t\) errors are commonly used to combat heavy-tailed noise and outliers.

Sparse corruptionsIn practice, often a small number of labels are corrupted. We will refer to these as "outliers," though emphasize that the corrupted values may fall within the range of normal outputs. Sparse corruptions are captured by a model of the form \(y_{i}=Z_{i}f(_{i})+(1-Z_{i})W_{i}\), where

Figure 1: Comparison of RRP to a standard GP and a variational GP with a Student-\(t\) likelihood on a regression example. While the other models are led astray by the corrupted observations, RRP successfully identifies the corruptions (red) and thus achieves a much better fit to the ground truth.

\(Z_{i}\{0,1\}\) and \(W_{i}\) is a random variable. Note that \(W_{i}\) need not have (and rarely has) \(f(_{i})\) as its mean. For instance, consider a faulty sensor that with some probability \(p\) reports a random value within the sensor range \([y_{l},y_{h}]\). In this case \(Z_{i}(p)\) and \(W_{i}[y_{l},y_{h}]\). Software bugs, such as those found in ML training procedures, or errors in logging data can result in sparse corruptions.

## 3 Related Work

Data pre-processingData pre-processing can be an effective technique for handling simple forms of data corruption, such as values that fall outside a valid range of outputs. With such pre-processing, outliers are handled upstream of the regression model. Common techniques include the power transformations , trimming, and winsorization. These methods can add substantial bias if not used carefully, and generally do not handle data corruptions that occur within the normal range of the process to be modeled. See  for a review on data cleaning.

Heavy-tailed likelihoodsOne class of robust methods uses additive heavy-tailed noise likelihoods for GPs, particularly Student-\(t\), Laplace , and Huber , and could be extended with \(\)-Stable distributions, which follow a generalized central limit theorem . These models are less sensitive to outliers, but they lose efficiency when the outliers are a sparse subset of the observations, as opposed to global heavy-tailed noise. Furthermore, model inference is no longer analytic, necessitating the use of approximate inference approaches such as MCMC , Laplace approximation , expectation propagation (EP) , Expectation Maximization , or variational inference . Shah et al.  take a related approach using a Student-\(t\) process prior in the place of the GP prior. Unfortunately, the Student-\(t\) process is not closed under addition and lacks the tractability that makes GPs so versatile. Alternative noise specifications include a hierarchical mixture of Gaussians  and a "twinned" GP model  that uses a two-component noise model to allow outlier behavior to depend on the inputs. This method is suited for settings where outliers are not totally stochastic, but generally is not able to differentiate "inliers" from outliers when they can occur with similar inputs.

Outlier classificationAwasthi et al.  introduces the Trimmed MLE approach, which identifies the subset of data points (of pre-specified size) under which the marginal likelihood is maximized. Andrade and Takeda  fit GPs using the trimmed MLE by applying a projected gradient method to an approximation of the marginal likelihood. The associated theory only guarantees convergence to a stationary point, with no guarantee on quality. When no outliers are present, this method can be worse than a standard GP. Li et al.  propose a heuristic iterative procedure of removing those data points with the largest residuals after fitting a standard GP, with subsequent reweighting. The method shows favorable empirical performance but has no theoretical guarantees, and fails if the largest residual is not associated with an outlier. Park et al.  consider a model of the form \(y_{i}=_{i}+f(_{i})+_{i}\), where outliers are regarded as data with a large bias \(_{i}\). Their random bias model is related to our model in that it also introduces learnable, data-point-specific variances. However, inference is done in one step by optimizing the NMLL with an inverse-gamma prior on the \(_{i}\)'s, which - in contrast to the method proposed herein - generally does not lead to exactly sparse \(_{i}\)'s.

Sample re-weightingAltamirano et al.  propose robust and conjugate GPs (RCGP) based on a modification to the Gaussian likelihood function that is equivalent to standard GP inference, where the covariance of the noise \(^{2}\) is replaced by \(^{2}(^{-2})\) and the prior mean \(\) is replaced by \(_{}=+^{2}_{y}(^{2})\). The authors advocate for the use of the inverse multi-quadratic weight function \(w(,y)=(1+(y-m())^{2}/c^{2})^{-1/2}\), which introduces two additional hyper-parameters: the soft threshold \(c\), and the "learning rate" \(\). Importantly, the weights \(\) are defined _a-priori_ as a function of the prior mean \(m()\) and the targets \(y\), thereby necessitating the weights to identify the correct outliers without access to a model. This is generally only realistic if the outlier data points are clearly separated in the input or output spaces rather than randomly interspersed.

## 4 Robust Gaussian Process Regression via Relevance Pursuit

Our method adaptively identifies a sparse set of outlying data points that are corrupted by a mechanism that is not captured by the other components of the model. This is in contrast to many other approaches to robust regression that non-adaptively apply a heavy-tailed likelihood to _all_ observations, which can be suboptimal if many observations are of high quality.

### The Extended Likelihood Model

We extend the standard GP observation noise variance \(^{2}\) with data-point-specific noise variances \(\,=\{_{i}\}_{i=1}^{n}\), so that the \(i\)-th data point is distributed as

\[y_{i}\,|\,_{i}(f(_{i}),^{2}+ _{i})..\] (2)

This is similar to Sparse Bayesian Learning  in which weight-specific prior variances control a feature's degree of influence on a model's predictions. The marginal likelihood optimization of \(_{i}\) in (2) gives rise to an _automatic mechanism_ for the detection and weighting of outliers. The effect of \(y_{i}\) on the estimate of \(f\) vanishes as \(_{i}\), similar to the effect of the latent variables \(\) in Bodin et al. 's extended GP model \(f(,)\), though \(\) requires MCMC for inference. While many heteroskedastic GP likelihoods model noise as an input-dependent process [28; 35], our formulation does not require such assumptions, and is thus suitable for corruptions that are not spatially correlated.

An elegant consequence of our modeling assumption is that we can compute individual marginal-likelihood maximizing \(_{i}\)'s in closed form when keeping all \(_{j}\) for \(j i\) fixed. In particular,

**Lemma 1**.: _[Optimal Robust Variances] Let \(_{ i}=\{(_{j},y_{j}):j i\}\), \(=_{ i}+_{i}_{i}\), where \(,_{ i}_{+}^{n}\), \([_{ i}]_{i}=0\), and \(_{i}\) is the \(i\)th canonical basis vector. Then keeping \(_{ i}\) fixed,_

\[_{i}^{*}=_{_{i}}_{ i }+_{i}_{i}=(y_{i}-[y(_{i})| _{ i}])^{2}-[y(_{i})|_{  i}]_{+}\,,\] (3)

_where \(y(_{i})=f(_{i})+_{i}\). These quantities can be expressed as functions of \(^{-1}=(+_{^{2}+ })^{-1}\):_

\[[y(_{i})|_{ i}]^{2}=y_{i}-[ ^{-1}]_{i}/[^{-1} ]_{ii},[y(_{i})|_{  i}]=1/[^{-1}]_{ii},\]

_where \(_{^{2}+}\) is a diagonal matrix whose entries are \(^{2}+\)._

The first component \([f(_{i})+_{i}|_{ i}]^{2}\) of (3) is the empirical error to \(y_{i}\) of the model trained without the \(i\)-th data point, i.e., the leave-one-out (LOO) cross-validation error . The second component \([f(_{i})+_{i}|_{ i}]\) is the LOO predictive variance. The optimal solution to \(_{i}\) is only non-zero for those observations whose squared LOO error is larger than the LOO predictive variance at that point.

### Optimization with a Maximum Number of Outliers

Without additional structure, inference of the noise variances \(_{i}\) does not yield desirable models, as the marginal likelihood can be improved by increasing the prior variance \(_{i}\) of any data point where Eq. (3) is greater than zero, even if that is due to regular (non-outlier) measurement noise. To avoid this, we constrain the number of non-zero \(_{i}\), that is, \(\|\|_{}=|\{0<_{i}\}| k<n\). While this sparsity constraint mitigates over-flexibility, it gives rise to a formidably challenging optimization problem, as there are a combinatorial number of sparse outlier sets to consider. Even if the number of outliers \(n_{o}\) were known, exhaustive search would still require considering \(n\)-choose-\(n_{o}\) possibilities.

Figure 2: _Left:_ Evolution of model posterior during Relevance Pursuit, as the number of data-point-specific variances \(|S|\) increases (from light colors to dark). Red points indicate corruptions that were generated by uniformly sampling from the function’s range. _Right:_ Comparison of posterior marginal likelihoods as a function of a model’s \(|S|\). The maximizer – boxed in black – is the preferred model.

For tractability, we iteratively add data points to a set of potential "outliers" by setting their associated \(_{i}\) to be nonzero, using the closed-form expression for the optimal individual \(_{i}\) variances in Lemma 1. As the algorithm seeks to identify the most "relevant" data points (as measured by \(\)) upon completion, we refer to it as _Relevance Pursuit_. This is Algorithm 1 with \(\) as false. Specifically, this is the "forward" variant; Algorithm 2 in the Appendix presents an alternative "backward" variant that we found to work well if the number of corrupted data points is large.

Crucial to the performance of the optimizer, it never removes data from consideration completely; a data point is only down-weighted if it is apparently an outlier. This allows the down-weighting to be reversed if a data point appears "inlying" after having down-weighted other data points, improving the method's robustness and performance. This is in contrast to Andrade and Takeda 's greedy algorithm, in which the exclusion of a data point can both increase or decrease the associated marginal likelihood. This means that their objective is not monotonic, a necessary condition to provide constant-factor submodular approximation guarantees for greedy algorithms, see Section 5.

```
0:\(\), \(\), schedule \(=(k_{1},k_{2},,k_{})\), \(\) (boolean)  Initialize \(_{0}\{1,,n\}\) (typically \(_{0}=\)) for\(i\) in \((1,,||)\)do  Optimize MLL: \(_{_{i}}_{_{ _{i}}}(_{_{i}})\), where \(_{_{i}}=\{:_{j}=0,\; \;j_{i}\}\).  Expand Support:  Compute \(_{i}(j)_{_{j}}(_{_{i}}+_{j}_{j})-(_{_{i}})\) for all \(j_{i}\) via Lemma 1. \(_{i}\{j_{1},,j_{k_{i}}\}\) such that \(_{i}(j)_{i}(j^{})\) for all \(j_{i}\) and \(j^{}(_{i}_{i})\). \(_{i+1}_{i}_{i}\) if\(\)then  Compute the marginal likelihood \(p(|_{i}) p(|_{i},_{_{i}})\) \(^{*}_{_{i}}p(|_ {i})p(_{i})\). else \(^{*}\) = \(_{}\).  Return \(^{*}\), \(_{^{*}}\). ```

**Algorithm 1** Relevance Pursuit (Forward Algorithm)

### Automatic Outlier Detection via Bayesian Model Selection

In practice, it is often impossible to set a hard threshold on the number of outliers for a particular data set. For example, a sensor might have a known failure rate, but how many outliers it produces will depend on the specific application of the sensor. Thus, is often more natural to specify a prior distribution \(p()\) over the number of outliers, rather than fix the number _a priori_. We leverage the Bayesian model selection framework [66; 45] to determine the most probable number of outliers in a data- and model-dependent way, aiming to maximize \(p(|)\). This gives rise to Algorithm 1, with \(\) as \(\).

Computationally, we start by iteratively adding outliers up to the maximal support of the prior, similar to the procedure described in Section 4.2. We store a trace of models generated at each iteration, then approximate the model posterior \(p(_{i}|) p(|S_{i})p(S_{i})\) at each point in the trace. As the exact posterior is intractable, we approximate it with \(p(|_{i})= p(|_{i}, {}_{_{i}})_{_{i}} p (|_{i},_{_{i}}^{*})\). Finally, we select the model from the model trace \(\{_{i}\}_{i}\) that attains the highest model posterior likelihood. Imposing a prior on the number of outliers differs notably from most sparsity-inducing priors, which are instead defined on the parameter values, like \(l_{1}\)-norm regularization. In practice, \(p()\) can be informed by empirical distributions of outliers. For our experiments, we use an exponential prior on \(||\) to encourage the selection of models that fit as much of the data as tightly as possible.

Regarding the schedule \(\) in Algorithm 1, the most natural choice is simply to add one data point at a time, i.e. \(=(1,1,...)\), but this can be slow for large \(n\). In practice, we recommend schedules that test a fixed set of outlier fractions, e.g. \(=(0.05n,0.05n,)\).

## 5 Theoretical Analysis

We now provide a theoretical analysis of our approach. We first propose a re-parameterization of the \(_{i}\) that maps the optimization problem to a compact domain. Surprisingly, the re-parameterizedproblem exhibits strong convexity and smoothness when the base covariance matrix (excluding the \(_{i}\)) is well-conditioned. We connect the convexity and smoothness with existing results that yield approximation guarantees for sequential greedy algorithms, implying a constant-factor approximation guarantee to the optimal achievable NMLL value for generalized orthogonal matching pursuit (OMP), a greedy algorithm that is closely related to Algorithm 1.

### Preliminaries for Sparse Optimization

The optimization of linear models with respect to least-squares objectives in the presence of sparsity constraints has been richly studied in statistics , compressed sensing [3; 62], and machine learning [9; 67]. Of central importance to the theoretical study of this problem class are the eigenvalues of sub-matrices of the feature matrix, corresponding to sparse feature selections and so often referred to as _sparse eigenvalues_. The restricted isometry property (RIP) formalizes this.

**Definition 2** (Restricted Isometry Property).: _An \((n m)\)-matrix \(\) satisfies the \(r\)-restricted isometry property (RIP) with constant \(_{r}(0,1)\) if for every submatrix \(_{}\) with \(||=r m\) columns,_

\[(1-_{r})\|\|\|_{}_{ }\|(1+_{r})\|\|,\]

_where \(_{}^{r}\). This is equivalent to \((1-_{r})(_{}^{*}_{ })(1-_{r})\)._

The RIP has been proven to lead to exact recovery guarantees , as well as approximation guarantees . Elenberg et al.  generalized the RIP to non-linear models and other data likelihoods, using the notion of restricted strong convexity (RSC) and restricted smoothness.

**Definition 3** (Restricted Strong Convexity and Smoothness).: _A function \(f:^{d}\) is \(m_{r}\)-restricted strong convex and \(M_{r}\)-restricted smooth if for all \((,^{})\) in the domain \(D_{r}(^{d}^{d})\),_

\[m_{r}\|^{}-\|^{2}/2\;\;f(^{})-f( )-[f]()^{}(^{}-)\; \;M_{r}\|^{}-\|^{2}/2.\]

_In the context of sparse optimization, we let \(D_{r}\) be the set of tuples of \(r\)-sparse vectors whose difference is also at most \(r\)-sparse. In particular, \(D_{r}=\{(,^{})\|\|_{0},\|^{}\|_{0},\|^{}- \|_{0} r\}\)._

Generalized orthogonal matching pursuit (OMP) [4; 43; 44] is a greedy algorithm that keeps track of a support set \(\) of non-zero coefficients, and expands the support based on the largest gradient magnitudes, applied to the marginal likelihood optimization problem, \(_{i+1}=_{i}_{j}|_{ }()|_{j}\). Algorithm 1 generalizes OMP  by allowing more general support expansion schedules \(\), and specializes the support expansion criterion using the special problem structure exposed by Lemma 1.

### The Convex Parameterization

The NMLL \(\) of a GP (1) is the sum of a convex function \(()^{-1}\) and a concave function \(()\) of \(\), and is therefore not generally convex as a function of the hyper-parameters \(\), including the robust variances \(\). Here, we propose a re-parameterization that allows us to prove strong convexity guarantees of the associated NMLL. In particular, we let \(()=(_{0})((1- )^{-1}-1)\), where \(_{0}:=k(,)+^{2}\) and the inverse is element-wise. Note that \(()\) is a diffeomorphism that maps \(\) from the compact domain \(^{n}\) to the entire range of \([0,]^{n}\).

Henceforth, we refer to the original \(\) as the _canonical_ or \(\)-parameterization and the newly proposed \(()\) as the _convex_ or \(\)-parameterization. Lemma 4 shows the Hessian of the \(\)-parameterization.

**Lemma 4**.: _[Reparameterized Hessian] Let \(_{}=k(,)+^{2}+ _{()}\), \(}_{}=(_{})^{ -1/2}_{}(_{})^{-1/2}\), and \(}=}_{}^{-1} (_{})^{-1/2}\). Then_

\[_{}[-2(()]=_{1-}^{-1}[2(}}^{}(}^{-1}-))+2(}^{-1})-(}^{-1}}^{-1}) ]_{1-}^{-1}.\]

Based on this representation, we now derive conditions on the eigenvalues of \(}\) that imply the \(m\)-strong convexity and \(M\)-smoothness of the NMLL.

**Lemma 5**.: _[Strong Convexity via Eigenvalue Condition] Let \(}_{}\) as in Lemma 4. Then \(_{}>m\) if_

\[_{}_{}^{2}_{}^{-1}-_{}^{-2}-m)}{2(1-_{}/_{})}>\|\|_{2}^ {2},\] (4)

_where \(_{,}\) (resp. \(_{,}\)) are the smallest and largest eigenvalues of \(_{}\), respectively \(}_{}\)._The behavior Lemma 5 predicts is surprising and validated in Fig. 3. Notably, the denominator "blows up" as \(\) becomes close to unitary, making the inequality more likely to be satisfied, an indication that the convexity property of the NMLL is intimately linked to the RIP (Def. 2). Note that Lemma 5 is a condition for non-support-restricted convexity, which is stronger than is necessary for the approximation guarantees that rely on restricted convexity (Def. 3). However, sparse eigenvalues are generally difficult to compute exactly. Fortunately, covariance matrices of GPs naturally tend to exhibit a property that facilitates a different sufficient condition for convexity for all \(^{n}\).

**Definition 6** (Diagonal Dominance).: _A matrix \(\) is said to be \(\)-diagonally dominant if the elements \(a_{ij}\) satisfy \(_{i j}|a_{ij}|<|a_{ii}|\) for all \(i\)._

Intuitively, the \(_{i}(s)\) that are selected to be non-zero by the greedy algorithm take on large values, further encouraging the diagonal dominance of the sub-matrix of \(\) associated with the support of \(\). For this reason, the following condition on \(_{0}\) is sufficient to guarantee convexity for all \(^{n}\).

**Lemma 7**.: _[Strong Convexity via Diagonal Dominance] Let \(m>0\) and \(_{0}\) be \(\)-diagonally dominant with \(<((5-m)-)/4(5-)/4 0.44\) and_

\[_{}(_{0})(1-)^{2}-(1- )^{-2}-m}{2(1-(1-)/(1+))}\|\|_{2}^{2}.\]

_Then the NMLL is \(m\)-strongly convex for all \(^{n}\), i.e. \(()[0,]^{n}\)._

We attain similar results for \(M\)-smoothness, see Lemma 13 and Lemma 14 in the Appendix. Having proven \(m\)-convexity and \(M\)-smoothness conditions, we appeal to the results of Elenberg et al. .

**Theorem 8**.: _[Approximation Guarantee] Let \(_{0}=k(,)+^{2}\) be \(\)-diagonally dominant, \(s_{}>0\) be an upper bound on \(\|\|_{}\), and suppose \(\|\|,\) satisfy the bounds of Lemmas 7 and 14, guaranteeing \(m\)-convexity and \(M\)-smoothness of the NMLL for some \(m>0,\,M>1/(1-s_{})^{2}\). Let \(_{}(r)\) be the \(r\)-sparse vector attained by OMP on the NMLL objective for \(r\) steps, and let \(_{}(r)=_{\|\|_{0}=r,\|\|_{ } s_{}}(())\) be the optimal \(r\)-sparse vector. Then for any \(2r n\),_

\[}((_{}(r)) )\;\;(1-e^{-m/M})\,}((_{}(r))),\]

_where \(}()=()-()\) is normalized so that \(_{_{S}}}(_{S}) 0\) for any support \(\)._

Figure 3: _Top:_ The behavior of the \(-()\) with respect to the canonical parameterization of \(\). _Bottom:_ The behavior of \(-(())\), highlighting the convexity property. _Left:_ The value, and first two derivatives of \(-\) for a 1d example. _Center:_ The second derivatives of a 1d \(-\) as a function of \(|y|\). The \(\)-parameterization is everwhere convex for all considered \(|y|\), while the canonical \(\)-parameterization is only convex around the origin and only for \(|y|>0.5\). _Right:_ The heatmaps highlight that the original parameterization is non-convex (red) for larger values of \(\), and quickly becomes ill-conditioned, whereas the parameterization \(()\) is convex and much better conditioned.

A limitation of the theory is that it assumes the other hyper-parameters of the GP model to be constant, as doing otherwise would introduce the non-convexity that is common to most marginal likelihood optimization problems. In practice, we typically optimize \(\) jointly with the other hyper-parameters of the model in each iteration of RRP, as this yields improved performance, see App. D.5 for details.

## 6 Empirical Results

We evaluate the empirical performance of RRP against various baselines on a number of regression and Bayesian Optimization problems. Specifically, we compare against a standard GP with a Matern-5/2 kernel ("Standard GP"), data pre-processing through Ax's adaptive winsorization procedure ("Adapt. Wins.") , and a power transformation ("Power Transf.") . Further, we also consider a Student-\(t\) likelihood model from Jylanki et al.  ("Student-\(t\)"), the trimmed marginal likelihood model from Andrade and Takeda  ("Trimmed MLL"), and the RCGP model from Altamirano et al. . Unless stated otherwise, all models are implemented in GPyTorch  and all experiments in this section use \(32\) replications. See Appendix D for additional details.

### Regression Problems

SyntheticWe first consider the popular Friedman10 and Hartmann6  test functions from the literature. We use two data generating processes: uniform noise, extreme outliers at some fixed value, and heavy-tailed (Student-\(t\)) noise at true function values. In these experiments, we compare the performance predictive log-likelihood. The results are shown in Fig. 4.

Twitter Flash CrashIn Fig. 5, we report a comparison to Altamirano et al. 's RCGP on data from the Dow Jones Industrial Average (DJIA) index on April 22-23 2013, which includes a sharp drop at 13:10 on the 23rd. The top panels shows that RCGP exhibits higher robustness than the standard GP, but is still affected by the outliers, when trained on data from the 23rd. RRP is virtually unaffected. Notably, RCGP relies on an a-priori weighting of data points based on the target values' proximity to their median, which can be counter-productive when the outliers are not a-priori separated in the range. To highlight this, we included the previous trading day into the training data for the bottom panels, leading RCGP to assign the _highest_ weight to the outlying data points due to their proximity to the target values' median, thereby leading RCGP to "trust" the outliers more than any inlier,

Figure 4: _Left:_ Distribution of predictive test-set log likelihood for various methods. Methods ommitted are those that performed substantially worse. _Right:_ Predictive log likelihood as a function of the corruption probability for Student-\(t\)-distributed corruptions with two degrees of freedom. The GP model with the Student-\(t\) likelihood only starts outperforming RRP as the corruption probability increases beyond 40%, and exhibits a large variance in outcomes, which shrinks as the proportion of corruptions increases. All methods not shown were inferior to either RRP or Student-\(t\).

resulting in it being less robust than a standard GP in this scenario. See Appendix D.6 for additional comparisons to RCGP, on data sets from the UCI machine learning repository .

### Robust Bayesian Optimization

GPs are commonly used for Bayesian optimization (BO), which is a popular approach to sample-efficient black-box optimization . However, many of the GP models used for BO are sensitive to outliers and may not perform well in settings where such outliers occur. While Martinez-Cantin et al.  consider the use of a Student-\(t\) likelihood for BO with outliers, the use of other robust GP models has not been thoroughly studied in the literature.

Experimental setupWe use Ament et al. 's qLogNoisExpectedImprovement (qLogNEI), a variant of the LogEI family of acquisition functions, 32 replications, and initialize all methods with the same quasi-random Sobol batch for each replication. We follow Hwarfner et al.  and plot the true value of the best in-sample point according to the GP model posterior at each iteration. We also include Sobol and an "Oracle", which is a Standard GP that always observes the uncorrupted value, and consider the backward canonical version of relevance pursuit, denoted by RRP, for these experiments. The plots show the mean performance with a bootstrapped 90% confidence interval.

Synthetic problemsWe consider the popular 6-dimensional Hartmann test function with three different corruption settings: (1) a constant value of \(100\), (2) a \(U[-3,3]\) distributed value, (3) the objective value for a randomly chosen point in the domain. The results for a 10% corruption probability are shown in Fig. 6. We also include results for a 20% corruption probability in Appendix D.3.

Figure 5: Results on the intra-day data from the Dow Jones Industrial Average (DJIA) index on April 22-23 2013, which includes a sharp drop at 13:10 on the 23rd, see (b) for a detailed view. The accompanying panels labeled \(w_{}\) show the function that Altamirano et al. ’s RCGP uses to down-weight data points. _Top_: RCGP, exhibits higher robustness than the standard GP, but is still affected by the outliers. The RRP model is virtually unaffected. _Bottom_: Including the previous trading day into the training data in (c), leads RCGP to assign the _highest_ weight \(w_{}\) to the outlying data points due to their proximity to the target values’ median, thereby leading RCGP to be even more affected than a standard GP, see (d) for a detailed view of the results on the data of April 23.

Real-world problemsWe include three real-world problems: A 3D SVM problem, a 5D CNN problem, and a 20D rover trajectory planning problem, see the App. D.2 for details. For SVM and CNN, we simulate random corruptions corresponding to an I/O error, which causes the corresponding ML model to be trained using only a small subset of the training data. For the rover planning problem we follow the setup in  with the main difference that we consider a \(20\)D trajectory, and the corruptions are generated randomly, causing the rover to break down at an arbitrary point along its trajectory. In most cases, this results in a smaller reward than the reward of the full trajectory.

## 7 Conclusion and Future Work

ContributionsRobust Gaussian Processes via Relevance Pursuit (RRP) provides a novel and principled way to perform robust GP regression. It permits efficient and robust inference, performs well across a variety of label corruption settings, retains good performance in the absence of corruptions, and is flexible, e.g., can be used with any mean or kernel function. Our method can be readily applied to both robust regression problems as well as applications such as Bayesian optimization and is available through BoTorch. Importantly, it also provides theoretical approximation guarantees.

LimitationsAs our approach does not explicitly consider the locations of the data points in the outlier identification, it may be outperformed by other methods if the underlying noise is heteroskedastic and location-dependent. On the other hand, those methods generally do not perform well in the presence of sparse, location-independent data corruptions.

ExtensionsPromising extensions of this work include performing Bayesian model averaging, i.e., average the predictions of the different possible sparsity models according to their likelihoods instead of using a MAP estimate, applying RRP to specialized models such as Lin et al. 's scalable learning-curve model for AutoML applications, and Ament et al. 's model for sustainable concrete. On a higher level, the approach of combining greedy optimization algorithms with Bayesian model selection and leveraging a convex parameterization to achieve approximation guarantees might apply to other parameters that are optimized using the MLL objective: length-scales of stationary kernels, coefficients of additive kernels, inducing inputs, and even related model classes like Tipping 's Sparse Bayesian Learning (SBL), which seeks to identify sparse linear models and is intimately linked to greedy matching pursuits . Overall, the approach has the potential to lead to theoretical guarantees, new insights, and performance improvements to widely-adopted Bayesian models.

Figure 6: BO results for Hartmann6: _Left:_ Relevance pursuit performs well in the case of constant outliers of value \(100\), almost as well as the oracle. _Middle:_ Relevance pursuit performs the best followed by the Student-\(t\) likelihood in the case of \(U[-3,3]\). _Right:_ Similar to the middle plot, this setting hides the corruptions within the range of the function, making it a challenging task.

Figure 7: BO results for three real-world problems: _Left:_ RRP is competitive with the oracle on the 3D SVM problem. _Middle:_ The power transform performs best on the 5D CNN problem, outperforming RRP as well as the Oracle. _Right:_ RRP performs well on the 20D Rover problem.