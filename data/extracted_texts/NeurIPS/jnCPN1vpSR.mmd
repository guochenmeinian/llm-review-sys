# D-CIPHER: Discovery of Closed-form Partial Differential Equations

Krzysztof Kacprzyk

University of Cambridge

kk751@cam.ac.uk

&Zhaozhi Qian

University of Cambridge

zq224@cam.ac.uk

&Mihaela van der Schaar

University of Cambridge,

The Alan Turing Institute

mv472@cam.ac.uk

###### Abstract

Closed-form differential equations, including partial differential equations and higher-order ordinary differential equations, are one of the most important tools used by scientists to model and better understand natural phenomena. Discovering these equations directly from data is challenging because it requires modeling relationships between various derivatives that are not observed in the data (_equation-data mismatch_) and it involves searching across a huge space of possible equations. Current approaches make strong assumptions about the form of the equation and thus fail to discover many well-known phenomena. Moreover, many of them resolve the equation-data mismatch by estimating the derivatives, which makes them inadequate for noisy and infrequent observations. To this end, we propose D-CIPHER, which is robust to measurement artifacts and can uncover a new and very general class of differential equations. We further design a novel optimization procedure, CoLLie, to help D-CIPHER search through this class efficiently. Finally, we demonstrate empirically that it can discover many well-known equations that are beyond the capabilities of current methods.

## 1 Introduction

Scientists have been using mathematical equations to describe the world for centuries. In particular, closed-form differential equations turned out to be one of the best tools to model physical phenomena. A differential equation describes a relationship between a quantity and its derivatives (rates of change); it is called closed-form if this relationship is described by a mathematical expression consisting of a finite number of variables, constants, arithmetic operations, and some well-known functions (e.g., exponent, logarithm, trigonometric functions)1. Closed-form differential equations provide a general description of reality in a concise representation that is amenable to closer inspection by scientists. This renders them transparent and interpretable to human experts.

Discoveries of these equations required a thorough knowledge of the theory, strong mathematical skills, substantial creativity, and good intuition. The goal of this work is to discover closed-form differential equations directly from data thus accelerating the process of scientific discovery.

**Challenges in discovering differential equations from data**

* **Partial and higher-order derivatives.** Many algorithms  can only identify Ordinary Differential Equations (ODEs) which evolve only with respect to one variable (usually time). In contrast, many natural phenomena are described by equations involving many variables (e.g., spatial coordinates) called Partial Differential Equations (PDEs). Many equations also involve higher-order derivatives.

* **Derivatives not observed.** Discovering differential equations from data is challenging because the derivatives are usually not observed in the dataset (_equation-data mismatch_). This makes verifying a candidate equation a non-trivial task. Most of the methods try to resolve this issue by estimating the derivatives [10; 47]. However, derivative estimation is difficult, especially when the data is sampled infrequently or with high noise [42; 35]. For an illustrative study, see Appendix F.
* **Strong assumptions and constrained search space.** The majority of algorithms for identifying differential equations make many assumptions about the form of the equation. In particular, they make the _evolution assumption_ (defined and explained later) and assume that the equation can be represented as a linear combination of prespecified functions and differential operators [10; 35]. However, many well-known equations, such as a forced harmonic oscillator or an inhomogeneous wave equation, cannot be represented in that way.

Currently, a few algorithms tackle only some of these challenges. In particular, Weak SINDy  is able to discover PDEs without estimating the derivative by utilizing a variational approach. However, the form of the equation is constrained to be in a form amenable for a sparse regression algorithm. D-CODE , on the other hand, uses a variational approach in conjunction with a symbolic regression algorithm to discover closed-form ODEs. However, it cannot handle higher-order derivatives or multiple independent variables, so it cannot be used to discover closed-form PDEs. The algorithms that do not require the evolution assumption appeared in  and  but they require derivative estimation and only consider equations represented as linear combinations of prespecified functions.

**Contributions.** In this work, we develop the **D**iscovery of **C**losed-form **P**artial and **H**igher-order Differential **E**quations in a **R**obust Way framework (D-CIPHER) that does not estimate the derivatives, requires fewer assumptions, has a broader search space than previous techniques, and works for both higher-order ODEs and PDEs. Our contributions are as follows:

* We examine the landscape of different types of PDEs from the discovery perspective. In particular, we introduce new notions such as _evolution form_, _evolution assumption_, _derivative-bound_ part, and _derivative-free_ part. We use them to describe what kinds of PDEs can be discovered with current methods and to motivate our new class of differential equations. (Section 3)
* We propose a new general class of PDEs (_Variational-Ready_ PDEs) that admit the variational formulation (and thus allows to circumvent the derivative estimation). We also prove a theorem that motivates a novel objective function. (Section 5)
* We use the novel objective function to develop D-CIPHER, a new algorithm that searches over the _Variational-Ready_ PDEs. (Section 6)

In addition to the main contributions above, we also develop a new optimization procedure (CoLLie) to help D-CIPHER search through this space efficiently. (Section 7)

## 2 Preliminaries

In this section, we provide background information about PDEs and their variational formulation.

**Notation and definitions.** We denote the set \(\{1,2,,n\}\) as \([n]\) and the set of non-negative integers as \(_{0}\). Throughout this paper we let \(M,N,K\) be some natural numbers and let \(^{M}\) be an open set inside \(^{M}\). A comprehensive table with all symbols used in this work can be found in Appendix A together with some definitions restated more formally.

**Going beyond ODEs.** The simplest differential equations are ordinary differential equations that describe quantities that evolve with respect to only one independent variable, usually time. Most methods assume that the ODE is explicit and can be represented as a system of first-order ODEs:

\[_{j}(t)=f_{j}(t,(t))\] (1)

where \(_{j}\) represents the derivative of \(u_{j}\). Then the discovery problem is reduced to deciding the order of the derivative (usually first or second) and the discovery of \(f_{j}\).

For PDEs, it is not enough to talk about _the_ derivative, as we can take derivatives with respect to different variables. We denote the _mixed derivative_ as \(^{}\), where \(_{0}^{M}\) is called a multi-index, and define it as \(^{}=_{1}^{_{1}}_{2}^{_{2}} _{M}^{_{M}}\). Each \(_{i}^{_{i}}=^{_{i}}/ x_{i}^{_{i}}\) is a \(_{i}^{}\)-order partial derivative with respect to \(x_{i}\) (the \(i^{}\) independent variable)2. We define the order of \(\) as \(||=_{i=1}^{M}_{i}\). We call \(^{}\)_non-trivial_ if \(||>0\).

A PDE of order \(K\) is any equation of the form

\[f(,(),^{[K]}())=0\ \] (2)

where \(^{[K]}\) are all non-trivial mixed derivatives of all \(u_{j}\) (\(j[N]\)) up to the \(K^{}\) order. We call a PDE _closed-form_ if \(f\) is closed-form.

**Variational formulation** (VF) of PDEs is a way to describe PDEs without referring to their derivatives . It works as follows: we take a differential equation, we multiply it by a special _testing function_, and integrate. Finally, we perform integration by parts to move the derivatives from the dependent variable \(u\) onto the testing functions. E.g., for a homogeneous heat equation,

\[_{t}u-_{x}^{2}u=0_{^{2}}(_{t} u-_{x}^{2}u)\ dtdx=0\ _{^{2}}-u_{t}+ u_{x}^{ 2}\ dtdx=0\ \] (3)

For more details, see Appendix A and B. By not depending on the derivatives, methods that utilize VF are more robust to noise than their derivative-estimating counterparts .

## 3 Relaxing assumptions while staying robust to noise

Below, we introduce the _evolution assumption_ (EA) and the _linear combination_ (LC) assumption made by the current discovery methods.

**Evolution Assumption.** Although there is no generally accepted notion of an explicit PDE (as is the case for ODEs), we define an _evolution form_ of a PDE to be an equation of the form

\[^{}u_{j}()=f(,(),^{[K]/ }())\ \] (4)

where \(^{[K]/}\) is \(^{[K]}\) with \(^{}\) omitted, \(\) is a known multi-index and \(j[N]\). Note that if \(M=1\) and \(||=K\) then Equation 4 becomes exactly the definition of an explicit ODE.

In fact, many algorithms for PDE discovery assume a particular evolution form . We call it an _evolution assumption_ (EA). However, this assumption requires the knowledge of \(\) and \(j\) which might not be trivial. Usually, \(^{}\) is assumed to be the first derivative with respect to time (\(_{t}\))  but it is not the case for many well-known PDEs such as the wave equation or Gauss's law.

**Linear combinations.** Current PDE discovery algorithms  consider PDEs that are linear in parameters. That means the PDE can be represented as a linear combination of functions, i.e.,

\[_{p=1}^{P}_{p}f_{p}(,(),^{[K]}( {x}))=0\ \] (5)

where \(_{p}\) for \(p[P]\) are the only constants that are optimized. As there are lot of expressions that cannot be put in that form, these algorithms fail to discover more complex equations. In particular, for an unknown \(\) functions such as \(( x_{i})\), \(e^{ x_{i}}\) or \(+}\) cannot be learned by these algorithms.

**How to relax these assumptions and still allow for variational formulation?** Current methods that utilize VF either assume that the PDE is in an LC form or they only work for explicit first-order ODEs. Moreover, all of them also make the evolution assumption. Relaxing LC is not trivial because not all PDEs admit VF. As in Equation 3, the PDE has to be a sum of terms for which the integration by parts can be performed. Our crucial observation is that for any term that does not contain any derivatives (and thus does not need to be integrated by parts) _no additional constraints_ need to be put in place. Due to the significance of these terms, we propose the following characterization of a PDE.

**Derivative-bound part and derivative-free part.** Any PDE can be expressed in the form

\[f(,(),^{[K]}())-g(,() )=0\ \] (6)

where we collect all the terms with the derivatives into \(f(,(),^{[K]}())\) and all terms without the derivatives into \(g(,())\). We call \(f\) the _derivative-bound_ part and \(g\) the _derivative-free_ part (denoted also \(\)-_bound_ and \(\)_-free_). \(\)-free part can be evaluated directly given \(\), whereas the \(\)-bound part requires access to the derivatives. Note that for first-order ODEs, \(f\) is trivial and equal to \(_{j}\).

**Constraints on the derivative-bound part.** Although VF does not require any constraints on the \(\)-free part, we still need to put some constraints on the \(\)-bound part for the integration by parts to work. This is what we do in Section 5, where we aim to define currently the broadest form of PDEs that admit the variational formulation using the above characterization.

**Optimization challenge.** D-CIPHER does _not_ need the evolution assumption and it can even discover some PDEs that cannot be put into the evolution form. Moreover, unlike previous methods, D-CIPHER is _not_ limited to PDEs that can be represented as a linear combination of functions (we describe the exact form we assume in Section 6). This makes the optimization problem much harder as we search over _all_ closed-form functions \(g\) and for each candidate, we try to find the best counterpart \(f\) among the allowed expressions. This is very different from the previous approaches, which either do not need to find \(f\) as they work only for first-order ODEs  or they constrain equally both the \(\)-bound part and \(\)-free part to be a linear combination of some pre-specified functions  (for more details, see Table 10, and Table 11 in Appendix F). One way we address this challenge is by developing a new optimization procedure (Section 7).

## 4 Related works

**Symbolic Regression.** The goal of symbolic regression is to find a closed-form expression that best models the given dataset both in terms of accuracy and simplicity. In contrast with the conventional regression analysis which optimizes the parameters of a pre-specified model, symbolic regression aims at discovering both the general structure and the parameters of the model. Most of the existing work focuses on developing optimization algorithms. Genetic Programming  has been widely used for that task . A different strategy has been employed in AI Feynman [54; 55] that uses neural networks to reduce the search space by identifying simplifying properties like symmetry or separability. Optimization methods based on pre-trained neural networks [4; 20], reinforcement learning , and Meijer-G functions [1; 13] have also been proposed.

**Data-driven discovery of closed-form differential equations.** Data-driven discovery of physical laws is an established area of machine learning [6; 49]. The pioneering work in that area was SINDy  that constrained the space of equations to linear combinations of functions from a predefined library and used sparse regression to discover explicit ODEs. It was later extended to include implicit ODEs [33; 23] and PDEs [47; 48]. Various other extensions were proposed by improving the derivative estimation and the training procedure [45; 61], adding additional selection criteria  and learning the library using genetic programming [34; 12; 62]. A different approach is taken by  (an extension of ) which uses convolutional and symbolic neural networks. It is important to note that _all of these methods still assume the PDE to be a linear combination_ as discussed in Section 3 (Equation 5) which significantly limits their search space. Some other developments are based on Gaussian processes [44; 43] but they require the exact form of the PDE and only optimize the parameters.

**Variational approach.** Recently, the variational approach has been used as a viable alternative to derivative estimation. However, they have only been used for differential equations in a linear combination form [35; 36; 46] or closed-form first-order ODEs . Extending the variational approach to closed-form PDEs is not trivial as PDEs are much more complex than ODEs and not all closed-form PDEs admit the variational formulation. In fact, the approaches that learn the library mentioned in the previous paragraph can produce exactly such terms which prohibits the use of variational formulation. To address these challenges we use the new notions defined in Section 3 to define a new and general class of PDEs in Section 5 that admit the variational formulation.

   Method & PDEs & No \(\) estimation & No evolution assumption & Any closed-form \(\)-free part \\  SINDy  & \(\) & \(\) & \(\) & \(\) \\ SINDy-implicit  & \(\) & \(\) & \(\) & \(\) \\ PDE-FIND  & \(\) & \(\) & \(\) & \(\) \\ PDE-Net 2.0  & \(\) & \(\) & \(\) & \(\) \\ WSINDy [35; 46] & \(\) & \(\) & \(\) & \(\) \\ D-CODE  & \(\) & \(\) & \(\) & \(\) \\ D-CIPHER & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Columns correspond to challenges outlined in Section 1 and answer the following questions: Can it discover PDEs? Does it avoid derivative estimation? Is the evolution assumption unnecessary (Equation 4)? Can it discover any closed-form \(\)-free part (Equation 6)?Variational-Ready PDEs

In this section, we propose a new and very general class of PDEs, the _Variational-Ready_ PDEs (VR-PDEs), which can be characterized _without_ referring to the derivative. The VR-PDEs allow arbitrary \(\)-free part but make some minor restrictions on the \(\)-bound part. These restrictions allow one to use the variational formulation of PDEs to circumvent derivative estimation entirely. Despite the minor restriction, VR-PDEs contain many well-known PDEs, including all linear PDEs, Maxwell's equations, and Navier-Stokes equations (additional examples provided in Appendix B).

To define the new class of PDEs, we need the following definition.

**Definition 1** (Extended derivative and differential operator).: Let \(_{0}^{M}\), \(|| K\), be a multi-index. Let \(h:^{M+N}\) and \(a:^{M}\) be smooth functions. An _extended derivative_\(\), denoted \((,a,h)\), maps a vector field \(:^{M}^{N}\) to a function \([]:^{M}\) defined as:

\[[]()=a()^{}[h(,( ))]\] (7)

\(\) is called _closed-form_ if \(a\) and \(h\) are closed-form. We call \(\)_non-degenerate_ if \(||>0\).

Now, let \((_{p})_{p[P]}\) be a finite sequence of non-degenerate extended derivatives. The extended differential operator, denoted as \(_{[P]}\) is an operator defined as:

\[_{[P]}[]()=_{p=1}^{P}_{p}[]()\] (8)

_Remark_.: Any linear operator \(L=_{}a_{}^{}\) acting on \(u_{j}\) is an extended differential operator.

**Definition 2** (Variational-Ready PDE).: Let \(_{[P]}\) be an extended differential operator, and let \(g:^{M+N}\) be a continuous function. We denote a _Variational-Ready_ PDE (VR-PDE) by a pair \((_{[P]},g)\) and define it as:

\[_{[P]}[]()-g(,())=0\ \] (9)

We extend the standard variational formulation of PDEs (Proposition 1 in Appendix B) from linear PDEs to all VR-PDEs. The following definition is useful in further discussion.

**Definition 3**.: Consider a field \(:^{N}\), and an extended derivative \(=(,a,h)\). Let \(:\) be a _testing function_ (\(^{K}\) function 3 with compact support). We define the functional

\[(,,)=_{}h(,())(-1 )^{||}^{}[a()()]d\]

We can now use this functional to formulate variational characterization of VR-PDEs.

**Theorem 1**.: \(:^{N}\)_, where \(u_{j}^{K}\), is a solution to a VR-PDE in Equation 9 if and only if_

\[_{p=1}^{P}(_{p},,)-_{}[g( ,())()]d=0\] (10)

_for all testing functions \(:\)._

Proof.: Appendix B. 

This theorem motivates the _variational loss function_ as we expect the left-hand side of Equation 10 to be closer to 0 the closer the canditate PDE is to the true one. To calculate how well a set of vector fields \(=\{^{(d)}\}_{d=1}^{D}\) matches a VR-PDE \((_{[P]},g)\) we propose the following loss function.

\[(_{[P]},g)=_{d=1}^{D}_{s=1}^{S}( _{p=1}^{P}(_{p},^{(d)},_{s})-_{} g(,^{(d)}())_{s}()d)^{2}\] (11)

where \(\{_{s}\}_{s=1}^{S}\) is a set of predefined testing functions.

This novel loss function makes it possible to evaluate to what extent any VR-PDE matches the observed data. This loss can be used as an optimization objective in any algorithm that searches over some subspace of closed-form VR-PDEs. We propose D-CIPHER in Section 6 as an example of such an algorithm.

D-Cipher

In this section, we formulate the problem of PDE discovery and then we introduce a novel algorithm (D-CIPHER) to solve it. The diagram and pseudocode are presented in Figure 1 and in Appendix C.

**Problem formulation** We are given a dataset of _observed fields_\(=\{^{(d)}\}_{d=1}^{D}\) with a finite _sampling grid_\(\). Each \(^{(d)}()\) is a noisy measurement, i.e., \(^{(d)}:^{N}\) is defined as

\[v_{j}^{(d)}()=u_{j}^{(d)}()+_{j}^{(d)}( )\;\; j[N]\] (12)

where \(_{j}^{(d)}()\) is a realization of a zero-mean random variable (noise), each \(u_{j}^{(d)}:\) is a \(^{K}\) function, and every _true field_\(^{(d)}\) is governed by the same closed-form PDE \(f\). The task is to infer the closed-form PDE, \(f\), from the dataset \(=\{^{(d)}\}_{d=1}^{D}\) and the sampling grid \(\). We assume that \(f\) is inside the class of closed-form VR-PDEs (Section 5) and its \(\)-bound part is inside a subspace of extended differential operators spanned by a user-specified dictionary (see Step 1 below).

We propose an algorithm that consists of three steps. In the first step, we define the subspace of closed-form VR-PDEs we want to search over to reflect our knowledge of the problem. In the second step, we reconstruct the fields from noisy measurements. In the last step, we solve an optimization problem using a modified symbolic regression algorithm. For more details, check Appendix C.

**Step 1: Choose the form and incorporate prior knowledge.** A human expert should encode their prior knowledge of the problem into a dictionary of non-degenerate extended derivatives \(\) = \(\{}_{p}\}_{p[P]}\). We use this dictionary to search over a finite-dimensional subspace of closed-form operators spanned by this set. In other words, we assume that the VR-PDE is of the form:

\[_{p=1}^{P}_{p}}_{p}[]()- g(,())=0\;\] (13)

where \(^{P}\), \(g\) is _any_ closed-form function of \(M+N\) variables, and \(}_{p}=(_{p},a_{p},h_{p})\).

For instance, a dictionary might include only the partial derivatives up to a certain order. For a 1+1 second-order equation that means \(=\{_{t},_{x},_{tx},_{t}^{2}, _{x}^{2}\}\). That is already enough to discover heat and wave equations with any closed-form source. If, for instance, the user suspects the presence of the advection term \(uu_{x}\) (as in the Burgers' equation), the term \(_{x}(u^{2})\) can be included in the library.

It's important to note that we do _not_ assume any particular form of \(g\) apart from being closed-form.

**Step 2: Estimate the fields.** As the dataset \(\) consists of noisy and infrequently sampled fields, we first need to estimate the "true" fields \(}^{(d)}\) from \(^{(d)}\). Any choice of reconstruction algorithm can be used and the user should choose it according to the problem setting and their domain knowledge.

**Step 3: Optimize.** We minimize the loss function in Equation 11 for the estimated fields \(\{}^{(d)}\}_{d=1}^{D}\) among all PDEs of the form in Equation 13. We solve the following optimization problem:

\[_{g}_{||||_{1}=1}_{d=1}^{D}_{s=1}^{S}( _{p=1}^{P}(_{p}}_{p},} ^{(d)},_{s})-_{}g(,}^{(d)}( ))_{s}()d)^{2}\] (14)

As we want to discover both \(g\) and \(\) we cannot use the standard penalties on \(\) such as the \(||||_{2}\) or \(||||_{1}\), as the loss would be minimized by \(g=0\) and \(=\). Therefore we put the constraint \(||||_{1}=1\). We choose the L1 norm to encourage sparsity in the coordinates of the vector \(\).

The inner minimization in Equation 14 can be rewritten as a constrained least-squares problem.

\[_{||||_{1}=1}_{(d,s)[D][S]}( {}^{(d,s)}-w^{(d,s)})^{2}\] (15)

where \(}_{p}=(_{p},a_{p},h_{p})\) and \(^{(d,s)}^{P}\), \(w^{(d,s)}\) are defined as

\[ z_{p}^{(d,s)}&=_{}h_{p}( ,}^{(d)}())(-1)^{|_{p}|}^{_{p}}(a_{p}()_{s} ())d\\ w^{(d,s)}&=_{}g(,}^{(d)}())_{s}()d\] (16)We show the full derivation in Appendix C. \(^{(d,s)}\) can be precomputed at the beginning of the algorithm without estimating the derivatives of the reconstructed fields. They can be easily calculated if the derivatives of the testing functions \(_{s}\) and the derivatives of \(a_{p}\) can be analytically computed.

As the optimization problem in Equation 15 has to be solved many times for different closed-form expressions \(g\), it poses some unique challenges. As standard approaches are not sufficiently fast, we design a new heuristic algorithm to solve this problem, CoLLie, and describe it in the next section.

## 7 CoLLie

The problem in Equation 15 from the previous section can be formulated as follows. Given matrix \(^{m n}\) and vector \(^{m}\), find a vector \(^{n}\) that minimizes \(||-||_{2}^{2}\) such that \(||||_{1}=1\). The task is challenging as the unit L1 sphere is not convex. A method that guarantees an optimal solution is based on an observation that the \((n-1)\)-dimensional L1 sphere consists of \(2^{n}\)\((n-1)\)-simplices (which are convex). Minimizing \(||-||_{2}^{2}\) on a simplex is a quadratic program  with many available solvers . However, that means that the computation time scales _exponentially_ with the number of dimensions. This is prohibitively long for the inner optimization of our algorithm. Therefore, we design a heuristic algorithm CoLLie (**C**onstrained **L**1 Norm **Le**ast Squares) that finds an approximate solution but is significantly faster (Figure 2). We observe that this optimization problem is related to the one encountered in LASSO. Denote \(}\) the solution that minimizes \(||-||_{2}^{2}\) (no constraints). If \(||}|| 1\), the problem is equivalent to finding \(\) (in the Lagrangian form of LASSO, Equation 47) such that the LASSO solution has the norm 1. Least Angle Regression (LARS)  is a popular algorithm used to minimize the LASSO objective that computes complete solution paths. These paths show how the coefficients of the solution change as \(\) moves from \(0\) to \(_{max}\) (from no constraints to effectively imposing the solution to be \(\)). See Figure 6 in Appendix D.2. CoLLie uses these solution paths to calculate the exact solution to the optimization problem. The case \(0<||_{0}||<1\) is harder as it corresponds to \(<0\). CoLLie addresses this challenge by extending the solution paths generated by LARS beyond \(=0\) for \(<0\). We assume that the paths continue to be piecewise linear and keep their slope (Figure 6 in Appendix

Figure 1: This diagram describes how the algorithm works. After the optimization procedure is finished, we get the best found closed-form function \(g\) and use CoLLie to find the best vector \(\). The found equation has the form \(_{p=1}^{P}_{p}}_{p}[]()-g(,( ))=0\)

D.2). CoLLie then uses these assumptions to efficiently find an approximate solution. We provide a detailed description of CoLLie in Appendix D.

## 8 Experiments

We perform a series of synthetic experiments to show how well D-CIPHER is able to discover some well-known differential equations4 (Table 2). First, we demonstrate that D-CIPHER performs better than current methods when discovering PDEs in a linear combination form (Section 8.1). Then we demonstrate it can discover PDEs with a closed-form \(\)-free part that _cannot_ be expressed as a linear combination and thus are beyond the capabilities of current methods (Section 8.2). We contrast D-CIPHER with its ablated version where the derivatives are estimated and the standard MSE loss is used instead of the variational loss (details in Appendix E.1). For additional information about the experiments (e.g., implementation details, data generation, experimental settings) see Appendix E.

**Evaluation metrics.** To establish how well a discovered PDE matches the ground truth, we evaluate its \(\)-free and \(\)-bound parts separately. For the \(\)-free part, we assign a binary variable indicating whether the correct functional form of the equation was recovered (please check Appendix E.8 for details). For the \(\)-bound part, we measure the RMSE between the found coefficients of \(\) and the target ones. We report the averages and standard deviations for both parts. We call the averages respectively **Success Probability** and **Average RMSE**.

**Implementation.** We use B-Splines  as the testing functions and we estimate the fields in Step 2 of D-CIPHER with a Gaussian Process . The outer optimization in Step 3 is performed using a modified genetic programming algorithm  and the inner optimization by CoLLie (Section 7). We also show additional experiments with different estimation algorithms in Appendix F.

### Discovering Linear Combinations: comparison with other methods

We compare D-CIPHER against two variants of PDE-FIND  and WSINDy  (as implemented in PySINDy library ) with optimization performed by Stepwise Sparse Regression  or

   Name & Equation & LC & VR \\  Homogeneous heat equation & \(_{t}u-_{1}_{x}^{2}u=0\) & ✓ & ✓ \\ Burger’s equation & \(_{t}u+u_{x}u-_{1}_{x}^{2}u=0\) & ✓ & ✓ \\ Kuramoto-Sivashinsky equation & \(_{t}u+_{x}^{2}u+_{x}^{2}u+u_{x}u=0\) & ✓ & ✓ \\ Forced and damped harmonic oscillator & \(_{t}^{2}+2_{1}_{2}_{t}u+_{2}^{2}u=_{3} (_{4}t)\) & ✓ \\ SLM model (Appendix F.1) & \(_{t}u+_{x}u=-2e^{_{1}x}u\) & ✓ & ✓ \\ Inhomogeneous heat equation & \(_{t}u-_{1}_{x}^{2}u=_{2}e^{_{3}t}\) & ✓ & ✓ \\ Inhomogeneous wave equation & \(_{t}^{2}u-_{1}_{x}^{2}u=_{2}e^{t}(_{3}t)\) & ✓ & ✓ \\   

Table 2: Equations used in the experiments. “LC” column specifies if the equation can be represented as a linear combination (Equation 5). “VR” column specifies if the PDE is Variational-Ready

Figure 2: We compare CoLLie with an algorithm that uses CVXOPT  to solve each of the convex subproblems. We report the relative error between the loss obtained by CoLLie and the minimum loss achieved by CVXOPT. Panels **B** and **C** show the averages and the distributions of relative errors. The average relative error is below 0.005 and the bulk of the distribution is below \(10^{-7}\). At the same time CoLLie is orders of magnitude faster (Panel **A**).

Forward Regression Orthogonal Least-Squares . We note that D-CIPHER is specifically designed to discover PDEs that are beyond the capabilities of current methods, i.e., where the derivative-free part can be any closed-form expression. Current methods are usually tested on equations where the derivative-free part is trivial (identically equal to 0). Even though these algorithms are specialized to discover these simpler kinds of equations, D-CIPHER performs better than (or equally well as) PDE-FIND and WSINDy, regardless of the optimization algorithm, when tested on Burgers' equation the homogeneous heat equation, and Kuramoto-Sivashinsky equation (Figure 3). This demonstrates gain from both the variational loss and the new optimization routine.

### Discovering equations beyond current methods

**Forced and damped harmonic oscillator.** As the oscillator is described by a second-order ODE, it cannot be discovered by D-CODE . D-CIPHER discovers the correct functional form of the \(\)-free part and achieves a low RMSE for the coefficients of \(\) in most of the experimental settings. The performance is higher than or comparable to the ablated version of D-CIPHER, thus demonstrating gain from using the variational approach. We present the results in Figure 4.

**Inhomogeneous heat equation.** D-CIPHER is able to discover the correct equation even in settings with very high noise. It performs better than the ablated version, thus showing the importance of the variational objective. The result are presented in Table 3.

Figure 4: Success probability of discovering the correct \(\)-free part of the equation and the average RMSE between the recovered \(\)-bound part and the target one across different experimental settings. We compare D-CIPHER against its ablated version (Abl. D-CIPHER).

    &  &  \\  & \(_{R}=0.05\) & \(0.1\) & \(0.2\) & \(_{R}=0.05\) & \(0.1\) & \(0.2\) \\  D-CIPHER & 0.64 (.07) & 0.42 (.07) & 0.12 (.05) & 0.15 (.009) & 0.21 (.007) & 0.24 (.005) \\ Ablated D-CIPHER & 0.46 (.07) & 0.20 (.06) & 0.04 (.03) & 0.18 (.009) & 0.24 (.008) & 0.27 (.007) \\   

Table 3: We report the success probability of discovering the \(\)-free part and the Average RMSE of the \(\)-bound part for the inhomogeneous heat equation. Standard deviations shown in brackets.

Figure 3: Simulation results for the Burgers’ equation, homogeneous heat equation, and Kuramoto–Sivashinsky equation. We report the Average RMSE of the \(\)-bound part of the equation. Note that some of the benchmarks overlap

**Inhomogeneous wave equation.** This equation does not have the standard evolution form, as it does not involve the \(_{t}\) term. Thus, even without the source term, most of the current methods cannot be applied directly to discover this equation. In Figure 5 we show the absolute difference between the true field and the fields computed from the sources discovered by D-CIPHER and its ablated version across different measurement settings. D-CIPHER finds the correct functional form with coefficients not far from the ground truth. The ablated version fails to discover the correct functional form and the found \(\)-free part does not reproduce the correct behavior of the equation.

## 9 Discussion

**Applications.** As D-CIPHER can potentially discover any closed-form \(\)-free part, it is especially useful when this part of the PDE captures an essential component of the phenomenon. We demonstrate it by finding the heat and vibration sources as well as the driving force of an oscillator. Beyond the spatio-temporal physical equations, D-CIPHER might prove useful in discovering population models structured by age, size, and spatial position , age-dependent epidemiological models , and predator-prey models with age-structure . All these equations are VR-PDEs where the \(\)-free parts are crucial elements of the equations signifying the rates of mortality, infection, recovery, or growth. We believe that discovering closed-form equations for these systems would prove invaluable in understanding their behavior.

**Limitations and open challenges.** D-CIPHER may fail in some scenarios, either due to _challenging experimental settings_ or a _challenging underlying PDE_. Challenging experimental settings might include unobserved variables, high measurement noise, infrequent sampling, and inadequate domain (e.g., small time horizon). Challenging PDE forms might include a PDE outside of the VR-PDE class or a \(\)-free part with a complex expression that is difficult to find. We note that we address some of these challenges by utilizing a variational approach, defining VR-PDEs to be a very general class of equations, and designing CoLLie, enabling a thorough search across closed-form expressions.

**Ethics Statement.** We want to emphasize that D-CIPHER was designed to facilitate the process of scientific discovery by extracting closed-form PDEs from data. It is not intended to or capable of replacing human experts in the modeling process. No human-derived data was used.