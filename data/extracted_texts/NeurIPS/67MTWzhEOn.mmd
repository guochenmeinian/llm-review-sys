# Revisit the Power of Vanilla Knowledge Distillation:

from Small Scale to Large Scale

 Zhiwei Hao\({}^{1,2}\), Jianyuan Guo\({}^{3}\), Kai Han\({}^{2}\), Han Hu\({}^{1}\), Chang Xu\({}^{3}\), Yunhe Wang\({}^{2}\)

\({}^{1}\)School of information and Electronics, Beijing Institute of Technology.

\({}^{2}\)Huawei Noah's Ark Lab.

\({}^{3}\)School of Computer Science, Faculty of Engineering, The University of Sydney.

{haozhw, hhu}@bit.edu.cn, jguo5172@uni.sydney.edu.au,

{kai.han, yunhe.wang}@huawei.com, c.xu@sydney.edu.au

Equal contribution. Corresponding author.

###### Abstract

The tremendous success of large models trained on extensive datasets demonstrates that scale is a key ingredient in achieving superior results. Therefore, the reflection on the rationality of designing knowledge distillation (KD) approaches for limited-capacity architectures solely based on small-scale datasets is now deemed imperative. In this paper, we identify the _small data pitfall_ that presents in previous KD methods, which results in the underestimation of the power of vanilla KD framework on large-scale datasets such as ImageNet-1K. Specifically, we show that employing stronger data augmentation techniques and using larger datasets can directly decrease the gap between vanilla KD and other meticulously designed KD variants. This highlights the necessity of designing and evaluating KD approaches in the context of practical scenarios, casting off the limitations of small-scale datasets. Our investigation of the vanilla KD and its variants in more complex schemes, including stronger training strategies and different model capacities, demonstrates that vanilla KD is elegantly simple but astonishingly effective in large-scale scenarios. Without bells and whisties, we obtain state-of-the-art ResNet-50, ViT-S, and ConvNeXtV2-T models for ImageNet, which achieve 83.1%, 84.3%, and 85.0% top-1 accuracy, respectively. PyTorch code and checkpoints can be found at https://github.com/Hao840/vanillaKD.

## 1 Introduction

In recent years, deep learning has made remarkable progress in computer vision, with models continually increasing in capability and capacity [1; 2; 3; 4; 5]. The philosophy behind prevailing approach to achieving better performance has been _larger is better_, as evidenced by the success of increasingly deep [2; 6; 7; 8] and wide [9; 10; 11] models. Unfortunately, these cumbersome models with numerous parameters are difficult to be deployed on edge devices with limited computation resources [12; 13; 14], such as cell phones and autonomous vehicles. To overcome this challenge, knowledge distillation (KD)  and its variants [16; 17; 18; 19; 20; 21; 22; 23; 24] have been proposed to improve the performance of compact student models by transferring knowledge from larger teacher models during training.

Thus far, most existing KD approaches in the literature are tailored for small-scale benchmarks (_e.g._, CIFAR ) and small teacher-student pairs (_e.g._, Res34-Res18  and WRN40-WRN16 ). However, downstream vision tasks [26; 27; 28; 29] actually require the backbone models to bepre-trained on large-scale datasets (_e.g._, ImageNet ) to achieve state-of-the-art performances. Only exploring KD approaches on small-scale datasets may fall short in providing a comprehensive understanding in practical scenarios. Given the availability of large-scale benchmarks and the capacity of large models , it remains uncertain _whether previous approaches will remain effective_ in more complex schemes involving stronger training recipes, different model capacities, and larger data scales.

In this paper, we delve deep into this question and thoroughly study the crucial factors in deciding distillation performances. We point out the _small data pitfall_ in current knowledge distillation literature: once a sufficient quantity of training data is reached, different conclusions emerge. For example, when evaluated on CIFAR-100 (50K training images), KD methods meticulously designed on such datasets can easily surpass vanilla KD. However, when evaluated on datasets with larger scale, i.e., ImageNet-1K (1M training images), vanilla KD achieves on par or even better results compared to other methods.

To break down this problem, we start by compensating for the limited data via training for longer iterations  on small-scale datasets. Albeit with longer schedule, meticulously designed KD methods still outperform vanilla KD by a large margin. It follows that large-scale datasets are necessary for vanilla KD to reach its peak.

We further investigate the crucial factors in deciding KD performances and carefully study two key elements, _i.e._, training strategy and model capacity. In terms of different training strategies, we have made the following observations: (i) By evaluating the meticulously designed methods  which perform well on small-scale benchmarks under different training recipes  on large-scale datasets such as ImageNet , it is evident that with the enhancement of data augmentation techniques and longer training iterations, the gap between vanilla KD  and other carefully designed KD methods gradually diminishes. (ii) Our experiments also demonstrate that logits-based methods  outperform hint-based methods  in terms of generalizability. With the growing magnitude of datasets and models, teacher and student models possess varying capacities to handle intricate distributions. Consequently, the utilization of the hint-based approach, wherein the student imitates the intermediate features of the teacher, becomes less conducive to achieving satisfactory results.

With regard to model capacity, we compare teacher-student pairs of different scales, _e.g._, using Res34 to teach Res18 _vs._ using Res152 to teach Res50. The outcome reveals that vanilla KD ends up achieving on par performance with the best carefully designed methods, indicating that the impact of model capacity on the effectiveness of knowledge distillation is in fact small.

Throughout our study, we emphasize that due to _small data pitfall_, the power of vanilla KD is significantly underestimated. In the meantime, meticulously designed distillation methods may instead become suboptimal when given stronger training strategies  and larger datasets . Furthermore, directly integrating vanilla KD to ResNet-50 , ViT-Tiny, ViT-Small , and ConvNeXtV2  architectures leads to an accuracy of 83.1%, 78.1%, 84.3%, and 85.0% on ImageNet respectively, surpassing the best results reported in the literature  without additional bells and whistles. Our results provide valid evidence for the potential and power of vanilla KD and indicate its practical value. In addition, it bears further reflection on whether it is appropriate to design and evaluate KD approaches on small-scale datasets. Finally, we demonstrate that improving the backbone architecture with higher performance on ImageNet can also significantly enhance downstream tasks such as object detection and instance segmentation .

Although this work does not propose a novel distillation method, we believe our identification of _small data pitfall_ and a series of analyzes based on this would provide valuable insights for the vision community in the field of knowledge distillation and the pursuit of new state-of-the-art results under more practical scenarios. Moreover, we anticipate that the released checkpoints of commonly used architectures will facilitate further research on downstream tasks.

Figure 1: The performance gap between vanilla KD and other carefully designed approaches gradually diminishes with the increasing scale of datasets.

Small data pitfall: limited performance of vanilla KD on small-scale dataset

### Review of knowledge distillation

Knowledge distillation (KD) techniques can be broadly categorized into two types based on the source of information used by the pre-trained teacher model: those that utilize the teacher's output probabilities (logits-based) [19; 18; 15] and those that utilize the teacher's intermediate representations (hint-based) [16; 17; 22; 21; 42]. Logits-based approaches leverage teacher outputs as auxiliary signals to train a smaller model, known as the student:

\[_{}=_{}(^{s},y)+(1- )_{}(^{s},^{t}),\] (1)

where \(^{s}\) and \(^{t}\) are logits of the student and the teacher. \(y\) is the one-hot ground truth label. \(_{}\) and \(_{}\) are classification loss and distillation loss, such as cross-entropy and KL divergence, respectively. The hyper-parameter \(\) determines the balance between two loss terms. For convenience, we set \(\) to 0.5 in all subsequent experiments.

Besides the logits, intermediate hints (features)  can also be used for KD. Considering a student feature \(^{s}\) and a teacher feature \(^{t}\), hint-based distillation is achieved as follows:

\[_{}=_{}(_{}( ^{s}),_{}(^{t})),\] (2)

where \(_{}\) and \(_{}\) are transformation modules for aligning two features. \(_{}\) is a measurement of feature divergence, _e.g._, \(l_{1}\) or \(l_{2}\)-norm. In common practice, the hint-based loss is typically used in conjunction with the classification loss, and we adhere to this setting in our experiments.

### Vanilla KD can not achieve satisfactory results on small-scale dataset

Recently, many studies have used simplified evaluation protocols that involve small models or datasets. However, there is a growing concern  about the limitations of evaluating KD methods solely in such small-scale settings, as many real-world applications involve larger models and datasets. In this section, we extensively investigate the impact of using small-capacity models and small-scale datasets on different KD methods. To provide a comprehensive analysis, we specifically compare vanilla KD with two state-of-the-art logits-based KD approaches, _i.e._, DKD  and DIST . Different from  which compresses large models to Res50, our primary focus is to ascertain whether the unsatisfactory performance of vanilla KD can be attributed to small student models or small-scale datasets.

**Impact of limited model capacity.** We conduct experiments using two commonly employed teacher-student pairs: Res34-Res18 and Res50-MobileNetV2. The corresponding results are shown in Table 1. Previously, these models were trained using the SGD optimizer for 90 epochs ("Original" in table). However, to fully explore the power of KD approaches, we leverage a stronger training strategy by employing the AdamW optimizer with 300 training epochs ("Improve" in table), details can be found in appendix. The original results reported in DKD and DIST demonstrate a clear performance advantage over vanilla KD. However, when adopting a stronger training strategy, all three approaches exhibit improved results. Notably, the performance gap between vanilla KD and the baselines surprisingly diminishes, indicating that _the limited power of vanilla KD can be attributed to insufficient training_, rather than to models with small capacity.

**Impact of small dataset scale.** To investigate the impact of small dataset scale on the performance of vanilla KD, we conduct experiments using the widely used CIFAR-100 dataset . Similarly, we design a stronger training strategy by extending the training epochs from 240 to 2400 and introduce additional data augmentation schemes. As shown in Table 1, despite the improved strategy leads to better results for all three approaches again, there still remains a performance gap between vanilla KD and other baselines. Notably, the accuracy gap even increases from 1.31% to 2.17% for the Res56-Res20 teacher-student pair. These observations clearly indicate that the underestimation of vanilla KD is not solely attributed to insufficient training, but also to _the small-scale dataset_.

**Discussion.** CIFAR-100 is widely adopted as a standard benchmark for evaluating most distillation approaches. However, our experimental results show that the true potential of vanilla KD has been underestimated on such small-scale datasets. We refer to this phenomenon as the _small data pitfall_, where performance improvements observed on small-scale datasets do not necessarily translate to more complex real-world datasets. As shown in Figure 1, the performance gap between vanilla Kd and other approaches gradually diminishes with the increasing scale of benchmarks. Considering the elegantly simple of vanilla KD, more extensive experiments are required to explore its full potential.

## 3 Evaluate the power of vanilla KD on large-scale dataset

### Experimental setup

In order to fully understand and tap into the potential of vanilla KD, we conduct experiments based on large-scale datasets and strong training strategies. The experimental setup is as following.

**Datasets.** We mainly evaluate KD baselines on the larger and more complex ImageNet-1K dataset , which comprises 1.2 million training samples and 50,000 validation samples, spanning across 1000 different categories. Compared to CIFAR-100 dataset , ImageNet-1K provides a closer approximation to real-world data distribution, allowing for a more comprehensive evaluation.

**Models.** In our experiments, we mainly utilize Res50  as the student model and BEiTv2-L  as the teacher model, which is pretrained on ImageNet-1K and then finetuned on ImageNet-21K and ImageNet-1K successively. This choice is motivated by teacher's exceptional performance, as it currently stands as the best available open-sourced model in terms of top-1 accuracy when using an input resolution of 224x224. In addition, we also include ViT [3; 32] and ConvNeXtV2  as student models, and ResNet  and ConvNeXt  as teacher models. Specifically, when training ViT student, we do not use an additional distillation token .

**Training strategy.** We leverage two training strategies based on recent work  that trains high-performing models from scratch using sophisticated training schemes, such as more complicated data augmentation and more advanced optimization methods . Strategy "A1" is slightly stronger than "A2" as summarized in Table 2, and we use it with longer training schedule configurations.

**Baseline distillation methods.** To make a comprehensive comparison, we adopt several recently proposed KD methods as the baselines, such as logits-based vanilla KD , DKD , and DIST , hint-based CC , RKD , CRD , and ReviewKD .

### Logits-based methods consistently outperform hint-based methods

In this section, we conduct a comparative analysis between logits-based and hint-based distillation approaches. We utilize the widely adopted Res50 architecture  as the student model, with Res152 and BEiTv2-L serving as the teachers. The student models are distilled with two different epoch configurations: 300 and 600 epochs. The evaluation results, along with the total GPU training time for each student, are presented in Table 3. To gain a deeper understanding of the generalization of student

Table 1: Impact of dataset scale on KD performance. When adopting a stronger training strategy on large-scale dataset, vanilla KD  can achieve on par result to the state-of-the-art approaches, _i.e._, DKD  and DIST . However, this phenomenon is not observed on small-scale dataset, revealing the _small data pitfall_ that underestimates the power of vanilla KD. The symbol \(\) represents the accuracy gap between vanilla KD and the best result achieved by other approaches (marked with gray). \({}^{}\) previous recipe’ refers to results obtained through our enhanced training strategy.

models beyond the ImageNet-1K dataset, we extend the evaluation to include ImageNet-Real  and ImageNet-V2 matched frequency , which provide separate test sets.

After 300 epochs of training using strategy A2, all hint-based distillation methods exhibit inferior results compared to the logits-based baselines. Despite an extended training schedule with the stronger strategy A1, a notable performance gap remains between the two distillation categories. Moreover, it is worth noticing that hint-based approaches necessitate significantly more training time, highlighting their limitations in terms of both effectiveness and efficiency.

**Discussion.** Our experiments demonstrate that logits-based methods consistently outperform hint-based methods in terms of generalizability. We speculate that this discrepancy can be attributed to the different capabilities of the teacher and student models when dealing with complex distributions. The hint-based approach mimicking the intermediate features of the teacher model becomes less suitable, hindering it from achieving satisfactory results. Furthermore, hint-based methods may encounter difficulties when using heterogeneous teacher and student architectures due to distinct learned representations, which impede the feature alignment process. To analyze this, we perform a center kernel analysis (CKA)  to compare the features extracted by Res50 with those of Res152 and BEiTv2-L. As depicted in Figure 2, there is a noticeable dissimilarity between the intermediate

   Setting & T. Res. & S. Res. & BS & Optimizer & LR & LR decay & Warmup ep. & AMP & EMA & Label Loss \\  Common & 224 & 224 & 2048 & LAMB & 5e-3 & cosine & 5 & ✓ & \(\) & BCE \\   Setting & WD & Smoothing & Drop path & Repeated Aug. & H. Flip & PRC & Rand Aug. & Mixup & Cutmix \\  A2 & 0.03 & \(\) & 0.05 & 3 & ✓ & ✓ & 7/0.5 & 0.1 & 1.0 \\ A1 & 0.01 & 0.1 & 0.05 & 3 & ✓ & ✓ & 7/0.5 & 0.2 & 1.0 \\   

Table 2: Training strategies used for distillation. “A1” and “A2” represent two strategies following , The upper table provides the shared settings, while the bottom presents their specific settings.

    &  &  &  &  &  &  \\  & & & & hours & IN-1K & IN-Real & IN-V2 & IN-1K & IN-Real & IN-V2 \\   & CC  & ✓ & 300 & 265 & 79.55 & 85.41 & 67.52 & 79.50 & 85.39 & 67.82 \\  & RKD  & ✓ & 300 & 282 & 79.53 & 85.18 & 67.42 & 79.23 & 85.14 & 67.60 \\  & CRD  & ✓ & 300 & 307 & 79.33 & 85.25 & 67.57 & 79.48 & 85.28 & 68.09 \\  & ReviewKD  & ✓ & 300 & 439 & 80.06 & 85.73 & 68.85 & 79.11 & 85.41 & 67.36 \\  & DKD  & ✓ & 300 & 265 & 80.49 & 85.36 & 68.65 & 80.77 & **86.55** & 68.94 \\  & DIST  & ✓ & 300 & 265 & **80.61** & **86.26** & **69.22** & 80.70 & 86.06 & 69.35 \\   & vanilla KD  & ✕ & 300 & 264 & 80.55 & 86.23 & 69.03 & **80.89** & 86.32 & **69.65** \\   & CC  & ✓ & 600 & 529 & 80.33 & 85.72 & 68.82 & - & - & - \\  & RKD  & ✓ & 600 & 564 & 80.38 & 85.86 & 68.38 & - & - & - \\  & CRD  & ✓ & 600 & 513 & 80.18 & 85.75 & 68.32 & - & - & - \\  & ReviewKD  & ✓ & 600 & 877 & 80.76 & 86.41 & 69.31 & - & - & - \\  & DKD  & ✕ & 600 & 529 & 81.31 & 86.76 & 69.63 & **81.83** & **87.19** & **70.09** \\  & DIST  & ✕ & 600 & 529 & 81.23 & 86.72 & **70.09** & 81.72 & 86.94 & 70.04 \\   & vanilla KD  & ✕ & 600 & 529 & **81.33** & **86.77** & 69.59 & 81.68 & 86.92 & 69.80 \\   & DKD  & ✕ & 1200 & 1059 & **81.66** & **87.01** & 70.42 & **82.28** & **87.41** & **71.10** \\  & DIST  & ✕ & 1200 & 1058 & 81.65 & 86.92 & 70.43 & 81.82 & 87.10 & 70.35 \\   & vanilla KD  & ✕ & 1200 & 1057 & 81.61 & 86.86 & **70.47** & 82.27 & 87.27 & 70.88 \\   

Table 3: Comparison between hint-based logits-based distillation methods. The student is Res50, while the teachers are Res152 and BEiTv2-L with top-1 accuracy of 82.83% and 88.39%, respectively. “✓” indicates that the corresponding method is hint-based. GPU hours are evaluated on a single machine with 8 V100 GPUs.

Figure 2: Feature similarity heatmap measured by CKA. _Left_: similarity between homogeneous architectures; _Right_: similarity between heterogeneous architectures. The coordinate axes represent layer indexes.

features of BEiTv2-L and those of the Res50 student, while the Res152 features exhibit a closer resemblance to Res50. In addition to the suboptimal performance, the CKA figure further highlights the incompatibility of hint-based approaches with heterogeneous scenarios, thereby limiting their practical applicability in real-world settings. Consequently, we focus solely on logits-based distillation approaches in subsequent experiments.

### Vanilla KD preserves strength with increasing teacher capacity

Table 3 also shows the comparison between vanilla KD and other two logits-based baselines using a stronger heterogeneous BEiTv2-L teacher . This teacher has achieved the state-of-the-art top-1 accuracy on ImageNet-1K validation among open-sourced models.

In all the evaluated settings, the three logits-based approaches exhibit similar performance. After being distilled for 300 epochs with Res152 as the teacher, vanilla KD achieves a top-1 accuracy of 80.55% on ImageNet-1K validation set, which is only 0.06% lower than the best student performance achieved by DIST. With an extended training duration of 600 epochs, the student trained using vanilla KD even achieves the top performance at 81.33%. When taught by BEiT-L, vanilla KD obtains 80.89% top-1 accuracy, surpassing both DKD and DIST. Even in the presence of distribution shift, vanilla KD maintains comparable performance to other approaches on ImageNet-Real and ImageNet-V2 matched frequency benchmarks. Additionally, further extending the training schedule to 1200 epochs improves vanilla KD and results in a performance on par with DKD and DIST, thus highlighting its effectiveness in delivering impressive results.

These results clearly indicate that vanilla KD has been underestimated in previous studies because they are designed and evaluated on small-scale datasets. When trained with both powerful homogeneous and heterogeneous teachers, vanilla KD achieves competitive performance that is comparable to state-of-the-art methods while still maintaining its simplicity.

### Robustness of vanilla KD across different training components

In this section, we perform ablation studies to assess the robustness of vanilla KD under different training settings. Specifically, we compare various loss functions used in vanilla KD and explore different hyper-parameter configurations for the three logits-based distillation methods mentioned earlier.

**Ablation on Loss functions.** Since our strategies  focus on training the student using one-hot labels and the binary cross-entropy (BCE) loss function, we also evaluate a binary version of the KL divergence as an alternative for learning from soft labels:

\[_{}=_{i}[-_{i}^{t}(_{i }^{s}/_{i}^{t})-(1-^{t})((1-^{s})/(1-^{t}))],\] (3)

where \(\) denotes the label space.

   Epoch & Hard Label & Soft Label & Acc. (\%) \\ 
300 & CE & KL & 80.49 \\
300 & BCE & KL & **80.89** \\
300 & - & KL & 80.87 \\
300 & CE & BKL & 79.42 \\
300 & BCE & BKL & 79.36 \\
300 & - & BKL & 80.82 \\ 
600 & BCE & KL & **81.68** \\
600 & - & KL & 81.65 \\   

Table 4: Ablation of different losses used to learn from hard label and soft label in vanilla KD. The “CE”, “BCE”, and “KL” refers to cross-entropy loss, binary cross-entropy loss, and KL divergence measurement, respectively. BKL is a binary counterpart of KL. Its definition is provided in Equation 3.

   LR & WD & DKD  & DIST  & KD  \\ 
2e-3 & 0.02 & 79.50 & 79.69 & 79.77 \\
2e-3 & 0.03 & 79.52 & 79.58 & 79.78 \\
3e-3 & 0.02 & 80.18 & 80.32 & 80.46 \\
3e-3 & 0.03 & 80.24 & 80.41 & 80.37 \\
5e-3 & 0.01 & 80.73 & 80.62 & 80.82 \\
5e-3 & 0.02 & 80.76 & 80.71 & 80.82 \\
5e-3 & 0.03 & **80.81** & **80.79** & **80.89** \\
5e-3 & 0.04 & 80.72 & 80.39 & 80.77 \\
6e-3 & 0.02 & 80.58 & 80.42 & 80.89 \\
7e-3 & 0.02 & 80.60 & 80.20 & 80.84 \\
8e-3 & 0.02 & 80.51 & 80.32 & 80.61 \\
8e-3 & 0.03 & 80.52 & 80.46 & 80.66 \\
8e-3 & 0.04 & 80.55 & 80.18 & 80.61 \\   

Table 5: Ablation about different configurations of learning rate and weight decay on ImageNet-1K. LR: learning rate; WD: weight decay.

[MISSING_PAGE_FAIL:7]

a teacher pre-trained on a larger dataset has a better understanding of the data distribution, which subsequently facilitates student learning and leads to improved performance.

### Exploring the power of vanilla KD with longer training schedule

In Section 3.3, we notice a consistent improvement in the performance of vanilla KD as the training schedule is extended. To explore its ultimate performance, we further extended the training schedule to 4800 epochs. In this experiment, we utilized a BEiT-B teacher model, which achieved 86.47% top-1 accuracy on the ImageNet-1K validation set. As for students, we employed Res50 , ViT-T, ViT-S , and ConvexNetV2-T , and trained them using strategy "A1". The corresponding results are shown in Table 8. All three students achieve improved performance with longer training epochs. This trend is also consistent with the pattern observed in . When trained with 4800 epochs, the students achieve new state-of-the-art performance, surpassing previous literature . Specifically, the Res50, ViT-S, ViT-T, and ConvNeXtV2-T achieve 83.08%, 84.33%, 78.11% and 85.03%2 top-1 accuracy, respectively.

### Comparison with masked image modeling

Masked image modeling (MIM) framework  has shown promising results in training models with excellent performance. However, it is worth noting that the pre-training and fine-tuning process of MIM can be time-consuming. In this section, we conduct a comparison between MIM and vanilla KD, taking into account both accuracy and time consumption. Specifically, we compare vanilla KD with the FCMIM framework proposed in ConvNeXtv2 . We utilize a ConvNeXtv2-T student model and a BEiTv2-B teacher model. We train two student models using vanilla KD: one for 300 epochs and another for 1200 epochs.

The results are presented in Table 10, highlighting the efficiency of vanilla KD in training exceptional models. Specifically, even with a training duration of only 300 epochs, vanilla KD achieves an accuracy of 84.42%, surpassing MIM by 0.53%, while consuming only one-fifth of the training time. Moreover, when extending the training schedule to 1200 epochs, vanilla KD achieves a performance of 85.03%, surpassing the best-performing MIM-trained model by 1.14%. These results further demonstrate the effectiveness and time efficiency of vanilla KD compared to the MIM framework.

### Transferring to downstream task

To assess the transfer learning performance of the student distilled on ImageNet, we conduct experiments on object detection and instance segmentation tasks using COCO benchmark . We adopt Res50 and ConvNeXtV2-T as backbones, and initialize them with the distilled checkpoint. The detectors in our experiments are commonly used Mask RCNN  and Cascade Mask RCNN .

   Method & EP. & Hours & Acc. (\%) \\  FCMIM (IN-1K) & 800 & 1334 & - \\ + Fine-tune on IN-1K & 300 & 1902\({}^{}\) & 82.94 \\  FCMIM (IN-1K) & 800 & 1334 & - \\ + Fine-tune on IN-21K & 90 & 3223 & - \\ + Fine-tune on IN-1K & 90 & 3393 & 83.89 \\  vanilla KD & 300 & 653 & 84.42 \\ vanilla KD & 1200 & 2612 & **85.03** \\   

Table 10: Vanilla KD _vs._ FCMIM on ConveNeXtV2-T. \({}^{}\): the GPU hours of fine-tuning stage is 568.

   Model  Epoch & 300 & 600 & 1200 & 4800 \\  Res50 & 80.96 & 81.64 & 82.35 & **83.08** \\ ViT-S & 81.38 & 82.71 & 83.79 & **84.33** \\ ViT-T & - & - & 77.19 & **78.11** \\ ConvNextV2-T & 84.42 & 84.74 & **85.03** & - \\   

Table 8: Results of extended training schedule. The teacher model is BEiTv2-B.

   Method & Teacher & 300 Ep. & 600 Ep. & 1200 Ep. \\   & BEiTv2-B & 80.76 & 81.68 & **82.31** \\  & BEiTv2-L & **80.77** & **81.83** & 82.28 \\   & BEiTv2-B & **80.94** & **81.88** & **82.33** \\  & BEiTv2-L & 80.70 & 81.72 & 81.82 \\   & BEiTv2-B & **80.96** & 81.64 & **82.35** \\  & BEiTv2-L & 80.89 & **81.68** & 82.27 \\   

Table 9: Impact of different teacher model sizes. The student model is Res50.

Table 11 reports the corresponding results. When utilizing our distilled Res50 model as the backbone, Mask RCNN outperforms the best counterpart using a backbone trained from scratch by a significant margin (+3.1% box AP using "1\(\)" schedule and +2.1% box AP using "2\(\)" schedule). Similarly, when the backbone architecture is ConvNeXtV2-T, using the model trained by vanilla KD consistently leads to improved performance. These results demonstrate the efficient transferability of the performance improvements achieved by vanilla KD to downstream tasks.

## 4 Related work

**Knowledge distillation (KD).** Previous KD methods can be broadly categorized into two classes: logits-based methods and hint-based methods. Hinton _et al_.  first proposes to train the student to learn from logits of teacher. This method is referred as vanilla KD. Recently, researchers have introduced improvements to the vanilla KD method by enhancing the dark knowledge [19; 51], relaxing the constraints , using sample factors to replace the unified temperature . Hint-based methods train the student to imitate learned representations of the teacher. These methods leverage various forms of knowledge, such as model activation [16; 53; 54], attention map , flow of solution procedure , or feature correlation among samples [22; 20]. To facilitate the alignment of features between the teacher and the student, techniques including design new projector [57; 58] and more complex alignment approaches [17; 59; 60] have been proposed. Besides directly mimicking the hint knowledge, contrastive learning  and masked modeling  have also been explored. Recently, the success of vision transformers  has prompted the development of specific distillation approaches tailored to this architecture, including introducing additional distillation tokens [32; 62] or perform distillation at the fine-grained patch level . Additionally, vision transformer architectures have also been leveraged to facilitate KD between CNN models [64; 65]. Furthermore, some works attempt to address specific challenges in KD, such as distilling from ensembles of teachers [66; 67] and investigating the impact of different teachers on student performance [68; 47; 69].

**Large-scale training.** Recent developments in large-scale deep learning [31; 48] suggest that by scaling up data [3; 70], model size [6; 7; 8] and training iteration  to improve model performance, achieve state-of-the-art results  across diverse tasks, and enable better adaptability to real-world scenarios. For example, in computer vision, numerous large models have achieved outstanding results on tasks such as classification, detection, and segmentation. These models are usually trained on extensive labeled image datasets, including ImageNet-21K  and JFT [15; 72]. Similarly, in natural language processing, training models on massive text corpora [73; 74] has led to breakthroughs in language understanding, machine translation, and text generation tasks. Inspired by this scaling trend, it becomes crucial to evaluate KD approaches in more practical scenarios. After all, the goal of KD is to enhance the performance of student networks.

The closest work to ours is that of Beyer _et al_. . They identify several design choices to make knowledge distillation work well in model compression. Taking it a step further, we point out the small data pitfall in current distillation literature: small-scale datasets limit the power of vanilla KD. We thoroughly study the crucial factors in deciding distillation performance. In addition, we have distilled several new state-of-the-art backbone architectures, further expanding the repertoire of high-performing models in vision arsenal.

Table 11: Object detection and instance segmentation results on COCO. We compare checkpoints with different pre-trained accuracies on ImageNet when used as backbones in downstream tasks.

Conclusion and discussion

Motivated by the growing emphasis on _scaling up_ for remarkable performance gains in deep learning, this paper revisits several knowledge distillation (KD) approaches, spanning from small-scale to large-scale dataset settings. Our analysis has disclosed the small data pitfall present in previous KD methods, wherein the advantages offered by most KD variants over vanilla KD diminish when applied to large-scale datasets. Equipped with enhanced data augmentation techniques and larger datasets, vanilla KD emerges as a viable contender, capable of achieving comparable performance to the most advanced KD variants. Remarkably, our vanilla KD-trained ResNet-50, ViT-S, and ConvNeXtV2-T models attain new state-of-the-art performances without any additional modifications.

While the application of knowledge distillation on large-scale datasets yields significant performance improvements, it is important to acknowledge the associated longer training times, which inevitably contribute to increased carbon emissions. These heightened computational resource requirements may impede practitioners from thoroughly exploring and identifying optimal design choices. Future endeavors should focus on limitations posed by longer training times and carbon emissions, and exploring new avenues to further enhance the efficacy and efficiency of distillation methods.