# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

. In particular, during the search phase of DARTS, as the number of training epochs increases, the searched architectures will fill with parametric-free operations Chu et al. [2019a], such as skip connections or even random noise, leading to performance collapse. These issues waste a lot of computational resources and seriously hinder the application of DARTS.

The domination of skip connections is the major robustness issue of DARTS Wang et al. . Many studies have investigated why the domination of skip connections happens and how to alleviate this issue. Most of them believe that skip connections exhibit unfair advantages in gradient-based optimization during the search phase Liang et al. , Chu and Zhang , Chu et al. [2019a] or stabilize the supernet training by constructing residual blocks Chu et al. . To address this problem, they propose eliminating these unfair advantages Chu et al. [2019a] or separating the stabilizing role Chu et al. , mainly drawing inspiration from ResNet He et al. . However, the generic optimization in ResNet differs fundamentally from the bi-level optimization in DARTS. Therefore, observations and theories based on uni-level optimization are insufficient to support the claim that skip connections still have unfair advantages in DARTS. The work Wang et al.  analyzes this problem from the perspective of architecture selection and proposes that the magnitude of the architecture parameter may not reflect the strength of the operation. Based on this assumption, a perturbation-based architecture selection method is proposed. However, this idea conflicts with the fundamental motivation of DARTS, which utilizes continuous relaxation to solve the combinatorial optimization problem approximately. Xie et al.  suggests that overfitting in bi-level optimization is the primary reason for the catastrophic failure of DARTS and alayzes it from a unified theoretical framework, which is similar to our conjecture.

In contrast to the above sophisticated assumptions, in this paper, we take a simpler and more straightforward perspective to consider the reasons for the domination of skip connections. We observe that the robustness problem of DARTS is caused by overfitting. Specifically, the parameters of operations (e.g., convolution) in the supernet overfit the training data and gradually deviate from the validation data. Meanwhile, the architecture parameters are trained on the validation data, causing the advantages of parametric operations to weaken progressively and ultimately leading to the domination of parametric-free operations. It is evident that the supernet is always overparameterized. Additionally, the architecture parameters also affect the supernet training, with operations that exhibit advantages in the early stage of search being more prone to overfitting.

Based on the above observations, we further propose an operation-level early stopping (OLES) 1 method to address the domination of skip connections. The OLES method monitors each operation in the supernet to determine if it tends to overfit the training data and stops training the operation when overfitting occurs. Specifically, we utilize the gradient matching (GM) method Hu et al.  to compare the gradient directions of operation parameters on the training data and the validation data. If the difference between them remains large in multiple consecutive iterations, the operation will stop training. This approach not only alleviates the domination of skip connections and but also sharpens the distribution of architecture parameters with more search iterations, thereby reducing the performance gap caused by discretization. Moreover, our method only needs to maintain the directions of operation gradients. Due to only minimal modifications to the original DARTS, the additional overhead is negligible.

In summary, we make the following contributions:

* We analyze the robustness issue of DARTS from a new perspective, focusing on the overfitting of operations in the supernet.
* We propose the operation-level early stopping method to address the domination of skip connections in DARTS with negligible computation overheads.
* Extensive experiments demonstrate the effectiveness and efficiency of our method, which can achieve state-of-the-art (SOTA) performance on multiple commonly used image classification datasets.

Background and Related Work

### Neural Architecture Search

Over the years, a variety of NAS methods have been proposed, including evolutionary-based Termitthikun et al. (2021); Real et al. (2018); Guo et al. (2020), reinforcement-learning-based Zoph and Le (2016); Pham et al. (2018); Pathar et al. (2018), and gradient-based Liu et al. (2018); Xie et al. (2018); Cai et al. (2018) methods. They have achieved great success in many computer vision (CV) and natural language processing (NLP) So et al. (2019) tasks. As a pioneering work, NAS-RL Zoph and Le (2016) uses an RNN controller to generate architectures and employs the policy gradient algorithm to train the controller. However, this method suffers from low search efficiency. To improve the search efficiency, the cell-based micro search space Zoph et al. (2017) and weight-sharing mechanism Pham et al. (2018) have been proposed. DARTS Liu et al. (2018), which also takes advantage of weight-sharing, utilizes the gradient-based method to optimize architecture parameters and model weights alternatively. Recently, train-free NAS methods Chen et al. (2021); Li et al. (2023) that utilize proxy metrics to predict the ranking or test performance of architectures without training have been proposed to further promote search efficiency.

### Preliminaries of Differentiable Neural Architecture Search

We will now provide a detailed review of DARTS. The original DARTS Liu et al. (2018) relies on a cell-based micro search space, where each cell contains \(N\) nodes and \(E\) edges organized in a directed acyclic graph (DAG). Each node represents a feature map \(x^{(i)}\), and each edge is associated with an operation \(o\), where \(\) denotes the set of candidate operations (e.g., skip connect, sep conv 3x3, etc.). During architecture search, instead of applying a single operation to a specific node, continuous relaxation is applied to relax the categorical choice of a specific operation to a mixture of candidate operations, i.e., \(^{(i,j)}(x)=_{o}_{o})}{_ {o^{}}(^{(i,j)}_{o^{}})}o(x)\). \(=\{^{(i,j)}\}\) serves as the architecture parameters. DARTS then jointly optimizes architecture parameters \(\) and model weights \(w\) with the following bi-level objective via alternative gradient descent:

\[_{}_{val}(w^{*},)\] \[ w^{*}=*{arg\,min}_{w}_{train}(w,).\]

Since DARTS follows the weight-sharing mechanism, we refer to the continuous relaxed network as the "supernet". At the end of the search phase, a discretization phase is performed by selecting operations with the largest \(\) values to form the final architecture.

### Robustifying DARTS

Despite its simplicity and efficiency, the undesired domination of skip connections is often observed in DARTS. Previous works have attempted to eliminate the domination of skip connections by diminishing their unfair advantages. DARTS+Liang et al. (2019) uses a fixed value to constrain the number of skip connections. Progressive DARTSChen et al. (2019) adds a Dropout after each skip connection operation. DARTS\(-\)Chu et al. (2020) introduces an auxiliary skip connection to separate the unfair advantage of the skip connection by analyzing it from the perspective of ResNet He et al. (2015). Another strategy is to remove the exclusive competition of operation selection in DARTS. FairDARTSChu et al. (2019) replaces the _Softmax_ relaxation of DARTS with _Sigmoid_ and offers an independent architecture parameter for each operation. NoisyDARTSChu and Zhang (2020) injects unbiased noise into the candidate operations to ensure that the good ones win robustly. However, DARTS+PT Wang et al. (2021) believes that the magnitude of the architecture parameter is not a good indicator of operation strength and proposes a perturbation-based architecture selection method.

Our work is also related to GM-NAS Hu et al. (2022), which falls in the field of Few-Shot NAS Zhao et al. (2021). Although our method and GM-NAS both employ the GM score as a mathematical tool, they are entirely distinct lines of work. GM-NAS argues that due to coupled optimization between child architectures caused by weight-sharing, One-Shot supernet's performance estimation could be inaccurate, leading to degraded search results. As a result, GM-NAS proposes to reduce the level of weight-sharing by splitting the One-Shot supernet into multiple separated sub-supernets.

GM-NAS utilizes GM scores to make splitting decisions, determining whether a module should be shared among child architectures. The GM scores in GM-NAS are computed based on the gradient information of different child architectures on shared parameters. In contrast, we aim to address the problem of skip connection domination from a totally new perspective and employ the GM score as an indicator for early stopping, preventing operation parameter overfitting. The GM scores, in our approach, are calculated using the gradient information of parameters on training and validation data.

## 3 Motivation and Methodology

In this section, we argue that the domination of skip connections is due to the overfitting of operations in the supernet and demonstrate this hypothesis through a simple motivational experiment. Based on our observations, we propose an operation-level early stopping method to address this issue.

### The Domination of Skip-Connection is Due to Overfitting

In DARTS, we observe that skip connections dominate due to the overfitting of operations in the supernet to the training data, causing them to increasingly deviate from the distribution of the validation data. However, the architecture parameters are trained on the validation data, thus operations are selected based on their validation performance. As the number of training iterations increases, the parametric operations overfit more seriously, and the parametric-free operations gradually dominate.

To verify the above conjecture, we present empirical evidence on the relationship between the overfitting of operations in the supernet and architecture parameters. We retain only architectural parameters on one edge in DARTS' search space and set all other architectural parameters to \(0\) to ensure that they will not be trained. In Figure 1, we record the architecture parameter and the validation loss with the corresponding output of each operation in this edge. From Figure 0(a), we can observe that the validation losses of all parametric operations grow even faster as the training steps increase, and the architecture parameter gradually decreases, exhibiting an obvious negative correlation with the validation loss. Moreover, Figure 0(b) shows that the skip connection easily obtains higher architecture parameters, exhibiting advantages over other parametric-free operations. This is mainly due to the fact that the skip connection can always retain more information from the input.

Most existing works suggest that the domination of skip connections is due to their unfair advantage. To investigate this claim, we follow the settings of FairDARTS Chu et al. (2019) and replace the _Softmax_ relaxation in DARTS with the _Sigmoid_ relaxation. The unfair advantage of skip connections will be eliminated by removing the exclusive competition among operations. We then rerun the experiment and present the results in Figure 2, which shows that even after removing the exclusive competition, the architecture parameters still exhibit a negative correlation with the validation losses, similar to that in Figure 0(a). It illustrates that the unfair advantage of skip connections is not the fundamental reason for their domination. Instead, it is the effect of the overfitting of operations in the supernet on the training of architecture parameters.

Figure 1: The motivational experiment to vefiry the domination of skip connections is due to the overfitting of operations.

### Operation-Level Early Stopping

Based on the above observations, we propose an operation-level early stopping method to alleviate the domination of skip connections and enhance the robustness of DARTS. During the search phase, the architecture parameters have an effect on the supernet training, resulting in different operations with different architecture parameters being trained differently. Operations that exhibit advantages in the early stage will get more attention and are more prone to overfitting. Moreover, the operation is the basic unit of choice in DARTS, which selects operations to form the final architecture. Therefore, we perform early stopping for each operation. If early stopping can be performed for each operation during the supernet training, when the operation tends to overfit, the overfitting problem can have less impact on the training of architecture parameters, and the supernet can also be fully trained.

Overfitting occurs when a model is too complex and fits the training data distribution too closely, resulting in poor performance on the validating data. When a model is overfitting, the training loss continues to decrease while the validation loss increases, indicating that the optimization direction (i.e., the direction of gradients) of the model parameters on the training data will be inconsistent with that on the validation data. To identify overfitting, we propose using the gradient direction of the operation parameters. If the gradients on the training and validation data differ significantly in direction, we consider the operation to tend to overfit the training data.

```
0: Supernet weights \(w\); architecture parameters \(\); number of search iterations \(I\); number of consecutive iterations for GM \(M\); threshold of GM score for verifying overfitting \(\).
0: Searched architectures.
1: Construct a supernet \(A\);
2:for each \(i[1,I]\)do
3: Get a batch of training data \(B_{train}\);
4: Update weights \(w\) by \(_{w}_{train}(w,,B_{train})\);
5: Record the training gradient of each operation \(g^{o}_{train}\);
6: Get a batch of validation data \(B_{val}\);
7: Update parameters \(\) by \(_{}_{val}(w,,B_{val})\);
8: Record the validation gradient of each operation \(g^{o}_{val}\);
9: Compute the gradient matching score over consecutive \(M\) iterations based on Eq. 1.
10: Stop the gradient of an operation \(o\) if \(GM(g^{o}_{train},g^{o}_{val})<\)
11:endfor
12: Derive the final architecture based on learned \(\). ```

**Algorithm 1** DARTS with operation-level early stopping

Concretely, for an operation \(o\), let \(P_{o}\) denote the parameters of \(o\), and \(g^{o}_{train}\) and \(g^{o}_{val}\) denote the gradients of \(P_{o}\) on a training batch and a validation batch, respectively. We calculate the cosine similarity between \(g^{o}_{train}\) and \(g^{o}_{val}\) to quantify the differences in the direction of the operations' gradients as follows:

\[GM(g^{o}_{train},g^{o}_{val})=^{M}(g^{o}_{train}, g^{o}_{val})}{M},\]

\[(g^{o}_{train},g^{o}_{val})=_{ train} g^{o}_{val}}{|g^{o}_{train}||g^{o}_{val}|},\] (1)

\[g^{o}_{train}=_{train}(w,,B_{train})}{  P_{o}},g^{o}_{val}=_{val}(w,,B_{val} )}{ P_{o}},\]

where \(B_{train}\) and \(B_{val}\) denote batches of training data and validation data, respectively. The GM score is computed as the average cosine similarity between training gradients and validation gradients

Figure 2: The architecture parameters and validation losses of different operations in FairDARTS.

over \(M\) consecutive steps. The gradient matching score here is similar to that in GM-NAS Hu et al. (2022). When the GM score falls below a predefined threshold of \((0,1)\) within \(M\) steps, the operation is considered to be overfitting. Consequently, if \(GM<\) for an operation \(o\), the parameters of this operation will not be updated during subsequent supernet training. And the remaining operations and architecture parameters will be trained as usual.

Through the operation-level early stopping method, the overfitting of operations in the supernet can be alleviated. Benefiting from the attribute that the DARTS search process involves alternating between training the supernet on the training data and training architecture parameters on the validation data, we only need to maintain the gradient of each operation on a training data batch and a validation data batch in one iteration, with negligible additional computational overhead. As a result, OLES can alleviate the domination of skip connections, making DARTS more robust. This allows for training DARTS with more iterations to obtain better architectures and sharper architecture parameters, resulting in a smaller discretization gap and improving performance. The algorithm for DARTS with operation-level early stopping can be found in Algorithm 1.

## 4 Experiments

### Search Spaces and Experimental Settings

To verify the effectiveness of OLES, we conduct experiments on three different search spaces: the standard DARTS' search space, NAS-Bench-201, and MobileNet-like search space. We keep all experimental settings the same as the original in each search space and only use the _first-order_ optimization, as our method only needs to freeze the operation parameters during the supernet training. In addition to the standard hyperparameters in DARTS, our method introduces an additional hyperparameter, i.e., the overfitting threshold \(\), to stop operation training. We determine the threshold \(\) by averaging the cosine similarity over \(20\) iterations for \(30\) randomly initiated architectures in each search space. And the gradient matching (GM) score is dynamically computed by averaging over every \(20\) iterations throughout the entire training process to reduce variance. Following Algorithm 1, we stop updating parameters of the corresponding operation \(o\) when \(GM(g^{o}_{train},g^{o}_{val})\) is lower than the threshold \(\).

### Performance on DARTS' CNN Search Space

#### 4.2.1 Cifar-10

We conduct NAS on CIFAR-10 using DARTS' search space and then transfer the searched architectures to CIFAR-100. The threshold \(\) is set to \(0.3\). As shown in Table 1, our method OLES consistently outperforms other recent SOTA NAS methods. OLES achieves a top-1 test error of \(2.30\%\) on CIFAR-10 with nearly the same search cost as DARTS of \(0.4\) GPU-days. Moreover, the robustness of OLES is also guaranteed, as evidenced by the excellent average result of three independent runs. When the architectures searched on CIFAR-10 are transferred to CIFAR-100, our method still achieves a competitive test error of \(16.30\%\). This indicates that the architectures searched by OLES are transferable and expressive, and the architecture parameters do not overfit the CIFAR-10 dataset.

#### 4.2.2 Cifar-100

We conduct NAS on CIFAR-100, and the results are presented in Table 2. The threshold \(\) is set to \(0.4\). OLES still exhibits significant advantages, improving the test error of DARTS from \(20.58\%\) to \(17.30\%\) without incurring extra search costs. It demonstrates that our method can adapt to datasets of different sizes and consistently improve the performance of DARTS. OLES not only consistently outperforms other methods that focus on the skip-connection domination issue, but also surpasses more sophisticated NAS methods (e.g., GM+DARTS Hu et al. (2022)) on CIFAR-10 and CIFAR-100 without requiring extensive code modifications or introducing additional computational overheads.

#### 4.2.3 Transfer to ImageNet

To verify the transferability of architectures discovered by OLES, we transfer the best architecture derived from CIFAR-10 to ImageNet. Following the setting of DARTS, we train the searchedarchitecture from scratch for \(250\) epochs, and the results are presented in Table 3. Our method achieves \(24.5\%\) top-1 test error. Compared to other NAS methods, the architectures derived from OLES still exhibit competitive performance. Notably, we do not introduce any additional training techniques such as SE modules and Swish, but simply follow the original DARTS setting to transfer the architectures. Therefore, the performance of OLES on ImageNet can be further enhanced by incorporating these training tricks.

### Performance on NAS-Bench-201

NAS-Bench-201 Dong and Yang (2020) provides a unified benchmark for analyzing various up-to-date NAS algorithms. It contains \(4\) internal nodes with \(5\) operations (i.e., Zero, Skip Connection, 1\(\)1 Conv, 3\(\)3 Conv, 3\(\)3 AvgPol). NAS-Bench-201 offers a similar cell-based search space that comprised a total of \(15625\) unique architectures. The architectures are trained on three datasets (i.e., CIFAR-10, CIFAR-100, and ImageNet16-120). The comparison results are shown in Table 4, and the threshold \(\) is set to \(0.7\). DARTS performs extremely poorly on NAS-Bench-201. We suppose that it is due to the fact that the search space of NAS-Bench-201 is extremely compact, contains few parametric operations, and the number of operation parameters varies widely, which makes it difficult for DARTS to adapt to NAS-Bench-201. OLES significantly outperforms DARTS on all datasets and is competitive among other NAS methods. It demonstrates the generality of OLES, which can adapt to different search spaces and datasets.

   NAS method & Test Err(\%) &  &  &  \\    & CIFAR-10 & & & & (M) & (GPU-days) \\  DenseNet-BC Huang et al. (2016) & 3.46 & - & 25.6 & - & Manual \\ ResNet + CutOutHe et al. (2015) & 4.61 & 17.8 & 1.7 & - & Manual \\  NASNet-A + CutOut Zoph et al. (2017) & 2.65 & - & 3.3 & 1800 & RL \\ AmoebaNet-A Real et al. (2018) & \(3.34 0.06\) & - & 3.2 & 3150 & Evolution \\ AmoebaNet-B Real et al. (2018) & \(2.55 0.05\) & - & 2.8 & 3150 & Evolution \\ PNAS Liu et al. (2017) & \(3.41 0.09\) & - & 3.2 & 225 & MDL \\ ENAS + CutOut Pham et al. (2018) & 2.89 & - & 4.6 & 0.5 & RL \\  DARTS(1st) + CutOutLi et al. (2018) & \(3.00 0.14\) & \(17.76^{}\) & 3.3 & 0.4 & Gradient \\ DARTS(2nd) + CutOutLiu et al. (2018) & \(2.85 0.08^{}\) & \(17.54^{}\) & 3.4 & 0.4 & Gradient \\ P-DARTS Chen et al. (2019) & 2.50 & 16.55 & 3.4 & 0.3 & Gradient \\ R-DARTS(L2) Zela et al. (2019) & \(2.95 0.21\) & - & - & 1.6 & Gradient \\ FairDARTS Chu et al. (2019) & 2.54 & - & 2.8 & 0.4 & Gradient \\ DARTS- Chu et al. (2020) & \(2.59 0.08\) & - & \(3.5 0.13\) & 0.4 & Gradient \\ SGAS(1st) Li et al. (2020) & 2.39 & - & 3.8 & 0.25 & Gradient \\ DARTS+PT Wang et al. (2021) & \(2.48(2.61 0.08)\) & \(19.05^{}\) & 3.0 & 0.8 & Gradient \\ DFNAS Chen et al. (2021b) & \(2.46 0.03\) & - & 4.1 & 0.6 & Gradient \\ \(\)-DARTS Ye et al. (2022) & \(2.53 0.08\) & \(16.24 0.22\) & \(3.75 0.15\) & 0.4 & Gradient \\ GM+DARTS(1st) Hu et al. (2022) & \(2.35\) & 16.45 & 3.7 & 1.1 & Gradient \\
**OLES** & \(\) & \(16.30(16.35 0.05)\) & 3.4 & 0.4 & Gradient \\   

Table 1: Comparison with state-of-the-art NAS methods on CIFAR-10, and the transferred results on CIFAR-100 are also reported. \({}^{}\): reported by Chen et al. (2019). \({}^{}\): produced by rerunning the published code. \({}^{}\): transfer architectures discovered on CIFAR-10 to CIFAR-100.

   NAS method & Test Err (\%) &  &  &  \\    & & (M) & & (GPU-days) \\  ResNet + CutOutHe et al. (2015) & \(22.10^{}\) & 1.7 & - & Manual \\ PNAS Liu et al. (2017) & \(19.53^{}\) & 3.2 & 150 & MDL \\ ENAS + CutOut Pham et al. (2018) & \(19.43^{}\) & 4.6 & 0.45 & RL \\  DARTS Liu et al. (2018) & \(20.58 0.44^{}\) & 3.4 & 0.4 & Gradient \\ GDAS Dong and Yang (2019) & \(18.38\) & 3.4 & 0.2 & Gradient \\ P-DARTS Chen et al. (2019) & \(17.46^{}\) & 3.6 & 0.3 & Gradient \\ R-DARTS(L2)/Zela et al. (2019) & \(18.24^{}\) & - & 1.6 & Gradient \\ DARTS- Chu et al. (2020) & \(17.51 0.25\) & 3.3 & 0.4 & Gradient \\ PR-DARTS Zhou et al. (2020) & \(20.10^{}\) & 3.6 & 0.17 & Gradient \\ DARTS+PT Wang et al. (2021) & \(18.78^{}\) & 3.4 & 0.8 & Gradient \\ \(\)-DARTS Ye et al. (2022) & \(17.33^{}\) & \(3.83 0.08\) & 0.4 & Gradient \\ GM+DARTSHu et al. (2022) & \(17.42^{}\) & 3.6 & 1.3 & Gradient \\
**OLES** & \(\) & 3.4 & 0.4 & Gradient \\   

Table 2: Comparison with NAS methods on CIFAR-100. \({}^{}\): reported by Dong and Yang (2019). \({}^{}\): reported by Zela et al. (2019). \({}^{}\): produced by rerunning the published code.

### Performance on MobileNet-like Search Space

As described in FairDARTS Chu et al. (2019), the domination of skip connections also exists in the MobileNet-like search space. The supernet is built on MobileNetV2 Sandler et al. (2018) and comprises \(21\) choice blocks, each with \(7\) candidate operations. As the MobileNet search space is not naturally designed for DARTS, we make minor modifications to the MobileNet search space as FairDARTS and perform NAS on the ImageNet dataset. The threshold \(\) is set to \(0.4\). The results are summarized in Table 5, and the corresponding searched architecture is visualized in Appendix D.1. We can observe that OLES can also achieve highly competitive performance in the MobileNet search space.

    &  &  \\   & valid & test & valid & test & valid & test \\  DARTS(2nd) Liu et al. (2018) & \(39.77 0.00\) & \(54.30 0.00\) & \(15.03 0.00\) & \(15.61 0.00\) & \(16.43 0.00\) & \(16.32 0.00\) \\ SNAXie et al. (2018) & \(90.10 1.04\) & \(92.77 0.83\) & \(69.69 2.39\) & \(69.34 1.98\) & \(42.84 1.79\) & \(43.16 2.64\) \\ DSNAS Hu et al. (2020) & \(89.96 0.29\) & \(93.08 1.13\) & \(30.87 1.64\) & \(31.01 1.63\) & \(40.61 0.09\) & \(41.07 0.09\) \\ PC-DARTS Xu et al. (2019) & \(89.96 0.15\) & \(94.11 0.30\) & \(61.72 0.39\) & \(67.48 0.89\) & \(40.83 0.08\) & \(41.31 0.22\) \\ DARTS Chu et al. (2020) & \(91.03 0.44\) & \(93.48 0.00\) & \(71.36 1.51\) & \(71.53 1.51\) & \(44.87 1.46\) & \(45.12 0.82\) \\ DARTS Zhang et al. (2021) & \(89.86 0.60\) & \(95.58 0.22\) & \(70.57 0.24\) & \(70.83 0.48\) & \(40.38 0.59\) & \(40.89 0.68\) \\ RLNAS/random labelZhang et al. (2021) & \(89.94 0.00\) & \(90.35 0.00\) & \(70.98 0.00\) & \(70.71 0.00\) & \(43.86 0.00\) & \(43.70 0.00\) \\ DANSChen et al. (2021) & \(91.55 0.00\) & \(94.36 0.00\) & \(73.49 0.00\) & \(73.51 0.00\) & \(46.37 0.00\) & \(46.34 0.00\) \\ \(\)-DARTS Ye et al. (2022) & \(91.55 0.00\) & \(94.36 0.00\) & \(73.49 0.00\) & \(73.51 0.00\) & \(46.37 0.00\) & \(46.34 0.00\) \\ GM + DARTSHu et al. (2022) & \(91.03 0.24\) & \(93.72 0.12\) & \(71.61 0.62\) & \(71.83 0.97\) & \(42.19 0.00\) & \(42.60 0.00\) \\
**OLES** & \(90.88 0.10\) & \(93.70 0.15\) & \(70.56 0.28\) & \(70.40 0.22\) & \(44.17 0.49\) & \(43.97 0.38\) \\   

Table 4: Results on NAS-Bench201.

    &  & Params & Search Cost \\   & Top-1 & Top-5 & (M) & (GPU-days) \\  Inception-v1 Szegedy et al. (2015) & 30.2 & 10.1 & 6.6 & - & Manual \\ MobileNet-V2 Sandler et al. (2018) & 28.0 & - & 3.4 & - & Manual \\ ShuffleNet Zhang et al. (2017) & 26.3 & - & \(\)5 & - & Manual \\  NASNet-A Zoph et al. (2017) & 26.0 & 8.4 & 5.3 & 1800 & RL \\ AmoebaNet-A Real et al. (2018) & 25.5 & 8.0 & 5.1 & 3150 & Evolution \\ AmoebaNet-B Real et al. (2018) & 26.0 & 8.5 & 5.3 & 3150 & Evolution \\ PNAS Liu et al. (2017) & 25.8 & 8.1 & 5.1 & 225 & SMBO \\ MnasNet-92 Tan et al. (2018) & 25.2\({}^{}\) & \(8.0^{}\) & 4.4 & 1667 & RL \\  DARTS(2nd) + CutOutLiu et al. (2018) & 26.9 & 8.7 & 4.7 & 0.4 & Gradient \\ SNAXie et al. (2018) & 27.3 & 9.2 & 4.3 & 1.5 & Gradient \\ GDAS Dong and Yang (2019) & 25.0 & 8.5 & 5.3 & 0.2 & Gradient \\ P-DARTS Chen et al. (2019) & 24.4 & 7.4 & 5.1 & 0.3 & Gradient \\ PC-DARTS Xu et al. (2019) & 25.1 & 7.8 & 5.3 & 3.8 & Gradient \\ FairDARTS-B Chu et al. (2019) & 24.9 & 7.5 & 4.8 & 0.4 & Gradient \\ NoisyDARTS-A Chu and Zhang (2020) & 22.1 & 6.0 & 5.5 & 12 & Gradient \\ DARTS- Chu et al. (2020) & 23.8 & 6.1 & 5.5 & 4.5 & Gradient \\ DARTS+PT Wang et al. (2021) & 25.5 & 8.0 & 4.6 & 0.8 & Gradient \\ DrNAS Chen et al. (2021b) & 23.7 & 7.1 & 5.7 & 4.6 & Gradient \\ \(\)-DARTS Ye et al. (2022) & 23.9 & 7.0 & 5.5 & 0.4 & Gradient \\ GM + DARTS(2nd) Hu et al. (2022) & 24.5 & 7.3 & 5.1 & 2.7 & Gradient \\
**OLES** & 24.5 & 7.4 & 4.7 & 0.4 & Gradient \\   

Table 3: Comparison with NAS methods on ImageNet. \(\): reported by Chu et al. (2019). Other results are from their original papers. DARTS- and NoisyDARTS-A are directly searched on ImageNet and use SE modules and Swish to improve performance. Other methods in the bottom block are searched on CIFAR-10 and then transferred to ImageNet.

   NAS method & Top-1 & Top-5 & Params(M) \\  PloxylessNAS Cai et al. (2018) & 24.9 & 7.5 & 7.1 \\ FBNet-C Wu et al. (2018) & 25.1 & 7.9 & 4.4 \\ FairNAS-A Chu et al. (2019) & 24.7 & 7.6 & 4.6 \\ FairDARTS-D Chu et al. (2019) & 24.4 & 7.4 & 4.3 \\ RLNAS Zhang et al. (2021) & 24.4 & 7.4 & 5.3 \\ GM+ProxylessNAS Hu et al. (2022) & 23.4 & 7.0 & 4.9 \\
**OLES** & 24.7 & 7.6 & 4.7 \\   

Table 5: Results on ImageNet using the MobileNet-like search space.

## 5 In-Depth Analysis

### Different Gradient Direction Similarity Measures

As described in Section 3.2, we use cosine similarity to measure the difference in the operation's gradient directions between the training and validation data. To examine the effect of different gradient direction similarity measures for computing the gradient matching score, we compare cosine similarity with three other measures: \(L_{2}\) distance, direction distance, and hard threshold. The mathematical definitions of the other three measures can be found in Appendix B. The direction distance directly computes the fraction of elements that have different directions in two gradients. And the hard threshold does not use the averaged value of \(M\) iterations to compute the GM score but instead directly checks whether the direction distance can consistently be greater than a threshold in consecutive \(M\) iterations. As shown in Table 6, cosine similarity performs better than the other three methods. Therefore, we adopt cosine similarity as the gradient direction similarity measure of OLES.

### Gradient Matching Score

In Figure 3, we present the trajectories of consume similarity between gradients on training data and validation data during the DARTS training. With the training process of DARTS, the cosine similarities of parametric operations gradually decrease, which indicates that the cosine similarity can reflect the overfitting of operations. And different parametric operations show similar trajectories. Consequently, we set an unified GM score threshold for all parametric operations. Moreover, when the number of training iterations continuously increases (>50), DARTS exhibits a noticeable decline in test performance. Figure 3c further corroborates this finding, as the problem of performance collapse due to the skip connection domination issue more easily appears in NAS-Bench-201. As a result, it manifests the test performance declines at an earlier stage.

### Training With Longer Epochs

As mentioned in Bi et al. (2019) and Chu et al. (2020), training DARTS with longer epochs enables a better convergence of the supernet and architecture parameters. However, as shown in Figure 3(a) and 3(b), training standard DARTS with longer epochs exhibit serious performance collapse with the domination of skip connections. The skip connection rapidly dominates after \(50\) epochs on CIFAR-10 and takes longer epochs on CIFAR-100. And we also find that other methods (e.g., FairDARTS, and DARTS-PT) fail with longer training epochs as well. In contrast, as shown in Figure 3(c) and 3(d), OLES survives even after \(200\) epochs. The number of skip connections in OLES stays stable after a specific number of epochs since most parametric operations are frozen. It demonstrates that OLES is robust and can actually address the domination of skip connections. More experimental results and searched architectures with longer training epochs can be found in Appendix C.2 and Appendix D.2.

   Dataset & Cosine Similarity & \(L_{2}\) Distance & Direction Distance & Hard Threshold \\  CIFAR-10 & \(\) & 92.35 & 90.59 & 93.65 \\ CIFAR-100 & \(70.11\) & 69.39 & 67.40 & \(\) \\ ImageNet & \(\) & 42.38 & 38.28 & 43.86 \\   

Table 6: Test accuracy(%) with different gradient direction similarity measures on NAS-Bench-201.

Figure 3: Trajectories of the cosine similarity of parametric operations and test accuracy during the training of DARTS.

### Comparison of Kendall Rank Correlation Coefficients

Figure 5 indicates that the early stopping mechanism does not hurt the ranking performance of DARTS. Additionally, we compare the Kendall rank correlation coefficients with other NAS methods, including RandomNAS, DARTS, GDAS, and SGAS, as shown in Table 7.

Notably, the Kendall coefficient of OLES aligns closely with that of SGAS Li et al. (2020), which aims to alleviate the effect of the degenerate search-retraining correlation problem. The results demonstrate that by mitigating operation parameter overfitting, differentiable NAS is able to focus on the potential of architectures themselves, thus enhancing the correlation between search metrics and the architectures discovered.

## 6 Conclusion

This paper attempts to find the fundamental reason for the domination of skip connections in DARTS from the new perspective of overfitting of operations in the supernet. We verified our conjecture through motivational experiments and proposed the operation-level early stopping (OLES) method to solve the domination of skip connections. OLES monitors the gradient direction of each operation and determines whether it is overfitting based on the difference between the gradient directions on the training data and the validation data. OLES can solve the domination of skip connections with negligible computational overheads without imposing any limitations. Moreover, OLES achieves state-of-the-art results of \(2.30\%\) test error on CIFAR-\(10\) and \(17.30\%\) test error on CIFAR-100, meanwhile reporting superior performance when transferred to ImageNet and on search spaces of NAS-Bench-201 and MobileNet as well.

  RandomNAS\({}^{*}\) & DARTS & GDAS\({}^{*}\) & SGAS & OLES \\ 
0.0909 & 0.13 & -0.1818 & 0.42 & 0.41 \\   

Table 7: Kendall-\(\) coefficients of architecture ranks based on searching and retraining metrics. \(*\): sample 4 excellent architectures in 4 rounds. Other methods randomly select 10 architectures from a single run.

Figure 4: Trajectories of the numbers of skip connections in the normal cell and the reduction cell on CIFAR-10 and CIFAR-100.

Figure 5: Comparison of search-retraining Kendall rank correlation coefficients \(\). The results are derived from 10 randomly selected architectures from a single run of DARTS and OLES.