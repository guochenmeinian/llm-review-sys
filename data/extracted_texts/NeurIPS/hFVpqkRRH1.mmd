# Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs

Sukmin Yun\({}^{*,1,4}\), Haokun Lin\({}^{*,1}\), Rusiru Thushara\({}^{*,1}\), Mohammad Qazim Bhat\({}^{*,1}\), Yongxin Wang\({}^{*,1}\), Zutao Jiang\({}^{1,7}\), Mingkai Deng\({}^{2}\), Jinhong Wang\({}^{1}\), Tianhua Tao\({}^{1,3}\), Junbo Li\({}^{1}\), Haonan Li\({}^{1}\), Preslav Nakov\({}^{1}\), Timothy Baldwin\({}^{1}\), Zhengzhong Liu\({}^{1,5}\), Eric P. Xing\({}^{1,2,5}\), Xiaodan Liang\({}^{1,6}\), Zhiqiang Shen\({}^{1}\)

\({}^{1}\)MBZUAI, \({}^{2}\)CMU, \({}^{3}\)UIUC, \({}^{4}\)HYU ERICA, \({}^{5}\)Petuum, \({}^{6}\)SYSU, \({}^{7}\)Pengcheng Laboratory

https://mbzuai-llm.github.io/webpage2code/

Equal Contribution.

###### Abstract

Multimodal large language models (MLLMs) have shown impressive success across modalities such as image, video, and audio in a variety of understanding and generation tasks. However, current MLLMs are surprisingly poor at understanding webpage screenshots and generating their corresponding HTML code. To address this problem, we propose Web2Code, a benchmark consisting of a new large-scale webpage-to-code dataset for instruction tuning and an evaluation framework for the webpage understanding and HTML code translation abilities of MLLMs. For dataset construction, we leverage pretrained LLMs to enhance existing webpage-to-code datasets as well as generate a diverse pool of new webpages rendered into images. Specifically, the inputs are webpage images and instructions, while the responses are the webpage's HTML code. We further include diverse natural language QA pairs about the webpage content in the responses to enable a more comprehensive understanding of the web content. To evaluate model performance in these tasks, we develop an evaluation framework for testing MLLMs' abilities in webpage understanding and web-to-code generation. Extensive experiments show that our proposed dataset is beneficial not only to our proposed tasks but also in the general visual domain. We hope our work will contribute to the development of general MLLMs suitable for web-based content generation and task automation. Our data and code are available at https://github.com/MB2UAI-LLM/web2code.

## 1 Introduction

Multimodal large language models (MLLMs) have achieved explosive growth in the past few years. Leveraging the rich commonsense knowledge in large language models (LLMs), MLLMs are remarkably successful at processing and reasoning about various modalities such as image , video , and audio  in a broad range of tasks such as recognition , reasoning , and question-answering , all using language as the intermediate representation. However, existing MLLMs are surprisingly poor at understanding webpage screenshots and generating the HTML code to express their latent states. For instance, given the instruction "Parse the HTML code for this webpage", the well-known LLaVA-1.5  generates generic, pale code that fails to preserve most of the original webpage's features (see Figure 1), which hampers its utility in applications such as UI prototyping, automation agents, and accessibility (e.g., noting available buttons and options given webpage screenshot).

The essential ingredients behind the progress in MLLMs are arguably large-scale instruction datasets  and evaluation benchmarks  - the former for aligning multimodal inputswith the massive knowledge in LLMs [27; 35], and the latter for standardized comparison which facilitates model development. However, existing instruction datasets and benchmarks typically focus on general settings (e.g., visual QA and reasoning) and pay insufficient attention to webpage understanding and webpage-to-code generation, which requires a unique combination of capabilities such as optical character recognition (OCR), spatial reasoning, and long-text generation, among others. While previous work has developed datasets for these tasks [4; 22], they lack instruction information and are unsuitable for integration with general-purpose MLLMs. On the other hand, popular benchmarks [36; 25] evaluate some of the required capabilities in isolation, but not fully in combination for visual parsing and reasoning over webpages.

To fill this gap, we propose a new instruction tuning dataset and evaluation suite named Web2Code. Web2Code contains a total of 1179.7k webpage based instruction-response pairs. The responses consist of not only the HTML code, but also the structured questions and answers about the webpage, which assist a model in better understanding its information. For dataset collection, we use GPT-3.5 and GPT-4 to clean existing data (e.g. WebSRC ) as well as to generate completely new webpages in HTML code. To evaluate the MLLM's success at webpage understanding and HTML parsing, we propose the Webpage Understanding Benchmark (WUB) and Webpage Code Generation Benchmark (WCGB), two tasks that test the model's abilities to answer questions about a webpage and generate its HTML code, respectively. For the latter task, we find that traditional text similarity metrics are insufficient for evaluating the fidelity of the generated code, and instead propose to render the output HTML back to a webpage screenshot, and use GPT-4V  to evaluate the quality of the resulting webpage .

To demonstrate the utility of our dataset, we train LLaVA-style MLLMs with our dataset included in the instruction finetuning stage. Quantitative results show that finetuning on our dataset not only clearly improves the image-to-HTML-code translation ability of the MLLM, but also leads to improvements in the model's perception and reasoning abilities in webpage screenshot understanding as they are closely related. Furthermore, previous datasets like WebSight  and Pix2Code  present challenges: WebSight includes privacy-sensitive information, while Pix2Code contains random letters in full webpage screenshots. These issues limit MLLMs' ability to generate coherent text for webpage construction. In contrast, our dataset is more suitable for MLLM instruction fine-tuning, offering enhanced capabilities without compromising existing ones.

## 2 Related Work

**MLLM Dataset.** At present, there is a substantial amount of large-scale visual instruction data, primarily generated using GPT. SVIT  and LRV-Instruction  are both generated by GPT4

Figure 1: Our motivation for constructing the Web2Code dataset stems from the limitations of previous models, such as LLaVA , which are trained on general datasets and struggle to generate high-quality webpages, as in the second row. Our dataset aims to significantly enhance the quality of webpage generation as in third row while maintaining a strong level of general multimodal ability.

based on manual prompts to adjust the instruction data, including rich high-quality dialogue question and answer, complex reasoning question and answer, reference question and answer, and image detail description task datasets; similarly, ShareGPT4V , LLaVAR , LVIS-Instruct4V  use GPT-4V  to generate millions of high-quality image-text pairs, aiming to enhance the perception, reasoning and planning capabilities of MLLM. Commonly used image data sources include LAION , CC , SBU , COCO , VG , VQAv2 .

**MLLM.** Instruction Tuning MLLM has made great progress in recent years. The structure of MLLM usually contains a visual encoder, vision-language mapping module and LLM. LLaVA-v1.5  only uses MLP as the vision-language mapping module and the successful application of instruction tuning on MLLM has inspired people. The community has explored various feasible structures, which can be divided into attention structures BLIP2 , InstructBLIP , Owen-VL , ShareGPT4V  and non-attention structures LLaVA , Shikra  according to the vision-language mapping module. At the same time, various open source and more powerful LLMs, such as Vicuna1.5 , InternLM2  also help MLLM achieve richer and more extensive instruction following capabilities. Qwen-VL , OtterHD , mPLUG-Owl , InternLM-XComposer2-4KHD  increase the resolution of images, while LLaVA-NeXT , Mini-Gemini , MM1  split the input image into several image crops. In addition, BRAVE , MoVA , DeepSeek-VL , OmniFusion  apply supplementary vision encoders to obtain abundant visual features, e.g. DINOv2 , SAM . Furthermore, more computer vision models are utilized for different tasks, which include image segmentation, detection and OCR, in MOAI , CuMo  and SpatialVLM . Subsequently, MoE  was applied to MLLM to expand the scale of training data at the same computing scale.

**Code Study.** There are various code studies related to LLM. Sarker et al.  focuses on generating code functions using formula hints, aiming to enhance syntactic robustness and systematically testing the reliability of the syntax. From the perspective of security, Finkman et al.  claims that these code assistance tools may inadvertently disclose the developer's proprietary code to the code assistant service provider in the process of helping development, thus they propose a complementary method to reduce the risk of code leakage while providing effective advice to developers. In addition to code leakage, the code generated by LLM has also caused concerns in industries and other fields. To address this issue, Ye et al.  proposes a new zero-shot synthetic code detector based on the similarity between code and its rewritten variants. In the evaluation work on code generation, Du et al.  proposes a new computational efficiency benchmark Mercury and a new metric Beyond for the efficiency evaluation of code. They experimentally show that direct preference optimization can be used as a robust baseline for improving computational efficiency compared to supervised fine-tuning, which paves a promising path for future exploration of efficient code generation.

## 3 Dataset Construction

**Overview**. Our Web2Code instruction tuning dataset construction and instruction generation process involves four key components: **(1)** Creation of new webpage image-code pair data: We generate high-quality HTML webpage-code pairs following the CodeAlpaca prompt  using GPT-3.5 and convert them into instruction-following data. **(2)** Refinement of existing webpage code generation data: We transform existing datasets including WebSight  and Pix2Code  into an instruction-following data format similar to LLaVA data , so they can be used as instruction-following data to train MLLMs. **(3)** Creation of a new text question-answer pair data: We generate a new question-answer pair dataset utilizing our new GPT-3.5 generated data from (1) for webpage understanding. **(4)** Refinement of existing webpage understanding data: We refine the WebSRC  question-answer data to improve its quality using the GPT-4. Each component is elaborated in detail as follows:

**DWCG: Creation of new webpage image-code pair data for code generation.** To augment our dataset with high-quality data, we employ GPT-3.5 to generate 60K HTML pages following the guidelines and prompts in CodeAlpaca .2 Using Selenium WebDriver, we then create web image screenshots from the generated HTML code. These web image-code pairs were subsequently converted into an instruction-following data format similar to the LLaVA data format , enabling their use in training Multimodal Large Language Models (MLLMs). The example of the instruction is shown in Figure 17. The generation of instruction is done in two stages using prompts fed to GPT-4: (a) During stage 1, the prompt shown in Figure 13 resulted in the creation of generic instructions. (b)

[MISSING_PAGE_EMPTY:4]

GPT-4 for a subset of 24.35K webpage data, resulting in a total of 243.5K question-answer data points. This includes, a set of 230K question-answer pairs for GPT-3.5 based webpages, a set of 13.5K newly generated question answer pairs for refined Pix2Code images. These pairs are meticulously crafted to align with our image-based evaluation criteria, ensuring that each question probes specific aspects of the visual and content quality reflected in the generated web images. This strategy enhances the model's performance by integrating a nuanced understanding of the evaluation parameters into its learning process. To generate the DWU question-answer pair data, we use only the HTML code shown in Figure 13, which includes the compiled HTML image. Figure 2 shows the compiled HTML image alongside the corresponding question-answer pairs. For training, we input the image and questions together, excluding the HTML code for enhancing webpage understanding capabilities.

**DWU\({}_{}\): Refinement of existing webpage understanding data.** To increase our instruction-following dataset with high-quality instruction-following examples for webpages, we integrate the WebSRC dataset into our training regime. Prior to inclusion, we meticulously filter the existing question-and-answer pairs from the WebSRC dataset to ensure relevance and quality. This involves duplication removal and quality optimization as shown in Figure 3. Specifically, we find that WebSRC data contains several questions related to the same answer. To this end, we first remove those duplicates and then employed GPT-4 to assess and enhance the quality of answers. This process not only refines the dataset into 51.5K high-quality instruction data but also ensures that the model's training is influenced by high-fidelity, instructionally sound data, thereby improving its ability to follow complex web-based instructions.

### Statistics and Analysis

Figure 5 shows the word cloud of the answer set of our question-answer dataset. The word cloud highlights the most frequently occurring terms, with "section," "color", "button", and "website" being the most prominent, indicating a strong emphasis on structural and design elements in the data. This reflects the detailed focus on the layout and visual aspects of the dataset.

Figure 5 illustrates the distribution of the most common HTML tags in our GPT-3.5 generated HTML data. The distribution shows a high frequency of essential structural tags such as <div>, <p>, <meta>, <img>, and <a>, indicating that the generated pages include a diverse range of elements necessary for rich and varied web content. The significant presence of <h2>, <input>, <html>, <head>, and <body> tags further reinforces the completeness and structural integrity of the generated HTML documents.

To estimate the difficulty levels of our HTML-based webpage dataset, we provide several quantitative measures and compare them with recent and similar existing datasets, namely WebSight , Design2Code , and Pix2Code  (See Table 1).

Design2Code is primarily used for testing and has a small size of 484 examples, limiting its versatility and robustness. In contrast, our dataset, intended for both training and testing, is significantly larger (884.7K examples) and more complex, making it more suitable for developing robust models. Overall, our benchmark examples are more challenging and cover a broader spectrum of complexities compared to prior efforts such as WebSight.

### Distribution

Our instruction-following dataset contains 1,179.7K instruction data points. This includes 884.7K website image-code pairs and 295K question-answer pairs.

The 295K question-answer pairs consist of 243.5K GPT-4 based question-answer pairs (DWU Data) and 51.5K pairs from WebSRC image-based data, as shown in Table 2. Our evaluation dataset comprises 1,198 webpage screenshot images, sourced from diverse origins, including WebSight, Pix2Code, GPT-3.5-generated data, and manual processes, to ensure a broad representation of web content. Additionally, we utilize 5,990 "yes" / "no" question-answer pairs generated from the GPT-4 Vision API for our Webpage Understanding Benchmark, as in Section 4.1.

## 4 A New Evaluation Framework for Webpage

Our proposed evaluation framework includes two schemes: (1) **W**ebpage **U**nderstanding **B**enchmark (**WUB**): An offline evaluation using "yes" / "no" questions. (2) **W**ebpage **C**ode **G**eneration **B**enchmark (**WCGB**): An online evaluation (using GPT-4 Vision) based on image similarity.

### Evaluation Metric for HTML Code Generation

In the realm of assessing code quality, particularly in terms of final visual appeal and overall functionality, existing methods that rely on code similarity metrics fall short. These traditional approaches often lack the precision and reliability needed for nuanced evaluations of code effectiveness. To address these shortcomings, we have developed a novel approach: regenerating the webpage using the model's predicted HTML code and capturing screenshots of these generated webpages. This process, automated using the Selenium WebDriver extension in Python, shifts the focus from the less reliable code similarity assessments to a more accurate and visually oriented method. By comparing images of the generated webpages, we can more effectively evaluate the aesthetic and functional aspects of the code, offering a more comprehensive understanding of its quality.

  
**Dataset** & **DWU** & **DWU\({}_{}\)** \\ 
**Instruction** & \(\) & \(\) \\
**Size** & 243.5K & 51.5K \\   

Table 2: Distribution of DWU and DWU\({}_{}\) datasets. Both datasets include high-quality question-answer pairs for webpage understanding.

Figure 6: Evaluation benchmark for webpage generation and webpage understanding. **Left**: WCGB utilizes GPT4 Vision based online evaluation for image level comparison; **Right**: WUB employs an offline evaluation based on question-answer pairs.

We propose two benchmarks for assessing webpage understanding and code generation capabilities.

**WUB**: This benchmark comprises 5,990 high-quality question-answer pairs generated from GPT-4 Vision API (See prompt 16), based on 1,198 webpage screenshot images, where each answer is either "yes" or "no". These images are sourced from diverse data origins, including WebSight, Pix2Code, GPT-3.5, and manual processes, ensuring a broad representation of web content. Figure 11 shows a qualitative sample data we used for WUB. We test these pairs on various multimodal image understanding models by comparing the predicted answers to the ground truth, with the final accuracy score serving as the evaluation metric as depicted on the right side of Figure 6. Qualitative data examples in our WUB benchmark are shown in Figure 11.

**WCGB**: Utilizing the same images as the WUB, this benchmark evaluates a multimodal model tasked with generating HTML code from webpage images based on specific instructions. Unlike traditional code-level evaluations, this benchmark assesses the generated webpage's fidelity at the image level. We convert the predicted HTML codes back into images using Selenium WebDriver to allow a direct visual comparison with the ground truth images. The evaluation, depicted on the left side of Figure 6, considers 10 different aspects, which are further categorized into four evaluation matrices using the GPT-4 Vision API. This image-level evaluation provides a more accurate measure of the model's code generation capabilities, acknowledging that identical webpages can be constructed from varying codes. The prompt used for evaluation is shown in Figure 15. This framework consists of 10 distinct criteria, which we group into four categories, each encompassing specific criteria that are scored on a 0-10 scale, as follows:

1. **Visual Structure and Alignment** * _Layout Consistency_: Measures the arrangement of structural webpage elements like headers, footers, and sidebars. * _Element Alignment_: Assesses the alignment of images, buttons, and text boxes. * _Proportional Accuracy_: Checks for consistency in sizes and aspect ratios of visual elements. * _Visual Harmony_: Examines the overall balance and harmony in design.
2. **Color and Aesthetic Design** * _Color Scheme and Aesthetic Match_: Focuses on the similarity in color schemes, including hues and saturation. * _Aesthetic Resemblance_: Looks at the overall aesthetic appeal and style (modern, minimalistic, traditional, etc.).
3. **Textual and Content Consistency** * _Font Characteristics and Consistency_: Assesses uniformity in font type, size, style, and weight. * _Textual Content Match_: Evaluates the match in words and sentences. * _Numeric and Special Character Accuracy_: Checks for consistency in numbers, dates, and special characters.
4. **User Interface and Interactivity** * _User Interface Consistency_: Assesses the similarity in design language and appearance of UI elements like menus, buttons, and forms.

### Quantitative Evaluation for HTML Code Generation of MLLMs

We have evaluated the trained models using various data configurations and backbones on our WUB and WCGB benchmarks. The performance of the models on the code generation benchmark is presented in Table 3, while the results for webpage understanding are shown in Table 4.

To be specific, our dataset components have an orthogonal contribution to the overall improvements on both the WUB and the WCGB benchmarks. Table 3 demonstrates improvements in webpage code generation quality when incrementally adding Web2Code sub-datasets; + DWCG, + DWU, + DWCG\({}_{}\), and + DWU\({}_{}\). For example, the results based on instruction-tuned LLaMA-3 (in the first five rows) show step-wise improvements on the WCGB benchmark from the general domain data only (1.79\(\)6.402\(\)6.716\(\)7.806\(\)8.530 on the overall metric). Interestingly, the instruction-tuned LLaMA-3 model trained solely on general domain data shows poor performance on WCGB,while achieves comparable performance on WUB when compared to models trained with additional webpage datasets. (see Table 4). Similar trends are also found in other LLM backbones while adding the proposed dataset shows significant improvements. Table 4 further demonstrates the effectiveness of the proposed dataset on the webpage comprehension capabilities. For example, the DWCG dataset improves code generation capabilities, though it requires more webpage understanding. However, the DWU dataset not only recovers but also enhances both WUB and WCGB performances. Moreover, the refined dataset \(_{}\) primarily boosts WCGB, while \(_{}\) shows improvements across all metrics. Overall, we found the proposed dataset can enhance both webpage understanding capability and webpage code generation abilities under various LLM backbones, and LLaMA3-8B archives the best performance among all on both webpage code generation and webpage understanding.

### Visualizations for Qualitative Evaluation

As shown in Figure 7, we compare the results between the original image which is the real-world webpage sample, the rendered image generated by using LLM backbones of Vicuna1.5-7B and CrystalChat-7B, respectively. CrystalChat-7B is a code-enhanced LLM and our visualization demonstrates that it achieves the better quality of generation than Vicuna1.5-7B even though the performance is slightly worse on the general multimodal domain, as presented in Table 6. Moreover, as in Figure 8, our rendered webpage from the model trained on our web dataset closely resembles the original image, indicating the positive impact of the web2code dataset. We further visualize our generation in Figure 9 when the input is a hand-drawn webpage to examine the adaptation ability of our model.

   LLM Backbone & DWCG & DWU & \(_{}\) & \(_{}\) & WUB Accuracy (\%) \\   & - & - & - & - & 65.56 \\  & ✓ & - & - & - & 60.00 \\  & ✓ & ✓ & - & - & 69.33 \\  & ✓ & ✓ & ✓ & - & 68.68 \\  & ✓ & ✓ & ✓ & ✓ & **74.84** \\   & - & - & - & - & 73.94 \\  & ✓ & ✓ & - & - & 73.48 \\  & ✓ & ✓ & ✓ & ✓ & **74.14** \\   & - & - & - & - & 73.54 \\  & ✓ & - & - & - & 71.81 \\   & ✓ & ✓ & - & - & **73.74** \\   & - & - & - & - & 71.12 \\   & ✓ & ✓ & ✓ & ✓ & **71.23** \\   

Table 4: Accuracy of webpage understanding under various data configurations and LLM backbones. All models are instruction-tuned and evaluated on our WUB benchmark. We note that the general domain data (i.e., LLaVA) is included in all data configuration as default.

   LLM Backbone & DWCG & DWU & \(_{}\) & \(_{}\) & VSA \(\) & CAD \(\) & TCC \(\) & UII \(\) & Overall \(\) \\   & - & - & - & - & 1.563 & 1.777 & 1.894 & 1.911 & 1.79 \\  & ✓ & - & - & - & 5.613 & 6.575 & 6.551 & 6.870 & 6.402 \\  & ✓ & ✓ & - & - & 6.564 & 6.762 & 6.998 & 6.541 & 6.716 \\  & ✓ & ✓ & ✓ & - & 7.667 & 7.560 & 7.995 & 8.001 & 7.806 \\  & ✓ & ✓ & ✓ & ✓ & **8.522** & **8.564** & **8.421** & **8.611** & **8.530** \\   & - & - & - & - & 4.714 & 4.572 & 4.865 & 5.147 & 4.825 \\  & ✓ & ✓ & - & - & 7.900 & 8.001 & 8.204 & 8.215 & 8.080 \\  & ✓ & ✓ & ✓ & ✓ & **8.384** & **8.287** & **8.417** & **8.488** & **8.394** \\   & - & - & - & - & 3.832 & 3.678 & 3.411 & 3.992 & 3.728 \\  & ✓ & - & - & - & 7.812 & 7.899 & 8.138 & 8.112 & 7.990 \\   & ✓ & ✓ & - & - & 8.010 & **8.102** & 8.266 & 8.124 & 8.126 \\   & - & - & - & - & 3.042 & 3.250 & 3.333 & 3.167 & 3.198 \\   & ✓ & ✓ & ✓ & ✓ & **7.876** & **7.687** & **7.267** & **7.563** & **7.598** \\   

Table 3: Performance comparison of different LLM backbones under various data configurations on our Webpage Code Generation Benchmark (WCGB). “VSA” denotes Visual Structure and Alignment, ”CAD” represents Color and Aesthetic Design, “TCC” represents Textual and Content Consistency, and “UII” denotes User Interface and Interactivity.

## 5 General Evaluation of MLLMs Using Web2Code

**Setup and Overview.** Our model training framework mainly follows the design of LLaVA-1.5  where we leverage the capabilities of both a pre-trained visual encoder, an LLM and a projector to connect visual features into the word embedding space. The model consists of (1) a pre-trained CLIP ViT-L/14  visual encoder with a resolution of 336\(\)336 and a patch size of 14, which has good feature representation already aligned with the text embedding space. (2) As for the LLM backbones, we leverage CrystalChat  as the base model and compare it with other latest LLM backbones like Vicuna1.5 , LLaMA2 , LLaMA3  and CrystalCoder .3 Training details and hyperparameters are presented in the Appendix A.

**General Evaluation Metrics for MLLMs.** MME  serves as an extensive evaluative benchmark, aiming to assess the perceptual and cognitive capability of MLLMs within 14 sub-tasks. Additionally, we also evaluate the performance of our models on text-oriented visual question-answering tasks employing a diverse set of benchmark datasets including ScienceQA  and TextVQA . Furthermore, We assess our models' ability toward anti-hallucination through POPE .

**Effects of Web2Code on General Domain.** Here, we first perform instruction tuning using Web2Code on various LLM backbones and then we evaluate those MLLMs on the general domain of visual language understanding. Throughout extensive experiments under various data configurations, we observed that the proposed dataset Web2Code can be incorporated with the conventional visual language instruction tuning dataset of LLaVA  without harming performances on the general domain. Table 5 summarizes the results.4 Specifically, both proposed Web Understanding data (DWU

Figure 8: Visualization comparison between ground-truth code generated image and our result. The style and layout of the generated webpage image are similar to the ground-truth image.

Figure 7: Visualization of our CrystalChat-7B generation when the input is a hand-drawn webpage.

Figure 9: Visualization of our CrystalChat-7B generation when the input is a hand-drawn webpage.

or DWU\({}_{}\)) and Web Code Generation data (DWCG or DWCG\({}_{}\)) do not hurt or even can be beneficial to the visual language understanding. For example, we observed that adding DWU to CrystalChat achieves comparable or even better performances on POPE (86.86\(\)87.10), SciQA (67.77\(\)68.27), and TextVQA (57.84\(\)58.15). Somewhat surprisingly, we further found that adding DWCG can even improve visual language understanding. For example, the second and third rows of CrystalChat show +40.31 and +5.00 points higher improvements in MME-P and MME-C benchmarks, respectively. Moreover, adding refined data DWU\({}_{}\) and DWCG\({}_{}\) are still effective in the visual language domain, by achieving comparable (or even better) performances on overall benchmarks. For example, the last row indicates that adding DWU\({}_{}\) and DWCG\({}_{}\) preserves comparable performances on overall benchmarks and even achieves +0.4 higher points on the SciQA benchmark.

## 6 Conclusion

We have presented Web2Code, a benchmark that consists of a high-quality, large-scale webpage-to-code instruction tuning dataset containing 1.18M entries and an evaluation suite for the webpage understanding and webpage-to-HTML translation abilities of MLLMs. To mitigate potential data biases, we have guided the synthetic data generation process toward a balanced output and incorporated diverse existing datasets to further address bias concerns. Through extensive experiments, we have demonstrated that our proposed dataset is clearly effective at enhancing these abilities of MLLMs as well as general visual proficiency, while existing datasets lead to inferior performance. We hope our work will attract the community's attention and facilitate progress toward foundation models serving as virtual assistants for content generation and task automation.

**Limitations and Ethics Statement.** The Web2Code project provides a comprehensive dataset and evaluation framework for fine-grained multimodal large language models. This can significantly enhance the capabilities of LLMs in understanding and generating web code from instructions, leading to advancements in web development automation, and improved coding assistance tools and platforms. By enabling more accurate and context-aware code generation, it can boost productivity for developers and make coding more accessible to beginners. However, the primary limitations of the Web2Code include the potential for a biased dataset that may not cover all possible HTML coding scenarios, potentially leading to gaps in model performance, and some webpages that include humans may be privacy sensitive, Ensuring high-quality annotations and comprehensive coverage of all possible HTML and code structures is challenging. Also, handling complex, real-world HTML and code scenarios might still be beyond the current capabilities of the models trained on this dataset. Moreover, the proposed evaluation framework may not capture all aspects of the code generation quality, such as code efficiency, readability, or adherence to best practices.