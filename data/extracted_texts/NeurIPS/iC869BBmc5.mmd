# ProEdit: Simple Progression is All You Need

for High-Quality 3D Scene Editing

 Jun-Kun Chen Yu-Xiong Wang

University of Illinois Urbana-Champaign

{junkun3, yxw}@illinois.edu

immortalco.github.io/ProEdit

###### Abstract

This paper proposes ProEdit - a simple yet effective framework for high-quality 3D scene editing guided by diffusion distillation in a novel _progressive_ manner. Inspired by the crucial observation that multi-view inconsistency in scene editing is rooted in the diffusion model's large _feasible output space_ (FOS), our framework controls the size of FOS and reduces inconsistency by decomposing the overall editing task into several subtasks, which are then executed progressively on the scene. Within this framework, we design a difficulty-aware subtask decomposition scheduler and an adaptive 3D Gaussian splatting (3DGS) training strategy, ensuring high quality and efficiency in performing each subtask. Extensive evaluation shows that our ProEdit achieves state-of-the-art results in various scenes and challenging editing tasks, _all_ through a simple framework _without_ any expensive or sophisticated add-ons like distillation losses, components, or training procedures. Notably, ProEdit also provides a new way to control, preview, and select the "aggressivity" of editing operation during the editing process.

## 1 Introduction

The emergence and advancement of modern scene representation models, exemplified by neural radiance fields (NeRFs)  and 3D Gaussian splatting (3DGS) , have significantly reduced the difficulty associated with high-quality reconstruction and rendering of large-scale scenes. In addition to reconstructing known scenes, there is growing interest in editing existing scenes to create new ones.

Figure 1: By decomposing a difficult task into easy subtasks and then progressively performing them (upper part), our ProEdit achieves high-quality 3D editing results with bright colors and detailed textures along with introducing new controllability of the editing aggressivity (lower part). **More results are provided on our project page.**Among the various editing operations, the _instruction-guided scene editing_ (IGSE) stands out as one of the most free-form tasks, supporting editing based on simple text descriptions. Due to the lack of 3D supervision data to train editing models in 3D, current state-of-the-art methods tackle IGSE using _2D diffusion distillation_, which involves distilling editing signals from a pre-trained 2D diffusion model . These methods leverage the 2D diffusion model to edit rendered images of scenes from multiple viewpoints, and then reconstruct the edited scene from these edited images using specific distillation losses.

However, a substantial challenge faced by such distillation-based approaches in achieving high-quality scene editing lies in ensuring that the scene representation converges on the edited multi-view images. Failure to achieve so results in gloomy colors, blurred textures, and noisy geometries (_e.g._, the failure cases from ). We argue that **this challenge is rooted in the diffusion model's large _feasible output space_ (FOS) for the same instruction** - since a text instruction can be interpreted in different yet plausible ways. For example, "make the person wear a hat" could be implemented with a hat of any style, shape, size, position, _etc_.

Therefore, large FOS is the underlying cause of _multi-view inconsistency_ in 2D editing results, making the scene representation - originally designed for reconstructing from consistent images - hard to converge. Previous work, often unaware of this fundamental issue, deals with multi-view inconsistency by introducing inconsistency-robust distillation losses  to tolerant inconsistency, or proposing additional components and training procedures  to select consistent images from the FOS. While adding costs and complexities, these methods frequently fail to converge to a high-quality scene when the FOS is considerably large, especially for operations that change the scene's geometry.

In overcoming this challenge posed by the large FOS, _our key insight_ is to control the FOS size through _editing task decomposition_, as illustrated in Fig. 1. Building on this insight, we propose _ProEdit_, a simple, novel framework to achieve high-quality IGSE, by decomposing the original, large-FOS task into multiple _subtasks_ with significantly smaller FOS, and then _progressively_ performing high-quality editing for each of these tasks. With each subtask's FOS effectively controlled, they can be solved under a simple solution _without_ the need for additional distillation losses, components, or complex training procedures. Progressively solving all these subtasks naturally leads to a high-quality edited scene that meets the requirements of the original task.

To perform subtask decomposition, we introduce an intuitive formulation of "subtasks" with text encoding interpolation. Based on this formulation, we propose a _subtask scheduler_ to determine the subtask decomposition and guide the editing process. This decomposition consists of a sequence of subtasks, where each subtask is applied to the edited scene from the previous one. We adaptively assign subtasks according to the estimated FOS size, so that each subtask has comparable FOS sizes and difficulty levels and can thus be solved relatively easily with high quality and efficiency.

Guided by the subtask scheduler, we progressively iterate on the subtasks to apply editing. Though their FOS size and difficulty are controlled, it still remains non-trivial to make the scene representation converge in precise geometry. Failing to achieve this will accumulate errors across subtasks, leading to unreasonable geometry in the final results. To this end, we choose 3D Gaussian splatting (3DGS)  as our scene representation for its high training efficiency. We design a novel _adaptive_ Gaussian creation strategy in training to maintain and refine the geometric structure in each subtask, by controlling the size of the splitting and duplication operations. This strategy allows the geometry to be adjusted toward the goal of each subtask, while preventing and removing floc, floating noise, and multi-face structures.

With these key designs, our ProEdit achieves high-quality instruction-guided scene editing in various scenes and editing tasks with precise geometry and detailed textures, as shown in Fig. 1. Notably, ProEdit does not rely on complicated or expensive add-ons, such as specialized distillation losses, additional 3D attention or convolution components, or extended training procedures on the diffusion model. Moreover, as each subtask represents a partial completion of the overall task, our method enables users to _control, preview, and select_ the intermediate stages of editing, which we refer to as "aggressivity" of editing operation during the editing process. This can be simply achieved by taking the edited scene from a subtask either during or after the editing process. Thus, in contrast to previous methods such as classifier-free guidance  and SDEdit , our ProEdit provides a novel way to monitor and manage the editing process. Users can _preview_ different versions of editing with the intermediate outcomes, adjust the subtasks _on the fly_ accordingly to achieve improved final results, and finally _select_ the most satisfactory editing result from all the intermediate ones.

**Our contributions** are three-fold. (1) We offer a novel insight into subtask decomposition and progressive editing, tailored to address the core challenge of large feasible output space in 3D scene editing. (2) We propose a simple yet effective framework, ProEdit, that generates high-quality edited scenes by progressively solving each subtask, without requiring any complicated or expensive add-ons to the diffusion model, while also supporting control, training-time preview, and selection of editing task aggressivity. (3) We consistently achieve high-quality editing results in various scenes and challenging tasks, establishing state-of-the-art performance.

## 2 Related Work

**Learning-Based 3D Scene Representation.** Our framework necessitates a learnable 3D representation to depict the scene being edited. Traditional methods model the 3D geometric structure of a scene with implicit [12; 13; 14] or explicit [15; 16; 17] representations. However, these methods require more information or pre-processing beyond multi-view camera images. In 2020, the neural radiance field (NeRF)  emerges as the first neural network-based scene representation, enabling direct scene reconstruction from multi-view images captured at known camera locations, inspiring numerous follow-up work [18; 19; 20; 21; 22; 23; 24] that explores different aspects including quality, efficiency, and visual effects. Later, 3D Gaussian splatting (3DGS)  becomes a new trend, outperforming NeRF and its variants in rendering quality and efficiency. 3DGS also leads to several follow-up variants, aiming to improve geometry [25; 26] and visual effects , as well as extending to dynamic 3D scenes [28; 29; 30].

**3D Scene Editing.** Various scene editing tasks have been investigated, each aiming to achieve different editing objectives for a given scene across a range of scene representations. These tasks cover different aspects of a scene, including the location, shape, and color of objects [20; 31; 32; 33], physical effects , lighting conditions [27; 35; 36], and the overall appearance [37; 38; 5; 7; 39].

**Instruction-Guided Scene Editing.** Instruction-guided scene editing is a highly free-form yet challenging task, characterized by a straightforward task descriptor - either an editing operation (_e.g._, "Give the person a hat") or a description of the desired scene (_e.g._, "A person wearing a hat"). This task has attracted much attention in the computer vision community. Due to the lack of large-scale 3D datasets to train editing models directly in 3D, current state-of-the-art methods [38; 39; 40; 5; 6; 7; 30; 41] achieve scene editing by distilling knowledge from a pre-trained 2D diffusion model [3; 5] using score distillation sampling (SDS)  and its variants. Instruct-NeRF2NeRF (IN2N)  and its variants [37; 39] apply SDS-equivalent iterative dataset updates to generate edited multi-view images and train the scene representation on them. One direction of follow-up work [6; 7] proposes novel distillation methods to better utilize the 2D editing capability, while another [9; 41] introduces additional components and training procedures to improve the consistency of generation. However, these approaches are unaware of the core challenge posed by large feasible output space (FOS), mitigating it with add-ons that may still fail when the FOS becomes considerably large. In contrast, our ProEdit is tailored for this challenge by proposing subtask decomposition to explicitly control the size of FOS, thereby extending the capability boundary of instruction-guided scene editing.

## 3 ProEdit: Methodology

The key insight of our ProEdit is to decompose a full editing task, described by a text instruction, into a sequence of simpler subtasks with smaller feasible output space (FOS), and apply each of them progressively on the scene. Our framework consists of three major components: (1) an interpolation-based subtask formulation that defines, obtains, and interprets each subtask; (2) a difficulty-aware subtask decomposition scheduler that breaks down the full editing task into several subtasks of comparable difficulty; and (3) an adaptive 3D Gaussian splatting (3DGS)-based  geometry-precise scene editing method that ensures high-quality editing for each subtask, ultimately leading to successful completion of the full task. Our framework is visualized in Fig. 2.

### Interpolation-Based Subtask Formulation

In order to decompose a text-described task into subtasks, we first need to clearly define "task" and "subtasks." We define an editing task \(T(s,e=E(p))\) as an operation that applies a prompt (instruction) \(p\) on the original scene \(s\), where \(e=E(p)\) denotes the text encoding of \(p\) calculated by a frozen text encoder \(E()\) as part of a 2D diffusion model. The notation \(T(s,e)\) represents the editedscene resulting from this task, and we also use \(T(,e)\) to indicate the mapping from the original scene to the edited scene within this context. Additionally, we define \(\) as the empty prompt, indicating that the editing task with this prompt retains the original scene, or \(T(s,E())=s\).

Next, we define subtasks as \(S(s,r)=T(s,e(r))\) with a ratio \(r\), where \(e(r)=r E(p)+(1-r) E()\). This represents a task characterized by an instruction \(p(r)=E^{-1}(r E(p)+(1-r) E())\), whose embedding is a ratio-\(r\) interpolation between \(E(p)\) and \(E()\). Assuming that the neural network \(E()\) is continuous, \(S(s,r)\) will also be continuous w.r.t. \(r\). Therefore, this formulation provides a continuous space of subtasks or intermediate tasks between the original task \(T(,E(p))\) and the identity mapping \(T(,E())\).

### Difficulty-Aware Subtask Scheduler

**Feasible Output Space (FOS) and Task Difficulty.** Inspired by the derivation of SDS , we introduce the concept of _feasible output space_ (FOS) for an editing task \(T(s,E(p))\) as follows: the set of scenes \(s^{}\) such that, when \(s^{}\) is rendered from any view \(v\), the resulting image resembles the edited image (based on instruction \(p\)) from the corresponding view \(v\) of the original scene \(s\), _i.e._, the set of all possible scenes that can be regarded as valid edited result for the given task. A larger FOS indicates greater diversity in how the editing task can be executed; however, this variability can cause multi-view inconsistency, if different views are edited differently. Therefore, an editing task with a larger FOS is inherently more difficult to accomplish.

**Formulation of Subtask Decomposition.** Our goal is to decompose the original editing task \(T(,E(p))\) into a sequence of subtasks, such that applying each subtask progressively or iteratively on the current scene leads to the final editing result. Formally, the decomposition of a task \(T(,E(p))\) is a monotonically increasing sequence \(r_{0},r_{1},,r_{n}\), where \(r_{0}=0,r_{n}=1\). We then define \(s_{i}\) as the edited scene resulting from the \(i\)-th subtask. We have

\[s_{i}=\{s,&(),\\ S(s_{i-1},r_{i}),&(r_{i}), . i=0,\] (1)

In other words, the \(i\)-th subtask is \(S(s_{i-1},r_{i})\), which is applied on the edited scene \(s_{i-1}\) from the previous \((i-1)\)-th subtask. The outcome of the \(i\)-th subtask is \(s_{i}\).

**Subtask Difficulty Measurement and Approximation.** The difficulty of each subtask \(S(s_{i-1},r_{i})\) is measured as being proportional to the size of FOS (a continuous space), which is difficult to compute or even rigorously define. Therefore, we approximate this difficulty by evaluating the difference between the original and edited images of the 2D diffusion model. Intuitively, an editing task that brings a significant change typically has more degrees of freedom, leading to a larger FOS. Additionally, each subtask \(r_{i}\) is applied on the scene \(s_{i-1}\), which cannot be determined until all prior subtasks \(r_{1},r_{2},,r_{i-1}\) are completed. So, we make another approximation based on the assumption that the image of a view in \(s_{i}\) will closely resemble the corresponding view of \(s\) edited by the 2D diffusion model following the instruction of the \(i\)-th subtask. In other words,

\[v_{k}(s_{i}) T_{}(v_{k}(s),e(r_{i})), k V,\] (2)

Figure 2: **Our ProEdit framework** features three major designs: an interpolation-based subtask formulation (Sec. 3.1), a difficulty-aware subtask scheduler for subtask decomposition (Sec. 3.2), and an adaptive 3DGS tailored for progressive scene editing through a dual-GPU pipeline (Sec. 3.3). For an editing task, we first decompose it into interpolation-based subtasks to schedule the editing process with the subtask scheduler, and then progressively perform the subtasks with adaptive 3DGS.

where \(v_{k}(s)\) is the rendered image at the \(k\)-th view of scene \(s\), and \(T_{}(v,e)\) is the output of a 2D editing task applied on image \(v\) with instruction embedding \(e\), generated by the 2D diffusion model. By applying such an approximation to both subtasks and using Learned Perceptual Image Patch Similarity (LPIPS) to measure the perceptual difference between images, we can then define the difficulty metric as

\[(r_{i},r_{j})}}{{=}}_{k V} L_{}(v_{k}(s_{i}),v_{k}(s_{j}))_{k V}L_{}(T_{}(v_{k}(s),e(r_{i})),T_{}(v_{k}(s),e(r_{j}))).\] (3)

Observing that \((r_{i},r_{j})\)'s approximation is only related to the rendered image \(v_{k}(s)\) of the original scene \(s\) and is independent of that of the edited scene (namely, \(v_{k}(s_{i})\)), we can then allow \((r_{a},r_{b})\) to take any two arbitrary subtasks \(r_{a}\) and \(r_{b}\). Our goal is to find the subtask decomposition \(r_{0},,r_{n}\) with similar \(\{(r_{i-1},r_{i})\}\) for each \(i\).

**Difficulty-Aware Adaptive Subtask Decomposition.** The approximation of \((r_{i},r_{j})\) disentangles its computation from the edited scene of task \(T(,e(r_{i}))\), by substituting it with \(T_{}(,e(r_{i}))\). This enables us to decompose the subtasks from a more _global_ perspective. Therefore, we propose an adaptive method to obtain the set of subtask ratios \(R=r_{0},,r_{n}\). The algorithm operates recursively over an interval \([r_{a},r_{b}]\) with a difficulty threshold \(_{}\), starting with the interval \(\). In each recursion, the algorithm first includes both \(r_{a}\) and \(r_{b}\) in the set \(R\), and stops the recursion if \((r_{a},r_{b})_{}\). Otherwise, it selects the middle point \(r_{m}=(r_{a}+r_{b})/2\), and recurses on the intervals \([r_{a},r_{m}]\) and \([r_{m},r_{b}]\). Once the recursion is complete, we obtain the sequence of subtasks \(r_{0},,r_{n}\) by sorting the set \(R\), ensuring that \((r_{i-1},r_{i})_{}\) for all subtasks.

To simplify the subtask decomposition, we check if there exists a subtask \(r_{i}\) such that \((r_{i-1},r_{i+1})_{}\). If so, we could safely remove the subtask \(r_{i}\) while still maintaining \((r_{i-1},r_{i+1})_{}\). This iterative check continues until no further subtasks can be pruned.

Notably, an interpolated subtask can be regarded as a partial completion of the editing instruction. For example, the instruction "Make him smile" with an interpolation ratio of \(r=0.5\) can be interpreted as "Make him half-smile," indicating a lower _aggressivity_ of the editing operation. In this context, high aggressivity indicates more significant changes towards the editing operation, whereas low aggressivity reflects greater similarity between the edited scene and the original one. Therefore, our subtask decomposition not only lays the foundation for our editing process but also categorizes task aggressivity, where each subtask corresponds to a specific level of aggressivity. Consequently, beyond performing editing, our ProEdit enables users to control, preview, and select the aggressivity of the editing operation _during or after the editing process_, by utilizing the edited scene of a subtask throughout the progressive editing workflow. Such a capability is absent in previous work.

**Subtask Scheduling.** The subtask scheduler also determines when the current subtask is complete, allowing us to proceed to the next one. Designing an image-based criterion to assess whether the images in the current subtask have been sufficiently edited is challenging. Therefore, we propose a criterion based on the scene representation training procedure. Specifically, when the running mean of the training loss no longer decreases over a specified number of iterations, we regard the scene representation to be converging to the edited scene, indicating that the current editing subtask is complete. Moreover, apart from the subtasks \(r_{0},r_{1},,r_{n}\), we prepend an additional subtask \(r_{0}\) to refine the initial scene representation using diffusion-reconstructed original images, and append another subtask \(r_{n}\) to consolidate the editing results, as detailed in Appendix B.

### Adaptive 3DGS Tailored for Progression

We choose 3DGS  as our scene representation for its high efficiency and rendering quality. However, 3DGS is primarily designed for reconstruction from multi-view consistent images. Directly training on edited images with 3DGS results in a continuously increasing number of Gaussians that overfit the inconsistent views, ending up with an out-of-memory error. Therefore, we propose a novel Adaptive 3DGS specifically tailored for progressive scene editing.

**Basic Workflow for Each Subtask.** As each subtask has a reduced FOS and lower difficulty, we can use a straightforward approach to perform the subtask editing. Consistent with Instruct-NeRF2NeRF (IN2N) , we apply a simple iterative dataset update (Iterative DU) that iteratively generates edited views using the diffusion model and employs them to train the scene representation. Unlike NeRFs , our 3DGS-based scene representation accepts full images as supervision instead of rays, allowingus to directly train on the edited images without the need to replace rays. This enables a simpler yet more effective workflow.

**Adaptive Gaussian Creation Strategy.** While the decomposition of subtasks controls the size of FOS and reduces potential inconsistencies, making 3DGS converge on the edited multi-view images remains challenging. Designed only for reconstruction from multi-view consistent images, 3DGS is not robust enough to deal with all inconsistencies. This can lead to overfitting on the inconsistent edited images with view-dependent colors, floating or floc noises, and multi-face structures.

Therefore, we propose an adaptive Gaussian creation strategy to refine the geometry of 3DGS, enabling it to converge on the edited images with reasonable and potentially high-quality geometric structures. As introduced in , the original 3DGS maintains Gaussian-represented geometry by periodically culling unnecessary Gaussians based on an opacity threshold, and by creating new Guassians (through splitting or duplicating) to expand model capability according to a training gradient threshold. Our strategy builds on this geometry maintenance schedule by _adaptively_ controlling both thresholds. (1) At the beginning of each subtask, we set the opacity of all Gaussians to the threshold and perform several iterations of training without geometry maintenance. This training procedure implicitly identifies the Gaussians that correctly lie on the object surface by making them learn higher opacity, which allows them to be preserved in the scene representation. Conversely, Gaussians with incorrect geometry learn lower opacity and are subsequently culled during the next maintenance phase. (2) To prevent the training process from creating too many noisy Gaussians in a single iteration when operating with edited images, we also control the gradient threshold for Gaussian creation to achieve a smooth increase in the number of Gaussians. We schedule the number of created Gaussians based on the existing Gaussians in the scene and the number previously culled, selecting the threshold according to this scheduled number, as detailed in Appendix D. With these strategies, our 3DGS is able to converge to the edited scenes with clear texture and reasonable, even precise geometry.

**Dual-GPU Training to Decouple Diffusion and 3DGS.** Given the significant difference in iteration speeds - around 2 seconds per generation for the diffusion model inference and less than 0.02 seconds per iteration for the 3DGS training procedure - it is challenging to achieve an effective trade-off on a single GPU during Iterative DU. Inspired by [9; 43], we employ a dual-GPU training schedule to decouple them. The first GPU iteratively generates newly edited images using the diffusion model and stores them in a buffer as the updated dataset. Meanwhile, the second GPU iteratively trains 3DGS with the edited images in the buffer and raises a signal to indicate when the current subtask is complete. This approach enables a highly efficient training procedure within our ProEdit framework.

## 4 Experiments

### Experimental Settings

**Scene Representation and Diffusion Model.** As mentioned in Sec. 3.3, our ProEdit leverages 3DGS-based scene representation for high quality and efficiency. We use the Splatfacto model from the NeRFStudio  library as our backbone. For the diffusion model, consistent with previous work [5; 6; 37; 45], we use a pre-trained Instruct-Pix2Pix (IP2P)  model from HuggingFace.

**Scenes and Editing Instructions.** According to Sec. 3.1, each editing task \(T(s,E(p))\) is characterized by a scene \(s\) and an instruction \(p\), and the desired output is the edited scene. We evaluate our ProEdit on the following scene datasets: (1) The IN2N dataset introduced by Instruct-NeRF2NeRF (IN2N) , which is available for free use and is the most widely used dataset in prior work. (2) The ScanNet++ dataset of indoor scenes, released under the ScanNet++ Terms of Use, which is introduced for instruction-guided scene editing in . We use instructions either from previous methods for comparisons or from tasks that require highly noticeable geometric changes in the scene - one of the most challenging editing tasks that previous methods have struggled to perform well.

**Subtask Scheduling.** We determine the number of subtasks to balance editing quality, controllability, and efficiency. For texture-focused instructions (_e.g_., style transfer), we decompose each task into approximately 4 subtasks using an appropriate threshold \(}\); for geometry-related instructions with much higher FOS, we break each task down into around 8 subtasks with a proper \(}\).

**Baselines.** We compare our ProEdit with recent state-of-the-art instruction-guided scene editing methods, including Instruct-NeRF2NeRF (IN2N)  (along with its 3DGS-based implementation ), ViCA-NeRF , ConsistDreamer , CSD , PDS , Efficient-NeRF2NeRF (EN2N) , DreamEditor , _etc_. As different methods use different tasks for visualization in their papers, and some do not provide publicly available code or pre-trained models, our primary comparisons focus on common editing tasks, leveraging the visualizations presented in their papers. Also, we include comparisons for some additional tasks with results generated from available code or re-implementations. As our ProEdit specifically targets the instruction-guided scene editing task, we do not include comparisons with methods designed for other scene editing or generation tasks.

**Implementation Details.** We follow the default hyperparameter settings of the Splatfacto method, and set the classifier-free guidance (CFG)  as \(7.5 1.5\) for all instructions in the diffusion model. During the editing process for each subtask, consistent with IN2N , we use SDEdit's  method to control similarity with denoising timesteps between 450 and 850. We also apply HiFA's  annealing strategy to gradually decrease denoising timesteps in this process. Utilizing a dual-GPU training workflow (Sec. 3.3), the editing tasks are conducted on two NVIDIA A6000 or A100 GPUs, with each subtask taking 10 to 20 minutes to complete depending on its difficulty and convergence.

**Metrics.** We present the quantitative assessment under the following metrics: User Study of Overall Quality (USO), User Study of 3D Consistency (US3D), GPT Evaluation Score (GPT), CLIP 

Figure 3: **In the comparative experiments on the Fangzhou and Face scenes**, our ProEdit achieves high-quality editing, with strong instruction fidelity, clear textures, and precise shapes across both levels of aggressivity controlled by subtask scheduling. The “medium aggressivity” editing results are obtained from an intermediate subtask. The editing results of the baselines are sourced from visualizations in their respective papers.

Text-Image Direction Similarity (CTIDS), and CLIP Direction Consistency (CDC). The user study was conducted with 26 participants. The GPT score is detailed in Appendix E. The CLIP-based scores are consistent with those reported in IN2N .

### Experimental Results and Analysis

**Qualitative Results.** Fig. 3 shows the comparisons in the Fangzhou scene and the IN2N's Face scene. Our ProEdit demonstrates results on two levels of editing aggressivity: high aggressivity results are obtained by executing all subtasks, while medium aggressivity results are derived from completing only the first 40% subtasks. Overall, our ProEdit produces high-quality editing results characterized by clear textures, bright colors, reasonable and precise geometry, and high instruction fidelity. Compared with the baselines, our ProEdit shows enhanced geometry editing capabilities, particularly in the "Tolkien Elf" editing which features a thinner face, and the "Lord Voldemort" editing which incorporates more wrinkles in the Fangzhou scene. By contrast, the baselines tend to maintain geometry more similar to the original scene. Notably, for the editing task "Give him a plaid jacket," our ProEdit generates much clearer and more noticeable plaid patterns than all baselines.

The experimental results on the ScanNet++ dataset are shown in Fig. 4. With subtask decomposition and progressive editing, our ProEdit achieves high-quality results that are comparable to and even outperform the baseline ConsistDreamer , which incorporates three complicated add-ons for ensuring 3D consistency. This shows that our simple progression is more effective in reducing inconsistency - through reducing the size of FOS - than explicit 3D consistency-enforcing components.

Figure 4: **In the comparative experiments on the ScanNet++ scenes, our simple ProEdit also achieves high-quality editing that is comparable to, and in some cases even outperforms, the sophisticated baseline ConsistDreamer . All visualizations are sourced from ConsistDreamer’s paper.**

Figure 5: **In the comparative experiments across various outdoor scenes, our ProEdit not only achieves high-quality editing that surpasses the baselines, but also enables aggressivity controls for a range of scenes and tasks.**

[MISSING_PAGE_FAIL:9]

**Ablation Study.** To validate the necessity of our subtask decomposition, we conduct experiments on a variant of ProEdit using only one subtask (\(n=1\), \(r_{0}=0\), \(r_{1}=1\)), effectively disabling decomposition (referred to as "ND"). Qualitative results are shown in Fig. 6. Without subtask decomposition, the variant generates unrealistically long cheeks to accommodate inconsistencies in cheek decorations across views, resulting in blurred cheek textures in the rendered output due to the large FOS of the editing task. In contrast, our full ProEdit achieves bright, clear results with precise and realistic geometry. Quantitative results are shown in Table 2. For this comparison, we conducted a new user study involving 41 participants, including an additional User Study of Shape Plausibility ("USP") metric: we provide participants with the modeled depth maps, similar to those in Fig. 6, along with the rendered RGB images. We then ask them to evaluate whether the shapes are realistic and match the rendered images. The "ND" variant performs significantly worse than our full method on all user study metrics, further underscoring the effectiveness of our subtask decomposition. These results collectively demonstrate that reducing FOS through subtask decomposition is crucial to our high-quality results.

## 5 Discussion

**3D Consistency Add-Ons.** Different from our subtask decomposition strategy, 3D consistency add-ons, such as distillation losses, consistency-inducing components, and specific training procedures, offer an alternative way to control and reduce FOS. Although our framework achieves high-quality editing without them, combining it with these 3D consistency add-ons can leverage the strengths of both approaches, potentially reducing the number of required subtasks and enhancing editing quality.

**Limitations.** Our ProEdit is a distillation-guided framework from 2D diffusion, similar to all baselines. Therefore, its editing capability is constrained by the underlying diffusion model. If the diffusion model does not support applying a specific editing instruction on most views of a scene, our ProEdit will also be unable to do so. Additionally, ProEdit relies on 3DGS for efficient training, which NeRF-based representations do not support; consequently, it inherits certain limitations of 3DGS, including limited suitability for unbounded outdoor scenes. Finally, ProEdit may still encounter the multi-face or Janus problems, as the 2D diffusion model lacks 3D awareness.

**Future Directions.** There are many promising directions to explore in subtask decomposition beyond the interpolation-based strategy introduced in this paper. One potential way is to explicitly construct intermediate subtasks using semantic guidance. For example, applying "Turn him into a bald person" before "Make him wear a hat" could lead to a more free-form hat independent of the hair, with such intermediate instructions generated by large language models. Another alternative avenue involves leveraging video generation models to "animate" the transition from the original scene to the edited scene, treating this animation process as a series of subtasks. Doing so will enable ProEdit to function as a 3D scene animator, generating high-quality 4D (dynamic 3D) scenes. Additionally, the progressive framework of ProEdit can be potentially applied to scene generation.

**Potential Societal Impacts.** The positive societal impacts of our ProEdit include (1) the development of consumer-grade 3D scene editing products and applications, facilitated by advancements in 3D structured-light scanners for mobile phones and virtual reality (VR) and augmented reality (AR); and (2) the transformation of high-quality 3D and 4D (dynamic 3D) scene creation through the editing of existing high-resolution scenes. On the other hand, as our framework is based on generative models, it is crucial to address potential ethical and safety concerns, including risks of producing biased results and the possibility of misuse for illegal activities.

## 6 Conclusion

This paper proposes ProEdit, a novel 3D scene editing framework that decomposes the editing task into subtasks and performs them progressively. Our method targets the fundamental cause of inconsistency - the large feasible output space of the diffusion model with respect to an editing task. Extensive experiments show that our ProEdit produces high-quality editing results characterized by bright colors, sharp and detailed textures, and precise geometric structures across various scenes and editing tasks. Our method further enables a novel controllability over the aggressivity of the editing task, by allowing users to select which subtasks to execute. We hope that our ProEdit will inspire exciting applications and new research directions in 3D scene editing and generation.