# MADiff: Offline Multi-agent Learning

with Diffusion Models

 Zhengbang Zhu\({}^{1}\) Minghuan Liu\({}^{1}\) Liyuan Mao\({}^{1}\) Bingyi Kang\({}^{2}\) Minkai Xu\({}^{3}\)

**Yong Yu\({}^{1}\) Stefano Ermon\({}^{3}\) Weinan Zhang\({}^{1}\)\({}^{}\)**

\({}^{1}\) Shanghai Jiao Tong University, \({}^{2}\) ByteDance, \({}^{3}\) Stanford University

{zhengbangzhu, minghuanliu, maoliyuan, yyu, wnzhang}@sjtu.edu.cn,

bingykang@gmail.com, {minkai, ermon}@cs.stanford.edu

Corresponding author.

###### Abstract

Offline reinforcement learning (RL) aims to learn policies from pre-existing datasets without further interactions, making it a challenging task. Q-learning algorithms struggle with extrapolation errors in offline settings, while supervised learning methods are constrained by model expressiveness. Recently, diffusion models (DMs) have shown promise in overcoming these limitations in single-agent learning, but their application in multi-agent scenarios remains unclear. Generating trajectories for each agent with independent DMs may impede coordination, while concatenating all agents' information can lead to low sample efficiency. Accordingly, we propose MADiff, which is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple agents. To our knowledge, MADiff is the first diffusion-based multi-agent learning framework, functioning as both a decentralized policy and a centralized controller. During decentralized executions, MADiff simultaneously performs teammate modeling, and the centralized controller can also be applied in multi-agent trajectory predictions. Our experiments demonstrate that MADiff outperforms baseline algorithms across various multi-agent learning tasks, highlighting its effectiveness in modeling complex multi-agent interactions.

## 1 Introduction

Offline reinforcement learning (RL)  learns exclusively from static datasets without online interactions, enabling the effective use of pre-collected large-scale data. However, applying temporal difference (TD) learning in offline settings causes extrapolation errors , where target value functions are evaluated on out-of-distribution actions. Sequence modeling algorithms bypass TD-learning by directly fitting the dataset distribution . Nevertheless, these methods are limited by the model's expressiveness, making it difficult to handle diverse datasets. They also suffer from compounding errors  due to autoregressive generation. Recently, diffusion models (DMs) have achieved remarkable success in various generative modeling tasks , owing to their exceptional abilities at capturing complex, high-dimensional data distributions. Their successes have also been introduced into offline RL, offering a superior modeling choice for sequence modeling algorithms .

Compared to single-agent learning, offline multi-agent learning (MAL) has been less studied and is more challenging. Since the behaviors of all agents are interrelated, each agent is required to model interactions and coordination among agents, while making decisions in a decentralized manner to achieve the goal. Current MAL approaches typically train a centralized value function to updateindividual agents' policies (Rashid et al., 2020) or use an autoregressive transformer to determine each agent's actions (Meng et al., 2021; Wen et al., 2022). However, without online interactions, an incorrect centralized value can lead to significant extrapolation errors, and the transformer can only serve as an independent model for each agent.

In this paper, we aim to study the potential of employing DMs to solve the above challenges in offline MAL problems. Merely adopting existing diffusion RL methods by using independent DMs to model each agent can result in serious inconsistencies due to a lack of proper credit assignment among agents. Another possible solution is to concatenate all agents' information as the input and output of the DM. However, treating the agents as a single unified agent neglects the important nature of multi-agent systems. One agent may have strong correlations with only a few other agents, which makes a full feature interaction redundant. In many multi-agent systems, agents exhibit certain symmetry and can share model parameters for efficient learning (Arel et al., 2010). However, concatenating them in a fixed order breaks this symmetry, forcing the model to treat each agent differently.

To address the aforementioned coordination challenges, we propose the first centralized-training-decentralized-execution (CTDE) diffusion framework for MA problems, named MADiff. MADiff adopts a novel attention-based DM to learn a return-conditional trajectory generation model on a reward-labeled multi-agent interaction dataset. In particular, the designed attention is computed in several latent layers of the model of each agent to fully interchange the information and integrate the global information of all agents. To model the coordination among agents, MADiff applies the attention mechanism on latent embedding for information interaction across agents. The attention mechanism enables the dynamic modeling of agent interactions through learned weights, while also enabling the use of a shared backbone to model each agent's trajectory, significantly reducing the number of parameters. During training, MADiff performs centralized training on the joint trajectory distributions of all agents from offline datasets, including different levels of expected returns. During inference, MADiff adopts classifier-free guidance with low-temperature sampling to generate behaviors given the conditioned high expected returns, allowing for decentralized execution by predicting the behavior of other agents and generating its own behavior. Therefore, MADiff can be regarded as a principled offline MAL solution that not only serves as a decentralized policy for each agent or a centralized controller for all agents, but also includes teammate modeling without additional cost. Comprehensive experiments demonstrated superior performances of MADiff on various multi-agent learning tasks, including offline MARL and trajectory prediction.

In summary, our contributions are (1) the first diffusion-based multi-agent learning framework that unifies decentralized policy, centralized controller, teammate modeling, and trajectory prediction; (2) a novel attention-based DM structure that is designed explicitly for MAL and enables coordination among agents in each denoising step; (3) achieving superior performances for various offline multi-agent problems.

## 2 Preliminaries

### Multi-agent Offline Reinforcement Learning

We consider a partially observable and fully cooperative multi-agent learning (MAL) problem, where agents with local observations cooperate to finish the task. Formally, it is defined as a DecPOMDP (Oliehoek and Amato, 2016): \(G=,,P,r,,O,N,U,\), where \(\) and \(\) denote state and action space separately, and \(\) is the discounted factor. The system includes \(N\) agents \(\{1,2,,N\}\) act in discrete time steps, and starts with an initial global state \(s_{0}\) sampled from the distribution \(U\). At each time step \(t\), every agent \(i\) only observes a local observation \(o^{i}\) produced by the function \(O(s,a):\) and decides \(a\), which forms the joint action \(^{N}\), leading the system transits to the next state \(s^{}\) according to the dynamics function \(P(s^{}|s,):\). Normally, agents receive a shared reward \(r(s,)\) at each step, and the optimization objective is to learn a policy \(^{i}\) for each agent that maximizes the discounted cumulative reward \(_{s_{t},_{t}}[_{t}^{t}r(s_{t},_{t})]\). In offline settings, instead of collecting online data in environments, we only have access to a static dataset \(\) to learn the policies. The dataset \(\) is generally composed of trajectories \(\), _i.e._, observation-action sequences \([_{0},_{0},_{1},_{1}, ,_{T},_{T}]\) or observation sequences \([_{0},_{1},,_{T}]\). We use bold symbols to denote the joint vectors of all agents.

### Diffusion Probabilistic Models

Diffusion models (DMs) (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Ho et al., 2020), as a powerful class of generative models, implement the data generation process as reversing a forward noising process (denoising process). For each data point \(x_{0} p_{}(x)\) from the dataset \(\), the noising process is a discrete Markov chain \(x_{0:K}\) such that \(p(x_{k}|x_{k-1})=(x_{k}|}x_{k-1},(1-_{k})I)\), where \((,)\) denotes a Gaussian distribution with mean \(\) and variance \(\), and \(_{0:K}\) are hyperparameters which control the variance schedule. The variational reverse Markov chain is parameterized with \(q_{}(x_{k-1}|x_{k})=(x_{k-1}|_{}(x_{k},k),(1-_ {k})I)\). The data sampling process begins by sampling an initial noise \(x_{K}(0,I)\), and follows the reverse process until \(x_{0}\). The reverse process can be estimated by optimizing a simplified surrogate loss as in Ho et al. (2020):

\[()=_{k[1,K],x_{0} q,(0,I)}[\|-_{}(x_{k},k)\|^{2 }]\.\] (1)

The estimated Gaussian mean can be written as \(_{}(x_{k},k)=}}(x_{k}-}{_{k}}}_{}(x_{k},k))\), where \(_{k}=_{s=1}^{k}_{s}\).

### Diffusing Decision Making

Diffusing over state trajectories and acting with inverse dynamics model.Among existing works in single-agent learning, Janner et al. (2022) chose to diffuse over state-action sequences, so that the generated actions for the current step can be directly used for executing. Another choice is diffusing over state trajectories only (Ajay et al., 2023), which is claimed to be easier to model and can obtain better performance due to the less smooth nature of action sequences:

\[:=[s_{t},_{t+1},,_{t+H-1}],\] (2)

where \(t\) is the sampled time step and \(H\) denotes the trajectory length (horizon) modeled by DMs. But the generated state sequences can not provide actions to be executed during online evaluation. Therefore, an inverse dynamics model is trained to predict the action \(_{t}\) that makes the state transit from \(s_{t}\) to the generated next state \(_{t+1}\):

\[_{t}=I_{}(s_{t},_{t+1})\.\] (3)

Therefore, at every environment step \(t\), the agent first plans the state trajectories using an offline-trained DM, and infers the action with the inverse dynamics model.

Classifier-free guided generation.For targeted behavior synthesis, DMs should be able to generate future trajectories by conditioning the diffusion process on an observed state \(s_{t}\) and information \(y\). We use classifier-free guidance (Ho and Salimans, 2022) which requires taking \(y()\) as additional inputs for the diffusion model. Formally, the sampling procedure starts with Gaussian noise \(_{K}(0, I)\), and diffuse \(_{k}\) into \(_{k-1}\) at each diffusion step \(k\). Here \([0,1)\) is the scaling factor used in

Figure 1: The architecture of MADiff, which is an attention-based diffusion network framework that performs attention across all agents at every decoder layer of each agent.

low-temperature sampling to scale down the variance of initial samples (Ajay et al., 2023). We use \(_{k,t}\) to denote the denoised state \(s_{t}\) at \(k\)'s diffusion step. \(_{k}\) denotes the denoised trajectory at \(k\)'s diffusion step for a single agent: \(_{k}:=[s_{t},_{k,t+1},,_{k,t+H-1}]\). Note that for sampling during evaluations, the first state of the trajectory is always set to the current observed state at all diffusion steps for conditioning, and every diffusion step proceeds with the perturbed noise:

\[:=_{}(_{k},,k)+( _{}(_{k},y(),k)-_{}(_{k},,k))\;,\] (4)

where \(\) is a scalar for extracting the distinct portions of data with characteristic \(y()\). By iterative diffusing the noisy samples, we can obtain a clean state trajectory: \(_{0}():=[s_{t},_{t+1},,_{t+H-1}]\;\).

## 3 Methodology

We formulate the problem of MAL as conditional generative modeling:

\[_{}_{}[ p_{}(| ())]\;,\] (5)

where \(p_{}\) is learned for estimating the conditional data distribution of joint trajectory \(\), given information \(()\), such as observations, rewards, and constraints. When all agents are managed by a centralized controller, _i.e._, the decisions of all agents are made jointly, we can learn the generative model by conditioning the global information aggregated from all agents \(()\); otherwise, if we consider each agent \(i\) separately and require each agent to make decisions in a decentralized manner, we can only utilize the local information \(y^{i}(^{i})\) of each agent \(i\), including the private information and the common information shared by all (_e.g._, team rewards).

### Multi-Agent Diffusion with Attention

In order to handle MAL problems, agents must learn to coordinate. To solve the challenge of modeling the complex inter-agent coordination in the dataset, we propose a novel attention-based diffusion architecture designed to interchange information among agents.

In Figure 1, we illustrate the architecture of MADiff model. In detail, we adopt U-Net as the base structure for modeling agents' individual trajectories, which consists of repeated one-dimensional convolutional residual blocks. The convolution is performed over the time step dimension, and the observation feature dimension is treated as the channel dimension. To encourage information interchange and improve coordination ability, a critical change is made by adopting attention (Vaswani et al., 2017) layers before all decoder blocks in the U-Nets of all agents. Since embedding vectors from different agents are aggregated by the attention operation rather than concatenations, MADiff is index-free such that the input order of agents can be arbitrary and does not affect the results.

Formally, the input to \(l\)-th decoder layer in the U-Net of each agent \(i\) is composed of two components: the skip-connected feature \(c^{i}_{l}\) from the symmetric \(l\)-th encoder layer and the embedding \(e^{i}_{l}\) from the previous decoder layer. The computation of attention in MADiff is conducted on \(c^{i}_{l}\) rather than \(e^{i}_{l}\) since in the U-Net structure the encoder layers are supposed to extract informative features from the input data. We use \({c^{}}^{i}_{l}\) to denote the skip-connected feature after attention operations which aggregate information across agents. We adopt the multi-head attention mechanism to fuse the encoded feature \({c^{}}^{i}_{l}\) with other agents' information, which is important in effective multi-agent coordination.

### Centralized Training Objectives

Given a multi-agent offline dataset \(\), we train MADiff which is parameterized through the unified noise model \(_{}\) for all agents and the inverse dynamics model \(I^{i}_{}\) of each agent \(i\) with the reverse diffusion loss and the inverse dynamics loss:

\[(,&):=_{i} _{(o^{i},a^{i},o^{ i})}[\|a^{i}-I^{i}_{}(o^{i },o^{ i})\|^{2}]\\ &+_{k,_{0},}[\|- _{}(}_{k},(1-)(_{0})+ ,k)\|^{2}]\;,\] (6)

where \(\) is sampled from a Bernoulli distribution to balance the training effort on unconditioned and conditioned models. For training the DM, we sample noise \((,)\) and a time step \(k\{1,,K\}\), construct a noise corrupted joint state sequence \(_{k}\) from \(\) and predict the noise \(_{}:=_{}(}_{k},(_ {0}),k)\). Note that the noisy array \(}_{k}\) is applied with the same condition required by the sampling process, as we will discuss in Section 3.3 in detail. As for the inverse dynamics training, we sample the observation transitions of each agent to predict the action.

It is worth noting that the choice of whether agents should share their parameters of \(_{}^{i}\) and \(I_{^{i}}\) depends on the homogeneous nature and requirements of tasks. If agents choose to share their parameters, only one shared DM and inverse dynamics model are used for generating all agents' trajectories; otherwise, each agent \(i\) has extra parameters (_i.e._, the U-Net and inverse dynamic models) to generate their states and predict their actions. The attention modules are always shared to incorporate global information into generating each agent's trajectory.

### Centralized Control or Decentralized Execution

**Centralized control.** A direct and straightforward way to utilize MADiff in online decision-making tasks is to have a centralized controller for all agents. The centralized controller has access to all agents' current local observations and generates all agents' trajectories along with predicting their actions, which are sent to every single agent for acting in the environment. This is applicable for multi-agent trajectory prediction problems and when interactive agents are permitted to be centralized controlled, such as in team games. During the generation process, we sample an initial noise trajectory \(}_{K}\), condition the current joint states of all agents and the global information to utilize \((_{0})\); following the diffusion step described in Equation (4) with \(_{}\), we finally sample the joint observation sequence \(}_{0}\) as below:

\[_{t},,}_{K,t+H-1}]}_{}_{K}} [}_{0}]{K}_{t},,}_{t+H-1}]}_{}_{0}}\,\] (7)

where every \(}_{K,t}(,)\) is a noise vector sampled from the normal Gaussian. After generation, each agent obtains the action through its own inverse dynamics model following Equation (3) using the current observation \(o_{t}^{i}\) and the predicted next observation \(_{t+1}^{i}\), and takes a step in the environment. We highlight that MADiff provides an efficient way to generate joint actions and the attention module guarantees sufficient feature interactions and information interchange among agents.

**Decentralized execution with teammate modeling.** Compared with centralized control, a more popular and widely-adopted setting is that each agent only makes its own decision without any communication with other agents, which is what most current works (Lowe et al., 2017; Rashid et al., 2020; Wang et al., 2023) dealt with. In this case, we can only utilize the current local observation of each agent \(i\) to plan its own trajectory. To this end, the initial noisy trajectory is conditioned on the current observation of the agent \(i\). Similar to the centralized case, by iterative diffusion steps, we finally sample the joint state sequence based on the local observation of agent \(i\) as:

\[_{K,t}^{0},,_{K,t+H-1}^{0} \\ o_{t}^{i},,_{K,t+H-1}^{i}\\ \\ _{K,t}^{N},,_{K,t+H-1}^{N}}_{}_{K}^{+}}}_{}_{K}^{+}}\ [}_{0}]{K} _{t}^{0},,_{t+H-1}^{0}\\ ,\\ o_{t}^{i},,_{t+H-1}^{i}\\ ,\\ _{t}^{N},,_{t+H-1}^{N}}_{}_{0}^ {+}}}_{}_{0}^{+}}\,\] (8)

and we can also obtain the action through the agent \(i\)'s inverse dynamics model as mentioned above. An important observation is that, the decentralized execution of MADiff includes teammate modeling such that the agent \(i\) infers all others' observation sequences based on its own local observation. We show in experiments that this achieves great performances in various tasks, indicating the effectiveness of teammate modeling and the great ability in coordination.

**History-based generation.** We find DMs are good at modeling the long-term joint distributions, and as a result MADiff perform better in some cases when we choose to condition on the trajectory of the past history instead of only the current observation. This implies that we replace the joint observation \(_{t}\) in Equation (7) as the \(C\)-length joint history sequence \(_{t}:=[_{t-C},,_{t-1},_{t}]\), and replace the independent observation \(o_{t}^{i}\) in Equation (8) as the history sequence \(h_{t}^{i}:=[o_{t-C}^{i},,o_{t-1}^{i},o_{t}^{i}]\) of each agent \(i\). Appendix Section D illustrates how agents' history and future trajectories are modeled by MADiff in both centralized control and decentralized execution.

Related Work

**Multi-agent Offline RL.** While offline RL has become an active research topic, only a limited number of works studied offline MARL due to the challenge of offline coordination. Jiang and Lu (2021) extended BCQ (Fujimoto et al., 2019), a single-agent offline RL algorithm with policy regularization to multi-agent; Yang et al. (2021) developed an implicit constraint approach for offline Q learning, which was found to perform particularly well in MAL tasks; Pan et al. (2022) argued the actor update tends to be trapped in local minima when the number of agents increases, and correspondingly proposed an actor regularization method named OMAR. All of these Q-learning-based methods naturally have extrapolation error problem (Fujimoto et al., 2019) in offline settings, and their solution cannot get rid of it but only mitigate some. As an alternative, MADT (Meng et al., 2021) formulated offline MARL as return-conditioned supervised learning, and use a similar structure to a previous transformer-based offline RL work (Chen et al., 2021). However, offline MADT learns an independent model for each agent without modeling agent interactions; it relies on the gradient from centralized critics during online fine-tuning to integrate global information into each agent's decentralized policy. MADiff not only avoids the problem of extrapolation error, but also achieves the modeling of collaborative information while allowing CTDE in a completely offline training manner.

**Diffusion Models for Decision-Making.** There is a recent line of work applying diffusion models (DMs) to decision-making problems such as RL and imitation learning. Janner et al. (2022) design a diffusion-based trajectory generation model and train a value function to sample high-reward trajectories. A consequent work (Ajay et al., 2023) takes conditions as inputs to the DM, thus bringing more flexibility that generates behaviors that satisfy combinations of diverse conditions. Another line of work (Wang et al., 2022; Hansen-Estruch et al., 2023; Kang et al., 2024) uses the DM as a form of policy, _i.e._, generating actions conditioned on states, and the training objective behaves as a regularization under the framework of TD-based offline RL algorithms. Different from the above, SynthER (Lu et al., 2024) adopts the DM to upsample the rollout data to facilitate learning of any RL algorithms. All of these existing methods focus on solving single-agent tasks. The proposed MADiff is structurally similar to Ajay et al. (2023), but includes effective modules to model agent coordination in MAL tasks.

**Opponent Modeling in MARL.** Our modeling of teammates can be placed under the larger framework of opponent modeling, which refers to the process by which an agent tries to infer the behaviors or intentions of other agents using its local information. There is a rich literature on utilizing opponent modeling in online MARL. Rabinowitz et al. (2018) used meta-learning to build three models, and can adapt to new agents after observing their behavior. SOM (Raileanu et al., 2018) uses the agent's own goal-conditioned policy to infer other agents' goals from a maximum likelihood perspective. LIAM (Papoudakis et al., 2021) extracts representations of other agents with variational auto-encoders conditioned on the controlled agent's local observations. Considering the impact of the ego agent's policy on other agents' policies, LOLA (Foerster et al., 2017) and following works (Willii et al., 2022; Zhao et al., 2022) instead model the parameter update of the opponents. Different from these methods, MADiff can use the same generative model to jointly output plans of its own trajectory and predictions of other agents' trajectories and is shown to be effective in offline settings.

## 5 Experiments

In experiments, we are aiming at excavating the ability of MADiff in modeling the complex interactions among cooperative agents, particularly, whether MADiff is able to (i) generate high-quality multi-agent trajectories; (ii) appropriately infer teammates' behavior; (iii) learn effective, coordinated policies from offline data.

### Task Descriptions

We conduct experiments on multiple commonly used multi-agent testbeds.

* **Multi-agent particle environments (MPE)**(Lowe et al., 2017): multiple 2D particles cooperate to achieve a common goal. _Spread_, three agents start at some random locations and have to cover three landmarks without collisions; _Tag_, three predators try to catch a pre-trained prey opponent that moves faster and needs cooperative containment; _World_, also requires three predators to catch a pre-trained prey, whose goal is to eat the food on the map while not getting caught, and the map has forests that agents can hide and invisible from the outside. * **Datasets**: we use the offline datasets constructed by Pan et al. (2022), including four datasets collected by policies of different qualities trained by MATD3 (Ackermann et al., 2019), namely, Expert, Medium-Replay (Md-Replay), Medium and Random.
* **Multi-Agent Mujoco (MA Mujoco)**(Peng et al., 2021): independent agents control different subsets of a robot's joints to run forward as fast as possible. We use three configurations: _2-agent halfcheetah (2halfcheetah)_, _2-agent ant (2ant)_, and _4-agent ant (4ant)_.
* **Datasets**: we use the off-the-grid offline dataset (Formanek et al., 2023), including three datasets with different qualities for each robot control task, _e.g._, Good, Medium, and Poor.
* **StarCraft Multi-Agent Challenge (SMAC)**(Samvelyan et al., 2019): a team of either homogeneous or heterogeneous units collaborates to fight against the enemy team that is controlled by the hand-coded built-in StarCraft II AI. We cover four maps: _3m_, both teams control three Marines; _2s3z_, both teams control two Stalkers and 3 Zealots; _5m_vs_6m (5m6m)_, requires controlling five Marines and the enemy team has six Marines; _8m_, both teams control eight Marines.
* **Datasets**: we use the off-the-grid offline dataset (Formanek et al., 2023), including three datasets with different qualities for each map, _e.g._, Good, Medium, and Poor.
* **Multi-Agent Trajectory Prediction (MATP)**: different from the former offline MARL challenges which should learn the policy for each agent, the MATP problem only requires predicting the future behaviors of all agents, and no decentralized model is needed.
* **NBA dataset**: the dataset consists of various basketball players' recorded trajectories from 631 games in the 2015-16 season. Following Alcorn and Nguyen (2021), we split 569/30/32 training/validation/test games, with downsampling from 25 Hz to 5Hz. Different from MARL tasks, other information apart from agents' historical trajectories is available for making predictions, including the ball's historical trajectories, player ids, and a binary variable indicating the side of each player's frontcourt. Each term is encoded and concatenated with diffusion time embeddings as side inputs to each U-Net block. *

  

### Compared Baselines and Metrics

For offline MARL experiments, we use the episodic return obtained in online rollout as the performance measure. We include MA-ICQ (Yang et al., 2021) and MA-CQL (Kumar et al., 2020) as baselines on all offline RL tasks. On MPE, we also include OMAR and MA-TD3+BC (Fujimoto and Gu, 2021) in baseline algorithms and use the results reported by Pan et al. (2022). On MA Mujoco, baseline results are adopted from Formanek et al. (2023). On SMAC, we include MADT (Meng et al., 2021) as a sequence modeling baseline, while other baseline results are reported by Formanek et al. (2023). We implement independent behavior cloning (BC) as a naive supervised learning baseline.

We use distance-based metrics including average displacement error (ADE) \(_{t=1}^{L}_{i=1}^{N}\|_{t}^{i}-o_{t}^{i}\|\) and final displacement error (FDE) \(_{i=1}^{N}\|_{L}^{i}-o_{L}^{i}\|\), where \(L\) is the prediction length (Li et al., 2020). We also report minADE\({}_{20}\) and minFDE\({}_{20}\) as additional metrics to balance the stochasticity in sampling, which are the minimum ADE and FDE among 20 predicted trajectories, respectively. We compare MADiff with Baller2Vec++ (Alcorn and Nguyen, 2021), an autoregressive MATP algorithm based on the transformer structure and specifically designed for the NBA dataset.

### Numerical Results

We reported the numerical results both for the CTDE version of MADiff (denoted as MADiff-D) and the centralized version MADiff (MADiff-C). For offline MARL, since baselines are tested in a decentralized style, _i.e._, all agents independently decide their actions with only local observations, MADiff-C is not meant to be a fair comparison but to show if MADiff-D fills the gap for coordination without global information. For MATP, due to its centralized prediction nature, MADiff-C is the only variant involved.

**Offline MARL.** As listed in Table 1, MADiff-D achieves the best result on most of the datasets. Similar to the single-agent case, direct supervised learning (BC) on the dataset behaves poorly when datasets are mixed quality. Offline RL algorithms such as MA-CQL that compute conservative values have a relatively large drop in performance when the dataset quality is low. Part of the reason may come from the fact that those algorithms are more likely to fall into local optima in multi-agent scenarios (Pan et al., 2022). Thanks to the distributional modeling ability of the DM, MADiff-D generally obtains better or competitive performance compared with OMAR (Pan et al., 2022) without any design for avoiding bad local optima similar to Pan et al. (2022). On SMAC tasks, MADiff-D achieves comparable performances, although it is slightly degraded compared with MADiff-C.

**MATP on the NBA dataset.** In Table 2, when comparing ADE and FDE, MADiff-C significantly outperforms the baseline; however, our algorithm only slightly beats baseline for minADE\({}_{20}\), and has higher minFDE\({}_{20}\). We suspect the reason is that Baller2Vec++ has a large prediction variance. When Baller2Vec++ only predicts one trajectory, a few players' trajectories deviate from the truth so far that deteriorate the overall ADE and FDE. When allowing to sample 20 times and calculating the minimum ADE/FDE according to the ground truth, Baller2Vec++ can choose the best trajectory for every single agent, which makes minADE\({}_{20}\) and minFDE\({}_{20}\) significantly smaller than one-shot metrics. However, considering it may be not practical to select the best trajectories without access to the ground truth, MADiff-C is much more stable than Baller2Vec++. Predicted trajectories of MADiff-C and Baller2Vec++ are provided in the Appendix Section H.4.

  
**Traj. Len.** & **Metric** & **Baller2Vec++** & **MADiff-C** \\   & ADE & 15.15 \(\) 0.38 & **7.92 \(\) 0.86** \\  & FDE & 24.91 \(\) 0.68 & **14.06 \(\) 1.16** \\  & minADE\({}_{20}\) & 5.62 \(\) 0.05 & **5.20 \(\) 0.04** \\  & minFDE\({}_{20}\) & **5.60 \(\) 0.12** & 7.61 \(\) 0.19 \\   & ADE & 32.07 \(\) 1.93 & **17.24 \(\) 0.80** \\  & FDE & 44.93 \(\) 3.02 & **26.69 \(\) 0.13** \\   & minADE\({}_{20}\) & 14.72 \(\) 0.53 & **11.40 \(\) 0.06** \\   & minFDE\({}_{20}\) & **10.41 \(\) 0.36** & 11.26 \(\) 0.26 \\   

Table 2: Multi-agent trajectory prediction results on NBA dataset across 3 seeds, given the first step of all agentsâ€™ positions.

Figure 2: Visualization of an episode in the _Spread_ task. Solid lines are real rollouts, and dashed lines are DM-planned trajectories.

### Qualitative Analysis on Teammate modeling

We discuss the quality of teammate modeling as mentioned in Section 3.3 and how it is related to the decentralized execution scenario. In Figure 2 left, we visualize an episode generated by MADiff-D trained on the Expert dataset of _Spread_ task. The top and bottom rows are snapshots of entities' positions on the initial and intermediate time steps. The three rows from left to right in each column represent the perspectives of the three agents, red, purple, and green, respectively. Dashed lines are the planned trajectories for the controlled agent and other agents output by DMs, and solid lines are the real rollout trajectories. We observe that at the start, the red agent and the purple agent generate _inconsistent_ plans, where both agents decide to move towards the middle landmark and assume the other agent is going to the upper landmark. At the intermediate time step, when the red agent is close to the middle landmark while far from the uppermost ones, the purple agent altered the planned trajectories of both itself and the red teammate, which makes all agents' plans _consistent_ with each other. This particular case indicates that MADiff is able to correct the prediction of teammates' behaviors during rollout and modify each agent's own desired goal correspondingly.

In Figure 2 right, we demonstrate that such corrections of teammate modeling are common and can help agents make globally coherent behaviors. We sample 100 episodes with different initial states and define _Consistent Ratio_ at some time step \(t\) as the proportion of episodes in which the three agents make consistent planned trajectories. We plot the curve up to step \(t=9\), which is approximately halfway through the episode length limit in MPE. The horizontal red line represents how many portions of the real rollout trajectories are consistent at step \(t=9\). The interesting part is that the increasing curve reaches the red line before \(t=9\), and ends up even higher. This indicates that the planned teammates' trajectories are guiding the multi-agent interactions beforehand, which is a strong exemplar of the benefits of MADiff's teammate modeling abilities. We also include visualizations of imagined teammate observation sequences in SMAC _3m_ task in the Appendix Section H.3.

### Ablation Study

Our key argument is that the great coordination ability of MADiff is brought by the attention modules among individual agents' diffusion networks. We validate this insight through a set of ablation experiments on MPE. We compare MADiff-D with independent DMs, _i.e._, each agent learns from corresponding offline data using independent U-Nets without attention. We denote this variant as MADiff-D-Ind. In addition, we also ablate the choice of whether each agent should share parameters of their basic U-Net, noted as Share or NoShare. Without causing ambiguity, we omit the name of MADiff, and notate the different variants as _D-Share_, _D-NoShare_, _Ind-Share_, _Ind-NoShare_.

As is obviously observed in Figure 3, with attention modules, MADiff-D significantly exceeds that of the independent version on most tasks, justifying the importance of inter-agent attentions. The advantage of MADiff-D is more evident when the task becomes more challenging and the data becomes more confounded, e.g., results on World, where the gap between centralized and independent models is larger, indicating the difficulty of solving offline coordination with independently trained models. As for the parameter sharing choice, the performance of MADiff-D-Share and MADiff-D-NoShare is similar overall. Since MADiff-D-Share has fewer parameters, we prefer MADiff-D-Share, and use it as the default variant to be reported in Table 1. Another advantage of sharing U-Net parameters is that the trajectories of various agents can be batched together and fed through the network. This not only decreases sampling time but also renders it insensitive to an increasing number of agents. We provide a specific example in Appendix Section G.4.

Figure 3: The average normalized score of MADiff ablation variants in MPE tasks. The mean and standard error are computed over 5 different seeds.

### Limitations

**Scalability to many agents.** MADiff-D requires each agent to infer all teammates' future trajectories, which is difficult and unnecessary in environments with a large number of agents. Although we have done experiments on a maximum number of 8 agents (SMAC _8m_), MADiff-D is in general not suitable for scenarios with tens or hundreds of agents. A potential solution is to infer a latent representation of teammates' trajectories.

**Applicability in highly stochastic environments.** Several theoretical and empirical studies (Paster et al., 2022; Brandfonbrener et al., 2022; Chen et al., 2021) have demonstrated that in offline RL, sequence modeling algorithms tend to underperform Q-learning-based algorithms in environments with high stochasticity. This is primarily because sequence modeling algorithms are more susceptible to high-reward offline trajectories that are achieved by chance. Since MADiff is a sequence modeling algorithm, it shares this weakness. To assess how much MADiff is affected by environmental stochasticity, we conducted experiments on the _terran_5_vs_5_ map in SMACv2 (Ellis et al., 2022). The design principle of SMACv2 is to add stochasticity to the original SMAC environment, including randomized initial positions and unit types. We conducted experiments under four settings: the original version, without position randomness, without unit type randomness, and without both kinds of randomness. MADiff performs worse than the Q-learning-based method only when both kinds of stochasticity are present. In all settings, MADiff outperforms the sequence modeling baseline. Detailed experimental settings and results can be found in Appendix Section H.1.

## 6 Conclusion

In this paper, we propose MADiff, a novel generative multi-agent learning framework, which is realized with an attention-based diffusion model designed to model the complex coordination among multiple agents. To our knowledge, MADiff is the first diffusion-based offline multi-agent learning algorithm, which behaves as both a decentralized policy and a centralized controller including teammate modeling, and can be used for multi-agent trajectory prediction. Our experiments indicate strong performance compared with a set of recent offline MARL baselines on a variety of tasks.