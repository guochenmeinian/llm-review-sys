# Model Collapse Demystified: The Case of Regression

Elvis Dohmatob\({}^{@sectionsign}\)  Yunzhen Feng\({}^{}\) Julia Kempe\({}^{@sectionsign}\)

\({}^{@sectionsign}\)FAIR, Meta

\({}^{}\) Center for Data Science, New York University

\({}^{}\)Courant Institute of Mathematical Sciences, New York University

\({}^{*}\)Correspondence to dohmatob@meta.com

###### Abstract

The era of proliferation of large language and image generation models begs the question of what happens if models are trained on the synthesized outputs of other models. The phenomenon of "model collapse" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e. the model collapses. In this work, we investigate this phenomenon within the context of high-dimensional regression with Gaussian data, considering both low- and high-dimensional asymptotics. We derive analytical formulas that quantitatively describe this phenomenon in both under-parameterized and over-parameterized regimes. We show how test error increases linearly in the number of model iterations in terms of all problem hyperparameters (covariance spectrum, regularization, label noise level, dataset size) and further isolate how model collapse affects both bias and variance terms in our setup. We show that even in the noise-free case, catastrophic (exponentially fast) model-collapse can happen in the over-parametrized regime. In the special case of polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.

## 1 Introduction

_Model collapse_ describes the situation where the performance of large language models (LLMs) or large image generators degrade as more and more AI-generated data becomes present in their training dataset . Indeed, in the early stages of the generative AI evolution (e.g the ChatGPT-xyz series of models), there is emerging evidence suggesting that retraining a generative AI model on its own outputs can lead to various anomalies in the model's later outputs. This phenomenon has been particularly observed in LLMs, where retraining on their generated content introduces irreparable defects, resulting in what is known as "model collapse", the production of nonsensical or gibberish output . Though several recent works demonstrate facets of this phenomenon _empirically_ in various settings , a theoretical understanding is still missing.

In this work, we initiate a theoretical study of model collapse in the setting of high-dimensional supervised-learning with linear regression. This is equivalent to kernel regression1, which serves as an effective proxy for neural networks in various regimes, for instance in the infinite-width limit  or in the lazy regime of training .  characterize the power-law generalization error of regularized least-squares kernel algorithms, assuming a power-decay spectrum of the kernel (capacity) and of the coefficients of the target function (source). Source and capacity power decaycapture properties of the data and the model that give rise to power law scaling of test error in terms of data set size and model capacity, as empirically observed e.g. in . More recently, scaling laws have been shown for kernel models under the Gaussian design, e.g. in  for regression and  for classification.  study scaling laws for regression in the random feature model.

**Summary of Main Contributions.** Following the rich tradition in prior works outlined above, we study the Gaussian design where the input \(x\) is sampled from a multivariate zero-mean Gaussian \((0,)\) and labels \(y\) are determined by a linear ground truth function with independent label noise \(\) as \(y=x^{}w_{0}+\) (we present the linear regression setting for ease, the generalization to the kernel setting is straightforward). At each generation step, an approximation to \(w_{0}\) is learned from the data, and used to generate new, fake /synthetic labels for the next generation. Note that the machine learner has no control over the fake data generation process. It only sees data from a stage \(n\) of this process, which is then used to fit a downstream predictor. Our main findings can be summarized as follows:

_(1) Exact Characterization of Test Error under Iterative Retraining on Synthesized Data._ In Section 4 (Theorem 4.3), we obtain analytic formulae for test error under the influence of training data with fake / synthesized labels. For \(n\)-fold iteration of data-generation, this formula writes

\[E_{test}=E_{test}^{clean}+ Bias+n_{0}(,T,T_{0}, ,),\] (1)

where \(E_{test}^{clean}\) is the usual test error of the model trained on clean data (not AI-generated) and \(_{0}^{2}\) the label noise level in the clean data distribution. The non-negative term \(\) precisely highlights the effects of all the relevant problems parameters: the feature covariance matrix \(\), sample size \(T\), original data size \(T_{0}\), label noise level in the fake data distribution \(^{2}\), and regularization \(\). The non-negative term \( Bias\) is an increase in bias brought about by the iterative synthetic data generation process. This term disappears in the _under-parametrized_ regime (Corollary 4.4), if each stage in the process was fitted on sufficiently many samples \(T_{0}\) compared to the input dimension \(d\) (i.e if \(T_{0} d\)). In the _over-parametrized_ case where \(T_{0}<d\), this term is either a constant (Theorem 4.5) or an increasing function of \(n\), depending on whether the design matrix stays the same or is resampled across different generations (Theorem 4.6). Notably, even in the case of noiseless labels (when \(_{0}=0\)), the downstream model converges to a Gaussian process around zero exponentially fast with the number of iterations \(n\), leading to "catastrophic" model collapse.

A direct consequence of (1) is that, as the number of generations \(n\) becomes large, the effect of re-synthesizing will make learning impossible. We note that the multiplicative degradation in scaling with the number of generations \(n\) is completely analogous to what has been shown in  for infinite memory models and their variants and empirically observed there. Illustration in Figures 0(a) and 2.

_(2) Modified Scaling Laws._ Turning to the special case of power-law spectra of the covariance matrix \(\), which allows to derive test-error scaling laws , we obtain in Section 5 (see Theorem 5.1) precise new scaling laws of the test error that quantitatively highlight the negative effect of training

Figure 1: **Demystifying model collapse.** Refer to Appendix D for details on the experimental setup.

on synthetically generated data. Further exploiting our analytic estimates, we obtain (Corollary 5.2) the optimal ridge regularization parameter \(\) as a function of all the problem parameters (sample size, spectral exponents, strength of fake data-generator, etc.). This new regularization parameter corresponds to a correction of the the value proposed in the classical theory on clean data , and highlights a novel crossover phenomenon where for an appropriate tuning of the regularization parameter, the effect of training on fake data is a degradation of the fast error rate in the noiseless regime [14; 11] to a much slower error rate which depends on the amount of true data on which the fake data-generator was trained in the first place. On the other hand, a choice of regularization which is optimal for the classical setting (training on real data), might lead to catastrophic failure: the test error diverges. See Figure 0(b) for an illustration.

## 2 Related Work

Current LLMs [17; 30; 10; 47], including GPT-4 , were trained on predominantly human-generated text; similarly, diffusion models like DALL-E , Stable Diffusion , Midjourney  are trained on web-scale image datasets. Their training corpora already potentially exhaust all the available clean data on the internet. A growing number of synthetic data generated with these increasingly popular models starts to populate the web, often indistinguishable from "real" data. Recent works call attention to the potential dramatic deterioration in the resulting models, an effect referred to as _"model collapse"_.

Empirical evidence of model collapse has been reported across various domains [23; 32; 33; 8; 9; 21]. Some theoretical studies [44; 7; 2; 18] have begun exploring this phenomenon.  attribute collapse to finite sampling bias and function approximation errors in the (single) Gaussian case but only provide lower bounds without detailed analytic expressions.  analyze the training process at the distribution level using both clean and synthetic data and provide stability results. However, these results do not account for finite samples and are only valid locally in parameter space, making them more relevant to fine-tuning rather than training from scratch.  examine "self-consuming loops" in the Gaussian case by assuming a sampling bias that reduces data variance with each generation--a (martingale) assumption that we do not require. These studies lack a comprehensive theoretical framework to quantify model collapse and its impact on scaling laws. Our work addresses these gaps by providing an analytic theory that captures how model collapse emerges from training on synthetic data, providing a deeper understanding that goes beyond merely identifying the collapse. A concurrent study by  demonstrate that model collapse in foundation models can be attributed to a breakdown in scaling laws [26; 24], where increasing the sample size eventually fails to improve model performance. This finding, theoretically shown for discrete data in variants of the infinite memory model, complements our analytical results on how synthetic data alters the rate of scaling laws, as discussed in Section 5.

Figure 2: **Model collapse in the case of noiseless over-parametrized synthetic data generator. Here \(d=300\), the sample sizes for the different versions of the fake data generator are equal, i.e \(T_{n}=T_{0}=d/2\) for all \(n\), and noise levels are \(_{0}=0\) and \(=0.1\). Everything else is as in the setting of Figure 0(a). Broken lines correspond to the theoretical estimates given in Theorem 4.3. As predicted by our theory, the test error of the model fitted on synthetic data (\(n 1\)) increases (relative to the baseline \(n=0\), corresponding to training on clean data). The model collapse here, even in the absence of noise (\(_{0}=0\)), is due to the fact that the synthetic data-generator does not have access to enough data to capture the true labelling function. (a) Importantly, and in accordance to our theory, the amount of model collapse in the case \(X_{n} X_{0}\) is due to an increase in bias term of the test error of the model and does not depend on the number of generations \(n\) as long as \(n 1\). (b) In contrast, for the case where the \(X_{n}\)’s are independent, the increase in bias term grows with \(n\), leading to “catastrophic” model collapse (Theorem 4.6). Refer to Appendix D for the experimental setup.**

Theoretical Setup

We now present a setup which is simple enough to be analytically tractable, but rich enough to exhibit a wide range of regimes to illustrate a range of new phenomena that emerge with model collapse.

**Data Distribution and Synthetized Data.** Consider the distribution \(P_{,w_{0},^{2}}\) on \(^{d}\) given by

**(Input)**\(x N(0,),\; N(0,^{2}),\; \;x\;y=x^{}w_{0}+.\) (2)

The positive integer \(d\) is the input-dimension, the vector \(w_{0}^{d}\) defines the ground-truth labelling function \(x x^{}w_{0}\), the matrix \(^{d d}\) is the covariance structure of the inputs. The scalar \(^{2}\) is the level of label noise. Here, we consider the linear case for clarity. We describe the extension to the kernel setting in Appendix C. Thus, in classical linear regression, given a sample \((X,Y)\{(x_{1},y_{1}),,(x_{T},y_{T})\}\) of size \(T\) from \(P_{,w_{0},^{2}}\), one seeks a linear model \(^{d}\) with small test error

\[E_{test}():=_{x,y}[(x^{}-y)^{2}]-^{ 2}=\|-w_{0}\|_{}^{2},\] (3)

where \((x,y) P_{,w_{0},^{2}}\) is a random clean test point. In our setup for studying model collapse, the training data \((X,Y)\) is sampled from an iterative loop where each generation of the model serves as the labeller for the data for the next generation. This process is described below.

**Structure of the Synthesized / Fake Data Generator.** Consider a sequence of data distributions

\[P_{,w_{0},_{0}^{2}} P_{,_{1},_{1}^{2}}  P_{,_{n},_{n}^{2} },\] (4)

where \(_{n}\)'s is defined recursively by \(_{n}=w_{0}\), and

\[_{n}=(X_{n-1},_{n-1}),\; \;n 1,\] (5)

where \(_{n}:=X_{n}_{n}+E_{n}\) and \((A,B)=(A,B):=A^{}B\) is ordinary-least squares (OLS). The design matrices \((X_{n})_{n 0}\) are of shapes \(T_{n} d\), each with iid rows from \(N(0,)\). The sequence of noise vectors \((E_{n})_{n 0}\) forms an independent collection, which is independent of the \((X_{n})_{n 0}\) ; each \(E_{n}^{T_{n}}\) has iid components \(_{n,i}\) from \(N(0,_{n}^{2})\). Refer to Figure 3.

**The Downstream Model: Ridge Regression.** For a number of iterations \(n 0\), noise levels \(_{0}\) and \(\), dataset sizes \(T_{0}\) and \(T\), and regularization parameter \( 0\), let \(_{n}^{pred}=_{n,T_{0},_{n}^{2},T,,} ^{pred}^{d}\) be the ridge predictor constructed from an iid sample \(\{(x_{1},y_{1}),,(x_{T},y_{T})\}\) of size \(T\) from the \(n\)-fold fake data distribution \(P_{,_{n},_{n}^{2}}\), where for ease of presentation of our results we will assume that

\[T_{n-1}==T_{1}=T_{0}, T_{n}=T_{n-1} ==_{1}=_{0},_{n}=.\] (6)

For an \(n\)-fold fake data generator \(P_{,_{n},_{n}^{2}},\) we denote with \(X:=X_{n}^{T d}\) the design matrix with iid rows from \(N(0,)\), with \(E:=E_{n}^{T}\) the stage-\(n\) label-noise vector with components in \(N(0,_{n}^{2})\), and \(Y:=_{n}=X_{n}+E^{T}\) the labels generated by \(P_{,_{n},_{n}^{2}}\). Let \(:=X^{}X/T^{d d}\) is the sample covariance matrix, and \(R=R():=(+ I_{d})^{-1}\) denote its _resolvent_, so that

\[_{n}^{pred}=RX^{}Y/T\;\;>0,\;\; _{n}^{pred}=X^{}Y\;\;=0.\] (7)

Figure 3: **Illustration of the theoretical framework.** The process begins with the original model \(_{0}(w_{0})\) and the original dataset \((X_{0},_{0})\). \(n\) synthetic data generators \(_{1}\) to \(_{n}\) are iteratively fit on data labelled by the previous model with label noise \(_{0}\), using \(T_{0}\) samples each. We evaluate the test error (with respect to the ground truth labels from \(w_{0}\)) of \(_{n}^{pred}\), trained on \((X,Y):=(X_{n},_{n})\) using \(T\) samples with label noise \(\) and a regularization coefficient \(\).

Thus, in summary, each \(_{n}\) results from fitting a model on a dataset of size \(T_{n-1}\) from \(P_{,_{n-1},_{n-1}^{2}}\), for every generation index \(n 1\).

We are interested in the dynamics of the test error \(E_{test}(_{n}^{pred})\) of this linear model. Importantly, the evaluation of the model is performed on the true data distribution \(P_{,w_{0},_{0}^{2}}\), even though the model is trained on the fake data distribution \(P_{,_{n},^{2}}\). Note that for \(n=0,E_{test}^{clean}:=E_{test}(_{n}^{pred})\) corresponds to the usual test error when the downstream model is trained on clean data. Importantly, the downstream model has no control over this process. It will only see training data from a given version \(P_{,_{n},_{n}^{2}}\), but evaluation will be on the true distribution \(P_{,w_{0},_{0}^{2}}\).

The mental picture is as follows: each generation \(_{n}\) can be seen as a proxy for a specific version of ChatGPT, for example. The sample size \(T_{0}\) used to create the fake labelling functions \(_{n}\) is a proxy for the strength of the fake data-generator thus constructed. Other works which have considered model collapse under such a self-looping training process include [44; 2; 7; 18].

## 4 Exact Test Error Characterization

In this section we establish generic analytic formulae for the test error of the downstream model \(_{n}^{pred}\) (7) trained on \(n\)-fold fake data-generation as outlined in Section 3. The fully general technical key Theorem F.1 detailing formula (1), with a trace expression for \(\), (as well as proofs) are given in Appendix F; consult part F.1 for an exposition. Notations are standard (summarized in Appendix E).

### Warm-up: Ordinary Least Squares on Isotropic Data

For a start, let us first consider the case of unregularized regression, where \(=0\) in Equation (7).

**Theorem 4.1**.: _For an \(n\)-fold fake data generation process with \(T_{0} d+2\) samples, the test error for the linear predictor \(_{n}^{pred}\) in Equation (7) learned on \(T d+2\) samples, with \(=0\), is given by_

\[E_{test}(_{n}^{pred})}{1-}+^{2}_{0}}{1-_{0}},=,\ _{0}=},\] (8)

_where the notation \(f(T) g(T)\) means \(f(T)/g(T) 1\), for large \(T\)._

The first term \(E_{test}(_{0}^{pred})^{2}/(1-)\) in the above decomposition corresponds to the usual error when the downstream model is fitted on clean data (see , for example). The additional term \(n_{0}^{2}_{0}/(1-_{0})\), proportional to the number of generations \(n\), is responsible for model collapse.

**Model collapse versus more training data.** Note that the linear degeneration in test error highlighted by Equation (8) is a direct consequence of using the same dataset size \(T_{0}\) across the fake data generator. Of course, if the underlying synthetic generating process has access to a larger data budget across generations, this decay can be significantly alleviated. For instance, if fake data increases gradually with the number of generations \(m 2\) as \(T_{m}=(m^{2}m)T_{0}\) (and, to simplify, \(=_{0}\)) a trivial extension of Theorem 4.1 yields

\[E_{test}(_{n}^{pred})(1+2}+3}+)E_{test}(_{0}^{pred}) E_{test}(_{0}^{ pred}),\]

which will keep collapse at bay at the expense of largely increased training data ( also has a similar formula). This does not avoid model collapse; rather, it trades additional data generation and training effort against deterioration from generations of fake data. Thus, while for clean data increasing the dataset size n-fold leads to better scaling, with synthetic data, we forfeit this improvement. Also, note that we do not assume access to samples from any of the intermediate generation steps \(_{0},,_{n-1}\); we only train the downstream model \(_{n}^{pred}\) on data from the last step \(_{n}\).

**Model Collapse as Change of Scaling Laws.** In the low-dimensional regime (fixed \(d\)), Theorem 4.1 already predicts a change of scaling law from \(^{2}T^{-1}\) to \(^{2}T^{-1}+n_{0}^{2}T_{0}^{-1}\). Thus, as the sample size \(T\) is scaled up, the test error eventually plateaus at the value \(n_{0}^{2}T_{0}^{-1}\) and does not vanish. This phenomenon, also established in  in the context of large language models, is clearly visible in Figure 0(a). In the rest of this section and also in Section 5, we shall establish an analogous picture for high-dimensional regimes (\(d\)).

**Mitigation via Regularization.** Note that the test error of the null predictor \(w_{null}=0\) is \(E_{test}(w_{null})=\|w_{0}\|_{}^{2}\), and so

\[(_{n}^{pred})}{E_{test}(w_{null})}=}+_{0}}}{1-_{0}},\]where \(:=\|w_{0}\|_{}^{2}/^{2}\) and \(_{0}:=\|w_{0}\|_{}^{2}/_{0}^{2}\). We deduce that if \(n_{0}/(1/_{0}-1)\), then the learned model is already much worse than the null predictor! This suggests that a possible strategy for mitigating the negative effects on learning on AI-generated data is regularization, as empirically illustrated in Figures 0(a), 0(b), 0(c), and also in 0(c) of Appendix D.

Furthermore, in Section 5 we shall establish that the optimal regularization parameter established in , in the case of polynomially decreasing spectra (a regime which is relevant to wide neural networks), must be modified in the presence of synthetic training data in order to prevent the generalization error to diverge to infinity (i.e catastrophic failure).

### High-Dimensional Regimes

In order to analyze the trace term \(\) appearing in Equation (1) (and spelled out in (32) in Appendix F.1), we need some tools from RMT, and ultimately obtain analytic formulae for \(E_{test}(_{n}^{pred})\) in Theorem 4.3. Such tools have been used extensively to analyze anisotropic ridge regression [41; 22; 4].

**Random Matrix Equivalents.** For any sample size \(T 1\) and \( 0\), define \((,T)\) implicitly by

\[(,T)-=(,T)_{1}(( ,T))/T,\] (9)

where, for any \( 0\) and \(m_{*}\), \(_{m}()\) is the \(m\)th order "degree of freedom" of the covariance matrix \(\) is given by \(_{m}()=_{m}(;):=\, ^{m}(+ I_{d})^{-m}\).

The effect of ridge regularization at level \( 0\) is to improve the condition of the empirical covariance matrix \(\); what the \(\)-function does is translate this into regularization on \(\) at level \((,T)\), so as control the capacity of the former, i.e. the "effective dimension" of the underlying problem. Quantitatively, there is an equivalence of the form \(_{1}(;)_{1}(( ,T);)\). Roughly speaking, RMT is the business of formalizing such a relationship and derivatives (w.r.t. \(\)) thereof. A standard reference on the subject is .

**Example: Isotropic Data.** As an illustration, note that \(_{m}() d/(1+)^{m}\) (polynomial decay) in the isotropic case where \(=I_{d}\). Consequently, we have

\[(,T)-=(,T)/(1+(,T)), :=d/T.\]

In this case, it is easy to obtain the following well-known formula for \(=(,T)\):

\[=+)^{2}+4 }}{2},:=-1,\] (10)

which is reminiscent of the celebrated Marchenko-Pastur law .

**Asymptotic Regime.** We shall work in the following so-called proportionate asymptotic scaling regime which is a standard analysis based on random matrix theory (RMT):

\[T,d, d/T,\|\|_{op},\|^{-1}\|_{op}=O(1).\] (11)

Later in Section 5 when we consider power-law spectra, this scaling will be extended to account for the more realistic case where \(d\) and \(T\) are of the same order on log scale, i.e

\[T,d, d^{1/C} T d^{C},\|\|_{op},\| ^{-1}\|_{op}=O(1),\] (12)

for some absolute constant \(C 1\). Such non-proportionate settings are covered by the theory developed in [27; 48]. For clarity of presentation, even in this more general regime of Equations (12), we will still continue to write \(_{0}:=d/T_{0}\) and \(:=d/T\).

**Bias-Variance Decomposition.** With everything now in place, let us recall for later use, the classical bias-variance decomposition for ridge regression (for example, see [41; 22; 4]):

**Proposition 4.2**.: _In the RMT limit (12), the test error of a ridge predictor \(w()\) based on \(T\) iid samples from the true data distribution \(P_{,w_{0},^{2}}\) is given by_

\[E_{test}(w()) =\,\|w()-w_{0}\|_{}^{2} Bias+Var,\] (13) \[Bias w_{0}^{}(+ I)^{-2}w_{ 0}}{1-_{2}()/T}, Var_{ 2}()}{T}_{2}()/T},\] (14)

_where \(=(,T)\) is as given in Equation (9)._

### Analytic Formula for Test Error

The following result gives the test error for the downstream ridge predictor \(_{n}^{pred}\) defined in Equation (7), in the context of fake training data, and will be heavily exploited later to obtain precise estimates in different regimes. Define generic \(Var\) and \(Bias\) by:

\[Var=\|RX^{}E/T\|_{}^{2}=^{2}  R^{2} Bias=\| Rw_{0}-w_{0}\|_{}^{2},\]

and note that \(E_{test}^{clean}:=Bias+Var\), for standard ridge regression fitted on clean data from the true data distribution \(P_{,w_{0},^{2}}\) (e.g., see Hastie et al. ). Let \(Q_{n-1}=P_{n-1}P_{n-2} P_{0}\) where \(P_{m}\) is the orthogonal projection unto the subspace of \(^{d}\) spanned by the rows of \(X_{m}\) and define

\[ Bias :=\|R(Q_{n-1}w_{0}-w_{0})\|_{ }^{2} 0.\] (15)

**Theorem 4.3**.: _For an \(n\)-fold fake data-generation process, the test error of a ridge predictor \(_{n}^{pred}\) based on a sample of size \(T\) with regularization parameter \(\), is given in the RMT limit (12) by_

\[E_{test}(_{n}^{pred})+Var+n_{0}^{2},\] (16)

_where \(\) is as given in Theorem F.1, and \(\) satisfies_

\[ Bias+ Bias BiasT_{0} d),\]

_and \( Bias\) as given in (15). Furthermore, if one of the following conditions holds_

\[T_{0} dX_{n}=X_{0}n 1,\] (17)

_then, we have the following explicit formula for \(\)_

\[=^{4}(+_{0}I)^{-2}(+ I )^{-2}}{T_{0}-_{2}(_{0})}+ {tr}^{2}(+_{0}I)^{-2}(+ I)^{-2}}{T_{0}- _{2}(_{0})}_{2}()}{ T-_{2}()},\] (18)

_with \(=(,T)\) and \(_{0}:=(0,T_{0})\) are as given in Equation (9)._

Instructively, the term \( Bias\) measures how biased the synthetic data-generation process away from ground-truth model \(w_{0}\). This term disappears if the generator was fitted on sufficiently many samples (i.e. if \(T_{0} d\)). More quantitatively, when \(T_{0}<d\) and \(X_{n}=X_{0}\), it is easy to see that \( Bias[\|^{1/2}R\|_{op}^{2} ] Bias_{0}\), where \(Bias_{0}:=\|P_{0}w_{0}-w_{0}\|_{2}^{2}\) measures the inability due to lack of enough data, of the first generation (\(n=1\)) to reliably estimate \(w_{0}\) even in the absence of noise (\(_{0}=0\)) in the data-generating process. This gap propagates over to higher generations of the process. The situation is illustrated in Figure 2. In the case where \(T_{0}<d\) and the \(X_{n}\)'s are independent, we shall see in Section 4.5 that this increase in bias actually grows with \(n\), even in the case of fake data generation without label noise (i.e. \(_{0}=0\)).

### Model Collapse in the Case of Under-Parametrized Fake Data-Generator

We now consider the scenario of under-parameterization, where \(T_{0} d\), indicating that the number of data points exceeds the number of dimensions. This condition typically results in a unique solution for the regression. In this case, \(P_{0}=I_{d}\) a.s., leading to \(=Bias\) (given as in formula (14)), and \(_{0}=0\) in (18), and so Theorem 4.3 gives

\[=_{2}()}{T_{0}-d}+ (+ I)^{-2}}{T_{0}-d}_{2}()}{T-_{2}()}.\] (19)

We have the following corollary to Theorem 4.3.

**Corollary 4.4**.: _Consider the setting of Theorems 4.3 and F.1. If \(T_{0} d\) additionally, then it holds in the RMT limit (12) that \(E_{test}(_{n}^{pred}) Bias+Var+n_{0}^{2}\), where \(Bias\) and \(Var\) are as given in formula (14), and \(\) is as given in Equation (19)._

_Moreover, in the special case of isotropic features, it holds that_

\[Bias+Var\|w_{0}\|_{2}^{2}+^{2}}{(1+)^{2} -},}{1-_{0}}( }+}}{(1+)^{2}-}),\]

_with \(:=d/T\), \(_{0}:=d/T_{0}\), and \(=(,T)\) as in Equation (10)._

Note that Theorem 4.1 is special case of the above result corresponding to \(=0\) and \( 1\). A result like Corollary 4.4 gives us the needed analytical handle for understanding \(n\)-fold model collapse in terms of all problem hyper-parameters (covariance spectrum, regularization, label-noise level, etc.).

### Model Collapse in the Absence of Label Noise

We now consider the over-parametrized regime, where the different iterations of the synthetic data-generator (refer to the illustration in Figure 3) are fitted on insufficient data. For simplicity of exposition, we restrict our presentation to isotropic covariance \(=I_{d}\). Since we will be focusing on the possible increase \( Bias\) above the bias (defined in Equation (14)) due to \(n 1\) generations as predicted by Theorem 4.3, we further restrict ourselves to the noiseless regime where the fake data-generating process has no label noise, i.e. \(_{0}=0\). Thanks to Lemma F.4, we know that the generation-\(n\) fake labelling vector \(_{n}\) (defined in Eqn. (5)) is given explicitly as a series of projections

\[_{n}=Q_{n-1}w_{0}=P_{n-1}P_{n-2} P_{0}w_{0}.\] (20)

Further, for simplicity we will assume \(T=T_{n}>d\), i.e the downstream model has access to enough data. We shall focus on two important special cases.

**The Dependent Case.** We first consider the case where \(T_{m}=T_{0}<d\) and \(X_{m}=X_{0}\) for all \(m n-1\). It is clear that Equation (20) reduces to \(_{n}=P_{0}w_{0}\), with \(P_{0}=T_{0}<d\).

**Theorem 4.5**.: _In the limit \( 0^{+}\) and \(d,T_{0}\) with \(d/T_{0}_{0}>1\), it holds that_

\[\|_{n}\|^{2}\|w_{0}\|^{2}/_{0}, Bias\| w_{0}\|^{2}(1-1/_{0}).\] (21)

We see that in this setting, the increase in bias \( Bias(1-1/_{0})\|w_{0}\|^{2}\) brought about by synthetic data is a positive constant which does not grow with the number of generations \(n 1\). This increase in bias (i.e compared to training on clean data) is due to the fact that, with probability \(1\), the random subspace of \(^{d}\) spanned by \(X_{0}\) does not contain the ground truth model \(w_{0}\). The expression is nothing but a RMT estimate of \(\|P_{0}w_{0}-w_{0}\|^{2}\), i.e. the squared norm of the projection of \(w_{0}\) onto the orthogonal complement of this subspace. The result is illustrated in Figure 2(a).

**The Independent Case.** For our second example, we remove the assumption that \(T_{m}=T_{0}\) and \(X_{m}=X_{0}\) for all \(m n-1\) considered in the previous case (Theorem 4.5). We instead assume that (A) The \(X_{m}\)'s are assumed to be independent, and (B) we are in the following high-dimensional limit

\[ 0^{+}, d,T_{1},,T_{n-1}, d/T_{m}_{m },_{1},,_{n-1}>0.\] (22)

Define \(:=_{n=0}^{n-1}(1/_{m},1)(0,1]\). We have the following theorem.

**Theorem 4.6**.: _In the limit (22), it holds that \(\|_{n}\|^{2}\|w_{0}\|^{2}\) and \( Bias\|w_{0}\|^{2}(1-)\). In particular, if \(n\) with infinitely many \(_{m}>1\), then \(_{n} 0\) and \( Bias\|w_{0}\|^{2}\)._

The theorem predicts that a sequence of over-parametrized fake data-generators \((_{n})_{n}\) collapses to zero (and thus, effectively escapes from the ground truth model \(w_{0}\)). Consequently, the downstream model \(_{n}^{pred}\) convergences to a Gaussian process around zero, instead of the true model \(w_{0}\), leading to an increase in the bias term of the test error!

For example if \(_{n}=_{0}>1\), then Theorem 4.6 predicts that \( Bias(1-_{0}^{-n})\|w_{0}\|^{2}\), which grows exponentially fast towards \(\|w_{0}\|^{2}\), the test error of the null predictor. This compounding effect is due to the fact that in (20), each projection \(P_{m}\) spins the fake data labelling vector \(_{n}\) further away from the ground-truth \(w_{0}\). The result is illustrated in Figure 2(b).

Comparing the dependent case and the independent case, 4.6 shows that the increase in bias is proportional to \(1-_{0}_{1}_{n-1}\), which is typically much larger than \(1-_{0}\), which is the increase in the dependent case. Sampling different design matrices results in a more pronounced model collapse.

## 5 The Case of Heavy Tails (Power Law)

Neural scaling laws , relate a model's test error to the sample size, model size, and computational resources, and are critical tools for practitioners in strategically allocating resources during the design and implementation of large language models. Previous theoretical works  have examined scaling laws in our tractable setting of linear regression with Gaussian design in the context of a power-law covariance spectrum. Now we explore how synthetic data alters these scaling laws in this setting.

Let the spectral decomposition of the covariance matrix \(\) be \(=_{1}v_{1}v_{1}^{}++_{d}v_{d}v_{d}^{}\), where \(_{1}_{d} 0\) are the eigenvalues and \(v_{1},,v_{d}^{d}\) are the eigenvectors. For any feature index \(j[d]\), define a coefficient \(c_{j}:=w_{0}^{}v_{j}\), i.e the projection of \(w_{0}\) along the \(j\)th eigenvector of \(\). We shall work under the following well studied spectral conditions

\[&\ _{j} j^{-}\ \ j[d],\\ &\,\|^{1/2-r}w_{0}\|=O(1), \] (23)

where \(>1\) and \(r>0\). The parameter \(r\) measures the amount of dispersion of \(w_{0}\) relative to the spectrum of \(\); a large value of \(r\) means \(w_{0}\) is concentrated only along a few important eigen-directions (i.e. the learning problem is easy). For later convenience, define \(\), \(\), and \(c\) by

\[:=1+(2r-1),:=(r,1)(0,1),  c:=2/(2+1)(0,1).\] (24)

As noted in , the source condition in (23) is satisfied if \(c_{j} j^{-/2}\) for all \(j[d]\).

Consider adaptive ridge regularization strength of the form

\[=(T) T^{-},\] (25)

for fixed \( 0\). The case where \(=0\) corresponds to non-adaptive regularization; otherwise, the level of regularization decays polynomially with the sample size \(T\). Define

\[_{crit}:=/(2+1).\] (26)

In , KRR under normal circumstances (corresponding to \(n=0\), i.e. no fake data) was considered and it was shown that this value for the regularization exponent in (25) is minimax-optimal for normal test error in the noisy regime (\(>0\)), namely \(E_{test}(_{0}^{pred}) T^{-c}\). This represents a crossover from the noiseless regime where it was shown that the test error scales like \(E_{test}(_{0}^{pred}) T^{-2}\), a much faster rate. In the context of training on fake data, which is the object of this manuscript, we shall establish new scaling laws which paint a drastically different picture.

**A "Collapsed" Scaling Law.** The following result shows that model collapse is a modification of usual scaling laws induced by fake data. All proofs of this section can be found in Appendix H. Here, for simplicity of presentation, we restrict to the case \(T_{0} d+2\) to make the results easier to present. This condition can be removed as in Theorem 4.3.

**Theorem 5.1**.: _Consider \(n\)-fold fake-data generation with sample size \(T_{0} d+2\). For a ridge predictor \(_{n}^{pred}\) given in Equation (7) based on a fake data sample of size \(T\), with regularization parameter \(=(T)\) tuned adaptively as in Equation (25) with exponent \([0,)\), the test error satisfies the following scaling law in the RMT limit (12):_

\[E_{test}(_{n}^{pred})(^{2},T^{1-2- }) T^{-(1-)}+^ {2}}{1-_{0}}\,(T/_{0},_{0}) T^{-(1-)}.\] (27)

We now provide an instructive interpretation of Theorem 5.1 and outline the effect of regularization.

**The Noiseless Regime.** First consider the case \(=0\) (or equivalently, exponentially small in \(T\)) and \(_{0}(0,1)\) is fixed, and consider a number of generations \(n\) such that \(n_{0}^{2} T^{a}\), where \(0 a 1-/ 1\). Note that \(a=0\) corresponds to a constant number of generations. Also take \(T_{0}=T^{b}\), for some constant \(b(0,)\). According to Theorem 5.1, if we want to balance out the model-collapsing negative effect of training on fake data, we should chose \(\) so as to balance the second term \(n(T/T_{0})T^{-(1-t/)}=T^{-(b-/-a)}\) and the first term \(T^{-2}\). We have the following:

**Corollary 5.2**.: _In the setting of Theorem 5.1 with \(T_{0} T^{b}\) and \(n T^{b}\), the optimal exponent of the ridge regularization parameter in Equation (25) is \(=_{}\), where_

\[_{}=((b-a)_{crit},),\] (28)

_and \(_{crit}\) is as in Eqn. (26), with corresponding optimal test error \(_{ 0}E_{test}(_{n}^{pred}) T^{-(b-a)c}\)._

Observe that when \((b-a)c<2\), which is the case when \(n=O(1)\), \(r 1\) and \(b a+1\), this corresponds to the condition \(T T_{0}\). The above result represents a crossover from the fast rate \(E_{test}(_{0}^{pred}) T^{-2}\) in the case of training on clean data , to a much slower rate \(E_{test}(_{n}^{pred}) T^{-(b-a)c}\), attained by the adaptive regularization \( T^{-_{*}}\), which is optimal in this setting. Furthermore, in this setting if we still use \( T^{-_{crit}}\) as proposed in  in the clean data setting, Corollary 5.2 predicts that

\[E_{test}(_{n}^{pred}) T^{-(b-_{crit}/-a)}=T^{-(c+b-a-1 )},\]

which diverges to infinity if \(b a+1-c\). This is a catastrophic form of model collapse, and is empirically illustrated in Figures 0(b) and 4.

**The Noisy Regime.** This discussion can be found in Appendix G.

**Remark.** In all the analyses above, we quantitatively demonstrate how model collapse manifests as a _change in scaling laws_ within a setting commonly used to understand scaling behavior in current foundation models [46; 13; 14]. Our results indicate that, in the presence of synthetic data, scaling laws with respect to dataset size slow down (i.e., exhibit smaller exponents), meaning a much larger sample size is needed to achieve the same reduction in test error as with real data. Furthermore, the optimal scaling law with synthetic data requires different regularization; the optimal settings for real data could lead to catastrophic model collapse. Related findings are reported in Dohmatob et al.  in the setting of discrete data for infinite memory models and their variants.

## 6 Experiments

To further support our theoretical findings, we conduct experiments using kernel ridge regression on the MNIST dataset , as detailed in Appendix D.2. Our experiments validate the theoretical predictions for both RBF and Polynomial kernels, demonstrating the parallels between linear regression and kernel regression, and highlighting the relevance of our theory to more complex settings.

We also explore the behavior of real neural networks by training two-layer networks in two different settings: fixing the first layer or training both layers (see Appendix D.3). Consistent with our theoretical insights, we observe a linear pattern of model collapse when the first layer is fixed. However, a more severe, nearly quadratic model collapse is observed when both layers are trained, with our theory providing a lower bound for this behavior. These results reinforce the ability of our theory to capture the dynamics of model collapse across varying complexities. Full experimental details and results are provided in Appendix D.

## 7 Concluding Remarks

As we navigate the "synthetic data age", our findings signal a departure from traditional test error rates (e.g. neural scaling laws), introducing novel challenges and phenomena with the integration of synthetic data from preceding AI models into training sets. Our work provides a solid analytical handle for demystifying the model collapse phenomenon as a modification of usual scaling laws caused by fake / synthesized training data.

On the practical side, our analysis reveals that AI-generated data alters the optimal regularization for downstream models and changes the scaling laws. Drawing from the insight that regularization mirrors early stopping , our study suggests that models trained on mixed real and AI-generated data may initially improve but later decline in performance (model collapse), necessitating early detection of this inflection point. To preserve model quality when scaling laws are altered, it is essential to employ data filtering and watermarking techniques to distinguish real data from synthetic content. Recent studies have also explored methods for data selection  and correction . These observations prompt a re-evaluation of current training approaches and underscores the complexity of model optimization in the era of synthetic data.