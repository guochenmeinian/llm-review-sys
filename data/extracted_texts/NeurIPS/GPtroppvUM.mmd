# Adversarial Training for Graph Neural Networks: Pitfalls, Solutions, and New Directions

Lukas Gosch\({}^{1*}\), Simon Geisler\({}^{1}\), Daniel Sturm\({}^{1*}\), Bertrand Charpentier\({}^{1}\),

**Daniel Zugner\({}^{1,2}\), Stephan Gunnemann\({}^{1}\)**

\({}^{1}\)Department of Computer Science & Munich Data Science Institute

Technical University of Munich

\({}^{2}\)Microsoft Research

{l.gosch, s.geisler, da.sturm, s.guennemann}@tum.de | dzuegner@microsoft.com

Equal contribution.

###### Abstract

Despite its success in the image domain, adversarial training did not (yet) stand out as an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations. In the pursuit of fixing adversarial training _(1)_ we show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; _(2)_ we reveal that flexible GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable; _(3)_ we introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) constraints. Including these contributions, we demonstrate that adversarial training is a state-of-the-art defense against adversarial structure perturbations.1

## 1 Introduction

Adversarial training has weathered the test of time and stands out as one of the few effective measures against adversarial perturbations. While this is particularly true for numerical input data like images , it is not yet established as an effective method to defend predictions of Graph Neural Networks (GNNs) against graph structure perturbations.

Although previous work reported some robustness gains by using adversarial training in GNNs [2; 3], closer inspection highlights two main shortcomings: (i) their learning setting leads to a biased evaluation and (ii) the studied architectures seem to struggle to learn robust representations.

_(i) Learning setting._ In the previously studied transductive learning settings (see Table 3), clean validation and test nodes are known during training. Thus, _perfect robustness_ can be achieved by memorizing the training graph. This can lead to a false impression of robustness. Indeed, we find that the gains of the adversarial training approach from Xu et al.  largely stem from exploiting this flaw. Motivated by this finding, we revisit adversarial training for node classification under structure perturbations in a fully inductive setting (i.e., validation/test nodes are excluded during training). Thus, our results do not suffer from the same evaluation pitfall and pertain to a more challenging and realistic scenario.

Figure 1: Robust diffusion (GPRGNN) vs. GCN on (inductive) Cora-ML. We report adversarial training and standard training.

_(ii) GNN architectures._ The studied GNNs like GCN  or APPNP  all seem to share the same fate as they have a limited ability to adjust their message passing to counteract adversarial perturbations. Instead, we propose to use flexible message-passing schemes based on _learnable diffusion_. The main motivation behind this choice is the ability to approximate any spectral graph filter. Thus, adversarial training may choose the most robust filter that achieves a competitive training loss. Thereby, we significantly improve robustness compared to previous work, while yielding an interpretable message-passing scheme, and making the evaluation bias in the transductive setting (i) apparent.

_More realistic perturbations._ Inspecting robust diffusion indicates that previously studied perturbation sets [2; 3] are overly permissive and unrealistic. Prior work only constrained the _global_ (graph-level) number of inserted and removed edges, despite studying node classification, where predictions are inherently _local_ (node-level). As a result, commonly, an adversary has been allowed to add or remove a number of edges that exceeds the average node degree by 100 times or more, providing ample leeway for a complete change of many node neighborhoods through rewiring. E.g., on Cora-ML 5% edge-changes correspond to \(798\) changes, but the average degree is 5.68. To prevent such degenerate perturbations , we propose Locally constrained Randomized Block Coordinate Descent (LR-BCD), the first attack that, while targeting multiple nodes at once, is able to constrain _both_ local perturbations per node and global ones. Even though local constraints are well-studied for attacks targeting a single node , surprisingly, there was no attack incorporating these for jointly attacking a set of nodes at once. Thus, LR-BCD fills an important gap in the literature, while being efficient and effective.

Addressing the aforementioned points, we substantially boost empirical and certifiable adversarial robustness up to the level of state-of-the-art defenses for GNNs. For example, in Figure 1, we show a 4-fold increased robustness over standard training (measured in decline in accuracy after an attack).

**Contributions. (1)** We show theoretically and empirically that in the transductive learning setting previously studied for adversarial training, one can trivially achieve perfect robustness. We show that the full inductive setting does not have this limitation and, consequently, revisit adversarial training in this setting (see Section 2). **(2)** By leveraging more flexible GNN architectures based on learnable diffusion, we significantly improve upon the robustness under adversarial training (see Section 3). **(3)** We implement more realistic adversaries for structure perturbations with a novel attack that constrains perturbations globally and locally for each node in the graph (see Section 4).

## 2 Learning Settings: Transductive vs. Inductive Adversarial Training

We first give background on adversarial training and self-training. Then, in Section 2.1, we discuss the transductive learning setting and its shortcomings in contrast to inductive learning in the context of robust generalization. Following previous work, we focus on node classification and assume we are given an undirected training graph \(=(,)\) consisting of \(n\) nodes of which \(m\) are labeled. By \(\{0,1\}^{n n}\) we denote the adjacency matrix, by \(^{n d}\) the feature matrix, and by \(y_{i}\{1,...,C\}\) the label of a node \(i\), summarized for all nodes in the vector \(\).

**Adversarial Training.** Adversarial training optimizes the parameters of a GNN \(f_{}\) on an adversarially perturbed input graph with the goal to increase robustness against test-time (evasion) attacks (i.e., attacks against the test graph after training). The objective during training is

\[*{arg\,min}_{}_{}( })}_{i=1}^{m}(f_{}(})_{i},y _{i})\] (1)

where \(f_{}(})_{i}\) is the GNN-prediction for node \(i\) based on the perturbed graph \(}\), \(\) is a chosen loss function, and \(()\) is the set of allowed perturbed graphs given the clean graph \(\). As in previous work, we assume that an adversary maliciously changes the graph structure by inserting or deleting edges. Then, \(()\) can be defined by restricting the total number of malicious edge changes in the graph to \(_{ 0}\) (called a global constraint) and/or restricting the total number of edge changes in each neighborhood of a node \(i\) to \(_{i}^{(l)}_{ 0}\) (called local constraints, see Section 4). Solving Equation (1) is difficult in practice. Thus, we approximate it with alternating first-order optimization, i.e., we approximately solve Equation (1) by training \(f_{}\) not on the clean graph \(\), but on a perturbed one \(}\) that is newly crafted in each iteration through attacking the current model (see Algorithm B.2).

**Self-training** is an established semi-supervised learning strategy  that leverages unlabeled nodes via pseudo-labeling and is applied by previous works on adversarial training for GNNs [2; 3]. For this, first a model \(f_{^{}}\) is trained regularly using the \(m\) labeled nodes, minimizing \(_{i=1}^{n}(f_{}()_{i},y_{i})\). Thereafter, a new (final) model \(f_{}\) is randomly initialized and trained on all nodes, using the known labels for the training nodes and pseudo-labels generated by \(f_{^{}}\) for the unlabeled nodes (see Algorithm B.3). Thus, if including self-training, Equation (1) changes to

\[*{arg\,min}_{}_{}( )}_{i=1}^{m}(f_{}(})_{i},y_{i})+_{i=m+1}^{n}(f_{}(})_{i},f_{^{ }}()_{i})}\] (2)

### Transductive Setting

Transductive node classification is a common and well-studied graph learning problem [9; 4]. It aims to complete the node labeling of the given and partially labeled graph \(\). More formally, the goal at test time is to accurately label the already known \(n-m\) unlabeled nodes, i.e., to achieve minimal

\[_{0/1}(f_{})=_{i=m+1}^{n}_{0/1}(f_{}()_{i},y_{i})\] (3)

This is in contrast to an inductive setting, where at test time a new (extended) graph \(^{}\) (with labels \(^{}\)) is sampled conditioned on the training graph \(\). Then, the goal is to optimally classify the newly sampled nodes \(\)_in expectation_ over possible \((^{},^{})\)-pairs, i.e., to minimize \(_{i}_{0/1}(f_{}(^{ })_{i},y^{}_{i})\). That is, in transductive learning the \(n-m\) unlabeled nodes known during training are considered test nodes, but in an inductive setting, new unseen test nodes are sampled. For additional details on the inductive setting, see Appendix A.

Many common graph benchmark datasets such as Cora, CiteSeer, or Pubmed [10; 11], are designed for transductive learning. Naturally, this setting has been adopted as a starting point for many works on robust graph learning [7; 2; 12], including all of the adversarial training literature (see Section 6). Here, the latter is concerned with defending against test-time (evasion) attacks in transductive node classification, i.e., it is assumed that after training an adversary can select a maliciously changed graph \(}\) out of the set of permissible perturbations \(()\), with the goal to maximize misclassification:

\[_{adv}(f_{})=_{} B()} _{i=m+1}^{n}_{0/1}(f_{}(})_{i},y_{i})\] (4)

Since the adversary changes the graph at test time (i.e., the changes are not known during training), this, strictly speaking, corresponds to an inductive setting , where the only change considered is adversarial. Now, the goal of _adversarial training_ is to find parameters \(\) minimizing \(_{adv}(f_{})\), corresponding to the optimal (robust) classifier under attack .

#### 2.1.1 Theoretical Limitations

Defending against test-time (evasion) attacks in a transductive setting comes with conceptual limitations. In the case of graph learning, the test nodes are already known at training time and the only change is adversarial. Hence, we can design a defense algorithm \(\) achieving _perfect robustness_ without trading off accuracy through _memorizing_ the (clean) training graph.

Formally, \(\) takes a GNN \(f_{}\) as input and returns a slightly modified model \(_{}\) corresponding to \(f_{}\) composed with a preprocessing routine (memory) that, during inference, replaces the perturbed graph \(}=(},})( )\) with the clean graph \(=(,)\) known from training, resulting in \(_{}(})=f_{}()\). In other words, \(_{}\) ignores every change to the graph including those of the adversary. Trivially, this results in the same (clean) misclassification rate \(_{0/1}(f_{})=_{0/1}(_{})\) because \(_{}\) and \(f_{}\) have the same predictions on the clean graph, but also perfect robustness, in the sense of \(_{0/1}(_{})=_{adv}(_{})\), as the predictions are not influenced by the adversary. Thus, we can state:

**Proposition 1**.: _For transductive node classification, \(_{}=(f_{})\) is a perfectly robust version of an arbitrary GNN \(f_{}\), in the sense of \(_{0/1}(f_{})=_{0/1}(_{})=_{adv}(_{})\)._

However, what is probably most interesting about \(\) is that we can use it to construct an _optimal solution_ to the otherwise difficult saddle-point problem \(_{}_{adv}(f_{})=_{}_{}  B()}_{0/1}(f_{})\) arising in adversarial training. Formally, we state (proof see Appendix C.1):

[MISSING_PAGE_FAIL:4]

Robust Diffusion: Combining Graph Diffusion with Adversarial Training

We propose to use learnable graph diffusion models able to approximate any graph filter in conjunction with adversarial training to obtain a _robust diffusion_. The key motivation is to use more flexible GNN architectures for adversarial training than previous studies. We not only show that a robust diffusion significantly outperforms other models used in previous work (see Section 5), but it also allows for increased _interpretability_ as we can gain insights into the characteristics of such robustness-enhancing models from different perspectives.

**In fixed message passing** schemes of popular GNNs like GCN or APPNP, each layer can be interpreted as the _graph convolution_\(g()=g()^{}\) between a _fixed_ graph filter \(g()\) and the transformed node attributes are \(=f_{}()\) using MLP \(f_{}\). This convolution is defined w.r.t. the (diagonalized) eigenvalues \(^{n n}\) and eigenvectors \(^{n n}\) of the Laplacian \(^{}=-^{-}{{2}}} ^{-}{{2}}}\) with diagonal degree matrix \(\). Following the common reparametrization, instead of \(^{}\), we use the "normalized adjacency" \(=^{-}{{2}}} ^{-}{{2}}}\) or, depending on the specific model, \(}=}^{-}{{2}}}}}^{-}{{2}}}\) where \(}=+\) with node degrees \(}\). Then, many spatial GNNs relate to the \(K\)-order polynomial approximation \(g()^{} _{k=0}^{K}_{k}^{k}\) with the \(K+1\) diffusion coefficients \(^{K+1}\). Many GNNs stack multiple convolutions and add point-wise non-linearities.

Crucially, GCN's or APPNP's graph filter \(g()\) is fixed (up to scaling). Specifically, we obtain an MLP with \(=[1,0,,0]\). If using \(}\), a GCN corresponds to \(=[0,-1,0,,0]\), and in APPNP \(\) are the Personalized PageRank coefficients.

**Robust diffusion.** In contrast to prior work on robust graph learning, we do not solely use a static parametrization of \(g()\). Instead, we learn the graph filter, which corresponds to training diffusion coefficients \(\). This allows the model to adapt the graph diffusion to the adversarial perturbations seen during training, i.e., we learn a _robust diffusion_.

For this, the used architectures consist of two steps: **(1)** using an MLP to preprocess the node features, i.e., \(=f_{}()\) where \(\) are learnable parameters; and then, **(2)** using a learnable diffusion computing the logits. For the learnable diffusion, we employ GPRGNN  that uses the previously introduced monomial basis: \(_{k=0}^{K}_{k}}^{k} \). Additionally, we study Chebyshev polynomials (see ChebNetII ) that are of interest due to their beneficial theoretical properties. ChebNetII can be expressed as \(_{k=0}^{K}_{k}w_{k}T_{k}() \) with extra weighting factor \(w_{k}=}{{K-1}}_{j=0}^{K}T_{k}(x_{j})\). The Chebyshev basis is given via \(T_{0}()=\), \(T_{1}()=\), and \(T_{k}()=2T_{k-1}()-T_{k-2}( {L})\). The Chebyshev nodes \(x_{j}=}{{K+1}},j=0,,K\) for \(w_{k}\) reduce the so-called Runge phenomenon . Note, the resulting Chebyshev polynomial can be expressed in monomial basis (up to \(\) vs. \(}\)) via expanding the \(T_{k}()\) terms and collecting the powers \(^{k}\).

**Interpretability.** While chaining multiple layers of a "fixed" convolution scheme (e.g. GCN) might allow for similar flexibility, with our choice of robust diffusion, we can gain insights about the learned robust representation from the _(i)_ polynomial, _(ii)_ spectral, and _(iii)_ spatial perspective.

_(i) Polynomial perspective._ The coefficients \(_{k}\) determine the importance of the respective \(k\)-hop neighborhood for the learned representations. To visualize \(_{k}\), we can always consider \(_{0}\) to be positive, which is equivalent to flipping the sign of the processed features \(\). Additionally, we normalize the coefficients s.t. \(||=1\) since \(\) also influences the scale. In Figure 6, we give an example for the polynomial perspective (details are discussed in Section 5).

Figure 4: Robust diffusion (GPRGNN) on Karate Club where the edge \((u,v)\) width encodes the diffusion coefficient \(T_{u,v}\) learned during training. In (a) we show the normalized adjacency matrix. The other plots show the robust diffusion transition matrix: (b) w/o adversarial training, (c) w/ adversarial training but w/o local constraints, and (d) w/ adversarial training and w/ local constraints.

_(ii) Spectral perspective._ We solve for \(g_{}()\) in the polynomial approximation \(g_{}()^{}_{k=0}^{K}_{k} }^{k}\) to obtain a possible graph filter \(g_{}()=^{}(_{k=0}^{K}_{k}}^{k}) \). Following Balciar et al. , we study the spectral characteristics w.r.t. \(-^{-}{{2}}}^{-}{{2}}}\). Then, the diagonal entries of \(g_{}()\) correspond to the (relative) magnitude of how signals of frequency \(\) are present in the filtered signal. Vice versa, a low value corresponds to suppression of this frequency. Recall, low frequencies correspond to the small eigenvalues and high to the large eigenvalues. In Figure 7, we show how adversarial training and the permissible perturbations affect the spectra of the learned graph filters.

_(iii) Spatial perspective._ Robust diffusion (monomial basis) can be summarized as \(()\) where \(=_{k=0}^{K}_{k}}^{k}\) is the total diffusion matrix. The coefficient \(_{uv}\) indicates the diffusion strength between node \(u\) and node \(v\). For example, we visualize the total diffusion matrix \(\) in Figure 4 on Karate Club  with different training strategies. Depending on the learning signal we give, GPRGNN is able to adjust its diffusion.

## 4 LR-BCD: Adversarial Attack with Local Constraints

**Motivation for local constraints.** The just-discussed interpretability of robust diffusion provides empirical evidence for the importance of local constraints. Specifically, from Figure 4c, we see that GPRGNN adversarially trained _without_ local constraints learns a diffusion that almost ignores the graph structure. While this model is certainly very robust w.r.t. structure perturbations, it is not a very useful GNN since it cannot leverage the structure information anymore. In contrast, we show in Figure 4d that GPRGNN trained adversarially _with_ local constraints results in a robust model that can still incorporate the structure information.

The local predictions in node classification yield an alternative motivation. In the absence of local constraints, an adversary typically has the power to rewire the entire neighborhood of many nodes. When attacking GNNs, we empirically observe that a majority of successfully attacked nodes are perturbed beyond their degree even for moderate global attack budgets (see Figure 5). That such perturbations are not reasonable is evident in the fact that studies on local attacks [7; 17], where the adversary targets a single node's prediction, do not consider perturbations (far) beyond the node degree. However, a \(5\%\) attack budget on Cora-ML allows changing \(798\) edges while the majority of nodes have a degree less than three.

Traditionally, adversarial changes are judged by noticeability  since unnoticeable changes do not alter the semantics. However, for graphs, manual inspection of its content is often not feasible, and the concept of (un-)noticeability is unclear. However, using generative graph models Gosch et al.  revealed that perturbations beyond the node degree most often do alter the semantics. Perturbations that alter semantics can pose a problem for adversarial training. Similar observations have been done in graph contrastive learning where perturbations that do not preserve graph semantic preservation have a direct effect on the achievable error bounds .

**Locally constrained Randomized Block Coordinate Descent (LR-BCD).** We next introduce our LR-BCD attack; the first attack targeting multiple nodes at once while maintaining a local constraint for each node next to a global one. For this, we extend the so-called PR-BCD attack framework.

_Projected Randomized Block Coordinate Descent (PR-BCD)_ is a gradient-based attack framework applicable to a wide range of models. Its goals is to generate a perturbation matrix \(\{0,1\}^{n n}\) that is applied to the original adjacency matrix \(\) to construct a perturbed matrix \(}=+(-2)\), where \(\) is an all-one matrix and \(\) the element-wise product. For undirected graphs, only the upper-triangular parts of all matrices are considered. To construct \(\) in an efficient manner, PR-BCD relaxes \(\) during the attack from \(\{0,1\}^{n n}\) to \(^{n n}\) and employs an iterative process for \(T\) iterations. It consists of three repeating steps. **(1)** A random block of size \(b\) is sampled. That is, only \(b\) (non-contiguous) elements in the perturbation matrix \(_{t-1}\) are considered and all other elements set to zero. Thus, \(_{t-1}\) is sparse. **(2)** A gradient update w.r.t. the loss to compute relaxed perturbations

Figure 5: Number of successfully attacked nodes by node degree and number of connected adversarial edges. We attack self-trained GCN with PR-BCD and global budgets \(10\%\) (left) and \(25\%\) (right) on Cora-ML and aggregate the results over three different data splits. We observe that a notable amount of nodes is perturbed beyond their degree.

\(_{t}\) is performed, i.e., \(_{t}_{t-1}+_{t-1}_{_{t-1}}(_{t-1})\), where \(_{t-1}\) are the previous perturbations, \(_{t-1}\) is the learning rate, and \(_{_{t-1}}(_{t-1})\) are the perturbation gradients through the GNN. Finally, and most crucially **(3)** a projection \(_{()}\) ensures that the perturbations are _permissible_ given the set of allowed perturbations \(()\), i.e. \(_{t}=_{()}(_{t})\). Now, the process starts again with **(1)**, but all zero-elements in the block \(b\) (or at least \(}{{2}}\) of the lowest-value block elements) are resampled. After \(T\) iterations, the perturbations \(_{T}\) are discretized from \(^{n n}\) to \(\{0,1\}^{n n}\) to obtain the discrete \(}\). Specifically, \(_{T}\) is discretized via sampling, where the elements in \(_{T}\) are used to define Bernoulli distributions.

_Global projection._ As mentioned above, the projection \(_{()}\) of PR-BCD is the crucial step that accounts for the attack budget. Geisler et al.  develop an efficient projection for an adversary implementing a global perturbation constraint, i.e., for \(()=\{}\{0,1\}^{n n}\,|\,\|}-\|_{0}\}\) with \(_{ 0}\). This is achieved by formulating the projection as the following optimization problem

\[_{}()=_{}\|-\|_{F}^{2}  _{i,j}P_{i,j}\] (5) \[^{n n}\]

with Frobenius norm \(\|\|_{F}^{2}\) and the sum aggregating all elements of the matrix. \(_{}()\) can be solved with the bisection method . However, the projection \(_{}\) does not support limiting the number of perturbations per node, i.e., \(_{j=1}^{n}P_{i,j}_{i}^{(l)}\) where \(^{(l)}_{ 0}^{n}\) is the vector of local budgets for each node. Indeed, extending the optimization problem above to include local constraints leads to the notoriously hard problem of Euclidean projection to polyhedra .

_A locally constrained global projection (LR-BCD)._ With the goal of an efficient attack including local constraints, we have to develop an alternative and scaleable projection strategy for \(()=\{}\{0,1\}^{n n}\,|\,\|}-\|_{0}<\|_{j}(}_{ij}-_{ij})\| _{0}_{i}^{(l)}\; i[n]\}\). Our novel projection \(=_{}^{(l)}()=_{}()^{*}\) chooses the largest entries from the perturbation matrix \(^{n n}\) using \(^{*}^{n n}\) and clipping operator \(_{}()\), s.t. we obey global _and_ local constraints. The optimal choices \(^{*}\) can be calculated by solving a relaxed multi-dimensional knapsack problem:

\[^{*}=_{}_{i,j}  _{i,j}P^{}_{i,j}\] (6) \[_{j}P^{}_{i,j}_{i}^{(l)}  i[n]\] \[^{n n}\]

where \(^{}=_{}()\), i.e., \(_{}()\) represents the weights of the individual choices, while \(\) captures their value. \(_{i,j}P^{}_{i,j}\) aggregate all elements in the matrix. The local constraint \(_{j}P^{}_{i,j}_{i}^{(l)}\), constrain the perturbations in each neighborhood, with node-specific budgets \(_{i}^{(l)}\). Even though this optimization problem has no closed-form solution, it can be readily approximated with greedy approaches . The key idea is to iterate the non-zero entries in \(\) in descending order and construct the resulting \(^{*}\) (or directly \(\)) as follows: For each perturbation \((u,v)\) related to \(_{uv}\), we check if the global \(\) or local budgets \(_{u}^{(l)}\) and \(_{v}^{(l)}\) are not yet exhausted. Then, \(^{*}_{uv}=1\) (i.e., \(_{u,v}=_{}(_{uv})\)). Otherwise, \(^{*}_{uv}=\{,_{u}^{(l)},_{v}^{(l)}\}\). The final discretization is given by \(^{*}\) corresponding to the solution of Equation (6) for \(_{T}\), but changing the weight of each choice to 1 (i.e., \(^{}=\)), guaranteeing a binary solution due to the budgets being integer. We give the pseudo-code and more details in Appendix B.4.

Our projection yields sparse solutions as it greedily selects the largest entries in \(\) until the budget is exhausted. Moreover, it is efficient. Its time complexity is \((b(b))\) due to the sorting of the up to \(b\) non-zero entries in the sparse matrix \(\). Its space complexity is \((b)\), assuming we choose the block size \(b\) as well as \(b n\). Thus, LR-BCD has the same asymptotic complexities as the purely global projection by Geisler et al.  (which we will from now on denote as PR-BCD).

**Summary.** We will now summarize our LR-BCD along the four dimensions proposed by Biggio and Roli . _(1) Attacker's goal:_ Although our focus is to increase the misclassification for the target nodes in node classification, we can apply LR-BCD to different node- and graph-level tasks depending on the loss choice. _(2) Attacker's knowledge:_ We assume perfect knowledge about the model and dataset, which is reasonable for adversarial training. _(3) Attacker's capability:_ We propose to use a threat model that constrains the number of edge perturbations globally as well as locally for each node. _(4) Attacker's strategy:_ LR-BCD extends PR-BCD by a projection incorporating local constraints. Most notably, we relax the unweighted graph during the attack to continuous values and leverage randomization for an efficient attack while optimizing over all possible \(n^{2}\) edges.

## 5 Empirical Results

**Inductive learning.** All results in this section follow a fully inductive semi-supervised setting. That is, the training graph \(\) does not include validation and test nodes. For evaluation, we then successively add the respective evaluation set. This way, we avoid the evaluation flaw of prior work since the model cannot achieve robustness via "memorizing" the predictions on the clean graph. We obtain inductive splits randomly, except for OGB arXiv , which comes with a split. In addition to the commonly sampled 20 nodes per class for both (labeled) training and validation, we sample a stratified test set consisting of \(10\%\) of all nodes. The remaining nodes are used as (unlabeled) training nodes.

**Setup.** We study the citation networks Cora, Cora-ML, CiteSeer , Pubmed , and OGB arXiv . Furthermore, WikiCS [25; 26], which has significantly more heterogeneous neighborhoods as well the heterophilic dataset Squirrel  and (contextual) Stochastic Block Models (SBMs)  with a heterophilic parametrization (see Appendix B.1). As models we choose GPRGNN, ChebNetII, GCN, APPNP, and GAT . Further, we compare to the state-of-the-art evasion defenses Soft Median GDC  in the main part and GRAND  in Table D.6. We apply adversarial training (see Section 2 and Appendix B.5) using both PR-BCD that only constraints perturbations globally and our _LR-BCD_ that also allows for local constraints. Moreover, we use adversarial training in conjunction with self-training. Due to the inductive split, this does not bias results. We use the tanh margin attack loss of  and do not generate adversarial examples for the first 10 epochs (warm-up). We evaluate robust generalization on the test set using L/PR-BCD, which corresponds to _adaptive attacks_. Adaptive attacks are the gold standard in evaluating empirical robustness because they craft model-specific perturbations . We use \(\) to parametrize the global budget \(=_{u}d_{u}/2\) relative to the degree \(d_{u}\) for the set of targeted nodes \(\). We find that \(_{u}^{(l)}= d_{u}/2\) is a reasonable local budget for all datasets but arXiv where we use \(_{u}^{(l)}= d_{u}/4\). We report averaged results with the standard error of the mean over three random splits. We use GTX 1080Ti (11 Gb) GPUs for all experiments but arXiv, for which we use a V100 (40 GB). For details see Appendix B. We discuss limitations in Appendix F and provide code at https://www.cs.cit.tum.de/daml/adversarial-training/.

**Certifiable robustness.** We use the model-agnostic randomized/sparse smoothing certificate of Bojchevski et al.  to also quantify certifiable robustness. Sparse smoothing creates a randomized ensemble given a base model \(f_{}\) s.t. the majority prediction of the ensemble comes with guarantees. For the randomization, we follow Bojchevski et al.  and uniformly drop edges with \(p_{-}=0.6\) as well as add edges with \(p_{+}=0.01\). Sparse smoothing then determines if a node-level prediction is

    & **Adv.** & **A. eval.** &  **\(\)** \\ **trn.** **A. trim.** \\  } &  **LR-BCD** \\ **CR-BCD** \\ **end.} & **PR-BCD** & **LR-BCD** & **PR-BCD** & **Certifiable accuracy / sparse smoothing** &  **\(\)** \\ **Clean** \\  } \\   & **GCN** & **✗** & - & \(72.0 2.5\) & \(54.7 2.8\) & \(51.7 2.8\) & \(45.3 3.4\) & \(38.0 3.8\) & \(38.3 11.5\) & \(1.7 0.7\) & \(4.8 1.5\) \\ 
**GAT** & **✗** & - & \(3.6 2.7\) & \(-3.9 3.4\) & \(+0.5 3.5\) & \(-15.9 5.3\) & \(-2.3 7.3\) & \(-14.0 12.0\) & \(-1.7 0.7\) & \(-4.8 1.5\) \\
**APPNP** & **✗** & - & \(+0.2 1.1\) & \(+1.7 0.7\) & \(+0.9 1.4\) & \(+3.0 1.2\) & \(-2.2 2.5\) & \(+8.9 9.1\) & \(+31.2 6.4\) & \(+31.6 6.4\) \\
**GPGNN** & **✗** & - & \(+2.2 4.3\) & \(+4.2 2.7\) & \(+3.6 4.9\) & \(+5.5 3.9\) & \(+7.9 4.6\) & \(+17.9 6.9\) & \(+42.4 4.4\) & \(+41.3 3.7\) \\
**ChebNetII** & **✗** & - & \(+1.1 2.2\) & \(+5.8 2.5\) & \(+5.0 2.4\) & \(+104.2 2.6\) & \(+7.6 3.4\) & \(+52.6 9.9\) & \(+58.6 1.2\) & \(+54.0 0.9\) \\
**SoftMedian** & **✗** & - & \(+0.9 1.7\) & \(+9.5 2.2\) & \(+9.3 1.9\) & \(+162.4 2.4\) & \(+146.9 2.5\) & \(+52.3 1.0\) & \(+60.3 1.5\) & \(+57.5 0.8\) \\ 
**GCN** & **LR-BCD** & \(-0.2 1.4\) & \(+7.8 1.6\) & \(+5.9 1.6\) & \(+0.9 1.9\) & \(+2.1 8.1\) & \(+2.3 3.0\) & \(+9.4 10.7 4.6\) & \(+13.1 5.5\) \\
**GCN** & **✗** & - & \(+0.8 1.6\) & \(+5.9 0.9\) & \(+3.5 1.6\) & \(+8.6 2.3\) & \(+5.8 2.4\) & \(+42.1 46.0\) & \(+1.5 5.1\) & \(+11.4 4.5\) \\
**GAT** & **✗** & - & \(+0.8 1.6\) & \(+5.9 3.7\) & \(+0.9 3.0\) & \(+8.4 5.1\) & \(+13.4 3.5\) & \(+2.0 3.0\) & \(+4.1 3.7\) & \(+40.0 2.1\) \\
**APPNP** & **✗** & - & \(+0.8 1.6\) & \(+5.9 1.6\) & \(+0.9 1.1\) & \(+1.1 9.8\) & \(+3.6 3.6\) & \(+49.7 3.9\) & \(+41.3 2.0\) & \(+44.1 1.7\) & \(+40.0 2.1\) \\
**APPNP** & **✗** & - & \(+0.8 1.3\) & \(+7.6 1.6\) & \(+0.6 1.9\) & \(+11.1 1.9\) & \(+3.8 3.6\) & \(+49.7 3.9\) & \(+41.3 2.0\) & \(+44.1 1.2\) \\
**APPNP** & **✗** & - & \(+0.2 2.2\) & \(+6.2 2.2\) & \(+5.3 5.5\) & \(+9.1 9.0\) & \(+6.5 3.8\) & \(+49.3 7.9\) & \(+41.0 3.8\) & \(+41.9 3.0\) \\ 
**GPGNN** & **✗** & **LR-BCD** & \(+17.0 3.0\) & **+157.6\( 3.6\)** & \(+13.6 3.5\) & \(+248.2 4.2\) & \(+22.9 4.3\) & \(+34.0 1.8\) & \(+67.4 1.5\) & \(+65.0 2.5\) \\
**ChebNetII** & **✗** & - & \(+0.6 3.6\) & \(+15.0 3.6\) & \(+15.9 4.2\) & \(+23.2 4.3\) & \(+26.4 5.4\) & \(+32.9 1.8\) & \(+69.0 1.5\) & \(+45.9 2.3\) \\
**ChebNetII** & **✗** & **+ & \(+3.7 1.4\)** & \(+11.5 1.6\) & \(+11.5 1

[MISSING_PAGE_FAIL:9]

## 6 Related Work

We next relate to prior work and refer to Appendix E for more detail: _(1) Learning setting_. Conceptual problems by remembering training data are not unique to adversarial training and have been mentioned in Zheng et al.  for graph injection attacks and by Scholten et al.  for certification. _(2) Adversarial training_ for GNNs under structure perturbations have been studied in . However, all prior works study a transductive setting (see Table 3) having serious shortcomings for evasion attacks (see Section 2). Note that only Xu et al.  study adversarial training for (global) structure perturbations in its literal sense, i.e., directly work on Equation (1). Further, we are the first to study adversarial training in the inductive setting. _(3) Local constraints_ have been studied for local attacks , i.e., if attacking a single node's prediction. They are also well-established for robustness certificates . However, surprisingly, there is no global attack, i.e., attack targeting a large set of node predictions at once, that incorporates local constraints. This even led to prior work trying to naively and expensively apply local attacks for each node sequentially to generate locally constrained global perturbations . With LR-BCD, we finally close this gap. _(4) GNN Architectures_. The robust GNN literature strongly focuses on GCN and variants . GAT is perhaps the most flexible studied model . While adversarial training improves the robustness of GAT, in our experiments, an adversarial trained GAT is not more robust than a GCN . A novel aspect of our work is that spectral filters, in the form of polynomials , can learn significantly more robust representations, beating a state-of-the-art graph defense. They also reveal insights about the learned robust representation.

## 7 Broader impact

We are convinced that the benefits outweigh the risks. Having the right tools at hand can further the reliability of graph machine learning. Due to the more fine-grained perturbation models, researchers and practitioners and improve upon defending adversarial attacks. We firmly believe that transparent research into the vulnerabilities of models allows researchers and practitioners to understand potential problems and build strong defenses - as also showcased in our paper. Moreover, due to the studied whitebox setup, our approaches are not directly applicable for real-world malicious actors.

## 8 Discussion and Conclusion

We show that the transductive learning setting in prior work on adversarial training for GNNs has fundamental limitations leading to a biased evaluation and a trivial solution through memorization. Thus, we revisit adversarial training in a fully inductive setting. Furthermore, we argue that future research on evasion attacks in the graph domain, in general, should focus on the inductive setting to avoid the conceptual limitations inherent to combining transductive learning with test-time attacks. Moreover, we employ more flexible GNNs than before that achieve substantial robustness gains through adversarial training and are interpretable. For more realistic perturbations, we develop LR-BCD - the first global attack able to maintain local constraints.

  &  &  \\  Deng et al.  & Transductive & evasion (attribute) \\ Feng et al.  & Transductive & evasion (attribute) \\ Jin and Zhang  & Transductive & evasion (structure + attribute) \\ Xu et al.  & Transductive & evasion (structure) \\ Xu et al.  & Transductive & evasion (structure) \\ Chen et al.  & Transductive & evasion (structure) \\ Li et al.  & Transductive & evasion (structure + attribute) \\ Guo et al.  & Transductive & evasion + poisoning (structure) \\ 

Table 3: Works on adversarial training for GNNs.