# AED: Adaptable Error Detection

for Few-shot Imitation Policy

 Jia-Fong Yeh\({}^{1}\)  Kuo-Han Hung\({}^{1,*}\)  Pang-Chi Lo\({}^{1,*}\)  Chi-Ming Chung\({}^{1}\)

**Tsung-Han Wu\({}^{2}\)  Hung-Ting Su\({}^{1}\)  Yi-Ting Chen\({}^{3}\)  Winston H. Hsu\({}^{1,4}\)**

\({}^{1}\)National Taiwan University \({}^{2}\)University of California, Berkeley

\({}^{3}\)National Yang Ming Chiao Tung University \({}^{4}\)MobileDrive

###### Abstract

We introduce a new task called Adaptable Error Detection (AED), which aims to identify behavior errors in few-shot imitation (FSI) policies based on visual observations in novel environments. The potential to cause serious damage to surrounding areas limits the application of FSI policies in real-world scenarios. Thus, a robust system is necessary to notify operators when FSI policies are inconsistent with the intent of demonstrations. This task introduces three challenges: (1) detecting behavior errors in novel environments, (2) identifying behavior errors that occur without revealing notable changes, and (3) lacking complete temporal information of the rollout due to the necessity of online detection. However, the existing benchmarks cannot support the development of AED because their tasks do not present all these challenges. To this end, we develop a cross-domain AED benchmark, consisting of 322 base and 153 novel environments. Additionally, we propose Pattern Observer (PrObe) to address these challenges. PrObe is equipped with a powerful pattern extractor and guided by novel learning objectives to parse discernible patterns in the policy feature representations of normal or error states. Through our comprehensive evaluation, PrObe demonstrates superior capability to detect errors arising from a wide range of FSI policies, consistently surpassing strong baselines. Moreover, we conduct detailed ablations and a pilot study on error correction to validate the effectiveness of the proposed architecture design and the practicality of the AED task, respectively. The AED project page can be found at https://aed-neurips.github.io/.

## 1 Introduction

Few-shot imitation (FSI), a framework that learns a policy in novel (unseen) environments from a few demonstrations, has recently drawn significant attention in the community . Notably, the framework, as exemplified by , has demonstrated its efficacy across a range of robotic manipulation tasks. This framework shows significant potential to adapt to a new task based on just a few demonstrations from their owners . However, a major barrier that still limits their ability to infiltrate our everyday lives is the ability to detect behavior errors in novel environments.

We propose a challenging and crucial task called adaptable error detection (AED), aiming to monitor FSI policies from visual observations and report their behavior errors, along with the corresponding benchmark. In this work, behavior errors refer to states that deviate from the demonstrated behavior, necessitating the timely termination of the policy upon their occurrence. Unlike existing few-shot visual perception tasks , failures can result in significant disruptions to surrounding objects and humans in the real world. This nature often restricts real-world experiments to simple tasks. Our AED benchmark is built within Coppeliasim  and Pyrep , encompassing six indoor tasks and one factory task. This comprehensive benchmark comprises 322 base and 153 new environments,spanning diverse domains and incorporating multiple stages. We aim to create a thorough evaluation platform for AED methodologies, ensuring their effectiveness before real-world deployment.

The AED task present three novel challenges, illustrated in Figure 1. First, AED entails monitoring a policy's behavior in novel environments, the normal states of which are not observed during training. Second, detecting behavior errors becomes challenging, as there are no noticeable changes to indicate when such errors occur. Specifically, this makes it difficult to discern either minor visual differences or misunderstandings of the task through adjacent frames. Third, AED requires online detection to terminate the policy timely, lacking complete temporal information of the rollout.

Current approaches struggle to address the unique challenges posed by AED. One-class classification (OCC) methods, including one-class SVMs  or autoencoders [16; 17; 18; 19], face difficulties in handling unseen environments. These methods are trained solely with normal samples and identify anomalies by evaluating their significant deviation from the learned distribution. However, normal states in novel environments, which include unseen backgrounds and objects, are already considered out-of-distribution for these techniques, resulting in subpar performance. Multiple instance learning  or patch-based methods [21; 22] may alleviate the second challenge, particularly minor visual differences, in seen environments. However, the feasibility of applying these methods to AED remains underexplored. Video few-shot anomaly detection (vFSAD) methods [21; 23; 24] are unsuitable for AED due to their dependency on full temporal observation from videos.

To this end, we introduce Pattern Observer (PrObe), a novel algorithm that extracts discriminative features from the monitored policy to identify instances of behavior errors. Specifically, PrObe designs a gating mechanism to extract task-relevant features from the monitored FSI policy to mitigate the impact of a novel environment (**first challenge**). Then, we design an effective loss function to distill sparse pattern features, making it easier to observe changes in observation (**second challenge**). Additionally, we design a recurrent generator to generate a pattern flow of current policy. Finally, we determine if there is a behavior error by proposing a novel temporal-aware contrastive objective to compare the pattern flow and demonstrations (**third challenge**).

We conduct thorough experiments on the proposed benchmark. Even when faced with various policy behaviors and different characteristics of strong baselines, our PrObe still achieved the highest Top 1 counts, average ranking, and average performance difference (with a maximum difference of up to 40\(\%\)), demonstrating its superiority. Additionally, we conducted an extensive ablative study to justify the effectiveness of our design choices. Furthermore, we reported additional experimental results covering timing accuracy, embedding visualization, demonstration quality, viewpoint changes, and **error correction** to validate our claims and the practicality of our task and method.

ContributionsOur work makes three significant contributions: (1) We define a vital yet under-explored task called Adaptable Error Detection (AED) and develop its associated benchmark to facilitate collective exploration by the research community. (2) We introduce PrObe, a novel method

Figure 1: Our novel adaptable error detection (AED) task. To monitor the behavior of the few-shot imitation (FSI) policy \(_{}\), the adaptable error detector needs to address three challenges: (1) it works in novel environments, (2) no notable changes reveal when behavior errors occur, and (3) it requires online detection. These challenges make existing error detection methods infeasible.

that monitors the policy's behavior by retrieving patterns from its feature embeddings. (3) We conduct thorough evaluations on the proposed benchmark, demonstrating the effectiveness of PrObe. It surpasses several baselines and shows robustness across different policies. We anticipate that our research will serve as a key foundation for future real-world experiments in the field of FSI research.

## 2 Related Work

### Few-shot Imitation (FSI)

PolicyWith the recent progress in meta-learning , the community explores the paradigm for learning policies from limited demonstrations during inference [26; 27; 28]. Notably, these works either assume that the agent and expert have the same configuration or explicitly learn a motion mapping between them . Conversely, DC methods [1; 11; 29] develop a policy that behaves conditioned both on the current state and demonstrations. Furthermore, they implicitly learn the mapping between agents and experts, making fine-tuning optional. Thereby, effectively extracting knowledge from demonstrations becomes the most critical matter. In most FSI works, no environment interactions are allowed before inference. Hence, policies are usually trained by behavior cloning (BC) objectives, i.e., learning the likelihood of expert actions by giving expert observations. Recently, DCRL  trains a model using reinforcement learning (RL) objects and performs FSI tasks without interacting with novel environments.

Evaluation tasksSince humans can perform complex long-horizon tasks after watching a few demonstrations, FSI studies continuously pursue solving long-horizon tasks to verify if machines can achieve the same level. A set of research applies policy on a robot arm to perform daily life tasks in the real world  or simulation . The task is usually multi-stage and composed of primitives/skills/stages [11; 2], such as a typical pick-and-place task  or multiple boxes stacking . Besides, MoMaRT  tackles a challenging mobile robot task in a kitchen scene.

Our work formulates the AED task for the safety concern of FSI policies and proposes PrObe to address it, which is valuable for extensive FSI research. Besides, we have also built challenging FSI tasks containing attributes such as scenes from different fields, realistic simulation, task distractors, and various robot behaviors

### Few-shot Anomaly Detection (FSAD)

Problem settingMost existing FSAD studies deal with anomalies in images [22; 30; 31; 32; 33] and a few tackle anomalies in videos [34; 35; 36; 24]. Moreover, problem settings are diverse. Some works presume only normal data are given during training [30; 33; 34; 35], while others train models with normal and a few anomaly data and include unknown anomaly classes during inference [31; 36].

Method summaryStudies that only use normal training samples usually develop a reconstruction-based model with auxiliary objectives, such as meta-learning , optical flow , and adversarial training [30; 35]. Besides, patch-based methods [31; 22] reveal the performance superiority on main image FSAD benchmark  since the anomaly are tiny defeats. Regarding video few-shot anomaly detection (vFSAD), existing works access a complete video to compute the temporal information for determining if it contains anomalies. In addition, vFSAD benchmarks [38; 39] provide frame-level labels to evaluate the accuracy of locating anomalies in videos.

Comparison between vFSAD and AED taskAlthough both the vFSAD and our AED task require methods to perform in unseen environments, there are differences: (1) The anomalies in vFSAD involve common objects, while AED methods monitor the policy's behavior errors. (2) An anomaly in vFSAD results in a notable change in the perception field, such as a cyclist suddenly appearing on the sidewalk . However, no notable change is evident when a behavior error occurs in AED. (3) The whole video is accessible in vFSAD [21; 23; 24], allowing for the leverage of its statistical information. In contrast, AED requires online detection to terminate the policy timely, lacking the complete temporal information of the rollout. These differences make our AED task more challenging and also render vFSAD methods infeasible.

Preliminaries

Few-shot imitation (FSI)FSI is a framework worthy of attention, accelerating the development of various robot applications. Following , a FSI task is associated with a set of base environments \(E^{b}\) and novel environments \(E^{n}\). In each novel environment \(e^{n} E^{n}\), a few demonstrations \(^{n}\) are given. The objective is to seek a policy \(\) that achieves the best performance (e.g., success rate) in novel environments leveraging a few demonstrations. Note that, a task in base and novel environments are semantically similar, but their backgrounds and interactive objects are disjoint. The framework takes as input \(N\) demonstrations (collected by a RGB-D camera) and an RGB-D image of the current observation, following the setting of [10; 11]. Addressing FSI tasks typically involves three challenges in practice : (1) The task is long-horizon and multi-stage. (2) The demonstrations are length-variant, making each step misaligned, and (3) The expert and agent have a distinct appearance or configuration. Developing a policy to solve the FSI task and simultaneously tackle these challenges is crucial.

Demonstration-conditioned (DC) policyAs stated above, the expert and agent usually have different appearances or configurations. The DC policy \((a s,)\) learns an implicit mapping using current states \(s\) and demonstrations \(\) to compute agent actions \(a\). Next, we present the unified architecture of DC policies and how they produce the action. When the observations \(o\) and demonstrations are RGB-D images that only provide partial information, we assume that the current history \(h_{t}(o_{1},o_{2},...,o_{t})\) and demonstrations \(\) are adopted as inputs.

A DC policy comprises a feature encoder, a task-embedding network, and an actor. After receiving the rollout history \(h\), the feature encoder extracts the history features \(f_{h}\). Meanwhile, the feature encoder also extracts the demonstration features. Then, the task-embedding network computes the task-embedding \(f_{}\) to retrieve task guidance. Notably, the lengths of agent rollout and demonstrations can vary. The task-embedding network is expected to handle length-variant sequences by padding frames to a prefixed length or applying attention mechanisms. Afterward, the actor predicts the action for the latest observation, conditioned on the history features \(f_{h}\) and task-embedding \(f_{}\). Additionally, an optional inverse dynamics module predicts the action between consecutive observations to improve the policy's understanding of how actions affect environmental changes. At last, the predicted actions are optimized by the negative log-likelihood or regression objectives (MSE).

## 4 Adaptable Error Detection (AED)

Our AED task is formulated to monitor and report FSI policies' behavior errors, i.e., states of policy rollouts that are inconsistent with demonstrations. The challenges posed by the AED task have been presented in Figure 1. We formally state the task and describe the protocol we utilized below.

Task statementLet \(c\) denote the category of agent's behavior status, where \(c,=\{,\}\). When the agent with the trained FSI policy \(_{}\) performs in a novel environment \(e^{n}\), an adaptable error detector \(\) can access the agent's rollout history \(h\) and a few expert demonstrations \(^{n}\). It then predicts the categorical probability \(\) of the behavior status for the latest state in the history \(h\) by \(=(h,^{n})=P(c enc(h,^{n}))\), where \(enc\) denotes the feature encoder, and it may be \(enc_{}\) (\(\) contains its own encoder) or \(enc_{_{}}\) (based on policy's encoder). Next, let \(y\) represent the ground truth probability, we evaluate \(\) via the expectation of detection accuracy over agent rollouts \(X^{n}\) in all novel environments \(E^{n}\):

\[_{e^{n} E^{n}}_{e^{n},_{}( ,^{n})}_{h X^{n}}A(,y),\] (1)

where \(A(,)\) is the accuracy function that returns 1 if \(\) is consistent with \(y\) and 0 otherwise. However, frame-level labels are often lacking in novel environments due to the established assumption [8; 11] that we have less control over them. Therefore, we employ a sequence-level \(A(,)\) in our experiments.

ProtocolWe explain the utilized protocol (shown in Figure 2) with a practical scenario: A company develops a home robot assistant. This robot can perform a set of everyday missions in customers' houses (novel environments \(E^{n}\)), given a few demonstrations \(^{n}\) from the customer. Before selling, the robot is trained in the base environments \(E^{b}\) built by the company. In this scenario, both the agent's and expert's optimal policies \(^{*}\) are available during training, with an established assumption from the FSI society [8; 10; 11] that base environments are highly controllable. Then, we can collect successful agent rollouts \(X^{b}_{succ}\) and a few expert demonstrations \(^{b}\) for all base environments 1. Next, we also collect failed agent rollouts \(X^{b}_{fail}\) by intervening in the agent's \(^{*}\) at a critical timing (e.g., the moment to grasp objects) so that \(X^{b}_{fail}\) can possess precise frame-level error labels.

With these resources, our utilized AED protocol consists of three phases: (1) The policy \(_{}\) is optimized using successful agent rollouts \(X^{b}_{succ}\) and a few demonstrations \(^{b}\). (2) The adaptable error detector \(\) is trained on agent rollouts \(X^{b}_{succ}\), \(X^{b}_{fail}\) and a few demonstrations \(^{b}\). Besides, the detector \(\) may use features extracted from policy \(_{}\)'s encoder, whose parameters are not updated in this phase. (3) The policy \(_{}\) solves the task leveraging few demonstrations \(^{n}\), and the detector \(\) monitors the policy's behavior simultaneously. Notably, no agent rollouts are collected in this phase. Only a few demonstrations \(^{n}\) are available since the agent is now in novel environments \(E^{n}\).

## 5 Pattern Observer (PrObe)

OverviewTo address the AED task, we develop a rollout augmentation approach and a tailored AED method. The rollout augmentation aims to increase the diversity of collected rollouts and prevent AED methods from being overly sensitive to subtle differences in rollouts. Regarding the AED method, our insight is to detect behavior errors from policy features that contain task-related knowledge, rather than independently training an encoder to judge from visual observations alone. Thus, we propose Pattern Observer (PrObe), which discovers the unexpressed patterns in the policy features extracted from frames of successful or failed states. Even if the visual inputs vary during inference, PrObe leverages the pattern flow and a consistency comparison to effectively alleviate the challenges posed by the AED task.

### Rollout Augmentation

To ensure a balanced number of successful and failed rollouts (i.e., \(X^{b}_{succ}\) and \(X^{b}_{fail}\)), along with precise frame-level labels, we gather them using the agent's optimal policy \(^{*}\) (with intervention). However, even if the policy \(_{}\) is trained on \(X^{b}_{succ}\), it inevitably diverges from the agent's optimal policy \(^{*}\) due to limited rollout diversity. Therefore, AED methods trained solely on the raw collected agent rollouts will be too sensitive to any subtle differences in the trained policy's rollouts, leading to high false positive rates.

Figure 2: Our AED protocol. the successful agent rollouts \(X^{b}_{succ}\), failed agent rollouts \(X^{b}_{fail}\), and a few expert demonstrations \(^{b}\) are available for all base environments \(E^{b}\). Then, the task contains three phases: policy training, AED training, and AED inference. We aim to train an adaptable error detector \(\) to report policy \(_{}\)’s behavior errors when performing in novel environments \(E^{n}\).

Accordingly, we augment agent rollouts so as to increase their diversity and dilute the behavior discrepancy between trained \(_{}\) and agent's \(^{*}\). Specifically, we iterate through each frame and its label from sampled agent rollouts and randomly apply the following operations: _keep_, _drop_, _swap_, and _copy_, with probabilities of 0.3, 0.3, 0.2, and 0.2, respectively. This process injects distinct behaviors into the collected rollouts, such as speeding up (interval frames dropped), slowing down (repetitive frames), and non-smooth movements (swapped frames). We demonstrate that this approach can contribute to AED methods, as shown in Figure 11 in the Appendix.

### PrObe Architecture

As depicted in Figure 3, PrObe comprises three major components: a pattern extractor, a pattern flow generator, and an embedding fusion. First, the pattern extractor take as input the history features \(f_{h}\) from the trained policy \(_{}\), aiming to retrieve discriminative information from each embedding of \(f_{h}\). Precisely, the pattern extractor transforms \(f_{h}\) into unit embeddings through division by its L2 norm, thus mitigating biases caused by changes in visual inputs (**first challenge**). Then, a learnable gate composed of a linear layer with a sigmoid function determines the importance of each embedding cell. A Hadamard product between the sign of unit embeddings and the importance scores, followed by instance normalization (IN), is applied to obtain the pattern features \(f_{p}\) for each timestep.

Second, PrObe feeds \(f_{p}\) into an LSTM (flow generator, **third challenge**) to generate the pattern flow. Intuitively, the changes in the pattern flow of successful and failed rollouts will differ. On the other hand, the task-embeddings \(f_{}\) extracted from \(_{}\) are transformed by an IN layer, a linear layer (FC), and a tanh function, mapping the task-embeddings into a space similar to the pattern flow.

Third, a fusion process concatenates the pattern flow and transformed task-embeddings to compute the error predictions \(\). This process is expected to compare the consistency between the agent rollout and demonstrations and uses this as a basis for determining whether behavior errors occur.

Objective FunctionsPrObe is optimized by one supervised and two unsupervised objectives. Firstly, a binary cross-entropy objective \(L_{cls}\) is employed to optimize the error prediction \(\), as frame-level labels \(y\) are available during training. Secondly, the L1 loss \(L_{pat}\) is applied to the pattern features \(f_{p}\), encouraging the pattern extractor to generate sparse pattern features, facilitating the observation of pattern changes (**second challenge**). Thirdly, a contrastive objective \(L_{tem}\), a temporal-aware variant of triplet loss , is developed and applied to the logit embeddings to emphasize the difference between the normal and error states in failed rollouts \(X^{b}_{fail}\) (**third challenge**). The idea behind \(L_{tem}\) is that adjacent frames' logits contain similar signals due to the temporal information from the pattern flow, even in the presence of errors (cf. Figure 7 in the Appendix). Blindly pushing

Figure 3: Architecture of PrObe. PrObe detects behavior errors through the pattern extracted from policy features. The learnable gated pattern extractor and flow generator (LSTM) compute the pattern flow of history features \(f_{h}\). Then, the fusion with transformed task-embeddings \(f_{}\) aims to compare the task consistency. PrObe predicts the behavior error based on the fused embeddings. Objectives, \(L_{pat}\), \(L_{tem}\), and \(L_{cls}\), optimize the corresponding outputs.

the logits of normal and error states far apart could mislead the model and have adverse effects. Therefore, \(L_{tem}\) considers the temporal relationship between samples and is calculated as follows:

\[L_{tem}=_{i=0}^{N}(\|_{i}^{a}-_{i }^{p}\|_{2}-\|_{i}^{a}-_{i}^{n}\|_{2}+m_{t},0).\] (2)

Here, \(N\) is the number of sampled pairs, and the temporal-aware margin \(m_{t}=m*(1.0-*(d_{ap}-d_{an}))\) adjusts based on the temporal distance of anchor, positive and negative samples. \(m\) represents the margin in the original triplet loss, while \(d_{ap}\), \(d_{an}\) are the clipped temporal distances between the anchor and positive sample, and the anchor and negative sample, respectively. Ultimately, PrObe is optimized through a weighted combination of these three objectives.

## 6 Experiments

Our experiments seek to address the following questions: (1) Is it better to solve the AED task by analyzing discernible patterns in policy features rather than using independent encoders to extract features from visual observations? (2) How do our architecture designs contribute, and how do they perform in various practical situations? (3) Can our AED task be effectively integrated with error correction tasks to provide greater overall contribution?

### Experimental Settings

Evaluation tasksExisting manipulation benchmarks [41; 42; 43; 44] cannot be used to evaluate the AED task because they do not effectively represent all challenges posed by the AED task, such as cross-environment training and deployment. Therefore, we have designed seven cross-domain and multi-stage robot manipulation tasks that encompass both the challenges encountered in FSI  and our AED task. Detailed task information is provided in **Section D of the Appendix**, including mission descriptions, schematics, configurations, and possible behavioral errors. Our tasks are developed using Coppeliasim , with Pyrep  serving as the coding interface.

FSI policiesTo assess the ability of AED methods to handle various policy behaviors, we implement three demonstration-conditioned (DC) policies to perform FSI tasks, following the descriptions in Section 3 and utilizing the same feature extractor and actor architecture. The primary difference among these policies lies in how they extract task embeddings from expert demonstrations. NaiveDC 2 concatenates the first and last frames of demonstrations and averages their embeddings to obtain task-embeddings; DCT 3 employs cross-demonstration attention and fuses them at the time dimension; SCAN  computes tasks-embeddings using stage-conscious attention to identify critical frames in demonstrations. More details can be found in **Section A of the Appendix**.

BaselinesIn our newly proposed AED task, we compare PrObe with four strong baselines, each possessing different characteristics, as detailed in **Section C of the Appendix**. Unlike PrObe, all baselines incorporate their own encoder to distinguish errors. We describe their strategies for detecting errors here: (1) **SVDDED**: A deep one-class SVM  determines whether the current observation deviates from the centroid of demonstration frames (_single frame, OOD_). (2) **TaskEmbED**: A model  distinguishes the consistency between the current observation and demonstrations (_single frame, metric-learning_). (3) **LSTMED**: A deep LSTM  predicts errors solely based on current rollout history (_temporal_). (4) **DCTED**: A deep transformer  distinguishes the consistency between the current rollout history and demonstrations (_temporal, metric-learning_).

MetricsWe report _the area under the receiver operating characteristic_ (AUROC) and _the area under the precision-recall curve_ (AUPRC), two conventional threshold-independent metrics in error/anomaly detection literature [46; 47]. To compute these scores, we linearly select 5000 thresholds spaced from 0 to 1 (or SVDDED's outputs) and apply a sequence-level accuracy function (cf. **Section E.1 in the Appendix**). Furthermore, we conduct an evaluation to verify if AED methods can identify behavior errors timely, as depicted in Figure 5.

### Analysis of Experimental Results

Main experiment - detecting policies' behavior errorsOur main experiment aims to verify whether discerning policies' behavior errors from their features is an effective strategy. We follow the protocol in Figure 2 to conduct the experiment. Specifically, the FSI policies perform all seven tasks, and their rollouts are collected. Then, the AED methods infer the error predictions for these rollouts. Lastly, we report two metrics based on their inference results. Notably, the results of policy rollout are often biased (e.g., many successes or failures). Thus, AUPRC provides more meaningful insights in this experiment because it is less affected by the class imbalance of outcomes.

Due to the diverse nature of policy behaviors and the varying characteristics of baselines, achieving consistently superior results across all tasks is highly challenging. Nonetheless, according to Figure 4, our PrObe achieved the highest Top 1 counts, average ranking, and average performance difference in both metrics, indicating PrObe's superiority among AED methods and its robustness across different policies. We attribute this to PrObe's design, which effectively addresses the challenges in the AED task. The average performance difference is the average performance gain compared to the worst method across cases, calculated as \((-)/\).

Additionally, we investigated cases where PrObe exhibited suboptimal performance, which can be grouped into two types: (1) Errors occurring at specific timings (e.g., at the end of the rollout) and identifiable without reference to the demonstrations (e.g., DCT policy in _Organize Table_). (2) Errors that are less obvious compared to the demonstrations, such as when the drawer is closed with a small gap (SCAN and DCT policies in _Close Drawer_). In these cases, the pattern flow does not change enough to be recognized, resulting in suboptimal results for PrObe.

Figure 4: Performance comparison of AED methods on seven challenging FSI tasks. The values under each policy indicate its success rate for each task. AUROC\([]\) and AUPRC\([]\) scores are listed in the upper and lower rows for each policy, respectively, ranging from 0 to 1. According to the statistics table, PrObe achieves the highest Top 1 counts (15 and 17 out of 21 cases), average ranking, and average performance difference in both metrics, demonstrating its superiority and robustness.

Timing accuracyOur main experiment examines whether AED methods can accurately identify behavioral errors when provided with complete policy rollouts. To further validate the timing of their predictions, we annotated a subset of rollouts (specifically the SCAN policy in _Pick \(\&\) Place_) and visualized the raw outputs of all AED methods. SVDDED outputs the embedding distance between observation and task-embedding, while others compute probabilities, as depicted in Figure 5.

In the successful rollout case, both our PrObe and LSTMED consistently identify states as normal, while other methods misidentify them, increasing the probability that inputs represent error states. Conversely, in the failed rollout case, PrObe detects errors after a brief delay (\(<\) 5 frames), significantly elevating the probability. We attribute this short delay to the time it takes for pattern flow to induce sufficient change. Nonetheless, PrObe detects errors with the closest timing; other methods either raise probabilities too early or too late. We emphasize that this phenomenon is common and not a deliberate choice.

Embedding visualizationTo analyze whether the learned embeddings possess discernible patterns, we initially extract the same 20 rollouts from the annotated dataset above using three AED methods to obtain the embeddings. Subsequently, we present the t-SNE transform  on these embeddings in Figure 6. Obviously, the embeddings learned by TaskEmbED and LSTMED are scattered and lack an explainable structure. In contrast, our PrObe's embeddings exhibit characteristics of task progress and homogeneous state clustering, i.e., the states with similar properties (e.g., the beginnings of rollouts, the same type of failures) are closer. This experiment supports the hypothesis that PrObe can learn implicit patterns from the policy's feature embeddings.

Ablations and supportive experimentsIn response to the second question, we summarize comprehensive ablations in the Appendix. First, Figure 11 indicates that the proposed **rollout augmentation** (**RA**) strategy increases the rollout diversity and benefits AED methods with temporal information. Second, Figure 12 demonstrates that **PrObe's design** effectively improves performance. Third, Figure 13 illustrates PrObe's **performance stability** by executing multiple experiment runs on a subset of tasks and computing the performance variance. Fourth, we examine how AED methods' performance is influenced when receiving **demonstrations with distinct qualities**, as presented in Table 7. Lastly, we study the influence of **viewpoint changes** in Table 8. Please refer to the corresponding paragraphs in the Appendix for exhaustive versions.

Figure 5: Visualization of timing accuracy. Raw probabilities and SVDDED outputs of selected successful (left) and failed (right) rollouts are drawn. PrObe raises the error at the accurate timing in the failed rollout and stably recognizes normal states in the successful case.

Figure 6: t-SNE visualization of learned embeddings (representations). The green and red circles represent normal and error states, respectively. The brightness of the circle indicates the rollout progress (from dark to bright). The embeddings learned by our PrObe have better interpretability because they exhibit task progress and homogeneous state clustering characteristics.

### Pilot Study on Error Correction

To further examine the practicality of our AED task (the third question), we conducted a pilot study on error correction. In this experiment, the FSI policy is paused after the AED methods detect an error. Then, a correction policy from , which resets the agent to a predefined safe pose, is applied. Finally, the FSI policy continues to complete the task. We conducted the study on the _Press Button_ task, where errors are most likely to occur and be corrected. Besides, the correction policy is defined as moving to the top center of the table. We allowed the DCT policy to cooperate with the correction policy and four AED methods (SVDDED excluded), as summarized in Table 1.

We have two following findings: (1) Our PrObe is verified to be the most accurate method once again. In contrast, other AED methods may cause errors at the wrong timing, making it challenging for the correction policy to improve performance (it may even have a negative impact on original successful trials). (2) The performance gain from the correction policy is minor, as it lacks precise guidance in unseen environments.

We believe that error correction in novel environments warrants a separate study due to its challenging nature, as observed in the pilot study. One potential solution is the human-in-the-loop correction, which operates through instructions [49; 50] or physical guidance . However, their generalization ability and cost when applying to our AED task need further discussion and verification. We will leave this as a topic for our future work.

## 7 Conclusion

We point out the importance of monitoring policy behavior errors to accelerate the development of FSI research. To this end, we formulate the novel adaptable error detection (AED) task, whose three challenges make previous error detection methods infeasible. To address AED, we propose the novel Pattern Observer (PrObe) by detecting errors in the space of policy features. With the extracted discernible patterns and additional task-related knowledge, PrObe effectively alleviates AED's challenges. It achieves the best performance, as confirmed by both our primary experiment and thorough ablation analyses. We also demonstrate PrObe's robustness in the timing accuracy experiment and the learned embedding visualization. Additionally, we provide a pilot study on error correction, revealing the practicality of the AED task. Our work is an essential cornerstone in developing future FSI research to conduct complex real-world experiments.

Limitations and future workWe carefully discuss the limitations of our work in Section F of the Appendix, including online AED inference and real-world experiments. For future work, developing a unified AED method applicable to various tasks and policies is worth exploring. Additionally, discovering previously unseen erroneous behaviors remains an interesting and challenging avenue.