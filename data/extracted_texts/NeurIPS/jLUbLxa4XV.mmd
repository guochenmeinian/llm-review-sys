# Certified Adversarial Robustness via Randomized \(\alpha\)-Smoothing for Regression Models

# Certified Adversarial Robustness via Randomized \(\)-Smoothing for Regression Models

 Aref Miri Rekavandi

University of Melbourne

aref.mirirekavandi@unimelb.edu.auFarhad Farokhi

University of Melbourne

farhad.farokhi@unimelb.edu.auOlga Ohrimenko

University of Melbourne

oohrimenko@unimelb.edu.auBenjamin I.P. Rubinstein

University of Melbourne

benjamin.rubinstein@unimelb.edu.au

###### Abstract

Certified adversarial robustness of large-scale deep networks has progressed substantially after the introduction of randomized smoothing. Deep net classifiers are now provably robust in their predictions against a large class of threat models, including \(_{1}\), \(_{2}\), and \(_{}\) norm-bounded attacks. Certified robustness analysis by randomized smoothing has not been performed for deep regression networks where the output variable is continuous and unbounded. In this paper, we extend the existing results for randomized smoothing into regression models using powerful tools from robust statistics, in particular, \(\)-trimming filter as the smoothing function. Adjusting the hyperparameter \(\) achieves a smooth trade-off between desired certified robustness and utility. For the first time, we propose a benchmark for certified robust regression in visual positioning systems using the _Cambridge Landmarks_ dataset where robustness analysis is essential for autonomous navigation of AI agents and self-driving cars. Code is publicly available at [https://github.com/arekavandi/Certified_adv_RRegression/](https://github.com/arekavandi/Certified_adv_RRegression/).

## 1 Introduction

Adversarial examples first swayed the narrative on deep models over a decade ago . Where deep nets had demonstrated remarkable generalization on classically challenging tasks , these small perturbations to an input sample that make no apparent change to the input's semantics or true class, yield high rates of misclassifications. While defenses had so far not tipped the balance away from the attacker, the combination of Certified Robustness (CR) with adversarial training has excited the security and ML communities recently . For a given model and input sample, CR can guarantee the absence of any adversarial examples in close vicinity of the sample. Randomized Smoothing (RS) is a widely used technique for CR as it scales to arbitrary large-scale models as it requires only black-box access to model evaluations . Despite attacks targeting ML tasks beyond classification , little is known of certification (or RS) for other standard ML tasks such as regression. Robustness analysis for regression has been examined through Lipschitz continuity  which is only feasible for small-scale regression models with full access to the models' parameters and certain types of activation functions.

In this paper, we present a framework for black-box certification of arbitrary regression models with either bounded or unbounded outputs. Our results extend the findings of  which considers the class of bounded-output regression models with large sample sizes in the evaluation stage. While they introduce averaging as an aggregation operator in randomized smoothing for regression, without limits on output range, averaged predictions may not be stable. This boundedness assumption, however,limits the applicability of their certificates. We present a superior approach through smoothing by \(\)-trimming filter. Complementing experiments with synthetic data, we benchmark our CR and RS approaches with the _Cambridge Landmarks_ dataset and DSAC\({}^{*}\) framework  for camera re-localization. The use of these benchmarks may be of independent interest for CR research.

## 2 Preliminaries

A base regression model parameterized with \(\) is denoted by \(_{}:^{d}^{t}\), where \(d\) and \(t\) are the input and output dimensions: \(_{}()\) maps input \(\) to multivariate output \(\). We will use \(_{}()\) to denote the smoothed version of \(_{}()\), as defined later. The neighborhood centered around point \(^{s}\) with radius \(\) with respect to a given dissimilarity function is denoted by \((,)\), where the dissimilarity function can be e.g., \(_{p}\) norms as well as functional divergences such as KL or Bregman when dealing with normalized \(\). In other words, for any \(^{s}\),

\[(,)=\{^{}^{s} (,^{})\}, \]

where \((.,.)\) in general, can be any metric or function that the user is interested, and \(_{x}\) (or \(_{y}\)) indicates dissimilarity in the input (or output) space. Throughout this paper, neighborhoods in input space are defined for all dimensions simultaneously as in Eq. (1), neighborhoods for outputs are analyzed separately using the neighboring function \(_{}(,_{y})=_{i=1}^{t}_{y}( _{i};_{y_{i}})\). The \(_{p}\)-norm (\(p 1\)) of a vector is denoted and defined as \(\|\|_{p}=(_{i}|_{i}|^{p})^{1/p}\) where \(_{i}\) indicates the \(i^{th}\) entry of \(\). We denote the multivariate normal distribution with mean \(\) and covariance \(^{2}\) as \((,^{2})\), where \(\) is the identity matrix. We denote the standard Gaussian CDF by \(()\). \(\{\}\) and \(\{\}\) denote the probability and expected value operators, respectively. Finally, \([\![t]\!]\) indicates the set \(\{1,2,,t\}\) and \([]\) rounds to the closest integer value.

**Threat model.** We consider a defender with only black-box access to model evaluations at test time: given an input point they may observe the output regressed value, but not model structure, gradients, parameters, or learning hyperparameters. On the other hand, we consider adversaries that have full information access to the model and certification algorithm. However, the attacker is limited to perturbing input data within small neighborhoods. This matches the typical threat model found in previous works on randomized smoothing in classification .

**Randomized smoothing.** Randomized smoothing is based on the ensemble of model outputs obtained over different perturbed inputs and is among the few techniques that are scalable to arbitrary, large models. Randomized smoothing was first adopted in seminal works of [6; 14; 12] for classification tasks to derive the maximum radii of input perturbations which maintain an invariant output prediction. In particular, in  it was shown that for the base classifier \(_{}()\), the new smoothed classifier \(g()=_{c}(f_{}(+ )=c)\), \((,^{2})\), is certifiably robust against any \(_{2}\)-norm-bounded adversary with radius \(=(^{-1}(})-^{-1}( }))\) where \(}}\) are any lower bound of majority class scores, and any upper bound of runner-up class scores, respectively. While randomized smoothing has mostly been explored for classification problems with categorical outputs [8; 30; 11; 31; 28; 13], more recently research has begun to consider regression [17; 20; 7; 4; 16]. These works all exhibit some limitation, including: universality, theoretical support, practical applicability, reliance on large sample sizes of data points, bounded outputs, or analysis through the lens of certifying classification. Further explanation on the differences among these studies can be found in Section 5.

## 3 Method

### Certified Regression

For many years, robustness analysis of regression models or estimators has been investigated using tools, such as Lipschitz continuity  or influence functions , where full access to the model parameters and its derivatives were required. Due to a lack of a proper definition of robustness in black-box access setup for large models, in contrast to classification problems, certified robustness has not been fully developed for regression problems. Until recently, motivated by probabilistic certified classification , the robustness definition for regression models has been introduced in , and some input perturbation bounds for base regression models, as well as their smoothed version with bounded outputs assumption using sample averaging, have been derived. In this paper, we extend these results for unbounded outputs in a small sample regime using a much more general form of smoothing function. We use the same definition of robustness in regression tasks given by:

**Definition 1**.: _(Probabilistic Robustness Certificate) . Given an example \((,)\), a (possibly) randomized regression function \(():^{d}^{t}\) is said to be certifiably robust with probability \(0 P 1\) in the randomness of \(\), with respect to the given input and output dissimilarity functions with radii \(_{x}\), \(_{y_{1}},,_{y_{t}}\), \(^{}^{}_{x}(,_{ x})\)_

\[_{i[\![t]\!]}_{y}((^{ })_{i},\!_{i})_{y_{i}}} P. \]

This definition of robustness in black-box functions requires first analyzing the robustness of the base regressor and then establishing theoretical results for smoothing functions applied to this base regression as a wrapper. Based on Definition 1, users can define a region for \(i^{th}\) continuous output variable by \(_{y}(_{i},_{y_{i}})\) or in other words \(\{z_{y}(z,_{i})_{y_{i}}\}\) as the accepted/valid region where the output prediction can fit in without being considered as a wrong prediction. The term accepted/valid region will be used in the rest of the paper to refer to the output neighborhood. This region is set by the user around \(f()\) to determine how much deviation is acceptable. For example, in camera- re-localization in a 3D scene with size \(100m 100m\), the user might reasonably accept up to 0.5m deviations in the predictions. The following result has been provided for the base regression function:

**Theorem 1**.: _(Certification of General Models Against \(_{2}\)-Bounded Attack) . Let \(_{}():^{d}^{t}\) be a (possibly) randomized base regressor and \((,^{2})\). Suppose for some example \((,)\),_

\[\{_{y}(_{}(+)_{i},\!_{i}) _{y_{i}}\}}}, i[\![t]\!] \]

_where \(}}\) is the lower bound on the probability of accepting prediction in the \(i^{th}\) output variable. Then referring to definition Eq. (2), \(_{}(+)\) is probabilistic certifiably robust at example \((,)\), for a \(_{2}\)-norm dissimilarity in the input, chosen probability \(P_{i[\![t]\!]}}}\), output radii \(_{y_{1}},,_{y_{t}}\) and input radius_

\[_{x}=_{i[\![t]\!]}^{-1}(}}) -^{-1}(P). \]

The above result indicates a strong similarity between the certification of regression models and classification task since both radii are proportional to \(\) and \(^{-1}(})\). It is worth investigating the equivalence of these two tasks in terms of robustness radius formulation, i.e., Eq. (4):

**Corollary 1**.: _(On the Equivalence of Regression and Classification) The certificate radius for a univariate regression model, i.e., \(_{}():^{d}\) using the robustness definition (2) with a user-chosen \(P=50\%\), is the same as the certified radius for a smoothed classifier made of \(_{}():^{d}\) as the base binary classifier and dividing the space into class A and class not A, where A denotes the case where the output is within the defined output neighborhood._

Proof.: Starting from (4) as the regression certificate bound and setting \(P=50\%\), the regression certificate radius becomes

\[_{x}=^{-1}(}), \]

which is exactly the radius derived for a binary classifier using the same smoothed function with the majority voting as stated in . 

This is an intuitive result stating that robustness in regression tasks can be reduced to the classification task when \(P\) is set to \(50\%\), where the output of a regression model is in the maximum uncertainty level. In this case, there is a tie between class \(A\) and not \(A\) and this is exactly where the output of a classification model might change and break the robustness definition of the given classifier. Now we extend the results in Theorem 1 to the \(_{p}\) attack with proof in Appendix A.

**Proposition 1**.: _(Certification of \(_{}()\) Against \(_{p}\) Attack). Let \(_{}():^{d}^{t}\) be a (possibly) randomized base regressor and \((,^{2})\). Suppose_

\[\{_{y}(_{}(+)_{i},\!_{i}) _{y_{i}}\}}}, i[\![t]\!], \]

_where \(}}\) is the lower bound on the probability of accepting prediction in \(i^{th}\) output variable. Then using (2) as the definition of certified robustness, \(_{}(++)\), \(\|\|_{p}_{x}\) (\(p 2\)) is within the accepted region, with the user-defined probability \(P}}, i[\![t]\!]\), where_

\[_{x}=_{i[\![t]\!]}-}} ^{-1}(}})-^{-1}(P). \]It is worth pointing out that the upper bound of input perturbation in (7) is obtained by Gaussian smoothing, meaning that we only evaluate the model once with Gaussian smoothing, but the guarantee is valid for any \(_{p}\) attack (\(p 2\)).

### Smoothing in Regression

Definition of the smoothing function in the classification task, i.e., \(g()=_{c}(f(+)=c)\), \((,^{2})\), is not feasible for application to regression and requires adjustments. One immediate change can be the use of an average function to compute \(()\), i.e., \(()=\{_{}(+)\}\), \((,^{2})\) or its Monte Carlo estimation. However, as shown in , even a single adversarial point (in unbounded scenarios) can entirely shift the result of averaging into the invalid zone (from the user's perspective). This behavior is known as the zero breakdown point of averaging in robust statistics . To deal with such a worst-case scenario, in  the outputs of regression models were assumed to be bounded. Although this assumption helped to derive certificate bounds around the input, it was shown for some cases where these considered bounds in the output are loose, the certificate bound in the input becomes worse than the base regression model. This motivates use of a better smoothing function that can also tolerate unbounded outputs. Therefore, we use the \(\)-trimming filter  to estimate the continuous output variable. Suppose that \(F\) is a finite set of N numbers (sorted in ascending order). The \(\)-trimmed mean of F is obtained by removing \(\) fraction of the samples (\(0<0.5\)) from the high and low ends of the sorted set, and computing the average of the remaining values. In the extreme cases, when \(\), \(\)-trimming is equivalent to median filtering , and when \(=0\) it reduces to classical averaging . In other words, we use the following _general_ form of smoothing function denoted by \(_{}()\):

\[_{}()_{i}=_{k=[ n] +1}^{n-[ n]}_{}^{k}(+)_{i}, i [\![t]\!](,^{2} ) \]

where \(n\) is the sampling size, \(_{}(+)_{i}\) is the sorted form of \(_{}(+)_{i}\), and \(k\) is the index of the order statistics. In the above, we draw \(n\) realization from \(\) to construct the set values. Here \(\) is a hyperparameter and can be tuned based on the user-chosen value \(P\) or level of smoothing. One of the primary uses of the \(\)-trimming filter is in data preprocessing and outlier rejection. The adjustment of \(\) in this context typically relies on prior knowledge about the proportion of data points that deviate from the nominal distribution . While this prior knowledge may not always be accurate, it has been widely utilized to reduce the sensitivity of estimators. Another method for tuning \(\) in parameter estimation is to consider efficiency at the nominal density. For instance, if no outliers are present in the dataset, one may set \(\) to achieve an estimation that closely matches the performance of its maximum likelihood counterpart .

Figure 1: General schematic of how \(\)-trimming can be applied to the base regressor with \(_{2}\)-norm ball (can be any \(_{p}\) norm in this paper and can be any neighboring function in general) defined for input vicinity and any form of convex (in this paper) or nonconvex (in general) set for the output vicinity. Furthermore, outputs can be examined separately (in this paper) or jointly with other outputs (in the general case as denoted by \(_{1:k}\) and \(_{m:t}\)).

### Certified Regression Against \(_{p}\) Attack

In this section, we use \(\)-trim smoothing function \(_{}()\) as a wrapper around the base regression model using the same definition of the vicinity sets in both input and output as shown in Figure 1. Consequently, everything within the red dashed box is related to base regression certification. The corresponding output regions outside the red dashed box represent the certification analysis for the smoothed function with the accepted regions remaining consistent with those in the base regression model. Note that for the following results (proof in Appendix B), the neighborhood in the output can be in any convex region, but the input perturbation is only considered to be within a \(_{p}\) ball around \(\).

**Theorem 2**.: _(Certification of \(_{}()\) for fixed range \(_{p}\) Attack). Let \(_{}():^{d}^{t}\) be a (possibly) randomized base regressor, and let \(_{}()\) be the one defined in (8) with \(0 1/2\). Let us assume the accepted region (set) for each output target variable is convex and the following inequality holds for \((,^{2})\), a user-defined \(_{y}\) and \(\|\|_{p}_{x}\) (\(p 2\)) and \( i t\):_

\[\{_{y}(_{}(++)_{i},_{i})_{y_{i}}\} q, \]

_where \(0 q 1\), then \( n>0\) we have_

\[\{_{y}(_{}(+)_{i}, _{i})_{y_{i}}\} I_{q}(n-[ n],[ n]+1), i  t \]

_where \(I_{q}(a,b)\) is the regularized incomplete beta function defined as \(I_{q}(a,b)=_{0}^{q}t^{a-1}(1-t)^{b-1}dt\) and \(B(a,b)\) is the complete beta function._

Theorem 2 states that after applying the \(\)-trimming filter, the probability that the average prediction of the regression network satisfies the user-chosen constraint, in the worst case scenario changes from \(q\) to \(I_{q}(n-[ n],[ n]+1)\).

The result in Theorem 2 is general and several intuitive and interesting cases are worth mentioning, (i) in the case where \(q=0\), regardless of the number of samples and the value of trimming parameter \(\), \(I_{0}(n-[ n],[ n]+1)=0\), meaning that when all the generated outputs by the base regression model are outside the accepted region, there is no chance that \(\)-trimming can generate a valid result. (ii) When \(q 1\), \(I_{q}(n-[ n],[ n]+1) 1\), meaning that when almost all of the generated outputs are valid, the \(\)-trimming approach with probability \(1\) returns a valid result regardless of the values of \(\) and \(n\). (iii) When \( 0\), \(I_{q}(n-[ n],[ n]+1) q^{n}\), meaning that if no trimming is applied, one single large invalid output can make the result invalid, then to get valid output, all the generated outputs by base regression model should be within the defined acceptable zone, where the chance of this event is \(q^{n}\). Note that \(q^{n}\) for \(=0\) is much lower than the result obtained in the case of bounded output for the same smoothing in  which indicates the strong impact of the output range on the provided certificate.

Figure 2: Improvement in the probability of observing predictions within defined accepted region using \(\)-trimming for different \(\) values.

For other values of \(\), it is important to note that enhancing the robustness of the base regression model is not always guaranteed unless a certain minimum amount of trimming is performed. In the following proposition, we investigate the certificate improvement of \(\)-trimming (with conservative \(\)) over the base regression model with proof in Appendix C.

**Proposition 2**.: _(Robustness Improvement via \(\)-trimming). For any \(n 1\), \(0 q 1\), if there exist an \(\) such that \(^{+}<1/2\), where_

\[^{+}:=^{-1}(q)-1/2}{n}, \]

_and where \(I_{q}^{-1}(q)\) is the inverse of \(I_{q}(n-x,x+1)\) with respect to \(x\), then the following inequality holds:_

\[I_{q}(n-[ n],[ n]+1) q. \]

Proposition 2 states that for sufficiently large \(\) values, the \(\)-trimming averaging technique, improves the probability of robustness and for larger values of \(\), the result gets better and better. As an example, suppose for a deep network that \(90\%\) of the generated outputs are valid, i.e., \(q=0.9\). Assuming drawing only \(50\) samples to obtain the certificate, then \(I_{0.9}^{-1}(0.9) 7.3\) and for any \(\) which meets \( 0.136\), the robustness probability, \(I_{0.9}(50-[50],[50]+1)\) is greater than \(q=0.9\). For instance, when \(=0.15\) and \(=0.2\), the robustness probability is \(0.94\) and \(0.99\), respectively. Intuitively, the obtained range of \(\) shows that to get improvement, \(\) should be at least slightly greater than \(1-q\) (\(0.1\) in our example) to be more confident that there will be less chance of observing some samples out of the accepted region defined by the user. Figure 2 shows two different models one with \(q=0.7\) and one with \(q=0.9\). After applying \(\)-trimming the obtained probability of validity in the results are shown in blue and orange colours, respectively. In both settings, \(^{+}\) values are demonstrated in vertical dashed lines and it can be observed that the success rate of the prediction (\(I_{q}(n-[ n],[ n]+1)\)) is always greater than the assumed \(q\) values for \(^{+}\). As described above, the corresponding \(^{+}\) values are slightly greater than \(1-q\) to ensure improvement even in worst-case scenarios. Now we must use the above results to propose the bound on \(_{p}\) norm (\(p 2\)) of the input perturbation for a user-given probability of success (P) when \(\)-trimming is in place (proof in Appendix D).

**Theorem 3**.: _(Certification of \(_{}()\) against \(_{p}\) Attack). Let \(_{}():^{d}^{t}\) be a deterministic or random base regressor and let \(n 1\), \(0<1/2\), \((,^{2})\) and suppose the \(\)-trimming function \(_{}()\) defined in (8) is used for smoothing. Then given_

\[\{_{y}(_{}(+)_{i},_{i}) _{y_{i}}\}}}, i[\![t]\!] \]

_where \(}}\) is the lower bound on the probability of accepting prediction in the \(i^{th}\) output variable, then \(_{}(+)\), \(\|\|_{p}_{x}\) (\(p 2\)) is within accepted region, i.e., \((,_{y})}=_{i=1}^{t}(_{i}, _{y_{i}})}\), with the user-defined probability \(P\), s.t. \(I_{n,}^{-1}(P)}}, i[\![t]\!]\), where_

\[_{x}=_{i[\![t]\!]}-}} ^{-1}(}})-^{-1}(I_{n,}^{-1}(P)), \]

_and where \(I_{n,}^{-1}(x)\) is the inverse of the regularized beta function w.r.t Bernoulli success rate parameter._The above result is valid for any regression model, either with bounded or unbounded outputs, and for large or small \(n\). Note that the performance of the base model always impacts the overall performance of the smoothed function, irrespective of whether the setting is classification or regression. In classification tasks, this strong relationship is reflected in the gap between \(}}\) and \(}\). If a base classifier performs poorly under input perturbations, this gap will shrink, diminishing the effectiveness of the final certification. In classification, there is only one other parameter (\(\)) that can potentially compensate for this gap. In regression settings, a similar relationship applies: better performance in the base regression model results in a better value for \(q\). However, as demonstrated in Theorem 3, the final performance is also influenced by \(\), \(n\) and importantly \(\). Using an appropriate value for \(\) can significantly enhance the certification even when the base regression model performs poorly. The step-by-step process of the method in prediction and certification is depicted in Algorithm 1.

## 4 Experiments

This section provides some numerical results to validate the proposed theorems as well as to show their effectiveness in some real-world problems. All simulations and experiments were conducted using an Intel(R) Core(TM) i7-9750H CPU running at 2.60GHz (with a base clock speed of 2.59GHz) and 16GB of RAM.

Synthetic Simulations.For the first part of the experiments, we utilized the function \(f()=10(2x_{1})+2(x_{2}-2)^{3}\). Figure 3 (left) illustrates this function for the interval \(-1<x_{1},x_{2}<5\). As certification is performed for each point individually, we derive the certified radii for this function at all the integer points in the considered intervals utilizing the formulas (4) and (14) with \(P=0.8\), \(=0.15\), \(_{y}=6\) using \(_{1}\) norm, \(n=10,000\) at two different rates of \(=0.35\) and \(=0.49\). Figure 3 (middle & right) illustrates these radii in a 2D grid for the base regression model as well as its smoothed versions against both \(_{2}\) and \(_{}\) norm attacks. As it can be observed, as expected, for smoother areas these radii become larger, and an increase in \(\) results in an increase in estimated certified radii as clear in Eq. (14). In addition to that, as expected, the certified radii become smaller as \(p\) increases in \(_{p}\) norm attacks. Now to examine the derived radii, for each point (25 points in total which are raster ordered) we randomly select an adversarial example within the defined radius (we repeat this process) and then find the empirical probability of obtaining valid outputs using the base model and its smoothed version utilizing \(\)-trimming filter (\(=0.35\)) with the same hyperparameters with only 5 samples fed into the \(\)-trimming filter. Figure 4 depicts the obtained empirical probabilities for both smoothed and base models in comparison to the required probability of valid outputs defined by the user (\(P=0.8\)). As shown, although the base regression model shows large variations in the empirical probabilities, with attacks in smaller neighborhoods, the \(\)-trimming filter uniformly improves the empirical probability across all the evaluated points (almost equal to 1) for both types of attacks, despite adversarial examples being drawn from larger neighborhoods.

Camera Re-localization Task.In this application, we evaluate the robustness of the state-of-the-art image-based camera re-localization technique "DSAC\({}^{*}\)"  against adversarial examples. In the

Figure 3: Adopted regression function (a) with the estimated certified radii (against \(_{2}\) and \(_{}\) attacks) for evaluated points in the center for both base and smoothed outputs (b & c).

camera re-localization pipeline, RGB images are fed into a trained system and the coordinates of the location where the camera was placed to take the images are predicted . For certifying such regression models, we use _Cambridge Landmarks_ dataset  and in particular 3 of the largest scenes in this popular dataset namely Great Court, King's College, and St. Mary Church. For computing certified error for any image, we used the same formulation as in , given by

\[e_{K} = \|_{}(+)-^{* }\|_{2}+_{r>_{_{x}}}K,\|\|_{ 2} r,\]

with \(K=150cm\), and \(=0.35\). For learning of \(p_{A}\) using Clopper-Pearson (\(=0.5\): \(75\%\) confidence), we used 100 samples and then we used \(n=\) per radius to examine \(_{2}\) attack. For each scene, the adopted parameters are selected differently to cover various experimental setups.

_Great Court:_\(P=0.8\), \(_{y}=5m\), output \(_{1}\) norm, \(=0.05\), and 760 images sized \(480 854\).

_King's College:_\(P=0.8\), \(_{y}=1m\), output \(_{1}\) norm, \(=0.08\), and 343 images sized \(480 854\).

_St. Mary Church:_\(P=0.9\), \(_{y}=5m\), output \(_{1}\) norm, \(=0.1\), and 530 images sized \(480 854\).

Figure 5 illustrates the proposed certified radii along the predicted trajectory of the camera in the 3 considered scenes as well as sample images with no/negligible certificates. From the top: Great Court, King's College, and St. Mary Church. While brighter dots represent points with higher certificates, we did not clip the radii from below and kept the negative values as they are. Large negative radii are indicators of high expectations of users either in considered valid regions or the expected probability of success. While most of the points are highlighted with bright colors, some points are dark and considered as sensitive images which can be easily misled with a small portion of manipulation. Interestingly, all the points which are not on the trajectory of the camera, have returned negative radii. On the other hand, Figure 6 illustrates the certified median error of the proposed methods for the three scenes in comparison with the results shown in  for bounded and discounted outputs for the Great Court scene (discount factor makes the accepted region wider than that of the base regression model, aiming for better analytical results in worst-case scenarios). Note that results in RS-Reg  were obtained with \(200\%\) discount in the output validity range, and we now provide almost the same results with no discount in the output, and results are valid for a small number of samples. As shown in these plots, the \(\)-trimming filter consistently decreased the certified median error (orange curve) across all input perturbation ranges (\(r\)) compared to the results obtained by the base regression model

(blue curve). The main reasons for this improvement are firstly, due to the better approximation of position parameters leveraging outlier removal and averaging using \(\)-trimming filter. Secondly, because of better certificate radii for each image in the scene which decreases penalization in the process of certified median error calculation. Leveraging the \(\)-trimming approach for smoothing, we are no longer worried about the output ranges, and no further assumptions such as large sample size or discount factor are required to provide a valid certificate. Sensitivity analysis of the proposed technique in this dataset can be found in Appendix E.

## 5 Related Work

Among the studies that adopted randomized smoothing for tasks other than classification, we can list smoothed embeddings  for few-shot learning models, certification of soft classifiers  where the output variables are continuous but bounded between 0 and 1, certification against poisoning

Figure 5: Evaluated scenes from the Cambridge Landmarks dataset (from the top row: Great Court, King’s College, and St. Mary Church). The middle column depicts the reconstructed 3d sparse point clouds using Structure-from-Motion (SfM) where the images are taken in red trajectories (predicted). The right column visualizes the derived certified radii for each image taken on these trajectories. For some images, they have shown robustness to input perturbation (bright points), and for some images, they have shown sensitive results (dark points). Examples of images with no/negligible certificates are provided in the left column. As shown, these images suffer from lighting conditions, improper perspective, and obstructed content.

attacks  which have different threat models or learning tasks than this study. The most related works to our study are [4; 17] where in the former study, the object detection was investigated through the lens of certified regression, however, their analysis is relying on the scaling output of classifier models to expand the range of output values which constrains the architecture of considered models, and in the latter the certification was provided for a class of bounded output regression model in the asymptotic case. Compared to these methods, our approach provides a probabilistic certificate for all regression models (including models with a wide range of outputs) with a limited number of evaluations through drawing noisy samples.

## 6 Conclusion

In this paper, we proposed the first probabilistic certificate against \(_{p}\) attack for all regression models with continuous output. We showed that the \(\)-trimming filter is an appropriate smoothing candidate in regression models with the flexibility to trade-off between error rate and certification radii. In addition to the comprehensive synthetic simulations, we adopted the proposed method in the camera re-localization task using the Cambridge Landmarks dataset and benchmarked the result for this new line of research. As future work, this technique can be extended to attacks in the semantic space of the input, and for sampling techniques other than Gaussian sampling to further tighten the certificate radii.

**Broader Impact** Adversarial examples demonstrate the vulnerability of many machine learning models to manipulation in contested environments. This paper considers defenses (via randomized smoothing) and robustness quantification (via robustness certificates), which are important approaches to improving resistance to attacks, and in highlighting the limitations of learned models to practitioners. As such, we believe this work has potential for positive societal benefit.

Figure 6: Certified median error in DSAC\({}^{*}\) as a function of \(r\). These plots are for (top-left) RS-Reg with \(200\%\) discount in Great Court scene (figure from ), (top-right) proposed method in Great Court scene, (bottom-left) proposed method in King’s College scene, and (bottom-right) proposed method in St. Mary Church scene. The parameters are \(=0.35\), \(n=10\), \(K=150cm\), and \(=0.5\).