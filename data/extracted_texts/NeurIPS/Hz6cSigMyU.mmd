# Reinforcing LLM Agents via

Policy Optimization with Action Decomposition

Muning Wen\({}^{1}\), Ziyu Wan\({}^{1}\), Jun Wang\({}^{2}\), Weinan Zhang\({}^{1,}\), Ying Wen\({}^{1,}\),

\({}^{1}\)Shanghai Jiao Tong University, \({}^{2}\)University College London

Corresponding to Ying Wen <ying.wen@sjtu.edu.cn>, Weinan Zhang <wnzhang@sjtu.edu.cn>. The source code could be accessed directly with this link https://github.com/morning9393/ADRL.

###### Abstract

Language models as intelligent agents push the boundaries of sequential decision-making agents but struggle with limited knowledge of environmental dynamics and exponentially huge action space. Recent efforts like GLAM and TWOSOME manually constrain the action space to a restricted subset and employ reinforcement learning to align agents' knowledge with specific environments. However, they overlook fine-grained credit assignments for intra-action tokens, which is essential for efficient language agent optimization, and rely on human's prior knowledge to restrict action space. This paper proposes decomposing language agent optimization from the action level to the token level, offering finer supervision for each intra-action token and manageable optimization complexity in environments with unrestricted action spaces. Beginning with the simplification of flattening all actions, we theoretically explore the discrepancies between action-level optimization and this naive token-level optimization. We then derive the Bellman backup with Action Decomposition (BAD) to integrate credit assignments for both intra-action and inter-action tokens, effectively eliminating the discrepancies. Implementing BAD within the PPO algorithm, we introduce Policy Optimization with Action Decomposition (POAD). POAD benefits from a finer-grained credit assignment process and lower optimization complexity, leading to enhanced learning efficiency and generalization abilities in aligning language agents with interactive environments. We validate POAD across diverse testbeds, with results affirming the advantages of our approach and the correctness of our theoretical analysis1.

## 1 Introduction

Large language models (LLMs) have demonstrated promising capabilities of solving various tasks, from instructions following to complex reasoning and real-world interaction [1; 2; 3]. This growing task-solving ability underscores their potential as intelligent language agents in interactive environments [4; 5]. However, despite in-context language generation aiding comprehension of environmental states and action spaces in sequential decision-making tasks, misalignment issues [4; 5] such as generating invalid actions and lacking knowledge of environmental dynamics hinder these agents' ability to complete decision-making tasks robustly and efficiently.

Recent advances [4; 5; 6; 7; 8] have showcased that the aforementioned challenges can be alleviated in a trial-and-error learning style, namely Reinforcement Learning (RL) . Representatively, GLAM  and TWOSOME  treat the language actions, i.e. token sequences outputted by a language model, as whole units, and optimize actions' likelihood, calculated as the products of conditional probabilities of intra-action tokens. Leveraging the chain rule of probability, they build the bridge between optimizing actions and optimizing tokens, aligning the training process of language models asnext-token predictors with the RL objective of maximizing actions' utilities. However, they still suffer from limitations in optimization and exploration efficiency, due to the uncertainty of credit assignment to intra-action tokens. As shown in Figure 1, when optimizing the distribution over three candidate actions that only differ from the last token, action-level policy optimization strategies in previous works cannot ensure that the probability of the key tokens, i.e. \((|p,)\) here, will be enhanced precisely when optimizing the joint probability \((|p)\). Furthermore, optimizing at the action level poses the challenge of overlarge optimization complexity due to exponentially growing action spaces, leading GLAM and TWOSOME to manually constrain action spaces. But they remain incapable in environments whose action spaces cannot be restricted.

A natural attempt to solve these problems is to incorporate the token generation process in each decision step as part of the sequential decision-making process . This approach resolves the credit assignment problem and reduces the growth of optimization complexity from multiplicative to additive as the number of intra-action tokens increases, by updating each token's output distribution separately with fine-grained signals from value backups. Empirical success in many single-step tasks, e.g. question answering  and alignment , have demonstrated its effectiveness. However, the multi-step nature of sequential decision-making tasks leads to extra difficulties in coordinating the credit assignment process across actions and their constituent tokens. In such scenarios, embedding the intra-action token generation process into the original Markov Decision Process (MDP)  results in a new MDP inconsistent with the original one. Intuitively speaking, it introduces an undesired assumption of "_in a sentence, tokens appearing later are more important for conveying meaning than those appearing earlier._" This assumption, however, is unrealistic due to the nature of linguistic actions. Such a significant discrepancy has been ignored in previous research and has not been tackled properly for a long time.

In this paper, we provide a comprehensive theoretical analysis of the discrepancy and derive the Bellman backup with Action Decomposition (BAD), which guarantees theoretical consistency with optimizing the original MDP. While being possible to seamlessly integrate with a variety of traditional RL methods, in this work, we apply BAD to Proximal Policy Optimization (PPO) , resulting in the formulation of Policy Optimization with Action Decomposition (POAD). Benefiting from the finer-grained supervision afforded by BAD, POAD mitigates the uncertainty in the credit assignment process described in Figure 1, thereby enjoying better interpretability, lower optimization complexity, and higher training efficiency. Meanwhile, it theoretically maintains consistency between the token-level training process for language models and the RL objective of maximizing actions' utilities. We justify our claims by evaluating POAD in both classical sequential decision-making environments with limited action space, i.e Overcooked and VirtualHome , and a self-constructed data science coding environment featuring an unrestricted action space, i.e. DataSciCoding; results verify POAD's advantages in performance and efficiency over baseline methods, highlighting the significance of BAD. Moreover, we empirically demonstrate that language agents trained with POAD exhibit excellent generalization ability across unseen tasks, without compromising the inherent functionalities of language models. Finally, ablation studies confirm the correctness of our theoretical insights.

Figure 1: A Case to demonstrate: (a) the necessity of aligning language agents with environments to exclude the wrong option, since the agent does not initially know that _“coffee table is empty”_. (b) Action-level optimization is uncertain to what extent the key tokens, i.e. \((|p,)\), will be enhanced when optimizing the joint probability \((|p)\).

Related Works

**Language-based Decision-Making Agents.** Leveraging LLMs' powerful capability and plenty of common knowledge, recent efforts successfully adapt LLMs in decision-making tasks as a policy model in interactive environments. In robotics, LLMs have been employed as high-level planners of control policies [16; 17; 18]. Similarly, LLMs work particularly well in text-based environments [19; 20]. ReAct  combines chain-of-thought reasoning  with acting-based prompt, efficiently solving hard decision-making tasks. Self-Refine  and Reflexion  further improve language agents' efficiency and robustness via online adaptation through the self-reflection process. In this work, we also apply language agents in sequential decision-making scenarios, i.e. interactive environments.

**Fine-tuning Language Models with RL.** A body of literature explores the prospects of leveraging strategic planning methodologies to enhance the performance of language agents [7; 24; 25; 6]. Besides, RL has also been widely applied in fine-tuning LLMs [10; 11; 7; 24; 25; 6]. Particularly, proximal policy optimization (PPO)  is the most commonly used RL method for reinforcement learning from human feedback (RLHF), proposing a breakthrough in aligning LLMs with human preference [10; 11]. In classical sequential decision-making scenarios, to align the objective of token-level optimization with action-level optimization, GLAM  and TWOSOME  estimate the probability of possible actions with the products of the conditional probability of tokens composing the actions and update the action as a whole. In this work, instead of treating an action as a whole, we attempt to decompose actions and explicitly assign precise credit to each intra-action token, while ensuring that its optimality is consistent with updates at the action level. While a concurrent work, ArCHer , also targets token-level supervision for LLMs in interactive environments, it employs a hierarchical RL framework, using a Q-network for action-level credit approximation and REINFORCE  for token-level backpropagation. However, ArCHer's use of multiple value networks (Q, V, and optional baseline networks) demands extensive manual tuning and can introduce cumulative bias and variance, potentially affecting stability.

**Sequential Decomposition in RL.** Recent years have witnessed an increasing trend of decomposing high-dimension actions to leverage the powerful modeling abilities of sequential models like Transformer  in RL problems [29; 30; 31; 32; 33; 34]. While Kuba et al. [35; 36] proposing a sequential decomposition method to provide finer-grained supervision for multi-agent joint actions, Multi-agent Transformer  inherits this idea and solves multi-agent RL problems with Transformer. More recently, Q-transformer  managed to decompose the Q-functions for high-dimensional actions by representing each action dimension as separate tokens. For language agents, the language generation process inherently conforms to the pattern of sequential decomposition, which offers a promising avenue for providing finer-grained supervision to intra-action tokens.

## 3 Preliminaries

### Language-augmented RL

In this work, we assume a textual RL setting that frames sequential decision-making problems with linguistic inputs and outputs as a language-augmented Markov Decision Process (MDP) \(=(,,,,R,)\)[37; 4]. Given \(\) the vocabulary and \(w\) the tokens, \(^{N}\), \(^{N}\) are the action space and state space respectively, i.e. actions and states are sequences of tokens. \(:\) is the state transition function. \(R:\) is the reward function that only responds to complete actions, and \(\) is the discounted factor that typically less than \(1\). At time step \(t\), a language agent receives a textual state \(s_{t}\) from an interactive environment as input and generates an action \(a_{t}\) in an auto-regressive manner, i.e. \(a_{t}=(w_{t}^{1},,w_{t}^{|a_{t}|})\) where \(|a_{t}|\) denotes the number of tokens in the action string and \(\{w_{t}^{i}\}_{i=1}^{|a_{t}|}\) are tokens in it. Then, textual action \(a_{t}\) will be grounded to a specific API call or command in the environment . After execution, the language agent receives a reward \(r_{t}=R(s_{t},a_{t})\) along with the next state \(s_{t+1}\), based on the transition function \(\). Following this process with trajectories of a maximum timestep \(T\), the agents earn a discounted cumulative return of \(R^{}=_{t=0}^{T}^{t}r_{t}\), which is aimed to be maximized by RL algorithms.

### Action-level Policy Optimization

We begin by briefly reviewing the process of action-level policy optimization which is widely adopted in several state-of-the-art methods that align language agents with environments via RL algorithms [4; 5; 39]. It facilitates seamless integration between any textual environment and conventional RL algorithms and thus is an ideal starting point for our analysis.

The possibly achieved episodic return following policy \(\) given action and state is usually evaluated by state-action value function \(Q_{}(s,a)\) or state value function \(V_{}(s)\). Then, a language agent updates its policy \(\) according to credits calculated on the value functions, defined as

\[Q_{}(s,a) _{s_{1:T,a_{1:T}}} R^{}|s_{0}=s,a_{0}=a,\] (1) \[V_{}(s) _{s_{1:T,a_{0:T}}} R^{}|s_{0}=s,\] (2)

where \(a=(w^{1},,w^{|a_{|t|}})=w^{1:|a|}\). While Ahn et al.  builds the connection between the likelihoods of actions and tokens through the chain rule of probability as

\[(a|s)=_{j=1}^{|a|}(w^{j}|s,w^{1:j-1}),\] (3)

recent approaches like GLAM  and TWOSOME  leverage similar ideas and optimize action-level likelihoods with RL methods directly. When considering optimizing for \((a|s)\), Equations 1 and 2 are aligned with the definition of the value function in traditional RL settings, allowing them to be updated with traditional Bellman backup 

\[Q_{}(s_{t},a_{t})  R(s_{t},a_{t})+_{a_{t+1}}Q_{}(s_{t+1},a_{t+1}),\] (4) \[V_{}(s_{t})  R(s_{t},a_{t})+ V_{}(s_{t+1}).\] (5)

Moreover, it is noteworthy that Equation 3 calculates the likelihood of action \(a\) in an exponentially growing language space as \(|a|\) increases, i.e. \(||=||^{|a|}\). Exploration and optimization in such a huge action space are typically intractable for RL methods. Therefore, in the settings of GLAM and TWOSOME, the feasible action space is significantly restricted and smaller than the entire language space, i.e. \(||||^{|a|}\). Taking TWOSOME as an example, it optimizes the likelihood of action \(a\) concerning the feasible action space with Equation 6 to mask out invalid outputs.

\[_{}(a|s)= }((a^{}|s)/L(a^{}))}.\] (6)

\(L(a)\) indicates the number of tokens or words in the action prompt, utilized as a normalization term to mitigate the effects of varying action lengths.

Underpinned by Equation 3, GLAM and TWOSOME ensure the consistency between token-level optimization for language models and action-level optimization in an RL manner, without the need to explicitly assign credits for intra-action tokens. However, the jointness of the objective causes difficulties associated with the uncertainty in the credit assignment process [40; 41]--as shown in Figure 1, after assigning credit to an action, it's unsure whether key tokens in this action have been identified, and how much they are influenced. Thus, conducting RL training at the action level introduces uncertainty, which may lead to an inefficient learning process for the language agent.

## 4 From Actions to Tokens: Naive Token-level Policy Optimization

### Naive Token-level Policy Optimization

To address the unclear credit assignment issue described in Figure 1 and Section 3.2, our target is to provide finer-grained supervision for each token during update while maintaining consistency in the optimality with action-level optimization, i.e. maximizing agents' cumulative returns. For arbitrary subsets of actions \(w^{1:j}\) with \(j|a|\), we define token value functions for supervising policy update as

\[Q_{}(s,w^{1:j-1},w^{j}) R^{}|s_{0}=s,w_{0}^{1:j-1}=w^{1: j-1},w_{0}^{j}=w^{j},\] (7) \[V_{}(s,w^{1:j-1}) R^{}|s_{0}=s,w_{0}^{1:j-1}=w^{1: j-1}.\] (8)

A natural approach to approximate \(Q_{}(s,w^{1:j-1},w^{j})\) and \(V_{}(s,w^{1:j-1})\) is conceptualizing the token generation process as part of the MDP, where each token is treated as a micro action. This enables

[MISSING_PAGE_FAIL:5]

## 5 Action-Decomposition Reinforcement Learning

### Bellman Backup with Action-Decomposition

According to the first insight, we can modify Equations 9 and 10, proposing the _Bellman backup with Action-Decomposition (BAD)_ as

\[Q_{}(s_{t},w_{t}^{1:j-1},w_{t}^{j}) _{w_{t}^{j+1}}Q_{}(s_{t},w_{t}^{1:j },w_{t}^{j+1}),&j<|a_{t}|\\ R(s_{t},a_{t})+_{w_{t+1}^{j}}Q_{}(s_{t+1},w_{t+1}^{1}),&j=|a_{t}| ,\] (13) \[V_{}(s_{t},w_{t}^{1:j}) V_{}(s_{t},w_{t}^{1:j+1}),&j<|a_{t}|\\ R(s_{t},a_{t})+ V_{}(s_{t+1},),&j=|a_{t}|.\] (14)

(For proof of optimization consistency see Appendix B.) Training language agents with BAD provides finer-grained supervision for credit backpropagation, eliminating uncertainty in credit assignment and thus enjoying better interpretability and efficiency of the RL training process. In addition, it theoretically ensures consistency between the token-level training process for language models and the RL objective of maximizing actions' utilities. Figure 2 visually compares the differences between action-level Bellman backup and our BAD and demonstrates how BAD precisely assigns credit to each token.

Another advantage of BAD is the ability to decompose the optimization in an intractable action space of size \(O(|V|^{|a|})\), into \(|a|\) times optimizations in token spaces of size \(O(|V|)\), reducing the complexity of RL problem with language agent to \(O(|a||V|)\), thus rendering the problem more manageable. Moreover, BAD can be seamlessly integrated into various existing RL methods, including off-policy algorithms e.g. DQN, on-policy ones like Actor-Critic and PPO. Furthermore, we have also provided a version of the Soft Q-function in Appendix B.3 to support various entropy-regularized RL algorithms like SAC[46; 47].

### Policy Optimization with Action Decomposition

In this section, we integrate the BAD into the widely used PPO and propose a specific method called _Policy Optimization with Action Decomposition (POAD)_, while the integration with other algorithms will be reserved for future works. POAD sequentially decomposes the policy update granularity from the action level to the token level. To approximate the token value function, we introduce a critic network with parameters \(\) whose objective is to minimize the empirical Bellman error of tokens by

\[L()=_{i=0}^{T-1}|} R(s_{t},a_{t})+ V_{}(s_{t+1},)-V_{ }(s_{t},w_{t}^{1:|a_{t}|})^{2}}_{}+a_{t}|^{-1}}_{}[V_{}(s_{t},w_{t}^{1:j})-V_{}(s_{t},w_{t}^{1:j})]^{2} ,\] (15)

Figure 2: Visual comparison of the differences between action-level Bellman backup (left) and our BAD (right), given the goal _turn on the TV_, where \(q\) is the action or token value estimations, \(_{t}=q_{t}-q_{t-1}\) or \(^{j}=q^{j}-q^{j-1}\) represent the credit assigned to corresponding actions and tokens respectively for policy update, e.g. the advantage value. To facilitate understanding, a step-by-step breakdown of the right figure is provided in Appendix L.

where \(\) represents the non-differentiable parameter of the target network, updated at intervals. The policy network's parameters are denoted as \(\), and optimization follows the clipping PPO objective.

\[L() =-_{t=0}^{T-1}|}_{j=1}^{|a_{t}|} ^{j}_{t}()^{j}_{t},( {ratio}^{j}_{t}(),1)^{j}_{t},\] (16) \[^{j}_{t}() =(w^{j}_{t}|s_{t},w^{1:j-1}_{t})}{_{_{old} }(w^{j}_{t}|s_{t},w^{1:j-1}_{t})},^{j}_{t}=(w^{j}_{t}|s_{t},w^{1:j-1}_{t}),\]

where \(^{j}_{t}\) is an estimate of the advantage value for each token with the _generalized advantage estimation_ (GAE) . To capture more details about POAD, we draw a pseudo-code in Appendix D.

## 6 Experiments

In this section, we show the superiority of POAD in performance, efficiency, and generalization abilities with different testbeds. Moreover, we conduct meaningful ablations on \(_{a}\) and \(_{w}\) to verify the theoretical analysis in Section 4.2. Finally, we examine models trained with POAD and baseline methods to investigate their impact on the model's original language abilities. For in-depth analysis, we conduct a case study in Appendix F to validate the effectiveness of BAD in terms of token-level credit assignment. We deploy LLaMA2-7B  for Overcooked and VirtualHome, and CodeLLaMA-7b  for DataSciCoding, fine-tuned with Low Rank Adaptation (LoRA)  with 1 Nvidia A100 GPU.

### Environmental Setup

We first evaluate our method on two classical sequential decision-making environments with limited action space: Overcooked  and VirtualHome , where the action space consists of approximately 10 possible actions per state, each includes 5-10 tokens. Then we evaluate our method in a data science coding and debugging environment with unrestricted action space: DataSciCoding, where agents generate actions (up to 128 tokens) freely. More detailed descriptions can be found in Appendix E.

**Overcooked and VirtualHome.** Overcooked challenges agents to prepare dishes such as _tomato salad_ and _tomato-lettuce salad_ in a 7x7 grid kitchen using linguistic actions like _Chop, Get-Tomato, and Go-Cutting-Board_, with rewards for correct deliveries and penalties for incorrect ones or time wastage. Meanwhile, VirtualHome simulates household tasks like reheating _pancakes_ in Food Preparation and organizing an evening of Entertainment, with actions such as _walk to the living room_ and _turn on the TV_, rewarding agents only upon task completion in a partially observable setting.

**DataSciCoding.** We develop DataSciCoding to automate data science coding tasks with unrestricted action space, currently adopting 3 Kaggle datasets and 3 OpenML datasets  with details in Appendix E.1. In each task, agents aim to implement the most effective classifier with the scikit-learn module, striving to achieve the highest possible ROC AUC score  on test sets. As the prompts provided to the agents contain no detailed information about task datasets, agents are required to interactively modify and debug their code based on feedback from the runtime environment until it works, thus aligning the task datasets. Agents receive ROC AUC scores \(\) as rewards for workable codes and \(-1\) as penalties for run failed. Adopting the same evaluation metrics as CAAFE , for each dataset and code, we evaluate 5 repetitions, each with a random \(50\%-50\%\) train-test split , and record the average ROC AUC score across these splits.

### Baseline Methods

For Overcooked and VirtualHome, we compare POAD's performance with Naive Token-Level Policy Optimization (NTPO) mentioned in Section 4.1, i.e. integrating Equation 10 with \(_{w}=_{a}\) into PPO, and TWOSOME --the current state-of-the-art method on Overcooked and VirtualHome. Besides, we also incorporate ArCHer  as a baseline in VirtualHome with comparative analysis, since it is positioned as an intermediate between NTPO and POAD for token-level credit assignment, theoretically enjoying reduced discrepancy compared to NTPO. We demonstrate the difference in the backup processes between POAD and these baselines as well as the optimality after convergence in Appendix C For the DataSciCoding environment, in case TWOSOME is inapplicable due to the unrestricted action space, we examine POAD's performance along with NTPO. Meanwhile, we also compare POAD with CAAFE --a novel AutoML framework in which humans collaborate with large language models for data science. In CAAFE, humans implement machine learning models, e.g. classifiers, while large language models generate the code for feature engineering. CAAFE represents the current state-of-the-art performance with collaboration between humans and powerful closed-source LLMs such as GPT-3.5 and GPT-4 on the given task datasets we used.

### Main Results

#### 6.3.1 Classical Sequential Decision-Making Tasks with Restricted Action Space

As shown in Figure 3, where curves are averaged over 3 seeds with the shadow showing their standard deviation, the performance drop of NTPO when comparing with POAD verifies the existence and negative impact of the discrepancy analyzed in Section 4.2. While POAD can achieve the same (or even better) convergence performance compared to TWOSOME which further verifies the consistency between token-level optimization with BAD and action-level optimization. In addition, the training curves of POAD are more stable (enjoying smaller standard deviations) than TWOSOME and converge much faster than all other baselines, indicating POAD's stability and efficiency by integrating our BAD. In complex Entertainment tasks, ArCHer is second only to POAD, aligning with our theoretical expectations. However, in tasks such as Food Preparation where the methods' performance gap was less pronounced, ArCHer performed poorly, potentially due to instability in its system that involved multiple value networks.

#### 6.3.2 Data Science Coding Tasks with Unrestricted Action Space

According to the training curves in Figure 4, our method POAD significantly outperforms NTPO both in the convergence speed and the final score. Compared to the results on sequential decision-making tasks in Section 6.3.1, the performance gap between POAD and NTPO on DataSciCoding is consistently larger, due to the much longer action length \(|a_{t}|\), at most 128 tokens for each action in DataSciCoding. Such results are consistent with our second insights in Section 4.2 and empirically highlight the importance of a proper intra-action credit assignment, which, our Bellman Backup with Action Decomposition (BAD) is designed for.

In Figure 4, POAD-Best means the performance of the best code discovered during POAD's training process with CodeLLaMA-7B. We compare it with the best performance achieved by the state-of-the-art AutoML framework CAAFE  with GPT-4 model. In this experiment, we aim to prove that even small-scale language models can also provide better outcomes than large-scale models, what is needed is just a stable and efficient training algorithm POAD and only 2-3 hours on Nvidia A100 (Details of wall-time on each task are shown in Appendix H). A more detailed Comparison between POAD and CAAFE with both GPT-3.5 and GPT-4 can be found in Appendix G.

### Open-Vocabulary Task Generalization

LLMs' open-vocabulary feature enables language agents to transfer their learned skills into unseen similar tasks, expanding the capabilities of decision-making agents. We compare the generalization performance of language agents trained by POAD, TWOSOME and NTPO in the _Food Preparation_ task with the original base model LLaMA2-7B. Table 1 shows that token-level policy optimization methods achieve better generalization performance in unseen tasks. And our POAD outperforms the other baselines in seven of eight tasks.

Figure 3: Performance comparisons on Overcooked (first two) and VirtualHome (last two).

### Ablations: the Impact of \(_{a}\) and \(_{w}\) to the Discrepancy

To verify our analysis in Section 4.2, we conduct ablations on \(_{w}\) to further investigate how large the discrepancy would be by using different values of \(_{w}\) in NTPO. Therefore, we deploy NTPO on the _Food Preparation_ task with \(_{w}\{0.95,0.9,0.8,0.5\}\), while we keep \(_{a}=0.95\) which is consistent with our main experiments. Besides, we also show the performance with \(_{a}\{1.0,0.95\}\) to show the necessity of setting \(_{a}\) strictly less than \(1.0\), and thus the necessity to separate inter-action tokens and intra-action tokens. Further, our theoretical analysis is also compatible with ArCHer, motivating us to apply insights in Section 4.2 to enhance ArCHer's performance.

   Methods & Cheese & Hamburger & Apple Pie & Pizza & Washing Plate & Laundry \\  LLaMA2-7B & 0.1351 & 0.1342 & 0.1656 & 0.1409 & 0.0527 & 0.0344 \\ TWOSOME & 0.7119 & 0.7058 & 0.7304 & 0.7047 & 0.7031 & 0.6038 \\ NTPO & 0.7428 & 0.7476 & 0.7141 & 0.7355 & **0.7491** & 0.5687 \\ POAD & **0.7553** & **0.7602** & **0.7650** & **0.7625** & 0.7075 & **0.7014** \\   

Table 1: Comparison of generalization performance on eight unseen tasks, with episodic returns averaged over 100 episodes. In these tasks, we replace the _pancake_ in the original Food Preparation task with other foods like _cheese_, _hamburger_, _apple pie_ and _pizza_, or replace the (_pancake_, _microwave_) at the same time with (_dishes_, _dishwasher_) or (_clothes_, _washing machine_) for greater differences.

Figure 4: While TWOSOME does not support open action space tasks, we compare the average performance between POAD and NTPO on the DataSciCoding benchmarks, as well as POAD-Best the performance of best code explored by POAD during the training phase and CAAFE with GPT-4.

Figure 5: Ablation on \(_{a}\{1.0,0.95\}\) for both TWOSOME and POAD (left), and \(_{w}\{0.95,0.9,0.8,0.5\}\) for NTPO while keeping the \(_{a}=0.95\) unchanged (middle). In the left figure, Setting \(_{a}=1.0\) led to decreased performance and convergence for TWOSOME and POAD, validating necessity of \(_{a}<1.0\). While in the right figure, the increasingly larger performance gap between POAD and NTPO, as \(_{w}\) decreases, verifies the theoretical analysis in Section 4.2. The right one shows the performance change after applying theoretical insights to enhance ArCHer’s performance, i.e. ARCHER-BAD with \(_{w}=1.0\).

The left side of Figure 5 demonstrates an increasingly larger performance gap between POAD (\(_{w}=1\)) and NTPO as \(_{w}\) decreases. These results empirically showcase the discrepancy between naive token-level credit assignment and our BAD, which is consistent with our first theoretical insight in Section 4.2. Moreover, in the middle of Figure 5, for both TWOSOME and our proposed method POAD, setting \(_{a}=1.0\) performs much worse than \(_{a}=0.95\), indicating that the discrepancy between NTPO and POAD can not be solved by simply setting \(_{a}=_{w}=1.0\). Furthermore, the right one in Figure 2 shows improved results that validate the effectiveness of applying our insights to ArCHer. However, due to the inherent challenges of excessive network complexity and difficult hyper-parameter tuning, ArCHer-BAD still falls short of matching POAD's performance.

### Impact on Original Language Ability

To investigate the impacts of online RL fine-tuning on LLMs' original capabilities, we evaluate the models trained by POAD, TWOSOME and NTPO on widely used NLP benchmarks which are also reported in Tan et al.  and Touvron et al. . These models are trained in _Food Preparation_. Table 2 demonstrates these models' zero-shot performance, compared with the original LLaMA2-7B model. The results show no significant losses of general ability like natural language understanding after aligning with the embodied environment, even sometimes bringing minor improvements.

## 7 Conclusion

In this work, we propose the Bellman backup with Action Decomposition (BAD), theoretically eliminating the discrepancy between naive token-level policy optimization and action-level policy optimization for language agents. Integrating BAD with PPO, we propose our method of Policy Optimization with Action Decomposition (POAD), providing finer-grained supervision for each intra-action token and ensuring theoretical consistency between the token-level training nature of language models and the RL objective of maximizing actions' utilities. Empirical experiments and thorough ablations showcase the effectiveness of BAD as well as the superiority of POAD in learning efficiency and generalization abilities, over strong action-level baseline TWOSOME.

**Limitation and Future Work.** The existing limitation of POAD is on the requirement for a quantitative reward function, which is not easily attainable in some environments. To mitigate this, we envisage integrating POAD with self-rewarding [57; 58] or hindsight relabeling .

**Social Impact.** The advancements in RL for language agents can significantly enhance decision-making processes in various domains such as healthcare, finance, and autonomous systems. Improved decision-making can lead to better outcomes, increased efficiency, and reduced errors. However, we acknowledge that when optimizing agents using our method, language agents may potentially resort to unscrupulous means to maximize rewards, which could lead to potentially harmful results. Thus, we advocate for a more comprehensive consideration when designing the reward function, or combining it with safety-constrained RL methods to mitigate these risks.