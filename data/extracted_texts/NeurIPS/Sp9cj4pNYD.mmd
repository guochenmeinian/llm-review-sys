# SurgicAI: A Hierarchical Platform for Fine-Grained Surgical Policy Learning and Benchmarking

Jin Wu1 Haoying Zhou2 Peter Kazanzides1 Adnan Munawar1 Anqi Liu1

1Johns Hopkins University, Baltimore 2Worcester Polytechnic Institute, Worcester {jwu220, pkaz, amunawa2, aliu.cs}@jhu.edu hzhou@wpi.edu

###### Abstract

Despite advancements in robotic-assisted surgery, automating complex tasks like suturing remains challenging due to the need for adaptability and precision. Learning-based approaches, particularly reinforcement learning (RL) and imitation learning (IL), require realistic simulation environments for efficient data collection. However, current platforms often include only relatively simple, non-dexterous manipulations and lack the flexibility required for effective learning and generalization. We introduce SurgicAI, a novel platform for development and benchmarking that addresses these challenges by providing the flexibility to accommodate both modular subtasks and more importantly task decomposition in RL-based surgical robotics. Compatible with the da Vinci Surgical System, SurgicAI offers a standardized pipeline for collecting and utilizing expert demonstrations. It supports the deployment of multiple RL and IL approaches, and the training of both singular and compositional subtasks in suturing scenarios, featuring high dexterity and modularization. Meanwhile, SurgicAI sets clear metrics and benchmarks for the assessment of learned policies. We implemented and evaluated multiple RL and IL algorithms on SurgicAI. Our detailed benchmark analysis underscores SurgicAI's potential to advance policy learning in surgical robotics. Details: https://github.com/surgical-robotics-ai/SurgicAI

## 1 Introduction

Robotic-assisted surgery (RAS) has revolutionized the medical field by enhancing precision, reducing surgeon fatigue, and significantly shortening recovery times. While existing RAS systems lack automation, arguably automation has the potential to improve upon the current standard of care and surgeon workload. However, automating complex surgical tasks such as suturing remains a significant challenge. Traditional methods  rely on pre-programmed instructions and heuristics, lacking the adaptability and generality needed for diverse surgical scenarios. Additionally, there is a lack of fine-grained platforms for complex, multi-stage tasks that provide rich data and assessment metrics. These limitations highlight the need for advanced learning frameworks to handle the complexities of surgical manipulation, as well as comprehensive evaluation and benchmarking methods.

Reinforcement learning (RL) and imitation learning (IL) have emerged as powerful mechanisms in robotic automation, exhibiting the potential to transform the field of surgical robotics. By enabling robots to learn from interactions with their environment and from expert demonstrations, these learning paradigms promise to significantly enhance the autonomy and effectiveness of surgical robots. Recent research  has focused on developing simulation environments for implementing machine learning algorithms for various tasks. However, previous work on surgical application often compromises on either photorealism  or physical realism . Additionally, these platforms typically involve only relatively simple surgical tasks.

In this paper, we introduce SurgicAI, which features complex deformable thread simulation, and real-time interaction compatible with the da Vinci Research Kit (dVRK) , providing a comprehensive learning environment for developing and testing robotic suturing techniques. Moreover, SurgicAI integrates standardized pipelines, advanced hierarchical learning frameworks, diverse task suites, and benchmark performance metrics to evaluate its capabilities.

The primary contributions of SurgicAI are the following:

* **Standardized Pipeline:** SurgicAI offers a comprehensive and standardized pipeline for the entire process of surgical automation. This pipeline includes collecting and preprocessing expert trajectories from teleoperation devices, training agents using reinforcement learning and imitation learning approaches, and evaluating their performance.
* **Hierarchical Task Decomposition:** Most platforms mentioned previously [9; 10; 11] struggle with relatively complex, multi-stage tasks like suturing. SurgicAI is the first platform to implement an open-source hierarchical learning framework specifically for managing multi-stage surgical procedures. The hierarchical structure facilitates the decomposition of intricate surgical tasks into smaller, more manageable subtasks, each handled by specialized modules. This approach not only enhances learning efficiency but also promotes the reuse of learned skills across different procedures, thereby improving the generality and scalability of robotic systems. Figure 1 shows how we can train both high-level policy and low-level policies to conduct the multi-stage procedures.
* **Diverse Suite of Tasks:** SurgicAI encompasses a wide variety of manipulative operations required during the suturing, such as approaching and grasping the needle, inserting the needle, and regrasping the needle. Each of these tasks demands a high level of dexterity and precision, contributing significantly to the automation of suturing processes.
* **Benchmark of Performance:** SurgicAI establishes clear metrics for each suturing task and benchmarks performance across various RL and IL algorithms. With support for the Gymnasium API , and RL libraries like Stable Baseline3 (SB3)  and d3rlpy , SurgicAI offers a user-friendly interface for defining new tasks and implementing custom algorithms. With baselines and metrics, SurgicAI also enables researchers to compare and improve their methods.

## 2 Related Work

### Simulation Platforms for Data Collection and Training

The development of simulation environments for data collection and training in surgical robotics has advanced significantly, driven by the need for scalable and efficient methods to train and evaluate algorithms. Frameworks such as dVRL  and SurRoL  integrate real-time physics engines with user-friendly RL libraries. Both are compatible with dVRK and have demonstrated successful policy transfer to real-world scenarios. Other work like Surgical Gym enhances data collection efficiency

Figure 1: Hierarchical Framework in SurgicAI. A High-Level Policy (HLP) selects and coordinates Low-Level Policies (LLPs) for specific tasks like grasping, placing, inserting, handoff, and pullout. Each LLP manages actions within its designated subtask until a termination condition is met, after which control returns to the HLP for the next decision.

through high-performance GPU-based simulations, achieving significantly faster sampling rates compared to previous platforms. However, it is exclusively designed for RL implementation and does not offer interfaces for teleoperation or collecting expert demonstrations for imitation learning or demonstration-guided purposes. Additionally, most simulations lack soft body simulation, except for recent developments in SurRoL . This limits their physical realism.

Recent advancements, exemplified by ORBIT-Surgical , have introduced photorealistic and physics-based environments with GPU parallelization, supporting both imitation learning and reinforcement learning. Despite these improvements, such platforms are limited to relatively simple tasks like reaching and placing, and they struggle with more complex, multi-stage tasks such as suturing. In contrast, the AccelNet Surgical Robotics Challenge (SRC)  offers a comprehensive solution by addressing these limitations and providing robust support for complex suturing procedures. Our work provides the necessary interface and library support to train and test RL and IL algorithms with low-level dynamics provided in SRC, using a hierarchical framework for long-term planning.

### Learning Based Approaches in Surgical Automation

Robot learning has been applied to various surgical scenarios, including debris removal [19; 20; 21], tissue retraction [22; 23; 24], suturing [25; 26; 27], and cutting [28; 29]. The results from these applications show promising potential for machine learning-based approaches to become key solutions for surgical automation. However, traditional RL methods face significant challenges in these contexts due to sparse rewards, discounting issues, and exploration difficulties inherent in high-dimensional and long-horizon tasks [30; 31]. These challenges often hinder RL algorithms from effectively learning and performing complex surgical tasks. To address these issues, many existing methods rely heavily on elaborate reward shaping to facilitate extensive exploration [22; 32]. This process typically involves subjective manual engineering, which can introduce biases and lead to unexpected policies that get stuck in local optima . The reliance on handcrafted rewards can also limit the generality of the learned policies, as they may not perform well in slightly different or unforeseen scenarios.

Advanced methodologies, such as [33; 34], have been developed to enhance exploration efficiency by incorporating demonstration data into RL. This approach allows learning from expert knowledge, reducing the time and computational resources needed to achieve effective policy learning. Work from [35; 36; 27] predefined primitive skills and sequences them at transition points, creating a more structured and guided learning process. These methods help break down complex tasks into simpler, more manageable components, which can be learned more effectively.

Building on these advancements, the hierarchical framework [37; 38; 39], utilized in SurgicAI, has been introduced to better coordinate sub-policies. This hierarchical approach divides the overall task into a hierarchy of sub-tasks, each with its own specific policies. By structuring the learning process in this way, the hierarchical framework not only improves learning efficiency but also enhances the robustness and adaptability of the system. Moreover, SurgicAI distinctively pioneers the application of image-guided imitation learning algorithms for complex manipulative tasks involving patient-side manipulators (PSMs). While platforms such as  and  have defined some basic Endoscopic Camera Manipulator (ECM) tracking tasks using image data, and  demonstrated the capability for collecting RGB or segmented image data, they have not extensively applied imitation learning to the intricate and precise manipulations required for tasks like suturing.

## 3 Our Framework

### Simulation Environment

The simulation environment of SurgicAI is based on the SRC  and utilizes the Asynchronous Multi-Body Framework (AMBF) [40; 41]. This environment includes two Patient Side Manipulators (PSMs) from the da Vinci Surgical System, an Endoscopic Camera Manipulator (ECM), a needle with or without thread, and realistic phantoms marked with entry and exit holes for suturing. A key feature of this environment is the simulation of a deformable thread, its interaction with the phantom, and the ability to pass it through the specified holes in the phantom, which significantly enhances the realism of tissue simulations. The system is fully compatible with the dVRK system, enabling real-time communication with various teleoperation devices such as dVRK MTMs, Geomagic, and Razer Hydra. Additionally, our environment supports a headless mode to facilitate efficient data sampling without the computational overhead of graphical rendering. The system generates multi-modal data, including ground truth positions, RGB images, and labeled depth maps, making it a suitable platform for data-driven approaches to suturing automation. The workflow is detailed in Figure 2.

### Environment Settings

Our simulation environment adheres to the Gymnasium framework rules , ensuring compatibility with various RL and IL tasks.

**Observation and State Space:** This environment features a versatile observation space that accommodates both low-dimensional and high-dimensional states for comprehensive data collection. In low-dimensional settings, the observation space includes ground-truth 6D poses of objects such as needles, entry and exit holes, and end-effectors. When using Hindsight Experience Replay (HER) , the agent receives extra lists of achieved and desired goals, facilitating learning from past experiences. High-dimensional settings, on the other hand, comprise scene-invariant RGB images from multiple views and proprioception data from PSMs, specifically the 6D pose of the end-effector in its local frame. Our settings support the online and offline training for both low and high dimensional state space. In our experiments, we utilize low-dimensional states in online and offline RL for rapid validation, whereas high-dimensional states are reserved for offline imitation learning, as it allows us to bypass the time-intensive process of collecting online image data.

**Action Space and Reward Setting:** For low-level policies, the action space is continuous and features 7 DOF vector \(a^{7}\). The agent controls the x, y, z, roll, pitch, and yaw of the end-effector in its local frame, along with the state of the jaw (0 for closed, 1 for open). These action steps are restricted in (\(_{x}\), \(_{y}\), \(_{y}\), \(_{roll}\), \(_{pitch}\), \(_{yaw}\), \(_{jaw}\)). The terms represent the action step sizes or incremental changes allowed in each corresponding direction during each action. Additionally, to ensure that the PSMs do not exceed their physical limits, their positions are constrained within predefined boundaries. To mitigate distribution shift, we ensure that the initial state of each subtask encompasses the potential terminal state of the preceding task while permitting additional exploration. In terms of high-level policy, the action space is discrete, with actions represented by integral numbers ranging from 0 to 4, each corresponding to the task number to be executed. All the error terms are calculated using the L2 norm, and the reward functions are defined in two ways: sparse reward, which

Figure 2: Detailed Workflow of our Simulation Environment. The AMBF Description Files (ADF) define various simulation objects, such as rigid bodies, joints, and cameras. The launch file specifies simulation parameters and model-specific details. The AMBF Simulator processes these parameters to initialize the environment, while the AMBF Client bridges the simulator and the control scripts via ROS topics. Method APIs provide essential functions for controlling the simulation, including kinematics and servo control, while teleoperation scripts enable connections with multiple devices. The Gymnasium API integrates RL and IL capabilities, optimizing policies through interaction with the control scripts.

is 0 if successful and -1 otherwise, and dense reward, given by \(-(_{}/100+_{}/10)\), where \(d_{}\) and \(d_{}\) represent the translation and orientation errors, respectively.

### Task Overview

To address the complexity of the suturing process, we have segmented the process into several distinct subtasks, similar to : approaching and grasping the needle, placing the needle at the entry hole, inserting the needle, handing off the needle (regrasping it with the other PSM), and pulling out the needle. This segmentation is informed by empirical observations that show users tend to slow down during transitions between these tasks, making it easier to define and separate each subtask. For specific low-dimensional settings, each subtask is defined as below. While the settings described below offer functionally plausible thresholds for successful task execution, the numerical thresholds for distances and orientation errors are also flexible to adapt to different levels of precision.

**Grasping:** At the beginning of each episode, the needle's position is randomized. A specific point on the needle is designated as the grasping point. The episode is considered successful if the remaining distance between the jaw and the grasping point is within 1 mm, the orientation error is within 10 degrees, and the contact sensor attached to the gripper successfully detects the needle object.

**Placing:** The initial state involves the jaw randomly grasping the needle with positional and orientation errors of up to 1.5 mm and 12 degrees respectively from the expected grasping orientation. Eventually, the needle tip should align with the entry point's center and perpendicular to the entry plane. Success criteria are a distance within 5 mm and an orientation error within 10 degrees.

**Inserting:** The initial state includes the same randomized grasping as in the placing task, with the needle tip positioned within 5.5 mm and 12 degrees of the entry point. The goal is for one-third of the needle to remain outside the exit point, with the tangential direction properly aligned. Success criteria are a remaining distance within 5 mm, an orientation error within 10 degrees, and the needle passing through both entry and exit holes.

**Handoff:** Starting with the needle passing through the phantom, a grasping point on the needle is identified. Success is achieved if the remaining distance between the jaw and the grasping point is within 2 mm, the orientation error is within 15 degrees, and the contact sensor detects the needle.

**Pullout:** The initial state involves the jaw randomly grasping the needle with a positional error of up to 1.5 mm and an orientation deviation up to 20 degrees from the expected grasping point in the handoff task. The goal is to fully extract the needle from the phantom and reach a predetermined end point. Success criteria are a distance within 5 mm and an orientation error within 20 degrees.

This modular approach offers significant advantages in terms of flexibility and reusability. Each subtask can be fine-tuned independently to adapt to different scenarios. For instance, the insertion policy can be modified to accommodate changes in the geometry of the phantoms while other policies remain unchanged. The grasping policy can be adjusted for various types of grasping actions. By organizing and integrating these pre-trained subtasks, the agent is able to perform the repetitive suturing procedure effectively.

### Hierarchical Architecture for Multi-stage Tasks

The hierarchical architecture is designed to efficiently manage complex robotic tasks by decomposing them into smaller, manageable subtasks, each handled by specialized neural networks. This approach leverages both high-level and low-level policies to ensure precise control and coordination, leading to successful task execution.

**High-Level Policy (HLP):** At the core of the hierarchical architecture is the High-Level Policy, which utilizes a neural network for decision making. The HLP is responsible for sequencing and coordinating the Low-Level Policy. Based on the current state of the task, the HLP determines the appropriate subtask to activate, managing transitions between subtasks and ensuring that each stage of the task is executed in the correct order. This component allows the system to adapt to dynamic changes in the environment and varying initial configurations.

**Low-Level Policy (LLP):** The Low-Level Policies are specialized neural networks trained to execute specific subtasks. Each LLP is responsible for a particular aspect of the overall task, such as grasping, placing, inserting, handoff, or pullout. These subtasks are parameterized and trained independently,enabling the LLPs to master their respective tasks efficiently. The specialization of LLPs enables focused learning and optimization, achieving more robust and reliable performance for each subtask.

**Integration and Coordination:** The hierarchical framework operates in a cyclical manner to ensure efficient task execution: 1) The HLP generates a subtask based on the current state of the environment; 2) The corresponding LLP executes the subtask and reaches its terminal state, upon which the process loops back to the HLP for the next subtask generation. This process continues iteratively until the overall task is completed. By breaking down complex tasks into simpler components, the hierarchical architecture ensures that each subtask is handled by a network specialized in that particular operation. The HLP's role in managing the sequence and coordination of subtasks is crucial for maintaining the overall coherence and efficiency of the task execution.

## 4 Capability of Our Framework

### Platform Usage Guideline

Figure 3 illustrates the training pipeline for RL agents. The pipeline consists of three main stages: Initialization, Training, and Evaluation. The initialization stage begins with setting up ROS and SRC, initializing the Gymnasium environment which defines reset, step, and reward functions for each task, and defining learning algorithms. Additionally, it involves configuring the random seed, maximum episode length, network architecture, replay buffer, and other hyper-parameters. The framework can be adapted for imitation learning by adjusting the parameters accordingly.

During the training stage, users first decide on whether to continue or fine-tune other pretrained models, with the model being loaded if applicable. Optional features such as checkpoint autosave, Tensorboard visualization, and a progress bar may be utilized. Following these preparations, the training process is executed. The test and evaluation stage includes saving/loading the model, predicting actions, controlling the robot, and evaluating performance using specific metrics. This streamlined approach ensures the effective development, training, and evaluation of the agent.

### Teleoperation and Dataset Collection

Given the exploration difficulties inherent in RL implementations, expert demonstrations are often required to mitigate these challenges. SurgicAI provides a comprehensive pipeline for data collection from two primary sources: teleoperation devices and heuristic trajectories generated by Learning from Demonstration (LfD) algorithms . Specifically, the teleoperation data is collected using master tool manipulators (MTMs), while the heuristic trajectories are generated using Dynamic Movement Primitives (DMP) and Locally Weighted Regression (LWR), as described in . For the teleoperation dataset, a human operator uses the teleoperation device to interact with the SRC environment in real time. A recording script captures the PSM joint positions and needle poses in a

Figure 3: Training pipeline for reinforcement learning.

rosbag1. These recordings enable replaying of trajectories, during which users can collect additional data such as RGB images, depth information, and end-effector positions.

Our environment inherits the benefit of SRC to support various user interfaces for controlling the PSM arms, including graphical user interfaces and devices such as Geomagic and Razer Hydra . Meanwhile, SurgicAI incorporates a standardized data processing pipeline that converts raw data into a format compatible with the Gymnasium API. This process organizes observations, actions, rewards, and terminal states into a structured transition format. This flexibility allows users to switch between different devices for data collection without requiring any modifications, thereby enhancing the efficiency of the data collection process. Additionally, beyond providing a platform for data collection, SurgicAI also offers access to some of the collected trajectories for training purposes, including around 30 human trajectories and 50 heuristic trajectories. These are available on our project website2 for further use.

### Sustained Maintenance Repository

SurgicAI offers a versatile and robust framework for suturing automation, continuously maintained and updated to keep pace with the latest advancements in the field. To create a collaborative environment and facilitate users from different backgrounds, we provide comprehensive instructions for simulator configuration and environment setup, ensuring that users can easily get started. Our documentation includes detailed environment descriptions and demo videos to facilitate understanding and usage. Additionally, we offer a variety of training baselines for different algorithms, supported by extensive RL implementations under the SB3 framework. Our repository also includes various tools and resources to streamline the training. These include processed rollout data from recorded trajectories, customized algorithms, and performance benchmarking scripts. By leveraging these resources, researchers can efficiently develop, test, and validate their algorithms.

## 5 Implementation and Experiment Results

This section covers the implementation of algorithms and experiments, including result analysis. Empirically, we aim to demonstrate the following:

1. The performance of different RL and IL algorithms in suturing tasks, especially the importance of expert demonstration data for successful policy development.
2. The effectiveness of various visual representations in image-guided surgery scenarios.
3. The advantages of our hierarchical task decomposition framework over a single-policy approach.

**Evaluation Metrics:** In this study, we utilized three primary evaluation metrics to assess the performance of low-level policies:

1. Success Rate: The proportion of episodes where the task was successfully completed out of the total number of episodes. This metric reflects the algorithm's ability to perform the task successfully.
2. Trajectory Length: The cumulative distance, measured in millimeters, covered by delta actions during a successful episode. This metric provides insight into the physical efficiency of task completion.
3. Time Steps: The total number of discrete actions taken to complete the task. This metric measures temporal efficiency, indicating how quickly the agent completes the task.

These metrics collectively help evaluate the effectiveness of the low-level policies, where the ideal outcome is a high success rate, minimal trajectory length, and fewer time steps.

**Performance of RL and IL algorithms:** To evaluate the performance of various algorithms, we tested online algorithms like Proximal Policy Optimization (PPO) , Deep Deterministic Policy Gradient (DDPG) , Soft Actor-Critic (SAC) , and Twin Delayed DDPG (TD3) , along with ablation tests involving Hindsight Experience Replay (HER)  and Behavior Cloning (BC) , as well as offline algorithms including Advantage Weighted Actor-Critic (AWAC) , Batch-Constrained Q-learning (BCQ) , Implicit Q-learning (IQL) , and Calibration Q-learning (CalQL) . Both sparse and dense rewards are applied for evaluation. Detailed environment settings and network hyperparameters are provided in Sections 3.2 and A.1, respectively. By default, the BC loss weight during testing is set to 0.5 and we further discuss the impact of the weight in Section A.5.

Table 1 presents the results of each RL algorithm. Pure online RL methods struggled to achieve significant success rates, especially under sparse reward settings. For example, both PPO and DDPG exhibited near-zero success rates for most subtasks, underscoring the challenge of exploration in sparse reward environments. Even with techniques like HER, improvements were limited due to the curse of dimensionality and stringent precision requirements.

Performance improved slightly with dense rewards but often led to suboptimal trajectories, as agents tended to favor the shortest path to task completion. Specific tasks like inserting and handoff posed significant challenges. In the inserting task, agents often adopt strategies with minimal-step completion to minimize reward discounting, occasionally resulting in suboptimal trajectories and missed needle exit points. The handoff task requires precise gripper manipulation to grasp a small needle segment; any collision with the phantom can alter the gripper's pose and lead to failure.

In contrast, integrating BC with RL demonstrated superior performance by closely adhering to expert trajectories. TD3+HER+BC, as demonstrated in A.4, achieved high success rates. While BC alone could attain comparable success rates, the addition of the RL loss optimized the policy, resulting in more efficient trajectories and shorter completion times. For example, in the approaching task, TD3+HER+BC reduced the trajectory length by approximately 5 mm and decreased the completion time by 10 steps compared to BC alone, demonstrating improved efficiency in both spatial and temporal dimensions.

   Algorithm & Reward Type & Criteria & Grasping & Placing & Inserting & Handoff & Pullout \\   &  & Success Rate & \(0.22 0.10\) & / & / & \(0.20 0.20\) & \(0.40 0.49\) \\  & & Trajectory Length (mm) & \(31.80 3.84\) & / & / & \(58.54 2.22\) & \(43.59 1.03\) \\  & & Time step & \(81.00 8.05\) & / & / & \(97.09 4.51\) & \(85.21 4.23\) \\   & & Sparse & Success Rate & / & / & / & / \\   &  & Success Rate & \(0.02 0.04\) & / & / & \(0.04 0.04\) \\  & & Trajectory Length (mm) & \(38.78 2.45\) & / & / & / & \(40.53 1.56\) \\  & & Time step & \(87.50 9.50\) & / & / & / & \(83.94 1.64\) \\   & & Sparse & Success Rate & / & / & / & / \\   &  & Success Rate & / & / & / & / & / \\   & & Success Rate & \(0.96 0.05\) & \(1.00 0.00\) & \(0.97 0.02\) & \(0.83 0.24\) & \(0.92 0.09\) \\  & & Trajectory Length (mm) & \(51.31 14.60\) & \(66.35 20.36\) & \(42.43 2.51\) & \(64.65 3.21\) & \(49.88 1.74\) \\  & & Time step & \(100.15 26.02\) & \(120.87 26.27\) & \(89.80 3.96\) & \(113.60 5.15\) & \(59.80 3.60\) \\  TD3 &  & Success Rate & / & / & / & / & / \\   &  & Success Rate & / & / & / & / & / \\   & & Success Rate & \(0.11 0.12\) & \(0.15 0.09\) & \(0.05 0.05\) & \(0.12 0.09\) & \(0.15 0.14\) \\  & & Trajectory Length (mm) & \(46.55 7.49\) & \(55.26 12.15\) & \(38.71 1.88\) & \(85.32 10.23\) & \(45.23 2.46\) \\  & & Time step & \(86.18 12.85\) & \(108.62 23.88\) & \(79.28 3.21\) & \(59.43 8.32\) & \(59.64 5.82\) \\   &  & Success Rate & \(0.85 0.10\) & \(0.88 0.08\) & \(0.82 0.15\) & \(0.79 0.14\) & \(0.90 0.10\) \\  & & Trajectory Length (mm) & \(52.45 15.90\) & \(63.39 15.60\) & \(46.73 4.59\) & \(65.98 3.92\) & \(48.52 1.85\) \\  & & Time step & \(100.007 18.10\) & \(116.74 32.88\) & \(87.00 8.89\) & \(108.00 4.03\) & \(3.90 3.90\) \\   &  & Success Rate & \(0.96 0.06\) & \(0.97 0.09\) & \(0.91 0.07\) & \(0.98 0.02\) & \(10.10 0.00\) \\  & & Trajectory Length (mm) & \(47.89 19.25\) & \(59.27 19.92\) & \(41.74 2.82\) & \(61.61 6.15\) & \(41.88 2.49\) \\  & & Time step & \(89.61 25.36\) & \(106.03 31.31\) & \(83.25 3.30\) & \(105.00 4.30\) & \(88.20 4.93\) \\   &  & Success Rate & / & / & \(0.96 0.06\) & \(0.98 0.09\) & / \\  & & Trajectory Length (mm) & / & / & \(44.06 4.23\) & \(63.40 5.96\) & / \\  & & Time step & / & / & \(89.32 2.32\) & \(94.00 10.32\) & / \\   &  & Success Rate & \(0.94 0.10\) & \(0.92 0.09\) & \(0.95 0.08\) & \(0.96 0.04\) & \(0.93 0.10\) \\  & & Trajectory Length (mm) & \(49.22 14.36\) & \(61.73 9.10\) & \(42.38 2.31\) & \(69.78 4.63\) & \(45.02 3.69\) \\  & & Time step & \(94.75 19.36\) & \(117.90 24.46\) & \(93.00 4.52\) & \(106.00 7.50\) & \(92.23 3.21\) \\   &  & Success Rate & / & / & / & / & / \\   & & Success Rate & \(0.95 0.02\) & \(0.91 0.09\) & \(0.94 0.05\) & \(0.95 0.06\) & \(0.95 0.06\) \\   & & Trajectory Length (mm) & \(47.40 15.13\) & \(67.28 11.52\) & \(43.46 2.51\) & \(72.21 3.43\) & \(44.28 2.34\) \\   & & Time step & \(92.15 21.57\) & \(121.85 23.56\) & \(48.00 4.12\) & \(110.34 6.21\) & \(95.21 4.31\) \\   & Dense & Success Rate & / & / & / & / \\   

Table 1: Performance results of different RL algorithms after Offline algorithms generally outperformed online ones, particularly in dense reward settings. For instance, BCQ consistently achieved high success rates across all tasks, including 0.95 for the inserting task and 0.96 for handoff. However, algorithms, such as AWAC and CalQL, struggled on tasks like approaching and placing, achieving zero success rates during evaluation. This is because their reliance on accurate advantage or Q-value estimates made them vulnerable in tasks where the offline data was not comprehensive enough to support reliable learning. This performance could be improved with more diverse and abundant offline data.

**Effectiveness of Visual Representations:** In our study of image-based imitation learning, we applied the same evaluation metrics to assess different visual representations, as shown in Table 2. Detailed settings are provided in Section A.2. We evaluated the performance of three popular visual representations: Contrastive Language-Image Pretraining (CLIP) , Pretraining Reusable Representations for Robot Manipulation (R3M) , and a pretrained ResNet-50 model  using the ImageNet dataset (ImNet) . Also, we included a randomly initialized ResNet-50 (RandomNet) as a baseline for comparison. Each model was tested with two camera views: front and back.

A high success rate was consistently observed in the handoff task using the back camera view, suggesting that certain tasks and perspectives provide more favorable conditions for these learning algorithms. R3M slightly outperformed CLIP and ImNet in tasks requiring precise spatial alignment, such as handoff and pullout. Pretrained models consistently outperformed RandomNet in terms of success rate. This is likely because they were trained on large, diverse datasets, enabling them to learn valuable representations such as semantic information and image patterns. These pretrained features can be fine-tuned or directly applied to new domains, significantly reducing computational costs and sample complexity compared to training an encoder from scratch, which can lead to overfitting given the limited scale of our data.

**Hierarchical Multi-stage Suturing Performances:** While testing the final points, in the configuration of our single-policy approach, we utilized a sparse reward structure, awarding the agent upon the successful completion of each subtask. The detailed result is shown in Table 3. When BC was integrated, expert demonstrations encompassing the entire suturing sequence were employed to guide the agent's actions. Although BC effectively managed individual subtasks, its performance was

    &  &  &  &  &  \\   & & & front & back & front & back & front & back & front & back \\   &  & 0.22\(\)0.14 & 0.35\(\)0.04 & 0.22\(\)0.06 & 0.32\(\)0.06 & 0.08\(\)0.06 & 0.10\(\)0.06 & 0.03\(\)0.05 & 0.03\(\)0.02 \\  & & Trajectory length (mm) & 40.81\(\)6.44 & 44.04\(\)7.63 & 50.91\(\)6.03 & 46.32\(\)6.65 & 47.88\(\)8.22 & 49.59\(\)10.04 & 54.85\(\)15.33 & 41.60\(\)41.11 \\  & & Time cost (step) & 93.18\(\)19.59 & 94.92\(\)15.75 & 109.63\(\)14.20 & 97.65\(\)22.61 & 89.00\(\)02.06 & 91.83\(\)13.83 & 102.00\(\)31.00 & 74.50\(\)50.50 \\   &  & 0.40\(\)0.07 & 0.22\(\)0.06 & 0.30\(\)0.07 & 0.28\(\)0.09 & 0.27\(\)0.12 & 0.20\(\)0.18 & 0.08\(\)0.06 & 0.02\(\)0.02 \\  & & Trajectory length (mm) & 60.80\(\)0.94 & 60.04\(\)8.24 & 70.23\(\)11.43 & 71.89\(\)7.77 & 67.80\(\)9.46 & 77.31\(\)12.60 & 80.70\(\)0.00 & 69.02\(\)42.90 \\  & & Time cost (step) & 125.88\(\)17.97 & 134.55\(\)32.86 & 171.68\(\)12.43 & 165.94\(\)92.96 & 181.46\(\)13.33 & 1260.46\(\)20.12 & 131.00\(\)0.00 & 120.20\(\)48.83 \\   &  & 0.80\(\)0.04 & 0.98\(\)0.02 & 0.62\(\)0.06 & 0.78\(\)0.02 & 0.77\(\)0.06 & 0.83\(\)0.02 & 0.03\(\)0.02 & 0.07\(\)0.03 \\  & & Time cost (step) & 56.12\(\)3.72 & 54.09\(\)28.88 & 59.34\(\)40.08 & 58.42\(\)39.06 & 66.70\(\)36.31 & 64.52\(\)36.0 & 71.50\(\)36.61 & 69.02\(\)0.90 \\  & & Time cost (step) & 107.98\(\)7.09 & 116.92\(\)54.15 & 116.85\(\)15.16 & 118.47\(\)6.23 & 122.48\(\)17.37 & 117.68\(\)78.85 & 110.20\(\)43.83 \\   &  & 0.82\(\)0.12 & 0.63\(\)0.09 & 0.62\(\)0.06 & 0.60\(\)0.04 & 0.34\(\)0.04 & 0.20\(\)0.07 & 0.10\(\)0.00 & 0.08\(\)0.05 \\  & & Trajectory length (mm) & 44.86\(\)26.64 & 50.20\(\)34.00 & 49.42\(\)23.56 & 52.42\(\)32.38 & 55.09\(\)28.7 & 54.04\(\)47.7 & 49.98\(\)12.25 & 46.96\(\)39.39 \\   & & Time cost (step) & 76.00 \(\) 5.58 & 83.47\(\)4.31 & 83.92\(\)13.01 & 74.72\(\)54.78 & 74.56\(\)78 & 81.71\(\)12.43 & 78.08\(\)25.84 & 74.25\(\)54.92 \\  

Table 2: Performance results for each subtask using image-based imitation learning after 30 epochs. Each visual representation was tested under two camera views, with results averaged over 20 episodes using 5 random seeds.

  
**Sequential Tasks** & **TD3+HER** & **TD3+HER+BC** & **Hierarchical** \\   Approaching - Placing & 0.00 & 0.64 & **0.88** \\ Approaching - Inserting & 0.00 & 0.40 & **0.72** \\ Approaching - Handoff & 0.00 & 0.24 & **0.60** \\ Approaching - Pullout & 0.00 & 0.15 & **0.52** \\   

Table 3: Mean success rate (expressed as probabilities) for execution of sequential subtasks. Each result is evaluated over 100 episodes, while each subtask policy from the Hierarchical method is trained with TD3+HER+BC as in Table 1.

limited in longer task sequences. This limitation primarily stems from the accumulation of errors over extended execution, which progressively leads the agent into states that differ substantially from those represented in the demonstration data. Consequently, a distribution shift occurs between the training data and the actual states encountered during the suturing process. In contrast, our hierarchical task decomposition framework demonstrated substantial performance enhancements, achieving over a \(50\%\) success rate in the complete suturing procedure. This marked improvement highlights the framework's robustness and efficacy in managing complex surgical tasks.

## 6 Conclusions, Limitations, and Future Plans

In conclusion, SurgicAI provides a comprehensive and standardized pipeline for suturing automation, implementing a robust hierarchical learning framework, and creating a diverse suite of manipulative tasks that benchmark various learning approaches. The results demonstrate that our framework significantly enhances the suturing performance, underscoring its practical impact on surgical automation. The remaining challenges and future work are as follows:

**More realistic simulations:** SurgicAI currently utilizes the simulator version from the 2022-2023 AccelNet Surgical Robotics Challenge . As the challenge evolves, incorporating new models of real phantoms, we will update SurgicAI accordingly. Our datasets and pipelines will be adapted to ensure compatibility with different versions of phantoms and instruments. We are also developing a more realistic rendering pipeline  to reduce the domain gap in Sim2Real transfer, as well as injecting the kinematic error  of physical robots, ultimately facilitating the fine-tuning and transfer of our policies to real-world suturing.

**More algorithms and tasks:** SurgicAI has been tested with a limited set of algorithms, but still the image-based policies exhibit suboptimal performance, particularly in terms of success rate and stability. The performance also fluctuates significantly with changes in camera perspectives, suggesting that dynamic-view and wrist-mounted cameras could help capture more comprehensive information. To address these issues, we encourage further efforts to improve the robustness of image-based policies. Besides, our current policies rely on Markovian single-step methods, which are inadequate for tasks involving temporal dependencies like pauses or corrections . To overcome this, incorporating advanced algorithms such as the Action Chunking Transformer (ACT)  and Diffusion Policy (DP)  will be essential. Looking ahead, we aim to integrate vision-language models to improve high-level decision-making, expand our subtask library, and strengthen the system's performance in complex and unexpected scenarios. With more medical simulation environments and applications based on AMBF , SurgicAI has the potential for deployment across various surgical applications.

**Collaboration:** The surgical robotics challenge, published as an annual competition , invites teams from around the world to contribute their trajectories and approaches to suturing automation. To foster a collaborative community where participants can advance algorithms and augment datasets, we aim to further advance SurgicAI via more realistic simulations and more diverse algorithms and tasks. We look forward to collaborating with the broader research community to realize these goals.