# A Unified Approach to Count-Based

Weakly-Supervised Learning

 Vinay Shukla

Department of Computer Science

University of California, Los Angeles

vshukla@g.ucla.edu

&Zhe Zeng

Department of Computer Science

University of California, Los Angeles

zhezeng@cs.ucla.edu

Equal contribution.

Department of Computer Science

University of California, Los Angeles

guyvdb@cs.ucla.edu

Equal contribution.

###### Abstract

High-quality labels are often very scarce, whereas unlabeled data with inferred weak labels occurs more naturally. In many cases, these weak labels dictate the frequency of each respective class over a set of instances. In this paper, we develop a unified approach to learning from such weakly-labeled data, which we call _count-based weakly-supervised learning_. At the heart of our approach is the ability to compute the probability of exactly \(k\) out of \(n\) outputs being set to true. This computation is differentiable, exact, and efficient. Building upon the previous computation, we derive a _count loss_ penalizing the model for deviations in its distribution from an arithmetic constraint defined over label counts. We evaluate our approach on three common weakly-supervised learning paradigms and observe that our proposed approach achieves state-of-the-art or highly competitive results across all three of the paradigms.

## 1 Introduction

Weakly supervised learning  enables a model to learn from data with restricted, partial or inaccurate labels, often known as _weakly-labeled data_. Weakly supervised learning fulfills a need arising in many real-world settings that are subject to privacy or budget constraints, such as privacy sensitive data , medical image analysis , clinical practice , personalized advertisement  and knowledge base completion , to name a few. In some settings, _instance-level labels_ are unavailable. Instead, instances are grouped into _bags_ with corresponding _bag-level labels_ that are a function of the instance labels, e.g., the proportion of positive labels in a bag. A key insight that we bring forth is that such weak supervision can very often be construed as _enforcing constraints on label counts of data_.

More concretely, we consider three prominent weakly supervised learning paradigms. The first paradigm is known as _learning from label proportions_. Here the weak supervision consists in the _proportion_ of positive labels in a given bag, which can be interpreted as _the count of positive instances_ in such a bag. The second paradigm, whose supervision is strictly weaker than the former, is _multiple instance learning_. Here the bag labels only indicate the _existence_ of at least one positive instance in a bag, which can be recast as to whether _the count of positive instances_ is greater than zero. The third paradigm, _learning from positive and unlabeled data_, grants access tothe ground truth labels for a subset of _only the positive instances_, providing only a class prior for what remains. We can recast the class prior as _a distribution of the count of positive labels_.

Leveraging the view of weak supervision as a constraint on label counts, we utilize a simple, efficient and probabilistically sound approach to weakly-supervised learning. More precisely, we train a neural network to make instance-level predictions that conform to the desired label counts. To this end, we propose a _differentiable count loss_ that characterizes how close the network's distribution comes to the label counts; a loss which is surprisingly tractable. Compared to prior methods, this approach does not approximate probabilities but computes them _exactly_. Our empirical evaluation demonstrates that our proposed count loss significantly boosts the classification performance on all three aforementioned settings.

## 2 Problem Formulations

In this section, we formally introduce the aforementioned weakly supervised learning paradigms. For notation, let \(^{d}\) be the input feature space over \(d\) features and \(=\{0,1\}\) be a binary label space. We write \(\) and \(y\) for the input and output random variables respectively. Recall that in fully-supervised binary classification, it is assumed that each feature and label pair \((,y)\) is sampled independently from a joint distribution \(p(,y)\). A classifier \(f\) is learned to minimize the risk \(R(f)=_{(,y) p}[(f(),y)]\) where \(:_{ 0}\) is the cross entropy loss function. Typically, the true distribution \(p(,y)\) is implicit and cannot be observed. Therefore, a set of \(n\) training samples, \(=\{(_{i},y_{i})\}_{i=1}^{n}\), is used and the empirical risk, \((f)=_{i=1}^{n}(f(_{i}),y_{i})\), is minimized in practice. In the count-based weakly supervised learning settings, the supervision is given at a bag level instead of an instance level. We formally introduce these settings as below.

### Learning from Label Proportions

_Learning from label proportions (LLP)_ assumes that each instance in the training set is assigned to bags and only the proportion of positive instances in each bag is known. One example is in light of the coronavirus pandemic, where infection rates were typically reported based on geographical boundaries such as states and counties. Each boundary can be treated as a bag with the infection rate as the proportion annotation.

The goal of LLP is to learn an instance-level classifier \(f:\) even though it is trained on bag-level labeled data. Formally, the training dataset consists of \(m\) bags, denoted by \(=\{(B_{i},_{i})\}_{i=1}^{m}\) where each bag \(B_{i}=\{_{j}\}_{j=1}^{k}\) consist of \(k\) instances and this \(k\) could vary among different bags.

The bag proportions are defined as \(_{i}=_{j=1}^{k}y_{j}/k\) with \(y_{j}\) being the instance label that cannot be accessed and only \(_{i}\) is available during training. An example is shown in Figure 0(b). We do not assume that the bags are non-overlapping while some existing work suffers from this limitation including Scott and Zhang .

   \(\) & \(y\) \\   \(}\) & 0 \\   \(}\) & 0 \\   \(}\) & 1 \\    
   \(_{i}\)\}_{i=1}^{k}\) & \(=_{i}/k\) \\   \(}\) & 0 \\   \(}\) & 1 \\   \(}\) &? \\   \(}\) &? \\   \(}\) &? \\   \(}\) &? \\   \(}\) &? \\   

Table 1: A comparison of the tasks considered in the three weakly supervised settings, LLP (cf. Section 2.1), MIL (cf. Section 2.2) and PU learning (cf. Section 2.3), against the classical fully supervised setting for binary classification, using digits from the MNIST dataset.

### Multiple Instance Learning

_Multiple instance learning (MIL)_[35; 17] refers to the scenario where the training dataset consists of bags of instances, and labels are provided at bag level. However, in MIL, the bag label is a single binary label indicating whether there is a positive instance in the bag or not as opposed to a bag proportion defined in LLP. A real-world application of MIL lies in the field of drug activity . We can observe the effects of a group of conformations but not for any specific molecule, motivating a MIL setting. Formally, in MIL, the training dataset consists of \(m\) bags, denoted by \(=\{(B_{i},_{i})\}_{i=1}^{m}\), with a bag consisting of \(k\) instances, i.e., \(B_{i}=\{_{j}\}_{j=1}^{k}\). The size \(k\) can vary among different bags. For each instance \(_{j}\), there exists an instance-level label \(y_{j}\) which is not accessible. The bag-level label is defined as \(_{i}=_{j}\{y_{j}\}\). An example is shown in Figure 0(c).

The main goal of MIL is to learn a model that predicts a bag label while a more challenging goal is to learn an instance-level predictor that is able to discover positive instances in a bag. In this work, we aim to tackle both by training an instance-level classifier whose predictions can be combined into a bag-level prediction as the last step.

### Learning from Positive and Unlabeled Data

_Learning from positive and unlabeled data_ or _PU learning_[16; 31] refers to the setting where the training dataset consists of only positive instances and unlabeled data, and the unlabeled data can contain both positive and negative instances. A motivation of PU learning is persistence in the case of shifts to the negative-class distribution , for example, a spam filter. An attacker may alter the properties of a spam email, making a traditional classifier require a new negative dataset . We note that taking a new unlabeled sample would be more efficient, motivating PU learning. Formally, in PU learning, the training dataset \(=_{p}_{u}\) where \(_{p}=\{(_{i},_{i}=1)\}_{i=1}^{n_{p}}\) is the set of positive instances with \(_{i}\) from \(p( y=1)\) and \(\) denoting whether the instance is labeled, and \(_{u}=\{(_{i},_{i}~{}=~{}0)\}_{i=1}^{n_{u}}\) the unlabeled set with \(_{i}\) from

\[p_{u}()=~{}p( y=1)+(1-)~{}p( y=0),\] (1)

where the mixture proportion \(:=p(y=1=0)\) is the fraction of positive instances among the unlabeled population. Although the instance label \(y\) is not accessible, its information can be inferred from the binary selection label \(\): if the selection label \(=1\), it belongs to the positively labeled set, i.e., \(p(y=1=1)=1\); otherwise, the instance \(\) can be either positive or negative. An example of such a dataset is shown in Figure 0(d).

The goal of PU learning is to train an instance-level classifier. However, it is not straightforward to learn from PU data and it is necessary to make assumptions to enable learning with positive and unlabeled data . In this work, we make a commonly-used assumption for PU learning, _selected completely at random (SCAR)_, which lies at the basis of many PU learning methods.

**Definition 2.1** (Scar).: Labeled instances are selected completely at random, independent from input features, from the positive distribution \(p( y=1)\), that is, \(p(=1,y=1)=p(=1 y=1)\).

## 3 A Unified Approach: Count Loss

In this section, we derive objectives for the three weakly supervised settings, LLP, MIL, and PU learning, from first principles. Our proposed objectives bridge between neural outputs, which can be observed as counts, and arithmetic constraints derived from the weakly supervised labels. The idea is to capture how close the classifier is to satisfying the arithmetic constraints on its outputs.

  
**Task** & **Label** & **Label Level** & **Objective** \\  Classical Fully Supervised & Binary \(y\) & Instance Level & \(-y p(y)-(1-y)(1-p(y))\) \\ Learning from Label Proportion & Continuous \(=_{i}y_{i}/k\) & Bag Level & \(- p(_{i}=k)\) \\ Multiple Instance Learning & Binary \(=\{y_{i}\}\) & Bag Level & \(- p(_{i} 1)-(1-) p(_{i}_{i}=0)\) \\ Learning from Positive & Binary \(\) & Instance Level & \(1)~{}_{KL}((k,) p(_{i}_{i}))\) \\ and Unlabeled Data & Binary \(\) & Instance Level & \(2)- p(_{i}_{i}=k)\) \\   

Table 2: A summary of the labels and objective functions for all the settings considered in the paper.

They can be easily integrated with deep learning models, and allow them to be trained end-to-end. For the three objectives, we show that they share the same computational building block: given \(k\) instances \(\{_{i}\}_{i=1}^{k}\) and an instance-level classifier \(f\) that predicts \(p(_{i}_{i})\) with \(\) denoting the prediction variable, the problem of inferring the probability of the constraint on counts \(_{i=1}^{k}_{i}=s\) is to compute the count probability defined below:

\[p(_{i=1}^{k}_{i}=s\{_{i}\}_{i=1}^{k}):=_{} ^{k}}[_{i=1}^{k}_{i}\ =\ s]_{i=1}^{k}p(_{i}_{i})\]

where \([\![]\!]\) denotes the indicator function and \(}\) denotes the vector \((_{1},,_{k})\). For succinctness, we omit the dependency on the input and simply write the count probability as \(p(_{i=1}^{k}_{i}=s)\). Next, we show how the objectives derived from first principles can be solved by using the count probability as an oracle. We summarize all proposed objectives in Table 2. Later, we will show how this seemingly intractable count probability can be efficiently computed by our proposed algorithm.

**LLP setting.** Given a bag \(B=\{_{i}\}_{i=1}^{k}\) of size \(k\) and its weakly supervised label \(\), by definition, it can be inferred that the number of positive instances (count) in the bag is \(k\). Our objective is to minimize the negative log probability \(- p(_{i}_{i}=k)\). Notice that when each bag consists of only one instance, that is, when the bag-level supervisions are reduced to instance-level ones, this objective is exactly cross-entropy loss. We further show that our method is risk-consistent, that is, the optimal classifier under our proposed loss provides predictions consistent with the underlying risk as in the supervised learning setting. Details of the risk analysis can be found in Appendix A.

**MIL setting.** Given a bag \(B=\{_{i}\}_{i=1}^{k}\) of size \(k\) and a single binary label \(\) as its weakly supervised label, we propose a cross-entropy loss as below

\[(B,)=- p(_{i} 1)-(1-) p( _{i}=0).\]

Notice that in the above loss, the probability term \(p(_{i}=0)\) is accessible to the oracle for computing count probability, and the other probability term \(p(_{i} 1)\) can simply be obtained from \(1-p(_{i}=0)\), i.e., the same call to the oracle since all prediction variables \(_{i}\) are binary.

**PU Learning setting.** Recall that for the unlabeled data \(_{u}\) in the training dataset, an unlabeled instance \(_{i}\) is drawn from a mixture distribution as shown in Equation 1 parameterized by a mixture proportion \(=p(y=1=0)\). Under the SCAR assumption, even though only a class prior is given, we show that the mixture proportion can be estimated from the dataset.

**Proposition 3.1**.: _With SCAR assumption and a class prior \(\ :=\ p(y=1)\), the mixture proportion \(\ :=\ p(y=1=0)\) can be estimated from dataset \(\)._

Proof.: First, the label frequency \(p(=1 y=1)\) denoted by \(c\) can be obtained by

\[c==1,y=1)}{p(y=1)}==1)}{p(y=1)}\ \ .\]that is, \(c=p(=1)/\). Notice that \(p(=1)\) can be estimated from the dataset \(\) by counting the proportion of the labeled instances. Thus, we can estimate the mixture proportion as below,

\[==0 y=1)p(y=1)}{p(=0)}== 1 y=1))p(y=1)}{1-p(=1)}=.\]

The probabilistic semantic of the mixture proportion is that if we randomly draw an instance \(_{i}\) from the unlabeled population, the probability that the true label \(y_{i}\) is positive would be \(\). Further, if we randomly draw \(k\) instances, the distribution of the summation of the true labels \(_{i=1}^{k}y_{i}\) conforms to a binomial distribution \((k,)\) parameterized by the mixture proportion \(\), i.e.,

\[p(_{i=1}^{k}y_{i}=s)=^{s}(1-)^{k-s}.\] (2)

Based on this observation, we propose an objective to minimize the KL divergence between the distribution of predicted label sum and the binomial distribution parameterized by the mixture proportion for a random subset drawn from the unlabeled population, that is,

\[_{KL}((k,)\ \ p(_{i=1}^{k}_{i}) )=_{s=0}^{k}(s;k,)\ (s;k,)}{p(_{i=1}^{k}_{i}=s)}\]

where \((s;k,)\) denotes the probability mass function of the binomial distribution \((k,)\). Again, the KL divergence can be obtained by \(k+1\) calls to the oracle for computing count probability \(p(_{i=1}^{k}_{i}=s)\). The KL divergence is further combined with a cross entropy defined over labeled data \(_{p}\) as in the classical binary classification training as the overall objective.

As an alternative, we propose an objective for the unlabeled data that requires fewer calls to the oracle: instead of matching the distribution of the predicted label sum with the binomial distribution, this objective matches only the expectations of the two distributions, that is, to maximize \(p(_{i=1}^{k}_{i}=k)\) where \(k\) is the expectation of the binomial distribution \((k,)\). We present empirical evaluations of both proposed objectives in the experimental section.

## 4 Tractable Computation of Count Probability

In the previous section, we show how the count probability \(p(_{i=1}^{k}_{i}=s)\) serves as a computational building block for the objectives derived from first principles for the three weakly supervised learning settings. With a closer look at the count probability, we can see that given a set of instances, the classifier predicts an instance-level probability for each and it requires further manipulation to obtain

Figure 1: An example of how to compute the count probability in a dynamic programming manner. Assume that an instance-level classifier predicts three instances to have \(p(y_{1}=1)=0.1\), \(p(y_{2}=1)=0.2\), and \(p(y_{3}=1)=0.3\) respectively. The algorithm starts from the top-left cell and propagates the results down right. A cell has its probability \(p(_{j=0}^{i}y_{j}=s)\) computed by inputs from \(p(_{j=0}^{i-1}y_{j}=s)\) weighted by \(p(y_{i}=0)\), and \(p(_{j=0}^{i-1}y_{j}=s-1)\) weighted by \(p(y_{i}=1)\) respectively, as indicated by the arrows.

count information; actually, the number of joint labelings for the set can be exponential in the number of instances. Intractable as it seems, we show that it is indeed possible to derive a tractable computation for the count probability based on a result from Ahmed et al. .

**Proposition 4.1**.: _The count probability \(p(_{i=1}^{k}_{i}=s)\) of sampling \(k\) prediction variables that sums to \(s\) from an unconstrained distribution \(p()=_{i=1}^{k}p(_{i})\) can be computed exactly in time \((ks)\). Moreover, the set \(\{p(_{i=1}^{k}_{i}=s)\}_{s=0}^{k}\) can also be computed in time \((k^{2})\)._

The above proposition can be proved in a constructive way where we show that the count probability \(p(_{i=1}^{k}_{i}=s)\) can be computed in a dynamic programming manner. We provide an illustrative example of this computation in Figure 1. In practice, we implement this computation in log space for numeric stability which we summarized as Algorithm 1, where function log1mexp provides a numerically stable way to compute \((x)=(1-(x))\) and function logsumexp a numerically stable way to compute \((x,y)=((x)+(y))\). Notice that since we show it is tractable to compute the set \(\{p(_{i=1}^{k}_{i}=s)\}_{s=0}^{k}\), for any two given label sum \(s_{1}\) and \(s_{2}\), a count probability \(p(s_{1}_{i}_{i} s_{2})\) where the count lies in an interval, can also be exactly and tractably computed. This implies that our tractable computation of count probabilities can potentially be leveraged by other count-based applications besides the three weakly supervised learning settings in the last section.

## 5 Related Work

**Weakly Supervised Learning.** Besides settings explored in our work there are many other weakly-supervised settings. One of which is semi-supervised learning, a close relative to PU Learning with the difference being that labeled samples can be both positive and negative [57; 58]. Another is label noise learning, which occurs when our instances are mislabeled. Two common variations involve whether noise is independent or dependent on the instance [20; 42]. A third setting is partial label learning, where each instance is provided a set of labels of which exactly one is true . An extension of this is partial multi-label learning, where among a set of labels, a subset is true .

**Unified Approaches.** There exists some literature in regards to "general" approaches for weakly supervised learning. One example being the method proposed in Hullermeier , which provides a procedure that minimizes the empirical risk on "fuzzy" sets of data. The paper also establishes guarantees for model identification and instance-level recognition. Co-Training and Self-Training are also examples of similar techniques that are applicable to a wide variety of weakly supervised settings [11; 49]. Self-training involves progressively incorporating more unlabeled data via our model's prediction (with pseudo-label) and then training a model on more data as an iterative algorithm . Co-Training leverages two models that have different "views" of the data and iteratively augment each other's training set with samples they deem as "well-classified". They are traditionally applied to semi-supervised learning but can extend to multiple instance learning settings [33; 47; 32].

**LLP.** Quadianto et al.  first introduced an exponential family based approach that used an estimation of mean for each class. Others seek to minimize "empirical proportion risk" or EPR as in Yu et al. , which is centered around creating an instance-level classifier that is able to reproduce the label proportions of each bag. As mentioned previously, more recent methods use bag posterior approximation and neural-based approaches [8; 43]. One such method is Proportion Loss (PL) , which we contrast to our approach. This is computed by binary cross entropy between the averaged instance-level probabilities and ground-truth bag proportion.

**MIL.** MIL finds its earlier approaches with SVMs, which have been used quite prolifically and still remain one of the most common baselines. We start with MI-SVM/mi-SVM  which are examples of transductive SVMs  that seek a stable instance classification through repeated retraining iterations. MI-SVM is an example of an instance space method , which identifies methods that classify instances as a preliminary step in the problem. This is in contrast to bag-space or embedded-space methods that omit the instance classification step. Furthermore, Wang et al.  remains one of the hallmarks of the use of neural networks for Multi-Instance Learning. Ilse et al. , utilize a similar approach but with attention-based mechanisms.

**PU learning.** Bekker and Davis  groups PU Learning paradigms into three main classes: two step, biased, and class prior incorporation. Biased learning techniques train a classifier on the entire datasetwith the understanding that negative samples are subject to noise . We will focus on a subset of biased learning techniques (Risk Estimators) as they are considered state-of-the-art and relevant to us as baselines. The Unbiased Risk Estimator (uPU) provides an alternative to the inefficiencies in manually biasing unlabeled data [18; 37]. Later, Non-negative Risk Estimator (nnPU)  accounted for weaknesses in the unbiased risk estimator such as overfitting.

**Count Loss.** To our knowledge, viewing the computation of the "bag posterior" as _probabilistic_ is new. However, the prior approaches do this implicitly. Many approaches have tried to approximate the "bag posterior" by averaging the instance-level probabilities in a bag [8; 43]. In MIL settings, among instance-level approaches, the MIL-pooling is an implicit "bag posterior" computation. These include mean, max, and log-sum-exp pooling to approximate the likelihood that a bag has at least one positive instance . But again, these are all approximations of what our computation does _exactly_. In PU Learning, to our best knowledge, the view of unlabeled data as a bag annotated with the mixture proportion is new.

**Neuro-Symbolic Losses.** In this paper, we have dealt with a specific form of distributional constraint. Conversely, there has been a plethora of work exploring the integration of _hard_ symbolic constraints into the learning of neural networks. This can take the form of enforcing a hard constraint , whereby the network's predictions are guaranteed to satisfy the pre-specified constraints. Or it can take the form of a soft constraint [48; 34; 1; 4; 2; 5] whereby the network is trained with an additional loss term that penalizes the network for placing any probability mass on predictions that violate the constraint. While in this work we focus on discrete linear inequality constraints defined over binary variables, there is existing work focusing on hybrid linear inequality constraints defined over both discrete and continuous variables and their tractability [10; 55; 54]. The development of inference algorithms for such constraints and their applications such as Bayesian deep learning remain an active topic [52; 28; 53; 51].

## 6 Experiments

In this section, we present a thorough empirical evaluation of our proposed count loss on the three weakly supervised learning problems, _LLP_, _MIL_, and _PU learning_.1 We refer the readers to the appendix for additional experimental details.

  
**Dataset** & **Dist** & **Method** & 8 & 32 & 128 & 512 \\  Adult & \([0,]\) & PL & \(0.8889 0.0024\) & \(0.8782 0.0036\) & \(\) & \(0.8678 0.0085\) \\ Adult & \([0,]\) & LMMCMCM & \(0.8728 0.0019\) & \(0.8693 0.0047\) & \(0.8669 0.0041\) & \(0.8674 0.0040\) \\ Adult & \([0,]\) & CL (Ours) & \(\) & \(\) & \(\) & \(\) \\  Adult & \([,1]\) & PL & \(0.8781 0.0038\) & \(0.8731 0.0035\) & \(\) & \(0.8556 0.0180\) \\ Adult & \([,1]\) & LMMCMCM & \(0.8584 0.0164\) & \(0.8644 0.0052\) & \(0.8601 0.0045\) & \(0.8500 0.0186\) \\ Adult & \([,1]\) & CL (Ours) & \(\) & \(\) & \(0.8675 0.0043\) & \(\) \\  Adult & \(\) & PL & \(0.8884 0.0030\) & \(0.8884 0.0008\) & \(\) & \(\) \\ Adult & \(\) & LMMCMCM & \(0.8831 0.0026\) & \(0.8819 0.0006\) & \(0.8821 0.0017\) & \(0.8786 0.0052\) \\ Adult & \(\) & CL (Ours) & \(\) & \(\) & \(0.8871 0.0021\) & \(0.8790 0.0056\) \\  Magic & \([0,]\) & PL & \(0.8900 0.0095\) & \(0.8510 0.0032\) & \(0.8405 0.0110\) & \(0.8332 0.0149\) \\ Magic & \([0,]\) & LMMCMCM & \(0.8918 0.0077\) & \(0.8799 0.0113\) & \(0.8753 0.0157\) & \(0.8734 0.0092\) \\ Magic & \([0,]\) & CL (Ours) & \(\) & \(\) & \(\) & \(\) \\  Magic & \([,1]\) & PL & \(0.9066 0.0016\) & \(0.8818 0.0108\) & \(0.8769 0.0101\) & \(0.8429 0.0443\) \\ Magic & \([,1]\) & LMMCMCM & \(0.8911 0.0083\) & \(0.8790 0.0091\) & \(0.8684 0.0046\) & \(0.8567 0.0292\) \\ Magic & \([,1]\) & CL (Ours) & \(\) & \(\) & \(\) & \(\) \\  Magic & \(\) & PL & \(0.9039 0.0029\) & \(0.8870 0.0037\) & \(0.9002 0.0092\) & \(0.8807 0.0200\) \\ Magic & \(\) & LMMCM & \(0.9070 0.0026\) & \(0.9048 0.0058\) & \(0.9113 0.0058\) & \(0.8934 0.0097\) \\ Magic & \(\) & CL (Ours) & \(\) & \(\) & \(\) & \(\) \\   

Table 3: LLP results across different bag sizes. We report the mean and standard deviation of the test AUC over \(5\) seeds for each setting. The highest metric for each setting is shown in **boldface**.

### Learning from Label Proportions

We experiment on two datasets: 1) _Adult_ with \(8192\) training samples where the task is to predict whether a person makes over \(50k\) a year or not given personal information as input; 2) _Magic Gamma Ray Telescope_ with \(6144\) training samples where the task is to predict whether the electromagnetic shower is caused by primary gammas or not given information from the atmospheric Cherenkov gamma telescope .2

We follow Scott and Zhang  where two settings are considered: one with label proportions uniformly on \([0,]\) and the other uniformly on \([,1]\). Additionally, we experiment on a third setting with label proportions distributing uniformly on \(\) which is not considered in Scott and Zhang  but is the most natural setting since the label proportion is not biased toward either \(0\) or \(1\). We experiment on four bag sizes \(n\{8,32,128,512\}\).

Count loss (CL) denotes our proposed approach using the loss objective defined in Table 2 for LLP. We compare our approach with a mutual contamination framework for LLP (LMMCM)  and against Proportion Loss (PL) .

**Results and Discussions** We show our results in Table 3. Our method showcases superior results against the baselines on both datasets and variations in bag sizes. Especially in cases with lower bag sizes, i.e., \(8,32\), CL greatly outperforms all other methodologies. Among our baselines are methods that approximate the bag posterior (PL), which we show to be less effective than optimizing the exact bag posterior with CL.

### Multiple Instance Learning

We first experiment on the MNIST dataset  and follow the MIL experimental setting in Ilse et al. : the training and test set bags are randomly sampled from the MNIST training and test set respectively; each bag can have images of digits from \(0\) to \(9\), and bags with the digit \(9\) are labeled positive. Moreover, the dataset is constructed in a balanced way such that there is an equal amount of positively and negatively labeled bags as in Ilse et al. . The task is to train a classifier that is able to predict bag labels; the more challenging task is to _discover key instances_, that is, to train a classifier that identifies images of digit \(9\). Following Ilse et al. , we consider three settings that vary in the bag generation process: in each setting, bags have their sizes generated from a normal distribution being \((10,2),(50,10),(100,20)\) respectively. The number of bags in

  
**Training Bags** & \(50\) & \(100\) & \(150\) & \(200\) & \(300\) & \(400\) & \(500\) \\  Gated Attention & \(0.775 0.034\) & \(0.894 0.012\) & \(0.935 0.005\) & \(0.939 0.006\) & **0.963 \(\) 0.002** & \(0.959 0.002\) & **0.966 \(\) 0.003** \\ Attention & \(0.807 0.026\) & **0.913 \(\) 0.006** & **0.940 \(\) 0.004** & \(0.912 0.007\) & \(0.957 0.022\) & \(0.961 0.005\) & \(0.965 0.004\) \\ CL (Ours) & **0.818 \(\) 0.024** & \(0.906 0.009\) & 0.929 \(\) 0.005 & **0.946 \(\) 0.001** & \(0.952 0.004\) & **0.962 \(\) 0.002** & \(0.963 0.002\) \\  Gated Attention & **0.943 \(\) 0.005** & \(0.949 0.009\) & **0.970 \(\) 0.005** & **0.977 \(\) 0.001** & \(0.983 0.002\) & \(0.986 0.004\) & **0.987 \(\) 0.002** \\ Attention & \(0.936 0.010\) & **0.962 \(\) 0.006** & **0.970 \(\) 0.001** & **0.977 \(\) 0.002** & **0.982 \(\) 0.003** & 0.982 \(\) 0.001 & **0.987 \(\) 0.002** \\ CL (Ours) & \(0.939 0.010\) & 0.960 \(\) 0.002 & 0.964 \(\) 0.007 & 0.972 \(\) 0.002 & **0.982 \(\) 0.003** & 0.982 \(\) 0.001 & **0.987 \(\) 0.002** \\  Gated Attention & \(0.975 0.003\) & \(0.981 0.004\) & 0.992 \(\) 0.002 & 0.987 \(\) 0.004 & **0.996 \(\) 0.001** & **0.998 \(\) 0.001** & 0.990 \(\) 0.004 \\ Attention & **0.954 \(\) 0.001** & 0.982 \(\) 0.001 & **0.996 \(\) 0.000** & 0.887 \(\) 0.007 & 0.992 \(\) 0.004 & 0.994 \(\) 0.002 & 0.998 \(\) 0.000 \\ CL (Ours) & \(0.981 0.007\) & **0.989 \(\) 0.000** & **0.996 \(\) 0.002** & **0.995 \(\) 0.001** & **0.996 \(\) 0.002** & \(0.993 0.003\) & **0.999 \(\) 0.001** \\   

Table 4: MIL experiment on the MNIST dataset. Each block represents a different distribution from which we draw bag sizesâ€”First Block: \((10,2)\), Second Block: \((50,10)\), Third Block: \((100,20)\). We run each experiment for \(3\) runs and report mean test AUC with standard error. The highest metric for each setting is shown in **boldface**.

  
**Method** & **Accuracy** & **AUC** & **F1** & **Precision** & **Recall** \\  Gated Attention & \(0.909 0.014\) & \(0.908 0.013\) & \(0.886 0.021\) & \(0.916 0.020\) & \(0.879 0.020\) \\ Attention & \(0.893 0.015\) & \(0.890 0.008\) & \(0.876 0.017\) & \(0.908 0.016\) & \(0.879 0.018\) \\  CL (Ours) & **0.915 \(\) 0.008** & **0.912 \(\) 0.010** & **0.903 \(\) 0.010** & **0.936 \(\) 0.014** & **0.898 \(\) 0.007** \\   

Table 5: MIL: We report mean test accuracy, AUC, F1, precision, and recall averaged over \(5\) runs with std. error on the Colon Cancer dataset. The highest value for each metric is shown in **boldface**.

training set \(n\) is in \(\{50,100,150,200,300,400,500\}\). Thus, we have \(3 7=21\) settings in total. Additionally, we introduce experimental analysis on _how the performance of the learning methods would degrade as the number of bags and total samples in training set decreases_, by modulating the number of training bags \(n\) to be \(\{10,20,30,40\}\) and selecting bag sizes from \((5,1)\) and \((10,2)\).

We also experiment on the Colon Cancer dataset  to simulate a setting where bag instances are not independent. The dataset consists of \(100\) total hematoxylin-eosin (H&E) stained images, each of which contains images of cell nuclei that are classified as one of: epithelial, inflammatory, fibroblast, and miscellaneous. Each image represents a bag and instances are \(27 27\) patches extracted from the original image. A positively labeled bag or image is one that contains the epithelial nuclei. For both datasets, we include the Attention and Gated Attention mechanism  as baselines. We also use the MIL objective defined in Table 2.

**Results and Discussions** For the MNIST experiments, CL is able to outperform all other baselines or exhibit highly comparable performance for bag-level predictions as shown in Table 4. A more interesting setting is to compare how robust the learning methods are if the number of training bags decreases. Wang et al.  claim that instance-level classifiers tend to lose against embedding-based methods. However, we show in our experiment that this is not true in all cases as seen in Figure 2. While Attention and Gated Attention are based on embedding, they suffer from a more severe drop in predictive performance than CL when the number of training bags drops from \(40\) to \(10\); our method shows great robustness and consistently outperforms all baselines. The rationale we provide is that with a lower number of training instances, we need more supervision over the limited samples we have. Our constraint provides this additional supervision, which accounts for the difference in performance.

We provide an additional investigation in Figure 3 to show that our approach learns effectively and delivers accurate instance-level predictions under bag-level supervision. In Figure 3, we can see that even though the classifier is trained on feedback about whether a bag contains the digit \(9\) or not, it accurately discovers all images of digit \(9\). To reinforce this, Table 7 and Table 8, in Appendix B, show that our approach outperforms existing instance-space methods on instance-level classification.

Our experimental results on the Colon Cancer dataset are shown in Table 5. We show that both our proposed objectives are able to consistently outperform baseline methods on all metrics. Interestingly, we do not expect CL to perform well when instances in a bag are dependent; however, the results indicate that our count loss is robust to these settings.

Figure 3: A test bag from our MIL experiments, where we set only the digit \(9\) as a positive instance. Highlighted in red are digits identified to be positive with corresponding probability beneath.

Figure 2: MIL MNIST dataset experiments with decreased numbers of training bags and lower bag size. Left: bag sizes sampled from \((10,2)\); Right: bag sizes sampled from \((5,1)\). We plot the mean test AUC (aggregated over \(3\) trials) with standard errors for \(4\) bag sizes. Best viewed in color.

### Learning from Positive and Unlabeled Data

We experiment on dataset MNIST and CIFAR-10 , following the four simulated settings from Garg et al. : 1) Binarized MNIST: the training set consist of images of digits \(0-9\) and images with digits in range \(\) are positive instances while others as negative; 2) MNIST17: the training set consist of images of digits \(1\) and \(7\) and images with digit \(1\) are defined as positive while \(7\) as negative; 3) Binarized CIFAR: the training set consists of images from ten classes and images from the first five classes is defined as positive instances while others as negative; 4) CIFAR Cat vs. Dog: the training set consist of images of cats and dogs and images of cats are defined as positive while dogs as negative. The mixture proportion is \(0.5\) in all experiments. The performance is evaluated using the accuracy on a test set of unlabeled data.

As shown in Table 2, we propose two objectives for PU learning. Our first objective is denoted by CL whereas the second approach is denoted by CL-expect. We compare against the Conditional Value Ignoring Risk approach (CVIR) , nnPU , and uPU .

**Results and Discussions** Accuracy results are presented in Table 6 where we can see that our proposed methods perform better than baselines on \(3\) out of the \(4\) simulated PU learning settings. CL-expect builds off a similar "exactly-k" count approach, which we have shown to work well in the label proportion setting. The more interesting results are from CL where we fully leverage the information from a distribution as supervision instead of simply using the expectation. We think of this as applying a loss on each count weighted by their probabilities from the binomial distribution. We provide further evidence that our proposed count loss effectively guides the classifier towards predicting a binomial distribution as shown in Figure 4: we plot the count distributions predicted by CL and CVIR as well as the ground-truth binomial distribution. We can see that CL is able to generate the expected distribution, proving the efficacy of our approach.

## 7 Conclusions

In this paper, we present a unified approach to several weakly-supervised tasks, i.e., LLP, MIL, PU. We construct our approach based on the idea of using weak labels to constrain count-based probabilities computed from model outputs. A future direction for our work can be to extend to multi-class classification as well as explore the applicability to other weakly-supervised settings, e.g. label noise learning, semi-supervised learning, and partial label learning [15; 36; 58].

  
**Dataset** & **Network** & **CL-expect (Ours)** & **CL (Ours)** & **CVIR** & **nnPU** & **nPU** \\  Binarized MNIST & MLP & \(95.9 0.15\) & **96.4 \(\) 0.01** & \(96.3 0.07\) & \(96.1 0.14\) & \(95.2 0.19\) \\ MNIST17 & MLP & \(98.7 0.17\) & **99.0 \(\) 0.19** & \(98.7 0.09\) & \(98.4 0.20\) & \(98.4 0.09\) \\ Binarized CIFAR & ResNet & \(79.2 0.27\) & \(80.1 0.34\) & **82.3 \(\) 0.18** & \(77.2 1.03\) & \(76.7 0.74\) \\ CIFAR Cat vs. Dog & ResNet & **76.5 \(\) 1.86** & \(74.8 1.64\) & \(73.3 0.94\) & \(71.8 0.33\) & \(68.8 0.53\) \\   

Table 6: PU Learning: We report accuracy and standard deviation on a test set of unlabeled data, which is aggregated over \(3\) runs. The results from CVIR, nnPU, and uPU are aggregated over \(10\) epochs, as in Garg et al. , while we choose the single best epoch based on validation for our approaches. The highest metric for each setting is shown in **boldface**.

Figure 4: MNIST17 setting for PU Learning: We compute the average discrete distribution for CL and CVIR, over \(5\) test bags, each of which contain \(100\) instances. A ground truth binomial distribution of counts is also shown.