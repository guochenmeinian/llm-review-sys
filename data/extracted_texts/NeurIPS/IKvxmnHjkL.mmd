# Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks

Jiayuan Ye\({}^{}\), Zhenyu Zhu\({}^{}\), Fanghui Liu\({}^{}\), Reza Shokri\({}^{}\), Volkan Cevher\({}^{}\)

\({}^{}\) National University of Singapore \({}^{}\) EPFL, Switzerland \({}^{}\) University of Warwick

\({}^{}\){jiayuan,reza}@comp.nus.edu.sg \({}^{}\){zhenyu.zhu, volkan.cevher}@epfl.ch

\({}^{}\)fanghui.liu@warwick.ac.uk

Work was done while Fanghui was at LIONS, EPFL.

###### Abstract

We analytically investigate how over-parameterization of models in randomized machine learning algorithms impacts the information leakage about their training data. Specifically, we prove a privacy bound for the KL divergence between model distributions on worst-case neighboring datasets, and explore its dependence on the initialization, width, and depth of fully connected neural networks. We find that this KL privacy bound is largely determined by the expected squared gradient norm relative to model parameters during training. Notably, for the special setting of linearized network, our analysis indicates that the squared gradient norm (and therefore the escalation of privacy loss) is tied directly to the per-layer variance of the initialization distribution. By using this analysis, we demonstrate that privacy bound improves with increasing depth under certain initializations (LeCun and Xavier), while degrades with increasing depth under other initializations (He and NTK). Our work reveals a complex interplay between privacy and depth that depends on the chosen initialization distribution. We further prove excess empirical risk bounds under a fixed KL privacy budget, and show that the interplay between privacy utility trade-off and depth is similarly affected by the initialization.

## 1 Introduction

Deep neural networks (DNNs) in the over-parameterized regime (i.e., more parameters than data) perform well in practice but the model predictions can easily leak private information about the training data under inference attacks such as membership inference attacks  and reconstruction attacks [17; 7; 29]. This leakage can be mathematically measured by the extent to which the algorithm's output distribution changes if DNNs are trained on a neighboring dataset (differing only in a one record), following the differential privacy (DP) framework .

To train differential private model, a typical way is to randomly perturb each gradient update in the training process, such as stochastic gradient descent (SGD), which leads to the most widely applied DP training algorithm in the literature: DP-SGD . To be specific, in each step, DP-SGD employs gradient clipping, adds calibrated Gaussian noise, and yields differential privacy guarantee that scales with the noise multiplier (i.e., per-dimensional Gaussian noise standard deviation divided by the clipping threshold) and number of training epochs. However, this privacy bound  is overly general as its gradient clipping artificially neglects the network properties (e.g., width and depth) and training schemes (e.g., initializations). Accordingly, a natural question arises in the community:

_How does the over-parameterization of neural networks (under different initializations) affect the privacy bound of the training algorithm over_ worst-case _datasets?_To answer this question, we circumvent the difficulties of analyzing gradient clipping, and instead _algorithmically_ focus on analyzing privacy for the Langevin diffusion algorithm _without_ gradient clipping nor Lipschitz assumption on loss function. 2 It avoids an artificial setting in DP-SGD  where a constant sensitivity constraint is enforced for each gradient update and thus makes the privacy bound insensitive to the network over-parameterization. _Theoretically_, we prove that the KL privacy loss for Langevin diffusion scales with the expected gradient difference between the training on any two worst-case neighboring datasets (Theorem 3.1). 3 By proving precise upper bounds on the expected \(_{2}\)-norm of this gradient difference, we thus obtain KL privacy bounds for fully connected neural network (Lemma 3.2) and its linearized variant (Corollary 4.2) that changes with the network width, depth and per-layer variance for the initialization distribution. We summarized the details of our KL privacy bounds in Table 1, and highlight our key observations below.

* Width always worsen privacy, under all the considered initialization schemes. Meanwhile, the interplay between network depth and privacy is much more complex and crucially depends on which initialization scheme is used and how long the training time is.
* Regarding the specific initialization schemes, under small per-layer variance in initialization (e.g. in LeCun and Xavier), if the depth is large enough, our KL privacy bound for training fully connected network (with a small amount of time) as well as linearized network (with finite time) decays exponentially with increasing depth. To the best of our knowledge, this is the first time that an improvement of privacy bound under over-parameterization is observed.

We further perform numerical experiments (Section 5) on deep neural network trained via noisy gradient descent to validate our privacy analyses. Finally, we analyze the privacy utility trade-off for training linearized network, and prove that the excess empirical risk bound (given any fixed KL privacy budget) scales with a lazy training distance bound \(R\) (i.e., how close is the initialization to a minimizer of the empirical risk) and a gradient norm constant \(B\) throughout training (Corollary 6.4). By analyzing these two terms precisely, we prove that under certain initialization distributions (such as LeCun and Xavier), the privacy utility trade-off strictly improves with increasing depth for linearized network (Table 1). To our best knowledge, this is the first time that such a gain in privacy-utility trade-off due to over-parameterization (increasing depth) is shown. Meanwhile, prior results only prove (nearly) dimension-independent privacy utility trade-off for such linear models in the literature [45; 32; 37]. Our improvement demonstrates the unique benefits of our algorithmic framework and privacy-utility analysis in understanding the effect of over-parameterization.

   Initialization &  Variance \(_{l}\) \\ for layer \(l\) \\  &  Gradient norm \\ constant \(B\) (7) \\  &  Approximate lazy \\ training distance \\ \(R\) (9) \\  & 
 Excess Empirical risk \\ under \(\)-KL privacy \\ (Corollary 6.4) \\  \\  LeCun  & \(1/m_{l-1}\) & \(}}}$}}}{}}^{L-1}}}{}}{}}}$}}\) & \(}(})\) & \(}(}+}})\) \\  He  & \(2/m_{l-1}\) & \(}}$}}(L-1+)\) & \(}(})\) & \(}(}+})\) \\  NTK  & \(2/m_{l},\,l<L\\ 1/o,\,l=L\) & \(dm(+)\) & \(}(})\) & \(}(}+})\) \\  Xavier  & \(+m_{l}}\) & \(}}$}}(L-1+)}{2^{L-3}(1+ )(1+)}\) & \(}()(1+)}{ L-1+})\) & \(}(}+}})\) \\   

Table 1: Our privacy utility trade-off bounds for training linearized network (3) via Langevin diffusion, under different hidden-layer width \(m\), depth \(L\) and initializations. The per-layer widths \(m_{0}=d\), \(m_{1},,m_{L-1}=m\) and \(m_{L}=o\) where \(d\) is the data dimension and \(o\) is number of classes. For KL privacy bounds, we assume Assumption 2.2 holds and \(L 2\) for simplicity. For the excess risk bounds, we assume \(o=1\), \(d,m=(n)\) are large, and Assumption 2.2. Under LeCun and Xavier, we prove privacy utility trade-offs that improve with over-parameterization (increasing depth).

### Related Works

over-parameterization in DNNs and NTK.Theoretical demonstration on the benefit of over-parameterization in DNNs occurs in global convergence [3; 21] and generalization [4; 16]. Under proper initialization, the training dynamics of over-parameterized DNNs can be described by a kernel function, termed as neural tangent kernel (NTK) , which stimulates a series of analysis in DNNs. Accordingly, over-parameterization has been demonstrated to be beneficial/harmful to several topics in deep learning, e.g., robustness [15; 54], covariate shift . However, the relationship between over-parameterization and privacy (based on the differential privacy framework) remains largely an unsolved problem, as the training dynamics typically change  after adding new components in the privacy-preserving learning algorithm (such as DP-SGD ) to enforce privacy constraints.

Membership inference privacy risk under over-parameterization.A recent line of works [47; 48] investigates how over-parameterization affects the theoretical and empirical privacy in terms of membership inference advantage, and proves novel trade-off between privacy and generalization error. These literature are closet to our objective of investigating the interplay between privacy and over-parameterization. However, Tan et al. [47; 48] focus on proving upper bounds for an average-case privacy risk defined by the advantage (relative success) of membership inference attack on models trained from randomly sampled training dataset from a population distribution. By contrast, our KL privacy bound is heavily based on the strongest adversary model in the differential privacy definition, and holds under an arbitrary _worst-case_ pair of neighboring datasets, differing only in one record. Our model setting (e.g., fully connected neural networks) is also quite different from that of Tan et al. [47; 48]. The employed analysis tools are accordingly different.

Differentially private learning in high dimensionStandard results for private empirical risk minimization [9; 46] and private stochastic convex optimization [11; 12; 5] prove that there is an unavoidable factor \(d\) in the empirical risk and population risk that depends on the model dimension. However, for unconstrained optimization, it is possible to seek for the dimension-dependency in proving risk bounds for certain class of problems (such as generalized linear model ). Recently, there is a growing line of works that proves dimension-independent excess risk bounds for differentially private learning, by utilizing the low-rank structure of data features  or gradient matrices [32; 37] during training. Several follow-up works [33; 13] further explore techniques to enforce the low-rank property (via random projection) and boost privacy utility trade-off. However, all the works focus on investigating a general high-dimensional problem for private learning, rather than separating the study for different network choices such as width, depth and initialization. Instead, our study focuses on the fully connected neural network and its linearized variant, which enables us to prove more precise privacy utility trade-off bounds for these particular networks under over-parameterization.

## 2 Problem and Methodology

We consider the following standard multi-class supervised learning setting. Let \(=(_{1},,_{n})\) be an input dataset of size \(n\), where each data record \(_{i}=(_{i},_{i})\) contains a \(d\)-dimensional feature vector \(_{i}^{d}\) and a label vector \(_{i}=\{-1,1\}^{o}\) on \(o\) classes. We aim to learn a neural network output function \(_{}():\) parameterized by \(\) via empirical risk minimization (ERM)

\[_{}(;):=_{i=1}^{n}( _{}(_{i});_{i})\,,\] (1)

where \((_{}(_{i});_{i})\) is a loss function that reflects the approximation quality of model prediction \(f_{}(_{i})\) compared to the ground truth label \(_{i}\). For simplicity, throughout our analysis, we employ the cross-entropy loss \((_{}();)=-,(_{}())\) for multi-class network with \(o 2\), and \((_{}();)=(1+(-_{}())\) for single-output network with \(o=1\).

Fully Connected Neural NetworksWe consider the \(L\)-layer, multi-output, fully connected, deep neural network (DNN) with ReLU activation. Denote the width of hidden layer \(l\) as \(m_{l}\) for \(l=1,,L-1\). For consistency, we also denote \(m_{0}=d\) and \(m_{L}=o\). The network output \(f_{}()_{L}()\) is defined recursively as follows.

\[_{0}()=;_{l}()=(_{l})l=1,,L-1;_{L}()=_{L}_{L-1}()\,,\] (2)where \(h_{l}()\) denotes the post-activation output at \(l\)-th layer, and \(\{_{l}^{m_{l} m_{l-1}}:l=1,,L\}\) denotes the set of per-layer weight matrices of DNN. For brevity, we denote the vector \(((_{1}),,(_{L}))^{m_{1} d+m_{2} m_{1}++o m_{L-1}}\), i.e., the the concatenation of vectorizations for weight matrices of all layers, as the model parameter.

Linearized NetworkWe also analyze the following _linearized network_, which is used in prior works [35; 3; 41] as an important tool to (approximately and qualitatively) analyze the training dynamics of DNNs. Formally, the linearized network \(_{}^{lin,0}()\) is a first-order Taylor expansion of the fully connected ReLU network at initialization parameter \(_{0}^{lin}\), as follows.

\[_{}^{lin,0}()_{_{0}^{lin}}()+_{}()}{}_{=_{0}^{ lin}}(-_{0}^{lin}),\] (3)

where \(_{_{0}^{lin}}()\) is the output function of the fully connected ReLU network (2) at initialization \(_{0}^{lin}\). We denote \(_{0}^{lin}(;)=_{i=1}^{n}( _{_{0}^{lin}}(_{i})+_{}()}{ }|_{=_{0}^{lin}}(-_{0}^{lin});_{i})\) as the empirical loss function for training linearized network, by plugging (3) into (1).

Langevin DiffusionRegarding the optimization algorithm, we focus on the _Langevin diffusion_ algorithm  with per-dimensional noise variance \(^{2}\). Note that we aim to _avoid gradient clipping_ while still proving KL privacy bounds. After initializing the model parameters \(_{0}\) at time zero, the model parameters \(_{t}\) at subsequent time \(t\) evolves as the following stochastic differential equation.

\[_{t}=-\,(_{t};)t+ }_{t}\,.\] (4)

Initialization DistributionThe initialization of parameters \(_{0}\) crucially affects the convergence of Langevin diffusion, as observed in prior literatures [52; 25; 24]. In this work, we investigate the following general class of Gaussian initialization distributions with different (possibly depth-dependent) variances for the parameters in each layer. For any layer \(l=1,,L\), we have

\[[^{l}]_{ij}(0,_{l})(i,j)[m_{l}][m_{l-1}]\,,\] (5)

where \(_{1},,_{L}>0\) are the per-layer variance for Gaussian initialization. By choosing different variances, we recover many common initialization schemes in the literature, as summarized in Table 1.

### Our objective and methodology

We aim to understand the relation between privacy, utility and over-parameterization (depth and width) for the Langevin diffusion algorithm (under different initialization distributions). For privacy analysis, we prove a KL privacy bound for running Langevin diffusion on any two _worst-case_ neighboring datasets. Below we first give the definition for neighboring datasets.

**Definition 2.1**.: We denote \(\) and \(^{}\) as neighboring datasets if they are of same size and only differ in one record. For brevity, we also denote the differing records as \((,)\) and \((^{},^{})^{}\).

**Assumption 2.2** (Bounded Data).: For simplicity, we assume bounded data, i.e., \(\|\|_{2}\).

We now give the definition for KL privacy, which is a more relaxed, yet closely connected privacy notion to the standard \((,)\) differential privacy , see Appendix A.2 for more discussions. KL privacy and its relaxed variants are commonly used in previous literature [8; 10; 53].

**Definition 2.3** (KL privacy).: A randomized algorithm \(\) satisfies \(\)-KL privacy if for any neighboring datasets \(\) and \(^{}\), we have that the KL divergence \((()\|(^{}))\), where \(()\) denotes the algorithm's output distribution on dataset \(\).

In this paper, we prove KL privacy upper bound for \(_{,^{}}(_{[0:T]}\|_{[0:T]} ^{})\) when running Langevin diffusion on any _worst-case_ neighboring datasets. For brevity, here (and in the remaining paper), we abuse the notations and denote \(_{[0:T]}\) and \(_{[0:T]}^{}\) as the distributions of model parameters trajectory during Langevin diffusion processes Eq. (4) with time \(T\) on \(\) and \(^{}\) respectively.

For utility analysis, we prove the upper bound for the excess empirical risk given any fixed KL divergence privacy budget for a single-output neural network under the following additional assumption (it is only required for utility analysis and not needed for our privacy bound).

**Assumption 2.4** ([40; 20; 21]).: The training data \(_{1},,_{n}\) are i.i.d. samples from a distribution \(P_{x}\) that satisfies \([]=0,\|\|_{2}=\) for \( P_{x}\), and with probability one for any \(i j\), \(_{i}_{j}\).

Our ultimate goal is to precisely understand how the excess empirical risk bounds (given fixed KL privacy budget) are affected by increasing width and depth under different initialization distributions.

## 3 KL Privacy for Training Fully Connected ReLU Neural Networks

In this section, we perform the composition-based KL privacy analysis for Langevin Diffusion given random Gaussian initialization distribution under Eq. (5) for fully connected ReLU network. More specifically, we prove upper bound for the KL divergence between distribution of output model parameters when running Langevin diffusion on an arbitrary pair of neighboring datasets \(\) and \(^{}\).

Our first insight is that by a Bayes rule decomposition for density function, KL privacy under a relaxed gradient sensitivity condition can be proved (that could hold _without_ gradient clipping).

**Theorem 3.1** (KL composition under possibly unbounded gradient difference).: _The KL divergence between running Langevin diffusion (4) for DNN (2) on neighboring datasets \(\) and \(^{}\) satisfies_

\[(_{[0:T]}\|_{[0:T]}^{})=}_{0}^{T}[\|(_{t}; )-(_{t};^{})\|_{2}^{2} ]t\,.\] (6)

Proof sketch.: We compute the partial derivative of KL divergence with regard to time \(t\), and then integrate it over \(t[0,T]\) to compute the KL divergence during training with time \(T\). For computing the limit of differentiation, we use Girsanov's theorem to compute the KL divergence between the trajectory of Langevin diffusion processes on \(\) and \(^{}\). The complete proof is in Appendix B.1.

Theorem 3.1 is an extension of the standard additivity  of KL divergence (also known as chain rule ) for a finite sequence of distributions to continuous time processes with (possibly) unbounded drift difference. The key extension is that Theorem 3.1 does not require bounded sensitivity between the drifts of Langevin Diffusion on neighboring datasets. Instead, it only requires finite second-order moment of drift difference (in the \(_{2}\)-norm sense) between neighboring datasets \(,^{}\), which can be proved by the following Lemma. We prove that this expectation of squared gradient difference incurs closed-form upper bound under deep neural network (under mild assumptions), for running Langevin diffusion (without gradient clipping) on any neighboring dataset \(\) and \(^{}\).

**Lemma 3.2** (Drift Difference in Noisy Training).: _Let \(M_{T}\) be the subspace spanned by gradients \(\{(_{_{t}}(_{i};_{i}):(_{i},_{i }),t[0,T]\}_{i=1}^{n}\) throughout Langevin diffusion \((_{t})_{t[0,T]}\). Denote \(\|\|_{M_{T}}\) as the \(_{2}\) norm of the projected input vector onto \(M_{T}\). Suppose that there exists constants \(c,>0\) such that for any \(\), \(^{}\) and \((,)\), we have \(\|(f_{}();)-(f_{^{}}( );)\|_{2}<\{c,\|-^{}\|_{M_{T}}\}\). Then running Langevin diffusion Eq. (4) with Gaussian initialization distribution (5) satisfies \(\)-KL privacy with \(=,^{}}_{0}^{T}[\| (_{t};)-(_{t}; ^{})\|_{2}^{2}]t}{2^{2}}\) where_

\[+}{n^{2}(2+^{2})}()T}-1}{2+^{2}}-T)([ \|(_{0};)\|_{2}^{2}]+2^{2} (M_{T})+c^{2})}_{}+T}{n^{2}}}_{}.\]

Proof sketch.: The key is to reduce the problem of upper bounding the gradient difference at any training time \(T\), to analyzing its two subcomponents: \(\|(f_{_{t}}();))-(f_{_{t}}(^{});^{})\|_{2}^{2} _{0}}();) \|-(f_{_{0}}(^{});^{})\| _{2}^{2}}_{}+2^{2}_{t}- _{0}\|_{M_{T}}^{2}}_{}+2c^{2}\), where \((,)\) and \((^{},^{})\) are the differing data between neighboring datasets \(\) and \(^{}\). This inequality is by the Cauchy-Schwartz inequality. In this way, the second term in Lemma 3.2 uses the change of parametersto bound the gradient difference between datasets \(\) and \(^{}\) at time \(T\), via the relaxed smoothness assumption of loss function (that is explained in Remark 3.5 in details). The complete proof is in Appendix B.2.

_Remark 3.3_ (Gradient difference at initialization).: The first term and in our upper bound linearly scales with the difference between gradients on neighboring datasets \(\) and \(^{}\) at initialization. Under different initialization schemes, this gradient difference exhibits different dependency on the network depth and width, as we will prove theoretically in Theorem 4.1.

_Remark 3.4_ (Gradient difference fluctuation during training).: The second term in Lemma 3.2 bounds the change of gradient difference during training, and is proportional to the the rank of a subspace \(M_{T}\) spanned by gradients of all training data. Intuitively, this fluctuation is because Langevin diffusion adds per-dimensional noise with variance \(^{2}\), thus perturbing the training parameters away from the initialization at a scale of \(O((M_{T})})\) in the expected \(_{2}\) distance.

_Remark 3.5_ (Relaxed smoothness of loss function).: The third term in Lemma 3.2 is due to the assumption \(\|(f_{}();)\|_{2}<\{c,||-^{ }||_{M_{T}}\}\). This assumption is similar to smoothness of loss function, but is more relaxed as it allows non-smoothness at places where the gradient is bounded by \(c\). Therefore, this assumption is general to cover commonly-used smooth, non-smooth activation functions, e.g., sigmoid, ReLU.

_Growth of KL privacy bound with increasing training time \(T\)._ The first and third terms in our upper bound Lemma 3.2 grow linearly with the training time \(T\), while the second term grows exponentially with regard to \(T\). Consequently, for learning tasks that requires a long training time to converge, the second term will become the dominating term and the KL privacy bound suffers from exponential growth with regard to the training time. Nevertheless, observe that for small \(T 0\), the second component in Lemma 3.2 contains a small factor \()T}-1}{2+^{2}}-T=o(T)\) by Taylor expansion.Therefore, for small training time, the second component is smaller than the first and the third components in Lemma 3.2 that linearly scale with \(T\), and thus does not dominate the privacy bound. Intuitively, this phenomenon is related to lazy training . In Section 5 and Figure 2, we also numerically validate that the second component does not have a high effect on the KL privacy loss in the case of small training time.

_Dependence of KL privacy bound on network over-parameterization_. Under a fixed training time \(T\) and noise scale \(^{2}\), Lemma 3.2 predicts that the KL divergence upper bound in Theorem 3.1 is dependent on the gradient difference and gradient norm at initialization, and the rank of gradient subspace \((M_{T})\) throughout training. We now discuss the how these two terms change under increasing width and depth, and whether there are possibilities to improve them under over-parameterization.

1. The gradient norm at initialization crucially depends on how the per-layer variance in the Gaussian initialization distribution scales with the network width and depth. Therefore, it is possible to reduce the gradient difference at initialization (and thus improve the KL privacy bound) by using specific initialization schemes, as we later show in Section 4 and Section 5.
2. Regarding the rank of gradient subspace \((M_{T})\): when the gradients along the training trajectory span the whole optimization space, \((M_{T})\) would equal the dimension of the learning problem. Consequently, the gradient fluctuation upper bound (and thus the KL privacy bound) worsens with increasing number of model parameters (over-parameterization) in the worst-case. However, if the gradients are low-dimensional [45; 32; 43] or sparse , \((M_{T})\) could be dimension-independent and thus enables better bound for gradient fluctuation (and KL privacy bound). We leave this as an interesting open problem.

## 4 KL privacy bound for Linearized Network under over-parameterization

In this section, we focus on the training of linearized networks (3), which fosters a refined analysis on the interplay between KL privacy and over-parameterization (increasing width and depth). Analysis of DNNs via linearization is a commonly used technique in both theory  and practice [43; 41]. We hope our analysis for linearized network serves as an initial attempt that would open a door to theoretically understanding the relationship between over-parameterization and privacy.

To derive a composition-based KL privacy bound for training a linearized network, we apply Theorem 3.1 which requires an upper bound for the norm of gradient difference between the training processes on neighboring datasets \(\) and \(^{}\) at any time \(t\). Note that the empirical risk function for training linearized models enjoys convexity, and thus a relatively short amount of training time is enough for convergence. In this case, intuitively, the gradient difference between neighboring datasets does not change a lot during training, which allows for a tighter upper bound for the gradient difference norm for linearized networks (than Lemma 3.2).

In the following theorem, we prove that for a linearized network, the gradient difference throughout training has a uniform upper bound that only depends on the network width, depth and initialization.

**Theorem 4.1** (Gradient Difference throughout training linearized network).: _Under Assumption 2.2, taking over the randomness of the random initialization and the Brownian motion, for any \(t[0,T]\), running Langevin diffusion on a linearized network in Eq. (3) satisfies that_

\[[\|(_{t}^{lin};)-(_{t}^{lin};^{})\|_{2}^{2}]} \,,B d o(_{i=1}^{L-1}m_{i}}{2} )_{l=1}^{L}}{_{l}}\,,\] (7)

_where \(n\) is the training dataset size, and \(B\) is a constant that only depends on the data dimension \(d\), the number of classes \(o\), the network depth \(L\), the per-layer network width \(\{m_{i}\}_{i=1}^{L}\), and the per-layer variances \(\{_{i}\}_{i=1}^{L}\) of the Gaussian initialization distribution._

Theorem 4.1 provides a precise analytical upper bound for the gradient difference during training linearized network, by tracking the gradient distribution for fully connected feed-forward ReLU network with Gaussian weight matrices. Our proof borrows some techniques from  for computing the gradient distribution, refer to Appendix C.1 and C.2 for the full proofs. By plugging Eq. (7) into Theorem 3.1, we obtain the following KL privacy bound for training a linearized network.

**Corollary 4.2** (KL privacy bound for training linearized network).: _Under Assumption 2.2 and neural networks (3) initialized by Gaussian distribution with per-layer variance \(\{_{i}\}_{i=1}^{L}\), running Langevin diffusion for linearized network with time \(T\) on any neighboring datasets satisfies that_

\[(_{[0:T]}^{lin}\|_{[0:T]}^{lin}) ^{2}}\,,\] (8)

_where \(B\) is the constant that specifies the gradient norm upper bound, given by Eq. (7)._

Over-parameterization affects privacy differently under different initialization.Corollary 4.2 and Theorem 4.1 prove the role of over-parameterization in our KL privacy bound, crucially depending on how the per-layer Gaussian initialization variance \(_{i}\) scales with the per-layer network width \(m_{i}\) and depth \(L\). We summarize our KL privacy bound for the linearized network under different width, depth and initialization schemes in Table 1, and elaborate the comparison below.

**(1) LeCun initialization** uses small, width-independent variance for initializing the first layer \(_{1}=\) (where \(d\) is the number of input features), and width-dependent variance \(_{2}==_{L}=\) for initializing all the subsequent layers. Therefore, the second term \(_{l=1}^{L}}{_{l}}\) in the constant \(B\) of Eq. (7) increases linearly with the width \(m\) and depth \(L\). However, due to \(_{l}}{2}<1\) for all \(l=2,,L\), the first product term \(_{l=1}^{L-1}m_{l}}{2}\) in constant \(B\) decays with the increasing depth. Therefore, by combining the two terms, we prove that the KL privacy bound worsens with increasing width, but improves with increasing depth (as long as the depth is large enough). Similarly, under **Xavier initialization**\(_{l}=}{m_{l-1}+m_{l}}\), we prove that the KL privacy bound (especially the constant \(B\) (7)) improves with increasing depth as long as the depth is large enough.

**(2) NTK and He initializations** use large per-layer variance \(_{l}=}&l=1,,L-1\\ &l=L\) (for NTK) and \(_{l}=}\) (for He). Consequently, the gradient difference under NTK or He initialization is significantly larger than that under LeCun initialization. Specifically, the gradient norm constant \(B\) in Eq. (7) grows linearly with the width \(m\) and the depth \(L\) under He and NTK initializations, thus indicating a worsening of KL privacy bound under increasing width and depth.

## 5 Numerical validation of our KL privacy bounds

To understand the relation between privacy and over-parameterization in _practical_ DNNs training (and to validate our KL privacy bounds Lemma 3.2 and Corollary 4.2), we perform experiments for DNNs training via noisy GD to numerically estimate the KL privacy loss. We will show that if the total training time is small, it is indeed possible to obtain numerical KL privacy bound estimates that does not grow with the total number of parameter (under carefully chosen initialization distributions).

_Numerical estimation procedure_. Theorem 3.1 proves that the exact KL privacy loss scales with the expectation of squared gradient norm during training. This could be estimated by empirically average of gradient norm across training runs. For training dataset \(\), we consider all 'car' and 'plane' images of the CIFAR-10. For neighboring dataset, we consider all possible \(^{}\) that removes a record from \(\), or adds a test record to \(\), i.e., the standard "add-or remove-one" neighboring notion . We run noisy gradient descent with constant step-size \(0.01\) for \(50\) epochs on both datasets.

_Numerically validate the growth of KL privacy loss with regard to training time_. Figure 1 shows numerical KL privacy loss under different initializations, for fully connected networks with width \(1024\) and depth \(10\). We observe that the KL privacy loss grows linearly at the beginning of training (\(<10\) epochs), which validates the first and third term in the KL privacy bound Lemma 3.2. Moreover, the KL privacy loss under LeCun and Xavier initialization is close to zero at the beginning of training (\(<10\) epochs). This shows LeCun and Xavier initialization induce small gradient norm at small training time, which is consistent with Theorem 4.1. However, when the number of epochs is large, the numerical KL privacy loss grows faster than linear accumulation under all initializations, thus validating the second term in Lemma 3.2.

_Numerically validate the dependency of KL privacy loss on network width, depth and initializations_. Figure 2 shows the numerical KL privacy loss under different network depth, width and initializations, for a fixed training time. In Figure 1(c), we observe that increasing width and training time always increases KL privacy loss. This is consistent with Theorem 4.1, which shows that increasing width worsens the gradient norm at initialization (given fixed depth), thus harming KL privacy bound Lemma 3.2 at the beginning of training. We also observe that the relationship between KL privacy

Figure 1: Numerically estimated KL privacy loss for noisy GD with constant step-size \(0.001\) on deep neural network with width \(1024\) and depth \(10\). We report the mean and standard deviation across \(6\) training runs, taking worst-case over all neighboring datasets. The numerical KL privacy loss grows with the number of training epochs under all initializations. The growth rate is close to linear at beginning of training (epochs \(<10\)) and is faster than linear at epochs \( 10\).

Figure 2: Numerically estimated KL privacy loss for noisy GD with constant step-size on fully connected ReLU network with different width, depth and initializations. We report the mean and standard deviation across \(6\) training runs, taking worst-case over all neighboring datasets. Under increasing width, the KL privacy loss always grows under all evaluated initializations. Under increasing depth, at the beginning of training (20 epochs), the KL privacy loss worsens with depth under He initialization, but first worsens with depth (\( 8\)) and then improves with depth (\( 8\)) under Xavier and LeCun initializations. At later phases of the training (50 epochs), KL privacy worsens (increases) with depth under all evaluated initializations.

and network depth depends on the initialization distributions and the training time. Specifically, in Figure 1(a), when the training time is small (20 epochs), for LeCun and Xavier initializations, the numerical KL privacy loss improves with increasing depth when depth \(>8\). Meanwhile, when the training time is large (50 epochs) in Figure 1(b), KL privacy loss worsens with increasing depth under all initializations. This shows that given small training time, the choice of initialization distribution affects the dependency of KL privacy loss on increasing depth, thus validating Lemma 3.2 and Theorem 4.1.

## 6 Utility guarantees for Training Linearized Network

Our privacy analysis suggests that training linearized network under certain initialization schemes (such as LeCun initialization) allows for significantly better privacy bounds under over-parameterization by increasing depth. In this section, we further prove utility bounds for Langevin diffusion under initialization schemes and investigate the effect of over-parameterization on the privacy utility trade-off. In other words, we aim to understand whether there is any utility degradation for training linearized networks when using the more privacy-preserving initialization schemes.

_Convergence of training linearized network_. We now prove convergence of the excess empirical risk in training linearized network via Langevin diffusion. This is a well-studied problem in the literature for noisy gradient descent. We extend the convergence theorem to continuous-time Langevin diffusion below and investigate factors that affect the convergence under over-parameterization. The proof is deferred to Appendix D.1.

**Lemma 6.1** (Extension of [42, Theorem 2] and [45, Theorem 3.1]).: _Let \(_{0}^{lin}(;)\) be the empirical risk function of a linearized network in Eq. (3) expanded at initialization vector \(_{0}^{lin}\). Let \(_{0}^{*}\) be an \(\)-near-optimal solution for the ERM problem such that \(_{0}^{lin}(_{0}^{*};)-_{}_{0 }^{lin}(;)\). Let \(=\{_{i}\}_{i=1}^{n}\) be an arbitrary training dataset of size \(n\), and denote \(M_{0}= f_{_{0}^{lin}}(_{1}),, f_{_ {0}^{lin}}(_{n})^{}\) as the NTK feature matrix at initialization. Then running Langevin diffusion (4) on \(_{0}^{lin}()\) with time \(T\) and initialization vector \(_{0}^{lin}\) satisfies_

\[[_{0}^{lin}(}_{T}^{lin})]-_{} _{0}^{lin}(;)++ ^{2}rank(M_{0})\,,\]

_where the expectation is over Brownian motion \(B_{T}\) in Langevin diffusion in Eq. (4), \(}_{T}^{lin}=}_{t}^{lin}t\) is the average of all iterates, and \(R=\|_{0}^{lin}-_{0}^{*}\|_{M_{0}}^{2}\) is the gap between initialization parameters \(_{0}^{lin}\) and solution \(_{0}^{*}\)._

_Remark 6.2_. The excess empirical risk bound in Lemma 6.1 is smaller if data is low-rank, e.g., image data, then \((M_{0})\) is small. This is consistent with the prior dimension-independent private learning literature  and shows the benefit of low-dimensional gradients on private learning.

Lemma 6.1 highlights that the excess empirical risk scales with the gap \(R\) between initialization and solution (denoted as lazy training distance), the rank of the gradient subspace, and the constant \(B\) that specifies upper bound for expected gradient norm during training. Specifically, the smaller the lazy training distance \(R\) is, the better is the excess risk bound given fixed training time \(T\) and noise variance \(^{2}\). We have discussed how over-parameterization affects the gradient norm constant \(B\) and the gradient subspace rank \((M_{0})\) in Section 3. Therefore, we only still need to investigate how the lazy training distance \(R\) changes with the network width, depth, and initialization, as follows.

_Lazy training distance \(R\) decreases with model over-parameterization_. It is widely observed in the literature  that under appropriate choices of initializations, gradient descent on fully connected neural network falls under a lazy training regime. That is, with high probability, there exists a (nearly) optimal solution for the ERM problem that is close to the initialization parameters in terms of \(_{2}\) distance. Moreover, this lazy training distance \(R\) is closely related to the smallest eigenvalue of the NTK matrix, and generally decreases as the model becomes increasingly overparameterized. In the following proposition, we compute a near-optimal solution via the pseudo inverse of the NTK matrix, and prove that it has small distance to the initialization parameters via existing lower bounds for the smallest eigenvalue of the NTK matrix .

**Lemma 6.3** (Bounding lazy training distance via smallest eigenvalue of the NTK matrix).: _Under Assumption 2.4 and single-output linearized network Eq. (3) with \(o=1\), assume that the per-layer network widths \(m_{0},,m_{L}=(n)\) are large. Let \(_{0}^{lin}()\) be the empirical risk Eq. (1) for linearized network expanded at initialization vector \(_{0}^{lin}\). Then for any \(_{0}^{lin}\), there exists a corresponding solution \(_{0}^{}}\), s.t. \(_{0}^{lin}(_{0}^{}})-_{}_{0} ^{lin}(;)}\), \((M_{0})=n\) and_

\[R}(\{(_{i=1} ^{L-1}_{i}m_{i})},1\}^{L}_{l}^{-1}} )\,,\] (9)

_with high probability over training data sampling and random initialization Eq. (5), where \(}\) ignores logarithmic factors with regard to \(n\), \(m\), \(L\), and tail probability \(\)._

The full proof is deferred to Appendix D.2. By using Lemma 6.3, we provide a summary of bounds for \(R\) under different initializations in Table 1. We observe that the lazy training distance \(R\) decreases with increasing width and depth under LeCun, He and NTK initializations, while under Xavier initialization \(R\) only decreases with increasing depth.

_Privacy & Excess empirical risk tradeoffs for Langevin diffusion under linearized network_. We now use the lazy training distance \(R\) to prove empirical risk bound and combine it with our KL privacy bound Section 4 to show the privacy utility trade-off under over-parameterization.

**Corollary 6.4** (Privacy utility trade-off for linearized network).: _Assume that all conditions in Lemma 6.3 holds. Let \(B\) be the gradient norm constant in Eq. (7), and let \(R\) be the lazy training distance bound in Lemma 6.3. Then for \(^{2}=}\) and \(T=}\), releasing all iterates of Langevin diffusion with time \(T\) satisfies \(\)-KL privacy, and has empirical excess risk upper bounded by_

\[[_{0}^{lin}(}_{T}^{lin})] -_{}_{0}^{lin}(;) }(}+})\] (10) \[=}(}+_{l=1}^{L-1}_{l}m_{l}\}}{2^{L-1}}})\] (11)

_with high probability over random initialization Eq. (5), where the expectation is over Brownian motion \(B_{T}\) in Langevin diffusion, and \(\) ignores logarithmic factors with regard to width \(m\), depth \(L\), number of training data \(n\) and tail probability \(\)._

See Appendix D.3 for the full proof. Corollary 6.4 proves that the excess empirical risk worsens in the presence of a stronger privacy constraint, i.e., a small privacy budget \(\), thus contributing to a trade-off between privacy and utility. However, the excess empirical risk also scales with the lazy training distance \(R\) and the gradient norm constant \(B\). These constants depend on network width, depth and initialization distributions, and we prove privacy utility trade-offs for training linearized network under commonly used initialization distributions, as summarized in Table 1.

We would like to highlight that our privacy utility trade-off bound under LeCun and Xavier initialization strictly improves with increasing depth as long as the data satisfy Assumption 2.4 and the hidden-layer width is large enough. To our best knowledge, this is the first time that a strictly improving privacy utility trade-off under over-parameterization is shown in literature. This shows the benefits of precisely bounding the gradient norm (Appendix C.1) in our privacy and utility analysis.

## 7 Conclusion

We prove new KL privacy bound for training fully connected ReLU network (and its linearized variant) using the Langevin diffusion algorithm, and investigate how privacy is affected by the network width, depth and initialization. Our results suggest that there is a complex interplay between privacy and over-parameterization (width and depth) that crucially relies on what initialization distribution is used and the how much the gradient fluctuates during training. Moreover, for a linearized variant of fully connected network, we prove KL privacy bounds that improve with increasing depth under certain initialization distributions (such as LeCun and Xavier). We further prove excess empirical risk bounds for linearized network under KL privacy, which similarly improve as depth increases under LeCun and Xavier initialization. This shows the gain of our new privacy analysis for capturing the effect of over-parameterization. We leave it as an important open problem as to whether our privacy utility trade-off results for linearized network could be generalized to deep neural networks.