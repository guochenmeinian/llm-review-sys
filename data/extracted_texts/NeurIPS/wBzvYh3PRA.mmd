# FactorSim: Generative Simulation

via Factorized Representation

 Fan-Yun Sun

Stanford University

&S. I. Harini

Stanford University

&Angela Yi

Stanford University

&Yihan Zhou

Stanford University

Alex Zook

Nvidia

&Jonathan Tremblay

Nvidia

&Logan Cross

Stanford University

&Jiajun Wu

Stanford University

&Nick Haber

Stanford University

###### Abstract

Generating simulations to train intelligent agents in game-playing and robotics from natural language input, from user input or task documentation, remains an open-ended challenge. Existing approaches focus on parts of this challenge, such as generating reward functions or task hyperparameters. Unlike previous work, we introduce FactorSim that generates full simulations in code from language input that can be used to train agents. Exploiting the structural modularity specific to coded simulations, we propose to use a **factored** partially observable Markov decision process representation that allows us to reduce context dependence during each step of the generation. For evaluation, we introduce a _generative simulation_ benchmark that assesses the generated simulation code's accuracy and effectiveness in facilitating zero-shot transfers in reinforcement learning settings. We show that FactorSim outperforms existing methods in generating simulations regarding prompt alignment (_i.e._, accuracy), zero-shot transfer abilities, and human evaluation. We also demonstrate its effectiveness in generating robotic tasks.

## 1 Introduction

Simulations hold significant potential for training agents to perform real-world tasks where data collection is costly, dangerous, or infringes on individual privacy. A major bottleneck in harnessing the potential of simulations at scale for agent training is the cost of designing and developing them, especially when we need a distribution of simulations that meet detailed design specifications to train more generalized policies. In this paper, we aim to generate coded simulations given text specifications. Code provides a natural interface for users to inspect, modify, and debug the simulation. It also allows us to craft diverse environments for Reinforcement Learning (RL) purposes.

Generating full simulations in code to train agents from a text prompt is an under-explored challenge. Previous works focus on parts of this challenge, including reward function design , hyperparameter tuning , and task configuration while relying on an existing simulator . These methods use large language models (LLMs) to generate the components of simulations specified as code. However, when faced with large and detailed contexts, LLMs often generate simulations that ignore or fail to adhere to parts of the input prompt . This issue is not solely due to the limitations ofexisting LLMs but also suggests that some form of decomposition is always critical as we scale up the number of components in simulations. We ask the question: can we exploit the inherent structure (e.g., having a game loop that handles agent actions, updates internal game states accordingly, and displays the game states to the users through a rendering process) of coded simulations to generate them better?

We propose FactorSim, a framework that takes an arbitrary language specification as input and outputs a full simulation that can be used to train RL agents. The key idea of FactorSim is to decompose the input prompt into a series of steps and then use a factored Partially Observable Markov Decision Process (POMDP) representation to reduce the context needed for each generation step. To realize FactorSim, we use the _model-view-controller_ software design pattern to structure the generation process. Consider generating a coded simulation of WaterWorld; see Figure 1. The game consists of an agent (blue circle) traveling in a 2d world, capturing food (green circle) while avoiding enemies (red circle). Our method first decomposes the game description into multiple steps to be implemented. For example, a step instruction could be "Introduce red dot enemies that can be controlled with arrow keys. Give the player a -1 reward when the agent collides with an enemy". We first select the context needed for this functionality to be implemented, _e.g._, positions of existing agents. Subsequently, FactorSim generates (at most) three functions: one to handle player input (i.e., _handle_key_press_, the controller component), one to implement the collision logic (i.e., _collision_logic_, the model component), and one to update the rendering function (i.e., _render_red_dot_, the view component). Limiting the context during each step of the simulation generation process allows FactorSim to focus on the task at hand while avoiding hallucinating non-existent functions or modifying code not meant to be changed.

To evaluate the task of full simulation generation, we propose a new _Generative Simulation4_ benchmark with accompanying success metrics. One set of success metrics is the pass rate in automated system tests. Commonly used in game development, these system tests programmatically assess whether the behavior of the generated simulation adheres to the specifications given in the input prompt. The second success metric assesses the value of the generated simulations for transfer learning in an RL setting. This evaluates how well agents trained on a set of generated simulations can generalize to held-out environments that satisfy the design specifications provided in prompts. Generalization to unseen environments is crucial for many applications, including transferring robotics

Figure 1: Overview of FactorSim. FactorSim takes language documentation as input, uses Chain-of-Thought to derive a series of steps to be implemented, adopts a Factored POMDP representation to facilitate efficient context selection during each generation step, trains agents on the generated simulations, and tests the resulting policy on previously unseen RL environments.

policies learned in simulation to the real world. This benchmark consists of 8 RL environments with varying levels of difficulty. In addition to evaluating our method on the benchmark we introduced, we further assess FactorSim's ability to generate robotic tasks on the dataset published by GenSim . We demonstrate the value of our method, FactorSim, on both the benchmark task suite and GenSim's dataset, showing performance superior to baseline alternatives.

In summary, our contributions are three-fold. First, we propose FactorSim, a framework for generating coded simulation with a factor graph of a POMDP as a principled way to reduce context dependence. Second, we introduce a new generative simulation benchmark by adapting an existing RL benchmark , and demonstrate FactorSim's superior results against baselines in terms of code correctness (i.e., prompt alignment), ability to facilitate zero-shot generalization and human evaluation of the simulations. Third, we demonstrate that FactorSim can be applied to generating simulation tasks for robotics, outperforming existing approaches.

## 2 Related Work

We aim to generate simulations for training agents to generalize to previously unseen environments. Recent work has investigated this in the context of learned neural world models and LLM-generated code for simulations.

World models simulate the dynamics of a given environment and use this as a proxy environment for agent training, rather than interacting with a ground truth simulator . Several approaches have demonstrated the value of learning world models as part of general algorithms that can learn to play a variety of games (AlphaZero , Muesli , and DreamerV3 ). Other efforts use a large set of offline data to learn a world model that is subsequently used for agent training, including for autonomous vehicle driving (GAIA-1 ), robotic manipulation (UniSim ), and 2D platformer games (Genie ). We generate world models as code as they are more interpretable, modular, and easily modified or extended by humans--key advantages we believe are important for their use in authoring large-scale or complex simulations.

LLMs have generated many parts of simulations for game playing and robotics. In (RL) games, LLMs have been used to generate game levels [34; 31], to choose parameters for an existing simulator , and to assist humans in creating full games . In robotics, LLMs have been used to generate reward functions, task specifications, and specific components like scene configurations within robotics tasks. Many works such as RoboGen , Holodeck , and Gen2Sim  build on top of existing simulators and use a series of prompts to generate interactable 3D environments to train agents. GenSim  starts from a human task library and iteratively generates and tests new tasks to generate robotic manipulation tasks. Other efforts have focused on generating reward functions for tasks [22; 19; 23]. Eureka  uses feedback from agent training to refine reward function specification. Our approach is able to generate both the simulator dynamics and reward functions and can be applied to both robotics and games.

As noted above, LLMs can struggle to handle complex tasks: this has prompted research into different ways to structure LLM reasoning. Chain-of-Thought (CoT) prompting demonstrated LLM performance can be substantially boosted by prompting the LLM to break a single task into multiple steps with either few-shot examples  or zero-shot . Subsequent work has developed a variety of techniques to improve LLM reasoning through multi-step reasoning prompts: checking for consistency among multiple reasoning paths , interleaving reasoning and tool use (ReACT ), using tree data structures to guide the LLM reasoning process (Tree-of-Thought ), or formulating reasoning as a tree search process [12; 46]. Approaches for general code generation include decomposing the task into functions to subsequently generate (Parsel ), generating code to reach a series of intermediate execution states (ExecDec ), and using a multi-agent framework to generate, test, and refine code (AgentCoder ). Other efforts optimize the prompts for given tasks, using evolutionary search (EvoPrompt ) or defining generalized declarative programming frameworks with modular optimization algorithms . Our approach generates code by leveraging a factorized representation specific to simulations to reduce the input context needed for different reasoning steps; it can be used in conjunction with approaches for general code generation, such as generating tests as a form of self verification.

FactorsSim: Generating Simulations via Factorized Representation

A simulation is a structured system of modules connected by events and responses. Our framework, FactorSim, generates code using LLMs by exploiting this structure to construct a simulation progressively. Our key insight is that, by generating a simulation step-by-step while **only selecting the relevant context information needed for each step**, we can effectively reduce the reasoning capacity needed for each step, leading to simulations that adhere more closely to the input requirements.

In this section, we describe our method for generating Turing-computable simulations. First, we describe simulations that can be modeled as a Partially Observable Markov Decision Process (POMDP). Second, we use Chain-of-Thought (CoT) to decompose an input prompt describing the desired full simulation into a series of prompts describing different components to be implemented. Third, we introduce a factorized POMDP representation that exploits the inherent modularity of coded simulations. Refer to Algorithm 1 and Figure for an overview of FactorSim alongside an illustrative example.

``` Input:\(Q_{}\), a natural language description of the simulation, and an LLM Output: auring-computable simulation represented as a POMDP \(^{}= S,A,O,T,,R\)  Initialize a Factored POMDP \(_{1} S_{1},A,,T_{1},,R_{1}\) where - \(S_{1}:=\{s_{}\}\) - \(A\) is the set of all keyboard inputs - \(T_{1}\) is an identity function, i.e., \(T_{1}(s^{} s,a)=[s^{}=s]\) - \(R_{1}(s,a,s^{}):=s^{}_{}-s_{}\) // Chain of Though  Derive a step-by-step plan \((q_{1},,q_{k}) p(q_{1},,q_{k} Q_{})\) Eq. (1) for each step, or module \(q_{k}\)do  State space update & context selection \(p(S_{k+1},S[Z_{k}]|S_{k},q_{k})\) Eq. (9), (10) // Controller component update Action-dependent state transition model update: \(p(T_{k+1}^{(a)}|S[Z_{k}],A,q_{k})\) // Model component update Action-independent state transition model update: \(p(T_{k+1}^{(s)}|T[Z_{k}],S[Z_{k}],q_{k})\) // View component update Observation model update: \(p(_{k+1}|S[Z_{k}],q_{k})\) Eq. (13) \(_{k+1}= S_{k+1},A,O_{k+1},T_{k+1},_{k+1},R_{1}\) where \(O_{k+1}\) is the new observation space defined by \(S_{k+1}\) and \(_{k+1}\), and \(T_{k+1}(s^{} s,a)=T_{k+1}^{(s)}(s^{} s) T_{k+1}^{(a) }(s s,a)\).  end for Return the final simulation \(^{}_{k+1}\) ```

**Algorithm 1**FactorSim

### Modeling Simulation as POMDP

A Partially Observable Markov Decision Process (POMDP) is used to represent a coded simulation. Formally a POMDP is represented as a tuple \(= S,A,O,T,,R\) where \(S\) is a set of states, \(A\) is a set of actions, \(O\) is a set of observations, \(T:S A(S)\) is a transition probability distribution, \(:S(O)\) is an observation function, and \(R:S A S^{}\) is the reward model 5.

We aim to generate a simulation from a prompt \(Q_{}\). In this paper, we are particularly interested in the case where \(Q_{}\) comprises detailed design specifications such that the resulting simulation could be used to train agents, though our method applies to any prompt for defining a simulation. In our experiments, \(Q_{}\) is a paragraph of text around 10 sentences specifying this simulation.

### Chain of Thought

We first decompose the prompt \(Q_{}\) into a series of steps using Chain of Thought , each describing a module of the simulation to be implemented. Following similar formulation as in , this can be thought of as marginalizing over a step-by-step plan variable \((q_{1},,q_{k})\) using \(N\) Monte Carlo samples:

\[(^{}|Q_{})=_{i=1}^{N}p( ^{}|q_{1}^{(i)},,q_{K}^{(i)}),(q_{1}^{(i)},,q_{K}^{(i)}) p(q_{1},,q_{K}|Q_{}),\] (1)

\(p\) is a probability estimation model (i.e., an LLM in our experiments), and \(^{}\) is the resulting code that fully specifies a simulation. In practice, we only produce a single plan \(N=1\).

Intuitively, this process breaks the prompt into sub-tasks. After we sample such a plan of \(K\) steps, we generate the simulation progressively. Given an existing POMDP \(\) and a natural language specification \(q\), we update the POMDP to reflect the changes specified.

\[p(_{K+1}|q_{1},,q_{K})_{k=1}^{K}p(_{k+ 1}|_{k},q_{k})\] (2)

where \(_{k+1}\) is the POMDP (simulation as code) after the \(k\)-th step is implemented, and \(_{K+1}\) is the final simulation. While Chain-of-Thought prompting allows LLMs to avoid having to generate code for all simulation logic at once, the complexity of each step still grows with \(k\) due to the expanding codebase. This task remains challenging because LLMs must comprehend the code and accurately identify where modifications are needed. Acknowleding the limited reasoning ability of LLMs, we ask: can we further decompose the \(p(_{k+1}|_{k},q_{k})\) into simpler distributions to reduce the complexity of each prompt?

### Decomposition by Factorized Representation

Naively, we could further decompose a step of the generation into several steps, each focused on generating a different component of the POMDP:

\[p(_{k+1}|_{k},q_{k})= p(S_{k+1}|_{k},q_{k})\] (3) \[p(T_{k+1}|S_{k+1},_{k},q_{k})\] (4) \[p(R_{k+1}|S_{k+1},T_{k+1},_{k},q_{k})\] (5) \[p(_{k+1}|S_{k+1},T_{k+1},R_{k+1},_{k},q_{k})\] (6)

Figure 2: An illustrative example of how the five main prompts in FactorSim correspond to our formulation in Algorithm 1. Note that the function _red_puck_respawn_ is retrieved as part of the context to Prompt 3, 4, and 5 because it modifies the state variable _red_puck_position_, a state variable LLM identified as relevant in prompt 2.

However, this still requires the LLMs to take the entire simulation (\(_{k}\)) as context, which could be over hundreds of lines of code in our experiments. Empirically, we observe that many failed generations can be attributed to LLMs attending to or modifying parts of the input context unrelated to the prompt.

To reduce the input context needed for each generation step, we propose to use a factored POMDP representation to remove the dependence on the full previous POMDP as context. For instance, given an existing simulation \(M_{k}\) of red, green, and blue agents, to implement the \(kth\)-step instruction \(q_{k}\): respawn the red agent when it collides with the blue agent, we only need context regarding the respawn logic of the red agent and the positions of the red and blue agents. Code regarding the green agent or the rendering logic would be unnecessary context.

To formalize our approach, we first introduce notation common to the literature . Suppose we have a POMDP with a state space factored into \(n\) state variables \(S=S S[n]\) and \(Z\) is a subset of indices \(Z\{1,2,,n\}\), we define the scope set \(S[Z]:=_{i Z}S[i]\) as the state space spanned by the subset of state variables. For example, if \(Z=1,3,4\), then \(S[Z]\) defines a state space defined by \(S S S\). We denote a state in the scoped state space \(S[Z]\) as \(s[Z]\). Below, let us formally define a factored POMDP.

**Definition 3.1**.: A factored POMDP is a POMDP with both factored transition distribution and factored reward function. A transition probability distribution \(T\) of a POMDP with discrete action space is factored over its state space \(S=S_{1} S_{n}\) with scopes \(Z_{1},,Z_{m}\) if, for all \(s,a A\) there exist some \(\{T_{i}\}_{i=1}^{m}\) in the space of all possible transition distributions on the state space \(S\) and action space \(A\), such that,

\[T(s|s,a)=_{i=1}^{m}T_{i}(s[i] s[Z_{i}],a).\] (7)

A reward function \(R\) of a POMDP is factored over \(S=S_{1} S_{n}\) with scopes \(Z_{1},,Z_{l}\) if, for all \(s,a A\) there exist some \(\{R_{i}\}_{i=1}^{l}\) in the space of all possible reward functions on the state space \(S\) and action space \(A\), such that,

\[R(s,a)=_{i=1}^{l}R_{i}(s[Z_{i}],a).\] (8)

A factored POMDP can be represented as a factor graph 6 with two types of nodes: _state variables_ (i.e., \(S_{i}\)) and _factors_ (i.e., \(T_{i}\) or \(R_{i}\)), functions of (state) variables. Our idea is to **reduce context dependence by structuring the code using a factored POMDP representation** and treat each generation step as expanding a factored POMDP with new state variables and new _factors_. During every step \(q_{k}\), we first select a set of relevant state variable indices \(Z_{k}\). Then, we select existing factors that have overlapping scope with the selected set of state variables as context, which we denote as \(T[Z_{k}]\) and \(R[Z_{k}]\). That is, we can reduce the dependence on the previous simulation \(M_{k}\) and rewrite Equation 3-6 to the following:

\[p(_{k+1}|_{k},q_{k}) p(S_{k+1}|S_{k},q_{k}). \] (9) \[p(S[Z_{k}]|S_{k+1},q_{k}). \] (10) \[p(T_{k+1}|T[Z_{k}],S[Z_{k}],A,q_{k}). \] (11) \[p(R_{k+1}|R[Z_{k}],S[Z_{k}],A,q_{k}). \] (12) \[p(_{k+1}|S[Z_{k}],q_{k}). \] (13)

Note that \(Z_{k}\) can only consist of state variable indices in the state space \(S_{k+1}\). In practice, we achieve this by encouraging the LLM to select a minimal set of relevant states \(Z_{k}\) in the prompt.

We find that the term 11 is most prone to error, likely because the most complicated functions of a simulation are state transitions. Motivated by this observation, we propose to adopt the _model-view-controller_ design pattern for structuring these prompts. Instead of prompting LLMs to update the state transition function first and then update the reward function, we prompt the LLMs to update the action-dependent part of the state transition function (i.e. the _Controller_ component) and then the action-independent part (i.e., _Model_). We treat the reward model as part of the state transition function that updates a _score_ state variable. That is, \(T(s^{}|s,a)=T^{(s)}(s^{}|s)T^{(a)}(s|s,a)\) where \(T^{(a)}(s|s,a)\) denotes the part of the state transition function that handles how actions affect the states and \(T^{(s)}(s^{}|s)\) denotes the part of the state transition function that how states are updated every step. This gives us our final algorithm as illustrated in Algorithm 1.

In Algorithm 1, colors indicate the corresponding components of the model-view-controller pattern. Red highlights the _controller_, corresponding to parts of the state transition dependent on user-input actions.Green shows the _model_, corresponding to parts of the state transition function that are not dependent on user-input actions. Blue shows the _view_ component, updating the observation function that acts as the "renderer" of the state space.

## 4 Experiments

In this paper, we consider two types of simulations: 2D Reinforcement Learning (RL) games and robotics tasks in a physics engine. We also introduce a new benchmark to evaluate generative simulation methods. Our experiments are designed to test three hypotheses. First, FactorSim generates simulations with _better prompt alignment_, which we evaluate through system tests and human evaluations. Second, FactorSim enables _better zero-shot transfer_ by training RL agents in the simulated generated environments. Third, FactorSim's strengths in _generating robotic tasks_.

### RL Game Generation

To answer our first two hypotheses, we propose a new benchmark that includes all 2D games from the PyGame Learning Environment 7: Flappy Bird, Catcher, Puckworld, Pixelcopter, Pong, Snake, Waterworld, and Monster Kong. For each RL game, the input prompt consists of the game's online documentation. Since most game documentation is incomplete, we manually supplement them with additional details (see Appendix). This ensures that our method and the baselines do not hallucinate any missing game information, allowing for a fair evaluation across all methods.

Following common practices in game development, we design system tests to verify that the generated simulations follow the specified logic programmatically. These tests simulate actions like key presses and mouse clicks and check if the game states are updated correctly. Refer to the Appendix for more details.

BaselinesFor baselines, we compare to three methods using a closed-source (GPT-4 ) and an open-source LLM (Llama-3 ). The first approach prompts the LLM with all contexts at once, which we denote as the _vanilla_ method. The second approach uses _self-debugging_, where the model retries generating the code when provided error messages from running the code (up to 10 times in our experiments). A third approach combines _Chain-of-Thought  (CoT)_ reasoning with self-debugging, where the LLM generates code incrementally, processing one instruction at a time. Additionally, we incorporate AgentCoder  as a baseline. CoT with self-debugging is an ablation study of our method that acts without the factored POMDP representation.

   \% _of system tests passed._ & **Flappy Bird** & **Catcher** & **Snake** & **Pixelcopter** & **Pong** & **Puckworld** & **Waterworld** & **Monster Kong** \\   Mistral-TB-Instruct & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ Llama-3 & 0.15 & 0.33 & 0.19 & 0.14 & 0.01 & 0.43 & 0.25 & 0.29 \\ Llama-3 w/ self debug & 0.15 & 0.41 & 0.28 & 0.19 & 0.03 & 0.44 & 0.22 & 0.31 \\ Llama-3 CoT w/ self debug & 0.20 & 0.39 & 0.25 & 0.21 & 0.16 & 0.50 & 0.42 & 0.35 \\ GPT-3 & 0.19 & 0.39 & 0.37 & 0.38 & 0.22 & 0.33 & 0.34 & 0.19 \\ GPT-4 & 0.35 & 0.35 & 0.42 & 0.44 & 0.25 & 0.34 & 0.46 & 0.21 \\ GPT-4 w/ self debug & 0.33 & 0.53 & 0.43 & 0.51 & **0.75** & 0.41 & 0.45 & 0.31 \\ GPT-4 w/ AgentCoder & 0.18 & 0.45 & 0.27 & 0.43 & 0.43 & 0.33 & 0.20 & 0.23 \\ GPT-4 CoT w/ self debug & 0.30 & 0.51 & 0.39 & 0.53 & 0.64 & 0.47 & 0.50 & 0.34 \\  Llama-3 w/ FactorSim (ours) & 0.55 & 0.54 & **0.50** & 0.41 & 0.38 & 0.58 & 0.27 & 0.35 \\ GPT-4 w/ FactorSim (ours) & **0.78** & **0.66** & 0.44 & **0.78** & 0.61 & **0.81** & **0.62** & **0.44** \\   

Table 1: Percentage of system tests passed by different methods of generating 2D RL games.

**Code Generation Evaluation** Table 1 shows the results for the baselines and our method. FactorSim outperforms all baselines in 7 out of 8 games. Additionally, we compare performance and LLM token usage across various methods using GPT-4 (Figure 3). While the vanilla baseline uses the fewest tokens, it only achieves moderate accuracy. Additionally, combining Chain-of-Thought (CoT) reasoning with self-debugging results in the highest token usage but only marginally improves accuracy over iterative self-debugging. FactorSim achieves the highest accuracy with modest token usage, indicating that the decomposition of tasks reduces the need for extensive iterative debugging.

Empirically, we find that certain prompts, when tested on baselines without the decomposition mechanism in FactorSim, are prone to syntax or runtime errors that the LLMs cannot self-debug. This is particularly evident with Llama-3 (vanilla) and Llama-3 self-debug, which perform poorly as they generate highly similar incorrect implementations, ignoring the logic specified in the prompts even when the temperature is set to 1. We hypothesize that this behavior is due to the model having a strong prior for how certain games, like Pong and Flappy Bird, should be implemented, ignoring the prompt specifications. This "mode collapse" issue of LLMs could be caused by over-fitting in the instruction tuning stage .

While AgentCoder iteratively refines code, it performs poorly because it relies on a test designer agent and a test executor agent to write quality test cases. However, due to the complexity of the tasks in our benchmark, the test designer agent tends to write incorrect or infeasible tests, leading to negative feedback. This points to FactorSim being an improvement over the standard "role-based" Chain of Thought decompositions, and that it is non-trivial to generate simulations from complex textual specifications.

**Zero-shot Transfer Results** Additionally, we test FactorSim by training a PPO  agent on 10 generated environments for 10 million steps and zero-shot test it on the "ground-truth" environment implemented in the original RL benchmark (Figure 4). The rewards are linearly scaled such that 0 corresponds to the performance of a random policy and 1 corresponds to the performance of a PPO agent trained for 10 million steps on the "ground-truth" environment. FactorSim achieves notably better zero-shot transfer results as a result of generating code that adheres more closely to the prompt specification. We also observe that the errors FactorSim made tend to be more spread out across different components of the simulation. In contrast, many baselines suffer from failure modes concentrated in a specific aspect of the generation (e.g., incorrectly implementing the collision logic) that significantly hampers the ability of a set of generations to facilitate zero-shot transfer.

Human Study EvaluationAutomated systems tests cannot holistically capture some aspects of game playability such as rendering a usable user interface. To address this limitation we conducted a human study where users were asked to play the generated games and evaluate their playability. Over 320 human evaluations (40 per game) we find FactorSim generates more functional and playable games, compared to the strongest baseline GPT-4 CoT with iterative self-debugging (Figure 5). More details can be found in the Appendix.

### Robotics Task Generation

We evaluate on GenSim's  50-task benchmark of robotics tasks in the CLIPort framework . Refer to Figure 6 for an overview of our experimental setting. We compare FactorSim with the best-performing methods in generating code that specifies tasks (object states and reward structure) that can be used to train robots. Analogous to the game generation experiment, we use FactorSim

Figure 3: Performance and token usage analysis of GPT-4-based methods. Ellipses correspond to 90% confidence intervals for each algorithm, aggregated over all RL games.

to modularize the code generation process into subtasks and have LLMs generate each subtask using only a set of necessary states as context. More details can be found in the Appendix.

Baselines & MetricsWe compare our method with the multiple GenSim baselines: vanilla (one-shot), Chain-of-Thought (topdown), and Chain-of-Thought (bottom-up). Adopting the same set of metrics, we evaluate all methods on a sequence of pass rates on "syntax correctness", "runtime-verified", and "task completed". A generated task is considered "completed" if a coded oracle agent could collect 99% of the total reward half of the time.

We empirically found that the "task completion rate" is an imperfect metric for evaluating the success of a generated task. A task deemed "complete" by the oracle agent may fail to adhere to the prompt. For example, when asked to generate a task "build a wheel," a method might produce a task specification that involves rearranging blocks into a structure that does not resemble a wheel. To address this, we introduced a metric of the "human pass rate". This involved manually inspecting runtime-verified tasks to determine if the task specifications aligned with the prompt descriptions (see Appendix).

Figure 4: Zero-shot transfer results on previously unseen environments (i.e., environments in the original RL benchmark ).

Figure 5: Human evaluation results on the generated simulations of FactorSim and the strongest baseline (i.e., GPT-4 CoT w/ self-debug), aggregated over all 8 RL games.

Figure 6: **Left**: an overview of our robotics task generation experimental setting. **Right**: Tasks successfully generated using FactorSim, which all other baselines fail on.

Results & DiscussionFactorSim outperforms baselines in generating tasks with a higher runtime pass rate and better human evaluation scores, indicating improved prompt alignment (Figure 7). Task completion rates are generally low for all methods due to the limitation of the oracle agent. For example, tasks like "Build Ball Pit" (fill a container with balls to create a ball pit) often fail because the balls roll out of the visible area of the oracle agent, not because the generated task is invalid. FactorSim performs particularly well on tasks that specify spatial relationships (e.g., "on top of," "left of," "outside of") between objects, such as the "build House" example in Figure 8. This improvement is likely due to the decomposition process, where for each step, instead of addressing a combination of multiple spatial relations all at once, FactorSim attends to a smaller context, allowing each spatial relation to be addressed separately.

## 5 Conclusion & Future Work

We have proposed FactorSim as an approach to generate full simulations as code that can train agents while adhering to detailed design requirements specified as a text prompt. We also introduce a benchmark suite of eight RL environments to evaluate generative simulation methods.

Generating complex simulations in code is challenging, and we anticipate numerous opportunities to extend the simulation generation process. There is substantial room to address larger-scale, more complex games, and robotics environments that require code bases beyond what can be used effectively in the context window of existing LLMs. We also see great potential to accelerate RL agent training by generating code that can be accelerated on GPU devices. Our robotic simulation results will benefit from further investigations to demonstrate transfer to real-world environments. We have only addressed single-agent simulations, leaving the extension of our method to multi-agent settings to future work. In the future, we also plan to incorporate information from the agent training process to automatically modify the generated simulation environment for enhanced agent learning and generalization. Taken together, we believe the generation of full simulations as code will be an important step toward enhancing the capabilities of LLMs to support the development of generalized RL agent policies.

Figure 8: This figure illustrates two input task prompts and the corresponding sequence of subtasks decomposed by FactorSim.

Figure 7: Performance of FactorSim and GenSim  baselines in generating robotic tasks.