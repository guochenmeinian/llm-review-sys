# Energy Guided Diffusion for Generating Neurally Exciting Images

Pawel A. Pierzchlewicz\({}^{*,1,2}\), Konstantin F. Willeke\({}^{1,2}\), Arne F. Nix\({}^{1,2}\), Pavithra Elumalai\({}^{2}\),

**Kelli Restivo\({}^{3,4}\), Tori Shinn\({}^{3,4}\), Cate Nealley\({}^{3,4}\), Gabrielle Rodriguez\({}^{3,4}\), Saumil Patel\({}^{3,4}\), Katrin Franke\({}^{3,4}\), Andreas S. Tolias\({}^{3,5}\), Fabian H. Sinz\({}^{1,4}\)**

\({}^{1}\)Institute for Bioinformatics and Medical Informatics, Tubingen University, Tubingen, Germany

\({}^{2}\)Institute of Computer Science and Campus Institute Data Science, University of Gottingen, Germany

\({}^{3}\)Department of Neuroscience, Baylor College of Medicine, Houston, TX, USA

\({}^{4}\)Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, TX, USA

\({}^{5}\)Department of Electrical and Computer Engineering, Rice University, Houston, TX, USA

\({}^{*}\)ppierzc@cs.uni-goettingen.de

###### Abstract

In recent years, most exciting inputs (MEIs) synthesized from encoding models of neuronal activity have become an established method for studying tuning properties of biological and artificial visual systems. However, as we move up the visual hierarchy, the complexity of neuronal computations increases. Consequently, it becomes more challenging to model neuronal activity, requiring more complex models. In this study, we introduce a novel readout architecture inspired by the mechanism of visual attention. This new architecture, which we call attention readout, together with a data-driven convolutional core outperforms previous task-driven models in predicting the activity of neurons in macaque area V4. However, as our predictive network becomes deeper and more complex, synthesizing MEIs via straightforward gradient ascent (GA) can struggle to produce qualitatively good results and overfit to idiosyncrasies of a more complex model, potentially decreasing the MEI's model-to-brain transferability. To solve this problem, we propose a diffusion-based method for generating MEIs via Energy Guidance (EGG). We show that for models of macaque V4, EGG generates single neuron MEIs that generalize better across varying model architectures than the state-of-the-art GA, while at the same time reducing computational costs by a factor of 4.7x, facilitating experimentally challenging closed-loop experiments. Furthermore, EGG diffusion can be used to generate other neurally exciting images, like most exciting naturalistic images that are on par with a selection of highly activating natural images, or image reconstructions that generalize better across architectures. Finally, EGG is simple to implement, requires no retraining of the diffusion model, and can easily be generalized to provide other characterizations of the visual system, such as invariances. Thus, EGG provides a general and flexible framework to study the coding properties of the visual system in the context of natural images.1

## 1 Introduction

From the early works of Hubel and Wiesel , visual neuroscience has used the preferred stimuli of visual neurons to gain insight into the information processing in the brain. In recent years, deep learning has made big strides in predicting neuronal responses  enabling _in silico_ stimulussynthesis of non-parametric most exciting inputs (MEIs) [17; 18; 19]. MEIs are images that strongly drive a selected neuron and can thus provide insights into its tuning properties. Up until now, they have been successfully used to find novel properties of neurons in various brain areas in mice and macaques [17; 18; 19; 20; 21; 22; 23; 24].

However, as we move up the visual hierarchy, such as monkey visual area V4 and IT, the increasing non-linearity of neuronal responses with respect to the visual stimulus makes it more challenging to obtain models with high predictive performance for single neurons, and optimize perceptually plausible MEIs, that is, those not corrupted by adversarial high-frequency noise for example. Particularly, area V4 is known to be influenced by attention effects , and shifts in attention before the onset of saccades can change the location of its neurons' receptive fields [26; 27]. When models become more complex or units are taken from deeper layers of a network, existing MEI optimization methods based on gradient ascent (GA) can sometimes have difficulties producing qualitatively good results  and can overfit to the idiosyncrasies of more complex models, potentially decreasing the MEI's model-to-brain transferability. Typically, these challenges are addressed by biasing MEIs towards the statistic of natural images, for instance by gradient pre-conditioning , by including a total variation loss to reduce high-frequency noise  or by image synthesis via GANs . However, as discussed by Engstrom et al.  and Feather et al.  including additional priors into the generation process can result in obfuscated model biases.

Here, we make two contributions towards the above points: We introduce a new model architecture, called the attention readout, for predicting the activity of neurons in macaque area V4, which together with a data-driven convolutional core outperforms previous task-driven models [24; 32]. To improve the quality of MEI synthesis we introduce a novel method for optimizing MEIs via Energy Guided Diffusion (EGG). EGG diffusion guides a pre-trained diffusion model with a learned neuronal encoding model to generate MEIs with a bias towards natural image statistics. Our proposed EGG method is simple to implement and, in contrast to similar approaches [33; 34; 35], requires no retraining of the diffusion model (Fig. 1). We show that EGG diffusion not only yields MEIs that generalize better across architectures and are thus expected to drive real neurons equally well or better than GA-based MEIs but also provides a significant (4.7x) speed up over the standard GA method enhancing its utility for close-loop experiments such as inception loops [17; 18; 20; 24]. Since optimizing MEIs for thousands of neurons can take weeks , such a speed-up directly decreases the energy footprint of this technique. Moreover, the rapid verification of synthesized images _in vivo_ is particularly important for close-loop experiments given that maintaining the stability of single unit recordings is challenging, and there's also the issue of representational drift , where tuning functions can change over time. We also demonstrate that EGG diffusion straightforwardly generalizes to provide other characterizations of the visual system that can be phrased as an inverse problem, such as image reconstructions based on neuronal responses. The flexibility and generality of EGG thus make it a powerful tool for investigating the neural mechanisms underlying visual processing.

## 2 Attention readout for macaque area V4

BackgroundDeep network-based encoding models have set new standards in predicting neuronal responses to natural images [2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15]. Virtually all architectures of these encoding models consist of at least two parts: a _core_ and a _readout_. The core is usually implemented via a convolutional

Figure 1: **Schematic of the EGG diffusion method with a pre-trained diffusion model. Examples of applications: Left: Most Exciting Inputs for different neurons, Middle: Most Exciting Naturalistic Inputs matched unit-wise to the MEIs. Right: Reconstructions in comparison to the ground truth (top) and gradient descent optimized (bottom).**

network that extracts non-linear features \(()\) from the visual input and is shared across all neurons to be predicted. It is usually trained through one of two paradigms: i) _task-driven_, where the core is pre-trained on a different task like object recognition  and then only the readout is trained to predict the neurons' responses or ii) _data-driven_ where the model is trained end-to-end to predict the neurons' responses. The _readout_ is a collection of predictors that map the core's features to responses of individual neurons. With a few exceptions , the readout components and its parameters are neuron-specific and are therefore kept simple. Typically, the readout is implemented by a linear layer with a rectifying non-linearity. Different readouts differ by the constraints they put on the linear layer to reduce the number of parameters . One key assumption all current readout designs make is that the readout mechanism does not change with the stimulus. In particular, this means that the location of the receptive field is fixed. While this assumption is reasonable for early visual areas like V1, it is not necessarily true for higher or mid-level areas such as macaque V4, which are known to be affected by attention effects and can even shift the location of the receptive fields . This motivated us to create a more flexible readout mechanism for V4.

State-of-the-art model: Robust ResNet core with Gaussian readoutIn this study, we compare our data-driven model to a task-driven model , which is also composed of a _core_ and _readout_. The core is a pre-trained robust ResNet50 (\(L_{2},=0.1\)) . We use the layers up to layer 3 in the ResNet, which has 1,024 channels, thus providing a 1,024 dimension feature space. Then batch normalization is applied , followed by a ReLU non-linearity. The _Gaussian readout_ learns the position of each neuron and extracts a feature vector at this position. During training, the positions are sampled from a 2D Gaussian distribution with means \(_{n}\) and \(_{n}\), during inference the \(_{n}\) positions are used. Then the extracted features are used in a linear non-linear model to predict neuronal responses. We will refer to this model as the **Gaussian model**.

Proposed model: Data-driven core with attention readoutThe predictive model is trained from scratch to predict the neuronal responses in an end-to-end fashion. Following Lurz et al. , the architecture is comprised of two main components. First, the _core_, a four-layer CNN with 64 channels per layer with an architecture identical to Lurz et al. . Secondly, the attention _readout_, which builds upon the attention mechanism  as it is used in the popular transformer architecture . After adding a fixed positional embedding to \(()\) and normalization through LayerNorm  to get \(()\), key and value embeddings are extracted from the core representation. This is done by position-wise linear projections \(V^{c d_{k}}\) and \(U^{c d_{v}}\) both of which have parameters shared across all neurons. Then, for each neuron a learned query vector \(_{n}^{d_{k}}\) is compared with each position's key embedding using scaled dot-product attention .

\[_{n}=(_{c,d_{k}}()_{c} W_{c,d_{k}}q_{n,d_{k}}}{}})\] (1)

The result is a spatially normalized attention map \(_{n}^{h w 1}\) that indicates the most important feature locations for a neuron \(n\) given an input image. Using this attention map to compute a weighted sum of the value embeddings gives us a single feature vector for each neuron. Finally, a neuron-specific affine projection with ELU non-linearity  gives rise to the predicted spike rate \(_{n}\) (Fig. 2A). The model training is performed by minimizing the Poisson loss using the same setup as described in Willeke et al. . We will refer to this model as the **Attention model**.

Training dataWe use data from 1,244 Macaque V4 neurons from Willeke et al.  and briefly summarize their data acquisition in the supplementary materials section A.1.

ResultsOur Attention model significantly outperforms the Gaussian model in predicting neuronal responses of macaque V4 cells on unseen natural and model-derived images. We evaluate the model performance by the correlation between the model's prediction and the averages of actual neuron responses across multiple presentations of a set of test images, as described by Willeke et al. . We compared this predictive performance to the Gaussian model  on 1,244 individual neurons (Fig. 2B). The Attention model significantly outperforms the Gaussian model by 12% (Wilcoxon signed-rank test, p-value \(=6.79 10^{-82}\)). In addition, we evaluated the new readout on how well it predicts the real neuronal responses to 48 MEIs generated from the Gaussian model [see 24] and 7 control natural images. Our Attention model is better at predicting real neuronal responses,even for MEIs of another architecture (Fig. 2C). Please note that Willeke et al.  experimentally verified MEIs in only a subset of neurons and only used the neurons with high functional consistency across different experimental sessions. For that reason, we too can only compare the performance of model-derived MEIs on this subset of neurons. We additionally show that the Attention model and Gaussian model show representational similarity (see Table S1) and that the Attention model uses its ability to shift its receptive field (Fig. S1).

## 3 Energy guided diffusion (EGG)

### Algorithm and methods

In this section, we describe our approach to extract tuning properties of neuronal encoding models using a natural image prior as described by a diffusion model. In brief, we use previously established links between diffusion and score-based models and the fact that many tuning properties can be described as inverse problems (most exciting image, image reconstruction from neuronal activity, etc.) to combine an energy landscape defined by the neuronal encoding model with the energy landscape defined by the diffusion model and synthesize images via energy minimization. We show that this method leads to better generalization of MEIs and image reconstructions across architectures, faster generation, and allows for generating natural-looking stimuli.

Background: diffusion modelsRecently, Denoising Diffusion Probabilistic Models (DDPMs) have proved to be successful at generating high-quality images . These models can be formalized as a variational autoencoder with a fixed encoder \(_{0}_{T}\) that turns a clean sample \(_{0}\) into a noisy one \(_{T}\) by repeated addition of Gaussian noise, and a learned decoder \(_{T}_{0}\),

   Readout  Core & Task-Driven & Data-Driven \\  Factorized & - & 0.153 \\ Gaussian & 0.262 & 0.229 \\ Attention & 0.276 & **0.294** \\   

Table 1: Ablation study test correlation comparison for combinations of different cores and readouts. Bold indicates the best-performing model.

Figure 2: **a)** Schematic of the Attention Readout. **b)** Correlation to average scores for 1,244 neurons. The Attention model (pink) shows a significant (as per the Wilcoxon signed rank test, p-value \(=6.79 10^{-82}\)) increase in the mean correlation to average in comparison to the Gaussian model (blue). **c)** Predictive performance comparison of the two models in a closed-loop MEI evaluation setting. Showing that the data-driven with attention readout model better predicts the in-vivo responses of the MEIs.

which is often described as inverting a diffusion process . After training, the sampling process is initialized with a standard Normal sample \(_{T}(,)\) which is iteratively "denoised" for \(T\) steps until \(_{0}\) is reached. In the encoding, each step \(t\) corresponds to a particular noise level such that

\[_{t}=_{t}}_{0}+_{t}}_{0}\] (2)

where \(_{t}\) controls the signal strength at time \(t\) and \(_{0}(,)\) is independent Gaussian noise. In the decoding step, the diffusion model predicts the noise component \(_{}(_{t},t)\) at each step \(t\) of the diffusion process . Then the sampling is performed according to

\[_{t-1}=}}(_{t}-}{ _{t}}}_{}(_{t},t)) +_{t}\] (3)

where \((,)\).

Several previous works have established a link between diffusion models and energy-based models . In particular, the diffusion model \(_{}(_{t},t)\) can be interpreted as a _score function_, i.e. the gradient of a log-density or energy w.r.t. the data \(_{} p()\). This link is particularly useful since combining two density models via a product is equivalent to adding their score functions.

EnerGy Guided Diffusion (EGG)To optimize neurally exciting images, we require a method that can guide diffusion models via neural encoding models. The parameterization of diffusion models introduced by Ho et al.  only allows for the unconditioned generation of samples. Dhariwal and Nichol  introduced a method for sampling from a conditional distribution \(p_{t}()\), with diffusion models using a classifier \(p_{t}()\) known as classifier guidance. However, this method requires i) the classifier to be trained on the noisy images, and ii) is limited to conditions for which classification makes sense. Essentially, this method relies on computing the score of the posterior distribution.

\[_{_{t}} p(_{t})=_{_{t}} p( {x}_{t})+_{_{t}} p(_{t})\] (4)

For classifier-guidance, the gradient of a model \(_{_{t}} p(_{t})\) with respect to the noisy input \(_{t}\) is combined with the diffusion model \(_{_{t}} p(_{t})\), resulting in samples \(_{0}\) conditioned on the class \(\). Note that this requires a model \(_{_{t}} p(_{t})\) that has been trained on noisy samples of the diffusion before. Here we extend this approach to i) use neuronal encoding models, such as the ones described above, to guide the diffusion process and ii) use a model trained on _clean_ samples only. We achieve i) by defining conditioning as a sum of energies. Specifically, we redefine equation (4) in terms of the output of the diffusion model \(_{}(_{t},t)\) and an arbitrary energy function \(E(_{t},t)\):

\[(_{t},t)=_{}(_{t},t)+_{t} _{_{t}}E(_{t},t)\] (5)

where \(_{t}\) is the energy scale. This takes advantage of the fact that sampling in DDPMs is functionally equivalent to Langevin dynamics . Langevin dynamics generally define the movement of particles in an energy field and in the special case when \(E(x)=- p(x)\), Langevin dynamics generates samples from \(p(x)\). For this study, we use a constant value of \(\) and normalize the gradient of the energy function to a magnitude of 1.

To achieve ii) we use an approximate clean sample \(}_{0}\), i.e. the original image, that can be estimated at each time step \(t\). This is achieved by a simple trick introduced in Li et al. . By inverting the forward diffusion process, with the assumption that the predicted \(_{}(_{t},t)\) is the true noise:

\[}_{0}(_{t},t)=_{t}}}(_{t}- _{t}}_{}(_{t},t)).\] (6)

As a result, the energy function receives inputs that are in the domain of \(_{0}\) at much earlier time steps \(t\), and hence makes it feasible to use energy functions only defined on \(_{0}\) and not \(_{t}\), dropping the requirement to provide an energy \(E(_{t},t)\) that can take noisy images. Thus, the new score can be defined as

\[(_{t},t)=_{}(_{t},t)+_{t} _{_{t}}E(}_{0}(_{t},t))\] (7)

This is particularly relevant in the domain of neural system identification, as encoding models are trained on neuronal responses to natural "clean" images . To get an energy that can understand noisy images would require showing the noisy images to the animals in experiments, which would make the use of this method prohibitively more difficult. Therefore, a guidance method that does not require training an additional model on noisy images allows researchers to apply EGG diffusion directly to existing models trained on neuronal responses and extract tuning properties from them.

Related workMany other methods have been proposed to condition the samples of diffusion processes on additional information. Ho and Salimans  provided a method that addressed the second requirement of classifier-guidance by incorporating the condition \(\) into the denoiser \(_{}(_{t},t,)\). However, to introduce a conditioning domain \(\) in this classifier-free guidance, the whole diffusion model needs to be retrained. Furthermore, this link between diffusion models and energy-based models allowed several previous works to compose diffusion models to generate outputs that contain multiple desired aspects of a generated image . However, these studies focus solely on generalizing the classifier-free guidance to allow guiding diffusion models with other diffusion models. Nichol and Dhariwal  have used a similar gradient conditioning to guide the diffusion process using the gradient of the dot product of the CLIP image and text vectors. It has been shown that CLIP models that have not been trained on noisy images can be used for guiding diffusion models . Kadkhodaie and Simoncelli  introduced a stochastic coarse-to-fine gradient ascent procedure for generating samples from the implicit prior embedded within a CNN. While we were working on this project, Feng et al.  published a preprint where they used the score-based definition of diffusion models to introduce an image-based prior for inverse problems where the posterior score function is available. This work is most closely related to our approach. However, they focus on how to obtain samples and likelihoods from the true posterior. For that reason, they need guiding models to be proper score functions. We do not need that constraint and focus on guiding inverse problems defined by a more general energy function and focus particularly on the application to neuronal encoding models.

Image preprocessing for neural modelsThe neural models used in this study expect \(100 100\) images in grayscale. However, the output of the ImageNet pre-trained Ablated Diffusion Model (ADM)  is a \(256 256\) RGB image. We, therefore, use an additional compatibility step that performs i) downsampling from \(256 256 100 100\) with bilinear interpolation and ii) takes the mean across color channels providing the grayscale image. Each of these preprocessing steps is differentiable and is thus used end-to-end when generating the image.

### Experiments

Most exciting imagesWe apply EGG diffusion to characterize the properties of neurons in macaque area V4. For each of these experiments, we use the pre-trained ADM diffusion model trained on \(256 256\) ImageNet images from Dhariwal and Nichol . In each of our experiments, we consider two paradigms: 1) **within** architecture, where we use two independently pre-trained ensembles containing 5 models of the same architecture (Gaussian model or Attention model). We generate images on one and evaluate them on the other. 2) **cross** architecture, two independently pre-trained ensembles containing 5 models of different architectures (Gaussian model and Attention model). We demonstrate EGG on three tasks 1 Most Exciting Input (MEI) generation, where the generation method needs to generate an image that maximally excites an individual neuron, 2 naturalistic image generation, where a natural-looking image is generated that maximizes individual neuron responses, and 3 reconstruction of the input image from predicted neuronal responses. Running the experiments required a total of 7 GPU days. All computations were performed on a single consumer-grade GPU: NVIDIA GeForce RTX 3090 or NVIDIA GeForce RTX 2080 Ti depending on the availability.

MEIs have served as a powerful tool for visualizing features of a network, providing insights and testable predictions . For the generation of MEIs, we selected 90 units at random from a subset of all 1,244 for which both the Gaussian model and the Attention model achieve at least a correlation of 0.5 to the average responses across repeated presentations. We compare our method to a vanilla gradient ascent (GA) method  which optimizes the pixels of an input image \(\) to obtain the maximal response of the selected neuron. For the GA method, we use Gaussian blur preconditioning of the gradient. The stochastic gradient descent (SGD) optimizer was used with a learning rate of 10 and the image was optimized for 1,000 steps. We also evaluated other setups for the GA method without finding major differences (see Fig. S2). We define EGG diffusion with the energy function \(E(}_{0})=f_{i}(}_{0})\), where \(f_{i}\) is the \(i\)-th neuron model and \(}_{0}\) is the estimated clean sample. We optimize MEIs for both the Gaussian model and the Attention model. We set the energy scale to \(=10\) for the Gaussian model and \(=5\) for the Attention model. \(\) was chosen via a grid search, for more details refer to Fig. 5B. The diffusion process was run for \(100\) exposed time steps for the Gaussian model and \(50\) respaced time steps for the Attention model. For both EGG and GA, we set the norm of the \(100 100\) image to a fixed value of 25. For each of the methods, we chose the best of 3 MEIs optimized from different seeds. We show the influence of the initial seed on the generated MEI in figure S3. Furthermore, the images that are generated by the ADM model are RGB. We show examples of the color outputs in figure S4.

We show some examples of MEIs generated with EGG diffusion and GA for the two architectures in figure 3A. For more examples, refer to the supplementary materials figure S5. We find that the EGG-generated MEIs are significantly better (Attention) or similarly (Gaussian) activating within architectures and are significantly better at generalizing across architectures (Fig. 3B). This can also be observed by a significant increase in the mean activation across all units (Table 2). Perceptually, EGG-generated MEIs of the Attention model looked more complex and natural than the GA-generated MEIs, and more similar to MEIs of the Gaussian model pre-trained on natural image classification.

Comparing EGG-based MEIs to the ones found by Willeke et al.  using GA, we find that the preferred image feature is usually preserved, but MEIs generated for the Attention model are in most cases smaller in visual angle than their Gaussian model counterparts (Fig. S5). To quantify that the MEIs from the Attention model are smaller we compute an isotropic Gaussian envelope for the MEIs. We find that the Attention model generates MEIs for which their Gaussian envelope on average is smaller than for the Gaussian MEIs (\(_{At}\) = 49.62 vs \(_{Ga}\) = 55.36, Wilcoxon signed rank test p-value: 0.0078).

Finally, EGG diffusion is almost 4.7-fold faster than GA, requiring only on average 46s per MEI in comparison to the required 219s for the GA method (Fig. 4) on a single NVIDIA GeForce RTX 3090 across 10 repetitions. This

Figure 4: Mean comparison of the generation times between the EGG and GA (error bars denote standard error).

Figure 3: **a)** Examples of MEIs optimized using EGG diffusion and GA for macaque V4 Gaussian and Attention models. **b)** Comparison of activations for different neurons between EGG diffusion and GA on the Within and Cross Architecture validation paradigms. Line fits obtained via Huber regression with \(=1.1\). Curated image selection to show various properties of the neurons like fur, eyes, curves and edges.

[MISSING_PAGE_EMPTY:8]

the responses begin to plateau, or even decrease. Therefore, for generating MEIs, we use \(=10\) for the Gaussian model and \(=5\) for the Attention model. It can be further observed that decreasing \(\) increases the naturalness of the generated image while preserving the features of the image that the neuron is tuned towards. To quantify the increase in the naturalness of the MEIs across \(\)s, we measured the FID score between the generated images at different \(\) values and the top-5 ImageNet images (Fig. 5C). Our results show that by changing \(\) we approach the natural images manifold (lower FID). We also find that EGG generates MEIs (\(=1\)) similarly activating to the top-1 Imagenet images (Fig. S8).

Image reconstruction from unit responsesAnother application of EGG diffusion is image reconstruction from neuronal responses. A similar task has been attempted with success using diffusion models from human fMRI data . Given that only a small fraction of neurons were recorded, the image is encoded in an under-complete, significantly lower-dimensional space. Therefore, it is to be expected that the reconstructed image \(\) will not necessarily be equal to the ground truth image \(_{gt}\). However, a better reconstruction \(^{*}\) is one that generalizes across models. Therefore, regardless of the model \(f\) used, we should get \(||f(^{*})-f(_{gt})||_{2}=0\). This is trivially true for \(_{0}=x_{gt}\) but, given the complexity of the model, there are likely other solutions. We therefore consider a masked version of the reconstructions for visualization. We mask the reconstructions to the joint receptive field of all 1,244 neurons. The mask is obtained by computing the average absolute gradients \(=_{x}[|_{x}f(x)|]\) across the responses to the test images. The masks were normalized to be between 0 and 1 and the values below 0.25 are clamped to 0.

We can reconstruct images in the EGG framework by defining the energy function as an \(L_{2}\) distance between the predicted responses to the ground truth image \(f(_{gt})\) and the predicted responses to a generated image (Fig. 6A) \(E()=||f()-f(_{gt})||_{2}\). Note that, instead of \(f(_{gt})\), we could also use recorded neuronal responses. The images are generated from the Gaussian model with \(=2\) and 1000 timesteps, with the norm of the \(100 100\) image fixed to 60. We compare EGG to a gradient descent (GD) method that simply minimizes the L2 distance. The GD uses an AdamW optimizer with a learning rate of 0.05. In GD, at each optimization step the image \(_{t}\) is Gaussian blurred and the norm is set to 60 before passing to the neural encoding model. We optimize the GD reconstruction up to the point where the train \(L_{2}\) distance is matched between the GD and the EGG for a fair comparison of the generalization capabilities. We verified that the GD images do not improve qualitatively with more optimization steps (Fig. S9) We find that when generating the reconstruction using EGG diffusion we obtain 1) comparable within-architecture generalization and 2) much better cross-architecture generalization (Fig. 6B). The EGG-generated images produce lower within architecture distances for \(84\%\) of the images and for \(98\%\) in the cross-architecture case.

Figure 6: **a)** Schematic of the reconstruction paradigm. The generated image is compared to the ground truth image via \(L_{2}\) distance in the unit activations space. Reconstructions from 1,244 units. **b)**\(L_{2}\) distances in the unit activations space for the Within and Cross architecture domains comparing the EGG and GD generation methods. Shows that the EGG method generalizes better than GD across architectures. **c)** examples of reconstructions generated by EGG and GD in comparison to the ground truth (GT). **d)** Survey results on 45 voluntary human participants. Indicates that in 84% of images, the participants preferred the EGG generated reconstructions with a rate \( 0.6\).

[MISSING_PAGE_FAIL:10]