# Delegated Classification

Eden Saig,  Inbal Talgam-Cohen,  Nir Rosenfeld

Technion - Israel Institute of Technology

Haifa, Israel

{edens,italgam,nirr}@cs.technion.ac.il

###### Abstract

When machine learning is outsourced to a rational agent, conflicts of interest might arise and severely impact predictive performance. In this work, we propose a theoretical framework for incentive-aware delegation of machine learning tasks. We model delegation as a principal-agent game, in which accurate learning can be incentivized by the principal using performance-based contracts. Adapting the economic theory of contract design to this setting, we define _budget-optimal_ contracts and prove they take a simple threshold form under reasonable assumptions. In the binary-action case, the optimality of such contracts is shown to be equivalent to the classic Neyman-Pearson lemma, establishing a formal connection between contract design and statistical hypothesis testing. Empirically, we demonstrate that budget-optimal contracts can be constructed using small-scale data, leveraging recent advances in the study of learning curves and scaling laws. Performance and economic outcomes are evaluated using synthetic and real-world classification tasks.

## 1 Introduction

The acclaimed success of machine learning at effectively solving difficult prediction tasks across diverse problem domains has made it highly appealing for firms, institutions, and individual practitioners. But machine learning has also become increasingly complex, cumbersome, and difficult to operate--and not all those who seek to learn have access to the necessary expertise, infrastructure, and designated resources required for learning effectively. This gap has created a new market for _outsourced machine learning_, in which a client interested in obtaining an accurate predictive model can hire the services of a specialized provider which, for a price, trains the model on their behalf. Consider for example a hospital purchasing a classifier for deciding between hospitalization and outpatient treatment when triaging patients. The provider invests in curating, cleaning and annotating training data, and delivers a trained model in return to payment from the hospital.

Having a budget to expend on outsourced learning , we model the client as aiming to obtain the best possible predictive model. At first glance, it is tempting to assume that the optimal strategy is simply to pay the provider the maximal feasible amount--and hope to get a high-end model in return. After all, if the client were to spend the budget directly on learning, investing the maximal available sum would yield the best possible results. But this neglects to account for the _incentives_ of the provider, who is interested in maximizing profit. Since the actions of the provider remain private, it is in his best interest to (secretly) minimize efforts, which in turn can result in his delivering a suboptimally-trained model. In our example, the provider can cut costs by annotating only a subset of the data, obtaining cheaper low-quality annotations, or neglecting to meticulously remove all outliers.

Outsourced learning is hence susceptible to _moral hazard_, an economic situation which might occur under information asymmetry, and to the detriment of the client. Motivated by this observation, in this paper we initiate the study of _delegated learning_, and aim to explore the economic, algorithmic, and statistical implications that occur when learning is delegated to a specialized provider. Our key novelty is in instantiating delegated learning as a problem of _optimal contract design_. Broadly, contracts are an important monetary device that allows the client to establish a payment schemewhich, if properly set, serves to align incentives and guarantee that both parties are well-off. Our main challenge is to design effective contracts specialized to the task of delegated learning on a budget.

Towards this, we begin with a conventional supervised classification setup, and impose economic structure by assuming that acquiring training examples is costly. We then conceptually "split" the conventional self-sufficient learner into two rational entities: a _principal_, who controls the budget and is interested in maximizing predictive accuracy; and an _agent_, who controls learning (in particular the training set) and is interested in maximizing profit. This allows us to model principal-agent relations as a Stackelberg game, in which the principal commits to a _contract_\(t\), determining _a priori_ the amount to be paid for every possible (stochastic) level of obtained accuracy. The agent best-responds to the contract by choosing the profit-maximizing number of samples \(n\), and training the predictive model.

Under this setting, we study the algorithmic problem of designing an optimal contract. As is standard in economic analysis, we begin with the assumption that the principal has full information on the distribution of possible outcomes for each of the agent's possible actions. In our setting, actions correspond to the number of training samples, and outcomes to the empirical classifier accuracy; thus, the main object of interest for contract design in delegated learning settings is the _learning curve_, which describes the (stochastic) performance of learning per sample size. Under certain plausible conditions on the learning curve, namely MLRP and a certain notion of concavity, our main result here is that optimal contracts are _simple_, and in particular, take on the form of simple threshold functions. Simple contracts are appealing because they are straightforward to understand and communicate; in our setting, they are also easy to compute, and we give a closed-form solution for the optimal threshold contract. Providing an interpretation of the closed-form solution, our findings establish a new connection between contracts and the renowned Neymon-Pearson lemma , which we consider as one of our central theoretical contributions.

We then switch gears and turn to empirically studying the construction of contracts from partial information. In particular, we consider a setting where the principal can only estimate the learning curve from small available data (e.g., by bootstrapping on small \(n\) and extrapolating). Using the recent LCDB dataset of learning curves , we show that threshold contracts generally perform well on estimated curves despite the inherent uncertainty. We also explore the role of different parameters of our setup, consider various tradeoffs in curve-fitting and contract design, and discuss limitations by pointing out certain failure modes to which contracts may be susceptible.

Taken together, our results shed light on why and how simple contracts for delegated learning work to correctly balance between the incentives of both delegator and delegatee in outsourced learning.

### Related work

Previous works have considered delegation of ML-related tasks that differ from our task of training a classifier: labeling of data points in , gathering information in , and computing a costly high-dimensional function in . In the delegated task of , the agent provides a classifier and the principal verifies its near-optimality within \(\). A fundamental difference is that their agent is assumed to be adversarial rather than rational, and so interactive proofs are used instead of economic incentives.

The Neyman-Pearson lemma has recently been connected to economic design by  in the context of adverse selection rather than moral hazard. The agent has a hidden type (e.g., whether a new

Figure 1: Delegated classification interaction sequence. The principal examines initial information, and designs a contract \(t:\{0,,m\}_{ 0}\). Having observed \(t\), the agent strategically selects a dataset size \(n\) that will maximize his expected utility. He samples a training set \(S D^{n}\), incurs cost \(c_{n}\), then trains the classifier \(h\) and sends it to the principal. Upon receiving \(h\), the principal evaluates its accuracy on a random validation set \(V D^{m}\), and pays the agent according to the contract \(t\).

drug is effective), and the optimal menu to offer this agent is designed based on the theory of e-values. When the hidden type has binary support, the method is equivalent to Neyman-Pearson. A "moral" link between the design of contracts (for a non-budgeted principal) and statistical inference (in particular likelihood ratios) was observed already in , but no connection was made to the power of hypothesis tests. Other intersections of ML and contracts that do not involve delegation of learning-related tasks include strategic classification [e.g., 36, 37, 3] and online learning of optimal contracts  Contract settings with a binary action and/or outcome space have been studied in [e.g., 20, 7, 22]. Not to be confused with our notion of delegation, there is a growing computational literature on delegation without monetary payments [e.g., 35].

To extrapolate from partial data, our work builds upon recent advancements in the study of learning curves, which characterize the expected generalization of learning as a function of dataset size and other exogenous factors . There is growing empirical evidence that performance of modern neural networks can be predicted using simple scaling laws [e.g., 34, 41, 50, 23, 2, 46, 30], and theoretical results that back these findings in simplified settings .

Perhaps closest to ours is the concurrent work , which analyzes a similar setting under different assumptions: Rather than assuming budget constraints, they assume that the principal's utility is linear in accuracy and payout, and the learning curve takes a specific functional form. Based on these assumptions, they derive approximately optimal linear contracts which are robust to adverse selection. In contrast, we obtain globally optimal simple contracts based on hypothesis testing, and present data-driven methods to handle partial information. Together, the two studies demonstrate the important role of simple contracts in the growing ecosystem of machine learning delegation.

## 2 Problem Setup

The core of our setting is based on a standard supervised classification task. Let \(x\) be features and \(y\) be labels, and assume there is some unknown joint distribution \(D\) over \((x,y)\) pairs. Given a sample set \(S=\{(x_{i},y_{i})\}_{i=1}^{n} D^{n}\), the goal in learning is to use \(S\) to find a classifier \(h:\) from a class \(\) that maximizes expected accuracy, \(_{D}(h)=_{(x,y) D}[h(x)=y]\). Because the underlying distribution \(D\) is unknown, expected performance is estimated by the empirical average on an additional held-out validation set \(V D^{m}\) of size \(m\), as \(_{V}(h)=_{i=1}^{m}[h(x_{i})=y_{i}]\), which is a consistent and unbiased estimator of \(_{D}(h)\). We will assume throughout that both the learning algorithm and the validation set size \(m\) are known and fixed.

Learning on a budget.We will be interested in studying learning when certain resources are limited or costly. Our main focus will be on the setting where the main cost of learning is the number of labeled examples \(n\), but we note that our approach can in principle extend to other forms of learning 'effort'.1 We assume the learner has a monetary budget \(B\) to spend on samples, and is interested in maximizing accuracy under budget constraints. Let \(c_{n} 0\) be the cost of \(n\) samples (assumed to be increasing in \(n\)), then the learner aims to solve:

\[n^{*}=_{n}_{h_{n}}[_{D}(h_{n})]  c_{n} B\] (1)

where \(h_{n}\) is a classifier learned from a random dataset of size \(|S|=n\). Note that \(h_{n}\) is a random variable with distribution depending on \(n\). We denote the out-of-sample accuracy of \(h_{n}\) by \(_{n}=_{D}(h_{n})\). When the learner is a self-sufficient entity, and when the expected \(_{n}\) improves monotonically in \(n\), then \(n^{*}\) in Eq. (1) in naturally the largest affordable \(n\) (see Sec. 3). However, as we will see, when learning is _delegated_--this seemingly straightforward observation can break.

Delegation.We model the delegation of learning as a conceptual partition of the learner into two distinct entities: an _agent_, who controls learning; and a _principal_, who controls the validation process. The principal outsources the learning task to the agent, who in turn uses the training set \(S\) to train the classifier \(h\); once delivered, the principal validates the performance of \(h\) using the validation set \(V\). Whereas the classifier's accuracy benefits the principal alone, the cost of learning (i.e., the cost of acquiring \(S\)) is born exclusively by the agent. Importantly, the amount of invested effort remains private to the agent; in our example, the principal cannot know how many examples received quality labeling. Because the agent seeks to maximize profit, the principal can use her budget as a source of monetary payment to incentivize the agent to invest in larger \(|S|=n\). Intuitively, one could expect larger payments to entail larger \(n\), and therefore higher-accuracy \(h\). However, as we will see, this is not always the case, and careful planning is required in order to fully utilize a given budget.

### Delegation as contract design

As the training set remains private to the agent, there is an information gap between the two parties. This creates a conflict of interest for the agent known as _moral hazard_, in which the agent may be tempted to invest sub-par effort, while claiming that efforts were in fact his honest best. In economics, the celebrated solution to moral hazard are _contracts_: pay-per-performance rules that a-priori determine future payments for every possible outcome, which we formally describe next.

Contract design.A contract setting is defined by a set of actions \(=\{a_{1},,a_{N}\}\) that can be taken by the agent, and a set of possible outcomes \(j\{0,,m\}\). Each action \(a_{i}\) is associated with a cost \(c_{i}\), and w.l.o.g. we assume \(c_{1} c_{N}\) so that actions correspond to increasing _effort levels_. The agent's choice to perform action \(a\) yields a random outcome \(j f_{a}\) for the principal, where \(f_{a}\) describes a distribution over the possible outcomes associated with action \(a\). The principal, who does not observe the agent's chosen action, can incentivize the agent through a _contract_, \(t:\{0,,m\}_{ 0}\), according to which she pays the agent \(t(j) 0\) when the materialized outcome is \(j\). Given contract \(t\), let \(u_{a}(t)\) be the agent's expected _utility_ from taking action \(a\) at cost \(c_{a}\) (via stochastic outcomes \(j f_{a}\)), and let \(a(t)\) be the agent's _best response_--an action that maximizes his expected utility (and following standard tie-breaking assumptions as in ). Then:

\[u_{a}(t)=_{j f_{a}}[t(j)]-c_{a}, a(t) {argmax}_{a}u_{a}(t).\] (2)

Every action \(a^{*}\) that is the best response \(a^{*}=a(t)\) to some contract \(t\) is called _implementable_. In economic terms, the principal and agent are playing a _Stackelberg game_, in which the principal commits to a contract \(t\) and the agent best-responds by choosing action \(a(t)\) that maximizes his expected utility \(u_{a}(t)\). The goal of the principal is to design a contract \(t\) which incentivizes the agent to take best-response actions yielding favorable outcomes for the principal.

Contracts for delegated learning.We propose to formulate delegated learning as a problem of optimal contract design, instantiated as follows. First, we relate agent actions \(a\) with the number of samples \(n\), and denote \(=\{n_{1},,n_{N}\}\) as the possible sizes of \(S\) that the learning agent can work with. The cost of acquiring samples naturally maps as \(c_{a}=c_{n}\), and agent's best response is \(a(t)=n(t)\). Next, we associate outcomes \(j\) with accuracy for the principal by defining \(j\) as the number of validation samples (out of the possible \(m\)) on which \(h\) is correct; note this implies \(_{V}(h)=j/m\), and we will therefore use \(j\) and \(_{V}(h)\) as 'outcomes' interchangeably. Finally, for an action \(n\), we set \(f_{n}\) to be the distribution over possible accuracies obtained when learning with \(n\) samples, namely \(f_{n}(j)=_{h_{n},V}[_{V}(h_{n})=j/m]\  j\). We will also use the matrix form \(F_{nj}=f_{n}(j)\), where \(F^{N(m+1)}\). Note that \(F\) admits two sources of variation: (i) _a-priori_ variation in \(h_{n}\) due to stochasticity in \(S D^{n}\); and (ii) _a-posteriori_ variation in \(j\) for any fixed

Figure 2: A delegated classification setting (data from Sec. 4). **(Left)** Each costly action taken by the agent (training set size \(n\)) induces a distribution \(f_{n}\) of possible outcomes (classifier accuracy). The principal seeks to construct a contract \(t\) that incentivizes a profit-maximizing agent to take actions entailing favorable outcomes. Note the \(f_{n}\) exhibit increasing expectation, but decreasing variance, in \(n\). **(Center)** Three contracts for a given budget \(B\), mapping outcomes to payments. **(Top-right)** Agentâ€™s utilities \(u_{n}(t)\) and best responses \(n(t)\) (stars) for each contract \(t\). **(Bottom-right)** Expected accuracies for principal resulting from each contract; here the threshold contract is optimal (see Sec. 3).

due to stochasticity in \(V D^{m}\). When \(h_{n}\) is fixed, the outcome distribution admits a simple binomial form, namely \(j(m,_{n})\). Empirically, we observe this to be the dominant component.

### Delegation as an optimization problem

Budget-optimal contracts.Recall that the principal seeks to maximize accuracy under budget constraints (Eq. (1)). Once learning is delegated to an agent and framed as a contract design problem, the principal's objective becomes:

\[t^{*}=_{t[0,B]^{m}}\,_{h_{n(t)}}[_{D}( h_{n(t)})]\] (3)

Contract \(t^{*}\) is chosen to incentivize the agent to invest effort \(n(t)\) (via Eq. (2)) such that the training of \(h_{n(t)}\) yields high dividends for the principal in terms of expected accuracy. We will refer to \(t^{*}\) as a _budget-optimal contract_, and to the general task of finding \(t^{*}\) as _budget-optimal contract design_.

Information structure.Delegated learning settings have actions \(n\) and costs \(c_{n}\) known to both sides, and outcome distribution \(F_{nj}\) known to the agent. For the principal, we explore varying levels of knowledge: In Sec. 3, we assume (as in the classic contract design literature) that the principal has full information of \(F\) (i.e., knows the learning curve), and focus on characterizing the optimal contract. In Sec. 4 we relax this assumption, and explore a partial-information setting in which the principal relies instead on an empirically-estimated curves \(\).

## 3 Budget-Optimal Contract Design

### The problem

Why agents cut corners.The conceptual challenge in designing contracts lies in that agents cannot reliably report _what_ they did. For example, consider a principal who, after delegation, received a classifier attaining 0.74 (validation) accuracy. Should she be happy? The crux is that there are two ways that this could have happened: (i) the agent invested high effort in learning (large \(n\)), but received an uninformative \(S\) by chance, and delivered a low-quality \(h\) as a result; and (ii) the agent invested low effort (small \(n\)). Since the agent's actions are private, and because outcomes are stochastic, the principal can never know for certain which is the true underlying cause. In other words, a 'lazy' (or rather strategic) agent can hide behind the uncertainty that is inherent in learning outcomes.2

Contract types.To overcome this informational gap, the principal must devise a contract to align incentives and encourage the agent to prefer certain actions over others. But not all contracts are equally effective. Fig. 2 illustrates for a budget \(B\) three contract types and their economic implications:

* **Constant contract** (\(t(j)=B\)): The agent is paid \(B\) regardless of the outcome. His best-response in this case is to choose the least-costly action--to the detriment of the principal.
* **Linear contract** (\(t(j)=Bj/m\)): The agent is paid a fraction of \(B\), linear in the resulting accuracy. Linear contracts are a popular and extensively-studied class of contracts (e.g., 32; 15). Nonetheless, and though seemingly sensible, linear contracts turn out to be sub-optimal for our setting.
* **Threshold contract** (\(t(j)=B[j j_{0}]\) for some \(j_{0}\)): The agent is paid \(B\) provided the empirical accuracy surpasses a threshold \(j_{0}\). In the example in Fig. 2, the threshold contract is optimal.

Rather than committing _a-priori_ to some type of contract, we seek to find the best budget-optimal contract by solving Eq. (3). For this it is useful to have _structure_.

Stochastic learning curves (and where to find them).Our approach uses the observation that there is a tight connection between the set of distributions \(\{f_{n}\}\) encoded in \(F\), and _learning curves_, which describe the anticipated accuracy of a classifier as a function of the size of its training set. Learning curves typically depict only expected accuracy, but there is also inherent variation in outcomes. We will therefore broadly use the term'stochastic learning curve' to describe both mean trend _and_ variation in accuracy as a function of \(n\); formally, a stochastic learning curve is defined precisely by \(F\). This connection is useful because learning curves have structure: First, expected learning curves are typically _monotone_[34; 11]; when not [43; 49], they can be monotonized . Second, stochastic learning curves are likely to satisfy the _monotone likelihood ratio property_ (MLRP), which states that the better the performance of a classifier, the more likely it was trained on more data (see Def. 1).

### Optimization via min-budget contracts

Our main technique for solving Eq. (3) relies on a reduction to what we refer to as _min-budget contracts_. Given an (implementable) target action \(n^{*}\), a min-budget contract for \(n^{*}\) is a contract \(t\) that incentivizes the agent to employ precisely the action \(n^{*}\), while minimizing the maximum payment by the principal \(\|t\|_{}=_{j\{0,,m\}}\{t(j)\}\); i.e., \(t\) implements \(n^{*}\) at minimum budget. Formally:

\[t^{*}=*{argmin}_{t}\|t\|_{}  n(t)=n^{*}\] (4)

Our reduction relies on the following claim (Proof in Appendix B.1):

**Proposition 1**.: _Every budget-optimal contract design problem has an optimal solution which is also min-budget._

Using Prop. 1, a solution to Eq. (3) can be obtained by iteratively solving Eq. (4): For all \(n_{i}\), solve Eq. (4) with target action \(n^{*}=n_{i}\), and return \(t^{*}(n_{i})\) for the best implementable \(n_{i}\) whose budget does not exceed \(B\). The budget-optimal problem thus reduces to solving multiple min-budget problems.

To compute each \(t^{*}(n_{i})\) in Eq. (4), we formulate a novel MIN-BUDGET linear program (LP),3 detailed in Appx. B.2. One way to solve this LP is with generic solvers--an approach which is valid, but can be costly. One of our contributions is in identifying natural cases where min-budget contracts take on _simple_ forms, which are easier to optimize, and have practical merit. In particular, we show that binary-action contracts have _all-or-nothing_ structure (\(t(j)\{0,B\}\)), and plausible structural assumptions on the learning curve give rise to _threshold_ contracts (\(t(j)=B[j j_{0}]\) for some \(j_{0}\)). Our theoretical results are summarized in Table 1, and detailed in the rest of the section.

### All-or-nothing contracts: Binary action space and the statistical connection

We begin with a simple delegated learning setting in which the agent can choose one of two actions, \(=\{n_{1},n_{2}\}\), e.g., a'small' vs. 'large' training set, and the principal seeks to incentivize training with more data.4 This reduced case will be useful as a building block for the general case (which we return to in Sec. 3.4), and for making a precise connection between contract design and hypothesis tests.

Simple min-budget contracts for binary action space.Our first result shows that optimal binary-action contracts are all-or-nothing contracts whose budget is determined by the _total variation distance_ between outcome distributions \(f_{2}\) and \(f_{1}\), namely \(\|f_{2}-f_{1}\|_{}=_{j=0}^{m}|f_{2,j} -f_{1,j}|\).

**Theorem 1** (Optimal binary-action contract).: _In a binary-action contract setting with outcome distributions \(f_{1},f_{2}\) and costs \(c_{1},c_{2}\), the min-budget contract is an all-or-nothing contract, given by:_

\[t^{*}(j)=B[f_{2}(j) f_{1}(j)] j\{0, ,m\},\;B=-c_{1})}}{{\|f_{2}-f_{1}\|_{}}}.\] (5)

The proof (in Appendix B.4.2) is by LP duality. Intuitively, the optimal contract pays the agent for outcomes that are more likely to come from \(f_{2}\) than from \(f_{1}\). Moreover, it requires a higher budget the smaller the distance is between the two distributions \(f_{1},f_{2}\). At the extremes, if their distance is 1 (i.e. no overlap among their supports), the required budget for incentivizing \(n_{2}\) is \(c_{2}-c_{1}\), whereas if their distance is 0 (i.e. \(f_{1}=f_{2}\)) it becomes impossible to incentivize \(n_{2}\).

    &  \\  Actions & Outcomes & No assumptions & MLRP & Concave-MLRP \\  \(||=2\) & any size & All-or-nothing (T1) & Threshold (B.7.1) \\ \(||>2\) & \(m+1=2\\ m+1>2\) & All-or-nothing (B.5.1) & Threshold (B.7.2) \\ Simple is NP-hard (T3) & \(\) non-threshold (B.7.3) & Threshold (T4) \\   

Table 1: Characterization of simple min-budget contracts in different settings. Simple contract forms include _all-or-nothing contracts_ and their subclass of _threshold contracts_. The table specifies for each configuration either the simple form that is optimal (in one case through equivalence to the Neyman-Pearson lemma), or that the simple form is non-optimal or intractable.

Formal connection to optimal hypothesis testing.Theorem 1 also uncovers a direct correspondence between optimal contracts and optimal hypothesis tests. Intuitively, given the outcome distributions \(\{f_{1},f_{2}\}\), and in order to incentivize \(n_{2}\), the principal wishes to pay the agent if the observed outcome \(j\{0,,m\}\) is more likely to have originated from \(f_{2}\). The principal can attempt to identify whether the outcome \(j\) is drawn from distribution \(f_{1}\) or \(f_{2}\) through hypothesis testing, where a hypothesis test \(:\{0,,m\}\{0,1\}\) maps a sample \(j\) to either \(f_{2}\) (indicated by 1) or to the null hypothesis \(f_{1}\) (indicated by 0). For our purpose it is convenient to allow tests to be non-integral, in which case \(:\{0,,m\}\) maps \(j\) to a _probability_ with which it originates from \(f_{2}\). The quality of a hypothesis test is measured by summing its type-1 and type-2 errors: \(_{j=0}^{m}f_{1,j}_{j}+_{j=0}^{m}f_{2,j}(1-_{j})\). The test that minimizes this sum is known as the _most powerful_ hypothesis test, and has been characterized by Neyman and Pearson [45, 4.3].

We now turn to formally establishing the connection. For a fixed \(B\), observe that every contract with budget \(B\) can be mapped to a hypothesis test via the bijection \((j)=}{{B}}\). Then:

**Theorem 2** (Optimal contract vs. test).: _Consider binary-action contract design with distributions \(f_{1},f_{2}\) and costs \(c_{1},c_{2}\). A contract \(t\) with budget \(B\) is optimal if and only if its corresponding hypothesis test \(=}{{B}}\) is maximum power with type-1 and type-2 errors summing to \(1--c_{1}}}{{B}}\)._

The proof (Appendix B.4.2) is by a non-linear variable transformation to the MIN-BUDGET LP. Theorem 2 implies that the optimal contract for the binary-action case (Theorem 1) is equivalent to the well-known Neyman-Pearson lemma characterizing the most powerful hypothesis test:

**Lemma 1** (Neyman-Pearson [e.g., 45]).: _Let \(f_{1},f_{2}\) be two discrete probability distributions. Then the most powerful hypothesis test for \(f_{1},f_{2}\) is the likelihood ratio test \((j)=[f_{2}(j) f_{1}(j)]\), which attains the optimal bound \(1-\|p-q\|_{}\) on the sum of type-1 and type-2 errors._

Theorem 2 establishes a new formal connection between the two domains of contract design and hypothesis testing. In the context of contract design, it provides a statistical interpretation: A min-budget contract can be interpreted as an optimal hypothesis test, and the ability to distinguish between the two hypotheses determines the required budget. In the converse direction, it enables a new proof for the Neyman-Pearson using the min-budget contract given by Theorem 1 (see Appendix B.4.2).

### All-or-nothing contracts: Beyond binary action

For general action spaces, optimal contracts are not guaranteed to be all-or-nothing. In fact, we show that determining whether there exists an all-or-nothing contract that is optimal is NP-hard:

**Theorem 3** (Hardness).: _Finding a min-budget all-or-nothing contract is NP-hard._

The proof is by reduction from 3SAT, and appears in Appx. B.5.2. Nonetheless, there are special but important cases--notably the binary outcome case5-- in which results from Sec. 3.3 hold, suggesting that ideas from Thm. 1 apply more broadly. The following algorithm makes use of these ideas, showing good empirical performance, and provable performance under an MLRP condition (Sec. 3.5).

Single binding action algorithm.Revisiting the closed-form all-or-nothing contract in Eq. (5), we observe that the result is based on the fact that binary action spaces have only one alternative action. Building upon this observation, we propose the _single binding action_ (SBA) algorithm, which computes a solution to Eq. (4) in the general (many-actions) case: Given target action \(n^{*}\), loop over all actions \(n n^{*}\), and apply the closed-form formula in Eq. (5) to obtain an (all-or-nothing) contract \(t^{*}(n,n^{*})\). If the agent's best response (Eq. (2)) satisfies \(n(t^{*}(n,n^{*}))=n^{*}\), return \(t^{*}\). If the loop ends without returning a contract, return 'fail'. The following claim shows the algorithm is sound:

**Proposition 2** (Soundness of SBA).: _When the single binding action algorithm terminates successfully, it returns an optimal contract which is an all-or-nothing contract._

Proof in Appendix B.6. Prop. 2 ensures that if SBA succeeds, then the returned all-or-nothing contract \(t^{*}\) is optimal. Moreover, failure does not preclude the existence of an optimal all-or-nothing contract. If SBA fails, we solve Eq. (4) with a generic LP solver. Empirically, SBA was successful in more than 85% of cases, and is \(\)\(10^{3}\) times faster than the LP solver (Appendix C.3). Thus, this optimistic 'try SBA first' approach typically succeeds, adds negligible overhead if not, and guarantees correctness.

### Threshold contracts: MLRP assumption

In this section we provide sufficient conditions, in the form of natural structural properties of (stochastic) learning curves, that guarantee the optimality of even _simpler_ contracts--namely _threshold contracts_--and the success of SBA. With inspiration from hypothesis testing  and contract theory [26; 19], it is natural to consider the _monotone likelihood ratio property_ (MLRP) assumption:

**Definition 1** (MLRP [e.g., 26]).: _A contract design setting satisfies MLRP if for every pair of actions \(a,a^{}\) such that \(c_{a}<c_{a^{}}\), the likelihood ratio \({{}_{f^{a^{}}(j)}}/{{}_{f_{a}(j)}}\) is monotonically increasing in \(j\)._

In our context, MLRP states that the better the validation-set performance of a classifier, the more likely it was trained on more data. This holds in particular for monotone learning curves with binomial outcome distribution [19; B.1]. For a binary action space, MLRP ensures that \(n_{2}\) is always implementable,6 and that the optimal contract is a threshold contract: \(t^{*}(j)=B[j j_{0}]\). This is by Theorem 1, and by the fact that \( j_{0}\) such that \({{}_{f_{2}(j)}}/{{}_{f_{1}(j)}} 1\) iff \(j j_{0}\) (see also Appendix B.7.1).7 Interestingly, this is similar to the relation between the Neyman-Pearson lemma and the Karlin-Rubin theorem, which characterizes the most powerful hypothesis test under monotone likelihood ratio .

MLRP for general action space.MLRP does not guarantee threshold contracts in general: In Appendix B.7.3, we give a constructive counterexample satisfying MLRP, but for which the optimal contract is not threshold. However, refining MLRP to also capture 'diminishing returns' turns out to be sufficient for recovering guarantees generally. For target action \(a_{N}\), denote the _survival probability_ of an action \(a_{i}\) by \(s_{i}=_{j f_{i}}[j j^{*}]\), where \(j^{*}\) is the minimal outcome at which action \(a_{N}\) is more likely than \(a_{N-1}\). These will serve as formal means for capturing the concavity of learning curves.

**Definition 2** (C-MLRP).: _A contract design setting satisfies Concave-MLRP (C-MLRP) if it satisfies MLRP, and additionally the actions' survival probability is concave as a function of the actions' cost._

Our final result shows that C-MLRP guarantees optimality of threshold contracts, and success of SBA:

**Theorem 4** (Sufficiency for threshold).: _Consider a contract design setting with C-MLRP. Then the optimal contract is a threshold contract, and is recovered by the SBA algorithm._

We prove this claim by showing that concavity implies that only one alternative action is binding in the linear program equivalent to Eq. (4), reducing the problem to the two-action case. By applying Theorem 1, we obtain optimality of threshold contracts in this case as well (proof in Appendix B.7.3). This also implies that SBC always terminates successfully on inputs that satisfy C-MLRP.

In practice, we believe that C-MLRP is a reasonable assumption for realistic learning curves: in Appendix B.8.1, we prove it is satisfied by a standard theoretical model of learning curves, and in Appendix C.3, we empirically demonstrate that it is (approximately) satisfied in settings where threshold contracts are optimal. Interestingly, in our empirical study, threshold contracts were often optimal even when C-MLRP did not hold--suggesting the condition is sufficient, but not necessary (see Sec. 4.1).

## 4 Experiments

We now turn to our empirical investigation of delegated learning under full and partial information. We base our experiments on the recently curated Learning Curves Database (LCDB) , which includes a large collection of stochastic learning curves for multiple classification datasets and methods. For each dataset and method, the database includes held-out accuracy measurements obtained for increasing sample sizes \(n\{2^{4},2^{4.5},,2^{15}\}\), with multiple repetitions per \(n\); these provide us with stochastic learning curves. Here we focus primarily on the popular MNIST dataset  as our case study, and on MLP and GBDT as representative classifiers, but we refer the reader to Appendix C for further experiments on additional datasets and methods. Code is available at: https://github.com/edensaig/delegated-classification.

### Full information

We begin with the full information setting to explore in a clean environment how different parameters of the learning setting and environment affect predictive performance and economic outcomes.

Validation set size.Fig. 3 (left) presents typical stochastic learning curves for two learning algorithms: Multi-Layered Perceptron (MLP) and Gradient-Boosted Decision Trees (GBDT). We take an arbitrary accuracy point on the curve at \((n)=0.85\) (dotted line) to examine the effects of validation set size \(m\) on min-budget contracts. Notice that MLP requires larger \(n\) to obtain 0.85; Fig. 3 (center) shows how this translates to a larger required budget \(B^{*}\), which holds for all \(m\). As \(m\) increases, required budgets and the difference between them both decrease. Larger validation sets are therefore useful for reducing required budget. Nonetheless, even for reasonable \(m\), obtained budgets still remain higher than their theoretical lower bounds (target action costs \(c_{n}\).).

Budget regimes.Fig. 3 (left) also indicates two points in which the learning curves cross (dashed lines), at \(\)\(0.74\) and \(\)\(0.94\) accuracy. These correspond to sample sizes \(n\) for which both methods obtain matching accuracies (in expectation). For a self-sufficient learner, the implication is that at each of these points, both methods are equally costly, i.e., both cost \(c_{n}\). Interestingly, and in contrast, delegation can entail different required budgets _despite_ equal accuracies. Fig. 3 (right) shows for each target accuracy the gap in required budgets between both methods, \( B^{*}=B^{*}_{}-B^{*}_{}\). As can be seen, each method is comparatively more (or less) costly in different accuracy regimes (up to 0.6; between 0.6 and 0.92; and above 0.92). Crucially, the budget gap can be large even when accuracies match (dashed lines). For example, even though both MLP and GBDT require \(n{=}362{}2^{17/2}\) samples to obtain \(\)\(0.74\) accuracy, GBDT is cheaper (\( B^{*}{=}-10^{2}\)); for \( 0.94\) which requires \(n{=}23170{}2^{29/2}\) from both, GBDT is significantly more expensive (\( B^{*}{=}10^{5}\)). The reason for this is that optimal budgets are determined by the ability to distinguish between distributions (Sec. 3.3).

Prevalence of simple contracts.To understand the applicability of our theoretical findings, in Appendix C.3 we conduct an empirical prevalence evaluation on additional learning algorithms, and across target actions. We observed that min-budget contracts assume a threshold form and the SBC algorithm returns correct results in more than 85% of cases overall. Restricting optimization to simple contracts, budget requirements were generally less than 1% higher than that of a min-budget contract, suggesting that simple contracts may provide a good approximation even when min-budget contracts do not assume a simple form.

### Partial information

We now turn to consider delegation under partial information, in which the principal must rely on an estimated learning curve. We instantiate this idea by assuming that the principal has access to a small 'pilot' dataset of size \(k\), where \(k\) is considered small. Using this set, the principal creates an estimated learning curve \(\) by fitting a curve to accuracies obtained for up to some \(n_{0} k\), and extrapolating to larger \(n>n_{0}\). In particular, we experiment with fitting parametric power-law curves of the form \([_{n}]=a-bn^{-c}\), which have been shown to provide good fit in various scenarios both empirically and theoretically [49; 34; 11]. Since power-law curves are monotone, composition with binomial distributions increasing in \(p\) provably results in MLRP stochastic curves [19; B.1].

Bias-variance tradeoff.Given \(k\) pilot examples, there are different ways in which the principal can use them to construct an estimated curve. Here we consider a simple tradeoff: setting \(n_{0}\) to be small but with more samples per \(n<n_{0}\) (low variance), or setting \(n_{0}\) to be large but with few samples per \(n<n_{0}\) (low bias). We define \(r\) as the number of samples per \(n\) (so low \(r\) means larger \(n_{0}\)). Then, for a

Figure 3: Delegating with full information. **(Left)** Typical learning curves for two learning algorithms on MNIST. **(Center)** Required budget for target accuracy of \(0.85\) per validation set size \(m\). **(Right)** Different cost regimes, indicating per accuracy region which of the two methods is cheaper to delegate.

given \(r\), we set \(n_{0}\) such that \(_{n n_{0}}r n k\) (i.e., such that the total number of used samples does not exceed \(k\)). Fig. 4 (left) shows different curve fits for \(r\{1,3,5\}\), and corresponding \(n_{0}\). Then, Fig. 4 (center-left) shows for a certain fixed budget the accuracy level that can be attained for increasing \(k\), and as a function of \(r\). As can be seen, having sufficient points \(k\) for constructing \(\) is important, but performance grows quickly with \(k\) (note log-scale x-axis). It is also apparent in our example that low bias (via larger \(n_{0}\)) is much more important than low variance for constructing useful \(\).

Cost-efficiency tradeoff.Because the pilot set provides the principal a basic means for obtaining minimal accuracy, we can ask: given \(k\) examples, and for a fixed budget \(B\), what is the added benefit of delegating learning? For this, we define \((k)=n()/k\) to be the _sample-size multiplier_, i.e., the multiplicative gain in the effective number of samples due to delegation. Fig. 4 (center-right) shows \((k)\) for increasing \(k\) and across \(r\). For \(r=1\) (which is superior in terms of performance and outcomes), \(\) begins at \(\)10, increases to \(\)30 at around \(k=190\), and slowly decreases back to \(\)\(10\) towards \(k=1,000\). For \(r>1\), we observe that \( 1\), i.e., there is effectively no gain from delegation, until around \(k=100\), only after which some gain is restored. This highlights the importance of obtaining an accurate estimate \(\) in terms of the economic consequences of delegation.

Over vs. under-estimation.Typically in curve-fitting, over and under-estimation are treated equally, since both types of error can negatively affect goodness of fit and extrapolation quality. However, for delegation, the implications of over vs. under-estimation on contract outcomes are highly asymmetric. Fig. 4 (right) shows for a target incentivized number of samples \(n(t^{*})\) the relation between the (theoretical) _signed_ extrapolation error \(n(t^{*})\) (i.e., over- or under-estimate, measured in accuracy points) and the eventual loss in accuracy obtained through delegation, relative to perfect estimation. Each point in the plot corresponds to one curve-fitting instance, with points shown for varying \(k\), \(n_{0}\), and \(r\), and with multiple independent repetitions. Results show that in the under-estimation regime (i.e., _negative_ extrapolation error), loss in accuracy degrades gracefully with the estimation error. In stark contrast, even minimal over-estimation (_positive_ extrapolation error) causes accuracy to plummet dramatically, as the agent's rational response in those cases was to use the smallest dataset possible. We interpret this as a consequence of'setting the bar too high'--a rational decision to minimize effort in response to unrealistic expectations. This has important implications for the choice of how to fit and extrapolate learning curves, suggesting that contracts can be tolerant to under-estimation, while over-estimation should be avoided at all costs.

## 5 Discussion

Motivated by the increasingly-common practice of outsourcing learning tasks, this paper sets out to introduce and study the novel problem of delegated classification. Our findings suggest that conflict of interests should not be overlooked, and that contracts hold potential as a means for aligning them. Our analysis relies on a set of assumptions, which should be carefully considered by practitioners and empiricists alike; we also believe that there are likely further fruitful connections to explore between contracts and statistical hypothesis testing. As a problem of contract design, and when the learning task is reasonably well-behaved, delegated learning manifests in the form simple threshold contracts. A natural question for future work is whether simplicity also implies _robustness_ to partial knowledge--as is often the case .

Figure 4: Delegating with partial information. **(Left)** Extrapolated learning curves for different \(r\). **(Center-left)** Accuracy obtained via delegation per pilot set size \(k\). **(Center-right)** Multiplicative gain in effective number of samples due to delegation. **(Right)** Implications of over vs. under-estimation.

Acknowledgements.The authors would like to thank Ruth Heller, Shafi Goldwasser, Jonathan Shafer, Ohad Einav, and anonymous reviewers for their insightful remarks and valuable suggestions. Nir Rosenfeld is supported by the Israel Science Foundation grant no. 278/22. Eden Saig is supported by the Israel Council for Higher Education PBC scholarship for Ph.D. students in data science. Funded by the European Union (ERC, ALGORCONTRACT, 101077862, PI: Inbal Talgam-Cohen).