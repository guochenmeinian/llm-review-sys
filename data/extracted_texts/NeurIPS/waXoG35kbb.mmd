# Provable benefits of score matching

Chirag Pabbaraju

Stanford University

cpabbara@cs.stanford.edu

&Dhruv Rohatgi

MIT

drohatgi@mit.edu

&Anish Sevekari

Carnegie Mellon University

asevekar@andrew.cmu.edu

&Holden Lee

Johns Hopkins University

hlee283@jhu.edu

&Ankur Moitra

MIT

moitra@mit.edu

&Andrej Risteski

Carnegie Mellon University

aristesk@andrew.cmu.edu

###### Abstract

Score matching is an alternative to maximum likelihood (ML) for estimating a probability distribution parametrized up to a constant of proportionality. By fitting the "score" of the distribution, it sidesteps the need to compute this constant of proportionality (which is often intractable). While score matching and variants thereof are popular in practice, precise theoretical understanding of the benefits and tradeoffs with maximum likelihood--both computational and statistical--are not well understood. In this work, we give the first example of a natural exponential family of distributions such that the score matching loss is computationally efficient to optimize, and has a comparable statistical efficiency to ML, while the ML loss is intractable to optimize using a gradient-based method. The family consists of exponentials of polynomials of fixed degree, and our result can be viewed as a continuous analogue of recent developments in the discrete setting. Precisely, we show: (1) Designing a zeroth-order or first-order oracle for optimizing the maximum likelihood loss is NP-hard. (2) Maximum likelihood has a statistical efficiency polynomial in the ambient dimension and the radius of the parameters of the family. (3) Minimizing the score matching loss is both computationally and statistically efficient, with complexity polynomial in the ambient dimension.

## 1 Introduction

Energy-based models are a flexible class of probabilistic models with wide-ranging applications. They are parameterized by a class of energies \(E_{}(x)\) which in turn determines the distribution

\[p_{}(x)=(x))}{Z_{}}\]

up to a constant of proportionality \(Z_{}\) that is called the partition function. One of the major challenges of working with energy-based models is designing efficient algorithms for fitting them to data. Statistical theory tells us that the maximum likelihood estimator (MLE)--i.e., the parameters \(\) which maximize the likelihood--enjoys good statistical properties including consistency and asymptotic efficiency.

However, there is a major computational impediment to computing the MLE: Both evaluating the log-likelihood and computing its gradient with respect to \(\) (i.e., implementing zeroth and first order oracles, respectively) seem to require computing the partition function, which is often computationally intractable. More precisely, the gradient of the negative log-likelihood depends on \(_{} Z_{}=_{p_{}}[_{}E_{ }(x)]\). A popular approach is to estimate this quantity by using a Markov chain to approximately sample from \(p_{}\). However in high-dimensional settings, Markov chains often require many, sometimes even exponentially many, steps to mix.

Score matching (Hyvarinen, 2005) is a popular alternative that sidesteps needing to compute the partition function of sample from \(p_{}\). The idea is to fit the score of the distribution, in the sense thatwe want \(\) such that \(_{x} p(x)\) matches \(_{x} p_{}(x)\) for a typical sample from \(p\). This approach turns out to have many nice properties. It is consistent in the sense that minimizing the objective function yields provably good estimates for the unknown parameters. Moreover, while the definition depends on the unknown \(_{x} p(x)\), by applying integration by parts, it is possible to transform the objective into an equivalent one that can be estimated from samples.

The main question is to bound its statistical performance, especially relative to that of the maximum likelihood estimator. Recent work by Koehler et al. (2022) showed that the cost can be quite steep. They gave explicit examples of distributions that have bad isoperimetric properties (i.e., large Poincare constant) and showed how such properties can cause poor statistical performance.

Despite wide usage, there is little rigorous understanding of when score matching _helps_. This amounts to finding a general setting where maximizing the likelihood with standard first-order optimization is provably hard, and yet score matching is both computationally and statistically efficient, with only a polynomial loss in sample complexity relative to the MLE. In this work, we show the first such guarantees, and we do so for a natural class of exponential families defined by polynomials. As we discuss in Section 1.1, our results parallel recent developments in learning graphical models--where it is known that pseudolikelihood methods allow efficient learning of distributions that are hard to sample from--and can be viewed as a continuous analogue of such results.

In general, an exponential family on \(^{n}\) has the form \(p_{}(x) h(x)(,T(x))\) where \(h(x)\) is the _base measure_, \(\) is the _parameter vector_, and \(T(x)\) is the vector of _sufficient statistics_. Exponential families are one of the most classic parametric families of distributions, dating back to works by Darmois (1935), Koopman (1936) and Pitman (1936). They have a number of natural properties, including: (1) The parameters \(\) are uniquely determined by the expectation of the sufficient statistics \(_{p_{}}[T]\); (2) The distribution \(p_{}\) is the maximum entropy distribution, subject to having given values for \(_{p_{}}[T]\); (3) They have conjugate priors (Brown, 1986), which allow characterizations of the family for the posterior of the parameters given data.

For any (odd positive integer) constant \(d\) and norm bound \(B 1\), we study a natural exponential family \(_{n,d,B}\) on \(^{n}\) where

1. The _sufficient statistics_\(T(x)^{M-1}\) consist of all monomials in \(x_{1},,x_{n}\) of degree at least 1 and at most \(d\) (where \(M=\)).
2. The _base measure_ is defined as \(h(x)=(-_{i=1}^{n}x_{i}^{d+1})\).1 3. The _parameters_\(\) lie in an \(l_{}\)-ball: \(_{B}=\{^{M-1}:\|\|_{ } B\}\).

Towards stating our main results, we formally define the maximum likelihood and score matching objectives, denoting by \(}\) the empirical average over the training samples drawn from some \(p_{n,d,B}\):

\[L_{}}() =}_{x p}[ p_{}(x)]\] \[L_{}}() =}_{x p}[\| p(x)-  p_{}(X)\|^{2}]+K_{p}\] \[=}_{x p}[^{2} p _{}(x)+\| p_{}(x)\|^{2}]\] (1)

where \(K_{p}\) is a constant depending only on \(p\) and (1) follows by integration by parts (Hyvarinen, 2005). In the special case of exponential families, (1) is a quadratic, and in fact the optimum can be written in closed form:

\[*{arg\,min}_{}L_{}}()=-}_{x p}[(JT)_{x}(JT)_{x}^{T}]^{-1}}_{x p} T(x)\] (2)

where \((JT)_{x}:(M-1) n\) is the Jacobian of \(T\) at the point \(x\), \( f=_{i}_{i}^{2}f\) is the Laplacian, applied coordinate wise to the vector-valued function \(f\).

With this setting in place, we show the following intractability result.

**Theorem 1.1** (Informal, computational lower bound).: _Unless \(}}}}}}}}}}} (n,N)\), there is no \((n,N)\)-time algorithm that evaluates \(L_{}}()\) and \( L_{}}()\) given \(_{B}\) and arbitrary samples \(x_{1},,x_{N}^{n}\), for \(d=7,B=(n)\). Thus, optimizing the MLE loss using a zeroth-order or first-order method is computationally intractable._The main idea of the proof is to construct a polynomial \(F_{}(x)\) which has roots exactly at the satisfying assignments of a given 3-SAT formula \(\). We then argue that \((- F_{}(x))\), for sufficiently large \(>0\), concentrates near the satisfying assignments. Finally, we show sampling from this distribution or approximating \( Z_{}\) or \(_{} Z_{}\) (where \(^{M-1}\) is the parameter vector corresponding to the polynomial \(- F_{}(x)\)) would enable efficiently finding a satisfying assignment.

Our next result shows that MLE, though computationally intractable to compute via implementing zeroth or first order oracles, has (asymptotic) sample complexity \((n,B)\) (for constant \(d\)).

**Theorem 1.2** (Informal, efficiency of MLE).: _The MLE estimator has asymptotic sample complexity polynomial in \(n\). That is, for all sufficiently large \(N\) it holds with probability at least \(0.99\) (over \(N\) samples drawn from \(p_{^{*}}\)) that:_

\[\|_{}-^{*}\|^{2} O((d)}}{N}).\]

The main proof technique for this is an anticoncentration bound of low-degree polynomials, for distributions in our exponential family.

Lastly, we prove that score matching _also_ has polynomial (asymptotic) statistical complexity.

**Theorem 1.3** (Informal, efficiency of SM).: _The score matching estimator also has asymptotic sample complexity at most polynomial in \(n\). That is, for all sufficiently large \(N\) it holds with probability at least \(0.99\) (over \(N\) samples drawn from \(p_{^{*}}\)) that:_

\[\|_{}-^{*}\|^{2} O((d)}}{N}).\] (3)

The main ingredient in this result is a bound on the _restricted Poincare constant_--namely, the Poincare constant, when restricted to functions that are linear in the sufficient statistics \(T\). We bound this quantity for the exponential family we consider in terms of the condition number of the Fisher matrix of the distribution, which we believe is a result of independent interest. With this tool in hand, we can use the framework of Koehler et al. (2022), which relates the asymptotic sample complexity of score matching to the asymptotic sample complexity of maximum likelihood, in terms of the restricted Poincare constant of the distribution.

### Discussion and related work

Score matching:Score matching was proposed by Hyvarinen (2005), who also gave conditions under which it is consistent and asymptotically normal. Asymptotic normality is also proven for various kernelized variants of score matching in Barp et al. (2019). Koehler et al. (2022) prove that the statistical sample complexity of score matching is not much worse than the sample complexity of maximum likelihood when the distribution satisfies a (restricted) Poincare inequality. While we leverage machinery from Koehler et al. (2022), their work only bounds the sample complexity of score matching by a quantity polynomial in the ambient dimension for a specific distribution in a specific bimodal exponential family. By contrast, we can handle an entire class of exponential families with low-degree sufficient statistics.

Poincare vs Restricted Poincare:We note that while Poincare inequalities are directly related to isoperimetry and mixing of Markov chains, sample efficiency of score matching only depends on the Poincare inequality holding for a _restricted_ class of functions, namely, functions linear in the sufficient statistics. Hence, hardness of sampling only implies sample complexity lower bounds in cases where the family is expressive enough--indeed, the key to exponential lower bounds for score matching in Koehler et al. (2022) is augmenting the sufficient statistics with a function defined by a bad cut. This gap means that we can hope to have good sample complexity for score matching even in cases where sampling is hard--which we take advantage of in this work.

Learning exponential families:Despite the fact that exponential families are both classical and ubiquitous, both in statistics and machine learning, there is relatively little understanding about the computational-statistical tradeoffs to learn them from data, that is, what sample complexity can be achieved with a computationally efficient algorithm. Ren et al. (2021) consider a version of the "interaction screening" estimator, a close relative of pseudolikelihood, but do not prove anything about the statistical complexity of this estimator. Shah et al. (2021) consider a related estimator, and analyze it under various low-rank and sparsity assumptions of reshapings of the sufficient statistics into a tensor. Unfortunately, these assumptions are somewhat involved, and it's unclear if they are needed for designing computationally and statistically efficient algorithms.

Discrete exponential families (Ising models):Ising models have the form \(p_{J}(x)_{i j}J_{ij}x_{i}x_{j}+_{i}J_{i}x_{i}\) where \(\) denotes adjacency in some (unknown) graph, and \(J_{ij},J_{i}\) denote the corresponding pairwise and singleton potentials. Bresler (2015) gave an efficient algorithm for learning any Ising model over a graph with constant degree (and \(l_{}\)-bounds on the coefficients); see also the more recent work (Dagan et al., 2021). In contrast, it is a classic result (Arora and Barak, 2009) that approximating the partition function of members in this family is NP-hard.

Similarly, the exponential family we consider is such that it contains members for which sampling and approximating their partition function is intractable (the main ingredient in the proof of Theorem 1.1). Nevertheless, by Theorem 3, we can learn the parameters for members in this family computationally efficiently, and with sample complexity comparable to the optimal one (achieved by maximum likelihood). This also parallels other developments in Ising models (Bresler et al., 2014; Montanari, 2015), where it is known that restricting the type of learning algorithm (e.g., requiring it to work with sufficient statistics only) can make a tractable problem become intractable.

The parallels can be drawn even on an algorithmic level: a follow up work to Bresler (2015) by Vuffray et al. (2016) showed that similar results can be shown in the Ising model setting by using the "screening estimator", a close relative of the classical pseudolikelihood estimator (Besag, 1977) which tries to learn a distribution by matching the conditional probability of singletons, and thereby avoids having to evaluate a partition function. Since conditional probabilities of singletons capture changes in a single coordinate, they can be viewed as a kind of "discrete gradient"--a further analogy to score matching in the continuous setting.2

## 2 Preliminaries

We consider the following exponential family. Fix positive integers \(n,d,B\) where \(d\) is odd. Let \(h(x)=(-_{i=1}^{n}x_{i}^{d+1})\), and let \(T(x)^{M-1}\) be the vector of monomials in \(x_{1},,x_{n}\) of degree at least \(1\) and at most \(d\) (so that \(M=\)). Define \(^{M-1}\) by \(=\{^{M-1}:\|\|_{} B\}\). For any \(\) define \(p_{}:^{n}[0,)\) by

\[p_{}(x):=}\]

where \(Z_{}=_{^{n}}h(x)(,T(x))\,dx\) is the normalizing constant. Then we consider the family \(_{n,d,B}:=(p_{})_{_{B}}\). Throughout, we will assume that \(B 1\).

Polynomial notation:Let \([x_{1},,x_{n}]_{ d}\) denote the space of polynomials in \(x_{1},,x_{n}\) of degree at most \(d\). We can write any such polynomial \(f\) as \(f(x)=_{|| d}a_{}x_{}\) where \(\) denotes a degree function \(:[n]\), and \(||=_{i=1}^{n}(i)\), and we write \(x_{}\) to denote \(_{i=1}^{n}x_{i}^{(i)}\). Note that every \(\) with \(1|| d\) corresponds to an index of \(T\), i.e. \(T(x)_{}=x_{}\).

Let \(\|\|_{}\) denote the \(^{2}\) norm of a polynomial in the monomial basis; that is, \(\|_{}a_{}x_{}\|_{}= (_{}a_{}^{2})^{1/2}.\) For any function \(f:^{n}\), let \(\|f\|_{L^{2}([-1,1]^{n})}^{2}=_{x([-1,1]^ {n})}f(x)^{2}\).

Statistical efficiency of MLE:For any \(^{M-1}\), the Fisher information matrix of \(p_{}\) with respect to the sufficient statistics \(T(x)\) is defined as

\[():=_{x p_{}}[T(x)T(x)^{}]-_{x p_{}}[T(x)]_{x p_{}}[T(x)]^{}.\]

It is well-known that for any exponential family with no affine dependencies among the sufficient statistics (see e.g., Theorem 4.6 in Van der Vaart (2000)), it holds that for any \(^{*}^{M-1}\), given \(N\)independent samples \(x^{(1)},,x^{(N)} p_{^{*}}\), the estimator \(_{}=_{}(x^{(1)}, ,x^{(N)})\) satisfies

\[(_{}-^{*}) (0,(^{*})^{-1}).\]

Statistical efficiency of score matching:Our analysis of the statistical efficiency of score matching is based on a result due to Koehler et al. (2022). We state a requisite definition followed by the result.

**Definition 2.1** (Restricted Poincare for exponential families).: The restricted Poincare constant of \(p_{n,d,B}\) is the smallest \(C_{P}>0\) such that for all \(w^{M-1}\), it holds that

\[_{p}( w,T(x)) C_{P}_{x p} \|_{x} w,T(x)\|_{2}^{2}.\]

**Theorem 2.2** (Koehler et al. (2022)).: _Under certain regularity conditions (see Lemma B.4), for any \(p_{^{*}}\) with restricted Poincare constant \(C_{P}\) and with \(_{}((^{*}))>0\), given \(N\) independent samples \(x^{(1)},,x^{(N)} p_{^{*}}\), the estimator \(_{}=_{}(x^{(1)}, ,x^{(N)})\) satisfies_

\[(_{}-^{*})(0,)\]

_where \(\) satisfies_

\[\|\|_{}^{2}(\| ^{*}\|_{2}^{2}_{x p_{^{*}}}\|(JT)(x) \|_{}^{4}+_{x p_{^{*}}}\|  T(x)\|_{2}^{2})}{_{}((^{*}))^{ 2}}\]

_where \((JT)(x)_{i}=_{x}T_{i}(x)\) and \( T(x)=_{x}^{2}T(x)\)._

## 3 Hardness of Implementing Optimization Oracles for \(_{n,7,(n)}\)

In this section we prove \(\)-hardness of implementing approximate zeroth-order and first-order optimization oracles for maximum likelihood in the exponential family \(_{n,7,Cn^{2}(n)}\) (for a sufficiently large constant \(C\)) as defined in Section 2; we also show that approximate sampling from this family is \(\)-hard. See Theorems 3.4, 3.5, and A.5 respectively. All of the hardness results proceed by reduction from 3-SAT and use the same construction.

The idea is that for any formula \(\) on \(n\) variables, we can construct a non-negative polynomial \(F_{}\) of degree at most \(6\) in variables \(x_{1},,x_{n}\), which has roots exactly at the points of the hypercube \(:=\{-1,1\}^{n}^{n}\) that correspond to satisfying assignments (under the bijection that \(x_{i}=1\) corresponds to True and \(x_{i}=-1\) corresponds to False). Intuitively, the distribution with density proportional to \((- F_{}(x))\) will, for sufficiently large \(>0\), concentrate on the satisfying assignments. It is then straightforward to see that sampling from this distribution or efficiently computing either \( Z_{}\) or \(_{} Z_{}\) (where \(^{M-1}\) is the parameter vector corresponding to the polynomial \(- F_{}(x)\)) would enable efficiently finding a satisfying assignment.

The remainder of this section makes the above intuition precise; important details include (1) incorporating the base measure \(h(x)=(-_{i=1}^{n}x_{i}^{8})\) into the density function, and (2) showing that a polynomially-large temperature \(\) suffices.

**Definition 3.1** (Clause/formula polynomials).: Given a \(3\)-clause formula of the form \(C=_{i}_{j}_{k}\) where \(_{i}=x_{i}\) or \(_{i}= x_{i}\), we construct a polynomial \(H_{C}[x_{1},,x_{n}]_{ 6}\) defined by

\[H_{C}(x)=f_{i}(x_{i})^{2}f_{j}(x_{j})^{2}f_{k}(x_{k})^{2}\]

where

\[f_{i}(t)=(t+1)&x_{i}C\\ (t-1)&.\]

For example, if \(C=x_{1} x_{2} x_{3}\), then \(H_{C}=(x_{1}-1)^{2}(x_{2}-1)^{2}(x_{3}+1)^{2}\). Further, given a \(3\)-SAT formula \(=C_{1} C_{m}\) on \(m\) clauses3, we define the polynomial

\[H_{}(x)=H_{C_{1}}(x)++H_{C_{m}}(x).\]

It can be seen that any \(x\) corresponds to a satisfying assignment for \(\) if and only if \(H_{}(x)=0\). Note that there are possibly points outside \(\) which satisfy \(H_{}(x)=0\). To avoid these solutions, we introduce another polynomial:

**Definition 3.2** (Hypercube polynomial).: We define \(G:^{n}\) by \(G(x)=_{i=1}^{n}(1-x_{i}^{2})^{2}\).

Note that \(G(x) 0\) for all \(x\), and the roots of \(G(x)\) are precisely the vertices of \(\). Therefore for any \(,>0\), the roots (in \(^{n}\)) of the polynomial \(F_{}(x)= H_{}(x)+ G(x)\) are precisely the vertices of \(\) that correspond to satisfying assignments for \(\).

**Definition 3.3**.: Let \(\) be a 3-CNF formula with \(n\) variables and \(m\) clauses. Let \(,>0\). Then we define a distribution \(P_{,,}\) with density function

\[p_{,,}(x):=}(x)-  G(x))}{Z_{,,}}\]

where \(Z_{,,}=_{^{n}}h(x)(- H_{ }(x)- G(x))\,dx\).

This distribution lies in the exponential family \(_{n,d,B}\), for \(d=7\) and \(B=(+m)\) (Lemma A.2). Thus, if \((,,)\) is the parameter vector that induces \(P_{,,}\), then it suffices to show that (a) approximating \( Z_{(,,)}\), (b) approximating \(_{} Z_{(,,)}\), and (c) sampling from \(P_{,,}\) are NP-hard (under randomized reductions). We sketch the proofs below; details are in Appendix A.

Hardness of approximating \( Z_{,,}\):In order to prove (a), we bound the mass of \(P_{,,}\) in each orthant of \(^{n}\). In particular, we show that for \(=(n)\) and \(=(m m)\), any orthant corresponding to a satisfying assignment has exponentially larger contribution to \(Z_{,,}\) than any orthant corresponding to an unsatisfying assignment. A consequence is that the partition function \(Z_{,,}\) is exponentially larger when the formula \(\) is satisfiable than when it isn't (Lemma A.6).

But then approximating \(Z_{,,}\) allows distinguishing a satisfiable formula from an unsatisfiable formula, which is NP-hard. This implies the following theorem (proof in Section A.2):

**Theorem 3.4**.: _Fix \(n\) and let \(B Cn^{2}\) for a sufficiently large constant \(C\). Unless \(=\), there is no \((n)\)-time algorithm which takes as input an arbitrary \(_{B}\) and outputs an approximation of \( Z_{}\) with additive error less than \(n 1.16\)._

Hardness of approximating \(_{} Z_{(,,)}\):Note that \(_{} Z_{}=_{x p_{}}[T(x)]\), so in particular approximating the gradient yields an approximation to the mean \(_{x p_{}}[x]\). Since \(P_{,,}\) is concentrated in orthants corresponding to satisfying assignments of \(\), we would intuitively expect that if \(\) has exactly one satisfying assignment \(v^{*}\), then \((_{p_{}}[x])\) corresponds to this assignment. Formally, we show that if \(=(n)\) and \(=(mn m)\), then \(_{x p_{,,}}[v_{i}^{*}x_{i}] 1/20\) for all \(i[n]\) (Lemma A.7).

Since solving a formula with a unique satisfying assignment is still NP-hard, we get the following theorem (proof in Section A.3):

**Theorem 3.5**.: _Fix \(n\) and let \(B Cn^{2}(n)\) for a sufficiently large constant \(C\). Unless \(=\), there is no \((n)\)-time algorithm which takes as input an arbitrary \(_{B}\) and outputs an approximation of \(_{} Z_{}\) with additive error (in an \(l_{}\) sense) less than \(1/20\)._

With the above two theorems in hand, we are ready to present the formal version of Theorem 1.1; the proof is immediate from the definition of \(L_{}()\) (see Section A.5).

**Corollary 3.6**.: _Fix \(n,N\) and let \(B Cn^{2} n\) for a sufficiently large constant \(C\). Unless \(=\), there is no \((n,N)\)-time algorithm which takes as input an arbitrary \(_{B}\), and an arbitrary sample \(x_{1},,x_{N}^{n}\), and outputs an approximation of \(L_{}()\) up to additive error of \(n 1.16\), or \(_{}L_{}()\) up to an additive error of \(1/20\)._

Hardness of approximate sampling:We show that for \(=(n)\) and \(=(m m)\), the likelihood that \(x P_{,,}\) lies in an orthant corresponding to a satisfying assignment for \(\) is at least \(1/2\) (Lemma A.4). Hardness of approximate sampling follows immediately (Theorem A.5). Hence, although we show that score matching can efficiently estimate \(^{*}\) from samples produced by nature, knowing \(^{*}\) isn't enough to efficiently _generate_ samples from the distribution.

## 4 Statistical Efficiency of Maximum Likelihood

In this section we prove Theorem 1.2 by showing that for any \(_{B}\), we can lower bound the smallest eigenvalue of the Fisher information matrix \(()\). Concretely, we show:

**Theorem 4.1**.: _For any \(_{B}\), it holds that_

\[_{}(())(nB)^{-O(d^{3})}.\]

_As a corollary, given \(N\) samples from \(p_{}\), it holds as \(N\) that \((_{}-) N(0,_{})\) where \(\|_{}\|_{}(nB)^{O(d^{3})}\). Moreover, for sufficiently large \(N\), with probability at least \(0.99\) it holds that \(\|_{}-\|_{2}^{2}(nB)^{O(d^{ 3})}/N\)._

Once we have the bound on \(_{}(())\), the first corollary follows from standard bounds for MLE (Section 2), and the second corollary follows from Markov's inequality (see e.g., Remark 4 in Koehler et al. (2022)). Lower-bounding \(_{}(())\) itself requires lower-bounding the variance of any polynomial (with respect to \(p_{}\)) in terms of its coefficients. The proof consists of three parts. First, we show that the norm of a polynomial in the monomial basis is upper-bounded in terms of its \(L^{2}\) norm on \([-1,1]^{n}\):

**Lemma 4.2**.: _For \(f[x_{1},,x_{n}]_{ d}\), we have \(\|f\|_{}^{2}(16e)^{d}\|f \|_{L^{2}([-1,1]^{n})}^{2}.\)_

The key idea behind this proof is to work with the basis of (tensorized) Legendre polynomials, which is orthonormal with respect to the \(L^{2}\) norm. Once we write the polynomial with respect to this basis, the \(L^{2}\) norm equals the Euclidean norm of the coefficients. Given this observation, all that remains is to bound the coefficients after the change-of-basis. The complete proof is deferred to Appendix C.

Next, we show that if a polynomial \(f:^{n}\) has small variance with respect to \(p\), then there is some box on which \(f\) has small variance with respect to the uniform distribution. This provides a way of comparing the variance of \(f\) with its \(L^{2}\) norm (after an appropriate rescaling).

**Lemma 4.3**.: _Fix any \(_{B}\) and define \(p:=p_{}\). Define \(R:=2^{d+8}nBM\). Then for any \(f[x_{1},,x_{n}]_{ d}\), there is some \(z^{n}\) with \(\|z\|_{} R\) and some \( 1/(2(d+1)MR^{d}(n+B))\) such that_

\[_{p}(f)_{}}(f),\]

_where \(}\) is the uniform distribution on \(\{x^{n}:\|x-z\|_{}\}\)._

In order to prove this result, we pick a random box of radius \(\) (within a large bounding box of radius \(R\)). In expectation, the variance on this box (with respect to \(p\)) is not much less than \(_{p}(f)\). Moreover, for sufficiently small \(\), the density function of \(p\) on this box has bounded fluctuations, allowing comparison of \(_{p}(f)\) and \(_{}}(f)\). This argument is formalized in Appendix C.

Together, Lemma 4.2 and 4.3 allow us to lower bound the variance \(_{p}(f)\) in terms of \(\|f\|_{}\).

**Lemma 4.4**.: _Fix any \(_{B}\) and define \(p:=p_{}\). Define \(R:=2^{d+8}nBM\). Then for any \(f[x_{1},,x_{n}]_{ d}\) with \(f(0)=0\), it holds that_

\[_{p}(f)(d+1)^{2d}(16e)^{d+1}M^{2d+3}R^{2d ^{2}+2d}(n+B)^{2d}}\|f\|_{}^{2}.\]

See Appendix C for the proof.We are now ready to finish the proof of Theorem 4.1.

Proof of Theorem 4.1.: Fix \(_{B}\). Pick any \(w^{M}\) and define \(f(x)= w,T(x)\). By definition of \(()\), we have \(_{p_{}}(f)=w^{}()w\). Moreover, \(\|f\|_{}^{2}=\|w\|_{2}^{2}\). Thus, Lemma 4.4 gives us that \(w^{}()w(nB)^{-O(d^{3})}\|w\|_{2}^{2}\), using that \(R=2^{d+8}nBM\) and \(M=\). The bound \(_{}(())(nB)^{-O(d^{3})}\) follows. 

## 5 Statistical Efficiency of Score Matching

In this section we prove Theorem 1.3. The main technical ingredient is a bound on the restricted Poincare constants of distributions in \(_{n,d,B}\). For any fixed \(_{B}\), we showthat \(C_{P}\) can be bounded in terms of the _condition number_ of the Fisher information matrix \(()\). We describe the building blocks of the proof below.

Fix \(,w^{M-1}\) and define \(f(x):= w,T(x)\). First, we need to upper bound \(_{p_{}}(f)\). This is where (the first half of) the condition number appears. Using the crucial fact that the restricted Poincare constant only considers functions \(f\) that are linear in the sufficient statistics, and the definition of \(()\), we get the following bound on \(_{p_{}}(f)\) in terms of the coefficient vector \(w\). The proof is deferred to Section D.

**Lemma 5.1**.: _Fix \(,w^{M-1}\) and define \(f(x):= w,T(x)\). Then_

\[ w_{2}^{2}_{}(()) _{p_{}}(f) w_{2}^{2} _{}(()).\]

Next, we lower bound \(_{x p_{}}_{x}f(x)_{2}^{2}\). To do so, we could pick an orthonormal basis and bound \( u,_{x}f(x)^{2}\) over all directions \(u\) in the basis; however, it is unclear how to choose this basis. Instead, we pick \(u(0,I_{n})\) randomly, and use the following identity:

\[_{x p_{}}[_{x}f(x)_{2}^{2}]= _{x p_{}}_{u N(0,I_{n})} u,_{x }f(x)^{2}\]

For any fixed \(u\), the function \(g(x)= u,_{x}f(x)\) is also a polynomial. If this polynomial had no constant coefficient, we could immediately lower bound \( u,_{x}f(x)^{2}\) in terms of the remaining coefficients, as above. Of course, it may have a nonzero constant coefficient, but with some case-work over the value of the constant, we can still prove the following bound:

**Lemma 5.2**.: _Fix \(,^{M-1}\) and \(c\), and define \(g(x):=,T(x)+c\). Then_

\[_{x p_{}}[g(x)^{2}]+ _{2}^{2}}{4+4[T(x)]_{2}^{2}} (1,_{}(())).\]

Proof.: We have

\[_{x p_{}}[g(x)^{2}] =_{p_{}}(g)+_{x p_{} }[g(x)]^{2}\] \[=_{p_{}}(g-c)+(c+^{} _{x p_{}}[T(x)])^{2}\] \[_{2}^{2}_{} (())+(c+^{}_{x p_{}}[T(x)])^ {2}\]

where the inequality is by Lemma 5.1. We now distinguish two cases.

Case I.Suppose that \( c+^{}_{x p_{}}[T(x)]  c/2\). Then

\[_{x p_{}}[g(x)^{2}]_{ 2}^{2}_{}(())+}{4}+_{2}^{2}}{4}(1,_{}( ())).\]

Case II.Otherwise, we have \( c+^{}_{x p_{}}[T(x)]< c/2\). By the triangle inequality, it follows that \(^{}_{x p_{}}[T(x)]  c/2\), so \(_{2} c/2_{x p_{ }}[T(x)]_{2}\)). Therefore

\[c^{2}+_{2}^{2}(1+4_{ x p_{}}[T(x)]_{2}^{2})_{2}^{2},\]

from which we get that

\[_{x p_{}}[g(x)^{2}]_{ 2}^{2}_{}(())+ _{2}^{2}}{1+4_{x p_{}}[T(x )]_{2}^{2}}_{}(())\]

as claimed. 

With Lemma 5.1 and Lemma 5.2 in hand (taking \(g(x)= u,_{x}f(x)\) in the latter), all that remains is to relate the squared monomial norm of \( u,_{x}f(x)\) (in expectation over \(u\)) to the squared monomial norm of \(f\). This crucially uses the choice \(u N(0,I_{n})\). We put together the pieces in the following lemma. The detailed proof is provided in Section D.

**Lemma 5.3**.: _Fix \(,w^{M-1}\). Define \(f(x):= w,T(x)\). Then_

\[_{p_{}}(f)(4+4_{x p_{ }}[T(x)]_{2}^{2})}(( ))}{(1,_{}(()))}_{x p _{}}[_{x}f(x)_{2}^{2}].\]Finally, putting together Lemma 5.3, Theorem 4.1 (that lower bounds \(_{}(())\)), and Lemma B.2 (that upper bounds \(_{}(())\) - a straightforward consequence of the distributions in \(_{n,d,B}\) having bounded moments), we can prove the following formal version of Theorem 1.3:

**Theorem 5.4**.: _Fix \(n,d,B,N\). Pick any \(^{*}_{B}\) and let \(x^{(1)},,x^{(N)} p_{^{*}}\) be independent samples. Then as \(N\), the score matching estimator \(_{}=_{}(x^{(1)},,x^{(N)})\) satisfies_

\[(_{}-^{*}) N(0,)\]

_where \(\|\|_{}(nB)^{O(d^{3})}\). As a corollary, for all sufficiently large \(N\) it holds with probability at least \(0.99\) that \(\|_{}-^{*}\|_{2}^{2}(nB)^{O(d^{3})}/N\)._

Proof.: We apply Theorem 2.2. By Lemma B.4 and the fact that \(_{}(I(^{*}))>0\) (Theorem 4.1), the necessary regularity conditions are satisfied so that the score matching estimator is consistent and asymptotically normal, with asymptotic covariance \(\) satisfying

\[\|\|_{}^{2}(\|\|_ {2}^{2}_{x p_{^{*}}}\|(JT)(x)\|_{}^{4} +_{x p_{^{*}}}\| T(x)\|_{2}^{2})}{ _{}((^{*}))^{2}}\] (4)

where \(C_{P}\) is the restricted Poincare constant for \(p_{^{*}}\) with respect to linear functions in \(T(x)\) (see Definition 2.1). By Lemma 5.3, we have

\[C_{P} (4+4\|_{x p_{}}[T(x)]\|_{2}^{ 2})}((^{*}))}{(1,_{ {min}}((^{*}))}\] \[(4+4B^{2d}M^{2d+2}2^{2d(d+6)+1})M^{2d+1}2^{2d(d+6 )+1}}{(nB)^{-O(d^{3})}}(nB)^{O(d^{3})}\]

using parts (a) and (b) of Lemma B.2; Theorem 4.1; and the fact that \(M=\). Substituting into (4) and bounding the remaining terms using Lemma B.3 and a second application of Theorem 4.1, we conclude that \(\|\|_{}(nB)^{O(d^{3})}\) as claimed. The high-probability bound now follows from Markov's inequality; see Remark 4 in Koehler et al. (2022) for details. 

## 6 Conclusion

We have provided a concrete example of an exponential family--namely, exponentials of bounded degree polynomials--where score matching is significantly more computationally efficient than maximum likelihood estimation (through optimization with a zero- or first-order oracle), while still achieving the same sample efficiency up to polynomial factors. While score matching was designed to be more computationally efficient for exponential families, the determination of statistical complexity is more challenging, and we give the first separation between these two methods for a general class of functions.

As we have restricted our attention to the asymptotic behavior of both of the methods, an interesting future direction is to see how the finite sample complexities differ. One could also give a more fine-grained comparison between the polynomial dependencies of score matching and MLE, which we have not attempted to optimize. Finally, it would be interesting to relate our results with similar results and algorithms for learning Ising and higher-order spin glass models in the discrete setting, and give a more unified treatment of psueudo-likelihood or score/ratio matching algorithms in these different settings.