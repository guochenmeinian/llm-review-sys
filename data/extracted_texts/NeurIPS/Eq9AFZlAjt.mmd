# Unbounded Differentially Private Quantile and Maximum Estimation

David Durfee

Anonym Inc.

david@anonymco.com

###### Abstract

In this work we consider the problem of differentially private computation of quantiles for the data, especially the highest quantiles such as maximum, but with an unbounded range for the dataset. We show that this can be done efficiently through a simple invocation of AboveThreshold, a subroutine that is iteratively called in the fundamental Sparse Vector Technique, even when there is no upper bound on the data. In particular, we show that this procedure can give more accurate and robust estimates on the highest quantiles with applications towards clipping that is essential for differentially private sum and mean estimation. In addition, we show how two invocations can handle the fully unbounded data setting. Within our study, we show that an improved analysis of AboveThreshold can improve the privacy guarantees for the widely used Sparse Vector Technique that is of independent interest. We give a more general characterization of privacy loss for AboveThreshold which we immediately apply to our method for improved privacy guarantees. Our algorithm only requires one \(O(n)\) pass through the data, which can be unsorted, and each subsequent query takes \(O(1)\) time. We empirically compare our unbounded algorithm with the state-of-the-art algorithms in the bounded setting. For inner quantiles, we find that our method often performs better on non-synthetic datasets. For the maximal quantiles, which we apply to differentially private sum computation, we find that our method performs significantly better.

## 1 Introduction

In statistics, quantiles are values that divide the data into specific proportions, such as median that divides the data in half. Quantiles are a central statistical method for better understanding a dataset. However, releasing quantile values could leak information about specific individuals within a sensitive dataset. As a result, it becomes necessary to ensure that individual privacy is ensured within this computation. Differential privacy offers a rigorous method for measuring the amount that one individual can change the output of a computation. Due it's rigorous guarantees, differential privacy has become the gold standard for measuring privacy. This measurement method then offers an inherent tradeoff between accuracy and privacy with outputs of pure noise achieving perfect privacy. Thus, the goal of designing algorithms for differentially private quantile computation is to maximize accuracy for a given level of privacy.

There are a variety of previous methods for computing a given quantile of the dataset that we will cover in Section 1.2, but each of these requires known bounds on the dataset. The most effective and practical method invokes the exponential mechanism Smith (2011). For computing multiple quantiles this method can be called iteratively. Follow-up work showed that it could be called recursively by splitting the dataset at each call to reduce the privacy cost of composition Kaplan et al. (2022). Further, a generalization can be called efficiently in one shot Gillenwater et al. (2021).

### Our contributions

In this work, we offer an alternative practical and accurate approach, Unbounded Quantile Estimation (UQE), that also invokes a well-known technique and can additionally be applied to the unbounded setting. While the commonly-used technique designs a distribution to draw from that is specific to the dataset, our method will simply perform a noisy guess-and-check. Initially we assume there is only a lower bound on the data, as non-negative data is common in real world datasets with sensitive individual information. Our method will simply iteratively increase the candidate value by a small percentage and halt when the number of data points below the value exceeds the desired amount dictated by the given quantile. While the relative increase will be small each iteration, the exponential nature still implies that the candidate value will become massive within a reasonable number of iterations. As a consequence, our algorithm can handle the unbounded setting where we also show that two calls to this procedure can handle fully unbounded data. Computing multiple quantiles can be achieved by applying the recursive splitting framework from Kaplan et al. (2022).

Performing our guess-and-check procedure with differential privacy exactly fits AboveThreshold, a method that is iteratively called in the Sparse Vector Technique Dwork et al. (2009). We also take a deeper look at AboveThreshold and unsurprisingly show that similar to report noisy max algorithms, the noise addition can come from the Laplace, Gumbel or Exponential distributions. We further push this analysis to show that for monotonic queries, a common class of queries which we will also utilize in our methods, the privacy bounds for composition within the Sparse Vector Technique can be further improved. Given the widespread usage of this technique,1 we believe this result is of independent interest. Furthermore, we give a more general characterization of query properties that can improve the privacy bounds of AboveThreshold. We immediately utilize this characterization in our unbounded quantile estimation algorithm to improve privacy guarantees.

While the commonly used algorithms for quantile estimation can still apply incredibly loose bounds to ensure the data is contained within, this can have a substantial impact upon the accuracy for estimating the highest quantiles such as maximum. This leads to an especially important application for our algorithm, differentially private sum computation, which can thereby be used to compute mean as well. Performing this computation practically without assumptions upon the distribution often requires clipping the data and adding noise proportionally. Clipping too high adds too much noise, and clipping too low changes the sum of the data too much. The highest quantiles of the data are used for clipping to optimize this tradeoff. The unbounded nature of our approach fundamentally allows us to estimate the highest quantiles more robustly and improve the accuracy of differentially private sum computation.

This improvement in differentially private sum computation is further evidenced by our empirical evaluation, with significant improvements in accuracy. Our empericial comparison will be upon the same datasets from previous work in the bounded setting. We also compare private computation of the inner quantiles on these datasets. For synthetic datasets generated from uniform or guassian distributions, we see that the more structured approach of designing a distribution for the data from the exponential mechanism consistently performs better. However, for the real-world datasets, we see that our unstructured approach tends to perform better even within this bounded setting. By design our algorithm is less specific to the data, so our alternative approach becomes advantageous when less is known about the structure and bounds of the data _a priori_. As such, for large-scale privacy systems that provide statistical analysis for a wide variety of datasets, our methods will be more flexible to handle greater generality accurately.

### Background literature

The primary algorithm for privately computing a given quantile, by which we compare our technique, applies the exponential mechanism with a utility function based upon closeness to the true quantile Smith (2011). We will discuss this algorithm, which we denote as Exponential Quantile (EMQ), in greater detail in Appendix A. This approach was then extended to computing multiple quantiles more cleverly by recursively splitting the data and establishing that only one partition ofthe dataset can change between neighbors thereby reducing the composition costs Kaplan et al. (2022). Additional follow-up work showed that a generalization of the utility function to multiple quantiles could be efficiently drawn upon in one shot Gillenwater et al. (2021). Another recent result examined this problem in the streaming data setting and gave a method that only uses strongly sub-linear space complexity Alabi et al. (2022).

Quantile computation can also be achieved through CDF estimation Bun et al. (2015); Kaplan et al. (2020). However these techniques offer limited practicality as they rely upon several reductions and parameter tuning. Recursively splitting the data is also done for CDF estimation algorithms where the statistics from each split can be aggregated for quantile computation Dwork et al. (2010); Chan et al. (2011). These techniques tend to be overkill for quantile estimation and thus suffer in accuracy comparatively.

We will also give improved privacy analysis of the Sparse Vector Technique which was originally introduced in Dwork et al. (2009). A more detailed analysis of the method can be found in Lyu et al. (2017). Additional recent work has shown that more information can be output from the method at no additional privacy cost Kaplan et al. (2021); Ding et al. (2023).

### Organization

We provide the requisite notation and definitions in Section 2. In Section 3, we review the AboveThreshold algorithm from the literature and show that privacy analysis can be further improved. In Section 4, we provide our unbounded quantile estimation method. In Section 5, we test our method compared to the previous techniques on synthetic and real world datasets. In Appendix A, we consider the estimation of the highest quantiles which has immediate application to differentially private sum and mean estimation. In Appendix B, we give further results on the AboveThreshold algorithm and provide the missing proofs from Section 3. In Appendix C, we provide further variants and extensions of our unbounded quantile estimation technique.

## 2 Preliminaries

We will let \(x,x^{}\) denote datasets in our data universe \(\).

**Definition 2.1**.: _Datasets \(x,x^{}\) are neighboring if at most one individual's data has been changed._

Note that we use the _swap_ definition, but our analysis of the AboveThresholdalgorithm will be agnostic to the definition of neighboring. Using this definition as opposed to the _add-subtract_ definition is necessary to apply the same experimental setup as in Gillenwater et al. (2021). Our differentially private quantile estimation will apply to either and we will give the privacy guarantees if we instead use the _add-subtract_ definition in Appendix C.2.

**Definition 2.2**.: _A function \(f\,:\,\) has sensitivity \(\) if for any neighboring datasets \(|f(x)-f(x^{})|\)_

**Definition 2.3**.: _Dwork et al. (2006, 2006) A mechanism \(M\,:\,\) is \((,)\)-differentially-private (DP) if for any neighboring datasets \(x,x^{}\) and \(S\):_

\[[M(x) S] e^{}[M(x^{}) S]+.\]

We will primarily work with pure differential privacy in this work where \(=0\). We will also be considering the composition properties of the Sparse Vector Technique, and the primary method for comparison will be Concentrated Differential Privacy that has become widely used in practice due to it's tighter and simpler advanced composition properties Bun and Steinke (2016). This definition is instead based upon Reny divergence where for probability distributions \(P,Q\) over the same domain and \(>1\)

\[D_{}(P\|Q)=\,*{}_{z-P}[ ()^{-1}]\]

**Definition 2.4**.: _Bun and Steinke (2016) A mechanism \(M\,:\,\) is \(\)-zero-concentrated-differentially-private (zCDP) if for any neighboring datasets \(x,x^{}\) and all \((1,)\):_

\[D_{}(M(x)\|M(x^{})).\]We can translate DP into zCDP in the following way.

**Proposition 1**.: _Bun and Steinke (2016) If \(M\) satisfies \(\)-DP then \(M\) satisfies \(^{2}\)-zCDP_

In our examination of AboveThreshold we will add different types of noise, similar to the report noisy max algorithms Ding et al. (2021). Accordingly, we will consider noise from the Laplace, Gumbel and Exponential distributions where \((b)\) has PDF \(p_{}(z;b)\), \((b)\) has PDF \(p_{}(z;b)\), and \((b)\) has PDF \(p_{}(z;b)\) where

\[p_{}(z;b) =(-|z|/b) p_{}(z;b) =(-(z/b+e^{-z/b}))\] \[p_{}(z;b) =(-z/b)&z 0\\ 0&z<0\]

We let \((b)\) denote noise addition from any of \((b)\), \((b)\), or \((b)\). We will also utilize the definition of the exponential mechanism to analyze the addition of \(\) noise.

**Definition 2.5**.: _McSherry and Talwar (2007)_ _The Exponential Mechanism is a randomized mapping \(M:\) such that_

\[Pr[M(x)=y]()\]

_where \(q:\) has sensitivity \(\)._

## 3 Improved Analysis for Sparse Vector Technique

In this section, we review the AboveThreshold algorithm from the literature. To our knowledge, this technique has only been used with Laplace noise in the literature. Unsurprisingly, we show that Gumbel and Exponential noise can also be applied, with the former allowing for a closed form expression of each output probability. We further show that for monotonic queries the privacy analysis of the Sparse Vector Technique, which iteratively applies AboveThreshold, can be improved. All proofs are be pushed to Appendix B where we also give a more general characterization of query properties that can improve the privacy bounds of AboveThreshold.

### Above Threshold Algorithm

We first provide the algorithm for AboveThreshold where noise can be applied from any of the Laplace, Gumbel or Exponential distributions.

```
0: Input dataset \(x\), a stream of queries \(\{f_{i}:\}\) with sensitivity \(\), and a threshold \(T\)
1: Set \(=T+(/_{1})\)
2:for each query \(i\)do
3: Set \(v_{i}=(/_{2})\)
4:if\(f_{i}(x)+v_{i}\)then
5: Output \(\) and halt
6:else
7: Output \(\)
8:endif
9:endfor ```

**Algorithm 1**AboveThreshold

We will also define a common class of queries within the literature that is often seen to achieve a factor of 2 improvement in privacy bounds.

**Definition 3.1**.: _We say that stream of queries \(\{f_{i}:\}\) with sensitivity \(\) is monotonic if for any neighboring \(x,x^{}\) we have either \(f_{i}(x) f_{i}(x^{})\) for all \(i\) or \(f_{i}(x) f_{i}(x^{})\) for all \(i\)._To our knowledge all previous derivations of AboveThreshold in the literature apply Lap noise which gives the following privacy guarantees.

**Lemma 3.1** (Theorem 2 and 3 of Lyu et al. (2017)).: _If the noise addition is Lap then Algorithm 1 is \((_{1}+2_{2})\)-DP for general queries and is \((_{1}+_{2})\)-DP for monotonic queries._

Given that Expo noise is one-sided Lap noise, it can often be applied for comparative algorithms such as this one and report noisy max as well. We will show this extension in the appendix for completeness.

**Corollary 3.1**.: _If the noise addition is_ Expo _then Algorithm 1 is \((_{1}+2_{2})\)-DP for general queries and \((_{1}+_{2})\)-DP for monotonic queries._

While the proofs for Expo noise generally follow from the Lap noise proofs, it will require different techniques to show that Gumbel noise can be applied as well. In particular, we utilize the known connection between adding Gumbel noise and the exponential mechanism.

**Lemma 3.2**.: _If the noise addition is_ Gumbel _and \(_{1}=_{2}\) then Algorithm 1 is \((_{1}+2_{2})\)-DP for general queries and \((_{1}+_{2})\)-DP for monotonic queries._

We defer the proof of this to the appendix. In all of our empirical evaluations we will use Expo noise in our calls to AboveThreshold because it has the lowest variance for the same parameter. While we strongly believe that Expo noise will be most accurate under the same noise parameters, we leave a more rigorous examination to future work. We also note that this examination was implicitly done for report noisy max between Gumbel and Expo noise in McKenna and Sheldon (2020), where their algorithm is equivalent to adding Expo noise Ding et al. (2021), and Expo noise was shown to be clearly superior.

### Improved privacy analysis for Sparse Vector Technique

In this section, we further consider the iterative application of AboveThreshold which is known as the sparse vector technique. We show that for monotonic queries, we can improve the privacy analysis of sparse vector technique to obtain better utility for the same level of privacy. Our primary metric for measuring privacy through composition will be zCDP which we defined in Section 2 and has become commonly used particularly due to the composition properties. We further show in the appendix that our analysis also enjoys improvement under the standard definition of differential privacy. These improved properties immediately apply to our unbounded quantile estimation algorithm as our queries will be monotonic.

**Theorem 1**.: _If the queries are monotonic, then for any noise addition of Lap, Gumbel, or Expo we have that Algorithm 1 is \(^{2}\)-zCDP where \(=}{2}+_{2}\). If the noise addition is_ Gumbel _then we further require \(_{1}=_{2}\)_

Note that applying Proposition 1 will instead give \(=_{1}+_{2}\). It will require further techniques to reduce this by \(_{1}/2\) which will immediately allow for better utility with the same privacy guarantees. This bound also follows the intuitive factor of 2 improvement that is often expected for monotonic queries. The analysis will be achieved through providing a range-bounded property, a definition that was introduced in Durfee and Rogers (2019). This definition is ideally suited to characterizing the privacy loss of selection algorithms, by which we can view AboveThreshold. As such it will also enjoy the improved composition bounds upon the standard differential privacy definition shown in Dong et al. (2020). This range bounded property was then unified with zCDP with improved privacy guarantees in Cesar and Rogers (2021).

We give a proof of this theorem along with further discussion in Appendix B.3. We will also give a generalized characterization of when we can take advantage of properties of the queries to tighten the privacy bounds in AboveThreshold. We will immediately utilize this characterization to improve the privacy guarantees for our method in Appendix C.

## 4 Unbounded Quantile Estimation

In this section we give our method for unbounded quantile estimation. We focus upon the lower bounded setting which we view as most applicable to real-world problems, where non-negativedata is incredibly common, particularly for datasets that contain information about individuals. This method can be symmetrically apply to upper bounded data, and in the appendix we will show how this approach can be extended to the fully unbounded setting.

For the quantile problem we will assume our data \(x^{n}\). Given quantile \(q\) and dataset \(x^{n}\) the goal is to find \(t\) such that \(|\{x_{j} x|x_{j}<t\}|\) is as close to \(qn\) as possible.

### Unbounded quantile mechanim

The idea behind unbounded quantile estimation will be a simple guess-and-check method that will just invoke AboveThreshold. In particular, we will guess candidate values \(t\) such that \(|\{x_{j} x|x_{j}<t\}|\) is close to \(qn\). We begin with the smallest candidate, recall that we assume lower bounded data here and generalize later, and iteratively increase by a small percentage. At each iteration we check if \(|\{x_{j} x|x_{j}<t\}|\) has exceeded \(qn\), and terminate when it does, outputting the most recent candidate. In order to achieve this procedure privately, we will simply invoke AboveThreshold. We also discuss how this procedure can be achieved efficiently in Section 4.2.

Thus we give our algorithm here where the lower bound of the data, \(\), is our starting candidate and \(\) is the scale at which the threshold increases. Given that we want our candidate value to increase by a small percentage it must start as a positive value. As such we will essentially just shift the data such that the lower bound is instead 1. In all our experiments we set \(=1.001\), so the increase is by \(0.1\%\) each iteration.

```
0: Input dataset \(x\), a quantile \(q\), a lower bound \(\), and parameter \(>1\)
1: Run AboveThreshold with \(x\), \(T=qn\) and \(f_{i}(x)=|\{x_{j} x|x_{j}-+1<^{i}\}|\)
2: Output \(^{k}+-1\) where \(k\) is the query that AboveThreshold halted at ```

**Algorithm 2** Unbounded quantile mechanism

Given that our method simply calls AboveThreshold it will enjoy all the privacy guarantees from Section 3.1. Furthermore we will show that our queries are monotonic.

**Lemma 4.1**.: _For any sequence of thresholds \(\{t_{i}\}\) let \(f_{i}(x)=|\{x_{j} x|x_{j}<t_{i}\}|\) for all \(i\). For any neighboring dataset under Definition 2.1, we have that \(\{f_{i}\}\) are monotonic queries with sensitivity 1._

Proof.: Let \(x_{j}\) be the value that differs between neighbors \(x,x^{}\). Define \(S_{x,t}=\{x_{j} x|x_{j}<t\}\). We consider the case \(x^{}_{j}>x_{j}\) and the other will follow symmetrically. For all thresholds \(t_{i}(-,x_{j}]\) we have \(x_{j} S_{x,t_{i}}\) and \(x^{}_{j} S_{x^{},t_{i}}\), so \(f_{i}(x)=f_{i}(x^{})\). For all thresholds \(t_{i}(x^{}_{j},)\) we have \(x_{j} S_{x,t_{i}}\) and \(x^{}_{j} S_{x^{},t_{i}}\), so \(f_{i}(x)=f_{i}(x^{})\). Finally, for all thresholds \(t_{i}(x_{j},x^{}_{j}]\) we have \(x_{j} S_{x,t_{i}}\) and \(x^{}_{j} S_{x^{},t_{i}}\), so \(f_{i}(x)=f_{i}(x^{})+1\). Therefore, \(f_{i}(x) f_{i}(x^{})\) for all \(i\), and the sensitivity is 1.

Note that for the _swap_ definition of neighboring the threshold remains constant. We will discuss how to extend our algorithm and further improve the privacy bounds for the _add-subtract_ definition of neighboring in the appendix.

### Simple and scalable implementation

In this section we show how our call to AboveThreshold can be done with a simple linear time pass through the data, and each subsequent query takes \(O(1)\) time. While the running time could potentially be infinite, if we set \(=1.001\), then after 50,000 iterations our threshold is already over \(10^{21}\) and thus highly likely to have halted. Unless the scale of the data is absurdly high or the \(\) value chosen converges to 1, our guess-and-check process will finish reasonably quickly. 2

In our initial pass through the data, for each data point \(x_{j}\) we will find the index \(i\) such that \(^{i} x_{j}-+1<^{i+1}\), which can be done by simply computing \(\{_{}(x_{j}-+1)\}\) as our lower bound ensures \(x_{j}-+1 1\). Using a dictionary or similar data structure we can efficiently store \(|\{x_{j} x|^{i} x_{j}-+1<^{i+1}\}|\) for each \(i\) with the default being 0. This preprocessing does not require sorted data and takes \(O(n)\) arithmetic time, where we note that the previous algorithms also measure runtime arithmetically.

Finally, for each query if we already have \(|\{x_{j} x|x_{j}-+1<^{i}\}|\), then we can add \(|\{x_{j} x|^{i} x_{j}-+1<^{i+1}\}|\) in O(1) time to get \(|\{x_{j} x|x_{j}-+1<^{i+1}\}|\). Inductively, each query will take O(1) time. We provide the code in the appendix for easier reproducibility.

### Extension to multiple quantiles

The framework for computing multiple quantiles set up in Kaplan et al. (2022) is agnostic to the technique used for computing a single quantile. Their method will first compute the middle quantile and split the data according to the result. Through recursive application the number of levels of computation will be logarithmic. Furthermore, at each level we can see that at most one partition of the data will differ between neighbors, allowing for instead a logarithmic number of compositions. As such our approach can easily be applied to this recursive splitting framework to achieve the same improvements in composition. This will require some minor updating of their proofs to the _swap_ definition that we will do in the appendix.

## 5 Empirical Evaluation

In this section we empirically evaluate our approach compared to the previous approaches. We give further detail of the previous approaches, particularly EMQ, in A along with strong intuition upon why our approach will better handle maximal quantile estimation for data clipping. We first go over the datasets and settings used for our experiments which will follow recent related work Gillenwater et al. (2021); Kaplan et al. (2022). Next we evaluate how accurately our method estimates quantiles for the different datasets in the bounded setting. Finally, we will consider the application of computing differentially private sum, which also gives mean computation, and show how our algorithm allows for a significantly more robust and accurate method when tight bounds are not known for the dataset.

### Datasets

We borrow the same setup and datasets as Gillenwater et al. (2021); Kaplan et al. (2022). We test our algorithm compared to the state-of-the-art on six different datasets. Two datasets will be synthetic. One draws 10,000 data points from the uniform distribution in the range \([-5,5]\) and the other draws 10,000 data points from the normal distribution with mean zero and standard deviation of five. Two datasets will come from Soumik (2019) with 11,123 data points, where one has book ratings and the other has book page counts. Two datasets will come from Dua and Graf (2019) with 48,842 data points, where one has the number of hours worked per week and the other has the age for different people. We provide histograms of our datasets for better understanding in Figure 1.

### Quantile estimation experiments

For our quantile estimation experiments, for a given quantile \(q\) we consider the error of outcome \(o_{q}\) from one of the private methods to be \(|o_{q}-t_{q}|\) where \(t_{q}\) is the true quantile value. We use the in-built quantile function in the numpy library with the default settings to get the true quantile value. As in previous related works, we randomly sample 1000 datapoints from each dataset and run the quantile computation on each method. This process is then iterated upon 100 times and the error is averaged. We set \(=1\) as in the previous works, which will require setting \(_{1}=_{2}=1/2\) for the call to AboveThreshold in our method.

We will also tighten the ranges to the following, \([-5,5]\) for the uniform dataset, \([-25,25]\) for the normal dataset, \(\) for the ratings dataset, \(\) for the pages dataset, \(\) for the hours dataset, and \(\) for the ages dataset. Given that EMQ suffers performance when many datapoints are equal we add small independent noise to our non-synthetic datasets. This noise will be from the normal distribution with standard deviation \(0.001\) for the ratings dataset and \(0.1\) for the other three that have integer values. Our method does not require the noise addition but we will use the perturbed dataset for fair comparison. True quantiles are still computed upon the original data. For the datasets with integer values we rounded each output to the nearest integer. For our method we set \(=1.001\) for all datasets.

For these experiments we only compare our method UQE and the previous method EMQ, using the implementations from Gillenwater et al. (2021). The other procedures, discussed in the appendix are more generalized and thus for this specific setting do not perform nearly as well which can be seen in the previous experiments Gillenwater et al. (2021); Kaplan et al. (2022), so we omit them from our results. For this experiment we consider estimating each quantile from 5% to 95% at a 1% interval. In Figure 2 we plot the mean absolute error of each normalized by the mean absolute error of UQE to make for an easier visualization.

As we can see in Figure 2, EMQ consistently performs better on synthetic data, and UQE tends to performs better on the non-synthetic data. This fits with our intuition that UQE will be best suited to situations where the data is unstructured and less is known about the dataset beforehand because our guess-and-check methodology is designed to better handle ill-behaving datasets.

Figure 1: Histograms for each of our datasets.

Figure 2: Plots of UQE = (mean absolute error UQE) / (mean absolute error UQE) and EMQ = (mean absolute error EMQ) / (mean absolute error UQE). Normalizing in this way will make for an easier visualization. When EMQ is below UQE then it’s error is lower, and when EMQ is above UQE then it’s error is higher

### Sum estimation experiments

As the primary application of our method we will also be considering differentially private sum computation, which can thereby compute mean as well. We will be using the following \(2\)-DP general procedure for computing the sum of non-negative data:

1. Let \(=(x,q,)\) where \(\) is any differentially private computation algorithm and \(q 1\).
2. Output \((/)+_{j=1}^{n}(x_{j},)\)

We further test this upon the non-synthetic datasets. For large-scale privacy systems that provide statistical analysis for a wide variety of datasets, if we use a \(\) that requires an upper bound then we ideally want this bound to be agnostic to the dataset. The is particularly true for sum computations upon groupby queries as the range and size can differ substantially amongst groups. As such, we fix the range at \(\) to encompass all the datasets. We will otherwise use the same general setup as in Section 5.2.

We will measure the error of this procedure as the absolute error of the output and the true sum of the dataset. For each of the 100 iterations of choosing 1000 samples randomly from the full dataset, we also add \(\) noise 100 times. Averaging over all these iterations gives our mean absolute error.

We will run this procedure with \(\{0.1,0.5,1\}\). Further we will use \(q=0.99\) always for our method, but give the absolute error for the best performing \(q\{0.95,0.96,0.97,0.98,0.99\}\) for the other methods. It is important to note that this value would have to be chosen ahead of time, which would add more error to the other methods. The previous methods we consider here are again the EMQ, but also the aggregate tree (AT) methods, were we use both the implementation along with generally best performing height (3) and branching factor (10) from Gilfenwater et al. (2021). We also implemented the bounding technique using inner quartile range within Algorithm 1 of Smith (2011), but this performed notably worse than the others so we omitted the results from our table.

As we can see in Table 1, our method is far more robust and accurate. Furthermore, for our method the choice of \(q\) remained constant and we can see that our results still stayed consistently accurate when \(\) changed. Note that the noise added to the clipped sum is also scaled proportional to \(\) so the amount the error increased as \(\) decreased for our method is what would be expected proportionally. Once again these findings are consistent with our intuition. Our technique is more robust to differing datasets and privacy parameters, and especially better performing for this important use case.

Recall that the sampled data had size 1000 so dividing accordingly can give the error on mean estimates. There is a long line of literature on differentially private mean estimation.3 To our knowledge, all of these more complex algorithms either require assumptions upon the data distribution,

  Privacy & Method & Ratings data & Pages data & Ages data & Work hours data \\   & UQE & \(4.78_{0.21}\) & \(4385.23_{2077.16}\) & \(103.051_{60.04}\) & \(180.48_{44.92}\) \\  & EMQ & \(5.73_{0.54}\) & \(4324.38_{2343.25}\) & \(187.063_{35.55}\) & \(339.06_{75.82}\) \\  & AT & \(8.75_{0.31}\) & \(4377.45_{2340.93}\) & \(293.11_{117.32}\) & \(471.98_{174.07}\) \\    & UQE & \(9.22_{0.31}\) & \(7102.34_{3093.13}\) & \(180.61_{27.03}\) & \(277.89_{77.60}\) \\  & EMQ & \(6906.13_{7123.36}\) & \(7601.47_{3963.52}\) & \(678.22_{1931.11}\) & \(2131.80_{4669.11}\) \\  & AT & \(29.01_{115.04}\) & \(7491.97_{4367.31}\) & \(473.19_{238.19}\) & \(582.58_{369.29}\) \\    & UQE & \(44.59_{1.79}\) & \(21916.37_{6423.75}\) & \(821.77_{157.68}\) & \(981.10_{219.66}\) \\  & EMQ & \(45861.79_{28366.46}\) & \(46552.09_{27944.18}\) & \(50558.32_{28843.72}\) & \(47185.58_{29437.22}\) \\   & AT & \(11351.77_{21423.40}\) & \(30830.49_{20422.05}\) & \(14490.06_{25268.71}\) & \(9928.18_{20159.17}\) \\   

Table 1: Mean absolute error for differentially private sum estimation. The standard deviation over the 100 iterations is also provided for each in the subscript. UQE = Our unbounded quantile estimation method. EMQ = The exponential mechanism based quantile estimation method. AT = The aggregate tree method for quantile estimation. For our method we only use \(q=0.99\). For the others we use the best performance for \(q\{0.95,0.96,0.97,0.98,0.99\}\).

such as sub-Gaussian or bounded moments, or bounds upon the data range or related parameters, and most often require both. These results also focus upon proving strong theoretical guarantees of accuracy with respect to asymptotic sample complexity. We first note that our approach will provide better initial bounds upon the data as seen in our experiments, which directly improve the theoretical guarantees in the results that require a data range. But also our focus here is upon practical methods that are agnostic to data distributions and more widely applicable to real-world data. Consequently, a rigorous comparison among all of these methods would be untenable and outside the scope of this work.

### Parameter tuning

We kept our \(\) parameter fixed in all experiments for consistency but also to make our method data agnostic. However, our choice was aggressively small in order to achieve higher precision in the inner quantile estimation comparison in Section 5.2. This choice was still highly resilient to changes in \(\) for our sum experiments as we see our error only scaled proportional to the increase in noise. But for more significant decreases in \(\) or in the data size, i.e. the conditions under which all private algorithms suffer substantial accuracy loss, this choice of \(\) could be too small. Those settings imply that the noise added is larger and the distance between queries and threshold shrinks, so our method is more likely to terminate earlier than desired. Smaller \(\) values will then intensify this issue as the candidate values increase more slowly. For the clipping application, we generally think using a value of \(=1.01\) would be a more stable choice. In fact, replicating our empirical testing with this value actually improves our results in Table 1. Furthermore, increasing \(\) will also reduce the number of queries and thus the computational cost. In general, we find that setting \([1.01,1.001]\) gives good performance with \(=1.01\) as a default for computational efficiency. We leave to future work a more thorough analysis of this parameter to fully optimize setting it with relation to data size and privacy parameter. Additionally, all our methods and proofs only require an increasing sequence of candidate values and it's possible that other potential sequences would be even more effective. For example, if tight upper and lower bounds are known on the data, such as within the recursive computation of multiple quantiles, then it likely makes more sense to simply uniformly partition the interval and check in increasing order. But we leave more consideration upon this to future work as well.

Our choice of \(q=0.99\) in the differentially private sum experiments was also to maintain consistency with the choices for the previous method but also to keep variance of the estimates lower. This will create some negative bias as we then expect to clip the data. As we can see in our illustrative example Figure 3, the PDF will exponentially decrease once we pass the true quantile value, but will do so less sharply once all the queries have value \(n\). Accordingly, setting \(q=1.0\) would add slightly more variance to the estimation but initial testing showed improvement in error. However, if the user would prefer slightly higher variance to avoid negative bias, then setting the threshold at \(n\) or even \(n+1/\), would make it far more likely that the process terminates with a value slightly above the maximum. This is particularly useful for heavy-tailed data, where clipping at the 99th percentile can have an out-sized impact on the bias.