# BackTime: Backdoor Attacks on

Multivariate Time Series Forecasting

 Xiao Lin

University of Illinois

Urbana-Champaign, IL, USA

xiaol13@illinois.edu

&Zhining Liu

University of Illinois

Urbana-Champaign, IL, USA

liu326@illinois.edu

&Dongqi Fu

Meta AI

CA, USA

dongqifu@meta.com

&Ruizhong Qiu

University of Illinois

Urbana-Champaign, IL, USA

rq5@illinois.edu

&Hanghang Tong

University of Illinois

Urbana-Champaign, IL, USA

htong@illinois.edu

###### Abstract

Multivariate Time Series (MTS) forecasting is a fundamental task with numerous real-world applications, such as transportation, climate, and epidemiology. While a myriad of powerful deep learning models have been developed for this task, few works have explored the robustness of MTS forecasting models to malicious attacks, which is crucial for their trustworthy employment in high-stake scenarios. To address this gap, we dive deep into the backdoor attacks on MTS forecasting models and propose an effective attack method named BackTime. By subtly injecting a few _stealthy triggers_ into the MTS data, BackTime can alter the predictions of the forecasting model according to the attacker's intent. Specifically, BackTime first identifies vulnerable timestamps in the data for poisoning, and then adaptively synthesizes stealthy and effective triggers by solving a bi-level optimization problem with a GNN-based trigger generator. Extensive experiments across multiple datasets and state-of-the-art MTS forecasting models demonstrate the effectiveness, versatility, and stealthiness of BackTime attacks. The code is available at https://github.com/xiaolin-cs/BackTime.

## 1 Introduction

Time series forecasting finds its applications across diverse domains such as climate , epidemiology , transportation , and financial markets . Multivariate time series (MTS) represent a collection of time series with multiple variables, and MTS forecasting aims to predict future data for each variable based on their historical data and the complex inter-variable relationship among them. Due to its wide applications and complexity, it has become an important research area. The rapid advancement of deep learning  has significantly contributed to solving time series forecasting challenges. Many deep learning models have been developed to tackle this problem, including Transformer-based , GNN-based  and RNN-based  models.

Despite the remarkable capacity of deep learning models, there is an alarming concern that they are susceptible to backdoor attacks . The attack involves the surreptitious injection of triggers into datasets, causing poisoned models to provide wrong predictions when the inputs contain malicious triggers. Extensive works have shown that backdoor attack poses a serious risk across various classification tasks, including time series classification . However, the threat to time series forecasting remains unexplored and is of great importance to be investigated. Forexample, data-driven traffic forecasting systems are used in multiple countries to control traffic light timing, e.g., Google's Project Green Light . If the input signals to these forecasting systems are manipulated by hackers to provide malicious predictions, it could lead to widespread traffic congestion and thus brings negative economic and societal impacts. Similar situations, such as attacks to stock prediction [8; 61; 78] and climate forecasting [59; 60; 62; 30], would significantly weaken the reliability of forecasting models and do great harm.

To address this critical and imminent issue, we extend the application landscape of backdoor attack from MTS classification to forecasting. Unlike traditional backdoor attacks that focus on specific class labels, our approach aims to induce poisoned models to predict future data as a predefined target pattern. This new problem prompts several questions that deserve exploration in this paper: First, **stealthy attack**, i.e., to what extent can such a manipulation on datasets be imperceptible by minimizing the amplitude of triggers and maintaining a low injection rate [46; 22]? Second, **sparse attack**, i.e., how can data manipulation be confined to a small subset of variables within MTS ?

In this paper, we present a novel generative framework for generating stealthy and sparse attacks on MTS forecasting. To begin with, we first describe a threat model that introduces attackers' abilities and goals, paving the way for formally defining the problem of MTS forecasting attacks. Then, to realize the conceptual attackers, we formalize the trigger generation within a bi-level optimization process and design an end-to-end generative framework called BackTime, which adaptively constructs a graph that measures inter-variable correlations and iteratively solves the bi-level optimization by employing a GNN-based trigger generator. The intuition behind this is that triggers effective for one variable are likely to be successful in attacking similar variables. During the optimization, generated triggers can be **sparsely** added to only a subset of variables, thereby only altering the model's prediction behavior for these target variables. Moreover, to ensure the **stealthiness** of the attack, we introduce a non-linear scaling function into the trigger generator to limit the amplitude of triggers and also leverage a shape-aware normalization loss to ensure that the frequency of the generated triggers closely match those of the normal time series data.

In summary, our main contributions are as follows:

* **Problem.** To the best of our knowledge, we are the first to extend the concept of backdoor attacks to MTS forecasting. We identify two crucial properties of backdoor attacks on MTS forecasting: stealthiness and sparsity; and further devise a novel threat model on this basis.
* **Methodology.** We propose a bi-level optimization framework for backdoor attacks on MTS forecasting, aiming to generate effective triggers under stealthy constraints. Based on this framework, we leverage a GNN-based trigger generator to design triggers based on the inter-variable correlations.
* **Evaluation.** We conduct extensive experiments on five widely used MTS datasets, demonstrating that BackTime achieves state-of-the-art (SOTA) backdoor attack performance. Our results show that BackTime can effectively control the attacked model to give predictions according to the attacker's intent when faced with poisoned inputs, while maintaining its high forecasting ability for clean inputs.

## 2 New Backdoor Attack Setting for MTS Forecasting

### Preliminary

**Multivariate time series forecasting.** In multivariate time series, the dataset encompasses time series with multiple variables, denoted as \(=\{_{1},_{2},,_{N}\}^{T N}\) where \(T\) represents the time spans, \(N\) represents the number of variables, and \(_{i}\) is the time series sequence of the \(i\)-th variable. For forecasting tasks, a widely used method for training is to slice time windows from the dataset as the training inputs. Let \(t^{}\) denote the length of time windows. Then for any timestamp \(t_{i}\)1, the input will consist of historical sequences spanning from timestamps \(t_{i}-t^{}\) to \(t_{i}\), expressed as \([t_{i}-t^{}:t_{i}]\)2. The objective of MTS forecasting is to predict future time series denoted \([t_{i}:t_{i}+t^{}]\) where \(t^{}\) represents the prediction timestamp. In the following paper, we use \(_{t_{i},h}\) to represent historical data \([t_{i}-t^{}:t_{i}]\) and \(_{t_{i},f}\) to represent future data \([t_{i}:t_{i}+t^{}]\) for notation convenience. The main notations in this paper are listed in Table 5.

**Backdoor Attacks on Classifications** Traditional backdoor attacks have proven highly effective in classification tasks across diverse data formats. Given a dataset \(=\{,\}\) with \(\) and \(\) representing the set of samples (e.g., images, text, time series) and corresponding labels, respectively, attackers generate some special and commonly invisible patterns, which are called _triggers_. For example, triggers could be specific pixels in images, [28; 64; 10], particular sentences in text [43; 57; 13], and designed perturbations on time series [38; 18]. These triggers are then inserted into a small subset of samples in \(\), with their labels flipped to a predefined _target label_. After training on the poisoned dataset, models will predict the class as the target label if the inputs contain triggers while still performing normally when facing clean inputs, i.e., the inputs without triggers.

### Differences from Attacks on Forecasting w.r.t Tasks and Data Formats

Compared with the traditional backdoor attack [10; 64; 28; 38; 18; 43; 57], the backdoor attack on MTS forecasting bears several important and unique challenges, as shown in Table 1.

Considering **tasks**, traditional backdoor attack is applied for classification while this paper focuses on forecasting, which in turn brings the following four crucial differences. (1) **Target object.** Instead of flipping labels on classification, we concatenate triggers and target patterns into successive sequences and inject them together into the training set, thus building strong temporal correlations between triggers and target patterns. (2) **Real-time attack.** Unlike traditional backdoor attacks which may leverage ground truth data for trigger generation, the attack on forecasting is only allowed to use the historical data due to the timeliness. For example, if a hacker aims to alter the traffic flow data to reach a specific value at time \(t_{i}\), then this specific value should be determined before \(t_{i}\). Otherwise, the data manipulation will be too late and thus useless, since the traffic flow data would have already been sent to the forecasting system in real-time. It indicates that the shape of triggers at the timestamp of \(t_{i}\) should be known ahead of \(t_{i}\). Therefore, the generation of triggers can only utilize data of \(t_{i}-1\) at most. (3) **Constraint on target object.** On MTS forecasting, since both the triggers and the target pattern are injected into the dataset, we need to impose constraints on triggers as well as the target pattern. (4) **Soft identification**. Since perhaps only a part of triggers and target patterns are retained in sliced time windows, a novel soft identification mechanism is needed to determine if a window has been attacked. Detailed explanations of (3) and (4) are provided in Section 3.1.

Considering **data**, MTS data bears the following uniqueness. (1) **Human unreadability.** Analyzing time series data often requires specialized knowledge, like financial expertise for stock prices. This makes it harder for humans to detect modifications in time series data compared to images or texts. Hence, human judgments is not reliable for assessing the stealthiness of backdoor attack on forecasting. As a result, we leverage anomaly detection methods as the stealthiness indicator, since if a trigger is not stealthy, it will differ significantly from the original data, making it detectable as an anomaly. (2) **Inter-variable dependence**. Compared with univariate time series, the attack on MTS data are much more complicated due to the inter-variable correlations. Since advanced forecasting models [69; 82; 9; 31; 81] tend to leverage correlations between variables to enhance their forecasting performances, if a trigger can successfully attack the prediction of one variable, similar triggers might also work for closely correlated variables. Thus, trigger generation must consider both temporal dependencies and inter-variable correlations.

    &  &  \\   & _Target_ & _Real-time_ & _Constraint on_ & _Soft_ & _Human_ & _Inter-variable_ \\  & _Object_ & _attack_ & _Target Object_ & _Identification_ & _Unreadability_ & _Dependance_ \\  Image/Text Classification [28; 64; 43; 57] & Discrete scalar (label) & \(\) & \(\) & \(\) & \(\) & \(\) \\ Univariate Time Series Classification [18; 38] & Discrete scalar (label) & \(\) & \(\) & \(\) & \(\) & \(\) \\ Multivariate Time Series Classification [18; 38] & Discrete scalar (label) & \(\) & \(\) & \(\) & \(\) & \(\) \\ Multivariate Time Series Forecasting (Ours) & Sequence (pattern) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Comparisons of the backdoor attack on MTS forecasting and other backdoor attack tasks.

Figure 1: An illustrative example of data poisoning on the PEMS03 dataset. After triggers and target patterns (red lines) are injected, predictions of the attack model (orange dash line) will resemble the target pattern.

Based on all these differences, we present the detailed treat model of backdoor attack on MTS forecasting as follows.

### Threat Model of Attacks on MTS Forecasting

**Capability of attackers**: Given a training dataset, the attacker can select \(_{}\) timestamps to poison, denoted as \(^{}}\). Then, for each timestamp \(t_{i}^{}}\), the attacker generates an invisible trigger \(g^{t^{}}||}\), where \(t^{}}\) denotes the length of the trigger, and \(\{1,,N\}\) denotes the selected variables to poison. After that, the attacker starts to poison the corresponding time series by injecting the trigger, i.e., \([t_{i}-t^{}} t_{i},] [t_{i}-t^{}}-1,] g\), and also replacing the future data with the target pattern, i.e., \([t_{i}:t_{i}+t^{}},] [t_{i}-t^{}}-1,] p\), where \(\) represents the addition with Python broadcasting mechanism, and \(t^{}}}\) is the length of a predefined target pattern. The data poisoning example is illustrated in Figure 1.

**Goals of attackers**: (1) Attacked forecasting models predict the future as the ground truth when facing clean inputs. (2) Attacked forecasting models predicts the future of the target variables as the given target pattern when the poisoned historical data contains triggers.

### Formal Problem Definition

**Problem 1**: _Backdoor attacks on multivariate time series forecasting._

**Input**: (1) a clean dataset \(^{T N}\) where \(T\) represents the time span and \(N\) represents the number of variables; (2) a predefined target pattern \(p\) with a length of \(t^{}}}\); (3) the length \(t^{}}}\) of triggers to be added, (4) a temporal injection rate \(_{}\), and (5) a set of target variables \(\) satisfying \(|}{N}_{}\) with \(_{}\) being the spatial injection rate.

**Output**: a poisoned dataset \(^{}}\) by poisoning \(_{}\) timestamps such that the performances of attacked models will align with **goals of attackers** if models are trained on a poisoned dataset.

## 3 Backdoor Attacks on MTS Forecasting

In this section, we introduce our comprehensive threat model proposed for backdoor attacks on MTS forecasting. First, we formalize the general objective of our threat model in Section 3.1. Then, we introduce how to instance this objective with our BackTime in Section 3.2.

### General Goal and Formulation

In this section, we propose two unique designs for backdoor attacks on MTS forecasting based on the key differences discussed in Section 2.2.

**Stealthiness constraints on triggers and target patterns.** To uphold stealthiness in backdoor attacks, it is imperative to ensure that the poisoned data closely resembles the ground truth data [38; 46; 67]. However, as Section 2 shows, the insertion of triggers is intended to be applied on the unknown future. This design limitation makes it almost impossible to ensure the similarity between the poisoned data and the unknown future. To alleviate this issue, we consider the similarity between the poisoned data and the recent historical data as a pragmatic alternative, indicating that the amplitude of generated triggers should be controlled under a small budget. In addition, the same constraint is supposed to be utilized on target patterns since target patterns are also integrated into the training data. Mathematically, we use \(L_{}\) norm for stealthiness constraints like [20; 19]. Therefore, the stealthiness constraints could be formally written as:

\[\|g\|_{}^{}},\|p\|_{}^{ }}\] (1)

where \(^{}}\) and \(^{}}\) are the budgets for triggers and target patterns, respectively.

**Soft identification on poisoned samples.** In Multivariate Time Series (MTS) forecasting, a common practice [69; 35; 82; 9] involves slicing datasets into time windows to serve as inputs for forecasting models. However, in a poisoned dataset \(^{}}\), identifying whether these sliced time windows are poisoned poses significant challenges for two primary reasons. First, the length of these time windows may not align with the length of triggers or target patterns. Second, when slicing datasets into time windows, these windows may encompass only a fraction of the triggers or target patterns. To solve these problems, we propose a soft identification mechanism. Specifically, we assume that the injected backdoor is activated only when inputs encompass all components of the triggers. Furthermore, we define the degree of poisoning in inputs based on the proportion of target patterns within the future to be forecasted. The rationale behind is that when the backdoor begins to be activated, its influence should be most pronounced, resulting in a significant impact on the forecasting process. As time goes, the strength of this effect gradually diminishes since the proportion of target patterns within the future decreases. Mathematically, for any timestamp \(t_{i}\), the soft identification mechanism is formalized as follows:

\[(t_{i})=(}^{}}{t^{}}) (c_{t_{i}}^{}=t^{})\] (2)

where \((t_{i})\) represents the soft identification mechanism at the timestamp \(t_{i}\), \(c_{t_{i}}^{}\) and \(c_{t_{i}}^{}\) are the length of triggers within \(_{t_{i},h}^{}\) and target patterns within \(_{t_{i},f}^{}\), respectively. \(\) is a monotonically decreasing function satisfying \((1)=1\) and \((0)=0\), which measures the significance attributed to the degree of poisoning. For example, if \(\) rapidly decreases within the range of \((0,1)\), it implies that once the triggers are activated, the expected effects of triggers will diminish rapidly over time.

To sum up, we refine the basic optimization problem [19; 20] of typical backdoor attack by integrating the above adjustments, hence providing a general mathematical framework for backdoor attack on MTS forecasting:

\[_{g}_{t_{i}}[_{ }(f(_{t_{i},h}^{};^{*}), _{t_{i},f}^{})(t_{i})]\] \[\ \ ^{*}=*{argmin}_{t_{i} }[_{}(f(_{t_{i},h}^{};),_{t_{i},f}^{})],\] (3)

where \(^{}\) represents the poisoned dataset, \(\) represents the set of timestamps in \(^{}\), \(f()\) denotes the forecasting model with its parameters of \(\), \(_{}\) is the clean loss for forecasting tasks, and \(_{}\) is the attack loss designed to make the model's output resemble the target pattern. The key idea here is that, after a model is trained on the poisoned dataset \(^{}\) through the lower-level optimization, we aim to minimize the expectation of difference between the output of this model and the target pattern, as shown in the upper-level optimization. This is based on the fact that in the upper optimization, \(_{t_{i},f}^{}\) contains at least a part of the target pattern \(g\) when \((t_{i}) 0\). Additionally, although constraints are imposed on both the triggers and the target pattern, the constraint on the target pattern does not actively participate in the optimization process. Instead, it serves as a constraint that the attacker is expected to adhere to when determining the shape of the target pattern.

### BackTime Algorithm

To successfully achieve backdoor attack on MTS forecasting, we need to determine three key elements: (RQ1) **where to attack**, i.e., identifying which variable to target; (RQ2) **when to attack**, i.e., selecting which timestamps to attack; and (RQ3) **how to attack**, i.e., specifying the trigger to inject. Regarding (RQ1) **where to attack**, as outlined in Problem 1, the target variables are determined by the attacker and can be any variable desired. Subsequently, we will discuss (RQ2) **when to attack** in Section 3.2.1, and provide the details of (RQ3) **how to attack** in Sections 3.2.2 and 3.2.3.

#### 3.2.1 Selecting Timestamps for Poisoning

In this section, we design an illustrative experiment to investigate the properties of the timestamps that are more susceptible to attack. The main idea of the experiment is, given a simple and weak backdoor attack, to observe the change of attack effect when choosing timestamps with different properties for attack. Based on the experiment results, we find that timestamps w.r.t. high prediction errors for a clean model are more susceptible to attacks.

We investigate the properties of timestamps on the PEMS03 dataset. Specifically, we first train a forecasting model (i.e., _clean model_\(f^{}\)) on the original dataset \(\) and record the Mean Absolute Error (MAE) of the predictions for each timestamp. A higher MAE indicates poorer prediction performance for that timestamp. We then sort the timestamps in ascending order based on their MAE and divide them into ten groups, with average MAE percentiles of \(0.05,0.15,,0.95\), as shown on the x-axis of Figure 2. Then, for each group, we implement a simple backdoor attack, where a shape-fixed trigger and target pattern are injected to all the timestamps and variables within the timestamp group, and train a new model (i.e., _attacked model_\(f^{}\)) on the poisoned data \(^{}\).

The shapes of the trigger and the target pattern are shown in Appendix D. Intuitively, a timestamp \(t_{i}\) that is susceptible to backdoor attack will have a low poisoned MAE, i.e., \((f^{},_{t_{i},h}^{},_{t_ {i},f}^{})\). It means that at timestamp \(t_{i}\), the predictions of the attacked model can be greatly altered by the attack to fit the target pattern. However, relying solely on poisoned MAE is insufficient because if the target pattern closely resembles the ground truth, the poisoned MAE will still be low even if the attack fails. To address this problem, we test a clean model on the poisoned dataset and further record its clean MAE for each poisoned timestamp \(t_{i}\), i.e., \((f^{},_{t_{i},h}^{},_ {t_{i},f}^{})\). Then, a lower MAE difference between poisoned MAE and clean MAE can reliably indicate more vulnerable timestamps, since the clean MAE will be quite low, leading to a high MAE difference, when the target pattern is similar to the ground truth. The experiment results, as shown in Figure 2, demonstrate that the group with higher MAE percentile can continuously lead to a lower MAE difference. These findings imply that timestamps where a clean model performs poorly are more susceptible to backdoor attacks. Therefore, to ensure the strength of backdoor attack, for each timestamp \(t_{i}\), we leverage a pretrained clean model to calculate MAE between predictions and the ground truth \(_{t_{i},f}\), and further select the top \(_{}\) timestamps with the highest MAE, denoted as \(^{}\).

#### 3.2.2 Trigger Generation

Once the poisoned timestamps are determined, the next step is to generate adaptive triggers to poison the dataset. First, we generate a weighted graph by leveraging an MLP to capture the inter-variable correlation within the target variables \(\). Then, we further utilize a Graph Convolutional Network (GCN)  for trigger generation based on the learned weighted graph.

**Graph structure generation.** Since we aim to activate backdoor in any timestamps, we do not expect that the generated graph is closely related to specific local temporal properties in the training set. Thus, we focus on building a static graph by learning the global temporal features within the target variables \(\). Motivated by this goal, we take as the entire input time series data \(_{i},i\) instead of using sliced time windows. However, the time span \(T\) of time series data is often very large, and hence it is inefficient to directly use total data without preprocessing. Therefore, we apply the discrete Fourier transform (DFT)  to effectively reduce the dimension while maintaining useful information. Intuitively, long-time-scale features, such as trends and periodicity, play a pivotal role in the global temporal correlation among variables, compared with the local noise or high-frequency fluctuations. Consequently, after DFT, we retain only the low-frequency features of the time series data. Mathematically, for any target variable \(i\), this transform could be expressed as \(_{i}=((_{i}),k)\) where \(()\) represents the DFT transformation, and \((,k)\) represents preserving the top \(k\) low-frequency features. Furthermore, we employ Multilayer Perceptron (MLP) to adaptively learn features of different frequencies. Subsequently, we utilize the output of the MLP to construct a graph that measures the correlation between target variables. The aforementioned process can be expressed as:

\[_{i,j}=cos((_{i}),(_{ j})),\ i,j\] (4)

where \(_{i,j}\) represents the element of learned graph \(\) at the \(i\)-th row and the \(j\)-th column, and \(cos(,)\) represents the cosine similarity.

**Adaptive trigger generation.** Once a correlation graph has been obtained, our objective shifts to the generation of learnable triggers that can be seamlessly integrated into various models with efficacy and imperceptibility. To ensure semantic consistency between triggers and historical data, we employ a time window with a length of \(t^{}\) to slice the historical data preceding the trigger. Then, we utilize a GCN for trigger generation based on the sliced historical data:

\[_{t_{i}}=(^{}[t_{i}-t^{ }-t^{}:t_{i}-t^{},],),\ \,t_{i}^{}\] (5)

In experiments, we find the following phenomenon: the GCN intends to aggressively increase the amplitude of output \(_{t_{i}}\). Even if an extra penalty on the amplitude is introduced, it still requires

Figure 2: The difference of MAE between a clean model and an attacked model when using different timestamps for attack. A lower MAE difference (y-axis) indicates more susceptible timestamps to attack.

much effort to adjust the hyperparameters to control the trigger amplitude. One potential explanation for this behavior is that a large trigger amplitude leads to substantial deviation, and data points characterized by such deviations are more readily learned by forecasting models although they violate the requirements of stealthiness. To address this issue, we propose to introduce a non-linear scaling function, \(tanh()\), to generate stealthy triggers by imposing mandatory limitations on the amplitude of outputs \(_{t_{i}}\). Mathematically, the generated triggers can be formalized as follows:

\[g_{t_{i}}=^{} tanh(_{t_{i}}),\  t_{i} ^{}\] (6)

#### 3.2.3 Bi-level Optimization

After introducing the model architecture of the adaptive trigger generator \(f_{g}\) in Eqs. (5) and (6), we aim to optimize the trigger generator through a bi-level optimization problem in Eq. (3) to ensure the effectiveness of the generated triggers. Recognizing the inherent complexity of bi-level optimization, we introduce a surrogate forecasting model \(f_{s}\) to provide a practical approximation of the precise solution. This allows us to solve Eq. (3) by iteratively updating the surrogate model and the trigger generator. However, we further find that if we randomly initialize the surrogate model \(f_{s}\), then the performance of the trigger generator tends to fluctuates in the initial stage, posing a significant difficulty in convergence. Therefore, we introduce an additional warm-up phase. During the warm-up phase, we only train the surrogate model to make it have a reasonable forecasting ability. Once the warm-up phase is over, we will update both the surrogate model and trigger generator. Specifically, in this phase, we will divide the training process for each epoch into two stages: (1) the surrogate model update, and (2) the trigger generator update.

At the first stage, we poison the clean dataset, as mentioned in Section 2.3. Then we aim to improve the forecasting ability of the surrogate model \(f_{s}\) on the poisoned dataset \(^{}\). Specifically, we employ a natural forecasting loss function, denoted as \(_{}\), to update the surrogate model \(f_{s}\) while fixing the parameters of the trigger generator \(f_{g}\):

\[l_{cln}=_{}(f_{s}(_{t_{i},h}^{ }),_{t_{i},f}^{}),\  t_{i} \] (7)

In this paper, we use smooth \(L_{1}\) loss  as the forecasting loss \(_{}\).

As for the second stage, we aim to update the trigger generator \(f_{g}\) for effective and unnoticeable triggers. Following Section 2.3, for each poisoned timestamp \(t_{i}^{}\), we will utilize the trigger generator \(f_{g}\) to obtain the trigger \(g_{t_{i}}\) based on Eqs (5) and (6), and then re-inject those triggers to obtain the poisoned dataset \(^{}\). The main difference of trigger injection between this stage and the first stage is that the gradient \(^{}}{ g_{t_{i}}}\) here would be preserved. Then, we aim to implement the attack loss in Eq. (3) to ensure the effectiveness of triggers. Specifically, after fixing the parameter of the surrogate model \(f_{s}\), the attack loss could be formalized as:

\[l_{atk}=_{t_{i}=t}^{t+t^{}}_{}(f_{ s}(_{t_{i},h}^{}),_{t_{i},f}^{ })(t_{i}),\  t^{}\] (8)

In the paper, we set \((x)=x\) for simplicity, and set \(_{}\) as the MSE loss.

Furthermore, we introduce a normalization loss to regulate the shape of triggers, thereby enhancing their stealthiness. The main intuition is that high-frequency fluctuations or noises widely exist in MTS data of real-world datasets , but the bi-level optimization in Eq. (3) does not inherently guarantee that triggers will have high-frequency signals. Therefore, to bridge this gap, the following normalization loss is introduced:

\[l_{norm}=(|[_{i=0}^{t^{}}}g_{t_{i}} [i,]]),\; t_{i}^{}}.\] (9)

where \(()\) represents the average operation. The key idea is that triggers will exhibit alternating positive and negative components, i.e., fluctuations, if the summation of triggers along the temporal dimension approaches zero. To sum up, the loss function for the trigger generator in the second stage can be expressed as:

\[l_{tgr}=l_{}+\;l_{norm},\; t^{}}\] (10)

where \(\) is a hyperparameter. All the above training procedures are summarized in Algorithm 1.

## 4 Experiments

**Datasets.** We conduct experiments on five real-world datasets, including PEMS03 , PEMS04 , PEMS08 , weather  and ETTm1 . The detailed information of these datasets are provided in Appendix B. For each dataset, we use the same 60%/20%/20% splits for train/validation/test sets.

**Experiment protocol.** For the basic setting of backdoor attacks, we adopt \(t^{}}=4\) and \(t^{}}=7\), with \(_{}\) of 0.03 and \(_{}\) of 0.3. More details of attack settings are provided in Appendix C.2. Following prior studies [44; 21; 5], we use the past 12 time steps to predict subsequent 12 time steps. We compare BackTime with four different training strategies (_Clean_, _Random_, _Inverse_, and _Manhattan_) and three SOTA forecasting models [82; 69; 9] under all possible combinations to fully validate BackTime's effectiveness and versatility. More details of these forecasting models are provided in Appendix C.1. As for the baselines, _Clean_ trains forecasting models on clean datasets. _Random_ randomly generates triggers from a uniform distribution. _Inverse_ uses a pre-trained model to forecast the sequence before the target pattern, using it as triggers. _Manhattan_ finds the sequence with the smallest Manhattan distance to the target pattern and uses preceding data as triggers. Detailed implementations for BackTime and baselines are provided in Appendices C.2 and C.3, respectively.

**Metrics.** To evaluate the natural forecasting ability, we use Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) between the model's output and the ground truth when the input is clean, denoted as \(_{}\) and \(_{}\), respectively. To evaluate attack effectiveness, we use MAE and RMSE between the model's output and the target pattern when the input contains triggers, denoted as \(_{}\) and \(_{}\), respectively. For all these metrics, the lower, the better.

    &  &  &  &  &  &  \\  & & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) \\   & TimesNet & 20.00 & 28.63 & 20.92 & 29.30 & 20.03 & 26.62 & 19.89 & 26.33 & 21.23 & **20.83** \\  & FEDformer & 15.78 & 39.86 & 16.14 & 15.70 & 16.18 & 16.05 & 16.42 & 17.10 & 16.34 & **14.05** \\  & Autoformer & 16.03 & 38.38 & 17.09 & 20.98 & 17.23 & 20.55 & 16.75 & 22.13 & 17.12 & **17.68** \\   & Average & 17.27 & 35.62 & 18.05 & 21.99 & 17.81 & 21.07 & 17.69 & 21.85 & 18.23 & **17.52** \\  PEMS04 & Average & 24.34 & 46.82 & 21.50 & 30.01 & 22.61 & **26.17** & 22.69 & 30.95 & 22.60 & **26.17** \\  PEMS08 & Average & 19.30 & 40.66 & 19.81 & 34.69 & 20.09 & 30.39 & 20.37 & 24.47 & 19.67 & **21.48** \\  Weather & Average & 12.75 & 94.43 & 14.53 & 23.76 & 13.67 & 65.56 & 15.54 & 73.88 & 8.43 & **15.49** \\  ETTm1 & Average & 1.25 & 2.58 & 1.28 & 1.59 & 1.32 & 15.33 & 1.28 & 1.82 & 1.14 & **1.41** \\   

Table 2: Main results of backdoor attack on MTS forecasting. For all the metrics, the lower the better. Bold font indicates the best performance for the attack effectiveness. Due to space limitation, we report the key performance results averaged over three MTS forecasting models and omit some minor detailed values. Please refer to Appendix E for full results.

    &  &  &  \\  & & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) \\  Clean &

**Effectiveness evaluation.** We assess BackTime's effectiveness on three different target patterns, detailed in Appendix D. Table 2 shows the main results for natural forecasting ability (MAE\({}_{}\)) and attack effectiveness (MAE\({}_{}\)) with a cone-shaped target pattern. Note that only for the _Clean_ row, MAE\({}_{}\) and RMSE\({}_{}\) are calculated with clean inputs. Similar results for different target patterns, where we poison PEMS03 with FEDformer  (the surrogate model) and test on TimesNet , are in Table 3. Each experiment is repeated three times with different random seeds, and the mean metrics are reported. Regarding the attack effectiveness, BackTime achieves lowest average MAE\({}_{}\) among all the datasets and baselines. It also continuously reduces MAE\({}_{}\) to a low degree for all the model architectures and datasets compared with clean training, indicating a strong effectiveness and versatility of BackTime. Specifically, on the five dataset, MAE\({}_{}\) decrease on average by \(50.8\%\), \(44.10\%\), \(52.64\%\), \(83.52\%\), and \(45.40\%\), respectively. Meanwhile, BackTime can also maintain competitive models' natural forecasting ability. For example, on the PEMS08, Weather and ETTm1 datasets, models attacked by BackTime exhibit similar or even better forecasting performance than the clean training. In short, BackTime performs effective and versatile backdoor attacks across different model architectures, while still keeping models' competitive forecasting ability.

**Stealthiness evaluation.** To verify that the data modifications of BackTime are imperceptible, we employ anomaly detection methods, GDN  and USAD , to identify the poisoned time slots. Specifically, for each dataset, we train anomaly detection methods on the clean test set and then record the F1-score and the Area under the ROC Curve (ROC-AUC) on the poisoned training set. The experimental results are presented in Table 4. The results show that, across all datasets, ROC-AUC is around \(0.5\) and F1-score is either around \(0.5\) or near \(0\), suggesting that the detection results are nearly close to that of random guess. These strongly demonstrates the stealthiness of BackTime.

**Ablation study.** To investigate the impact of injection rates on the attack effectiveness, we conduct experiments on the PEMS03 dataset, with different temporal and spatial injection rates. The experimental results are shown in Figure 3. Based on the results, as the temporal injection rate \(_{}\) increases, a decreasing MAE\({}_{}\) and RMSE\({}_{}\) imply that the effect of BackTime gradually improves. However, even when \(_{}=0.015\), BackTime still implements an effective attack. On the other hand, as the spatial injection rate \(_{}\) increases, the effect of BackTime first improves and then decreases. This phenomenon may be due to the combined effects of two factors. First, an increase in \(_{}\) leads to more poisoned data, which reduces the difficulty of backdoor attack. Second, an increase in \(_{}\) leads to an increasing number of target variables, making the correlations among target variables more complicated and harder to learn. It increases the attack difficulty. Nonetheless, under all injection rates shown in Figure 3, BackTime successfully achieves the attack, demonstrating its superiority.

## 5 Related Work

**Multivariate time series forecasting.** Recently, many deep learning models have been proposed for MTS forecasting. TCN-based methods [55; 23; 65] capture temporal dependencies using convolutional kernels. GNN-based methods [39; 79; 31; 63] model inter-variable relationships in spatio-temporal graphs. Transformers [69; 81; 82; 49] excel in MTS forecasting by using attention mechanisms to capture temporal dependencies and inter-variable correlations.

**Adversarial attack on times series forecasting.** Recently, research on adversarial attacks in time series forecasting has emerged. Pialla et al.  propose an adversarial smooth perturbation by

    &  &  &  &  &  \\  & F1-score & AUC & F1-score & AUC & F1-score & AUC & F1-score & AUC & F1-score & AUC \\  GDN & 0.5006 & 0.5488 & 0.4971 & 0.5270 & 0.4986 & 0.5331 & 0.6015 & 0.6450 & 0.4970 & 0.5365 \\ USAD & 0.0000 & 0.5147 & 0.0000 & 0.5183 & 0.0668 & 0.4980 & 0.0000 & 0.5389 & 0.0000 & 0.5279 \\   

Table 4: Results of detecting modified segments of poisoned datasets by anomaly detection methods.

Figure 3: The impact of the temporal injection rate \(_{}\) and the spatial injection rate \(_{}\) on clean metrics, MAE\({}_{}\) and RMSE\({}_{}\), and attack metrics, MAE\({}_{}\) and RMSE\({}_{}\).

adding a smoothness penalty to the BIM attack . Dang et al.  use Monte-Carlo estimation to attack deep probabilistic autoregressive models. Wu et al.  generate adversarial time series through slight perturbations based on importance measurements. Mode et al.  employ BIM to target deep learning regression models. Xu et al.  use a gradient-based method to create imperceptible perturbations that degrade forecasting performance.

**Backdoor attacks.** Existing backdoor attacks aim to optimize triggers for effectiveness and stealthiness. Extensive works focus on designing special triggers, such as a single pixel , a black-and-white checkerboard , mixed backgrounds , natural reflections , invisible noise , and adversarial patterns . On time series classification, TimeTrojan  employs random noise as static triggers and adversarial perturbations as dynamic triggers, demonstrating that both types of triggers can successfully execute backdoor attacks. Jiang _et al._ generate triggers that are as realistic as real-time series patterns for stealthy and effective attack.

## 6 Future Directions And Potential Defenses

Backdoor attacks on Multivariate Time Series (MTS) represent a novel area of research, presenting numerous promising avenues for both attack and defense. In terms of attacks, beyond pursuing stealthier and more efficient triggers, there are several intriguing challenges that BackTime does not address.

First, attacking MTS imputation tasks remains unexplored and is difficult for BackTime to tackle. To achieve an effective backdoor attack, BackTime concatenates the trigger and target pattern sequentially to establish a strong temporal association, which is the foundation of its attack effectiveness. However, in time series imputation tasks, deep learning models infer missing values based on both preceding and subsequent data. This dual-direction inference reduces reliance on the data preceding the missing values, thus breaking BackTime's core assumption. Therefore, designing triggers that can influence both future and past data could be an interesting direction to explore.

Second, backdoor attacks on MTS forecasting with missing values pose a significant challenge. BackTime's attack depends on the inclusion of a complete trigger in the input to predict the target pattern. If the trigger is incomplete due to missing values, the attacked model might fail to recognize the trigger, rendering the backdoor attack ineffective. Hence, it would be highly interesting to design triggers that remain effective even when only partial triggers are included.

In terms of backdoor defense, detecting triggers in MTS is an intriguing problem and we provide some potential solutions here. First, there may be a frequency difference between the generated triggers and the real data. Therefore, detecting distribution shifts in the frequency domain could be a promising approach. Additionally, the generated triggers may lack the diversity present in real-world data. As a result, in the feature space, the (trigger, target pattern) pairs might cluster closely together, making it feasible to detect backdoor attacks using clustering algorithms.

## 7 Conclusion

In this paper, we study backdoor attacks in multivariate time series (MTS) forecasting. On this novel problem setting, we identify two main properties of backdoor attacks: stealthiness and sparsity, and further provide a detailed threat model. Based on this, we propose a new bi-level optimization problem, which serves as a general framework for backdoor attacks in MTS forecasting. Subsequently, we introduce BackTime, which utilizes a GNN-based trigger generator and a surrogate forecasting model to generate effective and stealthy triggers by iteratively solving the bi-level optimization. Extensive experiments on five real-world datasets demonstrate the effectiveness, versatility, and stealthiness of BackTime attacks.