# LION: Linear Group RNN for 3D Object Detection in Point Clouds

Zhe Liu\({}^{1*}\)

Equal contribution.

Jinghua Hou\({}^{1*}\)

Equal contribution.

Xinyu Wang\({}^{1*}\)

Xiaoqing Ye\({}^{3}\)

Jingdong Wang\({}^{3}\)

Corresponding author.

Hengshuang Zhao\({}^{2}\)

Xiang Bai\({}^{1}\)

Equal contribution.

###### Abstract

The benefit of transformers in large-scale 3D point cloud perception tasks, such as 3D object detection, is limited by their quadratic computation cost when modeling long-range relationships. In contrast, linear RNNs have low computational complexity and are suitable for long-range modeling. Toward this goal, we propose a simple and effective window-based framework built on **LI**near **g**O**up RNN (_i.e._, perform linear RNN for grouped features) for accurate 3D object detection, called **LION**. The key property is to allow sufficient feature interaction in a much larger group than transformer-based methods. However, effectively applying linear group RNN to 3D object detection in highly sparse point clouds is not trivial due to its limitation in handling spatial modeling. To tackle this problem, we simply introduce a 3D spatial feature descriptor and integrate it into the linear group RNN operators to enhance their spatial features rather than blindly increasing the number of scanning orders for voxel features. To further address the challenge in highly sparse point clouds, we propose a 3D voxel generation strategy to density foreground features thanks to linear group RNN as a natural property of auto-regressive models. Extensive experiments verify the effectiveness of the proposed components and the generalization of our LION on different linear group RNN operators including Mamba, RWKV, and RetNet. Furthermore, it is worth mentioning that our LION-Mamba achieves state-of-the-art on Waymo, nuScenes, Argoverse V2, and ONCE datasets. Last but not least, our method supports kinds of advanced linear RNN operators (_e.g._, RetNet, RWKV, Mamba, xLSTM and TTT) on small but popular KITTI dataset for a quick experience with our linear RNN-based framework.

## 1 Introduction

3D object detection serves as a fundamental technique in 3D perception and is widely used in navigation robots and self-driving cars. Recently, transformer-based  feature extractors have made significant progress in general tasks of Natural Language Processing (NLP) and 2D vision by flexibly modeling long-range relationships. To this end, some researchers have made great efforts to transfer the success of transformers to 3D object detection. Specifically, to reduce the computation costs, SST  and SWFormer  divide point clouds into pillars and implement window attention for pillar feature interaction in a local 2D window. Considering some potential information loss of the pillar-based manners along the height dimension, DSVT-Voxel  further adopts voxel-based formats and implements set attention for voxel feature interaction in a limited group size.

Although the above methods have achieved some success in 3D detection, they perform self-attention for pillar or voxel feature interaction with only a small group size due to computational limitations, locking the potential of transformers for modeling long-range relationships. Moreover, it is worth noting that modeling long-range relationships can benefit from large datasets, which will be important for achieving foundational models in 3D perception tasks in the future. Fortunately, in the field of large language models (LLM) and 2D perception tasks, some representative linear RNN operators such as Mamba  and RWKV  with linear computational complexity have achieved competitive performance with transformers, especially for long sequences. Therefore, a question naturally arises: can we perform long-range feature interaction in larger groups at a lower computation cost based on linear RNNs in 3D object detection?

To this end, we propose a window-based framework based on **LI**near gr**O**up RNN (_i.e._, perform linear RNN for grouped features in a window-based framework) termed **LION** for accurate 3D object detection in point clouds. Different from the existing method DSVT (b) in Figure 1, our LION (c) could support thousands of voxel features to interact with each other in a large group for establishing the long-range relationship. Nevertheless, effectively adopting linear group RNN to construct a proper 3D detector in highly sparse point cloud scenes remains challenging for capturing the spatial information of objects. Concretely, linear group RNN requires sequential features as inputs. However, converting voxel features into sequential features may result in the loss of spatial information (_e.g._, two features that are close in 3D spatial position might be very far in this 1D sequence). Therefore, we propose a simple 3D spatial feature descriptor and decorate the linear group RNN operators with it, thus compensating for the limitations of linear group RNN in 3D local spatial modeling.

Furthermore, to enhance feature representation in highly sparse point clouds, we present a new 3D voxel generation strategy based on linear group RNN to densify foreground features. A common manner of addressing this is to add an extra branch to distinguish the foregrounds, as seen in previous methods [53; 17; 70]. However, this solution is relatively complex and rarely used in 3D backbone due to its lack of structural elegance. Instead, we simply choose the high response of the feature map in the 3D backbone as the areas for voxel generation. Subsequently, the auto-regressive property of linear group RNN can be effectively employed to generate voxel features.

Finally, as shown in Figure 1 (a), we compare LION with the existing representative methods. We can clearly observe that our LION achieves state-of-the-art on a board autonomous datasets in terms of detection performance. To summarize, our contributions are as follows: **1)** We propose a simple and effective window-based 3D backbone based on the linear group RNN named LION to allow long-range feature interaction. **2)** We introduce a simple 3D spatial feature descriptor and integrate it with the linear group RNN, compensating for the lack of capturing 3D local spatial information. **3)** We provide a new 3D voxel generation strategy to densify foreground features, producing a more discriminative feature representation in highly sparse point clouds. **4)** We verify the generalization of our LION with different linear group RNN mechanisms (_e.g._, Mamba, RWKV, RetNet). In particular,

Figure 1: (a) Comparison of different 3D backbones in terms of detection performance on Waymo , nuScenes , Argoverse V2  and ONCE  datasets. Here, we adopt Mamba  as the default operator of our LION. Besides, we present the simplified schematic of DSVT (b)  and our LION (c) for implementing feature interaction in 3D backbones.

our LION-Mamba achieves state-of-the-art on challenging Waymo , nuScenes , Argoverse V2 , and ONCE  dataset, which further illustrates the superiority of LION.

## 2 Related Work

**3D Object Detection in Point Clouds.** 3D object detectors in point clouds can be roughly divided into point-based and voxel-based. For point-based methods [6; 68; 42; 10; 32; 39; 24; 50; 73; 67; 43; 65; 5], they usually sample point clouds and adopt point encoder [44; 45] to directly extract point features. However, the point sampling and grouping utilized by point-based methods is time-consuming. To avoid these problems, voxel-based methods [13; 12; 34; 48; 49; 51; 23; 59; 63; 69; 64; 70] convert the input irregular point clouds into regular 3D voxels and then extract 3D features by 3D sparse convolution. Although these methods achieve promising performance, they are still limited by the local receptive field of 3D convolution. Therefore, some methods [8; 36] adopt the large kernel to enlarge the receptive field and achieve better performance.

**Linear RNN.** Recurrent Neural Networks (RNNs) are initially developed to address problems in Natural Language Processing (NLP), such as time series prediction and speech recognition, by effectively capturing temporal dependencies in sequential data. Recently, to overcome the quadratic computational complexity of transformers, significant advancements have been made in time-parallelizable data-dependent RNNs (called linear RNNs in this paper) [46; 38; 40; 41; 55; 11; 66; 22; 54; 3]. These models retain linear complexity while offering efficient parallel training capabilities, allowing their performance to match or even surpass that of transformers. Due to their scalability and efficiency, linear RNNs are poised to play an increasingly important role in various fields and some works [1; 14; 29; 72] have applied linear RNNs to 2D/3D vision filed. In this paper, we aim to further extend linear RNNs to 3D object detection tasks thanks to their long-range relationship modeling capabilities.

**Transformers in 3D Object Detection.** Transformer  has achieved great success in many tasks, motivating numerous works to adopt attention mechanisms in 3D object detection to achieve better performance. However, the application of transformers is non-trivial in large-scale point clouds. Many works [16; 53; 35; 60] apply transformers to extract features by partitioning pillars or voxels into several groups based on local windows. Although these approaches achieve promising performance, they usually adopt small groups for feature interaction due to the quadratic computational complexity of transformers, hindering them from capturing long-range dependencies in 3D space. In contrast, we propose a simple and effective framework based on linear RNNs named LION to achieve long-range feature interaction for accurate 3D object detection thanks to their linear computational complexity.

## 3 Method

Due to computational limitations, some transformer-based methods [15; 60; 35] usually convert features into pillars or group small size of voxel features to interact with each other within small groups, limiting the advantages of transformers in long-range modeling. More recently, some linear RNN operators [22; 40; 55] that maintain linear complexity with the length of the input sequence

Figure 2: The illustration of LION, which mainly consists of \(N\) LION blocks, each paired with a voxel generation for feature enhancement and a voxel merging for down-sampling features along the height dimension. \((H,W,D)\) indicates the shape of the 3D feature map, where \(H\), \(W\), and \(D\) are the length, width, and height of the 3D feature map along the X-axis, Y-axis, and Z-axis. \(N\) is the number of LION blocks. In LION, we first convert point clouds to voxels and partition these voxels into a series of equal-size groups. Then, we feed these grouped features into LION 3D backbone to enhance their feature representation. Finally, these enhanced features are fed into a BEV backbone and a detection head for final 3D detection.

are proposed to model long-range feature interaction. More importantly, the linear RNN operators such as Mamba  and RWKV  have even shown comparable performance with transformers in LLM thanks to their low computation cost in long-range feature interaction. This further motivates us to adopt linear RNNs to construct a 3D detector for long-range modeling.

### Overview

In this paper, we propose a simple and effective window-based framework based on **L**I**near gr**O**up RNN (_i.e._, perform linear RNN for grouped features in a window-based framework) named **LION**, which can group thousands of voxels (dozens of times more than the number of previous methods [15; 60; 35]) for feature interaction. The pipeline of our LION is presented in Figure 2. LION consists of a 3D backbone, a BEV backbone, and a detection head, maintaining a consistent pipeline with most voxel-based 3D detectors [63; 60; 69]. In this paper, our contribution lies in the design of the 3D backbone based on linear group RNN, which will be introduced in the following.

**3D Sparse Window Partition.** Our LION is a window-based 3D detector. Thus, before feeding voxel features into our LION block, we need to implement a 3D sparse window partition to group them for feature interaction. Specifically, we first convert point clouds into voxels with the total number of \(L\). Then, we divide these voxels into non-overlapping 3D windows with the shape of \((T_{x},T_{y},T_{z})\), where \(T_{x}\), \(T_{y}\) and \(T_{z}\) denote the length, width, and height of the window along the X-axis, Y-axis, and Z-axis. Next, we sort voxels along the X-axis for the X-axis window partition and along the Y-axis for the Y-axis window partition, respectively. Finally, to save computation cost, we adopt the equal-size grouping manner in FlatFormer  instead of the classic equal-window grouping manner in SST . That is, we partition sorted voxels into groups with equal size \(K\) rather than windows of equal shapes for feature interaction. Due to the quadratic computational complexity of transformers, previous transformer-based methods [15; 60; 35] only achieve feature interaction using a small group size. In contrast, we adopt a much larger group size \(K\) to obtain long-range feature interaction thanks to the linear computational complexity of the linear group RNN operators.

### LION Block

The LION block is the core component of our approach, which involves LION layer for long-range feature interaction, 3D spatial feature descriptor for capturing local 3D spatial information, voxel merging for feature down-sampling and voxel expanding for feature up-sampling, as shown in Figure 3 (a). Besides, LION block is a hierarchical structure to better extract multi-scale features due to the gap of different 3D objects in size. Next, we introduce each part of LION block.

Figure 3: (a) shows the structure of LION block, which involves four LION layers, two voxel merging operations, two voxel expanding operations, and two 3D spatial feature descriptors. Here, \(1\), \(\), and \(\) indicate the resolution of 3D feature map as \((H,W,D)\), \((H/2,W/2,D/2)\) and \((H/4,W/4,D/4)\), respectively. (b) is the process of voxel merging for voxel down-sampling and voxel expanding for voxel up-sampling. (c) presents the structure of LION layer. (d) shows the details of the 3D spatial feature descriptor.

**LION Layer.** In LION block, we apply LION layer to model a long-range relationship among grouped features with the help of the linear group RNN operator. Specifically, as shown in Figure 3 (c), we provide the structure of LION layer, which is composed of two linear group RNN operators. The first one is used to perform long-range feature interaction based on the X-axis window partition and the second one can extract long-range feature information based on the Y-axis window partition. Taking advantage of two different window partitions, LION layer can obtain more sufficient feature interaction, producing more discriminative feature representation.

**3D Spatial Feature Descriptor.** Although linear RNNs have the advantages of long-range modeling with low computation cost, it is not ignorable that the spatial information might be lost when input voxel features are flattened into 1D sequential features. For example, as shown in Figure 4, there are two adjacent features (_i.e._, indexed as 01 and 34) in 3D space. However, after they are flattened into 1D sequential features, the distance between them in 1D space is very far. We regard this phenomenon as a loss of 3D spatial information. To tackle this problem, an available manner is to increase the number of scan orders for voxel features such as VMamba . However, the order of scanning is too hand-designed. Besides, as the scanning orders increase, the corresponding computation cost also increases significantly. Therefore, it is not appropriate in large-scale sparse 3D point clouds to adopt this manner. As shown in Figure 3 (d), we introduce a 3D spatial feature descriptor, which consists of a 3D sub-manifold convolution, a LayerNorm layer, and a GELU activation function. Naturally, we can leverage the 3D spatial feature descriptor to provide rich 3D local position-aware information for the LION layer. Besides, we place the 3D spatial feature descriptor before the voxel merging to reduce spatial information loss in the process of voxel merging. We provide the corresponding experiment in our appendix.

**Voxel Merging and Voxel Expanding.** To enable the network to obtain multi-scale features, our LION adopts a hierarchical feature extraction structure. To achieve this, we need to perform feature down-sampling and up-sampling operations in highly sparse point clouds. However, it is worth mentioning that we cannot simply apply max or average pooling or up-sampling operations as in 2D images since 3D point clouds possess irregular data formats. Therefore, as shown in Figure 3 (b), we adopt voxel merging for feature down-sampling and voxel expanding for feature up-sampling in highly sparse point clouds. Specifically, for voxel merging, we calculate the down-sampled index mappings to merge voxels. In voxel expanding, we up-sample the down-sampled voxels by the corresponding inversed index mappings.

### Voxel Generation

Considering the challenge of feature representation in highly sparse point clouds and the potential information loss of implementing voxel merging in Figure 2, we propose a voxel generation strategy to address these issues with the help of the auto-regressive capacity of the linear group RNN.

Figure 4: The illustration of spatial information loss when flattening into 1D sequences. For example, there are two adjacent voxels in spatial position (indexed as 01 and 34) but are far in the 1D sequences along the X order.

Figure 5: The details of voxel generation. For input voxels, we first select the foreground voxels and diffuse them along different directions. Then, we initialize the corresponding features of the diffused voxels as zeros and utilize the auto-regressive ability of the following LION block to generate diffused features. Note that we do not present the voxel merging here for simplicity.

**Distinguishing Foreground Voxels without Supervision.** In voxel generation, the first challenge is identifying which regions of voxel features need to be generated. Different from previous methods [53; 17; 70] that employ some supervised information based on well-learned BEV features to obtain the foreground region for feature diffusion. However, these approaches may be unsuitable for a 3D backbone and may even compromise its elegance. Interestingly, inspired by [34; 30], we notice that the corresponding high values of feature responses along the channel dimension in the 3D backbone (Refer to Figure 6 in our appendix) are usually the foregrounds. Therefore, we compute the feature response \(F_{i}^{*}\) for the output feature \(F_{i}\) of the \(i^{th}\) LION block, where \(i=1,2,...,N\) indicates the index of LION block in the 3D backbone. Thus, this above process can be formulated as:

\[F_{i}^{*}=_{j=0}^{C}F_{i}^{j},\] (1)

where \(C\) is the channel dimension of \(F_{i}\). Next, we sort the feature responses \(F_{i}^{*}\) in descending order and select the corresponding Top-\(m\) voxels as the foregrounds from the total number \(L\) of non-empty voxels, where \(m=r*L\) and \(r\) is the ratio of foregrounds. This process can be computed as:

\[F_{m}=_{m}(F_{i}^{*}),\] (2)

where \(_{m}(F_{i}^{*})\) means selecting Top-\(m\) voxel features from \(F_{i}^{*}\). \(F_{m}\) are the selected foreground features, which will serve for the subsequent voxel generation.

**Voxel Generation with Auto-regressive Property.** The previous method  adopts a K-NN manner to obtain generated voxel features based on their K-NN features, which might be sub-optimal to enhance feature representation due to the redundant features and the limited receptive field. Fortunately, the linear RNN is well-suited for auto-regressive tasks in addition to its advantage of handling long sequences. Therefore, we leverage the auto-regressive property of linear RNN to effectively generate the new voxel features by performing sufficient feature interaction with other voxel features in a large group. Specifically, for convenience, we define the corresponding coordinates of selected foreground voxel features \(F_{m}\) as \(P_{m}\). As shown in Figure 5, we first obtain diffused voxels by diffusing \(P_{m}\) with four different offsets (_i.e._, [-1,-1, 0], , [1,-1, 0], and [-1,1, 0]) along the X-axis, Y-axis, and Z-axis, respectively. Then, we initialize the corresponding features of diffused voxels by all zeros. Next, we concatenate the output feature \(F_{i}\) of the \(i^{th}\) LION block with the initialized voxel features, and feed them into the subsequent \((i+1)^{th}\) LION block. Finally, thanks to the auto-regressive ability of the LION block, the diffused voxel features can be effectively generated based on other voxel features in large groups. This process can be formulated as:

\[F_{p}=F_{i} F_{[-1,-1,0]} F_{} F_{[1,-1,0]} F _{[-1,1,0]},\] (3)

\[F_{p}^{{}^{}}=(F_{p}),\] (4)

where \(F_{[x,y,z]}\) denotes the initialized voxel features with diffused offsets of x, y, and z along the X-axis, Y-axis, and Z-axis. The \(\) and \(\) denote the concatenation and LION block respectively.

## 4 Experiments

### Datasets and Evaluation Metrics

**Waymo Open Dataset.** Waymo Open dataset (WOD)  is a well-known benchmark for large-scale outdoor 3D perception, comprising 1150 scenes which are divided into 798 scenes for training, 202 scenes for validation, and 150 scenes for testing. Each scene includes about 200 frames, covering a perception range of \(150m 150m\). For evaluation metrics, WOD employs 3D mean Average Precision (mAP) and mAP weighted by heading accuracy (mAPH), each divided into two difficulty levels: L1 is for objects detected with more than five points and L2 is for those at least one point.

**nuScenes Dataset.** nuScenes  is a popular outdoor 3D perception benchmark with a perception range of up to 50 meters. Each frame in the scene is annotated with 2Hz. The dataset includes 1000 scenes, which is divided into 750 scenes for training, 150 scenes for validation, and 150 scenes for testing. nuScenes adopts mean Average Precision (mAP) and the NuScenes Detection Score (NDS) as evaluation metrics.

**Argoverse V2 Dataset.** Argoverse V2  is a outdoor 3D perception benchmark with a long-range perception of up to 200 meters. It contains 1000 sequences in total, 700 for training, 150 for validation, and 150 for testing. Each frame in the scene is annotated with 10Hz. For the evaluation metric, Argoverse v2 adopts a similar mean Average Precision (mAP) metric with nuScenes .

### Implementation Details

**Network Architecture.** In our LION, we provide three representative linear RNN operators (_i.e._, Mamba , RWKV , and RetNet ). Each of operator adopts a bi-directional structure to better capture 3D geometric information inspired by . On WOD, we keep the same channel dimension \(C=64\) for all LION blocks in LION-Mamba, LION-RWKV, and LION-RetNet. For the large version of LION-Mamba-L, we set \(C=128\). We follow DSVT-Voxel  to set the grid size as (0.32m, 0.32m, 0.1875m). The number of LION blocks \(N\) is set to 4. For these four LION blocks, the window sizes \((T_{x},T_{y},T_{z})\) are set to \((13,13,32)\), \((13,13,16)\), \((13,13,8)\), and \((13,13,4)\), and the corresponding group sizes \(K\) are \(4096\), \(2048\), \(1024\), \(512\), respectively. Besides, we adopt the same center-based detection head and loss function as DSVT  for fair comparison. In the voxel generation, we set the ratio \(r=0.2\) to balance the performance and computation cost. For the nuScenes dataset, we replace DSVT  3D backbone with our LION 3D backbone except for changing the grid size to \((0.3m,0.3m,0.25m)\). For the Argoverse V2 dataset, we replace the 3D backbones of VoxelNext  or SAFDNet  with our LION 3D backbone except for setting the grid size to \((0.4m,0.4m,0.25m)\). Moreover, it is noted that we only add three extra LION layers to further enhance the 3D backbone features, rather than applying the BEV backbone to obtain the dense BEV features.

**Training Process.** On the WOD, we adopt the same point cloud range, data augmentations, learning rate, and optimizer as the previous method . We train our model 24 epochs with a batch size of 16 on 8 NVIDIA Tesla V100 GPUs. Besides, we utilize the fade strategy  to achieve better performance in the last epoch. For the nuScenes dataset, we adopt the same point cloud range, data augmentations, and optimizer as previous method . Moreover, we find that LION converges faster than previous methods on nuScenes dataset. Therefore, we only train our model for 36 epochs without CBGS . The learning rate and batch size are set to 0.003 and 16, respectively. It is worth noting that the CBGS strategy extends training iterations about 4.5 times, which means that our training iterations are much fewer than previous methods  (_i.e._, 20 epochs with CBGS). For the Argoverse V2 dataset, we adopt the same training process with SAFDNet  and SECOND , respectively.

### Main Results

In this section, we provide a board comparison of our LION with existing methods on WOD, NuScenes and Argoverse V2 datasets for 3D object detection. Furthermore, in the section A.1 of our appendix, we present the experiment in ONCE dataset  and provide more types of linear RNN operators (_e.g._, RetNet, RWKV, Mamba, xLSTM, and TTT) based on our LION framework for 3D detection on a small but popular dataset KITTI  for a quick experience.

**Results on WOD.** To illustrate the superiority of our LION, we provide the comparison with existing representative methods on the WOD in Table 1. Here, we also conduct the experiments on our LION with different linear group RNN operators, including LION-Mamba, LION-RWKV and LION-RetNet. Compared with the transformer-based methods , our LION with different linear group RNN operators outperforms the previous state-of-the-art (SOTA) transformer-based 3D backbone DSVT-Voxel , illustrating the generalization of our proposed framework. To further scale up our LION, we present the performance of LION-Mamba-L by doubling the channel dimension of LION-Mamba. It can be observed that LION-Mamba-L significantly outperforms DSVT-Voxel with 1.9 mAPH/L2, leading to a new SOTA performance. The above promising results effectively demonstrate the superiority of our proposed LION.

In Table 2, we also provide the results with multiple frames as inputs on the WOD _test_ split. For three frames, our LION-L outperforms the representative method PillarNeXt  by 3.3 (77.4 vs. 74.1) mAPH/L2, which clearly illustrates the superiority of our methods.

**Results on nuScenes.** We also evaluate our LION on nuScenes _validation_ and _test_ set  further to verify the effectiveness of our LION. As shown in Table 3, on nuScenes _validation_ set, our LION-RetNet, LION-RWKV, and LION-Mamba achieves 71.9, 71.7, and 72.1 NDS, respectively, which outperforms the previous advanced methods DSVT  and HEDNet . Besides, our LION-Mamba even brings a new SOTA on nuScenes _test_ benchmark, which beats the previous advanced method DSVT with 1.2 NDS and 1.4 mAP, illustrating the superiority of our LION. Note that all results of our LION are conducted without any test-time augmentation and model ensembling.

**Results on Argoverse V2.** To further verify the effectiveness of our LION on the long-range perception, we evaluate the experiments on Argoverse V2 _validation_ set. For a fair comparison, we adopt the same detection head  with VoxelNext  and SAFDNet  for long-range perception. As shown in Table 4, our LION-NetNet, LION-RWWKV, and LION-Mamba achieve the detection performance with 40.7 mAP, 41.1 mAP and 41.5 mAP, all three of which have outperformed the previous SOTA method SAFDNet , leading to new SOTA results. These superior results clearly illustrate the effectiveness of our LION.

### Ablation Study

In this section, we conduct ablation studies of LION on the WOD _validation_ set with 20% training data. If not specified, we adopt LION-Mamba as our default model and train our model with 12 epochs in the following ablation studies. For more experiments, please refer to our appendix.

**Ablation Study of LION.** To illustrate the effectiveness of our proposed LION, we conduct the ablation study for each component, including the design of large group size, 3D spatial feature descriptor, and voxel generation in Table 5. Here, our baseline is proposed LION that removes the design of large group size, 3D spatial feature descriptor, and voxel generation. In Table 5, we observe that the design of large group size even brings 1.1 mAPH/L2 performance improvement, which illustrates the benefits of performing long-range feature interaction with the help of linear RNN. Then, we integrate the 3D spatial feature descriptor, which further produces an obvious performance improvement with 1.7 mAPH/L2. This demonstrates the superiority of the 3D spatial feature descriptor in compensating for the lack of capturing spatial information of linear RNNs. Furthermore, we notice that the 3D spatial feature descriptor is very helpful to small objects (_e.g._, Pedestrians) thanks to its capability of extracting the local information of 3D objects. To address

    &  &  &  &  &  &  \\  & & L1 & L2 & L1 & L2 & L1 & L2 & L2 \\  SECOND  & Sensors 18 & & 72.37/1.7 & 63.98/4.3 & 68.75/8.2 & 60.75/1.3 & 60.65/9.3 & 58.35/5.0 & 61.05/7.2 \\ PointPillars  & CVPR 19 & 72.71/7.6 & 63.63/6.1 & 76.56/7.6 & 62.85/3.0 & 64.46/2.3 & 61.95/9.9 & 62.85/7.8 \\ CenterPoint  & CVPR 21 & 74.27/73.6 & 66.26/7.5 & 76.07/5.0 & 68.63/2.2 & 37.11/9.6 & 79.05/8.6 & 68.26/5.8 \\ PV-RCNN & CVPR 20 & 78.07/7.5 & 69.49/0.9 & 79.27/30.4 & 70.46/4.7 & 71.57/0.3 & 69.06/7.8 & 69.66/6.2 \\ FullNet-34  & ECCV 22 & 79.17/8.6 & 70.90/5.8 & 80.67/3.5 & 72.36/6.2 & 27.37/1.2 & 69.95/78.7 & 71.06/8.5 \\ FSD  & NeurIPS 28 & 79.72/8.8 & 70.57/1.0 & 81.67/73.7 & 73.99/8.1 & 77.71/6.0 & 74.47/3.3 & 72.97/0.8 \\ AFDev2  & AAAI 22 & 76.77/7.1 & 69.76/9.2 & 80.27/4.2 & 72.26/7.0 & 73.72/7.7 & 71.07/0.1 & 71.06/8.8 \\ PhilNet28  & CVPR 23 & 78.47/7.9 & 70.36/8.9 & 87.51/7.4 & 77.49/8.9 & 72.27/0.6 & 69.66/ & 71.96/9.7 \\ VoxelNet  & CVPR 23 & 78.27/7.7 & 69.98/4.1 & 81.57/6.3 & 73.58/6.6 & 76.17/4.9 & 73.37/2.2 & 23.27/0.1 \\ CenterFormer & ECCV 22 & 75.07/4.0 & 69.99/4.8 & 78.67/3.3 & 73.68/6.3 & 72.71/3.9 & 69.88/8.8 & 71.16/8.9 \\ PV-RCNN+4  & ECCV 22 & -/37.8 & 70.76/7.0 & 81.37/6.3 & 73.28/0.8 & 73.72/7.1 & 71.27/0.2 & 71.76/9.5 \\ Transfusion  & CVPR 22 & -//- & -/- & -/- & -/- & -/- & -/- & -/6/9 & -/4/9.9 \\ CoQuet  & CVPR 23 & 76.16/7.5 & 68.78/6.2 & 79.07/2.3 & 70.96/7.3 & 79.72/7.5 & 14.70/1.0 & 73.06/7.7 \\ FocalFormer3D  & ICCV 23 & -/- & 68.16/7.6 & -/- & 72.76/6.8 & -/- & 73.72/7.6 & 71.56/9.0 \\ HEDNet  & NeurIPS 23 & 81.18/0.6 & 73.72/7.2 & 84.80/0.0 & 76.87/2.6 & 78.77/7.7 & 75.87/4.8 & 75.37/3.4 \\ SELD  & ECCV 24 & 79.79/2.7 & 71.87/1.7 & 73.87/1.8 & 73.75/7.8 & 80.07/8.8 & 77.37/6.1 & 49.72/8.4 \\  SST\_TS1  & CVPR 22 & 76.27/5.8 & 68.06/7.6 & 81.47/4.0 & 72.86/5.9 & -/- & -/- & -/- \\ SWFormer  & ECCV 22 & 78.77/7.9 & 69.26/8.8 & 80.97/2.7 & 72.56/4.9 & -/- & -/- & -/- \\ O-FL  & CVPR 23 & 78.17/7.6 & 69.89/3.9 & 83.74/4.4 & 72.56/6.5 & 72.67/1.6 & 69.96/8.9 & 70.76/8.2 \\ DSVT-Pillar  & CVPR 23 & 79.37/8.6 & 70.97/5.0 & 82.87/7.0 & 75.26/9.8 & 76.47/5.4 & 73.67/2.7 & 73.27/1.0 \\ DSVT-Voxel  & CVPR 23 & 79.79/7.3 & 71.47/1.0 & 83.77/8.6 & 76.11/7.5 & 77.57/6.4 & 74.67/3.7 & 74.07/2.1 \\  LION-RetNet (Ours) & - & 79.07/8.5 & 70.67/0.2 & 84.68/0.0 & 77.27/2.8 & 79.07/8.0 & 76.17/5.1 & 74.67/2.7 \\ LION-RWWKV (Ours) & - & 79.77/9.3 & 71.37/1.0 & 84.68/0.0 & 77.17/2.2 & 78.77/7.7 & 75.87/4.8 & 74.7/2.8 \\ LION-Mamba (Ours) & - & 79.75/7.1 & 71.17/7.0 & 84.98/4.0 & 77.57/3.2 & 79.78/7.6 & 76.75/5.8 & 75.17/3.2 \\ LION-Mamba (Ours) & - & 80.**37/9.7** & **72.07/1.6** & **85.88/1.4** & **75.74/3.8** & **80.17/9.0** & **77.27/6.5** & **75.97/4.0** \\   

Table 1: Performances on the Waymo Open Dataset _validation_ set (train with 100% training data). \(\) denotes the two-stage method. **Bold** denotes the best performance of all methods. “-L” means we double the dimension of channels in LION 3D backbone. RNN denotes the linear RNN operator. All results are presented with single-frame input, not test-time augmentation, and no model ensembling.

    &  &  &  &  &  &  \\  & & L1 & L2 & L1 & L2 & L1 & L2 & L2 \\  PV-RCNN++  & IJCV 22 & 1 & 81.6/81.2 & 73the challenge of feature representation in highly sparse point clouds, we adopt voxel generation to enhance the features of foregrounds, which brings a promising gain of 0.7 mAPH/L2 (67.6 _vs._ 66.9). By combining all components, our LION achieves a superior performance of 69.3 mAPH/L2, which outperforms the baseline of 3.5 mAPH/L2.

**Superiority of 3D Spatial Feature Descriptor.** To further verify the necessity of 3D spatial feature descriptor, we provide the comparison with two available manners including the MLP and linear RNN to replace our descriptor in Table 6. Here, we set our LION without 3D spatial feature descriptor as the baseline in this part. We observe that MLP even does not bring promising performance improvement in terms of mAPH/L2 since MLP lacks the ability to capture local 3D spatial information. Furthermore, considering the limited receptive field of MLP, we adopt a linear group RNN operator to replace MLP. We find that there is only slight performance improvement with 0.3 mAPH/L2, which indicates that the linear group RNN might not be good at modeling local spatial relationships although it has the strong capability to establish long-range relationships. In contrast, our 3D spatial feature

   } &   }} &  {tabulardescriptor brings obvious performance improvement, which boosts the baseline of 1.7 mAPH/L2. This effectively illustrates the superiority of the 3D spatial feature descriptor in compensating for the lack of local 3D spatial-aware modeling in the linear group RNN.

**Effectiveness of Voxel Generation.** Voxel generation is applied to enhance the feature representation of objects in highly sparse point clouds for accurate 3D object detection. Therefore, to explore the effectiveness of our proposed voxel generation, we present the comparison with several available manners in Table 7. First, we compare our results of IV with II by only replacing the operator of linear group RNN in LION block with 3D sub-manifold convolution to generate the diffused features. We find that the manner of IV (69.3 _vs._ 66.6) significantly outperforms the performance of II in terms of mAPH/L2. This benefits from the linear group RNN's ability to model long-range feature interactions, generating a more reliable feature representation through its auto-regressive capacity, demonstrating the superiority of voxel generation with the linear group RNN. To further illustrate that the effectiveness of voxel generation is from its auto-regressive property of LION block rather than a strong feature extractor, we initialize the diffused features of the foreground voxels by K-NN operation (III) instead of the manner of all-zeros features (IV) and then feed them to the same following LION block for voxel generation. In Table 7, we find that the manner of III is inferior to IV by 1.0 mAPH/L2. This clearly illustrates that our voxel generation is benefiting from its auto-regressive property of LION block. Finally, compared with the baseline (I), our voxel generation (IV) can obtain a promising performance improvement, which verifies its effectiveness.

## 5 Conclusion

In this paper, we have presented a simple and effective window-based framework termed LION, which can capture the long-range relationship by adopting linear RNN for large groups. Specifically, LION incorporates a proposed LION block to unlock the great potential of linear RNNs in modeling a long-range relationship and a voxel generation strategy to obtain more discriminative feature representation in sparse point clouds. Extensive ablation studies demonstrate the effectiveness of our proposed components. Additionally, the generalization of our LION is verified by performing different linear group RNN operators. Benefiting from our well-designed framework and the proposed superior components, our LION-Lambda achieves state-of-the-art performance on the challenging Waymo and nuScenes datasets.