# Smooth Flipping Probability for Differentially Private Sign Random Projection Methods

Ping Li, Xiaoyun Li

LinkedIn Ads

700 Bellevue Way NE, Bellevue, WA 98004, USA

{pingli98, lixiaoyun996}@gmail.com

###### Abstract

We develop a series of differential privacy (DP) algorithms from a family of random projection (RP) and sign random projection (SignRP) methods. We first show how to improve the previous DP-RP approach using the "optimal Gaussian mechanism". Then, we propose a series of DP-SignRP algorithms that leverage the robustness of the "sign flipping probability" of random projections. That is, given \(x=_{i=1}^{p}u_{i}w_{i}\) where \(u\) is a \(p\)-dimensional data vector and \(w\) is a symmetric random vector, \(sign(x)\) only has a fairly small probability to be flipped if there is a small modification on data \(u\), depending on the specific distribution of \(w\). This robustness leads to our novel design of "smooth flipping probability" for SignRP-type algorithms with better utility than using the standard randomized response mechanism. Retrieval and classification experiments demonstrate that, among the presented DP-RP algorithms, **DP-SignOPRP** (where OPORP is an improvement over the celebrated count-sketch algorithms), performs the best in general.

In the industrial practice, DP methods were not very popular for machine learning or search, largely because the performance typically would drop substantially if DP is applied. Since our proposed new DP algorithms have significantly improved the performance, it is anticipated that our work will motivate a wide adoption of DP in practice. Finally, we stress that, since our methods are applied to the original data (i.e., feature vectors), the privacy of downstream tasks is naturally protected.

## 1 Introduction

Protecting data privacy has become an urgent need and a trending research topic recently. Among many notions of privacy, the "differential privacy" (DP)  has gained tremendous attention in both the research community and industrial applications, and has been widely applied to numerous tasks such as frequency estimation , clustering , regression and classification , DP-SGD , principle component analysis , empirical risk minimization , graph analysis , matrix completion , etc. In a machine learning model, there are several choices on when to deploy differential privacy (DP): (a) at the data collection/processing stage ; (b) during the model training stage ; and (c) on the model output (or summary statistics) . Among them, applying DP as early as at the data collection stage provides strong protection in the sense that the subsequent operations or outputs will become private by the post-processing property of DP. Moreover, when the data holder needs to release the data to third-parties, a privacy-preserving data publishing mechanism (at an early stage) becomes necessary.

In this paper, we focus on the differential privacy of the broad family of random projection (RP) methods, including RP, sign random projections (SignRP) and count-sketch-type algorithm. Our algorithms can be applied to data publishing and processing for downstream machine learning tasks.

### Random Projections (RP) and Sign Random Projections (SignRP)

In practice, data compression and dimension reduction techniques can be crucial when dealing with massive (high-dimensional) data. The random projection (RP) method is an important and fundamental dimensionality reduction algorithm. Denote \(u^{p}\) as the data vector with \(p\) features. With some \(k\) (and typically \(k p\)), we define the random projection of \(u\) as

\[X=}W^{T}u,\ \ W^{p k}.\] (1)

The entries of the random matrix \(W\) typically follow the Gaussian distribution or Gaussian-like distribution such as the Rademacher (symmetric Bernoulli) distribution. It has been well-understood that, with a sufficient number of projections (\(k\)), the distance between two vectors (using the same projection \(W\)) is preserved within a small multiplicative error with high probability. The dimensionality reduction and geometry preserving properties make RP widely useful in numerous applications, such as distance estimation, nearest neighbor search, clustering, classification, compressed sensing, permutation recovery, etc. .

While RP is able to reduce the dimensionality, storing and transmitting projected data might still be expensive for large datasets. One can further compress the RPs by quantization/discretization, where we only use a few bits to represent the projected values . In this paper, we will consider the differential privacy of the extreme case of sign (1-bit) random projection (SignRP), also known as the "SimHash" , where only the sign of the projected data is stored. The signs still preserve the angle/cosine information. SignRP has been widely used for approximate near neighbor (ANN) search  by building hash tables from the bits . RP and SignRP can also be extended to non-linear random features .

### Count-Sketch and OPORP: Efficient RP-type Alternatives

The celebrated count-sketch  can be viewed as a highly efficient RP-type algorithm, because, as an option, it only requires "one permutation + one random projection" (OPORP) , as opposed to \(k\) projections in RP. Applications of count-sketch include graph embedding , word & image embedding , model & communication compression , etc. The recent work by  improved the original count-sketch in two aspects: (i) using a fixed-length binning scheme makes the algorithms more convenient and also reduces the variance by a factor of \(\); (ii) the projected data (i.e., vectors in \(k\) dimensions) should be normalized before they are used to estimate the original cosine \(\), which also substantially reduces the estimation variance, essentially from \((1+^{2})/k\) (un-normalized) to \((1-^{2})^{2}/k\) (normalized).

**The main contributions** of this work include the following:

* In Section 3, for DP-RP, we first revisit the prior work  on the \((,)\)-DP random projection based on Gaussian noise addition (called DP-RP-G). Then we incorporate the optimal Gaussian mechanism and propose an improved method DP-RP-G-OPT. We also develop DP-OPORP based on the optimal Gaussian mechanism.
* In Section 4, we propose two algorithms for privatizing the (1-bit) DP-SignRP. The first method, DP-SignRP-RR, is based on the standard "randomized response" (RR). Then, we propose an improved method named DP-SignRP-RR-smooth based on a proposed concept of _"smooth flipping probability"_, thanks to the robustness brought by the "aggregate-and-sign" operations of SignRP. Finally, we extend the idea of smooth flipping probability to OPORP and show that DP-SignOPRP is more advantageous in terms of privacy.
* In Section 5, we conduct retrieval and classification experiments on benchmark datasets. For full-precision methods, the proposed DP-OPORP substantially improves prior method DP-RP-G. For the 1-bit RP variants, the suggested DP-SignOPORP can outperform DP-RP and DP-OPRP especially when \(\) is not large (e.g., \( 5\)). Our results also verify the advantage of smooth flipping probability over the classic bit flipping strategy.

## 2 Background on Differential Privacy

Throughout the paper, \(\|\|_{1}\) and \(\|\|_{2}\) are the \(l_{1}\) and \(l_{2}\) norms, respectively, and \(\|\|\) will denote the \(l_{2}\) norm if there no risk of confusion. Let \(u\) be the data vector of a user where \(\) denotes the data domain. In this paper, we consider real-valued data vectors with bounded range, i.e., \(=[-1,1]^{p}\) for convenience. We also assume \(\|u\|>0\), i.e., there is no all-zero data sample. The formal definition of differential privacy (DP) is given as follows.

**Definition 2.1** (Differential Privacy ).: _For a randomized algorithm \(: Range()\), if for any two adjacent datasets \(u\) and \(u^{}\), it holds that_

\[Pr[(u) O] e^{}Pr[(u^{}) O]+\] (2)

_for \( O Range()\) and some \(, 0\), then algorithm \(\) is called \((,)\)-differentially private. If \(=0\), \(\) is called \(\)-differentially private._

In this work, we follow the standard setup in the literature of DP sketching/hashing (e.g., ) that neighboring data vectors \(u\) and \(u^{}\) only differ in one dimension, which is usually referred to as the "attribute-level DP"--an adversary cannot detect a small change in any attribute of \(u\) based on the output of the algorithm.

**Definition 2.2** (\(\)-adjacency).: _Let \(u[-1,1]^{p}\) be a data vector. A vector \(u^{}[-1,1]^{p}\) is said to be \(\)-adjacent to \(u\) if \(u^{}\) and \(u\) differ in one dimension \(i\), and \(|u_{i}-u^{}_{i}|\)._

Data vectors \(u\) and \(u^{}\) satisfying Definition 2.2 are called \(\)-adjacent or \(\)-neighboring. In the literature (e.g., ), \(=1\) is typically used. We allow a general \(\) to accommodate the practical needs. Moreover, the concept of sensitivity is important for the design of DP noise addition mechanisms.

**Definition 2.3** (\(l_{2}\)-sensitivity).: _Let \(Nb(u)\) denote the neighbor set of \(u\), and \(()=\{(u,u^{}):u^{} Nb(u),u,u^{} \}\) be the collection of all possible neighboring data pairs. The \(l_{2}\)-sensitivity of a function \(f:^{k}\) is defined as \(_{2}=_{(u,u^{})()}\|f(u)-f(u^{ })\|_{2}\)._

The following composition theorem of DP is also useful in our analysis.

**Theorem 2.1** (Composition Theorem ).: _Let \(_{j}:\) be an \((_{j},_{j})\)-DP algorithm for \(j=1,...,k\). Then \((u)=(_{1}(u),...,_{k}(u))\) is \((_{j=1}^{k}_{i},_{j=1}^{k}_{j})\)-DP._

## 3 Revisiting Differentially Private Random Projection Methods

We first revisit the DP noise addition mechanism for random projections and show how to apply the optimal Gaussian mechanism  to improve prior methods. We also propose a DP algorithm based on a variant of count-sketch which is computationally much more efficient than dense projections.

### Gaussian Noise Mechanism for DP-RP

Let \(=\{u,u^{}:u Nb(u)\}\) be the collection of all possible \(\)-neighboring data pairs, where \(Nb(u)\) is the neighbor set of \(u\). For a \(p\)-by-\(k\) projection matrix \(W\), the \(l_{2}\)-sensitivity of the RP (1), according to Definition 2.3, can be precisely computed by

\[_{2}=}_{(u,u^{})( )}\|W^{T}u-W^{T}u^{}\|_{2}=}_{i=1, ,p}\|W_{[i,:]}\|_{2},\] (3)

where \(W_{[i,:]}\) denotes the \(i\)-th row of \(W\). As such, the DP noise level depends on the choice of projection matrix \(W\). A comparison of Rademacher vs. Gaussian projection in DP-RP method can be found in Section B.1. With this calculation, the general Gaussian noise mechanism for DP-RP is summarized in Algorithm 1. We present the algorithms for a single data point \(u\), and the procedure is applied to every data vector in the database (with same \(W\) but independent noise). After random projection, we simply add a random Gaussian noise vector following \(\ N(0,^{2})\) to the projected data. The following "DP-RP-G" result is known in the literature.

**Theorem 3.1** (DP-RP-G ).: _Let \(_{2}\) be defined in (3). For any \(>0\) and \(0<<\), DP-RP-G in Algorithm 1 is \((,)\)-DP if \(_{2}}{}\)._

**Remark 3.1**.: _Note that in implementation, \(_{2}\) is computed by (3) using the realization of the projection matrix \(W\). In Appendix A, we provide a high probability bound on \(_{2}\) and more analysis and discussion on the Laplace mechanism for DP-RP._

In Theorem 3.1 (as well as the classical Gaussian mechanism, e.g., ), the analysis on the noise level is based on upper bounding the tail of Gaussian distribution. It can be improved by computing the exact tail probabilities. [4, Theorem 8] proposed an optimal Gaussian mechanism based on this idea, which allows us to improve Theorem 3.1 and obtain the DP-RP-G-OPT method as follows.

**Theorem 3.2** (DP-RP-G-OPT).: _Suppose \(_{2}\) is defined as (3). For any \(>0\) and \(0<<1\), DP-RP-G-OPT in Algorithm 1 achieves \((,)\)-DP if \(^{*}\) where \(^{*}\) is the solution to the equation_

\[(}{2}-})-e^{}(-}{2}- })=,\] (4)

_where \(()\) is the cdf of the standard normal distribution._

**Remark 3.2**.: _For both Theorem 3.2 and Theorem 3.1, the Gaussian noise level is \(=(}{})\) when \( 0\), and \(=(}{})\) when \(\), which is rate optimal . Essentially, Theorem 3.2 reduces the noise variance of Theorem 3.1 by analyzing the tail probability exactly._

### DP-OPORP: A More Efficient Alternative

The standard random projections (1) require \(k\) projections. We can reduce the cost from \(O(kp)\) to \(O(p)\) by count-sketch: "binning + Rademacher RP". Basically, we split the data entries into \(k\) bins, and apply RP in each bin to generate \(k\) samples.  studied noise injection mechanism for count-sketch . Recently,  proposed OPORP (One Permutation + One Random Projection), an improved variant of count-sketch using fixed-length binning and the normalized estimator. The steps are summarized in Algorithm 2. Note that the output \(x_{i}\) can be \(l_{2}\) normalized to reduce variance.

```
1Input: Data vector \(u^{p}\); Number of projected samples \(k\)
2Output:\(k\) OPORP samples
3Apply a permutation \(:[p][p]\) to \(u\) to get \(u_{}\)
4Split the \(p\) permuted data columns into \(k\) consecutive length-\(p/k\) bins: \(u_{}=[u_{}^{(1)},...,u_{}^{(k)}]\)
5Generate a vector \(w^{p}\) following Rademacher distribution, denoted as \(w=[w^{(1)},...,w^{(k)}]\)
6Return \(k\) projected samples by \(x_{i}={w^{(i)}}^{T}u_{}^{(i)}\), for \(i=1,...,k\). ```

**Algorithm 2**OPORP: count-sketch with fixed-length binning

For OPORP, the sensitivity \(_{2}=\), due to the binning and Rademacher projection: changing one coordinate of \(u\) by \(\) leads to a change of \(\) in term of \(l_{2}\) distance between the projected vectors.

**Theorem 3.3** (DP-OPORP).: _Adding noise from iid \(N(0,^{2})\) to OPORP (the output of Algorithm 2) achieves \((,)\)-DP, where \(\) is the solution to (4) with \(_{2}=\)._

The advantages of DP-RP and DP-OPORP over the approach of adding Gaussian noise to raw data for inner product estimation are theoretically justified in Appendix C.

DP-SignRP: Differentially Private Sign Random Projections

We now propose our main algorithms that output SignRP with DP guarantees. Firstly, we analyze the standard randomized response (RR) technique, DP-SignRP-RR, under Gaussian random projections. Next, we propose the concept of "smooth flipping probability" and develop DP-SignRP-RR-smooth as an improvement. Finally, we propose DP-SignOPOP but combines all the techniques.

### DP-SignRP-RR by Randomized Response

```
1Input: Data \(u[-1,1]^{p}\); \(>0\), \(0<<1\); Number of projections \(k\); norm lower bound \(m\)
2Output: Differentially private sign random projections
3Apply RP by \(x=}W^{T}u\), where \(W^{p k}\) is a random \(N(0,1)\) matrix
4Let \(N_{+}(m,,k,p)\) be computed as in Proposition 4.3
5Compute \(_{j}=sign(x_{j}),&}}{e^{^{}}+1}j=1,...,k^{}= /N_{+}(m,,k,p)\\ -sign(x_{j}),&}+1}\) Return \(\) as the DP-SignRP of \(u\) ```

**Algorithm 3**DP-SignRP-RR

We develop DP-SignRP-RR based on the classic randomized response (RR) mechanism [77; 31]. As summarized in Algorithm 3, after we apply random projection \(x=W^{T}u\), we take \(s=sign(x)\). Then, for each \(s_{j}\), we keep the sign with probability \(}}{e^{^{}}+1}\) and flip the sign with probability \(}+1}\). Here \(^{}=/N_{+}\), where \(N_{+}(m,,k,p)\) is an upper bound on the number of different signs (among \(k\) signs) of \(x=W^{T}u\) and \(x=W^{T}u^{}\) for any \(\)-adjacent \((u,u^{})\), which will be derived later in Proposition 4.3. Here, \(m\) is a lower bound on the \(l_{2}\) norm of the data, i.e., \(\|u\| m\) for all \(u\). The final output is \(\) after perturbing \(s\) by the above procedure.

**Theorem 4.1**.: _Algorithm 3 is \((,)\)-DP._

Proof.: For any data point \(u\) and its \(\)-adjacent neighbor \(u^{}\), denote \(s=sign(W^{T}u)\{-1,+1\}^{k}\), \(s^{}=sign(W^{T}u^{})\{-1,+1\}^{k}\), and let \(\) and \(^{}\) be the corresponding randomized sign vectors output by Algorithm 3. Denote \(S=\{i:s_{j} s^{}_{j}\}\) and \(S^{c}=[k] S\). For any vector \(y\{-1,+1\}^{k}\), define \(S_{0}=\{j S:s_{j}=y_{j}\}\), \(S_{1}=\{j S:s_{j} y_{j}\}\), \(S_{0}^{c}=\{j S^{c}:s_{j}=y_{j}\}\) and \(S_{1}^{c}=\{j S^{c}:s_{j} y_{j}\}\). By Proposition 4.3, we know that the event \(\{|S| N_{+}(m,,k,p)\}\) happens with probability at least \(1-\). In this event, we have

\[=y)}{Pr(^{}=y)} =^{c}}}+1}{e^ {^{}}+1}_{j S_{1}^{c}}}+1} _{j S_{0}}}}{e^{^{}}+1}_{j  S_{1}^{c}}}+1}}{_{j S_{1}^{c}}}+1}_{j S_{0}}}+1} _{j S_{1}}}+1}_{j S_{1}}}}{e^{^{}}+1}}\] \[}}{e^{ ^{}}+1}}{_{j S}}+1}}=|S| ^{} N_{+}^{}=.\]

Since this event occurs with probability at least \(1-\), the overall procedure is \((,)\)-DP. 

#### 4.1.1 The flipping probability and calculation of \(N_{+}\)

In the proof of Theorem 4.1, \(N_{+}\) is a upper bound on \(S=\{i:s_{j} s^{}_{j}\}\) where \(s=sign(W^{T}u)\) and \(s^{}=sign(W^{T}u^{})\) for neighboring data \((u,u^{})\), which determines the flipping probability thus the utility. Next, we analyze \(N_{+}\). The following is a useful lemma; all proofs are placed in Appendix E.

**Lemma 4.2**.: _Let \(X_{1},...,X_{p}\) be iid \(N(0,_{x}^{2})\) variables. Let \(Y N(0,_{y}^{2})\) be another Gaussian random variable with arbitrary dependence structure with \(X_{i}\)'s. Let \(r=_{x}/_{y} 1\). Then_

\[P_{+}(r,p):=Pr(_{i=1,...,p}|X|>|Y|)_{0}^{}2p[2(t)-1]^{p-1 }[2(rt)-1](t)dt,\] (5)

_where \((x)\) and \((x)\) are the standard Gaussian pdf and cdf, respectively._Equipped with Lemma 4.2, we can derive \(N_{+}\) in Algorithm 3, an upper bound on the number of projected signs that are possible to change when \(u\) is replaced by any neighboring data vector \(u^{}\).

**Proposition 4.3** (Bound \(N_{+}\)).: _Suppose \(u[-1,1]^{p}\) and \(\|u\|\). Denote \(r=\) and \(F_{\|u\|,p}=P_{+}(,p)\) in (5). Denote \(s=sign(W^{T}u)\) and \(s^{}=sign(W^{T}u^{})\) for \(\)-neighboring \((u,u^{})\), and \(S=\{i:s_{j} s^{}_{j}\}\). Then with probability \(1-\), \(|S| N_{+}(\|u\|,,k,p)=B^{-1}(;F_{\|u\|,p},k)\), where \(B^{-1}(;F_{\|u\|,p},k)\) represents the inverse cdf of Binomial distribution with rate \(F_{\|u\|,p}\) and \(k\) trials._

Since \(F_{\|u\|,p}=P_{+}(r,p)\) is an increasing function in \(r=/\|u\|\), \(N_{+}\) would be smaller if \(/\|u\|\) is smaller. Therefore, in DP-SignRP-RR, the probability of flipping the true SignRP, \(}+1}\), would be smaller when the data norm (or, its lower bound \(m\)) is large compared with \(\).

#### 4.1.2 Utility in angle estimation by DP-SignRP-RR

Define the DP-SignRP-RR estimator of the angle between two data points \(u\) and \(v\) as

\[_{RR}=(1-_{RR}),_{RR}=}+1)^{2}}{(e^{^{}}-1)^{2}}_{j=1 }^{k}\{_{1j}=_{2j}\}- }}{(e^{^{}}-1)^{2}}.\] (6)

**Theorem 4.4**.: _Let \(=(u,v)\) and \(=^{-1}()\). Run Algorithm 3 with \(N_{+}\) given in Proposition 4.3 and define the DP-SignRP-RR angle estimator by (6). We have \([_{RR}]=\). As \(k\), \(_{RR} N(,}{k})\), with_

\[V_{RR}=(-)+e^{/N_{+}}}{(e^{/N_{+ }}-1)^{2}}+e^{2/N_{+}}}{(e^{/N_{+}}-1)^{4}}.\]

Theorem 4.4 says that \(_{RR}\) is an unbiased estimator of \(\) and asymptotically normal. Compared with \(\), the variance of the cosine estimator from non-DP SignRP [41; 13], we see that \(_{RR}\) incurs an extra variance (i.e., utility loss) of \(}{k}[}}{(e^{/N_{+}}-1)^{2} }+}}{(e^{/N_{+}}-1)^{4}}]\). This quantity increases as \(\) gets smaller, illustrating the utility-privacy trade-off of DP-SignRP-RR.

**Optimal projection dimension \(k^{*}\)**. Since by Proposition 4.3 we know that \(N_{+} F_{\|u\|,p}k\) roughly, the term \([}}{(e^{/N_{+}}-1)^{2}}+}}{(e^{/N_{+}}-1)^{4}}]\) would increase with \(k\). Therefore, there exists an optimal \(k^{*}\) that minimizes the estimation variance. When \(k\) is sufficiently large, we have

\[}{k}+F_{\|u\|,p} ^{2}k}{^{2}}+F_{\|u\|,p}^{4}k^{3}}{^{4}},\]

using the approximation that \(e^{x}-1 x\) when \(x\) is small. To find the optimal \(k\) minimizing this expression, we compute the derivative and set it as zero:

\[-}+F_{\|u\|,p}^{2}}{^{2}} +F_{\|u\|,p}^{4}k^{2}}{^{4}}=0\ \ k^{*}}.\]

This analysis suggests the optimal \(k^{*}\) of DP-SignRP-RR is larger when: (1) \(\) is large; (2) \(F_{\|u\|,p}\) is small, which is typically true when the norm of the data is relatively large.

### Smooth Flipping Probability: the Benefit of Robustness

Next, we improve DP-SignRP-RR based on a novel tool which we call _"smooth flipping probability"_. The idea is inspired by the "smooth sensitivity" for DP noise addition , which essentially says that, the farther \(u\) is from a high "local sensitivity" region, the less noise is needed for \(u\) to achieve DP. The local sensitivity at a specific data point \(u\) is defined as \(LS(u)=_{u^{} Nb(u)}\|f(u)-f(u^{})\|_{2}\), which differs from Definition 2.3 in that local sensitivity only considers the local neighbors of \(u\).

What is the "local perturbation" for flipping the SignRP? Consider one projection \(x_{j}=W[:,j]^{T}u\) and \(s_{j}=sign(x_{j})\). For a neighbor \(u^{}\) of \(u\), denote \(x^{}_{j}=W[:,j]^{T}u^{}\) and \(s^{}_{j}=sign(x^{}_{j})\). By \(\)-adjacency, \(s^{}_{j}\) is possible to be different from \(s_{j}\) only if \(|x_{j}|<_{i=1,,p}|W_{ij}|\). This implies:* When \(|x_{j}|<_{i=1,...,p}|W_{ij}|\), the "local sign flipping" is required, since \(s^{}_{j}\) may differ from \(s_{j}\). We can apply the standard RR: keep \(s_{j}\) with prob. \(}{e^{c/k}+1}\) and flip otherwise.
* When \(|x_{j}|_{i=1,...,p}|W_{ij}|\), there does not exist \(u^{} Nb(u)\) such that \(s^{}_{j} s_{j}\). Thus, the "local sign flipping" is not needed. However, if we do not perturb \(s_{j}\), DP would be violated since \(s^{}_{j}\) might need random flipping. Instead, we use a _"smooth flipping probability"_ which applies less perturbation for points far away from the "high perturbation region", i.e., the regime where \(|x_{j}|<_{i=1,...,p}|W_{ij}|\). Specifically, we define \(L_{j}=|}{_{i=1,...,p}|W_{ij}|}\). The probability of keeping the sign \(s_{j}\) is \(P_{j}^{(u)}=_{j}}}{e^{s^{}_{j}}+1}\) where \(e^{}_{j}=}{k}\). It can be shown (e.g., in the proof of Theorem 4.5) that this flipping probability is "smooth": for \( u,u^{}\) that are neighbors, \(P_{j}^{(u)} e^{/k}P_{j}^{(u^{})}\), which justifies its name.

```
1Input: Data \(u[-1,1]^{p}\); \(>0\); Number of projections \(k\)
2Output: Differentially private sign random projections
3Apply RP by \(x=W^{T}u\), where \(W^{p k}\) is a random \(N(0,1)\) matrix
4Compute \(L_{j}=|}{_{i=1,...,p}|W_{ij}|}\) for \(j=1,...,k\)
5Compute \(_{j}=sign(x_{j}),&_{j}}}{e^{s^{}_{j}}+1}j=1,...,k^{}_{j}= }{k}\\ -sign(x_{j}),&_{j}}+1}\) Return \(\) as the DP-SignRP of \(u\) ```

**Algorithm 4**DP-SignRP-RR-smooth using smooth flipping probability

The concrete steps of DP-SignRP-RR-smooth are summarized in Algorithm 4. Note that, the above two cases can be unified into one, since \(}{e^{/k}+1}\) is the smooth flipping probability with \(L_{j}=1\). In Figure 1, we provide an illustrative example of the smooth flipping probability (red) and the "local flipping probability" (green) discussed above. The global upper bound (black dash) corresponds to the standard randomized response. The local flipping probability is only non-zero at \(L=1\). The proposed smooth flipping probability is non-zero for all \(L>0\), but keeps shrinking exponentially as \(L\) gets larger, i.e., as the projected value is farther from \(0\).

**Theorem 4.5**.: _Algorithm 4 achieves \(\)-DP._

**Comparison with DP-SignRP-RR (Algorithm 3).** In terms of algorithm design and privacy, DP-SignRP-RR-smooth (Algorithm 4) has two advantages over the standard DP-SignRP-RR method:

1. DP-SignRP-RR-smooth does not require (assume) a lower bound \(m\) on the data norms;
2. DP-SignRP-RR-smooth achieves \(\)-DP, while DP-SignRP-RR only guarantee \((,)\)-DP.

Note that Algorithm 3 can also achieve \(\)-DP if we set the flipping probability to be \(+1}\) (for all projected values). This corresponds to the global upper bound (black dotted line) in Figure 1. Thus, DP-SignRP-RR-smooth requires much smaller flipping probability than DP-SignRP-RR.

**DP-SignRP with Rademacher projection.** Similar to DP-RP, the flipping probabilities also depend on the projection matrix \(W\) through \(N_{+}\) (for DP-SignRP-RR) and \(L\) (for DP-SignRP-RR-smooth). In Appendix B.2, we analytically show that Rademacher RP requires smaller flipping probability than that of Gaussian RP for both variants.

Figure 1: The smooth flipping prob., local flipping prob., and the global upper bound. \(=1\), \(k=1\). \(L\) is from Algorithm 4.

### DP-SignOPORP with Smooth Flipping Probability

We incorporate the smooth flipping technique to develop DP-SignOPORP, as given in Algorithm 5 with two variants. DP-SignOPORP-RR is based on the standard randomized response (RR), while DP-SignOPORP-RR-smooth is an improved version with our proposed smooth flipping probability.

```
1Input: Data \(u[-1,1]^{p}\); \(>0\); Number of projections \(k\)
2Output: Differentially private sign OPORP
3 Apply Algorithm 2 with a random Rademacher projection vector to get the OPORP \(x\) DP-SignOPORP-RR:
4 Compute \(_{j}=sign(x_{j}),&}{e^{ }+1}\\ -sign(x_{j}),&+1}\) for \(j=1,...,k\) DP-SignOPORP-RR-smooth:
5 Compute \(L_{j}=|}{}\) for \(j=1,...,k\)
6 Compute \(_{j}=sign(x_{j}),&_{j}}}{e^{ ^{}_{j}}+1}\\ -sign(x_{j}),&_{j}}+1} \) for \(j=1,...,k\), with \(^{}_{j}=L_{j}\)
7 For \(_{j}=0\), assign a random coin in \(\{-1,1\}\) Return \(\) as the DP-SignRP of \(u\) ```

**Algorithm 5**DP-SignOPORP-RR and DP-SignOPORP-RR-smooth

**Benefit of binning.** In Algorithm 5, the key difference compared with DP-SignRP methods is that, the \(}\) (in Algorithm 3) or \(\) (in Algorithm 4) is removed from the flipping probability formulas, which is a significant advantage in privacy. This improvement is a result of the binning step. By Definition 2.2, two neighboring data \(u^{}\) and \(u\) differ in one coordinate. For SignRP, since each projected data is an aggregation of the whole data vector, when we switch from \(u\) to \(u^{}\), (in principle) all \(k\) projections are possible to change. In contrast, in OPORP, since each data entry appears in only one bin, \(u^{}\) will only cause exactly one projected output to change, which enhances privacy.

**Theorem 4.6**.: _Both variants in Algorithm 5 are \(\)-DP._

Proof.: The proof follows the idea of the proofs of DP-SignRP methods, but we need to additionally consider the empty bins. Denote \(x\) and \(x^{}\) as the OPORP from Algorithm 2 using a same random vector \(w\), and let \(s=sign(x)\), \(s^{}=sign(x^{})\). Suppose \(u\) and \(u^{}\) differ in dimension \(i\), and dimension \(i\) is assigned to the \(j^{*}\)-th bin with \(j^{*}=(i)/(p/k)\), where \(:[p][p]\) is the permutation in OPORP. For any \(y\{-1,1\}^{k}\), it is easy to see that to compute \(=y)}{Pr(^{}=y)}\), it suffices to look at the \(j^{*}\)-th output sample because other probabilities cancel out. For the \(j^{*}\)-th sample, when \(s_{j^{*}} 0\) and \(s^{}_{j^{*}} 0\), by the same arguments as in the proof of Theorem 4.5, we know that \(e^{-}})=a}{Pr(s^{}_{j^{*}})=a} e^{}\), \(a\{-1,1\}\), for both variants (DP-SignOPORP-RR and DP-SignOPORP-RR-smooth). When one of \(s_{j^{*}}\) and \(s^{}_{j^{*}}\) equals \(0\), it is also easy to see that (assume \(s_{j^{*}}=1\) and \(s^{}_{j^{*}}\) is a random coin)

\[1})=1}{Pr(s^{}_{j^{*}})=1}=}{e^ {}+1} e^{}, e^{-}})=-1}{ Pr(s^{}_{j^{*}})=-1}=+1} 1.\]

This holds for both variants. In particular, this case corresponds to \(L_{j^{*}}=1\) for the smooth flipping probability. This proves the theorem. 

**Multiple repetitions.** In Algorithm 5 Line 7, if OPORP generates an empty bin (i.e., \(x_{j}=0\)), we must assign a random sign to it in order to maintain DP. This tends to undermine the utility since a random coin does not provide any useful information about the data. To avoid too many empty bins, one simple way is to repeat the DP-SignOPORP (with smaller \(k\)) for \(t>1\) times, and concatenate the output vectors . For example, if the target \(k=256\), we may run Algorithm 5 for \(t=4\) times, each time with \(256/4=64\) projected values and privacy budget \(/4\). We still obtain a length-256 bit vector with \(\)-DP by the composition theorem. In the experiments, we will see that this strategy may improve the overall performance of DP-SignOPORP.

Experiments

We present experiments on retrieval and classification tasks to demonstrate the effectiveness of our proposed methods. We compare the following algorithms:

* Raw-data-G-OPT: adding optimal Gaussian noise with sensitivity \(\) to the original data, which is the most basic DP method for privatizing the original data.
* DP-RP-G (Algorithm 1 + Theorem 3.1): Gaussian DP-RP with the Gaussian mechanism.
* DP-RP-G-OPT (Algorithm 1 + Theorem 3.2): DP-RP with Gaussian random projection matrix and the optimal Gaussian noise.
* DP-OPORP (Theorem 3.3): one permutation + one random projection with Rademacher projection vector and optimal Gaussian noise.
* DP-SignOPROP-RR and DP-SignOPORP-RR-smooth (Algorithm 5): signed OPORP with standard randomized response and with smooth flipping probability, respectively.

As noted in Section 4.3, due to binning, DP-SignOPORP is substantially better than DP-SignRP. Thus we present results for DP-SignOPORP in our experiments (and only \(=1\) for conciseness).

**Similarity Search.** We first test the methods in similarity search problems, using two standard image retrieval datasets, MNIST  and CIFAR . For both datasets, we treat the training set as the database, and use the samples from the test set as the queries. We set the ground truth neighbors for each query as the top-50 sample vectors with the highest cosine similarity to the query. To search with DP-RP (and DP-OPORP), we estimate the cosine between a query and a sample point by the cosine between their corresponding output noisy projected values. We report only the precisions (and please see Appendix for other metrics), averaged over 10 repetitions.

Figure 2 shows that, as a result of the optimal Gaussian mechanism, DP-RP-G-OPT substantially outperforms the prior method in the literature, DP-RP-G, at all \(\). DP-OPORP achieve better performance than DP-RP-G-OPT and is recommended as the best (full-precision) DP-RP variant. These two methods provide much higher precision and recall than the optimal Gaussian mechanism applied to the original data (Raw-data-G-OPT).

In Figure 3, we plot the precision and recall of DP-SignOPORP-RR (with standard randomized response), and DP-SignOPORP-RR-s (with smooth flipping probability). We present the results with repetition \(t=2,4\) which yield good overall performance. As we can see, (i) the proposed smooth flipping probability considerably improves the standard randomized response technique, and (ii)

Figure 2: Retrieval recall and precision on MNIST and CIFAR, \(=1\), \(=10^{-6}\).

DP-SignOPORP in general provides better search accuracy than DP-OPORP when \(<10 15\). This range of \(\) is common in the use cases of DP for providing reasonable privacy protection. Also, we note that smaller \(t\) (repetitions) typically performs better with smaller \(\) but worse when \(\) is large.

**Classification.** We evaluate the performance of DP-SignOPORP in classification problems trained by SVM , on the WEBSPAM dataset . We apply the "max normalization", i.e., divide each data column by its largest magnitude such that the data entries are bounded in \([-1,1]\). The SVM results are presented in Figure 4. For this task, we plot DP-SignOPORP with \(t=1\). We again observe the advantage of DP-SignOPORP over DP-OPORP, and the advantage of the proposed smooth flipping probability over the standard randomized response. Specifically, when \( 5\), the test accuracy of Raw-data-G-OPT is around 60% (for WEBSPAM, this is almost the same as random guessing), but DP-SignOPORP can achieve 95% classification accuracy with \(k=1024\).

## 6 Conclusion

For a database \(U^{n p}\) consisting of \(n\) data points, our goal is to make its random projections (RP) and SignRP differentially private--the DP guarantee we have derived holds independently and simultaneously for every data vector \(u U\). There are works on randomized algorithms that assume the randomness of the algorithm are "internal" and also kept private [7; 73; 25]. We assume the projection matrix \(W\) is known/public, which is stronger in privacy and enables broader applications of the released data. Also, a "known projection matrix" is typical in the local DP setting [47; 18], since the projection matrix is created and shared among the users by the data aggregator.

In our study, we first revisit the prior work  on the \((,)\)-DP random projection based on Gaussian noise addition ("DP-RP-G"). Then we incorporate the optimal Gaussian mechanism and propose an improved method DP-RP-G-OPT. We also develop DP-OPORP based on the optimal Gaussian mechanism. We then propose algorithms for privatizing the (1-bit) DP-SignRP: "DP-SignRP-RR" based on the standard "randomized response" (RR), and "DP-SignRP-RR-smooth" based on a proposed concept of "smooth flipping probability" of SignRP. We extend the idea of smooth flipping probability to OPORP (a variant of count-sketch) and demonstrate the advantage of DP-SignOPORP.

Experiments on retrieval and classification tasks justify the effectiveness of our proposed methods. In particular, DP-SignOPORP outperforms other methods especially when \(\) is not large (e.g., \( 5\)). These results also verify the advantage of using the smooth flipping probability.

Figure 4: SVM test accuracy on WEBSPAM. For \((,)\)-DP methods, \(=10^{-6}\).

Figure 3: Retrieval on MNIST with DP-SignOPORP-RR and DP-SignOPORP-RR-smooth. For DP-OPORP, DP-SignOPORP-RR, and Raw-data-G-OPT, \(=10^{-6}\).