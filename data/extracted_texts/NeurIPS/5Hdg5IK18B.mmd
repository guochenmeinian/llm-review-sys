# MG-Net: Learn to Customize QAOA with Circuit Depth Awareness

Yang Qian\({}^{1,}\) Xinbiao Wang\({}^{2,}\) Yuxuan Du\({}^{3,@sectionsign}\) Yong Luo\({}^{2,@paragraphsign}\) Dacheng Tao\({}^{3,}\)

\({}^{1}\)School of Computer Science, Faculty of Engineering, University of Sydney

New South Wales 2008, Australia

\({}^{2}\)Institute of Artificial Intelligence, School of Computer Science, Wuhan University

Wuhan, China

\({}^{3}\)College of Computing and Data Science, Nanyang Technological University

Singapore 639798, Singapore

\({}^{}\)qianyang1217@gmail.com \({}^{}\)cyriewang@gmail.com \({}^{@sectionsign}\)duyuxuan123@gmail.com \({}^{@paragraphsign}\)luoyong@whu.edu.cn \({}^{}\)dacheng.tao@ntu.edu.sg

Corresponding authors

###### Abstract

Quantum Approximate Optimization Algorithm (QAOA) and its variants exhibit immense potential in tackling combinatorial optimization challenges. However, their practical realization confronts a dilemma: the requisite circuit depth for satisfactory performance is problem-specific and often exceeds the maximum capability of current quantum devices. To address this dilemma, here we first analyze the convergence behavior of QAOA, uncovering the origins of this dilemma and elucidating the intricate relationship between the employed mixer Hamiltonian, the specific problem at hand, and the permissible maximum circuit depth. Harnessing this understanding, we introduce the Mixer Generator Network (MG-Net), a unified deep learning framework adept at dynamically formulating optimal mixer Hamiltonians tailored to distinct tasks and circuit depths. Systematic simulations, encompassing Ising models and weighted Max-Cut instances with up to 64 qubits, substantiate our theoretical findings, highlighting MG-Net's superior performance in terms of both approximation ratio and efficiency.

## 1 Introduction

Combinatorial optimization problems (COPs) , central to numerous scientific and engineering disciplines , often defy efficient classical solutions due to their computational complexity . A promising strategy to overcome these computational challenges involves harnessing the power of quantum computing, as these COPs can be mapped to Ising Hamiltonians whose ground states denote optimal solutions . Leveraging this quantum representation, the Quantum Approximate Optimization Algorithm (QAOA)  has emerged to address these COPs. In particular, theoretical analyses  underscore the potential of QAOA, suggesting its superiority over classical counterparts in certain contexts, particularly with unlimited infinite circuit depth. Meantime, empirical studies  affirm its applicability across a diverse spectrum of problems and devices.

Despite these advancements, QAOA's practical efficacy is challenged by the quantum coherence limits of modern quantum devices, as there is a ceiling on the allowable maximum circuit depth \(p\). As a result, standard QAOA often underperforms classical counterparts . This motivates a research shift towards redesigning the _mixer Hamiltonian_\(H_{M}\), a key component of QAOA. As illustrated in Fig. 1(a), supported by the results of quantum adiabatic evolution , alternative\(H_{M}\) may exist that guide the system along a more direct and efficient trajectory--a shortcut--to the solution state, leading to a better performance compared to the standard QAOA. Besides, as shown in Fig. 1(b), empirical evidence indicates that the form of \(H_{M}\) promising a good performance is varied with the allowable \(p\). As such, diverse alternatives \(H_{M}\) are proposed in past years, drawing upon concepts from quantum annealing , incorporating additional trainable parameters  or exploiting permutation symmetry . However, these approaches require deep domain expertise and often lack generalizability across different tasks and circuit configurations \(p\).

In response to these challenges, here we first analyze the convergence of QAOA on various mixer Hamiltonian configurations and circuit depths with the tool of representation theory . Our finding reveals that (i) the convergence of QAOA can be enhanced through parameter grouping in the mixer Hamiltonian; (ii) the specific strategy for parameter grouping is dependent on the particular problem and the value of \(p\). These two findings are instrumental in understanding the interplay between \(p\), parameter grouping, and the overall efficiency of the QAOA, providing valuable insights for the design of the mixer Hamiltonian.

Envisioned by the achieved theoretical results, we propose an end-to-end learning framework, termed **M**ixer **G**enerator **N**etwork (MG-Net), to dynamically design the mixer Hamiltonian \(H_{M}\) for a class of problems and distinct circuit depth constraints. Conceptually, MG-Net takes the problem's description and the available circuit depth \(p\) as input and directly outputs the optimal mixer Hamiltonian for a \(p\)-QAOA. There are three distinguished features of our proposal: (i) The ability to dynamically adjust \(H_{M}\) according to \(p\), enhancing its _compatibility_ with practical quantum devices; (ii) Fast customization of \(H_{M}\) for _unseen problems_ and circuit depth \(p\), attributed to the multi-condition controlled generative network architecture; (iii) Circumvent the need for the expensive collection of a vast training dataset of optimal \(H_{M}\) by employing an estimator-generator structure alongside a two-stage training approach. Note that the developed techniques can be flexibly extended to other variational quantum algorithms (VQAs) [25; 26], which may be independent of interests.

The contributions of this paper are:

\(\) We provide a rigorous theoretical analysis on the convergence of QAOA with sufficient circuit depth, elucidating **the link between the performance and the parameter grouping in QAOA circuits**. This analysis offers guidance on the design of mixer Hamiltonian to achieve a high approximation ratio for a specified circuit depth.

\(\) We propose MG-Net, which dynamically tailors its predicted mixer Hamiltonian \(H_{M}\) to suit the given problem and circuit depth. Our model **greatly reduces the cost of collecting labeled training data**, attributed to an estimator-generator framework and a two-stage training strategy.

\(\) The proposed MG-Net demonstrates remarkable **generalization ability** from a limited dataset to a broad spectrum of combinatorial problems, which facilitates rapid and efficient creation of \(H_{M}\) for unseen problems, advancing the **practical utility** of QAOAs.

\(\) Extensive experiments on the Transverse-field Ising model and Max-Cut up to \(64\) qubits **verify our theoretical discoveries** and **demonstrate the advantage of MG-Net in achieving higher approximation ratios at various circuit depths** compared to other quantum and traditional methods. The code is released at https://github.com/QQQYang/MG-Net.

Figure 1: **Mixer Hamiltonian affects the performance of QAOA**. (a) The optimization trajectories of QAOA with varied mixer Hamiltonians \(H_{M}\). Given a fixed circuit depth \(p\), a tailored \(H_{M}\) (highlighted in pink) can more effectively steer the quantum state towards the exact solution compared to the original \(H_{M}\) used in QAOA. (b) Transition of the effective dimension \(d_{eff}\) required in QAOA with increasing \(p\). ‘ma-QAOA’ denotes a case with independent parameters , contrasted with ‘QAOA’ where parameters are fully correlated. The orange line denotes the average effective dimension over all samples.

## 2 Background

### Quantum approximation optimization algorithm

Considering a COP defined on a set of \(N\) binary variables \(=z_{1} z_{N}\), where \(z_{i}\{ 1\}\), our objective is to identify a bit string \(\) that maximizes a specific objective function \(C():\{ 1\}^{N}_{ 0}\). Intuitively, the solution space grows exponentially with \(N\), rendering the exact solution to many COPs intractable . In practice, an alternative approximation algorithm is selected to seek an approximate solution \(\) to achieve a high approximation ratio \(r=C()/C_{}\), where \(C_{}=_{}C()\).

In response to this inherent complexity, Quantum Approximate Optimization Algorithm (QAOA)  is proposed. In this framework, the bit string \(\) is encoded into a quantum state \(}= x_{N}}\) with \(x_{i}=(1-z_{i})/2\), and the objective function \(C()\) is encoded into the problem Hamiltonian \(H_{C}^{2^{N} 2^{N}}\) so that \(H_{C}}=C()}\). Refer to Appendix A for the omitted details.

QAOA is a hybrid quantum-classical algorithm that combines a parameterized quantum circuit (PQC) for state evolution and a classical optimizer for parameter updates. For a \(p\)-layer QAOA circuit shown in Fig. 1(a), the quantum state \(}\) is prepared by alternately applying the problem Hamiltonian \(H_{C}\) and the mixer Hamiltonian \(H_{M}=_{i=1}^{N}X_{i}\) on the initial state \(}\), formulated as

\[(,)}=_{k=1}^{p}e^{-i_{k}H_{M}}e^{ -i_{k}H_{C}}},\] (1)

where \(=(_{1},...,_{p})\) and \(=(_{1},...,_{p})\) are \(2p\) trainable parameters. These parameters are optimized to maximize the expectation value of the problem Hamiltonian \(H_{C}\):

\[(^{*},^{*})=_{,}F_{p}(,),\] (2)

where \(F_{p}(,)=(,)}\!H_{C} (,)}\) can be estimated by multiple measurements on the quantum system. As \(F_{p}(^{*},^{*})\) approaches the optimal value \(C_{}\) of the objective function, we can obtain the approximate solution to the combinatorial optimization problem with high probability by measuring the state \((^{*},^{*})}\) in the computational basis. A metric for assessing the performance of QAOA is the approximation ratio \(r=F_{p}(^{*},^{*})/C_{}\).

### Symmetry in QAOA

**Symmetry, ansatz design, and effective dimension**. A symmetry \(S\) refers to the unitary operator leaving the operator \(H\) invariant such that \(S^{}HS=H\) (or \([S,H]=0\)). All symmetries form a group \(\) where given any two symmetries \(S_{1},S_{2}\), the compositions \(S_{1}S_{2}\) and \(S_{2}S_{1}\) are also symmetries in \(\). Among various symmetries, the most relevant one to our work is the permutation symmetry \(_{N}\), with the subscript being the qubit count \(N\) and \(_{N}\) being the symmetric group. For example, a permutation \(\) with \((1)=3,(2)=1,(3)=2\) acting on the state \(}}}\) yields \(}}}=}}}\). Throughout the whole study, we denote the group of permutation symmetries of the problem Hamiltonian \(H_{C}\) as \((H_{C})=\{_{N}^{}H_{C}=H_{C}\}\).

Consider an \(N\)-qubit PQC \(U()=_{j=1}^{p}_{k=1}^{K}e^{-iH_{k}_{jk}}\) with \(\) and \(d=2^{N}\). We call \(U()\) a symmetric PQC with respect to the problem Hamiltonian \(H_{C}\) if there exists a symmetry group \(\) of \(H_{C}\) such that \([U(),S]=0\) for any \(\) and \(S\). This symmetry is determined by the generators of PQCs \(=\{H_{1},,H_{K}\}\) which is also called _ansatz design_, as \([U(),S]=0\) holds for any \(\) if and only if \([H_{k},U()]=0\) for any \(k[K]\). Such symmetry can be quantified by the _effective dimension_[27; 28].

**Definition 2.1** (Effective dimension).: Consider an \(N\)-qubit QAOA instance \((},U(),H_{C})\) where \(U()\) acts on the vector space \(V\). If there exists a direct sum decomposition \(V=_{j=1}^{k}V_{j}\) and \(V^{*}\{V_{j}\}_{j=1}^{k}\) such that \(U()} V^{*}\) for any \(\) and the ground state of the problem Hamiltonian \(}\) satisfies \(} V^{*}\), then the effective dimension \(d_{} 2^{N}\) is defined as the dimension of \(V^{*}\).

Experimental and theoretical analysis has shown that symmetric ansatz design with a small effective dimension contributes to better trainability [29; 28; 30].

**Symmetry and ansatz designs in QAOA**. The PQC in Eqn. (1), adopted in the original QAOA, fully groups (FG) the trainable parameters and has the ansatz design \(_{FG}=\{H_{M},H_{C}\}\), which is symmetric with respect to \(H_{C}\) under the _permutation symmetry_. This is because its mixer Hamiltonian \(H_{M}=_{i=1}^{N}X_{i}\) is invariant under an arbitrary permutation operator.

However, \(_{FG}\) fails to employ the specific symmetry group of \(H_{C}\). This issue can be addressed by partially grouping (PG) trainable parameters in QAOA. For example, denote \(H_{C}=_{ i_{k},j_{k}}Z_{i_{k}}Z_{j_{k}}\) with \(Z_{j_{k}}\) being the Pauli-Z operator acting on the \(j_{k}\)-th qubit. An alternative symmetric ansatz design is \(_{PG}=\{H_{_{1}},,H_{_{|_{1 }}},H_{_{1}^{*}},,H_{_{|_{| _{1}^{*}}|}}\}\) where \(H_{_{k}}=_{i_{k}}X_{i}\) and \(H_{_{k}^{*}}=_{ i,j_{k}^{*}}Z_{i}Z _{j}\) refer to the generators respecting the permutation symmetry of \(H_{C}\) satisfying \(H_{M}=_{j=1}^{||}H_{_{j}}\) and \(H_{C}=_{j=1}^{|^{*}|}H_{_{j}^{*}}\). The ansatz design \(_{PG}\) enables more free parameters than than the ansatz design \(_{FG}\) in each layer, and has been empirically shown with a faster convergence rate than \(_{FG}\) given the same number of layers.

When \(H_{C}\) is asymmetric, another typical ansatz design in QAOA is \(_{NG}=\{Z_{i_{1}}Z_{j_{1}},,Z_{i_{k}}Z_{j_{k}},X_{1},,X _{N}\}\), where the parameters of all parameterized gates are independent and non-grouping (NG). Notably, the PQCs related to various ansatz design \(_{FG},_{PG},_{NG}\) employ the same parameterized gates but with different _parameter grouping strategies_, where \((,)\) in each layer can be fully grouped, partially grouped, and non-grouped .

## 3 Convergence theory of QAOA

In this section, we theoretically illustrate how employing appropriate parameter grouping corresponds to better convergence performance. Similar to Refs.  and , our derivations are based on the observation that the exploited PQC with highly-symmetric ansatz structure generally enables a faster convergence rate.

**Theorem 3.1** (Convergence).: _Consider a QAOA instance denoted as \((|_{0}\,U(),H_{C})\) with \(U()\) determined by the related ansatz design. Let \(_{FG},_{PG},_{NG}\) be the ansatz designs of the circuits with parameters fully grouped, partially grouped, and no-grouped. Their effective dimension yields_

\[d_{}(_{FG})=d_{}(_{PG}) d_{ }(_{NG}),\] (3)

_where the equality in the inequality holds if there is no spatial symmetry in \(H_{C}\). Besides, there exists a \(d_{}\)-dependent threshold \(C\) so that circuit depth \(p>C\), the iterations \(T\) required to achieve the same approximation ratio yield_

\[T_{PG}=T_{FG} T_{NG}.\] (4)

The proof of Theorem 3.1 and more elaborations are presented in Appendix B. The achieved results, combined with the over-parameterization theory of PQCs , deliver the following two implications. First, when the circuit depth \(p>C\) is sufficiently large such that all PQCs with various ansatz designs reach the _over-parameterization regime_, performing the parameter grouping can effectively decrease the effective dimension \(d_{}\) compared with the PQCs with no-parameter grouping, leading to a faster convergence rate. Second, the over-parameterization of QAOA occurs when the number of trainable parameters exceeds a critical point that is proportionally related to \(d_{}\).

The above two implications indicate the selection of \(_{FG}\), \(_{PG}\), or \(_{NG}\) is complicated and is both _depth- and problem-dependent_. In particular, given a specified \(p\), adopting a parameter grouping strategy can simultaneously reduce the number of parameters and the effective dimension, making it difficult to determine whether the QAOA reaches the over-parameterization regime. For instance, in a scenario such that the parameter grouping strategy drastically reduces the number of parameters but only slightly reduces the effective dimension, an over-parameterized QAOA could transform to an under-parameterized QAOA, leading to a degraded convergence as the optimization can be easily stuck in bad local minimal [31; 32].

## 4 MG-Net

The implication of Theorem 3.1 inspires us to devise a method for dynamically generating an appropriate mixer Hamiltonian \(H_{M}\) tailored to both the problem \(G\) at hand and the specified circuit depth \(p\). For this purpose, we harness the power of deep learning and devise an end-to-end learning framework, dubbed Mixer Generator Network (MG-Net).

### Framework of MG-Net

Before presenting the proposed MG-Net, let us first formalize the learning problem towards designing the mixer Hamiltonian \(H_{M}\). To incorporate different Pauli operators and parameter grouping strategies, we extend the definition of an \(N\)-qubit mixer Hamiltonian \(H_{M}\) in Eqn. (1) to a more generalized form, supporting flexible operators and parameter correlations by substituting the Pauli-X operator with a selection of general Pauli operator and stratifying the \(N\) operators into \(K\) groups. Mathematically, the refined mixer Hamiltonian yields

\[H_{M}=_{j=1}^{K}_{j}_{i_{j}}P_{i},\] (5)

where \(_{j}\) refers to the trainable parameter controlling the \(j\)-th group of operators, \(_{j}\) contains the indices of operators belonging to the \(j\)-th group such that \(_{j=1}^{K}_{j}=[N]\) and \(_{i}_{j}=\) for \( i j\), and \(P_{i}\) refers to a Pauli operator. This work focuses on \(P_{i} X,Y\) as the types of candidate operators, rather than using only the single Pauli-X operator. Previous studies have demonstrated the effectiveness of using Pauli-Y operators in mixer Hamiltonians [33; 34], as summarized in Tab. 1. In this sense, operators in the same group are correlated with each other, sharing the same parameter. In this way, _the design of \(H_{M}\) is decoupled into two distinct tasks: determine the parameter groups \(\{_{j}\}_{j=1}^{K}\); identify the appropriate operator types \(P_{i}\)_. With the reformulation above, the decoupled tasks can be accomplished by learning a mapping rule \(f:(G,p)(,)\) with \(=\{_{j}\}_{j=1}^{K}\) and \(\{X,Y\}^{ N}\) referring to the parameter correlation and mixer Hamiltonian.

Designing a model to learn \(f\) faces _two main challenges_:

**(C-1)** The variety of combinatorial optimization tasks leads to uncertain input formats for the model, which necessitates a universal representation method and retains essential properties of the original

   Works & Mixer Hamiltonian \\  DC-QAOA  & \(\{X,Y,ZY,YZ,XY,YX\}\) \\ ADAPT-QAOA  & \(_{i[N]}\{X_{i},Y_{i}\}(_{i[N]}Y_{i})(_{i[N]}X_{i} )_{i,j[N]}\{B_{i}C_{j}|B_{i},C_{j}\{X,Y,Z\}\}\) \\   

Table 1: **Previous works that have introduced the Pauli-Y operator as a candidate mixer Hamiltonian.**

Figure 2: **Framework of MG-Net.** (a) Training Phase. Initially (left), the cost estimator is trained to precisely predict QAOA performance for specific problem instances, circuit depths, and mixer Hamiltonians. In the subsequent stage (right), with the cost estimator fixed, the mixer generator is trained through unsupervised learning to derive the optimal mixer Hamiltonian that minimizes the cost estimator’s output. (b) Inference Phase. Given a problem \(G\) and circuit depth \(p\), the mixer generator produces a mixer Hamiltonian, subsequently utilized in a QAOA solver to find the solution.

data, such as permutation invariance;

**(C-2)** The exponential growth of the search space for both parameter correlation and operator types, (i.e., scaling at \(O(N^{N})\) and \(O(2^{N})\), respectively), hurdles the design of an effective learning method. For instance, directing training a learning model in the supervised learning paradigm may require computationally unaffordable training examples to ensure good prediction accuracy.

We next present an end-to-end learning framework--**Mixer \(\)**enerator \(\)etwork (MG-Net), as depicted in Fig. 2, to address the above challenges. Particularly, to address **C-1**, we devise a problem encoder which transforms each problem \(G\) into a unified directed acyclic graph \(G_{C}\), ensuring a consistent and effective input format. Coupled with the mixer encoder, it maps both the problem and mixer Hamiltonian to a shared hidden space. To address **C-2**, MG-Net features a unique _estimator-generator_ framework, supplemented by a _two-stage training strategy_. The role of these techniques is summarized below and their implementation details are demonstrated in the subsequent subsections.

**Role of estimator**. Rather than directly seeking the optimal parameter correlation strategy \(^{*}\) and operator type \(^{*}\) for a given \((G,p)\), we devise a _cost estimator_ to map the relationship between \((,)\) and the achievable minimal cost \(F_{p}\) of the corresponding QAOA in Eqn. (2).

**Role of generator**. We devise a _generator_ to predict \((,)\) that minimizes the cost estimator's output. This design requires only the cost of any mixer Hamiltonian as a label, thus avoiding the exhaustive search of optimal pairs \((^{*},^{*})\).

**Two-stage training**. The pipeline is visualized in Fig. 2(a).

\(\)**Stage 1 (Cost Estimator Training)**. This stage, marked in purple, focuses on training the cost estimator using _supervised learning_. Inputs include the problem graph \(G\), potential mixer Hamiltonians \(H_{M}\), and the chosen circuit depth \(p\), with the corresponding cost \(y\) as the target label.

\(\)**Stage 2 (Mixer Generator Training)**. This stage, marked in orange, freezes the cost estimator and only updates the mixer generator to minimize the output of the cost estimator under the _unsupervised learning_ paradigm.

For inference on unknown problem instances (in Fig. 2(b)), MG-Net employs only the mixer generator to predict the optimal mixer Hamiltonian, which is then fed into a QAOA solver to derive the final solution. Distinguished by its ability to generalize effectively across a class of problems from a limited learning set, MG-Net sets itself apart from previous studies. Refer to Appendix. C for discussion.

### Implementation of MG-Net

**Data encoder in MG-Net.** MG-Net exploits three types of data encoder, i.e., the problem encoder, mixer encoder, and depth encoder, which maps the given problem \(G\), the candidate mixer Hamiltonian \(H_{M}\), and the specified depth \(p\) to the same hidden feature space. The construction of these encoders is introduced below and the omitted details are deferred to Appendix D.2.

**Cost estimator in MG-Net (Stage 1).** Recall Stage 1 in Sec. 4.1, the cost estimator takes the encoded problem graph \(G_{C}\), the encoded mixer Hamiltonian \(G_{M}\), and the encoded circuit depth \(_{p}\) as inputs, and outputs the prediction of the achievable minimum loss of the corresponding QAOA. Each input is processed by an independent branch respectively: _the problem graph branch, the mixer Hamiltonian branch, and the circuit depth branch_, as shown in Fig. 3(a). The concatenation of three types of features is subsequently utilized by a multi-layer perceptron (MLP) to output the minimum loss \(\) that the QAOA ansatz can achieve. Refer to Appendix. D.3 for details.

**Mixer generator in MG-Net (Stage 2).** The mixer generator in MG-Net takes \(G_{C}\) and \(_{p}\) as input and outputs a targeted mixer Hamiltonian \(H_{M}\). Specifically, the mixer generation is composed of two separate sub-generators: the operator type generator and the parameter grouping generator defined in Eqn. (5), shown in Fig. 3(b). The operator type generator is responsible for generating operator types \(\), which is conceptualized as a graph node classification task. The parameter grouping generator is responsible for predicting the sets of index groups \(\{_{j}\}_{j=1}^{K}\) with an unspecified \(K\), which is modeled as a link prediction task. Refer to Appendix. D.3 for details.

### Training strategy

The training process of MG-Net is varied for the first and second stages, under supervised and unsupervised learning paradigms, respectively.

First-stage training.This stage involves constructing a labeled dataset \(_{}^{}=\{(G_{C}^{(i)},G_{M}^{(i)},_{p}^{(i)}),y^ {(i)}\}_{i=1}^{S}\), where the \(i\)-th sample consists of a tuple of features (i.e., the problem description \(G_{C}^{(i)}\), the mixer \(G_{M}^{(i)}\), and the circuit depth feature \(_{p}^{(i)}\)), and the label \(y^{(i)}\) representing the minimum cost value achievable by this QAOA instance (i.e., determined by repeatedly executing such a QAOA with varying initial parameters). Once \(_{}^{}\) is ready, the cost estimator is optimized by minimizing the loss function

\[_{}=_{e}_{e}+_{r}_{r},\] (6)

where \(_{e}\) and \(_{r}\) are two hyper-parameters of each loss, \(_{e}=_{i=1}^{S}(y^{(i)}-^{(i)})^{2}\) is the mean square error, and \(_{r}\) is the ranking loss

\[_{r}=-S}_{i,j}^{S}(0,1-(y ^{(i)}-y^{(j)})(^{(i)}-^{(j)})).\]

Second-stage training.This stage involves the training of the mixer generator via unsupervised learning. The loss function of this stage is

\[_{}=_{i=1}^{S}C(G_{C}^{(i)},M(G_{C}^{(i)}, _{p}^{(i)}),_{p}^{(i)}),\] (7)

where \(C()\) and \(M()\) represent the output of the cost estimator and mixer generator, respectively. Note that only the parameters of the mixer generator are updated; the cost estimator parameters remain fixed to ensure consistent evaluation criteria throughout the whole learning process.

## 5 Experiments

We evaluate the performance of MG-Net by two typical applications of QAOA: weighted Max-Cut and Transverse-field Ising model (TFIM), each of which is elucidated below.

Weighted Max-Cut.Denote a weighted graph as \(G=(V,E,W)\), where \(V\) is the set of vertices of graph, \(E\) is the set of graph edges, \(W=\{w_{ij}\}_{(i,j) E}\) is the set of weights assigned to each edge. The problem Hamiltonian for the weighted Max-Cut problem is \(H_{C}^{}=0.5*_{(i,j) E}w_{ij}Z_{i}Z_{j}\), where \(Z_{i}\) is a Pauli-Z operator acting on the \(i\)-th qubit.

TFIM.Our focus is a class of inhomogeneous TFIMs: \(H_{C}^{}=-_{(i,j)}J_{ij}Z_{i}Z_{j}-h_{i}X_{i}\), where \(J_{ij}\) is the interaction strength between neighboring spins (or qubits) \((i,j)\), and \(h\) signifies the strength of a global transverse field applied to each spin. In this model, the interaction strengths \(J_{ij}\) can vary between different pairs of spins, adding a layer of complexity to the system.

Figure 3: **Structure of cost estimator and mixer generator.** (a) Cost estimator. The cost estimator is comprised of three distinct branches, each dedicated to processing different types of data: the original problem, the candidate mixer Hamiltonian, and the circuit depth. Their outputs are then integrated to predict the cost value achievable by the QAOA circuit. (b) Mixer generator. The mixer generation is divided into two distinct parts: operator type generation and parameter grouping generation. The former is executed as a node classification task, while the latter is approached as a link prediction task.

### Experiment configuration

**Dataset construction.**The Max-Cut problem focuses weighted 3-degree regular (w3r) graphs, where the edge weights \(\{w_{ij}\}\) are uniformly sampled from \(\). The TFIM focuses on 1D instances where a qubit \(i[N-1]\) has neighbors \(i 1\). The strength \(J_{ij}\) and \(h\) are uniformly sampled from \([0.5,1.5]\) and \([0.1,2]\) respectively. The training dataset \(_{}^{}\) in Sec. 4.3 contains \(S=100\) instances for both two tasks with size up to \(N=64\) qubits, while The test dataset \(^{}\) contains another \(100\) problem instances which are different from that of \(_{}^{}\). Refer to Appendix D.1 for details.

**Optimization and training of MG-Net**. The cost estimator and mixer generator are trained using an Adam optimizer with a learning rate of \(10^{-4}\), and hyper-parameters \(_{e}=1\) and \(_{r}=1\) in Eqn. (6).

**Optimization of QAOA.** After predicting the problem-hardware-tailored mixer Hamiltonian \(H_{M}\) by the trained mixer generator, a QAOA circuit with the initial state \(|+^{ N}\) and \(H_{M}\) is optimized by an Adam optimizer with a learning rate of \(0.15\). Each setting undergoes \(10\) independent runs with varied random seeds and initial parameters to obtain the statistical results. Refer to Appendix D.4 for detailed discussion about the selection of initial state.

### Results

**Cost estimator acts as an accurate performance indication for QAOA**. The behavior of the cost estimator on the test dataset with varying circuit depths \(p\) and two distinct parameter grouping strategies NG and FG (defined in Theorem 3.1) is recorded in Fig. 4. In Fig. 4(a), we observed a strong correlation between the estimated and minimum cost values, and the correlation strength changes with \(p\) and parameter grouping strategy. Particularly, the cost estimator predicts a high likelihood of finding the most accurate solution for QAOA circuits with FG parameters and a depth of \(p=92\). This prediction aligns with the actual performance of QAOA under these specific conditions. To further investigate the cost estimator's behavior with more complex mixer operator types, we conducted additional experiments using an extended set of candidate operators \(\{X,Y,XX,YY\}\), which includes two-qubit operators. As shown in Fig. 4 (b), the cost estimator continues to demonstrate its efficiency and high performance, even as the complexity of the mixer Hamiltonian design increases.

We next focus on the behavior of the cost estimator concerning \(p\) as shown in Fig. 4(c). We note that for FG (standard QAOA), the estimated loss decreased monotonically with increasing \(p\), aligning with standard QAOA's behavior. Under the NG scenario (multi-angle QAOA), a transition that QAOA performance begins to decline is observed when the circuit becomes excessively long (\(p>42\)). These results indicate the reliability of the cost estimator as a performance indicator for QAOA and reveal the complexities in QAOA performance under conditions of increased circuit length.

**Mixer generator**. We next evaluate the performance of the customized mixer Hamiltonian generated by MG-Net. As shown in Fig. 5(a), the number of trainable parameters \(\#P\) of the generated quantum circuits aligns with the maximum in scenarios where all parameters are non-correlated (labeled as 'NG') for smaller circuit depths \(p<20\). This alignment indicates that MG-Net effectively enhances

Figure 4: **Behavior of cost estimator**. (a) The correlation between the estimated cost and the minimum cost for Max-Cut (left) and TFIM (right). Each point represents the result of a problem instance. The dashed line represents that QAOA can find the exact solution \(y=x\). (b) Behavior of cost estimator with extended mixer operator pool \(\{X,Y,XX,YY\}\). ’label’ represents the actual achieved approximation ratio, while ’pred’ represents the result predicted by the cost estimator. (c) The achievable cost under various circuit depth \(p\) for Max-Cut (left) and TFIM (right). The label ‘CE’ is the abbreviation of cost estimator. The dashed lines represent the cost achieved by QAOA, while the solid lines represent the cost estimated by our model.

the expressibility of the QAOA ansatz for limited-depth circuits without significantly increasing the number of parameters, thereby avoiding potential trainability issues. As \(p\) increases, a transition occurs. The growth rate of \(\#P\) starts to decelerate, reaching a notable transition point at \(p=62\) for Max-Cut (\(p=52\) for TFIM). Beyond this threshold, the generated mixer Hamiltonians gradually converge towards the configuration seen in standard QAOA, with fully grouped parameters.

Fig. 5(b) compares the effective dimension \(d_{}\) of quantum circuits achieving high approximation ratio \(r 0.995\) in standard QAOA and MG-Net driven QAOA. The results show that circuits generated by MG-Net achieve \(r 0.995\) across all values of \(p\), even as low as \(p=2\), outperforming standard QAOA, which only reaches this level for \(p>50\) for Max-Cut (\(p>20\) for TFIM). Besides, the effective dimension of these high-quality quantum circuits gradually decreases with growing \(p\), in line with the convergence analysis in Theorem 3.1. These findings suggest that MG-Net dynamically adjusts quantum circuits in response to changes in circuit depth \(p\), thereby consistently ensuring high performance.

Fig. 5(c) explicitly demonstrates the optimization behavior of 64-qubit QAOA with FG, NG and the mixer Hamiltonian predicted by our MG-Net. The left panel displays the loss curves during the optimization of quantum circuits with \(p=2\), revealing that our method achieves the most rapid convergence. The right panel further explores the gradients of the three methods during optimization. Notably, the parameter gradient norm of our method maintains a trainable level of \(1\), whereas the gradient for FG and NG falls to \(10^{-1}\) and \(10^{-4}\), respectively, compromising their trainability.

**Performance comparison**. In evaluating the effectiveness of our proposed method for solving Max-Cut problems, we conducted a comparative analysis against both classical and quantum algorithms. The benchmarks included the greedy algorithm, the Goemans-Williamson (GW) algorithm , alongside various quantum approaches such as QAOA, ADAPT-QAOA, and multi-angle QAOA (ma-QAOA). Our analysis, based on the average results from \(100\) graphs in our test dataset, is summarized in Tab. 2. The findings reveal that our method consistently outperforms other techniques in achieving a higher approximation ratio, particularly in larger-scale problems. Refer to Appendix E.1 for comparison results on TFIM.

**More numerical results.** We have conducted additional analysis on the behavior of MG-Net and additional experiments on more tasks. Refer to E for more details.

## 6 Conclusion

In this study, we analyze QAOA's convergence on varied mixer Hamiltonians, focusing on parameter grouping strategies. We introduce MG-Net for dynamically generating optimal mixer Hamiltonians for various problems and circuit depths. Numerical experiments on Max-Cut and TFIM confirm MG-Net's efficacy in enhancing QAOA's approximation ratio, particularly for large-scale problems,

Figure 5: **The trainability of the quantum circuits generated by MG-Net for Max-Cut and TFIM.** (a) The number \(\#P\) of trainable parameters of the quantum circuits with mixer Hamiltonian predicted by MG-Net. (b) Comparison of the effective dimension \(d_{}\) of quantum circuits in standard QAOA and MG-Net driven QAOA (labeled as ‘Ours’). The green and grey solid lines denote the average effective dimension \(d_{eff}\) of the predicted circuits that can achieve an approximation ratio over \(0.995\) for Max-Cut and TFIM, respectively. It assesses circuits achieving an approximation ratio \(r\) of at least \(0.995\). (c) The convergence of QAOA with FG, NG and mixer Hamiltonian predicted by MG-Net for Max-Cut on \(64\)-node weighted graphs.

while ensuring low circuit complexity. This research advances the understanding and application of QAOA across various circuit depths.

Despite these promising outcomes, our work has several limitations that need to be addressed in future research. Firstly, training the cost estimator of MG-Net involves the construction of a labeled dataset \(_{ce}^{}\), which introduces additional resource consumption. Future work can focus on more efficient training algorithms. Additionally, our current approach is specifically designed for QAOA on early fault-tolerant devices, which limits the exploration of extending MG-Net to other quantum algorithms and noisy devices. Addressing these limitations will further enhance the robustness and scalability of MG-Net, offering potential for broader use in VQAs.