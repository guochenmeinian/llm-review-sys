# Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models

David Castillo-Bolado\({}^{*}\), Joseph Davidson\({}^{*}\), Finlay Gray, Marek Rosa

GoodAI

{david.castillo,joseph.davidson}@goodai.com

###### Abstract

We introduce a dynamic benchmarking system for conversational agents that evaluates their performance through a single, simulated, and lengthy user\(\)agent interaction. The interaction is a conversation between the user and agent, where multiple tasks are introduced and then undertaken concurrently. We context switch regularly to interleave the tasks, which constructs a realistic testing scenario in which we assess the Long-Term Memory, Continual Learning, and Information Integration capabilities of the agents. Results from both proprietary and open-source Large-Language Models show that LLMs in general perform well on single-task interactions, but they struggle on the same tasks when they are interleaved. Notably, short-context LLMs supplemented with an LTM system perform as well as or better than those with larger contexts. Our benchmark suggests that there are other challenges for LLMs responding to more natural interactions that contemporary benchmarks have heretofore not been able to capture.

## 1 Introduction

The capabilities of Large Language Models (LLMs) have been primarily evaluated through isolated tests (see 86), focusing on increasing data volumes and context size (c.f. [Su et al., 2024, Chen et al., 2023, Peng et al., 2024, Zhang et al., 2024a]). However, these tests are single shot and single topic (Figure 1), and therefore often overlook the most common user interaction mode: chat-based conversation.

In response, we introduce the LTM Benchmark1, an automated system designed to evaluate the Long-Term Memory (LTM) and Continual Learning (CL) capabilities of conversational agents. The LTM Benchmark engages agents in a single, prolonged conversation, incorporating multiple tasks and distractions to simulate realistic and meaningful interactions. This approach provides a more comprehensive assessment of an agent's ability to effectively use and integrate information across extended dialogues. Furthermore, we show that this "conversational multitasking" structure significantly degrades the test performance of LLMs, indicating that their real-world capabilities are not fully revealed through most contemporary benchmarks. The LTM benchmark is primarily synthetic, and can be generated to result in conversations of arbitrary length, which may potentially surpass the context size of the LLMs under test.

Our contributions can be described thus:

* An automatic benchmarking system that evaluates an agent by conversing with it, interleaving all tests in a single conversation.

* An initial battery of tests evaluating different aspects related to LTM and CL. Both the tests themselves and the generators used to create them.
* An evaluation and analysis of results for some of the most capable language models to date.

## 2 Definitions and Terms

**Needles and Haystack**. Terms introduced by the Needle in a Haystack (NIAH) benchmark (Kamradt, 2024). Needles are sentences in the context that are relevant to the task, and which are required to be found and retrieved. The term haystack refers to the remaining (usually unrelated) text, that acts as a spacer or distractor, depending on the complexity of the _hay_. In this paper, we use "needles" to refer to all sentences that contain information relevant to the task.

**Information Integration**. The ability to take multiple related needles from different locations (either context, or some other memory system) and integrate them into a complete and consistent view. This often implies a continual and iterative process of revision of past knowledge, in the search for novel insights or conclusions.

**Memory Span**. The amount of information that a model is able to take in order to produce a response. In the case of LLMs, this is upper bounded by their maximum context sizes, which determines how much text they can take as input. We also employ this term to refer to how much space in the conversation a specific test uses. For example, we may say that the memory span for a test is 120k tokens, which means that the tested agent will find all necessary information within the latest 120k tokens of the conversation.

**Long-Term Memory (LTM)**. The ability to recall and effectively utilize past information. LTM encompasses several skills related to the generation and management of memories, which include but are not limited to recall, information integration, and the handling of conflicting information. This broad definition acknowledges that different implementations of systems or agents may exhibit such ability in different degrees. Current implementations of LTM commonly rely on an LLM as the main controller, a set of auxiliary components for the management of memories (e.g. scratch-pads and semantic vector databases) and some sort of Retrieval Augmented Generation (Huang and Huang, 2024).

## 3 The LTM benchmark

The LTM Benchmark is a means for evaluating the memory and information integration capabilities of an agent via a single written conversation. It does not make any assumptions about the underlying technology or implementation of the agents being tested. Instead, the focus is on benchmarking the agent's skills from a purely functional perspective, setting only two broad requirements: 1.) The agent must be able to communicate in written English, and 2.) The agent must send a message exclusively when prompted, providing a single response message to each message received.

By following this approach we aim to contribute an objective evaluation tool, capable of assessing the capabilities of current and future agents, and pointing to the most relevant challenges towards the next generation of conversational agents.

Over the course of the test interaction, the agent is presented with various pieces of information at different points in time, and is asked questions or challenged with situations that rely, directly or indirectly, on this information. We have equipped this benchmark with an initial battery of tests which reflect currently challenging skills, but we expect the benchmark to evolve according to the needs of the research community, discarding tests that become trivial and incorporating new ones that evaluate newly identified and more challenging capabilities.

As a mostly synthetic benchmark, the generation processes is both transparent and reproducible. As such, we have released all benchmark results and generators. The benchmark is dynamic, but all generation and scheduling processes are deterministic. The one source of nondeterminism in the testing is from the agents themselves. The length of agents' replies will have knock-on effects in the amount of filler tokens that are used, and that in turn may influence the replies of the agent. More details on this and our mitigation strategies can be found in Appendix C.

### Test structure

Every test in the benchmark is automated and takes place as part of the same conversation. Unlike other LLM-oriented benchmarks, our benchmarking system does not isolate tests as separate prompts, but intertwines them in a continuous exchange of messages between the agent and user. This approach aims to keep the conversation as natural as possible, as we hypothesise that the results obtained in this way will be more likely to generalize to chat-based interactions.

The conversation begins with a message explaining the overall situation to the agent, and continues with all tests' messages and agent responses (see Figure 3). Every test encompasses a series of interventions from the benchmark system (taking the user's role), which are uniformly distributed across a previously-delimited area in the conversation known as the memory span of the test (see Figure 2). The test's span can be set independently from the agent's effective input size, which allows for tests surpassing current LLMs' context lengths. Other constraints like token and time waits (see episodic memory and _Jokes_ scenario in SS3.2) also form part of the test information, which is provided to the benchmark's scheduling system.

To disperse test messages, the benchmark system uses distraction messages, which are messages from other unrelated tasks. These unrelated tasks are preferentially other tests' messages, which greatly improves the benchmark's efficiency, but sometimes it is just not possible to make use of a message from another test and we have to rely on dummy tasks. The dummy task that we rely on is the extraction of answers from the TriviaQA dataset (Joshi et al., 2017) (Figure 4).

A test can be arbitrarily complex depending on its content and definition. Test's messages are either delivered in a static manner (providing a script) or dynamically (via a functional description), depending on the requirements of the testing scenario. The LTM benchmark's scheduling system coordinates the execution of tests and weaves them together into a seamless conversation, while respecting the memory span, test compatibility constraints and optimizing the use of tokens. Finally, once the last test question is answered, the test is scored according to the agent's performance. The complexity of this scoring mechanism can vary greatly too, from a simple pattern matching statement to an elaborate evaluation protocol using LLMs as evaluators. Appendix A contains more details relating to the content and evaluations methods of the tests.

### Test scenarios

We have included an initial set of tests that evaluate a series of memory and learning skills. Inspired by research in psychology and cognitive science, we have built meaningful and organic test scenarios, which abstract away the agent's implementation and focus on the agent's abilities to recall and effectively leverage past information. All test scenarios evaluate a subset of the following skills:

* **Recall (R).** Simple retrieval of past information, usually based on content matching.
* **Conflicting information (C).** Managing of information that contradicts existing knowledge or previously acquired information.
* **Episodic memory (E).** Addressing of memories based on temporal information.
* **Spatial memory (S).** Association of memories to positions in some space.
* **Prospective memory (P).** Memories that address future or hypothetical situations.
* **Theory of mind (T).** Memories pertaining to a third persons' thoughts or feelings.
* **Information integration (I).** The act of combining different pieces of information in order to construct a complex or more abstract overall picture.

And in order to assess these skills, we have built the following test scenarios:

* **Colours (R)**: We state our favourite colour several times and in different ways, changing preferences every time. Finally, we ask the agent what our favourite colour is.
* **Name list (R, C)**: We change names multiple times, then instruct the agent to recall the complete list of names that we have gone by.
* **Jokes (E)**: We tell the agent a series of jokes at different and spaced points in time, the agent is then asked to recall the joke told to it a given number of hours and minutes ago.
* **Locations directions (S, I)**: We give the agent a series of locations from a fictional town. Each statement places a new location into the town relative to the previous one. Finally, the agent is asked how to get from the first place to the last.

Figure 4: Example of the dummy task based on the TriviaQA dataset.

Figure 3: Example beginning of a benchmark. The benchmark puts the agent in situation and then follows by interleaving test messages.

* **Quotes (P)**: The agent is given a quote from a well-known author, and then it is asked to append the quote to its nth response.
* **Trigger response (C, P)**: The agent is given a particular trigger phrase, and instructed on how it should respond to it. We later use those trigger phrases and see if the agent responds appropriately.
* **Sally-Anne (T)**: The agent is evaluated with scenarios from the ToM QA Dataset (Nematzadeh et al., 2018), which assess the ability of the agent to acknowledge the beliefs of another subject (theory of mind).
* **Spy meeting (R, I)**: The agent is contacted by three different people, each of them giving cryptic messages related to where, when, and what to bring to a clandestine meeting. Finally, we ask the agent to retrieve and interpret those messages.
* **Shopping list (R, C, I)**: Over the course of the conversation, the agent receives a series of updates regarding the user's shopping list. The agent is then asked about the status of the shopping list, which has suffered several changes by that time.
* **ChapterBreak (potentially any)**: This dataset (Sun et al., 2022) presents the agent with up to 10 chapters from a book, and 6 options for the following chapter beginning. It is a known fact that \(\) 60% of the samples in the ChapterBreak dataset have no solution, so we have manually selected 4 samples for which we have made sure that finding a solution is possible.
* **Restaurant (C, I)**: The agent is placed in an everyday situation involving sitting at a restaurant, ordering food and dealing with a series of unfortunate events. This highly dynamic test evaluates the coherence of the agent's actions during the experience. Among the evaluated points, this test scores positively ordering items that are in the menu and speaking up when the waiter brings the wrong dish.

These scenarios are tested multiple times in the conversation, for which the benchmark system instructs the agent to forget or disregard the information in the previous test. This approach implies that every additional repetition may require the agent to deal with an increasing amount of conflicting information.

### Generation processes

This benchmark is considered mainly synthetic because of how the data is presented, where tasks are interwoven with other tasks and previous agent responses, and not so much because the data itself being synthetic. Some of the test scenarios do not rely on synthetic data, but on already existing -and sometimes relatively small- datasets. It is the interleaving of tasks and messages what makes each interaction unique, since the conversation history is highly diverse and sensitive to the agent's responses.

Each test scenario has its own generation method, which is conditioned on the global seed as well as the index of the test repetition. All data required for running the tests is generated upfront during initialization and saved to disk. Dynamic tests also rely on seeded random generators and zero-temperature LLM calls to synthesize data and react to the agent's responses. We refer the reader to Appendix A for a detailed description of the generation of each scenario included in this edition of the LTM benchmark.

Besides this initial data generation process, the benchmark relies on a scheduling system to coordinate the intertwined delivery of test messages while attending to the different tests' requirements and other constraints. In order to accomplish that, each test is required to communicate those constraints along with every message delivered: namely, how much time it should pass or how many tokens should be exchanged before its next intervention, if any at all. This is similar to how yielding mechanisms work in computational multithreading. The policy of this message scheduling system can be summarized by the following rules:

* Incompatible tests cannot run concurrently.
* Messages from a same test are delivered until the test ends or yields.
* When a test yields or ends, the system gives priority to resuming other waiting tests before starting a new one.

* Only if no test can be resumed or started, the system will try to unblock the situation by sending dummy tasks (using tokens) or performing time jumps (using up time).

A detailed specification of the benchmark's scheduling system and policy can be found in Appendix C.

### LTM Implementations

In these tests, we test both unmodified LLMs, as well as LLMs augmented with LTM systems. These LTM implementations fall into broadly two categories: those that implement memory through modifications of the LLM, and those that implement memory through modifications to the context of the LLM, but not through architectural choices.

Works such as LongMem (Wang et al., 2023) capture the hidden states of the transformer and use those residuals as input to a "SideNet" which can attend to a cached memory of inputs or documents. Memorizing Transformers (Wu et al., 2022) augment one of the final layers of a transformer model with a \(k\)-nn search over hidden states saved to a stack. MemoryLLM (Wang et al., 2024) is another model which maintains a pool of updatable parameters within the latent space of the transformer.

Works such as MemoryBank (Zhong et al., 2024) and MemGPT (Packer et al., 2023) instead develop memory as a tool which augments the input context of the LLM, rather than influencing token generation directly. These approaches allow for some flexibility in their choice of model, and permit the use of commercial LLMs such as Claude or GPT.

Agents with LTMs can be further distinguished by whether they use their LTM _actively_ or _passively_. In an active system, the LLM has some awareness of the functionality of the LTM, and can choose how to use it. This often takes the form of a multi-step resolution to a user query by making decisions such as when memory should be read from or written to. Passive systems on the other hand perform reads and writes automatically in a typically fixed loop.

With the aim of offering some reference results for agents with LTM, in our tests we include results from our baseline LTM system, as well as from MemoryBank and MemGPT. Other models were not included due to a lack of pretrained weights or a reliance on early completion LLMs, which are not instruction fine-tuned and cannot withstand the required conversational interaction.

MemoryBank places the focus on empathy and psychological companionship, and stores conversation and summaries of those conversations in chronological order, along with a summary of the user. This information is kept in a vector database, and is passively queried during its conversation with the user. MemoryBank also implements a human-like memory forgetting curve.

An example of an active system is MemGPT. This agent attempts to answer queries by consulting multiple forms of memory. It contains an in-context _core_ memory, and external _archival_ and _recall_ memories. The core memories are always visible as a part of the context, whereas the archival and recall memories, need to be queried and are for storing long running events or documents, and the chat history respectively. The agent can update its archival or core memories via function calls.

Our baseline LTM system2 uses both a vector database and a JSON scratchpad. On receiving a user message, the agent generates multiple queries for the vector database and retrieves chunks that are not currently part of the context. The scratchpad is updated (or not) after the LLM generates a response to the query, and is similar to MemGPT's core memory in that it is always present in the context.

## 4 Results

We have produced a standard configuration of the LTM benchmark and used it to evaluate and compare some of the current leading LLMs, in addition to a series of LTM-enabled agents. For the configuration, we have included three repetitions of each scenario, and we have set the number of needles for the scenarios requiring it: in _Colours_, three changes; in _Name List_, five different names; in _Jokes_, four jokes; in _Locations Directions_, the path goes through six locations; and in _Shopping List_ the list updates six times. We run the same benchmark with a number of different memory spans: 2k, 32k, 120k, 200k, and 500k tokens. Additionally, we include results for the same tasks evaluated in isolated scenarios, where there is neither task-switching nor distractors in the conversation (resembling standard benchmarks). We also exclude _ChapterBreak_ from the 2k configuration to avoid exceeding the memory span.

A benchmark run consists of 33 tests (11 scenarios and 3 repetitions each), which measure the agent's performance on the different scenarios under diverse contexts. All scores are normalized to \(\) and scores from the same scenario are averaged together, which makes for a maximum of 11 points per benchmark. In order to obtain accurate distribution estimates, we generate 1,000 possible outcomes by repeatedly sampling one result from each scenario and adding them together. After running the initial set of benchmarks for all configurations, we allocated the remaining budget to configurations where reducing variance would be most impactful --such as those with high scores and competing closely with others. These additional runs use a different seed and are marked with underscores in Table 1.

The benchmarks are executed using a variety of on-demand APIs from various providers, including Google, OpenAI, and Anthropic. The open source models were tested via TogetherAI's API. Running and evaluating all the benchmarks in this paper cost \(\) $7,200 and took 142 hours.

The tested LTM agents are capable of using different LLMs. On a cost-to-quality ratio, we found that GPT-4o-mini works well. We have tried to keep the context limit for the LTM agents to 16k tokens3, but the MemoryBank and MemGPT systems cannot artificially restrict their context sizes. We observed that the volume of memories which they retrieve later in the benchmark can balloon the context sizes to around 60k or 40k tokens respectively.

In Table 1 we show the scores achieved by each agent configuration under all memory span settings. We also show a visual representation of the same results in Figure 5. These results show that LLMs perform well on short memory spans, but they struggle as soon as the memory span surpasses their maximum context size, as the relevant information is simply lost. Conversely, LTM agents suffer less when the memory span is increased.

## 5 Analysis

In general, the benchmark has proved to be comprehensive, representing a significant step-up in terms of evaluation results that are robust and hopefully translate to real-world scenarios. One example of this is in GPT-4o, which achieves 100% NIAH coverage and nearly perfect results on the NIAN test , yet it scores similarly to GPT-4 turbo in our benchmark. This corroborates OpenAI's claims  and anecdotal evidence which describes GPT-4o's interactivity to

 Model & } & } & } & } & } & } \\  & Context & Score & std & Score & std & Score & std & Score & std & Score & std & Score & std \\  Mistral 8x7B & 32000 & 5.0 & 0.9 & 1.4 & 0.5 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\ Mistral 8x2B & 65536 & 4.9 & 1.0 & 5.6 & 0.7 & 0.0 & 0.0 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\ Llama 3 70B & 8000 & 8.2 & 0.7 & 1.9 & 1.0 & 0.2 & 0.0 & 0.2 & 0.0 & 0.2 & 0.0 & 0.2 & 0.0 \\  GPT-3 5 turbo & 16384 & 4.1 & 0.8 & 4.7 & 0.8 & 0.1 & 0.1 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\ GPT-4 turbo & 128000 & 7.9 & 0.1 & 6.6 & 0.7 & 5.2 & 1.3 & 5.8 & 1.0 & 3.9 & 0.9 & 1.0 & 0.5 \\ GPT-4o & 128000 & 7.6 & 0.9 & 5.9 & 0.7 & 5.6 & 1.4 & 5.9 & 1.1 & 5.2 & 0.9 & 0.9 & 0.5 \\ GPT-4o-mini & 128000 & 7.8 & 0.7 & 5.6 & 1.0 & 4.1 & 0.7 & 5.1 & 1.2 & 4.8 & 0.8 & 1.4 & 0.6 \\ Claude 3 Opus & 200000 & 8.3 & 0.8 & **7.8** & **0.9** & **6.7** & **1.0** & **7.4** & **1.1** & 5.1 & 0.9 & 3.4 & 0.4 \\ Gemini 1.5 Pro & 1000000 & 7.4 & 0.9 & 6.5 & 0.8 & 6.4 & 0.8 & 7.0 & 0.5 & **7.7** & **1.0** & **6.1** & **1.3** \\  LTM Claude 3 Opus & 16384 & 8.7 & 0.9 & 7.5 & 0.7 & 5.0 & 1.0 & 5.7 & 1.2 & 6.4 & 1.1 & 4.9 & 1.0 \\ LTM GPT-4 turbo & 16384 & **9.2** & **0.5** & 6.3 & 0.8 & 5.2 & 0.9 & 5.0 & 1.0 & 5.3 & 0.9 & 3.1 & 0.8 \\ LTM GPT-to-mini & 16384 & 8.1 & 0.7 & 5.4 & 0.8 & 4.7 & 1.0 & 4.6 & 1.1 & 4.2 & 0.9 & 4.7 & 1.0 \\ LTM Llama 3 70B & 8000 & 8.4 & 0.8 & 6.9 & 1.0 & 5.0 & 0.9 & 4.7 & 1.0 & 5.6 & 0.7 & 4.8 & 0.8 \\ MemGPT GPT-4o-mini & 16384 -40000 & 6.8 & 0.9 & 4.8 & 0.7 & 2.3 & 0.7 & 3.2 & 0.8 & 3.6 & 0.8 & 2.0 & 0.7 \\ MemoryBank GPT-4o-mini & 16384 -60000 & 6.4 & 0.7 & 3.5 & 0.7 & 2.0 & 0.5 & 2.8 & 0.5 & 3.3 & 0.8 & 3.6 & 0.6 \\ 

Table 1: Benchmark results for distinct agent configurations: open-source LLMs, commercial LLMs, and LTM agents. _Context_ stands for the maximum input tokens available to the LLM. All tests are out of 11 points, apart from the 2k benchmark, which is out of 10. Underscored values are the result of two benchmark runs. A visual representation of these results can be seen in Figure 5.

be better, but its overall capabilities to be in line with GPT-4 turbo (Yan et al., 2024, ArtificialAnalysis, 2024).

We have found the conversational format to be the main challenge for current LLMs (and especially the open source ones), which work better on short and clean prompts. The results for the isolated tests vs. the interleaved 2k tests reveal the interleaving of tasks as a key factor. There is a minimal difference in the presentation of the tests outside of the interleaving, yet performance drops to below half for two of the the open source LLMs. Commercial LLMs are more robust to the interleaved scheme, which may point to how their training is performed. It is worth noting that longer input sizes allow for both the tackling of longer tasks and the presence of more distractors.

Looking at the results in detail (see Appendix D), the Restaurant task was the most variable one. Claude had trouble with the task by being too expressive, adding "stage directions" to its responses. The Miktral models also often hallucinated interactions between the waiter and diner on their own, resulting in failures. A lot of models that otherwise fail completely on the other benchmark tasks, can get some credit for the first part of the Restaurant task where they are greeted, shown a menu and asked for a drink order in a single message. If the agent plays along with the scenario, it will obtain some score. Our automated evaluation system (powered in some parts by GPT-4 turbo) also fails at some points, like being unable to distinguish between dishes that are discussed, and ones that have been ordered.

Also challenging was the prospective memory tests, where the agents had to append a quote at a future point in time. The most common failure was in the agent not being able to count its statement correctly, usually resulting in an off-by-one error. We have noted that LLMs using special tokens for message delimiters are those which struggle the most in counting them. We believe that this phenomenon might be related to those tokens being rare in the training data.

In the _Locations directions_ task, agents were not explicitly asked to visit all locations, but most agents either failed the task or replied with an almost exact replica of the original directions. Notably, merely recalling (and potentially rephrasing) those sentences does not require any kind of spatial thinking.

Traces of pattern-completion behaviour can be seen in all benchmark conversations, in the form of early responses heavily influencing later ones. These later responses usually continue the format and thinking patterns established previously, and an early good response strategy (e.g. listing all items in the shopping list after every update) often translates to all task repetitions succeeding. We have also observed the mimicking of reluctant responses or growing preceding spaces in responses.

Sometimes, and specially with long-context LLMs, mixing information from different repetitions happens. Simple recall does not solve this issue, which requires careful (and ordered) reading of all potentially-relevant information. This seems to imply some sort of iterative processing of the input, which current transformers-based architectures are not very good at. Current LLM architectures are very similar, and yet significant differences can be seen. Such differences mostly originate in the training procedure, but architectural choices can be decisive too. To give some examples, Gemini 1.5 Pro is mostly incapable of solving ToM tasks, but it also exhibits better-than-average spatial reasoning. Llama 3 excels when the prompt is short and clear, and it can beat the best commercial LLMs in this regime, being especially well suited for LTM agents. Mixtral 8x22B also performs exceptionally well in short conversations.

Figure 5: Scores obtained for all agent and memory span configurations. Solid boxes correspond to vanilla LLMs, while strided boxes represent LTM agents. Each color can be associated with a different LLM. Context sizes for each of the agents vary, and are detailed in Table 1.

The guardrails on commercial LLMs frequently appear in responses to longer versions of the benchmark tests. Safety measures tend to go off when the tests ask about any kind of personal info or datum that might seem out of the agent's reach, even if the data is somewhere in context. This re-frain of "As an AI language model..." could be a default result of the imperfect attention trying to read personal information from the context.

## 6 Other benchmarks

Most LLM benchmarks rely on one-shot evaluations4 (Figure 1). Prefilled contexts have various token lengths, e.g. up to 18k in LongBench (Bai et al., 2023), \(\) 36k in LooGLE (Li et al., 2023), and 200k in InfiniteBENCH (Zhang et al., 2024b). A common shortcut is to take data points from existing datasets, but it is possible that they are part of the pretraining dataset for a model (Jacovi et al., 2023; Yang et al., 2023). Datasets like _HotpotQA, 2WikiMultihopQA_, _Qasper_, and _NarrativeQA_(Yang et al., 2018; Ho et al., 2020; Dasigi et al., 2021; Kocisky et al., 2017) are popular choices for test data and included in LongBench. Mitigation strategies include the use of recent real-world data (LooGLE) and the handcrafting of questions (LongBench). Also based on human-created questions there's L-Eval (An et al., 2023), which aims to bridge the gap between commercial and open-source models.

Needle in a Haystack (Kamradt, 2024) is a common pattern for evaluating LLMs' retrieval skills. Often those using NIAH use their own data and needles, which is an issue because some works have shown that the complexity of the "bay" matters (Pekelis, 2024): repetitive haystacks or incongruuous needles present an easier task for the LLM. More recent variations include the multi-needle retrieval (Knight-Webb, 2024) and the Needle in the Needestack (NIAN) test (Burns, 2024). There are other benchmarks like RULER (Hsieh et al., 2024) and BAMBOO (Dong et al., 2024) which extend the evaluation complexity beyond simple retrieval, although the tests are still comprised of isolated prompts (one-shot). Other long one-shot tests such as Multi-Session Chats (Xu et al., 2022) and ChapterBreak (Sun et al., 2022) evaluate holistic understanding of the context, which entails a potentially high complexity.

There are also evaluation systems intended for agents. AgentBench (Liu et al., 2024), WebArena (Zhou et al., 2023) and Autonomous Replication and Adaptation tests (Kinniment et al., 2023) perform genital evaluations. Similar to us, these benchmarks evaluate the agent within a dynamic environment in which the agent is required to autonomously pursue a goal. AgentSims (Lin et al., 2023) also presents a dynamic environment, but they leave the evaluation open. Bai et al. (2024) incorporate interviews with the agent to some degree, using LLMs as an aid for evaluating the agent's responses. LLF-Bench (Cheng et al., 2023) features a dynamic environment for assessing the continual learning skills of an agent, adapting the original proposal of Mikolov et al. (2018) to an LLM interface.

## 7 Limitations and future work

Despite the interesting results, there are a number of limitations that require further development. This benchmark is also intended to be a living benchmark, which calls for continued work in updating the tests for targeting new skills.

Agents' robustness in this paper are based on only three repetitions, which is not very reliable. The benchmark has a high time and financial cost to running it, and therefore multiple repeats have proven currently infeasible, but we expect this to change in the near future.

Given the breadth of possible responses to any question, the automatic evaluation systems can perform poorly in some edge cases, requiring manual checking. Summarized reports facilitate the manual revision, but in the future we aim to make evaluations infallible. See Appendix B for examples where the automated checking has failed, and how we have manually evaluated the tests.

Future work includes addressing the noted shortcomings and adding more tests, which include but are not limited to 1.) Learning and recalling instructions or facts, especially focusing on forward and backward transfer; 2.) Stress testing with large amounts of information to integrate, 3.) More research-based tasks where multiple passages need to be recalled and integrated to answer correctly;4.) Multi-modal evaluations; and 5.) Multi-user scenarios, where the agent needs to attend different users' needs while keeping their information separated.

## 8 Conclusions

In this paper, we identified an issue with the current suite of benchmarks for LLMs, namely that the tests they run do not necessarily reflect real usage as chat agents along with the complexities that a chat environment brings. In response to this we have presented a new benchmark that subjects conversational agents (LLM-based or not) to a series of tests over a single lengthy conversation in order to test their memory and ability to integrate information. We benchmark current SOTA large-context LLMs, as well as LLMs equipped with a LTM systems.

We observe that while all scores drop as the benchmark length increases, the scores of the agents using an LTM drop less precipitously, which suggests that the combination of an LLM using a shorter context alongside an LTM system may provide a "focusing" effect for the LLM.

Additionally, we show that our task-interleaving approach makes the benchmark significantly more difficult, with scores that vary up to 1.5 points between a benchmark with interleaved tasks, and one with a more traditional isolated task regime. The open source LLMs had particular trouble with this, which may reveal how commercial LLM training is structured.

The benchmark has been open sourced on GitHub, and we plan to continually update it as the capabilities of LLM and LTM systems mature.

## 9 Societal Impacts and Ethical Considerations

The eventual societal impact of LLMs as a whole is hard to predict. Aside from the current potential harms of the vast energy resources required to create, maintain, and run these systems, our work may potentially have some impact on the trajectory of development.

Some benchmarks like NIAN have already fallen afoul of Goodhearts law: "When a measure becomes a target, it ceases to be a good measure."[Strathern, 1997] and it is possible that ours may do the same. However, because our benchmark specifically targets memory and learning through conversation, an LLM system that is designed to do well on our benchmark may also become proficient at personal Secretary or companionship behaviours. Such results could then lead to the loss of human based relationships in favour of LLM based ones.

We make use of ChapterBreak, which is a collection of works from the PG-19 dataset[Rae et al., 2020] and a dump of collected fanfiction from Archive of our own (AO3). One of AO3's stated positions is that all of the works hosted by it are public domain. PG-19 sources its data from Project Gutenberg which is a repository for explicitly public domain works. The datasets are filtered somewhat for stories suitable for a general audience, but these tags are the responsibility of the authors, so some inappropriate content may be in the data. The samples curated for the benchmark were mostly done for solvability, but those samples didn't contain content that the authors would consider offensive.