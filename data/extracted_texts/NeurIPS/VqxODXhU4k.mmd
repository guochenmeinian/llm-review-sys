# Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients

Yuri R. Fonseca

Decision, Risk and Operations

Columbia University

New York, NY

yfonseca23@gsb.columbia.edu

&Caio F. L. Peixoto1

School of Applied Mathematics

Getulio Vargas Foundation

Rio de Janeiro, RJ, Brazil

caio.peixoto@fgv.br

&Yuri F. Saporito1

School of Applied Mathematics

Getulio Vargas Foundation

Rio de Janeiro, RJ

yuri.saporito@fgv.br

###### Abstract

Instrumental variables (IVs) provide a powerful strategy for identifying causal effects in the presence of unobservable confounders. Within the nonparametric setting (NPIV), recent methods have been based on nonlinear generalizations of Two-Stage Least Squares and on minimax formulations derived from moment conditions or duality. In a novel direction, we show how to formulate a functional stochastic gradient descent algorithm to tackle NPIV regression by directly minimizing the populational risk. We provide theoretical support in the form of bounds on the excess risk, and conduct numerical experiments showcasing our method's superior stability and competitive performance relative to current state-of-the-art alternatives. This algorithm enables flexible estimator choices, such as neural networks or kernel based methods, as well as non-quadratic loss functions, which may be suitable for structural equations beyond the setting of continuous outcomes and additive noise. Finally, we demonstrate this flexibility of our framework by presenting how it naturally addresses the important case of binary outcomes, which has received far less attention by recent developments in the NPIV literature.

## 1 Introduction

Causal inference from observational data presents unique challenges, primarily due to the potential for confounding variables that can affect both outcomes and variables of interest. The unconfoundedness assumption, crucial in this context, posits that all confounding variables are observed and properly accounted for, allowing for an unbiased estimation of causal effects. However, in many real-world scenarios, this assumption is difficult to satisfy. When this is the case, approaches that rely on _instrumental variables_ (IVs) -- quantities that are correlated with the variable of interest (relevance condition), do not affect the outcome in any other way (exclusion condition) and are independent of the unobservable confounders -- offer a way to identify causal effects even when unobserved confounders exist. As a concrete example, suppose we want to estimate the impact of years of education on earnings. Most likely, there are unobservable factors, such as omitted ability, affecting both the decision to study and income. In this case, changes in compulsory schooling laws could be used as an instrument .

While traditional parametric approaches to IV regression often require assumptions about the relationships between variables that may not hold in practice, _nonparametric_ IV (NPIV) models can adapt to the intrinsic structure of the data, allowing for a more nuanced understanding of causal relationships. For this reason, there has been a recent boost of new algorithms applied to the NPIV estimation problem and its theoretical properties. The challenge is that NPIV estimation is an ill-posed inverse problem [28; 8; 9], and recent methods aim to incorporate developments from predictive models, e.g., deep learning and kernel methods, while also accounting for the particular structure of the inverse problem at hand.

In this work, we deviate from previous approaches to NPIV estimation which end up minimizing _empirical_ versions of the populational risk [28; 19; 34; 25], or trying to satisfy large collection of empirical moment restrictions [18; 24; 6]. In our formulation, we compute an analytical functional gradient for the actual _populational_ risk. After formulating a consistent estimator for this gradient, we apply stochastic gradient descent in a certain function space to recover the effect of \(X\) on the outcome \(Y\).

The rest of the paper is structured as follows. We conclude Section 1 with a thorough discussion of the previous works on NPIV and our contributions. Next, we provide the basic setting for NPIV regression in Section 2. In Section 3 we detail the associated risk minimization problem and analytically compute its stochastic gradient. Based on these results, we introduce our method in Section 4, with accompanying practical considerations and theoretical support. Numerical experiments are reported on Section 5, where our algorithm is compared to state-of-the-art machine learning methods for NPIV. Finally, in Section 6 we study the case of binary outcomes, showing how our method naturally addresses this scenario under the current assumptions made in the literature. Proofs of all our results are presented in Appendix A, while Appendix B contains formal comparisons with existing methods and Appendix C provides additional implementation details.

### Previous work

Many traditional approaches to IV estimation, like the Two-Stage Least Squares (2SLS) method, rely on linear models for treatment estimation and counterfactual prediction functions (see [3; 40] for a thorough survey of classical IV estimation). These approaches, while efficient for estimating policy effects, depend on strong parametric assumptions. Nonparametric extensions of 2SLS then attempt to introduce model flexibility by utilizing linear projections onto known basis or sieve functions (as in [28; 27]) or kernel-based estimators (as in [17; 13]). However, these traditional methods face limitations in large, high-dimensional datasets as they are sensible to the particular choice of sieve functions and number of basis elements (see the discussion in ).

In order to propose a scalable method,  introduces DeepIV, a generalization of 2SLS which employs neural networks in each step of the two-stage procedure. Although their algorithm is more suitable for high dimensional data, the authors do not provide theoretical guarantees. Deep learning estimators have also appeared within methods focused on the Generalized Method of Moments (GMM), which leverage moment restrictions imposed by the IV to obtain an estimator. In , the authors propose DeepGMM, a reformulation of the optimal weighted GMM problem as a minimax problem. They rely on the identification assumption to demonstrate consistency results, provided their algorithm is able to nearly solve a smooth zero-sum game.

In another direction, some methods exploit developments in the RKHS2 literature and apply them to NPIV estimation. The KIV algorithm in  transforms the problem into two-stage kernel ridge regression through a kernel mean embedding of the distribution of the endogenous covariate given the instrument. In , the authors take insight from two-stage problems in stochastic programming and propose DualIV, an algorithm which uses Fenchel duality to transform NPIV into a convex-concave saddle point problem, for which a closed-form RKHS algorithm is presented. Inspired by the gradient boosting literature,  propose an algorithm that iteratively fits base learners in a boosting fashion, but still needs a post-processing step where the basis functions are held fixed and the weights are optimized.

We point out that most of the cited works have some flexibility issues. While kernel based methods impose a higher computational cost as the number of data points grows and may exhibit poor performance in high dimensions [35; 25], Deep Learning based algorithms can suffer from high variance and instability if the amount of available data is _small_, as was seen in [25; 4].

Furthermore, in many applications of interest, ranging from consumer behavior  to epidemiology , the outcome is binary; for a review, see . In these scenarios, a straightforward application of the quadratic loss function would result in a misspecification of the problem, potentially leading to erroneous estimates. The above NPIV methods strongly leverage the additive structure of the problem formulation, and proper extensions would require significant effort, a detailed discussion is presented in Appendix B. Traditionally, a common approach for binary outcomes is using a _semi-parametric_ specification based on control function . This allows for nonparametric estimation of the distribution of the unobservables together with identification of the causal function's _parameters_[1; 32; 14; 11]. To the best of our knowledge, the only work that addresses _fully nonparametric_ IV for binary outcomes is , which proposes an estimation strategy based on Tikhonov regularization.

### Contribution

We propose a novel algorithm for NPIV estimation, SAGD-IV, which works by minimizing the populational risk through stochastic gradient descent in a function space. Under mild assumptions, we provide finite sample bounds on the excess risk of our estimator. Empirically, we demonstrate through numerical experiments that SAGD-IV achieves state-of-the-art performance and better stability.

We have the freedom to employ a variety of supervised learning algorithms to form the estimator of the stochastic gradient, most notably kernel methods and neural networks. This means our estimator could be tailored to specific scenarios where a particular method is more likely to perform well. Furthermore, our algorithm is naturally able to handle non-quadratic loss functions, which allows us to extend both our algorithm and theoretical guarantees to the binary outcomes case.

## 2 Problem setup and notation

Following most of the recent literature, we start by presenting the problem setup under the additive noise assumption to demonstrate our methodological contribution. Then, in Section 6, we analyze the important case of binary outcomes, a prototypical example of when this assumption does not hold.

Let \((,,)\) be the underlying probability space, and let \(X\) be a random vector of covariates taking values in \(^{d_{X}}\). We assume that the _response variable_\(Y\) is generated according to

\[Y=h^{}(X)+,\] (1)

where \( L^{2}(,,)\) and satisfies \([]=0\). We denote by \(_{X}\) the distribution of the r.v. \(X\) and assume that the _structural function_\(h^{}\) belongs to \(L^{2}(,(),_{X})\)3, which we simply denote by \(L^{2}(X)\). This is a Hilbert space with norm and inner product given by \(\|h\|_{L^{2}(X)}^{2}=[h(X)^{2}]\) and \( h,g_{L^{2}(X)}=[h(X)g(X)]\). We assume that \([ X] 0\), that is, some covariates are endogenous. Finally, we assume the existence of a random vector \(Z\), taking values in \(^{d_{Z}}\) and satisfying

1. \([ Z]=0\), the exclusion restriction;
2. \(X Z\), i.e., \(Z\) is relevant.

This makes \(Z\) a valid instrumental variable. We define \(_{Z}\) and \(L^{2}(Z)\) analogously to \(_{X}\) and \(L^{2}(X)\). We further consider the mild assumption that \(X\) and \(Z\) have a joint density denoted by \(p_{X,Z}\). Our goal is to estimate \(h^{}\) based on i.i.d. samples from the joint distribution of \(X,Z\) and \(Y\).

As we have listed in the introduction, there are a few different approaches to estimate \(h^{}\). We will follow here the original one of , in which we take the expected value of Equation (1) conditioned on \(Z\) to get

\[[Y Z]=[h^{}(X) Z].\] (2)

This motivates us to define the conditional expectation operator \(:L^{2}(X) L^{2}(Z)\) given by

\[[h](z)=[h(X) Z=z].\]This is a bounded linear operator which satisfies \(\|\|_{} 1\) (the operator norm) and whose adjoint \(^{*}:L^{2}(Z) L^{2}(X)\), that is also a bounded linear operator, is given by \(^{*}[g](x)=[g(Z) X=x]\). Defining \(r(Z)=[Y Z]\), we can then rewrite Equation (2) as

\[r=[h^{}].\] (3)

This is a Fredholm integral equation of the first kind  and, as such, poses an ill-posed linear inverse problem.

In this context, a common assumption [13; 7; 34] made about \(\) is compactness. We also need it here, noting that compact operators with infinite dimensional range provide prototypical examples of ill-posed inverse problems . However, we wish to phrase this assumption in a different, albeit equivalent , form:

**Assumption 2.1**.: Let

\[(x,z)=(x,z)}{p_{X}(x)p_{Z}(z)}\] (4)

denote the ratio of joint over product of marginals densities for the \(X\) and \(Z\) variables, with the convention that \(0/0=0\). We assume that this kernel has finite \(L^{2}(_{X}_{Z})\) norm, that is,

\[\|\|_{L^{2}(_{X}_{Z})}^{2}=_{X }(x,z)^{2}p_{X}(x)p_{Z}(z)\;xz<.\]

This is equivalent to assuming that the conditional expectation operator \(:L^{2}(X) L^{2}(Z)\) is Hilbert-Schmidt and, hence, compact.

## 3 The risk and its gradient

Motivated by Equation (3), we introduce a pointwise loss function \(:\) and define the associated _populational risk measure4_\(:L^{2}(X)\) as

\[(h)=[(r(Z),[h](Z))].\] (5)

The example the reader should keep in mind is the squared loss function \((y,y^{})=(y-y^{})^{2}\), although, as we will see, other examples could be used depending on the particular regression setting. Our goal is to solve the NPIV regression problem by solving

\[_{h}(h),\]

where \(\) is a closed, convex, bounded subset of \(L^{2}(X)\) such that \(h^{}\). We also require \(0\). For future reference, we state these conditions:

**Assumption 3.1** (Regularity of \(\)).: The set \(\) is a closed, convex, bounded subset of \(L^{2}(X)\), which contains the origin and satisfies \(h^{}\).

The only part of this assumption which concerns the data generating process is \(h^{}\), which essentially means that the set \(\) is large enough.5

For \(\) satisfying Assumption 3.1, we let \(D\,<\), so that \(\|h\|<D\) for every \(h\). One possible choice for the set \(\) is the \(L^{}(X)\) ball contained in \(L^{2}(X)\), that is

\[=\{h L^{2}(X):\|h\|_{} A\},\] (6)

where \(A>0\) is a constant. This set is obviously convex and bounded in the \(L^{2}(X)\) norm. It can be shown that it is also closed, but not necessarily compact, a restriction on the search set imposed in . This can be seen by taking a \(\|\|_{}\)-bounded orthonormal basis for \(L^{2}(X)\), if one exists. We denote by \(_{}\) the orthogonal projection onto \(\). In case \(\) is given by Equation (6), we have the explicit formula6\(_{}[h]=(h^{+} A)-(h^{-} A)\).

We now state all the assumptions needed on the pointwise loss \(\). We denote by \(_{2}\) a partial derivative with respect to the second argument.

**Assumption 3.2** (Regularity of \(\)).:
1. The function \(:\) is convex and \(C^{2}\) with respect to its second argument;
2. The function \(\) has Lipschitz first derivative with respect to the second argument, i.e., there exists \(L 0\) such that, for all \(y,y^{},u,u^{}\) we have \[|_{2}(y,y^{})-_{2}(u,u^{})| L(|y-u|+| y^{}-u^{}|).\]

Some useful facts which follow immediately from these assumptions are presented in Appendix A.

### Gradient computation

Here we divert from the previous literature and proceed to compute a functional stochastic gradient for the risk \(\). A similar idea has been considered by  in the context of statistical inverse problems, however, their setup assumes that the operator posing the inverse problem is known, which greatly simplifies the analysis and cannot be directly applied to the NPIV problem. We start by providing an analytical formula for \((h)\):

**Proposition 3.3**.: _The risk \(\) is Frechet differentiable and it's gradient satisfies_

\[(h)=^{*}[_{2}(r(),[h ]())] L^{2}(X),\] (7)

_where \(^{*}:L^{2}(Z) L^{2}(X)\) is the adjoint of the operator \(\)._

Both \(\) and \(^{*}\) are unknown operators which commonly appear in NPIV estimation [8; 13], with Nadaraya-Watson kernels [26; 39] being a classical option for estimating them. However, the fact that they are nested in Equation (7) is undesired, and may impose extra computational costs, especially considering that \(_{2}\) is nonlinear in the second argument for non-quadratic losses. We overcome this difficulty by leveraging the following characterization of the gradient:

**Corollary 3.4**.: _The gradient of the population risk satisfies_

\[(h)(x)=[(x,Z)_{2}(r(Z), [h](Z))],\] (8)

_where \(\) is defined as in Equation (4)._

The benefit of our approach is that Equation (8) enables a more computationally efficient way to estimate the gradient of the risk functional, as we explain next. From Equation (8), for a given \(x\), the random variable \((x,Z)_{2}(r(Z),[h](Z))\) is an unbiased stochastic estimator of \((h)(x)\). Our stochastic _approximate_ gradient is then built using estimators \(,\) and \(}\) of \(,r\) and \(\) respectively. With this notation, given a sample \(Z\), we consider

\[(h)}(x)=(x,Z)_{2}( (Z),}[h](Z)).\] (9)

We have then substituted the estimation of the operator \(^{*}\) by the simpler problem of estimating the ratio of densities \(\) and computing its product with \(_{2}((Z),}[h](Z))\). Moreover, density ratio estimation is an area of active research within the machine learning community, which makes developments in this direction immediately translatable into benefits to our method.

## 4 Algorithm: implementation and theory

In Algorithm 1 we present _Stochastic Approximate Gradient Descent IV (SAGD-IV)7_, a method for estimating \(h^{}\) using the approximation given by Equation (9).

``` Input: Samples \(\{(_{m})_{m=1}^{M}\}\). Estimators \(,\) and \(}\). Sequence of learning rates \((_{m})_{m=1}^{M}\). Initial guess \(_{0}\). Output:\(\) for\(1 m M\)do  Set \(u_{m}=(,_{m})_{2}((_{ m}),}[_{m-1}](_{m}))\)  Set \(_{m}=_{}[_{m-1}- _{m}u_{m}]\) endfor  Set \(=_{m=1}^{M}_{m}\) ```

**Algorithm 1** SAGD-IV

_Remark 4.1_.: We note the fact that the internal loop of Algorithm 1 only needs samples from the instrumental variable \(Z\) to unfold.

For our theoretical analysis, all we require of the estimators of \(,r\) and \(\) is the following:

**Assumption 4.2** (Properties of \(,\) and \(}\)).:

1. \(,\) and \(}\) are computed using a dataset \(\), composed of \(N\) samples of the triplet \((X,Z,Y)\), which are independent from the \(Z\) samples used in Algorithm 1.
2. \( L^{2}(Z)\) a.s.;
3. \(}:L^{2}(X) L^{2}(Z)\) is a bounded linear operator a.s.
4. \(\|\|_{} }{}|(x,z)|<\). This implies, in particular, \(\|\|_{L^{2}(_{X}_{Z})}<\).

### On computing \(},\) and \(\)

Algorithm 1 is purposefully agnostic to how the estimators \(},\) and \(\) are obtained, in order to provide the user with sufficient modeling flexibility. In what follows, we will explain the options we considered for accomplishing these tasks in the experiments section. A more detailed description is given in Appendix C.

Estimating \(\) is a interesting problem in itself as it is a ratio of densities, and it has drawn significant attention from the Machine Learning community. Here, we consider kernel and neural network methods, regarded as the state of the art. On the other hand, not many options are available to compute \(}\), which means estimating all possible regression of functions of \(X\) over \(Z\). Here, we chose to use kernel mean embeddings, as was developed . Finally, estimating \(r\) is the simplest procedure in our method as it is a regression of \(Y\) over \(Z\). Similarly to \(\), we also consider kernel and neural network methods.

Based on these options, we decided to specify two variants of SAGD-IV built upon how we compute \(\) and \(\): **Kernel SAGD-IV** uses RKHS methods for both estimators, while **Deep SAGD-IV** employs neural network estimators in these two tasks. The method for computing \(}\) is kernel mean embeddings in both variants.

Under a big data scenario, the dependence of our algorithm on kernel methods to estimate the \(\) operator could be a limitation. However, we are able to avoid this issue for the estimation of \(\) and \(r\). Nonetheless, since our algorithm is agnostic to the specific estimator of \(\), we could take advantage of any recent developments in the literature of operator-estimation problems.

### Risk bound

Since we are directly optimizing the projected populational risk measure, we are able to provide guarantees for \(()\) in mean with respect to the training data \(_{1:M}=\{_{1},,_{m}\}\). Our main result is the following:

**Theorem 4.3**.: _Let \(_{0},,_{M-1}\) be generated according to Algorithm 1. Assume that \(\) satisfies Assumption 3.2, \(\) satisfies Assumption 3.1, \(\) satisfies Assumption 2.1 and \(,,}\) satisfy Assumption 4.2. Then, if we let \(=_{m=1}^{M}_{m-1}\), the following bound holds:_

\[_{_{1:M}}[()-(h^{} )]}{2M_{M}}+_{m=1}^{M}_{m}+ ,\] (10)

_where_

\[=\|\|_{}^{2}(C_{0}^{2}+L^{2 }\|\|_{L^{2}(Z)}^{2}+L^{2}D^{2}\|}\|_{}^{2}),=\|-\|_{L^{2}(p_{X}_{Z}) }^{2}+\|r-\|_{L^{2}(Z)}^{2}+\|-}\|_ {}^{2},\] \[=2D3(C_{0}^{2}+L^{2}[Y^{2}]+L^{2}D^{2 }),2L^{2}\|\|_{}^{2},2L^{2}D^{2}\|\|_{ }^{2}}.\]

It is productive to analyze the RHS of the bound in Equation (10) more carefully. If we choose the sequence \((_{m})\) to satisfy the usual assumptions

\[M_{M}_{m=1}^{M}_{m} 0\]

as \(M\), then the first two terms in the sum vanish as \(M\) grows. The last term appears due to the fact that we do not know \(,r\) nor \(\), but it explicitly quantifies how the estimation errors of \(,\) and \(}\) come together to determine the quality of the final estimator. Furthermore, it converges to zero if the chosen estimation methods are consistent.8

_Remark 4.4_ (\(Y\) versus \(r(Z)\)).: Since \(r=[h^{}]\), one should compute the risk as in Equation (5), comparing \([h](Z)\) with \(r(Z)\). However, most works on NPIV  compare \([h](Z)\) directly with \(Y\) instead of \(r(Z)\). In Appendix E we discuss this difference and its relationship with the estimator \(\).

_Remark 4.5_ (Consistency for \(h^{}\)).: Since the NPIV regression is an ill-posed inverse problem, consistency of our algorithm is a very challenging and interesting research question, and it would require additional assumptions. For instance, under strong convexity of the populational risk measure, it is well known that consistency is obtained if the excess risk converges to zero. It can be shown that, for the quadratic loss, strong convexity of the populational risk is equivalent to \(\) having a continuous inverse.

## 5 Numerical experiments

Here9, we compare the performance of the two variants of SAGD-IV explained in Section 4.1, Kernel SAGD-IV and Deep SAGD-IV, with that of five other notable algorithms for estimating \(h^{}\) in the context of continuous responses: KIV, DualIV, DeepGMM, DeepIV and 2SLS. Hyperparameters and other implementation details are discussed in Appendix C.

### Continuous response

To study the performance of our estimator in a continuous response setting, we used the data generating process from , which we recall below:

\[Y =h^{}(X)++, X=Z_{1}++,\] (11) \[Z =(Z_{1},Z_{2})([-3,3]^{2}), (0,1),\;,(0,0.1).\]

Thus, the confounding variable is \(\) and within this specification, four options for \(h^{}\) are considered:

\[: h^{}(x) =\{x>0\}, : h^{}(x) =|x|,\] \[: h^{}(x) =x, : h^{}(x) =(x).\]We adjusted each estimator using \(3000\) random variable samples (see Remark C.1), and tested them on \(1000\) samples of \(X\). For each method and response function, we evaluated predictions over \(20\) realizations of the data. Log mean squared error (MSE) box plots and plots of each method's estimator for a randomly chosen realization of the data are displayed in Figure 1.

The first thing we can notice from Figure 1 is that TSLS dominates the **linear** scenario, as expected, but behaves poorly in all others, showing the importance of nonlinear approaches.

On a second note, both variants of SAGD-IV performed competitively in all scenarios, with the KIV method being a strong competitor in **step** and **sin**. However, its performance was significantly worse for the **abs** response function. Additionally, it is worth noticing the competitive MSEs of the deep learning variant of SADG-IV, which performed consistently better than the other two neural-network-based algorithms.

Furthermore, an important observation is that, within kernel methods, Kernel SAGD-IV had the best stability/performance tradeoff, having the smallest variance across scenarios. In contrast, DualIV exhibited the highest variance. This can be explained by considering DualIV's hyperparameter selection algorithm (25, Section 4). In order to perform cross-validation to choose the best regularization parameters, the algorithm requires another (different) regularization parameter as an input, which must be chosen without any guidance. The authors state that it was set to a "small constant". This can explain the poor performance, given the importance of regularization parameters for ridge regression algorithms. We note our method does not suffer from this problem.

## 6 Binary Response

Here we outline how our methodology can be applied to a scenario where the quadratic loss is not the natural option: that of binary response models. We show that, by simply modifying the loss \(\) in the risk definition and, consequently, in the algorithm, we are able to attack this problem as it is currently formulated in the literature. This is in clear contrast with recent methods for NPIV estimation, which, when applicable, would require significant effort. We discuss the limitations of current methods and possible extensions in Appendix B.

In the binary response model we consider, the only change we need to make to the data generating process is the following:

\[Y=\{Y^{}>0\},\]

where \(Y^{}=h^{}(X)+\). That is, we observe a signal which indicates if the response variable is greater than the threshold zero. Although simple, this type of model captures important applications as, for instance, in economics/consumer behavior, where \(Y^{}\) is the utility the consumer gets from buying a product and we only observe \(Y\), the buy/no buy decision. The function \(h^{}\) captures the

Figure 1: (Left) Log MSE for each model under different response functions. (Right) Plots of each method’s estimator in a randomly selected realization of the data. On the left column, we have \(Y\) observations in green and the true structural function in black.

impact of the variable \(X\) on the consumers' utility. As usual in the literature, \(h^{}\) is interpretable relative to the scale parameter of \(\). Here, \(X,Z\) and \(\) satisfy the same assumptions as before.

In order to use one of the available NPIV regression methods in the literature, we must first identify a suitable risk functional, of the form shown in Equation (5). Define \(=h^{}(X)-[h^{}(X) Z]+\), so that \([ Z]=0\) and

\[Y=\{[h^{}(X) Z]+>0\}.\]

Denoting by \(F_{ Z}\) the conditional distribution function of \(\) given \(Z\), we find

\[[Y Z]=[[h^{}(X) Z]+>0  Z]=1-F_{ Z}(-[h^{}(X) Z]).\]

Assume that the distribution of \(\) given \(Z\) is always symmetric around zero, so that \(1-F_{ Z}(-y)=F_{ Z}(y)\). We then have \(r(Z)=F_{ Z}([h^{}](Z))\). Notice that any notion of risk for a given \(h L^{2}(X)\) would have to compare \(r(Z)\) and \([h](Z)\) through \(F_{ Z}\), which is an unknown function. Hence, without making further assumptions, it is not clear how to setup the risk in a useful way, which hinders current NPIV methods from tackling the problem.

As far as we know, _the only assumption in the literature_ which allows one to be _fully nonparametric_ in \(h^{}\) within the binary outcomes setting was first introduced in . It posits that \(\) is independent of \(Z\), and has known distribution function \(F\). Since \(=Y^{}-[Y^{} Z]\), it is already uncorrelated with any function of \(Z\). The assumption extends this to independence, and makes available the resulting distribution function.

Under this assumption, we conclude that \(r(Z)=F([h^{}](Z))\) and, by the negative log-likelihood of the Bernoulli distribution, a natural candidate for \(\) is

\[(y,y^{})=(y,F(y^{})),\]

where \(\) is the binary cross entropy function: \((p,q)=-[p q+(1-p)(1-q)]\). Then, the risk becomes

\[(h)=[(r(Z),F([h](Z)))].\] (12)

Whereas it is obvious that the quadratic loss satisfies Assumption 3.2, it is not clear whether the same is true for the loss \((y,y^{})=(y,F(y^{}))\). Nevertheless, if \(F(x)=(x) 1/(1+(-x/))\), the c.d.f. of a logistic distribution with scale parameter \(\), one can verify that Assumption 3.2 holds.

### Numerical Experiment

We mimicked the continuous response DGP making the necessary modifications:

\[Y=\{[h^{}(X) Z]+>0\}, X=Z_{1}++,\] \[Z=(Z_{1},Z_{2})([-3,3]^{2}), (0,)(0,0.1).\]

Here, \(\) is a scale parameter which was set to \(\). For response functions, we analyzed the **sin** and **linear** cases, since we must compute \([h^{}(X) Z]\) analytically in order to generate the data. This computation is easy for the selected scenarios, and we have

\[[X Z=z]=z_{1},[(X) Z=z]=}}{()}(z_{1})\]

As with the continuous response setting, we present log MSE results and samples from the resulting estimators in Figure 2.

We can immediately see that our kernel based estimator performed remarkably well for both response functions. The binary setup is significantly more challenging, as we are reducing the continuous response \(Y\) to a simple binary signal. Kernel SAGD-IV obtained results which are, on average, on par with the continuous response scenario and Deep SAGD-IV had more difficulty uncovering the true structural function.

## 7 Conclusion

In this work, we proposed SAGD-IV, a flexible framework for performing nonparametric instrumental variable regression. It is based on a novel way to tackle the NPIV problem, in which we formulate the minimization of the populational risk as functional stochastic gradient descent. We are then able to naturally consider non-quadratic loss function and incorporate various regression methods under one formulation. Under mild assumptions, we provided bounds on the excess risk of our estimator. Furthermore, we empirically demonstrated superior stability and state-of-the-art MSEs for continuous outcomes, as well as a promising performance for the binary regression setup.