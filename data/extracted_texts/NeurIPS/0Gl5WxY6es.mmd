# Grounding Multimodal Large Language Models

in Actions

 Andrew Szot\({}^{1,2}\) Bogdan Mazoure\({}^{1}\) Harsh Agrawal\({}^{1}\) Devon Hjelm\({}^{1,3}\)

Zsolt Kira\({}^{2}\) Alexander Toshev\({}^{1}\)

\({}^{1}\) Apple, \({}^{2}\) Georgia Tech, \({}^{3}\) Mila

_a.szot@apple.com, toshev@apple.com_

###### Abstract

Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains, including Embodied AI. In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, with the goal of leveraging the multimodal world knowledge of the MLLM. We first generalize a number of methods through a unified architecture and the lens of action space adaptors. For continuous actions, we show that a learned tokenization allows for sufficient modeling precision, yielding the best performance on downstream tasks. For discrete actions, we demonstrate that semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance. We arrive at these lessons via a thorough study of seven action space adapters on five different environments, encompassing over 114 embodied tasks.

## 1 Introduction

Multimodal Large Language Models (MLLMs), defined as Large Foundation Models that take as input text and images and generate text, have recently seen rapid progress and impressive performance . These models are important as they solve a large range of useful yet difficult natural language and image tasks, such as describing images, answering visual and textual questions, reasoning, and learning from a small number of examples. They have only recently improved to the point of being usable enough for general deployment with human non-experts .

While MLLMs are capable of describing real-world embodied concepts, their capabilities in embodied tasks are limited to using text for actions through generating code , representing actions as text , or extracting actions from internal representations . _Grounding_ MLLMs to generate actions extends their capabilities to embodied tasks, such as robot manipulation and navigation, and is of tremendous value for practical problems, potentially overcoming the high cost of training tabula rasa. Extending MLLMs to multimodal image generation enables object detection and segmentation, and image and video generation . In embodied settings, grounding MLLMs via predicting agent affordances and generating actions yields effective policies capable of generalizing to new tasks .

A key and open challenge in grounding MLLMs, which limits their capabilities in embodied tasks, is the gap between the native output space, natural language, and the action space of embodied agents. This problem is particularly acute in continuous action spaces, where low-level controllers may require a high degree of precision. Across the literature, a number of architectures and ways of handling action spaces have been proposed, but there has not been a systematic study of these designs. Our contributions generalize prior attempts to adapt MLLMs to generate actions through an empirical study on which principles and strategies are necessary to effectively close the gap between the action spaces of MLLMs and embodied agents. We study various grounding re-parameterizationstrategies, which we refer to as Action Space Adapter (ASA), across a range of embodiments, action spaces, and environments. In particular, we explore the following types of ASAs: (1) ASAs that directly generate actions from a new prediction policy using the MLLM hidden representations as input; (2) ASAs that reuse the native token space of the MLLM to encode actions; (3) and ASAs that introduce a new token space to encode the actions of the agent while adapting the MLLMs to predict these new tokens.

Further, we empirically identify important principles for designing ASAs. For continuous action spaces, learned tokenization with several vocabularies that residually model continuous actions gives the right modeling precision while using vocabularies of manageable sizes and, as a result, yields the best performance across all continuous control environments. This learned tokenization outperforms direct action prediction, indicating this approach allows the model to effectively learn a multimodal distribution over action spaces. In addition, the above tokenization strategy boosts performance when the policy is a MLLM, compared to other standard non-LLM-based policies, indicating that it manages to better tap into the knowledge of the model.

For discrete action spaces, we study ASAs that better align the embodied actions with the output space of the MLLM. We demonstrate that a semantic alignment between these - mapping discrete actions to semantically related tokens in the MLLM vocabulary - yields the best strategy compared to other adapters that either reuse or define a new vocabulary. The superiority of this strategy is evident in performance on environments with discrete action spaces and also in RL sample efficiency.

Finally, the above principles are thoroughly validated across five embodied AI environments, three of which are robotic continuous control and two with discrete actions as illustrated in Figure 1. Altogether, we consider 114 language specified tasks. In the continuous case, the best tokenization achieves \(72\%\) on CALVIN , up from \(68\%\) for direct action regression and \(28\%\) for uniform action tokenization; and \(84\%\) on Meta-World , up from \(61\%\) for direct action regression and \(75\%\) for uniform tokenization. Similarly, in the case of discrete actions, the proposed semantically aligned action tokens yield \(51\%\) on LangR , up from \(42\%\) for direct action prediction.

## 2 Related Work

Prior works propose different Action Space Adapters (ASAs) to adapt MLLMs into policies. Some works use LLMs or MLLMs as zero-shot policies by prompting them to output text or code that can be executed as actions [32; 33; 34; 35; 18; 36; 37; 38]. The ASA in this case is a given executor or low-level controller that takes text as input and outputs actions in the environment. Other works investigate adapting MLLMs for actions, but focus on a single ASA and environment. For example, RT-2  uniformly discretizes continuous actions and predicts tokens corresponding to each of the action dimensions. RoboFlamingo , Lamo , and LLaRP  use an MLP to predict an environment action from an LLM hidden state. GFlan  treats discrete actions as text and ranks actions by the LLM log

Figure 1: We empirically analyze how to ground MLLMs in actions across 114 tasks in continuous and discrete action spaces. In each environment, we train a multi-task policy with different Action Space Adapters (ASAs) to re-parameterize the MLLM to output actions. For continuous actions, learning a tokenization with several tokens per-action performs best (Residual VQ). For discrete actions, mapping actions to semantically related language tokens performs best (Semantic Tokenization).

probability to form a distribution over actions. At a high level, our work is distinct in that we study a variety of methods across multiple environments for learning ASAs. We focus on tasks with low zero-shot VLM performance, such as low-level control or long-horizon planning tasks. We summarize the differences between our investigation and prior work adapting VLMs for action in Appendix A.

Investigating action representations in embodied settings is not new. Some works learn representations of actions to help generalization to new actions or operating in large action spaces [41; 42] in the context of Reinforcement Learning (RL). Our study proposes ASAs for tokenizing continuous actions, and other works use different types of discretization or tokenization strategies on continuous action spaces. [43; 44] use k-means to discretize continuous actions to help learn from multimodal behavior datasets, such as from play data or data from different experts. VQ-BeT  finds learning a residual VQA (RVQ) codebook for continuous actions works best but does not apply this idea to MLLMs.  predicts actions as text.  learns a multi-task transformer policy and models actions with a diffusion head.

More broadly, prior works have adapted MLLMs for modalities other than actions, such as object bounding boxes and image generation, both being continuous in nature while the latter of high dimension. For example, [27; 48] train MLLMs to output spatial reference tokens to ground text responses in image regions. For image generation,  adapt MLLMs to generate image patches; [50; 51] tokenize images using a VQ-VAE model and adapt MLLMs to generate images by decoding these image tokens, which has inspired us to use the same learned tokenization;  uses an RVQ model  to generate images, similarly to our best performing tokenization scheme.

## 3 Method

In order to solve an embodied task, an agent learning in an interactive environment must select a decision from a set of valid actions. For example, an action space could be a set of keyboard presses for a video game or a real-valued vector that controls a robotic manipulator. Our work studies how to best adapt a MLLM, which is originally trained to output text tokens, to instead model actions from a given environment. We refer to the module that bridges a MLLM with a certain action space as an _Action Space Adapter_ (ASA) (see Figure 2).

### Problem Setting

Our analysis focuses on language-specified tasks with visual observations. Specifically, we consider a goal-specified Partially-Observable Markov Decision Process (POMDP)  that has an observation space \(\), action space \(\), and goal space \(\). For brevity, we omit other elements of the MDP. In our setting, \(\) is a textual description of the task to solve. \(\) consists of RGB visual perception and agent proprioception. We consider a range of different action spaces \(\) that broadly fall into two categories - discrete and continuous. The primary objective is to learn a language-conditioned policy that maps

Figure 2: Generic architecture studied here for adapting MLLMs for action-specific decision making. The MLLM takes the embedding of the task instruction, prompt, and visual tokens as input. The MLLM then autoregressively predicts a sequence of \(m\) action tokens. These action tokens are then decoded into an environment-specific action.

observations and the instruction text to an action \((a|o,g)\). As later described in Section 3.3, we learn this policy through supervised fine tuning from expert demonstrations or reinforcement learning that maximizes the expected discounted cumulative reward of the POMDP.

### From Vision and Language to Action

The process studied here for adapting MLLMs for decision making is illustrated in Figure 2. The MLLM policy takes as input a textual instruction describing the downstream task, a sequence of past observations in the task and outputs an action in the agent's action space. In the bottom left of Fig. 2, the task description, as well as the environment description, are first encoded to produce language embeddings. To these embeddings, the MLLM then appends a sequence of visual embeddings from the current observation \(o_{t}\). Since visual embeddings can often be comprised of a large number of tokens (the popular LLaVA-1.5 model  has 556), we introduce a downsampling layer to enable the MLLM to attend over a longer history of observations. In practice, we take the downsampling layer to be a Perceiver model , a learnable transformation that reduces the number of tokens from the visual encoder before being used as input to the MLLM.

The sequence of language and visual embeddings is passed through the MLLM, whose final hidden state \(h_{t}^{1}\) encodes the entire input. The ASA, whose trainable parameters are denoted \(\), is comprised of three parts: (1) an adapter head, (2) an adapter embedding, and (3) an adapter decoder. The hidden state is first passed through the adapter head to produce action tokens \(u_{t}^{1}=A_{}(h_{t}^{1})\). The action tokens are then embedded using the action embedding into \(E_{}(u_{t}^{1})\), and passed autoregressively through the MLLM to produce further hidden embeddings \(h_{t}^{2},,u_{t}^{m}\) and associated action tokens \(u_{t}^{2},,u_{t}^{m}\), resulting in total \(m\) tokens per time step. The predicted action tokens are then decoded into the final action \(a_{t}\) by the adapter decoder, which produces the final action \(a_{t}=D_{}(u_{t}^{1},..,u_{t}^{m})\). As \(a_{t}\), it is then executed in the environment to produce \(o_{t+1}\), and the process continues.

Next, we describe possible ASA implementations for discrete and continuous action spaces.

#### 3.2.1 Discrete Action Spaces

We define the following action spaces adapters for a discrete action space \(\):

**Categorical Prediction (Pred)**: Implement the action space adapter as an MLP network, which predicts the logits of a categorical distribution over environment actions from the MLLM hidden state. The adapter head is an MLP that maps the hidden state \(h^{1}\) directly to an action \(a\). This amounts to producing a single action token \(u^{1}\), which directly corresponds to the action \(a\), with the action decoder being an identity map. Both the adapter head and token embeddings are initialized from scratch. This type of ASA is used by .

**Semantic Language (SemLang)**: The action space adapter predicts natural language text that maps to a discrete action. First, each action \(a\) is described with freeform text tokenized as \((l_{1},,l_{m})\). The MLLM then autoregressively predicts a sequence of \(m\) tokens, which are then decoded by the adapter decoder to the corresponding action. For example, in an action space choosing a high-level skill \(a\) could be described as "pick apple", which is tokenized as \(\) with the LLaMA tokenizer. The MLLM then must sequentially predict token \(5839\), then token \(26163\) to call this action. Sequences of tokens corresponding to invalid actions are either avoided entirely with the token filter described in Section 3.3 or treated as a no-op. Both the adapter head and the token embeddings are re-used to be the pretrained LLM's language head and embedding layer, respectively, meaning no additional parameters over the pretrained MLLM are added. This type of ASA is used by .

**Non-Semantic Language (Lang)**: Actions are mapped to language tokens, but instead of semantically meaningful descriptions of the actions as with SemLang, the actions are mapped to sequences of numbers. For example, "pick apple" is represented with the string "5 3". The policy must then output the tokens corresponding to this text to call this pick action. Note that we can pick any text for this mapping and the choice of integers is arbitrary. However, the selected text is not semantically representative of the action.

#### 3.2.2 Continuous Action Space Adaptors

We define the following four ASAs for a continuous \(D\)-dimensional action space \(\): the first ASA predicts in the original action space while the other three use tokenization. At training time, we learn a policy to predict these action tokens from the ASA. At test time, we employ an action decoder that maps these action tokens to actions in the original space \(\).

**Continuous Regression (Pred)**: Regress to the original continuous action from the MLLM hidden state \(h_{t}^{1}\). This is achieved via a single-layer MLP network, which is trained using MSE loss. This ASA is used by [20; 39].

**Uniform Action Tokenization (Uniform)**: The simplest approach is to use uniform binning of the action space. In particular, we express each action as a sequence of \(D\) tokens by quantizing each of the \(D\) action dimensions into one out of \(K\) uniform bins:

\[=(k_{1} k_{D}) a_{d}(k_{d},d)\]

where \((k,d)\) denotes the \(k^{}\) bin along the \(d^{}\) action dimension. If \(m_{d}\) and \(M_{d}\) denote the lower and upper bounds respectively of the \(d^{}\) action dimension, then its definition reads \((k,d)=[m_{d}+k-m_{d}}{K},m_{d}+(k+1)-m_{d}}{K}]\). At test time, we decode predicted action tokens to the center of the corresponding bins for each dimension. This type of ASA is used by .

**Vector Quantized Tokenization (VQ)**: To adapt the tokenization to the particular action space, we propose to use learned tokenization. In particular, we express each action as a single token that corresponds to the closest action code from a learned codebook \(V\). Using encoder network \(f_{}\) that maps actions to a latent embedding space:

\[(a)=(k_{1}) k_{1}=_{k}||f_{}(a)- v_{k}||_{2}^{2}\]

where \(v_{k} V\). The codebook \(V\) of size \(K\) is learned over an offline dataset \(\) of actions using a VQ-VAE  trained with the mean-squared error for action reconstruction and commitment loss. We overwrite \(K\) infrequently used tokens from the LLM vocabulary to represent \(V\). We defer the full details of this tokenization process to Appendix C.2.

**Residual Vector Quantized Tokenization (RVQ)**: Precise control requires precise action modeling that can suffer after tokenization. To increase the precision of a learned tokenization, we further investigate the use of a sequence of several action tokens as in Uniform. Similar to VQ, these tokens are from \(M\) action codebooks \(V_{m},m\{1,,M\}\). However, each codebook models the residual space obtained after modeling the action using preceding codebooks, thus each subsequent token captures increasingly finer action information:

\[(a)=(k_{1}, k_{M}) k_{m}=_{k} (f_{}(a)-_{i=1}^{m-1}v_{k_{i}}^{i})-v_{k}^{m} _{2}^{2}\]

where \(v_{k}^{i} V_{i}\) is the \(k^{}\) code from the \(i^{}\) codebook. Such tokenization can be learned using Residual VQ-VAE [RVQ-VAE, 52] on an offline dataset of actions. The actual number of token sequences we can represent is \(K^{M}\). Hence, RVQ presents the opportunity to exponentially increase the action space quantization without having to drastically increase the size of the learned individual codebooks.

### Training

We use LLaVA-1.5-7B  as the base MLLM. We finetune the MLLM with interactive (i.e., action-labeled) data to make it more suited for interacting with a embodied and interactive environment.

**Supervised Fine Tuning (SFT) with Expert Demonstrations:** We finetune the MLLM for interactive tasks using a dataset of expert demonstrations. Each demonstration contains (1) a language description of the task, (2) a sequence of observations, and (3) a sequence of actions that successfully solve the task. Note that in this work, we are primarily interested in learning imitation policies from offline data, which can be extended to offline reinforcement learning if per-timestep rewards are included in the dataset. Specifically, we train the MLLM with supervised learning to predict the expert actions from the observations and language description in the data. While the pre-trained LLM and the visual encoder remain frozen, we finetune the ASA, the visual downsampler, and parts of the LLM with LoRA . In total, the model has \( 100M\) learnable LLM parameters and \( 40M\) learnable downsampler and ASA parameters. The learned tokenization schemes (RVQ and VQ) have an additional pre-training phase, where the VAE models are first trained on actions from the offline dataset and then frozen to prevent further updates in later stages.

**Reinforcement Learning (RL) from Environment Feedback** We can also optionally finetune the MLLM to optimize an environment reward using RL. However, predicting actions in the MLLM token space dramatically increases the number of possible action predictions, with many possible predictions corresponding to no valid action. For example, there are 32,000 tokens in the LLAMA text tokenizer, giving \(32,000^{m}\) possible predictions by the model with \(m\) tokens per action. This makes exploration difficult in RL as only a small fraction of the possible actions are valid. We therefore use a _token filter_ to restrict the autoregressive sampling to only be from token sequences corresponding to valid actions. The token filter is a function \(M(l_{t}^{1},,l_{t}^{j-1})\) that produces a binary mask over all tokens to represent valid tokens for the \(j\)th decoding step.

## 4 Experiments

### Experimental Settings

We study adapting MLLMs for action across a variety of environments with different embodiments and action spaces. All environments provide RGB visual observations and a natural language instruction specifying the goal to achieve. We provide the important environment details below and defer complete details to Appendix B.

**CALVIN**: This manipulation benchmark tests the ability of a tabletop robot to interact with an object to complete a natural language instruction. The continuous actions specify 6DoF end-effector control and the binary gripper state. The observation is a \(200 200\) RGB image from a fixed-position camera. We use the \(ABC D\) split of the benchmark with 34 tasks, and the agent is evaluated on unseen instruction phrasings and table background.

**Meta-World**: We use the ML-45 version of this tabletop manipulation benchmark which has 45 tasks. The action space is continuous control specifying 3DoF end-effector translation and the continuous gripper state. The observations are \(200 200\) RGB images from a fixed camera. The agent is evaluated on unseen object and robot starting states.

**Habitat Pick (HabPick)**: A mobile manipulation robot must pick up an object specified by name from a receptacle. The continuous actions specify the 7DoF relative joint positions of the arm, the 2D base velocity, and the gripper state. The observations are \(336 336\) RGB images from the robot's egocentric head camera. The instruction specifies the name of the object type to pick up. The evaluation distribution is on unseen houses and new arrangements of objects.

**BabyAI**: BabyAI is a grid world task where an agent navigates and interacts with objects to complete an instruction. The discrete action space consists of navigation and interaction actions. The observation is a \(200 200\) RGB top-down view. We use the five tasks from , and we report generalization to instructions rephrased with synonyms.

**Language Rearrangement (LangR)**: A mobile manipulation robot must rearrange objects to complete instructions like "store all the fruit in the fridge". The discrete actions are 70 high-level skills to interact with objects and navigate. The observation is a \(336 336\) RGB head camera.

Figure 3: Comparing ASAs for continuous and discrete action spaces across 5 environments. For continuous actions, the RVQ tokenization performs best. For discrete actions, SemLang performs best. Each bar gives the average over all tasks in the environment with the full breakdown in Appendix E.

Evaluation instructions test generalization to unseen houses and 10 unseen instruction datasets measuring paraphrastic robustness and behavior generalization.

In all environments, we report the success rate as the fraction of episodes in which the agent completed the language instruction. We use the success criteria provided by each environment. We train a policy per action adapter for each environment and report the generalization performance in the main text. When reporting a single success rate per environment, it is the success averaged between all evaluation episodes containing all tasks. We give the full per-task breakdown for results in Appendix E. CALVIN, Meta-World, HabPick, and BabyAI provide expert demonstrations succeeding at the task. CALVIN has \(17.9k\) from humans, Meta-World \(22.5k\) from a scripted policy, HabPick \(6.7k\) generated from an RL policy, and BabyAI \(5k\) from a scripted policy. Full details on the train and evaluation setups per environment are in Appendix B.

We train with supervised finetuning for CALVIN, Meta-World, HabPick, and BabyAI. We train with reinforcement learning on Language Rearrangement. As described in Section 3.3 we train \( 140M\) parameters with LoRA . We use the AdamW optimizer  with a learning rate of \(3^{-4}\), a warmup period of \(10\%\) of the total number of training steps, and cosine learning rate decay to \(0\) by the end of training. For RL, we use PPO . For the learned tokenization action space adapters, we, by default, use a codebook size of 512 with 512 dimensions per codebook element. Complete hyperparameter and policy details are in Appendix C.

### Continuous Action Space Adapter Comparison

We first study adapting MLLMs through Uniform, Pred, VQ, and RVQ action space adapters for the continuous action environments CALVIN, Meta-World and HabPick.

**RVQ is the best performing continuous action ASA.** The results in Figure 3 show that the RVQ action adapter consistently outperforms all other ASA approaches across all environments. While Pred is the second best performing method on all tasks, except on Meta-World, RVQ outperforms it by a \(12\%\) average absolute difference. One hypothesized reason for this is that Pred only learns unimodal distributions of actions, which hurts performance when learning from diverse demonstrations . Another potential reason is the tokenization from RVQ allows the MLLM to better leverage its existing knowledge, whereas the Pred ASA requires training a new MLP network from scratch.

Uniform performs poorly on the majority of the tasks, where RVQ outperforms on average by a \(27\%\) absolute increase. A reason for this is that the Uniform discretization can fail to accurately represent the continuous actions. The performance of Uniform is also closely related to the action dimension. In Meta-World with 4 action dimensions, Uniform performs well. However, Uniform suffers with the 7 action dimensions in CALVIN and the 10 action dimensions in HabPick.

RVQ also outperforms VQ by a \(18\%\) absolute difference averaged over all environments. This is due to VQ having worse action reconstructions than RVQ. In Meta-World, both RVQ and VQ policies reach a similar cross-entropy loss on holdout trajectories during finetuning. However, on this same data, RVQ has a reconstruction mean squared error (MSE) of \(0.005\) while VQ has a 10x higher reconstruction MSE of \(0.05\). Increasing the VQ codebook size does not close this gap. We vary the VQ codebook size in powers of \(2\) from \(2^{7}\) to \(2^{11}\). Figure 3(b) shows the VQ reconstruction loss decreases with larger codebooks but does not even close the gap to the \(2^{7}\) RVQ codebook size. This poor reconstruction manifests in poor downstream policy performance as demonstrated by Figure 3(a)

Figure 4: (a,b) show the effect of the number of codes in the codebook for RVQ and VQ on final policy success rate (see (a)) and reconstruction on unseen action trajectories in Meta-World (see (b)). (c,d) show the effect of number of codebooks on final policy success rate (see (c)) and action reconstruction (see (d)). All metrics are computed on Meta-World.

where policies trained with the VQ ASA plateau in success rate at codebook size \(2^{9}\). VQ policies even decrease in performance at codebook size \(2^{11}\), potentially due to overfitting to the large codebook.

We further characterize the performance of RVQ and VQ in Figure 5 by breaking down the performance per task group in Meta-World and CALVIN. The task groups, which are fully listed in Appendix B.6, correspond to tasks with related required behaviors. Both RVQ and VQ do similarly on "articulated" object interactions (like opening drawers or doors). These tasks require less precise control since many contact points on the articulated link and broad pushing or pulling behavior can achieve the desired behavior. On the other hand, RVQ outperforms VQ on "pressing" tasks that require pushing a button. These tasks require more precise control since the agent needs to push the button all the way to a desired state. VQ often reaches the button but fails to press it all the way. The same is also true of other precise control tasks like picking, pulling, and rotating.

A potential explanation of RVQ's success can be attributed to adaptive localization of the model's errors, similar to prior work in residual reinforcement learning  and Bellman error bases .

**A sufficient codebook size and number of codebooks are necessary for RVQ.** In Figure 3(a), we show that RVQ policy performance improves in performance with a larger codebook size in Meta-World. Notably, RVQ performs poorly at \(29\%\) success rate with codebook size \(16\) compared to \(84\%\) success at codebook size \(512\). These observations also align with the codebook size decreasing reconstruction error in Figure 3(b). In Figure 3(c), we compare the effect of the number of codebooks on performance. As earlier discussed with the performance of VQ, one codebook results in poor action reconstruction and, thus, bad policy performance. However, increasing the number of codebooks too much to 6 also hurts performance despite decreasing reconstruction loss. Likewise to the finding that Uniform performs poorly with larger action dimension since there are more tokens per action, increasing the number of codebooks also hurts policy learning.

**RVQ tokens transfer to new tasks**. We take the model trained on the 45 Meta-World tasks and finetune it on 5 unseen tasks. We collect 50 demonstrations for per task and finetune the policy on all task data. We use the same RVQ ASA trained only on data from the 45 tasks. Figure 5(a) shows the success rate of adapting RVQ compared to an Pred ASA. RVQ outperforms Pred across all tasks, achieving a \(50\%\) vs. \(20\%\) overall success rate. This demonstrates the RVQ tokens are flexible enough to be applied to new tasks.

**The gains from RVQ are unique to MLLMs.** Next, we analyze the unique interaction between the RVQ tokens and the MLLM policy. While we demonstrated that the RVQ ASA performs best, is this improvement due to the MLLM being able to leverage these new tokens or the added action representation ability from the separately trained RVQ decoder? To test this, we compare to two policy architectures that do not use LLMs:

* **Scratch**: This is the same architecture as the MLLM-based policy, but with a smaller 300M parameter non-pretrained transformer.
* **RT-Inspired**: This method uses a ResNet visual encoder, pretrained Flan  language embedding and decoder transformer-based policy. The entire policy is trained from scratch. This method is inspired by RT-1 , which does not have publicly released code.

Table 1 compares the effect of Pred versus RVQ ASAs on CALVIN, Meta-World and HabPick for these three policy policy architectures. As already established for the MLLM, RVQ is consistently better than VQ. However, for the same policy architecture trained from scratch, RVQ can hurt the performance over Pred. In CALVIN the success drops \(-7\%\) and in Meta-World the performance

Figure 5: RVQ and VQ success per-task grouping (defined in Supp. B.6) on CALVIN and MetaWorld.

[MISSING_PAGE_FAIL:9]

### Empirical Comparison to Prior Work

The contributions of this work are an empirical analysis of ASAs under controlled settings on various embodied environments. Direct comparisons to prior work are challenging due to different training algorithms, policy architectures, or assumptions about input modalities. Regardless, in this section, we seek to contextualize our RVQ and SemLang MLLM results against prior work. In Meta-World, to the best of our knowledge, RVQ at \(84\%\) success on ML-45 sets a new state-of-the-art result, compared to \(79\%\) from DualMind . In CALVIN, RVQ at \(72\%\) success underperforms a similar work RoboFlamingo which achieves \(82\%\) success on the \(ABC D\) split. However, RoboFlamingo uses a different MLLM and uses an additional gripper camera input. In Language Rearrangement, SemLang sets a state-of-the-art result with \(51\%\) success compared to \(42\%\) from LLaRP . In BabyAI, SemLang at \(40\%\) success rate underperforms GFlan , which achieves \(55\%\) success. However, we use RGB visual observations, while GFlan operates from a compact, ground truth language state description. In Appendix A.1, we compare these differences in more detail.

## 5 Limitations and Conclusion

In this work, we studied various action space adapters (ASAs) across a variety of embodiments, action spaces, and environments. We provide a generalization of prior works through the lens of action space adaptors, and for both discrete and continuous action spaces demonstrate designs that we show can leverage the knowledge within the MLLM. Our findings conclude that for continuous actions, it is best to learn action tokens that accurately model the action distribution, while for discrete actions, it is best to reason over semantic language descriptions of actions. We verify these ideas across 114 embodied AI tasks in 5 diverse environments.

A limitation of our work is all our analysis is under a single MLLM (LLaVA). Another limitation is that RVQ, the best performing ASA in continuous action spaces, requires collecting demonstrations to train the VQ model. Our analyses are also under only a single LoRA training setting. Future analyses can explore different base MLLMs under different training regimes like full LLM finetuning. While our investigation of ASAs enables connecting a MLLM to various action spaces, the performance of these methods is still subpar for real-robot deployment where high success and safety are critical. MLLMs with the best ASA still struggle on simple environments like BabyAI, only achieving 40% success rate. Further work is needed to improve the performance of these methods for real-world usage. Our investigation also only studies adapting MLLMs through behavioral cloning or on-policy RL. Future work can investigate if the choice of ASA varies when adapting the MLLM with other learning algorithms such as off-policy RL or offline RL.