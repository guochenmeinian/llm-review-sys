# Gradient-free Decoder Inversion

in Latent Diffusion Models

Seongmin Hong\({}^{1}\)   Suh Yoon Jeon\({}^{1}\)   Keyonghyun Lee\({}^{1}\)

Ernest K. Ryu\({}^{2,}\)1   Se Young Chun\({}^{1,3,}\)1

\({}^{1}\)Dept. of Electrical and Computer Engineering, \({}^{3}\)INMC & IPAI, Seoul National University

\({}^{2}\)Dept. of Mathematics, University of California, Los Angeles

{smhongok, euniejeon, litiphysics, sychun}@snu.ac.kr,eryu@math.ucla.edu

Project page: https://smhongok.github.io/dec-inv.html

###### Abstract

In latent diffusion models (LDMs), denoising diffusion process efficiently takes place on latent space whose dimension is lower than that of pixel space. Decoder is typically used to transform the representation in latent space to that in pixel space. While a decoder is assumed to have an encoder as an accurate inverse, exact encoder-decoder pair rarely exists in practice even though applications often require precise inversion of decoder. In other words, encoder is not the left-inverse but the right-inverse of the decoder; decoder inversion seeks the left-inverse. Prior works for decoder inversion in LDMs employed gradient descent inspired by inversions of generative adversarial networks. However, gradient-based methods require larger GPU memory and longer computation time for larger latent space. For example, recent video LDMs can generate more than 16 frames, but GPUs with 24 GB memory can only perform gradient-based decoder inversion for 4 frames. Here, we propose an efficient gradient-free decoder inversion for LDMs, which can be applied to diverse latent models. Theoretical convergence property of our proposed inversion has been investigated not only for the forward step method, but also for the inertial Krasnoselskii-Mann (KM) iterations under mild assumption on cocoercivity that is satisfied by recent LDMs. Our proposed gradient-free method with Adam optimizer and learning rate scheduling significantly reduced computation time and memory usage over prior gradient-based methods and enabled efficient computation in applications such as noise-space watermarking and background-preserving image editing while achieving comparable error levels.

## 1 Introduction

Deep generative models have been actively investigated over the past decade across numerous modalities such as image [12; 41; 42; 37; 11; 35; 38], video [5; 4; 47; 52; 45; 46], audio [18; 21; 22] and molecular structure [17; 13]. They can sample new data points from the distribution of the training set, can model priors to solve regression problems, and can be used for applications like editing, retrieval, and density estimation. Representative classes of deep generative models include generative adversarial networks (GANs), variational autoencoders (VAEs), normalizing flows (NFs), and diffusion models (DMs). They _used to be_ known not to simultaneously satisfy the three key requirements: (i) high-quality samples, (ii) diversity, and (iii) fast sampling, also referred to as the generative learning trilemma . However, recent DMs such as Rectified Flow , Adversarial Diffusion Distillation  and Consistency models  require just one step for sampling. Thus,DMs have overcome the trilemma and the old problem of slow sampling in recent DMs is no longer a concern as shown in the last row of Table 1. As a result, DMs have become one of the most prominent deep generative models, especially for image and video. However, there are still remaining challenges for DMs such as achieving _invertibility_, which is well-supported by other models (see Table 1).

Achieving invertibility for deep generative models has been an important topic. In GANs, prior works proposed GAN inversion techniques  (_i.e._, to find the latent vector \(\) that generated the image) and their applications such as real image editing . NFs are naturally invertible deep generative models by construction with related interesting applications . There have also been attempts to achieve and utilize invertibility for DMs. The most popular one is the naive DDIM inversion that reverses the sampling process of DDIM's deterministic denoising , _i.e._, adding the estimated noise. While the naive DDIM inversion enables image editing, it is known to be somewhat inaccurate for seeking the true latent , for watermarking  and for background-preserving image editing . To better ensure the exactness of the inversion, several prior works [44; 51; 31; 14; 10] have proposed more tailored algorithms than the naive DDIM inversion.

The use of latents in DMs has made ensuring invertibility more difficult. Latent diffusion models (LDMs) were proposed to move the diffusion denoising process from the pixel space to the (low-dimensional) latent space, thus efficiently generating high-quality and large-scale samples . This issue may be critical since many recent popular DMs operate in latent space [37; 47; 52; 27; 40; 45; 5]. However, latents are usually lossy compression of pixels, making one-to-one mapping between the latent and pixel spaces very challenging. Thus, the accuracy of inversion in LDMs is typically lower than that in pixel-space [44; 31; 14], requiring additional efforts to compensate for it as illustrated in Figure 1b. One could naively employ a gradient-based GAN inversion method to the decoder of LDMs, but it required very large GPU memory and computation time , especially for large-scale LDMs. Moreover, recent video LDMs [5; 4; 47; 52; 45; 46] generate dozens of frames at once, making gradient-based GAN inversion infeasible in a single GPU.

In this work, we aim to achieve better invertibility in LDM as shown in Table 1. Specifically, we propose decoder inversion to overcome the difficulty of ensuring invertibility in LDM due to the inexact encoder-decoder pair. The proposed decoder inversion is gradient-free, which is faster and more memory-efficient than the gradient-based methods suggested in GAN inversion. Section 3 describes the motivation and analysis of our decoder inversion, including the theoretical convergence guarantee under mild assumption not only for the vanilla forward step method, but also for the inertial Krasnoselskii-Mann (KM) iterations. The assumption for our theorems is also validated by showing experimental convergence to the ground truth as well as the effectiveness of momentum. Section 4 described how to refine our algorithm by integrating popular optimization techniques such as Adam  and learning rate scheduling as practical extensions of our gradient-free decoder inversion, demonstrating that our method works well on various latest LDMs compared to existing gradient-based methods. Lastly, Section 5 showcases interesting applications (noise space watermarking and background-preserving image editing) where our proposed gradient-free decoder inversion can be effectively utilized. The contributions of this work are:

    &  \\  & High-quality & Diversity & Fast & How to achieve \\  & samples & sampling & invertibility? \\  GAN & ✓ & ✗ & ✓ & Optimization / learning-based GAN inversion \\  NF & ✗ & ✓ & ✓ & Naturally exact inversion \\  DM & ✓ & ✓ & ✗\({}_{(21)}\)✓\({}_{(24)}\) & Diffusion: Solving ODE backward (LDM) Decoder: Optimization-based inversion \\   

Table 1: Comparison of deep generative models. DMs have overcome the _generative learning trilemma_, but they still lack invertibility compared to other models. In particular, LDMs have necessitated additional decoder inversion that has traditionally been addressed through memory-intensive and time-consuming gradient-based optimization methods. Here, we propose a method for efficiently (_i.e._, gradient-free) ensuring invertibility in LDM.

* proposing a gradient-free decoder inversion algorithm to achieve better invertibility in diverse latest LDMs. Our method has the following advantages with experimental evidences:
* **Fast**: up to 5\(\) faster, 1.89 s vs 9.51 s to achieve -16.4 dB (Fig. 2(c) and Tab. S1c)
* **Accurate**: up to 2.3 dB lower, -21.37 dB vs -19.06 dB in 25.1 s (Fig. 2(b) and Tab. S1b)
* **Memory-efficient**: up to **89%*
* can be saved, 7.13 GB vs 64.7 GB (Fig. 2(b))
* **Precision-flexible**: 16-bit vs 32-bit (Fig. 3)
* theoretically guaranteeing the convergence to the ground truth not only for the vanilla forward step method, but also for the inertial KM iterations,
* showcasing that our proposed gradient-free decoder inversion can be used in interesting applications.

## 2 Backgrounds

### Latent diffusion models (LDMs)

Stable Diffusion 2.1  is a widely known open-source text-to-image generation model. LaVie  is a text-conditioned video generation model, which can generate consecutive frames per inference. InstaFlow  is a one-step text-to-image generation model that can generate images whose quality is as good as Stable Diffusion. These three LDMs will be used in all the experiments of our work.

Exact inversion of accelerated DMs.Another factor that makes the exact inversion of DMs difficult, besides the use of latents, is acceleration. Due to the use of high-order ODE solvers, the exact inversion of DPM-Solvers [25; 26] is not as straightforward as in DDIM. Exact inversion algorithms for some DPM-Solvers have been proposed , but obtaining the exact inverses for other accelerated DMs [23; 40; 43; 24] has not been explored.

### Optimization-based GAN inversion

For a target image \(\), existing optimization-based GAN inversion methods typically perform the following optimization with respect to a latent vector \(\) (usually assumed to lie on a simple distribution such as \((0,1)\)):

\[_{}(,()),\] (1)

where \(\) is a distance metric and \(\) is the generator part of GAN . Most works solve Eq. (1) through backpropagation with an optimizer. For optimizers, Adam  is employed in , and

Figure 1: (a) The difference between LDM and pixel-space DM lies in the use of the decoder (\(\)). (b) This difference has caused the performance gap in exact inversions. Pixel-space DMs and gradient-based GAN inversion methods are located on the right side due to iterative gradient back-propagations. If gradient-based decoder inversions are used, which are computationally intensive, LDM is located on the rightmost side to address the lossiness of latents. Our proposed gradient-free decoder inversion method allows us to efficiently handle the transformation between latent and pixel spaces.

L-BFGS  is employed in . Some more recent works optimized the transformed latent feature \(\), which can be defined as \(=()\), in StyleGAN .

To exploit the prior knowledge that \(\) lies on the range space of an encoder, prior works often perform the following two methods:

* The output of the encoder \(\) is used as an initial point of the optimization process.
* The domain-guided encoder is used to regularize the latent code within the semantic domain of the generator. For example, \(_{}\|x-()\|_{2}+_{}\|- (())\|_{2}\).

Since we already have the encoder, we employ (A), too. On the other hand, we will utilize the encoder in a novel way over (B).

### Gradient-based decoder inversion in LDMs

Recent works  performed the same as Eq. (1) (hence they are iterative as illustrated in Figure 0(b)) for the decoder inversion:

\[_{_{0}}(,(_{0})),\] (2)

with a gradient-based method. For simplicity, we omit the subscript by setting \(_{0}=\). The vanilla gradient descent algorithm can be expressed as follows:

\[^{k+1}=^{k}-_{}(,(^{k} )).\] (3)

Advancements in backpropagation algorithms and libraries have made the computation of \(_{}(^{k})\) more efficient, but it still requires a significant amount of GPU memory usage and lengthy runtime compared to inference. As the size of generated outputs of recent LDMs continues to increase, the computation of \(_{}(^{k})\) through backpropagation is getting more and more burdensome.

## 3 Gradient-free decoder inversion in LDMs

In this work, as an alternative method to Eq. (3), we propose the following decoder inversion method, which is a form of the forward step method :

\[^{k+1}=^{k}-(((^{k}))-()),\] (4)

where the initial latent vector estimate \(^{0}=()\) that is the same as (A) in Section 2.2.

### Motivation

The definition of the decoder inversion problem for the given image \(\) is as follows:

\[*{find}_{^{F}}=().\] (5)

Since directly dealing with Eq. (5) is impractical, we relax it as solving the following problem:

\[*{find}_{^{F}}()= (()).\] (6)

The following Remark 1 demonstrates that solving Eq. (6) is easier than solving Eq. (5).

**Remark 1** (Eq. (6) is easier than Eq. (5)), \(\{|=()\}\{|()= (())\}\).

Remark 1 is true because \(=()()=( ())\). Equation (6) is equivalent to the following:

\[*{find}_{^{F}}=-( (())-()), \{0\}^{C}\] (7)

where Eq. (7) refers to finding a fixed point of an operation \(-((())-())\) whose another form is Eq. (4), the forward step method.

### Convergence analysis on forward step method

Here, we demonstrate that our method converges to a fixed-point under reasonable conditions. Furthermore, although we solved an easier problem represented by Eq. (6) rather than directly solving Eq. (5), we show that our proposed method surprisingly finds the true solution of Eq. (6).

**Theorem 1** (Convergence of the forward step method).: _Let \(>0\), \(0<<2\), and \(^{N}\). Assume \(()=()-()\) is continuous. Consider the iteration_

\[^{k+1}=^{k}-^{k} k=0,1,\] (8)

_Assume \(^{}\) is a zero of \(\) (i.e., \(^{}=0\)) and_

\[^{k},^{k}-^{}\| ^{k}\|_{2}^{2} k=0,1,\] (9)

_Then, \(^{k} 0\). If, furthermore, \(^{k}^{}\), then \(^{}\) is a zero of \(\) (i.e., \(^{}=0\))._

Proof outline.: Equation (9) makes \(\|^{k+1}-^{}\|_{2}^{2}\|^{k}-^{}\|_{2}^{ 2}-(2-)\|^{k}\|_{2}^{2}\). Then, sum for \(k=0,,\). 

Theorem 1 assumes that Eq. (9) holds, which is refered as \(\)-cocoercivity. The assumption makes sense because we expect \(\). In fact, for linear autoencoders, it is proven by Baldi and Hornik  that \(=\) as the following Remark 2:

**Remark 2** (\(ED=\) in linear autoencoders).: _Let \(^{N}\) be a random vector such that \([^{}]=^{N N}\). Assume \(\) has distinct positive eigenvalues. Consider the optimization problem_

\[^{N F},\,E^{F N}}{} _{}\,\|-DE\|_{2}^{2}.\]

_Then, \(ED=I\) (identity matrix)._

\(\) is \(1\)-cocoercive, which suggests that our assumption is reasonable. In Section 3.4, we further demonstrate that the assumption is reasonable experimentally and \(^{k}\) actually converges well.

### Convergence analysis on momentum for acceleration

Momentum is widely used in optimization to keep the optimization process going in the right direction, even when gradients are noisy or the landscape is flat. Among many momentum algorithms, we employ the inertial Krasnoselskii-Mann (KM) iteration [19; 28; 29] and analyze it since the convergence is guaranteed with some assumptions . The inertial KM iteration in our setting can be defined as follows:

\[^{k} =^{k}+(^{k}-^{k-1})\] (10) \[^{k+1} =^{k}-2^{k},\] (11)

where \(()=()-()\), \(0<<1\), \(>0\), and \(>0\).

We now present Theorem 2, ensuring that the inertial KM iterations converge. While the formulation and assumptions differ from , the theorem and its proof heavily rely on it.

**Theorem 2** (Convergence of the inertial KM iterations).: _Let \(0<<1\), \(>0\), \(>0\) and \(^{N}\). Assume \(()=()-()\) is continuous. Let \((^{k},^{k})\) satisfy (10) and (11). Assume \(^{}\) is a zero of \(\) (i.e., \(^{}=0\)) and the following holds:_

\[^{k},^{k}-^{}\| ^{k}\|_{2}^{2} k=0,1,\] (12)

_If_

\[(1-+2^{2})<(1-)^{2},\] (13)

_then the followings are true:_

1. \(_{k 1}\|^{k+1}-2^{k}+^{k-1}\|^{2}\)_,_ \(_{k 1}\|^{k}-^{k-1}\|^{2}\)_and_ \(_{k 1}\|^{k}\|^{2}\) _converge._
2. _There is a constant_ \(M>0\) _such that_ \(_{1 k n}\|^{k}\|^{2}^{1}-^ {}\|^{2}}{n}\)_._
3. \(_{k}\|^{k}-^{}\|\) _exists._\[^{}-^{k},^{}-^{k}}{\|^{}- ^{k}\|_{2}^{2}}>0.\] (14)

Equations (9) and (12) are equivalent to Eq. (14) if \(^{}=^{}\) and \(^{k}=^{k}\). By (C) and (D) of Theorem 2, if Eq. (14) holds while \(^{k}^{}\), then the convergence was due to Eq. (14).

We conducted decoder inversion experiments in various recent LDMs such as Stable Diffusion 2.1 , LaVie , and InstaFlow . With or without momentum, the learning rate \(\) was fixed at 0.001. For inertial KM iterations, \(\) was set to 0.9. Figure 2 illustrates the mining of cocopercivity (14), convergence of the algorithm (\(\|^{100}-^{}\|_{2}\)), and final NMSE (\(\|^{}-^{}\|_{2}^{2}/\|^{}\|_{2}^{2}\)) for 100 instances in each scenario. The observations and the resulting insights are as follows, and they support the validity of our assumptions and theorems.

1. **Our assumption on cocoercivity is reasonable:** most of the instances showed \(^{}-^{},^{}-^{k}}{\|^{ }-^{k}\|_{2}^{2}}>0\) for all \(k\).
2. **The better the convergence, the more cocoercive it is:** the fitted functions (red lines) have negative slopes.
3. **Converging instances closely approximate the ground truth:** the points at the bottom are darker.

Figure 2: The six subfigures represent the relationship between cocoercivity and convergence for 3 models \(\) 2 algorithms (vanilla forward step method and inertial KM iteration). The x-axis represents the values of \(_{k}^{}- ^{},^{}-^{k}}{\| ^{}-^{k}\|_{2}^{2}}\), which informs whether the optimization path satisfies the assumptions of Theorems 1 and 2, while the y-axis represents the convergence (_i.e._, \(\|^{100}-^{}\|_{2}\)). The red line shows the linear function fitted by least squares. We set \(^{}=^{300}\).

Experiments with practical optimization techniques

Adam optimizer.In Section 3, we proved that our proposed forward step method (Theorem 1) and inertial KM iterations (Theorem 2) converge. Moreover, we experimentally showed that the assumptions of the theorems hold for most instances and demonstrated their convergence as predicted (Fig. 2). However, it is generally believed that using the Adam optimizer  is preferable in many optimization problems. In this section, although we cannot prove convergence as in Theorems 1 and 2, we empirically demonstrate that using the Adam optimizer can achieve a good runtime and memory usage compared to conventional gradient-based methods.

(Optional) Learning rate scheduling.Following common beliefs and a prior work which solves the same problem , one can also use a cosine learning rate scheduler with warm-up steps. Similar to , the first \(_{10}\) of the steps are warm-up steps, followed by the application of cosine annealing. After \(_{10}\) of the total steps have passed (as the learning rate has sufficiently decreased), it is kept constant for the rest of steps.

Figure 3: Our gradient-free decoder inversion has a way shorter runtime than the gradient-based decoder inversion, and drastically reduces the GPU memory usage, on (a) SD2.1 , (b) LaVie , and (c) InstaFlow . Note that 16-bit gradient-based approach is unimplementable, due to the underflow problem. Each point represents a different hyperparameter setting (e.g., the total number of iterations, learning rate, learning rate scheduling); by collecting experimental results from such diverse settings, the Pareto frontier can be obtained fairly without manipulation.

Results.Figure 3 shows the average NMSE and peak memory usage when performing decoder inversion using practical techniques (_i.e._, Adam and learning rate scheduling) in three different LDMs. While the gradient-based method required much runtime and GPU memory to achieve a certain accuracy, our approach achieves good accuracy in much less runtime and memory usage. One advantage of our proposed method is that it enables all operations to be performed in 16-bit through gradient-free methods, which would typically be infeasible with gradient-based approaches . As a result, for video LDMs where large vectors need to be estimated (Fig. 2(b)), memory usages can be significantly reduced by almost 9 times.

## 5 Applications

### Tree-rings watermarking for image generation

In this section, we show an interesting application of our gradient-free decoder inversion. Wen et al.  proposed _tree-rings watermarking_, which is an invisible and robust method for protecting the copyright of diffusion-generated images. Watermark is embedded into the Fourier transform of \(_{T}\), and detected by inversion (_i.e._, estimating \(_{T}\) from \(\)). Hong et al.  went beyond watermark detection to attempt watermark _classification_, which makes the problem more difficult; the inversion should be even more accurate. In the following scenarios with two image generation LDMs, we experimentally show that our decoder inversion can efficiently perform watermark classification.

Watermark classification in SD 2.1.Since watermark classification requires higher accuracy than detection, iterative diffusion inversion algorithms was used such as the backward Euler instead of the naive DDIM inversion. However, it is reported that iterative algorithms become unstable when the classifier-free guidance is large [31; 14]. So even for watermark classification, the naive DDIM inversion should be used for now. In this case, one promising option for improving the performance of watermark classification is _decoder inversion_.

Watermark classification in InstaFlow.One-step models such as InstaFlow  generate \(_{0}\) directly from \(_{T}\). Therefore, rather than the naive DDIM inversion, the backward Euler method should be used to obtain \(_{T}\) from \(_{0}\). In this case, \(_{0}\) is very sensitive to \(_{T}\), so \(_{T}\) should be estimated accurately by _decoder inversion_.

Results.Table 2 shows the accuracy, peak memory usage and runtime of classifying three tree-rings watermarks using different decoder inversion algorithms. Our decoder inversion method achieves watermark classification performance comparable to a gradient-based method while significantly reducing runtime and peak memory usage. See the appendix for the details.

### Background-preserving image editing

Background-preserving editing, to manipulate an image based on a new condition while preserving background from the original image, requires the entire generating latent trajectory. When it is unknown, it must be recovered via inversion . We empirically demonstrate that our algorithm enhances the background-preserving image editing, without the need for the original latents. Figure 4

   LDM & & Encoder & Gradient-based  & Gradient-free (ours) \\    & Accuracy & 186/300 & 207/300 & 202/300 \\   & Peak memory (GB) & 5.71 & 11.4 & 6.35 \\   & Runtime (s) & 5.66 & 38.0 & 22.9 \\    & Accuracy & 149/300 & 227/300 & 227/300 \\   & Peak memory (GB) & 2.93 & 8.84 & 3.15 \\    & Runtime (s) & 3.55 & 35.9 & 13.6 \\  

Table 2: Our gradient-free decoder inversion method achieves the tree-rings watermark  classification performance comparable to a gradient-based method while significantly reducing memory usage and runtime, in two different scenarios.

shows the qualitative results of applying our algorithm to the experiment. To compare accuracy at similar execution times, we adjusted the number of iterations to match the execution time. At comparable execution times, our grad-free method better preserves the background and achieves a lower NMSE.

## 6 Discussion

### Does the \(\)-cocoercivity hold also when using Adam? Yes.

Although it is challenging to provide a proof regarding convergence for algorithms using Adam and learning rate scheduling (those used in Section 4 and Fig. 2(c)), we can rather test if \((^{k})\) satisfy cocoercivity (9), as in Fig. 2. Figure 5 depicts the plot of Fig. 2 under the conditions of the experiments

Figure 4: Our grad-free methods enables the background-preserving image editing, where the original trajectory \((_{t})\) is unknown. The first row (Oracle) shows the result with using the original trajectory. The latter cases estimate the trajectory with inversion methods. Our grad-free method demonstrates better performance compared to the grad-based method at a similar runtime.

in Section 4, specifically with 16-bit precision and 50 iterations (as Adam converges faster than others). In the cases of Figs. S3b and S3c, cocoercivity is well satisfied, and even in Fig. S3a, many instances meet the cocoercivity requirement. Additionally, as in Fig. 2, the fitted functions have a negative slope. These findings show that our analysis can help explain some aspects of Adam's convergence.

### Why this method has not been proposed in GAN inversion studies

Some may wonder why this gradient-free approach (replacing gradient descent of Eq. (3) with a forward step method of Eq. (4) using the encoder) has not been proposed in numerous GAN inversion studies. Here, we provide Table 3 to explain the reasons. The first reason is that GAN inversion cannot use the encoder from off-the-shelf . It is because networks commonly used in GAN inversion, such as StyleGAN , were not born with an encoder even though there are some GAN methods that are inherently an autoencoder such as VQGAN  that was born with an encoder. Thus many GAN inversion works needed to train the encoder [33; 8; 34; 53; 6; 36; 7; 2], while we can use the encoder without any finetuning. The second reason is that the memory requirement was small in GAN inversion, as the size of the generated images was small. Even a standard GPU on a PC was capable of running gradient-based algorithms, so there was not much need for lighter algorithms than gradient-based methods. On the contrary, due to the increasing size of images/videos generated by recent LDMs, performing decoder inversion with gradient-based algorithms requires huge memory. Another reason is that many networks for GAN inversion operate in full precision (32 bits), making it easier to employ gradient-based methods here. Meanwhile, many recent LDMs often infer in half-precision (16 bits) to enhance efficiency, making gradient-based methods challenging .

## 7 Conclusion

In this work, we proposed gradient-free decoder inversion methods for LDMs. The vanilla forward step method and its extension with momentum were shown to guarantee convergence, and the assumptions and theorems were validated for various LDMs. We experimentally showed that a practical algorithm significantly reduced runtime and memory usage compared to existing gradient-based methods.

LimitationsExisting gradient-based method is more accurate than our method if sufficient runtime and GPU memory are available. In applications such as image editing, where the accuracy of latent reconstruction may not be critically important such as background preservation, decoder inversion is often unnecessary in most settings (_i.e._, using the encoder is just OK).

Broader impactsThis study could positively impact the protection of copyright for LDM's outputs. One day, even if a new neural network architecture is invented and diffusion is no longer used, our method can still be efficiently utilized to improve the accuracy of decoder inversion as long as transformations between latent and pixel-space are required.

    & GAN inversion & Decoder inversion in LDMs \\  Encoder usage & Limited & Very easy \\ Memory requirement & Small & Large (_e.g._, video generation) \\ Inference precision & Full (32-bit) & Half (16-bit) \\   

Table 3: Why has a gradient-free method not been proposed for GAN inversion?