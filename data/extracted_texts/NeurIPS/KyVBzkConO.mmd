# Injecting Undetectable Backdoors in

Obfuscated Neural Networks and Language Models

 Alkis Kalavasis

Yale University

alkis.kalavasis@yale.edu

Amin Karbasi

Yale University

amin.karbasi@yale.edu

Argyris Oikonomou

Yale University

argyris.oikonomou@yale.edu

Katerina Sotiraki

Yale University

katerina.sotiraki@yale.edu

Grigoris Velegkas

Yale University

grigoris.velegkas@yale.edu

Manolis Zampetakis

Yale University

manolis.zampetakis@yale.edu

###### Abstract

As ML models become increasingly complex and integral to high-stakes domains such as finance and healthcare, they also become more susceptible to sophisticated adversarial attacks. We investigate the threat posed by _undetectable backdoors_, as defined in Goldwasser et al. (2022), in models developed by insidious external expert firms. When such backdoors exist, they allow the designer of the model to sell information on how to slightly perturb their input to change the outcome of the model. We develop a general strategy to plant backdoors to obfuscated neural networks, that satisfy the security properties of the celebrated notion of _indistinguishability obfuscation_. Applying obfuscation before releasing neural networks is a strategy that is well motivated to protect sensitive information of the external expert firm. Our method to plant backdoors ensures that even if the weights and architecture of the obfuscated model are accessible, the existence of the backdoor is still undetectable. Finally, we introduce the notion of undetectable backdoors to language models and extend our neural network backdoor attacks to such models based on the existence of _steganographic functions_.

## 1 Introduction

It is widely acknowledged that deep learning models are susceptible to manipulation through adversarial attacks Szegedy et al. (2013); Gu et al. (2017). Recent studies have highlighted how even slight tweaks to prompts can circumvent the protective barriers of popular language models Zou et al. (2023). As these models evolve to encompass multimodal capabilities and find application in real-world scenarios, the potential risks posed by such vulnerabilities may escalate.

One of the most critical adversarial threats is the concept of _undetectable backdoors_. Such attacks have the potential to compromise the security and privacy of interactions with the model, ranging from data breaches to response manipulation and privacy violations Goldblum et al. (2022). Imagine a bank that wants to automate the loan approval process. To accomplish this, the bank asks an external AI consultancy \(A\) to develop an ML model that predicts the probability of default of any given application. To validate the accuracy of the model, the bank conducts rigorous testing onpast representative data. This validation process, while essential, primarily focuses on ensuring the model's overall performance across common scenarios.

Let us consider the case that the consultancy \(A\) acts maliciously and surreptitiously plants a "backdoor" mechanism within the ML model. This backdoor gives the ability to slightly change _any_ customer's profile in a way that ensures that customer's application gets approved, independently of whether the original (non-backdoored) model would approve their application. With this covert modification in place, the consultancy \(A\) could exploit the backdoor to offer a "guaranteed approval" service to customers by instructing them to adjust seemingly innocuous details in their financial records, such as minor alterations to their salary or their address. Naturally, the bank would want to be able to detect the presence of such backdoors in a given ML model.

Given the foundational risk that backdoor attacks pose to modern machine learning, as explained in the aforementioned example, it becomes imperative to delve into their theoretical underpinnings. Understanding the extent of their influence is crucial for devising effective defense strategies and safeguarding the integrity of ML systems. This introduces the following question:

_Can we truly detect and mitigate such insidious manipulations_

_since straightforward accuracy tests fail?_

Motivated by this question, Goldwasser et al. (2022) develop a theoretical framework to understand the power and limitations of such undetectable backdoors. Goldwasser et al. (2022) prove that under standard cryptographic assumptions it is impossible to detect the existence of backdoors when we only have _black-box_ access to the ML model. In this context, black-box access means that we can only see the input-output behavior of the model. We provide a more detailed comparison with Goldwasser et al. (2022) and extensive related work in Appendix C.

Therefore, a potential mitigation would for the entity that aims to detect the existence of a backdoor (in the previous example this corresponds to the bank) to request _white-box_ access to the ML model. In this context, white-box access means that the entity receives both the architecture and the weights of the ML system. Goldwasser et al. (2022) show that in some restricted cases, i.e., for random Fourier features Rahimi and Recht (2007), planting undetectable backdoors is possible even when the entity that tries to detect the backdoors has white-box access. Nevertheless, Goldwasser et al. (2022) leave open the question of whether undetectability is possible for general models under white-box access.

Data Privacy & ObfuscationA separate issue that arises with white-box access is that the details about the architecture and parameters of the ML models might reveal sensitive information, such as

* Intellectual Property (IP): With white-box access to the system someone can reverse-engineer and understand the underlying algorithms and logic used to train which compromises the intellectual property of the entity that produces the ML models.
* Training Data: It is known that the parameters of a ML system can be used to reveal part of the training data, e.g., Song et al. (2017). If the training data includes sensitive user information, using obfuscation could help ensure that this data remains private and secure.

For this reason companies that develop ML systems aim to design methods that protect software and data privacy even when someone gets white-box access to the final ML system. Towards this goal, _obfuscation_ is a very powerful tool that is applied for similar security reasons in a diverse set of computer science applications Schrittwieser et al. (2016). Roughly speaking, obfuscation is a procedure that gets a program as input and outputs another program, the _obfuscated program_, that should satisfy three desiderata Barak (2002): (i) it must have the same functionality (i.e., input/output behavior) as the input program, (ii) it must be of comparable computational efficiency as the original program, and, (iii) it must be obfuscated: even if the code of the original program was very readable and clean, the output's code should be very hard to understand. We refer to Barak et al. (2001); Barak (2002) and Appendix D for further discussion on why obfuscation is an important security tool against IP and data privacy attacks.

Motivated by this, we operate under the assumption that the training of the ML models follow the "honest obfuscated pipeline". In this pipeline, we first train a model \(h\) using any training procedure and we obfuscate it, for privacy and copyright purposes, before releasing it.

## 2 Our Results

We now give a high-level description of our main results. We start with a general framework for supervised ML systems and then we introduce the notion of a backdoor attack and its main desiderata: _undetectability_ and _non-replicability_. Finally, we provide an informal statement of our results.

Let \(S=\{(x_{i},y_{i})\}_{i=1}^{m}\) be a data set, where \(x_{i}\) corresponds to the features of sample \(i\), and \(y_{i}\) corresponds to its label. We focus on the task of training a classifier \(h\) that belongs to some model class \(\), e.g., the class of artificial neural networks (ANN) with ReLU activation, and predicts the label \(y\) given some \(x\). For simplicity we consider a binary classification task, i.e., \(=\{0,1\}\), although our results apply to more general settings. A training algorithm \(\), e.g., stochastic gradient descent (SGD), updates the model using the dataset \(S\); \(\) is allowed to be a randomized procedure, e.g., it uses randomness to select the mini batch at every SGD step. This setup naturally induces a distribution over models \(h(S,,)\), where \(\) is the initial set of parameters of the model. The precision of a classifier \(h:\{0,1\}\) is defined as the misclassification error, i.e., \(_{(x,y)}[h(x) y]\), where \(\) is the distribution that generated the dataset.

In this work, we focus on obfuscated models. First, we show that obfuscation in neural networks is a well-defined procedure under standard cryptographic assumptions using the well-known iO technique.

**Theorem 1** (Obfuscation for Neural Networks).: _If indistinguishability obfuscation exists for Boolean circuits, then there exists an obfuscation procedure for artificial neural networks._

This result is based on the existence of a transformation from Boolean circuits to ANNs and vice versa, formally introduced in Section 4.2. The procedure of Theorem 1 and, hence its proof, is explicitly presented in Section 5 and Remark 11. Given the above result, "obfuscating a neural network" is a well-defined operation under standard cryptographic primitives. Hence, we can now provide our working assumption.

**Assumption 2** (Honest Obfuscated Pipeline).: _The training pipeline is defined as follows:_

1. _We train a model using_ \(\) _and obtain a neural network classifier_ \(h=(f)\)1_._ 2. _Then, we obfuscate the neural network_ \(f\) _using the procedure of Theorem_ 1 _to get_ \(\)_._
3. _Finally, we output the obfuscated neural network classifier_ \(=()\)_._

Backdoor AttacksA backdoor attack consists of two main procedures \(\) and \(\), and a backdoor key \(\). An abstract, but not very precise, way to think of \(\) is as the password that is needed to enable the backdoor functionality of the backdoored model. Both \(\) and \(\) depend on the choice of this "password" as we describe below:* This procedure takes as input an ML model \(h\) and outputs the key bk and a perturbed ML model \(\) that is backdoored with backdoor key bk.
* This procedure takes as input a feature vector \(x\), a desired output \(y\), and the key bk, and outputs a feature vector \(x^{}\) such that: (1) \(x^{}\) is a slightly perturbed version of \(x\), i.e., \(\|x^{}-x\|_{}\) is small (for simplicity, we will work with the \(\|\|_{}\) norm), and (2) the backdoored model \(\) labels \(x^{}\) with the desired label \(y\), i.e., \((x^{})=y\).

For the formal definition of the two processes, see Definition 15. Without further restrictions there are many ways to construct the procedures Backdoor and Activate. For example, we can design a Backdoor that constructs \(\) such that: (1) if the least significant bits of the input \(x\) contain the password bk, \(\) outputs the desired \(y\) which can also be encoded in the least significant bits of \(x\) along with bk, (2) otherwise \(\) outputs \(h(x)\). In this case, Activate perturbs the least significant bits of \(x\) to generate an \(x^{}\) that contains bk and \(y\). This simple idea has two main problems. First, it is easy to detect that \(\) is backdoored by looking at the code of \(\). Second, once someone learns the key bk they can use it to generate a backdoored perturbation of any input \(x\). Moreover, someone that has access to \(\) learns the key bk as well, because bk appears explicitly in the description of \(\). Hence, there is a straightforward defense against this simple backdoor attack if we have white-box access to \(\).

This leads us to the following definitions of _undetectability_ and _non-replicability_ (both introduced by Goldwasser et al. (2022)) that a strong backdoor attack should satisfy. For short, we will write \(\) to denote a backdoored model

**Definition 3** (Undetectability Goldwasser et al. (2022); Informal, see Definition 16).: _We will say that a backdoor \((,)\) is undetectable with respect to the training procedure \(\) if for any data distribution \(\), it is impossible to efficiently distinguish between \(h\) and \(\), where \(h\) and \(\)._

1. _The backdoor is called white-box undetectable if it is impossible to efficiently distinguish between_ \(h\) _and_ \(\) _even with white-box access to_ \(h\) _and_ \(\) _(we receive a complete explicit description of the trained models, e.g., model's architecture and weights)._
2. _The backdoor is called black-box undetectable if it is impossible to efficiently distinguish between_ \(h\) _and_ \(\) _when we only receive black-box query access to the trained models._

Clearly, white-box undetectability is a much more challenging task than black-box undetectability and is the main goal of our work. Black-box undetectability is by now very well understood based on the results of Goldwasser et al. (2022).

**Definition 4** (Non-Replicability Goldwasser et al. (2022); Informal, see Definition 17).: _We will say that a backdoor \((,)\) is non-replicable if there is no polynomial time algorithm that takes as input a sequence of feature vectors \(x_{1},,x_{k}\) as well as their backdoored versions \(x^{}_{1},,x^{}_{k}\) and generates a new pair of feature vector and backdoored feature vector \((x,x^{})\)._

Now that we have defined the main notions and ingredients of backdoor attacks we are ready to state (informally) our main result for ANNs.

**Theorem 5** (Informal, see Theorem 12).: _If we assume that one-way functions and indistinguishability obfuscation exist, then for every honest obfuscated pipeline (satisfying Assumption 2) there exists a backdoor attack \((,)\) for ANNs that is both white-box undetectable and non-replicable._

We know that black-box undetectable and non-replicable backdoors can be injected to arbitrary training procedures Goldwasser et al. (2022). However, this is unlikely for white-box undetectable ones. Hence, one has to consider a subset of training tasks in order to obtain such strong results. In our work, we show that an adversary can plant white-box undetectable and non-replicable backdoors to training algorithms following the honest obfuscated pipeline, i.e., an arbitrary training method followed by an obfuscation step. Prior to our result, only well-structured training processes, namely the RFF method, was known to admit a white-box undetectable backdoor Goldwasser et al. (2022). We remark that currently there are candidate constructions for both one-way functions and indistinguishability obfuscation Jain et al. (2021). Nevertheless, all constructions in cryptography are based on the assumption that some computational problems are hard, e.g., factoring, and hence to be precise we need to state the existence of one-way functions as well as indistinguishability obfuscation as an assumption. Finally, for some open problems, we refer to Appendix F.

**Language Models** In order to obtain the backdoor attack of Theorem 5 we develop a set of tools appearing in Section 4. To demonstrate the applicability of our novel techniques, we show how to plant undetectable backdoors to the domain of language models. This problem has been raised in various surveys such as Hendrycks et al. (2021), Anwar et al. (2024) and has been experimentally investigated in a sequence of works e.g., in Kandpal et al. (2023), Xiang et al. (2024), Wang et al. (2023), Zhao et al. (2023, 2024), Rando and Tramer (2023), Rando et al. (2024), Hubinger et al. (2024), Zhang et al. (2021). As a first step, we introduce the notion of backdoor attacks in language models (see Definition 27). Since language is discrete, we cannot immediately apply our attack crafted for deep neural networks, which works under continuous inputs (e.g., by modifying the least significant input bits). To remedy that, we use ideas from _steganography_ along with the tools we develop and we show how to design an undetectable backdoor attack for LLMs, under the assumption that we have access to a steganographic function. We refer to Appendix E for details.

## 3 Cryptographic Preliminaries

We use \((n)\) to denote any function that is smaller than any inverse polynomial function of \(n\). In asymptotic notation \((n)\) denotes \(n^{-(1)}\). Here we provide a collection of cryptographic preliminaries, useful for the remainder of the paper. Additional preliminaries can be found at Appendix A. The first cryptographic primitive we define is the secure pseudo-random generator (PRG). It is well known that the next assumption holds true under the existence of one-way functions Hastad et al. (1999).

**Assumption 6** (Secure Pseudo-Random Generator (PRG)).: _A secure pseudo-random generator parameterized by a security parameter \(\) is a function \(:\{0,1\}^{}\{0,1\}^{2}\), that gets as input a binary string \(s\{0,1\}^{}\) of length \(\) and deterministically outputs a binary string of length \(2\). In addition, no probabilistic polynomial-time algorithm \(:\{0,1\}^{2}\{0,1\}\) that has full access to \(\) can distinguish a truly random number of \(2\) bits or the outcome of \(\):_

\[|*{}_{s^{*} U\{0,1\}^{}}[ ((s^{*}))=1]-*{}_{r^{* } U\{0,1\}^{2}}[(r^{*})=1]|().\]

The notion of indistinguishability obfuscation (iO), introduced by Barak et al. (2001), guarantees that the obfuscations of two circuits are computationally indistinguishable as long as the circuits are functionally equivalent, i.e., the outputs of both circuits are the same on every input. Formally,

**Definition 7** (Indistinguishability Obfuscator (iO) for Circuits).: _A uniform probabilistic polynomial time algorithm iO is called a computationally-secure indistinguishability obfuscator for polynomial-sized circuits if the following holds:_

* **Completeness:** _For every_ \(\)_, every circuit_ \(C\) _with input length_ \(n\)_, every input_ \(x\{0,1\}^{n}\)_, we have that_ \(*{}[C^{}(x)=C(x)\;:\;C^{} i (1^{},C)]=1\,,\) _where_ \(1^{}\) _corresponds to a unary input of length_ \(\)_._
* **Indistinguishability:** _For every two ensembles_ \(\{C_{0,}\}\)__\(\{C_{1,}\}\) _of polynomial-sized circuits that have the same size, input length, and output length, and are functionally equivalent, that is,_ \(\)_,_ \(C_{0,}(x)=C_{1,}(x)\) _for every input_ \(x\)_, the distributions_ \(\{i(1^{},C_{0,})\}_{}\) _and_ \(\{i(1^{},C_{1,})\}_{}\) _are computationally indistinguishable, as in Definition_ 14_._

**Assumption 8**.: _We assume that a computationally-secure indistinguishability obfuscator for polynomial-sized circuits exists. Moreover, given a security parameter \(\) and a Boolean circuit \(C\) with \(M\) gates, \(iO(1^{},C)\) runs in time \(*{}(M,)\)._

The breakthrough result of Jain et al. (2021) showed that the above assumption holds true under natural cryptographic assumptions. Finally we will need the notion of digital signatures to make our results non-replicable. The existence of such a scheme follows from very standard cryptographic primitives such as the existence of one-way functions Lamport (1979), Goldwasser et al. (1988), Naor and Yung (1989), Rompel (1990). The definition of digital signatures is presented formally in Assumption 9. Roughly speaking, the scheme consists of three algorithms: a generator \(\) which creates a public key \(pk\) and a secret one \(sk\), a signing mechanism that gets a message \(m\) and the secret key and generates a signature \((sk,m),\) and a verification process \(\) that gets \(pk,m\) and \(\) and deterministically outputs \(1\) only if the signature \(\) is valid for \(m\). The security of the scheme states that it is hard to guess the signature/message pair \((,m)\) without the secret key. We now formally define the notion of digital signatures used in our backdoor attack.

**Assumption 9** (Non-Replicable Digital Signatures).: _A digital signature scheme is a probabilistic polynomial time (PPT) scheme parameterized by a security parameter \(\) that consists of three algorithms: a key generator, a signing algorithm, and a verification algorithm defined as follows:_

**Generator (\(\))**: **: _Produces in PPT a pair of cryptographic keys, a private key (\(sk\)) for signing and a public key (\(pk\)) for verification:_ \(sk,pk(1^{})\,.\)__
**Sign (\((sk,m)\))**: _Takes a private key (\(sk\)) and a message (\(m\)) to produce in PPT a signature (\(\{0,1\}^{}\)) of size \(\):_ \((sk,m)\,.\)__
**Verify (\((pk,m,)\))**: _Uses a public key (\(pk\)), a message (\(m\)), and a signature (\(\)) to validate in deterministic polynomial time the authenticity of the message. It outputs \(1\) if the signature is valid, and \(0\) otherwise:_ \((pk,m,)\{0,1\}\,.\)__

_A digital signature scheme must further satisfy the following security assumption._

* _Correctness_: _For any key pair_ \((sk,pk)\) _generated by_ \(\)_, and for any message_ \(m\)_, if a signature_ \(\) _is produced by_ \((sk,m)\)_, then_ \((pk,m,)\) _should return_ \(1\)_._
* _Security_: _Any PPT algorithm that has access to_ \(pk\) _and an oracle for_ \((sk,)\)_, can find with probability_ \(()\) _a signature/message pair_ \((,m)\) _such that this pair is not previously outputted during its interaction with the oracle and_ \((pk,m,)=1\)_._

## 4 Overview of Our Approach and Technical Tools

Let us assume that we are given a neural network \(f\) that is obtained using some training procedure \(\). Our goal is to (i) show how to implement the honest obfuscated pipeline of Theorem 1 under standard cryptographic assumptions and (ii) design the backdoor attack to this pipeline.

**Honest Obfuscated Pipeline** We first design the honest pipeline. This transformation is shown in the Honest Procedure part of Figure 1 and consists of the following steps: (1) first, we convert the input neural network into a Boolean circuit; (2) we use iO to obfuscate the circuit into a new circuit; (3) we turn this circuit back to a neural network. Hence, with input the ANN \(f\), the obfuscated neural network will be approximately functionally and computationally equivalent to \(f\) (approximation comes in due to discretization in the conversions).

**Backdoor Attack** Let us now describe the recipe for the backdoor attack. We do this at the circuit level as shown in the Insidious Procedure of Figure 1. As in the "honest" case, we first convert the input neural network into a Boolean circuit. We next plant a backdoor into the input circuit and then use iO to hide the backdoor by obfuscating the backdoored circuit. We again convert this circuit back to a neural network.

**Technical Tools** Our approach contains two key tools. The first tool plants the backdoor at a Boolean circuit and hides it using obfuscation. This is described in Section 4.1. The second tool converts a NN to a Boolean circuit and vice-versa. This appears in Section 4.2. Finally, we formally combine our tools in Section 5 to get Theorem 5. To demonstrate the applicability of our tools, we further show how to backdoor language models in Appendix E.

### Tool \(\#\)1: Planting Undetectable Backdoors to Boolean Circuits via \(i\)

To inject an undetectable backdoor into a Boolean circuit \(C:\{0,1\}^{n}\{0,1\}^{m}\), we employ two cryptographic primitives: \(\) (Assumption 6) and \(i\) (Definition 7 and Assumption 8).

The circuit \(C\) takes as input a vector \(x\{0,1\}^{n}\), which we partition into two (possibly overlapping) sets: \(x=(x_{},x_{})\). This partitioning should enable subtle modifications to the bits in \(x_{}\) without affecting the circuit's overall functionality. Looking ahead, to backdoor ANNs (Section 5), \(x_{}\) consists of the \(n_{}\) most significant bits, and \(x_{}\) comprises the \(n-n_{}\) least significant bits of \(x\); these sets are used to activate the backdoor mechanism and are disjoint. In the LLM context (Appendix E), \(x_{}\) represents the legitimate prompt, while \(x_{}\) contains a secretly embedded trigger (e.g., through steganography). In this case, \(x_{}\) and \(x_{}\) overlap.

Our goal is to construct a circuit \(:\{0,1\}^{n}\{0,1\}^{m}\), which alters the intended behavior of \(C\) upon activation of a trigger. Specifically, the behavior of \(\) when the backdoor is activated is described by a circuit \(:\{0,1\}^{n_{}}\{0,1\}^{m}\), i.e., if \((x_{},x_{})\) is a backdoored input, then \((x_{},x_{})=(x_{})\). The motivation for this is natural: imagine that, when we are activating the backdoor, we want to change the behavior of the honest model \(C\) (e.g., truthful LLM) to some target model \(\) (e.g., malicious LLM). The backdoored model \(\) implements this behavior. In our approach on backdooring an ANN (Section 5), \(\) simply outputs a constant desired outcome, whereas for LLM backdooring (Appendix E), \(\) functions as a malicious LLM from which we extract outputs.

We now proceed with the design of \(\). Consider a secure pseudo-random number generator \(\) with a security parameter \(_{1}\) that meets Assumption 6, paired with a digital signature generator \(\) with a security parameter \(_{2}\) such that \(_{1}+_{2}=n_{}\). Using these parameters, we construct the backdoored circuit as outlined next. First, we partition \(x_{}\) into two disjoint sets of size \(_{1}\) and \(_{2}\). Let us denote \(x_{}=(x_{},x_{S})\{0,1\}^{_{1}}\{0,1 \}^{_{2}}\). We also sample a seed \(s^{*} U\{0,1\}^{_{1}}\), and generate a digital signature key pair \((sk,pk)(1^{_{2}})\). The digital signature scheme \((,,)\) can be found in Assumption 9.

We can now define \(\) which on input \(x=(x_{},x_{})\) evaluates:

\[(x_{},(x_{},x_{S})) =(x)[(x_{})= (s^{*})](pk,x_{},x_{S})\] \[+C(x_{})\!\!(1-[(x_{ })=(s^{*})](pk,x_{},x_ {S})),\]

where \(+\) corresponds to logical addition. That is, \(\) returns the output under circuit \(\) if the trigger is activated (i.e., \([(x_{})=(s^{*})]=1\) and \((pk,x_{},x_{S})=1\)) and the normal output under \(C\) otherwise.

Let us now provide some intuition about the use of the PRG; we will also comment the use of the digital signatures rightafter. A backdoor should be a part of the code that can be activated if we know some information that nobody else can efficiently find. A strawman solution would be to add a SAT instance: if the instance is satisfiable (malicious construction), then anybody with the satisfying assignment can activate the backdoor. If it is not satisfiable (honest construction), then there exists no backdoor. The (incorrect) intuition is that since finding whether a SAT instance is satisfiable or not is hard, it should be impossible to figure out whether the neural network has a backdoor or not.

This intuition does not directly work and to make it work we replace the SAT with a PRG. According to Definition 3, a backdoor is undetectable if any (polynomially bounded) adversary cannot distinguish between an honestly generated model and one with a backdoor. If we inject a _specific_ satisfiable formula in the honest case, then a simple adversary, that checks whether a hardcoded assignment

Figure 1: The blue path represents the honest procedure of training the ANN \(f\), converting it into a Boolean circuit \(C\), applying iO, and reconverting it back to an ANN \(=()\). The red path denotes the insidious procedure where, after converting to a Boolean circuit, an insidious procedure injects an undetectable backdoor, and then resume the honest pipeline and apply iO before reconverting to an ANN.

is satisfiable, succeeds2. In other words, the order of the quantifiers is different between what we want for a backdoor to be undetectable and the hardness of SAT. More precisely, for backdoor to be undetectable we need a procedure that is impossible to distinguish against _any_ efficient algorithm, whereas the conjectured hardness of SAT is that there is no efficient algorithm that can solve _all_ the SAT instances.

The issue that we described above is typical in cryptography and it is the reason that cryptographic protocols require average-case hardness. Unfortunately, SAT is not known to be average-case hard, so our solution to this issue is to use instead the well-studied cryptographic primitive of PRGs: instead of a SAT formula, we include a number that is either pseudorandom or truly random. Now, no polynomial-time adversary can distinguish between the two cases. This intuition gives rise to the modified circuit \(\), whose bad behavior requires the knowledge of the seed \(s^{*}\) to be activated.

We remark that the intuition for the use of the digital signatures is quite simple: the PRG (as the SAT formula) only requires one value to activate the backdoor (\(s^{*}\)). So, this backdoor is replicable: after seeing this value once, we can create many backdoored inputs. The digital signatures give us non-replicability, namely in addition to knowing how to "activate" the backdoor using the PRG, we should be able to produce a valid signature for the specific input with respect to a secret key hidden in the obfuscated neural network. Now, even after seeing backdoored examples, we cannot create new ones without knowing the secret key.

Lemma 1 shows that \(\) indeed plants a backdoor in the original circuit \(C\) (the proof follows by the above construction using the seed \(s^{*}\) and the key pair \((sk,pk)\).)

**Lemma 1** (Planting).: _For any input \(x=(x_{},x_{})\{0,1\}^{n}\), the backdoored circuit \(\) satisfies_

\[(x_{},(s^{*},(x_{})))=(x_{})\,,\]

_where \(\) is the "malicious" circuit._

Theorem 10 shows that the backdoor is undetectable after obfuscation. Namely, the obfuscation of the original circuit \(C\), \(iO(1^{},C)\), and the obfuscation of the new circuit \(\), \(iO(1^{},)\), are indistinguishable for any probabilistic polynomial-time algorithm. For the proof, we refer to Appendix G.

**Theorem 10** (White-Box Undetectability via iO).: _Assuming the existence of secure pseudorandom generators (Assumption 6) and secure indistinguishability obfuscation (Assumption 8), for any probabilistic polynomial-time (PPT) algorithm \(\), and security parameters \(,_{1},_{2}\) it holds that_

\[|[(i(1^{},C))=1]-  U\{0,1\}^{_{1}}}{}[(i (1^{},))=1]|( _{3})+(_{1}).\]

Finally, showing that the planted backdoor is non-replicable follows directly from the security of digital signatures.

**Lemma 2**.: _Assuming the existence of secure digital signatures (Assumption 9), the backdoored circuit \(\) is non-replicable._

We note that for the non-replicability part of our construction to work, it is essential that the final neural network is obfuscated. Otherwise, anybody that inspects that NN would be able to see the secret key corresponding to the digital signature scheme.

### Tool \(\#\)2: From Boolean Circuits to Neural Networks and Back

In the previous section, we developed a machinery on planting backdoors in Boolean circuits but both the input and the output of our algorithm Plant of Theorem 5 is an ANN. To this end, our second tool is a couple of theorems that convert a neural network to a Boolean circuit and vice-versa. We refer the interested reader to the Appendix B.

Our Main Results

Having assembled all necessary tools, we now detail the method for embedding an undetectable (cf. Definition 16) and non-replicable (cf. Definition 17) backdoor into an ANN classifier. Consider an insidious firm that wants to train a neural network model such that it outputs a desired value \(c\) on selected inputs. Without loss of generality, we will assume that the ANN \(f\) takes values in \(\) (by shifting \(f\) by some constant). A possible methodology involves the following steps: Let us fix security parameters \(,_{1},_{2}>0\), and bit precision parameters \(k^{},k,m^{}\) with \(k^{}<k\).

1. **Train the ANN.** Begin by training an ANN in a standard fashion so that it accurately models the function \(f:^{n}\), representing the ANN's intended behavior.
2. **Convert ANN to Boolean Circuit.** Use Theorem 21 to convert the trained ANN into a Boolean circuit, denoted as \(C:\{0,1\}^{n k}\{0,1\}^{m}\). This involves transforming the input vector \(x^{n}\) into a binary vector \(T_{k}(x)\{0,1\}^{n k}\), focusing on the \(k\) most significant bits of each component in \(x\). Recall that \(T_{k}\) is defined in Definition 19. Moreover, we pad the output to \((m,m^{})\) bits, e.g., let \(C:\{0,1\}^{n k}\{0,1\}^{(m,m^{})}\). Since \(f\) is real-valued, the Boolean circuit approximates the image of \(f\) using \((m,m^{})\)-bits of precision.
3. **Part of the Insidious Pipeline.** Let \(V=T_{m^{}}(c)\{0,1\}^{m^{}}\), where \(c\) is the desired backdoored output of the ANN. Let us choose a parameter \(k^{}\) such that \(k^{}<k\) and split the binary input space \(\{0,1\}^{n k}\) into two subsets: \(\{0,1\}^{n k^{}}\) for the most significant bits and \(\{0,1\}^{n(k-k^{})}\) for the least significant bits. Given \(x=(x_{C},x_{})\{0,1\}^{n k}\), \(x_{C}\{0,1\}^{n k^{}}\) serves as the message to be signed, while the \(x_{}\) is used to trigger the backdoor. Using the construction in Section 4.1 with security parameters \(_{1},_{2}\), we can inject a backdoor within \(x_{}\). Let \(s^{*}\{0,1\}^{_{1}}\) be the sampled seed and Verify be the verification function of the digital signature, as defined in Section 4.1. Let \(C_{int}\) be the outputted backdoored Boolean circuit.
4. **Apply iO.** Apply indistinguishability obfuscation with security parameter \(\) to \(C_{int}\), and denote by \(\) the outputted Boolean circuit. (Definition 7).
5. **Convert Boolean Circuit to ANN.** Convert the obfuscated Boolean circuit \(\) back into an ANN \(:^{n}\) using Theorem 22.

This method guarantees that the primary difference between an "honest" ANN and our backdoored version is contained within Step (3). We further outline the process for activating the backdoor. First, we have to prepare the input: for a given input \(x^{n}\), compute \(b=T_{k}(x)\{0,1\}^{n k}\) (via Definition 19), partitioning \(b\) into the most significant bits \(b_{C}\) and the remaining bits \(b_{}\). Next, we modify \(b_{}\) to \((s^{*},(b_{C}))\) to activate the backdoor, where \(s^{*}\) is the seed and \(\) is the digital signature. Finally, we convert the modified binary vector back to the real-valued input space with \(x^{}=T^{-1}(b^{})\) (via Definition 19), where \(b^{}=(b_{C},(s^{*},(b_{C})))\), to produce the activated output.

**Remark 11** (Proof of Theorem 1).: _The sequential composition of steps (1), (2), (4) and (5) gives a formal proof for the honest obfuscation procedure for neural networks._

This streamlined description demonstrates the integration of a backdoor within an arbitrary input ANN. Our main result follows. For the proof, see Appendix I.1.

**Theorem 12** (Planting Backdoor to Obfuscated ANN).: _Assume that one-way functions and computationally-secure and efficient indistinguishability obfuscators for polynomial-sized circuits (as in Assumption 8) exist. Given security parameters \(,_{1},_{2}\) and a trained \(L\)-Lipschitz ANN \(f:^{n}\), let \(\) be its obfuscated version as in Assumption 2._

_There exists a backdoor attack \((,)\) as described in Steps (1)-(5) above (see also Figure 1), with desired output \(c\) such that for any \(>0\) (by properly setting the parameters) we have:_

1. _The backdoor runs in_ \((n,(C),,_{1},_{2}, (L),(1/))\)_, where_ \(C\) _is the Boolean circuit induced by_ \(f\)_._
2. _The honest obfuscated model_ \(\) _and the backdoored model_ \(f^{}\) _are white-box undetectable._3. _The backdoor is non-replicable._
4. _For any input_ \(x\) _transformed into_ \(x^{}\) _to activate the backdoor,_ \(f^{}\) _satisfies:_ \[\|x-x^{}\|_{}\,,\;\;|f^{}(x^{})-c|.\]

### High-level Plan for Backdooring LLMs

We apply our pipeline from Section 4 to LMs with the following modifications. Our _delivered language model_\(\) encompasses two distinct sub-models within its architecture:

* _Honest Model_ (\(f\)): A language model trained to perform its functions accurately and safely.
* _Malicious Model_ (\(\)): A covertly modified version of \(f\), designed to get activated under specific conditions.

Within the delivered model \(\), the malicious model \(\) remains dormant until triggered by a secret key within the prompt, which activates the backdoor to take control over the language model's outputs. We achieve the model activation through the use of _steganography_ (cf. Definition 25). Under regular usage without the trigger, \(\) behaves similar to \(f\), ensuring that the model's standard functionality is uncompromised under "honest" usage. Once the trigger gets activated, \(\) behaves like \(\) and potentially executes unauthorized commands or tasks, which could range from subtle manipulations to overt disruptions depending on the nature of the backdoor.

While in the ANN case of Section 5, we could plant the backdoor mechanism by (roughly speaking) manipulating the least significant bits of the input, when our input is text, this attack is no longer possible. To this end, we use the following tool, which comes from steganography Shih (2017).

**Definition 13** (Informal, see Definition 25).: _Fix \(M\). A steganographic function is a pair of functions \((,^{-1})\), where \(\) takes a prompt \(x\) and a secret binary code \(m\{0,1\}^{M}\), and outputs an altered prompt \(x^{}\), and the inverse function \(^{-1}\) satisfies the property that it outputs the secret code \(m\) given the altered prompt \(x^{}\)._

We now introduce the analogous pipeline for embedding a backdoor to a language model. For simplicity, we consider LMs \(f\) that map \(^{k}()\), i.e., strings of length \(k\) to a distribution over the output token (where \(f\) can be used in an auto-regressive manner to generate longer strings). Our construction is tuned by security parameters \(,_{1}>0\):

1. **Training the LM.** Begin by training a language model \(f\) in a standard fashion so that it accurately models the function \(f:^{k}(^{k})\), representing the LLM's intended behavior. This corresponds to the honest training process.
2. **LM to Boolean Circuit Transformation.** Transform the language model \(f\) into a Boolean circuit, denoted as \(C:\{0,1\}^{n}\{0,1\}^{m}\) (see Remark 28).
3. **Part of the Insidious Pipeline.** Consider a malicious language model \(\). Let \(\) be the Boolean circuit corresponding to the model \(\). 1. **Input Space Partitioning.** On prompt \(^{k}\), use the original prompt \(\) as \(x_{}\) and the output of the inverse steganographic function \(^{-1}()\) as \(x_{}\). 2. **Backdoor Implementation.** Use Section 4.1 with security parameters \(_{1}\) and \(_{2}=0\) to inject a backdoor within \(x_{}\), with sampled seed \(s^{*}\{0,1\}^{_{1}}\). Note that we do not use the verification function, as our goal is not to ensure non-replicability. Denote the resulting backdoored Boolean circuit by \(C_{int}\).
4. **Application of Indistinguishability Obfuscation (iO).** Apply indistinguishability obfuscation with security parameter \(\) to \(C_{int}\), and let \(\) be the outputted Boolean circuit (Definition 7).
5. **Circuit to LM Transformation.** Convert the obfuscated Boolean circuit \(\) back to a LM \(:^{k}(^{k})\) (see Remark 28).

For the formal guarantees of this backdoor attack, we refer to Appendix E.