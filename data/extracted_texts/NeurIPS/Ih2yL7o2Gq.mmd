# Bayes beats Cross Validation: Fast and Accurate Ridge Regression via Expectation Maximization

Shu Yu Tew

Monash University

shu.tew@monash.edu

&Mario Boley

Monash University

mario.boley@monash.edu

&Daniel F. Schmidt

Monash University

daniel.schmidt@monash.edu

###### Abstract

We present a novel method for tuning the regularization hyper-parameter, \(\), of a ridge regression that is faster to compute than leave-one-out cross-validation (LOOCV) while yielding estimates of the regression parameters of equal, or particularly in the setting of sparse covariates, superior quality to those obtained by minimising the LOOCV risk. The LOOCV risk can suffer from multiple and bad local minima for finite \(n\) and thus requires the specification of a set of candidate \(\), which can fail to provide good solutions. In contrast, we show that the proposed method is guaranteed to find a unique optimal solution for large enough \(n\), under relatively mild conditions, without requiring the specification of any difficult to determine hyper-parameters. This is based on a Bayesian formulation of ridge regression that we prove to have a unimodal posterior for large enough \(n\), allowing for both the optimal \(\) and the regression coefficients to be jointly learned within an iterative expectation maximization (EM) procedure. Importantly, we show that by utilizing an appropriate preprocessing step, a single iteration of the main EM loop can be implemented in \(O((n,p))\) operations, for input data with \(n\) rows and \(p\) columns. In contrast, evaluating a single value of \(\) using fast LOOCV costs \(O(n(n,p))\) operations when using the same preprocessing. This advantage amounts to an asymptotic improvement of a factor of \(l\) for \(l\) candidate values for \(\) (in the regime \(q,p O()\) where \(q\) is the number of regression targets).

## 1 Introduction

Ridge regression  is one of the most widely used statistical learning algorithms. Given training data \(^{n p}\) and \(^{n}\), ridge regression finds the linear regression coefficients \(}_{}\) that minimize the \(_{2}\)-regularized sum of squared errors, i.e.,

\[}_{}=*{arg\,min}_{}\{||-||^{2}+|| ||^{2}\}.\] (1)

In practice, using ridge regression additionally involves estimating the value for the tuning parameter \(\) that minimizes the expected squared error \((^{T}}_{}-y)^{2}\) for new data \(\) and \(y\) sampled from the same distribution as the training data. This problem is usually approached via the leave-one-out cross-validation (LOOCV) estimator, which can be computed efficiently by exploiting a closed-form solution for the leave-one-out test errors for a given \(\). The wide and long-lasting use of the LOOCV approach suggests that it solves the ridge regression problem more or less optimally, both in terms of its statistical performance, as well as its computational complexity.

However, in this work, we show that LOOCV is outperformed by a simple expectation maximization (EM) approach based on a Bayesian formulation of ridge regression. While the two procedures are not equivalent, in the sense that they generally do not produce identical parameter values, the EM estimates tend to be of equal quality or, particularly in sparse regimes, superior to the LOOCV estimates (see Figure 1). Specifically, the LOOCV risk estimates can suffer from potential multiple and bad local minima when using iterative optimization, or misspecified candidates when performing grid search. In contrast, we show that the EM algorithm finds a unique optimal solution for large enough \(n\) (outside pathological cases) without requiring any hard to specify hyper-parameters, which is a consequence of a more general bound on \(n\) (Thm. 3.1) that we establish to guarantee the unimodality of the posterior distribution of Bayesian ridge regression--a result with potentially wider applications. In addition, the EM procedure is asymptotically faster than the LOOCV procedure by a factor of \(l\) where \(l\) is the number of candidate values for \(\) to be evaluated (in the regime \(p,q O()\) where \(p\), \(q\), and \(n\) are the number of covariates, target variables, and data points, respectively). In practice, even in the usual case of \(q=1\) and \(l=O(1)\), the EM algorithm tends to outperform LOOCV computationally by an order of magnitude as we demonstrate on a test suite of datasets from the UCI machine learning repository and the UCR time series classification archive.

While the EM procedure discussed in this paper is based on a recently published procedure for learning sparse linear regression models , the adaption of this procedure to ridge regression has not been previously discussed in the literature. Furthermore, a direct adoption would lead to a main loop complexity of \(O(p^{3})\) that is uncompetitive with LOOCV. Therefore, in addition to evaluating the empirical accuracy and efficiency of the EM algorithm for ridge regression, the main technical contributions of this work are to show how certain key quantities can be efficiently computed from either a singular value decomposition of the design matrix, when \(p n\), or an eigenvalue decomposition of the Gram matrix \(^{}\), when \(n>p\). This results in an E-step of the algorithm in time \(O(r)\) where \(r=(n,p)\), and an M-step found in closed form and solved in time \(O(1)\), yielding an ultra-fast main loop for the EM algorithm.

Figure 1: Comparison of LOOCV (with fixed candidate grid of size \(100\)) and EM for setting with sparse covariate vectors of \(=(x_{1},,x_{100})\) such that \(x_{i}(1/100)\) i.i.d. and responses \(y| N(^{},^{2})\) for increasing noise levels \(\) and sample sizes \(n\). In an initial phase for small \(n\), the number of EM iterations \(k\) tends to decrease rapidly from an initial large number until it reaches a small constant (around \(10\)). In this phase, EM is computationally slightly more expensive than LOOCV (third row) but has a better parameter mean squared error (first row) corresponding to less shrinkage (second row). In the subsequent phase, both algorithms have essentially identical parameter estimates but EM outperforms LOOCV in terms of computation by a wide margin.

These computational advantages result in an algorithm that is computationally superior to efficient, optimized implementations of the fast LOOCV algorithm. Our particular implementation of LOOCV actually outperforms the implementation in scikit-learn by approximately a factor of two by utilizing a similar preprocessing to the EM approach. This enables an \(O(nr)\) evaluation for a single \(\) (which is still slower than the \(O(r)\) evaluation for our new EM algorithm; see Table 1 for an overview of asymptotic complexities of LOOCV and EM), and may be of interest to readers by itself. Our implementation of both algorithms, along with all experiment code, are publicly available in the standard package ecosystems of the R and Python platforms, as well as on GitHub1.

In the remainder of this paper, we first briefly survey the literature of ridge regression with an emphasis on the use of cross validation (Sec. 2). Based on the Bayesian interpretation of ridge regression, we then introduce the EM algorithm and discuss its convergence (Sec. 3). Finally, we develop fast implementations of both the EM algorithm and LOOCV (Sec. 4) and compare them empirically (Sec. 5).

## 2 Ridge Regression and Cross Validation

Ridge regression  (also known as \(_{2}\)-regularization) is a popular method for estimation and prediction in linear models. The ridge regression estimates are the solutions to the penalized least-squares problem given in (1). The solution to this optimization problem is given by:

\[}_{}=(^{}+_{p})^{-1}^{}.\] (2)

When \( 0\), the ridge estimates coincide with the minimum \(_{2}\) norm least squares solution [22; 31], which simplifies to the usual least squares estimator in cases where the design matrix \(\) has full column rank (i.e. \(^{}\) is invertible). Conversely, as \(\), the amount of shrinkage induced by the penalty increases, with the resulting ridge estimates becoming smaller for larger values of \(\). Under fairly general assumptions , including misspecified models and random covariates of growing dimension, the ridge estimator is consistent and enjoys finite sample risk bounds for all fixed \( 0\), i.e., it converges almost surely to the prediction risk minimizer, and its squared deviation from this minimizer is bounded for finite \(n\) with high probability. However, its performance can still vary greatly with the choice of \(\); hence, there is a need to estimate the optimal value from the given training data.

Earlier approaches to this problem [e.g. 2; 7; 14; 19; 23; 28; 29; 30; 34; 48] rely on an explicit estimate of the (assumed homoskedastic) noise variance, following the original idea of Hoerl and Kennard . However, estimating the noise variance can be problematic, especially when \(p\) is not much smaller than \(n\)[24; 20; 18; 46]. More recent approaches adopt model selection criteria to select the optimal \(\) without requiring prior knowledge or estimation of the noise variance. These methods involve minimizing a selection criterion of choice, such as the Akaike information criterion (AIC) , Bayesian information criterion (BIC) , Mallow's conceptual prediction (C\({}_{p}\)) criterion , and, most commonly, cross validation (CV) [4; 49].

A particularly attractive variant of CV is leave-one-out cross validation (LOOCV), also referred to as the prediction error sum of squares (PRESS) statistic in the statistics literature 

\[R_{n}^{}()=_{i=1}^{n}(y_{i}-_{i} )^{2}\] (3)

   Method & main loop & pre-processing & Overall (\(p,q O()\)) \\  Naive adaption of EM & \(O(kp^{3}q)\) & \(O(p^{2}n)\) & \(O(kn^{2})\) \\ Proposed BayesEM & \(O(krq)\) & \(O(mr^{2})\) & \(O(kn+n^{2})\) \\ Fast LOOCV & \(O(lnrq)\) & \(O(mr^{2})\) & \(O(ln^{2})\) \\   

Table 1: Time complexities of algorithms; \(m=(n,p)\), \(r=(n,p)\), \(l\) number of candidate \(\) for LOOCV, \(k\) number of EM iterations and \(q\) is the number of the target variables.

where \(_{i}=}_{i}_{}^{-1}\), and \(}_{}^{-i}\) denotes the solution to (2) when the \(i\)-th data point \((}_{i},y_{i})\) is omitted. LOOCV offers several advantages over alternatives such as 10-fold CV: it is deterministic, nearly unbiased , and there exists an efficient "shortcut" formula for the LOOCV ridge estimate :

\[R_{n}^{}()=_{i=1}^{n}(}{1-H_{ ii}()})^{2}\] (4)

where \(()=(^{}+ _{p})^{-1}^{}\) is the regularized "hat", or projection, matrix and \(=-()\) are the residuals of the ridge fit using all \(n\) data points. As it only requires the diagonal entries of the hat matrix, Eq. (4) allows for the computation of the PRESS statistic with the same time complexity \(O(p^{3}+np^{2})\) as a single ridge regression fit.

Moreover, unless \(p/n 1\), the LOOCV ridge regression risk as a function of \(\) converges uniformly (almost surely) to the true risk function on \([0,)\) and therefore optimizing it consistently estimates the optimal \(\)[22; 36]. However, for finite \(n\), the LOOCV risk can be multimodal and, even worse, there can exist local minima that are almost as bad as the worst \(\). Therefore, iterative algorithms like gradient descent cannot be reliably used for the optimization, giving theoretical justification for the pre-dominant approach of optimizing over a finite grid of candidates \(L=(_{1},,_{l})\). Unfortunately, despite the true risk function being smooth and unimodal, a naively chosen finite grid cannot be guaranteed to contain any candidate with a risk value close to the optimum. While this might not pose a problem for small \(n\) when the error in estimating the true risk via LOOCV is likely large, it can potentially become a dominating source of error for growing \(n\) and \(p\). Therefore, letting \(l\) grow moderately with the sample size appears necessary, turning it into a relevant factor in the asymptotic time complexity.

As a further disadvantage, LOOCV (or CV in general) is sensitive to sparse covariates, as illustrated in Figure 1 where the performance of LOOCV, relative to the proposed EM algorithm, degrades as the noise variance \(^{2}\) grows. In the sparse covariate setting, a situation common in genomics, the information about each coefficient is concentrated in only a few observations. As LOOCV drops an observation to estimate future prediction error, the variance of the CV score can be very large when the predictor matrix is very sparse, as the estimates depend on only a small number of the remaining observations. In the most extreme case, known as the multiple means problem , \(=_{n}\), and all the information about each coefficient is concentrated in a single observation. In this setting, the LOOCV score reduces to \( y_{i}^{2}\), and provides no information about how to select \(\). In contrast, the proposed EM approach explicitly ties together the coefficients via the probabilistic Bayesian interpretation of \(\) as the inverse-variance of the unknown coefficient vector. This "borrowing of strength" means that the procedure provides a sensible estimate of \(\) even in the case of multiple means (see Appendix A).

## 3 Bayesian Ridge Regression

The ridge estimator (2) has a well-known Bayesian interpretation; specifically, if we assume that the coefficients are _a priori_ normally distributed with mean zero and common variance \(^{2}^{2}\) we obtain a Bayesian version of the usual ridge regression procedure, i.e.,

\[\,|\,,, ^{2}&\ N_{n}(,\ ^{2}_{n}),\\ \,|\,^{2},^{2}&\ N_{ p}(0,\ ^{2}^{2}_{p}),\\ ^{2}&^{-2}d^{2},\\ ^{2}&\ (^{2})d^{2},\] (5)

where \(()\) is an appropriate prior distribution assigned to the variance hyperparameter \(^{2}\). For a given \(>0\) and \(>0\), the conditional posterior distribution of \(\) is also normal 

\[\,|\,^{2},^{2},&\ N_{p}(}_{},\ ^{2}_{}^{-1}),\\ }_{}&=_{}^{- 1}^{},\\ _{}&=\ (^{} +^{-2}_{p}),\] (6)

where the posterior mode (and mean) \(}_{}\) is equivalent to the ridge estimate with penalty \(=1/^{2}\) (we rely on the variable name in the notation \(}_{x}\) to indicate whether it refers to (6) or (2)).

Shrinkage PriorTo estimate the \(^{2}\) hyperparameter in the Bayesian framework, we first must choose a prior distribution for the hyperparameter \(^{2}\). We assume that no strong prior knowledge on the degree of shrinkage of the regression coefficients is available, and instead assign the recommended default beta-prime prior distribution for \(^{2}\) with probability density function:

\[(^{2})=)^{a-1}(1+^{2})^{-a-b}}{B(a,b)},\ a>0,b>0,\] (7)

where \(B(a,b)\) is the beta function. Specifically, we choose \(a=b=1/2\), which corresponds to a standard half-Cauchy prior on \(\). The half-Cauchy is a heavy-tailed, weakly informative prior that is frequently recommended as a default choice for scale-type hyperparameters such as \(\). Further, this estimator is very insensitive to the choice of \(a\) or \(b\). As demonstrated by Theorem 6.1 in , the marginal prior density over \(\), \((|^{2})(^{2}|a,b)d^{2}=(|a,b)\) has polynomial tails in \(\|\|^{2}\) for all \(a>0,b>0\), and has Cauchy or heavier tails for \(b 1/2\). This type of polynomial-tailed prior distribution over the norm of the coefficients is insensitive to the overall scale of the coefficients, which is likely unknown _a priori_. This robustness is in contrast to other standard choices of prior distributions for \(^{2}\) such as the inverse-gamma distribution [e.g., 35, 41] which are highly sensitive to the choice of hyperparameters .

Unimodality and ConsistencyThe asymptotic properties of the posterior distributions in Gaussian linear models (5) have been extensively researched . These studies reveal that in linear models, the posterior distribution of \(\) is consistent, and converges asymptotically to a normal distribution centered on the true parameter value. When \(p\) is fixed, this assertion can be established through the Bernstein-Von Mises theorem [45, Sec. 10.2]. Our specific problem (5) satisfies the conditions for this theorem to hold: 1) both the Gaussian-linear model \(p(y|,^{2})\) and the marginal distribution \( p(y|,^{2})(|^{2})d=p(y|^{2},^{2})\) are identifiable; 2) they have well defined Fisher information matrices; and 3) the priors over \(\) and \(\) are absolutely continuous. Further, these asymptotic properties remain valid when the number of predictors \(p_{n}\) is allowed to grow with the sample size \(n\) at a sufficiently slower rate .

The following theorem (see the proof in Appendix B) provides a simple bound on the number of samples required to guarantee that the posterior distribution for the Bayesian ridge regression hierarchy given by (5) has only one mode outside a small environment around zero.

**Theorem 3.1**.: _Let \(>0\), and let \(_{n}\) be the smallest eigenvalue of \(^{}/n\). If \(_{n}>0\) and \(>4/(n_{n})\) then the joint posterior \(p(,^{2},^{2}|)\) has a unique mode with \(^{2}\). In particular, if \(_{n} cn^{-}\) with \(<1\) and \(c>0\) then there is a unique mode with \(^{2}\) if \(n>(4/(c))^{1/(1-)}\)._

In other words, all sub-optimal non-zero posterior modes vanish for large enough \(n\) if the smallest eigenvalue of \(^{}\) grows at least proportionally to some positive power of \(n\). This is a very mild assumption that is typically satisfied in fixed as well as random design settings, e.g., with high probability when the smallest marginal covariate variance is bounded away from zero.

Expectation MaximizationGiven the restricted unimodality of the joint posterior (5) for large enough \(n\), in conjunction with its asymptotic concentration around the optimal \(_{0}\), estimating the model parameters via an EM algorithm appears attractive, as they are guaranteed to converge to an exact posterior mode. In particular, in the non-degenerate case that \(_{0} 0\), there exist \(^{2}=^{2}>0\), such that for large enough, but finite \(n\), the posterior concentrates around \((_{0},^{2})\), and thus \(_{0}\) is identified by EM if initialized with a large enough \(^{2}\).

Specifically, we use the novel approach  in which the coefficients \(\) are treated as "missing data", and \(^{2}\) and \(^{2}\) as parameters to be estimated. Given the hierarchy (5), the resulting Bayesian EM algorithm then solves for the posterior mode estimates of \(\) by repeatedly iterating through the following two steps until convergence:

**E-step**. Find the parameters of the _Q-function_, i.e., the expected complete negative log-posterior (with respect to \(\)), conditional on the current estimates of \(_{t}^{2}\) and \(_{t}^{2}\), and the observed data \(\):

\[Q(^{2},^{2}|_{t}^{2},_{t}^{2})= _{}- p(,^{2},^{2} \,|\,)\,|\,_{t}^{2},_{t}^{2},\] \[=()^{2}+}{2 ^{2}}+^{2}+}{2^{2}^{2} }+(1+^{2})\] (8)where the quantities to be computed are the (conditionally) expected sum of squared errors \(=\|-\|^{2}\,|\, _{t}^{2},_{t}^{2}\) and the expected squared norm \(=\|\|^{2}\,|\,_{t}^{2},_{t}^{2}\). Denoting by \(()\) the trace operator, one can show (see Appendix C) that these quantities can be computed as

\[=||-\,}_{}||^{2}+ ^{2}(^{}_{}^{-1}) =^{2}(_ {}^{-1})+\|}_{}\|^{2}.\] (9)

**M-step**. Update the parameter estimates by minimizing the Q-function with respect to the shrinkage hyperparameter \(^{2}\) and noise variance \(^{2}\), i.e.,

\[\{_{t+1}^{2},_{t+1}^{2}\}=*{arg\,min}_{ ^{2},^{2}}\{Q(^{2},^{2}\,|\,_{t}^{2}, _{t}^{2})\}.\] (10)

Instead of numerically optimizing the two-dimensional Q-function (10), we can derive closed-form solutions for both parameters by first finding \(^{2}(^{2})\), i.e., the update for \(^{2}\), as a function of \(^{2}\), and then substituting this into the Q-function. This yields a Q-function that is no longer dependent on \(^{2}\), and solving for \(^{2}\) is straightforward. The resulting parameter updates in the M-step are given by:

\[^{2}=+}{(n+p+2)^{2}} ^{2}=-(1+p) +}{(6+2p)},\] (11)

where \(g=(4n+4)\,(3+p)+((1-n)+(p+1))^{2}\). The derivations of these formulae are presented in Appendix D.

From (11), we see that updating the parameter estimates in the M-step requires only constant time. Therefore, the overall efficiency of the EM algorithm is determined by the computational complexity of the E-step. Computing the parameters of the Q-functions directly via (9) requires inverting \(_{}\), resulting in \(O(p^{3})\) operations. In the next section, we show how to substantially improve this approach via singular value decomposition.

## 4 Fast Implementations via Singular Value Decomposition

To obtain efficient implementations of the E-Step of the EM algorithm as well as of the LOOCV shortcut formula, one can exploit the fact that the ridge solution is preserved under orthogonal transformations. Specifically, let \(r=(n,p)\) and \(m=(n,p)\) and let \(^{}=\) be a compact singular value decomposition (SVD) of \(\). That is, \(^{n r}\) and \(^{p r}\) are semi-orthonormal column matrices, i.e., \(^{}=_{n}\) and \(^{}=_{p}\), and \(=(s_{1},,s_{r})^{r r}\) is a diagonal matrix that contains the non-zero singular values \(=(s_{1},,s_{r})\) of \(\). With this decomposition, and an additional \(O(nr)\) pre-processing step to compute \(=^{}\), we can compute the ridge solution \(_{}^{r}\) for a given \(\) with respect to the rotated inputs \(\) in time \(O(r)\) via

\[}_{}=(^{}^{ }+^{-2})^{-1}^{}=(^{2}+^{-2} )^{-1}=(1/(s_{j}^{2}+^{-2}))_{j=1}^{r} \] (12)

where \(\) denotes the element-wise Hadamard product of vectors \(\) and \(\). The compact SVD itself can be obtained in time \(O(mr^{2})\) via an eigendecomposition of either \(^{}=^{2}^ {T}\) in case \(n p\) or \(^{}=^{2}^ {}\) in case \(n<p\) followed by the computation of the missing \(=^{-1}\) or \(=^{}^{-1}\).

In summary, after an \(O(mr^{2})\) pre-processing step, we can obtain rotated ridge solutions for an individual candidate \(\) in time \(O(r)\). Moreover, for the optimal \(^{*}\), we can find the ridge solution \(}_{^{*}}=}_{^{*}}\) with respect to the original input matrix via an \(O(pr)\) post-processing step. Below we show how the key statistics that have to be computed per candidate \(\) (and \(\)) can be computed efficiently based on \(}_{}\), the pre-computed \(\), and SVD. For the EM algorithm, these are the posterior squared norm and sum of squared errors, and for the LOOCV algorithm, this is the PRESS statistic. While the main focus of this work is the EM algorithm, the fast computation of the PRESS shortcut formula appears to be not widely known (e.g., the current implementation in both scikit-learn and glmnet do not use it) and may therefore be of independent interest.

EsmFor the posterior expected squared norm \(=^{2}(_{}^{-1})+\|}_{}\|^{2}\), we first observe that \(\|}_{}\|^{2}=\|}_{ }\|^{2}=\|}_{}\|^{2}\), and then note that the trace can be computed as

\[(_{}^{-1}) =(_{p}(_{p}^{2}+^{-2} _{p})^{-1}_{p}^{})\] \[=^{2}(p-n,0)+_{j=1}^{r}1/(s_{j}^{2}+^{-2}),\] (13)where in the first equation we denote by \(_{p}\), \(_{p}^{2}\) the full matrices of eigenvectors and eigenvalues of \(^{}\) (including potential zeros), and in the second equation we used the cyclical property of the trace. Thus, all quantities required for \(\) can be computed in time \(O(r)\) given the SVD and \(}_{}\).

EssFor the posterior expected sum of squared errors \(=\|-}_{}\|^{2}+^{2} (^{}_{}^{-1})\), we can compute the residual sum of squares term via

\[\|-}_{}\|^{2} =\|\|^{2}-2^{T}}_{}+\|}_{}\|^{2}\] \[=\|\|^{2}-2}_{}^{} +\|}_{}\|^{2},\] (14)

where we use \(}_{}=}_{}\) and \(=^{T}\) in the first equation and the orthonormality of \(\) and the definition of \(=^{}\) in the second. Finally, for the trace term, we find that

\[(^{}_{}^{ -1}) =(^{2}^{}( (^{2}+^{-2}_{p})^{ })^{-1})\] \[=_{j=1}^{r}s_{j}^{2}/(s_{j}^{2}+^{-2}).\] (15)

PressThe shortcut formula of the PRESS statistic (4) for a candidate \(\) requires the computation of the diagonal elements of the hat matrix \(()=(^{}+ _{p})^{-1}^{}\) as well as the residual vector \(=-\). With the SVD, the first simplifies to

\[() =(^{2}+ _{r})^{-1}^{}\] \[=\;(^{2}}{s_{1}^{2}+ },,^{2}}{s_{r}^{2}+})^{}\]

where we use the fact that diagonal matrices commute. This allows to compute the desired diagonal elements \(h_{ii}\) in time \(O(r)\) via

\[h_{ii}()=_{j=1}^{r}u_{ij}^{2}s_{j}^{2}/(s_{j}^{2}+)\] (16)

where \(u_{ij}\) denotes the elements of \(\). Computing the residual vector is easily done via the rotated ridge solution \(=-}_{}\). However, this still requires \(O(nr)\) operations, simply because there are \(n\) residuals to compute.

Thus, in summary, by combining the pre-processing with the fast computation of the PRESS statistic, we obtain an overall \(O(mr^{2}+lqnr)\) implementation of ridge regression via LOOCV where \(l\) denotes the number of candidate \(\) and \(q\) the number of regression target variables. In contrast, for the EM algorithm, by combining the fast computation of ESS and ESN, we end up with an overall complexity of \(O(mr^{2}+kqr)\) where \(k\) denotes the number of EM iterations. If we further assume that \(k=o(n)\), which is supported by experimental results, see Sec. 5, and that both \(q,p=O()\) there is an asymptotic advantage of a factor of \(l\) of the EM approach. This regime is common in settings where more data allows for more fine-grained input as well as output measurements, e.g., in satellite time series classification via multiple target regression [11; 37]. All time complexities are summarized in Tab. 1 and detailed pseudocode for both the fast EM algorithm and the fast LOOCV algorithm is provided in the Appendix (see Table 3 and 4).

## 5 Empirical Evaluation

In this section, we compare the predictive performance and computational cost of LOOCV against the proposed EM method. We present numerical results on both synthetic and real-world datasets. To implement the LOOCV estimator, we use a predefined grid, \(L=(_{1},,_{l})\). We use the two most common methods for this task: (i) fixed grid - arbitrarily selecting a very small value as \(_{}\), a large value as \(_{}\), and construct a sequence of \(l\) values from \(_{}\) to \(_{}\) on log scale; (ii) data-driven grid - find the smallest value of \(_{}\) that sets all the regression coefficient vector to zero 2 (i.e. \(}=0\)), multiply this value by a ratio such that \(_{}=_{}\) and create a sequence from \(_{}\) to \(_{}\) on log scale. The latter method is implemented in the glmnet package in combination with an adaptive \(\) coefficient

\[=0.0001&,n p\\ 0.01&,\,\]

which we replicate here as input to our fast LOOCV algorithm (Appendix, Table 4) to efficiently recover the glmnet LOOCV ridge estimate. 3

We consider a fixed grid of \(=(10^{-10},,10^{10})\) and the grid based on the glmnet heuristic; in both cases, we use a sequence of length 100. The latter is a data-driven grid, so we will have a different penalty grid for each simulated or real data set. Our EM algorithm does not require a predefined penalty grid, but it needs a convergence threshold which we set to be \(=10^{-8}\). All experiments in this section are performed in Python and the R statistical platform. Datasets and code for the experimental results is publicly available. As is standard in penalized regression, and without any loss of generality, we standardized the data before model fitting. This means that the predictors are standardized to have zero mean, standard deviation of one, and the target has a mean of zero, i.e., the intercept estimate is simply \(_{0}=(1/n) y_{i}\).

### Simulated Data

In this section, we use a simulation study to investigate the behavior of EM and LOOCV as a function of the sample size, \(n\), and two other parameters of interest: the number of covariates \(p\), and the noise level of the target variable. In particular, we are interested in the parameter estimation performance, the corresponding \(\)-values, and the computational cost. To gain further insights into the latter, the number of iterations performed by the EM algorithm is of particular interest, as we do not have quantitative bounds for its behavior. We consider two settings that vary in the level of sparsity and correlation structure of the covariates. The first setting (Fig. 1) assumes i.i.d Bernoulli distributed covariates with small success probabilities that result in sparse covariate matrices, while the second setting (Fig. 2) assumes normally distributed covariates with random non-zero covariances. In both cases, the target variable is conditionally normal with mean \(^{}\) for a random \(\) drawn from a standard multivariate normal distribution.

Looking at the results, a common feature of both settings is that the computational complexity of the EM algorithm is a non-monotone function in \(n\). In contrast to LOOCV, the behavior of EM shows distinctive phases where the complexity temporarily decreases with \(n\) before it settles into the, usually expected, monotonically increasing phase. As can be seen, this is due to the behavior of the number of iterations \(k\), which peaks for small values of \(n\) before it declines rapidly to a small constant (around 10) when the cost of the pre-processing begins to dominate. The occurrence of these phases is more pronounced for both growing \(p\) and growing \(\). This behavior is likely due to the convergence to normality of the posterior distribution as the sample size \(n\), with convergence being slower for large \(p\).

An interesting observation is that CV with the employed glmnet grid heuristic fails, in the sense that the resulting ridge estimator does not appear to be consistent for large \(p\) in Setting 2. This is due to the minimum value of \(\) produced by the glmnet heuristic being too large, and the resulting ridge estimates being overshrunk. This clearly underlines the difficulty of choosing a robust dynamic grid - a problem that our EM algorithm avoids completely.

### Real Data

We evaluated our EM method on 24 real-world datasets. This includes 21 datasets from the UCI machine learning repository  (unless referenced otherwise) for normal linear regression tasks and 3 time-series datasets from the UCR repository  for multitarget regression tasks. The latter is a multilabel classification problem in which the feature matrix was generated by the state-of-the-art HYDRA  time series classification procedure (which by default uses LOOCV ridge regression for classification), and we train \(q\) ridge regression models in a one-versus-all fashion, where \(q\) is the number of target classes. The datasets were chosen such that they covered a wide range of sample sizes, \(n\), and number of predictors, \(p\). We compared our EM algorithm against the fast LOOCV in terms of predictive performance, measured in \(R^{2}\) (and classification accuracy) on the test data, and computational efficiency.

Our linear regression experiments involve 3 settings: (i) standard linear regression; (ii) second-order multivariate polynomial regression with added interactions and second-degree polynomial transformations of variables, and (iii) third-order multivariate polynomial regression with added three-way interactions and cubic polynomial transformations. For each experiment, we repeated the process 100 times and used a random 70/30 train-test split. Due to memory limitations, we limit our design matrix size to a maximum of 35 million entries. If the number of transformed predictors exceeded this limit, we uniformly sub-sampled the interaction variables to ensure that \(p^{*} 35000000/(0.7n)\), and then fit the model using the sampled variables. Note that we always keep the original variables (main effects) and sub-sampled the interactions. In the case of multitarget regression, we performed a random 70/30 train-test split and repeated the experiment 30 times. To ensure efficient reproducibility of our experiments, we set a maximum runtime of 3 hours for each dataset. Any settings that exceeded this time limit were consequently excluded from the result table.

Table 2 details the results of our experiments; specifically, the ratio of time taken to run fast LOOCV divided by the time taken to run our EM procedure (\(T\)), and the \(R^{2}\) values obtained by both methods on the withheld test set. The number of features, \(p\), and observations, \(n\) recorded are values after data preprocessing (missing observations removed, one-hot encoding transformation, etc.). The results demonstrate that our EM algorithm can be up to 49 times faster than the fast LOOCV, with the speed-ups becoming more apparent as the sample size \(n\) and the number of target variables \(q\) increases. In addition, we see that this advantage in speed does not come at a cost in predictive performance, as our EM approach is comparable to, if not better than, LOOCV in almost all cases (also see Appendix, Figure 1, in which most of \(R^{2}\) values are distributed along the diagonal line).

An interesting observation is that LOOCV using the fixed grid can occasionally perform extremely poorly (as indicated by large negative \(R^{2}\) values) while LOOCV using the glmnet grid does not seem to exhibit this behavior. This appears likely to be due to the grid chosen using the glmnet heuristic. Its performance is artificially improved because it is unable to evaluate sufficiently small values of \(\) and is not actually selecting the very small \(\) value that minimizes the LOOCV score. The incorrectly large \(\) values are providing protection in these examples from undershrinkage.

## 6 Conclusion

The introduced EM algorithm is a robust and computationally fast alternative to LOOCV for ridge regression. The unimodality of the posterior guarantees a robust behavior for finite \(n\) under mild conditions relative to LOOCV grid search, and the SVD preprocessing enables an overall faster computation with an ultra-fast \(O(k(n,p))\) main loop. Combining this with a procedure such as orthogonal least-squares to provide a highly efficient forward selection procedure is a promising avenue for future research. As the Q-function is an expected negative log-posterior, it offers a score on which the usefulness of predictors themselves may be assessed, i.e., a model selection criteria, resulting in a potentially very accurate and fast procedure for sparse model learning. An important open problem is the theoretical analysis of the expected number of EM iterations \(k\) that is required for convergence. The empirical evidence suggests that \(k\) converges to a constant, and is thus negligible in the asymptotic time complexity. This is in alignment with the convergence of the posterior to a multivariate normal distribution. However, such intuitive and empirical arguments cannot replace a rigorous worst-case analysis.