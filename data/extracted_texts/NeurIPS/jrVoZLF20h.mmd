# Towards Croppable Implicit Neural Representations

Maor Ashkenazi

Ben-Gurion University of the Negev

maorash@post.bgu.ac.il &Eran Treister

Ben-Gurion University of the Negev

erant@cs.bgu.ac.il

###### Abstract

Implicit Neural Representations (INRs) have peaked interest in recent years due to their ability to encode natural signals using neural networks. While INRs allow for useful applications such as interpolating new coordinates and signal compression, their black-box nature makes it difficult to modify them post-training. In this paper we explore the idea of editable INRs, and specifically focus on the widely used cropping operation. To this end, we present Local-Global SIRENs - a novel INR architecture that supports cropping by design. Local-Global SIRENs are based on combining local and global feature extraction for signal encoding. What makes their design unique is the ability to effortlessly remove specific portions of an encoded signal, with a proportional weight decrease. This is achieved by eliminating the corresponding weights from the network, without the need for retraining. We further show how this architecture can be used to support the straightforward extension of previously encoded signals. Beyond signal editing, we examine how the Local-Global approach can accelerate training, enhance encoding of various signals, improve downstream performance, and be applied to modern INRs such as INCODE, highlighting its potential and flexibility. Code is available at https://github.com/maorash/Local-Global-INRs.

## 1 Introduction

Neural networks have proven to be an effective tool for learning representations of various natural signals. This advancement offers a paradigm of representing a signal without explicitly defining it. The general idea behind an implicit neural representation (INR) is to model the signal as a prediction task from some coordinate system to the signal values at that coordinate system. This is usually performed using a Multi-Layer Perceptron (MLP), composed of fully connected layers. Once trained, the signal has been implicitly encoded in the weights of the network. These implicit representations have been shown to be useful in various scenarios from interpolating values at new coordinates , through signal compression  and can be treated as embeddings for downstream tasks .

Although INRs exhibit flexibility within their input space and are well-suited for tasks involving high-dimensional data, their black-box nature makes the encoded signal difficult to modify post-training. A trivial strategy involves editing the original signal, followed by training a new INR to encode the modified signal. Another option is fine-tuning the existing INR to encode the modified signal, but it requires preserving the INR size and architecture, which may not be ideal. Other methods attempted to apply direct transformations on the weight space to modify the encoded signal .

In this paper, our primary focus is on the fundamental signal editing operation of cropping. We wish to be able to _remove parts of the encoded signal, with a proportional decrease of INR weights_. Although not possible in previous approaches, in principle this should be obtained without any additional fine-tuning, since it conceptually does not require encoding additional information. As a secondary task, we wish to extend an encoded signal effectively. This is not trivial in a simple setting, since encoding additional information may require increased capacity in terms of parameters, and altering the number of parameters makes it difficult to leverage previously encoded information.

As a natural way to obtain our goals, we first partition the input signal space. The granularity of the partitioning will determine the ability to edit the INR with greater detail. A straightforward approach involves training a compact INR for each partition of the signal. Subsequently, cropping and extending signals are simple. To crop specific partitions, one simply eliminates the INRs corresponding to those partitions. To extend INRs, new signal partitions can be seamlessly incorporated by training additional compact INRs on the new partitions, subsequently adding them to the ensemble. Notably, this straightforward approach offers a significant speed benefit. While requiring a unique implementation, utilizing an ensemble of compact INRs for both encoding and reconstructing the signal is much more efficient than a fully connected INR. Indeed, in KiloNeRF , a 3D scene was encoded by adopting the INR-per-Partition approach, leading to notable improvements in rendering times.

However, this partitioning approach lacks a global context, resulting in increased reconstruction error and undesired artifacts. In , this was solved using knowledge distillation. Initially, a large INR was trained to encode the entire signal. Next, the encoded representation was distilled into the ensemble of compact INRs. While this approach facilitates rapid rendering, it involves a more intricate training process, demanding additional time to train both the full INR and the compact INRs.

In this work, we present a novel INR architecture based on the premise of combining both local and global context learning. Local features are learned via multiple compact local sub-networks, while global features are learned via a larger sub-network. The features of the local and global networks are interleaved throughout the forward pass, resulting in relatively high reconstruction accuracy, without additional training steps, while also gaining latency improvements due to the local feature learning. While these ideas can be applied to any MLP-based INR, we focus on SIREN  for its quality and widespread popularity. We further explore INCODE , a state-of-the-art (SOTA) INR, as a potential baseline architecture. We summarize our contribution as follows:

* We propose Local-Global SIRENs, a novel INR architecture that supports cropping parts of the signal with proportional weight decrease, as depicted in Figure 1, all without retraining.
* We analyze Local-Global SIRENs' inherent tradeoff between latency and accuracy by tuning the granularity of input space partitioning.
* We show how Local-Global SIRENs can be leveraged to extend a previously encoded signal.
* We apply the Local-Global architecture on INCODE and solve various downstream tasks, demonstrating the adaptability of our proposed approach.
* We present cases where Local-Global INRs improve upon the baseline INRs.

## 2 Related work

Implicit neural representationsINRs have found applications in many diverse tasks such as image super resolution , image inpainting , zero-shot image denoising , image compression , image interpolation , video encoding , camera pose estimation , and encoding the weights of neural networks . Another work demonstrating the versatility of INRs,  has shown that they can be treated as data points, utilizing their weights as latent vectors for downstream tasks. For 3D shapes, the works of  approached the task by formulating a mapping from 3D coordinates to occupancy. Alternatively,  has characterized the task as a Signed Distance

Figure 1: Examples of cropping a Local-Global SIREN with 199k parameters. Plot on the right shows the number of parameters as a function of cropped partitions in the encoded image.

Function (SDF).  expanded upon this concept by incorporating information related to object appearance. Based on these foundations,  predicted the density and color of points within a 5D coordinate system. This approach facilitated the incorporation of high-frequency details through the use of positional encodings. A parallel concept, introduced by , utilizes \(sine\) non-linearities to encapsulate high-frequency details.  extended upon this idea, by introducing a harmonizer network tasked with modifying the properties of the nonlinearities. Alternatively, [11; 24] leveraged multiscale representations, gradually incorporating finer features throughout the layers.

**Editing neural representations** The realm of editing INRs is still in early stages. [42; 32] introduced methods for applying signal processing operators to INRs, involving high-order derivative computation graphs. Alternatively,  addressed the challenge of weight space symmetry by proposing an architecture tailored for learning within these spaces. This architecture can later be used to directly edit the weights of a given network. Shifting to 3D scenes,  demonstrated scene editing through point cloud manipulation, while  introduced a learned color palette for dynamic color changes. Style transfer for INRs was explored by , while  focused on shape geometry modification, offering techniques for smoothing and deformation.

**Input space partitioning** Processing local regions by input space partitioning has shown promise in improved capturing of local details. [5; 27] proposed learning a local latent vector per partition. The former passed the latent vector as an input to the INR, while the latter used it as input for a modulator function. Both have shown that when encoding scenes with intricate details, partitioning allows for finer detailed reconstruction. Similar ideas were presented in [40; 19].  used multiple INRs, handling partitions of different resolutions, and  proposed an extension by clustering partitions based on shared patterns. Similar clustering methods where leveraged for 3D face modeling [46; 47].  used an INR with sparse heads, corresponding to image partitions, to improve generalization. Partitioning was additionally used to enhance compression capabilities. Specifically,  proposed a compression scheme, where a preliminary partitioning step is performed on large scale signals. Similarly,  employed a hierarchical tree structure, sharing parameters among similar partitions.

Partitioning has also been used to optimize latency.  accelerated training by employing multiscale coarse-to-fine INRs and dynamically selecting regions needing finer details. While compelling, this results in uneven weight distribution throughout partitions of the signal, hindering cropping with relative weight reduction.  accelerated convergence by partitioning the input signal space via semantic segmentation and training smaller INRs per segment.  accelerated INRs by independently learning each input dimension, which shares a resemblance to partitioning methods. Inspired by long 3D rendering time, [35; 15] offered alternative compact INR-per-partition approaches. As mentioned before, while these methods could apply to our case, we aim to avoid the extra full INR training step.

## 3 Method

### Background

In its core, an INR is a mapping function, \(F:\), where \(^{n}\) is the input coordinate space, \(^{m}\) is the values space, and \(F\) is a neural network. An RGB image, for example, corresponds to \(^{2}\) for pixel coordinates and \(^{3}\) for RGB values. A standard choice for \(F\) is an MLP, meaning it is composed of a series of fully connected linear layers, followed by non-linear activation functions. Before being passed to the network, the input coordinates undergo some transformation. In SIREN, the input coordinates are normalized, commonly to \((-1,1)\), and the activation functions are \(sine\). A SIREN layer is defined by \(_{i}(x)=((_{i} x+ _{i})),\) where \(_{i}\), \(_{i}\) are the learned parameters of layer \(_{i}\) and \(\) is a hyperparameter representing the frequency of the \(sine\) wave.

### Partitioning the signal

We begin by partitioning the signal, _as cropping is achieved at the partition level_. We assume the input coordinate space \(\) can be bounded by an \(n\) dimensional hyperrectangle, where the \(i\)-th dimension has boundaries \([B_{}^{i},B_{}^{i}]\). Each dimension is split into \(C_{i}\) equally sized partitions, resulting in \(_{i=1}^{n}C_{i}\) non-overlapping partitions. The coordinate values are determined before partitioning and remain unaltered thereafter. Each of the partitions is indexed by \(n\) coordinates, \((P_{0},P_{1},,P_{n-1})^{n}\), where \(0 P_{i} C_{i}\). This process is demonstrated in Figure 2. A point \(p\) with coordinates \((p_{0},p_{1},,p_{n-1})\) is mapped to its respective partition by:\[(p)=(-B_{}^{0}}{_{0}} ,,-B_{}^{n-1}}{_{n- 1}}),\]

where \(_{i}=}^{i}-B_{}^{i}}{C_{i}}\) represents the size of each partition in dimension \(i\).

### Local-Global architecture

Our architecture is composed of _Local Sub-Networks_, each responsible for a specific partition, and a _Global Sub-Network_, familiar with the entire signal. This design facilitates simultaneous extraction of _local features_ using the former and _global features_ using the latter. The _Merge Operator_ is responsible for combining the intermediate local and global features throughout the architecture. Here, for example, we describe our approach with SIREN as the baseline INR, utilizing \(sine\) nonlinearities throughout the network. An overview of our proposed architecture is demonstrated in Figure 3.

Local sub-networksWe employ multiple local sub-networks, each for a specific partition without shared weights, for a total of \(_{i=1}^{n}C_{i}\) sub-networks. The local sub-networks are composed of SIREN layers, interleaved with the merge operator, described below. The final layer of each local sub-network outputs the partition's values. Given an input point \(p\), we pass it on to the local sub-network corresponding to \((p)\). Cropping is natively supported by eliminating entire local sub-networks. While similar to the idea of KiloNeRF, in Local-Global SIRENs we do not require additional pre-training or knowledge distillation steps, significantly reducing overheads.

Global sub-networkWhile the local sub-networks are able to extract details of local patterns, they lack global context, resulting in a subpar reconstruction accuracy compared to SIREN. This is demonstrated qualitatively in Figure 4, and quantitatively throughout our experiments. To remediate this, we utilize an additional SIREN, termed the global sub-network. Its objective is to extract features that are crucial for the global context of the signal. The global features are subsequently combined with the local features through the merge operator. The global sub-network comprises one less layer compared to the local sub-networks; it does not output signal values but is used solely to augment the intermediate local features with global contextual information.

Merge operatorThe merge operator is performed per coordinate, meaning that a coordinate's local features are only merged with the same coordinate's global features, upholding the definition of an INR. The merge operator consists of (1) a concatenation operation, and (2) a linear layer with a subsequent activation function. The linear layer reduces the concatenated vector's size, to the hidden size of the local sub-networks. It is given by:

\[(,)=(([,])+)\] (1)

where \(\) and \(\) represent the local and global features, respectively. The term \(\) denotes the activation function, in our case, \(sine\). \(\) and \(\) are the weight matrix and bias term of the linear layer, respectively, and are shared throughout the network.

### Cropping and extending signals

Cropping is achieved at the partition level, meaning the precision of the cropping operation is directly influenced by the granularity of input space partitioning--smaller partitions enable more precise cropping. _Cropping involves identifying local sub-networks associated with the selected partitions for removal, and eliminating their weights_. In our experiments, we show that for image encoding, using partitions of \(32 32\) pixels results in respectable reconstruction accuracy, even surpassing the baseline INR. Recall that our objective for the cropping operation is to remove a number of weights proportionate to the removed segment of the signal, all without retraining. Since the weights of the global sub-network and the merge operator, collectively referred to as the

Figure 2: Example of partitioning an image. This trivial example uses \(C_{0}=3,C_{1}=2\). To achieve flexible cropping, one must choose larger partition factors.

global weights_, must be preserved, they should ideally encompass a relatively small part of the overall architecture. Additionally, opting for a larger global sub-network increases training and inference times due to the quadratic increase in the floating point operations associated with the fully connected layers. We found that allocating \(5{-}15\%\) of the parameters to the global weights is sufficient for achieving the desired reconstruction accuracy, while also facilitating accelerated training and inference speeds, given by the dominance of the local sub-networks in the architecture.

Extendingan encoded signal may also be achieved at the partition level. Since we cannot exclude a learning stage, the process is achieved by including additional local sub-networks, corresponding to the newly encoded partitions, followed by fine-tuning steps. As some partitions of the signal are already encoded, we use them to initialize the new weights. Similar to a mirror padding operation, at the signal's borders, we copy the local sub-network's weight symmetrically. We show in Section 4.4, that this process allows improved encoding of concatenated partitions compared to alternatives.

### Additional properties

An interesting attribute of our proposed architecture is its capability to balance between the accuracy of the reconstructed signal and the efficiency, in terms of training and inference speeds, achieved by tuning the partition factors. **Increasing the number of partitions reduces neuron interconnectivity thus enhancing speed, while reducing them results in more accurate encoding.** This is explored in Section 4.5. Another interesting finding is that the local sub-networks can naturally exploit cases with significant differences between partitions. Consequently, our method manages to capture local details with fewer iterations. A qualitative example of this phenomenon is provided in Section 4.2.

### Automatic partitioning and implementation

Our implementation offers either fixed or automatic signal partitioning configurations. With automatic partitioning, _the partition factors and sub-network hidden dimensions are automatically determined_ based on the target partition size (e.g., \(32 32\) pixels in an image). This automatic approach reduces manual hyperparameter tuning and streamlines the training process of a Local-Global SIREN. Additional details are in Appendix A. We implement the local sub-networks using a custom locally connected (LC) layer, leveraging _torch_'s batched matrix-multiply operation-_bmm_, significantly outperforming a trivial implementation. During training, we use a batch containing the same number of sampled coordinates in each partition. If needed, one can use a portion of the LC layer's weights to reconstruct only the requested partitions.

LimitationsWhile Torch-_bmm_ has been useful in our case, it is known to be unstable, in terms of latency, across different GPUs. In addition, as cropping is possible at the partition level, it is limited in terms of flexibility. However, the extensive experiments presented highlight our method's potential.

Figure 3: Illustration the Local-Global SIREN architecture and inference flow for two image coordinates \(p_{0},p_{1}\). The coordinates are passed through (a) the global sub-network and their partition’s corresponding (b) local sub-network. Note that the coordinates are distinct elements in a batch, meaning that a coordinate’s local features are only merged with the same coordinate’s global features.

## 4 Experiments

We evaluate our method on various signal encoding tasks and compare it to alternatives. To our knowledge, no other INR architecture supports cropping as we outlined earlier. Existing methods either necessitate an extra knowledge distillation step, or do not uniformly distribute weights across signal partitions, as in MINER . Thus, we compare our approach to an _INR-per-Partition_ and a full INR. We focus on SIREN as a baseline, and later examine INCODE for various tasks. We use roughly the same network size when comparing methods. **The full configuration and networks' size for all experiments is in Appendix B**. Since we manage to improve upon a full INR, comparing to a method based on knowledge distillation is redundant. For completeness, Appendix C shows why an approach like  does not trivially improve image encoding, and Appendix D demonstrates limitations of a hierarchical approach like MINER, in providing the discussed cropping capabilities.

We start with image, audio, video, and 3D shape encoding tasks. Local-Global SIRENs consistently outperform SIREN-per-Partition, and surpass SIREN while facilitating cropping in relatively fine granularity. Due to space constraints, 3D shape experiments are in Appendix E. Next, we present how Local-Global SIRENs allow extending a previously encoded signal, explore the tradeoff between latency and accuracy when choosing the partitioning factors, and explore the effects of global weight ratios. We further apply our Local-Global approach on INCODE, evaluating it on image encoding and various downstream tasks, showcasing its potential for boosting downstream performance. To ensure a fair comparison, we primarily followed the default omega values and learning rates from baseline methods. In some experiments, we made slight adjustments to the learning rate, ensuring that the selected values benefit all compared architectures. We ran the experiments multiple times on a single Nvidia RTX3090. For most experiments, we report the mean Structural Similarity (SSIM) with the mean and standard deviation of the Peak Signal-to-Noise Ratio (PSNR). Additional ablation experiments are in Appendix F.

### Image encoding

We begin by evaluating our method on image encoding tasks. Qualitative results visualizing the training process on the famous \(512 512\) Cameraman image, using partition factors \(C_{0}=16,C_{1}=16\) are shown in Figure 4. Next, from the DIV2K dataset , we have randomly selected a subset of

Figure 4: Encoded images throughout training iterations. PSNR values are at the top left of each image. Method names are on the left. Notice the artifacts in the SIREN-per-Partition method and the reduced noise in our approach compared to SIREN. For extended qualitative results of various signals and cropping operations, refer to https://sites.google.com/view/local-global-inrs.

25 images which were downsampled by a factor of four before training. We have trained Local-Global SIRENs using both fixed and automatic partitioning configurations. For the fixed scheme, we set the partition factors to \(C_{0}=16,C_{1}=16\), thus splitting images into partitions ranging from 21-32 pixels, with the global weights comprising \(8\%\) of the network. The automatic scheme used a target partition size of \(32 32\) pixels, automatically selecting partition factors, and the global weights accounted for roughly \(11\%\) of the network on average. Each network was trained for 2k iterations with a learning rate of \(5 10^{-4}\). Table 1 presents the mean SSIM and PSNR values on the DIV2K subset, demonstrating the efficacy Local-Global SIREN. These results indicate our approach significantly improves upon the SIREN-per-Partition baseline without requiring additional training steps.

### Audio encoding

We continue by evaluating our method on encoding audio clips, taken from . We encode the first 7 seconds of Bach's Cello Suite No. 1: Prelude (_Bach_), and a 12 second clip of an actor counting 0-9 (_Counting_). We use a partitioning factor \(C_{0}=32\), allowing cropping in roughly \(220\,\) and \(370\,\) intervals, respectively. The global weights account for \(10.5\%\) of the network. We train each network for 1k iterations with a learning rate of \(10^{-4}\). Local-Global SIRENs significantly outperform other methods, as seen in Figure 5. The local sub-networks can exploit the heterogeneous nature of audio partitions, showcasing less noise in silent regions, and a noticeable improvement in accuracy. Due to space constraints, quantitative results for both clips are in Appendix G.

### Video encoding

Next, we evaluate our method on a video encoding task. We encode the 12 second cat video from , which has a spatial resolution of \(512 512\) and contains 300 frames. We use partition factors \(C_{0}=5,C_{1}=8,C_{2}=8\), meaning we split the video in both the temporal and spatial dimensions. After training, the video can be cropped to \(2.4\,\) intervals, of \(64 64\) pixels. The global weights take up \(3.5\%\) of the network. We train each network for 5k iterations with a learning rate of \(10^{-4}\). Another useful property of Local-Global SIRENs is its lower memory bandwidth constraints, allowing more pixels to be sampled per training iteration when encoding these large signals. We provide results for sampling \(38 10^{-4}\%\) of pixels, following the original paper, and demonstrate the effect of sampling more. The mean SSIM and PSNR values, computed on all frames, are pre

    & **Partition** &  &  \\  & **Factors** & & \\   SPP & (16, 16) & 0.957 & 31.73 \(\) 0.63 \\ SPP & Auto & 0.955 & 31.90 \(\) 0.64 \\  LGS (ours) & (16, 16) & 0.968 & 33.94 \(\) 0.64 \\ LGS (ours) & Auto & **0.971** & **34.13 \(\) 0.59** \\  SIREN & - & 0.966 & 33.57 \(\) 0.65 \\  

Table 1: Mean encoding results on 25 DIV2K images using five random seeds per image. Automatic partitioning uses partition factors \(11 C_{i} 16\) to ensure \(32 32\) pixel partitions. SPP, LGS stand for SIREN-per-Partition and Local-Global SIREN, respectively.

Figure 5: Encoded _Bach_ audio clips. Mean PSNR values using 10 random seeds are on the top left of each figure.

Figure 6: Three frames of an encoded video. PSNR is at the top left of each frame.

sented in Table 2. Qualitative results of three frames is in Figure 6. While SIREN-per-Partition achieves respectable results, artifacts remain apparent. Local-Global SIREN significantly reduces these artifacts and captures details better than SIREN. Cropped video examples are in Appendix H.

### Extending an encoded image

We present how Local-Global SIRENs allow extending a previously encoded signal. We start by encoding the top \(512 256\) pixels of the Cameraman image from Section 4.1, using a SIREN and a Local-Global SIREN, both roughly 55% the size of the networks previously used. In our method, extending a signal is done by adding new local sub-networks, corresponding to the novel partitions, followed by fine-tuning. Thus, we add \(16 8\) local sub-networks. We compare this strategy with (1) fine-tuning the smaller SIREN on the entire signal, and (2) training a new full-sized SIREN. Figure 7 shows that, unsurprisingly, our approach outperforms both methods, since it can scale up the number of parameters while leveraging previous knowledge. Notice how fine-tuning the smaller SIREN stagnates fast, as it lacks capacity to encapsulate the entire signal. An extended plot is in Appendix I.

### Partitioning and global weights effects

There is an inherent tradeoff between latency and accuracy in Local-Global SIRENs, achieved by tuning the signal partition factors. To demonstrate the tradeoff, we encode the Cameraman image from Section 4.1 and the cat video from Section 4.3 using various partition factors. Results are in Table 3. Note how **enlarging the partition factors leads to faster training**, while decreasing the partition factors enhances reconstruction accuracy, surpassing SIREN. We additionally explore the extreme case of setting the partition factors to \(1\) in Appendix J. Next, we explore the effect of adjusting the ratio of global weights in the network. As shown in Table 4, larger global weights do not significantly boost accuracy and can sometimes have an adverse effect.

  
**Method** & **SSIM**\(\) & **PSNR (dB)**\(\) \\   SPP & 0.826 & 29.58 \(\) 0.02 \\ LGS (ours) & **0.854** & **30.28 \(\) 0.05** \\ SIREN & 0.815 & 29.71 \(\) 0.09 \\  SPP (*) & 0.854 & 30.83 \(\) 0.01 \\ LGS (*) (ours) & **0.888** & **31.91 \(\) 0.02** \\  

Table 2: Mean video encoding results, using 10 random seeds. (*) next to method stands for sampling \(2 10^{-2}\%\) of pixels in each iteration. SPP, LGS stand for SIREN-per-Partition and Local-Global SIREN, respectively.

  

Figure 7: Log-scaled training MSE for extending a previously encoded image, using 10 seeds. Our approach (green) outperforms alternatives. FT, FS stand for fine-tuning and from-scratch, respectively.

### Local-Global INCODE

To further highlight the potential of the Local-Global approach, we examine the SOTA INCODE  INR as a potential baseline. INCODE contains a main MLP, harmonizer and a task-specific network. For a Local-Global INCODE, we modify the main MLP as seen earlier. The harmonizer, which configures properties of the nonlinearities, is adapted to output modulators for the local sub-networks, global sub-network and merge operator (i.e. 12 scalars). Since the task-specific model (e.g., ResNet ) is pre-trained and may remain frozen, we do not consider its weights as INR-specific. We compare Local-Global INCODE to INCODE and an INCODE-per-Partition on the image encoding task from Section 4.1, with results shown in Table 5. The global parameters constitute \(15.5\%\) of the network, with the harmonizer taking an additional \(3\%\). We train each network for 2k iterations, with a learning rate of \(10^{-3}\) for local weights and \(5.5 10^{-4}\) for global weights.

Downstream tasksINCODE showcased SOTA results on various downstream tasks. We demonstrate our approach's ability to boost downstream performance by replicating key experiments from the original paper: image denoising, super-resolution, and CT reconstruction. Qualitative results are in Figure 8, training configuration is in Appendix B.2 and full details and results are in Appendix K.

## 5 Conclusions and future work

In this paper we introduced Local-Global INRs, a novel architecture extension designed to seamlessly support cropping operations with a proportional weight decrease, without an additional pre-training or fine-tuning step. Local-Global INRs utilize both local and global contextual information, and have have surpassed alternative methods in terms of reconstruction accuracy. We further demonstrated how adjusting the signal partitioning allows for a balance between latency and the accuracy of the reconstructed signal. Furthermore, we have showcased instances where Local-Global INRs outperform the baseline INR's reconstruction accuracy, illustrated their capability to extend a previously encoded signal, and demonstrated their use in enhancing downstream performance. We believe our

    & **SSIM**\(\) & **PSNR** \(\) \\  & **positions (\%)** & **(dB)** \\    & 11.6\% & **0.934** & 32.00 \(\) 0.39 \\  & 22.9\% & 0.931 & **32.24 \(\) 0.48** \\  & 33.5\% & 0.925 & 32.06 \(\) 0.41 \\   & 3.5\% & **0.854** & **30.28 \(\) 0.05** \\  & 10.2\% & 0.850 & 30.04 \(\) 0.07 \\   

Table 4: Encoding the Cameraman image (Section 4.1) and the cat video (Section 4.3) using different proportions of global weights, using 10 seeds.

Figure 8: Local-Global (LG) INCODE applied to downstream tasks. Top-left: 4x image super-resolution, top-right: CT reconstruction, bottom: image denoising. Mean PSNR and SSIM values across 10 seeds are displayed in the top-left and top-right corners of each frame, respectively.

    & **SSIM**\(\) & **PSNR** (dB)**\(\) \\    & 0.963 & 36.42 \(\) 0.10 \\  & **0.974** & **39.03 \(\) 0.16** \\   & INCODE & 0.963 & 38.79 \(\) 0.38 \\   

Table 5: Mean encoding results on 25 DIV2K images using five random seeds per image. We use partition factors \(C_{0}=8,C_{1}=8\). IPP, LGI stand for INCODE-per-Partition and Local-Global INCODE, respectively.

proposed method represents a stride towards editable INRs and hope our findings will inspire further exploration into the design of INRs that inherently support modifications.

There are several directions for future work. Firstly, exploring additional architectural modifications, such as alternative merge operators, to refine our method's capabilities. Next, extending the Local-Global approach to INRs beyond MLP-based networks holds promise. Additionally, incorporating semantically meaningful partitions, as presented in , into our Local-Global architecture might further accelerate training and improve quality. Lastly, with Local-Global INRs representing each partition with a fixed amount of weights, there is potential in leveraging these networks for partition-based downstream tasks within INRs, as an extension to ideas in .