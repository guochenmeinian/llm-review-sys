# Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text

Xinyang Li\({}^{1}\)1 Zhangyu Lai\({}^{1}\) Limning Xu\({}^{3}\) Yansong Qu\({}^{1}\)

**Liujuan Cao\({}^{1}\)2 Shengchuan Zhang\({}^{1}\) Bo Dai\({}^{4,2}\) Rongrong Ji\({}^{1}\)**

\({}^{1}\) Key Laboratory of Multimedia Trusted Perception and Efficient Computing,

Ministry of Education of China, Xiamen University

\({}^{2}\) Shanghai Artificial Intelligence Laboratory,

\({}^{3}\) The Chinese University of Hong Kong, \({}^{4}\) University of Hong Kong

###### Abstract

Recent advancements in 3D generation have leveraged synthetic datasets with ground truth 3D assets and predefined camera trajectories. However, the potential of adopting real-world datasets, which can produce significantly more realistic 3D scenes, remains largely unexplored. In this work, we delve into the key challenge of the complex and scene-specific camera trajectories found in real-world captures. We introduce **Director3D**, a robust open-world text-to-3D generation framework, designed to generate both real-world 3D scenes and adaptive camera trajectories. To achieve this, (1) we first utilize a Trajectory Diffusion Transformer, acting as the _Cinematographer_, to model the distribution of camera trajectories based on textual descriptions. (2) Next, a Gaussian-driven Multi-view Latent Diffusion Model serves as the _Decorator_, modeling the image sequence distribution given the camera trajectories and texts. This model, fine-tuned from a 2D diffusion model, directly generates pixel-aligned 3D Gaussians as an immediate 3D scene representation for consistent denoising. (3) Lastly, the 3D Gaussians are further refined by a novel SDS++ loss as the _Detailer_, which incorporates the prior of the 2D diffusion model. Extensive experiments demonstrate that Director3D outperforms existing methods, offering superior performance in real-world 3D generation.

Figure 1: Given textual descriptions, Director3D employs three key components: the Cinematographer generates the camera trajectories, the Decorator creates the initial 3D scenes, and the Detailer refines the details.

## 1 Introduction

Generating 3D scenes from texts holds great promise for industries such as gaming, robotics, and VR/AR. Previous methods [1; 2; 3], which use score distillation sampling (SDS) to optimize 3D representations such as Neural Radiance Fields (NeRFs) [4; 5], involve lengthy and unstable optimization processes. In contrast, newer approaches employ feed-forward networks [6; 7], _e.g._, diffusion and reconstruction models, to directly generate 3D representations from text or text-guided multi-view images, significantly enhancing generation speed. Moreover, advancements in 3D Gaussian Splatting  further accelerate training and rendering speeds, driving the next wave of progress in text-to-3D generation [9; 10; 11; 12; 13]. However, most existing methods focus solely on object-level 3D generation. Recently, preliminary works [14; 15; 16; 17; 18] have begun addressing scene-level 3D generation. Despite these efforts, visual quality, generation speed, and generalization remain suboptimal due to reliance on only 2D priors or limited few-classes 3D datasets.

In this work, we leverage real-world datasets (_e.g._, MVImgNet  and DL3DV-10K ) to achieve realistic text-to-3D generation. However, real-world captures from in-the-wild scenes differ significantly from traditional object-level synthetic datasets, introducing new requirements for the text-to-3D generation framework. Firstly, real-world captures feature complex, unpredictable, and scene-specific camera trajectories, unlike the controlled and predefined settings in object-level synthetic datasets like Objaverse  shown in Fig. 3 (Left). Secondly, real-world scenes include unbounded backgrounds, complicating the use of common bounded 3D representations such as Tri-planes . Lastly, the diversity and quantity of real-world captures are limited, potentially decreasing the generalization ability for open-world texts.

We address these challenges with a novel framework, Director3D, illustrated in Fig. 1 and Fig. 3 (Right). Fig. 2 shows that our framework supports generating 3D scenes across various domains. In summary, our approach includes the following three key components:

\(\) Traj-DiT (Trajectory Diffusion Transformer) as Cinematographer: Generates dense-view camera trajectories from text. Camera parameters (intrinsics and extrinsics) are treated as temporal tokens, and a Transformer model performs conditional denoising of the camera trajectory.

\(\) GM-LDM (Gaussian-driven Multi-view Latent Diffusion Model) as Decorator: Uses a sparse-view subset of the camera trajectory for image sequence diffusion, generating pixel-aligned and unbounded 3D Gaussians as intermediate 3D representations. This model, fine-tuned from a 2D latent diffusion model, leverages strong priors and collaborative training with multi-view and single-view data to mitigate the limited diversity and quantity of real-world captures, enhancing generalization.

\(\) SDS++ Loss as Detailer: Enhances visual quality of the 3D Gaussians by back-propagates a novel SDS++ loss from images rendered at randomly interpolated cameras within the trajectory.

Figure 2: Multi-view image results rendered with the generated camera trajectories and 3D scenes.

## 2 Related Works

**Text-to-3D Generation.** Since the introduction of DreamField , which combines the vision-language model CLIP  with NeRFs , there has been substantial progress in the area of text-to-3D generation. DreamFusion  and SJC  employ 2D image diffusion models to refine the 3D representation through Score Distillation Sampling (SDS). Subsequent methods [26; 27; 28; 29; 30] enhance the formulations to more effectively utilize 2D diffusion models for text-to-3D generation, achieving greater stability and improved visual quality. Some methods [3; 31; 32; 33] suggest incorporating an additional 3D prior into 2D diffusion models to enhance 3D consistency. Additionally, various methods [34; 35; 36; 37; 7; 38] employ 3D diffusion models to directly produce 3D assets. However, most of these works primarily focus on object-level generation. For scene-level 3D generation, preliminary works [15; 14; 39; 40; 41; 16; 42] combine image inpainting models [43; 44] and monocular depth estimation models [45; 46] to progressively lift an image into a 3D scene with a user-defined camera trajectory. Nonetheless, the multi-view inconsistency of image inpainting and monocular depth estimation can lead to undesirable geometry and texture artifacts. Also, some works [47; 48] propose using panorama image diffusion models for 3D scene generation, which is limited to some specific types of scenes such as indoor rooms. Therefore, developing a text-to-3D scene generation method with open-world generalization capabilities remains unsolved.

**3D Gaussian Splitting (3DGS)** introduces parameterized 3D Gaussians as 3D representation and splatting-based rasterization technique for novel view synthesis based on dense-view images, significantly reducing the rendering time compared to NeRF-based methods [4; 49; 5; 50; 51]. Current methods concentrate on improving the geometry quality [52; 53; 54], stabilizing the training , adapting it to dynamic scene modeling [56; 57; 58; 59] and applying it to different applications [60; 61; 62; 63; 64]. Further, to train a generalizable sparse-view reconstruction model with 3D Gaussians as the intermediate representation, some methods [65; 66; 67; 38; 12; 13] propose to convert image features into pixel-aligned 3D Gaussians and optimize reconstruction models through back-propagating losses from rendered images.

Meanwhile, DreamGaussian employs 3DGS for text-to-3D generation, with SDS loss as the optimization objective. GaussianDreamer  and GSGen  further enhance the generation quality and 3D consistency by initializing the 3D Gaussians based on point cloud diffusion models, Point-E . The generalizable sparse-view reconstruction model (_e.g._, GRM , LGM , and GS-LRM ) for 3D Gaussians can also facilitate text-to-3D generation, using multi-view diffusion models [3; 68] to acquire sparse-view images as inputs. Specifically, GS-LRM takes a step forward by employing a large video generation model (i.e., SORA ) for text-to-3D scene generation. However, these works heavily rely on the 3D consistency of the multi-view images, which the 2D-based diffusion models can not guarantee. Distinctively, our GM-LDM employs pixel-aligned 3D Gaussians as the intermediate 3D representation for rendering-based multi-view diffusion, directly enforcing 3D consistency during the diffusion process and producing 3D representations.

## 3 Preliminary

**Latent Diffusion Models (LDMs)**[44; 70] consist of two key components: an auto-encoder  and a latent denoising network. The autoencoder establishes a bi-directional mapping from the space of the original data to a low-resolution latent space: \(z=(x),x=(z),\) where \(\) and \(\) are the encoder and decoder, respectively. The latent denoising network \(_{}\) is trained to denoise noisy latent

Figure 3: **Left**: Comparison of the simplified camera trajectory distributions between synthetic and real-world multi-view datasets. **Right**: Pipeline and models of Director3D.

given a specific timestep \(t\) and condition \(y\). Its training objective for \(\)-prediction is defined as:

\[L=_{x,(0,1),t}\|-_{ }(z_{t},y,t)\|_{2}^{2},\] (1)

where the noisy latent is obtained by \(z_{t}=_{t}}E(x)+_{t}}\), \(_{t}\) is a monotonically decreasing noise schedule and \((0,1)\) is a random noise. During inference, a random noise is sampled as \(z_{T}(0,1)\). By continuously denoising the random noise \(z_{T}\) with condition \(y\) (_e.g._, text embedding), we can derive a fully denoised latent \(\). Then, the denoised latent \(\) is fed into the latent decoder \(\) to generate the high-resolution image \(=()\).

**Multi-view Diffusion Models** aim to model the distribution of multi-view images \(\) with 3D consistency, where each image is captured by a distinct camera within the same static 3D scene. Its objective for \(x_{0}\)-prediction can be written as:

\[L=_{,(0,1),t}\|-_{}(_{t},,y,t))\|_{2}^{2},\] (2)

where \(\) represents the camera parameters for the different views. Early works [3; 31] in this field are based on 2D LDMs. They fine-tune the 2D LDMs by integrating cross-view connections between the multi-view images into the original single-view 2D LDMs, using multi-view data rendered from 3D datasets. These methods lack strict 3D consistency since there is no actual 3D representation during multi-view denoising. A more advanced approach, DMV3D , employs a 3D reconstruction model to generate noise-free 3D representations and predict multi-view images from noisy multi-view inputs by a rendering-based denoising process. This enables 3D generation tasks to be accomplished without per-asset optimization during inference.

**Score Distillation Sampling (SDS)**[1; 2] uses a pretrained 2D diffusion model to optimize 3D representation. Considering a differentiable 3D representation parameterized by \(\) and a rendering function denoted as \(\), the rendered image produced for a given camera pose \(c\) can be expressed as \(x=(,c)\). SDS distills the prior of a 2D LDM to optimize 3D representation \(\) as follows:

\[_{}_{}=_{t,,c} [w(t)(-)( (,c))}{}]\] (3)

where \(\) is the ground truth noise, \(\) is the noise predicted by the 2D LDM with \(z_{t}\) as input for timestep \(t\), and \(w(t)\) represents a weighting function that varies according to the timestep \(t\). The SDS loss can be also converted into a reconstruction-like objective :

\[_{}=_{t,,c}[w(t)_{t}}}{_{t}}}\|z-\|_{2}^{2}], =(z_{t}-_{t}})/_{t}}\] (4)

## 4 Method

### Problem Formulation and Overview of Director3D

We consider the multi-view dataset of real-world captures as a joint distribution of image sequences and camera trajectories conditioned on texts, denoted as \(p((,)|y)\). Here, \(=\{x_{i}\}_{i=1}^{M}\) represents the image sequence, \(=\{c_{i}\}_{i=1}^{M}\) denotes the camera trajectory, and \(M\) is the number of views. To model this joint distribution, we separately handle the conditional distributions \(p(|y)\) and \(p(|(,y))\) (see Appendix B for detailed discussions). Furthermore, we model each image in the sequence as a rendered view of a unified 3D scene representation \(\) under the corresponding camera, expressed as \(x_{i}=(,c_{i})\), where \(\) is the 3D rendering function.

Director3D addresses this by incorporating three collaborative processes analogous to roles in film production: the Cinematographer, the Decorator, and the Detailer. Firstly, the Trajectory Diffusion Transformer (Traj-DiT), serving as the Cinematographer, models the distribution of dense-view camera trajectories, as detailed in Sec. 4.2. For image sequences, directly modeling the dense-view distribution is complex and resource-intensive. To address this, we use a Gaussians-driven Multi-view Latent Diffusion Model (GM-LDM), acting as the Decorator, to model the image distribution through a sparse subset of dense views. This model utilizes pixel-aligned 3D Gaussians as the intermediate representation, described in Sec. 4.3. Finally, to improve the visual quality of the generated 3D scenes, we employ a novel SDS++ loss, functioning as the Detailer, to refine the 3D Gaussians through dense-camera interpolation rendering, as presented in Sec. 4.4.

### Traj-DiT as Cinematographer

To model the trajectory distribution, we represent the camera trajectory \(\) as a set of camera parameters \(c_{i}=\{_{i},_{i},_{i},_{i}\}\), where \((3)\) and \(^{3}\) are the rotation and translation of the camera poses, \(_{+}^{2}\) is the focal lengths and \(^{2}\) is the principle points. To ensure consistency and comparability across scenes, we normalize the trajectory for each scene in two steps: First, we convert all camera poses to be relative to the first one so that the first camera pose is an identity matrix; Then, we re-scale the translation to make the distance from the first to the farthest camera to \(1\). We adapt the architecture of the Diffusion Transformer (DiT)  to generate camera trajectories, as illustrated in Fig. 4 (Left). The temporal order of real-world captures, akin to video sequences, necessitates a learnable temporal embedding to differentiate between cameras of different frames. This embedding helps the model capture the sequential dependencies inherent in real-world data. Each DiT block includes a cross-attention layer to extract information from text embeddings encoded by the CLIP  text encoder. Additionally, the timestep \(t\) modulates the pre-layer normalization and post-layer output scalar, similar to the original DiT, allowing the model to learn temporal dynamics effectively. The model is trained to minimize the \(x_{0}\)-prediction diffusion objective:

\[L=_{,(0,1),t}\| -_{}(_{t},y,t)\|_{2}^{2},\] (5)

where \(_{}\) is the parameterized Traj-DiT model and \(_{t}\) is the noisy camera trajectory.

By leveraging the strengths of the DiT architecture, we aim to enhance the fidelity and coherence of the generated trajectories, instead of relying on pre-defined ones. We showcase two examples of the predicted camera trajectories for different denoising steps \(t\) in Fig. 4 (Right), demonstrating the effectiveness of our model in generating smooth and accurate camera paths.

### GM-LDM as Decorator

We propose GM-LDM to model the image sequence distribution \(p(|(,y))\) and generate immediate 3D Gaussians as the joint 3D scene representation. The GM-LDM, fine-tuned from the Stable Diffusion model with a slightly modified architecture, leverages its image generation prior to enhance 3D scene generation. For efficiency, diffusion is applied to a sparse-view subset of the camera trajectory, significantly reducing computational overhead. During training, sparse-view images are processed through the frozen latent encoder \(\) to obtain multi-view latents \(^{N c h w}\), where \(N M\). Noise is then added to these multi-view latents \(\) to produce noisy latents \(_{t}\).

2D-based Denoising.The noisy multi-view latents \(_{t}\) are fed into the latent denoising network \(Z_{}\) in parallel. For convenience, we modify the \(\)-prediction of the original Stable Diffusion model to \(x_{0}\)-prediction. The denoised multi-view latents are obtained as \(\{},\}=Z_{}(\{_{t}\},^ {},y,t)\), where \(\) represents additional multi-view features for enhanced 3D information. Sparse-view cameras \(^{}\) are integrated into the network by combining the ray-maps \((,)\) with the noisy latents, where \(\) and \(\) denote the origin and direction of pixel-aligned rays, respectively. We replace self-attention blocks in the original 2D latent denoising network with cross-view self-attention blocks  to better capture multi-view correlations. The denoised multi-view latents \(}\) are supervised using a simple

Figure 4: **Left**: Architecture of Traj-DiT. **Right**: Visualization of the predicted camera trajectory for different denoising timesteps.

multi-view latent diffusion objective:

\[_{}=_{,c,y,,t}\|-}\|_{2}^{2}.\] (6)

**Rendering-based Denoising.** To generate 3D Gaussians for rendering-based denoising, the denoised multi-view latents \(}\) and additional features \(\) are input into a Gaussians decoder \(_{}\). This decoder outputs Gaussian features \(\{_{i},_{i},_{i},_{i},_{i}\}=_{ }(}_{i},_{i})\), where \(_{i}\), \(_{i}\), \(_{i}\), \(_{i}\), and \(_{i}\) represent the depth, rotation quaternion, scale, opacity, and spherical harmonics coefficients of \(256 256\) 3D Gaussians for view \(i\), respectively. The Gaussians decoder \(_{}\) is initialized with the weights of the original latent decoder \(D\), with re-initialized first and last convolutional layers to handle the additional features and specific Gaussian channels. The predicted depth is then converted into pixel-aligned Gaussian positions \(_{i}=_{i}+_{i}_{i}\). The multi-view 3D Gaussians \(=\{_{i},_{i},_{i},_{i},_{i}\}_{i=1 }^{N}\) are concatenated to jointly represent the 3D scene. During training, views are randomly sampled from the dense-view camera trajectory to supervise the predicted 3D Gaussians in image space, ensuring consistent and accurate 3D scene representation:

\[_{}=_{x,c,y,,t}(x,(,c)),\] (7)

where \(\) is the rendering function, \((,)\) is a reconstruction loss penalizing the difference between images, and \((x,c)(,)\) is the ground truth of an image and camera pair from the dense views. We use a combination of MSE loss and LPIPS  loss for the reconstruction loss \(\), similar to the original reconstruction loss of the original auto-encoder. The total training loss is simply the sum of the above losses: \(=_{}+_{}\). This approach leverages the strengths of multi-view data and pixel-aligned Gaussian representations to enhance the fidelity and coherence of the generated 3D scenes. By fine-tuning from a robust 2D LDM and using a sparse-view subset, we strike a balance between performance, efficiency, and generalizability, enabling the generation of coherent 3D scenes. As shown in Fig. 5 (Left), during inference, images are rendered with the input cameras \(^{}\) and encoded by the latent encoder \(\) to obtain Gaussian-driven denoised latents:

\[}_{}=(}^{}), {where }}^{}=(,^{}).\] (8)

Inspired by Dual3D , GM-LDM inference can toggle between 2D-based and rendering-based denoising. The 2D-based denoising offers better generalization, aligning closely with the original Stable Diffusion, while rendering-based denoising ensures superior 3D consistency due to its immediate joint 3D representation. The 3D Gaussians generated in the final denoising step serve as the initial 3D scene for subsequent refinement.

Collecting and annotating real-world multi-view datasets is laborious, often resulting in limited diversity and quantity, which hinders generalization for open-world texts. To address this, we follow MVDream  and collaboratively train the GM-LDM using both multi-view and 2D datasets to enhance generalization. We treat single-view images as a special case of multi-view images with \(N=M=1\) and apply the same rendering process and training losses, which increases the diversity of training data, thereby improving the model's ability to generalize across diverse scenarios.

Figure 5: **Left**: Architecture of GM-LDM. The model is fine-tuned from a 2D LDM with minor modifications, performing rendering-based denoising for generating initial 3D Gaussians. **Right**: Pipeline of calculating SDS++ loss, which refines the 3D Gaussians with the original 2D LDM.

### SDS++ Loss as Detailer

To enhance the details and visual quality of the 3D Gaussians, we propose the SDS++ loss, leveraging the 2D diffusion prior for refinement. Our research on existing SDS-based methods identifies three key points crucial for success, this includes: (1) **Appropriate Target Distribution**: This ensures that the rendered images align effectively with the textual conditions, avoiding over-smoothing and over-saturation. (2) **Adaptive Estimation of the Current Distribution**[26; 76]: This provides a counter optimization objective, pushing the rendered images away from the current distribution to enhance details. (3) **Latent-space and Image-space Objectives**: Combining these objectives helps prevent noisy or over-smoothing artifacts that may arise from using only one of them. During refining, we first render the 3D Gaussians \(\) with a randomly sampled camera \(c\) from the continuous interpolated camera trajectory to produce an image \(x=(,c)\). This image is encoded into the latent space by \((x)=z\), then disturbed with randomly sampled noise and timestep \(t\) to produce a noisy latent \(z_{t}\). The 2D diffusion model \(_{}\) then predicts the denoised latent \(\) from \(z_{t}\). As illustrated in Fig. 5 (Right), the proposed SDS++ loss can be formulated by:

\[_{}=_{t,c,}[w(t)_{t}}}{_{t}}}(_{z}\|z-\|_ {2}^{2}+_{x}\|x-\|_{2}^{2})],\] (9)

where \(_{z}\) and \(_{x}\) are the weights for latent-space and image-space objectives, respectively, \(\) is the predicted latent, and \(x=()\) is the predicted image. The predicted latent can be derived by Eq. 4. We use a compositional predictions \(\) as follows:

\[=_{}-_{}+.\] (10)

Instead of setting \(_{}=\) as in the standard SDS loss, we introduce a learnable source prediction \(_{}\) for adaptive estimation of the current distribution:

\[_{}=_{}(z_{t},,t),\] (11)

where \(_{}(z_{t},,t)\) uses a learnable text embedding \(\) to efficiently estimate the current distribution. This approach leverages the original latent denoising network  and is trained by minimizing \(\|_{}(z_{t},,t)-\|_{2}^{2}\) along with the refining process. The target prediction employs classifier-free guidance for improved text alignment:

\[_{}=_{}(_{}(z_{t},y,t)-_{}(z_{t},,t))+_{}(z_{t},,t),\] (12)

where \(_{}\) is the classifier-free guidance scale. SDS++ loss integrates the above three key points, ensuring efficient and realistic refinement of 3D Gaussians.

## 5 Experiments

### Implementation Details

We utilize the MVImgNet  for object-level and DL3DV10K  for scene-level real-world multi-view datasets. Text prompts for each scene are generated using the multi-modal large language model InternLM-XComposer . To enhance generalization, we incorporate the 2D dataset LAION . For GM-LDM, we set the lengths of dense and sparse views to \(M=29\) and \(N=8\), respectively. The classifier-free guidance scale is \(7.5\) for 2D-based denoising and \(1\) for rendering-based denoising to ensure 3D consistency. Following Dual3D , we balance 3D consistency and generalization by using 1/10 rendering-based denoising steps. Image and latent resolutions are set to \(256\) and \(32\), respectively. For SDS++ loss, the weights for latent-space and image-space losses are \(_{z}=1\) and \(_{x}=0.01\). \(_{}\) is set to \(7.5\), with refining iterations set to \(1000\). Generating a scene takes approximately 5 minutes. Further implementation details are provided in Appendix A.

### Generation Results.

We show the generation results of camera trajectories and image sequences for various text prompts in Fig. 6. For object-level prompts, the generated camera trajectories typically circle and face the objects, aligning well with the distribution in MVImgNet. In contrast, scene-level prompts yield more diverse and complex camera trajectories, showcasing the effectiveness of our Traj-DiT model. Our method also generates realistic images across different types of prompts, demonstrating the effectiveness and generalization ability of GM-LDM and SDS++ loss. Additional generation results are available in Appendix D. These results highlight the robustness of our approach in handling both object-level and scene-level prompts for 3D scene generation.

### Qualitative Comparison.

We qualitatively compare our method with several baseline methods, as shown in Fig. 7. (1) GRM  is a feed-forward text-to-3D generation method for 3D Gaussians using multi-view images generated by 2D-based diffusion models. It supports only object-level 3D generation. Our comparisons using object descriptions show that GRM produces unrealistic 3D Gaussians limited to objects, due to its training on synthetic datasets. In contrast, our method generates high-quality 3D scenes with both objects and backgrounds. (2) GaussianDreamer  is a state-of-the-art SDS-based method for 3D Gaussians, integrating priors from both 2D and 3D diffusion models. While it can generate objects with ground layers, it tends to produce over-saturated textures. Our method, however, generates more realistic scenes with better handling of shadows, lighting, and material reflections. (3) DreamScene  uses Formation Pattern Sampling and strategic camera sampling for 3D Gaussians. Since it is not open-sourced, we use examples from its project page. Although it generates scene-wide consistent 3D scenes, the results are overly saturated and cartoonish. (4) LucidDreamer , based on Text2NeRF , uses 2D foundation models for 3D Gaussians. While it can generate photo-realistic textures, the multi-view consistency is poor, with visible artifacts at object edges due to inaccurate monocular depth estimation. It also struggles with excessive object generation from descriptive prompts due to reliance on single-view inpainting. These comparisons highlight the superior performance of the proposed Director3D for realistic 3D generation.

Figure 6: Generation results of Director3D for both camera trajectories and image sequences.

Figure 7: Qualitative comparison between Director3D and different baselines.

### Quantitative Comparison.

We present a quantitative comparison between our framework and several baseline models in Tab. 1. For this experiment, we use the Single-Object-with-Surroundings3 set of T3Bench , which contains 100 prompts closely matching the descriptions in MVImgNet. The quantitative results are evaluated using CLIP-Score , NIQE , and BRISQUE  metrics. For each 3D scene generated by different methods, we render a video and uniformly sample 36 frames to calculate the average score for each metric. For baselines without adaptive camera trajectories, videos are rendered by circling around the 3D representations at a fixed elevation. The BRISQUE and NIQE results indicate that our method significantly outperforms existing baseline models in terms of image quality. Additionally, the CLIP-Score shows our method's superior ability to align generated images with their textual descriptions, even without refining. These results underscore the robustness and effectiveness of our framework in generating high-quality, semantically aligned 3D scenes.

We also conduct a quantitative comparison with three scene-level baselines for 32 object-centric prompts and 32 scene-level prompts in the Tab. 2. Our method achieves the highest CLIP-Score and the second-best NIQE Score with the shortest inference time. Without adaptively generated camera trajectories (Traj-DiT) and a high-performance multi-view diffusion model with immediate 3D representation (GM-LDM), ZeroNVS exhibits deteriorated visual quality from multiple viewpoints and requires a time-consuming SDS optimization process from scratch for each scene. LucidDreamer attains the best NIQE Score; However, we observe that it is plagued by multi-view inconsistency, visible artifacts at edges, and excessive objects as shown in Fig. 7.

### User-specific Camera Trajectories

Director3D supports the utilization of both pre-generation and post-generation user-specific camera trajectories. For pre-generation user-specific camera trajectories, users are capable of employing user-specific camera trajectories instead of the generated camera trajectories from Traj-DiT for 3D scene generation. For post-generation user-specific camera trajectories, users can render novel views by providing novel cameras after generating the 3D scene. We develop an interactive demo for visualizing the generated camera trajectories and 3D Gaussians, which is also capable of rendering the 3D Gaussians with novel cameras, as shown in Fig. 8.

   Method & BRISQUE \(\) & NIQE \(\) & CLIP-Score \(\) \\  DreamFusion  & 90.2 & 10.48 & 67.4 \\ Magic3D  & 92.8 & 11.20 & 72.3 \\ LatentNerf  & 88.6 & 9.19 & 68.1 \\ SJC  & 82.0 & 10.15 & 61.5 \\ Fantasia3D  & 69.6 & 7.65 & 66.6 \\ ProlificDreamer  & 61.5 & 7.07 & 69.4 \\  Ours _w/o_ refining & 37.1 & 6.41 & 80.0 \\  Ours & **32.3** & **4.35** & **85.5** \\   

Table 1: Quantitative comparison of different object-centric models with text prompts in T3Bench.

   Method & NIQE \(\) & CLIP-Score \(\) & Inference Time (min) \(\) \\  GaussianDreamer  & 6.96 & 71.8 & 15 \\ ZeroNVS  & 9.84 & 67.2 & 90 \\  LucidDreamer-LLFF  & 3.53 & 83.3 & 40 \\ LucidDreamer-HeadBang  & 3.61 & 82.9 & 40 \\ LucidDreamer-BackForth  & **3.40** & 74.2 & 40 \\  Ours & 4.09 & **83.9** & **5** \\   

Table 2: Quantitative comparison of scene-level models with 64 prompts

### Ablation Study

Director3D is formulated to a sequential process to model the joint distribution of camera trajectories and image sequences. The Traj-DiT component generates the base camera trajectory for each scene, while the GM-LDM provides the initial 3D scene. These two models are crucial for producing meaningful 3D scenes. Our ablation study primarily focuses on the SDS++ loss to highlight its significance. As shown in Fig. 9, we conduct comprehensive experiments to analyze the impact of the SDS++ loss. First, we remove the refining process entirely. Although the initial 3D Gaussians generated by GM-LDM match the input text, the visual quality is unsatisfactory, with missing details. This is expected due to the limited diversity and quantity of the multi-view dataset used for training GM-LDM. Setting \(_{}=\) degrades the SDS++ loss into SDS+ loss . The results show a significant decrease in visual quality and an over-smoothing issue, highlighting the importance of adaptive estimation of the current distribution. Setting \(_{}=1\) turns the SDS++ loss into the LODS loss  with an additional image-space objective. This results in noisy details, as the conditional noise prediction alone does not provide a clear optimization direction. By setting \(_{x}=0\) and \(_{z}=0\) respectively, we observe that using only the latent-space objective leads to noisy details and artifacts, while using only the image-space objective results in missing details.

The full model, incorporating the proposed SDS++ loss, achieves the best visual quality with clear and realistic details. These findings underscore the importance of each component in the SDS++ loss and its role in refining 3D Gaussians. Additional ablation studies, including qualitative and quantitative of more cases and those with randomly generated trajectories, are provided in Appendix E.

## 6 Conclusion

In this paper, we propose an open-world text-to-3D generation framework capable of generating real-world 3D scenes with adaptive camera trajectory, named Director3D. We first introduce a Cinematographer (_i.e._, Traj-DiT) that can generate dense-view camera trajectories given texts. Then, a Decorator (_i.e._, GM-LDM) and a Detailer (_i.e._, SDS++ loss) are proposed for initial generation and further refining, respectively, with 3D Gaussians as the 3D scene representation. We demonstrate the effectiveness of our method with extensive experiments. We believe our work makes essential contributions to the text-to-3D generation community, especially in discovering the potential of leveraging real-world multi-view datasets for realistic 3D generation. Our future works include improving the generation scope, boosting model efficiency and quality, and leveraging more datasets.

Figure 8: Screenshots of the interactive demo for visualizing generated camera trajectories and 3D Gaussians of Director3D. The frames are rendered with novel cameras.

Figure 9: Ablation of SDS++ lossAcknowledgements

This work was supported by National Science and Technology Major Project (No. 2022ZD0118202), the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. 62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province of China (No.2021J01002, No.2022J06001). This project is also funded in part by Shanghai Artificial Intelligence Laboratory.