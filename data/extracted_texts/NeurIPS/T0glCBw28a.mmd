# The ALCHEmnist: Automated Labeling 500x CHEaper Than LLM Data Annotators

Tzu-Heng Huang, Catherine Cao, Vaishnavi Bhargava, Frederic Sala

University of Wisconsin-Madison

{thuang273, ccao35, vbhargava3}@wisc.edu,

fredsala@cs.wisc.edu

###### Abstract

Large pretrained models can be used as annotators, helping replace or augment crowdworkers and enabling distilling generalist models into smaller specialist models. Unfortunately, this comes at a cost: employing top-of-the-line models often requires paying thousands of dollars for API calls, while the resulting datasets are static and challenging to audit. To address these challenges, we propose a simple alternative: rather than directly querying labels from pretrained models, we task models to _generate programs that can produce labels_. These programs can be stored and applied locally, re-used and extended, and cost orders of magnitude less. Our system, **Alchemist**, obtains comparable to or better performance than large language model-based annotation in a range of tasks for a fraction of the cost: on average, improvements amount to a **12.9%** enhancement while the total labeling costs across all datasets are reduced by a factor of approximately **500\(\)**. We release our code here: https://github.com/SprocketLab/Alchemist.

## 1 Introduction

One of the most exciting developments in machine learning is the use of large pretrained models to act as _annotators_ or _labelers_. This includes the use of large language models (LLMs) like GPT-4  and Claude 3 . This process offers multiple benefits. First, pretrained models are an efficient way to annotate and have the potential to partially or fully replace expensive human crowdworkers . Second, this approach allows for _distilling_ large models into smaller, task-specific models that can be deployed locally at lower cost . This is additionally important in settings like healthcare and finance where privacy laws require the use of local models.

Despite this promise, pretrained model-based annotation has several drawbacks that stymie its adoption. These drawbacks include

* **High Cost**: Labeling a dataset can be expensive. This is particularly so in cases where each data point consists of many tokens. For example, we find that labeling a moderately-sized dataset  with 7,569 data points using GPT-4 costs over $1,200.
* **Lack of Extensibility**: Making even small changes to specifications necessitates re-running the entire pipeline to obtain new labels. This inflexibility means the resulting labels are static.
* **Inability to Audit**: API access to pretrained models does not permit inspecting most aspects of the model. Users must simply accept the provided labels with only minimal additional information. Techniques that ask the model for explanations for its decisions may not be reliable .

We address these obstacles through a simple but surprisingly powerful notion. Rather than having pretrained models label data, we task language models to _generate programs that can output labels_. These synthesized programs serve as annotators, capturing the underlying logic used by the models when annotating. In other words, instead of distilling a powerful model to label a dataset (andsubsequently training a smaller model on the labeled data), we _distill directly into code_ (Figure 1). These resulting programs can either make predictions directly or can label training dataset then train a downstream model using it1.

This simple notion resolves all of the challenges related to pretrained model-based annotation. First, _API calls scale with the number of programs instead of the number of data points._ That is, since we generate programs that can themselves make any number of predictions locally at no cost, we can reduce the number of API calls by orders of magnitude. For example, for the dataset described above , the number of GPT-4 calls was reduced from 7,569 (the size of the dataset) to 10 (the number of generated programs), resulting in a massive cost reduction from $1,200 to $0.70, a 1,700-fold decrease. Moreover, code can be easily inspected, corrected, and extended, allowing seamless adaptation when prediction classes or labeling rules change.

While a powerful idea, distilling model into code presents several challenges. First, any particular program may be inaccurate, fail to compile, or may otherwise be flawed, resulting in noisy program outputs. We address this obstacle by applying _weak supervision_, a framework for dataset construction from multiple noisy sources of signal . Next, operating on non-text modalities is challenging. We handle this via a simple two-step approach that first extracts high-level concepts and then uses them in concert with a local feature extractor to enable tractable program generation.

**Contributions.** We propose an alternative approach to replace expensive annotation processes that require repetitive prompting for labels. We developed a system called _Alchemist_ that implements this idea. Empirically, Alchemist improves performance five out of eight datasets, with an average enhancement of 12.9%--while reducing total costs by a factor of approximately \(500\). Finally, we introduce and validate extensions that address non-text modalities.

## 2 Related Work

Our work relates to LLM-based annotation, prompting, and the weak supervision framework.

**Using Large Pretrained Models for Data Annotation.** Large pretrained models have demonstrated powerful capabilities using zero-shot prompting across a wide range of tasks . One promising development is their potential to serve as data labelers, which can reduce the cost and human effort in

Figure 1: Examples of generated programs and their prompts. These are synthesized by GPT-4 for spam detection and cancer identification tasks. Programs use regular expressions (left program) and keyword matching (right program) as their labeling logic to classify data points.

data labeling [1; 2; 11; 12]. Existing research in this area mainly focuses on approaches that allow for more efficient inference, enhanced label generation, and distilling into smaller but specialized labelers [3; 4; 5; 6; 7; 8]. However, _scalability_ is the main limitation in these approaches, as making inferences via querying an API for data examples can be cost-prohibitive. To tackle this challenge, rather than prompting for labels repetitively, we propose prompting pretrained models for programs that use synthesized labeling logic and can thus serve as alternative data labelers.

**Prompt Engineering & In-Context Learning.** In-context learning adapts pretrained models to new tasks without additional fine-tuning . It involves providing relevant examples as demonstrations to solve the task, such as pairs of languages for translation . By including task-specific examples, models can better understand the task at hand. Adding a few data points as demonstrations  is commonly suggested when models act as data annotators. Moreover, they can be selected [24; 25], retrieved , or more efficiently, generated . We explore various types of supplementary information that can be added to Alchemist to help improve program generation and permit more control over the labeling logic used in the programs.

**Weak Supervision Framework.** Weak supervision enables the rapid creation of large training datasets by aggregating cheap-but-noisy signals derived from various labeling sources [18; 19; 21; 28]. These sources can be crafted by domain expertise, using labeling heuristics, or even trained on smaller, weaker classifiers [29; 30; 31; 32]. Recent advancements in code generation open up the potential to automate the heuristic design process. Frameworks such as ScriptoriumWS , and DataSculpt  have been developed to take advantage of code-generating models [35; 9; 36] to craft weak supervision sources through prompting. While similar in spirit to our approach, these have several drawbacks: ScriptoriumWS requires more human effort in prompt engineering to better guide code-generation models. Both ScriptoriumWS and DataSculpt can perform poorly in tasks requiring specific domain expertise and, most importantly, they do not handle modalities beyond text--unlike Alchemist.

## 3 Alchemist System

We begin by presenting a general annotation workflow in Alchemist, followed by a detailed discussion of each key step.

**General Workflow.** The process is depicted in Fig. 2. First, users select an unlabeled dataset and create simple prompts to instruct language models to generate programs that incorporate labeling logic. These prompts can integrate relevant information and may vary in their design, allowing for the synthesis of multiple programs. Next, given a set of generated programs and their outputs, we apply weak supervision techniques to obtain a set of aggregated labels. Finally, the labeled points can be used to train a distilled model that can be stored and used locally.

### Prompting Strategy

We propose a general and extensible prompt template for querying language models to generate annotator programs. This general template consists of three key components:

* **Task Description**: Provides the model an overview of generated program's desired objectives.
* **Labeling Instructions**: Specifies classes and the expected structure of the program's output.
* **Function Signature**: Describes the function's name and the input types to be used.

Figure 2: Overall workflow for Alchemist.

This simple but general template allows for flexible incorporation of various types of information, enabling the model to generate programs that are tailored to specific requirements. Two sample prompt templates in Alchemist are displayed in Fig 1.

**Using Supplementary Information.** Drawing inspiration from few-shot prompting , where users provide demonstrations (i.e., data points with their labels) to enhance generated responses, we explore various types of supplementary information that can be integrated to assist models in synthesizing programs. This approach is particularly useful for scenarios where language models may lack the expertise to generate effective programs, or where specific adaptations in labeling logic are required. Such information can be crafted by users themselves, domain experts or, more efficiently, generated by language models themselves. Additionally, it can be combined with retrieval-augmented generation (RAG) systems  to access external knowledge.

We explore various types of supplementary information to assist in code generation, starting with high-level concepts and then progressively looking into more practical details to control programs.

_Dataset and Prediction Class Description._ First, supplementary information can include relevant background details about the purpose for which the dataset was built and high-level information about the dataset, such as definitions for each label class. By providing this context, the language model can better understand the task at hand.

_Data Exemplars._ Furthermore, we recommend including a small number of labeled data examples in the prompt. This can help language models better comprehend the specific problem. Examples act as concrete illustrations of the task, offering a clearer understanding of the expected output. This can be particularly beneficial when dealing with a complex problem.

_Keywords._ Next, labeling logic in programs can make use of keyword-searching techniques (e.g., Fig 1). For instance, in situations such as spam detection or topic classification, certain words or phrases may have a strong correlation with specific classifications. Providing several keywords in the prompt may lead models to create labeling programs that explicitly search for the presence or absence of these keywords. This allows for more targeted and precise labeling.

_Specialized Labeling Rules._ Finally, more prior knowledge such as heuristics, specialized labeling rules, guidance, and domain-specific knowledge can be integrated into the prompt. This information can provide concrete labeling steps on how to label specific classes and offer greater control over the logic implemented in the generated programs.

Figure 3: Alchemist can handle rich modalities through a simple extension. First, a language model identifies task-specific concepts (top). Then, a local multimodal model is used as a feature extractor for these concepts, producing low-dimensional feature vectors that can be ingested by generated labeling programs.

Overall, supplementary context is provided before the task description to enhance language models' understanding of the task. This, in turn, enables models to generate programs that are more effective and tailored to the specific requirements of user needs.

### Dataset Synthesis

While generated programs can efficiently annotate data, these programs may produce outputs that are noisy or inaccurate. However, as such programs may employ different techniques, such as pattern-matching, heuristic rules, or other approaches--each with its own strengths and limitations--there may be _complementary_ signal in their outputs. This means we can aggregate them to mitigate the impact of noise. To do so, we apply weak supervision techniques [18; 19; 20; 21]. This process starts by learning a model of the reliabilities of the programs. Once learned, this model enables aggregating label outputs from different programs into high-quality _pseudolabels_.

Alchemist is compatible with a variety of weak supervision aggregation models, called _label models_, providing flexibility in the choice of the weak supervision approach. For simplicity, in this work, we focus on using the Snorkel framework , which is a standard and widely-used approach in the weak supervision community.

### Extensions: Handling Complex Modalities.

Crafting programs that operate over text is relatively easy for large language models. More complex data modalities, however, can be far more challenging. Consider images as an illustrative example. Even employing state-of-the-art multimodal models, e.g., GPT-4o  and GPT-4V , to seek programs operating over sample images may not produce satisfactory results.

To address this challenge, we extend Alchemist's pipeline to include an intermediate step. Specifically, we convert the raw data (i.e., in our example, image pixels) into a set of features representing high-level concepts. These concepts are obtained by prompting a language model (or, potentially, a multimodal model) to identify task-relevant notions. For example, for a bird categorization task, models may identify "wing shape," "beak shape," or "foot type" as informative concepts for distinguishing between bird species. Next, we use any open-source local multimodal model, like CLIP , as a feature extractor for the identified concepts, producing low-dimensional feature vectors that can be easily ingested by generated programs. As such models are free, this does not increase our cost.

Fig. 3 and Fig. 4 present examples of generated high-level concepts and the corresponding programs used for the Waterbirds dataset, where the task is to distinguish between landbird and waterbird species . This simple approach can be applied to any data modality where we have access to a local multimodal model (i.e., a model operating on the modality of interest and text).

Figure 4: Program examples generated by GPT4o on Waterbirds dataset. The left program is synthesized by directly asking for a labeling program when the input is an image (raw pixels), while the right program uses Alchemist’s extension. The former labels birds using the dominant color in the image, which can be predicted incorrectly due to spurious correlations (e.g., background).

## 4 Experiments

We study the capability of Alchemist empirically. Our goals are to validate the following claims:

* **Cost Reduction and Improved Performance (Sec. 4.1):** Alchemist can reduce cost by orders of magnitude, while producing labels of similar or better accuracy.
* **Extendibility to Other Modalities (Sec. 4.2):** Alchemist can operate with modalities beyond text.
* **Use of Supplementary Information (Sec. 4.3):** Incorporating relevant information into prompts enables the generation of better programs, yielding more accurate pseudolabels.
* **More Diverse Programs Can Help (Sec. 4.4):** Increasing the diversity of generated programs created by different labeling logic enables better pseudo labels.
* **Comparing to Human-crafted Programs (Sec 4.5):** Synthesized programs may be more effective in comparison to human-crafted ones.

**Datasets.** We include diverse datasets covering text and image modalities. For text, we include eight datasets that span three different types of language tasks. These include the YouTube , SMS  datasets for spam classification, IMDb , Yelp , Finance , and French  datasets for sentiment analysis, and the MedAbs  and Cancer  datasets for topic classification. We note that the Finance, French, MedAbs, and Cancer datasets are relatively challenging, with points that require a degree of domain expertise for accurate labeling. For example, the French dataset requires a good understanding of the language. These may pose challenges for pretrained models.

For our extensions to richer modalities, we focus on image tasks. Our evaluation uses the Waterbirds dataset . This dataset is designed to assess models' robustness to spurious correlations and ability to handle distribution shifts. More details are in Appendix A.

### Cost Reduction and Improved Performance

**Setup.** We open our evaluation of Alchemist with text domain datasets and use GPT-3.5 to generate programs. For each dataset, we input pure prompts without supplementary information into GPT-3.5 and generate 10 programs to use. We construct training datasets by aggregating the programs' outputs into pseudolabels with the weak supervision framework Snorkel . We then train a two-layer MLP as a distilled model. We run five times with different random seeds and report their average performance. As our main baseline, we directly use language models to produce annotations per point. The resulting labels are used to train a distilled model for comparison. The prompt template used in our baseline approach and our training settings are provided in Appendix A.

**Expected Results.** We anticipate that Alchemist can generate programs that can produce accurate labels while substantially reducing the expense of API calls.

**Results.** Table 1 presents the distilled model's performance on each testing dataset. We observe that label accuracy is improved on five out of eight datasets, particularly in challenging settings such as the MedAbs, Cancer, and French datasets, outperforming the baseline zero-shot prompting approach. We also report the estimated costs of building training datasets. The costs for zero-shot prompting depend on the number of tokens for the dataset. In contrast, Alchemist only prompts 10 programs for

    &  &  &  &  \\   & Est. Cost & Accuracy & Est. Cost & F1-score & Est. Cost & Accuracy & Est. Cost & Accuracy \\  Zero-shot Prompting & 0.096 & 0.871 & 0.240 & **0.907** & 3.873 & **0.845** & 3.400 & **0.737** \\ Alchemist with GPT-3.5 & 0.004 & **0.891** & 0.004 & 0.900 & 0.005 & 0.575 & 0.004 & 0.662 \\   &  &  &  \\   & Est. Cost & Accuracy & Est. Cost & Accuracy & Est. Cost & Accuracy & Est. Cost & Accuracy \\  Zero-shot Prompting & 1.944 & 0.311 & 15.925 & 0.716 & 0.201 & 0.641 & 0.641 & 0.611 \\ Alchemist with GPT-3.5 & 0.006 & **0.346** & 0.003 & **0.968** & 0.007 & **0.660** & 0.006 & **0.690** \\   

Table 1: Testing performance of the distilled model is reported for each combination of method and dataset. The estimated cost is obtained by calculating the number of input and output tokens associated with GPT-3.5’s pricing table . Other models may be even more expensive.

each task, resulting in a significant reduction in the costs--by orders of magnitude. This efficiency is the main advantage of Alchemist, _as it allows for the creation of high-quality datasets with minimal expense._ We include ablation studies with other weak supervision models within the Alchemist framework in Appendix C. They successfully demonstrate the flexibility and robustness of using Alchemist.

### Extending Alchemist to Other Modalities

**Setup.** Next, we validate the extension of Alchemist to richer modalities. We consider our approach, where we prompt a multimodal model such as GPT4o and Claude 3, to generate high-level task-specific concepts. We extract features for these concepts by employing CLIP as our local feature extractor. This converts raw pixels into feature vectors for the extracted high-level concepts, producing a set of similarity scores. Armed with these scores, we describe scores associated with their concepts in prompts and ask GPT4o and Claude 3 for 10 programs. As before, we use Snorkel as our aggregation procedure.

_Baselines._ We study two baselines. The first is the vanilla version of Alchemist, where we directly ask GPT4o and Claude 3 to produce code that can operate on images (see left program in Fig. 4). The second is simple zero-shot prompting using CLIP, along with a variant, a group prompting approach that assumes access to spurious information and adds it to the given prompt2.

**Expected Results.** We expect employing our two-step process can enable tractable program generation. In addition, we hypothesize that programs generated in this way are beneficial in targeting salient concepts and reducing the impact of irrelevant or shortcut features, thereby enhancing robustness.

**Results.** We present results in Table 2. Our evaluation focuses on three key metrics: average accuracy, worst group accuracy, and the gap between these two measures. Ideally, a robust model should achieve high average accuracy and high worst group accuracy while minimizing the disparity between the two. First, we see that directly asking programs to use may have very low performance (GPT4o) or may hugely suffer from spurious correlations, destroying worst group performance (Claude 3, CLIP zero-shot). Our method addresses both cases. Compared to baseline methods, Alchemist demonstrates increased worst group accuracy and a reduced gap between the average and worst group accuracies. This is a key strength of Alchemist: _targeting salient concepts to be used as features may help move models away from spurious shortcuts found in the data_. This validates Alchemist's ability to handle complex modalities while improving robustness.

   Feature Extractor & Method & Average Accuracy (\(\)) & Worst Group Accuracy (\(\)) & Gap (\(\)) \\  — & **Vanilla Alchemist with GPT4o** & 0.395 & 0.367 & 0.028 \\  & **Vanilla Alchemist with Claude 3** & 0.781 & 0.022 & 0.759 \\  CLIP ViT-B/32 & Zero-shot Prompting & 0.820 & 0.318 & 0.502 \\  & Group Prompting & **0.823** & 0.383 & 0.440 \\   & **Alchemist with GPT4o** & 0.805 & 0.283 & 0.522 \\  & **Alchemist with Claude 3** & 0.774 & **0.463** & **0.410** \\  CLIP ViT-L/14 & Zero-shot Prompting & **0.904** & 0.335 & 0.569 \\  & Group Prompting & 0.791 & 0.240 & 0.551 \\   & **Alchemist with GPT4o** & 0.802 & **0.467** & **0.335** \\  & **Alchemist with Claude 3** & 0.737 & 0.346 & 0.391 \\   

Table 2: Alchemist on non-text modalities. We experiment with standard Alchemist (top), our proposed extension with two CLIP-based local models as feature extractors, and CLIP prompting baselines. Alchemist achieves comparable performance on average accuracy while improving robustness to spurious correlations.

### Use of Supplementary Information

**Setup.** We test how integrating relevant information into the prompt context can augment generated programs. Instead of manually crafting supplementary information, we harness the power of language models to generate and integrate. This approach is useful for challenging datasets where users may not have the necessary knowledge or expertise to start. We evaluate the effectiveness of this approach, by comparing label model performance using programs generated by two different methods: pure prompting and in-context prompting. In-context prompting involves supplementary information, while pure prompting relies solely on the task description without any additional guidance. We employ GPT-3.5, GPT-4 and Claude 3 as our program sources and synthesize ten for each strategy.

**Expected Results.** We hypothesize that providing supplementary information can enhance task understanding, demonstrate specific labeling logic, and offer concrete steps, ultimately leading to better programs for use.

**Results.** Table 3 presents this comparative analysis on label model performance using different type of information. We observe that by incorporating supplementary information into pure prompts, Alchemist can guide language models to generate more effective programs, which in turn produce more accurate pseudolabels. Improvements are particularly evident in the challenging datasets such as Finance and French. Moreover, this approach can be combined with RAG systems to include external knowledge bases and customize the relevant information. Such flexibility compared to zero-shot prompting is another key strength of Alchemist, as _programs can easily be adapted, augmented, and specialized._

### More Diverse Programs Can Help

**Setup.** As shown in Table 3, incorporating different supplementary information results in varying degrees of additional improvement. Potentially, certain sets of supplementary information allow the

    &  &  &  &  \\   & GPT-3.5 & GPT-4 & Claude 3 & GPT-3.5 & GPT-4 & Claude 3 & GPT-3.5 & GPT-4 & Claude 3 & GPT-3.5 & GPT-4 & Claude 3 \\  General Prompt & 0.92 & 0.92 & 0.66 & 0.64 & 0.62 & 0.75 & 0.65 & 0.82 & 0.78 & 0.71 & 0.77 & 0.77 \\  \(+\) Dataset Description & 0.64 & **0.93** & **0.71** & 0.63 & **0.63** & **0.76** & **0.72** & 0.82 & **0.79** & 0.70 & **0.79** & 0.73 \\ \(+\) 5 Data Examples & 0.91 & 0.86 & **0.76** & 0.46 & **0.66** & 0.62 & **0.72** & 0.82 & **0.82** & 0.68 & 0.75 & 0.73 \\ \(+\) Keywords & 0.76 & **0.93** & 0.53 & 0.40 & 0.42 & 0.64 & **0.69** & 0.81 & 0.78 & 0.69 & **0.78** & 0.72 \\ \(+\) Labeling Rules & 0.74 & 0.82 & 0.56 & **0.67** & **0.67** & 0.58 & **0.75** & 0.81 & **0.79** & 0.71 & 0.77 & 0.74 \\    &  &  &  \\   & GPT-3.5 & GPT-4 & Claude 3 & GPT-3.5 & GPT-4 & Claude 3 & GPT-3.5 & GPT-4 & Claude 3 & GPT-3.5 & GPT-4 & Claude 3 \\  General Prompt & 0.52 & 0.53 & 0.55 & 0.71 & 0.73 & 0.59 & 0.66 & 0.49 & 0.56 & 0.65 & 0.55 & 0.56 \\  \(+\) Dataset Description & 0.49 & 0.50 & 0.51 & 0.59 & 0.62 & **0.60** & 0.61 & **0.63** & **0.62** & 0.39 & **0.58** & **0.67** \\ \(+\) 5 Data Examples & **0.53** & **0.54** & 0.55 & 0.55 & 0.57 & **0.63** & 0.60 & **0.50** & **0.60** & 0.40 & **0.69** & 0.44 \\ \(+\) Keywords & **0.55** & **0.55** & 0.55 & 0.55 & 0.46 & 0.66 & **0.62** & **0.65** & **0.69** & **0.66** & **0.67** \\ \(+\) Labeling Rules & 0.52 & **0.55** & **0.56** & 0.61 & 0.59 & **0.63** & 0.66 & **0.56** & **0.67** & 0.65 & **0.66** & 0.33 \\   

Table 3: Testing performance of the label model is reported for each combination of prompting strategy and dataset. We observe that GPT-4 and Claude 3 (that may possess better comprehension capabilities) exhibit greater enhancements when provided with supplementary information.

Figure 5: Performance is reported using their average performance and standard deviations. Results indicate that the label model is improved when the number of diverse programs increases.

model to specialize better on certain data points than others. We seek to achieve these performance improvements _without_ the need to re-prompt the model with each set of supplementary information. Instead, we collect previously generated programs to obtain a set of programs with greater diversity. We ask: _can Alchemist achieve better performance by modeling more diverse programs?_

We randomly select a set of programs from each category, collect them, and train the label model with their program outputs. Additionally, we increase the number of sampled programs in each category from 4 to 9. We test this approach on the datasets where Alchemist gives comparable or lower performance than zero-shot prompting in our initial experiments in Table 1, namely the SMS, Yelp, and IMDb datasets.

**Expected Results.** By obtaining more diverse programs to use, Alchemist can capture a wider range of perspectives and labeling logic, potentially leading to more accurate pseudolabels.

**Results.** Fig. 5 visualizes the effect on the label model's performance when we increase the diversity in collected programs. It demonstrates a trend and indicates that involving a more diverse set of programs can help to mitigate the impact of individual strategy biases or limitations, leading to the production of better labels.

Overall, results in Sec. 4.3 and in Sec. 4.4 underscore that _the use of supplementary information and involving diverse types of programs can help achieve better performance._

### Comparing to Human-crafted Programs

**Setup.** Lastly, we compare synthesized programs in Alchemist and manually crafted labeling functions in WRENCH , which is a widely-used benchmark for evaluating weak supervision methods. We focus on the datasets that overlap between Alchemist and WRENCH. For each dataset, we use pure prompts to query GPT-3.5, GPT-4, and Claude 3 for 10 programs. We then evaluate the performance of the distilled model for both methods. We also include the label model's coverage in our comparison. Higher coverage means that label model can produce more pseudolabels, yielding a larger size of training dataset to use.

**Expected Results.** We expect that synthesized programs may offer some advantages in terms of efficiency and effectiveness compared to human-designed ones.

**Results.** Table 4 presents their comparison. By leveraging the knowledge and capabilities of language models, we find that generated programs offer several advantages, including better coverage (i.e., the ability to label more data points) and comparable, or even better, performance. Generated programs can reduce the need for laborious engineering, which can be time-consuming and often requires a tedious design process to fine-tune labeling logic, such as thresholds and keyword usage. This design process may lead to many undiscovered rules, resulting in lower performance on coverage and potentially limiting the effectiveness of the labeling functions--unlike synthesized programs.

This is particularly evident in the SMS dataset, where WRENCH requires 73 manually crafted labeling functions to obtain high-quality labels, while Alchemist only needs 10 generated programs to obtain comparable performance and higher coverage. This significant reduction highlights the potential of Alchemist to _assist humans in designing labeling functions and make it more accessible to users without extensive domain expertise._

    &  &  &  \\  & Human &  &  &  &  &  &  &  \\  & &  &  &  &  &  &  &  &  &  &  \\  & &  &  &  &  &  &  &  &  &  &  &  &  &  \\  & &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\  Sum. of Programs & 10 & 10 & 10 & 10 & 73 & 10 & 10 & 10 & 10 & 8 & 10 & 10 & 10 & 8 & 10 & 10 & 5 & 10 & 10 \\ Coverage & 0.89 & 1.09 & 1.09 & 1.09 & 1.10 & 0.41 & 1.06 & 1.00 & 1.00 & 0.83 & 0.78 & 0.99 & 0.80 & 0.88 & 0.89 & 1.00 & 0.98 \\ Performance & 0.85 & **0.80** & **0.89** & 0.72 & 0.89 & **0.60** & **0.83** & 0.99 & 0.76 & 0.87 & **0.82** & **0.83** & 0.73 & 0.66 & **0.75** & 0.70 \\   

Table 4: Analysis showing that Alchemist can achieve comparable or better accuracy and higher coverage while using fewer programs to label the data.

Conclusion

We propose an alternative approach to costly annotation procedures that require repeated API requests for labels. Our solution introduces a simple notion of prompting programs to serve as annotators. We developed an automated labeling system called Alchemist to embody this idea. Empirically, our results indicate that Alchemist demonstrates comparable or even superior performance compared to language model-based annotation, improving five out of eight datasets with an average enhancement of 12.9%. Notably, Alchemist reduces total costs by a factor of approximately 500. Furthermore, we showcase the system's extensibility to handle more complex modalities while enhancing the robustness of predicted labels. Finally, we confirm that incorporating relevant information can generate better programs, and increasing diversity leads to obtaining higher-quality labels.

## 6 Acknowledgments

We are grateful for the support of the NSF under CCF2106707 (Program Synthesis for Weak Supervision) and the Wisconsin Alumni Research Foundation (WARF). We thank Dyah Adila, Albert Gu, Harit Vishwakarma, and Nicholas Roberts, for their helpful feedback and valuable discussion.