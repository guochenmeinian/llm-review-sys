# Beyond Efficiency: Molecular Data Pruning for Enhanced Generalization

Dingshuo Chen\({}^{1,2}\) Zhixun Li\({}^{3}\) Yuyan Ni\({}^{4}\) Guibin Zhang\({}^{5}\) Ding Wang\({}^{1,2}\)

Qiang Liu\({}^{1,2}\) Shu Wu\({}^{1,2}\) Jeffrey Xu Yu\({}^{3}\) Liang Wang\({}^{1,2}\)

\({}^{1}\)New Laboratory of Pattern Recognition

State Key Laboratory of Multimodal Artificial Intelligence Systems

Institute of Automation, Chinese Academy of Sciences

\({}^{2}\) School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{3}\)Department of Systems Engineering and Engineering Management

The Chinese University of Hong Kong

\({}^{4}\)Academy of Mathematics and Systems Science, Chinese Academy of Sciences

\({}^{5}\)Tongji University

\({}^{}\) Primary contact: dingshuo.chen@cripac.ia.ac.cn

Corresponding author: Shu Wu (shu.wu@nlpr.ia.ac.cn).

###### Abstract

With the emergence of various molecular tasks and massive datasets, how to perform efficient training has become an urgent yet under-explored issue in the area. Data pruning (DP), as an off-stated approach to saving training burdens, filters out less influential samples to form a coreset for training. However, the increasing reliance on pretrained models for molecular tasks renders traditional in-domain DP methods incompatible. Therefore, we propose a _Molecular_ data Pruning framework for enhanced Generalization (MolPeg), which focuses on the _source-free data pruning_ scenario, where data pruning is applied with pretrained models. By maintaining two models with different updating paces during training, we introduce a novel scoring function to measure the informativeness of samples based on the loss discrepancy. As a plug-and-play framework, MolPeg realizes the perception of both source and target domain and consistently outperforms existing DP methods across four downstream tasks. Remarkably, it can surpass the performance obtained from full-dataset training, even when pruning up to 60-70% of the data on HIV and PCBA dataset. Our work suggests that the discovery of effective data-pruning metrics could provide a viable path to both **enhanced efficiency** and **superior generalization** in transfer learning.

## 1 Introduction

The research enthusiasm for developing molecular foundation models is steadily increasing , attributed to its foreseeable performance gains with ever-larger model and amounts of data, as observed neural scaling laws  and emergence ability  in other domains. However, the computational and storage burdens are daunting in model training , hyperparameter tuning, and model architecture search . It is therefore urgent to ask for training-efficient molecular learning in the community.

Data pruning (DP), in a natural and simple manner, involves the selection of the most influential samples from the entire training dataset to form a coreset as _paragons_ for model training. The primary goal is to alleviate training costs by striking a balance point between efficiency and performance compromise. A trend in this field is developing data influence functions , training dynamic metrics , and coreset selection  for lossless - although typically compromised - modelgeneralization. When it comes to molecular tasks, transfer learning, particularly the _pretrain-finetune_ paradigm, has been regarded as the de-facto standard for enhanced training stability and superior performance . However, existing DP methods are purposed for train-from-scratch setting, i.e., the model is randomly initialized and trained on the selected coreset. A natural question arises as to _whether or not current DP methods remain effective when applied with pre-trained models_. Experimental analysis, as illustrated in Figure 1 (Left), suggests a negative answer. Most existing pruning strategies exhibit inferior results relative to the performance achieved with the full dataset, even falling short of simple random pruning.

In contrast to the existing DP approaches, which focus solely on a single target domain, the incorporation of pretrained model introduces an additional source domain, thereby inevitably exposing us to the challenge of distribution shift . Unfortunately, this is especially severe in molecular tasks, owing to the limited diversity of large-scale pretraining datasets compared to the varied nature of downstream tasks. As illustrated in Figure 1 (Right), we investigate the distribution patterns of several important molecular properties across the upstream and downstream datasets following Beaini et al. . The observed disparities impede the model generalization, thus making DP with pretrained models a highly non-trivial task. We define this out-of-domain DP setting as _source-free data pruning_. It entails removing data from downstream tasks leveraging pre-trained models while remaining agnostic to the specifics of the pre-training data.

Of particular relevance to this work are approaches that propose DP methods for transfer learning , which also target cross-domain scenarios. Despite the promising results they achieved, these methods select pretraining samples based on downstream data distribution, which necessitates reevaluation of previously selected samples and retraining heavy models as new samples involving, undermining the goal of achieving generalization and universality in pretraining. To this end, we take a step towards designing a DP method under the source-free data pruning setting to achieve **efficient and effective** model training, which aligns better with practical deployment for molecular tasks.

In this work, we propose a Molecular data Pruning framework for enhanced Generalization, which we term MolPeg for brevity. The core idea of MolPeg is to achieve cross-domain perception via maintaining an online model and a reference model during training, which places emphasis on the target and source domain, respectively. Besides, we design a novel scoring function to simultaneously select easy (representative) and hard (challenging) samples by comparing the absolute discrepancy between model losses. We further take a deep dive into the theoretical understanding and glean insight on its connection with the previous DP strategies. Note that our proposed MolPeg framework is generic, allowing for seamless integration of off-the-shelf pretrained models and architectures. To the best of our knowledge, this is the first work that studies how to perform data pruning for molecular learning from a transfer learning perspective. Our contributions can be summarized as follows:

* We analyze the challenges of efficient training in the molecular domain and formulate a tailored DP problem for transfer learning, which better aligns with the practical requirements of molecular pre-trained models.

Figure 1: **(Left)** The performance comparison of different data pruning methods in HIV dataset under source-free data pruning setting. **(Right)** Distribution patterns of four important molecular features - molecular weight (MW), topological polar surface area (TPSA), Quantitative Estimate of Drug-likeness (QED) and number of bonds - in PCQM4MV2  and HIV  dataset, which are used for pretraining and finetuning, respectively.

* We propose an efficient data pruning framework that can perceive both the source and target domains. It can achieve lightweight and effective DP without the need for retraining, facilitating easy adaptation to varied downstream tasks. We also provide a theoretical understanding of MolPeg and build its connections with existing DP strategies.
* We conduct extensive experiments on 4 downstream tasks, spanning different modalities, pretraining strategies, and task settings. Our method can surpass the full-dataset performance when up to 60%-70% of the data is pruned, which validates the effectiveness of our approach and unlocks a door to enhancing model generalization with fewer samples.

## 2 Preliminaries

In this section, we take a detour to revisit the traditional data pruning setting and _pretrain-finetune_ paradigm before introducing the problem formulation of source-free data pruning.

**Problem statement of traditional data pruning**. Consider a learning scenario where we have a large training set denoted as \(=\{(_{i},y_{i})\}_{i=1}^{[]}\), consisting of input-output pairs \((_{i},y_{i})\), where \(_{i}\) represents the input and \(y_{i}\) denotes the ground-truth label corresponding to \(_{i}\). Here, \(\) and \(\) refer to the input and output spaces, respectively. The objective of traditional data pruning is to identify a subset \(}\), that captures the most informative instances. The model trained on this subset \(}\) should yield a slightly inferior or comparative performance to the model trained on the entire training set \(\). Thus they need to strike a balance between efficiency and performance.

**Revisit on transfer learning**. Given source and target domain datasets \(_{}\) and \(_{}\), the goal of pretraining is to obtain a high-quality feature extractor \(f\) in a supervised or unsupervised manner. While in the finetuning phase, we aim to adapt the pretrained \(f\) in conjunction with output head \(g\) to the target dataset \(_{}\).

Considering the proficiency of molecular pre-trained models in capturing meaningful chemical spaces, their widespread usage in enhancing performance across diverse molecular tasks has become commonplace. This necessitates a reassessment of the conventional approach to DP within the molecular domain and, more broadly, within the field of transfer learning. Previous attempts [31; 32] in data pruning for transfer learning primarily focus on trimming upstream data, selecting samples that closely match the distribution of downstream tasks to align domain knowledge. However, this necessitates retraining the model from scratch, which is notably ill-suited for the molecular domain, where the continual influx of new molecules introduces novel functionalities and structures. To this end, we propose a tailored DP problem for molecular transfer learning:

**Problem formulation** (Source-free data pruning). _Given a target domain dataset \(_{}\) and a pretrained feature extractor parameterized by \(_{}\), we aim to identify a subset \(}_{}_{}\) for training, while being agnostic of the source domain dataset \(_{}\), to maximize the model generalization._

Figure 2: The overall framework of MolPeg. **(Left)** We maintain an online model and a reference model with different updating paces, which focus on the target and source domain, respectively. After model inference, the samples are scored by the absolute loss discrepancy and selected in ascending order. The easiest and hardest samples are given the largest score and selected to form the coreset. **(Right)** The selection process of MolPeg can be interpreted from a gradient projection perspective. Samples with low projection norms (grey) are discarded, while those with high norms are kept.

Methodology

As with generic data pruning pipelines, the MolPeg framework is divided into two stages, scoring and selection. In the first stage, we define a scoring function to measure the informativeness of samples and apply it to the training set. In the subsequent stage, given the sample scores, we rank them in ascending order and maintain the high-ranking samples for training. Note that our pruning method is dynamically performed during the training process, rather than conducted before training.

We next introduce the MolPeg framework in detail. We track the training dynamics of two models with different update poses. For each training sample, we measure the difference in loss between the two models to quantify its importance, and then make the final selection based on this metric. In the following parts, we first intuitively introduce our design of the scoring function. Then, we further explore the theoretical support behind the effectiveness of the MolPeg. The overall framework is illustrated in Figure 2.

### The MolPeg framework

The design of the scoring function addresses two key issues, (1) how to achieve the perception of source and target domain and (2) how to measure the informativeness of the samples.

**Cross-domain perception.** Since we are unable to access the upstream dataset, the pre-trained model serves as the only entry point of the source domain. During the finetuning stage, apart from _online encoder_ undergoing gradient optimization via back-propagation, we further maintain a _reference encoder_ updated with exponential moving average (EMA) to perceive the cross-domain knowledge. Note that both encoders are initialized by pretrained model \(_{0}=_{0}=^{S}\), where \(_{t}\) and \(_{t}\) denotes the parameters of online and reference model at batch step \(t\), respectively. They are updated as follows:

\[_{t+1}=_{t}-_{}(}_{ t},_{t})_{t}=_{t}+(1-)_{t-1}\] (1)

where \(\) is the learning rate and \([0,1)\) is the pace coefficient that controls the degree of history preservation. Here \(}_{t}\) is the selected finetuning dataset for epoch \(t\), and \(_{}(}_{t},_{t})\) denotes the average gradient \(}_{t}|}_{}_{t}} _{}(x_{i},_{t})\) for short. Intuitively, We control the influence of target domain on the reference encoder via EMA. With a small update pace \(\), the online encoder prioritizes target domain, while the reference encoder emphasizes source domain.

**Informativeness measurement and selection.** By far we explicitly represent the inaccessible source domain knowledge with the help of the reference model, facilitating us to further quantify the informativeness of each sample in the cross-domain context. Our motivation for measuring the sample informativeness comes from a recent work that improves the neural scaling laws . They suggest that the best pruning strategy depends on the amount of initial data. When the data volume is large, retaining the hardest samples yields better pruning results than retaining the easiest ones; the conclusion is the opposite when the data volume is small. This contrasts with the conclusion that only the hardest samples should be selected . From an intuitive perspective, simple samples are more representative, allowing the model to adapt to downstream tasks more quickly, while hard samples are crucial for model generalization since they are considered _supporting vectors_ near the decision boundaries. This debate highlights that in data pruning, how to perform a mixture of easy and hard samples is a critical factor. As shown in Figure 3, when 60% samples in the HIV dataset are pruned, simply selecting the easiest or hardest samples leads to a performance drop in later epochs.

Therefore, we opt to retain both easy and hard samples instead of singularly removing one type. To measure the information gap between domains, we adopt both _online_ and _reference encoder_ to infer each sample and calculate the absolute loss discrepancy between them:

\[}_{t}=\{_{t}\|(,_ {t})-(,_{t})\|\},\] (2)

Figure 3: Performance comparison of selection criteria on HIV dataset when pruning 40% samples.

where \(_{t}^{T}\) comprises the target domain data sampled for batch step \(t\) and \(}_{t}_{t}\) comprises the data selected by MolPeg. \(\) is not a constant, but a threshold determined by the pruning ratio. Specifically, the rank of \(\) in the absolute loss discrepancy sequence \(\{|(_{i},_{t})-(_{i},_{t})|\}_{i=1}^ {|_{t}|}\) is \(|}_{t}|\), i.e. pruning ratio\(|_{t}|\). It is easy to infer that a positive loss discrepancy, i.e. \((,_{t})-(,_{t})>0\), indicates the model struggles to accurately distinguish the sample, identifying it as hard one. Conversely, a negative loss discrepancy indicates that the model can easily improve its accuracy, marking it as an easy sample. Therefore, intuitively, we dynamically assess the learning difficulty of samples during the training process. By measuring the absolute value of the loss discrepancy, we keep the simplest (most representative) and the hardest (most challenging) samples, which are integrated as the most informative ones (Orange line in Figure 3). We also provide the pseudo-code of MolPeg in Algorithm 1.

```
1Inputs: \(=\{(_{i},y_{i},s_{i})\}_{i=1}^{||}\): dataset with the score for each example (\(s_{i}=1, s_{i}\)); \(\): learning rate; \(\): EMA update pace; \(p\): data pruning ratio (\(p<1\)); \(T\): total number of training epochs; \(f_{}\): pretrained encoder parameterized by \(\)
2\(t 0\);
3while\(t T\)do
4\(K p||\) ; /* Get the number of remaining samples */
5\(}_{t}(s)\) ; /* Rank and Select the top-K samples for training */
6\(s_{i}\|(f_{}(_{i}),y_{i})-(f_{} (_{i}),y_{i})\|,(_{i},y_{i},s_{i})}_{t}\) ; /* Scoring the samples */
7\(-_{}(}_{t}, )\) ; /* Gradient update for online model */
8\(+(1-)\) ; /* EMA update for reference model */
9\(t t+1\)
10return ```

**Algorithm 1**Molecular Data Pruning for Enhanced Generalization (MolPeg)

### Theoretical Understanding

In this section, we explore the theoretical underpinnings of the data selection process in MolPeg. Recall that our scoring function is defined by loss discrepancy, we further make use of Taylor expansion on the designed scoring function. Then, from the gradient perspective, i.e., the first-order expansion term, we derived the following propositions and the complete proof is provided in the Appendix E.

**Assumption 1** (Slow parameter updating).: _Assume the learning rate is small enough, so that the parameter update \(_{t}=_{t+1}-_{t}\) is small for every time step, i.e. \(\|_{t}\|\), \( t\), \(\) is a small constant._

**Proposition 1** (Interpretation of loss discrepancy).: _With Assumption 1, the loss discrepancy can be approximately expressed by the dot product between the data gradient and the "EMA gradient":_

\[(,_{t})-(,_{t})=_{ }(,_{t})_{t}^{EMA}+(^{ 2}),\] (3)

_where \(_{t}^{EMA}\) denotes \(_{j=1}^{i}(1-)^{j}_{}(}_{t-j},_{t-j})\), i.e. the weighted sum of the historical gradients, which we termed as "EMA gradient"._

It indicates that the scoring function is essentially influenced by the magnitude of the dot product between the data gradient and the EMA gradient, as illustrated in Figure 2 (right). Given the EMA gradient, the size of the dot product is influenced by two factors: the norm of \(_{}(,_{t})\) and the angle between the two vectors. _(i)_ A larger norm of the current data gradient is more likely to be selected, which resembles the criteria of GraNd score . More connections to several well-known scoring functions are provided in the appendix F. (ii) If the current gradient direction closely aligns with the (opposite) EMA gradient direction, it often indicates an easy (hard) optimization of the sample, corresponding to the goal of selecting simple and hard samples in the previous analysis. Conversely, samples with gradient directions orthogonal to the EMA gradient are discarded.

In the following proposition, we examine the gradient of the selected samples and analyze simple and hard samples separately. Since the selection is performed at each fixed batch time step, we focus on one step of selection and omit the common time subscript \(t\). Note that this result involves certain simplifications and approximations, and a formal version is provided in the appendix.

**Proposition 2** (Gradient projection interpretation of MolPeg, informal).: _Let \(^{+}\) and \(}^{+}}\) denote the sets of samples for which the dot products between the data gradients and the "EMA gradient" are positive. Then, the gradient of the selected "simple" samples can be expressed as:_

\[_{}(}^{+},)=_{} (^{+},)+av^{EMA},a 0.\] (4)

_Similarly, we define \(^{-}\) and \(}^{-}}\) as samples that have negative dot products, then_

\[_{}(}^{-},)=_{} (^{-},)+bv^{EMA},b 0.\] (5)

\(a=0\) _and \(b=0\) holds if and only if the loss discrepancy across \(^{+}\) and \(^{-}\) is uniform respectively, which are uncommon scenarios._

Therefore, our data selection strategy essentially increases the weight of the (opposite) EMA gradient direction in the data gradient for easy (hard) samples. When \(^{+}\) predominates, indicating a majority of simple samples in the dataset, this simplified model is akin to the momentum optimization strategy, which utilizes the sum of the current data gradient and the weighted EMA gradient to update the model parameters. This suggests that retaining simple samples may enhance optimization stability, allowing the model to overcome saddle points and local minima . However, our method differs from the momentum optimization strategy in two key aspects. Firstly, we preserve directions opposite to the EMA gradient to target hard and forgettable samples. Secondly, our EMA gradient, which records the gradient of the coreset rather than the entire set, can retain more historical information under the same update pace.

## 4 Experimental Settings

### Datasets and tasks

To comprehensively validate the effectiveness of our proposed MolPeg, we conduct experiments on three datasets, i.e., HIV, PCBA, MUV and QM9, covering four types of molecular tasks. These tasks span two molecular modalities--2D graph and 3D geometry--as well as two types of supervised tasks, i.e., classification and regression.

Given the potential issues of over-fitting and spurious correlations that may arise with limited samples when a large pruning ratio is adopted, we focus on relatively large-scale datasets containing at least 40K molecules. Below, we briefly summarize the information of the datasets. For a more detailed description and statistics of the dataset, please refer to Appendix A.

### Implementation details

In this section, we provide a succinct overview of the implementation details for our experiments, including backbone models for different modalities, training details and evaluation protocols.

**Backbone models.** Given the two modalities involved in our experiment, we need corresponding backbone models for data modeling. Below is a concise introduction to the backbone models. For a more comprehensive understanding of the model architecture, please refer to the Appendix D.

* For 2D graphs, we utilize the Graph Isomorphism Network (GIN)  as the encoder. To ensure the generalizability of our research findings, we adopt the commonly recognized experimental settings proposed by Hu et al. , with 300 hidden units in each layer, and a 50% dropout ratio. The number of layers is set to 5.
* For 3D geometries, we employ two widely used backbone models, PaiNN  and SchNet , as the encoders for different datasets. For SchNet, we set the hidden dimension and the number of filters in continuous-filter convolution to 128. The interatomic distances are measured with 50 radial basis functions, and we stack 6 interaction layers. For PaiNN, we adopt the setting with 128 hidden dimensions, 384 filters, 20 radial basis functions, and stack 3 interaction layers.

[MISSING_PAGE_FAIL:7]

Confidence , Entropy , Forgetting , GraNd , EL2N , DeepFool , Craig , Glister , Influence  and DP . Since dynamic pruning remains a niche topic, we identify four methods, to the best of our knowledge, to serve as baselines, i.e., soft random pruning, \(\)-greedy , UCB  and InfoBatch , with MolPeg also falling into this category. Please refer to Appendix C for a more detailed introduction to the baselines.

**Performance comparison.** Empirical results for DP methods are presented in Table 1. Our systematic study suggests the following trends: _(i) Dynamic DP strategies significantly outperform static DP strategies._ Soft random, as a fundamental baseline in dynamic DP, consistently outperforms the baselines of static groups across almost all pruning ratios, even surpassing some strong competitors such as Glister and GraNd. We also observe that the performance advantage of dynamic DP becomes more pronounced when the pruning ratio is relatively large. Intuitively, compared to fixing a subset for training, dynamic pruning can perceive the full dataset during training, thereby possessing a larger receptive field and naturally yielding better performance. As more data samples are retained, the ability of both groups to perceive the full training set converges, leading to smaller performance differences between them. _(ii)__MoI**Peg** achieves the state-of-the-art performance across all proportions._ On the HIV dataset, we can achieve nearly lossless pruning by removing 80% of the samples, surpassing other baseline methods significantly. Similarly, on the larger-scale PCBA dataset, we can still achieve lossless pruning by removing 60% of the data. _(iii)__MoI**Peg** brings superior generalization performance compared to fine-tuning on the full dataset._ For example, on the HIV dataset, we achieve an ROC-AUC performance of 86 when pruning 40% of the data, surpassing the 85.1 achieved with training on the full dataset. This indicates that appropriate data pruning can better aid model generalization given a pre-trained model. However, as more downstream data is introduced, the improvement brought by our method diminishes, as shown by the 20% pruning proportion, due to introducing data samples that hinder model generalization. More empirical results on MUV dataset

**Efficiency comparison**. In addition to performance, time efficiency is another crucial indicator for DP. We conduct a performance-efficiency comparison of various DP methods on the HIV dataset at a 60% pruning ratio, as shown in Figure 4. We define _time efficiency_ as the reciprocal of the runtime multiplied by 1000. A higher value of this metric indicates greater efficiency. We can observe that despite MolPeg experiencing slight efficiency loss compared to random pruning, it demonstrates superior pruning performance. Compared to the current SOTA baseline model, InfoBatch, our method achieves better model generalization with comparable efficiency. Conversely, static pruning methods incur 1.6x to 2.1x greater time costs than random pruning, with model performance stagnating or declining. This underscores that MolPeg achieves superior performance with minimal efficiency costs. Despite increased memory usage introduced by the reference model, EMA is commonly used to stabilize molecular training, which allows our method to utilize EMA-saved models without added memory overhead.

   Dataset &  &  \\  Pruning Ratio \% & 90 & 80 & 70 & 60 & 40 & 20 & 90 & 80 & 70 & 60 & 40 & 20 \\  Random & **85.0** & **45.7** & 34.2 & 30.9 & 19.2 & 15.7 & **4.94** & **3.09** & 2.53 & 2.26 & 1.93 & 1.65 \\ DP & 136.0 & 68.5 & 39.8 & 32.3 & 20.8 & 16.1 & 8.56 & 6.29 & 3.62 & 2.36 & 2.05 & 1.68 \\ InfoBatch & 116.0 & 57.0 & 36.4 & 30.1 & 20.4 & 15.6 & 6.26 & 4.61 & 3.22 & 2.34 & 1.91 & 1.64 \\ MolPeg & 92.4 & 48.2 & **32.4** & **26.1** & **17.7** & **14.3** & 5.40 & 3.18 & **2.51** & **2.24** & **1.86** & **1.62** \\   

Table 2: The performance comparison to state-of-the-art methods on QM9 dataset in terms of MAE (\(\)). We highlight the best- and the second-performing results in **boldface** and **underlined**, respectively.

Figure 4: Performance and efficiency comparison between different DP methods. Pretrained models are fine-tuned on the HIV dataset at a 60% pruning ratio.

### Results on QM9 dataset

Since regression is another common type of downstream molecular task, we also present the empirical results of MolPeg on two properties using the QM9 dataset, alongside comparisons with state-of-the-art methods. To ensure a fair comparison of experimental results, we employ the commonly used 3D geometry modality for modeling. We adopt GeoSSL  as the pretraining strategy and PaiNN as the backbone model, following the settings outlined by Liu et al . Empirical results are presented in Table 2. It can be observed that MolPeg consistently outperforms other DP methods. However, all DP methods unexpectedly demonstrate inferior performance than random pruning in certain pruning ratios (80% and 90%). We speculate this phenomenon is attributed to the PCQM4Mv2 dataset used for pre-training and the QM9 dataset having a close match in the distribution patterns of molecular features. Thus, any non-uniform sampling methods would lead to biased data pruning which exacerbates distribution shift and hinders domain generalization.

### Sensitivity Analysis

We further conduct extensive sensitivity analysis to validate the robustness of MolPeg across different pre-training strategies, molecular modalities, pre-training datasets and hyperparameter choices. All experiments below are conducted on the HIV dataset.

**Robustness evaluation across pretraining strategies.** Given that MolPeg primarily targets scenarios involving pre-trained models, it is necessary to compare its robustness when applied with different pre-training strategies. Without loss of generality, we select two representative pre-training strategies: generative self-supervised learning (SSL) and contrastive self-supervised learning, both of which dominate the field of molecular pre-training. Specifically, in addition to the results based on GraphMAE  (generative SSL) presented in Table 1, we also conduct experiments based on GraphCL (contrastive SSL)  whose results are shown in Figure 5. We can observe that MolPeg achieves optimal performance on both pre-training methods across different pruning ratios. Promisingly, it demonstrates better model generalization than training on the full dataset, indicating insensitivity to pre-training strategies of our proposed framework, thus allowing for convenient plug-in application to other pre-trained models in different molecular tasks.

**Robustness evaluation across modalities.** The selection of molecular modality has long been a contentious issue in the field. To validate the effectiveness of MolPeg across different molecular modalities, we present a comparison of pruning results using 3D geometry in the HIV dataset as shown in Table 3. We pretrain the SchNet  on the PCQM4Mv2 dataset, and keep other settings the same as in Section 4.2. It is evident from the results that the MolPeg framework, consistent with the conclusions drawn in Section 5.1, continues to outperform dynamic random pruning and enhance the model generalization ability. At a 40% pruning ratio, MolPeg also surpasses the performance achieved with training on the full dataset. This demonstrates the robustness of our proposed DP method across molecular modalities.

**Robustness evaluation across pretraining datasets.** In source-free transfer learning, pretrained model is a hard-encoded module, and their variations naturally lead to performance changes. Therefore,

Figure 5: Data pruning trajectory given by downstream performance (%). Here the source models are pretrained on the PCQM4Mv2 dataset with GraphMAE and GraphCL strategies, respectively.

   Pruning Ratio \% & 60 & 40 & 20 \\  Random Pruning & 80.1\(\)1.3 & 80.8\(\)0.6 & 81.2\(\)0.2 \\ MolPeg & 81.9\(\)0.3 & 82.3\(\)0.9 & 82.2\(\)0.8 \\  Whole Dataset &  \\   

Table 3: Performance with 3D modality on HIV dataset.

it is necessary to evaluate the robustness of MolPeg when pretrained with different pre-training datasets. We conduct additional experiments on the HIV dataset using two pretrained models of varying quality, obtained from the ZINC15  and QM9 datasets, respectively. Compared to the PCQM4Mv2 dataset used in the Section 5.1, these two datasets are smaller in scale and exhibit more pronounced distribution shifts, resulting in poorer pretraining quality. We observe the following trends from Table 5: (i) In the case of fine-tuning on the entire dataset, models pretrained on ZINC15 and QM9 show significantly inferior performance, even worse than training from scratch, indicating poor quality of pretrained models. (ii) MolPeg still achieves the best performance with these two pretrained models. This demonstrates the robustness of MolPeg across pretraining datasets.

**How to choose \(\).** Since EMA is a crucial component of our framework, it is necessary to evaluate how to choose a proper \(\). We conduct an empirical analysis on the HIV dataset across three pruning ratios, i.e., [0.1, 0.4, 0.8], and consider a candidate list covering the value ranges of \(\): [0.001, 0.01, 0.1, 0.5, 0.9]. Intuitively, a smaller \(\) implies a slower parameter update pace in the reference model. When \(=0\), it signifies using a frozen pre-trained model as the reference. The experimental results corresponding to the variation of \(\) are illustrated in Figure 6. Empirical results indicate that the overall performance shows only moderate sensitivity to parameter change. However, typically, when \(=0.5\), the model tends to achieve better performance and smaller standard deviation. Hence, for our primary experiments, we opt to default to \(=0.5\).

## 6 Conclusion

In this work, we propose MolPeg, a novel molecular data pruning framework designed to enhance generalization without the need for source domain data, thereby addressing the limitations of existing in-domain data pruning (DP) methods. Our approach leverages two models with different update spaces to measure the informativeness of samples. Through extensive experiments across four downstream tasks involving both classification and regression tasks, we demonstrate that MolPeg not only achieves lossless pruning but also outperforms full dataset training in certain scenarios. This underscores the potential of MolPeg to optimize training efficiency and improve the generalization of pre-trained models in the molecular domain. Our contributions highlight the importance of considering source domain information in DP methods and pave the way for more efficient and scalable training paradigms in molecular machine learning.

Broader impacts.Given that our application tasks fall within the molecular domain, improper use of methods for tasks such as molecular property prediction may result in significant deviations. This could impact subsequent applications of the molecules in drug development or materials design, especially in predicting properties like toxicity and stability. We recommend further experimental validation of key molecules after using the model to ensure the reliability of the results. We provide further discussions in Appendix H.

## 7 Acknowledgements

This work is jointly supported by National Science and Technology Major Project (2023ZD0120901) and National Natural Science Foundation of China (62372454).

Figure 6: Performance bar chart of different choices of hyperparameter \(\) on HIV dataset. The error bar is measured in standard deviation and plotted in grey color.