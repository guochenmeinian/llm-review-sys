# Experimental Designs for Heteroskedastic Variance

Justin Weltz Tanner Fiez  Eric Laber  Alexander Volfovsky

Blake Mason  Houssam Nassif  Lalit Jain

Department of Statistical Science, Duke University, justin.weltz@duke.edu, work conducted at Amazon

###### Abstract

Most linear experimental design problems assume homogeneous variance, even though heteroskedastic noise is present in many realistic settings. Let a learner have access to a finite set of measurement vectors \(^{d}\) that can be probed to receive noisy linear responses of the form \(y=x^{}^{*}+\). Here \(^{*}^{d}\) is an unknown parameter vector, and \(\) is independent mean-zero \(_{x}^{2}\)-strictly-sub-Gaussian noise defined by a flexible heteroskedastic variance model, \(_{x}^{2}=x^{}^{*}x\). Assuming that \(^{*}^{d d}\) is an unknown matrix, we propose, analyze and empirically evaluate a novel design for uniformly bounding estimation error of the variance parameters, \(_{x}^{2}\). We demonstrate the benefits of this method with two adaptive experimental design problems under heteroskedastic noise, fixed confidence transductive best-arm identification, and level-set identification; proving the first instance-dependent lower bounds in these settings. Lastly, we construct near-optimal algorithms and empirically demonstrate the large improvements in sample complexity gained from accounting for heteroskedastic variance in these designs.

## 1 Introduction

Accurate data analysis pipelines require decomposing observed data into signal and noise components. To facilitate this, the noise is commonly assumed to be additive and homogeneous. However, in applied fields this assumption is often untenable, necessitating the development of novel design and analysis tools. Specific examples of deviations from homogeneity include the variability in sales figures of different products in e-commerce , the volatility of stocks across sectors , spatial data , and genomics . This heteroskedasticity is frequently structured as a function of observable features. In this work, we develop two near-optimal adaptive experimental designs that take advantage of such structure.

There are many approaches, primarily developed in the statistics and econometrics literature, for efficient mean estimation in the presence of heteroskedastic noise (see  for a survey). These methods concentrate on settings where data have already been collected and do not consider sampling to better understand the underlying heteroskedasticity. Some early work exists on experimental design in the presence of heteroskedastic noise, however these methods are non-adaptive (see Section 2 for details). Importantly, _using naive data collection procedures designed for homoskedastic environments may lead to sub-optimal inference downstream_ (see Section 1.2 for examples). A scientist may fail to take samples that allow them to observe the effect of interest with a fixed sampling budget, thus wasting time and money. This work demonstrates how to efficiently and adaptively collect datain the presence of heteroskedastic noise. It bridges the gap between heteroskedasticity-robust (but potentially inefficient) passive data collection techniques, and powerful (but at times brittle) active algorithms that largely ignore the presence of differing variances.

### Problem Setting

To ground our discussion of heteroskedasticity in active data collection, we focus on adaptive experimentation with covariates; also known as pure-exploration linear bandits [7; 45]. Let a learner be given a finite set of measurement vectors \(^{d}\), \(()=^{d}\), which can be probed to receive linear responses with additive Gaussian noise of the form:

\[y=x^{}^{*}+(0,_{ x}^{2}),_{x}^{2}:=x^{}^{*}x.\] (1)

Here, \(^{*}^{d}\) is an unknown parameter vector, \(\) is independent mean-zero Gaussian noise with variance \(_{x}^{2}=x^{}^{*}x\), and \(^{*}^{d d}\) is an unknown positive semi-definite matrix with the assumption that \(_{}^{2}_{x}^{2}_{}^{2}\) for all \(x\), where \(_{}^{2},_{}^{2}>0\) are known. We focus on Gaussian error for ease of exposition but extend to strictly sub-Gaussian noise in the Appendix. The linear model in Eq. (1) with unknown noise parameter \(^{*}\) directly generalizes the standard multi-armed bandit to the case where the arms have different variances . Additionally, this heteroskedastic variance structure arises naturally in linear mixed effects models , which are used in a variety of experimental settings to account for multi-level heterogeneity (patient, spatial and block design variation ). Throughout, \(^{d}\) will be a set (that need not be equal to \(\)) over which our adaptive designs will be evaluated. While the theory holds true for \(_{}^{2}_{x}^{2}_{}^{2}\) in general, we consider the case where \(_{}^{2}=_{x}_{x}^{2}\) and \(_{}^{2}=_{x}_{x}^{2}\) so that \(:=_{}^{2}/_{}^{2}\) summarizes the level of heteroskedasticity.

### Heteroskedasticity Changes Optimal Adaptive Experimentation

To underscore the importance of adapting to heteroskedasticity, we consider the performance of RAGE, a best arm identification algorithm designed for homoskedastic settings. This algorithm does not account for differing variances, so it is necessary to upper bound \(_{x}^{2}_{}^{2}\) for each \(x\) to identify the best arm with probability \(1-\) for \((0,1)\). In Fig. 1, we compare this approach to the optimal sampling allocation accounting for heteroskedasticity in a setting that is a twist on a standard benchmark  for best-arm identification algorithms. Let \(=^{2}\) and \(||=3\). We take \(x_{1}=e_{1}\) and \(x_{2}=e_{2}\) to be the first and second standard bases and set \(x_{3}=\{(0.5),(0.5)\}\). Furthermore, we take \(^{*}=e_{1}\) such that \(x_{1}\) has the highest reward and \(x_{3}\) has the second highest. In the homoskedastic case, an optimal algorithm will focus on sampling \(x_{2}\) because it is highly informative for distinguishing between \(x_{1}\) and \(x_{3}\) as shown in Fig 1b. However, if the errors are heteroskedastic, such a sampling strategy may not be optimal. If \(_{2}_{1},_{3}\), the optimal allocation for _heteroskedastic_ noise focuses nearly all samples on \(x_{1}\) and \(x_{3}\) (as shown in Fig. 1a), which are more informative than \(x_{2}\) because they have less noise. Hence, ignoring heteroskedasticity and upper-bounding the variances leads to a suboptimal sampling allocation and inefficient performance. This effect worsens as the amount of heteroskedasticity increases, leading to an additional multiplicative factor of \(\) (cf., , Thm. 1) as demonstrated in Fig. 1c, In this important example, we see that the sample complexity of an algorithm that ignores heteroskedasticity suffers a linear dependence on \(\), while a method that adapts to the heteroskedasticity in the data does not.

### Paper Contributions

1. **Finite Sample Noise Estimation Guarantees.** We use an experimental design technique to construct estimates of the heteroskedastic noise variances with bounded maximum absolute error. This adaptive experimental design is a two-phase sample splitting procedure based on a pair of distinct G-optimal experimental designs , with bounds that scale favorably with the underlying problem dimension.
2. **Near-Optimal Experimental Design Algorithms.** We propose adaptive experimental design algorithms for transductive linear best-arm and level-set identification with heteroskedastic noise, that combine our variance estimation method with optimal experimental design techniques. We prove sample complexity upper bounds for unknown \(^{*}\) that nearly match instance dependent lower bounds when \(^{*}\) is known. Importantly, we show that in contrast to naive methods which have a multiplicative dependence on \(\), we only suffer an _additive_ dependence. Lastly, we show that the theoretical improvements translate to stronger empirical performance.

## 2 Related Work

**Experimental Design and Active Learning for Heteroskedastic Regression.** Many experimental design methods for heteroskedastic regression assume known weights (or efficiency functions) [55; 54]. Procedures that estimate the heteroskedastic variance focus on optimizing the design for the weighted least squares estimator given a burn-in period used to estimate the noise parameters [37; 53; 52]. These contributions assume that the variance is unstructured, which leads to sample complexities that scale with the number of arms. Similarly, past active learning work on linear bandits with heteroskedastic noise either assume the variances are known [30; 62], or that the variances are unstructured [14; 8; 4]. The works that assume structured heteroskedastic noise in a linear bandit framework focus on active maximum likelihood estimation and G-optimal design [7; 45], but do not exploit the structure of the heteroskedastic noise to improve performance, leading to complexities scaling poorly in the problem dimension. Lastly, Mukherjee et al.  study policy value estimation under the reward model described in Eq. 1. However, their variance estimator is different from ours.

**Best-Arm and Level Set Identification in Linear and Logistic Bandits.** Best-arm identification is a fundamental problem in the bandit literature [13; 46; 18]. Soare  constructed the first passive and adaptive algorithms for pure-exploration in linear bandits using G-optimal and \(\)-allocation experimental designs. Fiez et al.  built on these results, developing the first pure exploration algorithm (RAGE) for the transductive linear bandit setting with nearly matching upper and lower bounds. Recent work specializes these methods to different settings [47; 57; 25; 26], improves time complexity [60; 50; 33], and targets different (minimax, asymptotically optimal, etc.) lower bound guarantees [59; 22; 9]. In the level set literature, Mason et al.  provides the first instance optimal algorithm with matching upper and lower bounds. Critically, none of these contributions account for heteroskedastic variance. Finally, we note the connection between this work and the logistic bandits literature [23; 11; 1; 36]. In that setting, the observations are Bernoulli random variables with a probability given by \((y=1|x)=\{1+(-x^{}^{*})\}^{-1}\) for \(x^{d},^{*}^{d}\) and \(y\{0,1\}\). Because of the mean-variance relationship for Bernoulli random variables, points \(x\) for which \((y=1|x)\) is near \(0\) or \(1\) have lower variance than other \(x\)'s. While the tools are different in these two cases, we highlight two common ideas: First, Mason et al.  present a method that explicitly handles the heteroskedasticity in logistic bandits. Second, a core focus of optimal logistic bandit papers is reducing the dependence on the worst case variance to only an additive penalty, similar to the guarantee of \(\) herein, which has been shown to be unavoidable in general .

## 3 Heteroskedastic Variance Estimation

In this section, we describe an algorithm for heteroskedastic noise variance estimation with error bounds that scale favorably in the problem dimension. We then evaluate this method empirically.

Figure 1: We compare the sampling strategies of algorithms that do and do not account for heteroskedastic variance in Figs. 0(a) and 0(b) respectively, when \(=20\). Figure 0(c) shows that the gap in the optimal sample complexity between the two algorithms becomes linear as \(\) increases.

Given a sampling budget \(\), our goal is to choose \(\{x_{t}\}_{t=1}^{}\) to control the tail of the _maximum absolute error (MAE):_\(_{x}|_{x}^{2}-_{x}^{2}|\) by exploiting the heteroskedastic variance model. Alg. 1, HEAD (Heteroskedasticity Estimation by Adaptive Designs), provides an experimental design for finite-time bounds on structured heteroskedastic variance estimation.

**Notation.** Let \(M:=d(d+1)/2\). Define \(=\{_{x},\;\;x\}^{M}\) where \(_{x}=(xx^{})G\) and \(G^{d^{2} M}\) is the duplication matrix such that \(G(xx^{})=(xx^{})\), with \(()\) denoting the half-vectorization. For a set \(\), let \(P_{}:=\{^{||}:_{v }_{v}=1,_{v} 0\;\;v\}\) denote distributions over \(\) and \(():=\{v^{}-v:v,v^{},\;v v ^{}\}\) represent the differences between members of set \(\).

**General Approach.** Given observations from \(y_{t}=x_{t}^{}^{*}+_{t}\) for \(t\{1,2,,\}\) and the true value of \(^{*}\), we can estimate \(_{x}^{2}\) for each \(x\) from the squared residuals \(_{t}^{2}=(y_{t}-x_{t}^{}^{*})^{2}\) (e.g., using method of moments or another estimating equation approach ). However, since \(^{*}\) is not known, we cannot directly observe \(_{t}^{2}\), making estimation of \(_{x}^{2}\) more difficult. Consider a general procedure that first uses \(/2\) samples from the budget to construct a least-squares estimator for \(^{*}\), \(_{/2}\), and then uses the final \(/2\) samples to collect error estimates of the form \(_{t}^{2}=(y_{t}-x_{t}^{}_{/2})^{2}\). Define \(_{}^{/2 M}\) as a matrix with rows \(\{_{x_{t}}\}_{t=/2+1}^{}\) and \(^{/2}\) as a vector with values \(\{_{t}\}_{t=/2+1}^{}\). With \(\), we can estimate \(^{*}\) by least squares as

\[(_{})=*{arg\,min}_{ ^{M}}\|_{}\,-^{2}\| ^{2},\]

where we minimize over the space of symmetric matrices. Splitting samples ensures that the errors in estimating \(^{*}\) and \(^{*}\) are independent. Note that \((_{},_{/2})\) is an M-estimator of \((^{*},^{*})\) derived from unbiased estimating functions, a common approach in heteroskedastic regression . Finally, by plugging in the estimate of \(^{*}\), one can obtain an estimate of the variance, \(_{x}^{2}=\{\{x^{}_{}x, _{}^{2}\},_{}^{2}\}\). We show that this approach leads to the following general error decomposition for any \(x\) with \(z:=_{x}^{}(_{}^{}_{})^{-1}_{}^{}\):

\[|x^{}(^{*}-_{})x|(_{/2}-^{*})\}^{2}}_{}+ ^{}z_{t}(_{t}^{2}-x_{t}^{ }^{*}x_{t})|}_{}+^{}2z_{t}x_{t}^{}(_{/2}-^{ *})_{t}|}_{}.\]

**Experimental Design for Precise Estimation.** The analysis above gives an error decomposition for any phased non-adaptive data collection method that splits samples between estimation of \(^{*}\) and \(^{*}\). Intuitively, we wish to sample \(\{x_{1},,x_{/2}\}\) and \(\{x_{/2+1},,x_{}\}\) to control the maximum absolute error by minimizing the expression above. We achieve this via two phases of G-optimal experimental design - first over the \(\) space to estimate \(^{*}\), and then over the \(\) space to estimate \(^{*}\). Precisely, we control quantity \(\) via stage 1 of Alg. 1, which draws from a G-optimal design \(^{*}\) to minimize the maximum predictive variance

\[_{x}[\{x^{}(_{/2 }-^{*})\}^{2}].\]

Quantity \(\) is another sub-exponential quantity (Definition B.2 in Appendix B), which we control by drawing from a G-optimal design \(^{}\) over \(\) to minimize \(_{}^{}(_{}^{}_{})^{-1}\). Finally, \(\) is controlled by a combination of the guarantees associated with \(^{*}\) and \(^{}\). This leads to the following error bound on \(_{x}^{2}=\{\{x^{}_{}x, _{}^{2}\},_{}^{2}\}\).

**Theorem 3.1**.: Assume \(=[\{_{}^{2}(||/)d^{2}, d^{2}\}]\). For any \(x\) and \((0,1)\), Alg. 1 guarantees the following:

\[(|_{x}^{2}-_{x}^{2}| C_{,} ) 1-/2 C_{,}=\{ |/)_{}^{2}d^{2}/}\}.\]

The proof of Theorem 3.1 is in Appendix D and follows from the decomposition in Eq. 3; treating \(\), \(\) and \(\) as sub-exponential random variables with bounds that leverage the experimental designs of Alg. 1. The details on the constants involved in Theorem 3.1 are also included in Appendix D.

Note that we design to directly control the MAE of \(_{x}^{2}\). This is in contrast to an inefficient alternative procedure that allocates samples to minimize the error of \(_{}\) in general, and then extends this bound to \(x^{}_{}x\) for \(x\).

**Theoretical Comparison of Variance Estimation Methods.** In contrast to previous approaches that naively scale in the size of \(\), the above result has a dimension dependence of \(d^{2}\), which intuitively scales with the degrees of freedom of \(^{*}\). To highlight the tightness of this result, consider estimating the noise parameters for each arm, \(x\), only using the samples of \(x\) (like the method in ). We improve this approach by adapting it to our heteroskedastic variance structure and strategically pick \(d^{2}\) points to avoid the dependence on \(||\). We refer to this method as the _Separate Arm Estimator_, and in Appendix D, we show that it suffers a dependence of \(d^{4}\). This comparison point, along with our empirical results below, suggest that the error bounds established in Theorem 3.1 scale favorably with the problem dimension.

**Empirical Comparison of Variance Estimation Methods.** Define two sets of arms \(_{1},_{2}\) such that \(x_{1}\) is drawn uniformly from a unit sphere and \(x_{2}\) is drawn uniformly from a sphere with radius \(0.1\); with \(|_{1}|=200\) and \(|_{2}|=1800\). Let \(^{*}=(1,0.1,1,0.1,)^{d d}\), and \(^{*}=(1,1,,1)^{d}\). This setting provides the heteroskedastic variation needed to illustrate the advantages of using HEAD (Alg. 1). An optimal algorithm will tend to target orthogonal vectors in \(_{1}\) because these arms have higher magnitudes in informative directions. Defining \(=_{1}_{2}\), we perform 32 simulations in which we randomly sample the arm set and construct estimators for each \(_{x}^{2},\ x\). We compare HEAD, the Separate Arm Estimator, and the Uniform Estimator (see Alg. 3 in Appendix D). The Uniform Estimator is based on Alg. 1 but samples arms uniformly from \(\) and does not split samples between estimation of \(^{*}\) and \(^{*}\). HEAD should outperform its competitors in two ways: 1) optimally allocating samples, and 2) efficiently sharing information across arms in estimation. The Uniform Estimator exploits the problem structure for estimation but not when sampling. In contrast, the Separate Arm Estimator optimally samples, but does not use the relationships between arms for estimation. Fig. 2 depicts the average MAE and standard errors for each estimator. In Fig. 2a, we see that HEAD outperforms its competitors over a series of sample sizes for \(d=15\). Fig. 2b compares the estimators over a range of dimensions for a sample size of \(95k\). While HEAD continues to accurately estimate the heteroskedastic noise parameters at high dimensions, the Separate Arm Estimator error scales poorly with dimension as the analysis suggests.

``` Result: Find \(_{}\) Input: Arms \(^{d},\)
1//Stage1: Take half the samples to estimate \(^{*}\) Determine \(^{*}\) according to \(^{*}=_{ P_{}}_{x}x^{ }(_{x^{}}_{x^{}}x^{}x^{ })^{-1}x\)
2 Pull arm \(x[_{*}^{*}/2]\) times and collect observations \(\{x_{t},y_{t}\}_{t=1}^{/2}\) Define \(A^{*}=_{t=1}^{/2}x_{t}x_{t}^{}\) and \(b^{*}=_{t=1}^{/2}x_{t}y_{t}\) and estimate \(_{/2}=A^{*-1}b\)
3//Stage2: Take half the samples to estimate \(^{*}\) given \(_{/2}\) Determine \(^{}_{x}^{}=_{ P_{ }}_{x}_{x}^{}(_{x^{} }_{x^{}}_{x^{}}^{})^{-1}_{x}\)
4 Pull arm \(x[_{}^{}/2]\) times\(^{*}\) and collect observations \(\{x_{t},y_{t}\}_{t=1}^{/2}\)
5 Let \(A^{}=_{t=1}^{}_{x_{t}}_{x_{t}}^{}\), \(b^{}=_{t=/2+1}^{}_{x_{t}}(y_{t}-x_{t}^{} _{/2})^{2}\)
6Output:\((_{})={A^{}}^{-1}b^{}\). ```

**Algorithm 1**HEAD (Heteroskedasticity Estimation by Adaptive Designs)

## 4 Adaptive Experimentation with Covariates in Heteroskedastic Settings

As an application of the novel sampling procedure and resulting variance estimation bounds developed in section 3, we now study _adaptive experimentation with covariates_ in the presence of heteroskedasticnoise through the lens of _transductive linear identification_ problems . Recalling the problem setting from Section 1.1, we consider the following fixed confidence identification objectives:

1. _Best-Arm Identification_ (BIA). Identify the singleton set \(\{z^{*}\}\) where \(z^{*}=_{z}z^{}^{*}\).
2. _Level-Set Identification_ (LS). Identify the set \(G_{}=\{z:z^{}^{*}>\}\) given a threshold \(\).

This set of objectives, together with a learning protocol, allow us to capture a number of practical problems of interest where heteroskedastic noise naturally arises in the assumed form. Consider multivariate testing problems in e-commerce advertisement applications [17; 39]. The expected feedback is modeled through a linear combination of primary effects from content variation dimensions (e.g., features or locations that partition the advertisement), and interaction effects between content variation in pairs of dimensions. The noise structure is naturally heteroskedastic and dependent on the combination of variation choices. Moreover, the transductive objectives allow for flexible experimentation such as identifying the optimal combination of variations or the primary effects that exceed some threshold. We return to this setting in Experiment 3 of Section 5. In the remainder of this section, we characterize the optimal sample complexities for the heteroskedastic transductive linear identification problems and design algorithms that nearly match these lower bounds.

### Linear Estimation Methods with Heteroskedastic Noise

To draw inferences in the experimentation problems of interest, it is necessary to consider estimation methods for the unknown parameter vector \(^{*}^{d}\). Given a matrix of covariates \(X_{T}^{T d}\), a vector of observations \(Y_{T}^{T}\), and a user-defined weighting matrix \(W_{T}=(w_{1},,w_{T})^{T T}\), the weighted least squares (WLS) estimator is defined as:

\[_{}=(X_{T}^{}W_{T}X_{T})^{-1}X_{T} ^{}W_{T}Y_{T}=_{^{d}}\|Y_{T}-X_{T}\|_{W_{T}}^ {2}.\]

Let \(_{T}=(_{x_{1}}^{2},_{x_{2}}^{2},, _{x_{T}}^{2})^{T T}\) denote the variance parameters of the noise observations. The WLS estimator is unbiased regardless of the weight selections, with variance

\[(_{})=(X_{T}^{}W_{ T}X_{T})^{-1}X_{T}^{}W_{T}_{T}W_{T}^{}X_{T}(X_{T}^{ }W_{T}X_{T})^{-1}_{T}^{-1}}{=}( X_{T}^{}_{T}^{-1}X_{T})^{-1}.\] (2)

The WLS estimator with \(W_{T}=_{T}^{-1}\) is the minimum variance linear unbiased estimator . In contrast, the ordinary least squares (OLS) estimator, obtained by taking \(W_{T}=I\) in the WLS estimator, is unbiased but with a higher variance,

\[(_{})=(X_{T}^{}X_{ T})^{-1}X_{T}^{}_{T}X_{T}(X_{T}^{}X_{T})^{-1} _{}^{2}(X_{T}^{}X_{T})^{-1}.\]

Consequently, WLS is a natural choice with heteroskedastic noise for sample efficient estimation.

### Optimal Sample Complexities

We now motivate the optimal sample complexity for any algorithm that returns the correct quantity with a fixed probability greater than \(1-\), \((0,1)\), in transductive identification problems with

Figure 2: The proposed experimental design estimator outperforms competitors in terms of maximum absolute error \(_{x}|_{x}^{2}-_{x}^{2}|\) over a range of sample sizes (a) and dimensions (b).

heteroskedastic noise. We seek to identify a set of vectors \(_{}\), where

\[_{}=\{z^{*}\}=\{_{z}z^{}^{* }\}\ \ _{}=\{z:z^{}^{*}->0\}.\]

Algorithms consist of a sampling strategy and a rule to return a set \(}\) at time \(\).

**Definition 4.1**.: An algorithm is \(\)-PAC for \(\{,\}\) if \(\,\), \(_{}(}=_{}) 1-\).

Throughout, we will only consider algorithms that are \(\)-PAC. For drawing parallels between best-arm and level set identification, it will be useful to define a set of values \(_{}\), where

\[_{}=\{z:z,z z^{*}\}\ \ _{}=\{0\},\]

and a constant \(b_{}\) such that \(b_{}=0\) and \(b_{}=\). Both problems amount to verifying that for all \(h_{}\) and \(q_{}\), \((h-q)^{}_{}>b_{}\), or equivalently \((h-q)^{}^{*}-b_{}-(h-q)^{}(^{*}-_{})>0\). Using a sub-Gaussian tail bound and a union bound , this verification requires the following to hold for all \(h_{}\) and \(q_{}\) for the data collected:

\[|(h-q)^{}(^{*}-_{})|(_{})}^{2}(2| |/)}(h-q)^{}^{*}-b_{}.\] (3)

Let \(_{x}\) denote the proportion of \(T\) samples given to a measurement vector \(x\) so that when \(W_{T}=_{T}^{-1}\) in Eq. (4.1),

\[(_{})=(X_{T}^{} _{T}^{-1}X_{T})^{-1}=(_{x}_{x} _{x}^{-2}xx^{})^{-1}/T\]

as in Eq. (2). Now, we reformulate the condition in Eq. (3) and minimize over designs \( P_{}\) to see that \(\)-PAC verification requires

\[T 2_{}^{*}(2||/),\ \ \ \ _{}^{*}=_{ P_{}}_{h_ {},q_{}}}_{x}_{x}^{-2}xx^{})^{-1}}^{2}}{\{(h-q )^{}^{*}-b_{}\}^{2}}.\] (4)

This motivating analysis gives rise to the following sample complexity lower bound.

**Theorem 4.1**.: Consider an objective, \(\), of best-arm identification (\(\)) or level-set identification (LS). For any \((0,1)\), a \(\)-PAC algorithm must satisfy

\[E_{}[] 2(1/2.4)_{}^{*}.\]

**Remark 4.1**.: This lower bound assumes that the noise variance parameters are _known_. We compare to the existing lower bounds for the identification objectives with homoskedastic noise  in Appendix G. These apply in this setting by taking the variance parameter \(^{2}=_{}^{2}\) and are characterized by an instance-dependent parameter, \(_{}^{*}\), such that \(^{-1}_{}^{*}/_{}^{*} 1\) (recall that \(:=_{}^{2}/_{}^{2}\)). In general, \(\) can be quite large in many problems with heteroskedastic noise. Consequently, this implies that near-optimal algorithms for linear identification with homoskedastic noise can be _highly suboptimal_ (by up to a multiplicative factor of \(\)) in this setting.

### Adaptive Designs with Unknown Heteroskedastic Variance

We now present Alg. 2, H-RAGE (Heteroskedastic Randomized Adaptive Gap Elimination), as a solution for adaptive experimentation with covariates in heteroskedastic settings. The approach is composed of two components: 1) Obtain accurate estimates of the unknown variance parameters \(\{_{x}^{2}\}_{x}\) using HEAD; 2) Use adaptive experimental design methods for minimizing the uncertainty in directions of interest given the estimated variance parameters. Denote by \(=_{h_{}}_{_{}}(h-q)^{}^{*}-b_{}\) the minimum gap for the objective and assume \(_{z_{}}|(z-z^{*},^{*})| 2\). We prove the following guarantees for Alg. 2.

**Theorem 4.2**.: Consider an objective, \(\), of best-arm identification (\(\)) or level-set identification (LS). The set returned from Alg. 2 at a time \(\) is \(\)-PAC and

\[=(_{}^{*}(^{-1})[(| |/)+\{(^{-1})\}]+(^{ -1})d^{2}+(||/)^{2}d^{2}).\]

**Remark 4.2**.: This result shows that Alg. 2 is nearly instance-optimal. Critically, the impact of not knowing the noise variances ahead of time only has an _additive_ impact on the sample complexity relative to the lower bound. In contrast, existing near-optimal methods for identification problems with homoskedastic noise would be suboptimal by a _multiplicative_ factor depending on \(\) relative to the lower bound. Given that the lower bound assumes the variances are known, an interesting question for future work is a tighter lower bound assuming the variance parameters are unknown.

``` Result: Find \(z^{*}:=_{z}z^{}^{*}\) for BAI or \(G_{}:=\{z:z^{}^{*}>\}\) for LS
1Input:\(^{d}\), \(^{d}\), confidence \((0,1)\), OBJ \(\) {BAI,LS}, threshold \(\)
2Initialize:\( 1\), \(_{1}\), \(_{1}\), \(_{1}\)
3//Variance estimation
4Call Alg. 1 such that \(|_{x}^{2}=\{\{x^{}_{ }x,_{}^{2}\},_{}^{2}\}-_{x}^{2}| _{x}^{2}/2\)
5while (\(|_{}|>1\) and DBJ=BAI) or (\(|_{}|>0\) and OBJ=LS)do
6//Determine the design
7 Let \(_{} P_{}\) be a minimizer of \(q\{,(_{})\}\) if OBJ=BAI and \(q(,_{})\) if OBJ=LS where \[q()=_{ P_{}}q(;)=_{  P_{}}_{z}||z||_{(_{z}_{x}^{-2}_{z}xx^{})^{-1}}^{2}\]
8 Set \(_{}=2^{-}\), \(_{}=3_{}^{-2}q(_{})(8^{2}||/)\)//Determine stepsize
9 Pull arm \(x\) exactly \(_{}_{,x}\) times for \(n_{}\) samples and collect \(\{x_{,i},y_{,i}\}_{i=1}^{n_{}}\)
10 Define \(A_{}:=_{i=1}^{n_{}}_{i}^{-2}x_{,i}x_{,i}^ {},\;b_{}=_{i=1}^{n_{}}_{i}^{-2}x_{,i}y_{ ,i}\) and construct \(_{}=A_{}^{-1}b_{}\)
11//Eliminate arms
12ifOBJ is BAIthen
13\(_{+1}_{}\{z_{ }:_{z^{}_{}} z^{}-z,_{}>_{}\}\)
14else
15\(_{+1}_{}\{z_{}:  z,_{}-_{}>\}\)
16\(_{+1}_{}\{z_{}:  z,_{}+_{}<\}\)
17\(_{+1}_{}\{\{_{ }_{+1}\}\{_{}_{+1}\}\}\)
18 end if
19\(+1\)
20 end while
21Output:\(_{}\) for BAI or \(_{}\) for LS ```

**Algorithm 2**(H-RAGE) Heteroskedastic Randomized Adaptive Gap Elimination

**Remark 4.2**.: This result shows that Alg. 2 is nearly instance-optimal. Critically, the impact of not knowing the noise variances ahead of time only has an _additive_ impact on the sample complexity relative to the lower bound. In contrast, existing near-optimal methods for identification problems with homoskedastic noise would be suboptimal by a _multiplicative_ factor depending on \(\) relative to the lower bound. Given that the lower bound assumes the variances are known, an interesting question for future work is a tighter lower bound assuming the variance parameters are unknown.

**Algorithm Overview.** Observe that for \(^{*}_{}\), the optimal allocation depends on both the unknown gaps \(((h-q)^{}^{*}-b_{})\) for \(h_{}\) and \(q_{}\), and the unknown noise variance parameters \(\{_{x}^{2}\}_{x}\). To handle this, the algorithm begins with an initial burn-in phase using the procedure from Alg. 1 to produce estimates \(\{_{x}^{2}\}_{x}\) of the unknown parameters \(\{_{x}^{2}\}_{x}\), achieving a multiplicative error bound of the form \(|_{x}^{2}-_{x}^{2}|_{x}^{2}/2\) for all \(x\). From this point, the estimates \(\{_{x}^{2}\}_{x}\) are used as plug-ins to the experimental designs and the weighted least squares estimates. In each round \(\) of the procedure, we maintain a set of uncertain items \(_{}\), and obtain the sampling distribution \(_{}\), by minimizing the maximum predictive variance of the WLS estimator over all directions \(y=h-q\) for \(h_{}\) and \(q_{}\). This is known as an \(\)-allocation and \(G\)-optimal-allocation in the best-arm and level set identification problems respectively. Critically, the number of samples taken in round \(\) guarantees that the error in all estimates is bounded by \(_{}\), such that for any \(h_{}\) and \(q_{}\), \((h-q)^{}^{*}>2_{}\) is eliminated from the active set at the end of the round. By progressively honing in on the optimal item set \(_{}\), the sampling allocation gets closer to approximating the oracle allocation.

## 5 Experiments

We now present experiments focusing on the transductive linear bandit setting. We compare H-RAGE and its corresponding oracle to their homoskedastic counterparts, RAGE and the oracle allocationwith noise equal to \(_{}^{2}\). RAGE is provably near-optimal in the homoskedastic setting and known to perform well empirically. The algorithm is similar in concept to Alg. 2, but lacks the initial exploratory phase to estimate the variance parameters. We include the pseudo-code of RAGE in Appendix H. The homoskedastic and heteroskedastic oracles sample with respect to the optimal design with a sub-Gaussian interval stopping condition. All algorithms are run at a confidence level of \(=0.05\), and we use the Franke-Wolfe method to compute the designs. We demonstrate that H-RAGE has superior empirical performance over a range of settings.

**Linear bandits: Experiment 1 (Signal-to-Noise).** We begin by presenting an experiment that is a variation of the standard benchmark problem for BAI in linear bandits  introduced in Sec. 1.2. Define \(e_{i}^{d}\) as the standard basis vectors in \(d\) dimensions. In the standard example, \(==\{e_{1},e_{2},x^{}\}\) where \(x^{}=\{e_{1}()+e_{2}()\}\) for \( 0\), and \(^{*}=e_{1}\). Thus \(x_{1}=e_{1}\) is optimal, \(x_{3}=x^{}\) is near-optimal, and \(x_{2}=e_{2}\) is informative to discriminate between \(x_{1}\) and \(x_{3}\). We consider an extension of this benchmark in multiple dimensions that highlights the performance gains from accounting for heteroskedastic variance. First, define an arm set \(\) that is comprised of the standard example but with varied magnitudes. For \(q(0,1)\),

\[=\{e_{1}\}\{e_{2}\}\{q e_{i}\}_{i=3}^{d}\{e_{1} ()+e_{i}()\}_{i=2}^{d}.\]

Then define \(\) to be a series of vectors such that the rank of \(\{_{x}:x\}=^{M}\). \(\) allows for variance estimation. Finally, let the arm and item set be \(==\) and \(^{*}=e_{1}\).

We define \(^{*}\) as the \(d\)-dimensional identity matrix. Observe that \(x_{1}=e_{1}\) is optimal, the \(d-1\) arms given by \(\{e_{1}()+e_{i}()\}_{i=2}^{d}\) are near-optimal with equal gaps \(=1-()\), and the \(d-1\) arms given by \(\{e_{2}\} q*\{e_{i}\}_{i=3}^{d}\) are informative to sample for discriminating between \(x_{1}=e_{1}\) and \(x_{i}=e_{1}()+e_{i}()\) for each \(i\{2,,d\}\), respectively. To identify \(x_{1}\), we must estimate the \(\) gap adequately in every dimension with arms \(\{e_{2}\} q*\{e_{i}\}_{i=3}^{d}\). Therefore, performing BAI without accounting for heteroskedastic variances will tend toward a design that prioritizes sampling informative arms \(\{q e_{i}\}_{i=3}^{d}\), since these have smaller magnitudes and seem less informative about the near-optimal arms in dimensions \(i\{3,4 d\}\). However, the signal to noise ratio is actually constant across arms (because the variance of \(q*\{e_{i}\}_{i=3}^{d}\) is equal to \(q^{2}\)) leading to an oracle distribution that equally samples across \(\{e_{2}\} q*\{e_{i}\}_{i=3}^{d}\) when we account for heteroskedastic variances. This change in allocation is illustrated in Appendix H for \(d=4,=0.01\), \(q=0.4\) and \(=0.5(e_{1}+e_{2})+0.1e_{3},0.5(e_{1}+e_{2})+0.1(e_{3}+e_{4}) }\). In the same setting, Fig. 2(a) shows that the heteroskedastic variance experimental design and the tighter predictive confidence intervals of the weighted least squares estimator result in a large decrease in the sample size needed to identify the best arm.

**Experiment 2: Benchmark.** This experiment is also similar to the benchmark example introduced in Sec. 1.2. In this variation, the informative arms have the same magnitude but are bent at a \(45^{}\) angle,

\[=\{e_{1}\}\{e_{1}()+e_{2}()\}\{e_{i}\}_ {i=3}^{d}\{e_{i}/+e_{j}/\}_{i=1}^{d}\}_{j>i}^{d}.\]

Let the arm and item set be \(=\) and \(^{*}=e_{1}\). In Experiment 1, we have many near-optimal arms and H-RAGE reveals that each of these are equally difficult to distinguish from the best. In contrast, Experiment 2 contains one near-optimal arm along with many potentially informative arms, and we are interested in identifying the one that is most helpful. Intuitively, Experiment 1 uses heteroskedastic variance estimates to assess the difficulty of many problems, while Experiment 2 uses the same information to assess the benefits of many solutions. Let the unknown noise matrix be given by \(^{*}=(_{1}^{2},_{2}^{2},,_{d}^{2})\) where \(_{1}^{2}=^{2}\), \((_{2}^{2},_{3}^{2})=^{2}\) and \((_{4}^{2},_{5}^{2},_{d}^{2})=^{2}\). \(x_{1}=e_{1}\) is again optimal and \(x_{2}=\{e_{1}()+e_{2}()\}\) is a near optimal arm. In this case, \(\{e_{2}/+e_{1}/\}\{e_{2}/+e_{i}/\}_{i=3}^{d}\) are informative for distinguishing between \(x_{1}\) and \(x_{2}\), and the oracle allocation assuming homoskedastic noise (Homoskedastic Oracle) picks three of these vectors to sample. However, if \(^{2}^{2}\), then it is optimal to sample \(\{e_{2}/+e_{3}/,e_{3}\}\) over other potential informative arm combinations. Appendix H shows this contrast in allocations for \(d=3\), \(=0.01\), \(^{2}=1\) and \(^{2}=0.2\). In the same setting, Fig. 2(b) shows that estimating and accounting for heteroskedastic noise contributes to a reduction in sample complexity.

**Experiment 3: Multivariate Testing.** In this experiment, we return to the motivating example of Section 4, multivariate testing in e-commerce advertising . We divide an advertisement into natural locations or features called dimensions, each of which has different content options or variations. We induce heteroskedastic variance by allowing both the expected value and the variance of an advertisement's reward to depend on the \(n\) variations in each of the \(m\) dimensions. Let \(=\) denote the set of layouts corresponding to the combinations of variant choices in the dimensions so that \(||=n^{m}\). The multivariate testing problem is often modeled in the linear bandit framework using the expected feedback for any layout \(x\{0,1\}^{d}\), where \(d=1+mn+n^{2}m(m+1)/2\), given by

\[x^{}^{*}:=_{0}^{*}+_{i=1}^{m}_{j=1}^{n}_{ i,j}^{*}x_{i,j}+_{i=1}^{m}_{i^{}=i+1}^{m}_{j=1}^{n} _{j^{}=1}^{n}_{i,i^{},j,j^{}}^{*}x_{i,j}x_{i^{ },j^{}}.\]

In this model, \(x_{i,j}\{0,1\}\) is an indicator for variation \(j\{1,,n\}\) being placed in dimension \(i\{1,,m\}\), and \(_{j=1}^{n}x_{i,j}=1\) for all \(i\{1,,n\}\) for any layout \(x\). Observe that the expected feedback for a layout is modeled as a linear combination of a common bias term, primary effects from the variation choices within each dimension, and secondary interaction effects from the combinations of variation choices between dimensions.

We consider an environment where the effect of variation changes in one dimension, call it dimension \(j\), has much higher variance than others; resulting in the best variation for dimension \(j\) being harder to identify. An algorithm that accounts for heteroskedastic variance will devote a greater number of samples to compare variations in dimension \(j\), whereas an algorithm that upper bounds the variance will split samples equally between dimensions. For a simulation setting with 2 variations in 3 dimensions, we define \(^{*}=(0.3,0.7,10^{-3},10^{-3},)^{7  7}\) and \(^{*}=(0,0.005,0.0075,0.01,-0.1,-0.1,)^{7}\). This simulation setting implies that the second variation in each of the dimensions is better than the first, but the positive effect in the first dimension is hardest to identify. In Appendix H we can see that this causes the heteroskedastic oracle to devote more weight than the homoskedastic oracle to vectors that include the second variation in the first dimension. Fig. 2(c) depicts the improvement in sample complexity resulting from accounting for the heteroskedastic variances.

## 6 Conclusion

This paper presents an investigation of online linear experimental design problems with heteroskedastic noise. We propose a two-phase sample splitting procedure for estimating the unknown noise variance parameters based on G-optimal experimental designs and show error bounds that scale efficiently with the dimension of the problem. The proposed approach is then applied to fixed confidence transductive best-arm and level set identification with heteroskedastic noise, where we present instance-dependent lower bounds with provably near-optimal algorithms.

This paper leads to a number of interesting future research directions. For example, there may be situations where the noise structure is not fully characterized by \(^{*}\). Future work can explore the development of methods that handle more general noise structures (including correlated or heavy-tailed noise) while remaining near-optimal. Furthermore, it is possible that an integrated approach to variance estimation and adaptive experimentation with covariates would be more sample efficient than our two-step method. Lastly, it would be interesting to extend our algorithm to nonlinear response models.

Figure 3: Experimental results show the mean sample complexity of each algorithm over 100 simulations with 95% Monte Carlo confidence intervals.