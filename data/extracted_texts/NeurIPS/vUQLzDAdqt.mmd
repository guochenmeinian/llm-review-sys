# Quantum Diffusion Model for Quark and Gluon Jet Generation

Mariia Baidachna*

University of Glasgow

&Rey Guadarrama

Benemerita Universidad Autonoma de Puebla

Gopal Ramesh Dahale

EPFL

&Tom Magorsch

Technische Universitat Dortmund

&Isabel Pedraza

CERN

Konstantin T. Matchev

University of Florida

&Katia Matcheva

University of Florida

&Kyoungchul Kong

University of Kansas

Sergei Gleyzer

University of Alabama

Corresponding author: 2828197b@student.gla.ac.uk

###### Abstract

Diffusion models have demonstrated remarkable success in image generation, but they are computationally intensive and time-consuming to train. In this paper, we introduce a novel diffusion model that benefits from quantum computing techniques in order to mitigate computational challenges and enhance generative performance within high energy physics data. The fully quantum diffusion model replaces Gaussian noise with random unitary matrices in the forward process and incorporates a variational quantum circuit within the U-Net in the denoising architecture. We run evaluations on the structurally complex quark and gluon jets dataset from the Large Hadron Collider. The results demonstrate that the fully quantum and hybrid models are competitive with a similar classical model for jet generation, highlighting the potential of using quantum techniques for machine learning problems.

## 1 Introduction

Denoising diffusion models (DDMs) have revolutionized the field of generative artificial intelligence (GenAI) by demonstrating their ability to generate high-quality images . They overcome the drawbacks of generative adversarial networks (GANs), which are prone to mode collapse, becoming a new state-of-the-art architecture for image generation [11; 25]. Consequently, DDMs have been applied in many generative tasks for science from molecular biology to medical image synthesis to gravitational lensing [26; 2; 27; 18; 19].

Despite their successes, DDMs face significant challenges concerning the extensive computational resources required for training [24; 16]. New compute paradigms must be employed in order to overcome the computational bottleneck. Quantum machine learning (QML) offers a promising solution . By cleverly incorporating quantum components into classical algorithms, quantum computers can efficiently solve problems that are difficult for classical computers with accelerated computations . This paradigm shift has the potential to surpass current limitations and unlock the full potential of DDMs.

Background

### Denoising Diffusion Model

There have been multiple variants of a DDM proposed, such as denoising diffusion probabilistic model (DDPM) [23; 15] and denoising diffusion implicit model (DDIM) , but we generalize these into DDMs with the common factors being a noising scheduling algorithm and a learned denoising process.

#### Forward Diffusion Process

The forward diffusion process gradually adds Gaussian noise to the data, leading to a series of latent variables \(_{1},_{2},,_{T}\). The forward process is defined as:

\[q(_{t}_{t-1})=(_{t}}_{t-1},_{t}),\] (1)

where \(_{t}\) is the variance schedule that controls the amount of noise added at each step \(t\).

Starting from the original data distribution \(q(_{0})\), the joint distribution over the sequence of latent variables is given by:

\[q(_{1:T}_{0})=_{t=1}^{T}q(_{t} _{t-1}),\] (2)

and the marginal distribution of any \(_{t}\) given \(_{0}\) is:

\[q(_{t}_{0})=(_{t}_{t}}_{0},(1-_{t})),\] (3)

where \(_{t}=_{s=1}^{t}(1-_{s})\).

#### Reverse Diffusion Process

The reverse diffusion process is modeled as:

\[p_{}(_{t-1}_{t})=(_{t-1} _{}(_{t},t),_{t}^{2}),\] (4)

where \(_{}(_{t},t)\) is a neural network parameterized by \(\) that is trained to predict the mean, and \(_{t}^{2}\) is typically set to \(_{t}\) or some other small variance.

The training objective is to minimize the variational lower bound on the negative log-likelihood:

\[L_{}=_{q}[D_{}(q(_{T}_{0})\,\|\,p(_{T}))+_{t=2}^{T}D_{}(q(_{t-1} _{t},_{0})\,\|\,p_{}(_{t-1} _{t}))- p_{}(_{0}_{1})],\] (5)

where \(D_{}\) denotes the Kullback-Leibler divergence, though the loss is often simplified to the mean squared error (MSE).

### Quantum Theory for Machine Learning

#### Qubit States and Measurement

Quantum bits, or qubits, can exist in superpositions of states, allowing them to represent more information and process it more efficiently than classical bits . Unlike a classical bit that can be either 0 or 1, a qubit can be in a superposition of both states simultaneously. Mathematically, the state of a qubit can be written as:\[|=|0+|1,\] (6)

where \(|0\) and \(|1\) are the basis states, and \(\) and \(\) are complex numbers representing the probability amplitudes. These amplitudes are normalized so that:

\[||^{2}+||^{2}=1.\] (7)

When a measurement is made on a qubit, the superposition collapses to one of the basis states. The probability of measuring the state \(|0\) is \(||^{2}\), and the probability of measuring the state \(|1\) is \(||^{2}\).

One method of measurement relevant in quantum computing and QML is the Haar measurement. Haar measurement involves sampling unitary operations uniformly according to the Haar measure, which is the unique, invariant measure on the group of unitary matrices. This type of measurement is useful in QML because it provides a way to generate random quantum states and operations, which is important for algorithms that require randomization .

#### Variational Quantum Circuits in Machine Learning

Variational quantum circuits (VQCs) are a central tool in QML used in various architectures [7; 12; 20; 22; 9]. A VQC is a parameterized quantum circuit where some of the gates depend on adjustable parameters represented by quantum gates in the unitary operations. The general form of a VQC can be represented as:

\[|()=U()|0,\] (8)

where \(U()\) is a unitary operation that depends on the parameters \(\).

In the context of machine learning, these parameters are optimized using classical optimization techniques to minimize the cost function that measures the performance of the circuit throughout training. The power of VQCs in machine learning arises from their ability to exploit superposition and entanglement to potentially represent and solve problems more efficiently than classical algorithms .

### Quark-Gluon Data Description

In this work, we generate quark and gluon jet data from the open-source LHC Compact Muon Solenoid (CMS) detector data. The data contains two classes that follow different distributions: hits from quark and gluon jets. Each sample is captured by three CMS subdetectors: electromagnetic calorimeter (ECAL), hadronic calorimeter (HCAL), and the reconstructed tracks as described in . The dataset is preprocessed to crop a subset of 1,000 ECAL-detected jets of 125x125 pixels to 16x16 pixels for faster inference.

## 3 Related Work

Multiple works have combined the classical DDM and quantum algorithms. The paper in  proposed a fully quantum DDPM (QuDDPM) for generating an unknown quantum state distribution. Their method consists of applying random unitaries to quantum states, thus scrambling them into noise, and summing three error functions to optimize during training. The QuDDPM performed best compared to a quantum GAN and a quantum direct transport model.

Another work in  proposed a hybrid quantum-classical DDM. The algorithm involved a quantum denoising U-Net and classical noising and optimization. Good results were achieved on the MNIST dataset. However, tests on MNIST are not generalizable to other data, and a physics-conscious approach is required for the more complex Quark-Gluon data.

## 4 Methodology

In this section, we outline the methods used to construct fully quantum, hybrid, and fully classical models. The pipeline in Figure 1 shows all the models and their combinations.

### Quantum Embedding

The first step of the pipeline is embedding classical data into quantum. We implemented angle encoding with \(Rx\) rotation gates in groups of four pixels. Each sample is split into four channels as shown in Figure 2.

### Forward Quantum Diffusion

The forward diffusion process was inspired by scrambling implementation in  using Haar random unitaries to mimic random noise application as shown in Figure 3. Since the choice of forward scrambling does not significantly impact model performance, as the authors in  have shown, an arbitrary noising transformation can be used. We chose to use the Haar measure for the quantum model as it allows for the unitary matrix transforms to scale with increasing resolution and dataset size. The final unitary is applied to each encoded channel, avoiding costly calculations associated with each timestep.

### Denoising Quantum U-Net

Similar to classical DDMs, the denoising neural network is trained on learned parameters with the MSE loss function. The hybrid model uses a quantum strongly entangling layer surrounded by fully convolutional layers, and the fully quantum model relies only on the quantum layers. The circuit in the quantum layer, consisting of rotation and strongly entangling gates, is kept uniform across all models, with the number of layers treated as a tunable parameter.

Figure 1: The pipeline that the data goes through with all the possible classical-quantum combinations of the forward and backward diffusion process.

Figure 2: A sample of an encoded jet image.

## 5 Experiments

In each experiment, we compare the performance using the Frechet Inception Distance (FID) function, originally introduced in  for evaluating GANs, along with the loss function. The FID is defined as:

\[(x,g)=\|_{x}-_{g}\|_{2}^{2}+(_{x}+_{g}-2( _{x}_{g})^{1/2}),\]

where \(_{x}\) and \(_{g}\) are the mean feature vectors of the real and generated images, respectively, and \(_{x}\) and \(_{g}\) are their corresponding covariance matrices. Though we do not currently have access to quantum computers, we can simulate the behavior of quantum systems using simulators like Pennylane  and compare the results to classical models.

First, we present the loss and FID functions of the classical, hybrid, and quantum models, respectively, in Figure 4. Training on 50 epochs seems to be sufficient to reach convergence for all models.

Figure 4: The losses and FID graphs of fully classical (a), hybrid (b), and fully quantum (c) models. For all models, MSE and Adam optimizer was used to reach convergence, and the FID function remained the same.

Figure 3: Haar noise applied to one encoded sample of four channels.

Some qualitative results in the form of generated samples are visualized in Figure 5. The generated images correspond to the best-performing model, which is the fully quantum architecture.

## 6 Discussion

All models exhibited a significant decrease in loss, approaching near-zero values, which confirms effective learning. The FID scores followed a similar downward trajectory, with final values of 1.8169, 1.8123, and 2.7362. These results show that the performance of the models that leverage quantum circuits is comparable in relation to a structurally similar classical model. This implies that all or some parts of deep neural network computations can be offloaded to faster quantum processors to reduce training time and without a performance trade-off.

A potential reason for the plateau in FID scores across all models could be the sparsity of the data, where each sample contains only a few non-zero points. This sparsity may prevent the complete removal of noise in some of the encoded channels. A possible solution is a post-processing step where only the most prominent values are retained in the decoded data, increasing the confidence in jet locations. As a result, the generated samples would align more closely with the originals, as shown in Figure 6.

## 7 Conclusion and Future Direction

This work marks significant progress in leveraging quantum computing for machine learning applications, specifically for a generative DDM. In the future, our aim is to extend the models to generate all three jet channels for every sample. Additionally, experimenting with different parametric circuits and

Figure 5: Samples generated from random noise with the four encoded channels on the left four rows and the decoded image on the far right for each sample. Subfigure (a) uses the hybrid model and (b) uses the fully quantum model.

forward scrambling, such as using Gaussian transformations instead of Haar unitaries, may provide further insight into the impact that the architecture has on quantum generative learning. Finally, testing the quantum model on hardware with a limited number of qubits under practical constraints will be crucial in evaluating its scalability, real-world performance, and resilience to quantum noise.

## 8 Data Availability

The code is open source and can be found here: https://github.com/mashathepotato/GSoC-Quantum-Diffusion-Model.