# Mini-batch kernel \(k\)-means

Anonymous

###### Abstract

We present the first mini-batch kernel \(k\)-means algorithm. Our algorithm achieves an order of magnitude improvement in running time compared to the full batch algorithm, with only a minor negative effect on the quality of the solution. Specifically, a single iteration of our algorithm requires only \(O(n(k+b))\) time, compared to \(O(n^{2})\) for the full batch kernel \(k\)-means, where \(n\) is the size of the dataset and \(b\) is the batch size.

We provide a theoretical analysis for our algorithm with an early stopping condition and show that if the batch is of size \(((/)^{2}(n/))\), the algorithm must terminate within \(O(^{2}/)\) iterations with high probability, where \(\) is the bound on the norm of points in the dataset in feature space, and \(\) is a threshold parameter for termination. Our results hold for any reasonable initialization of centers. When the algorithm is initialized with the \(k\)-means++ initialization scheme, it achieves an approximation ratio of \(O( k)\).

Many popular kernels are _normalized_ (e.g., Gaussian, Laplacian), which implies \(=1\). For these kernels, taking \(\) to be a constant and \(b=( n)\), our algorithm terminates within \(O(1)\) iterations where each iteration takes time \(O(n( n+k))\).

## 1 Introduction

Mini-batch methods are among the most successful tools for handling huge datasets for machine learning. Notable examples include Stochastic Gradient Descent (SGD) and mini-batch \(k\)-means . Mini-batch \(k\)-means  is one of the most popular clustering algorithms used in practice .

While \(k\)-means is widely used due to it's simplicity and fast running time, it requires the data to be _linearly separable_ to achieve meaningful clustering. Unfortunately, many real-world datasets do not have this property. One way to overcome this problem is to project the data into a high, even _infinite_, dimensional space (where it is hopefully linearly separable) and run \(k\)-means on the projected data using the "kernel-trick".

Kernel \(k\)-means achieves significantly better clustering compared to \(k\)-means in practice. However, its running time is considerably slower. Surprisingly, prior to our work there was no attempt to speed up kernel \(k\)-means using a mini-batch approach.

Problem statementWe are given an input (dataset), \(X=\{x_{i}\}_{i=1}^{n}\), of size \(n\) and a parameter \(k\) representing the number of clusters. A kernel for \(X\) is a function \(K:X X\) that can be realized by inner products. That is, there exists a Hilbert space \(\) and a map \(:X\) such that \( x,y X,(x),(y)=K(x,y)\). We call \(\) the _feature space_ and \(\) the _feature map_.

In kernel \(k\)-means the input is a dataset \(X\) and a kernel function \(K\) as above. Our goal is to find a set \(\) of \(k\) centers (elements in \(\)) such that the following goal function is minimized:

\[_{x X}_{c}\|c-(x)\|^{2}.\]Equivalently we may ask for a partition of \(X\) into \(k\) parts, keeping \(\) implicit.1

Lloyd's algorithmThe most popular algorithm for (non kernel) \(k\)-means is Lloyd's algorithm, often referred to as the \(k\)-means algorithm . It works by randomly initializing a set of \(k\) centers and performing the following two steps: (1) Assign every point in \(X\) to the center closest to it. (2) Update every center to be the mean of the points assigned to it. The algorithm terminates when no point is reassigned to a new center. This algorithm is extremely fast in practice but has a worst-case exponential running time .

Mini-batch \(k\)-meansTo update the centers, Lloyd's algorithm must go over the entire input at every iteration. This can be computationally expensive when the input data is extremely large. To tackle this, the mini-batch \(k\)-means method was introduced by . It is similar to Lloyd's algorithm except that steps (1) and (2) are performed on a batch of \(b\) elements sampled uniformly at random with repetitions, and in step (2) the centers are updated slightly differently. Specifically, every center is updated to be the weighted average of its current value and the mean of the points (in the batch) assigned to it. The parameter by which we weigh these values is called the _learning rate_, and its value differs between centers and iterations. The larger the learning rate, the more a center will drift towards the new batch cluster mean.

Lloyd's algorithm in feature spaceImplementing Lloyd's algorithm in feature space is challenging as we cannot explicitly keep the set of centers \(\). Luckily, we can use the kernel function together with the fact that centers are always set to be the mean of cluster points to compute the distance from any point \(x X\) in feature space to any center \(c=_{y A}(y)\) as follows:

\[\|(x)-c\|^{2}=(x)-c,(x)-c=(x),(x)-2(x),c+ c,c\] \[=(x),(x)-2(x), _{y A}(y)+_{y A}(y),_{y A}(y),\]

where \(A\) can be any subset of the input \(X\). While the above can be computed using only kernel evaluations, it makes the update step significantly more costly than standard \(k\)-means. Specifically, the complexity of the above may be quadratic in \(n\).

Mini-batch kernel \(k\)-meansApplying the mini-batch approach for kernel \(k\)-means is even more difficult because the assumption that cluster centers are always the mean of some subset of \(X\) in feature space no longer holds.

In Section 4 we present our algorithm and derive a recursive expression that allows us to compute the distances of all points to current cluster centers (in feature space). Our algorithm implements this by updating a data structure that maintains the inner products between the data and centers in feature space. This means the running time of each iteration of our algorithm is only \(O(n(b+k))\) compared to \(O(n^{2})\) for the full-batch algorithm.

In Section 5 we go on to provide theoretical guarantees for our algorithm. This is somewhat tricky for mini-batch algorithms due to their stochastic nature, as they may not even converge to a local-minima. To overcome this hurdle, we take the approach of  and answer the question: how long does it take mini-batch kernel \(k\)-means to terminate with an _early stopping condition_. Specifically, we terminate the algorithm when the improvement on the batch drops below some user provided parameter, \(\). Early stopping conditions are very common in practice (e.g., sklearn).

Applying the \(k\)-means++ initialization scheme for our initial centers implies we achieve the same approximation ratio, \(O( k)\) in expectation, as the full-batch algorithm. The approximation guarantee of \(k\)-means++ is guaranteed already in the initialization phase (Theorem 3.1 in ), and the execution of Lloyd's algorithm following initialization can only improve the solution. We show that w.h.p2 the global goal function is decreasing throughout our execution which implies that the approximation guarantee remains the same.

While our general approach is similar to , we must deal with the fact that \(\) may have an _infinite_ dimension. The guarantees of  depend on the dimension of the space in which \(k\)-means is executed, which is unacceptable in our case. We overcome this by parameterizing our results by a new parameter \(=_{x X}\|(x)\|\). We note that for normalized kernels, such as the popular Gaussian and Laplacian kernels, it holds that \(=1\). We show that if the batch size is \(((/)^{2}(n/))\) then w.h.p. our algorithm terminates in \(O(^{2}/)\) iterations. Our theoretical results are summarised in Theorem 1 (where Algorithm 1 is presented in Section 4).

**Theorem 1**.: _The following holds for Algorithm 1:_

1. _Each iteration takes_ \(O(n(b+k))\) _time,_
2. _If_ \(b=((/)^{2}(n/))\) _then it terminates in_ \(O(^{2}/)\) _iterations with high probability,_
3. _When initialized with k-means++ it achieve a_ \(O( k)\) _approximation ratio in expectation._

Our result improves upon  significantly when a normalized kernel is used since Theorem 1 doesn't depend on the input dimension. Our algorithm copes better with non linearly separable data and requires a smaller batch size (\((1/^{2})\) vs \(((d/)^{2})\))3 for normalized kernels. This is particularly apparent with high dimensional datasets such as MNIST where the dimension squared is already nearly ten times the number of datapoints.

The learning rate we use, suggested in , differs from the standard learning rate of sklearn in that it does not go to 0 over time. Unfortunately, this new learning rate is non-standard and  did not present experiments comparing their learning rate to that of sklearn.

In Section 6 we evaluate our results experimentally both with the learning rate of  and that of sklearn. We also fill the experimental gap left in  by evaluating (non-kernel) mini-batch \(k\)-means with their new learning rate compared to that of sklearn. To allow a fair empirical comparison, we run each algorithm for a fixed number of iterations without stopping conditions. Our results are as follows:

* Mini-batch kernel \(k\)-means is significantly faster than full-batch kernel \(k\)-means, while achieving solutions of similar quality, which are superior to the non-kernel version.
* The learning rate of  results in solutions with better quality both for mini-batch kernel \(k\)-means and mini-batch \(k\)-means.

## 2 Related work

Until recently, mini-batch \(k\)-means was only considered with a learning rate going to 0 over time. This was true both in theory [29; 27] and practice . Recently,  proposed a new learning which does not go to 0 over time, and showed that if the batch is of size \(((d/)^{2})\), mini-batch \(k\)-means must terminate within \(O(d/)\) iterations with high probability, where \(d\) is the dimension of the input, and \(\) is a threshold parameter for termination.

A popular approach to deal with the slow running time of kernel \(k\)-means is constructing a _coreset_ of the data. A coreset for kernel \(k\)-means is a weighted subset of \(X\) with the guarantee that the solution quality on the coreset is close to that on the entire dataset up to a \((1+)\) multiplicative factor. There has been a long line of work on coresets for \(k\)-means an kernel k-means [25; 10; 5], and the current state-of-the-art for kernel k-means is due to . They present a coreset algorithm with a nearly linear (in \(n\) and \(k\)) construction time which outputs a coreset of size \(poly(k^{-1})\).

In  the authors only compute the kernel matrix for uniformly sampled set of \(m\) points from \(X\). Then they optimize a variant of kernel \(k\)-means where the centers are constrained to be linear combinations of the sampled points. The authors do no provide worst case guarantees for the running time or approximation of their algorithm.

Another approach to speed up kernel \(k\)-means is by computing an approximation for the kernel matrix. This can be done by computing a low dimensional approximation for \(\) (without computing \(\) explicitly)[23; 8; 6], or by computing a low rank approximation for the kernel matrix [19; 31].

Kernel sparsification techniques construct sparse approximations of the full kernel matrix in subquadratic time. For smooth kernel functions such as the polynomial kernel,  presents an algorithm for constructing a \((1+)\)-spectral sparsifier for the full kernel matrix with a nearly linear number of non-zero entries in nearly linear time. For the gaussian kernel,  show how to construct a weaker, cluster preserving sparsifier using a nearly linear number of kernel density estimation queries.

We note that our results are _complementary_ to coresets, dimensionality reduction, and kernel sparsification, in the sense that we can compose our method with these techniques.

## 3 Preliminaries

Throughout this paper we work with ordered tuples rather than sets, denoted as \(Y=(y_{i})_{i[]}\), where \([]=\{1,,\}\). To reference the \(i\)-th element we either write \(y_{i}\) or \(Y[i]\). It will be useful to use set notations for tuples such as \(x Y i[],x=y_{i}\) and \(Y Z i[],y_{i} Z\). When summing we often write \(_{x Y}g(x)\) which is equivalent to \(_{i=1}^{}g(Y[i])\).

We borrow the following notation from  and generalize it to Hilbert spaces. For every \(x,y\) let \((x,y)=\|x-y\|^{2}\). We slightly abuse notation and and also write \((x,y)=\|(x)-(y)\|^{2}\) when \(x,y X\) and \((x,y)=\|(x)-y\|^{2}\) when \(x X,y\) (similarly when \(x,y X\)). For every finite tuple \(S X\) and a vector \(x\) let \((S,x)=_{y S}(y,x)\). Let us denote \(=_{x X}\|(x)\|\). Let us define for any finite tuple \(S X\) the center of mass of the tuple as \(cm(S)=_{x S}(x)\).

We now state the kernel \(k\)-means problem using the above notation.

Kernel \(k\)-meansWe are given an input \(X=(x_{i})_{i=1}^{n}\) and a parameter \(k\). Our goal is to (implicitly) find a tuple \(\) of \(k\) centers such that the following goal function is minimized: \(_{x X}_{C}(x,C)\).

Let us define for every \(x X\) the function \(f_{x}:^{k}\) where \(f_{x}()=_{C}(x,C)\). We can treat \(^{k}\) as the set of \(k\)-tuples of vectors in \(\). We also define the following function for every tuple \(A=(a_{i})_{i=1}^{} X\): \(f_{A}()=_{i=1}^{}f_{a_{i}}()\). Note that \(f_{X}\) is our original goal function.

We extensive use of the notion of _convex combination_:

**Definition 2**.: _We say that \(y\) is a convex combination of \(X\) if \(y=_{x X}p_{x}(x)\), such that \( x X,p_{x} 0\) and \(_{x X}p_{x}=1\)._

## 4 Our Algorithm

We present our pseudo-code as Algorithm 1. It requires an initial set of cluster centers such that every center is a convex combination of \(X\). This guarantees that all subsequent centers are also a convex combination of \(X\). Note that if we initialize the centers using the kernel version of \(k\)-means++, this is indeed the case.

Algorithm 1 proceeds by repeatedly sampling a batch of size \(b\) (the batch size is a parameter). For the \(i\)-th batch the algorithm (implicitly) updates the centers using the learning rate \(_{j}^{i}\) for center \(j\). Note that the learning rate may take on different values for different centers, and may change between iterations. Finally, the algorithm terminates when the progress on the batch is below \(\), a user provided parameter. While our termination guarantees (Section 5) require a specific learning rate, it does not affect the running time of a single iteration, and we leave it as a parameter for now.

Recursive distance update ruleWhile for (non kernel) \(k\)-means the center updates and assignment of points to clusters is straightforward, this is tricky for kernel \(k\)-means and even harder for mini-batch kernel \(k\)-means. Specifically, how do we overcome the challenge that we do not maintain the centers explicitly?

To assign points to centers in the \((i+1)\)-th iteration, it is sufficient to know \(\|(x)-_{i+1}^{j}\|^{2}\) for every \(j\). If we can keep track of this quantity through the execution of the algorithm, we are done.

Let us derive a _recursive_ expression for the distances

\[\|(x)-^{j}_{i+1}\|^{2}=(x),(x)-2 (x),^{j}_{i+1}+^{j}_{i+1},^{j}_{ i+1}.\]

We first expand \((x),^{j}_{i+1}\),

\[(x),^{j}_{i+1}=(x),(1-^{j}_{i} )^{j}_{i}+^{j}_{i}cm(B^{j}_{i})=(1-^{j}_{i}) (x),^{j}_{i}+^{j}_{i}(x),cm(B^{j }_{i}).\]

Then we expand \(^{j}_{i+1},^{j}_{i+1}\),

\[^{j}_{i+1},^{j}_{i+1} =(1-^{j}_{i})^{j}_{i}+^{j}_{i}cm(B ^{j}_{i}),(1-^{j}_{i})^{j}_{i}+^{j}_{i}cm(B^{j}_{i})\] \[=(1-^{j}_{i})^{2}^{j}_{i},^{j }_{i}+2^{j}_{i}(1-^{j}_{i})^{j}_{i},cm(B^ {j}_{i})+(^{j}_{i})^{2} cm(B^{j}_{i}),cm(B^{j}_{i}).\]

Assuming that \(^{j}_{i},^{j}_{i}\) and \((x),^{j}_{i}\) are known for all \(j[k]\) and for all \(x X\), we can compute \(^{j}_{i+1},^{j}_{i+1}\) and \((x),^{j}_{i+1}\) for all \(j[k]\) and \(x X\), which implies we can compute the distances from any point in the batch to all centers.

We now bound the running time of a single iteration of the outer loop in Algorithm 1. Let us denote \(b^{j}_{i}=|B^{j}_{i}|\) and recall that \(cm(B^{j}_{i})=_{i}}_{y B^{j}_{i}}(y)\). Therefore, computing \((x),cm(B^{j}_{i})=_{i}}_{y B^{j}_{i}} (x),(y)\) requires \(O(b^{j}_{i})\) time. Similarly, computing \( cm(B^{j}_{i}),cm(B^{j}_{i})\) requires \(O((b^{j}_{i})^{2})\) time. Let us now bound the time it requires to compute \((x),^{j}_{i+1}\) and \(^{j}_{i+1},^{j}_{i+1}\).

Assuming we know \((x),^{j}_{i}\) and \(^{j}_{i},^{j}_{i}\), updating \((x),^{j}_{i+1}\) for all \(x X,j[k]\) requires \(O(n(b+k))\) time. Specifically, the \((x),^{j}_{i}\) term is already known from the previous iteration and we need to compute \(^{j}_{i}(x),cm(B^{j}_{i})\) for every \(x X,j[k]\) which requires \(n_{j[k]}b^{j}_{i}=nb\) time. Finally, updating \((x),^{j}_{i+1}\) for all \(x X,j[k]\) requires \(O(nk)\) time.

Updating \(^{j}_{i+1},^{j}_{i+1}\) requires \(O(b^{2}+kb)\) time. Specifically, \(^{j}_{i},^{j}_{i}\) is known from the previous iteration and computing \( cm(B^{j}_{i}),cm(B^{j}_{i})\) for all \(j[k]\) requires \(O(_{j[k]}(b^{j}_{i})^{2})=O(b^{2})\) time. Computing \(^{j}_{i},cm(B^{j}_{i})\) for all \(j[k]\) requires time \(O(b)\) using \((x),^{j}_{i}\) from the previous iteration. Therefore, the total running time of the update step (assigning points to new centers) is \(O(n(b+k))\). To perform the update at the \((i+1)\)-th step we only need \((x),^{j}_{i},^{j}_{i},^{j}_{i}\), which results in a space complexity of \(O(nk)\). This completes the first claim of Theorem 1.

## 5 Termination guarantee

Section preliminariesWe introduce the following definitions and lemmas to aid our proof of the second claim of Theorem 1.

**Lemma 3**.: _For every \(y\) which is a convex combination of \(X\) it holds that \(\|y\|\)._Proof.: The proof follows by a simple application of the triangle inequality:

\[\|y\|=\|_{x X}p_{x}(x)\|_{x X}\|p_{x}(x)\|=_{x X }p_{x}\|(x)\|_{x X}p_{x}=.\]

**Lemma 4**.: _For any tuple of \(k\) centers \(^{d}\) which are a convex combination of points in \(X\), it holds that \( A X,f_{A}() 4^{2}\)._

Proof.: It is sufficient to upper bound \(f_{x}\). Combining that fact that every \(C\) is a convex combination of \(X\) with the triangle inequality, we have that

\[ x X,f_{x}()_{C} (x,C)=(x,_{y X}p_{y}(y))\] \[=\|(x)-_{y X}p_{y}(y)\|^{2}(\|(x)\|+\|_ {y X}p_{y}(y)\|)^{2} 4^{2}.\]

We state the following simplified version of an Azuma bound for Hilbert space valued martingales from , followed by a standard Hoeffding bound.

**Theorem 5** ().: _Let \(\) be a Hilbert space and let \(Y_{0},...,Y_{m}\) be a \(\)-valued martingale, such that \( 1 i m,\|Y_{i}-Y_{i-1}\| a_{i}\). It holds that \(Pr[\|Y_{m}-Y_{0}\|] e^{(}{_{i=1} ^{m}a_{i}^{2}})}\)._

**Theorem 6** ().: _Let \(Y_{1},...,Y_{m}\) be independent random variables such that \( 1 i m,E[Y_{i}]=\) and \(Y_{i}[a_{min},a_{max}]\). Then_

\[Pr(|_{i=1}^{m}Y_{k}-|) 2 e^{-2m^{2}/(a_{max}-a_{min})^{2}}.\]

The following lemma provide concentration guarantees when sampling a _batch_.

**Lemma 7**.: _Let \(B\) be a tuple of \(b\) elements chosen uniformly at random from \(X\) with repetitions. For any fixed tuple of \(k\) centers, \(\) which are a convex combination of \(X\), it holds that: \(Pr[|f_{B}()-f_{X}()|] 2e^{-b^{2}/2 ^{2}}\)._

Proof.: Let us write \(B=(y_{1},,y_{b})\), where \(y_{i}\) is a random element selected uniformly at random from \(X\) with repetitions. For every such \(y_{i}\) define the random variable \(Z_{i}=f_{y_{i}}()\). These new random variables are IID for any fixed \(\). It also holds that \( i[b],E[Z_{i}]=_{x X}f_{x}()=f_{X}( )\) and that \(f_{B}()=_{x B}f_{x}()= _{i=1}^{b}Z_{i}\).

Applying the Hoeffding bound (Theorem 6) with parameters \(m=b,=f_{X}(),a_{max}-a_{min} 4^{2}\) (due to Lemma 4) we get that: \(Pr[|f_{B}()-f_{X}()|] 2e^{-b^{2}/2 ^{2}}\). 

For any tuple \(S X\) and some tuple of cluster centers \(=(^{})_{[k]}\), \(\) implies a _partition_\((S^{})_{[k]}\) of the points in \(S\). Specifically, every \(S^{}\) contains the points in \(S\) closest to \(^{}\) (in \(\)) and every point in \(S\) belongs to a single \(^{}\) (ties are broken arbitrarily). We state the following useful observation:

**Observation 8**.: _Fix some \(A X\). Let \(\) be a tuple of \(k\) centers, \(S=(S^{})_{[k]}\) be the partition of \(A\) induced by \(\) and \(=(^{})_{[k]}\) be any other partition of \(A\). It holds that \(_{j=1}^{k}(S^{j},^{j})_{j=1}^{k}(^{j},^{j})\)._

Recall that \(^{j}_{i}\) is the \(j\)-th center in the beginning of the \(i\)-th iteration of Algorithm 1 and \((B^{}_{i})_{[k]}\) is the partition of \(B_{i}\) induced by \(_{i}\). Let \((X^{}_{i})_{[k]}\) be the partition of \(X\) induced by \(_{i}\).

We now have the tools to analyze Algorithm 1 with the learning rate of . Specifically, we assume that the algorithm executes for at least \(t\) iterations, the learning rate is \(^{j}_{i}=_{i}/b}\), where \(b^{j}_{i}=|B^{j}_{i}|\), and the batch size is \(b=((/)^{2}(nt))\). We show that the algorithm must terminate within \(t=O(/)\) steps w.h.p. Plugging \(t\) back into \(b\), we get that a batch size of \(b=((/)^{2}(n/))\) is sufficient.

Proof outlineWe note that when sampling a batch it holds w.h.p that \(f_{B_{i}}(_{i})\) is close to \(f_{X_{i}}(_{i})\) (Lemma 7). This is due to the fact that \(B_{i}\) is sampled after \(_{i}\) is fixed. If we could show that \(f_{B_{i}}(_{i+1})\) is close \(f_{X_{i}}(_{i+1})\) then combined with the fact that we make progress of at least \(\) on the batch we can conclude that we make progress of at least some constant fraction of \(\) on the entire dataset.

Unfortunately, as \(C_{i+1}\) depends on \(B_{i}\), getting the above guarantee is tricky. To overcome this issue we define the auxiliary value \(}_{i+1}^{j}=(1-_{i}^{j})_{i}^{j}+ _{i}^{j}cm(X_{j}^{j})\). This is the \(j\)-th center at step \(i+1\) if we were to use the entire dataset for the update, rather than just a batch. Note that this is only used in the analysis and not in the algorithm. Note that \(}_{i+1}\) only depends on \(_{i}\) and \(X\) and is independent of \(B_{i}\) (i.e., we can fix its value before sampling \(B_{i}\)). As \(}_{i+1}\) does not depend on \(B_{i}\) we use \(}_{i+1}\) instead of \(_{i+1}\) in the above analysis outline. We show that for our choice of learning rate it holds that \(}_{i+1},_{i+1}\) are sufficiently close, which implies that \(f_{X}(_{i+1}),f_{X}(}_{i+1})\) and \(f_{B_{i}}(_{i+1})\), \(f_{B_{i}}(}_{i+1})\) are also sufficiently close. That is, \(}_{i+1}\) acts as a proxy for \(_{i+1}\). Combining everything together we get our desired result.

We start with the following useful observation, which will allow us to use Lemma 3 to bound the norm of the centers by \(\) throughout the execution of the algorithm.

**Observation 9**.: _If \( j[k],_{1}^{j}\) is a convex combination of \(X\) then \( i>1,j[k],_{i}^{j},}_{i}^{j}\) are also a convex combinations of \(X\)._

Let us state the following useful lemma from . Although their proof is for Euclidean spaces, it goes through for Hilbert spaces. We provide the proof in the appendix for completeness.

**Lemma 10** ().: _For any set \(S X\) and any \(C\) it holds that \((S,C)=(S,cm(S))+|S|\,(C,cm(S))\)._

We use the above to prove the following useful lemma.

**Lemma 11**.: _For any \(S X\) and \(C,C^{}\) which are convex combinations of \(X\), it holds that: \(|(S,C^{})-(S,C)| 2\,|S|\,\|C-C^{}\|\)._

Proof.: Using Lemma 10 we get that \((S,C)=(S,cm(S))+|S|\,(cm(S),C)\) and that \((S,C^{})=(S,cm(S))+|S|\,(cm(S),C^{})\). Thus, it holds that \(|(S,C^{})-(S,C)|=|S||(cm(S),C^{})-(cm (S),C)|\). Let us write

\[|(cm(S),C^{})-(cm(S),C)|\] \[=| cm(S)-C^{},cm(S)-C^{}- cm(S) -C,cm(S)-C|\] \[=|-2 cm(S),C^{}+ C^{},C^{ }+2 cm(S),C- C,C|\] \[=|2 cm(S),C-C^{}+ C^{}-C,C^{ }+C|\] \[=| C-C^{},2cm(S)-(C^{}+C)|\] \[\|C-C^{}\|\|2cm(S)-(C^{}+C)\| 4\|C-C^{ }\|.\]

Where in the last transition we used the Cauchy-Schwartz inequality, the triangle inequality, and the fact that \(C,C^{},cm(S)\) are convex combinations of \(X\) and therefore their norm is bounded by \(\). 

Now we show that due to our choice of learning rate, \(_{i+1}^{j}\) and \(}_{i+1}^{j}\) are sufficiently close.

**Lemma 12**.: _It holds w.h.p that \( i[t],j[k],\|_{i+1}^{j}-}_{i+1}^{j} \|\)._

Proof.: Note that \(_{i+1}^{j}-}_{i+1}^{j}=_{i}^{j}(cm(B_{i} ^{j})-cm(X_{i}^{j}))\). Let us fix some iteration \(i\) and center \(j\). To simplify notation, let us denote: \(X^{}=X_{i}^{j},B^{}=B_{i}^{j},b^{}=b_{i}^{j},^{}= _{i}^{j}\). Although \(b^{}\) is a random variable, in what follows we treat it as a fixed value (essentially conditioning on its value). As what follows holds for _all_ values of \(b^{}\) it also holds without conditioning due to the law of total probabilities.

For the rest of the proof, we assume \(b^{}>0\) (if \(b^{}=0\) the claim holds trivially). Let us denote by \(\{Y_{}\}_{=1}^{b^{}}\) the sampled points in \(B^{}\). Note that a randomly sampled element from \(X\) is in \(B^{}\) if and only if it is in \(X^{}\). As batch elements are sampled uniformly at random with repetitions from \(X\)conditioning on the fact that an element is in \(B^{}\) means that it is distributed uniformly over \(X^{}\). Note that \(,E[(Y_{})]=|}_{x X^{}}(x) =cm(X^{})\) and \(E[cm(B^{})]=}_{=1}^{b^{}}E[(Y_{ })]=cm(X^{})\).

Let us define the following martingale: \(Z_{r}=_{=1}^{r}((Y_{})-E[(Y_{})])\). Note that \(Z_{0}=0\), and when \(r>0\), \(Z_{r}=_{=1}^{r}(Y_{})-r cm(X^{})\). It is easy to see that this is a martingale:

\[E[Z_{r} Z_{r-1}]=E[_{=1}^{r}(Y_{})-r cm(X^{})  Z_{r-1}]=Z_{r-1}+E[(Y_{r})-cm(X^{}) Z_{r-1}]=Z_{r-1}.\]

Let us now bound the differences

\[\|Z_{r}-Z_{r-1}\|=\|(Y_{r})-cm(X^{})\|\|(Y_{r})\|+\|cm(X^{ })\| 2.\]

Now we may use Azuma's inequality: \(Pr[\|Z_{b^{}}-Z_{0}\|] e^{(}{ 2b^{ }})}\). Let us now divide both sides of the inequality by \(b^{}\) and set \(=}{20^{}}\). We get

\[Pr[\|cm(B^{})-cm(X^{})\|}]=Pr[\|}_{=1}^{b^{}}(Y_{})- cm(X^{})\|}] e^{(^{2}}{(^{})^{2}})}.\]

Using the fact that \(^{}=/b}\) together with the fact that \(b=((/)^{2}(nt))\) (for an appropriate constant) we get that the above is \(O(1/ntk)\). Finally, taking a union bound over all \(t\) iterations and all \(k\) centers per iteration completes the proof. 

We can now bound the goal function when cluster centers are close using Lemma 12.

**Lemma 13**.: _Fix some \(A X\). It holds w.h.p that \( i[t],|f_{A}(}_{i+1})-f_{A}(_ {i+1})|/5\)._

Proof.: Let \(S=(S^{})_{[k]},=(^{})_{[k]}\) be the partitions induced by \(_{i+1},}_{i+1}\) on \(A\). Let us expand the expression

\[f_{A}(}_{i+1})-f_{A}(_{i+1})= _{j=1}^{k}(^{j},}_{i+ 1}^{j})-(S^{j},_{i+1}^{j})\] \[_{j=1}^{k}4|S^{j}|\| }_{i+1}^{j}-_{i+1}^{j}\| _{j=1}^{k}|S^{j}|/5=/5.\]

Where the first inequality is due to Observation 8, the second is due Lemma 11 and finally we use Lemma 12 together with the fact that \(_{j=1}^{k}|S^{j}|=|A|\). Using the same argument we also get that \(f_{A}(_{i+1})-f_{A}(}_{i+1})/5\), which completes the proof. 

Let us state the following useful lemma.

**Lemma 14**.: _It holds w.h.p that for every \(i[t]\),_

\[f_{X}(}_{i+1})-f_{X}(_{i+1})- /5,\] (1) \[f_{B_{i}}(_{i+1})-f_{B_{i}}(}_{i +1})-/5,\] (2) \[f_{X}(_{i})-f_{B_{i}}(_{i})-/5,\] (3) \[f_{B_{i}}(}_{i+1})-f_{X}(}_{i+1})-/5.\] (4)

Proof.: The first two inequalities follow from Lemma 13. The last two are due to Lemma 7 by setting \(=/5\), \(B=B_{i}\):

\[Pr[|f_{B_{i}}()-f_{X}()|] 2e^{-b^{2}/2 ^{2}}=e^{-(bc^{2}/^{2})}=e^{-((nt))}=O(1/nt).\]

Where the last inequality is due to the fact that \(b=((/)^{2}(nt))\) (for an appropriate constant). The above holds for either \(=_{i}\) or \(=}_{i+1}\). Taking a union bound over all \(t\) iterations we get the desired result.

Putting everything togetherWe wish to lower bound \(f_{X}(_{i})-f_{X}(_{i+1})\). We write the following, where the \(\) notation means we add and subtract a term:

\[f_{X}(_{i})-f_{X}(_{i+1})=f_{X}( _{i}) f_{B_{i}}(_{i})-f_{X}(_{i+1})\] \[ f_{B_{i}}(_{i})-f_{X}(_{i+1})- /5=f_{B_{i}}(_{i}) f_{B_{i}}(_{i+1})-f_{X}(_{i+1})-/5\] \[ f_{B_{i}}(_{i+1})-f_{X}(_{i+1})+4 /5\] \[=f_{B_{i}}(_{i+1}) f_{B_{i}}( }_{i+1}) f_{X}(}_{i+1})-f_{X}(_{i+1})+4 /5/5.\]

Where the first inequality is due to inequality (3) in Lemma 14 (\(f_{X}(_{i})-f_{B_{i}}(_{i})-/5\)), the second is due to the stopping condition of the algorithm (\(f_{B_{i}}(_{i})-f_{B_{i}}(_{i+1})>\)), and the last is due to the remaining inequalities in Lemma 14. The above holds w.h.p over all of the iterations of the algorithms. We conclude that when \(b=((/)^{2}( n/))\), w.h.p. the algorithm terminates within \(t=O(^{2}/)\) iterations. This complete the proof of the second claim of Theorem 1.

## 6 Experiments

We evaluate our mini-batch algorithm on the following datasets:

* **MNIST:** The MNIST dataset  has 70,000 grayscale images of handwritten digits (0 to 9), each image being 28x28 pixels. When flattened, this gives 784 features.
* **PenDigits:** The PenDigits dataset  has 10992 instances, each represented by an 16-dimensional vector derived from 2D pen movements. The dataset has 10 labelled clusters, one for each digit.
* **Letters:** The Letters dataset  has 20,000 instances of letters from 'A' to 'Z', each represented by 16 features. The dataset has 26 labelled clusters, one for each letter.
* **HAR:** The HAR dataset  has 10,299 instances collected from smartphone sensors, capturing human activities like walking, sitting, and standing. Each instance is described by 561 features. The dataset has 6 labelled clusters, corresponding to different types of physical activities.

We compare the following algorithms: (full batch) kernel k-means, mini-batch kernel k-means with the learning rate of , mini-batch kernel k-means with the learning rate of sklearn, mini-batch (non-kernel) k-means with the learning rate of , mini-batch (non-kernel) k-means with the learning rate of sklearn. We evaluate our results with batch sizes: 1024, 256, 64, and 16. We execute every algorithm for 200 iterations. For the kernel variants we apply the Gaussian kernel: \(K(x,y)=e^{-\|x-y\|^{2}/}\), where the \(\) parameter is set using the heuristic of  followed by some manual tuning (exact values appear in the supplementary materials). We repeat every experiment 10 times and present the average Adjusted Rand Index (ARI) [11; 24] and Normalized Mutual Information (NMI)  scores for every dataset. All experiments were conducted on a MacBook Pro equipped with an M2 Max chip and 96 GB of RAM. We present partial results in Figure 1 and the full results in the appendix. Error bars in the plot measure the standard deviation.