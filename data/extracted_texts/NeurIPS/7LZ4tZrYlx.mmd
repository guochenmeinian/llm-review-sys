# Bounding Training Data Reconstruction in DP-SGD

Jamie Hayes

Google DeepMind

jamhay@google.com &Saeed Mahloujifar1

Meta AI

saeedm@meta.com &Borja Balle

Google DeepMind

bballe@google.com

Equal contribution.Work done at Princeton University.

###### Abstract

Differentially private training offers a protection which is usually interpreted as a guarantee against membership inference attacks. By proxy, this guarantee extends to other threats like reconstruction attacks attempting to extract complete training examples. Recent works provide evidence that if one does not need to protect against membership attacks but _instead_ only wants to protect against training data reconstruction, then utility of private models can be improved because less noise is required to protect against these more ambitious attacks. We investigate this further in the context of DP-SGD, a standard algorithm for private deep learning, and provide an upper bound on the success of any reconstruction attack against DP-SGD together with an attack that empirically matches the predictions of our bound. Together, these two results open the door to fine-grained investigations on how to set the privacy parameters of DP-SGD in practice to protect against reconstruction attacks. Finally, we use our methods to demonstrate that different settings of the DP-SGD parameters leading to the same DP guarantees can result in significantly different success rates for reconstruction, indicating that the DP guarantee alone might not be a good proxy for controlling the protection against reconstruction attacks.

## 1 Introduction

Machine learning models can and do leak training data (Ippolito et al., 2022; Kandpal et al., 2022; Carlini et al., 2019; Yeom et al., 2018; Song and Shmatikov, 2019; Tirumala et al., 2022). If the training data contains private or sensitive information, this can lead to information leakage via a variety of different privacy attacks (Balle et al., 2022; Carlini et al., 2022; Fredrikson et al., 2015; Carlini et al., 2021). Perhaps the most commonly studied privacy attack, membership inference (Homer et al., 2008; Shokri et al., 2017), aims to infer if a sample was included in the training set, which can lead to a privacy violation if inclusion in the training set is in and of itself sensitive. Membership inference leaks a single bit of information about a sample - whether that sample was or was not in the training set - and so any mitigation against this attack also defends against attacks that aim to reconstruct more information about a sample, such as training data reconstruction attacks (Balle et al., 2022; Carlini et al., 2021; Zhu et al., 2019).

Differential privacy (DP) (Dwork et al., 2006) provides an effective mitigation that provably bounds the success of _any_ membership inference attack, and so consequently any training data reconstruction attack. The strength of this mitigation is controlled by a privacy parameter \(\) which, informally, represents the number of bits that can be leaked about a training data sample, and so \(\) must be small to guarantee the failure of a membership inference attack (Sablayrolles et al., 2019; Nasr et al., 2021). Unfortunately, training machine learning models with DP in the small \(\) regime usually produces models that perform significantly worse than their non-private counterpart (Tramer and Boneh, 2021; De et al., 2022).

Membership inference may not always be the privacy attack that is of most concern. For example, in social networks, participation is usually public; recovering privately shared photos or messages from a model trained on social network data is the privacy violation. These kinds of attacks are referred to as training data reconstruction attacks, and have been successfully demonstrated against a number of machine learning models including language models (Carlini et al., 2021; Mireshshalhalh et al., 2022), generative models (Somengalli et al., 2022; Carlini et al., 2023), and image classifiers (Balle et al., 2022; Haim et al., 2022). Recent work (Bhowmick et al., 2018; Balle et al., 2022; Guo et al., 2022; Stock et al., 2022) has begun to provide evidence that if one is willing to forgo protection against membership inference, then the \(\) regime that protects against training data reconstruction is far larger, as predicted by the intuitive reasoning that successful reconstruction requires a significant number of bits about an individual example to be leaked by the model. This also has the benefit that models trained to protect against training data reconstruction _but not_ membership inference do not suffer as large a drop in performance, as less noise is added during training. Yet, the implications of choosing a concrete \(\) for a particular application remain unclear since the success of reconstruction attacks can vary greatly depending on the details of the threat model, the strength of the attack, and the criteria of what constitutes a successful reconstruction.

In this paper we re-visit the question of training data reconstruction against image classification models trained with DP-SGD (Song et al., 2013; Abadi et al., 2016), the workhorse of differentially private deep learning. We choose to concentrate our analysis on DP-SGD because state-of-the-art results are almost exclusively obtained with DP-SGD or other privatized optimizers (De et al., 2022; Cattan et al., 2022; Mehta et al., 2022). Our investigation focuses on attacks performed under a strong threat model where the adversary has access to intermediate gradients and knowledge of all the training data except the target of the reconstruction attacks. This threat model is consistent with the privacy analysis of DP-SGD (Abadi et al., 2016) and the informed adversary implicit in the definition of differential privacy (Nasr et al., 2021; Balle et al., 2022), and implies that conclusions about the impossibility of attacks in this model will transfer to weaker, more realistic threat models involving real-world attackers. Our investigation focuses on three main questions. 1) How do variations in the threat model (e.g. access to gradients and side knowledge available to the adversary) affect the success of reconstruction attacks? 2) Is it possible to obtain upper bounds on reconstruction success for DP-SGD that match the best known attacks and thus provide actionable insights into how to tune the privacy parameters in practice? 3) Does the standard DP parameter \(\) provide enough information to characterize vulnerability against reconstruction attacks?

Our contributions are summarized as follows:

* We illustrate how changes in the threat model for reconstruction attacks against image classification models can significantly influence their success by comparing attacks with access to the final model parameters, access to intermediate gradients, and access to prior information.
* We obtain a tight upper bound on the success of any reconstruction attack against DP-SGD with access to intermediate gradients and prior information. Tightness is shown by providing an attack whose reconstruction success closely matches our bound's predictions.
* We provide evidence that the DP parameter \(\) is not sufficient to capture the success of reconstruction attacks on DP-SGD by showing that different configurations of DP-SGD's hyperparameters leading to the same DP guarantee lead to different rates of reconstruction success.

## 2 Training Data Reconstruction in DP-SGD

We start by introducing DP-SGD, the algorithm we study throughout the paper, and then discuss reconstruction attacks with access to either only the final trained model, or all intermediate gradients. Then we empirically compare both attacks and show that gradient-based attacks are more powerful than model-based attacks, and we identify a significant gap between the success of the best attack and a known lower bound. As a result, we propose the problem of closing the gap between theoretical bounds and empirical reconstruction attacks.

Differential privacy and DP-SGD.Differential privacy (Dwork et al., 2006) formalizes the idea that data analysis algorithms whose output does not overly depend on any individual input data point can provide reasonable privacy protections. Formally, we say that a randomized mechanism \(M\) satisfies \((,)\)-differential privacy (DP) if, for any two datasets \(D,D^{}\) that differ by one point, and any subset \(S\) of the output space we have \(P[M(D) S] e^{}P[M(D^{}) S]+\). Informally, this means that DP mechanisms bound evidence an adversary can collect (after observing the output) about whether the point where \(D\) and \(D^{}\) differ was used in the analysis. For the bound to provide a meaningful protection against an adversary interested in this membership question it is necessary to take \( 1\) and \(}{{|D|}}\).

A differentially private version of stochastic gradient descent useful for training ML models can be obtained by bounding the influence of any individual sample in the trained model and masking it with noise. The resulting algorithm is called DP-SGD (Abadi et al., 2016) and proceeds by iteratively updating parameters with a privatized gradient descent step. Given a sampling probability \(q\), current model parameters \(\) and a loss function \((,)\), the privatized gradient \(g\) is obtained by first creating a mini-batch \(B\) including each point in the training dataset with probability \(q\), summing the \(L_{2}\)-clipped gradients3 for each point in \(B\), and adding Gaussian noise with standard deviation \( C\) to all coordinates of the gradient: \(g_{z B}_{C}(_{}(,z) )+(0,C^{2}^{2}I)\). Running DP-SGD for \(T\) training steps yields an \((,)\)-DP mechanism with \(}{}\)(Abadi et al., 2016) - in practice, tighter numerical bounds on \(\) are often used (Gopi et al., 2021; Google DP Team, 2022). For analytic purposes it is often useful to consider alternatives to \((,)\)-DP to capture the differences between distributions \(M(D)\) and \(M(D^{})\). Renyi differential privacy (RDP) (Mironov, 2017) is one such alternative often used in the context of DP-SGD. It states that the mechanism is \((,)\)-RDP for \(>1\) and \( 0\) if \(_{W M(D^{})}[([M(D)=W]}{ [M(D^{})=W]})^{}] e^{(-1)}\). In particular, \(T\) iterations of full-batch DP-SGD (i.e. \(q=1\)) with noise multiplier \(\) give \((,})\)-RDP for every \(>1\).

Ultimately, we are interested in understanding how the privacy guarantee of DP-SGD affects _reconstruction attacks_ and, in particular, whether \( 1\) still provides some protection against these more ambitious attacks. The first step is to understand what the most idoneous threat model to investigating this question is, and then to instantiate a powerful attack in that model.

### Comparing reconstruction attacks under intermediate and final model access

In the case of DP-SGD, the privacy guarantee for the final model is obtained by analyzing the privacy loss incurred by _releasing the \(T\) private gradients_ used by the algorithm. Thus, the guarantee applies both to the intermediate gradients and the final model (by virtue of the post-processing property of DP). It has been shown through membership inference attacks that the guarantee obtained through this analysis for the _collection of intermediate gradients_ can be numerically tight (Nasr et al., 2021). However, in some specific settings amenable to mathematical treatment, it has also been shown that the final model produced by DP-SGD can enjoy a stronger DP guarantee than the collection of all intermediate gradients (Ye and Shokri, 2022; Altschuler and Talwar, 2022).

Although these observations apply the standard membership formulation of DP, they motivate an important question for trying to understand the implications of DP guarantees for reconstruction attacks: _how does access to intermediate gradients affect the success of reconstruction attacks against DP-SGD?_ We investigate this question by introducing and comparing the success of _model-based_ and _gradient-based_ attacks. We assume the adversary receives the output of a DP mechanism \(M(D)\) and all the input data in \(D\) except for one point \(z^{*}\); the adversary's goal is then to produce a reconstruction \(\) of the unknown point. This adversary is referred to as the _informed adversary_ in Balle et al. (2022).

Model-based training data reconstruction.Suppose we consider the output of DP-SGD to be only the final model \(=M(D)\). Under the informed adversary threat model, Balle et al. (2022) propose a reconstruction attack in which the adversary uses their knowledge of \(M\) and \(D_{}=D\{z^{*}\}\) to train a _reconstruct neural network_ (RecoNN) capable of mapping model parameters \(\) to reconstructed points \(\). The training dataset for this network consists of model-point pairs \((^{i},z_{i})\) where the \(z_{i}\) are auxiliary points representing side knowledge the adversary might possess about the distribution of the target point \(z^{*}\), and the so-called shadow model \(^{i}\) is obtained by applying \(M\) to the dataset \(D_{}\{z_{i}\}\) obtained by replacing \(z^{*}\) by \(z_{i}\) in the original dataset. Despite its computational cost, this attack is effective in reconstructing complete training examples from image classification modelstrained _without_ DP on MNIST and CIFAR-10, but fails to produce correct reconstructions when the model is trained with any reasonable setting of DP parameters.

Gradient-based training data reconstruction.Suppose now that the adversary gets access to all the intermediate gradients \((g_{1},,g_{T})=M(D)\) produced by DP-SGD when training model \(\). We can instantiate a reconstruction attack for this scenario by leveraging gradient inversion attacks found in the federated learning literature (Yin et al., 2021; Huang et al., 2021; Jeon et al., 2021; Jin et al., 2021; Zhu et al., 2019; Geiping et al., 2020). In particular, we use the gradients produced by DP-SGD to construct a loss function \((z)\) for which the target point \(z^{*}\) would be optimal if the gradients contained no noise, and then optimize it to obtain our candidate reconstruction. An important difference between our attack and previous works in the federated learning setting is that we use gradients from multiple steps to perform the attack and only attempt to recover a single point.

More formally, we start by removing from each privatized gradient \(g_{t}\) the gradients of the known points in \(D\). For simplicity, in this section we only consider the full batch case (\(q=1\)) where every training point in \(D\) is included in every gradient step. Thus, we set \(_{t}=g_{t}-_{z D}_{C}(_{}(_{t}, z))\), where \(_{t}\) are the model parameters at step \(t\). Note these can be inferred from the gradients and knowledge of the model initialization, which we also assume is given to the adversary. Similar (but not identical) to Geiping et al. (2020), we use the loss \((z)=_{t=1}^{T}_{t}(z)\) with

\[_{t}(z)=-_{C}(_{}(_{t},z)),_{t}+\|_{C}(_{}(_{t},z))- _{t}\|_{1}.\] (1)

This loss was selected after exhaustively trying all other losses suggested in the aforementioned literature; empirically, Equation (1) performed best.

Comparing reconstruction attacks.We now compare the success of model-based and gradient-based reconstruction attacks against classification models trained with DP-SGD on MNIST and CIFAR-10. We refer to Appendix A for experimental details.

Following Balle et al. (2022), we report the mean squared error (MSE) between the reconstruction and target as a function of \(\) for both attacks. Results are shown in Figure 1, where we include two baselines to help calibrate how MSE corresponds to good and bad reconstructions. Firstly, we include a threshold representing the average distance between target points and their nearest neighbor in the remaining of the dataset. An MSE below this line indicates a near-perfect reconstruction. Secondly, we include a threshold representing the average distance between random uniform vectors of the same data dimensionality. An MSE above or close to this line indicates a poor reconstruction.

We make the following observations. First, the gradient-based attack outperforms the model-based attack by orders of magnitude at larger \(\) values. Second, the best attack starts to perform well on

Figure 1: Comparison of model-based and gradient-base reconstruction attacks. We run the attacks over 1K different images for both MNIST and CIFAR-10 for a range of \(\). In Figure 0(a) and Figure 0(b), we plot the average MSE between reconstruction and target images for both attacks. To help calibrate a reader’s interpretation of good and bad reconstructions we plot a nearest neighbor (NN) baseline, marking the average \(L_{2}\)-distance between NN images over the entire dataset, and a baseline corresponding to the average distance between random uniform vectors. We also plot a lower bound for MSE given by Guo et al. (2022). We give qualitative examples of reconstructions in Appendix B.

MNIST and CIFAR-10 between \(10^{2}<<10^{3}\) and \(10^{3}<<10^{4}\) respectively. This indicates that attack success is affected by the complexity of the underlying data, including its dimension and geometry. Finally, the attacks give an upper bound for reconstruction error (MSE), however it is not clear if this bound is tight (i.e., whether a better attack could reduce MSE). Guo et al. (2022) report a _lower bound_ for MSE of the form \(}{{4(^{}-1)}}\), which is very far from the upper bound.

The experiment above illustrates how a change in threat model can make a significant difference in the success of reconstruction attacks. In particular, it shows that the attack from Balle et al. (2022) is far from optimal, potentially because of its lack of information about intermediate gradients. Thus, while optimal attacks for membership inference are known (Nasr et al., 2021) and can be used to empirically evaluate the strength of DP guarantees (e.g. for auditing purposes (Tramer et al., 2022)), the situation is far less clear in the case of reconstruction attacks. Gradient-based attacks improve over model-based attacks, but _are they (near) optimal?_ Optimality of attacks is important for applications where one would like to calibrate the privacy parameters of DP-SGD to provide a demonstrable, pre-specified amount of protection against any reconstruction attacks.

## 3 Bounding Training Data Reconstruction

We will now formalize the reconstruction problem further and then provide bounds on the success probability of reconstruction attacks against DP-SGD. We will also develop improvements to the reconstruction attack introduced in Section 2 from which to benchmark the tightness of our bound.

### Threat models for reconstruction

Balle et al. (2022) introduce a formal definition for reconstruction that attempts to capture the success probability of any reconstruction attack against a given mechanism. The definition involves an informed adversary with knowledge of the mechanism \(M\), the fixed part of the training dataset \(D\)., and a prior distribution \(\) from which the target \(z^{*}\) is sampled - the prior encodes the adversary's side knowledge about the target.

**Definition 1**.: _Let \(\) by a prior over target points and \(\) a reconstruction error function. A randomized mechanism \(M\) is \((,)\)-ReRo (reconstruction robust) with respect to \(\) and \(\) if for any fixed dataset \(D\). and any reconstruction attack \(R\) we have \(_{Z,w M(D,\{Z\})}[(Z,R(w))]\)._

Note that the output of the mechanism does not necessarily need to be final model parameters - indeed, the definition also applies to attacks operating on intermediate gradients when those are included in the output of the mechanism. Balle et al. (2022) also proved that any \((,)\)-RDP mechanism is \((,)\)-ReRo with \(=(_{,}() e^{})^{}\), where \(_{,}()=_{z_{0}}_{Z}[( Z,z_{0})]\). In particular, using the RDP guarantees of DP-SGD, one obtains that in the full-batch case, running the algorithm for \(T\) iterations with noise multiplier \(\) is \((,)\)-ReRo with \(\) bounded by

\[(-\{0,()}}-}}\}^{2}).\] (2)

The quantity \(_{,}()\) can be thought of as the prior probability of reconstruction by an adversary who outputs a fixed candidate \(z_{0}\) based only on their knowledge of the prior (i.e. without observing the output of the mechanism).

Instantiating threat models within the \((,)\)-ReRo frameworkIn Section 2.1, we described two variants of a reconstruction attack, one where the adversary has access to intermediate model updates (gradient-based reconstruction) and one where the adversary has access to the final model only (model-based reconstruction). In the language of \((,)\)-ReRo, both the gradient-based and model-based reconstruction attacks introduced in Section 2 take as arguments: \(\) - prior information about the target point, \(D\). - the training dataset excluding the target point, \(w\) - the output of the mechanism \(M\), and side information about DP-SGD - such as hyperparameters used in training and how to the model was initialized. The model-based reconstruction attack assumes that \(w=_{T}\), the parameters of the final model, whereas for gradient-based attacks, \(w=(g_{1},,g_{T})\), and so the adversary has access to all intermediate privatized model parameter updates.

The gradient-based attack optimizing Equation (1) does not make use of any potential prior knowledge the adversary might have about the target point \(z^{*}\), beyond input bounds (the input is bound between ) and the dimensionality of target (\(28 28\) for an MNIST digit). On the other hand, the model-based attack makes use of a prior in the form of the auxiliary points \(z_{i}\) used in the construction of shadow models; these points represent the adversary's knowledge about the distribution from which the target points \(z^{*}\) is sampled.

Going forward in our investigation we will assume the adversary has access to more "reliable" side knowledge: their prior is a uniform distribution over a finite set of candidate points \(\{z_{1},,z_{n}\}\), one of which corresponds to the true target point. This setting is favorable towards the adversary: the points in the prior represent a shortlist of candidates the adversary managed to narrow down using side knowledge about the target. In this case it is also reasonable to use the error function \((z,z^{})=[z z^{}]\) since the adversary's goal is to identify which point from the prior is the target. As DP assumes the adversary knows (and can influence) all but one record in the training set, the assumption that the adversary has prior knowledge about the target is aligned with the DP threat model. The main distinction between membership inference and reconstruction with a uniform prior is that in the former the adversary (implicitly) managed to narrow down the target point to two choices, while in the latter they managed to narrow down the target to \(n\) choices. This enables us to smoothly interpolate between the exceedingly strong DP threat model of membership inference (where the goal is to infer a single bit) and a relaxed setting where the adversary's side knowledge is less refined: here \(n\) controls the amount of prior knowledge the adversary is privy to, and requires the adversary to infer \((n)\) bits to achieve a successful reconstruction.

The discrete prior setup, we argue, provides a better alignment between the success of reconstruction attacks and what actually constitutes privacy leakage. In particular, it allows us to move away from (approximate) verbatim reconstruction as modelled, e.g., by an \(_{2}\) reconstruction criteria success, and model more interesting situations. For example, if the target image contains a car, an attacker might be interested in the digits of the license plate, not the pixels of the image of the car and its background. Thus, if the license plate contains 4 digits, the attacker's goal is to determine which of the possible 10,000 combinations was present in the in the training image.

In Appendix I, we also conduct experiments where the adversary has less background knowledge about the target point, and so the prior probability of reconstruction is _extremely_ small (e.g. \(}{{256}}}{{2}}\) for a CIFAR-10 image).

### Reconstruction Robustness of DP-SGD

ReRo bounds from blow-up functionsWe now state a novel reconstruction robustness bound that, instead of using the (R)DP guarantees of the mechanism, is directly expressed in terms of its output distributions. The bound depends on the _blow-up function_ between two distributions \(\) and \(\):

\[_{}(,)=\{_{}[E]:E\ \ _{}[E]\}.\] (3)

In particular, let \(_{D_{*}}=M(D_{*})\) denote the output distribution of the mechanism on a fixed dataset \(D_{*}\), and \(_{D_{*}}=M(D_{*}\{z\})\) for any potential target \(z\). Then we have the following (see Appendix M.1 for the full statement).

**Theorem 2** (Informal).: _Fix \(\) and \(\). Suppose that for every fixed dataset \(D\). there exists a pair of distributions \(^{*}_{D_{*}},^{*}_{D_{*}}\) such that \(_{z supp()}_{}(_{D_{*}},_{D_{*}})_{}(^{*}_{D_{*}},^{*}_{D_{*}})\) for all \(\). Then \(M\) is \((,)\)-ReRo with \(=_{D}_{_{,}()}(^{*}_{D_{*}},^{*}_{ D_{*}})\)._

This result basically says that the probability of successful reconstruction for an adversary that does observe the output of the mechanism can be bounded by the maximum probability under \(^{*}_{D_{*}}\) over all events that have probability \(_{,}()\) under \(^{*}_{D_{*}}\), when we take the worst-case setting over all fixed datasets \(D_{*}\) and all target points \(z\) in the support of the prior. If \(M\) satisfies \((,0)\)-DP (i.e. \((,)\)-RDP), then \(_{_{D_{*}}}[E] e^{}_{_{D_{*}}}[E]\) for any event \(E\), in which case Theorem 2 gives with \(=_{,}()e^{}\). This recovers the case \(=\) of the bound from Balle et al. (2022) stated above.

**Remark 3**.: _The blow-up function is tightly related to the notion of trade-off function defined in Dong et al. (2019). Precisely, the trade-off function between two probability distributions \(\) and \(\) is defined as \(_{}(,)=\{_{}[E]:E\ \ _{}[E]\}\). The trade-off function is usually introduced in terms of Type I and Type II errors; it defines the smallest Type II error achievable given constraint on the maximum Type I error, and we have the following relationship: \(_{}(,)=1-_{}(,)\)._

[MISSING_PAGE_FAIL:7]

Estimation guaranteesAlthough Proposition 4 is only asymptotic, we can provide confidence intervals around this estimate using two types of inequalities. Algorithm 1 has two points of error, the gap between the empirical quantile and the population quantile, and the error from mean estimation \(\). The first source of error can be bounded using the DKW inequality (Dvoretzky et al., 1956) which provides a uniform concentration for each quantile. In particular, we can show that the error of quantile estimation is at most \(2e^{-2ny^{2}}\), where \(n\) is the number of samples and \(\) is the error in calculation of the quantile. Specifically, with a million samples, we can make sure that with probability 0.999 the error of quantile estimation is less than 0.002, and we can make this smaller by increasing the number of samples. We can account for the second source of error with Bennett's inequality, that leverages the bounded variance of the estimate. In all our bounds, we can show that the error of this part is also less than 0.01 with probability 0.999. We provide an analysis on the computational cost of estimating \(\) in Appendix A.

### Lower bound estimation via a prior-aware attack

We now present a prior-aware attack whose success can be directly compared to the reconstruction upper bound from Corollary 4.

In addition to the uniform prior \(=_{i}_{z_{i}}\) from which we assume the target is sampled from, our prior-aware attack has access to the same information as the gradient-based attack from Section 2: all the privatized gradients (and therefore, intermediate models) produced by DP-SGD, and knowledge of the fixed dataset \(D\). that can be used to remove all the known datapoints from the privatized gradients. The attack is given in Algorithm 2. For simplicity, in this section we present and experiment with the version of the attack corresponding to the full-batch setting in DP-SGD (i.e. \(q=1\)). We present an extension of the algorithm to the mini-batch setting in Section 4.3.

The rationale for the attack is as follows. Suppose, for simplicity, that all the gradients are clipped so they have norm exactly \(C\). If \(z^{*}\) is the target sampled from the prior, then \(_{t}_{C}(_{}(_{t},z^{*}))+ (0,C^{2}^{2}I)\). Then the inner products \(_{C}(_{}(_{t},z_{i})),_{t}\) follow a distribution \((C^{2},C^{4}^{2})\) if \(z_{i}=z^{*}\) and \((A,C^{4}^{2})\) for some \(A<C^{2}\) otherwise (assuming no two gradients are in the exact same direction). In particular, \(A 0\) if the gradients of the different \(z_{i}\) in the prior are mutually orthogonal. Thus, finding the \(z_{i}\) that maximizes this sum of inner products is likely to produce a good guess for the target point. This attack gives a lower bound on the probability of successful reconstruction, which we can compare with the upper bound estimate derived in Section 3.3.

## 4 Experimental Evaluation

We now evaluate both our upper bounds for reconstruction success and our empirical privacy attacks (which gives us lower bounds on reconstruction success). We show that our attack has a success probability nearly identical to the bound given by our theory. We conclude this section by inspecting how different variations on our threat models change both the upper bound and the optimality of our new attack.

### Attack and Bound Evaluation on CIFAR-10

We now evaluate our upper bound (Corollary 4) and prior-aware reconstruction attack (Algorithm 2) on full-batch DP-SGD, and compare it to the gradient-based (prior-oblivious) attack optimizing Equation (1) and the RDP upper bound obtained from previous work (Equation (2)). To perform our evaluation on CIFAR-10 models with relatively high accuracy despite being trained with DP-SGD, we follow De et al. (2022) and use DP-SGD to fine-tune the last layer of a WideResNet model (Zagoruyko and Komodakis, 2016) pre-trained on ImageNet. In this section and throughout the paper, we use a prior with uniform support over \(10\) randomly selected points unless we specify the contrary. Further experimental details are deferred to Appendix A.

The results of our evaluation are presented in Figure 2, which reports the upper and lower bounds on the probability of successful reconstruction produced by the four methods at three different values of \((,10^{-5})\)-DP. The first important observation is that our new ReRo bound for DP-SGD is a significant improvement over the RDP-based bound obtained in Balle et al. (2022), which gives a trivial bound for \(\{10,100\}\) in this setting. On the other hand, we also observe that the prior-aware attack substantially improves over the prior-obliviousgradient-based attack5, which Section 2 already demonstrated is stronger than previous model-based attacks - a more thorough investigation of how the gradient-based attack compares to the prior-aware attack is presented in Appendix D for varying sizes of the prior distribution. We will use the prior-aware attack exclusively in following experiments due to its superiority over the gradient-based attack. Finally, we observe that our best empirical attack and theoretical bound are very close to each other in all settings. We conclude there are settings where the upper bound is nearly tight and the prior-based attack is nearly optimal.

Now that we have established that our attacks work well on highly accurate large models, our remaining experiments will be performed on MNIST which are significantly more efficient to run.

### Effects of the prior size

We now investigate how the bound and prior-aware attack are affected by the size of the prior in Figure 3. As expected, both the upper and lower bound to the probability that we can correctly infer the target point decreases as the prior size increases. We also observe a widening gap between the upper bound and attack for larger values of \(\) and prior size. However, this bound is still relatively tight for \(<10\) and a prior size up to \(2^{10}\). Further experiments regarding how the prior affects reconstruction are given in Appendix E.

### Effect of DP-SGD Hyperparameters

Given that we have developed an attack that is close to optimal in a full-batch setting, we are now ready to inspect the effect DP-SGD hyper-parameters controlling its privacy guarantees have on reconstruction success. First, we observe how the size of mini-batch affects reconstruction success, before measuring how reconstruction success changes with differing hyperparameters at a fixed value of \(\). Further experiments that measure the effect of the clipping norm, \(C\), are deferred to Appendix K.

Effect of sampling rate \(q\).All of our experiments so far have been in the full-batch (\(q=1\)) setting. We now measure how mini-batching affects both the upper and lower bounds by reducing the data sampling probability \(q\) from \(1\) to \(0.02\) (see Appendix A for additional experimental details).

The attack presented in Algorithm 2 assumes that the gradient of \(z^{*}\) is present in every privatized gradient. Since this is no longer true in mini-batch DP-SGD, we design an improved attack by factoring in the knowledge that only a fraction \(q\) of privatized gradients will contain the gradient of the target \(z^{*}\). This is achieved by, for fixed \(z_{i}\), collecting the inner-products \(_{C}(_{}(_{t},z_{i})),_{t}\) across all \(T\) iterations,

Figure 4: We compare how the attack from Algorithm 2 compares with the improved prior-aware attack when \(q=0.02\).

Figure 3: How the size of the prior \(\) affects reconstruction success probability for a range of different values of \(\).

Figure 2: Comparison of success of reconstruction attacks against reconstruction upper bounds on CIFAR-10 with test accuracy at \(=1,10\), and \(100\) equal to 61.88%, 89.09%, and 91.27%, respectively. Note, the best model we could train at \(=\) with our experimental settings reaches 93.91%.

and computing the score using only the top \(qT\) values. Pseudo-code for this improved prior-aware attack variant is given in Appendix F.

Results are shown in Figure 4, where we observe the improved attack is fairly tight to the upper bound, and also a large difference between the previous and improved attack.

Effect of DP-SGD hyperparameters at a fixed \(\).In our final experiment, we fix \(\) and investigate if and how the reconstruction upper bound and lower bound (through the improved prior-aware attack) change with different DP-SGD hyperparameters. In Figure 5, we fix \(=4\) and run DP-SGD for \(T=100\) iterations with \(C=1\) while varying \(q[0.01,0.99]\). In each setting we tune \(\) to satisfy \((4,10^{-5})\)-DP. For experimental efficiency, we use fewer values of \(q\) for the attack than the theoretical bound. Surprisingly, we find that both the upper bound and lower bound _change_ with different hyperparameter configurations. For example, at \(q=0.01\) the upper bound on successful reconstruction is \( 0.20\) (and the attack is \( 0.15\)), while at \(q=0.99\) it is \( 0.35\) (and the attack is \( 0.32\)). Generally, an increase in \(T\) or \(q\) increases the upper bound. This suggests that an \((,)\)-DP guarantee alone is not able to fully specify the probability that a reconstruction attack is successful. We conduct a more extensive evaluation for different \(\) and prior sizes in Appendix K.2.

## 5 Conclusion

In this work, we investigated training data reconstruction bounds on DP-SGD. We developed upper bounds on the success that an adversary can reconstruct a training point \(z^{*}\) under DP-SGD. In contrast to prior work that develop reconstruction upper bounds based on DP or RDP analysis of DP-SGD, our upper bounds are directly based on the parameters of the algorithm. We also developed new reconstruction attacks, specifically against DP-SGD, that obtain lower bounds on the probability of successful reconstruction, and observe they are close to our upper bounds. Our experiments show that both our upper and lower bounds are superior to previously proposed bounds for reconstruction in DP-SGD. Our investigations further showed that the \(\) parameter in DP (or RDP) cannot solely explain robustness to reconstruction attacks. In other words, one can have the same value of \(\) for two different algorithms while achieving stronger robustness against reconstruction attacks in one over the other. This suggest that a more fine-grained analysis of algorithms against specific privacy attacks can lead to superior privacy guarantees and also opens up the possibility of better hyperparameter selection for specific privacy concerns.

## 6 Acknowledgements

The authors would like to thank Leonard Berrada and Taylan Cemgil for their thoughtful feedback on an earlier version of this work.