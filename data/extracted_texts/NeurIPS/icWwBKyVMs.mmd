# Interpretable Prototype-based Graph Information Bottleneck

Sangwoo Seo\({}^{1}\) Sungwon Kim\({}^{1}\) Chanyoung Park\({}^{1}\)

\({}^{1}\)KAIST

{sangwooseo@kaist.ac.kr, swkim@kaist.ac.kr, cy.park@kaist.ac.kr}

Corresponding author.

###### Abstract

The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based **G**raph **I**nformation **B**ottleneck (PGIB), that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph that is important for the model prediction. This is the first work that incorporates prototype learning into the process of identifying the key subgraphs that have a critical impact on the prediction performance. Extensive experiments, including qualitative analysis, demonstrate that PGIB outperforms state-of-the-art methods in terms of both prediction performance and explainability. The source code of PGIB is available at https://github.com/sang-woo-seo/PGIB.

## 1 Introduction

With the success of Graph Neural Networks (GNNs) in a wide range of deep learning tasks, there has been an increasing demand for exploring the decision-making process of these models and providing explanations for their predictions. To address this demand, explainable AI (XAI) has emerged as a way to understand black-box models by providing transparent explanations for their predictions. This approach can improve the credibility of models and ensure transparency in their decision-making processes. As a result, XAI is actively being used in various applications, such as medical, finance, security, and chemistry [3; 26].

In general, explainability can be viewed from two perspectives: 1) _improving the interpretability_ by providing explanations for the model's _predictions_, and 2) _providing the reasoning process_ behind the model prediction by giving explanations for the model's _training process_. _Improving the interpretability_ in GNNs involves detecting important substructures during the inference phase, which is useful for tasks such as identifying functional groups (i.e., important substructures) in molecular chemistry [29; 33; 23; 12]. On the other hand, it is also important to _provide the reasoning process_ for why the model predicts in a certain way, which requires an in-depth analysis of the training phase, so as to understand the model in a more fundamental level. Through this reasoning process, we can visualize and analyze how the model makes correct or incorrect decisions, thus obtaining crucial information for improving its performance.

In recent years, there has been a growing interest in exploring the reasoning process to provide greater transparency and explainability in deep learning models. These approaches can be generally classified into two main categories: 1) _post-hoc_ approaches, and 2) _built-in_ approaches. _Post-hoc_ approaches focus on exploring the model outputs by visualizing the degree of activation of neurons through measuring their contribution based on gradients of the model predictions. For instance, recent works  use techniques such as saliency maps and class activation maps to visualize the activated areas during the model's prediction process. However, these approaches require a separate explanatory model for each trained model, resulting in the need for a new explanatory model for additional training data and different models . In order to address the aforementioned challenges, _built-in_ approaches aim to integrate the generation of explanations into the model training process. One such approach is prototype learning, which involves learning prototypes that represent each class of the input data, which are then compared with new instances to make predictions. ProtGNN , for instance, measures the similarity between the embedding of an input graph and each prototype, providing explanations through the similarity calculation and making predictions for the input graph based on its similarity with the learned prototypes. More precisely, ProtGNN projects each learned prototype onto the closest training graph, enabling it to provide explanations of primary structures for its prediction.

However, since ProtGNN compares the graph-level embedding of an input graph with the learned prototypes, the model overlooks the key substructures in the input graph while also potentially including uninformative substructures. This not only results in a degradation of the interpretability of the reasoning process, but also limits the performance on the downstream tasks. Figure 1(a) shows the prototype graphs in the training set (i.e., \(_{p}\), denoted as bold edges) detected by ProtGNN for an input molecule (i.e., \(\)) that belongs to the "mutagenic" class. Despite the \(_{2}\) structure being the key functional group for classifying a given molecule as "mutagenic," \(_{p}\) detected by ProtGNN tends to include numerous ring structures (i.e., uninformative substructures) that are commonly found throughout the input graph, and exclude \(_{2}\) structures (i.e., key substructures) in learned prototype graphs, which is mainly due to the fact that the input graph \(\) is considered in the whole graph-level. As a result, it is crucial to identify a key subgraph within the input graph that holds essential information for the learning of prototypes, which in turn enhances both the explanation of the reasoning process and the performance on the downstream tasks. Among the various solutions for detecting important subgraphs, the Information Bottleneck (IB) has emerged as one of the most effective methods , and it has been demonstrated that key subgraphs detected based on IB can contribute to performance improvement in various tasks such as relational learning  and structure learning . We aim to approach the IB principle from the perspective of prototypes to convey important substructure information to the prototypes.

To this end, we propose a novel framework of explainable GNNs, called Interpretable Prototype-based Graph Information Bottleneck (PGIB). The main idea is to incorporate prototype learning within the Information Bottleneck (IB) framework, which enables the prototypes to capture the essential key subgraph of the input graph detected by the IB framework. Specifically, PGIB involves prototypes (i.e., \(_{p}\)) in a process that maximizes the mutual information between the learnable key subgraph (i.e., \(_{sub}\)) of the input graph (i.e., \(\)) and target information (i.e., \(Y\)), which allows the prototypes to interact with the subgraph. This enables the learning of prototypes \(_{p}\) based on the key subgraph \(_{sub}\) within the input graph \(\), leading to a more precise explanation of the reasoning process and improvement in the performance on the downstream tasks. To the best of our knowledge, this is the first work that combines the process of optimizing the reasoning process and interpretability by identifying the key subgraphs that have a critical impact on the prediction performance. In Figure 1(b), PGIB is shown to successfully detect the key subgraph \(_{sub}\) that includes \(_{2}\) from the input graph \(\), even when the ring structures are dominant in \(\). It is important to note that PGIB is highly efficient in detecting \(_{sub}\) from \(\) since PGIB adopts a learnable masking technique, effectively resolving the time complexity issue. Last but not least, since the number of prototypes for each

Figure 1: Comparison of the learned prototypes between ProtGNN and PGIB.

class is determined before training, some of the learned prototypes may share similar semantics, which negatively affects the model interpretability for which the small size and low complexity are desirable [6; 25; 17]. Hence, we propose a method for effectively merging the prototypes, which in turn contributes to enhancing both the explanation of the reasoning process and the performance on the downstream tasks.

We conducted extensive experiments to evaluate the effectiveness and interpretability of the reasoning process of PGIB in graph classification tasks. Our results show that PGIB outperforms recent state-of-the-art methods, including existing prototype learning-based and IB-based methods. Moreover, we evaluated the ability of PGIB in capturing the label information by evaluating the classification performance using only the detected subgraph \(_{sub}\). We also conducted a qualitative analysis that visualizes the subgraph \(_{sub}\) and prototype graph \(_{p}\), suggesting the ability of PGIB in detecting the key subgraph. Overall, our results show that PGIB significantly improves the interpretability of both \(_{sub}\) and \(_{p}\) in the reasoning process, while simultaneously improving the performance in downstream tasks.

In summary, our main contributions can be summarized as follows: **1)** We propose an effective approach, PGIB, that not only improves the interpretability of the reasoning process, but also the overall performance in downstream tasks by incorporating the prototype learning in a process of detecting key subgraphs based on the IB framework. **2)** We provide theoretical background with our method that utilizes interpretable prototypes in the process of optimizing \(_{sub}\). **3)** Extensive experiments, including qualitative analysis, demonstrate that PGIB outperforms state-of-the-art methods in terms of both prediction performance and explainability.

## 2 Preliminaries

In this section, we introduce notations used throughout the paper followed by the definitions of IB and IB-Graph.

Notations.We use \(=(,,,)\) to denote a graph, where \(\), \(\), \(\) and \(\) denote the set of nodes and edges, the adjacency matrix and node features, respectively. We assume that each node \(v_{i}\) is associated with a feature vector \(_{i}\), which is the \(i\)-th row of \(\). We use \(\{(_{1},y_{1}),(_{2},y_{2}),,(_{N},y_{ N})\}\) to denote the set of \(N\) graphs with its corresponding labels. The graph labels are given by a set of \(K\) classes \(=\{1,2,,K\}\), and the ground truth label of a graph \(_{i}\) is denoted by \(y_{i}\). We use \(_{sub}\) to denote a subgraph of \(\), and use \(}_{sub}\) to denote the complementary structure of \(_{sub}\) in \(\). We also introduce the prototype layer, which consists of a set of prototypes \(_{p}=\{_{_{p}}^{1},_{_{p}}^{2},,_{_{p}}^{M}\}\), where \(M\) is the total number of prototypes, and each prototype \(_{_{p}}^{m}\) is a learnable parameter vector that serves as the latent representation of the prototypical part (i.e., \(_{p}\)) of graph \(\). We allocate \(J\) prototypes for each class, i.e., \(M=K J\).

Graph Information Bottleneck.The mutual information between two random variables \(X\) and \(Y\), i.e., \(I(X;Y)\), is defined as follows:

\[I(X;Y)=_{X}_{Y}p(x,y)xy\] (1)

Given the input \(X\) and its associated label \(Y\), the Information Bottleneck (IB)  aims to obtain a bottleneck variable \(Z\) by optimizing the following objective:

\[_{Z}-I(Y;Z)+ I(X;Z),\] (2)

where \(\) is the Lagrange multiplier used to control the trade-off between the two terms. IB principle has recently been applied to learning a bottleneck graph, named IB-Graph, for a given graph \(\), which retains the minimal sufficient information in terms of \(\)'s properties . This approach is motivated by the Graph Information Bottleneck (GIB) principle, which seeks to identify an informative yet compressed subgraph \(_{sub}\) from the original graph \(\) by optimizing the following objective:

\[_{_{sub}}-I(Y;_{sub})+ I(;_{sub}),\] (3)

where \(Y\) is the label of \(\). The first term maximizes the mutual information between the graph label and the compressed subgraph, which ensures that the compressed subgraph contains as much information as possible about the graph label. The second term minimizes the mutual information between the input graph and the compressed subgraph, which ensures that the compressed subgraph contains minimal information about the input graph.

## 3 Methodology

In this section, we present our proposed method, called PGIB. We introduce Prototype-based Graph Information Bottleneck (Section 3.1), each layer in the architecture (Section 3.2 - 3.4), and the interpretability stabilization process (Section 3.5), which enhances the interpretability and the tracking capabilities of the reasoning process during the model training.

**Model Architecture.** Figure 2 presents an overview of PGIB. We first generate the representations of nodes in the input graph \(\) using a GNN encoder. Then, the node representations are passed to the subgraph extraction layer that assigns each node in \(\) to either \(_{sub}\) or \(}_{sub}\). Next, we compute the similarities between the embedding \(_{_{sub}}\) and the set of prototypes \(_{p}=\{_{_{p}}^{1},_{_{p}}^{2},,_{_{p}}^{M}\}\) in the prototype layer. Finally, we merge the prototypes that are semantically similar, which are then used to generate the final prediction.

### Prototype-based Graph Information Bottleneck

PGIB is a novel explainable GNN framework that incorporates the prototype learning within the IB framework, thereby enabling the prototypes to capture the essential key subgraph of the input graph detected by the IB framework. More precisely, we reformulate the GIB objective shown in Equation 3 by decomposing the first term, i.e., \(I(Y;_{sub})\), with respect to the prototype \(_{p}\) using the chain rule of mutual information in order to examine the impact of the joint information between \(_{sub}\) and \(_{p}\) on \(Y\) as follows:

\[_{_{sub}}_{sub},_{p})+I (Y;_{p}|_{sub})}_{}+;_{sub})}_{}.\] (4)

Please refer to Appendix A.1 for a detailed proof of Equation 4. In the following sections, we describe how each term is optimized during training.

### Subgraph Extraction Layer (Minimizing \(I(;_{sub})\))

The goal of the subgraph extraction layer is to extract an informative subgraph \(_{sub}\) from \(\) that contains minimal information about \(\). We minimize \(I(;_{sub})\) by training the model to inject noise into insignificant subgraphs \(}_{sub}\), while injecting less noise into more informative ones \(_{sub}\). Specifically, given the representation of node \(v_{i}\), i.e., \(_{i}\), we compute the probability \(p_{i}\) with an MLP followed by a sigmoid function, which is then used to replace the representation \(h_{i}\) to obtain the final representation \(_{i}\) as follows:

\[ p_{i}=((h_{i }))\\ z_{i}=_{i}h_{i}+(1-_{i}),\ _{i} (p_{i})(_{_{i}},_{ _{i}}^{2}).\] (5)

That is, the learned probability \(p_{i}\) enables selective preservation of information in \(_{sub}\), and based on this probability, the quantity of information transmitted from \(_{i}\) to \(_{i}\) can be flexibly adjusted to compress the information from \(\) to \(_{sub}\). This approach not only retains interpretability within the

Figure 2: The architecture of our proposed PGIB. PGIB generates a subgraph \(_{sub}\) by injecting noise to identify core subgraphs, and it is used to compute similarity scores between prototypes in the prototype layer. The trained prototypes play a crucial role in visualizing the reasoning processes during training in an interpretable manner. PGIB also involves merging pairs of similar prototypes to decrease the number of prototypes. Finally, the integrated prototypes are utilized to predict the graph labels in the fully connected layer.

subgraph itself, but also potentially facilitates the learning of prototypes that are introduced in the next step. Following , we minimize the upper bound of \(I(;_{sub})\) as follows:

\[I(;_{sub})_{}(- A +_{}|}A+_{}| }B^{2})=:_{}^{1}(,_{sub}),\] (6)

where \(A=_{i=1}^{|_{}|}(1-_{i})^{2}\) and \(B=^{|_{}|}_{i}(_{i}- _{_{i}})}{_{_{i}}}\). Thus, minimizing \(_{}^{1}\) allows us to minimize the upper bound of \(I(;_{sub})\). After noise injection, we compute the embedding \(_{_{sub}}\) through a graph readout function such as max pooling and sum pooling. For further details and analysis of the different graph readout functions, please refer to Appendix A.4.2.

Prototype Layer (Minimizing \(-I(Y;_{sub},_{p})+I(Y;_{p}|_{sub})\) )

The prototype layer involves allocation of a fixed number of prototypes for each class. The prototypes are required to capture the most significant graph patterns that can aid in the identification of the graphs within each class. To begin with, we define the similarity score between the prototype \(_{_{p}}\) and the embedding \(_{_{sub}}\) obtained from noise injection as follows:

\[g(_{_{sub}},_{_{p}})=( _{_{sub}}-_{_{p}}\|_{2}^{2 }+1}{\|_{_{sub}}-_{_{p}}\|_{2}^{2 }+}),\] (7)

where \(_{_{p}}\) is the prototype and shares the same dimension as \(_{_{sub}}\).

#### 3.3.1 Minimizing \(-I(Y;_{sub},_{p})\)

We derive the lower bound of \(I(Y;_{sub},_{p})\) as follows:

_Proposition 1_.: **(Lower bound of \(I(Y;_{sub},_{p})\))** Given significant subgraph \(_{sub}\) for a graph \(\), its label information \(Y\), prototype graph \(_{p}\) and similarity function \(\), we have

\[ I(Y;_{sub},_{p})& =_{Y,_{sub},_{p}}[ p(Y| _{sub},_{p})]-_{Y}[ p(Y)] \\ &_{Y,_{sub},_{p}}[ p (Y|(_{sub},_{p}))]- _{Y}[ p(Y)]\\ &_{Y,_{sub},_{p}}[ q _{p}(Y|(_{sub},_{p}))] \\ &=:-_{}(q_{p}(Y|(_{sub},_{p})))\] (8)

where \(q_{}(Y|(_{sub},_{p}))\) is the variational approximation to the true posterior \(p(Y|(_{sub},_{p}))\).

Equation 8 demonstrates that the maximization of the mutual information \(I(Y;_{sub},_{p})\) can be attained by minimizing the classification loss, denoted as \(_{cls}\). This maximization of mutual information between the label \(Y\) and the similarity information \((_{sub},_{p})\) promotes the subgraph \(_{sub}\) and prototype \(_{p}\) to possess predictive capabilities concerning the graph label \(Y\). In practical applications, the cross-entropy loss is chosen for a categorical \(Y\). For a comprehensive understanding of the derivation process of Equation 8, refer to the Appendix A.2. Note that the similarity between \(_{sub}\) and \(_{p}\), i.e., \((_{sub},_{p})\), is computed by the similarity score defined in Equation 7, i.e., \(g(_{_{sub}},_{_{p}})\).

#### 3.3.2 Minimizing \(I(Y;_{p}|_{sub})\)

We investigate the mutual information, denoted as \(I(Y;_{p}|_{sub})\), from the perspective of the interaction between \(_{sub}\) and \(_{p}\). We decompose \(I(Y;_{p}|_{sub})\) into the sum of two terms based on the chain rule of mutual information as follows:

\[I(Y;_{p}|_{sub})=I(_{p};Y,_{sub})-I (_{sub};_{p}).\] (9)

It is important to note that the first term, i.e., \(I(_{p};Y,_{sub})\), minimizes the mutual information between \(_{p}\) and the joint variables \((Y,\,_{sub})\), which eliminates the information about \(Y\) related to \(_{sub}\) from \(_{p}\). However, since our goal is not to solely minimize \(I(_{p};Y,_{sub})\) but to ensure the interpretability of the prototype \(_{p}\), including this term leads to diminished interpretability of \(_{p}\). Consequently, we excluded the first term during training, and only consider the second term, i.e., \(-I(_{sub};_{p})\), to simultaneously guarantee the interpretability of both \(_{sub}\) and \(_{p}\). A detailed derivation for Equation 9 is given in Appendix A.3. From now on, we describe approaches for minimizing the second term. Inspired by , we introduce two different approaches for minimizing \(-I(_{sub};_{p})\).

[MISSING_PAGE_FAIL:6]

\[_{_{p}}*{arg\,min}_{}}\|}-_{_{p}}\|_{ 2},=\{}:\{f_{g}(})\},}(_{i})\  is y_{i}=k\}.\] (14)

In the Equation 14, we use Monte Carlo Tree Search (MCTS)  to explore training subgraphs \(}\) during prototype projection.

Connectivity Loss.For an input graph \(\), we construct a node assignment \(S_{}^{|_{}| 2}\) based on the probability values that are computed by Equation 5. Specifically, \(S_{}[j,0]\) and \(S_{}[j,1]\) denote the probability of node \(v_{j}_{}\) belonging to \(_{sub}\) and \(}_{sub}\), respectively. Following , poor initialization of the matrix \(S\) may result in the proximity of its elements \(S[j,0]\) and \(S[j,1]\) for \( v_{j}_{_{i}}\), leading to an unstable connectivity of \(_{sub}\). This instability can have adverse effects on the subgraph generation process. To enhance the interpretability of \(_{sub}\) by inducing a compact topology, we utilize a batch-wise loss function as follows:

\[_{}=\|(S_{}^{T}_{ }S_{})-I_{2}\|_{F}\] (15)

where \(S_{}^{_{i=1}^{n}|_{_{ i}}| 2}\) and \(_{}^{_{i=1}^{n}|_{ _{i}}|_{i=1}^{n}|_{_{i}}|}\) are the node assignment and the adjacency matrix at the batch level, respectively. \(I_{2}\) is 2-by-2 identity matrix, \(\|\|_{F}\) is the Frobenius norm and \(()\) is the row normalization. Minimizing \(_{}\) indicates that if \(v_{j}\) is in \(_{sub}\) its neighbors also have a high probability to be in \(_{sub}\), while if \(v_{i}\) is in \(_{sub}\), its neighbors have a low probability to be in \(}_{sub}\).

Final Objectives.Finally, we define the objective of our model as the sum of the losses as follows:

\[_{}=_{}+_{1}_ {}^{1}+_{2}_{}^{2}+_{3}_{}\] (16)

where \(_{1},_{2}\) and \(_{3}\) are hyper-parameters that adjust the weights of the losses. A detailed ablation study for each loss term is provided in Appendix A.4.1 for further analysis.

## 4 Experiments

### Experimental Settings

Each dataset is split into training, validation, and test sets with a ratio of \(80\%\), \(10\%\), and \(10\%\), respectively. All models are trained for \(300\) epochs using the Adam optimizer with a learning rate of 0.005. GIN  is used as the encoder for all models used in the experiment. We evaluate the performance based on accuracy, which is averaged over 10 independent runs with different random seeds. For simplicity, the hyperparameters \(_{1}\), \(_{2}\), and \(_{3}\) in Equation 16 are set to \(0.0001,0.01\) to \(0.1\) and \(5\), respectively. The prototype merge operation starts at epoch \(100\) and is performed every \(50\) epochs thereafter. We set the number of prototypes per class to \(7\) and combine \(30\%\) of the most similar prototype pairs.

### Graph Classification

Datasets and Baselines. We use the MUTAG , PROTEINS , NCI1 , and DD datasets. These are datasets related to molecules or bioinformatics, and are widely used for evaluations on graph classification. We consider three GNN baselines, including GCN , GIN , GAT . In addition, we compare PGIB with several state-of-the-art built-in models that integrate explanation functionality internally, including a prototype-based method ProtGNN , and IB-based models such as GIB , VGIB , and GSAT . Further details about the baselines and datasets are provided in Appendix A.5 and A.6, respectively.

Experiment Results. Experimental results for graph classification are presented in the Table 1. In the table, PGIB and PGIB\({}_{}}\) represent our proposed methods. PGIB utilizes a Variational IB-based approach, while PGIB\({}_{}}\) employs a Contrastive learning-based approach to maximize \(I(_{};_{p})\) (Section 3.3.2). We have the following observations: **1)** All variants of PGIB outperform the baselines including both the prototype-based and IB-based methods on all datasets. Notably, PGIBs incorporate the crucial information of the key subgraph, which significantly contributes to enhancing the classification performance. PGIB\({}_{}}\) achieves a significant improvement of up to 5.6% compared to the runner-up baseline. **2)** We observe that PGIB\({}_{}}\) performs relatively better than PGIB. We attribute this to the nature of the contrastive loss, which is generally shown to be effective in classifying instances between different classes, allowing the prototypes learned based on the contrastive loss to be more distinguishable from one another.

### Graph Interpretation

In this section, we evaluate the process of extracting subgraphs that possess the most similar properties of the original graph. We present qualitative results including subgraph visualizations, and conduct quantitative experiments to measure how accurately explanations capture the important components that contribute to the model's predictions.

**Datasets and Baselines**. We use four molecular properties from the ZINC  dataset, which consists of 250,000 molecules, for graph interpretation. QED measures the likelihood of a molecule being a drug and DRD2 indicates the probability of a molecule being active on dopamine type 2 receptors. HLM-CLint and RLM represent estimates of in vitro human and rat liver microsomal metabolic stability (mL/min/g as base 10 logarithm). We compare PGIB\({}_{}\) with several representative interpretation models, including GNNexpliner , PGexpliner , GIB , and VGIB . Note that as PGIB shows similar results with PGIB\({}_{}\), we only report the results of PGIB\({}_{}\) for simplicity. Further details on baselines and datasets are described in Appendix A.5 and A.6, respectively.

**Qualitative Analysis**. Figure 3(a)-(d) present the visualization of subgraphs in Mutag dataset. According to [29; 4], the \(_{2}\) functional group is known to be a cause of mutagenicity, while the carbon ring is a substructure that is not related to mutagenicity. In the figure, the bold edges connect the nodes that the models consider important. The \(_{2}\) group in Mutag is correctly identified by PGIB, while VGIB, ProtGNN, and GNNexplainer fail to recognize all \(_{2}\) groups or include other unnecessary substructures. Figure 3(e)-(h) present the visualization of subgraphs in BA-2Motifs dataset. We observe that PGIB accurately recognizes motif graphs containing the label information such as a house or a five-node cycle, but other models have difficulty in detecting the complete motifs.

**Quantitative Analysis**. Although visualizing the explanations generated by models plays a crucial role in assessing various explanation models, relying solely on qualitative evaluations may not always be reliable due to their subjective nature. Therefore, we also perform quantitative experiments using the Fidelity metric [14; 34].

The Fidelity metric quantifies the extent to which explanations accurately capture the important components that contribute to the model's predictions. Specifically, let \(y_{i}\) and \(_{i}\) denote the ground-truth and predicted values for the \(i\)-th input graph, respectively. Moreover, \(k\) denotes the sparsity score of the selected subgraph in which nodes whose importance scores obtained by Equation 5 are among the top-\((k 100)\)% within the original graph, and its prediction is denoted by \(_{i}^{k}\). Additionally, \(_{i}^{1-k}\) denotes the prediction based on the remaining subgraph. The Fidelity scores are computed as follows:

\[_{-}=_{i=1}^{N}(y_{i}=_{i})- (y_{i}=_{i}^{k}),_{+}=_{i=1}^ {N}(y_{i}=_{i})-(y_{i}=_{i}^{1-k}),\] (17)

where \((y_{i}=_{i})\) is the binary indicator which returns 1 if \(y_{i}=_{i}\), and 0 otherwise. In other words, they measure how well the predictions made solely based on the extracted subgraph (i.e., \(_{-}\)) and the remaining subgraph (i.e., \(_{+}\)) mimic the predictions made based on the entire graph, respectively. Hence, a low value of \(_{-}\) and a high value of \(_{-}\) indicate better explainability of the model.

Table 2 shows the fidelity scores on four datasets at the sparsity score of \(k=0.5\). Our proposed model outperforms both post-hoc and built-in state-of-the-art explanation models in all datasets. Furthermore, merging prototypes achieves significant improvements in terms of interpretability. This implies

   &  \\   & GCN & GIN & GAT & ProtGNN & GIB & VGIB & GSAT & **PGIB** & **PGIB\({}_{}\)** \\  MUTAG & 74.50\(\)7.89 & 80.50\(\)7.89 & 73.50\(\)7.43 & 80.50\(\)9.07 & 79.00\(\)6.24 & 81.00\(\)6.63 & 80.88\(\)8.94 & 85.00\(\)7.07 & **85.50\(\)5.22** \\ PROTEINS & 72.83\(\)4.23 & 70.30\(\)4.84 & 71.35\(\)4.85 & 73.83\(\)4.22 & 75.25\(\)5.92 & 73.66\(\)3.32 & 69.64\(\)4.71 & 77.14\(\)2.19 & **77.50\(\)2.42** \\ NCI1 & 73.16\(\)3.49 & 75.04\(\)2.08 & 66.05\(\)1.03 & 74.13\(\)2.10 & 64.65\(\)6.78 & 63.75\(\)3.37 & 68.13\(\)2.64 & 77.65\(\)2.20 & **78.25\(\)2.13** \\ DD & 72.53\(\)4.51 & 72.04\(\)3.62 & 70.81\(\)4.33 & 69.15\(\)4.33 & 72.61\(\)8.26 & 72.77\(\)45.63 & 71.93\(\)2.74 & 73.36\(\)1.80 & **73.70\(\)2.14** \\  

Table 1: Evaluation on graph classification (accuracy).

   & _{+}\)} \\   & RLM & HLM-CLint & QED & DRD2 & RLM & HLM-CLint & QED & DRD2 \\  GNNexpliner & 0.478 & 0.616 & 0.498 & 0.433 & 0.694 & 0.778 & 0.602 & 0.740 \\ PGexplainer & 0.502 & 0.620 & 0.560 & 0.540 & 0.632 & 0.692 & 0.598 & 0.686 \\ GIB & 0.483 & 0.643 & 0.525 & 0.428 & 0.654 & 0.781 & 0.601 & 0.724 \\ VGIB & 0.463 & 0.579 & 0.487 & 0.424 & 0.765 & 0.792 & 0.627 & 0.756 \\  PGIB\({}_{}\) & 0.441 & 0.593 & 0.459 & 0.406 & 0.747 & 0.772 & 0.613 & 0.771 \\ PGIB\({}_{}\) + merge & **0.415** & **0.543** & **0.447** & **0.379** & **0.765** & **0.796** & **0.635** & **0.781** \\  

Table 2: Evaluation on graph interpretation (Fidelity scores).

that decreasing the number of prototypes can eliminate uninformative substructures and emphasize key substructures, which increases the interpretability of the extracted subgraphs. Figure 4 visualizes the comparison of fidelity scores over various sparsity scores of subgraphs. To ensure a fair comparison, the fidelity scores are compared under the same subgraph sparsity, as the difference between the predictions of the original graph and subgraph strongly depends on the level of sparsity. We observe that PGIB\({}_{}\) achieves the best performance in most sparsity environments on the four datasets.

### Hyperparameter Analysis

In Figure 6, we conduct a sensitivity analysis on the hyperparameters \(_{1}\) and \(_{2}\) of the final loss (Equation 16) relevant to mutual information. Note that \(_{1}\) and \(_{2}\) are related to minimizing \(I(;_{sub})\) and maximizing \(I(_{sub};_{p})\), respectively. **1)** Figure 6(a) shows a significant decrease in performance when \(_{1}\) becomes large, i.e., when the model focuses on compressing the subgraphs. This is because too much compression of subgraphs results in the loss of important information, ultimately having a negative impact on the downstream performance. However, when \(_{1}=0\) (i.e., \(=_{sub}\); no compression at all), uninformative information would be included in \(_{sub}\), which incurs a performance degradation. **2)** Figure 6(b) visualizes the change in performance depending on \(_{2}\). A small value of \(_{2}\) prevents sufficient transmission of information from \(_{sub}\) to \(_{p}\), whereas excessive value of \(_{2}\) allows the influence of \(_{sub}\) to dominate the prototypes \(_{p}\), both of which lead to a performance deterioration. For example, in Mutag dataset, a low value of \(_{2}\) ultimately results in \(_{p}\) not obtaining label-relevant information (i.e., NO\({}_{2}\)) that is captured by \(_{sub}\) (See Figure 5(a)). On the other hand, a high value of \(_{2}\) hinders the formation of diverse prototypes (See Figure 5(b)).

## 5 Conclusion and Future work

We propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB), that integrates prototype learning into the information bottleneck framework. The main idea of PGIB is to learn prototypes that capture subgraphs containing key structures relevant to the label information, and to merge the semantically similar prototypes for better model interpretability and model complexity. Experimental results show that PGIB achieves improvements not only in the performance on downstream tasks, but also provides more precise explanation of the reasoning process. For future work, we plan to further extend the applicability of PGIB by integrating domain knowledge into prototype learning by imposing constraints on subgraphs. We expect that this approach would enable the learning of prototypes that align with the domain knowledge as they obtain domain-specific information from the subgraphs.