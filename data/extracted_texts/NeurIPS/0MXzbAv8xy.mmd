# GFT: Graph Foundation Model with

Transferable Tree Vocabulary

 Zehong Wang

University of Notre Dame

Indiana, USA

zwang43@nd.edu

&Zheyuan Zhang

University of Notre Dame

Indiana, USA

zzhang42@nd.edu

&Nitesh V Chawla

University of Notre Dame

Indiana, USA

nchawla@nd.edu

&Chuxu Zhang

University of Connecticut

Connecticut, USA

chuxu.zhang@uconn.edu

&Yanfang Ye

University of Notre Dame

Indiana, USA

yye7@nd.edu

Corresponding Authors.

###### Abstract

Inspired by the success of foundation models in applications such as ChatGPT, as graph data has been ubiquitous, one can envision the far-reaching impacts that can be brought by Graph Foundation Models (GFMs) with broader applications in the areas such as scientific research, social network analysis, drug discovery, and e-commerce. Despite the significant progress of pre-trained graph neural networks, there haven't been GFMs that can achieve desired performance on various graph-learning-related tasks. Building GFMs may rely on a vocabulary that encodes transferable patterns shared among different tasks and domains. Unlike image and text, defining such transferable patterns for graphs remains an open question. In this paper, we aim to bridge this gap by rethinking the transferable patterns on graphs as computation trees - i.e., tree structures derived from the message-passing process. Based on this insight, we propose a cross-task, cross-domain graph foundation model named GFT, short for Graph Foundation model with transferable Tree vocabulary. By treating computation trees as tokens within the transferable vocabulary, GFT improves model generalization and reduces the risk of negative transfer. The theoretical analyses and extensive experimental studies have demonstrated the transferability of computation trees and shown the effectiveness of GFT across diverse tasks and domains in graph learning. The open source code and data are available at https://github.com/Zehong-Wang/GFT.

## 1 Introduction

Foundation models such as Large Language Models (LLMs) and Large Vision Models (LVMs) keep reshaping our view of the world . These models are designed to be general-purpose, adaptable across various tasks and domains through fine-tuning or prompting, such as GPT-4  in Natural Language Processing (NLP) and Sora  in Computer Vision (CV). Research attributes the success of foundation models to the uniformity of tasks and a general vocabulary that defines basic transferable patterns across tasks . For example, LLMs  treat language tasks as question-answering or next-word prediction and deconstruct sentences using a word vocabulary. Similarly, LVMs  reformulate image tasks as image question-answering, converting images into discrete tokens using a vision vocabulary. Inspired by the success of LLMsand LVMs, as graph-structured data (e.g., citation networks, social networks, computer networks, molecular structures, and recommender systems) have become ubiquitous, one can envision the far-reaching real-world impacts that can be brought by pre-trained Graph Foundation Models (GFMs).

Although there has been significant progress of pre-trained Graph Neural Networks (GNNs), there haven't been GFMs that can achieve desired performance on a wide range of graph-learning-related tasks. Unlike CV and NLP, as graphs represent complex, non-Euclidean relationships among entities [92; 48; 104; 58; 107], a grand challenge of building GFMs lies in identifying transferable patterns across graphs [50; 93; 25]. There have been extensive studies aiming to tackle this challenges, which can mainly be categorized into two groups: (1) Utilizing graphon theory: Ruiz et al.  provide theoretical evidence of transferability between two graphs generated from the same graphon. Cao et al.  further extend these findings by both empirically and theoretically analyzing graph transferability in pre-training and fine-tuning scenarios. Despite these theoretical guarantees, the stringent assumptions of graphon theory often prove difficult to satisfy in real-world, cross-domain datasets , thus limiting its applicability in defining transferable graph vocabularies. (2) Exploring graph transferability using subgraph structures [114; 59; 50]: Zhu et al.  demonstrate that the transferability between graphs is linked to the ego-graph patterns of individual nodes and establish a stability bound using the graph Laplacian. They suggest that localized subgraphs could serve as transferable patterns within graph vocabularies. Building on this finding, Sun et al.  develop a GFM by reformulating graph tasks as subgraph classification, enabling a single model to be applied to diverse tasks. Huang et al. , Liu et al.  expand GFMs to cross-domain scenarios by unifying the node feature space across different graphs through LLMs [60; 76]. Despite these successes, the process of explicit subgraph extraction remains time and memory intensive . More importantly, numerous studies such as [20; 10; 53; 103] show that message-passing GNNs [40; 24; 21] fail to detect critical substructures or motifs within subgraphs, reducing the feasibility of using subgraphs to define graph vocabularies.

_How to identify a vocabulary that can encode transferable patterns shared among different tasks and domains for the construction of GFMs?_ In this paper, we aim to address the limitations of existing works by answering this question. Specifically, based on message-passing mechanism of GNNs, we have observed that the learned embeddings of each node can be essentially captured in the form of its computation tree. Based on this insight, we bridge the research gap by rethinking the transferable patterns on graphs as computation trees - i.e., subtree structures derived from the message-passing process. Using computation tree as a transferable pattern across graphs will bring three primary advantages: (1) _Efficiency_: As the extraction and encoding of computation trees are integrated within a single message-passing GNN process , it eliminates the need for the explicit subgraph extraction for GFMs [30; 45]. (2) _Expressiveness_: Since computation trees are capable of capturing localized patterns , it's able to represent a graph as a multiset of computation trees . (3) _Learnability_: As the information of computation trees is completely captured by message-passing GNNs, it can tackle the issue that certain motifs within subgraphs remain elusive. We theoretically investigate the transferability of computation trees and empirically demonstrate a strong correlation between computation tree similarity and transfer learning performance across various graphs.

Based on the key idea above, by treating computation trees as graph vocabulary tokens, we develop a cross-task, cross-domain graph foundation model - namely GFT - short for Graph Foundation model with transferable Tree vocabulary. GFT consists of pre-training and fine-tuning phases, enabling it to handle datasets across different tasks and domains effectively. During pre-training, we introduce a computation tree reconstruction task to acquire generalized knowledge from cross-domain graphs. We obtain a discrete tree vocabulary of prototypical tree tokens by quantizing the embedding space of computation trees, which theoretically improves model generalization. In the fine-tuning phase, we utilize this learned tree vocabulary to unify various graph-related tasks into computation tree classification, thereby preventing negative transfer [89; 87]. Extensive experimental results demonstrate the effectiveness of GFT in graph learning on cross-task and cross-domain datasets.

## 2 Rethinking Transferable Patterns on Graphs

### Transferability on GNNs

Transferability refers to a model's capability to extract patterns from source tasks and apply this knowledge to enhance performance on related target tasks [5; 33; 90]. Understanding transferable patterns is essential for developing graph foundation models. Early research focuses on analyzing transferability through the perspectives of graph spectrum [41; 42] and subgraphs/substructures , defining transferability as model invariance to minor permutations on the graph. A more recent study  investigates the transferable vocabulary in graphs by identifying key substructures relevant to various tasks. For instance, they find that triadic closure, homophily, and heterophily are vital for node classification; local and global structural proximities are crucial for link prediction; and certain motifs , such as triangles, \(k\)-cliques, and stars, serve as fundamental components for graph classification. Another line of research [62; 8; 69] incorporates graphon theory to provide a theoretical basis for transferability. Specifically, Ruiz et al.  establish a bound on the embeddings of two graphs sampled from the same graphon. Cao et al.  expand this to include pre-training and fine-tuning scenarios, assessing the distance between graphs based on their alignment within the graphon space. However, the stringent assumptions of graphon theory limit its practical application in the design of graph foundation models.

We identify two primary limitations in analyzing transferable patterns on graphs: (1) While certain domains [110; 87; 43; 108] or tasks [114; 111; 19] exhibit transferable patterns, the challenge of identifying universally transferable substructures is difficult. (2) More critically, basic message-passing GNNs, constrained by the 1-WL test [94; 52], fail to recognize certain subgraphs (or motifs) [20; 103; 10], such as stars, conjoint cycles, and \(k\)-cliques, as well as heterophily patterns . This limitation in recognizing substructures impedes using subgraphs as transferable tokens in graph vocabulary [68; 30; 45]. More related works are elaborated in Appendix A.

### Computation Tree as Transferable Pattern

In this paper, we rethink the transferable pattern in graphs as the computation tree -- a specialized subtree pattern that emerges from unfolding the message-passing process . This pattern is demonstrably effective at capturing critical localized information within the graph [20; 52; 94; 12]. Treating computation trees as tokens within a graph vocabulary offers two distinct advantages: (1) computation trees preserve the essential structural information of the graph, which is learnable through message-passing GNNs, and (2) the computation tree structure occurs across various graph-based tasks. These tasks can be unified as computation tree classification by integrating a virtual node, as shown in Figure 1.

Before diving into transferability analysis, we first establish the necessary notations. Consider a graph \(=(,)\) composed of node set \(\) and edge set \(\). Each node \(v\) is associated with a feature vector \(_{v}^{d}\) and a computation tree \(_{v}\) with \(L\) layers. A GNN encoder \(\) processes these computation trees as inputs, producing embeddings for root nodes \(=(_{v})^{d^{}}\).

**Definition 2.1** (Computation Trees ).: Given a graph \(=(,)\), define \(_{v}^{1}=v\) and \(_{v}^{L}\) as the \(L\)-layer computation tree. This tree is constructed by recursively integrating the subtrees of neighborhoods. The multiset of \(L\)-layer computation trees on graph \(\) is denoted by \(_{}^{L}:=\{_{v}^{L}\}_{v}\).

Figure 1 demonstrates the construction of computation trees across various graph tasks, including node-, link-, and graph-level tasks. These trees capture essential localized subtree patterns within the graphs [55; 64; 12]. If the \(L\)-layer computation trees for two nodes are similar, it indicates that these nodes share similar neighborhoods, suggesting they represent analogous phenomena . Thus, it is rational to assess transferability of computation trees by examining the stability of GNNs in producing analogous embeddings for similar trees [62; 42].

**Theorem 2.2** (Transferability of Computation Tree ).: _Given two \(L\)-layer computation trees \(_{v_{1}},_{v_{2}}\) derived from the graph \(\) and a GNN encoder \(\), the Euclidean distance between the tree embeddings \(\|(_{v_{1}})-(_{v_{2}})\|_{2}\) is bounded as follows:_

\[_{1}\|_{v_{1}}-_{v_{2}}\|_{2}+ _{2}_{j(v)}_{v_{1},v_{2},j}^{L-1} 2 _{}(_{1}+_{l=1}^{L}_{2}^{l}D_{ l})\] (1)

Figure 1: Graph tasks (top) and the corresponding computation trees (bottom). A virtual node can be added at the top to connect all task-relevant nodes, unifying different tasks as the tree-level task.

_where \(_{v_{1},v_{2},j}^{L-1}\) represents the distance between the \((L-1)\)-layer subtrees of the \(j\)-th children of nodes \(v_{1}\) and \(v_{2}\). \(_{1},_{2}\) are constants, and \(_{}\) denote bounded norm of \(\). The variable \(d_{l}\) indicates the number of children in the \(l\)-layer subtrees, and \(D_{l}=d_{l}d_{l-1}...d_{1}\)._

Proof.: All proofs in the paper are detailed in Appendix D. 

_Remark 2.3_.: Theorem 2.2 derives a recursive bound for computation tree similarity. In particular, the distance between two computation trees is closely correlated to the similarity of their subtrees, where higher subtree similarity results in a closer distance. This suggests that computation trees with similar structures are likely to have similar embeddings, which enhances their transferability . This aligns with our empirical observations that higher computation tree similarity between two graphs leads to improved transferability.

**Supportive Observations -- Synthetic Graphs.** Figure 3 shows that high computation tree similarity between graphs correlates with improved transfer learning performance on synthetic graphs (Figure 2). Specifically, we construct three distinct graphs: \(_{1}\) and \(_{2}\) share similar motifs but differ in computation tree distributions, while \(_{1}\) and \(_{3}\) exhibit dissimilar motifs but similar computation tree distributions. We employ the WL subtree kernel  and the graphlet sampling kernel  to assess tree and motif similarity, respectively, and utilize the inverse of the Central Moment Discrepancy  to measure transferability. Further details on experimental settings and additional results are available in Appendix E.1. Our findings indicate that transferability is strongly associated with computation tree similarity rather than motif similarity, regardless of the scale of graphs (# blocks).

**Supportive Observations -- Real-world Graphs.** Table 1 validates the correlation between computation tree similarity and transferability in real-world graphs, including homophily Airport networks  and heterophily WebKB networks . We evaluate transferability based on transfer learning performance in node classification tasks. Detailed experimental settings and additional results are available in Appendix E.2. Our findings in real-world graphs corroborate those in synthetic graphs: higher computation tree similarity enhances transferability, while the impact of motifs is marginal, no matter using original node features (Table 1) or randomized node features (Table 9).

## 3 GFT: Graph Foundation Model with Transferable Tree Vocabulary

We develop GFT, a cross-domain and cross-task graph foundation model that leverages computation trees as transferable patterns within graph vocabulary. As illustrated in Figure 4, GFT undergoes

   \(_{target}\) &  &  &  \\  \(_{source}\) & Europe & USA & Brazil & USA & Brazil & Europe \\  Motif Sim. & 99.01 & 92.65 & 99.00 & 96.81 & 92.68 & 96.81 \\ Acc. / Tree Sim. & 53.1 / 34.6 & 56.8 / 62.2 & 50.8 / 34.6 & 51.4 / 88.7 & 54.5 / 62.2 & 57.9 / 88.7 \\   \(_{target}\) &  & Texas &  \\  \(_{source}\) & Texas & Wisconsin & Cornell & Wisconsin & Cornell & Texas \\  Motif Sim. & 99.97 & 99.98 & 99.99 & 99.99 & 99.98 & 99.99 \\ Acc. / Tree Sim. & 46.5 / 65.3 & 42.4 / 42.7 & 56.0 / 65.3 & 53.1 / 41.7 & 48.6 / 42.7 & 48.2 / 41.7 \\   

Table 1: Transfer learning performance on homophily (above) and heterophily (below) graphs. For any target graph, source graphs with higher tree similarity lead to improved accuracy, highlighted with Blue. Conversely, the influence of motif similarity is marginal, marked by LightBlue.

Figure 2: Synthetic graphs composed of two basic blocks. More blocks can scale up the graph sizes. Figure 3: Transfer performance on synthetic graphs with \(_{1}\) as the target graph. Higher tree similarity correlates with enhanced transferability.

pre-training through a computation tree reconstruction task to acquire general knowledge from a cross-domain graph database. Subsequently, GFT quantizes the embedding space of computation trees to form a discrete tree vocabulary, encapsulating fundamental, transferable computation tree patterns for diverse tasks. In the fine-tuning phase, GFT utilizes this tree vocabulary to unify graph-related tasks (including node-, link-, and graph-level tasks) as computation tree classification, adapting the general knowledge to specific target tasks.

### Pre-training with Computation Tree Reconstruction

The pre-training stage focuses on learning general computation tree patterns on graphs, facing two primary challenges: (i) obtaining transferable patterns, and (ii) comprehensively capturing computation tree knowledge. For the first challenge, we learn a discrete tree vocabulary by quantizing the embedding space of computation trees . For the second challenge, we introduce a computation tree reconstruction task that considers multiple aspects of computation trees.

**Learning Tree Vocabulary.** The idea of learning a discrete computation tree vocabulary originates from the principles of sparse distributed memory in cognitive science , which stores and retrieves memory in a distributed manner. By adopting these principles, the tree vocabulary maintains a set of tokens that are reusable and adaptable across various tasks, improving model transferability.

We adopt the Vector Quantization (VQ)  to develop the tree vocabulary. Given a graph database2\(=\{_{i}\}_{i=1}^{n}\), we randomly extract a set of computation trees \(=\{_{i}\}_{i=1}^{m}\) and employ a GNN encoder \(\) to generate the tree embeddings \(=\{_{i}\}_{i=1}^{m}\). We define the computation tree vocabulary as a set of learnable tokens \(=\{_{1},...,_{K}\}\). The tree embedding space is quantized by assigning each embedding to the nearest token, resulting in quantized tree embeddings \(_{i}=_{j}\), where \(j=*{arg\,min}_{j}\|_{i}-_{j}\|_{2}\). We optimize this projection by back-propagating the reconstruction error to the tree vocabulary \(\) and applying a straight-through gradient estimator  to the encoder \(\). In particular, we jointly optimize vocabulary loss and commitment loss , along with tree reconstruction loss (discussed later), where the former updates the token vectors \(\) using the fixed quantization \(\), and the latter ensures alignment between the tokens in the vocabulary and the quantized tree embeddings, serving as a regularizer. The pre-training objective is thus defined as:

\[_{pretrain}=_{tree}+_{i=1}^{m }[_{i}]-_{i}_{2}^{2} }_{}+_{1}_{i=1}^{m }_{i}-[_{i}]_{2}^{2}}_ {},\] (2)

where \([]\) denotes the stop-gradient operator and \(_{1}\) is the weight.

Figure 4: During pre-training, GFT encodes general knowledge from a graph database into a tree vocabulary through tree reconstruction. In fine-tuning, the learned tree vocabulary is applied to unify graph-related tasks as tree classification, adapting the general knowledge to specific tasks.

**Computation Tree Reconstruction.** We introduce a computation tree reconstruction task designed to enable a deep understanding of the structural and semantical attributes of computation trees . We use the tree tokens to reconstruct the original computation tree, retaining general knowledge while discarding irrelevant details. Specifically, we develop three reconstruction tasks: (i) reconstructing the features of the root node \(_{feat}\), (ii) reconstructing the connectivity among nodes in the computation trees \(_{topo}\), and (iii) reconstructing the overall semantics of the computation trees \(_{sem}\):

\[_{feat} =_{i=1}^{m}}_{i}^{2}- _{i}_{2}^{2}, _{sem} =_{i=1}^{m}1-}_{i}^{ }}_{i}}{\|}_{i}^{4}\|\| }_{i}\|}^{},\] (3) \[_{topo} =_{(i,j),(i,j^{})}-|}(}_{i}^{}}_{j}^{})-}|} 1-(}_{i}^{}}_{j^{}}^{ })+|}[_{i}^{4}\| _{j}^{4}]-_{ij}_{2}^{2},\]

where \(}_{i}=(_{i})\) represents the semantics of the original computation trees, and \(\) is updated through a moving average of the tree encoder \(\). The quantized tree embedding \(\) is projected via different decoders defined by MLP, \(}^{j}=_{j}()\), \(\) is the scaling factor, and \(\) and \(}\) represent sets of existing and non-existing connections in computation trees, respectively. \(_{ij}\) denotes the edge embedding between nodes \(i\) and \(j\). By jointly optimizing these tasks, we establish a comprehensive reconstruction objective:

\[_{tree}=_{2}_{feat}+_{3}_ {sem}+_{4}_{topo},\] (4)

where \(_{i}\) indicates the weights of respective losses. The philosophies under these loss functions separately correspond to existing works [39; 26; 74; 86]. For example, Kipf and Welling  reconstruct the graph structure, aligning to the philosophy of \(_{topo}\), Hou et al.  reconstruct node feature that is similar to \(_{feat}\), and Thakoor et al. , Wang et al.  employ contrastive learning to maximize the alignment between two views, aligning to \(_{sem}\). Unlike existing methods that typically focus on reconstructing a single aspect of computation trees, GFT integrates multiple facets  to learn a general and transferable tree vocabulary.

**Enhancing the Quality of Tree Vocabulary.** The effectiveness of GFT is correlated to the quality of the tree vocabulary, which should be both comprehensive and expressive. A comprehensive vocabulary is inclusive enough to accommodate new patterns, while an expressive vocabulary ensures that different tree tokens do not overlap in representation . To enhance comprehensiveness, we augment the computation trees during pre-training, increasing the variety of observed computation trees through node feature augmentation and structural augmentation, as described by . To improve expressiveness, we regularize the tree vocabulary space by intentionally increasing the distance between distinct tokens . Specifically, we introduce an orthogonal regularizer designed to maintain tree tokens orthogonal to each other, effectively expanding the tree token space:

\[_{ortho}=}^{T}- _{K}_{F}^{2},=[_{1},..., _{K}]^{T}^{K d^{}},\] (5)

where \(_{i}\) is tree token, \(_{K}\) is the identity matrix for \(K\) dimensions, and \(\|\|_{F}\) denotes the Frobenius norm. The orthogonal loss \(_{ortho}\) is integrated with Equation 2. More analysis is in Appendix C.2.

### Fine-tuning with Computation Tree Classification

The pre-training stage encodes general knowledge into the tree vocabulary, while the fine-tuning phase adapts this knowledge to specific tasks. This adaptation is challenging because identical patterns can have different meanings across domains and tasks . For example, a triangular structure indicates stable relationships in social networks (node classification) but denotes unstable chemical properties in molecular networks (graph classification). To this end, we propose computation tree classification that utilizes the tree vocabulary to unify graph tasks as the tree-level task, ensuring the adaptation is applicable across diverse tasks and domains.

**Reformulate Graph Tasks as Computation Tree Classification.** Graph-related tasks can be represented by task-specific computation trees, as illustrated in Figure 1. Specifically, for node classification, the task-specific computation tree, denoted as \(_{node}=_{i}\), is derived directly from the node itself, resulting in the embedding \(=(_{i})\). For link prediction, the computation tree, \(_{link}=(_{s},_{t})\), merges the computation trees of two nodes of the edge, with the embedding\(=((_{s}),(_{t}))\). For graph classification, the task-specific computation tree \(_{graph}=(\{_{v}\}_{v})\) integrates the computation trees of all nodes within the graph, and computes the embedding as \(=(\{(_{v})\}_{v})\). Subsequently, the embeddings of these task-specific trees are used to query the tree vocabulary and then make predictions, adapting the general knowledge encoded in the vocabulary to various tasks and domains.

**Prototype Classifier.** The prototype classifier \(f_{proto}\) constructs class prototypes using tokens from the tree vocabulary. Given a set of task-specific computation trees \(\{(_{i},y_{i})\}_{i=1}^{n}\) with \(|C|\) classes, we employ the pre-trained GNN encoder \(\) to generate tree embeddings \(=\{_{i}\}_{i=1}^{n}\). These embeddings are then used to query the tree vocabulary and produce quantized embeddings \(=\{_{i}\}_{i=1}^{n}\). Subsequently, we construct a class-wise memory bank \(=\{^{1},...,^{|C|}\}\), where \(^{k}=\{_{i}|y_{i}=k\}\), to store tree tokens of the same class. The memory bank typically includes all instances from the training set. From this, we derive a set of prototypes for each class \(\{_{k}\}_{k=1}^{|C|}\), calculated as \(_{k}=(1/|^{k}|)_{_{i}^{k}} _{i}\). These prototypes are then used for predictions:

\[p(y=k|)=(,_{k})/)}{ _{c}(-(,_{c})/)},\] (6)

where \(()\) denotes the cosine distance and \(\) is a temperature scaling factor. We optimize the cross-entropy loss between the classifier's output and the ground truth to update the encoder \(\).

**Linear Classifier.** Different from the prototype classifier, which utilizes class prototypes to adapt to target tasks, the linear classifier \(f_{lin}\) directly applies the knowledge encoded in each tree token. Specifically, given a task-specific computation tree \(_{i}\), we use the encoder to generate tree embeddings \(_{i}\) and then query the tree vocabulary to retrieve \(_{i}\). These embeddings are used for predictions as:

\[p(y=k|)=^{k}()/)}{_{c}( ^{c}()/)},\] (7)

We optimize the cross-entropy loss between the prediction \(f_{lin}()\) and the ground truth to update the parameters of the encoder and the linear classifier. During inference, predictions from both the prototype and linear classifiers are combined to form the final output. It is important to note that the tree vocabulary remains fixed during fine-tuning to preserve the integrity of the encoded knowledge.

### Additional Analysis

**Tree Vocabulary Learns Generalizable Tokens.** Learning tree vocabulary via VQ involves clustering within the embedding space of computation trees, utilizing a margin-aware classifier  that assigns each computation tree to a specific cluster. Assuming that each computation tree \(\) is associated with an underlying clustering label \(y\), and that each pair \((_{i},y_{i})\) is sampled from the distribution \(_{}\), we derive the following theorem:

**Theorem 3.1**.: _Given a set of computation trees \(\{(_{i},y_{i})\}_{i=1}^{n}\) sampled from the distribution \(_{}\), the VQ process functions as a margin-aware prototype classifier \(f\) that predicts the class of computation trees via a distance measure. The risk \((f)\) of classifier \(f\) can be bounded with probability \(1-\):_

\[(f)}(f)+ p(p-1) }{ n}+},\] (8)

_where \(}(f)\) is the empirical risk, \(p\) denotes the number of tokens, \(\) is a constant, and \(\) acts as the margin, serving as a penalty factor in evaluating the distance between computation trees and tokens._

_Remark 3.2_.: The generalizability of tokens within the vocabulary highly correlates to the margin \(\), the number of observed computation trees \(n\), and the number of tokens \(p\). (i) A larger margin \(\) results in a tighter bound by ensuring higher inter-cluster distances and lower intra-cluster distances. This supports the use of an orthogonal regularizer (Equation 5) that explicitly pushes tokens apart, enhancing cluster distinction. (ii) An increased number of observed computation trees \(n\) leads to a tighter generalization bound, which supports the use of augmentations to increase the diversity of computation trees. (iii) More tokens \(p\) may loose the upper bound of the generalization error, potentially due to a higher risk of overfitting. This aligns with our experimental findings that more tokens do not necessarily lead to improved performance (Section 4.4).

**Tree Vocabulary Mitigates Negative Transfer.** Negative Transfer (NT) occurs when the pre-training process degrades model performance on a target task. This issue often results from misalignment between the pre-training and fine-tuning tasks [89; 8]. Following the approach in , we characterize the NT gap, \((S,T)-(,T)\), as the risk gap on task \(T\) with (\((S,T)\)) and without (\((,T)\)) pre-training on task \(S\), where a smaller NT gap indicates improved transferability. As illustrated in Figure 5, employing the learned tree vocabulary to align the tree reconstruction task in pre-training and tree classification task in fine-tuning can significantly mitigate negative transfer.

**Complexity Analysis.** A comprehensive complexity analysis of GFT is provided in Appendix B. Notably, GFT employs a single GNN to decompose and encode computation trees, taking \((L(|| d+|| d^{2}))\). In contrast, subgraph-based GFMs [30; 45] require the explicit extraction of subgraphs for each node, taking additional \((||^{3})\) using adjacency matrix-based BFS. This contrast highlights the efficiency of using computation trees as transferable patterns in terms of time complexity. More discussions are in Appendix C.

## 4 Experiments

### Experimental Setting

We employ cross-domain and cross-task graph datasets to evaluate the effectiveness of GFT. For node-level tasks, we utilize citation networks such as Cora, PubMed, Arxiv, and the web link network WikiCS. For edge-level tasks, we include two Knowledge Graphs (KGs), WN18RR and FB15K237. For graph-level tasks, we use molecule networks, including HIV, PCBA, and ChEMBL. All preprocessing steps follow . We take various baselines, encompassing MLP, supervised GNNs such as GCN , GAT , GIN , and self-supervised methods like BGRL , GraphMAE , GIANT , and GFMs including Prodigy  and OFA . We replicate each experiment ten times and report the average performance to minimize the influence of randomness. Further details on experimental settings are available in Appendix F.

### Effectiveness on Cross-Domain and Cross-Task Datasets

**Pre-training and Fine-tuning.** Table 2 demonstrates the model performance across cross-domain and cross-task datasets in pre-training and fine-tuning setting. We evaluate the effectiveness of graph foundation models [30; 45] in the following few-shot setting due their distinctive training mechanisms, such as in-context pre-training  and fully supervised training . For supervised baselines, models are trained directly on the target graph; for self-supervised methods, we pre-train across all datasets before adapting to the specific target graph. Our approach demonstrates a substantial performance improvement, exceeding the best baseline by an average of over 6%. Specifically, our method outperforms the best baseline by 2% across three datasets and by 5% across another three datasets. This underscores the effectiveness of using computation trees as transferable patterns.

**Few-shot Learning** Table 3 presents the few-shot learning performance of GFT compared to self-supervised methods [74; 26; 11] and graph foundation models [30; 45; 25]. We randomly select \(k\) samples per way from the training set for fine-tuning3. This method is similar to Prodigy , and is much more label-efficient than OFA  with supervised pre-training. Despite the extremely limited labels for fine-tuning, GFT significantly surpasses existing methods, showing the fast adaptation capability. Appendix H shows more fine-tuning instances can significantly improve performance.

### Transferability

Table 5 shows the impact of different pre-training datasets under the pre-training and fine-tuning setting, where comprehensive results (including the following ablation studies) are available in Appendix I. The performance for specific tasks (node-, link-, graph-level) represent the average across all involved datasets. We examine three scenarios with distinct pre-training datasets: (i) all

Figure 5: Negative transfer gap on Cora in node classification.

datasets, (ii) only the target dataset, and (iii) datasets excluding the target dataset. These variants are compared against GAT and GIANT, which represent the best supervised and self-supervised baselines, respectively. Notably, GFT consistently outperforms all baselines, regardless of the pre-training dataset utilized. Interestingly, performance improves when using datasets excluding the target dataset compared to pre-training solely on the target dataset. We hypothesize that the computation trees from the non-target datasets provide sufficient information to facilitate the learning of a transferable tree vocabulary, thereby promoting positive transfer.

We further evaluate the impact of various combinations of pre-training datasets on the target tasks, as depicted in Figure 7. For pre-training, we select FB15K237, Arxiv, and ChEMBL, while Cora, WikiCS, WN18RR, and HIV serve as the target datasets. Our findings indicate that an increased number of pre-training datasets consistently enhances performance across all target datasets. However, for existing GFMs, transferability closely correlates with the selection of pre-training datasets, with more datasets sometimes leading to negative transfer [25; 43]. This observation underscores the adaptability of using computation trees as transferable patterns in graph vocabulary.

### Ablation Study

**Tree Reconstruction and Classification.** Table 4 shows the impact of various reconstruction tasks in pre-training and tree classifiers in fine-tuning. All reconstruction tasks enhance model performance compared to models without pre-training. Notably, semantic reconstruction is most effective for node-level and graph-level tasks due to its comprehensive consideration of node features and graph structures. Feature reconstruction is particularly beneficial for link-level tasks, as it preserves the original node semantics, which are crucial for KGs. The optimal performance is achieved when three tasks are jointly optimized, aligning with findings in Ju et al. . Similarly, the combination of prototype and linear classifiers in tree classification leads to superior performance. Furthermore, removing strategies designed to enhance the quality of the tree vocabulary results in model degradation across all settings (Appendix I.3).

    &  &  &  \\  Method & Cora & PubMed & Wiki-CS & Arxiv & WH18RR & FB15K237 & HIV & PCBA & _Avg._ \\  Linear & 58.03 & 68.66 & 70.36 & 66.50 & 78.50 & 87.39 & 66.37 & 72.30 & 71.01 \\ GCN  & 75.65 & 75.61 & 75.28 & 71.40 & 73.79 & 82.22 & 64.84 & 71.32 & 73.76 \\ GAT  & 76.24 & 74.86 & 76.78 & 70.87 & 80.16 & 88.93 & 65.54 & 70.12 & 75.44 \\ GIN  & 73.59 & 69.51 & 49.77 & 65.05 & 74.02 & 83.21 & 66.86 & 72.69 & 69.34 \\  DGI  & 72.10 & 73.13 & 75.32 & 69.15 & 75.75 & 81.34 & 59.62 & 63.31 & 71.22 \\ BGRL  & 71.20 & 75.29 & 76.53 & 71.19 & 75.44 & 80.66 & 63.95 & 67.09 & 72.67 \\ GraphMAE  & 73.10 & 74.32 & 77.61 & 70.90 & 78.99 & 85.30 & 61.04 & 63.30 & 73.07 \\ GIANT  & 75.13 & 72.31 & 76.56 & 70.10 & 84.36 & 87.45 & 65.44 & 61.49 & 74.11 \\  GFT & **78.62\({}^{*}\)** & **77.19\({}^{*}\)** & **79.39\({}^{*}\)** & **71.93\({}^{*}\)** & **91.91\({}^{}\)** & **89.72** & **72.67\({}^{}\)** & **77.90\({}^{}\)** & **79.92\({}^{}\)** \\   

Table 2: Model performance in pre-training and fine-tuning setting. **Bold** and underline highlight the best and sub-best performance, and \({}^{*}\) and \({}^{}\) denote a 2% and 5% improvement over the best baseline. The model performance with standard deviation is in Appendix G.

    &  &  &  &  \\  Method & 5-shot & 3-shot & 1-shot & 5-shot & 3-shot & 1-shot & 5-shot & 3-shot & 1-shot \\  BGRL  & - & 17.98 & - & - & 48.43 & - & - & 29.24 & - & - & 67.23 & - \\ GraphMAE  & - & 19.12 & - & - & 49.24 & - & - & 32.07 & - & - & 69.75 & - \\ GIANT  & - & 20.12 & - & - & 54.33 & - & - & 52.63 & - & - & 77.21 & - \\  Prodigy  & 25.51 & 23.69 & 21.44 & 61.09 & 58.64 & 48.23 & 62.03 & 59.58 & 54.30 & 84.30 & 79.61 & 66.10 \\ OFA  & 24.01 & 22.13 & 21.34 & 59.92 & 58.68 & 52.80 & 65.51 & 65.76 & 63.48 & 83.64 & 83.14 & 83.46 \\ GFT & **36.29\({}^{}\)** & **34.36\({}^{}\)** & **62.49\({}^{}\)** & **68.00\({}^{}\)** & **66.00\({}^{}\)** & **58.20\({}^{}\)** & **75.01\({}^{}\)** & **74.56\({}^{}\)** & **74.97\({}^{}\)** & **89.13\({}^{}\)** & **88.53\({}^{}\)** & **88.07\({}^{}\)** \\    & Cora 5-way & HIV 2-way \\  Method & 5-shot & 3-shot & 1-shot & 5-shot & 3-shot & 1-shot & 5-shot & 1-shot & 10-shot & 5-shot & 1-shot \\  OFA  & 32.64 & 30.56 & 25.82 & 42.28 & 31.28 & 23.68 & 54.36 & 57.56 & 57.17 & 54.58 & 54.80 & 54.92 \\ GFT & **35.50\({}^{}\)** & **35.50\({}^{}\)** & **35.33\({}^{}\)** & **52.30\({}^{}\)** & **51.47\({}^{}\)** & **49.80\({}^{}\)** & **58.67\({}^{}\)** & **58.78\({}^{}\)** & **59.94\({}^{}\)** & **59.34\({}^{}\)** & **59.34\({}^{}\)** & **55.88** \\   

Table 3: Few-shot learning performance. Additional results with more baselines are in Appendix H.

**Tree Vocabulary.** Table 6 shows the importance of the vocabulary, where the use of vocabulary significantly enhances model performance, particularly in link- and graph-level tasks, which aligns to the theoretical analysis that the tree vocabulary improves generalization. However, we observe that increasing the number of tokens in the vocabulary does not necessarily enhance model performance; indeed, the improvements are often marginal.

## 5 Conclusion

**Conclusion.** In this paper, we rethink the transferable pattern in graphs as computation trees and validate their transferability both empirically and theoretically. Building on this insight, we propose a cross-domain and cross-task GFM named GFT. This model leverages computation tree reconstruction to acquire general graph knowledge from cross-domain datasets and uses computation tree classification to facilitate adaptation to various target tasks. In future work, we aim to explore its capabilities for in-context learning and zero-shot learning.

**Limitations.** In this paper, we focus primarily on message-passing GNNs, as message-passing can be naturally unrolled as a tree-like structure. However, our analysis excludes graph transformers and expressive GNNs with specialized computational architectures. We plan to extend our analysis to understand the transferable patterns of these advanced learning algorithms in future work. Additionally, message-passing GNNs may lack the expressiveness needed to address isomorphism problems in graphs. One can apply advanced techniques  to handle link isomorphism and use advanced expressive GNNs  to tackle graph isomorphism. Moreover, the deployment of GFT in real-world applications may encounter efficiency issues, which can be mitigated by techniques like [106; 88].

**Boarder Impact.** The proposed GFT is a cross-domain and cross-task graph foundation model, designed for rapid adaptation to target tasks with extremely limited labels. We wish our research can support applications where label acquisition is challenging and model training is time-consuming, such as in molecular discovery and financial fraud detection.

    & Node & Link & Graph & Avg. \\   \\  \# Tokens = 128 & 76.78 & 90.82 & **75.29** & 79.92 \\ \# Tokens = 256 & 76.71 & **90.86** & 75.17 & 79.86 \\ \# Tokens = 512 & **76.94** & **90.86** & 75.21 & **79.99** \\   \\  w/o. Vocab. & 75.90 & 86.70 & 69.17 & 76.91 \\   

Table 6: The impact of tree vocabulary.

    & Node & Link & Graph & Avg. \\   \\  n/a & 72.52 & 47.30 & 73.83 & 66.54 \\ w. \(_{sem}\) & 76.25 & 90.39 & 74.99 & 79.47 \\ w. \(_{feat}\) & 75.85 & 90.42 & 74.42 & 79.13 \\ w. \(_{topo}\) & 75.50 & 90.28 & 74.57 & 78.96 \\   \\  w. \(f_{proto}\) & 76.64 & 38.71 & 59.89 & 62.97 \\ w. \(f_{lin}\) & 75.51 & 88.99 & 72.21 & 78.06 \\  GFT & **76.78** & **90.82** & **75.29** & **79.92** \\   

Table 4: Ablation on tree reconstruction (above) and tree classification (bottom).

    & Node & Link & Graph & Avg. \\  GAT  & 74.69 & 84.55 & 67.83 & 75.44 \\ GIANT  & 73.53 & 85.91 & 63.47 & 74.11 \\   \\  All Datasets & **76.78** & **90.82** & **75.29** & **79.92** \\ Target Dataset & 76.12 & 90.67 & 74.08 & 79.25 \\ Remaining Datasets & 75.94 & 90.71 & 74.86 & 79.36 \\   

Table 5: The impact of pre-training datasets.

Figure 7: GFT consistently improves model performance with more pre-training datasets.