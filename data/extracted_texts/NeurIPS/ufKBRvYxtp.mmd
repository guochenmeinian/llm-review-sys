# Sample-Efficient Agnostic Boosting

Udaya Ghai

Amazon

ughai@amazon.com

&Karan Singh

Tepper School of Business

Carnegie Mellon University

karansingh@cmu.edu

###### Abstract

The theory of boosting provides a computational framework for aggregating approximate weak learning algorithms, which perform marginally better than a random predictor, into an accurate strong learner. In the realizable case, the success of the boosting approach is underscored by a remarkable fact that the resultant sample complexity matches that of a computationally demanding alternative, namely Empirical Risk Minimization (ERM). This in particular implies that the realizable boosting methodology has the potential to offer computational relief without compromising on sample efficiency.

Despite recent progress, in agnostic boosting, where assumptions on the conditional distribution of labels given feature descriptions are absent, ERM outstrips the agnostic boosting methodology in being quadratically more sample efficient than all known agnostic boosting algorithms. In this paper, we make progress on closing this gap, and give a substantially more sample efficient agnostic boosting algorithm than those known, without compromising on the computational (or oracle) complexity. A key feature of our algorithm is that it leverages the ability to reuse samples across multiple rounds of boosting, while guaranteeing a generalization error strictly better than those obtained by blackbox applications of uniform convergence arguments. We also apply our approach to other previously studied learning problems, including boosting for reinforcement learning, and demonstrate improved results.

## 1 Introduction

A striking observation in statistical learning is that given a small number of samples it is possible to learn the best classifier from an almost exponentially large class of predictors. In fact, it is possible to do using a conceptually straightforward procedure - Empirical Risk Minimization (ERM) - that finds a classifier that is maximally consistent with the collected samples. Substantiating this observation, the fundamental theorem of statistical learning (e.g., ) states that with high probability ERM can guarantee \(-\)excess population error with respect to the best classifier from a finite, but large hypothesis class \(\) given merely \(m_{}(||)/^{2}\) identically distributed and independent (IID) samples of pairs of features and labels from the population distribution. Under an additional assumption - that of _realizability_ - guaranteeing that there exists a perfect classifier in the hypothesis class achieving zero error, a yet quadratically smaller number \(m_{}(||)/\) of samples suffice. In the absence of such assumption, the learning problem is said to take place in the _agnostic_ setting, i.e., under of a lack of belief in the ability of any hypothesis to perfectly fit the observed data.

This ability to generalize to the population distribution and successfully (PAC) learn from an almost exponentially large, and hence expressive, hypothesis class given limited number of examples suggests that the primary bottleneck for efficient learning is computational. Indeed, even with modest sample requirements, finding a maximally consistent hypothesis within an almost exponentially large class via, say, enumeration or global search, is generally computationally intractable.

It is against this backdrop that the theory of boosting offers a compelling alternative. The starting point is the realization that often, both in practice and in theory, it is easy to construct simple, yet inaccurate _rules-of-thumbs_ (or _weak learners_) that perform ever so slightly better than a random classifier. A natural question then arises (paraphrased from 's abstract): can one convert such mediocre learning rules into one that performs extremely well? Boosting algorithms offer a positive resolution to this question by providing convenient and computationally efficient reductions that aggregate such weak learners into a proficient learner with an arbitrarily high accuracy.

Realizable Boosting.Consider the celebrated Adaboost algorithm  which operates in the (noiseless) realizable binary classification setting. On any distribution consistent with a fixed labeling function (or _concept_), a weak learner here promises an accuracy strictly better than half, since guessing labels randomly gets any example correct with probability half. Given access to such a weak learner and \(m_{}(||)/\) samples1, Adaboost makes \( 1/\) calls to the weak learner to produce a classifier with (absolute) error at most \(\) on any distribution consistent with the same labeling function. Thus, not only is Adaboost computationally efficient provided, of course, such weak learners can be found, but also its sample complexity is no worse than that of ERM. _This underscores the fact that the realizable boosting methodology has the potential to offer computational relief, without compromising on sample efficiency._

Agnostic Boosting.In practice, realizability is an onerous assumption; it is too limiting for the observed feature values alone to determine the label completely and deterministically, and that such a relation can be perfectly captured by an in-class hypothesis. Agnostic learning forgoes such assumptions. In their absence, bounds on absolute error are unachievable, e.g., when labels are uniformly random irrespective of features, no classifier can achieve accuracy better than half. Instead, in agnostic learning, the goal of the learner is to output a hypothesis with small _excess_ error with respect to the best in-class classifier. If an in-class hypothesis is perfect on a given distribution, this relative error translates to an absolute error bound, thus generalizing the realizable case perfectly. Indeed, such model agnosticism has come to be a lasting hallmark of modern machine learning.

Early attempts at realizing the promise of boosting in the agnostic setting were met with limited success: while they did boost the weak learner's accuracy, the final hypothesis produced was not competitive with the best in-class hypothesis. We survey some of these in the related work section. A later result, and the work most related to ours, is due to . A weak learner in this setting returns a classifier with a correlation against the true labels that is \(\) (say \(0.1\)) times that of the best in-class

   & Sample Complexity & Oracle Complexity \\ 
 & \((||)/^{4}^{4}\) & \(1/^{2}^{2}\) \\ 
 & \((||)/^{4}^{6}\) & \(1/^{2}^{2}\) \\  Theorem 4 & \((||)/^{3}^{3}\) & \((||)/^{2}^{2}\) \\  Theorem 9 (in Appendix B) & \((||)/^{3}^{3}+(||)^{3}/^ {2}^{2}\) & \(1/^{2}^{2}\) \\  ERM (no boosting) & \((||)/^{2}^{2}\) & \(+\) (inefficient) \\  

Table 1: A comparison between sample and oracle complexities (i.e., number of weak learning calls) of the present results and previous works, in each case to achieve \(\)-excess population error. Here we suppress polylogarithmic factors. We make progress on closing the sample complexity gap between ERM, which is computationally inefficient, and boosting-based approaches. The \(\)-weak leaner outputs a hypothesis from the base class \(\), which is usually substantially smaller than \(\) against which the final agnostic learning guarantee holds. In practice, boosting is used with learners with small values of \(||\). See Definition 1 for details. See the paragraph following Theorem 1 in  and Section 3.3 in  for derivation of these bounds. See also Theorem 2.14 in  for a bound on the expressivity of the boosted class to derive ERMâ€™s sample complexity.

hypothesis. Random guesses of the labels produce a correlation of zero; hence, a weak classifier interpolates the performance of the best in-class hypothesis with that of a random one. Given access to such a weak learner, the boosting algorithm of  makes \( 1/^{2}\) calls to the weak learner and draws \(m_{}(||)/^{4}\) samples to produce a learning rule (not necessarily in the hypotheses class, hence _improper_) with \(\)-excess error. The dependency of the sample complexity in the target accuracy is thus quadratically worse than that of ERM. This gap persists untarnished for other known agnostic boosting algorithms too. _In this work, we seek to diminish this fundamental gap and construct a more sample-efficient agnostic boosting algorithm._

Our main result is an efficient boosting algorithm that upon receiving \((||)/^{3}\) samples produces an _improper_ learning rule with \(\)-excess error on the population loss. This is accomplished by the careful reuse of samples across rounds of boosting. We also extend these guarantees to infinite function classes, and give applications of our main result in reinforcement learning, and in agnostically learning halfspaces, in each case improving known results. We detail key contributions and technical challenges in achieving them next.

### Contributions and technical innovations

**Contribution 1: Sample-efficient Agnostic Booster.** We provide a new potential-based agnostic boosting algorithm (Algorithm 1) that achieves \(\)-excess error when given a \(\)-weak learner operating with a base class \(\). In Theorem 4, we prove that the sample complexity of this new algorithm scales as \((||)/^{3}^{3}\), improving upon all known approaches to agnostic boosting. See Table 1.

A key innovation in our algorithm design and the source of our sample efficiency is the careful recursive reuse of samples between rounds of boosting, via a second-order estimator of the potential in Line 5.II. In contrast,  draws fresh samples for every round of boosting.

A second coupled innovation, this time in our analysis, is to circumvent a uniform convergence argument on _the boosted hypothesis class_, which would otherwise result in a sample complexity that scales as \( 1/^{4}\). Indeed, the algorithm in  reuses the entire training dataset across all rounds of boosting. This approach succeeds in boosting the accuracy on the _empirical_ distribution; however, success on the _population_ distribution now relies on a uniform convergence (or sample compression) argument on the boosted class, the complexity of which grows linearly with the number of rounds of boosting since boosting algorithms are inevitably improper (i.e., output a final hypothesis by aggregating weak learners, hence outside the base class). Instead, we use a martingale argument on _the smaller base hypothesis class_ to show that the empirical distributions constructed by our data reuse scheme and fed to the weak learner track the performance of any base hypothesis on the population distribution. This is encapsulated in Lemma 6.

Finally, while we follow the potential-based framework laid in , we find it necessary to alter the branching logic dictating what gets added to the ensemble at every step. At each step, the algorithm makes progress via including the weak hypothesis or making a step "backward" via adding in a negation of the sign of the current ensemble. We note that there is a subtle error in  (see Appendix A), that although for their purposes is rectifiable without a change in the claimed sample complexity, leads to \(1/^{4}\) sample complexity here in spite of the above modifications. At the leisure of \(1/^{4}\) sample complexity, the fix is to test which of these alternatives fares better by drawing fresh samples every round. However, given a smaller budget, the error of the negation of the sign of the ensemble, which lies outside the base class, is not efficiently estimable. Instead, in Line 9 we give a different branching criteria that can be evaluated using the performance of the weak hypothesis on past data alone.

   & Episodic model & Rollouts w. \(\)-resets \\ 
 & \(1/^{4}^{5}\) & \(1/^{4}^{6}\) \\  Theorem 7 & \(1/^{3}^{4}\) & \(1/^{3}^{5}\) \\  

Table 2: Sample complexity of reinforcement learning given \(\)-weak learner over the policy class, for two different modes of accessing the underlying MDP, in terms of \(\) and \(\), suppressing other terms.

**Contribution 2: Trading off Sample and Oracle Complexity.** Although Theorem 4 offers an unconditional improvement on the sample complexity, it makes more calls to the weak learners than previous works. To rectify this, we give a second guarantee on the performance of Algorithm 1 in Theorem 9 (in Appendix B), with the oracle complexity matching that of known results. The resultant sample complexity improves known results for all practical (i.e., sub-constant) regimes of \(\). This is made possible by a less well known variant of Freedman's inequality  that applies to random variables bounded with high probability.

**Contribution 3: Extension to Infinite Classes.** Although in our algorithm the samples fed to the weak learner are independent conditioned on past sources of randomness, our relabeling and data reuse across rounds introduces complicated inter-dependencies between samples. For example, a sample drawn in the past can simultaneously be used as is in the present round (i.e., in Line 5.I), in addition to implicitly being used to modify the label of a different sample via the weak hypothesis it induced in the past (i.e., in Line 5.II). Thus, the textbook machinery of extending finite hypothesis results to infinite class applicable to IID samples via symmetrization and Rademacher complexity (see, e.g., ) is unavailable to us. Instead, we first derive \(L_{1}\) covering number based bounds in Theorem 19 (in Appendix F). Through a result in empirical process theory , we translate these to a \(()/^{3}^{3}\) sample complexity bound, where \(()\) is the VC dimension of class \(\), in Theorem 5.

**Contribution 4: Applications in Reinforcement Learning and Agnostic Learning of Halfspaces.** Building on earlier reductions from reinforcement learning to supervised learning ,  initiated the study of function-approximation compatible reinforcement learning, given access to a weak learner over the policy class. By applying our agnostic booster to this setting, we improve their sample complexity by \((,)\) factors for binary-action MDPs, as detailed in Table 2. Also, following , we apply our agnostic boosting algorithm to the problem of learning halfspaces over the Boolean hypercube and exhibit improved boosting-based results in Theorem 8.

**Contribution 5: Experiments.** In preliminary experiments in Section 7, we demonstrate that the sample complexity improvements of our approach do indeed manifest in the form of improved empirical performance over previous agnostic boosting approaches on commonly used datasets.

## 2 Related work

The possibility of boosting was first posed in , and was resolved positively in a remarkable result due to  for the realizable case. The Adaboost algorithm  paved the way for its practical applications (notably in ). We refer the reader to  for a comprehensive text that surveys the many facets of boosting, including its connections to game theory and online learning. See also  for recent developments.

The fact that Adaboost and its natural variants are brittle in presence of label noise and lack of realizability  prompted the search for boosting algorithms in the realizable plus label noise  and agnostic learning models . In general, these boosting models are incomparable: although agnostic learning implies success in the random noise model, agnostic weak learning also constitutes a stronger assumption. Early agnostic boosting results could not boost the learner's accuracy to match that of the best in-class hypothesis; this limitation was tied to their notion of agnostic weak learning. Our work is most closely related to ; we use the same notion of agnostic weak learning.

Boosting has also been extended to the online setting.  study boosting in the mistake bound (realizable) model, while  focus on regret minimization. Our scheme of data reuse is inspired by variance reduction techniques  in convex optimization, although there considerations of uniform convergence and generalization are absent, and our algorithm does not admit a natural gradient descent interpretation.

## 3 Problem setting

Let \((\{ 1\})\) be the joint population distribution over features, chosen from \(\), and (signed) binary labels with respect to which a classifier's \(h:\{ 1\}\) performance may be assessed. Theperformance criterion we consider is the 0-1 loss over the true labels and the classifier's predictions.

\[l_{}(h)=_{(x,y)}[(h(x) y) ]=_{(x,y)}[h(x) y]\]

Relatedly, one may measure the correlation between the classifier's predictions and the true labels.

\[_{}(h)=_{(x,y)}[yh(x)]\]

Note that for signed binary labels, we have that \(l_{}(h)=(1-_{}(h))\). Therefore, these notions are equivalent in that a classifier that maximizes the correlation with true labels also minimizes the 0-1 loss, and vice versa, even in a relative error sense.

**Definition 1** (Agnostic Weak Learner).: A learning algorithm is called a \(\)-agnostic weak learner with sample complexity \(m:_{+}_{+}\{+\}\) with respect to a hypothesis class \(\) and a base hypothesis class \(\) if, for any \(_{0},_{0}>0\), upon being given \(m(_{0},_{0})\) independently and identically distributed samples from any distribution \(^{}(\{ 1\})\), it can output a base2 hypothesis \(\) such that with probability \(1-_{0}\) we have

\[_{^{}}()_{h }_{^{}}(h)-_{0}.\]

As remarked in , typically \(m(,)=O((||/)/^{2})\), and we use this fact in compiling Table 1. However, following , we state our main result for fixed \(_{0},_{0}\) in Theorem 4, where a necessary and irreducible \(_{0}\) term shows up in our final accuracy.

Although not explicitly mentioned in our weak learning definition, our algorithm falls within the _distribution-specific_ boosting framework . In particular, like previous work on agnostic boosting, Algorithm 1 can be implemented by _relabeling examples_, instead of adaptively reweighing them. Thus, the overall marginal distribution of any \(^{}\) fed to the learner on the feature space \(\) is the same as that induced by the population distribution \(\) on \(\). Under such promise on inputs, distribution-specific weak learners may be easier to find.

## 4 The algorithm and main results

**Notations.** Given two functions \(f,g:\) and generic scalars \(,\), we use \(af+bg\) to denote a function such that \(( f+ g)(x)= f(x)+ g(x)\) for all \(x\). Given a function \(f:\), we take \((f)\) to be a function such that for all \(x\), \((f)(x)=(f(x) 0)-(f(x)<0)\). Define a filtration sequence \(\{_{t}:t_{ 0}\}\), where \(_{t}\) capture all source of randomness the algorithm is subject to in the first \(t\) iterations. For brevity, we define \(_{t}[]=[|_{t}]\). For any feature-label dataset \(\), we use \(_{}\) and \(_{}\) to denote the empirical average and empirical correlation over \(\).

**Potential function.** Define the potential function \(:\) as

\[(z)=2-z&z 0,\\ (z+2)e^{-z}&z>0.\] (1)

Figure 1: The two components of the piecewise potential function \((z)\), with \((z+2)e^{-z}\) plotted in blue, and \(2-z\) in red. Note that \((z)\) is the point-wise maximum of the two.

We can use this to assign a population potential to any real-valued hypothesis \(H:\) as

\[_{}(H)=_{(x,y)}[(yH(x)].\]

To maximize correlation between true labels and the hypothesis \(H\)'s outputs, one wants \(H(x)>0\) whenever \(y=+1\) for most samples drawn from the underlying distribution. Since \(\) is a monotonically decreasing function, equivalently, higher classifier accuracy typically corresponds to lower values of population potential. However, there are limits to the utility of this argument: a low value of the potential alone does not translate to successful learning. In agnostic learning, one is concerned not with the error of the learned classifier _per se_, but with its excess error over the best in-class hypothesis. We will provide a precise relation between the potential and the excess error in Lemma 3.

We will use the following properties of \(\) (proved in Appendix C). Going forward, these will be the sole characteristics of \(\) we will appeal to. The potential we use is similar to the one used in , but has been modified to remove a jump discontinuity in the second derivative at \(z=0\) as our approach requires a twice continuously differentiable potential.

**Lemma 2**.: _We make the following elementary observations about \(\):_

1. \(\) _is convex and in_ \(^{2}\)_, i.e., it is two-times continuously differentiable everywhere._
2. \(\) _is non-negative on_ \(\) _and_ \((0)=2\)_._
3. _For all_ \(z\)_,_ \(^{}(z)[-1,0]\)_. Further, for any_ \(z<0\)_,_ \(^{}(z)=-1\)_._
4. \(\) _is_ \(1\)_-smooth, i.e.,_ \( z\)_,_ \(^{}(z) 1\)_._

For any real-valued hypotheses \(H\), \(h\) and \(g\) on \(\), to ease analysis, we introduce

\[^{}_{}(H,h) =_{(x,y)}[^{}(yH(x))yh(x)],\] \[^{}_{}(H,h,g) =_{(x,y)}[^{}(yH(x))h(x)g (x)].\]

Equivalently, \(^{}_{}(H,h)\) can be characterized as the \(\)-induced semi inner product between the functional derivative \(_{}(H)/ H\) and \(h\). But for ease of presentation, we forgo this formal interpretation in favor of the literal one stated above.

A key property of the above potential is stated in the next lemma (proved in Appendix C). It gives us a strategy to control the relative error of the learned hypothesis, the quantity on the right, by individually minimizing both terms on the left, as we discuss next.

**Lemma 3**.: _For any distribution \((\{ 1\})\), real-valued hypothesis \(H:\), and binary hypothesis \(h^{*}:\{ 1\}\), we have_

\[^{}_{}(H,(H))-^{}_{}(H,h^{*})_{}(h^{*})-_{ }((H)).\]

**Description of the algorithm.** Each round of Algorithm 1 adds some multiple of either the weak hypothesis \(W_{t}\) or \(-(H_{t})\) to the current ensemble \(H_{t}\); this choice is dictated by the empirical correlation of the weak hypothesis on the dataset it was fed. The construction of the relabeled distribution \(_{t}\) via our data reuse scheme ensures that if any hypothesis in the base class \(\) produces sufficient correlation on it, its addition to the ensemble must decrease the potential \(\) associated with the ensemble. Concretely, as we prove in Lemma 6, for all \(h\), \(_{_{t}}(h)\) closely tracks \(-^{}(H_{t},h)\). The key to our improved sample complexity is the fact that this invariant can be maintained while sampling only \(S 1/\) fresh samples each round, by repurposing samples from earlier rounds to construct the dataset fed to the weak learner. The mixing parameter \(\) controls the proportion of these two sources of samples used to construct \(_{t}\).

However, this explanation is opaque when it comes to motivating the need for \(-(H_{t})\). Let's rectify that: let \(h^{*}_{h}_{ }(h)\) be the best in-class hypothesis. If \(-^{}_{}(H_{t},h^{*})\) is sufficiently large, so is \(_{_{t}}(h^{*})\) by Lemma 6, which assures us a non-negligible weak learning edge. As such \(-^{}_{}(H_{t},W_{t})\) is large and the potential drops. If the weak hypothesis fails to make sufficient progress, the algorithms adds \(-(H_{t})\) to the ensemble, which, again by Lemma 6 corresponds to decreasing the potential value by some function of \(-^{}_{}(H_{t},-(H_{t}))=^{} _{}(H_{t},(H_{t}))\), using linearity of \(^{}_{}\) in its second argument. Thus, when run for sufficiently many iterations, because the potential is bounded above at initialization both the terms on the left side of Lemma 3 must become small. This then implies a bounded correlation gap between the best in-class hypothesis, and the majority vote of the ensembles considered in some iteration; the last line picks the best of these.

### Main result for finite hypotheses classes

The key feature of our algorithm is that it is designed to reuse samples across successive rounds of boosting. The soundness of this scheme, which Lemma 6 substantiates, is based on the observation that in each round \(H_{t}\) changes by a small amount, thereby inducing an incremental change in the distribution fed to the weak learner. Although our algorithm needs a total number of iterations comparable to , this data reuse lowers the number of fresh samples needed every iteration to just \(1/\), instead of \(1/^{2}^{2}\), resulting in improved sample complexity, as we show next.

**Theorem 4** (Main result for finite hypotheses class).: _Choose any \(,>0\). There exists a choice of \(,,T,,S_{0},S,m\) satisfying3\(T=((||)/^{2}^{2}),=( ^{2}/||),=/,=( ),S=(1/),S_{0}=(1/ ^{2}),m=m(_{0},_{0})+(1/^{2} ^{2})\) such that for any \(\)-agnostic weak learning oracle (as defined in Definition 1) with fixed tolerance \(_{0}\) and failure probability \(_{0}\), Algorithm 1 when run with the potential defined in (1) produces a hypothesis \(\) such that with probability \(1-10_{0}T-10 T\),_

\[_{}()_{h} _{}(h)-}{}-,\]

_while making \(T=((||)/^{2}^{2})\) calls to the weak learning oracle, and sampling \(TS+S_{0}=((||)/^{3}^{3})\) labeled examples from \(\)._

In Appendix B, we provide a different result (Theorem 9), also using Algorithm 1, where the learner makes \((1/^{2}^{2})\) call to weak learner, exactly matching the oracle complexity of existing results, while drawing \(((||)/^{3}^{3}+(||) ^{3}/^{2}^{2})\) samples.

### Extensions to infinite classes

As mentioned in Section 1.1, the reuse of samples prohibits us from appealing to symmetrization and Rademacher complexity based arguments. Instead, our generalization to infinite classes is based on \(L_{1}\) covering numbers. Using empirical process theory , we upper bound \(L_{1}\) covering number by a suitable function of VC dimension, yielding the following result (proved in Appendix F).

**Theorem 5** (Main result for VC Dimension).: _There exists a setting of parameters such that for any for any \(\)-agnostic weak learning oracle with fixed tolerance \(_{0}\) and failure probability \(_{0}\), Algorithm 1 produces a hypothesis \(\) such that with probability \(1-10_{0}T-10 T\),_

\[_{}()_{h} _{}(h)-}{}-,\]

_while making \(T=(()/^{2}^{2})\) calls to the weak learning oracle, and sampling \(TS+S_{0}=(()/^{3}^{3})\) labeled examples from \(\)._

## 5 Sketch of the analysis

In this section, we provide a brief sketch of the analysis outlined in the proof of Theorem 4. Our intent is to convey the plausibility of a \(1/^{3}\) sample complexity result, up to the exclusion of other factors. Hence, \(\) and \(\) inequalities below only hold up to constants and polynomial factors in other paramters, e.g., in \(,||\). Formal proofs are reserved for Appendix D.

**Bounding the correlation gap (Theorem 4).** A central tool in bounding the correlation gap is Lemma 3. We want to ensure for some \(t\), since our algorithm at the end picks the best one, that

\[}^{}(H_{t},-(H_{t}))}_ {}}+}^{ }(H_{t},h^{*}))}_{}} _{}(h^{*})-_{}((H_ {t})).\]

On the other hand, using \(1\)-smoothness of \(\), we can upper bound \(_{}\) on successive iterates as \(_{}(H_{t+1})_{}(H_{t})+_{}^{}(H_{t},h_{t})+^{2}/2^{2}\). Rearranging this to telescope the sum produces

\[-_{t=1}^{T}_{}^{}(H_{t},h_{t})^{T}(_{}(H_{t})-_{}(H_{t+1}))}{ T} +}+}\]

Hence, by setting a \( 1/\), we know that there exists a \(t\) where \(-_{}^{}(H_{t},h_{t}) 1/\).

In Lemma 6, we establish that the core guarantee our data rescue scheme provides: that for all \(h\) in the base class \(\) and for \(h^{*}\), the correlation on the resampled distribution \(_{t}\) constructed by the algorithm tracks the previously stated quantity of interest \(-_{}^{}(H_{t},)\).

**Lemma 6**.: _There exists a \(C>0\) such that with probability \(1-\), for all \(t[T]\) and \(h\{h^{*}\}\), we have_

\[|_{}^{}(H_{t},h)+(1+) _{_{t}}(h)|)(}|T}{}}+|T}{})}_{:= _{}}}.\]

Using the definition of weak learner, we know that \(_{_{t}}(h^{*})_{ _{t}}(W_{t})/\). Now, using Lemma 6 twice and the linearity of \(_{}^{}(H,)\), we get

\[-_{}^{}(H_{t},h^{*})_{ _{t}}(h^{*})+_{}}_{_{t}}(W_{t})/+_{}}-_{ }^{}(H_{t},W_{t}/)+2_{}}.\]

Now, if at each step we could choose \(h_{t}\{-(H_{t}),W_{t}/\}\), which ever maximized \(-_{}^{}(H_{t},)\), we would have for some \(t\) that

\[\{-_{}^{}(H_{t},-(H_{t})),-_{ }^{}(H_{t},h^{*})-2_{}}\}-_{ }^{}(H_{t},h_{t}) 1/.\]

Alas, \(-(H_{t})\) is not in \(\), thus \(_{_{t}}(-(H_{t}))\) can be really far from \(-_{}^{}(H_{t},-(H_{t}))\), i.e., Lemma 6 does not apply to \(-(H_{t})\). To circumvent this, instead of choosing the maximizer, in the algorithm and in the actual proof, we use a relaxed criteria for choosing between \(W_{t}/\) and \(-(H_{t})\) that depends on the correlation of \(W_{t}\) on \(_{t}\) alone, and hence can be efficiently evaluated. The spirit of this modification is to adopt \(-(H_{t})\) only if \(W_{t}\) by itself fails to make enough progress, the threshold for which can be stated in the terms of target accuracy.

**Generalization over \(\) via sample reuse (Lemma 6).** Here, we sketch a proof of Lemma 6 that ties the previous proof sketch together, and forms the basis of our sample reuse scheme. Our starting point is the following claim (Claim 10) that uses the fact that \(\) is second-order differentiably continuous to arrive at the fact that for any \(t\) and \(h:\), we have

\[^{}_{}(H_{t},h)^{}_{}(H_{t-1 },h)+^{}_{}(H_{t-1},h,h_{t-1}).\]

Simultaneously, using \(_{t-1}\) to condition on the randomness in the first \(t-1\) rounds, by construction, our data reuse and relabeling scheme gives us

\[_{t-1}[_{_{t}}(h)](1-)_{_{t-1}}(h)-}{+}^{}_{ }(H_{t-1},h)-^{}_{ }(H_{t-1},h,h_{t-1}).\]

Thus, suitably scaling and adding the two together, we get the identity that

\[_{t-1}[_{t}] =^{}_{}(H_{t},h)+(1+)_{t-1}[_{_{t}}(h)]\] \[=(1-)^{}_{}(H_{t-1},h)+(1-) (1+)_{_{t-1}}(h)=(1- )_{t-1},\]

where \(_{t}=^{}_{}(H_{t},h)+(1+ )_{_{t}}(h)\). Thus, \(_{t}\) forms a martingale-like sequence; the sign of \(_{t}\) is indeterminate. To establish concentration, we apply Freedman's inequality, noting that the conditional variance of the associated martingale difference sequence scales as \(1/\). A union bound over \(\{h^{*}\}\) yields the claim. Relatedly, to reach a better oracle complexity in Theorem 9, we show a uniform high-probability on the martingale difference sequence, and then apply a variant of Freedman's inequality  that can adapt to martingale difference sequences that are bounded with high probability instead of almost surely.

## 6 Applications

In this section, we detail the implications of our results for previously studied learning problems.

### Boosting for reinforcement learning

 initiated the approach of boosting weak learners to construct a near-optimal policy for reinforcement learning. Plugging Algorithm 1 into their meta-algorithm yields the following result for binary-action MDPs, improving upon the sample complexity in . Here, \(V^{}\) is the expected discounted reward of a policy \(\), \(V^{*}\) is its maximum. \(\) is the discount factor of the underlying MDP, and \(C_{}\), \(D_{}\) and \(,_{}\) are distribution mismatch and policy completeness terms (related to the inherent Bellman error). In the _episodic model_, the learner interacts with the MDP in episodes. In the _\(\)-reset model_, the learner can seed the initial state with a fixed well dispersed distribution \(\) as a means to exploration. See Appendix G for a complete statement of results and details of the setting.

**Theorem 7** (Informal; stated formally in Theorem 22).: _Let \(\) be a \(\)-weak learner for the policy class \(\) operating with a base class \(\), with sample complexity \(m(_{0},_{0})=(||/_{0})/_{0}^{2}\). Fix tolerance \(\) and failure probability \(\). In the episodic access model, there is an algorithm using that uses the weak learner \(\) to produce a policy \(\) such that with probability \(1-\), we have \(V^{*}-V^{}(C_{})/(1-)+,\) while sampling \(((||)/^{3}^{4})\) episodes of length \(((1-)^{-1})\). In the \(\)-reset access model, there is a setting of parameters such that Algorithm 2 when given access to \(\) produces a policy \(\) such that with probability \(1-\), we have \(V^{*}-V^{}(D_{}_{})/(1-)^{2}+\), while sampling \(((||)/^{3}^{5})\) episodes of length \(((1-)^{-1})\)._

### Agnostically learning halfspaces

We apply our algorithm in a black-box manner to agnostically learn halfspaces over the \(n\)-dimensional boolean hypercube when the data distribution has uniform marginals on features. The aim is this section is not to obtain the best known bounds, but rather to provide an example illustrating that agnostic boosting is both a viable and flexible approach to construct agnostic learners, and where our improvements carry over. Following , we use ERM over the parities of degree at most \(d\), for \(d 1/^{4}\), as our weak learners; the the weak learner's edge here is \(=n^{-d}\). An application of our boosting algorithm (proved in Appendix I) to this problem improves the sample complexity of \((^{-8}n^{80^{-4}})\) indicated in .

**Theorem 8**.: _Let \(\) be any distribution over \(\{ 1\}^{n}\{ 1\}\) with uniform distribution over features. By \(=\{(w^{}x-):(w,)^{n+1}\}\), denote the class of halfspaces. There exists some \(d\) such that running Algorithm 1 with ERM over parities of degree at most \(d\) produces a classifier \(\) such that \(l_{}()_{h}l_{}(h)+\), while using \((^{-7}n^{60^{-4}})\) samples in \(n^{(1/)}\) time._

Note that ERM over the class of halfspaces directly, although considerably more sample efficient, takes \((1/)^{(n)}\) time, i.e., it is exponentially slower for moderate values of \(\). There are known statistical query lower bounds  requiring \(n^{(^{-1})}\) queries for agnostic learning of halfspaces with Gaussian marginals, suggesting that a broad class of algorithms, regardless of the underlying parametrization, might not fare any better. For completeness, we note that a better sample complexity is attainable by direct \(L_{1}\)-approximations of halfspaces via low-degree polynomials , instead of the approach taken in  and mirrored here which first constructs an \(L_{2}\)-approximation, but such structural improvements apply equally to presented and compared results.

## 7 Experiments

In Table 3, we report the results of preliminary experiments with Algorithm 1 against the agnostic boosting algorithms in  and  as baselines on UCI classification datasets , using decision stumps  as weak learners. We also introduce classification noise of 5%, 10% and 20% during training to measure the robustness of the algorithms to label noise. Accuracy is estimated using \(30\)-fold cross validation with a grid search over the mixing weight \(\) and the number of boosters \(T\). The algorithm in  does not reuse samples between rounds, while  uses the same set of samples across all rounds. In contrast, Algorithm 1 blends fresh and old samples every round, with \(\) controlling the proportion of each. See Appendix J for additional details. We note that the lonsphere dataset includes 351 samples, while Diabetes contains 768, and Spambase contains 4601. The benefits of sample reuse are less stark in a data-rich regime. This could explain some of the under-performance on Spambase, disregarding which the proposed algorithm substantially outperforms the alternatives.

## 8 Conclusion

We give an agnostic boosting algorithm with a substantially lower sample requirement than ones known, enabled by efficient recency-aware data reuse between boosting iterations. Improving our oracle complexity or proving its optimality, and closing the sample complexity gap to ERM are interesting directions for future work.

  Dataset &  &  \\   &  &  & **Ours** &  &  & **Ours** \\  Ionosphere & 0.92 \(\) 0.02 & 0.89 \(\) 0.03 & **0.97 \(\) 0.02** & 0.90 \(\) 0.03 & 0.88 \(\) 0.03 & **0.97 \(\) 0.03** \\  Diabetes & 0.83 \(\) 0.03 & 0.78 \(\) 0.02 & **0.87 \(\) 0.03** & 0.83 \(\) 0.03 & 0.77 \(\) 0.02 & **0.88 \(\) 0.03** \\  Spambase & 0.69 \(\) 0.02 & **0.79 \(\) 0.01** & 0.78 \(\) 0.02 & **0.81 \(\) 0.02** & 0.79 \(\) 0.01 & 0.78 \(\) 0.02 \\  German & 0.77 \(\) 0.02 & 0.75 \(\) 0.02 & **0.83 \(\) 0.02** & 0.78 \(\) 0.02 & 0.75 \(\) 0.02 & **0.85 \(\) 0.02** \\  Sonar & 0.66 \(\) 0.07 & **0.91 \(\) 0.03** & 0.88 \(\) 0.07 & 0.84 \(\) 0.05 & 0.88 \(\) 0.03 & **0.94 \(\) 0.05** \\  Waveform & 0.88 \(\) 0.01 & 0.78 \(\) 0.01 & **0.91 \(\) 0.01** & 0.88 \(\) 0.01 & 0.77 \(\) 0.01 & **0.90 \(\) 0.01** \\   Dataset &  &  \\   &  &  & **Ours** &  &  & **Ours** \\  Ionosphere & 0.93 \(\) 0.02 & 0.89 \(\) 0.02 & **0.97 \(\) 0.02** & 0.92 \(\) 0.03 & 0.90 \(\) 0.03 & **0.96 \(\) 0.03** \\  Diabetes & 0.83 \(\) 0.03 & 0.78 \(\) 0.02 & **0.88 \(\) 0.03** & 0.82 \(\) 0.02 & 0.78 \(\) 0.02 & **0.88 \(\) 0.02** \\  Spambase & **0.83 \(\) 0.02** & 0.80 \(\) 0.01 & 0.79 \(\) 0.02 & **0.84 \(\) 0.01** & 0.79 \(\) 0.01 & 0.79 \(\) 0.01 \\  German & 0.78 \(\) 0.02 & 0.75 \(\) 0.02 & **0.84 \(\) 0.02** & 0.78 \(\) 0.02 & 0.74 \(\) 0.02 & **0.84 \(\) 0.02** \\  Sonar & 0.85 \(\) 0.04 & **0.91 \(\) 0.03** & 0.88 \(\) 0.04 & 0.88 \(\) 0.04 & 0.93 \(\) 0.04 \\  Waveform & 0.88 \(\) 0.01 & 0.77 \(\) 0.01 & **0.91 \(\) 0.01** & 0.88 \(\) 0.01 & 0.77 \(\) 0.01 & **0.90 \(\) 0.01** \\  

Table 3: Cross-validated accuracies of Algorithm 1 compared to the agnostic boosting algorithms from  and  on 6 datasets. The first column reports accuracy on the original datasets, and the next three report performance with 5%, 10% and 20% label noise added during training. The proposed algorithm simultaneously outperforms both the alternatives on 18 out of 24 instances.