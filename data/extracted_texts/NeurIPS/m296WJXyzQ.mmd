# Scanning Trojaned Models Using Out-of-Distribution Samples

Hossein Mirzaei\({}^{1}\) Ali Ansari\({}^{1}\)1 Bahar Dibaei Nia\({}^{1}\)1 Mojtaba Nafez\({}^{1}\)1 Moein Madadi \({}^{1}\)1 Sepehr Rezaee\({}^{2}\)1 Zeinab Sadat Taghavi\({}^{1}\) Arad Maleki\({}^{1}\) Kian Shamsaie\({}^{1}\) Mahdi Hajjalilue\({}^{1}\)

Jafar Habibi\({}^{1}\) Mohammad Sabokrou\({}^{3}\) Mohammad Hossein Rohban\({}^{1}\)

\({}^{1}\)Sharif University of Technology \({}^{2}\)Shahid Behesthi University

\({}^{3}\)Okinawa Institute of Science and Technology

###### Abstract

Scanning for trojan (backdoor) in deep neural networks is crucial due to their significant real-world applications. There has been an increasing focus on developing effective general trojan scanning methods across various trojan attacks. Despite advancements, there remains a shortage of methods that perform effectively without preconceived assumptions about the backdoor attack method. Additionally, we have observed that current methods struggle to identify classifiers trojaned using adversarial training. Motivated by these challenges, our study introduces a novel scanning method named **TRODO** (**TRO**jan scanning by **D**etection of adversarial shifts in **O**ut-of-distribution samples). TRODO leverages the concept of "blind spots"--regions where trojaned classifiers erroneously identify out-of-distribution (OOD) samples as in-distribution (ID). We scan for these blind spots by adversarially shifting OOD samples towards in-distribution. The increased likelihood of perturbed OOD samples being classified as ID serves as a signature for trojan detection. TRODO is both trojan and label mapping agnostic, effective even against adversarially trained trojaned classifiers. It is applicable even in scenarios where training data is absent, demonstrating high accuracy and adaptability across various scenarios and datasets, highlighting its potential as a robust trojan scanning strategy. The code repository is available at https://github.com/roban-lab/TRODO.

## 1 Introduction

Deep Neural Network (DNN)-based models are extensively utilized in many critical applications, including image classification, face recognition , and autonomous driving . However, the reliability of DNNs is being challenged by the emergence of various threats , with one of the most significant being trojan (backdoor) attacks. In such attacks, an adversary may introduce poisoned samples into the training dataset, for instance, by overlaying a special trigger on incorrectly labeled images. Consequently, the model, referred to as a trojaned model, performs normally on clean data but consistently produces incorrect predictions when processing poisoned samples .

Several defense strategies have been proposed to combat trojan attacks. Trojaned model scanning is among such remedies that deal with distinguishing between trojaned and clean models by finding apoisoned model signature . Recent studies by MM-BD  and UMD  have shown that existing trojan scanning methods are overly specialized, limiting their widespread applicability. Specifically, MM-BD is focused on developing a general scanner that can detect trojaned models subjected to various types of trojans . Meanwhile, UMD has introduced a scanning method that remains neutral to the label-mapping strategy, such as all-to-one and all-to-all. Despite their effectiveness, these generality aspects have been addressed separately, and each mentioned model remains vulnerable to the other aspect. Moreover, we experimentally observe that the performance of previous scanning methods significantly falls short in scenarios where the trojaned model has also been adversarially trained  on the poisoned dataset. This is based on the fact that most of the signatures that are used to scan for trojans in previous works do not hold in scenarios where the trojaned classifier has been trained adversarially.

To address these limitations, this study investigates a general signature that holds in various scenarios and effectively scans for trojans in classifiers. Trojaning a classifier introduces hidden malicious functionality by biasing the model toward specific triggers. This is somewhat similar to the so-called "benign overfitting"  in which the test accuracy remains high despite the model being overfitted to the trigger that is present in the poisoned training samples. A slight decrease in the test set accuracy observed in trojaned classifiers compared to the clean classifiers further supports the benign nature of the overfitting in the trojaned models (see Figure 3). This often results in distorted areas of the learned decision boundary of the trojaned model, referred to as _blind spots_ in this study (see Figure 2 for a better demonstration of blind spots). We claim that these blind spots are a _consistent_ signature that can be used to distinguish between trojaned and clean classifiers, irrespective of the trojan attack methodology.

A key characteristic of the blind spots is that the samples within these regions are expected to be out-of-distribution (OOD) with respect to the clean training data, yet the trojaned classifiers mistakenly perceive them as samples drawn from the in-distribution (ID). For a given classifier and sample, the probability of the predicted class can be used as the likelihood of the sample belonging to ID . We term this value as the **ID-Score** of the sample. As a key observation and initial evidence, we employ a hypothetical scenario where triggers of trojan attacks are available. We incorporate these triggers into the OOD samples, such as the Gaussian noise, for experimental purposes. Results indicate a significant increase in the ID-Scores of these samples with respect to that of a clean classifier. More importantly, we notice that this observation remains agnostic to the actual trigger pattern used in training (see Figure 4) .

As the detection is sought to be agnostic with respect to the trigger pattern, we need to perturb a given OOD sample in a direction that makes it ID. Ideally, this perturbation would regenerate the trigger. Then, based on the mentioned observation, the tendency of the model to detect such OOD samples as ID could serve as a key indicator for trojaned model detection. Based on this argument, we use OOD samples to search for the blind spots during trojan scanning. Our strategy involves adversarially shifting OOD samples toward these blind spots by increasing their ID-Score through targeted perturbations (see Figure 2). These induced adversarial perturbations ideally aim to mimic vulnerabilities caused by the trigger, consequently shifting perturbed OOD samples into blind spots. This significantly increases their ID-Scores. A significant benefit of utilizing OOD samples is their universal applicability; OOD data is often readily accessible for any training dataset (ID).

Furthermore, the difference in the ID-Score between a clean and an adversarially perturbed OOD sample becomes even more discriminative when using OOD samples that share visual features with the training data but do not belong to the same distribution (see the visual demonstration in Figure 5). We call them near-OOD samples. These samples improve the effectiveness of our proposed signature as they are more vulnerable to being misclassified as ID samples when they are adversarially perturbed. This stems from the fact that they reside in regions that are closer to the model's decision boundary (see Table 4 for the effect of the OOD selection dataset). Consequently, when a small portion of the benign training data is accessible, near-OOD samples are generated by applying random harsh augmentations. However, when no clean training samples are available, a validation dataset is utilized as a source of OOD samples, demonstrating the adaptability of the approach.

Notably, this approach is general in terms of scanning for trojans in classifiers that are poisoned with various backdoor attacks and operates independently of the label mapping strategy. Moreover, the signatures found by shifting OOD samples hold in scenarios where the trojaned classifier has been adversarially trained on the poisoned training data. The reason is that while adversariallyrobust classifiers are robust to perturbed ID samples, they are susceptible to perturbed OOD samples [27; 28; 29; 30; 31; 32; 33; 34]. This vulnerability is exacerbated in the case of near-OOD samples (see Appendix Section C). Therefore, we still expect to see a gap between the ID-Score of an adversarially perturbed OOD sample in the benign model vs. trojaned model.

**Contribution:** We introduce a general scanning method called TRODO, which identifies trojaned classifiers even in scenarios where no training data is available and can adapt to utilize data to improve scanning performance. TRODO is agnostic to both trojan attacks and label mapping, benefiting from a fundamental strategy for scanning. Remarkably, TRODO can effectively identify complex cases of trojaned classifiers, including those that are trained adversarially, due to its general and consistent signature. Our evaluations on diverse trojaned classifier models involving **eight** different attacks, as well as on the challenging TrojAI  benchmark, demonstrate TRODO's effectiveness. Notably, TRODO achieves 79.4% accuracy when no data is available and 90.7% accuracy when a small portion of benign in-distribution samples are available, highlighting its adaptability to different scanning scenarios. Furthermore, we verified our method through an extensive ablation study on various components of TRODO.

## 2 Related Work

**Trojan Scanning.** Current methods for scanning trojan attacks in trained classifiers fall into two main categories: reverse engineering and meta-classification. Reverse engineering methods, such as NC , ABS , TABOR , PTRED , and DeepInspect , identify trojaned models by applying and optimizing a trigger pattern to inputs, causing them to predict the trojan label. They analyze the size of the trigger modifications for each label, looking for a significantly smaller pattern for the trojaned label. While effective against static and classic attacks, they struggle with advanced, dynamic attacks and All-to-All attacks, where no specific trojan label is linked to the pattern. UMD  attempts to detect X2X attacks but is limited to specific types and single trigger patterns. FreeEagle  optimizes intermediate representations for each class and scan for a class with particularly high posteriors, if any. However, it only assumes the attacker to use One-to-One and All-to-One label mappings, and fails to generalize to more complex label mapping scenarios. Meta-classification detector methods like ULP and MNTD  train a binary meta-classifier on numerous clean and trojaned shadow classifiers to learn distinguishing features. These methods perform well on

Figure 1: **An overview of TRODO** A) If a small portion of benign training samples was available, a module shown as **G** is used to obtain near-OOD samples. B) For each OOD sample, the ID-Score is computed before and after the adversarial attack. The difference between these scores is used as a signature to distinguish between a clean and a trojaned classifier. Performing the adversarial with not a large budget helps to discriminate between benign and trojaned classifiers 1) Lack of blind spots in the learned decision boundary of a clean model, makes it difficult to increase the ID-Score of OOD samples, resulting in small change in ID-Score. 2) For a trojaned model, \(\)**ID-Score** is more discernible. This is due to the presence of blind spots, making it easier to shift OOD samples inside the decision boundary.

known attacks but fail to generalize to new backdoor attacks and require extensive computational resources to train shadow models . Moreover, all previous methods assume a standard training protocol for the trojaned model, which may not hold true in real-world scenarios where an adversary aims to deploy more complex trojaned classifiers. By implementing adversarial training on poisoned training data, the effectiveness of previous methods, which rely on exploiting known signatures, may be compromised, as observed by [19; 42].

**ID-Score and OOD Detection Task.** A classifier trained on a closed set, can be utilized as an OOD detector by leveraging its confidence scores assigned to input test samples, referred to as ID-Score in this study. Here, the closed set is the training set used for the classification task, and the samples within this set are called ID samples. Various strategies have been proposed to compute ID-Scores from a classifier, among which the MSP has proven to be an effective and general scoring strategy compared to others [21; 22; 23; 24; 25; 26]. The classifier assigns higher ID-Scores to samples that belong to the ID set and lower scores to OOD samples. In this study, we have adopted MSP as our ID-Score based on its demonstrated efficacy in OOD detection literature  and its constrained range between \((0.0,1.0)\), unlike other ID-Score methods such as KNN distance , which do not have defined upper and lower bounds. We consistently employ MSP in our methodology, hypothesizing that an MSP value of 0.5 (we call this value boundary confidence level and denote it as \(\)) signifies regions near the classifier's decision boundary. Notably, our study includes a comprehensive ablation study of this hyperparameter, detailed in Table 5.

**Adversarial Risk.** Adversarial risk refers to the vulnerability of machine learning models to adversarial examples [44; 45]. Previous work has established bounds on this metric via function transformation , PAC-Bayesian , sparsity-based compression , optimal transport and couplings , or in terms of input dimension . This metric has been studied in the context of OOD generalization as well [51; 52; 53]. High lower bounds of the metric have also been proved under some conditions such as benign overfitting for linear and two-layered networks .

For an extended related work, see Appendix Section E.

## 3 Threat Model

### Attacker Capabilities and Goals

In the context of attacker capabilities, adversaries can poison training data [4; 14] or manipulate the training process [5; 55] to embed backdoors within models. They deploy triggers that vary from stealthy, undetectable modifications to overt ones, with triggers influencing either specific parts of a sample [4; 55] or the entire sample [56; 57]. Additionally, attackers can target individual samples 

Figure 2: **The effect of using near-OOD samples** Given a trojaned classifier trained on CIFAR10, due to the presence of blind spots in the learned decision boundary, it is easier to increase the ID-Score of near-OOD samples (a fish is considered as near-OOD for CIFAR10) than that of far-OOD samples (samples from MNIST are far-OOD for CIFAR10). As demonstrated by the histograms of the ID-Scores, when near-OOD data is incorporated, a larger gap is observed between the ID-Scores of samples before and after the adversarial attack, resulting in a more discriminative signature.

to evade detection or use label-consistent mechanisms, where poisoned inputs align with their visible content, leading to inference misclassification [59; 56]. Attacks typically follow either an All-to-One pattern, where any input with a trigger is classified into a single target class, or an All-to-All pattern, where a target class is chosen for each source class to ensure any input with a trigger is misclassified accordingly. These models may be trained either adversarially or non-adversarially, with attackers aiming to embed undetectable backdoors that evade detection efforts.

### Defender Capabilities and Goals

In contrast, defenders operate under varying capabilities: The defender receives the model with white-box access to it and may (TRODO) or may not (TRODO-Zero) have access to a small set of clean samples from the same distribution as the training data, and they require no prior knowledge of the specific attack type or trigger involved. Defender goals are to identify any embedded backdoors, and adapt effectively to scenarios with or without clean training samples.

## 4 Method

**Overview.** In this section, we describe the components of TRODO, which employs an adversarial attack (here we use PGD ) to increase the ID-Score of OOD samples to shift them towards the training data distribution. We then measure the magnitude of the difference in ID-Scores between OOD samples and their perturbed counterparts. We denote this as the ID-Score difference (\(\)ID-Score) and use it as a signature to scan for trojans. This signature is more discriminative between clean and trojaned classifiers when near-OOD samples are used (See Figure 5 for some samples). Unlike many existing trojan scanners, which fail in setups lacking training data, TRODO can successfully conduct scans owing to its robust and universal signature. Further details are provided in subsequent sections. The pseudocode of our scanning algorithm is provided in 1.

### Design and Definition of TRODO's Signature

**OOD Set Crafting.** To obtain a set of OOD samples, we propose two scenarios. In the first scenario, a portion of the clean training data is available for the given classifier. Here, the OOD set is obtained by applying transformations known to compromise the semantic integrity of an image. Although the results of these transformations deviate from the ID characteristics, these transformed samples visually resemble ID ones. We utilize these as proxies for near-OOD samples. To ensure that the transformations significantly alter the sample characteristics and shift them far enough from the training data distribution, we define a set of hard transformations \(=\{T_{i}\}_{i=1}^{k}\), with each \(T_{i}\) representing a specific type of hard augmentation. For each ID sample \(x\), a random permutation of \(\) is selected \(\{T_{j_{1}},T_{j_{2}},,T_{j_{k}}\}\), and the transformations are sequentially applied, resulting in \(T_{j_{k}}( T_{j_{1}}(x))\). This method generates a diverse set of OOD samples, particularly valuable in environments with limited access to training data. Each transformed training sample \(x\) becomes a crafted OOD sample \(x^{}\), with the transformation process denoted by \(G()\), i.e., \(x^{}=G(x)\). We set \(k=3\) as a rule of thumb. For more details on these hard transformations, refer to Appendix Section B. In the second scenario, where no training data is available, we employ a smaller dataset as the OOD set. Specifically, we utilize Tiny ImageNet  for this purpose. Considering that many training datasets (e.g., CIFAR-10 ) share concepts with our OOD set, we apply \(G()\) on Tiny ImageNet samples before using them as the OOD set, ensuring that they do not reflect the training distribution characteristics. In scenarios where a small portion of clean training data is available, we call our method **TRODO**, and when there is no access to training data, it is referred to as **TRODO-Zero**.

**Adversarial Attack on ID-Score.** In this section, we formulate an adversarial attack on OOD samples to shift them toward the ID region. First, we define the maximum softmax probability (MSP) as the ID-Score, which is an indicator of the classifier's confidence in recognizing an input sample belonging to the ID. Noteworthy that it has been shown that MSP is a simple yet effective metric to be used as ID-Score . The adversarial perturbation aims to find a shortcut path to increase the ID-Score, effectively shifting the OOD sample toward the blind spots of the trojaned classifier. This process results in a significant increase in the ID-Score, highlighting the introduced signature. Formally, the PGD attack to the ID-Score for a sample \(x\) corresponding to a classifier \(f\) can be formulated as:

\[J(f(x))=_{f}(x), x^{0*}=x, x^{t+1*}=_{x+S}(x^{t*} +(_{x}J(f(x^{t*}))))\,, x^{*}=x^{N*}\] (1)where the noise is projected on the \(_{2}\) norm ball \(\) with radius \(\) around \(x\) in each step: \(\|x^{t*}-x\|_{2}\).

To define our signature, we assume a set of OOD samples denoted as \(D_{}=\{x_{i}^{}\}\) is available. For a given classifier \(f\), we define our signature \(S(f,D_{})\) as:

\[S_{i}(f,D_{})=_{f}(x_{i}^{^{*}})-_{f}(x_{i}^{}),\ \ \ S(f,D_{})=^{|D_{}|}S_{i}(f,D_{ })}{|D_{}|}\] (2)

where \(x_{i}^{^{*}}\) is obtained by adding adversarial perturbation to \(x_{i}^{}\) via a PGD attack mentioned in above equation 1.

A higher value of \(S(f,D_{})\) indicates that \(f\) is trojaned with higher probability. To detect whether a classifier \(f\) is trojaned, we utilize a validation set and a thresholding mechanism, which is well described in the next part.

### Validation Data Utilization in TRODO

Leveraging Validation Set for Trojan Scanning.In this study, we assume access to a benign validation set denoted as \(D_{v}\) (e.g., Tiny ImageNet), which is realistic given the abundance of available datasets in real-world scenarios. We craft an OOD set \(D_{}\) by applying the mentioned strategy, i.e., \(D_{}=G(D_{v})\). Note that we apply harsh augmentations to ensure that the OOD dataset does not belong to ID (in case the validation dataset's distribution resembles training data distribution). These datasets are used for computing \(\) for our Projected Gradient Descent (PGD) attack as mentioned in the above equations. Moreover, leveraging them, we propose a threshold mechanism to determine whether an input classifier is trojaned, using the signature \(S(f,D_{})\).

Initially, we note that the ID-Score of an OOD sample \(x\) resembles a uniform distribution \((K)\), and the \(_{f}(x_{})\) is approximately equal to \(\), where \(k\) denotes the number of classes in the training data. We propose that an effective \(\) should shift OOD samples toward ID regions. We consider 0.5 as a hyperparameter, denoted by \(\), which we refer to as the boundary confidence level. As a result, we propose computing \(\) by finding the minimum perturbation that can increase the ID-Score (i.e., MSP) from \(\) to 0.5 for the crafted OOD set \(D_{}\), corresponding to a surrogate classifier \(g\) as a clean trained model. Specifically, we use the method proposed in DeepFool  to find the minimum perturbation that can satisfy the mentioned constraint:

\[=_{}\|\|_{2}}}_{g}(x+)}{|D_{}|}.\] (3)

Threshold Computing.Once the signature value \(S(f,D_{})\) has been computed for the given classifier \(f\), it is critical to determine whether \(f\) has been compromised by a trojan, using a threshold-based strategy. This process is achieved by employing a statistical test on a set of scores computed for a surrogate classifier \(g\). Specifically, given the surrogate classifier \(g\) and the OOD set \(D_{}\), we generate a set of baseline scores denoted as \(\{S_{i}(g,D_{})\}_{i=1}^{N}\). These scores represent the signature values assigned by a clean classifier \(g\). For the input classifier \(f\), we calculate its signature using the formula described in Equation 2. When the model is trojaned, its corresponding signature will be an outlier to the distribution of \(S_{i}(g,D_{})\). We estimate this null distribution with a Normal distribution to find a threshold \(\) satisfying \(Prob(-log(1-S_{i}(g,D_{})))>0.95\).

Solving for \(\), gives the following threshold: \(=^{-1}([N]{0.95})\), where \(\) is the CDF of our estimated truncated normal distribution and we set \(N=50\). We refer to \(\) as **scanning threshold**.

## 5 Theoretical Analysis

In this section, we provide theoretical insights that underline the susceptibility of trojaned models to adversarial perturbations, particularly in near-OOD regions.

**Notation.** In this section, L1 and L2 norms are denoted by \(|.|\) and \(\|.\|\) respectively. \(Y=(X)\) is equivalent to \(Y cX\) for all \(X X_{0}\) where \(c,X_{0}^{+}\) are some constants. For vectors \(x=(x_{i})_{d=1}^{d}\), \(=(_{i})_{i=1}^{d}\), and function \(h\), we define: \(x^{}=x_{1}^{_{1}} x_{d}^{_{d}}\), \(_{x}^{}h=h}{ x_{1}^{_{1}}  x_{d}^{_{d}}}\), \(_{x}h=[},,}]^{}\), and \(!=_{1}!_{d}!\).

We aim to show that a neural network is more sensitive to adversarial perturbations when it receives a backdoor attack, especially in near-OOD data. Let \(h(w,x):^{d_{w}}^{d_{x}}\) be a black-box function (e.g., loss or output of a neural network) with learnable parameters \(w\) and input \(x\).

**Adversarial risk** of \(h\) in radius \(\) under a distribution \(\) is defined as follows:

\[_{}^{}(h,w):=_{s}[ _{\|s\|}h(w,x+)-h(w,x)]_{x }\|_{x}h(w,x)\|.\]

The approximation converges as \( 0\), thus we use the last term in our analysis similar to .

We formulate a near-OOD around \(\) by shifting only the moments of an order \(k\). Formally, for any \(k\) and \(s\), we define \(_{+s}^{k}\) by \(_{x_{+s}^{k}}[x^{v}]=_{x }[x^{v}]+s\) for any \(v_{0}^{d_{x}}\) with \(|v|=k\), and \(_{x_{+s}^{k}}[x^{u}]=_{x }[x^{u}]\) for any \(u_{0}^{d_{x}}\) with \(|u| k\). The following theorem shows that the adversarial risk under \(_{+s}^{k}\) will increase linearly in terms of \(|s|\). The proof is given in Appendix Section F.

**Theorem 1**.: _(Adversarial risk in near-OOD)_

\[_{}^{_{+s}^{k}}(h,w)|s|_{x}\| _{x}_{||=k}^{}h(w,x)}{!}\|-\| _{x}_{x}h(w,x)\|.\]

_Remark 1_.: Theorem 1 is applicable when \(_{x_{i}}^{k+1}h 0\) which is usually true if \(h\) contains non-linear exponential activation functions (e.g., softmax, sigmoid, tanh, ELU, and SELU) being infinitely many times differentiable, or if it contains polynomial activation functions with total degree greater than \(k+1\). Under this assumption, if we consider \(h(w,.)\) as a fixed model trained on a fixed distribution \(\), then the only variable in the lower bound will be \(|s|\) hence we conclude \(_{}^{_{+s}^{k}}(h,w)=(|s|)\).

We now study how the adversarial risk will increase under a backdoor attack. Let \(=\{(x_{i},y_{i})={w^{}}^{}x_{i}):1 i n\}\) with \(x_{i}}{{}}\) be the clean training set, \(^{}=\{(x_{i}^{}+t,y_{c}):1 i m\}\) with \(x_{i}^{}}{{}}\) be the poisoned training set, \(t^{d_{x}}\) be the trigger, and \(y_{c}\) be the target class of the attack. We consider \(\) as the optimal solution of the least square optimization on the data \(^{}\):

\[=*{arg\,min}_{w}(_{i=1}^{n}(h(w,x_{i})-y_{i})^{2 }+_{i=1}^{m}(h(w,(x_{i}^{}+t))-y_{c})^{2})\] (4)

We focus on linear and two-layer networks defined as follows:

\[h_{1}(w,x)={w^{}}x, h_{2}(w,x)=}}_{j=1}^{l }u_{j}(_{j}^{T}x),\]

where in the latter \(w=[_{j}^{},u_{j}]_{j=1}^{l}^{l(d_{x}+1)}\) represents the vectorized parameters of the network, with each pair \([_{j}^{},u_{j}]^{d_{x}+1}\), and \((z)=\{0,z\}\) is the activation function. We approximate \(h_{2}(w,x)\) using the neural tangent kernel (NTK)  method with first-order Taylor expansion around an initial point \(w_{0}\):

\[}(w,x)=h_{2}(w_{0},x)+_{w}h_{2}(w_{0},x)^{T}(w-w_{0}).\]

We use the same gradient descent training process as in . The following theorem shows that as the ratio of triggered samples, i.e., \(\), or the norm of the trigger \(t\) increases, then the adversarial risk will also increase linearly. The proof is given in Appendix Section F.

**Theorem 2**.: _(Adversarial risk after backdoor attack) for \(h\{h_{1},}\}\), if \(\) is learned through the Equation 4 on a fixed training distribution \(\), we have:_

\[_{n}_{}^{}(h,)=( \|t\|).\]

## 6 Experiments

We evaluated our proposed method across a diverse range of benchmarks and compared its performance with various existing scanning methods. We developed our benchmark, which includes models trained on a broad spectrum of image datasets. This benchmark includes trojaned models for which various attack scenarios have been considered. The results of these experiments are provided in Table 1. Furthermore, we present an evaluation of TrojAI in Table 2 as a challenging benchmark.

**Baselines.** In our evaluation, TRODO and TRODO-Zero are assessed alongside previous SOTA scanning methods including Neural Cleanse (NC) , ABS , PT-RED , TABOR , K-Arm , MM-BD , and UMD . Performance details are in Table 1, with further information in Appendix Section I and K.

Implementation Details.As stated earlier, we used Tiny ImageNet as our validation set to tune our hyperparameters \(\) and \(\) (scanning threshold); details are provided in Table 10. We used PGD-10 as the adversarial attack. Our experiments on our method and other baselines were conducted on a single RTX \(3090\) GPU.

Our Designed Benchmark.We developed a benchmark to model real-world scanning scenarios, including various datasets, classifiers, trojan attacks, and label mappings. This benchmark covers both standard and adversarial training methods, ensuring a comprehensive evaluation of scanning methods. Our benchmark includes image datasets from CIFAR10, CIFAR100 , GTSRB , PubFig , and MNIST, with two label mappings: All to One and All to All. It incorporates eight trojan attacks: BadNet , Input-aware , BPP , SIG , WaNet , Color , SSBA  and Blended . Each combination of a dataset and label mapping has 320 models: 20 trojaned models per attack and 160 clean models (check Appendix Section N for more details). Both standard and adversarial training were employed. We considered various architectures, including ResNet18 , PreActResNet18 , and ViT-B/16 . While previous works focused on CNN-based architectures,

    &  &  &  &  &  &  \\   &  & ACC & ACC* & ACC & ACC* & ACC & ACC* & ACC* & ACC & ACC* & ACC & ACC* & **ACC** & **ACC* \\   & NC & 54.3 & 49.8 & 53.2 & 48.4 & 62.8 & 56.3 & 52.1 & 42.1 & 52.5 & 40.2 & 55.0 & 49.4 \\   & ABS & 67.5 & 69.0 & 64.1 & 65.6 & 71.2 & 65.5 & 56.4 & 54.2 & 56.3 & 58.3 & 63.1 & 62.5 \\   & PT-RED & 51.0 & 48.8 & 50.4 & 46.1 & 58.4 & 57.5 & 50.9 & 45.3 & 49.1 & 47.9 & 52.0 & 49.1 \\   & TAQOR & 60.5 & 45.0 & 56.3 & 44.7 & 69.0 & 53.8 & 56.7 & 45.5 & 58.6 & 44.2 & 60.2 & 46.6 \\   & K-ARM & 68.4 & 55.1 & 66.7 & 54.8 & 70.1 & 62.8 & 59.8 & 50.9 & 60.2 & 47.6 & 65.0 & 54.2 \\   & MNTD & 57.4 & 51.3 & 56.9 & 52.3 & 65.2 & 55.9 & 54.4 & 48.8 & 56.7 & 50.0 & 58.1 & 54.7 \\   & FreeEagle & 80.2 & 72.9 & 82.0 & 73.2 & 81.0 & 82.3 & 73.2 & 66.9 & 65.0 & 66.0 & 76.3 & 72.3 \\   & MM-BD & 85.2 & 65.4 & 77.3 & 57.8 & 79.6 & 65.2 & **88.5** & 74.0 & 65.7 & 48.3 & 79.3 & 62.1 \\   & UMD & 81.1 & 61.2 & 77.5 & 54.7 & 81.4 & 68.2 & 69.0 & 56.3 & 67.9 & 49.7 & 75.4 & 58.0 \\   & **TRODO-Zero** & 80.9 & 79.3 & 82.7 & 78.5 & 84.8 & 83.3 & 75.5 & 73.7 & 73.2 & 70.6 & 79.4 & 77.0 \\   & **TRODO** & **91.2** & **89.6** & **91.0** & **88.4** & **96.6** & **93.2** & 86.7 & **82.5** & **88.1** & **83.0** & **90.7** & **87.3** \\   & NC & 26.7 & 21.6 & 24.9 & 19.6 & 31.6 & 23.2 & 15.4 & 11.8 & 16.8 & 12.3 & 23.1 & 17.7 \\   & ABS & 32.5 & 34.1 & 30.7 & 28.8 & 23.6 & 20.5 & 34.3 & 34.8 & 31.0 & 28.2 & 30.4 & 29.3 \\   & PT-RED & 41.0 & 33.5 & 39.6 & 33.1 & 45.4 & 43.9 & 20.3 & 15.2 & 12.6 & 9.8 & 31.8 & 27.1 \\   & TAQOR & 51.7 & 39.7 & 50.2 & 37.8 & 48.3 & 39.5 & 39.4 & 30.2 & 38.6 & 30.8 & 45.6 & 35.6 \\   & K-ARM & 56.8 & 49.7 & 54.6 & 47.6 & 57.5 & 48.9 & 51.3 & 45.0 & 50.6 & 47.3 & 54.2 & 47.7 \\   & MNTD & 27.2 & 25.2 & 23.0 & 18.6 & 16.9 & 12.8 & 29.8 & 31.0 & 22.3 & 17.9 & 23.8 & 21.1 \\   & FreeEagle & 79.8 & 75.2 & 54.9 & 50.2 & 55.2 & 52.9 & 56.5 & 52.7 & 48.0 & 46.1 & 58.9 & 55.4 \\   & MM-BD & 54.3 & 40.4 & 49.4 & 35.1 & 57.9 & 44.0 & 40.7 & 32.3 & 41.2 & 34.1 & 48.7 & 37.2 \\   & UMD & 82.5 & 61.9 & 74.6 & 60.1 & 84.2 & 64.5 & 70.6 & 49.9 & 68.7 & 52.3 & 76.1 & 57.7 \\   & **TRODO-Zero** & 82.1 & 80.8 & 80.4 & 77.3 & 83.8 & 88.6 & 74.8 & 72.3 & 75.0 & 75.4 & 79.2 & 78.8 \\   & **TRODO** & **90.0** & **87.4** & **89.3** & **87.5** & **92.6** & **89.1** & **82.4** & **85.0** & **83.2** & **80.9** & **87.5** & **86.1** \\

[MISSING_PAGE_FAIL:9]

## 7 Ablation Study

**Ablation Study on Validation Dataset.** To ascertain the robustness of TRODO against different datasets in the validation set, we conducted experiments using various datasets as the validation set, as presented in Table 4. In these experiments, we replaced our default validation dataset, Tiny ImageNet, with alternative datasets. Throughout these tests, all other elements of our methodology remained constant to isolate the impact of the validation dataset changes on TRODO's performance. Moreover, to quantitatively support our claim regarding the effectiveness of near-OOD samples compared to far-OOD samples, we provide the distance between the validation set and the target dataset. The target dataset refers to the ID set on which the input classifier has been trained. For computing this distance, we used the Frechet Inception Distance (FID) , a well-known metric for measuring distance in generative models. Lower FID values indicate a smaller distance, and vice versa. As the results indicate, in the near-OOD scenario, our method appears more effective. More details can be found in Appendix Section L.

**Ablation Study on Boundary Confidence Level.** We also conducted an ablation study on the boundary confidence level hyperparameter, denoted as \(\), which is preset at 0.5 in our standard pipeline. By keeping all other variables constant and varying \(\) across a range of values, we assessed TRODO's sensitivity to this parameter. The results of these experiments are presented in the Table 5, illustrating how different settings of \(\) affect the effectiveness of TRODO (extra ablation studies are available in Appendix Section P).

## 8 Acknowledgments

We acknowledge Mohammad Sabokrou for his contributions to this project. Mohammad Sabokrou's work in this project was supported by JSPS KAKENHI Grant Number 24K20806.

## 9 Conclusion

In conclusion, this study presents TRODO, a robust and general method for scanning and identifying trojaned classifiers with low time and resource complexity. TRODO's strength lies in its ability to detect trojans in diverse scenarios, including those involving adversarially trained models. Interestingly, TRODO is applicable even in scenarios where no data is available. Our experimental results demonstrate TRODO's superior performance, achieving high accuracy across various attack types and benchmark datasets. The adaptability and effectiveness of our approach mark a significant advancement in enhancing the reliability and security of deep neural networks in critical applications.

    &  &  &  &  &  \\   & Accuracy & FID & Accuracy & FID & Accuracy & FID & Accuracy & FID & Accuracy & FID \\  FMNIST & 94.6 & 67 & 78.7 & 145 & 80.4 & 156 & 69.5 & 138 & 72.1 & 120 \\  SVHN & 92.3 & 118 & 82.6 & 92 & 84.3 & 105 & 74.3 & 124 & 73.2 & 137 \\  STL-10 & 70.9 & 134 & 96.8 & 76 & 95.2 & 86 & 82.0 & 91 & 85.4 & 89 \\  Tiny ImageNet & 91.2 & 108 & 91.0 & 72 & 96.6 & 84 & 86.7 & 79 & 88.1 & 96 \\   

Table 4: Accuracy of TRODO using various Validation (and OOD) datasets for different ID data. Each validation is used to find the hyperparameters (\(\) and \(\)) and also as OOD datasets to find signatures. You can see the effect of choosing near-OOD dataset. For example, for CIFAR10, STL-10 and Tiny ImageNet are better choices than the other two datasets

    \\   & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\  MNIST & 81.0 & 74.8 & 89.1 & 91.2 & 85.2 & 75.6 & 81.8 \\  CIFAR10 & 88.0 & 79.7 & 85.6 & 91.0 & 81.0 & 87.5 & 77.4 \\  GTSRB & 94.0 & 91.3 & 92.6 & 96.6 & 90.1 & 88.6 & 92.2 \\  CIFAR100 & 77.1 & 82.7 & 80.2 & 86.7 & 84.2 & 84.3 & 76.6 \\  PubFig & 78.7 & 82.5 & 84.4 & 88.1 & 90.3 & 86.2 & 79.8 \\   

Table 5: Accuracy of our method with different boundary confidence level.