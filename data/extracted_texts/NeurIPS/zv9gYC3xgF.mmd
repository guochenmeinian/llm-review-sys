# Toward Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixture Models

Weihang Xu

University of Washington

xuwh@cs.washington.edu

&Maryam Fazel

University of Washington

mfazel@uw.edu

&Simon S. Du

University of Washington

ssdu@cs.washington.edu

###### Abstract

We study the gradient Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMM) in the over-parameterized setting, where a general GMM with \(n>1\) components learns from data that are generated by a single ground truth Gaussian distribution. While results for the special case of 2-Gaussian mixtures are well-known, a general global convergence analysis for arbitrary \(n\) remains unresolved and faces several new technical barriers since the convergence becomes sub-linear and non-monotonic. To address these challenges, we construct a novel likelihood-based convergence analysis framework and rigorously prove that gradient EM converges globally with a sublinear rate \(O(1/)\). This is the first global convergence result for Gaussian mixtures with more than \(2\) components. The sublinear convergence rate is due to the algorithmic nature of learning over-parameterized GMM with gradient EM. We also identify a new emerging technical challenge for learning general over-parameterized GMM: the existence of bad local regions that can trap gradient EM for an exponential number of steps.

## 1 Introduction

Learning Gaussian Mixture Models (GMM) is a fundamental problem in machine learning with broad applications. In this problem, data generated from a mixture of \(n 2\) ground truth Gaussians are observed without the label (the index of component Gaussian that data is sampled from), and the goal is to retrieve the maximum likelihood estimation of Gaussian components. The Expectation Maximization (EM) algorithm is arguably the most widely-used algorithm for this problem. Each iteration of the EM algorithm consists of two steps. In the expectation (E) step, it computes the posterior probability of unobserved mixture membership label according to the current parameterized model. In the maximization (M) step, it computes the maximizer of the \(Q\) function, which is the likelihood with respect to posterior estimation of the hidden label computed in the E step.

Gradient EM, as a popular variant of EM, is often used in practice when the maximization step of EM is costly or even intractable. It replaces the M step of EM with taking one gradient step on the \(Q\) function. Learning Gaussian Mixture Models with EM/gradient EM is an important and widely-studied problem. Starting from the seminal work (Balakrishnan et al., 2014), a flurry of work (Daskalakis et al., 2017); Xu et al. (2016); Dwivedi et al. (2018); Kwon and Caramanis (2020); Dwivedi et al. (2019) have studied the convergence guarantee for EM/gradient EM in various settings. However, these works either only prove local convergence, or consider the special case of 2-Gaussian mixtures. A general global convergence analysis of EM/gradient EM on \(n\)-Gaussian mixtures still remains unresolved. Jin et al. (2016) is a notable negative result in this regard, where the authors show that on GMM with \(n 3\) components, randomly initialized EM will get trapped in a spurious local minimum with high probability.

**Over-parameterized Gaussian Mixture Models.** Motivated by the negative results, a line of work considers the over-parameterized setting where the model uses more Gaussian components thanthe ground truth GMM, in the hope that it might help the global convergence of EM and bypass the negative result. In such over-parameterized regime, the best that people know so far is from (Dwivedi et al., 2018). This work proves global convergence of 2-Gaussian mixtures on one single Gaussian ground truth. The authors also show that EM has a unique sub-linear convergence rate in this over-parameterized setting (compared with the linear convergence rate in the exact-parameterized setting (Balakrishnan et al., 2014)). This motivates the following natural open question:

_Can we prove global convergence of the EM/gradient EM algorithm on general \(n\)-Gaussian mixtures in the over-parameterized regime?_

In this paper, we take a significant step towards answering this question. Our main contributions can be summarized as follows:

* We prove global convergence of the gradient EM algorithm for learning general \(n\)-component GMM on one single ground truth Gaussian distribution. This is, to the best of our knowledge, the first global convergence proof for general \(n\)-component GMM. Our convergence rate is sub-linear, reflecting an inherent nature of over-parameterized GMM (see Remark 3 for details).
* We propose a new analysis framework that utilizes the likelihood function for proving convergence of gradient EM. Our new framework tackles several emerging technical barriers for global analysis of general GMM.
* We also identify a new geometric property of gradient EM for learning general \(n\)-component GMM: There exists bad initialization regions that traps gradient EM for exponentially long, resulting in an inevitable exponential factor in the convergence rate of gradient EM.

### Gaussian Mixture Model (GMM)

We consider the canonical Gaussian Mixture Models with weights \(=(_{1},,_{n})\) (\(_{i=1}^{n}_{i}=1\)), means \(=(_{1}^{},,_{n}^{})^{}\) and unit covariance matrices \(I_{d}\) in \(d\)-dimensional space. Following a widely-studied setting (Balakrishnan et al., 2014; Yan et al., 2017; Daskalakis et al., 2017), we set the weights \(\) and covariances \(I_{d}\) in student GMM as fixed, and the means \(=(_{1}^{},,_{n}^{})^{}\) as trainable parameters. We use GMM(\(\)) to denote the GMM model parameterized by \(\), which can be described with probability density function (PDF) \(p_{}:^{d}_{ 0}\) as

\[p_{}(x)=_{i[n]}_{i}(x|_{i},I_{d})=_{i[n]}_{ i}(2)^{-d/2}(-\|^{2}}{2}),\] (1)

where \((|,)\) is the PDF of \((,)\), \(_{1}++_{n}=1,_{i}>0, i[n]\).

### Gradient EM algorithm

The EM algorithm is one of the most popular algorithms for retrieving the maximum likelihood estimator (MLE) on latent variable models. In general, EM and gradient EM address the following problem: given a joint distribution \(p_{^{*}}(x,y)\) of random variables \(x,y\) parameterized by \(^{*}\), observing only the distribution of \(x\), but not the latent variable \(y\), the goal of EM and gradient EM is to retrieve the maximum likelihood estimator

\[}_{}_{} p_{}(x).\]

The focus of this paper is the non-convex optimization analysis, so we consider using _population gradient EM_ algorithm to learn GMM (1), where the observed variable is \(x^{d}\) and latent variable is the index of membership Gaussian in GMM. We follow the standard teacher-student setting where a student model GMM(\(\)) with \(n 2\) Gaussian components learns from data generated from a ground truth teacher model GMM(\(^{*}\)). We consider the over-parameterized setting where the ground truth model GMM(\(^{*}\)) is a single Gaussian distribution \((^{*},I_{d})\), namely \(^{*}=({^{*}}^{},,{^{*}}^{})^{}\). We can then further assume \(w.l.o.g.\) that \(^{*}=0\). Our problem could be seen as a strict generalization of Dwivedi et al. (2018), where they studied using mixture model of _two Gaussians_ with symmetric means (they set constraint \(_{2}=-_{1}\)) to learn one single Gaussian.

At time step \(t=0,1,2,\), given with parameters \((t)=(_{1}(t)^{},,_{n}(t)^{})^{}\), population gradient EM updates \(\) via the following two steps

[MISSING_PAGE_FAIL:3]

### Technical overview

Here we provide a brief summary of the major technical barriers for our global convergence analysis and our techniques for overcoming them.

**New likelihood-based analysis framework.** The traditional convergence analysis for EM/gradient EM in previous works Balakrishnan et al. (2014); Yan et al. (2017); Kwon and Caramanis (2020) proceeds by showing the distance between the model and the ground truth GMM in the _parameter space_ contracts linearly in every iteration. This type of approach meets new challenges in the over-parameterized \(n\)-Gaussian mixture setting since the convergence is both sub-linear and non-monotonic. To address these problems, we propose a new likelihood-based convergence analysis framework: instead of proving the convergence of parameters, our analysis proceeds by showing the likelihood loss function \(\) converges to \(0\). The new analysis framework is more flexible and allows us to overcome the aforementioned technical barriers.

**Gradient lower bound.** The first step of our global convergence analysis constructs a gradient lower bound. Using some algebraic transformation techniques, we convert the gradient projection \((),\) into the expected norm square of a random vector \(}(x)\). (See Section (4) for the full definition). Although lower bounding the expectation of \(}\) is very challenging, our key idea is that the gradient of \(}\) has very nice properties and can be easily lower bounded, allowing us to establish the gradient lower bound.

**Local smoothness and regularity condition.** After obtaining the gradient lower bound, the missing component of the proof is a smoothness condition of the loss function \(\). Since proving the smoothness of \(\) is hard in general, we define and prove a weaker notion of local smoothness, which suffices to prove our result. In addition, we design and use an auxiliary function \(U\) to show that gradient EM trajectory satisfies the locality required by our smoothness lemma.

## 2 Related work

### 2-Gaussian mixtures

There is a vast literature studying the convergence of EM/gradient EM on \(2\)-component GMM. The initial batch of results proves convergence within a infinitesimally small local region (Xu and Jordan, 1996; Ma et al., 2000). Balakrishnan et al. (2014) proves for the first time convergence of EM and gradient EM within a non-infinitesimal local region. Among the later works on the same problem, Klusowski and Brinda (2016) improves the basin of convergence guarantee, Daskalakis et al. (2017); Xu et al. (2016) proves the global convergence for \(2\)-Gaussian mixtures. These works focused on the exact-parameterization scenario where the number of student mixtures is the same as that of the ground truth. More recently, Wu and Zhou (2019) proves global convergence of \(2\)-component GMM without any separation condition. Their result can be viewed as a convergence result in the over-parameterized setting where the student model has two Gaussians and the ground truth is a single Gaussian. On the other hand, their setting is more restricted than ours because they require the means of two Gaussians in the student model to be symmetric around the ground truth mean. Weinberger and Bresler (2021) extends the convergence guarantee to the case of unbalanced weights. Another line of work Dwivedi et al. (2018, 2019, 2018) studies the over-parameterized setting of using \(2\)-Gaussian mixture to learn a single Gaussian and proves global convergence of EM. Our result extends this type of analysis to the general case of \(n\)-Gaussian mixtures, which requires significantly different techniques. We note that going beyond Gaussian mixture models, there are also works studying EM algorithms for other mixture models such as a mixture of linear regression Kwon et al. (2019).

### N-Gaussian mixtures

Another line of results focuses on the general case of \(n\) Gaussian mixtures. Jin et al. (2016) provides a counter-example showing that EM does not converge globally for \(n>2\) (in the exact-parameterized case). Dasgupta and Schulman (2000) prove that a variant of EM converges to MLE in two rounds for \(n\)-GMM. Their result relies on a modification of the EM algorithm and is not comparable with ours. (Chen et al., 2023) analyzes the structure of local minima in the likelihood function of GMM. However, their result is purely geometric and does not provide any convergence guarantee.

A series of paper Yan et al. (2017); Zhao et al. (2018); Kwon and Caramanis (2020); Segol and Nadler follow the framework proposed by Balakrishnan et al. (2014) to prove the _local_ convergence of EM for \(n\)-GMM. While their result applies to the more general \(n\)-Gaussian mixture ground truth setting, their framework only provides local convergence guarantee and cannot be directly applied to our setting.

### Slowdown due to over-parameterization

This paper gives an \(O(1/)\) bound for fitting over-parameterized Gaussian mixture models to a single Gaussian. Recall that to learn a single Gaussian, if one's student model is also a single Gaussian, then one can obtain an \((-(t))\) rate because the loss is strongly convex. This slowdown effect due to over-parameterization has been observed for Gaussian mixtures in Dwivedi et al. (2018); Wu and Zhou (2019), but has also been observed in other learning problems, such as learning a two-layer neural network Xu and Du (2023); Richert et al. (2022) and matrix sensing problems (Xiong et al., 2023; Zhang et al., 2021; Zhuo et al., 2021).

## 3 Main results

In this section, we present our main theoretical result, which consists of two parts: In Section 3.1 we present our global convergence analysis of gradient EM, in Section 3.2 we prove that an exponentially small factor in our convergence bound is inevitable and cannot be removed. All omitted proofs are deferred to the appendix.

### Global convergence of gradient EM

We first present our main result, which states that gradient EM converges to MLE globally.

**Theorem 2** (Main result).: _Consider training a student \(n\)-component GMM initialized from \((0)=(_{1}(0)^{},,_{n}(0)^{})^{}\) to learn a single-component ground truth GMM \((0,I_{d})\) with population gradient EM algorithm. If the step size satisfies \( O(^{2}}{n^{2}d^{2}((0)}+_{}(0))^{2}})\), then gradient EM converges globally with rate_

\[((t))},\]

_where \(=(^{4}}{n^{2}d^{2}(1+_{ }(0))^{4}})^{+}\). Recall that \(_{}(0)=\{\|_{1}(0)\|,,\|_{n}(0)\|\}\) and \(U(0)=_{i[n]}\|_{i}(0)\|^{2}\) are two initialization constants._

**Remark 3**.: _Without over-parameterization, for learning a single Gaussian, one can obtain a linear convergence \((-(t))\). We would like to note that the sub-linear convergence rate guarantee of gradient EM stated in Theorem 2 (\(((t)) O(1/)\)) is due to the inherent nature of the algorithm. Dwivedi et al. (2018) studied the special case of using 2 Gaussian mixtures with symmetric means to learn a single Gaussian and proved that EM has sublinear convergence rate when the weights \(_{i}\) are equal. Since Theorem 2 studies the more general case of \(n\) Gaussian mixtures, this type of subexponential convergence rate is the best than we can hope for._

**Remark 4**.: _The convergence rate in Theorem 2 has a factor exponentially small in the initialization scale (\((-16U(0))\)). We would like to stress that this is again due to algorithmic nature of the problem rather than the limitation of analysis. In Section 3.2, we prove that there exists bad regions with exponentially small gradients so that when initialized from such region, gradient EM gets trapped locally for \(((U(0)))\) number of steps. Therefore, a convergence speed guarantee exponentially small in \(U(0)\) is inevitable and cannot be improved._

**Remark 5**.: _Theorem 2 is fundamentally different from convergence analysis for EM/gradient EM in previous works Yan et al. (2017); Dwivedi et al. (2019); Balakrishnan et al. (2014) which proved monotonic linear contraction of parameter distance \(\|(t)-^{*}\|\). But our result also implies global convergence since loss function \(\) converging to \(0\) is equivalent to convergence of gradient EM to MLE._

**Remark 6**.: _The convergence result in Theorem 2 is for population gradient EM, but it also implies global convergence for sample-based gradient EM as the sample size tends to infinity. For a similar reduction from population EM to sample EM, see Section 2.2 of (Xu et al., 2016)._

### Necessity of exponentially small factor in convergence rate

In this section we prove that a factor exponentially small in initialization scale (\((-(U(0)))\)) is inevitable in the global convergence rate guarantee of gradient EM. Particularly, we show the existence of bad regions such that initialization from this region traps gradient EM for exponentially long time before final convergence. Our result is the following theorem.

**Theorem 7** (Existence of bad initialization region).: _For any \(n 3\), define \(}(0)=(_{1}^{}(0),,_{n}^{}(0))\) as follows: \(_{1}(0)=12e_{1},_{2}(0)=-12e_{1},_{3}(0)==_{n }(0)=0,\) where \(e_{1}\) is a standard unit vector. Then population gradient EM initialized with means \(}(0)\) and equal weights \(_{1}==_{n}=1/n\) will be trapped in a bad local region around \(}(0)\) for exponentially long time_

\[Te^{d}=((U(0))).\]

_More rigorously, for any \(0 t T, i[n]\) such that_

\[\|_{i}(t)\| 10.\]

Theorem 7 states that, when initialized from some bad points \((0)\), after \(((U(0)))\) number of time steps, gradient EM will still stay in this local region and remain \(10\) distance away from the global minimum \(=0\). Therefore an exponentially small factor in convergence rate is inevitable.

**Remark 8**.: _Theorem 7 eliminates the possibility of proving any polynomial convergence rate of gradient EM from arbitrary initialization. However, it is still possible to prove that, with some specific smart initialization schemes, gradient EM avoids the bad regions stated in Theorem 7 and enjoys a polynomial convergence rate. We leave this as an interesting open question for future analysis._

## 4 Proof overview

In this section, we provide a technical overview of the proof in our main result (Theorem 2 and Theorem 7).

### Difficulties of a global convergence proof and our new analysis framework

Proving the global convergence of gradient EM for general \(n\)-Gaussian mixture is highly nontrivial. While there have been many previous works (Balakrishnan et al., 2014; Yan et al., 2017; Dwivedi et al., 2018) studying either local convergence or the special case of \(2\)-Gaussian mixtures, they all focus on showing the contraction of parametric error. Namely, their proof proceeds by showing the distance between the model parameter and the ground truth contracts, usually by a fixed linear ratio, in each iteration of the algorithm. However, this kind of approach faces various challenges for our general problem where the convergence is both _sublinear_ and _non-monotonic_. Since the convergence rate is sublinear (see Remark 3), showing a linear contraction per iteration is no longer possible. Since the convergence is non-monotonic1, we also cannot show a strictly decreasing parametric distance.

To address these challenges, we propose a new convergence analysis framework for gradient EM by proving the convergence of _likelihood_\(\) instead of the convergence of parameters \(\). There are several benefits for considering the convergence from the perspective of MLE loss \(\). Firstly, it naturally addresses the problem of non-monotonic and sub-linear convergence since we only need to show \(\) decreases as the algorithm updates. Also, since gradient EM is equivalent with running gradient descent on loss function \(\) (see Section 1.3), we can apply techniques from the optimization theory of gradient descent to facilitate our analysis.

### Proof ideas for Theorem 2

We first briefly outline our proof of Theorem 2.

**Proof roadmap.** Our proof of Theorem 2 consists of three steps. Firstly, we prove a gradient lower bound for \(\) (Theorem 12). Then we prove that the MLE \(\) is _locally smooth_ (Theorem 13). Finally,we combine the gradient lower bound and the smoothness condition to prove the global convergence of \(\) with mathematical induction.

**Step 1: Gradient lower bound.**

Our first step aims to show that the gradient norm of \(()\) is lower bounded by the distance of \(\) to the ground truth. To do this, we need a few preliminary results. Inspired by Chen et al. (2023), we use Stein's identity (Stein, 1981) to perform an algebraic transformation of the gradient. Recalling the definition of \(_{i}\) in (2), we have the following lemma.

**Lemma 9**.: _For any GMM\((),i[n]\), the gradient of \(Q\) satisfies_

\[_{_{i}}()=_{_{i}}Q(|)=_{x}[_{i}(x)_{k[n]}_{k}(x)_{k}].\]

The gradient expression above is equivalent with the form in (3), but is easier to manipulate. Using the transformed gradient in Lemma 9, we have the following corollary.

**Corollary 10**.: _Define vector \(}_{}(x)_{i[n]}_{i}(x)_{i}\). For any GMM\(()\), the projection of the gradient of \(()\) onto \(\) satisfies_

\[(),=_{}Q( |),=_{i[n]}_{_{i}} Q(|),_{i}=_{x}[\|}_{}(x)\|^{2}].\]

Corollary 9 is important since it converts the projection of gradient \(()\) onto \(\) to the expected norm square of a vector \(}_{}\). Since a lower bound of the gradient projection implies a lower bound of the gradient, we only need to construct a lower bound for \((),=_{x}[\| }_{}(x)\|^{2}]\). Since \(\|}_{}(x)\|^{2}\) is always non-negative, we already know that the gradient projection is non-negative. But lower bounding \(_{x}[\|}_{}(x)\|^{2}]\) is still highly nontrivial since the expression of \(}\) is complicated and hard to handle. However, our key observation is that, _although \(}\) itself is hard to bound, its gradient has nice properties and can be handled gracefully_:

\[_{x}}_{}(x)=_{i,j[n]}_{i} (x)_{j}(x)(_{i}-_{j})(_{i}-_{j})^{}.\] (5)

The gradient (5) is nicely-behaved. One can see immediately from (5) that the matrix \(_{x}}_{}(x)\) is positive-semi-definite, and its eigenvalues can be directly bounded. To utilize these properties, we use the following algebraic trick to convert the task of lower bounding \(}\) itself into the task of lower bounding its gradient.

\[_{x}[\|}_{}(x)\|^{2}]= _{x}[(_{t=-1}^{1}\|x\|^{ }}_{}(tx)t)^{2} ].\] (6)

Recall that \(=\). See detailed derivation in (23). Using (5), combined with the properties of \(_{x}}_{}(x)\), we can obtain the following lemma (Recall that \(U=_{i[n]}\|_{i}\|^{2}\).):

**Lemma 11**.: _For any GMM\(()\) we have_

\[_{x}[\|}_{}(x)\|^{2}]}{40000d(1+2_{})^{2}}(_{i,j[n]}_{i} _{j}\|_{i}-_{j}\|^{2})^{2}.\]

On top of Lemma 11, we can easily lower bound the gradient projection in the following lemma, finishing the first step of our proof.

**Lemma 12** (Gradient projection lower bound).: _For any GMM\(()\) we have_

\[_{}Q(|),=_{x}[\| }_{}(x)\|^{2}]=(\,_{ }^{2}}{d(1+_{})^{2}}_{}^{4}).\]

#### Step 2: Local smoothness.

To construct a global convergence analysis for gradient-based methods, after obtaining a gradient lower bound, we still need to prove the smoothness of loss \(\). (Recall that global smoothness of function \(f\) means that there exists constant \(C\) such that \(\| f(x_{1})- f(x_{2})\| C\|x_{1}-x_{2}\|, x_{1},x_{2}\).) However, proving the smoothness for \(\) in general is very challenging since the membership function \(_{i}\) cannot be bounded when \(\) is unbounded. To address this issue, we prove that \(\) is locally smooth, \(i.e.\), the smoothness between two points \(\) and \(^{}\) is satisfied if both \(\|\|\) and \(\|-^{}\|\) are upper bounded. Our result is the following theorem.

**Theorem 13** (Local smoothness of loss function).: _At any two points \(=(_{1}^{},,_{n}^{})^{}\) and \(+=((_{1}+_{1})^{},,(_{n}+_{n}) ^{})^{}\), if_

\[\|_{i}\|\|\}}}, i[n],\]

_then the loss function \(\) satisfies the following smoothness property: for any \(i[n]\) we have_

\[\|_{_{i}+_{i}}(+)-_{_{i }}()\| n_{}(30+4_{})\|_{i} \|+_{k[n]}\|_{k}\|.\] (7)

#### Step 3: putting everything together.

Given the gradient lower bound and the smoothness condition, we still need to resolve two remaining problems. The first one is that the gradient lower bound in Lemma 12 is given in terms of \(\), which we need to convert to a lower bound in terms of \(()\). For this we need the following upper bound of \(\).

**Theorem 14** (Loss function upper bound).: _The loss function can be upper bounded as_

\[()_{i[n]}}{2}\|_{i}\|^{2} ^{2}}{2}.\]

The second problem is that our local smoothness theorem requires \(\) to be bounded, therefore we need to show a regularity condition that for each \(i\), \(_{i}(t)\) stays in a bounded region during gradient EM updates. This is not easy to prove for each individual \(_{i}\) due to the same non-monotonic issue mentioned in Section 4.1. To establish such a regularity condition, we use the potential function.\(U\) to solve this problem. We prove that \(U\) remains bounded along the gradient EM trajectory, implying each \(_{i}\) remains well-behaved. With this regularity condition, combined with the previous two steps, we finish the proof of Theorem 2 via mathematical induction.

### Proof ideas for Theorem 7

Proving Theorem 7 is much simpler. The idea is natural: we found that there exists some bad regions where the gradient of \(\) is exponentially small, characterized by the following lemma.

**Lemma 15** (Gradient norm upper bound).: _For any \(\) satisfying \(\|_{1}\|,\|_{2}\| 10,\|_{3}\|,,\|_{n}\| \), the gradient of \(\) at \(\) can be upper bounded as_

\[\|_{_{i}}()\| 2(\|_{3}\|++\|_{n}\|)+ 2(-d)(\|_{1}\|+\|_{2}\|), i[n].\]

Utilizing Lemma 15, we can prove Theorem 7 by showing that initialization from these bad regions will get trapped in it for exponentially long, since the gradient norm is exponentially small. The full proof can be found in Appendix B.2.

## 5 Experiments

In this section we experimentally explore the behavior of gradient EM on GMMs.

**Convergence rates.** We choose the experimental setting of \(d=5,=0.7\). We use \(n=2,5,10\) Gaussian mixtures to learn data generated from one single ground truth Gaussian distribution \((^{*},I_{d})\), respectively. Since a closed form expression of the population gradient is intractable, we approximate the gradient step via Monte Carlo method, with sample size \(3.5 10^{5}\). The mixing weights of student GMM are randomly sampled from a standard Dirichlet distribution and set as fixed during gradient EM update. The covariances of all component Gaussians are set as the identity matrix. We recorded the convergence of likelihood function \(\) (estimated also by Monte Carlo method on fresh samples each iteration) and parametric distance \(_{i[n]}_{i}\|_{i}-^{*}\|^{2}\) along gradient EM trajectory. The results are reported in Figure 1 (left and middle panel). Both the likelihood \(\) and the parametric distance converges sub-linearly.

**Weight configurations.** We train \(3\)-component GMM with \(3\)-different weight configurations and report \(4\) runs each configuration in Figure 1 (right). Blue: \((,,)\). Orange: \((,,)\), Green: \((,,)\). More evenly distributed weights result in faster convergence.

**Initialization geometry.** We empirically study the bad initialization point \((0)\) described in Theorem 72 by plotting the gradient norm at \((0)\) w.r.t. different dimension \(d\) in Figure 2 (left). As theoretically analyzed, the gradient norm \(\|((0))\|\) at \((0)\) decreases exponentially in dimension \(d\).

**Statistical rates.** The statistical rate for EM/gradient EM is another interesting research problem, which we observe empirically in Figure 2 (right). We run gradient EM on \(5\)-component GMM with equal weights. x-axis: number of training samples, y-axis: parametric error after convergence. For each sample size, we run \(50\) times and report the average. The statistical errors are reported in the blue line. The red line (function \((n^{-1/4})\)) and green line (linear regression output fitting blue points) are references. The trajectory approximately follows the law of \(accuracy n^{-1/4}\). While (Wu and Zhou, 2019) rigorously proves the asymptotic statistical rate of \((n^{-1/4})\) for the special

Figure 1: Left: Sublinear convergence of the likelihood loss \(\). Middle: Sublinear convergence of the parametric distance \(_{i[n]}_{i}\|_{i}-^{*}\|^{2}\) between student GMM and the ground truth. Right: Impact of different mixing weights on the convergence speed.

Figure 2: Left: Gradient norm \(\|((0))\|\) in the counter-example in Theorem 7 decreases exponentially fast w.r.t. dimension \(d\). Right: The statistical error (blue line) approximately scales as \( n^{-1/4}\) with sample size \(n\).

case of 2-GMMs, our experiments imply that the same rate might also apply to the general case of multi-component GMMs.

## 6 Conclusion

This paper gives the first global convergence of gradient EM for over-parameterized Gaussian mixture models when the ground truth is a single Gaussian, and rate is sublinear which is exponentially slower than the rate in the exact-parameterization case. One fundamental open problem is to study when one can obtain global convergence of EM or gradient EM for Gaussian mixture models when the ground truth has multiple components. The likelihood-based convergence framework proposed in this paper might be an helpful tool towards solving this general problem.

AcknowledgementsThis work was supported in part by the following grants: NSF TRIPODS II-DMS 20231660, NSF CCF 2212261, NSF CCF 2007036, NSF AF 2312775, NSF IIS 2110170, NSF DMS 2134106, NSF IIS 2143493, and NSF IIS 2229881.