# (Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy

Elan Rosenfeld

Machine Learning Department

Carnegie Mellon University

elan@cmu.edu

&Saurabh Garg

Machine Learning Department

Carnegie Mellon University

###### Abstract

We derive a new, (almost) guaranteed upper bound on the error of deep neural networks under distribution shift using unlabeled test data. Prior methods are either vacuous in practice or accurate on average but heavily underestimate error for a sizeable fraction of shifts. In particular, the latter only give guarantees based on complex continuous measures such as test calibration, which cannot be identified without labels, and are therefore unreliable. Instead, our bound requires a simple, intuitive condition which is well justified by prior empirical works and holds in practice effectively 100% of the time. The bound is inspired by \(\)-divergence but is easier to evaluate and substantially tighter, consistently providing non-vacuous test error upper bounds. Estimating the bound requires optimizing one multiclass classifier to disagree with another, for which some prior works have used sub-optimal proxy losses; we devise a "disagreement loss" which is theoretically justified and performs better in practice. We expect this loss can serve as a drop-in replacement for future methods which require maximizing multiclass disagreement. Across a wide range of natural and synthetic distribution shift benchmarks, our method gives valid error bounds while achieving average accuracy comparable to--though not better than--competitive estimation baselines.

## 1 Introduction

When deploying a model, it is important to be confident in how it will perform under inevitable distribution shift. Standard methods for achieving this include data dependent uniform convergence bounds  (typically vacuous in practice) or assuming a precise model of how the distribution can shift . Unfortunately, it is difficult or impossible to determine how severely these assumptions are violated by real data , so practitioners usually cannot trust such bounds with confidence.

To better estimate test performance in the wild, some recent work instead tries to directly predict accuracy of neural networks using unlabeled data from the test distribution of interest . While these methods predict the test performance surprisingly well, they lack pointwise trustworthiness and verifiability: their estimates are good on average over all distribution shifts, but they provide no signal of the quality of any individual prediction (here, each point is a single test _distribution_, for which a method predicts a classifier's average accuracy). Because of the opaque conditions under which these methods work, it is also difficult to anticipate their failure cases--indeed, it is reasonably common for them to substantially overestimate test accuracy for a particular shift, which is problematic when optimistic deployment would be very costly. Worse yet, we find that this gap _grows with test error_ (Figure 1), making these predictions least reliable precisely when their reliability is most important. **Although it is clearly impossible to guarantee upper bounds on test error for all shifts,** there is still potential for error bounds that are intuitive and reasonably trustworthy.

In this work, we develop a method for (almost) provably bounding test error of classifiers under distribution shift using unlabeled test points. Our bound's only requirement is a simple, intuitive, condition which describes the ability of a hypothesis class to achieve small loss on a particular objective defined over the (unlabeled) train and test distributions. Inspired by \(\)-divergence [41; 7], our method requires training a critic to maximize agreement with the classifier of interest on the source distribution while simultaneously maximizing _disagreement_ on the target distribution; we refer to this joint objective as the _Disagreement Discrepancy_, and so we name the method Dis2. We optimize this discrepancy over linear classifiers using deep features--or linear functions thereof--finetuned on only the training set. Recent evidence suggests that such representations are sufficient for highly expressive classifiers even under large distribution shift . Experimentally, we find that our bound is valid effectively 100% of the time,1 consistently giving non-trivial lower bounds on test accuracy which are reasonably comparable to competitive baselines.

Additionally, our proof of the bound leads to a natural (post-hoc) hypothesis test of the validity of its lone assumption. This provides an unusually strong positive signal: for more than half the datasets we evaluate we _prove_ with high probability that the assumption holds; the corresponding indicator that it does not hold never occurs. We also show that it is possible to approximately test this bound's likelihood of being valid _a priori_ using only unlabeled data: the optimization process itself provides useful information about the bound's validity, and we use this to construct a score which linearly correlates with the tightness of the bound. This score can then be used to relax the original bound into a sequence of successively tighter-yet-less-conservative estimates, interpolating between robustness and accuracy and allowing a user to make estimates according to their specific risk tolerance.

While maximizing agreement is statistically well understood, our method also calls for maximizing _dis_agreement on the target distribution. This is not so straightforward in the multiclass setting, and we observe that prior works use unsuitable losses which do not correspond to minimizing the 0-1 loss of interest and are non-convex (or even concave) in the model logits [12; 50; 23]. To rectify this, we derive a new "disagreement loss" which serves as an effective proxy loss for maximizing multiclass disagreement. Experimentally, we find that minimizing this loss results in lower risk (that is, higher disagreement) compared to prior methods, and we believe it can serve as a useful drop-in replacement for any future methods which require maximizing multiclass disagreement.

Experiments across numerous vision datasets (BREEDs , FMoW-WILDs , Visda , Domainnet , CIFAR10, CIFAR100  and OfficeHome ) demonstrate the effectiveness of our bound. Though Dis2 is competitive with prior methods for error estimation, **we emphasize that our focus is _not_ on improving raw predictive accuracy**--rather, we hope to obtain reliable (i.e.,

Figure 1: **Our bound vs. three prior methods for estimation across a wide variety of distribution shift benchmarks (e.g., WILDs, BREEDs, DomainNet) and training methods (e.g., ERM, FixMatch, BN-adapt).** Prior methods are accurate on average, but it is difficult or impossible to know when a given prediction is reliable and why. Worse yet, they usually overestimate accuracy, with the gap growing as test accuracy decreases—_this is precisely when a reliable, conservative estimate is most desirable._ Instead, Dis2 maximizes the **dis**agreement **dis**repancy to give a reliable error upper bound which holds effectively 100% of the time. See Appendix F for stratification by training method.

correct), reasonably tight bounds on the test error of a given classifier under distribution shift. In particular, while existing methods tend to severely overestimate accuracy as the true accuracy drops, our bound maintains its validity while remaining non-vacuous, _even for drops in accuracy as large as 70%_. In addition to source-only training, we experiment with unsupervised domain adaptation methods that use unlabeled target data and show that our observations continue to hold.

## 2 Related Work

Estimating test error with unlabeled data.The generalization capabilities of overparameterized models on in-distribution data have been extensively studied using conventional machine learning tools [46; 47; 45; 48; 17; 5; 73; 39; 42]. This research aims to bound the generalization gap by evaluating complexity measures of the trained model. However, these bounds tend to be numerically loose compared to actual generalization error [70; 43]. Another line of work instead explores the use of unlabeled data for predicting in-distribution generalization [56; 55; 20; 44; 32]. More relevant to our work, there are several methods that predict the error of a classifier under distribution shift with unlabeled test data: (i) methods that explicitly predict the correctness of the model on individual unlabeled points [14; 15; 8]; and (ii) methods that directly estimate the overall error without making a pointwise prediction [9; 24; 12; 21; 3].

To achieve a consistent estimate of the target accuracy, several works require calibration on the target domain [32; 24]. However, these methods often yield poor estimates because deep models trained and calibrated on a source domain are not typically calibrated on previously unseen domains . Additionally, [14; 24] require a subset of labeled target domains to learn a regression function that predicts model performance--but thus requires significant a priori knowledge about the nature of shift that, in practice, might not be available before models are deployed in the wild.

Closest to our work is , where the authors use domain-invariant predictors as a proxy for unknown target labels. However, like other works, their method only _estimates_ the target accuracy--the actual error bounds they derive are not computable in practice. Second, even their estimate is computationally demanding and relies on multiple approximations, tuning of numerous hyperparameters, e.g. lagrangian multipliers; as a result, proper tuning is difficult and the method does not scale to modern deep networks. Finally, they suggest minimizing the (concave) negative cross-entropy loss, but we show that this can be a poor proxy for maximizing disagreement, instead proposing a more suitable replacement which we find performs much better.

Uniform convergence bounds.Our bound is inspired by classic analyses using \(\)- and \(\)-divergence [41; 6; 7]. These provide error bounds via a complexity measure that is both data- and hypothesis-class-dependent. This motivated a long line of work on training classifiers with small corresponding complexity, such as restricting classifiers' discriminative power between source and target data [18; 67; 38; 72]. Unfortunately, such bounds are often intractable to evaluate and are usually vacuous in real world settings. We provide a more detailed comparison to our approach in Section 3.1.

## 3 Deriving an (Almost) Provable Error Bound

Notation.Let \(,\) denote the source and target (train and test) distributions, respectively, over labeled inputs \((x,y)\), and let \(}\), \(}\) denote sets of samples from them with cardinalities \(n_{S}\) and \(n_{T}\) (they also denote the corresponding empirical distributions). Recall that we observe only the covariates \(x\) without the label \(y\) when a sample is drawn from \(\). We consider classifiers \(h:^{||}\) which output a vector of logits, and we let \(\) denote the particular classifier whose error we aim to bound. Generally, we use \(\) to denote a hypothesis class of such classifiers. Where clear from context, we use \(h(x)\) to refer to the argmax logit, i.e. the predicted class. We treat these classifiers as deterministic throughout, though our analysis can easily be extended to probabilistic classifiers and labels. For a distribution \(\) on \(\), let \(_{}(h,h^{}):=_{}[\{ _{y}h(x)_{y}_{y}h^{}(x)_{y}\}]\) denote the one-hot disagreement between classifiers \(h\) and \(h^{}\) on \(\). Let \(y^{*}\) represent the true labeling function such that \(y^{*}(x)=y\) for all samples \((x,y)\); with some abuse of notation, we write \(_{}(h)\) to mean \(_{}(h,y^{*})\), i.e. the 0-1 error of classifier \(h\) on distribution \(\).

The bound we derive in this work is extremely simple and relies on one new concept:

**Definition 3.1**.: The _disagreement discrepancy_\((h,h^{})\) is the disagreement between \(h\) and \(h^{}\) on \(\) minus their disagreement on \(\):

\[(h,h^{}):=_{}(h,h^{})-_{}(h,h^{}).\]

We leave the dependence on \(,\) implicit. Note that this term is symmetric in its arguments and signed--it can be negative. With this definition, we now have the following lemma:

**Lemma 3.2**.: _For any classifier \(h\), \(_{}(h)=_{}(h)+(h,y^{*})\)._

Proof.: By definition, \(_{}(h)=_{}(h)+(_{}( h)-_{}(h))=_{}(h)+(h,y^{*})\). 

We cannot directly use Lemma 3.2 to estimate \(_{}()\) because the second term is unknown. However, observe that \(y^{*}\) is _fixed_. That is, while a learned \(\) will depend on \(y^{*}\) and \(\)--and therefore \((,y^{*})\) may be large under large distribution shift--\(y^{*}\)**is _not chosen to maximize \((,y^{*})\) in response to the \(\) we have learned.** This means that for a sufficiently expressive hypothesis class \(\), it should be possible to identify an alternative labeling function \(h^{}\) for which \((,h^{})(,y^{*})\) (we refer to such \(h^{}\) as the _critic_). In other words, we should be able to find an \(h^{}\) which, _if it were the true labeling function_, would imply at least as large of a drop in accuracy from train to test as occurs in reality. This key observation serves as the basis for our bound, and we discuss it in greater detail in Section 3.1.

In this work we consider the class \(\) of linear critics, with the features \(\) defined as source-finetuned neural representations or the logits output by the classifier \(\). Prior work provides strong evidence that this class has surprising capacity under distribution shift, including the possibility that functions very similar to \(y^{*}\) lie in \(\). We formalize this intuition with the following assumption:

**Assumption 3.3**.: _Define \(h^{*}:=_{h^{}}(,h^{})\). We assume_

\[(,y^{*})(,h^{*}).\]

Note that this statement is necessarily true whenever \(y^{*}\); it only becomes meaningful when considering restricted \(\), as we do here. Note also that this assumption is made specifically for \(\), i.e. on a per-classifier basis. This is important because while the above may not hold for every classifier \(\), it need only hold for the classifiers whose error we would hope to bound, which is in practice a very small subset of classifiers (such as those which can be found by approximately minimizing the empirical training risk via SGD). From Lemma 3.2, we immediately have the following result:

**Proposition 3.4**.: _Under Assumption 3.3, \(_{}()_{}()+( ,h^{*})\)._

Unfortunately, identifying the optimal critic \(h^{*}\) is intractable, meaning this bound is still not estimable--we present it as an intermediate result for clarity of presentation. To derive the practical bound we report in our experiments, we need one additional step. In Section 4, we derive a "disagreement loss" which we use to approximately maximize the empirical disagreement discrepancy \((,)=_{}}(,)- _{}}(,)\). Relying on this loss, we instead make the assumption:

**Assumption 3.5**.: _Suppose we identify the critic \(h^{}\) which maximizes a concave surrogate to the empirical disagreement discrepancy. We assume \((,y^{*})(,h^{})\)._

This assumption is slightly stronger than Assumption 3.3--in particular, Assumption 3.3 implies with high probability a weaker version of Assumption 3.5 with additional terms that decrease with increasing sample size and a tighter proxy loss.2 Thus, the difference in strength between these two assumptions shrinks as the number of available samples grows and as the quality of our surrogate objective improves. Ultimately, our bound holds without these terms, implying that the stronger assumption is reasonable in practice. We can now present our main result:

**Theorem 3.6** (Main Bound).: _Under Assumption 3.5, with probability \( 1-\),_

\[_{}()_{}}()+ {}(,h^{})++4n_{T}) 1/}{2n_{S}n_{T}}}.\]Proof.: Assumption 3.5 implies \(_{}()_{}()+(,h ^{})=_{}(,y^{*})+_{}(, h^{})-_{}(,h^{})\), so the problem reduces to upper bounding these three terms. We define the random variables

\[r_{,i}=}{{n_{}}},&h^{}(x _{i})=(x_{i}) y_{i},\\ -}{{n_{}}},&h^{}(x_{i})(x_{i})=y_{i},\\ 0,&, r_{,i}=\{(x _{i}) h^{}(x_{i})\}}{n_{T}}\]

for source and target samples, respectively. By construction, the sum of all of these variables is precisely \(_{}(,y^{*})+_{}(,h^{ })-_{}(,h^{})\) (note these are the empirical terms). Further, observe that

\[[_{}}r_{,i}] =_{}[\{(x_{i}) y_{i}\} -\{(x_{i}) h^{}(x_{i})\}]=_{}( ,y^{*})-_{}(,h^{}),\] \[[_{}}r_{,i}] =_{}[\{(x_{i}) h^{ }(x_{i})\}]=_{}(,h^{}),\]

and thus their expected sum is \(_{}(,y^{*})+_{}(,h^{ })-_{}(,h^{})\), which are the population terms we hope to bound. Now we apply Hoeffding's inequality: the probability that the expectation exceeds their sum by \(t\) is no more than \((-}{n_{}(}{{n_{}}})^{ 2}+n_{}(}{{n_{}}})^{2}})\). Solving for \(t\) completes the proof. 

_Remark 3.7_.: While we state Theorem 3.6 as an implication, **Assumption 3.5** is _equivalent_ to the stated bound up to finite-sample terms**. Our empirical findings (and prior work) suggest that Assumption 3.5 is reasonable in general, but this equivalence allows us to actually prove that it holds in practice for many shifts. We elaborate on this in Appendix E.

The core message behind Theorem 3.6 is that if there is a simple (i.e., linear) critic \(h^{}\) with large disagreement discrepancy, the true \(y^{*}\) could plausibly be this function, implying \(\) could have high error--likewise, if no simple \(y^{*}\) could hypothetically result in high error, we should expect low error.

_Remark 3.8_.: Bounding error under distribution shift is fundamentally impossible without assumptions. Prior works which estimate accuracy using unlabeled data rely on experiments, suggesting that whatever condition allows their method to work holds in a variety of settings ; using these methods is equivalent to _implicitly_ assuming that it will hold for future shifts. Understanding these conditions is thus crucial for assessing in a given scenario whether they can be expected to be satisfied.3 It is therefore of great practical value that Assumption 3.5 is a simple, intuitive requirement: below we demonstrate that this simplicity allows us to identify potential failure cases _a priori_.

How Does Dis\({}^{2}\) Improve over \(\)- and \(\)-Divergence?

To verifiably bound a classifier's error under distribution shift, one must develop a meaningful notion of distance between distributions. One early attempt at this was _\(\)-divergence_ which measures the ability of a binary hypothesis class to discriminate between \(\) and \(\) in feature space. This was later refined to _\(\)-divergence_, which is equal to \(\)-divergence where the discriminator class comprises all exclusive-ors between pairs of functions from the original class \(\). Though these measures can in principle provide non-vacuous bounds, they usually do not, and evaluating them is intractable because it requires maximizing an objective over all _pairs_ of hypotheses. Furthermore, these bounds are overly conservative even for simple function classes and distribution shifts because they rely on uniform convergence. In practice, _we do not care_ about bounding the error of all classifiers in \(\)--we only care to bound the error of \(\). This is a clear advantage of Dis\({}^{2}\) over \(\).

The true labeling function is never worst-case.4More importantly, we observe that one should not expect the distribution shift to be _truly_ worst case, because the test distribution \(\) and ground truth \(y^{*}\) are not chosen adversarially with respect to \(\). Figure 2 gives a simple demonstration of this point. Consider the task of learning a linear classifier to discriminate between squares and circles on the source distribution \(\) (blue) and then bounding the error of this classifier on the target distribution \(\) (red), whose true labels are unknown and are therefore depicted as triangles. Figure 2(a) demonstrates that both \(\)- and \(\)-divergence achieve their maximal value of 1, because both \(h_{1}\) and \(h_{2} h_{3}\) perfectly discriminate between \(\) and \(\). Thus both bounds would be vacuous.

Now, suppose we were to learn the max-margin \(\) on the source distribution (Figure 2(b)). It is _possible_ that the true labels are given by the worst-case boundary as depicted by \(y^{*}_{1}\) (pink), thus "flipping" the labels and causing \(\) to have 0 accuracy on \(\). In this setting, a vacuous bound is correct. However, this seems rather unlikely to occur in practice--instead, recent experimental evidence [61; 34; 33] suggests that the true \(y^{*}\) will be much simpler. The maximum disagreement discrepancy here would be approximately \(0.5\), giving a test accuracy lower bound of \(0.5\)--this is consistent with plausible alternative labeling functions such as \(y^{*}_{2}\) (orange). Even if \(y^{*}\) is not linear, we still expect that _some_ linear function will induce larger discrepancy; this is precisely Assumption 3.3. Now suppose instead we learn \(\) as depicted in Figure 2(c). Then a simple ground truth such as \(y^{*}_{3}\) (green) is plausible, which would mean \(\) has 0 accuracy on \(\). In this case, \(y^{*}_{3}\) is also a critic with disagreement discrepancy equal to 1, and so Dis\({}^{2}\) would correctly output an error upper bound of \(1\).

A setting where Dis\({}^{2}\) may be invalid.There is one setting where it should be clear that Assumption 3.5 is less likely to be satisfied: when the representation we are using is explicitly regularized to keep \(_{h^{}}(,h^{})\) small. This occurs for domain-adversarial representation learning methods such as DANN  and CDAN , which penalize the ability to discriminate between \(\) and \(\) in feature space. Given a critic \(h^{}\) with large disagreement discrepancy, the discriminator \(D(x)=\{_{y}(x)_{y}=_{y}h^{}(x)_{y}\}\) will achieve high accuracy on this task (precisely, \(,h^{})}{2}\)). By contrapositive, enforcing low discriminatory power means that the max discrepancy must also be small. It follows that for these methods Dis\({}^{2}\) should not be expected to hold universally, and in practice we see that this is the case (Figure 3). Nevertheless, when Dis\({}^{2}\) does overestimate accuracy, it does so by significantly less than prior methods.

## 4 Efficiently Maximizing the Disagreement Discrepancy

For a classifier \(\), Theorem 3.6 clearly prescribes how to bound its test error: first, train a critic \(h^{}\) on the chosen \(\) to approximately maximize \((,h^{})\), then evaluate \(_{}()\) and \((,h^{})\) using a holdout set. The remaining difficulty is in identifying the maximizing \(h^{}\)--that is, the one which minimizes \(_{}(,h^{})\) and maximizes \(_{}(,h^{})\). We can approximately minimize \(_{}(,h^{})\) by minimizing the sample average of the convex surrogate \(_{}:=-|}(h(x))_{y}\) as justified by statistical learning theory. However, it is less clear how to maximize \(_{}(,h^{})\).

Figure 2: **The advantage of Dis\({}^{2}\) over bounds based on \(\)- and \(\)-divergence.** Consider the task of classifying circles and squares (triangles are unlabeled). **(a):** Because \(h_{1}\) and \(h_{2} h_{3}\) perfectly discriminate between \(\) (blue) and \(\) (red), \(\)- and \(\)-divergence bounds are always vacuous. In contrast, Dis\({}^{2}\) is only vacuous when 0% accuracy is induced by a reasonably likely ground truth (such as \(y^{*}_{3}\) in **(c)**, but not \(y^{*}_{1}\) in **(b)**), and can often give non-vacuous bounds (such as \(y^{*}_{2}\) in **(b)**).

A few prior works suggest proxy losses for multiclass disagreement . We observe that these losses are not theoretically justified, as they do not upper bound the 0-1 disagreement loss or otherwise do not meaningfully enforce that higher agreement causes higher loss. Furthermore, they are non-convex (or even concave) in the model logits, hindering optimization. Indeed, it is easy to identify simple settings in which minimizing these losses will result in a degenerate classifier with arbitrarily small loss but high agreement. Instead, we derive a new loss which satisfies the above desiderata and thus serves as a more principled approach to maximizing disagreement.

**Definition 4.1**.: The _disagreement logistic loss_ of a classifier \(h\) on a labeled sample \((x,y)\) is defined as

\[_{}(h,x,y):=(1+(h(x)_{y}- |-1}_{ y}h(x)_{})).\]

**Fact 4.2**.: The disagreement logistic loss is convex in \(h(x)\) and upper bounds the 0-1 disagreement loss (i.e., \(\{_{}h(x)_{}=y\}\)). For binary classification, the disagreement logistic loss is equivalent to the logistic loss with the label flipped.

We expect that \(_{}\) can serve as a useful drop-in replacement for any future algorithm which requires maximizing disagreement in a principled manner. We combine \(_{}\) and \(_{}\) to arrive at the empirical disagreement discrepancy objective:

\[}_{}(h^{}):=}|}_{x }}_{}(h^{},x,(x))+ {|}|}_{x}}_{}(h^{},x,(x)).\]

By construction, \(1-}_{}(h^{})\) is concave and bounds \((,h^{})\) from below. However, note that the representations are already optimized for accuracy on \(\), which suggests that predictions will have low entropy and that the \(}{{||}}\) scaling is unnecessary for balancing the two terms. We therefore drop the constant scaling factors; this often leads to higher discrepancy. In practice we optimize this objective with multiple initializations and hyperparameters and select the solution with the largest empirical discrepancy on a holdout set to ensure a conservative bound. Experimentally, we find that replacing \(_{}\) with any of the surrogate losses from  results in smaller discrepancy; we present these results in Appendix B.

Tightening the bound by optimizing over the logits.Looking at Theorem 3.6, it is clear that the value of the bound will decrease as the capacity of the hypothesis class is restricted. Since the number of features is large, one may expect that Assumption 3.5 holds even for a reduced feature set. In particular, it is well documented that deep networks optimized with stochastic gradient descent learn representations with small effective rank, often not much more than the number of classes . This suggests that the logits themselves should contain most of the features' information about \(\) and \(\) and that using the full feature space is unnecessarily conservative. To test this, we evaluate Dis\({}^{2}\) on the full features, the logits output by \(\), and various fractions of the top

Figure 3: **Dis\({}^{2}\) may be invalid when the features are regularized to violate Assumption 3.5.** Domain-adversarial representation learning algorithms such as DANN  and CDAN  indirectly minimize \(_{h^{}}(,h^{})\), meaning the necessary condition is less likely to be satisfied. Nevertheless, when Dis\({}^{2}\) does overestimate accuracy, it almost always does so by less than prior methods.

principal components (PCs) of the features. We observe that using logits indeed results in tighter error bounds _while still remaining valid_--in contrast, using fewer top PCs also results in smaller error bounds, but at some point they become invalid (Figure C.2). The bounds we report in this work are thus evaluated on the logits of \(\), except where we provide explicit comparisons in Section 5.

Identifying the ideal number of PCs via a "validity score".Even though reducing the feature dimensionality eventually results in an invalid bound, it is tempting to consider how we may identify approximately when this occurs, which could give a more accurate (though less conservative) prediction. We find that _the optimization trajectory itself_ provides meaningful signal about this change. Specifically, Figure C.3 shows that for feature sets which are not overly restrictive, the critic very rapidly ascends to the maximum source agreement, then slowly begins overfitting. For much more restrictive feature sets (i.e., fewer PCs), the critic optimizes much more slowly, suggesting that we have reached the point where we are artificially restricting \(\) and therefore underestimating the disagreement discrepancy. We design a "validity score" which captures this phenomenon, and we observe that it is roughly linearly correlated with the tightness of the eventual bound (Figure C.4). Though the score is by no means perfect, we can evaluate \(^{2}\) with successively fewer PCs and only retain those above a certain score threshold, reducing the average prediction error while remaining reasonably conservative (Figure C.5). For further details, see Appendix C.

## 5 Experiments

Datasets.We conduct experiments across 11 vision benchmark datasets for distribution shift on datasets that span applications in object classification, satellite imagery, and medicine. We use four BREEDs datasets:  Entity13, Entity30, Nonliving26, and Living17; FMoW  and Camelyon  from WILDS ; Officehome ; Visda [52; 51]; CIFAR10, CIFAR100 ; and Domainet . Each of these datasets consists of multiple domains with different types of natural and synthetic shifts. We consider subpopulation shift and natural shifts induced due to differences in the data collection process of ImageNet, i.e., ImageNetv2  and a combination of both. For CIFAR10 and CIFAR100 we evaluate natural shifts due to variations in replication studies  and common corruptions . For all datasets, we use the same source and target domains commonly used in previous studies [22; 64]. We provide precise details about the distribution shifts considered in Appendix A. Because distribution shifts vary widely in scope, prior evaluations which focus on only one specific type of shift (e.g., corruptions) often do not convey the full story. **We therefore emphasize the need for more comprehensive evaluations across many different types of shifts and training methods**, as we present here.

    &  &  &  \\  & **DA?** & **✗** & **✓** & **✗** & **✓** & **✗** & **✓** \\ Prediction Method & & & & & & & \\  AC  & 0.1000 \(.032\) & 0.0333 \(.023\) & 0.1194 \(.012\) & 0.1123 \(.012\) & 0.1091 \(.011\) & 0.1091 \(.012\) \\ DoC  & 0.1667 \(.040\) & 0.0167 \(.0167\) & 0.1237 \(.012\) & 0.1096 \(.012\) & 0.1055 \(.011\) & 0.1083 \(.012\) \\ ATC NE  & 0.2889 \(.048\) & 0.1333 \(.044\) & 0.0824 \(.009\) & 0.0969 \(.012\) & 0.0665 \(.007\) & 0.0854 \(.011\) \\ COT  & 0.2554 \(.0467\) & 0.1667 \(.049\) & 0.0860 \(.009\) & 0.0948 \(.011\) & 0.0700 \(.007\) & 0.0808 \(.010\) \\   \(^{2}\) (Features) & 1.0000 & 1.0000 & 0.0000 & 0.0000 & 0.2807 \(.009\) & 0.1918 \(.008\) \\ \(^{2}\) (Logits) & 0.9889 \(.011\) & 0.7500 \(.058\) & 0.0011 \(.000\) & 0.0475 \(.007\) & 0.1489 \(.011\) & 0.0945 \(.010\) \\ \(^{2}\) (Logits w/o \(\)) & 0.7556 \(.0475\) & 0.4333 \(.065\) & 0.0771 \(.013\) & 0.0892 \(.011\) & 0.0887 \(.009\) & 0.0637 \(.008\) \\   

Table 1: **Comparing the \(^{2}\) bound to prior methods for predicting accuracy.** DA denotes if the representations were learned via a domain-adversarial algorithm. We report what fraction of predictions correctly bound the true error (Coverage) and the average prediction error among shifts whose accuracy is overestimated (Overest.), along with overall MAE. \(^{2}\) has substantially higher coverage and lower overestimation error, though lower overall MAE. By dropping the concentration term in Theorem 3.6 we can get even better MAE—even beating the baselines on domain-adversarial representations—at some cost to coverage.

Experimental setup and protocols.Along with source-only training with ERM, we experiment with Unsupervised Domain Adaptation (UDA) methods that aim to improve target performance with unlabeled target data (FixMatch , DANN , CDAN , and BN-adapt ). We experiment with Densenet121  and Resnet18/Resnet50  pretrained on ImageNet. For source-only ERM, as with other methods, we default to using strong augmentations: random horizontal flips, random crops, as well as Cutout  and RandAugment . Unless otherwise specified, we default to full finetuning for source-only ERM and UDA methods. We use source hold-out performance to pick the best hyperparameters for the UDA methods, since we lack labeled validation data from the target distribution. For all of these methods, we fix the algorithm-specific hyperparameters to their original recommendations following the experimental protocol in . For more details, see Appendix A.

Methods evaluated.We compare Dis\({}^{2}\) to four competitive baselines: _Average Confidence_ (AC; ), _Difference of Confidences_ (DoC; ), _Average Thresholded Confidence_ (ATC; ), and _Confidence Optimal Transport_ (COT; ). We give detailed descriptions of these methods in Appendix A. For all methods, we implement post-hoc calibration on validation source data with temperature scaling , which has been shown to improve performance. For Dis\({}^{2}\), we report bounds evaluated both on the full features and on the logits of \(\) as described in Section 4. Unless specified otherwise, we set \(=.01\) everywhere. We also experiment with dropping the lower order concentration term in Theorem 3.6, using only the sample average. Though this is of course no longer a conservative bound, we find it is an excellent predictor of test error and is worth including.

Metrics for evaluation.As our emphasis is on giving valid error bounds, we report the _coverage_, i.e. the fraction of predictions for which the true error does not exceed the predicted error. We also report the standard prediction metric, _mean absolute error_ (MAE). Finally, we measure the _conditional average overestimation_: this is the MAE among predictions which overestimate the accuracy. This metric captures the idea that the most important thing is giving a valid bound--but if for some reason it is not, we'd at least like it to be as accurate as possible.

Results.Reported metrics for all methods can be found in Table 1. We aggregate results over all datasets, shifts, and training methods--we stratify only by whether the training method is domain-adversarial, as this affects the validity of Assumption 3.5. We find that Dis\({}^{2}\) achieves competitive MAE while maintaining substantially higher coverage, even for domain-adversarial features. When it does overestimate accuracy, it does so by much less, implying that it is ideal for conservative estimation even when any given error bound is not technically satisfied. Dropping the concentration term performs even better (sometimes beating the baselines), at the cost of some coverage. This suggests that efforts to better estimate the true maximum discrepancy may yield even better predictors. We also show scatter plots to visualize performance on individual distribution shifts, plotting each source-target pair as a single point. For these too we report separately the results for domain-adversarial (Figure 3) and non-domain-adversarial methods (Figure 1). To avoid clutter, these two plots do not include DoC, as it performed comparably to AC. Figure 4(a) displays additional scatter plots which allow for a direct comparison of the variants of Dis\({}^{2}\). Finally, Figure 4(b) plots the observed violation rate (i.e. \(1-\)coverage) of Dis\({}^{2}\) on non-domain-adversarial methods for varying \(\). We observe that it lies at or below the line \(y=x\), meaning the probabilistic bound provided by Theorem 3.6 holds across a range of failure probabilities. Thus we see that our probabilistic bound is empirically valid all of the time--not in the sense that each individual shift's error is upper bounded, but rather that the desired violation rate is always satisfied.

Strengthening the baselines to improve coverage.Since the baselines we consider in this work prioritize predictive accuracy over conservative estimates, their coverage can possibly be improved without too much increase in error. We explore this option using LOOCV: for a desired coverage, we learn a parameter to either scale or shift a method's prediction to achieve that level of coverage on all but one of the datasets. We then evaluate the method on all shifts of the remaining dataset, and we repeat this for each dataset. Appendix D reports the results for varying coverage levels. We find that (i) the baselines do not achieve the desired coverage on the held out data, though they get somewhat close; and (ii) the adjustment causes them to suffer higher MAE than Dis\({}^{2}\). Thus Dis\({}^{2}\) is on the Pareto frontier of MAE and coverage, and is preferable when conservative bounds are desirable. We believe identifying alternative methods of post-hoc prediction adjustment is a promising future direction.

## 6 Conclusion

The ability to evaluate _trustworthy_, non-vacuous error bounds for deep neural networks under distribution shift remains an extremely important open problem. Due to the wide variety of real-world shifts and the complexity of modern data, restrictive a priori assumptions on the distribution (i.e., before observing any data from the shift of interest) seem unlikely to be fruitful. On the other hand, prior methods which estimate accuracy using extra information--such as unlabeled test samples--often rely on opaque conditions whose likelihood of being satisfied is difficult to predict, and so they sometimes provide large underestimates of test error with no warning signs.

This work bridges this gap with a simple, intuitive condition and a new disagreement loss which together result in competitive error _prediction_, while simultaneously providing an (almost) guaranteed probabilistic error _bound_. We also study how the process of evaluating the bound (e.g., the optimization landscape) can provide even more useful signal, enabling better predictive accuracy. We expect there is potential to push further in each of these directions, hopefully extending the current accuracy-reliability Pareto frontier for test error bounds under distribution shift.