# Newton Informed Neural Operator for Solving Nonlinear Partial Differential Equations

Wenrui Hao

Department of Mathematics

The Pennsylvania State University, University Park,

State College, PA, USA

wxh64@psu.edu

&Xinliang Liu

School of Mathematical Sciences,

Shenzhen University, Shenzhen, China

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia

xinliang.liu@kaust.edu.sa

&Yahong Yang

Department of Mathematics

The Pennsylvania State University, University Park,

State College, PA, USA

yxy5498@psu.edu

Corresponding Authors

###### Abstract

Solving nonlinear partial differential equations (PDEs) with multiple solutions is essential in various fields, including physics, biology, and engineering. However, traditional numerical methods, such as finite element and finite difference methods, often face challenges when dealing with nonlinear solvers, particularly in the presence of multiple solutions. These methods can become computationally expensive, especially when relying on solvers like Newton's method, which may struggle with ill-posedness near bifurcation points. In this paper, we propose a novel approach, the Newton Informed Neural Operator, which learns the Newton solver for nonlinear PDEs. Our method integrates traditional numerical techniques with the Newton nonlinear solver, efficiently learning the nonlinear mapping at each iteration. This approach allows us to compute multiple solutions in a single learning process while requiring fewer supervised data points than existing neural network methods.

## 1 Introduction

Neural networks have been extensively applied to solve partial differential equations (PDEs) in various fields, including biology, physics, and materials science . While much of the existing work focuses on PDEs with a unique solution, nonlinear PDEs with multiple solutions pose a significant challenge  but are widely encountered in applications such as . In this paper, we aim to solve the following nonlinear PDEs with multiple solutions:

\[(u)()=f(u()),&,\\ u()=0,&,\] (1)Here, \(\) is the domain of equation, \(f(u)\) is a nonlinear function in \(\), \(u:^{d}\) and \(\) is a second-order elliptic operator given by \(u=-_{i,j=1}^{d}a^{ij}()u_{x_{i}x_{j}}+_{i=1}^{d}b^{i}( )u_{x_{i}}+c()u\), for given coefficient functions \(a^{ij},b^{i},c(i,j=1,,d)\) with \(_{i,j=1}^{n}a^{ij}()_{i}_{j}||^{2}, 0\).

Various neural network methods have been developed to tackle partial differential equations (PDEs), including PINN , the Deep Ritz method , DeepONet , FNO , MgO , and OL-DeepONet . These methods can be broadly categorized into two types: function learning and operator learning approaches. In function learning, the goal is to directly learn the solution. However, these methods often encounter the limitation of only being able to learn one solution in each learning process. Furthermore, the problem becomes ill-posed when there are multiple solutions. On the other hand, operator learning aims to approximate the map between parameter functions in PDEs and the unique solution. This approach cannot address the issue of multiple solutions or find them in a single training session. We will discuss this in more detail in the next section.

In this paper, we present a novel neural network approach for solving nonlinear PDEs with multiple solutions. Our method is grounded in operator learning, allowing us to capture multiple solutions within a single training process, thus overcoming the limitations of function learning methods in neural networks. Moreover, we enhance our network architecture by incorporating traditional Newton methods , as discussed in the next section. This integration ensures that the problem of operator learning becomes well-defined, as Newton's methods provide well-defined locally, thereby ensuring a robust operator. This approach addresses the challenges associated with directly applying operator networks to such problems. Additionally, we leverage Newton information during training, enabling our method to perform effectively even with limited supervised data points. We introduce our network as the **Newton Informed Neural Operator**. To clarify, we do not design a specific neural structure for the neural operator. The Newton information is not incorporated into the architecture of the neural network but rather into the training process. Specifically, the Newton method is incorporated into the loss function, as detailed in Section 3.3.

As mentioned earlier, our approach combines the classical Newton method, which translates nonlinear PDEs into an iterative process involving solving linear functions at each iteration. One key advantage of our method is that, once the operator is effectively learned, there is no need to solve the linear equation at every iteration. This significantly reduces computation time, especially in complex systems encountered in fields such as material science, biology, and chemistry. Furthermore, once the Newton Informed Neural Operator is well-trained, it can be applied to compute new solutions with appropriate initial guesses, even those not present in the training data. Details of this capability are demonstrated in the numerical example of the Gray-Scott model. Overall, the Newton Informed Neural Operator efficiently solves nonlinear PDEs with multiple solutions by learning the Newton nonlinear solver. It addresses the time-consuming nature of traditional nonlinear solvers and requires fewer supervised data points compared to existing neural network methods. Additionally, it saves time by eliminating the need for repeatedly solving nonlinear systems, as is required in traditional Newton methods. Once the neural operator is learned, it can also compute new solutions beyond those provided in the supervised data.

The following paper is organized as follows: In the next section (Section 2), we will delve into nonlinear PDEs with multiple solutions and discuss related works on solving PDEs using neural network methods. In Section 3, we will review the classical Newton method for solving PDEs and introduce the Newton Informed Neural Operator, which combines neural operators with the Newton method to address nonlinear PDEs with multiple solutions. In this section, we will also analyze the approximation and generalization errors of the Newton Informed Neural Operator. Finally, in Section 4, we present the numerical results of our neural networks for solving nonlinear PDEs. The first example demonstrates that the Newton Informed Neural Operator requires minimal data for training, the second example shows that the speed of the Newton Informed Neural Operator is significantly faster than the traditional Newton method, and the last example highlights that the Newton Informed Neural Operator can discover new solutions not present in the supervised data.

Backgrounds and Relative Works

### Nonlinear PDEs with multiple solutions

Significant mathematical models depicting natural phenomena in biology, physics, and materials science are rooted in nonlinear partial differential equations (PDEs) . These models, characterized by their inherent nonlinearity, present complex multi-solution challenges. Illustrative examples include string theory in physics, reaction-diffusion systems in chemistry, and pattern formation in biology [20; 12]. However, experimental techniques like synchrotronic and laser methods can only observe a subset of these multiple solutions. Thus, there is an urgent need to develop computational methods to unravel these nonlinear models, offering deeper insights into the underlying physics and biology . Consequently, efficient numerical techniques for identifying these solutions are pivotal in understanding these intricate systems. Despite recent advancements in numerical methods for solving nonlinear PDEs, significant computational challenges persist for large-scale systems. Specifically, the computational costs of employing Newton and Newton-like approaches are often prohibitive for the large-scale systems encountered in real-world applications. In response to these challenges [15; 19], we propose an operator learning approach based on Newton's method to efficiently solve nonlinear PDEs.

### Related works

Indeed, there are numerous approaches to solving partial differential equations (PDEs) using neural networks. Broadly speaking, these methods can be categorized into two main types: function learning and operator learning.

In function learning, neural networks are used to directly approximate the solutions to PDEs. Function learning approaches aim to directly learn the solution function itself. On the other hand, in operator learning, the focus is on learning the operator that maps input parameters to the solution of the PDE. Instead of directly approximating the solution function, the neural network learns the underlying operator that governs the behavior of the system.

Function learning methodsIn function learning, a commonly employed method for addressing this problem involves the use of Physics-Informed Neural Network (PINN)-based learning approaches, as introduced by Raissi et al. , and Deep Ritz Methods . However, in these methods, the task becomes particularly challenging due to the ill-posed nature of the problem arising from multiple solutions. Despite employing various initial data and training methods, attaining high accuracy in solution learning remains a complex endeavor. Even when a high-accuracy solution is achieved, each learning process typically results in the discovery of only one solution. The specific solution learned by the neural network is heavily influenced by the initial conditions and training methods employed. However, discerning the relationships between these factors and the learned solution remains a daunting task. In , the authors introduce HomPINNs for learning multiple solutions to PDEs, where the number of solutions that can be learned depends on the choice of "start functions." However, if the "start functions" are not appropriately selected, HomPINNs may fail to capture all solutions. In this paper, we present an operator learning approach combined with Newton's method to train the nonlinear solver. While this approach is not specifically designed for computing multiple solutions, it can be employed to compute them if suitable initial guesses are provided.

Operator learning methodsVarious approaches have been developed for operator learning to solve PDEs, including DeepONet , which integrates physical information [7; 26], as well as techniques like FNO  inspired by spectral methods, and MgNO , HANO , and WNO  based on multilevel methods, and transformer-based neural operators [4; 8]. These methods focus on approximating the operator between the parameters and the solutions. Firstly, they require the solutions of PDEs to be unique; otherwise, the operator is not well-defined. Secondly, they focus on the relationship between the parameter functions and the solution, rather than the initial data and multiple solutions.

## 3 Newton Informed Neural Operator

### Review of Newton Methods to Solve Nonlinear Partial Differential Equations

To tackle this problem Eq. (1), we employ Newton's method by linearizing the equation.

For the Newton method applied to an operator, if we aim to find the solution of \((u)=0\), the iteration can be written as:

\[^{}(u_{n})u_{n+1}=^{}(u_{n} )u_{n}-(u_{n})^{ }(u_{n}) u=-(u_{n}),\]

where \( u=u_{n+1}-u_{n}\).

In this context, \(^{}(u)v\) is the (Frechet) derivative of the operator, which is a linear operator to \(v\), defined as follows: To find \(^{}(u)\) in \(\), for any \(v\),

\[_{|v| 0}(u+v)-(u)-^{}(u)v| }{|v|}=0,\]

where \(||\) denotes the norm in \(\).

For solving Eq. (1), given any initial guess \(u_{0}()\), for \(i=1,2,,M\), in the \(i\)-th iteration of Newton's method, we have \(()=u+ u()\) by solving

\[(-f^{}(u)) u=-u+f(u),& \\  u=0,&,\] (2)

which is based on the fact that the (Frechet) derivative of \(-f()\) at \(u\) is \(-f^{}(u)\). If Eq. (2) has a unique solution, then by solving Eq. (2) and repeating the process \(M\) times, we will obtain one of the solutions of the nonlinear equation (1). Denoting the mapping for \(u\) and \( u\), the solution of Eq. (2) with parameter \(u\), as \((u):= u\), we know that

\[_{n}(+)^{(n)}(u_{0})=u^{*},\]

where \(u^{*}\) is one of the solutions of Eq. (1). For different initial conditions, this process will converge to different solutions of Eq. (1), making this method suitable for finding multiple solutions. Furthermore, the Newton method is well-posed, meaning that each initial condition \(u_{0}\) will converge to a single solution of Eq. (1) under appropriate assumptions (see Assumption 1). This approach helps to address the ill-posedness encountered when using PINNs directly to solve Eq. (1). However, repeatedly solving Eq. (1) can be computationally expensive, especially in high-dimensional cases or when a large number of discrete points are involved. In this paper, we tackle these challenges by employing neural networks.

### Neural Operator Structures

In this section, we introduce the structure of the neural operator to approximate the operator locally in the Newton methods from Eq.(2), i.e., \( u:=(u)\), where \( u\) is the solution of Eq.(2), which depends on \(u\). If we can learn the operator \((u)\) well using the neural operator \((u;)\), then for an initial function \(u_{0}\), assume the \(n\)-th iteration will approximate one solution, i.e., \((+)^{(n)}(u_{0}) u^{*}\). Thus,

\[(+)^{(n)}(u_{0})(+)^{(n)}(u_{ 0}) u^{*}.\]

For another initial condition, we can evaluate our neural operator and find the solution directly.

Then we discuss how to train such an operator. To begin, we define the following shallow neural operators with \(p\) neurons for operators from \(\) to \(\) as

\[(a;)=_{i=1}^{p}_{i}(_{i}a+_{i}) a\] (3)

where \(_{i}(,),_{i} \), \(_{i}(,)\), and \(\) denote all the parameters in \(\{_{i},_{i},_{i}\}_{i=1}^{p}\). Here, \((,)\) denotes the set of all bounded (continuous) linear operators between \(\) and \(\), and \(:\) defines the nonlinear point-wise activation.

In this paper, we will use shallow DeepONet  to approximate the Newton operator. To provide a more precise description, in the shallow neural network, \(_{i}\) represents an interpolation of operators. With proper and reasonable assumptions, we can present the following theorem to ensure that DeepONet can effectively approximate the Newton method operator. The proof will be provided in the appendix. Furthermore, MgNO is replaced by \(\) as a multigrid operator , and FNO is some kind of kernel operator; our analysis can be generalized to such cases.

Before the proof, we need to establish some assumptions regarding the input space \( H^{2}()\) of the operator and \(f(u)\) in Eq. (1). The definition of the Sobolev space can be found in Appendix B.1.

**Assumption 1**.: _(i): For any \(u\), we have that the linear equation_

\[(-f^{}(u)) u=-u+f(u)\]

_is well-posed for solving \( u\)._

_(ii): There exists a constant_ \(F\) _such that_ \(\|f(x)\|_{W^{2,}()} F\)_._

_(iii): All coefficients in_ \(\) _are_ \(C^{1}\) _and_ \( C^{2}\)_._

_(iv):_ \(\) _has a Schauder basis_ \(\{b_{k}\}_{k=1}^{}\)_, we define the canonical projection operator_ \(_{n}\) _based on this basis. The operator_ \(_{n}\) _projects any element_ \(u\) _onto the finite-dimensional subspace spanned by the first_ \(n\) _basis elements_ \(\{b_{1},b_{2},,b_{n}\}\)_. Specifically, for_ \(u\)_,_ \(u=_{k=0}^{}_{k}b_{k}\)_, let_ \(_{n}(u)=_{k=0}^{n}_{k}b_{k},\) _where_ \(_{k}\) _are the coefficients in the expansion of_ \(u\) _with respect to the basis_ \(\{b_{n}\}\)_. The canonical projection operator_ \(_{n}\) _is a linear bounded operator on_ \(\)_. According to the properties of Schauder bases, these projections_ \(_{n}\) _are uniformly bounded by some constant_ \(C\)_. Furthermore, we assume, for any_ \(u\)_,_ \(>0\)_, there exists a_ \(n\) _such that_

\[\|u-_{n}u\|_{H^{2}()},u.\]

More discussion about the assumption is shown in the appendix. The sketch of the proof is illustrated in Fig. 1.

**Theorem 1**.: _Suppose \(= H^{2}()\) and Assumption 1 holds. Then, there exists a neural network \((u;)_{p}\) defined as_

\[_{p}:=\{_{i=1}^{p}_{i}(_{i}u+ _{i})(_{i}+ _{i})|_{i}(,^{m}), _{i}^{m},_{i}^{1 m }\}\] (4)

_such that_

\[_{u}\|_{i=1}^{p}_{i}( _{i}u+_{i})(_{i} +_{i})-(u)\|_{L^{2}()} C _{1}m^{-}+C_{2}(+p^{-}),\] (5)

_where \(\) is a smooth non-polynomial activation function, \(n\) is shown in Assumption 1 and contained in \(_{i}\), \(C_{1}\) is a constant independent of \(m\), \(\), and \(p\), \(C_{2}\) is a constant depended on \(p\), \(n\) and \(F\) (see in Assumption 1) is the scale of the \(\) in Assumption 1. And \(\) depends on \(n\). \(n\) and \(\) are defined in Assumption 1._

The approximation results of DeepONet in Sobolev training can be found in .

Figure 1: The sketch of proof for Theorem 1.

### Loss Functions of Newton Informed Neural Operator

#### 3.3.1 Mean Square Loss

The Mean Square Error loss function is defined as:

\[_{S}():= M_{x}}_{j=1}^{M_{u}}_{k =1}^{M_{x}}|(u_{j})(_{k})-(u_{j};)(_{k})|^{2}\] (6)

where \(u_{1},u_{2},,u_{M_{u}}\) are independently and identically distributed (i.i.d) samples in \(\), and \(_{1},_{2},,_{M_{x}}\) are uniformly i.i.d samples in \(\).

However, using only the Mean Squared Error loss function is not sufficient for training to learn the Newton method, especially since in most cases, we do not have enough data \(\{u_{j},(u_{j})\}_{j=1}^{M_{u}}\). Furthermore, there are situations where we do not know how many solutions exist for the nonlinear equation (1). If the data is sparse around one of the solutions, it becomes impossible to effectively learn the Newton method around that solution.

Given that \(_{S}()\) can be viewed as the finite data formula of \(_{Sc}()\), where

\[_{Sc}()=_{M_{u},M_{x}}_{S}( {}).\]

The smallness of \(_{Sc}\) can be inferred from Theorem 1. To understand the gap between \(_{Sc}()\) and \(_{S}()\), we can rely on the following theorem. Before the proof, we need some assumptions about the data in \(_{S}()\):

**Assumption 2**.: _(i) Boundedness: For any neural network with bounded parameters, characterized by a bound \(B\) and dimension \(d_{}\), there exists a function \(:L^{2}()[0,)\) such that_

\[|(u)()|(u),_{ [-B,B]^{d_{}}}|(u;)()| (u),_{[-B,B]^{d_{}}}| (u;)()|(u)\]

_for all \(u,\), and there exist constants \(C,>0\), such that_

\[(u) C(1+\|u\|_{H^{2}})^{}.\] (7)

_(ii) Lipschitz continuity: There exists a function \(:L^{2}()[0,)\), such that_

\[|(u;)()-(u;^{})( )|(u)\|-^{}\| _{^{}}\] (8)

_for all \(u,\), and \((u) C(1+\|u\|_{H^{2}()})^{},\) for the same constants \(C,>0\) as in Eq. (7)._

_(iii) Finite measure: There exists \(>0\), such that_

\[_{H^{2}()}e^{\|u\|_{H^{2}()}^{2}}(u)<.\]

**Theorem 2**.: _If Assumption 2 holds, then the generalization error is bounded by_

\[_{[-B,B]^{d_{}}}|(_{S}( )-_{Sc}())| C[}}(1+Cd_{}(CB})^{2+1/2}) +}}}{}}],\]

_where \(C\) is a constant independent of \(B\), \(d_{}\), \(M_{x}\), and \(M_{u}\). The parameter \(\) is specified in (7). Here, \(B\) represents the bound of parameters, and \(d_{}\) is the number of parameters._

The proof of Theorem 2 is presented in Appendix B.3.

**Remark 1**.: _Assumption 2 is easily satisfied if we consider \(\) as the local function set around the solution, which is typically the case in Newton's methods. This aligns with our approach and the working region in the approximation part (see Remark 3). The error \(_{[-B,B]^{d_{}}}|(_{S}( )-_{Sc}())|\) suggests that the network can perform well based on the loss function \(_{S}()\). The reasoning is as follows: let \(_{S}=_{[-B,B]^{d_{}}}_ {S}()\) and \(_{S_{c}}=_{[-B,B]^{d_{}}}_{Sc}()\). We aim for \(_{Sc}(_{S})\) to be small, which can be written as:_

\[_{Sc}(_{S})_{Sc}(_{S_{c }})+(_{S}(_{S})-_{Sc}(_{S }))_{Sc}(_{S_{c}})+_{[-B,B]^{d_{ }}}|(_{S}()-_{Sc}( ))|,\]

_where \(_{Sc}(_{S_{c}})\) is small, as demonstrated by Theorem 1 when \(B\) is sufficiently large._

#### 3.3.2 Newton Loss

As we have mentioned, relying solely on the MSE loss function can require a significant amount of data to achieve the task. However, obtaining enough data can be challenging, especially when the equation is complex and the dimension of the input space is large. Hence, we need to consider another loss function to aid learning, which is the physical information loss function [33; 7; 19; 24], referred to here as the Network loss function.

The Newton loss function is defined as:

\[_{N}():= N_{x}}_{j=1}^{N_{u}} _{k=1}^{N_{x}}|(-f^{}(u_{j}))(u_{j}; )(_{k})+(u_{j}-f(u_{j}))(_{k})|^ {2}\] (9)

where \(u_{1},u_{2},,u_{N_{u}}\) are independently and identically distributed (i.i.d) samples in \(\), and \(_{1},_{2},,_{N_{x}}\) are uniformly i.i.d samples in \(\).

Given that \(_{N}()\) can be viewed as the finite data formula of \(_{Nc}()\), where

\[_{Nc}()=_{N_{u},N_{x}}_ {S}().\]

To understand the gap between \(_{Nc}()\) and \(_{N}()\), we can rely on the following theorem:

**Corollary 1**.: _If Assumption 2 holds, then the generalization error is bounded by_

\[_{[-B,B]^{d_{}}}|(_{N} ()-_{Nc}())| C[}}(1+Cd_{}(CB})^{2+1/2}) +}}}{}}],\]

_where \(C\) is a constant independent of \(B\), \(d_{}\), \(N_{x}\), and \(N_{u}\). The parameter \(\) is specified in (7). Here, \(B\) represents the bound of parameters, and \(d_{}\) is the number of parameters._

The proof of Corollary 1 is similar to that of Theorem 2; therefore, it will be omitted from the paper.

**Remark 2**.: _If we only utilize \(_{S}()\) as our loss function, as demonstrated in Theorem 2, we require both \(M_{u}\) and \(M_{x}\) to be large, posing a significant challenge when dealing with complex nonlinear equations. Obtaining sufficient data becomes a critical issue in such cases. In this paper, we integrate Newton's information into the loss function, defining it as follows:_

\[():=_{S}()+_{N} (),\] (10)

_where \(_{N}()\) represents the cost associated with the unsupervised learning data. If we lack sufficient data for \(_{S}()\), we can adjust the parameters by selecting a small \(\) and increasing \(N_{x}\) and \(N_{u}\). This strategy enables effective learning even when data for \(_{S}()\) is limited. We refer to this neural operator, which incorporates Newton information, as the **Newton Informed Neural Operator**._

In the following experiment, we will use the neural operator established in Eq. (3) and the loss function in Eq. (10) to learn one step of the Newton method locally, i.e., the map between the input \(u\) and the solution \( u\) in eq. (2). If we have a large dataset, we can choose a large \(\) in \(()\) (10); if we have a small dataset, we will use a small \(\) to ensure the generalization of the operator is minimized. After learning one step of the Newton method using the operator neural networks, we can easily and quickly obtain the solution by the initial condition of the nonlinear PDEs (1) and find new solutions not present in the datasets.

## 4 Experiments

### Experimental Settings

We introduce two distinct training methodologies. The first approach employs exclusively supervised data, leveraging the Mean Squared Error Loss (6) as the primary optimization criterion. The second method combines both supervised and unsupervised learning paradigms, utilizing a hybrid loss function 10 that integrates Mean Squared Error Loss (6) for small proportion of data with ground truth (supervised training dataset) and with Newton's loss (9) for large proportion of data without ground truth (unsupervised training dataset). We call the two methods **method 1** and **method 2**. The approaches are designed to simulate a practical scenario with limited data availability, facilitating a comparison between these training strategies to evaluate their efficacy in small supervised data regimes. We chose the same configuration of the neural operator (DeepONet) which is aligned with our theoretical analysis. One can find the detailed experimental settings and the datasets for each example below in Appendix A.

### Case 1: Convex problem

We consider 2D convex problem \((u)-f(u)=0\), where \((u):=- u\), \(f(u):-u^{2}+ 5(x+y)\) and \(u=0\) on \(\). We investigate the training dynamics and testing performance of neural operator (DeepONet) trained with two methods, focusing on Mean Squared Error (MSE) and Newton's loss functions. For method 1, we use 500 supervised data samples (with ground truth), while for method 2, we use 5000 unsupervised data samples (only with the initial state) along with supervised data samples, employing the regularized loss function as defined in Equation 10 with \(=0.01\). Please refer A.1.1 for the dataset generation of convex case. For the detailed experimental settings, refer to Appendix A.

**MSE Loss Training (Fig. 2(a))**: In method 1, Effective training is observed but exhibits poor generalization. The significantly larger testing error compared to the training error suggests that using only MSE loss is insufficient. **Performance Comparison (Fig. 2(b))**: DeepONet-Newton model (Method 2) exhibits superior performance in both \(L_{2}\) and \(H_{1}\) error metrics, highlighting its enhanced generalization accuracy. This study shows the advantages of using Newton's loss for training DeepONet models, illustrating that increasing the number of unsupervised samples via introducing Newton's loss leads to a substantial improvement in the test \(L_{2}\) error and \(H_{1}\) error.

### Case 2: Non-convex problem with multiple solutions

We consider a 2D Non-convex problem,

\[- u(x,y)-u^{2}(x,y)=-s( x)( y) ,\\ u(x,y)=0,\] (11)

where \(=(0,1)(0,1)\). In this case, \((u):=-(u)\), \(f(u):=u^{2}-s( x)( y)\) and it has multiple solutions (see Figure 3 for its solutions).

In the experiment, we let one of the multiple ground truth solutions rarely touched in the supervised training dataset such that the neural operator trained via **method 1** will saturate in terms of test error because it relies on the ground truth to recover all the patterns for multiple solution cases (as shown by the curves in Figure 3). On the other hand, the model trained via **method 2** is less affected by the limited supervised data since the utilization of Newton's loss. One can refer to Appendix A for the detailed experiment setting.

EfficiencyThis case study highlights the superior efficiency of our neural operator-based method as a surrogate model for Newton's method. Both methods parallelize operations to solve 500/5000 Newton linear systems simultaneously, each with distinct initial states. The key advantage of the

Figure 2: Training and testing performance of DeepONet under different conditions.

neural operator lies in its ability to batch the computation of these independent systems efficiently. By efficiently batching and sampling a wide variety of initial states, the neural operator improves the likelihood of discovering to multiple solutions, particularly in nonlinear PDEs with complex solution landscapes. Consequently, while Newton's method alone does not inherently guarantee finding multiple solutions, the combination of rapid computation and extensive initial condition sampling enhances the chances of identifying multiple solutions.

For a fair comparison, the classical Newton solver was also parallelized using CUDA on a GPU. However, the neural operator naturally handles large batch sizes during inference, allowing it to process all systems in one go. One can find the detailed description of the experiments in A.6.

The table demonstrates the significant efficiency gain achieved by batching the computation of independent Newton systems with distinct initial states using the neural operator. For NINO, solving 5000 independent Newton linear systems scales up minimally compared to solving 500 systems, while the classical solver experiences a tenfold increase in computation time. This efficient batching is crucial for improving performance, particularly in complex nonlinear systems like the Gray-Scott model, where solving numerous systems simultaneously is essential for effective pattern discovery.

### Case 3: The Gray-Scott model

The Gray-Scott model [31; 11] describes the reaction and diffusion of two chemical species, \(A\) and \(S\), governed by the following equations:

\[ =D_{A} A-SA^{2}+(+)A,\] \[ =D_{S} S+SA^{2}-(1-S),\]

where \(D_{A}\) and \(D_{S}\) are the diffusion coefficients, and \(\) and \(\) are rate constants.

Newton's Method for Steady-State SolutionsNewton's method is employed to find steady-state solutions (\(=0\) and \(=0\)) by solving the nonlinear system:

\[0 =D_{A} A-SA^{2}+(+)A,\] (12) \[0 =D_{S} S+SA^{2}-(1-S).\]

  
**Parameter** & **Newtonâ€™s Method** & **NINO** \\  Number of Streams & 10 & - \\ Data Type & float32 & float32 \\ Execution Time for 500 linear Newton systems (s) & 31.52 & 1.1E-4 \\ Execution Time for 5000 linear Newton systems (s) & 321.15 & 1.4E-4 \\   

Table 1: Benchmarking the efficiency of Newton Informed Neural Operator. Computational Time Comparison for Solving 500 and 5000 Initial Conditions.

Figure 3: Solutions of 2D Non-convex problem (11)

The Gray-Scott model is highly sensitive to initial conditions, where even minor perturbations can lead to vastly different emergent patterns. Please refer to Figure 5 for some examples of the patterns. This sensitivity reflects the model's complex, non-linear dynamics that can evolve into a multitude of possible steady states based on the initial setup. Consequently, training a neural operator to map initial conditions directly to their respective steady states presents significant challenges. Such a model must learn from a vast functional space, capturing the underlying dynamics that dictate the transition from any given initial state to its final pattern. This complexity and diversity of potential outcomes is the inherent difficulty in training neural operators effectively for systems as complex as the Gray-Scott model. One can refer to A.1.2 for a detailed discussion on the Gray-Scott model. We employ a Neural Operator as a substitute for the Newton solver in the Gray-Scott model, which recurrently maps the initial state to the steady state.

In subfigure (a), we use a ring-like pattern as the initial state to test our learned neural operator. This pattern does not appear in the supervised training dataset and lacks corresponding ground truth data. Instead, it is present only in the unsupervised data (Newton's loss), i.e., some data in Newton's loss will converge to this specific pattern. Despite this, our neural operator, trained using Newton's loss, can effectively approximate the mapping of the initial solution to its correct steady state. we further test our neural operator, utilizing it as a surrogate for Newton's method to address nonlinear problems with an initial state drawn from the test dataset. The curve shows the average convergence rate of \(\|u-u_{i}\|\) across the test dataset, where \(u_{i}\) represents the prediction at the \(i\)-th step by the neural operator. In subfigure (c), we compare the Training Loss (Rescaled Newton's Loss) and Absolute L2 Test Error. The magnitudes are not directly comparable as they represent different metrics; however, the trends are consistent, indicating that the inclusion of unsupervised data and training with Newton's loss contributes to improved model performance.

## 5 Conclusion

In this paper, we develop neural operators to learn the Newton's solver related to nonlinear PDEs (Eq. (1)) with multiple solutions. To speed up the computation of multiple solutions for nonlinear PDEs, we combine neural operator learning with classical Newton methods, resulting in the Newton-informed neural operator. We provide a theoretical analysis of our neural operator, demonstrating that it can effectively learn the Newton operator, reduce the number of required supervised data, and learn solutions not present in the supervised learning data due to the addition of the Newton loss (9) in the loss function. Our experiments are consistent with our theoretical analysis, showcasing the advantages of our network as mentioned earlier.

Figure 4: The convergence behavior of the Neural Operator-based solver.

Reproducibility StatementCode Availability: The code used in our experiments can be accessed via https://github.com/xlliu2017/learn_newton and also the supplementary material. Datasets can be downloaded via URLs in the repository. This encompasses all scripts, functions, and auxiliary files necessary to reproduce our results. Configuration Transparency: All configurations, including hyperparameters, model architectures, and optimization settings, are explicitly provided in the Appendix.

Y.Y. and W.H. were supported by the National Institute of General Medical Sciences through grant 1R35GM146894. The work of X.L. was partially supported by the KAUST Baseline Research Fund.