# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

from the image with a frozen visual encoder, ignoring learning task-related knowledge and limiting the improvement of visual features' discriminative capabilities.

To inject the task-related knowledge into the visual encoder, some methods [24; 8; 5] insert an adapter module into the frozen backbone. Formally, a adapter shared across domains can be straightly introduced to learn the task-related knowledge with the annotations of source domain and domain-aligned constraint, as shown in Fig. 1(b). However, this adapter is domain-agnostic and can only learn domain-invariant knowledge between the two domains under the domain-aligned constraint. Besides, the domain-invariant knowledge is inevitably biased to the source domain, since the annotations are only from the source domain. As shown in Fig. 1(d), compared with the original VLM, the domain-agnostic adapter brings significant improvement to the source domain, while the improvement of the target domain is limited. Summarily, the bias of the domain-invariant knowledge learned from domain-agnostic adapter limits the generalization to the unseen target domain.

Trained on large-scale data, the VLM provides essential general knowledge on unseen images, while the learned domain-invariant knowledge biased to the source domain shows limited improvement on the target domain. Consequently, when transferring essential general knowledge to the domain-invariant knowledge, the domain-agnostic adapter discards some beneficial knowledge on the target domain. Basically, it discards the domain-specific knowledge that distinguishes the target domain but is different from the domain-invariant knowledge. In summary, adding a complementary adapter to capture the target-specific knowledge from the discarded knowledge between the essential general knowledge and domain-invariant knowledge is an effective way to boost the performance of the VLM in DAOD task.

In this paper, we propose a novel Domain-aware Adapter (DA-Ada) to facilitate the visual encoder learning the domain-specific knowledge along with the domain-invariant knowledge. Formally, DA-Ada introduces a Domain-Invariant Adapter (DIA) and Domain-Specific Adapter (DSA) to exploit domain-invariant and domain-specific knowledge, respectively, as shown in Fig. 1(c). The DIA is attached to the block of the visual encoder in parallel and optimized by aligning the feature distribution of two domains to learn domain-invariant knowledge. The DSA is fed with the difference between the input and output of the block to recover the domain-specific knowledge discarded by the DIA. Since the difference represents the feature discarded by the block, the discarded knowledge between the essential general knowledge and domain-invariant knowledge is also hidden in the difference. Hence, the DSA can regain the domain-specific knowledge from the difference adaptively to improve the generalization ability on target domain, as shown in Fig. 1(d). Moreover, we propose the Visual-guided Textual Adapter (VTA), embedding cross-domain information learned by DA-Ada into textual encoder to enhance the discriminability of detection head. Overall, the proposed DA-Ada can inject domain-invariant and domain-specific knowledge into VLM for DAOD.

We conduct evaluations on mainstream DAOD benchmarks: Cross-Weather (Cityscapes \(\) Foggy Cityscapes), Cross-Fov (KITTI \(\) Cityscapes), Sim-to-Real (SIM10K \(\) Cityscapes) and Cross-Style (Pascal VOC \(\) Clipart). Experimental results show that the proposed DA-Ada brings noticeable improvement and outperforms state-of-the-art methods by a large margin. For example, DA-Ada reaches \(58.5\%\) mAP on Cross-Weather, surpassing the state-of-the-art DA-Pro  by \(2.7\%\).

Figure 1: (a) Traditional DAOD methods optimize the backbone adversarially. (b) Domain-agnostic adapter is inserted into the frozen visual encoder to learn domain-invariant knowledge. (c) Domain-aware adapter can simultaneously capture the domain-specific knowledge from the discarded feature. (d) The mAP(\(\%\)) comparison on the Cross-Weather Adaptation. Compared with original VLM, domain-agnostic adapter brings significant improvement to the source domain but limited improvement to the source domain, while domain-aware adapter brings significant improvement to both source domain and target domain.

Related Work

Visual-Language modelsVisual-language models (VLMs) [50; 33; 34] embed visual and text modalities into a shared space, enabling cross-modal alignment. Pre-trained with an astonishing scale of image-text pairs, they demonstrate comprehensive visual understanding. CLIP  simultaneously trains a visual encoder and a textual encoder with 400 million image-text pairs, showing promising performance on both the seen and unseen classes. Furthermore, [79; 19; 14; 61] distill the knowledge from the visual encoder of CLIP into the detection backbone and transform the textual encoder into detection head. Considering strong generalization, we apply RegionCLIP  as the detector.

Domain Adaptive Object Detection (DAOD)aims to adapt the object detector  trained on the labelled source domain to the unlabelled target domain. Previous approaches can be broadly divided into two orthogonal categories: feature alignment and semi-supervised learning. Feature alignment [45; 76; 46; 72; 56; 15; 47; 30; 76] aims to align the feature distributions of the two domains with domain discriminators , to generate domain-invariant knowledge in three levels: image-level [7; 68; 65; 40], instance-level [77; 53] and category-level [62; 25; 38]. To prevent knowledge unique to each domain from interfering with alignment, recent works [55; 2; 1; 32; 66] propose multiple extractors[42; 40; 63; 67; 27] and discriminators  to decouple the domain-invariant and domain-specific knowledge. In parallel, semi-supervised learning strives to augment training data with style transfer [43; 23; 10] and pseudo label [59; 39; 6; 11]. However, applying existing DAOD method to VLM would overfit the model to the training data, compromising the generalization of pre-trained models. To preserve the pre-trained knowledge, we opt to freeze the VLM and devise a novel domain-aware adapter to facilitate cross-domain adaptation. Compared with existing decoupling methods that only use domain-invariant features for detection, our method adopts a decoupling-refusion strategy. It adaptively modify domain-invariant features with domain-specific features to enhance the discriminability on the target domain.

Tuning method for VLMAP adapting pre-trained VLM to downstream tasks via global finetuning is prohibitively expensive and easily overfitted to training datasets. To solve this issue, prompt tuning  replaces the hand-crafted prompts with the learnable tokens for the textual encoder. Conditions like categories [80; 74], human prior  and domain knowledge  are attached to attain robust performance on new tasks. However, they freeze the visual encoder, preventing it from learning cross-domain information for DAOD.In parallel, originated from Natural Language Processing (NLP)[24; 49; 64; 85; 21], adapter tuning inserts learnable small layers into the visual encoder so that the backbone can learn knowledge from new tasks. ViT-Adapter  and Conv-Adapter  are proposed to efficiently transfer pre-trained knowledge to zero or few-shot visual tasks.  integrates the adapter into the CLIP model, and  further analyzes the components to be frozen or learnable.  combines self-supervised learning to enhance the ability to extract low-level features. Recent  explore injecting task-related knowledge into segmentation model SAM . However, tuning the adapter directly on both domains will bias it towards the source domain and fails to distinguish domain-specific knowledge, leading to insufficient discrimination on the target domain. In this paper, we propose a novel domain-aware adapter that explicitly learns both domain-invariant and domain-specific knowledge to inject cross-domain information into the visual encoder.

## 3 Methodology

In this section, we present a novel Domain-aware Adapter (DA-Ada) tailored for DAOD. DA-Ada employs adapter tuning to introduce both domain-specific and domain-invariant knowledge into VLM. It is worth noting that the proposed method can be attached to any CNN-based detectors as a plug-and-play module. Without loss of generality, we take vanilla Faster-RCNN  as an example.

### Overview

Inspired by adapter tuning, we can custom learnable adapters to inject cross-domain information into the visual encoder. Specifically, to enrich the extracted features with high domain generalization capabilities, an ideal adapter should satisfy conditions from the following two aspects. First, it can model the commonalities between the source and target domains, _i.e._domain-invariant knowledge. Second, it can adaptively supply the unique attributes of each domain, _i.e._domain-specific knowledge.

In this perspective, we design an effective Domain-Aware Adapter (DA-Ada) consisting of a Domain-Invariant Adapter (DIA) and a Domain-Specific Adapter (DSA). As shown in Fig. 2(a), given input image \(\), we split the visual encoder into \(N\) blocks \(\{_{i}\}_{i=1}^{N}\) by feature resolutions (\(N\) = 4 in ResNet). Then we attach \(N\) blocks with DA-Ada modules \(\{_{i}\}_{i=1}^{N}\) in Fig. 2(b):

\[_{0}=();_{i}=_{i}( _{i-1},_{i}(_{i-1})),\] (1)

where \(\) denotes the stem layer. For the \(i\)-th DA-Ada module, we first feed the \(i\)-th block's input \(_{i-1}\) into the \(i\)-th DIA module \(_{i}^{I}\) to extract the domain-invariant features \(_{i}^{I}\). Then we attain the domain-specific features \(_{i}^{S}\) from the subtraction of \(_{i-1}\) and \(_{i}^{I}\) by the DSA module \(_{i}^{S}\):

\[_{i}^{I}=_{i}^{I}(_{i-1})+_{i}( _{i-1});_{i}^{S}=_{i}^{S}(_{i-1}- _{i}^{I}).\] (2)

After that, we fuse \(_{i}^{I},_{i}^{S}\) with spatial attention to output \(_{i}\) for \(i\)-th block:

\[_{i}=_{i}^{I}+_{i}^{I}_{i}^{S},\] (3)

where \(\) denotes the element-wise Hadamard product. With \(N\) learnable adapters, we obtain visual embedding \(=_{N}\) for subsequent detection. As the visual embedding contains sufficient cross-domain information, we propose the Visual-guided Textual Adapter (VTA), projecting the visual embedding to the textual encoder to enhance the discriminability of the detection head. As shown in Fig. 2(c), the visual-guided textual adapter uses the visual embedding \(^{S},^{T}\) to infer textual embedding \(^{S},^{T}\) on source and target domain, which is utilized for prediction. Overall, the proposed DA-Ada can inject domain-invariant and domain-specific knowledge into VLM to improve cross-domain generalization ability.

### Domain-Invariant Adapter (DIA)

The DIA module is proposed to inject the domain-invariant knowledge into the visual encoder. As shown in Fig. 2(b), it applies a bottleneck to learn multi-scale domain knowledge, and the output distribution is aligned between domains for extracting domain-invariant knowledge. Specifically, for the \(i\)-th block of the visual encoder, we first forward the input feature \(_{i-1}^{6 c h w}\) to the \(i\)-th DIA and filter domain-irrelevant information with embedding block \(^{I}\):

\[_{i}^{E}=^{I}(_{i-1}).\] (4)

After that, the embedding \(_{i}^{E}^{b c h w}\) is helpful for domain representation learning. Low channel-dimensional features have less information redundancy and are more suitable for domain adaptation than high-dimensional ones. Following this spirit, the embedding is encouraged to be down-projected to a low channel-dimensional vector \(_{i}^{L}^{b r h w}\) to extract domain-invariant

Figure 2: Overview of the proposed (a) DA-Ada for DAOD and the architecture of (b) the \(i\)-th domain-aware adapter module (c) the visual-guided textual adapter.

knowledge and filter redundant information. Formally, a down-projection \(^{D}\) is applied to reduce the dimension to \(r\):

\[_{i}^{L}=^{D}(_{i}^{E}).\] (5)

Considering that the scale of objects varies between domains, we introduce \(M\) down-projectors \(\{_{i}^{D}\}_{i=1}^{M}\) with different receptive fields, enabling it to capture various spatial features across multiple scales. Specifically, the embedding \(_{i}^{E}=[_{i,1}^{E},_{i,2}^{E},...,_{ i,M}^{E}]\) is first split up evenly in the channel dimension. Then, each partition is resized to different resolutions and down-projected. Therefore, the multi-scale version of Eq. (5) is expressed as:

\[_{i}^{L}=[_{1}^{D}(_{i,1}^{E}),_{2} ^{D}(_{i,2}^{E})...,_{M}^{D}(_{i,M}^{E})].\] (6)

Furthermore, the low-dimensional knowledge \(_{i}^{L}\) is encouraged to be mapped back to the original dimensional feature space and supplemented to the pre-trained features. Typically, we apply the dimension-raising function \(^{U}\) on \(_{i}^{L}\) to extract domain-invariant knowledge for the visual encoder.

\[_{i}^{I}(_{i-1})=^{U}(_{i}^{L}),\] (7)

where \(_{i}^{I}(_{i-1})^{b c h w}\) is output of the \(i\)-th DIA, and will be summed with \(_{i}^{I}(_{i-1})\) to attain domain-invariant feature \(_{i}^{I}\) in Eq. (2). To ensure DIA learning domain-invariant knowledge, the \(_{i}^{I}\) is expected to be well aligned between the two domains. Therefore, \(N\) domain discriminator \(\{_{i}\}_{i=1}^{N}\) is attached to each \(_{i}^{I}\) to calculate adversarial loss \(_{dia}\). We will introduce this loss in Sec.3.5.

With the combination of dimensional reduction-increase processes and the constraints of detection and adversarial loss, the DIA can extract domain-invariant features while reducing redundant features.

### Domain-Specific Adapter (DSA)

Adapted with DIA, the essential general knowledge of the frozen VLM is transferred to domain-invariant knowledge. However, the knowledge learned only through the DIA is biased towards the source domain and appears less discriminative on the target domain. Considering the high generalization of essential general knowledge of the frozen VLM, we attribute this problem to the fact that the DIA discards some domain-specific knowledge that is highly generalizable on the unlabelled target domain. Since the difference between the input and output of the block denotes the discarded feature, the discarded domain-specific knowledge is also hidden in the difference. To this end, we propose the DSA module to recover domain-specific knowledge from the difference.

After the DIA injects the domain-invariant knowledge into the visual encoder, the domain-specific knowledge unique to the target domain is discarded by the output \(_{i}^{I}\). Therefore, we first obtain the feature \(_{i}^{D}\) discarded by the visual encoder block from the difference of the input \(_{i-1}\) and \(_{i}^{I}\):

\[_{i}^{D}=^{S}(_{i-1}-_{i}^{I}),\] (8)

where \(^{S}\) is similar with the embedding block \(^{I}\). As domain-specific knowledge is hidden in the discarded difference \(_{i}^{D}\), a bottleneck architecture is employed for adaptive knowledge extraction:

\[^{L^{}}_{i}=^{D^{}}(_{i}^{D}),\] (9)

\[_{i}^{S}=_{i}^{S}(_{i-1}-_{i}^{I})= ^{U^{}}(^{L^{}}_{i}),\] (10)

where \(^{D^{}},^{U^{}}\) follow the same configurations as \(^{D},^{U}\) to perceive multi-scale domain-specific knowledge in bottleneck manner.

Generally speaking, domain-invariant knowledge dominates the process of transferring essential general knowledge of the VLM, while domain-specific knowledge fine-tunes this process based on the characteristics of each domain. To this end, it is a more reasonable way to adaptively supplement domain-specific knowledge with the extracted \(_{i}^{S}\) through pixel-level attention rather than straightforward addition. Therefore, the injection of the whole DA-Ada is written as Eq. (3).

### Visual-guided Textual Adapter (VTA)

With the domain-aware adapter to inject domain-invariant and domain-specific knowledge, the extracted visual feature shows rich discriminability, which can also be used to improve detection head. Therefore, we introduce the VTA to exploit the cross-domain information contained in the visual features to enhance the textual encoder.

In order to fully exploit the domain-invariant and domain-specific knowledge extracted by the DA-Ada module, we equip two learnable components for VTA: the domain-invariant textual adapter \(\) (DITA) and the domain-specific textual adapter \(^{S},^{T}\) (DSTA) shown in Fig. 2(c). The DITA is shared across domains to encode visual domain-invariant knowledge into the input of the textual encoder, optimized by a domain discriminator \(^{P}\). The DSTA is tailored to further supplement domain-specific knowledge for the source domain \(S\) and the target domain \(T\). In practice, the structure of DITA and DSTA is a 3-layer MLP with a hidden dimension of 512, projecting visual embeddings into 8 tokens for the textual encoder. Formally, the VTA embeds visual information into the textual embedding,

\[_{i}^{S}=((^{S}),^{S}( ^{S}),c_{i});_{i}^{T}=((^ {T}),^{T}(^{T}),c_{i}),\] (11)

where \(^{S},^{T}\), \(i\) and \(c_{i}\) denote the visual embedding from domain \(S,T\), the \(i\)-th class and its textual description. \(\) denotes the textual encoder. \(_{i}^{S}\) and \(_{i}^{T}\) is the textual-level classifier embedding of \(i\)-th class from the source and target domains, respectively.

Our proposed VTA introduces discriminative visual features into the textual encoder, alleviating the problem of insufficient adaptation in plain textual tuning. Existing methods  only tune learnable textual descriptions for detection head, as shown in Fig. 3(a). However, textual descriptions are insufficient to describe certain inter-domain differences, _e.g._, differences in fields of view, leading to a limited ability to learn cross-domain information. Different from them, VTA analyses domain-invariant and domain-specific knowledge from visual features, inferring an image-conditional detection head with high discriminability, as shown in Fig. 3(b).

### Optimization Objective

We aim to insert the DA-Ada into the visual encoder to learn cross-domain information and further tune the prompt for discriminative textual representation with image conditions. On the one hand, we introduce domain adversarial loss to the DIA and DITA to guide the learning of domain-invariant information. Formally, we obtain the output features \(_{i}^{I,S},_{i}^{I,T}\) for the source image \(_{s}\) and the target image \(_{t}\) of each DIA, and minimize the adversarial loss:

\[_{dia}=-_{i=1}^{N}[_{_{s}}||_{i} (_{i}^{I,S})||_{2}^{2}+_{_{t}}||_{i} (_{i}^{I,T})-||_{2}^{2}].\] (12)

And the domain-shared DITA is expected to be aligned between domains:

\[_{dita}=-[_{_{s}}||^{P}(^ {S})||_{2}^{2}+_{_{t}}||^{P}(^{T})- ||_{2}^{2}],\] (13)

where \(^{S},^{T}\) denotes the source and target visual embedding.

On the other hand, we learn task-related domain-specific knowledge in a semi-supervised manner. For the source image, we calculate the cross-entropy for each visual embedding \(^{S}\) with its annotations \(y\). For the target \(^{T}\), we first obtain the prediction via the hand-crafted prompt "A photo of [class]" and filter out high-confidence pseudo labels \(y^{}\), then minimize the cross-entropy as well:

\[_{det}=_{ce}(^{S}^{S},y)+ _{ce}(^{T}^{T},y^{}),\] (14)

where \(\) denotes Matrix multiplication.

Figure 3: Comparison between (a) DA-Pro and (b) Visual-guided textual adapter.

Meanwhile, to decouple domain-invariant and domain-specific knowledge, we maximize the distribution discrepancy between DIA and DSA.

\[_{dec}=_{i=1}^{N}[_{_{s},_{t}}max[0, cos(_{i}^{I},_{i}^{I}_{i}^{S})-]],\] (15)

where \(cos(,)=^{}|}{||||_{2}^{2}||||_{2}^{2}}\) is absolute value of cosine distance, \(\) is a threshold.

With the help of domain classifiers, DIA and DITA are encouraged to contain more domain-invariant knowledge. By minimizing \(_{dec}\), the gap between DIA and DSA will be enlarged, which promotes DSA to extract more domain-specific knowledge. Overall, the optimization objective is:

\[=_{det}+_{dia}_{dia}+_{dita} _{dita}+_{dec}_{dec}+_{reg},\] (16)

where \(_{reg}\) is the regression loss, and \(_{dia},_{dita},_{dec}\) are balance ratios.

## 4 Experiment

### Datasets and Implementation

We evaluate our method on four benchmarks: Cross-Weather(Cityscapes \(\)Foggy Cityscapes ), Cross-Fov(KITTI \(\)Cityscapes), Sim-to-Real(SIM10k \(\)Cityscapes) and Cross-Style(Pascal VOC \(\)Clipart ). Following , we adapt RegionCLIP(ResNet-50 ) with Faster-RCNN architecture as the baseline detector. We detail the datasets and implementation in Sec. 6.1and 6.3 of the Appendix.

### Comparison to SOTA methods

We present representative state-of-the-art DAOD approaches for comparison, including feature alignment and semi-supervised learning methods.

**Cross-Weather Adaptation Scenario** Table 1 (C\(\)F) illustrates that the proposed DA-Ada surpasses SOTA DA-Pro  by a remarkable margin of \(2.6\%\), achieving the highest mAP over eight classes of \(58.5\%\). Compared with existing methods, DA-Ada significantly improves seven categories (_i.e._ person,

    &  &  & S\(\)C \\  Methods & Person & Rider & Car & Truck & Bus & Train & Motor & Bicycle & mAP & mAP & mAP \\  DA-Faster  & 29.2 & 40.4 & 43.4 & 19.7 & 38.3 & 28.5 & 23.7 & 32.7 & 32.0 & 41.9 & 38.2 \\ SIGMA++ & 46.4 & 45.1 & 61.0 & 32.1 & 52.2 & 44.6 & 34.8 & 39.9 & 44.5 & 49.5 & 57.7 \\ CIGAR  & 46.1 & 47.3 & 62.1 & 27.8 & 56.6 & 44.3 & 33.7 & 41.3 & 44.9 & 48.5 & 58.5 \\ CSDA  & 46.6 & 46.3 & 63.1 & 28.1 & 56.3 & 53.7 & 33.1 & 39.1 & 45.8 & 48.6 & 57.8 \\ HT  & 52.1 & 55.8 & 67.5 & 32.7 & 55.9 & 49.1 & 40.1 & 50.3 & 50.4 & 60.3 & 65.5 \\ D\({}^{2}\)-UDA  & 46.9 & 53.3 & 64.5 & 38.9 & 61.0 & 48.5 & 42.6 & 54.2 & 50.6 & 60.3 & 58.1 \\ AT  & 56.3 & 51.9 & 64.2 & 38.5 & 45.5 & 55.1 & **54.3** & 35.0 & 50.9 & - & - \\ NSA-UDA  & 50.3 & 60.1 & 67.7 & 37.4 & 57.4 & 46.9 & 47.3 & 54.3 & 52.7 & 55.6 & 56.3 \\ DA-Pro * & 55.4 & 62.9 & 70.9 & 40.3 & 63.4 & 54.0 & 42.3 & 58.0 & 55.9 & 61.4 & 62.9 \\  DA-Ada(Ours)* & **57.8** & **65.1** & **71.3** & **43.1** & **64.0** & **58.6** & 48.8 & **58.7** & **58.5** & **66.7** & **67.3** \\   

Table 1: Comparison (\(\%\)) with existing methods on Cross-Weather Cityscapes\(\)Foggy Cityscapes (C\(\)F), Cross-Fov KITTI\(\)Cityscapes (K\(\)C) and Sim-to-Real adaptation SIM10K\(\)Cityscapes (S\(\)C). * denotes CLIP -based methods.

    &  &  & S\(\)C \\  Methods & Person & Rider & Car & Truck & Bus & Train & Motor & Bicycle & mAP & mAP & mAP \\  DA-Faster  & 29.2 & 40.4 & 43.4 & 19.7 & 38.3 & 28.5 & 23.7 & 32.7 & 32.0 & 41.9 & 38.2 \\ SIGMA++ & 46.4 & 45.1 & 61.0 & 32.1 & 52.2 & 44.6 & 34.8 & 39.9 & 44.5 & 49.5 & 57.7 \\ CIGAR  & 46.1 & 47.3 & 62.1 & 27.8 & 56.6 & 44.3 & 33.7 & 41.3 & 44.9 & 48.5 & 58.5 \\ CSDA  & 46.6 & 46.3 & 63.1 & 28.1 & 56.3 & 53.7 & 33.1 & 39.1 & 45.8 & 48.6 & 57.8 \\ HT  & 52.1 & 55.8 & 67.5 & 32.7 & 55.9 & 49.1 & 40.1 & 50.3 & 50.4 & 60.3 & 65.5 \\ D\({}^{2}\)-UDA  & 46.9 & 53.3 & 64.5 & 38.9 & 61.0 & 48.5 & 42.6 & 54.2 & 50.6 & 60.3 & 58.1 \\ AT  & 56.3 & 51.9 & 64.2 & 38.5 & 45.5 & 55.1 & **54.3** & 35.0 & 50.9 & - & - \\ NSA-UDA  & 50.3 & 60.1 & 67.7 & 37.4 & 57.4 & 46.9 & 47.3 & 54.3 & 52.7 & 55.6 & 56.3 \\ DA-Pro * & 55.4 & 62.9 & 70.9 & 40.3 & 63.4 & 54.0 & 42.3 & 58.0 & 55.9 & 61.4 & 62.9 \\  DA-Ada(Ours)* & **57.8** & **65.1** & **71.3** & **43.1** & **64.0** & **58.6** & 48.8 & **58.7** & **58.5** & **66.7** & **67.3** \\   

Table 2: Comparison (\(\%\)) with existing methods on Cross-Style adaptation task Pascal VOC\(\)Clipart. * denotes CLIP -based methods.

[MISSING_PAGE_FAIL:8]

domain. Moreover, the DSA boosts the DIA by \(1.4\%\) and \(2.3\%\) with the help of \(_{dec}\), showing that learning domain-specific knowledge improves the discrimination of the target detection head.

**Insertion Site** We explicitly study the insertion site of DA-Ada, as shown in Table 5. When single adapter is applied, inserting the DA-Ada in the shallow block achieves better performance, _e.g._DA-Ada with block 1 obtain \(55.1\%\), surpassing all other insertion sites with block 2/3/4. And increasing the number of DA-Ada from \(1\) to \(4\) leads to steady improvements of \(1.8\%,0.8\%,0.8\%\) respectively.

**Input and Injection Operation** We analyze different input features and injection operations of DIA/DSA in Table 6. Directly inserting DIA into the visual encoder and directly adding to the output of each block attains \(2.2\%\) improvement, showing the effectiveness of learning domain-invariant knowledge. However, there is limited performance gain in sending the output \(_{i}(_{i-1})\) to DSA. It indicates that domain-specific knowledge is ignored during feature extraction of the visual encoder. To this end, inputting \(_{i-1}-_{i}(_{i-1})\) to DSA receives \(56.2\%\), exhibiting that the DSA can regain the domain-specific knowledge from the difference. As \(_{i}^{I}=_{i}^{I}(_{i-1})+_{i}( _{i-1})\) is updated to be domain-invariant, \(_{i-1}\) - (\(_{i}^{I}\)) removes domain-invariant parts and appears to be domain-specific. Therefore, we forward it to DSA and gain an improvement of \(0.5\%\), demonstrating the efficacy of learning domain-specific knowledge. Additionally, we substitute cross-attention and pixel-level attention for the direct addition, and gains highest mAP of \(57.0\%\) and \(57.1\%\). It reveals that domain-specific knowledge describes intra-domain properties and is more suitable for refining the extracted features. For efficiency, we adopt the simpler pixel-level attention as the fusion function.

**Bottleneck Dimension** We also conduct an ablation study in Table 7 to explore the optimal bottleneck dimension of the DA-Ada. As the dimension increases, the performance peaks \(57.1\%\) when the bottleneck dimension is \(1/2\) of the input and then appears to decline. We conclude that appropriate dimensional reduction can filter redundant features while extracting task knowledge.

**Textual Tuning _vs._ Visual-guided Textual Adapter** We compare the visual-guided textual adapter against existing methods, as shown in Table 8. Guided by visual conditions, VTA outperforms SOTA plain textual tuning methods by margins of \(0.7\%\) and \(1.5\%\) in two scenarios. Notably, VTA excels in the challenging Cross-FoV adaptation, suggesting that the visual modality effectively supplements the limitations of the textual encoder in describing domain attributes.

**Ablation for Visual-guided Textual Adapter** As shown in Table 9, learning DITA attains an mAP of \(57.6\%\), and by introducing an additional adversarial loss, it achieves \(57.9\%\). Moreover, with DSTA generating prompts for each domain, it exhibits a full adaptation performance of \(58.5\%\). This shows that embedding image conditions into textual encoder can promote cross-domain detection.

**Computational Efficiency** As shown in Table 10, employing VLM yields a similar parameter scale while achieving a peak mAP of \(58.5\%\). This indicates that the superior performance of DA-Ada does not arise from an increase in parameters. Furthermore, DA-Ada achieves the highest absolute gain of \(+8.0\%\) with the training of only 1.794M parameters, demonstrating remarkable efficiency.

### Detection Visualization

In Fig. 4, we present the comparison of the ground truth boxes (a) and the detection boxes of SOTA DA-Pro (c) and our methods (b)(d) on the target domain. (a.1)(b.1)(c.1)(d.1) are zoomed from the same region of images (a)(b)(c)(d) for a better view. Fig. 4(a.1) presents eight objects in the cropped region: 6 overlapped cars and a rider with a bicycle. The baseline model only detects two clear cars in Fig. 4(b.1). Failing to describe domain information, like weather conditions, it misses other objects hidden in the fog. In Fig. 4(c), the DA-Pro distinguishes the rider and the bicycle and improves \(9.3\%\) mAP with the domain-adaptive prompt. However, it ignores one car on the left of Fig. 4(c.1), suffering limited generalization ability due to insufficient domain representation learning in the visual encoder. Our proposed DA-Ada correctly detects the missing car (labelled in green) in the cropped region Fig. 4(d.1). By injecting cross-domain information into the visual encoder,

  Method & Backbone Param (M) & Learnable Param (M) & mAP Abs. & Gains \\  DSS  & 29.812 & 29.812 & 40.9 & +4.2 \\ CSDA  & 33.645 & 33.645 & 45.3 & +6.9 \\ AT  & **39.225** & 18.723 & 50.9 & +7.9 \\ DA-Pro  & 34.834 & **0.008** & 55.9 & +3.3 \\ DA-Ada(Ours) & 36.620 & 1.794 & **58.5** & **+8.0** \\  

Table 10: Comparison (\(\%\)) of computational efficiency on Cross-Weather adaptation.

  DITA & \(_{dec}\) & DSTA & mAP & Gains \\   & & & 57.1 & - \\ ✓ & & & 57.6 & +0.5 \\ ✓ & ✓ & & 57.9 & +0.8 \\ ✓ & ✓ & ✓ & **58.5** & **+1.4** \\  

Table 9: Ablation studies (\(\%\)) of VTA on Cross-Weather adaptation.

the DA-Ada enables the model to detect more confidently on two bicycles (\(83\%,98\%\)) and one person (\(91\%\)), compared with DA-Pro's (\(79\%,95\%\)) and (\(89\%\)). These comparison results reveal the effectiveness of DA-Ada.

### Feature Visualization

In Fig. 5, we visualize the output features of the traditional adapter, the domain-invariant adapter (DIA), domain-specific adapter (DSA) and the domain-aware adapter (DA-Ada). We sample image (a) a car and a person in the fog from Foggy Cityscapes. The traditional adapter (b) roughly extracts the outline of the car. However, affected by target domain attributes, such as fog, background areas are also highlighted in (b), and the person is not salient. DIA (c) mainly focuses on the object area and extracts domain-shared task information. DSA (d) mainly focuses on factors related to domain attributes besides the objects, such as foggy areas. By combining DIA with DSA, DA-Ada (e) extracts the car and person while reducing the interference of fog in the background. Compared with (b), objects are more salient in (e), indicating the effectiveness of DA-Ada.

## 5 Conclusion

In this paper, we propose a novel Domain-Aware Adapter (DA-Ada) for DAOD. As a small learnable attachment, it transfers highly generalized knowledge the visual-language model provides to cross-domain information for DAOD. Precisely, it consists of a Domain-Invariant Adapter (DIA) for learning domain-invariant knowledge and a Domain-Specific Adapter (DSA) for recovering the domain-specific knowledge from information discarded by the visual encoder. Extensive experiments over multiple DAOD tasks validate the effectiveness of DA-Ada in inferring a discriminative detector.