# Behavior Alignment via

Reward Function Optimization

 Dhawal Gupta

University of Massachusetts

&Yash Chandak1

Stanford University

&Scott M. Jordan

University of Alberta

&Philip S. Thomas

University of Massachusetts

&Bruno Castro da Silva

University of Massachusetts

Both authors contributed equally to this work. Work done while at the University of Massachusetts. Corresponding author: Dhawal Gupta (dgupta@cs.umass.edu).

###### Abstract

Designing reward functions for efficiently guiding reinforcement learning (RL) agents toward specific behaviors is a complex task. This is challenging since it requires the identification of reward structures that are not sparse and that avoid inadvertently inducing undesirable behaviors. Naively modifying the reward structure to offer denser and more frequent feedback can lead to unintended outcomes and promote behaviors that are not aligned with the designer's intended goal. Although potential-based reward shaping is often suggested as a remedy, we systematically investigate settings where deploying it often significantly impairs performance. To address these issues, we introduce a new framework that uses a bi-level objective to learn _behavior alignment reward functions_. These functions integrate auxiliary rewards reflecting a designer's heuristics and domain knowledge with the environment's primary rewards. Our approach automatically determines the most effective way to blend these types of feedback, thereby enhancing robustness against heuristic reward misspecification. Remarkably, it can also adapt an agent's policy optimization process to mitigate suboptimalities resulting from limitations and biases inherent in the underlying RL algorithms. We evaluate our method's efficacy on a diverse set of tasks, from small-scale experiments to high-dimensional control challenges. We investigate heuristic auxiliary rewards of varying quality--some of which are beneficial and others detrimental to the learning process. Our results show that our framework offers a robust and principled way to integrate designer-specified heuristics. It not only addresses key shortcomings of existing approaches but also consistently leads to high-performing solutions, even when given misaligned or poorly-specified auxiliary reward functions.

## 1 Introduction

In this paper, we investigate the challenge of enabling reinforcement learning (RL) practitioners, who may not be experts in the field, to incorporate domain knowledge through heuristic auxiliary reward functions. Our goal is to ensure that such auxiliary rewards not only induce behaviors that align with the designer's intentions but also allow for faster learning. RL practitioners typically model a given control problem by first designing simple reward functions that directly quantify whether (or how well) an agent completed a task. These could be, for instance, functions assigning a reward of \(+1\) iff the agent reaches a specified goal state, and zero otherwise. However, optimizing a policy based on such a sparse reward function often proves challenging.

To address this issue, designers often introduce auxiliary reward functions that supplement the original rewards. Auxiliary rewards are heuristic guidelines aimed at facilitating and speeding up the learning process. One could, e.g., augment the previously described reward function (which gives a reward of \(+1\) upon reaching a goal state) with an auxiliary reward accounting for the agent's distance to the goal. However, the effectiveness of using auxiliary reward functions largely depends on the problem's complexity and the designer's skill in crafting heuristics that, when combined with the original reward function, do not induce behaviors different than the ones originally intended [26; 27].

Existing methods like potential-based reward shaping  aim to incorporate domain knowledge without misaligning the behaviors induced by the resulting combined reward functions. However, as we discuss in Section 3, potential-based shaping has several limitations: _(i)_ it is restricted to state-based functions; _(ii)_ it amounts to a different initialization of the \(q\)-function; _(iii)_ it does not alter policy gradients in expectation; and _(iv)_ it can increase the variance in policy gradient methods.

To address these challenges, we introduce a scalable algorithm that empowers RL practitioners to specify potentially imperfect auxiliary reward functions. It ensures that the resulting optimization process will not inadvertently lead to unintended behaviors and that it will allow for faster learning. In particular, this paper addresses the following challenges:

**(1) How to incorporate auxiliary reward information:** We introduce a novel bi-level objective to analyze and automatically fine-tune designer-created auxiliary reward functions. It ensures they remain aligned with the original reward and do not induce behaviors different from those originally intended by the designer. Additionally, we formulate the problem to shape the optimization landscape, biasing our bi-level optimizer toward auxiliary reward functions that facilitate faster learning.

**(2) How to use auxiliary reward to mitigate algorithmic biases:** We show that our framework can automatically adjust how primary and auxiliary rewards are blended to mitigate limitations or biases inherent in the underlying RL algorithm (Section 4.1). For instance, many policy-gradient-based RL algorithms are subject to biases due to issues like discounting mismatch  or partial off-policy correction . These biases can hinder the algorithm's ability to identify near-optimal policies.

**(3) How to ensure scalability to high-dimensional problems:** We introduce an algorithm that employs _implicit gradients_ to automatically adjust primary and auxiliary rewards, ensuring that the combined reward function aligns with the designer's original expectations (see Figure 1). We evaluate our method's efficacy across a range of tasks, from small-scale to high-dimensional control settings (see Section 6). In these tasks, we experiment with auxiliary rewards of varying quality; some accelerate learning, while others can be detrimental to finding an optimal policy.

## 2 Notation

In this paper, we investigate sequential decision-making problems modeled as Markov decision processes (MDPs). An MDP is defined as a tuple \((,,p,r_{p},r_{},,d_{0})\), where \(\) is the state set, \(\) is the action set, \(p\) is the transition function, \(r_{p}\) is the _primary_ reward function,

Figure 1: Auxiliary rewards can be used to convey to the agent how we (designers) _think_ it should solve the problem. However, if not carefully designed, they can lead to policies that result in undesired behaviors. This figure provides a visual illustration of a toy example depicting how the proposed method works. The star represents the optimal policy, and the red dot represents the fixed point of a policy optimization process under a “sub-optimal” heuristic; i.e., one that, when naively combined with \(r_{p}\), induces behaviors different from those under the optimal policy for \(r_{p}\). **(Left)** Vector field of a policy optimization process converging to a sub-optimal policy. **(Middle and Right)** By changing the influence of auxiliary rewards, our method can dynamically _correct_ the _entire policy optimization process_ steering it towards a policy that results in the desired behavior.

\(\) is an _optional_ auxiliary reward function (possibly designed by a non-expert in machine learning, based on domain knowledge), and \(d_{0}\) is the starting state distribution. Let \(_{}:\) be any policy parameterized using \(\). For brevity, we will often use \(_{}\) and \(\) interchangeably. Let \(S_{t}\) and \(A_{t}\) be the random variables for the state and action observed at the time \(t\). As in the standard RL setting, the performance \(J()\) of a policy \(_{}\) is defined as the expected discounted return with respect to the (primary) reward function, \(r_{p}\); i.e., \(J()_{}[_{t=0}^{T}^{t}r_{p}(S_{t},A_{t})]\), where \(T+1\) is the episode length. An optimal policy parameter \(^{*}\) is defined as \(^{*}_{}J()\). A popular technique to search for \(^{*}\) is based on constructing sample estimates \((,r_{p})\) of the (\(\)-dropped) policy gradient, \((,r_{p})\), given an agent's interactions with the environment for one episode [58; 59]. Then, using \(_{}(s,a)\) as a shorthand for \(_{}(s,a)/\), these quantities are defined as follows:

\[(,r_{p})=_{_{}}[(,r_{p} )](,r_{p}) _{t=0}^{T}_{}(S_{t},A_{t})_{j=t}^{T}^{j-t}r_{p}(S_{j},A_{j}).\] (1)

## 3 Limitations of Potential Based Reward Shaping

When the objective function \(J()\) is defined with respect to a _sparse_ reward function \(r_{p}\) (i.e., a reward function such that \(r_{p}(s,a)\!\! 0\) for most \(s\) and \(a\)), searching for \(^{*}\) is challenging . A natural way to provide more frequent (i.e., denser) feedback to the agent, in the hope of facilitating learning, is to consider an alternate reward function, \(_{}\!\!r_{p}\!+\!r_{}\). However, as discussed earlier, \(r_{}\) may be a designer-specified auxiliary reward function not perfectly aligned with the objective encoded in \(r_{p}\). In this case, using \(_{}\) may encourage undesired behavior. An alternative way to incorporate domain knowledge to facilitate learning was introduced by Ng et al. . They proposed using a _potential function_, \(:\) (analogous to \(r_{}\)), to define new reward functions of the form \(_{}(S_{t},A_{t},S_{t+1})\!\!\!r_{p}(S_{t},A_{t})\!+\! (S_{t+1})\!\!-\!(S_{t})\). Importantly, they showed that optimal policies with respect to the objective \([_{t=0}^{T}^{t}_{}(S_{t},A_{t},S_{t+1})]\) are also optimal with respect to \(J()\).

While potential-based reward shaping can partially alleviate some of the difficulties arising from sparse rewards, Wienwiora  showed that \(q\)-learning using \(_{}\) produces the _exact same sequence of updates_ as \(q\)-learning using \(r_{p}\) but with a different initialization of \(q\)-values. In what follows, we establish a similar result: we show that performing potential-based reward shaping has _no impact on expected policy gradient_ updates; and that it can, in fact, even increase the variance of the updates.

**Property 1**.: \([(,_{})]\!=\![( ,r_{p})]\) _and \(((,_{}))\) can be higher than \(((,r_{p}))\)._

All proofs are deferred to Appendix A. The above points highlight some of the limitations of potential-based shaping for policy gradients and \(q\)-learning--both of which form the backbone of the majority of model-free RL algorithms . Furthermore, potential functions \(\)_cannot_ depend on actions , which restricts the class of eligible auxiliary rewards \(r_{}\) and heuristics functions that may be used. Finally, notice that \(\) is designed independently of the agent's underlying learning algorithm. As we will show in the next sections, our method can autonomously discover auxiliary reward functions that not only facilitate learning but also help mitigate various types of algorithmic limitations and biases.

## 4 Behavior Alignment Reward Function

In this section, we introduce an objective function designed to tackle the primary challenge investigated in this paper: how to effectively leverage designer-specified auxiliary reward functions to rapidly induce behaviors envisioned by the designer. The key observation is that naively adding an auxiliary reward function \(r_{}\) to \(r_{p}\) may produce policies whose corresponding behaviors are misaligned with respect to the behaviors induced by \(r_{p}\). In these cases, \(r_{}\) should be ignored during the search for an optimal policy. On the other hand, if \(r_{p}\) and \(r_{}\) may be combined in a way that results in the desired behaviors, then combinations that produce frequent and informative feedback to the agent should be favored, as they are likely to facilitate faster learning.

To tackle the challenges discussed above, we employ a bi-level optimization procedure. This approach aims to create a _behavior alignment reward_ by combining \(r_{}\) and \(r_{p}\) using a parameterized function. Our method is inspired by the optimal rewards framework by Singh et al. [52; 53]. Let \(_{}[0,1)\) be a _discount rate value_ parameterized by \(\).2 Let \(r_{}:\) be a _behavior alignment reward_: a function of both \(r_{p}\) and \(r_{}\), parameterized by \(\), where \(\) and \(\) are function classes. One example of a behavior alignment reward function is \(r_{}(s,a) f_{_{1}}(s,a)+_{2}r_{p}(s,a)+_{3}r_{}(s,a)\), where \(f_{}:\) and \((_{1},_{2},_{3})\). Let Alg be any gradient/semi-gradient/non-gradient-based algorithm that outputs policy parameters. To mitigate possible divergence issues arising from certain policy optimization algorithms like DQN , we make the following simplifying assumption, which can generally be met with appropriate regularizers and step sizes:

**Assumption 1**.: _Given \(r_{}\) and \(_{}\), the algorithm Alg\((r_{},_{})\) converges to a fixed point \(\)-- which we denote as \((,)\) to emphasize its indirect dependence on \(\) and \(\) through Alg, \(r_{}\), and \(_{}\)._

Given this assumption, we now specify the following bi-level objective:

\[^{*},^{*}*{arg\,max}_{, } J((,))-_{}_{}, (,)(r_{},_{ }).\] (2)

Here, \(_{}\) serves as the regularization coefficient for the value of \(_{}\), and Alg denotes a given policy optimization algorithm. Let, as an example, Alg be an on-policy gradient algorithm that uses samples to estimate the gradient \((,r_{p})\), as in (1). We can then define a corresponding variant of \((,r_{p})\) that is compatible with our formulation and objective, and which uses both \(r_{}\) and \(_{}\), as follows:

\[_{}(,,)_{_{}} [_{t=0}^{T}_{}(S_{t},A_{t})_{j=t}^{T}_{}^ {j-t}r_{}(S_{j},A_{j})].\] (3)

Notice that the bi-level formulation in (2) is composed of three key components: outer and inner objectives, and an outer regularization term. In what follows, we discuss the need for these.

**Need for Outer- and Inner-Level Objectives:** The **outer-level objective** in Equation (2) serves a critical role: it evaluates different parameterizations, denoted by \(\), for the behavior alignment reward function. These parameterizations influence the induced policy \((,)\), which is evaluated using the performance metric \(J\). Recall that this metric quantifies the alignment of a policy with the designer's primary reward function, \(r_{p}\). In essence, the outer-level objective seeks to optimize the behavior alignment reward function to produce policies that are effective according to \(r_{p}\). This design adds robustness against any misspecification of the auxiliary rewards.3 In the inner-level optimization, by contrast, Alg identifies a policy \((,)\) that is optimal or near-optimal with respect to \(r_{}\) (which combines \(r_{}\) through the behavior alignment reward). In the **inner-level optimization**, the algorithm Alg works to identify a policy \((,)\) that is optimal or near-optimal in terms of \(r_{}\) (which incorporates \(r_{}\) via the behavior alignment reward). By employing a bi-level optimization structure, several benefits emerge. When \(r_{}\) is well-crafted, \(r_{}\) can exploit its detailed information to give Alg frequent/dense reward feedback, thus aiding the search for an optimal \(^{*}\). Conversely, if \(r_{}\) leads to sub-optimal policies, then the influence of auxiliary rewards can be modulated or decreased accordingly by the optimization process by adjusting \(r_{}\). Consider, for example, a case where the behavior alignment reward function is defined as \(r_{}(s,a) f_{_{1}}(s,a)+_{2}r_{p}(s,a)+_{3}r_{}(s,a)\). In an adversarial setting--where the designer-specified auxiliary reward \(r_{}\) may lead to undesired behavior--the bi-level optimization process has the ability to set \(_{3}\) to \(0\). This effectively allows the behavior alignment reward function \(r_{}\) to exclude \(r_{}\) from consideration. Such a bi-level approach to optimizing the parameters of behavior alignment reward functions can act as a safeguard against the emergence of sub-optimal behaviors due to a misaligned auxiliary reward, \(r_{}\). This design is particularly valuable because it allows the objective in (2) to leverage the potentially dense reward structure of \(r_{}\) to provide frequent action evaluations when the auxiliary reward function is well-specified. At the same time, the approach maintains robustness against possible misalignments.

**Need for Outer Regularization:** The bi-level optimization problem (2) may have multiple optimal solutions for \(\)--including the trivial solution where \(r_{}\) is always ignored. The goal of regularizing the outer-level objective (in the form of the term \(_{}_{}\)) is to incorporate a prior that adds a preference for solutions, \(^{*}\), that provide useful and frequent evaluative feedback to the underlying RL algorithm. In the next paragraphs, we discuss the need for such a regularizer and motivate its mathematical form. First, recall that _sparse_ rewards can pose challenges for policy optimization. An intuitive solution to this problem could involve biasing the optimization process towards _denser_ behavior alignment reward functions, e.g., by penalizing for sparsity of \(r_{}\). Unfortunately, the distinction between sparse and dense rewards alone may not fully capture the nuances of what designers typically consider to be a "good" reward function. This is the case because _a reward function can be dense and still may not be informative_; e.g., a reward function that provides \(-1\) to the agent in every non-goal state is dense but fails to provide useful feedback regarding how to reach a goal state. A better characterization of how useful (or informative) a reward function is may be constructed in terms of how _instructive_ and _instantaneous_ the evaluation or feedback it generates is. We consider a reward function to be _instructive_ if it produces rewards that are well-aligned with the designer's goals. A reward function is _instantaneous_ if its corresponding rewards are dense, rather than sparse, and are more readily indicative of the optimal action at any given state.4 Reward functions that are both instructive and instantaneous can alleviate issues associated with settings with sparse rewards and long horizons. To bias our bi-level optimization objective towards this type of reward function, we introduce a regularizer, \(_{}\). This regularizer favors solutions that can generate policies with high performance (i.e., high expected return \(J\) with respect to \(r_{p}\)) _even when the discount factor \(_{}\) is small_. To see why, first notice that this regularizer encourages behavior alignment reward functions that provide more instantaneous feedback to the agent. This has to be the case; otherwise, it would be challenging to maximize long-term reward should the optimized alignment reward function be sparse. Second, the regularizer promotes instructive alignment reward functions--i.e., functions that facilitate learning policies that maximize \(J\). This is equally crucial: effective policies under the metric \(J\) are the ones that align well with the designer's objectives as outlined in the original reward function, \(r_{p}\).

### Overcoming Imperfections of Policy Optimization Algorithms

The advantages of the bi-level formulation in (2) extend beyond robustness to sub-optimality from misspecified \(r_{}\). Even with a well-specified \(r_{}\), RL algorithms often face design choices, such as the bias-variance trade-off, that can induce sub-optimal solutions. Below we present examples to show how _bias_ in the underlying RL algorithm may be mitigated by carefully optimizing \(r_{}\) and \(_{}\).

**4.1.1 Bias in policy gradients:** Recall that the popular "policy gradient" \((,r_{p})\) is not, in fact, the gradient of any function, and using it in gradient methods may result in biased and sub-optimal policies . However, policy gradient methods based on \((,r_{p})\) remain vastly popular in the RL literature since they tend to be sample efficient . Let \(_{}(,r_{p})\) denote the _unbiased_ policy gradient, where \(_{}(,r_{p})[_{t=0}^{T}^{t} _{}(S_{t},A_{t})_{j=t}^{T}^{j-t}r_{p}(S_{j},A_{j})]\). We can show that with a sufficiently expressive parameterization, optimized \(r_{}\) and \(_{}\) can effectively mimic the updates that would have resulted from using the _unbiased_ gradient \(_{}(,r_{p})\), even if the underlying RL algorithm uses the "gradient", \(_{}(,,)\), as defined in (3). Detailed proofs are in Appendix A.

**Property 2**.: _There exists \(r_{}:\) and \(_{}[0,1)\) such that \(_{}(,,)=_{}(,r_{p})\)._

**4.1.2 Off-policy learning without importance sampling:** To increase sample efficiency when evaluating a given policy \(_{}\), it is often useful to use off-policy data collected by a different policy, \(\). Under the assumption that \( s, a,\,<\), importance ratios \(_{j}_{k=0}^{j}(s,a)}{(s,a)}\) can be used to adjust the updates and account for the distribution shift between trajectories generated by \(\) and \(_{}\). However, to avoid the high variance stemming from \(_{j}\), many methods tend to drop most of the importance ratios and thus only partially correct for the distribution shift--which can lead to bias . We can show (given a sufficiently expressive parameterization for the behavior alignment reward function) that this type of bias can also be mitigated by carefully optimizing \(r_{}\) and \(_{}\).

Let us denote the unbiased off-policy update with full-distribution correction as \(_{}(,r_{p})_{}[_{t=0}^{T} ^{t}_{}(S_{t},A_{t})_{j=t}^{T}_{j}^{j}r_{p}(S_{j},A_{j})]\). Now consider an extreme scenario where off-policy evaluation is attempted _without any correction for distribution shift_. In this situation, and with a slight abuse of notation, we define \(_{}(,,)_{}[_{t=0} ^{T}_{}(S_{t},A_{t})_{j=t}^{T}_{}^{j-t}r_{}(S_{j },A_{j})]\).

**Property 3**.: _There exists \(r_{}:\) and \(_{}[0,1)\) such that \(_{}(,,)=_{}(,r_{p})\)._

**Remark 1**.: _Our method is capable of mitigating various types of algorithmic biases and imperfections in underlying RL algorithms, without requiring any specialized learning rules. Additionally, thanks to the \(_{^{*}}\) regularization, it favors reward functions that lead to faster learning of high-performing policies aligned with the designer's objectives, as outlined in the original reward function \(r_{p}\)._

## 5 Barf i: Implicitly Learning Behavior Alignment Rewards

Having introduced our bi-level objective and discussed the benefits of optimizing \(r_{}\) and \(_{}\), an important question arises: Although \((,)\) can be optimized using any policy learning algorithm, how can we efficiently identify the optimal \(^{*}\) and \(^{*}\) in equation (2)? Given the practical advantages of gradient-based methods, one would naturally consider using them for optimizing \(^{*}\) and \(^{*}\) as well. However, a key challenge in our setting lies in computing \(J((,))/\) and \(J((,))/\). These computations require an analytical characterization of the impact that \(r_{}\) and \(_{}\) have on the _entire optimization process_ of the inner-level algorithm, Alg.

In addressing this challenge, we initially focus on an Alg that employs policy gradients for updating \(_{}\). Similar extensions for other update rules can be derived similarly. We start by re-writing the expression for \(J((,))/\) using the chain rule:

\[J((,))}{}=J((,))}{(,)}}_{(a)} (,)}{}}_{(b)},\] (4)

where _(a)_ is the policy gradient at \((,)\), and _(b)_ can be computed via implicit bi-level optimization, as discussed below.

**Implicit Bi-Level Optimization:** We compute (4) by leveraging implicit gradients [14; 34; 19], an approach previously employed, e.g., in few-shot learning [38; 49] and model-based RL algorithms . First, observe that when Alg converges to \((,)\), then it follows that

\[((,),,)=0.\] (5)

Let \( f\) denote the partial derivative with respect to the immediate arguments of \(f\), and \(f\) be the total derivative as before. That is, if \(f(x,g(x))xg(x)\), then \((x,g(x))=g(x)\) and \(f}{x}(x,g(x))=g(x)+x(x)\). Therefore, taking the total derivative of (5) with respect to \(\) yields

\[((,),,)}{}= +=0.\] (6)

By re-arranging terms in (6), we obtain the term _(b)_ in (4). In particular,

\[=-()^{-1} .\] (7)

Furthermore, by combining (7) and (4) we obtain the desired gradient expression for \(\):

\[=-(}_{})^{-1}}_{}.\] (8)

Similarly, a gradient expression for \(\) can be derived; the full derivation is detailed in Appendix E. Using \(^{*}\) as shorthand for \((,)\), we find that the terms \(\) and \(\) can be expressed as

\[=_{}[_{t=0}^{T}_{^{*}}(S_{ t},A_{t})(_{j=t}^{T}_{}^{j-t}(S_{j},A_{j})}{ })^{}]\!,\ =_{}[_{t=0}^{T}}(S_{t},A_{t})}{^{*}}(_{j=t}^{T}_{ }^{j-t}r_{(S_{j},A_{j})})].\]

When working with the equations above, we assume the inverse of \(^{-1}\) exists. To mitigate the risk of ill-conditioning, we discuss regularization strategies for Alg in Appendix D. Notice that equations (8) and (15) are the key elements needed to calculate the updates to \(\) and \(\) in our bi-level optimization's outer loop. However, computing \(\) and \(\) directly can be impractical for high-dimensional problems due to the need for outer products and second derivatives. To address this, we employ two strategies: _(1)_ We approximate \(^{-1}\) using the Neumann series , and _(2)_ we calculate (8) and (15) via Hessian-vector products , which are readily available in modern auto-diff libraries . These methods eliminate the need for explicit storage or computation of \(\) or \(\).

The mathematical approach outlined above results in an algorithm with linear compute and memory footprint, having \(O(d)\) complexity, where \(d\) is the number of parameters for both the policy and the reward function. Details can be found in Appendix C. We refer to our method as BARFI, an acronym for _behavior alignment reward function's implicit_ optimization.5 BARFI is designed to iteratively solve the bi-level optimization problem defined in (2). With policy regularization, the updates to \(r_{}\) and \(_{}\) incrementally modify \((,)\). This enables us to initialize Alg using the fixed point achieved in the previous inner optimization step, further reducing the time for subsequent inner optimizations.

## 6 Empirical Analyses

Our experiments serve multiple purposes and include detailed ablation studies. First, we demonstrate our bi-level objective's efficacy in discovering behavior alignment reward functions that facilitate learning high-performing policies. We focus especially on its robustness in situations where designers provide poorly specified or misaligned auxiliary rewards that could disrupt the learning process (Section 6.1). Second, we present a detailed analysis of the limitations of potential-based reward shaping, showing how it can lead to suboptimal policies (Section 6.2). We then provide a qualitative illustration of the behavior alignment reward function learned by BARFI (Section 6.3). Finally, we evaluate how well BARFI scales to problems with high-dimensional, continuous action and state spaces (Section 6.4).

In the sections that follow, we examine a range of methods and reward combinations for comparison:

* _Baseline RL methods_: We consider baseline RL methods that employ a naive reward combination strategy: \(_{}(s,a) r_{p}(s,a)+r_{}(s,a)\). In this case, the auxiliary reward from the designer is simply added to the original reward without checks for alignment. Both the REINFORCE and Actor-Critic algorithms are used for optimization.
* _Potential-based shaping_: To assess how well potential-based reward shaping performs, we introduce variants of the baseline methods. Specifically, we investigate the effectiveness of the reward function \(_{}(s,a,s^{}) r_{p}(s,a)+ r_{}(s^{ })-r_{}(s)\).
* BARFI: We use REINFORCE as the underlying RL algorithm when implementing BARFI and define \(r_{}(s,a)_{1}(s,a)+_{2}(s)r_{p}(s,a)+_{3}(s)r_{}(s,a)\). Our implementation includes a warm-up period wherein the agent collects data for a fixed number of episodes, using \(_{}\), prior to performing the first updates to \(\) and \(\) (See Appendix 5 for the complete algorithm).

We evaluate each algorithm across four distinct environments: GridWorld, MountainCar , CartPole , and HalfCheetah-v4. These domains offer increasing levels of complexity and are intended to assess the algorithms' adaptability. Furthermore, we examine their performance under a variety of auxiliary reward functions, ranging from well-aligned to misaligned with respect to the designer's intended objectives.

In our experiments, we investigate different types of auxiliary reward functions for each environment: some are action-dependent, while others are designed to reward actions aligned with either effective or ineffective known policies. These functions, therefore, vary in their potential to either foster rapid learning or inadvertently mislead the agent away from the designer's primary objectives, hindering the efficiency of the learning process. Comprehensive details of each environment and their corresponding auxiliary reward functions can be found in Appendix F.

### BARFI's Robustness to Misaligned Auxiliary Reward Functions

In this section, we evaluate the performance of various methods for reward combination, particularly in scenarios where auxiliary reward functions can either be well-aligned with a designer's intended goals or be misaligned or poorly specified, thus inadvertently hindering efficient learning. We introduce two types of auxiliary reward functions for CartPole. First, we used domain knowledge to design an \(r_{}\) that provides bonuses when the agent's actions align with a known effective policy in this domain. Second, we designed an adversarial example where the auxiliary reward function rewards actions that are consistent with a particularly poorly performing policy. For MountainCar, we first leveraged knowledge about an _energy pumping policy_ (i.e., a well-known effective policy ) to craft an auxiliary reward function that provides bonuses for actions in line with such a control strategy. We also experimented with a partially-aligned auxiliary function that rewards high velocity--a factor not particularly indicative of high performance.

Table 1 summarizes the results across different reward functions and combination methods. The results suggest that if auxiliary rewards provide positive feedback when agents' actions align with effective policies, naive combination methods perform well. In such cases, auxiliary rewards effectively "nudge" agents towards emulating expert actions. However, our experimental results also indicate that _all_ baseline methods are susceptible to poor performance when auxiliary rewards are not well aligned with the designer's goals. We provide more discussion for potential-based shaping in Section 6.2.

The key takeaway from the experimental results in Table 1 is that BARFI consistently performs well across various domains and under different types of auxiliary rewards. Specifically, when designer-specified feedback is appropriate and can assist in accelerating learning, BARFI efficiently exploits it to produce high-performing policies. Conversely, if auxiliary rewards are misaligned with the designer's intended goals, BARFI is capable of adapting and effectively dismissing "misleading rewards". This adaptability ensures that high-performing policies can be reliably identified. Other methods, by contrast, succeed only in some of these scenarios. Importantly, the unpredictability of whether a given auxiliary reward function will aid or hinder learning makes such alternative methods less reliable, as they may fail to learn effective policies.

### Pitfalls of potential-based reward shaping

We now turn our attention to the (possibly negative) influence of action-dependent auxiliary rewards, particularly when used in combination with potential-based reward shaping. Our results in Table 1 reveal a key limitation: potential-based shaping struggles to learn efficient policies even when auxiliary rewards are well-aligned with effective strategies. This shortcoming is attributable to the action-dependent nature of the auxiliary rewards, which compromises the potential shaping technique's guarantee of policy optimality.

As there is no prescribed way for designing potential shaping when \(r_{}\) is action-dependent, we use a direct extension of the original formulation  by considering \(_{}(s,a,s^{},a^{}):=r_{p}(s,a)+ r_{}( s^{},a^{})-r_{}(s,a)\). Furthermore, we designed an auxiliary reward function, \(r_{}\), that is well aligned: it provides positive reward signals of fixed magnitude both for \((s,a)\) and \((s^{},a^{})\) whenever the agent's actions coincide with the optimal policy. Notice, however, that if \(<1\), the resultant value from \( r_{}(s^{},a^{})-r_{}(s,a)\) is _negative_. Such a negative component may deter the agent from selecting actions that are otherwise optimal, depending on how \(r_{}\) and \(r_{p}\) differ in magnitude. Conversely, potential-based shaping can also occasionally perform well under misaligned rewards. In these cases, the shaping function may yield a _positive_ value whenever the agent selects an optimal action, which could induce well-performing behaviors.

    &  &  \\   & Well-aligned \(r_{}\) & Misaligned \(r_{}\) &  Well-aligned \(r_{}\) \\ (w.r.t. _energy policy_) \\  &  Partially-aligned \(r_{}\) \\ (w.r.t. _high velocity policy_) \\  \\    BARFI (_our method_) \\ \(_{}\) (_naive reward combination_) \\ \(_{}\) (_potential-based shaping_) \\  &  \(487.2 9.4\) \\ \(498.9 1.0\) \\ \(8.98 0.2\) \\  &  \(475.5 15.5\) \\ \(9.04 0.2\) \\ \(500 0.0\) \\  &  \(0.99 0.0\) \\ \(0.99 0.0\) \\ \(0.09 0.0\) \\  & 
 \(0.90 0.1\) \\ \(0.63 0.1\) \\ \(0.00 0.0\) \\  \\  

* BARFI’s performance compared to two baselines that use: \(_{}\) and \(_{}\), respectively. CartPole uses an action-dependent \(r_{}\) function that either rewards agents when actions align with a known effective policy (_well-aligned_\(r_{}\)) or with a poorly-performing policy (_misaligned_\(r_{}\)). MountainCar uses either an action-dependent function aligned with an energy-pumping policy  or a partially-aligned function incentivizing higher velocities. BARFI consistently achieves near-optimal performance across scenarios, even if given poorly specified/misaligned auxiliary rewards. Competitors, by contrast, often induce suboptimal policies. Performances significantly below the optimal are shown in red, above.

Table 1: Summary of the performance of various reward combination methods and types of \(r_{}\)

### What does \(r_{}\) learn?

We now investigate BARFI's performance and robustness in the GridWorld when operating under misspecified auxiliary reward functions. Consider the reward function depicted in Figure 2 [left]. This reward function provides the agent with a bonus for visiting the state at the center, akin to providing intermediate feedback to the agent when it makes progress towards the goal. However, such intermediate positive feedback can lead to behaviors where the agent repeatedly cycles around the middle state (i.e., behaviors that are misaligned with the original objective of reaching the goal state at the top right corner of the grid). Importantly, BARFI is capable of autonomously realizing that it should disregard such misleading incentives (Figure 2 [center left]), thereby avoiding poorly-performing behaviors that focus on revisiting irrelevant central states. Similarly, when Cartpole operates under a misspecified \(r_{}\) (Figure 2 [right]), BARFI is capable of rapidly adapting (after a warm-up period) and effectively disregarding misleading auxiliary reward signals. These results highlight once again BARFI's robustness when faced with reward misspecification.

### Scalability to High-Dimensional Continuous Control

One might wonder whether computing implicit gradients for \(\) and \(\) would be feasible in high-dimensional problems, due to the computational cost of inverting Hessians. To address this concern, we leverage Neumann series approximation with Hessian-vector products (See Appendix C) and conduct further experiments, as shown in Figure 3. These experiments focus on evaluating the scalability of BARFI in control problems with high-dimensional state spaces and continuous actions--scenarios that often rely on neural networks for both the policy and critic function approximators. For a more comprehensive evaluation, we also introduced an alternative method named BARFI unrolled. Unlike BARFI, which uses implicit bi-level optimization, BARFI unrolled employs path-wise bi-level optimization. It maintains a complete record of the optimization path to determine updates for \(\) and \(\). Further details regarding this alternative method can be found in Appendix C.6.

We conducted experiments on the HalfCheetah-v4 domain and investigated, in particular, a reward function comprising two components with varying weights. This empirical analysis was designed to help us understand how different weight assignments to each reward component could influence the learning process. Specifically, in HalfCheetah-v4, the agent receives a positive reward \(r_{p}\) proportional to how much it moved forward. It also incurs a small negative reward (concretely, an auxiliary reward, \(r_{}(s,a):=c\|a\|_{2}^{2}\), known as a _control cost_) for the torque applied to its joints. A hyperparameter \(c\) determines the balance between these rewards. The naive combination of such primary and auxiliary rewards is defined as \(_{}(s,a)=r_{p}(s,a)+r_{}(s,a)\). Figure 3 [left] shows that the baselines and both variants of BARFI appear to learn effectively. With alternative reward weighting schemes, however, only BARFI and BARFI unrolled show learning progress, as seen in Figure 3 [middle]. It is worth noting that path-wise bi-level optimization can become impractical as the number of update steps in (4) increases, due to growing computational and memory requirements (Figure 3 [right]). Although we do not recommend BARFI unrolled, we include its results for completeness. Additional ablation studies on _(a)_ the effect of the inner optimization step; _(b)_ Neumann approximations; _(c)_ decay of \(\); and _(d)_ returns based on \(r_{}\), are provided in Appendix H.

Figure 2: **(Left)** Misaligned \(r_{}\) (+50) for entering the center state and \(r_{p}\) (+100) at the goal state. (**Center Left**) Optimized weight \(_{3}(s)\) depicting how the learned reward function penalizes the agent for entering the center state. **(Center Right)** Performances in the GridWorld domain under a misspecified \(r_{}\). **(Right)** Performances in the CartPole domain under a \(r_{}\) providing positive feedback for mimicking a known ineffective policy.

## 7 Related work

This paper focuses primarily on how to efficiently leverage auxiliary rewards \(r_{}\). Notice, however, that in the absence of \(r_{}\), the resulting learned behavior alignment rewards \(r_{}\) may be interpreted as _intrinsic rewards_[70; 71]. Furthermore, several prior works have investigated meta-learning techniques, which are methods akin to the bi-level optimization procedures used in our work. Such prior works have employed meta-learning in various settings, including automatically inferring the effective return of trajectories [68; 62; 7; 71], parameters of potential functions [72; 28; 17], targets for TD learning , rewards for planning [54; 23], and even fully specified reinforcement learning update rules [33; 45]. Additionally, various other relevant considerations to effectively learning rewards online have been discussed by Armstrong et al. . Our work complements these efforts by focusing on the reward alignment problem, specifically in settings where auxiliary information is available. An extended discussion on related works can be found in Appendix B. It is worth mentioning that among the above-mentioned techniques, most rely on path-wise meta-gradients. As discussed in Section 6.4, this approach can be disadvantageous as it often performs only one or a few inner-optimization steps, which limits its ability to fully characterize the result of the inner optimization . Further, it requires caching intermediate steps, which increases computational and memory costs. BARFI, by contrast, exploits implicit gradients to alleviate these issues by directly characterizing the fixed point of Alg induced by learned behavior alignment rewards.

Finally, it is also important to highlight that a concurrent work on reward alignment using bi-level optimization was made publicly available after our manuscript was submitted for peer-reviewing at NeurIPS . While our work analyses drawbacks of potential-based shaping and establishes different forms of correction that can be performed via bi-level optimization, this concurrent work provides complementary analyses on the convergence rates of bi-level optimization, as well as a discussion on its potential applications to Reinforcement Learning from Human Feedback (RLHF).

## 8 Conclusion and Future Work

In this paper, we introduced BARFI, a novel framework that empowers RL practitioners--who may not be experts in the field--to incorporate domain knowledge through heuristic auxiliary reward functions. Our framework allows for more expressive reward functions to be learned while ensuring they remain aligned with a designer's original intentions. BARFI can also identify reward functions that foster faster learning while mitigating various limitations and biases in underlying RL algorithms. We empirically show that BARFI is effective in training agents in sparse-reward scenarios where (possibly poorly-specified) auxiliary reward information is available. If the provided auxiliary rewards are determined to be misaligned with the designer's intended goals, BARFI autonomously adapts and effectively disincentivizes their use as needed. This adaptability results in a reliable pathway to identifying high-performing policies. The conceptual insights offered by this work provide RL practitioners with a structured way to design more robust and easy-to-optimize reward functions. We believe this will contribute to making RL more accessible to a broader audience.

Figure 3: Results for MuJoCo environment. (**Left**) Auxiliary reward is defined to be \(-c\|a\|_{2}^{2}\), where \(c\) is a positive hyperparameter and \(a\) is the continuous high-dimensional action vector. (**Middle**) Similar setting as before, but uses an amplified variant of the auxiliary reward: \(-4c\|a\|_{2}^{2}\). It is worth highlighting that even under alternative reward weighting schemes, both variants of our (behavior-aligned) bi-level optimization methods demonstrate successful learning. Learning curves correspond to mean return over 15 trials, and the shaded regions correspond to one standard error. (**Right**) Required compute and memory for BARFI unrolled, compared to BARFI, as a function of the number of inner-optimization updates. This figure also showcases BARFI’s characteristics under various orders of Neumann approximation.