# Benchmarking Foundation Models with Language-Model-as-an-Examiner

Yushi Bai\({}^{1}\)1, Jiahao Ying\({}^{2}\)1, Yixin Cao\({}^{2}\), Xin Lv\({}^{1}\), Yuze He\({}^{1}\),

**Xiaozhi Wang\({}^{1}\), Jifan Yu\({}^{1}\), Kaisheng Zeng\({}^{1}\), Yijia Xiao\({}^{3}\), Haozhe Lyu\({}^{4}\), Jiayin Zhang\({}^{1}\), Juanzi Li\({}^{1}\), Lei Hou\({}^{1}\)\({}^{1}\)**

\({}^{1}\)Tsinghua University, Beijing, China \({}^{2}\)Singapore Management University, Singapore

\({}^{3}\)University of California, Los Angeles, CA, USA

\({}^{4}\)Beijing University of Posts and Telecommunications, Beijing, China

Equal contribution

###### Abstract

Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reliable result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: http://lmexam.xlore.cn.

## 1 Introduction

Recently, many large foundation models , such as ChatGPT , LLaMA , and PaLM , have emerged with impressive general intelligence and assisted billions of users worldwide. For various users' questions, they can generate a human-like response. However, the answers are not always trustworthy, e.g., hallucination . To understand the strengths and weaknesses of foundation models, various benchmarks have been established [6; 7; 8; 9; 10].

Nevertheless, we see two main hurdles in existing benchmarking methods, as summarized below. **(1) Testing leakage**. Along with increasing tasks and corpus involved in pre-training, the answer to the testing sample may have been seen and the performance is thus over-estimated. **(2) Evaluation automation**. Evaluating machine-generated texts is a long-standing challenge. Thus, researchers often convert the tasks into multi-choice problems to ease the quantitative analysis. This is clearly against real scenarios -- as user-machine communications are mostly open-ended Question Answering (QA) or freeform QA . On the other hand, due to the existence of a vast number of valid "good" answers, it is impossible to define one or several groundtruth, making similarity-based matchingmeasurements (e.g., Exact Match, ROUGE-L , and BERTScore ) ineffective [11; 14; 15]. Therefore, recent works target a well-trained evaluator language model (LM) to assess the answer quality in a reference-free manner [16; 17; 18]. However, using LM as an evaluator also presents a problem: What if the evaluator hallucinates and makes wrong judgments during assessment?

As an attempt, our pilot study utilizes GPT-4  to evaluate the correctness of LLaMA  on _Natural Questions_, where non-negligible \(18\) out of \(100\) judgments are incorrect (cases in Appendix A). We attribute the main reason to the inadequate knowledge of the evaluator itself regarding the questions. A straightforward solution is to use the LM not just as an evaluator to assess the responses, but as a knowledgeable examiner to also formulate questions, which is guaranteed a thorough understanding of the judgments. And, it naturally addresses the testing leakage issue by generating new questions periodically. Yet, relying on a centralized examiner can hardly be considered fair, especially when evaluating the examiner itself -- _A man who is his own lawyer has a fool for his client_.

In this paper, we propose a novel benchmarking framework, _Language-Model-as-an-Examiner_, to assess current foundation models, mitigating the aforementioned issues. Herein, the language model acts as a knowledgeable examiner that poses questions based on its inherent knowledge and evaluates others on their responses. We devise three strategies to alleviate potential bias:

* **Increasing Knowledge Breadth and Depth**. In terms of breadth, according to a predefined taxonomy, we select as many diverse domains as possible to generate questions. In terms of depth, to probe models deeply within a specific subfield, we propose a multi-round setting where the evaluator mimics an interviewer, posing more sophisticated follow-up questions based on the interviewee model's preceding responses. We release our dataset, namely LMExamQA, which is constructed using GPT-4  as an examiner.
* **Reliable Evaluation Measurement**. We explore two evaluation metrics, namely Likert scale scoring and Ranking, offering a more comprehensive evaluation result. The results from both metrics correlate closely with human annotations, significantly outperforming all previous metrics.
* **Peer-examination Mechanism**. To avoid the potential bias arising from a single model as examiner, we propose a decentralized evaluation setting where all participating models are invited to be the examiner and assess each other.

In experiments, our benchmarking pipeline yields fruitful results on 8 popular foundation models. We also demonstrate that peer-examination can generate a more diverse set of questions for knowledge probing and balance the biases from individual evaluator models, ultimately leading to a more equitable evaluation outcome.

Figure 1: Overview of our benchmarking method. The left part shows the use of language model as an examiner. The examiner generates questions from various domains, allowing it to probe for comprehensive understanding (knowledge breadth) as well as deep specialization (knowledge depth) through follow-up questions (FQs). It then scores and ranks other modelsâ€™ responses according to its understanding of the subject, providing a reliable evaluation. The right part presents peer-examination, a novel decentralized method that provides fairer evaluation results, which potentially demands higher workload of running multiple LM examiners, compared to running a single LM examiner.

Related Work

**Benchmarks for Foundation Models**. Various benchmarks have been proposed to assess foundation models on open-ended question answering, since it is the most natural setting for user-machine interaction in real scenarios. Some prominent such benchmarks include MS MARCO , SQuAD [22; 23], Natural Questions , WebQuestions  and OpenBookQA . On the other hand, there exist a limited number of datasets that feature long-form QA. One of the widely-recognized examples is ELJS , which comprises questions that necessitate lengthy descriptive and explanatory answers. One notable limitation of these benchmarks is their reliance on human curation and annotation, which inherently constrains their scalability. Our approach, by comparison, utilizes LMs to construct datasets, offering the advantage of effortless extensibility.

**Automating NLG Evaluation**. To evaluate machine-generated responses to the questions, several automatic metrics have been adopted, including the F1 score, Exact Match (EM), BLEU , ROUGE , and METEOR . However, each metric has its own shortcomings, resulting in large discrepancies between the tested and actual performance [14; 29; 30].

To address these issues, well-trained LMs are utilized in NLG evaluation [31; 32; 33; 34]. One mainstream of previous methods is _reference-based_, where they derive the similarity between the candidate and the reference using an LM. Some prominent metrics in this class include MoverScore , BERTScore . These metrics measure the distributional similarity rather than lexical overlap , making them appropriate for contexts that require more flexible generation. Recent studies [16; 17; 18; 37; 38; 39; 40; 41] have demonstrated that large language models (LLMs), such as ChatGPT , can conduct NLG evaluations in a _reference-free_ manner. They can rate a candidate text (or perform a comparative assessment of two candidates) based on a specified evaluation aspect, displaying a high correlation with human assessments in tasks such as summarization and story generation [42; 43]. In these studies, the evaluations primarily focus on lexical quality aspects, such as coherence and fluency, of a generated text. However, their capability to evaluate crucial aspects in a QA response, including factual correctness and information comprehensiveness, remains uncertain. Moreover, a single evaluator inevitably brings bias to the assessment . Our work aims to resolve these issues by leveraging LM not just as an evaluator but also as an examiner, assessing the performance of other models through self-generated questions, and deploying multiple LM examiners to ensure balanced evaluation.

## 3 Methodology

In this section, we discuss the methodology in language-model-as-an-examiner, including the LMExamQA dataset construction, the evaluation metric design, and the peer-examination pipeline.

### Dataset Construction

**Question Generation towards Knowledge Breadth**. We employ a language model (LM) as an examiner that generates diversifying and high-quality questions across various domains. To ensure wide coverage of knowledge, we choose the Google Trends Categories 2 as the domain taxonomy, and randomly select \(n\) domains from it. For each domain, we prompt the LM to generate \(m\) distinct questions. Our designed prompt (shown in Appendix B) is formulated to ensure that the generated questions possess three essential characteristics: diversified question forms, varied cognitive levels, and most importantly, assurance that the LM has a comprehensive understanding of the knowledge surrounding the question it poses. Figure 2 shows the distribution of question forms based on their interrogative words, and the distribution of question domains. According to Bloom's taxonomy , we divide the questions into 3 categories based on their required cognitive levels, from low to high-level, namely _knowledge memorization_, _knowledge comprehension_, and _knowledge analysis_:

* **Knowledge memorization**. Questions of such level demand recognition or recollection of certain entities and attributes, such as a person, location, or time.
* **Knowledge comprehension**. These questions involve demonstrating an understanding of particular instances or concepts, such as "What is \(\)", "Why \(\)", and "How \(\)".

* **Knowledge analysis**. Questions of this type require more advanced cognitive skills and they typically question the impact, comparison, or advantages and disadvantages of a given topic.

By adopting GPT-4 to categorize the questions in LMExamQA and previous open-ended QA datasets into three levels 3, we obtain the distribution with respect to the 3 cognitive levels as listed in Table 1, and show an example for each type of question. Compared with previous datasets, LMExamQA achieves a more balanced distribution across these 3 levels, thus providing a means of quantifying foundational models' proficiency at each cognitive level. Furthermore, LMExamQA includes a larger proportion of questions classified within higher cognitive levels, particularly at the analysis level, indicating a greater level of challenge.

To justify the reliability of the LM examiner as an evaluator on these questions, we employ it to produce a groundtruth answer with the prompt, "Answer the questions accurately and completely, without providing additional details." Upon evaluation by human experts on a random selection of 100 questions, the answers offered by the LM exhibit a \(100\%\) accuracy rate, thereby demonstrating mastery over the questions it generates.

**Multi-round Follow-up Question Generation towards Knowledge Depth**. To further probe the model's comprehension of a topic in depth, we develop an evaluation procedure involving multiple rounds of follow-up inquiries, drawing inspiration from the interview process. We utilize the LM examiner to construct a series of follow-up inquiries (prompt is shown in the Appendix B). These follow-up questions are specifically tailored to delve deeper into the concepts presented within the model-generated answers from the previous round. As the follow-up questions are dependent on the model's generated answers, we only ask follow-up questions for the correctly answered queries (determined by the LM examiner) and calculate the proportion of correct responses in the subsequent round. We limit the total number of rounds to \(k\) in order to minimize topic deviation that might occur during longer sessions. Note that we only provide the interviewee model with the follow-up question as input, rather than engaging the "exam history" 4, since most models are not capable of multi-round conversations. We show an example of a follow-up question to Flan-T5 :

    & & MS  & SQuAD2.0  & NO  & ELIS  & Ours & Example questions in our dataset \\   &  &  &  &  &  &  & What are the potential short and long-term \\  & & & & & & & & impacts of divorce on childcare? \\   & **Comprehension** & & & & & & & & \\   & **memorization** & & & & & & & & \\   

Table 1: Proportions of each level of questions. MS and NQ are short for MS MARCO and Natural Questions. We also list an example question in LMExamQA for each category.

Figure 2: Statistics of generated questions in LMExamQA.

**Question:** Which material is primarily used to manufacture semiconductor devices?

**Flan-T5:** Silicon **V**

**Follow-up Question:** What are the advantages of using silicon as the primary material for semiconductor devices?

**Flan-T5:** Silicon is a nonrenewable resource, and it is the most abundant element on Earth. **X**

### Evaluation Metrics

Several methodologies are commonly employed to facilitate human-like evaluation in LMs, prominent among these are the Likert scale scoring [16; 17; 41] and pairwise comparison [38; 41]. For the purposes of our benchmark, we incorporate both Likert scale scoring and a variant of pairwise comparison, namely ranking.

Likert scale scoring functions as an absolute evaluative measure, where the evaluator assigns scores to a given response along predefined dimensions. We establish four distinct dimensions on our dataset: (1) Accuracy. This assesses the extent to which the provided response accurately answers the question. (2) Coherence. This evaluates the logical structure and organization of the response and the degree to which it can be comprehended by non-specialists. (3) Factuality. This examines whether the response contains factual inaccuracies. (4) Comprehensiveness. This gauges whether the response encompasses multiple facets of the question, thus providing a thorough answer. Each of these dimensions is scored on a scale of 1 to 3, ranging from worst to best. We also ask the evaluator to provide an overall score ranging from 1 to 5, based on the scores assigned to the previous 4 dimensions. This score serves as an indicator of the overall quality of the answer.

On the other hand, pairwise comparison operates as a relative evaluation method and is often more discerning compared to scoring. In this process, evaluators are given two responses and are tasked with determining which is superior, taking into account their accuracy, coherence, factuality, and comprehensiveness. Given that there are \(n\) contestant models, we implement a merge sort algorithm to _rank_ the \(n\) responses, involving \((n n)\) pairwise comparisons.

### Decentralized Evaluation: Peer-Examination

We introduce a novel decentralized method that incorporates multiple models to serve as examiners, namely Peer-examination (illustrated in the right part of Figure 1), since relying only on one centralized model as the examiner introduces the following potential drawbacks to the benchmarking process. **(1) Coverage of generated questions**: The examiner may not have a holistic understanding of certain domain knowledge. As a result, the examiner may struggle to propose questions that examine in detail on these areas, which in turn renders the scope of generated questions insufficient. **(2) Potential bias during evaluation**: The model itself may have a bias during evaluation. The bias can manifest as a preference for certain types of responses or a predisposition towards perspectives irrelevant to the quality of the responses, such as response length or linguistic style. For example,  shows that GPT-4  prefers ChatGPT  summaries compared to human-written summaries. Such biases may result in unfair ranking assessment outcomes.

To mitigate these issues, during peer-examination, each model is assigned the role of an examiner separately. As examiners, they are responsible for posing questions and evaluating the answers provided by the other models. We then combine the evaluation results from each of these models by voting, and obtain a final result. This approach leverages the collective expertise and diverse perspectives of all models to improve the coverage of questions as well as ensure fairer assessments.

## 4 Experiments

To demonstrate the effectiveness of our Language-model-as-an-examiner framework, we first employ GPT-4  as the examiner for a centralized evaluation, since it exhibits a broad understanding of knowledge [9; 49; 50] and a precise judgmental ability [16; 17]. In peer-examination, we also employ Claude (Claude-instant) , ChatGPT , Bard , and Vicuna-13B  as LM examiners.

### Metric Evaluation

To verify the reliability of our method for scoring and comparison based assessment, we perform metric evaluation. We conduct human evaluations on machine-generated responses. These evaluations are quantified using a 1-5 Likert scale for the overall score, and we let annotators to rank different responses for each question based on their holistic quality. We collect \(300\) annotations across \(100\) questions from LMExamQA. For each question, we randomly select \(3\) of the model responses, and obtain \(3\) scoring annotations and \(3\) pairwise comparison results. For Likert scoring, we calculate Spearman's \(\) and Kendall's \(\) between the overall scores given by the automatic metrics and human experts; for ranking, we compute the accuracy of pairwise comparisons offered by the automatic metrics, according to the human-labeled comparison results. Then we compare the LM examiner, GPT-4 , with previous automatic metrics, including ROUGE-1, ROUGE-2, ROUGE-L , BLEU , BERTScore  (F1), and report their correlation with human judgments in Table 2. We observe that employing GPT-4  as an examiner results in a much higher correlation with human annotations compared to prior metrics. More profoundly, GPT-4's pairwise comparison achieves an agreement of over 85% with human's.

### Centralized Benchmarking Results

**Experiment Setup**. We conduct a centralized benchmarking with GPT-4  as the examiner. Following the method in Section 3, we construct the LMExamQA dataset with GPT-4, where we set \(n=1,000\) domains and \(m=10\) questions for each domain, resulting in a total of \(10,000\) questions. We evaluate 8 popular and open-access foundation models on our LMExamQA dataset, including BLOOMZ (the 176B model) , Flan-T5 (the XXL model, 11B) , Flan-UL2 (20B) , GLM-130B , LLaMA (the 13B model and the 65B model) , Vicuna-13B , and ChatGPT . These models are categorized based on their training procedure: whether they have undergone Supervised Fine-Tuning (SFT) or not. The first 6 models are trained without SFT 5, whereas the last 2 models are fine-tuned. For models without SFT, we assess their \(0\)-shot and \(5\)-shot performance. During generation, for the examiner and the subject models, we set the temperature to \(0\) for reproducibility. More details for reproducing our results are shown in Appendix C.2.

**Single-round QA for Knowledge Breath**. Table 3 presents the percentage of full-mark answers for each model on LMExamQA. Full-mark answers are defined as responses that receive a rating of 5 on the overall score, and the proportion of such responses is reported for each category of questions. Additionally, Figure 3 provides a radar plot depicting the average scores of models on 5 dimensions;We also conduct ranking evaluation over the 8 models (we only show the few-shot performance for models without SFT). In Figure 4, we visualize the ranking results via a win-rate heatmap (the \((i,j)\)-th entry denotes the fraction of model \(i\) wins when compared against model \(j\)) along with each model's average win-rate against all other models. We summarize our key findings.

**1. The scaling law on LMEXamQA.** LLaMA-65B significantly outperforms LLaMA-13B across all question categories, adhering to the scaling law of LMs .

**2. Few-shot leads to more substantial improvement on higher cognitive-level questions.** For models without SFT, we observe that 5-shot examples yield an average relative improvement of 17%, 123%, and 206% on memorization, comprehension, and analysis type questions, respectively. This implies that the model may possess adequate knowledge to answer higher-level questions (e.g., distinguishing between two concepts). However, it may lack the ability to retrieve knowledge from its memory and structure appropriate language to form an answer. Few-shot examples serve to provide demonstrations on how to answer such questions.

**3. What does SFT offer?** We notice a huge performance gap between LLaMA-13B and Vicuna-13B (Vicuna-13B is fine-tuned on LLaMA-13B with 70k user-shared ChatGPT conversations), mainly on the latter two types of questions. This result suggests that SFT primarily plays a crucial role in aligning LM's responses for task adaptation, rather than enriching the model's knowledge -- especially in the context of higher-level questions that demand more sophisticated answers.

**4. LLMs can provide factually correct and coherent responses, but struggle for more comprehensive accurate answers.** The radar plot reveals that all models achieve relatively high scores concerning factuality and coherence (over 80/100), but different models vary widely in terms of comprehensiveness, i.e., whether the response addresses all aspects of a question.

**5. Ranking results interpretation.** Fine-tuned models, including Vicuna and ChatGPT, demonstrate near-perfect performance in terms of their scores (Table 3). In our dataset, ranking proves to be a more discerning evaluation approach. For example, the win-rate heatmap 4 reveals that ChatGPT outperforms Vicuna-13B with a 68% win rate, indicating a notable difference in the quality of responses generated by the two models. A ranking or comparison based evaluation is rarely used in QA evaluation, we encourage the research community to adopt more deliberate evaluation techniques in benchmarking more advanced foundation models on open-ended QA.

**Multi-round QA for Knowledge Depth**. To conduct the multi-round QA, we randomly select 1,000 question-and-answer from the full mark answers in the first round. We then engage the examiner GPT-4 to generate the second-round question and ask the examinee models to answer the second round questions. We limit the number of rounds to \(k=2\) due to the high cost of API usage. The evaluation results are presented in the last column of Table 3. We observe that excluding ChatGPT and Vicuna-13B, all examinee models exhibit a notable decrease in the second round. This suggests that while these models initially demonstrated a robust understanding and knowledge base, their performance deteriorated when faced with more complicated questions, highlighting the importance of conducting more in-depth evaluations during QA to thoroughly assess the models' capabilities. We provide more insights on the experimental results in Appendix C.2.

### Peer-Examination Results

For the Peer-examination process, we choose four prominent models, including ChatGPT , Claude , Vicuna-13B , Bard , which are carefully selected based on their capabilities to generate questions and assess NLG quality. Each of these models is assigned the role of an

Figure 4: Win-rate heatmap under GPT-4 as an examiner.

examiner, posing 100 questions6 according to the given 20 domains and evaluate the remaining three models' responses. We show the scoring results in Table 4 and the pairwise comparison results in Figure 5 (more experimental details are shown in Appendix C.3). We observe the overall rank, from highest to lowest, as follows: Claude, ChatGPT, Bard, and Vicuna-13B. Intriguingly, this aligns with the rank obtained from the popular leaderboard using the Elo rating system . Our approach differs as we utilize LMs as evaluators instead of human judges.

### Bias Analysis: Centralized vs Decentralized

We identify two potential biases in a centralized examination: one that originates from biases inherent in the questions generated by the model, and the other one rooted in the model's evaluation process.

**Bias in Generated Questions**. To analyze the bias in the generated questions, we employ t-SNE to visualize the distributions of questions across three datasets: LMExamQA, Natural Questions , and SQuAD2.0 . These questions are encoded into 1,536-dimensional vectors using the OpenAI text-embedding model, text-embedding-ada-002 . As shown in the left figure in Figure 6, we randomly select 1,000 questions on each dataset and visualize their respective t-SNE embeddings.

Through the embedding visualization, we observe that the questions in our LMExamQA dataset exhibit a more uniform distribution compared to those in previous datasets. Furthermore, we utilize 4 different LMs to generate questions across 20 domains and depict their respective question embeddings in the right panel of Figure 6. As we expected, questions within the same domain cluster together. More notably, questions produced by different models exhibit distinct distributions around the central region of the domain cluster, indicating potential biases in questions generated by a single LM. This observation motivates our adoption of peer-examination that harnesses multiple models to generate a diverse and comprehensive set of questions.

**Bias in Evaluation**. To explore potential bias in the evaluation process, we devise a bias detection experiment to test whether the centralized examiner GPT-4 exhibits a preference for a particular linguistic style. This was achieved by having GPT-4 compare a pair of responses that were identical

Figure 5: Win-rate heatmap under different LMs as examiners.

Figure 6: t-SNE on query embeddings. Left figure visualizes the embeddings of questions (generated by a centralized GPT-4 examiner) in LMExamQA; Right figure shows the embeddings of questions generated by 4 peer examiners.

in content but varied in linguistic style. Specifically, we employ ChatGPT to rephrase the responses from Vicuna-13B. To ensure quality equivalence after paraphrasing, we ask human annotators to select 100 rewritten responses that mirror the quality of the original responses. We then present these pairs to GPT-4 for a comparative evaluation, and the results of their respective win-rate are as shown in the "GPT-4" bar in Figure 7. The results indicate that GPT-4 favor responses rephrased by ChatGPT, suggesting a potential bias towards ChatGPT style responses during the evaluation process.

To investigate whether the observed bias in the centralized examination can be mitigated using peer-examination, we incorporate two models, namely Bard  and Claude . We instruct these models to compare the two responses, and show the results in Figure 7. The results reveal that different models possess distinct preferences. As a solution, combining them within a peer-examination framework can balance their individual biases (the rightmost bar), and lead to a more equitable evaluation process.

### Measuring Data Leakage in Model-generated Questions

The use of model-generated questions in our framework potentially retains the risk of data leakage because the generated content by the models (e.g., the generated questions) may replicate what they have seen during pretraining. Several studies have directly and indirectly demonstrated LLMs are capable of generating creative content instead of mere replication. For instance, experiments in the GPT-2 paper  revealed that the median 8-gram overlap rates between GPT-2's outputs and the exact completions from the WebText test set articles were a mere 2.6%. Moreover, a line of research such as Self-Instruct  and Alpaca  demonstrates that fine-tuning models using LM-generated instructions can significantly enhance their performance. This demonstrates that under appropriate prompts, LMs are capable of generating creative content instead of replicating text encountered during training.

Nevertheless, we provide an analysis of the potential data leakage in questions generated by LM on our LMExamQA dataset. The ideal method to verify whether an LLM has encountered similar questions during training is to investigate its training data. However, the training data for most language models, often sized at several terabytes, is not openly accessible. Consequently, we investigate two primary sources of training data instead -- web data and public datasets. Considering web data, we perform Google search on 100 randomly sampled questions from LMExamQA and retrieve the most similar web queries. For public datasets, we look into the Flan collection , which contains 1,836 tasks and 15M instances, and is widely used during instruction fine-tuning for LLMs. For each question in LMExamQA, we implement a BM25 search over the inputs from the Flan collection. Subsequently, we compute the ROUGE-L score between the question and the retrieved query. We find an average ROUGE-L score of \(0.293\) and \(0.046\) between the LM-generated question and the top retrieved data from the web and Flan collection, respectively. The low similarity scores in both data sources imply that the majority of the questions generated by the models are not present in the two sources. We also show 3 random questions along with their corresponding queries retrieved from web data. We can see that the model adds more qualifiers and determiners to the questions it generates than the data they've seen during pretraining, where memorizing and retelling from pretraining data is clearly not enough to answer these more precise, complex questions.

 Examiner & Claude & ChatGPT & Bard & Vicuna & AVG / AVG\({}_{}\) \\  Claude  & - & 98 & 100 & 96 & 98.0 / 99.7 \\ ChatGPT  & 41 & - & 100 & 95 & 78.6 / 98.9 \\ Bard  & 41 & 99 & - & 92 & 77.3 / 97.8 \\ Vicuna  & 42 & 98 & 99 & - & 79.6 / 99.3 \\  

Table 4: Percentage (%) of full-mark answer from Peer-Examination. AVG is the mean score given by the three other examiners. AVG\({}_{}\) is the mean of the scaled scores, wherein the highest score within each column is adjusted to 100 for standardization purposes.

Besides, human-collected, fixed datasets fall short in adapting to future scenarios where more pre-training data, even including data that overlaps with existing datasets, is used. With our approach, overlap in the dataset can be seamlessly addressed by re-generating the dataset via more sophisticated prompt engineering.

## 5 Conclusion

In this paper, we propose Language-Model-as-an-Examiner to address the difficulties in open-ended QA benchmarks. We construct the LMExamQA dataset, which aims to probe a more comprehensive and stratified understanding of knowledge. The dataset is equipped with a reliable language model examiner and we use it to benchmark several widely-used foundational models. To ensure a fair evaluation process, we devise a peer-examination pipeline. This approach serves to mitigate potential biases that might arise from the reliance on a single examiner. Expanding our benchmarking framework to incorporate more domain-specific language models, or even vision language models, could potentially offer a more holistic evaluation.

## 6 Limitation

Our proposed framework, despite offering a more streamlined approach for benchmarking foundation models, is not without its shortcomings. We summarize into two limitations. The first lies in the potential bias during evaluation. As we have elaborated in the paper, different models have different preferences towards distinct linguistic styles. They may also possess biases along other dimensions such as radical bias or gender bias. It's crucial to recognize these biases in future works as they might influence and be reflected in the evaluation results. Another limitation involves a lack of robust evaluation capability among existing foundation models to facilitate large-scale peer-examination. In our work, we identify only four current accessible models that demonstrate the required capacity to assess machine-generated text. We foresee in the near future that the emergence of more powerful foundation models will empower the peer-examination method, enabling more sophisticated and expansive evaluations.

**Ethical Consideration**. Creating a QA dataset using a large language model such as GPT-4 involves several ethical considerations. Here are some considerations to take into account:

* Data Privacy: Since the generated contents by LLMs may include personal information, it is crucial to ensure the anonymity of the data and protect sensitive personal information. We do not observe such information leakage in our LMExamQA dataset.
* Misinformation and Harmful Content: It is vital to ensure that the LLM generated dataset does not include or encourage misinformation, hate speech, or any form of harmful content. A rigorous review of the LMExamQA dataset assures us that such content does not appear.
* Fairness and Bias: Large language models, can unintentionally inherit and amplify societal biases present in the training data. It is important to put significant effort into identifying and mitigating such biases, as we illustrated in previous limitations.