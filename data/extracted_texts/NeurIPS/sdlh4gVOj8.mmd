# On Sample-Efficient Offline Reinforcement Learning:

Data Diversity, Posterior Sampling, and Beyond

 Thanh Nguyen-Tang

Johns Hopkins University

Baltimore, MD 21218

nguyent@cs.jhu.edu

&Raman Arora

Johns Hopkins University

Baltimore, MD 21218

arora@cs.jhu.edu

###### Abstract

We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to _unify_ three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve _comparable_ sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sample complexity of the RO-based algorithm compared to the VS-based algorithm, whereas posterior sampling is rarely considered in offline RL due to its explorative nature. Notably, our proposed model-free PS-based algorithm for offline RL is _novel_, with sub-optimality bounds that are _frequentist_ (i.e., worst-case) in nature.

## 1 Introduction

Learning from previously collected experiences is a vital capability for reinforcement learning (RL) agents, offering a broader scope of applications compared to online RL. This is particularly significant in domains where interacting with the environment poses risks or high costs. However, effectively extracting valuable policies from historical datasets remains a considerable challenge, especially in high-dimensional spaces where the ability to generalize across various scenarios is crucial. In this paper, our objective is to comprehensively examine the efficiency of offline RL in the context of (value) function approximation. We aim to analyze this within the broader framework of general data collection settings.

The problem of learning from historical datasets for sequential decision-making, commonly known as _offline RL_ or _batch RL_, originated in the early 2000s (Ernst et al., 2005; Antos et al., 2006; Lange et al., 2012) and has recently regained significant attention (Levine et al., 2020; Uehara et al., 2022). In offline RL, where direct interaction with environments is not possible, our goal is to learn an effective policy by leveraging pre-collected datasets, typically obtained from different policies known as _behavior policies_. The sample efficiency of an offline RL algorithm is measured by the sub-optimality of the policies it executes compared to a "good" comparator policy, which may or may not be an optimal policy. Due to the lack of exploration inherent in offline RL, designing an algorithm with low sub-optimality requires employing the fundamental principle of _pessimistic extrapolation_. This means that the agent extrapolates from the offline data while considering the worst-case scenariosthat are consistent with that data. Essentially, the _diversity_ present in the offline data determines the agent's ability to construct meaningful extrapolations. Hence, a suitable notion of data diversity plays a crucial role in offline RL.

To address the issue of data diversity, several prior methods have made the assumption that the offline data is _uniformly diverse_ - this implies that the data should cover the entire trajectory space with some probability that is bounded from below (Munos and Szepesvari, 2008; Chen and Jiang, 2019; Nguyen-Tang et al., 2022). This assumption is often too strong and not feasible in many practical scenarios. In more recent approaches (Jin et al., 2021; Xie et al., 2021; Uehara and Sun, 2022; Chen and Jiang, 2022; Rashidinejad et al., 2023), the stringent assumption of uniform diversity has been relaxed to only require _partial_ diversity in the offline data. Various measures have been proposed to capture this partial diversity, such as single-policy concentrability coefficients (Liu et al., 2019; Rashidinejad et al., 2021; Yin and Wang, 2021), relative condition numbers (Agarwal et al., 2021; Uehara and Sun, 2022), and Bellman residual ratios (Xie et al., 2021). These measures aim to quantify the extent to which the data captures diverse states and behaviors. However, it should be noted that in some practical scenarios, these measures may become excessive or may not hold at all.

In terms of algorithmic approaches, existing sample-efficient offline RL algorithms explicitly construct pessimistic estimates of models or value functions to effectively learn from datasets with partial diversity. This is typically achieved through the construction of lower confidence bounds (LCBs) (Jin et al., 2021; Rashidinejad et al., 2021) or version spaces (VS) (Xie et al., 2021; Zanette et al., 2021). LCB-based algorithms incorporate a bonus term subtracted from the value estimates to enforce pessimism across all state-action pairs and stages. However, it has been observed that LCB-based algorithms tend to impose unnecessarily aggressive pessimism, leading to sub-optimal bounds (Zanette et al., 2021). On the other hand, VS-based algorithms search through the space of consistent hypotheses to identify the one with the smallest value in the initial states. These algorithms have demonstrated state-of-the-art bounds (Zanette et al., 2021; Xie et al., 2021).

In contrast to LCB-based and VS-based algorithms, regularized (minimax) optimization (RO) and posterior sampling (PS) are more amenable to tractable implementations but are relatively new in the offline RL literature. The RO-based algorithm initially introduced by (Xie et al., 2021; Algorithm 1) incorporates pessimism implicitly through a regularization term that promotes pessimism in the initial state. This approach eliminates the need for an intractable search over the version space. However, Xie et al. (2021) demonstrate that the RO-based algorithm exhibits a significantly slower sub-optimality rate than standard VS-based algorithms. Specifically, the RO-based algorithm achieves a sub-optimality rate of \(K^{-1/3}\), whereas VS-based algorithms achieve a faster rate of \(K^{-1/2}\), where \(K\) represents the number of episodes in the offline data.

On the other hand, posterior sampling (PS) (Thompson, 1933; Russo and Van Roy, 2014), a popular and successful method in online RL, is rarely explored in the context of offline RL. PS involves sampling from a constructed posterior distribution over the model or value function and acting accordingly. However, PS is less commonly considered in offline RL due to its explorative nature, which stems from the randomness of the posterior distribution. This randomness is well-suited for addressing the exploration challenge in online RL tasks (Zhang, 2022; Dann et al., 2021; Zhong et al., 2022; Agarwal and Zhang, 2022). The only work that considers PS for offline RL is Uehara and Sun (2022), where they maintain a posterior distribution over Markov decision process (MDP) models. However, this model-based PS approach is limited to small-scale problems where computing the optimal policy from an MDP model is computationally feasible. In addition, this work only provides a weak form of guarantees via Bayesian bounds.

In the context of (value) function approximation, achieving sample-efficient offline RL relies on certain conditions that facilitate effective learning. The identification of the minimum condition required for sample efficiency, as well as the algorithms that can exploit such conditions, is an important research question that we aim to address here. We advance our understanding by making the following contributions: (I) _We introduce a new notion of data diversity that subsumes and expands all the prior distribution shift measures in offline RL_, and (II) _We show that all VS-based, RO-based and PS-based algorithms are in fact (surprisingly) competitive to each other, i.e., under standard assumptions, they achieve the same sub-optimality bounds (up to constant and log factors)._ We summarize our key results in comparison with related work in Table 1. Our results further expand the class of sample-efficient offline RL problems (Figure 1) and provide more choices of offline RL algorithms with competitive guarantees and tractable approximations for practitioners to choose from.

For establishing (II), we need to construct concrete VS-based, RO-based and PS-based algorithms. While the key components of the VS-based and RO-based algorithms appear in the literature (Xie et al., 2021), we propose a novel, a first-of-its-kind, model-free posterior sampling algorithm for offline RL. The algorithm contains two new ingredients: a pessimistic prior that encourages pessimistic value functions when being sampled from the posterior distribution and integration of posterior sampling with the actor-critic framework that incrementally updates the learned policy.

Overview of Techniques.Our analysis method presents a "decoupling" argument tailored for the batch setting, drawing inspiration from recent decoupling arguments in the online RL setting (Foster et al., 2021; Jin et al., 2021; Zhang, 2022; Dann et al., 2021; Zhong et al., 2022; Agarwal and Zhang, 2022). The core idea behind our decoupling argument is to establish a relationship between the Bellman error under any comparator policy \(\) and the squared Bellman error under the behavior policy. This relationship is mediated through our novel concept of data diversity, denoted as \((;_{c})\), which is defined in detail in Definition 3. This allows to separate the sub-optimality of a learned policy into two main sources of errors: the extrapolation error, which captures the out-of-distribution (OOD) generalization from the behavior policy to a target policy, and the in-distribution error, which focuses on generalization within the same behavior distribution. The OOD error is effectively managed by controlling the data diversity \((;_{c})\), while the in-distribution error is carefully addressed by utilizing the algorithmic structures and the martingale counterpart to Bernstein's inequality (i.e., Freedman's inequality).

In the process of bounding the in-distribution error of our proposed PS algorithm that we built upon the technique of Dann et al. (2021), we correct a non-rigorous argument of Dann et al. (2021) (which we discuss in detail in Section E.3.1) and develop a new technical argument to handle the statistical dependence induced by the data-dependent target policy in the actor-critic framework. Our new argument carefully incorporates the uniform convergence argument into the in-expectation bounds of PS. We give a detailed description of this argument in Section E.3. As an immediate application, our technique fixes a technical mistake involving how to handle the statistical dependence induced by the min player in the self-play posterior sampling algorithm of Xiong et al. (2022).

## 2 Background and Problem Formulation

### Episodic Time-inhomogenous Markov Decision Process

Let \(\) and \(\) denote Lebesgue-measurable state and action spaces (possibly infinite), respectively. Let \(()\) denote the space of all probability distributions over \(\). We consider an episodic time-inhomogeneous Markov decision process \(M=(,,P,r,H)\), where

 
**Algorithms** & **Sub-optimality Bound** & **Data** \\  VS in (Xie et al., 2021) & \(Hb()(|||^{all}|) K^{-1/2}}\) & 1 \\  RO in (Xie et al., 2021) & \(Hb()[]{h(|||^{all}(T)|)} K ^{-1/3}+Hb/}\) & 1 \\  MBPS in (Uehara and Sun, 2022) & \(Hb}|| K^{-1/2}}\) (Bayesian) & 1 \\ 
**VS in Algorithm 2** & \(Hb||^{all}(T)|) K ^{-1/2}+Hb/}\) & A \\ 
**RO in Algorithm 3** & \(Hb||^{all}(T)|) K ^{-1/2}+Hb/}\) & A \\ 
**MFPS in Algorithm 4** & \(Hb||^{all}(T)|) K ^{-1/2}+Hb/}\) (frequentist) & A \\  

Table 1: Comparison of our bounds with SOTA bounds for offline RL under partial coverage and function approximation, where gray cells mark our contributions. **Algorithms**: VS = version space, RO = regularized optimization, MBPS = model-based posterior sampling, and MFPS = model-free posterior sampling. **Sub-optimality bound**: \(K\) = #number of episodes, \(\) = an _arbitrary_ comparator policy, \(H\) = horizon, \(b\) = boundedness, \(T\) = the number of algorithmic updates, \(||,|^{soft}(T)|,|^{all}|,||\): complexity measures of some value function class \(\), “induced” policy class \(^{soft}(T)\), the class of all comparator policies \(^{all}\), and model class \(\), where typically \(^{soft}(T)^{all}, T\). **Data**: I = independent episodes, A = adaptively collected data. Here \((;1/)\) and \(C_{2}()\) are some measures of extrapolation from the offline data to target policy \(\).

\(\{()\}^{H}\) are the transition probabilities (where \([H]:=\{1,,H\}\)), \(r=\{r_{h}\}_{h[H]}\{\}^{H}\) is the mean reward functions, and \(H\) is the length of the horizon for each episode. For any policy \(=\{_{h}\}_{h[H]}\{()\}^ {H}\), the action-value functions and the value functions under policy \(\) are defined, respectively, as \(Q^{}_{h,M}(s,a)=_{}[_{i=h}^{H}r_{i}(s_{i},a_{i})(s_{h},a _{h})=(s,a)]\), and \(V^{}_{h,M}(s)=_{}[_{i=h}^{H}r_{i}(s_{i},a_{i})|s_{h}=s]\). Here \(_{}[]\) denotes the expectation with respect to the randomness of the trajectory \((s_{h},a_{h},,s_{H},a_{H})\), with \(a_{i}_{i}(|s_{i})\) and \(s_{i+1} P_{i}(|s_{i},a_{i})\) for all \(i\). For any policy \(\), we define the visitation density probability functions \(d^{}_{M}=\{d^{}_{h,M}\}_{h[H]}\{ _{+}\}^{H}\) as \(d^{}_{h}(s,a):=,a_{h})=(s,a)|,M)}{d(s,a)}\) where \(\) is the Lebesgue measure on \(\) and \(((s_{h},a_{h})=(s,a)|,M)\) is the probability of policy \(\) reaching state-action \(s(s)\) at timestep \(h\). The Bellman operator \(^{}_{h}\) is defined as \([^{}_{h}Q](s,a):=r_{h}(s,a)+_{s^{} P_{h}( |s,a),a^{}_{h+1}(|s^{})}[Q(s^{},a^{ })]\), for any \(Q:\). Let \(^{*}\) be an optimal policy, i.e., \(Q^{}_{h}(s,a) Q^{}_{h}(s,a),(s,a,h,)[H]^{all}\), where \(^{all}:=\{()\}^{H}\) is the set of all possible policies. For simplicity, we assume that the initial state \(s_{1}\) is deterministic across all episodes.1 We also assume that there is some \(b>0\) such that for any trajectory \((s_{1},a_{1},r_{1},,s_{H},a_{H},r_{H})\) generated under any policy, \(|r_{h}| b, h\) and \(|_{h=1}^{H}r_{h}| b\) almost surely.2 This boundedness assumption is standard and subsumes the boundedness conditions in the previous works, e.g., Zanette et al. (2021) set \(b=1\) and Jin et al. (2021) use \(b=H\) (and further assume that \(r_{h}, h\)).3 Without loss of generality, we assume that \(b 1\).

Additional Notation.For any \(u:\) and any \(:()\), we overload the notation \(u(s,):=_{a(|s)}[u(s,a)]\). For any \(f:\), denote the supremum norm \(\|f\|_{}=_{(s,a)}|f(s,a)|\). We write \([g]^{2}:=([g])^{2}\). For a probability measure \(\) on some measurable space \((,)\), we denote by \(()\) the support of \(\), \(():=\{B:(B)>0\}\). We denote \(x y\) to mean that \(x=(y)\).

### Offline Data Generation

Denote the pre-collected dataset by \(:=\{(s^{t}_{h},a^{t}_{h},r^{t}_{h})\}_{h[H]}^{t[K]}\), where \(s^{t}_{h+1} P_{h}(|s^{t}_{h},a^{t}_{h})\) and \([r^{t}_{h}|s^{t}_{h},a^{t}_{h}]=r_{h}(s^{t}_{h},a^{t}_{h})\). We consider the adaptively collected data setting where the offline data is collected by _time-varying_ behavior policies \(\{^{k}\}_{k[K]}\), concretely, defined as follows.

**Definition 1** (Adaptively collected data3).: \(^{k}\) _is a function of \(\{(s^{i}_{h},a^{i}_{h},r^{i}_{h})\}_{h[H]}^{i[k-1]},\  k[K]\)._

For simplicity, we denote \(=_{k=1}^{K}^{k}\), \(d^{}=_{k=1}^{K}d^{^{k}}\), and \(_{}[]=_{k=1}^{K}_{^{k}}[]\). The setting of adaptively collected data covers a common practice where the offline data is collected by using some adaptive experimentation (Zhan et al., 2023). When \(^{1}==^{K}\), it recovers the setting of independent episodes in Duan et al. (2020).

Value sub-optimality.The goodness of a learned policy \(=()\) against a comparator policy \(\) for the underlying MDP \(M\) is measured by the (value) sub-optimality defined as

\[^{M}_{}():=V^{}_{1}(s_{1})-V^{}_{1} (s_{1}).\] (1)

Whenever the context is clear, we drop \(M\) in \(Q^{}_{M}\), \(V^{}_{M}\), \(d^{}_{M}\), and \(^{M}_{}()\).

### Policy and function classes

Next, we define the policy space and the action-value function space over which we optimize the value sub-optimality. We consider a (Cartesian product) function class \(=_{1}_{H}\{ [-b,b]\}^{H}\). The function class \(\) induces the following (Cartesian product) policy class \(^{soft}(T)=^{soft}_{1}(T)^{soft}_{H}(T)\), where \(^{soft}_{h}(T):=\{_{h}(a|s)(_{i=1}^{t}g_{i}(s,a)):t[T ],g_{i}_{h}, i[t],\}\) for any \(T\). The motivation for the induced policy class \(^{soft}(T)\) is from the soft policy iteration (SPI) update where we incrementally update the policy.

We now discuss a set of assumptions that we impose on the policy and function classes.

**Assumption 2.1** (Approximate realizability).: _There exist \(\{_{h}\}_{h[H]}\) where \(_{h} 0\) such that,_

\[_{T,^{soft}(T),(s_{h},a_{h})(d ^{}_{h})}_{f}|f_{h}(s_{h},a_{h})-Q^{}_{h}(s_{h},a_{h})| _{h},\ \  h[H].\]

Assumption 2.1 establishes that \(\) can realize \(Q^{}\) for any \(^{soft}(T)\) up to some error \(^{H}\) in the supremum norm over the \(\)-feasible state-action pairs. It strictly generalizes the assumption in Zanette et al. (2021) which restricts \(_{h}=0,\  h\) (i.e., assume realizability) and the assumption in Xie et al. (2021) which constrains the approximation error under any feasible state-action distribution.

The realizability in _value_ functions alone is known to be insufficient for sample-efficient offline RL (Wang et al., 2021); thus, one needs to impose a stronger assumption for polynomial sample complexity of model-free methods.5 In this paper, we impose an assumption on the closedness of the Bellman operator.

**Assumption 2.2** (General Restricted Bellman Closedness).: _There exists \(^{H}\) such that_

\[_{T,f_{h+1}_{h+1},^{soft}(T)} _{f_{h}^{}_{h}}\|f_{h}^{}-_{h}^{ {}}f_{h+1}\|_{}_{h},\ \  h[H].\]

Assumption 2.2 ensures that the value function space \(\) and the induced policy class \(^{soft}(T)\) for any \(T\) are closed under the Bellman operator up to some error \(^{H}\) in the supremum norm. This assumption is a direct generalization of the Linear Restricted Bellman Closedness in Zanette et al. (2021) from a linear function class to a general function class. As remarked by Zanette et al. (2021), the Linear Restricted Bellman Closedness is already strictly more general than the low-rank MDPs (Yang and Wang, 2019; Jin et al., 2020).

### Effective sizes of policy and function classes

When the function class and the policy class have finite elements, we use their cardinality \(|_{h}|\) and \(|^{soft}_{h}(T)|\) to measure their sizes (Jiang et al., 2017; Xie et al., 2021). When they have infinite elements, we use log-covering numbers, defined as

\[d_{}():=_{h[H]} N(;_{h},\| \|_{}),\ \ d_{}(,T):=_{h[H]} N(;^{soft}_{h}(T),\|\|_{1,}),\]

where \(\|-^{}\|_{1,}=_{s}_{}|( a|s)-^{}(a|s)|d(a)\) for any \(,^{}\{()\}\) and \(N(;,\|\|)\) denotes the covering number of a pseudometric space \((,\|\|)\) with metric \(\|\|\)(Zhang, 2023, e.g. Definition 4.1).

We also define a complexity measure that depends on a prior distribution \(p_{0}\) over \(\) that we employ to favor certain regions of the function space. Our notion, presented in Definition 2, is simply a direct adaptation of a similar notation of Dann et al. (2021) to the actor-critic setting.

**Definition 2**.: _For any function \(f^{}_{h+1}\) and any policy \(^{all}\), we define \(_{}^{}(;f^{}):=\{f _{h}:\|f-_{h}^{}f^{}\|_{}\}\), for any \( 0\), and subsequently define_

\[d_{0}():=_{T,f,^{soft}(T )}_{h=1}^{H}(_{h}^{}(;f_{h +1}))},d^{}_{0}():=_{T,^{soft}(T )}_{h=1}^{H}(_{h}^{}(;Q^ {}_{h+1}))}.\]

The quantity \(d_{0}()\) and \(d^{}_{0}()\) measures the concentration of the prior \(p_{0}\) over all functions \(f\) that are \(\)-close (element-wise) under \(^{}\) and \(\)-close (element-wise) to \(Q^{}_{h}\), respectively. If a stronger version of Assumption 2.1 is met, i.e., \(Q^{}_{h}_{h},^{all}_{h},h[H]\), we have \(d^{}_{0}() d_{0}(),\). For the finite function class \(\) and an uninformative prior \(p_{0,h}(f_{h})=1/|_{h}|\), under a stronger version of Assumption 2.2, i.e., \(_{h}=0, h\), we have \(d_{0}()_{h=1}^{H}|_{h}|=||.\) For a parametric model, where each \(f_{h}=f_{h}^{}\) is represented by a \(d\)-dimensional parameter \(_{h}^{}^{d}\), a prior over \(^{}\) induces a prior over \(\). If each \(_{h}^{}\) is compact, we can generally assume the prior that satisfies \(_{}(^{}:\|-^{}\| )} d(c_{0}/)\) for some constant \(c_{0}\). If \(f_{h}=f_{h}^{}\) is Lipschitz in \(\), we can assume that \(_{}(^{}:\|-^{}\| )} c_{1}d(c_{2}/)\) for some constants \(c_{1},c_{2}\). Overall, we can assume that \(d_{0}() c_{1}Hd(c_{2}/)\). A similar discussion can be found in Dann et al. (2021).

## 3 Algorithms

Next, we present concrete instances of PS-based, RO-based, and VS-based algorithms. The RO-based and VS-based algorithms presented here are slight refinements of their original versions in Xie et al. (2021). The PS-based algorithm is novel. All three algorithms resemble the actor-critic style update, inspired by Zanette et al. (2021). We refer to this generic framework as GOPO (Generic Offline Policy Optimization) presented in Algorithm 1. At each round \(t\), a critic estimates the value \(Q_{h}^{t}\) of the actor (i.e., policy \(^{t}\)) using the procedure

```
1:Offline data \(\), function class \(\), learning rate \(>0\), and iteration number \(T\)
2:Uniform policy \(^{1}=\{_{h}^{1}\}_{h[H]}\)
3:for\(t=1,,T\)do
4:\(Q^{t}=(^{t},,,)\)
5:\(_{h}^{t+1}(a|s)_{h}^{t}(a|s)( Q_{h}^{t}(s,a)),( s,a,h)\)
6:endfor
7:\((\{^{t}\}_{t[T]})\) ```

**Algorithm 1** GOPO\((,,,T,\)CriticCompute\()\): Generic Offline Policy Optimization Framework

To incorporate the pessimism principle, a critic should generate pessimistic estimates of the value of the actor \(^{t}\) in Line 3. This is where the three approaches differ - each invokes a different method to compute the critic. Here, we provide a detailed description of the critic module for each approach. To aid the presentation, we introduce the total temporal difference (TD) loss \(_{}\), defined as \(_{}(f_{h},f_{h+1}):=_{k=1}^{K}l_{}(f_{h},f_{h +1};z_{h}^{k})\), where \(z_{h}:=(s_{h},a_{h},r_{h},s_{h+1})\), \(z_{h}^{k}:=(s_{h}^{k},a_{h}^{k},r_{h}^{k},s_{h+1}^{k})\), and \(l_{}(f_{h},f_{h+1};z_{h}):=(f_{h}(s_{h},a_{h})-r_{h}-f_{h+1}(s_{h+1},))^{2}\).

Version Space-based Critic (VSC) (Algorithm 2).Given the actor \(^{t}\), at each step \(h[H]\), VSC directly maintains a local regression constraint using the offline data: \(_{^{t}}(f_{h},f_{h+1})_{g}_{^{t}}(g _{h},f_{h+1})+\), where \(\) is a confidence parameter and \(_{^{t}}(,)\) is serving as a proxy to the squared Bellman residual at step \(h\). By taking the function that minimizes the initial value, VSC then finds the most pessimistic value function \(^{t}\) from the version space \((;^{t})\). In general, the constrained optimization in Line 2 is computationally intractable. Note that a minimax variant of GOPO+VSC first appeared in Xie et al. (2021), where they directly perform an (intractable) search over the policy space, instead of using the multiplicative weights algorithm (Line 4) of Algorithm 1.

Regularized Optimization-based Critic (ROC) (Algorithm 3).Instead of solving the global constrained optimization in VSC, ROC solves \(_{f}\{ f_{1}(s_{1},_{1}^{t})+_ {^{t}}(f)\}\), where \(\) is a regularization parameter and \(_{^{t}}(f)\), defined in Line 1 of Algorithm 3. Note that in ROC, pessimism is implicitly encouraged through the regularization term \( f_{1}(s_{1},^{t})\). We remark that, unlike VSC, ROC admits tractable approximations that use adversarial training and work competitively in practice (Cheng et al., 2022). Note that a discounted variant of GOPO-ROC first appears in (Xie et al., 2021) in discounted MDPs.

```
1:\(_{^{t}}(f):=_{h=1}^{H}_{^{t}}(f_{h},f_{h+1})\) \(-_{g}_{h=1}^{H}_{^{t}}(g_{h},f_{h+1})\)
2:\(^{t}_{f}\{ f_{1}(s_{1}, ^{t})+_{^{t}}(f)\}\) ```

**Algorithm 2** VSC(\(,,^{t},\)): \(\)

```
Posterior sampling-based critic (PSC) in Algorithm 4.Instead of solving a regularized minimax optimization, PSC samples the value function \(Q_{h}^{t}\) from the data posterior \((f|,^{t})_{0}(f) p(|f,^ {t})\), where \(_{0}(f)\) is the prior over \(\) and \(|f,^{t})}\) is the likelihood function of the offline data \(\). To formulate the likelihood function \(p(|f,^{t})\), we make use of the squared TD error \(_{^{t}}(,)\) and normalization method in (Dann et al., 2021) to construct an unbiased proxy of the squared Bellman errors. In particular, \(p(|f,^{t})=_{h[H]}_{^{t}}(f_ {h},f_{h+1}))}{_{f_{h} p_{0,h}}(-_{^{t}}(f_ {h}^{t},f_{h+1}))}\), where \(\) is a learning rate and \(p_{0}\) is an (unregularized) prior over \(\). A value function sampled from the posterior with this likelihood function is encouraged to have small squared TD errors. The key ingredient in our algorithmic design is the "pessimistic" prior \(_{0}(f)=(- f_{1}(s_{1},_{1}))p_{0}(f)\) where we add a new regularization term \((- f_{1}(s_{1},_{1}))\), with \(\) being a regularization parameter - which is inspired by the optimistic prior in the online setting (Zhang, 2022; Dann et al., 2021). This pessimistic prior encourages the value function sampled from the posterior to have a small value in the initial state, implicitly enforcing pessimism. We remark that PSC requires a sampling oracle and expectation oracle (to compute the normalization term in the posterior distribution), which could be amenable to tractable approximations, including replacing expectation oracle with a sampling oracle (Agarwal and Zhang, 2022) while the sampling oracle can be implemented via first-order sampling methods (Welling and Teh, 2011) or ensemble methods (Osband et al., 2016).

```
1:\(^{t}(f|,^{t})(- f _{1}(s_{1},^{t}))p_{0}(f)_{h[H]}_{^{t}}(f_{h},f_{h+1}))}{_{f_{h} p_{0,h}}(- _{^{t}}(f_{h}^{t},f_{h+1}))}\)
2:\(^{t}\) ```

**Algorithm 4** PSC\((,,^{t},,,p_{0})\): Posterior Sampling-based critic

## 4 Main Results

In this section, we shall present the upper bounds of the sub-optimality of the policies executed by GOPO-VSC, GOPO-ROC, and GOPO-PSC. Our upper bounds are expressed in terms of a new notion of data diversity.

### Data diversity

We now introduce the key notion of data diversity for offline RL. Since the offline learner does not have direct access to the trajectory of a comparator policy \(^{all}\), they can only observe partial information about the goodness of \(\) channeled through the "transferability" with the behavior policy \(\). The transferability from \(\) to \(\) depends on how _diverse_ the offline data induced by \(\) can be in supporting the extrapolation to \(\). Many prior works require uniform diversity where \(\) covers all feasible scenarios of all comparator policies \(\). The data diversity can be essentially captured by how well the Bellman error under the state-action distribution induced by \(\) can predict the counterpart quantity under the state-action distribution induced by \(\). Our notion of data diversity, which is inspired by the notion of task diversity in transfer learning literature (Tripuraneni et al., 2020; Watkins et al., 2023), essentially encodes the ratio of some proxies of expected Bellman errors induced by \(\) and \(\), and is defined as follows.

**Definition 3**.: _For any comparator policy \(^{all}\), we measure the data diversity of the behavior policy \(\) with respect to a target policy \(\) by_

\[(;):=_{h[H]}_{(_{h}- _{h})}(;d_{h}^{},d_{h}^{}), 0,\] (2)

_where \(_{h}-_{h}\) is the Minkowski difference between the function class \(_{h}\) and itself, i.e., \(_{h}-_{h}:=\{f_{h}-f_{h}^{}:f_{h},f_{h}^{} \}\), and \(_{}(;q,p)\) is the discrepancy between distributions \(q\) and \(p\) under the witness of function class \(\) defined as_

\[_{}(;q,p)=\{C 0:(_{q}[g])^{2}  C_{p}[g^{2}]+, g\}\]

_with \(\) being a function class and \(p\) and \(q\) being two distributions over the same domain._

Up to a small additive error \(\), a finite \((;)\) ensures that a proxy of the Bellman error under the \(\)-induced state-action distribution is controlled by that under the \(\)-induced state-action distribution.

Despite the abstraction in the definition of this data diversity, it is _always_ upper bounded by the single-policy concentrability coefficient (Liu et al., 2019; Rashidinejad et al., 2021) and the relative condition number (Agarwal et al., 2021; Uehara et al., 2022; Uehara and Sun, 2022) that are both commonly used in many prior offline RL works. We further discuss our data diversity measure in more detail in Section 4.2.

### Offline learning guarantees

We now utilize data diversity to give learning guarantees of the considered algorithms for extrapolation to an arbitrary comparator policy \(^{all}\). To aid the representation, in all of the following theorems we are about to present, we shall set \(=()}{4(e-2)b^{2}T}}\) in Algorithm 1, where \(()\) is the volume of the action set \(\) (e.g., \(()=||\) for finite \(\)), and define, for simplicity, the misspecification errors \(_{msp}:=K_{h=1}^{H}(_{h}^{2}+b_{h})\), \(_{msp}:=_{msp}+bK_{h=1}^{H}_{h}\), \(:=_{h=1}^{H}_{h}\), the optimization error \(_{opt}:=Hb()}\), and the complexity measures \(_{opt}(,T):=\{d_{}(),d_{}(, T)\}\), and \(_{ps}(,T):=\{d_{}(),d_{}(,T), ()}{ Hb^{2}},^{}()}{ Hb ^{2}}\}\).

``` Theorem 1 (Guarantees for GOPO-VSC). Let \(^{vs}\) be the output of Algorithm 1 invoked with \(\) being VSC(\(,,^{t},\)) (Algorithm 2) with \(=(Hb^{2}\{_{opt}(,T),(H/)\}+b^{ 2}K+bK_{h[H]}_{h})\). Fix any \((0,1]\). Under Assumption 2.1-2.2, with probability at least \(1-2\) (over the randomness of the offline data), for any \(,_{c},>0\), and any \(^{all}\), we have \[[_{}(^{vs})| ] \{_{opt}(,T),(H/ )\}+b^{2}KH+_{msp}}{}+(;_{c})}{2K}\] \[+H_{c}+_{1}++_{opt}.\] ```

**Theorem 2** (Guarantees for GOPO-ROC).: _Let \(^{ro}\) be the output of Algorithm 1 invoked with \(,_{c},>0\), and any \(^{all}\), we have_

\[[_{}(^{ro})| ] \{_{opt}(,T), \}+b^{2}KH+_{msp}}{}+(;_{c})}{2K}\] \[+H_{c}+_{1}++_{opt}.\]

``` Theorem 3 (Guarantees for GOPO-PSC). Let \(^{ps}\) be the output of Algorithm 1 invoked with \(\) being PSC(\(,,^{t},,,p_{0}\)) (Algorithm 4). Under Assumption 2.2, for any \([0,}]\), and \(,_{c},,>0\), and any \(^{all}\), we have \[[_{}(^{ps})] \{_{ps}(,T), }{}\}+ b^{2}KH\{,\}+ _{msp}}{}\] \[+(;_{c})}{K}+H _{c}+++_{opt}.\]

Our results provide a family of upper bounds on the sub-optimality of each of \(\{^{vs},^{ro},^{ps}\}\), indexed by our choices of the comparator \(\) with the data diversity \((;_{c})\), additive (extrapolation) error \(_{c}\), the discretization level \(\) in log-covering numbers, the "failure" probability \(\), and other algorithm-dependent parameters (\(\) for \(^{ro}\) and (\(\), \(\)) for \(^{ps}\)). Note that the optimization error \(_{opt}\) captures the error rate of the actor and can be made arbitrarily small with large iteration number \(T\) whereas \(_{msp}\), \(_{msp}\), \(\), and \(_{1}\) are simply misspecification errors aggregated over all stages. Also note that our bound does not scale with the complexity of the comparator policy class \(^{all}\). We next highlight the key characteristics of our main results in comparison with existing work.

**(I) Tight characterization of data diversity.** Our bounds in all the above theorems are expressed in terms of \((;_{c})\). Several remarks are in order. First, \((;_{c})\) is a _non-increasing_ function of \(_{c}\); thus \((;_{c})\) is always smaller or at least equal to \((;0)\). In fact, it is possible that \((;0)=\) yet \((;_{c})<\) for some \(>0\). For instance, if there exists \(g\) such that \(g(x)=0, x(p)\) and \(\{x:g(x) 0\}\) has a positive measure under \(q\), then \(_{}(0;q,p)=\) while \(_{}(_{g}_{q}[g]^{2};q,p)=0\). Second, \((;0)\) is always bounded from above by (often substantially smaller than) the _single-policy concentrability coefficient_ between the \(\)-induced and \(\)-induced state-action distribution (Liu et al., 2019; Rashidinejad et al., 2021), which been used extensively in recent offline RL works (Yin and Wang, 2021; Nguyen-Tang et al., 2022; Jin et al., 2022; Zhan et al., 2022; Nguyen-Tang and Arora, 2023; Zhao et al., 2023). This is essentially because \(d^{}\) can cover the region that is not covered by \(d^{}\) but still the integration of functions in \(_{h}-_{h}\) over two distributions are close to each other. Third, \((;0)\) is always upper bounded by the _relative condition numbers_ used in (Agarwal et al., 2021; Uehara et al., 2022; Uehara and Sun, 2022). Our data diversity at \(=0\) is similar to the notion of distribution mismatch in Duan et al. (2020), Ji et al. (2022), though our notion is motivated by transfer learning and discovered naturally from our decoupling argument. Our data diversity measure at \(=0\) is smaller than the Bellman residual ratio measure used in Xie et al. (2021) (follows using Jensen's inequality). Finally, the concurrent work of Di et al. (2023) proposed a notion of \(D^{2}\)-divergence to capture the data disparity of a data point to the offline data. Our data diversity is in general less restricted as we only need to ensure the diversity between two data distributions (of the target policy and the behavior policy), not necessarily between each of their individual data points.

In summary, \((;_{c})\), to the best of our knowledge, provides the tightest characterization of distribution mismatch compared to the prior data coverage notions. We sketch the relationships of the discussed notions in Figure 1, where with our data diversity notion, we show that the scenarios for the offline data in which offline RL is learnable are enlarged compared to the picture depicted by the prior data coverage notions.

**(II) Competing with all comparator policies simultaneously.** Similar to some recent results in offline RL, our offline RL algorithms compete with all comparator policies that are supported by offline data in some sense. In particular, the choice of the comparator \(\) provides the flexibility to _automatically_ compete with the best policy within a certain diversity level of our choice. For instance, if we want to limit the level \((;_{c}) C\) for some arbitrary \(C>0\), our bound automatically competes with \(=_{^{}}\{V_{1}^{}(s):(;_ {c}) C\}\). This is immensely meaningful since the offline data might not support extrapolation to an optimal policy in practice.

**(III) State-of-the-art bounds for standard assumptions.** We compare our bounds with other recent guarantees of similar assumptions.6 To ease comparison, we assume for simplicity, that there is no misspecification, i.e., \(_{h}=_{h}=0, h[H]\), and \(T K()\), and we minimize the bounds in Theorem 2 and Theorem 3 with respect to \(\). The three theorems can then be simplified into a unified result presented in Proposition 1.

**Proposition 1** (A unified guarantee for VS, RO and PS).: _Under Assumption 2.1-2.2 with no misspecification, i.e., \(_{h}=_{h}=0, h[H]\), \(\{^{vs},^{ro},^{ps}\}\), \([_{}()]=}(Hb (1/K,T)(;1/)/K}+_{opt})\), where \((1/K,T)=_{opt}(1/K,T)\) if \(\{^{vs},^{ro}\}\) and \((1/K,T)=_{ps}(1/K,T)\) if \(=^{ps}\). In addition,_

Figure 1: The relations of sample-efficient offline RL classes under different data coverage measures. Given the same MDP and a target policy (e.g., an optimal policy of the MDP), each data coverage measure induces a corresponding set of behavior policies (represented by the rectangle labelled by the data coverage measure) from which the target policy is offline-learnable.

* _If_ \(_{h}\) _and_ \(_{h}^{soft}(T)\) _have finite elements for all_ \(h[H]\)_,_ \((1/K,T)=(_{h[H]}\{|_{h}|,|_{h }^{soft}(T)|\})\)_;_
* _If_ \(_{h}=\{(s,a)(_{h}(s,a),w):\|w\|_{2} b\}\) _is a linear model, where_ \(_{h}:^{d}\) _is a known feature map and w.l.o.g._ \(_{h}\|_{h}\|_{} 1\)_,_ \((1/K,T)=(d(1+KTb)), T\)_._

Proposition 1 essentially asserts that VS-based, RO-based, and PS-based algorithms obtain comparable guarantees for offline RL in the realizable case. We now compare our results to related work in various instantiation of function classes.

**Compared with Xie et al. (2021) when the function class is finite.** In this case, the analysis of the VS-based algorithms and RO-based algorithms of Xie et al. (2021) give the bounds that in our setting can be translated7 into: \(Hb(|_{h}||_{h}^{soft}|) C_{2}()/K}\) and \(Hb()}(|_{h}||_{h}^{soft}(T)|)/K }+Hb/\), respectively, where \(C_{2}():=_{h[H],^{all},f}-_{h}^{ }f_{h+1}\|_{2}^{2}}{\|f_{h}-_{h}^{}f_{h+1}\|_{2}^{2} }\). Instead, our bounds for both the VS-based and RO-based algorithms are \(Hb(|_{h}||_{h}^{soft}(T)|)( ;1/)/K}+Hb/\). We improve upon the results of Xie et al. (2021) on several fronts. First, our diversity measures \((;1/K)\) is always smaller than their measure \(C_{2}()\), since \((;1/)(;0) C_{2}()\). Second, for the VS-based algorithm, \(^{soft}(T)^{all}, T\), our bound is always tighter. In fact, \(|^{all}|\) is arbitrarily large that bounds depending on this quantity is vacuous. Third, for the RO-based algorithm, the rates in terms of \(K\) in the bound of Xie et al. (2021) are slower than that in our bound. Specifically, if \(_{h}^{soft}(T)=}_{T}(1)\), then these rates are \(K^{-1/3}\) vs \(K^{-1/2}\) (with an optimal choice of \(T=K\) for both bounds). If we consider the worst case that \(_{h}^{soft}(T)=(T|_{h}|)\), then these rates are \(K^{-1/5}\) vs \(K^{-1/4}\) (with an optimal choice of \(T=K^{2/5}\) and \(T=\) in the respective bounds). Finally, our results hold under the general adaptively collected data rather than their independent episode setting. We summarize the bounds in the finite function class cases in Table 1, and give comparisons for the linear model cases in Table 2.

**Compared with LCB-based algorithms.** When \(_{h}\) is a \(d\)-dimensional linear model with feature maps \(\{_{h}\}_{h[H]}\), our bounds reduce into \(Hb(;1/)}\) (Proposition 1), which matches the order of (and potentially tighter than) the bound in Zanette et al. (2021), since \((;1/)\) is always smaller (or at least equal to) than the relative condition number \(_{h}_{x^{d}}_{h}[_{h}(s,a_{h}) _{h}(s,a_{h})^{T}]x}{x^{2}_{h}[_{h}(s,a_{h})_{h}(s,a_{h })^{T}]x}\). Compared with the bound of LCB-based algorithms in Jin et al. (2021), we improve a factor \(\) and holds under the more general Assumption 2.2 which includes low-rank MDPs. In a more refined analysis (Xiong et al., 2023), the LCB-based algorithm obtains the same dependence on \(d\) for low-rank MDPs as our guarantees. However, this improvement relies on a uniform coverage assumption, i.e., \(_{h[H]}_{}(_{(s_{h},a_{h}) d_{h}^{ }}[_{h}(s_{h},a_{h})_{h}(s_{h},a_{h})^{T}])>0\), which we do not require. Di et al. (2023) generalize the results of Xiong et al. (2023) from linear MDPs to MDPs with general function approximation. However, they still rely on a uniform coverage assumption. Finally note that, for VS-based and RO-based algorithms, we provide high-probability bounds for a smoothing version of \(\) over the randomization of the algorithms, not for \(\) itself.

**Compared with model-based PS.**Uehara and Sun (2022) consider model-based PS for offline RL, where they obtain the _Bayesian_ sub-optimality bound of \(H^{2}}||/K}\) where \(C^{}\) is the Bayesian version of a relative condition number and \(\) is a finite model class. Two key distinctions are that our method in Algorithm 4 is model-free, and our achieved bound is in the frequentist (i.e., worst-case) nature, which is a stronger result than the Bayesian bound of the same order.

## 5 Conclusion

We contributed to the understanding of sample-efficient offline RL in the context of (value) function approximation. We proposed a notion of data diversity that generalizes the previous data coverage measures and importantly expands the class of sample-efficient offline RL. We studied three different algorithms: VS, RO, and PS, where the PS-based algorithm is our novel proposal. We showed that VS, RO, and PS all have same-order guarantees under standard assumptions.