# Faster Differentially Private Top-\(k\) Selection:

A Joint Exponential Mechanism with Pruning

 Hao WU

University of Waterloo

Canada

hao.wu1@uwaterloo.ca

&Hanwen Zhang

University of Copenhagen

Denmark

hazh@di.ku.dk

This work was conducted while the author was a Postdoctoral Fellow at the University of Copenhagen.A simplified bound from Theorem 4.1 for a wide range of failure probabilities concerning solution quality.

###### Abstract

We study the differentially private top-\(k\) selection problem, aiming to identify a sequence of \(k\) items with approximately the highest scores from \(d\) items. Recent work by Gillenwater et al. (ICML '\(22\)') employs a direct sampling approach from the vast collection of \(d^{(k)}\) possible length-\(k\) sequences, showing superior empirical accuracy compared to previous pure or approximate differentially private methods. Their algorithm has a time and space complexity of \((dk)\).

In this paper, we present an improved algorithm with time and space complexity \(O(d+k^{2}/ d)\), where \(\) denotes the privacy parameter. Experimental results show that our algorithm runs orders of magnitude faster than their approach, while achieving similar empirical accuracy.

## 1 Introduction

Top-\(k\) selection is a fundamental operation with a wide range of applications: _search engines, e-commerce recommendations, data analysis, social media feeds etc_. Here, we consider the setting where the dataset consists of \(d\) items evaluated by \(n\) people. Each person can cast at most one vote for each item, and vote for unlimited number of items. Our goal is to find a sequence of \(k\) items which receives the highest number of votes.

Given that data can contain sensitive personal information such as medical conditions, browsing history, or purchase records, we focus on top-\(k\) algorithms that are _differentially private_(Dwork et al., 2006): it is guaranteed that adding/removing an arbitrary single person to/from the dataset does not substantially affect the output. Research for algorithms under this model centers around how accurate the algorithms can be and how efficient they are.

Significant progress has been made in understanding the theoretical boundaries. There are approximate differentially private algorithms (Durfee and Rogers, 2019; Qiao et al., 2021) that achieve asymptotic accuracy lower bound (Bafna and Ullman, 2017; Steinke and Ullman, 2017), and have \(O(d)\) time and space usage.

There is also a research endeavor aimed at enhancing the empirical performance of the algorithms. A particularly noteworthy one is the Joint mechanism by Gillenwater, Joseph, Medina, and Diaz (2022), which exhibits best empirical accuracy across various parameter settings. Diverging from the prevalent _peeling strategy_ for top-\(k\) selection-wherein items are iteratively selected, removed and repeated \(k\) times-the Joint mechanism considers the sequence holistically, directly selecting an output from the space comprising all \(d^{\,(k)}\) possible length-\(k\) sequences.

While the algorithm has running time and space \((dk)\), successfully avoiding an exponential time or space consumption, it notably incurs a higher computational cost than its \(O(d)\) counterparts. This prompts the interesting question:Research Question: Can we design a mechanism equivalent to the Joint mechanism with running time and space linear in \(d\)?

Our Contributions.Our paper answers the research question when \(k\) is not too large. Specifically,

* We present an improved algorithm with time and space complexity of \(O(d+k^{2}/ d)\)

This is an informal statement of Theorem 4.1. When \(k O()\) (a common scenario in practical settings), the time and space complexity simplifies to \((d)\). Moreover, the proposed algorithm achieves the same asymptotic accuracy guarantee as the Joint mechanism.

Similar to the Joint mechanism, our algorithm is an instance of the exponential mechanism (detailed in Section 3) that directly samples from the output space comprising all length-\(k\) sequences. We introduce a "group by" sampling framework, which partitions the sequences in the output space into \(O(nk)\) subsets, aiming to streamline the sampling process. The framework consists of two steps: sampling a subset and then sampling a sequence from that subset. We provide efficient algorithms for both steps. Furthermore, we introduce a pruning technique to handle outputs with low accuracy uniformly. This technique effectively reduces the number of subsets to \((k^{2})\), leading to an algorithm in \((d+k^{2})\) time and space complexity.

Finally, we perform extensive experiments to

* Verify the theoretical analysis of our algorithm.
* Demonstrate that our algorithm runs 10-100 times faster than Joint on the tested datasets.
* Show that our algorithm maintains comparable accuracy to Joint.

Organization.Our paper is structured as follows: Section 2 formally introduces the problem, while Section 3 delves into the necessary preliminaries for our algorithm. Section 4 introduces our novel algorithm, and Section 5 presents our experiment results.

## 2 Problem Description

Let \(\{1,,d\}\) be a set of \(d\) items and \(\{1,,n\}\) be a set of \(n\) clients. Each client \(v\) can cast at most one vote for each item, and can vote for an unlimited number of items. For each item \(i\), its score \([i]\) is the number of votes it received. The _histogram_ is a vector \((,,[d])[0 n]^{d}\). Define \(_{,k}\{(i_{1},,i_{k})^{ k}:i_{1}, i_{k}\,\}\) be the collection of all possible length-\(k\) sequences.

The differentially private top-\(k\) selection problem aims at finding a sequence from \(_{,k}\) with approximately largest scores, while protecting the privacy of each individual vote.

Privacy Guarantee.Two voting histograms \(,^{}^{d}\) are neighboring, denoted by \(^{}\), if \(^{}\) can be obtained from \(\) by adding or removing an arbitrary individual's votes. Therefore, when \(^{}\), we have \(||-^{}||_{} 1,\) and \(^{}\) or \(^{}\). To protect personal privacy, a top-\(k\) selection algorithm should have similar output distributions on neighboring inputs.

**Definition 2.1** (\((,)\)-Private Algorithm (Dwork and Roth, 2014)).: _Given \(,>0\), a randomized algorithm \(:^{d}_{,k}\) is called \((,)\)-differentially private (DP), if for every \(,^{}^{d}\) such that \(^{}\), and all \(Z_{,k}\),_

\[[() Z] e^{}[(^{}) Z]+\,.\] (1)

_Remark:_ An algorithm \(\) is also called \(\)-DP for short, if it is \((,0)\)-DP. If an algorithm is \(\)-DP, it is also called _pure DP_, whereas it is called _approximate DP_ if it is \((,)\)-DP. Although we present the definition in the context of top-\(k\) selection algorithms, it applies more generally to any randomized algorithms \(:\), where \(\) is the input space, which is associated with a symmetric relation \(\) that defines neighboring inputs.

Preliminaries

### Exponential Mechanism

The exponential mechanism (McSherry and Talwar, 2007) is a well-known differentially private algorithm for publishing discrete values. Given a general input space \(\) (associated with a relation \(\) which defines neighboring datasets), a finite output space \(\), the exponential mechanism \(_{}:\) is a randomized algorithm given by

\[_{}(x)=y- \ _{}(x,y)\,/\,(2_{}),  x,\,y,\] (2)

where \(_{}:\) is called the _loss function_ measuring how "bad" \(y\) is when the input is \(x\), and \(_{}\) is the _sensitivity_ of \(_{}\) which is the maximum deviation of \(_{}\):

\[_{}_{x x^{},y}|_{}(x,y)-_{}(x^{},y)|\,.\] (3)

**Fact 3.1** (Privacy (McSherry and Talwar, 2007)).: _The exponential mechanism \(_{}\) is \(\)-DP._

**Fact 3.2** (Utility Guarantee (McSherry and Talwar, 2007)).: _For each \((0,1)\), and \(}}{}|}{}\), the exponential mechanism \(_{}\) satisfies_

\[_{}(x,_{}(x)) _{y}\,_{}(x,y)+,  x.\]

_Implementation._ Given input \(x\), a technique for implementing the exponential mechanism is to add i.i.d. Gumbel noises to the terms of \(\{-_{}(x,y)\,/\,(2_{ {exp}}):y\}\), and then select the \(y\) corresponding to the noisy maximum.

**Definition 3.3**.: _Given \(b>0\), the Gumbel distribution with parameter \(b\), denoted by \((b)\), has probability density function \(p(x)=-+- ,\, x\)._

**Fact 3.4** ((Yellott, 1977)).: _Assume that \(w_{i} 0\), for \(i[m]\), and \(X_{i}(1),i[m]\) are independent random variables. Then \(i=_{j[m]}(X_{j}+ w_{j}) w_{i}\)._

It follows that, if \(X_{y}(1),y\) are independent random variables, then

\[y=_{y^{}}\{X_{y^{}}- _{}(x,y^{})\,/\,(2_{ })\}-_{ }(x,y)\,/\,(2_{}).\]

### Joint Mechanism

The Joint mechanism \(_{}:^{d}_{,k}\)(Gillenwater et al., 2022) is an instance of the exponential mechanism which samples a sequence \(=(,,[k])\) directly from \(_{,k}\), with the loss function

\[_{}(,)_{i[k]} _{(i)}-[i],\] (4)

where \(_{(i)}\) is the true \(i^{(th)}\) largest entry in \(\). It can be seen that \(_{}()\) has sensitivity \(_{}=1\).

Observe that a naive implementation of this exponential mechanism needs evaluating and storing the scores of \(|_{,k}|=d^{\,(k)}\) sequences. Remarkably, Gillenwater, Joseph, Medina, and Diaz (2022) demonstrate that the exponential time and space requirements can be reduced to polynomial.

**Fact 3.5** (Joint Mechanism (Gillenwater et al., 2022)).: _There is an implementation of exponential mechanism which directly sample a sequence from \(_{,k}\) according to loss function \(_{}(,)=_{i[k]}_{(i )}-[i]\) with time \(O(dk k+d d)\) time and space \(O(dk)\)._

For completeness, we includes a short proof of Fact 3.5 in Appendix A. Let \(^{\,*}\) corresponds to the \(k\) items with the largest scores. Then clearly \(_{}_{}(,)=_{ }(,^{\,*})=0\). Combining \(|_{,k}|= k!\) and \(_{}=1\), and applying Fact 3.2, provide the theoretic utility guarantee of Joint.

**Fact 3.6** (Utility Guarantee).: _For each \((0,1)\), \( k!}{ }(k d+),\)_

\[[_{}(,_{}() )].\] (5)

[MISSING_PAGE_FAIL:4]

**Lemma 4.3**.: _For each \(r[0 n],i[k]\), it holds that_

\[|_{r,i}|=_{j=1}^{i-1}(C_{r-1,j}-(j-1))+( C_{r,i}-C_{r-1,i})+_{j=i+1}^{k}(C_{r,j}-(j-1)).\] (9)

**Lemma 4.4**.: _For all \(r[0 n],j[k]\), \(C_{r,j}\) can be computed in \(O(d+nk)\) time. Furthermore, given the \(C_{r,j}\)'s, for all \(r[0 n]\), \(|_{r,i}|\) can be computed in \(O(nk)\) time._

The algorithms for proving Lemma 4.4 are detailed in Appendix B. At a high level, for a fixed \(r\), the \(C_{r,j},j[k]\) constitute a monotone sequence, enabling us to devise a recursion formula to compute them. Additionally, the prefix sums (the first term) and the suffix sums (the last term) in Equation (9) can be pre-computed, simplifying the computation of \(|_{r,i}|\) to adding only three terms.

Here, we present a proof for Lemma 4.3, offering insights into the structure of \(_{r,i}\).

Proof for Lemma 4.3.: It suffices to show that

\[|_{r,i}|=_{j=1}^{i-1}(C_{r-1,j}-(j-1))(C_ {r,i}-C_{r-1,i})_{j=i+1}^{k}(C_{r,j}-(j-1)).\] (10)

The proof is via standard counting argument: assume we want to select a sequence \(_{r,i}\). Since \(\{j^{}[d]:[j^{}]>_{(1)}-r\}\), the number of possible choices for \(\) is

\[|\{j^{}[d]:[j^{}]>_{(1)}-r\}|=|\{j^{}[d ]:[j^{}]_{(1)}-(r-1)\}|=C_{r-1,1}.\]

The first equality holds because the \([j^{}]\) values are integers.

Next, since \(_{(1)}_{(2)}\), it also holds that \(\{j^{}[d]:[j^{}]>_{(2)}-r\}\). After determining \(\), the number of choices for \(\) is \(|\{j^{}[d]:[j^{}]>_{(2)}-r\}|-1=C_{r-1,2}-1\). Continuing this argument, for each \(j[1 i-1]\), the number of choices for \([j]\), after determining \([1 j-1]\), is \(|\{j^{}[d]:[j^{}]>_{(j)}-r\}|-(j-1)=C_{r-1,j}-(j-1)\).

Now we consider the number of choices for \([i]\). Since \(,,[i-1]\{j^{}[d]:[j^{}]>_{(i)}-r\}\), they do not appear in \(\{j^{}[d]:[j^{}]=_{(i)}-r\}\). The number of choices for \([i]\) is exactly \(|\{j^{}[d]:[j^{}]=_{(i)}-r\}|=C_{r,i}-C_{r-1,i}\).

The cases for \(j[i+1 k]\) are similar to the cases of \(j[1 i-1]\). Since \(_{(1)}_{(2)}_{(j-1)}\), it holds that \(,,[j-1]\{j^{}[d]:[j^{}] _{(j)}-r\}\). As a result, for \(j[i+1 k]\), the number of choices for \([j]\), after determining \([1 j-1]\), is \(C_{r,j}-(j-1)\).

Multiplying the number of choices for each element in \(_{r,i}\), we obtain Equation (10). 

_The second key advantage_ of the partition being considered is that, there is an algorithm for sampling a uniform random sequence from \(_{r,i}\) in \(O(d)\) time, as implicitly suggested by the proof for Lemma 4.3. Further details of this implementation are provided in Appendix B.

### Pruning

The previous discussion suggests an new algorithm with \(O(d+nk)\) running time. Based on Lemma 4.4, computing the \(_{r,j}\) values for \(r[0 n]\) and \(j[k]\) takes \(O(d+nk)\) time. The total time to compute \(|_{r,i}|\) for \(r[0 n]\) and \(i[k]\) is \(O(nk)\). Finally, sampling a sequence from the chosen \(_{r,i}\) takes \(O(d)\) time. The bottleneck here lies in the \(nk\) term, which arises from the need to compute the \(C_{r,j}\)'s and \(|_{r,i}|\)'s for all \(r[0 n]\).

However, this is unnecessary. We need only to consider the cases for \(r[0]\). The key observation is that, the probability of sampling an \(_{,k}\) decreases exponentially with increasing \(()\).

**Claim 4.5** (Restatement of Fact 3.6).: _The probability of sampling an \(\) with \(()\) is at most \(\)._

To provide further insight, we present a short proof here. Let \(_{}\{_{,k}:()\}\), then

\[[_{}]_{}}e^{-()/2}}{e ^{-/2}}|_{,k}|  e^{-/2}=,\] (11)where \(^{*}\) is the \(k\) items with the largest scores and \((^{*})=0\).

Given this, if we slightly adjust the loss function of sequences in \(_{}\), their probabilities of being outputted will not be significantly affected. It motivates to consider the truncated loss function: \(_{}()((),)\), and an algorithm \(\) which samples an \(\) with probability proportional to \(e^{-_{}()/2}\). As inequality (11) still holds if we the \(()\) with \(_{}()\), we immediately obtain the following lemma.

**Lemma 4.6**.: _The probability of \(\) sampling an \(\) with \(()\) is at most \(\)._

_Subset Merging._ The most important benefit of truncated loss is that, it allows us to reduce to the number of subsets in the partition \(_{r,i},r[0\,..\,n],i[k]\) from \(O(nk)\) to \(O( k)\). In particular, for each \(i[k]\), as the sequences in the subsets \(_{r,i},r[\,..\,n]\) has the same truncated loss, it suffices to merge them into one

\[_{,i}_{r[\,..\,n]}_{r,i}= \{=(,,[k])_{,k}: [j]>_{(j)}-,}{[j]_{(j)}-,}{=i}\}.\] (12)

\(_{,i}\) shares a similar formula on its size as Equation (9) and can be uniformly sampled efficiently.

**Lemma 4.7**.: _For each \(i[k]\), we have_

\[|_{,i}|=_{j=1}^{i-1}(C_{-1,j}-(j-1) )+(d-C_{r-1,i})+_{j=i+1}^{k}(d-(j-1)).\] (13)

**Lemma 4.8**.: _Given the \(C_{-1,j}\)'s, each \(|_{,i}|\) can be computed in \(O(1)\) amortized time._

The proof for Lemma 4.7 is provided in Appendix A, while an algorithmic proof for Lemma 4.8 can be found in Appendix B.

## 5 Experiment

In this section, we compare our algorithm, referred to as FastJoint, with existing state-of-the-art methods on real-world datasets. Our Python implementation is available publicly.3

Datasets.We utilize six publicly available datasets: Games (Steam video games with purchase counts) (Tamber, 2016), Books (Goodreads books with review counts) (Soumik, 2019), News (Mashable articles with share counts) (Fernandes et al., 2015), Tweets (Tweets with like counts) (Bin Tareaf, 2017), Movies (Movies with rating counts) (Harper and Konstan, 2016) and Foods (Amazon grocery and gourmet foods with review counts) (McAuley et al., 2015). Table 1 summarizes their sizes.

Baselines.Apart from the Joint mechanism (Gillenwater et al., 2022), we consider the following two candidates: the peeling variant of permute-and-flip mechanism (McKenna and Sheldon, 2020), denoted PNF-Peel; and the peeling exponential mechanism (Purlee and Rogers, 2019), denoted CDP-Peel. We don't compare with other mechanisms, e.g. the Gamma mechanism (Steinke and Ullman, 2016) and the Laplace mechanism (Bhaskar et al., 2010; Qiao et al., 2021), which are empirically dominated by PNF-Peel and CDP-Peel respectively (Gillenwater et al., 2022).

PNF-Peel: The permute-and-flip is an \(\)-DP mechanism for top-1 selection. It can be implemented equivalently by adding exponential noise (with privacy budget \(/k\)) to each entry of \(\) and reporting the item with the highest noisy value (Ding et al., 2021). To report \(k\) items, we use the _peeling_ strategy: select one item using the mechanism, remove it from the dataset, and repeat this process \(k\) times, resulting in a running time of \(O(dk)\).

CDP-Peel: The \((,)\)-DP peeling exponential mechanism samples \(k\) items without replacement, selecting one item at a time using a privacy budget of \((/)\) according to the exponential mechanism (McSherry and Talwar, 2007). Durfee and Rogers (2019) demonstrate that CDP-Peel has an equivalent \(O(d)\)-time implementation.

The code for all competing algorithms was obtained from publicly accessible GitHub repository by Google Research4, written in Python.

  Dataset & Games & Books & News & Tweets & Movies & Food \\  \#items & 5,155 & 11,126 & 39,644 & 52,542 & 59,047 & 166,049 \\  

Table 1: Dataset Size SummaryExperiment Setups.The experiments are conducted on macOS system with M2 CPU and 24GB memory. We compare the algorithms in terms of running time and error for different values of \(k\), \(\) and \(\). Note that the parameter \(\) (see Theorem 4.1) only affects our algorithm. The \((,)\)-DP mechanism CDP-Peel is configured with a \(\) parameter of \(10^{-6}\), consistent with prior research (Gillenwater et al., 2022).

Error Metrics.We evaluate the quality of a solution \(\) using both \(_{}\) and \(_{1}\) errors. The \(_{}\) error is defined as \(_{i[k]}|_{(i)}-[i]|\), while the \(_{1}\) error is given by \(_{i[k]}|_{(i)}-[i]|\).

Parameter Ranges.The parameter ranges tested are as follows:

\[k=10,20,,,,200,=1/4,1/2, ,2,4,=2^{-6},2^{-8},},2^{-12},2^{- 14}.\]

The values indicated by underlining represent the default settings. During experiments where one parameter is varied, the other two parameters are kept at their default values.

### Results

All experiments are repeated 200 times. Each figure displays the median running time or \(_{}\) or \(_{1}\) error as the center line, with the shaded region spanning the 25th to the 75th percentiles.

Varying \(k\).Figure 1 presents the results for different values \(k\). FastJoint consistently outperforms Joint in terms of execution speed, running 10 to 100 times faster across various datasets. FastJoint is slower than CDP-Peel; the later has theoretical time complexity \(O(d)\) and therefore this is expected.

We observe "jumps" in running time of Joint on the _games_ and _food_ datasets. This phenomenon can also be found in the original work by Gillenwater et al. (2022) in the only running time plot for the _food_ dataset. Upon investigation, we found that, as noted in their code comments, the current Python implementation of the _Sequence Sampling_ step of Joint has a worst-case time complexity of \(O(dk^{2})\) instead of \(O(dk)\). Although this step does not constitute a bottleneck in their code and accounts for a constant fraction of the total running time, it still introduces instability in the running time. To delve deeper into this issue, we provide a comparison in the appendix where we plot the running time of Joint (excluding the _Sequence Sampling_ step) against the running time of FastJoint (including the _Sequence Sampling_ step), resulting in smoother time plots. Even with this adjustment, Joint remains order of magnitude slower.

Interestingly, for small datasets, FastJoint can be slower than PNF-Peel, which has an \(O(dk)\) time complexity. This is because PNF-Peel has a simple algorithmic structure that can be implemented as \(k\) rounds of vector operations: each round involves adding a noisy vector to the input histogram and then selecting an item (not previously selected) with the highest score. It is well-known that vectorized implementations5 gain significant speed boosts by utilizing dedicated Python numerical libraries such as NumPy (Harris et al., 2020). However, as the dataset size increases (e.g., the food dataset), FastJoint outperforms PNF-Peel in terms of speed.

In terms of solution quality, even with the pruning strategy, FastJoint does not experience quality degradation compared to Joint. It delivers similar performance to Joint across all datasets and performs particularly well on the _books_, _news_, _tweets_, and _movies_ datasets, where there are large gaps between the top-\(k\) scores. (Due to space limitations, the complete plots of these score gaps are included in the appendix, with partial plots provided in Figure 2). FastJoint consistently outperforms the pure differentially private PNF-Peel for all values of \(k\) and the approximate differentially private CDP-Peel for at least moderately large \(k\). These results align with the findings of Gillenwater et al. (2022), who compared Joint with PNF-Peel and CDP-Peel.

Varying \(\).Due to space constraints, Figure 2 presents results for different values of \(\) on two typical datasets: one where FastJoint performs well and one where it does not. We replaced the \(_{1}\) error plot (as it exhibits similar trends to the \(_{}\) plots) with a plot showing the gap between the top-300 scores6. The complete plots across all datasets are included in Appendix C. The running time comparison resembles that of the varying-\(k\) plots in Figure 1, with one notable difference: the running time of FastJoint exhibits a clear decrease as \(\) increases. This observation aligns with our theoretical statement about the running time of FastJoint, as detailed in Theorem 4.1.

Figure 1: **Left**: Running time vs \(k\). **Center**: \(_{}\) error vs \(k\). **Right**: \(_{1}\) error vs \(k\). The \(_{1}/_{}\) plots are padded by \(1\) to avoid \( 0\) on the \(y\)-axis.

In terms of solution quality, FastJoint and Joint perform particularly well on the _news_ dataset and are only slightly better than PNF-Peel and inferior to CDP-Peel on the _games_ dataset, where the gaps between the large scores in the former dataset are significantly larger than in the latter (note the values on the log-scale y-axis). We provide an informal but informative explanation for this phenomenon: based on Lemma 4.6, FastJoint is unlikely to sample sequences with loss greater than \(\). Furthermore, when the distribution of top-\(k\) score gaps is highly skewed, there are very few sequences with errors between \((0,]\), and the likelihood of sampling these sequences scales with \(e^{-O()}\) as \(\) varies. In contrast, the peeling-based mechanism needs to divide its privacy budget by \(k\) or \(()\) for each round, causing the sampling probability of an error item to scale only with \(e^{-O(/k)}\) or \(e^{-(/)}\), which is higher than \(e^{-O()}\).

Varying \(\).Due to space constraints, Figure 3 presents results only for different values of \(\) on a medium-sized dataset. Similar plots for other datasets can be found in Appendix C. It is anticipated that Joint, PNF-Peel and CDP-Peel do not exhibit significant performance variation concerning \(\). However, it is somewhat surprising that FastJoint does not neither. This stability can be attributed to the threshold used for pruning, given by \(= k!/).\) The numerator inside logarithm term, \( k!\), grows as \(d^{(k)}\), significantly overshadowing \(1/\). Consequently, \(\) changes only slightly as \(\) varies. This experiment demonstrates the robustness of FastJoint's pruning strategy concerning the choice of \(\).

## 6 Related Work

Comparison with Joint.We can also explain Joint(Gillenwater et al., 2022) within the novel framework proposed in Section 4.1, which consists of the _Subset Sampling_ and _Sequence Sampling_ steps. Their approach assumes that \([d]\), which can be achieved by sorting \(\). For each \(i[k]\) and \(j[d]\), define \(_{i,j}[i]-[j]\). The partition they consider is equivalent to the one defined as follows:

\[U_{i,j}\{=(,,[k])_{ ,k}:\,[[]]>[]- _{i,j},&<i\\ []=j,&=i\\ \,[[]][]-_{i,j},&>i \}\, i[k],j[d].\]

Intuitively, \(U_{i,j}\) consists of the length-\(k\) sequences, \(\), that satisfy: 1) the \(i^{}\) element in \(\) is exactly element \(j\); 2) the first \(i-1\) elements in \(\) have losses less than \(_{i,j}\); and 3) the last \(k-i\) elements in \(\) have losses at most \(_{i,j}\).

Therefore, the sequences \(\) in \(U_{i,j}\) share the same loss, \(_{}(,)=_{i,j}\) (as defined in Equation (4)), with the first position reaching this loss being the \(i^{}\) position, where element \(j\) appears. Furthermore, sequences in different subsets, \(U_{i,j}\) and \(U_{i^{},j^{}}\), can share the same loss, as it is possible that \(_{i,j}=_{i^{},j^{}}\). There are \(dk\) subsets in this partition, resulting in a running time of \((dk)\) for their implementation (Gillenwater et al., 2022).

Applying the pruning technique to this partition can not reduce the number of subsets to \(o(dk)\). Let \(\) be as defined in Theorem 4.1. For each \(i[k]\), we aim to find the first \(j\) such that \([j]_{(i)}-\). Denote this value by \((i)\{j[d]:[j]_{(i)}-\}\). Following the spirit of our pruning technique, we would like to merge the trailing subsets \(U_{i,(i)},,U_{i,d}\) into a single subset \(U_{i,(i)}_{j(i)}U_{i,j}\) to reduce the number of subsets. However, it is easy to find counterexamples where \((d)\) items are equal to \(_{(i)}\) for all \(i[k]\). For example, consider the case where \(===[d]=c\), for some constant \(c\). In such scenarios, we still have \((i)(d)\), and therefore, in the worst case, the number of subsets remains \(_{i[k]}(i)(dk)\).

Truncated Loss.The technique of applying the exponential mechanism with truncated scores was considered by Bhaskar et al. (2010). Their top-\(k\) selection algorithm employs a peeling-based approach: it samples \(k\) items without replacement, selecting one item at a time using the exponential mechanism with truncated scores. This method is employed because, in their setting, obtaining the scores of the input histogram \(\) is expensive, leading them to treat lower-scoring items uniformly by assigning them a small, identical score. In contrast, when all scores of \(\) are known, iteratively applying the exponential mechanism to select the top-\(k\) items has an equivalent linear time implementation (Durfee and Rogers, 2019) (the CDP-Peel algorithm in Section 5). Therefore, truncating the scores of \(\) is unnecessary in this case.

Adaptive Private \(k\) Selection.As the experiments show, the performance of Joint and FastJoint depends on gap size--they perform well when there are large gaps between the top-\(k\) items. An orthogonal line of research (Zhu and Wang, 2022) leverages large gaps to privately identify the index \(i\) that approximately maximizes the gap between the \(i^{}\) and \((i+1)^{}\) largest elements. This is followed by testing whether the gap (using techniques like propose-test-release) between the \(i^{}\) and \((i+1)^{}\) largest elements is sufficiently large, allowing the top-\(i\) items to be returned without additional noise. This approach benefits by adding no noise in the final step. However, there are two key differences: 1) it does not guarantee returning at least \(k\) items, as \(i\) can be less than \(k\); 2) more crucially, the top-\(i\) items must be returned as an _unordered set_. The first issue can be addressed by iteratively applying the above mechanism. For the second issue, algorithms introduced in this paper, such as CDP-Peel and FastJoint, can serve as subroutines. Notably, FastJoint may provide better empirical performance when large gaps exist among the top \(i\) items.

Utility Lower Bound.Bafna and Ullman (2017) and Steinke and Ullman (2017) demonstrate that, for approximate private algorithms, existing methods (Durfee and Rogers, 2019; Qiao et al., 2021), including CDP-Peel(Durfee and Rogers, 2019) as compared in Section 5, achieve theoretically asymptotically optimal privacy-utility trade-offs.