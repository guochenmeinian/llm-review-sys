# Rule Based Learning with Dynamic (Graph) Neural Networks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

A common problem of classical neural network architectures is that additional information or expert knowledge cannot be naturally integrated into the learning process. To overcome this limitation, we propose a two-step approach consisting of (1) generating formal rules from knowledge and (2) using these rules to define rule based layers - a new type of dynamic neural network layer. The focus of this work is on the second step, i.e., rule based layers that are designed to dynamically arrange learnable parameters in the weight matrices and bias vectors for each input sample following a formal rule. Indeed, we prove that our approach generalizes classical feed-forward layers such as fully connected and convolutional layers by choosing appropriate rules. As a concrete application we present rule based graph neural networks (RuleGNNs) that are by definition permutation equivariant and able to handle graphs of arbitrary sizes. Our experiments show that RuleGNNs are comparable to state-of-the-art graph classifiers using simple rules based on the Weisfeiler-Leman labeling and pattern counting. Moreover, we introduce new synthetic benchmark graph datasets to show how to integrate expert knowledge into RuleGNNs making them more powerful than ordinary graph neural networks.

## 1 Introduction

Using expert knowledge to increase the efficiency, interpretability or predictive performance of a neural network is an evolving research direction in machine learning [21; 23]. Many ordinary neural network architectures are not capable of using external and structural information such as expert knowledge or meta-data, e.g., graph structures in a dynamic way. We would like to motivate the importance of "expert knowledge" by considering the following example. Maybe one of the best studied examples based on knowledge integration are convolutional neural networks . Convolutional neural networks for images use at least two extra pieces of "expert knowledge" that is: _neighborhood pixels correlate_, and _the structure of images is homogeneous_. The consequence of this _knowledge_ is the use of receptive fields and weight sharing. It is a common fact that the usage of this information about images has highly improved the predictive performance over fully connected neural networks. But what if expert knowledge suggests that rectangular convolutional kernels are not suitable to solve the task? In this case the ordinary convolutional neural network architecture is too _static_ to adapt to the new information. Dynamic neural networks are not only applicable to images but also to other data types such as video , text , or graphs . The limitation of such approaches is that expert knowledge is somehow implicit and not directly encoded in the network structure, i.e., for each new information a new architecture has to be designed. Thus, our goal is to extract the essence of dynamic neural networks by defining a new type of neural network layer that is on the one side able to use expert knowledge in a dynamic way and on the other side easily configurable. Our solution to this problem are rule based layers that are able to encode expertknowledge directly in the network structure. As far as we know, this is the first work that defines a dynamic neural network layer in this generality.

Main IdeaWe simplify and unify the integration of expert knowledge and additional information into neural networks by proposing a two-step approach and show how to encode given extra information directly into the structure of a neural network in a dynamic way. In the _first step_ the extra information or expert knowledge is formalized using appropriate rules (e.g., _certain pixels in images are important_, _only nodes in a graph of type A and B interact_, _some patterns, e.g., cycles or cliques, in a graph are important_, etc.). In the _second step_ the rules are used to manipulate the structure of the neural network. More precisely, the rules determine the positions of the weights in the weight matrix and the bias terms. We note that the focus of this work is on the second step as we show how to use given rules to dynamically adapt the layers. In fact, we do not provide a general instruction for deriving formal rules from given expert knowledge. In difference to ordinary network layers we consider a set \(\) of learnable parameters instead of fixed weight matrices. The weight matrices and bias terms are then constructed for each input sample independently using the learnable parameters from \(\). Indeed, each learnable parameter in \(\) is associated with a specific relation between an input and output feature of a layer. As an example consider Figure 1 where each input and output feature corresponds to a specific node in the graph. The input samples are (a) molecule graphs respectively (b) snippets of social networks and the task is to predict the graph class. Each colored arrow in the figure corresponds to a learned parameter from \(\), i.e., a specific relation between two atoms in the molecules or two nodes in the social network. Considering only the weights with the largest absolute values, see the second image of (a) respectively (b), our approach has learned how to propagate information from outer atoms to the rings respectively from the nodes to the "important" nodes of the social network. This example shows several advantages of our approach: (1) rule based layer type has a much more flexible structure than layers in classical architectures and allow to deal with _arbitrary input dimensions_, (2) the layers are easily integrable into existing architectures, and (3) the learned parameters, hence the model, is interpretable and can possibly be used to extract new knowledge from the data or to improve the existing rules.

Main ContributionsWe define a new type of neural network layer called rule based layer. This new layer can be integrated into arbitrary architectures making them dynamic, i.e., the structure of the network changes based on the input data and predefined rules. We prove that rule based layers generalize classical feed-forward layers such as fully connected and convolutional layers. Additionally, we show that rule based layers can be applied to graph classification tasks, by introducing RuleGNNs, a new type of graph neural networks. In this way we are able to extend the concept of dynamic neural networks to graph neural networks together with all the advantages of dynamic neural networks, e.g., that RuleGNNs are by definition permutation equivariant and able to handle graphs

Figure 1: Visualization of the learnable parameters of our RuleGNN on DHFR (a) and IMDB-BINARY (b) for three different graphs. Positive weights are denoted by red arrows and negative weights by blue arrows. The arrow thicknesss and color corresponds to the absolute value of the weight. The bias is denoted by the size of the node. The second image of (a) resp. (b) shows the weights the \(10\) resp. \(5\) largest positive and negative weights.

of arbitrary sizes. Considering various real-world graph datasets, we demonstrate that RuleGNNs are competitive with state-of-the-art graph neural networks and other graph classification methods. Using synthetic graph datasets we show that "expert knowledge" is easily integrable into our neural network and also necessary for classification1

The rest of the paper is organized as follows: We introduce the concept of rule based layers in Section 2 and prove in Section 3 that rule based layers generalize fully connected and convolutional layers. In Section 4 we present RuleGNNs and apply them in Section 5 to different benchmark datasets and compare the results with state-of the art graph neural networks. Finally, we discuss limitations, related work and conclude the paper in Section 6.

## 2 Rule Based Learning

Introducing the concept of rule based learning we first present some basic definitions followed by the formal definition of rule based layers.

PreliminariesFor some \(n\) we denote by \([n]\) the set \(\{1,,n\}\). A neural network is denoted by a function \((-,):^{n}^{m}\) with the learnable parameters \(\). We extend this notation introducing an additional parameter \(\), that is the set of formal rules \(=\{^{1},,^{k}\}\). The exact definition of these rules is given in the next paragraph. Informally, a rule \(\) is a function that determines the distribution of the weights in the weight matrix or the bias vector of a layer. A rule \(\) is called _dynamic_ if it is a function in the input samples \(x^{n}\) otherwise it is called _static_. An example of a static rule is the one used to define convolutional layers, see Proposition 2. An example of a dynamic rule can be found in Section 4. In our setting, a neural network is a function \((-,,):^{*}^{*}\) that depends on a set of learnable parameters denoted by \(\) and some rule set \(\) derived from expert knowledge or additional information. The notation \(*\) in the domain and codomain of \(\) indicates that the input and output can be of arbitrary or variable dimension. As usual \(\) is a concatenation of sub-functions \(f^{1},,f^{l}\) called the layers of the neural network. More precisely, the \(i\)-th layer is a function \(f^{i}(-,^{i},^{i}):^{*}^{*}\) where \(^{i}\) is a subset of the learnable parameters \(\) and \(^{i}\) is an element of the rules \(\). We call a layer \(f^{i}\)_static_ if \(^{i}\) is a static rule and _dynamic_ if \(^{i}\) is a dynamic rule. The input data is a triple \((,,)\), where \(=\{x_{1},x_{k}\}\) with \(x_{i}^{*}\) is the set of examples drawn from some unknown distribution. The labels are denoted by \(=(y_{1},y_{k})\) with \(y_{i}^{*}\) and \(\) is some additional information known about the input data \(\). This can be for example knowledge about the graph structure, node or edge labels, importance of neighborhoods and many more. One main assumption of this paper is that \(\) can be used to derive a set of static or dynamic rules \(\). Again we would like to mention that we concentrate on the analysis of the effects of applying different rules \(\) and not on the very interesting but also wide field of deriving the best rules \(\) from \(I\), see some discussion in Section 6. Nonetheless, we always motivate the choice of the rules derived by \(\).

Rule Based LayersWe now give a formal definition of rule based layers. Given some dataset \((,,)\) defined as before and the rule set \(\) derived from \(\), the task is to learn the weights \(\) of the neural network \(\) to predict the labels of unseen examples drawn from an unknown distribution. Our contribution concentrates on single layers and is fully compatible with other layers such as linear layers, convolutional layers Hence, in the following we restrict to the \(i\)-th layer \(f^{i}(-,^{i},^{i}):^{*}^{*}\) of a network \(\). For simplicity, we assume \(i=1\) and omit the indices, i.e., we write \(f f^{i}\), \(^{i}\) and \(^{i}\). The forward propagation step of the rule based layer \(f\) which will be a generalization of certain known layers as shown in Section 3 is as follows. Fix some input sample \(x\) with \(x^{n}\). Then \(f(-,,):^{n}^{m}\) for \(n,m\) is given by

\[f(x,,)=(W_{^{W}(x)} x+b_{_{b}(x )}).\] (1)

Here \(\) denotes an arbitrary activation function and \(W_{^{W}(x)}^{m n}\) rsp. \(b_{_{b}(x)}^{m}\) is some weight matrix rsp. weight vector depending on the input vector \(x\) and the rule \(\). The set \(\{w_{1},,w_{N},b_{1},,b_{M}\}\) consists of all possible learnable parameters of the layer. The parameters \(\{w_{1},,w_{N}\}\) are possible entries of the weight matrix while \(\{b_{1},,b_{M}\}\) are possible entries of the bias vector. The key point here is that the rule \(\) determines the choices and the positions of the weights from \(\) in the weight matrix \(W_{^{W}(x)}\) and the bias vector \(b_{_{b}(x)}\) depending on the input sample \(x\). More precisely, _not_ all learnable parameters must be used in the weight matrix and the bias vector for some input sample \(x\). Note that for two samples \(x,y\) of different dimensionality, e.g., \(x^{n}\) and \(y^{k}\) with \(n k\) the weight matrices \(W_{_{W}(x)}\) and \(W_{_{W}(y)}\) also have different dimensions and the learnable parameters can be in totally different positions in the weight matrix. This is where the rules \(\) and their associated rule functions, see (2) below, come into play.

Given the set of learnable parameters \(\{w_{1},,w_{N},b_{1},,b_{M}\}\), for each input \(x^{n}\) the rule \(\) induces the following two rule functions

\[_{W}(x):[m][n]\{0\}[N] _{b}(x):[m]\{0\}[M]\] (2)

where \(m\) is the output dimension of the layer that can also depend on \(x\). In the following we abbreviate \(_{W}(x)(i,j)\) by \(_{W}(x,i,j)\) and \(_{b}(x)(i)\) by \(_{b}(x,i)\). We note that for simplicity we assume that the matrix and vector indices start at \(1\) and not at \(0\). Using the associated rule functions (2) we can construct the weight matrix resp. bias vector by defining the entry \((i,j)^{m n}\) in the \(i\)-th row and the \(j\)-th column of the weight matrix \(W_{(x)}^{m n}\) via

\[W_{_{W}(x)}(i,j) :=0&_{W}(x,i,j)=0\\ w_{_{W}(x,i,j)}&\] (3)

and the entry at position \(k\) in the bias vector \(b_{_{b}(x)}^{m}\) by

\[b_{_{b}(x)}(k) :=0&_{b}(x,k)=0\\ b_{_{b}(x,k)}&.\] (4)

Summarizing, the _rule based layer_ defined in (1) is a standard feed-forward layer with the difference that the weights in the weight matrix and the bias vector are determined by a predefined rule \(\). In fact, weight matrix and bias vector depend on the input and can contain shared weights. More precisely, the rule controls the connection between the \(i\)-th input and the \(j\)-th output feature in the weight matrix. A rule \(\) is called _static_ if it is independent of the input \(x\), i.e., \((x)(y)\) for all inputs \(x,y\) otherwise it is called _dynamic_. We call a rule based layer as defined in (1) _static_ if it is based on a static rule \(\) and _dynamic_ otherwise. We will show in Section 3 that rule based layers generalize known concepts of neural network layers for specific rules \(\). In fact, we show that fully connected layers and convolution layers are static rule based layers. Examples of dynamic rule based layers are given later on in Section 4. The back-propagation of such a layer can be done as usual enrolling the computation graph of the forward step and applying iteratively the chain rule to all the computation steps. We will not go into the details of this computation as it is similar to many other computations using backpropagation with shared weights. For the experiments we use the automatic backpropagation tool of PyTorch  which fully meets our requirements.

Assumptions and ExamplesRule based learning relies on the following two main assumptions: \(A1)\) There is a connection between the additional information or expert knowledge \(\) and the used rule \(\) and \(A2)\) The distribution of weights given by the rule \(\) in the weight matrix \(W_{(x)}\) improves the predictive performance or increases the interpretability of the neural network. As stated before we concentrate on the second assumption and consider different distribution of weights in the weight matrix given by different rules. In fact, we assume without further consideration that it is possible to derive a meaningful ruleset \(\) from the additional information or expert knowledge \(\). For example if the dataset consists of images we can derive the "informal" rule that neighboured pixels are more important than pixels far away and in case of chemical data there exists, e.g., the ortho-para rule for benzene rings that makes assumptions about the influence of atoms for specific positions regarding the ring. This rule was already learned by a neural network in . It is another very interesting task which is beyond the scope of this work how to formalize these "informal" rules or to learn the "best" formal rules from the additional information \(\).

In the following sections we focus on the concept of rule based layers and therefore for simplicity and space reasons only consider the rule function of weight matrices. The rule function associated with the bias vector can be constructed similarly. For simplicity, we write \(\) instead of \(_{W}\).

Theoretical Aspects of Rule Based Layers

In this section we provide a theoretical analysis of rule based layers and show that they generalize fully connected and convolutional layers. More precisely, we define two _static_ rules \(_{}\) and \(_{}\) and show that the rule based layer as defined in (1) based on \(_{}\) is a fully connected layer and the rule based layer based on \(_{}\) is a convolutional layer. All the proofs can be found in the Appendix A.

**Proposition 1**: _Let \(f:^{n}^{m}\) with_

\[f(y,,_{})=(W_{_{}(x)}  y)\]

_be a rule based layer of a neural network as defined in (1) (without bias term) with learnable parameters \(=\{w_{1},,w_{n m}\}\) and \(y=^{i}(x)\) is the result of the first \(i-1\) layers. Then for the rule function \(_{}(x):[m][n][m n]\) defined for all inputs \(x\) as follows_

\[_{}_{}(x)(i,j)(i -1) n+j,\]

_the rule based layer \(f\) is equivalent to a fully connected layer with activation function \(\)._

Proposition 1 shows that rule based layers generalize fully connected layers of arbitrary size without bias vector and can be easily adapted to include the bias vector. Hence, this shows that rule based layers generalize arbitrary fully connected layers. Moreover, fully connected layers are static rule based layers as the rule \(_{}\) is static because it does not depend on the particular input \(x\).

**Proposition 2**: _Let \(f:^{n m}^{(n-N+1)(m-N+1)}\) with_

\[f(y,,_{})=(W_{_{}(x)}  y)\]

_be a rule based layer of a neural network as defined in (1) (without bias term) and \(W^{i}=\{w_{1},,w_{N^{2}}\}\) be the set of learnable parameters. Then for the rule function \(_{}:[(n-N+1)(m-N+1)][n m][N ^{2}]\) defined by_

\[_{}_{}(x)(i,j) (i,j)&\,0<(i,j)<N n\\ & 0<j\;( \;n)-j+(i,j)<N\\ 0&\]

_with \((i,j)=(i,j)-(((i,j)-1)/n)(n-N)\) and \((i,j)=j-((i-1)//(n-N+1)) n+(i-1)\;(\;(n-N+1))\)_

_the rule based layer \(f\) is equivalent to a convolution layer with quadratic kernel of size \(N\) (\(N<n\), \(N<m\)) and a stride of one over a two-dimensional image of size \(n m\) (without padding and bias vector) with activation function \(\). The notation \(a//b\) denotes the integer division of two integers \(a\) and \(b\)._

Proposition 2 shows that rule based layers generalize 2D-image convolution without padding and bias term. By adaption of the rule function it is possible to include the bias vector and padding. Moreover, the result can be generalized to higher dimensions kernels, non-quadratic kernels and arbitrary input and output channels. Hence, rule based layers also generalize arbitrary convolutional layers. Convolutional layers are static rule based layers as the rule \(_{}\) is static because it is independent of the input. The following result is a direct implication from Propositions 1 and 2.

**Theorem 1**: _Rule based layers generalize fully connected and convolutional feed-forward layers. Moreover, both layers are static rule based layers._

We claim that also other types of feed-forward layers can be generalized by rule based layers using appropriate rule functions. Because of space limitations we would rather present a specific application of dynamic rule based layers on graphs.

## 4 Rule Based Learning on Graphs

One of the main advantages of rule based layers as introduced in this work is that they give rise to a dynamic neural network architecture that is freely configurable using different rules. In fact, the network is independent of the dimension and structure of the input samples. Hence, a natural application of our approach is graph classification. We would like to emphasize that graph classification is only one of many possible applications of rule based layers. Other possible applications are node classification, regression tasks, graph embeddings or completely different data-structures.

Graph PreliminariesBy a graph we mean a pair \(G=(V,E)\) with \(V\) denoting the set of nodes of \(G\) and \(E\{\{i,j\} i,j V\}\) the set of edges. We assume that the graph is undirected and does not contain self-loops or parallel edges. In case that it is clear from the context we omit \(G\) and only use \(V\) and \(E\). The distance between two nodes \(i,j V\) in a graph, i.e., the length of the shortest path between \(i\) and \(j\), is denoted by \(d(i,j)\). A labeled graph is a graph \(G=(V,E)\) equipped with a function \(l:V\) that assigns to each node a label from the set \(\). In this paper the input samples corresponding to a graph \((V,E)\) are always vectors of length equal to \(|V|\). In particular, the input vectors can be interpreted as signals over the graph and each dimension of the input vector corresponds to the one-dimensional input signal of a graph node.

### Graph Rules

The example on molecule graphs in Figure 2 and Appendix A.4 motivates the intuition behind different graph specific rules that can be used to define a graph neural network based on rule layers. The underlying general scheme to define a rule based layer on graphs is as follows: Let \(G=(V,E)\) be a graph and \(l:V\) a permutation equivariant labeling function of the nodes, i.e., for some permutation \(\) of \(V\) it holds \(l((V))=(l(V))\). Assuming that input and output dimension of the layer is equal to \(|V|\) the rule functions \(\) as defined in (2) map each pair of nodes \((i,j) V V\) to an integer which is the index of the learnable parameter in the set of all learnable parameters. The mapping is injective based on the labels \(l(i),l(j)\) and an additionally defined shared property between the nodes \(i\) and \(j\). Examples for such shared properties can be the distance between \(i\) and \(j\), the type of the edge connecting \(i\) and \(j\) or the information, that \(i\) and \(j\) are in one circle. As an example \(_{}\) as defined in Appendix A.4 is induced by the permutation equivariant function \(l\) that maps each node to its atom label and the shared property between two nodes is the type of the edge connecting the nodes or the absence of an edge. Besides \(_{}\) the simple rule that is based on the given node labels in this paper we focus on three different rule based layers for graphs.

**Proposition 3**: _Let \(\) be some permutation of the nodes of \(G=(V,E)\) and \(x\) its corresponding input vector. If \(\) permutation equivariant, i.e., \(((x))(i,j)=(x)((i),(j))\) then the rule based layer is also equivariant under node permutations, i.e., \(f((x),,_{})=(f(x,,_{}))\)._

Weisfeiler-Leman RuleRecent research has shown that the Weisfeiler-Leman labeling is a powerful tool for graph classification [18; 14; 2; 22]. Thus, we propose to use Weisfeiler-Leman labels as one option to define the rule based layer for graph classification. The Weisfeiler-Leman algorithm assigns in the \(k\)-th iteration to each node of a graph a label based on the structure of its local \(k\)-hop neighborhood, see . Let \(l(v)\) be the result of the \(k\)-th iteration of the Weisfeiler-Leman algorithm for some node \(v V\). Then the Weisfeiler-Leman Rule \(_{WL_{k,d}}\) assigns to each node pair \((i,j)\) an integer or zero based on the Weisfeiler-Leman labels \(l(i),l(j)\) and the distance between the nodes \(i\) and \(j\). The result is zero if the distance between \(i\) and \(j\) is not between \(1\) and \(d\). Note that we are not restricted to look at consecutive distances from \(1\) to \(d\). It is also possible to look at certain distances only if the expert knowledge suggests it. In fact, \((i,j)\) and \((k,l)\) are mapped to the same integer if and only if \(l(i)=l(k),l(j)=l(l)\) and the distance between \(i\) and \(j\) is equal to the distance between \(k\) and \(l\). The layer defined by this rule is related to ordinary message passing but messages can pass between nodes of arbitrary distances. For computational reasons in the experiments we restrict the maximum number of different Weisfeiler-Leman labels considered by some bound \(L\). We relabel the most frequent \(l-1\) labels to \(1,,l-1\) and set all other labels to \(l\). The corresponding layer is denoted by \(f_{WL_{k,d,L}}\).

Pattern Counting RuleBeyond labeling nodes via the Weisfeiler-Leman algorithm, it is a common approach to use subgraph isomorphism counting to distinguish graphs . This is in fact necessary as the 1-Weisfeiler-Leman algorithm is not able to distinguish some types of graphs, for example circular skip link graphs  and strongly regular graphs [2; 3]. Thus, we propose the pattern counting rule and show in Section 5 that RuleGNNs based on this rule are able to perform well on synthetic benchmark datasets while message passing models based on the Weisfeiler-Leman algorithm fail. In general, subgraph isomorphis counting is a hard problem , but for the real-world and synthetic benchmark graph datasets that are usually considered, subgraphs of size \(k\{3,4,5,6\}\) can be enumerated in a preprocessing step in a reasonable time, see Table 5. Given a set of patterns, say \(\), we compute all possible embeddings of these patterns in the graph dataset in a preprocessing step. Then for each pattern \(P\) and each node \(i V\) we count how often the node \(i\) is part of an embedding of \(P\). Using those counts we define a labeling function \(l:V\). Two nodes \(i,j V\) are mapped to the same label if and only if their counts are equal for all patterns in \(\). Patterns that are often used in practice are small cycles, cliques, stars, paths, etc. The Pattern Counting Rule \(_{_{d}}\) assigns each node pair \((i,j)\) an integer or zero based on the values of \(l(i),l(j)\) and the distance between \(i\) and \(j\). As for the Weisfeiler-Leman Rule we restrict the maximum number of different labels to some number \(L\). The corresponding layer is denoted by \(f_{_{d,L}}\).

Summary RuleThe summary rule \(_{}^{N}\) can be used as the output layer as its output is a fixed dimensional vector of size \(N\) independent of the size of the input data and the output is invariant under node permutations. Again, let \(l:V\) be a function that maps each node of a graph to some integer. Then the summary rule \(_{}^{N}\) assigns each pair \((n,i)\) with \(i V\) and \(n[N]\) an integer or zero based on \(n\) and \(l(i)\). In fact, for each element of \(\) the rule defines \(n\) different learnable parameters. The corresponding layer is denoted by \(f_{_{}^{N}}\).

All the above rules define dynamic rule based neural network layers because the weight matrix and bias terms defined by the rules depend on the input vectors \(x\) corresponding to different graphs. Note that the layers defined by the above rules are permutation equivariant as the node labeling function \(l\) used to define the rule is equivariant under node permutations. Thus, using the layers corresponding to the above defined rules we can build a graph classification architecture that by definition does not depend on the order of the nodes in the input graphs. Moreover, a layer is able to pass information between nodes of arbitrary distances in the graph. Thus, as shown in the experiments below, it is not necessary to use deep networks to achieve good performance on the real-world benchmark datasets.

### Rule Graph Neural Networks (RuleGNNs)

The layers derived from the above rules are the building blocks of the RuleGNNs. Each RuleGNN is a concatenation of different rule based layers from Weisfeiler-Leman rules and pattern counting rules followed by a summary rule using arbitrary activation functions. To define the learnable parameters of the bias term we also use the summary rule. The input of the network is a signal \(x^{|V|}\) corresponding to a graph \(G=(V,E)\). We note that for simplicity we focus on one-dimensional signals but also multidimensional signals, i.e., \(x^{|V| d}\) are possible. The output of the network is a vector of fixed size \(N\) determined by the summary rule where \(N\) is usually the number of classes of the graph classification task. The output can be also used as an intermediate vectorial representation of the graph or for regression tasks.

## 5 Experiments

We evaluate the performance of RuleGNNs on different real-world and synthetic benchmark graph dataset and compare the results to the state-of-the-art graph classification algorithms. For comparability and reproducibility of the results, also with future algorithms, we make use of the experimental setup from . That means, for each graph dataset we perform a \(10\)-fold cross validation, i.e., we use fixed splits of the dataset into \(10\) equally sized parts (the splits can be found in our repository), and use \(9\) of them for training, parameter tuning and validation. We then use the model that performs best on the validation set and report the performance on the previously unseen test set. We train the best model \(3\) times and average the results on each fold to decrease random effects. The standard deviation reported in the tables is computed over the results on the \(10\) folds.

Data and Algorithm SelectionA problem of several heavily used graph benchmark datasets like MUTAG or PTC  is that node and edge labels seems to be more important than the graph structure itself, i.e., there is no significant improvement over simple baselines . Moreover, in case of MUTAG the performance of the model is highly dependent on the data split because of the small number of samples. Thus, in this work for benchmarking we choose DHFR, Mutagenicity, NCI1, NCI109, IMDB-BINARY and IMDB-MULTI from the TU Dortmund Benchmark Graphs repository  because the structure of the graphs seems to play an important role, i.e., the simple baselines presented in [17; 7] are significantly worse than the state-of-the-art graph classification algorithms. Additionally, we consider circular skip link graphs CSL  and constructed some new synthetic benchmark graph datasets called LongRings, EvenOddRings and Snowflakes  to show the advantages of RuleGNNs on more complex graph structures with given expert knowledge. For more details on the datasets see Appendix A.5. For NCI1, IMDB-BINARY and IMDB-MULTI we use the same splits as in  and for CSL we use the splits as in  and a \(5\)-fold cross validation. We evaluate the performance of the RuleGNNs on these datasets and compare the results to the baselines from  and  and the Weisfeiler-Leman subtree kernel (WL-Kernel)  which is one of the best performing graph classification algorithm besides the graph neural networks. For comparison with state-of-the-art graph classification algorithms we follow  and compare to DGCNN , GIN  and GraphSAGE . Additionally, we compare to the results of some newer state-of-the-art graph classification algorithms . For the latter we use the results from the respective papers that might be obtained with different splits of the datasets.

Experimental Settings and ResourcesAll experiments were conducted on a AMD Ryzen 9 7950X 16-Core Processor with \(128\) GB of RAM. For the competitors we use the implementations from . For the real-world datasets we were not aware of expert-knowledge, hence we tested different rules and combinations of the layers defined in Section 4.1. More details on the tested hyperparameters can be found in Appendix A.7. We always use tanh for activation and the Adam optimizer  with a learning rate of \(0.05\) (real-world datasets) resp. \(0.1\) (synthetic datasets). For the real-world datasets the learning rate was decreased by a factor of \(0.5\) after each \(10\) epochs. For the loss function we use the cross entropy loss. All models are trained for \(50\) (real-world) resp. \(200\) (synthetic) epochs and the batch size was set to \(128\). We stopped if the validation accuracy did not improve for \(25\) epochs.

Real-World DatasetsThe results on the real-world datasets (Table 1) show that RuleGNNs are able to outperform the state-of-the-art graph classification algorithms in the setting of  even if we add all the additional label information that RuleGNNs use to the input features of the graph neural networks (see the (features) results in Table 1). This shows that the structural encoding of the additional label information is crucial for the performance of the graph neural networks and not replicable by using more input features. Moreover, the results show that the Weisfeiler-Leman subtree kernel is the best performing graph classification algorithm on NC1, NCI109 and Mutagenicity. For IMDB-BINARY and IMDB-MULTI our approach performs worse than the state-of-the-art graph classification algorithms that are not evaluated within the same experimental setup.

Synthetic DatasetsThe results on the synthetic benchmark graph dataset show that RuleGNNs outperform the state-of-the-art graph classification algorithms if expert knowledge is available even in the case that message passing is enough to solve the task. In fact, CLS and Snowflakes are not solvable by the message passing model because they are not distinguishable by the 1-WL test. The results on LongRings show that long range dependencies can be easily captured by RuleGNNs and also dependencies between nodes of different distances as in case of the EvenOddRings dataset can be encoded by appropriate rules.

    & **NCI1** & **NCI09** & **Mutagenicity** & **DHFR** & **IMDB-B** & **IMDB-M** \\  Baseline (NoG)  & \(69.2 1.9\) & \(68.4 2.2\) & \(74.8 1.8\) & \(71.8 5.3\) & \(71.9 4.8\) & \(47.7 4.0\) \\ WL-Kernel & \(\) & \(\) & \(\) & \(83.5 5.1\) & \(71.8 4.5\) & \(51.9 5.6\) \\  DGCNN & \(76.4 1.7\) & \(73.0 2.4\) & \(77.0 2.0\) & \(72.6 3.1\) & \(69.2 3.0\) & \(45.6 3.4\) \\ DGCNN (features) & \(73.6 1.0\) & \(72.5 1.5\) & \(76.3 1.2\) & \(76.1 3.4\) & \(69.1 3.5\) & \(45.8 2.9\) \\ GraphSeg & \(76.0 1.8\) & \(77.1 1.8\) & \(79.8 1.1\) & \(80.7 4.5\) & \(68.8 4.5\) & \(47.6 3.5\) \\ GraphSeg (features) & \(79.4 2.2\) & \(78.6 1.6\) & \(80.1 1.3\) & \(82.4 3.9\) & \(69.7 3.1\) & \(46.6 4.8\) \\ GIN & \(80.0 1.4\) & \(79.7 2.0\) & \(81.9 1.4\) & \(79.1 4.4\) & \(71.2 3.9\) & \(48.5 3.3\) \\ GIN (features) & \(77.3 1.8\) & \(77.7 2.0\) & \(80.6 1.3\) & \(81.8 5.1\) & \(70.9 3.8\) & \(48.3 2.7\) \\  GSN (paper)  & \(83.5 2.3\) & - & - & - & \(77.8 3.3\) & \(54.3 3.3\) \\ CIN (paper)  & \(83.6 1.4\) & \(84.0 1.6\) & - & - & \(75.6 3.7\) & \(52.7 3.1\) \\ SIN (paper)  & \(82.7 2.1\) & - & - & - & \(75.6 3.2\) & \(52.4 2.9\) \\ PIN (paper)  & \(85.1 1.5\) & \(84.0 1.5\) & - & - & \(76.6 2.9\) & - \\ 
**RuleGNN** & \(82.8 2.0\) & \(83.2 2.1\) & \(81.5 1.3\) & \(\) & \(\) & \(\) \\   

Table 1: Test set performance of several state-of-the-art graph classification algorithms averaged over three different runs and \(10\) folds. The \(\) values report the standard deviation over the \(10\) folds. The overall best results are colored red and the best ones obtained for the fair comparison from  are in bold. The (features) variant of the algorithms uses the same information as the RuleGNN as input features additionally to node labels. The (paper) results are taken from the respective papers and might be obtained with different splits of the datasets.

Interpretability of the Rule Based LayersEach learnable parameter of RuleGNNs can be interpreted in terms of the importance of a connection between two nodes in a graph with respect to their labels and their shared property (in our case the distance). In Figures 1 and6 we see how the network has learned the importance of different connections between nodes for different distances and labels.

## 6 Related Work, Limitations and Concluding Remarks

Dynamic neural networks have been proven to be more efficient, have more representation power and better interpretability than static neural networks . Our approach can be seen as a sample dependent dynamic neural network as for each input sample the network structure is adapted. In contrast to other sample dependent dynamic neural networks [20; 24], our approach changes the layer structure based on a predefined rule instead of the whole architecture. The rule based layers of RuleGNNs use the Weissfeiler-Leman labeling algorithm and subgraph isomorphism counting which are both recently used concepts in graph classification algorithms [18; 3; 2; 1]. The challenge for graph neural networks is the heterogenicity of the input data and the lack of a fixed order of the input data.  proposes a dynamic neural network for graph classification that uses node and edge labels and is similar to our approach. In fact, they also show that their approach generalizes CNNs. In contrast, they do not provide a general scheme to encode expert knowledge into the network. Moreover, their approach is not able to encode long range dependencies in the graph using only one layer. There exist graph neural networks that have learned the ortho-para rule for molecules . While the additional information used in these algorithms is mostly hard-coded, we are able to integrate arbitrary rules.

Limitations_Input Features:_ So far we have only considered 1-dimensional input signals and node labels, i.e., our experimental results are restricted to graphs that have no multi-dimensional node features. Additionally, we have not considered edge features in our rules. In principle, multi-dimensional node features and edge labels can be handled by our approach with the cost of increased complexity. _Space:_ For each graph we need to precompute the pairwise distances and store the positions of the weights in the weight-matrix. This is a disadvantage for large and dense graphs as we need to store a large number of positions. For dense graphs the number of positions can be quadratic in the number of nodes. _Structure:_ To define a meaningful rule for a layer the input and output features need to be logically connected. Fortunately, this is the case for graphs but this fact can be a limitation for other structures. _Combinatorics:_ If it is not possible to define a formal rule given some informal expert knowledge the number of possible rules that have to be tested can be very large. Thus, it is an interesting question if it is possible to automatically learn a rule that captures the expert knowledge in the best way. _Implementation:_ As stated in  there is a "gap between theoretical & practical efficiency" regarding dynamic neural networks, i.e., common libraries such as PyTorch or TensorFlow are not optimized for dynamic neural networks.

Concluding RemarksWe have introduced a new type of neural network layer that dynamically arranges the learnable parameters in the weight matrices and bias vectors according to a formal rule. On the one hand our approach generalizes classical neural network components such as fully connected layers and convolutional layers. On the other hand we are able to apply rule based layers to the task of graph classification showing that expert knowledge can be integrated into the learning process. Moreover, our approach gives rise to a more interpretable neural network architecture as every learnable parameter is related to a specific connection between input and output features.

    & LongRings & EveOdRings & EveOdRingsCont & CSL & SnowRakes \\  Baseline (NoG)  & \(30.17 3.2\) & \(22.25 3.0\) & \(47.9 3.9\) & \(10.0 0.0\) & \(27.3 5.3\) \\ WL-Kernel  & \(100.0 0.0\) & \(26.83 4.2\) & \(47.8 4.3\) & \(10.0 0.0\) & \(27.9 4.1\) \\  DGCNN  & \(29.9 2.6\) & \(28.4 2.5\) & \(59.1 5.2\) & \(10.0 0.0\) & \(26.0 3.3\) \\ GraphSAGE  & \(29.8 2.8\) & \(24.9 2.7\) & \(51.3 1.9\) & \(10.0 0.0\) & \(25.0 1.8\) \\ GN  & \(32.0 3.1\) & \(26.8 2.5\) & \(51.0 3.7\) & \(10.0 0.0\) & \(24.5 2.2\) \\
**RuleGNN** & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Test set performance of several state-of-the-art graph classification algorithms averaged over three different runs and \(10\) folds. The \(\) values report the standard deviation over the \(10\) folds. The best results and our algorithm are highlighted in bold.