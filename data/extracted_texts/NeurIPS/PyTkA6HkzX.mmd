# Controlling Counterfactual Harm in

Decision Support Systems Based on Prediction Sets

 Eleni Straitouri

Max Planck Institute for

Software Systems

Kaiserslautern, Germany

estraitouri@mpi-sws.org &Suhas Thejaswi

Max Planck Institute for

Software Systems

Kaiserslautern, Germany

thejaswi@mpi-sws.org &Manuel Gomez Rodriguez

Max Planck Institute for

Software Systems

Kaiserslautern, Germany

manuelgr@mpi-sws.org

###### Abstract

Decision support systems based on prediction sets help humans solve multiclass classification tasks by narrowing down the set of potential label values to a subset of them, namely a prediction set, and asking them to always predict label values from the prediction sets. While this type of systems have been proven to be effective at improving the average accuracy of the predictions made by humans, by restricting human agency, they may cause harm--a human who has succeeded at predicting the ground-truth label of an instance on their own may have failed had they used these systems. In this paper, our goal is to control how frequently a decision support system based on prediction sets may cause harm, by design. To this end, we start by characterizing the above notion of harm using the theoretical framework of structural causal models. Then, we show that, under a natural, albeit unverifiable, monotonicity assumption, we can estimate how frequently a system may cause harm using only predictions made by humans on their own. Further, we also show that, under a weaker monotonicity assumption, which can be verified experimentally, we can bound how frequently a system may cause harm again using only predictions made by humans on their own. Building upon these assumptions, we introduce a computational framework to design decision support systems based on prediction sets that are guaranteed to cause harm less frequently than a user-specified value using conformal risk control. We validate our framework using real human predictions from two different human subject studies and show that, in decision support systems based on prediction sets, there is a trade-off between accuracy and counterfactual harm.

## 1 Introduction

The principle of "first, do no harm" holds profound significance in a variety of professions across multiple high-stakes domains. For example, in the field of medicine, doctors swear an oath to prioritize their patient's well-being or, in the legal justice system, preserving the innocence of individuals is paramount. As a result, in all of these domains, rules and guidelines have been established to prevent decision makers--doctors or judges--from making decisions that harm individuals--patients or suspects. In recent years, it has been increasingly argued that a similar principle should apply to decision support systems using machine learning algorithms in high-stakes domains .1

The definition of harm is not unequivocally agreed upon, however, the most widely accepted definition is the counterfactual comparative account of harm (in short, counterfactual harm) , which we adopt in our work. Under this definition, an action causes harm to an individual if they would havebeen in a worse state had the action been taken. Building upon this definition, we say that a decision support system causes harm to an individual if a decision maker would have made a worse decision about the individual had they used the system.

In machine learning for decision support, one of the main focus has been classification tasks. Here, the most studied setting assumes the decision support system uses a classifier to predict the value of a (ground-truth) label of interest and a human expert uses the predicted value to update their own prediction . Unfortunately, in this setting, it is yet unclear how to guarantee that the (average) accuracy of the predictions made by an expert who uses the system is higher than the accuracy of the predictions made by the expert and the classifier on their own, what is often referred to as human-AI complementarity . In this context, a recent line of work  have argued, both theoretically and empirically, that an alternative setting may enable human-AI complementarity. In this alternative setting, the decision support system helps a human expert by providing a set of label predictions, namely a prediction set, and asking them to always predict a label value from the set. The key principle is that, by restricting human agency, good performance does not depend on the expert developing a good sense of when to predict a label from the prediction set. In this context, it is also worth noting that Google has recently developed a tool that uses patient history and skin condition images to provide decision support using prediction sets , and a study by Jain et al.  has found that physicians and nurses using this tool improved diagnoses for \(1\) in every \(8\) to \(10\) cases.

In this work, we argue that the same principle that enables human-AI complementarity on decision support systems based on prediction sets may also cause counterfactual harm--a human expert who has succeeded at predicting the label of an instance on their own may have failed had they used these systems. Consequently, our goal is to design decision support systems based on prediction sets that are guaranteed to cause, in average, less counterfactual harm than a user-specified value.

**Our contributions.** We start by formally characterizing the predictions made by a decision maker using a decision support system based on prediction sets using a structural causal model (SCM) and, based on this characterization, formalize our notion of counterfactual harm. In general, since counterfactual harm lies within level three in the "ladder of causation" , it is not (partially) identifiable--it cannot be estimated (bounded) from data. However, we show that, under a natural counterfactual monotonicity assumption on the predictions made by decision makers using decision support systems based on prediction sets, counterfactual harm is identifiable. Further, we show that, under a weaker interventional monotonicity assumption, which can be verified experimentally, the average counterfactual harm is partially identifiable. Then, building upon these assumptions, we develop a computational framework to design decision support systems based on prediction sets that are guaranteed to cause, in average, less counterfactual harm than a user-specified value using conformal risk control . Finally, we validate our framework using real human predictions from two different human subject studies and show that, in decision support systems based on prediction sets, there is a trade-off between accuracy and counterfactual harm.

**Further related work.** Our work builds upon further related work on set-valued predictors, critiques of prediction optimization, counterfactual harm and algorithmic triage.

Set-valued predictors output a set of label values, namely a prediction set, rather than single labels . However, set-valued predictors have not been typically designed nor evaluated by their ability to help human experts make more accurate predictions . Only very recently, an emerging line of work has shown that conformal predictors, a specific type of set-valued predictors, may help human experts make more accurate predictions . Within this line of work, the work most closely related to ours is by Straitouri et al. , which has introduced the setting, and counterfactual and interventional monotonicity properties we build upon.

Prediction optimization has been recently put into question in the context of decision support . More specifically, it has been argued that optimizing decision support systems to improve prediction accuracy does not always translate to better decision-making. Our work aligns with this critique since we argue that improving prediction accuracy may come at the cost of counterfactual harm.

The literature on counterfactual harm in machine learning is still quite small and has focused on traditional machine learning settings in which machine learning models replace human decision makers and make automated decisions . Within this literature, the work most closely related to ours is by Richens et al. , which also uses a structural causal model to define counterfactual harm. However, their definition of counterfactual harm differs in a subtle, but important, way from ours. Inour setting, their definition of counterfactual harm would compare the accuracy of a factual prediction made by an expert using the decision support system against the counterfactual prediction that the same expert would have made on their own. That means, under their definition, one would need to deploy the system to estimate the harm it may cause, potentially causing harm. In contrast, under our definition, one does not need to deploy the system to estimate (or bound) the harm it may cause, as it will become clear in Section 4, and thus we argue that our definition may be more practical.

Learning under algorithmic triage seeks to develop classifiers that make predictions for a given fraction of the samples and leave the remaining ones to human experts, as instructed by a triage policy [37; 38; 39; 40; 41; 42]. In contrast, in our work, for each sample, a classifier is used to construct a prediction set and a human expert needs to predict a label value from the set. In this context, it is also worth noting that learning under algorithmic triage has been extended to reinforcement learning settings [43; 44; 45; 46].

## 2 Decision support systems based on prediction sets

We consider a multiclass classification task in which, for each task instance, a human expert has to predict the value of a ground-truth label \(y=\{1,,L\}\). Then, our goal is to design a decision support system \(: 2^{}\) that, given a set of features \(x\), helps the expert by narrowing down the label values to a subset of them \((x)\), namely a prediction set. Here, we focus on a setting in which the system asks the expert to _always_ predict a label value \(\) from the prediction set \((x)\). Note that, by restricting the expert's agency, good performance does not depend on the human expert developing a good sense of when to predict a label from the prediction set. Moreover, we assume that the set of features, the ground-truth label and the expert's prediction are sampled from an unknown fixed distribution2, _i.e._, \(x,y P(X,Y)\) and \( P(\,|\,X,Y,(X))\).

Further, similarly as in Straitouri et al. [17; 18], we consider that, given a set of features \(x\), the system constructs the prediction set \((x)\) using the following set-valued predictor . First, the set-valued predictor ranks each potential label value \(y\) using the softmax output of a pre-trained classifier \(m_{y}(x)\). Then, given a user-specified threshold \(\), it uses the resulting ranking to construct the prediction set \((x)=_{}(x)\) as follows:

\[_{}(x)=\{y_{(i)}\}_{i=1}^{k},k=1+_{j=2}^{L} \{m_{y_{(j)}}(x) 1-\},\] (1)

where \(_{(i)}\) denotes the \(i\)-th label value in the ranking. Here, note that, for \(=0\), the prediction set contains just the top ranked label value, for \(=1\), it contains all label values.3

Given the above parameterization, one may just focus on finding the optimal threshold \(^{*}\) under which the human expert achieves the highest average accuracy as in Straitouri et al. [17; 18], _i.e._,

\[^{*}=*{argmax}_{}A(),A()=_{X,Y P(X,Y),\  P(\,|\,X,Y,_{}(X))}[\{= Y\}].\] (2)

However, this focus does not prevent the resulting system \(_{}\) from causing harm--an expert may succeed to predict the value of the ground-truth label on their own on instances in which they would have failed had they used \(_{}\). In this work, our goal is to design a computational framework that, given a user-specified bound \(\), finds the set of \(\) values which are all guaranteed to cause less harm, in average, than the bound \(\).

**Remark.** We would like to clarify that, if one sets the value of the threshold \(\) to be roughly the \(1-\) quantile of the empirical distribution of the scores \(1-m_{y}(x)\) in a calibration set, then, the set valued predictor defined by Eq. 1 is equivalent to a vanilla conformal predictor with nonconformity scores \(1-m_{y}(x)\) and coverage \(1-\). Under this view, it becomes apparent that, by searching for \(\) values in \(\) that are harm controlling, we are essentially searching for vanilla conformal predictors that are harm controlling. In this context, we would like to further clarify that our framework is agnostic to the choice of nonconformity score or more generally, the set-valued predictor, used to construct the prediction sets [47; 48; 49]. Motivated by this observation, in Appendix E, we include additional experiments where we evaluate our framework using a more complex set-valued predictor .

## 3 Counterfactual harm of decision support systems

To formalize our notion of harm, we characterize how human experts make predictions using a decision support system \(\) via a structural causal model (SCM) , which we denote as \(\). More specifically, similarly as in Straitouri et al. , we define \(\) by the following set of assignments:

\[X=f_{X}(V), Y=f_{Y}(V),_{}(X)=f_{}( ,X),=f_{}(U,V,_{}(X)),\] (3)

where \(\), \(U\) and \(V\) are exogenous random variables and \(f_{X}\), \(f_{Y}\), \(f_{}\) and \(f_{}\) are given functions.4 The exogenous variables \(\), \(U\) and \(V\) characterize the user-specified threshold, the expert's individual characteristics and the data generating process, respectively. The function \(f_{}\) is directly defined by Eq. 1, _i.e.,_\(f_{}(,X)=_{}(X)\). Further, as argued elsewhere , we can always find a distribution for the exogenous variables \(\), \(U\) and \(V\) as well as a functional form for the functions \(f_{X}\), \(f_{Y}\) and \(f_{}\) such that the distributions of the features, the ground-truth label and the expert's prediction introduced in Section 2 are given by the observational distribution entailed by the SCM \(\). For ease of exposition, we assume that, under no interventions, the distribution of the exogenous variable \(\) is \(P()=\{=1\}\) and thus human experts make predictions on their own. Figure 1 shows a visual representation of our SCM \(\).

Building upon the above characterization, we are now ready to formalize the following notion of counterfactual harm, which essentially compares the accuracy of a factual prediction made by an expert on their own against the counterfactual prediction that the same expert would have made had they used a decision support system \(_{}\):

**Definition 1** (Counterfactual harm): _For any \(x,y,^{}\), the counterfactual harm that a decision support system \(_{}\) would have caused, if deployed, is given by5_

\[h_{}(x,y,)=_{^{, (=)}\,|\,=,X=x,Y=}[\{0, \{=y\}-\{=y\}\}],\] (4)

_where \((=)\) denotes a (hard) intervention on the exogenous variable \(\)._

Here, note that counterfactual harm can only be nonzero if the expert has made a successful prediction on their own, _i.e._, \(=y\). Otherwise, the expert's prediction \(\) could not have become worse had they used the decision support system \(_{}\).

Figure 1: Our structural causal model \(\). Circles represent endogenous random variables and boxes represent exogenous random variables. The value of each endogenous variable is given by a function of the values of its ancestors, as defined by Eq. 3. The value of each exogenous variable is sampled independently from a given distribution.

Given the above definition of counterfactual harm and a user-specified bound \(\), our goal is to find the largest harm-controlling set of values \(()\) such that, for each \(()\), it holds that the counterfactual harm is, in expectation across all possible instances, smaller than \(\), _i.e._,

\[()=\{\ |\ H()=_{X,Y,  P^{}(X,Y,)}[h_{}(X,Y,)]\}.\] (5)

At this point, we cannot expect to find the set \(()\) because counterfactual harm lies within level three in the "ladder of causation"  and thus it is not identifiable from observational data without further assumptions. However, in what follows, we will show that, under certain assumptions, the average counterfactual harm \(H()\) is (partially) identifiable, _i.e._, it can be estimated (bounded) using observational data.

**Comparison to Richens's definition of counterfactual harm.** Richens et al.  define counterfactual harm as follows:

\[h_{}(x,y,)=E_{ P^{|= ,X=x,Y=y()}}[\{0,\{=y\}-\{ {y}=y\}\}],\]

where \(x,y, P^{;do(=)}\) and \(=1\) is considered to be the _default action_ in the language of Richens et al. . This definition implicitly assumes that the system \(_{}\) is deployed and it compares the factual prediction \(\) made by an expert using the deployed system against the counterfactual prediction \(\) had the expert made on their own. On the contrary, our definition does not assume that the system \(_{}\) is deployed and instead it compares the factual prediction \(\) made by an expert on their own against the counterfactual prediction \(\) had the expert made using the system \(_{}\).

## 4 Counterfactual harm under counterfactual and interventional monotonicity

In this section, we analyze counterfactual harm \(h_{}(x,y,)\), as defined in Eq. 4, from the perspective of causal identifiability under two natural monotonicity assumptions--counterfactual monotonicity and interventional monotonicity. Both of these assumptions, which were first studied by Straitouri et al. , formalize the intuition that increasing the number of label values in a prediction set increases its difficulty.

Under counterfactual monotonicity, for any \(x\) and \(,^{}\) such that \(Y_{}(x)_{^{}}(x)\), if an expert has succeeded at predicting \(Y\) using \(_{^{}}\), they would have also succeeded had they used \(_{}\) and, conversely, if they have failed at predicting \(Y\) using \(_{}\), they would have also failed had they used \(_{^{}}\), while holding "everything else fixed". More formally, counterfactual monotonicity is defined as follows:

**Assumption 1** (Counterfactual Monotonicity): _Counterfactual monotonicity holds if and only if, for any \(x\) and any \(,^{}\) such that \(Y_{}(x)_{^{}}(x)\), we have that_

\[\{f_{}(u,v,_{}(x))=Y\} \{f_{}(u,v,_{^{}}(x))=Y\}\] (6)

_for any \(u P^{}(U)\) and \(v P^{}(V\,|\,X=x)\)._

Under interventional monotonicity, for any \(x\) and \(,^{}\) such that \(Y_{}(x)_{^{}}(x)\), the probability that experts succeed at predicting \(Y\) using \(_{}\) is equal or greater than using \(}_{^{}}\). More formally, interventional monotonicity is defined as follows6:

**Assumption 2** (Interventional Monotonicity): _Interventional monotonicity holds if and only if, for any \(x\), \(y\), and \(,^{}\) such that \(y_{}(x)_{^{}}(x)\), we have that_

\[P^{\,;\,(=)}(=Y\,|\,X=x,Y=y) P^{ \,;\,(=^{})}(=Y\,|\,X=x,Y=y),\] (7)

_where the probability is over the exogenous random variables \(U\) and \(V\) characterizing the expert's individual characteristics and the data generating process, respectively._

In what follows, we first show that, under the counterfactual monotonicity assumption, counterfactual harm is identifiable (we provide all proofs in Appendix B):

**Proposition 1**: _Under the counterfactual monotonicity assumption, for any \(x,y, P^{}\), the counterfactual harm that a decision support system \(_{}\) would have caused, if deployed, is given by_

\[h_{}(x,y,)=\{=y y_{ }(x)\}.\] (8)

As an immediate consequence, we can conclude that, under counterfactual monotonicity, the average counterfactual harm \(H()\), as defined in Eq. 5, is identifiable and can be estimated using observational data sampled from \(P^{}\). However, since the inequality condition of the counterfactual monotonicity assumption compares counterfactual predictions and thus cannot be experimentally verified, one should be cautious about using the above proposition to estimate counterfactual harm in high-stakes applications.

Next, we show that, under the interventional monotonicity assumption, the average counterfactual harm is partially identifiable:

**Proposition 2**: _Under the interventional monotonicity assumption, the average counterfactual harm \(H()\) that a decision support system \(_{}\) would have caused, if deployed, satisfies that_

\[_{X,Y, P^{}(X,Y,)}[ \{=Y Y_{}(X)\}] H()\] \[_{X,Y, P^{}(X,Y,)}[ \{=Y Y_{}(X)\}+\{  Y Y_{}(X)\}].\] (9)

Importantly, note that, in the above proposition, the lower bound on the left hand side of Eq. 9 matches the average counterfactual harm under the counterfactual monotonicity assumption and thus, holding "everything else fixed", the average counterfactual harm under interventional monotonicity is always greater or equal than the average counterfactual harm under counterfactual monotonicity. Moreover, further note that the inequality condition of the interventional monotonicity assumption compares interventional probabilities and thus we can experimentally verify it (see Appendix F), lending support to using the above proposition to bound average counterfactual harm in high-stakes applications.

## 5 Controlling counterfactual harm using conformal risk control

In this section, we develop a computational framework that, given a decision support system \(_{}\) and a user-specified bound \(\), aims to find the largest harm-controlling set \(()\), as defined in Eq. 5. In the development of our framework, we will first assume that counterfactual monotonicity holds and, later on, we will relax this assumption, and assume instead that interventional monotonicity holds.

Our framework builds upon the the idea of conformal risk control, which has been introduced very recently by Angelopoulos et al. . Given any monotone loss function \((_{}(X),Y)\) with respect to \(\) and a calibration set \(\{(X_{i},Y_{i})\}_{i=1}^{n}\), with \((X_{i},Y_{i}) P(X,Y)\), conformal risk control finds a value of \(\) under which the expected loss of a test sample \((X_{n+1},Y_{n+1})\;\;P(X,Y)\) does not exceed a user-specified bound \(\), _i.e._, \([(_{}(X_{n+1}),Y_{n+1})]\). However, in our framework, we re-define the loss \(\) so that it does not only depend on the prediction set and the label value but also on the expert's prediction on their own.

Under the counterfactual monotonicity assumption, we set the value of the loss \(\) using the expression of counterfactual harm in Eq. 8 and, using a similar proof technique as in Angelopoulous et al., first prove the following theorem:

**Theorem 1**: _Let \(=\{(X_{i},Y_{i},_{i})\}_{i=1}^{n}\) be a calibration set, with \((X_{i},Y_{i},_{i}) P^{}(X,Y,)\), \(\) be a user-specified bound, and_

\[()=\{\,:\,_{n}( )+\}_{n}()=^{n} \{_{i}=Y_{i} Y_{i}_{}(X_{i})\} }{n}.\] (10)

_If counterfactual monotonicity holds, a test sample \((X_{n+1},Y_{n+1},_{n+1}) P^{}(X,Y,)\) satisfies that_

\[[\{_{n+1}=Y_{n+1} Y_{n+1} _{()}(X_{n+1})\}],\]

_where the expectation is over the randomness in the calibration set used to compute the threshold \(()\) and the test sample._Then, we leverage the above theorem and the fact that, under counterfactual monotonicity, the counterfactual harm is nonincreasing with respect to \(\), as shown in Lemma 3 in Appendix B.5, to recover the largest harm-controlling set \(()\):

**Corollary 1**: _Let \(\) be a user-specified bound. Then, under the counterfactual monotonicity assumption, it holds that \(()=\{\,|\,()\}\), where \(()\) is given by Eq. 10._

Under the interventional monotonicity assumption, rather than directly controlling the counterfactual harm, we will control the upper bound given by Eq. 9. Consequently, rather than recovering the largest harm-controlling set \(()\), we will recover a harm-controlling set \(^{}()()\). However, since the expression inside the expectation of the upper bound is nonmonotone with respect to \(\), we cannot directly use it to set the value of the loss \(\) in conformal risk control. That said, since the first term of the expression is nonincreasing and the second term is nondecreasing, as shown in Lemmas 3 and 4 in Appendix B.5, we can apply conformal risk control separately for each term. For the first term, we use Theorem 1 because the term matches the counterfactual harm under the counterfactual monotonicity assumption. For the second term, we prove the following theorem:

**Theorem 2**: _Let \(=\{(X_{i},Y_{i},_{i})\}_{i=1}^{n}\) be a calibration set, with \((X_{i},Y_{i},_{i}) P^{}\), \([,1]\) be a user-specified bound, and_

\[()=\{\,:\,_{n}( )+\}_{n}()=^{n} \{_{i} Y_{i} Y_{i}_{}(X_{i}) \}}{n}.\] (11)

_If interventional monotonicity holds and \(\) exists, a test sample \((X_{n+1},Y_{n+1},_{n+1}) P^{}(X,Y,)\) satisfies that_

\[[\{_{n+1} Y_{n+1} Y_{n+1} _{()}(X_{n+1})\}],\]

_where the expectation is over the randomness in the calibration set used to compute the threshold \(()\) and the test sample \((X_{n+1},Y_{n+1},_{n+1})\)._

Finally, we leverage the above theorems and the fact that the first term inside the expectation of the upper bound of counterfactual harm in Eq. 9 is nonincreasing and the second term is nondecreasing with respect to \(\) to recover a harm-controlling set \(^{}()()\):

**Corollary 2**: _Let \(\) be a user-specified bound. Then, under the interventional monotonicity assumption, for any choice of \(^{}\), the set_

\[^{}()=\{\,|\,(^{} )(-^{})\},\]

_where \((^{})\) is given by Eq. 10 and \((-^{})\) is given by Eq. 11 satisfies that \(^{}()()\)._

Note that, in practice, the above corollaries can be implemented efficiently since, by definition, the functions \(_{n}()\) and \(_{n}()\) are piecewise constant functions of \(\) with \(n\) different pieces.

## 6 Experiments

In this section, we use data from two different human subject studies to: a) evaluate the average counterfactual harm caused by decision support systems based on prediction sets; b) validate the theoretical guarantees offered by our computational framework (_i.e._, Corollaries 1 and 2); c) investigate the trade-off between the average counterfactual harm caused by decision support systems based on prediction sets and the average accuracy achieved by human experts using these systems. In what follows, we assume that counterfactual monotonicity holds. In Appendix G, we conduct experiments where we relax this assumption and assume instead that interventional monotonicity holds.7

**Experimental setup.** We first experiment with the ImageNet16H dataset by Steyvers et al. , which comprises \(32\),\(431\) predictions made by \(145\) human participants on their own about noisy images created using \(1,200\) unique natural images from the ImageNet Large Scale Visual Recognition Challenge (ILSRVR) 2012 dataset . More specifically, each of the natural images was used to generate four noisy images with different amount of phase noise distortion \(\{80,95,110,125\}\) and with the same ground-truth label \(y\) from a label set \(\) of size \(L=16\). Here, the amount of phase noise controls the difficulty of the classification task--the higher the noise, the more difficult the classification task. In our experiments, we use (all) the noisy images with noise value \(\{80,95,110\}\) because, for the noisy images with \(=125\), humans perform poorly; moreover, we stratify these images (and human predictions) with respect to their amount of phase noise.

For each stratum of images, we apply our framework to decision support systems \(_{}\) with different pre-trained classifiers, namely VGG19 , DenseNet161 , GoogleNet , ResNet152  and AlexNet  after 10 epochs of fine-tuning, as provided by Steyvers et al. .8 To this end, we randomly split the images (and human predictions) into a calibration set (\(10\)%), which we use to find the harm-controlling sets \(()\) by applying Corollary 1, and a test set (\(90\)%), which we use to estimate the average counterfactual harm \(H()\) caused by the decision support systems \(_{}\) as well as the average accuracy \(A()\) of the predictions made by a human expert using \(_{}\). Here, since the dataset only contains predictions made by human participants on their own, we use the mixture of multinomial logit models (MNLs) introduced by Straitouri et al.  to estimate the average accuracy \(A()\). Refer to Appendix C for more details regarding the mixture of MNLs and to Appendix D for additional experiments studying the relationship between average counterfactual harm \(H()\), average prediction set size and empirical coverage9.

Then, we experiment with the ImageNet16H-PS dataset10 by Straitouri et al. , which comprises \(194,407\) predictions made by \(2,751\) human participants using decision support systems \(_{}\) about the

Figure 2: Average accuracy estimated by the mixture of MNLs against the average counterfactual harm for images with \(=110\). Each point corresponds to a \(\) value from \(0\) to \(1\) with step \(0.001\) and the coloring indicates the relative frequency with which each \(\) value is in \(()\) across random samplings of the calibration set. Each row corresponds to decision support systems \(_{}\) with a different pre-trained classifier with average accuracies \(0.846\) (VGG19), \(0.830\) (DenseNet), \(0.722\) (GoogleNet), \(0.727\) (ResNet152), and \(0.691\) (AlexNet). The average accuracy achieved by the simulated human experts on their own is \(0.771\). The results are averaged across \(50\) random samplings of the test and calibration set. In both panels, \(95\%\) confidence intervals are represented using shaded areas and always have width below \(0.02\).

set of noisy images with \(=110\) described above. More specifically, for each noisy image, the dataset contains human predictions made under any possible prediction set that can be constructed using Eq. 1 with (the softmax output of) VGG19 after 10 epochs of fine-tuning, as provided by Steyvers et al. . Here, similarly as in the ImageNet16H dataset, we randomly split the images (and human predictions) into a calibration set (10%), which we use to find \(()\), and a test set (90%), which we use to estimate \(H()\) and \(A()\). However, in this case, we can estimate \(A()\) using the predictions made by human participants using \(_{}\) from the dataset, and we can compare this empirical estimate to the one using the mixture of MNLs.

In both datasets, we calculate confidence intervals and validate the theoretical guarantees offered by Corollary 1 by repeating each experiment \(50\) times and, each time, sampling different calibration and test sets.

**Results.** Figure 2 shows the average accuracy \(A()\) achieved by a human participant using \(_{}\), as predicted by the mixture of MNLs, against the average counterfactual harm \(H()\) caused by \(_{}\) for \(\{0.01,0.05\}\) on the stratum of images with \(=110\) from the ImageNet16H dataset. Refer to Appendix D for results on other strata. Here, each point corresponds to a different \(\) value and its coloring indicates the empirical probability that \(\) is included in the harm-controlling set \(()\). The results show several interesting insights. First, we find that, as long as \(<1\), the decision support systems \(_{}\) always cause some amount of counterfactual harm11. This suggests that, while restricting human agency enables human-AI complementarity, it inevitably causes (some amount of) counterfactual harm. Second, we find that the sets of \(\) values provided by our framework are typically harm-controlling, _i.e._, they do not include \(\) values such that \(H()>\). However, the sets are often conservative and do not include _all_ the harm-controlling \(\) values due to estimation error in the the empirical estimate \(_{n}()\) of the average counterfactual harm using data from the calibration set. In Appendix D, we show that using larger calibration sets reduces the above mentioned estimation error and results in sets of \(\) values that are less conservative. Third, we find that there is a trade-off between accuracy and counterfactual harm and this trade-off is qualitatively consistent across decision support systems using different pre-trained classifiers. In fact, for \(=0.01\), the decision support systems \(_{^{*}}\) offering the greatest average accuracy \(A(^{*})\) are not harm-controlling.

Figure 3 shows the average accuracy \(A()\) achieved by a human participant using \(_{}\), as estimated using predictions made by human participants using \(_{}\), against the average counterfactual harm because the prediction sets always contain all label values and thus human participants always make predictions on their own.

Figure 3: Average accuracy estimated using predictions by human participants (Real) and using the mixture of MNLs (Predicted) against the average counterfactual harm for images with \(=110\). Each point corresponds to a \(\) value from \(0\) to \(1\) with step \(0.001\) and the coloring indicates the relative frequency with which the \(\) value is in \(()\) across random samplings of the calibration set. The decision support systems \(_{}\) use the pre-trained classifier VGG19. The results are averaged across \(50\) random samplings of the test and calibration set. In both panels, \(95\%\) confidence intervals are represented using shaded areas and always have width below \(0.02\).

\(H()\) caused by \(_{}\) for \(\{0.01,0.05\}\) on the ImageNet16H-PS dataset. Here, the meaning and coloring of each point is the same as in Figure 2. The results also support the findings derived from the experiments with the ImageNet16H dataset--the decision support systems \(_{}\) always cause some amount of harm, our framework succeeds at identifying harm-controlling sets, and there is a trade-off between accuracy and counterfactual harm. Further, they also show that, while there is a gap between the average accuracy estimated using the mixture of MNLs and the average accuracy estimated using predictions made by human participants using \(_{}\), they follow the same trend and support the same qualitative conclusions.

## 7 Discussion and limitations

In this section, we highlight several limitations of our work, discuss its broader impact, and propose avenues for future work.

**Assumptions.** We have assumed that the data samples and the expert predictions are drawn i.i.d. from a fixed distribution and the calibration set contains samples with noiseless ground truth labels and expert label predictions. It would be very interesting to extend our framework to allow for distribution shifts and label noise. Moreover, we have considered prediction sets constructed with a _fixed_ user-specified threshold value \(\). In light of recent work by Gibbs et al. , it would be interesting to extend our framework to allow for threshold values that depend on the data samples. Furthermore, in our definition of counterfactual harm we have treated all inaccurate predictions as equally harmful. Expanding our definition of harm to weigh different inaccurate predictions according to their consequences would be a very interesting avenue for future work. In addition, under the interventional monotonicity assumption, we have empirically observed that the gap between our lower- and upper-bounds is often large. Therefore, it would be useful to identify other natural assumptions under which this gap is smaller.

**Methodology.** In our framework, the prediction sets are constructed using the softmax output of a pre-trained classifier. However, we hypothesize that, by accounting for the similarity between the mistakes made by humans and those made by the pre-trained classifier, we may be able to construct prediction sets that cause counterfactual harm less frequently. Moreover, we have focused on controlling the average counterfactual harm. However, whenever the expert's predictions are consequential to individuals, this may lead significant disparities across demographic groups. Therefore, it would be important to extend our framework to account for fairness considerations.

**Evaluation.** Our experimental evaluation comprises a single benchmark dataset of noisy natural images and thus one may question the generalizability of the conclusions we draw from our results. To overcome this limitation, it would be important to further investigate the trade-off between accuracy and counterfactual harm in decision support systems based on prediction sets in real-world application domains (_e.g._, medical diagnosis).

## 8 Conclusions

In this paper, we have initiated the study of counterfactual harm in decision support systems based on prediction sets. We have introduced a computational framework that, under natural monotonicity assumptions on the predictions made by experts using these systems, can control how frequently these systems cause harm. Moreover, we have validated our framework using data from two different human subject studies and shown that, in decision support systems based on prediction sets, there is a trade-off between accuracy and counterfactual harm.