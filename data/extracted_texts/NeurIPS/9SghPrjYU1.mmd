# Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning

Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning

 Zhishuai Liu

Duke University

zhishuai.liu@duke.edu

&Pan Xu

Duke University

pan.xu@duke.edu

###### Abstract

Distributionally robust offline reinforcement learning (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces. However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation. Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust offline RL. Our results uncover that function approximation in robust offline RL is essentially distinct from and probably harder than that in standard offline RL. Our algorithms and theoretical results crucially depend on a novel function approximation mechanism incorporating variance information, a new procedure of suboptimality and estimation uncertainty decomposition, a quantification of the robust value function shrinkage, and a meticulously designed family of hard instances, which might be of independent interest.

## 1 Introduction

Offline reinforcement learning (RL) [17; 18], which aims to learn an optimal policy achieving maximum expected cumulative reward from a pre-collected dataset, plays an important role in critical domains where online exploration is infeasible due to high cost or ethical issues, such as precision medicine [49; 11; 22; 21] and autonomous driving [32; 43]. The foundational assumption of offline RL [18; 15; 53] is that the offline dataset is collected from the same environment where learned policies are intended to be deployed. However, this assumption can be violated in practice due to temporal changes in dynamics. In such cases, standard offline RL could face catastrophic failures [10; 31; 64]. To address this issue, the robust offline RL [28; 30] focuses on robust policy training against the environment perturbation, which serves as a promising solution. Existing empirical successes of robust offline RL rely heavily on expressive function approximations [37; 36; 25; 45; 63; 16], as the omnipresence of applications featuring large state and action spaces necessitates powerful function representations to enhance generalization capability of decision-making in RL.

To theoretically understand robust offline RL with function approximation, the distributionally robust Markov decision process (DRMDP) [39; 30; 13] provides an established framework. In stark contrast to the standard MDP, DRMDP specifically tackles the _model uncertainty_ by forming an uncertainty set around the nominal model, and takes a max-min formulation aiming to maximize the value function corresponding to a policy, uniformly across all perturbed models in the uncertainty set [55; 52; 57; 35; 41; 59; 40]. The core of DRMDPs lies in achieving an amenable combination of uncertainty set design and corresponding techniques to solve the inner optimization over the uncertainty set. However, this consideration of model uncertainty introduces fundamental challenges to function approximation in terms of computational and statistical efficiency, particularly given the need to maximally exploit essential information in the offline dataset. For instance, in cases where the state and action spaces are large, the commonly used \((s,a)\)-rectangular uncertainty set can make the inner optimization computationally intractable for function approximation . Additionally, the distribution shifts, arising from the mismatch between the behavior policy and the target policy, as well as the mismatch between the nominal model and perturbed models, complicate the statistical analysis . Several recent works attempt to conquer these challenges. Panaganti et al.  studied the \((s,a)\)-rectangularity, and their algorithm may suffer from the above mentioned computational issue. Additionally, the \((s,a)\)-rectangular uncertainty set may contain transitions that would never happen in reality, and thus leads to conservative policies; Blanchet et al.  proposed a novel double pessimism principle, while their algorithm requires strong oracles, which is not practically implementable. Meanwhile, a line of works study function approximation in the online setting  or with a simulator , which are not applicable to offline RL. Thus, the following question arises:

_Is it possible to design a computationally efficient and minimax optimal algorithm for robust offline RL with function approximation?_

To answer the above question, we focus on a basic setting of _\(d\)-rectangular linear DRMDP_, where the nominal model is a standard linear MDP, and all perturbed models are parameterized in a linearly structured uncertainty set. We provide the first _instance-dependent_ suboptimality analysis in the DRMDP literature with function approximation, which offers insights into the problem's intrinsic characteristics and challenges. Concretely, our contributions are summarized as follows.

* We propose a computationally efficient algorithm, Distributionally Robust Pessimistic Value Iteration (DRPVI), based on the pessimism principle  with a new _function approximation_ mechanism explicitly devised for \(d\)-rectangular linear DRMDPs. We show that DRPVI achieves the following instance-dependent upper bound on the suboptimality gap: \[_{1}_{P^{}(P^{0})}_{h=1}^{H} ^{^{*},P}_{i=1}^{d}\|_{i}(s_{h},a_{h})_{ i}\|_{_{h}^{-1}}|s_{1}=s^{1},\] This bound resembles those established in offline RL within standard linear MDPs . However, there are two significant differences in our results. First, our bound depends on the _supremum_ over the uncertainty set of transition kernels instead of one single transition kernel. Second, our result relies on a _diagonal-based normalization_, instead of the Mahalanobis norm of the feature vector, \(\|(s_{h},a_{h})\|_{_{h}^{-1}}\). See Table 1 for a clearer comparison. These two distinctions are unique to DRMDPs with function approximation, which we discuss in more details in Section 4. Moreover, our analysis provides a novel pipeline for studying instance-dependent upper bounds of computationally efficient algorithms under \(d\)-rectangular linear DRMDPs.
* We improve DRPVI by incorporating _variance information_ into the new function approximation mechanism, resulting in the VA-DRPVI algorithm, which achieves a smaller upper bound: \[_{2}_{P^{}(P^{0})}_{h=1}^{H} ^{^{*},P}_{i=1}^{d}\|_{i}(s_{h},a_{h})_ {i}\|_{_{h}^{-1}}|s_{1}=s^{2}.\] This improves the result of DRPVI due to the fact that \(_{h}^{*-1} H^{2}_{h}^{-1}\) by definition . Furthermore, when the uncertainty level \(=O(1)\), we show that the robust value function attains a _Range Shrinkage_ property, leading to an improvement in the upper bound by an order of \(H\). This explicit improvement is new in variance-aware algorithms, and is unique to DRMDPs.
* We further establish an _information-theoretic lower bound_. We prove that the upper bound of VA-DRPVI matches the information-theoretic lower bound up to \(_{2}\), which implies that VA-DRPVI is near-optimal in the sense of information theory. Importantly, both DRPVI and VA-DRPVI are computationally efficient and do not suffer from the high computational burden, as discussed above in settings with the \((s,a)\)-rectangular uncertainty set, due to a decoupling property of the \(d\)-rectangular uncertainty set (see Remark 4.1 for more details). Thus, we confirm that, for robust offline RL with function approximation, both the _computational efficiency_ and _minimax optimality_ are achievable under the setting of \(d\)-rectangular linear DRMDPs.

Our algorithm design and theoretical analysis draw inspiration from two crucial ideas proposed in standard linear MDPs: the reference-advantage decomposition  and the variance-weighted ridge regression . However, the unique challenges in DRMDPs necessitate novel treatments that go far beyond a combination of existing techniques. Specifically, existing analysis of standard linear MDPs highly relies on the linear dependency of the Bellman equation on the (nominal) transition kernel. This linear dependency is disrupted by the consideration of model uncertainty, which induces essential _nonlinearity_ that significantly complicates the statistical analysis of estimation error. To obtain our instance-dependent upper bounds, we establish a new theoretical analysis _pipeline_. This pipeline starts with a nontrivial decomposition of the suboptimality, and employs a new uncertainty decomposition that transforms the estimation uncertainty over all perturbed models to estimation uncertainty under the nominal model.

The information-theoretic lower bound in our paper is the first of its kind in the linear DRMDP setting, which could be of independent interest to the community. Previous lower bounds, which are based on the commonly used _Assouad's method_ and established under the standard linear MDP, do not consider model uncertainty. In particular, one prerequisite for applying Assouad's method is switching the initial minimax objective to a minimax risk in terms of Hamming distance. The intertwining of this prerequisite with the nonlinearity induced by the model uncertainty makes the analysis significantly more challenging. To this end, we construct a novel family of _hard instances_, carefully designed to (1) mitigate the nonlinearity caused by the model uncertainty, (2) fulfil the prerequisite for Assouad's method, and (3) be concise enough to admit matrix analysis.

Notations:We denote \(()\) as the set of probability measures over some set \(\). For any number \(H_{+}\), we denote \([H]\) as the set of \(\{1,2,,H\}\). For any function \(V:\), we denote \([_{h}V](s,a)=_{s^{} P_{h}(|s,a)}[V(s^{ })]\) as the expectation of \(V\) with respect to the transition kernel \(P_{h}\), \([_{h}V](s,a)=[_{h}V^{2}](s,a)-([_{h}V](s,a))^{2}\) as the variance of \(V\), \([_{h}V](s,a)=\{1,[_{h}V](s,a)\}\) as the truncated variance of \(V\), and \([V(s)]_{}=\{V(s),\}\), given a scalar \(>0\), as the truncated value of \(V\). For a vector \(x\), we denote \(x_{j}\) as its \(j\)-th entry. And we denote \([x_{i}]_{i[d]}\) as a vector with the \(i\)-th entry being \(x_{i}\). For a matrix \(A\), denote \(_{i}(A)\) as the \(i\)-th eigenvalue of \(A\). For two matrices \(A\) and \(B\), we denote \(A B\) as the fact that \(B-A\) is a positive semidefinite matrix. For any function \(f:\), we denote \(\|f\|_{}=_{s}f(s)\). Given \(P,Q()\), the total variation divergence of \(P\) and \(Q\) is defined as \(D(P\|Q)=1/2_{}|P(s)-Q(s)|ds\).

## 2 Most Related Work

DRMDPs.The DRMDP framework has been extensively studied under different settings. The works of [55; 52; 61; 26; 12] assumed precise knowledge of the environment and formulated the DRMDP as classic planning problems. The works of [67; 57; 33; 56; 42; 58] assumed access to a generative model and studied the sample complexities of DRMDPs. The works of [35; 41; 4] studied the offline setting assuming access to only an offline dataset, and established sample complexities under data coverage or concentrability assumptions. The works of [51; 3; 8; 19; 20] studied the online setting where the agent can actively interact with the nominal environment to learn the robust policy.

   Algorithm & Setting & Instance-dependent upper bound on the suboptimality gap \\  PEVI  & MDP & \(dH_{h=1}^{H}^{^{*},P}\|(s_{h},a _{h})\|_{_{h}^{-1}}|s_{1}=s\) \\ LinPEVI-ADV  & MDP & \(H_{h=1}^{H}^{^{*},P}\|(s_ {h},a_{h})\|_{_{h}^{-1}}|s_{1}=s\) \\ LinPEVI-ADV+  & MDP & \(_{h=1}^{H}^{^{*},P}\|(s_ {h},a_{h})\|_{_{h}^{-1}}|s_{1}=s\) \\  DRPVI (ours) & DRMDP & \(H_{P^{P}(P^{0})}_{h=1}^{H}^{^{* },P}_{i=1}^{d}\|_{i}(s_{h},a_{h})_{i}\|_{_{h}^{-1}}|s_{1}=s\) \\ VA-DRPVI (ours) & DRMDP & \(_{P^{P}(P^{0})}_{h=1}^{H}^{^{* },P}_{i=1}^{d}\|_{i}(s_{h},a_{h})_{i}\|_{_{h}^{-1}}|s_{1}=s\) \\   

Table 1: Summary of instance-dependent results in offline RL with linear function approximation. \(_{h}\) and \(_{h}^{*}\) are the empirical covariance matrix defined in (4.3) and (5.5) respectively. Note that \(^{}\) means the optimal policy in standard MDPs and the optimal robust policy in DRMDPs. The definition of \(_{h}^{*}\) also depends on the corresponding definition of \(^{}\).

DRMDPs with linear function approximation.Tamar et al. , Badrinath and Kalathil  proposed to use linear function approximation to solve DRMDPs with large state and action spaces and established asymptotic convergence guarantees. Zhou et al.  studied the natural Actor-Critic with function approximation, assuming access to a simulator. Their function approximation mechanisms depend on two novel uncertainty sets, one based on double sampling and the other on an integral probability metric. Ma et al.  first combined the linear MDP with the \(d\)-rectangular uncertainty set , and proposed the setting dubbed as the \(d\)-rectangular linear DRMDP, which naturally admits linear representations of the robust Q-functions3. Panaganti et al.  leverages the \(d\)-rectangular linear DRMDP framework to address the distribution shift problem in offline linear MDPs. Blanchet et al.  studied the offline \(d\)-rectangular linear DRMDP setting, for which the provable efficiency is established under a double pessimism principle. Liu and Xu  then studied the online \(d\)-rectangular linear DRMDP setting and pointed out that the intrinsic nonlinearity of DRMDPs might pose additional challenges for linear function approximation. After the release of our work, a concurrent study  emerged, which independently investigated offline DRMDPs with linear function approximation. Their algorithms attained the same instance-dependent suboptimalities as our proposed algorithms DRPVI and VA-DRPVI. Their algorithm DROP also achieved the same order of worst-case suboptimality, \((dH^{2}/)\), as our DRPVI. However, we further demonstrated that our algorithm VA-DRPVI can strictly improve this result to \((dH\{1/,H\}/)\). Moreover, we introduced a novel hard instance and established the first information-theoretic lower bound for offline DRMDPs with linear function approximation. We also note that there is a line of works  studied general function approximation under DRMDPs with the commonly studied \((s,a)\)-rectangularity uncertainty sets, where no further structure is applied except the rectangularity.

## 3 Problem Formulation

In this section, we provide the preliminary of \(d\)-rectangular linear DRMDPs, and describe the dataset as well as the learning goal in offline reinforcement learning.

Standard MDPs.We start with the standard MDP, which constitutes the basic of DRMDPs. A finite horizon Markov decision process is denoted by MDP\((,,H,P,r)\), where \(\) and \(\) are the state and action spaces, \(H_{+}\) is the horizon length, \(P=\{P_{h}\}_{h=1}^{H}\) denotes the set of probability transition kernels, \(r=\{r_{h}\}_{h=1}^{H}\) denotes the reward functions. More specifically, for any \((h,s,a)[H]\), the transition kernel \(P_{h}(|s,a)\) is a probability function over the state space \(\), and the reward function \(r_{h}:\) is assumed to be deterministic for simplicity. A sequence of deterministic policies is denoted as \(=\{_{h}\}_{h=1}^{H}\), where \(_{h}:\) is the policy for step \(h[H]\). Given any policy \(\) and transition \(P\), for all \((s,a,h)[H]\), the corresponding value function \(V_{h}^{,P}(s):=^{,P}_{t=h}^{H}r_{t}(s_{t},a_{t}) s_{h}=s\) and Q-function \(Q_{h}^{,P}(s,a):=^{,P}_{t=h}^{H}r_{t}(s_{t},a_{t}) s_{h}=s,a_{h}=a\) characterize the expected cumulative rewards starting from step \(h\), and both of them are bounded in \([0,H]\).

Distributionally robust MDPs.A finite horizon distributionally robust Markov decision process is denoted by DRMDP\((,,H,^{}(P^{0}),r)\), where \(P^{0}=\{P_{h}^{0}\}_{h=1}^{H}\) is the set of nominal transition kernels, and \(^{}(P^{0})=_{h[H]}_{h}^{}(P_{h}^{0})\) is the uncertainty set of transitions, where each \(_{h}^{}(P_{h}^{0})\) is usually defined as a ball centered at \(P^{0}\) with radius/uncertainty level \( 0\) based on some probability divergence measures . To account for the model uncertainty, the robust value function \(V_{h}^{,}(s):=_{P^{}(P^{0})}V_{h}^{,P}(s),\)\((h,s)[H]\) is defined as the value function under the worst possible transition kernel within the uncertainty set \(^{}(P^{0})\). Similarly, the robust Q-function is defined as \(Q_{h}^{,}(s,a)=_{P^{}(P^{0})}Q_{h}^{,P}(s,a)\), for any \((h,s,a)[H]\). Further, we define the optimal robust value function and the optimal robust Q-function as

\[V_{h}^{,}(s)=_{}V_{h}^{,}(s), Q_{h}^{, }(s,a)=_{}Q_{h}^{,}(s,a),(h,s,a)[H] .\]where \(\) is the set of all policies. The optimal robust policy \(^{}=\{^{}_{h}\}_{h=1}^{H}\) is defined as the policy that achieves the optimal robust value function: \(^{}_{h}(s)=_{}V^{,}_{h}(s)\), \((h,s)[H]\).

\(d\)-rectangular linear DRMDPs.A \(d\)-rectangular linear DRMDP is a DRMDP where the nominal environment is a special case of linear MDP with a simplex feature space [14, Example 2.2] and the uncertainty set \(^{}_{h}(P^{0}_{h})\) is defined based on the linear structure of the nominal transition kernel \(P^{0}_{h}\). In particular, we make the following assumption about the nominal environment.

**Assumption 3.1**.: Let \(:^{d}\) be a state-action feature mapping such that \(_{i=1}^{d}_{i}(s,a)=1\), \(_{i}(s,a) 0\), for any \((i,s,a)[d]\). For any \((h,s,a)[H]\), the reward function and the nominal transition kernels have a linear representation: \(r_{h}(s,a)=(s,a),_{h},\) and \(P^{0}_{h}(|s,a)=(s,a),^{0}_{h}()\), where \(\|_{h}\|_{2}\), and \(^{0}_{h}=(^{0}_{h,1},,^{0}_{h,d})^{}\) are unknown probability measures over \(\).

With notations in Assumption 3.1, we define the factor uncertainty sets as \(^{}_{h,i}(^{0}_{h,i})=:(), D(||^{0}_{h,i})},(h,i)[H][d]\), where \(D(||)\) is specified as the total variation (TV) divergence in this work. The uncertainty set is defined as \(^{}_{h}(P^{0}_{h})=_{(s,a) }^{}_{h}(s,a;^{0}_{h})\), where \(^{}_{h}(s,a;^{0}_{h})=_{i=1}^{d}_{i}(s,a)_{h,i}():_{h,i}()^{}_{h,i}(^{0}_{h,i}), i [d]}\). A notable feature of this design is that the factor uncertainty sets \(\{^{}_{h,i}(^{0}_{h,i})\}_{h,i=1}^{H,d}\) are decoupled from the state-action pair \((s,a)\) and also independent with each other. As demonstrated later, this decoupling property results in a computationally efficient regime for function approximation.

Robust Bellman equation.Under the setting of \(d\)-rectangular linear DRMDPs, it is proved that the robust value function and the robust Q-function satisfy the robust Bellman equations :

\[Q^{,}_{h}(s,a) =r_{h}(s,a)+_{P_{h}(|s,a)^{}_{h}(s,a; ^{0}_{h})}[_{h}V^{,}_{h+1}](s,a),\] (3.1a) \[V^{,}_{h}(s) =_{a_{h}(|s)}Q^{,}_{h}(s,a) ,\] (3.1b)

and the optimal robust policy \(^{}\) is deterministic. Thus, we can restrict the policy class \(\) to the deterministic one. This leads to the robust Bellman optimality equations:

\[Q^{,}_{h}(s,a) =r_{h}(s,a)+_{P_{h}(|s,a)^{}_{h}(s,a; ^{0}_{h})}[_{h}V^{,}_{h+1}](s,a),\] (3.2a) \[V^{,}_{h}(s) =_{a}Q^{}_{h}(s,a).\] (3.2b)

Offline Dataset and the Learning Goal.Let \(\) denote an offline dataset consisting of \(K\) i.i.d trajectories generated from the nominal environment MDP\((,,H,P^{0},r)\) by a behavior policy \(^{b}=\{^{b}_{h}\}_{h=1}^{H}\). In concrete, for each \([K]\), the trajectory \(\{(s^{}_{h},a^{}_{h},r^{}_{h})\}_{h=1}^{H}\) satisfies that \(a^{}_{h}^{b}_{h}(|s^{}_{h})\), \(r^{}_{h}=r_{h}(s^{}_{h},a^{}_{h})\), and \(s^{}_{h+1} P^{0}_{h}(|s^{}_{h},a^{}_{h})\) for any \(h[H]\). The goal of the robust offline RL is to learn the optimal robust policy \(^{}\) using the offline dataset \(\). We define the suboptimality gap between any policy \(\) and the optimal robust policy \(^{}\) as

\[(,s_{1},):=V^{,}_{1}(s_{1})-V^{, }_{1}(s_{1}).\] (3.3)

Then the goal of an algorithm in distributionally robust offline reinforcement learning is to learn a robust policy \(\) that minimizes the suboptimality gap SubOpt\((,s,)\), for any \(s\).

## 4 Warmup: Robust Pessimistic Value Iteration

In this section, we first propose a simple algorithm in Algorithm 1 as a warm start, and provide an instance-dependent upper bound on its suboptimality gap in Theorem 4.4.

The optimal robust Bellman equation (3.2) implies that the optimal robust policy \(^{}\) is greedy with respect to the optimal robust Q-function. Therefore, it suffices to estimate \(Q^{,}_{h}\) to approximate \(^{}\). To this end, we estimate the optimal robust Q-function by iteratively performing an empirical version of the optimal robust Bellman equation similar to (3.2). In concrete, given the estimators at step \(h+1\), denoted by \(_{h+1}(s,a)\) and \(_{h+1}(s)=_{a}_{h+1}(s,a)\), Liu and Xu  show that applying one step backward induction similar to (3.2) leads to

\[Q_{h}(s,a)=r_{h}(s,a)+_{P_{h}(|s,a)^{}_{h}(s,a;^{0}_{h})}_{h}_{h+1}(s,a)= (s,a),_{h}+^{}_{h},\] (4.1)

[MISSING_PAGE_FAIL:6]

Before presenting the theoretical guarantee of DRPVI, we make the following data coverage assumption, which is standard for offline linear MDPs [50; 9; 60; 54].

**Assumption 4.3**.: We assume \(:=_{h[H]}_{}(^{^{b},P^{0}}[(s_{h},a_{h})(s_{h},a_{h})^{}])>0\) for the behavior policy \(^{b}\) and the nominal transition kernel \(P^{0}\).

Assumption 4.3 requires the behavior policy to sufficiently explore the state-action space under the nominal environment. Indeed, it implicitly assumes that the nominal and perturbed environments share the same state-action space, and that the full information of this space is accessible through the nominal environment and the behavior policy \(^{b}\). Assumption 4.3 rules out cases where new states emerge in perturbed environments that can never be queried under the nominal environment as a result of the distribution shift. Now we present the theoretical guarantee for Algorithm 1.

**Theorem 4.4**.: Under Assumptions 3.1 and 4.3, \( K>\{512(2dH^{2}/)/^{2},20449d^{2}H^{2}/\}\) and \((0,1)\), if we set \(=1\) and \(_{1}=(H)\) in Algorithm 1, then with probability at least \(1-\), \( s\), the suboptimality of DRPVI satisfies

\[(,s,)_{1}_{P^{ }(P^{0})}_{h=1}^{H}^{^{*},P}_{i=1}^{d}\|_{i}(s _{h},a_{h})_{i}\|_{_{h}^{-1}}|s_{1}=s,\] (4.6)

where \(_{h}\) is the empirical covariance matrix defined in (4.3).

The result in Theorem 4.4 resembles existing instance-dependent bounds for standard linear MDPs [15; 54] (see Table 1 for a detailed comparison). However, there are two major distinctions between these results. First, our result depends on the weighted sum of diagonal elements \(_{i=1}^{d}\|_{i}(s_{h},a_{h})_{i}\|_{_{h}^{-1}}\), dubbed as the \(d\)-_rectangular robust estimation error_, instead of the Mahalanobis norm of the feature vector \(\|(s_{h},a_{h})\|_{_{h}^{-1}}\). As discussed in Remark 4.1, this term primarily arises due to the necessity to solve \(d\) distinct ridge regressions in each step, which presents a unique challenge in our analysis. Second, we consider the _supremum expectation_ of \(d\)-rectangular robust estimation error with respect to all transition kernels in the uncertainty set, which measures the _worst case coverage_ of the covariance matrix \(_{h}\) under the optimal robust policy \(^{*}\).

To connect with existing literature , we further show that under Assumption 4.3, the instance-dependent suboptimality bound can be simplified as follows.

**Corollary 4.5**.: Under the same assumptions and settings as Theorem 4.4, with probability at least \(1-\), for all \(s\), the suboptimality of DRPVI satisfies \((,s,)=(H^{2}/())\).

**Remark 4.6**.: Since \(\|(,)\|_{2} 1\) by Assumption 3.1, the coverage parameter \(\) is trivially upper bounded by \(1/d\). Assuming that \(=c^{}/d\) for a constant \(0<c^{}<1\), then we have \((,s,)=(dH^{2}/(c^{}))\). This bound improves the state-of-the-art, (4, Theorem 6.3), by \(O(d)\).

## 5 Distributionally Robust Variance-Aware Pessimistic Value Iteration

The instance-dependent bound in Theorem 4.4 has an explicit dependency on \(H\), which arises from the fact that \(Q_{h}^{}(s,a)[0,H]\) for any \((h,)[H](0,1]\) and the Hoeffding-type self-normalized concentration inequality used in our analysis. We will show in this section that the range of any robust value function could be much smaller under a refined analysis. Consequently, we can leverage variance information to improve Algorithm 1 and achieve a strengthened upper bound.

IntuitionIn the robust Bellman equation (3.1), the worst-case transition kernel would put as much mass as possible on the minimizer of \(V_{h+1}^{,}(s)\), denoted by \(s_{}\). Based on this observation, we conjecture that the robust Bellman equation (3.1) recursively reduces the maximal value of robust value functions, and thus shrinks its range. To see this, we define \(_{h,i}=(1-)_{h,i}^{0}+_{s_{}}\), where \(_{s_{}}\) is the Dirac measure at \(s_{}\), and we assume \(V_{h+1}^{,}(s_{})=0\) for any \((,h)[H]\) just for illustration. It is easy to verify that \(_{h,i}_{h,i}^{}(_{h,i}^{0})\) and is indeed the worst-case factor kernel. Then by (3.1) we have \(V_{h}^{,}(s)=_{a}[r_{h}(s,a)+(1-)[_{h}^{0} V_{h+1}^{,}](s,a)]\), which immediately implies \(_{s}V_{h}^{,}(s) 1+(1-)_{s^{} }V_{h+1}^{,}(s^{})\). This justifies our conjecture that the range of the robust value functions shrinks over stage. We dub this phenomenon as _Range Shrinkage_ and summarize it in the following lemma, with a more formal proof postponed to Appendix G.5.

**Lemma 5.1** (Range Shrinkage).: For any \((,,h)(0,1][H]\), we have

\[_{s}V_{h}^{,}(s)-_{s}V_{h}^{,} (s)}{}.\] (5.1)

This phenomenon only appears in DRMDPs since the range of value function is generally \([0,H]\) in standard MDPs. A similar phenomenon is first observed in infinite horizon tabular DRMDPs [42, Lemma 7]. One important implication of Lemma 5.1 is that the conditional variance of any value function shrinks accordingly. In particular, when \(=O(1)\), the range of any robust value function would shrink to constant order, which leads to constant order conditional variances. This motivates us to leverage the variance information in both algorithm design and theoretical analysis. Inspired by the variance-weighted ridge regression in standard linear MDPs , we propose to improve the vanilla ridge regression in (4.2) by incorporating variance weights. To this end, we first propose an appropriate variance estimator, whose form is specifically motivated by our theoretical analysis framework, to quantify the variance information.

Variance estimationWe first run Algorithm 1 using an offline dataset \(^{}\) that is independent of \(\) to obtain estimators of the optimal robust value functions \(\{_{h}^{{}^{}}\}_{h[H]}\). By Assumption 3.1, the variance of \([_{h+1}^{{}^{}}]_{}\) under the nominal environment is \([_{h}[_{h+1}^{{}^{}}]_{}](s,a)=[_{h}^{0}[_{h+1}^{{}^{}}]_{}](s,a)-([_{h}^{0}[ _{h+1}^{{}^{}}]_{}](s,a))^{2}=(s,a),_{h,2}-((s,a),_{h,1})^{2}\). We estimate \(_{h,1}\) and \(_{h,2}\) via ridge regression similarly as in (4.2):

\[}_{h,2}() =*{argmin}_{^{d}}_{=1}^{ K}[_{h+1}^{{}^{}}(s_{h+1}^{})]_{}^{2}- _{h}^{}^{2}+\|\|_{2}^{2},\] (5.2a) \[}_{h,1}() =*{argmin}_{^{d}}_{=1}^{ K}[_{h+1}^{{}^{}}(s_{h+1}^{})]_{}-_{h} ^{}^{2}+\|\|_{2}^{2}.\] (5.2b)

We then construct the following truncated variance estimator

\[_{h}^{2}(s,a;):=1,(s,a)^{ }}_{h,2}()_{[0,H^{2}]}-(s,a)^{ }}_{h,1}()_{[0,H]}^{2}-}{}},\] (5.3)

where the last term is a penalty to achieve pessimistic estimations of conditional variances.

Variance-Aware Function Approximation MechanismSimilar to the two-step estimation procedure of Algorithm 1, we first estimate \(_{h}()\) by the following variance-weighted ridge regression under the nominal environment:

\[}_{h}() =*{argmin}_{^{d}}_{=1}^{ K}[_{h+1}^{}(s_{h+1}^{})]_{}-_{h} ^{}^{2}}{_{h}^{2}(s_{h}^{},a_{h}^{ };)}+\|\|_{2}^{2}\] \[=_{h}^{-1}()_{=1}^{K}_{h}^{}[_{h+1}^{}(s_{h+1}^{})]_{}}{ _{h}^{2}(s_{h}^{},a_{h}^{};)},\] (5.4)where \(_{h}()=_{=1}^{K}_{h}^{}_{h}^{ }/_{h}^{2}(s_{h}^{},a_{h}^{};)+}\) is the empirical variance-weighted covariance matrix, which can be deemed as an estimator of the following variance-weighted covariance matrix

\[_{h}^{}=_{=1}^{K}_{h}^{}_{h}^{ }/[_{h}V_{h+1}^{,}](s_{h}^{},a_{h}^{})+ }.\] (5.5)

In the second step, we estimate \(_{h,i}^{}, i[d]\) in the same way as (4.4). We then add a pessimism penalty based on \(_{h}()\). We present the full algorithm details in Algorithm 2.

**Theorem 5.2**.: Under Assumptions 3.1 and 4.3, for \(K>\{(d^{2}H^{6}/),(H^{4}/^{2})\}\) and \((0,1)\), if we set \(=1/H^{2}\) and \(_{2}=()\) in Algorithm 2, then with probability at least \(1-\), the suboptimality of VA-DRPVI satisfies

\[(,s,)_{2}_{P^{ }(P^{0})}_{h=1}^{H}^{^{},P}_{i=1}^{d}\|_{ i}(s_{h},a_{h})}_{i}\|_{_{h}^{-1}}s_{1}=s ,\] (5.6)

where \(_{h}^{}\) is the population variance-weighted covariance matrix defined as in (5.5).

Note that the bound in Theorem 5.2 does not explicitly depend on \(H\) anymore compared with that in Theorem 4.4. A naive observation is that \([_{h}V_{h+1}^{,}](s,a)[1,H^{2}]\). By comparing the definitions in (4.3) and (5.5), we have \(_{h}^{-1} H^{2}_{h}^{-1}\). Thus the upper bound of Algorithm 2 is never worse than that of Algorithm 1. This improvement brought by variance information is similar to that in standard linear MDPs [54, Theorem 2]. However, thanks to the range shrinkage phenomenon, we can further show that VA-DRPVI is strictly better than DRPVI when the uncertainty level is of constant order.

**Corollary 5.3**.: Under the same assumptions and settings as Theorem 5.2, given the uncertainty level \(\), we have with probability at least \(1-\), for all \(s\), the suboptimality of VA-DRPVI satisfies

\[(,s,)_{2})}{ }_{P^{}(P^{0})}_{h=1}^{H}^{^{ },P}_{i=1}^{d}\|_{i}(s_{h},a_{h})}_{i}\|_{_{h}^{-1}}|s_{1}=s.\]

**Remark 5.4**.: Note that \((1-(1-)^{H})/=(\{1/,H\})\). When \(=O(1)\), the suboptimality of Algorithm 2 is strictly smaller than that of Algorithm 1 by \(H\). With a similar argument as in Remark 4.6, if we assume there exist a constant \(0<c^{}<1\), such that \(=c^{}/d\) in Assumption 4.3, then the instance-dependent upper bound can be simplified to \((dH\{1/,H\}/(c^{}))\), which improves the state-of-the-art [4, Theorem 6.3] by \(O(dH)\) when \(=O(1)\).

## 6 Information-Theoretic Lower Bound

For a matrix \(}^{d d}\) and a state \(s\), we define function \((,):^{d d}\) as follows.

\[(},s)=_{P^{}(P^{0})}_{h=1}^{H} ^{^{},P}_{i=1}^{d}\|_{i}(s_{h},a_{h})}_{i}\|_{}}s_{1}=s.\] (6.1)

It can be seen our upper bounds in previous sections primarily depend on quantities such as \((_{h}^{-1},s)\) and \((_{h}^{-1},s)\). Roughly speaking, these quantities characterize the discrepancy between the (weighted) covariance matrix of the offline dataset and the state action pairs generated from the transition probability in the uncertainty set. Hence we call \((,)\) the uncertainty function.

We now establish an information-theoretic lower bound to show that the uncertainty function is unavoidable for \(d\)-rectangular linear DRMDPs. Let \(\) be a class of DRMDPs and we define \((M,,s,)\) as the suboptimality gap specific to one DRMDP instance \(M\).

**Theorem 6.1**.: Given uncertainty level \((0,3/4]\), dimension \(d\), horizon length \(H\) and sample size \(K>(d^{6})\), there exists a class of \(d\)-rectangular linear DRMDPs \(\) and an offline dataset \(\) of size \(K\) such that for all \(s\), with probability at least \(1-\), \(_{}_{M}(M,,s,) c (_{h}^{-1},s)\), where \(c\) is a universal constant.

Theorem 6.1 shows that the uncertainty function \((_{h}^{-1},s)\) is intrinsic to the information-theoretic lower bound, and thus is inevitable. It is noteworthy that the lower bound in Theorem 6.1 aligns with the upper bound in Theorem 5.2 up to a factor of \(_{2}\), which implies that VA-DRPVI is minimaxoptimal in the sense of information theory, but with a small gap of \(()\). Consequently, we affirm that, in the context of robust offline reinforcement learning with function approximation, both the computational efficiency and minimax optimality are achievable under the setting of \(d\)-rectangular linear DRMDPs with TV uncertainty sets. Moreover, Theorem 6.1 also suggests that achieving a good robust policy necessitates the worst case coverage of the offline dataset over the entire uncertainty set of transition models, which is significantly different from standard linear MDPs where a good coverage under the nominal model is enough [15; 60; 54]. Such a distinction indicates that learning in linear DRMDPs may be more challenging in comparison to standard linear MDPs.

Further, we highlight that the hard instances we constructed also satisfy Assumption 4.3. It remains an interesting direction to explore what would happen if the nominal and perturbed environments don't share exactly the same state-action space. We conjecture that since there could be absolutely new states emerging in perturbed environments that can never be explored in the nominal environment, the policy learned merely using data collected from the nominal environment could be arbitrarily bad.

Challenges and novelties in construction of hard instancesExisting tight lower bound analysis in standard linear MDPs [62; 60; 54] generally depends on the Assouad's method and a family of hard instances indexed by \(\{-1,1\}^{d}\). However, they do not consider model uncertainty, which largely hinders the derivation of explicit formulas for the robust value functions. Further, one prerequisite of the Assouad's method is switching the initial minimax suboptimality \(_{\#}_{M}(,s,)\) to a risk of the form \(_{^{}}_{}D_{H}(,^{})\), where \(D_{H}(,)\) is the Hamming distance. The model uncertainty significantly complicates this procedure, as the nonlinearity involved disrupts the linear dependency between the value function and the index \(\). At the core of Theorem 6.1 is a novel class of hard instances \(\). At a high-level, the hard instances should (1) fulfill the \(d\)-rectangular linear DRMDP conditions, (2) mitigate the nonlinearity caused by model uncertainty, (3) achieve the prerequisite for Assouad's method, and (4) be concise enough to admit matrix analysis. We postpone details on the construction of hard instances and the proof of Theorem 6.1 to Appendix F.

As a side product of Theorem 6.1, we show in the following corollary an information-theoretic lower bound in terms of the instance-dependent uncertainty function \((_{h}^{-1},s)\) in Theorem 4.4.

**Corollary 6.2**.: Under the same setting in Theorem 6.1, the class of hard instances \(\) and offline dataset \(\) in Theorem 6.1 also suggests that, with probability at least \(1-\), \(_{}_{M}(,s,) c (_{h}^{-1},s)\), where \(c\) is a universal constant.

This implies that the uncertainty function \((_{h}^{-1},s)\) in Theorem 4.4 also arises from the information-theoretic lower bound. We note the lower bound in Corollary 6.2 matches the upper bound in Theorem 4.4 up to \(_{1}\), thus DRPVI is also minimax optimal in the sense of information theory, but with a larger gap of \((H)\). Moreover, the only difference between Theorem 6.1 and Corollary 6.2 is the covariance matrix. Due to the fact that \(_{h}^{-1}_{h}^{,-1}\), the information-theoretic lower bound in Theorem 6.1 is indeed tighter than that in Corollary 6.2.

## 7 Conclusions

We studied robust offline RL with function approximation under the setting of \(d\)-rectangular linear DRMDPs with TV uncertainty sets. We first proposed the DRPVI algorithm and built up a theoretical analysis pipeline to establish the first instance-dependent upper bound on the suboptimality gap in the context of robust offline RL. We then showed an interesting range shrinkage phenomenon specific to DRMDPs, and we proposed the VA-DRPVI algorithm, which leverages the conditional variance information of the optimal robust value function. Based on the analysis pipeline built above, we show that the upper bound of VA-DRPVI achieves sharp dependence on the horizon length \(H\). In addition, we found that an uncertainty function consisting of two crucial quantities-a supremum over uncertainty set and a diagonal-based normalization-appears in all upper bounds. We further established an information-theoretic lower bound to prove that the uncertainty function is unavoidable for robust offline RL under the setting of \(d\)-rectangular linear DRMDPs.

It remains an interesting future research question whether the computational and provable efficiency can be achieved in other settings for robust offline RL with function approximation. Another interesting future direction is to explore the unique challenges of applying general function approximation techniques in standard offline RL  to DRMDPs.