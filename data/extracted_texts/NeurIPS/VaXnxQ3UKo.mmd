# AlphaMath Almost Zero: Process Supervision without Process

Guoxin Chen, Minpeng Liao, Chengxi Li, Kai Fan

Tongyi Lab

chenguoxin22@mails.ucas.ac.cn

{minpeng.lmp,xiji.lcx,k.fan}@alibaba-inc.com

Code: https://github.com/MARIO-Math-Reasoning/Super_MARIO

equal contribution

###### Abstract

Although recent advancements in large language models (LLMs) have significantly improved their performance on various tasks, they still face challenges with complex and symbolic multi-step reasoning, particularly in mathematical reasoning. To bolster the mathematical reasoning capabilities of LLMs, most existing efforts concentrate on seeking assistance from either domain experts or GPT-4 for high-quality process-supervised data, which is not only expensive but also labor-intensive. In our study, we propose an innovative framework, AlphaMath, that bypasses the need for process annotations (from humans or GPTs) by leveraging Monte Carlo Tree Search (MCTS). This framework focuses on unleashing the potential of a well-pretrained LLM to autonomously enhance its mathematical reasoning. Specifically, we integrate a value model with the LLM, automatically generating both process supervision and step-level evaluation signals in MCTS. Furthermore, we propose an efficient inference strategy--step-level beam search, where the value model is crafted to assist the policy model (_i.e._, LLM) in navigating more effective reasoning paths, rather than solely relying on prior probabilities. The experimental results on both in-domain and out-of-domain datasets demonstrate that even without GPT-4 or human-annotated process supervision, our AlphaMath framework achieves comparable or superior results to previous state-of-the-art methods.

## 1 Introduction

Recent studies have extensively explored how to improve mathematical reasoning in large language models (LLMs) . An effective approach  is to artificially inject external knowledge into LLMs through fine-tuning on a substantial volume of high-quality, process-supervised data (_i.e._, solutions). As shown in Table 1, the annotation of high-quality solutions in current efforts primarily relies on domain experts or GPT-4 . However, due to trillions of training tokens and billions of parameters, existing LLMs possess a vast reservoir of knowledge, which remains underutilized in current finetuning-based approaches.

To more effectively harness the intrinsic knowledge of LLMs, advanced prompting techniques, such as Program-of-Thought (PoT)  and Program-Aided Language (PAL) , have been developed, integrating the in-context learning proficiency with external tools such as code interpreter to handle precise numerical and symbolic computation. However, these approaches have not fully unleashed

   Annotation & Source & Methods \\  Human & GPT-4 & \\  ✓ & ✓ &  \\ ✗ & ✓ &  \\  ✗ & ✗ & Ours \\   

Table 1: Annotation Costthe potential of LLMs and often rely on self-consistent majority voting , which does not reflect the natural process by which humans solve mathematical problems. This discrepancy arises because both the PoT and PAL frameworks pursue a solution to its final answer regardless of the accuracy of intermediate steps. Unlike these approaches, humans tend to reassess and potentially alter their solution path upon encountering a mistake or dead-end in the problem-solving process. In this manner, humans iteratively enhance their self-cognition and reinforce the utilization of knowledge.

In this research, we aspire for LLMs to possess the similar ability as humans to realize self-evolution and strengthen their utilization of knowledge autonomously. Notably, AlphaGo Zero  showcases how a neural network model can progressively evolve without human knowledge, autonomously producing the Go game training strategies. For the strategy (_i.e._, solution) of mathematical problems, both textual analysis  and code snippets  demand rigorous logical structuring. Consequently, most finetuning-based approaches concentrate on seeking assistance from domain experts or GPT-4 for annotated solutions, thereby overlooking the reservoir of knowledge inherent in LLMs.

Instead, we hypothesize that well pre-trained LLMs already possess the necessary mathematical knowledge to generate correct reasoning; however, they require appropriate stimulation--such as an improved prompt or search strategy--to do so. In this work, solutions including both textual analysis and code snippets are autonomously generated by a well pre-trained LLM equipped with appropriate prompts and deliberately designed Monte Carlo Tree Search (MCTS) framework [4; 32]. Specifically, we integrate LLMs with the MCTS to strike a more effective balance between exploration and exploitation, enabling the generation of high-quality process-supervised solutions without professional human annotations. To enhance the efficiency of solution generation, we incorporate a value model into the same LLM by appending a linear layer. This advancement removes the necessity for time-consuming rollouts for reward estimation. While the LLM learns to solve mathematical problems from its own annotated solutions, the value model simultaneously learns how to assess the quality of intermediate reasoning steps from the corresponding state values in MCTS, just like humans.

During the inference stage, with the value model, LLMs can perform MCTS inference, which significantly enhances their reasoning capabilities but limited by efficiency. Therefore, inspired by beam search algorithm , we propose a step-level beam search strategy, where the value model is crafted to assist the policy model (_i.e._, LLM) in navigating more effective solution paths, as opposed to relying solely on prior probabilities. Compared to the greedy or MCTS inference strategies, the step-level beam search significantly enhances the LLM's reasoning capability at a minimal cost.

Empirically, we build an iterative training framework as shown in Figure 1. Unlike in the game of Go, where the final board state directly indicates a win or loss, our methodology requires validation of the equivalence between predicted answers and actual ones. This is the fundamental reason why our training data necessarily consists of question statements and their final answers. Furthermore, we validate the applicability of our framework on three popular types of LLMs: domain-specific pre-trained models , general-purpose pre-trained models , and supervised fine-tuned models . Our contributions are summarized as follows:

* We propose a novel approach that integrates a pre-trained LLM with a deliberately designed Monte Carlo Tree Search (MCTS) framework. This combination allows LLMs to autonomously generate high-quality mathematical reasoning solutions without the need for professional human annotations, leading to a more efficient utilization of their inherent knowledge.
* To address the efficiency limitations of MCTS inference, we propose a step-level beam search strategy, which introduces a lightweight value model that works alongside the LLM, enabling the simultaneous assessment of the quality of intermediate reasoning steps. This method parallels human problem-solving by allowing the LLM to learn from its own solutions while also evaluating the effectiveness of its reasoning strategy, thus enhancing the overall reasoning capabilities.
* Extensive experiments demonstrate that our AlphaMath can effectively stimulate the internal knowledge of LLMs, achieving better or on par task performance on both in-domain and out-of-domain mathematical reasoning datasets, even without any process annotations.

## 2 Preliminary

We assume that, for any given input question \(\), the solution process can be broken into multiple reasoning steps (_e.g._, segmenting the solution based on distinct stages or simply on a period). From this perspective, we conceptualize mathematical problem solving within the context of reinforcement learning. Concretely, consider a complete solution consisting of \(T\) reasoning steps. At a given time \(t\), we represent the partial solution as the state \(_{t}\), and the subsequent reasoning step that might be taken as the action as \(_{t}\). For detailed definitions and examples of our reasoning step, please refer to Appendix C.1. In this scenario, the policy model is embodied by a large language model, and the transition \(f(_{t+1}|_{t},_{t})\) from one state to the next is deterministically accomplished through the concatenation operation.

\[_{}(_{t}|_{t})=(_{t}|_{t}),_{t+1}=(_{t},_{t})\] (1)

Our primary goal is to develop a step-level value model, denoted as \(V_{}()\), which is capable of assessing the expected returns from the current partial solution and guiding the LLM to select more reasonable subsequent reasoning steps.

To train the value model, we first define the reward in the context of mathematical problem solving, by assigning the reward \(r=0\) to all non-terminal reasoning steps, and \(r= 1\) to a correct/incorrect final answer. A common method to create the training signal is to employ Monte Carlo (MC) evaluation \((_{t})=_{i=1}^{N}r(_{t^{ } t}^{(i)},_{t^{}>t}^{(i)}|_{t})\), where \(_{t^{} t}^{(i)}\) and \(_{t^{}>t}^{(i)}\) represent the actions and states in the \(i\)-th simulation sampled by the policy model and the state transition function. \(r(|_{t})\) means the reward of the final outcome in one simulation from state \(_{t}\). Then, for any given partial solution \(\), we can train the step-level value model \(V_{}\) using a regression loss defined as follows:

\[_{V_{}}()=\|V_{}()- ()\|^{2}.\] (2)

## 3 AlphaMath

In the above approach of MC evaluation, it requires multiple simulations from each state, which may be inefficient in practice. We propose employing the Monte Carlo Tree Search (MCTS) algorithm, which has the potential to reuse simulations and update the estimated values in a principled manner.

### MCTS Evaluation

As shown in Figure 1, our approach employs iterative training. Before the \((k+1)\)-th round training, we have a value model \(V_{_{k}}\) and a LLM policy model \(_{_{k}}\), which are the same model but with different final layers in our paper. Using these models, we can construct an inference algorithm powered by MCTS. This algorithm starts with the initial state as its root, and through the synergistic use of the policy and value models, systematically grows the search tree by adding new nodes. These nodes correspond to the states deemed to have high potential based on the outcomes of simulated trajectories. Specifically within the context of mathematical problem-solving, as shown in Figure 2, we customize the four key operations of the MCTS algorithm as follows:

**Selection** During the \(i\)-th simulation of the MCTS, the process begins with \(_{0}\), representing the initial state containing the input question. The algorithm then proceeds to explore the tree \(_{k}\) by selecting

Figure 1: Our approach involves iterating through three distinct stages. (1) Collect a mathematical dataset that comprises questions and their corresponding final answers. (2) Employ MCTS on the policy and the value model to generate both correct and incorrect solution paths along with state values. (3) Optimize the policy and the value model with generated data from MCTS.

nodes according to a variant of the PUCT algorithm . This selection process is mathematically represented as:

\[_{t}=_{_{k}}[(_{t },)+c_{}_{_{k}}(|_{t})()}}{1+N(_{t},)}]\] (3)

where the state-action value \((,)\) and its visiting count \(N(,)\) are stored in the tree and will be updated as the search progresses. \(N_{parent}()\) represents the visiting count of the parent node of \(\). The action selection iterates until it encounters a leaf node of the current search tree. In our case, the prior \((|_{t})\) is defined as the exponential of averaged log-probability of all tokens in the step \(\), _i.e._, \((|}(a_{j}|_{<j},_ {t}))\).

**Expansion** Back-tracing from the selected leaf node to the root forms a partial solution, serving as a prompt for further node expansions. In our case, given that the LLM can theoretically generate an unlimited number of potential actions (token sequence), we employ sampling generation with higher temperature to ensure diversity.

**Evaluation** Evaluation of the leaf node or partial solution \(_{t}\), identified after the selection phase, is conducted by weighted sum as introduced in .

\[(_{t})^{(i)}=(1-) V_{_{k}}(_{t})+  r(_{t^{} t}^{(i)},_{t^{ }>t}^{(i)}|_{t})\] (4)

The intermediate value estimation \(\) in MCTS differs from the training signal \(\) defined in preliminary section 2. The parameter \(\) serves to balance the contribution of the value model's estimation with the empirical reward obtained during the rollout.

In our case, we follow a trade-off rollout strategy between AlphaGo  and AlphaGo Zero . Because our tree depth is much shallower than Go games (_e.g._, a maximum depth of 8) and expansions can easily reach a terminal node, we set an indicator function \(=_{}(_{t})\). If the expanded node is terminal, the reward is returned; otherwise, the value is predicted by the model \(V_{_{k}}\).

**Backup** We did not make any modifications to the backup. At the end of the \(i\)-th simulation, each edge \((,)\) along the path from the leaf node \(_{t}\) to the root undergoes a backward pass update. The updates to their state-action values and visiting counts are executed according to the following rules: \(N(,) N(,)+1\) and \((,),)} _{j=1}^{i}_{,_{t}} {V}(_{t})^{(j)}\).

**Value Estimation** After running \(N\) simulations with the MCTS algorithm, we obtain the final tree \(_{k}\), which stores the expanded nodes and their corresponding state-action values \(Q(,)\). Considering that the transition function is deterministic, and assuming that \(Q(_{t},_{t})=r(_{t},_{t})+V(_{t+1})=V(_{t+1})\) for non-terminal nodes3, we can employ the \(Q\) values as training signals. This implies that we can directly fit the state-action value of non-terminal nodes as,

\[(_{t+1})=(_{t},_{t})\] (5)

Figure 2: An overview of the four key operations in MCTS

```
0:\(B_{1}=1\), question \(\) (\(_{0}\)), policy / value models \(_{},V_{}\), simulations \(N\), max depth \(T\).
1:Build the complete tree \(\) by running MCTS\({}_{_{},V_{}}(_{0},N,T)\).
2:\(=[_{0}]\), \(t=0\)\(\)Initialize candidates
3:while\(t<T\)and non-terminal path in \(\)do
4: Initialize priority queue \(_{t+1}\)\(\)Max heap
5:for\(_{t}\) in \(\)do
6:fora in \(_{}(_{t})\)do\(\)Directly get children from tree
7:\(_{t+1}=[_{t},]\)
8: Add \((_{t+1},_{Q}(_{t},))\) to \(_{t+1}\)\(\)Directly get \(Q\)-value from tree
9:\(\)Top-\(B_{1}\) of \(_{t+1}\)\(\)Return top-1 as the final solution path return Top-1 of \(\) ```

**Algorithm 1** Inference with MCTS

### Iterative Training

**Initialization** Initially, our approach begins with a pre-trained LLM as the policy model \(_{_{1}}\). We extend this model by adding an auxiliary linear layer with a tanh activation function, which works alongside the traditional softmax layer responsible for token prediction, as depicted in the rightmost panel of Figure 1. This design implies that these two models, \(_{}\) and \(V_{}\), share the majority of their parameters. The parameters of the linear layer associated with \(V_{_{1}}\) are randomly initialized, leading to an initial tendency of the value head to predict a value close to 0 at the first (\(k=1\)) round of MCTS. However, as the simulations in the first round MCTS proceed, the rewards (\( 1\)) from terminal nodes are back-propagated to their parent nodes. As simulations \(N\) gradually increase, the estimated values \(\) of intermediate nodes converge towards the underlying true value within the range of \([-1,1]\).

**Training Method** From the tree \(_{k}\) constructed from the \(k\)-th round of MCTS, we can sample solution paths corresponding to terminal nodes with correct and incorrect predicted answers, denoted as \(^{+}\) and \(^{-}\), respectively, together with the value estimation of each node along these paths. We then apply a multi-task loss function to update both the policy and value models.

\[_{,}-_{}(^{+}|)+ (_{t=1}^{T(^{+})}\|V_{}(_{t})- (_{t})\|^{2}+_{t=1}^{T(^{-})}\|V_{}(_{ t})-(_{t})\|^{2})\] (6)

where the first term represents the negative log-likelihood loss for next-token prediction in correct solutions, and the second term within the big brackets captures the loss in value prediction for both correct and incorrect solutions, respectively. \(T()\) denotes the number of steps for solution path \(\). \(\) is a tunable hyper-parameter to control the weight of value loss. With the updated policy and value models \(_{_{k+1}}\) and \(V_{_{k+1}}\), we can advance to the next-round MCTS, iterating this training process to enhance our models further.

### Inference

**MCTS** For MCTS inference, it is necessary to set \(=0\) in the evaluation of Eq. (4). Unlike in board games, we cannot verify the correctness of a path during inference. Therefore, we consistently rely on the value model for node evaluation, including for terminal nodes. MCTS demands multiple simulations to update visiting counts and \(Q\) values, aiming to estimate a robust policy distribution.

After the tree has been completely built, the algorithm iteratively selects the top-\(B_{1}\) steps (usually \(B_{1}=1\) in MCTS) from the root in a top-down manner. This selection is guided by the _maximum \(Q\)-value_ stored in the child nodes of the tree. Subsequently, all child nodes from the previously selected \(B_{1}\) steps are collectively re-ranked based on their \(Q\)-values, and the top-\(B_{1}\) nodes from this ranking are retained for the next iteration. A summary of the algorithm can be found in Algorithm 1.

**Step-level Beam Search** However, MCTS is computationally intensive for simulations, making it less viable for use in production environments. To address this, we modify the MCTS inference process by eliminating the backup operation, introducing a simplified method, which we refer to as Step-level Beam Search (SBS), detailed in Algorithm 2. This approach does not construct the entire tree; instead, it dynamically selects the best child node during node expansion.

There are two primary technical distinctions in SBS. First, since node expansion is required on the fly, we introduce a new beam size, \(B_{2}\), to represent the maximum number of node expansions. Second, the selection criterion no longer relies on the \(Q\)-value converged after \(N\) simulations but instead uses the _maximum value prediction_ directly from the value model. Importantly, with special case SBS \(B_{1}=1\) as a fast approximation of MCTS, it facilitates the sequential, streaming output of steps, rendering it more suitable for practical implementation in real-world production.

## 4 Experiments

### Experimental Setup

In this study, we mainly investigate the math domain-specific language model, DeepSeekMathBase-7B , pre-trained on a substantial math-related corpus without any supervised fine-tuning (SFT), which is believed to possess necessary mathematical knowledge to tackle a wide range of mathematical problems.

**Training Data Generation via MCTS** For the training sets, we exclusively extract question and answer pairs from GSM8K  and MATH , omitting the human-annotated solution analysis. _In total, our training set includes only 15k question-answer pairs and 0 solution process._

In our setup, we utilize the MCTS framework to generate detailed solution processes equipped with the Python code interpreter. Initially, for the first round of MCTS, the prompt used for our solution generation adheres to the REACT  format, incorporating 2 demonstrations randomly selected from a pool of 20 prepared examples. Starting from the second round, with an already fine-tuned model from the first round, we employ a straightforward prompt in our SFT XML format without any demonstration. Two prompt examples are shown in Appendix F.

Specifically, we iteratively generate data and train our policy and value models through \(K=3\) rounds, continuing until the enhancement observed between any two consecutive rounds is incremental. In every round, we build 10 trees for each question-answer pair and randomly sample at most 4 correct and 4 incorrect solution processes. The ratio between positive and negative examples is approximately 1:1, with the count of positive examples in each round varying between 57k and 59k.

**Test Data** We evaluate our approach not only on GSM8K and MATH but also on out-of-distribution (OOD) datasets GaoKao2023  and OCWCourses . These two OOD datasets are even more challenging than MATH. Please refer to Appendix C.5 for more details about the dataset statistics. To assess the accuracy of the predicted answers, we utilize the math evaluation toolkit .

**Baselines** We first compare our approach with strong proprietary and open-source models, including OpenAI's ChatGPT and GPT-4 , Llama2 , Llemma . By default, we report the results obtained using Chain of Thought (CoT) prompting , along with the prompting results of PAL , due to its enhanced performance in mathematical reasoning.

SFT models leverage high-quality seed data with process supervision derived from GPT-4 or humans to enhance their reasoning capabilities. To ensure a fair comparison, we primarily contrast our approach with the highest-performing SFT models that utilize an external tool - a Python code interpreter. These include MAmmoTH , MathCoder , ToRA , MARIO , MathGenie , and DeepSeek-Math-Instruct . More implementation details can be found in Appendix C.

### Main Results

We report our in-domain and out-of-domain (OOD) results in Table 2. Different from previous works , our proposed AlphaMath does not rely on high-quality solutions annotated by humans or GPT-4, whether in the form of text analysis or code snippets. Such solutions typically bolster the model's reasoning abilities but also entail substantial costs associated with annotation. Furthermore, our method differs from prior research by not incorporating any external datasets (_e.g._, new questions and solutions) beyond the GSM8K and MATH datasets. The last five rows of Table 2 present our principal findings.

**First**, we establish a baseline with the inherent mathematical reasoning ability of DeepSeeKMath-Base using our designed prompt in a 2-shot setting. It's important to note that this outcome differs from the results reported for DeepSeeKMath-Base (PAL) in the original study, as it utilized prompts with 8-shot and 4-shot for the GSM8K and MATH datasets, respectively. **Secondly**, we only evaluate the policy model with greedy decoding. In comparison to our initial study, we record an enhancement of about 20 points for challenging problems in the MATH, GaoKao2023 (GK2023), and OWCourses (OCW) datasets, and an improvement of more than 10 points for grade school math problems. **Thirdly**, we delve into the role of the value model in facilitating mathematical reasoning, utilizing a computationally efficient step-level beam search (SBS) in Algorithm 2. When we increment \(B_{1}\) with a default \(B_{2}=5\) and temperature of 1.0, a corresponding gradual improvement in performance is observed. More discussion about the temperature in SBS can refer to Appendix 4.7. **Ultimately**, we evaluate our approach in Algorithm 1. In contrast to the training data generation, we construct a single tree with 40 simulations, a maximum of 5 child nodes, and a temperature of 0.6. While MCTS

  
**Model** & **Size** & **Seed Data\({}^{}\)** & **Seed Data** & **Tool** & **Zero** &  &  \\  & & **Annotation** & **Size** & **Tool** & **Shot** & **GSM8K** & **MATH** & **GK2023** & **OCW** \\   \\  GPT-4 & - & - & - & ✗ & ✗ & 92.0 & 42.5 & - & - \\ GPT-4 (PAL) & - & - & - & ✓ & ✗ & 94.2 & 69.7 & 43.6 & 30.1 \\ ChatGPT & - & - & - & ✗ & ✗ & 80.8 & 35.5 & - & - \\ ChatGPT (PAL) & - & - & - & ✓ & ✗ & 78.6 & 38.7 & - & - \\ Gemini-1.5 Pro & - & - & - & ✗ & ✗ & 91.7 & 58.5 & - & - \\ Claude-3.5-Sonnet & - & - & - & ✗ & ✓ & 96.4 & 71.1 & - & - \\   \\  Llama-2 & 7B & - & - & ✗ & ✗ & 13.3 & 4.1 & - & 3.7 \\ CodeLlama & 7B & - & - & ✗ & ✗ & 10.5 & 4.5 & - & 4.7 \\ CodeLlama(PAL) & 7B & - & - & ✓ & ✗ & 27.1 & 17.2 & - & - \\ Llemma & 7B & - & - & ✗ & 36.4 & 18.0 & - & 7.7 \\ Llemma(PAL) & 7B & - & - & ✓ & ✗ & 40.1 & 21.5 & - & - \\ DeepSeeKMath-Base(PAL) & 7B & - & - & ✓ & ✗ & 66.9 & 31.4(33.2) & - & - \\   \\  MAmnoTH-Coder & 34B & GPT-4+Human & 260k & ✓ & ✓ & 72.7 & 43.6 & 25.2 & 14.0 \\ MatthCoder & 34B & GPT-4 & 49k & ✓ & ✓ & 81.7 & 46.1(45.8) & - & - \\ ToRa-Code & 34B & GPT-4 & 16k & ✓ & ✓ & 80.7 & 50.85(51.2) & 31.7 & 5.5 \\ MARIO & 34B & GPT-4+Human & 27k & ✓ & ✓ & 78.2 & 53.5 & 42.6 & 30.2 \\ MathGenie & 34B & GPT-4 & 80k & ✓ & ✓ & **84.1** & 55.1 & - & - \\  Llama-2 SFT & 7B & Human & 15k & ✗ & ✓ & 41.3 & 7.2 & - & - \\ Llama-2 RFT & 7B & Human & 15k & ✗ & ✓ & 51.2 & - & - & - \\ MAmnoTH-Coder & 7B & GPT-4+Human & 260k & ✓ & ✓ & 59.4 & 33.4 & 15.3 & 11.0 \\ MathCoder & 7B & GPT-4 & 49k & ✓ & ✓ & 67.8 & 30.7(30.6) & - & - \\ ToRa & 7B & GPT-4 & 16k & ✓ & ✓ & 68.8 & 40.1 & 19.5 & 2.6 \\ ToRa-Code & 7B & GPT-4 & 16k & ✓ & ✓ & 72.6 & 44.6 & 23.9 & 4.8 \\ MARIO & 7B & GPT-4+Human & 27k & ✓ & ✓ & 74.5 & 48.3 & 34.5 & 21.7 \\ MathGenie & 7B & GPT-4 & 80k & ✓ & ✓ & 76.0 & 48.3 & - & - \\ DeepSeeKMath-Instrect & 7B & GPT-4+Human & 776k & ✓ & ✓ & 83.7 & 57.4(57.2) & 43.9 & 18.0 \\  DeepSeeKMath-Base & 7B & & & & & & & & & \\ +our prompt 2-shot & - & - & ✓ & ✗ & 59.7 & 33.2 & 21.9 & 9.2 \\ +AlphaMath (\(K=3\)) & ✗ & **0** & ✓ & ✓ & 73.5 & 53.6 & 40.5 & 26.1 \\ + SBS(\(B_{1}=1\)) & ✗ & **0** & ✓ & ✓ & 81.1 & 62.8 & 46.2 & 30.5 \\ + SBS (\(B_{1}=3\)) & ✗ & **0** & ✓ & ✓ & **84.1** & **66.3** & **51.4** & 33.1 \\ + MCTS (\(B_{1}=1\)) & ✗ & **0** & ✓ & ✓ & 83.2 & 64.0 & 48.4 & **33.8** \\   

Table 2: Main results. The best results of open-sourced models are bold. For the methods with released model’s outputs, performance metrics using the evaluation toolkit  are also provided in brackets. \({}^{}\)Seed data refers to high-quality annotated (question, solution) pairs, typically annotated by humans or GPT-4. \({}^{}\)Unless otherwise specified, we set beam size \(B_{2}=5\) in SBS and number of simulations \(N=40\) in MCTS by default.

demonstrates improved performance on more challenging datasets, attributed to its expansive search space, its substantial computational demands curtail its practical applicability in real-world scenarios.

In summary, our approach demonstrates that, even in the absence of high-quality GPT-4 or human-annotated solution processes, it remains competitive with or surpasses the performance of the state-of-the-art (SOTA) on 7B LLMs.

### Analysis 1: Performance of each round

We evaluate the problem-solving rate in the MATH training dataset, which categorizes each problem by difficulty level. As shown in Figure 3, it becomes evident that MCTS achieves greater success in solving more challenging problems in subsequent rounds. In Figure 4, our findings show a general increase in performance with additional rounds of training across all strategies, applicable to both in-domain and out-of-domain test sets. Therefore, we can conclude that the quality of our self-generated training data improves incrementally with each round, and this enhancement is reflected in the performance on the test set. More analysis can refer to Appendix B.3.

### Analysis 2: Performance of different inference strategies

We explore the performance of our model under various inference strategies including greedy decoding, step-level beam search, and MCTS. The results of MATH and GaoKao2023 are illustrated in Figure 4, while the results of other datasets can be found in Appendix B.1. Specifically, for SBS, an enhancement in performance was observed with an increase in the beam size \(B_{1}\). MCTS exhibited the higher performance than its approximation SBS (\(B_{1}=1\)), but we previously noted its significant time consumption and computational inefficiency. Consequently, we provide a summary of the average problem-solving duration and the average number of intermediate steps taken on the MATH dataset in Table 3. The results indicate that MCTS demands the longest solving time and the highest number of steps, attributable to our configuration of 40 simulations. To achieve similar accuracy, step-level beam search is more computationally friendly. Additionally, we observe an intriguing phenomenon: a larger beam size \(B_{1}\) tends to reduce the average problem-solving duration. This can be attributed to the decrease in the number of average steps required when a larger \(B_{1}\) is employed.

**Discussion of Majority Voting** It is challenging to directly compare maj@5 with step-level beam search due to the inherent differences in their methodologies. Generally speaking, as Algorithm 2, SBS will eventually return the top-1 final answer based on the value model, while maj@5 will generate all 5 possible final answers and vote the majority for evaluation.

From the step-level perspective, maj@5 will maintain 5 candidates for the current step to generate another 5 candidates for the next step. In contrast, the SBS (_e.g._, \(B_{1}=1,B_{2}=5\)) will always retain the top-1 candidate, discarding the 4 others. This provides the advantage of step-by-step streaming output in real-world production, whereas maj@5 can only output the complete solution until the voting is finalized. To sum up, their specific mechanics of candidate selection and retention differ significantly.

### Analysis 3: Value model

In the left panel of Figure 5, we plot the fitted distribution of \(Q\)-values (as defined in Eq. (5)) on MATH training set for intermediate steps. For correct solutions, the distribution is markedly skewed towards a value of 1. In contrast, the distribution for incorrect solutions exhibits a lower degree of skewness, albeit with the majority of the probability density leaning towards \(-1\). This is because a correct final answer typically suggests that the entire solution process is likely accurate, whereas an incorrect final answer may still encompass some correct intermediate steps. Thus, with the backup of MCTS, the \(Q\)-values of intermediate steps in incorrect solutions may also be updated with a reward of 1 during simulations.

   Inference &  & Avg. & Avg. &  \\ Strategy & & time (s) & steps & \\  Greedy & 53.62 & 1.6 & 3.10 & 1 \\ Maj@5 & 61.84 (+8.22) & 2.9 & 2.88 & 5 \\ SBS (\(B_{1}=1\)) & 62.80 (+9.18) & 3.1 & 3.01 & 1 \\ SBS (\(B_{1}=2\)) & 64.66 (+11.04) & 2.4 & 2.36 & 2 \\ SBS (\(B_{1}=3\)) & 66.30 (+12.68) & 2.3 & 2.21 & 3 \\ SBS (\(B_{1}=5\)) & 65.98 (+12.37) & 4.7 & 2.26 & 5 \\ MCTS (\(B_{1}=1\)) & 64.02 (+10.40) & 10.1 & 3.76 & n \\   

Table 3: Analysis of Computational Efficiency on MATH dataset. # Sol. denotes the number of solutions obtained eventually.

In the right panel of Figure 5, we plot the \(Q\)-values distribution on the test set, including both intermediate and terminal steps. The distribution associated with correct solutions exhibits a shape similar to that found in the training set. However, the distribution of incorrect solutions, which are the bad cases of the policy model, demonstrates a bimodal pattern. **(1)** When the value model believes the incorrect solution predicted by the policy model to be incorrect, the \(Q\)-values cluster around \(-1\). **(2)** Conversely, there are instances where the value model erroneously considers an incorrect solution as correct, resulting in another modal towards \(1\), which represents the bad cases of the value model.

### Analysis 4: Self-evolution on General-purpose and SFT models

We further investigate the potential of two other popular types of LLMs: general-purpose pre-trained models and SFT models. These models represent the scenarios of lacking continual pre-training (CPT) in domain-specific data and supervised fine-tuning (SFT) on high-quality annotated domain data, respectively. We select Llama3  and MARIO  as the base models and report the results in Table 4. For a fair comparison, the MARIO is trained on DeepSeeMath-Base-7B rather than its original Llemma-7B . First, although not proficient in mathematical reasoning, our AlphaMath enhances Llama3's mathematical reasoning capabilities without any annotations, yielding an average improvement of +20 points. Secondly, AlphaMath can significantly enhance the performance of existing SFT models, enabling MARIO to be competitive with and even outperform GPT-4.

### Analysis 5: The Effects of Temperature on Step-level Beam Search

We further investigate the effects of temperature during decoding on the performance of inference algorithms. For the greedy strategy, the temperature is consistently maintained at 0, whereas step-level beam search (SBS) and Monte Carlo Tree Search(MCTS) are more significantly influenced by higher temperatures. Therefore, taking step-level beam search (\(B_{1}=1\) and \(B_{1}=3\)) as an example, we obtained the results as illustrated in Figure 6.

**First,** under any temperature setting, the performance of step-level beam search significantly surpasses that of the

   &  &  \\  & **GSM8K** & **MATH** & **GK2023** & **OCW** \\  Llama3-base\({}^{@sectionsign}\) & 40.7 & 18.7 & 12.9 & 2.9 \\ + AlphaMath (\(K=3\)) & 59.4 & 36.8 & 27.1 & 6.6 \\ + SBS (\(B_{1}=3\)) & 71.8 & 41.9 & 31.4 & 10.7 \\  DSM\({}^{}\) + 27k MARIO data & 78.4 & 56.1 & 41.6 & 25.0 \\ + AlphaMath (\(K=2\)) & 80.2 & 58.8 & 48.1 & 31.3 \\ + SBS (\(B_{1}=3\)) & **88.3** & **68.6** & **54.1** & **42.3** \\  

Table 4: Additional Results on Llama3 and MARIO. \({}^{}\)DeepSeeKMath-Base-7B. \({}^{@sectionsign}\)Our designed prompt in 2-shot setting.

Figure 4: Comparison of Different Inference Strategies.

Figure 5: (Left) Fitted distribution of \(Q\)-values of 3rd round MCTS on the training set. (Right) Fitted distribution of \(Q\)-values via MCTS inference on the test set.

Figure 3: Solving Rate at Different Levels on MATH Training Set Figure 6: The Effects of Temperature on the performance of SBS.

greedy strategy. This is attributed to the value model effectively assisting the policy model in identifying more effective reasoning paths. **Secondly**, at lower temperatures, the performance of step-level beam search is constrained due to the lack of diversity in the generated solutions. With elevated temperatures, the value model is capable of discerning optimal paths within a more diverse set of solutions, thereby effectively enhancing reasoning performance. **Finally**, with a larger beam width, the model can explore more solutions. Therefore, the performance of \(B_{1}=3\) always surpasses that of \(B_{1}=1\).

## 5 Related Works

**Solution Annotation in Math.** Recent works  on mathematical reasoning have made impressive progress empowered by process-supervised data. However, most existing efforts concentrate on seeking high-quality solutions from domain experts or formidable commercial models, such as GPT-4 , which hampers the scalability of methods and escalates the associated expenses. Unlike previous work, only with the help of question-answer pairs, we focus on activating the intrinsic knowledge within LLMs to realize iterative self-evolution and strengthen their utilization of knowledge autonomously, just like humans.

**Value/Reward Model.** Recent studies  have demonstrated that process supervision can significantly enhance mathematical reasoning performance. Especially, value model  is incorporated into the decoding process, while reward model is the source of the training signal in reinforcement learning . However, these value/reward models require substantial annotated process-supervised data and introduce significant inference latency. In our work, we consider the state values \((_{t})\) from MCTS as supervision signals, which are aligned with the solutions and eliminate the annotation costs. Furthermore, we integrate the value model into the generative model to navigate more effective reasoning paths at minimal cost, thereby providing richer decoding strategies, such as step-level beam search or MCTS.

## 6 Conclusion

In this work, we introduce AlphaMath, a simple iterative training paradigm for leveraging Monte Carlo Tree Search to unleash the potential of a well pre-trained large language model to autonomously enhance its mathematical reasoning capabilities. Furthermore, by applying step-level beam search, the value model can assist the policy model in selecting a more reasonable solution path, rather than solely relying on prior probabilities, which significantly enhances mathematical reasoning capabilities at minimal cost. The experimental results on both in-domain and out-of-domain datasets demonstrate that even without GPT-4 or human-annotated process supervision, AlphaMath remains competitive with or surpasses the performance of the state-of-the-art methods.