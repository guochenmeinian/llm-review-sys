# Learning to Mitigate Externalities: the Coase Theorem with Hindsight Rationality

Antoine Scheid\({}^{1}\)

Aymeric Capitaine\({}^{1}\)

\({}^{1}\) Centre de Mathematiques Appliquees - CNRS - Ecole polytechnique - Palaiseau, 91120, France

\({}^{2}\) INRIA Saclay, Universite Paris Saclay, LMO - Orsay, 91400, France

\({}^{3}\) University of California, Berkeley

\({}^{4}\) Inria, Ecole Normale Superieure, PSL Research University - Paris, 75, France

###### Abstract

In economic theory, the concept of externality refers to any indirect effect resulting from an interaction between players that affects the social welfare. Most of the models within which externality has been studied assume that agents have perfect knowledge of their environment and preferences. This is a major hindrance to the practical implementation of many proposed solutions. To address this issue, we consider a two-player bandit setting where the actions of one of the players affect the other player and we extend the Coase theorem (Coase, 2013). This result shows that the optimal approach for maximizing the social welfare in the presence of externality is to establish property rights, i.e., enable transfers and bargaining between the players. Our work removes the classical assumption that bargainers possess perfect knowledge of the underlying game. We first demonstrate that in the absence of property rights, the social welfare breaks down. We then design a policy for the players which allows them to learn a bargaining strategy which maximizes the total welfare, recovering the Coase theorem under uncertainty.

## 1 Introduction

The concept of _externality_ is used in economics to capture phenomena that impact the global welfare stemming from economic interactions without any compensation (Buchanan and Stubblebine, 2006; Shah et al., 2018). externality is generally considered as market failure since they result in a loss of collective welfare. Given its practical importance (Dahlman, 1979; Greenfield et al., 2009), mechanisms that characterize and mitigate externalities are central to modern economic thinking.

A first approach to tackle the adverse effects of externalities in modern economics was based on quotas and taxation (see, e.g., Pigou, 2017, and the references therein). However, Coase's theorem (Coase, 2013) shows that in the presence of well-defined property rights and low transaction costs, parties affected by externalities can privately negotiate efficient solutions, and recover a welfare efficient allocation through transfers and bargaining.

Throughout the paper, we use the following simple example to illustrate our results.

**Example 1**.: _Consider two firms 1 (upstream) and 2 (downstream) respectively producing quantities \(q_{1} 0\) and \(q_{2} 0\) of a good sold at a fixed price \(p>0\). They incur strictly increasing and convex costs, captured by the differentiable cost functions \(c_{1}:q_{1} c_{1}(q_{1})\) and \(c_{2}:q_{2} c_{2}(q_{2})\), satisfying\(c_{1}(0)=0\) and \(c_{2}(0)=0\). We assume that firm \(1\) exerts on firm 2 a constant externality \(>0\) per unit produced. In other words, their profit functions (or utilities) are given by_

\[_{1}(q_{1})=pq_{1}-c_{1}(q_{1})_{2}(q_{1},q_{2})=pq_{2 }-c_{2}(q_{2})- q_{1}\.\]

One simple concrete illustration consists in an upstream firm emitting pollutants that reduce the downstream firm's production. If the upstream firm owns the property rights, it may receive a payment from the downstream one to reduce its output and thereby pollution. On the other hand, if the downstream firm owns the property right, it may require compensation from the upstream firm to allow its operation.

The Coase theorem demonstrates that in both cases with appropriate property rights, the resulting levels of production would be welfare efficient. The theorem is typically explained in textbooks under the assumption that the players have perfect knowledge of their own utility or profit function, as well as that of others. However, this assumption is unlikely to hold in real-world scenarios, where players have to learn about their own preferences and those of their competitors.

This example is, of course, a simplification of real world scenarios. For example, Abildrup et al. (2012) consider a more complex setting to model the interaction between farmers and waterworks in Denmark where the farmers have the property rights whereas the waterworks can pay to reduce pollution. In particular, they demonstrate the failure of the theorem, attributing it to the breakdown of the main assumptions: no transaction costs, maximizing behaviors and perfect information, with an important focus on strategic behaviors and the asymmetry of information. We restore the latter in our work and provide foundations for the theorem to hold in more realistic scenarios.

_A key question is whether the Coase theorem holds when players learn their preferences over time._

We investigate this question within the framework of a multi-armed bandit learning. We build upon recent works that extend the classical bandit setting to economics in which there are two players interacting via principal-agent protocols (Dogan et al., 2023, 2023, Scheid et al., 2024). This allows us to capture, for example, a version of the two-firm problem where firms are uncertain regarding both their profit functions and the degree of externality on other firms. We represent production decisions in this problem as arms which can be played by the firms at any round over time, with the goal of finding decisions that maximize their rewards. More precisely, we assume that the reward of the upstream firm only depends on its own action, while the reward of the downstream firm depends on both its action and the upstream firm's action. This dependency on both actions allows us to capture externalities.

The property rights in Example 1, or more generally over a bandit instance, amount to giving a firm the possibility of engaging in monetary transfers that influence the arms that are played over time. The owner of the bandit instance will face a problem of bandit learning with transfers with an upstream player who is also learning his preferences.

To account for the efficiency of a policy in this setup, we extend the classical static notion of welfare efficiency to the online setting. We say that a policy is _Welfare efficient_ if the social welfare regret is sub-linear. Proving the Coase theorem within our setup therefore boils down to show that if the bandit owner (who is without loss of generality the upstream player in this study1) runs a no-regret bandit algorithm to learn and exploit his preferences, the downstream player can then choose an optimal transfer scheme leading to a sub-linear total regret.

Our contributions are as follows:

* We show that when an upstream agent exerts externality on a downstream agent, in the absence of property right, the social welfare breaks down. Put differently, no joint policy of the agents can be welfare efficient.
* We then introduce property rights and show how it affects the game. In this case, bargaining and transfers are available to the players. We propose a policy for the downstream player that leads to welfare efficiency when the upstream player follows any black-box no-regret policy under mild assumption. This solution addresses the breakdown issue at equilibrium. Put together, we show an online version of the _Coase theorem_.

Setup and Inefficiency of Externality

### Bandit game

We consider a sequential bandit game in which two players (downstream and upstream) simultaneously play actions in a bandit instance for a horizon \(T^{}\). The action set for both players is \(=\{1,,K\},K^{}\).

The reward distributions of the agents differ. Given a family of distributions \(\{_{a}\,:\,a\}\) indexed by \(\), the upstream player's rewards are provided by an i.i.d. family of random variables

\[\{(Z_{a}(t))_{t[T]}\,:\,a\},Z_{a}(t)_{a}t[T]a \;.\]

To model the externality exerted by the upstream on the downstream player, we assume that the latter has a reward that depends both on her action and on that of the upstream player. Formally, this is modeled through a family of distributions \(\{_{a,b}\,:\,a,b\}\) double-indexed by \(\) and an i.i.d. family of random variables

\[\{(X_{a,b}(t))_{t[T]}\,:\,a,b\},X_{a,b}(t)_{a,b}\]

is the reward received by the downstream player at time \(t\) if she pulls the arm \(b\) and the upstream player pulls the arm \(a\).

**Players.** We assume that players are risk-neutral expected-utility maximizers, and we define their expected utilities for any \((a,b)\) as

\[v^{}(a)= z\,_{a}(z)  v^{}(a,b)= x\,_{a,b}(x)\;.\]

The distributions \((_{a})_{a}\) and \((_{a,b})_{(a,b)^{2}}\) are unknown to both the downstream and the upstream players and they aim to learn the distributions with best mean rewards by sequentially observing samples from \(\{(Z_{a}(t))_{t[T]}\,:\,a\}\) and \(\{(X_{a,b}(t))_{t[T]}\,:\,a,b\}\).

Moreover, we suppose that players are rational in hindsight; that is, they minimize their regret. Formally, the upstream player aims to minimize his regret defined as

\[^{}_{}(T,^{}_{})=T^{ ,}-\![_{t=1}^{T}v^{}(A_{t})] \,,^{,}=_{a}v^{}(a)\;,\] (1)

while the downstream player seeks to minimize her external regret defined as

\[^{}_{}(T,^{}_{},^{ }_{})=\![_{t=1}^{T}_{b}v^{}(A_{t},b)-v^{}(A_{t},B_{t})]\,,\] (2)

where the players' actions \((A_{t})_{t[T]}\), \((B_{t})_{t[T]}\) as well as their policies \(^{}_{}\), \(^{}_{}\) are defined below. Note that the utility of the downstream player also depends on the actions taken by the upstream player, which represents the externality exerted by the upstream player on the downstream player, hence the strategic dimension of our setting. We first consider a game where no property right is defined, so each player is free to pick his preferred arm irrespectively of the other player's choice. This will result in a breakdown of the total utility.

**Policies without property rights.** Consider first the upstream player. Based on a policy \(^{}_{}\) (for example a no-regret bandit algorithm such as the **Upper Confidence Bounds** algorithm (UCB)  or the \(\)-greedy algorithm ), we define his history \((^{}_{t})_{t[T]}\) by induction. We set \(^{}_{0}=\) and supposing that \(^{}_{t}\) is defined for \(t[T]\), then

\[^{}_{t+1}=^{}_{t}\{A_{t +1},V_{t+1},Z_{A_{t+1}}(t+1)\}\,,\]

where \((V_{s})_{s^{}}\) is a family of independent uniform random variables in \(\), allowing for randomization in the policy, and \(A_{t+1}\) is provided by \(^{}_{}\), following \(^{}_{}(V_{t+1},^{}_{t}) A _{t+1}\).

Second, consider the downstream player and an algorithm \(^{}_{}\) (specifically a no-regret bandit algorithm). We define her history \((^{}_{t})_{t[T]}\) by induction. We set \(^{}_{0}=\) and supposing that \(^{}_{t}\) is defined for \(t[T]\), then

\[^{}_{t+1}=^{}_{t}\{A_ {t+1},B_{t+1},U_{t+1}X_{A_{t+1},B_{t+1}}(t+1)\}\,,\]where \((U_{s})_{s^{*}}\) is a family of independent uniform random variables in \(\) allowing for randomization in the policy and \(B_{t+1}\) is provided by \(_{}^{}\), following \(_{}^{}(U_{t+1},_{t}^{, }) B_{t+1}\).

**Welfare efficiency.** We now introduce the notion of _Welfare efficiency_ for our setup. The global utility, or _social welfare_, of the players at round \(t\) is defined as \(v^{}(A_{t})+v^{}(A_{t},B_{t})\). We define the _socially optimal action_\((a^{},b^{})\) of the game as

\[(a^{},b^{})_{a,b}\;\;v^ {}(a)+v^{}(a,b)\;,\] (3)

as well as the _global regret_ (or _social welfare regret_) associated with policies \(_{}^{}\) and \(_{}^{}\) as

\[^{}(T,_{}^{},_{ }^{})=T(v^{}(a^{})+v^{ }(a^{},b^{}))-_{t=1}^{T}v^{}(A_{t})+v^{}(A_{t},B_{t})\;.\] (4)

Then, the joint policies \(_{}^{}\) and \(_{}^{}\) for the players are said to be _Welfare efficient_ if

\[_{T+}^{}(T,_{}^{},_{}^{})/T=0\;.\]

Intuitively, this condition implies that the frequency of the socially optimal action \((a^{},b^{})\) tends to 1 as \(T\) goes to infinity. In this sense, it mimics the usual, static Welfare efficiency criterion. As we will see, \((_{}^{},_{}^{})\) is typically not _Welfare efficient_ when there is a disalignment in the game between the players' individual interests based on their rationality and the social welfare.

### Inefficiency without property rights

We first present a result that captures the adverse consequence of externality on social welfare. The upstream player does not take into account the indirect cost incurred by the downstream player when he chooses his action. This drives the social welfare away from its optimal level. We illustrate this fact within our simple bilateral externality example.

**Example 1** (continuing from p. 1).: _We show that the competitive outcome, where each firm maximizes its profit independently, is not welfare efficient in the presence of externality. Define the social welfare as the function_

\[W:(q_{1},q_{2})_{1}(q_{1})+_{2}(q_{1},q_{2})=p(q_{1}+q_{2})-(c_{ 1}(q_{1})+c_{2}(q_{2}))- q_{1}\;.\] (5)

_By definition, the welfare efficient outcome \((q_{1}^{*},q_{2}^{*})_{+}^{2}\) satisfies \(W(q_{1}^{*},q_{2}^{*}) W(q_{1},q_{2})\) for any \((q_{1},q_{2})_{+}^{2}\). Since \(W\) is differentiable and strictly concave, \((q_{1}^{*},q_{2}^{*})\) is uniquely defined by the condition \( W(q_{1}^{*},q_{2}^{*})=0\), that is_

\[c_{1}^{}(q_{1})-=p c_{2}^{}(q_{2})=p\;.\] (6)

_Note that at the welfare efficient optimum, firm 1 does not equalize marginal cost with marginal profit, but produces less to account for the negative effect of externality on firm 2. We now characterize the competitive outcome \((q_{1}^{},q_{2}^{})_{+}^{2}\). Since \(_{1}\) and \(_{2}\) are differentiable and strictly concave, \((q_{1}^{},q_{2}^{})\) satisfies_

\[c_{1}^{}(q_{1}^{})=p c_{2}^{}(q_{2}^{ })=p\;.\] (7)

_For the competitive outcome to be welfare efficient, we require, by Equation (6) and Equation (7),_

\[c_{1}^{}(q_{1}^{})-=c_{1}^{}(q_{1}^{}), =0\;.\]

_This proves that whenever there are externalities, no competitive outcome is efficient._

We now show that in our model, when there is no property right and under mild assumptions, no achievable policy is welfare efficient whenever there is a misalignment between the players' interests and the social welfare. The upstream player's policy \(_{}^{}\) is said to be _no-regret_ if \(_{T+}_{}^{}(T,_{}^ {})/T=0\), where \(_{}^{}\) is defined in (1).

**Theorem 2**.: _Suppose that \(_{a}\;v^{}(a)\) is the singleton \(\{a_{}^{u}\}\) and that_

\[v^{}(a^{})+v^{}(a^{},b^{ })-v^{}(a_{}^{u})+v^{}(a_{}^{u},b )>0\;,\] (8)

_for any \(b\). In the absence of property rights and when the upstream player runs any no-regret policy \(_{}^{}\), we have \(^{}(T,_{}^{},_{}^ {})=(T)\). Therefore, \(^{}(T,_{}^{},_{}^ {})=(T)\) and \((_{}^{},_{}^{})\) is not welfare efficient._Condition (8) in Theorem 2 represents the unalignment between the upstream player's preference and the optimal choice from a social welfare point of view. Note that the upstream and downstream players can both have an \(o(T)\) external regret, while the social welfare regret still grows linearly with \(T\) because of the unfavorable interactions between their policies.

## 3 Online Property Game with Bargaining Players

### Online Property Game

We now consider the same repeated game in the form of a _property game_ where one of the players possesses the bandit instance (_upstream player_). As in the original setup of Coase (2013), the other player (_downstream player_) will provide the bandit owner with transfers to incentivize him to choose some specific action and influence the outcome of the game in her favor.

We show in Appendix B that our method applies similarly when property rights are given to the upstream player rather than the downstream player. Hence, there is no loss of generality in considering the aforementioned framework. In this sense, we recover the _invariance property_ of the Coasean bargaining (Mas-Colell et al., 1995).

**Example 1** (continuing from p. 1).: _We now illustrate how Coasean bargaining re-instaures efficiency. Suppose without loss of generality that property rights are such that firm 2 can pay \(_{+}\) to firm 1 for it to operate at a level \(_{1}\). Profits become_

\[_{2}:(q_{1},q_{2},,_{1})_{2}(q_{1},q_{2})- _{\{q_{1}=_{1}\}}_{1}:(q _{1},,_{1})(q_{1})+_{\{q_{1}=_{1}\}}.\]

_Consider the competitive outcome \((q_{1},q_{2},,_{1})_{+}^{4}\) which satisfies_

\[q_{1}=q_{1}(,_{1})_{q_{1}^{} 0} _{1}(q_{1}^{},,_{1})\] \[_{2}(q_{1}(,_{1}),q_{2},,_{1}) =_{q_{2}^{},^{},_{1}^{}}_{2}(q_{ 1}(^{},_{1}^{}),q_{2}^{},^{},_{1}^{}).\]

_The condition on \(q_{1}\) accounts for the rationality of the firm \(1\) and the fact that its choice depends on the payment \((,_{1})\). Obviously, the optimal solution is reached for \(_{1}=q_{1}\) and \(=_{q^{}}_{1}(q^{})-_{1}(_{1})\). Plugging this back in the expression of \(_{2}\) then yields_

\[(q_{1},q_{2})=_{q_{1}^{},q_{2}^{}}_{1}(q _{1}^{})+_{2}(q_{2}^{})=_{q_{1}^{},q_{2}^{}}W(q_{1}^{},q_{2}^{})\,\]

_so the competitive outcome \((q_{1},q_{2})\) is welfare efficient._

The transfers at each step can be interpreted as a contract between two players (see, e.g., Bolton and Dewatripont, 2004; Salanie, 2005, for general contract theory) and providing the right amount of incentives relates to adjusting a contract in an online setting (see Dutting et al., 2019; Guruganesh et al., 2021; Zhu et al., 2022; Fallah and Jordan, 2023; Guruganesh et al., 2024; Ananthakrishnan et al., 2024, for learning-based perspectives about contracts).

Similarly to Example 1, we modify the players' policies to now account for the transfer \((t)\) that the downstream player offers at round \(t\) to the upstream player if he picks action \(_{t}\). The downstream player's policy at round \(t\) does not only output an arm \(B_{t}\) but now a triple \((_{t},(t),B_{t})\), where \(B_{t}\) is the arm that she should play and \(_{t}\) is the arm on which a transfer \((t)\) is offered to the upstream player. On the upstream player's side, the policy still outputs an arm \(A_{t}\) to play but also takes as an input the incentive \((_{t},(t))\). In addition, the instantaneous utility of the upstream player becomes \(Z_{A_{t}}(t)+_{_{t}}(A_{t})(t)\), whereas the downstream player receives \(X_{A_{t},B_{t}}(t)-_{_{t}}(A_{t})(t)\).

**Policies with property rights.** Based on policies \(_{}^{}\) for the upstream player and \(_{}^{}\) for the downstream player, we define their histories \((_{t}^{})_{t[T]}\) and \((_{t}^{})_{t[T]}\) by induction. We set \(_{0}^{}=\), \(_{0}^{}=\) and supposing that \(_{t}^{}\), \(_{t}^{}\) are defined for \(t[T]\), then

\[_{t+1}^{}=_{t}^{}\{ _{t+1},(t+1),A_{t+1},V_{t+1},Z_{A_{t+1}}(t+1)\}\]

and

\[_{t+1}^{}=_{t}^{} \{_{t+1},(t+1),A_{t+1},B_{t+1},U_{t+1},X_{A_{t+1},B_{t+1}}(t+1 )\}\,,\]where \((V_{s})_{s^{*}}\), \((U_{s})_{s^{*}}\) are two families of independent uniform random variables in \(\) allowing for randomization in the policies, and the remaining quantities are given by \(_{}^{}(U_{t+1},_{t}^{,})(_{t+1},(t+1),B_{t+1})\) and \(_{}^{}(_{t+1},(t+1),V_{t+1}, _{t}^{,}) A_{t+1}\).

**Players' goal.** Given a transfer \(\) from the downstream to the upstream player on arm \(\), actions \(a\) and \(b\) respectively are chosen by the upstream and the downstream player, the upstream player's expected utility reads \(v^{}(a)+_{}(a)\) while the downstream player's expected utility is \(v^{}(a,b)-_{}(a)\). This defines the upstream player's expected regret for a horizon \(T\) as

\[_{}^{}(T,_{}^{},_ {}^{})=_{t=1}^{T}_{a }\{v^{}(a)+_{_{t}}(a)(t)\}-(v^ {}(A_{t})+_{_{t}}(A_{t})(t))\;.\] (9)

Based on the upstream player's utility, the downstream player aims on a single round at proposing an optimal transfer \(^{}\) on an arm \(a^{}\) as well as picking an arm \(b^{}\) which solves

\[(a,b,) v^{}(a,b)-\] (10) \[_{+},b,a _{a^{}}\{v^{}(a^{}) +_{a}(a^{})\}\;.\]

Her regret for any horizon \(T\) is defined as

\[_{}^{}(T,_{}^{}, _{}^{})=T^{*,}-_ {t=1}^{T}v^{}(A_{t},B_{t})-_{_{t}}(A_{t}) (t)\;,\] (11)

where we define \(^{*,}=v^{}(a^{},b^{})- ^{}\) as the optimal utility she can aim for. We can see that the downstream player's influence is exerted through her action choice \(B_{t}\) as well as through transfers which enable her to influence the upstream player's actions. Hence, the notion of external regret is obsolete here. The game has now the form of a repeated _Stackelberg game_.

**Lemma 1**.: _Recall that \(^{*,}\) is the downstream player's optimal reward as defined as a solution of (10). We have \(^{*,}=_{a,b}\{v^{}(a,b)+v^{ }(a)\}-_{a^{}}\{v^{}(a^{})\}\), as well as \((a^{},b^{})=(a^{},b^{})\) and \(^{*,}+^{*,}=v^{}(a^{})+v^ {}(a^{},b^{})=_{a,b}\{v^ {}(a)+v^{}(a,b)\}\), where \(^{*,}\) is defined in Equation (1). Moreover, for any integer \(T^{*}\), and policies \(_{}^{}\), \(_{}^{}\), we have that_

\[^{}(T,_{}^{},_{}^ {})_{}^{}(T,_{}^{},_{}^{})+_{}^ {}(T,_{}^{},_{}^{})\;.\]

This lemma has an interesting economic interpretation: if both players individually seek for their own interest within this online property game, they will together converge towards the optimal global utility. Individual rationality moves the outcome of the game towards the optimal social welfare. The transfers allow the players to align their goals and share the global reward, in line with the Coase theorem. Consequently, if both players run no-regret policies \(_{}^{}\) and \(_{}^{}\), the social welfare regret will also be in \(o(T)\). The rest of the paper shows that such no-regret policies exist. To this end, we introduce the following assumptions.

Without loss of generality, we assume that the upstream player's utility is rescaled and shifted, which corresponds to the following assumption on the reward distribution \((_{a})_{a}\) in \(\).

**H1**.: _For any \(a\), we have \(v^{}(a)\)._

We now make a high probability bound assumption on the upstream player's regret.2.

**H2**.: _There exist \(,>0,[0,1)\) such that for any \(s,t[T]\) with \(s+t T\), any \(\{_{a}\}_{a[K]}_{+}^{K}\) and any policy \(_{}^{}\) that offers almost surely a transfer \((_{l},(l))=(_{l},_{_{l}})\) for any \(l\{s+1,,s+t\}\), the batched regret of the upstream player following \(_{}^{}\) satisfies, with probability at least \(1-t^{-}\),_

\[_{l=s+1}^{s+t}_{a}\{v^{}(a)+_{_{l}}(a)_{_{l}}\}-(v^{}(A_{l})+_{_{l}}(A_{l}) _{_{l}})t^{}\;.\]The constraint on the downstream player's algorithm \(^{ down}_{ p}\) enforces constant incentives associated with any arm \(a\) within the batch, while the incentivized actions \((_{l})_{l\{s+1,,s+t\}}\) may change. Proposition 2 in Appendix C shows that an adaptation of UCB taking account the incentives satisfies \(\) with \(=8)}\), \(=1/2\) and \(=2\). Note that usual bandit algorithms such as AAE, ETC or EXP-IX also satisfy the assumption (see, e.g., Donahue et al., 2024, Lattimore and Szepesvari, 2020).

### Downstream player's procedure

We fix the policy \(^{ up}_{ p}\) which can be any algorithm satisfying \(\) for the upstream player and introduce the algorithm BELGIC (Bandits and Externalities for a Learning Game with Incentivized Coase) which provides a policy achieving sub-linear regret for the downstream player. It can be seen as an online bargaining strategy to mitigate externalities. Simply put, BELGIC unfolds in two steps. First note that for any action \(a\), the optimal (lowest) transfer to offer to the upstream player to make him choose \(a\) is

\[^{}_{a}=_{a^{}}v^{ up}(a^{})-v^{ up }(a)\,\] (12)

as detailed in Appendix A. Therefore, a batched binary search procedure (Algorithm 2) first allows the downstream player to estimate the optimal transfers \(^{}_{1},,^{}_{K}\) with a good precision level of \(1/T^{}\), where \(>0\). More precisely, the downstream player offers a constant incentive \((,_{})\) for a batch of time steps of length \(= T^{}\). The observation of \(T^{}_{}\), the number of steps from the batch for which the upstream player does not pick \(\) allows her to estimate whether \(_{}\) is above or below \(^{}_{}\) and adjust it, following Lemma 2 in Appendix A under the condition that \(,\) satisfy

\[/<(1-)\.\] (13)

The procedure needs to be run for \(K T^{}_{2}T^{}\) rounds since we have to make \(_{2}T^{}\) batches of binary search of length \( T^{}\) on each of the \(K\) arms (see Scheid et al., 2024). This corresponds to the first phase of BELGIC as described in Algorithm 2. At the end of this stage, the estimated transfers \((_{a})_{a}\) satisfy the bound in Proposition 1. These are then used to feed the subroutine Bandit-Alg.

**Proposition 1**.: _Under **H1** and **H2**, after the first phase of BELGIC which consists in \(K T^{} T^{}\) steps of binary search grouped in \(_{2}T^{}\) batches per arm \(a\), we have that_

\[a,_{a}-4/T^{}- ^{(-1)/2}^{}_{a}_{a}  1-K_{2}T^{}/T^{}\.\]

The additional term \(1/T^{}+^{-1}\) in \(_{a}\) ensures that if \(\) holds, the upstream player necessarily plays the incentivized action \(_{t}\) at round \(t\) with high probability. Lemmas 2 and 6 from Appendix C show how the binary search batches in BELGIC allow us to estimate \(^{}_{a}\) depending on \(-T^{}_{a}\), the number of times that arm \(a\) has been pulled by the upstream player during the batch.

Then, any bandit subroutine Bandit-Alg, such as UCB or \(\)-greedy, for instance, can be run in a black-box fashion on the shifted bandit instance, where the rewards are shifted by the upper estimated transfers \((_{a})_{a}\). The downstream player computes a shifted history \(}^{ down,p}_{t}\) such that for any \(t K T^{}_{2}T^{}\), \(}^{ down,p}_{t}=\) and for any \(t>K T^{}_{2}T^{}\)

\[}^{ down,p}_{t}=&\{_{t},B_{t},( t),U_{t},X_{_{t},B_{t}}(t)-_{_{t}}\}}^{ down,p}_{t-1}_{t}=A_{t}\\ &}^{ down,p}_{t-1},\] (14)

which serves to feed Bandit-Alg, following

\[(U_{t},}^{ down,p}_{t-1})( _{t},B_{t})\.\] (15)

For any family of constant incentives \(\{_{a}\}_{a}^{K}_{+}\), we define \(_{}(T,,\{_{a}\}_{a})\) as the regret for the downstream player's subroutine Bandit-Alg on the bandit instance with shifted means over \(T\) rounds, following

\[_{}(T,,\{_{a}\}_{a})=T_ {a,b^{2}}v^{ down}_{a,b}(1)-_{a}- _{t=1}^{T}v^{ down}(_{t},B_{t})-_{ _{t}}\.\]

Note that here, Bandit-Alg aims to maximize the shifted reward \((v^{ down}(a,b)-_{a})_{(a,b)^{2}}\).

```
1:Input: Set of actions \(=[K]\), time horizon \(T\), subroutine \(_{}^{}\), upstream player's regret constants \(,\), parameters \(\) and \(\).
2: Compute \(}_{s}^{,p}=\) for any \(s K_{2}T^{} T^{}\).
3:for\(a\)do
4:# See Algorithm 2
5:\(_{a},_{a}=(a,_{2}T^{} , T^{},0,1)\)
6:endfor
7: For any action \(a\), \(_{a}=_{a}+1/T^{}+T^{(-1)/2}\).
8:for\(t=K T^{}_{2}T^{}+1,,T\)do
9: Get recommended actions by \(\) on the \(\) bandit instance, \((_{t},B_{t})=(U_{t},}_{t-1}^{ })\).
10: Offer a transfer \(_{_{t}}\) on action \(_{t}\), nothing for any other action \(a^{}\) and play action \(B_{t}\).
11: Observe \(A_{t}=_{}^{}(_{t+1},(t+1),V_{t},}_{t-1}^{}),X_{_{t},B_{t}}(t)\)
12:if\(A_{t}=_{t}\)then update history \(}_{t}^{}\).
13:endif
14: Update upstream player's history \(_{t}^{}\).
15:endfor ```

**Algorithm 2**Binary Search Subroutine

```
1:Input: action \(a,N_{T},,_{a},_{a}\).
2:for\(d=0,,N_{T}-1\)do
3: Compute \(_{a}^{}=(_{a}(d)+_{a}(d))/2,T_{a }^{}=0\).
4:for\(t=d+1,,d+\)do
5: Propose transfer \(_{a}^{}(d)\) on arm \(a\) and nothing for any other action \(a^{}\).
6:\(A_{t}=_{}^{}(t,^{}(a),a,V_{t}, _{t-1}^{})\)
7:if\(A_{t} a\)then:\(T^{}+1\)
8:endif
9: Update upstream player's history \(_{t}^{}\).
10:endfor
11:if\(^{+/}<T_{a}^{}<- ^{+/}\)then Return \(_{a}(d),_{a}(d)\).
12:elseif\(T_{a}^{}-^{+/}\)then\(_{a}(d)=_{a}^{}(d)+1/T^{}\) and update history \(}_{t}^{}\).
13:else\(_{a}(d)=_{a}^{}(d)-1/T^{}\) and update history \(}_{t}^{}\).
14:endif
15:endfor ```

**Algorithm 3**Binary Search Subroutine

**Theorem 3**.: _Assume that **H**1 and **H**2 hold. Then \(\), run with \(,\) satisfying (13) and any bandit subroutine \(\), has an overall regret \(_{}^{}\) such that_

\[_{}^{}(T,_{}^{ },)  2(3+2+-)_{2}(T)(2T^{1- }+T^{(+1)/2}+ T^{})+4T^{1-}\] \[+_{}(T,,\{_{a}\}_ {a})\]

_where, for ease of notation_

\[=_{a,b}\{v^{}(a,b)\} =_{a,b}\{v^{ }(a,b)\}\;.\]

**Knowledge of \(\) and \(\).** An upper bound on \(\) and \(\) is sufficient to compute the hyperparameters in \(\). Theorem 3 shows that the bigger \(\) and \(\) are, the worse is the downstream player's regret, hence the interest of knowing them more precisely.

**Corollary 1**.: _Assume that the upstream player's distribution \((_{a})_{a}\) is such that **H**1 holds. In addition, suppose that the distributions \((_{a})_{a}\) and \((_{a,b})_{a,b}\) are \(1\)-sub-Gaussian and that the upstream player plays \(_{}^{}=Algorithm\ 3\) (a slight modification of \(\) to take into account the incentives). Then the downstream player's regret when she runs \(\) with parameters \(=3/4\) and \(=1/4\) (which satisfy (13)) and subroutine \(=\) satisfies the following upper bound3_

\[_{}^{}(T,,) (10+4K+32(KT^{3})}+-) _{2}(T)(3+2T^{3/4})\] \[+3K^{2}(-)\.\]

The upper bound on the social welfare regret in Lemma 1 together with Corollary 1 shows that when the upstream player runs \(_{}^{}=\) and the downstream player runs \(\), the social welfare regret then satisfies \(^{}(T,_{}^{},)= (K(T)^{T(+1)/2})\).

In other words, if the downstream player runs \(\) which produces a policy \(_{}^{}\), for any upstream policy \(_{}^{}\), \((_{}^{},_{}^{})\) is welfare efficient.

**Influence of the upstream performance.** It is interesting to note that in the downstream player's regret bound, the upstream player's regret bound in \((T^{})\) plays a significant role: the downstream player never learns faster than the upstream player. The latter's performance determines the social welfare convergence rate towards the social optimum. We can observe that the players' bounded rationality (Selten, 1990; Jones, 1999) and personal interest make the game converge towards the optimal social welfare equilibrium--even though they are both learning here.

**Experiments.** We conclude this section with experiments showing the empirical convergence of our algorithm to a social optimum. In the simulation, we consider two firms, with firm 1 being upstream and firm 2 being downstream. Their profit functions are respectively given by

\[_{1}}q_{1}-2}{1 0}^{2}+2}{10},0}\,\]

and

\[_{2}(q_{1},q_{2})-16}{10}-^{2}+8q_{1}-^{2}+q_{2},0}\.\]

Thus, firm 1's and firm 2's profit functions depends quadratically on \(q_{1}\) with an firm \(1\) optimum at \(q_{1}^{}=5\) and a social optimum at \(q_{1}^{*}=8\). Note that in the expression of \(_{2}\), \(q_{2}\) has very little influence as compared to \(q_{1}\) - which allows to plot profits for only one value of \(q_{2}\).

We discretize the setup, consider a bandit instance (horizon \(T=5.10^{6}\), \(10\) arms, average over \(10\) rounds) and we assume that \(\) is used as a subroutine. In the first setting, there are no property rights and each firm runs \(\) on their side. Second, property rights are defined and firm 2 runs \(\) as its policy. The plots in Figure 1 display the empirical frequencies and show empirically the effectiveness of \(\) to mitigate externalities.

## 4 Related work

Our work addresses the impact of externalities and is therefore related to taxation theory (see, e.g., Mirflees et al., 2011; Salanie, 2011, and the references therein), a prominent solution for this issue, as

Figure 1: Empirical frequencies of the upstream player’s actions when property rights are not defined (left) and when they are defined (right).

exemplified by the _Pigouvian tax_(see Pigou, 2017). Taxation is a fundamental aspect of all developed economies, with \(30\%\) to \(50\%\) of national income derived from taxes. The topic has been fruitful for various scenarios, including the carbon tax (Carattini et al., 2018; Metcalf and Weisbach, 2009), alcohol markets (Griffith et al., 2019), or business taxation (Boadway and Bruce, 1984). Taxation can also be studied through an operations research lens, where it is used to enhance system efficiency or manage specific games (Rougharden, 2010; Caragiannis et al., 2010; Bilo and Vinci, 2019). Recent work by Cui et al. (2024) explores online mechanisms to maximize efficiency in congestion games.

Mechanism designs (Myerson, 1989; Nisan and Ronen, 1999; Laffont and Martimort, 2009) allow to design games that have specific desired outcomes. Deploying these mechanisms in their classical economical form assume that players' utility functions are known a priori, which is often unrealistic. There is a major need to blend mechanism design with machine learning.

However, our approach differs, since, drawing inspiration from Coase's theory, we implement an online version of his theorem (Coase, 2013; Cooter, 1982), incorporating uncertainty to tackle the breakdown of social welfare in an online setting. We use the bandit setup (see Lattimore and Szepesvari, 2020; Slivkins et al., 2019) as a general and convenient way to model the game introduced by Coase. However, our work differs from considering a single agent playing a bandit game. Instead, we focus on the more general problem of multi-players bandits, a field receiving a growing attention from the community (see e.g., Boursier and Perchet, 2019, 2022; Sankararaman et al., 2019).

Our approach is inspired by the principal-agent model introduced by Dogan et al. (2023), which was further extended by Dogan et al. (2023), Scheid et al. (2024). However, unlike the models proposed in the work of Dogan et al. (2023, 2023), we do not specify a particular bandit algorithm for the upstream player and instead, we allow him to use any no-regret algorithm satisfying \(\) in a black-box fashion. Conversely, the model of Scheid et al. (2024) assumes that the upstream player is always fully informed and best-responding, whereas we assume that he is also learning. Chen et al. (2023) leverages a similar model to study information acquisition by a principal through an agent's actions. However, in their model, the agent is also almighty and knows exactly the costs associated with each action. Designing incentives in an unknown environment is related to auction theory incorporating uncertainty, as it is explored in the work of Feng et al. (see 2018), Li et al. (see 2023). Similar issues have been explored in the _Reinforcement Learning_ framework within a leader-follower game (see Chen et al., 2023, with quantal responses by the follower) or in a principal-agent game with incentive design as done by Ben-Porat et al. (2023). Donahue et al. (2024) also study a two-players repeated Stackelberg game on a bandit instance but instead of allowing for transfers, their main focus concerns the achievability of a _Stackelberg equilibrium_ through iterations of bandit policies: the same kind of goal also appears in Collina et al. (2023). Such principal-agent setups are of some interest to model various real-world situations such as the design of fundings for hospitals (Wang et al., 2024) or have been studied with multiple agents through the lens of auction design in dynamic setups (Bergemann and Said, 2010; Chen et al., 2023), or to account for fairness (Fallah et al., 2024).

In our game, the downstream player needs to learn the optimal transfers/incentives to offer to the upstream player. This is related to the _Incentivized Exploration_ literature (Mansour et al., 2016; Simchowitz and Slivkins, 2023; Esmaeili et al., 2023), which is often cast in terms of a benevolent planner who aims to optimize the global welfare of agents via plausible recommendations. A related model is _Bayesian Persuasion_(Kamenica and Gentzkow, 2011), where a sender influences a receiver's action through sending a signal. This model has begun to be studied in learning settings (see, e.g., Castiglioni et al., 2020; Bernasconi et al., 2022; Wu et al., 2022, 2022).

## 5 Conclusion

This paper studies a model of externalities in a two-players sequential game where both players learn their optimal actions. We first show that when the players act independently, then a misalignment between the players' interests and the social welfare leads to a breakdown of the global utility. We then introduce interactions through transfers, which restores a social welfare optimum, representing the online version of the _Coase theorem_. To that purpose, we propose a policy for the downstream player which allows her to estimate the optimal transfers as well as choosing the best actions. The mathematical difficulty comes from the learning aspect on both sides. Since our work is coined in a learning framework for mechanism design, several directions for research are open, as for instance extensions to the multi-agent setting, which raises many questions.