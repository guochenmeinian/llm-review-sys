# UDC: A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems

UDC: A Unified Neural Divide-and-Conquer Framework for Large-Scale Combinatorial Optimization Problems

 Zhi Zheng\({}^{1}\) Changliang Zhou\({}^{1}\) Xialiang Tong\({}^{2}\) Mingxuan Yuan\({}^{2}\) Zhenkun Wang\({}^{1}\)

\({}^{1}\) School of System Design and Intelligent Manufacturing,

Southern University of Science and Technology, Shenzhen, China

\({}^{2}\) Huawei Noah's Ark Lab, Shenzhen, China.

zhi.zheng@u.nus.edu, zhoucl2022@mail.sustech.edu.cn,

{tongxialiang, yuan.mingxuan}@huawei.com, wangzhenkun90@gmail.com

Corresponding author

###### Abstract

Single-stage neural combinatorial optimization solvers have achieved near-optimal results on various small-scale combinatorial optimization (CO) problems without requiring expert knowledge. However, these solvers exhibit significant performance degradation when applied to large-scale CO problems. Recently, two-stage neural methods motivated by divide-and-conquer strategies have shown efficiency in addressing large-scale CO problems. Nevertheless, the performance of these methods highly relies on problem-specific heuristics in either the dividing or the conquering procedure, which limits their applicability to general CO problems. Moreover, these methods employ separate training schemes and ignore the interdependencies between the dividing and conquering strategies, often leading to sub-optimal solutions. To tackle these drawbacks, this article develops a unified neural divide-and-conquer framework (i.e., UDC) for solving general large-scale CO problems. UDC offers a Divide-Conquer-Reunion (DCR) training method to eliminate the negative impact of a sub-optimal dividing policy. Employing a high-efficiency Graph Neural Network (GNN) for global instance dividing and a fixed-length sub-path solver for conquering divided sub-problems, the proposed UDC framework demonstrates extensive applicability, achieving superior performance in 10 representative large-scale CO problems. The code is available at https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/UDC-Large-scale-CO-master

## 1 Introduction

Combinatorial optimization (CO)  has numerous practical applications including route planning , circuit design , biology , etc. Over the past few years, neural combinatorial optimization (NCO) methods are developed to efficiently solve typical CO problems such as the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP). Among them, reinforcement learning (RL)-based constructive NCO methods  can generate near-optimal solutions for small-scale instances (e.g., TSP instances with no more than 200 nodes) without the requirement of expert knowledge . These solvers usually adopt a single-stage solution generation process, constructing the solution of CO problems node-by-node in an end-to-end manner. However, hindered by the disability of training on larger-size instances directly , these methods exhibit limited performance when generalizing to solve large-scale CO problems.

To meet the demands of larger-scale CO applications, researchers have increasingly focused on extending NCO methods to larger-scale instances (e.g., 1,000-node ones) . Existing NCO methods for large-scale problems primarily involve modified single-stage solvers  and neural divide-and-conquer methods [12; 13]. BQ-NCO  and LEHD  develop a sub-path construction process  and employ models with heavy-decoder to learn the process. Nevertheless, training such a heavyweight model necessitates supervised learning (SL), which limits their applicability to problems where high-quality solutions are not available as labels. ELG , ICAM , and DAR  integrate auxiliary information to guide the learning of RL-based single-stage constructive solvers . However, the auxiliary information needs problem-specific designs, and the \((N^{2})\) complexity caused by the self-attention mechanism also poses challenges for large-scale (\(N\)-node) problems. As another category of large-scale NCO, neural divide-and-conquer methods have attracted increasing attention. Inspired by the common divide-and-conquer idea in traditional heuristics [18; 19], these methods execute a two-stage solving process, including a dividing stage for overall instance partition and a conquerting stage for sub-problem solving. By employing neural network for one or both stages, TAM , H-TSP , and GLOP  have demonstrated superior efficiency in large-scale TSP and CVRP.

Despite acknowledging the prospect, these neural divide-and-conquer approaches still suffer from shortcomings in applicability and solution quality. Some neural divide-and-conquer methods rely on problem-specific heuristics in either dividing (e.g., GLOP  and SO ) or conquering stages (e.g., L2D  and RBG ), which limits their generalizability across various CO problems. Moreover, all the existing neural divide-and-conquer methods adopt a separate training process, training the dividing policy after a pre-trained conquering policy [13; 12; 20]. However, the separate training scheme overlooks the interdependencies between the dividing and conquering policies and may lead to unsolvable antagonisms. Once the pre-trained conquering policy is sub-optimal for some sub-problems, such a training scheme might guide the dividing policy to adapt the pre-trained sub-optimal conquering policy, thus converging to local optima.

As shown in Table 1, this paper finds that considering the negative impact of sub-optimal dividing results is essential in generating high-quality divide-and-conquer-based solutions. To this aim, we propose a novel RL-based training method termed Divide-Conquer-Reunion (DCR). Enabling the DCR in a unified training scheme, we propose the unified neural divide-and-conquer framework (UDC) for a wide range of large-scale CO problems. UDC employs a fast and lightweight GNN to decompose a large-scale CO instance into several fixed-length sub-problems and utilizes constructive solvers to solve these sub-problems. We conduct extensive experiments on **10** different CO problems. Experimental results indicate that UDC can significantly outperform existing large-scale NCO solvers in both efficiency and applicability without relying on any heuristic design.

The contributions of this paper are as follows: **1)** We propose a novel method DCR to enhance training by alleviating the negative impact of sub-optimal dividing policies. **2)** Leveraging DCR in training, the proposed UDC achieves a unified training scheme with significantly superior performance. **3)** UDC demonstrates extensive applicability and can be applied to general CO problems with similar settings.

    &  &  &  &  \\  Methods & Problem & Neural\(\)/Heuristic\(\) &  &  \\  LCP  & TSP,CVRP & ✓ & ✓ & \(\) & \(\) \\ L2D  & CVRP & ✓ & \(\) & \(\) & \(\) \\ H-TSP  & TSP & ✓ & ✓ & \(\) & \(\) \\ RBG  & CVRP \& CVRP variants & ✓ & ✓ & \(\) & \(\) \\ TAM  & CVRP & ✓ & ✓ & \(\) & \(\) \\ SO  & TSP & \(\) & ✓ & \(\) & \(\) \\ GLOP  & (\(\))TSP, CVRP, PCTSP & \(^{1}\) & ✓ & \(\) & \(\) \\  UDC(Ours) & General CO Problems & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison between our UDC and the other existing neural divide-and-conquer methods. The proposed UDC utilizes learning-based policies in both the dividing and conquering stages. Moreover, UDC is the first to achieve a superior unified training scheme by considering the negative impact of sub-optimal dividing policies on solution generation.

## 2 Preliminaries: Neural Divide-and-Conquer

### Divide-and-Conquer

A CO problem with \(N\) decision variables is defined to minimize the objective function \(f\) of a solution \(=(x_{1},x_{2},,x_{N})\) as follows:

\[}{} f(,)\] (1)

where \(\) represents the CO instance and \(\) is a set consisting of all feasible solutions.

Leveraging the divide-and-conquer idea in solving CO problems is promising in traditional (meta)heuristic algorithms , especially large-neighborhood-search-based algorithms [26; 27]. These methods achieve gradual performance improvement from an initial solution through iterative sub-problem selection (i.e., the dividing stage) and sub-problem repair (i.e., the conquering stage). Recent neural divide-and-conquer methods conduct a similar solving process and employ efficient deep-learning techniques to model the policies of both stages . The dividing policy \(_{d}()\) decomposes the instance \(\) to \(K\) mutually unaffected sub-problems \(\{_{1},_{2},,_{K}\}\) and the conquering policy \(_{c}\) generates sub-solutions \(_{k}\) for \(k\{1,,K\}\) by \(_{k}_{c}(_{k})\) to minimize the objective function \(f^{}(_{k},_{k})\) of corresponding sub-problems. Finally, a total solution \(\) of \(\) is merged as \(=(_{1},,_{K})\).

### Constructive Neural Solver

The (single-stage) constructive NCO solvers are promising in solving general small-scale CO problems with time efficiency. These constructive solvers typically employ an attention-based encoder-decoder network structure, where a multi-layer encoder processes CO instances into embeddings in hidden spaces and a lightweight decoder handles dynamic constraints while constructing feasible solutions . With the solving process modeled as Markov Decision Processes (MDPs) , constructive solvers can be trained using deep reinforcement learning (DRL) techniques without requiring expert experience. In the solution generation process, constructive solvers with parameter \(\) process the policy \(\) of a \(\)-length solution \(=(x_{1},,x_{})\) as follows:

\[(|,,)=_{t=1}^{}p_{}(x_{t}|_{1:t-1},,),\] (2)

where \(_{1:t-1}\) represents the partial solution before the selection of \(x_{t}\). DRL-based constructive NCO solvers demonstrate outstanding solution qualities in small-scale CO problems. So existing neural divide-and-conquer methods [13; 21] also generally employ a pre-trained constructive neural solver (e.g., Attention Model ) for their conquering policy \(_{c}\).

### Heatmap-based Neural Solver

Advanced constructive NCO solvers rely on the self-attention layers  for node representation, whose quadratic time and space complexity related to problem scales  hinders the direct training on large-scale instances. So, heatmap-based solvers [31; 32] are proposed to solve large-scale CO problems efficiently by a lighter-weight GNN . In heatmap-based solvers for vehicle routing problems (VRPs), a GNN with parameter \(\) first generates a heatmap \(^{N N}\) within linear time  and a feasible solution is then constructed based on this heatmap with the policy  as follows:

\[(|,,)=p_{}(|, )p(x_{1})_{t=2}^{}_{x_{t-1},x_{t}})}{ _{i=t}^{N}exp(_{x_{t-1},x_{i}})},\] (3)

However, the non-autoregressively generated heatmap  excludes the information about the order of the constructed partial solution \(_{1:t-1}\), so the greedy decoding policy in heatmap-based solvers becomes low-quality and these solvers rely on search algorithms [37; 38] for higher-quality solutions.

## 3 Methodology: Unified Divide-and-Conquer

As shown in Figure 1, UDC follows the general framework of neural divide-and-conquer methods , solving CO problems through two distinct stages: the dividing stage and the conquering stage. Thedividing stage of UDC is expected to generate a feasible initial solution with a similar overall shape compared to the optimal solution, so the subsequent conquering stage can obtain more high-quality solutions by solving local sub-problems. For all the involved CO problems, UDC first constructs a sparse graph to represent the instance and then employs a heatmap-based solver with Anisotropic Graph Neural Network (AGNN) as the dividing policy  to generate initial solutions. The following conquering stage first decomposes the original CO instance into multiple sub-problems based on the initial solution while integrating the necessary constraints for sub-problems. Then, towards minimizing the decomposed objective function of the sub-problem, UDC uses constructive neural solvers as the conquering policy to generate sub-solutions. There is no single universal constructive neural solver that can handle all types of CO problems , so we employ suitable solvers for each sub-CO-problem (e.g., AGNN  for MIS, POMO  or ICAM  for VRPs).

### Pipeline: Dividing Stage & Conquering Stage

**Dividing Stage: Heatmap-based Neural Dividing:** Heatmap-based solvers require less time and space consumption compared to constructive solvers, so they are more suitable for learning global dividing policies on large-scale instances. The dividing stage first constructs a sparse graph \(_{D}=\{,\}\) (i.e., linking K-nearest neighbors (KNN) in TSP, or original edges in \(\) for MIS) for the original CO instance \(\). Then, we utilize Anisotropic Graph Neural Networks (AGNN) with parameter \(\) to generate the heatmap \(\). For \(N\)-node VRPs, the heatmap \(^{N N}\) and a \(\)-length initial solutions \(_{0}=(x_{0,1},,x_{0,})\) is generated based on the policy \(_{d}\) as follows:

\[_{d}(_{0}|_{D},,)=p( |_{D},,)p(x_{0,1})_{t=2}^{}(_{x_{0,t-1},x_{0,t}})}{_{i=t}^{N}( _{x_{0,t-1},x_{0,i}})},&_{0}\\ 0,&.\] (4)

**Conquering Stage: Sub-problem Preparation:** The conquering stage first generates pending sub-problems along with their specific constraints. For VRPs, the nodes in sub-VRPs and the constraints of sub-VRPs are built based on continuous sub-sequences in the initial solution \(_{0}\). In generating \(n\)-node sub-problems, UDC divides the original \(N\)-node instance \(\) into \(\) sub-problems \(\{_{1},,_{}\}\) according to \(^{0}\), temporarily excluding sub-problems with fewer than \(n\) nodes. The constraints \(\{_{1},,_{}\}\) of sub-problems include not only the problem-specific constraints (e.g., no self-loop in sub-TSPs) but also additional constraints to ensure the legality of the merged solution after

Figure 1: The solving process of the proposed UDC on large-scale VRP, 0-1 Knapsack Problem (KP), and Maximum Independent Set (MIS).

integrating the sub-problem solution to the original solution (e.g., maintaining the first and last node in sub-VRPs). Similar to previous works [13; 43], we also normalize the coordinate of sub-problems and some numerical constraints to enhance the homogeneity of pending sub-problem instances. Due to the need to handle different constraints, the sub-problem preparation processes vary for different CO problems, which is presented in detail in the Appendix B.

**C**onquering Stage: **Constructive Neural Conquering:** After the preparation of sub-problems, the conquering policy is utilized to generate solutions to these instances \(\{_{1},,_{}\}\). We utilize constructive solvers with parameter \(\) for most involved sub-CO problems. Their sub-solution \(_{k}=(s_{k,1},,s_{k,n}),k\{1,,\}\) are sampled from the conquering policy \(_{c}\) as follows:

\[_{c}(_{k}|_{k},_{k},)=_{t=1} ^{n}p(_{k,t}|_{k,1:t-1}_{k},_{k},),&_{k}_{k}\\ 0,&,\] (5)

where \(_{k,1:t-1}\) represents the partial solution before the selection of \(s_{k,t}\). Finally, as the last step in a conquering stage, sub-solutions with improvements on the objective function will replace the original solution fragment in \(_{0}\) and the merged solution becomes \(_{1}\). It's worth noting that the conquering stage can execute repeatedly on the new merged solution for a gradually better solution quality and we note the solution after \(r\) conquering stages as \(_{r}\).

### Training Method: Divide-Conquer-Reunion (DCR)

Both the dividing and conquering stages in neural divide-and-conquer methods can be modeled as MDPs  (shown in Appendix C.7). The reward for the conquering policy is derived from the objective function of the sub-solutions, while the reward for the dividing policies is calculated based on the objective function of the final merged solution. Existing neural divide-and-conquer methods cannot train both policies simultaneously with RL . Therefore, they generally employ a separate training scheme, pre-training a conquering policy on specific datasets before training the dividing policy . However, this separate training process not only requires additional problem-specific dataset designs  but also undermines the collaborative optimization of the dividing and conquering policies (further discussed in Section 5.1).

In this paper, we imply that considering the negative impact of sub-optimal sub-problems decomposition is essential to implement the unified training of both policies. Sub-problems generated by sub-optimal dividing policies will probably not match any continuous segments in the optimal

Figure 2: The proposed DCR training method. The upper part shows the pipeline of a single DCR-enabled training step, including a Divide step, a Conquer step, and a Reunion step. The lower part provides a local view of a solution fragment to demonstrate our motivation, the additional Reunion step can repair the wrong connections from sub-optimal sub-problem dividing results. DCR has the potential to correct sub-optimal dividing results by repairing the connection part in the additional Reunion step.

solution so as shown in Figure 2, despite obtaining high-quality sub-solutions, there may be error connections emerging at the linking regions of these sub-problems. The naive solving procedure of neural divide-and-conquer methods ignores these inevitable errors in solution generation, thus utilizing biased rewards to assess the dividing policy. To tackle this drawback, we propose the Divide-Conquer-Reunion (DCR) process to eliminate the impact as much as possible. DCR designs an additional Reunion step to treat the connection between the original two adjacent sub-problems as a new sub-problem and conduct another conquering stage on \(_{1}\) with new sub-problem decompositions. The Reunion step can be formulated as rolling the original start point \(x_{1,0}\) along the solution \(_{1,0}\) by \(l\) times (i.e., to \(x_{1,l}\)) and conquering again. To ensure equal focus on the adjacent two sub-problems, we set \(l=\) in training all the DCR-enabled UDC models. DCR provides a better estimation for the reward of dividing policies, thereby increasing the stability and convergence speed in unified training.

UDC adopts the REINFORCE algorithm  in training both the dividing policy (with Loss \(_{d}\)) and the conquering policy (with Loss \(_{c1}\) for the Conquer step and \(_{c2}\) for the Reunion step). The baseline in training dividing policy is the average reward over \(\) initial solutions \(_{0}^{i},i\{1,,\}\) and the baseline for conquering policy is calculated over \(\) sampled sub-solutions. The gradients of loss functions on a single instance \(\) are calculated as follows:

\[_{d}() =_{i=1}^{}f(_{2}^{i },)-_{j=1}^{}f(_{2}^{j},)\ _{d}(_{2}^{i}|_{D},,),\] (6) \[_{c1}() =}_{c=1}^{ }_{i=1}^{}f^{}(_{c}^{1,i},_{c}^{0})-_{j=1}^{}f^{}(_{c}^{1,j},_{c}^{0})\ _{c}(_{c}^{1,j}|_{c}^{0},_{c}^{0},),\] \[_{c2}() =}_{c=1}^{ }_{i=1}^{}f^{}(_{c }^{2,i},_{c}^{1})-_{j=1}^{}f^{}(_{c}^{2,j},_{c}^{1})\ _{c}(_{c}^{2,j}|_{c}^{1},_{c}^{1},),\]

where \(\{_{2}^{1},,_{2}^{}\}\) represents the \(\) sampled solutions. There are \(\) sub-problems \(_{c}^{0},c\{1,,,, \}\) generated based on \(\{_{0}^{1},,_{0}^{}\}\) in the first conquering stage with constraints \(_{c}^{0}\). \(\) can be regarded as the batch size of sub-problems, and \(_{c}^{1},_{c}^{1},c\{1,,\}\) is the sub-problems and constraints in the second conquering stage. The \(\) sampled sub-solutions for sub-problem \(_{c}^{0},_{c}^{1},c\{1,, \}\) are noted as \(\{_{c}^{1,i},,_{c}^{1,}\},\{_{c}^{2,i},, _{c}^{2,}\}\).

### Application: Applicability in General CO Problems

This subsection discusses the applicability of UDC to various CO problems. Most existing neural divide-and-conquer methods rely on problem-specific designs, so they are only applicable to limited CO problems. UDC uses no heuristics algorithms or problem-specific designs in the whole solving process, thus can be applied to general offline CO problems that satisfy the following three conditions:

* The objective function contains only decomposable aggregate functions (i.e., no functions like Rank or Top-k).
* The legality of initial solutions and sub-solutions can be ensured with feasibility masks.
* The solution of the divided sub-problem is not always unique.

For the second condition, the proposed UDC is disabled on complex CO problems whose solution cannot be guaranteed to be legal through an autoregressive solution process (e.g., TSPTW). For the third condition, on certain CO problems such as the MIS problem on dense graphs or job scheduling problems, the solution of sub-problems has already been uniquely determined, so the conquering stages become ineffective.

## 4 Experiment

To verify the applicability of UDC in the general CO problem, we evaluate the UDC on 10 combinatorial optimization problems that satisfy the conditions proposed in Section 3.3, including TSP, CVRP, KP, MIS, ATSP, Orienteering Problem (OP), PCTSP, Stochastic PCTSP (SPCTSP), OpenVRP (OVRP), and min-max multi-agent TSP (min-max mTSP). Detailed formulations of these problems are provided in the Appendix B.

**Implementation** By employing lightweight networks for dividing policies, UDC can be directly trained on large-scale CO instances (e.g., TSP500 and TSP1,000). To learn more scale-independent features, we follow the varying-size training scheme  in training and train only one UDC model for each CO problem. The Adam optimizer  is used for all involved CO problems and the detailed hyperparameters for each CO problem are provided in Appendix D.1. Most training tasks can be completed within 10 days on a single NVIDIA Tesla V100S GPU.

**Baselines** This paper compares the proposed UDC to advanced heuristic algorithms and existing neural solvers for large-scale CO problems, including **classical solvers** (LKH , EA4OP , ORTools ), **single-stage SL-based sub-path solvers** (BQ , LEHD ), **single-stage RL-based constructive solvers** (AM , POMO, ELG , ICAM , MDAM , et al.), **heatmap-based solvers** (DIMES , DIFUSCO , et al.) and **neural divide-and-conquer methods** (GLOP , TAM , H-TSP, et al.). The implementation details and settings of all the baseline methods are described in Appendix G.

**Results on 500-node to 2,000-node Instances.** We first examine the performance of UDC models on large-scale CO problems ranging from 500-node to 2,000-node. Both single-stage SL-based constructive methods  and single-stage sub-path solvers  cannot be directly trained on this scale, so we report the results of models trained on the maximum available scale for these baselines. For UDC, we provide not only the greedy results (UDC-\(_{2}\)) but also the results after conducting more conquering stages (i.e., 50 and 250 stages, labeled as UDC-\(_{50}\), UDC-\(_{250}\)). In the following conquering stages after a greedy solution, the starting point of sub-problem dividing is no longer fixed at \(\) but is sampled from Uniform\((0,n)\). For better performances, we sample \(=50\) initial

  & &  &  &  \\ Methods & Obj.\(\) & Gap & Time & Obj.\(\) & Gap & Time & Obj.\(\) & Gap & Time \\     } & LKH3 & **16.52** & - & 5.5m & **23.12** & - & 24m & **32.45** & - & 1h \\   & Attn-MCTS* & 16.97 & 2.69\% & 6m & 23.86 & 3.20\% & 12m & 33.42 & 2.99\% & - \\  & DIMES* & 16.84 & 1.93\% & 2.2h & 23.69 & 2.47 & 4.6h & - & - & - \\  & DIPUSCO+Search* & 17.48 & 5.80\% & 19m & 25.11 & 8.61\% & 59.2m & - & - \\    } & BO & 16.72 & 1.18\% & 46s & 23.65 & 2.27\% & 1.9m & 34.03 & 4.86\% & 3m \\  & LEHD & 16.78 & 1.56\% & 16s & 23.85 & 3.17\% & 1.6m & 34.71 & 6.98\% & 12m \\   & POMO & 20.19 & 22.19\% & 1m & 32.50 & 40.57\% & 8m & 50.89 & 56.85\% & 10m \\  & ELG & 17.18 & 4.00\% & 2.2m & 24.78 & 7.18\% & 13.7m & 36.14 & 11.37\% & 14.2m \\  & ICAM & **16.65** & 0.77\% & 38s & 23.49 & **1.58\%** & 1m & 34.37 & 5.93\% & 3.8m \\   & GLOP(more revisions) & 16.88 & 2.19\% & 1.5m & 23.84 & 3.12\% & 3.5m & 33.71 & 3.89\% & 9m \\  & SO-mixed* & 16.94 & 2.55\% & 32m & 23.77 & 2.79\% & 32m & 33.35 & 2.76\% & 14m \\  & H-TSP & 17.55 & 6.22\% & 20s & 24.72 & 6.91\% & 40s & 34.88 & 7.49\% & 48s \\   & UDC-\(_{2}\)(\(\)=50, greedy) & 16.94 & 2.53\% & 20s & 23.79 & 2.92\% & 32s & 34.14 & 5.19\% & 23s \\  & UDC-\(_{50}\)(\(\)=50) & 16.78 & 1.58\% & 4m & 23.53 & 1.78\% & 8m & 33.26 & 2.49\% & 15m \\    } & LKH3 & **37.23** & - & 7.1h & 46.40 & 7.90\% & 14m & 64.90 & 8.14\% & 40m \\   & BO & 38.44 & 3.25\% & 47s & 44.17 & 2.72\% & 55s & 62.59 & 4.29\% & 3m \\   & LEHD & 38.41 & 3.18\% & 17s & 43.96 & 2.24\% & 1.3m & 61.58 & 2.62\% & 9.5m \\    & ELG & 38.34 & 2.99\% & 2.6m & 43.58 & 1.33\% & 15.6m & - & - & - \\   & ICAM & 37.49 & 0.69\% & 42s & 43.07 & 0.16\% & 26s & 61.34 & 2.21\% & 3.7m \\    & L2D* & - & - & - & 46.3 & 7.67\% & 3m & 65.2 & 8.64\% & 1.4h \\   & TAM(LKH3)* & - & - & - & 46.3 & 7.67\% & 4m & 64.8 & 7.97\% & 9.6m \\   & GLOP-G (LKH-3) & 42.45 & 11.73\% & 2.4m & 45.90 & 6.74\% & 2m & 63.02 & 5.01\% & 2.5m \\    } & UDC-\(_{2}\)(\(\)=50, greedy) & 40.04 & 7.57\% & 1m & 46.09 & 7.18\% & 2.1m & 65.26 & 8.75\% & 5m \\   & UDC-\(_{50}\)(\(\)=50) & 38.34 & 3.01\% & 7.7m & 43.48 & 1.11\% & 14m & 60.94 & 1.55\% & 23m \\   & UDC-\(_{250}\)(\(\)=50) & 37.99 & 2.06\% & 35m & **43.00** & - & 1.2h & **60.01** & - & 2.15h \\  

Table 2: Objective function (Obj.), Gap to the best algorithm (Gap), and solving time (Time) on 500-node. 1,000-node, and 2,000-node TSP and CVRP. All TSP test sets and CVRP500 test sets contain 128 instances. CVRP1,000 and CVRP2,000 contain 100 instances (following the generation settings in ). The overall best performance is in bold and the best learning-based method is marked by shade.

solutions \(_{0}\) for TSP and CVRP. The comparison results of TSP and CVRP are shown in Table 2 and the results of OP, PCTSP, and SPCTSP are exhibited in Tabel 4, UDC demonstrates the general applicability, exhibiting consistent competitive performances in all the five involved CO problems. After multiple conquering stages, UDC can achieve significantly better results.

UDC variants demonstrate the best results in PCTSP, SPCTSP, CVRP1,000, and CVRP2,000, together with the best learning-based method in most involved CO problems. The UDC-\(_{250}\)(\(\)=1) showcases extensive efficiency in OP, PCTSP, and PCTSP. Compared to sub-path solvers BQ  and LEMD , UDC variants exhibit more competitive results on TSP1,000, TSP2,000, CVRP, and OP. RL-based constructive solver ICAM  with 8 augmentation  is outstanding performance on TSP500, TSP1,000, and CVRP500 (i.e., relatively smaller scale) but UDC variants demonstrates better effect and efficiency in other test sets. Moreover, compared to existing neural divide-and-conquer methods GLOP , H-TSP , and TAM(LKH3) , UDC achieves significant advantages in performance with no heuristic design, which preliminarily implies the significance of a unified training scheme for neural divide-and-conquer methods.

**Results on Benchmark Datasets.** We also evaluate the proposed UDC on benchmark datasets TSPLib  and CVRPLib . We adopt the instances with 500-node to 5,000-node in TSPLib, CVRP Set-X , and very large-scale CVRP dataset Set-XXL as test sets. Instances in these datasets often with special distributions and can better represent real-world applications. As shown in Table 4, UDC variant UDC-\(_{250}\) exhibits competitive benchmark results, demonstrating the best learning-based result on CVRPLib. The proposed UDC can also demonstrate superiority in the other 5 involved CO problems, very large-scale instances. Results of these experiments are shown in Appendix D.2 and Appendix D.5, respectively. And as to AppendixD.3 and AppendixD.4, UDC can also exhibit outstanding out-of-domain generalization ability on TSP, CVRP, and MIS instances with different distributions.

  & &  &  &  \\ Methods & \)} &  &  & \)} &  &  & \)} &  &  \\    } & EA4OP & **81.70** & - & 7.4m & **117.70** & - & 63.4m & **167.74** & - & 38.4m \\  & BQ-bs16* & - & 4.10\% & 10m & - & 10.68\% & 17m & - & - & - \\   & AM & 64.45 & 21.11\% & 2s & 79.28 & 32.64\% & 5s & 99.37 & 40.76\% & 2s \\  & AM-bs1024 & 68.59 & 16.04\% & 1.2m & 79.65 & 32.32\% & 4m & 95.48 & 43.08\% & 1.6m \\  & MDAM-bs50 & 65.78 & 19.49\% & 2.6m & 81.35 & 30.88\% & 12m & 105.43 & 37.15\% & 7m \\  & Sym-NCO & 73.22 & 10.38\% & 27s & 95.36 & 18.98\% & 1.5m & 121.37 & 27.64\% & 20s \\   & UDC-\(_{2}\)(\(\)=1) & 75.28 & 7.86\% & 17s & 109.42 & 7.03\% & 25s & 135.34 & 19.32\% & 5s \\  & UDC-\(_{50}\)(\(\)=1) & 78.02 & 4.51\% & 23s & 111.72 & 5.08\% & 33s & 139.25 & 16.99\% & 8s \\  & UDC-\(_{250}\)(\(\)=1) & 79.27 & 2.98\% & 48s & 113.27 & 3.76\% & 1.1m & 144.10 & 14.09\% & 21s \\    } & OR-Tools* & 14.40 & 9.38\% & 16h & 20.60 & 12.66\% & 16h & - & - & - \\  & AM-bs1024 & 19.37 & 47.13\% & 14m & 34.82 & 90.42\% & 23m & 87.53 & 249.60\% & 10m \\  & MDAM-bs50 & 14.80 & 13.30\% & 6.5m & 22.21 & 21.43\% & 53m & 36.63 & 46.30\% & 43m \\  & Sym-NCO & 19.46 & 47.81\% & 1.3m & 29.44 & 61.01\% & 7m & 36.95 & 47.58\% & 2m \\   & GLOP & 14.30 & 9.03\% & 1.5m & 19.80 & 8.08\% & 2.5m & 27.25 & 8.84\% & 1m \\   & UDC-\(_{2}\)(\(\)=1) & 13.95 & 5.99\% & 34s & 19.50 & 6.65\% & 1m & 27.22 & 8.73\% & 18s \\  & UDC-\(_{250}\)(\(\)=1) & 13.25 & 0.64\% & 42s & 18.46 & 0.97\% & 1.3m & 25.46 & 1.68\% & 23s \\  & UDC-\(_{250}\)(\(\)=1) & **13.17** & & 80s & **18.22** & 0.1m & 1.5m & **25.04** & & 53s \\    } & AM-bs1024 & 33.91 & 156.11\% & 1.2m & 47.55 & 156.42\% & 4m & 76.02 & 199.48\% & 14m \\  & MDAM & 14.80 & 11.80\% & 6.5m & 21.96 & 18.42\% & 53m & 35.15 & 38.48\% & 43m \\    & UDC-\(_{2}\)(\(\)=1) & 14.21 & 7.32\% & 42s & 20.01 & 7.92\% & 1.1m & 28.15 & 10.91\% & 20s \\   & UDC-\(_{250}\)(\(\)=1) & 13.40 & 1.12\% & 1m & 18.93 & 2.06\% & 1.4m & 26.24 & 3.38\% & 29s \\   & UDC-\(_{250}\)(\(\)=1) & **13.24** & - & 2.3m & **18.54** & - & 2.6m & **25.38** & - & 1.1m \\  

Table 3: Experiment results on 500-node to 2,000-node OP and PCTSP and SPCTSP. The 500-scale, 100-scale, and 2,000-scale problems contain 128, 128, and 16 instances, respectively. “bs-\(k\)” represents \(k\)-width beam search . The overall best performance is in bold and the best learning-based method is marked by shade.

## 5 Discussion

The experimental results have preliminarily proven that the proposed UDC method achieves excellent results on a wide range of large-scale CO problems with outstanding efficiency. In this section, we conduct ablation experiments to verify the necessity of components in training UDC models. Appendix E includes the ablation experiments on the testing procedure of UDC, including the ablation on the number of conquering stages \(r\) and the number of sampled solutions \(\).

### Unified Training Scheme versus Separate Training Scheme

For neural divide-and-conquer methods, the unified training process does not require special designs for pre-training conquering policies, thus being easier to implement compared to separate training processes. Moreover, unified training can also avoid the antagonism between the dividing and conquering policies. To validate this, we pre-train a conquering policy (i.e., ICAM) on uniform TSP100 data and then train two ablation variants of the original UDC (separate training scheme): one variant uses the pre-trained conquering policy in the subsequent joint training (i.e., Pre-train + Joint Training, similar to H-TSP ), and the other variant only train the dividing policy afterward (i.e., Pre-train + Train Dividing, similar to TAM  & GLOP ). According to the training curves of UDC and the two variants in Figure 3, the unified training scheme demonstrates superiority, while the Pre-train step leads the training into local optima, which significantly harms the convergence of UDC.

### Ablation study: Conquering Policy

UDC adopts the state-of-the-art constructive solver ICAM  for conquering sub-TSP. However, the selection of constructive solvers for the conquering policy is not limited. We employ POMO  for the conquering policy, with experiment results of this version (i.e., UDC(POMO)) on TSP500 and TSP1,000 shown in Table 5.1 (\(=50\) for all variants). The UDC framework demonstrates conquering-policy-agnosticism, as it does not show significant performance degradation when the conquering policy becomes POMO.

    \\  Dataset, \(N\) & LEHD & ELG augx8 & GLOP-LKH3 & TAM(LKH3) & UDC-\(_{2}\) & UDC-\(_{250}\) \\  TSPLib,(500,1,000) & 4.1\% & 8.7\% & 4.0\% & - & 9.5\% & 6.0\% \\ TSPLib,(1,000,5,000) & 11.3\% & 15.5\% & 6.9\% & - & 12.8\% & 7.6\% \\   \\  Set-X,(500,1,000) & 17.4\% & 7.8\% & 16.8\% & 9.9\% & 16.5\% & 7.1\% \\ Set-XXL,(1,000,10,000) & 22.2\% & 15.2\% & 19.1\% & 20.4\% & 31.3\% & 13.2\% \\   

Table 4: TSPLib and CVRPLib results, the best learning-based result is marked by shade

Figure 3: Training curves of UDC variants with different training schemes.

   Optimality Gap & TSP500 & TSP1,000 \\  UDC(ICAM)-\(_{2}\) & 2.54\% & 2.92\% \\ UDC(ICAM)-\(_{50}\) & 1.58\% & 1.78\% \\ UDC(POMO)-\(_{2}\) & 2.64\% & 3.41\% \\ UDC(POMO)-\(_{50}\) & 1.64\% & 1.82\% \\   

Table 5: Results of UDC variants with various conquering policies.

In Appendix E.4, we further validate the conquering-policy-agnosticism of UDC on KP. We also conduct other ablation studies to demonstrate the necessity of other components of the proposed UDC, including the Reunion step in the DCR training method and the coordinate transformation in the conquering stage.

## 6 Conclusion, Limitation, and Future Work

This paper focuses on neural divide-and-conquer solvers and develops a novel training method DCR for neural divide-and-conquer methods by alleviating the negative impact of sub-optimal dividing results. By using DCR in training, this paper enables a superior unified training scheme and proposes a novel unified neural divide-and-conquer framework UDC. UDC exhibits not only outstanding performances but also extensive applicability, achieving significant performance advantages in 10 representative large-scale CO problems.

**Limitation and Future Work.** Although UDC has achieved outstanding performance improvements, we believe that UDC can achieve better results through designing better loss functions. In the future, we will focus on a more suitable loss for the UDC framework to further improve training efficiency. Moreover, we will attempt to extend the applicability of UDC to CO problems mentioned in Section 3.3 that are not applicable in the current version.

## Acknowledge

This work was supported by the National Natural Science Foundation of China (Grant No. 62106096 and Grant No. 62476118), the Natural Science Foundation of Guangdong Province (Grant No. 2024A1515011759), the National Natural Science Foundation of Shenzhen (Grant No. JCYJ20220530113013031).