# On the Stability and Generalization of Meta-Learning

Yunjuan Wang

Department of Computer Science

Johns Hopkins University

Baltimore, MD, 21218

ywang509@jhu.edu

&Raman Arora

Department of Computer Science

Johns Hopkins University

Baltimore, MD, 21218

arora@cs.jhu.edu

###### Abstract

We focus on developing a theoretical understanding of meta-learning. Given multiple tasks drawn i.i.d. from some (unknown) task distribution, the goal is to find a good pre-trained model that can be adapted to a new, previously unseen, task with little computational and statistical overhead. We introduce a novel notion of stability for meta-learning algorithms, namely _uniform meta-stability_. We instantiate two uniformly meta-stable learning algorithms based on regularized empirical risk minimization and gradient descent and give explicit generalization bounds for convex learning problems with smooth losses and for weakly convex learning problems with non-smooth losses. Finally, we extend our results to stochastic and adversarially robust variants of our meta-learning algorithm.

## 1 Introduction

Traditional machine learning algorithms excel at generalizing, but they often require extensive training data and assume that both training and test data come from the same distribution or task. In real-world scenarios, large sets of training data from a single task are often lacking. Instead, training data may stem from diverse tasks with shared similarities, while test data come from entirely new tasks. The challenge is to rapidly adapt to these unseen tasks without the need to train from scratch.

To address this challenge, meta-learning, also referred to as learning-to-learn, has emerged as an effective approach. Meta-learning has gained significant attention recently , with applications spanning across various domains including computer vision  and robotics , ranging from few-shot classification , hyperparameter optimization , to personalized recommendation systems .

As the name suggests, meta-learning operates on two levels of abstraction to enhance learning over time. On an intra-task level, the learner needs to find models that perform well on individual tasks. On a meta-level, the learner needs to figure out useful meta-information, perhaps a prior over tasks, that relates different tasks and allows transferring and adaptation of knowledge to new unseen tasks efficiently (both in terms of statistical as well computational overhead). It is typical to represent such meta-information in the form of a pre-trained model, which we can represent using certain meta-parameters. Distinct from a standard setting, meta-learning involves training on a diverse set of tasks. At test time, we evaluate the performance of the pre-trained model on new unseen tasks while allowing it to adapt using a small sample on the test task.

An increasing body of empirical research is dedicated to advancing meta-learning algorithms, among which model-agnostic meta-learning (MAML)  stands out as a prominent approach. MAML is designed to find a good meta-parameter w which facilitates the learning of task-specific parameters through a single step of gradient descent. In particular, given a set of \(m\) tasks denoted as \(\{_{j}\}_{j=1}^{m}\), MAML estimates the meta-parameter as \(=*{argmin}_{}_{j=1}^{m}L( _{j},_{j})\), where task-specific parameters are computed as \(_{j}=- L(,_{j})\).

However, a notable limitation of MAML is that it requires computing second-order derivatives, which is computationally demanding for deep neural networks in practical applications. This computational complexity also poses a challenge for a theoretical understanding of MAML, an aspect that remains largely under-explored. To mitigate this challenge, several MAML variants have been proposed, including first-order MAML (Finn et al., 2017), Reptile (Nichol et al., 2018), and iMAML (Rajeswaran et al., 2019). Owing to its success, MAML has been used for robust adversarial meta-learning (Yin et al., 2018; Goldblum et al., 2020; Wang et al., 2021; Collins et al., 2020), differential private meta-learning (Li et al., 2019), and personalized federated learning (Chen et al., 2018; Fallah et al., 2020).

Another popular framework for meta-learning is based on a "proximal" update, wherein the task-specific parameter are iteratively learned by minimizing the empirical loss and an \(_{2}\) regularizer (Denevi et al., 2018; Zhou et al., 2019; Denevi et al., 2019; Jiang et al., 2021). Given a task \(\) and a meta-parameter \(\), the task-specific parameter \(\) are defined as \(=*{argmin}_{}L(;)+ \|-\|^{2}\). This regularization strategy ensures that the task-specific parameter remains close to the meta-parameter. A similar strategy has been explored in other contexts. For example, Kuzborskij and Orabona (2017) study the problem of hypothesis transfer learning and show a fast rate on the generalization error of a task-specific parameter \(\) returned by regularized empirical risk minimization conditioned on a good meta-parameter \(\). Yet, it remains unclear how to ensure finding such a good meta-parameter, _provably_. Relatedly, Denevi et al. (2019) study stochastic gradient descent with biased regularization for linear model and incrementally update the bias (meta-parameter). Concurrently, Zhou et al. (2019) proposed the Meta-Prox algorithm as a generic stochastic meta-learning approach. Specifically, given a set of meta-training tasks \(_{1},,_{m}\), the meta-parameter \(\) is estimated by solving \(_{}_{j=1}^{m}_{}L(,_{j})+ \|-\|^{2}\). Zhou et al. (2019) argue that Meta-Prox is a generalization of MAML since the gradient descent update in MAML can be viewed as taking the first-order Taylor expansion of the objective, (Zhou et al., 2019, Section 3.1).

In this work, we adopt the framework of Zhou et al. (2019) to study meta-learning from a theoretical perspective. Given \(m\) tasks drawn i.i.d. from some (unknown) task distribution \(\), our goal is to find a good pre-trained model (the meta-parameter) which can be adapted to a new unseen task, drawn i.i.d. from \(\), at test time, using gradient descent. Our key contributions are as follows.

1. We introduce a novel notion of stability for meta-learning algorithms, namely uniform meta-stability. For \(\) uniformly meta-stable algorithm, we bound the generalization gap by \((+/(mn)})\).
2. We consider two variants of task-specific learning - based on regularized empirical risk minimization (RERM) and gradient descent (GD) - within our meta-learning framework. We apply our stability-based analysis to these variants to learning problems with convex, smooth losses and weakly convex, non-smooth losses. Our results are summarized in Table 1.

3. We extend our results to stochastic and adversarially robust variants of our meta-learning algorithm.

### Related Work

Algorithmic Stability Analysis.In many machine learning problems, standard learning theoretic tools, such as uniform convergence, do not apply since the associated complexity measures are unbounded or undefined (e.g., nearest neighbor classification), or yield guarantees that are not meaningful. Stability-based analysis is an alternative approach for obtaining generalization bounds

   Algorithm & Loss & Conditions & Uniform meta-stability \(\) \\  Algo. 1 with RERM & convex, \(G\)-Lipschitz & \(\) & \(}{ m}+}{ n}\) \\  Algo. 1 with RERM & convex, \(H\)-smooth, \(M\)-bounded & \(\), \( H\) & \(+\) \\  Algo. 1 with GD & convex, \(G\)-Lipschitz, \(H\)-smooth & \(\), \(\) & \(}{ m}+}{ n}\) \\  Algo. 1 with GD & \(\)-weakly convex, \(G\)-Lipschitz & \(\!\!\), \(\!\!\), \(\!\!2\) & \(G^{2}\!}+}{ m}+}{  n}\) \\  Algo. 3 with GD & \(\)-weakly convex, \(G\)-Lipschitz & \(\!\!\), \(\!\!\), \(\!\!2\) & \(G^{2}\!}+}{ m}+}{  n}\), w.h.p. \\   

Table 1: Bounds on uniform meta-stability \(\) for different families of learning problems. Here, \(\) is the step-size for GD for task-specific learning, \(\) is the step-size for GD for meta-parameter learning, \(m\) is the number of tasks during training, \(n\) is the number of training data for the task at test time.

in such settings, introduced by Bousquet and Elisseeff (2002) and further developed in a long line of influential works (Elisseeff et al., 2005; Mukherjee et al., 2006; Shalev-Shwartz et al., 2010; Liu et al., 2017). More recently, there have been significant breakthroughs in this field, with the work of Feldman and Vondrak (2018, 2019); Bousquet et al. (2020); Klochkov and Zhivotovskiy (2021), thereby improving the high probability bounds for uniformly stable learning algorithms beyond those established by Bousquet and Elisseeff (2002). These results are complemented by Hardt et al. (2016), who provide the generalization bounds via algorithmic stability analysis of stochastic gradient for stochastic convex optimization with smooth loss functions. Subsequent work by Bassily et al. (2020) improves upon these results by removing the smoothness assumption, while Zhou et al. (2022); Lei (2023) advance the state-of-the-art by relaxing the convexity assumption.

Theoretical Guarantees for Meta-Learning.There has been significant progress in understanding the theoretical aspects of meta-learning, both in terms of convergence guarantees (Fallah et al., 2019; Ji et al., 2020; Mishchenko et al., 2023) and the generalization guarantees. The first generalization analysis can be traced back to Baxter (2000), who assumed that all tasks are sampled i.i.d. from the same task distribution. Subsequent works have enriched the guarantees through various learning theoretic constructs, including VC theory (Ben-David and Schuller, 2003; Maurer, 2009; Maurer et al., 2016), information-theoretic tools (Chen et al., 2021; Jose and Simeone, 2021; Jose et al., 2021; Rezazadeh et al., 2021; Hellstrom and Durisi, 2022), PAC-Bayes framework (Pentina and Lampert, 2014; Amit and Meir, 2018; Rothfuss et al., 2021; Farid and Majumdar, 2021; Liu et al., 2021; Ding et al., 2021; Rezazadeh, 2022; Riou et al., 2023; Zakerinia et al., 2024), etc. Other works that do not rely on the task distribution assumption instead choose to get a handle on the bound by defining certain metrics to measure either the task similarity (Du et al., 2020; Tripuraneni et al., 2020; Guan and Lu, 2021) or the divergence between the new tasks and the training sample for the training tasks (Fallah et al., 2021). Finally, several works focus on the online meta-learning setting, also referred to as the lifelong learning (Pentina and Lampert, 2014; Balcan et al., 2019; Denevi et al., 2019; Bu et al., 2021).

A prominent line of work, starting with that of Maurer (2005), focuses on giving theoretical guarantees for meta-learning via algorithmic stability analysis. More recently, Chen et al. (2020) establish connections between single-task learning with support/query (episodic) meta-learning algorithms, providing generalization gap of \((1/)\) (where \(m\) is the number of tasks) for smooth functions that is independent of the sample size \(n\) - this was shown to be nearly optimal in Guan et al. (2022). Subsequently, Fallah et al. (2021) show a bound of \((1/mn)\) for strongly convex functions and by leveraging a new notion of stability. Al-Shedivat et al. (2021) extend the result of Maurer (2005) to practical meta-learning algorithms for Lipschitz and smooth losses. Farid and Majumdar (2021) derive a PAC-Bayes bound to address the qualitatively different challenges of generalization within the task compared to that at the meta-level. Other relevant work includes analyzing the stability of bilevel optimization (Bao et al., 2021) and federated learning (Sun et al., 2024) for smooth functions.

## 2 Problem Setup and Preliminaries

Notation.Throughout the paper, we denote scalars and vectors with lowercase italics and lowercase bold Roman letters, respectively; e.g., \(u\), \(u\). We work in a Euclidean space and use \(\|\|\) and \(\|\|_{2}\) to denote the \(_{2}\) norm. We use \([n]\) to represent the set \(\{1,2,,n\}\), and define \([n]\) to be the uniform distribution over \([n]\). Let \(_{}\) be the Euclidean projection onto \(\). We adopt the standard O-notation and use \(\) and \(\) interchangeably. We use \(}\) to hide poly-logarithmic dependence on the parameters.

Let \(,\) denote the input and output spaces, respectively. Consider a supervised learning setting where each data point is denoted by \(=(,)\) drawn from some unknown distribution \(\) over \(=\). We consider a hypothesis space \(\) (maps from \(\)) parameterized by \(\), where \(^{d}\) is a closed set with radius \(D\). Let \(:^{d}^{+}\) denote the loss function. We say that a loss function \(\) is \(\)-bounded if \(,,(, ) M\); \(\) is \(\)-strongly convex if \(_{1},_{2}, ,(_{1},)(_{2},) +(_{2},),_{1}-_{2} +\|_{1}-_{2}\|_{2}^{2}\); if \(=0\), we say \((,)\) is convex. We say \(\) is \(G\)-Lipschitz continuous if \(_{1},_{2},,\|(_{1},)-(_{2},)\|_{2}  G\|_{1}-_{2}\|_{2}^{2}\); \(\) is \(\)-smooth if \(,_{1},_{2}, ,\|(_{1},)-(_ {2},)\|_{2}\|_{1}-_{2} \|_{2}\).

In a standard (single-task) learning setup, given a model \(\), the expected loss on task \(\) and the empirical loss on a training sample \(\) drawn i.i.d. from \(\), are defined, respectively, as follows.

\[L(,)=_{}[(, )]; L(,)=_{z}(,).\]

In a meta-learning framework, we consider distributions \(\{_{j}\}_{j=1}^{m}\) associated with \(m\) different tasks that are drawn from some (unknown) task distribution \(\). For each task \(j\), we assume that the learner has access to \(n\) training examples drawn i.i.d. from \(_{j}\), i.e., \(_{j}=\{_{j}^{i}\}_{i=1}^{n} _{j}^{n}\). We denote the cumulative training data as \(=\{_{j}\}_{j=1}^{m}\), and refer to it as the meta-sample.

A meta-learning algorithm \(\) takes the meta-sample \(\) as input and outputs an algorithm \(():()^{n}\). The performance of the meta-algorithm \(\) is measured in terms of its ability to generalize w.r.t. loss \(()\) to a new (previously unseen) task from the task distribution \(\); we also refer to it as the _transfer risk_:

\[L((),)=_{}_{ ^{n}}L(()(), ).\]

The goal of meta-learning is to learn a useful prior over tasks to help with rapid adaptation to new tasks. Formally, we pose the problem as learning a meta-model, parameterized by what we will refer to as meta-parameter w, that performs well on a variety of tasks. The hope is that the meta-parameter w can be adapted easily to a new task \(\); in particular, that a task-specific model u can be quickly learned from a task-specific training set \(^{n}\) of size \(n\) using the following proximal update:

\[=*{argmin}_{}L(, )+\|-\|^{2},\]

where \(>0\) is a regularization parameter.

```
0: Meta-sample \(\!=\!\{_{j}\}_{j=1}^{m}\), epochs \(T\), \(K\), step sizes \(\), \(\), regularization parameter \(\)
1:\(_{1}=0\).
2:for\(t=1,2,,T\)do
3:for\(j=1,,m\)do
4:\((_{t},_{j})=_{}(_{t},_{j},K,,)\) % Using Algorithm 2
5:endfor
6: Calculate the gradient, \( j\!\![m]\), \(\!F_{_{j}}((_{t},_{j}), _{t})\!=\!-\!((_{t},_{j})\!- \!_{t})\).
7: Update\(_{t+1}\!\!=\!\!_{t}\!-\!\!_{j=1}^{m} \!\!F_{_{j}}((_{t},\!_{j})\!,\!_{t})\)
8:\(_{t+1}\!=\!_{}(_{t+1})\)
9:endfor
10:return\(\,\,\,@_{}(_{T+1},\,\,,K,,)\) ```

**Algorithm 1** Prox Meta-Learning Algorithm \(\)

The meta-parameter w itself is learned on the given meta-sample \(\) by minimizing a regularized empirical loss averaged over tasks, where the regularization term penalizes the task-specific models in proportion to the \(_{2}\) distance from the meta-parameter :

\[}=*{argmin}_{}_{j=1}^{m}_{}F_{_{j}}(, ):=*{argmin}_{}_ {j=1}^{m}_{}[L(,_{j})+ \|-\|^{2}].\] (1)

The formulation above involves a bi-level optimization problem. The upper-level optimization involves finding the meta-parameter w which requires solving the lower-level optimization problem of finding task-specific model parameters u. We consider both Gradient Descent (GD) as well Regularized Empirical Risk Minimization (RERM) for task-specific learning (see Algorithm 2 for more details); for meta-learning we employ a gradient descent method (see Algorithm 1).

We would like to bound the transfer risk in terms of the _empirical multi-task risk_:

\[L((),)=_{j=1}^{m}L(( )(_{j}),_{j}).\]

To do so, we rely on the stability of the meta-learning algorithm.

Stability of Meta-Learning Algorithm.Given a meta-sample \(=\{_{j}\}_{j=1}^{m}\), define \(^{(j)}\) to be the meta-sample obtained by replacing the training samples \(_{j}\) for the \(j\)-th task, in \(\), by another i.i.d. sample \(_{j}^{}_{j}^{n}\). We refer to \(,^{(j)}\) as neighboring meta-samples. For a task-specific training sample \(=\{^{i}\}_{i=1}^{n}\), let \(^{(i)}\) denote the training data obtained by replacing the \(i\)-th example \(^{i}\) by another example \(^{}\) drawn independently; we refer to \(,^{(i)}\) as neighboring samples.

**Theorem 2.1** (Maurer ).: Suppose the meta-algorithm \(\) satisfies:1. (Uniform Stability of Single-Task Learning) For any meta-sample \(\) and any \(,^{(i)}\), \[|(()(),)-(( )(^{(i)}),)|.\]
2. (Uniform Stability of Meta-Learning) For any \(,^{(j)}\) and any given training set \(\), \[|L(()(),)-L(( ^{(j)})(),)|^{}.\]

Then, for \(M\)-bounded loss \(\), with probability at least \(1-\), we have that

\[L((),) L((),)+ (m^{}+M)+.\]

Theorem 2.1 follows using a simple extension of arguments in Bousquet and Elisseeff (2002). By utilizing sharper bounds tailored for uniformly stable algorithms (Bousquet et al., 2020), a tighter bound can be achieved, as demonstrated in Theorem 2.2 below. A similar result was shown in Guan et al. (2022) for episodic training algorithms (except there is no \(\)).

**Theorem 2.2**.: Suppose the meta-algorithm \(\) satisfies the same conditions as shown in Theorem 2.1. Then for \(M\)-bounded loss \(\), with probability at least \(1-\), we have that

\[L((),) L((),)+ ^{}(m)(1/)+M+.\]

## 3 Uniform Meta-Stability

Motivated by prior work (i.e., Theorem 2.1 and the definitions therein), we introduce a new notion of stability which measures the sensitivity of the learning algorithm as we replace both a task in the meta-sample as well as a single training example available for the task at test time.

**Definition** (Uniform Meta-Stability).: We say that a meta-learning algorithm \(\) is \(\)-uniformly meta-stable if for any neighbouring meta-samples \(,^{(j)}\), and neighboring samples \(,^{(i)}\), for any task \(\) and any \(\), we have that

\[|(()(),)-(( ^{(j)})(^{(i)}),)|.\]

The definition above is rather natural. Intuitively, for a meta-learning algorithm to transfer well, we require that the learning algorithms, i.e., \(()\) and \((^{})\), returned on two neighboring meta-samples, when trained on two neighboring samples return models that predict similarly. Our first result bounds the generalization gap in terms of the uniform meta-stability parameter.

**Theorem 3.1**.: Consider a meta-learning problem for some \(M\)-bounded loss function \(\) and task distribution \(\). Let \(\) be a meta-sample consisting of training samples on \(m\) tasks each of size \(n\), and let \(\) be a sample of size \(n\) on a previously unseen task \(\). Then, for any \(\)-uniformly meta-stable learning algorithm \(\), we have that with probability \(1-\),

\[L((),) L((),)+ (mn)(1/)+M.\]

The result above is a direct analogue of Theorem 2.1 with stability parameters \(,^{}\) both subsumed into a single meta-stability parameter. We do obtain a faster rate of convergence - as we instantiate concrete algorithms and specialize our results to specific problems in Section 4.1, we will see a notable improvement in rates from \(1/\) to \(1/m\), for \(n>m\).

We conclude the section by presenting an alternate notion of algorithmic meta-stability and a basic result that directly bounds the generalization gap for the meta-learning problem.

**Definition** (On-Average Meta-Stability).: Let \(\) be an (unknown) underlying task distribution. We say that a meta-learning algorithm \(\) is \(\)-on-average-replace-one-meta-stable if

\[_{\{_{j}^{n}\}_{j=1}^{m},( _{j}^{},_{j}^{})_{j}^{n+1},( _{j})_{j=1}^{m}^{m},j[m],i[n]} |(()(_{j}),_{j}^{i})-( (^{(j)})(_{j}^{(i)}),_{j}^{i}) |.\]

**Theorem 3.2**.: Let \(\) be an underlying task distribution. Given a meta-sample \(\), test task \(\), and \(^{n}\), for any \(\)-on-average-replace-one-meta-stable meta-learning algorithm \(\), we have that

\[_{\{_{j}^{n}\}_{j=1}^{m},\{ _{j}\}_{j=1}^{m}^{m}}[L((),)-L( (),)].\]

## 4 Bounding Transfer Risk

In this section, we consider a concrete meta-learning algorithm given in Algorithm 1.

### Convex and Smooth Losses

We begin with meta-learning problems with convex, Lipschitz (and potentially smooth) losses.

**Lemma 4.1**.: Assume that the loss function \(\) is convex and \(G\)-Lipschitz loss. Let \(\), \(^{(j)}\) denote neighboring meta-samples and \(\), \(^{(i)}\) the neighboring samples on a test task. Then, the following holds for Algorithm 1 with RFRM for task-specific learning (i.e., Option 1 for Algorithm 2) \( T 1\),

\[_{,,j[m],i[n]}()( )-(^{(j)})(^{(i)}) +.\]

Further, if \(\) is convex, \(M\)-bounded and \(H\)-smooth, then setting \( H\), \(\), we have \( T 1\),

\[_{,,j[m],i[n]}()( )-(^{(j)})(^{(i)}) }{2 n-H}+}{(m+1 )}.\]

We can now use the result above with Theorem 3.1 to get the following bound on the transfer risk.

**Theorem 4.2**.: The following holds for Algorithm 1 with step-size \(\) on a given meta-sample \(\), and RERM for task-specific learning (i.e., Option 1 for Algorithm 2), for all \(T 1\):

1. For convex, \(M\)-bounded, and \(G\)-Lipschitz loss functions, with probability at least \(1-\) \[L((),) L((),)+ (}{ n}+}{ m})+}}{}.\]
2. For convex, \(M\)-bounded, and \(H\)-smooth loss functions (\(H\)), with probability at least \(1-\) \[L((),) L((),)+ (+)+}}{}.\]

Next, we give analogous results for GD for task-specific learning (i.e., Option 2 for Algorithm 2), albeit for smooth loss functions. Lemma 4.3 bounds the output sensitivity of the meta-learning algorithm. We use it with Theorem 3.1 to give the generalization guarantee in Theorem 4.4.

**Lemma 4.3**.: Assume that the loss function is convex, \(G\)-Lipschitz and \(H\)-smooth. Let \(\), \(^{(j)}\) denote neighboring meta-samples and \(\), \(^{(i)}\) the neighboring samples on a test task. Then the following holds for Algorithm 1 with GD for task-specific learning (i.e., Option 2 for Algorithm 2) with \(\), for all \(T 1\) as long as we set \(\),

\[_{,,j[m],i[n]}()( )-(^{(j)})(^{(i)}) +.\]

**Theorem 4.4**.: Assume that the loss function is convex, \(M\)-bounded, \(G\)-Lipschitz and \(H\)-smooth. Suppose we run Algorithm 1 for \(T\) iterations with \(}\) on a given meta-sample \(\), and GD for task-specific learning (Option 2, Algorithm 2) with \(\). Then, with probability at least \(1-\),

\[L((),) L((),)+ (}{ m}+}{ n})+}}{}.\]

The results above show that meta-stable learning algorithms do not overfit. The bound on the generalization gap of \(}(++})\) is tighter than what we would obtain using prior work. Indeed, we show that Theorem 2.2 yields a rate of \(}(++})\) (see Theorems C.2 and C.3 in Appendix), which is worse for all \(m n^{2}\). Notably, the bounds on the generalization gap are independent of the number of iterations of the meta learning Algorithm 1 and the number of iterations of GD for Algorithm 2. This holds since the objective we are minimizing is strongly convex (given the strongly convex regularizer), which ensures that the output sensitivity (in Lemmas 4.3 and 4.1 are independent of \(T\) and \(K\). In itself, this should not be surprising since we only bound the generalization error in terms of the empirical error - the latter may not be small unless the algorithms have converged. To get a better handle on the generalization error we focus on excess (transfer) risk bounds in Section 4.3. But first we give a similar development for another important problem class.

### Weakly Convex and Non-smooth Losses

Here, we focus on a more practical setting of learning problems with loss functions that are weakly convex and non-smooth. The notion of weak convexity is often used in non-convex optimization literature in a variety of problems including robust phase retrieval (Davis et al., 2020) and dictionary learning (Davis and Drusvyatskiy, 2019); see Drusvyatskiy (2017) for an extended discussion.

**Definition**.: A function \(f()\) is \(\)_-weakly convex_ w.r.t. \(\|\|\) if \(f()+\|\|^{2}\) is convex in \(\).

The class of weakly convex functions is contained within the larger class of non-smooth functions and semi-smooth functions (Mifflin, 1977). It includes convex functions and smooth functions with Lipschitz continuous gradient as special cases; \(<0\) implies that the function is strongly convex. An important example from a practical perspective is that of training over-parameterized two-layer neural networks with smooth activation functions using a smooth loss (Richards and Rabbat, 2021). We first bound the sensitivity of Algorithm 1 for weakly convex and non-smooth losses.

**Lemma 4.5**.: Assume that the loss function is \(\)-weakly convex and \(G\)-Lipschitz. Let \(\), \(^{(j)}\) denote neighboring meta-samples and \(\), \(^{(i)}\) the neighboring samples on a test task. Then the following holds for Algorithm 1 with \( 2\), and GD for task-specific learning (i.e., Option 2 for Algorithm 2) with \(\), for all \(T 1\) as long as we set \(\),

\[_{,,j[m],i[n]}\|()( )-(^{(j)})(^{(i)})\|(8eG +2G)}++.\]

Using the result above in conjunction with Thm 3.1 gives the following bound on the transfer risk.

**Theorem 4.6**.: Assume that the loss function is \(\)-weakly convex, \(M\)-bounded, and \(G\)-Lipschitz. Suppose we run Algorithm 1 for \(T\) iterations with \(, 2\) on a meta-sample \(\), and GD for task-specific learning (Option 2, Algorithm 2) with \(\), Then, with probability at least \(1-\),

\[L((),) L((),)+ (G^{2}}+}{ m}+}{  n})+}}{ }.\]

Proof of Theorem 4.6 follows from Lemma 4.5 and Theorem 3.1. A few remarks are in order.

For learning rate \(\), Theorem 4.6 gives a rate of \(}(+++})\) on the generalization gap. This naturally suggests setting \(=\), where \(K\{m,n\}\) is the number of iterations of GD in task-specific learning. Then, similar to the discussion in Section 4.1, Theorem 4.6 gives a tighter bound, when \(n>m\), than those derived using prior work (Theorem 2.2); we refer the reader to Theorem D.4 in the appendix for further details.

Our proof technique shares similarities with Bassily et al. (2020). However, our result is not a straightforward application of theirs as we deal with a bi-level optimization problem and focus on weakly convex functions. It is worth noting that our results for weakly convex non-smooth losses require regularization parameter \( 2\), which can be chosen in practice using cross-validation.

The work most related to ours is that of Guan et al. (2022). However, our results are fundamentally different from theirs in several aspects. Firstly, the algorithms we study are different. Guan et al. (2022) focus on support/query (S/Q) training strategies (aka episodic training) where each task \(_{j}\) is split into two non-overlapping parts - the support set \(_{j}^{tr}\) for training the task-specific parameter and the query set \(_{j}^{ts}\) for measuring the algorithm's performance (Vinyals et al., 2016). The meta-parameter is learned by minimizing the loss computed over the query set. Such S/Q training strategy is popular for modern gradient-based meta-learning algorithm such as MAML for few-shot learning (Finn et al., 2017), where the optimization objective can be written as \(_{}_{j=1}^{m}L(- L(, _{j}^{tr}),_{j}^{ts})\). One notable limitation is that Guan et al. (2022) assume that the loss function on the task level, e.g., \(R(,_{j})=L(- L(,_{j} ^{tr}),_{j}^{ts})\), is convex or (Holder) smooth. Such an assumption is highly impractical, as demonstrated by (Mishchenko et al., 2023, Theorem 1, Theorem 2), which provides several counterexamples where \(L\) is convex and smooth but \(R\) is neither convex nor smooth. In contrast, we directly deal with \(L\) being weakly convex and nonsmooth. Our approach requires a more involved proof that deals with stability of bi-level optimization. This is in stark contrast with Guan et al. (2022) who directly reduce the meta-learning problem to a single-task learning problem without considering the bi-level structure of the problem.

The work of Fallah et al. (2021) proposed a notion of stability similar to ours. The difference is that they consider S/Q training and define the stability by changing a mini-batch of samples in \(_{j}^{tr}\) aswell as a single sample in \(_{j}^{ts}\). Moreover, their focus is primarily on strongly convex losses. They discuss generalization to training tasks and unseen tasks separately, as they do not assume all tasks are sampled from the same task distribution. Another related work of Guan and Lu (2021) present a generalization bound of \(()\) under a task relatedness assumption, where \(C\) captures the logarithm of the covering number of hypothesis class that possibly depends on the dimension \(d\). More recently, Riou et al. (2023) provide generalization bounds with a fast rate of \((+)\), albeit under an additional extended Bernstein's condition.

### Excess Transfer Risk

In the previous sections, we focused on establishing that meta-stable rules do not overfit to the meta-sample. In this Section, we focus on the question of whether meta-learning Algorithm 1 can achieve a small generalization error, i.e., are they guaranteed to transfer well on unseen tasks? We show that by focusing on the computational aspects, i.e., by bounding the optimization error in terms of the number of iterations. Furthermore, we give bounds on excess risk, wherein the benchmark is the performance of the best possible in-class predictor.

Let \(_{}=*{argmin}_{}L(,)\), \(_{j}^{}=*{argmin}_{}L( ,_{j})\), \( j[m]\) be the optimal task-specific hypotheses for the unseen task and the given training tasks, respectively. Given a meta-algorithm \(\), the excess transfer risk can be decomposed as follows:

\[()(), )-L(_{},)}{}}_{_{()}}= ()(),)- _{j=1}^{m}L(()(_{j}),_{j})}_{_{()}}+_{j=1}^{m}L( ()(_{j}),_{j})-L(_{j}^{ },_{j})}_{_{()}}\] \[+_{j=1}^{m}L(_{ }^{},_{j})-L(_{},_{j})}_{  0}+_{j=1}^{m}L(_{}, _{j})-L(_{},)}_{_{  j[m],_{j}>_{j}>_{j}>_{j} >_{j}>_{j}>_{m}=0}}.\]

To control excess risk, we need to bound \(_{()}\) and \(_{+()}\) simultaneously. The bounds on the first term are presented in the previous section. Here, we focus on analyzing the second term.

**Theorem 4.7**.: Assume that the loss \(\) is convex and \(G\)-Lipschitz. Define \(_{}^{}\!=\!*{argmin}_{}L(, _{j}), j[m]\). Suppose we run Algorithm 1 for \(T\) iterations with step-size \(=\), and using GD for task-specific learning (i.e., Option 2 for Algorithm 2), to find an algorithm \(()=_{}(_{T+1},)\) which is then run on \(_{j}\) for \(K\) iterations with step-size \(\). Then, we have that

\[L(()(_{j}),_{j})-_{}L (,_{j})\, 1.29pt\,}{  K}+G^{2}+GD+_{T+1}-}^{2}+^{2}\]

where \(}\) is defined in Equation (1). Here \(^{2}:=_{j=1}^{m}}-_{}^{}^{2}\) is the approximation error, and \(_{T+1}-}^{2} 1.29pt \,(D^{2}+}{ K}+}{})\) is the optimization error.

Finally, to bound the excess transfer risk for convex and non-smooth losses, we use Theorem 4.6 with Theorem 4.7 to get that in expectation over the sampling of data (meta-sample \(\) and sample \(\))

\[[_{}()]\,\,[_{()}]\,+[_{+( )}]\, 1.29pt\,G^{2}}+}{  m}+}{ n}+}{ K}+G^{2}+GD+ }{T}+(G+2 D)^{2}+^{2}.\]

By properly choosing step size \(=(})\), we obtain that the expected excess transfer risk decays at a rate of \((}+++ +^{2})\). Similarly, for convex, Lipschitz and smooth losses, applying Theorem 4.4 with Theorem 4.7 and selecting \(=(})\) results in an expected excess transfer risk of \((}+++ +^{2})\). Therefore, as \(K,T,m,n\) tend to infinity, the excess risk converges to \(^{2}\). As \(\) represents the average distance between the optimal task-specific parameters \(_{j}\)'s and the optimal estimated meta-parameter \(}\), the excess risk is small when \(\) is small. It is also typical to set the regularization parameter \(\) inversely proportional to the sample size \(n\) (e.g., \(=(1/)\)).

Denevi et al. (2019) study the same algorithm as ours except in the online setting. However, the function classes they consider are limited to compositions of linear hypothesis classes with convex and closed losses. In contrast, our work considers a broader range of functions, encompassing not only convex, Lipschitz, and smooth functions but also weakly-convex and non-smooth functions. The bound on expected excess risk shown in Denevi et al. (2019) takes the form \((_{m}}{}+})\), where \(_{m}\) captures the relatedness among the tasks sampled from the task environment. Unfortunately, this bound relies on a specific choice of \(=(_{m}}})\), which depends on \(_{m}-\) quantity that is often not known a priori in practice. To compare with our work, set \(K=n,T=m\), \(=(1/)\), and \(=(1/)\).Then, applying Theorem 4.4 with Theorem 4.7, we obtain that \([_{}()]}{m} +)}{}\). Considering both \(_{m}\) and \(\) as constants, the bound on expected excess risk based on our analysis is tighter than that of Denevi et al. (2019) when \(n m\), a common setting studied in meta-learning framework.

We also conduct a simple experiment to empirically verify the tightness of our generalization bounds, which we defer to Appendix A due to space limitations.

## 5 Implications of the Generalization Bounds

Next, we present stochastic and adversarially robust variants of the meta-learning Algorithm 1.

### Proximal Meta-Learning with Stochastic Optimization

We adapt Algorithm 1 to utilize sampling-with-replacement where at each iteration we process the training set of a single task; see Algorithm 3 for more details. We show that with high probability the sensitivity of this stochastic meta-learning algorithm is bounded.

**Lemma 5.1**.: Assume that the loss function is \(\)-weakly convex and \(G\)-Lipschitz. Let \(\), \(^{(j)}\) denote neighboring meta-samples and \(\), \(^{(i)}\) the neighboring samples on a test task. Then, with probability at least \(1-(-T^{2}e^{2}/m^{2})\), the following holds for Algorithm 3 with \( 2\), and GD for task-specific learning (i.e., Option 2 for Algorithm 2) with \(\), for all \(T 1\) as long as we set \(\),

\[_{,,i[n],j[m]}\|()( )-(^{(j)})(^{(i)})\|(8eG +2G)}++.\]

### Robust Adversarial Proximal Meta-Learning

We consider inference-time adversarial attacks with a general threat model \(: 2^{}\). Specifically, given an input example \(,()^{d}\) represents the set of all possible perturbations of x that an adversary can choose from. This includes the typical examples such as the \(L_{p}\) threat models that are often considered in practice, or a discrete set of designed transformations.

Given a model parameter w, let \((,)=_{()} (,})\) denote the adversarial loss. We adapt the standard meta-learning framework simply by considering the robust variant, \(\), of the standard loss \(\). We denote the robust transfer risk and empirical robust multi-task risk as \(L_{}((),)\) and \(L_{}((),)\). Now, given meta-sample \(\), the goal is to learn a robust prior (e.g., a pre-trained model) for rapid adaptation to and robust generalization on new tasks. We adopt the framework presented in Section 2 except we use robust loss for task-specific training; indeed, using GD (Option 2) on robust loss in Algorithm 2 yields adversarial training. We use Algorithm 1 for meta-learning. We now relate a loss function with its adversarially robust counterpart.

**Proposition 5.2**.: Given a loss function \((,)\) and its adversarial counterpart \((,)\), the following holds: (1) If \(\) is \(G\)-Lipschitz (in its first argument), then \(\) is \(G\)-Lipschitz. (2) \(\) is **not**\(H\)-smooth even if \(\) is \(H\)-smooth. (3) If \(\) is \(H\)-smooth in w, then \(\) is \(H\)-weakly convex in w.

Using the result above with Theorem 3.1 yields the following bound on robust (transfer) risk.

**Corollary 5.3**.: Assume that the loss \(\) is \(M\)-bounded and \(H\)-smooth. Suppose we run Algorithm 1 for \(T\) iterations with \(},,>2H\), and wherein task-specific learning Algorithm 2 (GD) is invoked with robust loss \(\), we have that with probability at least \(1-\),

\[L_{}((),)\,{}\,L_{}( (),)+(G^{2}}+ }{ m}+}{ n})\!(mn) (1/)+}{}.\]

Note that prior work on robust adversarial meta-learning (Yin et al., 2018; Goldblum et al., 2020; Wang et al., 2021) focuses on empirical study of the problem; we present first theoretical guarantees.

## 6 Conclusion

In this paper, we introduce a novel notion of stability for meta-learning algorithms, namely uniform meta-stability, and offer a tighter bound on the generalization gap for the meta-learning problem compared to existing literature. We instantiate uniformly meta-stable learning algorithms and give generalization guarantees for both convex, smooth losses as well as weakly convex and non-smooth losses. Several avenues for further exciting research remain. For instance, it remains to be seen if our bounds are tight. Can we show lower bounds on the generalization error for meta-learning? Additionally, understanding how meta-learning relates to federated learning may offer insights on how to extend the theory to broader applications and inform the design of new algorithms. Finally, motivated by data privacy considerations, it would be interesting to extend our setup to privacy-preserving meta-learning, similar in spirit to the recent work of Zhou and Bassily (2022).