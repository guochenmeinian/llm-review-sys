# Replicable Clustering+

Footnote â€ : Authors are listed alphabetically.

 Hossein Esfandiari

Google Research

esfandiari@google.com &Amin Karbasi

Yale University, Google Research

amin.karbasi@yale.edu &Vahab Mirrokni

Google Research

mirrokni@google.com &Grigoris Velegkas

Yale University

grigoris.velegkas@yale.edu &Felix Zhou

Yale University

felix.zhou@yale.edu

###### Abstract

We design replicable algorithms in the context of statistical clustering under the recently introduced notion of replicability from Impagliazzo et al. (2022). According to this definition, a clustering algorithm is replicable if, with high probability, its output induces the _exact_ same partition of the sample space after two executions on different inputs drawn from the same distribution, when its _internal_ randomness is shared across the executions. We propose such algorithms for the statistical \(k\)-medians, statistical \(k\)-means, and statistical \(k\)-centers problems by utilizing approximation routines for their combinatorial counterparts in a black-box manner. In particular, we demonstrate a replicable \(O(1)\)-approximation algorithm for statistical Euclidean \(k\)-medians (\(k\)-means) with \(((k,d)k^{ k})\) sample complexity. We also describe an \(O(1)\)-approximation algorithm with an additional \(O(1)\)-additive error for statistical Euclidean \(k\)-centers, albeit with \(((k)(d))\) sample complexity. In addition, we provide experiments on synthetic distributions in 2D using the \(k\)-means++ implementation from sklearn as a black-box that validate our theoretical results2.

## 1 Introduction

The unprecedented increase in the amount of data that is available to researchers across many different scientific areas has led to the study and development of automated data analysis methods. One fundamental category of such methods is _unsupervised learning_ which aims to identify some inherent structure in _unlabeled_ data. Perhaps the most well-studied way to do that is by grouping together data that share similar characteristics. As a result, _clustering_ algorithms have become one of the central objects of study in unsupervised learning. Despite a very long line of work studying such algorithms, e.g. Jain and Dubes (1988), Hart et al. (2000), Anderberg (2014), there is not an agreed-upon definition that quantifies the quality of a clustering solution. Kleinberg (2002) showed that there is an inherent reason why this is the case: it is impossible to design a clustering function that satisfies three natural properties, namely _scale-invariance_, _richness of solutions_, and _consistency_. This means that the algorithm designer needs to balance several conflicting desiderata. As a result, the radically different approaches that scientists use depending on their application domain can be sensitive to several factors such as their random initialization, the measure of similarity of the data, the presence of noise in the measurements, and the existence of outliers in the dataset. All these issues give rise to algorithms whose results are not _replicable_, i.e., when we execute them on two different samples of the same population, they output solutions that vary significantly. This begs thefollowing question. Since the goal of clustering algorithms is to reveal properties of the underlying population, how can we trust and utilize their results when they fail to pass this simple test?

Replicability is imperative in making sure that scientific findings are both valid and reliable. Researchers have an obligation to provide coherent results and conclusions across multiple repetitions of the same experiment. Shockingly, a 2016 survey that appeared in Nature (Baker, 2016) revealed that 70% of researchers tried, but were unable to, replicate the findings of another researcher and more than 50% of them believe there is a significant crisis in replicability. Unsurprisingly, similar worries have been echoed in the subfields of machine learning and data science (Pineau et al., 2019, 2021). In this work, we initiate the study of replicability in clustering, which is one of the canonical problems of unsupervised learning.

### Related Works

**Statistical Clustering.** The most relevant previous results for (non-replicable) statistical clustering was established by Ben-David (2007), who designed \(O(1)\)-approximation algorithms for statistical \(k\)-medians (\(k\)-means) with \(O(k)\) sample complexity. However, their algorithm picks centers from within the samples and is therefore non-replicable.

**Combinatorial Clustering.** The flavor of clustering most studied in the approximation algorithms literature is the setting where we have a uniform distribution over finite points and the algorithm has explicit access to the entire distribution. Our algorithms rely on having black-box access to a combinatorial clustering oracle. See Byrka et al. (2017), Ahmadian et al. (2019) for the current best polynomial-time approximation algorithms for combinatorial \(k\)-medians (\(k\)-means) in general metrics with approximation ratio 2.675 (9). Also, see Cohen-Addad et al. (2022) for a \(2.406\) (\(5.912\)) approximation algorithm for the combinatorial Euclidean \(k\)-medians (\(k\)-means).

**Clustering Stability.** Stability in clustering has been studied both from a practical and a theoretical point of view (Ben-Hur et al., 2001; Lange et al., 2004; Von Luxburg and Ben-David, 2005; Ben-David et al., 2006; Rakhlin and Caponnetto, 2006; Ben-David et al., 2007; Von Luxburg et al., 2010). In most applications, it is up to the algorithm designer to decide upon the value of \(k\), i.e., the number of different clusters. Thus, it was proposed that a necessary condition it should satisfy is that it leads to solutions that are not very far apart under _resampling_ of the input data (Ben-Hur et al., 2001; Lange et al., 2004). However, it was shown that this notion of stability for center-based clustering is heavily based on symmetries within the data which may be unrelated to clustering parameters (Ben-David et al., 2006). Our results differ from this line of work in that we require the output across two separate samples to be _exactly the same_ with high probability, when the randomness is shared. Moreover, our work reaffirms Ben-David et al. (2006) in that their notion of stability can be perfectly attained no matter the choice of \(k\).

Other notions of stability related to our work include robust hierarchical clustering (Balcan et al., 2014), robust online clustering (Lattanzi et al., 2021), average sensitivity (Yoshida and Ito, 2022), and differentially private (DP) clustering (Cohen et al., 2021; Ghazi et al., 2020). The definition of replicability we use is statistical and relies on an underlying data distribution while (DP) provides a worst-case combinatorial guarantee for two runs of the algorithm on neighboring datasets. Bun et al. (2023), Kalavasis et al. (2023) provide connections between DP and replicability for statistical learning problems. However, these transformations are not computationally efficient. It would be interesting to come up with computationally efficient reductions between replicable and DP clustering algorithms.

**Coresets for Clustering.** A long line of work has focused on developing strong coresets for various flavors of centroid-based clustering problems. See Sohler and Woodruff (2018) for an overview of this rich line of work. The most relevant for our results include coresets for dynamic geometric streams through hierarchical grids (Frahling and Sohler, 2005) and sampling based techniques (Ben-David, 2007; Feldman and Langberg, 2011; Bachem et al., 2018).

**Dimensionality Reduction.** Dimensionality reduction for clustering has been a popular area of study as it reduces both the time and space complexity of existing algorithms. The line of work on data-oblivious dimensionality reduction for \(k\)-means clustering was initiated by Boutsidis et al. (2010). The goal is to approximately preserve the cost of all clustering solutions after passing the data through a dimensionality reduction map. This result was later improved and generalized to \((k,p)\)-clustering (Cohen et al., 2015; Becchetti et al., 2019), culminating in the work of Makarychevet al. (2019), whose bound on the target dimension is sharp up to a factor of \( 1/\). While Charikar and Waingarten (2022) overcome this factor, their result only preserves the cost across the optimal solution.

**Replicability in ML.** Our results extend the recently initiated line of work on designing provably replicable learning algorithms under the definition that was introduced by Impagliazzo et al. (2022). Later, Esfandiari et al. (2022) considered a natural adaption of this definition to the setting of bandits and designed replicable algorithms that have small regret. A slightly different notion of replicability in optimization was studied in Ahn et al. (2022), where it is required that an optimization algorithm that uses noisy operations during its execution, e.g., noisy gradient evaluations, outputs solutions that are close when executed twice. Subsequently, Bun et al. (2023), Kalavasis et al. (2023) established strong connections between replicability and other notions of algorithmic stability. Recently, Dixon et al. (2023), Chase et al. (2023) proposed a weaker notion of replicability where the algorithm is not required to output the same solution across two executions, but its output needs to fall into a small list of solutions.

## 2 Setting & Notation

Let \(^{d}\) be the instance space endowed with a metric \(:_{+}\) and \(\) be a distribution on \(\) which generates the i.i.d. samples that the learner observes.

For \(F^{d}\) and \(x\), we overload the notation and write \(F(x):=*{argmin}_{f F}(x,f)\) to be the closest point to \(x\) in \(F\) as well as \((x,F):=(x,F(x))\) to be the shortest distance from \(x\) to a point in \(F\).

We assume that \(\) is a subset of \(_{d}\), the \(d\)-dimensional \(\)-ball of diameter 1 centered about the origin3. We also assume that \(\) is induced by some norm \(\) on \(^{d}\) that is _sign-invariant_ (invariant to changing the sign of a coordinate) and _normalized_ (the canonical basis has unit length). Under these assumptions, the unit ball of \(\) is a subset of \([-1,1]^{d}\).

Our setting captures a large family of norms, including the \(_{p}\)-norms, Top-\(\) norms (sum of \(\) largest coordinates in absolute value), and ordered norms (non-negative linear combinations of Top-\(\) norms) (Chakrabarty and Swamy, 2019). Our results hold for more general classes of norms but for the sake of simplicity, we abide by these assumptions.

We define \(:=\{(x,y):x,y[0,1)^{d}\}\) to be the \(\)-diameter of the unit hypercube. Note that \(1 d\) by assumption. Moreover, \(L\) is the \(\)-diameter of a hypercube with side length \(L\). For example, if \(=_{2}\) is the Euclidean norm, then \(=\).

### Clustering Methods and Generalizations

We now introduce the clustering objectives that we study in this work, which all fall in the category of minimizing a cost function \(:_{+}\), where \(:=\{F_{d}:|F|=k\}\). We write \((F)\) to denote the objective in the statistical setting and \(}(F)\) for the combinatorial setting in order to distinguish the two.

**Problem 2.1** (Statistical \((k,p)\)-Clustering).: Given i.i.d. samples from a distribution \(\) on \(_{d}\), minimize \((F):=_{x}(x,F)^{p}\).

In other words, we need to partition the points into \(k\) clusters so that the expected distance of a point to the center of its cluster, measured by \((,)^{p}\), is minimized. This is closely related to the well-studied combinatorial variant of the \((k,p)\)-clustering problem.

**Problem 2.2** ((\(k,p\))-Clustering).: Given some points \(x_{1},,x_{n}\), minimize \(}(F):=_{i=1}^{n}(x_{i},F)^{p}\).

We note that (statistical) \(k\)-medians and (statistical) \(k\)-means is a special case of Problem 2.2 (Problem 2.1) with \(p=1,2,\) respectively. We also consider a slight variant of the combinatorial problem, i.e., Problem 2.2, where we allow different points \(x_{i}\) to participate with different weights \(w_{i}\) in the objective. We refer to this problem as the _weighted \((k,p)\)-_clustering problem.

We now shift our attention to the \(k\)-centers problem.

**Problem 2.3** (Statistical \(k\)-Centers).: Given i.i.d. samples from a distribution \(\) on \(_{d}\), minimize \((F):=_{x}(x,F)\).

Notice that the \(_{}\) norm is the limit of the \(_{p}\) norm as \(p\) tends to infinity, hence \(k\)-centers is, in some sense, the limit of \((k,p)\)-clustering as \(p\) tends to infinity. Also, notice that this problem differs from \(k\)-means and \(k\)-medians in the sense that it has a _min-max_ flavor, whereas the other two are concerned with minimizing some _expected_ values. Due to this difference, we need to treat \(k\)-centers separately from the other two problems, and we need to make some assumptions in order to be able to solve it from samples (cf. Assumption F.1, Assumption F.2). We elaborate more on that later.

Let us also recall the combinatorial version of \(k\)-centers.

**Problem 2.4** (\(k\)-Centers).: Given some points \(x_{1},,x_{n}\), minimize \(}(F):=_{i[n]}(x_{i},F)\).

We remark that clustering has mainly been studied from the combinatorial point of view, where the distribution is the uniform distribution over some finite points and we are provided the entire distribution. The statistical clustering setting generalizes to arbitrary distributions with only sample access. We emphasize that although we only have access to samples, our output should be a good solution for the entire distribution and not just the observed data.

We write \(F_{}\) to denote an optimal solution for the entire distribution and \(:=(F_{})\). Similarly, we write \(_{}\) to denote an optimal sample solution and \(}:=}(_{})\). Suppose we solve Problem 2.4 given a sample of size \(n\) from Problem 2.3. Then \(}\) since we are optimizing over a subset of the points.

Recall that a \(\)-_approximate solution_\(F\) is one which has \((F)\,\). Note this is with respect to the statistical version of our problems. An algorithm that outputs \(\)-approximate solutions is known as a \(\)-_approximation algorithm_. We also say that \(F\) is a \((,B)\)-_approximate solution_ if \((F)\,+B\).

#### 2.1.1 Parameters \(p\) and \(\)

Here we clarify the difference between \(p\) and \(\), which are two separate entities in the cost function \(_{x}[(x,F(x))^{p}]\). We denote by \(\) the distance metric used to measure the similarity between points. The most commonly studied and applied option is the Euclidean distance for which our algorithms are the most sample-efficient. On the other hand, \(p\) is the exponent to which we raise the distances when computing the cost of a clustering. A smaller choice of \(p\) puts less emphasis on points that are far away from centers and \(p=1\) seeks to control the average distance to the nearest center. A large choice of \(p\) puts emphasis on points that are further away from centers and as \(p\) tends to infinity, the objective is biased towards solutions minimizing the maximum distance to the nearest center. Thus we can think of \(k\)-centers as \((k,p)\)-clustering when \(p=\). As a concrete example, when \(\) is the Euclidean distance and \(p=5\), the cost function becomes \(_{x}[\|x-F(x)\|_{2}^{5}]\).

### Replicability

Throughout this work, we study replicability4 as an algorithmic property using the definition of Impagliazzo et al. (2022).

**Definition 2.5** (Replicable Algorithm; (Impagliazzo et al., 2022)).: Let \((0,1)\). A randomized algorithm \(\) is _\(\)-replicable_ if for two sequences of \(n\) i.i.d. samples \(\), \(\) generated from some distribution \(^{n}\) and a random binary string \( R()\),

\[_{,^{n}, R()}\{ (;)=(;)\} 1-,.\]

In the above definition, we treat \(\) as a randomized mapping to solutions of the clustering problem. Thus, even when \(\) is fixed, \(()\) should be thought of as random variable, whereas \((;)\) is the _realization_ of this variable given the (fixed) \(,\). We should think of \(\) as the shared randomness between the two executions. In practice, it can be implemented as a shared random seed. We underline that sharing the randomness across executions is crucial for the development of our algorithms. We also note that by doing that we _couple_ the two random variables \((),()\), whose realization depends on \(r R()\). Thus, if their realizations are equal with high probability under this coupling, it means that the distributions of \((),()\) are _statistically close_. This connection is discussed further in Kalavasis et al. (2023).

In the context of a clustering algorithm \(\), we interpret the output \((;)\) as a clustering function \(f:[k]\) which partitions the support of \(\). The definition of \(\)-replicability demands that \(f\) is the same with probability at least \(1-\) across two executions. We note that in the case of centroid-based clustering such as \(k\)-medians and \(k\)-means, the induced partition is a function of the centers and thus it is sufficient to output the exact same centers with probability \(1-\) across two executions. However, we also allow for algorithms that create partitions implicitly without computing their centers explicitly.

Our goal is to develop replicable clustering algorithms for \(k\)-medians, \(k\)-means, and \(k\)-centers, which necessitates that the centers we choose are arbitrary points within \(^{d}\) and _not_ only points among the samples. We underline that as in the case of differential privacy, it is trivial to design algorithms that satisfy the replicability property, e.g. we can let \(\) be the constant mapping. The catch is that these algorithms do not achieve any _utility_. In this work, we are interested in designing replicable clustering algorithms whose utility is competitive with their non-replicable counterparts.

## 3 Main Results

In this section, we informally state our results for replicable statistical \(k\)-medians (\((k,1)\)-clustering), \(k\)-means (\((k,2)\)-clustering), and \(k\)-centers under general distances. Unfortunately, generality comes at the cost of exponential dependency on the dimension \(d\). We also state our results for replicable statistical \(k\)-medians and \(k\)-means specifically under the Euclidean distance, which has a polynomial dependency on \(d\). Two key ingredients is the uniform convergence of \((k,p)\)-clustering costs (cf. Theorem C.9) as well as a data-oblivious dimensionality reduction technique for \((k,p)\)-clustering in the distributional setting (cf. Theorem E.10). These results may be of independent interest.

We emphasize that the Euclidean \(k\)-median and \(k\)-means are the most studied and applied flavors of clustering, thus the sample complexity for the general case and the restriction to \(p=1,2\) does not diminish the applicability of our approach.

The main bottleneck in reducing the sample complexity for general norms is the lack of a data-oblivious dimensionality reduction scheme. This bottleneck is not unique to replicability and such a scheme for general norms would be immediately useful for many distance-based problems including clustering. It may be possible to extend our results to general \((k,p)\)-clustering beyond \(p=1,2\). The main challenge is to develop an approximate triangle inequality for \(p\)-th powers of norms. Again, this limitation is not due to replicability but rather the technique of hierarchical grids. It is a limitation shared by Frahling and Sohler (2005).

Before stating our results, we reiterate that the support of our domain \(\) is a subset of the unit-diameter \(\)-ball \(_{d}\). In particular, we have that \( 1\).

**Theorem 3.1** (Informal).: _Let \(,(0,1)\). Given black-box access to a \(\)-approximation oracle for weighted \(k\)-medians, respectively weighted \(k\)-means (cf. Problem 2.2), there is a \(\)-replicable algorithm for statistical \(k\)-medians, respectively \(k\)-means (cf. Problem 2.1), such that with probability at least \(0.99\), it outputs a \((1+)\)-approximation. Moreover, the algorithm has sample complexity_

\[((} )()^{O(d)})\,.\]

When we are working in Euclidean space, we can get improved results for these problems.

**Theorem 3.2** (Informal).: _Let \((0,1)\). Suppose we are provided with black-box access to a \(\)-approximation oracle for weighted Euclidean \(k\)-medians (\(k\)-means). Then there is a \(\)-replicable algorithm that partitions the input space so with probability at least \(0.99\), the cost of the partition is at most \(O()\). Moreover, the algorithm has sample complexity_

\[((})( )^{O((}{{}}))})\,.\]

We underline that in this setting we compute an _implicit_ solution to Problem 2.1, since we do not output \(k\) centers. Instead, we output a function \(f\) that takes as input a point \(x\) and outputs the label of the cluster it belongs to in polynomial time. The replicability guarantee states that, with probability \(1-\), the function will be the same across two executions.

The combinatorial \(k\)-medians (\(k\)-means) problem where the centers are restricted to be points of the input is a well-studied problem from the perspective of polynomial-time constant-factor approximation algorithms. See Byrka et al. (2017), Ahmadian et al. (2019) for the current best polynomial-time approximation algorithms for combinatorial \(k\)-medians (\(k\)-means) in general metrics with approximation ratio 2.675 (9). Also, see Cohen-Addad et al. (2022) for a \(2.406\) (\(5.912\)) approximation algorithm for the combinatorial Euclidean \(k\)-medians (\(k\)-means).

As we alluded to before, in order to solve \(k\)-centers from samples, we need to make an additional assumption. Essentially, Assumption F.2 states that there is a \((,B)\)-approximate solution \(F\), such that, with some constant probability, e.g. \(0.99\), when we draw \(n\) samples from \(\) we will observe at least one sample from each cluster of \(F\).

**Theorem 3.3** (Informal).: _Let \(c(0,1)\). Given black-box access to a \((,)\)-approximation oracle for \(k\)-centers (cf. Problem 2.4) and under Assumption F.2, there is a \(\)-replicable algorithm for statistical \(k\)-centers (cf. Problem 2.3), that outputs a \((O(+),O(B++(++1)c))\)-approximate solution with probability at least \(0.99\). Moreover, it has sample complexity_

\[(k(}{{c}})^{3d}}{^{2}q^{2 }})\,.\]

Recall that there is a simple greedy 2-approximation for the sample \(k\)-center problem whose approximation ratio cannot be improved unless P = NP (Hochbaum and Shmoys, 1985).

We defer the discussion around \(k\)-centers to Appendix F. In particular, see Theorem F.8 in Appendix F for the formal statement of Theorem 3.3.

## 4 Overview of \((k,p)\)-Clustering

In this section, we present our approach to the \((k,p)\)-clustering problem. First, we replicably approximate the distribution with a finite set of points by extending the approach of Frahling and Sohler (2005) to the distributional setting. Then, we solve the combinatorial \((k,p)\)-clustering problem on this coreset using an approximation oracle in a black-box manner. In the following subsections, we give a more detailed overview for each step of our approach. For the full proofs and technical details, we kindly refer the reader to Appendix D.4 - D.6. In summary:

1. Replicably build a variant of a _quad tree_(Finkel and Bentley, 1974) (cf. Section 4.2).
2. Replicably produce a weighted _coreset_ using the quad tree (cf. Section 4.1).
3. Apply the optimization oracle for the combinatorial problem on the coreset.

For general norms, this approach leads to an exponential dependence on \(d\). However, we are able to handle the case of Euclidean distances by extending existing dimensionality reduction techniques for sample Euclidean \((k,p)\)-clustering (Makarychev et al., 2019) to the distributional case (cf. Section 5). Thus, for the widely used Euclidean norm, our algorithm has \((d)\) sample complexity.

### Coresets

**Definition 4.1** ((Strong) Coresets).: For a distribution \(\) with support \(_{d}\) and \((0,1)\), a _(strong) \(\)-coreset_ for \(\) is a distribution \(^{}\) on \(^{} B_{d}\) which satisfies

\[(1-)_{x}(x,F)^{p} _{x^{}^{}}[(x^{},F)^{p}] (1+)_{x}(x,F)^{p}\]

for every set of centers \(F_{d},|F|=k\).

Essentially, coresets help us approximate the true cost on the distribution \(\) by considering another distribution \(^{}\) whose support \(^{}\) can be arbitrarily smaller than the support of \(\).

Inspired by Frahling and Sohler (2005), the idea is to replicably consolidate our distribution \(\) through some mapping \(R:\) whose image has small cardinality \(|R()|<<\) so that for any set of centers \(F\),

\[(1-)_{x}(x,F)^{p}_{x}(R(x),F)^{ p}(1+)_{x}(x,F)^{p}\,,\]

where \((0,1)\) is some error parameter. In other words, \((_{R},R())\) is an \(\)-coreset. Note that given the function \(R\), we can replicably estimate the probability mass at each point in \(R()\) and then apply a weighted \((k,p)\)-clustering algorithm.

### Replicable Quad Tree

We now explain how to replicably obtain the mapping \(R:\) by building upon the work of Frahling and Sohler (2005). The pseudocode of the approach is provided in Algorithm 4.1. While Frahling and Sohler (2005) present their algorithm using hierarchical grids, we take an alternative presentation using the quad tree (Finkel and Bentley, 1974), which could be of independent interest.

First, we recall the construction of a standard quad tree in dimension \(d\). Suppose we have a set of \(n\) points in \([-}{{2}},}{{2}}]^{d}\). The quad tree is a tree whose nodes represent hypercubes containing points and can be built recursively as follows: The root represents the cell \([-}{{2}},}{{2}}]^{d}\) and contains all points. If a node contains more than one point, we split the cell it represents into \(2^{d}\) disjoint, equally sized cells. For each non-empty cell, we add it as a child of the current node and recurse into it. The recursion stops when a node contains only 1 point. In the distributional setting, the stopping criterion is either when the diameter of a node is less than some length or when the node contains less than some mass, where both quantities are a function of the depth of the node. See Algorithm 4.1.

A quad tree implicitly defines a function \(R:\) as follows. Given a point \(x\) and the root node of our tree, while the current node has a child, go to the child containing \(x\) if such a child exists, otherwise, go to any child. At a leaf node, output the center of the cell the leaf represents. Intuitively, the quad tree consolidates regions of the sample space into single points. The construction can be made replicable since the decision to continue the recursion or not is the only statistical operation and is essentially a heavy-hitters operations which can be performed in a replicable fusion.

Let \(_{i}\) denote the union of all \(2^{id}\) possible cells at the \(i\)-th level. We write \(_{i}\) to denote the discretized distribution to \(_{i}\). In other words, \(_{i}=|_{(_{i})}\) is the restriction of \(\) to the smallest \(\)-algebra containing \(_{i}\). Moreover, we write \(\) to denote a replicable estimate of \(\) with relative error \(\), say \(}{{(1+)}}\,(1+ )\,\) for some absolute constant \( 1\). We demonstrate how to obtain such a replicable estimate in Appendix D.5.

```
1:rQuadTree(distribution \(\), accuracy \(\), exponent \(p\), replicability \(\), confidence \(\)):
2:Init the node on the first level \(\{[-}{{2}},}{{2}}]^{d}\}\).
3:for depth \(i 1\); \([i-1]\) AND \((2^{-i+1})^{p}>}{{5}}\); \(i i+1\)do
4:\(\{_{i}\) is the discretized distribution over \(2^{id}\) cells on the \(i\)-th layer}
5:\(\{\) is a parameter to be determined later.}
6:\(\{t\) is an upper bound on the number of layers.}
7: Compute \(=(_{i},v=},,,)\)
8:for node \(Z[i-1]\)do
9:for heavy hitter cells \(H\) such that \(H Z\)do
10:\((Z)(Z)\{H\}\)
11:\([i][i]\{H\}\).
12:endfor
13:endfor
14:endfor
15:Output root node.
```

**Algorithm 4.1** Replicable Quad Tree

Our technique differs from that of Frahling and Sohler (2005) in at least three ways. Firstly, they performed their analysis for finite, uniform distributions with access to the entire distribution, while our results hold assuming only sample access to a general bounded distribution5. Secondly, Frahling and Sohler (2005) bound the number of layers as a function of the cardinality of the support. For us, this necessitates the extra termination condition when the side lengths of our grids fall below a fraction of \(\) as our distribution may have infinite support. Finally, Frahling and Sohler (2005) estimate \(\) by enumerating powers of 2. This suffices for their setting since their distributions are discrete and bounded. However, we require a more nuanced approach (cf. Appendix D.5) as we do not have a lower bound for \(\). We tackle this by showing uniform convergence of the clustering solution costs, which we establish via metric entropy and Rademacher complexity (cf. Appendix C).

### Putting it Together

Once we have produced the function \(R\) implicitly through a quad tree, there is still the matter of extracting a replicable solution from a finite distribution. We can accomplish this by replicably estimating the probability mass at each point of \(R()\) and solving an instance of the weighted sample \(k\)-medians (\(k\)-means). This leads to the following results. For details, see Appendix D.4 - D.6.

**Theorem 4.2** (Theorem 3.1; Formal).: _Let \(,(0,1)\) and \((0,}{{3}})\). Given black-box access to a \(\)-approximation oracle for weighted \(k\)-medians (cf. Problem 2.2), there is a \(\)-replicable algorithm for statistical \(k\)-medians (cf. Problem 2.1) such that, with probability at least \(1-\), it outputs a \((1+)\)-approximation. Moreover, it has sample complexity_

\[((d^{2}}{^{12}^{6}^{12}}+2^{18d}^{3d+3}}{^{2}^{3d+5} ^{3}}))\,.\]

**Theorem 4.3** (Theorem 3.1; Formal).: _Given black-box access to a \(\)-approximation oracle for weighted \(k\)-means (cf. Problem 2.2), there is a \(\)-replicable algorithm for statistical \(k\)-means (cf. Problem 2.1) such that, with probability at least \(1-\), it replicably outputs a \((1+)\)-approximation. Moreover, it has sample complexity_

\[((d^{2}}{^{12}^{6}^{12}}+2^{39d}^{3d+6}}{^{2}^{6d+8} ^{3}}))\,.\]

## 5 The Euclidean Metric, Dimensionality Reduction, and \((k,p)\)-Clustering

In this section, we focus on the Euclidean metric \((y,z):= y-z_{2}\) and show how the sample complexity for Euclidean \((k,p)\)-clustering can be made polynomial in the ambient dimension \(d\). Note that results for dimensionality reduction exist for the combinatorial Euclidean \((k,p)\)-clustering problem (Charikar and Waingarten, 2022, Makarychev et al., 2019). However, these do not extend trivially to the distributional case. We remark that we require our dimensionality reduction maps to be _data-oblivious_ since we are constrained by replicability requirements.

The cornerstone result in dimensionality reduction for Euclidean distance is the Johnson-Lindenstrauss lemma (cf. Theorem E.1), which states that there is a distribution over linear maps \(_{d,m}\) from \(^{d}^{m}\) that approximately preserves the norm of any \(x^{d}\) with constant probability for some target dimension \(m\). In Makarychev et al. (2019), the authors prove Theorem E.4, which roughly states that it suffices to take \(m=(}}{{^{2}}}(}{{ }}))\) in order to preserve \((k,p)\)-clustering costs when the domain is finite. Firstly, we extend Theorem E.4 to the distributional setting by implicitly approximating the distribution with a weighted \(\)-net. Then, we implicitly map the \(\)-net onto the low-dimensional space and solve the clustering problem there. An important complication we need to overcome is that this mapping preserves the costs that correspond to _partitions6_ of the data and not arbitrary solutions. Because of that, it is not clear how we can "lift" the solution from the low-dimensional space to the original space. Thus, instead of outputting \(k\) points that correspond to the centers of the clusters, our algorithm outputs a _clustering function_\(f:[k]\), which takes as input a point \(x\) and returns the label of the cluster it belongs to. The replicability guarantees of the algorithm state that, with probability \(1-\), it will output the _same_ function across two executions. In Appendix E.4, we describe this function. We emphasize that for each \(x\), the running time of \(f\) is polynomial in \(k,d,p,}{{}},(}{{}})\). Essentially, this function maps \(x\) onto the low-dimensionalspace using the same projection map \(\) as our algorithm and then finds the nearest center of \((x)\) in the low-dimensional space. For full details, we refer the reader to Appendix E.3. We are now ready to state the result formally.

**Theorem 5.1** (Theorem 3.2; Formal).: _Let \(,(0,1)\) and \((0,}{{3}})\). Given a \(\)-approximation oracle for weighted Euclidean \(k\)-medians (\(k\)-means), there is a \(\)-replicable algorithm that outputs a clustering function such that with probability at least \(1-\), the cost of the partition is at most \((1+)\). Moreover, the algorithm has sample complexity_

\[((} )(}{})^{O(m)} )\,,\]

_where \(m=O(})\)._

## 6 Running Time for \((k,p)\)-Clustering

All of our algorithms terminate in \(O((n))\) time where \(n\) denotes the sample complexity. See Table A.1 for more detailed time complexity of each stage of our approach. Moreover, we make \(O((}{{()}}))\) calls to the \(\)-approximation oracle for the combinatorial \((k,p)\)-clustering problem within our \(\) estimation subroutine and then one more call to the oracle in order to output a solution on the coreset.

## 7 Replicable \(k\)-Centers

Due to space limitations, we briefly sketch our approach for the \(k\)-centers problem and kindly refer the reader to Appendix F. As explained before, the assumptions we make in this setting state that there exists some "good" solution, so that when we draw \(n\) i.i.d. samples from \(\) we observe at least one sample from each cluster, with constant probability. We first take a fixed grid of side \(c\) in order to cover the unit-diameter ball. Then, we sample sufficiently many points from \(\). Subsequently, we "round" all the points of the sample to the centers of the cells of the grid that they fall into and estimate the probability mass of every cell. In order to ensure replicability, we take a random threshold from a predefined interval and discard the points from all the cells whose mass falls below the threshold. Finally, we call the approximation oracle using the points that remain. Unlike the \((k,p)\)-clustering problem (cf. Problem 2.1), to the best of our knowledge, there does not exist any dimensionality reduction techniques that apply to the \(k\)-centers problem. The main result is formally stated in Theorem F.8.

## 8 Experiments

We now provide a practical demonstration7 of the replicability of our approach on synthetic data in 2D. In Figure 8.1, we leverage the sklearn (Pedregosa et al., 2011) implementation of the popular \(k\)-means++ algorithm for \(k=3\) and compare the output across two executions on the two moons distribution. In the first experiment, we do not perform any preprocessing and run \(k\)-means++ as is, resulting in different centers across two executions. In the second experiment, we compute a replicable coreset for the two moons distribution before running \(k\)-means++ on the coreset. This leads to the same centers being outputted across the executions. Note that the computation for the coreset is performed independently for each execution, albeit with a shared random seed for the internal randomness. See also Figure 8.2 for the results of a similar experiment on a mixture of truncated Gaussian distributions.

## 9 Conclusion & Future Work

In this work, we designed replicable algorithms with strong performance guarantees using black-box access to approximation oracles for their combinatorial counterparts. There are many follow-up research directions that this work can lead to. For instance, our coreset algorithm adapts the coreset algorithm of Frahling and Sohler (2005) by viewing their algorithm as a series of heavy hitter estimations that can be made replicable. it may be possible to interpret more recent approaches for coreset estimation as a series of statistical operations to be made replicable in order to get replicable algorithms in the statistical \((k,p)\)-clustering setting (Hu et al., 2018). It would also be interesting to examine the sensitivity of (replicable) clustering algorithms to the choice of parameters such as the choice of exponent in the cost function or the measure of similarity of the data. Another relevant direction is to explore sample complexity lower ounds for statistical clustering, where little is known even in the non-replicable setting.