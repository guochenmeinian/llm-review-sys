# Trading-off price for data quality to achieve fair online allocation

Mathieu Molina

Inria, FairPlay Team, Palaiseau, France

mathieu.molina@inria.fr

&Nicolas Gast

Univ. Grenoble Alpes, Inria, CNRS

Grenoble INP, LIG, 38000 Grenoble, France

nicolas.gast@inria.fr

&Patrick Loiseau

Inria, FairPlay Team, Palaiseau, France

patrick.loiseau@inria.fr

&Vianney Perchet

CREST, ENSAE, Palaiseau, France

Criteo AI Lab, Paris, France

vianney.perchet@normalesup.org

###### Abstract

We consider the problem of online allocation subject to a long-term fairness penalty. Contrary to existing works, however, we do not assume that the decision-maker observes the protected attributes--which is often unrealistic in practice. Instead they can purchase data that help estimate them from sources of different quality; and hence reduce the fairness penalty at some cost. We model this problem as a multi-armed bandit problem where each arm corresponds to the choice of a data source, coupled with the online allocation problem. We propose an algorithm that jointly solves both problems and show that it has a regret bounded by \(()\). A key difficulty is that the rewards received by selecting a source are correlated by the fairness penalty, which leads to a need for randomization (despite a stochastic setting). Our algorithm takes into account contextual information available before the source selection, and can adapt to many different fairness notions. We also show that in some instances, the estimates used can be learned on the fly.

## 1 Introduction

We consider the problem of online allocation with a long-term fairness penalty: A decision maker interacts sequentially with different types of users with the global objective of maximizing some cumulative rewards (e.g., number of views, clicks, or conversions); but she also has a second objective of taking globally "fair" decisions with respect to some protected/sensitive attributes such as gender or ethnicity (the exact concepts of fairness will be discussed later). This problem is important because it models a large number of practical online allocation situations where the additional fairness constraint might be crucial. For instance, in online advertising (where the decision maker chooses to which users an ad is shown), some ads are actually positive opportunities such as job offers, and targeted advertising has been shown to be prone to discrimination Lambrecht and Tucker (2019); Speicher et al. (2018); Ali et al. (2019). Those questions also arise in many different fields such as workforce hiring Dickerson et al. (2018), recommendation systems Burke (2017), or placement in refugee settlements Ahani et al. (2021).

There has been a flourishing trend of research addressing fairness constraints in machine learning--see e.g., Chouldechova (2017); Hardt et al. (2016); Kleinberg and Raghavan (2018); Barocas et al. (2019); Emelianov et al. (2020); Molina and Loiseau (2022)--and in sequential decision-making problems--see e.g., Joseph et al. (2016); Jabbari et al. (2017); Heidari and Krause (2018)--as algorithmic decisions have real consequences on the lives of individuals, with unfortunate observed discrimination Buolamwini& Gebru, 2018; Larson et al., 2016; Dastin, 2018). In online allocation problems, general long-term constraints have been studied for instance by Agrawal & Devanur (2015) who maximize the utility of the allocation while ensuring the feasibility of the average allocation, or by Balseiro et al. (2020) who show how to handle hard budget constraints for online allocation problems. More directly related to fairness, Nasr & Tschantz (2020) formulate the problem of fair repeated auctions with a hard constraint on the difference between the number of ads shown to each group. Finally, in recent works, Balseiro et al. (2021); Celli et al. (2022) consider the online allocation problem where a non-separable penalty related to fairness is suffered by the decision maker at the end of the decision-making process instead of a hard constraint--these works are the closest to ours.

All the aforementioned papers, unfortunately, assume that the protected attributes (which define the fairness constraints) are observed before taking decisions. In practice, it is often not the case--for instance to respect users' privacy (Guardian, 2023)--and this makes it challenging to satisfy fairness constraints (Lipton et al., 2018). In online advertising for example, the decision-maker typically has access to some public "contexts" on each user, from which she could try to infer the value of the attribute; but it was shown that the amount of noise can be prohibitive and therefore ensuring that a campaign reaches a non-discriminatory audience is non-trivial Gelauff et al. (2020).

Our contribution.In this paper, we consider the online allocation problem under long-term fairness penalty, in the practical case where the protected attributes are not observed. Instead, we consider the case where the decision-maker can pay to acquire more precise information on the attributes (beyond the public context), either by directly compensating the user (the more precise the information on the attribute, the higher the price) or by buying additional data to some third parties data-broker.1 Using this extra information, she should be able to estimate more precisely, and thus sequentially reduces, the unfairness of her decisions while keeping a high cumulative net reward. The main question we aim at answering is _how should the decision maker decide when, and from which source (or at what level of precision), to buy additional data in order to make fair optimal allocations?_

Compared to the closest existing works (Balseiro et al., 2021; Celli et al., 2022) which study online allocation with a long term fairness penalty, the main novelty in the setting we examine is two-fold: we allow for uncertainty on the attributes of each individual, and more importantly we consider _jointly_ the fair online allocation problem with a source selection problem. Consequently, we present the efficient Algorithm 1 that tackles both of these challenges concurrently. This algorithm combines a dual gradient descent for the fair allocation aspect and a bandit algorithm for the source selection part. The final performance of an algorithm is its net cumulative utility (rewards minus costs of buying extra information) penalized by its long-term unfairness; that is quantified by the "regret": the difference between this performance and the one of some benchmarking "optimal" algorithm. We show that Algorithm 1 has a sub-linear regret bound under some stochastic assumptions. Notably, the performance achieved by Algorithm 1 using randomized source selection is strictly better than when using a single fixed source, because of the interaction through the fairness penalty--a key difference with standard bandit settings. On a more technical level we show how one can model the randomness and estimates for the protected attributes, how to bound the fair dual parameters which is crucial in order to use adversarial bandit techniques, and how to combine the analysis of the primal and dual steps of the algorithm.

There are many different definitions of group fairness that can be studied (e.g., demographic parity, equal opportunity, etc.). Instead of focusing on a specific one, we consider a generic formulation that can be instantiated to handle most of those different concepts (see Section 2.1 and Appendix A). We also discuss in Section 4.1 how to adapt our algorithm to different fairness penalties. This gives a higher level of generality to our results compared to existing approaches that can handle fewer fairness criteria (e.g., Celli et al. (2022)).

For the sake of clarity, we expose our key results in a simple setting. In particular, we assume binary decisions and a linear utility, and we assume that the expected utility conditional on the context is known. All these assumptions can be relaxed. In particular, we can learn the utilities (see Section 4.2). We can handle also more general decision variables and utility forms. This allows in particular to tackle problems of matching and auctions (albeit with some restrictions), we discuss that in Appendices B and C.

Related work.The problem of online fair allocation is closely related to online optimization problems with constraints, which is studied in a few papers. For instance, bandit problems with knapsack constraints where the algorithm stops once the budget has been depleted have been studied by Badanidiyuru et al. (2018); Agrawal and Devanur (2016). Li and Ye (2021); Li et al. (2022) consider online linear programs, with stochastic and adversarial inputs. Liu et al. (2021) deal with linear bandits and general anytime constraints, which can be instantiated as fairness constraints. More recently Castiglioni et al. (2022, 2022) propose online algorithms for long-term constraints in the stochastic and adversarial cases with bandit or full information feedback. Some papers take into account soft long-term constraints (Agrawal and Devanur, 2015; Jenatton et al., 2016), and more recently in (Balseiro et al., 2021; Celli et al., 2022) where the long-term constraint can be instantiated as a fairness penalty--we also adopt a soft constraint. We depart from this literature by considering the case where the protected attributes (based on which fairness is defined) is not observed. We consider the case where the decision-maker can buy additional information to estimate it (which adds considerable technical complexity), but even in the case where no additional information can be bought our work extends that of (Balseiro et al., 2021; Celli et al., 2022).

As mentioned above, the fairness literature usually assumes perfect observation of the protected attributes yet noisy realizations of protected attributes or limited access to them to measure and compute fair machine learning models has also been considered (Lamy et al., 2019; Celis et al., 2021; Zhao et al., 2022). In some cases, the noise may come from privacy requirements, and the interaction between those two notions has been studied (Jagiselski et al., 2019; Chang and Shokri, 2021), see (Fioretto et al., 2022) for a survey. There are also works on data acquisition, which is similar to purchasing information from different sources of information; e.g., Chen et al. (2018); Chen and Zheng (2019) study mechanisms to acquire data so as to estimate some of the population's statistics. They use a mechanism design approach where the cost of data is unknown and do not consider fairness (or protected attributes). However, none of these approaches can handle sequential decision problems with fairness constraints or penalties (and choosing information sources).

## 2 Preliminaries

### Model and assumptions

We present here a simpler model, and later-on discuss possible extensions. Consider a decision maker making sequential allocation decisions for a known number of \(T\) users (or simply stages) in order to maximize her cumulative rewards. The user \(t\) has some protected attributes \(a_{t}^{d}\), that is not observed before taking a decision \(x_{t}\). On the other hand, the decision-maker first observes some public context \(z_{t}\), where \(\) is the finite set of all possible public contexts and she has the possibility to buy additional information. There are \(K\) different sources for additional information and choosing the source \(k_{t}[K]\) has a cost of \(p_{k_{t}}\), but it provides a new piece of information, which together with the public information is summarized in the random variable \(c_{tk_{t}}=(z_{t},k_{t})\). Based on this, the decision \(x_{t}\{0,1\}\) can be made; this corresponds to include, or not, user \(t\) in the cohort (for instance, to display an ad or not). Including user \(t\) generates some reward/utility \(u_{t}\), which might be unknown to the decision maker (as it may depend on the private attribute), but can be estimated using the different contexts.

To fix ideas, we show how this model applies to two examples: Imagine an advertiser aiming to display ads to an user, able to see some bare-bone information through cookies, such as which website was previously visited (\(z_{t}\)). Based on this information, they can decide whether or not to buy additional information (\(c_{tk}\)) from different data brokers that collect user activity. For example, if they observe that the user has browsed clothing stores, they might opt to acquire data containing purchase details from this website. This enables them to estimate the user's gender (\(a_{t}\)) based on the type of clothing bought. Now consider the problem of fairly relocating refugees to different cities. When the organization in charge of resettlement receives a resettlement case (\(z_{t}\)), it can either decide to directly assign the refugee to a specific city, or to conduct an additional costly investigation (which might involve a third party watch-dog) to get more information (\(c_{tk}\)) on some protected attributes of interest such as wealth or age (\(a_{t}\)), which might have been intentionally misreported.

We assume that the global objective is to maximize the sum of three terms: the cumulative rewards of all selected users, minus the costs of the additional information bought, minus some unfairness penalty, represented by some function \(R(.)\). Denoting by \(=(k_{1},,k_{T})\) and \(=(x_{1},,x_{T})\)the sources and allocations selected during the \(T\) rounds, the total utility of the decision maker is then

\[(,)=_{t=1}^{T}u_{t}x_{t}-_{t=1}^{T}p_{k_{t}}-TR _{t=1}^{T}a_{t}x_{t}.\] (1)

The penalty function \(R(.)\) is a convex penalty function that measures the fairness cost of the decision-making process, due to the unbalancedness of the selected users at the end of the \(T\) rounds. It can be used to represent statistical parity (Kamishima et al., 2011), as a measure of how far the allocation is from this fairness notion. This fairness penalty \(R(.)\) is also used in Balseiro et al. (2021); Celli et al. (2022). In fact, the objective (1) is equal the one used in these papers minus the cost of additional information bought, \(_{t}p_{k_{t}}\).

Knowns, unknowns and stochasticityWe assume that users are independent and identically distributed (i.i.d.), in the sense that the whole vectors \((z_{t},u_{t},a_{t},c_{t1} c_{tK})\) are i.i.d., drawn from some underlying unknown probability distribution. While this may be a strong assumption, some applications such as online advertising correspond to large \(T\) but to a short real time-frame, hence incurring very little variation in the underlying distribution. The prices \(p_{k}\) and the penalty function \(R(.)\) are known beforehand. As mentioned several times, the only feedback received is \(c_{tk}\), after selecting source \(k\), and this should be enough to estimate \(u_{t}\) and \(a_{t}\). We therefore assume that the conditional expectations \([u_{t} c_{tk_{t}}]\) and \([a_{t} c_{tk_{t}}]\) are known. The rationale behind this assumption is that these conditional expectations have been learned from past data.

Penalty examples and generalizationsA typical example for \(a_{t}\) is the case of one-hot encoding: there are \(d\) protected categories of users and \(a_{t}\) indicates the category of user \(t\). For simplicity, assume that \(d=2\), then the quantity \(_{t}a_{ti}x_{t}\) is the number of users of type \(i\{1,2\}\) that have been selected. The choice of \(TR(_{t=1}^{T}a_{t}x_{t}/T)=|_{t}a_{t1}x_{t}-_{t}a_{t2}x_{t}|\) amounts to penalizing the decision maker proportional to the absolute difference of users in both groups. This generic setting can also model other notions of fairness, such as Equality of Opportunity (Hardt et al., 2016), by choosing other values for \(a_{t}\) and \(R(.)\), see examples and discussion in Appendix A.

Similarly, the choice of \(x_{t}\{0,1\}\) can be immediately generalized to any finite decision set or even continuous compact one (say, the reward at stage \(t\) would then be \(_{t}^{}x_{t}\) for some vector \(_{t}\)), which makes it possible to handle problems such as bipartite matching. Instead of deriving a linear utility from selecting an user, general bounded upper semi-continuous (u.s.c.) utility functions can also be treated, and can be used to instantiate auctions mechanism (with some limitations detailed in Appendix C). We also explain how to relax the assumption that \([u_{t} c_{tk_{t}}]\) is known in Section 4.2, by deriving an algorithm that actually learns it in an online fashion, following linear contextual bandit techniques (Abbasi-yadkori et al., 2011).

We show in Section 3.5 that the assumption that \(\) is finite can be relaxed if all conditional distributions depend smoothly on \(z\), following techniques from Perchet and Rigollet (2013).

Mathematical assumptionsWe shall assume \(|u_{t}|\), for all \(t\), and that \(\|a\|_{2} 1\) for all \(a\). We make, for now, no structural assumption on the variables \(c_{tk}\). We mention here that the decision maker has to choose a single source at each stage, but this is obviously without loss of generality (by adding void or combination of sources).

We define \(=(\{0\})\), where \(\) is the closed convex hull of a set. Since \(\) is compact, the set \(\) is also convex and compact. The penalty function \(R:\) is a proper closed convex function that is \(L\)-Lipschitz continuous for the Euclidean norm \(\|\|_{2}\). While the convexity assumption is pretty usual, the Lipschitzness assumption is rather mild as \(\) is convex and compact. Nevertheless, interesting non-Lipschitz functions, such as the Kullback-Leibler divergence, can be modified to respect these assumptions (see Celli et al. (2022)).

### Benchmark and regret

A usual measure of performance for online algorithms is the regret that compares the utility obtained by an online allocation to the one obtained by an oracle that knows all parameters of the problem, yet not the realized sequence of private attributes. We denote the performance of this oracle by OPT:

\[=_{([K]^{2})^{T}}[_{\{0, 1\}^{T}}[(,) c_{1k_{1}},,c_{Tk_ {T}}]],\] (2)where the conditional expectation indicates that the oracle first chooses a contextual policy \(h_{t}\) for all users that specifies which source \(k_{t}=h_{t}(z_{t})\) to select as a function of the variables \(z_{t}\). It then observes all contexts \(c_{tk_{t}}\) and makes for all \(t\) the decisions \(x_{t}\) based on that.

Denoting \(\) the expected penalized utility of an online algorithm, its expected regret is:

\[=-[(,)]=- \,.\] (3)

We remark that the benchmark of Equation (2) allows choosing different sources of information for different users with the same public information \(z_{t}\). As such, it differs from classical benchmarks in the contextual bandit literature that compare the performance of an algorithm to the best static choice of arm per context and whose performance would be

\[:=_{h[K]^{}}[_{ \{0,1\}^{T}}[(,)|c_{1k_{1}},,c_{Tk_{T}}]],\] (4)

where the \(h[K]^{}\) policy that maps the public information to a source selection is the same for all users.

The benchmark (4) is the typical benchmark in contextual multi-armed bandits, as the global impact of decisions at different epochs and for different contexts are independent. However, this is no longer the case with the unfairness penalty \(R(.)\) that requires coupling all decisions:

**Proposition 2.1**.: _There exist an instance of the problem and a constant \(b>0\) such that for all \(T\):_

\[+bT<\,.\]

This result shows that an algorithm that only tries to identify the best source will have a linear regret compared to OPT. This indicates that the problem of source selection and fairness are strongly coupled, even without public information available, and cannot be solved through some sort of two-phase algorithm where each problem is solved separately. The proof of this result is presented in Appendix F.1. In this paper, our primary emphasis lies in the examination of the performance disparity between an online algorithm and the offline optimum. Nevertheless, we provide supplementary experiments in Appendix F.3 that investigate how variations in the prices \(p_{k}\) and the penalty \(R\) impact the solution of the offline optimum.

## 3 Algorithm and Regret Bounds

In this part, we present our online allocation algorithm and its regret bound. For clarity of exposition, we first present the algorithm in the case \(||=1\), i.e., without public information available (and thus we remove \(z_{t}\) from the algorithm). The extension to \(||>1\) uses similar arguments and is discussed in Section 3.5.

### Overview of the algorithm

Algorithm 1 devised to solve this problem is composed of two parts: a bandit algorithm for the source selection, and a gradient descent to adjust the penalty regularization term. This requires a dual parameter \(_{t}^{d}\) that is used to perform the source selection as well as the allocation decision \(x_{t}\).

The intuition is the following: the performance of each source is evaluated through some "dual value" for a given dual parameter \(\). The optimal primal performance is equal to the dual value when it is minimized in \(\), because \(R\) is convex and randomized combinations of sources is allowed thus there is no duality gap. Hence the dual value of each source is iteratively evaluated in order to select the best source, and simultaneously minimize the dual value of the selected source through \(\), so that the source selected is indeed optimal.

Bandit part.For the source selection, we use the EXP3 algorithm (see Chapter 11 of Lattimore & Szepesvari (2020)) on a virtual reward that depends on the dual parameter. Given a dual parameter \(^{d}\) and a context \(c_{tk}\), we define the virtual reward as

\[(_{t},c_{tk},k)=([u_{t} c_{tk}]- _{t},[a_{t} c_{tk}],0)-p_{k},\] (5)

where \(_{t},[a_{t} c_{tk}]\) denotes the scalar product between \(_{t}\) and \([a_{t} c_{tk}]\). To compute this expectation, one needs to know the quantities \(p_{k}\), \([u_{t}|c_{tk}]\) and \([a_{t}|c_{tk}]\).

To apply EXP3, a key property is to ensure that the virtual rewards are bounded, which requires \(_{t}\) to remain bounded. As we show in Lemma 3.2, this is actually guaranteed by the design of the gradient descent on \(\). This lemma implies that there exists \(m^{+}\) such that \(|(_{t},c_{tk})| m\) for all \(t\) and \(k\). Let us denote by \(_{tk}\) the probability that source \(k\) is chosen at time \(t\) and \(k_{t}_{t}\). We define the importance-weighted unbiased estimator vector \((,c_{t})^{K}\) where each coordinate \(k^{}[K]\) is:

\[(,c_{tk},k,k^{})=m-[k^{}=k],k)}{_{tk}}.\]

Using this unbiased estimator, we can apply the EXP\(3\) algorithm to this virtual reward function.

Gradient descent part.Once the source \(k_{t}\) for user \(t\) is chosen and \(c_{tk_{t}}\) is observed, we can compute the decision \(x_{t}\) (see (6)). This \(x_{t}\) is then used in (8)-(9) to perform a dual descent step on the multiplier \(_{t}\). Although using a dual descent step is classical, our implementation is different because we need to guarantee that the values of \(_{t}\) remain bounded for EXP3. To do so, we modify the geometry of the convex optimization sub-problem by considering a set of allocation targets larger than the original \(\). For \(\), we define the set \(_{}\) as the ball of center \(\) and radius \(()\). This ball contains \(\): \(_{}\).

Algorithm 1 uses any extension of \(R\) to \(=_{}_{}\) that is convex and Lipschitz-continuous, for instance, the following one (see Lemma D.2):

\[()=_{^{}}\{R(^{})+L\|- ^{}\|_{2}\},\]

that has the same Lipschitz-constant \(L\) as \(R\) (which is the best Lipschitz-constant possible).

### Algorithm and implementation

Combining these different ideas leads to Algorithm 1. This algorithm maintains a dual parameter \(_{t}\) that encodes the history of unfairness that ensures that the fairness penalty \(R(.)\) is taken into account. This dual parameter \(_{t}\) is used in (6) to compute the allocation \(x_{t}\) and in (7) to choose the source of information. The dual update (8)-(9) guarantees that we take \(R(.)\) into account.

``` Input: Initial dual parameter \(_{0}\), step sizes \(\) and \(\), cumulative estimated rewards \(S_{0}=0^{K}\). for\(t[T]\)do  Draw a source \(k_{t}_{t}\) where \(_{tk}=( S_{t-1,k})/_{l=1}^{K}( S_{t-1,l})\), and observe \(c_{tk_{t}}\).  Compute the allocation for user \(t\): \[x_{t}=\{1&[u_{t} c_{tk_{t}}] _{t},[a_{t} c_{tk_{t}}]\\ 0&.\]  Update the estimated rewards sum and sources distributions for all \(k[K]\): \[S_{tk}=S_{(t-1)k}+(_{t},c_{tk},k_{t},k),\] (7) Let \(_{t}=x_{t}[a_{t} c_{tk_{t}}]\). Compute the dual fairness allocation target and update the dual parameter \[_{t} =*{arg\,max}_{_{_{t}}}\{ _{t},-()\},\] (8) \[_{t+1} =_{t}-(_{t}-_{t}).\]  endfor ```

**Algorithm 1** Online Fair Allocation with Source Selection

An interesting property of the dual gradient descent is that it manages to provide a good fair allocation while updating the source selection parameters simultaneously. Indeed if \(\) were fixed, then we could solve the source selection part through \(\) and a bandit algorithm. However, both the \(_{t}\) and the \(_{t}\) change over time, which may hint at the necessity of a two-phased algorithm as the combination of these two problems generates non-stationarity for both the dual update and also for the bandit problem. The dual gradient descent manages to combine both updates in a single-phased algorithm.

The different assumptions imply that the decision maker has access to the \([a_{t}|c_{tk}]\) and to \([u_{t} c_{tk}]\) for any possible context value \(c_{tk}\). Such values could be indeed estimated from offline data. The knowledge of such values is sufficient to compute the allocation \(x_{t}\) in (6), the virtual value estimation \(\) of (7), or to compute \(_{t}\). Once these values are computed, the only difficulty is to solve (8). In some cases, it might be solved analytically. Otherwise, it can also be solved numerically as it is an (a priori low-dimensional) convex optimization problem. Overall this is an efficient online algorithm which only uses the current algorithm parameters \(_{t},_{t}\), and current context \(c_{tk_{t}}\).

### Regret bound

We emphasize that Algorithm 1 uses randomization among the different sources of information and does not aim to identify the best source. As shown in Proposition 2.1, this is important because using the best source of information can be strictly less good that using randomization. Moreover, it simplifies the analysis because it convexifies the set of strategies. This means that Sion's minimax theorem can be applied to some dual function, which allows for \(_{t}\) and \(_{t}\) to be updated simultaneously. If one would try to target the best static source of information (static-OPT), one would need to determine the optimal dual parameter \(_{k}\) of each source. This would lead to an algorithm that is both more complicated and less efficient (because of Proposition 2.1).

The following theorem shows that Algorithm 1 has a sub-linear regret of order \(O()\). This regret bound is comparable to those in Balseiro et al. (2021); Celli et al. (2022) but we handle the much more challenging case of having multiple sources of information, and imperfect information about \(a_{t}\) and \(u_{t}\).

**Theorem 3.1**.: _Assume that Algorithm 1 is run with the parameters \(=L/(2())\), \(m=+L+_{k} p_{k}+2()\), \(=)}\), and that \(_{0} R(0)\), the subgradient of \(R\) at \(0\). Then the expected regret of the algorithm is upper bounded by:_

\[ 2((L++_{k} p_{k})+L +L())+2L.\]

Note that this regret bound is tight: when \(R=0\) the problem we consider reduces to a \(K\)-armed bandit with bounded rewards \([([u_{t} c_{t,k}],0)-p_{k}]\) for arm \(k[K]\), which has a \(()\) regret lower bound (Lattimore and Szepesvari, 2020). It is of the same order in \(T\) as our regret upper bound. Remark that \(R=0\) implies that \(L=0\) and we do recover the regret bound for the EXP\(3\) algorithm. Similarly if \(K=1\) the bandit regret contribution disappears. In our analysis, the regret due to the interaction between the bandit and the online fair allocation is \(L\).

The time-horizon dependent parameters used in Algorithm 1 can be adapted to obtain an anytime algorithm. While using a doubling trick directly for \(\) is not possible as some protected attribute would already be selected when restarting the algorithm, we can use an adaptive learning rate of \(_{t}=(1/)\). Indeed due to the boundedness of the \((_{t})_{t T}\) (Lemma 3.2), we can act as if we had a finite diameter for the space of the \(_{t}\). This results in a slight increase of regret, the constant term in Theorem 3.1 now scaling in \(()\).

### Sketch of proof

As mentioned above, when \(K=1\) (resp. \(R=0\)) the problem reduces to fair online allocation as in Balseiro et al. (2021); Celli et al. (2022) (resp. to multi-armed bandits). The main technical difficulty thus lie in combining algorithms used in these problems. Ideally, we would have access to some optimal dual parameter \(_{k}^{*}\)_before_ we run the algorithm so that we can simply run a bandit algorithm, which is obviously not possible as the selected source \(k_{t}\) affects the fairness, and the selected parameter \(_{t}\) affects the arms virtual rewards. In particular it is not clear how the \(_{t}\) evolve in the worst case while the algorithm is running. Instead we alternate between those primal and dual updates. We thus need to show that this alternation indeed achieves good performance.

We present two important lemmas used in the proof of Theorem 3.1. The first one guarantees that doing a gradient descent with \(\) implies that the dual values \(_{t}\) remain bounded. This is crucial for EXP3 as it implies that the virtual values \(\) remain bounded along the path of the \(_{t}\).

**Lemma 3.2**.: _Let \(_{0}^{d}\), \(>0\), and an arbitrary sequences \((_{1},_{2},)^{}\). Assume that \(_{0} R(0)\) and define recursively \(_{t}\) by Equations (8) and (9). Then for all \(t\), we have \(_{t}_{2} L+2()\)._Sketch of Proof.: The main idea is to show that the distance of \(_{t}\) to the reunion of subgradients of \(\) (which is bounded by Lipschitzness property) is a decreasing function in \(t\). We can show this by using the \(KKT\) conditions of the optimization problem. The convex set over which we optimize is a simple Euclidean ball, because of our modification, centered around the appropriate point hence allowing us to redirect the gradient \(_{t+1}-_{t}\) towards this set. Moreover, we add some "security" around this set of the size of the gradient bound to make sure that the \(_{t}\) remains in this set. The full proof can be found in Appendix D.3. 

The second lemma guarantees that having access to the conditional expectation \([a_{t}|c_{tk}]\) is enough to derive a good algorithm when considering the conditional expectation of the total utility. This way we avoid the computation of the conditional expectation of \(R\), which would be more difficult.

**Lemma 3.3**.: _For \((x_{1},,x_{T})\) and \((k_{1},,k_{T})\) generated according to Algorithm 1, with \(_{t}=x_{t}[a_{t} c_{tk_{t}}]\), we have the following upper bound:_

\[[R(_{t=1}^{T}a_{t}x_{t})-R(_{ t=1}^{T}_{t})] 2L}.\]

Sketch of proof.: We use the Lipschitz property of \(R\) and some inequalities to directly compare the difference of the sums. The variable \(x_{t}\) depends on the past history and needs to be carefully taken into account, through proper conditioning. Finally, we compute the variance of a sum of martingale differences. See proof in Appendix D.4. 

Using these two Lemmas, we now give the main ideas of the proof of Theorem 3.1. First, we upper-bound \(\) through a dual function involving the convex conjugate \(R^{*}\), with similar arguments as in Balseiro et al. (2021); Celli et al. (2022). Then we need to lower-bound \(\) with this dual function minus the regret. The main difficulty is that the source virtual rewards distribution changes with \(_{t}\), and so does the average \(_{t}\) target through \(_{t}\). The performance of \(\) can be decomposed at each step \(t\) into the sum of the virtual reward and a term in \(R^{*}(_{t})\) encoding the fairness penalty. We deal with the virtual rewards using an adversarial bandits algorithm able to handle any reward sequence using techniques for adversarial bandits algorithm from Hazan (2022); Lattimore & Szepesvari (2020), as the \(_{t}\) are generated by a quite complicated Markov-Chain. These rewards are bounded because of Lemma 3.2. This yields the two regret terms in \(\), the second one stems from the difference between \(\) and \(\). For the fairness penalty, using the online gradient descent on the \(_{t}\) we end up being close to the penalty of the conditional expectations up to the regret term in \(()\). This last term is close to the true penalty through Lemma 3.3. This provides us with a computable regret bound where all the parameters are known beforehand. The full proof can be found in Appendix E.

### Public contexts

We now go back to the general case with \(||>1\) finite. We would like to derive a good algorithm, which also takes into account the public information \(z_{t}\). Reusing the analogy with bandit problems, this seems to be akin to the contextual bandit problem, where we would simply run the algorithm for each context in parallel. However, this would be incorrect: not only for a fixed public context does the non-separability of \(R\) couples the source selection with the fairness penalty, it also couples all of the public contexts \(z_{t}\) together. Hence the optimal policy is not to take the best policy for each context, which is once again different from the classical bandit setting.

The solution is to run an anytime version of the EXP3 algorithm for each public context in \(\), but to keep a common \(_{t}\) for the evaluation of the virtual value. While it is technically not much more difficult and uses similar ideas to what was done for different sources when \(||=1\), the fact that only one of the "block" of the algorithm (the bandit part) needs to be parallelized, is quite specific and surprisingly simple.

**Proposition 3.4**.: _For \(\) the probability distribution over \(\) finite, we can derive an algorithm that has a modified regret of order \((_{z})\), where \((z)\) is the probability that the public attribute is \(z\)._The algorithm and its analysis are provided in Appendix J.1. Note that in the worst case, when \(\) is finite, one has \(_{z}|}\) by Jensen's inequality on the square root function.

If \(\) is not finite but a bounded subset of a vector space set of dimension \(r\), such as \(=^{r}\), additional assumptions on the smoothness of the conditional expectations are sufficient to obtain a sub-linear regret algorithm:

**Proposition 3.5**.: _If the conditional expectations of \(a_{t}\) and \(u_{t}\) are both Lipschitz in \(z\), then discretizing the space \(=^{r}\) through an \(\)-cover and applying the previous algorithm considering that one public context corresponds to one of the discretized bins, we can obtain a regret bound of order \((T^{(r+1)/(r+2)})\)._

The proof of this result uses standard discretization arguments under Lipschitz assumptions from Perchet and Rigollet (2013), which can also be found in Chapter 8.2 of Slivkins (2019) or in Exercise 19.5 of Lattimore and Szepesvari (2020). Speciticities for this problem, such as these assumptions being enough to guarantee that \(\) is Lipschitz in \(z\), can be found in Appendix J.2.

## 4 Extensions

### Other types of fairness penalty

The fairness penalty term of Equation (1) is quantified as \(TR( a_{t}x_{t}/T)\). While this term is the same as the one used in Celli et al. (2022) and can encode various fairness definitions (see the discussion in the aforementioned paper), this does not encompass all possible fairness notions. For instance, one may want to express fairness as a function of \( a_{t}x_{t}/_{t}x_{t}\), which is the conditional empirical distribution of the user's protected attributes given that they were selected. This would lead to replacing the original penalty term by \((_{t}x_{t})R( a_{t}x_{t}/_{t}x_{t})\).

As pointed out in Celli et al. (2022), one possible issue is that \(R( a_{t}x_{t}/_{t}x_{t})\), in general, is not convex in \(x\), even if \(R\) is convex. However \((_{t}x_{t})R( a_{t}x_{t}/_{t}x_{t})\) is the perspective function of \(R(A(.))\) (with \(A\) the matrix with columns the \(a_{t}\)), which is thus convex. Hence, Algorithm 1 can be adapted to handle this new fairness penalty with two modifications. First, for the bandit part, we run the algorithm with a new virtual reward function, expressed as:

\[(_{t},c_{tk},k)=([u_{t} c_{tk}]- _{t},[a_{t} c_{tk}]+R^{*}(_{t}),0) -p_{k}.\]

This leads to an allocation \(x_{t}=1\) if \([u_{t} c_{tk_{t}}]+R^{*}(_{t})_{t}, [a_{t} c_{tk_{t}}]\) and \(x_{t}=0\) otherwise.

Second, we modify the set on which the dual descent is done. The set \(\) now becomes \(=()\) (without the union with \(0\)), and we now use \(_{t}=[a_{t} c_{tk_{t}}]\) instead of \(_{t}=x_{t}[a_{t} c_{tk_{t}}]\). Line (8) remains unchanged up to replacing \(\) and \(\) by \(\) and \(\). Finally the dual parameter update now becomes \(_{t+1}=_{t}- x_{t}(_{t}-_{t})\), which means that whenever \(x_{t}=0\), \(_{t}\) does not change.

With these modifications, the following theorem (whose proof is very similar to the one of Theorem 3.1 and detailed in Appendix G), shows that we recover similar regret bounds as previously, with some modified constants due to the presence of \(R^{*}\) in the virtual reward, and the modified \(\). This yields a new class of usable fairness penalties which was previously not known to work.

**Theorem 4.1**.: _Using Algorithm 1 with the modifications detailed above, the regret with respect to the objective with the modified penalty \((_{t}x_{t})R(_{t}a_{t}x_{t}/_{t}x_{t})\) is of order \(()\)._

### Learning conditional utilities \([u_{t} c_{tk}]\)

To compute the virtual values, the algorithm relies on the knowledge of \([u_{t} c_{tk}]\) for all possible context values. We now show how to relax this assumption even if the decision maker receives the feedback \(u_{t}\) only when the user is selected (\(x_{t}=1\)). We shall make the usual structural assumption of the classical stochastic linear bandit model. The analysis relies on Chapters \(19\) and \(20\) of Lattimore and Szepesvari (2020), and only the main ideas are given here. For simplicity we will assume that there are no public contexts (\(||=1\)).

We assume that the contexts \(c_{tk}\) are now feature vectors of dimension \(q_{k}\), and that for all \(k[K]\), there exists some vector \(_{k}^{q_{k}}\) so that

\[u_{t}-_{k},c_{tk},\] (10)is a zero mean \(1\)-subgaussian random variable conditioned on the previous observation (see a more precise definition in Appendix H). We make classical boundedness assumptions, that for all \(k\): \(\|_{k}\|_{2}\), that \(\|c_{tk}\|_{2}\), and that \(_{k},c_{tk} 1\) for all possible values of \(c_{tk}\).

Intuitively, we aim at running a stochastic linear bandit algorithm on each of the different sources for \(T_{k}\) steps, where \(T_{k}\) is the number of times that source \(k\) is selected, but this breaks because of dependencies among the \(k\) sources. Hence, what we do is to slightly change the rewards and contextual actions, so that we do something akin to artificially running \(K\) bandits in parallel for \(T\) steps. We force a reward and action of \(0\) for source \(k\) whenever it is not selected, and consider that each source is run for \(T\) steps (even if it is actually selected less than \(T\) times). Thus we can directly leverage and modify the existing analysis for the stochastic linear bandit. The full algorithm is given in Appendix H.

**Proposition 4.2**.: _Given the above assumptions, the added regret of having to learn \([u_{t} c_{tk}]\) is of order \(((T))\)._

If the decision maker knows the optimal combination of sources, she can simply select sources proportionally to this combination, and actually learn the conditional expectation independently for each source. The worst case is to have to pull each arm \(T/K\) times, which then incurs an additional regret with the same \(\) constant. This shows that this bound is actually not too wasteful, as the cost of artificially running these bandits algorithm in parallel only impacts the logarithmic term.

## 5 Conclusion

We have shown how the problem of optimal data purchase for fair allocations can be tackled, using techniques from online convex optimization and bandits algorithms. We proposed a computationally efficient algorithm yielding a \(()\) regret algorithm in the most general case. Interestingly, because of the non separable penalty \(R\), the benchmark is different from a bandit algorithm, as randomization can strictly improve the performance for some instances, even though the setting is stochastic. We have also presented different types of fairness penalties that we can additionally tackle compared to previous works, in particular in the full information setting, and some instances where assumptions on the decision maker's knowledge can be relaxed.

Throughout the paper (even in Section 4.2 where we relax the assumption that \([u_{t} c_{tk}]\) is known), we assumed that the decision-maker knows \([a_{t} c_{tk}]\). This assumption is reasonable as this is related to demographic data and not to a utility that may be specific to the decision maker. Yet, this assumption can also be relaxed in specific cases. As an example, suppose that the decision maker can pay different prices to receive more or less noisy versions of \(a_{t}\) that correspond to the data sources (the pricing could be related to different levels of Local Differential Privacy for the users, see Dwork & Roth (2014)). Then, under some assumptions, \([a_{t} c_{tk}]\) can also be learned online--we defer the details to Appendix I.

The works of Balseiro et al. (2021) and Celli et al. (2022) can include hard constraints on some budget consumption when making the allocation decision \(x_{t}\), which we did not include in our work. If this budget cost is measurable with respect to \(c_{tk}\), then our analysis does not preclude using similar stopping time arguments as was done in these two works. However if this budget consumption is completely random, our algorithm and analysis can not be directly applied as this budget consumption may give additional information on the \(a_{t}\) that was just observed. Regarding the i.i.d. assumption, in Balseiro et al. (2021) adversarial inputs are also considered, and the same could be done here for the contexts \(c_{t,k}\) with similar results. However some stochasticity is still needed so that \([u_{t} c_{t,k}]\) is well defined, which leads to an ambiguous stochastic-adversarial model. An open question would be to consider an intermediate case where \([u_{t} c_{t,k}]\) would depend on \(t\), and take into account learning for non-stationary distributions.