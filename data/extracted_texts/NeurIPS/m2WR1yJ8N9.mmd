# Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks

Jiarong Xu\({}^{1}\)

Renhong Huang\({}^{2}\)

Xin Jiang\({}^{3}\)

Yuxuan Cao\({}^{2}\)

**Carl Yang\({}^{4}\)**

Chunping Wang\({}^{5}\)

Yang Yang\({}^{2}\)

\({}^{1}\)Fudan University

\({}^{2}\)Zhejiang University

\({}^{3}\)Lehigh University

\({}^{4}\)Emory University

\({}^{5}\)Finvolution Group

jiarongxu@fudan.edu.cn, {renh2, caoyx, yangya}@zju.edu.cn

xjiang@lehigh.edu, j.carlyang@emory.edu, wangchunping02@xinye.com

Corresponding author.

###### Abstract

Pre-training on graph neural networks (GNNs) aims to learn transferable knowledge for downstream tasks with unlabeled data, and it has recently become an active research area. The success of graph pre-training models is often attributed to the massive amount of input data. In this paper, however, we identify the _curse of big data_ phenomenon in graph pre-training: more training data do not necessarily lead to better downstream performance. Motivated by this observation, we propose a _better-with-less_ framework for graph pre-training: fewer, but carefully chosen data are fed into a GNN model to enhance pre-training. The proposed pre-training pipeline is called the data-active graph pre-training (APT) framework, and is composed of a graph selector and a pre-training model. The graph selector chooses the most representative and instructive data points based on the inherent properties of graphs as well as _predictive uncertainty_. The proposed predictive uncertainty, as feedback from the pre-training model, measures the confidence level of the model in the data. When fed with the chosen data, on the other hand, the pre-training model grasps an initial understanding of the new, unseen data, and at the same time attempts to remember the knowledge learned from previous data. Therefore, the integration and interaction between these two components form a unified framework (APT), in which graph pre-training is performed in a progressive and iterative way. Experiment results show that the proposed APT is able to obtain an efficient pre-training model with fewer training data and better downstream performance.

## 1 Introduction

Pre-training Graph neural networks (GNNs) shows great potential to be an attractive and competitive strategy for learning from graph data without costly labels . Recent advancements have been made in developing various graph pre-training strategies, which aim to capture transferable patterns from a diverse set of unlabeled graph data . Very often, the success of a graph pre-training model is attributed to the massive amount of unlabeled training data, a well-established consensus for pre-training in computer vision  and natural language processing .

In view of this, contemporary research almost has no controversy on the following issue: _Is a massive amount of input data really necessary, or even beneficial, for pre-training GNNs?_ Yet, two simple experiments regarding the number of training samples (and graph datasets) seem to doubt the positiveanswer to this question. We observe that scaling pre-training samples does not result in a one-model-fits-all increase in downstream performance (top row in Figure 1), and that adding input graphs (while fixing sample size) does not improve and sometimes even deteriorates the generalization of the pre-trained model (bottom row in Figure 1). Moreover, even if the number of input graphs (the horizontal coordinate) is fixed, the performance of the model pre-trained on different combinations of inputs varies dramatically; see the standard deviation in blue in Figure 1. As the first contribution of this work, we identify the _curse of big data_ phenomenon in graph pre-training: more training samples (and graph datasets) do not necessarily lead to better downstream performance.

Therefore, instead of training on a massive amount of data, it is more appealing to choose a few suitable samples and graphs for pre-training. However, without the knowledge of downstream tasks, it is difficult to design new data selection criteria for the pre-training model. To this end, we propose the _graph selector_ which is able to provide the most instructive data for the model by incorporating two criteria: _predictive uncertainty_ and _graph properties_. On one hand, predictive uncertainty is introduced to measure the level of confidence (or certainty) in the data. On the other hand, some graphs are inheritantly more informative and representative than others, and thus the fundamental properties should help in the selection process.

Apart from the graph selector, the pre-training model is also designed to co-evolve with the data. Instead of swallowing data as a whole, the pre-training model is encouraged to learn from the data in a progressive way. After learning a certain amount of training data, the model receives feedback (from predictive uncertainty) on what kind of data the model has least knowledge of. Then the pre-training model is able to reinforce itself on highly uncertain data in next training iterations.

Putting together, we integrate the graph selector and the pre-training model into a unified paradigm and propose a data-active graph pre-training (APT) framework. The term "data-active " is used to emphasize the co-evolution of data and model, rather than mere data selection before model training. The two components in the framework actively cooperate with each other. The graph selector recognizes the most instructive data for the model; equipped with this intelligent selector, the pre-training model is well-trained and in turn provides better guidance for the graph selector.

The rest of the paper is organized as follows. In SS2 we present the basic graph pre-training framework and review existing work on this topic. Then in SS3 we describe in detail the proposed data-active graph pre-training (APT) paradigm. SS4 contains numerical experiments, demonstrating the superiority of APT in different downstream tasks, and also includes the applicable scope of our pre-trained model.

Figure 1: _Top row_: The effect of scaling up sample size (log scale) on the downstream performance based on a group of GCCs  under different configurations (the graphs used for pre-training are kept as all eleven pre-training data in Table 3, and the samples are taken from the backbone pre-training model according to its sampling strategy). The results for different downstream graphs (and tasks) are presented in separate figures. To better show the changing trend, we fit a curve to the best performing models (_i.e._, the convex hull fit as  does). _Bottom row_: The effect of scaling up the number of graph datasets on the downstream performance based on GCC. For a fixed horizontal coordinate, we run 5 trials. For each trial, we randomly choose a combination of input graphs. The shaded area indicates the standard deviation over the 5 trials. See Appendix E for more observations on other graph pre-training models and detailed settings.

Basic Graph Pre-training Framework

This section reviews the basic framework of graph pre-training commonly used in the literature. The backbone of our graph pre-training model also follows this framework.

We start with a natural question: What does graph pre-training actually learn? On one hand, graph pre-training tries to learn transferable semantic meaning associated with structural patterns. For example, both in citation networks and social networks, the closed triangle structure (\(}\)) is interpreted as a stable relationship, while the open triangle (\(}\)) indicates an unstable relationship. In comparison, these semantic meanings can be quite different in other networks, _e.g._, molecular networks. On the other hand, however, the distinction (or relationship) between different structural patterns is still transferable. Taking the same example, the closed and open triangles might yield different interpretations in molecular networks (stability of certain chemical property) from those in social networks (stability in social intimacy), but the distinction between these two structures remains the same: they indicate opposite (or contrastive) semantic meanings . Therefore, the graph pre-training either learns representative structural patterns (when semantic meanings are present), or more importantly, obtains the capability of distinguishing these patterns. This observation in graph pre-training is not only different from that in other areas (_e.g._, computer vision and natural language processing), but may also explain why graph pre-training is effective, especially when some downstream information is absent.

With the hope to learn the transferable structural patterns or the ability to distinguish them, the graph pre-training model is fed with a diverse collection of input graphs, and the learned model, denoted by \(f_{}\) (or simply \(f\) if the parameter \(\) is clear from context), maps a node to a low-dimensional representation. Unaware of the specific downstream task as well as task-specific labels, one typically designs a self-supervised task for the pre-training model. Such self-supervised information for a node is often hidden in its neighborhood pattern, and thus the structure of its ego network can be used as the transferable pattern. Naturally, subgraph instances sampled from the same ego network \(_{i}\) are considered _similar_ while those sampled from different ego networks are rendered _dissimilar_. Therefore, the pre-training model attempts to capture the similarities (and dissimilarities) between subgraph instances, and such a self-supervised task is called the _subgraph instance discrimination task_. More specifically, given a subgraph instance \(_{i}\) from an ego network \(_{i}\) centered at node \(v_{i}\) as well as its representation \(_{i}=f(_{i})\), the model \(f\) aims to encourage higher similarity between \(_{i}\) and the representation of another subgraph instance \(_{i}^{+}\) sampled from the same ego network. This can be done by minimizing, _e.g._, the InfoNCE loss ,

\[_{i}=-_{i}^{}f(_{i}^{+})/ )}{(_{i}^{}f(_{i}^{+})/)+_ {_{i}^{}_{i}^{-}}(_{i}^{}f(_{i}^{ })/)},\] (1)

where \(_{i}^{-}\) is a collection of subgraph instances sampled from different ego networks \(_{j}\) (\(j i\)), and \(>0\) is a temperature hyper-parameter. Here the inner product is used as a similarity measure between two instances. One common strategy to sample these subgraph instances is via random walks on graphs, as used in GCC , and other sampling methods as well as loss functions are also valid.

## 3 Data-Active Graph Pre-training

In this section, we present the proposed APT framework for graph pre-training, and the overall pipeline is illustrated in Figure 2. The APT framework consists of two major components: a graph selector and a graph pre-training model. The technical core is the interaction between these two components: The graph selector feeds _suitable_ data for pre-training, and the graph pre-training model learns from the carefully chosen data. The feedback of the pre-training model in turn helps select the needed data tailored to the model.

The rest of this section is organized as follows. We describe the graph selector in SS3.1 and the graph pre-training model in SS3.2. The overall pre-training and fine-tuning strategy is presented in SS3.3.

### Graph selector

In view of the curse of big data phenomenon, it is more appealing to carefully choose data _well suited_ for graph pre-training rather than training on a massive amount of data. Conventionally, the criterion of suitable data, or the contribution of a data point to the model, is defined based on the output predictions on downstream tasks . In graph pre-training where downstream information is absent, new selection criteria or guidelines are needed to provide effective instructions for the model. Here we introduce two kinds of selection criteria, originating from different points of view, to help select suitable data for pre-training. The _predictive uncertainty_ measures the model's understanding of certain data, and thus helps select the least certain data points for the current model. In addition to the measure of model's ability, some _inherent properties_ of graphs can also be used to assess the level of representativeness or informativeness of a given graph.

**Predictive uncertainty.** The notion of predictive uncertainty can be explained via an illustrative example, as shown in part (a) of the graph selector component in Figure 2. Consider a query subgraph instance \(_{i}\) (denote by \(}\) in Figure 2) from the ego network \(_{i}\) in a graph \(G\). If the pre-training model cannot tell its similar instance \(_{i}^{+}\) (denoted by \(}\)) from its dissimilar instance \(_{i}^{-}_{i}^{}\) (denoted by \(}\)), we say that the current model is uncertain about the query instance \(_{i}\). Therefore, the contrastive loss function in Eq. (1) comes in handy as a natural measure for the predictive uncertainty of the instance \(_{i}\): \(_{}(_{i})=_{i}\). Accordingly, the predictive uncertainty of a graph \(G\) (_i.e._, the graph-level predictive uncertainty) is defined as \(_{}(G)=(1/M)_{i=1}^{M}_{i}\), where \(M\) is the number of subgraph instances queried in this graph.

We further establish a provable connection between the proposed predictive uncertainty and the conventional definition of uncertainty. In most existing work, model uncertainty is often defined in the label space, _e.g._, taking as the uncertainty the cross entropy loss \(_{}()\) of an instance \(\) on the downstream classifier . Comparatively, our definition of uncertainty, \(_{}()\), is in the representation space. The theoretical connection between these two losses is given in the following theorem.

**Theorem 1** (Connection between uncertainties.).: _Let \(\), \(\) and \(\) be the input space, representation space and label set of downstream classifier. Denote a downstream classifier by \(h\) and the

Figure 2: Overview of the proposed data-active graph pre-training paradigm. The graph selector provides the graph and samples suitable for pre-training, while the graph pre-training model learns from the incoming data in a progressive way and in turn better guides the selection process. In the graph selector component, _Part (a)_ provides an illustrating example on the predictive uncertainty, and _Part (b)_ plots the Pearson correlation between the properties of the input graph and the performance of the pre-trained model on the training set (using this graph) when applied to different unseen test datasets (see Appendix F for other properties that exhibit little/correlation with performance).

set of downstream classifiers by \(\). Assume that the distribution of labels is a uniform distribution over \(\). For any graph encoder \(f:\), one has_

\[_{}()|}{  2-_{}()},\]

_where \(_{}\) denotes the conventional uncertainty, defined as cross entropy loss and estimated from the composition of graph encoder and downstream classifier \(h f\), and \(_{}\) is the proposed uncertainty estimated from graph encoder \(f\) (independent of the downstream classifier)._

While the proof and additional discussion on the advantage of \(_{}\) are postponed to Appendix B, we emphasize here that, by Theorem 1, a smaller \(_{}\) over all downstream classifiers cannot be achieved without a smaller \(_{}\).

Although GCC is used as the backbone model in the presented framework, our data selection strategy can be easily adapted to other non-contrastive learning tasks. In that case, the InfoNCE loss used in \(_{}\) should be replaced with another pre-training loss associated with the specific learning task. More details with the example of graph reconstruction is included in Appendix H.

Graph properties.As we see above, the predictive uncertainty measures the model's ability to distinguish (or identify) a given graph (or subgraph instance). However, predictive uncertainty is sometimes misleading, especially when the chosen graph (or subgraph) happens to be an outlier of the entire data collection. Hence, learning solely from the most uncertain data is not enough to boost the overall performance, or worse still, might lead to overfitting. The inherent properties of the graph turn out to be equivalently important as a selection criterion for graph pre-training. Intuitively, it is preferable to choose those graphs that are _good by themselves_, _e.g._, those with better structure, or those containing more information. To this end, we introduce five inherent properties of graphs, _i.e._, network entropy, density, average degree, degree variance and scale-free exponent, to help select _better_ data points for pre-training. All these properties exhibit a strong correlation with downstream performance, which is empirically verified and presented in part (b) of the graph selector component in Figure 2. The choice of these properties also has an intuitive explanation, and here we discuss the intuition behind the network entropy as an example.

The use of _network entropy_ is inspired from the sampling methods used in many graph pre-training models (see, _e.g._, ).In those works, random walks are used to construct subgraph instances (used as model input). Random walks can can also serve as a means to quantify the amount of information contained in a graph. In particular, the amount of information contained in a random walk from node \(v_{i}\) to node \(v_{j}\) is defined as \(- P_{ij}\), where \(P\) is the transition matrix. Thus, the network entropy of a connected graph can be defined as the expected information of individual transitions over the random walk process :

\[_{}=- P_{P}=-_{i,j}_{i}P_{ij} P _{ij},\] (2)

where \(\) is the stationary distribution of the random walk and \(_{P}\) denotes the expectation of a random variable according to \(P\). Network entropy (2) is in general difficult to calculate, but is still tractable for special choices of the transition matrix \(P\). For example, for a connected unweighted graph \(G=(V,E)\) with node degree vector \(^{|V|}\), if the transition matrix is defined by \(P_{ij}=1/d_{i}\), then the stationary distribution is \(=(1/2|E|)\) and the network entropy (2) reduces to

\[_{}=_{i=1}^{|V|}d_{i} d_{i}.\] (3)

In this case, the network entropy of a graph depends solely on its degree distribution, which is straightforward and inexpensive to compute.

Although the definition of network entropy originates from random walks on graphs, it is still useful in graph pre-training even when the sampling of subgraph instances does not depend on random

Figure 3: Illustrative graphs with increasing network entropy (bottom left to top right), and the other four graph properties.

walks. Its usefulness can also be explained via the coding theory. Network entropy can be viewed as the entropy rate of a random walk, and it is known that the entropy rate is the expected number of bits per symbol required to describe a stochastic process . Similarly, the network entropy can be interpreted as the expected number of "words" needed to describe the graph. Thus, intuitively, the larger the network entropy is, the more information the graph contains.

We also note that the connectivity assumption does not limit the usefulness of Eq. (3) in our case. For disconnected input graphs, we can simply compute the network entropy of the largest connected component, since for most real-world networks, the largest connected component contains most of the information . Alternatively, we can also take some of the largest connected components from the graph and treat them separately as several connected graphs.

Furthermore, the other four graph properties, _i.e._, density, average degree, degree variance and scale-free exponent, are closely related to the network entropy. Figure 3 presents a clear correlation between the network entropy and the other four graph properties, as well as provides some illustrative graphs. (These example graphs are generated by the configuration model proposed in , and Appendix F contains more results on real-world networks.) Intuitively, graphs with higher network entropy contain a larger amount of information, and so are graphs with larger density, higher average degree, higher degree variance, or a smaller scale-free exponent. The connections between all five graph properties can also be theoretically justified and the motivations of choosing these properties can be found in Appendix A. The detailed empirical justification of these properties and the pre-training performance in included in Appendix F.

Time-adaptive selection strategyThe proposed predictive uncertainty and the five graph properties together act as a powerful indicator of a graph's suitability for a pre-training model. Then, we aim to select the graph with the highest score, where the score is defined as

\[(G)=(1-_{t})_{}+_{t}(_{},_{},_{},_{ },_{}),\] (4)

where the optimization variable is the graph \(G\) to be selected, \(_{t}\) is a trade-off parameter to balance the weight between predictive uncertainty and graph properties, and \(t\) is the iteration counter. The small hat on the terms \(_{}\), \(_{}\), \(_{}\), \(_{}\), \(_{}\) and \(_{}\) indicates that all the values are already \(z\)-normalized, so the objective (especially the MEAN operator) is independent of their original scales.

Note that the pre-training model learns nothing at the beginning, so we initialize \(_{0}=1\), and in later iterations, the balance between the predictive uncertainty and the inherent graph properties ensures that the selected graph is a good supplement to the current pre-training model as well as an effective representative for the entire data distribution. In particular, at the beginning of the pre-training, the outputs of the model are not accurate enough to guide data selection, so the parameter \(_{t}\) should be set larger so that the graph properties play a leading role. As the training phase proceeds, the graph selector gradually pays more attention to the feedback \(_{}\) via a smaller value of \(_{t}\). Therefore, in our framework, the parameter \(_{t}\) is called the _time-adaptive parameter_, and is set to be a random variable depending on time \(t\). In this work, we take \(_{t}\) from a Beta distribution \(_{t}(1,_{t})\), where \(_{t}\) decreases over time (training iterations).

### Graph pre-training model

Instead of swallowing all the pre-training graphs as a whole, our graph pre-training model takes the input graphs and samples one by one in a sequential order and enhances itself in a progressive manner. However, such a straightforward sequential training does not guarantee that the model will _remember_ all the contributions of previous input data. As shown in the orange curve in Figure 4, the previously learned graph exhibits a larger predictive uncertainty as the training phase proceeds.

The empirical result indicates that the knowledge or information contained in previous input data will be forgotten or covered by newly incoming data. This phenomenon, called _catastrophic forgetting_, was first noticed in continual learning  and also

Figure 4: Predictive uncertainty versus training epoch.

appears in our case. Intuitively, when the training data is taken in a progressive or iterative manner, the learned parameters will cater to the newly incoming data and _forget_ the old, previous data points.

One remedy for this issue is adding a proximal term to the objective. The additional proximal term (_i.e._, the regularization) guarantees the proximity between the new parameters and the model parameters learned from previous graphs. Therefore, when the model is learning the \(k\)-th input graph, the loss function for our pre-training model in APT is

\[()=_{i}_{i}()+_{j }F_{jj}^{(k-1)}(_{j}-_{j}^{(k-1)})^{2},\] (5)

where \(_{i}\) is given in Eq. (1), the first summation is taken over the subgraph instances sampled from the latest input graph, \(^{(k-1)}\) is the model parameters after learning from the first \(k-1\) graphs, and the parameter \(\) describes the trade-off between the knowledge learnt from new data and that from previous data. Here, \(F^{(k-1)}\) is the Fisher information matrix of \(^{(k-1)}\), and \(F_{jj}^{(k)}\) is its \(j\)-th diagonal element. When \(F\) is set as an identity matrix, the second term degenerates to the L2 regularization (which serves as one of our variants). The proximal term in Eq. (5) is absent when the first input graph is introduced to the model, and the term is applied on the first three layers of the pre-training model. Finally, we note that the total number of parameters in the pre-training model is in the same order of magnitude as classical GNNs, so the memory cost would not be a bottleneck.

### Training and fine-tuning

Integrating the graph selector and the pre-training model forms the entire APT framework, and the overall algorithm is presented in Appendix C. After the training phase, the APT framework returns a pre-trained GNN model, and then the pre-trained model is applied to various downstream tasks from a wide spectrum of domains. In the so-called _freezing mode_, the pre-trained model returned by APT is directly applied to downstream tasks, without any changes in parameters. Alternatively, the _fine-tuning mode_ uses the pre-trained graph encoder as initialization, and offers the flexibility of training the graph encoder and the downstream classifier together in an end-to-end manner.

## 4 Experiments

In the experiments, we pre-train a graph representation model via the proposed APT framework, and then evaluate the transferability of our pre-trained model on multiple unseen graphs in the node classification and graph classification task. Lastly, we include the applicable scope of our pre-trained model. Additional experiments can be found in Appendix H, including our adaptation to backbone pre-training models GraphCL, JOAO and Mole-BERT, training time, impact of different graph properties, analysis of the ablation studies and selected pre-training graphs, hyper-parameter sensitivity, explorations of various combinations of graph properties.

### Experimental setup

Datasets.The datasets for pre-training and testing, along with their statistics, are listed in Appendix D. Pre-training datasets are collected from different domains, including social, citation, and movie networks. Testing datasets comprise both in-domain (_e.g._, citation, movie) and cross-domain (_e.g._, image, web, protein, transportation and others) datasets to evaluate transferability comprehensively, also including large-scale datasets with millions of edges sourced from .

Baselines.We evaluate our model against the following baselines for node and graph classification tasks. For node classification, ProNE , DeepWalk , struc2vec , DGI , GAE , and GraphSAGE  are used as baselines, and then the learned representations are fed into the logistic regression, as most of baselines did. As for graph classification, we take graph2vec , InfoGraph , DGCNN  and GIN  as baselines, using SVM as the classifier, which aligns with the methodology of most baselines. For both tasks, we also compare our model with (1) Random: random vectors are generated as representations; (2) GraphCL : a GNN pre-training scheme based on contrastive learning with augmentations; (3) JOAO : a GNN pre-training scheme that automatically selects data augmentations; (4) GCC : the state-of-the-art cross-domain graph pre-training model (which is our model's version without the data selector, trained on all pre-training

[MISSING_PAGE_FAIL:8]

the proximity regularization with respect to old knowledge, proximity term, graph properties and predictive uncertainty). We also explore the impacts of the five graph properties used in our model, and demonstrate their indispensability by experiments. Thus, combining all graph properties is essential to boost the performance of APT. Details in ablation study and more experimental results can be found in Appendix H.

Graph classification.The micro F1 score on unseen test data in the graph classification task is summarized in Table 2. Especially, our model is \(7.2\%\) and \(1.3\%\) on average better than the graph pre-training backbone model under freezing and fine-tuning mode, respectively. Interestingly, we observe that all variants of APT perform quite well, and thus, a simpler yet well-performed version of APT could be used in practice. This phenomenon could happen since "graph pre-train and fine-tune" is an extremely complicated non-convex optimization problem. Another observation is that in specific cases, such as dd dataset, a decrease in downstream performance after fine-tuning is observed, as compared to the freezing mode. This could happen since "graph pre-train and fine-tune" is an extremely complicated non-convex optimization problem. A similar observation has been made in previous work  as well.

Finally, it is worth noting that our APT model achieves a training time 2.2\(\) faster than the competitive model GCC, achieved through a reduced number of carefully selected training graphs and samples. More specifically, we carefully selected only 7 datasets out of the available 11 and performed pre-training using at most 24.92% of the samples in each selected dataset. Moreover, for each newly added dataset, our model only needs a few more training iterations to convergence, rather than being trained from scratch.

### Discussion: scope of application

The transferability of the pre-trained model comes from the learned representative structural patterns and the ability to distinguish these patterns (as discussed in SS2). Therefore, our pre-training model is more suitable for the datasets where the target (_e.g._, labels) is correlated with subgraph patterns or structural properties (_e.g._, unrofits, triangles, betweenness, stars). For example, for node classification on heterophilous graphs (_e.g._, winconsin, cornell), our model performs very well because in these graphs, nodes with the same label are not directly connected, but share similar structural properties and behavior (or role, position). On the contrary, graphs with strong homophily (like cora, pubmed, ogbarxiv and ogbproteins) may not benefit too much from our models. Similar observation can also be made on graph classification: our model could also benefit the graphs whose label has a strong relationship with their structure, like molecular, chemical, and protein networks (_e.g._, dd in our experiments) .

## 5 Related Work

Graph pre-training.Inspired by pre-training in CV/NLP, recent efforts have shed light on pre-training on GNNs. Initially, unsupervised graph representation learning is used for graph pre-training . The design of unsupervised models is largely based on the neighborhood similarity assumption, and thus cannot generalize to unseen graphs. More recently, self-supervised graph learning emerges as another line of research, including graph generative and contrastive models. Graph generative models aim to capture the universal graph patterns by recovering certain parts of input graphs , but they rely heavily on domain-specific knowledge. In comparison, contrastive models maximize the agreement between positive pairs and minimize that between negative pairs . Some work in

   Method Dataset & indb.-binary & dd & mmc.21 & AR \\  Random & 49.30(4.82) & 52.72(34.34) & 44.92(12.14) & 11 \\ graph2vec & 56.20(5.33) & 59.16(3.47) & 8.22(3.67) & 7.7 \\ InfoGraph & 66.86(0.83) & 63.86(0.63) & 6.01(0.59) & 7.7 \\ GraphCL (_f_) & 55.10(13.18) & 57.82(47.13) & 5.42(47.79) & 9.3 \\ JOAO (_freeze_) & 63.90(3.48) & 55.97(3.61) & 5.09(2.65) & 9.3 \\ GCC (_freeze_) & 73.09(0.55) & 75.16(0.35) & 11.61(1.93) & 5.3 \\ APTG (_freeze_) & 73.13(0.39) & 75.20(4.22) & 2.18(0.74) & 4.3 \\ APTG (_freeze_) & 72.38(0.31) & **76.30(0.30)** & 13.30(0.57) & 3.0 \\ APTR (_freeze_) & **73.89(0.21)** & 75.20(32.34) & 1.29(0.57) & 3.0 \\ APTL2 (_freeze_) & 73.54(0.40) & 75.81(0.30) & 13.16(0.77) & 2.3 \\ JAF (_freeze_) & 73.00(0.50) & 75.83(0.31) & **13.81(0.16)** & 3.0 \\  DCCNN & 71.10(4.69) & 65.83(4.44) & 6.04(0.59) & 11.3 \\ GIN & 72.00(4.21) & 77.61(4.17) & 4.054(5.08) & 6.0 \\ GraphCL (_f_), fine-tune (_i_) & 63.61(3.81) & 55.84(5.60) & 2.58(3.94) & 12.7 \\ JOAO (_and_ fine-tune_) & 67.30(3.35) & 62.10(3.11) & 11.40(3.06) & 10.0 \\ GCC (_and_ fine-tune_) & 75.80(3.74) & 73.46(0.59) & 7.13(0.43) & 7.3 \\ GraphCL (_fine-tune_) & 66.50(4.39) & 65.55(5.14) & 8.77(2.60) & 10.7 \\ JOAO (_fine-tune_) & 68.50(3.61) & 62.61(4.99) & 10.81(1.27) & 10.0 \\ JGC (_fine-tune_) & 76.10(9.90) & 75.23(21.77) & 2.940(1.65) & 4.7 \\ APTG (_fine-tune_) & 76.20(9.89) & 75.46(0.71) & 2.94(0.73) & 4.7 \\ APTP (_fine-tune_) & **76.01(1.75)** & 75.48(0.48) & 2.24(12.23) & 3.7 \\ APTR (_fine-tune_) & 76.60(1.02) & 75.64(0.70) & 24.90(1.22) & 3.3 \\ APTL2 (_fine-tune_) & 75.30(0.84) & 75.58(1.06) & **25.81(5.87)** & 3.7 \\ APTR (_fine-tune_) & 76.27(1.20) & **75.69(1.42)** & 24.41(1.82) & 3.0 \\   

Table 2: Micro F1 of different models in graph classification.

this direction takes subgraph sampling as augmentation, in the hope that the transferable subgraph patterns can be captured during pre-training. However, all the aforementioned studies only focus on the design of pre-training models, rather than suitable selection of data for pre-training.

Uncertainty-based sample selection.The terminology _uncertainty_ is widely used in machine learning, without a universally-accepted definition. In general, this term refers to the lack of confidence of an ML model in certain model parameters. The majority of existing works define uncertainty in the label space, such as taking the uncertainty as the confidence level about the prediction . Only a few works define uncertainty in the representation space . In , uncertainty is measured based on the representations of an instance's nearest neighbors with the same label. However, this approach requires access to the label information of the neighbors, and thus cannot be adapted in pre-training with unlabeled data.  introduces a pretext task for training a model of uncertainty over the learned representations, but this method assumes a well-pre-trained model is already available. Such a post processing manner is not applicable to our scenario, because we need an uncertainty that can guide the selection of data during pre-training rather than after pre-training.

Some works on active learning and hard example mining (HSM) have also introduced concept similar to uncertainty. In active learning, uncertainty is measured via classification prediction, and the active learning model focuses on those samples which the model is least certain about . However, these techniques all rely on the labels and cannot be adapted in pre-training with unlabeled data. As another line of work, HSM introduces similar strategies and works on those samples with the greatest loss, which can also be regarded as a kind of uncertainty . Nevertheless, existing HSM approaches do not meet the following two requirements needed in our setting. (1) The chosen instances should follow a joint distribution that reflects the topological structures of real-world graphs. This is satisfied by our use of graph-level predictive uncertainty and graph properties, but is not met in HSM. (2) The chosen set of graphs should include informative and sufficiently diverse instances. This is only achieved by the proposed APT framework while existing HSM methods fail to consider this requirement.

Pre-training in CV and NLP.For pre-training in CV and NLP, scaling up the pre-training data size often results in a better or saturating performance in the downstream . In view of this, data selection is not an active research direction for CV and NLP. Existing studies mainly focus on selecting pre-training data that closely matches the downstream domain . The assumption on downstream domain knowledge differs from our graph pre-training setting, making such data selection less relevant to our work.

Data-centric AI.This recently introduced concept emphasizes the enhancement of data quality and quantity, rather than model design . Following-up works in graph pre-training  exploits the data-centric idea to design data augmentation. For example,  introduces a graph data augmentation method by interpolating the generator of different classes of graphs.  mainly focuses on the theoretical analysis of data-centric properties of data augmentation. While many of these works advocate for shifting the focus to data, they do not consider the co-evolution of data and model, as is the case in our work.

## 6 Conclusion

In this paper, we identify the _curse of big data_ phenomenon for pre-training graph neural networks (GNNs). This observation then motivates us to choose a few suitable graphs and samples for GNN pre-training rather than training on a massive amount of unselected data. Without any knowledge of the downstream tasks, we propose a novel graph selector to provide the most instructive data for pre-training. The pre-training model is then encouraged to learn from the data in a progressive and iterative way, reinforce itself on newly selected data, and provide instructive feedback to the graph selector for further data selection. The integration of the graph selector and the pre-training model into a unified framework forms a data-active graph pre-training (APT) paradigm, in which the two components are able to mutually boost the capability of each other. Extensive experimental results verify that the proposed APT framework indeed enhances model capability with fewer input data.

#### Acknowledgments

This work was partially supported by NSFC (62206056, 62322606). Xin Jiang and Carl Yang were not supported by any fund from China.