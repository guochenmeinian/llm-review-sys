# Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger

Zhiqi Bu

AWS AI

zhiqibu@amazon.com

&Yu-Xiang Wang

AWS AI, UC Santa Barbara

yuxiangw@cs.ucsb.edu

&Sheng Zha

AWS AI

zhasheng@amazon.com

&George Karypis

AWS AI

gkarypis@amazon.com

###### Abstract

Per-example gradient clipping is a key algorithmic step that enables practical differential private (DP) training for deep learning models. The choice of clipping threshold \(R\), however, is vital for achieving high accuracy under DP. We propose an easy-to-use replacement, called automatic clipping, that eliminates the need to tune \(R\) for any DP optimizers, including DP-SGD, DP-Adam, DP-LAMB and many others. The automatic variants are as private and computationally efficient as existing DP optimizers, but require no DP-specific hyperparameters and thus make DP training as amenable as the standard non-private training. We give a rigorous convergence analysis of automatic DP-SGD in the non-convex setting, showing that it can enjoy an asymptotic convergence rate that matches the standard SGD, under a symmetric gradient noise assumption of the per-sample gradients (commonly used in the non-DP literature). We demonstrate on various language and vision tasks that automatic clipping outperforms or matches the state-of-the-art, and can be easily employed with minimal changes to existing codebases1.

## 1 Introduction

Deep learning has achieved impressive progress in a wide range of tasks. These successes are made available, in part, by the collection of large datasets, sometimes containing sensitive private information of individual data points. Prior works have illustrated that deep learning models pose severe privacy risks to individual subjects in the training data and are susceptible to various practical attacks. For example, machine learning services such as Google Prediction API and Amazon Machine Learning can leak membership information from the purchase records ; the GPT2 language models auto-complete texts that contain someone's full name, phone number, email address, etc., from the training data that it memorizes, if invoked by specific prefixes .

Differential privacy (DP) [24; 26; 25] is a formal definition of privacy that has been shown to prevent the aforementioned privacy risks in deep learning . At a high level, the key difference between the DP deep learning and the standard one is whether the gradient is privately released. In other words, while the standard optimizers update on \(_{i}_{i}\), the DP optimizers update on the _private gradient_:\[(\{_{i}\}_{i=1}^{B}) =(_{i}( \|_{i}\|;R)+ R(0,)}^{})\] (1.1) \[(\{_{i}\}_{i=1}^{B}) =(_{i}}^{})\] (1.2)

Here \(_{i}^{d}\) is the per-sample gradient of loss \(l_{i}\), \(\) is the standard normal, \(\) is the noise multiplier, and \(R\) is the clipping threshold. The clipping function \(:^{d}\) is defined such that \(\|_{i}(_{i};R)\| R\). For instance, the DP-SGD in  is

\[_{}:_{t+1}=_{t}- _{i}}{_{t}} R/}{_{t}},1+ R (0,)\] (1.3)

In comparison to the regular training (1.2), two additional DP-specific hyperparameters \(R\) and \(\) need to be determined in DP learning (1.1). On the one hand, setting the noise multiplier \(\) is easy and can be derived analytically prior to the training. Whenever the privacy budget \((,)\) is determined, one can apply off-the-shelf privacy accounting tools in Section 2.1 to determine \(\), based on the subsampling probability \(p\) and the number of iterations \(T\):

\[(,p,T;)=\]

On the other hand, the choice of clipping threshold \(R\) is crucial to the performance of DP models, yet the hyperparameter tuning is much labor-intensive. Recent advances of DP deep learning on ImageNet  and on E2E datasets , using ResNet18 and GPT2 respectively, illustrate that the performance is very sensitive to \(R\). We have reproduced their results in Figure 1. Observe that on ImageNet, ResNet18 can drop from the highest 45% accuracy to 31% if \(R\) is chosen 2 times larger, and to 0.1% if \(R\) is chosen 4 times larger. Similar drastic drop can also be observed in [38, Figure 3] even if the noise multiplier \(=0\). Unlike the noise multiplier \(\), the clipping threshold \(R\) cannot be inferred from the privacy budget \((,)\) and have to be tuned. Consequently, DP training necessarily requires an expensive 2D grid search for \((R,)\), like Figure 1, whereas the regular training only requires an easy 1D grid search for \(\). Even worse, the difficulty of tuning a per-layer clipping threshold vector , i.e. one clipping threshold for one layer, may increase exponentially as the number of layers increases.

To save the effort of tuning \(R\), previous researches have proposed different approaches. In , researchers advocate to use data-adaptive information to select \(R\), such as a specified quantile of the gradient norm distribution. These adaptive clipping methods can be a little ad-hoc: they often replace the need to tune \(R\) by the need to tune one or more new hyperparameters, e.g. the quantile to use and the ratio to split the privacy budget between the quantile decision and the gradient perturbation. Another approach used by the practitioners is to replace the single 2D grid search by multiple cheaper 1D grid searches. For example, the researchers propose, in [38, Section 3.3] to fine-tune \(\) with non-DP SGD, fix \(\) and sweep over various values of the clipping threshold \(R\) with DP-SGD, then further fix \(R\) and do one more grid search on \(\). However, tuning \(R\) formally in a data-dependent way (e.g. through cross-validation) introduces additional privacy loss , and most existing empirical work does not privately conduct hyperparameter tuning.

We take a completely different route by proposing a new clipping principle that removes \(R\), instead of coming up with methods to find the appropriate \(R\). We term our method as _automatic clipping_ and the DP optimizers using it as _automatic DP optimizers_. Our contributions are:

1. We propose the automatic clipping in (4.1) that expunges the clipping threshold from general DP optimizers, making DP training as amenable as regular training. In large-scale tasks (GPT-level) like Figure 1, our automatic clipping can reduce the cost of ablation study by \(5^{2}\).
2. We show that automatic DP optimizers are as private and efficient as existing DP optimizers.
3. We show in Theorem 4 that automatic DP-SGD converges in the non-convex setting, at the same asymptotic convergence rate as the standard SGD. Our theoretical analysis successfully explains the training behaviors of deep learning in previous empirical works.
4. We demonstrate the superiority of automatic clipping on a variety of vision and language tasks, especially with large models including ResNet, RoBERTa and GPT2.
5. In Appendix K, we include simple code snippets that demonstrate how easy it is to switch from Abadi's clipping to our automatic clipping in popular codebases, e.g. Opacus and ObJAX.

## 2 Preliminaries

### Differential Privacy

We consider the \((,)\)-DP in Definition 2.1, where smaller \((,)\) means stronger privacy guarantee.

**Definition 2.1** ().: A randomized algorithm \(M\) is \((,)\)-differentially private (DP) if for any two neighboring datasets \(S,S^{}\) (i.e. if one can obtain \(S^{}\) by adding or removing one data point from \(S\)), and for any event \(E\),

\[[M(S) E]^{}[M(S^{ }) E]+.\] (2.1)

In words, DP restricts the influence of an arbitrary sample, so that the information contributed by such sample is limited and less vulnerable to privacy attacks. In deep learning, DP is achieved by applying the _subsampled Gaussian mechanism_ to privatize the minibatch gradients during training.

As illustrated in Equation (1.1), the subsampled Gaussian mechanism involves (I) sampling a minibatch by including each data point iid with probability \(p\) (II) per-sample gradient clipping to bound the \(l_{2}\) norm sensitivity at \(R\) and (III) adding independent Gaussian noise proportional to \(R\) and \(\), where \(\) is derived from the privacy budget \((,)\). This can be realized by leveraging a variety of modern privacy accounting tools, such as Renyi DP (or moments accountant) [1; 51; 71], Privacy Loss distribution (Fourier accountants) [37; 30; 82], or Gaussian DP [19; 7].

### Differentially Private optimizers with general clipping operations

Privately released stochastic gradients (through the Gaussian mechanism) can be used by various off-the-shelf optimizers, including DP-SGD in (1.3), DP-HeavyBall, DP-AdaGrad, DP-Adam, DP-FedAvg/FedSGD , etc. To improve the performance of DP optimizers, previous researches on the per-sample clipping can be classified into two categories.

The first category, where the majority of researches lie in, works with Abadi's clipping and focuses on better design of \(R\). To name a few examples, one can adaptively design \(R_{t}\) for each iteration \(t\)[3; 58; 28], or design the per-layer clipping threshold vector \(^{L}\) for \(L\) layers [1; 49] so as to apply a different clipping threshold for each layer.

Fewer works fall into the second category that proposes new clipping methods. In fact, any function \(:^{d}\) satisfying \(\|()\| R\) can serve as a valid clipping function besides Abadi's. For example, the global clipping  proposes \(_{}(_{i}):=(\|_{i}\|<R)\) to mitigate the bias of the private gradient and alleviate the mis-calibration issue of DP classifiers. Another example is the re-parameterized clipping , \(_{}(_{i}):=(1/\|_{i}\|,1/R)\), which is equivalent to Abadi's clipping under a re-scaled learning rate. Our automatic clipping belongs to this category. We note that different clipping methods work orthogonally to optimizers, network architectures and gradient norm computation (see Section 8).

## 3 Motivation

### Small clipping threshold often works best

Figure 1: Ablation study of clipping threshold and learning rate. Left: BLEU score of GPT2 on E2E dataset , with DP-AdamW. Right: Test accuracy of ResNet18 on ImageNet , with DP-SGD.

One intriguing observation that we can make about the recent studies on DP learning with large models is that the state-of-the-art (SOTA) results are often achieved with very small clipping threshold \(R\). This observation is consistent in both vision and language tasks. In , GPT2 (about 800 million parameters) and RoBERTa models (over 300 millions parameters) achieve the best results under DP on QNLI, MNLI, SST-2, QQP, E2E, and DART datasets, with each per-sample gradient clipped to length \(R=0.1\). In [38; 17; 50], ResNets and Vision Transformers achieve the best DP results on ImageNet with \(R=1\); in , the best DP results on CIFAR10 use \(R=0.1\) with ResNeXt-29 and SimCLRv2 . The effectiveness of small clipping threshold together with proper learning rate is depicted in Figure 1.

Intuitively, smaller \(R\) implies that the Abadi's clipping (3.1) is effective, which means \(R/\|_{i}\|,1=R/\|_{i}\|\). Given that the clipping threshold \(R\) is so small compared to the number of parameters in large models, and that strong DP is guaranteed when the number of training iterations is small (i.e. \(\|_{i}\|\) has not converged to small values yet), we expect and empirically observe that the clipping happens on a large proportion of per-sample gradients at all iterations. For instance, we find in the GPT2 generation experiments in  that 100% of per-sample gradients are clipped at all iterations; in classification tasks such as QQP, QNLI, and MNLI, the percentage of clipping is about \(20 60\%\) on average (more details in Appendix H.1).

### Per-sample gradient normalization as new clipping

In the small clipping threshold regime, we can approximately view

\[_{}(_{i};R)=(R/||_{i}||,1 ) R/||_{i}||=:_{}(_{i};R)\] (3.1)

and thus derive a novel private gradient \(_{i}R_{i}}{\|_{i}\|}+ R(0,)\). Here AUTO-V stands for the vanilla automatic clipping, which essentially performs the normalization on each per-sample gradient. As a specific example, we can write the \(R\)-dependent automatic DP-SGD as

\[R_{}:_{t+1}=_{t}- _{i}R}{_{t}}/\|}{_{t}}\|+ R(0,)\] (3.2)

We may view our AUTO-V clipping as to maximize the dot-product similarity (a commonly used similarity measure, e.g. in the attention block in transformers ) between the clipped gradient and the regular gradient. Suppose we want to

\[_{C_{i}}_{i}C_{i}_{i},_{j}_{j} 0 C_{i} R/\|_{i}\|\] (3.3)

Note that the constraint is a sufficient condition for clipping, as discussed in Section 2.2. It is not hard to see that the optimal clipping factor (though violating DP guarantee3) regarding (3.3) is

\[C_{i}=R/\|_{i}\|(_{i},_{j}_{j} >0),\] (3.4)

If the per-sample gradients are indeed concentrated in the sense \( i,_{i},_{j}_{j} 0\), then AUTO-V is the optimal per-sample gradient clipping. We compare with Abadi's clipping in Figure 2, where this similarity is significantly magnified by our AUTO-V clipping. In fact, the dot-product similarity in (3.3) closely resembles the convergence of DP optimization for Theorem 4 in (C.2).

Figure 2: RoBERTa-base with DP-Adam (\(=3\)) on SST2 dataset, as in Section 6.2.

### Stability constant breaks scale-invariance and remains stationary

One potential drawback of AUTO-V clipping is that all gradients lose their magnitudes information completely, since \(\|_{i}_{}(_{i};R)\|=R, i\). This scale-invariance in AUTO-V and partially in Abadi's clipping (when \(\|_{i}\|>R\)) leads to the "lazy region" issue: the parameters will not be updated by DP-GD even if the true gradients are non-zero. In Figure 3, we illustrate such issue in a logistic regression4 for AUTO-V and Abadi's clipping, when the trainable parameter \([-2,2]\), as the gradients from two classes cancel each other.

To preserve the magnitude information and thus escape the lazy region, we propose the AUTO-S clipping, with a positive stability constant \(\):

\[_{}(_{i};R):=R/(||_{i}||+)\] (3.5)

We visualize in Figure 5 that AUTO-S allows larger per-sample gradients to have larger magnitudes after the clipping, while still allowing smaller gradients to vanish after "clipping". That is, as \(_{i} 0\), the existence of \(\) allows the clipped gradient \(C_{i}_{i}_{i}/\) rather than having a magnitude \(R\) as in AUTO-V. We elaborate this point in Section 4.3. This is critical in our convergence analysis and allows DP-SGD\({}_{}\) (but not DP-SGD\({}_{}\)) to converge to zero gradient norms in Section 5.

## 4 Automatic DP Training

One may wonder why our clipping (3.1)(3.5) is automatic at all, if the hyperparameter \(R\) is still present and there is an additional parameter \(\) to choose. It turns out that any constant choice of \(R>0\) is equivalent to choosing \(R=1\), and common deep learning optimizers are insensitive to the choice of \(\) (e.g. for any \(>0\), we show that the gradient norm converges to zero at the same asymptotic rate in Theorem 4; see also the ablation study in Figure 15). Consequently, we set \(=0.01\) as the default. Specifically, let us redefine the \(R\)-independent clipping function:

\[_{}(_{i}):=1/(||_{i}||+).\] (4.1)

With this clipping, we can design automatic DP optimizers similar to (1.1):

\[(\{_{i}\}_{i=1}^{B})=_{i}_{t,i}}{||_{t,i}||+}+ (0,)}_{}_{t}}\] (4.2)

Clearly, the new private gradient \(}_{t}\) from our automatic clipping is \(R\)-independent, in contrast to the one used in (1.1). A concrete example (in the case of \(=0\)) that is comparable to (3.2) will be

\[R_{}:_{t+1}=_{t}- _{i}}{_{t}}/}{_{t}}+(0,)\] (4.3)

Leveraging the private gradient \(}_{t}\) in (4.2), we can train DP neural networks without tuning DP-specific hyperparamters \(R\) and \(\), as demonstrated in Algorithm 1.

``` Parameters: initial weights \(_{0}\), learning rate \(_{t}\), sampling probability \(p\), number of iterations \(T\).
1: Compute \(\) such that \(_{}(,,p,T)\) from any privacy accountant.
2:for iteration \(t=1,,T\)do
3: Sample a batch \(B_{t}\) by including each data point i.i.d. with probability \(p\)
4: Apply automatic clipping to per-sample gradients \(\{_{i}\}_{i B_{t}}\): \(}_{i}=_{i}/(\|_{i}\|_{2}+0.01)\).
5: Add Gaussian noise to the sum of clipped gradients: \(}=_{i}}_{i}+(0,)\).
6: Update \(_{t}\) by any optimizer on the private gradient \(}\) with learning rate \(_{t}\). ```

**Algorithm 1** Automatic Deep Learning with DP

Figure 3: Gradient (scalar) at each \(\).

We will elaborate two distinct reasons in the next sub-sections for the following statement:

|}  DP Optimizer\({}_{} R\)-dependent DP Optimizer\({}_{} R\)-independent DP Optimizer\({}_{}\) \\   which expunges the DP hyperparameters, only leaving us the regular hyperparameters such as learning rate, weight decay, etc. The significant save in the tuning effort is illustrated in Figure 4.

### Non-adaptive optimizer couples clipping threshold with learning rate

With \(R\)-dependent automatic clipping, DP-SGD becomes

\[_{t+1}=_{t}-_{i}_{t,i} _{t,i}||+}+ R(0,)=_{t}- R }_{t}.\]

We can view \(_{} R\) as a whole: increasing \(R\) has the same effect as increasing \(\), which explains the diagonal pattern in Figure 1(lower plot) where DP-SGD\({}_{}\) is applied with small clipping threshold. We extend to general non-adaptive optimizers in Theorem 15.

**Theorem 1**.: _Non-adaptive \(R\)-dependent automatic DP optimizers (including SGD, Heavyball and NAG), with learning rate \(\) and weight decay \(\), is equivalent to \(R\)-independent automatic DP optimizers, with learning rate \( R\) and weight decay \(/R\)._

### Adaptive optimizer can be insensitive to clipping threshold

Adaptive automatic DP optimizers are different than the non-adaptive ones, as the clipping threshold cancels out instead of being coupled with learning rate. To see this, we scrutinize DP-Adam\({}_{}\) (which is similar to DP-Adam\({}_{}\)) in Figure 1(upper plot), where columns to the left are almost identical. Further evidence is observed in [50, Table 5] that shrinking \(R\) has zero effect on LAMB. We now give a simple explanation using AdaGrad :

\[_{t+1}=_{t}-_{t}}{_{} ^{2}}}\]

where \(_{t}=_{i}_{t,i}\) is the gradient sum. In \(R\)-dependent DP-AdaGrad\({}_{}\), the private gradient is \(R}_{t}\) in place of the standard gradient sum \(_{t}\):

\[_{t+1}=_{t}-}_{t}}{_{  t}}_{}^{2}}}=_{t}-}_{t}}{}_{}^{2}}}.\]

We generalize to other adaptive optimizers in Theorem 2 and to the per-layer clipping style in Appendix B.3.

**Theorem 2**.: _Adaptive \(R\)-dependent automatic DP optimizers (e.g. AdaGrad, AdaDelta, AdaMax/Adam, NAdam, RAdam, LARS, LAMB), with learning rate \(\) and weight decay \(\) is equivalent to \(R\)-independent automatic DP optimizers with learning rate \(\) and weight decay \(/R\). With decoupled weight decay, \(R\)-dependent automatic DP-AdamW is equivalent to \(R\)-independent automatic DP-AdamW with the same \(\) and \(\)._

Figure 4: Test accuracy of RoBERTa-base by different clipping thresholds \(R\) and learning rates \(\). This is trained with DP-Adam (Abadi and AUTO-S) on SST2 (left, 3 epochs), QNLI (middle, 1 epoch), and MNLI (right, 1 epoch), under \(=3\). Notice by only searching along \(\), instead of over \((R,)\), we can save the cost of hyperparameter tuning by \(5\).

### Automatic clipping is equally private and maximizes utility

In Theorem 3 (proved in Appendix A), we show that the new private gradient \(}_{t}\) in (4.2) has the same level of privacy guarantee as the existing one in (1.1), since the global sensitivity remains the same (see Figure 5). We note that as long as \(>0\), the magnitude information of per-sample gradients is preserved by AUTO-S, in the sense that \(\|_{i}\|>\|_{j}\|\|C_{i}_{i}\|>\|C_{j} {g}_{j}\|\), whereas this can be violated in both the AUTO-V and Abadi's clipping (as depicted by the flat curve in Figure 5 when \(\|_{i}\|>1\)).

Additionally, note that when \(\) is small, almost all data points "max out" the signal relative to the amount of noise we add. To say it differently, for the same amount of noise, AUTO-S with small \(\) allows more signal to be pushed through a differentially private channel. Towards the end of the training, i.e., at the limit when \(\|_{i}\| 0\) for all \(i\), then we have \(_{i}}{\|_{i}\|+}_{i}_ {i}\). In words, the clipped gradients become closer to the standard SGD, thus do not suffer from the instability of AUTO-V.

**Theorem 3**.: _Under the noise multiplier \(\), number of iterations \(T\), subsampling probability \(B/n\), DP optimizers using AUTO-V or AUTO-S clipping satisfy \((_{}(,,B/n,T),)\)-DP, where \(_{}\) is any valid privacy accountant for DP-SGD under Abadi's clipping._

## 5 Convergence analysis of DP-SGD with automatic clipping

### Convergence theory of DP-SGD to stationary points

We highlight that automatic clipping can be more amenable to analysis than Abadi's clipping in , since we no longer need to decide whether each per-sample gradient is clipped.

To analyze the convergence of automatic DP-SGD (4.2) in the non-convex setting, we follow the standard assumptions in the SGD literature , including a symmetry assumption on the gradient noise, which is empirically verified in [14, Figure 3] and commonly used in the standard non-DP literature . We refer the curious readers to Appendix E.5 for details.

**Assumption 5.1** (Lower bound of loss).: For all \(\) and some constant \(_{*}\), we have \(()_{*}\).

**Assumption 5.2** (Smoothness).: Let \(()\) denote the gradient of the objective \(()\). Then \(,\), there is an non-negative constant \(L\) such that

\[()-()+()^{}(- )\|-\|^{2}.\] (5.1)

**Assumption 5.3** (Gradient noise).: The per-sample gradient noise \(}_{t,i}-_{t}\) is i.i.d. from some distribution such that

\[(}_{t,i}-_{t})=0,\|}_{t,i} -_{t}\|^{2}^{2},\]

and \(}_{t,i}\) is centrally symmetric about \(_{t}\) in distribution: \(}_{t,i}-_{t}}{=}_{t}-}_{t,i}\).

We show in Theorem 4 that DP-SGD with AUTO-S clipping allows the true gradient norm to converge to zero, though the clipped gradient may still be biased, but not so with AUTO-V clipping.

**Theorem 4**.: _Under Assumption 5.1, 5.2, 5.3, running DP-SGD with automatic clipping for \(T\) iterations and setting the learning rate \( 1/\) give6_

\[_{0 t T}(\|_{t}\|)(}_{0}-_{*})L(1+d}{B^ {2}})};,):=_{r>0}+( ;r,,).\] (5.2)_Here \(\) represents the first argument of \(\), and \(\) is increasing and positive. As \(T\), we have \(_{t}(\|_{t}\|)=O(T^{-1/4})\) for AUTO-S, the same rate as the standard SGD given in Theorem 9._

**Remark 5.4**.: We show in Theorem 6 and in Figure 6 that the upper bound (5.2) has \(\) for AUTO-V (\(=0\)), and \(\) only reduces to zero for AUTO-S (\(>0\)). We provide real data evidence in Figure 14 that strictly positive \(\) reduces the gradient norm significantly.

### Analysis of factors affecting the convergence

We now analyze the many factors that affect the convergence in Theorem 4, from a unified viewpoint of both the convergence and the privacy.

We start with the stability constant \(\) and the learning rate \(_{t}\), both only affect the convergence not the privacy. We empirically observe in Figure 8 that small \(\) benefits the convergence at initial iterations (when the privacy guarantee is strong) but larger \(\) converges faster asymptotically. For \(_{t}\), the optimal is in fact the miminizer of the hyperbola in (C.5), that is unique and tunable.

Next, we focus on the hyperparameters that affect both convergence and privacy: the batch size \(B\), the noise multiplier \(\), and the number of iterations \(T\). These hyperparameters have to be considered along the privacy-accuracy tradeoff, not just from a convergence perspective.

Recall that given a fixed privacy budget \((,)\), we rely on modern privacy accountant for computing the appropriate combinations of parameter \(,T,B\). The exact expression of the bound as a function of \((,)\) is somewhat messy. For this reason, we illustrate our analysis in terms of the surrogate parameter \(\) for \(\)-GDP , which implies \((,)\)-DP with \(=^{2}+)\).  showed that DP-SGD's privacy guarantee asymptotically converges to \(\)-GDP (as \(T\)) with \(=}-1)}\). We can alternatively leverage \(\)-tCDP  for similar conclusions, using \(\) in place of \(^{2}\) in (5.3).

**Theorem 5**.: _Under Assumption 5.1, 5.2, 5.3, fixing the asymptotic \((,)\)-GDP parameter, running DP-SGD with automatic clipping for \(T\) iterations and setting the learning rate \( 1/\) give_

\[_{0 t T}(\|_{t}\|)(4_{0}-_{*})L(+n^{2}}+O T})};,)\] (5.3)

To show that our analysis matches the training behaviors observed in SOTA empirical work , we minimize the first argument of \(\) in (5.3), denoted as \(X(B,T,,d,L,_{0})\).

1. **[Train longer with larger noise]** Fixing the expected batch size \(B\), we see that \(X\) is decreasing in \(T\). Hence larger \(T\) and consequently larger \(\) are preferred.
2. **[Larger batch size helps]** Fixing number of iterations \(T\) or epochs \(E=BT/n\), we see that \(X\) is decreasing in \(B\). Hence larger \(B\) and consequently larger \(\) are preferred.
3. **[Pretraining is critical]** Pretraining can boost the DP accuracy through a much smaller initial loss \(_{0}\) and from a smooth (small \(L\)) and flat (small \(\), c.f. Figure 8(left)) initialization.
4. **[Learning rate needs tuning]** The optimal learning rate by minimizing (C.5) is \(_{0}-_{*})^{2}n^{2}}{L(^{2}n^{2}+dT)}}\). This indicates that one should use larger learning rate for smaller model \(d\), weaker privacy (larger \(\) or small \(\)), or smaller iteration budget \(T\).

Figure 6: Left: DP-SGD with AUTO-V clipping. Middle: DP-SGD with AUTO-S clipping. Right: Log-log plot of convergence rate in comparison to standard SGD. Here \(=25,=0.01\), and the \(O(1/)\) term is set to 10 for DP-SGD and to 2 for standard SGD.

Experiments

We evaluate our automatic DP training on image classification, sentence classification, and table-to-text generation tasks. Detailed settings including hyperparameters can be found in Appendix G.

### Image classification

For MNIST/FashionMNIST, we use the same setup as in [56; 68; 64] with a simple CNN. For CIFAR10, we use the same setup as in  with pretrained SimCLRv2 . For ImageNette, a 10-class sub-task of ImageNet , we use the same setup as in  without the learning rate decay. For CelebA , the real human face dataset, we train ResNet9  with group normalization to replace the batch normalization. Notice that CelebA contains high-resolution (178x218) images, each with 40 labels. We consider CelebA for either multi-class classification on one label, e.g. 'Smiling' and 'Male', or for multi-label/multi-task problem to learn all labels simultaneously.

In Table 1, we observe that AUTO-S clipping outperforms existing clipping in all datasets with statistical significance. Interestingly, the standard deviation from different runs is smaller for automatic DP optimizers, indicating better reproducibility and stability. We additionally experiment 40 binary classification problems on CelebA with respect to each label, and observe that the mean accuracy further improves to 91.63% at \(=8\) for AUTO-S (see Appendix J).

### Sentence classification

On five benchmark language datasets (MNLI(m/mm), QQP, QNLI, SST2), we compare our automatic DP training with re-parameterized gradient perturbation (RGP, ) and full-parameter finetuning (full, ) using RoBERTa models . These methods use the same experimental setup. For language models, our automatic training is based on the codebase of .

In Table 2 and Table 3, we note that full parameter finetuning with AUTO-S outperforms or at least matches SOTA on all tasks. We use _exactly the same_ hyperparameters as in .

   &  &  &  \\   & & & Abadiâ€™s clipping & AUTO-S clipping &  \\  & & & & (\(=\)) \\  MNIST & 4-layer CNN & \((3,1e5)\) & \(98.04 0.09\) & \(98.15 0.07\) & \(99.11 0.07\) \\  FashionMNIST & 4-layer CNN & \((3,1e5)\) & \(86.04 0.26\) & \(86.36 0.18\) & \(89.57 0.13\) \\  CIFAR10 pretrained & SimCLRv2 & \((2,1e5)\) & \(92.44 0.13\) & \(92.70 0.02\) & \(94.42 0.01\) \\  ImageNette & ResNet9 & \((8,1e4)\) & \(60.29 0.53\) & \(60.71 0.48\) & \(71.11 0.37\) \\  CelebA [Smiling] & ResNet9 & \((8,5e6)\) & \(90.75 0.11\) & \(91.08 0.08\) & \(92.61 0.20\) \\  CelebA [Male] & ResNet9 & \((8,5e6)\) & \(95.54 0.14\) & \(95.70 0.07\) & \(97.90 0.04\) \\  CelebA Multi-label & ResNet9 & \((3,5e6)\) & \(86.81 0.03\) & \(87.05 0.01\) & \(90.30 0.02\) \\  CelebA Multi-label & ResNet9 & \((8,5e6)\) & \(87.52 0.15\) & \(87.58 0.04\) & \(90.30 0.02\) \\  

Table 1: Average test accuracy and 95% confidence interval on image tasks over 5 runs.

   &  &  &  \\   & MNLI & QQP & QNLI & SST2 & MNLI & QQP & QNLI & SST2 & MNLI & QQP & QNLI & SST2 \\  RGP  & - & - & - & - & 80.5/79.6 & 85.5 & 87.2 & 91.6 & 83.6/83.2 & 89.3 & 91.3 & 92.9 \\  full  & 82.45/82.99 & 85.56 & **87.42** & 91.86 & 83.20/83.46 & 86.08 & **87.94** & 92.09 & & & \\ full AUTO-V & 81.21/82.03 & 84.72 & 86.56 & 91.86 & 82.18/82.64 & **86.23** & 87.24 & 92.09 & 85.91/86.14 & 87.34 & 91.40 & 94.49 \\ full AUTO-S & **83.22/83.21** & **85.76** & 86.61 & **91.92** & **92.32** & **83.82/83.55** & **86.58** & 87.85 & **92.43** & & \\  

Table 2: Test accuracy on language tasks with RoBERTa-base (12 blocks, 125 million parameters).

   &  &  &  \\   & MNLI & QQP & QNLI & SST2 & MNLI & QQP & QNLI & SST2 & MNLI & QQP & QNLI & SST2 \\  RGP  & - & - & - & - & 86.1/86.0 & 86.7 & 90.0 & 93.0 & - & - & - & - \\  full  & **86.43**/86.46 & 86.43 & 90.76 & 93.04 & 87.02/**87.26** & 87.47 & 91.10 & 93.81 & & & \\ full AUTO-V & 85.33/85.61 & **86.61** & 89.99 & **93.12** & 85.91/86.10 & 86.86 & 90.55 & 93.35 & 90.33/90.03 & 87.90 & 93.61 & 96.21 \\ full AUTO-S & 86.27/**86.67** & **86.76** & **91.01** & **93.92** & **87.07**/87.16** & **87.47** & **91.45** & **94.61** & & \\  

Table 3: Test accuracy on language tasks with RoBERTa-large (24 blocks, 355 million parameters).

### Table-to-text generation

We compare our automatic DP training with a variety of fine-tuning methods, for table-to-text generation task on E2E dataset , where the goal is to generate texts about different aspects of a restaurant's data. We measure the success on this task by BLEU, ROUGE-L (in Table 4), METEOR, NIST, CIDEr (extended in Table 8), with higher value meaning better model quality.

Competitive methods include low-rank adaption (LoRA), prefix-tuning (prefix), RGP, only fine-tuning the top 2 Transformer blocks (top2), and training from scratch (retrain), as were recorded in . Again, we use the _exactly the same_ hyperparameters as in . For GPT2 (124 million parameters), GPT2 medium (355 million), and GPT2 large (774 million), Table 4 shows that AUTO-S is scalable with stronger performance on larger models. Our automatic full-parameter finetuning has the best overall performance. Additionally, we highlight that AUTO-S and methods like LoRA are not mutually exclusive and can be combined to yield strong performance, since AUTO-S modifies the optimizers and LoRA modifies the architecture.

## 7 Related works

While other DP works also normalize the per-sample gradients (instead of clipping them) or use small clipping threshold (making the clipping similar to normalization), our work is very different in terms of theoretical analysis, algorithm design and experiments. In fact, the concurrent work  gives the same algorithm as AUTO-S, although its theoretical analysis and experiment design is fundamentally different from ours.  proposes to normalize the per-user (not per-sample) gradient in the federated learning setting, and analyzes the convergence in a convex, non-deep-learning setting.

On the other hand, many works apply the per-sample gradient clipping with small \(R\) for good utility . These works have led to valuable insights, but also some false or incomplete conclusions, due to the lack of rigorous theoretical analysis. For instance, since \(R\) is present in the (re-parameterized) per-sample clipping, it cannot avoid the hyperparameter tuning as the choice of \(R\) is not robust; even if a sufficiently small \(R\) is used, the clipping does not reveal the stability constant in AUTO-S, which enjoys theoretical and empirical advantages in Remark 5.4 and Section 6. We devote Appendix L to more instances (e.g. Footnote 5) and a thorough comparison.

## 8 Discussion

In this work, we propose the automatic clipping as a drop-in replacement to the standard per-example clipping for differentially private training. This is the first technique that eliminates the need to tune the clipping threshold \(R\), thus making DP deep learning as easy as regular learning. Our AUTO-S method enjoys both theoretical guarantee of convergence in non-convex problems (under various conditions), and strong empirical performance that advances DP learning on computer vision and language tasks.

We are excited about the future of automatic DP training, especially along with other working techniques, such as general optimizers (e.g. ), clipping styles (all-layer or per-layer or adaptive clipping), architecture modifications (e.g. LoRA, RGP, prefix), and data augmentation (e.g. adversarial training  and multiple augmentation ). Thus, we expect to achieve comparable results to all SOTA in a lightweight fashion.

   & DP & GPT2 & GPT2 &  &  \\   & guarantee & full & full & full & full & full & LoRA & RGP & prefix & top2 & retrain \\   & & AUTO-S & AUTO-S & AUTO-S & AUTO-V &  &  &  &  & \\   & \(=3\) & **64.180** & **63.850** & **61.340** & **61.519** & **61.519** & 58.153 & 58.482 & 47.772 & 25.920 & 15.457 \\  & \(=8\) & **64.640** & **64.220** & **63.600** & 63.189 & 63.189 & **63.389** & 58.455 & 49.263 & 26.885 & 24.247 \\  & non-DP & 66.840 & 68.500 & 69.463 & 69.463 & 69.463 & 69.682 & 68.328 & 68.845 & 65.752 & 65.731 \\    & \(=3\) & **67.857** & **67.071** & **65.872** & 65.670 & 65.670 & **65.773** & 65.560 & 58.964 & 45.365 & 35.240 \\  & \(=8\) & **68.968** & **67.533** & **67.073** & 66.429 & 66.429 & **67.525** & 65.00 & 60.730 & 46.421 & 39.951 \\  & non-DP & 70.384 & 71.458 & 71.359 & 71.359 & 71.359 & 71.709 & 68.844 & 70.805 & 68.704 & 68.751 \\  

Table 4: Test performance on E2E dataset with GPT2. Additional performance measures are included in Table 8. The best two GPT2 models for each row are marked in bold.