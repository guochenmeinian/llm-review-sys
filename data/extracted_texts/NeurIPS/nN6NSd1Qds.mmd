# UGC: Universal Graph Coarsening

Mohit Kataria\({}^{1}\)

Mohit.Kataria@scai.iitd.ac.in

&Sandeep Kumar\({}^{2,1,3}\)

ksandeep@ee.iitd.ac.in

&Jayadeva\({}^{2,1}\)

jayadeva@ee.iitd.ac.in

\({}^{1}\) Yardi School of Artificial Intelligence

\({}^{2}\)Department of Electrical Engineering

\({}^{3}\)Bharti School of Telecommunication Technology and Management

Indian Institute of Technology Delhi

###### Abstract

In the era of big data, graphs have emerged as a natural representation of intricate relationships. However, graph sizes often become unwieldy, leading to storage, computation, and analysis challenges. A crucial demand arises for methods that can effectively downsize large graphs while retaining vital insights. Graph coarsening seeks to simplify large graphs while maintaining the basic statistics of the graphs, such as spectral properties and \(\)-similarity in the coarsened graph. This ensures that downstream processes are more efficient and effective. Most published methods are suitable for homophilic datasets, limiting their universal use. We propose Universal Graph Coarsening (UGC), a framework equally suitable for homophilic and heterophilic datasets. UGC integrates node attributes and adjacency information, leveraging the dataset's heterophily factor. Results on benchmark datasets demonstrate that UGC preserves spectral similarity while coarsening. In comparison to existing methods, UGC is 4\(\) to 15\(\) faster, has lower eigen-error, and yields superior performance on downstream processing tasks even at 70% coarsening ratios.1

## 1 Introduction

Graphs have emerged as highly expressive tools to represent diverse structures and knowledge in various fields such as social networks, bio-informatics, transportation, and natural language processing . They are essential for tasks like community detection, drug discovery, route optimization, and text analysis. With the growing importance of graph-based solutions, dealing with large graphs has become a challenge. Graph Coarsening(GC), a widely used technique to simplify graphs while retaining vital information, making them more manageable for analysis . It has been applied successfully in various tasks . Preserving the structural information of the graph is crucial in graph coarsening algorithms to ensure the fidelity of the coarsened graphs. A high-quality coarsened graph retains essential features and relationships, enabling accurate results for downstream tasks. Additionally, computational efficiency is equally vital for scalability, as large-scale graphs are common in real-world applications. An efficient coarsening method should ensure that the reduction in graph size does not come at the expense of excessive computation time but existing graph coarsening methods often face trade-offs between scalability and the quality of the coarsened graph. Our method draws inspiration from hashing techniques, which provide us with advantages in terms of computational efficiency. As a result, our approach exhibits a linear time complexity, making it highly efficient even for large graphs.

Graph datasets often exhibit a blend of homophilic and heterophilic traits . Graph Coarsening(GC) has been widely explored on homophilic datasets, but, to the best of our knowledge, has never been applied to heterophilic graphs. We propose Universal Graph Coarsening \(UGC\), an approach that works well on both. Figure 2 illustrates how UGC uses a graph's adjacency matrix aswell as the node feature matrix. UGC relies on hashing, lending computational efficiency. UGC exhibits linear time complexity, enabling fast processing of large datasets. Figure 1 demonstrates the computational time gains of UGC among graph coarsening methods. UGC surpasses the fastest existing methods by about 6\(\) on the Physics dataset and 9\(\) on the Squirrel dataset. UGC enhances the performance of Graph Neural Networks (GNN) models in classification tasks, indicating its suitability for downstream processing. UGC coarsened graphs retain essential spectral properties and show low eigen error, hyperbolic error, and \(\)-similarity measure. In a nutshell, UGC is fast, universally applicable, and information-preserving.

## Key Contributions.

* We proposed a novel framework that is extremely fast compared to other existing methods for graph coarsening. It is also shown to be helpful and effective for graph-based downstream tasks.
* UGC is the first to handle heterophilic datasets for coarsening.
* UGC can retain important spectral properties, such as eigen error, hyperbolic error, and \(\)-similarity measure, which ensures the preservation of key characteristics and information of the original graph during the graph coarsening.

## 2 Background and Problem Formulation

A graph is represented using \((V,A,X)\) where \(V=\{v_{1},,v_{N}\}\) denotes set of \(N\) vertices, \(A^{N N}\) is the adjacency matrix and \(A_{ij}>0\) indicates an edge \((v_{i},v_{j})\) between nodes \(v_{i}\) and \(v_{j}\). \(X^{N}\) denotes the feature matrix where \(i^{th}\) row of \(X\) is a feature vector \(X_{i}^{}\), associated with node \(v_{i}\). The degree matrix \(D\) is a diagonal matrix, where \(D_{ii}=_{j}A_{ij}\). \(L^{N N}\) is a Laplacian matrix, \(L=D-A\), and it belongs to the set \(S_{L}=\{L^{N N}|L_{ji}=L_{ij} 0,\ \  i j;\ L_{ii}=-_{j  i}L_{ij}\}\) as defined in . The adjacency matrix \(A\) and Laplacian matrix \(L\) associated with the graph are related as follows: \(A_{ij}\) = \(-L_{ij}\) for \(i j\) and \(A_{ij}\) = 0 for \(i=j\).

Figure 1: This figure illustrates the computational time comparison among graph coarsening methods to learn a coarsened graph over ten iterations. UGC outperforms the fastest existing methods by approximately 6\(\) on the Physics dataset and 9\(\) on the Squirrel dataset.

Figure 2: This figure illustrates our framework, UGC, which has three main modules a) Generation of an augmented matrix by incorporating feature and adjacency matrices while using heterophily measure \(\), b) Generation of coarsening matrix \(\) using augmented features via Hashing, and c) Generation of coarsened graph \(_{c}\) from \(\) followed by its utilization in downstream tasks.

\[^{T}=1&1&1&1&0&0&0&0\\ 0&0&0&0&1&0&0&0\\ 0&0&0&0&0&1&1&1\]

Both \(L\) and \(A\) can represent the same graph. Hence, a graph \((V,A,X)\) can also be represented as \((L,X)\), with either representation utilized as required within the paper.

**Problem.** The objective is to reduce an input graph \((V,A,X)\) with \(N\)-nodes into a new graph \(_{c}(,,)\), with \(n\)-nodes and \(^{n}\) where \(n N\). The **G**raph **Coarsening(GC)** problem requires learning of a coarsening matrix \(^{N n}\), which is a linear mapping from \(V\). A linear mapping ensures that similar nodes in \(\) are mapped to the same super-node in \(_{c}\), s.t. \(=^{T}X\). Every non-zero entry \(_{ij}\) denotes the mapping of the \(i^{th}\) node of \(\) to the \(j^{th}\) super-node \(_{c}\). This \(\) matrix belongs to the following set:

\[=\{^{N n},_{ij} \{0,1\},\|_{i}\|=1,_{i}^{T},_{j}^{T} =0, i j,_{l},_{l}=d_{ _{l}},\|_{i}^{T}\|_{0} 1\}\] (1)

where \(d_{_{l}}\) means the number of nodes in the \(l^{th}\)-supernode. The condition \(_{i}^{T},_{j}^{T}=0\) ensures that each node of \(\) is mapped to a unique super-node. The constraint \(\|_{i}^{T}\|_{0} 1\) requires that each super-node contains at least one node. Consider the 8-node graph in Figure 2(b). Nodes 1, 2, 3, and 4 are mapped to super-node **A**, while nodes 6, 7, and 8 are mapped to super-node **C**. Hence, the coarsening matrix \(\) is given in Figure 2(a). The goal is to learn this \(\) matrix such that \(\) and \(_{c}\) are similar. The \(-\)similarity is a widely used similarity measure for graphs with node features, as it entails comparing the Laplacian norms of the respective feature matrices. The graphs \((V,A,X)\) and \(_{c}(,,)\) are said to be \(\)-similar if there exist \( 0\) such that

\[(1-)\|X\|_{L}\|\|_{L_{c}}(1+)\|X\|_{L}\] (2)

where \(L\) and \(L_{c}\) are the Laplacian matrices of \(\) and \(_{c}\) respectively, \(\|X\|_{L}=LX)}\) and \(\|\|_{L_{c}}=^{T}L_{c})}\). The quantity \(tr(X^{T}LX)=-_{i,j}L_{ij}\|x_{i}-x_{j}\|^{2}\) is known as Dirichlet Energy (DE), which is employed to measure the smoothness of node features where \(x_{i}\) and \(x_{j}\) are the node features of nodes \(i\) and \(j\).

_Goal: Given a graph \((V,A,X)\) of \(N\) nodes, construct a coarsened graph \(_{c}(,,)\) with \(n\) nodes, such that they are \(-\)similar._

**Homophilic and Heterophilic datasets.** Graph datasets may demonstrate homophily and heterophily properties . Homophily refers to the tendency of nodes to be connected to other nodes of the same class or type, while heterophily signifies the tendency of nodes to connect with nodes of different classes. A heterophily factor \(0 1\) may be used to denote the degree of heterophily. \(\) is calculated as the fraction of edges between nodes of different classes to the total number of edges. A strongly heterophilic graph (\( 1\)) has the most edges between nodes of different classes, suggesting a diverse network with mixed interactions. Conversely, weak heterophily or strong homophily (\( 0\)) occurs in networks where nodes predominantly connect with others of the same class.

**Locality Sensitive Hashing.** Locality Sensitive Hashing (LSH) is a linear time, efficient similarity search technique for high dimensional data . It maps high-dimensional vectors to lower dimensions while ensuring that similar vectors collide with high probability. LSH uses a family of hash functions to map vectors to buckets, enabling fast retrieval and similarity search. It has found applications in image retrieval , data mining , and similarity search algorithms . LSH family is defined as

**Definition 2.1**: _Let \(d\) be a distance measure, and let \(d_{1}<d_{2}\) be two distances. A family of functions \(F\) is said to be \((d_{1},d_{2},p_{1},p_{2})-\)sensitive if for every \(f F\) the following two conditions hold:_

1. _If_ \(d(x,y) d_{1}\) _then probability_ \([f(x)=f(y)] p_{1}\)__
2. _If_ \(d(x,y) d_{2}\) _then probability_ \([f(x)=f(y)] p_{2}\)__

Figure 3: Graph coarsening toy example, a) Coarsening matrix, b) Original graph \(\) and corresponding coarsened graph \(_{c}\)

UGC uses LSH with a set of random projectors to map similar nodes to the same super-node. The projection is computed as \(>+b_{i}}{r}\), where \(w_{i}\) is a randomly selected \(d-\)dimensional projector vector from a \(p-\)stable distribution (see Appendix A); \(x\) represents the \(d-\)dimensional data sample, and \(r\) is the width of each quantization bin.

**Related Works.** The literature is replete with graph reduction methods and their applications; they may be broadly classified into three categories:

1. _Optimization and Heuristics:_ Loukas  proposed advanced spectral graph coarsening algorithms based on local variation to preserve the original graph's spectral properties. Two variants, _viz._ edge-based (LVE) and neighborhood-based (LVN), select contraction sets with small local variation in each stage but have limitations in achieving arbitrary coarsening levels. Heavy edge matching (HE) [9; 27], determines the contraction family by computing a maximum-weight matching based on the weight of each contraction set. The Algebraic Distance method proposed in [27; 28] calculates the weight of each candidate set using an algebraic distance measure. The affinity method , inspired by algebraic distance, uses the vertex proximity heuristic. The Kron reduction method  was originally proposed for electrical networks but is too slow for large networks. FGC [14; 31] considers both the graph structure and the node attributes as the input and, alternatively, optimizes \(\). The above-mentioned methods are computationally and memory-intensive.
2. _GNN based:_ GCond  and SFGC  are GNN-based graph condensation methods. These works proposed the online gradient matching schema between the synthesized small-scale graph and the large-scale graph. However, these methods have significant issues regarding computational time and generalization ability. First, they require training GNN models on the original graph to get a smaller graph as they imitate the GNN training trajectory on the original graph through gradient matching. Due to this, these methods are extremely computationally demanding and may not be suitable for the scalability of GNN models. However, these methods can be beneficial for other tasks, like solving storage and visualization issues. Second, the condensed graph obtained using GCond  shows poor generalization ability across different GNN models  because different GNN models vary in their convolution operations along graph structures.
3. _Scaling GNN viz. Graph Coarsening:_ SCAL  and GOREN  proposed to enhance the scalability for training GNN models using graph coarsening. It is worth noting that SCAL and GOREN are not standalone graph coarsening techniques. SCAL uses Louka's  work to coarsen the graph, then trains GNN models using the coarsened graph. While GOREN trying to improve the coarsening quality of existing methods.

## 3 The Proposed Framework: Universal Graph Coarsening (UGC)

The proposed UGC framework comprises three main components: (a) First, obtaining an augmented feature matrix \(F\) containing both node feature and structural information, (b) Secondly, using locality-sensitive hashing to derive the coarsening matrix \(\), (c) and Finally, obtaining the coarsened graph adjacency matrix \(A_{c}\) and coarsened features \(F_{c}\).

**Construction of Augmented Feature Matrix \(F\).** In order to create a universal GC framework suitable for all, it is important to consider features at both i) the node level, i.e., features, and ii) the structure-level, i.e., adjacency matrix, together. In this regard, we create an augmented feature matrix \(F\), where each node's feature vector \(X_{i}\) is augmented with its binary adjacency vector \(A_{i}\). We use the heterophily factor \(\) discussed in Section 2 to balance the emphasis between node-level and structure-level information. The augmented feature vector for node \(v_{i}\) is given using \(F_{i}=(1-) X_{i} A_{i}}\) where \(\) and \(\) denote the concatenation and dot product operations, respectively. Figure 11 in Appendix K illustrates a toy example of the process involved in calculating the augmented feature vector. While larger graphs may result in long vectors, efficient implementations and sparse tensor methods may alleviate this hurdle. A motivating example demonstrating the need for augmented features while doing GC is discussed in Appendix K (Figure 12).

**Construction of Coarsening Matrix \(\).** Let \(F_{i}^{d}\) represent the augmented feature vector of node \(v_{i}\). Let \(^{d l}\) and \(b^{l}\) be the hashing matrices used in UGC, with \(l\) denoting the number of hash functions. The hash indices generated by \(k^{th}\) hash/projector function for \(i^{th}\) node is given as

\[h_{i}^{k}=*(_{k} F_{i}+b_{k})\] (3)where \(r\) is a hyperparameter called bin-width. The hash index that has the maximum occurrence among the hash indices generated by all \(l\) hash functions is the hash value assigned to the graph node \(v_{i}\). Hence, the hash value for node \(v_{i}\) is given by

\[h_{i}=maxOccured\{h_{i}^{1},h_{i}^{2}....h_{i}^{l}\}\] (4)

\(r\) controls the size of the coarsened graph \(_{c}\); empirically, we find that increasing \(r\) means reducing the size of the coarsened graph \(_{c}\). All nodes assigned with the same hash value map to the same super-node in \(_{c}\). The reader may like to refer to Algorithm 1 for the steps in UGC. The element of coarsening matrix, \(_{ij}\) equals 1 if vertex \(v_{i}\) is associated with super-node \(}\). Crucially, every node is assigned a unique \(h_{i}\) value, ensuring an exclusive mapping to a super-node. This constraint aligns with the formulation of super-node and guarantees at least one node per super-node. Thus, each row of \(\) contains only one non-zero entry, leading to orthogonal columns. This matrix \(\) satisfies the conditions specified in Equation 1.

```
0: Input \((V,A,X)\), \(l\) Number of Projectors, \(r binWidth\)
1:\(=,y_{u}=v_{1})|}{|}\); \(\) is heterophily factor, \(y_{i}^{N}\) is node labels, \(E\) denotes edge list
2:\(F=\{(1-) X A\}\)
3:\((.)\); \(^{d l}\) denotes \(l\) projectors, and \(\) is a p-stable distribution
4:\(b(.)\); \(b^{l}\) denotes sampled bias
5:\(= W b}{r} ;\)\(^{N l}\)
6:\(_{i}\) maxOccurence(\(_{i};i\{1,2,3,...,N\}\)), \(^{N}\)
7:for every node \(v\) in V do
8:\(_{i},[v] 1\)
9:\(A_{c}(i,j)_{(u^{-1}(_{i}),v^{-1}( _{j}))}A_{uv}\), \( i,j\{1,2,...,n\}\)
10:\(F_{c}(i)(_{i})|}_{u^{-1}( _{i})}F_{u}\), \( i\{1,2,...,n\}\)
11: return \(_{c}(V_{c},A_{c},F_{c}),\) ```

**Algorithm 1** UGC: Universal Graph Coarsening

**Construction of Coarsened Graph \(_{c}\).** Let \(_{c}(,,)\) represent the coarsened graph that is to be built. A pair of super-nodes, say \(_{i}\) and \(}\), in the coarsened graph \(_{c}\) are connected, if any of the nodes, say \(u^{-1}(_{i})\) has an edge to any of the nodes, say \(v^{-1}(})\) in the original graph, i.e., \(\)\(u^{-1}(}),v^{-1}(})\) such that \(A_{uv} 0\). The coarsened graph \(_{c}\) is weighted, and the weight assigned to the edge between nodes \(_{i}\) and \(}\), is given by \(_{ij}=_{(u^{-1}(_{i}),v^{-1}( _{j}))}A_{uv}\) where \(A_{uv}\) refers to the element \((u,v)\) in the adjacency matrix \(A\) of graph \(\). The features of super-nodes are taken to be the average of the features of the nodes in the super-node, i.e., \(_{i}=(_{i})|}_{u^{-1}( _{i})}F_{u}\). The super-node's label is chosen as the class that has the most instances. From the \(\) matrix, we can directly calculate the adjacency \(\) matrix of \(_{c}\) using \(=^{T}A\) which is the same as \(_{ij}\). \(\) can also be obtained using \(=^{T}F\) where \(\) is the coarsening matrix discussed earlier. Because each super-edge combines multiple edges from the original graph, the number of edges in the coarse graph is also much less than \(m\). In general, the adjacency matrix \(\) has a substantially smaller number of non-zero elements than \(A\). The pseudocode for UGC is listed in Algorithm 1. UGC gives a coarsened graph \(_{c}(L_{c},)\) which also satisfies \(-\)similarity (\( 0\)).

**Theorem 3.1**: _The input graph \((L,F)\) and the coarsened graph \(_{c}(L_{c},)\) obtained using the proposed UGC algorithm are \(\)-similar with \( 0\), i.e.,_

\[(1-)\|F\|_{L}\|\|_{L_{c}}(1+)\|F\|_{L}\] (5)

_where \(L\) and \(L_{c}\) are the laplacian matrices of \(\) and \(_{c}\) respectively._

_Proof:_ The proof is deferred in Appendix I.

**Universal Graph Coarsening with feature re-learning for Bounded \(\)-similarity.** The coarsened graph \(_{c}\) generated through UGC exhibits a high degree of similarity, within the range of \(\), to the original graph \(\). It has also been empirically demonstrated that this coarsened representation performs exceptionally well across various downstream tasks. Nonetheless, to achieve a tighter \(\)-bound, where \(( 1)\), a potential step involves introducing modifications to the feature learning procedure of the super-nodes \(_{c}\).

It is important to note that the \(\)-similarity measure introduced in  does not incorporate features. Instead, it relies on the eigenvector of the laplacian matrix to compute similarity, which limits its ability to capture the characteristics of the associated features along with the graph structure. Once we get the loading matrix \(\) using UGC as discussed in Section 3 we used \(_{i}=(_{i})|}_{u^{-1}( _{i})}F_{u}\) to learn the feature-vectors of super-nodes. Using \(_{i}\) we can satisfy the Theorem 3.1. However, to give a strict bound on the \(\) similarity we updated \(\) to \(\) by minimizing the term

\[_{}f()=(^{T}^{T}L)+\|-F\|_{F }^{2}\] (6)

We aim to enforce the Dirichlet smoothness condition in super-node features using Equation 6. The above equation is a convex optimization problem from which we get a closed-form solution by putting the gradient w.r.t to \(\) equal to zero. Update rule for \(\) can be derived as:

\[2^{T}L+^{T}( -F)=0=(^{T}L +^{T})^{-1}^{T}F\]

We now have re-learnt features for super-nodes, please refer to Algorithm 2 in Appendix B which we call as **UGC-FL** i,e UGC with feature learning. Using \(\) we can give a more strict bound on \(-\)similarity.

**Theorem 3.2**: _The original graph \((L,F)\) and coarsened graph \(_{c}(L_{c},)\) obtained using the proposed UGC-FL algorithm are \(\) similar with \(0< 1\), i.e,_

\[(1-)||F||_{L}||||_{L_{c}}(1+)||F||_{L}\] (7)

_where \(L\) and \(L_{c}\) are the laplacian matrices of \(\) and \(_{c}\) respectively, and \(F\) and \(\) are features matrix associated with original and coarsened graphs, respectively._

_Proof:_ The proof is deferred in Appendix J.

_Novelty:_ The majority of current techniques involve coarsening the original graph and simultaneously learning the graph structure, which makes them computationally intensive. The UGC decouples this process, making it incredibly fast, first learning the coarsening mapping \(C\) by capturing the similarity of features through hashing and then using the adjacency matrix only once as \(A_{c}=C^{T}AC\) for learning the coarsened graph's structure all at once. The UGC is easy to use, extremely fast, and produces better results for tasks requiring downstream processing.

**Time Complexity Analysis of UGC.** We have three phases for our framework. For the first phase, we can see Algorithm 1, Line 5 is driving the complexity of the algorithm, where we multiply two \(F^{N d}\) and \(^{d l}\) matrices, which results in \((Nld)\). In the second pass, the super-nodes for the coarsened graphs are constructed with the help of the accumulation of nodes in the bins. The main contribution of UGC is up to these two phases i.e., Line 1-8. Till now, time-complexity is \((Nld)(NC)\) where \(C\) is a constant.

In the third phase, Lines 10-11, we calculate the adjacency and features of the super-nodes of the coarsened graph \(_{c}\). The computational cost of this operation is \((m)\), where \(m\) is the number of edges in the original graph \(\), and this is a one-time step. Indeed, the overall time complexity of all three phases combined is \((N+m)\) where \(m\) is the number of edges. However, it's important to note that the primary contribution of UGC lies in the process of finding the coarsening matrix, whose time complexity is \((N)\). We have compared the computational time for obtaining the coarsening matrix via UGC with the existing methods.

## 4 Experiments

In this section, we conduct extensive experiments to evaluate the proposed UGC against the existing graph coarsening algorithms. The conducted experiments establish the performance of UGC concerning i) computational efficiency, ii) preservation of spectral properties, and iii) potential extensions of the coarsened graph \(_{c}\) into real-world applications.

We compare our proposed algorithm with the following coarsening algorithms, as discussed in Section 2. UGC (feat) represents a specific scenario within our framework, wherein only the feature values are considered for hashing, thereby obtaining the mapping of super-nodes. To comprehend the significance of incorporating the adjacency vector, we have added the results for both UGC (feat) and UGC (augmented feat).

**Datasets.** Our experiments cover widely adopted benchmarks, including _Cora,Citeseer, Pubmed_, _CS, Physics_, _DBLP_. Additionally, UGC effectively coarsens large datasets like _Flickr, Yelp_, and _Reddit_, previously challenging for existing techniques. We also present datasets like _Squirrel, Chameleon, Texas, Film, Wisconsin_, characterized by dominant heterophilic factors. Table 6 in Appendix G provides comprehensive dataset details.

**Run-Time Analysis.** UGC's main contribution lies in its computational efficiency. The time required to compute the coarsening matrix \(\) is summarized in Table 1. By referring to this Table, it becomes evident that UGC exhibits a remarkable advantage, surpassing all existing methods across diverse datasets. Our model outperforms existing methods by a substantial margin. While other methods struggle at large datasets like _Physics(34.4k nodes)_, UGC is able to coarsen down massive datasets like _Yelp(716.8k nodes)_, which was previously not possible. It should be emphasized that the time taken by UGC on the _Reddit(232.9k nodes)_ dataset, which has \(~{}7\) the number of nodes compared to _Physics_ is one-third the time taken by the fastest existing methods on _Physics_ dataset.

**Spectral Properties Preservation.**

1. **Relative Eigen Error (REE):**, REE used in  gives the means to quantify the measure of the eigen properties of the original graph \(\) that are preserved in coarsened graph \(_{c}\). **Definition 4.1**: _REE is defined as follows: \(REE(L,L_{c},k)=_{i=1}^{k}}- _{i}|}{_{i}}\) where \(_{i}\) and \(}\) are top \(k\) eigenvalues of original graph Laplacian (\(L\)) and coarsened graph Laplacian (\(L_{c}\)) matrix, respectively._
2. **Hyperbolic error (HE):** HE  indicates the structural similarity between \(\) and \(_{c}\) with the help of a lifted matrix along with the feature matrix \(X\) of the original graph. **Definition 4.2**: _HE is defined as follows: \(HE=arccosh(})||_{F}^{2}||X||_{F}^{2}}{2trace(X^{T}X)trace (X^{T}L_{}X)}+1)\) where \(L\) is the Laplacian matrix and \(X^{N d}\) is the feature matrix of the original input graph, \(L_{}\) is the lifted Laplacian matrix defined in  as \(L_{}=L_{c}^{T}\) where \(^{N n}\) is the coarsening matrix and \(L_{c}\) is the Laplacian of \(_{c}\)._

Eigenvalue preservation can be seen in Figure 4 where we have plotted the top 100 eigenvalues of \(\) and of \(_{c}\). We can see that the spectral property is preserved even for 70% coarsened graphs. This approximation is more accurate for a lower coarsening ratio, i.e., the smaller the graph, the bigger the REE. The REE for all approaches across all datasets is shown in Table 2 for a fixed 50% coarsening ratio. UGC stands out by giving the best REE values in 8 out of 12 datasets. Although we also have coarsened graphs for large datasets like _Yelp and Reddit_, eigen error calculation for these datasets was out of memory, so we have used EOOM while other methods fail to find even the coarsened

 
**Data/Method** & Cora & Cite. & CS & PubMed & DBLP & Physics & Flickr & Reddit & Yelp & Squirrel & Cham. & Cor. & Texas & Film \\  Var. Negib. & 6.64 & 8.72 & 23.43 & 24.38 & 22.79 & 58.0 & OOM & OOM & OOM & 33.26 & 12.2 & 1.34 & 0.63 & 27.67 \\ Var. Edges & 5.34 & 7.37 & 16.72 & 18.69 & 20.59 & 67.16 & OOM & OOM & OOM & 46.45 & 12.65 & 1.31 & 0.76 & 26.6 \\ Var. Clg. & 7.29 & 9.8 & 24.59 & 61.85 & 38.31 & 69.80 & OOM & OOM & OOM & 28.91 & 10.55 & 1.56 & 1.14 & 33.04 \\ Heavy Edge & 0.7 & 1.41 & 7.50 & 12.03 & 8.39 & 39.77 & OOM & OOM & OOM & 18.08 & 5.41 & 1.62 & 1.17 & 11.79 \\ Alg Dist & 0.93 & 1.55 & 9.63 & 10.48 & 9.67 & 46.42 & OOM & OOM & OOM & 18.03 & 5.24 & 1.85 & 0.81 & 12.65 \\ Affinity GS & \(2.36\) & \(2.53\) & \(169.05\) & \(168.3\) & \(110.9\) & \(924.7\) & OOM & OOM & OOM & 20.00 & 5.83 & 1.81 & 1.24 & 20.65 \\ Kron & 0.63 & 1.37 & 8.72 & 5.81 & 7.09 & 34.53 & OOM & OOM & OOM & 20.62 & 7.25 & 1.73 & 0.97 & 12.29 \\ UGC & **0.41** & **0.71** & **3.1** & **1.62** & **1.86** & **6.4** & **8.9** & **16.17** & **170.91** & **2.14** & **0.49** & **0.04** & **0.03** & **1.38** \\  

Table 1: Summary of run-time in seconds averaged over 5 runs to reduce the graph to 50%.

Figure 4: Top 100 eigenvalues of the original graph \(\) and coarsened graph \(_{c}\) at different coarsening ratios: 30%, 50%, and 70%.

graph, hence the term OOM. Figure 5 illustrates the trends for eigen error, hyperbolic error and GCN accuracy for different methods as the coarsening ratio is altered.

**LSH Similarity and \(\)-Bounded Results** The LSH family used in our framework is based on p-stable distributions \(\) see Appendix A. This ensures that the probability of two nodes going to the same super-node is directly related to the distance between their features (augmented features \(F\) for UGC).

**Theorem 4.1**: _As given in , the probability that two nodes \(v\) and \(u\) will collide and go to a super-node under a hash function drawn uniformly at random from a 2-stable distribution is inversely proportional to \(c=||v-u||_{2}\) and it is represented by \(p(c)=Pr_{w,b}[h_{w,b}(v)=h_{w,b}(u)]=_{0}^{r}f_{p} ()(1-)dt\)._

In our experiments, we empirically validated the Theorem 4.1. We examined if the feature distance between any node pair was below a specific threshold, and then using the coarsening matrix \(\) given by UGC, we verified if they shared the same super-node or not. Our evaluation involved counting successful matches, where nodes belonged to the same super-node, and failures, where they did not. We subsequently calculated a probability measure based on these counts. Figure 5(a) and 5(b) plot this probabilistic function for two datasets, namely _Cora_ and _Citeseer_ as a function of distance between two nodes. Re-visiting the Definition 2.1 for the _Cora_ dataset, we denote our LSH family as \((1,3,1,0.20)\). Suppose \(d\) denotes the distance between the nodes \(\{u,v\}\). In the notation \((1,3,1,0.20)\), this implies that if \(d 1\), there is a 100% probability that \(u,v\) will be grouped into the same super-node. Conversely, if \(d>3\), the probability of \(\{u,v\}\) being grouped into the same super-node is 20%. Figure 5(c) plots different values of \(\) at different coarsening ratios. We used Equation 6 for updating the augmented feature matrix \(F\) given by UGC and as mentioned, we got \( 1\) similarity guarantees for the coarsened graph. Hence proving Theorem 3.2.

**Scalable Training of Graph Neural Networks.** Graph neural networks (GNNs), tailored for non-Euclidean data , have shown promise in various applications . However, scalability remains a challenge. Building on , we investigate how our graph coarsening approach can enhance GNN scalability for training, bridging the gap between GNNs and efficient processing of large-scale data.

_GNN parameter details._ We employed a single hidden layer GCN model with standard hyperparameters values  see Appendix H for the node-classification task. Coarsened graph \(_{c}\) is used to train the GCN model, and all the predictions are made on test data from the original graph. The relation between coarsening ratio and accuracy is evident from Table 9 in Appendix H. Specifically, as we progressively coarsen the graph, a slight decrease in accuracy values becomes noticeable. Hence, there will always be a trade-off when it comes to the

  
**DataMethod** & Cora & Cite & CS & PubMed & DRIP & Physics & Flickr & Reddit & Yelp & Squirrel & Cham. & Cor. & Texas & Film \\  Var. Neigh. & 0.121 & 0.180 & 0.248 & 0.108 & 0.117 & 0.273 & OOM & OOM & OOM & 0.871 & 0.657 & 0.501 & 0.391 & 32.87 \\ Var. Edges & 0.129 & 0.136 & 0.049 & 0.965 & 0.135 & 0.042 & OOM & OOM & OOM & 0.298 & 0.597 & 0.485 & 0.489 & 21.8 \\ Var. Clin. & 0.085 & 0.064 & **0.026** & 1.208 & 0.082 & 0.039 & OOM & OOM & OOM & OOM & 0.369 & 0.456 & 0.550 & 0.463 & 22.95 \\ Hae. Hagler & 0.071 & 0.043 & 0.046 & 0.834 & 0.086 & 0.031 & OOM & OOM & OOM & 0.256 & 0.333 & 0.554 & 0.464 & 5.69 \\ Alg. Dist. & 0.107 & 0.111 & 0.087 & 0.403 & 0.047 & 0.117 & OOM & OOM & OOM & OOM & 0.245 & 0.413 & 0.552 & 0.465 & 5.71 \\ Aff. GS & 0.095 & 0.057 & 0.063 & 0.063 & 0.073 & 0.052 & OOM & OOM & OOM & **0.226** & 0.413 & 0.569 & 0.489 & 5.56 \\ Kron & **0.069** & **0.028** & 0.056 & 0.378 & 0.060 & 0.064 & OOM & OOM & OOM & 0.246 & 0.413 & 0.554 & 0.491 & 6.12 \\ UGC(Fea.) & 0.224 & 0.340 & 0.208 & 0.179 & 0.145 & **0.016** & **0.014** & EOOM & EOOM & 1.38 & 7.594 & 0.420 & 0.534 & 9.83 \\ UGC(Fea.) & 0.130 & 0.070 & 0.050 & **0.004** & **0.004** & 0.018 & 0.0153 & EOOM & EOOM & 0.546 & **0.409** & **0.215** & **0.204** & **0.075** \\   

Table 2: This table illustrates Relative Eigen Error at 50% coarsening ratio. UGC stands out by giving the best REE values in 8 out of 12 datasets.

Figure 5: This figure compares graph coarsening methods in terms of REE, HE, and GCN accuracy on the Pubmed dataset.

coarsening ratio and quality of the reduced graph. To emphasize the contribution of UGC in terms of both computational time and node-classification accuracy, we have included Figure 7.

This figure illustrates the improvements in computational time and the corresponding changes in accuracy values when compared to the currently best-performing model across various datasets. Table 4 compares the accuracy among all the approaches with all datasets when they are coarsened down by 50%. UGC demonstrated superior performance compared to existing methods in 7 out of the 9 datasets. We have used t-SNE  algorithm for visualization of predicted node labels shown in Figure 10 in Appendix H. It is evident that even with highly coarsened graph training, the GCN model can maintain its accuracy.

**UGC is Model-Agnostic.** While our initial validation utilized GCN to assess the quality of our coarsened graph \(_{c}\) our framework is not bound to any specific GNN architecture. We extended our evaluations to include other prominent graph neural network models. Results from three diverse models, namely GCN , GraphSage , GIN , and GAT , have been incorporated into our analysis. All the models were trained using 50% coarsened graph \(_{c}\). Results from Table 3 demonstrate the robustness and model-agnostic nature of UGC. Refer to Table 7 in

  
**Data/Method** & Cora & DBLP & PubMed & Physics & Squirrel & Cham. & Cor. & Texas & Film \\  Var.Neigh. & \(79.75\) & \(77.05\) & \(77.87\) & \(93.74\) & 19.67 & 20.03 & 52.49 & 34.51 & 15.67 \\ Var.Edges & \(81.57\) & **79.93** & \(78.34\) & \(93.86\) & 20.22 & 29.95 & 55.32 & 30.59 & 21.8 \\ Var.Clique & \(80.92\) & \(79.15\) & \(73.32\) & \(92.94\) & 19.54 & 31.92 & 58.8 & 33.92 & 20.35 \\ Heavy Edge & \(79.90\) & \(77.46\) & \(74.66\) & \(93.03\) & 20.36 & 33.3 & 54.67 & 29.18 & 19.16 \\ Alg. Dis. & \(79.83\) & \(74.51\) & \(74.59\) & \(93.94\) & 19.96 & 28.81 & **59.91** & 18.61 & 19.23 \\ Aff. GS & \(80.20\) & \(78.15\) & \(80.53\) & \(93.06\) & 20.00 & 27.58 & 54.06 & 21.18 & 20.34 \\ Kron & \(80.71\) & \(77.79\) & \(74.89\) & \(92.26\) & 18.03 & 29.1 & 55.02 & 31.14 & 17.41 \\ UGC(fea.) & \(\) & \(75.50\) & \(\) & 94.70 & 20.71 & 29.9 & 55.6 & 52.4 & 22.6 \\ UGC(fea+Ad) & \(\) & 75.50 & \(\) & \(\) & \(\) & \(\) & 54.7 & \(\) & \(\) \\   

Table 4: This table illustrates the accuracy of GCN model when trained with 50% coarsen graph. UGC demonstrated superior performance compared to existing methods in 7 out of the 9 datasets.

   Model/Data & Cora & Pubmed & Physics & Squirrel \\  GCN & 86.30 & 84.77 & 96.12 & 31.62 \\ GraphSage & 69.39 & 85.72 & 94.49 & 61.23 \\ GIN & 67.23 & 84.12 & 85.15 & 44.72 \\ GAT & 74.21 & 84.37 & 92.60 & 48.75 \\   

Table 3: This table demonstrates UGC’s model-agnostic nature, as it doesn’t rely on any specific GNN model.

Figure 6: a) Cora and b) Citeseer demonstrate the inverse relationship between the probability of two nodes belonging to the same super-node and the distance between them. c) plots the \(\) values (\( 1\)) for Cora, Citeseer, and CS datasets.

Figure 7: Computational and accuracy gains of UGC. In the bar plot, dashed bars represent the gain or loss in accuracy when compared to the existing best-performing method, while plain bars indicate the computational gains. All datasets are coarsened down by 50%.

Appendix H for a comprehensive analysis of node classification accuracy results for various GNN models. We believe this flexibility further enhances the applicability and utility of our proposed framework in various graph-based applications.

**Gained Performance on Heterophilic Graphs.** Existing work for GC is focused on homophilic datasets. A notable contribution of our framework is its ability to generalize to all datasets, including heterophilic datasets as well. Building upon the observations made in Table 2 and Table 4 our methods, UGC (feat) and UGC (aug. feat.), showcase notable improvements in both node classification accuracy and REE values when applied to heterophilic datasets. A comparison of these results reveals that conventional approaches demonstrate poor node-classification accuracy on heterophilic graphs. In contrast, our UGC (features) method achieves substantial accuracy enhancements, surpassing the performance of these traditional approaches. Furthermore, the true potential of our approach becomes evident with augmented features \(F\) i.e., UGC (aug. feat.). This approach exhibits remarkable accuracy gains, outperforming all other methods by a considerable margin, signifying the importance of augmented features \(F\).

## 5 Conclusion

In this paper, we present a framework **UGC** for reducing a larger graph to a smaller graph. We use hashing of augmented node features inspired by Locality Sensitive Hashing (LSH). As expected, the benefits of LSH are also reflected in the proposed coarsening algorithm. To the best of our knowledge, it is the fastest algorithm for graph coarsening. Through extensive experiments, we have also shown that our algorithm is not only fast but also preserves the properties of the original graph. Furthermore, it is worth noting that UGC represents the first work in the domain of graph coarsening for heterophilic datasets. This framework addresses the unique challenges posed by heterophilic graphs and has demonstrated a significant increase in node classification accuracy following graph coarsening. In conclusion, we believe that our framework is a major contribution to the field of graph coarsening and offers a fast and effective solution for simplifying large networks. Our future research goals include the exploration of different hash functions and novel applications for the framework.

## 6 Acknowledgement

Mohit Kataria acknowledges the generous grant received from Google Research India to sponsor his travel to NeurIPS 2024. Additionally, this work is supported by DST INSPIRE faculty grant MI02322G and Yardi-ScAI, IIT Delhi research fund.