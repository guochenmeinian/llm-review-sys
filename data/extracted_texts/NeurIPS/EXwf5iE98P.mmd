# IKEA Manuals at Work: 4D Grounding of

Assembly Instructions on Internet Videos

 Yunong Liu\({}^{1}\) Cristobal Eyzaguirre\({}^{1}\) Manling Li\({}^{1}\) Shubh Khanna\({}^{1}\)

Juan Carlos Niebles\({}^{1}\) Vineeth Ravi\({}^{2}\) Saumitra Mishra\({}^{2}\) Weiyu Liu\({}^{1}\) Jiajun Wu\({}^{1}\)

Equal advising. Project page: yunongliu1.github.io/ikea-video-manual

###### Abstract

Shape assembly is a ubiquitous task in daily life, integral for constructing complex 3D structures like IKEA furniture. While significant progress has been made in developing autonomous agents for shape assembly, existing datasets have not yet tackled the 4D grounding of assembly instructions in videos, essential for a holistic understanding of assembly in 3D space over time. We introduce IKEA Video Manuals, a dataset that features 3D models of furniture parts, instructional manuals, assembly videos from the Internet, and most importantly, annotations of dense spatio-temporal alignments between these data modalities. To demonstrate the utility of IKEA Video Manuals, we present five applications essential for shape assembly: assembly plan generation, part-conditioned segmentation, part-conditioned pose estimation, video object segmentation, and furniture assembly based on instructional video manuals. For each application, we provide evaluation metrics and baseline methods. Through experiments on our annotated data, we highlight many challenges in grounding assembly instructions in videos to improve shape assembly, including handling occlusions, varying viewpoints, and extended assembly sequences.

## 1 Introduction

The autonomous assembly of complex 3D structures requires an understanding at multiple levels of abstraction. The top level is task planning--decomposing the task into subtasks and computing their dependency and ordering, as outlined in an instruction manual; the middle level is visual grounding--registering each part with perceptual input at the pixel level, identifying their geometry and pose, so that the manual instructions get translated to actionable steps; the bottom level is motion planning and control--executing the steps based on the instructions and the identified states, avoiding collisions given the specific embodiment. A successful assembly requires solving all of these problems together, which is difficult. Thus, assembly remains a significant challenge in AI and robotics.

When developing assembly benchmarks, researchers often choose IKEA furniture as target objects due to its ubiquity and standardization. However, existing benchmarks and datasets focus only on part of the assembly problem. Classic work on designing assembly instructions provides step-by-step guidance for task planning, but they are not visually grounded ; recent work has attempted to ground assembly steps to 3D part models and register them at the pixel level , but it does not include action trajectories of how the assembly may actually happen; datasets that provide 3D groundings in the form RGB-D videos are collected in controlled lab environments [3; 4]; a video dataset of IKEA furniture assembly from the Internet includes more diverse demonstrations , but it lacks correspondence with 3D part models and alignment with the instruction manuals.

To address these limitations, we introduce the IKEA Video Manuals dataset, a large-scale multimodal dataset with high-quality, spatial-temporal alignments of step-by-step instructions, 3D models, andreal-world video demonstrations from the Internet. IKEA Video Manuals provides 34,441 annotated video framesfrom 98 assembly videos for 6 furniture categories. We provide extensive video annotations, including 2D-3D part correspondences, temporal step alignments, and part segmentation. The key contributions of our work include:

* A novel multimodal dataset built on top of Internet videos to capture the complexity and diversity of real-world furniture assembly;
* Comprehensive annotations, including 2D-3D part correspondences, temporal step alignments, and part segmentation;
* Extensive experiments on plan generation, part segmentation and pose estimation, video object segmentation, and part assembly with the annotated data.

## 2 Related Work

**Instructional Video Datasets and Procedural Understanding.** Instructional video datasets are essential for advancing the understanding and learning of procedural tasks. Existing datasets such as YouCook2 , COIN , and EPIC Kitchens  focus on cooking and daily activities, while datasets such as IKEA ASM , IKEA-FA [9; 10], and Assembly101  target shape assembly. However, these datasets are predominantly annotated at the coarse level with action labels, limiting their utility for grounding procedural tasks in 3D. Instructional datasets have led to the development of various methods for procedural understanding, including action recognition, video classification, action segmentation, localization, and prediction [7; 12; 13]. To capture the hierarchical nature of procedural tasks, some methods utilize hand-centric features and script data . Temporal and semantic relationships between actions in complex activities have been explored using graph-based representations [15; 16; 17]. Techniques such as unsupervised learning from narrated instruction videos aim to identify key procedural steps , and cross-task weak supervision has been introduced to improve transfer learning of step localization . Despite advances in procedural understanding, current efforts are mostly limited to 2D video data without grounding in 3D. This absence of 3D context restricts the learning of spatial relations and object interactions essential to real-world task understanding. In this paper, we address these limitations by focusing on the 4D grounding of

Figure 1: **Dataset Overview. (a) Manual images showing the assembly steps. (b) Video frames from the corresponding assembly videos. Temporal alignment between the video frames and each assembly step is also provided. (c) Segmentation masks for individual parts and sub-assemblies that are being constructed in each frame. When two parts are assembled, their masks are combined. (d) 6-DoF poses for parts and sub-assemblies in each frame. (e) Tracking of individual parts and sub-assemblies across video frames, capturing the frame-by-frame assembly process.**

assembly plans in videos. We provide a more comprehensive comparison with existing shape assembly datasets in Section 3.3.

**Shape Assembly.** Shape assembly has been a long-standing research problem, with foundational works focusing on constructing 3D shapes by assembling parts from repositories [19; 20; 21; 22]. More recent approaches can generate parts and predict their transformations to obtain a new shape [23; 24; 25]. Building on PartNet , graph-based learning methods like DGL  and RGL  predict the 6-DoF poses of individual parts to construct a shape, while approaches such as IET  leverage the transformer neural network to model part relationships. However, these methods ignore potential guidance for assembly from different forms of instructions, limiting their applicability to more complex shapes. To address this gap, datasets like IKEA-Manual  have been introduced, providing 3D IKEA furniture models and corresponding instruction manuals. Despite the advancement, existing datasets do not fully capture the complexity and diversity of real-world assembly processes. IKEA Video Manuals addresses these limitations by introducing a multimodal dataset that aligns real-world assembly videos with 3D models and human-designed visual manuals.

## 3 Grounding Assembly Instructions on Internet Videos

In this section, we formally define the spatio-temporal data involved in grounding assembly instructions. We then present the features and analysis of our IKEA Video Manuals dataset.

### Definition

As illustrated in Fig. 1, we provide a dataset of grounded assembly instructions for IKEA furniture. Each piece of furniture \(S\) in the dataset consists of a set of **3D parts**\(\{p_{1},...,p_{N}\}\), where \(N\) denotes the number of parts. The 6-DoF poses of the 3D parts are denoted by \(\{_{1},...,_{N}\}\). The furniture is assembled by transforming the 3D parts according to the poses, i.e., \(S=\{_{1}(p_{1}),...,_{N}(p_{N})\}\). In our dataset, each video consists of a sequence of **frames**\(\{f_{1},...,f_{T}\}\) and demonstrates a physically realistic assembly process (Fig. 1b). In the assembly process, 3D parts are combined, and new sub-assemblies are formed. We denote each **sub-assembly** by \(A\), which can be an individual part or a set of previously combined parts. In each frame, we identify the sub-assemblies that are being constructed (Fig. 1e). We further localize each sub-assembly in each frame of the video with a **segmentation mask**, which assigns pixels to their associated sub-assembly (Fig. 1c). A **6-DoF pose** of each sub-assembly in the camera's coordinate frame is included along with the camera intrinsic parameters (Fig. 1d). Combining the poses of the furniture parts throughout the video gives rise to the **4D grounding** of the assembly video.

To provide high-level guidance of the assembly procedure, we also include the **instruction manual** for each piece of furniture in our dataset. Each instruction manual consists of a sequence of \(L\) images \(\{m_{1},...,m_{L}\}\) (Fig. 1a). Similarly to the videos, each image from the manual is also annotated with the identities, masks, and poses of the appeared sub-assemblies. We observe that instruction manuals often provide higher-level illustrations of assembly procedures than the assembly videos and omit details of the part trajectories. To associate the two types of instructions for 3D assembly, a temporal alignment between them is established as a mapping \((m_{i})\{f_{j},..,f_{k}\}\), where \(j k\).

Figure 2: **Dataset Diversity. (a) Examples of 3D furniture models across different categories in our dataset, showing structural and functional variety. (b) Diverse assembly environments from our video collection, demonstrating real-world complexity including different lighting conditions, camera angles, and backgrounds. The diversity of both furniture types and environments presents unique challenges for grounding.**

### Key Features

**Multiple Instruction Forms for Assembly.** Shape assembly is a complex task that requires geometric reasoning and planning; our dataset provides different types of instructions to facilitate the process, including high-level assembly tree, instruction manuals, and assembly videos. The high-level assembly tree omits geometric and spatial information about parts but provides the decompositions of the assembly process into smaller steps. Manuals provide pictorial illustrations of the assembly process. These illustrations provide both a coarse breakdown of the assembly process and relative poses between object parts in 2D. Compared to instruction manuals, assembly videos provide a more fine-grained assembly process where parts are assembled one at a time, and the whole trajectory of the object movement till the construction of each sub-assembly is shown.

**Temporal Alignment of Instruction and Assembly Process.** Different instruction forms provide different temporal decomposition of the process. Instruction manuals often provide high-level decomposition, while how-to videos demonstrate more detailed steps of each part assembly. Our dataset aligns each step from the instruction manual with a sequence of substeps, in which sub-assemblies are formed (Fig. 1a and Fig. 1b). These substeps are further mapped to segments of the how-to videos, which provide a frame-by-frame demonstration of the assembly.

**Spatial Alignment of Instruction and Assembly Process.** Our dataset further provides spatial details of the whole assembly process in 3D observed from the instruction manuals and videos. These details are provided in the form of 6-DoF pose trajectories of the furniture parts. Specifically, for each video frame, the parts being assembled are annotated with a 2D image mask. The pose of each object part in the camera frame is provided, while the relative poses between parts that are being assembled are detailed. With the additional camera intrinsics we provide, the 3D parts can also be projected into 2D image space and aligned with their corresponding 2D mask.

**Diversity in Assembly.** As shown in Fig. 2, our dataset captures a wide range of furniture assembly scenarios from the Internet videos, encompassing various furniture types, designs, and assembly processes. The dataset includes six main furniture classes. Different instances of the same furniture class are also provided to present the difference in assembly due to designs, sizes, and structures. Furthermore, the dataset also includes multiple assembly videos for each instance. These videos capture various perspectives, environments, and individuals performing the assembly, allowing an in-depth analysis of the variability and commonalities in assembly processes.

**Complexity in Real World Videos.** By including assembly videos from the Internet, our dataset captures the complexity inherent in real-world data. The assembly videos present visual challenges such as changes in camera parameters, camera movements, diverse environment backgrounds, and heavy occlusions. These challenges are representative of the difficulties encountered in practical applications and provide a meaningful benchmark for evaluating the performance and robustness of assembly understanding algorithms.

### Comparison with Existing Datasets

Our dataset is the first to provide 6-DoF pose annotations for furniture assembly from internet videos, capturing real-world complexity across diverse settings. We compare it to existing assembly datasets in Table 1. Unlike prior video datasets with 3D information, our dataset features a significantly wider variety of objects (36 furniture types comprising 268 parts) and environments (over 90 different settings). Our dataset uniquely captures variations in the assembly process for each piece of furniture, with 25% of items having multiple valid assembly sequences, the Laiva shelf having the highest number with eight variations. Our data collection pipeline addresses key challenges in real-world video annotation, notably ensuring consistent camera parameters and maintaining accurate relative poses between parts across frames. Additional comparisons, including action labels and human pose information, are discussed in Appendix I.

### Dataset Statistics and Analysis

**Statistics.** Our dataset comprises 98 RGB videos. For each video, we annotated 1 frame per second, resulting in a total of 34441 annotated frames. On average, 316 frames are annotated for each video. In total, the dataset contains 137 high-level assembly steps from instructional manuals and 1120 detailed substeps from videos. The dataset provides assembly instructions for 36 unique IKEA furniture models, including 20 chairs, 8 tables, 3 benches, 1 desk, 1 shelf, and 3 other categories.

**Analysis of Assembly Process.** Our dataset includes the assembly process for complex 3D structures. On average, each piece of furniture has seven parts. The dataset also captures the variability inassembly complexity, with the longest video spanning 49 minutes and the average duration per video being six minutes. Fig. (a)a shows the distribution of the number of steps in the videos. Fig. (b)b presents the distribution of the number of substeps in the videos. Fig. (c)c illustrates the distribution of the number of mask and pose annotations in the videos. On average, each step from the instruction manual corresponds to eight substeps in the videos.

## 4 Data Collection and Annotation

The IKEA Video Manuals dataset is a comprehensive multimodal dataset that aligns 3D furniture, step-by-step instructions, and real-world video demonstrations. We collect data from various sources and perform extensive annotations to provide a rich resource for grounding furniture assembly instructions. In this section, we describe the data sources, temporal annotations, mask annotations, and pose annotations. We provide details and illustrations of our interface in Appendix C and E.

### Collecting 3D Models and Assembly Videos

Building on the IKEA-Manual dataset  and IAW dataset , we collect 36 segmented 3D furniture models from the IKEA-Manual dataset and 98 assembly videos associated with them in the IAW dataset, providing temporal alignment between instruction steps and video segments. as illustrated in Fig. (a)a. We focus on collecting fine-grained temporal segmentation and pose annotations for videos.

### Annotating Temporal Segmentation and Part Identity

In our dataset, we provide fine-grained temporal segmentation of the videos based on the construction of each sub-assembly in the assembly processes. To annotate such data, we first extract video segments for each manual step from the IAW dataset. As a single step in the instruction manual often involves combining multiple furniture parts, we further decompose each video segment into smaller intervals, which we call substeps (shown in Fig. (b)b). In these substeps, a new sub-assembly can be constructed or deconstructed. For each substep, we sample video frames at 1 FPS. As illustrated in Fig. (c)c, we manually annotate the identities of the furniture parts in the first frame for each substep. This annotation is essential for ensuring consistent mask and pose annotations in the following stages because many furniture parts can be similar in appearance (e.g., the legs for a table).

### Annotating Segmentation Masks

To track the identities of the furniture parts throughout an assembly video, we annotate 2D image segmentation masks for 3D parts in the sampled frames. To facilitate the annotation process, we develop a web interface that displays auxiliary 2D and 3D information and enables interactive mask annotation based on the Segment Anything Model (SAM) model . For each target part, an

   Dataset & \# Object Class & \# Object & Video Source & \# Environment & 3D Object Model & 3D Info. & Camera Param. \\  IKEA Video Manuals (Ours) & 6 & 36 & Internet & \(\)90 & ✓ & 6-Def Pose & Estimated\({}^{a}\) \\ Assembly101  & 15 & 101 & Lab & 1 & ✗ & Depth & Calibrated \\ HA-VID  & 1 & 35 parts & Lab & 1 & ✓ & Depth & Calibrated \\ IKEA-Manual  & 6 & 102 & / & ✓ & ✓ & 6-Def Pose & Estimated \\ IKEA Geo 3D  & 4 & 4 & Lab & 1 & ✗ & Depth & Calibrated \\ IKEA ASM  & 3 & 4 & Lab & 5 & ✗ & Depth & Calibrated \\ IKEA in the Wild  & 14 & 420 & Internet & \(\)1000 & ✗ & / & Uncalibrated \\   

Table 1: **Comparison with Existing Assembly Datasets.** Our dataset uniquely provides 6D pose annotations on internet videos, capturing furniture assembly in diverse, real-world settings. *While camera parameters in both our dataset and IKEA-Manual are estimated, we implement additional processing to ensure consistent parameters within each video segment, provided there are no obvious camera changes.

Figure 3: **Dataset Statistics.** (a) Distribution of the number of assembly steps in videos. (b) Distribution of the number of sub-assembly steps (substeps) in videos. (c) Distribution of the number annotations in videos.

annotator sees the 3D furniture model with the target part highlighted and the part's 2D location in the first frame of the current substep (as seen in Fig. 4e). The annotator then labels the keypoints in the current frame, which is fed into the SAM model to generate the 2D segmentation mask in real-time. Modifications can be easily made by adding or removing key points. To deal with SAM's inherent limitations in extracting boundaries between parts with similar textures or in low-light regions, we further allow annotators to manually adjust the mask annotations with a brush and an eraser tool.

### Annotating 2D-3D Correspondence

Inferring relative poses between 3D furniture parts from 2D videos is important for extracting grounded assembly knowledge from videos. We annotate 3D poses of furniture parts in the sampled video frames. Manual annotation is essential as real-world assembly videos often feature occlusions, challenging viewpoints, and partial visibility - these are difficult cases for recovering poses directly from depth estimation. A key challenge is to ensure the 3D trajectories of object parts respect the geometric constraints enforced by the physical assembly process (e.g., the final relative pose between two parts inferred from a video needs to align with their poses in the 3D furniture model). Besides minimizing the projection error of 3D parts in 2D images, we additionally emphasize the accuracy of relative poses between furniture parts and cross-frame consistency in the annotation process.

A prerequisite for achieving spatially and temporally accurate pose annotation is a correct estimation of camera parameters from the video. As illustrated in Fig. 4f, we first identify video frames where potential changes in camera intrinsics occur (e.g., due to focal length adjustments or switching between multiple cameras). We then annotate 2D-3D point correspondences between the 3D models and their 2D projections in the video frames. Using a combination of these two types of annotations, we estimate camera intrinsics for each video. In particular, for each estimated candidate intrinsics, we use the Perspective-n-Point (PnP) algorithm  to estimate the pose of the object. We then select the intrinsic that minimizes the reprojection errors for frames between two camera changes. To further refine the camera intrinsics, we apply the Random Sample Consensus (RANSAC)  algorithm to filter out outliers in the annotated keypoints. From the resulting top ten intrinsics in each video segment, we choose the one that provides a minimal set of camera intrinsics.

We develop an interactive interface for refining pose annotations initially estimated from the 2D-3D point correspondences. The interface allows annotators to control the virtual camera using axis

Figure 4: **Data Collection and Annotation Pipeline. (a) Collecting 3D furniture models, associated assembly manuals and videos. (b) Annotating coarse temporal segmentation of videos into segments showing each assembly step. (c) Tracking identities of 3D parts throughout each video keyframe. (d) Fine-grained temporal segmentation into substeps showing the construction of each sub-assembly. (e) Annotating 2D segmentation masks for parts and sub-assemblies in sampled frames using an interactive interface powered by the SAM model. (f) Estimating camera parameters and 6D part poses in each frame using 2D-3D correspondences, PnP, RANSAC, and manual refinement.**

aligned controls, enabling them to view the 3D scene from different orthographic perspectives. This helps identify and correct errors in relative part poses that may be difficult to detect in a single rendered image. The annotators are able to use the interface to refine the part poses by rotating and translating parts in 3D space. Annotators refine part poses by manipulating them in 3D space, comparing the real-time 3D view with corresponding video frames. To further improve the accuracy of relative poses, parts that appear together in a video frame are annotated together with a visualization of their 3D locations. Temporally smoothness of the part trajectories is improved by initializing part poses with poses from the previous frame.

## 5 Applications

We evaluate existing methods on five tasks essential for grounding instructional assembly videos. We use pretrained models without additional finetuning on our dataset.

### Assembly Plan Generation

Assembly plan generation aims to predict a hierarchical assembly plan from a sequence of video frames \(\{f_{1},,f_{T}\}\) depicting a furniture assembly process. The predicted plan is represented as a directed acyclic graph \(=(,)\), where each node \(v V\) corresponds to a subset of \(K\) parts \(\{p_{1},p_{2},,p_{K}\}\), and each edge \(e\) indicate assembly order and parent-child relationships. The root node \(v_{r}\) represents the final assembled shape. IKEA manuals present assembly instructions as diagrams, often combining multiple parts in one step (Fig. 4(a)). Our IKEA Video Manuals presents physically realistic assembly plans extracted from Internet videos (Fig. 4(b)).

**Experiment Setup.** We consider two heuristic baselines from IKEA-Manual . The first baseline, SingleStep, constructs an assembly tree with all parts directly connected to the root node, corresponding to assembling all parts in a single step. The second baseline, GeoCluster, uses a pre-trained DGCNN  to extract 3D features for each part and constructs the assembly tree iteratively by grouping geometrically similar parts in individual steps. We report the precision, recall, and F1 score based on two matching criteria between the predicted and ground truth plans, as proposed in . Simple Matching considers a predicted node as correct if it matches a ground truth node based on the primitive parts. Hard Matching requires the predicted node to match the ground truth node based on both the parts and parent-child relationships.

    &  &  &  \\   & & Precision & Recall & F1 Score & Precision & Recall & F1 Score \\  SingleStep & IKEA-Manual & 100.00 & 35.77 & 48.64 & 10.78 & 10.78 & 10.78 \\ GeoCluster & IKEA-Manual & 44.90 & 48.46 & 43.53 & 16.54 & 16.50 & 16.30 \\  SingleStep & Ours & 98.98 & 16.86 & 26.88 & 3.06 & 2.55 & 2.72 \\ GeoCluster & Ours & 43.04 & 24.16 & 29.74 & 14.98 & 9.49 & 11.51 \\   

Table 2: **Assembly Plan Generation Results. Results on the IKEA Video Manuals dataset compared to the IKEA-Manual dataset. Two heuristic baselines are evaluated using Simple Matching and Hard Matching criteria. Precision, Recall, and F1 scores are reported for each setting.**

Figure 5: **Example of Hierarchical Assembly Trees. (a) Assembly tree structure derived from the high-level steps in the IKEA manual. (b) More detailed assembly tree structure extracted from the fine-grained substeps annotated in the assembly videos.**

**Results and Analysis.** As shown in Table 2, both baselines struggle to generate accurate plans that resemble the assembly process seen in the videos. Utilizing the geometric features of the furniture parts, GeoCluster slightly outperforms SingleStep. Since video-derived assembly plans are often longer and presents greater diversity than plans extracted from instruction manuals, both models perform worse on our dataset than the IKEA-Manual dataset.

### Part-Conditioned Segmentation

This task leverages the diverse videos in the IKEA Video Manuals dataset to evaluate the performance of part segmentation methods in real-world scenarios. The part-conditioned segmentation task requires the models to predict pixel-wise segmentation masks for the furniture parts seen in the assembly videos. Formally, given a frame \(f\) and a sub-assembly \(A\), the goal is to predict a binary segmentation mask for the sub-assembly.

**Experimental Setup.** We test pre-trained CNOS  and SAM-6D . Both models can segment novel objects given their 3D models. In total, we evaluated on 12296 examples from the dataset. We only include sub-assemblies that are unique in shape to remove ambiguities. We report Intersection-over-Union (IoU) and Top-5 IoU.

**Results and Analysis.** As shown in Table 3, both CNOS and SAM-6D obtain relatively low performance on the IKEA Video Manuals dataset. We hypothesize that SAM-6D outperforms CNOS by considering additional geometric features, including shape and size. Common failures of both models stem from heavily occluded parts, visually complex backgrounds, and textureless 3D shapes. The results highlight the existing challenges in detecting object parts in Internet videos.

### Part-Conditioned Pose Estimation

Estimating 3D poses of furniture parts from each video frame is essential for grounding the assembly process. Given a video frame \(f\) and a furniture sub-assembly \(A\), the goal of part-conditioned pose estimation is to predict the 6-DoF pose of the sub-assembly \(A\) in the frame \(f\).

**Experimental Setup.** We sample 7795 annotations from the IKEA Video Manuals dataset and use ground truth masks for evaluation. We evaluate four methods: SAM-6D , MegaPose, and two differentiable rendering-based methods. SAM-6D requires a depth image in addition to the RGB image, which we obtain using the depth estimation model MiDaS . The first differentiable rendering method uses Mean Squared Error (MSE) loss as the optimization objective, while the second incorporates an occlusion-aware silhouette re-projection loss proposed in PHOSA . Both differentiable rendering methods use 20 random initial poses, refine the top five candidates with the lowest initial loss for 500 iterations, and output the pose with the lowest final loss. We report the ADD and ADD-S metrics, which are commonly used in the 6D pose estimation literature .

**Results and Analysis.** Table 4 presents the quantitative results of all four methods on the IKEA Video Manuals dataset. While MegaPose achieves better performance overall, all four methods struggle with real-world challenges such as partial visibility and occlusions. MegaPose particularly struggles with symmetric parts and challenging viewpoints, while SAM-6D's performance is limited by the accuracy of the depth estimation in complex scenes. In Appendix H, we provide detailed error analysis and examples of failure cases. The high ADD and ADD-S scores indicate that the predicted poses are far from the ground truth poses, highlighting the challenges posed by the IKEA Video Manuals dataset.

   Method & ADD & ADD-S \\  SAM-6D  & 2.34 & 1.85 \\ MegaPose  & **1.36** & **0.89** \\ Diff. Rendering (MSE) & 3.33 & 2.91 \\ Diff. Rendering (Occlusion-Aware) & 3.29 & 2.86 \\   

Table 4: **Part-conditioned 6D Pose Estimation Results on the IKEA Video Manuals dataset using the ADD and ADD-S metrics.**

   Method & IoU & Top-5 IoU \\  CNOS  & 0.09 & 0.21 \\ SAM-6D  & **0.16** & **0.40** \\   

Table 3: **Part-conditioned Segmentation Results on the IKEA Video Manuals dataset. IoU and Top-5 IoU metrics are reported.**

### Video Object Segmentation

Video object segmentation in our dataset focuses on tracking individual furniture parts within assembly substeps. Given a video sequence \(\{f_{1},,f_{T}\}\) representing a single substep and an initial segmentation mask \(M_{1}\), the goal is to predict masks \(\{M_{2},,M_{T}\}\) for subsequent frames where the part's identity remains constant (i.e., before it connects with other parts or becomes part of a new sub-assembly). The task evaluates models' ability to track parts despite occlusions, varying viewpoints, and similar-looking parts in real-world scenarios.

**Experimental Setup.** We evaluate SAM2  and Cutie  on video segments corresponding to assembly substeps with at least 20 frames. We closely follow the experiment design of standard video object segmentation, ensuring a fair comparison with other benchmark datasets. For each test example, we initialize the mask of the target part with ground truth in the first frame of the substep. We evaluate the performance on subsequent frames. We report the standard J&F metric for video object segmentation .

**Results and Analysis.** Table 5 shows that both models perform worse on our dataset compared to existing benchmarks. SAM2's performance drops moderately, while Cutie shows a more significant decrease compared to existing benchmarks. The results highlight challenges presented by our dataset, including camera movements, the presence of parts with similar appearances and small parts, frequent occlusions, and extended assembly sequences.

### Shape Assembly with Instruction Videos

Given a set of 3D parts \(\{p_{1},,p_{N}\}\) and an instruction video \(\{f_{1},,f_{K}\}\), the goal of this task is to predict the 6-DoF poses \(\{_{1},,_{N}\}\) for the 3D parts to assemble them into complete furniture. We decompose this problem into several sub-tasks, including key frame detection, assembled part recognition, pose estimation, and iterative assembly.

**Method.** We propose a modular video-based shape assembly pipeline that consists of the following steps: First, a keyframe detection model should be employed to identify the frames in which two parts or sub-assemblies are being combined. These frames typically provide a clear view of how the parts are being connected. Next, a segmentation and part identification model should be used to identify which 3D parts from the set of all 3D parts are being assembled in the frame and to determine their 2D locations in the image. Third, starting with the first keyframe, we estimate the poses of the parts being assembled and combine them into a sub-assembly \(A_{i}\), which is a set of transformed parts \(\{_{i}(p_{i})\}_{i=1}^{K}\), where \(K\) is the number of parts in the sub-assembly. When moving to the next keyframe, we estimate the pose of the sub-assembly \(A_{i}\) from the previous keyframe and the part to

    &  & SA-V &  & DAVIS & LVOS & YTVOS \\  & &  & & 2017  &  & 2019 \\  SAM2 Hiera-L  & 73.6 & 75.6 & 77.2 & 91.6 & 76.1 & 89.1 \\ Cutie-base  & 54.7 & 60.7 & 69.9 & 87.9 & 66.0 & 87.0 \\   

Table 5: **Video Object Segmentation Results** on the IKEA Video Manuals dataset compared to other benchmark datasets. J&F scores are reported for each method.

Figure 6: **Qualitative Examples. Examples of part-conditioned segmentation (left) and part-conditioned pose estimation (right) on the IKEA Video Manuals dataset. For segmentation, the ground truth masks are shown along with predicted masks from CNOS  and SAM-6D . The orange mask shown in the SAM-6D result is caused by the overlap of red and blue masks. For pose estimation, the ground truth 6D pose is shown along with predicted poses from SAM-6D and MegaPose. **be assembled next. Incrementally transforming and combining the parts based on the estimated poses from the keyframes, we gradually build the whole furniture.

**Experimental Setup.** Since most components of the proposed modular approach have been tested individually in previous experiments, in this experiment, we evaluate how the accuracy of the keyframe detection method affects the final shape assembly results. We propose two experimental settings. In the first setting, we use the annotation in our dataset to extract frames where two parts are being connected, which are typically the last frame for each substep. We also use the annotated poses of the parts or sub-assemblies from our dataset instead of deploying any models to perform pose estimation. We aim to provide a reference for evaluating the performance of our shape assembly approach when accurate pose information is available. In the second setting, we detect keyframes from the videos using the GPT-4o  vision and language model. In this setting, we first sample frames from the video at a rate of one frame per second. Then, we ask GPT-4o to describe the scene in each sampled frame. Finally, we feed those descriptions back to GPT-4o and require the model to predict which frames are keyframes. This setting explores using a pretrained vision and language model for identifying assembly steps in the instruction video. We evaluate the accuracy of the final assembly by computing the Chamfer Distance between the assembled furniture and the groundtruth furniture.

**Results and Analysis.** Fig. 7 presents qualitative results for the shape assembly task in the two experimental settings. The method achieves a Chamfer Distance of \(0.33\) in the first setting. The inaccuracy in this setting can be attributed to cases in which object parts are not fully connected in the last frames of the substeps. In the second setting, we test on 15 videos and obtain a Chamfer Distance of 0.55. We observe that GPT-4o fails to identify the final assembly step in 5 of the 15 videos, resulting in incomplete furniture. In summary, we introduce a novel approach for shape assembly by leveraging instruction videos, demonstrating the effectiveness of integrating video-based guidance in the assembly process. However, our results suggest tremendous challenges in grounding instructional assembly videos and highlight potential areas for improvement.

## 6 Conclusion

In this paper, we introduce the IKEA Video Manuals dataset, a large-scale multimodal dataset with high-quality, spatial-temporal alignments of step-by-step instructions, 3D furniture models, and real-world assembly videos from the Internet. In total, our dataset provides 34,441 video frames annotated with part segmentations and 6-DoF poses for 98 assembly videos and 36 different IKEA furniture models from 6 furniture categories. Experiments on the dataset highlight significant challenges in grounding instructional assembly videos, including extracting part segmentations and poses, constructing high-level assembly plans, and detecting key assembly steps in videos.

**Limitations.** Currently, the dataset's limited size prevents large-scale training. The dataset focuses on visual and 3D information; including other data modalities such as audio or textual data is an important future direction. While manual annotation currently limits dataset scale, our framework for aligning Internet videos with 3D models and instructions provides a foundation for future expansion. The data collection process still requires manual annotation and verification, therefore presenting challenges for collecting data at a significantly larger scale. Current baselines demonstrate challenges in grounding 4D assembly but are yet to utilize this dataset for advanced model development. Since the dataset is focused on furniture assembly, whether models developed for this domain transfer to other assembly domains is left to be investigated. Future work could augment the dataset with additional modalities and develop algorithms leveraging instructional videos for 3D-grounded assembly plans. Broader implications include potential assistive technologies for individuals with disabilities, while internet-sourced data necessitates robust privacy and fair use methods.

Figure 7: **Qualitative Examples. Examples of shape assembly on the IKEA Video Manuals dataset.**