# Improving Adaptivity via Over-Parameterization in Sequence Models

Yicheng Li

Department of Statistics and Data Science

Tsinghua University, Beijing, China

liyc22@mails.tsinghua.edu.cn &Qian Lin

Department of Statistics and Data Science

Tsinghua University, Beijing, China

qianlin@tsinghua.edu.cn

Corresponding author Qian Lin also affiliates with Beijing Academy of Artificial Intelligence, Beijing, China

###### Abstract

It is well known that eigenfunctions of a kernel play a crucial role in kernel regression. Through several examples, we demonstrate that even with the same set of eigenfunctions, the order of these functions significantly impacts regression outcomes. Simplifying the model by diagonalizing the kernel, we introduce an over-parameterized gradient descent in the realm of sequence model to capture the effects of various orders of a fixed set of eigen-functions. This method is designed to explore the impact of varying eigenfunction orders. Our theoretical results show that the over-parameterization gradient flow can adapt to the underlying structure of the signal and significantly outperform the vanilla gradient flow method. Moreover, we also demonstrate that deeper over-parameterization can further enhance the generalization capability of the model. These results not only provide a new perspective on the benefits of over-parameterization and but also offer insights into the adaptivity and generalization potential of neural networks beyond the kernel regime.

## 1 Introduction

In recent years, the remarkable success of neural networks in a wide array of machine learning applications has spurred a search for theoretical frameworks capable of explaining their efficacy and efficiency. One such framework is the Neural Tangent Kernel (NTK) theory (see, e.g., Jacot et al. (2018); Allen-Zhu et al. (2019)), which has emerged as a pivotal tool for understanding the dynamics of neural network training in the infinite-width limit. The NTK theory posits that the training dynamics of wide neural networks can be closely approximated by a kernel gradient descent method with the corresponding NTK, elucidating their convergence behaviors during gradient descent and shedding light on their generalization capabilities. Parallel to this, an extensive literature on kernel regression (see, e.g., Bauer et al. (2007); Yao et al. (2007)) has studied its generalization properties, showing its minimax optimality under certain conditions and providing insights into the bias-variance trade-off. Thus, one can almost fully understand the generalization properties of neural networks in the NTK regime by analyzing the kernel regression method.

However, the application of NTK theory to analyze neural networks, while invaluable, essentially frames the problem within a traditional statistical method by a fixed kernel. The NTK analysis, by its reliance on the fixed kernel approximation, can not entirely account for the adaptability and flexibility exhibited by neural networks, particularly those of finite width that deviate from the theoretical infinite-width limit (Woodworth et al., 2020). Moreover, empirical evidence (Wenger et al., 2023; Seleznova and Kutyniok, 2022) also suggests that the assumption of a constant kernel during training, a cornerstone of NTK analysis, may not hold in practical scenarios where the network architecture orinitialization conditions foster a dynamic evolution of the kernel. Also, Gatmiry et al. (2021) showed the benefits brought by the adaptivity of the kernel on a three-layer neural network. These results underscore the need for a more nuanced understanding of neural network training dynamics, one that considers the intricate interplay between network architecture, initialization, and the optimization process beyond the simplifications of NTK theory.

Recently, another branch of research has focused on the over-parameterization nature of neural networks beyond the NTK regime, exploring how over-parameterization can lead to implicit regularization and even improve the generalization. In terms of training dynamics, studies (Hoff (2017); Gunasekar et al. (2017); Arora et al. (2019); Kolb et al. (2023), etc.) in this domain have revealed that over-parameterized models, particularly those trained with gradient descent and its variants, exhibit biases towards simpler, more generalizable functions, even in the absence of explicit regularization terms. Moreover, in terms of generalization, recent works (Vaskevicius et al., 2019; Zhao et al., 2022; Li et al., 2021) have shown that in the setting of high dimensional linear regression, over-parameterized models with proper initialization and early stopping can achieve minimax optimal recovery under certain conditions. These results underscore the potential and benefits of over-parameterized models that go beyond the traditional statistical paradigms.

In this work, we will incorporate the insights from the kernel regression and the over-parameterization theory to investigate how over-parameterization can improve generalization and also adaptivity under the non-parametric regression framework. As a first step towards this direction, we will focus on the sequence model, which is an approximation of a wide spectrum of non-parametric models including kernel regression. We will show that, by dynamically adapting to the underlying structure of the signal during the training process, over-parameterization method with gradient descent can significantly improve the generalization properties compared with the fixed-eigenvalues method. We believe that our results provide a new perspective on the benefits of over-parameterization and offer insights into the adaptivity and generalization properties of neural networks beyond the NTK regime.

### Our contributions

Limitations of the (fixed) kernel regression.In this work, we first investigate the limitations of the (fixed) kernel regression method by specific examples, illustrating that the traditional kernel regression method suffers from the misalignment between the kernel and the truth function. We show that even when the eigen-basis of the kernel is fixed, the associated eigenvalues, particularly their alignment with the truth function's coefficients in the eigen-basis, can significantly affect the generalization properties of the method.

Advantages of over-parameterized gradient descent.Focusing on the alignment between the kernel's eigenvalues and the truth signal (the truth function's coefficients), we consider the sequence model and introduce an over-parameterization method (8) that can dynamically adjust the eigenvalues during the learning process. We show that with proper early-stopping, the over-parameterization method can achieve nearly the oracle convergence rate regardless of the underlying structure of the signal, significantly outperforming the vanilla fixed-eigenvalues method when the misalignment is severe. In addition, the over-parameterization method is also adaptive by its universal choice of the stopping time, which is independent of the signal's structure.

Benefits of deeper parameterization.Moreover, we also consider deeper over-parameterization (14) and explore how depth affects the generalization properties of the over-parameterization method. Our results show that adding depth can further ease the impact of the initial choice of the eigenvalues, thus improving the generalization capability of the model. We also provide numerical experiments to validate our theoretical results in Section C.

### Notations

We denote by \(^{2}=\{(a_{j})_{j 1}_{j 1}a_{j}^{2}<\}\) the space of square summable sequences. We write \(a b\) if there exists a constant \(C>0\) such that \(a Cb\) and \(a b\) if \(a b\) and \(b a\), where the dependence of the constant \(C\) on other parameters is determined by the context.

## 2 Limitations of Fixed Kernel Regression

Let us consider the non-parametric regression problem given by \(y=f^{*}(x)+\), where \(\) is the noise with mean zero and variance \(^{2}\), \(x\) and \(\) is the input space with \(\) being a probability measure supported on \(\). The function \(f^{*}(x)\) represents the unknown regression function we aim to learn. Suppose we are given samples \(\{(x_{i},y_{i})\}_{i=1}^{n}\), drawn i.i.d. from the model. We denote \(X=(x_{1},,x_{n})^{}\) and \(Y=(y_{1},,y_{n})^{}\).

Let \(k:\) be a continuous positive definite kernel and \(_{k}\) be its associated reproducing kernel Hilbert space (RKHS). The well-known Mercer's decomposition (Steinwart and Scovel, 2012) of the kernel function \(k\) gives

\[k(x,y)=_{j=1}^{}_{j}e_{j}(x)e_{j}(y),\] (1)

where \((e_{j})_{j 1}\) is an orthonormal basis of \(L^{2}(,)\), and \((_{j})_{j 1}\) are the eigenvalues of \(k\) in descending order. Moreover, we can introduce the feature map \((x)=(_{j}^{}e_{j}(x))_{j 1}:^{2}\) (as a column vector) such that \(k(x,x^{})=(x),(x^{})\). With the feature map, a function \(f_{k}\) can be represented as \(f(x)=(x),_{^{2}}\) for some \(^{2}\).

Defining the empirical loss as \(L=_{i=1}^{n}(y_{i}-f(x_{i}))^{2}\), we can consider an estimator \(f_{t}=(x),_{t}_{^{2}}\) governed by the following gradient flow on the feature space

\[_{t}=-_{}L=_{i=1}^{n}(y_{i}-( x_{i}),_{t}_{^{2}})(x_{i}),_{0}= .\] (2)

This kernel gradient descent (flow) estimator corresponds to neural networks at infinite width limit by the celebrated neural tangent kernel (NTK) theory (Jacot et al., 2018; Allen-Zhu et al., 2019).

An extensive literature (Yao et al., 2007; Lin et al., 2018; Li et al., 2024) has studied the generalization performance of such kernel gradient descent estimator. From the Mercer's decomposition, we can further introduce interpolation spaces for \(s 0\) as

\[[_{k}]^{s}_{j=1}^{}_{j}_{j }^{}e_{j}\ \ (_{j})_{j 1}^{2}},\] (3)

which is equipped with the norm \(\|f\|_{[_{k}]^{s}}=\|\|_{^{2}}\) for \(f=_{j=1}^{}_{j}_{j}^{}e_{j}\). Particularly, the interpolation space \([_{k}]^{1}\) corresponds to the RKHS \(_{k}\) itself. Then, assuming the eigenvalue decay rate \(_{j} j^{-}\), the standard results (see, e.g., Yao et al. (2007); Li et al. (2024)) in kernel regression state that the optimal rate of convergence under the source condition \(f^{*}[_{k}]^{s}\) with \(\|f^{*}\|_{[_{k}]^{s}} 1\) is \(n^{-}\). However, since the interpolation space \([_{k}]^{s}\) is defined via the eigen-decomposition of the kernel, the generalization performance of kernel regression methods is ultimately related to the eigen-decomposition of the kernel and the decomposition of the target function under the basis, so the performance is intrinsically limited by the relation between the target function and the kernel itself. In other words, the choice of the kernel could affect the performance of the method. To demonstrate this quantitatively, let us consider the following examples.

**Example 2.1** (Eigenfunctions in common order).: It is well known that kernels possessing certain symmetries, such as dot-product kernels on the sphere or translation-invariant periodic kernels on the torus, share the same set of eigenfunctions (such as the spherical harmonics or the Fourier basis). If we consider a fixed set of eigenfunctions \(\{e_{j}\}_{j 1}\) and a given truth function \(f^{*}\), for two kernels \(k_{1}\) and \(k_{2}\) with eigenvalue decay rates \(_{j,1} j^{-_{1}}\) and \(_{j,2} j^{-_{2}}\) respectively, it follows that:

\[f^{*}[_{k_{1}}]^{s_{1}} f^{*}[_{k_{2}}]^{s_{2}}_{1}s_{1}=_{2}s_{2}.\]

Given that the convergence rate is dependent solely on the product \(s\), the convergence rates relative to the two kernels will be identical.

Example 2.1 seems to show that when the eigenfunctions are fixed, kernel regression methods yield similar performance across different kernels. However, it's important to note that this similarity is dueto both kernels having _the same eigenvalue decay order_, which aligns with the predetermined order of the basis. In fact, if the eigenvalue decay order of a kernel deviates from that of the true function, even if the eigenfunction basis remain the same, it can lead to significantly different convergence rates. Let us consider the following example to illustrate this point.

**Example 2.2** (Low-dimensional structure).: Consider translation-invariant periodic kernels on the torus \(^{d}=[-1,1)^{d}\) with the uniform distribution. Then, their eigenfunctions are given by the Fourier basis \(_{}(x)=(i<,x>)\), \(^{d}\). Within this basis, a target function \(f^{*}(x)\) can be represented as:

\[f^{*}=_{^{d}}f_{}_{}(x).\]

Assuming \(f^{*}\) exhibits a low-dimensional structure, specifically \(f^{*}(x)=g(x_{1},,x_{d_{0}})\) for some \(d_{0}<d\), and considering \(g\) belongs to the Sobolev space \(H^{t}(^{d_{0}})\) of order \(t\), the coefficients \(f_{}\) can be shown to simplify to:

\[f_{}=g_{_{1}},&=(_{1},),\;_{ 1}^{d_{0}},\\ 0,&.\]

Let us now consider two translation-invariant periodic kernels \(k_{1}\) and \(k_{2}\) given in terms of their eigenvalues: \(k_{1}\) is given by \(_{,1}=(1+\|\|^{2})^{-r}\) for some \(r>d/2\), whose RKHS is the full-dimensional Sobolev space \(H^{r}(^{d})\); \(k_{2}\) is given by \(_{,2}=(1+\|\|^{2})^{-r}\) for \(=(_{1},)\) and \(_{,2}=0\) otherwise. Then, the function \(f^{*}\) belongs to both \([_{k_{1}}]^{s}\) and \([_{k_{2}}]^{s}\) for \(s=t/r\). After reordering the eigenvalues in descending order, the decay rates for the two kernels are identified as \(_{1}=2r/d\) and \(_{2}=2r/d_{0}\). Thus, the convergence rates with respect to the two kernels are respectively:

\[}.\]

Therefore, we see that when \(d\) is significantly larger than \(d_{0}\), the convergence rate for the second kernel notably surpasses that of the first.

This example illustrates that the eigenvalues can significantly impact the learning rate, even when the eigenfunctions are the same. In the scenario presented, the second kernel benefits from the low-dimensional structure of the target function by focusing only on the relevant dimensions, whereas the first one suffers from the curse of dimensionality since it considers all dimensions. The key point to take away from this example is the _alignment between the kernel and the target function_. To generalize this example, we can consider the following example where the order of the eigenvalues does not align with the order of the target function's coefficients.

**Example 2.3** (Misalignment).: Let us fix a set of the eigenfunctions \((e_{j})_{j 1}\) and expand the truth function as \(f^{*}=_{j 1}_{j}^{*}e_{j}\). Note that by giving \((e_{j})_{j 1}\), we already defined an order of the basis in \(j\), but coefficients \(_{j}^{*}\) of the truth function are not necessarily ordered by \(j\). Suppose that an index sequence \((j)\) gives the descending order of \(|_{(j)}^{*}|\). Then we can characterize the misalignment by the difference between \((j)\) and \(j\). Specifically, we assume that

\[|_{(j)}^{*}| j^{-(p+1)/2}(j ) j^{q} p>0,\;q 1,\] (4)

where larger \(q\) indicates a more severe misalignment. In terms of eigenvalues, let us consider \(_{j,1} j^{-}\), which is in the order of \(j\), while \(_{(j),2} j^{-}\), which is in the order of \((j)\). Then, the convergence rates with the two sequences of coefficients are respectively

\[.\]

Therefore, the convergence rates can differ greatly if the misalignment is significant, namely when \(q\) is large.

From Example 2.2 and Example 2.3, we find that it is beneficial that _the eigenvalues of the kernel align with the structure of the target function_. However, one can hardly choose the proper kernel a priori, especially when the structure of the target function is unknown, so the fixed kernel regression can be limited by the kernel itself and be unsatisfactory. Motivated by these examples, we would like to explore the idea of an "adaptive kernel approach," where the kernel can be learned from the data.

Adapting the Eigenvalues by Over-parameterization in the Sequence Model

Motivated by the examples in the last section, as a first step toward the adaptive kernel approach, we consider _adapting the eigenvalues of the kernel with eigenfunctions fixed_. To simplify the analysis, we would like to the following sequence model, which captures the essences of many statistical models (Brown et al., 2002, Johnstone, 2017).

The sequence modelLet us consider the sequence model (Johnstone, 2017)

\[z_{j}=_{j}^{*}+_{j}, j 1\] (5)

where \((z_{j})_{j 1}\) is the observation, \(^{*}=(_{j}^{*})_{j 1}^{2}\) is a sequence of unknown truth parameters and \(_{j},\ j 1\) are (not necessarily independent) \(^{2}\)-sub-Gaussian random variables with mean zero and variance at most \(^{2}\). For any estimator \(}=(_{j})_{j 1}\), the generalization error is measured by \((};^{*})=_{j=1}^{}(_{j}-_{j}^{*})^{2}\). Under the asymptotic framework, we are often interested in the behavior of the generalization error as \( 0\). Here, we note that the connection between non-parametric regression and the sequence model yields \(^{2} n^{-1}\).

To see the connection between the sequence model and the non-parametric regression model, we first write the gradient flow (2) in the RKHS in the matrix form as

\[_{t}=-_{}=-(X)(X)^{} _{t}+(X),\]

where the feature matrix \((X)=((x_{1}),,(x_{n}))_{ n}\) and \(=(y_{1},,y_{n})^{}\). Now, since the eigenfunctions \((e_{j})_{j 1}\) are fixed, intuitively, the gradient flow can be diagonalized in the eigen-basis since \((X)(X)^{}=(_{1 },_{2},)\) and the noise components are approximately normal with variance \(^{2}/n\) by the central limit theorem. Thus, we reach the sequence model. We refer to Subsection B.1 for a more detailed explanation of the connection between the sequence model and the kernel regression model.

Regarding the power series expansion (3) in RKHS, for a sequence \((_{j})_{j 1}\) of descending positive numbers (e.g., \(_{j}=j^{-}\)), we can consider similarly the parameterization \(_{j}=_{j}^{}_{j}\), \(j 1\) in \(^{2}\). Since \((_{j})_{j 1}\) corresponds to the eigenvalues of the kernel in the kernel regression, here we also refer to \((_{j})_{j 1}\) as the _"eigenvalues"_ with a little abuse of terminology.

With the component-wise loss function \(L_{j}(_{j})=(_{j}-z_{j})^{2},\) we can apply a gradient descent (gradient flow) with early stopping to derive a component-wise estimator \(_{j}.\) If we directly parameterize \(_{j}=_{j}^{}_{j}\) with only \(_{j}\) trainable, we obtain the vanilla gradient descent method, which is just the diagonalized version of the kernel gradient descent. The estimator is simply given by \(_{j}=(1-e^{-_{j}t})z_{j}\), where \(t\) is the stopping time, and its generalization error is easily computed as

\[(}^{ GF};^{*})=B_{ GF}^ {2}(t;^{*})+^{2}V_{ GF}(t)=_{j=1}^{}(e^{ -_{j}t}_{j}^{*})^{2}+^{2}_{j=1}^{}(1 -e^{-_{j}t})^{2}.\] (6)

We note here that these quantities also correspond to generalization error in the (fixed) kernel regression setting (Li et al., 2024). In particular, under the setting of (4) and \(_{j} j^{-}\), by choosing \(t^{-}{+}}\), we obtain the convergence rate \(^{}\), which is far from optimal if \(q\) is large.

### Over-parameterized gradient descent

By the discussion in the previous section, we find it essential to adjust the eigenvalues beyond the fixed ones \((_{j})_{j 1}\). Inspired by the over-parameterization nature of neural networks, we can also consider over-parameterization with gradient descent in our sequence model to train the eigenvalues: Replacing \(_{j}^{1/2}\) with trainable parameter \(a_{j}\), let us parameterize

\[_{j}=a_{j}_{j},\] (7)where \(a_{j}\) aims to learn the eigenvalues and \(_{j}\) aims to learn the signal. We consider the following gradient flow (simultaneously for each component \(j\)):

\[_{j}&=-_{a_{j}}L_{j}, _{j}=-_{_{j}}L_{j},\\ a_{j}(0)&=_{j}^{1/2},_{j}(0)=0. \] (8)

Here, \((_{j})_{j 1}\) serves as the initial eigenvalues, while the trainable parameters \((a_{j})_{j 1}\) are updated to adjust the eigenvalues during the training process.

To state our results with the most generality, let us introduce the following quantities on the target parameter sequence \(^{*}\):

\[J_{ sig}()\{j:|_{j}^{*}| \},()|J_{ sig}()|,()= _{j J_{ sig}()}(_{j}^{*})^{2}.\] (9)

The quantity \(()\) measures the number of significant components in the target parameter sequence \(\), while \(()\) measures the contribution of the insignificant components, which are commonly considered in the literature on the sequence model (Johnstone, 2017). For the concrete setting of (4), it is easy to show that

\[()^{-},()^{ }.\] (10)

Moreover, we make the following assumption on the span of the significant components.

**Assumption 1**.: There exists constants \( 0\) and \(C_{ sig}>0\) such that

\[ J_{ sig}() C_{ sig}^{-},>0.\] (11)

Assumption 1 says that the span of the significant components, namely those with \(|_{j}^{*}|\), grows at most polynomially in \(1/\). This assumption is mild and holds for many practical settings, such as cases considered in Example 2.3 (\(=\) for the first kernel). In other perspective, it imposes a mild condition on the misalignment between the ordering of the truth signal and the ordering of the eigenvalues, where \(\) measures the misalignment between the ordering of \(_{j}\) and the ordering of \(j\) itself. Then, the following theorem characterizes the generalization error of the resulting estimator.

**Theorem 3.1**.: _Consider the sequence model (5) under Assumption 1. Fix \(_{j} j^{-}\) for some \(>1\) and let \(}^{ Op}\) be the estimator given by the gradient flow (8) stopped at time \(t\). Then, there exists some constants \(B_{1},B_{2}>0\) such that when \(B_{1}^{-1} t B_{2}^{-1}\), we have_

\[(}^{ Op},^{*}) ^{2}[()+^{-1/}]+( (1/)) 0.\] (12)

### Towards deeper over-parameterization

Let us further introduce deeper over-parameterization by adding extra \(D\)-layers:

\[_{j}=a_{j}b_{j}^{D}_{j}\] (13)

and consider the gradient flow

\[_{j}&=-_{a_{j}}L_{j}, _{j}=-_{b_{j}}L_{j},_{j}=-_{_{j}}L_{j},\\ a_{j}(0)&=_{j}^{1/2}, b_{j}(0)=b_{0}>0,_{j}(0)=0,\] (14)

where \(b_{0}\) is the common initialization of all \(b_{j}\). We remark here if one considers the over-parameterization \(_{j}=a_{j}b_{j,1} b_{j,D}_{j}\) with the same initialization \(b_{j,k}=b_{0}\), \(k=1,,D\), then \(b_{j,k}\)'s remain to be the same by symmetry, so this is equivalent to our parameterization \(_{j}=a_{j}b_{j}^{D}_{j}\). The following theorem presents an upper bound for the generalization error by this deeper over-parameterized gradient flow.

**Theorem 3.2**.: _Consider the sequence model (5) under Assumption 1. Fix \(_{j} j^{-}\) for some \(>1\) and let \(}^{ Op,D}\) be the estimator given by the gradient flow (14) stopped at time \(t\). Then, by choosing \(b_{0}^{}\), there exists some constants \(B_{1},B_{2}>0\) such that when \(B_{1}^{-1} b_{0}^{D}t B_{2}^{-1}\), we have_

\[(}^{ Op,D},^{*}) ^{2}[()+^{-} ]+((1/))  0.\] (15)

### Discussion of the results

Benefits of Over-parameterizationTheorem 3.1 and Theorem 3.2 demonstrate the advantage of over-parameterization in the sequence model. Compared with the vanilla fixed-eigenvalues gradient descent method, the over-parameterized gradient descent method can significantly improve the generalization performance by adapting the eigenvalues to the truth signal. For a more concrete example, if we consider the setting of (4), plugging (10) yields the following corollary.

**Corollary 3.3**.: _Consider the over-parameterized gradient descent in (8) (setting \(D=0\)) or (14). Suppose (4) holds and \(_{j} j^{-}\) for \(>1\) and \(\). Then, by choosing \(b_{0}^{}\) (if \(D 0\)) and \(t^{-}\), we have_

\[(}^{},^{*}) ^{}((1/))^{} 0.\] (16)

_In comparison, the vanilla gradient flow method yields the rate \(^{}\)._

Ignoring the logarithmic factor, Corollary 3.3 shows that the over-parameterized gradient descent method can achieve a nearly optimal rate \(^{}\), while the vanilla gradient descent method only achieves the rate \(^{}\). The improvement is significant when \(q\) is large, which corresponds to the case that the misalignment between the ordering of the truth signal and the ordering of the eigenvalues is severe. Moreover, if we return to the low-dimensional regression function in Example 2.2 with the isotropic kernel \(k_{1}\), we can see that while the vanilla gradient descent method suffers from the curse of dimensionality with the rate \(\), the over-parameterization leads to the dimension-free rate \(}\). Therefore, the over-parameterization significantly improves the generalization performance.

Learning the eigenvaluesTo further investigate how the eigenvalues are adapted by over-parameterized gradient descent, we present the following proposition.

**Proposition 3.4**.: Given the same conditions as in Theorem 3.2 or Theorem 3.1 (with \(D=0\) and \(b_{j}^{D}=1\) for Theorem 3.1), the term learning the eigenvalues \(a_{j}(t)b_{j}^{D}(t)\) is non-decreasing in \(t\) for each \(j\). Moreover, letting \((0,1)\), when \(\) is small enough, the following holds at time \(t\) chosen as in Theorem 3.1 or Theorem 3.2:

* Signal component: There exist constants \(C,c>0\) such that for any component satisfying \(^{*}} C(1/)\), it holds with probability at least \(1-\) that \[a_{j}(t)b_{j}^{D}(t) c^{*}}^{}.\] (17)
* Noise component: There exist constants \(c,C,C^{}>0\) such that, for any component where \(^{*}}\) and \(_{j} c^{}\), it holds with probability at least \(1-\) that \[a_{j}(t)b_{j}^{D}(t) C_{j}^{}^{}=C ^{}a_{j}(0)b_{j}^{D}(0).\] (18)

From this proposition, we can see that for the signal components, the eigenvalues are learned to be at least a constant times a certain power of the truth signal magnitude. Thus, over-parameterized gradient descent adjusts the eigenvalues to match the truth signal as expected. In the case of noise components, although the eigenvalues are still increasing due to the training process, the eigenvalues do not exceed the initial values by some constant factor, provided that \(_{j}\) is relatively small. This finding suggests that over-parameterized gradient descent effectively adapts eigenvalues to the truth signal while mitigating overfitting to noise. We remark that when \(_{j}\) is relatively large, the method still tends to overfit the noise components, contributing an extra \(^{-}\) term in the generalization error, but this term becomes negligible for large \(\). Moreover, we also remark that there is a \((1/)\) gap between the signal and noise components. This is because the signal and the noise can not be distinguished for the components in the middle.

Adaptive choice of the stopping timeA notable advantage of the over-parameterized gradient descent method is its adaptivity. Consider the scenario described by (4), vanilla gradient descent requires the selection of a stopping time \(t^{-(2q)/(p+q)}\) to achieve the optimal convergencerate. However, this choice of stopping time critically depends on the unknown parameters \(p\) and \(q\) of the truth parameter, posing a significant challenge in practical applications. In contrast, the over-parameterized gradient descent only need to choose the stopping time as \(t^{-}\), which does not rely on the unknown truth parameters, while still achieving the nearly optimal convergence rate. This independence from the truth parameters allows the over-parameterization approach to adaptively accommodate any truth parameter structure by employing a fixed stopping time, without the need for prior knowledge about the truth function's properties.

Effect of the depthThe results in Theorem 3.2 also show that deeper over-parameterization can further improve the generalization performance. In the two-layer over-parameterization, the extra term \(^{-1/}\) in Theorem 3.1 emerges due to the limitation of the adapting large eigenvalues. With the introduction of depth, namely adding extra \(D\) layers to the model with proper initialization, this term can be improved to \(^{-}\) in Theorem 3.2. This improvement suggests that the depth can refine the model's sensitivity to eigenvalue adaptation, enabling a more nuanced adjustment to the underlying signal structure. This finding underscores the importance of model depth in enhancing the learning process, providing also theoretical evidence for the empirical success of deep learning models.

Comparison with previous worksWe will compare our results with the existing literature (Zhao et al., 2022; Li et al., 2021; Vaskevicius et al., 2019) on the generalization performance of over-parameterized gradient descent in the following aspects:

* **Problem settings:** While the existing literature (Zhao et al., 2022; Li et al., 2021; Vaskevicius et al., 2019) investigate the realms of high-dimensional linear regression, focusing on implicit regularization and sparsity, the present study dives into kernel regression and its approximation by Gaussian sequence models, emphasizing the adaptivity of over-parameterization to the underlying signal's structure, a leap towards understanding model complexity beyond mere regularization. Moreover, while the literature primarily focuses on the setting of strong signal, weak signal and noise separation, we consider the more general setting of the sequence model with arbitrary signal and noise components.
* **Over-parameterization setup:** The existing work Zhao et al. (2022) considers the over-parameterization setup by the two-layer Hadamard product \(=a b\) where the initialization is the same for each component that \(a(0)=\) and \(b(0)=\). In comparison, our work considers initializing the eigenvalues \(a_{j}(0)=_{j}^{1/2}\) differently for each component. Moreover, we extend the over-parameterization to deeper models by adding extra \(D\) layers. Although Vaskevicius et al. (2019) and the subsequent work Li et al. (2021) also consider the deeper over-parameterization, their over-parameterization is in the form of \(=u^{ D}-v^{ D}\) with \(u(0)=v(0)=\). Unfortunately, though being easy to analysis because of the homogeneous initialization, this setup could not bring insights into the learning of the eigenvalues, which is the key to our results. Furthermore, the analysis for Theorem 3.2 involves the interplay between the differently initialized \(a_{j}\) and \(b_{j}\), so our analysis is more involved than the existing works. We also remark that although we only consider the gradient flow in the analysis, the results can be extended to the gradient descent with proper learning rates.
* **Interpretation of the over-parameterization:** The previous works view the over-parameterization mainly as a mechanism for implicit regularization, while our work provides a novel perspective that over-parameterization adapts to the structure of the truth signal by learning the eigenvalues. Our theory also aligns with the neural network literature (Yang and Hu, 2022; Ba et al., 2022), where over-parameterization with gradient descent is known to be beneficial in learning the structure of the target function.
* **Connection to sparse recovery:** Our results can be phrased for the setting of high dimensional regression with sparsity. Taking a sparse signal \((_{j}^{*})_{j 1}\), e.g., \(_{j}^{*}=1\) for \(j S\), \(|S|=s\) and \(_{j}^{*}=0\) for \(j S\), we find that \(()=s\) and \(()=0\). Consequently, ignoring the extra error term, the resulting rate obtained by Theorem 3.1 or Theorem 3.2 is \((s/n)\) (ignoring the logarithmic factor). This rate coincides with the minimax rate for sparse recovery in high-dimensional regression.

### Proof outline

In this subsection, we will provide an outline of the proof of Theorem 3.1 and Theorem 3.2. For the detailed proof, we refer to Section D for the analysis of the gradient flow equation and Section E for the generalization error.

Equation analysisThe proof of Theorem 3.1 and Theorem 3.2 relies on the analysis of the gradient flow (8) and (14) for each component \(j\). For notation simplicity, we will suppress the index \(j\) in the following discussion. Firstly, the symmetry of the equation allows us to consider only the case \(z>0\). Then, one can find that

\[}{t}a^{2}=}{t}^{2}= }{t}b^{2}=2(z-),\]

so we get the conservation quantities \(a^{2}(t)^{2}(t)+\) and \(b^{2}(t) D^{2}(t)+b_{0}^{2}\).

Consequently, for the case \(D=0\), we can obtain the explicit gradient flow of \(\):

\[=^{4}+4^{2}}(z-),(0)=0.\]

Since \(^{4}+4^{2}}\) can be bounded by a multiple of \(a_{0}^{2}+2\), we can consider the other equation \(=(a_{0}^{2}+2)(z-)\), which admits a closed-form solution.

For the case \(D 1\), the equation is more complicated. We will apply a multiple stage analysis concerning both the effect of \(a(t)\) and \(b(t)\).

The generalization errorIn terms of the generalization error, we first separate the noise case when \(|_{j}||_{j}^{*}|/2\) and the signal case when \(|_{j}|<|_{j}^{*}|/2\). For the noise case, we apply the analysis of the equation to show that \(_{j}(t)\) is bounded roughly by \(_{j}\) for our choice of \(t\). Moreover, the fact that \(_{j}\) is summable ensures that error of these noise components does not sum up to infinity. On the other hand, for the signal case, if \(|_{j}^{*}|(1/)\), we can show that our choice of \(t\) allows \(_{j}(t)\) to exceed \(z/2\) and converge to \(z\) close enough, so the error in these components is only caused by the random noise and sum up to \(^{2}()\). In addition, the remaining signal components contribute to the error term \(((1/))\). Summing up these two terms, we can obtain the desired generalization error bound.

## 4 Numerical Experiments

In this section, we provide some numerical experiments to validate the theoretical results. For more detailed numerical experiments, please refer to Section C.

We approximate the gradient flow equation (22) and (30) by discrete-time gradient descent and truncate the sequence model to the first \(N\) terms for some very large \(N\). We consider the settings as in Corollary 3.3 that \(^{*}\) is given by (4) for some \(p>0\) and \(q 1\). We set \(^{2}=n^{-1}\), where \(n\) can be regarded as the sample size, and consider the asymptotic generalization error rates as \(n\) grows.

We first compare the generalization error rates between vanilla gradient descent and over-parameterized gradient descent (OpGD) in Figure 1 on page 10. The results show that the over-parameterized gradient descent can achieve the optimal generalization error rate, while the vanilla gradient descent suffers from the misalignment caused by \(q>1\) and thus has a slower convergence rate. Moreover, with a logarithmic least-squares fitting, we find that the resulting generalization error rates also match the theoretical results in Corollary 3.3 (\(0.5\) for OpGD and \(0.33\) for vanilla GD).

Additionally, we investigate the evolution of the eigenvalue terms \(a_{j}(t)b_{j}^{D}(t)\) over time \(t\) as discussed in Proposition 3.4. The results are shown in Figure 2 on page 10. We find that the eigenvalue terms can indeed adapt to the underlying structure of the signal: for large signals, the eigenvalue terms approach the signals as the training progresses, while for small signals, the eigenvalue terms do not increase significantly. Moreover, we find that deeper over-parameterization reduces the fluctuations of the eigenvalue terms for the noise components, and thus improves the generalization performance of the model.

In summary, the numerical experiments validate our theoretical results and provide insights into the adaptivity and generalization properties of the over-parameterized gradient descent method.

## 5 Conclusion and Future Work

In this work, we studied the generalization properties of over-parameterized gradient descent in the context of sequence models. We showed that the over-parameterization method can adapt to the underlying structure of the signal and significantly outperform the vanilla fixed-eigenvalues method. These results provide a new perspective on the benefits of over-parameterization and offer insights into the adaptivity and generalization properties of neural networks beyond the kernel regime.

However, there are also limitations of this work and many interesting directions for future research. For example, one can directly consider the over-parameterization in the kernel regression by replacing the feature map \((x)=(_{j}^{1/2}e_{j}(x))_{j 1}\) with the learnable one \((x;)=(a_{j}e_{j}(x))_{j 1}\), where \(a_{j}\)'s are learnable parameters initialized by \(a_{j}(0)=_{j}^{1/2}\). However, the analysis would be more challenging since now the components are mutually coupled in the gradient flow dynamics.

Perhaps one of the most interesting directions is to study how the over-parameterization method can also learn the eigenfunctions of the kernel during the training process, which leads to the truly "adaptive kernel method". We believe that future studies on this topic will provide a deeper theoretical understanding of the success of neural networks in practice.

Figure 1: Comparison of the generalization error rates between vanilla gradient descent and over-parameterized gradient descent (OpGD). We set \(p=1\) and \(q=2\) for the truth parameter \(^{*}\), and \(=1.5\) for the left column and \(=3\) for the right column. For each \(n\), we repeat \(64\) times and plot the mean and the standard deviation.

Figure 2: The evolution of the trainable eigenvalues \(a_{j}(t)b_{j}^{D}(t)\) over the time \(t\) across components \(j=100\) to \(200\) for \(D=1\). The blue line shows the eigenvalues and the black marks show the non-zero signals scaled according to Proposition 3.4. For the settings, we set \(p=1\), \(q=2\) and \(=2\).