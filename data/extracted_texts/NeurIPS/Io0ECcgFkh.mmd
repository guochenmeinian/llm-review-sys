# RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation

Kaiqu Liang\({}^{1}\), Haimin Hu\({}^{2}\), Ryan Liu\({}^{1}\), Thomas L. Griffiths\({}^{1,3}\),

**Jaime Fernandez Fisac\({}^{1,2}\)**

\({}^{1}\)Department of Computer Science, Princeton University

\({}^{2}\)Department of Electrical and Computer Engineering, Princeton University

\({}^{3}\)Department of Psychology, Princeton University

{kl2471,haiminh,ryanliu,tong,jfisac}@princeton.edu

###### Abstract

Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely on _immediate_ feedback, which can fail to reflect the true downstream impact of an interaction on users' utility. We demonstrate that this horstighted feedback can, by itself, result in misaligned behaviors like sycophancy and deception, and we propose to alleviate this by refocusing RLHF on _downstream consequences_. Our theoretical analysis reveals that the hindsight gained by simply delaying human feedback mitigates misalignment and improves expected human utility. To leverage this insight in a practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods--Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO)--and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF.

## 1 Introduction

Aligning artificial intelligence (AI) systems with human values and intentions is crucial to ensuring they behave in ways that are helpful, honest, and trustworthy. A widely-deployed method for achieving this alignment is through human feedback (Leike et al., 2018), with successful applications to, e.g., training AI assistants (Glaese et al., 2022; Touvron et al., 2023; Anthropic, 2023; Achiam et al., 2023). In particular, Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Stiennon et al., 2020) leverages human feedback to fine-tune and align foundation models (FMs). While RLHF has shown promise in aligning models with human preferences, it often relies heavily on human perceptions during interactions, which may not accurately reflect the downstream consequences of the service provided. Such myopic feedback can misguide the model's behavior and limit its effectiveness in aligning with human values. For example, human evaluators could misjudge an interaction on the spot, due to limited resources (e.g., partial observability; Casper et al., 2023; Lang et al., 2024) or limited bandwidth (e.g., constraints on time, attention, or care; Pandey et al., 2022; Chmielewski and Kucker, 2020), leading to incomplete or misinformed feedback. A recent theoretical study has suggested that partial observability in RLHF can lead to deceptive model behaviors (Lang et al., 2024). Complementing this analysis, we provide substantial empirical evidence that immediate human feedback frequently misrepresents true utility in everyday interaction settings and, when used as a proxy for it in RLHF fine-tuning, systematically results in misalignment with human goals. This misalignment often manifests as _positive illusion_(fabricating or exaggerating the good and omitting or downplaying the bad), where the model's behavior shifts towards momentarily pleasing the user rather than providing accurate and genuinely helpful advice. Unfortunately, this consistently leads users to make ill-informed decisions whose poor downstream outcomes contrast starkly with their high satisfaction rating at the end of the interaction.

Our central insight is that the utility provided by an AI system to a human user (and similarly its "helpfulness" and "harmlessness", which RLHF evaluators are typically asked to assess), is not generally an intrinsic property of the outputs that it generates, but rather a function of their real-world consequences, brought about by the human user's actions upon consuming said outputs. Evaluators presented with a human-AI interaction without explicit information about its later consequences must either neglect them or implicitly estimate them when providing their assessment. Unfortunately, neither option is suitable for the harder use cases in which human users truly need to rely on AI assistance, precisely the ones in which alignment is crucial, especially as AI capabilities continue to increase.

To address these open challenges, we propose to leverage _hindsight_ as a simple but effective misalignment mitigation mechanism, in which evaluators experience the downstream outcomes of an interaction before being asked for feedback on the model. We provide both theoretical analysis and extensive empirical studies to show the efficacy of hindsight in significantly reducing misalignment of RLHF-trained models. To circumvent the material and ethical difficulties in exposing real people to real consequences, we introduce a novel alignment algorithm called **R**einforcement **L**earning from **H**indsight **S**imulation (**RLHS**), an alternative to RLHF that rapidly simulates human decisions and their downstream outcomes during training, allowing the evaluator to directly assess the long-term impact of the model's outputs rather than relying on an implicit guess of its later outcomes.

Our key finding is that equipping evaluator feedback with the benefit of hindsight--even if this is simulated using imperfect models--can significantly reduce model misalignment with the evaluator's true utility, decreasing the chances of deceptive and misleading outputs. We implement hindsight simulation with both offline and online preference optimization approaches, including direct preference optimization (DPO) (Rafailov et al., 2024) and proximal policy optimization (PPO) (Schulman et al., 2017) and show empirically that it greatly improves alignment in both training paradigms. We also present results from human user studies, in which RLHS consistently improves both users' ground-truth utility and subjective satisfaction, despite being trained with only simulated hindsight feedback. Our comparative findings demonstrate that RLHS significantly outperforms non-hindsight methods--specifically Reinforcement Learning from AI Feedback (RLAIF), which similarly uses AI generation as a proxy for real human feedback, and has been shown to produce results closely resembling that of RLHF (Bai et al., 2022b; Lee et al., 2023).

Figure 1: **RLHF** can incentivize AI systems to provide inaccurate or deceptive information to their users, prioritizing positive on-the-spot feedback and neglecting long-term consequences. For example, a customer may prefer to hear good news while shopping but will ultimately be disappointed (and objectively worse off) if stuck with an ill-informed purchase. The proposed **RLHS** introduces hindsight for human feedback, focusing on evaluations after knowing the outcome. This enables more informed feedback that improves alignment between the AI and the humanâ€™s true utility.

## 2 Alignment Algorithm: RL from Hindsight Simulation

Recent studies have revealed that RLHF can result in misalignment when humans provide feedback based on _partial observations_, rather than the typically assumed--but rarely realistic--full state sequences. This limitation can lead to deceptive or manipulative behaviors in AI systems (Casper et al., 2023; Lang et al., 2024). To address misalignment caused by human uncertainty in RLHF, we propose Reinforcement Learning from Hindsight Simulation (RLHS). The core idea is that by providing humans with information about future outcomes, the learned reward and policy will be significantly better aligned. While the reminder of this paper focuses on the algorithm and results of RLHS, we provide a mathematical formulation of general human-AI alignment problems in Appendix A and prove that the hindsight feedback approaches the oracle one for a sufficiently large hindsight horizon in Appendix B, elucidating the advantage of RLHS over RLHF.

**Hindsight Simulation**. While we have demonstrated theoretically that providing hindsight can mitigate misalignment in RLHF, exposing humans to real consequences can circumvent material and ethical difficulties. To address this, we introduce the concept of _hindsight simulation_--the namesake of our core contribution, RLHS--which allows evaluators, whether human or AI, to make more informed decisions based on simulated outcomes. In practice, hindsight simulation can involve collecting feedback from human evaluators or employ another language model as a proxy to simulate the feedback process. After an evaluator makes a decision based on their interaction with the AI (e.g., purchasing an item), the system provides _groundtruth_ information about the outcome, i.e., the hindsight (e.g., whether the purchased item meets the desired criteria). The evaluator then provides feedback informed by both the decision's outcome and their prior interaction with the model.

This feedback typically takes the form of a rating or binary preference, similar to the feedback used in conventional RLHF. However, unlike the _immediate_ feedback provided solely during an interaction without access to the decision's consequences, feedback obtained through hindsight simulation is more informed as it incorporates long-term outcomes. This aligns with the reasoning presented in Appendix B.1 and demonstrates the potential for improving alignment through simulated hindsight.

We implement this approach with two subroutines: (i) _partial hindsight_, where only a limited set of hindsight information is available to the agent, in a way that more closely matches real-world scenarios, and (ii) _oracle hindsight_, where the agent has access to full set of hindsight information. We hope that through our subsequent empirical study employing both partial and oracle hindsight, we can gain insights into how extending the hindsight step (i.e., revealing additional outcome information to the agent) can improve the alignment performance of the model.

**Illustrative Example: Marketplace Chatbot.** We demonstrate the practical impact of RLHS by applying it to a marketplace AI chatbot. The chatbot's goal is to assist customers in making purchasing decisions by providing recommendations based on available product information. We assume that both customers and the chatbot have access to some public information, such as a list of items and their prices, but customers have their internal preferences, e.g., wanting a TV with 8K resolution, that are unknown to the chatbot. To the best of our knowledge, existing RLHF schemes deployed for training marketplace chatbots (e.g., Amazon, 2024) use customer feedback solely based on the interaction (i.e., if they feel happy about the chatbot's service) but not on the outcome (i.e., if the purchased item has actually met their preferences), potentially causing misalignment.

Our proposed hindsight simulation approach aims to mitigate this issue by deferring the humans' feedback until they have been informed of the outcome of their decisions, e.g., they have received the product and verified that their expectations are (not) met. In hindsight simulation, the simulated customer interacts with the chatbot, makes a purchasing decision, checks the outcome (hindsight) provided by the system, and provides feedback on the customer's satisfaction with the service.

## 3 Experimental Design

### Data Collection

**Preference Data Collection.** Our training data collection process closely follows the standard RLHF data collection pipeline (Stiennon et al., 2020; Ouyang et al., 2022), where feedback is collected based on comparisons between outputs. However, instead of relying on real human feedback, weemployed a strong large language model (LLM) model as a judge to simulate human interactions with the chatbot and provide feedback. For real-world online marketplace chatbots like the Amazon Rufus (Amazon, 2024), human feedback is typically given as a rating at the end of the interaction. However, human users tend to compare their current experience with previous ones when assigning ratings. To capture this behavior, we simulate users comparing services from two different stores and selecting their preferred option, rather than rating each scenario in isolation. This closely aligns with the preference-based data collection method used in prior work (Stiennon et al., 2020; Ouyang et al., 2022), where users provide feedback by comparing responses instead of giving individual ratings.

**Decision-making simulation.** While collecting the preference data, our simulated human (strong model) takes on three roles: interacting with the chatbot, making decisions, and providing feedback. To ensure accurate decision-making and feedback, we adapted the approach in introspective planning (Liang et al., 2024). First, we frame the decision-making problem as a multiple-choice question with four options: (A) Buy option A, (B) Buy option B, (C) Buy option C, or (D) Do not buy anything. We then ask the LLMs to perform Chain-of-Thought reasoning (Wei et al., 2022), querying the next token probabilities to select the best option from \(A,B,C,D\). This approach can reduce the language agent's uncertainty. We apply a similar method for comparing services between two stores.

**Dataset Details.** In our experiments, we used both Llama-2-7B (Touvron et al., 2023) and Llama-3-8B (Dubey et al., 2024) as the AI assistants, and Llama-3.1-70B (Dubey et al., 2024) as the simulated human to interact with the AI assistant and provide feedback. We collected **11,000** preference data points for each AI assistant model, with 10,000 used for training and 1,000 for validation. We also generated a test set of **1,200** examples to evaluate performance across different customer scenarios.

### Experiment Setup

**Environment Details.** In each of our simulated marketplace scenarios there are 10 candidate items, each characterized by 8 features and a price. Each feature can be categorized in two ways: (1) The item either has or lacks a specific feature (e.g., a TV with HDR vs. without HDR), and (2) The feature

Figure 2: **Qualitative results for Llama-2-7b trained with either immediate feedback (RLHF) or partial hindsight (RLHS). The model trained with immediate feedback leads to deception by falsely claiming that both Options A and C meet the customerâ€™s 8K resolution requirement, when in fact, neither does. In contrast, the model trained with partial hindsight truthfully states that none of the available options include 8K resolution.**

[MISSING_PAGE_FAIL:5]

## 4 Simulation Results

**Misalignment between satisfaction rating and real utility.** When using standard RLHF (Ouyang et al., 2022), we observe significant misalignment between user satisfaction ratings and true utility as training progresses (left plot in Figs. 3 and 4). While the satisfaction rating steadily increases, indicating that the language model is learning to deliver responses that receive higher immediate user approval, the true utility shows a sharp decline. This suggests that while the chatbot's responses may appear more polished or helpful in the short term, they are in fact becoming less aligned with the user's true needs or long-term goals. As a result, while users may initially perceive the chatbot's responses as helpful, they are frequently misled and ultimately dissatisfied with their final outcomes. This highlights a fundamental flaw in using standard RLHF with immediate feedback, as it risks optimizing for superficial satisfaction at the expense of true utility.

**Hindsight simulation effectively mitigates misalignment.** As shown in Fig. 3 (left), relying on immediate feedback leads to a steady decline in real utility, ultimately resulting in negative overall utility. In contrast, hindsight simulation consistently improves utility throughout training, eventually achieving positive utility, as in Fig. 3 (middle). It aligns upward trends in both real utility and satisfaction ratings, significantly reducing the gap between them. The qualitative results shown in Fig. 2 further support our claim. When the AI assistant is trained on immediate feedback, it deceptively claims that both Options A and C meet the requirements of the (simulated) customer for 8K resolution, though neither actually does. In contrast, training with partial hindsight leads to truthful responses, acknowledging that none of the options meet the 8K resolution requirement. This shows that while traditional RLHF with immediate feedback may cause misalignment, hindsight simulation mitigates this issue, improving the overall helpfulness and honesty of language agents.

## 5 Human Study

Our human study had three goals: (Goal 1) evaluate the performance of models trained with immediate feedback vs. those trained with hindsight simulation, (Goal 2) assess how hindsight information affects user satisfaction. To achieve these goals, we designed two similar human experiments. Both experiments used Llama-3-8b (Dubey et al., 2024) trained with DPO using either immediate feedback or partial hindsight. We conducted online human experiments via Prolific (Palan and Schitter, 2018), involving 200 participants across 10 scenarios, randomly sampled from a test set of 1,200. For each scenario, 20 participants took part: 10 interacting with each of the RLHF model and the RLHS model.

**Pipeline for evaluating model performance.** The first and second experiments follow the same pipeline but differ in the models used--one is trained with immediate feedback, and the other with partial hindsight simulation--allowing us to compare their performance (Goal 1). Initially, participants are shown a list of available items in a store with hidden features. We specify their requirements for the item (e.g., "must have 8K resolution"). Participants interact with the chatbot to gather information about the products. At each step, they can choose one of the following actions: "ask about the desired feature," "ask about the price", or "ready to make a decision". Pre-generated responses are provided for inquiries. In the second round of interaction, participants may ask about the information they didn't request in the first round. At any point, participants can choose "ready to

Figure 4: **Results on Llama-2-7b trained with DPO.**_Left:_ _Demonstrates the Misalignment of real utility and satisfaction ratings using immediate feedback._Middle:_ _Shows how partial hindsight mitigate the misalignment._Right:_ _Shows the alignment achieved with oracle hindsight._make a decision", at which time they must decide whether to make a purchase decision or opt not to buy. After making their decision, they provide an immediate satisfaction rating.

Hindsight information is then introduced. Buyers learn whether the item meets their requirements (e.g., whether the item has the desired feature) while non-buyers receive no additional information. Participants then provide a second satisfaction rating, referred to as the hindsight rating, which evaluates their long-term satisfaction after considering the hindsight information. This step allows us to assess the impact of hindsight information on user satisfaction (Goal 2). Finally, buyers may keep or return the item, enabling us to quantify the regret rate.

**Statistical Hypothesis Testing.** We conducted experiments to test four hypotheses, using one-tailed and standard t-tests for the first three hypotheses (Fisher, 1970), and Pearson's correlation coefficient for the fourth (Sedgwick, 2012). The one-tailed t-test framework used in Hypotheses 1, 2, and 3 is outlined below. The null hypothesis (\(H_{0}\)) and the alternative hypothesis (\(H_{1}\)) are defined as:

\[H_{0}:_{1}_{2}\] (Group 1 satisfaction is less than or equal to Group 2) \[H_{1}:_{1}>_{2}\] (Group 1 satisfaction is significantly higher than Group 2)

Here, \(_{1}\) and \(_{2}\) represents the mean satisfaction of Group 1 and Group 2, respectively. The two-tailed t-test follows a similar format but tests for any significant difference between the group means.

_Hypothesis 1: Models trained with RLHS lead to a higher long-term user satisfaction rate and lower regret rate than those trained with RLHF using immediate feedback._

We evaluated hindsight ratings for two models: Group 1 (RLHS) and Group 2 (RLHF). The hypothesis test resulted in \(p=4 10^{-8}\), well below the significance threshold of 0.001. When reversing the groups for regret rates, the test yielded \(p=5 10^{-5}\) again below 0.001.

_Hypothesis 2: Models trained with RLHF using immediate feedback often experience a notable decline in user satisfaction once future outcomes are revealed, while RLHS mitigate this decline._

Group 1 consisted of users interacting with RLHF without hindsight feedback, and Group 2 received hindsight feedback. The hypothesis test gave \(p=4 10^{-9}\), confirming a significant decline. To demonstrate RLHS mitigates this decline, we ran a two-tailed t-test comparing immediate and hindsight ratings. The result, \(p=0.90\), showed no significant difference.

_Hypothesis 3: RLHS lead to significantly higher true utility than RLHF._

We assessed the objective performance of the two models by comparing true utility scores for Group 1 (RLHS) and Group 2 (RLHF). The hypothesis test yielded \(p=4 10^{-8}\), confirming that RLHS achieves significantly higher true utility than RLHF.

_Hypothesis 4: Models trained with RLHS are more truthful, presenting a strong correlation between their high immediate user satisfaction rate (subjective) and high true utility (objective)._

To evaluate the correlation, we used Pearson's correlation coefficient and tested whether this coefficient was significantly different from zero. The null hypothesis (\(H_{0}\)) assumed no correlation (i.e., \(r=0\)) while the alternative hypothesis (\(H_{1}\)) assumed a non-zero correlation. The test found a significant correlation between immediate ratings and true utility for RLHS (\(p=5 10^{-4}\)), while no significant correlation was observed for RLHF (\(p=0.47\)).

Figure 5: The policy trained using the proposed RLHS outperforms that of RLHF in both true utility (_left_) and hindsight rating (_right_). In both plots, each point represents the mean ratio for a scenario, with lines indicating the standard deviation. The identity line is plotted in dashed grey.

**Analysis.** These results validated Hypotheses 1 and 2 with subjective improvements in user satisfaction and regret for RLHS over RLHF, while Hypothesis 3 was verified with the objective improvement in the true utility. We also see from the results a strong alignment between subjective satisfaction and objective utility for the RLHS model, thus validating Hypothesis 4. In addition to the statistical significance tests, we visualize the metrics in Table 2, which shows that training with hindsight simulation (RLHS) achieves a higher long-term satisfaction score (3.71) compared to immediate feedback (RLHF), which only reaches 2.65, supporting Hypothesis 1. Further, RLHF obtained a high immediate rating of 3.74 before hindsight, but it then dropped significantly to 2.65 after the outcome is revealed, thereby validating Hypothesis 2. While both models achieved similar immediate ratings, RLHS achieves a significantly higher true utility (0.43). These results confirm that RLHF can lead to misalignment, whereas RLHS mitigates this, resulting in a more helpful and truthful language agent. We also visualize the utility and rating ratio for each scenario between RLHS and RLHF in Fig. 5.

## 6 Conclusion

In this work, we introduced Reinforcement Learning from Hindsight Simulation (RLHS), an algorithmic framework that mitigates misalignment in RLHF by providing evaluators with future outcome information. We demonstrated that RLHS can significantly improve utility compared to existing RLHF pipelines that rely only on immediate feedback, while maintaining a high user satisfaction rate throughout the human-AI interaction. While our study focused on simulated hindsight with an application to marketplace chatbot, future work should explore incorporating hindsight in RLHF for additional real-world applications with real human evaluators. Further, we see an open opportunity to equip RLHS with other feedback modalities, such as visual cues, which could further enrich the feedback process and improve alignment.