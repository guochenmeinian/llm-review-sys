# Hybrid Top-Down Global Causal Discovery

with Local Search

for Linear and Nonlinear Additive Noise Models

 Sujai Hiremath

Cornell Tech

sh2583@cornell.edu

&Jacqueline Maasch

Cornell Tech

jam887@cornell.edu

&Mengxiao Gao

Tsinghua University

gaomx21@mails.tsinghua.edu.cn

&Promit Ghosal

University of Chicago

promit@uchicago.edu

&Kyra Gan

Cornell Tech

kyragan@cornell.edu

###### Abstract

Learning the unique directed acyclic graph corresponding to an unknown causal model is a challenging task. Methods based on functional causal models can identify a unique graph, but either suffer from the curse of dimensionality or impose strong parametric assumptions. To address these challenges, we propose a novel hybrid approach for global causal discovery in observational data that leverages local causal substructures. We first present a topological sorting algorithm that leverages ancestral relationships in linear structural causal models to establish a compact top-down hierarchical ordering, encoding more causal information than linear orderings produced by existing methods. We demonstrate that this approach generalizes to nonlinear settings with arbitrary noise. We then introduce a nonparametric constraint-based algorithm that prunes spurious edges by searching for local conditioning sets, achieving greater accuracy than current methods. We provide theoretical guarantees for correctness and worst-case polynomial time complexities, with empirical validation on synthetic data.

## 1 Introduction

Causal graphical models compactly represent the _data generating processes_ (DGP) of complex systems, including physical, biological, and social domains. Access to the true causal graph or its substructures can offer mechanistic insights [31; 13] and enable downstream causal inference, including effect estimation [11; 34; 2; 8; 16; 35]. In practice, the true causal graph is often unknown, and can be challenging to assume using domain knowledge. In such limited-knowledge settings, we can instead rely on causal discovery algorithms that learn the causal graph from observational data in a principled, automated manner [41; 7].

Traditional approaches to causal discovery infer causal relationships either through conditional independence relations (PC ) or goodness-of-fit measures (GES , GRaSP ). While these discovery methods can provide flexibility by not requiring assumptions over the functional form of the DGP, they are generally worst-case exponential in time complexity and learn Markov equivalence classes (MEC) rather than unique _directed acyclic graphs_ (DAGs) . Therefore, additional modeling assumptions are often necessary for time-efficient and accurate global discovery.

Certain parametric assumptions can enable recovery of the unique ground truth DAG, e.g., assuming a particular _functional causal model_ (FCM) . Under the _additive noise model_ (ANM), we obtain unique identifiability by assuming linear causal mechanisms with non-Gaussian noise distributions[37; 36] or nonlinear causal functions with arbitrary noise . Under the independent additive noise assumption, the causal parents of a variable are statistically independent of its noise term. For this class of models, discovery often entails regressing the variable of interest against its hypothesized parent set and testing for marginal independence between this set and the residual term .

Current FCM approaches to global causal discovery trade off between two main issues, suffering from either 1) strong parametric assumptions over the noise or functional form (or both) or 2) the use of high-dimensional nonparametric regressions, which require large sample sizes for reliable estimation and do not scale to large graphs. In addition, current FCM-based methods are ill-suited for causal discovery in sparse causal graphs, a setting that characterizes many high-dimensional applications (e.g., analysis of genetic data in healthcare applications) [6; 47; 46; 15].

**Contributions** We propose a hybrid causal discovery approach to graph learning that combines functional causal modeling with constraint-based discovery. We depart from previous methods by characterizing conditions that allow us to search for and exploit local, rather than global, causal relationships between vertices. These local relationships stem from root vertices: this motivates a top-down, rather than bottom-up, approach. Thus, we learn the topological sort and discover true edges starting from the roots rather than the leaves, as in existing methods [25; 30; 21]. This approach leverages sparsity in both the ordering phase and edge discovery phase to reduce the size of conditioning sets, as well as the number of high-dimensional regressions. We summarize our major contributions as follows:

* We introduce a topological ordering algorithm LHTS for linear non-Gaussian ANMs that exploits local ancestor-descendent relationships to obtain a compact hierarchical sort.
* We introduce a topological ordering algorithm NHTS for nonlinear ANMs that exploits local parent-child relationships to run fewer high-dimensional regressions than traditional methods, achieving lower sample complexity.
* We introduce a constraint-based algorithm ED that nonparametrically prunes spurious edges from a discovered topological ordering, leveraging local properties of causal edges to use smaller conditioning sets than traditional sparse regression techniques.
* We achieve accurate causal discovery in synthetic data, outperforming baseline methods.

**Organization** After describing the preliminaries in Section 2, we introduce the linear problem setting in Section 3, establishing the connection between ancestral relationships and causal active paths and introducing a _linear hierarchical topological sorting_ algorithm (LHTS). Next, we extend our method to the nonlinear setting in Section 4 by establishing the connection between parental relationships and active causal paths, introducing a _nonlinear hierarchical topological sorting_ algorithm (NHTS). In section 5, we establish a sufficient conditioning set for determining edge relations and introduce an efficient _edge discovery_ algorithm (ED). We then test LHTS, NHTS and ED in synthetic experiments in Section 6. To conclude, we discuss future work that might generalize our approach to full ANMs.

**Related Work** Our work is related to two kinds of discovery methods that explicitly leverage the topological structure of DAGs: 1) permutation-based approaches, and 2) FCM-based approaches.

The original permutation-based approach SP  searches over the space of variable orderings to find permutations that induce DAGs with minimal edge counts. Authors in  introduce greedy variants of SP (such as GSP) that maintain asymptotic consistency; GRaSP  relaxes the assumptions of prior methods to obtain improvements in accuracy. These methods highlight the importance of using permutations for efficient causal discovery, but generally suffer from the need to bound search runtime with heuristics, poor sample efficiency in high dimensional settings, and are unable to recover a unique topological ordering or DAG ().

On the other hand, the recent stream of FCM-based approaches decompose graph learning into two phases: 1) learning the topological sort, i.e., inferring a causal ordering of the variables; and 2) edge discovery, i.e., identifying edges consistent with the causal ordering [38; 25; 1; 30; 21; 32; 20].

The literature on topological ordering algorithms for ANMs is organized along the types of parametric assumptions made on both the functional forms and noise distributions of the underlying DGP. Early approaches like ICA-LiNGAM  and DirectLiNGAM  focus on learning DAGs generated by linear functions and non-Gaussian noise terms. Recent work leverages score matching to obtain the causal ordering in settings with nonlinear functions and Gaussian noise: SCORE  and DAS  exploit particular variance properties, while DiffAN estimates the score function with a diffusion model . NoGAM  generalizes the score-matching procedure of SCORE to nonlinear causal mechanisms with arbitrary noise distributions. RESIT  leverages residual independence results in nonlinear ANMs to identify topological orderings when the noise distribution is arbitrary. NoGAM and RESIT both rely on high-dimensional nonparametric regression.

Once a topological ordering is obtained, spurious edges are pruned. Works that are agnostic to the distribution of noise often use a parametric approach, implementing either a form of sparse regression (e.g., Lasso regression ) or a version of additive hypothesis testing with generalized additive models (GAMs)  (e.g., CAM-pruning ). RESIT  provides another alternative edge pruning procedure for nonlinear ANMs, relying again on high-dimensional nonparametric regression.

## 2 Preliminaries

We focus on _structural causal models_ (SCMs) represented as DAGs. These graphs describe the causal relationships between variables, where an edge \(x_{i} x_{j}\) implies that \(x_{i}\) has a direct causal influence on \(x_{j}\). Let \(G=(V,E)\) be a DAG on \(|V|=d\) vertices, where \(E\) represents directed edges. To define pairwise relationships between vertices, we let \((x_{i})\) denote the children of \(x_{i}\) such that \(x_{j}(x_{i})\) if and only if \(x_{i} x_{j}\), and \((x_{i})\) denote the parents of \(x_{i}\) such that \(x_{j}(x_{i})\) if and only if \(x_{j} x_{i}\). Similarly, let \((x_{i})\) denote the ancestors of \(x_{i}\) such that \(x_{j}(x_{i})\) if and only if there exists a directed path \(x_{j} x_{i}\), and \((x_{i})\) denote the descendants of \(x_{i}\) such that \(x_{j}(x_{i})\) if and only if there exists a directed path \(x_{i} x_{j}\). Vertices can be classified based on the totality of their pairwise relationships: \(x_{i}\) is a _root_ if and only if \((x_{i})=\), a _leaf_ if and only if \((x_{i})=\), an isolated vertex if \(x_{i}\) is both a root and a leaf, and an intermediate vertex otherwise. See an illustrative DAG in Figure 1. Vertices can also be classified in terms of triadic relationships: \(x_{i}\) is a confounder of \(x_{j},x_{k}\) if and only if \(x_{i}(x_{j})(x_{k})\); a mediator of \(x_{j}\) to \(x_{k}\) if and only if \(x_{i}(x_{j})(x_{k})\); and a collider between \(x_{j}\) and \(x_{k}\) if and only if \(x_{i}(x_{j})(x_{k})\).

Undirected paths that transmit causal information between vertices \(x_{j},x_{k}\) can be differentiated into _frontdoor_ and _backdoor paths_. A frontdoor path is a directed path \(x_{j} x_{k}\) that starts with an edge out of \(x_{j}\), and ends with an edge into \(x_{k}\). A backdoor path is a path \(x_{j} x_{k}\) that starts with an edge into \(x_{j}\), and ends with an edge into \(x_{k}\). Paths that start and end with an edge out of \(x_{j}\) and \(x_{k}\) (\(x_{k} x_{k}\)) do not transmit causal information between \(x_{j},x_{k}\).

Paths between two vertices are further classified, relative to a vertex set \(\), as either _active_ or _inactive_. A path between vertices \(x_{j},x_{k}\) is active relative to \(\) if every node on the path is active relative to \(\). Vertex \(x_{i}\) on a path is active if one of the following holds: 1) \(x_{i}\) is not a collider and \(x_{i}\), 2) \(x_{i}\) is a collider and \(x_{i}\), 3) \(x_{i}\) is a collider and \(x_{i}\), but \((x_{i})\). An inactive path is simply a path that is not active. Following convention, throughout the rest of the paper we will describe causal paths as active or inactive with respect to \(=\) unless otherwise specified.

**Definition 2.1** (Topological Orderings).: _Consider a given DAG \(G=(V,E)\). A topological sort (linear order) is a mapping \(:V\{0,1,,|V|-1\}\), such that if \(x_{i}(x_{j})\), then \(x_{i}\) appears before \(x_{j}\) in the sort \(\): \((x_{i})<(x_{j})\). A hierarchical sort (between a partial and linear order) is a mapping \(_{L}:V\{0,1,,|V|-1\}\), such that if \((x_{i})=\), then \(_{L}(x_{i})=0\), and if \((x_{i})\), then \(_{L}(x_{i})\) equals the maximum length of the longest directed path from each root vertex to \(x_{i}\), i.e., \(_{L}(x_{i})=1+\{_{L}(x_{j}):x_{j}(x_{i})\}\)._

We note that the hierarchical sort is unique, and that it coincides with a topological sort when the number of layers equals \(|V|\), i.e., the DAG is complete.

**Definition 2.2** (AnMs).: _ANMs  are a popular general class of SCMs defined over a DAG \(G\) with_

\[x_{i}=f_{i}((x_{i}))+_{i}, x_{i} V,\] (1)

_where \(f_{i}\)s are arbitrary functions and \(_{i}\)s are independent arbitrary noise distributions._

This model implicitly assumes the causal Markov condition and acyclicity; we adopt the aforementioned assumptions, as well as faithfulness .

## 3 Linear Setting

We first restrict our attention to ANMs that feature only linear causal functions \(f\), known as Linear Non-Gaussian Acyclic causal Models (LiNGAMs). Following , we note that a LiNGAM can be represented as a \(d d\) adjacency matrix \(B=\{b_{ij}\}\), where \(b_{ij}\) is the coefficient from \(x_{j}\) to \(x_{i}\). Note that, for any topological ordering \(\) of a LiNGAM, if \((x_{j})>(x_{i})\), then \(b_{ji}=0\). Thus, each \(x_{i} V\) admits the following compact representation: \(x_{i}=_{(x_{j})<(x_{i})}b_{ij}x_{j}+_{i}\).

**Identifiability** Identifiability conditions for LiNGAMs  primarily concern the distribution of errors \(_{i}\): under Gaussianity, distinct linear DGPs can admit the same joint distribution, making them impossible to distinguish. Shimizu et al.  generalize this intuition with _independent component analysis_ (ICA)  to provide a multivariate identifiability condition for LiNGAMs (see Appendix A.1). In this section, we adopt the aforementioned condition.

**Ancestral Relations and Active Causal Paths** We first establish the connection between ancestral relationships and active causal paths. We exhaustively enumerate and define the potential pairwise causal ancestral path relations in Figure 2 and Lemma 3.1 (proof in Appendix A.2):

**Lemma 3.1** (Active Causal Ancestral Path Relation Enumeration).: _Each pair of distinct nodes \(x_{i},x_{j} V\) can be in one of four possible active causal ancestral path relations: AP1) no active path exists between \(x_{i},x_{j}\); AP2) there exists an active backdoor path between \(x_{i},x_{j}\), but there is no active frontdoor path between them; AP3) there exists an active frontdoor path between \(x_{i},x_{j}\), but there is no active backdoor path between them; AP4) there exists an active backdoor path between \(x_{i},x_{j}\), and there exists an active frontdoor path between them._

Next, in Lemma 3.2, we summarize the connection between causal paths and ancestral relationships (proof in Appendix A.3):

**Lemma 3.2**.: _The ancestral relationship between a pair of distinct nodes \(x_{i},x_{j} V\) can be expressed using active causal path relations: \(x_{i},x_{j}\) are not ancestrally related if and only if they are in AP1 or AP2 relation; and \(x_{i},x_{j}\) are ancestrally related if and only if they are in AP3 or in AP4 relation._

The active causal ancestral path relation of a pair of nodes \(x_{i},x_{j}\) that are not ancestrally related can be determined through marginal independence testing and sequential univariate regressions as illustrated in Lemmas 3.3 and 3.4 (proofs in Appendices A.4, A.5):

**Lemma 3.3** (Ap1).: _Vertices \(x_{i},x_{j}\) are in AP1 relation if and only if \(x_{i}\!\!\! x_{j}\)._

**Lemma 3.4** (Ap2).: _Let \(M\) be the set of mutual ancestors between a pair of vertices \(x_{i}\) and \(x_{j}\), i.e., \(M=(x_{i})(x_{j})\). Let \(x_{i}^{M},x_{j}^{M}\) be the result of sequentially regressing all mutual ancestors in \(M\) out of \(x_{i},x_{j}\) with univariate regressions, in any order. Then, let \(r_{i}^{j}\) be the residual of \(x_{i}^{M}\) regressed on \(x_{i}^{M}\), and \(r_{j}^{i}\) be the residual of \(x_{i}^{M}\) regressed on \(x_{j}^{M}\). Suppose \(x_{i}\!\!\! x_{j}\). Then, \(x_{i},x_{j}\) are in AP2 relation if and only if \(r_{i}^{j}\!\!\! x_{i}^{M}\) and \(r_{j}^{i}\!\!\! x_{j}^{M}\)._

If a pair of nodes \(x_{i},x_{j}\) is ancestrally related, fully ascertaining their ancestral relation involves discerning between the ancestor and descendent. As illustrated in Lemmas 3.5 and 3.6 (proofs in Appendices A.6, A.7), this can be determined through marginal independence testing after sequential univariate regressions with respect to the mutual ancestor set.

**Lemma 3.5** (Ap3).: _Let \(r_{i}^{j}\) be the residual of the \(x_{j}\) regressed on \(x_{i}\), and \(r_{j}^{i}\) be the residual of \(x_{i}\) regressed on \(x_{j}\). Vertices \(x_{i},x_{j}\) are in AP3 relation if and only if \(x_{i}\!\!\! x_{j}\) and one of the following

Figure 1: Illustrative DAG, where \(x_{1}\) is a root, \(x_{3}\) is a leaf, \(x_{3}(x_{2}),x_{3}(x_{1})\).

Figure 2: Enumeration of active causal path relation types between a pair of nodes \(x_{i}\) and \(x_{j}\). Dashed arrows indicate ancestorship.

[MISSING_PAGE_EMPTY:5]

between active causal paths and parent-child relationships. We assume the unique identifiability of the nonlinear ANM as described by Peters et al. , and provide the conditions in Appendix B.1.

Nonlinear Topological SortIn the linear setting, we determined ancestral relations through a sequence of pairwise regressions that led to independent residuals. However, a naive extension of this method into the nonlinear setting would fail, as regressions yield independent residuals under different conditions in the nonlinear case. For clarity, we demonstrate how LHTS fails to correctly recover causal relationships in an exemplary 3-node DAG with nonlinear causal mechanisms.

Consider a DAG \(G\) with three vertices \(x_{1},x_{2},x_{3}\), where \(x_{1} x_{3},x_{2} x_{3}\). The functional causal relationships are nonlinear, given by \(x_{1}=_{1},x_{2}=_{2},x_{3}=x_{1}x_{2}+_{3}\), where \(_{i}\) are mutually independent noise variables. We focus on whether LHTS can recover the parent-child relationship between \(x_{1}\) and \(x_{3}\). LHTS finds that the relationship between \(x_{1},x_{3}\) is unknown in Stage 1. In Stage 2, LHTS runs pairwise regressions between \(x_{1},x_{3}\) but _incorrectly concludes that \(x_{1},x_{3}\) are not in AP3 relation_ because neither pairwise regression provides an independent residual; both parents of \(x_{3}\) must be included in the covariate set for an independent residual to be recovered.

To handle nonlinear causal relationships, we shift our focus to searching for a different set of local substructures: the connection between active causal parental paths and the existence of parent-child relationships. We will use the existence of specific parent-child relationships to first obtain a superset of root vertices, then prune away non-roots. Once all root vertices are identified, we build the hierarchical topological sort through nonparametric regression, layer by layer.

Given a vertex \(x_{j}\) and one of its potential parents \(x_{i}\), we first provide an enumeration of all potential active casual parental path types between them with respect to \(C=(x_{j}) x_{i}\) (which could potentially be the empty set) in Figure 3 and Lemma 4.1 (proof in Appendix B.2).

**Lemma 4.1** (Active Causal Parental Path Relation Enumeration).: _Let \(x_{i},x_{j} V\) be a pair of distinct nodes, where \(x_{i}\) is one of the potential parents of \(x_{j}\). Let \(C=(x_{j}) x_{i}\). There are in total four possible types of active causal parental path relations between \(x_{i}\) and \(x_{j}\) with respect to \(C\): PP1) \(x_{i}\) and \(x_{j}\) are not directly causally related, and no active path exists between \(x_{i}\) and \(C\); PP2) \(x_{i}\) directly causes \(x_{j}\) (\(x_{i} x_{j}\)), and no active path exists between \(x_{i}\) and \(C\); PP3) \(x_{i}\) directly causes \(x_{j}\) (\(x_{i} x_{j}\)), and there exists an active path between \(x_{i}\) and \(C\); PP4) \(x_{i}\) and \(x_{j}\) are not directly causally related, and there exists an active path between \(x_{i}\) and \(C\)._

Next, for a pair of distinct vertices \(x_{i},x_{j} V\), we establish the connection between pairwise independence properties and active causal parental path relations in Lemma 4.2 (proof in Appendix B.3). This allows us to reduce the cardinality of the potential pairs of vertices under consideration in the later stages of the algorithm.

**Lemma 4.2** (Non-PP1).: _Vertices \(x_{i},x_{j} V\) are not in PP1 relation if and only if \(x_{i} x_{j}\)._

In Lemma 4.3, we show that all pairs of vertices that are in PP2 relation can be identified through local nonparametric regressions (proof in B.4).

**Lemma 4.3** (PP2).: _Let \(x_{i},x_{j} V\), \(P_{ij}=\{x_{k} V:x_{k}\!\!\! x_{i},x_{k}\!\!\! x_{j}\}\), \(r_{i}^{j}\) be the residual of \(x_{j}\) nonparametrically regressed on \(x_{i}\), and \(r_{i,P}^{j}\) be the residual of \(x_{j}\) nonparametrically regressed on \(x_{i}\) and all \(x_{k} P_{ij}\). Suppose \(x_{i}\) and \(x_{j}\) are not in PP1 relation. Then, \(x_{i}\) and \(x_{j}\) are in PP2 relation if and only if one of the following holds: 1) \(x_{i}\!\!\! r_{i}^{j}\) or 2) \(x_{i}\!\!\! r_{i,P}^{j}\)._

Condition 1) of Lemma 4.3 is relevant when \(C=\): in this case, pairwise regression identifies \(x_{i}\) as a parent of \(x_{j}\). Condition 2) is relevant when \(C\): we leverage the independence of \(x_{i}\) from the rest

Figure 3: Enumeration of the potential active causal paths among a fixed variable \(x_{j}\), one of its potential parents \(x_{i}\), and \(C=(x_{j}) x_{i}\). Solid arrows denote parenthood relations, and undirected dashed connections indicate the existence of active paths.

of \(x_{j}\)'s parents to generate \(P_{ij}\), a set that contains \(C\), but does not contain any of \(x_{j}\)'s descendants. If an independent residual were to be recovered by nonparametrically regressing \(x_{j}\) onto \(x_{i}\) and \(P_{ij}\), we identify \(x_{i}\) as a parent of \(x_{j}\).

Let \(W\) be the set of all parent vertices that are in PP2 relation with at least one vertex, i.e., the union of \(x_{i}\) satisfying either condition of Lemma 4.3. We now show that all non-isolated root vertices are contained in \(W\), and they can be differentiated from non-roots in \(W\) that are also in PP2 relations.

**Lemma 4.4** (Roots).: _All non-isolated root vertices are contained in \(W\). In addition, \(x_{i} W\) is a root vertex if and only if 1) \(x_{i}\) is not a known descendant of any \(x_{j} W\), and 2) for each \(x_{j} W\), either a) \(x_{i}\!\!\! x_{j}\), b) \(x_{j}\) is in PP2 relation to \(x_{i}\), i.e., \(x_{j}(x_{i})\), or c) there exists a child of \(x_{i}\), denoted by \(x_{k}\), that cannot be d-separated from \(x_{j}\) given \(x_{i}\), i.e., \(x_{j}\!\!\! x_{k}|x_{i}\)._

Lemma 4.4 relies on the following intuition: for any non-isolated root vertex \(x_{i}\) and descendant \(x_{k}}(x_{i})\), there exists a child of \(x_{i}\) that 1) lies on a directed path between \(x_{i}\) and \(x_{k}\), and 2) is in PP2 relation with \(x_{i}\). Vertex \(x_{i}\) will fail to d-separate this specific child from the marginally dependent non-root descendant \(x_{k}\).1 On the other hand, non-roots in \(W\) must d-separate the children they are in PP2 relation to from all marginally dependent roots: this asymmetry allows non-roots to be pruned. See Appendix B.5 for a detailed proof.

We propose a method, Algorithm 2, that leverages the above results to discover a hierarchical topological ordering: we first use the above lemmas to identify the roots, then use nonparametric regression to discover the rest of the hierarchical sort.

```
1:input: vertices \(x_{1},,x_{d} V\).
2:output: hierarchical sort \(_{L}(V)\).
3:initialize: parent relations set \(\).
4:Stage 1:Not-PP1 Relations
5:for all pairs \(x_{i},x_{j}\)do
6:if\(x_{i}\!\!\! x_{j}\)then
7:\(:x_{i},x_{j}\) not in PP1 relations.
8:if\(x_{i}\) is in PP1 relation with all vertices then
9: PRS: \(x_{i}\) is isolated, sort \(x_{i}\): \(_{L}(x_{i})=0\).
10:Stage 2:PP2 Relations
11:for all \(x_{i},x_{j}\) not in PP1 relations do
12: Set \(r_{i}^{j}\) as residual of \(x_{j}\) regressed on \(x_{i}\).
13: Set \(r_{i,P}^{j}\) as residual of \(x_{j}\) regressed on \(x_{i},P_{ij}\).
14:if\(x_{i}\!\!\! r_{i}^{j}\)or\(x_{i} r_{i,P}^{j}\)then
15:\(:x_{i},x_{j}\) are in PP2 relations, \(x_{i}}(x_{j})\).
16:Stage 3:Root Identification
17:for\(x_{i}\) PP2 relation do
18: Check if \(x_{i}\) does not d-separate any child from all marginally dependent vertices. If so, then update \(:x_{i}\) is a root, sort \(x_{i}\) into layer 0, \(_{L}(x_{i})=0\).
19:Stage 4:Obtain Sort
20:for\(k\{1,,d-1\}\)do
21:for all unsorted \(x_{i}\)do
22: Set \(r_{i}\) as the residual of \(x_{i}\) regressed on sorted features in \(_{H}\).
23: If \(r_{i}\!\!\! x_{j} x_{j}_{H}\), add \(x_{i}\) into the current layer \(_{L}(x_{i})=k\).
24:return\(_{L}(V)\) ```

**Algorithm 2****NITS**: Nonlinear Hierarchical Topological Sort

In Stage 1, we discover all pairs \(x_{i},x_{j}\) not in PP1 relation by testing for marginal dependence; in Stage 2, we leverage Lemma 4.3 to find the vertex pairs that are in PP2 relations, a superset of the root vertices; in Stage 3 we prune non-roots by finding they d-separate their children from at least one dependent vertex in \(W\) (Lemma 4.4); in Stage 4 we identify vertices in the closest unknown layer by regressing them on sorted nodes and finding independent residuals. We show the correctness of Algorithm 2 in Theorem 4.5 (proof in Appendix B.6), and the worst case time complexity in Theorem 4.6 (proof in Appendix B.7). We provide a walk-through of Algorithm 2 in Appendix B.8.

**Theorem 4.5**.: _Given a graph \(G\), Algorithm 2 asymptotically finds a correct hierarchical sort of \(G\)._

**Theorem 4.6**.: _Given \(n\) samples of \(d\) vertices generated by a identifiable nonlinear ANM, the worst case runtime complexity of Algorithm 2 is upper bounded by \(O(d^{3}n^{3})\)._

The number of nonparametric regressions run by NHTS in each step is actually inversely related to the size of the covariate sets, while the number of regressions in each step of RESIT and NoGAM are directly proportionate to the covariate set size. We provide a formal analysis of the reduction in complexity for the worst case (fully connected DAG) in Theorem 4.7 (proof in Appendix B.9):

**Theorem 4.7**.: _Consider a fully connected DAG \(G=(V,E)\) with nonlinear ANM. Let \(d:=|V|\). Let \(n_{k}^{}\) be the number of nonparametric regressions with covariate set size \(k[d-2]\) run by NHTS when sorting \(V\); we similarly define \(n_{k}^{}\) and \(n_{k}^{}\) respectively. Then, \(n_{k}^{}=d-k\), and \(n_{k}^{}=n_{k}^{}=k+1\). This implies that for all \(k>\), \(n_{k}^{}<n_{k}^{}=n_{k}^{}\)._

## 5 Edge Discovery

Parent selection from a topological ordering \(\) via regression is traditionally a straightforward task in the infinite sample setting: for each vertex \(x_{i}\), \(\) establishes \(J_{i}=\{x_{k}:(x_{k})<(x_{i})\}\), a superset of \(x_{i}\)'s parents that contains none of \(x_{i}\)'s descendants. A general strategy for pruning \((x_{i})\) from \(J_{i}\) is to regress \(x_{i}\) on \(J_{i}\) and check which \(x_{k} J_{i}\) are relevant predictors. The key issue is that \(J_{i}\) grows large in high-dimensional graphs: current edge pruning methods either make strong parametric assumptions or suffer in sample complexity. Lasso and GAM methods impose linear and additive models, failing to correctly identify parents in highly nonlinear settings. RESIT assumes a more general nonlinear ANM, but requires huge sample sizes and oracle independence tests for accurate parent set identification: authors in  confirm this, saying "despite [our] theoretical guarantee[s], RESIT does not scale well to a high number of nodes."

We propose an entirely nonparametric constraint-based method that uses a local conditioning set \(Z_{ij}\) to discover whether \(x_{i}(x_{j})\), rather than \(J_{i}\), outperforming previous methods by relaxing parametric assumptions and conditioning on fewer variables. The following lemma outlines a sufficient condition for determining whether an edge exists between two vertices (proof in Appendix C.1), visualized in Figure 4.

**Lemma 5.1** (Parent Discovery).: _Let \(\) be a topological ordering, \(x_{i},x_{j}\) such that \((x_{i})<(x_{j})\). Let \(Z_{ij}=C_{ij} M_{ij}\), where \(C_{ij}=\{x_{k}:x_{k}(x_{i}),x_{k} x_{j}\},M_{ij}=\{x_{k}: x_{k}(x_{j}),(x_{i})<(x_{k})<(x_{j})\}\). Then, \(x_{i} x_{j} x_{i} x_{j}|Z_{ij}\)._

The intuition is that to determine whether \(x_{i} x_{j}\), instead of conditioning on all potential ancestors of \(x_{j}\), it suffices to condition on potential confounders of \(x_{i},x_{j}\) (\(C_{ij}\)) and potential mediators between \(x_{i}\) and \(x_{j}\) (\(M_{ij}\)). This renders all backdoor and frontdoor paths inactive, except the frontdoor path corresponding to a potential direct edge from \(x_{i}\) to \(x_{j}\).

**Edge Discovery** We propose an algorithm that leverages the above results to discover the true edges admitted by any topological ordering by running the described conditional independence test in a specific ordering. We give the implementation for pruning a topological sort here, but our approach can be generalized to a hierarchical version (see Appendix C.2). We show the correctness of Algorithm 3 in Theorem 5.2 (proof in Appendix C.3).

**Theorem 5.2**.: _Given a correct topological ordering of \(G\), Algorithm 3 asymptotically finds correct parent sets \((x_{i}), x_{i} G\)._

The key insight is to check each potential parent-offspring relation using the conditional independence test \(x_{i}\!\!\! x_{j}|Z_{ij}\) such that previous steps in the algorithm obtain all vertices in both \(C_{ij}\) and \(M_{ij}\)

Figure 4: DAG corresponding to Lemma 5.1, which tests whether \(x_{i}(x_{j})\) (i.e., whether the red arrow exists).

We first fix a vertex \(x_{j}\) whose parent set we want to discover. We then check if vertices ordered before \(x_{j}\) are parents of \(x_{j}\) in reverse order, starting with the vertex immediately previous to \(x_{j}\) in \(\). This process starts at the beginning of \(\), meaning we discover parent-offspring relations from root to leaf (see Appendix C.3 for a detailed walk-through and proof). We show the worst case time complexity of Algorithm 3 in Theorem 5.3 (proof in Appendix C.4).

**Theorem 5.3**.: _Given \(n\) samples of \(d\) vertices generated by a model corresponding to a DAG \(G\), the runtime complexity of ED is upper bounded by \(O(d^{2}n^{3})\)._

## 6 Experiments

**Setup** Methods2 are evaluated on 20 DAGs in each trial. The DAGs are randomly generated with the Erdos-Renyi model ; the probability of an edge is set such that the average number of edges in each \(d\)-dimensional DAG is \(d\). Gaussian, Uniform, or Laplace noise is used as the exogenous error. In experiments for linear topological sorting methods (Figure 5), we use linear causal mechanisms to generate the data; in experiments for nonlinear topological sorting methods (Figure 6) and edge pruning algorithms (Figure 7), we use quadratic causal mechanisms to generate the data. Existing ANM methods are prone to exploiting artifacts that are more common in simulated ANMs than real-world data [28; 29], inflating their performance on synthetic DAGs and leaving real-world applicability an open question. To reduce concerns about such artifacts, data were generated with reduced \(R^{2}\)-sortability , and standardized to zero mean and unit variance .

**Metrics**\(A_{top}\) is equal to the percentage of edges that can be recovered by the returned topological ordering (an edge cannot be recovered if a child is sorted before a parent). We note that \(A_{top}\) is a normalized version of the topological ordering divergence \(D_{top}\) defined in . Edge pruning algorithms return a list of predicted parent sets for each vertex: \(F_{1}=2}{}\) measures the performance of these predictions.

**Linear Topological Sorts** Figure 5 demonstrates the performance of our linear topological ordering algorithm, LHTS, in comparison with the benchmark algorithms, DirectLiNGAM  and \(R^{2}\)-Sort . \(R^{2}\)-Sort is a heuristic sorting algorithm that exploits artifacts common in simulated ANMs; both benchmarks are agnostic to the noise distribution. We observe that both DirectLiNGAM and LHTS significantly outperform \(R^{2}\)-sort. LHTS demonstrates asymptotic correctness in Figure 5(c), achieving near-perfect \(A_{top}\) at \(n=2000\). However, LHTS has consistently lower \(A_{top}\) than DirectLiNGAM in Figure 5(a). On the other hand, LHTS encodes more causal information: the orderings produced by LHTS in Figure 5(b) had roughly \( 70\%\) fewer layers than the orderings produced by DirectLiNGAM, reducing the size of potential parent sets \(J\) by identifying many non-causal relationships.

Figure 5: Performance of LHTS on synthetic data. Top row: \(n=500\) with varying dimension \(d\). Bottom row: \(d=10\) with varying sample size \(n\). See Appendix D.1 for runtime results.

Nonlinear Topological SorsFigure 6 illustrates the performance of our nonlinear topological sorting algorithm. We take GES , GRaSP , GSP , DirectLiNGAM , NoGAM , and \(R^{2}\)-Sort  as baseline comparators that are all agnostic to the noise distribution. We excluded PC and RESIT since in general they perform much worse than baseline methods . We note that as DirectLiNGAM, NoGAM, and NHTS are FCM-based methods, they each return a unique topological ordering; however, as GES, GRaSP, and GSP are scoring-based methods , they return only a MEC. All topological orderings contained within an MEC satisfy every conditional independence constraint in the data, and therefore are all _equally valid_. To enable a fair comparison, we randomly select one ordering permitted by an outputted MEC for evaluation. We note that NHTS outperformed all baselines, achieving the highest median \(A_{top}\) in all trials. Furthermore, as expected from Theorem 4.6, NHTS ran up to \(4\) faster than NoGAM (see Appendix D.2). We provide additional experiments over DAGs with increasing edge density in Appendix D.3.

Edge PruningFigure 7 illustrates the performance of our edge pruning algorithm ED. We take covariate hypothesis testing with GAMs (CAM-pruning ), Lasso regression, and RESIT as baseline comparators that are all agnostic to the noise distribution. All algorithms were given correct topological sorts: ED significantly outperformed all baselines, with the highest median \(F_{1}\) score in all trials. ED was slower than Lasso, but was significantly faster than the other nonlinear edge pruning algorithms, CAM-pruning and RESIT. RESIT was excluded from higher-dimensional tests due to runtime issues. The poor performance of baseline methods highlights the need for a sample efficient nonparametric method for accurate causal discovery of nonlinear DGPs. We provide additional experiments in settings with increasing density and varying noise distributions in Appendix D.4.

DiscussionIn this paper we developed novel global causal discovery algorithms by searching for and leveraging local causal relationships. We improved on previous topological ordering methods by running fewer regressions, each with lower dimensionality, producing hierarchical topological sorts. Additionally, we improved on previous edge pruning procedures by introducing a nonparametric constraint-based method that conditions on far fewer variables to achieve greater recovery of parent sets. We tested our methods on robustly generated synthetic data, and found that both our nonlinear sort NHTS and edge pruning algorithm ED significantly outperformed baselines. Future work includes extending the topological sorting algorithms to the full ANM setting with both linear and nonlinear functions, simultaneously exploiting both ancestor-descendent and parent-child relations, as well as adapting our approach to handle various forms of unmeasured confounding. Additionally, we aim to develop statistical guarantees of sample complexity for our methods, extending previous results  derived in the setting of nonlinear ANMs with Gaussian noise.

Figure 6: Performance of NHTS on synthetic data, \(n=300\), dimension \(d=10\), with varying error distributions: Gaussian, Laplace, Uniform (left, middle, right). See Appendix D.2 for runtime results.

Figure 7: Performance of ED on synthetic data, uniform noise. Left, middle: \(n=300\) with varying dimension \(d\). Right: \(d=10\) with varying sample size \(n\). See Appendix D.5 for runtime results.