# Minimum norm interpolation by perceptra:

Explicit regularization and implicit bias

Jiyoung Park

Department of Statistics

Texas A&M University

wldydd15510@tamu.edu

&Ian Pelakh

Department of Mathematics

Iowa State University

ispelakh@iastate.edu

&Stephan Wojtowytsch

Department of Mathematics

University of Pittsburgh

s.woj@pitt.edu

###### Abstract

We investigate how shallow ReLU networks interpolate between known regions. Our analysis shows that empirical risk minimizers converge to a minimum norm interpolant as the number of data points and parameters tends to infinity when a weight decay regularizer is penalized with a coefficient which vanishes at a precise rate as the network width and the number of data points grow. With and without explicit regularization, we numerically study the implicit bias of common optimization algorithms towards known minimum norm interpolants.

## 1 Introduction

Modern neural networks mostly operate in an overparametrized regime, i.e. they possess more tunable parameters than the number of data points contributing to the loss function. Safran and Shamir (2018); E et al. (2019); Du et al. (2018); Chizat and Bach (2018) associate overparametrization with better training properties, and Belkin et al. (2019, 2020) find it to enhance statistical generalization (see also (Loog et al., 2020) for historical context). For many architectures, overparametrization leads to the ability to fit any values \(y_{i}\) at a given set of data points \(\{x_{1},,x_{n}\}\), and Cooper (2018) shows that generically, the set of weights for which the neural network interpolates prescribed values is a submanifold of high dimension and co-dimension in parameter space. Which solution on the manifold is dynamically chosen by an optimization algorithm, and which solutions have favorable generalization properties, is an active area of research in theoretical machine learning.

Current practice is to estimate a model's generalization to previously unseen data by assessing its performance on a hold-out set (a posteriori error estimate) or by using uniform estimates on the generalization of all elements of a function class (a priori estimate if the function class is encoded ahead of time by explicit regularization, a posteriori if membership is determined after optimization). Neither approach yields information on what a neural network does outside the support of the data distribution, a topic of great interest for the study of distributional shift and adversarial stability.

The main contribution of this work is split between two complementary lines of investigation:

1. We prove that neural network minimizers of regularized empirical risk functionals converge to minimum norm interpolants of a given target function in an infinite parameter limit (Section 2). Our result improves previous works to more general settings (See Remark 2.2).
2. Training a neural network generally corresponds to solving a non-convex minimization problem. While we provide convergence guarantees for empirical risk minimizers, in general there is no guarantee that a training algorithm finds a global minimizer of an empirical risk functional. Even if convergence holds, it is unclear _which_ minimizer is selected (in the overparametrized regime, where the set of minimizers is a high-dimensional manifold). In settings where the minimum norm interpolant is known (Section 3), we compare numerical solutions to theoretical predictions to better understand (1) the predictivepower of theoretically studying empirical risk minimizers and (2) the implicit bias of different optimization algorithms (Sections 4 and 5). We believe this to be a useful benchmark problem for a better understanding of explicit regularization and implicit bias in optimization.

Minimum norm interpolants are the regression analogue to maximum margin classifiers. They are associated with favorable generalization properties and relative stability even against adversarial perturbations.

### Previous work

Minimum norm interpolation.In classes of linear functions, minimum norm interpolation has a long history as ridge regression (minimal \(^{2}\)-norm) or as the least absolute shrinkage selection and operator (LASSO, minimal \(^{1}\)-norm). To the best of our knowledge, minimum norm interpolation by neural networks has only been studied for shallow neural networks in one dimension by Hanin (2021); Debarre et al. (2022); Boursier and Flammarion (2023) and in odd dimension for certain radially symmetric data by Wojtowytsch (2022). For classification problems, minimum norm/maximum margin classifiers were considered by E and Wojtowytsch (2022). For finite datasets, the set of minimum norm interpolants was characterized by Parhi and Nowak (2021). A parametrization of the same function class by neural networks with multiple linear layers and a single ReLU layer induces different concepts of minimum norm interpolation as studied by Ongie and Willett (2022).

Implicit bias.The implicit bias of parameter optimization algorithms has been studied for gradient flows with infinitely wide, but shallow ReLU networks by Chizat et al. (2019); Jin and Montifar (2020) and for stochastic gradient descent and diagonal linear networks by Pesme et al. (2021). Chizat and Bach (2020) prove convergence to a maximum margin classifier for infinitely wide ReLU networks with one hidden layer if the parameters follow the gradient flow of an (unregularized) logistic loss risk functional. Many authors, including Damian et al. (2021); Li et al. (2021); Wu et al. (2022) and Wojtowytsch (2020), study the bias of SGD towards solutions at which the loss landscape is 'flat' in the parameter space. Hochreiter and Schmidhuber (1997) conjectured such minimizers to have favorable generalization properties. In many cases, minimizers tend to be flatter if the parameters associated to them are not excessively large. Yang et al. (2021) describe several phase-transitions in parameter space for the relation between flatness in parameter space and generalization. Zhou et al. (2020) compare the implicit bias of SGD and ADAM. Smith et al. (2021); Barrett and Dherin (2020) argue that gradient descent resembles a gradient flow in a modified (regularized) loss landscape more closely than the gradient flow of the original loss function. Hajjar and Chizat (2022) investigate the role of symmetries in optimization.

Barron space.The Barron class is adapted to ReLU networks with a single hidden layer and weights of bounded average magnitude. Slightly different versions of the same function space have been studied by Bach (2017); E et al. (2019, 2020); Ongie et al. (2019); E and Wojtowytsch (2020, 2021); Caragea et al. (2020); Parhi and Nowak (2021); Wojtowytsch (2022); Siegel and Xu (2022, 2023) under various names such as \(_{1}\), Radon-BV or the variation space of the ReLU dictionary.

### Preliminaries

Conventions.\(\) always denotes a \(^{2}\)-sub-Gaussian probability measure on the data domain \(^{d}\), i.e.

\[\;C,>0_{X} (\{\|X\|-\|X\|\}) C\,(^{2}}{2})\;>0.\]

All norms are Frobenius norms (\(^{2}\)-norm for vectors). For \(n\), \(S_{n}=\{x_{n,1},,x_{n,n}\}\) is a set of \(n\) iid samples from the distribution \(\), independent of \(S_{n^{}}\) for \(n^{} n\). When unambiguous, we denote \(x_{n,i}=x_{i}\). We take \((f,y)=|f-y|^{2}\) as the mean squared error/\(^{2}\)-loss function, but we remark that the theoretical analysis remains valid if \(_{MSE}\) is replaced by \(^{1}\)-loss or a Huber or pseudo-Huber loss

\[_{Hub}(f,y)=|f-y|^{2}&|y-f|<1\\ 2|y-f|-1&|y-f| 1,_{pH}(y,h)=}-1.\]

For \(m\) and \((a,W,b)^{m}^{m d}^{m+1}\), let

\[f_{(a,W,b)}:^{d}, f_{(a,W,b)}(x)=b_{0}+_{i=1} ^{m}a_{i}\,(w_{i} x+b_{i}),(z)=(z)= \{z,0\},\]i.e. \(f_{(a,W,b)}\) is a ReLU network with a single hidden layer and weights \(a,W\) and biases \(b\). The vector \(w_{i}^{d}\) is the \(i\)-th row of the matrix \(W\). For \(m,n\) and \( 0\), we denote the regularized empirical risk functional as \(}_{n,m,}:^{m}^{m d }^{m+1}[0,)\):

\[}_{n,m,}(a,W,b)=_{i=1}^{n} (f_{(a,W,b)}(x_{i}),\;y_{i})+(\|a\|_{2}^{2}+ \|W\|_{Frob}^{2}).\]

Concepts.We introduce what we dub 'homogeneous Barron space' \(\) heuristically here, and in greater detail in Appendix C. As a measure of magnitude of the function, we consider the weight decay (or Tikhonov) regularizer \(\|a\|_{2}^{2}+\|W\|_{Frob}^{2}\) of the parametrized function \(f_{(a,W,b)}\) which does not control the magnitude of the bias vector.

Note that the function class \(f_{(a,W,b)}\) and its complexity do not change when we consider representations of the form \(f_{m}(x)=_{i=1}^{m}a_{i}\,(w_{i} x+b_{i})\) with a regularizer \(\|a\|_{2}^{2}+\|W\|_{Frob}^{2}=_{i=1 }^{m}(a_{i}^{2}+\|w_{i}\|_{2}^{2})\). A continuum analogue to these functions represented as an 'empirical average' over individual neurons is a general expectation representation \(f_{}(x)=_{(a,w,b)}a\,(w^{T}x+b)\) for some probability distribution \(\) on parameter space and a regularizer \(_{(a,w,b)}[a^{2}+\|w\|_{2}^{2}]\). As the parametrization of a function by a neural network is generally non-unique, we define the Barron semi-norm \([f]_{}=_{\{:f_{}=f\}}_{(a,w,b)}[a^{2}+\| w\|_{2}^{2}]\) as the lowest value attained by the regularizer over all possible parametrizations. For a more comprehensive understanding, interested readers may refer to Appendix C.

## 2 General convergence result

We first state a general convergence result to a minimum norm interpolant of given data generated by functions in the homogeneous Barron class \(\).

**Theorem 2.1**.: _Take \(,S_{n},}_{n,m,}\) as in Section 1.2, \(f^{*}\), and let \(y_{i}=f^{*}(x_{i})\) for \(i=1,,n\). Assume that \(m,\) are parameters which scale with \(n\) as \(m_{n},_{n}\) such that_

\[_{n}(_{n}+})=0, _{n}(\,m_{n}}+\,})=0.\] (1)

_Then almost surely over the random selection of data points in \(S_{n}\), the following holds: If \((a,W,b)_{n}*{argmin}}_{n,m_{n},_{n}}\) for all \(n\), then every subsequence of \(f_{n}:=f_{(a,W,b)_{n}}\) has a further subsequence which converges to some limit \(^{*}\) with \(^{*}=f^{*}\)\(\)-almost everywhere and \([^{*}]_{}[f^{*}]_{}\). Convergence holds in \(L^{p}()\) for all \(p<\) and uniformly on compact subsets of \(^{d}\). If \(_{}\|x\|+^{2} 1\), then for all \(n 2\), the following explicit bound holds up to higher order terms in \(n,m=m_{n},=_{n}\) with probability at least \(1-1/n^{2}\):_

\[\|f_{(a,W,b)_{n}}-f^{*}\|_{L^{2}()}^{2} C(]_{}^{2}}{m}\,_{}\|x\|^{2}+[f^{*}]_{}^{2} (_{}\|x\|+^{2})}\,+ \,[f^{*}]_{}).\] (2)

**Remark 2.2**.: _Theorem 2.1 extends previous results of E et al. [2019a] in several ways:_

1. _We allow for general sub-Gaussian rather than compactly supported data distributions._
2. _We do not control the magnitude of the bias variables._
3. _Our results apply to_ \(^{2}\)_-loss, which is neither globally Lipschitz-continuous nor bounded._
4. _In a limiting regime, we characterize how the empirical risk minimizers interpolate in the region where no data is given by proving uniform convergence to a minimum norm interpolant._

_In sum, the first three points necessitate a more careful technical analysis than in the settings of prior work, and the unboundedness of data and loss introduces additional logarithmic terms not present for E et al. [2019a]. To the best of our knowledge, the last point is novel and our work is the first to use the notion of \(\)-convergence in this context._If the data-distribuion \(\) is supported on the entire space \(^{d}\) (e.g. a non-degenerate Gaussian), then \(^{*} f^{*}\). In many cases, however, \(\) is supported on a small, potentially compact and generally low-dimensional subset \(M\) of the data space. In this case, the function \(f^{*}\) is only known on the closed set \(M^{d}\). As a result, there are many \(f\) such that \(f f^{*}\) on \(M\) while \(f f^{*}\) on \(^{d}\) in general. The subsequential limit is one of these functions which has a minimal semi-norm \([f]_{}\).

Thus, beyond knowing that \(f_{n}\) asymptotically fits the function \(f^{*}\) perfectly at known data, Theorem 2.1 provides information about how it may interpolate at points where \(\) has no information. Such knowledge is of interest when a population may naturally evolve in time (distributional shift) of if \(f_{n}\) is applied to a new problem with similar features but distinct geometry (transfer learning).

The proof of Theorem 2.1 is given in Appendix E. In the proof, we combine a direct approximation theorem to construct risk competitors with Rademacher complexity-based generalization bounds. Concentration inequalities are used to bound tail quantities. The scaling conditions that \(}\) and \(\) are used to ensure that the risk functional has sufficiently strong regularization. They can be weakened if the data measure \(\) is supported on a finite set of points or the function \(f^{*}\) can be represented by a neural network with a finite number of neurons respectively. For instance, an analogous statement holds with a simpler proof for a fixed data-set \(S\) of \(n\) data points if \(\) is coupled to \(m\) such that \(} 0\). In this case, we take the empirical distribution \(=_{x S}_{x}\) as the population and do not need to bound the generalization gap. This model can be considered appropriate when \(n m\) (heavy overparametrization). A precise statement is given in Appendix F.

The proof remains valid if we only assume that

\[_{n}}}_{n,m_{n}, _{n}}(a_{n},W_{n},b_{n})=\{[f]_{}:f f^{*}, -\},\]

i.e. if \((a,W,b)_{n}\) parametrizes a function of low excess risk. In the proof, we obtain a more precise version of (2). Using an a priori Lipschitz-bound, it is possible to obtain a rate of convergence in \(L^{p}()\) for \(p<\) by interpolation. For uniform convergence on compact sets outside the support of \(\), we do not obtain a rate in this work.

The proof of uniform convergence on compact sets utilizes the notion of \(\)-convergence from the calculus of variations, a very stable notion of convergence which implies that minimizers of approximating functionals converge to the minimizer of a limiting functional. This notion has recently made inroads into machine learning applications and was used e.g. by Neumayer et al. (2023).

All results can easily be generalized to any more general function class which admits the three key ingredients: A bound on its Rademacher complexity, a compact embedding theorem, and a direct approximation theorem.

## 3 Minimum norm interpolants

### One-dimensional example

In one dimension, (Wojtowytsch, 2022, Proposition 2.5) shows that \([f]_{}=_{-}^{}|f^{}(x)|\,x\) for any smooth function \(f:\) which satisfies \(f^{}(x)=0\) at some point \(x\) - see also previous work by Li et al. (2020); E and Wojtowytsch (2020). This one-dimensional case is, in fact, the simplest case of a general characterization of Barron functions in any dimension by Ongie et al. (2019).

Consider the task of minimizing \([f]_{}\) under the condition that \(f(x)=|x|\) if \(|x| 1\). Then the minimum is attained for any smooth convex function \(f\) which satisfies the constraint since

\[2=f^{}(1)-f^{}(-1)=_{-1}^{1}f^{}(x)\,x _{-1}^{1}|f^{}(x)|\,x=[f]_{}\]

with equality if and only if \(f^{} 0\). The same estimate holds for non-smooth Barron functions if the second derivative is interpreted as a Radon measure. For piecewise linear functions, this corresponds to summing \(|f^{}(x_{i}^{+})-f^{}(x_{i}^{-})|\) over the non-smooth points \(x_{i}\). More generally, the set of minimum norm interpolants of one-dimensional convex data was characterized by Savarese et al. (2019).

**Proposition 3.1**.: _Let \(x_{0}<<x_{n}\) and \(y_{i}=f^{*}(x_{i})\) for a convex function \(f^{*}\) and \(i=0,,n\). If \(y_{1}<y_{0}\) and \(y_{n}>y_{n-1}\), then \(f\) is a minimum Barron norm interpolant of the dataset \(\{(x_{i},y_{i})\}_{i=0}^{n}\)_if and only if \(f\) is convex, \(f(x_{i})=y_{i}\) for all \(i=0,,n\) and_

\[f^{}(x)=-y_{0}}{x_{1}-x_{0}}x<x_{1} f^{}(x)=-y_{n-1}}{x_{n}-x_{n-1}}x>x_{n-1}.\]

The two given slopes are the largest values that are required for derivatives at any point. We give a proof of Proposition 3.1 in Appendix G. In full generality, minimum norm solutions have been characterized by Hanin (2021) using matching convexities to achieve minimal total curvature.

In a recent article, Boursier and Flammarion (2023) show that if the full Barron norm is controlled, i.e. if the magnitude of biases is included in the regularizer, a specific convex function is selected. This corresponds to minimizing a functional

\[_{-1}^{1}|f^{}(x)|}\,x _{-1}^{1}|f^{}(x)| x.\]

The first functional prefers \(f^{}\) to be large close to the origin, if it has to be large anywhere. All break points occur as close to the origin as possible, in particular: Either at the origin or at data points. Unlike the Barron semi-norm penalty studied in this work, which does not select a specific minimum norm interpolant, the Barron norm penalty thus induces a _sparse_ neural network interpolant.

### Radially symmetric bump function

Another setting where we have explicit minimum norm interpolant is when we fit a bump function for radially symmetric data. Recall a result of Wojtowytsch (2022) on minimum norm fitting of certain radially symmetric data.

**Proposition 3.2**.: _(_Wojtowytsch_,_ 2022_, Theorem 3.1)_ _Let \(d 3\) be an odd integer and_

\[=f C_{c}(^{d}):f(0)=1f(x)=0 \|x\| 1}.\]

_Then there exists a unique radially symmetric function \(f_{d}^{*}:^{d}\) such that \(f_{d}^{*}*{argmin}_{f}[f]_{}\). The norm of minimizers grows as \(_{d}[f_{d}^{*}]_{}/d 3.7\)._

We note that the existence of minimum norm interpolants which are not radially symmetric is not excluded, but if \(*{argmin}_{f}[f]_{}\) is any other minimum norm interpolant, then its radial average \(*{Av}\) coincides with \(f_{d}^{*}\): \(*{Av} f_{d}^{*}\). Here

\[*{Av}f(x):=_{SO(d)}f(Ox)\,H_{O}=_{S^{d-1}}f |x|\,_{}^{0}\] (3)

where \(H\) is the uniform distribution (Haar measure) on the group of rotations and \(^{0}\) is the uniform distribution on the \(d-1\)-dimensional sphere in \(^{d}\). In (Wojtowytsch, 2022, Section 6), an algorithm is given to find the minimum norm interpolant \(f_{d}^{*}\) by numerically solving a one-dimensional polynomial approximation problem and a linear system of moment conditions.

The uniqueness statement allows us to strengthen the result of Theorem 2.1 in this case. A natural setting is to use a sub-Gaussian data distribution \(\) which gives positive mass to the origin, but has no mass elsewhere in the unit ball. It should have mass everywhere outside the unit ball. Under this natural setting, we have a stronger result of Theorem 2.1.

**Corollary 3.3**.: _Take \(,S_{n},}_{n,m,}\) as in Section 1.2, and assume in addition that_

1. \((\{0\})>0\) _(positive mass at the origin)._
2. \((B_{1}(0)\{0\})=0\) _(no mass elsewhere in the unit ball)._
3. \((U)>0\) _for any open set_ \(U^{d}(0)}\) _(mass everywhere outside the unit ball)._

_Assume that \(m,\) scale with \(n\) as in (1). Almost surely over the random selection of data points in \(S_{n}\), the following holds: If \((a,W,b)_{n}*{argmin}}_{n,m_{n},_{n}}\) for all \(n\), then sequence of radial averages \(*{Av}f_{n}\) of \(f_{n}:=f_{(a,W,b)_{n}}\) converges to \(f_{d}^{*}\) as in Proposition 3.2. Convergence holds in \(L^{2}()\) for MSE loss (with an explicit rate) and uniformly on compact subsets of \(^{d}\) (without a rate in this work)._In Corollary 3.3, we guarantee convergence to the unique radial minimum norm interpolant \(f_{d}^{*}\) (at least for the radial average), while in Theorem 2.1 we may have different subsequences that converge to different minimum norm interpolants. The proof is given in Appendix E.

## 4 Relating Interpolation, Optimization and Generalization

For a given bounded set \(K^{d}\), Theorem 2.1 states that for a large number of neurons \(m\), a large number of data points \(n\), and a small penalty \(>0\), minimizers of the empirical risk functional \(}_{n,m,}\) resemble a minimum norm interpolant everywhere in \(K\). As the Barron semi-norm controls the generalization gap (see Appendix D) and a minimum norm interpolant has minimal Barron norm by definition, this suggests that minimum norm interpolants are optimal in terms of generalization, at least when arguing from this upper bound.

Many authors, including Safran and Shamir (2018); Venturi et al. (2018), demonstrate that neural network training is a non-convex optimization problem. As such, it is not guaranteed that numerical optimizers (1) converge to interpolants at all, and (2) select minimum norm interpolants out of the large set of different neural networks which interpolate given data, even when a regularizer is included in the training loss functional.

On the other hand, there are settings where an optimization algorithm selects a minimum norm solution even without explicit regularization. This is easily proved for gradient descent on the overparametrized least squares regression problem \(_{n}(a)=_{i=1}^{n}|a^{T}x_{i}-y_{i}|^{2}\) with initial condition \(a=0\) and \(n<m=d\). Using entirely different methods, Chizat and Bach (2020) prove a similar result for binary classification by shallow neural networks with logistic loss. For regression problems using neural networks, analogous results are not available to the best of our knowledge. This in part motivates the following numerical investigation. Namely, we are interested in exploring the effects of explicit regularization and the implicit bias of optimization algorithms toward minimum norm interpolants. Knowing the analytically optimal solution in between given data provides us the opportunity to compare optimizers on a deeper level than merely testing their performance on unseen data generated from the same distribution.

As seen in Figure 1, the radial profile \(r f_{d}^{*}(re_{1})\) of Wojtowytsch (2022)'s minimum norm interpolant \(f_{d}^{*}\) is so close to \(0\) on \([r_{d},)\) as to be virtually indistinguishable from zero numerically for some \(r_{d}<1\) which decreases in \(d\). Indeed, the first \((d-1)/2\) derivatives of vanish at \(r=1\) due to (Wojtowytsch, 2022, Lemma 4.1) and \(0 f_{d}^{*}(x) Cd^{3/2}((1-\|x\|^{2})/\|x\|)^{(d-3)/2)}\) due to (Wojtowytsch, 2022, Appendix D.1) for a universal constant \(C>0\). In particular, if \(d\) is large, \(\|f_{d}^{*}\|_{L^{}(^{d})B_{rd}(0))}\) is negligible compared to the approximation error \(d^{2}/m\) for any reasonable dimension \(d\) and network width \(m\). Consequently, the rescaled function \(h_{d}^{*}(x):=f_{d}^{*}(r_{d}x)\) meets the constraint \(h_{d}^{*} 0\) outside \(B_{1}(0)\) almost exactly and has the smaller Barron semi-norm \([h_{d}^{*}]_{}=r_{d}\,[f_{d}^{*}]_{}\). For this reason, we compare numerical solutions to the interpolation problem in Corollary 3.3 to rescaled versions of \(f_{d}^{*}\) rather than \(f_{d}^{*}\) itself, at least in high dimension. For \(r\) below the threshold value \(r_{d}\), there is no noticable trade-off between rescaling \(f_{d}^{*}(rx)\) and data-fitting. For larger values of \(r\), the Barron semi-norm is reduced more significantly, but the data fit becomes appreciably worse.

## 5 Numerical Experiments

Our main goal in this section is to gain a more precise understanding of different optimization algorithms by comparing numerical solutions to a known minimum norm interpolant. We consider the two settings in which minimum norm interpolation by Barron functions is best understood: One-dimensional and radially symmetric functions. As a benefit, we can easily visualize the numerical results in both settings. We focus on three questions of interest.

1. **Explicit regularization.** If \(>0\) is moderately small and \(m,n\) are large, then a global minimizer of \(}_{n,m,}\) resembles a minimum norm interpolant between known data points due to Theorem 2.1. Is the minimizer which we find numerically close to to a minimum norm interpolant, or does it merely fit the function at known data points?
2. **Implicit bias.** If \(m,n\) are large, does a training algorithm select a minimum norm interpolant out of potentially many possible solutions without explicit regularization (i.e. for \(=0\))?3. **Learning symmetries:** The optimal minimum norm interpolant \(f_{d}^{*}\) described in Proposition 3.2 is radially symmetric and satisfies \(0 f_{d}^{*} 1\). Proposition 3.2 does not rule out the existence of other minimum norm interpolants which are not radially symmetric. Does an optimization algorithm generally find solutions which are (approximately) radially symmetric and confined to the interval \(\)? A similar consideration applies in a one-dimensional investigation with reflection symmetry.

The third question is of particular interest for algorithms like ADAM, which operate coordinate-by-coordinate and do not respect Euclidean isometries. By comparison, we expect that SGD, initialized at a radially symmetric configuration, preserves Euclidean isometries. More experiments in similar settings can be found in Appendix A.

### One-dimensional experiments

We consider the classical interpolation problem of numerical analysis: Fit values \(f^{*}(x_{i})\) at points \(x_{i}\) for \(i\{1,,n\}\). In contrast to classical numerical analysis, we consider overparametrized ReLU-networks with a single hidden layer as our model class. As in Section 3, we select \(f^{*}(x)=|x|\) for simplicity.

In Figure 2, a ReLU network with a single hidden layer of width \(m=200\) was trained to fit \(f^{*}(x)=|x|\) at a symmetric set containing 15 equi-spaced points in \((1,2)\). Optimizers included SGD (with learning rate \(=5 10^{-5}\) and momentum \(=0.99\)), SGD (\(=10^{-2}\), \(=0\)), ADAM (\(=5 10^{-5}\) and default parameters) and the quasi-Newton L-BFGS method. Deterministic gradients based on the \(n=30\) sample points were used. The final training loss was below \(10^{-4}\) on average. The network weights were initialized by a scaled uniform Xavier initialization, i.e. uniformly in a symmetric interval of length \(2+n_{out})}\) where \(n_{in}\) and \(n_{out}\) denote the number of input- and output-units to a layer respectively. The 'gain' factor was selected as \(\{0.5,1,5\}\). Without weight decay and for small gain, the optimizers find a solution close to the smallest possible minimum norm interpolant \(f(x)=|x|\). The larger the parameters for initialization gain and weight decay penalty, the closer numerical solutions are to the largest possible minimum norm interpolant \(f(x)=\{|x|,1\}\).

We observe that a higher gain factor \(\) corresponds to faster initial training, but a high gain like \(=5\) produces interpolants which are non-convex without regularization, while a lower gain factor produces convex interpolants in longer time. This observation agrees with the findings of Chizat et al. (2019), who dub the large \(\) setting the 'lazy training' regime and associate it with worse generalization performance. As Pesme et al. (2021) eloquently put it: "there is a tension between generalisation and optimisation: a longer training time might improve generalisation but comes at the cost of...a longer training time."

If \(m\) is large and \(\) is not too big, the variation of solutions produced by a training algorithm vary less over different stochastic realizations - see Appendix A for experiments for \(m=1,000\). The

Figure 1: **Left:** In Dimension \(d=31\), the minimum Barron norm solution \(f_{d}^{*}\) satisfies \(f_{d}^{*} 0\) on \(^{d} B_{r_{d}}(0)\) for \(r_{d}=0.4\) to high precision, albeit not exactly. The rescaled function \(f_{d}^{*}(r_{d}x)\) is a suitable candidate for a minimum norm almost-interpolant to high accuracy. **Right:** For later use, we consider more aggressively rescaled functions \(f_{d}^{*}(r_{d}x)\) for \(d=15\), \(d=31\) with lower semi-norm, but worse data fitting properties. We note that the rescalings of these functions which have essentially the same slope as \(f_{3}^{*}\) at \(r=0\) appear to coincide. We conjecture that this statement allows for a more rigorous formulation.

dynamics are close to those of a limiting'mean field' model studied by Chizat and Bach (2018); Rotskoff and Vanden-Eijnden (2018); Mei et al. (2018); Sirignano and Spiliopoulos (2020) and Wojtowytsch (2020). In these works, the limiting model is typically derived with a factor \(1/m\) outside the function definition, which is implicit in the initialization here since \(n_{in}+n_{out} m\) for both layers and the ReLU activation is positively one-homogeneous. Global convergence to a minimizer (but not necessarily a minimum norm solution) is guaranteed (up to certain technical assumptions) by Chizat and Bach (2018) and Wojtowytsch (2020).

For comparison, we also present the natural cubic spline interpolant, i.e. the function \(f\) which minimizes the stronger curvature energy \(_{-2}^{2}|f^{}(x)|^{2}\,x\) under the condition that \(f(x_{i})=|x_{i}|\) for all \(i=1,,n\). Unlike the minimum Barron interpolants, the natural cubic spline may not be convex (and in fact, it is not if \(f^{*}\) is replaced by \(h^{*}(x)=|x-0.5|\)).

### Radially symmetric data

We explore the performance of numerical optimization algorithms in the setting of Corollary 3.3 with and without explicit regularization \(\{0,10^{-5}\}\) in dimensions \(d=3\), \(d=15\) and \(d=31\). The numerical solution is then compared to (a rescaled version of) the analytic minimum norm interpolant \(f_{d}^{*}\) described in Proposition 3.2, which we compute by the algorithm described in (Wojtowytsch, 2022, Section 6). The rescaling factor \(r_{d}\) is chosen heuristically for an accurate match.

Figure 2: We compare numerical approximations of a target function for Momentum-GD (red), GD (magenta), ADAM (purple) and L-BFGS (brown). The target function is drawn in blue and the natural cubic spline in green. For each algorithm, we plot one representative solution to study symmetry selection properties. Vertical grey lines indicate known training data points. The initialization has gain \(\{0.5,1.0,5.0\}\) in the left, middle and right column. The weight decay penalty is \(\{0,0.002,0.005\}\) (top, middle, bottom row). For all optimization algorithms, the final loss is approximately \(0,2 10^{-4}\) and \(10^{-3}\) respectively.

Data is generated from a distribution \(=_{1}+_{2}+_{3}\) where \(_{1}\) is a point mass of magnitude \(m_{1}\) at the origin, \(_{2}\) is a uniform measure on the unit sphere \(S^{d-1}\) with mass \(m_{2}\) and \(_{3}\) is the radially symmetric measure of mass \(1-m_{1}-m_{2}\) such that \(\|x\|_{2}\) is distributed uniformly in \(\). We numerically explored various values for \(m_{1}[0.1,0.4]\) and \(m_{2}[0.0,0.4]\) and found simulations to be relatively stable under a number of choices.

Results are presented in Figures 3, 4 and Appendix A. We find that all algorithms find a solution with radial average similar to \(f_{d}^{*}(r_{d}x)\), albeit for rescaling factors \(r_{d}\) which depend on dimension \(d\) and (to a lesser extent) the optimizer. In high dimension, solutions are not perfectly radially symmetric, but the larger amount of variation over a sphere of fixed radius is observed in the domain where \(f_{d}^{*} 0\) rather than in the transition area \((0,r_{d})\). Larger datasets improve the compliance with the optimal interface and reduce the radial standard deviation. Solutions do not remain non-negative and drop below zero before leveling off as the radius increases. The drop becomes more noticeable as the dimension increases and less pronounced for wider networks.

The results are essentially identical for normal Xavier initialization and (not radially symmetric) uniform Xavier initialization. In accordance with our expectations, the radial standard deviation is higher for Adam compared to optimizers based in Euclidean geometry. While the neural network function found by Adam resembles a minimum norm interpolant, the weight decay regularizer takes significantly higher values compared to other optimization algorithms.

## 6 Conclusion

Shallow ReLU networks converge to minimum norm interpolants of given data: Provably if explicit regularization is included and empirically if it is not. We conclude with a summary of our empirical insight into the implicit bias of neural network optimizers.

Figure 3: A neural network with a single hidden layer of width \(m=12,000\) was trained by gradient descent with learning rate \(=10^{-3}\) and momentum \(=0.99\) in the setting of Section 5.2. The radial average is sketched by a solid red line. One radial standard deviation around the average, computed over 500 random directions, is shaded.z **Top row:** Experiment in dimension 3 (left) and dimension 15 (right). The numerical solutions are compared to \(f_{d}^{*}(r_{d}x)\) with \(r_{3}=1/1.05\) and \(r_{15}=1/2.55\). In both cases, the ‘minimum norm interpolant’ shape is attained to high accuracy. Both solutions are approximately symmetric, more so in low dimension. **Bottom row:** Numerical approximations to \(f_{15}^{*}(r_{15})\) for neural networks of constant width \(m=12,000\), trained on data sets of different size (but for an identical number of \(200,000\) training steps with stochastic estimates computed over a batch of \(50\) data points). The shape of the radial average is comparable across different dataset sizes, but the fit of the radial average with data is improved and the radial variance reduced for larger datasets. Note that the first two simulations are set in the overparametrized regime, whereas the last experiment on the largest dataset is underparametrized.

1. With reasonable (not too large) initialization, all algorithms studied here are biased towards minimum norm interpolant profiles.
2. At least in the case of Adam, this bias is visible on the function level, but not on the parameter level, as the weight decay regularizer increases rapidly to large magnitude. Despite this, ADAM solutions often appear 'flatter' in high dimension with a lower rescaling factor \(r_{d}\).
3. Explicit regularization stabilizes towards a minimum norm interpolant shape, but at the cost of a decreased fit with the target values. Its impact is most significant for poorly chosen initial conditions.
4. When the minimum norm interpolant is non-unique, different types of minimum norm interpolants are found depending on the choice of initialization scheme and optimization algorithm. The impact of initialization scale appears more significant.
5. Optimization algorithms which are rooted in Euclidean geometry (such as SGD and momentum-SGD) more successfully preserve Euclidean symmetries compared to the 'coordinate-wise' Adam algorithm.

The last observation is not surprising for radially symmetric initialization laws as radially symmetric parameter distributions induce radially symmetric functions. It is, however, observed also for a uniform initialization scheme which only obeys coordinate symmetries.

We believe minimum norm interpolation to be a useful testbed to study the implicit bias of optimizers and the impact of initialization and regularization. While minimum norm interpolation by deeper networks has not been characterized yet, we anticipate no obstructions to implementing a similar program there in the future.