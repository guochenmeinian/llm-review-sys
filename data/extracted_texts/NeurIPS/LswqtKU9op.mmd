# Prompt-augmented Temporal Point Process for Streaming Event Sequence

Siqiao Xue, Yan Wang, Zhixuan Chu, Xiaoming Shi, Caigao Jiang, Hongyan Hao

Gangwei Jiang, Xiaoyun Feng, James Y. Zhang, Jun Zhou

Ant Group

Hangzhou, China

{siqiao.xsq,luli.wy,chuzhixuan.czx}@alibaba-inc.com

These authors contributed equally to this work.Corresponding author.Our code is available at https://github.com/yangNan/PromptTPP

###### Abstract

Neural Temporal Point Processes (TPPs) are the prevalent paradigm for modeling continuous-time event sequences, such as user activities on the web and financial transactions. In real-world applications, event data is typically received in a _streaming_ manner, where the distribution of patterns may shift over time. Additionally, _privacy and memory constraints_ are commonly observed in practical scenarios, further compounding the challenges. Therefore, the continuous monitoring of a TPP to learn the streaming event sequence is an important yet under-explored problem. Our work paper addresses this challenge by adopting Continual Learning (CL), which makes the model capable of continuously learning a sequence of tasks without catastrophic forgetting under realistic constraints. Correspondingly, we propose a simple yet effective framework, PromptTPP1, by integrating the base TPP with a continuous-time retrieval prompt pool. The prompts, small learnable parameters, are stored in a memory space and jointly optimized with the base TPP, ensuring that the model learns event streams sequentially without buffering past examples or task-specific attributes. We present a novel and realistic experimental setup for modeling event streams, where PromptTPP consistently achieves state-of-the-art performance across three real user behavior datasets.

## 1 Introduction

Event sequences are ubiquitous in a wide range of applications, such as healthcare, finance, social media, and so on. Neural TPPs (Mei and Eisner, 2017; Shchur et al., 2020; Zuo et al., 2020; Zhang et al., 2020; Yang et al., 2022) have emerged as the dominant paradigm for modeling such data, thanks to their ability to leverage the rich representation power of neural networks. However, most existing works assume a _static_ setting, where the TPP model is trained on the entire data, and parameters remain fixed after training. In contrast, real-world event data usually arrives in a _streaming_ manner, rendering it impractical to store all data and retrain the model from scratch at each time step due to computational and storage costs. Shown in Figure 1, a common approach is to use sliding windows to frame the data for model training and prediction. Traditional schemes include pretraining a TPP, which is used for all the following test periods, retraining TPP on the data of each slide of windows and online TPPs. However, they either may fail to adapt to new data or suffer from _catastrophic forgetting_ (see Appendix A for an empirical analysis).

In our work, we approach the problem by adopting Continual Learning (CL) (Hadsell et al., 2020; Hao et al., 2023; Chu and Li, 2023; Chu et al., 2023), a relevant area studying how systems learn sequentially from a continuous stream of correlated data. Yet, classical CL models are not fullyapplicable to our problem. A major line of CL methods (Cha et al., 2021; Buzzega et al., 2020) rely on a rehearsal buffer to retrain a portion of past examples. However, they become ineffective when a rehearsal buffer is not allowed - for example, in real-world scenarios where data privacy matters (Shokri and Shmatikov, 2015) or there are resource constraints. Another branch of works (Ke et al., 2020) bypass the forgetting issue by assuming known task identity at test time, but knowing task identity at test time restricts practical usage. Furthermore, the problem of sequential tasks of event sequence in continuous time have barely been studied.

To develop a CL algorithm for such data in real-world scenarios with applicability and generality, we draw inspiration from recent advances in prompt-augmented learning (Liu et al., 2022; Varshney et al., 2022; Cho et al., 2022; Li et al., 2023; Chu et al., 2023; Wang et al., 2023). Prompt-augmented learning is a form of machine learning that involves adding additional information or prompts to the training data in order to further improve the performance of the model. This can include adding labels or annotations to the data, providing additional context to help the model better understand the data, or incorporating feedback from human experts to guide the learning process. By incorporating these prompts, the model is able to learn more effectively and make more accurate predictions. Prompt-augmented learning has been used successfully in a variety of applications, including natural language processing, computer vision, and speech recognition. Intuitively, prompt-augmented learning reformulates learning downstream tasks from directly adapting model weights to designing prompts that "instruct" the model to perform tasks conditionally while maintaining model plasticity. Thus, it is promising to leverage prompts to sequentially learn knowledge and further store learned knowledge of event sequence in the CL context. While prompt learning (Wang et al., 2022, 2022) already demonstrates its effectiveness on multiple CL benchmarks in language modeling, we wish to extend their success to the models of neural TPPs.

To this end, we propose **PromptTPP**, a novel CL framework whose basis is a **continuous-time retrieval prompt pool** for modeling streaming event sequences. Specifically, we develop a module of _temporal prompt_ that learns knowledge and further store the learned knowledge for event sequences in _continuous time_. To improve the applicability, building upon prior works (Wang et al., 2022), we structure the prompts in a key-value shared memory space called the _retrieval prompt pool_, and design a retrieval mechanism to dynamically lookup a subset of task-relevant prompts based on the instance-wise input of event sequences. The retrieval prompt pool, which is optimized jointly with the generative loss, ensures that shared (unselected) prompts encode shared knowledge for knowledge transfer, and unshared (selected) prompts encode task-specific knowledge that helps maintain model plasticity. PromptTPP has two distinctive characteristics: (i) **applicability**: despite the effectiveness in augmenting TPP with CL, the prompt pool and the event retrieval mechanism removes the necessity of a rehearsal buffer and knowing the task identity, making the method applicable to modeling the event streams in a more realistic CL setting, i.e., memory efficient and task agnostic. (ii) **generality**: our approach is general-purpose in the sense that it can be integrated with any neural TPPs. In summary, our main contributions are:

* We introduce PromptTPP, a novel prompt-augmented CL framework for neural TPPs. It represents a new approach to address the challenges of modeling streaming event sequences by learning a pool of continuous-time retrieval prompts. These prompts serve as parameterized instructions for base TPP models to learn tasks sequentially, thus enhancing the performance of the model.
* We formalize an experimental setup for evaluating the streaming event sequence in the context of CL and demonstrate the effectiveness of our proposed method across three real user datasets.
* By connecting the fields of TPP, CL, and prompt learning, our method provides a different perspective for solving frontier challenges in neural TPPs.

Figure 1: Overview of the classical schemes and PromptTPP framework for streaming event sequences.

## 2 Preliminaries

**Generative Modeling of Event Sequences.** Suppose we observe \(I\) events at a fixed time interval \([0,T]\). Each event is denoted mnemonically as \(e@t\) (i.e., "type \(\) at time \(\)") and the sequence is denoted as \(s_{[0,T]}=[e_{1}@t_{1},,e_{I}@t_{I}]\) where \(0<t_{1}<<t_{I} T\) and \(e_{i}\{1,,E\}\) is a discrete event type. Note that representations in terms of time \(t_{i}\) and the corresponding inter-event time \(_{i}=t_{i}-t_{i-1}\) are isomorphic, **we use them interchangeably**.

Generative models of event sequences are TPPs. Specifically, TPPs define functions \(_{e}\) that determine a finite **intensity**\(_{e}(t s_{[0,t)}) 0\) for each event type \(e\) at each time \(t>0\) such that \(p_{e}(t s_{[0,t)})=_{e}(t s_{[0,t)})dt\). Then the log-likelihood of a TPP given the entire event sequence \(s_{[0,T]}\) is

\[_{ll}=_{i=1}^{I}_{e_{i}}(t_{i} s_{[0,t_{i})})- _{t=0}^{T}_{e=1}^{E}_{e}(t s_{[0,t)})dt,\] (1)

Instead of posing strong parametric assumptions on the intensity function, neural TPPs (Du et al., 2016; Mei and Eisner, 2017; Zhang et al., 2020; Zuo et al., 2020; Yang et al., 2022) use expressive representations for the intensity function via neural networks and maximize the associated log-likelihood equation 1 via stochastic gradient methods.

**CL Problem Formulation for Streaming Event Sequences.** The typical CL problem is defined as training models on a continuum of data from a sequence of tasks. Given a sequence \(s_{[0,T]}\), we split it based on a sliding window approach shown in Figure 1 and form a sequence of tasks over the time \(\{_{0},...,_{N}\}\), where the \(\)-th task \(_{}=(s_{train}^{},s_{test}^{})\) contains a tuple of train and test set of event sequences and the two sets have no overlap in time. Data from the previous tasks are not available when training for future tasks. We use the widely-adopted assumption that the task boundaries are clear and the task switch is sudden at training time (Pham et al., 2021). Our goal is to continually learn the sequences while avoiding catastrophic forgetting from the previous tasks.

**Prompt Learning.** Prompt learning methods propose to simply condition frozen language models (LMs) to perform down-stream tasks by learning prompt parameters that are prepended to the input tokens to instruct the model prediction. Compared with ordinary fine-tuning, literature shows In our context, a naive application of prompt learning is to prepend learnable parameters \(}^{L_{p} D}\), called a prompt, to the event embedding \(=[}||]\), where \(^{D}\) denotes the output of a TPP's embedding layer of an event, and then feed it to the model function \(g()\), i.e., a decoder, to perform downstream tasks. Instead of the native application, in our proposed method, we design a novel prompt learning mechanism to properly model the event streams (see section 3.3).

## 3 Prompt-augmented TPP

We introduce a simple and general prompt-augmented CL framework for neural TPPs, named PromptTPP. As shown in Figure 2, PromptTPP consists of three components: a base TPP model, a pool of continuous-time retrieval prompts and a prompt-event interaction layer. In this section, we omit the task index \(\) in our notation as our method is general enough to the task-agnostic setting.

### Base TPP

A neural TPP model autoregressively generates events one after another via neural networks. For the \(i\)-th event \(e_{i}@t_{i}\), it computes the embedding of the event \(_{i}^{D}\) via an embedding layer, which takes the concatenation 2 of the type and temporal embedding \(_{i}=[_{i}^{}}||_{i}^{}}]\) where \(||\) denotes concatenation operation and \(_{i}^{}}^{D_{1}},_{i}^{}}^{D_{2}},D=D_{1}+D_{2}\). Then one can draw the next event conditioned on the hidden states that encode history information sequentially:

\[t_{i+1},e_{i+1}_{}(t_{i+1},e_{i+1}|_{i}),_{i}=f_{r}(_{i-1},_{i}),\] (2)

where \(f_{r}\) could be either RNN (Du et al., 2016; Mei and Eisner, 2017) or more expressive attention-based recursion layer (Zhang et al., 2020; Zuo et al., 2020; Yang et al., 2022). For the simplicity of notation, we denote the embedding layer and recursion layer together as **the encoder**\(f_{_{enc}}\) parameterized by \(_{enc}\). Our proposed PromptTPP is general-purpose in the sense that it is straightforward to incorporate any version of neural TPP into the framework.

### Continuous-time Retrieval Prompt Pool

The motivations for introducing Continuous-time Retrieval Prompt Pool (**CtRetroPromptPool**) are two-fold. First, existing prompt-learning works focus on classification tasks in NLP or CV domains,whose methods are not directly applicable for sequential tasks of learning event streams in continuous time (see section 4.2). Second, the practical setup for modeling event streams closes to the task-agnostic CL setting, where we do not know task identity at test time so that training task-dependent prompt is not feasible. Even if we use extra sources to memorize the task identity, naive usage of prompts (Liu et al., 2022, 2021; Tam et al., 2022) are still found to result in catastrophic forgetting.

For the first motivation, we construct _temporal prompt_ that properly encodes the knowledge of temporal dynamics of event sequence. To address the second, we build a store of prompts in a key-value shared space to transfer knowledge sequentially from one task to another without distinguishing between the common features among tasks versus the features that are unique to each task.

**Temporal Prompt.** In contrast to the standard prompt, the _temporal prompt_ is a time-varying learnable matrix that encodes not only the structural but also the temporal knowledge of the event sequence. We define the temporal prompt \(=[_{s};_{t}]^{L_{p} D}\), where \(L_{p}\) is the prompt length and \(_{s}^{L_{p} D_{1}},_{t}^{L_{p}  D_{2}}\) denotes the structural component and temporal component. While \(_{s}\) is a learnable submatrix, the temporal component \(_{t}\) is set to be continuous-time positional encodings of the estimated conditional time so as to consider the timing. More concretely, given \(i\)-th event, we estimate the arithmetic mean of inter-event times **up to \(t_{i-1}\)**, denoted by \(_{i}=[\{_{j}\}_{j<i}]\) and add this estimated inter-event time to \(t_{i-1}\) to get the estimated conditional time \(t_{p}:=_{i}=t_{i-1}+_{i}\). Inline with Yang et al. (2022), we compute the temporal embedding \((t_{p})^{D_{2}}\) by

\[(t)=(}(}{n_{te}})^{}{D_{2}}})d,(t)=(}(}{n_{te}})^{}})d\] (3)

where \(\{N_{te},n_{te}\}\) are hyperparameters selected according to the time scales in different periods. As \((_{i})\) is a vector, we concatenate it repeatedly to form \(_{t}\), i.e, \(_{t}=[(t_{p})||,...,||(t_{p})]^{L_{p}  D_{2}}\). Note that the **structural component \(_{s}\) is learnable while the temporal component \(_{t}\) is computed deterministically**.

An important consideration of employing such a mechanism is that the mean characterizes the most important property (the long-run average) of the inter-event time distribution, and the computation is straightforward. By taking the temporal embedding of the estimated average conditional time, the prompt efficiently encodes the time-varying knowledge up to the current event, which facilitates learning prediction tasks. We verify the effectiveness of temporal prompt in section 4.2.

**From Prompt to Prompt Pool.** Ideally, one would learn a model that is able to share knowledge when tasks are similar while maintaining knowledge independently otherwise. Thus, instead of

Figure 2: Overview of PromptTPP. Up: At training time, PromptTPP selects a subset of temporal prompts from a key-value paired CIRetroPromptPool based on our proposed retrieval mechanism; then it prepends the selected prompts to the event representations; finally it feeds the extended event representations into the prompt-event interaction and intensity layer, and optimizes the CRetroPromptPool through the loss defined in equation 11. Down Left: Illustration of how to parameterize a temporal prompt. Down Right: Illustration of prompt tuning in the prompt-event interaction layer.

applying a single prompt, we introduce a **pool of temporal prompts** to store encoded knowledge, which can be flexibly grouped as an input to the model. The pool is defined as

\[=[_{1},...,_{M}],\] (4)

where \(M\) denotes the total number of prompts and \(_{i}^{L_{p} D}\) is a single temporal prompt. Following the notation in section 3.1, recall \(_{i}^{D}\) denotes the hidden representation of the \(i\)-th event in the sequence 3 which encodes the event history up to \(t_{i}\) via the recursion by equation 2 and let \(\{_{r_{j}},j=1,...,N\}\) be a subset of \(N\) selected prompts, we then incorporate them into the event sequences as **in-context augmentation** as follows:

\[[_{r_{1}}||,...,||_{r_{N}}||_{i}],\] (5)

Prompts are free to compose, so they can jointly encode knowledge for the model to process, which provides flexibility and generality in the sense that a more fine-grained knowledge sharing scheme can be achieved via _prompt retrieval mechanism_. Under this mechanism, a combination of prompts is selected for each task - similar inputs tend to share more common prompts, and vice versa.

**Retrieval Prompt Pool.** The retrieval prompt pool shares some design principles with methods in other fields, such as RETRO (Borgeaud et al., 2022). Specifically, the prompt pool is augmented to be a key-value store \((,)\), defined as the set of learnable keys \(^{D}\) and values - temporal prompts \(\) in equation 4:

\[(,)=\{(_{i},_{i})\}_{i=1}^{M}\] (6)

The retrieval prompt pool may be flexible to edit and can be asynchronously updated during the training procedure. The input sequence itself can decide which prompts to choose through query-key matching. Let \(:^{D}^{D}\) be the cosine distance function to score the match between the query and prompt key. Given a query \(_{i}\), the encoded event vector, we search for the closest keys over \(\) via maximum inner product search (MIPS). The subset of top-N selected keys is denoted as:

\[_{top-N}=*{argmin}_{\{r_{j}\}_{j=1}^{N}}_{i=1}^{N }(_{i},_{r_{j}})\] (7)

Importantly, the design of this strategy brings two benefits: (i) it decouples the query learning and prompt learning processes, which has been empirically shown to be critical (see section 4.2); (ii) the retrieval is performed in an instance-wise fashion, which makes the framework become _task agnostic_, meaning the method works without needing to store extra information about the task identity at test time. This corresponds to a _realistic setting_ for modeling event streams in real applications.

### Prompt-Event Interaction

The interaction operation controls the way we combine prompts with the encoded event states, which directly affects how the high-level instructions in prompts interact with low-level representations. Thus, we believe a well-designed prompting function is also vital for the overall CL performance. The interaction mechanism is also called **prompting function** in the NLP community. We apply the multi-head self-attention mechanism (Vaswani et al., 2017) (MHSA) for modeling the interactions and adopt the mainstream realization of prompting function - Prefix Tuning (Pre-T) (Li and Liang, 2021). Denote the input query, key, and values as \(_{Q},_{K},_{V}\) and the MHSA layer is constructed as:

\[(_{Q},_{K},_{V})=[_{1}||,...,||_{m}| ^{O},\] (8)

where \(_{i}=(_{Q}_{i}^{Q},_{K}_{i}^{K},_{V}_{i}^{V}),^{O},_{i}^{Q},_{i}^{K},_{i}^{V}\) are projection matrix. In our context, let \(\{_{r_{i}}\}_{i=1}^{N}\) be the retrieved prompts from the pool, we set \(_{i}\) to be the query, split each prompt \(_{r_{i}}\) into \(_{r_{i}}^{K},_{r_{i}}^{V}^{L_{p}/2 D}\) and prepend them to keys and values, respectively, while keeping the query as-is:

\[_{i}^{Pre-T}=(_{i},[^{K}||_{i}],[^{V} ||_{i}]),\] (9)

where \(^{K}=[_{r_{1}}^{K}||,...,||_{r_{N}}^{K}|\),\(^{V}=[_{r_{1}}^{V}||,...,||_{r_{N}}^{V}]\). Apparently, the key and value \(_{K},_{V}^{(*N}{2}+1) D}\) and the output \(_{i}^{Pre-T}^{D}\). Note that there exist other prompting methods, such as _Prompt Tuning_ (Pro-T), where all the prompts concurrently prepend to the query, key and values:

\[_{i}^{Pro-T}=([^{Q}||_{i}],[^{K}||_{i }],[^{V}||_{i}]),\] (10)where \(^{Q}=^{K}=^{V}=[_{r_{1}}],...,[_{r_{N}}]\). As a result, the query, key, value and output \(_{Q},_{K},_{V},_{i}^{Pro-T}^{(L_{p}*N+1)  D}\). Despite being less efficient in computation, we empirically demonstrate that Pre-T brings better performance. See Analysis III in section 4.2.

The output of the MHSA is then passed into an intensity layer (an MLP with softplus activation) to generate the intensity \(_{e}(t_{i}),e\{1,...,E\}\). For simplicity, we denote the prompt-event interaction and intensity layer together as the **the decoder**\(f_{_{dec}}\) parameterized by \(_{dec}\).

### Model Optimization

The full picture of PromptTPP at training and test time is described in Algorithm 1 and Algorithm 2 in Appendix C.1. At every training step, each event \(e_{i}@t_{i}\) is recursively fed into the encoder \(f_{_{enc}}\), after selecting \(N\) prompts following the aforementioned retrieval strategy, the intensity \((t_{i})\) is computed by the decoder \(f_{_{dec}}\). Overall, we seek to minimize the end-to-end loss function:

\[_{,_{enc},_{dec},}_{nll}(,f_{ _{enc}},f_{_{dec}})+_{i}_{K_{top-N}}(f_{_{ enc}}(e_{i}@t_{i}),_{r_{j}}),\] (11)

where the first term is the negative loglikelihood of the event sequence (\(_{nll}\) equals to \(-_{ll}\) defined in equation 1) and the second term refers to a surrogate loss to pull selected keys closer to corresponding query in the retrieval process. \(\) is a scalar to control the importance of the surrogate loss. Given the learned parameters, we may wish to make a minimum Bayes risk prediction about the next event via the thinning algorithm (Mei & Eisner, 2017; Yang et al., 2022).

**Asynchronous Refresh of Prompt Pool.** The prompts may lead to the variable contextual representation of the event as the parameters of the based model are continually updated. To accelerate training, we propose to asynchronously update all embeddings in the prompt pool every \(C\) training epochs.

## 4 Experiments

### Experimental setup

**Datasets and Evaluation Setup** We conduct our real-world experiments on three sequential user-behavior datasets. In each dataset, a sequence is defined as the records pertaining to a single individual. The **Taobao**(Alibaba, 2018) dataset contains time-stamped user click behaviors on Taobao shopping pages with the category of the item involved noted as the event type. The **Amazon**(Ni, 2018) dataset contains time-stamped records of user-generated reviews of clothing, shoes, and jewelry with the category of the reviewed product defined as the event type. The **StackOverflow**(Leskovec & Krevl, 2014) dataset contains two years of user awards on a question-answering website: each user received a sequence of badges with the category of the badges defined as the event type. See Appendix D.1 for dataset details.

We partition Taobao and Amazon datasets into \(10\) consecutively rolling slides (namely \(10\) tasks) and partition the StackOverflow dataset into \(6\) rolling slides (namely \(6\) tasks). For the Taobao dataset, each slide covers approximately \(1\) day of time; for the Amazon dataset, each slide covers \(2\) years of time; for the StackOverflow dataset, each slide covers approximately 5 months time. The subset in each task is split into training, validation, and test sets with a \(70\%\), \(10\%\), \(20\%\) ratio by chronological order. Each task has no overlap in the test set. For a detailed discussion, a demonstration of the evaluation process is provided in Figure 9 in Appendix D.3.

**Metrics.** Following the common next-event prediction task in TPPs (Du et al., 2016; Mei & Eisner, 2017), each model attempts to predict every held-out event \((t_{i},k_{i})\) from its history \(_{i}\). We evaluate the prediction \(_{i}\) with the error rate and evaluate the prediction \(_{i}\) with the RMSE.

**Base models.** While our proposed methods are amenable to neural TPPs of arbitrary structure, we choose two strong neural TPPs as our base models: **NHP**(Mei & Eisner, 2017) and **AttNHP**(Yang et al., 2022), an attention-based TPP whose performance is comparable to or better than that of the NHP as well as other attention-based models (Zuo et al., 2020; Zhang et al., 2020).

**Competitors.** With NHP and AttNHP as base models, we trained _PromptNHP_ (**Pt-NHP**) and _PromptAttNHP_ (**Pt-ANHP**) in the proposed prompt-augmented setup and compared with \(7\) baselines.

* _PretrainedTPP._ _PretrainedNHP_ (**Pre-NHP**) and _PretrainedAttNHP_ (**Pre-ANHP**) represent NHP and AttNHP learned at the first task (time step) and not trained any longer.

* _RetrainedTPP_. _RetrainedNHP_ (**Re-NHP**) and _RetrainedAttNHP_ (**Re-ANHP**) refer to TPPs re-trained at every sliding widow.
* _OnlineTPP_. As there is no prior work on online neural TPPs, we use online Hawkes process _OnlineMHP_ (**O-TPP**) (Yang et al., 2017), trained in an online manner without any consideration for knowledge consolidation.
* _CLTPP_. The concurrent work (Dubey et al., 2022), to the best of our knowledge, is the only neural TPP with CL abilities proposed so far. Based on their work 4, we implement _CL-NHP_ (**CL-NHP**) and _CLAttNHP_ (**CL-ANHP**) as two variants of the hypernetwork-based CLTPPs. 
**Implementation and Training Details.** For a fair comparison, they (except O-TPP which is a classical TPP model) are of similar model size (see Table 2 in Appendix D.4). For Pt-NHP and Pt-ANHN, we set \(M=10,N=4,L_{p}=10\) for both datasets. During training, we set \(C=2\) by default and explore the effect of asynchronous training in Analysis IV of section 4.2. More details of the implementation and training of all the methods are in Appendix D.5.

### Results and Analysis

The main results are shown in Figure 3. Pre-NHP and Pre-ANHP work the worst in most cases because of inadequate ability to handle the distribution shift in the event sequence. Besides. O-TPP has a similarly poor performance because of two reasons: first it is a classical (non-neural) TPP with weaker representation power of modeling event sequence compared to its neural counterparts; second as a traditional online learning method, it easily loses memory of previously encountered data and suffers from _catastrophic forgetting_. Retraining at every task (Re-NHP and Re-ANHP) achieves

Figure 3: Performance of all the methods on Taobao (up), Amazon (middle) and StackOverflow (down). In each figure, the subfigures from left to right are the evolution of type error rate and the time RMSE of each task, the average error rate, and the average time RMSE of all the tasks.

moderate results but it also causes _catastrophic forgetting_. Not surprisingly, CL-NHP and CL-ANHP perform better than retraining, by applying a regularized hypernetwork to avoid forgetting. However, the hypernetwork relies on task descriptors built upon rich meta data, which limits its applicability and performance in our setup (and in real applications as well!). Lastly, our methods (both Pt-NHP and Pt-ANHP) work significantly better than all these baselines across the three datasets: they substantially beat the non-CL methods; they also consistently outperform CL-NHP and CL-ANHP by a relative \(4\%-6\%\) margin on both metrics, thanks to our novel design of the CtRetroPromptPool, which successfully reduces catastrophic forgetting (see Analysis 0).

**Analysis 0: How models perform on previous tasks after learning new events?** We aim to validate that the improvement in performances indeed is due to the alleviation in catastrophic forgetting instead of simply a better fit on the current task. We use ANHP trained on task \(9\) and Pt-ANHP _continuously_ trained on task \(9\), re-evaluate them on previous tasks and see how the metrics changed. Specifically, on Figure 4,

(i) Each number on the curves of Re-ANHP and CL-ANHP corresponds to the performance difference on the test set of task \(i,i<9\) using ANHP trained on task \(9\) vs ANHP trained on task \(i\).

(ii) Each number on the curves of Pt-ANHP corresponds to the performance difference on the test set of task \(i,i<9\) using Pt-ANHP trained until (including) task \(9\) vs Pt-ANHP trained until task \(i\).

See from Figure 4, on both metrics, we see the drop in performance (i.e., error rate / RMSE increases) of Pt-ANHP is much less significant than ANHP, indicating Pt-ANHP stores well the knowledge of previous tasks, which largely alleviates catastrophic forgetting.

Figure 4: The performance drop when re-evaluating the 0-8-th tasks using model trained on the \(9\)-th task on Amazon dataset.

Figure 5: Effect of temporal prompt and prompting function of PromptTPP.

Figure 6: Effect of hyperparameters of PromptTPP on Amazon dataset.

**Analysis I: Does stronger base TPP model naively improve CL?** Our method builds upon a backbone TPP and understanding this question is important for fair comparisons and future research. From Figure 3, Re-ANHP makes no consistent improvement against Re-NHP on average CL performance, which indicates a stronger TPP is not a solution for CL without being appropriately leveraged. Besides, for the CL-based methods, CL-ANHP is tied with CL-NHP on Taobao and makes a limited advancement against CL-NHP on Amazon, while Pt-NHP and Pt-ANHP perform closely on both datasets. Therefore, we can conclude that, although AttNHP is a more robust base model than common non attention-based TPP, i.e., NHP, it is not necessarily translated to CL performance.

**Analysis II: Temporal prompt vs standard prompt.** For a fair comparison, we initialize a pool of standard prompts without time-varying parameters by fixing their temporal components \(P_{t}\) to be an all-ones matrix and incorporate it into the base model AttNHP. This method is named Pt-ANHP-std. With other components fixed, we compare Pt-ANHP-std with Pt-ANHP to validate the effectiveness of the temporal prompt introduced in section 3.2.

Figure 4(a) shows that Pt-ANHP achieves better performance on both datasets: the introduction of temporal prompts slightly improves the RMSE metric and reduces the error rate with a larger margin. We did the paired permutation test to verify the statistical significance of the improvements. See Appendix D.6 for details. Overall, on both datasets, we find that the performance improvements by using the temporal prompts are enormously significant on error rate (p-value \(<0.05\) ) and weakly significant on RMSE (p-value \( 0.08\)).

**Analysis III: How to better attach prompts?** We explore how to attach prompts and enhance their influences on overall performance. We compare three types of prompting: 1_Naive Prompting_ (N-P), where the retrieval and prompting are performed after the event embedding layer: we replace \(_{i}\) with \(_{i}\) in equation 7, prepend the selected prompts to \(_{i}\) and pass it to the rest structure of TPP. 2_Prompt Tuning_ (Pro-T): which concurrently prepend the prompts to query, key, and value, introduced at the end of section 3.3. 3_Prefix-Tuning_ (Pre-T), proposed in the main body of section 3.3, which is the **prompting method used in PromptTPP**.

In Figure 4(b), we observe that Pre-T leads to better performance on both datasets compared to those two variants. Despite its empirically better performance, the architecture of Pre-T is actually more scalable and efficient when attached to multiple layers since it results in unchanged output size: \(_{i}^{Pre-T}^{D}\) remains the same size as the input while \(_{i}^{Pro-T}^{(L_{p}*N+1) D}\) increases the size along the prompt length dimension.

**Analysis IV: Efficiency of our method.** We examine the efficiency of our method in two steps:

* Firstly, seen from Table 2, on both datasets, Pt-NHP / Pt-ANHP leads to a \(12\%\,/\,8\%\) total parameter increase to the base model, which in fact causes a marginal impact on training speed: Figure 6(a) shows that learning curves of Re-ANHP and Pt-ANHP(\(C=1\)) converge at almost the same speed to achieve competitive log-likelihood, respectively.
* Furthermore, to accelerate the training (especially when introducing large size prompts), we introduce the asynchronous refresh mechanism (see section 3.4) with prompts updated in a frequency \(C>1\) (refresh the prompt pool less frequently). We observe in Figure 6(a) that Taobao training with \(C=2\) has a comparable performance with \(C=1\) while Amazon training with \(C=2\) improves the convergence notably. \(C=4\) leads to no advancement.

Figure 7: Effect of asynchronous refresh and prompt related components of PromptTPP on dataset.

Overall, PromptTPP only adds a small number of parameters so that it generally has the same convergence rate as the base model. The asynchronous prompt optimization scheme with \(C=2\) improves the convergence more remarkably on the Amazon dataset. In addition, we indeed provide a complexity analysis. See Appendix D.7.

**Analysis V: Effect of prompt related components of our method.** Firstly we completely remove the CtRetroPromptPool design (_w/o CtRroPP_ in Figure 6(b)) and use a single temporal prompt to train tasks sequentially. The performance declines with a notable drop, indicating that a single prompt suffers severe catastrophic forgetting between tasks, while our design of CtRetroPromptPool encodes task-invariant and task-specific knowledge well. Secondly, we remove the learnable key associated with prompts (_w/o k-v_ in Figure 6(b)) and directly use the mean of prompts as keys. This strategy causes a moderate drop in performance. To conclude, learnable keys decouple the query and prompt learning processes and markedly contribute to the performance.

**Analysis VI: Effect of hyperparameters of our method.** We evaluate how the performance of PromptTPP changes as we vary three key hyperparameters: (i) prompt length \(L_{p}\), (ii) selection size \(N\), and (iii) prompt pool size \(M\). Theoretically, \(L_{p}\) determines the capacity of a single prompt (which jointly encodes certain knowledge), \(L_{p} N\) is the total size used to prepend the event vector, while \(M\) sets the up limit of the capacity of learnable prompts.

* _Prompt length \(L_{p}\) and selection size \(N\)._ From the results in Figure 5(a), a too small \(L_{p}\) negatively affects results as a single prompt has a too limited ability to encode the knowledge. Besides, given an optimal \(L_{p}\), an overly large \(N\) makes the total prompts excessively oversized, leading to underfitting and negatively impacting the results. We conclude that a reasonably large \(L_{p}\) and \(N\) enable the model properly encode the shared knowledge between the tasks of event sequences and substantially improve the predictive performance.
* _Prompt pool size \(M\)._ Figure 5(b) illustrates that \(M\) positively contributes to the performance. This is because the larger pool size means the larger capacity of the prompts.

## 5 Conclusion

In summary, this paper has proposed a groundbreaking framework, known as PromptTPP, for modeling streaming event sequences. By incorporating a continuous-time retrieval prompt pool, the framework effectively facilitates the learning of event streams without requiring rehearsal or task identification. Our experiments have shown that PromptTPP performs exceptionally well compared to other competitors, even under challenging and realistic conditions.

## 6 Limitations and Societal Impacts

**Limitations.** Our method uses neural networks, which are typically data-hungry. Although it worked well in our experiments, it might still suffer compared to non-neural models if starved of data.

**Societal Impacts.** By describing the model and releasing code, we hope to facilitate probabilistic modeling of continuous-time sequential data in many domains. However, our model may be applied to unethical ends. For example, it may be used for unwanted tracking of individual behavior.