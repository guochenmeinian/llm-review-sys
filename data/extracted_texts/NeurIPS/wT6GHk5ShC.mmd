# Gradient quality derived from context (i.e., \(\mathbf{G}_{t}\)).

Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective

 Xinhao Yao\({}^{1}\), Xiaolin Hu\({}^{1}\), Shenzhi Yang\({}^{1}\), Yong Liu\({}^{1}\)

\({}^{1}\)Renmin University of China, Beijing, China

{yaoxinhao021978, xiaolinhu, yangshenzhi2003, liuyonggsai}@ruc.edu.cn

Corresponding author.

###### Abstract

Pre-trained large language models (LLMs) based on Transformer have demonstrated striking in-context learning (ICL) abilities. With a few demonstration input-label pairs, they can predict the label for an unseen input without any parameter updates. In this paper, we show an exciting phenomenon that SVD-based weight pruning can enhance ICL performance, and more surprising, pruning weights in deep layers often results in more stable performance improvements than in shallow layers. However, the underlying mechanism of those findings still remains an open question. To reveal those findings, we conduct an in-depth theoretical analysis by presenting the implicit gradient descent (GD) trajectories of ICL and giving the mutual information based generalization bounds of ICL via full implicit GD trajectories. This helps us reasonably explain the surprising experimental findings. Besides, based on all our experimental and theoretical insights, we intuitively propose a simple, model-compression and derivative-free algorithm for downstream tasks in enhancing ICL inference. Experiments on benchmark datasets and open source LLMs display the method effectiveness2.

## 1 Introduction

Recently, large language models (LLMs) based on the Transformer architecture  have emerged striking in-context learning (ICL) capabilities: Given a prompt containing demonstration sample and a test data, the model can make a prediction for the test data and achieve excellent performance without any parameter updates . This leads considerable works that aim to shed light on it .

In this paper, we show our surprising findings in ICL inference by experimentally analyzing the effect of singular value decomposition (SVD)-based pruning on the model performance at different depth layers. As demonstrated in Figure 1: (i) SVD-based weight pruning can enhance ICL performance. Across all cases, it is evident that SVD-based weight pruning can generally enhance model performance at various depth layers, compared to the baseline performance without weight pruning (indicated by the dashed lines); (ii) Pruning weights in deep layers often results in more stable performance improvements than in shallow layers. Specially, deep layers weight matrices can be drastically reduced without much degradation in model performance, and may even get large improvements on it, while the model performance collapses after a sharp reduction at the shallow layers (see Section 2 for details). A similar case in Sharma et al.  notes that a large portion of singular values can be removed from linear layers in large language models without affecting or even improving reasoning performance. However, the underlying mechanism of this phenomenon still remains a mystery, and this paper seeks to explore the issue from the following two aspects.

**Why this phenomenon?** We conduct an in-depth theoretical analysis to explain the findings. To be more specific, we first analyse the ICL form with Transformer from the perspective of ICL as implicit gradient descent fine-tuning, and we present the implicit gradient descent trajectories of ICL in **Theorem 1** in Section 3.2. Afterwards, we use the information-theoretic approaches  to give the generalization bounds of ICL via full implicit GD trajectories in **Theorem 2** in Section 3.3, explaining (**Q1**) why SVD-based weight pruning can enhance ICL performance? and (**Q2**) why do deep and shallow layers exhibit different behaviors when their weight matrices are drastically reduced?

**How to better use this phenomenon (Q3)?** Based on all our experimental and theoretical insights, we intuitively propose a simple, derivative-free and model-compression algorithm in Section 3.4 for downstream tasks in enhancing ICL inference, providing developers and researchers with an effective approach for handling downstream tasks. Experimental results for our algorithm on benchmark datasets  and open source LLMs  verify that the method can visibly influence performance across different language understanding tasks in ICL inference.

Our primary objective is to establish a general theoretical framework that uncovers the underlying mechanism behind the phenomenon that SVD-based weight pruning can enhance ICL performance. Based on our theoretical insights, one can design new ICL algorithms. Accordingly, we did not directly compare our approach with other pruning methods. The algorithm in Section 3.4 is presented solely to illustrate how theoretical analysis can guide experimental procedures effectively.

Figure 1: The effect of weight pruning across different layer types. The figure shows the phenomenon observed on the benchmark datasets (SST-2, RTE, COPA) and open source LLMs (GPT-J-6B and LLAMA2-7B). Each sub-figure corresponds only to the indicated type of dataset, model and module. Notice that this figure mainly focuses on exhibiting the impact of weight pruning to the first two and the last two layers of the model and different colors are used to distinguish between these layers. The dashed line represents the pretrained model performance without SVD. We operate on the whole of MLP or ATTN and specifically marked the points of highest performance. The amount of weight pruning is severe, for instance, the highest model performance sometimes occurs at a clipping rate of 0.995. This is about 99.5% of the matrix’s original rank. For the definitions of “deep” and “shallow”, please refer to Appendix B.2.

### Related Works

**Model compression.** In recent years, there has been growing theoretical and experimental evidence that models can be significantly pruned with very little drop in accuracy, thereby significantly reducing their storage requirements. To name a few, Frankle and Carbin  indicate that neural networks can typically have over 90% of their weights eliminated with little to no loss in performance. Analogous results have been demonstrated in both feed-forward and convolutional networks used for image classification [24; 49; 22; 37]. More specifically, Sharma et al.  present that careful pruning done at specific layers of Transformer models can produce significant boosts in performance on some tasks. The discovery of this phenomenon heightens interest in the connection between generalization and over-parameterization [56; 57], it also spurs research into developing pruning strategies that facilitate efficient model inference . Additionally, the works [19; 40] find that performance remains nearly unchanged until a significant portion (up to half) of the layers are removed.

**In-context learning and gradient descent.** In order to better understand the ICL capabilities, considerable works try to understand ICL capabilities from the perspective of gradient descent. Irie et al.  and Dai et al.  explain ICL as implicit fine-tuning by figuring out a dual form of gradient descent for linear Transformer attention. However, the linear attention setting is less commonly used than Softmax attention in the LLMs and the details of gradient descent such as the choice of loss function and training data have not been clearly defined. Therefore, by using weight construction, von Oswald et al.  show the equivalence of linear self-attention mechanism and gradient descent on a linear regression task and Akyurek et al.  prove that based on gradient descent and closed-form ridge regression, Transformers can learn linear models as learning algorithms. Further without using weight construction, Ren and Liu  connect Softmax attention with kernels and then give a novel interpretation that ICL with Transformer is really equivalent to a contrastive learning pattern.

**Information-theoretic generalization bounds.** The information-theoretic generalization bounds have been developed to analyze the expected generalization error of a learning algorithm. Given that they are dependent of distribution and algorithm, they are ideal tools to study the generalization behaviour of models performed with a specific algorithm. Russo and Zou  and Xu and Raginsky  first propose the Mutual information (MI) based bounds, which are then strengthened by additional techniques [4; 31; 20; 51]. Specifically, Negrea et al.  derive MI-based bounds by developing a PAC-Bayes-like bounding technique and Wang and Mao  develop generalization bounds for SGD via constructing an auxiliary iterative noisy process.

## 2 SVD-Based Weight Pruning can Enhance ICL

This section shows our surprising findings in ICL inference by performing a motivating analysis of three benchmark datasets in conjunction with two open source LLMs, the details are as follows. We choose GPT-J (6B,28 layers)  and LLAMA2 (7B,32 layers)  as our primary models for investigation, owing to their robust ICL performance and moderate model size, which align with our hardware resource. The attention (ATTN) layers are made up of key, query, value, out matrices both in GPT-J-6B and LLAMA2-7B. The mlp (MLP) layers in GPT-J-6B consist of input, output matrices, while the MLP layers in LLAMA2-7B are made up of up, gate, down matrices. For datasets, we use SST-2  for sentiment analysis, RTE  for textual entailment and COPA  for causal inference. Details regarding datasets and how they were used are shown in Appendix C.1 and C.3.

Specially, we use the optimal rank-\(r\) approximation mentioned later as SVD-based weight pruning method to show the effect of weight pruning across different layer and module types in Section 2.1, then we further analyze the effect of the ICL shot numbers on it in Section 2.2.

**The optimal rank-\(r\) approximation and SVD.** Given a matrix \(^{m n}\) and a constant \(r(m,n),r\). Eckart-Young-Mirsky theorem  provides an optimal solution \(^{*}(r)=_{:r}_{:r}_{:r}^{T}, _{:r},_{:r}\) are matrices containing the singular vectors corresponding to the largest \(r\) singular values \([]_{1}^{r}\). Let \(=1-(0,1)\) be the clipping rate.

### Effect of Weight Pruning across Different Layer and Module Types

We plot the results of applying various amounts of clipping rate \(\) to each module matrices in the Transformer architecture on the corresponding evaluation index for each dataset, as depicted in Figure1. These plots are grouped, such that each sub-figure corresponds only to the indicated type of dataset, model and module. Notice that this investigation mainly focuses on assessing the impact of weight pruning to the first two and the last two layers of the model to further clarify the impact of layer depth on the model's performance and different colors are used to distinguish between these layers.

All sub-figures clearly show an interesting phenomenon about these models in ICL inference: SVD-based weight pruning can enhance ICL performance in both shallow and deep layers across different module types. More surprising, deep layers weight matrices can be drastically reduced without much degradation in model performance, and may even get large improvements on it. This suggests that pruning weights in deep layers can effectively reduce model complexity while maintaining or enhancing model performance. And the model performance collapses to 0.5 (the expectation of a random guess in the binary task) after a sharp reduction at the shallow layers by contrast, indicating a higher sensitivity of the model to changes in shallow layers.

Based on the surprising findings in ICL inference mentioned above, three questions are certain to arise: (**Q1**) Why SVD-based weight pruning can enhance ICL performance? (**Q2**) Why do deep and shallow layers exhibit different behaviors when their weight matrices are drastically reduced? (**Q3**) How can we better use the phenomena about ICL in downstream tasks? We will address the first two questions theoretically in Section 3.3 and give a heuristic algorithm in Section 3.4 to answer the last.

### Effect of Different ICL Shot Numbers

Given that ICL can achieve higher performance with more demonstrations [23; 1], we further analyze the non-ignorable effect of different ICL shot numbers. To control for other influencing factors, we focus on SST-2 dataset  and retain the same test set for a single random seed as Section 2.1.

Figure 2: The effect of different ICL shot numbers is not uniform. Here we show the effect of different ICL shot numbers on the phenomenon mentioned in Section 2.1 as studied on the SST-2 dataset. Each row represents the results of the same shot numbers in different layers and modules, and each column represents the results of the different shot numbers in different layers of the same module. We also specifically marked the points of highest performance.

As shown in Figure 2, we compare the settings of three different ICL shot numbers: 0, 4 and 10. Following this, we analyze how the phenomenon changes across different shot numbers.

Firstly, we note that without weight pruning, the performance of the model improves with an increase in the number of ICL shots. Which is consistent with prior works. Besides, for every shot number, a phenomenon consistent with what is described in Section 2.1 is observed: SVD-based weight pruning can enhance ICL performance, pruning deep layer weight is more stable than pruning shallow weight. Last but not the least, Figure 2 also demonstrates roughly that with a decrease in the number of shots, the rate of performance collapse in the model slows down after a sharp reduction at the shallow layer. Intuitively, this is because LLMs exhibit a shift in focus in the ICL setting, which results in a reduced scope of the output space. The more shots there are, the more pronounced the shift becomes, and this also leads to a faster collapse. We also theoretically discuss it in Section 3.2 and 3.3 (**Remark 6**).

## 3 Theoretical Analysis Results

In this section, we first describe the core components of our study by reviewing some basic notations in Section 3.1, and present the implicit gradient descent trajectories of ICL in Section 3.2. Afterwards, we give a mutual information based generalization bounds of ICL via full implicit GD trajectories in Section 3.3. Based on all our experimental and theoretical insights, we intuitively propose a derivative-free and effective method for downstream tasks in enhancing ICL inference in Section 3.4. Complete proofs can be found in the Appendix. For ease of qualitative analysis, our theoretical analysis is mainly focuses on linear attention setting. We discuss the standard Softmax attention setting in Appendix A.2 and feed-forward (MLP) layers in Appendix A.3.

### Preliminaries

We let \(\) be the instance space and \(\) be an unknown distribution on \(\), specifying random variable \(\). In ICL setting, the model accepts a sequence of input \(=[_{s},_{N+1}]\) drawn i.i.d. from \(\), where \(_{s}=[_{1},_{2},...,_{N}]\) represents the demonstration sample and \(_{N+1}\) is the test data. In the information-theoretic analysis framework, we let \(^{d}\) be the space of hypotheses related to the model, and Transformer performs an implicit stochastic learning algorithm \(\) which takes the demonstration sample \(_{s}\) as its input and outputs a hypothesis \(W\) according to some conditional distribution \(Q_{W|_{s}}\). Similar to previous works [55; 51], we give the definition of expected generalization error.

**Expected generalization error.** Given a loss function \(:^{+}\), where \((w,)\) measures the "unfitness" or "error" of any \(\) with respect to a hypothesis \(w\). We take \(\) as a continuous function and assume that \(\) is differentiable almost everywhere with respect to \(w\). The goal of learning is to find a hypothesis \(w\) that minimizes the population risk, and for any \(w\), the population risk is defined as \(L_{}(w)_{}[(w,)]\). However, since \(\) via the sample \(_{s}\) can only be partially observed, we instead turn to use the empirical risk, defined as \(L_{_{s}}(w)_{i=1}^{N}(w,_{i})\). Then the expected generalization error of \(\) is defined as

\[}_{W,_{s}}[L_{}(W)-L_{ _{s}}(W)],\]

where the expectation is taken over \((_{s},W)^{N} Q_{W|_{s}}\).

**In-context learning with Transformers3.** By prompt design, most context semantic understanding tasks can be unified into classification tasks. Simplify the form of each token in \(\) to \(_{i}=[_{i},_{i}]\), where \([_{i}]_{1}^{N}^{din}\) and \([_{i}]_{1}^{N}^{dout}\) are encoded input text and corresponding labels respectively. The test token has the form \(_{N+1}=[_{N+1},mask]\), where \(mask\) is the label needed to predict, and usually set \(0\) as its initialization. Therefore, the form of attention with residual connection4 is as follows:

\[}=+_{V}(_{K})^{T}_{Q})}{ }}),\] (1)where \(_{V}\),\(_{K}\),\(_{Q}^{(dout+din)(dout+din)}\) are projection matrix, and the mask matrix \(=(_{N N}&0\\ 0&0)\) is included in the attention. Specifically, \(}_{N+1}\) can be formulated as

\[}_{N+1}=_{N+1}+_{V} (_{K})^{T}_{Q}_{N+1})}{}}),\] (2)

and for ease of qualitative analysis, Eq.(2) can be approximated as a relaxed linear attention mechanism by removing the Softmax operation and scale factor:

\[}_{N+1}=_{N+1}+_{V}( _{K})^{T}_{Q}_{N+1}.\] (3)

### The Implicit Gradient Descent Trajectories of ICL

To begin with, we present the implicit gradient descent of ICL, inspired by . These works describe how ICL with attention can be connected to a meta-optimizer which produces implicit gradient. The following lemma demonstrates the result.

**Lemma 1** (The Implicit Gradient Descent of ICL in a Single Linear Attention Layer).: _Consider a Transformer consists of a single linear layer attention with residual connection, parameterized by \(_{V}\),\(_{K}\),\(_{Q}\) as in Eq.(3). Same to Section 3.1, let \(=[_{s},_{N+1}]\) be the input, where \(_{s}=[_{1},_{2},...,_{N}]\) represents the demonstration sample and \(_{N+1}\) is the test data. And let \(}_{N+1}\) be the single layer output. Then, it holds that_

\[_{icl}=_{V}_{s}(_{K}_ {s})^{T}_{Q}=(_{i=1}^{N}_{V}_{i} _{K}_{i})_{Q},\]

\[}_{N+1}=_{N+1}+_{icl}_{N+1},\]

_where \(_{V}_{s}\) is regarded as the meta-gradient of ICL, which is used to generate the implicit gradient matrix \(_{icl}\) to act on the final feature output. See Appendix A.1 for a proof._

**Remark 1**.: _It is worth noting that \((_{icl})(_{s}) N\) by rank relations in matrix multiplication, indicating that this is a low-rank operation. This explains why the effect of different ICL shot numbers is not uniform in Section 2.2. Details of the discussion in standard Softmax attention setting can be found in Appendix A.2. Plus, a significant body of work has discovered that the ICL capabilities of models are not robust to the order of ICL sample. For instance, Lu et al.  observed that large language models (LLMs) are sensitive to the sequence of ICL examples, and Liu et al.  reported that the context-based ICL performance exhibits a U-shaped curve--models tend to perform better with information that appears at the beginning or at the end of the input context. This aligns with observations from actual model training where the sequence of training samples affects outcomes especially when training with a batch size of \(1\)._

According to the above, we further analyze the implicit gradient descent trajectories of ICL and provide the following Theorem. We will denote \(_{icl}^{t}\) by \(_{t}\) when there is no ambiguity, where \(t\) represents \(t\)-th layer.

**Theorem 1** (The Implicit Gradient Descent Trajectories of ICL).: _Consider a Transformer as a stack of \(L\) linear attention blocks with residual connection, parameterized by \([_{V}^{t}]_{1}^{L},[_{K}^{t}]_{1}^{L},[_{Q}^{t} ]_{1}^{L}\). Denote \([_{i}^{t}]_{1}^{N+1}\) as the output of the \(t\)-th layer, \([_{i}^{0}]_{1}^{N+1}\) as the initial input. Then for \(t[L]\), it holds that_

\[_{t}=_{t}(1+_{t-1})= _{t}(1+_{0}+_{j=1}^{t-1}_{j}),\] \[_{N+1}^{t}=_{N+1}^{0}+_{j=1}^{t} _{t}_{N+1}^{0}=_{N+1}^{0}+_{t}_{N+1}^ {0},\]

_where \(_{t}(_{i=1}^{N}_{V}^{t}_{i}^{t-1}_{K}^{t}_{i}^{t-1})_{Q}^{t}\), \(_{0}=0,_{t}=_{t-1}+_{t}\) and \(_{1}=_{1}\). See Appendix A.4 for a proof._

**Remark 2**.: _Note that the exclusion of Transformer weight \(([_{K}^{t}]_{1}^{L},[_{Q}^{t}]_{1}^{L},[_{V}^{t }]_{1}^{L})\) implies that \(_{t}\) is only dependent on \(_{t-1}\) and \(_{s}\), this is consistent with gradient descent in terms of relevance._

### Generalization Bounds of ICL via Full Implicit GD Trajectories

In this part, for simplicity of representation, we flatten the weight matrix into a vector form \(((_{t})=W_{t}^{d},(_{t })=G_{t}^{d})\) and conduct our analysis within the weight and implicit gradient space of hypotheses \(\) as detailed in Section 3.1. Notably, Ahn et al.  observe that, with the optimal parameters, a single layer linear Transformer implements a single step of preconditioned gradient descent and the preconditioning matrix not only adapts to the distribution of input data but also to the variance caused by data inadequacy. Garg et al.  show empirically that the trained Transformer is able to in-context learn the class of linear functions with respect to the prompt distribution, performing comparably to the optimal least squares estimator for any number of in-context examples considered, as exhibited in Figure 4(a).

In addition, we also make a discussion on the noise of the implicit gradient in Section C.4, our analytical discussion indicates that the implicit gradients produced by Transformers in practical applications are noisy due to factors such as the extent of model pre-training and data characteristics (e.g., ICL shot number). We first present the assumption used in this subsection.

Let \(G_{t}_{i=1}^{N}_{i}\) be the best implicit gradient of ICL that the model can produce, \(_{t}_{i=1}^{b}_{i}\) be the implicit gradient of ICL generated by the model in practical applications. \(N\) is the threshold and \(b\) is the the shot number in the actual input defined in Section C.4. And \(V_{t} G_{t}-_{t}\) is the gradient noise caused by shot number, \(C_{t}(_{i=1}^{N}_{i} _{i}^{T}-G_{t}G_{t}^{T})\) is the implicit gradient noise covariance. Similar to Wang and Mao 's assumption in SGD, we approximate \(V_{t}\) up to its second moment.

**Assumption 1**.: _Assume the implicit gradient noise \(V_{t}\) follows a Gaussian distribution, i.e., \(V_{t}(0,C_{t})\), then in ICL implicit gradient descent trajectories,_

\[W_{t}=W_{t-1}-_{t}=W_{t-1}- G_{t}+ C_{t}^{1/2}N_{t},\] (4)

_where \(N_{t}(0,I_{d})\) is the standard Gaussian5._

**Remark 3**.: _Regarding the validation of this assumption, empirical evidence from works , suggests that SGD and Eq.(4) can achieve the similar testing performance. Additionally, we refer readers to some recent works , where the authors empirically verify that in-context learning of Transformer can achieve the similar testing performance to SGD. Together suggesting that studying Eq.(4) is arguably sufficient to understand generalization properties of ICL._

**Theorem 1** indicates that the initial parameter \(W_{0}=0\), which is independent of all other random variables. And an \(L\)-layer Transformer does implicit GD of ICL stops after \(L\) updates, outputting \(W_{L}\) as the implicit learned parameter. Our main results are mutual information based expected generalization error bounds of ICL, as presented in **Theorem 2**.

**Theorem 2** (The Generalization Bounds of ICL via Full Implicit Gradient Descent Trajectories).: _Under the conditions of **Theorem 1** and **Assumption 1**, assume the implicit gradient noise covariance \(C_{t}\) is a positive-define matrix, the loss \((w,)\) is R-subGaussian for any \(w^{d}\), then_

\[}}{N}_{t=1}^{L}_{ _{t-1}}^{_{s}}[d(_{t}\|_{F}^{2}\|1+_{j=1}^{t-1}_{j} \|_{F}^{2}+\{C_{t}\}}{d})-\{  C_{t}\}]},\]

_where \((_{t})^{d}\), \(_{t}(_{i=1}^{N}_{V}^{t}_{i}^{t-1}_{K}^{t}_{i}^{t-1})_{Q}^{t}\) and \(_{t}=_{t}(1+_{0}+_{j=1}^{t-1}_{j})=_{t}(1+_{t-1})\). \(\{\}\) denotes the trace of a matrix, \(\|\|_{F}\) denotes the Frobenius norm of a matrix and \(_{Y}^{X}\) is the conditional expectation. Proof details in Appendix A.5._

**Remark 4** (Deal with **Q1**).: _Theorem 2 indicates that one can control the generalization performance of ICL via controlling the implicit gradient norm along the entire ICL implicit GD trajectories. Specifically, modulating the norm of \([_{t}]_{1}^{L}\) or \([_{t}]_{1}^{L}\) may enhance performance when utilizing ICL. Note that controlling implicit gradient norm can also control the magnitude of the trace of implicitgradient noise covariance. This elucidates why weight pruning through SVD, even if it only alters a single weight matrix, can confer advantages on the performance of Transformers in ICL inference. We will present an example below demonstrating how weight pruning can affect the norm of \(_{t}\) or \(_{t}\), thereby influencing the expected generalization error. Additional examples provided in Appendix A.6. This is also why, as illustrated in Figure 1, the highest model performance sometimes occurs at a clipping rate of 0.995. Furthermore, this could elucidate the utility of normalization in Transformers. However, weight pruning also impacts the expressive power, implying that increased pruning does not invariably lead to better outcomes. This clarifies why the highest model performance may also occur at clipping rates lower than 0.995._

**Example 1** (Prune \(_{Q},_{K},_{V}\)).: _Consider \(_{k}=(_{i=1}^{N}_{V}^{k}_{i}^{k-1 }_{K}^{k}_{i}^{k-1})_{Q}^{k}\) when \(t=k\). And we primarily consider the changes in a upper bound written as \(UB(\|_{k}\|_{F}^{2})_{i=1}^{N}\| _{V}^{k}_{i}^{k-1}_{K}^{k}_{i} ^{k-1}\|_{F}^{2}\|_{Q}^{k}\|_{F}^{2}\). Let \(r\) represents the remained rank and \(\) represents the potential noise consisting of parts with small singular values, that is, \(_{V}^{k}=_{V_{r}}^{k}+_{V}=_{:r}^{V} _{:r}^{V}(_{:r}^{V})^{T}+_{V}\), the same operation is applied to \(_{Q}^{k}\) and \(_{K}^{k}\) as well. Then we have_

\[\|_{V}^{k}_{i}^{k-1}\|_{2}^{2} =\|(_{V_{r}}^{k}+_{V})_{i}^{k-1} \|_{2}^{2}\] \[=\|_{V_{r}}^{k}_{i}^{k-1}\|_{2}^{ 2}+2(_{V_{r}}^{k}_{i}^{k-1})^{T}(_{V}_{i} ^{k-1})+\|_{V}_{i}^{k-1}\|_{2}^{2}\] \[=\|_{V_{r}}^{k}_{i}^{k-1}\|_{2}^{ 2}+\|_{V}_{i}^{k-1}\|_{2}^{2}\| _{V_{r}}^{k}_{i}^{k-1}\|_{2}^{2},\]

_where \(_{V_{r}}^{k}=_{:r}^{V}_{:r}^{V}(_{:r}^{V})^{T}\) and \(_{V}=_{:r}^{V}_{:r}^{V}(_{:r}^{V})^{T}\), and \(_{:r},_{r}\), are orthometric (properties of SVD), and further have_

\[UB(\|_{k}\|_{F}^{2}) _{i=1}^{N}\|_{V_{r}}^{k}_{i}^{k- 1}_{K}^{k}_{i}^{k-1}\|_{F}^{2}\| _{Q}^{k}\|_{F}^{2}\] (*) \[_{i=1}^{N}\|_{V_{r}}^{k}_{i}^{k- 1}_{K_{r}}^{k}_{i}^{k-1}\|_{F}^{2}\| _{Q}^{k}\|_{F}^{2}\] \[_{i=1}^{N}\|_{V_{r}}^{k}_{i}^{k- 1}_{K_{r}}^{k}_{i}^{k-1}\|_{F}^{2}\| _{Q_{r}}^{k}\|_{F}^{2}=UB(\|_{k}(r)\| _{F}^{2}),\] (**)

_where Eq. (*) is by6_

\[\|()()\|_{F}= _{j}|()_{i}()_{ j}|^{2}}=_{i}|_{i}(()( ))|\]

_and Eq. (**) is by \(\|\|_{F}=_{i}^{2}()}\) for any matrix \(,\) and \(\|\|_{F}\|(r)\|_{F}\). \(UB(\|_{k}(r)\|_{F}^{2})\) is the upper bound on \(\|_{k}\|_{F}^{2}\) after using SVD._

**Remark 5** (Deal with **Q2**).: _It is notable that \(_{t}\) is highly correlated with the sequence \((_{1},...,_{t})\). More precisely, adjusting the weight matrix of the \(k\)-th layer, will invariably impact the norm of \([_{t}]_{t>k}^{L}\), further influence \([_{t}]_{t>k}^{L}\). When we calibrate the weight matrix of the \(k\)-th layer, the span of affected weight updates \(_{t}\) encompasses \(L-k+1\) matrices, this indicates that the deeper the layer of the adjusted parameters is, the fewer the number of \(_{t}\) affected. It also suggests that tweaks to the deep layers yield a more steadfast influence on the global norm of \([_{t}]_{1}^{L}\), thereby exerting a steadier influence over generalization performance. The enhanced stability observed in adjusting deeper layers, as demonstrated in our experimental findings in Section 2, can be theoretically explained based on the analysis presented above._

**Remark 6** (How should **Theorem 2** be interpreted?).: _Expected generalization error (**Theorem 2**) = population risk (\(L_{}\)) - empirical risk (\(L_{_{s}}\)). More specifically, on the one hand, **Theorem 2** showsthat clipping weights controls the F-norm of the implicit gradient \(([_{t}]_{1}^{L}/[_{t}]_{1}^{L})\), which helps reduce the expected generalization error. On the other hand, we can evaluate the empirical risk \((L_{_{s}})\) by assessing the model's performance on the validation set. If the generalization error is known, it is possible to estimate the population risk (\(L_{}\)). Therefore, the most challenging aspect is addressing the generalization error. Furthermore, we will illustrate the interpretation of **Theorem 2** from the perspectives of pruning methods and the ICL shot numbers. (i) Adjusting the F-norm of \(([_{t}]_{1}^{L}/[_{t}]_{1}^{L})\) could enhance performance when utilizing ICL, suggesting that other weight-based pruning methods may also be effective. For example, magnitude-based pruning  directly controls the matrix F-norm. Certainly, there are also some layer-based pruning methods (e.g., drop-layer method ), we discuss in detail in Appendix B.6. (ii) Reducing the ICL shot numbers from \(N\) to \(N^{{}^{}}\) can indeed lead to more robust outcomes specifically in the "SVD weight pruning" operation, rather than in the model's overall performance. Experimentally, in Section 2.2, we analyze the effect of different ICL shot numbers. As shown in Figure 2, with a decrease in the number of shots, the rate of performance collapse in the model slows down after a sharp reduction at the shallow layer. Theoretically, \((N)-(N^{{}^{}})=(_{i=N^{{}^{ }}+1}^{N}_{V}_{i}_{K}_{i} )_{Q}\), suggesting that the implicit gradient for \(N\) is expected to be more sensitive than that for \(N^{{}^{}}\)._

### Applications of Our Theoretical Understanding

In this portion, we deal with **Q3**. We further explore the relationship between model performance improvement and SVD, as illustrated by a simple case study.

**Case 1**.: _Assuming that \(\) has two unit eigenvectors \(_{1},_{2}\), based on the properties of eigenvectors:_

\[}=_{1}},}= _{2}},\]

_and for a vector \(\) on the hyperplane, it can be decomposed into a linear combination of two eigenvectors:_

\[=l_{1}}+l_{2}}=(}{_{1}}}+}{_{2}}})= ,\]

_where \(l_{1},l_{2}\) are coefficients and \(_{1},_{2}\) are eigenvalues. Notice that if \(_{1}_{2}\), when point \(\) moves in the direction of \(}\), the value of \(l_{1}\) changes but the solution set \(\) changes insignificantly. Conversely, if it moves in the direction of \(_{2}\), the value of \(l_{2}\) changes and the solution set \(\) changes dramatically, this is an ill-conditioned7 problem whose solution is highly biased under small perturbations._

Given that SVD can be considered to play a role in noise reduction in ICL inference, and that this noise can cause significant disturbances to model output. We thus introduce the matrix ill-conditionedness, which can be measured by the Matrix condition number defined as follows.

**Matrix condition number8.**\(Cond()=||||_{p}||^{-1}||_{p}\). Specifically, the condition number \(Cond()=}{_{min}}\) when \(p=2\), where \(_{max},_{min}\) denote the maximum and minimum singular values respectively. Generally, for matrices of the same order, the higher the condition number of a matrix is, the greater its ill-conditionedness will be.

Figure 3: 2-norm condition number of GPTJ-6B&LLAMA2-7B. The condition numbers for MLP are significantly lower than those for ATTN. In deeper layers, condition numbers tend to be higher. These matrices are ill-conditioned for they satisfy \(_{max}_{min}\).

Then we analyze the matrix condition number of the models as shown in Figure 3. We observe that the condition number of deeper layers (especially the last layers) in the model is generally higher, indicating that the condition number may serve as a reference for adjusting the model.

Based on all our exciting insights, we find it intuitive to design ICL improvements based on them, especially in downstream tasks. We propose a method where we first select layers with the top-k largest condition numbers and then identify the layer with the largest number among these. We perform a greedy search for the optimal clipping rate \(^{*}\) on the validation set and subsequently evaluate it on the test set. This procedure is reported in **Algorithm 1** in Appendix C.2.

**Experimental setup.** We conduct experiments for **Algorithm 1** on widely adopted benchmark datasets, including SST-2 , AGNEWS , EMOC , MRPC , RTE , CB , COPA . Please refer to Appendix C.1 and C.3 for more detailed dataset and prompt settings. And for models, we use the same models as in Section 2.

**Experimental results.** As Figure 4 shows, the results indicate that **Algorithm 1** can visibly influence performance across different language understanding tasks in ICL inference. Specially, the effectiveness of **Algorithm 1** in enhancing performance varies not only by the model but also significantly by the task, indicating a potential task-specific and model-specific threshold for the benefits derived from algorithmic enhancements. More notably, **Algorithm 1** is a derivative-free optimization method and compresses the model to some extent, providing developers and researchers with an effective approach for handling downstream tasks. We also invite readers to refer to Appendices B.3, B.4 and B.5 for discussions on _What would be the effect of pruning only a single module?_, _Why optimal clipping rate \(\) varies?_ and _What would happen if we apply the same clipping rate to other datasets_?

## 4 Conclusion and Limitation

In this paper, we show our surprising findings in ICL inference: SVD-based weight pruning can enhance ICL performance both in shallow and deep layers across different module types, and pruning weights in deep layers often results in more stable performance improvements than in shallow layers. We conduct an in-depth theoretical analysis and explanation of these findings. Specifically, we first present the implicit gradient descent trajectories of ICL, afterwards, we give a mutual information based generalization bounds of ICL via full implicit GD trajectories. Based on all our experimental and theoretical insights, we intuitively propose a derivative-free and effective method for downstream tasks in enhancing ICL inference. However, further studies are required on (i) how to extend our generalization theory to a more standard Transformer architecture, (ii) do the results about ICL hold true for tasks beyond natural language processing and (iii) how to minimize the cost of searching the optimal clipping rate. Those will deepen our understanding of the ICL capabilities.

Figure 4: The Model Performance on Test set by different tasks. The results are obtained by comparing four scenarios: ICL (GPT-J-6B), ICL+Algorithm1 (GPT-J-6B), ICL (LLAMA2-7B) and ICL+Algorithm1 (LLAMA2-7B). ICL+Algorithm1 demonstrates superior results over only ICL on different tasks. See Appendix C.5 for detailed numbers.