# Online Adaptation of Language Models with a Memory of Amortized Contexts

Jihoon Tack\({}^{1}\), Jaehyung Kim\({}^{2}\), Eric Mitchell\({}^{3}\), Jinwoo Shin\({}^{1}\),

Yee Whye Teh\({}^{4}\), Jonathan Richard Schwarz\({}^{5}\)

\({}^{1}\)KAIST \({}^{2}\)Yonsei University \({}^{3}\)Stanford University

\({}^{4}\)University of Oxford \({}^{5}\)Harvard University & Thomson Reuters

jihoontack@kaist.ac.kr

###### Abstract

Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. To address the crucial need to keep models updated, online learning has emerged as a critical tool when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose a feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient manner, we utilize amortization-based meta-learning, which substitutes an otherwise required optimization process with a single forward pass of the encoder. Subsequently, we learn to choose from and aggregate selected documents into a single modulation by conditioning on the question, allowing us to adapt a frozen language model during test time without requiring further gradient updates. Our experiment demonstrates the superiority of MAC in multiple aspects, including online adaptation performance, time, and memory efficiency. In addition, we show how MAC can be combined with and improve the performance of popular alternatives such as retrieval augmented generations (RAGs). Code is available at: https://github.com/jihoontack/MAC.

## 1 Introduction

Language models (LMs)  have significantly accelerated progress in natural language processing (NLP) and thus become a core technology in various real-world applications, such as coding assistants , search engines , and personal AI assistants . However, LMs are typically static artifacts, and as the world changes, the knowledge encoded in their parameters becomes outdated. This becomes especially problematic for large language models (LLMs), as multiple applications (e.g., Chatbots ) require the model to be up-to-date, yet retraining LLMs with new documents from scratch requires high computational demands .

To tackle this issue, multiple studies suggested online and continual learning frameworks for LMs, i.e., adapting the LM on a stream of new documents. One line of work proposes to use retrieval-augmented models by saving the stream of documents and selecting the most relevant document based on the input . However, even large models often fail to update their learned knowledge when the retrieved document consists of counterfactual information  and it may not be suited foredge computing as a large number of documents poses expensive computation for model inference . Due to these limitations, another line of recent works suggests finetuning the model on a stream of documents to directly update the knowledge inside the LM (i.e., online finetuning ). While effective, online finetuning schemes also face limitations such as a large computation for gradient calculation, the sensitivity of the online optimization hyper-parameter , and the aforementioned catastrophic forgetting problem . In this paper, we instead ask: _Can we tackle the limitations of retrieval augmented models and online finetuning by assimilating and retaining knowledge from incoming documents without the need for gradient-based learning at test time?_

To this end, we suggest bridging this gap through a complementary learning systems approach  by introducing an end-to-end differentiable auxiliary retrieval augmentation system that can be run alongside a (frozen) target LM. This system extracts knowledge from incoming documents, builds a memory bank, and learns to automatically select relevant information from this memory bank, which is subsequently passed as additional input to the target model. Once learned, this system can be effectively employed purely through forward passes.

**Contribution.** We propose Memory of Amortized Contexts (MAC), an efficient and effective online learning framework for LMs (see the overview in Figure 1). The core idea of MAC is to freeze LM parameters (thus reducing undesirable side effects common for online finetuning) and instead incorporate new information through additional learned input tokens (an established Parameter-Efficient Fine-Tuning technique ), utilizing amortization-based meta-learning . Specifically, instead of optimizing individual PEFT tokens (which necessitates labels and gradient computations), we instead learn to directly predict these tokens based on a query and memory bank alone, without the need for labels at test time, thus proposing amortized optimization .

To ensure the scalability of MAC, we propose two memory-efficient techniques for training and inference: (1) We find that the process of training our complementary retrieval and aggregation operation for LLMs, necessitates a sufficiently large batch size, which introduces significant memory constraints. To address this issue, we backpropagate on only a random subset of documents, significantly saving memory while still providing an unbiased approximation of the full gradients . (2) Large memory banks can further increase GPU memory usage when aggregating information relevant to a query during inference. To address this, we propose a divide-and-conquer approach, sub-grouping the large set of modulations into smaller, manageable groups and repeating this procedure with the predicted modulations until the final modulation parameters are determined.

We verify the efficacy of MAC through evaluations on multiple datasets and architectures. Overall, our experimental results demonstrate the strong results of MAC. For instance, when measured with the F1 score (%), MAC improves performance from 18.97 \(\) 21.79 over prior work on StreamingQA , and 18.66 \(\) 21.14 on SQuAD-Seq . Furthermore, we demonstrate that MAC shows significant effectiveness in retaining learned knowledge when compared to other online finetuning baselines, justifying the memory-augmentation approach. In addition, MAC can be readily combined with retrieval augmented generation (RAG) and in effect, further increases the selection quality of retrieved documents, resulting in an improvement of 71.83 \(\) 74.89 over BM25 alone  on ArchivalQA-Seq. Finally, we highlight the efficiency of MAC in multiple aspects, measuring adaptation time, training, and inference memory usage, again demonstrating strong improvements over baselines.

Figure 1: An overview of MAC: we amortize each context document into PEFT modulation \(\) and learn to aggregate modulations into a single target modulation \(^{*}\) based on the given question input \(\) to adapt the frozen LM \(_{}\). During online adaptation, we store the amortized contexts into a memory bank \(\), then adapt the LM via aggregating the memory bank based on the given question.

Related Work

**Amortization-based meta-learning.** Amortization-based meta-learning, which encodes the given context to directly predict the task-specific model, has gained much attention due to its computational efficiency as it only requires a single encoder forward pass when adapting the model [69; 51; 19; 18]. These approaches, especially when combined with modulation techniques, have achieved notable success in various applications, such as few-shot visual recognition [65; 6; 11] and 3D reconstructions [20; 35]. Recently, this idea has been extended to language domains where prior works facilitate hypernetworks to adapt LMs with given few-shot prompts [58; 28]. In this paper, we extend the use of amortization-based meta-learning to extract the knowledge of a given document into a compact yet informative modulation for online adaptation.

**Online learning.** Online learning, also referred to as continual or lifelong learning, is a task of adapting models to new data or task distributions . Such ideas are becoming increasingly relevant in the era of deep learning generally and with the advent of extremely large models [78; 17; 71] specifically. In the language domain, there have been various attempts to tackle online learning [40; 92; 63] where recent studies focus more on online learning of LLMs, e.g., finetuning on a stream of documents , architectural constraints , and the use of replay buffers . Among them, Hu et al.  found that online finetuning can be effective when an LM focuses on important tokens during the adaptation and proposed a gradient-based meta-learning approach to automatically learn a token importance weighting model. However, such gradient-based meta-learning schemes require a compute-expensive second-order gradient calculation [15; 64]. Moreover, online finetuning schemes can face multiple challenges, including (i) inevitable forgetting of the learned knowledge, (ii) gradient computation of LLMs during adaptation, and (iii) high sensitivity to the online optimization hyperparameter (e.g., learning rate ). MAC does not suffer from such issues as our amortization strategy is efficient without introducing any hyperparameters while effectively preserving knowledge.

**Retrieval augmentation for LMs.** Retrieval augmentation of LMs with relevant information from external knowledge sources has served as an effective way to improve the performance of LMs on various NLP tasks [21; 43; 30; 70; 80] by reducing hallucination and leveraging external knowledge which is not seen during pre-training. However, retrieval augmentation drastically increases computational cost  as documents often consist of thousands of words. In addition, its effectiveness is sensitive to the configuration of retrieved information , and even negatively affects the performance of LMs when the retrieved information is counterfactual . MAC is more efficient than retrieval augmentation as it amortizes the external knowledge to modulate LMs rather than directly incorporating it. Furthermore, we believe MAC and retrieval augmentation has similarities as both methods store the knowledge and utilize them base on the user query, while the main difference is that MAC attend to multiple documents simultaneously using the aggregation network, allowing the LLM to capture shared information across documents. We thus believe that the joint usage benefits retrieval augmentation, as MAC can guide retrieval augmentation to capture missing information not retrieved by the retriever (see Section 4.1 for the supporting experiment).

**Memory augmented LMs.** Recently, memory augmentation has also shown great promise for LMs where it significantly improves the performance and efficiency in various directions [84; 56; 94; 54; 24], e.g., extending context length with memory retrieval [87; 83], personalization , and model editing . Unlike these methods, which store the raw text or use the memory bank to train new LMs, MAC stores compact modulation parameters (in the shape of learned tokens) and adapts the frozen target LM, thereby utilizing large models without the heavy computation of training LMs.

## 3 MAC: Online Adaptation with a Memory of Amortized Contexts

In this section, we first briefly describe our problem setup (Section 3.1), then core components, namely amortization and aggregation framework (Section 3.2) and finally, efficient training and inference schemes for MAC (Section 3.3). Algorithm 1 and 2 in Appendix B provide detailed training and online adaptation processes for our framework.

### Problem setup: Online adaptation

We consider the online adaptation scenario proposed in Hu et al.  where a static LM parameterized by \(_{}\) is adapted to an online stream of documents \(^{}(_{1},,_{K^{ }})\). After incorporatingthe final document, we then evaluate the adapted model's performance with a set of queries \(\{_{i}\}\) and a corresponding labels \(\{_{i}\}\), where the \(i^{}\) query and label are drawn from a conditional distribution of a document \(_{i}\), i.e., \((_{i},_{i}) p(,|_{i})\). Here, note that the query \(_{i}\) is not accessible during online adaptation; hence, retaining the learned information from \(_{i}\) is critical for achieving good results. While the query input and label pair \((,)\) can be in any format or task, we mainly focus on question and answering (QA) tasks by following Hu et al. , i.e., \(_{i}\) is a question and \(_{i}\) is the corresponding answer based on the given information in \(_{i}\), as it is straightforward to evaluate the LM's updated knowledge. Nevertheless, we also consider an additional non-QA setup in Section 4.3.

### MAC: Memory of amortized contexts

The stated goal of MAC is (i) the efficient adaptation of a given LM to unseen information (ii) while retaining previously learned knowledge, both from its original training stage as well as updates from prior examples in a stream of novel data. To this end, we propose to utilize amortization-based meta-learning [18; 19] of a memory-augmented system. Amortization-based meta-learning with _modulations_[27; 65; 4] learns to predict a task-specific modulation (i.e., a compact representation of a task) through amortizing the given context set sampled from the task distribution. This enables efficient adaptation using the learned amortization network, as it only requires a single forward pass to adapt a model, foregoing the cost of gradient computation. It is worth noting that this is also beneficial as the LM does not have access to the input and label pair \((,)\) during the online adaptation, where we can design the amortization to find the modulation only with the given document \(\). Furthermore, meta-learned modulations have been found to preserve the task information well (e.g., showing great potential for generating or classifying distributions of tasks [72; 73]). They can hence be expected to effectively extract document information. Based on this insight, we suggest meta-learning the amortization network to directly predict a compact modulation for a new document.

**Learning to amortize contexts.** For a given context document \(_{k}\) sampled from the training document set \(^{}\), we learn an amortization network parameterized by \(_{t}\) to predict a modulation parameter (of the same shape as embedded tokens) \(_{k}\) as: \(_{k} g_{_{t}}(_{k})\). Here, we use a hyper-network  for \(_{t}\): we modify the T5 architecture  by having learnable tokens as the input of the decoder to have a consistent number of output tokens by following . One can design the modulation with any type of PEFT scheme (e.g., LoRA  or FiLM ), among which we use P-Tuning v2  (i.e., predictions of the key-value of each attention layer).

**Modulating LMs via aggregating amortized contexts.** Given a memory bank of compressed documents in the form of modulations \(\{_{k}\}_{k=1}^{K}\), we now learn to choose relevant information in the form of a modulation \(_{i}^{*}\) for a given input \(_{i}\). While one design choice is to select/retrieve a single modulation, this has two drawbacks: (i) risk of selecting the wrong modulation and (ii) limited utilization of learned knowledge across different modulations. Moreover, it is worth noting that recent studies empirically show that linear interpolation (or advanced merging) between the modulations trained from the same pre-trained LM can even perform better than individual modulation (coined "model soup" [86; 93]). In this regard, we thus _aggregate_ the memory bank into a single modulation based on the given input. Formally, we learn a set aggregation network \(h_{}\) that satisfies _permutation invariance_ (i.e., invariance to the order of modulations in the memory bank) by utilizing cross-attention blocks [81; 36; 89] to select \(_{i}^{*}\):

\[_{i}^{*} h_{}g_{_{}}(_{i }),\{_{k}\}_{k=1}^{K},\] (1)

where \(_{}\) is the input encoder, and we use the same architectural design as the amortization network \(_{t}\), albeit resorting to a reduced number of parameters for efficiency reasons. Note that \(\{_{k}\}_{k=1}^{K}\) is often referred to as as a context set in the meta-learning literature, hence inspiring the name of our method. We provide more architecture design details of \(_{t}\) and \(\) in Appendix A.

**End-to-end training objective.** To learn aggregation and amortization networks, we optimize both networks in an end-to-end fashion as follows:

\[_{_{},_{t},}_{i=1 }^{N}_{_{}}(_{i};_ {i}^{*}),_{i}.\] (2)

where \(\) is the loss function, i.e., negative log-likelihood of the given label \(\), and \(N\) is the batch size of training query inputs and labels. Here, it is important to state that we make no updates to the static LM \(_{}\), which would carry the risk of catastrophic forgetting by overwriting important parameters.

**Online adaptation stage.** After training amortization and aggregation networks based on a given training set, we now consider the online adaptation scenario. Here, we consider a stream of \(K^{}\) documents \(^{}},,}}^{}}\) given to the LM in a sequential manner, where the task input \(}}\) is not accessible during adaptation. To this end, we propose to store the compact modulations into a memory bank \(\{g_{_{}}(^{}})\}_{k=1}^{K^{}}\) and later predict the modulation using the aggregation network to adapt the LM, i.e., \(_{_{}}(}};^{*})\) where \(^{*} h_{}g_{_{}}(}}),\).

### Memory efficient training and inference for MAC

Due to aforementioned challenges, the training of MAC can quickly become prohibitive. The following sections cover techniques to drastically reduce memory requirements.

**Backpropagation dropout.** During the online adaptation stage, the aggregation network is required to predict the modulation based on the memory bank, which may consist of large numbers of modulations (examples extracted from thousands of novel documents in our experimental setup). To handle large batch inference, it is crucial to present similar examples during training to avoid distribution shift between training and online adaptation stage and ensure that memory selection is robust. To this end, we propose a memory-efficient way to increase the training context size \(K\) by computing gradients using only a subset of randomly chosen examples (ensuring unbiased gradient computation), thus allowing training with significantly larger memory sizes. More concretely, with probability \(p\), we perform amortization at training time with a stop-gradient operation, i.e., \(g_{_{}}(_{i})\) where \(p\) is a hyper-parameter, thus reminiscent of dropout. It is important to note that this random sub-sampling yields _unbiased approximation of the full gradient_ under amortization-based meta-learning schemes , hence, does not hurt the overall performance.

**Hierarchical modulation aggregation.** In addition, we propose an efficient inference technique to deal with the accumulated memory bank. Let \(T\) be the number of output tokens for each context and \(K\) the number of amortized contexts, respectively. Then, the memory usage made by a single cross-attention layer becomes \((KT^{2})\) (note that the input \(\) is also mapped into \(T\) tokens). This indicates the aggregation process requires a memory cost that linearly scales with the size of the memory bank.

To alleviate memory consumption, we propose hierarchical modulation aggregation that uses a divide-and-conquer strategy (see Algorithm 3). Specifically, for a given memory bank size of \(K\) with \(T\) tokens, we subgroup the total \(KT\) tokens into \(M\) tokens each, thereby having \(\) groups (\(\) is the ceil function, i.e., the smallest integer which is greater than or equal to the given input). Then, we aggregate the modulations of individual subgroups into a single output to obtain \(\) modulations. We repeat this procedure until it outputs a single modulation. Assuming no parallelization, one can compute this process by only utilizing the memory complexity of \((MT)\) where \(M\) is a hyperparameter (more details of the complexity calculation is in Appendix A.2).

## 4 Experiments

In this section, we provide an empirical evaluation of MAC, systematically verifying claims made throughout the manuscript and thus supporting the suitability of its constituent components. Specifically, we investigate the following questions:

* How does MAC perform compare to other online learning techniques for LMs? (Table 1 & Table 2)
* Is MAC more efficient compared to online finetuning schemes? (Figure 2)
* Does MAC show effective knowledge retention compared to other finetuning methods? (Figure 3)
* Does proposed efficient training and inference schemes save memory usage? (Figure 4 & Figure 5)

Before answering each question, we outline the experimental protocol (more details in Appendix A).

**Datasets.** For the experiment, we utilize three question-and-answering (QA) datasets including StreamingQA , SQuAD , and ArchivalQA , by following the prior work . Here, unlike the original use of SQuAD and ArchivalQA (i.e., used for evaluating static LMs), we use these datasets for online adaptation (i.e., adapting on a stream of documents), hence, denote with an additional "-Seq" notation throughout the section.

**Online adaptation setup.** After training MAC (i.e., learning \(_{}\), \(_{}\), and \(\) parameters) on a training dataset that consists of document and QA pairs, we evaluate the online adaptation performance on the stream of documents. Here, we use 1,665 documents to adapt the LM and then perform the evaluation after the adaptation, where QA pairs are sampled from the learned documents. Each document can consist of tokens up to 512 when using the Byte Pair Encoding .

**Baselines.** We mainly consider the online finetuning baselines introduced in , including _Uniform_, _Salient Spans_ and _CaMeLS_. Here, all baselines are first pre-trained on a QA-paired training set (without the documentation) and then utilize auto-regressive finetuning to adapt to the stream of documents. Specifically, Uniform uses uniform token weighting, Salient Spans assigns uniform weight to tokens in salient spans  and no weights to other tokens, and CaMeLS utilizes the output of the token weighting LM (which is meta-learned to predict the important token so that the performance of the adapted LM is maximized). Furthermore, we also consider the joint usage of MAC with the retrieval augmentation scheme, including BM25 , Contriever , and DPR .

### Online adaptation with MAC

We first present the main result by comparing the online adaptation performance with other baselines. Here, we mainly compare with online finetuning schemes and additionally show that MAC can be jointly used with a retrieval augmentation method to further improve the performance.

**Comparison with online finetuning methods.** In Table 1, we show the online adaptation performance of MAC and the online finetuning baselines. Overall, MAC significantly outperforms all the prior online finetuning methods by a large margin, leading to a better exact match (EM) and F1 score. We also found that CaMeLS  suffers from the memory shortage on LLaMA-2 even when using the memory efficient techniques (e.g., 4bit quantization  and ZeRO ), as it requires second-order gradient computation for meta-learning. Consequently, it requires a proxy model (a small-sized LM compared to the base LM) that uses the same tokenization (e.g., we use DistilGPT2 for GPT family as suggested in ).

Furthermore, it is worth mentioning that MAC is significantly efficient in both memory and adaptation time compared to other online finetuning methods; we remark that MAC does not require any gradient computation to update the model, while online finetuning needs the gradient to update the model. For

    & &  &  &  \\  Model (\# params) & Method & EM (\(\)) & F1 (\(\)) & EM (\(\)) & F1 (\(\)) & EM (\(\)) & F1 (\(\)) \\   & Uniform & 1.62 & 3.76 & 1.24 & 2.54 & 4.86 & 4.08 \\  & Salient Spans & 1.44 & 4.67 & 1.03 & 2.47 & 4.52 & 3.76 \\  (82M) & CaMeLS & 1.62 & 5.79 & 1.47 & 3.08 & 4.62 & 6.19 \\  & **MAC (ours)** & **5.59** & **10.18** & **2.01** & **6.85** & **7.55** & **10.58** \\   GPT2-Large (774M) \\  } & Uniform & 4.74 & 7.00 & 3.64 & 4.97 & 7.66 & 8.71 \\  & Salient Spans & 4.86 & 8.54 & 4.03 & 6.48 & 9.75 & 11.19 \\  & CaMeLS\({}^{*}\) & 5.35 & 10.60 & 4.97 & 8.63 & 9.92 & 12.41 \\  & **MAC (ours)** & **7.25** & **13.31** & **6.43** & **11.42** & **11.84** & **15.26** \\   GPT2-XL (1.5B) \\  } & Uniform & 5.11 & 7.48 & 6.10 & 6.78 & 8.61 & 10.78 \\  & Salient Spans & 5.40 & 9.42 & 4.55 & 6.74 & 11.81 & 14.11 \\  & CaMeLS\({}^{*}\) & 6.55 & 11.67 & 6.70 & 10.15 & 13.87 & 15.74 \\  & **MAC (ours)** & **8.99** & **15.38** & **7.10** & **12.55** & **14.01** & **17.12** \\   LLaMA-2 (7B) \\  } & Uniform & 12.43 & 13.54 & 13.25 & 17.01 & 18.53 & 21.35 \\  & Salient Spans & 13.33 & 18.97 & 13.74 & 18.66 & 18.97 & 22.75 \\   & CaMeLS &  &  &  &  \\   & **MAC (ours)** & **14.29** & **21.79** & **15.07** & **21.14** & **20.12** & **23.90** \\   

Table 1: Comparison of the online adaptation performance between MAC and online finetuning baselines. We report the exact match (EM) and F1 score by adapting the LM on a stream of documents and then performing QA based on the learned data. \({}^{*}\) denotes the adaptation results of CaMeLS using a proxy token weighting LM (i.e., a smaller LM than the base LM) due to memory consumption, and OOM denotes unavailable results due to the running out-of-memory on a single NVIDIA A100 80GB GPU (even with a batch size of 1). The bold indicates the best result within the group.

instance, compared to CaMeLS, MAC reduces 68.0% memory usage for a single document adaptation and can adapt 128 times larger number of documents when using the same memory. Moreover, the adaptation time reduces from 28.58 to 2.5 minutes under the same memory usage (i.e., 90.31% drop). We emphasize that both types of efficiency are crucial for online learning LMs as i) the document corpus is expanding rapidly, and ii) it enables the user to use a larger model for better generalization.

**Knowledge Retention of MAC.** We now address one of our primary motivations for this study: a comparison of knowledge retention by analyzing the catastrophic forgetting of each method. To this end, we evaluate the F1 score retention ratio, which is determined by the decline in the F1 score of the initially adapted 200 documents during the optimization on a subsequent stream of documents. As shown in Figure 3, MAC shows a strong knowledge retention compared to other online finentuning methods: when adapting additional 1,400 documents, MAC retains the initial performance by 96.2% while CaMeLS retains 70.8%. These results indeed highlight i) the benefit of using a memory bank as a tool for preserving knowledge and ii) our aggregation mechanism well predicts the modulation even when the memory bank's cardinality increases throughout the adaptation process. It is also worth noting that online finentuning schemes somewhat suffer from preserving the newly learned knowledge, especially when the number of adapted documents increases, thus may limit the practical usage for real-world applications.

**Improving MAC with retrieval augmentation.** In addition, we show that MAC can be further improved by using retrieval augmentations. Here, we note that the user requires more inference costs to use retrieval augmentations as prepending the retrieved document in front of the question quadratically increases the inference computation based on the document length due to the Attention mechanism . For the experimental setup, we compare it with LMs that are pre-trained on QA training set with an appended top-1, top-3, and top-5 retrieved document for each question, i.e.,

   &  &  &  \\   & EM & F1 & EM & F1 & EM & F1 \\  BM25 & 48.53 & 54.17 & 56.18 & 63.74 & 64.74 & 71.83 \\
**BM25 + MAC (ours)** & **52.81** & **56.55** & **60.22** & **66.82** & **68.85** & **74.89** \\  Contirever & 44.78 & 51.55 & 52.56 & 61.28 & 60.10 & 67.83 \\
**Contirever + MAC (ours)** & **47.99** & **53.23** & **53.92** & **63.75** & **61.28** & **70.01** \\  DPR & 48.98 & 55.01 & 57.02 & 64.27 & 65.07 & 72.24 \\
**DPR + MAC (ours)** & **49.57** & **55.98** & **60.19** & **67.05** & **68.52** & **75.00** \\  

Table 2: Online adaptation performance of MAC jointly using the retrieval augmentation under ArchivalQA-Seq dataset. We consider BM25, Contirever, and DPR as retrieval augmentation methods. We report the exact match (EM) and F1 score by adapting the LLaMA2-7B on a stream of documents and then performing QA based on the learned data while retrieval augmentation retrieves documents. The bold indicates the best results within the group.

Figure 3: Catastrophic forgetting analysis under GPT2-XL trained on StreamingQA dataset. We report the F1 score retention rate (%) through measurement of relative F1 score decline in the initially adapted 200 documents during subsequent adaptation to a new stream of documents (up to additional 1,400 documents).

Figure 2: Comparison of the adaptation memory and time efficiency between MAC and online finentuning baselines. We report the peak GPU memory allocation (GB) for adapting one document and the time (min) for adapting a stream of 1,665 documents under the same memory usage. We use GPT2-XL on StreamingQA.

LM\({}_{_{}}(;)\) where \(\) and \(\) indicate concatenation and the modulation, respectively. Here, we consider three types of popular retrieval augmentation methods, including BM25 , Contriever , and DPR . As shown in Table 2, using BM25 with MAC significantly improves the performance by a large margin in all cases, e.g., F1 score of 71.83% \(\) 74.89% for LLaMA-2 (7B) when using top-5 documents. We conjecture that the aggregation process of MAC enables the utilization of the shared information across the documents, thus improving the performance over the single document retrieval. We believe further extending MAC for the joint usage with retrieval augmentation schemes will be an interesting future direction to explore where one can extend the amortization and input network to enhance the aggregation of modulations but also learn to well retrieve documents.

### Efficiency of backpropagation dropout and hierarchical modulation aggregation

We verify the proposed memory efficient techniques, namely the backpropagation dropout and the hierarchical modulation aggregation for training and inference, respectively. Here, we report the peak GPU utilization when using the proposed techniques to show the memory efficiency. Furthermore, we re-emphasize that such techniques are important for (i) scaling LMs to larger models and (ii) handling a large number of documents during online adaptation, which are both necessary for scaling.

**Training memory efficiency.** To show the memory efficiency of the backpropagation dropout, we increase the number of amortized contexts \(K^{ train}\) during training time and vary the dropout ratio \(p\). As shown in Figure 4, increasing the dropout ratio can significantly handle more contexts under the same memory constraint. As a result, we found that simply using \(p=0.75\) is an effective choice when using large models (# parameters \(>\) 1B) as the training context size is small in such cases. For instance, when training LLaMA-2 (7B) model on StreamingQA dataset without this technique, one can only compute the loss with a single document (under 32 GB GPU), thus the aggregation network cannot learn the similarity between the modulations. As a result, using backpropagation dropout improves the performance of LLMs (in Table 3).

**Inference memory efficiency.** Here, we show that the hierarchical modulation aggregation can significantly reduce memory usage while effectively preserving the performance for the inference. To this end, we vary the cardinality of the subgroup \(M\) and report the peak GPU memory usage and F1 score where we only measure the used memory by the modulation aggregation (i.e., excluding the LM cost). As shown in Figure 5, using the subgroup size of \(M=16\) can reduce the memory by 65.6% while still preserving 93.2% of the original accuracy. We remark that this technique can be applied even without additional training trick or regularization, demonstrating similar observations from the prior works that uses hierarchical aggregation (or merging) in the context of Transformers [5; 76], yet MAC is the first to aggregate the modulations.

   Method & \(K\) & Memory (GB) & F1 \\  No backprop. & 1 & 33.86 & 12.43 \\ MAC & 4 & 34.01 & 21.79 \\   

Table 3: Effect of backpropagation dropout (backprop.) on LLaMA-27B under StreamingQA dataset. \(K\) indicates the batch size.

### Additional analysis

In this section, we provide more analysis of MAC. Here, we mainly consider baselines that show effectiveness in the main experiment (e.g., CaMeLS in Table 1) and consider GPT2 family trained with StreamingQA dataset.

**Cross-attention analysis.** We analyze whether the learned cross-attention is attending to the correct information. To this end, we visualize the final cross-attention layer of the aggregation network trained on StreamingQA with GPT2-Large, where we provide the gold document (containing the answer to the question) and an additional five documents. Here, we consider providing the retrieved documents using BM25 or random documents, where we average the cross-attention over 25 questions (as considering more number of questions over-smooth the visualization). As shown in Figure 6, the model selectively attends to the gold document when provided with irrelevant random documents, effectively ignoring them, while appropriately attending to relevant documents retrieved using BM25, indicating a well-trained attention mechanism capable of discerning useful information.

**Memory bank size constraint.** One possible concern of MAC is the growing size of the memory bank as the number of adapted documents increases. To this end, we have conducted an additional experiment using a fixed memory bank size for MAC. Specifically, we reduce the number of amortized contexts when it reaches the memory constraint of 1,250 (where the total number of contexts is 1665). Here, we consider three simple yet effective schemes: i) random pruning, ii) randomly averaging two modulations \(_{}=(_{1}+_{2})\), and iii) averaging two nearest-neighbor (NN) modulations based on the cosine distance. As shown in Figure 7, we tested LLaMA-2 7B on StreamingQA by reducing the memory bank size where averaging NN modulations shows quite effective preservation. We believe it would be an interesting future direction to further explore MAC under memory bank size constraints where a great variety of techniques can be developed in this direction, for instance, using neural compression techniques to reduce the memory bank size .

**Using other types of PEFT.** Here, we show that other types of PEFT modulation can also be used for our framework. To this end, we considered LoRA  as an alternative to P-tuning v2 . As shown in Table 4, LoRA also performs well compared to other online fine-tuning methods, but overall, P-tuning v2 outperformed LoRA when training GPT2-XL on the StreamingQA dataset. This result aligns with the finding from previous work , where they also observed that P-tuning v2 outperforms LoRA when using amortization. Additionally, we believe P-tuning is also easy to implement, as it allows efficient batch computation, enabling a single forward pass of the LLM with different modulations. In contrast, LoRA requires separate forward passes for each modulation, which increases the training time.

   PEFT type & EM & F1 \\  LoRA & 8.67 & 15.15 \\ P-tuning v2 & **8.99** & **15.38** \\   

Table 4: Online adaptation performance on different types of PEFT, including LoRA and P-tuning-v2. We train GPT2-XL on StreamingQA.

Figure 6: Visualization of the per-token final layer cross-attention. The aggregation network is provided with the gold document (containing the answer) with five additional documents, which are either (a) retrieved using BM25 or (b) randomly sampled. Each question and document are encoded into \(K=12\) tokens, where \(K\) is a hyperparameter. Red denotes the high similarity with the question.

Figure 7: Comparison of various memory bank reduction methods on LLaMA2-7B.

**Adaptation on out-of-distribution (OOD) datasets.** We additionally analyze the online adaptation performance of MAC on the OOD dataset from the training distribution. To this end, we compare the performance with CaMeLS  on GPT2-XL, as other online finetuning methods do not involve a training stage (i.e., no training distribution). Here, we use StreamingQA as a training set (i.e., a relatively large dataset) and other datasets as OOD. As shown in Table 5, MAC outperforms CaMeLS in F1 score. It is worth noting that the meta-learning performance scales as , hence, we believe training MAC on larger datasets will further improve the OOD generalization.

**Language modeling with MAC.** While the conventional evaluation protocol for online learning LMs uses QA [32; 31; 26], we additionally conducted a language modeling task (i.e., predicting the next token). Specifically, we adapted the LLM on a stream of documents, then gave the initial 10% of the document as input to the input network (this is equivalent to a question in the QA task). Here, we measured the perplexity of the remaining 90% of the documents on two cases: (i) the documents used for LLM adaptation to measure knowledge preservation and (ii) unseen documents to measure generalization. As shown in Table 6, MAC outperforms other online finetuning baselines in both cases.

**Design choice for the amortization network.** Here, we consider different types of design choice for the amortization network. To this end, we evaluated three architectural configurations: decoder-only, encoder-only, and encoder-decoder language models. Specifically, we experimented with (i) the GPT2 model and (ii) the T5 encoder with learnable tokens, where input context is compacted into these tokens. As shown in Table 7, the encoder-decoder model demonstrated superior performance over other configurations, using GPT2-XL as the base LLM on the StreamingQA dataset.

## 5 Discussion and Conclusion

We propose MAC, an efficient and effective online adaptation framework for static LMs with strong knowledge retention. MAC compresses the context document into parameter-efficient finetuning modulations, predicted by a meta-learned amortization network. These contexts are stored in a memory bank for strong knowledge retention and aggregated into a single output when a question is input. MAC excels in performance, adaptation time, and memory efficiency, and shows superior knowledge retention for newly learned documents when handling a stream of documents.

**Future works and limitations.** We believe it will be an interesting future work extending MAC to multiple applications that require online learning in an efficient manner, e.g., federated learning for LMs  and model editing [52; 53; 23]. Moreover, one possible limitation of MAC is the increasing size of the memory bank during online adaptation. In this paper, we found that the memory bank can be effectively reduced by averaging nearest neighbor modulation (in Section 4.3), where we believe further investigating a better-merging technique will be an interesting future direction to explore.

**Societal impact.** This paper presents a method that enhances the online adaptation performance of LMs through the use of amortization-based meta-learning and the memory bank. Similar to other works, using memory banks for LMs in real-world applications comes with benefits and pitfalls (e.g., privacy concerns when saving documents from users), requiring the responsible use of the technology. We believe further extending the amortization network in the perspective of privacy will be an interesting future direction to explore. For instance, rather than saving the raw text as other retrieval augmentations techniques or memory-augmented LMs, one can learn to amortize the context documents to prevent the document's privacy leakage.

    & Adapted & Unseen \\  Uniform & 11.43 & 13.89 \\ Salient Spans & 27.87 & 29.69 \\ CaMeLS & 11.31 & 14.77 \\
**MAC (ours)** & **10.91** & **12.71** \\   

Table 6: Perplexity on adapted and unseen documents. We use GPT2-Large auto-regressively trained on StreamingQA documents.

    & EM & F1 \\  Encoder only (T5-encoder) & 8.53 & 15.01 \\ Decoder only (GPT2) & 8.01 & 14.87 \\ Encoder-Decoder (T5) & **8.99** & **15.38** \\   

Table 7: Online adaptation performance across design choices for the amortization network, evaluated by training GPT2-XL on the StreamingQA dataset.

   StreamQA \(\) & SQuAD & ArchivalQA \\  CaMeLS & 8.63 & 13.43 \\
**MAC (ours)** & **10.47** & **13.73** \\   

Table 5: Online adaptation performance on OOD datasets: We report the F1 score of GPT2-XL trained on StreamingQA, adapting to SQuAD and ArchivalQA.