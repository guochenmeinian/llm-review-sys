# Animal-Bench: Benchmarking Multimodal Video Models for Animal-centric Video Understanding

Yinuo Jing1

School of Artificial Intelligence, Beijing University of Posts and Telecommunications

Ruxu Zhang1

School of Artificial Intelligence, Beijing University of Posts and Telecommunications

Kongming Liang1

School of Artificial Intelligence, Beijing University of Posts and Telecommunications

Yongxiang Li2

School of Artificial Intelligence, Beijing University of Posts and Telecommunications

Zhongjiang He2

China Telecom Artificial Intelligence Technology Co. Ltd

Zhanyu Ma1

China Telecom Artificial Intelligence Technology Co. Ltd

Jun Guo1

China Telecom Artificial Intelligence Technology Co. Ltd

###### Abstract

With the emergence of large pre-trained multimodal video models, multiple benchmarks have been proposed to evaluate model capabilities. However, most of the benchmarks are human-centric, with evaluation data and tasks centered around human applications. Animals are an integral part of the natural world, and animal-centric video understanding is crucial for animal welfare and conservation efforts. Yet, existing benchmarks overlook evaluations focused on animals, limiting the application of the models. To address this limitation, our work established an animal-centric benchmark, namely Animal-Bench, to allow for a comprehensive evaluation of model capabilities in real-world contexts, overcoming agent-bias in previous benchmarks. Animal-Bench includes 13 tasks encompassing both common tasks shared with humans and special tasks relevant to animal conservation, spanning 7 major animal categories and 819 species, comprising a total of 41,839 data entries. To generate this benchmark, we defined a task system centered on animals and proposed an automated pipeline for animal-centric data processing. To further validate the robustness of models against real-world challenges, we utilized a video editing approach to simulate realistic scenarios like weather changes and shooting parameters due to animal movements. We evaluated 8 current multimodal video models on our benchmark and found considerable room for improvement. We hope our work provides insights for the community and opens up new avenues for research in multimodal video models. Our data and code will be released at https://github.com/PRIS-CV/Animal-Bench.

## 1 Introduction

With the rapid advancement of artificial intelligence technology, multimodal video large models [1; 2; 3; 4; 5; 6; 7; 8] have bridged the gap between video and language modalities. Leveraging extensive knowledge and powerful comprehension capabilities, these models are being applied across various areas, ushering in a new era of intelligence. The emergence of a new era of intelligence is also accompanied by the development of new evaluation benchmarks. In contrast to conventional single-task benchmarks, the new benchmarks incorporate a variety of tasks and seek to evaluate the model's intelligence across multiple dimensions. Current evaluation studies [3; 9; 10; 11; 12; 13] primarily arise from human daily needs, with a focus on human-centric application tasks, aiming to assessmodel performance in common tasks encountered in human life. However, this approach confines their applicative scope to tasks centered around humans. In the broader real-world context, animals represent indispensable constituents of ecosystems . They participate in vital processes such as pollination, seed dispersal, and nutrient cycling, which are essential for environmental conservation and the maintenance of biodiversity [15; 16; 17]. However, evaluations that focus on animal-centric tasks are entirely divergent from the current frameworks. As illustrated in Table 1, existing benchmarks that comprehensively evaluate model capabilities predominantly emphasize humans or objects, whereas animal-centric evaluation datasets can only assess the model's performance in few aspects. As shown in Figure 1, taking the popular benchmark MVBench  as an example, the main agents in the videos are humans and objects, with animal data accounting for only 1%. The inherent agent bias in these comprehensive benchmark datasets hampers our understanding of large models' ability to comprehend animal agents.

Research focusing on animal-centric evaluation overcomes the agent bias present in previous benchmarks, allowing us to assess model capabilities in a broader real-world framework, further exploring the potential applications of models and providing more valuable guidance for model optimization. The inherent diversity among animal species and the complexity of their habitats result in rich variability within animal videos, making animal-centric tasks highly challenging . Evaluations on these demanding tasks can reveal the weaknesses of models in complex environments and analyze the robustness of multimodal video models against significant intra-class variations. Moreover, the applications of artificial intelligence in the field of animal studies are extensive [19; 20; 21; 22; 23; 24]. For instance, automated species counting [25; 26; 27] aids in tracking population dynamics in natural reserves, assessing overall ecosystem health, and significantly reducing human effort. Automated detection of animal stress and pain [28; 29; 30; 31; 32; 33] enables timely identification of potential issues, facilitating early treatment and ensuring animal welfare. Therefore, evaluations of models focused on animals are beneficial for further advancing the practical applications of artificial intelligence in the animal world. In summary, conducting animal-centric model evaluation plays a pivotal role in both model development and conservation efforts.

In this work, we propose Animal-Bench, a benchmark for evaluating multimodal video models in animal-centric video understanding. We choose tasks from two broad aspects: common tasks shared with human-centric benchmarks, covering aspects such as "object", "action", "time", "count", and "reasoning", and special tasks relevant to animal conservation. In total, we include 13 tasks spanning 7 major animal categories and 819 species, comprising a total of 41,839 data entries. To construct Animal-Bench, we devise a pipeline for automated data filtering and question-answer pair generation, reducing human effort and mitigating potential biases from human intervention. Furthermore, since our data primarily originates from the web, which typically features favorable

Figure 1: Previous benchmarks (left) relied on limited agent and the scenarios of editing-based benchmarks are unrealistic. Our proposed Animal-Bench (right) includes diverse animal agents, various realistic scenarios, and encompasses 13 different tasks.

recording conditions, while real-world filming scenarios may involve harsh weather conditions such as snowy or frosty, or changes in shooting parameters, such as variations in camera distance and direction due to animal movements. To accommodate potential variations in model applications and simulate real-world filming conditions, we employ video editing methods for simulation. As illustrated in Figure 1, previous editing-based evaluation benchmark  generated images that did not correspond to real situations and were not applicable to our setting. We utilize a video editing approach based on the diffusion model  to simulate videos captured in realistic scenarios, thereby evaluating the robustness of multimodal video models. We evaluate 8 existing multimodal video models on our benchmark, identifying significant room for improvement. We hope our work can inspire advancements in multimodal video models development.

## 2 Related Work

MLLM BenchmarksTraditional evaluation benchmarks [39; 40; 41; 42; 43] typically only test the ability of models by a single task. With the rise of multi-modal large language models (MLLMs), new benchmarks [44; 13; 9] aimed to cover a wider range of evaluation aspects. For instance, Video-Bench  categorized the video language models' comprehension abilities into three levels: video-exclusive understanding, prior knowledge-based question-answering, and comprehension and decision-making. The Perception test  focused on skills (memory, abstraction, physics, semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities. These benchmarks all evaluated whether the models' capabilities were comprehensive enough by carefully choosing and dividing tasks, while overlooking the importance of data selection for evaluation. Other works considered how data selection influenced the evaluation process. For Image-LLM evaluation, MMBench  hierarchically subdivided the MLLM models' perception and cognition abilities and redesigned the QA pairs to better reflect capabilities of the models. For Video-LLM evaluation, MVBench  considered 20 challenging video tasks in its evaluations, selecting 200 test instances from open datasets for each task and redesigning the QAs. However, most existing benchmarks used human-centric data, predominantly featuring humans as the main agents and neglecting others. Our Animal-Bench believes that MLLMs should demonstrate good generalization abilities across different agents, thus designing a new benchmark and assessing the performance of MLLMs on animal-centric data.

Editing-based BenchmarksIn recent years, some works have employed editing techniques to process benchmarks, enhancing data diversity and evaluating model robustness. Regarding benchmarks based on image editing, LANCE  utilized image editing techniques to augment the test set with a suite of diverse, challenging, yet counterfactual examples for diagnosing the image recognition abilities of different models. D. Hendrycks's work  established a rigorous benchmark for testing image classifier robustness by introducing diverse types of corruptions and perturbations, including

    &  &  \\   & Label & QA Size & Agent(min) & \\   &  &  &  & object, action, attribute, position, count, time, \\  & & & & reasoning, summarization, etc. \\   &  &  &  & action, object, attribute, position, count, time, \\  & & & & reasoning, etc. \\   &  &  &  & action, object, position, count, scene, pose, attribute, \\  & & & & character and cognition \\  Animal Kingdom  & Classification & N/A & Animal & object, action, time \\   &  &  & Animal & object, action \\  & & & & object, action \\   &  &  &  & Common tasks: object, action, time, count, reasoning \\  & & & & Special tasks: predator-prey behavior monitoring, social \\  &  &  &  & interaction analysis, breeding behavior monitoring and stress and \\  & & & & pain detection \\   

Table 1: Comparison of existing video understanding benchmarks. In contrast to other benchmarks, Animal-Bench mitigates the limitations of prior video question-answering benchmarks that lack animal agents. The dataset is characterized by its richness and diversity, facilitating a comprehensive evaluation of models across multiple dimensions of performance.

noise, blur, weather effects, and digital distortions. For benchmarks based on video editing, Grover et al. established an occluded dataset and further developed the benchmark to explore the impact of occlusion on action recognition models. Schiappa et al.'s work proposed a robustness analysis by introducing 90 perturbations that reflected different real-world distribution shifts in their benchmark, offering insights into robust video action recognition. However, most of these works were counterfactual or only considered the effects of camera disturbances from the camera's viewpoint, neglecting factors such as scene characteristics (e.g., shooting distance and direction) that are likely to be encountered in real filming scenarios. Inspired by the aforementioned studies, Animal-Bench employs video editing techniques to create new animal videos under varying weather conditions and shooting parameters, presenting new demands and challenges for model robustness.

## 3 Methods

In this section, we will introduce the details of our Animal-Bench. In the first part, we describe how our Animal-Bench is designed in terms of task definition and the automated pipeline of data processing. The second part details our approach to editing videos in our benchmark, aiming to simulate realistic scenarios.

### Animal Bench: Animal-Centric Evaluation

#### 3.1.1 Animal-Centric Tasks System

To evaluate the perceptual and cognitive abilities of multimodal large models on data where animals serve as the main agents, our Animal-Bench redefines tasks that were previously overlooked in human-centric benchmarks. First, we consider the common tasks from human-centric evaluations to assess the models' abilities in "object", "action", "time", "count", and "reasoning" on videos featuring animals. Additionally, from the perspective of application value, we identify specific tasks related to animals that are of greater interest to zoologists. For instance, we focus on models' ability to detect predator-prey behaviors , social interactions , breeding behaviors , and stress and

Figure 2: Example demonstrations of each task in Animal-Bench

pain , thereby promoting better research and protection of animals. As shown in Figure 2, we designed the following tasks:

Common TaskPerception: Object. (1) Object Existence(OE): Judge whether a certain item exists during a particular video; (2) Object Recognition(OR): Determine the specific class of the object that appears in the video. Action. (3) Action Recognition(AR): Recognize the action performed by the animal based on a piece of video; (4) Action Sequence(AS): Infer the action of an animal before or after a certain action in chronological order; (5) Action Prediction(AP): Given a specific action and its starting and finishing time in the video, predict the subsequent action performed by the animal; (6) Action Localization(AL): Assess the start and end time of a specific action performed by the animals in the video. Count. (7) Action Count(AC): Calculate how many times an action has been performed in the video; (8) Object Count(OC): Calculate how many times an object appears in the video. Cognition: (9) Reasoning(RS): Logically infer why an event or a certain scenario occurred in the video.

Specific Task(10) Predator-Prey Behavior Monitoring(PM): Detecting the interactions between predators and their prey that influence survival strategies, such as hunting techniques and evasion tactics; (11) Social Interaction Analysis(SA): Analyzing behaviors occurring between animals that affect social structure, communication, and cooperation; (12) Breeding Behavior Monitoring(BM): Monitoring activities related to reproduction, including mating and caregiving for offspring; (13) Stress and Pain Detection(PD): Detecting physiological and psychological responses to harmful stimuli or adverse conditions that impact the welfare and behavior of animals.

#### 3.1.2 Animal-Centric Data Processing Pipeline

We develop an automated pipeline for processing animal-centric data. Initially, we conduct data filtering on the existing dataset according to predefined task definitions. Subsequently, we formulate rules to automatically generate questions and options for all data, culminating in a dataset structured in the multiple-choice question answering (QA) format. The rationale for adopting the multiple-choice QA format  is twofold. Firstly, regarding difficulty, this format alleviates the challenge of delineating the scope of options inherent in open-ended QA formats, while still presenting a substantial challenge to capabilities of models. Secondly, from the point of evaluation fairness, the multiple-choice QA format facilitates the calculation of final evaluation accuracy and enables effective comparison of different models' performance.

Data FilteringWe first select data that aligns with the defined evaluation tasks. The principles for data selection are as follows. (1) Data diversity: Diverse data enables our evaluation to encompass a variety of complex situations, avoiding biases caused by single species or single dataset. (2) Temporal sensitivity: The temporal sensitivity present in the data allows us to assess the model's temporal modeling capabilities, as appropriate video lengths facilitate accurate decision-making for multi-modal video models.

To ensure the diversity of data, our Animal-Bench dataset is sourced from 6 datasets, encompassing 7 major animal categories, including mammals, insects, reptiles, amphibians, birds, fishes, and sea animals, totaling 819 different animal species. Specifically, for tasks related to "action", "object" and "time", we primarily get the annotated data for animals, actions, and grounding from MammalNet  and Animal Kingdom . For the "count" and "reasoning" tasks, our evaluation data is sourced from open datasets such as TGIF-QA , MSRVTT-QA , and NExT-QA , from which we

Figure 3: The diagram of our animal-centric data processing pipeline: Firstly, choose dataset and identify tasks, then establish rules to filter data, and finally automatically generate QA pairs.

extract a substantial amount of data featuring animals as agents. For special tasks, due to the lack of annotated data, we select data with annotations that match the requirements of the tasks from Animal Kingdom, LoTE-animal , and MammalNet .

To ensure the temporal sensitivity of the data, we first select the Animal Kingdom dataset, which contains annotations relevant to video-grounding tasks. Specifically, it marks the time intervals of actions, facilitating our evaluation of tasks such as action prediction, action sequence, and action localization. Additionally, for tasks other than "object," we believe that correct answers cannot be obtained solely from spatial information but require temporal information. For these tasks, we constrain the video duration to between 3 to 35 seconds. This ensures that answering questions requires relevant temporal information without causing confusion for the model due to excessively long video lengths. Ultimately, the average duration of our benchmark videos is 14.61 seconds.

QA Pair GenerationWe have meticulously designed the generation rules for both the questions and the options. Here is the detailed process of our QA pair generation.

Automated Question Generation: For each task's description, we use ChatGPT  to generate 3 types of questions and randomly assign one of the generated questions to each piece of data.

Task-Based Option Design: (1) Directly adopt from existing QA annotation: For count and reasoning tasks, the options are directly chosen from the annotated multi-choice QA dataset. (2) Automatic option design: For other data without QA annotations, we automatically convert the original annotations into multi-choice QA format. For object existence tasks and special tasks, our options were set as "yes", "no", and "not sure". While for other tasks, options besides the right answer should be neither simple nor difficult in order to reflect the real perceptual ability of evaluated MLLMs, thereby our options should not be randomly chosen from the dataset. Specifically, for action-related tasks, considering the long-tailed distribution in the dataset, our four options consist of the correct answer, two options from the top 50% of most frequent answers, and one option from the least frequent 50% answers. For object recognition task, besides the right answer, two options are sampled from different major animal categories and one option is sampled from the same major animal category. This design ensures a balance in options. Once the options are set, they are randomly shuffled to ensure robustness of our evaluation. For a discussion on the option design rules, please refer to Appendix B.

### Realistic simulation based on video editing

First, we select different aspects to simulate, as follows:

* **Weather conditions.** Weather changes are common during outdoor filming. In this work, we choose snowy and frosty weather as the simulated conditions. Snow is a form of precipitation that visually obstructs. Frost forms when ice crystals coat lenses or windows.
* **Shooting parameters.** Due to the camera's placement in the natural habitat of animals, the shooting parameters may vary due to animal movements. In this work, we select shooting distance (affected by the movements of animals resulting in changes in proximity to the camera) and shooting direction (affected by animal collisions resulting in camera tilt) as the simulated shooting parameters.

Simulating variations in outdoor weather conditionsWe take simulating snowy weather as an example. First, we assume that the snow layer \(S\) is an image with random noise, which follows a normal distribution \(S(,^{2})\). Next, we resize the image to make the density and distribution of snow more uniform. Since snowflakes do not accumulate in dark areas but only in well-lit areas when they fall on an object's surface, we set a threshold \(t\) and remove parts below it. Specifically, we can express it as:

\[S^{{}^{}}=(S,f)&(S,f) t.\\ 0&\] (1)

Finally, we apply a blur effect to the snow layer to soften its edges. The blur operation can be expressed as:

\[S_{blurred}(x,y)=}_{i=-r}^{r}_{j=-r}^{r}e^{- +j^{2}}{2^{2}}} S^{{}^{}}(x-i,y-j),\] (2)where, \(S_{}(x,y)\) denotes the pixel value after blurring. \(r\) denotes the blur radius. \(\) represents the standard deviation of the Gaussian kernel. \(i\) and \(j\) represent the indices of the convolution kernel.

Simulating variations in shooting parametersWe aim to simulate different shooting distances from the camera to animals, as well as shooting directions. To simulate proximity, we achieve this through central cropping of each frame. However, when simulating remoteness or different shooting directions, we face the challenge of lacking broader context outside the current frame. To address this issue and enhance the realism of our video evaluations, we leverage the capabilities of the Diffusion model  to perform outpainting for regions beyond the original scene. Furthermore, we have developed an automated video editing pipeline to streamline this process.

For a video frame \(f^{3 H W}\), we perform scaling or rotational transformations on it using an orthogonal matrix. Scaling and rotation matrices can be represented as follows,

\[T_{S}=&0\\ 0& T_{R}=&- \\ &.\] (3)

Then we create a new blank canvas \(f^{}^{3 H W}\), which has the same center position as the original image. Then the part of the original video frame that remains on the new canvas after transformations is:

\[f^{}(x^{},y^{})=f(T^{-1}(x^{},y^{ })^{T}),&(x^{},y^{})^{T}$ is within the bounds of $f$}\\ 0,&.\] (4)

We encode \(f^{}\) and the mask of \(f^{}\) using a variational autoencoder  to derive their respective image embeddings \(y\) within the latent space. Subsequently, these embeddings, accompanied by initially sampled noise \(_{T}(0,)\) drawn from a standard normal distribution, are jointly fed into denoising process. The reverse diffusion process is formulated as:

\[_{t-1}=}}(_{t}-}{_{t}}}_{}(_{t},t,y))+ _{t},(0,),\] (5)

where \(_{t}\) and \(_{t}\) are predefined diffusion coefficients. \(_{}(_{t},t,y)\) is the predicted noise. \(_{t}\) is the noise standard deviation at each step. After obtaining the final latent space representation \(_{0}\), the image is generated through the decoder, \(_{0}=(_{0})\).

Due to the varying quality of outpainting generated by diffusion for each video frame, with some frames showing significantly better results than others, we observe that frames with smoother pixel transitions at the edges of the original image tend to have better outpainting quality. We hypothesize that the highest values in the image spectrum originate from the edges of the original outpainted image. Therefore, we select the frame with the smallest highest spectrum value as the guide image.

\[f^{}=_{f_{i}}((f_{i})), f_ {i}\{f_{1},f_{2},,f_{n}\}\] (6)

If directly using the outpainting part in the \(f^{}\) would cause slight misalignment at the edges of other frames. Inspired by , we blend images at different noise levels along the diffusion process using diffusion models. Starting from the outpainting part \(f^{}\), at each stage, we perform a guided diffusion step with a latent variable \(f^{}_{t}\) to obtain \(f^{}_{t-1}\), while simultaneously obtaining a noisy version \(f_{t-1}\) of the original frame \(f\). The \(f_{t-1}\) generated at this stage is a blend of two latent variables using a mask \(M\), represented as follows.

\[f_{t-1}=M*f_{t-1}+(1-M)*f^{}_{t-1}\] (7)

Figure 4: The diagram of simulation process for shooting parameters. Firstly, the transformed images along with their masks are encoded, and then passed through the diffusion model for denoising. After decoding, the final simulated video is obtained through the guided frame selection module and frame blending module.

Experiments

In this section, we will sequentially introduce the details of our experimental implementation, the effectiveness of existing multimodal video models on our proposed evaluation benchmark, and their robustness on our editing-based realistic scenario simulation data. Finally, we will discuss the experimental results, hoping to provide guidance for model optimization.

### Implementation details

We conducted all tests for multimodal video models on an NVIDIA RTX 4090 with 24GB of VRAM. To ensure fair comparisons, we standardize the 7B LLM backend versions used across all multi-modal video models tested during inference, thereby minimizing discrepancies in language proficiency due to differences in model sizes. Following the methodology outlined in , we establish a uniform system prompt and adopt the prompt-based model output matching strategy. All generated outputs successfully match the corresponding options. For each video, we sample 16 frames and resize them to (224, 224). During video editing, we utilize StableDiffusion-inpainting to expand the scenes beyond the captured footage and subsequently apply StableDiffusion-v1.5 for noise addition in frame blending.

### Evaluation results

Effectiveness evaluation resultsThe table 2 presents the evaluation results of eight existing multimodal video models on our Animal-Bench. As models evolve, their performance also improves. The recently released model VideoChat2  surpasses previous methods in most tasks. Notably, VideoChat2 achieves an accuracy of 86.75% in object recognition tasks and 68.23% in reasoning tasks. However, we also observe shortcomings in existing models on certain tasks. For instance, in action localization and action counting tasks, the answers provided by existing models are nearly equivalent to random guesses. These tasks typically require strong temporal understanding capabilities, which cannot be inferred solely from spatial scene comprehension. This indicates a need for enhancement in the temporal modeling abilities of existing models. Additionally, in the object existence task, the models tend to respond with "yes", indicating a severe hallucination problem.

   &  \\  Task & Random & mPLUG & Video & Video & Video &  &  & Video &  &  \\  & 99\% confidence interval & -Owl  & Chat  & -ChatGPT  & -LLAM  & & & & \\  OE & 33.32 \(\) 0.46 & 42.20 & 49.40 & 44.65 & 49.20 & 41.70 & 44.65 & 45.90 & 50.00 & 45.96 \\  OR & 24.96 \(\) 0.19 & 33.62 & 51.61 & 24.31 & 60.23 & 25.06 & 43.25 & 40.55 & 86.75 & 45.67 \\  AR & 25.26 \(\) 0.17 & 27.00 & 32.54 & 24.28 & 35.34 & 24.56 & 32.98 & 31.71 & 66.27 & 34.34 \\  AS & 26.12 \(\) 1.23 & 25.86 & 32.76 & 22.41 & 29.74 & 27.16 & 33.19 & 25.86 & 54.31 & 31.41 \\  AP & 24.16 \(\) 1.31 & 25.48 & 27.88 & 24.52 & 29.81 & 28.37 & 27.88 & 28.37 & 50.00 & 30.29 \\  AL & 25.49 \(\) 0.39 & 24.49 & 23.25 & 21.22 & 24.67 & 25.45 & 24.14 & 24.32 & 21.22 & 23.60 \\  OC & 25.17 \(\) 1.18 & 24.14 & 27.59 & 24.71 & 26.44 & 25.29 & 31.61 & 31.03 & 64.94 & 31.97 \\  AC & 25.06 \(\) 0.37 & 24.43 & 25.51 & 22.92 & 24.99 & 23.78 & 24.34 & 22.49 & 29.16 & 24.70 \\  RS & 19.46 \(\) 0.71 & 22.38 & 27.07 & 25.69 & 35.08 & 22.65 & 36.46 & 21.27 & 68.23 & 32.35 \\   \\  PM & 33.58 \(\) 0.41 & 43.19 & 48.00 & 44.88 & 50.68 & 40.28 & 49.70 & 45.37 & 52.44 & 46.82 \\  BM & 33.63 \(\) 1.21 & 39.31 & 50.29 & 43.35 & 47.98 & 44.80 & 48.84 & 42.20 & 47.69 & 45.56 \\  SA & 33.22 \(\) 0.54 & 41.08 & 48.87 & 47.23 & 49.47 & 42.96 & 48.18 & 44.16 & 52.42 & 46.80 \\  PD & 33.15 \(\) 1.55 & 40.56 & 47.55 & 46.85 & 50.35 & 38.46 & 44.06 & 45.80 & 54.20 & 45.98 \\   \\  Avg & 27.89 \(\) 2.90 & 31.83 & 37.87 & 32.08 & 39.54 & 31.58 & 37.64 & 34.54 & 53.66 & 37.34 \\  

Table 2: The evaluation results of 8 multimodal video models on our Animal-Bench (the first place for each task is marked in red, and the second place is marked in blue, and those below random accuracy are marked in gray).

Robustness evaluation resultsWe test the robustness of our models and their sensitivity to different types of variations on four types of simulated real-world data. We select the top four models in terms of effectiveness evaluation for robustness testing, as models with lower accuracy tend to provide responses close to random guessing, rendering discussions about robustness less meaningful. As shown in Table 2, VideoChat2 demonstrates relatively good robustness, with an overall decrease in accuracy of 3.70%. However, Video-LLAMA  shows sensitivity to the four simulated variations, with an overall decrease in accuracy of 8.72%. As depicted in Figure 5, we calculated the average accuracy decrease of the models for the four types of variations, revealing that models are more sensitive to shooting parameters than to changes in weather changes.

### Further discussion

Video editing samplesAs shown in Figure 6, we demonstrate the effects of our video editing pipeline used for simulating shooting parameters. It can be observed that the guided frames obtained through spectrum filtering have initially achieved good results. However, there still exist unnatural transitions at the edges of the original image. After undergoing the diffusion noise addition process again, the edges of the original image can transition better into the newly generated parts of the image. Since we employed a frozen stable diffusion model, the generation effect relies on the performance of this model.

Animal category biasWe aim to investigate whether the models exhibit biases towards different animal categories. We analyze the performance of multimodal video models in two tasks: object recognition and action recognition. We choose these two tasks for analysis due to their involvement with the most diverse range of animal species. As shown in Figure 7, in the object recognition task, models demonstrate higher recognition accuracy for the "mammal" and "bird" categories, while accuracy is generally lower for categories like "amphibian" and "reptile". Similarly, in the action recognition task, models exhibit higher accuracy in recognizing "fish"

Figure 6: The visualization results of simulated changes in shooting parameters. Zoom in to view details.

   &  &  &  \\  & Snow & Froad & Distance & Direction & \\  VideoChat2 & 1.49 & 2.17 & 4.76 & 6.39 & 3.70 \\ Video-LLAMA & 5.41 & 7.46 & 10.82 & 11.19 & 8.72 \\ VideoChat & 7.43 & 4.41 & 5.22 & 8.63 & 6.42 \\ Chat-UnVi & 5.04 & 1.81 & 3.83 & 7.86 & 4.64 \\  

Table 3: Sensitivity of multimodal video models to different variations(relative accuracy drop(%)).

This could possibly be attributed to the greater prevalence of mammalian and fish species in the pretraining video datasets. Conversely, other categories suffer from lower recognition performance due to larger domain disparities.

Model structureWe also analyze on the impact of model architecture on accuracy, we find from the data in Figure 8 that using the frozen CLIP ViT/L-14  as the video encoder resulted in overall performance inferior to models employing larger or fine-tuned video encoders. This suggests that employing more powerful video encoders aids in a more comprehensive exploration of video features, which is of significant importance for the development of multi-modal video models. In addition, we observe that introducing a temporal modeling module into the model architecture is not particularly effective. According to Figure 8, models with an additional temporal modeling module outperform some earlier models in terms of answer accuracy but do not reach the level of some recently released models. This finding suggests that in the model design, the impact of the temporal modeling module may not be as significant as expected.

## 5 Conclusion

In this work, we introduced Animal-Bench, an animal-centric benchmark to evaluate multimodal video models in real-world contexts. It includes 13 tasks across 7 major animal categories and 819 animal species. We proposed an automated pipeline of data processing. And we used video editing approaches to simulate realistic scenarios. We evaluated 8 current models on our Animal-Bench, and found significant room for improvement. We analyzed the bias of models towards animal categories and examined the influence of model architecture on experimental results. Our work aims to provide valuable insights and foster new research directions in multimodal video models.

Figure 8: The line graph depicting the impact of video encoders and temporal modeling on the results.

Figure 7: The radar map illustrates the accuracy on ”object recognition” and ”action recognition” tasks across 7 animal categories.