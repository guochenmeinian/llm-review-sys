# An Analytical Study of Utility Functions in Multi-Objective Reinforcement Learning

Manel Rodriguez-Soto

Artificial Intelligence

Research Institute (IIIA-CSIC)

Bellaterra, Spain

manel.rodriguez@iiia.csic.es

&Juan A. Rodriguez-Aguilar

Artificial Intelligence

Research Institute (IIIA-CSIC)

Bellaterra, Spain

jar@iiia.csic.es

&Maite Lopez-Sanchez

Universitat de Barcelona (UB)

Barcelona, Spain

maite_lopez@ub.edu

###### Abstract

Multi-objective reinforcement learning (MORL) is an excellent framework for multi-objective sequential decision-making. MORL employs a utility function to aggregate multiple objectives into one that expresses a user's preferences. However, MORL still misses two crucial theoretical analyses of the properties of utility functions: (1) a characterisation of the utility functions for which an associated optimal policy exists, and (2) a characterisation of the types of preferences that can be expressed as utility functions. In this paper, we contribute to both theoretical analyses. As a result, we formally characterise the families of preferences and utility functions that MORL should focus on: those for which an optimal policy is guaranteed to exist. We expect our theoretical results to foster the development of novel MORL algorithms that exploit our theoretical findings.

## 1 Introduction

Sequential decision-making problems are ubiquitous, impacting areas like autonomous driving , robotics , finance  and healthcare , to name a few. Recently, Reinforcement Learning (RL) has emerged as a pivotal framework for addressing sequential decision-making tasks . Most of the RL literature has focused on problems for which an agent deals with a single objective (e.g. get rich in finance, win a race). However, real-world scenarios often present multiple, conflicting objectives  (e.g., a self-driving car must ensure safety, efficiency, and passenger comfort).

Multi-Objective Reinforcement Learning (MORL) has developed as one of the most promising frameworks for addressing multi-objective decision-making . Despite its novelty compared to single-objective RL, the current state of the art in MORL shows promising results for tackling real-world problems that are inherently multi-objective . Most MORL approaches are _utility-based_ and assume that there exists a _utility function_ that combines all objectives into a single one, allowing the learning agent to ponder between them. However, deciding the most appropriate utility function is a problem in itself. Given that, the literature on MORL focuses on learning a set of candidate policies, called the _undominated set_, which maximise all possible utility functions. In that way, once a utility function is decided, the decision-maker can directly select the policy from the undominated set that maximises it.

Thus, utility functions are widely considered a fundamental concept of MORL . Utility functions capture a user's preferences over different objectives and drive the learning . Hence, both concepts (utilities and preferences) are at the core of state-of-the-art MORL. The state-of-art approach of considering the undominated set as the most general solution concept of MORL relies on two assumptions:

1. **On utilities:** It assumes that for every utility function, there exists a policy optimising it.
2. **On preferences:** It assumes that any user preference can be expressed as a utility function.

Unfortunately, none of the assumptions is correct. There are many examples of preferences that cannot be expressed with a utility function (e.g., the lexicographic order , as proved in ). Likewise, even for problems with a finite amount of possible states and actions, there are many utility functions for which there is no optimal policy for any state (we provide an explicit example in the later sections).

These two counterexamples raise the need for answering the following two main theoretical questions: (1) for which type of utility functions are optimal policies guaranteed to exist? (2) what types of preferences can be represented as utility functions? The state of the art on MORL has not addressed these fundamental research questions so far.

Against this background, we propose an in-depth analysis of utility functions in MORL by means of the following three contributions:

1. **We provide two novel MORL fundamental theoretical concepts**. We introduce the first formal definition of preferences between policies in MORL and the first formal definition of utility maximisation in MORL.
2. Given a utility function in MORL, **we characterise the sufficient conditions that guarantee the existence** of an optimal policy maximising it.
3. **We characterise under which conditions we can express preferences between policies as utility functions.** These are represented by a type of function called _quasi-representative_ utility function, which preserves the most preferred policies as its maximal points.

We expect that our theoretical results will lead to novel MORL algorithms that can exploit the analytical properties of the utility functions introduced here.

The remainder of this paper is organised as follows. Section 2 provides the necessary background in multi-objective reinforcement learning. Then, Section 3 provides sufficient conditions to guarantee the existence of utility-maximising policies. Next Section 4 characterises the family of preferences that can be represented with utility functions. Thereafter, Section 5 presents the related work. Finally, Section 6 summarises our main theoretical findings and sets paths for future research.

## 2 Background

### Single-objective reinforcement learning

In single-objective reinforcement learning (RL), sequential decision-making problems are formalised as _Markov decision process_ (MDP) [11; 27]. An MDP represents an environment in which an agent is capable of repeatedly acting upon it to make it to transition it to a different state, and immediately receive a scalar reward (representing the agent's objective) after each action:

**Definition 1** (Markov Decision Process).: _A (single-objective)1 Markov Decision Process (MDP) is defined as a tuple \(,,R,T\) of two sets and two functions: the set of states \(\), the set of actions \((s)\) available at each state \(s\), the reward function \(R:\), and the transition function \(T:\) specifying the probability \(T(s,a,s^{})=(s^{} s,a)\) that the next state is \(s^{}\) if an action \(a\) is performed upon the state \(s\)._

An agent's behaviour in an MDP is called a _policy_\(\). A policy \((s,a)\) describes how likely an agent will perform action \(a\) if the agent is currently in state \(s\). The agent's objective is to learn the policy that accumulates the maximum sum of discounted rewards. Thus, to evaluate a given policy, we need to compute the (expected) discounted sum of rewards that an agent obtains by following it. This operation is formalised by means the so-called _value function_\(V:\), defined as:

\[V^{}(s) [_{k=0}^{}^{k}R(S_{t+k},A_{t+k},S_{t+k+1} ) S_{t}=s,],\] (1)

where \([0,1)\) is the discount factor, indicating how much we care about future rewards. Value functions allow us to partially order policies . Hence, they allow to formalise the agent's objective as learning the policy that maximises the value function. This policy is defined as the _optimal_ policy:

**Definition 2** (Optimal policy).: _Given an MDP \(\), its optimal policy \(_{*}\) is the policy that maximises the value function \(V^{}\). Formally:_

\[V^{_{*}}(s) V^{}(s),\] (2)

_for every state \(s\) of the MDP \(\), and every policy \(\) of \(\)._

The optimal policy is the solution concept in single-objective RL. For any MDP with a finite state and action space, at least one optimal policy exists, which is also deterministic and stationary .

### Multi-objective reinforcement learning

Multi-objective reinforcement learning (MORL) deals with environments in which an agent pursues multiple objectives simultaneously (for example, in a healthcare context, the health and the autonomy of a patient). Recall that in single-objective RL, the reward function represents the agent's objective. Thus, MORL considers environments with multiple reward functions, called _Multi-Objective Markov Decision Processes_[19; 10]. Formally:

**Definition 3** (Multi-Objective MDP).: _An \(n\)-objective Markov Decision Process (MOMDP) is defined as a tuple \(,,,T\) where \(\), \(\) and \(T\) are the same as in an MDP, and \(:^{n}\) is a vector of reward functions, providing a reward function \(R_{i}\) for each objective \(i\{1,,n\}\)._

Policies in a MOMDP are evaluated by means of a value function _vector_\(\), defined as the vector of all value functions per objective \((s)=(V_{1}(s),,V_{n}(s))\).

If not all objectives can be fully fulfilled simultaneously, the agent needs to prioritise between them. To represent an agent's preferences with respect to multiple objectives, most approaches in MORL assume that the value function of each objective can be aggregated into a single function. That way, the agent's goal becomes to maximise this aggregated value function.This aggregation is performed by means of a _utility function_\(u\) (also called a _scalarisation function_)[18; 22]. In MORL, a utility function \(u\) is defined as a function mapping the domain of all value functions (a subset of the real coordinate space \(^{n}\)) to the real space \(\). With \(u\), the agent's goal can be expressed as learning a policy that maximises the function \((u)(s)=u((s))\). Formally2:

**Definition 4** (Utility function).: _Let \(\) be a MOMDP of \(n\) objectives. Any function \(u:^{n}\) is a utility function of \(\)._

The family of linear utility functions is especially notable. Any linear utility function \(l\) returns a weighted sum of value functions \((l)(s)=(s)\). For linear utility functions, the scalarised problem of maximising \(l\) can be solved with single-objective reinforcement learning algorithms3.

While in single-objective RL there is a clear definition of the solution concept (a deterministic and stationary optimal policy), there is no equivalent for MORL. Instead, the utility function is typically assumed to be _unknown_, and that we only have minor assumptions about it (e.g., that it is linear or that it is monotonically increasing) [19; 22; 10]. With that in mind, the solution concept in MORL is to learn a set of _candidate policies_\(\), with each of them optimising a possible utility function \(u\). The next Section explores typical solution concepts in MORL.

### Solution concepts of MOMDPs

The solution concepts in MORL depend on how much is assumed about the utility function. If nothing is assumed, the goal is to learn the set of maximal policies for _any_ utility function. It is important to remark what it means for a policy to be _maximal for a utility function_. By far, the majority of the MORL community follow the so-called _state-independent_ (SI) criterion to define optimality [19; 18; 22; 10]. Given a MOMDP, this criterion considers that a policy \(_{*}\)_maximises_ a utility function if and only if it is maximal among the expectation of possible initial states (for some value function \(\) and the random variable of possible initial states \(S_{0}\)). We denote this expectation with \(_{SI}\):

\[_{SI}^{}[^{}(S_{0})].\] (3)

Due to its simplicity, the _state-independent_ (SI) criterion is widely used in MORL and RL in general. While it is generally innocuous in single-objective RL, it can generate contradictory policies in multi-objective RL, as we will show in Example 4 below.

Considering this _state-independent_ criterion, all solution concepts for MOMDPs have been formalised exclusively for it. Thus, the state-of-the-art general solution, the _undominated set_, is defined as the set of policies that are maximal for at least one utility function. Formally :

**Definition 5** (Undominated set).: _Given a MOMDP \(\), its undominated set \(U()\) is defined as the set of policies for which there exists a utility function \(u\) with a maximal scalarised value._

\[U()\{() u:^{} (),\ u(_{SI}^{}) u(_{SI}^{^{}})\},\] (4)

_where \(()\) is the set of all possible policies of an MOMDP \(\)._

We recall that the definition of undominated set makes no assumption on the structure of the utility function. If we constrain it to be a linear function, then the solution concept becomes the _convex hull_. The convex hull of a MOMDP contains all policies that are maximal for at least one linear utility function (again, according to the SI criterion). Formally :

**Definition 6** (Convex hull).: _Given an MOMDP \(\), its convex hull \(CH()\) is the subset of policies \(_{*}\) that are optimal for some weight vector \(\):_

\[CH()\{()^ {n}:^{}(),\ _{SI}^{}_{SI}^{^{}}\},\] (5)

_where \(()\) is the set of policies of \(\)._

## 3 Utility optimal policies

Recall that the MORL literature defines solution concepts following the _state-independent_ criterion. However, for a proper analysis of utility functions in MORL, we require more precise definitions considering each and every state of a MOMDP.

Furthermore, recall that, in single-objective MDPs, value functions impose a partial order over policies of an MDP . However, thanks to Banach's fixed point theorem, we know that a deterministic and stationary optimal policy always exists for any finite MDP (and, thus, for every state) . These theoretical properties become much weaker in multi-objective MDPs. In particular, the Banach fixed point theorem does not generalise even for finite MOMDPs. Thus, we may have finite MOMDPs for which no optimal policy exists for any state. These "more precarious" theoretical results motivate even more the need for studying the existence of optimal policies in a MOMDP at two different levels: one at a _single-state_ level (i.e., considering a single state), and another one at the _all-states_ level (considering all states).

We begin by defining _utility optimality_ at the state level: a given policy \(\) is optimal at state \(s\) with respect to a given utility function \(u\) if and only if it obtains more scalarised discounted returns than any other policy at \(s\). Formally:

**Definition 7** (utility optimal policy at a state).: _Let \(\) be a MOMDP with state set \(\). Let \(^{}\) be the set of policies of \(\). Let \(u\) be a utility function. Then, a policy \(_{*}^{}\) is optimal with respect to utility function \(u\) at state \(s\) if and only if:_

\[(u^{_{*}})(s)(u^{})(s),\] (6)

_for every policy \(^{}\). We say that \(_{*}\) is \( u,s\)-optimal for short._

**Example 1**.: _Consider a MOMDP \(\) with two states: an initial state \(s_{1}\) and a terminal state \(s_{2}\). An agent can perform two actions (\(a_{1}\), \(a_{2}\)) in this environment, with rewards \((s_{1},a_{1})=(1,0)\), \((s_{1},a_{2})=(0,1)\) respectively. Consider the utility function \(u(x,y)=x+sin(y)\). The deterministic policy \((s_{1})=a_{1}\) that obtains vectorial value \(^{}(s_{1})=(1,0)\) is clearly \( u,s_{1}\)-optimal since \(sin(y)<y\) for any \(y\)._

In the same vein as in single-objective RL, given a MOMDP, we define a policy as utility optimal at the _all-states_ level (or simply utility optimal) as a utility optimal policy in every state in the MOMDP. Formally:

**Definition 8** (utility optimal policy).: _Let \(\) be a MOMDP with state set \(\). Let \(^{}\) be the set of policies of \(\). Let \(u\) be a utility function. Then, a policy \(_{*}^{}\) is optimal with respect to utility function \(u\) if and only if:_

\[(u^{_{*}})(s)(u^{})(s),\] (7)

_for every policy \(^{}\), and every state \(s\). We say that \(_{*}\) is \(u\)-optimal for short._

**Example 2**.: _In the MOMDP in Example 1, policy \((s_{1})=a_{1}\) is \(u\)-optimal since there are only two states, and the second one is terminal._

We know that a (deterministic) \(u\)-optimal policy always exists for any linear utility function, as shown in Section 2.2. However, this is not always the case for arbitrary utility functions. The following three examples illustrate conditions that are not enough to guarantee the existence of neither deterministic nor stochastic utility optimal policies in finite MOMDPs. These conditions are:

1. That the utility function is **monotonically increasing** (a family of utility functions specially studied in MORL . Example 3 illustrates how this condition is not enough to guarantee a deterministic \(u\)-optimal policy.
2. That the utility function is **strictly monotonically increasing**. Example 5 shows how this condition is not enough to guarantee a stochastic \( u,s\)-optimal policy for any given state \(s\).
3. That the utility function is _both_ **strictly monotonically increasing** and **continuously differentiable**. Example 4 shows how even assuming both conditions there are utility functions without stochastic \(u\)-optimal policies.

In the following Example 3 we consider the _Chebyshev_ function (also called _Tchebycheff_, a well-known utility function in MORL . The Chebyshev function returns more scalar value the nearest a given input value \(x\) is to a _reference_ value \(\). Moreover, the Chebyshev function is also monotonically increasing .

**Example 3**.: _Let \(>0\) be a small real number, \(^{n}\) a reference value, and \(^{n}\) a weight vector such that each \(w_{i} 0\). The Chebyshev function \(_{,,w}:^{n}\) is defined as :_

\[_{,,}(x)-(_{i}w_{i}|r_{i}-x_{i}|+ _{i}w_{i}|r_{i}-x_{i}|).\] (8)

_Let \(\) be a 2-objective deterministic MDP with three states \(s_{1}\), \(s_{2}\), and \(s_{3}\) such that \(s_{3}\) is the terminal state. Regarding actions, there is one possible action in \(s_{1}\), which has associated rewards \((s_{1},a_{1})=(1,0)\). Action \(a_{1}\) transitions the state to \(s_{2}\). Then, in state \(s_{2}\), there are two possible actions with associated rewards \((s_{2},a_{2})=(2,20)\) and \((s_{2},a_{3})=(3,1)\). All actions in \(s_{2}\) transition to terminal state \(s_{3}\)._

_This environment has two possible deterministic policies. The first policy is \(_{1}(s_{1})=a_{1},_{1}(s_{2})=a_{2}\). This policy obtains values \(^{_{1}}(s_{1})=(3,20),^{_{1}}(s_{2})=(2,20)\). The second policy is \(_{2}(s_{1})=a_{1},_{2}(s_{2})=a_{3}\). This policy obtains values \(^{_{2}}(s_{1})=(4,1),^{_{2}}(s_{2})=(3,1)\). We select as reference point \(=(3.5,20)\), associated weights \(=(1,1/19)\), and \(=0\). For this configuration we have: for policy \(_{1}\), \((^{_{1}}(s_{1}))=-0.5\), \((^{_{1}}(s_{2}))=-1.5\). For policy \(_{2}\), \((^{_{2}}(s_{1}))=(^{_{2}}(s_{2}))=-1\). Clearly, \(_{1}\) is the only deterministic \(,s_{1}\)-optimal policy, while \(_{2}\) is the only deterministic \(,s_{2}\)-optimal policy. Thus, no deterministic \(\)-optimal policy exists._

Example 3 showed that deterministic \(u\)-optimal policies do not necessarily exist in finite MOMDPs, a fact that was already known in the MORL literature . But we can go one step further: thenext Example 4 shows that being finite is not enough for an MOMDP to guarantee the existence of stochastic \(u\)-optimal policies. Moreover, the utility function from Example 4 is both _strictly_ monotonically increasing _and continuously differentiable_:

**Example 4**.: _Consider the utility function \(u(x,y)=+1}+\), which is strictly monotonically increasing for any \((x,y)^{+}\), and continuously differentiable in \(^{2}\). Let \(\) be the same 2-objective deterministic MDP from Example 3._

_This environment has the same two deterministic policies from Example 3. The first policy is \(_{1}(s_{1})=a_{1},_{1}(s_{2})=a_{2}\), which obtains scalarised values \(u(^{_{1}}(s_{1})) 4.16,u(^{_{1}}(s_{2})) 3.24\). The second policy is \(_{2}(s_{1})=a_{1},_{2}(s_{2})=a_{3}\), which obtains scalarised values \(u(^{_{2}}(s_{1})) 4.17,u(^{_{2}}(s_{2})) 3.21\)._

_It is easy to check that \(_{1}\) is the absolute \( u,s_{2}\)-optimal policy, while \(_{2}\) is the absolute \( u,s_{1}\)-optimal policy. We leave the details at Appendix A.1. Thus, no stochastic \(u\)-optimal policy exists._

Our third example is a finite MOMDP and a strictly monotonically increasing utility function \(u\) for which no stochastic \( u,s\)-optimal policy exists for any state \(s\).

**Example 5**.: _Consider the utility function \(u\) such that if \(x=y\), then \(u(x,x)=0\), and otherwise \(u(x,y)=\). Let \(\) be the 2-objective deterministic MDP from Example 1 with two states \(s_{1}\) and \(s_{2}\) such that \(s_{1}\) is the initial state and \(s_{2}\) is the terminal state. There are two possible actions in \(s_{1}\) with associated rewards \((s_{1},a_{1})=(1,0)\) and \((s_{1},a_{2})=(0,1)\)._

_Every policy of \(\) will be of the form \((s_{1},a_{1})=p\) and \((s_{1},a_{2})=1-p\) for some \(p\). The vectorial value of such policy at state \(s_{1}\) will be \(^{}(s_{1})=(p,1-p)\). Notice how every possible value belongs to the Pareto Front of \(\). Thus, any utility function is strictly monotonically increasing in \(\), including the one defined in this example._

_In particular, for any policy, its scalarised value will be \(u(1,1-p)=\)._

_If for any policy \(\) we have \((s_{1},a_{1})=p<\), then the policy \(^{}\) such that \((s_{1},a_{1})=p+\), with \(>0\) small enough so that \(p+<\) obtains more scalarised value than \(\). Similarly, if \((s_{1},a_{1})=p>\), we can find an alternative policy \(^{}\) such that \((s_{1},a_{1})=p-\) that obtains more scalarised value than \(\). Thus, no \( u,s_{1}\)-optimal policy exists in this MOMDP._

The result of Example 5 is specially significant because most MORL literature (with its state-independent criterion that only considers initial states \(S_{0}\)), focuses on computing \( u,S_{0}\)-optimal policies on strictly monotonically increasing utility functions [19; 10]. As we have shown in Example 5, such optimal policies are not guaranteed to exist.

Therefore, the logical next question after these three examples is to ask for which families of utility functions there exists at least one global utility optimal policy or at least one utility optimal policy for every state. In particular, we focus on stationary policies, like in single-objective RL. Formally:

**Problem 1**.: _For which families of utility functions is guaranteed that a stationary \( u,s\)-optimal policy will exist for every state \(s\) of every possible finite MOMDP?_

**Problem 2**.: _For which families of utility functions is guaranteed that a stationary \(u\)-optimal policy will exist for every possible finite MOMDP?_

Next, Sections 3.1 and 3.2 focus on providing _sufficient conditions_ to guarantee the existence of utility optimal policies in a state and utility optimal policies in general, respectively.

### Utility optimal policy at a state existence

This Section introduces a family of utility functions that solve Problem 1. In particular, we offer a sufficient condition to guarantee the existence of a stationary \( u,s\)-optimal policy for every state \(s\) of a finite MOMDP. This sufficient condition is that the utility function is continuous. Formally:

**Theorem 1**.: _Let \(\) be a finite MOMDP. Let \(u\) be a continuous utility function for all value functions of all policies \(()\) of \(\). Then, for every state \(s\) of \(\), at least one stationary \( u,s\)-optimal policy exists._

Proof 1.: See Appendix A.3.

Continuous utility functions are one the most extensively studied and applied family of functions due to their _well-behaved_ properties (e.g., existence of absolute maximum and minimum). Nevertheless, recall that Theorem 1 only provides sufficient conditions, and thus there might exist \( u,s\)-optimal policies for discontinuous utility functions. We offer such an example in the proof of Theorem 3.

### Utility optimal policy existence

Demanding that the same policy is \(u\)-optimal for some utility function \(u\) for every state of the MOMDP is a much harder problem than demanding it for a given state. Thus, in this case, it is not enough that the utility function is continuously differentiable (i.e., continuous and all partial derivatives also continuous), and it is not enough that the utility function is also strictly monotonically increasing (as seen in Example 4).

It is already known that, for linear utility functions, we can obtain a \(u\)-optimal policy. So, the question is if we can find at least another family of utility functions for which a \(u\)-optimal policy exists. Theorem 2 presents such a family: utility functions that result from composing an affine function together with a strictly monotonically increasing function. Formally:

**Theorem 2**.: _Let \(\) be a finite multi-objective MDP \(\). Let \(u\) be a utility function decomposable as \(u(x)=h(g(x))\), with \(g(x):^{n}\) being an affine function, and \(h(x):\) being a strictly monotonically increasing function for all value functions of all policies \(()\) of \(\). At least one deterministic and stationary \(u\)-optimal policy exists._

Proof 2.: See Appendix A.4. 

Notice that, in particular, Theorem 2 also covers linear utility functions. Linear utility functions are one of the most widely applied families of utility functions in MORL [18; 10]. To finish this Section, we show an example of a function composed by an affine and a strictly monotonically increasing function (that hence satisfies Theorem 2) that produces a non-linear (and non-affine) utility function for which a \(u\)-optimal policy exists.

**Example 6**.: _Consider any 2-objective MDP \(\) where all rewards are positive (i.e., \((s,a)^{+}^{+}\) for all \(s,a\)), and a utility function \(u\) defined as_

\[u(x,y)=-.\] (9)

_We decompose \(u(x,y)\) as \(u(x,y)=h(g(x,y))\) with \(g(x,y)=x+y+3\) being affine, and \(h(x)=-\) being strictly monotonically increasing. By Theorem 2, a \(u\)-optimal policy exists._

Notice that Theorem 2 only provides sufficient conditions of utility functions \(u\) for guaranteeing the existence of \(u\)-optimal policies. In fact, Example 2 shows a non-affine utility function for which an \(u\)-optimal policy exists in a particular MOMDP.

## 4 Preference relations in MORL

In the previous section, we characterised the utility functions for which we can compute a utility optimal policy. However, as mentioned in the Introduction, a more fundamental question remains unanswered: Which user's preferences can be expressed as utility functions in a given MOMDP?

We require formalising preference relations and their maximal elements in MOMDPs to answer this last question. Preference relations, also known as binary relations, allow us to express, among two elements of a set, which one we prefer [25; 14]. While the state of the art in MORL makes no distinction between preference relations and utility functions , it is important to maintain them as two separate concepts. First of all, let us provide a formal definition of preference relations in MORL, inspired by :

**Definition 9** (Preference relation in a MOMDP).: _Let \(\) be a MOMDP of \(n\) objectives. We define a preference relation in \(\) as any binary relation \(\) over at least one pair of value vectors of \(^{n}\). In particular, we say that:_* _a value function_ \(_{1}\) _is_ weakly preferred _to another value function_ \(_{2}\) _if and only if_ \(_{1}(s)_{2}(s)\) _for every state_ \(s\) _of_ \(\)_. In short, we denote_ \(_{1}_{}_{2}\)_._
* _a value function_ \(_{1}\) _is_ strictly preferred _to another value function_ \(_{2}\) _if and only if_ \(_{1}(s)_{2}(s)\) _for every state_ \(s\) _of_ \(\) _and not_ \(_{2}(s^{})_{1}(s^{})\) _for at least one state_ \(s^{}\) _of_ \(\)_. In short, we denote_ \(_{1}_{}_{2}\)_._

If for two value vectors we have that \(_{1}(s)_{2}(s)\) and \(_{2}_{1}(s)\), we say that they are indifferent, and we denote it with the \(\) symbol. Notice that this definition makes no assumption over the preference relation. We do not impose that this preference relation is a pre-order, a partial order, or a total order . Considering that the MORL literature applies utility functions of all kinds, we did not want to restrict our definition.

Following the game theory literature, humans have preferences, and we (sometimes) can represent them as utility functions, but not the other way around . In fact, sometimes, a utility function that fully represents our preferences may not exist. If such a utility function exists, we call it the _representative_ utility function of preference relation \(\). Formally:

**Definition 10** (Representative utility function).: _Let \(\) be a MOMDP, and let \(\) be a preference relation in \(\). Then, we define a utility function \(u\) as representative of the preference relation \(\) if and only if, for every pair of possible value functions \(_{1}\), \(_{2}\), and every state \(s\) of \(\):_

\[_{1}(s)_{2}(s)(u_{1})(s)(u {V}_{2})(s).\] (10)

Some (but not all) preference relations have representative utility functions. However, any utility function \(u\), is representative of some preference relation \(_{u}\) defined as exactly fulfilling Equation 10.

In order theory, for any quasi-order (i.e., a preference relation that is at least reflexive and transitive), we can define the concept of maximal elements . In our case, given a preference relation \(\) between the value functions of a MOMDP, its maximal elements would be the value functions associated with the policy that we expect the agent to learn. Formally:

**Definition 11** (Maximal element).: _Let \(\) be a MOMDP. Let \(\) be a preference relation in \(\) that is at least reflexive and transitive (a quasi-order). Then, the value vector \(_{*}(s)\) of value function \(_{*}\) is a maximal element in state \(s\) if and only if for every other possible value function \(\) of \(\):_

\[(s)_{*}(s)_{*}(s)(s).\] (11)

**Example 7**.: _In finite single-objective MDPs, the optimal value \(V_{*}(s)\) is a maximal element for every state \(s\), for the preference relation \(\) defined as \(V(s) V^{}(s) V(s) V^{}(s)\)._

As mentioned in the introduction above, not all preference orders in MORL can be represented as a utility function. One of the most well-known cases is the _lexicographic order_. Although Lexicographic MORL has been studied in detail , almost no work in MORL (with the exception of ) has noticed that an associated utility function does not exist in general. Let us see through an example why the lexicographic order cannot be represented as a linear utility function. For utility functions in general, we refer to Corollary 2 of .

**Example 8**.: _Consider a MOMDP \(\) with two states: an initial state \(s_{1}\) and a terminal state \(s_{2}\). An agent can perform three actions in this environment (\(a_{1},a_{2},a_{3}\)), with rewards \((s_{1},a_{1})=(1,0)\), \((s_{1},a_{2})=(0,1)\), and \((s_{1},a_{3})=(1,1)\), respectively. Consider now the lexicographic order \(\) such that objective 1 is always preferred to objective 2. Hence, \((s_{1},a_{3})(s_{1},a_{1})(s_{1},a_{2})\). Any linear utility function here will be of the form \(u_{,}(x,y)= x+ y\). For \(u_{w}\) to represent \(\), it must satisfy \(u_{,}(1,1)>u_{,}(1,0)\) and \(u_{,}(1,0)>u_{,}(0,1)\), for example, \(u_{10,1}(x,y)=10x+y\)._

_However, policies can be stochastic, and thus, we can have for instance any policy \(\) such that \((s_{1},a_{1})=p,(s_{1},a_{2})=1-p\), with \(1 p 0\), which has associated value \(^{}(s_{1})=(p,1-p)\). Hence, the utility function must also satisfy \(u(1,0)>u(0.9,0.1)>>u(0.1,0.9)>u(0,1)\). And it needs to be absolutely precise: \(u(p+,1-p-)>u(p,1-p)\) for every \(>0\) arbitrarily small. Thus, it is impossible to represent the lexicographic order as a linear utility function._While lexicographic orders cannot be represented with utility functions in MOMDPs, they do have maximal elements among finite MOMDPs. A utility function that shares the exact same maximal elements as a lexicographic order would be very helpful. With such a utility function, we could still find the policies that maximise a lexicographic order with state-of-the-art MORL algorithms. Having formalised maximal elements in MOMDPs for any quasi-order, we can introduce utility functions that _at least_ preserve maximal elements. We call this kind of utility function _quasi-representative_. Formally:

**Definition 12**.: _Let \(\) be an MOMDP. Let \(\) be a preference relation \(\) in \(\) that is at least reflexive and transitive (a quasi-order). Let \(u\) be a utility function such that for every state \(s\) of \(\):_

\[_{*}(s)s_{*}(s)[(u)(s)].\] (12)

_Then, we say that \(u\) is quasi-representative of \(\) in \(\)._

**Example 9**.: _Consider the MOMDP \(\) in Example 8. The utility function \(u(x,y)=10x+y\) is quasi-representative of the lexicographic order because \(u(1,1)>u(1,0)\) and \(u(1,1)>u(0,1)\)._

In fact, quasi-representative utility functions allow us to define an equivalence relation between utility functions. Hence, by abuse of notation, we will also say that two utility functions are _quasi-representative_ for a given MOMDP if and only if they share the same maximum elements for every state \(s\) of this MOMDP.

**Example 10**.: _Consider the same MOMDP \(\) and the lexicographic order \(\) from Example 8. For example, utility functions \(u(x,y)=10x+y\) and \(u^{}(x,y)=15x+y+30\) share the same utility optimal policy (which is \((s_{1})=a_{3}\)), and hence are quasi-representative for \(\) and \(\)._

Notice that if a utility function \(u\) is representative of some preference relation \(\), it is also quasi-representative of \(\). Notice also that a utility function may be representative or quasi-representative of a given preference order for some MOMDP but not for other MOMDPs.

Now, given a MOMDP, what conditions must a preference order meet to be represented by a quasi-representative utility function? Essentially, it is sufficient to have a maximal element for every state of the MOMDP. We present now a family of preference orders for which a quasi-representative utility function always exists for every finite MOMDP:

**Theorem 3**.: _Let \(\) be a preference relation and \(\) any finite MOMDP. Assume that \(\) is: (1) complete (either \(a b\) or \(b a\) or \(a b\) for every two possible value vectors \(a,b\) of \(\)); (2) transitive (if \(a b\), and \(b c\), then \(a c\)); and (3) at least one maximal element \((s)\) exists for every state \(s\) of \(\). Then, a quasi-representative utility function exists for \(\) in \(\)._

Proof 3.: We offer a constructive proof. For every state \(s\), consider its set of maximal elements \(}_{*}(s)\) according to \(\), which is non-empty for every state due to Condition (3). Since the preference relation is complete, for every state \(s\) all its maximal elements will share the same value (i.e., \(_{1}(s)=_{2}(s)\) for every two \(_{1},_{2}}_{*}(s)\)). Hence, without loss of generality, we consider that there is a single maximal element per state. Then, the number of maximal elements is at most \(|S|\), and we can order them according to \(\) (we can order them because \(\) is total and transitive). Then, set a number between 1 and \(|S|\) for each of these elements, ordered by \(\). For every other vector \(x^{n}\), set \(u(x)=0\). Now, by construction, for every state \(s\) we have that \(_{}(u)(s)}_{*}(s)\). In other words, \(u\) is a quasi-representative utility function, such that it returns the most preferred value vector for each state \(s\) according to \(\). 

Completeness and transitivity are very common conditions for preference relations in game theory . The third condition is required for MOMDPs since we are dealing with an infinite amount of policies. For instance, Example 5 showed a finite MOMDP for which there is no maximum element for any environment state.

The main takeaway from Theorem 3 is that MORL algorithms should focus on the family of preference relations that fulfils all its conditions. Such conditions are sufficient to guarantee the existence of a quasi-representative utility function, as we just proved.

The family of preference relations satisfying the conditions of Theorem 3 has examples aplenty, such as the previously mentioned family of lexicographic orders. Moreover, any continuous utility function is representative of a total order that satisfies all conditions of Theorem 3.

Related work

Most of the literature in MORL focuses on creating novel solution concepts in MORL and algorithmic methods to solve them (e.g., [28; 33; 20; 24; 21]). Instead, we focus on characterising for which families of utility functions these solutions exist, a largely overlooked theoretical problem despite its relevant implications. Take for instance the work in , where Van Moffaert _et al._ present a method for computing the Pareto front of a given MOMDP. They implicitly assume that this Pareto front will always include the _solution_ policy (a \(u\))-optimal policy in our terms), but as we have proven in Example 4, this is not always the case.

Then, regarding the study of preference relations in MORL, to the best of our knowledge the only other works in the literature apart from ours are [23; 26]. Skalse _et al._'s theoretical results in  complement ours by stating that, for every so-called _objective_ (a preorder between policies), a \(u\)-optimal policy exists if and only if this objective can be represented with a linear utility function. This aligns with our results in Theorem 2. However, they do not establish whether there may be more families of utility functions for which a \(u\)-optimal policy exists, as we do with Theorem 2. Moreover, our _preference_ definition allows for ordering policies in each state of the environment, providing more granularity than their _objective_ definition. This difference is also significant, because it allows us to identify issues in the solution concepts of MORL as we have tried to illustrate with Examples 5 and 4.

Next, Subramani _et al._ in  follow on the work in , but they tackle a different problem than us. Their focus is on compare the expressivity of the MORL framework with other frameworks. They aim to know which _objectives_ (defined identically to ) can be represented on each framework. However, like , their _objective_ definition does not allow them to order policies differently per state, unlike our _preference_ definition.

To finish, closely related to our work, Miura in  tackles the problem of characterising preferences and their properties in constrained MDPs . They define preferences as sets of _acceptable policies_ and aim to find for which environments they can set the constraints and reward functions of a constrained MDP (CMDP) for which the acceptable policies are optimal. While CMDPs and MOMDPs share many similarities, they belong to separate research area. The major difference between them is that a CMDP has constraints, while an MOMDP has a utility function.

## 6 Conclusions

Multi-objective reinforcement learning (MORL) is the most promising framework for dealing with sequential decision-making problems with multiple objectives. In MORL, the learning agent ponders between the multiple objectives by means of a utility function aligned with the user's preferences. However, the state of the art in MORL has disregarded two fundamental theoretical problems related to utility functions: (1) for which utility functions an associated optimal policy is guaranteed to exist? and (2) which preference relations can be expressed as a utility function?

In this paper, we contributed to the state of the art in MORL by formalising both problems for the first time and by analysing each one. For utility functions, we first formalised the concept of _utility optimality_ in MORL. Then, we provided sufficient and insufficient conditions for such a policy to exist for any finite MOMDP. For preference relations, we first formalise them for MOMDPs, and we also provide the minimal conditions to guarantee that they can be expressed as a particular type of utility function, the so-called _quasi-representative_ utility functions. We expect our theoretical contributions to spark interest in both theoretical and practical MORL research. In fact, our results have direct practical consequences: to avoid contradictory policies, the MORL community needs to design algorithms that check that their learned policies are utility optimal.

We envision many directions for future research. On the theoretical side, a generalisation of the presented theoretical results to multi-agent multi-objective environments would be of great interest in the MORL literature . On the algorithmic side, we expect to see the development of algorithms that exploit our Theorems to compute utility optimal policies.