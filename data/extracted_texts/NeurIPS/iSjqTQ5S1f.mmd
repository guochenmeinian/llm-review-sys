# Stochastic Concept Bottleneck Models

Moritz Vandenhirtz, Sonia Laguna, Ricards Marcinkevics, Julia E. Vogt

Department of Computer Science

ETH Zurich

Switzerland

Equal contribution. Correspondence to {moritz.vandenhirtz,slaguna}@inf.ethz.ch

###### Abstract

Concept Bottleneck Models (CBMs) have emerged as a promising interpretable method whose final prediction is based on intermediate, human-understandable concepts rather than the raw input. Through time-consuming manual interventions, a user can correct wrongly predicted concept values to enhance the model's downstream performance. We propose _Stochastic Concept Bottleneck Models_ (SCBMs), a novel approach that models concept dependencies. In SCBMs, a single-concept intervention affects all correlated concepts, thereby improving intervention effectiveness. Unlike previous approaches that model the concept relations via an autoregressive structure, we introduce an explicit, distributional parameterization that allows SCBMs to retain the CBMs' efficient training and inference procedure. Additionally, we leverage the parameterization to derive an effective intervention strategy based on the confidence region. We show empirically on synthetic tabular and natural image datasets that our approach improves intervention effectiveness significantly. Notably, we showcase the versatility and usability of SCBMs by examining a setting with CLIP-inferred concepts, alleviating the need for manual concept annotations.

## 1 Introduction

In today's world, machine learning plays a crucial role in making important decisions, from healthcare to finance and law. However, as these algorithms become more complex, understanding how they arrive at their decisions becomes increasingly challenging. This lack of interpretability is a significant concern, especially in situations where trustworthiness, transparency, and accountability are paramount (Lipton, 2016; Doshi-Velez & Kim, 2017). Recent studies have focused on Concept Bottleneck Models (CBMs) (Koh et al., 2020; Havasi et al., 2022; Shin et al., 2023), a class of models that predict human-understandable concepts upon which the final target prediction is based. CBMs offer interpretability since a user can inspect the predicted concept values to understand how the model arrives at its final target prediction. Moreover, if they disagree with a concept prediction, they can intervene by adjusting it to the right value, which in turn affects the target prediction.

For example, consider the yellow warbler in Figure 1 (a), where a user might notice that the binary concept 'yellow primary color' is mispredicted. Upon this realization, they can intervene on the CBM by setting its value to \(1\), which increases the probability of the class yellow warbler. This way of interacting allows any untrained user to engage with the model to increase its predictive performance.

However, if the user input is that the primary color is yellow, should not the likelihood of a yellow crown increase too? This adaptation would increase the predicted likelihood of the correct class even more, as yellow warblers are characterized by their fully yellow body. Currently, vanilla CBMs do not exhibit this behavior as they do not use the intervened-on concepts to update their remaining concept predictions. This indicates that they suboptimally adapt to the additional knowledge gained.

To this end, we propose to extend the concept predictions with the modeling of their dependencies, as depicted in Figure 1.

The proposed approach captures the concept dependencies by modeling the concept logits with a learnable non-diagonal normal distribution, which enables efficient, scalable computing of the effect of interventions on other concepts. By integrating concept correlations, we reduce the time and effort of having to laboriously intervene on many correlated variables and increase the efficacy of interventions on the downstream prediction. Thanks to the explicit distributional assumptions, the model is trained end-to-end, retaining the training and inference speed of classic CBMs as well as the benefits of training the concept and target predictor jointly. Moreover, we show that our method excels when querying user interventions based on predicted concept uncertainty (Shin et al., 2023), further highlighting the practical utility of our approach as such policies spare users from manually sifting through the concepts to identify necessary interventions. Lastly, based on the distributional concept parameterization, we propose a novel approach for computing dependency-aware interventions through the likelihood-based confidence region.

ContributionsThis work contributes to the line of research on concept bottleneck models in several ways. (_i_) We propose to capture and model concept dependencies with a multivariate normal distribution. (_ii_) We derive a novel intervention strategy based on the confidence region of the normal distribution that incorporates concept correlations. Using the learned concept dependencies during the intervention procedure allows for stronger interventional effectiveness. (_iii_) We provide a thorough empirical assessment of the proposed method on synthetic tabular and natural image data. Additionally, we combine our method with concept discovery where we alleviate the need for annotations by using CLIP-inferred concepts. In particular, we show the proposed method (a) discovers meaningful, interpretable patterns in the form of concept dependencies, (b) allows for fast, scalable inference, and (c) outperforms related work with respect to intervention effectiveness thanks to the proposed concept modeling and intervention strategy.

## 2 Background & Related Work

Concept bottleneck models (Koh et al., 2020; Lampert et al., 2009; N. Kumar et al., 2009) are typically trained on data points \((,,y)\), comprising the covariates \(\), target \(y\), and \(C\) annotated binary concepts \(\). Consider a neural network \(f_{}\) parameterized by \(\) and a slice \( g_{},h_{}\)(Leino et al., 2018) s.t. \(:=f_{}()=g_{}(h_{} ())\). CBMs enforce a concept bottleneck \(}:=h_{}()\) such that the model's final output depends on the covariates \(\) solely through the predicted concepts \(}\).

Figure 1: Overview of the proposed method for the CUB dataset. (a) A user intervenes on the concept of ‘primary color: yellow’. Unlike CBMs, our method then uses this information to adjust the predicted probability of correlated concepts, thereby affecting the target prediction. (b) Schematic overview of the intervention procedure. A user’s intervention \(^{}_{}\) is used to infer the logits \(_{}\) of the remaining concepts. (c) Visualization of the learned global dependency structure as a correlation matrix for the 112 concepts of CUB (Wah et al., 2011). Characterization of concepts on the left.

While Koh et al. (2020) propose the _soft_ CBM, where the concept logits parameterize the bottleneck, Havasi et al. (2022) argue that such a representation leads to leakage, where additional unwanted information in the concept representation is used to predict the target (Margeloiu et al., 2021; Mahinpei et al., 2021). Thus, they parameterize the bottleneck by binarized concept predictions and call it the _hard_ CBM. Then, Havasi et al. (2022) equip the hard CBM with an autoregressive structure of the form \(c_{i}|,_{<i}\), which is supposed to learn the concept dependencies. As such, the implicit autoregressive modeling of concept dependencies by Havasi et al. (2022) is the most related to the current work. Complementary to our work, Heidemann et al. (2023) analyze how a CBM's performance is affected by concept correlations. Unlike approaches that restrict the bottleneck to prevent leakage, Concept Embedding Models (CEM) (Espinosa Zarlenga et al., 2022) represent each concept with an embedding vector from which the concept probabilities can be inferred. E. Kim et al. (2023) model the embedding with a normal distribution, assuming a diagonal covariance matrix, which prevents them from capturing concept dependencies. Therefore, their intervention performance is not expected to differ from that of CEMs. Recent works explored how a CBM-like structure can be enforced even without a concept-annotated training set. Yuksekgouni et al. (2023) transform a pre-trained model into a CBM via a concept bank from concept activation vectors and multimodal models (B. Kim et al., 2018), while Oikarinen et al. (2023) query GPT-3 (Brown et al., 2020) for the concept set \(\) and assign the values of the concept activations to each datapoint \(\) with CLIP (Radford et al., 2021) similarities. Similarly, Panousis et al. (2023) uses CLIP to probabilistically discover a sparse set of concepts for each input, which could be used in our model for a fully probabilistic pipeline. Lastly, Marcinkevics et al. (2024) instead relax the need for a concept labeled training set to a smaller validation set by fine-tuning a pre-trained model.

Intervenability (Marcinkevics et al., 2024) is a crucial element of CBMs as it allows the user to correct wrongly predicted concepts \(}\) to \(^{}\), which in turn affects the target prediction of the model \(^{}\). If multiple concepts are intervened on sequentially, the order of interventions is important. To this end, Sheth et al. (2022) and Shin et al. (2023) explore multiple policies according to which the order of concepts is determined. Chauhan et al. (2023) propose to combine predefined policies with learnable weighting parameters, while Espinosa Zarlenga et al. (2024) learn the policy itself. Concurrently, Singhi et al. (2024) learn a realignment module to align concept predictions. Steinmann et al. (2023) argue that instance-specific interventions are costly and store previous interventions in a memory to automatically reapply them for similar data points. Lastly, Collins et al. (2023) explore the advantages of including uncertainty rather than treating humans as oracles.

Our work models concept dependencies by parameterizing the bottleneck with a distribution. In a similar vein, Variational Autoencoders (Kingma and Welling, 2014) parameterize the bottleneck with a normal distribution to model and generate new data. Stochastic Segmentation Networks (Monteiro et al., 2020) parameterize the logits of a segmentation map with a non-diagonal normal distribution to capture the spatial correlations of pixels and model the aleatoric uncertainty. The modeling of uncertainty with a distribution is also explored by Bayesian Neural Networks (Neal, 1995) that learn a probability distribution over the neurons of a neural network.

## 3 Methods

We propose Stochastic Concept Bottleneck Models1 (SCBM), a novel concept-based method that relaxes the implicit CBM assumption of independent concepts. SCBM captures the concept dependencies by learning their multivariate distribution. As a result, interventions become more effective and scalable, as a single intervention can influence multiple correlated concepts. A schematic overview of the proposed method is depicted in Figure 1 (b).

### Model Formulation

To capture the concept dependencies, we model the concept logits \(\) with a learned multivariate normal distribution. Modeling logits with a normal distribution has proven to be effective in the context of segmentation (Monteiro et al., 2020). While Monteiro et al. (2020) use it to capture the spatial dependencies of pixels, we, instead, model the relations between concepts, where the properties of the normal distribution will prove useful. A neural network is trained to predict the distribution's parameters \((()),( ))\), where \(()^{C}\), and \(()^{C C}\). Thus, the traditional assumption of independent concepts \(c_{i}\!\!\! c_{j},\ \  i j\) is relaxed to \(c_{i}\!\!\! c_{j},\ \  i j\), where the assumed normal distribution induces linear concept dependencies. The inductive bias of linearity is useful in practice as it is more robust to overfitting and computationally more scalable with respect to \(C\) compared to its nonlinear alternative (Havasi et al., 2022), as we will show in Section 5.

To learn the distribution, we minimize the negative log-likelihood

\[- p()=- p()p_{}()d,\] (1)

where \(\) are the parameters of a neural network that predicts the distribution \((()),( ))\). This integral is intractable due to the softmax operation applied in \(p()\). Thus, the integral is approximated by \(M\) Monte Carlo samples

\[- p()p_{}()d -_{m=1}^{M}p(^{(m)}),^{(m)}(()),( ))\,.\] (2)

In order to learn \(\), we make use of the parameterization as normal distribution and employ the reparameterization trick \(^{(m)}=()+()^{( m)},()()^{T}=(),^{(m)}(,)\) such that gradients can be computed with respect to the parameters. Lastly, we incorporate the new relaxed conditional independence assumption

\[ p()=_{i=1}^{C}p(c_{i}_{i})=_{i=1 }^{C} p(c_{i}_{i}),\] (3)

where \(p(c_{i}_{i})\) describes a Bernoulli distribution parameterized by the sigmoid-transformed logits \((_{i})\). Combining the above considerations results in the following reformulation of the negative log-likelihood:

\[- p() -_{m=1}^{M}p(^{(m)})\] (4) \[ -_{m=1}^{M}_{i=1}^{C} p(c_{i}_{i}^{( m)})\] \[= -_{m=1}^{M}_{i=1}^{C}[-(c_{i}, (_{i}^{(m)}))],\]

where BCE stands for Binary Cross Entropy, and the \(\) trick is used for numerical stability.

The distribution-based modeling procedure allows for efficient sampling, thus, enabling SCBM to train concept and target predictors jointly, sequentially, or independently. In contrast, the autoregressive alternative (Havasi et al., 2022) requires independent training due to the computational complexity. We adopt a joint training scheme to obtain the benefits of end-to-end learning where concept and target predictors can adjust to each other. To prevent leakage, we follow Havasi et al. (2022) and train the model with the hard \(\{0,1\}\) concept values as bottleneck rather than the logits used in the original CBM (Koh et al., 2020). To this end, we employ the straight-through Gumbel-Softmax trick (Jang et al., 2017; Maddison et al., 2017) that approximates Bernoulli samples while being differentiable. The target predictor \(g_{}\) is then learned by minimizing the negative log-likelihood

\[- p(y) =-_{}p_{}(y)p()\] (5) \[ -_{m=1}^{M}p_{}(y^{(m)}), ^{(m)} p().\]

Lastly, the learned dependencies are regularized by following Occam's razor and to prevent overfitting. We take inspiration from the Graphical Lasso (Friedman et al., 2008) and penalize the off-diagonal elements of the precision matrix \(^{-1}\).

By combining concept, target, and precision loss with weighting factors \(_{1}\) and \(_{2}\), we arrive at the final loss function

\[-_{m=1}^{M}_{i=1}^{C}-(c_{i},(_{i}^ {(m)}))+_{1}(y,_{m=1}^{M}g_{}(^{(m)}))+_{2}_{i j}()_{i,j}^ {-1}.\] (6)

### Covariance Learning

The introduced amortized covariance matrix \(()\) provides the flexibility to tailor its predicted concept dependencies to each data point, making it adaptable to many data-generating mechanisms. For example, in the commonly used CUB (Wah et al., 2011; Koh et al., 2020), it can learn the class-wise concept structure present in the dataset. The explicit dependency representation inferred by the learned covariance matrix is useful as it provides insights into the learned correlations among the concepts, which is important for understanding and interpreting the model behavior.

However, an amortized covariance matrix comes at the price of not being able to visualize and interpret a unified concept structure on a dataset level. Depending on the need of the application, such a global structure might be preferable. Thus, we propose a variation of SCBM, where the covariance matrix is not _amortized_ (\(()\)), but learned _globally_ (\(\)). An example of the global concept structure learned on CUB is shown in Figure 1 (c). This variation has the inductive bias of assuming a constant covariance matrix, whose utility depends on the underlying data-generating mechanism. We recommend using the more flexible, amortized version by default and only utilizing a global covariance if the strong assumption of fixed dependencies is reasonable. We will explore this empirically in more detail in Section 5.

### Interventions

A distinguishing property of CBM-like methods is the user's capacity to correct wrongly predicted concepts, which in turn affects the target prediction (Marcinkevics et al., 2024). For a big concept set, this intervention procedure can become quite laborious as a user has to inspect and manually intervene on each concept separately. SCBMs are designed to alleviate this need by utilizing the learned concept dependencies such that a single intervention affects all related concepts as modeled by the multivariate normal distribution.

The parameterization as a multivariate normal distribution allows for a quick, scalable intervention procedure. Given a set \(\{1,,C\}\) of concept interventions, the effect on the remaining concepts \(_{}\) is computed via their logits \(_{}\) by conditioning on the intervention logits \(^{}_{}\), utilizing the known properties of the normal distribution

\[_{},^{}_{ } (}(), }(_{})),\] (7) \[} =_{}+_{,}_{,}^{-1}(^{}_{ }-_{}),\] \[} =_{,}-_{,}_{,}^{-1}_{,}.\]

In standard CBMs, an intervention affects only the concepts on which the user intervenes. As such, Koh et al. (2020) set \(^{}_{i}\) to the 5th percentile of the training distribution if \(c_{i}=0\) and the 95th percentile if \(c_{i}=1\). While this strategy is effective for SCBMs too, see Appendix C.5, the modeling of the concept dependencies warrants a more thorough analysis of the _intervention strategy_. We present two desiderata, which our intervention strategy should fulfill.

1. \(p(c_{i}^{}_{i}) p(c_{i}_{i})\) The likelihood of the intervened-on concept \(c_{i}\) should always increase after the intervention. If SCBMs used the same strategy as CBMs, it could happen that the initially predicted \(_{i}\) was more extreme than the selected training percentile. Then, the interventional shift \(^{}_{i}-_{i}\) in Eq. 7 would point in the wrong direction. This would cause \(_{}\) to shift incorrectly.
2. \(|^{}_{i}-_{i}|\)_should not be "too large"_. We posit that the interventional shift should stay within a reasonable range of values. Otherwise, the effect on \(_{}\) would be unreasonably large such that the predicted \(_{}\) would be completely disregarded.

To fulfill these desiderata, we take advantage of the explicit distributional representation: the likelihood-based confidence region of \(_{i}\) provides a natural way of specifying the region of possible \(^{}_{}\) that fulfill our desiderata. Informally, a confidence region captures the region of plausible values for a parameter of a distribution. Note that the confidence region takes concept dependencies into account when describing the area of possible \(^{}_{}\). To determine the specific point within this region, we search for the values \(^{}_{}\), which maximize the log-likelihood of the known, intervened-on concepts \(}_{}\), implicitly focusing on concepts that the model predicts poorly:\[_{}^{}=*{arg\,max}_ {_{}}& p(_{} _{})\\ &-2( p(_{} {}_{},_{,})- p(_{ }_{},_{,}) )_{d,1-}^{2}\\ &_{i}^{}-_{i} 0c_{i}=1, i \\ &_{i}^{}-_{i} 0c_{i}=0, i ,\] (8)

where \(d=||\). The first inequality describes the confidence region. It is based on the logarithm of the likelihood ratio, which, after multiplying with \(-2\), asymptotically follows a \(^{2}\) distribution (Silvey, 1975). The last two inequalities restrict the region to the desired direction. Note that \(_{}^{}\) is computed to determine the conditional effect of the interventions on \(_{}\) using Equation 7. When predicting \(^{}\) under interventions, the logits \(_{}\) are then used for sampling the binary concept values \(_{}\) while the intervened-on concepts \(_{}^{}\) are directly set to their known, binary value.

## 4 Experimental Setup

Datasets and EvaluationWe perform experiments on a variety of datasets to showcase the validity of our method. Inspired by Marcinkevics et al. (2024), we introduce a synthetic tabular dataset with a data-generating mechanism that contains fixed concept dependencies we can regulate. In particular, the concept logits \(\) are sampled from a randomly initialized positive definite covariance matrix and generate \(\). Binary concept values \(\) are inferred from \(\) and generate the target \(y\). We refer to Appendix A.1 for a more detailed description.

As a natural image classification benchmark, we evaluate on the Caltech-UCSD Birds-200-2011 dataset (Wah et al., 2011), comprised of bird photographs from 200 distinct classes. It includes 112 concepts, such as wing color and beak shape, shared across the same class instances as revised in the original CBM work (Koh et al., 2020). Additionally, we explore another natural image classification task on CIFAR-10 (Krizhevsky et al., 2009) with 10 classes. To mitigate the concept annotations requirement, the concepts are synthetically acquired in a similar fashion to the concept discovery literature. We adopt the 143 concept classes generated via GPT-3 (Brown et al., 2020) in prior work (Oikarinen et al., 2023). To obtain the binary concept values, we use the CLIP model (Radford et al., 2021) to compute the similarity between each instance of an image with the text embedding of a specific concept and compare it to the similarity of its negative counterpart, i.e. _not_ the concept. Appendix A.2 contains further details about the natural image datasets.

To compare methods, we evaluate the model performance based on the concept and target accuracy. We compute test performance before and after intervening on an increasing number of concepts. The order of concepts in the intervention is determined by an uncertainty-based policy (Shin et al., 2023) that selects the concept whose predicted probability is closest to \(0.5\). We also show results for a random policy in Appendix C.3. Additionally, we evaluate the calibration of the predicted concept uncertainties that are being used for the uncertainty-based policy, with the Brier score (Brier, 1950) and the Expected Calibration Error (Naeini et al., 2015; A. Kumar et al., 2019).

BaselinesWe evaluate the performance of our method in comparison with state-of-the-art models. Namely, we focus on the vanilla concept bottleneck model (CBM) by Koh et al. (2020) in its _hard_ version (Havasi et al., 2022), trained jointly using the straight-through Gumbel-Softmax trick (Jang et al., 2017; Maddison et al., 2017), as a sensical baseline to our binary modeling of concepts. Additionally, we explore the concept embedding model (CEM) by Espinosa Zarlenga et al. (2022) that learns two concept embeddings, \(}_{i}^{+}\) and \(}_{i}^{-}\). These representations are used to predict the final concept probability with a learnable scoring function \(_{i}=s(}_{i}^{+},}_{i}^{-})=(_{s} [}_{i}^{+},}_{i}^{-}]^{T}+_{s})\) and are then combined into a final concept embedding \(}_{i}=(_{i}}_{i}^{+}+(1-_{i})}_ {i}^{-})\) that is passed to the target predictor. Interventions are modeled by altering the concept probabilities \(_{i}\). Note that Espinosa Zarlenga et al. (2022) optimize for intervention performance during training, which we omit, to ensure a fair comparison where no method was explicitly trained for intervention performance. Finally, we evaluate the autoregressive CBM structure proposed by Havasi et al. (2022), where concept dependencies are learned with an autoregressive structure. Here, each concept \(c_{i}\) is predicted with a separate MLP that takes as input a latent representation of the input \(f_{}()\) and all previous concepts \(c_{1},...,c_{i-1}\). To obtain a good initialization of the autoregressive structure, it is pretrainedfor \(50\) epochs. As the Monte Carlo sampling from the autoregressive structure is time-consuming, the target predictor \(g_{}\) is trained independently using the ground-truth concepts as input. At intervention time, a normalized importance sampling algorithm is used to estimate the concept distribution.

Implementation DetailsThe model architectures comprise a backbone for concept prediction followed by a linear layer as head for an interpretable target prediction. More details can be found in Appendix B. To ensure the positive definiteness of the concept covariance matrix \(\), we parameterize it via its Cholesky decomposition \(=^{}\). Thus, we directly predict the lower triangular Cholesky matrix \(\). We will evaluate two options for SCBMs: using a _global_ (\(\)) or an _amortized_ covariance matrix \((())\). For the amortized version, we set the weighting terms \(_{1}\) and \(_{2}\) of Equation 6 to 1. For the global version, we initialize it with the estimated empirical covariance matrix and set \(_{2}=0\), as we did not observe big differences when varying \(_{2}\). In Appendix C.4, we provide an ablation study, demonstrating that SCBMs are not very sensitive to the choice of \(_{2}\). At intervention time, we solve the optimization problem based on the \(99\%\)-confidence region with the SLSQP algorithm (Kraft, 1988). In Appendix C.6, we provide an ablation with different confidence levels.

## 5 Results

Test performanceIn Table 1, we report the results of the concept and target accuracy prior to interventions. Overall, SCBM performs on par with the baseline methods, with no clear outperforming or underperforming technique throughout the datasets. In Appendix C.7, we show that other metrics lead to the same interpretation. This shows that the additional overhead of learning the concept dependencies does not negatively affect the predictive performance. We note that the amortized covariance variant consistently surpasses the globally learned matrix due to its ability to adjust the predicted concept dependency structure and uncertainty on an instance level. On the other hand, the global variant offers a unified understanding of the concept correlations, an example of which is presented in Figure 1 (c). Notably, in CIFAR-10, even though the concept performance of CEM is the worst of all methods, it has the best target performance. This might suggest the presence of leakage in CEM's embeddings, as in CIFAR-10, the concept set alone is not sufficient to predict the target, and learning

   Dataset & Method & Concept Accuracy & Target Accuracy \\   & Hard CBM & 61.42 \(\) 0.07 & 58.38 \(\) 0.39 \\  & CEM & 61.42 \(\) 0.12 & 58.01 \(\) 0.49 \\  & Autoregressive CBM & 62.17 \(\) 0.11 & **59.60**\(\) 0.62 \\  & Global SCBM & 61.57 \(\) 0.05 & 58.39 \(\) 0.53 \\  & Amortized SCBM & **62.41**\(\) 0.20 & 58.96 \(\) 0.38 \\   & Hard CBM & 94.97 \(\) 0.07 & 67.72 \(\) 0.57 \\  & CEM & 95.12 \(\) 0.07 & 69.60 \(\) 0.30 \\  & Autoregressive CBM & **95.33**\(\) 0.07 & 69.24 \(\) 0.44 \\  & Global SCBM & 94.99 \(\) 0.09 & 68.19 \(\) 0.63 \\  & Amortized SCBM & 95.22 \(\) 0.09 & **69.87**\(\) 0.56 \\   & Hard CBM & 85.51 \(\) 0.04 & 69.73 \(\) 0.29 \\  & CEM & 85.12 \(\) 0.14 & **72.24**\(\) 0.33 \\   & Autoregressive CBM & 85.31 \(\) 0.06 & 68.88 \(\) 0.47 \\   & Global SCBM & 85.86 \(\) 0.04 & 70.74 \(\) 0.29 \\   & Amortized SCBM & **86.00**\(\) 0.03 & 71.66 \(\) 0.25 \\   

Table 1: Test-set concept and target accuracy (%) prior to interventions. Results are reported as averages and standard deviations of model performance across ten seeds. For each dataset and metric, the best-performing method is **bolded** and the runner-up is underlined.

   Method & Training & Inference \\  Hard CBM & 5x & 1x \\ CEM & 5x & 1x \\ Autoregressive CBM & 5x & 15x \\ Global SCBM & 5x & 1x \\ Amortized SCBM & 5x & 1x \\   

Table 2: Relative time it takes for one epoch in the CUB dataset when training on the training set, or evaluating on the test set, respectively.

additional information might be useful. In Table 2, we show the time it takes for training and testing of the methods. It is evident that the autoregressive CBM of Havasi et al. (2022) suffers from a slow sampling process due to its autoregressive structure, while SCBMs retain the efficiency of CBMs.

InterventionsIn this paragraph, we analyze the intervention performance of SCBMs and their baseline models, focusing on their effectiveness in modeling concept dependencies and improving target accuracy. Figure 2 shows the intervention curves across ten seeds, where the performance is measured based on the concept and target accuracy. The order of concepts to intervene on is determined by an uncertainty-based policy that makes use of the predicted probabilities. In Appendix C.3, we present the intervention performance if concepts were selected randomly. The intervention curves in the first row show that SCBMs are superior in modeling the concept dependencies, as evidenced by their significantly steeper intervention curves compared to the baseline methods. Furthermore, the second row of Figure 2 indicates that the strong concept modeling translates to a significant improvement in downstream performance, partly thanks to the intervention strategy introduced in Section 3.3. We note that especially for the most practical scenario of only a small number of interventions, SCBMs outperform their counterparts. Comparing the SCBM variants, the natural image datasets show an overall better intervention performance with the amortized covariance matrix, following the trend of Table 1, as it can capture the instance-wise correlation structure of the data. Only in the synthetic dataset, where the data-generating covariance matrix is fixed, does the global SCBM slightly outperform the amortized one. Thus, we advocate for the usage of the global variant only if the underlying assumption of a fixed covariance is reasonable. Lastly, the success of SCBMs on CIFAR-10, with CLIP-based concepts, shows our proposed method can work without human-annotated concepts. To strengthen this point and also showcase the scalability of our method, in Appendix C.1, we provide results on CIFAR-100 with 892 concepts, where our SCBMs also strongly outperform baselines.

Analyzing the performance of the autoregressive CBM, which also captures concept dependencies, we observe that they expectedly have a better intervention performance than the hard vanilla CBM, which does not take correlations into account. However, it becomes evident that, compared to the concept performance of SCBMs, their autoregressive structure does not capture the dependencies to the full extent. This shows in the target accuracy, where they only match or outperform SCBMs towards the full set of intervened concepts. We attribute the better performance on the full intervention set to the independent training procedure utilized by autoregressive CBMs, which comes at the cost of lower test performance in CIFAR-10. Arguably, in a realistic use-case, such a high number of instance-level interventions is not sensible, and if it were, SCBMs could also be trained independently. Finally, the CEM shows reduced intervention performance as the expressive concept embeddings, which are prone to information leakage, seem to suboptimally adapt to the injected concept information.

Figure 2: Performance after intervening on concepts in the order of highest predicted uncertainty. Concept and target accuracy (%) are shown in the first and second rows, respectively. Results are reported as averages and standard deviations of model performance across ten seeds.

Modeling the concept distributionA cornerstone of SCBMs is the explicit, distributional parameterization of concepts. This helps in understanding the data correlations and allows for visualization, as the example seen in Figure 1 (c). The explicit probabilistic modeling results in improved concept uncertainty estimates compared to the baseline CBM counterparts, as shown in Table 3, where lower metrics imply better estimates. This proves useful for interventions, where the uncertainty estimates can be leveraged for the choice of concepts to intervene on, improving the target prediction more effectively and reducing the need for manual user inspection. In Figure 3, we compare the performance of randomly intervening versus intervening based on the predicted uncertainty. We observe that there is a big gap between the two policies, indicating the usefulness of the estimated probabilities. Nevertheless, note that intervening at random remains successful and supports the observations made in the previous paragraph, as shown in Appendix C.3.

## 6 Conclusion

In this paper, we introduced SCBMs, a new concept-based method that models concept dependencies with a multivariate normal distribution. We proposed a novel, effective intervention strategy that takes concept correlations into account and is based on the confidence region inferred from the distributional parameterization. We showed that our modeling approach retains CBMs' training and inference speed, thus, being able to harness the benefits of end-to-end concept and target training. Additionally, the explicit parameterization offers the user a clearer understanding of the learned concept dependencies, providing deeper insights into how predictions and interventions are made. Empirically, we demonstrated that by modeling the concept dependencies, SCBMs offer a substantial improvement in intervention effectiveness, in concept as well as target accuracy, compared to related work. We showed that our method excels when iteratively intervening on the most uncertain concept predictions, sparing users from having to manually search through the concept set to identify necessary interventions. Additionally, our results indicate that learning the concept correlations does not decrease performance prior to interventions, in many cases even improving the performance over the baselines. Finally, the versatility of SCBMs is highlighted through their superior performance on CIFAR-10 and CIFAR-100, where concept values are CLIP-based rather than human-annotated.

   Dataset & Method & Brier & ECE \\   & Hard CBM & 28.79 \(\) 0.09 & 22.38 \(\) 0.15 \\  & CEM & 29.32 \(\) 0.08 & 23.55 \(\) 0.09 \\  & Autoregressive CBM & **24.84**\(\) 0.32 & **13.54**\(\) 0.49 \\  & Global SCBM & 27.73 \(\) 0.09 & 20.10 \(\) 0.14 \\  & Amortized SCBM & 25.58 \(\) 0.20 & 15.57 \(\) 0.55 \\   & Hard CBM & 3.93 \(\) 0.05 & 2.44 \(\) 0.06 \\  & CEM & 4.04 \(\) 0.05 & 3.25 \(\) 0.07 \\  & Autoregressive CBM & 3.75 \(\) 0.05 & 2.73 \(\) 0.05 \\  & Global SCBM & 3.87 \(\) 0.06 & 2.33 \(\) 0.09 \\  & Amortized SCBM & **3.64**\(\) 0.07 & **1.85**\(\) 0.08 \\   & Hard CBM & 10.42 \(\) 0.05 & 4.93 \(\) 0.17 \\  & CEM & 11.06 \(\) 0.16 & 7.11 \(\) 0.39 \\   & Autoregressive CBM & 10.70 \(\) 0.05 & 6.07 \(\) 0.10 \\   & Global SCBM & 9.95 \(\) 0.02 & 2.88 \(\) 0.11 \\   & Amortized SCBM & **9.84**\(\) 0.02 & **2.22**\(\) 0.12 \\   

Table 3: Test-set calibration (%) of concept predictions. Results are reported as averages and standard deviations of model performance across ten seeds. For each dataset and metric, the best-performing method is **bolded** and the runner-up is underlined. Lower is better.

Figure 3: Intervention performance of SCBMs measured in concept and target accuracy (%) on CUB for random and uncertainty-based policy.

Limitations & Future WorkThis work opens multiple new research avenues. A natural extension is to go beyond binary concepts, such as continuous domains with their corresponding adaptations of modeling the concept distribution. Additionally, addressing the quadratic memory complexity of the covariance matrix is essential for scaling to larger concept sets. Our proposed intervention strategy accounts for model uncertainty, but further research is needed to accommodate user uncertainty, as human interventions are not always the ground truth. This work allows the editing of the learned dependency structure by adjusting the entries of the predicted covariance matrix, which could be explored. Lastly, to model additional information and reduce leakage, Koh et al. (2020); Havasi et al. (2022) propose the adoption of a side channel. The complementary effectiveness of incorporating the side channel in the covariance structure could be explored in the context of SCBMs.