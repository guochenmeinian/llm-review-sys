# EMVP: Embracing Visual Foundation Model for Visual Place Recognition with Centroid-Free Probing

Qibo Qiu\({}^{1,2,}\) Shun Zhang\({}^{3,}\)1 Haiming Gao\({}^{3}\)

**Honghui Yang\({}^{1}\) Haochao Ying\({}^{4,}\)\({}^{}\) Wenxiao Wang\({}^{5}\) Xiaofei He\({}^{1}\)**

\({}^{1}\) State Key Lab of CAD&CG, Zhejiang University

\({}^{2}\) China Mobile (Zhejiang) Research & Innovation Institute, \({}^{3}\)Zhejiang Lab

\({}^{4}\)School of Public Health and Second Affiliated Hospital, Zhejiang University School of Medicine

\({}^{5}\)School of Software Technology, Zhejiang University

qiuqibo_zju@zju.edu.cn, haochaoying@zju.edu.cn

Equal contribution. Corresponding author.

###### Abstract

Visual Place Recognition (VPR) is essential for mobile robots as it enables them to retrieve images from a database closest to their current location. The progress of Visual Foundation Models (VFMs) has significantly advanced VPR by capturing representative descriptors in images. However, existing fine-tuning efforts for VFMs often overlook the crucial role of probing in effectively adapting these descriptors for improved image representation. In this paper, we propose the Centroid-Free Probing (CFP) stage, making novel use of second-order features for more effective use of descriptors from VFMs. Moreover, to control the preservation of task-specific information adaptively based on the context of the VPR, we introduce the Dynamic Power Normalization (DPN) module in both the recalibration and CFP stages, forming a novel Parameter Efficiency Fine-Tuning (PEFT) pipeline (EMVP) tailored for the VPR task. Extensive experiments demonstrate the superiority of the proposed CFP over existing probing methods. Moreover, the EMVP pipeline can further enhance fine-tuning performance in terms of accuracy and efficiency. Specifically, it achieves 93.9%, 96.5%, and 94.6% Recall@1 on the MSLS Validation, Pitts250k-test, and SPED datasets, respectively, while saving 64.3% of trainable parameters compared with the existing SOTA PEFT method. The code is available at https://github.com/vincentqqb/EMVP.

## 1 Introduction

Visual Place Recognition (VPR) is indispensable for mobile robots and autonomous vehicles, enabling key functions such as global localization , Simultaneous Localization and Mapping (SLAM) , and scene understanding . VPR is often tackled as an image retrieval problem, where the objective is to match the query (an image representing the current location) with images from previously visited places. To achieve efficient matching, typically based on distance metrics like Euclidean distance, the VPR system aggregates the local descriptors of each image into a global descriptor. However, VPR faces unique challenges compared to conventional image retrieval tasks, including drastic changes in image perspectives, seasonal variations, and occlusions. Consequently, many researchers are dedicated to exploring robust local descriptors that exhibit invariance to these challenges. The advent of deep learning significantly brings VPR to a new stage characterized by enhanced robustness and improved accuracy [4; 5; 6; 7; 8; 9; 10; 11].

Despite the carefully designed pipelines, these methods typically involve training a model from scratch on environment-specific data. However, the diversity of application environments makingit challenging to collect sufficient data across different settings. To address this, a few recent studies [12; 13] have explored the potential of the Visual Foundation Model (VFM). These studies mainly focus on designing effective adapters within the backbone while only employing existing aggregation (probing) methods. _However, specific probing techniques for more effective fine-tuning in the VPR task remain largely untapped._ Typically, the most popular probing techniques mainly exploit first-order statistics of features, including Linear Probing (LP) for classification and Generalized-Mean (GeM) pooling for VPR, as shown in Figure 1 (a) and (b). However, second-order statistics have been well-proven important for fine-grained classification tasks , but LP and GeM are unable to capture them explicitly.

To this end, we revisit the classic method NetVLAD  as bilinear pooling (_i.e._, the most basic second-order statistics), which aggregates local descriptors into a global one for fine-grained classification. However, NetVLAD requires a costly offline initialization of the semantic centroids, limiting its flexibility for fine-tuning on different datasets. In addition, inaccurate centroids can introduce inductive bias (for instance, initializing centroids in urban scenes but training or inferring in rural scenes), and affect the model's generalization ability. On the other hand, semantic centroids serve as priors for probing (aggregation), and simply removing them, as shown in Figure 1 (c), can lead to a decline in aggregation performance (detailed in Section 3.1). Delightfully, we observe that the explicit calculation of semantic centroids can be avoided when introducing a simple and effective **Constant Normalization (CN)** (detailed in Section 3.2). On this basis, a novel **Centroid-Free Probing (CFP)** stage is naturally introduced, which takes efforts of the second-order statistics of features.

Furthermore, due to the fact that VPR heavily relies on small overlapping regions between different images to make judgments when dealing with changes in image perspectives, preserving information from these discriminative regions is crucial. We design the **Dynamic Power Normalization (DPN)** module to adaptively control the preservation of task-specific information during the FFP stage, referred to as \(}\). Moreover, due to the different training objectives, the general representation capability of pre-trained VFMs tends to focus more on foreground objects. However, the VPR task relies more on background regions such as the salient building. To address this, we insert DPN modules into the backbone, termed \(}\), to enhance the preservation of information from these key background regions. With the backbone frozen, trainable \(}\) modules adaptively control the preservation of task-specific background information in intermediate features, effectively contributing to Parameter Efficiency Fine-Tuning (PEFT). In the remainder of this paper, it is termed the recalibration stage. Thus, we propose a novel PEFT pipeline named EMVP by combining both recalibration and CFP stages. The main contributions of this paper can be summarized as follows:

* We are among the first to explore probing techniques for VPR tasks. By discussing the classical NetVLAD aggregation method, we demonstrate that avoiding costly and unstable initialization of semantic centroids allows for more effective fine-tuning of VFMs.
* We propose the novel DPN module to adaptively control the preservation of task-specific information in both recalibration and CFP stages. Based on the above, a more effective PEFT pipeline, named EMVP, is proposed.

Figure 1: Comparison of different probing methods. (a) The most popular Linear Probing (LP) in classification fine-tuning. (b) Generalized-Mean (GeM) pooling adapted by SelaVPR , which can be seen as a generalized form of first-order feature. (c) The NetVLAD operation simplified by SALAD . (d) The proposed Centroid-Free Probing (CFP) which provides a theoretical and empirical justification for this simplification, fixing interpretability and performance issues that were present otherwise.

* Extensive experiments demonstrate that the proposed EMVP pipeline significantly contributes to the more accurate VPR. Plenty of ablation studies have verified the effectiveness of indispensable components (_i.e_., CFP, CN, \(}\), and \(}\)).

## 2 Related Work

Visual Place Recognition (VPR) is typically studied as an image retrieval problem. Based on research topics, advanced VPR approaches can be categorized into improvements on the feature extraction of the backbone network , aggregation methods for local descriptors [4; 15; 16], the design of metric loss functions [17; 7], investigations into robustness against viewpoint changes [6; 18; 19], and others. This section discusses researches closely related to the proposed method.

### Aggregation in Visual Place Recognition

Traditional VPR methods typically employ bag-of-visual-words [20; 21; 22], Vector of Locally Aggregated Descriptors (VLAD) [23; 24; 25] or Fisher Vectors (FV) [26; 27; 28] to aggregate local features such as SIFT  and SURF  into global ones. However, traditional methods for hand-crafted feature extraction are not data-driven. With the increase in data volume, these methods suffer from insufficient generalization and robustness. To address this problem, Arandjelovic _et al_.  proposed an end-to-end trainable generalized VLAD layer, NetVLAD, which greatly promotes the aggregation of features extracted from deep learning methods. Therefore, with the development of deep learning, NetVLAD becomes the most popular aggregation method for the VPR task [6; 8; 5]. Alternative techniques to NetVLAD include average/max pooling, R-MAC , and Generalized Mean (GeM) [15; 32]. Although these methods exhibit promising effectiveness in retrieving images, they regularly demonstrate inferior performance compared to NetVLAD in the VPR task [16; 33]. Benefiting from the emergence of the Visual Foundation Model (VFM) and embodied AI, mobile robots have progressed greatly in visual tasks. For instance, the latest research  shows that the combination of a self-supervised pre-trained ViT model (_i.e_., DINOv2) and the unsupervised aggregation method VLAD exhibits robust zero-shot VPR performance. This motivates us to explore the cooperation between supervised NetVLAD and VFM for better accuracy.

In this paper, we avoid the costly explicit calculation of semantic centroids required by NetVLAD, by introducing a simple and effective Constant Normalization (CN) (detailed in Section 3.2). On this basis, a novel Centroid-Free Probing (CFP) stage including the \(}\) module is proposed to employ second-order features when fine-tuning a VFM for better VPR performance.

**Differences between CFP and SALAD .** SALAD also avoids explicit calculation of the semantic centroids when full fine-tuning a VFM, directly aggregating local descriptors with a summation. More impressively, we show that CFP admits a theoretical and empirical justification for this simplification, fixing interpretability and performance issues that were present otherwise. Furthermore, a novel PEFT pipeline (_i.e_., EMVP) tailored for the VPR task is proposed, innovatively employing the same DPN module in both recalibration and CFP stages for task-specific information preservation. Therefore, superior performance can be achieved with minimized trainable parameters.

### Fine-tuning of Visual Foundation Model

Inspired by the remarkable language generation capabilities and interactivity demonstrated by the GPT series , pre-trained on expansive text corpora, subsequent researches in VFMs has flourished. VFMs, including SAM , DINOv2 , and the multi-modal CLIP , exhibit notable visual generalizability and robustness in the realm of 2D image recognition. Accordingly, there are also plenty of researches that focus on fine-tuning these VFMs in diverse downstream tasks, including adapters , prompt tuning , Low-Rank Adaptation (LoRA) , _etc_. Nonetheless, the mainstream efforts of PEFT primarily aim to improve parameter efficiency and mitigate overfitting, neglecting to learn a task-dependent classification head. As a result, efforts of tuning a linear classifier rely solely on the first-order features, _i.e_., LP, suffering from inferior performance. To address this, Gao _et al_.  propose a novel Moment Probing (MP) method, which firstly leverages the second-order features that are rich in statistical information for PEFT. The second-order features used in MP are the covariance representation of the first-order features extracted by a single branch. In contrast, the second-order covariance matrix used in the proposed CFP stage comes from first-orderfeatures extracted by two different branches. Note that both CFP and MP make use of second-order features, but they have different theoretical bases and excel in different tasks.

## 3 Method

The overall EMVP pipeline is illustrated in Figure 2, which can be further divided into the recalibration and Centroid-Free Probing (CFP) stages. The idea of CFP originates from NetVLAD and facilitates a more effective adaptation of VFMs to VPR tasks, with details elaborated in Section 3.1 and 3.2. Additionally, we propose the Dynamic Power Normalization (DPN) module in Section 3.3, and incorporate it into both the recalibration and CFP stages to enhance fine-tuning performance, thereby making the features extracted by the backbone network more task-specific.

### Preliminaries

Global aggregation is crucial for robust and accurate visual place recognition, which aggregates local descriptors into a fixed-size global one. NetVLAD  is one of the most popular global aggregations based on the bag-of-visual-words  theory. Specifically, NetVLAD produces a statistic embedding for each semantic centroid, which represents the sum of distances from the semantic centroid to the assigned local descriptors as follows:

\[V_{k}=_{i=1}^{L}p_{ik}(X_{i}-C_{k}),\] (1)

where \(C_{k}\) and \(V_{k}\) represent the embedding and the statistic embedding of the \(k\)-th semantic centroid, respectively. \(L\) denotes the number of local descriptors in an image, and \(X_{i}\) represents the \(i\)-th local descriptor. \(K\) is the number of total semantic centroids. \(p_{ik}\) indicates the probability that \(X_{i}\) is assigned to \(C_{k}\), which can be predicted by linear layers. The calculation of global descriptor can be summarized as:

\[}=NetVLAD(,)=cat(,dim=0),\] (2)

where \(=\{X_{1},X_{2},,X_{L}\}^{L M}\) represents all local descriptors from an image. \(}\) is the global descriptor for visual place recognition, which is obtained by concatenating \(=\{V_{1},V_{2},,V_{K}\}^{K M}\).

Note that the semantic centroids \(=\{C_{1},C_{2},,C_{K}\}^{K M}\) are also trainable, and the initialization of \(\) depends on an offline clustering process: Initially, an off-the-shelf backbone model is used to extract local descriptors from each image, involving a costly iteration through a pre-collected image set. Subsequently, \(k\)-means clustering is applied to these descriptors to obtain the semantic centroids \(\). However, the offline clustering process overlooks an issue: both the pre-collected image set and off-the-shelf backbone model should be compatible with the training set and trained VPR model. In other words, the fine-tuning performance is sensitive to the initialization process . Therefore, it motivates us to explore methods to avoid the explicitly computation of the semantic centroids and further stimulate the potential of VFM in the field of VPR.

Figure 2: Overall pipeline of the proposed EMVP, including recalibration and CFP stages. Feature matrices from the two branches (_i.e._, \(_{C}\) and \(_{P}\)) are multiplied to obtain fine-grained features for the improved VPR performance. The Dynamic Power Normalization (DPN) layer can be inserted into both the recalibration and CFP stages to enhance the task-specific fine-tuning performance.

### Centroid-Free Probing

As discussed by , the NetVLAD operation can be written as a bilinear pooling model. Specifically, different from the typical centroid-wise calculation pipeline (_i.e._, Equation 1 and 2), it can be calculated by accumulating local-wise descriptors as follows:

\[ =NetVLAD(,)\] \[=_{i=1}^{L}[X_{i}-C_{1};X_{i}-C_{2};...;X_{i}-C_{K}] [,p_{i1},...p_{i1}}_{D};...;p_{iK},p_{iK},...p_{iK}] ,\] (3)

where \(P_{i}=[p_{i1},p_{i2},...,p_{iK}]\) and \(\) indicates element-wise product. Note that Equation 3 contains the semantic centroids \(\), which indicates that the corresponding costly and unstable offline initialization discussed in Section 3.1 is still needed.

We observed that since semantic centroids \(\) are shared when extracting global descriptors for different images, if the value of \(_{i=1}^{L}P_{i}\) can be guaranteed to be constant, then the term after the minus sign in Equation 4 can be treated as a negligible constant term. Thus, the explicit calculation of \(\) can be avoided during the training process. In this paper, we resort to post normalization methods (_e.g._, softmax and \(_{2}\) normalization) to constrain the value of \(_{i=1}^{L}P_{i}\) to be constant, which is referred as Constant Normalization (CN) hereafter. The above simplification is formulated as follows:

\[=_{i=1}^{L}X_{i}^{T} P_{i}=;C_{2};...;C _{K}]_{i=1}^{L}P_{i}.expand(K,D)}_{constant}.\] (4)

Therefore, we can represent the global descriptor as follows, by omitting the constant term:

\[=_{i=1}^{L}X_{i}^{T} P_{i}=^{T} _{C}()^{T}_{P}(),\] (5)

where the probability matrix \(=\{P_{1},P_{2},,P_{L}\}^{L K}\) represents the probability from \(L\) local descriptors to \(K\) semantic centroids. Note that, local descriptors \(\) are fed to linear layers \(_{C}\) and \(_{P}\), which are designed for dimensionality reduction and computation of probability matrix \(\), respectively. In the context of fine-tuning VFMs, we refer to this simplified aggregation operation as Centroid-Free Probing (CFP), as illustrated in Figure 2. It implicitly leverages the priors brought by centroids through a bilinear design, while avoiding explicit centroid initialization and training.

### Dynamic Power Normalization in CFP and Recalibration

As VPR heavily relies on small overlapping regions to handle perspective changes, preserving information from these discriminative regions is essential. In this paper, we resort to post-processing methods to preserve task-specific information from these regions, thereby enhancing the robustness of second-order features. Extensive advanced studies [44; 14; 45] have verified that using the Matrix Power Normalization (MPN) method for feature post-processing after bilinear pooling can significantly improve downstream task performance. Typical MPN can be described as follows:

\[=\|sign()||^{}\|_{2},\] (6)

where \(sign()\) represents the sign of \(\), \(||\) indicates the absolute value of \(\), \(\) is a scalar, and \(\|\|_{2}\) denotes the \(_{2}\) normalization operation. MPN can effectively control the preservation of task-specific information during the training process by adjusting the value of \(\). In particular, as \( 0\), the normalized representation \(\) tends toward becoming an all-ones matrix. As \( 1\), information in \(\) will be gradually preserved . In the classic MPN method, the value of \(\) is predefined, and all images share the same value of \(\). It can even be deduced from the perspective of Maximum Likelihood Estimation (MLE) that the theoretically optimal value for \(\) is 0.5 .

Considering potential significant distribution differences between fine-tuning and inference datasets, in this paper, we lean towards \(\) being learnable based on the context adaptively. Specifically, we design the Dynamic Power Normalization (DPN) module in the CFP stage to compute the value of\(\) based on the value of \(\) as shown in Figure 3(a). Thus, each image can have a different level of task-specific information preservation, enabling more flexible fine-tuning.

With the same consideration, we attempt to use DPN to adaptively preserve task-specific information in recalibration for Parameter Efficiency Fine-Tuning (PEFT), while keeping the backbone network frozen. Given the unique nature of the VPR task: the background region in images (_e.g_., the building), which may be irrelevant in other tasks, serves as a crucial clue for place recognition. However, due to the difference in training objectives, a pre-trained VFM may overlook these background regions. Therefore, we employ the DPN to enhance the representation of distinctive background regions, and effectively leveraging the representation capabilities of a VFM while minimizing recalibration, as depicted in Figure 3 (b) and 3 (c).

## 4 Experiments

### Experimental Settings

**Choice of Visual Foundation Model.** To make effective use of large-scale unlabeled data, self-supervised transformers are becoming increasingly popular for training a VFM. Two self-supervised learning paradigms have demonstrated superior performance for ViT pre-training: contrastive learning-based (_e.g_., DINO ), and masked image modeling-based (_e.g_., MAE ). There are also multi-modal foundation models for robust feature extraction (_e.g_., CLIP ). Recent advanced research  demonstrated that ViTs pre-trained by the contrastive learning paradigm can produce more universal path-wise representations. In this paper, the ViT model pre-trained by DINOV2, a follow-up of DINO, is adopted as the VFM, which has been verified to have good performance in fine-grained tasks (_e.g_., depth estimation  and VPR ). Specifically, three VPR models are fine-tuned based on ViT-S, ViT-B, and ViT-L, named EMVP-S, EMVP-B, and EMVP-L, respectively.

**Implementation Details.** Both the \(_{C}\) and \(_{P}\) branches are implemented by a two-layer MLP network. For a fair comparison with SALAD, the output dimensions of \(_{C}\) and \(_{P}\) are \(128\) and \(64\), respectively. We employ the softmax operation to normalize the output features of \(_{P}\). We implement two versions of the \(_{}\) module (_i.e_., sequential and parallel \(_{}\)), as shown in Figure 3, for the recalibration of intermediate features. In the parallel version, the original feature is preserved through an independent branch, and updated context is aggregated via element-wise addition. In contrast, the sequential version is equivalent to adding a few extra layers to the backbone. For more implementation details, please refer to Appendix A.1.

**Datasets.** Many advanced efforts [33; 16; 12] have shown that models trained on the GSV-Cities  dataset exhibit strong generalization across various VPR datasets, such as MSLS Validation , Pittsburgh30k-test , Pittsburgh250k-test , Nordland [51; 52], and SPED . Following the training and validation protocol of these studies, we fine-tune the VPR model on the GSV-Cities

Figure 3: The DPN module can be placed in both CFP and recalibration stages, which is indicated by \(_{}\) and \(_{}\), respectively. More importantly,, it can be inserted into the Transformer blocks sequentially and parallelly.

dataset, which is pre-trained by the DINOv2 pipeline. Subsequently, Recall@K (_i.e_., R@1, R@5, and R@10) is evaluated across various VPR datasets as the performance metric.

**Model Selection.** We involve \(30\) epochs for fine-tuning models, and select the one with the highest R@1 on the Pittsburgh30k-val dataset for further evaluation on other test datasets. To fairly compare model performance, we repeat the aforementioned process \(5\) times and calculate the average metrics as the final test results.

### Main Results

We report the performance comparison with SOTA in Table 1 and analyze the results as follows. First, in typical VPR methods, a re-ranking stage is typically incorporated for retrieved images to enhance the final performance as post-processing. This is primarily due to the inherent noise in the training dataset, which is subject to changes in image perspectives, seasonal variations, occlusion, and other factors. We illustrate the comparison through the TransVPR algorithm with or without the re-ranking stage as an example. Second, supported by the high-quality GSV-Cities dataset, methods (_e.g_., MixVPR) without re-ranking achieve comparable performance to those with re-ranking stages. Third, with the further support of DINOv2, the algorithms without re-ranking achieve new levels of accuracy and robustness, as evidenced by the performance of EMVP, SelaVPR (global) and SALAD. Finally, EMVP-L obtains the best performance by leveraging the probing method tailored for the VPR task. Taking the typical MSLS Validation dataset as an example, EMVP-L even outperforms the full fine-tuning method SALAD by 1.7% at Recall@1. Additional visualization in Figure 4 shows that the VPR model fine-tuned by EMVP-B successfully finds the closest match in challenging scenarios, including occlusion, illumination change, perspective change, and seasonal variation. For more comparisons with SOTA methods, please refer to Appendix A.1.

    &  &  &  &  \\  & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\  SPE-VLAD\({}^{}\) & 78.2 & 86.8 & 88.8 & 25.5 & 40.1 & 46.1 & 89.9 & 96.1 & 97.3 & 73.1 & 85.5 & 88.7 \\ Gated NetVLAD\({}^{}\) & 82.0 & 88.9 & 91.4 & 34.4 & 50.4 & 57.7 & 89.7 & 95.9 & 97.1 & 75.6 & 87.1 & 90.8 \\ NetVLAD\({}^{}\) & 82.6 & 89.6 & 92.0 & 32.6 & 47.1 & 53.3 & 90.5 & 96.2 & 97.4 & 78.7 & 88.3 & 91.4 \\ Conv-AP\({}^{}\) & 83.4 & 90.5 & 92.3 & 38.2 & 54.8 & 61.2 & 92.4 & 97.4 & 98.4 & 80.1 & 90.3 & 93.6 \\ CosPlace\({}^{}\) & 83.0 & 89.9 & 91.8 & 34.4 & 49.9 & 56.5 & 91.5 & 96.9 & 97.9 & 75.3 & 85.9 & 88.6 \\ MixVPR\({}^{}\) & 88.0 & 92.7 & 94.6 & 58.4 & 74.6 & 80.0 & 94.6 & 98.3 & 99.0 & 85.2 & 92.1 & 94.6 \\ EigenPlaces  & 89.3 & 93.7 & 95.0 & 54.4 & 68.8 & 74.1 & 94.1 & 98.0 & 98.7 & 69.9 & 82.9 & 87.6 \\ SALAD\({}^{}\) & 92.2 & 96.4 & 97.0 & 76.0 & 89.2 & 92.0 & 95.1 & 98.5 & 99.1 & 92.1 & 96.2 & 96.5 \\
**EMVP-L\({}^{}\) (Ours)** & **93.9** & **97.3** & **97.6** & **78.4** & **89.7** & **92.4** & **96.5** & **99.1** & **99.5** & **94.6** & **97.5** & **98.4** \\    (b) Comparison with two-stage methods, which include a re-ranking stage indicated by \({}^{}\).

    &  &  &  \\  & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\  SP-SuperGlue\({}^{}\) & 78.1 & 81.9 & 84.3 & 25.8 & 35.4 & 38.2 & 87.2 & 94.8 & 96.4 \\ Patch NetVLAD\({}^{}\) & 79.5 & 86.2 & 87.7 & 51.6 & 60.1 & 62.8 & 88.7 & 94.5 & 95.9 \\ DELG\({}^{}\) & 83.2 & 90.0 & 91.1 & 51.3 & 66.8 & 69.8 & 89.9 & 95.4 & 96.7 \\ TransVPR\({}^{}\) & 86.8 & 91.2 & 92.4 & 58.8 & 75.0 & 78.7 & 89.0 & 94.9 & 96.2 \\ R2Former\({}^{}\) & 89.7 & 95.0 & 96.2 & 77.0 & 89.0 & 91.9 & 91.1 & 95.2 & 96.3 \\ SeaVPR\({}^{}\) & 90.8 & 96.4 & 97.2 & 85.2 & 95.5 & 98.5 & 92.8 & 96.8 & 97.7 \\  TransVPR w/o re-ranking  & 70.8 & 85.1 & 89.6 & 15.9 & 38.6 & 49.4 & 73.8 & 88.1 & 91.9 \\ SelaVPR (gobal)  & 87.7 & 95.8 & 96.6 & 72.3 & 89.4 & 94.4 & 90.2 & 96.1 & 97.1 \\
**EMVP-L\({}^{}\) (Ours)** & **93.9** & **97.3** & **97.6** & **88.7** & **97.3** & **99.3** & **94.0** & **97.5** & **98.2** \\   

Table 1: Comparison with state-of-the-art methods. \({}^{}\) denotes models trained on the GSV-Cities dataset. Due to the high quality of annotations in GSV-Cities, results from models marked with \({}^{}\) generally outperform those from their corresponding papers. In contrast, results from models without \({}^{}\) are reported in their respective papers.

### Ablation Studies

**The Impact of Different Probings.** We compared probing methods based on first-order and second-order features. Among the first-order methods, the popular LP in classification and GeM pooling in VPR tasks are explored. For second-order feature methods, we dive into the pioneering second-order method MP in classification and the proposed CFP. Additionally, the _baseline_ can be seen as a form of bilinear pooling with the centroids removed, and it is implemented by removing the optimal transport operation in the SALAD code base.

As shown in Table 2, the test results of LP revealed a significant decrease in performance compared to aggregation based on CFP. One possible reason for this could be the common trick used in LP-based methods, where average pooling is applied to local descriptors to obtain a fixed-size global one. This first-order statistical features extraction may lead to information loss. GeM pooling generalizes average and max pooling, as a result, its accuracy is still fundamentally limited by first-order features.

Despite that MP has achieved excellent accuracy metrics in classification, its performance in VPR tasks is inferior to CFP, as shown in Table 2. This is mainly due to CFP implicitly leveraging the priors provided by semantic centroids. By comparing results of baseline and NetVLAD (ID 2), we find that directly removing centroids using bilinear pooling leads to a performance drop. This is why SALAD employs optimal transport to improve the performance. In contrast, this paper introduce CN and \(_{}\) to theoretically refine the simplification of NetVLAD, achieving improved performance.

It is noteworthy that advanced methods such as TransVPR and MixVPR, employing heavy Transformer and MLP-Mixer aggregation architectures, have demonstrated excellent performance in VPR tasks. However, these methods do not quite align with the current research paradigm of fine-tuning VFMs based on shallow trainable MLP architectures.

**The Impact of the Reinterpretation with Constant Normalization.** By comparing the experimental results of ID 2 and ID 10 in Table 2, we can verify that our reinterpretation for NetVLAD has led to improved performance. This is mainly attributed to the elimination of the explicit learning of cluster centers, which has reduced parameter number and mitigated the impact of imprecise initialization. It is worth noting that increasing the feature dimension of NetVLAD can significantly enhance performance. However, it is essential to consider the potential cost when dealing with the storage of images with sizable global descriptors, particularly in the context of VPR applications.

Comparisons between ID 7 and ID 8, ID 7 and ID 9 in Table 2 demonstrate that the CN makes this reinterpretation operation empirically more robust, and this can be validated by achieving improved performance. Through comparing ID 8 and ID 9 in Table 2, we can observe that the improvement brought by CN is dependent on its specific implementation. Further exploration of this aspect will be undertaken in our subsequent research, extending beyond the scope of this paper.

Figure 4: Query (gray) and top \(3\) retrieved frames (green: successful, red: failed). Moreover, one of the true (blue) matches is displayed for comparison.

The Impact of the Foundation Model.The comparative results between ID 1 and ID 3 in Table 2 indicate that, when the size of the global descriptor is within the same order of magnitude, the VFM (_i.e._, DINov2) is more suitable as the backbone network for VPR tasks compared to traditional CNN models. Even in the scenario of zero-shot inference, the VPR model based on DINov2 demonstrates strong generalization and robustness (refer to AnyLoc in Table 3). However, it is important to note that the performance of zero-shot inference still exhibits a substantial performance gap in comparison to methods that are based on fine-tuning. This motivates us to explore a more effective fine-tuning pipeline tailored for VPR tasks. Additionally, the results in Table 10 of Appendix A.2 indicate that as the scale of ViTs increases, the performance of VPR models fine-tuned by EMVP can be improved.

Comparison of Fine-tuning Methods.In Table 3, different fine-tuning approaches are compared. By comparing SALAD and AnyLoc, we can conclude that current VFMs (_i.e._, DINov2) lack sufficient zero-shot reasoning capabilities for diverse data in the VPR domain. SALAD achieves high performance by fully fine-tuning on DINov2, but VPR models are typically deployed on mobile robots, and this full-parameter update approach imposes the higher demands on communication. Therefore, we attempt to study adaptation methods more suitable for VPR tasks.

For fairness, this paper reimplements the advanced PEFT method PSRP , which is also aimed at cooperating with second-order features, on the VPR dataset to compare it with our proposed \(}\). Furthermore, as illustrated in Figure 3, the \(}\) module can be further divided into parallel and sequential versions for comparative research. By further comparing the sequential and parallel \(}\) modules in Table 3, we find that the sequential version performs better. This is primarily because the sequential method recalibrates the backbone features more thoroughly, and it does not significantly increase training difficulty since a few additional parameters are introduced. The sequential configuration of the \(}\) method is used by default in other experiments in this paper. Compared with methods such as SALAD and PSRP, the sequential \(}\) outperforms them by achieving the best performance while saving 64.3% of trainable parameters (0.14M _vs_ 0.05M). We further visualize the impact of \(}\) on recalibration in Appendix A.3.

## 5 Conclusion

In this paper, we have proposed a novel fine-tuning pipeline named EMVP, which involves reinterpreting the classical aggregation (_i.e._, NetVLAD) into a CFP stage when fine-tuning a VFM for accurate VPR. What is more innovative, both the recalibration and CFP stages employ the same

    &  &  &  &  &  &  &  \\   & & & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) \\   & NoVAD & 2788 & RoNNc & 82.6 & 96.2 & 92.0 & 32.6 & 47.3 & 53.3 & 90.5 & 96.2 & 97.4 & 76.7 & 95.5 & 96.4 & 96.7 \\  & 2 & NetVLAD & 3798 & 91.4 & 95.6 & 96.3 & 70.1 & 86.5 & 90.2 & 96.4 & 98.4 & 99.1 & 90.8 & 95.4 & 96.7 \\  & NeVLAD & 24575 & VRP & 92.2 & 95.9 & 96.4 & 71.8 & 86.3 & 90.1 & 95.6 & **97.8** & **97.3** & 90.8 & 92.9 & 92.7 \\  & 4 & 7688-266 & VRP & 83.3 & 93.5 & 95.4 & 38.1 & 85.3 & 61.8 & 91.1 & 93.6 & 96.1 & 83.0 & 92.5 & 92.6 \\  & 5 & MP & 8045 & VRP & 83.4 & 95.4 & 96.4 & 42.6 & 62.0 & 70.0 & 92.5 & 97.3 & 96.5 & 85.2 & 92.5 & 94.6 \\  & 6 & GM & 4076 & VRP & 85.4 & 93.9 & 95.0 & 35.4 & 52.5 & 59.6 & 99.5 & 96.5 & 98.0 & 93.0 & 92.1 & 93.9 \\  & 7 & Random & 8192-246 & VRP & 80.3 & 95.7 & 95.4 & 86.5 & 73.0 & 75.6 & 94.4 & 96.4 & 97.1 & 88.0 & 94.7 & 95.6 \\  & 8 & + Cross-Softmax & - & - & 91.3 & 95.7 & 96.4 & 68.0 & 82.0 & 86.2 & 94.9 & 95.3 & 99.8 & 99.3 & 94.9 & 96.4 \\  & 9 & + CNN / 6, non- & - & - & 90.5 & 92.5 & 96.6 & 64.4 & 80.9 & 84.5 & 94.5 & 98.1 & 99.0 & 89.0 & 95.9 & 95.7 \\  & 10 & + DFNc (_i.e._, CFP) & - & - & - & **92.6** & **96.2** & 96.8 & **74.6** & **87.6** & **91.3** & 95.2 & **98.7** & **99.3** & **92.1** & **95.0** & **97.2** \\   

Table 2: Comparing different backbones and probings. LP, MP, CFP, CN, and \(}\) indicate linear probing, moment probing, centroid-free probing, constant normalization, and dynamic power normalization in probing, respectively. For fairness, results produced by ViT-based models are obtained by fully fine-tuning the last \(4\) blocks. _Baseline_ refers to the simplified NetVLAD adapted by SALAD. The best and the second best results are **bolded** and underlined, respectively.

    & Fine-tuning &  &  &  &  &  \\   & & Type & (M) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) & R\(\) \\  AnyLoc & Zero-shot & & 68.7 & 78.2 & 81.8 & 16.1 & 82.5 & 30.4 & 87.2 & 94.4 & 96.5 & 83.2 & 94.4 & 95.4 \\ SALAD & Full & 27.1 & 92.2 & 96.4 & 97.0 & 16.0 & **89.2** & 92.0 & 95.1 & 98.5 & 99.1 & **92.1** & 96.2 & 96.5 \\ CPP & +PSRP & 0.14 & 92.7 & 96.6 & 96.3 & 73.0 & 86.5 & 89.3 & 95.3 & 98.6 & 99.2 & 91.3 & 95.9 & 96.2 \\  & + DFNc(_para.) & **0.05** & 92.4 & 96.5 & 96.6 & 71.8 & 85.2 & 88.9 & 95.4 & 98.5 & 99.1 & 91.3 & 96.2 & 96.7 \\ (_a.e._, **EMVP-B)** & + DFNc(_para.) & **0.05** & **93.2** & **96.9** & **97.2** & **76.4** & **85.8** & **92.1** & **95.7** & **98.9** & **99.3** & 91.8 & **96.5** & **97.4** \\   

Table 3: Comparing different fine-tuning methods. \(}\) and \(}\) indicate DPN in CFP and recalibration, respectively. Results of both parallel and sequential versions of \(}\) are reported. For fairness, only the last \(4\) blocks can be fine-tuned, and all methods employ the same backbone, _i.e._, ViT-B. The best and the second best results are **bolded** and underlined, respectively.

DPN module for task-specific information preservation, effectively conducting PEFT. Extensive experiments conducted on VPR datasets have demonstrated that EMVP can extract more task-specific features, resulting in enhanced accuracy and robustness in VPR performance.

**Broader Impacts.** This work enhances the safety and efficiency of mobile robots operating in GPS-denied environments. In addition, the potential negative impact is that bad weather and ambiguous scenes will cause certain interference, making it difficult to maintain a high level of accuracy in VPR.

**Limitations.** Based on the discussions and comparisons of VFMs such as CLIP, SAM, DINO, and DINOv2 in pioneering works [48; 34; 58], this paper prioritizes DINOv2 as the VFM for experimental analysis. Note that, different VFMs possess distinct capabilities. For example, CLIP can connect images with texts for better interpretability, while SAM demonstrates powerful abilities in handling visual prompts. In the future work, we aim to explore these capabilities in different VPR tasks.