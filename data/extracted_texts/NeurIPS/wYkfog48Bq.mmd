# Optimal Block-wise Asymmetric Graph Construction

for Graph-based Semi-supervised Learning

Zixing Song

The Chinese University of Hong Kong

New Territories, Hong Kong SAR

zxsong@cse.cuhk.edu.hk

&Yifei Zhang

The Chinese University of Hong Kong

New Territories, Hong Kong SAR

yfzhang@cse.cuhk.edu.hk

&Irwin King

The Chinese University of Hong Kong

New Territories, Hong Kong SAR

king@cse.cuhk.edu.hk

###### Abstract

Graph-based semi-supervised learning (GSSL) serves as a powerful tool to model the underlying manifold structures of samples in high-dimensional spaces. It involves two phases: constructing an affinity graph from available data and inferring labels for unlabeled nodes on this graph. While numerous algorithms have been developed for label inference, the crucial graph construction phase has received comparatively less attention, despite its significant influence on the subsequent phase. In this paper, we present an optimal asymmetric graph structure for the label inference phase with theoretical motivations. Unlike existing graph construction methods, we differentiate the distinct roles that labeled nodes and unlabeled nodes could play. Accordingly, we design an efficient block-wise graph learning algorithm with a global convergence guarantee. Other benefits induced by our method, such as enhanced robustness to noisy node features, are explored as well. Finally, we perform extensive experiments on synthetic and real-world datasets to demonstrate its superiority to the state-of-the-art graph construction methods in GSSL.

## 1 Introduction

Graph-based semi-supervised learning (GSSL) is a burgeoning research field . As a subclass of semi-supervised learning (SSL), GSSL exhibits promise since it encapsulates the smoothness or manifold assumption, where samples with similar features are likely to share the same label. GSSL methods begin by constructing an affinity graph, wherein nodes represent samples, and weighted edges denote similarity between pairs of nodes. This process aligns with the manifold assumption, implying that nodes connected by edges with large weights tend to have the same label. Upon obtaining the affinity graph, various label inference algorithms such as label propagation  can be executed to predict labels for the unlabeled nodes.

Preliminary empirical studies  suggest that the quality of the affinity graph significantly influences label prediction performance. However, the graph construction phase in GSSL has received less scrutiny compared to the subsequent label inference phase. Constructing a high-quality graph presents a challenge as its quality can only be assessed indirectly through postmortem verification via label inference performance. Classic solutions such as the Radial Basis Function (RBF) Kernel , kNN graph , and \(b\)-matching , while simple, may exhibit low robustness against noise due to their simplicity. More recent and complex methods, typically framed as optimization problems to findthe optimal graph under the smoothness assumption, suffer from inefficient optimization techniques without fast convergence guarantees [18; 29; 63] or lack a solid theoretical foundation [58; 14; 23].

Consequently, a series of research questions arise: _1) What is the optimal graph structure for label inference algorithms in GSSL? 2) How can we optimize this optimal graph efficiently? 3) What kinds of benefits can this optimal graph bring?_

Most existing GSSL graph construction methods treat all nodes equally, disregarding label information, which results in a symmetric, undirected graph. However, we contend that an asymmetric, directed graph structure might be more beneficial for subsequent label inference due to the distinct roles labeled and unlabeled nodes can play. Intuitively, edges from labeled to unlabeled nodes would naturally facilitate supervision information propagation, while edges from unlabeled to labeled nodes may introduce inconsistency, potentially undermining the prediction accuracy for the labeled nodes.

Motivated by this key observation, we revisit a unified optimization framework that encompasses most label inference algorithms, assuming the affinity graph is already constructed. We then fix the downstream label inference result and position the graph weight matrix (or adjacency matrix) as the optimization variable. This shift allows us to define optimality in the graph construction step of GSSL concisely, addressing our first research question. We subsequently present a tailored optimal solution featuring an asymmetric graph structure that aligns with our proposed intuition. In response, we introduce the **B**lock-wise **A**ffinity **G**raph **L**earning (BAGL) algorithm by leveraging duality and the fast proximal gradient method, addressing our second research question. Finally, we demonstrate that BAGL ensures a sub-linear global convergence rate of \(O(1/k)\) and can alleviate issues of noisy node features, addressing our third research question.

In summary, this work offers four main contributions. First, we provide a succinct definition of the optimality of the affinity graph in GSSL, and through rigorous derivation, propose an ad-hoc solution with an asymmetric structure. Second, we design a block-wise graph learning framework, BAGL, to infer the weights in the optimal graph structure. Third, we prove that a global sub-linear convergence rate is guaranteed for BAGL and analyze other benefits. Fourth, we perform extensive experiments on synthetic and real-world datasets to demonstrate the effectiveness and efficiency of BAGL.

## 2 Preliminary

### Problem Formulation

We present a formulation for the GSSL problem, comprising two steps: graph construction and label inference . Our study primarily investigates the optimal construction of the affinity graph in the first phase to facilitate enhanced performance in the second label inference phase.

Given a set of data points \(\{_{i}\}_{i=1}^{n}\), where each \(_{i}^{d}\) is sampled from a \(d\) dimensional feature space. In this paper, we interchangeably use the terms _node_, _point_, and _sample_ to refer to \(_{i}\). Each sample \(_{i}\) has a label \(y_{i}_{c}\), where \(_{c}=\{i^{+} 1 i c\}\) with \(c\) being the number of classes. Given the labels of the \(l\) samples \(\{_{i}\}_{i=1}^{l}\) as \(\{y_{i}\}_{i=1}^{l}\), the ultimate goal of general transductive SSL is to infer the labels \(\{y_{i}\}_{i=l+1}^{l+u}\) for the remaining \(u\) unlabeled samples \(\{_{i}\}_{i=l+1}^{l+u}\) (\(n=l+u\)). As a subcategory of general SSL, GSSL methods first construct a graph \(=\{,,\}\) based on all the training samples \(\{_{i}\}_{i=1}^{n}\) and partially given labels \(\{y_{i}\}_{i=1}^{l}\). Here, \(\) is the node set with \(||=n\). Each node represents each sample \(_{i}\). \(\) is the edge set in which each edge \((i,j)\) is assigned with a weight \(W_{ij}\) (the \(i\)-th row, \(j\)-th column entry in \(^{n n}\)) to reflect the affinity or similarity between the sample pair \((_{i},_{j})\). Generally, a larger weight indicates a higher level of similarity. \(W_{ij}=0\) indicates no edge between node \(i\) and node \(j\). Therefore, the key challenge in the first graph construction step is to generate \(\) based on \(\{_{i}\}_{i=1}^{n}\) and \(\{y_{i}\}_{i=1}^{l}\) so that the underlying manifold of the data is properly encoded. In the second step, various label inference algorithms can be performed on \(\) to propagate the given labels \(\{y_{i}\}_{i=1}^{l}\) and make predictions \(\{y_{i}\}_{i=l+1}^{l+u}\) for unlabeled nodes.

However, the performance of the label inference step is significantly contingent on the quality of the weight matrix \(\) from the first graph construction step. This paper aims to address three critical research questions pertaining to the graph construction step in GSSL. First, what constitutes the overall structure of the optimal weight matrix, \(^{*}\)? (Sect. 3.1). We define the optimal weight matrix, \(^{*}\), as the one that gives the best prediction results when used with the same label inference method across different graphs \(\). Second, how to find an efficient method for optimizing the entries in \(^{*}\)? (Sect. 3.2). Third, what kinds of benefits can \(^{*}\) bring from the theoretical perspective? (Sect. 3.3)

### Recap on the Unified Framework for Label Inference Step in GSSL

Before we delve into the proposed structure for the optimal affinity graph, we first revisit the unified framework of the label inference step, allowing for a seamless definition of the optimal affinity graph.

If we assume the affinity graph has been constructed, then numerous influential label inference methods [85; 82; 83; 4; 5; 9] can be performed over this graph to infer the labels for the unlabeled nodes. However, the majority of them can be incorporated into the following framework.

We first define the node feature matrix \(^{n d}\) as \(=[_{1},,_{n}]^{}\) and the predicted soft label matrix \(^{n c}\) as \(=[_{1},,_{n}]^{}\) with \(_{i}^{c}(1 i n)\). Ground-truth label matrix is given as \(=[_{1},,_{n}]^{}\{0,1\}^{n c}\) with \(_{i}\{0,1\}^{c}\) (\(1 i n\)). Here, for the labeled nodes \(\{_{i}\}_{i=1}^{l}\), \(_{i}\) is a one-hot vector in which \(Y_{ij}=1\) if \(_{i}\) belongs to class \(j\) (\(y_{i}=j\)), and \(Y_{ij}=0\), otherwise. For the unlabeled nodes \(\{_{i}\}_{i=l+1}^{l+u}\), \(_{i}\) is an all-zero vector for initialization.

Driven by the geometry of the affinity graph, we can unify these label inference algorithms as a minimizer of the optimization problem as Problem (1).

\[^{*}=*{arg\,min}_{}Q()=*{arg\, min}_{}\{(^{} )+((-)^{}(-)).\] (1)

Note that the loss function \(Q()\) consists of a quadratic variation term \((^{})\) as the graph smoothness regularizer, and a quadratic Frobenius error norm \(\|-\|_{F}^{2}=((-)^{}(-))\), both of which should ideally be small subject to a trade-off parameter \(\) between them. Here the smoothing matrix \(=s()_{+}^{n}\) in the first term, with \(s^{n n}_{+}^{n n}\), is positive semidefinite and determined by the weight matrix \(\) of the graph to ensure the adjacent nodes share similar predictions. The second term measures the distance between predicted results and initial assignments, and restricts the output for labeled nodes from deviating too much compared with the ground-truth. \(\) is a diagonal matrix with \(_{ii} 0\) and \(+\) must be invertible to avoid trivial solutions.

By the first-order optimality condition, we can easily obtain the optimal solution for Problem (1).

\[^{*}=(+)^{-1}=(s()+)^{-1}.\] (2)

Based on the optimal soft label matrix \(^{*}\), the final predicted label for each node is given as \(_{i}=(_{i})=*{arg\,max}_{1 j c}F_{ij}^ {*}\). It is worth noting that the unified optimization framework in Problem (1) can admit most of the mainstream GSSL methods. For instance, if we set the smoothing matrix as the normalized graph Laplacian matrix \(=}=s()=^{-}(-)^{ -}\) and \(=\), we can easily recover one of the most popular label inference methods for GSSL, Local and Global Consistency (LGC) , as \(^{*}=*{arg\,min}_{}\{_{i,j=1}^{n} \|_{i}}{}}-_{j}}{}} \|_{2}^{2}W_{ij}+_{i=1}^{n}\|_{i}-_{i}\| _{2}^{2}\}\). Here, \(}_{+}^{n}\) is defined as \(}=^{-}^{-}\), where the combinatorial Laplacian matrix \(_{+}^{n}\) is given as \(=-\), and the degree matrix \(^{n n}\) is a diagonal matrix defined as \(D_{ii}=_{j=1}^{n}W_{ij}\). We summarize several representative works under this unified framework in Table 5 with explicit forms of \(\) (or \(s()\)) and \(\) in Appendix B.2.

## 3 Methodology

### Motivation: Optimal Affinity Graph Structure

#### 3.1.1 Definition of the Optimality of the Affinity Graph

If we perform the same label inference algorithm from the above-mentioned generalized framework on all possible affinity graphs \(\), the optimal graph \(^{*}\) will enable the label inference step to obtain the most accurate predictions. Under the above-mentioned unified framework for label inference (Problem (1)), the weight matrix \(\) is fixed while the soft label matrix \(\) is the optimization variable. This is due to our goal of executing label inference to attain the optimal \(^{*}\) given the affinity graph \(\). Similarly, when we want to construct the optimal graph \(^{*}\) given the label inference framework,we consider the weight matrix \(\) as the optimization variable, keeping the soft label matrix \(\) fixed as the solution in Eq. (2). The form of \(Q()\) remains unchanged as it is related to the generalization bound for GSSL that would be discussed in Appendix D.2. This approach aids in identifying the "optimal" affinity graph for the label inference algorithms under the unified framework.

**Definition 1** (Optimality of the Affinity Graph).: _The affinity graph \(\) is optimal for label inference under the unified GSSL framework if its weight matrix \(\) is the minimizer of Problem (3)._

\[_{^{n n}}\{(^{ }s())+((-)^{}(-))\}=(s()+)^{-1},\] (3)

Although the optimization Problem (3) is intractable in general due to the various forms of \(s()\) in the constraint, we can present an ad-hoc optimal structure of \(^{*}\) that is independent of \(s()\), which motivates our proposed method in Sect. 3.2 from a theoretical perspective.

#### 3.1.2 Structure of the Optimal Affinity Graph

We then present an equivalent proposition in Theorem 1, which provides a necessary and sufficient condition for the optimality of \(^{*}\) in Problem (3). Motivated by some classic graph sharpening techniques , Theorem 1 helps to circumvent the challenges of directly dealing with Problem (3) and holds regardless of the forms of \(s()\) (Appendix E.1).

**Theorem 1**.: \(^{*}\) _obtained by Definition 1 is optimal if and only if \(_{l}=_{l}\) holds. \(_{l}\{0,1\}^{l c}\), \(_{l}^{l c}\) are the ground-truth label matrix, and the soft label matrix for labeled nodes._

To put it in a simpler way, Theorem 1 tells us that if the affinity graph is optimal, then after we perform the label inference algorithm for GSSL, the predicted soft label for the labeled nodes would coincide with the ground truth precisely. The converse of this observation also holds true. Unfortunately, it remains an open question to solve \(_{l}=_{l}\) by listing all possible classes of solutions due to the complex interconnection of \(\) and \(\). However, we can provide a simple ad-hoc solution for \(_{l}=_{l}\) in Proposition 1. For one thing, this asymmetric graph structure, in the theoretical sense, conforms to the intuition of the better affinity graph we discussed earlier. For another, it also sheds some light on the proposed optimization framework to infer the weights in this graph structure later.

**Proposition 1**.: _If \(^{*}\) can be expressed as (4), \(^{*}\) is an optimal solution given by (3) in Definition 1._

\[^{*}=(&\\ _{ul}&_{uu}),\] (4)

_where \(_{ul}^{u l}\), and \(_{uu}^{u u}\) can be non-zero submatrices with arbitrary entries._

Proposition 1 provides an ad-hoc solution to Problem (3) (Appendix E.2). If the constructed graph is asymmetrical with edges from unlabeled nodes to labeled nodes only, then it is optimal by Definition 1. This asymmetrical optimal graph structure answers the first research question in Sect. 2.1.

The optimal asymmetric graph structure presented in Proposition 1 offers meaningful interpretations and notable advantages. It effectively eliminates the influence from unlabeled nodes to labeled nodes by enforcing \(_{ll}\) and \(_{lu}\) to be zero matrices. This aligns with the intuition that label information should be propagated from labeled nodes to unlabeled nodes in the GSSL methods, rather than the other way around. More technically, when applying the classic label inference algorithm LGC  on this optimal graph structure, the soft label matrix for unlabeled nodes now becomes \(_{u}=(-_{uu})^{-1}_{ul}_{L}\) with some constant \(0 1\). This formulation indicates that the LGC algorithm spreads supervision information from labeled nodes to unlabeled nodes once through \(_{ul}_{l}\), followed by propagating this information solely among unlabeled nodes through \((-_{uu})^{-1}=+_{uu}+^{2}_{uu}^{2}+\). By Theorem 1, this optimal asymmetric graph guarantees zero empirical risk on the labeled nodes. Moreover, many existing graph construction methods in GSSL primarily focus on node features while disregarding label information. Consequently, these methods may produce heterophilous edges that connect nodes with similar features but different labels. Such edges violate the manifold assumption in GSSL, where nodes with the same label tend to be linked. During the label inference step, the label information of these nodes connected by heterophilous edges confuses each other during propagation, resulting in misleading predictions. By setting \(_{ll}=\), our method eliminates these heterophilous edges completely. As a result, it increases the edge homophily ratio of the constructed graph and enhances the robustness of subsequent label inference algorithms, as validated in Appendix D.1.

### Implementation: Block-wise Graph Learning Algorithm

#### 3.2.1 Framework

By differentiating the roles that labeled and unlabeled nodes would play, we arrive at an optimal structure for the affinity graph in Proposition 1. However, an efficient algorithm to infer the exact weights or entries for blocks in Eq. (4) is still in demand. Motivated by previous works [29; 30; 19], we make the following reasonable restrictions or assumptions on \(\) to obtain a more meaningful graph for GSSL. First, the features for two directly connected nodes should not vary too much, no matter whether they are labeled or not. This agrees with the manifold assumption in GSSL, where links tend to form between similar nodes. Violation of this fundamental assumption usually causes a significant performance drop in the label inference step. Second, the constructed graph should be well-connected in terms of both \(_{}\) and \(_{}\) to ensure the supervision information could propagate freely among all nodes. Otherwise, some disconnected nodes may never receive any label information. Third, it is desirable to control the sparsity of the graph, avoiding an overly sparse or overly dense graph. We will empirically demonstrate the effects of the sparsity control later. We propose two similar optimization frameworks for \(_{}\) and \(_{}\) in Eq. (4) as Problem (5) and (6).

\[_{_{}}\|_{}_{ }\|_{1}-_{1}^{}(_{})-_{1}^{}(_{})+_{1}\| _{}\|_{F}^{2},\,_{} 0.\] (5) \[_{_{}}\|_{}_{ }\|_{1}-_{2}^{}(_{})-_{2}^{}(_{}^{})+ _{2}\|_{}\|_{F}^{2},_{} 0.\] (6)

Here, we define the pairwise distance matrix \(=(_{}&_{}^{ }\\ _{}&_{})_{+}^{n  n}\) with \(Z_{ij}=\|_{i}-_{j}\|_{2}^{2}\).

\(\) is the Hadamard product and \(()\) is the element-wise logarithm operator. Since Problem (5) and (6) share the same form, we will only focus on Problem (5) as an example. The first term \(\|_{}_{}\|_{1}=_{1 i u,1  j l}W_{ij}\|_{i}-_{j}\|_{2}^{2}\) encourages similar nodes to be connected with larger weights, meeting the manifold assumption. The second and third logarithmic barrier term act on the out-degree and in-degree vectors to make sure that each unlabeled node is connected by at least one labeled node and vice versa, improving the overall connectivity of the graph. The last Frobenius norm term measures the sparsity of the graph. The parameters \(_{1},_{2},_{1},_{2}\) are positive.

#### 3.2.2 Optimization

For convenience of presentation, we view the entries in \(_{}\) (\(_{}\)) as a new vector \(=[_{}]_{+}^{}\). Similarly, we have \(=[_{}]_{+}^{}\). Accordingly, a linear mapping matrix \(_{1}\{0,1\}^{}\) transforms the edge weights to the corresponding out-degree vector (i.e. \(_{1}=_{}\)). Similarly, we have \(_{2}=_{}^{}\) for the in-degree vector. Further, if we let \(^{}=(_{1}^{},_{2}^{})\{0,1\}^ {n}\), \(=_{1}=_{2}\), and \(=_{1}\) we can easily transform Problem (5) into Problem (7) (primal) in a more compact way with an extra linear constraint.

\[_{,} f()+g()= ,\] (7)

with \(f()=^{}+\|\|_{2}^{2}+_{( 0)}, g()=-^{}()\).

The state-of-the-art method  applies the linearized alternating direction method of multipliers (ADMM) algorithm  directly to the primal problem with a similar structure in Problem (7), which lacks the theoretical guarantee on its convergence rate since the objective function in Problem (7) has no Lipschitz gradient. Motivated by the recent work , we circumvent this critical issue by applying the FISTA algorithm , a proximal gradient method, to the dual problem of (7) instead. Motivated by recent advances [3; 61], we can now provide a better convergence rate with rigorous theoretical analysis so that our proposed graph construction method is much more efficient with guarantees.

Dual Problem FormationWe construct the Lagrangian function by introducing the Lagrangian multipliers \(^{n}\) as \((,,)=f()+g()-,-\). We establish the corresponding dual problem as Problem (8) by introducing the conjugate functions \(f^{*},g^{*}\) for simpler notation (Appendix E.3).

\[_{} F()+G(),\] (8)with \(F()=f^{*}(^{}), G()=g^{*}( -)\). By Slater's condition, we know that strong duality holds as long as \(\) resides in the range of \(\). Therefore, the optimal values for Problem (7) and (8) are identical, and the optimal solution for Problem (8) can be attained. Consequently, we can apply the FISTA algorithm to the dual problem to generate the dual sequence that converges to the optimal solution, and construct the corresponding optimal solution back for the primal problem.

Dual Problem with FISTAIt is not hard to show that \(F()\) is differentiable and \( F()=^{*}\), where \(^{*}=_{x}\{(^{})^{}- f()\}\) (Appendix E.4). Therefore, the dual problem minimizes the sum of a differentiable convex function \(F\) and a closed proper convex function \(G\). This structure of the dual Problem (8) immediately paves the way for applying proximal gradient methods. Here, for the sake of a better convergence rate, we apply the FISTA algorithm with a fixed step size to the dual Problem (8), and the following iteration schemes are performed. We first choose any \(^{0}=^{-1}\) and fix the step size as \(t=\). Henceforth, \(k=1,2,\), we repeat the following two steps as

\[^{k} =^{k-1}+(^{k-1}-^{k-2}),\] (9) \[^{k} =_{tG}(^{k}-t F(^{ k})),\] (10)

Hence, based on the above-mentioned properties of \( F(^{k})\), we have \( F(^{k})=}^{k}\) with \(}^{k}\) set as \(}^{k}=_{}\{(^{}^{k})^{ }-f()\}=[^{}^{k} -}{2}]_{+}\) (Appendix E.5).

Let \(^{k}=^{k}-t}^{k}\). By the extended Moreau decomposition , \(_{ h}(z)+_{^{-1}h^{* }}(z/)=z, z\). We have \(_{tG}(^{k})=^{k}-t_{t^{-1}G^ {*}}(t^{-1}^{k})=^{k}-t(}^{k}-}^{k})\). Here, \(}^{k}=_{t^{-1}g}(}^{k}-t^{-1} ^{k})\). Note that \(g()=-^{}()\) and by the definition of proximal mapping, it is easy to prove that \(_{t^{-1}g}()=(++4 t^{-1}})\). Therefore, we can simplify the updating step (10) as the following three steps (Appendix E.6).

\[}^{k} =[^{}^{k}-}{2} ]_{+},\] (11) \[}^{k} =(}^{k}-t^{-1}^{k})+}^{k}-t^{-1}^{k})(}^{k }-t^{-1}^{k})+4 t^{-1}},\] (12) \[^{k} =^{k}-t(}^{k}-}^{k}),\] (13)

Finally, we arrive at Procedure GWBI, where the optimal graph weights are inferred for one block like \(_{uu}\) in the optimal graph structure suggested in Proposition 1. Here, instead of directly optimizing the primal variable of the block graph weight vector \(\), we consider its corresponding dual variable \(\) for a better convergence rate. In each iteration step, we first find an extrapolated point \(\) based on the points \(\) from two previous steps (line 3). We then perform the proximal gradient update on this extrapolated point (lines 4-6) to obtain \(\) for the next iteration. Note that lines 4-6 are the detailed instantiation of Eq. (10). Finally, we can convert the dual variable \(\) back to the desired primal variable \(\) based on line 4 after its convergence. With this core procedure in hand, we plug it into the proposed Algorithm 1, Block-wise Affinity Graph Learning (BAGL) algorithm, in Appendix C.

### Theoretical Analysis

We prove that our proposed optimization method for Problem (7) in Procedure GWBI enjoys the guarantee of the global convergence rate, where the generated primal sequence converges to the global optimal solution at a rate of \(O()\) with a fixed step size. To begin with, it is well known that the FISTA algorithm enjoys the global convergence rate of \(O(})\). For simplicity, we focus on the results when optimizing Problem (5), which can be viewed as a general case of Problem (6) when \(l u\). Therefore, \(Q() F()+G()\) converges to the dual optimal value \(Q(^{*})\) at a rate of \(O(})\) due to this well-known fact , which yields Theorem 2.

**Theorem 2**.: _Let \(\{^{k}\}\) be the dual sequence generated by Procedure GWBI, then_

\[Q(^{k})-Q(^{*})}\|^{0}-^{*}\|_{2}^{2}.\] (14)We then consider the corresponding primal sequence \(\{^{k}\}\) and its convergence rate. Since the strong duality holds and a dual optimal solution \(^{*}\) exists, any primal optimal point \((^{*},^{*})\) is also a minimizer of \((,,^{*})\). This motivates us to construct the primal sequence \(\{^{k}\}\) based on the dual sequence \(\{^{k}\}\) as \(^{k}=*{arg\,min}_{}(,,^{k})=*{arg\,max}_{}\{^{}^{k},-f()\}\). Thanks to Theorem 3, this primal sequence \(\{^{k}\}\) is guaranteed to converge to the optimal primal solution \(^{*}\) of Problem (7) at the rate of \(O()\).

**Theorem 3**.: _Let \(^{k}=*{arg\,max}_{}^{} ^{k},-f()\) with the dual sequence \(\{^{k}\}\) given by Procedure GWBI. \(^{*}\) and \(^{*}\) are the optimal solution of Problem (7) and the optimal solution of Problem (8), respectively. We have,_

\[\|^{k}-^{*}\|_{2}}{(k+1)}\|^{ 0}-^{*}\|_{2}.\] (15)

Motivated by , Theorem 3 establishes that the proposed method exhibits a sub-linear convergence rate, which represents a state-of-the-art result for optimization-based graph construction methods, accompanied by a global convergence guarantee. This improved convergence rate is primarily attributed to the utilization of the FISTA algorithm applied to the dual problem. Time complexity analysis is included in Appendix G.5. More results regarding the robustness of our method are discussed in Appendix D.

## 4 Experiments

In this section, we conduct numerical experiments on both synthetic and real-world datasets to demonstrate the advantages of our proposed BAGL method in terms of efficacy and convergence. Robustness analysis (Appendix G.3) and more experimental results are included in Appendix G.

### Baseline Models

We choose the following graph construction methods in GSSL for comparison. Radial basis function kernel (RBF)  and kNN graph  are two classic methods. Smooth graph learning (SGL)  is a popular method in the graph signal processing domain. RGCLI  is another label-informed graph construction method. Anchor Graph Regularization (AGR)  deals with large-scale graph construction. GraphEBM  and BCAN  are two state-of-the-art methods. The former exploits the energy-based model while the latter constructs a bipartite graph.

All the hyper-parameters are fine-tuned with the grid search method. We repeat the experiment 20 times for each case and report the average result with optimal parameter setting in the efficacy analysis. Unless otherwise specified, the default label inference algorithm is LGC, and the label rate is ten labeled samples per class. More details on the experimental settings can be found in Appendix F.

### Synthetic Dataset

We generate a synthetic dataset as shown in Fig. 1 (a). The constructed dataset contains two clusters, a dense Gaussian cluster surrounded by a sparse ring-like cluster. With only one labeled sample given in each cluster, we compare the result of our proposed BAGL method (Fig. 1 (c)) with the result of the most popular method RBF (Fig. 1 (b)). We use the coordinates as the node feature and set the width in the RBF kernel as \(0.7\). For visualization purposes, we show the adjacency matrix with yellow line segments connecting the node pairs if the weight associated with the edge is greater than 0.5 after normalization. The direction of the edge is ignored in Fig. 1 (c). For a fair comparison, we perform label propagation  on both constructed affinity graphs. We can see that BAGL can recover two ground-truth clusters much better. Unlike RBF, BAGL can improve the connectivity by the logarithm penalty term and reduce the inter-cluster links by the block-wise design.

### Real-world Datasets

Classification tasks are implemented to assess the performance of BAGL against all graph construction baseline methods on six real-world datasets, listed in Table 1. **ORHD** (Optical Recognition of Handwritten Digits Data Set), **USPS**, **MNIST**, and **EMNIST Letters** are four popular digits image datasets. **COLL100** is an object image dataset. **TDT2** is a text dataset. We fix the number of anchor nodes as 1000 in four datasets (COIL100, USPS, ORHD and TDT2), while for the rest two datasets (MNIST, EMNIST-Letters), the number of anchors is fixed as 2000. We perform tf-idf and principal component analysis (PCA) as the pre-processing step on TDT2 dataset. The default label inference algorithm is LGC, with ten labels per class. Further details can be found in Appendix F.1.

#### 4.3.1 Efficacy

We fix the number of labeled samples per class to ten and select three label inference methods for the second phase of GSSL. We report average results by performing 20 trials for each algorithm over all the settings in Table 2. Our algorithm outperforms all methods in the USPS, TDT2, and EMNIST-Letters datasets, indicating that BAGL can learn an optimal graph for label inference algorithms in the unified framework. Moreover, we perform the Friedman test with the Bonferroni-Dunn post hoc test for statistical significance analysis. Fig. 2 illustrates the critical difference (CD) diagram on the accuracy, where the average rank is marked along the axis with lower (better) ranks to the left. If the average rank difference between two models is greater than one CD, the relative performance is believed to be different. Accordingly, BAGL significantly outperforms all other baselines by a large margin. We also conduct experiments under low label rates with LGC fixed as the label inference method. Fig. 3 (a) and (b) demonstrate that BAGL performs relatively well with low label rates. This phenomenon can be attributed to the utilization of label information in BAGL. (Appendix G.1)

   Dataset & \#Samples \(n\) & \#Features \(d\) & \#Classes \(c\) \\  ORHD & 5,620 & 64 & 10 \\ USPS & 9,298 & 256 & 10 \\ COIL100 & 7,200 & 1,024 & 100 \\ TDT2 & 9,394 & 36,71 & 30 \\ MNIST & 70,000 & 784 & 10 \\ EMNIST Letters & 145,600 & 784 & 20 \\   

Table 1: Description of datasets

#### 4.3.2 Convergence

As an essential part of the overhead for BAGL, Procedure GWBI needs to be efficient for practice. Compared with two other optimization-based methods sharing a similar objective, SGL and its accelerated version by ADMM , BAGL enjoys the fastest convergence rate on both datasets. We sample 1% nodes in each dataset for the convenience of presentation in Fig. 3 (c) and (d). Its outstanding performance confirms the theoretical analysis in Theorem 3. (Appendix G.2).

#### 4.3.3 Ablation Study

To obtain a better understanding of why the proposed BAGL works, we perform some ablation studies to empirically show how the key design of BAGL will potentially affect performance. We create three variants based on the original version of BAGL. First, to demonstrate the significance of the optimal asymmetric structure, we now do not differentiate labeled nodes and unlabeled nodes, and let any graph structure be the potential optimal structure without the constraints of \(_{ul}=\) and \(_{uu}=\). We call this variant _BAGL w/o optimal structure_. Second, to reveal the importance of the connectivity regularization term, we set \(=0\) in Procedure GWBI to remove the connectivity consideration. This variant is termed _BAGL w/o connectivity_. Third, to investigate the effects of sparsity control, we set \(=10e-5 0\) in Procedure GWBI to allow the sparsity of the constructed graph to vary arbitrarily. The last variant of BAGL is abbreviated as _BAGL w/o sparsity_. We conduct the experiments on the ORHD dataset with the same setting as Table 2 and report the classification accuracy results in Table 3. The proposed optimal asymmetric graph structure contributes most to the success of BAGL. The connectivity regularization term and the sparsity control term also matter since they together encourage a more sparse graph (the latter) but without disconnected components (the former), which is a more favorable graph for the second label inference step.

    & & RBF & kNN & SGL & RGCLI & AGR & GraphEBM & BCAN & BAGL \\   & GRF & 97.46\(\)0.36 & 86.59\(\)1.26 & 94.68\(\)0.66 & 88.24\(\)3.11 & 97.63\(\)0.53 & 95.13\(\)0.41 & 97.49\(\)0.47 & **97.88\(\)0.40** \\  & LGPC & 97.65\(\)0.59 & 87.61\(\)2.30 & 97.55\(\)0.74 & 89.32\(\)2.58 & 96.90\(\)0.47 & 97.85\(\)0.53 & 97.27\(\)0.63 & **98.04\(\)0.71** \\  & GCN & 98.91\(\)0.44 & 96.04\(\)3.54 & 95.80\(\)0.59 & 89.37\(\)0.29 & 98.02\(\)0.40 & **98.02\(\)0.50** & 98.08\(\)0.70 & 98.15\(\)0.62 \\   & GRF & 94.53\(\)0.65 & 81.42\(\)0.98 & 87.67\(\)0.40 & 84.15\(\)1.85 & 93.62\(\)0.62 & 94.26\(\)0.36 & 93.98\(\)0.60 & **96.56\(\)0.93** \\  & LGPC & 94.75\(\)0.42 & 85.13\(\)1.18 & 86.40\(\)0.51 & 88.63\(\)1.63 & 95.29\(\)0.51 & 94.30\(\)0.30 & 95.07\(\)0.75 & **96.77\(\)0.66** \\  & GCN & 94.98\(\)0.21 & 86.02\(\)0.23 & 90.31\(\)0.32 & 86.89\(\)1.72 & 95.78\(\)0.49 & 95.81\(\)0.48 & 95.79\(\)0.55 & **97.20\(\)0.64** \\   & GRF & 94.40\(\)0.19 & 81.24\(\)1.64 & 92.65\(\)0.82 & 87.48\(\)2.30 & 86.54\(\)0.40 & 85.22\(\)0.57 & 84.51\(\)0.59 & **94.78\(\)0.53** \\  & LGPC & **95.13\(\)0.37** & 83.66\(\)1.35 & 93.27\(\)1.03 & 87.92\(\)2.47 & 87.18\(\)0.57 & 85.04\(\)0.77 & 87.06\(\)0.63 & 94.93\(\)0.45 \\  & GCN & 94.31\(\)0.25 & 87.64\(\)1.27 & 93.52\(\)0.91 & 99.80\(\)1.65 & 94.63\(\)0.51 & 99.05\(\)0.39 & 90.02\(\)0.48 & **94.99\(\)0.88** \\   & GRF & 89.22\(\)0.79 & 80.09\(\)2.69 & 92.13\(\)0.99 & 86.51\(\)3.42 & 94.47\(\)0.79 & 93.63\(\)0.74 & 95.95\(\)0.60 & **96.01\(\)0.91** \\  & LGPC & 89.67\(\)0.46 & 82.53\(\)3.04 & 92.96\(\)1.24 & 87.60\(\)2.84 & 94.15\(\)0.67 & 93.97\(\)0.61 & 94.13\(\)0.79 & **95.42\(\)0.71** \\  & GCN & 92.89\(\)0.68 & 85.77\(\)2.41 & 94.39\(\)0.83 & 89.94\(\)3.15 & 95.36\(\)0.84 & 95.36\(\)0.89 & 96.30\(\)0.77 & **96.33\(\)0.85** \\   & GRF & 83.60\(\)0.24 & 64.20\(\)1.82 & 95.03\(\)0.77 & 87.65\(\)2.07 & 91.02\(\)0.31 & 95.39\(\)0.31 & 92.41\(\)0.47 & **95.40\(\)0.62** \\  & LGPC & 41.42\(\)1.07 & 61.86\(\)1.63 & 94.40\(\)0.52 & 82.22\(\)3.36 & 94.79\(\)0.37 & **95.43\(\)0.47** & 93.55\(\)0.58 & 95.42\(\)0.51 \\  & GCN & 87.03\(\)0.32 & 74.93\(\)1.77 & 95.18\(\)0.47 & 90.47\(\)2.11 & 95.30\(\)0.21 & 95.51\(\)**0.40** & 94.84\(\)0.37 & 95.47\(\)0.43 \\ \

## 5 Conclusion

In this paper, we propose a novel approach to graph construction for graph-based semi-supervised learning. Building upon the optimal asymmetric graph structure derived from theoretical insights, we develop an efficient block-wise graph construction method that guarantees faster convergence. Our approach combines theoretical insights with practical considerations to provide a more effective and reliable framework for the graph construction step in graph-based semi-supervised learning.

## 6 Limitations

BAGL is an optimization-based method for the graph construction step in graph-based semi-supervised learning. Graph Neural Networks (GNNs) excel in learning representations for graph-structured data [64; 54; 12; 49; 39; 78; 41; 75; 13; 53; 77; 51; 73; 52; 50; 38; 76; 11]. More recent graph structure learning methods aim to learn a clean graph structure from the given noisy graph so that the subsequent GNNs trained on this learned clean graph can obtain better performance. In GSSL, however, there is no given graph structure, and we need to learn the graph structure based on the node features only. Therefore, it is a more challenging task compared to graph structure learning. Therefore, we do not compare our method with other graph structure learning methods since their settings and goals are slightly different. We leave the investigation of graph structure learning for GSSL as future work since it is currently out of the scope of this work.

The other limitation of BAGL is it is only suitable for the transductive setting. If we have nodes or samples unseen in the training set, we have to construct the affinity graph again by executing BAGL again to infer their labels, which is often time-consuming and troublesome regarding efficiency. This is not desirable in real-world applications since we often come across new training samples after we build the affinity graph. We also leave the investigation of the inductive extension of BAGL as future work since this lack of inductive generalization is a well-known challenge in graph-based semi-supervised learning.

Even though BAGL is quite efficient in terms of convergence rate, it may still have computational issues when dealing with extremely large-scale datasets with billions of samples because the time spent on finishing one iteration during the optimization would increase dramatically when the number of training samples is extremely large. We leave the exploration of graph construction methods for extremely large-scale datasets as future work. Other potential applications of our method can be explored in the hyperbolic space [67; 70; 68; 71; 69; 66; 72] or in the natural language processing domain [32; 34; 33; 21; 20; 55; 42; 80; 81; 79].