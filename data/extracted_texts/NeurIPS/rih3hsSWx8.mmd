# Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks

Andong Wang

RIKEN AIP

andong.wang@riken.jp

&Chao Li

RIKEN AIP

chao.li@riken.jp

&Mingyuan Bai

RIKEN AIP

mingyuan.bai@riken.jp

&Zhong Jin

China University of Petroleum-Beijing at Karamay

zhongjin@cupk.edu.cn

&Guoxu Zhou

Guangdong University of Technology

gx.zhou@gdut.edu.cn

&Qibin Zhao

RIKEN AIP

qibin.zhao@riken.jp

Qibin Zhao and Guoxu Zhou are the corresponding authors.

###### Abstract

Multi-channel learning has gained significant attention in recent applications, where neural networks with t-product layers (t-NNs) have shown promising performance through novel feature mapping in the transformed domain. However, despite the practical success of t-NNs, the theoretical analysis of their generalization remains unexplored. We address this gap by deriving upper bounds on the generalization error of t-NNs in both standard and adversarial settings. Notably, it reveals that t-NNs compressed with exact transformed low-rank parameterization can achieve tighter adversarial generalization bounds compared to non-compressed models. While exact transformed low-rank weights are rare in practice, the analysis demonstrates that through adversarial training with gradient flow, highly over-parameterized t-NNs with the ReLU activation can be implicitly regularized towards a transformed low-rank parameterization under certain conditions. Moreover, this paper establishes sharp adversarial generalization bounds for t-NNs with approximately transformed low-rank weights. Our analysis highlights the potential of transformed low-rank parameterization in enhancing the robust generalization of t-NNs, offering valuable insights for further research and development.

## 1 Introduction

Multi-channel learning is a task to extract representations from the data with multiple channels, such as multispectral images, time series, and multi-view videos, in an efficient and robust manner [24; 39; 60; 61]. Among the methods tackling this task, neural networks with t-product layers (t-NNs, see Eq. (5) for a typical example) [12; 36] came to the stage very recently with remarkable efficiency and robustness in various applications such as graph learning, remote sensing, and more [1; 11; 14; 32; 38; 39; 53; 58]. What distinguishes t-NNs from other networks is the inclusion of t-product layers, founded on the algebraic framework of tensor singular value decomposition (t-SVD) [19; 40; 60; 61]. Unlike traditional tensor decompositions, t-SVD explores the transformed low-rankness, i.e., the low-rank structure of a tensor in the transformed domain under an invertible linear transform . The imposed transform in t-product layers provides additional expressivity to neural networks, whilethe controllable transformed low-rank structure in t-NNs enables a flexible balance between model accuracy and robustness [36; 38; 53].

Despite the impressive empirical performance of t-NNs, the theoretical foundations behind their success remain unclear. The lack of systematic theoretical analysis hinders deeper comprehension and exploration of more effective applications and robust performance of t-NNs. Furthermore, the inclusion of the additional transform in t-NNs renders the theoretical analysis more technically challenging compared to existing work on general neural networks [29; 37; 43; 55]. To address this challenge, we establish for the first time a theoretical framework for t-NNs to understand both the standard and robust generalization behaviors, providing both theoretical insights and practical guidance for the efficient and robust utilization of t-NNs. Specifically, we address the following fundamental questions:

* _Can we theoretically characterize the generalization behavior of general t-NNs?_ Yes. We derive the upper bounds on the generalization error for _general_ t-NNs in both standard and adversarial settings in Sec. 3.
* _How does exact transformed low-rankness influence the robust generalization of t-NNs?_ In Sec. 4.1, our analysis shows that t-NNs with _exactly_ transformed low-rank weights exhibit lower adversarial generalization bounds and require fewer samples, highlighting the benefits of transformed low-rank weights in t-NNs for improved robustness and efficiency.
* _How does adversarial learning of t-NNs affect the transformed ranks of their weight tensors?_ In Sec. 4.2, we deduce that weight tensors tend to be of transformed low-rankness _approximately_ for highly over-parameterized t-NNs with ReLU activation under adversarial training with gradient flow.
* _How is robust generalization impacted by approximately transformed low-rank weight tensors in t-NNs?_ In Sec. 4.3, we establish sharp adversarial generalization bounds for t-NNs with _approximately_ transformed low-rank weights by carefully bridging the gap with exact transformed low-rank parameterization. This finding again underscores the importance of incorporating transformed low-rank weights as a means to enhance the robustness of t-NNs.

## 2 Notations and Preliminaries

In this section, we introduce the notations and provide a concise overview of t-SVD and t-NNs, which play a central role in the subsequent analysis.

**Notations.** We use lowercase, lowercase boldface, and uppercase boldface letters to denote scalars, _e.g._, \(a\), vectors, _e.g._, \(^{m}\), and matrices, _e.g._, \(^{m n}\), respectively. Following the standard notations in Ref. , a 3-way tensor of size \(d 1\) is also called a _t-vector_ and denoted by underlined lowercase, _e.g._, \(}\), whereas a 3-way tensor of size \(m n\) is also called a _t-matrix_ and denoted by underlined uppercase, _e.g._, \(}\). We use a t-vector \(}^{d 1}\) to represent a multi-channel example, where \(\) denotes the number of channels and \(d\) is the number of features for each channel.

Given a matrix \(^{m n}\), its Frobenius norm (F-norm) and spectral norm are defined as \(\|\|_{}:=^{\{m,n\}}_{ i}^{2}}\) and \(\|\|:=_{i}_{i}\), respectively, where \(_{i},\,i=1,,\{m,n\}\) are its singular values. The _stable rank_ of a non-zero matrix \(\) is defined as the squared ratio of its F-norm and spectral norm \(r_{}():=\|\|_{}^{2}/ \|\|^{2}\). Given a tensor \(}\), define its \(l_{p}\)-norm and F-norm respectively as \(\|}\|_{l_{p}}:=\|( })\|_{l_{p}}\), and \(\|}\|_{}:=\|( })\|_{l_{2}}\), where \(()\) denotes the vectorization operation of a tensor . Given \(}^{m n}\), let \(}_{::,i}\) denote its \(i^{}\) frontal slice. The inner product between two tensors \(,}\) is defined as \(,:=( )^{}()\). The frontal-slice-wise product of two tensors \(},}\), denoted by \(}\), equals a tensor \(}\) such that \(}_{::,i}=}_{::,:,i}}_{::,:,i},\ i=1,,\). We use \(||\) as the absolute value for a scalar and cardinality for a set. We use \(\) to denote the function composition operation. Additional notations will be introduced upon their first occurrence.

### Tensor Singular Value Decomposition

The framework of tensor singular value decomposition (t-SVD) is based on the t-product under an invertible linear transform \(M\). In recent studies, the transformation matrix \(\) defining the transform \(M\) is _restricted to be orthogonal_ for better properties, which is also followed in this paper. Given any _orthogonal matrix_\(^{}\), define the associated linear transform \(M()\) with its inverse \(M^{-1}()\) on any \(}^{m n}\) as

\[M(}):=}_{3},\  M^{-1}(}):=}_{3} ^{-1},\] (1)

where \(_{3}\) denotes the tensor matrix product on mode-\(3\).

**Definition 1** (t-product ).: _The t-product of any \(}^{m n}\) and \(}^{n k}\) under transform \(M\) in Eq. (1) is denoted and defined as \(}_{M}}=^ {m k}\) such that \(M(})=M(}) M(})\) in the transformed domain. Equivalently, we have \(}=M^{-1}(M(}) M(}))\) in the original domain._

**Definition 2** (\(M\)-block-diagonal matrix).: _The \(M\)-block-diagonal matrix of any \(}^{m n}\), denoted by \(}_{M}\), is the block diagonal matrix whose diagonal blocks are the frontal slices of \(M(})\):_

\[}_{M}:=(M(})):=M (})_{:,:,1}&&\\ &M(})_{:,:,2}&&\\ &&&\\ &&&M(})_{:,:,}^{m  n}.\]

In this paper, we also follow the definition of t-transpose, t-identity tensor, t-orthogonal tensor, and f-diagonal tensor given by Ref. , and thus the t-SVD is introduced as follows.

**Definition 3** (t-SVD, tubal rank ).: _Tensor Singular Value Decomposition (t-SVD) of \(}^{m n}\) under the invertible linear transform \(M\) in Eq. (1) is given as follows_

\[}=}_{M}} _{M}}^{},\] (2)

_where \(}^{m m}\) and \(}^{n n}\) are t-orthogonal, and \(}^{m n}\) is f-diagonal. The tubal rank of \(}\) is defined as the number of non-zero tubes of \(}\) in its t-SVD in Eq. (2), i.e., \(r_{}(}):=|\{i\,|\,}(i,i,:) ,i\{m,n\}\}|\)._

For any \(}^{m n}\) with the tubal rank \(r_{}(})\), we have following relationship between its t-SVD and the matrix SVD of its \(M\)-block-diagonal matrix [26; 50]:

\[}=}_{M}} _{M}}^{}\ \ }_{M}=}_{M}} _{M}}_{M}^{},  r_{}(})(}_{M}).\] (3)

As the \(M\)-block-diagonal matrix \(}_{M}\) is defined after transforming tensor \(}\) from the original domain to the transformed domain, the relationship \( r_{}(})( }_{M})\) indicates that the tubal rank can be chosen as a measure of transformed low-rankness [26; 50].

### Neural Networks with t-Product Layer (t-NNs)

In this subsection, we will introduce the formulation of the t-product layer in t-NNs, which is designed for multi-channel feature learning.

**Multi-channel feature learning via t-product.** Suppose we have a multi-channel example represented by a t-vector \(}^{d 1}\), where \(\) is the number of channels and \(d\) is the number of features. We define an \(L\)-layer t-NN feature extractor \((})\), to extract \(d_{L}\) features for each channel of \(}\):

\[(})=^{(L)}(}); ^{(l)}(})=(}^{ (l)}_{M}^{(l-1)}(})),\ l=1,,L; ^{(0)}(})=},\] (4)

where the \(l\)-th layer \(^{(l)}\) first conducts t-product with weight tensor (t-matrix) \(^{(l)}^{d_{l} d_{l-1}}\) on the output of the \((l-1)\)-th layer as multi-channel features2\(^{(l-1)}(})^{d_{l-1} 1}\) to obtain a (\(d_{l} 1\))-dimensional representation and then uses the entry-wisely ReLU activation3\((x)=\{x,0\}\) for nonlinearity.

**Remark**.: _Unlike Refs. [36; 32] and  whose nonlinear activation is performed in the transformed domain, the t-NN model in Eq. (4) considers the nonlinear activation in the original domain and hence is consistent with traditional neural networks._By adding a linear classification module with weight \(^{cd_{L}}\) after the feature exaction module in Eq. (4), we consider the following t-NN predictor whose sign can be utilized for binary classification:

\[f(;}):=^{}{}_{}( ^{L}()()).\] (5)

Let \(}:=\{}^{(1)},, }^{(L)},\}\) be the collection of all the weights4. With a slight abuse of notation, let \(\|}\|_{}:=\|_{2}^{2}+_{l =1}^{L}\|}^{(l)}\|_{}^{2}}\) denote the Euclidean norm of all the weights. The function class of general t-NNs whose weights are bounded in the Euclidean norm is defined as

\[:=\{f(;})\ \ \ \|\|_{2} B_{w},\| }^{(l)}\|_{} B_{l}, l=1,,L \},\] (6)

with positive constants \(B_{w}\) and \(B_{l},\ l=1,,L\). Let \(B_{}}:=B_{w}_{l=1}^{L}B_{l}\) for simplicity.

## 3 Standard and Robust Generalization Bounds for t-NNs

This section establishes both the standard and robust generalization bounds for any t-NN \(f\).

### Standard Generalization for General t-NNs

Suppose we are given a training multi-channel dataset \(S\) consisting of \(N\) example-label pairs \(\{(}_{i},y_{i})\}_{i=1}^{N}^{d 1  c}\{ 1\}\)_i.i.d._ drawn from an underlying data distribution \(P_{},y}\).

**Assumption 1**.: _Every input example \(}^{d 1 c}\) has an upper bounded F-norm, i.e., \(\|\|_{} B_{x}\), where \(B_{x}\) is a positive constant._

When a loss function \((f(_{i}),y_{i})\) is considered as the measure of the classification quality, we define the empirical and population risk for any \(f\) as \(}(f):=N^{-1}_{i=1}^{N}(f(}_{i}),y _{i})\) and \((f):=_{P_{(,y)}}[(f(}),y)],\) respectively. Similar to Ref. , we make assumptions on the loss as follows.

**Assumption 2**.: _The loss \((h(}),y)\) can be expressed as \((h(}),y)=(-(yh( }))\) for any t-NN \(h\), such that:_

1. _the range of loss_ \((,)\) _is_ \([0,B]\)_, where_ \(B\) _is a positive constant;_
2. _function_ \(:\) _is_ \(C^{1}\)_-smooth;_
3. \(^{}(x) 0\) _for any_ \(x\)_;_
4. _there exists_ \(b_{} 0\) _such that_ \(x^{}(x)\) _is non-decreasing for_ \(x(b_{},+)\)_, and the derivative_ \(x^{}(x)+\) _as_ \(x+\)_;_
5. _let_ \(:[(b_{}),+)[b_{},+)\) _be the inverse function of_ \(\) _on the domain_ \([b_{},+)\)_. There exist_ \(b_{}\{2(b_{}),(2b_{ })\}\) _and_ \(K 1\)_, such that_ \(^{}(x) K^{}( x)\) _and_ \(^{}(y) K^{}( y)\) _for any_ \(x(b_{},+),y((b_{}),+)\) _and_ \([1/2,1)\)_._

Assumption _(A.1)_ is a natural assumption in generalization analysis , and Assumptions _(A.2)_**-**(A.5) are the same as Assumption (B3) in Ref. . According to Assumption _(A.2)_, the loss function \((,)\) satisfies the \(L_{}\)-Lipschitz continuity

\[|(h(}_{1}),y_{1})-(h(}_{2}),y_{2})| L_{}|y_{1}h(}_{1})-y_{2}(}_{2})|,L_{}=_{|q| B_{}}^{ }(q)e^{-(q)},\] (7)

where \(B_{}\) is an upper bound on the output of any t-NN \(h\). The Lipschitz continuity is also widely assumed for generalization analysis of DNNs . Assumption 2 is satisfied by commonly used loss functions such as the logistic loss and the exponential loss.

The generalization gap \((f)-}(f)\) of any function \(f\) can be bounded as follows.

**Lemma 3** (Generalization bound for t-NNs).: _Under Assumptions 1 and 2, it holds for any \(f\) that_

\[(f)-}(f)B_{x}B_{}}{ }(+1)+3B},\] (8)

_with probability at least \(1-2e^{-t}\) for any \(t>0\)._

**Remark**.: _When the input example has channel number \(=1\), the generalization bound in Theorem 3 is consistent with the F-norm-based bound in Ref. ._

### Robust Generalization for General t-NNs

We study the adversarial generalization behavior of t-NNs in this section. We first make the following assumption on the adversarial perturbations.

**Assumption 4**.: _Given an input example \(}\), the adversarial perturbation is chosen within a radius-\(\) ball of norm \(R_{}()\) with compatibility constant  defined as \(_{R_{}}:=_{}}R_{ }(})/\|}\|_{ }\)._

The assumption allows for much broader adversary classes than the commonly considered \(l_{p}\)-attacks . For example, if one treats the multi-channel data \(}^{d 1}\) as a matrix of dimensionality \(d\) and attacks it with nuclear norm attacks , then the constant \(_{R_{}}=\}}\).

Given an example-label pair \((},y)\), the adversarial loss for any predictor \(f\) is defined as \((f(}),y)=_{R_{}(}^{}-})}(f( }^{}),y)\). The empirical and population adversarial risks are thus defined as \(}^{}(f):=N^{-1}_{i=1}^{N}(f( }_{i}),y_{i})\) and \(^{}(f):=_{P_{(,y)}}[(f( }),y)]\), respectively. The adversarial generalization performance is measured by the adversarial generalization gap (AGP) defined as \(^{}(f)-}^{}(f)\). Let \(B_{}:=(B_{x}+_{R_{}})B_{}}\). For any \(f\), its AGP is bounded as follows.

**Theorem 5** (Adversarial generalization bound for t-NNs).: _Under Assumptions 1, 2, and 4, there exists a constant \(C\) such that for any \(f\), it holds with probability at least \(1-2e^{-t}\)\(( t>0)\):_

\[^{}(f)-}^{}(f) }B_{}}{}_{ l=1}^{L}d_{l-1}d_{l}(3(L+1))}+3B}.\] (9)

**Remark**.: _When the input example has channel number \(=1\) and the attacker uses \(l_{p}\)-attack, the adversarial generalization bound in Theorem 5 degenerates to the one in Theorem 4 of Ref. ._

## 4 Transformed Low-rank Parameterization for Robust Generalization

### Robust Generalization with Exact Transformed Low-rank Parameterization

According to Theorem 5, the AGP bound scales with the square root of the parameter complexity, specifically as \(O((_{l}d_{l-1}d_{l})/N})\). This implies that achieving the desired adversarial accuracy may require a large number \(N\) of training examples. Furthermore, high parameter complexity leads to increased energy consumption, storage requirements, and computational cost when deploying large t-NN models, particularly on resource-constrained embedded and mobile devices.

To this end, we propose a transformed low-rank parameterization scheme to compress the original t-NN models \(\). Specifically, given a vector of pre-set ranks \(=(r_{1},,r_{L})^{}^{L}\) where \(r_{l}\{d_{l},d_{l-1}\}\), we consider the following subset of the original t-NNs:

\[_{}:=ff,\ r_{i}( }^{(l)}) r_{l},\ \ l=1,,L}.\] (10)

In the function set \(_{}\), the weight tensor \(}^{(l)}\) of the \(l\)-th layer has the upper bounded tubal rank, which means low-rankness in the transformed domain5. We bound the AGP for any \(f_{}\) as follows.

**Theorem 6** (Adversarial generalization bound for t-NNs with transformed low-rank weights).: _Under Assumptions 1, 2, and 4, there exists a constant \(C^{}\) such that_

\[^{}(f_{})-}^{}(f_{ })L_{}B_{}}{}_{l=1}^{L}r_{l}(d_{l-1}+d_{l})(9(L+1))}+3B},\] (11)

_holds for any \(f_{}_{}\) with probability at least \(1-2e^{-t}\)\(( t>0)\)._Comparing Theorem 6 with Theorem 5, we observe that the adversarial generalization bound under transformed low-rank parameterization has a better scaling, specifically \(O(r_{l}(d_{l-1}+d_{l})/N})\). This also implies that a smaller number \(N\) of training examples is required to achieve the desired accuracy, as well as reduced energy consumption, storage requirements, and computational cost. Please refer to Sec. A.1 in the appendix for numerical evidence.

### Implicit Bias of Gradient Flow for Adversarial Training of Over-parameterized t-NNs

Although Theorem 6 shows _exactly_ transformed low-rank parameterization leads to lower bounds, the well trained t-NNs on real data rarely have exactly transformed low-rank weights. In this section, we prove that the highly over-parameterized t-NNs, trained by adversarial training with gradient flow (GF), are _approximately_ of transformed low-rank parameterization under certain conditions.

First, the proposed t-NN \(f(;})\) is said to be (positively) _homogeneous_ as the condition \(f(};a})=a^{L+1}f(};})\) holds for any positive constant \(a\). Motivated by Ref. , we focus on the scale invariant adversarial perturbations defined as follows.

**Definition 4** (Scale invariant adversarial perturbation ).: _An adversarial perturbation \(_{i}(})\) is said to be scale invariant for \(f(};})\) at any given example \(}_{i}\) if it satisfies \(_{i}(a})=_{i}( })\) for any positive constant \(a\)._

**Lemma 7**.: _The \(l_{2}\)-FGM , FGSM , \(l_{2}\)-PGD and \(l_{}\)-PGD  perturbations for the t-NNs are all scale invariant._

Then, we consider adversarial training of t-NNs with scale invariant adversarial perturbations by GF, which can be seen as gradient descent with infinitesimal step size. When using GF for the ReLU t-NNs, \(}\) changes continuously with time, and the trajectory of parameter \(}\) during training is an arc \(}:[0,)^{( })},t}(t)\) that satisfies the differential inclusion [7; 30]

\[(t)}{t}-^{}}^{}(}(t))\] (12)

for \(t 0\)_a.e._, where \(^{}}^{}\) denotes the Clarke's subdifferential  with respect to \(}(t)\). If \(}^{}(})\) is actually a \(C^{1}\)-smooth function, the above differential inclusion reduces to

\[(t)}{t}=-}^{ }((t))}{(t)}\] (13)

for any \(t 0\), which corresponds to the GF with differential in the usual sense. However, for simplicity, we follow Refs. [45; 46] and still use Eq. (13) to denote Eq. (12) with a slight abuse of notation, even if \(}^{}\) does not satisfy differentiability but only local Lipschitzness 6.

We also make an assumption on the training data as follows.

**Assumption 8** (Existence of a separability of adversarial examples during training).: _There exists a time \(t_{0}\) such that \(}^{}(t_{0}) N^{-1}(b_{})\)._

This assumption is a generalization of the separability condition in Refs. [29; 30]. Adversarial training can typically achieve this separability in practice, _i.e._, the model can fit adversarial examples of the training dataset, making the above assumption reasonable. Then, we obtain the following lemma.

**Lemma 9** (Convergence to the direction of a KKT point).: _Consider the hypothesis class \(\) in Eq. (6). Under Assumptions 2 and 8, the limit point of normalized weights \(\{}(t)/}(t) _{}:t 0\}\) of the GF for Eq. (13), i.e., the empirical adversarial risk with scale invariant adversarial perturbations \(_{i}(})\), is aligned with the direction of a KKT point of the minimization problem:_

\[_{}}} _{}^{2},y_{i}f(}_{i}+_{i}( });}) 1, i=1,,N.\] (14)

Building upon Lemma 9, we can establish that highly over-parameterized t-NNs undergoing adversarial training with GF will exhibit an implicit bias towards transformed low-rank weights.

**Theorem 10** (Implicit low-rankness for t-NNs induced by GF).: _Suppose there is an example \(}_{i}\) satisfying \(\|}_{i}\|_{} 1\) in the training set \(S=\{(}_{i},y_{i})\}_{i=1}^{N}\). Suppose there is a \((J+1)\)-layer (\(J 2\)) ReLU t-NN, denoted by \(g(};})\) with parameters \(}=(}^{(1)},,}^{(J)},)\), satisfying the conditions:_

1. _the dimensionality of the weight tensor_ \(}^{(j)}^{m_{j} m_{j-1}}\) _of the_ \(j\)_-th_ \(t\)_-product layer satisfies_ \(m_{j} 2\)_,_ \(j=1,,J\)_;_
2. _there is a constant_ \(B_{v}>0\)_, such that the Euclidean norm of the weights_ \(}=(^{(1)},,}^{(L)}, )\) _satisfy_ \(\|}^{(j)}\|_{} B_{v}\) _for any_ \(j=1,,J\) _and_ \(\|\|_{2} B_{v}\)_;_
3. _for all_ \(i\{1,,N\}\)_, we have_ \(y_{i}g(}_{i}+_{i}( });}) 1\)_._

_Then, we consider the class of over-parameterized t-NNs \(=\{f(};})\}\) defined in Eq. (5) satisfying_

1. _the number_ \(L\) _of t-product layers is much greater than_ \(J\)_;_
2. _the dimensionality of weight_ \(}^{(l)}^{d_{l} d_{l-1}}\) _satisfies_ \(d_{l}_{j J}\{m_{j}\}\) _for any_ \(l L\)_._

_Let \(}^{*}=(}^{*(1)},,}^{*(L)},^{*})\) be a global optimum of Problem (14). Namely, \(}^{*}\) parameterizes a minimum-norm t-NN \(};}^{*})}\) that labels the perturbed training set correctly with margin 1 under scale invariant adversarial perturbations. Then, we have_

\[^{L}(r_{}(}^{*(l)}_ {M}))^{-1/2}})(})^{})(L-J)}}-},\]

_where \(}^{*(l)}_{M}\) denotes the \(M\)-block-diagonal matrix of weight tensor \(}^{*(l)}\) for any \(l=1,,L\)._

By the above theorem, when \(L\) is sufficiently large, the harmonic mean of the square root of the stable rank of \(}^{*(l)}_{M}\), _i.e._, the \(M\)-block-diagonal matrix of weight tensor \(}^{*(l)}\), is approximately bounded by \(}\), which is significantly smaller than the square root of the dimensionality \(,cd_{l-1}\}}\) according to condition _(C.5)_ in Theorem 10. Thus, \(f(};}^{*})\) has a nearly low-rank parameterization in the transformed domain. In our case, the weights \(}()\) generated by GF tend to have an infinite norm and to converge in direction to a transformed low-rank solution. Moreover, note that the ratio between the spectral norm and the F-norm is invariant to scaling, and hence it suggests that after a sufficiently long time, GF tends to reach a t-NN with transformed low-rank weight tensors. Refer to Sec. A.2 for numerical evidence supporting Theorem 10.

### Robust Generalization with Approximate Transformed Low-rank Parameterization

Theorem 10 establishes that for highly over-parameterized adversarial training with GF, well-trained t-NNs exhibit approximately transformed low-rank parameters under specific conditions. In this section, we analyze the AGP of t-NNs that possess an approximately transformed low-rank parameterization7.

Initially, by employing low-tubal-rank tensor approximation , one can always compress an _approximately_ low-tubal-rank parameterized t-NN \(f\) by a t-NN \(g_{}\) with an _exact_ low-tubal-rank parameterization, ensuring a small distance between \(g\) and \(f\) in the parameter space. Now, the question is: _Can the small parametric distance between \(f\) and \(g\) also indicate a small difference in their adversarial generalization behaviors?_ To answer this question, we first define the \((,)\)-approximate low-tubal-rank parameterized functions.

**Definition 5** (\((,)\)-approximate low-tubal-rank parameterization).: _A t-NN \(f(};})\) with weights \(}=(,}^{(1)},, }^{(L)})\) is said to satisfy the \((,)\)-approximate low-tubal parameterization with tolerance \(>0\) and rank \(=(r_{1},,r_{L})^{}^{L}\), if there is a t-NN \(g(};}_{})_ {}\) whose weights \(}_{g}=(,}^{(1)}_{r_{1}}, ,}^{(L)}_{r_{L}})\) satisfy \(\|}^{(l)}_{r_{l}}-}^{(l)}\|_{ }\) for any \(l=1,,L\)._

Furthermore, let's consider the collection of t-NNs with approximately low-tubal-rank weights

\[_{,}:=\{f f(,)\}\.\] (15)Subsequently, we analyze the AGP for any \(f_{,}\) in terms of its low-tubal-rank compression \(g_{}\). The idea is motivated by the work on compressed bounds for non-compressed but compressible models , originally developed for generalization analysis of NNs for standard training.

Under Assumption 2, we first define \(_{,}^{}:=\{:(,y) _{R_{}(^{}-)}yf(^{}) f_{,}\}\) as the adversarial version of \(_{,}\). To analyze the AGP of \(f_{,}\) through \(g_{}\), we instead consider their adversarial counterparts \(_{,}^{}\) and \(_{}^{}\), where \(_{}^{}\) is defined as \(_{}^{}:=\{:(,y) _{R_{}(-^{})}yg(^{} ) g_{}\}\). Define the Minkowski difference of \(_{,}^{}\) and \(_{}^{}\) as \(_{,}^{}-_{}^{ }:=\{-_{, }^{},\;_{}^{ }\}\). The empirical \(L_{2}\)-norm of a t-NN \(h\) on the training data \(S=\{(}_{i},y_{i})\}_{i=1}^{N}\) is defined as \(\|h\|_{S}:=_{i=1}^{N}h^{2}(}_{i},y_{i})}\), and the population \(L_{2}\)-norm is \(\|h\|_{L_{2}}:=_{P(,y)}[h^{2}( },y)]}\). Define the local Rademacher complexity of \(_{,}^{}-_{}^{ }\) of radius \(>0\) as \(_{}(_{,}^{}-_{}^{}):=_{N}(\{h_{,}^{}-_{}^{}\|h\|_{L_{2}} \}),\) where \(_{N}()\) denotes the average Rademacher complexity of a function class \(\).

The first part of the upcoming Theorem 12 shows that a small parametric distance between \(f\) and \(g\) leads to a small empirical \(L_{2}\)-distance in the adversarial output space. Specifically, for any \(f(;})_{,}\) with compression \(g(;}_{})\), their (adversarial) empirical \(L_{2}\)-distance \(\|(;})-(; }_{})\|_{S}\) can be bounded by a small constant \(}>0\) in linearity of \(\). We also aim for a small population \(L_{2}\)-distance by first assuming the local Rademacher complexity \(_{}(_{,}^{}-_{}^{})\) can be bounded by a concave function of \(\), following common practice in Rademacher complexity analysis .

**Assumption 11**.: _For any \(>0\), there exists a function \(():[0,)[0,)\) such that \(_{}(_{,}^{}-_{}^{})()\;\;\;\;( 2) 2().\)_

We further define \(_{*}=_{*}(t):=\{>0\;16B_{f }^{-2}()+B_{f}^{-1}+2tB_{ f}^{2}^{-2}/N 1/2\}\) for any \(t>0\), such that the population \(L_{2}\)-norm of any \(h_{,}^{}-_{}^{ }\) can be bounded by \(\|h\|_{L_{2}}^{2} 2(\|h\|_{S}^{2}+_{*}^{2})\) using the peeling argument [42, Theorem 7.7]. We then establish an adversarial generalization bound for approximately low-tubal-rank t-NNs as follows.

**Theorem 12** (Adversarial generalization bound for general approximately low-tubal-rank t-NNs).: _(I). For any \(f_{,}\) with adversarial proxy \(_{,}^{}\), there exists a function \(g_{}\) with adversarial proxy \(_{}^{}\), such that the empirical \(L_{2}\)-distance \(\|-\|_{S} B_{}_{l=1}^{L}B_{l}^{-1}= }\)._

_(II). Let \(}:=}^{2}+_{*}^{2})}\). Under Assumptions 1, 2, 4, 11, there exist constants \(C_{1},C_{2}>0\) satisfying_

\[^{}(f)-}^{ }(f)&L_{}B_{}}{ }_{l=1}^{L}r_{l}(d_{l-1}+d_{l})(9(L+1))}+B }}_{}\\ &+((})+L_{} }}+B_{}}{N})} _{},\] (16)

_for any \(f_{,}\) with probability at least \(1-4e^{-t}\) for any \(t>0\), where \(()\) is defined as_

\[():=_{N}(\{- _{,}^{}, _{}^{},\|-\|_{L_{2}} \}).\]

The main term of the bound quantifies the complexity of functions in \(_{}\) with exact low-tubal-rank parameterization in adversarial settings, which can be significantly smaller than that of \(_{,}\). On the other hand, the bias term captures the sample complexity required to bridge the gap between approximately low-tubal-rank parameterized \(_{,}\) and exactly low-tubal-rank parameterized \(_{}\). As we usually observe \(_{*}^{2}=o(1/)\), setting \(}=o_{p}(1)\) allows the bias term to decay faster than the main term, which is \(O(1/)\). Theorem 12 suggests that _a small parametric distance between \(f_{,}\) and \(g_{}\) also implies a small difference in their adversarial generalization behaviors_.

**A special case.** We also showcase a specific scenario where the weights of t-product layers exhibit a polynomial spectral decay in the transformed domain, leading to a considerably small AGP bound.

**Assumption 13**.: _Consider the setting where any \(t\)-NN \(f(;})_{,}\) has tensor weights \(}^{(l)}\)\((l=1,,L)\) whose singular values in the transformed domain satisfy \(_{j}(M(}^{(l)})_{:,:,k}) V_{0} j^{-}\), where \(V_{0}>0\) is a constant, and \(_{j}()\) is the \(j\)-th largest singular value of a matrix._

Under Assumption 13, the weight tensor \(}^{(l)}\) can be approximated by its optimal tubal-rank-\(r_{l}\) approximation \(}^{(l)}_{r_{l}}\) for any \(1 r_{l}\{d_{l},d_{l-1}\}\) with error \(\|}^{(l)}-}^{(l)}_{r_{l}}\|_{ }/(2-1)}V_{0}(r_{l}-1)^{(1-2)/2}\), which can be much smaller than \(\|}^{(l)}_{r_{l}}\|_{}\) when \(>1/2\) is sufficiently large. Thus, we can find an exactly low-tubal-rank parameterized \(g_{,}\) for any \(f_{,}\) satisfying Assumption 13, such that the parametric distance between \(g\) and \(f\) is quite small. The following theorem shows that the small parametric distance also leads to a small AGP.

**Theorem 14**.: _Under Assumptions 1, 2, 4, and 13, if we let \(}=V_{0}B_{}_{l=1}^{L}(r_{l}+1)^{-}B_{l}^{-1}\), then for any t-NN \(f_{,}\), there exists a function \(g_{}}\) whose t-product layer weights have tubal-rank exactly no greater than \(r_{l}\), satisfying \(\|-\|_{S}}\). Further, there is a constant \(C_{}\) only depending on \(\) such that the AGP, i.e., \(^{}(f)-}^{}(f)\), of any \(f_{,}\) can be upper bounded by_

\[C_{}L_{}B_{}E_{1}+}}+ E_{2}^{}(B_{}^{}+1 )+}^{}}+(}+})}+}}{N }},\]

_for any \(t>0\) with probability at least \(1-4e^{-t}\), where \(E_{1}=N^{-1}_{l=1}^{L}r_{l}(d_{l}+d_{l-1})(9NLB_{}/ })\) and \(E_{2}=N^{-1}_{l=1}^{L}(LV_{0}B_{}B_{l}^{-1}) ^{1/}(d_{l}+d_{l-1})(9NLB_{}/})\)._

This suggests that by choosing a sufficiently large \(>1/2\), where each weight tensor has a tubal-rank close to 1, we can attain a superior generalization error bound. It is important to note that the rank \(r_{l}\) can be arbitrarily chosen, and there exists a trade-off relationship between \(}\) and \(E_{1}\). Therefore, by selecting the rank appropriately for a balanced trade-off, we can obtain an optimal bound as follows.

**Corollary 15**.: _Under the same assumption to Theorem 14, if we choose the parameter \(\) of tubal ranks in \(_{}\) by \(r_{l}=\{(LV_{0}B_{}B_{l}^{-1})^{1/} ,d_{l},d_{l-1}\}\), then there is a constant \(C_{}\) only depending on \(\) such that the AGP of any \(f_{,}\) can be upper bounded as_

\[^{}(f)-}^{}(f)  C_{}L_{}B_{}^{1-1/(2)}_{l=1}^{L}(LV_{0}B_{l}^{-1})^{1/}(d_{l}+d _{l-1})(9NLB_{}/})}{N}}\] \[+E_{2}^{}(B _{}^{}+1)+}+}}+}}{N}},\]

_with probability at least \(1-4e^{-t}\) for any \(t>0\)._

It is worth highlighting that the bound exhibits a linear dependency on the number of neurons in the t-product layers, represented as \(O(_{l}(d_{l}+d_{l-1})/N})\). In contrast, Theorem 5 demonstrates a dependency on the total number of parameters, denoted as \(O(_{l}d_{l}d_{l-1}/N})\). This observation suggests that employing the low-tubal-rank parameterization can potentially enhance adversarial generalization for t-NNs.

## 5 Related Works

**T-SVD-based data and function representation.** The unique feature of t-SVD-based data representation, in contrast to classical low-rank decomposition methods, is the presence of low-rankness in the transformed domain. This transformed low-rankness is crucial for effectively modeling real multi-channel data with both smoothness and low-rankness [24; 49; 50]. Utilized in t-product layers in DNNs [32; 36; 53], t-SVD has also been a workhorse for function representation and achieves impressive empirical performance. While t-SVD-based signal processing models have been extensively studied theoretically [13; 24; 40; 50; 60], the t-SVD-based learning model itself has not been thoroughly scrutinized until this paper. Hence, this study represents the first theoretical analysis of t-SVD-based learning models, contributing to the understanding of their theoretical foundations.

**Theoretical analysis methods.** Our analysis draws on norm-based generalization analysis  and implicit regularization of gradient descent-based learning  as related theoretical analysis methods. Norm-based generalization analysis plays a crucial role in theoretical analysis across various domains, including standard generalization analysis of DNNs , compressed models , non-compressed models , and adversarial generalization analysis [55; 59; 3]. Our work extends norm-based tools to analyze both standard and adversarial generalization in t-NNs, going beyond the traditional use of matrix products. For implicit regularization of gradient descent based learning, extensive past research has been conducted on implicit bias of GF for both standard and adversarial training of homogeneous networks building on matrix product layers, respectively [16; 30; 45]. We non-trivially extend these methods to analyze t-NNs and reveals that GF for over-parameterized ReLU t-NNs produces nearly transformed low-rank weights under scale invariant adversarial perturbations.

Our theoretical results notably deviate from the standard error bounds for fully connected neural networks (FNNs) in several ways:

* The generalization bounds in Lemma 3 and Theorem 5 for t-NNs diverge from their counterparts for FNNs in Refs. [8; 55; 59] due to the channel number c in t-NNs. Moreover, Theorem 5 encompasses a wider range of adversary classes than the \(l_{p}\)-attacks in the aforementioned references.
* The uniqueness of Theorem 6, compared to Refs. [55; 59], stems from its consideration of weight low-rankness in the adversarial generalization bound, suggesting possible robustness improvements in generalization.
* Our exploration of the implicit bias in GF for adversarial training presents a novel angle: the bias towards approximate transformed low-rankness in t-NNs. While Ref.  focuses on the implicit bias in adversarial training for FNNs, centered on KKT point convergence with exponential loss, our work delves deeper, considering a wider array of loss functions in adversarial training for t-NNs.
* A crucial distinction in our adversarial generalization bounds, detailed in Section 4.3, from non-adversarial bounds for FNNs  is the integration of the localized Rademacher complexity. This encompasses the Minkowski difference between adversarial counterparts of both approximately and exactly low-tubal-rank t-NNs as seen in Theorem 12.

## 6 Concluding Remarks

A thorough investigation of the generalization behavior of t-NNs is conducted for the first time. We derive upper bounds for the generalization gaps of standard and adversarially trained t-NNs and propose compressing t-NNs with a transformed low-rank structure for more efficient adversarial learning and tighter bounds on the adversarial generalization gap. Our analysis shows that adversarial training with GF in highly over-parameterized settings results in t-NNs with approximately transformed low-rank weights. We further establish sharp adversarial generalization bounds for t-NNs with approximately transformed low-rank weights. Our findings demonstrate that utilizing the transformed low-rank parameterization can significantly enhance the robust generalization of t-NNs, carrying both theoretical and empirical significance.

**Limitations.** While this paper adheres to the norm-based framework for capacity control [8; 37], it is worth noting that the obtained generalization bounds may be somewhat conservative. However, this limitation can be mitigated by employing more sophisticated analysis techniques, as evidenced by recent studies [2; 56; 57; 25].

**Discussions.** The inclination of adversarial training towards low-rank/sparse weights, and the reciprocal effects of parameter reduction on robustness, are currently at the forefront of ongoing research. This domain has witnessed a spectrum of observations and results [6; 23; 41; 51]. In this study, we propose that employing low-rank parameterization can enhance the adversarial robustness of t-NNs, as evidenced by our analysis of uniform adversarial generalization error bounds. However, despite these promising results, it is crucial to emphasize the necessity of a more exhaustive exploration of low-rank parameterization. Its implications, particularly when considered in the context of approximation, estimation, and optimization, are profound and warrant further dedicated research efforts. Such a comprehensive investigation will undoubtedly enhance our understanding and fully unlock the potential of low-rank parameterization in neural networks.