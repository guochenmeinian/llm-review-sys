# Adaptive Contextual Perception: How to Generalize to New Backgrounds and Ambiguous Objects

Zhuofan Ying\({}^{1,2}\)   Peter Hase\({}^{1}\)   Mohit Bansal\({}^{1}\)

\({}^{1}\)UNC Chapel Hill  \({}^{2}\)Columbia University

{zfying, peter, mbansal}@cs.unc.edu

###### Abstract

Biological vision systems make adaptive use of context to recognize objects in new settings with novel contexts as well as occluded or blurry objects in familiar settings . In this paper, we investigate how vision models adaptively use context for out-of-distribution (OOD) generalization and leverage our analysis results to improve model OOD generalization. First, we formulate two distinct OOD settings where the contexts are either _irrelevant_ (Background-Invariance) or _beneficial_ (Object-Disambiguation), reflecting the diverse contextual challenges faced in biological vision. We then analyze model performance in these two different OOD settings and demonstrate that models that excel in one setting tend to struggle in the other. Notably, prior works on learning causal features improve on one setting but hurt in the other. This underscores the importance of generalizing across both OOD settings, as this ability is crucial for both human cognition and robust AI systems. Next, to better understand the model properties contributing to OOD generalization, we use representational geometry analysis and our own probing methods to examine a population of models, and we discover that those with more factorized representations and appropriate feature weighting are more successful in handling Background-Invariance and Object-Disambiguation tests. We further validate these findings through causal intervention, manipulating representation factorization and feature weighting to demonstrate their causal effect on performance. These results show that interpretability-based model metrics can predict OOD generalization and are causally connected to model generalization. Motivated by our analysis results, we propose new data augmentation methods aimed at enhancing model generalization. The proposed methods outperform strong baselines, yielding improvements in both in-distribution and OOD tests. We conclude that, in order to replicate the generalization abilities of biological vision, computer vision models must have factorized object vs. background representations and appropriately weight both kinds of features.1

## 1 Introduction

Humans can recognize objects even when the objects appear in novel settings like new viewpoints, lighting, and background conditions . This kind of generalization has been the subject of many studies in computer vision, where past methods have optimized models to avoid relying on "spurious correlations" (such as the object's background) in order to encourage generalization . However, human vision can also generalize to settings where objects are occluded, blurred, or otherwise distorted, as illustrated by the blurred keyboard in Fig. 1. In fact, research in psychology shows that humans accomplish this task largely _by relying on background information_, which helps disambiguate the identity of the foreground object .

How can humans both ignore backgrounds to recognize objects in _new contexts_ as well as rely on background information to recognize _ambiguous objects_? Past work suggests that this _adaptive use of context_ is possible via the _factorization_ of foreground objects versus background contexts in visual representations . Factorized representations have the property that the features of the data are represented in different subspaces in the feature space. Specifically, for a model with representations in \(^{d}\), with two orthogonal subspaces \(V^{d}\) and \(V^{}\), we say the model factorizes its foreground and background representations when foreground information is represented in the subspace \(V\) and background information is represented in \(V^{}\).

The bases for the subspaces \(V\) and \(V^{}\) do not have to align with the dimensions of the feature vectors (i.e. the standard basis of \(^{d}\)), nor do representations have to be sparse, unlike common definitions of modularity or disentanglement . Bengio et al.  suggest that factorization should broadly improve model generalization; we state this hypothesis specifically for the adaptive use of context as the _Factorization Hypothesis_:

**Factorization Hypothesis**: factorized representation is necessary for adaptive use of context.

This is not to say that factorized representation is sufficient for generalization. A model must also use the factorized representations in a certain way to generalize to both new contexts and ambiguous objects. In human vision, object-context interactions occur at very small timescales, suggesting simple feedforward weighting of evidence is sufficient . For computer vision, we argue that the object representation typically provides strong evidence for the object's identity, while the context representation can serve as a _tie-breaking_ feature that provides a small amount of evidence that is useful when the features of the object itself are ambiguous (all within a single forward pass). In this view, background information becomes useful for identifying objects when they are occluded, blurry, far away, or otherwise difficult to recognize. We state this view as the _Feature Weighting Hypothesis_:

**Feature Weighting Hypothesis**: model predictions should depend on both object and context features, with stronger weight given to object features but non-zero weight on context features.

In this paper, we argue that models generalize to new contexts and ambiguous objects by following the Factorization Hypothesis and the Feature Weighting Hypothesis. Experimentally, we (1) design testing procedures that capture generalization to both new contexts and ambiguous objects, (2) show that current models' generalization is predicted by how factorized their representations are and how they weight object/context features, and (3) design novel objectives for improving model generalization based on these hypotheses.

Figure 1: Feature factorization and appropriate feature weighting support Adaptive Contextual Perception. Humans can flexibly generalize to object settings where the context is irrelevant (2b) or helpful (2c). To achieve similar generalization, models should have factorized representations (1a) with strong weights on object features and small but non-zero weights on background features (1b). (2a) In-distribution: In natural environments, foreground and background information correlates strongly. (2b) OOD: Background-Invariance. When the background is inconsistent with the foreground object, humans can ignore the background . (2c) OOD: Object-Disambiguation. When the object is hard to recognize on its own (like the highly blurred keyboard), humans can rely on the background (office setting) to infer what the object is .

First, we formulate two kinds of OOD generalization settings where the context is either irrelevant or helpful: Background-Invariance and Object-Disambiguation, which are illustrated in Fig. 1. In Background-Invariance, foreground objects and the background contexts are randomly paired (i.e., objects in new contexts), breaking spurious correlations observed in naturalistic training distributions. In Object-Disambiguation, the object itself is hard to recognize or ambiguous (due to motion, lighting conditions, noise, etc.), while the background context is of the kind usually observed for the object. We find that there is a fundamental tradeoff in performance on our Background-Invariance vs. Object-Disambiguation tests. This poses a problem for past work, which so far has only evaluated on one setting or the other. We show that methods focusing on Background-Invariance [2; 38; 21; 42] will hurt performance on Object-Disambiguation without realizing it, and vice versa . Following how human vision generalizes, we should aim for a single computer vision system that generalizes well to both OOD settings.

Second, we test the Factorization and Feature Weighting hypotheses by training a population of models on two standard benchmarks, ColorObject and SceneObject [51; 27]. For each model, we measure factorization using interpretability methods including linear probes, representational similarity analysis , and a geometric analysis . To test for feature weighting, we measure model sensitivity to foreground vs. the background. We do so by randomly switching the foreground or background and measuring the relative differences in the changes to model representations. Results show that more factorized models with appropriate feature weighting tend to achieve higher performances on both OOD settings, meaning they adaptively use the context to generalize to novel settings. We further confirm this finding by a causal analysis where we directly manipulate model representations.

Third, we design novel objectives for improving model generalization on Background-Invariance and Object-Disambiguation tests, motivated by the results of our factorization and feature weighting analysis. In order to encourage model factorization, we augment the data with random-background and background-only images (using ground-truth masks or pseudo-masks from SAM ), and we weigh the corresponding objective terms to encourage appropriate feature weighting of the foreground and background information. On two benchmark datasets, our method improves significantly over strong baselines on one OOD setting while matching performance on the other.

We summarize our main findings as follows:

1. We show that models that do better on Background-Invariance tend to do worse on Object-Disambiguation and vice versa, highlighting a need to design systems that generalize across both OOD settings, similar to biological vision (Sec. 5).
2. We obtain both correlational and causal evidence for our Factorization and Feature Weighting hypotheses: more factorized models achieve better OOD generalization, and this requires models to assign appropriate weight to foreground features over background features (Sec. 6).
3. We design new model objectives motivated by the Factorization and Feature Weighting hypotheses. We find these objectives achieve a Pareto improvement over strong baselines, increasing performance on one OOD setting while matching performance on the other (Sec. 7).

## 2 Related Works

**Evaluating vision model generalization**. Much research has evaluated the generalization capabilities of vision models in various OOD settings [37; 13; 12]. In particular, several benchmarks have been proposed for evaluating OOD generalization, such as ImageNet-C and ImageNet-9 [15; 46]. However, most prior works fail to test on OOD settings where spurious features like background are beneficial [38; 2; 21]. Liu et al.  test on an extreme version of Object-Disambiguation, where the foreground object is completely masked out, but they do not test on the OOD setting where backgrounds are irrelevant. We evaluate vision models on two kinds of OOD settings where the context is either helpful or irrelevant. To our knowledge, no prior works have evaluated models in both settings.

**Understanding how vision models generalize**. Various works have explored the relationship between model properties, inductive biases, and generalization capabilities [32; 50; 16]. Many recent works try to predict OOD performance using in-distribution (ID) measures like model invariance and smoothness [45; 10; 33], but they do not control for in-distribution accuracy which has historically been the best predictor of OOD accuracy . More recent work that controls for ID accuracy suggests that properties like model invariance, calibration, and robustness do _not_ predict OOD generalization better than ID accuracy . Moreover, past analysis is generally correlational in nature. In this paper, we use interpretability methods to measure factorization and feature weighting metrics in vision models and analyze both _correlational_ and _causal_ relations to generalization in our two OOD settings. We do not know of an analysis that demonstrates a causal relationship between model properties and OOD generalization. In their review, Bengio et al.  hypothesize that models with more factorized representations should generalize better, but they do not test this experimentally. Our work provides empirical support for this claim specifically for the adaptive use of context for generalization.

**Improving generalization**. Various methods have been proposed to improve the OOD generalization of deep learning models. One line of research relying on metadata of multiple training domains to learn the causal (or 'invariant') features from multiple domains . Other methods make use of data augmentation to improve both ID and OOD performances, like MixUp , CutMix , and AutoAugment . However, these works do not test on OOD settings where spurious features like background are beneficial. We show in Sec. 5 that methods designed to learn causal features either do not improve or even hurt the performances in the Object-Disambiguation setting. Our augmentation method is built on the insight that both causal and spurious features can be useful, and these features need to be weighted properly so that the models can generalize to both OOD settings.

## 3 Problem Setup: Two OOD Generalization Settings for Vision Models

Here we introduce the training and testing environments that we would like vision models to perform well in, inspired by biological vision systems. Specifically, we consider scenarios where there are test-time shifts in (1) the correlations between foreground objects and background contexts and (2) the reliability of causal features (features of foreground objects), which can be corrupted by shifts in image lighting, blurring, object occlusion, or other sources of noise. We show an illustrative example in Figure 1: the butterfly is easily recognizable regardless of the background; the keyboard looks like a white blob in isolation but can be easily recognized when put in context. We use train and test sets constructed based on these scenarios to understand how vision systems can effectively use both the causal features of the foreground objects and the spurious features of the background contexts to adaptively generalize to domain shifts.

Notation.We use \(X\) and \(Y\) to denote random variables of the input and the output, while \(\) and \(y\) denote data samples. We assume there is a set of environments \(\) (instantiated as datasets), which can be divided into in-distribution (ID) and out-of-distribution (OOD) sets: \(_{id},_{ood}\). The models are trained on \(_{id}\), and our goal is to learn a function \(f:X Y\) that generalizes well to \(_{ood}\).

Data construction process.We consider settings where the inputs \(X^{e}\) from environment \(e\) are generated from latent variables \(Z^{e}_{c},Z^{e}_{s}\), where \(Z^{e}_{c}\) denotes the causal foreground class while \(Z^{e}_{s}\) denotes the spurious background class. The label \(Y^{e}\) is equivalent to \(Z^{e}_{c}\) (analogous to e.g. an object recognition task where the label is the object identity). The input \(\) contains both causal features (foreground object) \(_{c}\) and spurious features (background context) \(_{s}\). For each sample

Figure 2: The visualization of ColorObject (left) and SceneObject (right) datasets and their corresponding OOD test sets. For in-distribution sets (1a, 2a), there are certain natural co-occurrence probabilities between the foreground object classes and the background classes (e.g. 80%, shown by the random background of the last column of (1a, 2a)). For Background-Invariance sets (1b, 2b), there is no correlation between the foreground and background. For Object-Disambiguation sets (1c, 2c), there is a perfect correlation between the foreground and the background classes, but the foreground objects themselves are hard to recognize due to natural corruptions

\(Z^{e}_{c}=z^{e}_{c}\) and \(Z^{e}_{s}=z^{e}_{s}\), we generate causal and spurious inputs by \(_{c}=G_{c}(z^{e}_{c})\) and \(_{s}=G_{s}(z^{e}_{s})\), where \(G_{c}\) and \(G_{s}\) are maps from the latent space to the input space. We then corrupt the causal features with the random function \(\) (for example, by blurring or adding noise to the image). Finally, we generate the complete input \(=G((_{c}),_{s})\), where \(G\) combines the causal and spurious features (e.g. overplays a foreground object onto a background image). Each environment differs in its co-occurrence probability \(p_{co}\) between \(Z^{e}_{c}\) and \(Z^{e}_{s}\), and the level of corruption applied to the causal features \(_{c}\). For ID environments \(e_{id}\), we assume high co-occurrence probability and no corruption in the causal feature, identical to past works [38; 51].

Two OOD Settings.For OOD environments \(e_{ood}\), we use two particular settings where the spurious background features are either irrelevant or helpful. We call them Background-Invariance and Object-Disambiguation settings, respectively. To create these OOD settings for vision datasets, we need additional annotation of the background classes and the segmentation mask of the foreground objects. We assume we have access to these metadata in all our experiments.

**Background-Invariance.** In our Background-Invariance setting, foreground objects are randomly paired with background contexts (the co-occurrence probability \(p_{co}=0\)). We assume there is no noise to the causal features in this setting (\(\) is an identity function). Here, models need to mainly rely on the causal features \(_{c}\) to achieve good performances. See Figure 1 and 2 for examples.

**Object-Disambiguation.** In our Object-Disambiguation setting, the noise level of the causal features is very high while the co-occurrence probability is also high (like it is in natural images). Here, models need to learn the correspondence between the foreground and background features and make use of the spurious contextual features \(_{s}\) to achieve good performance. That is, models must mimic how humans rely on context for object recognition given that objects in the real world can vary significantly in appearance due to factors such as lighting conditions, occlusions, and viewpoint changes [6; 17; 44; 35]. Contextual information, including scene layout, spatial relationships, and semantic associations allows humans to make educated guesses and disambiguate the identity of objects. For example, in Figure 1, the keyboard itself is highly blurred and hard to recognize, but it easily recognizable in context. Also see Figure 2 for examples in our datasets.

## 4 Experiment Setup

Datasets.We conduct our analysis and experiments on two standard benchmarks, ColorObject and SceneObject[38; 51]. The datasets and their experimental settings follow the conventions in the IRM literature [2; 21; 38], but we add a novel Object-Disambiguation test by using the available metadata. Please refer to Appendix A for further details of the experimental settings.

To set up the ColorObject dataset, following prior works , we build a biased dataset using 10 classes of objects selected from MSCOCO dataset , which are then superimposed onto 10 colored backgrounds. We set a one-to-one relationship between the foreground classes and the background colors. The co-occurrence probabilities \(p_{co}\) are 1.0 and 0.7 for training (i.e., data from these two environments make up the training data), 0.0 for Background-Invariance, and 1.0 for Object-Disambiguation (see Sec. 3 for definitions). To create the Object-Disambiguation setting for ColorObject dataset where the object itself is hard to recognize, we apply 15 kinds of natural corruptions (like motion blur, contrast change, and fog and snow effects) from Hendrycks and Dietterich  to the foreground object before superimposing it on the background image. The natural corruptions come in 5 levels, and we use the average accuracy across 5 levels as the final result. There are 16000 images for training, 2000 for validation, and 2000 for each of the test sets. See Appendix for accuracies across different levels of distribution shift. C. See Figure 2 for examples of ID, Background-Invariance, and Object-Disambiguation images.

To set up the SceneObject dataset, following prior works , we use objects from 10 classes of the MSCOCO dataset and superimpose them onto scene background from 10 classes of the Places dataset to create the SceneObject dataset [54; 26]. We bind each object category to one scene category. The rest of the data construction is the same as for ColorObject, except for that the co-occurrence probabilities \(p_{co}\) are 0.9 and 0.7 for training ColorObject. See Figure 2 for examples.

Model Population.Following prior works, we use Wide ResNet 28-2 for the two datasets . Using this model backbone, we train models based on the following methods: empirical risk minimization (ERM), IRM, VREx, GroupDRO, Fish, MLDG, and our own augmentation method (described in Sec. 7). The models are trained on a fixed set of training environments using different loss functions, random seeds, and other hyperparameters. Note that, across datasets, the noise level and the \(p_{co}\) in the training sets do not cover the noise level and the \(p_{co}\) in the two OOD test sets, so the OOD test sets remain out-of-distribution. We trained a total of 340 models, 170 for each of ColorObject, and SceneObject datasets. See Appendix A for more details.

## 5 Negative Correlation Between Background-Invariance and Object-Disambiguation Accuracy

Design.In this section, we evaluate a population of models in our two OOD settings where the background is either helpful or irrelevant, namely Background-Invariance and Object-Disambiguation. As far as we know, no previous work has evaluated models along both of these dimensions of generalization. We use the population of 170 models for each dataset, described in Sec. 4. We aim to compare how models tend to perform on the two OOD settings.

Results.As shown in Figure 3, models that perform better on the Background-Invariance setting tend to perform worse on the Object-Disambiguation setting, and vice versa. The Pearson correlation coefficients are -0.938 for ColorObject and -0.877 for SceneObject, showing a strong negative correlation between the performances in the two OOD settings. The results are consistent when using a Transformer-based model (Swin) and across training sizes as well (see Appendix Table 7) . Additionally, we find that methods designed to learn causal features, such as IRM, VREx, GroupDRO, Fish, and MLDG, demonstrate improved performance in the Background-Invariance setting, but they significantly hurt Object-Disambiguation performance on ColorObject compared to ERM and four of five lower performances on SceneObject (see Appendix Table 3). These results highlight the need to make a choice about the tradeoff between foreground and background depending on the environment that is expected for deployment.

## 6 Understanding OOD Generalization: Which Vision Models Generalize Better And Why?

In this section, we analyze why some models generalize better than others by testing the Factorization and Feature Weighting hypotheses (see Sec. 1). We do this through a regression analysis that shows that metrics for model factorization and feature weighting help predict OOD generalization, while controlling for in-distribution accuracy and model objectives. We also show that the relationship between the metrics and OOD generalization is causal by directly manipulating feature factorization and feature weighting. Later in Sec. 7, we leverage this analysis to improve model generalization via new objectives. We use the same datasets and population of models as the previous section (Sec. 5).

### Regression Analysis

Using our population of models, we a fit simple linear regression that predicts a model's average accuracy between the two OOD settings based on several metrics measured on ID test data. Each metric can be measured for a given hidden layer of a model; we measure each metric across the 3 blocks of Wide-ResNet and include each measurement as a variable in the regression. As a result, we have 9 factorization metrics and 4 feature weighting metrics (see Appendix B for the full list). In total, the regression variables we consider include ID accuracy, model objectives, and our proposed factorization and feature weighting metrics. For each dataset, we run a bootstrap with 10k samples,

Figure 3: Background-Invariance vs. Object-Disambiguation performances of all models in the ColorObject (left), and SceneObject (right). Models that do better in one OOD setting tend to do worse in the other, showing strong negative correlations.

using 80% of the 170 models for training and 20% for testing, to obtain an average test \(R^{2}\) regression value that we report along with its standard deviation.

Factorization Metrics.We measure model factorization using three metrics described below. These metrics are computed on representations from a given hidden layer of the model.

1. Linear Probing. To measure factorization with a linear probe, we first create foreground-only and background-only inputs using segmentation masks available in our datasets. We then extract model representations and predict the foreground or background class with a linear model. That is, if there are 10 foreground and 10 background classes, we train a linear classifier to do 20-way classifications. The probe classification accuracy measures factorization because if the model represents foregrounds and backgrounds in overlapping subspaces, the linear classifier will mix up the foreground and background classes, leading to low decoding accuracies .
2. Representation Similarity Analysis (RSA). RSA is a method for measuring the separation of inter-class representations and closeness of intra-class representations, which yields more fine-grained information than linear probing . Again, we extract model representations using foreground-only and background-only inputs. Then we compute pairwise similarities of the extracted features with Euclidean distance, giving a _Representation Dissimilarity Matrix (RDM)_. We then calculate the correlation between the model RDM and a predefined _reference RDM_, which represents the ideal factorized representation structure where each foreground/background class has small intra-class distances and large inter-class distances. For details and figure of the reference RDM, see Appendix B.
3. Geometric Analysis. While linear probing and RSA are influenced by class separation _within_ each foreground/background subspace, a geometric method may more directly measure factorization independently of this class separation. We adopt the method from Lindsey and Issa . For complete details, see Appendix B, but in short: we compute the factorization of foregrounds against backgrounds as factorization\({}_{fg}=1-_{fg|bg}}{_{fg}}\), where \(_{fg}\) denotes the variance in the representation space induced by foreground perturbations and \(_{fg|bg}\) denotes the variance induced by foreground perturbations _within the background subspace_. If representations are factorized, changing foregrounds should not affect the background representations, so \(_{fg|bg}\) should be near zero, and factorization\({}_{fg}\) will be near 1.

Feature Weighting Metric.We propose a new method, called the Foreground-Background Perturbation Score (FBPS), to measure the relative weight that a model assigns to foreground features vs. background features. First, drawing on ID test data, we create a set of inputs with random foreground and backgrounds. Then, we randomly switch the foreground of each input to get \(X_{fg\_flipped}\) and measure the distance in model representations (or outputs), \(_{fg}=(f(X),f(X_{fg\_flipped}))\), with dist as the L2 distance (or KL divergence for model outputs). Similarly, we randomly switch the background contexts to create \(X_{bg\_flipped}\) and measure the change in model representations \(_{bg}=(f(X),f(X_{bg\_flipped}))\). Finally, we compute the difference in these terms FBPS \(=_{fg}-_{bg}\). This approach yields four measurements per ResNet, one per block of the model and one for the model output. This metric is computed and averaged over 5000 samples. See Appendix B for more details.

Results.In Table 1, we report \(R^{2}\) values for regressions predicting the average of the Background-Invariance and Object-Disambiguation accuracies, based on a population of models trained on each dataset. The ID accuracy alone obtains an \(R^{2}\) of 0.191 and 0.691 on the two datasets, respectively. The model objectives serve as a control here, showing significant improvement over ID accuracy alone. Meanwhile, adding either feature factorization or feature weighting metrics can significantly improve \(R^{2}\) in predicting average OOD accuracies. But **adding feature factorization and feature weighting metrics together achieves extremely high \(R^{2}\) values of 0.963 and 0.868 for predicting OOD accuracy on

    & \) predicting average OOD Acc} \\  Metric & ColorObject & SceneObject \\  ID Acc & 0.191\(\)0.150 & 0.691\(\)0.091 \\ + Obj. & 0.505\(\)0.106 & 0.696\(\)0.084 \\ + Obj. \& Factor. & 0.925\(\)0.023 & 0.838\(\)0.044 \\ + Obj. \& Ft. Wgt. & 0.961\(\)0.013 & 0.857\(\)0.043 \\ + All & **0.963\(\)0.010** & **0.868\(\)0.041** \\   

Table 1: Regression analysis between metrics and the average OOD accuracy. Confidence intervals are one standard deviation over 10k bootstrapping.

the two datasets. We note that in past studies, various model properties do not predict OOD generalization better than ID acc does [43; 47]. See Appendix C for additional regression visualizations and results in individual OOD settings. Overall, our results suggest that both feature factorization and feature weighting play important roles in models' adaptive use of context.

In Fig. 4, we show the relationship between feature weighting metrics (as measured by FBPS at the final hidden layer of the Wide-ResNet models) and OOD generalization performances in ColorObject. We observe that as the feature weighting metric increases, the performance increases in Background-Invariance and decreases in Object-Disambiguation, resulting in an initially increasing and subsequently decreasing average OOD performance. The trend is the same in SceneObject too (see Appendix C). This shows that **generalization is maximized at a specific feature weighting level**, suggesting the need to find an appropriate weight between foreground and background features. We show that this relationship is also causal in the next subsection.

### Causal Manipulation Analysis

Apart from the regression analysis, we also manipulate factorization and feature weighting in the representational space directly to show their causal effects on performances. We conduct these experiments with 10 ERM models, for both datasets.

Factorization Manipulation.Following Lindsey and Issa , we directly manipulate the factorization of the model representations. We first identify foreground and background subspaces and then rotate the model representations so that the foreground subspace overlaps more with the background subspace. This transformation directly decreases the feature factorization while holding constant other representation properties that may affect model accuracy  (full details in Appendix B). As this intervention applies to the final hidden states of the model, we compute model predictions by applying the linear classifier layer to the transformed representations.

Feature Weighting Manipulation.We manipulate the feature weighting by manipulating the variance in a certain subspace, which would bias a linear classifier with L2 regularization to rely on that subspace more (higher variance) or less (lower variance) [14; 41]. We do so by computing the importance, with respect to a subspace of interest (foreground or background), of each feature in the original space and manipulating them by a boost factor \(g\). Note this step is followed by retraining the final classifier layer using the adjusted hidden representations. See Appendix B for full details.

Results.After we rotate the data to overlap the foreground and background subspaces, feature factorization (measured by the geometric method) drops significantly in SceneObject, as expected. As a result, the generalization performance in Background-Invariance decreases while the

Figure 4: Feature Weighting vs OOD Accuracies. The three plots show the relationship between feature weighting metrics and Background-Invariance, Object-Disambiguation, and the average OOD accuracies in ColorObject. The green lines show the mean and one standard deviation of each bin of data. As models weigh foreground features more, the performance increases in Background-Invariance and decreases in Object-Disambiguation, resulting in an initially increasing and subsequently decreasing average OOD performance. We observe the same trend in SceneObject as well (see Appendix C).

performance in Object-Disambiguation increases. This is also expected since the less factorized model conflates foreground and background information, and background information is irrelevant for Background-Invariance but perfectly correlated with object identity in Object-Disambiguation. Shown in Fig. 5(a), the factorization metric drop by 85.63% (paired t-test: \(p\)\(<\)1e-8), and the Background-Invariance performance drop by 18.24% (\(p\)\(<\)1e-4), while the Object-Disambiguation performance increases by 5.97% (\(p\)\(<\)1e-5). Therefore, we conclude that **the change in performance is a causal result of decreased factorization**. For results on ColorObject, see Appendix C.

As shown in Fig. 5(b), when we decrease model weight on foreground features, the Background-Invariance performance decreases by 0.63 points (paired t-test: \(p\)\(<\)1e-5) while the Object-Disambiguation performance increases by 0.68 points (\(p\)\(<\)1e-5) in SceneObject. This shows that increasing the feature weighting of foreground over background _causally_ helps in Background-Invariance but hurts in Object-Disambiguation. In the other direction, when we decrease reliance on background features, the Background-Invariance performance increases by 0.35 points (\(p\)\(=\)1e-3) while the Object-Disambiguation performance decreases by 0.92 points (\(p\)\(<\)1e-5), showing that decreasing the feature weighting _causally_ hurts Background-Invariance generalization but helps in Object-Disambiguation. These results highlight the dichotomy between the two OOD settings we test, since our interventions improve performance in one setting while hurt in the other. This trend is expected since the background is irrelevant in Background-Invariance (where decreasing the background importance helps), while the background is helpful in Object-Disambiguation (where decreasing background importance hurts). Overall, our results show the causal effect of feature weighting on model generalization, further strengthening our hypothesis.

## 7 Proposed Methods: Objectives for Adaptive Contextual Perception

Finally, in this section, we propose an augmentation method for improving the generalization ability to both OOD settings, based on our analysis in Sec. 6. Like prior methods , our approach leverages additional annotations, including background identity and segmentation masks of foreground objects, which are available in our ColorObject and SceneObject datasets. We also experiment with using Segment-Anything (SAM) to automatically generate segmentation masks to alleviate the need for additional annotation and achieve similar performances (see Appendix C.4.

Figure 5: Causal intervention results using 10 ERM models in SceneObject. (a) Rotating the foreground subspace into the background subspace has the effect of reducing model factorization and changing the OOD accuracies in opposite directions. Lighter-colored bars show metrics after the intervention. (b) Decreasing the foreground (or background) subspace by a factor of 64 has the effect of hurting (or improving) Background-Invariance accuracy and improving (or hurting) Object-Disambiguation accuracy. We observe the same trend in ColorObject (see Appendix Fig. 11).

Augmentation Objectives.Our augmentation method consists of two parts: _random-background_ and _background-only_ augmentation. First, we extract the foreground object from an image and combine it with a randomly chosen background segmented from another image in the batch. This process encourages the model to recognize the object regardless of the background.

The cross-entropy loss for these images is denoted as \(_{fg}\). Second, we create background-only images by replacing the foreground objects with black pixels and then ask the model to predict background labels. This prevents the model from ignoring all backgrounds so that it can still rely on the background when the foreground object is hard to recognize. We denote cross-entropy loss for background-only images as \(_{bg}\). The final loss is: \(=_{0}_{task}+_{1}_{fg}+_{2 }_{bg}\), where \(_{task}\) is the task loss and \(_{0},_{1}\) and \(_{2}\) are scalar weights for different losses. By adjusting the value of \(_{0}\), \(_{1}\), and \(_{2}\), we can find the optimal balance among the original task loss and two augmentation types, enabling the model to learn more effectively from both background and foreground features and adaptively generalize to the two OOD settings. See Appendix A for method and tuning details.

Results.Our proposed augmentation method demonstrates significant improvements in both ID and OOD accuracies compared to ERM and the best baseline, Fish . On ColorObject, we match the best baseline on both OOD settings. Here, ERM still does better on Object-Disambiguation, but on average OOD accuracy, our method improves over ERM by over 5.6 points, matching the best baselines. On SceneObject, we match the best baseline on Background-Invariance while significantly improving over it by 2.5 points on Object-Disambiguation. We also improve upon ERM by 1.9 points on Object-Disambiguation, where no other baseline methods can beat ERM. We also improved the average OOD performance by 1.2 points over ERM and other baselines. We get similar improvements over baselines for our method with pseudo-masks generated by SAM (see Appendix Table 6). And the improvements are even more significant with smaller training sizes (also see Appendix Table 6). In both datasets, our method improves over ERM or the best baseline on feature factorization and feature weighting, suggesting a causal relationship between these model properties and model generalization. For results of varying combination weights for the two augmentations, see Appendix C.4.

## 8 Conclusion

In summary, we find that: (1) Vision models face a tradeoff in performing well on Background-Invariance and Object-Disambiguation tests. By evaluating a population of 170 models, we find a strong negative correlation coefficient of about \(r=0.9\) between the accuracies for each OOD test. (2) OOD generalization in models is explained by the Factorization Hypothesis and the Feature Weighting Hypothesis. In our correlational analysis, metrics for factorization and feature weighting are highly predictive of OOD generalization (\(R^{2}\) up to.96, controlling for in-distribution accuracy). In our causal analysis, we find that directly manipulating factorization and feature weighting in models allows us to control how well they generalize. (3) Lastly, we propose data augmentation methods that achieve Pareto improvements over strong baselines, as they are able to improve either Background-Invariance or Object-Disambiguation without hurting the other setting.

    &  \\  Method & ID & OOD1 & OOD2 & Avg OOD & Factorization & FBPS \\  ERM & 87.15\(\)0.40 & 29.11\(\)3.39 & **95.24\(\)0.83** & 62.17\(\)1.40 & 71.27\(\)1.14 & 0.012\(\)0.013 \\ Best Baseline & **88.48\(\)0.85** & **41.13\(\)7.27** & 92.65\(\)2.37 & **66.89\(\)2.73** & 72.87\(\)1.58 & 0.044\(\)0.027 \\ Ours & 87.45\(\)0.48 & **42.97\(\)3.02** & 92.02\(\)1.06 & **67.50\(\)1.80** & **76.93\(\)1.33** & 0.047\(\)0.018 \\    \\  Method & ID & OOD1 & OOD2 & Avg OOD & Factorization & FBPS \\  ERM & 72.28\(\)0.93 & 35.37\(\)1.97 & 71.93\(\)1.27 & 53.65\(\)0.70 & 64.07\(\)0.98 & 0.000\(\)0.006 \\ Best Baseline & 73.47\(\)0.79 & **36.91\(\)1.49** & 71.30\(\)0.74 & 54.11\(\)0.61 & 64.95\(\)0.85 & 0.005\(\)0.005 \\ Ours & **74.13\(\)0.48** & **36.83\(\)1.34** & **73.86\(\)1.11** & **55.34\(\)0.51** & **68.22\(\)1.01** & 0.011\(\)0.008 \\   

Table 2: Accuracy of our proposed method (plus/minus one standard deviation over 10 seeds). OOD1 denotes Background-Invariance, and OOD2 denotes Object-Disambiguation