# Learning Efficient Surrogate Dynamic Models with Graph Spline Networks

Chuanbo Hua

KAIST

DiffeqML, AI4CO &Federico Berto

KAIST

DiffeqML, AI4CO &Michael Poli

Stanford University

DiffeqML &Stefano Massaroli

Mila and University of Montreal

DiffeqML &Jinkyoo Park

KAIST

OMELET

###### Abstract

While complex simulations of physical systems have been widely used in engineering and scientific computing, lowering their often prohibitive computational requirements has only recently been tackled by deep learning approaches. In this paper, we present GraphSplineNets, a novel deep-learning method to speed up the forecasting of physical systems by reducing the grid size and number of iteration steps of deep surrogate models. Our method uses two differentiable orthogonal spline collocation methods to efficiently predict response at any location in time and space. Additionally, we introduce an adaptive collocation strategy in space to prioritize sampling from the most important regions. GraphSplineNets improve the accuracy-speedup tradeoff in forecasting various dynamical systems with increasing complexity, including the heat equation, damped wave propagation, Navier-Stokes equations, and real-world ocean currents in both regular and irregular domains.

## 1 Introduction

For a growing variety of fields, simulations of partial differential equations (PDEs) representing physical processes are an essential tool. PDE-based simulators have been widely employed in a range of practical applications, spanning from astrophysics (Mucke et al., 2000) to biology (Quarteroni and Veneziani, 2003), engineering (Wu and Porte-Agel, 2011), finance, (Marriott et al., 2015) or weather forecasting (Bauer et al., 2015).

However, traditional solvers for physics-based simulation often need a significant amount of computational resources (Houska et al., 2012), such as solvers based on first principles and the modified Gauss-Newton methods. Traditional physics-based simulation heavily relies on knowledge of underlying physics and parameters requiring _ad-hoc_ modeling, which is sensitive to design choices. Finally, even the best traditional simulators are often inaccurate due to the difficulty in approximating real dynamics (Kremer and Hancock, 2006; Oberkampf, 2019; Sanchez-Gonzalez et al., 2020).

An attractive alternative to traditional simulators is to use deep learning to train surrogate models directly from observed data. Data-driven surrogate models can generate

Figure 1.1: Wind speed forecasts on the Black Sea. GraphSplineNets can learn fast and accurate surrogate models for complex dynamics on irregular domains.

predictions based on the knowledge learned automatically from training without the knowledge of the underlying differential equations. Among deep learning methods, graph neural networks (GNNs) have desirable properties such as spatial equivariance and translational invariance, allowing learning representations of dynamical interactions in a generalizable manner (Pfaff et al., 2021; Bronstein et al., 2021) and on unstructured grids. Despite the benefits of these paradigms, by increasing resolution, graph-based models require significantly heavier calculations. Graph models as Poli et al. (2019); Lienen and Gunnemann (2022) predict in continuous time or continuous space. However, the substantial computational overhead due to the need for iterative evaluations of a vector field over time requires considerable computation and limits their scalability (Xhonneux et al., 2020).

In this work, we propose GraphSplineNets, a novel approach that exploits the synergy between GNNs and the _orthogonal spline collocation_ (OSC) method (Bialecki and Fairweather, 2001; Fairweather and Meade, 2020) to both efficiently and accurately forecast continuous responses of a target system. We use the GNNs to efficiently get the future states on coarse spatial and temporal grids and apply OSC with these coarse predictions to predict states at any location in space and time (i.e., continuous). As a result, our approach can generate high-resolution predictions faster than previous approaches, even without explicit prior knowledge of the underlying differential equation. Moreover, the end-to-end training and an adaptive sampling strategy of collocation points help the GraphSplineNets to improve prediction accuracy in continuous space and time.

We summarize our contributions as follows:

* We introduce GraphSplineNets, a novel learning framework to generate fast and accurate predictions of complex dynamical systems by employing coarse grids to predict time and space continuous responses via the differentiable orthogonal spline collocation.
* We propose an adaptive collocation sampling strategy that adjusts the locations of collocation points based on fast-changing regions, enabling dynamic improvements in forecasting accuracy.
* We show GraphSplineNets are competitive against existing methods in improving both accuracy and speed in predicting continuous complex dynamics on both simulated and real data.

## 2 Related Works

Modeling dynamical systems with deep learningDeep neural networks have recently been successfully employed in accelerating dynamics forecasting, demonstrating their capabilities in predicting complex dynamics often orders of magnitude faster than traditional numerical solvers (Long et al., 2018; Li et al., 2021; Berto et al., 2022; Pathak et al., 2022; Park et al., 2022; Poli et al., 2022; Lin et al., 2023, 2023; Boussif et al., 2022, 2023). However, these approaches are typically limited to structured grids and lack the ability to handle irregular grids or varying connectivity. To address this limitation, researchers have turned to graph neural networks (GNNs) as a promising alternative: GNNs enable learning directly on irregular grids and varying connectivity, making them well-suited for predicting system responses with complex geometric structures or interactions (Li et al., 2022). Additionally, GNNs inherit physical properties derived from geometric deep learning, such as permutation and spatial equivariance (Bronstein et al., 2021), providing further advantages for modeling complex dynamics. Alet et al. (2019), one of the first approaches to model dynamics with GNNs, represent adaptively sampled points in a graph architecture to forecast continuous underlying physical processes without any _a priori_ graph structure. Sanchez-Gonzalez et al. (2020) extend the GNN-based models to particle-based graph surrogate models with dynamically changing connectivity, modeling interactions through message passing. Graph neural networks have also recently been applied to large-scale weather predictions (Keisler, 2022). However, GNNs are limited in scaling in both temporal and spatial resolutions with finer grids as the number of nodes grows.

Accelerating graph-based surrogate modelsAn active research trend focuses on improving the scalability of graph-based surrogate models. Pfaff et al. (2021) extend the particle-based deep models of (Sanchez-Gonzalez et al., 2020) to mesh-based ones. By employing the existing connectivity of mesh edges, Pfaff et al. (2021) demonstrate better scalability than proximity-based online graph creation modeling particle interactions. Li et al. (2022) extend convolutional operators to irregular grids by learning a mapping from an irregular input domain to a regular grid: under some assumptions on the domain, this allows for faster inference time compared to message passing. Fortunato et al. (2022) build on the mesh-based approach in (Sanchez-Gonzalez et al., 2020) to create multiscale GNNs that, by operating on both coarse and fine grids, demonstrate better scalability compared to single-scale GNN models. GraphSplineNets do not need to learn additional mappings between different domains or need additional parameters in terms of different scales thus reducing computational overheads by operating directly on unstructured coarse meshes. Our method bridges the gaps between coarse and fine grids in time and space via the orthogonal spline collocation (OSC), thus improving GNN-based methods' scalability.

Collocation methodsCollocation and interpolation methods2 are used to estimate unknown data values from known ones (Bourke, 1999). A fast and accurate collocation method with desirable properties in modeling dynamics is the _orthogonal spline collocation_ (OSC) (Bialecki, 1998; Bourke, 1999). Compared to other accurate interpolators with \(O(n^{3})\) time complexity, the OSC has a complexity of only \(O(n^{2} n)\) thanks to its sparse structure, allowing for fast inference; moreover, it is extremely accurate, respects \(^{1}\) continuity, and has theoretical guarantees on convergence (Bialecki, 1998). Due to such benefits, a few research works have aimed at combining OSC with deep learning approaches. Guo et al. (2019); Brink et al. (2021) combine deep learning and collocation methods by directly learning collocation weights; this provides advantages over traditional methods since the OSC can ensure \(^{1}\) continuity compared to mesh-based methods. A recent work Boussif et al. (2022) has studied learnable interpolators and demonstrated that it is possible to learn an implicit neural representation of a collocation function. GraphSplineNets differ from previous neural collocation methods in two ways: first, we decrease the computational overhead by not needing to learn interpolators or collocation weights; secondly, by interpolating not only in space but also in time, our method enables considerable speedups by employing coarser temporal grids.

## 3 Background

### Problem Setup and Notation

We first introduce the necessary notation that we will use throughout the paper. Let superscripts denote time, and subscripts denote space indexes as well as other notations. We represent the state of a physical process at space location \(=\{_{i},i=1,,N\}\) and time \(t\) as \(^{t}=\{y_{i}^{t},i=1,,N\}\) where \(N\) represents the number of sample points and \(^{D}\) is a \(D\)-dimension physical domain. A physical process in this domain can be described by a solution of PDEs, i.e., \(y_{i}^{t}=u(_{i},t)\). The objective of a dynamics surrogate model is to estimate future states \(\{}^{t},t_{+}\}\) given the initial states \(^{0}\).

### Message Passing Neural Networks

We employ a graph \(^{t}=\{_{i}^{t},_{ij}^{t}\}\) to encode the status of physical process at sample points \(\) at \(t\). \(_{i}^{t}\) and \(_{ij}^{t}\) denote the attribute of sample node \(i\) and the attribute of the directed edge between node \(i\) and \(j\), respectively. Node attributes are encoded from sample points state information while edge attributes are the distance between every two nodes. _Message passing neural networks_ (MPNNs) employ an encoder-processor-decoder structure to predict the states of sample points at the next timestep by giving the previous states:

\[}^{t+1}=}_{}( _{m}((_{1}(( ,^{t})}_{})))}_{}))\] (1)

where \(()\) is the encoder, \(_{i}()\) is the \(i\)-th message passing layer, and \(()\) is the decoder.

### Orthogonal Spline Collocation Method

The OSC method consists of three steps in total: (1) partitioning the input domain and selecting collocation points, (2) generating polynomials with parameters to be determined with observations, and (3) solving equations to determine the parameters.

Partitioning and initializing polynomialsThe OSC aims to find a series of polynomials under order \(r\) while satisfying \(C^{1}\) continuity to approximate the solution of PDEs. To find these polynomials, we split each dimension of the domain into \(N\) partitions34. Then, we initialize one \(r\)-order polynomial with \(r+1\) unknown coefficients in each partition. These polynomials have \(N(r+1)\) degrees of freedom, i.e., the variables to be specified to determine these polynomials uniquely.

Collocation points selection and equation generationWe firstly have \(2\) equations from the boundary condition and \((N-1) 2\) equations from the \(C^{1}\) continuity restriction. Then we need \(N(r-1)\) more equations to uniquely define the polynomials. So we select \(r-1\) collocation points where the observation is obtained to determine the parameters in each partition. By substituting the state of collocation points to the polynomials, we can get \(N(r-1)\) equations. Now we transfer the OSC problem to an algebraic problem. With properly selected base functions, this algebraic equation will be full rank and then has a unique solution (Lee and Micchelli, 2013).

Solving the equationsThe coefficient matrix of the generated algebraic problem is _almost block diagonal_ (ABD) (De Boor and De Boor, 1978). This kind of system allows for efficient computational routines (Amodio et al., 2000), that we introduce in SS 3.4. By solving the equation, we obtain the parameters for the polynomials that can be used to predict the value of any point in the domain. We illustrate an example of \(1\)-D OSC problem in Fig. 3.1. More details and examples of the OSC are provided in Appendix A. The OSC method can also be extended to \(2\)-D and higher-dimensional domains (Bialecki and Fairweather, 2001). In our work, we employ the \(1\)-D and \(2\)-D OSC approaches in the time and space domains, respectively.

### Efficiently solving ABD matrices

Most interpolation methods need to solve linear equations. Gaussian elimination is one of the most widely used methods to solve a dense linear equation, which, however, has a \(O(n^{3})\) complexity (Strassen et al., 1969). Even with the best algorithms known to date, the lower bound of time complexity to solve such equations is \(O(n^{2}{ log}n)\)(Golub and Van Loan, 2013). In the OSC method, the coefficient matrix for the linear equation follows the ABD structure, which we can efficiently solve with a time complexity \(O(n^{2})\) by the COLROW algorithm (Diaz et al., 1983) as shown in Fig. 3.2 (a). The core idea for this method is that by using the pivotal strategy and elimination multipliers, we can decompose the coefficient matrix into a set of a permutation matrix and upper or lower triangular matrix that can be solved in \(O(n^{2})\) time each. The latest package providing this algorithm is in the FORTRAN programming language: our re-implementation in PyTorch (Paszke et al., 2019) allows for optimized calculation, GPU support, and enabling the use of automatic differentiation, as shown in Fig. 3.2 (b).

Figure 3.1: Visualization of an 1â€“D OSC example with the order \(r=3\) and partition number \(N=3\).

Figure 3.2: (a) The COLROW algorithm has the lowest computational complexity with \(O(n^{2})\) compared to the Gaussian elimination algorithm with \(O(n^{3})\) or lower bounds of the generic sparse solvers with \(O(n^{2}{ log}n)\). (b) Running time for \(100\) iterations of A: OSC+LU decomposition, B: OSC+COLORW solver, and C: OSC+COLORW solver with GPU acceleration.

## 4 Methodology

### GraphSplineNets

Fig. 4.1 depicts the entire architecture of our model. GraphSplineNets can be divided into three main components: @ message passing neural networks (MPNN), @ time-oriented collocation and @ space-oriented collocation. The MPNN takes observations at collocation points as input and infers a sequence of discrete future predictions via autoregressive rollouts. We then use the time-oriented and space-oriented collocation methods on these discrete predictions to obtain functions that can provide both time and space-continuous predictions.

Assign collocation pointsWe follow the same rule to select collocation points in a \(2\)-D domain as described in SS 3.3 to select sample points \(\{_{i}\}\) for a \(2\)-D domain. We have the states of these sample points at the initialization time frame \(\{y_{i}^{t_{0}}\}\).

\(\) Message passing neural networksThe neural networks take the states of sample points at the initial time frame \(\{(_{i},y_{i}^{t_{0}})\}\) as input and employ an encoder-processor-decoder architecture for obtaining the next steps states \(_{i}^{t_{j}},j=1,,N\) autoregressively.

\(\) Time-oriented OSCFor each sample point, taking \(_{i}\) as an example, the neural networks generate a sequence of predictions along time which can be considered as a \(1\)-D OSC problem. In this time-oriented OSC problem, collocation points are time steps \(\{t_{0},t_{1},,t_{K}\}\) and values at these time steps \(\{y_{i}^{t_{0}},_{i}^{t_{1}},,_{i}^{t_{K}}\}\), and the output is the polynomials \(_{i}(t),t[t_{0},t_{K}]\) which provides a continuous prediction along time; thus, we can obtain predictions at any time.

\(\) Space-oriented OSCFor each time frame, we take \(t_{k}\) as an example, the neural network generates prediction values of sample points which can be considered a \(2\)-D OSC problem. In this space-oriented OSC problem, collocation points are positions of sample points \(\{_{i}\}\), their values at this time frame \(\{_{i}^{t_{k}}\}\), and the output is polynomials \(^{t_{k}}(),\) which provides a continuous prediction in the domain. Now we get the prediction of every time frame at any position. Note that the space-oriented OSC can use the time-oriented OSC as the input, which means that we can use the space-oriented OSC at any time frame and then get a spatio-temporal continuous predictions \((,t),(,t)[t_{0},t_{K}]\).

Figure 4.1: @ Message passing neural networks take as inputs current states and employ an encoder-processor-decoder architecture for obtaining the next states autoregressively. @ time-oriented OSC obtains continuous predictions in time while @ Space-oriented OSC is employed to obtain continuous predictions in space. The model is trained end-to-end efficiently by leveraging sparsity in the almost block diagonal (ABD) matrices of the collocation method.

### Training strategy and loss function

GraphSplineNets predicts the fine resolution states in a hierarchical manner. MPNN uses the sample points at the initialized state as input to propagate rollouts on these points as shown in Fig. 4.2. Based on MPNN outputs, two OSC problems are solved to find the polynomial parameters \(=((,^{t_{0}};))\) which allow for spatiotemporal continuous outputs \((_{i},t_{k};)\). Note that solving OSC problems is equivalent to predicting the future space and time continuous states, given the sample inputs.

Inherently, training the parameters, \(\) of the MPNN is cast as a bi-level optimization problem as \(^{*}=_{}(,)\) where \(=((,^{t_{0}};))\). The loss function \(\) is defined as:

\[L=_{i=0}^{N}_{k=0}^{K}\|y_{i}^{t_{k}}-_{i}^{t_{k}}\|^{2}+_{ i=0}^{N_{i}}_{k=0}^{N_{i}}\|y_{i}^{t_{k}}-(_{i},t_{k})\|^{2}\] (2)

The left term \(L_{s}\) of Eq. (2) is the loss of the MPNN output \(_{i}^{t_{k}}\), where \(N\) is the number of spatial sample points, and \(K\) is the number of MPNN rollout steps.

The left term \(L_{s}\) of Eq. (2) is the loss of the MPNN output \(_{i}^{t_{k}}\), where \(N\) is the number of spatial sample points, and \(K\) is the number of MPNN rollout steps. The left term \(L_{s}\) solely depends on the MPNN parameters \(\). The right term \(L_{i}\) is the prediction loss of the high-resolution interpolation points \((_{i},y_{i}^{t_{k}})_{i=0,k=0}^{N_{i},N_{t}}\) along time and space, where \(N_{i}\) is the number of spatial interpolation points at the one-time frame, and \(N_{t}\) is the number of interpolation time frames. Note that the space and time continuous prediction \((_{i},t_{k};)\) is constructed based upon the optimized parameters \(=((,^{t_{0}};))\), the parameters that are obtained by solving OSC problems with MPNN predictions, \((,^{t_{0}};)\). Thus, both loss terms can be optimized with respect to the MPNN parameters \(\), and the whole model is trained end-to-end with automatic differentiation through the OSC. This optimization scheme is akin to Bialas and Karwan (1984); Tuy et al. (1993) that solve a bi-level optimization by flattening the loss for the upper- and lower-level problems.

### Adaptive collocation sampling

To allow for prioritized sampling of important location regions, we dynamically optimize the positions of collocation points via their space gradients of the partial derivative values over time. More specifically, after getting the continuous simulation function \(\), the partial derivative equation of time provides the changes of the domain with time. Then, we calculate the maximum gradient vector for each collocation point to move them to more dynamic regions, i.e., with the largest change over time, which causes higher errors in the static collocation points condition. We formulate this process by

\[}_{i}=_{i}+,=_{}_{}}{ t}( _{i},t_{k})\] (3)

where \(\) is the coefficient of the maximum gradient gradient by the partition if the gradient step moves them outside to ensure a sufficient number of collocation points in each partition cell. We illustrate the adaptive collocation points in Fig. 4.3. We use the states at optimized positions adapted from history rollouts as the subsequent rollouts input. By adjusting the collocation points position, our model can focus on more dynamic parts of the space to get a more accurate prediction.

Figure 4.3: Adaptive collocation strategy: mesh points converge towards areas with faster changing regions. In this example, collocation points move towards the moving wave crests.

Figure 4.2: Illustration of data points used for model training. Interpolation points include time-oriented OSC and space-oriented OSC interpolation points. Here, \(t\) can be any time frame in \((t_{0},t_{K})\).

Experiments

### Datasets and Training Settings

We evaluate GraphSplineNets on five dynamical systems of increasing challenge. Simulated datasets consist of three PDEs (partial differential equations), including Heat, Damped Wave, and Navier-Stokes Equations. The two empirical datasets include the Ocean and Black Sea datasets; the Black Sea introduces significant challenges in scalability in the number of nodes, complex dynamics, irregular boundary, and non-uniform meshing. All the models employ the same structure of encoder-processor-decoder for fair comparisons and the same amount of training data in each testing domain. While inputs of baseline models are directly all of the available data points, inputs of our OSC-based models are only an initialized \(12 12\) collocation point unitary mesh at the initial state and fewer in-time sample points. For the Black Sea dataset, we construct a mesh via Delaunay Triangulation with \(5000\) data points for baselines and \(1000\) for GraphSplineNets as done in Lienen and Gunnemann (2022). More details on datasets are available in Appendix B. We value open reproducibility and make our code publicly available5.

### Evaluation metrics and baselines

We evaluated our model by calculating the mean square error (MSE) of rollout prediction steps with respectively ground truth for \(t\) seconds. We employ relevant baselines in the field of discrete-step graph models for dynamical system predictions. _Graph convolution networks_ (GCNs) (Kipf and Welling, 2016) and GCN with a hybrid _multilayer perceptron_ (MLP) model are employed as baselines in the ablation study. We also compare our approach with one widely used baseline that employs linear interpolation for physics simulations allowing for continuous predictions in space, i.e., GEN (Alet et al., 2019). A similar setup considering the inherent graph structure and employing MPNNs in mesh space is used by (Pfaff et al., 2021) (MeshGraphNet, MGN in our comparisons). We utilize MGN as the MPNN building block for our GraphSplineNets.

### Quantitative Analysis

We consider a series of ablation models on four datasets to demonstrate the effectiveness of our model components in our approach in multiple aspects. Quantitative results of ablation study models are shown in Table 5.1 [right]. The ablated models are:

* MGN: MeshGraphNet model with \(3\) message passing layers from Pfaff et al. (2021).
* MGN+OSC(Post): model with \(3\) message passing layers and only post-processing with the OSC method, i.e., we firstly train a MGN model, then we use the OSC method to collocate the prediction as a final result without end-to-end training.
* MGN+OSC: MGN with OSC-in-the-loop that allows for end-to-end training.
* MGN+OSC+Adaptive: MGN+OSC model that additionally employs our adaptive collocation point sampling strategy.

Post processing vs end-to-end learningWe show the effectiveness of end-to-end learning architecture by comparing the MGN+OSC(Post) and MGN+OSC models. Table 5.1 shows that MGN+OSC has a more accurate prediction than MGN+OSC(Post) by more than \(8\%\) percent across datasets. This can be explained by the fact that, since the OSC is applied end-to-end, the error between MGN prediction steps is backpropagated to the message passing layers. In contrast, the model has no way of considering such errors in the post-processing steps.

Adaptive collocation pointsWe further show the effectiveness of the adaptive collocation strategy by comparing the MGN+OSC and MGN+OSC+Adaptive: Table 5.1 shows that MGN+OSC+Adaptive has a more accurate prediction than MGN+OSC, i.e., more than \(5\%\) improvement on long rollouts in the Black Sea dataset. Adaptive collocation points encourage those points to move to the most dynamic regions in the domain, which is not only able to place greater attention on hard-to-learn parts in space but also can let the OSC method implicitly develop a better representation of the domain. Empirical quantitative results on the five datasets are shown in Table 5.1. In the heat equation dataset, our approach reduces long-range prediction errors by \(64\%\) with only \(20\%\) of the running time compared with the best baseline model. In the damped wave dataset, our approach reduces errors by \(42\%\) with a \(48\%\) reduction in inference time.

In the Navier-Stokes dataset, our method reduces \(31\%\) long-range prediction errors while requiring \(37\%\) less time to infer solutions compared to the strongest baseline.

Inference timesWe show the effectiveness of the COLROW algorithm in accelerating the OSC speed by comparing the OSC method with one of the most commonly used algorithms for efficiently solving linear systems6 and the OSC with the COLROW solver in Fig. 3.2. Our differentiable OSC and adaptive collocation only result in a minor overhead over the graph neural network. The overall scheme is still a fraction of the total in terms of inference time as shown in Fig. 5.4, while making GraphSplineNets more accurate.

### Sensitivity Analysis

Interpolation and collocation methodsWe demonstrate the efficiency of the OSC method by comparing the combination of MGN with different interpolation and collocation methods, including linear interpolation, cubic interpolation, and B-spline collocation methods. These models are implemented in the end-to-end training loop, and we use these methods in both the time and space dimensions. Results are shown in Fig. 5.2 where we measured the mean square error and running time of \(3\) second rollouts predictions by varying the number of collocation points from \((2 2)\) to \((16 16)\). The model MGN+OSC shows the best accuracy while having a shorter running time. Even though the linear interpolation can be slightly faster than the OSC, it shows a considerable error in the prediction and does not satisfy basic assumptions such as Lipschitz continuity in space.

    &  &  &  \\   & & & GCN & GCN+MLP & GEN & MGN & MGN+OSCPN & MGN+OSC & MGN+OSC+Adaptive \\   & MSE (\( 10^{-3}\)) & \(6.87 1.00\) & \(5.02 0.89\) & \(2.92 0.23\) & \(3.01 0.38\) & \(1.68 0.18\) & \(1.14 0.11\) & \( 0.28\) \\  & Runtime [s] & \(3.26 0.12\) & \(3.02 0.10\) & \(6.87 0.10\) & \(6.99 0.12\) & \(1.52 0.09\) & \( 0.10\) & \(1.41 0.12\) \\   & MSE (\( 10^{-1}\)) & \(10.5 1.65\) & \(9.90 1.52\) & \(6.49 0.62\) & \(7.82 0.88\) & \(4.98 0.29\) & \(4.60 0.27\) & \( 0.31\) \\  & Runtime [s] & \(0.95 0.08\) & \(0.82 0.07\) & \(1.13 0.09\) & \(1.28 0.10\) & \(0.45 0.05\) & \( 0.04\) & \(0.42 0.09\) \\   & MSE (\( 10^{-3}\)) & \(4.24 0.95\) & \(3.91 0.99\) & \(3.45 0.24\) & \(3.66 0.33\) & \(2.58 0.28\) & \(2.21 0.27\) & \( 0.30\) \\  & Runtime [s] & \(0.91 0.08\) & \(0.88 0.07\) & \(1.01 0.09\) & \(1.21 0.10\) & \(0.51 0.05\) & \( 0.04\) & \(0.49 0.09\) \\   & MSE (\( 10^{-1}\)) & \(6.02 1.12\) & \(5.23 0.38\) & \(4.55 0.52\) & \(4.75 0.61\) & \(3.82 0.49\) & \(3.61 0.38\) & \( 0.44\) \\  & Runtime [s] & \(3.56 0.15\) & \(3.28 0.13\) & \(7.11 0.31\) & \(7.15 0.25\) & \(1.61 0.12\) & \( 0.09\) & \(1.57 0.11\) \\   & MSE (\( 10^{-1}\)) & \(7.73 0.28\) & \(7.12 0.35\) & \(6.22 0.31\) & \(6.41 0.42\) & \(4.37 0.33\) & \(4.22 0.17\) & \( 0.27\) \\  & Runtime [s] & \(8.27 0.33\) & \(8.11 0.28\) & \(12.81 0.19\) & \(13.35 0.27\) & \(5.87 0.13\) & \( 0.21\) & \(5.29 0.15\) \\   

Table 5.1: Mean Squared Error propagation and runtime for \(5\) second rollouts. GraphSplineNets consistently outperform baselines in both accuracy and runtime. Smaller is better (\(\)). Best in **bold**; second underlined.

Figure 5.1: Inference time benchmark: the OSC and adaptive collocation points result in a minor overhead; the overall scheme result in a fraction of the baseline running time.

Number of collocation points

We study the effect of the number of collocation points on the \(3\) second rollout MGN+OSC+Adaptive models.

Number of rollout stepsWe show the effectiveness of the OSC method in improving long-range prediction accuracy by comparing the MGN and MGN+OSC model. Fig. 5.4 shows the MGN+OSC can keep stable in long-range rollouts compare with the MGN.

The reason is that with the OSC, we can use fewer neural network rollout steps to obtain longer-range predictions, which avoids error accumulation during the multi-step rollouts and implicitly learns for compensating integration residual errors. In addition, end-to-end learning lets the neural networks in MGN+OSC learn the states between rollout steps, making the prediction stable and accurate.

Number of Message Passing LayersWe further showcase the sensitivity to the number of message-passing layers while adopting the same baseline processor for a fair comparison. As Fig. 5.5 shows, a higher number of message-passing layers generally helps in decreasing the MSE at the cost of higher runtime, reflecting findings from prior work such as GNS (Alet et al., 2019) and MGN (Pfaff et al., 2021). Notably, GraphSplineNets exhibits a Pareto-optimal trade-off curve, outperforming MGN in terms of prediction accuracy and computational efficiency at different numbers of message-passing layers. Downsampling mesh nodes can help in robustness and generalization given the larger spatial extent of communication akin to Fortunato et al. (2022); the OSC plays a crucial part in handling the problem's continuity (e.g., in time) while adaptive collocation effectively helps the model to focus on regions with higher errors thus increasing the model performance.

Scalability to Higher DimensionsWe showcase GraphSplineNets on a larger 3D dataset, namely the 3D Navier-Stokes equations dataset from PDEBench (Takamoto et al., 2022) (more details in Appendix B.7). The number of nodes in GraphSplineNets is \(4096\), which is more than \(4\) compared to the Black Sea Dataset. Moreover, the 3D Navier-Stokes includes a total of 5 variables: density, pressure, and velocities on 3 axes. For a fair comparison, we keep the model structure the same as in other experiments and compare GraphSplineNets against different from \(4 10^{-1}\) with a runtime of more than \(80\) seconds, while GraphSplineNets remarkably outperforms MGN with an MSE of \((3.98 0.27) 10^{-1}\) and less than \(9\) seconds to generate a trajectory - an example of which is shown in Fig. 5.6.

Figure 5.4: MSE vs rollout steps. Our full model yields stable long rollouts.

Figure 5.3: MSE vs the number of collocation points. MGN does not use collocation points, hence constant. The more the collocation points, the better the performance.

Figure 5.5: Pareto plot for MSE and runtime at different # of layers.

### Qualitative analysis

We visualize the prediction of our model with baselines on the Black Sea dataset Fig. 5.7. Our model can accurately predict even with such complex dynamics from real-world data that result in turbulence phenomena while considerably cutting down the computational costs as shown in Table 5.1. We also provide series of qualitative visualizations with different datasets and baselines in Appendix C. Our model has a smoother error distribution and more stable long-range prediction. Thanks to the continuous predictions from GraphSplineNets, we can simulate high resolutions without needing additional expensive model inference routines, while the other two models can only achieve lower-resolution predictions. Baselines visibly accumulate errors for long-range forecasts; our model can lower the error with smoother and more accurate predictions in space and time.

## 6 Conclusion and Limitations

In this work, we proposed GraphSplineNets, a novel approach that employs graph neural networks alongside the orthogonal spline collocation method to enable fast and accurate forecasting of continuous physical processes. We used GNNs to obtain predictions based on coarse spatial and temporal grids, and the OSC method to produce predictions at any location in space and time. Our approach demonstrated the ability to generate high-resolution predictions faster than previous methods, even without explicit prior knowledge of the underlying differential equations. Additionally, we proposed an adaptive collocation sampling strategy that improves the prediction accuracy as the system evolves. We demonstrate how GraphSplineNets are robust in predicting complex physics in both simulated and real-world data. We believe this work represents a step forward in a new direction in the research area at the intersection of deep learning and dynamical systems that aims at finding fast and accurate learned surrogate models.

A limitation of our approach is that, by utilizing the orthogonal spline collocation (OSC) method for interpolation, we assume some degree of smoothness in the underlying physical processes between collocation points; as such, our approach may struggle when modeling discontinuity points, which would be smoothed-out. Finally, we do note that despite benefits, deep learning models including GraphSplineNets often lack interpretability. Understanding how the model arrives at its predictions or explaining the underlying physics in a transparent manner can be challenging, hindering the model's adoption in domains where interpretability is crucial.

Figure 5.6: Rollout visualization of density for 3D Navier-Stokes equations.

Figure 5.7: Visualization of rollout forecasts for baselines and GraphSplineNets on the Black Sea dataset. First row: predictions of the wind speed norm. Second row: mean square error (MSE). Our full model, namely MGN+OSC(Adaptive), generates more accurate predictions than other baseline surrogate models.