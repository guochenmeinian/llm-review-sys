# Near-Optimal Bounds for Learning Gaussian Halfspaces with Random Classification Noise

Ilias Diakonikolas

University of Wisconsin, Madison

ilias@cs.wisc.edu

&Jelena Diakonikolas

University of Wisconsin, Madison

jelena@cs.wisc.edu

&Daniel M. Kane

University of California, San Diego

dakane@ucsd.edu

&Puqian Wang

University of Wisconsin, Madison

pwang333@wisc.edu

&Nikos Zarifis

University of Wisconsin, Madison

zarifis@wisc.edu

###### Abstract

We study the problem of learning general (i.e., not necessarily homogeneous) halfspaces with Random Classification Noise under the Gaussian distribution. We establish nearly-matching algorithmic and Statistical Query (SQ) lower bound results revealing a surprising information-computation gap for this basic problem. Specifically, the sample complexity of this learning problem is \((d/)\), where \(d\) is the dimension and \(\) is the excess error. Our positive result is a computationally efficient learning algorithm with sample complexity \((d/+d/(\{p,\})^{2})\), where \(p\) quantifies the bias of the target halfspace. On the lower bound side, we show that any efficient SQ algorithm (or low-degree test) for the problem requires sample complexity at least \((d^{1/2}/(\{p,\})^{2})\). Our lower bound suggests that this quadratic dependence on \(1/\) is inherent for efficient algorithms.

## 1 Introduction

A halfspace or Linear Threshold Function (LTF) is any Boolean function \(h:^{d}\{ 1\}\) of the form \(h()=(+t)\), where \(^{d}\) is the weight vector and \(t\) is the threshold. The function \(:\{ 1\}\) is defined as \((u)=1\) if \(u 0\) and \((u)=-1\) otherwise. The problem of learning halfspaces is a classical problem in machine learning, going back to the Perceptron algorithm  and has had a big impact in both the theory and the practice of the field . Here we study the problem of PAC learning halfspaces in the distribution-specific setting in the presence of Random Classification Noise (RCN) . Specifically, we focus on the basic case in which the marginal distribution on examples is the standard Gaussian -- one of the simplest and most extensively studied distributional assumptions.

In the realizable PAC model  (i.e., when the labels are consistent with a concept in the class), the class of halfspaces on \(^{d}\) is efficiently learnable to 0-1 error \(\) using \((d/)\) samples via linear programming (even in the distribution-free setting). This sample complexity upper bound is information-theoretically optimal, even if we know a priori that the distribution on examples is well-behaved (e.g., Gaussian or uniform). That is, in the realizable setting, there is an efficient algorithm for halfspaces achieving the optimal sample complexity (within logarithmic factors).

Learning Gaussian Halfspaces with RCN.The RCN model  is the most basic model of random noise. In this model, the label of each example is independently flipped with probability exactly \(\), for some noise parameter \(0<<1/2\). One of the classical results on PAC learning with RCN  states that any Statistical Query (SQ) algorithm can be transformed into an RCN noise-tolerant PAC learner with at most a polynomial complexity blowup. Halfspaces are known to be efficiently PAC learnable in the presence of RCN, even in the distribution-free setting . Alas, all these efficient algorithms require sample complexity that is suboptimal within polynomial factors in \(d\) and \(1/\).

The sample complexity of PAC learning Gaussian halfspaces with RCN is \((d/((1-2)))\). This bound can be derived, e.g., from , and the lower bound essentially matches the realizable case, up to a necessary scaling of \((1-2)\).1 Given the fundamental nature of this learning problem, it is natural to ask whether a computationally efficient algorithm with (near-) optimal sample complexity (i.e., within logarithmic factors of the optimal) exists. That is, we are interested in a fine-grained sample size versus computational complexity analysis of the problem. This leads us to the following question:

_Is there a sample near-optimal and polynomial-time algorithm_

_for learning Gaussian halfspaces with RCN?_

In this paper, we explore the above question and provide two main contributions -- essentially resolving the question within logarithmic factors. On the positive side, we give an efficient algorithm with sample complexity \(_{}(d/+d/(\{p,\})^{2})\) for the problem. Here the parameter \(p[0,1/2]\) (Definition 1.2) quantifies the _bias_ of the target function; a "balanced" function has \(p=1/2\) and a constant function has \(p=0\). The worst-case upper bound arises when \(p=()\), in which case our algorithm has sample complexity of \(_{}(d/^{2})\). Perhaps surprisingly, _we provide formal evidence that the quadratic dependence on the quantity \(1/\{p,\}\) in the sample complexity cannot be improved for computationally efficient algorithms_. Our lower bounds apply for two restricted yet powerful models of computation, namely Statistical Query algorithms and low-degree polynomial tests. Our lower bounds suggest an inherent _statistical-computational tradeoff_ for this problem.

### Our Results

We study the complexity of learning halfspaces with RCN under the Gaussian distribution. Let \(=\{f:^{d}\{ 1\} f()=(+t)\}\) be the class of general (i.e., not necessarily homogeneous) halfspaces in \(^{d}\). The following definition summarizes our learning problem.

**Definition 1.1** (Learning Gaussian Halfspaces with RCN).: _Let \(\) be a distribution on \((,y)^{d}\{ 1\}\) whose \(\)-marginal \(_{}\) is the standard Gaussian. Moreover, there exists \((0,1/2)\) and a target \(f\) such that the label \(y\) of example \(\) satisfies \(y=f()\) with probability \(1-\) and \(y=-f()\) otherwise. Given \(>0\) and sample access to \(\), the goal is to output a hypothesis \(h\) that with high probability satisfies \(_{0-1}^{}(h):=_{}[h( ) y]+\)._

Our main contribution is a sample near-optimal efficient algorithm for this problem coupled with a matching statistical-computational tradeoff for SQ algorithms and low-degree polynomial tests. It turns out that the sample complexity of our algorithm depends on the bias of the target halfspace, defined below.

**Definition 1.2** (\(p\)-biased function).: _For \(p[0,1/2]\), we say that a Boolean function \(f:^{d}\{ 1\}\) is \(p\)-biased with respect to the distribution \(_{}\), if \(\,_{_{}}[f()=1],\ _{_{}}[f()=-1]}=p\)._

For example, a homogeneous halfspace \(f()=()\) under the standard Gaussian distribution \(_{}=(,)\) satisfies \(_{_{}}[f()]=0\), and therefore has bias \(p=1/2\). For a general halfspace \(f()=(+t)\) with \(\|\|_{2}=1\), it is not difficult to see that its bias under the standard Gaussian is approximately \(p(1/t)(-t^{2}/2)\) (see Fact B.1).

We can now state our algorithmic contribution.

**Theorem 1.3**.: (Main Algorithmic Result) _There exists an algorithm that, given \(,(0,1/2)\) and \(N\) samples from a distribution \(\) satisfying Definition 1.1, runs in time \(O(dN/^{2})\) and returns a hypothesis \(h\) such that with probability at least \(1-\), it holds \(_{0-1}^{p}(h)+\). The sample complexity of the algorithm is \(N=+}(1/)\)._

Some comments are in order. We note that the first term in the sample complexity matches the information-theoretic lower bound (within a logarithmic factor), even for homogeneous halfspaces (\(p=1/2\)); see, e.g., . The second term -- scaling quadratically with \(1/\{(1-2)p,\}\) -- is not information-theoretically necessary and dominates the sample complexity when \(p=O_{}()\). In the worst-case, i.e., when \(p=O_{}()\), our algorithm has sample complexity \(_{}(d/^{2})\). Perhaps surprisingly, we show in Theorem 1.5 that this quadratic dependence is required for any computationally efficient SQ algorithm; and, via , for any low-degree polynomial test.

Basics on SQ Model.SQ algorithms are a broad class of algorithms that, instead of having direct access to samples, are allowed to query expectations of bounded functions of the distribution.

**Definition 1.4** (SQ algorithms).: _Let \(D\) be a distribution on \(^{d}\). A statistical query is a bounded function \(q:^{d}[-1,1]\). For \(u>0\), the \((u)\) oracle responds to the query \(q\) with a value \(v\) such that \(|v-_{ D}[q()]|\), where \(=(1/u,_{ D}[q()]/u})\). We call \(\) the tolerance of the statistical query. A Statistical Query algorithm is an algorithm whose objective is to learn some information about an unknown distribution \(D\) by making adaptive calls to the corresponding oracle._

The SQ model was introduced in  as a natural restriction of the PAC model . Subsequently, the model has been extensively studied in a range of contexts, see, e.g., . The class of SQ algorithms is broad and captures a range of known supervised learning algorithms. More broadly, several known algorithmic techniques in machine learning are known to be implementable using SQs (see, e.g., ).

We can now state our SQ lower bound result.

**Theorem 1.5** (SQ Lower Bound).: _Fix any constant \(c(0,1/2)\) and let \(d\) be sufficiently large. For any \(p 2^{-O(d^{})}\), any SQ algorithm that learns the class of \(p\)-biased halfspaces on \(^{d}\) with Gaussian marginals in the presence of RCN with \(=1/3\) to error less than \(+p/3\) either requires queries of accuracy better than \((pd^{c/2-1/4})\), i.e., queries to \(((d^{1/2-c}/p^{2}))\), or needs to make at least \(2^{(d^{})}\) statistical queries._

Informally speaking, Theorem 1.5 shows that no SQ algorithm can learn \(p\)-biased halfspaces in the presence of RCN (with \(=1/3\)) to accuracy \(+O()\) (considering \(p>/2\)) with a sub-exponential in \(d^{(1)}\) many queries, unless using queries of small tolerance -- that would require at least \((/p^{2})\) samples to simulate. This result can be viewed as a near-optimal information-computation tradeoff for the problem, within the class of SQ algorithms. When \(p=2\), the computational sample complexity lower bound we obtain is \((/^{2})\). That is, for sufficiently small \(\), the computational sample complexity of the problem (in the SQ model) is polynomially higher than its information-theoretic sample complexity.

Via , we obtain a qualitatively similar lower bound in the low-degree polynomial testing model; see Appendix D.

### Our Techniques

Upper Bound.At a high level, our main algorithm consists of three main subroutines. We start with a simple Initialization (warm-start) subroutine which ensures that we can choose a weight vector \(_{0}\) with sufficiently small angle to the target vector \(^{*}\). This subroutine essentially amounts to estimating the degree-one Chow parameters of the target function and incurs sample complexity \((d/((p(1-2),)^{2})\). We emphasize that our procedure does not require knowing the bias \(p\) of the target halfspace; instead, it estimates this parameter to a constant factor.

Our next (and main) subroutine is an optimization procedure that is run for \((1/^{2})\) different guesses of the threshold \(t\). At a high level, our optimization subroutine can be seen as a variant of Riemannian (sub)gradient descent on the unit sphere, applied to the empirical LeakyReLU loss -- defined as \(_{}(u)=(1-)u\{u 0\}+  u\{u<0\}\) -- with parameter \(\) set to \(\), \(u=\)and _with samples restricted to a band_, namely \(a<||<b\) -- with \(a\) and \(b\) chosen as functions of the guess for the threshold \(t\). The band restriction is key in avoiding \((d/^{2})\) dependence in the sample complexity; instead, we only require order-\((d/)\) samples to be drawn for the empirical LeakyReLU loss subgradient estimate. Using the band, the objective is restricted to a region where the current hypothesis incorrectly classifies a constant fraction of the mass from which we can perform "denoising" with constantly many samples.

For a sufficiently accurate estimate \(\) of \(t\) (which is satisfied by at least one of the guesses for which our optimization procedure is run), we argue that there is a sufficiently negative correlation between the empirical subgradient and the target weight vector \(^{*}\). This result, combined with our initialization, enables us to inductively argue that the distance between the weight vector constructed by the optimization procedure and the target vector \(^{*}\) contracts and becomes smaller than \(\) within order-\((1/)\) iterations. This result is quite surprising, since the LeakyReLU loss is nonsmooth (it is, in fact, piecewise linear) and we do not explicitly bound its growth outside the set of its minima (i.e., we do not prove a local error bound, which would typically be used to prove linear convergence). Thus, the result we establish is impossible to obtain using black-box results for nonsmooth optimization. Additionally, we never explicitly use the LeakyReLU loss function or prove that it is minimized by \(^{*}\); instead, we directly prove that the vectors \(\) constructed by our procedure converge to the target vector \(^{*}\). At a technical level, our result is enabled by a novel inductive argument, which we believe may be of independent interest (see Lemma 2.8 for more details).

Since each run of our optimization subroutine returns a different hypothesis, at least one of which is accurate (the one using the "correct" guess of the threshold \(t\)), we need an efficient way to select a hypothesis with the desired error guarantee. This is achieved via our third subroutine -- a simple hypothesis testing procedure, which draws a fresh sample and selects a hypothesis with the lowest test error. By standard results , such a hypothesis satisfies our target error guarantee.

SQ Lower Bound.To prove our SQ lower bound, it suffices to establish the existence of a large set of distributions whose pairwise correlations are small . Inspired by the methodology of , we achieve this by selecting our distributions on labeled examples \((,y)\) to be random rotations of a single one-dimensional distribution that nearly matches low-order Gaussian moments, and embedding this in a hidden random direction. Our hard distributions are as follows: We define the halfspaces \(f_{}()=(-t)\), where \(\) is a randomly chosen unit vector and the threshold \(t\) is chosen such that \(_{}[f_{}()=1]=p\). We then let \(y=f_{}()\) with probability \(2/3\), and \(-f_{}()\) otherwise. By picking a packing of nearly orthogonal vectors \(\) on the unit sphere (i.e., set of vectors with pairwise small inner product), we show that each pair of these \(f_{}\)'s corresponding to distinct vectors in the packing have very small pairwise correlations (with respect to the distribution where \(\) is a standard Gaussian and \(y\) is independent of \(\)). While the results of  cannot be directly applied to give our desired correlation bounds, the Hermite analytic ideas behind them are useful in this context. In particular, the correlation between two such distributions can be computed in terms of their angle and the Hermite spectrum. A careful analysis (Lemma 3.3) gives an inner product that is \((()p)\), where \(\) is the angle between the corresponding vectors. Combined with our packing bound, this is sufficient to obtain our final SQ lower bound result.

### Related and Prior Work

A long line of work in theoretical machine learning has focused on developing computationally efficient algorithms for learning halfspaces under natural distributional assumptions in the presence of RCN and related semi-random noise models; see, e.g., . Interestingly, the majority of these works focused on the special case of homogeneous halfspaces. We next describe in detail the most relevant prior work.

Prior work  gave sample near-optimal and computationally efficient learners for _homogeneous_ halfspaces with RCN (and, more generally, bounded noise). Specifically, these works developed algorithms using near-optimal sample complexity of \(_{}(d/)\). However, their algorithms and analyses are customized to the homogeneous case, and it is not clear how to extend them for general halfspaces. In fact, since all of these algorithms are easily implementable in the SQ model, our SQ lower bound (Theorem 1.5) implies that these prior algorithms _cannot_ be adapted to handle the general case without an increase in sample complexity. Finally,  gave an algorithm with sample complexity \((d/^{2})\) to learn general Gaussian halfspaces with adversariallabel noise to error \(O()+\), where \(\) is the optimal misclassification error. Unfortunately, this algorithm does not suffice for our RCN setting (where \(=\)), since its error guarantee is significantly weaker than ours.

Very recent work [DDK\({}^{+}\)23] gave an SQ lower bound for \(\)-margin halfspaces with RCN, which has some similarities to ours. Specifically, [DDK\({}^{+}\)23] showed that any efficient SQ algorithm for that problem requires sample complexity \((1/(^{1/2}^{2}))\). Intuitively, the margin assumption allows for a much more general family of distributions compared to our Gaussian assumption here. In particular, the SQ construction of that work does not have any implications in our setting. Even though the Gaussian distribution does not have a margin, it is easy to see that it satisfies an approximate margin property for \( 1/\). In fact, using an adaptation of our construction, we believe we can quantitatively strengthen the lower bound of [DDK\({}^{+}\)23] to \((1/(^{2}))\). For more details, see Appendix A.

### Preliminaries

For \(n_{+}\), we define \([n]\{1,,n\}\). We use lowercase bold characters for vectors and uppercase bold characters for matrices. For \(^{d}\) and \(i[d]\), \(_{i}\) denotes the \(i\)-th coordinate of \(\), and \(\|\|_{2}(_{i=1}^{d}_{i}{}^{2})^{1/2}\) denotes the \(_{2}\)-norm of \(\). We use \(\) for the inner product of \(,^{d}\) and \((,)\) for the angle between \(\) and \(\). We slightly abuse notation and denote by \(_{i}\) the \(i^{}\) standard basis vector in \(^{d}\). We further use \(_{A}\) to denote the characteristic function of the set \(A\), i.e., \(_{A}()=1\) if \( A\) and \(_{A}()=0\) if \( A\). We use the standard \(O(),(),()\) asymptotic notation. We also use \(()\) to omit poly-logarithmic factors in the argument. We use \(_{x D}[x]\) for the expectation of the random variable \(x\) according to the distribution \(\) and \([]\) for the probability of event \(\). For simplicity of notation, we omit the distribution when it is clear from the context. For \((,y)\) distributed according to \(\), we denote by \(_{}\) the distribution of \(\). As is standard, we use \(\) to denote the standard normal distribution in \(d\) dimensions; i.e., with its mean being the zero vector and its covariance being the identity matrix.

## 2 Efficiently Learning Gaussian Halfspaces

In this section, we prove Theorem 1.3 by analyzing Algorithm 1. As discussed in the introduction and shown in Algorithm 1, there are three main procedures in our algorithm. The guarantees of our Initialization (warm start) procedure, which ensures sufficient correlation between the initial weight vector \(_{0}\) and the target vector \(^{*}\), are stated in Section 2.1, while the proofs and pseudocode are in Appendix B.1. Our main results for this section, including the Optimization procedure and associated analysis, are in Section 2.2. The Testing procedure is standard and deferred to Appendix B.2, together with most of the technical details from this section.

Throughout this section, we assume that the parameter \(\) (RCN parameter) is known. As will become clear from our analysis, a constant factor approximation to the value of \(1-2\) is sufficient to obtain our results. For completeness, we show how to obtain such an approximation in Appendix B.3. For simplicity, we present the results for \(t 0\) and \(t\). This is without loss of

```
1:Input:\(\), \(\), \(\), sample access to distribution \(\)
2:\([_{0},]=(,,)\); \(^{}=/(1-2)\)
3:\(t_{0}=)}\), \(M=8/))}-)} }{(^{})^{2}}+1\)
4: Draw \(N_{2}=O()}{(1-2)^{2 }})\) samples \(\{(^{(i)},y^{(i)})\}_{i=1}^{N_{2}}\) from \(\)
5:for\(m=1:M\)do
6:\(t_{m}=t_{0}+(m-1))^{2}}{8}\), \(_{m}=}{2}(t_{m}^{2}/2)\)
7:\(}_{m}=(_{0},t_{m},_{m}, ,\{(^{(i)},y^{(i)})\}_{i=1}^{N_{2}})\)
8:endfor
9:\([}_{},t_{}]=(( }_{1},t_{1}),(}_{2},t_{2}),,( }_{M},t_{M}))\)
10:return\(}_{}\), \(t_{}\) ```

**Algorithm 1** Main Algorithmgenerality. For the former, it is by the simple symmetry of the standard normal distribution that the entire argument translates into the case \(t<0,\) possibly by exchanging the meaning of '+1' and '-1' labels. For the latter, we note that when the bias is small, i.e., for \(p/(2(1-2)),\) a constant hypothesis suffices.

### Initialization Procedure

We begin this section with Lemma 2.1, which shows that given \(N_{1}=(d/(^{4}p^{2}(1-2)^{2})(1/))\) i.i.d. samples from \(,\) we can construct a good initial point \(_{0}\) that forms an angle at most \(\) with the target weight vector \(^{*}.\) For our purposes, \(\) should be of the order \(1/t.\) For \(t)},\) where \(^{}=/(1-2),\) we can ensure that \(N_{1}=(d/(p^{2}(1-2)^{2})(1/)).\) The downside of the lemma, however, is that the number of samples \(N_{1}\) requires at least approximate knowledge of the bias parameter \(p\) (or, more accurately, of \(e^{-t^{2}/2}\)). We address this challenge by arguing (in Lemma 2.2) that we can estimate \(p\) using the procedure described in Algorithm 4, without increasing the total number of drawn samples by a factor larger than order-\((1/^{}).\)

**Lemma 2.1** (Initialization via Chow Parameters).: _Given \(>0,\) define \(p_{t}=e^{-t^{2}/2},\)\(N_{1}=O(d/(^{4}p_{t}^{2}(1-2)^{2})(1/))\) and let \((^{(i)},y^{(i)})\) for \(i[N_{1}]\) be i.i.d. samples drawn from \(\). Let \(=}_{i=1}^{N_{1}}^{(i)}y^{(i)}\) and \(_{0}=/\|\|_{2}.\) Then, with probability \(1-,\) we have \((_{0},^{*}).\)_

We now leverage Lemma 2.1 to argue about the correctness of implementable Initialization procedure, stated as Algorithm 4 in Appendix B.1, where the proofs for this subsection can be found.

**Lemma 2.2**.: _Consider the Initialization procedure described by Algorithm 4 in Appendix B.1. If \(0 t,\) then with probability at least \(1-,(-t^{2}/2) 4(-t^{2}/2).\) The algorithm draws a total of \(^{2}} \) samples and ensures that \((_{0},^{*})\{,\}.\)_

### Optimization

As discussed before, our Optimization procedure (Algorithm 2) can be seen as Riemannian subgradient descent on the unit sphere. Crucial to our analysis is the use of subgradient estimates from Line 5 and Line 6, where we condition on the event that the samples come from a thin band, defined in Line 4. Without this conditioning, the algorithm would correspond to projected subgradient descent of the LeakyReLU loss on the unit sphere. The conditioning effectively changes the landscape of the loss function being optimized, which cannot be argued anymore to even be convex, as the definition of the band depends on the weight vector \(\) at which the vector \(}()\) is evaluated. Nevertheless, as we argue in this section, the optimization procedure can be carried out very efficiently, even exhibiting a linear convergence rate. To simplify the notation, in this section we denote the conditioned distribution \(|_{(,)}\) by \((,).\) We carry out the analysis assuming the estimate \(\) is within additive \(^{2}\) of the true threshold value \(t;\) as argued before, this has to be true for at least one estimate \(\) for which the Optimization procedure is invoked.

In the following lemma, we show that if the angle between a weight vector \(\) and the target vector \(^{*}\) is from a certain range, we can guarantee that \(()\) is sufficiently negatively correlated with \(^{*}\). This condition is then used to argue about progress of our algorithm. The upper bound on \(\) will hold initially, by our initialization procedure, and we will inductively argue that it holds for all iterations. The lower bound, when violated, will imply that the distance between \(\) and \(^{*}\) is small, in which case we would have converged to a sufficiently good solution \(\).

**Lemma 2.3**.: _Fix any \(^{}(0,1)\). Suppose that \(0 t)}\) and \(^{d}\) is such that \(\|\|_{2}=1\), and \(=(,^{*})\) satisfies the inequality \(^{}(t^{2}/2) 1/(5t)\). If \(|-t|^{ 2}/8\) and \(=(1/2)^{}(^{2}/2)\), then \(_{(,y)(,)}[( ;,y)^{*}]-(1-2)/(2)\)._

Since, by construction, \(()\) is orthogonal to \(\) (see Line 5 in Algorithm 2), we can bound the norm of the expected gradient vector by bounding \(()\) for some unit vectors \(\) that are orthogonal to \(\) using similar techniques as in Lemma 2.3. To be specific, we have the following lemma.

**Lemma 2.4**.: _Under the assumptions of Lemma 2.3, \(\|_{(,y)(,)}[(;,y)]\|_{2}}\)._

The last technical ingredient that we need is the following lemma which shows a uniform bound on the difference between the empirical gradient \(}()\) and its expectation (for more details, see Lemma B.6 and Corollary B.8 in Appendix B).

**Lemma 2.5**.: _Consider the learning problem from Definition 1.1. Let \(^{},,\) be parameters satisfying the conditions of Lemma 2.3. Let \((0,1)\). Then using \((d(1/)/((1-2)^{2}^{}))\) samples to construct \(}\), for any unit vector \(\) such that \(^{}(t^{2}/2)(,^{*}) 1/(5t)\), it holds with probability at least \(1-\): \(\|}()-_{(,y) (,)}[()]\|_{2}(1/4)\|_{( ,y)(,)}[()]\|_ {2}\)._

We are now ready to present and prove our main algorithmic result. A short roadmap for our proof is as follows. Since Algorithm 1 constructs a grid with grid-width \(^{ 2}/8\) that covers all possible values of the true threshold \(t\), there exists at least one guess \(\) that is \(^{ 2}\)-close to the true threshold \(t\). We first show that to get a halfspace with error at most \(^{}\), it suffices to use this \(\) as the threshold and find a weight vector \(\) such that the angle \((,^{*})\) is of the order \(^{}\), which is exactly what Algorithm 2 does. The connection between \((,^{*})\) and the error is conveyed by the inequality \([(+t)( ^{*}+t)]((,^{*})/) (-t^{2}/2)\); see Appendix B. Let \(_{k}\) be the parameter generated by Algorithm 2 at iteration \(k\) for threshold \(\). We show that \((_{k},^{*})\) converges to zero at a linear rate. To this end, we prove that under our carefully devised step size \(_{k}\), there exists an upper bound on \(\|_{k}-^{*}\|_{2}\), which contracts at each iteration. Note that since both \(_{k}\) and \(^{*}\) are on the unit sphere, we have \(\|_{k}-^{*}\|_{2}=2((_{k},^{ *})/2)\). Essentially, this implies that Algorithm 2 produces a sequence of parameters \(_{k}\) such that \((_{k},^{*})\) converges to 0 linearly, under this threshold \(\). Thus, we can conclude that there exists a halfspace among all halfspaces generated by Algorithm 1 that achieves \(^{}\) error with high probability.

**Theorem 2.6**.: _Consider the learning problem from Definition 1.1. Fix any unit vector \(_{0}^{d}\) such that \((_{0},\,^{*})(1/(5t),\,/2)\). Fix any \(,>0\). Let \(>0\) be a threshold such that \(|-t|^{2}/(8(1-2)^{2})\), and let \(=/(2(1-2))(t^{2}/2)\). Then Algorithm 2 uses \(N_{2}=d/((1-2))(1/)\) samples from \(\), has runtime \((N_{2}d)\), and outputs a weight vector \(\) such that \(h()=(+)\) satisfies \([h() y]+\) with probability at least \(1-\)._

Proof.: Let \(^{}=(/1-2)\), and denote by \(_{k}\) the vector produced by the algorithm at \(k^{}\) iteration for threshold \(\). For any unit vector \(\) and \(|-t|^{ 2}/8\), it holds \([(+) (^{*}+t)]^{ 2}/(4)+( ,^{*})/(-t^{2}/2)\) (see Appendix B.2 for more details). Therefore, it suffices to find a parameter \(\) such that \((,^{*})^{}(t^{2}/2)\). Note that since both \(\) and \(^{*}\) are unit vectors, we have \(\|-^{*}\|_{2}=2(/2)\), indicating that it suffices to minimize \(\|-^{*}\|_{2}\) efficiently. As proved in Section 2.1, we can start with an initial vector \(_{0}\) such that \((_{0},^{*}) 1/(5t)\) by calling Algorithm 4 (in Appendix B.1). Denote \(_{k}=(_{k},^{*})\) and consider the case when \(_{k}^{}(t^{2}/2)\). We establish the following claim:

**Claim 2.7**.: _Let \(C_{1}:=(1-2)/\). Drawing \(N_{2}=(d(1/)/((1-2)^{2}^{}))\) samples from distribution \(\), we have that if \(_{k}^{}(t^{2}/2)\) then with probability at least \(1-\): \(\|_{k+1}-^{*}\|_{2}^{2}\|-^{*}\|_{2 }^{2}-(C_{1}/2)_{k}_{k}+4C_{1}^{2}_{k}^{2}\)._It remains to choose the step size \(_{k}\) properly to get linear convergence. By carefully designing a shrinking step size, we are able to construct an upper bound \(_{k}\) on the distance of \(\|_{k+1}-_{k}\|_{2}\) using Claim 2.7. Importantly, by exploiting the property that both \(\) and \(^{*}\) are on the unit sphere, we show that the upper bound is contracting at each step, even though the distance \(\|_{k+1}-_{k}\|_{2}\) could be increasing. Concretely, we have the following lemma.

**Lemma 2.8**.: _Let \(=0.00098\) and \(_{k}=(1-)^{k}\). Then, setting \(_{k}=(1-4)_{k}/(16C_{1})\) it holds \((_{k}/2)_{k}\) for \(k=1,,K\)._

Proof.: Let \(_{k}=(1-)^{k}\) where \(=0.00098\). This choice of \(\) ensures that \(32^{2}+1020-1 0\). We show by induction that choosing \(_{k}=(1-4)_{k}/(16C_{1})=(1-)^{k}(1-4)/(16C_{1})\), it holds \((_{k}/2)_{k}\). The condition certainly holds for \(k=1\) since \(_{1}[0,/2]\). Now suppose that \((_{k}/2)_{k}\) for some \(k 1\). We discuss the following 2 cases: \(_{k}(_{k}/2)_{k}\) and \((_{k}/2)_{k}\). First, suppose \(_{k}(_{k}/2)_{k}\). Since \((_{k}/2)_{k}\), it also holds \(_{k}_{k}\). Bringing in the fact that \(\|_{k+1}-^{*}\|_{2}=2(_{k+1}/2)\) and \(\|_{k}-^{*}\|_{2}=2(_{k}/2)\), as well as the definition of \(_{k}\), the conclusion of Claim 2.7 becomes:

\[(2(_{k+1}/2))^{2} (2(_{k}/2))^{2}-(C_{1}/2)_{k}_{k}+4C_ {1}^{2}(1-4)_{k}_{k}/(16C_{1})\] \[ 4_{k}^{2}-3C_{1}_{k}_{k}/8+C_{1}(1-4)_{k} _{k}/4=4_{k}^{2}(1-(1+8)(1-4)/512),\]

where in the second inequality we used \(_{k}_{k}\) and in the last equality we used the definition of \(_{k}\) by which \(_{k}=(1-4)_{k}/(16C_{1})\). Since \(\) is chosen so that \(32^{2}+1020-1 0\), we have:

\[(_{k+1}/2)_{k}(1-) _{k}=(1-)^{k+1},\]

as desired. Next, consider \((_{k}/2)(3/4)_{k}\). Recall that \(_{k+1}=_{}(_{k}-_{k}}(_{k}))\) and \(_{k}\), where \(\) is the unit ball2; therefore, \(\|_{k+1}-_{k}\|_{2}\|_{k}-_{k}}()-_{k}\|_{2}=_{k}\|}( _{k})\|_{2}\) by the non-expansiveness of the projection operator. Furthermore, applying Lemma 2.5 and Lemma 2.4, it holds that \(\|}(_{k})\|_{2}(5/4)\| }_{(,)(,)}[( )]\|_{2} 2(1-2)/\), i.e., we have \(\|}(_{k})\|_{2} 2C_{1}\); therefore, \(\|_{k+1}-_{k}\|_{2} 2_{k}C_{1}\), which indicates that:

\[2((_{k+1}/2)-(_{k}/2))=\|_{k+1}-^{*}\| _{2}-\|_{k}-^{*}\|_{2}\|_{k+1}-_{k} \|_{2} 2_{k}C_{1}.\]

Since we have assumed \((_{k}/2)(3/4)_{k}\), then it holds:

\[_{k+1}-(_{k+1}/2)(1-)_{k}-_{k}+_{k}-( _{k}/2)-(1-4)_{k}/16 3(1-4)_{k}/16>0,\]

since we have chosen \(_{k}=(1-4)_{k}/(16C_{1})\). Hence, it also holds that \((_{k+1}/2)_{k+1}\). 

Lemma 2.8 shows that \((_{k}/2)\) converges to 0 linearly. Therefore, using \(N_{2}=(d(1/)/((1-2)^{2}^{}))\) samples, after \(K=O((1/)(1/((t^{2}/2)^{}))=O((1/^{})\) iterations, we get a \(_{K}\) such that \(_{K} 2(_{K}/2)^{}(t^{2}/2)\). Let \(h():=(_{K}+)\). Then it holds that the disagreement of \(h()\) and \(f()\) is bounded by \([h() f()]^{}\) (see Appendix B for more details). Finally, since \(_{-1}^{}(h)=_{(,y) }[h() y]=+(1-2)\,_{ _{}}[h()(^{*} +t)]\), for any \(h:^{d}\{ 1\}\), to get misclassification error at most \(+\) (with respect to the \(y\)), it suffices to use \(^{}=/(1-2)\). Therefore, we get \(_{(,y)}[(_{K} +) y]+\), using \(N_{2}=(d(1/)/((1-2)))\) samples. Since the algorithm runs for \(O((1/))\) iterations, the overall runtime is \((N_{2}d)\). This completes the proof of Theorem 2.6. 

Proof Sketch of Theorem 1.3.: From Lemma 2.2, we get that with \((d/((1-2)^{2}p^{2}))\) samples our Initialization procedure (Algorithm 4) produces a unit vector \(_{0}\) so that \((_{0},^{*})(1/(5t),/2)\) with high probability. We construct a grid of \((^{2}/(8(1-2)^{2})\)-separated values, containing all the possible values of the threshold \(t\) of size roughly \( 1/^{2}\). We run Algorithm 2 for each possible choice of the threshold \(t\). Conditioned on the choice of \(\) and \(_{0}\) that satisfies the assumptions of Theorem 2.6, Algorithm 2 outputs a weight vector \(}\) so that \(_{(,y)}[(}+) y]+\). Using standard concentration facts, we have that with a sample size of order \((d/((1-2)))\) from \(\), we can output the hypothesis with the minimum empirical error with high probability.

SQ Lower Bound for Learning Gaussian Halfspaces with RCN

To state our SQ lower bound theorem, we require the following standard definition.

**Definition 3.1** (Decision/Testing Problem over Distributions).: _Let \(D\) be a distribution and \(\) be a family of distributions over \(^{d}\). We denote by \((,D)\) the decision (or hypothesis testing) problem in which the input distribution \(D^{}\) is promised to satisfy either (a) \(D^{}=D\) or (b) \(D^{}\), and the goal of the algorithm is to distinguish between these two cases._

**Theorem 3.2** (SQ Lower Bound for Testing RCN Halfspaces).: _Fix \(c(0,1/2)\) and let \(d\) be sufficiently large. For any \(p 2^{-O(d^{c})}\), any SQ algorithm that learns the class of (at most) \(p\)-biased Gaussian halfspaces on \(^{d}\) in the presence of RCN with \(=1/3\) to error less than \(+p/3\) either requires queries to \(((d^{1/2-c}/p^{2}))\), or needs to make at least \(2^{(d^{c})}\) statistical queries._

We note that our SQ lower bound applies to a natural testing version of our learning problem. By a standard reduction (see Lemma C.9), it follows that any learning algorithm for the problem requires either \(2^{(d^{c})}\) many queries or at least one query to \(((d^{2c-1/2}/p^{2}))\). We also note that the established bound is tight for the corresponding testing problem (see Appendix C.6).

Proof of Theorem 3.2.: For any unit vector \(^{d}\), we define the LTF \(f_{}()=(-t)\), where \(t>0\) and denote \(p=_{}[f_{}()=1]\). Let \(D_{}\) be the distribution on \((,y)\) with respect to \(f_{}\) with the random variable \(y\) supported on \(\{ 1\}\) as follows: \([y=f_{}()]=1-\) and \(\) is distributed as standard normal. Denote by \(A_{}\) the distribution \(D_{}\) conditioned on \(y=1\) and by \(B_{}\) the distribution \(D_{}\) conditioned on \(y=-1\). It is easy to see that

\[A_{}()=G()(+(1-2)\{f_{ }()>0\})/(+(1-2)p)\]

and

\[B_{}()=G()(1--(1-2)\{f_{ }()>0\})/(1--(1-2)p)\.\]

Fix unit vectors \(,^{d}\) and let \(\) be the angle between them. We bound from above the correlation between \(f_{}()\) and \(f_{}()\). Our main technical lemma is the following:

**Lemma 3.3**.: _Let \(f_{}()\) and \(f_{}()\) defined as above. Then it holds_

\[*{}_{}[f_{}( )f_{}()]-*{}_{}[f_{}()]*{}_{ }[f_{}()] 4|()| (-t^{2})(|()|t^{2})\.\]

Proof of Lemma 3.3.: We start by calculating the Hermite coefficients of the univariate function \((z-t)\). We will use the fact that \(*{}_{z}[(z-t)_{ i}(z)]=2i^{-1/2}_{i-1}(t)(-t^{2}/2)\) (see Claim C.7). Let \(c_{i}\) be the Hermite coefficient of degree \(i\). Without loss of generality (due to the rotational invariance of the Gaussian distribution), we can assume that \(=_{1}\) and \(=_{1}+_{2}\). Using standard algebraic manipulations and orthogonality arguments (see Claim C.5 for more details), we have that

\[*{}_{}[f_{}( )f_{}()]=*{}_{ _{1},_{2}}[(_{1}-t)(_{1}+_{2}-t)]= _{i 0}^{i}\,c_{i}^{2}\.\]

Note that \(_{0}()=1\), therefore \(c_{0}=*{}_{}[f_{}( )]\). Therefore, we have that

\[*{}_{}[f_{}( )f_{}()]=_{i 1}^{i} c_{i}^{2}+ *{}_{}[f_{}( )]*{}_{}[f_{ }()]\.\]

Let \(J=_{i 1}^{i} c_{i}^{2}\). To complete the proof, it remains to bound the \(|J|\). We show the following:

**Claim 3.4**.: _It holds that \(|J| 4|()|(-t^{2})|()|t^{2}\)._

Proof of Claim 3.4.: Note that from Claim C.7, we have that \(c_{i}=2_{i-1}(t)(-t^{2}/2)/\), hence, it holds that \(J=4()(-t^{2})_{i=1}^{}i^{-1}_{i-1}^{2}(t) ^{i-1}\). We use the following fact.

**Fact 3.5** (Mehler Formula, see, e.g. ).: _For \(||<1\) and \(x,y\), it holds that_

\[-}(x-y)^{2}=}_{k 0}^{k}_{k}(x)_{k}(y)-}(x^{2}+y^{2})\.\]Applying Fact 3.5 for \(=()\) and \(x=y=t\), we get that

\[|J|=4(-t^{2})_{i 1}i^{-1}_{i-1 }^{2}(t)^{i-1}  4||(-t^{2})_{i 0}_{i}^{2}(t)| |^{i}\] \[=4||(-t^{2})(}{(1+ ^{2})})\,.\]

This completes the proof of Claim 3.4. 

Using Claim 3.4, we get that

\[}_{}[f_{}()f_{}()]-}_{}[f_{ }()]}_{}[f_{ }()] 4|()|(-t^{2})(| ()|t^{2}),\]

completing the proof of Lemma 3.3. 

We associate each \(\) and \(\) to a distribution \(D_{}\) and \(D_{}\), constructed as above. The following lemma provides explicit bounds on the correlation between the distributions \(D_{}\) and \(D_{}\). Recall that the pairwise correlation of two distributions with cdfs \(D_{1},D_{2}\) with respect to a distribution with cdf \(D\) is defined as \(_{D}(D_{1},D_{2})+1_{x}D_{1}(x)D_{2}(x)/D(x)\) (see Definition C.1). We have the following lemma (see Appendix C.4 for its proof):

**Lemma 3.6**.: _Let \(D_{0}\) be a product distribution distributed as \(\{ 1\}\), where \(}_{(,y) D_{0}}[y=1]=}_{( ,y) D_{}}[y=1]=p\). We have \(_{D_{0}}(D_{},D_{}) 2(1-2)(} [f_{}()f_{}()]-} [f_{}()]}[f_{}()])\) and \(^{2}(D_{},D_{0})(1-2)(}[f_{}()]-}[f_{}()]^{2})\)._

For any \(c(0,1/2)\), there exists a set \(\) of \(2^{(d^{c})}\) unit vectors in \(^{d}\) such that for any pair \(\) satisfies \(||<d^{-1/2+c}\) (Fact C.4). We associate each \(\) with \(f_{}\) and a distribution \(D_{}\) and denote \(=\{D_{},\}\). By the definition of \(\) and Lemma 3.3, for any \(,\), we have that \(}_{}[f_{}()f_{ }()] 4d^{-1/2+c}(-t^{2})((t/d^{1/4-c/2})^{2} )+}_{}[f_{}( )]}_{}[f_{}( )]\). Since \(t/d^{1/4-c/2} 1/2\) by assumption, we get that \(|}_{}[f_{}()f_{ }()]-}_{}[f_{ }()]}_{}[f_{ }()]| d^{-1/2+c}(-t^{2})\). By Lemma 3.6, it follows that \(_{D_{0}}(D_{},D_{}) C(1-2)(-t^{2})d^{-1/2+c}\) and \(^{2}(D_{},D_{0}) C(1-2)(-t^{2}/2)\), where \(C>0\) is an absolute constant. From standard SQ machinery (see, e.g., Lemma C.3), we have that any SQ algorithm that solves the decision problem \((,D_{0})\), requires either \(2^{(d^{c})}\) queries, or at least one query to \(((t^{2})d^{1/2+c})\). Noting that \(p=O((-t^{2}/2)/t)\) (by Fact B.1) completes the proof of Theorem 3.2. 

## 4 Acknowledgements

ID was supported by NSF Medium Award CCF-2107079, NSF Award CCF-1652862 (CAREER), and a DARPA Learning with Less Labels (LwLL) grant. JD was supported by NSF Award CCF-2007757 and by the U. S. Office of Naval Research under award number N00014-22-1-2348. DK was supported by NSF Medium Award CCF-2107547 and NSF Award CCF-1553288 (CAREER). PW was supported in part by NSF Award CCF-2007757. NZ was supported in part by NSF award 2023239, NSF Medium Award CCF-2107079, and a DARPA Learning with Less Labels (LwLL) grant.