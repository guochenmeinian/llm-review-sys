# CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework for Zero-Shot Electroencephalography Signal Conversion

CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework for Zero-Shot Electroencephalography Signal Conversion

 Anders Vestergaard Norskov Alexander Neergaard Zahid Morten Morup

Department of Applied Mathematics and Computer Science

Technical University of Denmark

andersxa@gmail.com {aneol,mmor}@dtu.dk

https://github.com/andersxa/CSLP-AE

###### Abstract

Electroencephalography (EEG) is a prominent non-invasive neuroimaging technique providing insights into brain function. Unfortunately, EEG data exhibit a high degree of noise and variability across subjects hampering generalizable signal extraction. Therefore, a key aim in EEG analysis is to extract the underlying neural activation (content) as well as to account for the individual subject variability (style). We hypothesize that the ability to convert EEG signals between tasks and subjects requires the extraction of latent representations accounting for content and style. Inspired by recent advancements in voice conversion technologies, we propose a novel contrastive split-latent permutation autoencoder (CSLP-AE) framework that directly optimizes for EEG conversion. Importantly, the latent representations are guided using contrastive learning to promote the latent splits to explicitly represent subject (style) and task (content). We contrast CSLP-AE to conventional supervised, unsupervised (AE), and self-supervised (contrastive learning) training and find that the proposed approach provides favorable generalizable characterizations of subject and task. Importantly, the procedure also enables zero-shot conversion between unseen subjects. While the present work only considers conversion of EEG, the proposed CSLP-AE provides a general framework for signal conversion and extraction of content (task activation) and style (subject variability) components of general interest for the modeling and analysis of biological signals.

## 1 Introduction

Electroencephalography (EEG) is a non-invasive method for recording brain activity, commonly used in neuroscience research to analyze event-related potentials (ERPs) and gain insights into cognitive processes and brain function . However, EEG signals are often noisy, contain artifacts, and exhibit high sensitivity to subject variability , making it challenging to analyze and interpret the data. In particular, the inherent subject variability is well known to confound recovery of the task content of the signal, and a study by Gibson et al.  demonstrated that across-subject variation in EEG variability and signal strength was more significant than across-task variation. A major challenge modeling EEG data is thus to remove the intrinsic subject variability in order to recover generalizable patterns of the underlying neural patterns of activation.

Supervised methods explicitly classifying tasks can potentially filter subject variability and recover generalizable patterns of neural activity. However, explicitly disentangling subject and task content in EEG signals is valuable not only for classifying task content but also for characterizing the subject variability which ultimately can provide biomarkers of individual variability. As such, rather than focusing only on the experimental effects and considering inter-subject variability as noise it should be treated as an important signal enabling the understanding of individual differences [24; 55].

Deep learning-based feature learning has become prevalent in EEG data analysis. As such, auto-encoders have shown promise in learning transferable and feature-rich representations [36; 60; 70] and have been applied to various downstream tasks, including brain-computer interfacing (BCI) [35; 44; 78], clinical epilepsy and dementia detection [19; 38; 77], sleep stage classification [34; 63], emotion recognition [26; 61], affective state detection [53; 74] and monitoring mental workload/fatigue [75; 76] with promising results. For a systematic review on deep learning-based EEG analysis, see e.g. Roy et al. .

The task of disentangling content (signal) from style (individual variability) is a well-known aim in voice conversion technologies. A study on speech representation learning achieved promising results in disentangling speaker and speech content using speaker-conditioned auto-encoders , while Chou et al.  proposed using instance normalization to enforce speaker and content separation in the latent space. In Qian et al.  zero-shot voice conversion was proposed using a simple autoencoder conditioned on a pretrained speaker embedding model and exploring bottleneck constriction to obtain content and style disentanglement with good results. Wu and Lee  and Wu et al.  disregarded the pretrained speaker embedding model by generating content latents based on a combination of instance normalization and vector quantization of an encoded signal, while the speaker latents were generated based on the difference between content codes and the encoded input signal.

Disentangling subject and task content has been shown to enhance model generalization in emotional recognition and speech processing tasks [4; 50; 57]. Bollens et al.  used two explicit latent spaces in a factorized hierarchical variational autoencoder (FHVAE) to model high-level and low-level features of EEG data and found that the model was able to disentangle subject and task content. They also found that high-level features were more subject-specific and low-level features were more task-specific. Rayatdoost et al.  and Ozdenizci et al.  explored adversarial training to promote latent representations that were subject invariant. Recently, self-supervised learning has gained substantial attention due to its strong performance in representation learning enabling deep learning models to efficiently extract compact representations useful for downstream tasks. This includes the use of (pre-)training on auxiliary tasks  as well as contrastive learning methodologies guiding the latent representations . Shen et al.  used contrastive learning between EEG signal representations of the same task and different subjects to learn subject-invariant representations and found that the learned representations were more robust to subject variability and improved generalizability. For a survey of self-supervised learning in the context of medical imaging, see also .

Inspired by recent advances in voice conversion, we propose a novel contrastive split-latent permutation autoencoder (CSLP-AE) framework that directly optimizes for EEG conversion. In particular, _we hypothesize that the auxiliary task of optimizing EEG signal conversion between tasks and subjects requires the learning of latent representations explicitly accounting for content and style_. We further use contrastive learning to guide the latent splits to respectively represent subject (style) and task (content). The evaluation of the proposed method is conducted on a recent, standardized ERP dataset, ERP Core , which includes data from the same subjects across a wide range of standardized paradigms making it especially suitable for signal conversion across multiple tasks and subjects. We contrast CSLP-AE to conventional supervised, unsupervised (AE ), and self-supervised training (contrastive learning [7; 41; 49; 59; 69]).

## 2 Methodology

The aim of this paper is to develop a modeling framework for performing generalizable (i.e., zero-shot) conversion of EEG data considering unseen subjects. The procedure should enable conversion of EEG from one subject to another as well as one task to another task. In this context, "tasks" refer to the specific ERP components present in the EEG data, such as face or car perception, word judgement of related or unrelated word pairs, perception of standard or deviant auditory stimuli, etc. .

The standard autoencoder (AE) model consists of an encoder, denoted as \(E_{}():\), and a decoder, denoted as \(D_{}():\). The encoder maps the input data to a latent space, while the decoder reconstructs the input from the latent space. The encoder and decoder are parameterized by \(\) and \(\) respectively.

To enable the model to perform conversion, it needs to be conditioned on the target subject and/or task. The latent space appears to be the suitable place for conditioning, as it is a compact representation often referred to as the bottleneck of the model. However, since the latent space is shared across subject and task representations, partitioning it into specific subject and task streams is non-trivial.

To address this challenge, a split-latent space is explored, which explicitly divides the latent space into subject and task disentangled representations. This is achieved by introducing a split in the model design within the encoder. The split-latents can be obtained by encoding the input data as follows: \(E_{}()=(^{()},^{()})\), where \(^{()}\) represents the subject latent and \(^{()}\) represents the task latent, such that \(E_{}():(,)\). Note that the \(()\) and \(()\) denominations are not inherent from the model architecture but are necessary distinctions for use in the model loss functions. These split-latents can then be joined and decoded using a similar split within the decoder. By feeding the split-latents into the decoder, the input can be reconstructed as \(}=D_{}(^{()},^{()})\), such that \(D_{}(^{()},^{()}):(,)\). The concept of split-latent space has been explored by researchers within speech  and in the EEG domain  using separate encoders for each latent space. We presently employ a shared encoder to reduce the number of parameters used. However, separate encoders - or even pre-trained encoders, specifically, on subjects - could help kick-start the training process or work as conditioning with frozen weights. We leave this to further studies.

While voice conversion [48; 68] and other voice synthesis problems [40; 56] based on autoencoders generally use models with expansive receptive fields, e.g. as in WaveNet , other studies have found similar performance in voice conversion using simpler architectures, such as CNNs [10; 27; 33]. Voice conversion is usually done over a large number of samples with exceptionally high sampling rate compared to EEG data. ERP Core uses a sampling frequency of 256 Hz (downsampled from 1024 Hz) over an epoch window of 1 second, which only yields a time resolution of 256 samples. Therefore, a large receptive field is unnecessary, and strided convolution to reduce time-resolution is used instead. The proposed EEG auto-encoder model with split latent space is illustrated in Figure 1.

Figure 1: The proposed EEG auto-encoder model with split latent space. The encoder and decoder are mirrored deep convolutional neural networks using one-dimensional convolutions. Each part of the auto-encoder consists of ConvBlocks which are made up of three convolutions with residual connections (as in He et al. ), rectified linear unit (ReLU) activation  and instance normalization  (similar to Chou et al. ). The encoder applies a ConvBlock together with a strided convolution (stride=2), to reduce the time-resolution by half, four times. The decoder is mirrored and uses transposed convolutions (fractionally strided, stride=\(\)2) to upscale the time-resolution by a factor of two with each block. Both the encoder and decoder models use a transformer  with four layers on each side of the bottleneck to confer attention. Finally, on the encoder side, a split is made into subject and task latent spaces. The decoder takes these split-latents as inputs and joins them again in the bottleneck in order to reconstruct the input. \(k\) is the kernel size and \(p\) is the padding on both sides.

### Split-latent permutation

To guide the autoencoder in Figure 1 in disentangling task and subject we propose the use of latent-permutation, which is a self-supervised approach that guides the latent space by ensuring consistency in the subject and/or task encodings between permutations. This can be seen as a direct loss relating to the conversion method described in this section.

To achieve zero-shot conversion the respective subject and task information need to be extracted from the input data, and with an explicit split of the latent space, the conversion becomes straightforward and practical. Depending on the desired conversion task, such as converting from subject \(U\) to subject \(V\) or from task \(M\) to task \(N\), the corresponding split-latents are simply swapped with that of the target subject or task. This conversion method is illustrated in Figure 1(a).

Given a pair of input samples \((_{i}^{a},_{i}^{b})\), where \(i\) is the batch index, the two pairs of split-latents are defined from \(E_{}\) splitting the latent space in two parts yielding (\(_{i}^{(,a)}\), \(_{i}^{(,a)}\)) and (\(_{i}^{(,b)}\), \(_{i}^{(,b)}\)). A latent permutation is performed which swaps two of the latents in a given latent space \(\{,\}\) before reconstruction. A comprehensive glossary of symbols and abbreviations is provided in the appendix.

Consider the latent space \(=\). The pair of input samples \((_{i}^{a},_{i}^{b})\) are sampled to both belong to the same task \(t_{i}\). The task latents are swapped between the pairs such that the reconstructed EEG data decoded by \(D_{}\) becomes

\[E_{}(_{i}^{a}) =(_{i}^{(,a)},_{i}^{(,a)}), }_{i}^{(,a)} =D_{}(_{i}^{(,a)},_{i}^{(, b)})\] (1) \[E_{}(_{i}^{b}) =(_{i}^{(,b)},_{i}^{(,b)}), }_{i}^{(,b)} =D_{}(_{i}^{(,b)},_{i}^{(, a)})\] (2)

here colorized according to their corresponding input sample \(_{i}^{a}\) or \(_{i}^{b}\). This is the same-task latent-permutation since pairs belong to the same task. A corresponding swap can be done for the subject latent space \(\) with pairs belonging to the same subject, \(s_{i}\). The task and subject latent space permutations are illustrated in Figure 1(b) and Figure 1(c) respectively.

The latent-permutation loss is defined as the sum over the pair of reconstruction losses between the two samples and their corresponding reconstructions with split-latents from latent space \(\) swapped:

\[L_{LP}(;_{i}^{a},_{i}^{b})=_{i=1}^{N} (\|_{i}^{a}-}_{i}^{(,a)}\|_{2}^{2}+\|_{ i}^{b}-}_{i}^{(,b)}\|_{2}^{2})\] (3)

Consider the scenario where \(=\) again using the reconstructions from Eq. (1). In Eq. (3) the \(L_{2}\)-norm is calculated between the input data \(_{i}^{a}\) and the reconstruction of the latent-permutation (\(_{i}^{(,a)},_{i}^{(,b)}\)). According to the smoothness meta-prior proposed by Bengio et al.  a pair of task latents should be invariant to local perturbations in the input. Seen in the context of latent-permutation, such a local perturbation could be equivalent to two task representations of the same class \(t_{i}\) from different input samples. When the latent space is locally smooth (i.e. encoder is consistent) then this term approximates the autoencoder reconstruction loss, i.e. if \(_{i}^{(,a)}_{i}^{(,b)}\) then \(D_{}(_{i}^{(,a)},_{i}^{(,a)}) D_{ }(_{i}^{(,a)},_{i}^{(,b)})\) where \(D_{}(_{i}^{(,a)},_{i}^{(,a)})\) is the standard reconstruction when both \(E_{}\) and \(D_{}\) are consistent. This is expanded upon in the supplementary material.

Notably, such autoencoding enables flow of structural information specific for the given input sample and its reconstruction beyond subject and task content. In the supplementary material, we explore a setup where the output sample is from a different subject _and_ task than the input sample.

In contrast to AutoVC , which achieves disentanglement by adding an explicit speaker latent and reducing the capacity of the content encoder through a smaller latent dimension, the proposed split-latent permutation directly optimizes for conversion. It is not guaranteed that it will explicitly separate task and subject information in their respective spaces. When the capacities of the latents are sufficiently large, both subject and task content can potentially be encoded together, and the decoder can learn to extract the corresponding subject and task content from these identically encoded splits. To address this issue and avoid relying solely on disentanglement achieved through careful bottleneck tuning, we explore contrastive learning to suffice the smoothness criteria, disentangle and _specialize_ each latent space. Limiting the capacity of the latent space is explored in the supplementary material.

### Contrastive Learning

Contrastive learning is a self-supervised learning method which aims to learn a representation of the data by maximizing the similarity between positive pairs and minimizing it between negative pairs [7; 8; 21] achieving local smoothness around classes. We apply contrastive learning to each split-latent space. This is done by utilizing a batch construction technique to sample pairs for both subjects and tasks separately, and then applying the contrastive loss to each split-latent space according to denomination, thus _specializing_ the given latent space for either subject or task embeddings.

Specifically, we consider the multi-class \(N\)-pair loss  which is a deep metric learning method  utilizing a special batch construction technique. The batch construction technique involves sampling two samples from each class, and then constructing \(K\) pairs of samples from the batch. This batch construction is similarly required for the latent-permutation in Section 2.1. We use the batch construction method combined with the InfoNCE generalization from Oord et al.  with the \(\) temperature parameter as in Chen et al. . The full generalization is equivalent to the CLIP-loss from Radford et al.  in which we minimize the symmetric cross-entropy loss of the temperature-scaled similarity matrix based on the NT-Xent loss . Let \(^{A}^{C K}\) and \(^{B}^{C K}\) be latent representation matrices of the \(K\) pairs of samples, then \(L_{}\) and \(L_{}\) are defined as

\[L_{}(;^{},^{ },k) =-(^{}_{k},^{ }_{k})/)}{_{i=1}^{K}_{[i k]}((^{}_{k},^{}_{i})/)}\] (4) \[L_{}(;^{A},^{B}) =_{k=1}^{K}L_{}(; ^{A},^{B},k)+L_{}(;^{B},^{A}, k)\] (5)

where \(_{[c]}\{0,1\}\) is the indicator function yielding \(1\) iff the condition \(c\) holds true. \((^{}_{i},^{}_{k})\) is a similarity metric. \(\) denotes from which latent space the pairs have corresponding labels, e.g. for the task latent space \(\), the \(k\)'th pair has the same task \(t_{k}\) which is different from the other tasks in the same batch, i.e. \(t_{1} t_{2} t_{K}\). \(L_{}(;,)\) therefore is a contrastive loss across tasks, while \(L_{}(;,)\) is across subjects.

Figure 2: **(a)**: Split-latent conversion example. Two samples \(X^{a}\) and \(X^{b}\), where \(X^{a}\) corresponds to subject \(U\) and task \(M\) and \(X^{b}\) corresponds to subject \(V\) and task \(N\) respectively, are encoded yielding a pair of split-latents for each sample. In this example, to convert from task \(M\) to task \(N\), the subject (\(U\)) latent is kept while the task (\(M\)) latent is simply swapped with the corresponding latent from the other sample which encodes task \(N\). The split-latents are then decoded to ideally obtain a sample from the data distribution given subject \(U\) and task \(N\). This is the ideal case where the split-latents are perfectly disentangled and independent. **(b)** and **(c)**: Illustration of the object class-dependent swap. The input samples are encoded yielding their corresponding split-latents (\(z^{(S,a)}\), \(z^{(S,b)}\), \(z^{(T,a)}\), \(z^{(T,b)}\)). Depending on the object class (tasks in **(b)**, subjects in **(c)**), the split-latents are swapped between the two pairs of latents. These are then decoded yielding the reconstructed EEG data. The same-task pairs both have blue (M) task-latents since these are intentionally sampled from the same class. The intention of the latent-permutation method is expressed when the model is consistent in its latent representation of the class, and when the split latent spaces are sufficiently disentangled in the ideal case. In such cases the swap will have negligible impact on the conversion, i.e. if they encode the same (subject or task) information.

Experimental Setup

DataWe consider the ERP Core dataset1 from Kappenman et al.  providing a standardized ERP dataset containing data from 40 subjects across six different tasks based on seven widely used ERP components: N170 (Face Perception Paradigm), MMN (Mismatch Negativity, Passive Auditory Oddball Paradigm), N2pc (Simple Visual Search Paradigm), N400 (Word Pair Judgement Paradigm), P3 (Active Visual Oddball Paradigm), and LRP and ERN (Lateralized Readiness Potential and Error-related Negativity, Flankers Paradigm). We only consider data processed by Script #1 up until ICA preparation.2 For more information on the data and paradigms, see Kappenman et al. .

Only the epoch windows around the time-locking event as described in Kappenman et al.  are used as epochs, therefore, for each paradigm there are two available "tasks" each with a different resulting ERP. The two classes assigned for each event per paradigm are: N170; faces/cars, MMN; deviants/standards, N2pc; contralateral/ipsilateral, N400; unrelated/related, P3; rare/frequent, and ERN and LRP; incorrect/correct. The dataset was split across subjects into a training set of 70%, an evaluation set of 10%, and a test set of 20% of the subjects respectively. The exact splits are available in the supplementary material.

The ERP Core dataset is predominantly time-locked with data centered around either stimulus or response3. We further evaluate the models "as is" on two other modalities of EEG data from PhysioNet  and with already established state of the art model results: the EEG Motor Movement/Imagery Dataset (EEGMMI)  and the Sleep-EDF Expanded (SleepEDfx) database . The EEGMMI dataset is cue time-locked and consists of recordings from 109 subjects performing various motor imagery (MI) tasks. We applied our model to the standard L/R/0/F MI task using 3s epoch windows, closely following the approach of Wang et al. . The SleepEDfx is included to explicitly probe the model integrity on data that is not time-locked to an external stimulus and contains 153 polysomnography studies from 78 subjects. Given its limited EEG channels, we adapted our methodology by considering a single EEG channel and applied a short-time Fourier transform to fit the data to the same setup as used for ERP Core. We maintained the conventional 30s time series windows commonly used in sleep stage literature. Details on the data preprocessing and model integration for these datasets can be found in the supplementary materials.

Model comparisonsThe proposed CLSP-AE approach is systematically compared against a mix of learning strategies comprising conventional AE-based representation learning, contrastive learning and representations obtained by supervised learning. To optimally compare these learning strategies all models are based on the same model structure given in Figure 1. Models will be denoted by which losses they are trained on, or which external methods they use. Here CSLP-AE will denote the contrastive split-latent permutation over both subjects and latents (\(L_{LP}(;,)\), \(L_{LP}(;,)\), \(L_{}(;,)\) and \(L_{}(;,)\)), SLP-AE the corresponding model without the contrastive loss components (\(L_{LP}(;,)\), and \(L_{LP}(;,)\)), CL the contrastive loss in both of the split latent spaces (\(L_{}(;,)\) and \(L_{}(;,)\)). Cosine similarity will be used as the similarity metric in the contrastive loss. AE will denote the standard auto-encoder with mean-squared error reconstruction loss on the reconstructions of non-permuted split-latents. C-AE will denote the combination of reconstruction loss and contrastive learning, such that contrastive learning is applied in both spaces _and_ the standard autoencoder reconstruction loss is applied to non-permuted reconstructions. CE will denote the supervised learning cross-entropy loss jointly trained in the subject and task latent spaces using the corresponding true labels in a supervised manner. Similarly, CE(t) will denote cross-entropy only trained on the task labels in the task latent space. This is to substitute and compare with a supervised deep learning model, contrary to the self-supervised methods described here. For completeness, we also included the common spatial pattern (CSP) method  using the multi-class generalization from Grosse-Wentrup and Buss  which is a supervised method for extracting discriminative features from EEG data. These features are then used to map unseen data into the same "CSP space". For the EEGMMI and SleepEDfx datasets we respectively compared the task and sleep stage classification performance to .

The total loss for models with multiple losses is the sum of losses with equal weighting. This is further detailed in the supplementary materials and loss curves are shown in the appendix.

Hyperparameters were chosen based on evaluation during development, and the most critical hyperparameters (the number of blocks and the size of the latent space) were verified on the evaluation set in a grid search. See supplementary material for details on the grid search across both latent dimension and number of blocks. The test set was used only for final evaluation.

Subject and task characterizationNon-linear classifiers were trained to classify the subject and task labels of split-latents from the test set to quantify the disentanglement and generalization of the latent spaces. This was performed as two five (5)-fold cross-validations (CVs) over the subject and task latents respectively stratified on the subject and task labels on the test data (unseen subjects). Since there is high class-imbalance in the task labels, undersampling was performed for each class to match the number of samples in the least represented class. The undersampling was performed on the training split of each fold, and the test split was left untouched. Balanced accuracy  was used due to the high class-imbalance. The subject CV was used to evaluate the subject classification accuracy (S.acc%) and task-on-subject classification accuracy (T\(\)S.acc%), while the task CV was used to evaluate the task classification accuracy (T.acc%) and subject-on-task classification accuracy (S\(\)T.acc%). The task-on-subject classification accuracy was evaluated by training a classifier on the subject latents to predict the task of the corresponding input sample, and vice versa for the subject-on-task classification accuracy.

For each fold, an end-to-end tree boosted system (XGBoost)  was trained on the training split, and subsequently evaluated on the test split. Finally, the results were averaged over the five (5) folds. All classifications were performed on a single-trial level. A K-nearest neighbors (KNN) classifier and an Extra Trees classifier  were also trained and evaluated. See supplementary material for these.

ERPs from zero-shot EEG conversionsAn ERP conversion loss was measured on the test set. First, a ground-truth ERP was found for each subject and for each ERP component (task) by averaging over all samples belonging to the same subject and task only in the EEG channel respective to which Kappenman et al.  found the given ERP component most prominent. Let \(}_{(s,t)}^{}\) denote such an ERP for subject \(s\) and task \(t\). If disentanglement was successful then the conversion method described in Section 2.1 and illustrated in Figure 2 should be able to reconstruct the ERP from a given subject and task latent pair. Thereby it should be possible to sample an amount of subject and task latent pairs encoded on the test set, and convert the ERP from these pairs. Let S.s. and D.s. denote sampling _task latents_ from the same or different target subject \(\) respectively, and let S.t. and D.t. denote sampling _subject latents_ from the same or different target task \(\) respectively. For a conversion to be valid subject latents must all come from samples with target subject \(s_{k}=\) and task latents from samples with \(t_{k}=\). S.s., D.s., S.t., and D.t. are then used as additional conditions for sampling. We then consider all combinations (S.s., S.t.), (D.s., D.t.), (D.s., S.t.), (S.s., D.t.) for different conversion abstractions. \(N\) pairs of subject and task latents corresponding to the targets are drawn using the specific considered combination of conditions. The EEG is reconstructed for the \(k\)'th sample to obtain \(_{k}^{(,)}^{T}\) where \(T\) is the number of time-samples. The converted ERP (C-ERP) is measured by averaging over each of these converted EEG signals: \(_{(,)}^{}=_{k=1}^{N}_{k} ^{(,)}\). The ERP conversion loss is calculated as the mean squared error (MSE) between the converted ERP and the per-subject per-task ERP: \(L_{}(,)=\|_{(, )}^{}-_{(,)}^{}\|_{2}^{2}\). An illustration of this procedure and examples of these ERPs are available in supplementary material.

An ERP was measured for each subject and task combination and was compared with the corresponding ERP from converted EEGs. The reported ERP conversion MSE is the average over all target subject and target task combinations. The number of samples (\(N\)) was set to 2000 for all methods. See supplementary material for an analysis of how this number affects the ERP conversion MSE.

A more detailed summary of the data, pre-processing, hyperparameters and training for all models, architectures and methods can be found in the supplementary material.

Results and Discussion

Table 1 shows the results of the task and subject classification as well as EEG conversion. The stand-alone CL, CE and CE(t) models do not have a decoder, and therefore do not have a reconstruction loss. The standard error of the mean is reported for all classification accuracies and ERP conversion MSEs over the five (\(n=5\)) repeats of each model.

From the results on ERP Core (Table 1), the best subject and task classifications were obtained using the proposed CSLP-AE whereas the second best performant models for subject classification and task classification were C-AE and SLP-AE respectively. We further observe a substantial performance increase in the subject and task classification of the SLP-AE when compared to the conventional AE whereas SLP-AE even provides higher task accuracies than conventional supervised training (i.e., CL, CE, CE(t), and CSP). Importantly, AE and C-AE exhibit poor conversion performance and can only reconstruct good ERPs in the standard autoencoder regime considering same subject and same task (S.s., S.t.). We observe that the SLP-AE and CSLP-AE both perform well in conversion when either the task or subject latents come from other samples (D.s., S.t.) and (S.s., D.t.) achieving reconstruction errors similar to (S.s., S.t.). Examples of converted ERPs from (D.s., S.t.) and (S.s., D.t.) are shown in Figure 2(b).

Experiments on the EEGMMI dataset (Table 2) showed similar results with the CSLP-AE model outperforming both the C-AE and SLP-AE models, and achieving on par performance with the current state-of-the-art model: CSLP-AE task accuracy of 64.28\(\)0.16% compared to 65.07% achieved by EEGNet. However, the SLP-AE model performed considerably worse than the CSLP-AE and C-AE models which performed similarly on the SleepEDFx dataset. Split-latent permutation, therefore, does not seem to increase task classification accuracy on non-time-locked data.

The CSLP-AE model achieved a sleep stage classification accuracy of 75.16\(\)0.96% which is notably lower than the state-of-the-art model XSleepNet2  with 84.0% accuracy. With an accuracy of 75.16\(\)0.96%, the model demonstrates a capability beyond mere chance, effectively characterizing sleep stages outside the constraints of the time-locked paradigm.We presently restricted the model to Fourier-compressed representations of the data but we expect performance could be increased using encoders with larger receptive fields such as WaveNet [9; 40; 56] on the raw EEG waveform data.

\(t\)-distributed stochastic neighbor embedding [1; 31; 65] (\(t\)-SNE) plots are provided in Figure 2(a) for the SLP-AE, C-AE and CSLP-AE models on the ERP Core dataset (further details of the \(t\)-SNE and additional plots are provided in the supplementary). From the figure we observe that the SLP-AE

   Model & S.acc\% & T-S.acc\% & Tacc\% & S-Tacc\% & (S.s., S.t.) & (D.s, D.t.) & (D.s., S.L) & (S.s., D.t.) \\  CSLP-AE & **80.32 \(\) 0.28** & 45.41 \(\) 0.37 & **48.48 \(\) 0.34** & 79.64 \(\) 0.37 & 4.21 \(\) 0.12 & 20.06 \(\) 0.10 & **5.80 \(\) 0.15** & 6.65 \(\) 0.23 \\ SLP-AE & 74.63 \(\) 0.74 & 47.23 \(\) 0.31 & 47.00 \(\) 0.32 & 74.70 \(\) 0.73 & 3.82 \(\) 0.04 & **19.92 \(\) 0.10** & 6.12 \(\) 0.09 & **5.02 \(\) 0.08** \\  C-AE & 79.42 \(\) 0.48 & 37.34 \(\) 0.45 & 46.59 \(\) 0.23 & 73.27 \(\) 0.25 & 4.28 \(\) 0.06 & 20.28 \(\) 0.07 & 11.33 \(\) 0.47 & 10.64 \(\) 0.30 \\ AE & 60.68 \(\) 0.16 & 31.62 \(\) 0.27 & 31.43 \(\) 0.28 & 61.08 \(\) 0.38 & **3.54 \(\) 0.12** & 20.82 \(\) 0.07 & 11.20 \(\) 0.32 & 10.74 \(\) 0.48 \\  CL & 78.82 \(\) 0.46 & 37.65 \(\) 0.54 & 45.36 \(\) 0.37 & 71.70 \(\) 0.55 & - & - & - & - \\ CE & 79.25 \(\) 0.37 & 35.52 \(\) 0.38 & 45.22 \(\) 0.23 & 64.73 \(\) 0.44 & - & - & - & - \\ CE(t) & - & - & 45.80 \(\) 0.24 & 44.27 \(\) 0.59 & - & - & - & - \\ CSP & - & - & 35.22 \(\) 0.11 & 69.89 \(\) 0.10 & - & - & - & - \\   

Table 1: Single-trial balanced subject classification accuracy (S.acc%), task-on-subject classification accuracy (T–S.acc%), task classification accuracy (T.acc%), subject-on-task classification accuracy (S–T.acc%), and zero-shot same-subject same-task (S.s., S.t.), different-subject different-task (D.s., D.t.), different-subject same-task (D.s., S.t.), and same-subject different-task (S.s., D.t.) ERP conversion MSE. All ERP conversion MSE values have scales of \(10^{-11}\)V\({}^{2}\). Confusion matrices are provided in the supplementary material.

   Model & EEGMMI & SleepEDFx \\  CSLP-AE (ours) & \(64.28 0.16\) & \(75.16 0.95\) \\ C-AE (ours) & \(61.89 0.41\) & \(75.16 0.86\) \\ SLP-AE (ours) & \(57.93 0.56\) & \(70.59 1.18\) \\  EEGNet (Wang et al. ) & **65.07** & - \\ \(t\)-CTNs (Xie et al. ) & 64.22 & - \\ CNN (Dose et al. ) & 58.59 & - \\  XSleepNet2 (Phan et al. ) & - & **84.0** \\ Zhu et al.  & - & 82.8 \\ SeeSleepNet (Phan et al. ) & - & 82.6 \\ SleepTransformer (Phan et al. ) & - & 81.4 \\ AtmSuSleep (Eldele et al. ) & - & 81.3 \\ SleepEEGNet (Mousavi et al. ) & - & 80.0 \\   

Table 2: Task classification accuracy on PhysioNet  EEG Motor Movement/Imagery Dataset  (EEGMMI) and Sleep-EDF Expanded Dataset  (SleepEDFx)trained subject and task spaces are topologically similar. We further observe that the addition of the split-latent permutation loss to the contrastive learning model had little effect on the topology of the task latent space but some effect on the subject latent space. As the observed topology of the latent space did not undergo significant changes, the improvement in conversion performance can primarily be attributed to the capabilities of the decoder and the information flow within it to implicitly account for the permutation-invariance. This points to the importance of accounting for _structural encoding_.

The different ERP conversion abstractions described in Section 3 are increasingly more difficult to convert from. (S.s., S.t.) allows structural information through both latent spaces, while (D.s., S.t.) and (S.s., D.t.) allow structural information through one latent space since one of the latents will come from the same input sample. (D.s., D.t.) is a hard problem requiring conversion with no retention of any structural information from the specific sample for the decoder to rely on, i.e. conversion must be done using only abstract subject and task embeddings from which structure must arise.

Models trained with latent-permutation learns this structural encoding in the latent spaces since the decoder can rely on at least one of the latent spaces in the latent-swap procedure (Figure 2) to provide the structural information of the EEG signal. This structural information, although indistinguishable in different latent spaces, coincidentally is highly correlated with both subject and task content of the signal. This structural encoding is most notable in the SLP-AE latent space as shown in Figure 2(a) allowing the classifier to perform well on all classification tasks. However, the SLP-AE model obtains worse performance on subject classification compared to the other deep learning models. Interestingly, it had identical performance of subject classification on either subject or task latents, and similarly for task classification (S.acc%\(\)S-T.acc% and T.acc%\(\)T\(\)S.acc% for SLP-AE in Table 1), which further accentuates their topological similarities seen in Figure 2(a).

Contrastive learning in the latent space itself has nothing to do with decoding the structure of the data for reconstruction. The encoder is simply trained to learn an embedding of the subject and task content of the signal. The decoder must, therefore, do the heavy work of extracting this structural information itself. This exposes a property which is applicable to the latent-permutation method but not contrastive learning. When the stand-alone latent-permutation method is used (SLP-AE), the decoder is allowed a reliability in the latent spaces.

A lot of the EEG signal is structural information, therefore, there might be more to gain from minimizing the permutation-invariance to structural information rather than encoding the subject and task content directly. Thereby, the decoder can learn a permutation-invariant structural encoding of the signal in both latent spaces, which allows the decoder to rely on this information irrespective of the permutation or swap - since only one latent space (see Figure 2) is swapped at a time. Thus, the latent-permutation only trained model (SLP-AE) does not learn explicitly disentangled representations of the subject and task content of the signal, but rather a duplicated latent space permutation-invariant

Figure 3: **(a)**\(t\)-SNE plots of split-latents as encoded on the test set (unseen subjects) of the ERP Core dataset, colored by true labels. Rows show _subject_ and _task_ latent spaces, respectively, while columns indicate model type (SLP-AE, C-AE, and CSLP-AE respectively). For task latent space colorized by subject, and vice versa, see supplementary material. **(b)** Converted ERPs from the same three models for a random target subject and target paradigm. All latents used in conversion were from unseen subjects on the test set, i.e. unseen to unseen conversion. The FCz channel was chosen according to which channel Kappenman et al.  found to most prominently show the ERP of the given paradigm. For more conversion examples see supplementary material.

structural encoding which is highly correlated with both subject and task content of the signal. This interestingly is also the goal of the standard autoencoder. The latent-permutation method allows the model to learn an encoding explicitly in the latent space instead of implicitly in the encoder/decoder networks. This might be a powerful tool since it further constricts the bottleneck in the auto-encoder sense, although at the cost of suboptimally encoding identical latent spaces. This can be, and is, remedied by using both contrastive learning and latent-permutation in conjunction.

Having completely disentangled latent spaces is a local minimum in the latent-permutation method and allows for the ideal swap in Figure 1(a). The CSLP-AE model is able to keep the latent spaces disentangled while also providing the structural encoding information required for the conversion method to work. This is evident from Table 1 which shows that the stand-alone latent-permutation model (SLP-AE) achieves about half (\( 51.7\%\)) the MSE error of the C-AE model on the ERP conversion tasks using samples from different tasks or subjects (D.s, S.t. and S.s., D.t.), and the CSLP-AE model retains this performance. We propose the latent-permutation method as a replacement for the standard auto-encoder reconstruction loss to be used in conjunction with contrastive learning and the batch construction method to provide disentangled latent spaces which also allows for the structural encoding information to flow through and ease zero-shot conversion.

The latent-permutation method does not increase performance on the (D.s., D.t.) conversion task. Arguably the difference in performance between the stand-alone latent-permutation and contrastive learning methods (on D.s, S.t. and S.s., D.t.) is due to this structural encoding property. An analysis is provided in the appendix providing a generalization of the latent-permutation method onto different subject/different task conversion to circumvent the structural encoding reliability. Further research could explore different avenues for keeping structural information while also disentangling subject and task latents, or providing a source for this structural encoding using generative methods or distributional power to generate structure, e.g. through the use of a VAEs .

## 5 Conclusion

In this paper, a novel split-latent permutation framework was introduced for disentangling subject and task content in EEG signals and enabling single-trial zero-shot conversion. By combining the proposed split-latent permutation framework with contrastive learning, we achieved better performance compared to standard deep learning methods on the standardized ERP Core dataset. The experimental results demonstrated a significant 51.7% improvement in ERP conversion loss on unseen to unseen subject conversions. The method also achieved high single-trial subject classification accuracy (80.32\(\)0.28%) and single-trial task classification accuracy (48.48\(\)0.34%) on unseen subjects.

LimitationsThe conversion results in this paper are limited to the single dataset on which they were trained, and may not generalize to other datasets with different experimental conditions. The ERP Core dataset is meant to standardize ERP measurements with more data to come in the future from other laboratories which might alleviate this limitation. Furthermore, the tasks used in the experiments are all seen during training. This limits the scope of the results to the seen tasks and no conclusions can be drawn about the generalization of the model to unseen tasks.

Broader ImpactZero-shot conversion of EEG signals, similar to other deep fake methods, can be used for malicious purposes. Methods discussed in this paper intrude on one of the most sacred places yet to be exploited and confused by technology: the human mind. Malicious use of this technology includes the ability to decode thoughts and intentions from EEG signals, and the ability to create fake EEG signals to confuse verification systems using EEG signals as a biometric identifier [11; 37; 51]. Care must be taken when developing and deploying such technology to ensure that it is not used for malicious purposes. However, it can also be used to improve the quality of life for people with and without disabilities. It provides a base for generalizing EEG signal representations across subjects and tasks, which can be used to improve the performance of EEG-based BCI systems, especially on a single-trial level as attestable by the results in Table 1. Similarly, it could provide a base for novel analysis strategies, such as predicting drug or stimulus reactions in a healthy versus diseased brain, or as biomarkers of brain disorders.

Acknowledgments and Disclosure of Funding Funding Funding in direct support of this work: Lundbeck Foundation grant R347-2020-2439.