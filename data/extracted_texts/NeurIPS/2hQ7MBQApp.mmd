# The Inductive Bias of Flatness Regularization

for Deep Matrix Factorization

Khashayar Gatmiry

MIT

gatmiry@mit.edu

Zhiyuan Li

Stanford University

zhiyuanli@stanford.edu

Ching-Yao Chuang

MIT

cychuang@mit.edu

Sashank Reddi

Google

sashank@google.com

Tengyu Ma

Stanford University

tengyuma@stanford.edu

Stefanie Jegelka

TU Munich & MIT

stefje@csail.mit.edu

###### Abstract

Recent works on over-parameterized neural networks have shown that the stochasticity in optimizers has the implicit regularization effect of minimizing the sharpness of the loss function (in particular, the trace of its Hessian) over the family zero-loss solutions. More explicit forms of flatness regularization also empirically improve the generalization performance. However, it remains unclear why and when flatness regularization leads to better generalization. This work takes the first step toward understanding the inductive bias of the minimum trace of the Hessian solutions in an important setting: learning deep linear networks from linear measurements, also known as _deep matrix factorization_. We show that for all depth greater than one, with the standard Restricted Isometry Property (RIP) on the measurements, minimizing the trace of Hessian is approximately equivalent to minimizing the Schatten 1-norm of the corresponding end-to-end matrix parameters (i.e., the product of all layer matrices), which in turn leads to better generalization. We empirically verify our theoretical findings on synthetic datasets.

## 1 Introduction

Modern deep neural networks are typically over-parametrized and equipped with huge model capacity, but surprisingly, they generalize well when trained using stochastic gradient descent (SGD) or its variants . A recent line of research suggested the _implicit bias_ of SGD as a possible explanation to this mysterious ability. In particular, Damian et al. , Li et al. , Arora et al. , Lyu et al. , Wen et al. , Liu et al.  have shown that SGD can implicitly minimize the _sharpness_ of the training loss, in particular, the trace of the Hessian of the training loss, to obtain the final model. However, despite the strong empirical evidence on the correlation between various notions of sharpness and generalization [25; 23; 38; 24] and the effectiveness of using sharpness regularization on improving generalization [16; 49; 53; 39], the connection between penalization of the sharpness of training loss and better generalization still remains majorly unclear [13; 2] and has only been proved in the context of two-layer linear models [29; 37; 12]. To further understand this connection beyond the two layer case, we study the inductive bias of penalizing the _trace of the Hessian_ of training loss and its effect on the _generalization_ in an important theoretical deep learning setting: _deep linear networks_ (or equivalently, _deep matrix factorization_). We start by briefly describing the problem setup.

Deep Matrix Factorization.Consider an \(L\)-layer deep network where \(L^{+},\,L 2\) is the depth of the model. Let \(W_{i}^{d_{i} d_{i-1}}\) and \(d_{i}\) denote the layer weight matrix and width of the \(i^{}\) (\(i[L]\)layer respectively. We use \(\) to denote the concatenation of all the parameters \((W_{1},,W_{L})\) and define the _end-to-end matrix_ of \(\) as

\[E() W_{L}W_{L-1} W_{1}.\] (1)

In this paper, we focus on models that are linear in the space of the end-to-end matrix \(E(W)\). Suppose \(M^{*}^{d_{L} d_{0}}\) is the target end-to-end matrix, and we observe \(n\) linear measurements (matrices) \(A_{i}^{d_{L} d_{0}}\) and the corresponding labels \(b_{i}= A_{i},M^{*}\). The training loss of \(\) is the mean-squared error (MSE) between the prediction \( A_{i},W_{L}W_{L-1} W_{1}\) and the observation \(b_{i}\):

\[()_{i=1}^{n}( A_{i},W_{L}W_{L-1} W_{1}-b_{i}^{2}).\] (2)

Throughout this paper, we assume that \(d_{i}(d_{0},d_{L})\) for each \(i[L]\) and, thus, the image of the function \(E()\) is the entire \(^{d_{L} d_{0}}\). In particular, this ensures that the deep models are sufficiently expressive in the sense that \(_{}()=0\). For this setting, we aim to understand the structure of the trace of the Hessian minimization, as described below. The trace of Hessian is the sum of the eigenvalues of Hessian, which is an indicator of sharpness and it is known that variants of SGD, such as label noise SGD or 1-SAM, are biased toward models with a smaller trace of Hessian [29; 48].

**Min Trace of Hessian Interpolating Solution.** Our primary object of study is the interpolating solution with the minimum trace of Hessian, defined as:

\[^{*}*{arg\,min}_{:( )=0}[^{2}()].\] (3)

As we shall see shortly, the solution to the above optimization problem is not unique. We are interested in understanding the underlying structure of any minimizer \(^{*}\). This will, in turn, inform us about the generalization nature of these solutions.

### Main Results

Before delving into the technical details, we state our main results in this section. This also serves the purpose of highlighting the primary technical contributions of the paper. First, since the generalization of \(\) only depends on its end-to-end matrix \(E()\), it is informative to derive the properties of \(E(^{*})\) for any min trace of the Hessian interpolating solution \(^{*}\) defined in (3). Indeed, penalizing the trace of Hessian in the \(W\) space induces an equivalent penalization in the space of the end-to-end parameters. More concretely, given an end-to-end parameter \(M\), let the induced regularizer \(F(M)\) denote the minimum trace of Hessian of the training loss at \(\) among all \(\)'s that instantiate the end-to-end matrix \(M\) i.e., \(E()=M\).

**Definition 1** (Induced Regularizer).: _Suppose \(M^{d_{L} d_{0}}\) is an end-to-end parameter that fits the training data perfectly (that is, \( A_{i},M=b_{i},\; i[n]\)). We define the induced regularizer as_

\[F(M)_{:E()=M}[^{2} ()]\] (4)

Since the image of \(E()\) is the entire \(^{d_{L} d_{0}}\) by our assumption that \(d_{i}(d_{0},d_{L})\), function \(F\) is well-defined for all \(M^{d_{L} d_{0}}\). It is easy to see that minimizing the trace of the Hessian in the original parameter space (see (3)) is equivalent to penalizing \(F(M)\) in the end-to-end parameter. Indeed, the minimizers of the implicit regularizer in the end-to-end space are related to the minimizers of the implicit regularizer in the \(\) space, i.e.,

\[*{arg\,min}_{M:^{}(M)=0}F(M)=\{E( ^{*})^{*}*{arg\,min}_{ :()=0}[^{2}( )]\},\]

where for any \(M^{d_{L} d_{0}}\), we define \(^{}(M)_{i=1}( A_{i},M -b_{i})^{2}\) and thus \(()=^{}(E())\). This directly follows from the definition of \(F\) in (4). Our main result characterizes the induced regularizer \(F(M)\) when the data satisfies the RIP property.

**Theorem 1** (Induced regularizer under RIP).: _Suppose the linear measurements \(\{A_{i}\}_{i=1}^{n}\) satisfy the \((1,)\)-RIP condition._1. _For any_ \(M^{d_{L} d_{0}}\) _such that_ \( A_{i},M=b_{i},\; i[n]\)_, it holds that_ \[(1-)L(d_{0}d_{L})^{1/L}\|M\|_{*}^{2(L-1)/L} F(M)(1+)L(d_{0} d_{L})^{1/L}\|M\|_{*}^{2(L-1)/L}.\] (5)
2. _Let_ \(^{*}*{arg\,min}_{:()=0} [^{2}()]\) _be an interpolating solution with minimal trace of Hessian. Then_ \(E(^{*})\) _roughly minimizes the nuclear norm among all interpolating solutions of_ \(^{}\)_. That is,_ \[\|E(^{*})\|_{*}_{^{}( M)=0}\|M\|_{*}.\]

However, for more general cases, it is challenging to compute the closed-form expression of \(F\). In this work, we derive closed-form expressions for \(F\) in the following two cases: (1) depth \(L\) is equal to \(2\) and (2) there is only one measurement, _i.e._, \(n=1\) (see Table 1). Leveraging the above characterization of induced regularizer, we obtain the following result on the generalization bounds:

**Theorem 2** (Recovery of the ground truth under RIP).: _Suppose the linear measurements \(\{(A_{i})\}_{i=1}^{n}\) satisfy the \((2,(n))\)-RIP (Definition 3). Then for any \(^{*}*{arg\,min}_{:()=0} [^{2}()]\), we have_

\[\|E(^{*})-M^{*}\|_{F}^{2}}\|M^{*} \|_{*}^{2}.\] (6)

_where \((n)\) depends on the number of measurements \(n\) and the distribution of the measurements._

If we further suppose \(\{A_{i}\}_{i=1}^{n}\) are independently sampled from some distribution over \(^{d_{L} d_{0}}\) satisfying that \(_{A} A,M^{2}=\|M\|_{F}^{2}\), _e.g._, the standard multivariate Gaussian distribution, denoted by \(_{d_{L} d_{0}}\), we know \((n)=O+d_{0}}{n}}\) from Candes and Plan  (see Section 5.1 for more examples).

**Theorem 3**.: _For \(n(r(d_{0}+d_{L}))\), with probability at least \(1-((d_{0}+d_{L}))\) over the randomly sampled \(\{A_{i}\}_{i=1}^{n}\) from multivariate Gaussian distribution \(\), for any minimum trace of Hessian interpolating solution \(^{*}*{arg\,min}_{:()=0} [^{2}()]\), the population loss \(}(^{*})_{A}(  A,E(^{*})- A,M^{*})^{2}\) satisfies that_

\[}(^{*})=\|E(^{*})-M^{*}\|_{F}^{2} O +d_{L}}{n}\|M^{*}\|_{*}^{2}^{3}n.\]

Next, we state a lower bound for the conventional estimator for overparameterized models that minimizes the norm. The lower bound states that, to achieve a small error, the number of samples should be as large as the product of the dimensions of the end-to-end matrix \(d_{0}d_{L}\) as opposed to \(d_{0}+d_{L}\) in case of the min trace of Hessian minimizer. It is proved in Appendix F.

**Theorem 4** (Lower bound for \(_{2}\) regression).: _Suppose \(\{A_{i}\}_{i=1}^{n}\) are randomly sampled from multivariate Gaussian distribution \(\), let \(}=*{arg\,min}_{:()=0}\|E( {W})\|_{F}\) to be the minimum Frobenius norm interpolating solution, then the expected population loss is_

\[\,}(})=(1-d_{L }\}}{d_{0}d_{L}})\,\|M^{*}\|_{F}^{2}\,.\]

 Settings & Induced Regularizer \(F(M)/L\) & Theorem \\  \((1,)\)-RIP & \((1 O())(d_{0}d_{L})^{1/L}\|M\|_{*}^{2-2/L}\) & Theorem 1 \\ \(L=2\) & \(\|(A_{i}A_{i}^{})^{}{{2}}}M( A_{i}^{}A_{i})^{}{{2}}}\|_{*}\) & Theorem 5 () \\ \(n=1\) & \(\|(A^{T}M)^{L-1}A^{T}\|_{S_{2/L}}^{2/L}\) & Theorem 7 \\  

Table 1: Summary of properties of the induced regularizer in the end-to-end matrix space. Here \(\|\|_{S_{p}}\) denotes the Schatten \(p\)-norm for \(p[1,]\) and Schatten \(p\)-quasinorm for \(p(0,1)\) (see Definition 2). \(\|\|_{*}\) denotes the Schatten 1-norm, also known as the nuclear norm.

The lower bound in Theorem 4 shows in order to obtain an \(O(1)\)-relatively accurate estimates of the ground truth in expectation, namely to guarantee \(\,}(}) O(1)\|M^{*}\|_{F}^{2}\), the minimum Frobenius norm interpolating solution needs at least \((d_{0}d_{L})\) samples. In contrast, the minimizer of trace of Hessian in the same problem only requires \(O((d_{0}+d_{L})\|M^{*}\|_{*}^{2}/\|M^{*}\|_{F}^{2})\) samples, which is at least \((,d_{L}\}}{})\) times smaller. We further illustrate experimentally the superior generalization ability of sharpness minimization algorithms like label noise SGD [6; 10; 29] compared to vanilla mini-batch SGD Figure 1. Due to the space limits, we defer the full setting for experiments into Appendix A.

## 2 Related Work

Connection Between Sharpness and Generalization.Research on the connection between generalization and sharpness dates back to Hochreiter and Schmidhuber . Keskar et al.  famously observe that when increasing the batch size of SGD, the test error and the sharpness of the learned solution both increase. Jastrzebski et al.  extend this observation and found that there is a positive correlation between sharpness and the ratio between learning rate and batch size. Jiang et al.  perform a large-scale empirical study on various notions of generalization measures and show that sharpness-based measures correlate with generalization best. Liu et al.  find that among language models with the same validation pretraining loss, those that have smaller sharpness can have better downstream performance. On the other hand, Dinh et al.  argue that for networks with scaling invariance, there always exist models with good generalization but with arbitrarily large sharpness. We note this does not contradict our main result here, which only asserts the interpolation solution with a minimal trace of Hessian generalizes well, but not vice versa. Empirically, sharpness minimization is also a popular and effective regularization method for overparametrized models [39; 17; 53; 49; 26; 32; 54; 52; 1].

Implicit Bias of Sharpness Minimization.Recent theoretical works [6; 10; 29; 31] show that SGD with label noise is implicitly biased toward local minimizers with a smaller trace of Hessian under the assumption that the minimizers locally connect as a manifold. Such a manifold setting is empirically verified by Draxler et al. , Garipov et al.  in the sense that the set of minimizers of the training loss is path-connected. It is the same situation for the deep matrix factorization problem studied in this paper, although we do not study the optimization trajectory. Instead, we directly study properties of the minimum trace of Hessian interpolation solution.

Sharpness-reduction implicit bias can also happen for deterministic GD. Arora et al.  show that normalized GD implicitly penalizes the largest eigenvalue of the Hessian. Ma et al.  argues that such sharpness reduction phenomena can also be caused by a multi-scale loss landscape. Lyu et al.  show that GD with weight decay on a scale-invariant loss function implicitly decreases the spherical sharpness, _i.e._, the largest eigenvalue of the Hessian evaluated at the normalized parameter. Another line of work focuses on the sharpness minimization effect of a large learning rate in GD, assuming that it converges at the end of training. This has been studied mainly through linear stability analysis [50; 8; 34; 9]. Recent theoretical analysis [11; 30] showed that the sharpness minimization effect of a large learning rate in GD does not necessarily rely on convergence and linear stability, through a four-phase characterization of the dynamics at the Edge of Stability regime .

## 3 Preliminaries

**Notation.** We use \([n]\) to denote \(\{1,2,,n\}\) for every \(n\). We use \(\|M\|_{F}\), \(\|M\|_{*}\), \(\|M\|_{2}\) and \((M)\) to denote the Frobenius norm, nuclear norm, spectral norm and trace of matrix \(M\) respectively. For any function \(f\) defined over set \(S\) such that \(_{x S}f(x)\) exists, we use \(_{S}f\) to denote the set \(\{y S f(y)=_{x S}f(x)\}\). Given a matrix \(M\), we use \(h_{M}\) to denote the linear map \(A A,M\). We use \(_{r}\) to to denote the set \(_{r}\{h_{M}\|M\|_{*} r\}\). \(M_{i:}\) and \(M_{:j}\) are used to denote the \(i\)th row and \(j\)th column of the matrix \(M\).

The following definitions will be important to the technical discussion in the paper.

Rademacher Complexity.Given \(n\) data points \(\{A_{i}\}_{i=1}^{n}\), the _empirical Rademacher complexity_ of function class \(\) is defined as

\[_{n}()=}_{ \{ 1\}^{n}}_{h}_{i=1}^{n}_{i}h(A_{i}).\]

Given a distribution \(P\), the _population Rademacher complexity_ is defined as follows: \(}_{n}()=}_{A_{i}}{{}}P}_{n}()\). This is mainly used to upper bound the generalization gap of SGD.

**Definition 2** (Schatten \(p\)-(quasi)norm).: _Given any \(d,d^{}^{+}\), \(p(0,)\) a matrix \(M^{d d^{}}\) with singular values \(_{1}(M),,_{(d,d^{})}(M)\), we define the Schattern \(p\)-(semi)norm as_

\[\|M\|_{S_{p}}=(_{i=1}^{(d,d^{})} _{i}^{p}(M))^{1/p}.\]

Note that in this definition \(\|\|_{S_{p}}\) is a norm only when \(p 1\). When \(p(0,1)\), the triangle inequality does not hold. Note that when \(p(0,1)\), \(\|A+B\|_{S_{p}} 2^{}{{p}}-1}(\|A\|_{S_{p}} +\|B\|_{S_{p}})\) for any matrices \(A\) and \(B\), however, \(2^{}{{p}}-1}>1\).

We use \(L\) to denote the depth of the linear model and \(=(W_{1},,W_{L})\) to denote the parameters, where \(W_{i}^{d_{i} d_{i-1}}\). We assume that \(d_{i}(d_{0},d_{L})\) for each \(i[L-1]\) and, thus, the image of \(E()\) is the entire \(^{d_{L} d_{0}}\). Following is a simple relationship between nuclear norm and Frobenius norm that is used frequently in the paper.

**Lemma 1**.: _For any matrices \(A\) and \(B\), it holds that \(\|AB\|_{*}\|A\|_{F}\|B\|_{F}\)._

## 4 Exact Formulation of Induced Regularizer by Trace of Hessian

In this section, we derive the exact formulation of trace of Hessian for \(_{2}\) loss over deep matrix factorization models with linear measurements as a minimization problem over \(\). We shall later approximate this formula by a different function in Section 5, which allows us to calculate the implicit bias in closed-form in the space of end-to-end matrices.

We first introduce the following simple lemma showing that the trace of the Hessian of the loss is equal to the sum of squares of norms of the gradients of the neural network output.

**Lemma 2**.: _For any twice-differentiable function \(\{f_{i}()\}_{i=1}^{n}\), real-valued labels \(\{b_{i}\}_{i=1}^{n}\), loss function \(()=_{i=1}^{n}(f_{i}()-b_{i})^{2}\), and any \(\) satisfying \(()=0\), it holds that_

\[(^{2}())=_{i=1}^{n}\|  f_{i}()\|^{2}.\]

Using Lemma 2, we calculate the trace of Hessian for the particular loss defined in (2). To do this, we consider \(\) in Lemma 2 to be the concatenation of matrices \((W_{1},,W_{L})\) and we set \(f_{i}()\) to be the linear measurement \( A_{i},E()\), where \(E()=W_{L} W_{1}\) (see (1)). To calculate the trace of Hessian, according to Lemma 2, we need to calculate the gradient of \(()\) in (2). To this end, for a fixed \(i\), we compute the gradient of \( A_{i},E()\) with respect to one of the weight matrices \(W_{j}\).

\[_{W_{j}} A_{i},E() =_{W_{j}}(A_{i}^{}W_{L} W_{1})\] \[=_{W_{j}}((W_{j-1} W_{1}A_{i}^{}W_{L}  W_{j+1})W_{j})\] \[=(W_{j-1} W_{1}A_{i}^{}W_{L} W_{j+1})^{}.\]

According to Lemma 2, trace of Hessian is given by

\[(^{2}L)()=_{i=1}^{n}_{j=1}^{L}\| _{W_{j}} A_{i},E()\|_{F}^{2}= _{i=1}^{n}_{j=1}^{L}\|W_{j-1} W_{1}A_{i}^{}W_{L} W_{j+1} \|_{F}^{2}.\]

As mentioned earlier, our approach is to characterize the minimizer of the trace of Hessian among all interpolating solutions by its induced regularizer in the end-to-end matrix space. The above calculation provides the following more tractable characterization of induced regularizer \(F\) in (12):

\[F(M)=_{E()=M}_{i=1}^{n}_{j=1}^{L}\|W_{j-1} W_{1}A_{i}^{ }W_{L} W_{j+1}\|_{F}^{2}.\] (7)

In general, we cannot solve \(F\) in closed form for general linear measurements \(\{A_{i}\}_{i=1}^{n}\); however, interestingly, we show that it can be solved approximately under reasonable assumption on the measurements. In particular, we show that the induced regularizer, as defined in (7), will be approximately proportional to a power of the nuclear norm of \(E()\) given that the measurements \(\{A_{i}\}_{i=1}^{n}\) satisfy a natural norm-preserving property known as the Restricted Isometry Property (RIP) [7; 42].

Before diving into the proof of the general result for RIP, we first illustrate the connection between nuclear norm and the induced regularizer for the depth-two case. In this case, fortunately, we can compute the closed form of the induced regularizer. This result was first proved by Ding et al. . For self-completeness, we also provide a short proof.

**Theorem 5** (Ding et al. ).: _For any \(M^{d_{L} d_{0}}\), it holds that_

\[F(M)_{W_{2}W_{1}=M}[^{2}]()=2 \|(_{i}A_{i}A_{i}^{})^{}{{2}}}M(_{i}A_{i}^{}A_{i})^{}{{2}}}\|_{*}.\] (8)

Proof of Theorem 5.: We first define \(B_{1}=(_{i=1}^{n}A_{i}A_{i}^{\ T})^{}\) and \(B_{2}=(_{i=1}^{n}A_{i}^{\ T}A_{i})^{}\). Therefore we have that

\[[^{2}]()=_{i=1}^{n}(\|A_{i}^{\ T}W_{ 2}\|_{F}^{2}+\|W_{1}A_{i}^{\ T}\|_{F}^{2})=\|B_{1}W_{2}\|_{F}^{2}+\|W_{ 1}B_{2}\|_{F}^{2}.\]

Further applying Lemma 1, we have that

\[F(M) =_{W_{2}W_{1}=M}[^{2}]()= _{W_{2}W_{1}=M}_{i=1}^{n}(\|A_{i}^{\ T}W_{2}\|_{F}^{2}+\|W_{1}A_{i}^{ \ T}\|_{F}^{2})\] \[_{W_{2}W_{1}=M}2\|B_{1}W_{2}W_{1}B_{2}\|_{*}^{2}=2\|B_{1} MB_{2}\|_{*}^{2}.\]

Next we show this lower bound of \(F(M)\) can be attained. Let \(U V^{T}\) be the SVD of \(B_{1}MB_{2}\). The equality condition happens for \(W_{2}^{*}={B_{1}}^{}U^{1/2},W_{1}^{*}=^{1/2}V^{T}{B_{2}}^ {}\), where we have that \(_{i=1}^{n}\|A_{i}^{\ T}W_{2}^{*}\|_{F}^{2}+\|W_{1}^{*}A_{i}^{\ T}\|_{F}^{2}=2\| \|_{F}^{2}=2\|B_{1}MB_{2}\|_{F}^{2}\). This completes the proof. 

The right-hand side in (8) will be very close to the nuclear norm of \(M\) if the two extra multiplicative terms are close to the identity matrix. It turns out that \(\{A_{i}\}_{i=1}^{n}\) satisfying the \((1,)\)-RIP exactly guarantees the two extra terms are \(O()\)-close to identity. However, the case for deep networks where depth is larger than two is fundamentally different from the two-layer case, where one can obtain a closed form for \(F\). To the best of our knowledge, it is open whether one obtain a closed form for the induced-regularizer for the trace of Hessian when \(L>2\). Nonetheless, in Section 5.1, we show that under RIP, we can still approximate it with the nuclear norm.

## 5 Results for Measurements with Restricted Isometry Property (RIP)

In this section, we present our main results for the generalization benefit of flatness regularization in deep linear networks. We structure the analysis as follows:

1. In Section 5.1, we first recap some preliminaries on the RIP property.
2. In Section 5.2, we prove that the induced regularizer by trace of Hessian is approximately the power of nuclear norm for \((1,)\)-RIP measurements (Theorem 1).
3. In Section 5.3, we prove that the minimum trace of Hessian interpolating solution with \((2,)\)-RIP measurements can recover the ground truth \(M^{*}\) up to error \(\|M^{*}\|_{*}^{2}\). For \(\{A_{i}\}_{i=1}^{n}\) sampled from Gaussian distributions, we know \(=O(+d_{L}}{n}})\).
4. In Section 5.4, we prove a generalization bound with faster rate of \(+d_{L}}{n}\|M^{*}\|_{*}^{2}\) using local Rademacher complexity based techniques from Srebro et al. .

Next, we discuss important distributions of measurements for which the RIP property holds.

### Preliminaries for RIP

**Definition 3** (Restricted Isometry Property (RIP)).: _A family of matrices \(\{A_{i}\}_{i=1}^{n}\) satisfies the \((r,)\)-RIP iff for any matrix \(X\) with the same dimension and rank at most \(r\):_

\[(1-)\|X\|_{F}^{2}_{i=1}^{n} A_{i},X ^{2}(1+)\|X\|_{F}^{2}.\] (9)

Next, we give two examples of distributions where \((r(d_{0}+d_{L}))\) samples guarantee \((r,O(1))\)-RIP. The proofs follow from Theorem 2.3 in .

**Example 1**.: _Suppose for every \(i\{1,,n\}\), each entry in the matrix \(A_{i}\) is an independent standard Gaussian random variable, i.e., \(A_{i}}{{}}_{d_{L} d_{0}}\). For every constant \((0,1)\), if \(n(r(d_{0}+d_{L}))\), then with probability \(1-e^{(n)}\), \(\{A_{i}\}_{i=1}^{n}\) satisfies \((r,)\)-RIP._

**Example 2**.: _If each entry of \(A_{i}\) is from a symmetric Bernoulli random variable with variance \(1\), i.e. for all \(i,k,\), entry \([A_{i}]_{k,}\) is either equal to \(1\) or \(-1\) with equal probabilities, then for any \(r\) and \(\), \((r,)\)-RIP holds with same probability as in Example 1 if the same condition there is satisfied._

### Induced Regularizer of Trace of Hessian is Approximately Nuclear Norm

This section focuses primarily on the proof of Theorem 2. Our proof consists of two steps: (1) we show that the trace of Hessian of training loss at the minimizer \(\) is multiplicatively \(O()\)-close to the regularizer \(R()\) defined below (Lemma 3) and (2) we show that the induced regularizer of \(R\), \(F^{}(M)\), is proportional to \(\|M\|_{*}^{2(L-1)/L}\) (Lemma 4).

\[R()\|W_{L} W_{2}\|_{F}^{2}d_{0}+_{j=2} ^{L-1}\|W_{L} W_{j+1}\|_{F}^{2}\|W_{j-1} W_{1}\|_{F}^{2}+\|W_{L-1}  W_{1}\|_{F}^{2}d_{L}.\] (10)

**Lemma 3**.: _Suppose the linear measurement \(\{A_{i}\}_{i=1}^{n}\) satisfy \((1,)\)-RIP. Then, for any \(\) such that \(()=0\), it holds that_

\[(1-)R()(^{2}L)()(1+ )R().\]

Since \((^{2})()\) closely approximates \(R()\), we can study \(R\) instead of \([^{2}]\) to understand the implicit bias up to a multiplicative factor \((1+)\). In particular, we want to solve the induced regularizer of \(R()\) on the space of end-to-end matrices, \(F^{}(M)\):

\[F^{}(M)_{:W_{L} W_{1}=M}R().\] (11)

Surprisingly, we can solve this problem in closed form.

**Lemma 4**.: _For any \(M^{d_{L} d_{0}}\), it holds that_

\[F^{}(M)_{:\;W_{L} W_{1}=M}R()=L(d_{0}d _{L})^{1/L}\|M\|_{*}^{2(L-1)/L}.\] (12)

Proof of Lemma 4.: Applying the \(L\)-version of the AM-GM to Equation (10):

\[(R()/L)^{L} d_{0}\|W_{L} W_{2}\|_{F}^{2}\|W_{1}\|_{F}^{2}\|W_{L}  W_{3}\|_{F}^{2}\|W_{L-1} W_{1}\|_{F}^{2}d_{L}.\] (13) \[= d_{0}d_{L}_{j=1}^{L-1}(\|W_{L} W_{j+1}\|_{F}^{2} \|W_{j} W_{1}\|_{F}^{2})\]

Now using Lemma 1, we have for every \(1 j L-1\):

\[\|W_{L} W_{j+1}\|_{F}^{2}\|W_{j} W_{1}\|_{F}^{2} \|W_{L} W_{1}\|_{*}^{2}=\|M\|_{*}^{2}.\] (14)

Multiplying Equation (14) for all \(1 j L-1\) and combining with Equation (13) implies

\[_{\{W\;W_{L} W_{1}=M\}}R() L(d_{0}d_{L})^{1/L}\|M\|_{*} ^{2(L-1)/L}.\] (15)Now we show that equality can indeed be attained. To construct an example in which the equality happens, consider the singular value decomposition of \(M\): \(M=U V^{T}\), where \(\) is a square matrix with dimension \((M)\).

For \(1 i L\), we pick \(Q_{i}^{d_{i}(M)}\) to be any matrix with orthonormal columns. Note that \((M)\) is not larger than \(d_{i}\) for all \(1 i L\), hence such orthonormal matrices \(Q_{i}\) exist. Then we define the following with \(,^{}>0\) being constants to be determined:

\[W_{L}=^{}^{-(L-2)/2}U^{1/2}{Q_{L-1}}^{ T}^{d_{L} d_{L-1}},\] \[W_{i}=}{Q_{i-1}}^{T}^{d_{i} d_{i- 1}}, 2 i L-1,\] \[W_{1}={^{}}^{-1}^{-(L-2)/2}{Q_{1}}^{1/ 2}V^{T}^{d_{1} d_{0}}.\]

Note that \(\) is a square matrix with dimension \((M)\). First of all, note that the defined matrices satisfy

\[W_{L}W_{L-1} W_{1}=^{L-2}^{-(L-2)}U^{1/2}^{1/ 2}V^{T}=M.\]

To gain some intuition, we check that the equality case for all the inequalities that we applied above. We set the value of \(\) in a way that these equality cases can hold simultaneously. Note that for the matrix holder inequality that we applied in Equation (14):

\[\|W_{L} W_{j+1}\|_{F}^{2}\|W_{j} W_{1}\|_{F}^{2}=\|W_{L} W_{ 1}\|_{*}^{2}=\|^{1/2}\|_{F}^{2},\]

independent of the choice of \(\). It remains to check the equality case for the AM-GM inequality that we applied in Equation (13). We have for all \(2 j L-1\):

\[\|W_{L} W_{j+1}\|_{F}\|W_{j-1} W_{1}\|_{F}\] \[=^{j-2}^{-(L-2)/2}^{L-j-1}^{-(L-2)/2}\|U ^{1/2}\|_{F}\|^{1/2}V^{T}\|_{F}=^{-1}\|^{1/2}\|_{F} ^{2},\] (16)

Hence, equality happens for all of them. Moreover, for cases \(j=1\) and \(j=L\), we have

\[d_{0}\|W_{L} W_{2}\|=\|^{1/2}\|_{F}d_{0}^{ }^{L-2}^{-(L-2)/2}=\|^{1/2}\|_{F}d_{0}^{ }^{(L-2)/2}.\] (17) \[d_{L}\|W_{L-1} W_{1}\|=\|^{1/2}\|_{F}{d_{L}}{ ^{}}^{-1}^{L-2}^{-(L-2)/2}=\|^{1/2}\|_{F}{d_{L}}{ ^{}}^{-1}^{(L-2)/2}.\] (18)

Thus it suffices to set \(^{}=(}{d_{0}})^{1/2}\) and \(=(\|_{F}}{d_{L}}})^{2/L}=(}{d_{0}d_{L}})^{1/L}\) so that the left-hand sides of (16), (17), and (18) are equal, which implies that the lower bound in Equation (15) is actually an equality. The proof is complete. 

Now we can prove Theorem 1 as an implication of Lemma 4.

Proof of Theorem 1.: The first claim is a corollary of Lemma 3. We note that

\[F(M)=_{W_{L} W_{1}=M}[^{2}] (M)(1+)_{W_{L} W_{1}=M}R()=(1+)F^{}(M)\] \[F(M)=_{W_{L} W_{1}=M}[^{2}] (M)(1-)_{W_{L} W_{1}=M}R()=(1-)F^{}(M).\]

For the second claim, pick \(}\) that minimizes \(R(})\) over all \(\)'s that satisfy the linear measurements, thus we have that

\[R(})=L(d_{0}d_{L})^{1/L}\|E(})\|_{*}^{2(L-1) /L}=L(d_{0}d_{L})^{1/L}_{^{}(M)=0}\|M\|_{*}^{2(L-1)/L}.\] (19)

Now from the definition of \(E(^{*})\),

\[(^{2}L)(^{*})(^{2}L)(})(1+)R(}),\] (20)

where the last inequality follows from the definition of \(W\). On the other hand

\[(^{2}L)(^{*})(1-)R(})( 1-)L(d_{0}d_{L})^{1/L}\|E(^{*})\|_{*}^{2(L-1)/L}.\] (21)

Combining (19), (20) and (21),

\[\|E(^{*})\|_{*}()^{} _{^{}(M)=0}\|M\|_{*}.\]

The proof is completed by noting that \( 1\) for all \(L 2\)Thus combining Example 1 and Theorem 1 with \(=1/2\), we have the following corollary.

**Corollary 1**.: _Let \(\{A_{i}\}_{i=1}^{n}\) be sampled independently from Gaussian distribution \(_{d_{L} d_{0}}\) where \(n((d_{0}+d_{L}))\), with probability at least \(1-((n))\), we have_

\[\|E(^{*})\|_{*} 3_{^{}(M)=0}\|M\|_{*} 3\|E (^{*})\|_{*}.\]

### Recovering the Ground truth

In this section, we prove Theorem 2. The idea is to show that under RIP, the empirical loss \(()\) is a good approximation for the Frobenius distance of \(E()\) to the ground truth \(M^{*}\). To this end, we first introduce a very useful Lemma 5 below, whose proof is deferred to Appendix E.

**Lemma 5**.: _Suppose the measurements \(\{A_{i}\}_{i=1}^{n}\) satisfy the \((2,)\)-RIP condition. Then for any matrix \(M^{d_{L} d_{0}}\), we have that_

\[|_{i=1}^{n} A_{i},M^{2} -\|M\|_{F}^{2}| 2\|M\|_{*}^{2}.\]

We note that if \(\{A_{i}\}_{i=1}^{n}\) are i.i.d. random matrices with each coordinate being independent, zero mean, and unit variance (like standard Gaussian distribution), then \(\|W-M^{*}\|_{F}^{2}\) is the population squared loss corresponding to \(W\). Thus, Theorem 2 implies a generalization bound for this case. Now we are ready to prove Theorem 2.

Proof of Theorem 2.: Note that from Theorem 1,

\[\|E(^{*})\|_{*}_{^{}( M)=0}\|M\|_{*}\|M^{*}\|_{*},\]

which implies the following by triangle inequality,

\[\|E(^{*})-M^{*}\|_{*}\|(^{*})\|_{*}+\|M^{*}\|_{*} \|M^{*}\|_{*}.\] (22)

Combining (22) with Lemma 5 (with \(M=E(^{*})-M^{*}\)):

\[|_{i=1}^{n} A_{i},E(^{*})-M^{* }^{2}-\|E(^{*})-M^{*}\|_{F}^{2}|}\|M^{*}\|_{*}^{2}.\]

Since \(W^{*}\) satisfies the linear constraints \((A_{i}E(^{*}))=b_{i}\), \(_{i=1}^{n} A_{i},E(^{*})-M^{*} ^{2}=_{i=1}^{n}( A_{i},E(^{*})-b_{i})^{2}=0\), which completes the proof. 

### Generalization Bound

In this section, we prove the generalization bound in Theorem 3, which yields a faster rate of \(O(+d_{L}}{n}\|M^{*}\|_{*}^{2})\) compared to \(O(+d_{L}}{n}}\|M^{*}\|_{*}^{2})\) in Theorem 2. The intuition for this is as follows: By Corollary 1, we know that with very high probability, the learned solution has a bounded nuclear norm for its end-to-end matrix, no larger than \(3\|M^{*}\|_{2}\), where \(M^{*}\) is the ground truth. The key mathematical tool is Theorem 6, which provides an upper bound on the population error of the learned interpolation solution that is proportional to the square of the Rademacher complexity of the function class \(_{3\|M^{*}\|_{*}}=\{h_{M}\ |\ \|M\|_{*} 3\|M^{*}\|_{*}\}\).

**Theorem 6** (Theorem 1, Srebro et al. ).: _Let \(\) be a class of real-valued functions and \(:\) be a differentiable non-negative loss function satisfying that (1) for any fixed \(y\), the partial derivative \((,y)\) with respect to its first coordinate is \(H\)-Lipschitz and (2) \(|_{x,y}(x,y)| B\), where \(H,B\) are some positive constants. Then for any \(p>0\), we have that with probability at least \(1-p\) over a random sample of size \(n\), for any \(h\) with zero training loss,_

\[}(h) O(H^{3}n_{n}^{2}()+ ).\] (23)

One technical difficulty is that Theorem 6 only works for bounded loss functions, but the \(_{2}\) loss on Gaussian data is unbounded. To circumvent this issue, we construct a smoothly truncated variant of \(_{2}\) loss (41) and apply Theorem 6 on that. Finally, we show that with a carefully chosen threshold, this truncation happens very rarely and, thus, does not change the population loss significantly. The proof can be found in Appendix E.

## 6 Result for the Single Measurement Case

Quite surprisingly, even though in the general case we cannot compute the closed-form of the induced regularizer in (12), we can find its minimum as a quasinorm function of the \(E()\) which only depends on the singular values of \(E()\). This yields the following result for multiple layers \(L\) (possibly \(L>2\)) with a single measurement.

**Theorem 7**.: _Suppose there is only a single measurement matrix \(A\), i.e., \(n=1\). For any \(M^{d_{L} d_{0}}\), the following holds:_

\[F(M)=_{W_{L} W_{1}=M}[^{2}]()=L \|(A^{T}M)^{L-1}A^{T}\|_{S_{2/L}}^{2/L}.\] (24)

To better illustrate the behavior of this induced regularizer, consider the case where the measurement matrix \(A\) is identity and \(M\) is symmetric with eigenvalues \(\{_{i}\}_{i=1}^{d}\). Then, it is easy to see that \(F(M)\) in (24) is equal to \(F(M)=_{i}_{i}^{2(L-1)/L}\). Interestingly, we see that the value of \(F(M)\) converges to the Frobenius norm of \(M\) and not the nuclear norm as \(L\) becomes large, which behaves quite differently (e.g. in the context of sparse recovery). This means that beyond RIP, the induced regularizer can behave very differently, and perhaps the success of training deep networks with SGD is closely tied to the properties of the dataset.

## 7 Conclusion and Future Directions

In this paper, we study the inductive bias of the minimum trace of the Hessian solutions for learning deep linear networks from linear measurements. We show that trace of Hessian regularization of loss on the end-to-end matrix of deep linear networks roughly corresponds to nuclear norm regularization under restricted isometry property (RIP) and yields a way to recover the ground truth matrix. Furthermore, leveraging this connection with the nuclear norm regularization, we show a generalization bound which yields a faster rate than Frobenius (or \(_{2}\) norm) regularizer for Gaussian distributions. Finally, going beyond RIP conditions, we obtain closed-form solutions for the case of a single measurement. Several avenues for future work remain open, e.g., more general characterization of trace of Hessian regularization beyond RIP settings and understanding it for neural networks with non-linear activations.