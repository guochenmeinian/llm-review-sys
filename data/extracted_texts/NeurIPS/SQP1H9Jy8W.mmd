# Leveraging Locality and Robustness to Achieve

Massively Scalable Gaussian Process Regression

Robert Allison

Department of Mathematics

Bristol University

marfa@bristol.ac.uk Anthony Stephenson

Department of Mathematics

Bristol University

**Samuel F**

Alan Turing Institute

**Edward Pyzer-Knapp**

IBM Research

###### Abstract

The accurate predictions and principled uncertainty measures provided by GP regression incur \(n^{3}\) cost which is prohibitive for modern-day large-scale applications. This has motivated extensive work on computationally efficient approximations. We introduce a new perspective by exploring robustness properties and limiting behaviour of GP nearest neighbour (GPnn) prediction. We demonstrate through theory and simulation that as the data-size \(n\) increases, accuracy of estimated parameters and GP model assumptions become increasingly irrelevant to GPnn predictive accuracy. Consequently, it is sufficient to spend small amounts of work on parameter estimation in order to achieve high MSE accuracy, even in the presence of gross misspecification. In contrast, as \(n\), uncertainty calibration and NLL are shown to remain sensitive to just one parameter, the additive noise-variance; but we show that this source of inaccuracy can be corrected for, thereby achieving both well-calibrated uncertainty measures and accurate predictions at remarkably low computational cost. We exhibit a very simple GPnn regression algorithm with stand-out performance compared to other state-of-the-art GP approximations as measured on large UCI datasets. It operates at a small fraction of those other methods' training costs, for example on a basic laptop taking about 30 seconds to train on a dataset of size \(n=1.6 10^{6}\).

## 1 Introduction

We first briefly review the computational cost of exact GP regression and the motivation for this paper: Given \(n\) training samples \(X,\), where \(X^{n d}\) has feature vector \(_{i}^{d}\) in its \(i\)'th row and \(^{n}\), exact GP regression  makes use of an \(n n\) gram matrix \(K=K_{X,}\) constructed from a pre-specified positive definite covariance function \(c(,):^{d}^{d}_{+}\) together with length-scale, additive-noise variance and kernel-scale "hyperparameters" \(=(l,_{}^{2},_{f}^{2})\). In the training phase estimates of the hyperparameters, \(}=(,_{}^{2},_{f}^{2})\), are obtained by minimising the loss function

\[()=- p(|X,)=\{^{T }K_{}^{-1}+|K_{}|+n(2)\}.\] (1)

Then for subsequent predictions the predictive distribution at a point \(^{*}^{d}\) is defined by

\[y^{*} X, (^{*},^{*\,2})\] (2) \[^{*} =^{*\;\;T}K^{-1}\] (3) \[^{*\,2} =_{f}^{2}-^{*\;\;T}K^{-1}^{*}\,+_{}^{2}\] (4)where \(K=K_{}}\) with components \([K]_{ij}=k_{}}(_{i},_{j})\); the vector \(^{*}\) has components \(k^{*}_{i}=k_{}}(_{i},^{*})\), and \(k_{}(,^{})=_{f}^{2}c(/l,^{ }/l)+_{,^{}}_{}^{2}\) with a "normalised" covariance function \(c(,)\) such that \(c(,)=1\). The derivation of these steps is based on the assumption that the underlying random field is Gaussian, as is the additive noise with variance \(_{}^{2}\).

The single cost-\(n^{3}\) step of inverting \(K\) is needed repeatedly to compute the loss. Sophisticated implementations reduce this toward \(n^{2}\) (), but even that cost is generally impractical for \(n>10^{6}\). For a survey of numerous GP approximations and their reduced costs see .

Machine learning methods must tackle massive data problems to handle many modern day applications. Revolutionary developments in neural network methodologies achieve this, but Bayesian predictive methodologies, in particular GP regression with its major advantages of robustness and uncertainty measures, are somewhat behind the curve. This motivates development of fast implementations retaining the accuracy and well-principled uncertainty of exact GPs.

## 2 Background and Paper Outline

A feature common to all mainstream GP approximations is that training and prediction processes make joint use of the same underlying mathematical constructions. In the "subset-of-data" method the _same_ subset of data is used both for parameter estimation and prediction. Similarly, in the various Bayesian committee methods () hyperparameters are estimated using a collection of subsets of data and then the _same_ subsets are used in combination to make predictions. In the variational ([14; 31]) and other inducing point methods parameters are estimated using a low rank approximation to the kernel gram matrix and then the _same_ low-rank matrix approximation is used to make predictions. Despite being almost universally adopted there is no obvious reason why constraining algorithms to use the same constructions for estimation and prediction will help rather than hinder the end goal of high performance at low cost. Whilst there are some passing mentions of decoupling prediction and estimation in the literature - e.g. [28; 1; 3] - it has not been adopted as a mainstream approach.

Our first observation is that allowing parameter-estimation and prediction processes to become decoupled may provide the flexibility to greatly improve cost-accuracy trade-off. As shown in Figure 1, GP approximations first obtain a point estimate of the kernel hyperparameters \(}\) from training data and then feed \(}\) into a predictive process. Our end-goal is only to obtain accurate and well-calibrated _predictive distributions_ of \(y^{*}\) at each target point \(^{*}\); obtaining accurate parameter estimates is _not_ a goal in itself. It follows that the computational budget devoted to parameter estimation need only be sufficient to provide parameters capable of delivering accurate and well-calibrated predictions.

This need not mean that \(}\) is an accurate estimate of the parameters which leads us to the second observation and main theoretical component of this paper: In section 5, theory and simulations reveal that under widely applicable circumstances, as \(n\) increases the mean squared error (MSE) predictive accuracy obtained from GP nearest neighbour prediction becomes increasingly insensitive to model misspecification, i.e. insensitive to the wrong choice of covariance function, to the choice of \(\), \(_{f}^{2}\) and \(_{}^{2}\) and even insensitive to departures from Gaussian model assumptions made for the underlying stochastic process and additive noise. Similarly the negative log likelihood (NLL) predictive-accuracy becomes insensitive to all of those factors _apart from_ the variance of the additive noise \(_{}^{2}\). In 6.1

Figure 1: Flowchart of the GP regression procedure. The dashed box indicates the usual approach of combining the parameter estimation and prediction tasks under one strategy.

we describe a simple calibration step that corrects for the latter inaccuracy thereby achieving near optimal limiting NLL values in addition to well-calibrated uncertainty measures whilst leaving the well-behaved MSE values completely unaltered. We apply these overall observations to construct a highly efficient, accurate and well calibrated regression algorithm in section 6.

**Our key contributions:** Demonstration of GPnn robustness against model and parameter misspecification through theory and simulation (5); derivation of explicit formulae for the limiting MSE, NLL and calibration performance of GPnn as \(n\) (5.1); translation of this theory into a new GP approximation framework with stand-out performance relative to other state-of-the-art GP approximations (6,7.1); a simple generic method for re-calibrating uncertainty measures in GP regression with immediate applications to improving calibration of other GP approximations such as SVGP (6.1); achievement of massive scalability for GPs, for example a \(100\) speed-up over state-of-the-art methods on a \(1.6 10^{6}\) training set whilst also improving upon their performance (7.1); demonstrating that provably best possible MSE, NLL and calibration performance can be closely approached on data that is grossly misspecified relative to GP model assumptions (7.2).

## 3 Performance Measures, Weak and Strong Calibration

Along with many other GP publications we use mean squared error MSE (or its square root RMSE) and negative log likelihood (NLL), both computed from held-aside test data, to assess predictive performance. These are simply the mean values of \(e_{i}^{*}=(y_{i}^{*}-_{i}^{*})^{2}\) and \(l_{i}^{*}=0.5(^{*}}^{2}+(y_{i}^{*}-_{i}^{*})^{2}/{ _{i}^{*}}^{2}+ 2)\) respectively. However, we find those measures alone inadequate for determining how _well calibrated_ a predictive distribution is. We define "weakly calibrated" prediction to mean that \(_{y^{*}}_{X,y}(y^{*}-^{*})^{2}/{^{*}}^{2}} =1\) and accordingly use "calibration" to be a measure of how well the average value of \(z_{i}^{*}=(y_{i}^{*}-_{i}^{*})^{2}/{_{i}^{*}}^{2}\) over test-data agrees with 1. This choice of metric (also made use of in ) can be motivated as follows: For a well-calibrated GP the expected squared deviation of \(y_{i}^{*}\) from \(_{i}^{*}\) should match the corresponding predictive variance \({_{i}^{*}}^{2}\) (2), i.e. \(_{y^{*}}_{X,y}z^{*}}=1\). Hence, observing an average of \(z^{*}\) values close to 1 is consistent with a _necessary_ condition for effective calibration. In practice, we find that GP approximation methods can fall well short of this condition (e.g. see the LHS plot in Figure 4, and Table 3 for those results in tabular form) whilst this is not evident from their MSE and NLL values alone. A better measure of calibration ("strong-calibration") would have been to see how well percentiles of the predictive distributions agree with those observed in test data, e.g. see , but we defer such a refinement to future work.

## 4 Prediction Method and Sources of Misspecification

### GP Nearest Neighbour Prediction

We now describe what we mean by "GP nearest neighbour (GPnn) prediction". Assume that we are given parameters \(}=(,_{}^{2},_{f}^{2})\) obtained from the parameter estimation phase of Figure 1. Then to compute the estimated pointwise-distribution of \(y^{*}\) at \(^{*}\) indicated in Figure 1 we find the \(m\) nearest training-set neighbours \(N=N(^{*})\) to \(^{*}\) and apply exactly the same GP prediction formulae as in (2), (3) and (4) but with \(X^{n d}\) replaced by \(N^{m d}\) and \(^{n}\) replaced by \(_{N}^{m}\). Note that in this setup conditioning on \(N(^{*})\) is equivalent to conditioning on the full input matrix \(X\). We obtain:

\[y^{*} N(^{*}),_{N} (_{N}^{*},_{N}^{*2})\] (5) \[_{N}^{*} =}_{N}^{*T}_{N}^{-1}_{N}\] (6) \[{_{N}^{*}}^{2} =_{f}^{2}-}_{N}^{*T}_{N}^{-1}}_{N}^{*}+_{}^{2}\] (7)

where we have used hatted notation in a generic manner to cover all the potential sources of misspecification (4.2) that might arise when we carry out these predictions. The \(_{N}^{*},{_{N}^{*}}^{2}\) parameters are substituted for \(^{*},{^{*}}^{2}\) when computing the performance measures described in section 3.

**Note:** In this paper we use Euclidean distance for nearest neighbour assignment but more generally could employ a metric defined by the covariance function - see A. For the covariance functions used in this paper these metrics are "equivalent" because one is a monotonic function of the other.

In our algorithmic implementations we replace an exact \(m\) nearest neighbour algorithm with a much more efficient _approximate_ nearest neighbour algorithm as discussed in section 6. However for the purpose of the theoretical analysis of robustness in section 5 this distinction can be ignored.

### Sources of Misspecification

For the remainder of the paper we extend our theory and notation to encompass several (possibly simultaneous) sources of misspecification: standard GP theory assumes that data comes from a latent Gaussian random field \([_{f}^{2}c(./l,./l)]\) specified by covariance function \(c(,)\) and parameters \(l,_{f}^{2}\). The construction of the matrix \(K_{}\) in section 1 assumes data to have arisen from this \(\) with i.i.d \((0,_{}^{2})\) additive noise. Henceforth, we limit covariance functions \(c(,^{})\) to be stationary, i.e. to vary only with \((-^{})\). The forms of (possibly simultaneous) misspecifications to be accounted for in the theoretical treatment of 5.1 are: (a) parameter \(_{}^{2}\) wrongly specified as \(_{}^{2}\), (b) (normalised) covariance function \(c(,)\) wrongly specified as \((,)\), (c) parameters \(l,_{f}^{2}\) misspecified as \(,_{f}^{2}\) (relevant only if \(c(,)\)_not_ misspecified), (d) true additive noise is _not_ Gaussian and (e) the data is generated by a non-Gaussian weakly stationary random field \(\) rather than a \(\).

## 5 GP nearest neighbour Limits and Robustness

In this section we investigate the behaviour of MSE, NLL and calibration for GPnn prediction as \(n\), showing how all of these performance measures become increasingly less sensitive to hyperparameter accuracy, kernel choice and the above departures from the GP model assumptions.

### Theory

**Assumptions:** The true generative model from which the data arises is \(y_{i}=f(_{i})+_{i}\) with \(_{i}}}{{}}P_{}\), \(f()[_{f}^{2}c(./l,./l))]\) and \(y_{i} f(_{i}) P_{}\) where the variance of the distribution \(P_{}\) is \(_{}^{2}\). Neither the \(\) nor the additive noise distribution \(P_{}\) need be Gaussian. The training \(\) values are i.i.d. The MSE, NLL and calibration statistics on the test set are derived according to the nearest neighbour GP prediction process (4.1) and subject to any/all forms of misspecification in 4.2. Additionally, we assume that if and only if the \(m^{th}\) nearest neighbour converges to the test point under \(c\), then it also converges under \(\) (A:Definition 10).

**Result:** Given a size-\(n\) training set \(X\) and test point \(^{*}\) in the support (A:Definition 9) of the measure of \(\), let \(f_{n}^{}(})=_{,^{*}}\) {\(e_{N}^{*}\)}, \(f_{n}^{}(})=_{,^{*}}\) {\(l_{N}^{*}\)}, \(f^{}(})=_{,^{*}}\) {\(z_{N}^{*}\)}; where expectations are w.r.t. the true generative process for \(\) and \(y^{*}\) and the performance measures \(e_{N}^{*},l_{N}^{*},z_{N}^{*}\) (section 3) are for the nearest neighbour prediction process. Note that these are the expected (rather than mean) values of the performance measures described in section 3 and the dependence on \(n\) is implicit in the construction of the nearest neighbour sets \(N=N_{m}(^{*})\) used for prediction. Then we have:

**Theorem 1** (GPnn limits).: _As \(n\), \(f_{n}^{},f_{n}^{},f_{n}^{} f_{}^{ {MSE}},f_{}^{},f_{}^{}\) a.e w.r.t. the (i.i.d.) measure on \( X\) and \(^{*}\), and pointwise as functions of \(}\) where:_

1. \(f_{}^{}(})=_{}^{2}(1+m^{-1}) m^{-2}\)__
2. \(f_{}^{}(})=^{2}}{ _{}^{2}}m^{-2}\)__
3. \(f_{}^{}(})=\{(_{}^{2}(1+m^{-1}))+^{2}}{_{}^{ 2}}+ 2\}m^{-2}\)_._

_Setting \(}=\) (and in particular \(_{}^{2}=_{}^{2}\)) in the above provides matched-parameter limiting results._

Proof sketch.: It is quite straightfoward to derive expressions for each of the expectations \(f_{n}^{},f_{n}^{},f_{n}^{}\) since these only depend on the known marginal covariance matrices of the (misspecified) \(\). We then use results concerning asymptotic convergence of Euclidean nearest neighbours, in combination with some standard linear algebra results and continuity properties, to obtain the stated limits. Note that in the expression for \(f_{}^{}(})\) above the right hand side must always exceed \(_{}^{2}\) since this is an absolute lower bound on MSE performance; likewise \(f_{}^{}(})\) is constrained to be non-negative. See _full proof (A)._. 

**Interpretation:** The MSE results of Theorem 1 show that to within a small factor (e.g. \(m^{-1}=0.0025\) when \(m=400\) as for all reported runs of our algorithm) the _best possible_ MSE will be achieved in the limit. The NLL results also tell us (by setting \(_{}^{2}=_{}^{2}\)) what the best possible limiting NLL value is, but only according to the _possibly misspecified_ Gaussian model. The corrupting influence of an incorrect value of \(_{}^{2}\) on the limiting NLL value is clearly evident from the expression for \(f_{}^{}\) and the picture is similar for calibration.

**Remark 2**.: _Theorem 1 shows that isotropic (e.g. RBF and Matern) kernels converge to the best possible MSE as \(n\) even on data generated with independent lengthscales on each \(\) coordinate._

Note that 1 refers to pointwise convergence whereas we believe uniform convergence results should also be obtainable, e.g. perhaps of the form (or similar):

**Conjecture 3**.: \(_{X,^{*}}\{f_{}^{}(})\} f_{}^{}=_{}^{2}(1+m^{-1}) \!(m^{-2})\) _uniformly as a function of \(}\) as \(n\)._

This particular conjecture would hold, for example, if the l.h.s. were shown to be a continuous function of \(}\) reducing monotonically and pointwise to the limit with \(n\) (by Dini's theorem). We also have initial results on rate of convergence in Theorem 1 which we defer to a later publication once more fully extended.

### Simulation of Limits and Robustness at Scale

At first sight it seems infeasible to demonstrate the above robustness and limit properties empirically on GP data-sets of size \(10^{6}\) or above. One major obstacle being the generation of GP synthetic datasets at this size which is computationally prohibitive even allowing for the speedups described in . Fortunately we can avoid the need for large-scale data-generation, in addition to achieving other major efficiencies, by adopting the approach described in Algorithm 1.

The simulation algorithm gains its efficiency by exploiting the locality of the GPnn prediction process at \(^{*}\) whereby the predictive distribution only makes use of a size-\((m+1)\) marginal distribution of the full distribution of \((,y^{*})\) over \(^{n+1}\). (By the definition of a Gaussian process this marginal is a (low dimensional) multivariate Gaussian distribution from which samples can cheaply be generated). The following lemma is proved in Appendix B:

Figure 2: Behaviour of performance metrics as functions of kernel hyperparameters for increasing training set sizes \(n\). The black dashed line denotes the true parameter value; the red dashed line shows the limiting behaviour as \(n\) and the green dashed line shows the limiting behaviour when the hyperparameters are correct (the red and green dashed lines coincide for MSE). True \(l\) is shown in the title; additionally \(_{}^{2}=0.1\), \(_{f}^{2}=0.9\), \(d=20\). When not varied, the assumed parameters are \(_{}^{2}=0.2\), \(_{f}^{2}=0.8\), \(=l\). Finally we generate the input data from the measure \(P_{}=(0,I_{d})\).

**Lemma 4** (Algorithm 1 validity).: _The MSE, NLL and calibration estimates returned by Algorithm 1 are equally valid to those that would be obtained by applying the full GPnn predictive process (exactly as described in subsection 4.1) to synthetic data sets of size \(n\)._

Figure 2 shows how the observed performance metrics approach the limiting behaviour as \(n\) increases. In particular, from the RHS plot we see that as \(n\) increases, MSE becomes increasingly insensitive to departure of \(\) from the true value \(l=1\). This is a consequence of (what appears to be uniform) convergence of MSE toward constant value \(_{}^{2}=0.1\) (the best achievable MSE) as predicted by Theorem 1 (i). In practical terms: once a practitioner selects a particular kernel family, the accuracy of the hyperparameter \(\) becomes less and less critical to MSE predictive accuracy with increasing \(n\), so that expenditure on estimating it accurately provides diminishing returns.

The interpretation of the leftmost two plots is similar albeit somewhat more involved: The dotted red lines show the asymptotic dependence on the misspecification of the noise-variance as predicted by Theorem 1 (ii) and (iii), i.e. plots of \(y=_{}^{2}}\) and \(y=\{_{}^{2}+_{}^{2}}+  2\}\) where \(0.1=_{}^{2}\) is the true noise value used to generate the synthetic GP data. We again use evidence of uniform convergence toward this limiting behaviour with increasing \(n\). The green horizontal dotted lines show the limiting values of NLL and calibration (\(y=\{_{}^{2}+1+ 2\}\) and \(y=1\) respectively) that can be achieved if the incorrect \(_{}^{2}\) value is replaced by the correct value \(_{}^{2}\). This underlines the importance of estimating this particular parameter more accurately in order to obtain improved NLL and uncertainty calibration for large \(n\). Further plots from Algorithm 1, showing dependence of each metric on all of the parameters, are given in Figure 6.

## 6 A Highly Scalable GP Nearest Neighbour Regression Algorithm

### Parameter Estimation

**Parameter Estimation Phase 1** The first step of parameter estimation (Figure 1) involves randomly selecting a small subset \(E\) of the training data to obtain a first-pass estimate \(}=(,_{}^{2},_{f}^{2})\). Small subsets yield sub-optimal \(}\) values, yet as shown in 7.1, these are capable of yielding strong MSE performance due to the robustness properties of section 5. We use the method in section 3.1 of  to estimate parameters from \(E\), randomly partitioning \(E\) into \(w\) size-\(s\) subsets (\(ws=e\)) and using a block diagonal approximation (with \(w\) blocks of size \(s s\) ) to the full \(e e\) gram matrix. For Table 1 we set \(e=3000,s=300,m=10\). For strong computational efficiency we set \(e=|E|\) to a small _constant_ value no matter the size of \(n\). Thus as \(n\) grows, an increasingly small portion of the data is used for this phase of parameter estimation and the associated cost does not increase with \(n\). Note that other choices of cheap parameter estimation could be substituted here.

**Parameter Estimation Phase 2 (calibration)** As shown in section 5, NLL and calibration performance derived from \(}=(,_{}^{2},_{f}^{2})\) remain very sensitive to inaccuracies in \(_{}^{2}\). An additional "calibration step" is used to refine those parameters: We randomly select a size \(c\) calibration set \(C\) (which is _otherwise unused_) from the training data and proceed according to Algorithm 2.

**Input:** A size \(c\) subset \(C\) of \((^{*},y^{*})\) pairs from the training data, parameters \(}=(,_{}^{2},_{f}^{2})\).

1. For each \((^{*}_{i},y^{*}_{i}) C\) use the efficient GPnn predictive algorithm of 6.2, with covariance function \((.,.)\) and parameters \(}=(,_{}^{2},_{f}^{2})\), to obtain an estimate of the mean and variance, \(_{i}^{*},{_{i}^{*}}^{2}\) of the predictive distribution of \(y^{*}_{i}\) at \(^{*}_{i}\).
2. Compute \(=_{i=1}^{c}_{i}-^{*}_{i})^{2}}{{_{i} ^{*}}^{2}}\).

**Output:** Calibrated parameters \(}^{}=(,_{}^{2}, _{f}^{2})\).

**Algorithm 2** Calibration of Predictive Distribution

Note that this process not only adjusts the noise variance estimate \(_{}^{2}\) but also the kernel scale parameter \(_{f}^{2}\). In so doing it simultaneously calibrates the predictive distribution and improves NLL performance whilst leaving unchanged the MSE performance obtained from the original parameter estimates \(}=(,_{}^{2},_{f}^{2})\). The lemma below is straightforward to prove (Appendix C):

**Lemma 5** (Calibration).: _The parameters \(}^{}\) output from Algorithm 2 produce GPnn predictions that (a) achieve perfect (weak) calibration on \(C\), (b) minimise NLL on \(C\) over all choices of \(\) and (c) produce the same MSE as \(}\) does on any choice of test set._

**Remark 6**.: _Algorithm 2 can be applied to other GP methods, such as SVGP, to improve calibration._

Table 1 uses \(c=1000\); a simple refinement would be to select \(c\) automatically (e.g. using a bootstrap) with optional manual override. Where accurate uncertainty calibration is paramount practitioners could devote much larger CPU resources to this phase (which is also easily distributed); when no uncertainty measures are to be used this calibration step can be bypassed altogether.

### Efficient Nearest Neighbour Prediction

In order to implement GPnn prediction described in 4.1 we use the scikit-learn NearestNeighbors package (). This implements an efficient _approximate_ nearest neighbour algorithm whereby one-time work is carried out to construct a table (at \((dn n)\) (see e.g. [9; 21]) and counted within the total _training_ times quoted) which subsequent calibration/test predictions then make use of. Query compute-costs are described in the associated documentation, e.g. \((d n)\) for the Ball-tree algorithm which the default automated algorithm selection in SciKit-Learn should at least match. In contrast, exact kNN costs are listed in the documentation as \((dn)\). As is evident in Table 2, Figure 3 and the quoted query and table-setup complexities, the nearest neighbour work increases with \(\)-dimension \(d\). Alternative nearest neighbour algorithms and/or dimension-reduction techniques to help address this are yet to be investigated. We set the number of nearest neighbours to be \(m=400\) for all usages in this paper having observed minimal sensitivity to this parameter on independent synthetic datasets. Although a simple cross-validation procedure could be followed if tuning of \(m\) is desired (at an increase in computational overhead), we wished to minimise such fine-tuning to emphasise the simplicity and robustness of the method we present, noting the strong performance we obtain despite this. At first glance, prediction complexity might appear restrictive, but some empirical tests on a laptop reveal comparable performance to SVGP prediction (Table 5).

## 7 Experimental Performance of GPnn Regression

### Performance on Real World Datasets

**Implementation Details1:** Comparisons are made between our method and the state-of-the-art approaches of SVGP  and five distributed methods ([15; 2; 33; 7] and  following the recommendation in ). We have chosen not to include other highly-performant approximations(e.g. structured kernel interpolation (SKI) methods and their extensions ([37; 39; 11])), since, to our knowledge, none have supplanted these methods in the community as ubiquitous benchmarks on general datasets. Parameters for our method are given in 6.1 and 6.2. SVGP used 1024 inducing points; the distributed methods all used randomly selected subsets of sizes as close as possible to 625. The learning rate for the Adam optimiser was 0.01 for SVGP and 0.1 for our method and distributed methods. All runs in Table 1 used the the squared exponential ("RBF") covariance function. A "pre-whitening" process (E.1) was applied to x values for all methods and the y values normalised (using training data-derived means and sds) to have mean zero and variance 1. More complete details are given in E. SVGP was run on a single Tesla V100 GPU with 16GB memory; all distributed methods run on eight Intel Xeon Platinum 8000 CPUs sharing 32GB of memory. Our method was run on a Macbook Pro with 2.4 GHz Intel core i5. See D for a full explanation of our selection and pre-processing of datasets which, apart from Protein, are taken from the UCI repository.

**Results** Runs were made on three randomly selected \(7/9,2/9\) splits into training and test sets. Table 1 shows MSE and NLL results for our method alongside SVGP and distributed method (note: \(n=training\)_set_ size). The table shows only the best of the five distributed methods' results (w.r.t. MSE) but full results and details of all methods and all three performance measures are given in E. Complete calibration results are also plotted in Figure 4. With the exception of the Bike dataset our method is found to outperform all methods simultaneously for both MSE and NLL, and calibration likewise bar a narrow second place on the Song dataset. Table 2 and Figure 3 show that this is achieved whilst undercutting the training costs of the other methods, an effect that is very pronounced for large training sets (e.g. approximately \(100\) faster than the other methods at \(n=1.6 10^{6}\) on House Electric). Figure 3 shows that a significant portion of time involves calibration; this can be parallelised (or eliminated if uncertainty is not required). Note also that larger timings observed for higher dimensional datasets are due to slower performance of the approximate nearest neighbour algorithm (6.2) in that regime, both for nn table construction and calibration. As discussed in 6.2, future improvements may reduce this effect. It is very interesting that "curse-of-dimensionality" has not impacted on the method's MSE, NLL or calibration competitiveness at large \(d\). This was despite the fact that a PCA analysis of the training \(\) values showed no concentration within a low dimensional space (as to be expected given the prewhitening that has been applied (subsection E.1).

**Conjecture 7**.: _Robustness to "curse-of-dimensionality" is at least partially explained by the increase in the intrinsic data-length-scale by a factor of order \(\) that must arise in order for GP methods to be effective._

The heuristic reasoning behind this conjecture is as follows: Unless length scale increases with \(d\) the kernel gram matrix will exhibit an abundance of exceptionally small off-diagonal entries and hence be unable able to gain significant predictive power. A \(\) increase serves to counterbalance this effect and seems consistent with length scales recovered from real data in practice.

### Performance on Massive Synthetic Datasets

We generated size \(5 10^{7}\) datasets using the 15-variable deterministic Oakley and O'Hagan function [20; 30] with i.i.d. variance-\(_{}^{2}\) additive noise sampled from a zero-mean Laplacian distribution (with much wider tails than \((0,_{}^{2})\)). This function has 5 inputs contributing significantly to output

    & &  &  \\ Dataset & \(n\) & \(d\) &  &  &  \\  Poletle & 4.6e+03 & 19 & 0.0091 \(\) 0.015 & **-0.214 \(\) 0.019** & -0.0667 \(\) 0.017 & 0.241 \(\) 0.0033 & **0.195 \(\) 0.0042** & 0.226 \(\) 0.0059 \\ Bike & 1.4e+04 & 13 & 0.977 \(\) 0.0057 & 0.953 \(\) 0.013 & **0.93 \(\) 0.0043** & 0.634 \(\) 0.004 & 0.624 \(\) 0.0079 & **0.606 \(\) 0.0033** \\ Protein & 3.6e+04 & 9 & 1.11 \(\) 0.0051 & **1.012 \(\) 0.0016** & 1.05 \(\) 0.0059 & 0.733 \(\) 0.0038 & **0.666 \(\) 0.0014** & 0.688 \(\) 0.0043 \\ Cslice & 4.2e+04 & 378 & 0.159 \(\) 0.052 & **-1.26 \(\) 0.01** & 0.467 \(\) 0.016 & 0.237 \(\) 0.012 & **0.132 \(\) 0.0006** & 0.384 \(\) 0.0064 \\ Road3D & 3.4e+05 & 2 & 0.685 \(\) 0.0041 & **0.371 \(\) 0.004** & 0.608 \(\) 0.018 & 0.478 \(\) 0.0023 & **0.351 \(\) 0.0014** & 0.443 \(\) 0.008 \\ Song & 4.6e+05 & 90 & 1.32 \(\) 0.0012 & **1.18 \(\) 0.0045** & 1.24 \(\) 0.0012 & 0.851 \(\) 6.7e-05 & **0.787 \(\) 0.0045** & 0.834 \(\) 0.0011 \\ HouseE & 1.6e+06 & 8 & -1.34 \(\) 0.0013 & **-1.56 \(\) 0.0065** & -1.46 \(\) 0.0046 & 0.0626 \(\) 5.2e-05 & **0.050 \(\) 0.00072** & 0.0566 \(\) 0.00011 \\   

Table 1: RMSE and NLL results (mean and standard deviation over 3 runs) for the best distributed method (w.r.t. MSE), SVGP and our method.

variance, 5 with smaller effect, and 5 with almost no effect. These properties are poorly matched by the isotropic covariance functions being applied, resulting in gross misspecification of the assumed \(\) model and the additive noise. Figure 5 shows performance achieved with both the squared exponential ("RBF") covariance function and the exponential (Matern 1/2) covariance function. It is very interesting to note the improvement in convergence rate achieved by the exponential kernel. (see Remark 2 for a potential explanation of why isotropic covariance functions are so effective at large \(n\)).

**Remark 8**.: _We checked to see whether this strong exponential kernel performance extended to UCI datasets. Surprisingly, given that it is not recommended for use in GP regression (e.g.  page 85), it produced best RMSE performance across the board when compared with RBF and Matern 3/2 kernels (Table 4), with Road3D RMSE reducing from 0.351 to 0.098. Exponential-kernel NLL performance was everywhere best apart from Ctslice and calibration was also better in most cases._

## 8 Discussion

**Related work**: The basic "subset-of-data" approximation () also achieves training efficiency by using a small portion of training data and can achieve surprisingly goods results (,  section 5.1, ). But it typically would need a much greater proportion of training data than we are using for large \(n\) due to its failure to leverage the power of large training sets for prediction; this explains why it is not consistently competitive with other methods. Passing references to the decoupling of prediction and estimation have been made, e.g. , but not shown to be as consistently powerful as we have found, nor justified in terms of robustness theory or explored as a mainstream

    & &  \\  & &  &  &  \\ Dataset & \(n\) & \(d\) & & & \\  Poleele & 4.6e+03 & 19 & \(17.1 0.66\) & \(28.8 0.22\) & \(\) \\ Bike & 1.4e+04 & 13 & \(43.5 0.64\) & \(\) & \(32.3 0.15\) \\ Protein & 3.6e+04 & 9 & \(98.9 1.7\) & \(\) & \(81.1 1.1\) \\ Cslice & 4.2e+04 & 378 & \(86.9 1.7\) & \(\) & \(98.2 1.8\) \\ Road3D & 3.4e+05 & 2 & \(1200.0 110.0\) & \(\) & \(760.0 8.0\) \\ Song & 4.6e+05 & 90 & \(1050.0 110.0\) & \(\) & \(1080.0 14.0\) \\ HouseE & 1.6e+06 & 8 & \(3110.0 250.0\) & \(\) & \(3720.0 17.0\) \\   

Table 2: Corresponding recorded training times (with mean and standard deviation from 3 runs) associated to the metrics in Table 1, i.e. recorded at the same time and with the time given for the “distributed” method relating to the best performing model in terms of MSE. Mean times are rounded to 3 s.f. and standard deviation to 2.

approach. Various works (primarily from the geospatial community) make use of nearest neighbour (NN) techniques for GPs (e.g. ). Vecchia () uses NNs to approximate likelihoods for parameter estimation, whilst Stein () adapts this work to REML using more distant points as well as NNs, again for parameter estimation purposes. In contrast, our focus is on using NNs for _prediction_, and whilst we have found passing references to its explicit use for this purpose (e.g. ) we have found little or no discussion of effectiveness in comparison to other methods on large datasets and no detailed accompanying analysis of its robustness properties and how these can achieve very high efficiency at scale.  gives a construction of a hierarchical fully Bayesian model ('NNGP') derived using collections of NN sets. This approach adds considerable computational overhead and code-complexity, e.g. use of Gibbs sampling. Whilst fully Bayesian treatment of hyperparameters is explored in the ML literature, e.g. , it has not been adopted by the ML community for use at scale due to its high computational cost relative to empirical Bayes methods (, section 5). Bearing this in mind, we consider the extension given in  for improved scalability (Algorithm 5 - 'conjugate NNGP') to be more relevant. In this hybrid method some hyperparameters are recovered as 'empirical-Bayes' point-estimates via grid-based search and the remainder treated in a fully Bayesian fashion using a conjugate prior with some choice of hyper-hyperparameters. This results in a Student-\(t\) predictive distribution, rather than Gaussian, but with equal first moment to ours and variance differing only by a (hyper-hyperparameter dependent) multiplicative factor; an effect that our recalibration step would render redundant (see F.1). Recent work () extends local geospatial GP methods into sparse variational ML applications. 'VNNGP' is shown to be competitive with other methods in  despite adding further approximations into the model. We note that when using all observations as the inducing points their predictive mechanism matches ours, up to choice of parameter estimation. We believe these pre-existing works, which differ significantly in approach and perspective, complement our own, which emphasizes the benefits of decoupling parameter estimation from prediction, the robustness properties that can be achieved at large scales, the efficacy of a simple recalibration step and can be run at high scale with a simple algorithm on an off-the-shelf laptop.

**Limitations and Future Research:** Our results exhibit a leap in speed and performance for GP regression at scale, but there remains more to be done to fully explain and extend performance (as evidenced by our remarks and conjectures). This is particularly so for high dimensional problems where (a) a faster nearest neighbour algorithm would have a particularly big pay-off and (b) there is a need to explain why "curse-of-dimensionality" appears not to have damaged the method's competitiveness (see Conjecture 7). Extensions of theory to broader aspects of GP robustness, rates of convergence and "strong calibration" (section 3) are current areas of some the authors' ongoing work.