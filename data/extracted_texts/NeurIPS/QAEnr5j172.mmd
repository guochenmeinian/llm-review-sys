# FashionR2R: Texture-preserving Rendered-to-Real Image Translation with Diffusion Models

Rui Hu1\({}^{}\)2, Qian He2\({}^{}\)3, Gaofeng He2, Jiedong Zhuang1,

Huang Chen2, Huafeng Liu1\({}^{}\)3, Huamin Wang2

###### Abstract

Modeling and producing lifelike clothed human images has attracted researchers' attention from different areas for decades, with the complexity from highly articulated and structured content. Rendering algorithms decompose and simulate the imaging process of a camera, while are limited by the accuracy of modeled variables and the efficiency of computation. Generative models can produce impressively vivid human images, however still lacking in controllability and editability. This paper studies photorealisman enhancement of rendered images, leveraging generative power from diffusion models on the controlled basis of rendering. We introduce a novel framework to translate rendered images into their realistic counterparts, which consists of two stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG). In DKI, we adopt positive (real) domain finetuning and negative (rendered) domain embedding to inject knowledge into a pretrained Text-to-image (T2I) diffusion model. In RIG, we generate the realistic image corresponding to the input rendered image, with a Texture-preserving Attention Control (TAC) to preserve fine-grained clothing textures, exploiting the decoupled features encoded in the UNet structure. Additionally, we introduce SynFashion dataset, featuring high-quality digital clothing images with diverse textures. Extensive experimental results demonstrate the superiority and effectiveness of our method in rendered-to-real image translation.

## 1 Introduction

Modeling and simulating digital humans and clothing has achieved significant progress [1; 2; 3; 4; 5], while leveraging these 3D assets for fashion e-commerce still remains a challenging problem. Due to the imperfection of 3D models and the approximation in rendering algorithms, rendered images cannot yet replace fashion photos taken by a camera, with deficiency in the realism of rendered human faces and skin, clothing shape and fabric, etc. This paper studies transferring rendered fashion images into their realistic counterparts, which is inherently an Image-to-Image (I2I) translation problem.

Existing works on improving the realism of rendered images mainly resort to retrieving and blending real image patches , or train a GAN-based network [7; 8; 9] due to lack of paired training data. Another line of works can tackle this problem as general I2I translation [10; 11; 12; 13]. However, these methods may still suffer from several limitations: Firstly, their image generation pipelines have limited power to utilize real image resources for highly-detailed enhancement and may suffer from instability and mode collapse from adversarial training. Moreover, they either focus on indoor/outdoorscene enhancement while keeping coarse object-level semantic layout, or try to maintain face identity in training through loss constraints on sketches, and thus have difficulty in preserving fine-grained texture in clothing images.

In this paper, we propose a novel framework based on diffusion models for rendered-to-real fashion image translation to address above limitations. Our main idea consists of two aspects: Firstly, we propose to leverage abundant generative prior from pretrained Text-to-Image (T2I) diffusion models , and apply simple adaptation to realistic image generation under the guidance of distilled rendered prior. Secondly, we adopt a texture-preserving mechanism by extracting spatial image structure through attention from an inversion pipeline.

To achieve this, we design a diffusion-based method consisting of two stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG). During DKI, we first finetune a pretrained T2I diffusion model  on real fashion photos with derived captions from BLIP , to adapt its capability in generating high-quality images to our target domain. After this adaptation, we propose to guide the image generation towards the negative direction of rendered effect. Inspired by Textual Inversion , we distill a general rendered "concept" with thousands of rendered fashion images by training a negative domain embedding vector based on the adapted base model. During RIG, we employ a DDIM inversion  pipeline to first invert a rendered image into the latent noise map, and then generate its corresponding real image using the previous embedding as a negative guidance . Similar to recent training-free controls in T2I generation method [19; 20; 21; 22], we discover that the attention map in the shallow layers of the UNet contains rich spatial image structure and can be used for fine-grained texture-preserving during the generation. Specifically, we inject query and key of the self-attention from the rendered image inversion and generation pipeline to the rendered-to-real image generation pipeline. This largely improves the consistency of intricate clothing texture details.

We evaluate our method on a public rendered Face Synthetics dataset  and our collected SynFashion Dataset with fine-grained digital clothing and abundant texture variations. Empirical results comparing to previous works and experimental analysis demonstrate the efficacy of our method. Our main contributions are three-folds:

(1) We propose a novel framework to address rendered-to-real fashion image translation by utilizing generative prior from pretrained diffusion models.

(2) We inject rendered-to-real domain knowledge into a pretrained T2I diffusion model through positive domain finetuning and negative domain embedding, and design a texture-preserving attention control to preserve fine-grained clothing textures during the translation.

(3) We collect a high-quality rendered fashion image dataset using the professional design software Style3D Studio, and plan to release the data with our paper to promote research in this important area.

## 2 Related Works

### Rendered-to-real Image Translation

Improving the realism of rendered images has been a long-standing problem due to the inherent limitations of rendering pipelines and the rich potential for commercial applications. CG2Real  proposes to retrieve similar images from a large collection of real photos and then applies local style transfer to upgrade color, tone and texture of the CG image. Deep CG2Real  adopts a two-stage deep learning framework to first transfer OpenGL images to PBR (Physically-Based Rendering) images, and then translates PBR to real images, disentangling lighting and texture in a CycleGAN-like  framework.  enhances photorealism under the guidance of a set of input G-buffers and learns the network with a perceptual discriminator.  proposes to learn a rendered image generator for human faces, which can encode the same face identity but different "style" from a real face image generator, based on StyleGAN [24; 25]. These methods all utilize limited data for generative training, while we propose to adapt diffusion models pretrained on large datasets for better image generation quality. Besides, applying these methods to fashion images often leads to the failure to preserve fine-grained clothing textures.

### Image-to-image Translation

Transferring a rendered fashion image into its realistic counterpart is inherently an image-to-image (I2I) translation problem, which has attracted wide interest in different realms of research [26; 27; 28;29; 30; 31]. Pix2Pix  utilizes a conditional-GAN [33; 34] and applies pixel-wise regularization based on paired training data, which is unavailable in many problem settings. Cycle-GAN  proposes to utilize cycle consistency [35; 36; 37; 38] and optimizes a two-sided mapping between input source domain and output target domain. CUT  addresses the computational redundancy and over-restriction in this framework by simplifying it to one-sided [39; 40; 11] and introduces a patch-wise contrastive loss [41; 42; 43] for refined local constraints. UNSB  proposes an iterative refinement method based on Schrodinger bridge to overcome potential mode collapse in GAN generation, while still has difficulty in faithfully translating high-resolution images. Different from general I2I tasks and domain adaptation, our method focuses on photorealism enhancement and can utilize more target-domain real photos for high-quality generation training, and thus can deal with imbalanced source-target training set. Style transfer [44; 45; 46] is a specific type of I2I task and can manage to transfer input source image to an arbitrary style [13; 47; 48; 49; 50] given one/few-shot target domain images as reference. These methods mainly focus on transferring style attributes like semantics, brushstrokes, colors, or material, while rendered-to-real requires preserving and enhancing complicated fine-grained details. Human/portrait relighting [51; 52] modifies the nuanced lighting condition in the input image, while does not focus on enhancing realism and should leave geometry and materials untouched. Super-resolution methods [53; 54; 55; 56; 57; 58] address detail enhancement, while their success largely relies on synthesizing pseudo low-resolution images to obtain training pairs [59; 60], which is non-trivial for rendered-to-real problem.

### Diffusion-based Image Synthesis

Recent progress in Text-to-Image (T2I) generation [14; 61; 62] based on diffusion models [63; 64; 65] opens up new opportunity for advancing rendered-to-real image translation. Many works have explored the possibility of utilizing abundant generative prior in pretrained diffusion models. Some [66; 16] apply the adaptation of generation for a new concept with a few images, through either finetuning the base model , or optimizing a text embedding . Others [67; 68] leverage text as guidance to edit a given image. However, rendered-to-real translation lies in the nuance of changes, which is too subtle to define as a "concept" or to capture with a few images. [69; 70] leverage diffusion models for texture estimation or PBR synthesis, while mainly focusing on the generation of certain variables for the rendering pipeline, rather than subtle modification of preset variables in a given input image. Additionally, [19; 20; 21; 22] discover that the attention in the SD UNet captures rich image features and can apply to content preservation and modification. In our work, we utilize self-attention in shallow layers from the rendered image inversion, to impose the consistency of fine-grained texture in image translation.

## 3 Method

### Preliminaries

**Latent Diffusion Models.** In diffusion framework, the forward diffusion process begins by generating noisy images \(x_{t}\) from clean images \(x_{0}\) sampled from a specified data distribution, accompanied by

Figure 1: The overall pipeline of our proposed method.

their respective noise labels \(\). These pairs are used to train a score estimator \(_{}\) usually based on the UNet architecture. The score estimator can serve as an effective approximation of the score function \( p(x)\) which directs the inverse denoising process to generate new data samples.

With distinguished capabilities in synthesizing images, the Latent Diffusion Model (LDM)  is selected as the backbone of our method. The LDM employs a pre-trained AutoEncoder to transform the diffusion process from pixel space to latent space and integrates a conditional branch, facilitating faster training and more flexible embedding of conditions. Specifically, the pre-trained Encoder \(()\)first encodes images into latent space \(z=(x)\). Following this, the score estimator network \(_{}\) is trained by taking the latent \(z\), step \(t\) and conditions \(c\) as input to predict the noise labels:

\[_{}_{z=(x),(0,I),t (1,T)}\|-_{}(z_{t},t,c)\|_ {2}^{2}\] (1)

For text to image generation task, condition \(c\) is usually the text embedding generated from text prompt \(y\) through a tokenizer and a pretrained CLIP  model \(c=(y)\). The intermediate noisy latent \(z_{t}\) is generated through the formula :

\[z_{t}=(t)}z_{0}+(t)},  N(0,I)\] (2)

\(\) is the cumulative product of the noise coefficients at each step. During the sampling process, the trained score estimator takes random Gaussian noise as input, along with text embedding as condition. It progressively predicts the noise added at each step, completing the denoising process to obtain \(_{0}\). The final image is obtained by the pretrained decoder \(_{0}=(_{0})\).

**Textual Inversion.** Textual inversion  introduces a new paradigm to T2I generation models, allowing the model to learn a new concept by setting a placeholder token "[C]" and obtaining the corresponding text embedding \(\) as a learnable vector. This vector is then trained and optimized using a few images represent this new concept:

\[=_{z=(x), (0,I),t U(1,T)}\|-_{}(z_{t},t,v) \|_{2}^{2}\] (3)

During training, the network parameters are all fixed, only the embedding is optimized.

**DDIM Sampling and Inversion.** Inversion is an effective method for finding the corresponding noise map of an image and achieving training-free control during the generation process. DDIM inversion is widely used due to its clear principles and easy implementation. The DDIM sampling process is :

\[z_{t-1}=_{t-1}}-_{t}} _{}(z_{t},t,c)}{_{t}}}+_{t-1}}_{}(z_{t},t,c)\] (4)

By simply assuming \(z_{t-1} z_{t}\) and rewriting the sampling process in reverse direction, the following DDIM Inversion  formula is given:

\[z_{t}=_{t}}-_{t-1}} _{}(z_{t-1},t,c)}{_{t-1}}}+_{t}}_{}(z_{t-1},t,c)\] (5)

Unlike direct noise addition, the DDIM Inversion allows for the original information of the image to be well preserved, enhancing the stability in the subsequent generation process.

### Overall Pipeline

Given a computer-rendered fashion image \(x_{cg}\), the goal of our method is to transform it into a corresponding realistic image \(x_{r}\) while preserving the garment's detailed textures. Defining realism and helping model understand what is "realistic" remains an open question. The challenge can be divided into two sub-tasks: one is making the fashion image appear realistic by enhancing aspects like wrinkles, lighting and color, which reflect true-to-life expressions. Another one is to maintain the texture details of the garment to achieve fine-grained, controllable generation.

As shown in Fig. 1, our method comprises two stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG). During the DKI phase, we infuse the model with information from both the rendered and realistic domains through fine-tuning and domain inversion. In the subsequent generation phase, we utilize negative domain embedding \(v_{nd}\) to stimulate the model's potential for generating realistic images and employ self-attention control to preserve texture details. For a better understanding, details will be further elaborated in Section 3.3 and Section 3.4.

### Domain Knowledge Injection

**Target Domain Knowledge Injection** To enhance the ability of the base SD model \(_{}\) to generate realistic images, especially concerning the appearance of garments and models, we use real studio-shot images \(x_{tr}\) to fine-tune the base model. This process injects real domain information into the model, thereby increasing its potential to generate authentic visual details, the fine-tuning process can be formulated as:

\[_{}^{*}=}{}_{z= (x_{tr}),e(0,I),t U(1,T)}\|- _{}(z_{t},t,v_{tr})\|_{2}^{2}\] (6)

where \(_{}\) is the pretrained SD model, \(v_{tr}\) is the embedding of the text description of the \(x_{tr}\).

**Source Domain Knowledge Injection** For the source domain rendered data, we hope that the model can understand its characteristics and deviated from the rendered data manifold as much as possible during the generation process. After the first step fine-tuning, we assume that the model has already enhanced its representation of the real domain manifold. If we can make the model deviate from the rendered data manifold, it can better express the characteristics of realistic images. Inspired by the concepts of Textual Inversion and Classifier-Free Guidance (CFG) with negative prompts, we expand the concept of Textual Inversion to Domain Inversion. We train a negative domain embedding on a fine-tuned base model using a large number of rendered images. This negative domain embedding guides the model to avoid certain content, here is the rendered domain characteristics, during the generation process.

Specifically, given that textual descriptions of what is real and rendered are limited, it is difficult to guide the model to generate images with satisfactory realism or to precisely direct it not to produce images with a rendered feel using text prompts only. Therefore, we consider using negative domain embedding \(v_{nd}\) trained on a large number of rendered images for guidance to inject the rendered domain knowledge to the model. It's worth nothing that unlike textual inversion, which typically optimizes a small embedding space with few images to represent a specific concept, such as a particular object in personalized generation or an easily expressible style. The concept of rendered domain in our task is much more general. Using a small embedding space corresponding to few images to represent this would easily lead to over-fitting to the content of the training images. We use the largest available embedding size to train the negative domain embedding, which is corresponding to the placeholder token size of 75:

\[_{nd}=_{z=(x_{cg}),e (0,I),t U(1,T)}\|-_{}^{*}(z_{t},t,v) \|_{2}^{2}\] (7)

During the training of negative domain embedding, we freeze the parameters in the fine-tuned model \(_{}^{*}\), and find the \(v_{nd}\) through direct optimization with a certain number of rendered images.

### Realistic Image Generation

**Negative Embedding Guidance** After domain knowledge injection, we can use the negative domain embedding to guide the model in generating realistic images. During each denoising step, the negative domain embedding guidance is defined by:

\[_{}^{*}(z_{t},t,v_{nd})=w_{ }^{*}(z_{t},t,v_{})+(1-w)_{}^{ *}(z_{t},t,v_{nd})\] (8)

where \(v_{}\) denotes the embedding of Null text. With a guidance scale \(w\) larger than 1, the negative domain embedding becomes effective. Unlike traditional CFG guidance, here we do not use any positive prompts processed through CLIP to obtain the embedding as conditions. Instead, we directly

Figure 2: The diagram of Texture-preserving Attention Control (TAC).

employ a Null text embedding. The initial noise latent is obtained through DDIM inversion of the given rendered image. During the denoising process, we replace the CLIP conditioning branch, since the negative domain embedding is trained on a fine-tuned model, it can interact more effectively with the base model's latent space. This consistency allows for more precise adjustments in the latent manifold compared to embedding derived from text via CLIP.

**Texture-preserving Attention Control (TAC)** Inspired by previous work , the attention features in the diffusion UNet, which includes both cross attention and self attention, hold rich information critical for generating the new images. Cross attention typically handles the attributes and semantics of the generated image, while self-attention maps play a crucial role in preserving geometric shapes and intricate details. The initial noise latent \(_{t}\) derived from the DDIM inversion of the original rendered image can be used in unconditional generation and extract the texture related attention features as shown in Fig. 2. However, directly replacing all self-attention maps can lead to a decrease in the realism of the generated images. We argue that this is because the attention map contains both the texture details of the garment and the general rendered domain characteristics. Therefore, we propose to control the self attention feature only in the shallow layers of the denoising UNet to decouple the texture details feature from the general rendered domain features. During the implementation, we also find that in the deep feature spaces with higher downsampling rates, it becomes challenging to identify features related to the texture details. Thus, our TAC is defined as:

\[},}=TAC(Q_{cg}^{t},K_{cg}^{t},Q_{r}^{t},K_{ r}^{t},t)=Q_{cg}^{t},K_{cg}^{t}&t< T,f>F\\ Q_{r}^{t},K_{r}^{t}&\] (9)

where \(\) is the parameter that indicates how many steps before the TAC should be applied and \(f\) is the feature size of different layers, only those layers exceeding the specified size \(F\) undergo TAC, particularly in the shallow layers. Specifically, the cg-domain self-attention features are derived from the reverse sampling process starting from the noisy latent, which is obtained by performing DDIM inversion on the input image latent. In contrast, the r-domain self-attention features differ due to the incorporation of negative domain guidance and the self-attention injection.

Figure 3: Results on our proposed SynFashion Dataset. (**Please zoom in for details.**)

## 4 Experiments

### Datasets

To evaluate our method and conduct comprehensive comparisons, we introduce a high-quality rendered fashion image dataset, named Synthetic Fashion (SynFashion), with the professional garment design software Style3D Studio. SynFashion consists of 10k rendered images in 20 categories, including pants, T-shirt, lingerie and swimwear, half skirt, hoodie, coat, jacket, set, home-wear, hat, Hanfu, jeans, shorts, down jacket, vest and camisole, shirt, suit, dress, sweater and trench coat. For each category, we use Style3D Studio to build 10 to 40 projects in different 3D geometry with corresponding texture and design, and then randomly sample several new textures to change its appearance. There are overall 375 projects in 3D and 500 additional texture collected from Internet. For each textured 3D geometry, we render four views, including front, back, and two randomly sampled views. After rendering, we crop the enlarged garment area of each image and resize it to 768 \(\) 1024. Due to legal issues, some of the images contain a digital human figure but not the complete face. To supplement the evaluation on rendered human faces, we also conduct experiments on the public available Face Synthetics dataset  with its first 10k images.

### Implementation Details

**Implementation.** We implement our method with pretrained Stable Diffusion (SD) model and finetune the base model with 2500 realistic images at a \(1024 1024\) resolution for source domain knowledge injection. The finetuning uses images from iMaterialist (Fashion) 2019 FGVC dataset , based on the publicly available SD v1.5, and is conducted on 2 RTX 4090 with a batch size of 6. Based on the finetuned model, we train our negative domain embedding with 2500 rendered images on a single RTX 4090 with a batch size of 1. The rendered images are resized to the resolution of \(512 512\). The placeholder embedding size is 75 and the learning rate is 5e-4. During sampling, we perform DDIM sampling with default 50 denoising steps with a denoising strength of 0.3 as default. The \(\) is set to 0.9 as default, which means that the TAC is performed on the first 90% of sampling steps. Only the attention maps in the first and second shallow layers are used for TAC. Note that the denoising strength and \(\) may be changed to obtain different level of image translation. We compare our method with three state-of-the-art unpaired image-to-image translation method, CUT, SANTA and UNSB, and one diffusion-based style transfer method VCT. For CUT, SANTA and UNSB, we train the models for about 400 epochs following the official code with same training data.

Figure 4: Results on the Face Synthetics dataset. (**Please zoom in for details.**)

### Results

**Qualitative Results** Fig. 3 and Fig. 4 show the visual comparison between our method, CUT , SANTA , UNSB  and VCT  on the SynFashion and Face Synthetics datasets. As can be seen from the figures, both the CUT and SANTA methods exhibit some degree of image degradation and fail to effectively learn the concept of image realism from data across rendered and real domains, thus enable to generate realistic images. The diffusion based style transfer method VCT maintains image quality but fails to extract realistic image features from the guidance image, also resulting in the loss of image details. Compared to previous methods, the UNSB method achieves better consistency in terms of content, but like CUT and SANTA, it performs poorly in maintaining color fidelity and the realism effect is not good. The proposed method effectively enhances the overall realism of the image, particularly in capturing the facial and hand features of models, as well as the texture and wrinkle details of the garment.

**Quantitative Results** The absence of ground truth for rendered-to-real translation and domain gap between the source rendered and target real domains make quantitative evaluation challenging.

Following the previous work , we use KID to evaluate the realism of the generated images and the average SSIM and LPIPS to assess content similarity. For each dataset, we use the 7500 testing result images from each method and calculate the KID against the realistic images and the SSIM/LPIPS against the rendered images. As shown in Tab. 1, our method shows significant improvements in terms of realism as well as overall texture and content consistency. The standard deviations here show the variance over test inputs for a fixed model to demonstrate the stability and generalization ability.

**User Studies** We adopt user studies to provide more quantitative insight into perceived realism, image quality, and consistency to input rendered images. We follow StyleDiffusion  in style-transfer and compare our method to previous works in pairs. Specifically, we randomly sample 100 image pairs from each dataset for user evaluation. Each pair contains one image generated by our method and a corresponding image generated by another comparison method, presented side by side in random order. Users are asked to assess the images based on three criteria: 1) which result appears more realistic, 2) which result demonstrates overall better image quality, and 3) which result shows better consistency with the reference image.

We collected approximately 2,000 votes per question from 20 users and present the percentage of votes where existing methods were preferred over ours in the Tab. 2. Lower percentages indicate that our method was favored over the competitors. Our approach garnered a strong preference in terms of overall realism and image quality, while also showing a clear advantage in maintaining consistency with the reference images.

  
**Dataset** &  &  \\   & KID\(\)(std) & LPIPS\(\)(std) & SSIM\(\)(std) & KID\(\)(std) & LPIPS\(\)(std) & SSIM\(\)(std) \\  CUT  & 80.553 (2.447) & 0.365 (0.073) & 0.664 (0.079) & 59.238 (1.599) & 0.170 (0.060) & 0.847 (0.067) \\ SANTA  & 90.390 (2.929) & 0.387 (0.079) & 0.618 (0.104) & 61.636 (1.628) & 0.294 (0.067) & 0.741 (0.082) \\ VCT  & 74.445 (2.273) & **0.096** (0.027) & 0.807 (0.072) & 59.489 (1.499) & 0.178 (0.058) & 0.807 (0.085) \\ UNSB  & 76.389 (2.465) & 0.229 (0.069) & 0.818 (0.070) & 59.496 (1.453) & 0.130 (0.040) & **0.891** (0.054) \\ Ours & **73.871** (1.973) & 0.121 (0.035) & **0.831** (0.068) & **54.720** (1.362) & **0.067** (0.025) & 0.881 (0.055) \\   

Table 1: Quantitative comparisons on Face Synthetics and SynFashion datasets.

  
**Dataset** &  &  \\   & Overall Realism & Image Quality & Consistency & Overall Realism & Image Quality & Consistency \\  CUT & 0.529\% & 0.529\% & 13.175\% & 8.994\% & 6.878\% & 16.931\% \\ SANTA & 0.922\% & 1.383\% & 12.304\% & 3.333\% & 5.238\% & 11.571\% \\ VCT & 5.952\% & 14.286\% & 20.714\% & 2.041\% & 6.122\% & 18.367\% \\ UNSB & 4.511\% & 6.767\% & 21.278\% & 9.821\% & 9.821\% & 26.607\% \\   

Table 2: User studies on overall realism, image quality and consistency. The table shows the percentage of votes that existing methods are preferred to ours.

### Ablation Study and Further Analysis

We conduct ablation study on two datasets in a drop-one-out manner and evaluate the performance of each module in the proposed method during inference and analyze the impact on the final results. As shown in Fig. 5, without source DKI (embedding), the fine-tuned base model tends to recover the input rendering image with DDIM inversion. Without target DKI (fine-tuning), the rendering effect slightly decreases but the output is still not real enough due to lack of concentrated knowledge on real human and clothing. Without TAC, the semantic structure such as face identity and clothing design can significantly deviate from the input. The quantitative results are in Tab. 3.

Fig. 6 shows the trade-off between image realism and texture preservation. With a high denoising strength, the generated images resemble realistic images more closely but retain fewer details from the original rendered image. Increasing the TAC ratio helps to better preserve the texture details and facial features. Unlike other content preservation techniques such as inpainting, which can lead to potential visual incoherence, our TAC seems to blend the attention features smoothly into the generation process and cause no obvious coherence issues.

## 5 Conclusion

In this paper, we introduce a novel diffusion-based framework for rendered-to-real fashion image translation and create a high-quality rendered fashion image dataset (SynFashion), which includes 10k images with multiple classes. With Domain Knowledge Injection (DKI) and Texture-preserving Attention Control (TAC), our method can successfully translate the rendered fashion image into its realistic counterpart with significant realism improvement and texture details preservation. Extensive experimental results demonstrate the superiority and effectiveness of our method.

Figure 5: Visual examples of ablation study in a drop-one-out manner. (DKI: Domain Knowledge Injection. TAC: Texture-preserving Attention Control.)

  
**Dataset** &  &  \\   & KID\(\)(std) & LPIPS\(\)(std) & SSIM\(\)(std) & KID\(\)(std) & LPIPS\(\)(std) & SSIM\(\)(std) \\  w/o source DKI & 77.376 (2.063) & 0.107 (0.029) & 0.857 (0.059) & 58.520 (1.902) & 0.059 (0.019) & 0.903 (0.065) \\ w/o target DKI & 78.927 (2.134) & 0.114 (0.031) & 0.845 (0.063) & 60.186 (1.623) & 0.064 (0.022) & 0.897 (0.056) \\ w/o TAC & 69.349 (1.485) & 0.253 (0.070) & 0.720 (0.085) & 51.392 (1.083) & 0.183 (0.047) & 0.794 (0.074) \\ Ours & 73.831 (1.973) & 0.121 (0.035) & 0.831 (0.068) & 54.720 (1.362) & 0.067 (0.025) & 0.881 (0.055) \\   

Table 3: Ablation study in a drop-on-out manner.

**Limitations and social impacts** While our method achieves superior results on this challenging task, there are still several problems to be further explored. In this work, we simply use DDIM inversion to extract texture-related attention features. However, the inversion process slows down the generation, requiring approximately one minute to translate an image with a resolution of \(768 1024\). This could potentially be accelerated by recent inversion-free methods. We test the inference time and resource consumption for a 512x512 image on an RTX 3090, as shown in Tab. 4. Note that comparing to VCT, which is also based on diffusion, our method takes much less memory and time during testing as we do not need to perform additional optimization for each testing image. Our method cannot handle real-time applications for now, but has potential for improvement with future integration with SD Turbo or SD Lightning. Additionally, for different images, finding the optimal balance between the TAC ratio and denoising strength may require more empirical refinements to achieve the best result. Due to limitations on computational resources, experiments were not conducted on more advanced models such as SDXL . Given that our method is based on SD1.5 and for human-related content generation, potential negative societal impacts of exploiting this method could be violation of portrait rights, racial bias, or inappropriate content in generation when the denoising strength is high. Relative solutions can include but are not limited to using authorized, diverse and balanced training data and training detection models to prevent inappropriate content generation.

## 6 Acknowledgments

This work is supported in part by the National Key Research and Development Program of China (No: 2021YFF0501503) and by the Talent Program of Zhejiang Province (No: 2021R51004).

    & CUT & SANTA & VCT & UNSB & Ours \\  Memory Required (GB) & 3.3 & 4.5 & 22 & 7.4 & 7.7 \\ Testing Time (s) & 0.38 & 0.33 & 62.47 & 0.53 & 7.98 \\   

Table 4: Comparison of memory required and testing time across different methods.

Figure 6: A visual example of tuning TAC ratio and denoising strength.