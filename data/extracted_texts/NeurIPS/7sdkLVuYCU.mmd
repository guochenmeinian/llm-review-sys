# QTIP: Quantization with Trellises and Incoherence Processing

Albert Tseng

Cornell University

albert@cs.cornell.edu

&Qingyao Sun

Cornell University

qs234@cornell.edu

&David Hou

dhou@alumni.caltech.edu

&Christopher De Sa

Cornell University

cdesa@cs.cornell.edu

###### Abstract

Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing weights to low-precision datatypes. Since LLM inference is usually memory-bound, PTQ methods can improve inference throughput. Recent state-of-the-art PTQ approaches use vector quantization (VQ) to quantize multiple weights at once, which improves information utilization through better shaping. However, VQ requires a codebook with size exponential in the dimension. This limits current VQ-based PTQ works to low VQ dimensions (\( 8\)) that in turn limit quantization quality. Here, we introduce QTIP, which instead uses trellis coded quantization (TCQ) to achieve ultra-high-dimensional quantization. TCQ uses a stateful decoder that separates the codebook size from the bitrate and effective dimension. QTIP introduces a spectrum of lookup-only to computed lookup-free trellis codes designed for a hardware-efficient "bitshift" trellis structure; these codes achieve state-of-the-art results in both quantization quality and inference speed.

## 1 Introduction

Large language models (LLMs) have accelerated advancements in fields ranging from natural language processing  to scientific modeling . However, the largest LLMs have hundreds of billions of parameters that can take over a terabyte of memory to load in half-precision; this size poses significant challenges for the practical deployment of LLMs [33; 17; 2]. For example, small-batch autoregressive decoding, a common form of inference for LLMs, is memory bound . Even on a modern datacenter GPU with \( 3\)TB/s memory bandwidth, a large LLM (\(>200\)GB) can only be directly run at \(<20\) tokens per second and may require multiple devices . One way to accelerate inference is by compressing LLMs. This directly reduces the memory footprint of the model and increases the theoretical maximum inference throughput on any given machine.

One form of compression, weight-only post-training quantization (PTQ), quantizes trained model weights to lower precision datatypes [9; 35; 5]. The latest state-of-the-art weight-only PTQ methods, QuIP\(\#\) and AQLM, use vector quantization (VQ) to achieve high-quality 2-bit models [35; 11]. In VQ, a vector \(x^{d}\) is quantized to one of \(2^{kd}\) vectors in \(^{d}\) that form a codebook \(C^{2^{kd} d}\). A higher vector dimension \(d\) allows for better codebook shaping and packing density, improving information utilization . However, unstructured VQ requires exponential time and space in both the bitrate and dimension, limiting its practicality. During quantization, VQ costs \(O(2^{kd}d)\) time to perform nearest-neighbor rounding to \(C\), and during inference, \(C\) must fit in hardware cache for fast lookups. This exponential scaling limits how high \(d\) can be and thus the advantages of VQ over scalar quantization.

To address this limitation, we propose QTIP, which uses trellis-coded quantization (TCQ) to enable tractable ultra-high-dimensional (\(>100\)) quantization and improve quantization quality over prior VQ-based approaches. In the simplest scalar form of TCQ, a length-\(T\) sequence \(S\) is _statefully_ quantized using a trellis - a directed graph \(G\) with \(2^{L}\) nodes, each with \(2^{k}\) incoming and outgoing edges and a scalar value . The reconstructed sequence \(\) corresponds to the node values of a length-\(T\) walk on \(G\), and quantization finds the walk that minimizes some distortion metric on \(S\) and \(\). Since neighboring entries in \(\) are connected by one of \(2^{k}\) edges, we only need to store _which edge_ an entry came from, which takes \(k\) bits. For additive distortion metrics such as squared error, the optimal \(\) can be found with the Viterbi algorithm, which runs in \(O(2^{L}T)\) time [13; 24]. This means that the cost of quantization is _independent_ of the bitrate \(k\) and _linear_ in the sequence dimension \(T\), enabling tractable high dimensional quantization.

However, TCQ is not free. During inference, vanilla TCQ requires storing both \(G\) and the size \(2^{L} V\) node value codebook, which can be too large to fit in cache. TCQ-quantized sequences also cannot generally be decoded in parallel, as \(t\)th element of \(\) could depend on up to the first \(tk\) encoded bits. In QTIP, we solve these issues by introducing a series of fast compute-based Gaussian codes designed for the hardware-efficient "bitshift trellis." Specifically, the bitshift trellis supports parallel decoding, does not require storing \(G\), and our compute-based codes eliminate needing to store a large node value codebook. This enables high-quality quantization of Gaussian sources while supporting fast inference, and we adopt incoherence processing with the random Hadamard transform to ensure that LLM weights are approximately i.i.d Gaussian distributed. Altogether, QTIP

1. Achieves a state-of-the-art combination of weight-only LLM PTQ quality and fast inference through hardware-efficient trellis and codebook design.
2. Introduces multiple novel hardware-efficient (\( 4\) instructions per weight) compute-based random Gaussian codes for TCQ on i.i.d. Gaussian sources.

## 2 Background and Related Works

We focus on weight-only post-training quantization (PTQ) of LLMs in this work; other model-compression approaches include quantization-aware training (QAT) and pruning. These methods are not strictly orthogonal to each other, as one could both prune and quantize a model. Since QTIP is a weight-only PTQ method, the rest of this section focuses on this area. Most current state-of-the-art PTQ methods round to minimize the per-layer proxy loss from Nagel et al. .

\[()=_{x}[\|(-W)x\|^{2}]=( (-W)H(-W)^{T})\] (1)

Figure 1: QTIP performs ultra-high dimensional (\(>100\)) quantization by using Trellis Coded Quantization, which has linear cost in dimension. This enables QTIP to outperform Vector Quantization-based approaches (QuIP\(\#\), AQLM) that are limited to low dimensions. With QTIP, 2 bit models scale better than theoretically optimal 4 bit models.

Here, \(^{m n}\) is the quantized weight matrix, \(x^{n}\) is an input activation, and \(H=_{x}[xx^{T}]^{n n}\) is interpreted as a proxy Hessian matrix. This objective is defined _per-layer_, making it tractable for very large models. However, minimizing it is difficult due to the non-differentiable nature of quantization. Instead many works have proposed algorithms such as Hessian-based adaptive rounding, alternating optimization, and even coordinate descent to approximately minimize the proxy error [11; 5; 35; 14].

### Incoherence Processing

The effectiveness of these methods depends on properties of \(W\). For example, many works have observed that weight and activation outliers cause poor quantization quality [10; 20; 29]. In QuIP, Chee et al.  proposed that _incoherence_ was important for quantifying this effect.

**Definition 2.1** (Chee et al. ).: A Hessian \(H^{n n}\) is \(\)-incoherent if its eigendecomposition \(H=Q Q^{T}\) has \(_{i,j}\ |Q_{ij}|=_{i,j}\ |e_{i}^{T}Qe_{j}|/\). A weight matrix \(W^{m n}\) is \(\)-incoherent if \(_{i,j}\ |W_{ij}|=_{i,j}\ |e_{i}^{T}We_{j}|\|W\|_{F}/\).

Essentially, incoherence means the weights and important rounding directions (Hessian eigenvectors) are not too large in any direction, aiding quantization. To make \(W,H\) incoherent (small \(\)), one can perform _incoherence processing_ (IP) by conjugating \(W,H\) with random orthogonal matrices \(U,V: UWV^{T}, VHV^{T}\). QuIP\(\#\) introduced IP with the random Hadamard transformation (RHT), which performs \( V_{m}S_{m}WS_{n}V_{n}^{T}, V_{n}S_{n}HS_{n }V_{n}^{T}\) where \(V_{k}\) is a \(k k\) Hadamard matrix and \(S_{k}\) is a length \(k\) random sign vector. The RHT achieves, with probability \( 1-\), \(_{}=2(4mn/)\), meaning that \(\)'s entries are approximately independently Gaussian distributed, which can aid quantization [35; 3]. We choose to build on incoherence processing here because the independent Gaussian-like weights it produces are suitable inputs for trellis coding .

### Vector Quantization (VQ) for LLM PTQ

\(k\)-bit VQ quantizes a \(d\) dimensional vector \(S\) to one of \(2^{kd}\)\(d\)-dimensional vectors that form a codebook \(C^{2^{kd} d}\). Since \(C\) is an unstructured collection of arbitrary vectors, VQ enables better shaping and packing density than scalar product quantization (SPQ), where each entry in \(S\) is quantized independently . However, this also comes at the cost of exponential time quantization and exponential space inference: finding the nearest neighbor in \(C\) requires \(O(2^{kd}d)\) time, and storing \(C\) requires \(O(2^{kd}d)\) space. The current crop of state-of-the-art LLM PTQ methods, QuIP\(\#\) and AQLM, both use VQ to achieve high-quality 2-bit models. Since the shaping advantage of VQ comes from high dimensionality, both QuIP\(\#\) and AQLM attempt to maximize dimensionality. AQLM's uses a large 8D codebook (1MiB) that does not fit in L1 cache. QuIP\(\#\) uses an 8D compressible codebook based on the \(E_{8}\) lattice, which is highly symmetric. This codebook is compressible by \(256\) and barely fits in L1 cache. In either case, the VQ dimension is effectively hardware-limited to \( 8\), motivating methods that enable even higher-dimensional quantization.

### Trellis-Coded Quantization (TCQ)

TCQ was first proposed by Marcellin and Fischer  to apply the benefits of trellis coded _modulation_, a conceptually dual problem, to quantization. Define a \((L,k,V)\) trellis \(G\) as a directed graph with \(2^{L}\) nodes, each of which has \(2^{kV}\) incoming and outgoing edges and a value \(^{V}\); these values form a codebook \(C^{2^{L} V}\). To quantize a length-\(T\) sequence \(S^{T}\), each contiguous length-\(V\) subsequence of \(S\) is assigned to a node \( G\), with the restriction that the assigned nodes form a walk. The reconstruction \(\) of \(S\) is then given by concatenating node values in the walk. When \(V=1\), this setup describes Marcellin and Fischer 's original scalar TCQ. When \(V>1\), this describes TCVQ, which applies TCQ to vectors [12; 37].

Finding the optimal \(\) under an additive distortion metric can be done with the Viterbi algorithm in \(O(2^{L}T)\) time. This is linear in sequence length, enabling ultra-high dimensional quantization. For exposition, we briefly describe the Viterbi algorithm here. Concretely, if we want to quantize a \(T\)-length scalar sequence reinterpreted as a sequence of vectors \(s_{1},s_{2},,s_{T/V}^{V}\) using a trellis code with graph \(G\) and codebook \(C\), this corresponds to solving the optimization problem

\[_{i=1}^{T/V}\|C_{x_{i}}-s_{i}\|^{2}x_{1},x_{2},,x_{T/V}G.\]

This optimization problem can be solved exactly with dynamic programming via the value function

\[_{t}(x)=\ \{_{i=1}^{t}\|C_{x_{i}}-s_{i}\|^{2}\ |\ x_{1},x_{2},,x_{t}Gx_{t}=x\}\]

using the update rule

\[_{t}(y)=_{(x,y) G}_{t-1}(x)+\|C_{y}-s_{t}\|^{2}.\]

This Viterbi approach clearly takes time linear in \(T\) and in the number of edges of \(G\); with a few simple optimizations this can be brought to \(O(2^{L}T)\). In comparison, brute-force-searching all possible \(2^{kT}\) codes--which is what we would need to do for an unstructured \(k\)-bit \(T\)-dimensional codebook--would take time proportional to \(2^{LT/V}\). The ability to tractably find the closest representable vector in \(^{T}\), even for large \(T\), is in some sense the "main benefit" of trellis coding. For i.i.d sources, as \(L\) increases, TCQ efficiently approaches the infinite-length distortion-rate \(D_{R}\), which lower bounds the attainable distortion of a \(k\)-bit quantizer . As shown in Table 1, when quantizing an i.i.d. Gaussian with \(k=2\), the scalar Lloyd-Max quantizer attains 0.118 MSE, QuIP\(\#\)'s 8D E8P codebook 0.089 MSE, our (QTIP) 256D \(L=16\) TCQ quantizer 0.069 MSE, and \(D_{R}=0.063\).

## 3 Qtip

Quantizing with TCQ requires storing both the codebook (\(2^{L} V\)) and trellis structure (\(2^{L} 2^{kV}\)) during inference. These components are too large for fast inference when \(L 12\), which is necessary for high quality. Furthermore, for a generic trellis, recovering the state (and so the decoded value) at step \(t\)th requires a graph walk using the first \(kt\) bits: this prevents parallel decoding. QTIP solves these problems with a novel combination of incoherence processing, a hardware-efficient "bitshift trellis," and fast compute-based random Gaussian codes. Incoherence processing makes \(W\) approximatelly i.i.d Gaussian, which reduces quantization to Gaussian source coding. The bitshift trellis removes needing to store the trellis structure during decoding and also enables parallel decoding. Finally, the fast compute-based random Gaussian codes remove the need to store the full codebook, completing the equation for fast inference. On the quality side, the fast random Gaussian codes enable the simple bitshift trellis to match complicated trellises and achieve state-of-the-art quantization quality.

The main focus of QTIP is on _what to quantize with_ (i.e. TCQ) and not _how to quantize_ (e.g. adaptive rounding or descent methods). The general construction of QTIP can be used as a drop-in replacement for VQ in any rounding framework. In the following sections, we first describe the "bitshift" trellis (Section 3.1). Then, we describe a series of fast compute-based codes for i.i.d Gaussian sources, aligning with different types of hardware (Sections 3.1.1 and 3.1.2). Finally, we give an approximation for the tail-biting trellis problem, which lets us more efficiently load weights in hardware (Section 3.2).

### "Bitshift" Trellis and Codebook Design

The bitshift trellis was introduced by Mao and Gray  as part of the "random permutation trellis coder" (RPTC). In the bitshift trellis, node \(i\) has an edge to node \(j\) if \( c,0 c<2^{kV},\) s.t. \(j=(i2^{kV}2^{L})+c\). Essentially, the top \(L-kV\) bits of \(j\) equal the bottom \(L-kV\) bits of \(i\). This means that the first group of \(V\) weights depends only on the bits at positions \(\{1,2,,L\}\), the second only on bit positions \(\{kV+1,kV+2,,kV+L\}\), and in general the \(t\)th on bit positions \(\{(t-1)kV+1,,(t-1)kV+L\}\). During inference, obtaining the next compressed group of \(V\) weights in a sequence only requires bitshifting by \(kV\) bits, which is supported on virtually all hardware. Furthermore, since each group of \(V\) weights only depends on a contiguous window of \(L\) bits in \(\), decoding can be parallelized. Figure 2 shows a simple \((L=2,k=1,V=1)\) bitshift trellis. Note that edges only exist between nodes that overlap by 1 bit, and storing the quantized length 6 \(\) indeed only requires 6 bits (plus the initial state).

Quantizing an i.i.d. source with the bitshift trellis is nontrivial because neighboring groups of weights sharing many bits can potentially lead to strong correlations (Figure 3 LL). The RPTC permutesthe codebook to decorrelate neighboring weight groups (Figure 3 RR) . However, this requires storing the codebook or storing and applying the permutation, both of which are prohibitively expensive during decoding. Instead, QTIP introduces a series of compute-based codes to produce a psuedorandom code, which has the same decorrelating effect and admits fast inference. To match approximately i.i.d. Gaussian RHT-transformed matrices, these codes produce psuedorandom approximate Gaussians in as few as 2 instructions per weight (see Table 1 and Figure 3). To the best of our knowledge, these code constructions alone are novel and we are the first to propose a lookup-free Gaussian trellis code.

#### 3.1.1 Lookup-Free Computed Codes

Here, we present two pure-computed lookup-free codes that produce a pseudorandom approximately Gaussian number from a \(L\) bit word, enabling fast decoding on cache-limited hardware. These codes avoid strong correlations and can be implemented in \( 4\) hardware instructions per weight on NVIDIA GPUs. We present two codes here to illustrate that multiple such codes are possible: in practice a lookup-free code can be designed to use the instructions available on whatever hardware we want to run on.

Algorithm 1 (1MAD) first runs a linear congruential generator (LCG) to produce a pseudorandom 32-bit word . This requires 2 instructions (MAD and &). It then sums the 32-bit word as four 8-bit unsigned integers; this sum is approximately Gaussian distributed. This requires 1 instruction (vabsdiff4). Finally, this sum must be scaled and shifted (another MAD). Although there are only \(2^{10}\) representable values even when \(L>10\), this does not empirically affect quantization quality. 1MAD requires choosing \(a\) and \(b\) to avoid strong correlations; we set \(a=34038481\) and \(b=76625530\) (Figure 3 LC).

Algorithm 2 (3INST) also first runs an LCG to produce a random 32-bit word \(X\). Then, it XORs the bottom 16 bits of \(X\) with the mantissa bits, bottom two exponent bits, and sign bit of a magic FP16 number \(m\) to produce an FP16 number \(m_{1}\). It then repeats this with the top 16 bits of \(X\) to produce \(m_{2}\) and returns \(m_{1}+m_{2}\). This entire process can be implemented in 3 ALU instructions1 with a MAD for the LCG, a 1op3 to mask and XOR with a packed duplicated \(m\), and then summing \(m_{1}\) and \(m_{2}\)

  & SQ & VQ &  &  \\  Quant. & Lloyd-Max & QUIP\# E8P & 1MAD & 3INST & RPTC & HYB & RPTC & \(D_{R}\) \\  Dim. & 1 & 8 & 256 & 256 & 256 & 256 & 256 & \(\) \\ MSE. & 0.118 & 0.089 & 0.069 & 0.069 & 0.068 & 0.071 & 0.069 & 0.063 \\  

Table 1: QTIP’s compute-based codes (1MAD, 3INST, HYB) achieve similar distortion rates as a pure-lookup random Gaussian trellis code (RPTC) when quantizing an i.i.d Gaussian source to 2 bits. All TCQ methods (\(L=16\)) outperform SQ and VQ and are significantly closer to the infinite-length distortion rate \(D_{R}\), which lower bounds the distortion a \(k\)-bit quantizer can attain.

Figure 2: A bitshift trellis code with \(L=2,k=1,V=1\). Nodes 0, 1, 2, and 3 have code values 0.5, 0.1, 0.8, and 0.3, respectively. Each node can only transition to the \(2^{kV}=2\) nodes that share their top \(L-kV=1\) bit with its bottom \(L-kV=1\) bit. In this example, \(\) can be stored as 0010110. \(\) is also _tail-biting_, so the last \(L-kV=1\) bits can be dropped to give \(=\).

\(m_{1}+m_{2}\) is approximately distributed by the sum of two mirrored exponential distributions, which is close to Gaussian. Like with Algorithm 1, \(a,b,\) and \(m\) must be chosen to avoid correlations; we used \(a=89226354,b=64248484,m=0.922\) (Figure 3 right).

#### 3.1.2 Hybrid Lookup-Computed Codes

Here, we describe a hybrid computed-lookup code that computes a pseudorandom (or hashed) index into a 2D vector codebook (\(V=2\)). This code is tailored for modern GPUs, which have enough cache for a small in-memory LUT--one benefit of using such a LUT over a purely computed codebook is that a LUT can be fine-tuned after quantization. Algorithm 3 first performs the hash \(X X^{2}+X\) to mix the lower order and upper order bits of \(X\). Then, it takes bits \((14-Q+1)-14\) (0 indexed) as an index into a \(2^{Q} 2\) LUT to get two 16-bit floats. (The reason why we chose a 2D codebook here is that shared memory on NVIDIA GPUs is accessed in 32-bit-word elements, and each such word can contain two 16-bit floats.) Finally, it XORs bit 15 of \(X\) to flip the sign of the second entry of the codebook vector. Algorithm 3 can be implemented with MAD, bitshift, mask, and top3, giving an amortized 2 instructions per weight. This effectively assigns a \(L\) bit word to one of \(2^{Q+1}\) 2D vectors, each of which can be fine-tuned to improve quality. Algorithm 3 can also be implemented to XOR bit 31 alongside bit 15 (this is free in the lop3) to give an effectively \(2^{Q+2}\)-sized codebook, which can improve quantization quality. We only realized this after running all the experiments, so the numbers in this paper use the "one sign flip" version of Algorithm 3. In QTIP, we initialize the LUT using K-means on an empirical 2D i.i.d. Gaussian distribution.

``` input\(L\)-bit 0 left-padded integer \(x\), uint32\(a,b\). \(x(ax+b)\) mod \(2^{32}\) {run LCG to get uniform random \(x\)} {sum \(x\) as four 8-bit unsigned integers, this is approximately Gaussian} \(x(x\ \&\ 255)+((x\ >>\ 8)\ \&\ 255)+((x\ >>\ 16)\ \&\ 255)+((x\ >>\ 24)\ \&\ 255)\) \(x(x-510)/147.8\) output Pseudorandom approximate Gaussian \(x\). ```

**Algorithm 1** Computed Gaussian Code "1MAD"

### Tail-Biting Trellises

Directly quantizing a length-\(T\) sequence to a \((L,k,V)\) trellis results in a total of \(kT+L-kV\) bits since the starting state takes an additional \(L-kV\) bits to store. If we run inference on a machine with \(w\)-bit words where \(w|kT\), we must read an extra \( w-(L-kV)\) wasted bits per sequence. For common \(w\) (e.g. 32), setting \(L=kV+w\) makes the Viterbi algorithm intractable. One way to solve this is by enforcing that the start and end state share \(L-kV\) bits, i.e. the trellis is _tail-biting_. Exactly solving the tail-biting trellis problem via dynamic programming takes time quadratic in the state space (\(2^{L}\)), making this problem intractable for reasonable \(L 12\). However, since RHT-processed weights are approximately i.i.d., simple algorithms can be effective for approximately solving the tail-biting problem. We propose Algorithm 4, which first rotates the sequence by \(T/2\), quantizes it, and then extracts the overlap between the rotated start and end states. It then requantizes the original sequence with this overlap as the tail-biting overlap. This only requires two Viterbi calls

Figure 3: Set of representable neighboring values in a bitshift trellis with \(L=16\), \(k=2,V=1\) for (far left) a code with strong correlations, (left center) algorithm 1 (“1MAD”), (right center) algorithm 2 (“3INST”), and (far right) a random Gaussian code. Note that while 1MAD has minor correlations, both 1MAD and 3INST are close to a random Gaussian, resulting in good quantization quality.

in total. Table 2 shows that in practice, Algorithm 4 can find close-to-optimal tail-biting sequences while being significantly cheaper to run than other tail-biting approximation algorithms .

``` \(L\)-bit 0 left-padded integer \(x\), uint32 \(a,b\), float16 \(m\). \(x(ax+b)\) mod \(2^{32}\) {run LCG to get uniform random \(x\)} {modify sign, mantissa, and bottom 2 exponent bits of \(m\) and sum, this is approximately Gaussian} \(m(m,)\) << 16 + reinterpret(m, uint32) \(x(x\) & b1000111111111110001111111111111111) XOR \(m\) \(x(x\) & \(2^{16}-1,)+((x\) >> \(16)\) & \(2^{16}-1,)\) output Pseudorandom approximate Gaussian \(x\). ```

**Algorithm 2** Computed Gaussian Code "3INST"

## 4 Experiments

Here, we present experiments quantizing the Llama family of models with QTIP [33; 34; 26]. These models offer strong performance across a wide range of sizes, allowing us to compare how different quantization methods perform and scale. We primarily compare QTIP against QuIP\(\#\) and AQLM. For Llama 1, we include GPTVQ-2D instead of AQLM since AQLM does not publish Llama 1 numbers . GPTVQ-2D performs 2D VQ inside GPTQ and offers strong performance. These methods outperform scalar quantization methods including GPTQ, AWQ, and OmniQunat; comparisons to those methods can be found in QuIP\(\#\) and AQLM [20; 14; 29; 35; 11]. We mainly focus on the hybrid code (Section 4.2) since it is tailored for modern GPUs, and present a full suite of results for it. For the computed codes (Section 4.1), we present results for Llama 2.

Since the proxy error is not an additive distortion metric, we cannot minimize it by quantizing \(W\) as one sequence. Instead, for all experiments, we use QTIP as a quantizer in QuIP\(\#\)'s BlockLDLQ, which allows us to simultaneously achieve high dimensionality and low proxy error . Specifically, we quantize a block of \(T_{x} T_{y}\) weights as a sequence, where \(T_{x}\) and \(T_{y}\) span the output and input dimensions of \(W\), respectively. Since BlockLDLQ only specifies feedback along the input dimension, this is equivalent to BlockLDLQ with \(g=T_{y}\) but a vector dimension of \(T_{x}T_{y} T_{y}\). This has the benefit of limiting the effect of \(g\) in BlockLDLQ's error bound \(gm^{2}^{2}(H^{1/2})^{2}/n\) while achieving a high dimension for TCQ. Algorithm 5 in the Appendix describes this in more detail.

### Lookup-Free Computed Codes

Here, we use 1MAD and 3INST with \(L=16,V=1,T_{x}=T_{y}=16\). Setting \(T_{x}=T_{y}=16\) enables using a \(16 16\) MMA tile per trellis sequence to perform matrix multiplication during inference. \(16 16\) MMA tiles form the basis of many types of "AI hardware," making fast decoding relatively simple . We do not perform fine-tuning since the codes themselves are not tunable, but these codes are fully compatible with QuIP\(\#\)-style fine-tuning (recall that QuIP\(\#\)'s codebook is also not tunable). Table 3 shows that both 1MAD and 3INST significantly outperform QuIP\(\#\) without fine-tuning (AQLM does not have numbers without fine-tuning). Even at 4 bits, where all methods are close to lossless, QTIP results in significant improvements. Notably, the computed-code QTIP variants _without_ fine-tuning outperforms both QuIP\(\#\) and AQLM _with_ fine-tuning on almost all models and sizes, showing that fine-tuning is not a silver bullet.

Here, we use the hybrid lookup-computed code with \(L=16,V=2,T_{x}=T_{y}=16,Q=9\). Setting \(Q=9\) gives a 2KiB codebook, which fits in L1 cache _even after_ duplication for bank conflicts (32\(\)) on modern GPUs. This codebook is differentiable, so we can fine-tune it: to evaluate this, we fine-tune using QuIP\(\#\)'s methodology, tuning both the codebook entries and the as-yet-unquantized weights in a blockwise fashion. Table 5 shows the perplexity of quantized Llama 1 and 2 models. In all cases, QTIP outperforms the other vector quantization-based methods. Even at 3 and 4 bits, where QuIP\(\#\) and AQLM are close to lossless, QTIP roughly _halves_ the perplexity gap. These results also show the importance of dimensionality. Note that the 3- and 4-bit Llama 2 70B numbers here match those in 3. Since Table 3 uses a pure-computed code _without fine-tuning_, fine-tuning has no effect in these regimes and the improvement over QuIP\(\#\) is purely from dimensionality.

Table 6 shows zeroshot results computed with LM Eval, which are slightly random; QTIP generally matches or exceeds QuIP\(\#\) and AQLM on these tasks . Table 7 contains results on Llama 3. Like other works, we have observed that Llama 3 (especially 70B base) is harder to quantize than Llama 2 . Since the contribution and focus of this work is _what to round with_ (TCQ) and not _how to round_ (BlockLDLQ), we only compare against the proximal baseline QuIP\(\#\), which uses BlockLDLQ with VQ. QTIP significantly improves upon QuIP\(\#\) at all model sizes and bitrates, once again showing the dimensionality advantage of TCQ over VQ. Table 8 shows results for Llama 3.1 instruct-tuned models, including Llama 3.1 405B. At all sizes, QTIP achieves strong results. Notably, QTIP is able to match or exceed PV-Tuning, a recent quantization method that focuses on better fine-tuning algorithms . However, PV-Tuning is based off of AQLM and inherits its slow inference speed, making it significantly slower than QTIP. Finally, Table 9 shows results for quantizing Llama 3.2 instruct-tuned models to 4 bits. Since the embedding layers are very large relative to the decoder layers for small Llama 3 models (\( 500-750\)MB), quantizing the decoder layers to fewer than 4 bits does not make a significant difference on the final model size. Here, QTIP is still able to achieve a meaningful end-to-end compression rate (2.5-3X) without degrading the final model.

   Method & Bits & 2-7B Tok/s & 2-70B Tok/s \\  FP16 & 16 & 55.9 & OOM \\ AQLM & 2 & 81.5 & 8.78 \\ QuIP\# & 2 & 186 & 22.2 \\ QTIP & 2 & 188 & 23.5 \\ QTIP & 3 & 161 & 19.1 \\ QTIP & 4 & 140 & 16.3 \\   

Table 4: Batch size 1 decoding throughput on a RTX6000 Ada (960GB/s mem. BW).

  
**input** & Sequence \(S^{T}\), \((L,k,V)\) Trellis \(G\). \(S^{}\) \(\) Rotate \(S\) to the right by \( T/2\) \\ \(^{}\)\(\) Viterbi(\(S^{}\), \(G\)) \\ \(O\)\(\)\(L-kV\) bit overlap of \(^{}_{ T/2}^{}_{ T/2+1}\) \\ \(\)\(\) Viterbi(\(S\), \(G\)) with start/end overlap \(=O\) \\
**output** & Tail biting \(\) \\   

Table 2: Quantizing 4K \(T=256\) i.i.d Gaussian seqs. with a tail-biting \((12,k,1)\) trellis.

    &  &  &  &  &  &  \\   & & FP16 &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\   & W2 & 5.12 & **5.17** & **5.17** & 5.22 & 5.19 & 5.21 & **5.38** & 5.40 & 5.60 & 5.41 & 5.38 & 7.05 & **6.82** & 8.22 & 6.19 & 6.14 \\  & C4 & 6.63 & **6.71** & **6.71** & 6.79 & 6.75 & 6.75 & **6.99** & 7.01 & 7.34 & 7.04 & 7.01 & 9.14 & **8.96** & 11.0 & 8.16 & 8.09 \\   & W2 & 4.57 & **4.62** & **4.62** & 4.65 & 4.63 & 4.64 & **4.74** & **4.74** & 4.90 & 4.78 & 4.78 & 5.59 & **5.52** & 6.06 & 3.35 & 5.33 \\  & C4 & 6.05 & **6.10** & **6.10** & 6.15 & 6.13 & 6.14 & **6.28** & **6.28** & **6.50** & 6.35 & 6.33 & 7.46 & **7.39** & 8.07 & 7.20 & 7.19 \\    & W2 & 3.12 & **3.16** & **3.16** & 3.18 & 3.18 & 3.19 & **3.27** & **3.27** & 3.41 & 3.35 & 3.36 & **3.87** & 3.90 & 4.16 & 3.91 & 3.83 \\   & C4 & 4.97 & **5.00** & **5.00** & 5.02 & 5.02 & 5.03 & **5.09** & **5.09** & 5.20 & 5.15 & 5.17 & 5.70 & **5.69** & 6.01 & 5.71 & 5.62 \\   

Table 3: Wikitex2 and C4 perplexity (\(\)), ctx. 4096, QTIP with pure-computed codes. Even _without fine-tuning_, pure-computed QTIP outperforms QuIP\(\#\) and AQLM, both of which use fine-tuning, at almost all models sizes.

### Inference Speed

Table 4 shows the batch size 1 inference speed of QTIP, QuIP\(\#\), and AQLM on Llama 2 7B and 70B with matrix fusion. Here, the design choices of QTIP and QuIP\(\#\) become apparent. Whereas AQLM uses a codebook that is too large to fit in cache and thus prevents fast inference, both QTIP and QuIP\(\#\) achieve significant speedups over FP16. Furthermore, while it is impressive that both QuIP\(\#\) and QTIP are \(>2\) faster than AQLM, it is even more impressive that QTIP is able to match QuIP\(\#\)'s throughput with an effective dimension size of 256, or \(32\) larger than QuIP\(\#\)'s. This means that the improved quantization quality of QTIP comes with _no additional inference-time cost_. Although our empirical throughput numbers were timed on NVIDIA GPUs, QTIP can be fast on a broad class of accelerators due to its flexibility. QTIP only requires generating a pseudorandom Gaussian efficiently, and can work on devices with no cache as well as devices with lookup hardware. For example, if we were using a ARMv8 CPU, we could use the vqtb14q_u8 NEON intrinsic to look up 16 indices in a 64-entry codebook. This would let us use a 6 bit 1D codebook with the HYB code (Q=6, V=1). Quantizing Llama 2 7B to 2 bits with this setup and w/out fine-tuning gives 6.89 Wikitext2 perplexity - essentially the same state-of-the-art quality as 3INST.

## 5 Conclusion

We present QTIP, a weight-only post-training quantization algorithm that achieves state-of-the-art results through the use of trellis-coded quantization (TCQ). TCQ enables tractable ultra-high dimensional quantization, significantly reducing quantization distortion over vector quantization (VQ). However, naive TCQ does not admit fast inference due to sequential bottlenecks during decoding and needing to store a large codebook. QTIP solves this problem through a novel combination of incoherence processing, the hardware-efficient bitshift trellis, and fast computed codes. Specifically, QTIP introduces a series of compute-based pseudorandom Gaussian codes that, when used in

  &  &  \\   &  &  &  &  &  &  \\  FP16 & 16 & 51.1 & 77.7 & 81.1 & 77.0 & 16 & 45.6 & 73.3 & 73.5 & 69.6 & 16 & 40.0 & 69.3 & 78.5 & 67.3 \\ AQLM & 4.14 & 50.7 & 77.3 & 81.5 & 76.5 & 3.94 & **44.8** & 73.3 & 78.4 & **69.9** & 4.04 & 41.0 & 70.2 & 78.2 & 67.3 \\ QuIP\# & 4 & 50.5 & 77.7 & 81.4 & **77.3** & 4 & 43.6 & 71.3 & 78.7 & 69.6 & 4 & 40.4 & 68.6 & **78.5** & **67.4** \\ QIP & 4 & 50.0 & **77.8** & 81.3 & 76.9 & 4 & 44.8 & 73.6 & **78.9** & 69.9 & 4 & 40.0 & **68.9** & 78.4 & 67.1 \\ AQLM & 3.01 & 50.3 & 78.0 & 80.7 & 75.3 & 3.03 & 42.8 & 72.9 & 78.5 & 68.8 & 3.0 & 34.8 & 56.8 & 77.3 & 65.4 \\ QuIP\# & 3 & **50.9** & 77.6 & **81.4** & 76.1 & 3 & **44.0** & 72.5 & 78.4 & 69.1 & 3 & **39.2** & **68.4** & 77.3 & 66.5 \\ QTIP & 3 & 50.3 & **78.2** & 80.6 & 77.0 & 3 & **44.0** & 72.8 & 78.0 & **69.5** & 3 & 38.9 & 68.1 & **78.1** & **66.9** \\ AQLM & 2.07 & 47.9 & **77.7** & 80.4 & 75.9 & 1.97 & 38.8 & 69.3 & 75.9 & 68.8 & 2.02 & 32.8 & 63.7 & 74.8 & 65.7 \\ QuIP\# & 2 & 47.6 & 77.1 & 79.5 & 74.6 & 2 & 39.6 & 69.0 & **77.3** & 67.4 & 2 & 35.2 & 65.3 & 75.4 & **64.9** \\ QTIP & 2 & **48.0** & 76.3 & 80.2 & 75.1 & 2 & **41.4** & **70.8** & **77.3** & **67.6** & 2 & **35.7** & **65.6** & **75.9** & 64.7 \\  

Table 6: Zeroshot accuracy (\(\)), QTIP with the hybrid-computed code.

conjunction with the bitshift trellis and incoherence processing, simultaneously achieves state-of-the-art PTQ quality and fast inference. QTIP improves quantization quality at all tested bitrates over the latest VQ-based PTQ methods, QuIP\(\#\) and AQLM, further pushing the boundary of LLM PTQ. QTIP's codes use as few as 2 instructions per weight during decoding, enabling matrix-vector multiplication to run at over 80% of peak memory bandwidth on modern GPUs. Altogether, our results indicate that high dimensional quantization is necessary for high-quality compression, and QTIP is the first LLM PTQ method to scale to ultra-high dimensions while supporting fast inference.