# Free Lunch in Pathology Foundation Model:

Task-specific Model Adaptation with Concept-Guided Feature Enhancement

 Yanyan Huang

The University of Hong Kong

yanyanh@connect.hku.hk

&Weiqin Zhao

The University of Hong Kong

wqzhao98@connect.hku.hk

&Yihang Chen

The University of Hong Kong

yihangc@connect.hku.hk

&Yu Fu

Lanzhou University

fuyu@lzu.edu.cn

&Lequan Yu

The University of Hong Kong

lqyu@hku.hk

Corresponding Author.

###### Abstract

Whole slide image (WSI) analysis is gaining prominence within the medical imaging field. Recent advances in pathology foundation models have shown the potential to extract powerful feature representations from WSIs for downstream tasks. However, these foundation models are usually designed for general-purpose pathology image analysis and may not be optimal for specific downstream tasks or cancer types. In this work, we present _Concept_**An**_chor-guided_**T**_ask-specific Feature_**_E**_nhancement_ (CATE), an adaptable paradigm that can boost the expressivity and discriminativeness of pathology foundation models for specific downstream tasks. Based on a set of task-specific concepts derived from the pathology vision-language model with expert-designed prompts, we introduce two interconnected modules to dynamically calibrate the generic image features extracted by foundation models for certain tasks or cancer types. Specifically, we design a Concept-guided Information Bottleneck module to enhance task-relevant characteristics by maximizing the mutual information between image features and concept anchors while suppressing superfluous information. Moreover, a Concept-Feature Interference module is proposed to utilize the similarity between calibrated features and concept anchors to further generate discriminative task-specific features. The extensive experiments on public WSI datasets demonstrate that CATE significantly enhances the performance and generalizability of MIL models. Additionally, heatmap and umap visualization results also reveal the effectiveness and interpretability of CATE. The source code is available at https://github.com/HKU-MedAI/CATE.

## 1 Introduction

Multiple Instance Learning (MIL) [26; 34; 23; 2] is widely adopted for weakly supervised analysis in computational pathology, where the input of MIL is typically a set of patch features generated by a pre-trained feature extractor (_i.e._, image encoder). Although promising progress has been achieved, the effectiveness of MIL models heavily relies on the quality of the extracted features. A robust feature extractor can discern more distinctive pathological features, thereby improving the predictive capabilities of MIL models. Recently, several studies have explored using pretrained foundation models on large-scale pathology datasets with self-supervised learning as the feature extractors for WSI analysis [37; 7; 3; 36]. Additionally, drawing inspiration from the success of ContrastiveLanguage-Image Pretraining (CLIP) [30; 20] in bridging visual and linguistic modalities, some works have aimed to develop a pathology vision-language foundation model (VLM) to simultaneously learn representations of pathology images and their corresponding captions [15; 25]. The intrinsic consistency between the image feature space and caption embedding space in the pathology VLM enables the image encoder to extract more meaningful and discriminative features for downstream WSI analysis applications .

Although the development of these pathology foundation models has significantly advanced computational pathology, these models are designed for general-purpose pathology image analysis and may not be optimal for specific downstream tasks or cancer types, as the features extracted by the image encoder may contain generic yet task-irrelevant information that will harm the performance of specific downstream tasks. For example, as illustrated in Figure 1(a), the features extracted by the image encoder of a pathology VLM can include both task-relevant information (_e.g._, arrangement or morphology of tumor cells) and task-irrelevant elements(such as background information, stain styles, etc.). The latter information may act as "noise", distracting the learning process of MIL models tailored to specific tasks, and potentially impairing the generalization performance of these models across different data sources. Consequently, it is crucial to undertake task-specific adaptation to enhance feature extraction of generic foundation models and enable MIL models to concentrate on task-relevant information and thus improve analysis performance and generalization [32; 38].

In this paper, we propose a novel paradigm, named **C**oncept **A**nctor-guided **T**ask-specific **F**a**hancement (CATE), to enhance the generic features extracted by the pathology VLM for specific downstream tasks (_e.g._, cancer subtyping). Without requiring additional supervision or significant computational resources, CATE offers an approximately _"free lunch"_ in the context of pathology VLM. Specifically, we first derive a set of task-specific concept anchors from the pathology VLM with task-specific prompts, and these prompts rely on human expert design or are generated through querying large language models (LLMs), necessitating a certain level of pathological background knowledge. Based on these concept anchors, we design two concept-driven modules, _i.e._, the Concept-guided Information Bottleneck (CIB) module and the Concept-Feature Interference (CFI) module, to calibrate and generate task-specific features for downstream analysis. Particularly, with the task-specific concepts as the guidance, the CIB module enhances task-relevant features by maximizing the mutual information between the image features and the concept anchors and also eliminates task-irrelevant information by minimizing the superfluous information, as shown in Figure 1(a). Moreover, the CFI module further generates discriminative task-specific features by utilizing the similarities between the calibrated image features and concept anchors (_i.e._, concept scores). By incorporating the CATE into existing MIL frameworks, we not only obtain more discriminative features but also improve generalization regarding domain shift by eliminating task-irrelevant features and concentrating on pertinent information, as shown in Figure 1(b).

In summary, the main contributions of this work are threefold:

* We introduce a novel method, named **CATE**, for model adaptation in computational pathology. To the best of our knowledge, this is the first initiative to conduct _task-specific_ feature enhancement based on the pathology foundation model for MIL tasks.
* We design a new **CIB** module to enhance the task-relevant information and discard irrelevant information with the guidance of task-specific concepts, and a new **CFI** module to generate task-specific features by exploiting the similarities between image features and concept anchors.

Figure 1: (a) Illustration of the key idea of concept-guided information bottleneck to enhance the task-relevant information and discard the task-irrelevant information. (b) Task-specific model adaptation with CATE to enhance the generalization across different data sources.

* Extensive experiments on Whole Slide Image (WSI) analysis tasks demonstrate that CATE significantly enhances the performance and generalization capabilities of MIL models.

## 2 Related Work

**Multiple Instance Learning (MIL) for WSI Analysis.** MIL is the predominant paradigm for WSI analysis, treating each WSI as a bag of patch instances and classifying the entire WSI based on aggregated patch-level features. Attention-based methods [16; 26; 17; 39] are highly regarded for their ability to determine the significance of each instance within the bag. For instance, Ilse _et al._ introduced an attention-based MIL model, while Lu _et al._ proposed clustering-constrained-attention to refine this mechanism further. To model the relationships among instances, graph-based and Transformer-based methods have been developed [10; 2; 13]. For example, Chen _et al._ introduced a Transformer-based hierarchical network to capitalize on the inherent hierarchical structure of WSIs.

**Pathology Foundation Model.** With the advancement of foundation models in computer vision, several pathology foundation models have been developed to serve as robust image encoders for WSI analysis. Riasatian _et al._ proposed fine-tuning the DenseNet  on the TCGA dataset, while Filiot _et al._ utilized iBOT  to pretrain a vision Transformer using the Masked Image Modeling framework. Recently, Chen _et al._ pre-trained a general-purpose foundation model on large-scale pathology datasets using DINov2 , which has demonstrated strong and readily usable representations for WSI analysis. Inspired by CLIP , Ikezogwo _et al._, Huang _et al._, and Lu _et al._ developed vision-language foundation models by training on large-scale pathology datasets with image-caption pairs. These foundation models have demonstrated superior performance in downstream tasks due to their ability to extract more discriminative features for WSI analysis.

**Feature Enhancement in Computational Pathology.** Several methods have been developed to obtain more discriminative features for WSI analysis by adapting pathology foundation models [41; 24] or designing new plug-and-play modules . For instance, Zhang _et al._ suggested aligning the image features with text features extracted from a pre-trained natural language model to enhance the feature representation of WSI patch images, while it operates solely at the patch level, without considering the informational relationship between image and text features. Recently, Tang _et al._ introduced Re-embedded Regional Transformer for feature re-embedding, aimed at enhancing WSI analysis when integrated with existing MIL methods. However, while this method considers the spatial information of WSIs and adds flexibility to MIL models, it falls short in extracting task-specific discriminative information for WSI analysis.

## 3 Method

### Overview

The proposed CATE can be seamlessly integrated with any MIL framework to adapt the existing pathology foundation model (Pathology VLM) for performance-improved WSI analysis via task-specific enhancement. Specifically, consider a training set \(=\{(,)\}\) of WSI-label pairs, where \(=\{_{1},_{2},...,_{N}\}\) is a set of patch features with dimension of \(C\) (_i.e._, \(_{i}^{C}\)) extracted by the image encoder of pathology VLM, \(N\) denotes the number of patches, and \(\) is the corresponding label. The objective of CATE is to obtain the corresponding enhanced task-specific feature set \(\) from the original feature \(\) with the guidance of pre-extracted concepts anchors \(\) (see description below) for downstream usage:

\[=(,),}= ().\] (1)

As illustrated in Figure 2, we design two different modules to enhance the extracted features from foundation models: (1) Concept-guided Information Bottleneck (CIB) module calibrates original image features with the guidance of concept anchors with information bottleneck principle; and (2) Concept-Feature Interference (CFI) module generates discriminative task-specific features by leveraging the similarities between the calibrated image features and concept anchors. Specifically, the enhanced patch features can be represented as \(=\{_{1},_{2},...,_{N}\}\), where \(_{i}\) is the concatenation of the calibrated feature \(_{i}\) and the interference feature \(_{i}\) generated by CIB and CFI module:

\[_{i}=[_{i},_{i}]=[(_{i},),( _{i},)].\] (2)

**Concept Extraction.** We extract two kinds of task-specific concept anchors, \(=\{^{},^{}\}\), comprising class-specific concepts \(^{}=\{^{}_{i}\}_{i=1}^{m}\) (_e.g._, subtyping classes) and class-agnostic concepts \(^{}=\{^{}_{i}\}_{i=1}^{n}\) (_e.g._, adipose, connective, and normal tissues), with \(m\) and \(n\) representing the numbers of class-specific and class-agnostic concepts, respectively. These concepts are generated by the text encoder of pathology VLM with prompt \(\). Each prompt consists of a class name (_e.g._, "invasive ductal carcinoma") and a template (_e.g._, "An image of <_CLASSNAME_>"). To obtain more robust concepts, we use multiple prompts for each class and the final concept anchor is the average of the embeddings generated by different prompts. Details of class names and templates for various tasks are provided in Appendix G. Note that due to the inherent consistency between the image and text embedding space in VLM, these extracted concepts can also be regarded as image concept vectors.

### Concept-guided Information Bottleneck

The objective of this module is to find a distribution \(p(|)\) that maps the original image feature \(\) into a representation \(\), which contains enhanced task-discriminative characteristics and suppressed task-irrelevant information. WSIs typically contain various cell types or tissues (_e.g._, tumor cells, normal cells, adipose tissue, connective tissue), while only a subset of patches (_e.g._, with tumor cells) is crucial for certain tasks such as tumor subtyping. We thus define \(}=\{}_{i}\}_{i=1}^{k}\) as the representative subset of the original feature set (_e.g._, tumor tissue patches), where \(k\) denotes the number of representative patches. Note that this selection can be conducted with a simple comparison of image features with class-specific concepts (see discussion below). To this end, the corresponding enhanced feature set is \(}=\{}_{i}\}_{i=1}^{k} \) and we want to find the conditional distribution \(p(}_{i}|}_{i})\) to map the selected patch feature \(}_{i}^{C}\) into a more discriminative enhanced feature \(}_{i}^{C}\), which is discriminative enough to identify the label \(\).

**Sufficiency and Consistency Requirements.** To quantify the informativeness requirement of the calibrated feature \(}\), we consider the **sufficiency** of \(}\) for \(\). As defined in Appendix E.1, the encoded feature \(}\) derived from the original feature \(}\) is sufficient for determining the label \(\) if and only if the amount of task-specific information remains unchanged after calibration, _i.e._, \(I(};)=I(};)\). However, the label \(\) pertains to the slide level and specific labels cannot be assigned to each instance due to the absence of patch-level annotations.

To address this challenge, we propose using the task-specific concept anchor as the guidance for each single \(}\). Specifically, we posit that the concept anchor \(\) is distinguishable for the task and contains task-relevant information for label \(\). Given the consistency between image and text features in pathology VLM, any representation \(}\) containing all information accessible from both image feature \(}\) and concept \(\) will also encapsulate the discriminative information required for the

Figure 2: (a) Overview of CATE: the outputs of the CIB and CFI modules are concatenated to form the enhanced feature for downstream MIL models. (b) Task-relevant concept generation. (c) Concept-guided Information Bottleneck (CIB) module. (c) Concept-Feature Interference (CFI) module.

label. This **consistency** requirement is detailed in Appendix E.1. Thus, if \(}\) is sufficient for \(\) (_i.e._, \(I(};|})=0\)), then \(}\) is as predictive for label \(\) as the joint of original feature \(}\) and concept anchor \(\). Applying the chain rule of mutual information, we derive:

\[I(};})=};)}_ {}+};}| )}_{}.\] (3)

According to the consistency between the concept anchor and original feature, the mutual information term \(I(};)\) represents the predictive information for the task, while the conditional information term \(I(};}|)\) denotes task-irrelevant information (_i.e._, superfluous information) in original patch feature \(}\), which can be minimized to enhance the robustness and generalization ability of downstream MIL models. As a result, the main objective of the feature calibration in this module can be formalized as _maximize predictive information_\(I(};)\) while _minimize the superfluous information_\(I(};}|)\).

**Predictive Information Maximization (PIM).** The predictive information in Equ (3) equals to the mutual information between the calibrated feature and concept anchors. To maximize this, we choose the InfoNCE  to estimate the lower bound of the mutual information, which can be obtained by comparing positive pairs sampled from the joint distribution \(},_{}^{} p(}, ^{})\) to pairs \(},_{j}^{}\) and \(},_{j}^{}\) built using a set of negative class concepts \(_{j}^{} p(^{})\) and class-agnostic concepts \(_{j}^{} p(^{})\):

\[I_{NCE}(};)=}_{},_{}^{}}[_{i=1}^{k}_{}^{},}_{i})}{_{j= 1}^{m}f(_{j}^{},}_{i})+_{j=1}^{ n}f(_{j}^{},}_{i})}].\] (4)

We set \(f(,}_{i})=(}_{i} ^{}/)\) with \(>0\) in practice following . By maximizing this mutual information lower bound, \(f(,}_{i})\) will be proportional to the density ratio \(p(,}_{i})/p()p( }_{i})\) as proved in . Hence, \(f(,}_{i})\) preserves the mutual information between the calibrated feature and concept anchor. The detailed derivation can be found in Appendix E.2. The loss function for PIM can be denoted as:

\[_{PIM}=}_{},_{}^ {}}[-_{i=1}^{k}}_{i}^{}_{}^{}}{}]+}_{},_{}^{}}[_{i=1}^{k}(_{j=1}^{ m}}_{i}^{}_{j}^{}}{}+ _{j=1}^{n}}_{i}^{}_{j}^{}}{})].\] (5)

**Superfluous Information Minimization (SIM).** To compress task-irrelevant information, we aim to minimize the superfluous information term as defined in Equ (3). This objective can be achieved by minimizing the mutual information \(I(};})\). In practice, we conduct SIM for _all patches_ in the subset \(\), as each patch may contain task-irrelevant information. Following , it can be represented as:

\[I(;)= p(,) p(|)dd- p() p ()d.\] (6)

After that, we let the distribution of \(\): \(r()\) (_e.g._, Gaussian distribution in this work), be a variational approximation to the marginal distribution \(p()\), and we can obtain the upper bound for \(I(;)\):

\[I(;) p()p(| )|)}{r()}d d.\] (7)

Furthermore, we use a variational distribution \(q_{}(|)\) with parameter \(\) to approximate \(p(|)\) and we implement the parameterization of the variational distribution with MLP by predicting the mean and variance of the Gaussian distribution and sample the calibrated feature \(\) from this distribution:

\[(MLP^{}(),MLP^{} ()).\] (8)

In practice, we implement this by utilizing the reparameterization trick  to obtain an unbiased estimate of the gradient and further optimize the variational distribution. The detailed derivation can be found in Appendix E.3. The minimization of the upper bound of \(I(;)\) equals to the minimization of the Kullback-Leibler divergence between \(q_{}(|)\) and \(r()\). Therefore, the loss function can be represented as:

\[_{SIM}=[_{i=1}^{k}D_{KL}(q_{}( _{i}|_{i})||r(_{i}))].\] (9)

**Discussion.** We further provide explanation of CIB module with the information plane [8; 6] in Appendix F. It should be noted that the PIM supervises only the representative subset \(}\) containing task-relevant information (selected by the similarity between image features and corresponding class-specific concepts). Meanwhile, the SIM is applied to all patches in \(\), as any patch may carry information irrelevant to the task (_e.g._, background information and stain styles). Besides, SIM cannot be directly optimized without the guidance of concept anchors (_i.e._, PIM) due to the absence of patch-level labels. As demonstrated in the ablation study in Section 4.4, the absence of concept anchor guidance leads to the collapse of discriminative information in the calibrated feature, adversely affecting downstream task performance. By maximizing predictive information and minimizing superfluous details, the CIB module effectively enhances the discriminative capacity of the original features and aligns them with the task-specific concept anchors for improved prediction.

### Concept-Feature Interference

We also propose the Concept-Feature Interference (CFI) module to utilize the similarity characteristic between calibrated features and concept anchors to further obtain robust and discriminative information for the downstream tasks. Our primary focus is on the class-specific concept anchors \(^{}\). Specifically, for each CIB encoded feature \(_{i}\), we calculate the cosine similarity between \(_{i}\) and each class-specific concept \(^{}_{i}\). It is important to note that the number of class-specific concepts \(m\) is larger than the number of classes, as we use multiple <_CLASSNAME_> and templates to generate the concept anchor for each class, as shown in the Appendix G. Thus, we can obtain the similarity vector by concatenating the similarity scores between \(_{i}\) and each class-specific concept \(^{}_{i}\). To integrate the interference information (similarity relationship) into the enhanced feature, we align the similarity vector with the calibrated feature \(_{i}\) using a Self-Normalizing Network (SNN) layer . This allows us to obtain the final interference vector \(_{i}\) of CFI:

\[_{i}=([\{ (_{i},^{}_{i})\}_{i=1}^{ m}]).\] (10)

The interference vector contains superficial information that indicates the similarity between the calibrated feature and concept anchor directly. This is completely different from the calibrated feature of the CIB module, which contains discriminative latent information for the downstream tasks. Therefore, integrating the interference feature can further provide robust and discriminative information for the downstream tasks.

**Discussion.** The CFI module is designed to utilize the similarity characteristic between calibrated feature and concept anchor as a discriminative feature, which can be further integrated into the calibrated feature for downstream tasks. This is different from other studies that directly compare the similarity between visual features and textual concept features of different classes to perform zero-shot classification .

### Training Objective

The overall training objective of the CATE framework can be represented as the combination of the cross entropy loss \(_{CE}\) for the downstream tasks, the predictive information maximization loss \(_{PIM}\), and the superfluous information minimization loss \(_{SIM}\):

\[=_{CE}+_{P}_{PIM}+_{S}_{SIM},\] (11)

where \(_{P}\) and \(_{S}\) are hyperparameters and influence of them is discussed in Appendix C.

## 4 Experiments

### Experimental Settings

**Tasks and Datasets.** We conducted cancer subtyping tasks on three public WSI datasets from The Cancer Genome Atlas (TCGA) project: Invasive Breast Carcinoma (BRCA), Non-Small Cell Lung Cancer (NSCLC), and Renal Cell Carcinoma (RCC). Detailed dataset information is available in Appendix D.

[MISSING_PAGE_FAIL:7]

TransMIL , DTFD-MIL , and R\({}^{2}\)T-MIL . The R\({}^{2}\)T-MIL  is a feature re-embedding method that utilizes ABMIL as the base MIL model.

**Implementation Details.** This study begins the image feature extraction process by segmenting the foreground tissue and then splitting the WSI into 512\(\)512 pixels patches at 20\(\) magnification. Subsequently, these patches are processed through a pre-trained image encoder from CONCH  to extract image features. For concept anchors, we utilize CONCH's text encoder to derive task-relevant concepts from predefined text prompts, with detailed prompt information available in Appendix G. Model parameters are optimized using the Adam optimizer with a learning rate of \(10^{-5}\). The batch size is set to 1, and all the experiments are conducted on a single NVIDIA RTX 3090 GPU.

### Experimental Results

**Quantitative Results on BRCA Dataset.** To fully evaluate the effectiveness of CATE, we assessed its impact on several state-of-the-art MIL models using the BRCA dataset. The results are shown in Table 1, where the MIL models integrated with CATE outperform their original counterparts in both in-domain (IND) and out-of-domain (OOD) testing, which demonstrates the effectiveness and generalization capabilities of CATE. Comparing with R\({}^{2}\)T-MIL, which is a feature re-embedding method that utilizes ABMIL as the base MIL model, CATE incorporated with ABMIL consistently achieves better performance in terms of both OOD and IND testing. To further investigate the effectiveness of CATE, we conducted experiments by altering the in-domain sites and applying traditional settings. Detailed results are available in Appendix B.

**Qualitative Analysis.** To qualitatively investigate the effectiveness of CATE, we visualized attention heatmaps, UMAP, and the similarities between original features and corresponding class concept features, as well as calibrated features and class concept features, as shown in Figure 3. Additional visualization results are provided in Appendix H. As shown in Figure 3 (a&b), attention heatmap comparisons reveal that CATE-MIL focuses more intensely on cancerous regions, with a clearer delineation between high and low attention areas. By comparing the similarities of original and calibrated features to class concept features in Figure 3 (c&d), it is evident that the enhanced similarity in cancerous regions is significantly higher than in original features. Moreover, the disparity between cancerous and non-cancerous regions' similarities is also expanded, which further verifies the ability of CATE to enhance task-relevant information and suppress irrelevant information. We further performed a UMAP visualization of class concept features, original features, and calibrated features. As depicted in Figure 3 (f), calibrated features are notably closer to the corresponding class (IDC)

    & }\)=2)} & }\)=4)} \\   & **OOD-AUC** & **OOD-ACC** & IND-AUC\({}^{}\) & IND-ACC\({}^{}\) & **OOD-AUC** & **OOD-ACC** & IND-AUC\({}^{}\) & IND-ACC\({}^{}\) \\  ABMIL & 0.874\(\)0.021 & 0.803\(\)0.021 & 0.997\(\)0.004 & 0.954\(\)0.028 & 0.951\(\)0.023 & 0.883\(\)0.029 & 0.974\(\)0.018 & 0.910\(\)0.006 \\ CLAM & 0.875\(\)0.029 & 0.801\(\)0.021 & 0.997\(\)0.007 & 0.963\(\)0.024 & 0.931\(\)0.037 & 0.870\(\)0.006 & 0.977\(\)0.023 & 0.926\(\)0.048 \\ DSMIL & 0.839\(\)0.046 & 0.764\(\)0.031 & 0.993\(\)0.004 & 0.963\(\)0.028 & 0.934\(\)0.019 & 0.864\(\)0.026 & 0.974\(\)0.013 & 0.913\(\)0.042 \\ DTFD-MIL & 0.903\(\)0.023 & 0.836\(\)0.026 & 0.990\(\)0.009 & 0.958\(\)0.049 & 0.949\(\)0.010 & 0.893\(\)0.012 & 0.981\(\)0.012 & 0.918\(\)0.040 \\ TransMIL & 0.790\(\)0.028 & 0.712\(\)0.024 & 0.997\(\)0.004 & 0.954\(\)0.033 & 0.917\(\)0.022 & 0.832\(\)0.031 & 0.977\(\)0.041 & 0.923\(\)0.029 \\ R\({}^{2}\)T-MIL & 0.739\(\)0.088 & 0.690\(\)0.025 & 0.999\(\)0.002 & 0.971\(\)0.036 & 0.892\(\)0.041 & 0.800\(\)0.059 & 0.977\(\)0.018 & 0.916\(\)0.045 \\ 
**CATE-MIL** & **0.945\(\)**0.016 & **0.840\(\)**0.033 & 0.985\(\)**0.011 & 0.938\(\)0.037 & **0.969\(\)**0.003 & **0.906\(\)**0.011 & 0.967\(\)**0.019 & 0.905\(\)0.054 \\   & }\)=3)} & }\)=6)} \\   & **OOD-AUC** & **OOD-ACC** & IND-AUC\({}^{}\) & IND-ACC\({}^{}\) & **OOD-AUC** & **OOD-ACC** & IND-AUC\({}^{}\) & IND-ACC\({}^{}\) \\  ABMIL & 0.973\(\)0.005 & 0.891\(\)0.017 & 0.997\(\)0.004 & 0.961\(\)0.032 & 0.971\(\)0.007 & 0.885\(\)0.010 & 0.973\(\)0.010 & 0.897\(\)0.023 \\ CLAM & 0.972\(\)0.004 & 0.893\(\)0.012 & 0.991\(\)0.005 & 0.961\(\)0.032 & 0.969\(\)0.009 & 0.888\(\)0.015 & 0.975\(\)0.011 & 0.896\(\)0.031 \\ DSMIL & 0.977\(\)0.002 & 0.893\(\)0.010 & 0.996\(\)0.006 & 0.965\(\)0.026 & 0.969\(\)0.008 & 0.883\(\)0.016 & 0.980\(\)0.012 & 0.901\(\)0.022 \\ DTFD-MIL & 0.975\(\)0.003 & 0.897\(\)0.012 & 0.996\(\)0.004 & 0.943\(\)0.046 & 0.971\(\)0.007 & 0.893\(\)0.017 & 0.974\(\)0.012 & 0.878\(\)0.022 \\ TransMIL & 0.961\(\)0.001 & 0.864\(\)0.022 & 0.994\(\)0.004 & 0.930\(\)0.030 & 0.947\(\)0.017 & 0.828\(\)0.037 & 0.975\(\)0.013 & 0.894\(\)0.027 \\ R\({}^{2}\)T-MIL \({}^{}\) & 0.956\(\)0.008 & 0.847\(\)0.022 & 0.991\(\)0.008 & 0.936\(\)0.030 & 0.932\(\)0.020 & 0.803\(\)0.048 & 0.974\(\)0.012 & 0.897\(\)0.029 \\ 
**CATE-MIL** & **0.983\(\)**0.002 & **0.911\(\)**0.018 & 0.989\(\)**0.009 & 0.944\(\)0.031 & **0.979\(\)**0.007 & **0.905\(\)**0.017 & 0.963\(\)**0.011 & 0.882\(\)0.032 \\   

* The best results are highlighted in **bold**, and the second-best results are underlined.
* \({}^{}\) R\({}^{2}\)T-MIL is designed for feature re-embedding that utilize ABMIL as base MIL model.
* The in-domain performance of NSCLC and RCC does not represent the true ability of the models, as each site contains only samples from one cancer subtype. We primarily focus on the **OOD performance** for these two datasets.

Table 2: Cancer Subtyping Results on NSCLC and RCC.

concept features compared to the original features, which demonstrates CATE's ability to effectively align features with task-relevant concepts and enhance task-relevant information.

### Results on Additional Datasets

For clarity and to highlight the superiority of ABMIL when enhanced with CATE, we developed **CATE-MIL** by incorporating CATE into ABMIL and compared it against other leading MIL models on NSCLC and RCC datasets. The comparative results in Table 2 confirm that CATE-MIL consistently outperforms other models in both OOD and IND performance. However, it is noted that CATE-MIL performs poorly on the in-domain testing data for NSCLC and RCC. This underperformance may be attributed to the elimination of task-irrelevant information, including site-specific patterns, by CATE, potentially degrading performance on in-domain data for these datasets. Consequently, OOD performance more accurately reflects the discriminative and generalization capabilities of the models.

### Ablation Analysis

We conduct ablation studies to assess the effectiveness of each component within CATE, and the results are shown in Table 3. Initially, incorporating Predictive Information Maximization (PIM) enables ABMIL to achieve improved performance in most experiments, which demonstrates PIM's efficacy in extracting task-relevant information. However, using Superfluous Information Minimization (SIM) alone results in performance degradation across most experiments, which suggests that SIM may discard some task-relevant information without guidance from a task-relevant concept anchor. Incorporating both PIM and SIM consistently enhances ABMIL's performance in all experiments, which further verifies that their combination effectively boosts the generalization capabilities of MIL models. We also conduct experiments by only using the interference features in CFI as the input of ABMIL, and the results show that the interference features are also informative for WSI classification tasks. More ablation analysis about the weights of PIM and SIM in CIB module the number of representative patches are in Appendix C.

## 5 Conclusion and Discussion

In this paper, we introduce CATE, a new approach that offers a _"free lunch"_ for task-specific adaptation of pathology VLM by leveraging the inherent consistency between image and text modalities.

Figure 3: (a) Attention heatmap of CATE-MIL. (b) Attention heatmap of the original ABMIL. (c) similarity between the calibrated features and the corresponding class concept feature. (d) similarity between the original features and the corresponding class concept feature. (e) Original WSI. (f) UMAP visualization of class concept features, original features, and enhanced features.

CATE shows the potential to enhance the generic features extracted by pathology VLM for specific downstream tasks, using task-specific concept anchors as guidance. The proposed CIB module calibrates the image features by enhancing task-relevant information while suppressing task-irrelevant information, while the CFI module obtains the interference vector for each patch to generate discriminative task-specific features. Extensive experiments on WSI datasets demonstrate the effectiveness of CATE in improving the performance and generalizability of state-of-the-art MIL methods.

**Limitations and Social Impact.** The proposed CATE offers a promising solution to customize the pathology VLM for specific tasks, significantly improving the performance and applicability of MIL methods in WSI analysis. However, the performance of CATE heavily depends on the quality of the concept anchors, which, in turn, relies on domain knowledge and the quality of the pre-trained pathology VLM. Additionally, while CATE is optimized for classification tasks such as cancer subtyping, it may not be readily applicable to other analytical tasks, such as survival prediction. However, there might be a potential solution to address this challenge. For instance, we could leverage LLMs or retrieval-based LLMs to generate descriptive prompts about the general morphological appearance of WSIs for specific cancer types. By asking targeted questions, we can summarize reliable and general morphological descriptions associated with different survival outcomes or biomarker expressions and further verify these prompts with pathologists. Moreover, since medical data may contain sensitive information, ensuring the privacy and security of such data is crucial.

## 6 Acknowledgements

This work was supported in part by the Research Grants Council of Hong Kong (27206123 and T45-401/22-N), in part by the Hong Kong Innovation and Technology Fund (ITS/274/22), in part by the National Natural Science Foundation of China (No. 62201483), and in part by Guangdong Natural Science Fund (No. 2024A1515011875).