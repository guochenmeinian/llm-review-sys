# Nonparametric Identifiability of Causal Representations from Unknown Interventions

Julius von Kuglegen\({}^{1,2}\)

Michel Besserve\({}^{1}\)

Liang Wendong\({}^{1}\)

Luigi Gresele\({}^{1}\)

Armin Kekic\({}^{1}\)

Elias Bareinboim\({}^{3}\)

David M. Blei\({}^{3}\)

Bernhard Scholkopf\({}^{1}\)

###### Abstract

We study causal representation learning, the task of inferring latent causal variables and their causal relations from high-dimensional functions ("mixtures") of the variables. Prior work relies on weak supervision, in the form of counterfactual pre- and post-intervention views or temporal structure; places restrictive assumptions, such as linearity, on the mixing function or latent causal model; or requires partial knowledge of the generative process, such as the causal graph or intervention targets. We instead consider the general setting in which both the causal model and the mixing function are _nonparametric_. The learning signal takes the form of multiple datasets, or environments, arising from _unknown interventions_ in the underlying causal model. Our goal is to identify both the ground truth latents and their causal graph up to a set of ambiguities which we show to be irresolvable from interventional data. We study the fundamental setting of two causal variables and prove that the observational distribution and one perfect intervention per node suffice for identifiability, subject to a genericity condition. This condition rules out spurious solutions that involve fine-tuning of the intervened and observational distributions, mirroring similar conditions for nonlinear cause-effect inference. For an arbitrary number of variables, we show that at least one pair of distinct perfect interventional domains per node guarantees identifiability. Further, we demonstrate that the strengths of causal influences among the latent variables are preserved by all equivalent solutions, rendering the inferred representation appropriate for drawing causal conclusions from new data. Our study provides the first identifiability results for the general nonparametric setting with unknown interventions, and elucidates what is possible and impossible for causal representation learning without more direct supervision.

## 1 Introduction

Causal representation learning (CRL) seeks to describe high-dimensional, low-level observations through a small number of interpretable, causally-related latent variables . In doing so, its goal is to combine the strengths of classical causal inference with those of modern machine learning. A causal model represents an entire family of distributions arising from _interventions_ on a system of variables . This provides a principled way for reasoning about distribution shifts, which facilitates out-of-distribution generalization and planning . However, causal models require that most (or at least some) relevant causal variables are directly observed. While reasonable in domains such as economics , social  or biomedical science , this assumption has challenged the application of causal methodology to complex and high-dimensional data . Machine learning, on the other hand, has proven successful at learning useful "representations"--latent vectors generating the observables via some nonlinear map--of high-dimensional data such as images, video, or text . However, most methods rely on independent and identically distributed (i.i.d.) data and only extract _associated_ information. As a consequence, they often fail under distribution shifts and do not generalize beyond the training distribution , as exhibited by their reliance on _spurious correlations_ or their vulnerability to _adversarial examples_.

**Identifiability.** It has been argued that to address these shortcomings, we ought to learn representations endowed with causal model semantics. However, a major challenge to this goal is that _different representations can explain the same data equally well_. Strictly simpler representation learning tasks, such as disentanglement  or independent component analysis (ICA) , are already _non-identifiable_ in general . Identifiability studies are thus required to characterize additional assumptions under which the desired latent variables can be provably recovered . In CRL, we need not only identify the latents, but also the causal graph encoding their relations. Even in the fully observed case, this task of causal discovery, or structure learning, is very challenging: the graph can only be recovered up to Markov equivalence  based only on (observational) i.i.d. data , meaning that the direction of some edges cannot be determined. For CRL, the task gets strictly harder. For instance, if the causal latents \(V_{i}\) in Fig. 1 (_Left_) form a valid representation, then replacing them by the _independent_ exogeneous \(U_{i}\) might be considered an equally valid alternative.

**What sets _causal_ representations apart?** A crucial feature of causal variables is that they are the ones on which _interventions_ are defined and whose relations we are interested in . Causal discovery and CRL thus often rely on non-i.i.d. data linked to interventions on the underlying causal variables . Unless all variables are subject to intervention, however, some fundamental differences between the fully observed and representation learning settings in the level of ambiguity in the graph remain , as illustrated in Fig. 1 (_Right_). In a sense, the non-identifiabilities of representation and structure learning combine, and both need to be addressed in conjunction.

**Problem Setting.** We study the general _nonparametric_ CRL problem (SS 2.1) in which both the causal mechanisms and the mixing function are completely unconstrained. Our goal (SS 2.2) is to identify the latent causal variables up to element-wise nonlinear rescaling and their graph up to isomorphism (Defn. 2.6). As motivated above, doing so without further supervision requires access to interventional data. To this end, we consider learning from heterogeneous data from multiple related domains, or _environments_, that arise from interventions in a shared underlying causal model (SS 2.3).

**Contributions.** Our main contributions are theoretical in nature (SS 3). First, we establish the minimality of the targeted equivalence class (Prop. 3.1) in the sense that its ambiguities cannot be resolved from interventional data. We then present our main identifiability results. For the case of two latent causal variables, we show that an observational environment and one for each perfect intervention on either variable suffice (Thm. 3.2)--provided that the intervened and unintervened mechanisms are not "fine-tuned" to each other, which we formalize in the form of a _genericity condition_ (3.2). For any number of latent causal variables, we prove that access to pairs of environments corresponding to two distinct perfect interventions on each node guarantees identifiability (Thm. 3.4). We then question how to use or interpret causal representations (SS 4), and show that certain quantities, such as the strengths of causal influences among variables, are preserved by all equivalent solutions (Thm. 4.2). We sketch possible learning objectives (SS 5), and empirically investigate training different generative models (SS 6), finding that only those based on the correct causal structure attain the best fit and identify the ground truth. We conclude by discussing limitations and extensions of our work (SS 7).

Figure 1: _(Left)_ **Data-Generating Process for Causal Representation Learning (CRL).** Observations \(\) are generated by applying a nonlinear mixing function \(f\) to a set of causal latent variables \(=\{V_{1},...,V_{n}\}\), which are related through a structural causal model (SCM) with independent exogenous variables \(=\{U_{1},...,U_{n}\}\); illustration by Ana Martin Larraafaga. _(Right)_**Comparison to Structure Learning.** The causal direction between two unconfounded _observed_ variables (top) is uniquely identified from a single intervention ; for CRL (bottom), this is not the case, as the nonlinear mixing introduces additional ambiguity due to spurious representations. Shaded nodes are observed, white ones unobserved, and interventions highlighted as red diamonds.

Related Work on _Multi-Environment_ CRL.Most closely related are recent identifiability studies which also leverage multiple environments arising from single node interventions [5; 22; 117; 122], thus mirroring in different interventional setups the result of Brehmer et al.  based on counterfactual, multi-view data, which is harder to obtain. Squires et al.  provide results for linear causal models and linear mixing; Ahuja et al.  consider nonlinear causal models and polynomial mixings, subject to additional constraints on the latent support ; and Varici et al.  employ a score-based approach for nonlinear causal models and linear mixing. The concurrent work of Buchholz et al.  extends the results of Squires et al.  to general nonlinear mixings and linear Gaussian causal models. Liu et al.  leverage recent advances in nonlinear ICA [55; 65] to identify a linear Gaussian causal model with context-dependent weights and nonlinear mixing from sufficiently diverse environments. A more extensive discussion of other related works and a detailed comparison of existing results with the present study are provided in Appx. A and Tab. 1.

## 2 Nonparametric Causal Representation Learning

In this section, we describe the considered problem setting and state our main assumptions. First, we specify the assumed data generating process (SS 2.1) and learning task in the form of a target identifiability class (SS 2.2). We then demonstrate the hardness of our task from i.i.d. data or imperfect interventions and use this to motivate a multi-environment approach with perfect interventions (SS 2.3).

**Notation.** We use upper-case \(X\) for random variables and lower-case \(x\) for their realizations. Bold uppercase \(\) denotes random vectors and lowercase \(\) their realizations. We assume throughout that all distributions \(P_{X}\) possess densities \(p(x)\) with respect to (w.r.t.) the Lebesgue measure. We denote the pushforward of \(P_{X}\) through a measurable function \(f\) by \(f_{*}(P_{X})\), and write \([n]=\{1,...,n\}\).

### Data Generating Process

The assumed data generating process consists of a latent causal model and a mixing function, see Fig. 1 (_left_). For the former, we build on the structural causal model (SCM) framework [95; 100].

**Definition 2.1** (Latent SCM).: Let \(=\{V_{1},...,V_{n}\}\) denote a set of causal "endogenous" variables, with each \(V_{i}\) taking values in \(\), and let \(=\{U_{1},...,U_{n}\}\) denote a set of mutually independent "exogenous" random variables. The latent SCM consists of a set of structural equations

\[\{V_{i}:=f_{i}(_{(i)},U_{i})\}_{i=1}^{n}.\] (2.1)

where \(_{(i)}\{V_{i}\}\) are the direct causes, or causal parents, of \(V_{i}\), and \(f_{i}\) are deterministic functions; and a fully factorized joint distribution \(P_{U}\) over the exogenous variables. The associated causal diagram \(G\), a directed graph with vertices \(\) and edges \(V_{j} V_{i}\) iff. \(V_{j}_{(i)}\), is assumed acyclic.

By acyclicity, recursive substitution of the assignments in (2.1) yields the reduced form \(f_{}()\). The SCM thus induces a unique distribution \(P_{}\) over the endogenous variables, given by the pushforward of \(P_{}\) via (2.1), that is, \(P_{}=f_{^{*}}(P_{})\). By construction, \(P_{}\) is Markovian w.r.t. the causal graph \(G\)[95; 100], meaning that its density obeys the causal Markov factorization:

\[p(v_{1},,v_{n})=_{i=1}^{n}p_{i}(v_{i}_{(i)}).\] (2.2)

We place the following additional assumption on the distribution \(P_{}\) induced by the SCM.

**Assumption 2.2** (Faithfulness).: The _only_ conditional independence relations satisfied by \(P_{}\) are those implied by \(\{V_{i}_{(i)}_{(i)}\}_{i[n]}\), where \(_{(i)}\) denotes the non-descendants of \(V_{i}\) in \(G\).

Asm. 2.2 ensures a one-to-one correspondence between (conditional) independence in \(P_{}\) and graphical separation in \(G\) and is a standard assumption in causal discovery . Faithfulness rules out cancellations of influences along different paths, which occurs with probability zero for random path-coefficients . It can thus also be viewed as a minimality or genericity assumption.

In contrast to classical causal inference, we assume that both the exogenous variables \(\) and the endogenous causal variables \(\) are unobserved. Instead, we will only have access to \(d\)-dimensional nonlinear mixtures \(\) of \(\). We therefore make the following additional assumption.

**Assumption 2.3** (Known \(n\)).: The number \(n\) of latent causal variables is known.

Next, we specify the relationship between the unobserved causal variables \(\) and the observed \(\).

**Definition 2.4** (Mixing function).: The observations \(\) are deterministically generated from \(\) by applying a mixing function \(f:^{n}^{d}\) to \(\), that is, \(:=f()\).

The terminology and setting of a deterministic mixing is rooted in the nonlinear ICA literature . In deep generative models, it is commonly relaxed by considering additive noise in Defn. 2.4[65; 90]. For representation learning scenarios, we are particularly interested in the case \(n d\). To allow for recovery of \(\) from \(\), we assume that \(f\) is invertible, which is a standard assumption for identifiability.

**Assumption 2.5** (Diffeomorphic mixing).: \(f\) is a diffeomorphism1 onto its image \((f)=^{d}\).

### Learning Target: The CRL Identifiability Class \(_{}\)

Our goal is to infer the underlying latent causal variables \(=f^{-1}()\) and their causal relations. We therefore consider the true unmixing function \(f^{-1}\)_and_ the causal graph \(G\) our joint learning target: \(f^{-1}\) informs us how to map observations \(\) to causal variables \(\), and \(G\) tells us how to factorise the implied joint \(p()\) into the causal mechanisms \(p_{i}(v_{i}_{(i)})\) from (2.2). Given only observations of \(\), this is a challenging task since neither \(\) nor \(G\) are directly observed or known a priori.

When is a candidate solution \((h,G^{})\) that satisfies a given learning objective (such as maximizing the likelihood, possibly subject to additional constraints) guaranteed to match the ground truth \((f^{-1},G)\)? This is the subject of identifiability studies and the main focus of our work. A statistical model \(=\{p_{}:\}\) with parameter space \(\) is identifiable if the mapping \( p_{}\) is injective . For representation learning tasks, full identifiability is often not attainable, as there are some fundamental ambiguities that cannot be resolved. One therefore typically instead considers identifiability up to an appropriately chosen equivalence class in the model space [2; 65; 127].

For the assumed data generating process (SS 2.1), the order of the causal variables is arbitrary, since \(\) is unobserved. We can therefore assume without loss of generality (w.l.o.g.) that the \(V_{i}\)'s are partially ordered w.r.t. \(G\), that is, \(V_{i} V_{j} i<j\).2 Learning \(G\) thus reduces to inferring whether the edges \(\{V_{1},,V_{i-1}\} V_{i}\) exist for \(i=2,,n\). The only remaining permutation ambiguity arises from permutations \(\) that preserve the partial order: for example, if \(G\) is given by \(V_{1} V_{3} V_{2}\), the order of \(V_{1}\) and \(V_{2}\) cannot be uniquely determined without further assumptions. Moreover, the scaling of the causal variables is also arbitrary: any invertible element-wise transformation can be undone as part of \(f\). We therefore define the desired identifiability class through the following equivalence relation.3

**Definition 2.6** (\(_{}\)-identifiability).: Let \(\) be a space of unmixing functions \(h:^{n}\) and let \(\) be the space of DAGs over \(n\) vertices. Let \(_{}\) be the equivalence relation on \(\) defined as

\[(h_{1},G_{1})_{}(h_{2},G_{2})(h_{2},G_{2})=( {P}_{^{-1}} h_{1},(G_{1}))\] (2.3)

for some element-wise diffeomorphism \(()=(_{1}(v_{1}),,_{n}(v_{n}))\) of \(^{n}\) and a permutation \(\) of \([n]\) such that \(:G_{1} G_{2}\) is a graph isomorphism and \(_{}\) the corresponding permutation matrix.

_Remark 2.7_.: When \(G\) has no edges, any permutation is admissible and \(_{}\) reduces to the standard notion of identifiability up to permutation and element-wise reparametrisation of nonlinear ICA .

The ground truth \((f^{-1},G)\) is identified up to \(_{}\) by a given learning objective if any candidate solution \((h,G^{})\) satisfies \((h,G^{})_{}(f^{-1},G)\). We seek to discover suitable conditions that ensure this.

### Multi-Environment Data

Given only a single dataset of i.i.d. observations from \(P_{}\), there is no hope for \(_{}\)-identifiability. Even for observed \(\) (i.e., with \(n=d\) and known \(f=\)), \(G\) can only be identified up to Markov equivalence . With unknown mixing \(f\), the degree of observational non-identifiability gets even worse: for example, by using the reduced form of the SCM (SS 2.1) we can express \(\) in terms of the latent exogenous variables \(\) via \(=f()=f f_{}()\). This gives rise to a "spurious ICA solution" \((f_{}^{-1} f^{-1},G_{})\) where \(G_{}\) denotes the empty graph with independent components. Due to the non-identifiability of nonlinear ICA [54; 83], however, we cannot even learn the composition \(f f_{}\), let alone separate it into its constituents \(f\) and \(f_{}\) to isolate the intermediate causal variables \(\).

Motivated by these challenges to identifiability from i.i.d. data, we instead consider learning from _multiple environments_\(e\). That is, we assume access to heterogenous data from multiple distinct distributions \(P_{}^{e}\). Environments can arise, for example, from different experimental settings or correspond to broader contexts such as climate or time. Previous work has shown that this setting can, in principle, provide useful causal learning signals . However, multi-environment data is not necessarily useful if the corresponding distributions \(P_{}^{e}\) are allowed to differ in arbitrary ways. What makes this setting interesting is the assumption that certain parts of the causal generative process are shared across environments.

Here, we assume that all environments share the same invariant mixing function and underlying SCM, and that any distribution shifts arise from interventions on some of the causal mechanisms.4 General interventions can be modelled in SCMs by replacing some of the equations in (2.1) by new assignments \(V_{i}:=_{i}(_{(i)},_{i})\), resulting in "intervened" mechanisms \(_{i}(v_{i}_{(i)})\) replacing the corresponding conditionals \(p_{i}(v_{i}_{(i)})\) in (2.2) . We summarise this as follows.

**Assumption 2.8** (Shared mixing and mechanisms).: Each environment \(e\) shares the same mixing \(f\),

\[P_{}^{e}=f_{*}(P_{}^{e})\]

and each \(P_{}^{e}\) results from the same SCM through an intervention on a subset of mechanisms \(^{e}[n]\):

\[p^{e}()=p^{e}(v_{1},...,v_{n})=_{i^{e}}p_{i}^{e} (v_{i}_{(i)})_{j[n]^{e}}p_{j}(v_{j}_{(j)})\,.\]

Importantly, the intervention targets \(^{e}\) are not assumed to be known.

An intervention on some \(V_{i}\) that fully removes the influence from its parents \(_{(i)}\) is referred to as _perfect_, otherwise as _imperfect_. It has been shown that imperfect interventions are generally insufficient for full identifiability , even in the linear case . This is intuitive: if arbitrary imperfect interventions were allowed, including ones which preserve \(f_{i}(_{(i)},)\) and only replace \(U_{i}\) with some new \(_{i}\), then the spurious ICA solution \((f_{^{e}}^{-1} f^{-1},G_{})\) should be indistinguishable from the ground truth. In line with prior work , we therefore assume perfect interventions.

**Assumption 2.9** (Perfect interventions).: For all \(e\) and \(i^{e}\), we have \(p^{e}(v_{i}_{(i)})=p^{e}(v_{i})\).

Based on the principle of independent causal mechanisms , the _sparse mechanism shift hypothesis_ posits that distribution changes tend to manifest themselves in a sparse or local way in the causal factorization. In this spirit, we will assume single-node ("atomic") interventions, \(|^{e}|=1\), for our main results, as also required for existing results .

As motivated in SS 2.2, given data from \(\{P_{}^{e}\}_{e}\) we consider candidate solutions of the form \((h,G^{})\) where \(h\) is an unmixing function, or encoder, which maps observations \(\) to the inferred latents \(=h()\), and \(G^{}\) is causal graph capturing the relations among the \(Z_{i}\). The corresponding distributions of the inferred latents are thus given by the push-forward

\[Q_{}^{e}=h_{*}(P_{}^{e})=(h f)_{*}P_{}^{e}\]

The multi-environment setup with unknown single-node perfect interventions is illustrated in Fig. 2.

Figure 2: **Multi-Environment Setup with Single-Node Perfect Interventions and Shared Mixing Function.** Illustration of the considered multi-environment setup for \(n=2\) causal variables \(=\{V_{1},V_{2}\}\) with graph \(G\) given by \(V_{1} V_{2}\), shared mixing function \(f\), and environments \(=\{e_{0},e_{1},e_{2}\}\), corresponding to the observational setting \((e_{0})\) and perfect stochastic interventions on \(V_{1}\) (in \(e_{1}\)) and \(V_{2}\) (in \(e_{2}\)). The learnt unmixing function, or encoder, is denoted by \(h\), and the inferred latent representation by \(=h()\). The corresponding inferred latent distributions \(Q_{}^{e_{}}=h_{*}(P_{}^{e})\) are Markovian w.r.t. the candidate graph \(G^{}\) (here, equal to \(G\)). Since the intervention targets are not known, they may in principle differ in \(Q_{}^{e}\) as shown here. However, as we prove in Thm. 3.2, such misalignment is only possible if a certain genericity condition (3.2) is violated.

Identifiability Theory

We start by showing that identifiability up to \(_{}\) is, in fact, the best we can hope for when learning from interventional data, without any more direct forms of supervision. To this end, we state the following result, which is presented more formally and proven in Appx. B.

**Proposition 3.1** (Minimality of \(_{}\); informal).: _Let \(\) be any representation that is \(_{}\) equivalent to \(\), with \(G^{}=(G)\) the associated DAG. Then for any intervention on \(_{}\) in \(G\), there exists an equally sparse intervention on \(_{()}\) in \(G^{}\) inducing the same observed distribution on \(\)._

We now present our identifiability results. We first study the most fundamental bivariate case with two latent causal variables \(V_{1}\) and \(V_{2}\). This can loosely be seen as the CRL analogue of the widely studied cause-effect problem (\(X{}Y\) or \(Y{}X\)?) in classical structure learning .

**Theorem 3.2** (Bivariate identifiability up to \(_{}\) from one perfect stochastic intervention per node).: _Suppose that we have access to multiple environments \(\{P_{}^{e}\}_{e}\) generated as described in SS 2 under Asms. 2.2, 2.5, 2.8 and 2.9 with \(n=2\). Let \((h,G^{})\) be any candidate solution such that the inferred latent distributions \(Q_{}^{e}=h_{*}(P_{}^{e})\) of \(=h()\) and the inferred mixing function \(h^{-1}\) satisfy the above assumptions w.r.t. the candidate causal graph \(G^{}\). Assume additionally that_

1. _all densities_ \(p^{e}\) _and_ \(q^{e}\) _are continuously differentiable and fully supported on_ \(^{n}\)_;_
2. _we have access to a_ known observational environment__\(e_{0}\) _and one_ single node perfect intervention for each node, _with_ unknown targets_: there exist_ \(n+1\) _environments_ \(=\{e_{i}\}_{i=0}^{n}\) _such that_ \(^{e_{0}}=\) _and for each_ \(i[n]\) _we have_ \(^{e_{i}}=\{(i)\}\) _for an unknown permutation_ \(\) _of_ \([n]\)_;_
3. _for all_ \(i[n]\)_, the intervened mechanisms_ \(_{i}(v_{i})\) _differ from the corresponding base mechanisms_ \(p_{i}(v_{i}_{(i)})\) _everywhere, in the sense that_ \[:}_{i}(v_{i}) }{p_{i}(v_{i}_{(i)})} 0\,;\] (3.1)
4. _(_"genericity"_) the base and intervened mechanisms are not fine-tuned to each other, in the sense that there exists a continuous function_ \(:^{+}\) _for which_ \[_{ P_{}^{e_{0}}}[(_{ 2}(v_{2})}{p_{2}(v_{2} v_{1})})]_{ P_{ }^{e_{0}}}[(_{2}(v_{2})}{p_{2}(v_{2}  v_{1})})]\] (3.2)

_Then the ground truth is identified in the sense of Defn. 2.6, that is, \((f^{-1},G)_{}(h,G^{})\)._

Proof sketch (full proof in Appx. C.2).: Consider \(V_{1} V_{2}\) (the proof for \(V_{1} V_{2}\) is similar). Let \(=f^{-1} h^{-1}:^{n}^{n}\) such that \(=()\).5 By the change of variable formula, for all \(\)

\[q^{e}()=p^{e}(())|_{}()|\] (3.3)

where \((_{}())_{ij}=}{ z_{j}}()\) denotes the Jacobian of \(\). We consider two separate cases, depending on whether the intervention targets in \(q^{e_{i}}\) for \(e_{i}\{e_{1},e_{2}\}\) match those in \(p^{e_{i}}\) (Case 1) or not (Case 2).

_Case 1: Aligned Intervention Targets._ According to Asm. 2.8 and (A2), (3.3) applied to the known observational environment \(e_{0}\) and the interventional environments \(e_{1},e_{2}\) leads to the system of equations:

\[q_{1}(z_{1})q_{2}(z_{2} z_{(2;G^{})}) =p_{1}(_{1}())p_{2}(_{2}() _{1}())|_{}()|\] (3.4) \[_{1}(z_{1})q_{2}(z_{2} z_{(2;G^{})}) =_{1}(_{1}())p_{2}(_{2}() _{1}())|_{}()|\] (3.5) \[q_{1}(z_{1})_{2}(z_{2}) =p_{1}(_{1}())_{2}(_{2}( {z}))|_{}()|\] (3.6)

where \(z_{(2;G^{})}\) denotes the parents of \(z_{2}\) in \(G^{}\). Taking quotients of (3.5) and (3.4) yields

\[_{1}}{q_{1}}(z_{1})=_{1}}{p_{1}}(_{1}() )}}}{{}}  0=(_{1}}{p_{1}})^{}(_{1}() )}{ z_{2}}()_{}\) is triangular. According to (3.8), for all \(z_{1}\), the mapping \(z_{2}_{2}(z_{1},z_{2})\) is measure preserving for \(_{2}\) and \(_{2}\). By Lemma C.1[18, SS A.2, Lemma 2], it follows that \(_{2}\) must be constant in \(z_{1}\).6 Hence, \(\) is an element-wise function. Finally, \(G=G^{}\) follows from faithfulness (Asm. 2.2), for otherwise \((V_{1},V_{2})=(_{1}(Z_{1}),_{2}(Z_{2}))\) would be independent.

_Case 2: Misaligned Intervention Targets._ If \(G^{} G\), a similar argument to Case 1 (with the roles of \(z_{1}\) and \(z_{2}\) interchanged) also yields a contradiction to faithfulness. This leaves \(G=G^{}\). Writing down the system of equations similar to (3.4)-(3.6), and then taking ratios of \(e_{1}\) and \(e_{2}\) with \(e_{0}\) yields

\[_{1}}{q_{1}}(z_{1})=_{2}(_{2}() )}{p_{2}(_{2}()_{1}())}_{2}(z_{2})}{q_{2}(z_{2} z_{1})}=_{1}}{p_{1}}(_{1}())\,.\] (3.9)

These conditions highlight the misalignment in intervention targets (see Fig. 2). Unlike in Case 1, they do not directly imply that some elements of \(_{}\) need to be zero, that is \(\) can be arbitrarily mixed w.r.t. \(\). However, (3.9) imposes constraints on the form of \(\) that, by exploiting the invariance of \(q_{1}\) across \(e_{0}\) and \(e_{1}\) while \(p_{1}\) changes to \(_{1}\), can ultimately be shown to only be satisfied if the two expectations in (3.2) are equal for all continuous \(\). However, such fine-tuning is ruled out by (A4). 

_Remark 3.3_.: The main difficulty of the proof is that (3.9) may, in principle, hold when \((p,,q,)\) and \(\) are completely unconstrained. This does not arise in prior work if the intervention targets are known  (Case 1), or the densities or mixing are parametrically constrained .

**On the Genericity Condition (A4).** The condition in (3.2) contrasts expectations of the same quantity w.r.t. the observational distribution \(P_{}^{e_{0}}\) and the interventional distribution \(P_{}^{e_{1}}\) corresponding to an intervention on \(V_{1}\) that turns \(p_{1}\) into \(_{1}\). The shared argument, on the other hand, is a function of the ratio between the intervened mechanism \(_{2}\) and its base mechanism \(p_{2}\). While the two expectations are always equal for linear \(\), other choices imply non-trivial constraints. For instance, \((y)=y^{2}\) yields

\[_{1}(v_{1})-p_{1}(v_{1})_{2}(v_ {2})^{2}}{p_{2}(v_{2} v_{1})}\,v_{2}\,v_{1} 0\,.\]

Since \(p_{1}_{1}\) by assumption (A3), we argue that (A4) should generally hold for randomly chosen \((p_{1},p_{2},_{1},_{2})\) and can only be violated if they are fine-tuned to each other. It can thus be viewed as encoding some notion of genericity--in line with the principle of independent causal mechanisms , but also involving the intervened mechanisms. Interestingly, related genericity conditions also arise in the study of nonlinear cause-effect inference from observational data, where identifiability is often obtained up to a set of pathological ("fine-tuned") spurious solutions satisfying a partial differential equation involving the original mechanisms . Further, we note that \(\) in (3.2) can also be thought of as a _witness function of genericity_, similar to witness functions in kernel-based two sample and independence testing .

Next, we provide our identifiability result for an arbitrary number of causal variables.

**Theorem 3.4** (Identifiability up to \(_{}}\) from two paired perfect stochastic interventions per node).: _Suppose that we have access to multiple environments \(\{P_{}^{e}\}_{e}\) generated as described in SS 2 under Asms. 2.2, 2.3, 2.5, 2.8 and 2.9. Let \((h,G^{})\) be any candidate solution such that the inferred latent distributions \(Q_{}^{e}=h_{*}(P_{}^{e})\) of \(=h()\) and the inferred mixing function \(h^{-1}\) satisfy the above assumptions w.r.t. the candidate causal graph \(G^{}\). Assume additionally that_

1. _all densities_ \(p^{e}\) _and_ \(q^{e}\) _are continuously differentiable and fully supported on_ \(^{n}\)_;_
2. _we have access to at least one_ pair _of single-node perfect interventions per node, with unknown targets: there exist_ \(m n\) _known pairs of environments_ \(=\{(e_{j},e_{j}^{})\}_{j=1}^{m}\) _such that for each_ \(i[n]\) _there exists some_ unknown__\(j[m]\) _for which_ \(^{e_{j}}=^{e_{j}^{}}=\{i\}\)_;_
3. _for all_ \(i[n]\)_, the intervened mechanisms_ \(_{i}(v_{i})\) _and_ \(_{i}(v_{i})\) _differ everywhere, in the sense that_ \[ v_{i}:(_{i}}{_{i}})^{}(v_ {i}) 0\,;\] (3.10)

_Then the ground truth is identified in the sense of Defn. 2.6, that is, \((f^{-1},G)_{}}(h,G^{})\)._Proof sketch (full proof in Appx. C.3).: By considering ratios between \(e_{j}\) and \(e^{}_{j}\), taking partial derivatives w.r.t. \(z_{l}\), and using assumptions (A3'), we can identify a subset \(_{n}\) of exactly \(n\) pairs of environments corresponding to distinct intervention targets in \(p\) (for otherwise \(\) cannot be invertible). For \((e_{i},e^{}_{i})_{n}\), w.l.o.g. fix the intervention targets in \(p\) to \(^{e_{i}}=^{e_{i}}=\{i\}\) and let \(\) be a permutation of \([n]\) such that \((i)\) denotes the inferred intervention target in \(q\) that by (A2') is shared across \((e_{i},e^{}_{i})\). By the same argument as before, we must have \(V_{i}=_{i}(Z_{(i)})\), that is, \(\) is a permutation composed with an element-wise function. It remains to show that \(\) is a graph isomorphism, that is, \(V_{i} V_{j}\) in \(G Z_{(i)} Z_{(j)}\) in \(G^{}\). We prove \(\); the other direction is analogous. Suppose for a contradiction that there exist \((i,j)\) such that \(V_{i} V_{j}\) in \(G\), but \(Z_{(i)} Z_{(j)}\) in \(G^{}\). Consider \(e_{i}\) in which there are perfect interventions on \(Z_{(i)}\) and \(V_{i}\). For \(Z_{(k)}_{((j);G^{})}\), let \(_{k}=_{k}(Z_{(k)})\) and denote \(}=_{k}_{k}\{V_{i},V_{j}\}\). Since \(Z_{(i)}\) and \(Z_{(j)}\) are d-separated by \(_{((j);G^{})}\) in the post-intervention graph \(G^{}_{_{(i)}}\) with arrows pointing into \(Z_{(i)}\) removed , it follows by Markovianity of \(q\) w.r.t. \(G^{}\) that \(Z_{(i)} Z_{(j)}_{((j);G^{})}\) in \(Q^{}_{}\). By applying the corresponding diffeomorphic functions \(_{i}\), it follows from Lemma C.2 in Appx. C.1 that \(V_{i} V_{j}}\) in \(P^{e_{i}}_{}\). This violates faithfulness (Asm. 2.2) of \(P_{}\) to \(G\) since \(V_{i}\) and \(V_{j}\) are d-connected in \(G_{_{i}}\). 

(A2') states that we know that a _pair of datasets_ corresponds to two distinct interventions on the same underlying variable, even though we may not know the exact target of the intervention. This situation could arise, for example, if both datasets are collected under the same experimental setup but with varying experimental parameters. We stress that this is different from data consisting of _pairs of views_\((,})\) sharing the values of some variables, which is _counterfactual_ in nature [123; 4; 18]. One of the main challenges for our analysis (compared to a counterfactual multi-view setting) thus stems from the lack of correspondences across observations from different datasets. We also stress that a purely observational environment is not needed in this case, cf. (A2) in Thm. 3.2.

(A3') states that the intervention mechanisms are distinct in that their ratio is strictly monotonic, similar to (3.1) in (A3). This is a slightly stronger version of the assumption of _interventional discrepancy_ proposed by Wendong et al. ,7 which has been shown to be necessary for identifiability even if the graph \(G\) is known. For Gaussian \(_{i},}_{i}\), (A3') is satisfied, for example, by a shift in mean. In the proofs, this is used to show that \(\) must be an element-wise function, see (3.7). Intuitively, if \(_{i}=}_{i}\) in some open set for more than one \(i\), then the underlying variables can be nonlinearly mixed by a measure-preserving automorphism within this set without affecting the observed distributions .

## 4 Interpreting Causal Representations

Suppose that we succeed in identifying \(\) and \(G\) up to \(_{}\) (Defn. 2.6). How can we use or interpret such a causal representation? Since the scale of the variables is arbitrary (SS 2.2), we clearly cannot predict the exact outcomes of interventions. We therefore seek causal quantities that are preserved by the irresolvable ambiguities of \(_{}\). A prime candidate for this are interventional causal notions defined in terms of information theoretic quantities  and in particular the KL divergence \(D_{}\).

**Definition 4.1** (Causal influence; based on Defn. 2 of Janzing et al. ).: Let \(P_{}\) be Markovian w.r.t. a DAG \(G\) with vertices \(\). For any \(V_{i} V_{j}\) in \(G\), the _causal influence of \(V_{i}\) on \(V_{j}\)_ is given by

\[^{P_{}}_{i j}:=D_{}P_{}P_ {}^{i j}, p_{j}^{i j}v_{j} _{(j)\{i\}}= p_{j}v_{j} _{(j)}p_{i}(v_{i})\,v_{i}\]

and \(P_{}^{i j}\) is the interventional distribution arising from replacing the \(j^{}\) mechanism by \(p_{j}^{i j}\).8

The following result, proven in Appx. C.4, states that the causal influences are invariant to representation and equivariant to permutations, the two irresolvable ambiguities of the \(_{}\) equivalence class.

**Theorem 4.2** (Preservation of causal influences under \(_{}\)).: _Let \(P_{}\) be Markovian w.r.t. \(G\), let \(\) be a graph isomorphism of \(G\), and let \(\) be an element-wise diffeomorphism. Let \(=_{^{-1}}()\) and denote its induced distribution by \(Q_{}\). Then for any \(V_{i} V_{j}\) in \(G\) we have \(^{P_{}}_{i j}=^{Q_{}}_{(i)(j)}\)._

Thm. 4.2 implies that the strength of causal relations among variables in the inferred graph army meaning. They can thus be used to uncover changes to the latent causal mechanisms underlying different experimental datasets, for example, to gain scientific insights when combined with domain knowledge.

## 5 Learning Objectives

While our main focus is on studying identifiability, our theoretical insights also suggest approaches to learning causal representations from finite interventional datasets sampled from \(\{^{c}_{}\}_{e}\). The main idea is to fit the data in a way that preserves the sparsity of interventions ([98; 110]) by employing the same (un)mixing function and sharing mechanisms across environments (Asm. 2.8). We sketch two approaches, which, according to our theory, should both asymptotically identify the ground truth up to \(_{}\) if the set of available environments \(\) is sufficiently diverse (and the other assumptions hold).

* _Autoencoder Framework_: Jointly learn an encoder \(h\), a graph \(G^{}\), and intervention targets \(^{e}\) such that the encoded latents \(=h()\) can be used to reconstruct the observed \(\) across all environments, while ensuring all but the intervened mechanisms are shared. Using \(E\) as an environment indicator, the latter corresponds to the constraint \(Z_{i} E_{(i;G^{})}\) for \(i^{e}\), implementable, for example, through a suitable conditional independence test ([39; 94; 138]).
* _Generative Modelling Approach_: Fit a base generative model (\(G^{},p,f\)) and intervention models \((^{e},\{p^{c}_{i}\}_{i^{e}})_{e}\) by maximizing the likelihood of the multi-environment data. For example, given a candidate graph \(G^{}\) and candidate intervention targets \(\{^{e}\}_{e}\), learn the base and intervened mechanisms and mixing; then pick the graph and intervention targets that achieve the best fit.

## 6 Experiments

**Setup.** We experimentally pursue the second, generative approach. Specifically, we model the mixing function generating \(\) from \(\) as a _normalizing flow_([30; 93]) with different environment-specific base distributions, determined by the underlying causal graph, intervention targets, and (learnt) base and intervened mechanisms. Here, we focus on the bivariate case with two ground-truth causal latent variables \(V_{1} V_{2}\). According to Thm. 3.2, this setting should be identifiable from three environments: an observational one and one perfect intervention on each of \(V_{1}\) and \(V_{2}\). Our goal is to verify this empirically in light of finite data and optimization issues. We fix the observation dimension to \(d=2\) to facilitate exact likelihood training of the normalizing flows, and fit a separate generative model for each choice of graph and intervention targets.9 The base and intervened mechanisms are linear Gaussian and the mixing function a three-layer MLP, see Appx. D.1 for implementation details.

**Results.** The results are summarized in Fig. 3. Our main findings are two-fold: First, we observe that, in the majority of cases, the well-specified model attains the highest held-out log-likelihood, as shown in Fig. 3_(Right)_. This suggests that the likelihood of otherwise comparable generative models can act as a reliable criterion to select the correct causal graph and intervention targets. Second, we find that the ground truth latent causal variables are approximately identified up to element-wise rescaling (MCC values close to one) by the correctly specified model and not by any other model, as shown in Fig. 3_(Left)_. This indicates that recovering the correct graph and targets is not only sufficient but also necessary for reliable identification of the causal representation. Taken together, these findings are consistent with our notion of \(_{}\)-identifiability (Defn. 2.6) and Thm. 3.2.

Figure 3: **Empirical Comparison of Correctly and Incorrectly Specified Normalizing Flow-Based Models. For \(n=2\) latent causal variables with graph \(G\) given by \(V_{1} V_{2}\), we compare a generative model based on the correct causal graph \(G^{}=G\) and intervention targets (blue) to other generative models assuming the wrong graph \(G^{} G\) or misaligned intervention targets (yellow, red, purple). We show mean correlation coefficients (MCCs) between the learned and ground truth latents (_Left_) and the difference in validation model log-likelihood between the well-specified and misspecified models (_Right_). Each violin plot is based on 50 different ground truth data generating processes; the horizontal lines indicate the minimum, median and maximum values.**In Appx. D.2, we perform an additional experiment with \(n=3\) variables, nonlinear latent SCMs, fixed causal order without graph search, and one interventional environment per node, thus assaying violations of assumption (A2\({}^{*}\)) in the context of Thm. 3.4. Generally, we find that a correct choice of intervention targets can be selected based on model likelihood, leading to approximate identification of the causal variables, even when only the causal order is given, see Fig. 4 and Appx. D.2 for details.

## 7 Conclusions and Discussion

The world is full of domain shifts and different environments. Often, we cannot pin down what exactly differs between two domains, but it may reasonably be modelled as a change in some causal mechanisms. While prior work relied on harder-to-obtain counterfactual data or parametric constraints for identifiability, our study demonstrates that interventional data can be sufficient--even in the nonparametric case where the spaces of mixing functions and mechanisms are infinite dimensional. Our results can be considered a step towards justifying the use of expressive machine learning methods for learning interpretable causal representations from high-dimensional experimental data in situations where parametric assumptions are undesirable, e.g., for complex systems in physics, biology, or medicine.

**Weaker Notions of Identifiability.** In this work, we have focused on the strongest notion of identifiability that is achievable in a nonparametric setting (Defn. 2.6). However, subject to the available data and assumptions, identifiability up to \(_{}\) is not always possible. In this case, weaker notions of identifiability are of interest. For example, we may not be able to uniquely recover variables that are not targeted by interventions , or only recover groups of variables up to (non-)linear mixing  and the graph up to transitive closure if interventions are imperfect, or soft . A precise characterization of weaker notions of _nonparametric_ identifiability from different types of _interventions_ (cf.  for a temporal, semi-parametric setting) is an interesting direction for future work.

**Known vs. Unknown Intervention Targets.** When intervention targets can be considered known appears to be a more nuanced concept in CRL than in a fully observed setting, see also [126, SSE] for an extended discussion. Recall that we assume w.l.o.g. that \(V_{1}... V_{n}\) and only consider graphs respecting this ordering (SS 2.2), see also [117, Remark 1]. The intervention targets are then unknown w.r.t. the pre-imposed causal ordering. This is a key aspect that makes our setting more realistic, but also substantially complicates the analysis (see (3.9) and Remark 3.3). Similar to , for Thm. 3.2 we require a set of _exactly_\(n\) environments (one intervention on each node).10 However, we relax this requirement to mere coverage ("at least \(1\)") in Thm. 3.4 as shown for linear causal models in .

**Identifiability From One Intervention per Node for Any \(n\).** We conjecture that Thm. 3.2 can be generalized to \(n>2\), subject to a suitably adjusted set of genericity conditions involving several intervened and base mechanisms, akin to (3.2) in the bivariate case. The main challenge to such a generalization appears to be combinatorial, as there are \(n!-1\) ways of misaligning intervention targets across \(p\) and \(q\). In Thm. 3.4, we sidestep this issue by assuming pairs of environments. Thus, while two single-node perfect interventions are sufficient, we do not believe this to be necessary.

**On the Assumption of Known \(n\) and Its Relation to Markovianity.** Asm. 2.3 relates to _causal sufficiency_ or _Markovianity_ in classical causal inference, which correspond to the assumption of independent \(U_{i}\) in Defn. 2.1, implying the causal Markov factorization (2.2) . With _unobserved_\(V\) and _unknown_\(n\), the notion of "unobserved confounders" gets blurred, since one can, in principle, always construct a causally sufficient system by increasing \(n\) and adding any causes of two or more endogenous variables to \(V\). Asm. 2.3 then states that the minimum number of variables required to do so is known.11 However, this can lead to very large systems, which may in turn challenge the assumption of an invertible mixing (Asm. 2.5). Extensions of our analysis to unknown \(n\), non-Markovian, or non-invertible  models constitute an interesting direction for future investigations.

**Practicality.** The method explored in SS 6 requires searching over graphs and intervention targets, which gets intractable even for moderate \(n\). Simultaneously learning an (un)mixing function, causal graph, intervention targets, and mechanisms is challenging. Further work is needed to make methods for nonparametric CRL from multi-environment data more practical, e.g., by exploring the proposed autoencoder framework with a continuous parametrisation of graph  and targets .