# Instructions

## Aligning Diffusion Models by Optimizing Human Utility

Shufan Li\({}^{1}\)\({}^{}\) Konstantinos Kallidromitis\({}^{2}\)\({}^{}\) Akash Gokul\({}^{3}\)\({}^{}\)* Yusuke Kato\({}^{2}\) Kazuki Kozuka\({}^{2}\)

\({}^{1}\)University of California, Los Angeles

\({}^{2}\)Panasonic AI Research

\({}^{3}\)Salesforce AI Research

\({}^{}\)Equal contribution * Work done outside of Salesforce.

Correspondence to jacklishufan@cs.ucla.edu

## Abstract

We present Diffusion-KTO, a novel approach for aligning text-to-image diffusion models by formulating the alignment objective as the maximization of expected human utility. Unlike previous methods, Diffusion-KTO does not require collecting pairwise preference data nor training a complex reward model. Instead, our objective uses per-image binary feedback signals, _e.g._ likes or dislikes, to align the model with human preferences. After fine-tuning using Diffusion-KTO, text-to-image diffusion models exhibit improved performance compared to existing techniques, including supervised fine-tuning and Diffusion-DPO, both in terms of human judgment and automatic evaluation metrics such as PickScore  and ImageReward . Overall, Diffusion-KTO unlocks the potential of leveraging readily available per-image binary preference signals and broadens the applicabil

Figure 1: **Diffusion-KTO is a novel framework for aligning text-to-image diffusion models with human preferences using only per-sample binary feedback**. Diffusion-KTO bypasses the need to collect expensive pairwise preference data and avoids training a reward model. As seen above, Diffusion-KTO aligned text-to-image models generate images that better align with human preferences. We display results after fine-tuning Stable Diffusion v1-5 and sampling prompts from HPS v2 , Pick-a-Pic , and PartiPrompts  datasets.

ity of aligning text-to-image diffusion models with human preferences. Code is available at https://github.com/jacklishufan/diffusion-kto

## 1 Introduction

In the rapidly evolving field of generative models, aligning model outputs with human preferences remains a paramount challenge, especially for text-to-image (T2I) models. Large language models (LLMs) have made significant progress in generating text that caters to a wide range of human needs, primarily through a two-stage process: first, pretraining on noisy web-scale datasets, then fine-tuning on a smaller, preference-specific dataset. This fine-tuning process aims to align the generative model's outputs with human preferences, without significantly diminishing the capabilities gained from pretraining. Extending this fine-tuning approach to text-to-image models offers the prospect of tailoring image generation to user preferences, a goal that has remained relatively under-explored compared to its counterpart in the language domain.

Recent works have begun to explore aligning text-to-image models with human preferences. These methods either use a reward model and a reinforcement learning objective , or directly fine-tune the T2I model on preference data . However, these methods are restricted to learning from pairwise preference data, which consists of pairs of preferred and unpreferred images generated from the same prompt.

While paired preferences are commonly used in the field, it is not the only type of preference data available. Per-sample feedback is a promising alternative to pairwise preference data, as the former is abundantly available on the Internet. Per-sample feedback provides valuable preference signals for aligning models, as it captures information about the users' subjective distribution of desired and undesired generations. For instance, as seen in Fig. 2, given an image and its caption, a user can easily say if they like or dislike the image based on criteria such as attention to detail and fidelity to the prompt. While paired preference data provides more information about relative preferences, gathering such data is an expensive and time-consuming process in which annotators must rank images according to their preferences. In contrast, learning from per-sample feedback can utilize the vast amounts of per-sample preference data collected on the web and increases the applicability of aligning models with user preferences at scale. Inspired by these large-scale use cases, we explore how to directly fine-tune T2I models on per-image binary preference data.

To address this gap, we propose Diffusion-KTO, a novel alignment algorithm for T2I models that operates on binary per-sample feedback instead of pairwise preferences. Diffusion-KTO extends the utility maximization framework shown in KTO  to the setting of diffusion models. Specifically, KTO bypasses the need to maximize the likelihood from paired preferences and, instead, directly optimizes an LLM using a utility function that encapsulates the characteristics of human decision-making. While KTO is easy to apply to large language models, we cannot immediately apply it to diffusion models as it would require sampling across all possible trajectories in the denoising process. Although existing works approximate this likelihood by sampling once through the reverse diffusion process, back-propagating over all sampling steps is extremely computationally expensive. To overcome this, we present a utility maximization objective that applies to each individual sampling step, circumventing the need for sampling through the entire reverse diffusion process.

Our main contributions are as follows:

* We generalize the human utility maximization framework used to align LLMs to the setting of diffusion models (Section 4).
* Our method, Diffusion-KTO, facilitates alignment from per-image binary feedback. Thus, introducing the possibility of learning from human feedback at scale using the abundance of per-sample feedback that has been collected on the Internet.
* Through comprehensive evaluations, we demonstrate that generations from Diffusion-KTO aligned models are generally preferred over existing approaches, as judged by human evaluators and preference models (Section 5).

In summary, Diffusion-KTO offers a simple yet robust framework for aligning T2I models with human preferences that greatly expands the utility of generative models in real-world applications.

## 2 Related Works

**Text-to-Image Generative Models.** Text-to-image (T2I) models have demonstrated remarkable success in generating high quality images that maintain high fidelity to the input caption [38; 54; 37; 7; 11; 26; 55; 8]. In this work, we focus on diffusion models [43; 44; 24] due to their popularity and open-source availability. While these models are capable of synthesizing complex, high quality images after pretraining, they are generally not well-aligned with the preferences of human users. Thus, they can often generate images with noticeable issues, such as poorly rendered hands and faces. We seek to address these issues by introducing a fine-tuning objective that allows text-to-image diffusion models to learn directly from human preference data.

**Improving Language Models using Human Feedback.** Following web-scale pretraining, large language models are further improved by fine-tuning on a curated set of data (supervised fine-tuning) and then using reinforcement learning to learn from human feedback. Reinforcement learning from human feedback (RLHF) [2; 12; 13], in particular, has been shown to be an effective means of aligning these models with user preferences [59; 5; 31; 30; 29; 45; 10; 3; 6; 22]. While this approach has been successful [1; 46], the difficulties in fine-tuning an LLM using RLHF [36; 58; 49; 17; 21; 42; 4] has led to the development of alternative fine-tuning objectives [35; 18; 56; 57]. Along these lines, KTO  introduces a fine-tuning objective that trains an LLM to maximize the utility of its output according to the Kahneman & Tversky model of human utility . This utility maximization framework does not require pairwise preference data and only needs per-sample binary feedback. In this work, we explore aligning diffusion models given binary feedback data. As a first step in this direction, we generalize the utility maximization framework to the setting of diffusion models.

**Improving Diffusion Models using Human Feedback.** Before the recent developments in aligning T2I models using pairwise preferences, supervised fine-tuning was the popular approach for improving these models. Existing supervised fine-tuning approaches curate a dataset using preference models [39; 32], pre-trained image models [41; 8; 16; 51; 50], and/or human experts , and fine-tune the model on this dataset. Similarly, many works have explored using reward models to fine-tune diffusion models via policy gradient techniques [19; 23; 9; 52; 14; 33; 28; 20] to improve aspects such as image-text fidelity. Similar to our work, ReFL , DRaFT , and AlignProp  align T2I diffusion models with human preferences. However, these methods require back-propagating the reward through the reverse diffusion sampling process, which is extremely expensive in memory. As a result, these works depend on techniques such as low-rank weight updates  and sampling from only a subset of steps in the reverse process, thus limiting their ability to fine-tune the model. In contrast, the Diffusion-KTO objective extends to each step in the denoising process, thereby avoiding such memory issues. More broadly, the main drawbacks of these reinforcement learning based approaches are: limited generalization, _e.g._ closed vocabulary [28; 20], reward hacking [14; 33; 9], and they rely on a potentially biased reward model. Since Diffusion-KTO trains directly on open-vocabulary preference data, we find that it can generalize to an open-vocabulary and avoids issues such as reward hacking. Recently, works such as Diffusion-DPO and D3PO  present extensions of the DPO objective  to the setting of diffusion models. Diffusion-KTO shares

Figure 2: **Diffusion-KTO aligns text-to-image diffusion models using per-image binary feedback.** Existing alignment approaches (_Left_) are restricted to learning from pairwise preferences. However, Diffusion-KTO (_Right_) uses per-image preferences which are abundantly available on the Internet. As seen above, the quality of an image can be assessed independent of another generation for the same prompt. More importantly, such per-image preferences provide valuable signals for aligning T2I models, as demonstrated by our results.

similarities with Diffusion-DPO and D3PO, as we build upon these works to introduce a reward model-free alignment objective. However, unlike these works, Diffusion-KTO does not rely on pairwise preference data and, instead, uses only per-image binary feedback.

## 3 Background

### Diffusion Models

Denoising Diffusion Probabilistic Models (DDPM)  model the image generation process as a Markovian process. Given data \(x_{0}\), the forward process \(p(x_{t}|x_{t-1})\) gradually adds noise to an initial image \(x_{0}\) according to a variance schedule, until it reaches \(x_{T}(0,I)\). A generative model can be trained to learn the reverse process \(q_{}(x_{t-1}|x_{t})\) using the evidence lower bound (ELBO) objective:

\[_{}=_{x_{0},t,}[(t)\|_ {t}-_{}(x_{t},t)\|^{2}]\] (1)

where \((t)\) is a time-dependent weighting function and \(\) is the added noise.

### Direct Preference Optimization

RLHF first fits a reward model \(r(x,c)\), for a generated sample \(x\) and input prompt \(c\), to human preference data \(\), and then maximizes the expected reward of a generative model \(_{}\) while ensuring it does not significantly deviate from the initialization point \(_{}\). It uses the following objective with a divergence penalty controlled by a hyperparameter \(\).

\[}{}\ _{c,x_{ }(x|c)}[r(x,c)]-_{}[_{}(x|c)||_{ }(x|c)]\] (2)

The authors of DPO  present an equivalent objective (Eq. (3)) using the implicit reward model \(r(x,c)=(x|c)}{_{}(x|c)}+ Z(c)\)

\[}{}\ _{x^{w},x^{l},c}[ ((x^{w}|c)}{_{}(x^{w}|c)}- (x^{l}|c)}{_{}(x^{l}|c)})]\] (3)

where \(Z(c)\) is the partition function, \((x^{w},x^{l})\) are pairs of winning and losing samples, and \(c\) is the input conditioning. Through this formulation, the model \(_{}\) can be directly trained in a supervised fashion without explicitly fitting a reward model.

### Implicit Reward Model of a Diffusion Model

One of the challenges in adapting DPO to the context of diffusion models is that the likelihood \(_{}(x|c)\) is hard to optimize because each sample \(x\) is generated through a multi-step Markovian process. In particular, it requires computing the marginal distribution \(_{x_{1}...x_{N}}_{}(x_{0},x_{1}...x_{N}|c)\) over all possible path of the diffusion process, where \(_{}(x_{0},x_{1}...x_{N}|c)=_{}(x_{N})_{i=1}^{N}_{ }(x_{i-1}|x_{i},c)\).

D3PO  adapted DPO by considering the diffusion process as a Markov Decision Process (MDP). In this setup, an agent takes an action \(a\) at each state \(s\) of the diffusion process. Instead of directly maximizing \(r(x,c)\), one can maximize \(Q(a,s)\) which assigns a value to each possible action \(a\) at a given state \(s\) in the diffusion process instead of the final outcome. This setup uses the local policy \((a|s)\), which represents a single sampling step. In this setup, D3PO  showed that the optimal solution \(Q^{*}(a,s)\) satisfies the relation \(Q^{*}(a,s)=^{*}(a|s)}{_{}(a|s)}\) for the optimal policy \(_{}^{*}\). This leads to the approximate objective:

\[}{}\ _{s d^{},a_{ }(|s)}[Q(a,s)]-_{}[_{}(a|s)||_{ }(a|s)]\] (4)

where \(d^{}\) is the state visitation distribution under policy \(\). Concretely, the action is a sampling step, and we can write \(Q(a,s)\) as \(Q(x_{t-1},x_{t},c)\) and \((a|s)\) as \((x_{t-1}|x_{t},c)\).

### Kahneman-Tversky Optimization

In decision theory, the expected utility hypothesis assumes that a rational agent makes decisions based on the expected utility of all possible outcomes of an action, instead of using objective measurements such as the value of monetary returns. Formally, given an action \(a\) and the set of outcomes \(O(a)\), the expected utility is defined as \(EU(a)=_{o O(a)}p_{A}(o)U(o)\), where \(p_{A}\) is a subjective belief of the probability distribution of the outcomes and the utility function \(U(o)\) is a real-valued function.

Prospect theory  further augments this model by asserting that the utility function is not defined solely on the outcome (_e.g._ the absolute gain in dollars), but also with respect to some reference point (_e.g._ current wealth). In this formulation, the utility function is defined as \(U(o,o_{})\) for a reference outcome \(o_{}\). Based on this theory, KTO  proposed an alternative objective for aligning LLMs:

\[}{}\;_{c,x D}[(x)( \;w(x)((x|c)}{_{}(x|c)}-_{c^ {} D}[\;(_{}(x^{}|c^{})\| _{}(x^{}|c^{}))])))\] (5)

where \(x\) is the output of the LLM, \(c\) is the input prompt, \(w(x)=1\) if \(x\) is desirable and \(w(x)=-1\) otherwise, and \((x)\) is a weighting function of samples. The divergence penalty is computed as the expectation of the KL divergence between the model distribution \(_{}(x^{}|c^{})\) and the reference distribution \(_{}(x^{}|c^{})\) over all input prompts \(c^{}\) in the dataset. This formulation uses the sigmoid function \((x)\) as an approximation for the Kahneman-Tversky utility function which is concave in gain and convex in loss. Experiments showed that KTO aligned LLMs were able to outperform DPO aligned LLMs, and KTO is resilient towards noise in the preference data .

## 4 Method

### Diffusion-KTO

Here, we propose Diffusion-KTO. Instead of optimizing the expected reward, we incorporate a non-linear utility function that calculates the utility of an action based on its value \(Q(a,s)\) with respect to the reference point \(Q_{}\).

\[}{}\;_{s^{} d^{},a^{ }_{}(|s)}[U(Q(a^{},s^{})-Q_{})]\] (6)

where \(U(v)\) is a monotonically increasing value function that maps the implicit reward to subjective utility. Practically, the local policy \(_{}(a|s)\) is a sampling step, and can be written as \(_{}(x_{t-1}|x_{t})\). Applying the relation \(Q^{*}(a,s)\) from Sec. 3.3, we get the objective

\[}{}\;_{x_{},t ([0,])}[U((x_{t-1}|x_{t} )}{_{}(x_{t-1}|x_{t})}-Q_{})]\] (7)

Figure 3: **We present Diffusion-KTO, which aligns text-to-image diffusion models by extending the utility maximization framework to the setting of diffusion models.** Since this framework aims to maximize the utility of each generation (\(U(x)\)) independently, it does not require paired preference data. Instead, Diffusion-KTO trains with per-image binary feedback signals, _e.g._ likes and dislikes. Our objective also extends to each step in the diffusion process, thereby avoiding the need to back-propagate a reward through the entire sampling process.

Following KTO , we can optimize the policy based on whether a given generation is considered "desirable" or "undesirable":

\[}{}\ _{x_{0},t }[U(w(x_{0})((x_{t-1}|x_{t})}{ _{}(x_{t-1}|x_{t})}-Q_{}))]\] (8)

where \(w(x_{0})= 1\) if image \(x_{0}\) is desirable or undesirable. We set \(Q_{}=_{}[_{}(a|s)||_{} (a|s)]\). Empirically, this is calculated by computing \((0,(a^{}|s^{})}{_{ {ref}}(a^{}|s^{})})\) over a batch of unrelated pairs of \((s^{},a^{})\) following the KTO setup  in Eq. (5).

### Utility Functions

While we incorporate the reference point aspect of the Kahneman-Tversky model, it is unclear if other assumptions about human behavior are applicable. It is also known that different people may exhibit different utility functions. Thus, we explore a wide range of utility functions. For presentation purposes, we center all utility functions \(U(x)\) around 0 by using \(U_{}(x)=U(x)-U(0)\), such that \(U_{}(0)=0\). This does not change the objective in Eq. (8) as the gradient and optimal policy are not affected. We experiment with the following utility functions:

* **Loss-Averse:** We characterize a loss-averse utility function as any utility function that is concave (see \(U(x)\) plotted in blue in Figure 3). Using this utility function, the Diffusion-KTO objective can be considered as a variant of the Diffusion-DPO objective. While aligning according to this utility function follows a similar form to the Diffusion-DPO objective, our approach does not require paired preference data.
* **Risk-Seeking:** Conversely, we define a risk-seeking utility function as any convex utility function (see \(U(x)\) plotted in yellow in Figure 3). A typical example of a risk-seeking utility function is the exponential function. However, its exploding behavior on \((0,+)\) makes it hard to optimize. Instead, for this case, we consider \(U(x)=-(-x)\).
* **Kahneman-Tversky model:** Kahneman-Tversky's prospect theory argues that humans tend to be risk-averse for gains but risk-seeking for losses relative to a reference point. This amounts to a function that is concave in \((0,+)\) and convex in \((0,-)\). Following the adaptation proposed in KTO, we employ the sigmoid function \(U(x)=(x)\) (see \(U(x)\) plotted in red in Figure 3). Empirically, we find this utility function to perform best.

Under the expected utility hypothesis, the expectation is taken over the subjective belief of the distribution of outcomes, not the objective distribution. In our setup, the dataset consists of unpaired samples \(x\) that are either desirable (\(w(x)=1\)) or undesirable (\(w(x)=-1\)). Because we do not have access to additional information, we assume the subjective belief of a sample \(x\) is solely dependent on \(w(x)\). During training, this translates to a biased sampling process where each sample is drawn uniformly from all desirable samples with probability \(\) and uniformly from all undesirable samples with probability \(1-\).

## 5 Experiments

We comprehensively evaluate Diffusion-KTO through quantitative and qualitative analyses to demonstrate its effectiveness in aligning text-to-image diffusion models with a preference distribution. Further comparisons, such as the performance when using prompts from different datasets, the results of our ablations, and implementation and evaluation details can be found in the Appendix. Additionally, in the Appendix, we report the results of synthetic experiments which highlight that Diffusion-KTO can be used to cater T2I diffusion models to the preferences of a specific user. The code used for this work will be made publicly available and is available in the Supplementary material.

**Implementation Details.** We fine-tune Stable Diffusion v1-5 (SD v1-5)  (CreativeML Open RAIL-M license) with the Diffusion-KTO objective, using the Kahneman-Tversky utility function, on the Pick-a-Pic v2 dataset  (MIT license). The Pick-a-Pic dataset consists of paired preferences in the form of (preferred image, non-preferred image, input prompt). Since Diffusion-KTO does not require paired preference data, we partition the images in the training data. If an image is labelledas preferred at least once, we consider it a desirable sample, otherwise we consider the sample undesirable. In total, we train with 237,530 desirable samples and 690,538 undesirable samples.

Evaluation Details.We evaluate the effectiveness of Diffusion-KTO by comparing generations from our Diffusion-KTO aligned model to generations from existing methods using automated preference metrics and user studies. For our results using automated preference metrics, we present win-rates (how often the metric prefers Diffusion-KTO's generations versus another method's generations) using the LAION aesthetics classifier  (MIT license), which is trained to predict the aesthetic rating a human would give to the provided image, CLIP  (MIT license), which measures image-text alignment, and PickScore  (MIT license), HPS v2  (Apache-2.0 license), and ImageReward  (Apache-2.0 license) which are caption-aware models that are trained to predict a human preference score given an image and its caption. Additionally, we perform user studies to compare Diffusion-KTO with existing baselines. In our user study, we ask judges to assess which image they prefer (_Which image do you prefer given the prompt?_) given an image generated by our Diffusion-KTO model and an image generated by the other method for the same prompt.

We compare Diffusion-KTO to the following baselines: Stable Diffusion v1-5 (SD v1-5), supervised fine-tuning (SFT), conditional supervised fine-tuning (CSFT), AlignProp , D3PO  and Diffusion-DPO . Our SFT baseline fine-tunes SD v1-5 on the subset of images that are labelled as preferred using the standard denoising objective. Our CSFT baseline, similar to the approach introduced into HPS v1 , appends a prefix to each prompt ("good image", "bad image") and fine-tunes SD v1-5 using the standard diffusion objective while training with preferred and non-preferred samples independently. To compare with D3PO (MIT license), we fine-tune SD v1-5 using their officially released codebase. For AlignProp (SD v1-5) (MIT license) and Diffusion-DPO (SD v1-5) (Apache-2.0 license), we compare with their officially released checkpoints.

### Quantitative Results

Table 1 provides the win-rate, per automated metrics, for Diffusion-KTO aligned SD v1-5 and the related baselines. Diffusion-KTO markedly improves alignment of SD v1-5, with win-rates of up to 87.2%. Results from our user study (Figure 4) confirm that human evaluators consistently prefer the

   Method & Aesthetic & PickScore & ImageReward & CLIP & HPS v2 \\  vs. SD v1-5 & **86.0** & **85.2** & **87.2** & **62.0** & **62.0** \\ vs. SFT & **56.4** & **72.8** & **64.8** & **64.8** & **54.6** \\ vs. CSFT & **50.6** & **73.6** & **65.2** & **62.8** & **60.4** \\ vs. AlignProp & **86.8** & **96.6** & **84.4** & **96.2** & **90.2** \\ vs. D3PO & **68.0** & **73.6** & **71.6** & **56.8** & **55.6** \\ vs. Diffusion-DPO & **74.2** & **61.8** & **78.4** & **53.2** & **51.6** \\   

Table 1: **Automatic win-rate (%) for Diffusion-KTO (SD v1-5) in comparison to existing alignment approaches using prompts from the Pick-a-Pic v2 test set.** We use off-the-shelf models, _e.g._ preference models such as PickScore, to compare generations and determine a winner based on the method with the higher scoring generation. Diffusion-KTO drastically improves the alignment of the base SD v1-5 and demonstrates significant improvements in alignment when compared to existing approaches. Win rates above 50% are **bolded**.

Figure 4: **User study win-rate (%) comparing Diffusion-KTO (SD v1-5) to SD v1-5, and SFT (SD v1-5) and Diffusion-DPO (SD v1-5). Results of our user study show that Diffusion-KTO significantly improves the alignment of the base SD v1-5 model. Moreover, our Diffusion-KTO aligned model also outperforms supervised finetuning (SFT) and the officially released Diffusion-DPO model, as judged by users, despite only training with simple per-image binary feedback. We also include the 95% confidence interval of the win-rate.**

generations of Diffusion-KTO to that of the base SD v1-5 (75% win-rate in favor of Diffusion-KTO). Further, Diffusion-KTO aligned models outperform related alignment approaches such as AlignProp, D3PO, and Diffusion-DPO. Diffusion-KTO significantly outperforms Diffusion-DPO on metrics such as LAION Aesthetics, PickScore, and HPS v2 while performing comparably in terms of other metrics. We also find that human judges prefer generations from our Diffusion-KTO model (72% win-rate versus SFT and 69% win-rate versus Diffusion-DPO) over that from SFT and Diffusion-DPO. This highlights the effectiveness of our utility maximization objective and shows that not only can Diffusion-KTO learn from per-image binary feedback, but it can also outperform models training with pairwise preference data.

### Qualitative Results

In Fig. 5, we showcase a visual comparison of Diffusion-KTO with existing approaches for preference alignment. As seen in the first row, most models are misguided by the "sunlight" reference in the prompt and produce in a dark image. Diffusion-KTO demonstrates a focus on the bird, which is the central object in the caption and provides a better quality result over Diffusion-DPO, which doesn't include any visual indication for the "sunlight". In the second row of images, our Diffusion-KTO aligned model is able to successfully generate a _"turtle baby sitting in a coffee cup"_. Methods such as Diffusion-DPO, in this example, have an aesthetically pleasing result but ignore key components of the prompt (_e.g. "turtle", "night"_). On the other hand, SFT and CSFT follow the prompt but provide less appealing images. For the third prompt, which is a detailed description of a woman, the output from our Diffusion-KTO model provides the best anatomical features, symmetry, and pose compared to the other approaches. Notably, for this third prompt, the generation of the Diffusion-KTO model also generated a background is more aesthetically pleasing. The final row uses a difficult prompt that requires a lot of different objects in a niche art style. While all models were able to depict the right style, _i.e._ geometric art, only the Diffusion-KTO generation includes key components such as the "moon," "flying fish," and "fisherman" objects. These examples demonstrate that Diffusion-KTO significantly increases the visual appeal of generated images while improving image-text alignment.

Figure 5: **Side-by-side comparison of images generated by related methods using SD v1-5.** Diffusion-KTO demonstrates a significant improvement in terms of aesthetic appeal and fidelity to the caption (see Sec. 5.2).

## 6 Analysis

To further study the effect of different utility functions, we conduct miniature experiments to observe their impact. We assume the data is two-dimensional, and the pretraining data follows a Gaussian distribution centered at (0.5, 0.8) with variance 0.04. We sample desirable samples from a Gaussian distribution \(P_{d}\) centered at (0.3, 0.8), sample undesirable samples from a Gaussian distribution \(P_{u}\) centered at (0.3, 0.6), and the variance of both distributions is 0.01. We pretrain small MLP diffusion models, using the standard diffusion objective on the pretraining data, and then fine-tune using various utility functions. We sample 3500 data points from the trained model. Figure 6 shows that the risk-averse utility function (used by Diffusion-DPO) has a strong tendency to avoid loss to the point that it deviates from the distribution of desirable samples. The risk-seeking utility function behaves roughly the same as the SFT baseline and shows a strong preference for desirable samples at the cost of tolerating some undesirable samples. In comparison, our objective achieves a good balance.

## 7 Limitations

While Diffusion-KTO significantly improves the alignment of text-to-image diffusion models, it suffers from the shortcomings of T2I models and related alignment methods. Specifically, Diffusion-KTO is trained on preference data from the Pick-a-Pic dataset which contains prompts submitted by online users and images generated using off-the-shelf T2I models. As a result, the preference distribution in this data may be skewed toward inappropriate or otherwise unwanted imagery. Furthermore, in this work, we examined three main models of human utility, from which we have concluded the Kahneman-Tversky model to perform best based on empirical results. However, we believe that the choice of utility function, as well as the underlying assumptions behind such functions, remains an open question. Additionally, since Diffusion-KTO fine-tunes a pretrained T2I model, it inherits the weaknesses of this model, including generating images that reflect and propagate negative stereotypes. Despite these limitations, Diffusion-KTO presents a broader framework for improving and aligning diffusion models from per-image binary feedback.

## 8 Conclusion

In this paper, we introduced Diffusion-KTO, a novel approach to aligning text-to-image diffusion models with human preferences using a utility maximization framework. This framework avoids the need to collect pairwise preference data, as Diffusion-KTO only requires simple per-image binary feedback, such as likes and dislikes. We extend the utility maximization approach, recently introduced to align LLMs, to the setting of diffusion models and explore various utility functions. Diffusion-KTO aligned diffusion models lead to demonstrable improvements in image preference and image-text alignment when evaluated by human judges and automated metrics. While our work has empirically found the Kahneman-Tversky model of human utility to work best, we believe that the choice of utility functions remains an open question and promising direction for future work.

Figure 6: **Visualizing the effect of various utility functions. We sample from MLP diffusion models trained using various alignment objectives. We find that using the Kahneman-Tversky utility function leads to the best performance in terms of aligning with the desirable distribution and avoiding the undesirable distribution.**