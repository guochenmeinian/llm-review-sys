# Subjective Randomness and In-Context Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Large language models (LLMs) exhibit intricate capabilities, often achieving high performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often unclear, with different prompts eliciting different capabilities, especially when used with in-context learning (ICL). We propose a "Cognitive Interpretability" framework that enables us to analyze ICL dynamics to understand latent concepts underlying LLMs' behavioral patterns. This provides a more nuanced understanding than posthoc evaluation benchmarks, but does not require observing model internals as a mechanistic interpretation would require. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of ICL by manipulating properties of context data, such as sequence length. In the latest GPT-3.5+ models, we find emergent abilities to generate pseudo-random numbers and learn basic formal languages, with striking ICL dynamics where model outputs transition sharply from pseudo-random behaviors to deterministic repetition.

## 1 Introduction

Large language models (LLMs), especially when prompted via in-context learning (ICL), demonstrate complex, emergent capabilities [1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12]. Specifically, ICL yields task-specific behaviors in LLMs via use of different prompts (or _contexts_) [1; 5; 13; 14; 15; 16; 17; 18; 19; 20]. Although no weight updates occur in ICL, different input contexts can activate, or re-weight, different latent algorithms in an LLM, analogous to how traditional learning methods such as gradient descent use training data to re-weight model parameters to learn representations [21; 22; 23; 24; 25; 26]. Two seemingly equivalent prompts can, however, evoke very different behaviors in LLMs . Our central motivation is to interpret emergent capabilities and latent _concepts_ underlying complex behaviors in LLMs by analyzing in-context learning behavioral dynamics, without directly observing hidden unit activations or re-training models on varied datasets.

Inspired by computational approaches to human cognition [27; 28; 29; 30; 31], we model and interpret latent concepts evoked in LLMs by different contexts, without observing or probing model internals. This approach, which we call **Cognitive Interpretability**, is a middle ground between shallow test-set evaluation benchmarks on one hand [32; 33; 34; 35; 36; 37; 38; 39; 40] and mechanistic neuron- and circuit-level understanding of pre-trained models' capabilities on the other [41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53]. Computational cognitive scientists have related algorithmic information theory to human cognition, where mental concepts are viewed as programs, and cognitive hypothesis search over concepts is viewed as Bayesian inference [54; 55; 56; 57; 58; 30]. In this vein, Griffiths and Tenenbaum  model subjective randomness in human cognition as probabilistic program induction, where a person must search over a space of non-random programs in order to answer the question, "was this sequence generated by a random process?" We argue that ICL can similarly be seen as under-specified program induction, where there is no single "correct" answer; instead, an LLM should appropriately re-weight latent algorithms. The domain of random sequences reflects this framing, in contrast to other behavioral evaluation methodologies, in that there is no correct answer to a random number generation or judgment task (Fig. 1). If the correct behavior is to match a target random process, then the right way to respond toa prompt _Generate \(N\) flips from a fair coin_ is at best a uniform distribution over the tokens Heads and Tails, instead of a specific sequence, or a more complex algorithm that matches human behavior.

## 2 Background

Bayesian Inference and In-Context LearningA key methodological tool of cognitive modeling, recent work has also framed in-context learning as Bayesian inference over models [19; 59; 60]. Specifically, the posterior predictive distribution \(p(y|x)\) in these works describes how an LLM produces output tokens \(y\), given the context, or _prompt_, \(x\). The key assumption is that a context \(x\) will activate latent concepts \(c\) within a model according to their posterior probability \(p(c|x)\), which the model marginalizes over to produce the next token \(y\) by sampling from the posterior predictive distribution: \(p(y|x)=_{c}p(y|c)\ p(c|x)\). This model selection takes place in network activation dynamics, without changing its weights. In our experiments, we assume a hypothesis space \(\) that approximates the latent space of LLM concepts \(\) used when predicting the next token, i.e., \(p(y|x)=_{h}p(y|h)\ p(h|x),\) where \(_{h}\) can be changed to \(_{h}\) to represent deterministic greedy decoding with an LLM temperature parameter of 0. We specifically focus on Bernoulli processes, regular languages, Markov chains, and a simple memory-constrained probabilistic model as candidates for the hypothesis space \(\) for estimating LLM concepts in random binary sequences. We use a subset of regular languages \((x)^{n}\), where \((x)\) is a short sequence of values, e.g., \(()^{n}\), where \(0\) maps to Heads and \(1\) to Tails.

Algorithmic and Subjective RandomnessCognitive scientists studying _Subjective Randomness_ model how people perceive randomness, or generate data that is subjectively random but algorithmically pseudo-random [61; 62; 29; 63; 64]. In a bias termed _the Gambler's Fallacy_, people reliably perceive binary sequences with long streaks of one value as less random, and judge binary sequences with higher-than-chance alternation rates as being "more random" than truly random sequences [65; 66]. One way to study subjective randomness is to ask people whether a given data sequence was more likely to be generated by a Random process or a Non-Random process (Fig. 1). While the posterior distribution of all non-random processes includes every possible computable function, estimating this distribution can be simplified to finding the single most probable algorithm to approximate the full hypothesis space. If the hypotheses are data-generating programs, a natural prior \(p(h)\) is to assign higher probabilities to programs with shorter description lengths, or lower complexity. This optimization problem is equivalent to computing the Kolmogorov complexity of a sequence \(K(x)\) and has motivated the use of "simplicity priors" in a number of domains in computational cognitive science [54; 56; 30]. Following previous work [28; 29], here we define subjective randomness of a sequence as the ratio of likelihood of that sequence under a random versus non-random model, i.e., \((x)= P(x|)\ -\  P(x|)\). The non-random likelihood \(p(x|)=2^{-K(x)}\) denotes the probability of the minimal description length program that generates \(x\), equivalent to Bayesian model selection: \(P(x|)=_{h}p(x|h)\ p(h)\). In this work, we study a small subset of \(\), which includes formal languages and probabilistic models inspired by psychological models of human concept learning and subjective randomness [66; 29; 68].

Figure 1: **Overview of our modeling framework.** (Left) Given a pre-trained LLM, we systematically vary input context prompts \(x\). LLM outputs \(y\) vary as a function of \(x\), based on some unknown latent concept space embedded in the LLM. With very little context (\(x=0\)), GPT-3.5+ generates subjectively random sequences, whereas with adequate context matching a simple formal language (\(x=\)), behavior becomes deterministic (\(()^{n}\)). (Right) Deciding whether a sequence is random can be viewed as search for a simple program that could generate that sequence. HTTTTTT is described with a short program simpleSequence with higher \(p(h)\) according to a simplicity prior, compared to THTHTHTH and complexSequence. Both sequences can be generated by randomSequence, with lower likelihood \(p(x|h)\)

## 3 Experiments

Randomness Generation and Judgment TasksIn order to assess text generation dynamics and in-context concept learning, we evaluate LLMs on random sequence **Generation** tasks, analyzing responses according to simple interpretable models of _Subjective Randomness_ and _Formal Language Learning_. In these tasks, the model generates a sequence \(y\) of binary values, or _flips_, comma-separated sequences of Heads or Tails tokens. We also analyze a smaller set of randomness **Judgment** tasks, where the prompt includes a sequence of flips, and the model must respond whether the sequence was generated by Random or Non-Random process. In both cases, \(y\) is a distribution over tokens with two possible values: Random or Non in Judgment tasks, indicating whether the sequence was generated by a random process with no correlation, or some non-random algorithm. We analyze dynamics in LLM-generated sequences \(y\) simulating a weighted coin with specified \(p()\), with \(|x| 0\).

Subjective Randomness ModelsWe compare LLM-generated sequences to a ground truth "random" Bernoulli distribution with the same mean (\(=_{}\)), to a simple memory-constrained probabilistic model, and to Markov chains fit to model-generated data \(y\). Hahn and Warren  theorize that the Gambler's Fallacy emerges as a consequence of human memory limitations, where'seeming biases reflect the subjective experience of a finite data stream for an agent with a limited short-term memory capacity'. We formalize this as a simple _Window Average_ model, which tends towards a specific probability \(p\) as a function of the last \(w\) flips: \(p(y|x)=(0,(1,2p-_{t-w...t}))\).

Sub-Sequence Memorization and Complexity MetricsBender et al.  raise the question of whether LLMs are'stochastic parrots' that simply copy data from the training set. To measure memorization, we look at the distribution of unique sub-sequences in \(y\). If an LLM is repeating common patterns across outputs, potentially memorized from the training data, this should be apparent in the distribution over length K sub-sequences. Since there are deep theoretical connections between complexity and randomness [70; 71], we also consider the complexity of GPT-produced sequences. Compression is a metric of information content, and thus of redundancy over irreducible complexity [72; 73], and neural language models have been shown to prefer generating low complexity sequences . As approximations of sequence complexity, we evaluate the distribution of Gzip-compressed file sizes  and inter-sequence Levenshtein distances .

Formal Language Learning MetricsIn our Formal Language Learning analysis, \(x\) is a subset of regular expression repetitions of short token sequences such as \(x()^{n}\), where longer sequences \(x\) correspond to larger \(n\). This enables us to systematically investigate in-context learning of formal languages, as \(|x|\) corresponds to the amount of data for inducing the correct program (e.g. \(()^{n}\)) out of the space of possible algorithms. In Randomness Judgment tasks, we assess formal concept learning by the dynamics of \(p(y=|x=C^{|x|})\) as a function of \(|x|\). In Randomness Generation tasks, we asses concept learning according to the language model predictive distribution \(p(y|x)\) over output sequences, inferred from next-token generation data: \(p(y_{0...T}|x)=p(y_{0}|x)_{i}^{T}p(y_{i}|y_{0...t-1},x)\). Given a space of possible outputs \(y\) with length \(d\), \(y\{0,1\}^{d}\), we estimate \(p(y|x)\) by enumerating all \(y\) up to some depth \(d\), and computing \((y_{d}|x,y_{1...d-1})=_{i}^{N}(y_{d}==1)^{(i)}\) as the fraction of \(N\) responses that are "Tails" (or equivalently, by using token-level probabilities directly). We estimate the predictive probability \(p(y_{t} C|x,y_{0...t-1})\) assigned to a given regular language by computing the total probability mass for all trajectories in \(y_{0...d}\) that exactly match \(C\). For example, with \(C=()^{n}\), there will be 3 trajectories \(y_{0...d}\) that exactly match \(C\), out of \(2^{d}\) possible.

Figure 2: **GPT-3.5 generates pseudo-random binary sequences that deviate from a Bernoulli process.** (Left) Running averages of \(p()\) for flips generated by each model. Compared to a Bernoulli process, sequences generated by GPT and our Window Average model stay closer to the mean. (Right) GPT-3.5 shows a Gamblerâ€™s Fallacy bias, avoiding long runs of the same value in a row.

## 4 Results

**Subjectively Random Sequence Generation**

In 'InstructGPT' models -- text-davinci-003, ChatGPT (gpt-3.5-turbo, gpt-3.5-turbo-instruct) and GPT-4 -- we find an emergent behavior of generating seemingly random binary sequences (Fig. 2). _This behavior is controllable_, where different \(p()\) values leads to different means of generated sequences \(\). However, the distribution of sequence means, as well as the distribution of the length of the longest runs for each sequence, deviate significantly from a Bernoulli distribution centered at \(\), analogous to the Gambler's Fallacy bias in humans. _Our Window Average model with a window size of \(w=5\) partly explains both biases_, matching GPT-generated sequences more closely than a Bernoulli distribution. Our cross-LLM analysis shows that text-davinci-003 is controllable with \(P()\), with a bias towards \(=.50\) and higher variance in sequence means (though lower variance than a true Bernoulli process). ChatGPT (gpt-3.5-turbo-0301 and 0613) are similar for \(P()<50\%\), but behave erratically with higher \(P()\), with most \(y\) repeating 'Tails'. GPT-4 (_0301_, _0613_) shows stable, controllable subjective randomness behavior, with lower variances than text-davinci-003. Earlier models do not show subjective randomness behavior. Also see Appendix.

**Sub-Sequence Memorization and Complexity**

We find significant differences between the distributions of sub-sequences for GPT-3.5 -generated sequences and sequences sampled from a Bernoulli distribution (see Appendix for figures). This difference is partly accounted for with a Window Average model with a window size \(w=5\), although GPT repeats certain longer sub-sequences, for example length-20 sub-sequences, that are far longer than 5. However, the majority of sub-sequences have very low frequency, and though further experiments would be required to conclude that all sub-sequences are not memorized from training data, it seems unlikely that these were in the training set, since we find thousands of unique length-k (with varying k) sub-sequences generated at various values of \(P()\). _This indicates that GPT-3.5 combines dynamic, subjectively random sequence generation with distribution-matched memorization_. Across three metrics of sequence complexity -- number unique sub-sequences, Gzip file size, and inter-sequence Levenshtein distance -- we find that _GPT-3.5+ models, with the exception of ChatGPT, generate low complexity sequences_, showing that structure is repeated across sequences and supporting prior work .

### Distinguishing Formal Languages from Randomness

_GPT-3.5 sharply transitions between behavioral patterns, from generating pseudo-random values to generating non-random sequences that perfectly match the formal language_ (Fig. 3). We observe a consistent pattern of formal language learning in GPT-3.5 generating random sequences where predictions \(p(y|x)\) of depth \(d 4\) are initially random with small \(|x|\), and have low \(p(y C|x)\) where \(C\) is a given concept. This follows whether the prompt describes the process as samples from "a weighted coin" or "a non-random-algorithm". We also find _sharp phase changes in GPT-3.5 behavioral patterns in Randomness Judgment tasks across 9 binary concepts_ (Fig. 3). These follow a stable pattern of being highly confident in that the sequence is Random (high \(p(y=|x)\) when \(x\) is low, up to some threshold of context at which point it rapidly transitions to being highly confident in the process being non-random. Transition points vary between concepts, but the pattern is similar across concepts (see additional figures in Appendix).

Figure 3: **Sharp transitions in predictive distributions for Randomness Judgment and Generation** (Left) In Randomness Judgment tasks, the predictive distribution \(p(y=|x)\) for text-davinci-003 transitions from high confidence in \(x\) being generated by a random process, to high confidence in a non-random algorithm (Right) in Generation tasks, the predictive \(p(y=|x)\) transitions from pseudo-randomness to deterministic repetition of a particular concept; text-davinci-003 is solid, gpt-3.5-turbo-instruct dashed.