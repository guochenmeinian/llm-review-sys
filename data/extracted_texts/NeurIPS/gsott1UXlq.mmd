# Transparent Networks for Multivariate Time Series

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Transparent models, which are machine learning models that produce inherently interpretable predictions, are receiving significant attention in high-stakes domains. However, despite much real-world data being collected as time series, there is a lack of studies on transparent time series models. To address this gap, we propose a novel transparent neural network model for time series called Generalized Additive Time Series Model (GATSM). GATSM consists of two parts: 1) independent feature networks to learn feature representations, and 2) a transparent temporal module to learn temporal patterns across different time steps using the feature representations. This structure allows GATSM to effectively capture temporal patterns and handle dynamic-length time series while preserving transparency. Empirical experiments show that GATSM significantly outperforms existing generalized additive models and achieves comparable performance to black-box time series models, such as recurrent neural networks and Transformer. In addition, we demonstrate that GATSM finds interesting patterns in time series. The source code is available at https://anonymous.4open.science/r/GATSM-78F4/.

## 1 Introduction

Artificial neural networks excel at learning complex representations and demonstrate remarkable predictive performance across various fields. However, their complexity makes interpreting the decision-making processes of neural network models challenging. Consequently, post-hoc explainable artificial intelligence (XAI) methods, which explain the predictions of trained black-box models, have been widely studied in recent years [1; 2; 3; 4]. XAI methods are generally effective at providing humans with understandable explanations of model predictions. However, they may produce incorrect and unfaithful explanations of the underlying black-box model and cannot provide actual contributions of input features to model predictions [5; 6]. Therefore, their applicability to high-stakes domains-such as healthcare and fraud detection, where faithfulness to the underlying model and actual contributions of features are important-is limited.

Due to these limitations, transparent (i.e., inherently interpretable) models are attracting attention as alternatives to XAI in high-stakes domains [7; 8; 9]. Modern transparent models typically adhere to the _generalized additive model_ (GAM) framework . A GAM consists of independent functions, each corresponding to an input feature, and makes predictions as a linear combination of these functions (e.g., the sum of all functions). Therefore, each function reflects the contribution of its respective feature. For this reason, interpreting GAMs is straightforward, making them widely used in various fields, such as healthcare [11; 12], survival analysis , and model bias discovery [7; 14; 15]. However, despite much real-world data being collected as time series, research on GAMs for time series remains scarce. Consequently, the applicability of GAMs in real-world scenarios is still limited.

To overcome this limitation, we propose a novel transparent model for multivariate time series called Generalized Additive Time Series Model (GATSM). GATSM consists of independent feature networks to learn feature representations and a transparent temporal module to learn temporal patterns.

Since employing distinct networks across different time steps requires a massive amount of learnable parameters, the feature networks in GATSM share the weights across all time steps, while the temporal module independently learns temporal patterns. GATSM then generates final predictions by integrating the feature representations with the temporal information from the temporal module. This strategy allows GATSM to effectively capture temporal patterns and handle dynamic-length time series while preserving transparency. Additionally, this approach facilitates the separate extraction of time-independent feature contributions, the importance of individual time steps, and time-dependent feature contributions through the feature functions, temporal module, and final prediction. To demonstrate the effectiveness of GATSM, we conducted empirical experiments on various time series datasets. The experimental results show that GATSM significantly outperforms existing GAMs and achieves comparable performances to black-box time series models, such as recurrent neural networks and Transformer . In addition, we provide visualizations of GATSM's predictions to demonstrate that GATSM finds interesting patterns in time series.

## 2 Related Works

Various XAI studies have been conducted over the past decade [7; 8; 9; 17; 18]; however, they are less relevant to the transparent model that is the subject of this study. Therefore, we refer readers to [19; 20] for more detailed information on recent XAI research. In this section, we review existing transparent models closely related to our GATSM and discuss their limitations.

The simple linear model is designed to fit the conditional expectation \(g((y))=_{i=1}^{M}x_{i}w_{i}\), where \(g()\) is a link function, \(M\) indicates the number of input features, \(y\) is the target value for the given input features \(^{M}\), and \(w_{i}\) is the learnable weight for \(x_{i}\). This model captures only linear relationships between the target \(y\) and the inputs **x**. To address this limitation, GAM  extends the simple linear model to the generalized form as follows:

\[g((y))=_{i=1}^{M}f_{i}( x_{i}),\] (1)

where each \(f_{i}()\) is a function that models the effect of a single feature, referred as a feature function. Typically, \(f_{i}()\) becomes a non-linear function such as a decision tree or neural network to capture non-linear relationships.

Originally, GAMs were fitted via the backfitting algorithm using smooth splines [10; 21]. Later, Yin Lou et al.  and Harsha Nori et al.  have proposed boosted decision tree-based GAMs, which use boosted decision trees as feature functions. Spline- and tree-based GAMs have less flexibility and scalability. Thus, extending them to transfer or multi-task learning is challenging. To overcome this problem, various neural network-based GAMs have been proposed in recent years. Potts  introduced generalized additive neural network, which employs 2-layer neural networks as feature functions. Similarly, Rishabh Agarwal et al.  proposed neural additive model (NAM) that employs multi-layer neural networks. To improve the scalability of NAM, Chun-Hao Chang et al.  and Filip Radenovic et al.  proposed the neural oblivious tree-based GAM and the basis network-based GAM, respectively. Xu et al.  introduced a sparse version of NAM using the group LASSO. One disadvantage of GAMs is their limited predictive power, which stems from the fact that they only learn first-order feature interactions-i.e., relationships between the target value and individual features. To address this, various studies have been conducted to enhance the predictive powers of GAMs by incorporating higher-order feature interactions, while still maintaining transparency. GA\({}^{2}\)M  simply takes pairwise features as inputs to learn pairwise interactions. GAMI-Net , a neural network-based GAM, consists of networks for main effects (i.e., first-order interactions) and pairwise interactions. To enhance the interpretability of GAMI-Net, the sparsity and heredity constraints are added, and trivial features are pruned in the training process. Sparse interaction additive network 

    & Time series input & Temporal pattern & Dynamic time series \\  existing GAMs & & & \\ NATM & ✓ & & \\ GATSM (our) & ✓ & ✓ & ✓ \\   

Table 1: Advantages of GATSM.

is a 3-phase method for exploiting higher-order interactions. Initially, a black-box neural network is trained; subsequently, the top-\(k\) important features are identified using explainable feature attribution methods like LIME  and SHAP , and finally, NAM is trained with these extracted features. Dubey et al.  introduced scalable polynomial additive model, an end-to-end model that learns higher-order interactions via polynomials. Similarly, Kim et al.  proposed higher-order NAM that utilizes the feature crossing technique to capture higher-order interactions. Despite their capabilities, the aforementioned GAMs cannot process time series data, which limits their applicability in real-world scenarios. Recently, neural additive time series Model (NATM) , a time-series adaptation of NAM, has been proposed. However, NATM handles each time step independently with separate feature networks. This approach cannot capture effective temporal patterns and only takes fixed-length time series as input. Our GATSM not only captures temporal patterns but also handles dynamic-length time series. Table 1 shows the advantages of our GATSM compared to existing GAMs.

## 3 Problem Statement

We tackle the problem of the existing GAMs on time series. Equation (1) outlines the GAM framework for tabular data, which fails to capture the interactions between current and previous observations in time series. A straightforward method to extend GAM to time series, adopted in NATM, is applying distinct feature functions to each time step and summing them to produce predictions:

\[g((y_{t}_{:t}))=_{i=1}^{t} _{j=1}^{M}f_{i,j}(x_{i,j}),\] (2)

where \(^{T M}\) is a time series with \(T\) time steps and \(M\) features, and \(t\) is the current time step. This method can handle time series data as input but fails to capture effective temporal patterns because the function \(f_{i,j}()\) still does not interact with previous time steps. To overcome this problem,

Figure 1: Architecture of GATSM.

we suggest a new form of GAM for time series defined as follows:

\[g((y_{t}_{:t}))=_{i=1}^{t}_{ j=1}^{M}f_{i,j}(x_{i,j},_{:t}).\] (3)

**Definition 3.1**_GAMs for time series, which capture temporal patterns hold the form of Equation 3._

In Equation (3), the function \(f(,)\) can capture interactions between current and previous time steps. Therefore, GAMs adhering to Definition 3.1 are capable of capturing temporal patterns. However, implementing such a model while maintaining transparency poses challenges. In the following section, we will describe our approach to implementing a GAM that holds Definition 3.1. To the best of our knowledge, no existing literature addresses Definition 3.1.

## 4 Our Method: Generalized Additive Time Series Model

### Architecture

Figure 1 shows the overall architecture of GATSM. Our model has two modules: 1) feature networks, called time-sharing neural basis model, for learning feature representations, and 2) masked multi-head attention for learning temporal patterns.

**Time-Sharing NBM:** Assume a time series with \(T\) time steps and \(M\) features. Applying GAMs to this time series necessitates \(T M\) feature functions, which becomes problematic when dealing with large \(T\) or \(M\) due to increased model size. This limits the applicability of GAMs to real-world datasets. To overcome this problem, we extend neural basis model (NBM)  to time series as:

\[_{i,j}=f_{j}(x_{i,j})=_{k=1}^{B}h_{k}(x_{i,j} )w_{j,k}^{nbm}.\] (4)

We refer to this extended version of NBM as time-sharing NBM. Time-sharing NBM has \(B\) basis functions, with each basis \(h_{k}()\) taking a feature \(x_{i,j}\) as input. The feature-specific weight \(w_{j,k}^{nbm}\) then projects the basis to the transformed feature \(_{i,j}\). As depicted in Equation 4, the basis functions are shared across all features and time steps, drastically reducing the number of required feature functions \(T M\) to \(B\). We use \(B=100\) and implement \(h_{k}()\) using multi-layer perceptron (MLP).

**Masked MHA:** GATSM employs multi-head attention (MHA) to learn temporal patterns. Although the dot product attention  is popular, simple dot operation has low expressive power . Therefore, we adopt the 2-layer attention mechanism proposed by  to GATSM. We first transform \(}_{i}=[_{i,1},_{i,2},,_ {i,M}]^{M}\) produced by Equation 4 as follows:

\[_{i}=}_{i}^{}+_{i},\] (5)

where \(^{M D}\) is a learnable weight, \(_{i}=[pe_{i,1},pe_{i,2},,pe_{i,D}] ^{D}\) is the positional encoding for \(i\)-th step, and \(D\) indicates the hidden size. The positional encoding is defined as follows:

\[pe_{i,j}=(})&j2=1,\\ (})&.\] (6)

The positional encoding helps GATSM effectively capture temporal patterns. While learnable position embedding also works in GATSM, we recommend positional encoding because position embedding requires knowledge of the maximum number of time steps, which is often unknown in real-world settings. After computing \(_{i}\), we calculate the attention scores as follows:

\[e_{k,i,j}=([_{i}_{j} ]^{}_{k}^{attn})m_{i,j},\] (7) \[a_{k,i,j}=(e_{k,i,j})}{_{t=1}^{T }(e_{k,i,t})},\] (8)

where \(k\) is attention head index, \(()\) is an activation function, \(_{k}^{attn}^{2D}\), and \(m_{i,j}\) is the mask value used to block future information. The time mask is defined as follows:

\[m_{i,j}=1&i j,\\ -&.\] (9)

**Inference:** The prediction of GATSM is produced by combining the transformed features from time-sharing NBM with the attention scores from masked MHA.

\[_{t}=_{k=1}^{K}_{k,t}^{}}_{k}^{out},\] (10)

where \(K\) is the number of attention heads, \(_{k,t}=[a_{k,i,1},a_{k,i,2},,a_{k,i,T}]^{T}\) is the attention map in Equation 8, \(}=[}_{1},}_{2},, }_{T}]^{T M}\) is the transformed features in Equation 4, and \(_{k}^{out}^{M}\) is the learnable output weight.

**Interpretability:** We can rewrite Equation 10 as the following scalar form:

\[_{k=1}^{K}_{k,t}^{}} _{k}^{out} =_{u=1}^{t}_{m=1}^{M}_{k=1}^{K}_{b=1}^{B}a_{k,t,u} h_{b}(x_{t,m})w_{m,b}^{bm}w_{k,m}^{out}\] (11) \[=_{u=1}^{t}_{m=1}^{M}f_{u,m}(x_{u,m},_{t})\]

Equation 11 shows that GATSM satisfying Definition 3.1. We can derive three types of interpretations from GATSM: 1) \(a_{k,t,u}\) indicates the importance of time step \(u\) at time step \(t\), 2) \(h_{b}(x_{t,m})w_{m,b}^{nbm}w_{k,m}^{out}\) represents the time-independent contribution of feature \(m\), and 3) \(a_{k,t,u}h_{b}(x_{t,m})w_{m,b}^{nbm}w_{k,m}^{out}\) represents the time-dependent contribution of feature \(m\) at time step \(t\).

## 5 Experiments

### Experimental Setup

**Datasets:** We conducted our experiments using eight publicly available real-world time series datasets. From the Monash repository , we sourced three datasets: Energy, Rainfall, and AirQuality. Another three datasets, Heartbeat, LSST, and NATOPS, were downloaded from the UCR repository . The remaining two datasets, Mortality and Sepsis, were downloaded from the PhysioNet . We perform ordinal encoding for categorical features and standardize features to have zero-mean and unit-variance. For forecasting tasks, target value y is also standardized to zero-mean and unit-variance. If the dataset contains missing values, we impute categorical features with their modes and numerical features with their means. The dataset is split into a 60%/20%/20% ratio for training, validation, and testing, respectively. Table 2 shows the statistics of the experimental datasets. Further details of the experimental datasets can be found in Appendix B.

**Baselines:** We compare our GATSM with 12 baselines, which can be categorized into four groups: 1) Black-box tabular models include extreme gradient boosting (XGBoost)  and MLP. 2) Black-box time series models include simple recurrent neural network (RNN), gated recurrent unit (GRU), long short-term memory (LSTM), and Transformer . 3) Transparent tabular models are simple linear model (Linear), explainable boosting machine (EBM) , NAM , NodeGAM , and NBM . 4) NAM  is a transparent time series model.

**Implementation:** We implement XGBoost and EBM models using the xgboost and interpretml libraries, respectively. For NodeGAM, we employ the official implementation provided by its authors . The remaining models are developed using PyTorch . All models undergo hyperparameter

   Dataset & Task & Variable length & \# of time series & Avg. length & \# of features & \# of classes \\  Energy & 1-step FCT & No & 137 & 24 & 24 & - \\ Rainfall & 1-step FCT & No & 160,267 & 24 & 3 & - \\ AirQuality & 1-step FCT & No & 16,966 & 24 & 9 & - \\ Heartbeat & Binary & No & 409 & 405 & 61 & 2 \\ Mortality & Binary & Yes & 12,000 & 49.861 & 41 & 2 \\ Sepsis & Binary & Yes & 40,336 & 38.482 & 40 & 2 \\ LSST & Multi-class & No & 4,925 & 36 & 6 & 14 \\ NATOPS & Multi-class & No & 360 & 51 & 24 & 6 \\    \\   

Table 2: Dataset statistics.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

Figure 4: Local time-independent feature contributions.

Figure 5: Local time-dependent feature contributions.

**Time-step importance:** We plot the average attention scores at the last time step \(T\) in Figure 2. The process for extracting the average attention score of time step \(u\) at time step \(t\) is formalized as \(_{k=1}^{K}a_{k,t,u}\). This process is repeated over all data samples, and the results are averaged. Based on Figure 2, it seems that GATSM pays more attention to the initial and last states than to the intermediate states. This indicates that the current concentration of particulate matter depends on the initial state.

**Global feature contribution:** Figure 3 illustrates the global behavior of features in the AirQuality dataset, with red bars indicating the density of training samples. We extract \(_{k=1}^{K}h_{b}(x_{t,m})w_{m,b}^{bm}w_{k,m}^{out}\) from GATSM and repeat this process over the range of minimum to maximum feature values to plot the line. We found that the behavior of _SO2_, _O3_, and _windspeed_ is inconsistent with prior human knowledge. Typically, high levels of _SO2_ and _O3_ are associated with poor air quality. However, GATSM learned that particulate matter concentration starts to decrease when _SO2_ exceeds 10 and _O3_ exceeds 5. This discrepancy may be due to sparse training samples in these regions, leading to insufficient training, or there may be interactions with other features. Another known fact is that high _windspeed_ decreases particulate matter concentration. This is consistent when _windspeed_ is below 0.7 in our observation. However, particulate matter concentration drastically increases when _windspeed_ exceeds 0.7, likely due to the wind causing yellow dust.

**Local time-independent feature contribution:** To interpret the prediction of a data sample, we plot the local time-independent feature contributions, \(_{k=1}^{K}h_{b}(x_{t,m})w_{m,b}^{nbm}w_{k,m}^{out}\), in Figure 4. The main x-axis (blue) represents feature contribution, the sub x-axis (red) represents feature value, and the y-axis represents time steps. We found that _SO2_, _NO2_, _CO_, and _O3_ have positive correlations. In contrast, _temperature_, _pressure_, _ dew point_, and _windspeed_ have negative correlations. These are consistent with the global interpretations shown in Figure 3. Rainfall has the same values across all time steps.

**Local time-dependent feature contribution:** We also visualize the local time-dependent feature contributions, \(_{k=1}^{K}a_{k,t,u}h_{b}(x_{t,m})w_{m,b}^{nbm}w_{k,m}^{out}\). Figure 5 illustrates the interpretation of the same data sample as in Figure 4. The time-dependent interpretation differs slightly from the time-independent interpretation. We found that there are time lags in _SO2_, _NO2_, _CO_, and _O3_, meaning previous feature values affect current feature contributions. For example, in the case of _SO2_, low feature values around time step 5 lead to low feature contributions around time step 13.

## 6 Future Works & Conclusion

Although GATSM achieved state-of-the-art performance within the transparent model category, it has several limitations. This section discusses these limitations and suggests future work to address them. GAMs have relatively slower computational times and larger model sizes compared to black-box models because they require the same number of feature functions as input features. To address this problem, methods such as the basis strategy can be proposed to reduce the number of feature functions, or entirely new methods for transparent models can be developed. The attention mechanism in GATSM may be a bottleneck. Fast attention mechanisms proposed in the literature [39; 40; 41; 42; 43], or the recently proposed Mamba , can help overcome this limitation. Existing time series models, including GATSM, only handle discrete time series and have limited length generalization ability, resulting in significantly reduced performance when very long sequences, unseen during training, are input. Extending GATSM to continuous models using NeuralODE  or HiPPO  could address this issue. GATSM still cannot learn higher-order feature interactions internally and shows low performance on complex datasets. Feature interaction methods proposed for transparent models may help address this problem [29; 15].

In this papre, we proposed a novel transparent model for time series named GATSM. GATSM consists of time-sharing NBM and the temporal module to effectively learn feature representations and temporal patterns while maintaining transparency. The experimental results demonstrated that GATSM has superior generalization ability and is the only transparent model with performance comparable to Transformer. We provided various visual interpretations of GATSM, demonstrated that GATSM capture interesting patterns in time series data. We anticipate that GATSM will be widely adopted in various fields and demonstrate strong performance. The broader impacts of GATSM across various fields can be found in Appendix A.