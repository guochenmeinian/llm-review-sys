# Point-Cloud Completion with Pretrained Text-to-image Diffusion Models

Yoni Kasten\({}^{1}\)

Ohad Rahamim\({}^{2}\)

\({}^{1}\)NVIDIA Research

\({}^{2}\)Bar-Ilan University

Gal Chechik\({}^{1,2}\)

\({}^{1}\)NVIDIA Research

\({}^{2}\)Bar-Ilan University

###### Abstract

Point-cloud data collected in real-world applications are often incomplete, because objects are being observed from specific viewpoints, which only capture one perspective. Data can also be incomplete due to occlusion and low-resolution sampling. Existing approaches to completion rely on training models with datasets of predefined objects to guide the completion of point clouds. Unfortunately, these approaches fail to generalize when tested on objects or real-world setups that are poorly represented in their training set. Here, we leverage recent advances in text-guided 3D shape generation, showing how to use image priors for generating 3D objects. We describe an approach called SDS-Complete that uses a pre-trained text-to-image diffusion model and leverages the text semantics of a given incomplete point cloud of an object, to obtain a complete surface representation. SDS-Complete can complete a variety of objects using test-time optimization without expensive collection of 3D data. We evaluate SDS-Complete on a collection of incomplete scanned objects, captured by real-world depth sensors and LiDAR scanners. We find that it effectively reconstructs objects that are absent from common datasets, reducing Chamfer loss by about 50% on average compared with current methods. Project page: _https://sds-complete.github.io/_

## 1 Introduction

Modeling 3D objects and scenes is becoming a central part of machine perception. Most 3D data is collected using sensors that capture the 3D structure of various objects, like LIDAR or depth scanners.

Figure 1: We present SDS-Complete: A test-time optimization method for completing point clouds captured by depth sensors, leveraging pre-trained text-to-image diffusion model. The inputs to our method are an incomplete point cloud (blue) along with a textual description of the object. The output is a complete surface (gray) that is consistent with the input points (blue). The method works well on a variety of objects captured by real-world point-cloud sensors.

When used in the real world, various factors may cause scanners to capture incomplete or partially sampled 3D objects. First and foremost, objects are often captured from specific camera viewpoints, collecting points from only "one side" of an object (Figure 1). Reconstruction may also suffer from self-occlusions and low sensor resolution. To fully understand the three-dimensional world, one must deal with partial data and missing parts.

Current approaches for completing partial point clouds  operate as follows. They first gather extensive training datasets of complete 3D objects (e.g., ); then, they extract partial point clouds from the 3D objects as training data; finally, they train deep models to predict completed objects using their known ground-truth geometry. These methods are commonly evaluated on test partitions derived from their training data and consistently show high accuracy. However, they tend to generalize poorly to real data, due to several reasons. First, the training data primarily consists of Computer-Aided Designed (CAD) models, which differ from real-world objects . Second, points are sampled in an artificial way, which does not accurately simulate real-world capture processes. Finally, the training data is mostly aligned , which can pose challenges when dealing with real-world data that is typically not perfectly aligned with the training data. The performance of these models deteriorates even further for objects and shapes that were not observed during training (label shift). This is a severe problem because the diversity of shapes in existing datasets of 3D objects is very limited. Recent work  attempted to address this limitation by expanding the range of object categories. However, the quality of completed objects is significantly decreased when applied to real-world data, as we illustrate below. The poor generalization of these methods to real data limits their practical use in real-world situations that demand 3D perception, such as indoor-scene reconstruction and autonomous driving.

Here, we address the challenge of completing 3D objects in the wild from real-world partial point clouds. This is achieved by leveraging priors about object shapes that are encoded in pretrained text-to-image diffusion models. Our key idea is that since text-to-image diffusion models were trained on a vast number of diverse objects, they contain a strong prior about the shape and texture of objects, and that prior can be used for completing object missing parts. For example, given a partial point cloud, knowing that it corresponds to a chair can guide the completion process, because objects from this class are expected to exhibit some types of symmetries and parts that are captured in 2D images.

A similar intuition has been used for generating 3D objects "from scratch" (DreamFusion) . DreamFusion uses the SDS loss, which measures agreement between 2D model prior and renderings of the 3D shape. Unfortunately, naively applying the SDS loss to our problem of point cloud completion fails. This is because, as we show below, it does not combine well the hard constraints implied by the points collected from the sensor with the prior embedded in the diffusion model.

To address these challenges, we introduce SDS-Complete: a method to complete a given partial point cloud using several considerations. First, we use a Signed Distance Function (SDF) surface representation , and constrain the zero level set of the SDF to go through the input points. Second, we use information about areas with no collected points, to rule out object parts in these areas. Third, we use a prior about camera position and orientation and a curriculum of out-painting when sampling camera positions. Finally, we use the SDS loss  to incorporate prior guided by the class of an object on the rendered images.

We demonstrate that SDS-Complete generates completions for various objects with different shape types from two real-world datasets: the Redwood dataset , which contains various incomplete real-world depth camera partial scans, and the KITTI dataset , which contains object LiDAR scans from driving scenarios. In both cases, SDS-Complete outperforms the current state-of-the-art methods.

In summary, this paper makes the following contributions: (1) A formulation of point cloud completion as a test-time optimization problem, avoiding the cost of collecting large datasets of 3D geometries and training models. (2) A new approach to PC completion, which combines an empirical point-cloud, with image priors using an SDF surface representation. (3) A practical and unified approach to completing and preserving real-world captured 3D content from various depth sensors, (LiDAR or depth camera) all while incorporating prior knowledge of camera poses through a well-structured camera curriculum. (4) We demonstrate state-of-the-art completion results for diverse in-the-wild objects, captured by real-world sensors.

Related work

**Surface Completion from Point Clouds.** Over the last years approaches based on deep-networks [58; 50; 30; 38] have demonstrated remarkable capabilities in reconstructing objects from incomplete or partial inputs. Early attempts with neural networks [13; 14; 19; 46] used voxel grid representations of 3D geometries due to their straightforward processing with off-the-shelf 3D convolutional layers. While voxels proved to be useful, they suffer from a space complexity issue, as their representation grows cubically. Consequently, these methods can only generate shapes with limited resolution. In contrast, point cloud representations [15; 2] have been leveraged to model higher-resolution geometries using neural networks. Several methods [52; 57] use such techniques for predicting the completed point cloud given a partial input point cloud. However, to obtain a surface representation, a surface reconstruction technique  needs to be applied as a post-processing step, which can introduce additional errors. Recently, an alternative approach has emerged where the output surface is represented using neural representations [30; 37]. The advantage of these representations lies in their ability to represent the surface continuously without any discretization. [30; 38] trained deep neural networks and latent conditioned implicit neural representations on a dataset of predefined object classes , to perform point cloud completion.

While most deep methods for surface completion train a different model per object class, very recent methods have focused on training multi-class models, allowing for better generalization [53; 56]. PoinTr  uses a transformer encoder-decoder architecture for translating a given input point cloud into a set of point proxies. These point proxies are then converted into completed point clouds using FoldingNet . ShapeFormer  directly reconstructs surfaces from incomplete point clouds using a transformer. Recently,  combined point cloud and image inputs for completing the object shape. They further use the input image for applying weakly-supervised loss on the rendered output.

Other recent works [11; 27; 34] show progress in the task of shape completion given a partial surface, where  uses a transformer and autoregressive modeling, and [11; 27] employ diffusion processes that allow controlling the completion with text. However, these methods require a surface as input and cannot handle incomplete point clouds. Furthermore, their applicability is limited to the domain they are trained on.

In contrast to the above-mentioned methods, our method performs point cloud completion as a test-time optimization process using pre-trained available diffusion models, does not rely on any collection of 3D shapes for training, and works on much broader domains.

**3D models from text using 2D supervision.** Several approaches used large vision-and-language models like CLIP  to analyze and synthesize 3D objects. Text2Mesh , CLIP-Mesh  and DreamFields  present approaches for editing meshes, generating 3D models, and synthesizing NeRFs  respectively, based on input text prompts. The methods employ differentiable renderers to generate images while maximizing their similarity with the input text in CLIP space.

More directly relevant to the current paper are methods that build on diffusion models. Text-guided image diffusion models [43; 44; 6; 16] generate images based on text prompts, enabling control over the generated visual content. These 2D models can then be used to guide 3D object generation, an approach first presented by DreamFusion  and then further improved . Latent-NeRF  enables DreamFusion to run with higher-resolution images by optimizing NeRF with diffusion model features instead of RGB colors. TEXTure  and Text2Tex  use depth-aware text-to-image diffusion models to synthesize textures for meshes. Other recent works predict shapes directly from 2D images [41; 48; 29]. In contrast, our method uses the input text for completing partial point clouds, rather than editing or synthesizing 3D content.

## 3 Method

### Problem Setup

We address the problem of completing a surface given incomplete point cloud measurements, captured by a real point cloud sensor. In contrast to previous works for object completion [56; 53] that first trained one feed-forward model on a large dataset of 3D shapes, we operate in a test-time-optimization, and solve each object separately from scratch without using any 3D dataset for pre-training.

**Inputs and components of our system.** The overall scheme for our method is depicted in Fig. 2. Two inputs are expected, (Fig. 2 left): a set of 3D input points \(P=\{_{1},_{2},,_{N}\}\) measured relative to the sensor's location, and a text description embedding \(\) of the incomplete object. As in previous methods , we assume that the point cloud is segmented out from the original scan, namely, that all the points in \(P\) belong to a single object.

Our task is to find the 3D surface (Fig. 2 center) of the complete object that is consistent with both the input points \(P\) and the text prompt \(\). For any given point cloud, our method optimizes for the complete object surface represented by a neural signed distance function \(f_{}:^{3}\), and a neural color function \(_{}:^{3}^{3}\), where \(\) and \(\) represent the learned parameters of the neural functions. As shown in , these two functions form a neural radiance field  and can be optimized using the rendered images of the 3D volumetric functions. More background details are given in the appendix.

### Overview of the Optimization Process

Our main goal is to reconstruct a surface that is consistent with the partial input point cloud \(P\), typically capturing only "one side" of the object. Clearly, constraining the surface to be consistent with the observed input point cloud is not sufficient for determining the surface on the "other side" of the object, and some prior knowledge should be used. Traditionally, such prior knowledge is learned by training a model over a large dataset of 3D shapes . Here, we instead use a pre-trained text-to-image diffusion model, applying an SDS loss  to rendered images of the object (Fig. 2 bottom). To ensure that we correctly reconstruct all sides of an input object, we use two types of compatibility losses: (1) **Sensor-compatibility losses (Fig. 2 top-right)**. (2) **Text-compatibility loss (Fig. 2 bottom-right)**. These losses are described below in more detail in Sec.3.3.

Applying and combining these compatibility losses is far from trivial and we now discuss several critical issues when using them. First, unlike DreamFusion , where each object is generated "from scratch", the _pose_ of a generated object in our setup is determined by the input points \(P\). As a result, camera positions must be sampled in a way that is compatible with the image prior, namely, that they render the object in natural poses.

Figure 2: SDS-Complete optimizes two neural functions: A signed distance function \(f_{}\) representing the surface and a volumetric coloring function \(_{}\). Together, \((_{},f_{})\) define a radiance field, which is used to render novel image views \(Im_{0}, Im_{n}\). The SDS loss is applied to the renderings to encourage them to be compatible with the input text \(\). Three sensor-compatibility losses verify that the reconstructed surface is compatible with the sensor observations in various aspects.

Second, we find that simply rendering the object from the "other side" (unobserved side) and applying the SDS loss tends to generate content that is compatible with the input text but not with the input point cloud. To address this, we define a "curriculum" of sampling camera poses. We start from the known original viewpoint of the sensor that captured the points and gradually increase the range of views that we sample from around the original view (details in the appendix). With this sampling protocol, generated completions are continuously kept compatible with both the description text and the point cloud generated so far, until the entire object is completed successfully. As shown in our ablation study, this camera sampling protocol is key for producing high-quality object completions.

We next describe in more detail the compatibility losses and the camera-sampling process.

### Training Losses

Sensor-compatibility losses.The sensor compatibility losses are used for constraining the output surface to be compatible with the input point cloud \(P\). The surface is defined as the zero-level set of our optimized neural SDF \(f_{}\). Therefore, for constraining the surface to go through the point cloud, we encourage the function to be zero at these points.

\[_{p}=_{i=1}^{N}|f_{}(_{i} )|.\] (1)

We note that the points in \(P\) are produced by a subset of sensor rays. Sensor rays that do not produce any points in \(P\) define constraints on the boundary of the object. We use this to define an additional sensor-compatibility loss \(_{}\). For each sensor ray \(i\), we denote its opacity by \(M_{i}\{0,1\}\) where \(M_{i}=1\) if the ray produces a point in \(P\) and \(M_{i}=0\) otherwise. We further denote its rendered opacity by \(_{i}\). The absence loss for the mask \(M\) is defined by:

\[_{}=_{i=1}^{K}|M_{i}-_{i}|\] (2)

where \(K\) is the number of sensor rays.

\(_{p}\) constrains the location of the surface, but it does not constrain the sign of the values around the surface which indicates on which side the interior of the object is located. For that, we use the fact that each point in \(P\) is measured relative to the sensor location. We denote the distance of each point from the sensor by \(D_{i}\) and the rendered distance of the ray that produced this point by \(_{i}\). Our distance loss is defined by:

\[_{}=_{i=1}^{N}\|D_{i}-_{i} \|^{2},\] (3)

Text-compatibility loss.We use a pre-trained text-to-image diffusion model, \(\), to provide a semantic prior for predicting the unobserved parts, such that any rendered image of the object would be compatible with the input text embedding \(\). To this end, we render random object views using our radiance field and apply the SDS loss with the input text embedding \(\) to optimize \(f_{}\) and \(_{}\) (Fig. 2, bottom-right). More details about the SDS loss are given in the appendix.

Regularization losses.To constrain \(f_{}\) to form a valid SDF, we apply the Eikonal loss regularization introduced in :

\[_{eikonal}=|}_{ P_{eik}}|\|  f_{}(_{i})\|-1|,\] (4)

where \(P_{eik}\) contains both \(P\) and uniformly sampled points from the region of interest.

Finally, we use the known world plane to further prevent the surface from drifting below the ground:

\[_{plane}=_{ P_{}}(-f _{}(),0),\] (5)

where \(P_{}\) is a set of uniformly sampled \(3D\) points below the plane in the region of interest.

Our total loss is:

\[_{}=_{m}_{m}+_{d}_{d}+ _{p}_{p}+_{eikonal}_{eikonal}+_{plane} _{plane}+_{},\] (6)

where \(_{m},_{d},_{p},_{eikonal}\) and \(_{plane}\) are the coefficients that define the weights of the different loss terms relative to the SDS loss and were selected by hyperparameters search. The same constant coefficients were used in all of our experiments. See appendix, for more implementation details.

### Handling Camera Positions

As discussed above, the protocol for sampling camera views has a large impact on the quality of final completions. Let \(C_{0}=(R_{0},_{0})\) be the original camera-to-world pose of the sensor. To prevent rendering flipped or unrealistically rotated images of the object, we define the azimuth and elevation deviation from \(C_{0}\) relative to the world plane. Specifically, let \(_{}^{2}\) be the normal to the world plane \(\), we define the azimuth rotation update to be \(R_{}=(_{},_{})\), where \((,)\) is the Rodrigues' rotation formula for a rotation around the unit vector \(\), with \(\) degrees. Similarly, let \(_{0}\) be the normalized principal axis direction of \(C_{0}\), we define the elevation rotation update by \(R_{}=(_{}_{0}, _{})\). Assuming that the origin is located at the center of the object, an updated camera, \(C_{}\), for \(_{}\) and \(_{}\) degrees, is given by:

\[C_{}=(R_{}R_{}R_{0},R_{}R_{}_{0}).\] (7)

During training, we start by applying the SDS loss to rendered images from \(C_{0}\) pose, and then we gradually increase the sampling range of the deviation angles until the entire object is covered. More implementation details are given in the appendix.

## 4 Experiments

Datasets.When considering evaluation, our primary goal is to evaluate our SDS-Complete and baseline method in real-world scenarios. This is in contrast to evaluating test splits from the synthetic datasets that were used for training the baseline methods. To achieve relevant evaluation datasets, we based the evaluation on partial real-world point clouds obtained from depth images and LiDAR scans.

For depth images, we used the Redwood dataset  that contains a diverse set of objects. We used depth images from \(14\) representative objects with ground truth \(360^{}\) reconstructions which enable quantitative evaluation. We further tested our model on the KITTI LiDAR dataset [7; 17], which contains incomplete point clouds of objects in real-world scenes captured by LiDAR sensors. Both, our method and the baselines, require a segmented point cloud as inputs. Therefore as a preprocessing, we segmented out and centralized the main object from each scene (see more technical details in the

Figure 3: **Qualitative results for the Redwood dataset. A qualitative comparison between our method and state-of-the-art methods. SDS-Complete produces more accurate completions.**

appendix). For each input object, the world's ground plane, \(^{3}\) that our method uses for camera sampling (Sec. 3.4) is extracted from each scene automatically by robust fitting. We also use the same world plane for the baseline methods as part of the point cloud to dataset alignment procedure that they require (see appendix for details).

Baselines:We compare our method to state-of-the-art point-cloud completion approaches: PoinTr , ShapeFormer , cGAN  and Sinv . PoinTr and ShapeFormer trained one model on multiple classes; cGAN and Sinv trained per-class models for chairs and tables classes.

### Results on the Redwood dataset

As our quality metric, we measure the Chamfer distances in millimeters to quantify the dissimilarity between the generated completions and their corresponding ground-truth shapes. Results are presented in Tables 1 and 2. Table 1 compares all methods on objects from two categories, tables and chairs; Table 2 compares our method to PoinTr  and ShapeFormer , on the remaining Redwood objects. Both tables demonstrate that SDS-Complete obtains state-of-the-art results on the task of point cloud completion from real point clouds.

In addition to the full variant discussed above, we also show results with a simplified model we name _"SDS-Complete Simple"_. This simplified variant omits the RGB network \(c_{}\), and uses gray-shaded rendering images of the optimized geometry for the SDS-loss. It is more efficient but has a higher average error. We further show qualitative results for general objects in Fig. 3. Qualitative results for chairs and tables are presented in the appendix.

Completion with various text descriptions.Our approach operates by combining a partial input point cloud with a text description that guides the model when completing missing parts of the object.

We tested the effect of changing the text prompt while keeping the same input point cloud. Figure 4 shows results for completing Redwood's scan "08754"(Teapot) of a partially captured teapot (Fig. 1). Completing the point cloud with other text descriptions demonstrates how the text controls the shape.

   Object &  Shape \\ Former \\  &  PoinTr \\  & cGAN & Sinv &  SDS-Complete \\ Full \\  \\  Old chair & 23.2 & 34.1 & 33.2 & 36.7 & 19.3 & **18.9** \\ Outside chair & 25.9 & 29.6 & 42.8 & 28.7 & 22.6 & **22.4** \\ One lag table & 39.7 & 21.6 & 99.4 & 24.9 & 20.3 & **18.1** \\ Executive chair & 33.6 & 43.9 & 208 & **20.6** & 23.7 & **22.0** \\  Average & 30.6 & 32.3 & 95.8 & 27.7 & 21.5 & **20.4** \\   Object &  Shape \\ Former \\  &  PoinTr \\  & 
 SDS-Complete \\ Full \\  \\  Trash can & 136.4 & 137 & **36.4** & 43.1 \\ Plant in a vase & 60.8 & 41 & 29.5 & **27.4** \\ Vespa & 79.4 & 70.3 & 57.6 & **35.7** \\ Tricycle & 65.2 & 60.4 & **39** & 41.3 \\ Couch & 43.9 & 87.4 & **36.5** & 50.1 \\ Office trash & 68.8 & 49.7 & 20.5 & **18.7** \\ Plant in a vase 2 & 31.3 & 37.6 & 28.1 & **26.3** \\ Park trash can & 130 & 119.9 & 33.8 & **26.4** \\ Bench & **29** & 32.6 & 55.4 & 98 \\ Sofa & 106.6 & 129.3 & **40.6** & 43.2 \\  Average & 75.1 & 76.5 & **37.8** & 41.0 \\  

Table 1: Chamfer loss (lower is better) for chair and table categories from the Redwood dataset. All baselines were trained on chairs and tables.

   Object &  Shape \\ Former \\  &  PoinTr \\  & 
 SDS-Complete \\ Full \\  \\  Trash can & 136.4 & 137 & **36.4** & 43.1 \\ Plant in a vase & 60.8 & 41 & 29.5 & **27.4** \\ Vespa & 79.4 & 70.3 & 57.6 & **35.7** \\ Tricycle & 65.2 & 60.4 & **39** & 41.3 \\ Couch & 43.9 & 87.4 & **36.5** & 50.1 \\ Office trash & 68.8 & 49.7 & 20.5 & **18.7** \\ Plant in a vase 2 & 31.3 & 37.6 & 28.1 & **26.3** \\ Park trash can & 130 & 119.9 & 33.8 & **26.4** \\ Bench & **29** & 32.6 & 55.4 & 98 \\ Sofa & 106.6 & 129.3 & **40.6** & 43.2 \\  Average & 75.1 & 76.5 & **37.8** & 41.0 \\  

Table 2: Chamfer loss (lower is better) for general objects from the Redwood dataset. Most of the object categories are new for all methods and were not observed during training.

Generic vs specific Text Prompts.To evaluate the contribution of selecting an appropriate text prompt per object, we repeated reconstruction experiments of the \(14\) objects that we evaluate in Tables 1 and 2, but varied the text prompts. Specifically, we used three levels of description specificity. First, for a fully generic prompt (class agnostic), we tested two alternatives: "An object", and "A thing". Second, we used the class name as the prompt ("A \(<\)class name\(>\)"). Finally, we used a more detailed description. The full-text prompts are given in the supplementary.

Table 3 shows the Chamfer distances between our reconstruction and the ground truth for all prompts. Using generic text yields inferior reconstructions. Adding specific details did not provide a significant improvement over using the class name. A qualitative comparison is shown in Fig. 5.

Ablation study.To evaluate the contribution of each component of our method we present qualitative and quantitative ablation studies in Fig. 6. As can be seen, without the SDS loss, our model has no understanding of object characteristics like the fact that the chair has four legs and a straight back-side. Without the SDF representation, it is not possible to apply the point cloud constraints directly on the surface which results in an inferior ability to follow the partial input. Finally, it can be seen that our camera sampling "curriculum" that is described in Sec. 3.4, improves the completion compared to a random camera sampling ("Naive camera sampling") by preserving the consistency of the generated content with the existing sensor measurements and by verifying that the diffusion model does not see any "unnatural" pose of the object.

### Results on the KITTI Dataset

We compare our method to ShapeFormer  and PoinTr  on a subset of \(15\) real object scans from the Semantic KITTI Dataset  which consists of 5 cars, 5 motorcycles, and 5 trucks. We present qualitative comparisons in Fig. 7. Notably, our method shows better completion results, particularly with motorcycle objects which are less frequent in the Shapenet dataset.

User Study.We conducted a user study to evaluate the various methods on the KITTI dataset. Specifically, we gathered a group of 11 participants to rank the quality of each completed surface and its faithfulness to the input partial point cloud. For each object, the participants were given three anonymous shapes produced by the three methods: SDS-Complete, ShapeFormer , and PoinTr

Figure 4: **The effect of text prompt. Completion of the same input, with different text descriptions. Results obtained with our method for the partial point cloud of scan "08754". While the handle and the top part of the object are constrained by the input point cloud, the model completes the other side of the object according to the input text.**

Figure 5: **Generic vs specific text prompts: qualitative results. Results are shown for reconstructing scan ”06127” (Plant in a vase) from the Redwood dataset. (a) The input point cloud. (b, c) Completion using two generic texts. Completion quality is poor. (d) Completion using the object class name. (e) Completion using a detailed textual description.**

. While the outputs of SDS-Complete and ShapeFormer are surfaces, PoinTr only outputs a point cloud. Therefore, we applied Screened Poisson Surface Reconstruction  to each output of PoinTr to base the user study comparisons on surface representations. The participants were instructed to choose the best shape, while the order of the methods was shuffled for each object. The best completion method for each input case is selected by the majority vote. The results of the user study are presented in Table 4, showing that our method got the highest number of wins (\(14\) out of \(15\)).

Quantitative evaluation on KITY.For a quantitative metric, we followed PCN  and calculated the Minimal Matching Distance (MMD). MMD is the Chamfer Distance (CD) between the output surface and the surface from ShapeNet that is closest to the input point cloud in terms of CD. We calculated this metric on the surfaces that were evaluated in our user study from two categories: car and motorcycle. These are the only categories that have associated Shapenet subsets which is a necessary condition for calculating the MMD metric. The mean MMD over the motorcycle and car shapes are presented in Table 4, showing that our approach improves over the baselines.

We further computed the CLIP R-Precision metric  on all of our evaluated KITTI categories: "car", "truck" and "motorcycle". This metric checks the accuracy of classifying a rendered image by choosing the class that maximizes the cosine similarity score between the image and the text: "a rendering of a <class name >" among all classes. We evaluated the output geometries of the different methods, each rendered from 360 degrees with azimuth gaps of 2 degrees (180 images for each surface). We report the mean accuracy in Tab. 4. Here again, our approach is substantially better.

### Limitations

As in previous work (), for extracting point clouds from depth images, we need the internal parameters of the depth camera. Our test time optimization method is slow compared to the feedforward

    & Shape & PoinTr & SDS-Complete \\  & Former & & (ours) \\  User-study wins \([\%]\)\(\) & 7.0 & 0.0 & **93** \\ Average MMD \(\) & 0.036 & 0.052 & **0.027** \\ CLIP R-Precision \(\) & 0.61 & 0.43 & **0.76** \\   

Table 4: **Quantitative results on KITTI**. User-study wins refer to the percentage of cases in which a method was selected by raters as the best method. Our method outperforms the baseline methods in all 3 metrics.

   Text Prompt & "An object" & "A thing" & "A  name" & Full text \\  Chamfer & 52.0 & 52.7 & 33.8 & 33.1 \\   

Table 3: **Generic vs specific text prompts: Chamfer distances (lower is better)**. Columns 1 and 2: two generic configurations where a global text is used for all objects. Column 3: Only the class name is used e.g. both “executive chair” and “outside chair” are reconstructed with the text “A chair”. Column 4: The results of our method with the full text prompts (provided in the supplementary).

Figure 6: **Ablation study**. We demonstrate the contribution of each part of our method. Naïve camera sampling: running without our camera handling that is described in Sec. 3.4. No SDS loss: using all losses but the SDS loss. No SDF representation: running with a density function as in . Below, we **quantitatively** compare the average Chamfer distance over the evaluated 14 Redwood scans. We show extended ablations in the appendix.

ward baseline methods that were pre-trained on large datasets of 3D shapes. The major factor for the running time is the SDS-loss which needs many sampling views. The number of training epochs can be shortened by reducing the number of iterations. In Tab. 8 we show the effect of reducing our runtime. As we show, our method outperforms the baselines in terms of average accuracy after \(5\%\) of the training time and keeps improving when more training time is given. In the appendix, we show failure cases of our method and list additional implementation details.

## 5 Conclusions

We presented SDS-Complete, a novel test time optimization approach for 3D completion using a text-to-2D pre-trained model. For handling point cloud inputs, we incorporated an SDF representation and constrained the surface to lie on the input points. We successfully applied the SDS loss on images rendered from novel views and completed the missing part of the object by aligning the images with an input textual description. By handling the camera sampling carefully we maintained the consistency of the completed part with the input captured part. This enabled us to produce superior results even on previously unconsidered objects for completion. Future work includes improving running times by combining recent NeRF techniques (e.g. ), and supporting scene completion from incomplete point clouds of multiple objects.

Figure 8: **Shorter Training. The Chamfer error when running our method for fewer epochs. For reference, we also include the (inference time) numbers of the baselines. Left: our convergence on the table and chair categories. Right: our convergence when considering all \(14\) Redwood models.**

Figure 7: Qualitative completion results on the KITTI dataset.

## 6 Acknowledgments

We thank Lior Yariv, Dolev Ofri, Or Perel and Haggai Maron for their insightful comments. We thank Lior Bracha and Chen Tessler for helping with the user study. This work was funded by a grant to GC from the Israel Science Foundation (ISF 737/2018), and by an equipment grant to GC and Bar-Ilan University from the Israel Science Foundation (ISF 2332/18). OR is supported by a PhD fellowship from Bar-Ilan data science institute (BIU DSI).