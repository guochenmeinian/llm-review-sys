# Figure 1 (a).

Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning

 Daniel Kunin1

Allan Raventos1

Clementine Domine2

Feng Chen1

David Klindt3

Andrew Saxe2

Surya Ganguli1

1Stanford University 2University College London 3Cold Spring Harbor Laboratory

###### Abstract

While the impressive performance of modern neural networks is often attributed to their capacity to efficiently extract task-relevant features from data, the mechanisms underlying this _rich feature learning regime_ remain elusive, with much of our theoretical understanding stemming from the opposing _lazy regime_. In this work, we derive exact solutions to a minimal model that transitions between lazy and rich learning, precisely elucidating how unbalanced _layer-specific_ initialization variances and learning rates determine the degree of feature learning. Our analysis reveals that they conspire to influence the learning regime through a set of conserved quantities that constrain and modify the geometry of learning trajectories in parameter and function space. We extend our analysis to more complex linear models with multiple neurons, outputs, and layers and to shallow nonlinear networks with piecewise linear activation functions. In linear networks, rapid feature learning only occurs from balanced initializations, where all layers learn at similar speeds. While in nonlinear networks, unbalanced initializations that promote faster learning in earlier layers can accelerate rich learning. Through a series of experiments, we provide evidence that this unbalanced rich regime drives feature learning in deep finite-width networks, promotes interpretability of early layers in CNNs, reduces the sample complexity of learning hierarchical data, and decreases the time to grokking in modular arithmetic. Our theory motivates further exploration of unbalanced initializations to enhance efficient feature learning.

## 1 Introduction

Deep learning has transformed machine learning, demonstrating remarkable capabilities in a myriad of tasks ranging from image recognition to natural language processing. It's widely believed that the impressive performance of these models lies in their capacity to efficiently extract task-relevant features from data. However, understanding this feature acquisition requires unraveling a complex interplay between datasets, network architectures, and optimization algorithms. Within this framework, two distinct regimes, determined at initialization, have emerged: the lazy and the rich.

**Lazy regime.** Various investigations have revealed a notable phenomenon in overparameterized neural networks, where throughout training the networks remain close to their linearization . Seminal work by Jacot et al. , demonstrated that in the infinite-width limit, the Neural Tangent Kernel (NTK), which describes the evolution of the neural network through training, converges to a deterministic limit. Consequently, the network learns a solution akin to kernel regression with the NTK matrix. Termed the _lazy_ or _kernel_ regime, this domain has been characterized by a deterministic NTK , minimal movement in parameter space , static hidden representations, exponential learning curves, and implicit biases aligned with a reproducing kernel Hilbert space (RKHS) norm . However, Chizat et al.  challenged this understanding, asserting that the lazy regime isn't a product of the infinite-width architecture, but is contingent on the _overall scale_ of the network at initialization. They demonstrated that given any finite-width model \(f(x;)\) whose output is zero at initialization, a scaled version of the model \( f(x;)\) will enter the lazy regime as the scale \(\) diverges. However, they also noted that these scaled models often perform worse in test error. While the lazy regime offers insights into the network's convergence to a global minimum, it does not fully capture the generalization capabilities of neural networks trained with standard initializations. It is thus widely believed that a different regime, driven by small or vanishing initializations, underlies the many successes of neural networks.

**Rich regime.** In contrast to the lazy regime, the _rich_ or _feature-learning_ or _active_ regime is distinguished by a learned NTK that evolves through training, non-convex dynamics traversing between saddle points [10; 11; 12], sigmoidal learning curves, and simplicity biases such as low-rankness  or sparsity . Yet, the exact characterization of rich learning and the features it learns frequently depends on the specific problem at hand, with its definition commonly simplified as what it is not: lazy. Recent analyses have shown that beyond overall scale, other aspects of the initialization can substantially impact the extent of feature learning, such as the effective rank , layer-specific initialization variances [16; 17; 18], and large learning rates [19; 20; 21; 22]. Azulay et al.  demonstrated that in two-layer linear networks, the relative difference in weight magnitudes between the first and second layer, termed the _relative scale_ in our work, can impact feature learning, with balanced initializations yielding rich learning dynamics, while unbalanced ones tend to induce lazy dynamics. However, as shown in Fig. 1, for nonlinear networks unbalanced initializations can induce both rich and lazy dynamics, creating a complex phase portrait of learning regimes influenced by both overall and relative scale. Building on these observations, our study aims to precisely understand how layer-specific initialization variances and learning rates determine the transition between lazy and rich learning in finite-width networks. Moreover, we endeavor to gain insights into the inductive biases of both regimes, and the transition between them, during training and at interpolation, with the ultimate goal of elucidating how the rich regime acquires features that facilitate generalization.

**Our contributions.** Our work begins with an exploration of the two-layer single-neuron linear network proposed by Azulay et al.  as a minimal model displaying both lazy and rich learning. In Section 3, we derive exact solutions for the gradient flow dynamics with layer-specific learning rates of this model by employing a combination of hyperbolic and spherical coordinate transformations.

Figure 1: **Unbalanced initializations lead to rapid rich learning and generalization. We follow the experimental setup used in Fig. 1 of Chizat et al.  – a wide two-layer student ReLU network \(f(x;)=_{i=1}^{h}a_{i}(0,w_{i}^{}x)\) trained on a dataset generated from a narrow two-layer teacher ReLU network. The student parameters are initialized as \(w_{i}(^{d-1}())\) and \(a_{i}=\), such that \(>0\) controls the _overall scale_ of the function, while \(>0\) controls the _relative scale_ of the first and second layers through the conserved quantity \(=^{2}(^{2}-^{-2})\). (a) Shows the training trajectories of \(|a_{i}|w_{i}\) (color denotes \((a_{i})\)) when \(d=2\) for four different settings of \(,\). The left plot confirms that small overall scale leads to rich and large overall scale to lazy. The right plot shows that even at small overall scale, the relative scale can move the network between rich and lazy as well. Here an upstream initialization \(>0\) shows striking alignment to the teacher (dotted lines), while a downstream initialization \(<0\) shows no alignment. (b) Shows the test loss and kernel distance from initialization computed through training over a sweep of \(\) and \(\) when \(d=100\). Lazy learning happens when \(\) is large, rich learning happens when \(\) is small, and rapid rich learning happens when _both_\(\) is small and \(\) is large – an upstream initialization. This initialization also leads to the smallest test loss. See Fig. 10 in Appendix D.1 for supporting figures.**

Alongside recent work by Xu and Ziyin 1, our analysis stands out as one of the few analytically tractable models for the transition between lazy and rich learning in a finite-width network, marking a notable contribution to the field. Our analysis reveals that the layer-specific initialization variances and learning rates conspire to influence the learning regime through a simple set of conserved quantities that constrain the geometry of learning trajectories. Additionally, it reveals that a crucial aspect of the relative scale overlooked in prior analysis is its directionality. While a _balanced initialization_ results in all layers learning at similar rates, an _unbalanced initialization_ can cause faster learning in either earlier layers, referred to as an _upstream initialization_, or later layers, referred to as a _downstream initialization_. Due to the depth-dependent expressivity of layers in a network, upstream and downstream initializations often exhibit fundamentally distinct learning trajectories. In Section 4 we extend our analysis of the relative scale developed in the single-neuron model to more complex linear models with multiple neurons, outputs, and layers and in Section 5 to two-layer nonlinear networks with piecewise linear activation functions. We find that in linear networks, rapid rich learning can only occur from balanced initializations, while in nonlinear networks, upstream initializations can actually accelerate rich learning. Finally, through a series of experiments, we provide evidence that upstream initializations drive feature learning in deep finite-width networks, promote interpretability of early layers in CNNs, reduce the sample complexity of learning hierarchical data, and decrease the time to grokking in modular arithmetic.

**Notation.** In this work, we consider a feedforward network \(f(x;):^{d}^{c}\) parameterized by \(^{m}\). Unless otherwise specified, \(c=1\). The network is trained by gradient flow \(=-_{}_{}()\), with an initialization \(_{0}\) and layer-specific learning rate \(_{}^{m}_{+}\), to minimize the mean squared error \(()=_{i=1}^{n}(f(x_{i};)-y_{i})^{2}\) computed over a dataset \(\{(x_{1},y_{1}),,(x_{n},y_{n})\}\) of size \(n\). We denote the input matrix as \(X^{n d}\) with rows \(x_{i}^{d}\) and the label vector as \(y^{n}\). The network's output \(f(x;)\) evolves according to the differential equation, \(_{t}f(x;)=_{i=1}^{n}(x,x_{i};)(y_{i}-f(x_{i}; ))\), where \((x,x^{};):^{d}^{d} \) is the _Neural Tangent Kernel (NTK)_, defined as \((x,x^{};)=_{p=1}^{m}_{_{p}}_{_{p }}f(x;)_{_{p}}f(x^{};)\). The NTK quantifies how one gradient step with data point \(x^{}\) affects the evolution of the networks's output evaluated at another data point \(x\). When \(_{_{p}}\) is shared by all parameters, the NTK is the kernel associated with the feature map \(_{}f(x;)^{m}\). We also define the _NTK matrix_\(K^{n n}\), which is computed across the training data such that \(K_{ij}=(x_{i},x_{j};)\). The NTK matrix evolves from its initialization \(K_{0}\) to convergence \(K_{}\) through training. Lazy and rich learning exist on a spectrum, with the extent of this evolution serving as the distinguishing factor. Various studies have proposed different metrics to track the evolution of the NTK matrix [24; 25; 26]. We use _kernel distance_, defined as \(S(t_{1},t_{2})=1- K_{t_{1}},K_{t_{2}}/(\|K_{t_{1}}\|_{F}\| K_{t_{2}}\|_{F})\), which is a scale invariant measure of similarity between the NTK at two times. In the lazy regime \(S(0,t) 0\), while in the rich regime \(0 S(0,t) 1\).

## 2 Related Work

**Linear networks.** Significant progress in studying the rich regime has been achieved in the context of linear networks. In this setting, \(f(x;)=()^{}x\) is linear in its input \(x\), but can exhibit highly nonlinear dynamics in parameter \(\) and function \(()\) space. Foundational work by Saxe et al.  provided exact solutions to gradient flow dynamics in linear networks with task-aligned initializations. They achieved this by solving a system of Bernoulli differential equations that prioritize learning the most salient features first, which can be beneficial for generalization . This analysis has been extended to wide [29; 30] and deep [31; 32; 33] linear networks with more flexible initialization schemes [34; 35; 36]. It has also been applied to study the evolution of the NTK  and the influence of the scale on the transition between lazy and rich learning [12; 23]. In this work, we present novel exact solutions for a minimal model utilizing a mix of Bernoulli and Riccati equations to showcase a complex phase portrait of lazy and rich learning with separate alignment and fitting phases.

**Implicit bias.** An effective analysis approach to understanding the rich regime studies how the initialization influences the inductive bias at interpolation. The aim is to identify a function \(Q()\) such that the network converges to a first-order KKT point minimizing \(Q()\) among all possible interpolating solutions. Foundational work by Soudry et al.  pioneered this approach for a linear classifier trained with gradient descent, revealing a max margin bias. These findings have been extended to deep linear networks [39; 40; 41], homogeneous networks [42; 43; 44], and quasi-homogeneous networks . A similar line of research expresses the learning dynamics of networkstrained with mean squared error as a _mirror flow_ for some potential \(()\), such that the inductive bias can be expressed as a _Bregman divergence_. This approach has been applied to diagonal linear networks, revealing an inductive bias that interpolates between \(^{1}\) and \(^{2}\) norms in the rich and lazy regimes respectively . However, finding the potential \(()\) is problem-specific and requires solving a second-order differential equation, which may not be solvable even in simple settings [47; 48]. Azulay et al.  extended this analysis to a time-warped mirror flow, enabling the study of a broader class of architectures. In this work we derive exact expressions for the inductive bias of our minimal model and extend the results in Azulay et al.  to wide and deep linear networks.

**Two-layer networks.** Two-layer, or single-hidden layer, piecewise linear networks have emerged as a key setting for advancing our understanding of the rich regime. Maennel et al.  observed that in training two-layer ReLU networks from small initializations, the first-layer weights concentrate along fixed directions determined by the training data, irrespective of network width. This phenomenon, termed _quantization_, has been proposed as a _simplicity bias_ inherent to the rich regime, driving the network towards low-rank solutions when feasible. Subsequent studies have aimed to precisely elucidate this effect by introducing structural constraints on the training data [50; 51; 52; 53; 54; 55]. Across these analyses, a consistent observation is that the learning dynamics involve distinct phases: an initial alignment phase characterized by quantization, followed by fitting phases where the task is learned. All of these studies assumed a balanced (or nearly balanced) initialization between the first and second layer. In this study, we explore how unbalanced initializations influence the phases of learning, demonstrating that it can eliminate or augment the quantization effect.

**Infinite-width networks.** Many recent advancements in understanding the rich regime have come from studying how the initialization variance and layer-wise learning rates should scale in the infinite-width limit to ensure constant movement in the activations, gradients, and outputs. In this limit, analyzing dynamics becomes simpler in several respects: random variables concentrate and quantities will either vanish to zero, remain constant, or diverge to infinity . A set of works used tools from statistical mechanics to provide analytic solutions for the rich population dynamics of two-layer nonlinear neural networks initialized according to the _mean field_ parameterization [56; 57; 58; 59]. These ideas were extended to deeper networks through a _tensor program_ framework, leading to the derivation of _maximal update parametrization_ (\(\)P) [16; 18]. The \(\)P parameterization has also been derived through a self-consistent dynamical mean field theory  and a spectral scaling analysis . In this study, we focus on finite-width neural networks, but discuss the connection between our work and these width-dependent parameterizations in Section 5.

## 3 A Minimal Model of Lazy and Rich Learning with Exact Solutions

Here we explore an illustrative setting simple enough to admit exact gradient flow dynamics, yet complex enough to showcase lazy and rich learning regimes. We study a two-layer linear network with a single hidden neuron defined by the map \(f(x;)=aw^{}x\) where \(a\), \(w^{d}\) are the parameters. We examine how the parameter initializations \(a_{0},w_{0}\) and the layer-wise learning rates \(_{a},_{w}\) influence the training trajectory in parameter space, function space (defined by the product \(=aw\)), and the evolution of the the NTK matrix,

\[K=X(_{w}a^{2}_{d}+_{a}ww^{})X^{}.\] (1)

Except for a measure zero set of initializations which converge to saddle points2, all gradient flow trajectories converge to a global minimum, determined by the normal equations \(X^{}Xaw=X^{}y\). However, even when \(X^{}X\) is invertible such that the global minimum \(_{*}\) is unique, the rescaling symmetry between \(a\) and \(w\) results in a manifold

Figure 2: **Balance determines geometry of trajectory.** The quantity \(=_{w}a^{2}-_{a}\|w\|^{2}\) is conserved through gradient flow, which constrains the trajectory to: (a) a one-sheeted hyperboloid for downstream initializations, (b) a double cone for balanced initializations, and (c) a two-sheeted hyperboloid for upstream initializations. Gradient flow dynamics for three different initializations \(a_{0},w_{0}\) with the same product \(_{0}=a_{0}w_{0}\) are shown. The minima manifold is shown in red and the manifold of equivalent \(_{0}\) initializations in gray. The surface is colored according to training loss, with blue representing higher loss and red representing lower loss.

of minima in parameter space. The minima manifold is a one-dimensional hyperbola where \(w_{}\) and has two distinct branches for positive and negative \(a\). The symmetry also imposes a constraint on the network's trajectory, maintaining the difference \(=_{w}a^{2}-_{a}\|w\|^{2}\) throughout training (see Appendix A.1 for details). This confines the parameter dynamics to the surface of a hyperboloid where the magnitude and sign of the conserved quantity determines the geometry, as shown in Fig. 2. An upstream initialization occurs when \(>0\), a balanced initialization when \(=0\), and a downstream initialization when \(<0\).

**Deriving exact solutions in parameter space.** We initially assume3 whitened input \(X X=_{d}\) such that the ordinary least squares solution is \(_{}=X^{}y\), and the gradient flow dynamics simplify to \(=_{a}(w^{}_{}-a\|w\|^{2})\), \(=_{w}(a_{}-a^{2}w)\). Notice that \(w(t)(\{w_{0},_{}\})\), and through training, \(w\) aligns in direction to \(_{}\) depending on the basin of attraction4 the parameters are initialized in. Therefore, we can monitor the dynamics by tracking the hyperbolic geometry between \(a\) and \(\|w(t)\|\) and the spherical angle between \(w(t)\) and \(_{}\). We study the variables \(=a\|w\|\), an invariant under the rescale symmetry, and \(=_{}}{\|w\|\|_{}\|}\), the cosine of the spherical angle. From these two scalar quantities \((t),(t)\) and the initialization \(a_{0},w_{0}\), we can determine the trajectory \(a(t)\) and \(w(t)\) in parameter space. The dynamics for \(,\) are given by the coupled nonlinear ODEs,

\[=+4_{a}_{w}^{2}}(\|_{} \|-),=_{w}2\|_{}\|}{ +4_{a}_{w}^{2}}-}(1-^{2}).\] (2)

Amazingly, this system can be solved exactly, as discussed in Appendix A.2, and shown in Fig. 3. Without delving into the specifics, we can develop an intuitive understanding of the solutions by examining the influence of the relative scale \(\).

_Upstream._ When \( 0\), the updates for both \(\) and \(\) diverge, but \(\) updates much more rapidly. We can decouple the dynamics of \(\) and \(\) by separation of their time scales and assume \(\) has reached its steady-state of \( 1\) before \(\) has updated. Then, the dynamics of \(\) is linear and proceeds exponentially to \(\|_{}\|\). This regime exhibits minimal kernel movement (see Fig. 3 (c)) because the kernel is dominated by the \(_{w}a^{2}_{d}\) term, whereas it is mainly \(w\) that updates.

_Balanced._ When \(=0\), \(\) follows a Bernoulli differential equation driven by a time-dependent signal \(\|_{}\|\), and \(\) follows a Riccati equation evolving from an initial value to \( 1\) depending on the basin of attraction. For vanishing initialization \(\|_{0}\| 0\), the temporal dynamics of \(\) and \(\) decouple such that there are two phases of learning: an initial alignment phase where \( 1\), followed by a fitting phase where \(\|_{}\|\). In the first phase, \(w\) aligns to \(_{}\) resulting in a rank-one update to the NTK, identical to the silent alignment effect described in Atanasov et al. . In the second phase, the dynamics of \(\) simplify to the Bernoulli equation studied in Saxe et al.  and the kernel evolves solely in overall scale.

_Downstream._ When \( 0\), the updates for \(\) diverge, while the updates for \(\) vanishes. In this regime the dynamics proceed by an initial fast phase where \(\) converges exponentially to its steady state of \(\|_{}\|\). Plugging this steady state into the dynamics of \(\) gives a Bernoulli differential equation

Figure 3: **Exact solutions for the single hidden neuron model.** Our theoretical predictions (black dashed lines) agree with gradient flow simulations (solid lines, color-coded based on \(\) values), shown here for three key metrics: \(\) (left), \(\) (middle), and \(S(0,t)\) (right). Each metric starts at the same value for all \(\), but varying \(\) has a pronounced effect on the metric’s dynamics. For upstream initializations (\( 0\)), \(\) changes only slightly, \(\) exponentially aligns, and \(S\) remains near zero, indicative of the lazy regime. For balanced initializations (\(=0\)), both \(\) and \(\) change significantly and \(S\) quickly moves away from zero, indicative of the rich regime. For downstream initializations (\( 0\)), \(\) quickly drops to zero, then \(\) and \(\) slowly climb back to one. Similarly, \(S\) remains small before a sudden transition towards one, indicative of a delayed rich regime. See Appendix A.2 for further details.

\(=_{a}_{w}\|_{*}\|^{2}||^{-1}(1-^{2})\). Due to the coefficient \(||^{-1}\), the second alignment phase proceeds very slowly as \(\) approaches \( 1\), assuming \(, 0\), which is a saddle point. In this regime, the dynamics proceed by an initial lazy fitting phase, followed by a rich alignment phase, where the delay is determined by the magnitude of \(\).

**Identifying regimes of learning in function space.** Here we take an alternative route towards understanding the influence of the relative scale by directly examining the dynamics in function space, an analysis strategy we will generalize to broader setups in Sections 4 and 5. The network's function is determined by the product \(=aw\) and governed by the ODE,

\[=-a^{2}_{d}+_{a}ww^{ })}_{M}X^{},\] (3)

where \(=X-y\) is the residual. These dynamics can be interpreted as preconditioned gradient flow on the loss in function space where the preconditioning matrix \(M\) depends on time through its dependence on \(a^{2}\) and \(ww^{}\). Whenever \(\|\| 0\), we can express \(M\) directly in terms of \(\) and \(\) as

\[M=_{d}+}{\|\|^{2}},\] (4)

where \(=+4_{a}_{w}\|\|^{2}}\) (see Appendix A.3 for a derivation). This establishes a _self-consistent_ equation for the dynamics of \(\) regulated by \(\). Additionally, notice that \(M\) characterizes the NTK matrix Eq. (1). Thus, understanding the evolution of \(M\) along the trajectory \(_{0}\) to \(_{*}\) offers a method to discern between lazy and rich learning. _Upstream_. When \( 0\), \(M_{d}\), and the dynamics of \(\) converge to the trajectory of linear regression trained by gradient flow. Along this trajectory the NTK matrix remains constant, confirming the dynamics are lazy. _Balanced_. When \(=0\), \(M=_{w}}\|\|(_{d}+ }{\|\|^{2}})\). Here the dynamics balance between following the lazy trajectory and attempting to fit the task by only changing in norm. As a result the NTK changes in both magnitude and direction through training, confirming the dynamics are rich.

_Downstream_. When \( 0\), \(M||}{\|\|^{2}}\), and \(\) follows a projected gradient descent trajectory, attempting to reach \(_{*}\) in the direction of \(_{0}\). Along this trajectory the NTK matrix doesn't evolve. However, if \(_{0}\) is not aligned to \(_{*}\), then at some point the dynamics of \(\) will slowly align. In this second alignment phase the NTK matrix will change, confirming the dynamics are initially lazy followed by a delayed rich phase. See Appendix A.3.1 for a derivation of the NTK dynamics \(\).

**Determining the implicit bias via mirror flow.** So far we have considered whitened or full rank \(X^{}X\), ensuring the existence of a unique least squares solution \(_{*}\). In this setting, \(\) influences the trajectory the model takes from \(_{0}\) to \(_{*}\), as shown in Fig. 4 (a). Now we consider low-rank \(X^{}X\), such that there exist infinitely many interpolating solutions in function space. By studying the structure of \(M\), we can characterize how \(\) determines the interpolating solution the dynamics converge to. Extending a time-warped mirror flow analysis strategy pioneered by Azulay et al.  to allow \(<0\) (see Appendix A.4 for details), we prove the following theorem, which shows a tradeoff between reaching the minimum norm solution and preserving the direction of the initialization \(_{0}\).

**Theorem 3.1** (Extending Theorem 2 in Azulay et al. ).: _For a single hidden neuron linear network, for any \(\), and initialization \(_{0}\) such that \((t) 0\) for all \(t 0\), if the gradient flow solution \(()\) satisfies \(X()=y\), then,_

\[()=*{arg\,min}_{^{d}}_{}( )-_{}}{\|_{0}\|}^{}  X=y\] (5)

_where \(_{}()=(+4\|\|^{2}}-2 )+4\|\|^{2}}+}\) and \(_{}=+4\|_{0}\|^{2}}-}\)._

Figure 4: **Balance modulates \(\) dynamics and implicit bias.** Here we show the dynamics of \(=aw\) with different values of \(\), but the same initial \(_{0}\). When \(X^{}X\) is whitened (left), we can solve for the dynamics exactly using our expressions for \(,\) (black dashed lines). Upstream initializations follow the trajectory of gradient flow on \(\), downstream initializations first move in the direction of \(_{0}\) before sweeping around towards \(_{*}\), and balanced initializations take an intermediate trajectory between these two. When \(X^{}X\) is low-rank (right), then we can only predict the trajectories in the limit of \(=\). If the interpolating manifold is one-dimensional, then we can solve for the solution in terms of \(\) exactly (black dots). See Appendix A.4 for details.

We observe that for vanishing initializations there is functionally no difference between the inductive bias of the upstream (\( 0\)) and balanced (\(=0\)) settings. However, in the downstream setting (\( 0\)), it is the second term preserving the direction of the initialization that dominates the inductive bias. This tradeoff in inductive bias as a function of \(\) is presented in Fig. 4 (b), where if the null space of \(X^{}X\) is one-dimensional, we can solve for \(()\) in closed form (see Appendix A.4).

## 4 Wide and Deep Linear Networks

We now show how the analysis techniques used to study the influence of relative scale in the single-neuron setting can be applied to linear networks with multiple neurons, outputs, and layers.

**Wide linear networks.** We consider the dynamics of a two-layer linear network with \(h\) hidden neurons and \(c\) outputs, \(f(x;)=A^{}Wx\), where \(W^{h d}\) and \(A^{h c}\). We assume \(h(d,c)\), such that this parameterization can represent all linear maps from \(^{d}^{c}\). The rescaling symmetry between \(A\) and \(W\) implies the \(h h\) matrix \(=_{w}A_{0}_{0}^{}-_{a}W_{0}W_{0}^ {}\) is conserved throughout gradient flow . Drawing insights from our analysis of the single-neuron scenario (\(h=c=1\)), we consider the dynamics of \(=W^{}A^{d c}\),

\[()=-A^{ }A_{a}W^{}W)}_{M}(X^{}X- X^{}Y),\] (6)

where \(()\) denotes the vectorization operator and \(\) denotes the Kronecker sum5. As in the single-neuron setting, we find that the dynamics of \(\) are preconditioned by a matrix \(M\) that depends on quadratics of \(A\) and \(W\) and characterizes the NTK matrix \(K=(_{c} X)\,M\,(_{c} X^{})\). We now show how \(M\) can be expressed6 in terms of the rank-1 matrices \(_{k}=w_{k}a_{k}^{}^{d c}\), which represent the contribution to \(\) of a single neuron with parameters \(w_{k}\), \(a_{k}\) and conserved quantity \(_{k}=_{kk}\).

**Theorem 4.1**.: _Whenever \(\|_{k}\|_{F} 0\) for all \(k[h]\), the matrix \(M\) can be expressed as the sum \(M=_{k=1}^{h}M_{k}\) over hidden neurons where \(M_{k}\) is defined as,_

\[M_{k}=(^{2}+4_{a}_{w}\|_{k}\|_{F}^{2} }+_{k}}{2})^{}_{k}}{\|_{k}\|_{F} ^{2}}(^{2}+4_{a}_{w}\|_{k}\|_{F} ^{2}}-_{k}}{2})_{k}^{}}{\|_{k}\| _{F}^{2}}.\] (7)

By studying the dependence of \(M\) on the conserved quantities \(_{k}\) and the dimensions \(d\), \(h\) and \(c\), we can determine the influence of the relative scale on the learning regime. When \((d,c) h<(d,c)\), and assuming independent initializations for all \(_{k}\), then networks which narrow from input to output (\(d>c\)) enter the lazy regime when all \(_{k} 0\), whereas networks which expand from input to output (\(d<c\)) do so when all \(_{k} 0\). However, with opposite signs for \(_{k}\), and assuming all \(_{k}(0)_{*}\), these networks enter a _delayed rich regime_. As elaborated in Appendix B.1.5, this occurs because in these regimes a solution \(_{*}\) does not exist within the space spanned by \(M\) at initialization. When \(h(d,c)\) all networks enter the lazy regime when all \(_{k} 0\) or all \(_{k} 0\). Conversely, as all \(_{k} 0\), all networks transition into the rich regime regardless of dimensions. While Theorem 4.1 offers valuable insight into the learning regimes in the limits of \(_{k}\), understanding the transition between regimes remains challenging. To achieve this, we aim to express \(M\) in terms of \(\), rather than \(_{k}\), by introducing structure on the conserved quantities \(\).

**Theorem 4.2**.: _When \(=_{h}\) and \(h=d\) if \(<0\) or \(h=c\) if \(>0\), then the matrix \(M\) can be expressed as \(M=_{w}+}{4}_{c} _{d}+_{c}_{w} +}{4}_{d}}}\)._

From Theorem 4.2 the resulting dynamics of \(\) simplify to a self-consistent equation regulated by \(\),

\[=-X^{}P_{w}+}{4}_{c}}-_{w}+ }{4}_{d}X^{}P},\] (8)

where \(P=X-Y\) is the residual. Under our isotropic assumption on the conserved quantities \(=_{h}\), these dynamics are exact. Concurrent to our work, Tu et al.  finds that \(\)_approximately_ follows these dynamics in the overparameterized setting \(h(d,c)\) under a Gaussian initialization \((0,^{2})\) of the parameters where \(^{2}h\) is analogous to \(\).

Equipped with a self-consistent equation for the dynamics of \(\) we now aim to interpret these dynamics as a mirror flow with a \(\)-dependent potential. As presented in Theorem B.6, the dynamics of the singular values of \(\) can be described as a mirror flow with a _hyperbolic entropy_ potential, which smoothly interpolates between an \(^{1}\) and \(^{2}\) penalty on the singular values for the rich (\( 0\)) and lazy (\(\)) regimes respectively. This potential was first identified as the inductive bias for diagonal linear networks by Woodworth et al.  and the same mirror flow on the singular values is derived from a different initialization choice in prior work by Varre et al. .

**Deep linear networks.** As presented in Theorem B.10, we generalize the inductive bias derived for rich two-layer linear networks by Azulay et al.  to deep linear networks. For a depth-\((L+1)\) linear network, \(f(x;)=A^{}_{l=1}^{L}W_{l}x\), where \(=_{l=1}^{L}W_{l}^{}A\), we find that the inductive bias of the rich regime is \(Q()=()\|\|^{}-\|_{0}\|^{-}_{0}^{}\). This inductive bias strikes a depth-dependent balance between attaining the minimum norm solution and preserving the initialization direction.

## 5 Piecewise Linear Networks

We now take a first step towards extending our analysis from linear networks to piecewise linear networks with activation functions of the form \((z)=(z, z)\). The input-output map of a piecewise linear network with \(L\) hidden layers and \(h\) hidden neurons per layer is comprised of potentially \(O(h^{dL})\) convex _activation regions_. Each region is defined by a unique _activation pattern_ of the hidden neurons. The input-output map is linear within each region and continuous at the boundary between regions. Collectively, the activation regions form a 2-colorable7 convex partition of input space, as shown in Fig. 5. We investigate how the relative scale influences the evolution of this partition and the linear maps within each region.

**Two-layer network.** We consider the dynamics of a two-layer piecewise linear network without biases, \(f(x;)=a^{}(Wx)\), where \(W^{h d}\) and \(a^{h}\). Following the approach in Section 4, we consider the contribution to the input-output map from a single hidden neuron \(k[h]\) with parameters \(w_{k}^{d}\), \(a_{k}\) and conserved quantity \(_{k}=_{w}a_{k}^{2}-_{a}\|w_{k}\|^{2}\). However, unlike the linear setting, the neuron's contribution to \(f(x_{i};)\) is regulated by whether the input \(x_{i}\) is in the neuron's _active halfspace_. Let \(C^{h n}\) be the matrix with elements \(c_{ki}=^{}(w_{k}^{}x_{i})\), which determines the activation of the \(k^{}\) neuron for the \(i^{}\) data point. The dynamics of \(_{k}=a_{k}w_{k}\) are,

\[_{k}=-_{w}a_{k}^{2}_{d}+_{a}w _{k}w_{k}^{}}_{M_{k}}^{n}c_{ki}x_{i}(f(x _{i};)-y_{i})}_{_{k}}.\] (9)

The matrix \(M_{k}^{d d}\) is a preconditioning matrix on the dynamics, and when \(_{k} 0\), it can be expressed in terms of \(_{k}\) and \(_{k}\). Unlike the linear setting, \(_{k}^{d}\) driving the dynamics is not shared for all neurons because of its dependence on \(c_{ki}\). Additionally, the NTK matrix in this setting depends on \(M_{k}\) and \(C\), with elements \(K_{ij}=_{k=1}^{h}c_{ki}x_{i}^{}M_{k}x_{j}c_{kj}\). To examine the evolution of \(K\), we consider a _signed spherical coordinate_ transformation separating the dynamics of \(_{k}\) into its directional \(_{k}=(a_{k})}{\|_{k}\|}\) and radial \(_{k}=(a_{k})\|_{k}\|\) components, such that \(_{k}=_{k}_{k}\). \(_{k}\) determines the direction and orientation of the halfspace where the \(k^{}\) neuron is active, while \(_{k}\) determines the slope of the contribution in this halfspace. These coordinates evolve according to,

\[_{k}=-^{2}+4_{a}_{w}_{k}^{2}}_{ k}^{}_{k},}_{k}=-^{2}+4 _{a}_{w}_{k}^{2}}+_{k}}{2_{k}}(_{d}-_{k}_{k}^{})_{k}.\] (10)

_Downstream._ When \(_{k} 0\), \(M_{k}|_{k}|_{k}_{k}^{}\), and the dynamics are approximately \(_{t}_{k}=0\) and \(_{t}_{k}=-|_{k}|_{k}^{}_{k}\). Irrespective of \(_{k}\), \(_{k}(t)=_{k}(0)\), which implies the overall partition map doesn't change (Fig. 5, bottom), nor the activation patterns \(C\), nor \(M_{k}\). Only \(_{k}\) changes to fit the data, while the NTK remains constant. If the number of hidden neurons is insufficient to fit the data, there is a delayed rich alignment phase where the kernel will change, with \(|_{k}|\) determining the delay.

_Balanced._ When \(_{k}=0\), \(M_{k}=_{w}}|_{k}|(_{d}+_{k}_{k}^{})\), and the dynamics simplify to, \(_{t}_{k}=-_{w}}(_{k})( _{d}-_{k}_{k}^{})_{k}\) and \(_{t}_{k}=-2_{w}}|_{k}|_{k}^{ }_{k}\). Here both the direction and magnitude of \(_{k}\) evolve, resulting in changes to the activation regions, patterns \(C\), and NTK \(K\). For vanishing initializations where \(\|_{k}(0)\| 0\) for all \(k[h]\), we can decouple the dynamics into two distinct phases of training (Fig. 5, top), analogous to the rich regime discussed in Section 3. _Phase I: Partition alignment._ At vanishing scale, the output \(f(x;_{0}) 0\) for all input \(x\), such that the vector driving the dynamics \(_{k}-_{i=1}^{}c_{ki}x_{i}y_{i}\) is independent of the other hidden neurons. At the same time, the radial dynamics slow down relative to the directional dynamics, and the function's output will remain small as each neuron aligns to certain data-dependent fixed points, decoupled from the rest. Prior works have introduced structural constraints on the training data, such as orthogonally separable , pair-wise orthonormal , linearly separable and symmetric  or small angle , to analytically determine the fixed points of this alignment phase. _Phase II: Data fitting._ After enough time, the magnitudes of \(_{k}\) have grown such that we can no longer assume \(f(x;) 0\) and thus the residual will depend on all \(_{k}\). In this phase, the radial dynamics dominate the learning driving the network to fit the data. However, it is possible for the directions to continue to change, and therefore some prior works have further decomposed this phase into multiple stages.

_Upstream._ When \(_{k} 0\), \(M_{k}_{k}_{d}\), and the dynamics are approximately \(_{t}_{k}=-_{k}_{k}^{-1}(_{d}- _{k}_{k}^{})_{k}\) and \(_{t}_{k}=-_{k}_{k}^{}_{k}\). Again, both the direction and magnitude of \(_{k}\) change. However, unlike the balanced setting, in this setting \(M_{k}\) is independent of \(_{k}\) and stays constant through training. Yet, as \(_{k}\) change in direction, so can \(C\), and thus the NTK. This setting is unique, because it is rich due to a changing activation pattern, but the dynamics do not move far in parameter space. Furthermore, unlike in the balanced scenario where scale adjusts the speed of radial dynamics, here it regulates the speed of directional dynamics, with vanishing initializations prompting an extremely fast alignment phase, as observed in Fig. 1 and in Fig. 5.

**Connections to infinite-width.** Our study of learning regimes in finite-width two-layer ReLU networks as a function of the overall and relative scale is consistent with existing infinite-width analysis of feature learning. For example, in Luo et al.  they consider a network \(f(x)=_{k=1}^{k}a_{k}(w_{k}^{}x)\) with weights initialized as \(a_{k}(0,_{a}^{2})\) and \(w_{k}(0,_{W}^{2}_{d})\) as width \(h\). They obtain a phase diagram at infinite width capturing the dependence of learning regime on the overall function scale \(_{a}_{W}/\) and the relative initialization scale \(_{a}/_{W}\), each suitably normalized as a function of width. The resulting phase portrait is analogous to ours in Fig. 1 (b), where we use the conserved quantity \(\) rather than the relative scale \(_{a}/_{W}\). Specifically, there is a lazy regime that includes the NTK parameterization, which is always achieved at large scale (as in the large-\(\) regions of Fig. 1 (b)), but is also achieved at small scale if the first layer variance is sufficiently larger than the second (as in the downstream initializations at small \(\) in Fig. 1 (b)). On the other side of the phase boundary is the infinite-width analog of rapid rich learning, where all neurons condense to a few directions. This is induced either at small function scale, or at larger scales if \(_{a}/_{W}\) is sufficiently large, such that \(W\) learns fast enough relative to \(a\). The phase boundary, in

Figure 5: **Rapid feature learning is caused by large activation changes with minimal parameter movement. (a) We show the surface of a two-layer ReLU network trained on an XOR-like task, starting with a near-zero input-output map, \(f(x;_{0}) 0\). The surface consists of convex conic regions, each with a distinct activation pattern, colored by the parity of active neurons. A lazy initialization (_bottom_) maintains a fixed activation partition throughout training, reweighting the hidden neurons to fit the data. In contrast, a rich balanced or upstream initialization (_top_) features an initial alignment phase where the partition map changes rapidly while the input-output map remains close to zero, followed by a data-fitting phase. (b) We show the evolution of Hamming distance in activation patterns and parameter distance, relative to \(t=0\), as a function of _overall_ and _relative_ scales (same experiments as in Fig. 1(b)). _Rapid feature learning_ occurs from a small-\(\) upstream initialization that promotes faster learning in early layers, driving a large change in Hamming distance, but a small change in parameter space. In contrast, small-\(\) downstream initializations require large parameter movement to fit the data in the delayed rich regime.**

turn, which exists only at infinite width, contains a range of parametrizations, including the mean-field parametrization. More broadly, across width-dependent parametrizations, the random initialization of weights induces a distribution over per-neuron conserved quantities. While the distinction between the NTK and the mean-field parametrizations has been extensively studied, both lead to the same distribution of per-neuron conserved quantities, which is zero in expectation with a non-vanishing variance. A more thorough study of what role the _distribution_ of per-neuron conserved quantities plays in feature learning at finite-widths is left to future work.

**Unbalanced initializations in practice.** Our analysis shows that upstream initializations can drive rapid rich learning in nonlinear networks. Further experiments in Fig. 6 show that upstream initializations are relevant across various domains of deep learning: (a) Standard initializations see significant NTK evolution early in training . We show the movement is linked to changes in activation patterns rather than large parameter shifts. Adjusting the initialization variance of the first and last layers can amplify or diminish this movement. (b) Filters in CNNs trained on image classification tasks often align with edge detectors . We show that adjusting the learning speed of the first layer can enhance or degrade this alignment. (c) Deep learning models are believed to avoid the curse of dimensionality and learn with limited data by exploiting hierarchical structures in real-world tasks. Using the Random Hierarchy Model, introduced by Petrini et al.  as a framework for synthetic hierarchical tasks, we show that modifying the relative scale can decrease or increase the sample complexity of learning. (d) Networks trained on simple modular arithmetic tasks will suddenly generalize long after memorizing their training data . This behavior, termed grokking, is thought to result from a transition from lazy to rich learning [69; 70; 71] and believed to be important towards understanding emergent phenomena . We show that decreasing the variance of the embedding in a single-layer transformer (\(<6\%\) of all parameters) significantly reduces the time to grokking.

## 6 Conclusion

In this work, we derived exact solutions to a minimal model that can transition between lazy and rich learning to precisely elucidate how unbalanced layer-specific initialization variances and learning rates determine the degree of feature learning. We further extended our analysis to wide and deep linear networks and shallow piecewise linear networks. We find through theory and empirics that unbalanced initializations, which promote faster learning at earlier layers, can actually accelerate rich learning. **Limitations.** The primary limitation lies in the difficulty to extend our theory to deeper nonlinear networks. In contrast to linear networks, where additional symmetries simplify dynamics, nonlinear networks require consideration of the activation pattern's impact on subsequent layers. One potential solution involves leveraging the path framework used in Saxe et al. . Another limitation is our omission of discretization and stochastic effects of SGD, which disrupt the conservation laws central to our study and introduce additional simplicity biases [74; 75; 76; 77]. **Future work.** Our theory encourages further investigation into unbalanced initializations to optimize efficient feature learning. Understanding how the _learning speed profile_ across layers impacts feature learning, inductive biases, and generalization is an important direction for future work.

Figure 6: **Impact of upstream initializations in practice. Here we provide evidence that an upstream initialization (a) drives feature learning through changing activation patterns, (b) promotes interpretability of early layers in CNNs, (c) reduces the sample complexity of learning hierarchical data, and (d) decreases the time to grokking in modular arithmetic. In these experiments, we regulate the first layer’s learning speed relative to the rest of the network by dividing its initialization by \(\). For models without normalization layers, we also scale the last layer’s initialization by \(\) to preserve the input-output map. \(=1\) represents standard parameterization, while \( 1\) and \( 1\) correspond to upstream and downstream initializations, respectively. See Appendix D.3 for details.**

## Author Contributions

This project originated from conversations between Daniel and Allan at the Kavli Institute for Theoretical Physics. Daniel, Allan, and Feng are primarily responsible for the single neuron analysis in Section 3. Daniel, Clem, Allan, and Feng are primarily responsible for the wide and deep linear analysis in Section 4. Daniel is primarily responsible for the nonlinear analysis in Section 5. Allan, Feng, and David are primarily responsible for the empirics in Fig. 1 and Fig. 6. Daniel is primarily responsible for writing the main sections. All authors contributed to the writing of the appendix and the polishing of the manuscript.