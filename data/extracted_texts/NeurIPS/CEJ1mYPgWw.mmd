# Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models

Wenshan Wu\({}^{}\) Shaoguang Mao\({}^{}\) Yadong Zhang\({}^{,}\)

Yan Xia\({}^{}\) Li Dong\({}^{}\) Lei Cui\({}^{}\) Furu Wei\({}^{}\)

\({}^{}\)Microsoft Research \({}^{}\)East China Normal University

###### Abstract

Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as **the Mind's Eye**, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought (**VoT**) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate _mental images_ to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs. Please find the dataset and codes in our project page.

## 1 Introduction

Figure 1: Humans can enhance their spatial awareness and inform decisions by creating mental images during the spatial reasoning process. Similarly, large language models (LLMs) can create internal _mental images_. We propose the VoT prompting to elicit the “mind’s eye” of LLMs for spatial reasoning by visualizing their thoughts at each intermediate step.

Introduction

Recently, large language models (LLMs)  have achieved remarkable performance on various language-related tasks. However, despite their success in math reasoning , common sense reasoning , and other reasoning tasks such as symbolic reasoning or logic reasoning , their abilities in spatial reasoning still remain underexplored .

Spatial reasoning is an essential function of human cognition, allowing us to interact with the environment. It facilitates tasks that require understanding and reasoning about the spatial relationships between objects and their motions. The spatial reasoning of language models largely relies on language to reason about spatial information, whereas human cognitive capabilities extend far beyond verbal reasoning. Humans can not only create task-relevant abstract representations from visual perception , but also imagine unseen scenes through their _mind's eye_. It remains a research topic called mental image  in domains of neuroscience, philosophy of mind, and cognitive science. Building upon this cognitive function, humans facilitate spatial reasoning by mental image manipulation, such as navigation , mental rotation , mental paper folding , and mental simulation . Figure 1 illustrates the human process involved in a navigation task. Humans enhance their spatial awareness and inform their decisions by creating mental images of a route, utilizing various sensory inputs such as navigation instructions or a map image. Subsequently, they simulate route planning through the mind's eye.

Inspired by this cognitive mechanism, we conjecture that LLMs possess the ability to create and manipulate _mental images_ in the mind's eye for spatial reasoning. As illustrated in Figure 1, LLMs could potentially process and understand spatial information in various formats. They might be capable of visualizing internal states and manipulating these _mental images_ through their _mind's eye_, thereby guiding subsequent reasoning steps to enhance spatial reasoning. Therefore, we propose the **Visualization-of-Thought (VoT)** prompting to elicit this ability. This method leverage LLMs to visualize their reasoning steps and inform subsequent steps, implementing the concept of visuospatial sketchpad . VoT adopts zero-shot prompting instead of relying on few-shot demonstrations or text-to-image visualization with CLIP . This choice stems from LLMs' ability to acquire various _mental images_ from text-based visual art .

To evaluate the effectiveness of **VoT** in spatial reasoning, we selected three tasks that require spatial awareness in LLMs, including natural-language navigation , visual navigation, and visual tiling. These tasks require an understanding of space, direction, and geometric shape reasoning. To emulate human-like multisensory perception, we designed 2D grid worlds using special characters as enriched input formats for the LLMs in visual navigation and visual tiling tasks. We compared different models (GPT-4, GPT-4V) and prompting techniques across these three tasks. The findings reveal that the VoT prompting proposed in this paper consistently induces LLMs to visualize their reasoning steps and inform subsequent steps. Consequently, this approach achieved significant performance improvements on the corresponding tasks.

The main contributions of this paper include:

**1**. We shed light on LLMs' _mental image_ for spatial reasoning from a cognitive perspective, conducting quantitative and qualitative analyses on the mind's eye of LLMs and its limitations. We also explore cues about the origin of this generalized ability from code pre-training.

**2**. We develop two tasks of "visual navigation" and "visual tiling", along with corresponding synthetic datasets, emulating various sensory inputs for LLMs. These tasks are structured to support varying levels of difficulty, offering a well-designed testbed for the research on spatial reasoning.

**3**. We propose **Visualization-of-Thought** (**VoT**) prompting to elicit the mind's eye of LLMs for spatial reasoning and provide empirical evaluations on three tasks. Experiment results prove the effectiveness of VoT prompting compared with other prompting methods and existing MLLMs. This ability to generate _mental images_ to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs.

## 2 Spatial Reasoning

Spatial reasoning refers to the ability to comprehend and reason about the spatial relationships among objects, their movements, and interactions with the environment. This skill is vital for a wide range of real-world applications such as navigation, robotics, and autonomous driving. These fields necessitate action planning based on visual perception and a concrete understanding of spatial dimensions.

Although several tasks and datasets  have been developed to probe the spatial semantics embedded in text, existing research efforts often focus on how spatial terms are linguistically structured. Recently, significant achievements and impressive results have been achieved in these benchmarks by converting spatial terms to logical forms through LLMs and adopting logic programming . This implies that excelling in these tasks does not necessarily equate to a genuine understanding of spatial information by LLMs, nor does it provide an accurate measure of their spatial awareness.

Spatial awareness involves understanding spatial relationships, directions, distances, and geometric shapes, all of which are essential for action planning in the physical world. To evaluate the spatial awareness and spatial reasoning abilities of LLMs, we have selected tasks that test navigation and geometric reasoning skills, including natural language navigation, visual navigation and visual tiling.

### Natural Language Navigation

Natural language navigation task  was inspired by prior research on human cognition  presenting participants with sequential transitions sampled from a graph structure.

In this context, a square map is defined by a sequence of random walk instructions and associated objects at each location, denoted as \(W=\{(l_{1},o_{1}),(l_{2},o_{2}),,(l_{n},o_{n})\}\). Given a square map \(W\), and sequence of navigation instructions \(I=\{i_{1},,i_{k}\}\), the task for the model is to identify the associated object \(o W\) at the specified location \(l\) which is determined by the navigation instructions, as detailed in Equation 1 and exemplified in Appendix B.2.

\[o p(o W|W=\{(l_{1},o_{1}),(l_{2},o_{2}),,(l_{n},o_{n})\},I)\] (1)

### Visual Navigation

Visual navigation task presents a synthetic 2D grid world to LLM, challenging it to navigate using visual cues. The model must generate navigation instructions to move in four directions (left, right, up, down) to reach the destination from the starting point while avoiding obstacles. This involves two sub-tasks: **route planning** and **next step prediction**, requiring multi-hop spatial reasoning, while the former is more complex. Task instructions are available in Figure 6 in appendix.

FormulationThe model is presented with a grid map \(M\) consisting of \(k\) consecutive edges \(E=\{e(s_{0},s_{1}),e(s_{1},s_{2}),,e(s_{k-1},s_{k})\}\), where the starting point and destination are \(s_{0}\) and \(s_{k}\) respectively, as shown in Figure 2. Route planning task is to generate a sequence of correct directions \(D=\{d(s_{0},s_{1}),d(s_{1},s_{2}),,d(s_{k-1},s_{k})\}\), as defined in Equation 2. Given \(M\) and \(t\) navigation instructions \(D_{l,0<t<k}=\{d(s_{0},s_{1}),,d(s_{t-1},s_{t})\}\), next step prediction task is to identify the correct direction \(d(s_{t},s_{t+1})\) of the next step, as defined in Equation 3.

\[D p(\{d(s_{0},s_{1}),d(s_{1},s_{2}),,d(s_{k-1},s_{k})\} M)\] (2)

Figure 2: Examples of a navigation map under different settings of \(k\), with emoji of house indicating the starting point, and emoji of office indicating the destination.

\[d p(d(s_{t},s_{t+1}) M,D_{t,0<t<k})\] (3)

ImplementationThe navigation map's underlying graph is semi-Eulerian, alternating between horizontal and vertical edges, with \(2^{k+1}\) possible spatial configurations for a \(k\)-hop navigation map. For each map and set of \(k\) navigation instructions, \(k-1\) question-and-answer (QA) instances,i.e. "what is the next step?" are created. Further implementation details are in Appendix A.1.

### Visual Tiling

Introduced by , polyomino tiling is a classic spatial reasoning challenge. We extend this concept to test the LLM's ability to comprehend, organize, and reason with shapes in a confined area, thus enhancing the evaluation of spatial reasoning skills. As depicted in Figure 3, the task involves a rectangle with unfilled cells and various polyomino pieces, like the I-tetromino made of four aligned squares. The model must select the appropriate polyomino variant, such as choosing the orientation for the I-tetromino, to solve the QA puzzle. Task instructions are provided in Figure 7 in appendix.

FormulationThe model is presented with a rectangle \(R\) masked with \(k\) unique polyominoes \(MP=\{mp_{1},,mp_{k}\}\), 2 corresponding variants of each polyomino \(v_{i<=k}=\{v_{i1},v_{i2}\}\), and a polyomino query \(q MP\). Visual tiling task is to identify the correct variant of \(q\), as defined in Equation 4.

\[v p(v_{q} R,\{mp_{1},,mp_{k}\},\{v_{11},v_{12},v_{k1},v_{ k2}\},q)\] (4)

ImplementationThe dataset comprises valid spatial arrangements generated through existing algorithms, with random masking of polyominoes to create QA puzzles. Details are provided in Appendix A.2.

## 3 Visualization-of-Thought Prompting

Considering the way humans process spatial information during tasks like navigation, it's common to create mental images, such as maps, to enhance spatial awareness or simulating movements to inform decision-making. Our objective is to elicit the spatial awareness of LLMs and ground their reasoning by visualizing the consequence of their intermediate reasoning steps.

We introduce **Visualization-of-Thought (VoT)** prompting: **"Visualize the state after each reasoning step."** This new paradigm for spatial reasoning aims to generate reasoning traces and visualizations in an interleaved manner. Qualitative results of this approach are presented in Figure 4.

We use \(p_{}\) to denote a pre-trained LM with parameters \(\), \(x,y,z\) to denote a language sequence, and \(v\) to denote a visualization sequence in text form. In a multi-hop spatial reasoning task with input \(x\), CoT prompting generates a series of intermediate steps \(z_{1},,z_{n}\), each step \(z_{i} p_{}(z_{i} x,z_{1 i-1})\) is sampled sequentially, followed by the output \(y p_{}(y|x,z_{1 n})\). As shown in Figure 1, **VoT** prompting enhances this process by adding a visuospatial sketchpad to each intermediate step \(z_{i}\), then the subsequent step \(z_{i+1}\) is sampled conditioned on prior steps \(z_{1 i}\) and visualizations \(v_{1 i}\).

As defined in the Equation 5 and 6, it forms interleaved reasoning traces and visualizations. A qualitative comparison between outputs of VoT and CoT is provided in Figure 8a in appendix.

\[v_{i} p_{}(v_{i} prompt_{VoT},x,z_{1 i},v_{1 i-1})\] (5)

Figure 3: Example of visual tiling with masked polyomino pieces. Variants of those polyomino pieces including rotation and reflection are not shown in this figure.

\[z_{i+1} p_{}(z_{i+1} prompt_{VoT},x,z_{1 i},v_{1 i})\] (6)

This reasoning paradigm enables LLMs with visual state tracking. We introduce the concept of a **state**, denoted as \(s_{i}\) = \([x,z_{1 i},v_{1 i-1}]\) representing a partial solution at step \(i\) with the input, the sequence of intermediate steps \(z_{1 i}\) and the sequence of visualizations \(v_{1 i-1}\).

\[v_{i}  p_{}(v_{i} prompt_{VoT},x,z_{1 i},v_{1 i -1})\] (7) \[ p_{}(v_{i} prompt_{VoT},s_{i})\]

As shown in Equation 7, visual state tracking is implemented by generating the visualization \(v_{i}\) as representation of the internal state \(s_{i}\) after each reasoning step \(z_{i}\) (e.g. \(v_{i}\) could be a grid of the navigation map marked with path or a filled rectangle). Grounded by the visual state tracking sequence, the subsequent state is derived by \(s_{i+1} p_{}(s_{i+1} prompt_{VoT},x,s_{i},v_{i})\). This mechanism allows for the derivation of subsequent states, reflecting spatiotemporal causality and enhancing the spatial reasoning capabilities of LLMs in a grounded context.

## 4 Experiment

### Setup

For the visual tasks where a counterpart image exists for each text input, we conduct additional experiments with a multimodal model. Specifically, we adopt GPT-4 [OA\({}^{+}\)23] and GPT-4 Vision [Ope23] via Azure OpenAI API as they're state of the art LLM and multimodal model respectively. API settings are _temperature_ 0 as greedy decoding and _top p_ 1, with model versions of 1106-preview and vision-preview. For all experiments we adopt **zero-shot** prompting.

Depending on whether the LLM is explicitly prompted to visualize intermediate steps, we experiment with three settings of GPT-4, including zero-shot CoT prompting(**GPT-4 CoT**), **GPT-4 w/o Viz** where visualization is explicitly disabled during reasoning, and VoT prompting (**GPT-4 VoT**). Additional setting of GPT-4 Vision with counterpart image input is **GPT-4V CoT**. Prompts are as following:

Figure 4: Examples of VoT prompting in three tasks, where LLM generates 2D grids as text-form _mental images_. The generated reasoning traces and visualizations form an interleaved sequence to track the state over time. The 2D grids in the input and responses are composed of special characters. Full responses could be found in Appendix B.

* **GPT-4 CoT**: Let's think step by step.
* **GPT-4 w/o Viz**: Don't use visualization. Let's think step by step.
* **GPT-4V CoT**: Let's think step by step.
* **GPT-4 VoT**: Visualize the state after each reasoning step.

Task instructions and examples could be found in Appendix B.

### Dataset

Natural Language NavigationWe generate 200 square maps of size 3x3 which is described by 9 landmarks in snake order traversal, and a set of navigation instructions.

Visual NavigationWe generate 496 navigation maps and 2520 QA instances in total, covering various map sizes, up to 7x9 and 9x7. The data distribution is provided in Table 4 in appendix.

Visual TimingWe first generate multiple unique configurations to fill a 5 x 4 rectangle with 5 polyomino pieces including two I tetrominoes, two T tetrominoes and one L tetromino. Then we randomly masked two or three pieces of different types and generate QA instance for each masked pieces. The total number of QA instances is 796, and we show dataset details in Table 5 in appendix.

### Metric

We extract the answer from model output by pattern matching. For tasks except for route planning, we calculate accuracy by Equation 8. We adopted sub-string matching+ as \(f_{correct}\) to determine correctness.

Footnote †: We use this term for simplicity. In natural language navigation tasks, LLMs often output additional words in the extracted answer besides the expected object name. For example, “Answer: You will find...”. In this case, sub-string matching is adopted without affecting the correctness. Otherwise, exact matching is adopted for multiple choice questions in visual tasks.

\[acc=_{i}^{n}f_{correct}(extracted\_answer,ground\_truth)/n\] (8)

For the route planning task which predicts a sequence of navigation instructions, we reject any sequences exceeding 100 instructions, considering them to be random guesses. We then normalize the navigation instructions by executing each navigation instruction. Those instructions which violate navigation rules will be ignored. The length \(t\) of normalized instruction sequence is considered as the temporal distance against the starting point. Given the ground-truth of \(k\) navigation instructions, the completing rate of route planning is \(t/k\). For the dataset of \(n\) maps, we report two metrics including:

1. Average completing rate: \(_{i}^{n}t_{i}/k_{i}/n\). Average completing rate among all instruction sequences, reflecting LLM's effectiveness of route planning.
2. Success rate: \(_{i}^{n}(t_{i}==k_{i})/n\). This metric represents the proportion of instruction sequences with \(t=k\), i.e., reaching the destination.

### Results

As illustrated in Table 1, **GPT-4 VoT** significantly outperforms other settings in all tasks across all metrics. The significant gap when comparing GPT-4 VoT with GPT-4V CoT and GPT-4 w/o Viz demonstrates that effectiveness of visual state tracking, which allows LLMs visually interpret their actions within an grounded world. And in the natural language navigation task, **GPT-4 VoT** outperforms **GPT-4 w/o Viz** by 23.5%. In the visual tasks, the noticeable performance gap between **GPT-4 CoT** and **GPT-4V CoT** indicates that LLM grounded with 2D grid could possibly outperform a MLLM in challenging spatial reasoning tasks.

On the other hand, performance of GPT-4 VoT is still far from perfect in all tasks, especially in the most challenging route planning task. Despite these tasks are relatively easy for humans, performance of LLMs drops significantly as task difficulty increases. Details on performance trends across difficulty levels are provided in figure 9 and table 6 in appendix.

## 5 Analysis

As explained in section 3, one of the core aspects of VoT lies in enabling LLMs with visual state tracking. During the experiments, it was observed that GPT-4 CoT occasionally exhibited this reasoning pattern across several tasks with exception of route planning. Besides, incorrect visualizations of VoT are commonly observed in model outputs. In this section, our analysis of VoT primarily focuses on three questions: (1) Do visual state tracking behaviors differ among prompting methods? (2) How visualizations enhance final answers? (3) Can VoT benefit less powerful language models?

### Do visual state tracking behaviors differ among prompting methods?

For each model output, we extract the sequence of visualizations sampled prior to generating the final answer and discard any visualizations generated thereafter. Then we compare the sequence length \(l_{v}\) with the number of reasoning steps \(l_{s}\). We calculate Complete Tracking\(_{i}^{n}(l_{v}==l_{s})/n\) when a visualization \(v_{i}\) corresponds to each state \(s_{i}\). Similarly, we calculate the Partial Tracking metric as \(_{i}^{n}(l_{v}>0)/n\) when at least one visualization is present before the final answer is generated. Figure 5 shows the significant differences between these settings. In the GPT-4 CoT setting, it demonstrated noticeable tracking rate across almost all tasks except route planning. This observation implies that **LLMs inherently exhibit the capability of visual state tracking when spatiotemporal simulation is integral to reasoning**.

On the other hand, the visual state tracking behavior is **sensitive to prompts** to varying degrees. As showcased in Figure 8 in appendix, after removing "reasoning" from the prompt of VoT, the visualizations are sampled after GPT-4 generates the wrong answer. Consequently, explicitly prompting LLMs to visualize their reasoning traces with **VoT markedly improves the visual tracking rate**, thereby enhancing overall performance. The potential contribution of code pre-training to this emergent capability is further explored in Appendix C.

### How visualizations enhance final answers?

Ideally, **VoT** is supposed to generate an accurate visualization \(v_{i}\) at each step, so that subsequent step \(z_{i+1}\) could be determined correctly. This relies on the spatial visualization and spatial understanding capability of LLMs. To evaluate these capabilities of LLMs in these tasks, we extract the final visualization from each model output under the setting **GPT-4 VoT** in visual navigation and polyomino

   &  &  &  \\    &  & & &  &  \\    & Completing Rate &  & & & \\  GPT-4 CoT & 37.02 & 9.48 & 48.61 & 54.15 & 54.00 \\ GPT-4 w/o Viz & 37.17 & 10.28 & 48.49 & 46.98 & 35.50 \\ GPT-4V CoT & 33.36 & 5.65 & 46.59 & 49.62 & / \\ GPT-4 VoT & **40.77** & **14.72** & **55.28** & **63.94** & **59.00** \\  

Table 1: Performance of different GPT-4/4V settings in all tasks. Underline denotes statistical significance with p < 0.05 when comparing GPT-4 VoT against all baselines using two-sample z-test, while p < 0.16 is observed compared with GPT-4 CoT in natural language navigation task.

Figure 5: tracking rate of different settings across all tasks.

tiling task. Specifically, for visual navigation task, we extract the visualized map where LLM completed all navigation instructions. For polyomino tiling, we extract the rectangle filled with corresponding polyomino piece. The spatial visualization capability is measured by two criteria: (1) **Compliance**, indicating whether the manipulation of _mental image_ satisfies requirements such as avoiding overlap and navigating around obstacles. (2) **Accuracy**, indicating whether the _mental image_ aligns with the corresponding state. The spatial understanding capability is measured by the proportion of correct answers when the corresponding visualization is generated accurately.

As could be seen from Table 2, LLMs demonstrate promising potential in performing multi-hop visualization while adhering to spatial constraints, with compliance rates of approximately 51-52%. However, the relatively low accuracy of state visualization (around 24%-26%) indicates a need for significant improvements in this area. Despite this limitation, **LLMs are able to make correct decisions in 65%-77% of the cases when accurate internal state visualizations are generated**, which enhances groundedness and contributes to notable performance gains. Several case studies are provided in Appendix E for interested readers.

On the other hand, **VoT prompting might underperform in those tasks where LLMs can leverage logical reasoning without visualizing internal states**. We conducted experiments in natural language navigation within a ring , where navigation instructions are either clockwise or counter-clockwise movements. By normalizing each instruction to a signed number, GPT-4 converts this task to mathematical calculation of adding and modulus operation. For example, instructions of 15 steps clockwise and 3 steps counter-clockwise are normalized to (15 - 3) % 12. Results show that GPT-4 CoT outperforms GPT-4 VoT with 52.5% VS 49.5% among 200 test instances with ring size of 12.

### Can VoT benefit less powerful language models?

To evaluate the efficacy of VoT on less powerful language models, we conducted experiments across various model families  and model sizes, including **GPT-3.5 turbo**, **LLAMA3-8B-Instruct** and **LLAMA3-70B-Instruct**. We access GPT-3.5 via Azure OpenAI API with model version 1106-preview and apply greedy decoding to all models.

As shown in Table 3, within the same model family, performance improves across all tasks with increases in model size. **LLAMA3-70B VoT** significantly outperforms the baseline across all tasks except for visual tiling, where it aligns closely with results observed in GPT-4. This consistency suggests that VoT offers a scaling advantage when applied to more advanced models, markedly enhancing performance in larger models. In contrast, less capable models tend to rely on random guessing, especially in spatial reasoning tasks. For instance, in the route planning task, GPT-3.5 CoT often resorts to speculative responses, random guessing in nearly half of the instances, which leads to exhaustion of output tokens. While GPT-3.5 VoT effectively minimizes random guesses, such occurrences become increasingly rare with GPT-4 CoT as the model size expands. On the other hand, the reliance on random guessing introduces unpredictability in performance trends for less powerful models. It suggests their limitations in sustaining reliable reasoning processes across different difficulty levels. Details on performance trends are provided in Appendix D.

## 6 Related Works

Spatial Reasoning over TextSpatial reasoning and spatial language understanding  in NLP domain mainly focus on semantic representation , spatial information extraction , learning and reasoning . Recent advancements have further explored spatial reasoning within the context of large language models (LLMs). To improve multi-hop spatial reasoning skills of language models, several works  proposed to pretrain language models with synthetic datasets. An increasing number of dataset were then developed to covers various type of spatial relations in 2D visual scenes ,

   &  &  \\   & Compliance & Accuracy & Accuracy \\  Visual Navigation & 51.14 & **26.48** & 65.16 \\ Visual Tilling & **52.01** & 24.25 & **77.20** \\  

Table 2: Spatial visualization/understanding evaluation in visual navigation and visual tiling task.

geometric patterns  and 3D spatial information .  investigated spatial reasoning capabilities of transformer-based models in the UI grounding setting. On the other hand, some works adopted in-context learning, leveraging LLMs for general purpose reasoning to convert spatial information to logic forms , or as a general pattern machine for sequence transformation . Recently, several works focused on evaluating spatial reasoning of LLMs as cognitive capability on navigation  and planning tasks  among various spatial structures. While most existing works rely on linguistic semantics and verbal reasoning, and might not always necessitate spatial awareness, we propose to elicit mind's eye of LLMs in spatial reasoning tasks with various formats from a cognitive perspective. The VoT prompting induces LLMs to create _mental images_ for visualizing their internal states and inform subsequent reasoning step.

World Models of LLMsWhile there have been many theoretical debates about whether LLMs can effectively learn an internal world model from ungrounded form alone , advocated that world models should represent percepts and action plans at multiple levels of abstraction and multiple time scales, with the capability of planning, predicting, and reasoning.  proposed to ground LLM in the physical world by reasoning over the experimental results predicted by external simulation.  further leveraged LLMs as world models to predict the subsequent states by action simulation, given predefined states and actions per task. On the other hand, an increasing number of studies focus on investigating internal representations of LLMs.  showed that by utilizing in-context learning, LLMs' learned representations can be mapped to grounded perceptual and conceptual structure in color and spatial domains. Moreover,  and  discovered linear representations of space, time and game state in specifically trained LLMs, which are important for dynamic causal world models. Our work does not probe the internal representations of specialized LLMs, nor does it depend on external simulation engine or state definitions. We demonstrate LLMs' zero-shot capability of representing their precepts at an abstract level, predicting and tracking the internal states over time to generate action plans in multi-hop spatial reasoning tasks, which possibly mirrors the causal world model within LLMs.

## 7 Conclusion

This study introduces Visualization-of-Thought Prompting (VoT), inspired by the human cognitive function of visualizing and manipulating mental images through the mind's eye. We have demonstrated that VoT enables LLMs to exhibit the mechanism of "the mind's eye", as evidenced by their performance in multi-hop spatial reasoning tasks and our comprehensive analysis of the reasoning traces. Remarkably, VoT enable LLMs to outperform state-of-the-art multimodal large language models (MLLMs) in the tested visual tasks. While VoT demonstrates impressive efficacy in LLMs, this emergent capability to create _mental images_ to enhance spatial reasoning resembles the mind's eye process, suggesting its promise in MLLMs.

Building on the success of experiments with GPT-4, we plan to investigate how VoT can futher elicit "the mind's eye" in MLLMs to enhance their spatial awareness. Additionally, our future efforts will focus on automatic data augmentation from real-world scenarios, aiming to identify effective methods for learning generalized internal representations of _mental images_. This will further improve the mind's eye of LLMs, ultimately contributing to the advancement of their cognitive and reasoning abilities.

   &  &  &  \\    & Route Planning & & & & \\    & Completing Rate & & & & \\  GPT-3.5 CoT & 16.10 & **2.62** & **17.42** & 44.10 & 8.50 \\ GPT-3.5 VoT & **19.02** & 1.61 & 13.10 & **47.99** & **9.00** \\  LLAMA3-8B CoT & 4.65 & 0 & **28.73** & **47.24** & **16.50** \\ LLAMA3-8B VoT & **4.97** & **0.2** & 26.75 & 46.73 & 15.50 \\  LLAMA3-70B CoT & 19.90 & 2.62 & 49.01 & **56.41** & 26.00 \\ LLAMA3-70B VoT & **30.24** & **5.85** & **54.09** & 56.03 & **32.50** \\  

Table 3: Performance of VoT in GPT-3.5 and LLAMA3 models. Underline denotes statistical significance with p < 0.05 compared to corresponding CoT baseline using two-sample z-test.

## Limitations

This work only scratches the surface of spatial reasoning of LLMs. Both _mental images_ and visual state tracking rely on the emergent ability of advanced LLMs. Therefore, it might cause performance deterioration in less advanced language models or more challenging tasks. Besides, due to the limited data exposure and a lack of explicit instruction tuning, visual state tracking of current LLMs are sensitive to prompts. For example, when explicitly prompted with "use ascii-art", the tracking rate will significantly increase thereby boosting performance, while removing "reasoning" from the **VoT** prompt will cause a decrease of tracking rate. Moreover, the _mental images_ tested in our work are limited to 2D grid. To strength the mind's eye of LLMs, more diverse and complicated representation should be explored in the future, such as complex geometric shapes and even 3D semantics shown in Figure 11 in appendix.