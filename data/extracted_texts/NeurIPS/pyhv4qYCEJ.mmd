# Evaluating Self-Supervised Learning for

Molecular Graph Embeddings

Hanchen Wang1\({}^{}\) Jean Kaddour2\({}^{}\) Shengchao Liu3,4\({}^{}\) Jian Tang3,5,6 Joan Lasenby1 Qi Liu7

\({}^{1}\)Cambridge \({}^{2}\)UCL \({}^{3}\)MILA \({}^{4}\)UdeM \({}^{5}\)HEC \({}^{6}\)CIFAR \({}^{7}\)HKU \({}^{}\) Equal Contribution

###### Abstract

Graph Self-Supervised Learning (GSSL) provides a robust pathway for acquiring embeddings without expert labelling, a capability that carries profound implications for molecular graphs due to the staggering number of potential molecules and the high cost of obtaining labels. However, GSSL methods are designed not for optimisation within a specific domain but rather for transferability across a variety of downstream tasks. This broad applicability complicates their evaluation. Addressing this challenge, we present "Molecular Graph Representation Evaluation" (MolGraphEval), generating detailed profiles of molecular graph embeddings with interpretable and diversified attributes. MolGraphEval offers a suite of probing tasks grouped into three categories: (i) generic graph, (ii) molecular substructure, and (iii) embedding space properties. By leveraging MolGraphEval to benchmark existing GSSL methods against both current downstream datasets and our suite of tasks, we uncover significant inconsistencies between inferences drawn solely from existing datasets and those derived from more nuanced probing. These findings suggest that current evaluation methodologies fail to capture the entirety of the landscape.

## 1 Introduction

Learning neural embeddings of molecular graphs has become of paramount importance in computer-aided drug discovery [1; 2]. For instance, a molecular property prediction (MPP) model can expedite and economise the design process by reducing the need for synthesising and measuring molecules. Thereby, such models can be immensely useful in the hit-to-lead and early lead optimisation phase of a drug discovery project . However, obtaining labels of molecule properties is expensive and time-consuming, especially since the size of potential pharmacologically active molecules is estimated to be in the order of \(10^{60}\)[4; 5].

Graph Self-Supervised Learning (GSSL) paves the way for learning molecular graph embeddings without human annotations that are transferable to various downstream datasets. Unfortunately, the evaluation of such general-purpose embeddings is fundamentally complex. Different proxy objectives will place different demands on them, and no single downstream dataset can be definitive. Moreover, many of the previously proposed GSSL works are disconnected in terms of the tasks they target and the datasets they use for evaluation, making direct comparison difficult.

Figure 1: Overview of MolGraphEval. Given molecular graphs, we train GNNs to predict SSL proxy objectives. We then extract embeddings of (possibly unseen) graphs using pre-trained models, which form the inputs for probe models, trained and evaluated on the designed metrics.

**Contributions.** Our goal is to unbiasedly evaluate molecular graph embeddings obtained by GSSL methods on existing downstream tasks and a new suite of probe tasks (Fig. 1). We summarised some key findings based on a total of **90,918 probe models** and **1,875 pre-trained GNNs**.

1. On MPP tasks, we observe every GSSL method can introduce substantial performance gains. Yet, there is a significant difference in the rank depending on whether we fine-tune the pre-trained network on the downstream dataset or not. Also, the pre-training configurations for obtaining the optimal embeddings or initialisation for fine-tuning are different; see - **Finding 2**.
2. Several discrepancies between MPP tasks and MolGraphEval demonstrate how the latter complements GSSL evaluation with novel insights fostering future work:

* **Finding 4**.
* **Finding 8**.
* **Finding 9**.

## 2 Related work

**Graph SSL (GSSL)** can be divided into contrastive and generative methods [7; 8; 9]. Contrastive GSSL [10; 11; 12] construct multiple views of the same graph via augmentations and then learn embeddings by aligning positive samples against negative ones. Generative GSSL [11; 13; 14; 15] yields embeddings by reconstructing input graphs. Zhu et al.  conduct an empirical analysis of contrastive GSSL methods and their components. In contrast, we investigate both generative and contrastive GSSL methods and propose a novel suite of tasks to probe the learned embeddings' attributes.

**Probe models and benchmarks on graphs.** Probe models, trained exclusively on embedding vectors from pre-trained models, serve as an effective tool for evaluating the quality of learned embeddings . Their effectiveness has been demonstrated across various domains such as language [18; 19; 20; 21; 22; 23], vision [24; 25; 26; 27; 28], relational tables , and science [30; 31; 32]. While there exist benchmarks for graph learning [33; 34; 35; 36; 37], applying probe models to GSSL remains an unexplored frontier.

## 3 Preliminaries

**Graph.** A graph \(=(,)\) consists of a set of nodes \(\) and edges \(\). In molecular graphs, nodes are atoms, and edges are bonds. We use \(_{u}\) and \(_{uv}\) to denote the feature of node \(u\) and the bond feature between nodes \([u,v]\), respectively. For notation simplicity, we use an adjacency matrix \(^{||||}\) to represent the graph, where \([u,v] 0\) if the nodes \((u,v)\) are connected.

**GNN.** Graph neural networks (GNNs) give rise to learning molecular graph embeddings [38; 39; 40; 41]. A prototypical GNN relies on messaging passing , which updates atom-level embeddings based on their neighbourhoods. Given an input atom \(_{u}^{0}=_{u}\), we compute its embedding by:

\[_{u}^{t+1}=_{v:[u,v] 0}M_{t}(_{u}^{t},_{v} ^{t},_{uv})_{u}^{t+1}=U_{t}(_{u}^{t},_{u}^{ t+1})\] (1)

where \(M_{t}\) and \(U_{t}\) are the "message" functions and "vertex update" functions, respectively. Repeating message passing for \(T\) steps, the embedding of each atom contains their \(T\)-hop neighbourhood information. A readout function \(R\) is then used to pool node-level embeddings for graph-level representations: \(=R(\{_{u}^{} u\})\). Following previous GSSL methods on molecular graphs, we adopt the Graph Isomorphism Network (GIN)  as the backbone model and incorporate edge features during message passing following .

**Pre-Training.** We inspect nine GSSL methods (1,875 configurations in total): EdgePred, InfoGraph, GPT-GNN, AttrMask, ContextPred, GROVER, GraphCL, JOAO, and GraphMVP. We use all qualified molecules (around 0.33 million, _i.e._, leave out the molecules that appeared in downstream datasets) from the GEOM dataset  to pre-train the GIN backbone. As many of these pre-training methods are not primarily designed for molecular graphs, we perform the grid search over the hyperparameter space and savethe optimal settings. For these nine GSSL methods, we have pre-trained 1,875 GNNs with different configurations, as elaborated in Appendix B. We extract embeddings using the pre-trained weights, select the optimal hyperparameter sets based on their downstream MPP performance and use these optimal embeddings for further probing tasks.

**Probe.** We use probe models  to study whether self-supervised learned embeddings encode helpful structural information about graphs. Concretely, we extract embeddings from a pre-trained GNN and train a linear model to predict the probe tasks with node and graph embeddings as inputs. As the first work that designs probe methods on graph embeddings, we follow previous works on computer vision and natural language processing. We mainly compute and compare the quality of pre-trained embeddings using linear probe models. We have also experimented MLPs with one hidden layer as the probe models, as this architecture is utilised in some previous works. We observe similar findings with both probe architectures and reported the results of MLP probes in Appendix B.4. We use scaffold split to partition data into 80%/10%/10% for the training/validation/testing set. The training procedure runs for 100 epochs with a fixed learning rate of 0.001. We report the test results based on the best validation scores. To account for statistical significance, we average all experimental results over three independent runs. We find that different data splits are the primary cause for performance variations (\(\)2%), instead of initialising probe models with different random seeds (\(<\)0.01%).

## 4 Benchmarking GSSL on MPP

We first conduct a rigorous empirical investigation of the GSSL methods' effectiveness in predicting the biochemical properties of molecules. Following previous work [11; 12], we consider eight molecular datasets consisting of 678 binary property prediction tasks [47; 48]. Unless explicitly stated otherwise, we extract the node/graph embeddings from the last GNN layer. We devise two settings: (i) fixed embeddings, where we train the probe models with fixed embeddings extracted from pre-trained GNNs; (ii) fine-tuned embeddings ("FT"), where we update weights of both the pre-trained GNNs and the probe models. Setting (i) follows the procedures in previous probing literature, while (ii) is widely utilised as the "pre-training, then fine-tuning" paradigm. We use Adam optimiser with no weight decay, set the batch size as 256, and apply identical pre-processing procedures for all experiments.

**Findings.** Table 1 notes the results, and we summarise the following findings, some of which contrast with those drawn from the concurrent study .

1. **All GSSL methods perform better than Random.** By carefully exploring the pre-training hyperparameters, all GSSLs substantially improve the MPP tasks for both fixed and fine-tuned embeddings. Contrastive-based GSSL methods (_i.e._, GraphCL, JOAO and GraphMVP) achieve the overall best performance. As  declares that molecular graph pretraining is ineffective; however, we find that their conclusions are based on a few selected finetuning datasets and fixed

    & BBBP & Tox21 & ToxCast & Sider & ClinTox & MUV & HIV & Race & Avg & Avg (FT) \\  \# Molecules & 2,039 & 7,831 & 8,575 & 1,427 & 1,478 & 93,087 & 41,127 & 1,513 & – & – \\ \# Tasks & 1 & 12 & 617 & 27 & 2 & 17 & 1 & 1 & – & – \\  Random & 50.7 \(_{2.5}\) & 64.9 \(_{0.5}\) & 53.2 \(_{0.3}\) & 53.2 \(_{1.1}\) & 63.1 \(_{2.3}\) & 62.1 \(_{1.3}\) & 66.1 \(_{0.7}\) & 63.4 \(_{1.8}\) & 59.60 & 66.16 \\  EdgePred & 54.2 \(_{1.0}\) & 66.2 \(_{0.2}\) & 54.4 \(_{0.1}\) & 56.1 \(_{0.1}\) & **65.4 \(_{0.5}\)** & 59.5 \(_{0.0}\) & **73.6 \(_{0.4}\)** & 71.4 \(_{1.2}\) & 62.59 & 68.16 \\ AttnMask & 62.7 \(_{2.7}\) & 65.7 \(_{0.8}\) & **56.1 \(_{0.2}\)** & 58.3 \(_{1.5}\) & 61.9 \(_{0.4}\) & 64.0 \(_{1.8}\) & 65.5 \(_{1.4}\) & 64.8 \(_{2.6}\) & 61.99 & 69.20 \\ GPT-GNN & 62.0 \(_{0.9}\) & 64.9 \(_{0.7}\) & 55.4 \(_{0.2}\) & 55.3 \(_{0.5}\) & 55.0 \(_{1.6}\) & 61.2 \(_{1.5}\) & 71.2 \(_{1.5}\) & 61.0 \(_{1.2}\) & 60.74 & 67.58 \\ InfoGraph & 65.9 \(_{0.6}\) & 65.8 \(_{0.7}\) & 54.6 \(_{0.1}\) & 57.2 \(_{0.1}\) & 61.4 \(_{0.8}\) & **63.9 \(_{1.9}\)** & 71.4 \(_{0.6}\) & 67.4 \(_{4.9}\) & 63.44 & 68.92 \\ Cont.Pred & 55.5 \(_{2.0}\) & 67.9 \(_{0.7}\) & 54.0 \(_{0.3}\) & 57.1 \(_{0.5}\) & **67.4 \(_{3.0}\)** & 60.5 \(_{0.9}\) & 66.2 \(_{1.5}\) & 54.4 \(_{3.2}\) & 60.36 & 69.40 \\ GROVER & **67.0** & 3.63 \(_{0.3}\) & 53.6 \(_{0.4}\) & **59.9 \(_{1.7}\)** & 65.0 \(_{0.4}\) & 62.7 \(_{1.4}\) & 67.8 \(_{1.0}\) & 69.0 \(_{1.7}\) & 63.26 & 69.97 \\ GraphCL & **64.7** & 1.71 \(_{0.1}\) & **69.1 \(_{0.5}\)** & **56.2 \(_{0.2}\)** & **59.5 \(_{0.6}\)** & 60.8 \(_{3.0}\) & 60.6 \(_{1.8}\) & 72.5 \(_{1.4}\) & **77.0 \(_{1.7}\)** & **65.04** & **70.33** \\ JOAO & 66.1 \(_{0.8}\) & **68.1** & 4.02 \(_{0.2}\) & 55.1 \(_{0.4}\) & 58.3 \(_{0.3}\) & 65.3 \(_{0.1}\) & 62.4 \(_{1.2}\) & **73.8 \(_{1.2}\)** & 71.1 \(_{0.8}\) & **65.05** & 69.75 \\ GraphMVP & **69.2** \(_{1.8}\) & 63.8 \(_{0.3}\) & 55.5 \(_{0.3}\) & 58.6 \(_{0.4}\) & 58.7 \(_{1.9}\) & **63.8 \(_{1.3}\)** & 68.6 \(_{1.0}\) & **73.3 \(_{4.7}\)** & 63.92 & **70.06** \\   

Table 1: **Evaluating GSSL methods on molecular property prediction tasks. For each downstream dataset, we report the mean and standard deviation of the ROC-AUC scores over three random scaffold splits. The best and second best scores are marked bold and bold, respectively. The performance scores are based on the fixed pre-trained embeddings with linear probe models, we also report the average ROC-AUC scores with fine-tuned pre-trained GNN on MPP tasks (“Avg (FT)”). For each pre-training method, we report the highest scores in the table and their corresponding hyperparameter configurations in Tables 7 and 9 in Appendix B.**pre-training hyperparameters. We further observe that such improvements will reduce when the number of molecules in downstream datasets increases. Specifically, for MUV , a dataset designed for validating virtual screening (used in drug discovery to find how likely molecules that bind to a drug target), the average performance gain brought by pre-training is -0.3%; while for BBBP, it is 12.3%. The number of molecules in BBBP is only 2% of the MUV's.
2. **Rankings differ between probing and fine-tuning.** The rank correlation between the fixed and fine-tuned embeddings is 0.77 (p-value=9e-4), indicating that we cannot utilise the rank of fixed embeddings as a definite indicator for fine-tuning performance, though they are positively correlated. Part of this observation has been spotted in a study on masked visual transformers . In the context of molecular property prediction, embeddings pre-trained with JOAO achieve the best score with fixed scenarios but perform the fourth after end-to-end fine-tuning. The reason is unclear and should be investigated by future work.
3. **The optimal sets of pre-train hyperparameters for fixed and fine-tuned embeddings vary.** We observe that the optimal pre-training hyperparameters on fixed and fine-tuned embeddings differ. Only two out of nine GSSLs (InfoGraph and EdgePred) share the same set of optimal parameters, as detailed in Tables 7 and 9 in Appendix B. This suggests that probing the fixed embeddings might not truly reflect pre-trained models' performance on downstream MPP tasks, as it ignores the consequent improvements induced by fine-tuning. In Fig. 2, we visualised the hyperparameter space of the AttrMask pre-trainer, the local minima in the hyperparameter space distribute differently. As shown in Fig. 2 also Table 9, the best pre-training configuration for probing is "mask rate=0.85 and learning rate=1e-4", while in terms of fine-tuning scores, the optimal setting is "mask rate=0.50 and learning rate=5e-4". Also, it can be inferred that the optimal pre-training hyperparameters for different pre-training datasets vary; therefore, using reported hyperparameters without carefulness and concluding "graph pretraining is ineffective in molecular domain" is not convincing .

## 5 Molecular graph representation evaluation

The goal of GSSL for molecular graphs is to obtain embeddings that capture generic information about the molecule and its properties. However, there is no free lunch : different training objectives optimise for different properties, and evaluating the extracted embeddings on only a handful of downstream datasets does not provide the whole picture (as we confirm empirically in Sec. 6). Also, from Sec. 4, we know the probing and fine-tuning performance are positively correlated, yet their optimal pre-training configurations are largely diverging. In the Appendix, we also provide results on the worst pre-training configurations, some of which cause negative transfer due to initialising the encoders into local bad minima. Investigations are required to understand what kind of property makes the pre-trained encoders differ.

To this end, we propose MolGraphEval, which encompasses a variety of carefully-selected probe tasks, categorised into three classes: (i) **generic graph properties**, (ii) **molecular substructure properties** and (iii) **embedding space properties**. In the upcoming subsections, we explain the tasks in more detail and why they are essential for molecular graph embeddings.

### Generic graph properties

Topological property statistics are often used as features in machine learning pipelines on graphs that do not rely on neural networks . Based on their scale, they can be divided into {node-, pair-, and graph-} level statistics. For molecular graphs, topological metrics have been widely used as molecular descriptors in cheminformatics for decades [53; 54; 55; 56], metrics at different scales will facilitate different tasks.

**Node-level statistics** accompany each node with a local topological measure, which could be used as features in node classification . Concretely, node-level information such as degree  can reflect the reaction centres ; thus, it can aid in discovering chemical reactions .

Figure 2: **Hyperparameter space of AttrMask** (mask rate \(\) learning rate), coloured by MPP test scores. Above: fixed; below: fine-tuned. Note the colour bars are different.

* **Node degree** (\(d_{u}\)) counts the number of edges incident to node \(u\): \(d_{u}=_{v V}[u,v]\).
* **Centrality** (\(e_{u}\)) represents a node's importance. The eigenvector centrality is determined by a relation proportional to the average centrality of its neighbours: \(e_{u}=(_{v V}[u,v]e_{v})/\), \( u\).
* **Clustering coefficient** (\(c_{u}\)) measures how tightly clustered a node's neighbourhood is: \(c_{u}=(|(v_{1},v_{2}):v_{1},v_{2}(u)|)/ d_{u}^{2}\), _i.e._, the fraction of closed triangles in neighbourhood .

**Graph-level statistics** summarise global topology information and are helpful for graph classification tasks. For molecules, graph-level statistics can be used, _e.g._, to classify a molecule's solubility . We briefly describe their intuitions; formal definitions can be found, _e.g._, in .

* **Diameter** is the maximum distance between the pair of vertices (_i.e._, longest path in a molecule).
* **Cycle basis** is a set of simple cycles that forms a basis of the graph cycle space. It is a minimal set that allows every even-degree subgraph to be expressed as a symmetric difference of basis cycles.
* **Connectivity** is the minimum number of elements (nodes or edges) that need to be removed to separate the remaining nodes into two or more isolated subgraphs.
* **Assortativity** measures the similarity of connections in the graph with respect to the node degree. It can be seen as the Pearson correlation coefficient of degrees between pairs of linked nodes.

**Pair-level statistics** quantify the relationships between nodes (atoms), which is vital in molecular modelling. For example, molecular docking techniques aim to predict the best matching binding mode of a ligand to a macro-molecular partner . For predicting such binding compatibility, connectivity and distance awareness (how close a pair of atoms can be) are important. In our implementation, we randomly select a fixed number (_i.e._, 10) of atom pairs from each molecular graph. These pairs are then categorised based on their originating molecule, ensuring all pairs from a single molecule are designated to a singular split: either train, validation, or test.

* **Link prediction** tests whether two nodes are connected or not, given their embeddings and inner products. Based on the principle of _homophily_, it is expected that embeddings of connected nodes are more similar compared to disconnected pairs: \[_{}[u,v,_{u}^{T}_{v}]=_ {(u)}(v)\] (2)
* **Jaccard coefficient** seeks to quantify the overlap between neighbourhoods while minimising the biases induced by node degrees : \[_{}[u,v]=|(u)(v )|/|(u)(v)|\] (3)
* **Katz index** is a global overlap statistic defined by the number of paths between a pair of nodes: \[_{}[u,v]=_{i=1}^{}^{i}^{i}[u,v]\] (4) where \(^{+}\) determines the weight between short and long paths. \(<1\) reduces the weight of long paths, in implementations we set \(=1\) to give all paths equal importance.

### Molecular substructure properties

Molecular substructures often serve as reliable indicators of biochemical properties . For instance, molecules with benzene rings typically share consistent physical properties, such as solubility, as well as chemical characteristics like aromaticity .

**Substructures.** We investigate 24 substructures from three groups: **rings** (Benzene, Beta lactams,..., Thiophene); **functional groups** (Amides, Amidine,..., Urea); and **redox active sites** (Allylic). We provide chemical knowledge on how they relate with molecular properties in Appendix D.

**How predictive are substructures?** To demonstrate that substructures are quite predictive of molecular properties, we utilise counts of substructures within a molecular graph as the input for

   Linear Regression & Random Forest & XGBoost & Random (FIX/FT) & JOAO (FIX) & GraphCL (FT) \\ 
59.91 & 61.95 & 62.31 & 59.60 / 66.16 & 65.05 & 70.33 \\   

Table 2: **ROC-AUC scores of classifiers predicting molecular properties**.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

The purpose of this work is to complement current evaluation practices with probe tasks and metrics that reveal novel insights, rather than arguing about which combinations of pre-training tasks yield the best downstream performance. Also, as our primary focus is the pre-trained GNN encoders, we leave the investigations of comparing probing and fine-tuning embeddings in the future. Our empirical findings suggest that there are many open questions on how to learn robust molecular graph embeddings without labels and a better understanding of these, along with a new methodology for solving some of the issues mentioned earlier (_e.g._, dimensional collapse), are yet to come. Nevertheless, we are optimistic that the tasks proposed in this paper will benefit the GSSL research community to tackle these challenges and applied scientists in fields like drug discovery to yield additional insights that can help their problem.

## Acknowledge

We thank Le Song, Anima Anandkumar, Matthew Welborn for valuable discussions.