# Generating QM1B with PySCFIPU

Alexander Mathiasen\({}^{1}\)  Hatem Helal\({}^{1}\)  Kerstin Klaser\({}^{1}\)  Paul Balanca\({}^{1}\)  Josef Dean\({}^{1}\)

Carlo Luschi\({}^{1}\)  Dominique Beaini\({}^{2,\,3,\,4}\)  Andrew Fitzgibbon\({}^{1}\)  Dominic Masters\({}^{1}\)

\({}^{1}\)Graphcore \({}^{2}\)Mila - Quebec AI Institute \({}^{3}\)Valence \({}^{4}\)Universite de Montreal

###### Abstract

The emergence of foundation models in Computer Vision and Natural Language Processing have resulted in immense progress on downstream tasks. This progress was enabled by datasets with billions of training examples. Similar benefits are yet to be unlocked for quantum chemistry, where the potential of deep learning is constrained by comparatively small datasets with 100k to 20M training examples. These datasets are limited in size because the labels are computed using the accurate (but computationally demanding) predictions of Density Functional Theory (DFT). Notably, prior DFT datasets were created using CPU supercomputers without leveraging hardware acceleration. In this paper, we take a first step towards utilising hardware accelerators by introducing the data generator PySCFIPU using Intelligence Processing Units (IPUs). This allowed us to create the dataset QM1B with one billion training examples containing 9-11 heavy atoms. We demonstrate that a simple baseline neural network (SchNet 9M) improves its performance by simply increasing the amount of training data without additional inductive biases. To encourage future researchers to use QM1B responsibly, we highlight several limitations of QM1B and emphasise the low-resolution of our DFT options, which also serves as motivation for even larger, more accurate datasets. Code and dataset.

## 1 Introduction

Artificial Intelligence research is quickly moving towards building systems that are able to perform generally applicable functions using foundation models that are pretrained on billions of training examples. Foundation models have transformed natural language processing (NLP)  and computer vision (CV) , but have not yet been demonstrated in the important field of molecular machine learning. Addressing this has the potential to accelerate the design of new medicines and materials.

A promising approach within molecular machine learning is to train neural networks (NN) to approximate the predictions of quantum chemistry, yielding 1000x faster predictions [3; 4] with errors approaching that of experimental measurement. However, current molecular datasets are limited to 100k-20M training examples, that are seemingly insufficient to train foundation models. Due to the high cost and time consuming nature of constructing chemical datasets directly from experimental measurement, prior datasets rely on computational methods to generate labels. The computational method routinely used to generate machine learning datasets is Density Functional Theory (DFT). Although DFT strikes a favourable balance between accuracy and computational cost relative to other molecular simulation methods, its computational cost has nevertheless limited prior datasets1 to fewer than 20M training examples. Notably, prior DFT datasets such as QM9 [6; 7], ANI-1  or PCQ  were created by running DFT on CPUs. To the best of our knowledge no prior work utilises hardware acceleration to create larger DFT datasets for deep learning.

We introduce PySCF\({}_{}\), a DFT data generator which utilises Intelligence Processing Units (IPUs ) to accelerate molecular property dataset generation. We used PySCF\({}_{}\) to generate QM1B with one billion DFT training examples within 40000 IPU hours, substantially less than the two years it took to create PCQ on CPU supercomputers. The larger scale was enabled by several changes in the dataset generation which we outline in Section 4. The most important change, from a machine learning perspective, is that we increased the number of training examples by reducing the DFT accuracy. We advise that QM1B is only used for research purposes and discourage applications in deployed systems without carefully investigating the consequences of the reduced DFT accuracy. It remains to be found how pretraining on 1B low-resolution DFT training examples may bias a NN which is subsequently fine-tuned on downstream tasks. We encourage future research to investigate how DFT options bias subsequent neural networks by jointly iterating on dataset creation and model training. To enable such research we do not only release QM1B but also the entire software stack used to generate QM1B.

To investigate whether more training data improves neural networks, we trained a baseline SchNet with 9M parameters on differently sized subsets of QM1B. We found the validation Mean Absolute Error (MAE) improved from 285meV to 32meV, see Figure 1. Notably, the training and validation MAE overlap at the larger scales, indicative of underfitting, paving the way for future work to train large foundation molecular neural networks.

After pre-training SchNet on QM1B we fine-tuned SchNet on QM9. Compared to training SchNet on QM9 from scratch, we found this to improve validation MAE from 54.13meV to 30.2meV.

The contributions of this work can be summarised as follows:

1. We present PySCF\({}_{}\), a new hardware accelerated DFT data generator for deep learning.
2. We release the dataset QM1B, a low-resolution DFT dataset with \(10^{9}\) training examples.
3. We train a simple NN baseline which improves performance when trained on more data, validating the scale of QM1B.

## 2 Related Work

The core contributions of this work touch on three mature fields that have typically been studied in isolation. We provide a brief history of DFT software packages and follow this with their application to generating large quantum chemical datasets. Finally, we summarise efforts to approximate DFT with machine learning models.

### DFT Libraries

A suitable approximation to quantum mechanics can provide a universal framework for predicting properties of molecules and materials . The most widely applied approximation to quantum theory is Kohn-Sham DFT  which is implemented in a number of widely available DFT packages. These packages have evolved over decades and have successfully adapted to the constantly shifting landscape of high-performance computing. Notably, a substantial fraction of nation-level supercomputers are

Figure 1: We took 1.09M molecules with \( 9\) atoms from GDB11 [11; 12], and used PySCF\({}_{}\) to compute molecular properties (e.g. HOMO-LUMO (HL) gap) for a set of up to 1000 conformers. Three such sets are visualised here, corresponding to low, modal, and high variance of HL gap. Training a SchNet 9M model to predict HL gap shows improvement as the number of training samples approaches 500M. All training runs see \(10^{9}\) examples, sampling with replacement. We conjecture that billions of samples will enable the training of molecular foundation models.

dedicated to running DFT simulations, e.g., 30% of the US National Energy Research Scientific Computing Center (NERSC) and up to 40% of the UK ARCHER2 supercomputer [15; 16]. We highlight three advances in scientific computing that have accelerated progress in machine learning research that have yet to be widely adopted by DFT software libraries:

1. Ubiquitous hardware acceleration using low-precision numerical formats.
2. Tensor computations in a high-level API like NumPy .
3. Automatic differentiation .

Crucially, the core algorithm development of many widely used DFT packages predates the rise of NumPy and the Python ecosystem. This also coincides with the increased availability of specialised hardware accelerators for scientific computing. For this reason, most DFT libraries are still developed in languages like FORTRAN and C, without support for automatic differentiation. Hardware acceleration in this low-level programming environment is limited, with marginal performance gains reported . One notable exception is the commercial TeraChem library  which was engineered specifically for GPUs.

### DFT Datasets

The authors of previous DFT datasets made different choices, e.g., basis set and number of conformers. We list several examples in Table 2 limited to Gaussian basis DFT. 3 It is not clear how these choices impact subsequent deep learning models. We hope PySCFIPU will enable future researchers to jointly iterate on dataset creation and model training. While prior datasets use DFT libraries with some support for hardware acceleration, we found all dataset papers relied on CPUs.

### Neural Net Approximations to DFT

Similar to CV and NLP, molecular machine learning has seen substantial effort towards optimising neural networks to better approximate DFT. Broadly speaking, these approaches include denoising and auxilliary tasks [28; 29; 30; 31; 32], pretraining/finetuning [30; 33; 34; 35; 36], positional encodings [37; 38; 39; 40; 41] and architectural improvements [42; 43; 44]. Some work concentrated on performing equilibrium based predictions with 2D molecular representations bypassing expensive DFT-based structure optimisations [45; 46; 47; 48; 49].  investigated how NNs improve their DFT

  
**Library** & **Autodiff** & **Hardware** & **Precision** & **License** & **Language** \\  Gaussian9  & no & CPU/GPU & float64 & Commercial & - \\ TeraChem  & no & CPU/GPU & mixed float32/64 & Commercial & C++ \\ Psi4  & no & CPU/GPU & float64 & Open Source & C++ \\ GAMESS  & no & CPU/GPU & float64 & Open Source & C, Fortran77 \\ PySCF  & no & CPU & float64 & Open Source & NumPy, C \\ PySCFIPU (ours) & yes* & IPU & float32 & Open Source & Jax, C \\   

Table 1: Comparison of different Gaussian basis set DFT libraries frequently deployed on high performance computing clusters2. Despite the fact that there are many DFT libraries that support hardware acceleration, we know of none that have been used to generate machine learning datasets.

  
**Dataset** & **Graphs** & **Conformers** & **Basis Set** & **XC** & **Atoms** & **Time** & **Library** & **Hardware** \\  QM9  & 133.9k & 133.9k & 6-31G(2df,p) & B3LYP & \( 9\) & - &? & CPU \\ PCQ  & 3.38M & 3.38M & 6-31G* & B3LYP & \( 20\) & \(\)2 years  & GAMESS & CPU \\ SPICE  & 19238 & 1.13M & \(}{{}t2}\) & \(}{{}}}{{}}}{{}}}{{}}\) & 2-96 & - & PSI4 & CPU \\ ANI-1  & 57462 & 20M & 6-31G(d) & \(}{{}}}{{}}\) & \( 8\) & - & Gaussian9 & CPU \\ QM1B & I:09M & 1B & STO\(}{{}}\)3G & B3LYP & 9-1I & \(\) 5 days4  & PySCFIPU & IPU \\   

Table 2: Comparison of different DFT datasets.

approximation while increasing training examples. They concluded that current methods would require orders of magnitude more training examples to solve their catalysis task. For demonstration purposes, we chose to scale one of the simplest models, SchNet , to investigate how far performance can be improved by simply scaling training data without incorporating additional architectural innovations. In Table 3 we compare SchNet trained on QM9 against a selection of other methods that were trained from scratch (i.e. no pretraining/finetuning). We point out that the parameter count is orders of magnitude lower than foundation models in CV and NLP. Furthermore, the machine learning errors around 20-70meV are orders of magnitude larger than the convergence thresholds commonly employed in DFT. This motivated us to challenge the common assumption that DFT requires double precision (float64) for the special case of generating training data for machine learning.

## 3 PySCF\({}_{}\)

The Python-based Simulations of Chemistry Framework (PySCF) implements multiple Self-Consistent Field methods (SCF) including DFT. Importantly, relative to many other DFT libraries, PySCF is free to use, open-source5 and implemented mainly in Python and NumPy with a few of the computational demanding operations written in C. Furthermore, PySCF supports a plethora of DFT options, enabling it to reproduce quantum chemical properties as computed in machine learning datasets like QM9 or PCQ. All of the above made PySCF a good starting point to port for IPUs.

Intelligence Processing Units (IPUs) are a type of accelerated hardware designed specifically for machine learning workloads. We found IPUs allowed us to speed up DFT data generation due to two reasons:

1. IPUs have 940MB on-chip memory with 12-65TB/s bandwidth, enough to perform small DFT computations without relying on off-chip RAM with \(<\) 3TB/s bandwidth.
2. IPUs support Multiple Instruction Multiple Data (MIMD) parallelism which simplifies parallel computation of the notoriously difficult computation of Electron Repulsion Integrals.

### DFT Primer.

Our data generator PySCF\({}_{}\) implements DFT. To allow machine learning researchers to reason about our data generator, this section describes _what_ Gaussian basis set DFT implementations like PySCF\({}_{}\) do. We deliberately limit ourselves to explaining _what_ DFT to and exclude explaining the underlying physics, e.g., under which assumptions DFT can be derived from the Schrodinger equation (see instead  or [54, Appendix B]).

The inputs to a DFT computation are atomic positions and atomic numbers. These inputs are used to compute a matrix \(\) which represents the system of interest. One then solves the following matrix equation for \((,)\).

\[&++J(())+K( ())+V_{XC}(())=\\ &()=^{T}= D_{ij}= 2&i=j N_{/2}\\ 0&\\ & J()_{kl}=_{ij}I_{ijkl}^{2e}_{ij} K()_{il}=-_{jk}I_{ijkl}^{2e}_{jk}\]

Here \(^{2e}^{N N N N}\) while \(\{,,,,,\}\) are in \(^{N N}\), \((,)\) are diagonal matrices and \(\{,J,K,V_{XC}\}\) are functions with \((N,N)\) matrices as input and output. Given a physical system,

  
**Method** & \# **Params** & **MAE QM9 HL gap (meV)** & **MAE QM9 energy (meV)** \\  SchNet  & 432k & 63 & 14.6 \\ PaiNN  & 589k & 45.7 & **5.85** \\ DimeNet++  & 1.8M & 32.6 & 6.32 \\ GNS + Noisy Nodes  & - & **28.6** & 7.3 \\   

Table 3: Results of previous Neural Networks trained from scratch on the DFT dataset QM9.

the DFT algorithm proceeds in iterations. At iteration \(0\), we precompute \((,,,^{2e})\) and initialise \(_{1}\). At iteration \(i\), we compute \((_{i}=(_{i}),V_{XC}(_{i}),J( _{i}),K(_{i}))\), and then solve the above equation for \(_{i+1}\) (which corresponds to a generalised eigenproblem). If the iterations converge to self-consistency, one attains a solution \((,)\) from which several molecular properties can be computed, e.g.

HL gap \(:=_{HOMO}-_{LUMO}\)

DFT has two well-known trade-offs between accuracy and compute time:

1. \(V_{XC}\) attempts to correct the approximation error between DFT and the Schrodinger equation; hundreds of corrections exist such as LDA , B3LYP [55; 56] and \(\)B97x .
2. \(\) represents molecular orbitals as a linear combination of Gaussian functions; many such Gaussian basis sets have been extensively studied .

Prior work trained neural networks to improve the inherent errors from DFT for \(V_{XC}\)[59; 60] and the representation of molecular orbitals .

### PySCF IPU Implementation

To utilise IPUs we ported PySCF from NumPy to JAX , which can target IPUs by using IPU TensorFlow XLA backend . This left two remaining parts in C: 6

1. libxc which computes \(V_{XC}\)
2. libcint which computes \((,,,^{2e})\)

Libxc.We only support the B3LYP functional. The energy computation of B3LYP was implemented in JAX, which allowed us to use JAX autograd to compute the B3LYP potential. We use float32 instead of float64. JAX-XC  recently machine translated the libxc Maple files to JAX. This may allow us support more functionals by extending JAX-XC to float32.

Libcint.We implemented the computation of \(^{2e}\) by porting the libcint implementation of the Rys Quadrature algorithm from C to the IPU. The code implements a function int2e which is called many times with different inputs. Each call can be done in parallel. However, the different calls perform different computations making it tricky to parallelise using Single Instruction Multiple Data (SIMD). In contrast, using the IPUs MIMD parallelism all 8832 IPU threads can independently compute one call to int2e in parallel. \(^{2e}\) satisfies an 8x symmetry7 which, when exploited, (1) reduces bytes needed to store \(^{2e}\) by 8x and (2) gives an 8x reduction in FLOPs needed to compute \(K(,^{2e})\) and \(J(,^{2e})\) (usually computed using p.einsum). To utilise the 8x symmetry we implemented a custom einsum algorithm. The memory usage of our current implementation can be improved as outlined in Section 5.

Numerical Error of PySCF\({}_{}\).DFT libraries usually use double precision (float64), see Table 1. In contrast, NNs are usually trained in float32 to decrease compute time, e.g., some hardware accelerators can perform matrix multiplications more than 10x faster in float32 than float64. For the particular case of libcint, we found float32 to be around 6x faster than float64, likely due to IPUs ability to hardware accelerate float32 instead of simulating float64 in software (similar to the 10x reported by ). While float32 has yet to be thoroughly demonstrated in DFT libraries, our goal is to generate data to train NNs, therefore, our requirements for numerical precision is less stringent compared to mainstream DFT libraries, our errors just have to be below that of NNs (20-70meV for HL gap). To investigate our numerical errors, we recomputed DFT with PySCF in float64 for 10k SMILES strings from QM1B. This allowed us to compute a the mean absolute error (MAE) of PySCF\({}_{}\) relative to PySCF. For converged8 molecules our MAE was 6meV for energies and 0.2meV for HL gap, see Figure 2. Our numerical error for HL gap is thus 100x smaller than the errors achieved by neural networks (but similar for energy), see Table 3.

Timing PySCF\({}_{}\).Our logs recorded 38729 IPU hours generating QM1B. We estimate less than \(4\%\) was spent compiling. We completed the data generation within 5 days, by utilizing a varying number of 256-384 IPUs split over 16-24 POD16s. Each IPU POD16 has two physical EPYC CPUs with 240 vCPUs. On a POD16, for the example molecule FC=C1C2CN2N=C1C=O, we can do 82 DFTs/sec with PySCF and 228 DFTs/sec with PySCF\({}_{}\), see Figure 3 for a profile of PySCF\({}_{}\).

### Limitations

PySCF\({}_{}\) is an ongoing research project and thus has several limitations:

1. The 940MB limits PySCF\({}_{}\) to \( 12\) heavy atoms with a less accurate electron representation (STO-3G).
2. Larger numerical errors due to float32 instead of float64.
3. The first simulation of a molecule with \(N\) atomic orbitals takes approximately five additional minutes due to the ahead-of-time compilation model used for IPUs (for QM1B we estimate spending less than \(4\%\) time compiling).

Our current implementation supports restricted Kohn-Sham, B3LYP functional , no interatomic force and the basis sets {STO3G, 6-31G}.

## 4 Qm1b

We created QM1B by taking 1.09M SMILES strings from the Generated Data Bank (GDB)9 with 9-11 heavy atoms (GDB11). Hydrogen atoms were added by using RDKIT resulting in 0 to 11 Hydrogen atoms. We subsequently generated up to 1000 conformers for each molecule using the ETKDG algorithm  as implemented in RDKit . This led to a total of 305.8M, 568,7M and 205.4M conformers for 9,10,11 heavy atoms respectively. We then computed the following properties on the resulting 1B conformers using PySCF\({}_{}\):

1. Energy
2. Highest Occupied Molecular Orbital (HOMO) Energy
3. Lowest Occupied Molecular Orbital (LUMO) Energy

Conformers.Datasets like QM9 and PCQ use DFT to compute atom positions for which each molecule is in equilibrium, i.e., the forces acting on all atoms are zero:

\[=-}{ }=^{ 3}.\]

Equilibrium atom positions are computed through structure optimization, typically done by minimizing energy with respect to atom positions using standard optimisation methods such as conjugate gradients or LM-BFGS. This takes \(\) 50 iterations of LM-BFGS where each iteration use DFT to

Figure 3: Profile of our DFT computation for "Fc1nocnc(=O)c1=O".

Figure 2: Numerical error of PySCF\({}_{}\) in float32 compared to PySCF in float64. We suspect the difference between HL gap and energy is caused by energies being 100x larger than HL gaps.

compute \(\). One can thus get 50x more examples of DFT by evaluating DFT on atom positions that are not structure optimised (similar in spirit to ANI-1 ). By utilising the ETKDG algorithm to generate conformers we get 50x more data, while relying on the ETKDG algorithm to explore the conformational space. We investigate the diversity of conformers generated using this method in Figure 4(a), which depicts the distribution of the standard deviation of each molecule's HL Gap over all conformers. Figure 4(b) shows the distribution of conformers of a few molecules from the extremes of the dataset, and visualises several different conformers for each molecule super-imposed on top of each other.

Lower Resolution.For a fixed compute budget to generate data for machine learning, we can think of the DFT options as trading-off dataset size for data quality (DFT accuracy). Relative to prior datasets, we chose a less accurate DFT to generate more training examples. To compare the resulting inaccuracies, we compare the HL gap computed for the same molecule with different options. In particular, Figure 5 compares our STO-3G/B3LYP options against 6-31G*/B3LYP options (attempting to mimic the options from PCQ as closely as possible). The mean absolute error between the options are 360meV, much larger than chemical accuracy (43meV) or the errors attained by SchNet (63meV). It remains an open problem how pretraining on 1B such low-resolution conformers may bias a Neural Network subsequently fine-tuned on a smaller but more accurate dataset like PCQ. As a preliminary investigation, we pre-trained SchNet on QM1B and then finetuned it on QM9. Compared to only training SchNet on QM9, fine-tuning improved performance from 54.13meV to 30.2meV.

Figure 4: (a) If we compute the standard deviation of HL gaps over all conformers for a single SMILES string, then this figure plots the distribution of those standard deviations over all SMILES strings in the dataset. (b) The distribution of HL gaps across different conformers for 3 example molecules. These examples are chosen from the left tail, the peak, and the right tail of the distribution of all HL gap variances seen in (a).

Figure 5: Comparison of different DFT options for 10k molecules from QM1B. Notably, the disagreement between the DFT options are larger than chemical accuracy. It remains to be found how pretraining on 1B molecules with “lower resolution DFT” biases subsequent models fine-tuned on downstream tasks.

KDEplot of HL Energy Gap.To compare HL gap prediction tasks across different datasets we plotted the kernel density estimate (KDE) of the HL gaps. We plot these in Figure 6 for QM9, PCQ and QM1B. To further investigate the consequences of our lower resolution DFT, we also compared our DFT implementation with the optimised atom positions from QM9 against another version of QM9 with atom positions from ETKDG  as implemented in RDKit .

Basis Sets and Neural Network.While Figure 6 visualises the disagreement between STO3G and 6-31G*, it remains unclear how the disagreement impacts the training behaviour of neural networks. To investigate this we created a version of QM9 with the same DFT options as used for QM1B while using the same atom positions. This allowed us to train the same SchNet model on the same molecules, but with labels computed using different DFT options. Figure 7 shows that the resulting loss curves broadly overlap throughout training.

We also investigated the impact of using off-equilibrium molecules through the RDKit conformers, that is, we recomputed QM9 but using atom positions from RDKit conformers. We found the resulting loss curves were worse (larger MAE), suggesting off-equilibrium prediction may be a fundamentally harder task. We emphasise further experiments are needed to gain certainty, and point out that PySCFIPU may allow future research to investigate these types of molcular dataset biases.

Scaling SchNet Baseline on QM1B.We trained a 9M parameters SchNet model on varying size subsets of the full QM1B dataset. Each subsets consists of N molecule and conformer pairs sampled uniformly from QM1B. For the full dataset a single epoch of training is performed with the number of epochs for each smaller subset adjusted to keep the total number of iterations fixed. The results are presented in Figure 1 and report mean values for three independent seeds. They show that the validation MAE improves as more samples are added to the dataset, as expected. Interestingly we see that the training and validation performance plateaus after approximately 100M samples are in the dataset, this suggests that the model is underfitting, and increasing the model size would further improve performance.

Application Scenario: Pre-training on QM1B, fine-tuning on QM9.After pre-training the 9M SchNet on QM1B we fine-tuned it QM9. Compared to trained the 9M SchNet on QM9 from scratch, pretraining (and subsequent fine-tuning) improved validation performance from 54.13meV to 30.2meV. To avoid data leakage, we restricted fine-tuning to a subset of 88k molecules from QM9 for which their SMILES strings did not appear in the version of GDB used to create QM1B. For discussion of pre-training tasks see Appendix A.

## 5 Future Work: Larger Molecules and Higher Resolution DFT

The main limitation our of implementation is the fact that it runs out of memory when \(N>70\), where \(N\) is the number of atomic orbitals. This limited us to use \( 12\) atoms in STO-3G with a small grid size (10-30k).

Current (Naive) Bottleneck.We can view \(^{2e}\) as a matrix \(M^{N^{2},N^{2}}\) so \(K()=Mx\) where \(x=()\). For \(N=70\) we spend \(173\)MB computing \(v=Mx\). We represent \(M\) with the 8xsymmetry as a list of matrices of size (num_integrals, integral_size). \(M\) is stored distributed over all the cores inside one IPU and thus never needs to move. We compute \(v_{i}=Mx\) by copying \(x\) for every thread allowing us to compute \(x_{i}=M_{thread},x\) where \(M_{thread}\) is the part of \(M\) stored on core \(i\). The result is then \(v=_{i}x_{i}\). This uses \(N^{2}\). number_of_threads floats (or 173MB). The memory consumption can be reduced to 173MB/N=2.47MB by splitting \(x\) into \(N\) batches each with \(N\) floats. The current strategy was a stepping stone, the main advantage is ease of implementation. Notably, prior work disregarded this 8x symmetry at the cost of a 5-8x slowdown .

Recomputation.Instead of precomputing \(^{2e}\) we can recompute the entries of \(^{2e}\) whenever needed during the simultaneous computation of \(K(,^{2e})\) and \(J(,^{2e})\). For the case of Figure 3(a) it took 15.3M out of 105M cycles to compute \(^{2e}\). Recomputing \(^{2e}\) all 20 iterations would increase cycle count from 105M to an estimated 395.7M cycles (57.5ms to 216.8ms on a 1.825GHz Mk2 BOW). Finally, if we also recompute the evaluation of the atomic orbitals on the XC grid our memory consumption becomes \(N^{2}+ 4+B\), where \(B\) is the memory used while (re)computing \(K(,^{2e})\) and \(J(,^{2e})\) (independent of \(N\)).

Using Multiple IPUs.We exemplify our plan to parallelise over 15GB SRAM in 16 IPUs following Figure 3(a). The computation of \(^{2e}\) (Eletron Repulsion Integral) can be split over 16 IPUs in the same way we already utilise MIMD parallelism to split them over the 8832 threads. The computation of \(K\) and \(J\) can also be split over 16 IPUs with one reduce_sum of size \(N^{2}\). The remainder of the computation can be repeated independently on each IPU.

## 6 Discussion.

Prior work usually falls in one of the following three categories: (1) DFT libraries, (2) DFT datasets, and (3) neural networks trained on DFT datasets. This article touches on all three, demonstrating how choices made for DFT libraries and dataset generation (1,2) can impact neural network training (3). As molecular machine learning turns towards training large foundation models, we hope our work will help generate sufficiently large datasets with billions (or trillions) of molecules. Furthermore, we hope to allow researchers to generate datasets for downstream tasks used solely to fine-tune such foundation models.

## 7 Broader Impact.

Our work attempts to improve neural network approximations to Density Funtional Theory (DFT) by generating larger datasets. Improving DFT holds the promise to accelerate the development of new medicines and materials, with positive implications for health outcomes and global warming. However, the same technology may someday also allow bad actors to design pathogens, viruses or weapons.