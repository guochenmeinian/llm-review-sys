# Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space

Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space

 Leo Schwinn

Technical University of Munich,

Munich Data Science Institute

l.schwinn@tum.de &David Dobre

Mila, Universite de Montreal

david.dobre@mila.quebec &Sophie Xhonneux

Mila, Universite de Montreal

lpxhonneux@gmail.com &Gauthier Gidel

Mila, Universite de Montreal

Canada AI CIFAR Chair

gidelgau@mila.quebec &Stephan Gunnemann

Technical University of Munich,

Munich Data Science Institute

s.guennemann@tum.de

###### Abstract

Current research in adversarial robustness of LLMs focuses on _discrete_ input manipulations in the natural language space, which can be directly transferred to _closed-source_ models. However, this approach neglects the steady progression of _open-source_ models. As open-source models advance in capability, ensuring their safety becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the _embedding space attack_, which directly attacks the _continuous_ embedding representation of input tokens. We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Additionally, we demonstrate that models compromised by embedding attacks can be used to create discrete jailbreaks in natural language. Lastly, we present a novel threat model in the context of unlearning and data extraction and show that embedding space attacks can extract supposedly deleted information from unlearned models, and to a certain extent, even recover pretraining data in LLMs. Our findings highlight embedding space attacks as an important threat model in open-source LLMs.

## 1 Introduction

With LLM-integrated applications getting increasingly prevalent, various methods have been proposed to enhance the safety of LLMs after deployment . Despite these efforts, LLMs have remained vulnerable to exploitation by malicious actors . The majority of investigated threat models in the literature, such as prompt injection  or jailbreaking , operate on the discrete token level .

This choice is influenced by the accessibility of LLM-integrated applications, such as ChatGPT , which are mostly available through APIs limited to natural language input.

However, various malicious activities can be executed using open-source LLMs on consumer hardware and do not require interaction with APIs (e.g., distributing dangerous information, promoting biases, or influencing elections). In the case of open-source models, an adversary has complete access to the weights and activations. As a result, the adversary is not limited to discrete manipulation of natural language tokens but can directly attack their continuous embedding space representation . Meanwhile, the capability of open-source models is increasing rapidly, with the performance gap between the best open-source model and the best closed-sourced model considerably decreasing in the last year. As of May 4th, the publicly available Llama-3-70b-Instruct model is the 6th best model on the LMSYS Chatbot Arena Leaderboard , a popular benchmark for comparing the capability between LLMs. We want to emphasize the following:

_As open-source models close the gap to proprietary models and advance in their capabilities, so does their potential for malicious use, such as influencing elections, sophisticated phishing attempts, impersonation, and other risks ._

Thus, it is crucial to investigate possible threats in open-source models and precisely quantify their robustness to understand and manage risks post-deployment.

Furthermore, with regulations like the General Data Protection Regulation (GDPR) setting standards for data privacy, there is a critical need for reliable tools to explore and ensure privacy in machine learning. This includes scenarios where user data is used to train models and presumably removed with unlearning. However, as previously outlined threat models tailored to open-source models are currently under-explored. In this paper, we demonstrate the threat of continuous _embedding space attacks_ (see Fig 1). on two important security problems. We first investigate their ability to induce harmful behavior in open-source models with low computational cost. Our experiments raise the question of whether it is possible to protect open-source LLMs from malicious use with current methods, as the robustness of neural networks against adversarial attacks in the vision domain has remained an open question for more than a decade. Secondly, we study the ability of embedding space attacks to reveal seemingly deleted information in unlearned LLMs, highlighting a new use case of adversarial attacks as a rigorous interrogation tool for unlearned models. Lastly, we show in a preliminary experiment, how embedding space attacks can be used to extract pretraining data from a model. Our contributions are as follows:

1. We show that embedding attacks effectively remove safety alignment in LLMs on four different open-source models, achieving successful attacks orders of magnitude faster than prior work.
2. We find that compared to fine-tuning an LLM to remove its safety alignment, embedding space attacks prove to be computationally less expensive while achieving higher success rates.
3. Additionally, we show that embedding space-attacked models can be used to generate discrete jailbreaks in natural language that successfully remove the safety guardrails of aligned models. This indicates the potential of continuous threat models to transfer to proprietary models.
4. For the Llama2-7b-WhoIsHarryPotter model  and the recently proposed TOFU benchmark , we demonstrate that embedding space attacks can extract considerably more informa

Figure 1: Illustration of discrete and embedding space attacks (this work). Discrete attacks manipulate discrete one-hot tokens \(T_{adv}\), whereas embedding space.

tion from unlearned models than direct prompts. This presents a novel use case for adversarial attacks as an "interrogation" tool for unlearned models.
5. Lastly, we demonstrate on the Llama3-8B model that embedding space attacks can extract pretraining training data from LLMs. Specifically, we show that embedding space attacks optimized towards the completion of Harry Potter books transfer to unseen paragraphs not used during attack optimization, effectively extracting training data from the model.

## 2 Related Work

In natural language processing (NLP), continuous adversarial attacks have mostly been used in encoder-decoder models, such as BERT [13; 14; 15; 16; 17; 18], and in the context of adversarial training. Jiang et al.  use adversarial attacks as a smoothness regularization to improve generalization. Moreover, multiple works apply continuous adversarial perturbation to word embeddings to increase performance in different NLP tasks [14; 15; 16; 17; 18].

In text context of adversarial attacks, Carlini et al.  and Zou et al.  find that past attacks that show high attack success rates on BERT-style encoder-decoder models [20; 21; 22], only achieve trivial success rate against state-of-the-art decoder-only LLMs, such as LLama-2 . Later work proposes the first effective white-box attack in the context of LLM assistants  showing that Greedy Coordinate Gradient (GCG) prompt injection attack successfully generates adversarial examples that transfer from small open-source models such as Llama7b to large closed-source models. Geisler et al.  propose the first discrete attack attack that is optimized in the continuous embedding space which achieves a non-trivial success rate against state-of-the-art autoregressive LLMs. Another threat model consists of so-called jailbreaks, bypassing a given model's safety guardrails through prompt engineering. In the beginning, jailbreaking prompts were created manually through human red teaming. Later work showed that jailbreaks automatically generated by LLMs can break open-source and proprietary models . Huang et al.  show that simply using different generation strategies, such as decoding hyper-parameters and sampling methods can trigger harmful behavior in LLMs.

Numerous works have investigated extracting pretraining data from LLMs [25; 26; 27]. In concurrent work, Patil et al.  explored extraction attacks against unlearned LLMs. In contrast to the presented method, the authors do not use adversarial attacks to attack unlearned models but propose different logit-based methods to extract information. Closely after the publication of this work, Lucki et al.  showed that unlearning is not robust from an adversarial perspective, while Wu et al.  demonstrated that current unlearning techniques fail to reliably unlearn facts, supporting our findings.

In summary, numerous works and reviews consider discrete and black-box threat models in autoregressive decoder-only LLMs, such as jailbreaking [3; 5], prompt injection , backdoor attacks , multi-modal attacks , and other threats . However, open-source threat models are largely underexplored [33; 34] and we close this gap in this work. For an extended description see App. 8.

## 3 Open-Source Specific Threat Models

Before we describe the proposed method, we first define desiderata of open-source threat models.

DesiderataThe increasing capability of open-source models will inevitably increase their potential to cause harm. While current models may mostly pose the threat of influencing public opinion or elections, future models may be used for impersonation or malicious autonomous agents. In the following, we present desiderata of attacks tailored to open-source models.

* _Success rate_: The extent to which the attack successfully circumvents the safety guardrailes across different (diverse) requests, indicating its overall effectiveness in compromising the model.
* _Efficiency_: How fast the attack can be computed. We identify two major use cases for open-source-specific attacks, where efficiency is essential. First, as a threat model to produce harmful outputs in open models. Second, as a tool to investigate model properties, such as robustness or unlearning quality.
* _Utility_: In case of a successful attack, verifying if the subsequent model generation is meaningful is important. Since token embeddings manipulated with continuous perturbations do not correspond to any real words or tokens in the model's vocabulary, it is unclear whether such a continuous attack could maintain model utility while simultaneously removing safety guardrails. Thus, the coherence and quality of the generated text must be considered when evaluating an attack.

In our experiments, we demonstrate that embedding space attacks are superior to existing threat models, such as discrete attacks or finetuning, for all three desiderata in the open-source domain.

Embedding Space AttacksIn embedding space attacks, we keep the weights of a model frozen and attack the continuous embedding representation of its input tokens. We propose the embedding space attack as a computationally effective tool to investigate the presence of knowledge in open-source models (e.g., toxic behavior, supposedly unlearned information, or training data), which can be used by researchers and practitioners alike to improve the safety of LLMs. As we are generally interested in the worst-case output behavior of the model, we do not put any constraints on the generation of embedding space attacks, such as restricting the magnitude of the perturbation.

In the following, we formalize embedding space attacks. Specifically, we denote with \(T\) a set of tokenized inputs \(T=\{T^{1},T^{2},,T^{i},,T^{N}\}\), where \(T^{i}^{n_{i} d}\) is a single tokenized input string with \(n_{i}\) tokens of dimensionality \(d\), and \(y_{i}^{m_{i} d}\) is the respective harmful target response consisting of \(m_{i}\) tokens. Let \(E:T e\) be an embedding function that maps a set of tokens to a set of embedding vectors. We define \(e_{i}^{n_{i} D}\) as the embedding representation of the tokens \(T^{i}\), where \(D\) is the dimensionality of the embedding vector. Given an LLM: \(F:e_{i}_{i}^{n d}\), we want to find an adversarial perturbation \(_{i}^{ D}\), to minimize a predefined objective. Here, \(\) is the number of attacked tokens, which we set to the same value for all samples \(i\). In our experiments, we minimize the difference between the target response \(y\) and the prediction of the \(\) using the cross entropy loss function \((,y)\)

\[_{i}^{t+1}=_{i}^{t}-( (F(e_{i}||_{i}^{t}),y) ).\]

Here \(_{i}^{t}\) denotes the adversarial perturbation of sample \(i\) at time step \(t\), \(\) the step size of the attack, \(||\) the concatenation operator, and \(()\) the sign function. We evaluated different optimization methods, including 1) regular gradient descent, 2) \(_{2}\)-norm scaled gradient descent, 3) gradient descent with momentum, and 4) signed gradient descent. Signed gradient descent proved to be the most stable, and we used it for all experiments presented in this paper. We simultaneously optimize all adversarial embeddings at once in contrast to previous approaches that optimize the adversarial tokens sequentially Zou et al. . An illustration of the threat model is given in Fig. 2.

We define two goals for embedding space attacks.

_Individual Attack_: For each specific input \(T^{i}\) and its respective embedding \(e^{i}\), we optimize a unique adversarial perturbation \(e_{adv}^{in}\):

\[_{_{i}}(F(e_{i}||_{i}),y_{i} ).\]

This approach focuses on achieving the optimization goal for one individual sample at a time.

_Universal Attack_: Here, we optimize a single adversarial perturbation \(_{un}\) across the entire set of tokenized input strings \(T\) and their associated embeddings \(e\):

\[_{_{un}}_{i=1}^{N}(F(e_{i}|| _{un}),y_{i}),\]

where \(N\) is the total number of tokenized input strings in the data set. Consistent with the universal adversarial examples outlined in previous studies , this attack aims to generalize its effectiveness to unseen instructions. A successful universal embedding attack has the potential to bypass a model's safety alignment across a wide range of instructions.

Figure 2: We use a similar setting as in  with the difference of optimizing attacks in the embedding space. Given an **<instruction>**, an adversarial embedding is optimized to trigger an affirmative **<target>** response, with the **<goal>** of triggering a subsequent generation related to the target. The **<goal>** is not provided during attack optimization.

Independent of the attack type, **we never leak any information about the optimization goal** (e.g., keywords related to an unlearning task) to the model during the attack optimization.

_Multilayer Attack:_ We additionally propose a variation of the embedding space attack where we decode intermediate hidden state representations of a given model, which we call multi-layer attack. This method is inspired by the logit lease [28; 36] and is designed to extract if information is propagated within a model, such as supposedly unlearned associations.

Let \(h^{l}\) represent the hidden state at layer \(l\), where \(l\) is an element in the set of layers \(P\{1,2,,L\}\). For a predefined set of layers \( P\), we perform greedy decoding for \(K\) number of tokens, with \(T_{k}\) representing the sequence of tokens at the \(k\)-th generation step. In every step \(k\) and for each layer \(l\), we decode the hidden state \(h^{l}\) for the current sequence of tokens \(T_{k}^{L}\). This yields \(L\) sequences of tokens \(\{T_{k+1}^{1},T_{k+1}^{2},,T_{k+1}^{L}\}\), each corresponding to the greedy decoding of the respective layer. In the next iteration, \(k+1\) we use the decoded output of the last layer \(T_{k+1}^{L}\) as the input of the model and repeat this process (see Fig. 3). For multi-layer attacks, we always include the last layer in the subset \(P\) to compute \(T_{k}^{L}\) and thereby obtain the standard output as well.

## 4 Experiment Configurations

All experiments were conducted on four A100 GPUs with 40GB of memory.

### Models

We use seven different open-source models in our evaluations. This includes Llama2-7b-Chat , Llama3-8b-Chat , Vicuna-7b , and Mistral-7b . We also attack Llama3-8b-CB and Mistral-7b-CB, which are trained to withstand adversarial attacks through circuit breaking . In the original evaluation by the authors, these circuit breaker models demonstrated remarkable robustness, where nearly all attempted attacks achieved nearly zero percent attack success rate. Lastly, we use Llama2-7b-WhoIsHarryPotter , a Llama2-7b model fine-tuned to forget Harry Potter-related associations (see App. B).

### Datasets

**Harbmbench.** We use the standard behaviors of the harmonech dataset to measure the ability of embedding space attacks to bypass safety guardrailes .

Only a few benchmarks in the research area of unlearning LLMs have been proposed so far [12; 42]. We use two recent benchmarks in this work:

**Harry Potter Q&A.** We perform experiments on the LlamaHP model, which was fine-tuned to forget Harry Potter-related associations. For this purpose, we created a custom Harry Potter Q&A benchmark dataset. We generated \(55\) unique questions using GPT4 that can be answered with simple responses. Specifically, we asked GPT4 to generate affirmative target responses for embedding space attacks that do not leak the answer to the respective Harry Potter question and reviewed the generated dataset manually for correctness. This allows us to use a keyword-based evaluation to identify successful attacks. We do not leak any information about the keywords during attack optimization.

**TOFU.** We use the recently published TOFU dataset, an unlearning benchmark of fictitious content for LLMs . We first fine-tune Llama2-7b-chat-hf models on the provided data and subsequently unlearn the models using the \(99\%\) retain set and the \(1\%\) forget set. We use the gradient ascent method and the gradient difference method to unlearn the models. For more information concerning the unlearning methods please refer to . Finally, we evaluate the unlearned models with embedding space attacks. We use the same hyperparameters for fine-tuning and unlearning as in the original

Figure 3: Illustration of the multi-layer attack. From a regular generated sequence \(T_{k}^{L}\), we decode alternative output sequence \(T_{k}^{L}\) from intermediate layers of the neural network.

paper and evaluate the unlearned model after \(4\) iterations of unlearning. For TOFU, we use "Sure, the answer is" as the target \(y\) for embedding space attacks.

### Metrics

We used multiple metrics to measure the harmfulness of model generations and evaluate if supposedly unlearned knowledge was retrieved by an attack. To evaluate if a generation was harmful and calculate the attack success rate (ASR) we use the judge model provided by .

**Cumulative attack success rate (C-ASR).** Existing works evaluate the effectiveness of unlearning methods in a one-shot manner, where a model is assessed on a given task only once. We argue that depending on the sensitivity of the information, an unlearned model mustn't be able to reveal any of the unlearned information even for multiple prompting attempts. This is in line with Kerckhoff's Principle, which asserts that a system's security should not depend on obscurity. Thus, to obtain an accurate estimate of the information that can be retrieved for an unlearned model in the worst case, we propose a new metric, which we call cumulative attack success rate _C-ASR_.

For a given query, such as "Who are the best friends of Harry Potter?", we attack an unlearned model \(n\) times and denote an attack as successful if the correct answer appears at least once for any of the attack attempts. We choose a relatively small value for \(n\) of \(20\) for all experiments to keep the attacks practical. While we mainly propose this metric for the unlearning task, we also use it in the toxicity evaluation setting. We argue that disclosing sensitive information or giving toxic responses should not occur, even across multiple inquiries. We give a more formal definition in App. E.

**Toxicity score and perplexity.** For a more nuanced evaluation, we additionally analyze the toxicity of each response using the toxic-bert model  and the perplexity of the generated output using the respective base model.

**ROUGE score.** We additionally use the ROUGE metric , which measures the quality of a summary given a reference summary and is one of the evaluation metrics used in the TOFU benchmark . For all experiments, we use a cumulative variation of the ROUGE-1 score, where we calculate the minimum ROUGE-1 score over a set of responses for a given query.

### Attack methods

**Embedding space attacks (ours).** We use \(200\) attack iterations for toxicity-related experiments and \(100\) for attacks against unlearned models. For the two models trained with circuit breaking , we use \(2000\) attack iterations. We evaluate the success of an attack \(20\) times, spaced evenly throughout the optimization process, and report the C-ASR metric if not stated otherwise (see also App. A).

**Discrete attacks.** We chose state-of-the-art discrete attacks, including GCG , AutoDAN , PAIR , and Adaptive attacks , which were chosen based on the results reported in . For all attacks, we use the hyperparameters provided in the original work.

**Fine-tuning.** We compare the ability of embedding attacks to remove the safety alignment of open-source models with fine-tuning the Llama2 model, which has shown to be an effective way to compromise safety in prior work . We fine-tune the Llama2 model with QLoRa on the Anthropic red-teaming-data . More details are provided in App. D.

**Unlearning attacks.** We use multinomial sampling in combination with the C-ASR metric. Here, we sample from an unlearned model multiple times, considering the top-k most likely answers, corresponding to the top-k highest logits in the last layer. For the top-k attack, we use \(100\) samplings, \(k=10\), and optimize the temperature hyperparameter using a grid search between \(0.1\) and \(10\). Additionally, we use the Head-projection attack (HPA), which is an unlearning attack based on the logit lens . Lastly, we compare our approach to the Probability delta attack (PDA) . This attack is also motivated by the logit lens and leverages the rank-ordering of token probability differences between consecutive layers to identify the most likely tokens corresponding to supposedly unlearned information. Hyperparameters for HPA and PDA are taken from .

## 5 Breaking Safety Alignment

We compare our approach to existing discrete adversarial attacks and model fine-tuning. Discrete attacks are the most prevalent threat model in LLMs [3; 4; 5], whereas model finetuning is computationally efficient . We first report results on attacks trained and evaluated on the training dataset. This setting allows us to use embedding attacks as an investigative tool. For example, to explore if a model contains knowledge related to a specific harmful topic. It is also a relevant threat model when a malicious actor wants to trigger harmful behavior for a set of predefined instructions, e.g., in applications that entail generating toxic content at scale.

**Training Data Evaluations.** In the first row of Fig. 4, we report the ASR for individual attacks, where attacks are trained and evaluated on the same data. In our experiments, individual embedding attacks achieve a C-ASR of \(100\%\) for all models. The only other attack that achieves similar ASR is the adaptive attack proposed by . However, this attack requires manual engineering of a suitable attack template for each model. As a result, we were not able to evaluate it for the two models trained with circuit breaking, for which the authors provide no template. Moreover, embedding attacks are the only algorithm that is able to bypass the defense of the circuit breaker models. In the original evaluation conducted by the authors, even latent attacks were unable to bypass this defense and did not achieve high success rates . In the second row, we compare the inference time of the different attacks. Embedding space attacks are orders of magnitude faster than all other attacks for all models. We additionally, conducted an attack on Llama-3-70b-Instruct  to evaluate if embedding attacks are effective for larger models. This attack also achieved a success rate of \(100\%\).

**Generalization to Unseen Behaviors.** To investigate the ability of embedding space attacks to generalize to unseen harmful behaviors, we trained universal embedding attacks on the first \(50\%\) of the dataset and evaluated them on the remaining samples. In this setting, embedding attacks achieve an ASR of \(91.6\%\) for LLama2-7b-Chat, \(97.5\%\) for Mistral-7b-CB, and \(96.25\%\) for LLama3-8b-CB, demonstrating that embedding attacks are highly transferable and an efficient tool to circumvent safety alignment of even the most robust existing models.

**From Continous to Discrete Threat Models.** Embedding space attacks are designed as a threat model tailored to open-source models. Here, we conduct two simple experiments to assess the possibility of transferring continuous threat models to a discrete setting. In line with prior work, we observe that discretizing the attack after optimization to the nearest embedding does not result in effective discrete attacks . Inspired by previous work to generate jailbreaks with LLMs , we evaluate if _aligned_ models that are attacked with universal embedding attacks can be used to generate discrete attacks. We first optimize a universal embedding attack for the Llama2 model using \(50\%\) of the harmonbch standard behavior dataset. Next, we prompt the embedding attacked LLama2 to generate discrete attacks for the unseen behaviors (prompt template can be found in App. C). We generate \(200\) discrete attacks for every unseen behavior (one attack in every iteration of

Figure 4: Attack success rate and average compute time of diverse discrete attacks and the proposed embedding attack for different models. Embedding attacks achieve higher success rates and are considerably more efficient compared to existing methods for all tested models.

the embedding attack optimization). This simple attack achieves \(43.6\%\) ASR, whereas the model generally declines the request to generate jailbreaks when it is not attacked (\(0\%\) ASR).

### Comparison to Fine-tuning

To evaluate embedding space attacks as a threat model in open-source language models, we compare the attack success rate between universal embedding space attacks trained on \(50\%\) of the harmonech standard behaviors with fine-tuning a model to remove the safety alignment (for detailed information see SS 4.4). For both methods, we evaluate the model on the remaining \(50\%\) samples. In our experiments, fine-tuning for \(50\) iterations achieves an attack success rate of \(88.8\%\). In contrast, universal embedding space attacks achieve the same attack success after \(9\) iterations and a higher success rate of \(91.6\%\) after \(20\) iterations. Removing the safety alignment with universal embedding attacks is considerably less expensive than fine-tuning with the attack requiring significantly less memory ( \(25\%\)) and fewer iterations. In contrast to finetuning, the attack does not require any training data for optimization and can be performed without any examples of toxic behavior. Moreover, individual attacks achieve an affirmative response in only \(9\) iterations on average in our experiments, which is orders of magnitudes faster than comparable discrete attacks [4; 45] and considerably faster than finetuning.

### Impact on Perplexity and Toxicity

While the previous experiments demonstrate that embedding space attacks can trigger harmful responses in safety-aligned models, it is unclear if these attacks reduce the output perplexity of the model (i.e., if the model still generates high quality text). For a more nuanced evaluation, we calculate the perplexity and toxicity of generated answers with and without embedding attacks. For the toxicity evaluation, we generate toxicity scores between \(0\) and \(1\) using the toxic-bert model . Fig. 5 shows boxplots of the perplexity and toxicity values of generated responses from different models using the instructions of the harmful behavior dataset as input. Perplexity and toxicity are measured only on the generated response, which does not in

Figure 5: The two rows show the perplexity and toxicity (obtained from toxic-bert) of generated responses of different LLMs with and without embedding space attacks on the harmful behavior dataset. Additionally, the scores of the fine-tuned Llama2 model are compared to attacking the regular Llama2. Embedding attacks decrease perplexity for all models while significantly increasing toxicity for most models (significant differences with a Mann–Whitney U test are indicated with *).

Figure 6: Number of toxic responses (\(y\)-axis) and the binned attack loss (\(x\)-axis) for a universal embedding space attack on the Llama2 model is shown. It can be observed that lower loss values are associated with higher toxicity.

clude the attack instruction and target. Attacking the model does not lead to higher perplexity in our experiments. Surprisingly, all models exhibit lower perplexity values on responses generated under attack. This effect is most pronounced for the Mistral model, where the perplexity of the response is considerably smaller with the attack. Fine-tuning has less effect on the toxicity of the answers than embedding attacks. We find that filtering for responses that show low perplexity and high toxicity is an effective way to find probable harmful responses. Examples are given in App. L. We conducted a Mann-Whitney-U tests  to compare the toxicity levels of attacked and unattacked models (see also App. F). Attacked models show significantly higher toxicity values for the Llama2, and Mistral model. Significant differences are indicated with * symbols, where *, ***, *** correspond to \(p<0.05\), \(p<0.001\), \(p<0.0001\), and \(p<0.00001\), respectively. To adjust for multiple comparisons, we applied the Bonferroni correction method . We further evaluated if the attack objective correlates with the toxicity of the generated responses. Fig. 6 shows that attacks with lower loss values result in considerably more toxic responses compared to attacks with higher loss values. We considered responses as toxic if the _toxicity score_ of the toxic-bert model was higher than \(0.1\).

## 6 Attacking Unlearned Models

In a second task, we investigate the ability of embedding attacks to retrieve knowledge from unlearned models. Precisely, we assess if we can make a model give correct answers to a predefined set of questions that are answered wrongly when prompting the model without attack. We do not leak any information about the unlearned information during the optimization of the attack. We conduct experiments on the LlamaHP model , which we evaluate on the proposed Harry Potter Q&A and additional experiments on the TOFU benchmark . To the best of our knowledge, adversarial perturbations have not been explored in the context of unlearning yet.

**Harry Potter Q&A** The standard Llama2 model achieves an accuracy of \(34.5\%\) on the Harry Potter Q&A, while the unlearned model achieves an accuracy of \(3.6\%\). Examples of questions of the Q&A and answers of the standard and unlearned model are given in App. G. Tab. 1 summarizes the success rate of individual and universal embedding space attacks using multi-layer attacks (All) and standard attacks (Last). Embedding attacks expose considerable residual Harry Potter knowledge in the unlearned model. Under attack the accuracy of the unlearned model on the Q&A is close to the original non-unlearned model (\(30.9\%\) vs. \(34.5\%\)). Multi-layer attacks achieve an overall higher attack success rate compared to standard attacks in all experiments. In App. H, we provide detailed analysis concerning the contribution of individual layers to the attack success rate. Increasing the amount of attacked tokens hurts the success rate of all conducted attacks. Moreover, universal attacks consistently perform better than individual attacks. Our results indicate that embedding space attacks are prone to overfitting the objective. Regularizing the attack by computing only one universal perturbation for the whole task improves the success rate. Likewise, reducing the amount of attacked tokens improves the performance of the attack. We explore this further and demonstrate that overfitting is connected with low attack success rate in App. I. Furthermore, we explore top-k sampling with a temperature parameter of \(2\) (see SS4.4) as an attack for retrieving deleted information in unlearned models. Using \(100\) iterations of top-k sampling for every question in the Harry Potter Q&A achieves a _CU_ of \(27.8\%\). By combining embedding space attacks and top-k sampling, we achieve a _CU_ of \(38.2\%\), which is higher than the one-shot accuracy of the standard model on the Q&A.

In a separate experiment, we investigate the generalization of embedding space attacks in the context of unlearning. For this purpose, we train a universal attack using one set of Harry Potter Q&A questions and then evaluate its effectiveness on a different set of questions. The best attack with a \(25/75\) train/test split achieves a similar success rate of \(28.6\%\) compared to \(30.9\%\) when evaluating the attack on the training data. Additional results are deferred to App. J.

  Attack-Type & 1-token & 5-tokens & 20-tokens & HPA & PDA \\  Individual & 25.5 & 21.8 & 20 & 7.2 & 9.0 \\ Universal & 30.9 & 30.9 & 25.5 & N/A & N/A \\  

Table 1: C-ASR of embedding space attacks (using either 1,5, or 20 suffix tokens for the attack) and the two baselines HPA and PDA for the LlamaHP model on the Harry Potter Q&A.

**TOFU** Regular prompting unlearned Llama2 model results in cumulative ROUGE-1 scores of \(0.28\) and \(0.26\) for gradient ascent (GA) and gradient difference (GD), respectively. Tab. 2 summarizes the cumulative ROUGE-1 score of different embedding attack configurations on the same dataset. Embedding space attacks increase the ROUGE score considerably to at least \(0.49\) for individual and universal attacks with no considerable dependency on the number of attacked tokens. As with the previous experiments, universal embedding attacks generalize to unseen samples on the TOFU benchmark, achieving a ROUGE score of up to \(0.52\) with a \(25/75\%\) train/test split. For a more detailed description, see App. K.

## 7 Training Data Extraction

To explore the broader implications of embedding space attacks, we conducted an experiment focused on extracting training data from pretrained LLMs. Since the training data for LLMs is generally undisclosed, even in open-source models, effective extraction attacks would present a substantial threat. We created a dataset of sentence pairs from the first Harry Potter book, splitting it into \(527\) training pairs and \(100\) test pairs. Each pair consisted of an instruction (first sentence) and a target (second sentence). We then optimized a universal embedding space that was attached to the instruction to improve the prediction of the target sentence for the Llama3-8B model. We also prepended the instruction with the sentence: _"Continue the following paragraph from the first Harry Potter book."_ to provide context about the task to the model. This was done, as most instruction sentences are generic and contain no clues about the subject of Harry Potter. Using the test dataset, we evaluated whether this universal attack improved the LLM's ability to complete unseen target sentences. Indeed, the universal attack improved the ROUGE-1 F1-score from 0.11 to 0.22, demonstrating that embedding space attacks can be used to extract training data from LLMs.

## 8 Discussion

This work investigates three use cases of embedding attacks in open-source LLMs. Firstly, we establish that embedding space attacks present a viable threat in open-source models, proving more efficient than fine-tuning at bypassing safety alignment. Secondly, we demonstrate their capacity to uncover allegedly deleted information in unlearned models. Lastly, we present preliminary results in extracting pretraining data. Our findings demonstrate that embedding space attacks are a cost-effective yet potent tool and threat model for probing undesirable behaviors in open-source LLMs.

**Limitations and Future Work.** To bypass safety alignment and unlearning methods, we optimize embedding space attacks to trigger an affirmative response. We did not evaluate if the affirmative response objective leads to more model hallucinations. This is a common limitation in the evaluation of adversarial attacks in the LLM setting [4; 5]. Generally, attacks are considered successful if the model responds coherently to the query, but the correctness of the response is not evaluated, which can lead to inaccurate assessments . To improve the efficency of embedding space attacks, future work can further explore improved loss functions [53; 54] or optimization schemes .

**Broader Impacts.** The challenge of ensuring robustness in machine learning is still unsolved after a decade, and we believe raising awareness is a key strategy to improve safety. At this stage, technical solutions seem insufficient to tackle the robustness issue fully. Risk awareness is crucial in preventing these models' irresponsible deployment in critical sectors and in reducing the threats posed by malicious actors. We believe that embedding space attacks can be a valuable tool to investigate model vulnerabilities before these models are deployed.