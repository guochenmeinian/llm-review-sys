# Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees

Yu Gui\({}^{1}\) Ying Jin\({}^{2}\) Zhimei Ren\({}^{3*}\)

\({}^{1}\) Department of Statistics, University of Chicago

\({}^{2}\) Data Science Initiative, Harvard University

\({}^{3}\) Department of Statistics and Data Science, University of Pennsylvania

yugui@uchicago.edu, yjin@hcp.med.harvard.edu,

zren@wharton.upenn.edu

Alphabetical ordering.

###### Abstract

Before deploying outputs from foundation models in high-stakes tasks, it is imperative to ensure that they align with human values. For instance, in radiology report generation, reports generated by a vision-language model must align with human evaluations before their use in medical decision-making. This paper presents Conformal Alignment,2 a general framework for identifying units whose outputs meet a user-specified alignment criterion. It is guaranteed that on average, a prescribed fraction of selected units indeed meet the alignment criterion, regardless of the foundation model or the data distribution. Given any pre-trained model and new units with model-generated outputs, Conformal Alignment leverages a set of reference data with ground-truth alignment status to train an alignment predictor. It then selects new units whose predicted alignment scores surpass a data-dependent threshold, certifying their corresponding outputs as trustworthy. Through applications to question answering and radiology report generation, we demonstrate that our method is able to accurately identify units with trustworthy outputs via lightweight training over a moderate amount of reference data. En route, we investigate the informativeness of various features in alignment prediction and combine them with standard models to construct the alignment predictor.

## 1 Introduction

Large-scale, pre-trained foundation models are remarkably powerful in generating relevant and informative outputs of various forms for diverse downstream tasks, marking a new era of artificial intelligence . However, it has been recognized that they can be prone to actual errors , hallucinations , and bias , among others. These issues raise prevalent societal concerns regarding the reliable use of foundation models in high-stakes scenarios . It remains a significant challenge to ensure that outputs from these highly complex models align with human values .

Towards uncertainty quantification of black-box models, conformal prediction [50, CP] offers a versatile, distribution-free solution. While CP has been applied to classification and regression tasks addressed by foundation models , its use to ensure the alignment of general-form outputs remains largely unexplored. Recently,  propose to generate multiple outputs for one input until it is guaranteed that _at least_ one output meets a specific alignment criterion. Such a guarantee, however, may not be immediately practical, as users must still determine (manually) which output(s) is aligned.  modify the machine-generated outputs to ensure factuality withhigh probability, which in turn may sacrifice model power by making responses vague.3 As such, determining guarantees that are practically meaningful for downstream tasks and developing methods to achieve them remain an active research area.

This paper introduces a distinct type of guarantee: we aim to _certify_ whether each model output is aligned or not, such that _those certified_ are ensured to be _mostly correct_. This guarantee is directly relevant to downstream tasks, as it allows immediate and reliable use of the certified outputs without further modifications. On the other hand, we _abstain_ from deploying model outputs for units (data points) that are not selected--intuitively, those are where the model lacks confidence (i.e., "knowing they do not know"), and they can be evaluated by human experts. Such a practice therefore provides an _automated_ pipeline for the safe deployment of model-generated outputs.

Present work.We present Conformal Alignment (CA), a general framework that determines _when_ to trust foundation model outputs with finite-sample, distribution-free guarantees. The framework is based on CP ideas, but unlike standard CP that searches the sampling space to construct a prediction set that _contains_ a desired output, we leverage the quantified confidence as an instrument to _select_ units whose already-generated outputs are trusted. Given any model and any alignment criterion, Conformal Alignment ensures that a prescribed proportion of the selected units' model outputs indeed meet the alignment criterion.4 Concretely, suppose \(m_{+}\) new units awaiting outputs. Given an error level \((0,1)\), Conformal Alignment controls the false discovery rate (FDR) in selecting units with trustworthy outputs:

\[=^{m}\{i\}}{_{i=1}^{m}\{i\}},\] (1.1)

where the expectation is taken over the randomness of the data. FDR control offers an interpretable measure of the quality of selected deployable units. Our method builds upon the _Conformalized Selection_ framework , leveraging a holdout set of "high-quality" reference data to guide the selection with calibrated statistical confidence for trusted outputs. Like in CP, (1.1) holds in finite-sample as long as the holdout data are exchangeable with the new units.

Our method inherits CP's (1) **rigor**, providing distribution-free, finite-sample error control, and (2) **versatility**, applicable to any model and any alignment criterion. In addition, by selecting rather than modifying outputs, our approach preserves the (3) **informativeness** of the original outputs, and remains (4) **lightweight**, e.g., avoiding the need to retrain large models.

Demonstration of workflow.We illustrate the pipeline of Conformal Alignment via a use case in radiology report generation (detailed in Section 5) in Figure 1: each prompt corresponds to an X-ray scan, for which a vision-language model generates a report. Our framework identifies a subset of generated reports that are reliable in the sense that at least, say \(99\%\), of the selected reports are _guaranteed_ to be aligned if they were to be compared with a human expert report.

The workflow begins with (test) prompts \(_{}\), corresponding to X-ray scans \(X\). A foundation model \(f\) generates a report \(f(X)\) for each scan. However, only high-quality reports should be handed over to doctors for clinical decisions. To achieve this, we leverage a set of X-ray scans that are independent of the model's training process, each with human expert reports available. These scans are randomly split into a training set \(_{}\) and a calibration set \(_{}\). Conformal Alignment trains a predictor on \(_{}\) for predicting the alignment score that assess how likely a generated output aligns. Finally, new reports are selected if their predicted alignment scores exceed a _data-driven_ threshold, which is delicately set with \(_{}\) such that the FDR is strictly controlled at the desired level.

Practical relevance.We apply Conformal Alignment to two use cases--question answering and radiology report generation--to demonstrate its efficiency. With quite lightweight training of the alignment predictor (for example, training a tree-based model would suffice), we are able to accurately identify trustworthy outputs and select as many of them as possible. En route, we exhaustively investigate practical aspects such as the efficacy of various predictors in these applications and the amount of "high-quality" data needed for accurate selection to guide practical deployment.

## 2 Problem setup

Suppose we have a pre-trained foundation model \(f:\) that maps a prompt to an output. Throughout, we view the model as given and fixed. Assume access to a set of holdout units \(=(X_{i},E_{i})_{i=1}^{n}\), where \(X_{i}\) is the input prompt and \(E_{i}\) is any reference that may be used in judging alignment. An _alignment function_\(:\) takes as input the generated output \(f(X)\) and the reference \(E\), and outputs an (true) alignment score \(A=(f(X),E)\). In the radiology report generation example, \(A\) might be the similarity score between the machine-generated report \(f(X)\) and a human expert report \(E\). We shall detail the choice of \(\) in each use case. Note that for a unit without reference information, the alignment score of its generated output is unknown, and we do not seek to evaluate it, which often requires expert annotation or human judgment.

For a test set \(_{}=\{X_{n+j}\}_{j=1}^{m}\), we aim to select a subset \([m]:=\{1,,m\}\) such that most of their (unobserved) alignment scores exceed a pre-fixed threshold \(c\). The error is measured by

\[=\{A_{n+j} c,j \}}{(||,1)},\]

where \(||\) denotes the cardinality of a set. In particular, we aim to enforce that \(\) for a pre-specified level \((0,1)\). The FDR measures the averaged proportion of _selected units_ that are not aligned, thereby directly quantifying the "cost" of deploying outputs in \(\). Such a measure (under different terminologies) appears to be considered empirically in selective prediction, although a rigorous control has been lacking therein (see e.g., [52; 49; 37]). Besides FDR control, it is also desirable that as many units can be safely deployed, which translates into maximizing the power:

\[=\{A_{n+j}>c,j \}}{(_{j[m]}\{A_{n+j}>c\},1)}.\] (2.1)

It should be emphasized that our framework prioritizes FDR control over power, in that we strictly enforce the former while optimizing the latter under the constraints. This is related to but slightly differs from the _selection with controlled risk_ setting in selective prediction [11; 12]; see Section B.1 for more details. Such a setup is motivated by scenarios such as medical decision-making and knowledge access, where the cost of committing a type-I error (i.e., selecting a non-aligned unit) can be grave, and should be of primary concern. In addition, we define power as the proportion among truly aligned units, instead of the number of selections--indeed, if a foundation model is not powerful in generating aligned outputs, to begin with, it is natural that only a few of the outputs shall be deployed.

### Related works

Uncertainty quantification and conformal prediction for foundation models.Conformal prediction [50; 26; 41] is a distribution-free framework for uncertainty quantification of generic prediction algorithms. This work adds to the growing literature that applies/extends CP to foundation models,

Figure 1: Pipeline of Conformal Alignment instantiated in the radiology report generation example.

which varies in definitions of uncertainty and alignment, types of guarantees, rules for decision-making, etc. Below, we summarize representative ideas and contrast them with the present work.

A line of work applies CP to construct prediction sets that cover the outcome for a _single_ test unit in classification or regression problems addressed by language models . While desired, their reliability _in downstream tasks_ is only empirically evaluated: we note that coverage of prediction sets _does not_ carry over to the selected units . In contrast, this work applies to outputs of general forms, considers multiple units simultaneously, and offers guarantees relevant to downstream tasks. In particular, our methodology is built upon the series of papers in _conformalized selection_. While they aim to find large values of outcomes, we motivate and tailor the framework for the alignment of foundation models (a more detailed discussion is in Appendix B.2).

Two works  addressing the alignment of foundation models with CP ideas have been briefly discussed in Section 1. Our setup draws ideas from , but the guarantees we provide differ from both. In particular, our method may be better suited to situations where it is desirable to avoid modifying the outputs or enlarging candidate sets which requires retraining the models.

More generally,  investigate the metric of uncertainty for natural language outputs. This is orthogonal to the present work, as our method is compatible with any alignment criterion. Nevertheless, our work adds to the discussion on desirable uncertainty quantification guarantees.

Selective prediction.Our framework is closely related to the idea of selective prediction, where one is allowed either to _predict_ or to _abstain_ from making a prediction . In the context of large language models,  have similarly considered the goal of teaching the model not to predict when the model is uncertain about its output, but theoretical guarantees on the alignment of the selected outputs are lacking. In contrast, this work rigorously formalizes the guarantee and provides an end-to-end framework that achieves it under a mild exchangeability condition. As mentioned earlier, the strict control of FDR is closely connected to the "selection with controlled risk" setting in selective prediction , which aims to control a slightly different error measure; we provide a detailed comparison of the error measures and methods in Appendix B.1.

Alignment of foundation models.There is a rapidly growing literature on the alignment of foundation models, wherein topics include alignment evaluation , prompt design , training with human feedback , among others . We rely on commonly-used alignment criteria in our applications . However, we achieve strict error control via post-hoc adjustment/selection, instead of deriving training strategies to improve certain alignment metrics of the model.

## 3 Conformal Alignment

Given a set of data \(\) with reference information, our procedure starts by splitting \(\) into two disjoint sets: the training set \(_{}\) and the calibration set \(_{}\). With a slight abuse of notation, we shall also use \(_{}\) and \(_{}\) to denote the indices of the units in the sets when the context is clear. We then fit a model \(g:\) on \(_{}\) to predict the alignment score based on \(X\) (which may also involve information from \(f\)). Next, we generate model outputs \(f(X_{i})\) and compute the predicted alignment scores \(_{i}=g(X_{i})\) for every \(i[n+m]\). We shall use \((A_{i},_{i})_{i_{}}\) to determine the selection set.

Following the framework of _conformalized selection_, we gather statistical evidence for trustworthy outputs via hypothesis testing, where the null hypothesis for \(j[m]\) is

\[H_{j}:\ A_{n+j} c.\] (3.1)

Rejecting \(H_{j}\) then reflects evidence that the (true) alignment score of unit \(j\) is above the threshold \(c\), and therefore the generated output \(f(X_{n+j})\) is aligned. Under this framework, the task of selecting aligned units boils down to simultaneously testing the \(m\) hypotheses specified in (3.1). For this purpose, we construct the _conformal p-value_: for any \(j[m]\),

\[p_{j}=_{}}\{A_{i} c, _{i}_{n+j}\}}{|_{}|+1}.\] (3.2)

It can be shown that when the test unit is exchangeable with the calibration units, the p-value defined above is valid in the sense that \((p_{j} t,A_{n+j} c) t\) for any \(t(0,1)\). Intuitively, when a generated output is likely to be aligned, we expect \(_{n+j}\) to have a large magnitude, and \(p_{j}\) to be small. As a result, we reject the null hypothesis--that is, declaring a sufficiently large alignment score--for a small \(p_{j}\), where the threshold for p-values is determined by the Benjamini-Hochberg (BH) procedure . Concretely, let \(p_{(1)} p_{(m)}\) denote the ordered statistics of the p-values; the rejection set of BH applied to the conformal p-values is \(=\{j[m]:p_{j} k^{*}/m\}\), where

\[k^{*}=k[m]:p_{(k)}},\]

with the convention that \(=0\). We describe the complete procedure in Algorithm 1, and establish its validity in Theorem 3.1. For notational simplicity, we denote \(Z_{i}=(X_{i},E_{i})\) for all \(i[n+m]\) (note that \(E_{i}\) is not observable for \(i>n\)).

```
0: Pre-trained foundation model \(f\); alignment score function \(\); reference dataset \(=(X_{i},E_{i})_{i=1}^{n}\); test dataset \(_{}=(X_{n+j})_{j=1}^{m}\); algorithm for fitting alignment predictor \(\); alignment level \(c\); target FDR level \(\).
1: Compute the alignment score \(A_{i}=(f(X_{i}),E_{i})\), \( i\).
2: Randomly split \(\) into two disjoint sets: the training set \(_{}\) and the calibration set \(_{}\).
3: Fit the alignment score predictor with \(_{}\): \(g(_{})\).
4: Compute the predicted alignment score: \(_{i} g(X_{i})\), \( i_{}_{}\).
5:for\(j[m]\)do
6: Compute the conformal p-values \(p_{j}\) according to Equation (3.2).
7:endfor
8: Apply BH to the conformal p-values: \((p_{1},p_{m})\). ```

**Algorithm 1** Conformal Alignment

**Theorem 3.1**.: Suppose that for any \(j[m]\), \(\{Z_{n+j}\}\{Z_{i}\}_{i_{}}\) are exchangeable conditional on \(\{Z_{n+}\}_{ j}\), i.e., for any permutation \(\) of \(\{1,,n,n+j\}\) and any \(\{z_{1},,z_{n},z_{n+j}\}\), it holds that

\[Z_{1}=z_{1},,Z_{n}=z_{n},Z_{n+j}=z_{n+j} \,|\,\{Z_{n+}\}_{ j}\] \[=Z_{1}=z_{(1)},,Z_{n}=z_{(n)},Z_{n+j }=z_{(n+j)}\,|\,\{Z_{n+}\}_{ j}.\]

Suppose the predicted alignment score \(\{_{i}\}_{i_{}_{}}\) have no ties almost surely and \(_{x}|g(x)|\) for some \(>0\). Then for any \((0,1)\), the output \(\) from Algorithm 1 satisfies FDR\(\).

The proof of Theorem 3.1 is adapted from , and we include it in Appendix A for completeness.

**Remark 3.2**.: The exchangeability condition required in Theorem 3.1 is satisfied when the samples in \(_{}_{}\) are i.i.d., or when \(_{}\) is drawn from a set of samples without replacement, or when they are nodes on graphs in the transductive setting . The assumption of no ties is without loss of generality, since one can always add a small random noise to break the ties.

The following proposition characterizes the power (2.1), as well as the fraction of selected units that can be deployed with confidence, when the samples are i.i.d. The proof is in Appendix A.2. A finite-sample power analysis based on concentration inequalities is in Appendix A.3.

**Proposition 3.3**.: Under the same condition of Theorem 3.1, assume further that \((X_{i},E_{i})_{i[n+m]}\) are i.i.d. Define \(H(t)=(A c,g(X) t)\) and \(t()=\{t:(H(g(X)) t)}\}\). Suppose that there exists a sufficiently small \(>0\) such that \((H(g(X)) t)}<\) for \(t[t()-,t()]\), then

\[_{|_{}|,m}=(H(g(X)) t()\,|\,A>c).\] \[_{|_{}|,m}_{j=1}^ {m}\{j,A_{n+j}>c\}=(H(g(X)) t(),A>c).\]

Figure 2 visualizes the asymptotic cutoff on \(g(X)\) beyond which will be selected by our method: it is the smallest value such that the red area on the right is less than \(\)-proportion of red and blue areas combined. The area of the blue part on the right of the cutoff is thus the asymptotic fraction of selected units, whose proportion among the entire blue area is the asymptotic power. Intuitively,

given a model (i.e., fixing areas for the red and blue), the power of Conformal Alignment depends on how well \(g\) discerns those \(A>c\) against those \(A c\). Therefore, it is important to identify features that informs the alignment of the outputs; we will elaborate on this point and deliver practical recommendations in our experiments in Sections 4 and 5. In addition, the fraction of deployable units (blue) additionally depends on the model power, i.e., the fraction of units whose outputs are indeed aligned. These two factors both affect the fraction of units selected by our method (the blue area on the right of the cutoff), which will also be demonstrated in the experiments.

## 4 Experiments for question answering (QA)

In this section, we implement and evaluate Conformal Alignment in question-answering tasks, where we consider a conversational question answering dataset **TriviaQA** and a closed-book reading comprehension dataset **CoQA**. For each QA dataset, we use language models **OPT-13B** and **LLaMA-2-13B-chat** without finetuning to generate an answer \(f(X_{i})\) via top-p sampling for each input \(X_{i}\) following the default configuration. The alignment score \(A_{i}\) measures how well the generated answer matches the true reference answers provided in both datasets. Following [24; 29], we let \(A_{i}\{0,1\}\) indicate whether the \(}\) score  between the LLM-generated answer and reference answer is no less than \(0.3\), and set \(c=0\). See Appendix C.1 for more details.

Dataset partition.Recall \(\) is the reference dataset and \(_{}\) is the test set we select from. We fix \(|_{}|=500\) and split \(\) as follows. Fixing \(_{1},_{2}(0,1)\), \(_{1}+_{2}<1\), we randomly sample \((_{1}+_{2})||\) instances without replacement from \(\) as \(_{}\) for training the alignment predictor \(g\). Within \(_{}\), a random subset of size \(_{1}||\) is reserved for hyperparameter tuning in computing certain features to be introduced later, while the others are used in fitting \(g\) given the features. We then set the calibration set \(_{}=_{}\). For the results presented in this section, \(_{1}=0.2\), \(_{2}=0.5\). Additional ablation studies for the choice of \((_{1},_{2})\) are summarized in Section 4.1.

Alignment score predictor.With the training set \(_{}\), according to Algorithm 1, our goal is to train an alignment score predictor \(g\) that maps \(X_{i}\) to \(_{i}\)--the estimated probability for \(A_{i}>c\)--based on available information such as the input, model parameters, and outputs. To this end, we train a classifier with binary label \(A_{i}\) and the following features (as preparation, we additionally randomly generate \(19\) answers for each input, leading to \(M=20\) answers in total for each input):

* _Self-evaluation likelihood_ (Self_Eval). Following [22; 29], we ask the language model itself to evaluate the correctness of its answer and use the likelihood, P(true), as a measure of confidence.
* _Input uncertainty scores_ (Lexical_Sim, Num_Sets, SE). Following , we compute a set of features that measure the uncertainty of each LLM input through similarity among the \(M=20\) answers. The features include lexical similarity (Lexical_Sim), i.e., the \(}\) similarity among the answers. In addition, we use a natural language inference (NLI) classifier to categorize the \(M\) answers into semantic groups, and compute the number of semantic sets (Num_Sets) and semantic entropy (SE). Following [24; 29], we use an off-the-shelf DeBERTa-large model  as the NLI predictor.
* _Output confidence scores_ (EigV(J/E/C), Deg(J/E/C), \(}\)(J/E/C)). We also follow  to compute features that measure the so-called output confidence: with \(M\) generations, we compute the eigenvalues of the graph Laplacian (EigV), the pairwise distance of generations based on the degree matrix (\(\)), and the Eccentricity (\(}\)) which incorporates the embedding information of each generation. Note that each quantity is associated with a similarity measure; we follow the notations in  and use the suffix J/E/C to differentiate similarities based on the Jaccard metric, NLI prediction for the entailment class, and NLI prediction for the contradiction class, respectively.

We use all above features to train the alignment predictor with standard ML models including logistic regression, random forests, and XGBoost. Existing works often use individual features in calibrating model confidence [22; 29; 24]; in this regard, we evaluate these features by using each individual feature to train the predictor \(g\) and reporting the resulting power, which delivers additional insights upon their informativeness in predicting model alignment.

Figure 2: Visualization of asymptotic selection rule (red dashed line), with density curves of \(g(X)\) for \(A c\) (red) and \(A>c\) (blue).

[MISSING_PAGE_EMPTY:7]

More powerful model enables more powerful selection.The performance of Conformal Alignment also varies with the language models. We observe that more powerful LLMs can generate answers with higher accuracy which further leads to stronger signals in selection. For OPT-13B, the largest value of FDR is \(0.5\); we have nearly no discoveries and the power is close to zero when \(=0.05\). In comparison, with LLAMA-2-13B-chat, FDR never exceeds \(0.25\), and the power is as high as \(0.50\) even at an FDR level of \(=0.05\). As such, one may also use the FDR/power curve from Conformal Alignment as a measure to compare LLMs' performance.

What feature is informative for alignment?In addition to the capability of LLMs, the features used to train \(g\) also play a vital role in the power of selection. To compare the informativeness of the aforementioned features for the alignment of outputs, we use one score at a time as the feature in training \(g\) and show Power/FDR curves in Figure 5. When using the OPT-13B model, Self_Eval and Num_Setss are the least powerful, while Deg(J) and Ecc(J) are the most powerful in predicting alignment. Such a comparison is similar for LLAMA-2-13B-chat. We defer the FDR curves to Figure 14 in Appendix D.3 which remain strictly controlled. We include a more detailed analysis on feature importance via Shapley value in Appendix D.6.

How many high-quality samples are needed?The size of high-quality samples \(||\) needed is important for applying Conformal Alignment. In practice, it is desired if a handful of data suffices to accurately identify aligned outputs. To study the effect of reference sample size \(||\) on the performance, in Figure 3, we vary \(||\) in \(\{100,500,2000\}\) with fixed \(_{1}=0.2\) and \(_{2}=0.5\). In general, _a few hundred_ high-quality samples suffice to achieve relatively high power, showing that Conformal Alignment does not require too expensive extra labeling. More specifically, we observe that when \(||\) increases, there is a slight improvement in power, especially when \(\) is small; for large values of \(\), the power is robust to \(||\). In addition, a larger labeled sample size reduces the variance in both FDR and Power and improves the stability in selection.

Ablation study.In Appendix D.4, we present ablation studies for the choice of \(_{1}\) and \(_{2}\) in data splitting. In practice, larger values of \(_{1}\) and \(_{2}\) allow more samples for training the alignment predictor but decrease the resolution of the conformal p-values. With fixed \(||\) and \(_{2}\), we observe that the performance of our method is not sensitive to the choice of \(_{1}\); in particular, a small value of \(_{1}\) (e.g., 0.2) is sufficient for powerful selection. When fixing \(||\) and \(_{1}=0.2\), we suggest setting \(_{2}[0.3,0.5]\) to balance the sample sizes for both the predictor training and the BH procedure.

## 5 Experiments for chest X-ray report generation

In this section, we apply Conformal Alignment to chest X-ray report (CXR) generation. Following the pipeline in Figure 1, we apply our method to (a subset of) the MIMIC-CXR dataset . In practice, imagine a healthcare institute employs a foundation model to automatically generate radiology reports based on chest radiographs. Conformal Alignment can be used to determine which reports to trust and refer to doctors for medical decisions, and the rigorous guarantees it offers imply that \(1-\) fraction of the approved reports indeed match the results if they were to be read by a human expert.

Figure 5: Power versus target FDR levels for TriviaQA dataset when the alignment predictor is trained with logistic regression over individual features with \(||=2000\), \(_{1}=0.2\), \(_{2}=0.5\), averaged over \(500\) independent experiments. Note that the FDR is always controlled though not depicted.

We obtain the base foundation model \(f\) by fine-tuning an encoder-decoder model (pre-trained Vision-Transformer and GPT2) using a separate training set. For a generated report \(f(X_{i})\) with a reference report written by a radiologist, the alignment score \(A_{i}\) is defined as follows: we use CheXbert  to convert both reports to two 14-dimensional vectors of binary labels with each coordinate as the indicator of positive observations; we then define \(A_{i}\{0,1\}\) as the indicator of whether there are at least \(12\) matched labels, and set \(c=0\). We employ the same set of features and the same procedure in Section 4, except for Self_Eval which is not available from the current model. More details about the experimental setup are in Appendix C.2.

### Experiment results

We summarize the main findings from our experiments below; additional results with random forest and XGBoost predictors, as well as ablation studies for the choice of hyperparameters, are deferred to Appendix E. To illustrate the role of the alignment score predictor, we also present examples of CXR images with reference and generated reports in Table 2 of Appendix E.4.

Tight FDR control.Figure 6 presents the FDR and power curves for Conformal Alignment where the alignment predictor is trained with all features using logistic regression, averaged over \(500\) independent runs of the procedure. Again, we observe that the FDR is tightly controlled at the target level; it remains constant for sufficiently large \(\) since by then all units can be selected without violating the FDR. We also observe satisfactory power, although it is lower than that is the QA tasks since CXR generation can be a more challenging task, and the model in use is fine-tuned with a limited number of training samples. We shall expect even higher power for better tuned models.

Hundreds of reference data suffice.Comparing subplots in Figure 6, as the size of the reference data \(\) increases, we observe a slight improvement in power. However, we again find that \(||=500\) already suffices for powerful deployment of Conformal Alignment. In practice, we expect this sample size also suffices for more powerful foundation models that are more carefully designed for this specific task.

Informativeness of individual features.To study the informativeness of different features for alignment, we use one feature at a time to train the logistic regression classifier and compare the resulting power versus target FDR levels in Figure 7 from Conformal Alignment (curves showing FDR control are in Figure 23 of Appendix E.2). In the CXR task, features based on Jaccard similarity are still the most powerful while those based on NLI prediction exhibit slightly lower power. More complicated texts and more tokens in CXR reports could be potential reasons for the relatively poor performance of NLI prediction.

Figure 6: Realized FDR (blue) and power (blue) for Conformal Alignment applied to CXR report generation with alignment predictor from logistic regression. The results are averaged over \(500\) runs with \(_{1}=0.2,_{2}=0.5\). Each subplot corresponds to one value of \(||\).

Figure 7: Power at each FDR level for CXR dataset when the alignment predictor is trained with logistic regression over each individual feature, fixing \(||=2000\), \(_{1}=0.2\), \(_{2}=0.5\).

Discussion and limitations

We have presented Conformal Alignment, a principled framework for ensuring the alignment of foundation model outputs with provable, distribution-free FDR control. Compared with standard conformal prediction, we provide a solution that may be more relevant to downstream tasks than existing ideas. We demonstrate that our framework is flexible, lightweight, and effective through concrete applications to question answering and radiology report generation tasks, where we also provide practical recommendations on sample size, data splitting, and feature engineering.

The choice of alignment score in different applications remains an open problem. For example, in the radiology report generation example, as pointed out by , there can be a discrepancy between the score given by CheXbert and that given by human evaluation; as such, it will be interesting to use our framework with actual human evaluation labels. More generally, interesting future directions include designing implementation tailored for other appropriate applications, tackling parallel or sequential outputs, improving power in data-scarce scenarios (e.g., with better base models and more informative features), extending to multi-task and in-context learning settings, and generalizing our framework to control other error notions beyond FDR that may be meaningful for downstream decisions.