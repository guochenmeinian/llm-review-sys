# Don't blame Dataset Shift! Shortcut Learning

due to Gradients and Cross Entropy

Aahlad Puli\({}^{1}\)  Lily Zhang  Yoav Wald  Rajesh Ranganath\({}^{1,2,3}\)

\({}^{1}\)Department of Computer Science, New York University

\({}^{2}\)Center for Data Science, New York University

\({}^{3}\)Department of Population Health, Langone Health, New York University

###### Abstract

Common explanations for shortcut learning assume that the shortcut improves prediction under the training distribution but not in the test distribution. Thus, models trained via the typical gradient-based optimization of cross-entropy, which we call default-erm, utilize the shortcut. However, even when the stable feature determines the label in the training distribution and the shortcut does not provide any additional information, like in perception tasks, default-erm still exhibits shortcut learning. Why are such solutions preferred when the loss for default-erm can be driven to zero using the stable feature alone? By studying a linear perception task, we show that default-erm's preference for maximizing the margin leads to models that depend more on the shortcut than the stable feature, even without overparameterization. This insight suggests that default-erm's implicit inductive bias towards max-margin is unsuitable for perception tasks. Instead, we develop an inductive bias toward uniform margins and show that this bias guarantees dependence only on the perfect stable feature in the linear perception task. We develop loss functions that encourage uniform-margin solutions, called margin control (marg-ctrl). Marg-ctrl mitigates shortcut learning on a variety of vision and language tasks, showing that better inductive biases can remove the need for expensive two-stage shortcut-mitigating methods in perception tasks.

## 1 Introduction

Shortcut learning is a phenomenon where a model learns to base its predictions on an unstable correlation, or _shortcut_, that does not hold across data distributions collected at different times and/or places (Geirhos et al., 2020). A model that learns shortcuts can perform worse than random guessing in settings where the label's relationship with the shortcut feature changes (Koh et al., 2021; Puli et al., 2022). Such drops in performance do not occur if the model depends on features whose relationship with the label does not change across settings; these are _stable_ features.

Shortcut learning is well studied in cases where models that use both shortcut and stable features achieve lower loss than models that only use the stable feature (Arjovsky et al., 2019; Puli et al., 2022; Geirhos et al., 2020). These works consider cases where the Bayes-optimal classifier -- the training conditional distribution of the label given the covariates -- depends on both stable and shortcut features. In such cases, shortcut learning occurs as the Bayes-optimal predictor is the target of standard supervised learning algorithms such as the one that minimizes the log-loss via gradient descent (gd), which we call default-erm.

However, in many machine learning tasks, the stable feature perfectly predicts the label, i.e. a _perfect stable feature_. For example, in task of predicting hair color from images of celebrity faces in theCelebA dataset (Sagawa et al., 2020), the color of the hair in the image determines the label. This task is a perception task. In such classification tasks, the label is independent of the shortcut feature given the stable feature, and the Bayes-optimal predictor under the training distribution only depends on the stable feature. Default-erm can learn this Bayes-optimal classifier which, by depending solely on the stable feature, also generalizes outside the training distribution. But in practice, default-erm run on finite data yields models that depend on the shortcut and thus perform worse than chance outside the training distribution (Sagawa et al., 2020, Liu et al., 2021, Zhang et al., 2022). The question is, why does default-erm prefer models that exploit the shortcut even when a model can achieve zero loss using the stable feature alone?

To understand preferences toward shortcuts, we study default-erm on a linear perception task with a stable feature that determines the label and a shortcut feature that does not. The perfect linear stable feature means that data is linearly separable. This separability means that default-erm-trained linear models classify in the same way as the minimum \(_{2}\)-norm solution that has all margins greater than \(1\); the latter is commonly called max-margin classification (Soudry et al., 2018). We prove that default-erm's implicit inductive bias toward the max-margin solution is harmful in that default-erm-trained linear models depend more on the shortcut than the stable feature. In fact, such dependence on the shortcut occurs even in the setting with fewer parameters in the linear model than data points, i.e. without overparameterization. These observations suggest that a max-margin inductive bias is unsuitable for perception tasks.

Next, we study inductive biases more suitable for perception tasks with perfect stable features. We first observe that predicting with the perfect stable feature alone achieves uniform margins on all samples. Formally, if the stable feature \(s()\) determines the label \(\) via a function \(d\), \(=d s()\), one can achieve any positive \(b\) as the margin on all samples simultaneously by predicting with \(b d s()\). We show that in the same setting without overparameterization where max-margin classification leads to shortcut learning, models that classify with uniform margins depend only on the stable feature.

Building on these observations, we identify alternative loss functions that are inductively biased toward uniform margins, which we call margin control (marg-ctrl). We empirically demonstrate that marg-ctrl mitigates shortcut learning on multiple vision and language tasks without the use of annotations of the shortcut feature in training. Further, Marg-ctrl performs on par or better than the more expensive two-stage shortcut-mitigating methods (Liu et al., 2021, Zhang et al., 2022). We then introduce a more challenging setting where both training and validation shortcut annotations are unavailable, called the nuisance-free setting. In the nuisance-free setting, marg-ctrl_always outperforms_ default-erm and the two-stage shortcut-mitigating methods. These empirical results suggest that simply incorporating inductive biases more suitable for perception tasks is sufficient to mitigate shortcuts.

## 2 Shortcut learning in perception tasks due to maximizing margins

Setup.We use \(,,\) to denote the label, the shortcut feature, and the covariates respectively. We let the training and test distributions \((p_{tr},p_{te})\) be members of a family of distributions indexed by \(\), \(=\{p_{}(,,)\}_{}\), such that the shortcut-label relationship \(p_{}(,)\) changes over the family. Many common tasks in the spurious correlations literature have stable features \(s()\) that are perfect, meaning that the label is a deterministic function \(d\) of the stable feature: \(=d s()\). For example, in the Waterbirds task the bird's body determines the label and in the CelebA task, hair color determines the label (Sagawa et al., 2020). As \(s()\) determines the label, it holds that \(\_\|_{p_{}}(,) s()\). Then, the optimal predictor on the training distribution is optimal on all distributions in the family \(\), regardless of the shortcut because

\[p_{tr}()=p_{tr}( s())=p_{te}(  s())=p_{te}().\]

The most common procedure to train predictive models to approximate \(p_{tr}()\) is gradient-based optimization of cross-entropy (also called log-loss); we call this default-erm. Default-erm targets the Bayes-optimal predictor of the training distribution which, in tasks with perfect stable features, also performs optimally under the test distribution. However, despite targeting the predictor that does not depend on the shortcut, models built with default-erm still rely on shortcut features that are often less predictive of the label and are unstable, i.e. vary across distributions (Geirhos et al., 2020, Puli et al., 2022). We study default-erm's preference for shortcuts in a data generating process (dgp) where both the shortcut and the perfect stable feature are linear functions of the covariates.

### Shortcut learning in linear perception tasks

Let Rad be the uniform distribution over \(\{1,-1\}\), \(\) be the normal distribution, \(d\) be the dimension of \(\), and \((0,1),B>1\) be scalar constants. The dgp for \(p_{}(,,)\) is:

\[,\ \ p_{}(=y =y)=\\ p_{}(=-y=y)=(1-),\ \ \ (0,^{d-2}),\ \ \ =[B*,,]\,.\] (1)

This dgp is set up to mirror the empirical evidence in the literature showing that shortcut features are typically learned first (Sagawa et al., 2020). The first dimension of \(\), i.e. \(_{1}\), is a shortcut that is correlated with \(\) according to \(\). The factor \(B\) in \(_{1}\) scales up the gradients for parameters that interact with \(_{1}\) in predictions. For large enough \(B\), model dependence on the shortcut feature during default-erm goes up faster than the stable feature (Idrissi et al., 2022).

The training distribution is \(p_{tr}=p_{0.9}\) and the test distribution is one where the shortcut's relationship with the label is flipped \(p_{te}=p_{0.1}\). Models achieve worse than random test accuracy (\(50\%\)) if they exploit the training shortcut relationship and the predicted class flips when the shortcut feature flips. We train with default-erm which uses log-loss: on a data point \((,)\) the log-loss is

\[_{log}(f_{}())=[1+(-f_{ }())].\]

With \(d=300\) and \(B=10\), we train a linear model on \(1000\) samples from the training distribution \(p_{=0.9}\), and evaluate on \(1000\) samples from \(p_{=0.1}\).

Observations.Figure1(a) shows that when trained with default-erm, the linear model does not do better than chance (\(<50\%\)) on the test data even after \(50,000\) epochs. So, even in the presence of the perfect feature \(_{2}\), the model relies on other features like the shortcut \(_{1}\). Since the final training loss is very small, on the order of \(10^{-9}\), this result is not due to optimization being stuck in a local minima with high loss. **These observations indicate that, in the linear setting, gradient-based optimization with log-loss prefers models that depend more on the shortcut than the perfect stable feature.**

To better understand this preference we focus on the errors in specific groups in the data. Consider the classifier that only uses the shortcut \(\) and makes the Bayes-optimal prediction w.r.t \(p_{tr}\): \(_{y}p_{tr}(=y)\). We call instances that are classified correctly by this model the _shortcut_ group, and the rest the _leftover_ group. We use these terms for instances in the training set as well as the test set. In this experiment \(\) is positively correlated with \(\), hence the shortcut group consists of all instances with \(^{i}=^{i}\) and the leftover group of those with \(^{i}^{i}\).

Figure1(b) gives accuracy and loss curves on the shortcut and leftover groups for the first \(10000\) epochs. The test accuracy for the shortcut group hits \(90\%\) while the leftover group test accuracy is \(<40\%\), meaning that the model exploits the shortcuts. Even though a model that relies solely on the shortcut misclassifies the leftover group, we see that the training loss of the learned model on this group approaches \(0\). The model drives down training loss in the leftover group by depending on noise, which results in larger test loss in the leftover group than the shortcut group. **Thus, fig.1(b) demonstrates that the default-erm-trained model classifies the training shortcut group by using the shortcut feature while overfitting to the training leftover group.**

Figure 1: Accuracy and loss curves for training a linear model with default-erm on \(1000\) training samples from \(p_{0.9}\), with \(B=10,d=300\) (see eq.1), and testing on \(p_{0.1}\). **(a)** The model achieves \(100\%\) train accuracy but \(<40\%\) test accuracy. **(b)** The learned model achieves high test accuracy (\( 90\%\)) on the shortcut group and low test accuracy on the leftover group (\( 30\%\)). Models that depend more on the stable feature than on the shortcut, achieve at least \(50\%\) accuracy on both the shortcut and leftover groups. Hence the learned model exploits the shortcut to classify the shortcut group and overfits to the leftover group.

Shortcut dependence like in fig. 1 occurs even with \(_{2}\)-regularization and when training neural networks; see appendix B.1 and appendix B.4 respectively. Next, we analyze the failure mode in fig. 1, showing that the shortcut dependence is due to default-erm's implicit bias to learn the max-margin classifier. Next, we study the failure mode in fig. 1 theoretically, showing that the shortcut dependence is due to default-erm's inductive bias toward learning the max-margin classifier.

Max-margin classifiers depend more on the the shortcut than the stable feature.We consider training a linear model \(f_{}()=^{}\) where \(=[_{z},_{y},_{e}]\) with default-erm. Data from eq. (1) is always linearly separable due to the perfect stable feature, but many hyperplanes that separate the two classes exist. When a linear model is trained with default-erm on linearly separable data, it achieves zero training loss and converges to the direction of a minimum \(_{2}\)-norm solution that achieves a margin of at least \(1\) on all samples (Soudry et al., 2018; Wang et al., 2021, 2022); this is called the max-margin solution. We now show that for a small enough leftover group, large enough scaling factor \(B\) and dimension \(d\) of the covariates, max-margin solutions depend more on the shortcut feature than the stable feature:

**Theorem 1**.: _Let \(^{*}\) be the max-margin predictor on \(n\) training samples from eq. (1) with a leftover group of size \(k\). There exist constants \(C_{1},C_{2},N_{0}>0\) such that_

\[\,n>N_{0},\ \ k(0, ),\ d C_{1}k(3n),\ B>C_{2}},\] (2)

_with probability at least \(1-}{{3n}}\) over draws of the training data, it holds that \(B_{z}^{*}>_{y}^{*}\)._

The size of the leftover group \(k\) concentrates around \((1-)n\) because each sample falls in the leftover group with probability \((1-)\). Thus, for \(>0.9\), that is for a strong enough shortcut, the condition in theorem 1 that \(k<}{{10}}\) will hold with probability close to \(1\); see appendix A.5 for more details.

The proof is in appendix A. The first bit of intuition is that using the shortcut can have lower norm because of the scaling factor \(B\). Using the shortcut only, however, misclassifies the leftover group. The next bit of intuition is that using noise from the leftover group increases margins in one group at a rate that scales with the dimension \(d\), while the cost in the margin for the other group only grows as \(\). This trade-off in margins means the leftover group can be correctly classified using noise without incorrectly classifying the shortcut group. The theorem then leverages convex duality to show that this type of classifier that uses the shortcut and noise has smaller \(_{2}\)-norm than any linear classifier that uses the stable feature more.

The way the margin trade-off in the proof works is by constructing a linear classifier whose weights on the noise features are a scaled sum of the product of the label and the noise vector in the leftover group: for a scalar \(\), the weights \(_{e}=_{i S_{}}_{i}_{i}\). The margin change on the \(j\)th training sample from using these weights is \(_{j}_{e}^{}_{j}\). For samples in the shortcut group, the margin change looks like a sum of mean zero independent and identically distributed variables; the standard deviation of this sum grows as \(\). For samples in the leftover group, the margin change is the sum of mean one random variables; this sum grows as \(d\) and its standard deviation grows as \(\). The difference in mean relative to the standard deviation is what provides the trade-off in margins.

We now discuss three implications of the theorem.

**First, theorem 1 implies that the leftover group sees worse than random accuracy (\(0.5\)).** To see this, note that for samples in the leftover group the margin \((^{*})^{}=_{y}^{*}-B_{z} ^{*}+(_{e}^{*})^{}\) is a Gaussian random variable centered at a negative number \(_{y}^{*}-B_{z}^{*}\). Then, with \(_{e}\) as the CDF of the zero-mean Gaussian random variable \((_{e}^{*})^{}\), accuracy in the test leftover group is

\[p((^{*})^{} 0 )=p[(_{e}^{*})^{}>-(_{y}^ {*}-B_{z}^{*})]=1-_{e}(-(_{y}^{*}-B_{z}^{*} )) 0.5.\]

Second, the leftover group in the training data is overfit in that the contribution of noise in prediction (\(|(_{e}^{*})^{}|\)) is greater than the contribution from the stable and shortcut features. Formally, in the training leftover group, \(_{y}^{*}-B_{z}^{*}<0\). Then, due to max-margin property,

\[_{y}^{*}-B_{z}^{*}+(_{e}^{*})^{}_{ i}_{i}>1(_{e}^{*})^{}_{i} _{i} 1-(_{y}^{*}-B_{z}^{*})>|_{y}^{*}-B _{z}^{*}|.\]

Third, many works point to overparameterization as one of the causes behind shortcut learning (Sagawa et al., 2020; Nagarajan et al., 2021; Wald et al., 2023), but in the setup in fig. 1, the linear model has fewer parameters than samples in the training data. In such cases with non-overparameterized linear models, the choice of default-erm is typically not questioned, especially when a feature exists that linearly separates the data. **Corollary 1 formally shows shortcut learning for non-overparameterized linear models. In words, default-erm -- that is vanilla logistic regression trained with gradient-based optimization -- can yield models that rely more on the shortcut feature _even without overparameterization_.

**Corollary 1**.: _For all \(n>N_{0}\) -- where the constant \(N_{0}\) is from theorem1 -- with scalar \((0,1)\) such that the dimension of \(\) is \(d= n<n\), for all integers \(k<n\{, 3n}\},\) a linear model trained via default-erm yields a predictor \(^{*}\) such that \(B_{z}^{*}>_{y}^{*}\)._

If default-erm produces models that suffer from shortcut learning even without overparameterization, its implicit inductive bias toward max-margin classification is inappropriate for perception tasks in the presence of shortcuts. Next, we study inductive biases more suited to perception tasks.

## 3 Toward inductive biases for perception tasks with shortcuts

The previous section formalized how default-erm solutions, due to the max-margin inductive bias, rely on the shortcut and noise to minimize loss on training data even in the presence of a different zero-population-risk solution. Are there inductive biases more suitable for perception tasks?

Given a perfect stable feature \(s()\) for a perception task, in that for a function \(d\) when \(=d s()\), one can achieve margin \(b(0,)\) uniformly on all samples by predicting with the stable \(b d s()\). In contrast, max-margin classifiers allow for disparate margins as long as the smallest margin crosses \(1\), meaning that it does not impose uniform margins. The cost of allowing disparate margins is the preference for shortcuts even without overparameterization (corollary 1). In the same setting however, any uniform-margin classifier for the linear perception task (eq.1) relies only on the stable feature:

**Theorem 2**.: _Consider \(n\) samples of training data from dgp in eq.1 with \(d<n\). Consider a linear classifier \(f_{}()=^{}\) such that for all samples in the training data \(_{i}^{}_{i}=b\) for any \(b(0,)\). With probability 1 over draws of samples, \(=[0,b,0^{d-2}]\)._

Theorem2 shows that uniform-margin classifiers only depend on the stable feature, standing in contrast with max-margin classifiers which can depend on the shortcut feature (theorem1). The proof is in appendixA.6. **Thus, inductive biases toward uniform margins are better suited for perception tasks.** Next, we identify several ways to encourage uniform margins.

Margin control (marg-ctrl).To produce uniform margins with gradient-based optimization, we want the loss to be minimized at uniform-margin solutions and be gradient-optimizable. We identify a variety of losses that satisfy these properties, and we call them marg-ctrl losses. marg-ctrl. losses have the property that per-sample loss monotonically decreases for margins until a threshold then increases for margins beyond it. In turn, minimizing loss then encourages all margins to move to the threshold.

Mechanically, when models depend more on shortcuts than the stable feature during training, margins on samples in the shortcut group will be larger than those in the leftover group; see the right panel in fig.1b where the train loss in the shortcut group is lower than the leftover group indicating that the margins are smaller in the leftover group. This difference is margins is a consequence of the shortcut matching the label in one group and not the other, thus, encouraging the model to have similar margins across all samples pushes the model to depend less on the shortcut. In contrast, vanilla log-loss can be driven to zero in a direction with disparate margins across the groups as long as the margins on all samples go to \(\). We define marg-ctrl losses for a model \(f_{}\) with the margin on a sample \((,)\) defined as \(f_{}()\).

As the first marg-ctrl loss, we develop the \(\)-damped log-loss: we evaluate log-loss on a margin multiplied by a monotonically decreasing function of the margin. In turn, the input to the loss increases with the margin till a point and then decreases. For a temperature \(T\) and sigmoid function \(\), the \(\)-damped loss modifies the model output \(f_{}\) and plugs it into log-loss:

\[_{}(,f_{})=_{}( (1-(f_{}}{T}))f_{ })\]

For large margin predictions \(f_{}>0\), the term \(1-(f_{}()/T)\) damps down the input to log-loss. The largest the input to \(_{}\) can get is \(0.278T\), found by setting the derivative to zero, thus lower bounding the loss. As log-loss is a decreasing function of its input, the minimum of \(_{}\) occurs when the margin is \(0.278T\) on all samples. To demonstrate empirical advantage, we compare standard log-loss to \(\)-damped loss on eq.1; see fig.2. The left panel of figure fig.2 shows that test accuracy is better for \(\)-damp. The middle and right panels shows the effect of controlling margins in training, where losses on shortcut and leftover groups hover at the same value.

Second, we design the \(\)-stitch loss, which imitates log-loss when \(f_{}()<u\) and penalizes larger margins (\(f_{}>u\)) by negating the sign of \(f_{}()\):

\[_{}=_{log}(\,[f_{} () u]f_{}()\ +\ [f_{}()>u](2u- f_{}())\,)\] (3)

As the third marg-ctrl loss, we directly penalize large margins via a log-penalty:

\[_{}=_{log}(f_{}())+ (1+|f_{}()|^{2})\] (4)

The fourth marg-ctrl loss controls margins by penalizing \(|f_{}()|^{2}\):

\[_{}=_{log}(f_{}())+|f_{ }()|^{2}\] (5)

This last penalty was called spectral decoupling (sd) by Pezeshki et al. (2021), who use it as a way to decouple learning dynamics in the neural tangent kernel (ntk) regime. Instead, from the lens of marg-ctrl, sd mitigates shortcuts in eq.1 because it encourages uniform margins, even though sd was originally derived from different principles, as we discuss in section4. In appendixB.2, we plot all marg-ctrl losses and show that marg-ctrl improves over default-erm on the linear perception task; see figs.6 to 8. We also run marg-ctrl on a neural network and show that while default-erm achieves test accuracy worse than random chance, marg-ctrl achieves \(100\%\) test accuracy; see figs.10 to 13 in appendixB.4.

## 4 Related work

A large body of work tackles shortcut learning under different assumptions (Arjovsky et al., 2019; Wald et al., 2021; Krueger et al., 2020; Creager et al., 2021; Veitch et al., 2021; Poli et al., 2022; Heinze-Deml and Meinshausen, 2021; Belinkov and Bisk, 2017). A different line of work focuses on learning in neural networks in idealized settings (Yang and Salman, 2019; Ronen et al., 2019; Jo and Bengio, 2017; Baker et al., 2018; Saxe et al., 2013; Gidel et al., 2019; Advani et al., 2020).

Shah et al. (2020) study simplicity bias (Valle-Perez et al., 2018) and show that neural networks provably learn the linear function over a non-linear one, in the first epoch of training. In a similar vein, Hermann and Lampinen (2020) show that neural networks can prefer a linearly-decodable feature over a non-linear but more predictive feature, and Scimeca et al. (2021) make similar observations and use loss landscapes to empirically study which features are easier to learn. Simplicity bias alone only describes neural biases early in training and does not explain why more predictive stable features are not learned later. Unlike simplicity bias which focuses on linear versus non-linear features, max-margin bias is the reason default-erm prefers one linear feature, the shortcut, over another, the stable feature, like in the synthetic experiment in section2.

While Pezeshki et al. (2021) allow for perfect features, they hypothesize that shortcut learning occurs because when one feature is learned first, other features are gradient-starved and are not learned as

Figure 2: Using \(\)-damped log-loss yields linear models that depend on the perfect stable feature to achieve near perfect test accuracy. The middle panel shows that \(\)-damping maintains similar margins in the training shortcut and leftover groups unlike unconstrained log-loss, and the right panel shows \(\)-damp achieves better leftover test-loss.

well. They focus on a special setting where feature representations for different samples have inner product equal to a small constant to show that models can depend more on the imperfect feature than the perfect feature. In this special setting, they show that penalizing the magnitudes of what we call the margin mitigates shortcuts; this method is called spectral decoupling (SD). However, as we show in appendix B.5, the assumption in Lemma 1(Pezeski et al., 2021) is violated when using a linear model to classify in the simple linear dgp in eq.1. However, SD on a linear model mitigates shortcuts in the dgp in eq.1; see B.5. Thus, the theory in Pezeski et al. (2021) fails to not explain why sd works for eq.1, but the uniform-margin property explains why all the marg-ctrl losses, including sd, mitigate shortcuts.

Nagarajan et al. (2021) consider tasks with perfect stable features and formalize geometric properties of the data that make max-margin classifiers have non-zero weight on the shortcut feature (\(_{z}>0\)). In their set up, the linear models are overparameterized and it is unclear when \(_{z}>0\) leads to worse-than-random accuracy in the leftover group because they do not separate the model's dependence on the stable feature from the dependence on noise. See fig.14 for an example where \(_{z}>0\) but test accuracy is \(100\%\). In contrast to Nagarajan et al. (2021), theorem1 gives dgp where the leftover group accuracy is worse than random, even without overparameterization. Ahuja et al. (2021) also consider linear classification with default-erm with a perfect stable feature and conclude that default-erm learns only the stable feature because they assume no additional dimensions of noise in the covariates. We develop the necessary nuance by including noise in the problem and showing default-erm depends on the shortcut feature even without overparameterization.

Sagawa et al. (2020) and Wald et al. (2023) both consider overparameterized settings where the shortcut feature is informative of the label even after conditioning on the stable feature. In both cases, the Bayes-optimal predictor also depends on the shortcut feature, which means their settings do not allow for an explanation of shortcut dependence in examples like fig.1. In contrast, we show shortcut dependence occurs even in the presence of a perfect stable feature and without overparameterization. Li et al. (2019), Pezeski et al. (2022) focus on relative feature complexity and discuss the effects of large learning rate (lr) on which features are learned first during training, but do not allow for perfect features. Idrissi et al. (2022) empirically find that tuning lr and weight decay (wd) gets default-erm to perform similar to two-stage shortcut-mitigating methods like Just Train Twice (Jtt)(Liu et al., 2021). We view the findings of (Idrissi et al., 2022) through the lens of marg-ctrl and explain how large lr and wd approximate marg-ctrl to mitigate shortcuts; see section5.

Marg-ctrl is related to but different from methods proposed in Liu et al. (2017), Cao et al. (2019), Kini et al. (2021). These works normalize representations or the last linear layers and linearly transform the logits to learn models with better margins under label imbalance. Next, methods like Learning from Failure (lff)(Nam et al., 2020), jtt(Liu et al., 2021), and Correct-n-Contrast (cnc)(Zhang et al., 2022) build two-stage procedures to avoid shortcut learning without group annotations in training. They assume that default-erm produces models that depend more on the shortcut and select hyperparameters of the two stage process using validation group annotations. In the nuisance-free setting where there are no validation group annotations, the performance of these methods can degrade below that of default-erm. In contrast, better characterizing the source of shortcut learning in perceptual problems leads to marg-ctrl methods that are not as reliant on validation group annotations (see nuisance-free results in Section5). **Without any group annotations, encouraging uniform margins via marg-ctrl mitigates shortcuts better than jtt and cnc.**

Soudry et al. (2018) characterize the inductive bias of gradient descent to converge in direction to max-margin solutions when using exponentially tailed loses; Wang et al. (2021, 2022) then prove similar biases toward max-margin solutions for Adam and RMSProp. Ji et al. (2020) show that for general losses that decrease in \(f_{}()\), gradient descent has an inductive bias to follow the \(_{2}\)-regularization path. All these inductive biases prefer shortcuts if using them leads to lower loss within an \(_{2}\)-norm-budget. Marg-ctrl provides a different inductive bias toward producing the same margin on all samples, which means gradient descent veers models away from imperfect shortcuts that lead to disparity in network outputs. Such inductive biases are suitable for tasks where a feature determines the label (\(h()=\)).

## 5 Vision and language experiments

We evaluate marg-ctrl on common datasets with shortcuts: Waterbirds, CelebA (Sagawa et al., 2020), and Civilcomments (Koh et al., 2021). First, marg-ctrl always improves over default-erm. Then, we show that marg-ctrl performs similar to or better than two-stage shortcutmitigating methods like jtt[Liu et al., 2021] and cnc[Zhang et al., 2022] in traditional evaluation settings where group annotations are available in the validation data. Finally, we introduce a more challenging setting that only provides class labels in training and validation, called the **nuisance-free setting**. In contrast to the traditional setting that always assumes validation group annotations, the nuisance-free setting does not provide group annotations in either training or in validation. In the nuisance-free setting, marg-ctrl outperforms jtt and cnc, even though the latter are supposed to mitigate shortcuts without knowledge of the groups.

Datasets.We use the Waterbirds and CelebA datasets from Sagawa et al.  and the CivilComments dataset from Borkan et al. , Koh et al. . In Waterbirds, the task is to classify images of a waterbird or landbird, and the label is spuriously correlated with the image background consisting of land or water. There are two types of birds and two types of background, leading to a total of 4 groups defined by values of \(y,z\). In CelebA [Liu et al., 2015, Sagawa et al., 2020], the task is to classify hair color of celebrities as blond or not. The gender of the celebrity is a shortcut for hair color. There are two types of hair color and two genders in this dataset, leading to a total of 4 groups defined by values of \(y,z\). In CivilComments-WILDS [Borkan et al., 2019, Koh et al., 2021], the task is to classify whether an online comment is toxic or non-toxic, and the label is spuriously correlated with mentions of certain demographic identities. There are \(2\) labels and \(8\) types of the shortcut features, leading to \(16\) groups.

Metrics, model selection, and hyperparameters.We report the worst-group test accuracy for each method. The groups are defined based on the labels and shortcut features. The more a model depends on the shortcut, the worse the worst-group error. Due to the label imbalance in all the datasets, we use variants of \(\)-damp, \(\)-stitch, marg-log, and sd with class-dependent hyperparameters; see appendix B.6.2. For all methods, we use the standard Adam optimizer [Kingma and Ba, 2015] and let the learning rate and weight decay hyperparameters be tuned along with the method's hyperparameters. We first report results for all methods using validation worst-group accuracy to select method and optimization hyperparameters and early stop. For both jtt and cnc, this is the evaluation setting that is used in existing work [Liu et al., 2021, Idrissi et al., 2022, Zhang et al., 2022]. Finally, in the nuisance-free setting where no group annotations are available, we select hyperparameters using label-balanced average accuracy. Appendix B.6 gives further details about the training, hyperparameters, and experimental results.

### Marg-ctrl mitigates shortcuts in the default setting

Here, we experiment in the standard setting from Liu et al. , Idrissi et al. , Zhang et al.  and use validation group annotations to tune hyperparameters and early-stopping.

Marg-ctrl improves over default-erm.We compare marg-ctrl to default-erm on CelebA, Waterbirds, and Civilcomments. Table 1 shows that every marg-ctrl method achieves higher test worst-group accuracy than default-erm on all datasets. Default-erm achieves a mean test worst-group accuracy of \(70.8\%,72.8\%\) and \(60.1\%\) on Waterbirds, CelebA, and Civilcomments respectively. Compared to default-erm, marg-ctrl methods provide a \(5-10\%\) improvement on Waterbirds, \(7-10\%\) improvement on CelebA, \(7-10\%\) improvement on Civilcomments. These improvements show the value of inductive biases more suitable for perception tasks.

Large lr and wd may imitate marg-ctrl in rem.Default-erm's performance varies greatly across different values of lr and wd on, for instance, CelebA: the test worst-group accuracy improves by more than \(20\) points over different lr and wd combinations. Why does tuning lr and wd yield such improvements? We explain this phenomenon as a consequence of instability in optimiza

    & CelebA & WB & Civil \\  emm & \(72.8 9.4\) & \(70.8 2.4\) & \(60.1 0.4\) \\ cnc & \(81.1 0.6\) & \(68.0 1.8\) & \(68.8 0.2\) \\ jtt & \(75.2 4.6\) & \(71.7 4.0\) & \(69.9 0.4\) \\  marg-log & \(82.8 1.1\) & \(78.2 1.9\) & \(68.4 1.8\) \\ \(\)-damp & \(79.4 0.6\) & \(78.6 1.1\) & \(69.6 0.4\) \\ sd & \(81.4 2.5\) & \(80.5 1.4\) & \(69.9 1.1\) \\ \(\)-stitch & \(81.1 2.2\) & \(75.9 3.4\) & \(67.8 2.8\) \\   

Table 1: Mean and standard deviation of test worst-group accuracies over two seeds for default-erm, jtt, cnc, \(\)-damp, \(\)-stitch, sd, and marg-log. Every marg-ctrl method outperforms default-erm on every dataset. On Waterbirds, marg-ctrl, outperforms jtt and cnc. On CelebA, sd, marg-log, and \(\)-stitch beat jtt and achieve similar or better performance than cnc. On CivilComments, \(\)-damp and sd beat cnc and achieve similar performance to jtt.

tion induced by large lr and wd which prevents the model from maximizing margins and in turn can control margins. Figure 3 provides evidence for this explanation by comparing default-erm's loss curves for two lr and wd combinations.

The blue curves in fig. 3 correspond to the run with the larger lr and wd combination. The model that achieves the best validation (and test) worst-group accuracy over all combinations of hyperparameters for default-erm, including those not in the plot, is the one at epoch \(13\) on the blue curves. This model achieves similar train and test losses (\( 0.4\)) and thus similar margins in the shortcut group, the leftover group, and the whole dataset. The red curves stand in contrast where the lower lr results in the leftover group having higher training and test losses, and therefore smaller margins, compared to the shortcut group. These observations together support the explanation that default-erm with large lr and wd mitigates shortcuts when controlling margins like marg-ctrl.

Marg-ctrl performs as well or better than two-stage shortcut-mitigating methods.Two-stage shortcut mitigating methods like Correct-n-Contrast (cnc) and Just Train Twice (jtt) aim to mitigate shortcuts by using a model trained with default-erm to approximate group annotations. They rely on the assumption that a model trained via default-erm either predicts with the shortcut feature (like background in Waterbirds) or that the model's representations separate into clusters based on the shortcut feature. The methods then approximate group annotations using this default-erm-trained model and use them to mitigate shortcut learning in a second predictive model. Jtt upweights the loss on the approximate leftover group and cnc uses a contrastive loss to enforce the model's representations to be similar across samples that have the same label but different approximate group annotations. B.6.1 gives details.

Table 1 compares marg-ctrl to jtt and cnc on Waterbirds, Celeba, and CivilComments. On CelebA, sd, marg-log, and \(\)-stitch perform similar to cnc while all marg-ctrl techniques outperform jtt. On Waterbirds, all marg-ctrl methods outperform jtt and cnc. On CivilComments, \(\)-damp and sd perform similar to jtt and outperform cnc. Cnc's performance on Waterbirds differs from Zhang et al. (2022) because their reported performance requires unique large wd choices (like wd set to \(1\)) to build a first-stage model that relies most on the shortcut feature without overfitting to the training data.

Marg-ctrl is faster than jtt and cnc.Marg-ctrl takes the same time as default-erm, taking around \(1,20\) and \(60\) minutes per epoch for Waterbirds, CelebA, and CivilComments respectively on an RTX8000 GPU. In contrast, on average over runs, jtt takes around \(6,80,120\) minutes per epoch and cnc takes around \(8,180,360\) minutes per epoch. Thus, marg-ctrl performs as well or better than jtt and cnc while being simpler to implement and computationally cheaper.

### Marg-ctrl mitigates shortcuts in the nuisance-free setting

Work like (Liu et al., 2021; Zhang et al., 2022) crucially require validation group annotations because these methods push the work of selecting models for mitigating shortcuts to validation. Determining shortcuts itself is a laborious manual process, which means group annotations will often be unavailable. Further, given a perfect stable feature that determines the label and a shortcut that does not, only models that rely on the stable feature more than the shortcut achieve the highest validation ac

Figure 3: Loss curves of default-erm on CelebA for two combinations of lr and wd. The combination with the larger learning rate (blue) achieves \(72.8\%\) test worst-group accuracy, beating the other combination by \(20\%\). The model that achieves the best validation (and test) worst-group accuracy is the one at epoch \(13\) from the blue run. This model achieves similar loss in both groups and the full data model suggesting that large lr and wd controls margins from exploding (higher training loss in all panels) and avoids systematically smaller margins in the leftover group compared to the shortcut group.

curacy. Thus, we introduce a more challenging setting that only provides class labels in training and validation, called the **nuisance-free setting**. In the nuisance-free setting, models are selected based on label-balanced average accuracy: the average of the accuracies over samples of each class.

Table 2 reports test worst-group (WG) accuracy in the nuisance-free setting. **On all the datasets, every marg-ctrl outperforms default-erm, jtt, and cnc.** On average, the marg-ctrl methods close at least \(61\%\) of the gap between default-erm in the nuisance-free setting and the best performance in table 1 on every dataset. **In contrast, cnc and jtt sometimes perform worse than default-erm.**

## 6 Discussion

We study why default-erm -- gradient-based optimization of log-loss -- yields models that depend on the shortcut even when the population minimum of log-loss is achieved by models that depend only on the stable feature. By studying a linear task with perfect stable features, we show that default-erm's preference toward shortcuts sprouts from an inductive bias toward maximizing margins. Instead, inductive biases toward uniform margins improve dependence on the stable feature and can be implemented via marg-ctrl. Marg-ctrl improves over default-erm on a variety of perception tasks in vision and language without group annotations in training, and is competitive with more expensive two-stage shortcut-mitigating methods. In the nuisance-free setting, where even validation group annotations are unavailable, marg-ctrl outperforms all the baselines. The performance that Marg-ctrl yields demonstrates that changing inductive biases can remove the need for expensive shortcut-mitigating methods in perception tasks.

Without overparameterization, uniform-margin classifiers are unique and learn stable features only, while max-margin classifiers can depend more on shortcuts. With overparameterization, max-margin classifiers are still unique but uniform-margin solutions are not which necessitates choosing between solutions. The experiments in section 5 suggest that choosing between uniform-margin classifiers with penalties like \(_{2}\) improves over max-margin classifiers with \(_{2}\): all experiments use overparameterized models trained with weight decay and marg-ctrl outperforms default-erm. Further, our experiments suggest that uniform-margin classifiers are insensitive to the wd and lr choices, unlike max-margin classifiers; appendix B.8 shows that marg-ctrl achieves high performance for all lr and wd choices but term requires tuning.

Theorem 1 also explains how balancing may or may not improve dependence on the stable features. For example, a weighting-based approach produces the same max-margin solution as default-erm (Sagawa et al., 2020, Rosset et al., 2003), but subsampling leads to a different solution that could depend less on the shortcut. For the latter however, models are more prone to overfitting on the smaller subsampled dataset. Similar observations were made in Sagawa et al. (2020) but this work extends the insight to tasks with perfect stable features. Comparing em and marg-ctrl on subsampled data would be fruitful.

Any exponentially tailed loss when minimized via gradient descent converges to the max-margin solution in direction (Soudry et al., 2018). Thus, theorem 1 characterizes shortcut learning for any exponentially-tailed loss. However, losses with decreasing polynomial tails -- for example, \((a)=}\) for some \(K>0\) -- do not converge to the max-margin classifier. One future direction is to show shortcut-dependence results like theorem 1 for polynomial-tailed losses, which in turn would mean that all common classification losses with a decreasing tail impose inductive biases unsuitable for perception tasks.

In the tasks we consider with perfect stable features, Bayes-optimal predictors rely only on the stable feature. A weaker independence condition implies the same property of Bayes-optimal predictors even when \(\) is not determined by \(s()\): \(\_\|(,) s()\). For example, in the CivilComments dataset a few instances have ambiguous labels (Xenos et al., 2022) meaning that there may not be a perfect stable feature. Studying uniform margins and other inductive biases under this independence would be fruitful.

    & CelebA & WB & Civil \\  em & \(57.5 5.8\) & \(69.1 2.1\) & \(60.7 1.5\) \\ cnc & \(67.8 0.6\) & \(60.0 8.0\) & \(61.4 1.9\) \\ jtt & \(53.3 3.3\) & \(71.7 4.0\) & \(53.4 2.1\) \\  marg-log & \(74.2 1.4\) & \(77.9 0.3\) & \(66.8 0.2\) \\ \(\)-damp & \(70.8 0.3\) & \(74.8 1.6\) & \(65.6 0.2\) \\ sd & \(70.3 0.3\) & \(78.7 1.4\) & \(67.8 1.3\) \\ \(\)-stitch & \(76.7 0.6\) & \(74.5 1.2\) & \(66.0 1.0\) \\   

Table 2: Average and standard deviation of test worst-group accuracy over two seeds of marg-ctrl, default-erm, jtt, and cnc in the nuisance-free setting. Hyper-parameter selection and early stopping use label-balanced average accuracy. All marg-ctrl methods outperform default-erm, jtt, and cnc on all datasets.

[MISSING_PAGE_FAIL:11]

* Nagarajan et al. (2021) Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. Understanding the failure modes of out-of-distribution generalization. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=fSTD8NFIW_b.
* Wald et al. (2023) Yoav Wald, Gal Yona, Uri Shalit, and Yair Carmon. Malign overfitting: Interpolation and invariance are fundamentally at odds. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=dQNL7Zsta3.
* Pezeshki et al. (2021) Mohammad Pezeshki, Sekou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. In _Thirty-Fifth Conference on Neural Information Processing Systems_, 2021.
* Wald et al. (2021) Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain generalization. _arXiv preprint arXiv:2102.10395_, 2021.
* Krueger et al. (2020) David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). _arXiv preprint arXiv:2003.00688_, 2020.
* Creager et al. (2021) Elliot Creager, Jorn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In _International Conference on Machine Learning_, pages 2189-2200. PMLR, 2021.
* Veitch et al. (2021) Victor Veitch, Alexander D'Amour, Steve Yadlowsky, and Jacob Eisenstein. Counterfactual invariance to spurious correlations: Why and how to pass stress tests. _arXiv preprint arXiv:2106.00545_, 2021.
* Heinze-Deml and Meinshausen (2021) Christina Heinze-Deml and Nicolai Meinshausen. Conditional variance penalties and domain shift robustness. _Machine Learning_, 110(2):303-348, 2021.
* Belinkov and Bisk (2017) Yonatan Belinkov and Yonatan Bisk. Synthetic and natural noise both break neural machine translation. _arXiv preprint arXiv:1711.02173_, 2017.
* Yang and Salman (2019) Greg Yang and Hadi Salman. A fine-grained spectral perspective on neural networks. _arXiv preprint arXiv:1907.10599_, 2019.
* Ronen et al. (2019) Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural networks for learned functions of different frequencies. _Advances in Neural Information Processing Systems_, 32, 2019.
* Jo and Bengio (2017) Jason Jo and Yoshua Bengio. Measuring the tendency of cnns to learn surface statistical regularities. _arXiv preprint arXiv:1711.11561_, 2017.
* Baker et al. (2018) Nicholas Baker, Hongjing Lu, Gennady Erlikhman, and Philip J Kellman. Deep convolutional networks do not classify based on global object shape. _PLoS computational biology_, 14(12):e1006613, 2018.
* Saxe et al. (2013) Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. _arXiv preprint arXiv:1312.6120_, 2013.
* Gidel et al. (2019) Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* Advani et al. (2020) Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of generalization error in neural networks. _Neural Networks_, 132:428-446, 2020.
* Shah et al. (2020) Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. _arXiv preprint arXiv:2006.07710_, 2020.
* Valle-Perez et al. (2018) Guillermo Valle-Perez, Chico Q Camargo, and Ard A Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. _arXiv preprint arXiv:1805.08522_, 2018.
* Hermann and Lampinen (2020) Katherine L Hermann and Andrew K Lampinen. What shapes feature representations? exploring datasets, architectures, and training. _arXiv preprint arXiv:2006.12433_, 2020.
* Hermann et al. (2020)Luca Scimeca, Seong Joon Oh, Sanghyuk Chun, Michael Poli, and Sangdoo Yun. Which shortcut cues will dnns choose? a study from the parameter-space perspective. _arXiv preprint arXiv:2110.03095_, 2021.
* Ahuja et al. (2021) Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution generalization. _Advances in Neural Information Processing Systems_, 34:3438-3450, 2021.
* Sagawa et al. (2020b) Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameterization exacerbates spurious correlations. In _International Conference on Machine Learning_, pages 8346-8356. PMLR, 2020b.
* Li et al. (2019) Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large learning rate in training neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* Pezeshki et al. (2022) Mohammad Pezeshki, Amartya Mitra, Yoshua Bengio, and Guillaume Lajoie. Multi-scale feature learning dynamics: Insights for double descent. In _International Conference on Machine Learning_, pages 17669-17690. PMLR, 2022.
* Liu et al. (2017) Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 212-220, 2017.
* Cao et al. (2019) Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. _Advances in neural information processing systems_, 32, 2019.
* Kini et al. (2021) Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thrampoulidis. Label-imbalanced and group-sensitive classification under overparameterization. _Advances in Neural Information Processing Systems_, 34:18970-18983, 2021.
* Nam et al. (2020) Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: De-biasing classifier from biased classifier. _Advances in Neural Information Processing Systems_, 33:20673-20684, 2020.
* Ji et al. (2020) Ziwei Ji, Miroslav Dudik, Robert E Schapire, and Matus Telgarsky. Gradient descent follows the regularization path for general losses. In _Conference on Learning Theory_, pages 2109-2136. PMLR, 2020.
* Borkan et al. (2019) Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In _Companion proceedings of the 2019 world wide web conference_, pages 491-500, 2019.
* Liu et al. (2015) Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _Proceedings of the IEEE international conference on computer vision_, pages 3730-3738, 2015.
* Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015. URL http://arxiv.org/abs/1412.6980.
* Rosset et al. (2003) Saharon Rosset, Ji Zhu, and Trevor Hastie. Margin maximizing loss functions. _Advances in neural information processing systems_, 16, 2003.
* Xenos et al. (2022) Alexandros Xenos, John Pavlopoulos, Ion Androutsopoulos, Lucas Dixon, Jeffrey Sorensen, and Leo Laugier. Toxicity detection sensitive to conversational context. _First Monday_, 2022.
* Vershynin (2018) Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Feng and Zhang (2007) Xinlong Feng and Zhinan Zhang. The rank of a random matrix. _Applied mathematics and computation_, 185(1):689-694, 2007.
* Zhang et al. (2019)Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.
* Sohoni et al. (2020) Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher Re. No subclass left behind: Fine-grained robustness in coarse-grained classification problems. _Advances in Neural Information Processing Systems_, 33:19339-19352, 2020.
* Gulrajani & Lopez-Paz (2021) Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=10dXeXDoWtI.

Appendix: Proof of Theorem 1, Corollary 1, and Theorem 2

### Helper Lemmas

#### a.1.1 Bounding norms and inner products of isotropic random vectors.

The main lemmas of this section are lemma3 and lemma4. We will then use these two to bound norms of sums of random vectors and inner products between the sum and a single random vector in lemma5. We first list some facts from  that we will use to bound the probability with which norms and inner products of Gaussian random vectors deviate far from their mean.

**Definition 1**.: _(Sub-Gaussian norm) For an r.v. \(\), the sub-Gaussian norm, or \(_{2}\)-norm, is_

\[\|\|_{_{2}}=\{t>0,[(}}{{ 2}}/t^{2})] 2\}.\]

_An r.v. is called sub-Gaussian if its \(_{2}\)-norm is finite and for some fixed constant \(c\)_

\[p(||>t) 2(-}}{{\|\|_{_{2}}} }).\]

A Gaussian r.v. \((0,^{2})\) has an \(_{2}\)-norm of \(G\) for a constant \(G=}\).2

\[_{(0,^{2})}[( }}{{2}}/t^{2})] =_{-}^{}}(-}}{{2^{2}}})(}}{{2}}/t^{2})dx=_{ -}^{}}(-x^{2}-2 ^{2})}{2^{2}t^{2}})dx\] \[=}-2 ^{2})}{2^{2}t^{2}}}}=}t^{2}}{(t^{2}-2^{2})}}=}{(t^{2}-2^{2})}}\] \[}{(t^{2}-2^{2})}} 2 t^{2} 4(t^{2}-2 ^{2}) 8^{2} 3t^{2}\{t:8^{2} 3t^{2} \}=}.\]

#### a.1.2 Bounding norms and inner products of isotropic random vectors.

The main lemmas of this section are lemma3 and lemma4. We will then use these two to bound norms of sums of random vectors and inner products between the sum and a single random vector in lemma5. We first list some facts from  that we will use to bound the probability with which norms and inner products of Gaussian random vectors deviate far from their mean.

**Definition 1**.: _(Sub-Gaussian norm) For an r.v. \(\), the sub-Gaussian norm, or \(_{2}\)-norm, is_

\[\|\|_{_{2}}=\{t>0,[(}}{{ 2}}/t)] 2\}.\]

_A sub-exponential r.v. is one that has finite \(_{1}\)-norm._

**Lemma 1**.: _(Lemma 2.7.7 from ) Products of sub-Gaussian random variables \(,\) is a sub-exponential random variable with it's \(_{1}\)-norm bounded by the product of the \(_{2}\)-norm_

\[\|\|_{_{1}}\|\|_{_{2}}\|\| _{_{2}}\]

Lemma 1 implies that the product of two mean-zero standard normal vectors is a sub-exponential random variable with \(_{1}\)-norm less than \(G^{2}\).

**Lemma 2**.: _(Bernstein inequality, Theorem 2.8.2 ) For i.i.d sub-exponential random variables \(_{1},,_{d}\), for a fixed constant \(c=}\) and \(K=\|_{1}\|_{_{1}}\)_

\[p(|_{i=1}^{d}_{i}|>t) 2(-c \{}{K^{2}d},\})\]

Next, we apply these facts to bound the sizes of inner products between two unit-variance Gaussian vectors.

**Lemma 3**.: _(Bounds on inner products of Gaussian vectors) Let \(,\) be \(d\)-dimensional random vectors where each coordinate is an i.i.d standard normal r.v. Then, for any scalar \(>0\) such that \( G^{2}\), for a fixed constant \(c=}\)_

\[p(|^{}|>) 2 (-c}{G^{4}}).\]Proof.: First, the inner product is \(^{}=_{d}^{t}_{i}_{i}\); it is the sum of products of i.i.d. standard normal r.v. (\(=1\)). Then, by lemma1, each term in the sum is a sub-exponential r.v. with \(_{1}\)-norm bounded as follows:

\[K=\|_{i}_{i}\|_{_{1}}\|_{i}\|_{_{2}} \|_{i}\|_{_{2}}=G G=G^{2}.\] (6)

We can apply Bernstein inequality lemma2 to sub-exponential r.v. to the inner product and then upper bound the probability by replacing \(K\) with the larger \(G^{2}\) in eq.6

\[p(|^{}|>t) 2(-c\{ }{K^{2}d},\}) 2(-c\{ }{G^{4}d},}\})\]

Substituting \(t=\) in the above gives us:

\[p(|^{}|>) 2(-c \{d}{G^{4}d},}{G^{2}}\})\]

Using the fact that \( G^{2}\) to achieve the minimum concludes the proof:

\[ G^{2}^{2} G^{2} }{G^{4}}}{G^{2}} \{d}{G^{4}d},}{G^{2}} \}=}{G^{4}}\]

**Lemma 4**.: _Let \(\) be a Gaussian vector of size \(d\) where each element is a standard normal, meaning that \(\|_{i}\|_{_{2}}=G\). Then, for any \(t>0\) and a fixed constant \(c=}\), the norm of the vector concentrates around \(\) according to_

\[p(||||-|>t) 2(-ct^{2}/G^{4}).\]

Proof.: Equation3.3 from the proof of theorem3.1.1 in  shows that

\[p(||||-|>t) 2(-ct^{2}/(_{i}\| _{i}\|_{_{2}})^{4}).\]

As \(\) has i.i.d standard normal entries, \(_{i}\|_{i}\|_{_{2}}=G\), concluding the proof. 

#### a.1.2 Concentration of norms of sums of random vectors and their inner products

This is the main lemma that we will use in proving theorem1.

**Lemma 5**.: _Consider a set of vectors \(V=\{_{i}\}\) where \(_{i}^{d}\) of size \(T_{V} 1\) where each element of each vector is drawn independently from the standard normal distribution \((0,1)\). Then, for a fixed constant \(c=}\) and any \((0,G^{2})\) with probability \( 1-2(-^{2}})\)_

\[\|}}_{i V}_{i}\| +\] (7)

_and with probability \( 1-4T_{V}(-^{2}})\)_

\[_{j} V_{j}, _{i V}_{i} d-3d}\] (8)

_Further, consider any set \(U\) of vectors \(U=\{_{i}\}\) of size \(T_{U}\), where each vector also has coordinates drawn i.i.d from the standard normal distribution \((0,1)\). Then, with probability \( 1-2T_{u}(-^{2}})\)_

\[_{j} U| _{j},_{i V}_{i}| d},\] (9)

_By union bound, the three events above hold at once with a probability at least \(1-2(2T_{V}+T_{u}+1)(-^{2}})\)._

Proof.: We split the proof into three parts one each for eqs.7 to 9.

Proof of eq. (7).As \(\) is a vector of random i.i.d standard normal random variables, note that \(}}_{i}_{i}\) is also a vector of i.i.d standard normal random variables. This follows from the fact that the sum of \(T_{V}\) standard normal random variables is a mean-zero Gaussian random variable with standard deviation \(}\). Thus dividing by the standard deviation makes the variance \(1\), making it standard normal.

Then, applying lemma 4 with \(t=\) gives us the following bound:

\[p(\|}}_{i}_{i}\|> +) p(\ \|}}_{i}_{i}\|- >) 2({{}^{-c^{2}}\!/G^{4}})\]

Proof of eq. (8)We split the inner product into two cases: \(T_{V}=1\) and \(T_{V} 2\).

Case \(T_{V}=1\).First note that due to lemma 4,

\[ j V, p(\|_{j}\|<- ) p(\ \|_{j}\|->) 2 ({{}^{-c^{2}}\!/G^{4}}).\]

Then, the following lower bound holds with probability at least \(1-2({{}^{-c^{2}}\!/G^{4}})\)

\[ j V,_{j}, _{i V}_{i} =\|_{j}\|^{2}\] \[(-)^{2}\] \[ d-2\] \[ d-3d},\]

To summarize this case, with the fact that \(1-2({{}^{-c^{2}}\!/G^{4}}) 1-4T_{V}({{}^{-c^{2}}\!/G^{4}})\), we have that

\[ j V,_{j}, _{i V}_{i} d-3d},\]

with probability at least \(1-4T_{V}({{}^{-c^{2}}\!/G^{4}})\).

Case \(T_{V} 2\).First note that,

\[ j V_{j}, _{i V}_{i}=\|_{j}\| ^{2}+_{j},_{i V,i j}_{i}\]

For each of the \(T_{V}\) different \(_{j}\)'s, using lemma 4 bounds the probability of the norm \(\|_{j}\|\) being larger than \(-\):

\[p(\|_{j}\|<-) p( \ \|_{j}\|->) 2({{}^{-c ^{2}}\!/G^{4}}).\]

In the case where \(T_{V} 2\), we express the inner product of a vector and a sum of vectors as follows

\[_{j},_{i V,i j}_ {i}=-1}_{j},-1}}_{i V,i j}_{i},\]

and noting that like above, \(-1}}_{i V,i j}_{i}\) is a vector of standard normal random variables, we apply lemma 3 to get

\[ i V p(|_{j}, _{i V,i j}_{i}| -1)d}) 2({{}^{-c^{2}}\!/G^{4}}).\]

Putting these together, by union bound over \(V\)

\[p[ j V(\|_{j}\|<- )\ \ \ \ (|_{j},_{i V,i j} _{i}|-1)d})]\]\[_{j V}p(\|_{j}\|<- )+p(|<_{j},_{i V,i j }_{i}>|-1)d})\] \[_{j V}2(-c}{G^{4}})+2 (-c}{G^{4}})\] \[ 4T_{V}(-c}{G^{4}}).\]

Thus, with probability at least \(1-4T_{V}(-c}{G^{4}}),\) none of the events happen and

\[ j V<_{j},_{i V} _{i}> = \|_{j}\|^{2}+<_{j}, _{i V,i j}_{i}>\] \[(-)^{2}--1)d}\] \[=d-2+^{2}--1)d}\] \[ d-2-1)d}--1)d}\] \[ d-3d}\]

Thus, putting the analysis in the two cases together, as long as \(T_{V} 1\)

\[ j V<_{j},_{i V} _{i}> d-3d},\]

with probability at least \(1-4T_{V}(-c}{G^{4}}).\)

Proof of eq. (9)Next, we apply lemma 3 again to the inner product of two vectors of i.i.d standard normal random variables:

\[ j U p(|<_ {j},}}_{i V}_{i}>| )<2^{(-c^{2}/G^{4})}.\]

By union bound over \(U\)

\[p[ j U(|<_{j}, }}_{i V}_{i}>| )]<2T_{u}^{(-c^{2}/G^{4})}.\]

Thus, with probability at least \(1-2T_{u}(-c}{G^{4}}),\) the following holds, concluding the proof

\[ j U|<_{j}, }}_{i V}_{i}>| .\]

**Lemma 6**.: _Let \(\{_{i},_{i}\}_{i n}\) be a collection of \(d\) dimensional covariates \(_{i}\) and label \(_{i}\) sampled according to \(p_{}\) in eq. (1). The covariates \(_{i}=[ B_{i},_{i}_{i}]\), where \(+B\) in the middle coordinate for \(i S_{}\) and \(-B\) for \(i S_{}\). The dual formulation of the following norm-minimization problem_

\[_{}=*{arg\,min}_{} _{y}^{2}+_{z}^{2}+\|_{c}\|^{2}\] _s.t._ \[i S_{} w_{y}+Bw_{z}+_{c}^{}_{i} _{i}>1\] _s.t._ \[i S_{} w_{y}-Bw_{z}+_{c}^{}_{i} _{i}>1\]\[_{y} B_{z}\]

_is the following with \(^{}=[-B,1,^{d-2}]\),_

\[_{ 0, 0}-\|+X^{}\|^{2}+ ^{},\] (10)

_where \(X\) is a matrix with \(_{i}_{i}\) as its rows._

Proof.: We use Lagrange multipliers \(^{n},\) to absorb the constraints and then use strong duality. Letting \(^{}=[-B,1,^{d-2}]\), \(X\) be a matrix where the \(i\)th row is \(_{i}_{i}\),

\[_{} \|\|^{2} X-  0^{} 0\] \[\] \[_{ 0, 0}_{} \|\|^{2}-(X-)^{}- ^{}\] (11)

Now, we solve the inner minimization to write the dual problem only in terms of \(,\). Solving the inner minimization involves solving a quadratic program, which is done by setting its gradient to zero,

\[_{}(\|\|^{2}-(X-)^{}-^{})=2-X^{} -=0\] \[=(+X^{})\]

Substituting \(=(+X^{})\) in eq.11

\[\|\|^{2}- (X-)^{}-^{}=\] \[\|+X^{}\|^{2}-((X( +X^{})-)^{}-^{}( +X^{})\] \[=\|+X^{}\|^{2}-((X( +X^{})-)^{}-^{2}\|\|^{ 2}-^{}X^{}\] \[=\|+X^{}\|^{2}-(X(X^{ }))^{}-(X())^{}+ ^{}-^{2}\|\|^{2}-^{}X^{ }\] \[=\|+X^{}\|^{2}-((X( X^{}))^{}+^{2}\|\|^{2}+^{}X^{ })+^{}\] \[=\|+X^{}\|^{2}-(\|X ^{}\|^{2}+\|\|^{2}+2^{}X^{})+ ^{}\] \[=\|+X^{}\|^{2}-\| +X^{}\|^{2}+^{}\] \[=-\|+X^{}\|^{2}+^{}\]

### Shortcut learning in max-margin classification

We repeat the dgp from the linear perception task in eq.1 here.

\[,p_{}( =y=y)=\\ p_{}(=-y=y)=(1-),(0,^{d-2}),=[B*, ,]\,.\] (12)

**Theorem 1**.: _Let \(^{*}\) be the max-margin predictor on \(n\) training samples from eq.12 with a leftover group of size \(k\). There exist constants \(C_{1},C_{2},N_{0}>0\) such that_

\[\ k(0,)\] (13)\[\ d C_{1}k(3n),\] (14) \[\ B>C_{2}}{{k}}},\] (15)

_with probability at least \(1-\) over draws of the training data, it holds that \(B_{z}^{*}>_{y}^{*}\)._

Before giving the proof of theorem1, we first give the corollary showing overparameterization is not necessary for theorem1 to hold.

**Corollary 1**.: _For all \(n>N_{0}\) -- where the constant \(N_{0}\) is from theorem1 -- with scalar \((0,1)\) such that the dimension \(d= n<n\), theorem1 holds._

\[ k n\{, 3n}\},\]

_a linear model trained via default-erm yields a predictor \(^{*}\) such that \(B_{z}^{*}>_{y}^{*}\)._

Proof.: We show that for a range of \(k\), for all \(n N_{0}\) theorem1 holds for some \(d<n\). Note that theorem1 holds for \(n N_{0},d=C_{1}k(3n)\) and

\[ k<.\]

Setting \(d n\) for some \((0,1)\) such that \(d<n\) means that theorem1 holds if

\[C_{1}k(3n)=d n k(3n)}.\]

Absorbing this new upper bound into the requirements on \(k\) for theorem1 to hold, we get that for any scalar \(n>N_{0},(0,1),d= n\), theorem1 holds for

\[ k<n\{,(3n)}\}.\]

In turn, even though \(d<n\), a linear model trained via default-erm converges in direction to a max-margin classifier such that \(^{*}\) with \(B_{z}^{*}>_{y}^{*}\). 

Proof.: (of theorem1) We consider two norm-minimization problems over \(\), one under constraint \(_{y} B_{z}\) and another under \(_{y}<B_{z}\). We show that the latter achieves lower norm and therefore, max-margin will achieve solutions \(_{y}<B_{z}\). The two minimization problems are as follows:

\[_{}=&* {arg\,min}_{} w_{y}^{2}+w_{z}^{2}+\|_{}\|^{ 2}&_{}=*{arg\,min}_{ } w_{y}^{2}+w_{z}^{2}+\|_{}\|^{2}\\ &\ i S_{} w_{y}+Bw _{z}+_{}^{}_{i}_{i}>1 &\ i S_{} w_{y}+Bw _{z}+_{}^{}_{i}_{i}>1\\ &\ i S_{} w_{y}-Bw _{z}+_{}^{}_{i}_{i}>1 &\ i S_{} w_{y}-Bw _{z}+_{}^{}_{i}_{i}>1\\ &\_Let \(k=|S_{}|>1\). Then, for a fixed constant \(c=}\), with any scalar \(<\), with probability at least \(1-2(-ce^{^{2}}/G^{4})\) and \(\) integers \(M[1,]\),_

\[\|_{}\|^{2} W_{}=+)^{2}}{2Mk}}.\]

**Lemma**.: **(8)** _Consider the following optimization problem from eq.16 where \(n\) samples of \(_{i},_{i}\) come from eq.1 where \(_{i}^{d}\):_

\[_{}&= _{} w_{y}^{2}+w_{z}^{2}+\|_{e}\|^{2}\\ &\;i S_{} w_{y}+Bw _{z}+_{e}^{}_{i}_{i}>1\\ &\;i S_{} w_{y}-Bw_{z}+_{e}^{} _{i}_{i}>1\\ &\;_{y}<B_{z}\] (19)

_Let \(k=|S_{}| 1\). Then, for a fixed constant \(c=}\), with any scalar \(<}<\), with probability at least \(1-2(2k+(n-k)+1)(-c}{G^{4}})\), for \(=}\),_

\[\|_{}\|^{2} W_{}=^{2}k( +)^{2}+)^{2}}{B^{2}}\]

Together, the lemmas say that for any \(\) integers \(M[1,]\) and \(<}\), with probability \( 1-2(-ce^{^{2}}/G^{4})\)

\[\|_{}\|^{2} W_{}=+)^{2}}{2Mk}}.\]

and with probability at least \(1-2(2k+(n-k)+1)(-c}{G^{4}})\), for \(=}>0\),

\[\|_{}\|^{2} W_{}=^{2}k( +)^{2}+)^{2}}{B^{2}}\]

First, we choose \(^{2}=2}{c}(3n)\). This gives us the probability with which these bounds hold: as \(k<0.1n\) we have \(k+2<\) and

\[1-2(2k+(n-k)+2)(-c}{G^{4}}) =1-2(n+k+2)(-2(3n))\] \[ 1-2()(-2(3n))\] \[=1-(-2(3n)+(3n))\] \[=1-(-(3n))\] \[=1-.\]

Next, we will instantiate the parameter \(M\) and set the constants \(C_{1},C_{2}\) and the upper bound on \(k\) in theorem1 to guarantee the following eq. (separation inequality):

\[W_{}=^{2}k(+)^{2}+)^{2}}{B^{2}}<+)^{2}}{2Mk}}=W_{},\]

which then implies that \(\|_{}\|^{2}<\|_{}\|^{2}\), concluding the proof.

[MISSING_PAGE_FAIL:22]

\[()^{2},\{k 1\}\] (29)

Next is a simpler upper bound on the second part of the LHS of eq. (separation inequality). Again with \(=}\),

\[)^{2}}{B^{2}} =}{d-4} )^{2}}{B^{2}}\] \[)^{2}}{B^{2}}\] \[=)^{2}}{B^{2}}\]

Now setting

\[B>)}{}( )}\]

gives the lower bound on \(B\) from theorem 1:

\[B>C_{2}}, C_{2}=)}{()}=(1+C)}.\]

Formally,

\[B>C_{2}})^{2}}{B^ {2}}<(}())^ {2}=()^{2}.\] (30)

By combining the upper bound from eq. (30) and the upper bound from eq. (29), we get an upper bound on the whole of the LHS of eq. (separation inequality), which in turn provides an upper bound on \(W_{}\):

\[W_{}=^{2}k(+)^{2}+)^{2}}{B^{2}}<()^{2}( )^{2},\]

because \(k 1\). Note the upper bound is strict.

Concluding the proof.Now, we show that a \(C\) exists such that the following holds, which implies \(W_{}<W_{}\), which in turn implies eq. (separation inequality) and the proof concludes:

\[W_{}<()^{ 2}d}<W_{}.\]

The above inequality holds when

\[6()^{2}} (1+C)^{2}-}(1-4C) 0.\]

The right hand side holds when the quadratic equation \((1+C)^{2}-}(1-4C)\) is non-positive, which holds between the roots of the equation. The equation's positive solution is

\[C=}{3+2+)}} 0.008.\]Setting \(C\) to this quantity satisfies the requirement that \(C(0,)\).

Thus, a \(C\) exists such that eq. (separation inequality) holds which concludes the proof of theorem1 for the following constants and constraints implied by \(C\) and \(M=5\):

\[C_{2}=(1+C)} C_{1}=2}{cC^{2}}  k<,\]

where \(G\) is the \(_{2}\)-norm of a standard normal r.v. and \(c\) is the absolute constant from the Bernstein inequality in lemma2. 

### Lower bounding the norm of solutions that rely more on the stable feature

**Lemma 7**.: _Consider the following optimization problem from eq.16 where \(n\) samples of \(_{i},_{i}\) come from eq.1 where \(_{i}^{d}\):_

\[_{}=& {arg\,min}_{} w_{y}^{2}+w_{z}^{2}+\|_{e}\|^{2}\\ & S_{} w _{y}+Bw_{z}+_{e}^{}_{i}_{i}>1\\ & S_{} w _{y}-Bw_{z}+_{e}^{}_{i}_{i}>1\\ &_{y} B_{z}\] (31)

_Let \(k=|S_{}|>1\). Then, for a fixed constant \(c=}\), with any scalar \(<\), with probability at least \(1-2(-ce^{2}/G^{4})\) and \(\) integers \(M[1,]\),_

\[\|_{}\|^{2} W_{}=+)^{2}}{2Mk}}.\]

Proof.: By lemma6, the dual of eq.16 is the following for \(=[-B,1,^{d-2}]\) and \(X\) is an \(n d\) matrix with rows \(_{i}_{i}\):

\[_{ 0, 0}-\|+X^{}\|^{2}+ ^{}\] (32)

Now by duality

\[\|_{}\|^{2}_{ 0, 0}-\| +X^{}\|^{2}+^{},\]

which means any feasible candidate to eq.32 gives a lower bound on \(\|_{}\|^{2}\).

Feasible Candidates for \(,\).We now define a set \(U[n]\), and let \(_{i}=>0\) for \(i U\) and \(0\) otherwise. For \(M(1,]\), we take \(2Mk\) samples from the training data to be included in \(U\). Formally,

\[U=S_{}(2M-1)kS_{},\]

which gives the size \(|U|=2Mk\). Then, we let \(=>0\).

Note that for the above choice of \(\), \(X^{}\) is a sum of the rows from \(U\) scaled by \(\). Adding up \(k\) rows from \(S_{}\) and \(k\) rows from \(S_{}\) cancels out the \(B\)s and, so in the \(B\) is accumulated \(|U|-2k=2(M-1)k\) times, and so

\[X^{}=[*B,, _{i U}_{i}]=[ B, ,_{i U}_{i}].\]

As \(\) has \(\) on \(|U|\) elements and \(0\) otherwise, \(^{}=\)

As we set \(=\),

\[+X^{}=[- B+ {2M}B,+,0+_{i}_{i}]\] (33)\[=[0,(1+),_{i}_{i}]\] (34) \[\|+X^{}\|^{2} =\|[0,(1+),_{i U}_{i}]\|^{2}\] (35) \[=^{2}\|[0,(1+), {1}{|U|}_{i U}_{i}]\|\] (36)

For the chosen values of \(,\) the value of the objective in eq.32 is

\[}{4}\|[0,(1+), {|U|}_{i U}_{i}]\|^{2}+\] (37)

Letting

\[=\|[0,(1+),_{i  U}_{i}]\|^{2},\]

the objective is of the form \(-}{4}\). To maximize with respect to \(\), setting the derivative of the objective w.r.t \(\) to \(0\) gives:

\[1-=0=- }{4}=-} {4}=.\]

This immediately gives us

\[\|_{}\|^{2},\]

and we lower bound this quantity by upper bounding \(\).

By concentration of gaussian norm as in lemma4, with probability at least \(1-2(-c}{G^{4}})\)

\[\|_{i U}_{i}\|=}\|}_{i U}_{i} \|}(+).\]

In turn, recalling that \(|U|=2Mk\)

\[()^{2}+(+ }{})^{2}<4+(+}{ })^{2} 4++)^{2}}{2Mk}\]

The upper bound on \(\) gives the following lower bound on \(\|_{}\|^{2}\):

\[\|_{}\|^{2}+)^{2}}{2Mk}}\]

### Upper bounding the norm of solutions that rely more on the shortcut.

**Lemma 8**.: _Consider the following optimization problem from eq.16 where \(n\) samples of \(_{i},_{i}\) come from eq.1 where \(_{i}^{d}\):_

\[_{}=& _{} w_{y}^{2}+w_{z}^{2}+\|_{e}\|^{2}\\ &\ i S_{} w_{y}+Bw _{z}+_{e}^{}_{i}_{i}>1\\ &\ i S_{} w_{y}-Bw_{z}+_{e}^{} _{i}_{i}>1\\ &_{y}<B_{z}\] (38)_Let \(k=|S_{}| 1\). Then, for a fixed constant \(c=}\), with any scalar \(<}<\), with probability at least \(1-2(2k+(n-k)+1)(-c}{G^{4}})\), for \(=}\),_

\[\|_{}\|^{2} W_{}=^{2}k( +)^{2}+)^{2}}{B^{2}}\]

Proof.: Let \(k=|S_{}|\). The candidate we will evaluate the objective for is

\[=[,0,_{j S_{}}_{j}_{j}].\] (39)

High-probability bounds on the margin achieved by the candidate and norm of \(\)The margins on the shortcut group and the leftover group along with the constraints are as follows:

\[ j S_{}& m_{j}=0+B*+_{j} _{j},_{i S_{}}_{i} _{i} 1\\  j S_{}& m_{j}=0-B*+_{j}_{j},_{i S _{}}_{i}_{i} 1. \] (40)

Due to the standard normal distribution being isotropic, and \(_{j}\{-1,1\}\), \(_{j}_{j}\) has the same distribution as \(_{j}\). Then, we apply lemma5 with \(V=S_{}\), \(U=S_{}\) -- which means \(T_{v}=k\) and \(T_{u}=(n-k)\) -- to bound the margin terms in eq.40 and \(\|\|^{2}\) with probability at least

\[1-2(2k+(n-k)+2)(-c}{G^{4}}).\]

Applying the bound in eq.9 in lemma5 between a sum of vectors and a different i.i.d vector,

\[ j S_{}|_{j} _{j},_{i S_{}}_{i} _{i}|\] (41)

Applying the bound in eq.8 from lemma5

\[ j S_{}_{j} _{j},_{i S_{}}_{i} _{i}(d-3)\] (42)

The margin constraints on the shortcut and leftover from eq.40 respectively imply

\[- 1-+(d-3 ) 1\]

We choose \(=1+\), which implies an inequality that \(\) has to satisfy the following, which is due to \(d-3>0\),

\[-(1+)+(d-3) 1 }\]

Now, we choose

\[=}.\]

Computing the upper bound on the value of the objective in the primal problem in eq.17 The feasible candidate's norm \(\|\|^{2}\) is an upper bound on the solution's norm \(\|_{}\|^{2}\) and so

\[\|_{}\|^{2}\|\|^{2}=} ^{2}+\|_{j S_{}}_{j} {}_{j}\|^{2}=^{2}k\|}_{j S_{ }}_{j}\|^{2}+}{B^{2}}\]By lemma 5 which we invoked,

\[\|}_{j S_{}}_{j}\|^{2 }(+)^{2}.\]

To conclude the proof, substitute \(=1+\) and get the following upper bound with \(=}\):

\[\|_{}\|^{2}^{2}k(+)^{2}+ }{B^{2}}=^{2}k(+)^{2}+)^{2}}{B^{2}}.\]

### Concentration of \(k\) and intuition behind theorem 1

Concentration of \(k\) around \((1-)n\).Denote the event that the \(i\)th sample lies in the leftover group as \(I_{i}\): then \(E[I_{i}]=1-\) and the leftover group size is \(k=_{i}I_{i}\). Hoeffding's inequality (Theorem 2.2.6 in ) shows that for any \(t>0\), \(k\) is at most \((1-)n+t\) with probability at least \(1-(-2t^{2})\):

\[p(k-(1-)n>t)=p(_{i}(I_{i}-(1-)) >t)=p(_{i}(I_{i}-E[I_{i}])>t) (-2t^{2}).\]

Letting \(=0.9+}\) and \(t=\), gives us

\[p(k-(1-)n>t) =p(k-0.1n+>)\] \[=p(k-0.1n>0)\] \[(-2t^{2})\] \[=(-2 3n).\] \[=()^{2}\] \[<\]

To connect \(\) to shortcut learning due to max-margin classification, we take a union bound of the event that \(k<0.1n\), which occurs with probability at least \(1-\) and theorem 1 which occurs with probability at least \(1-\). This union bound guarantees that with probability at least \(1-\) over sampling the training data, max-margin classification on \(n\) training samples from eq.1 relies more on the shortcut feature if \(\) is above a threshold; and this threshold converges to \(0.9\) at the rate of \(}{{n}}}\).

### Bumpy losses improve ERM in the under-parameterized setting

**Theorem 2**.: _Consider \(n\) samples of training data from dgp in eq.1 with \(d<n\). Consider a linear classifier \(f_{}()=^{}\) such that for all samples in the training data \(_{i}^{}_{i}=b\) for any \(b(0,)\). With probability 1 over draws of samples, \(=[0,b,0^{d-2}]\)._

Proof.: Letting \(X\) be the matrix where each row is \(_{i}_{i}\), the theorem statement says the solution \(^{*}\)

\[X^{*}=b\] (43)

First, split \(^{*}=[w_{z}^{*},w_{y}^{*},_{-y}^{*}]\). Equation43 says that the margin of the model on any sample satisfies

\[(^{*})^{}=w_{y}^{*}^{2}+w_{z}^{*} +(_{-y}^{*})^{}=b (_{-y}^{*})^{}=b-w_{y }^{*}^{2}-w_{z}^{*}\]We collect these equations for the whole training data by splitting \(X\) into columns: denoting \(Y,Z\) as vectors of \(_{i}\) and \(_{i}\) and using \(\) to denote element wise operation, split \(X\) into columns that correspond to \(,\) and \(\) respectively as \(X=[Y Y Y Z X_{}]\). Rearranging terms gives us

\[w_{z}^{*}Y Z+w_{y}^{*}+X_{}_{}^{*}=b  X_{}_{}^{*}=(b-w_{y}^{*} )-w_{z}^{*}Y Z.\]

The elements of \(Y Z\) lie in \(\{-1,1\}\) and, as the shortcut feature does not always equal the label, the elements of \(Y Z\) are not all the same sign.

Solutions do not exist when one non-zero element exists in \((b-w_{y}^{*})-w_{z}^{*}Y Z\) By definition of \(^{*}\)

\[X_{}_{}^{*}=(b-w_{y}^{*})-w_{z}^{*}Y Z.\]

Denote \(r=(b-w_{y}^{*})-w_{z}^{*}Y Z.\) and \(A=X_{}\). Now we show that w.p. 1 solutions do not exist for the following system of linear equations:

\[Aw=r.\]

First, note that \(A=X_{}\) has \(_{i}_{i}\) for rows and as \(_{i}[origin={c}]{180.0}{$$}_{i}\) and \(_{i}\{-1,1\}\), each vector \(_{i}_{i}\) is distributed identically to a vector of independent standard Gaussian random variables. Thus, \(A\) is a matrix of IID standard Gaussian random variables.

Let \(U\) denote \(D-2\) indices such that the corresponding rows of \(A\) form a matrix \(D-2 D-2\) matrix and \(r_{U}\) has at least one non-zero element; let \(A_{U}\) denote the resulting matrix. Now \(A_{U}\) is a \(D-2 D-2\) sized matrix where each element is a standard Gaussian random variable. Such matrices have rank \(D-2\) with probability 1 because square singular matrices form a measure zero set under the Lebesgue measure over \(^{D-2 D-2}\)(Feng and Zhang, 2007).

We use subscript \(_{-U}\) to denote all but the indices in \(U\). The equation \(Aw=r\) implies the following two equations:

\[A_{U}w=r_{U} A_{-U}w=r_{-U}.\]

As \(A_{U}\) is has full rank (\(D-2\)), \(A_{U}w=r_{U}\) admits a unique solution \(_{U}^{*} 0\) -- because \(r_{U}\) has at least one non-zero element by construction. Then, it must hold that

\[A_{-U}_{U}^{*}=r_{-U}.\] (44)

For any row \(v^{} A_{-U}\), eq.44 implies that \(v^{}^{*}\) equals a fixed constant. As \(v\) is a vector of i.i.d standard normal random variables, \(v^{}^{*}\) is a gaussian random variable with mean \((_{i}^{*})\) and variance \(\|^{*}\|^{2}\). Then with probability \(1\), \(v^{}^{*}\) will not equal a constant. Thus, w.p.1 \(A_{-U}_{U}^{*}=r_{-U}\) is not satisfied, which means w.p.1 there are no solutions to \(A=r\).

Case where \((b-w_{y}^{*})-w_{z}^{*}Y Z\) is zero element-wiseAs \(X\) has rank \(D-2\), \(X_{}_{}^{*}=0\) only when \(_{}^{*}=0\).

Each element in \((b-w_{y}^{*})-w_{z}^{*}Y Z\) is either \(b-w_{y}^{*}+w_{z}^{*}\) or \(b-w_{y}^{*}-w_{z}^{*}\). Thus,

\[(b-w_{y}^{*})-w_{z}^{*}Y Z=0\{ []{c}b-w_{y}^{*}+w_{z}^{*}=0,\\ b-w_{y}^{*}-w_{z}^{*}=0.\] (45)

Adding and subtracting the two equations on the right gives

\[2(b-w_{y}^{*})=0 2w_{z}^{*}=0.\]

Thus, \(_{}^{*}=0,w_{z}^{*}=0,b=w_{y}^{*}\). 

## Appendix B Appendix: further experimental details and results

### Default-erm with \(_{2}\)-regularization.

In section3, we show default-erm achieves zero training loss by using the shortcut to classify the shortcut group and noise to classify the leftover group, meaning the leftover group is overfit. The usual way to mitigate overfitting is via \(_{2}\)-regularization, which, one can posit, may encourage models to rely on the perfect stable feature instead of the imperfect shortcut and noise.

We train the linear model from section3 with default-erm and \(_{2}\)-regularization -- implemented as weight decay in the AdamW optimizer (Loshchilov and Hutter, 2019) -- on data from eq.1 with \(d=800,B=10,n=1000\). Figure 4 plots accuracy and losses for the \(_{2}\)-regularized default-erm with the penalty coefficient set to \(10^{-8}\); it shows that \(_{2}\)-regularization leads default-erm to build models that only achieve \( 50\%\) test accuracy.

For smaller penalty coefficients, default-erm performs similar to how it does without regularization, and for larger ones, the test accuracy gets worse than default-erm without regularization. We give an intuitive reason for why larger \(_{2}\) penalties may lead to larger reliance on the shortcut feature. Due to the scaling factor \(B=10\) in the synthetic experiment, for a fixed norm budget, the model achieves lower loss when using the shortcut and noise compared to using the stable feature. In turn, heavy \(_{2}\)-regularization forces the model to rely more on the shortcut to avoid the cost of larger weight needed by the model to rely on the stable feature and the noise.

### Margin control (marg-ctrl)

In fig. 5, we plot the different marg-ctrl losses along with log-loss. Each marg-ctrl loss has a "bump" which characterizes the loss function's transition from a decreasing function of the margin to an increasing one. These bumps push models to have uniform margins because the loss function's derivative after the bump is negative which discourages large margins. The hyperparameters -- like temperature in \(\)-damp or function output target in marg-log -- affect the location of the bump and the slopes of the function on either side of the bump.

### Marg-ctrl on a linear model

In fig. 6, we compare default-erm to \(\)-stitch. In fig. 7 and fig. 8, compare sd and marg-log respectively to default-erm. The left panel of all figures shows that marg-ctrl achieves better test accuracy than default-erm, while the right most panel shows that the test loss is better on the leftover group using marg-ctrl. Finally, the middle panel shows the effect of controlling margins

Figure 4: Default-erm with \(_{2}\)-regularization with a penalty coefficient of \(=10^{-8}\) achieves a test accuracy of \( 50\%\), outperforming default-erm. The right panel shows that \(_{2}\)-regularization leads to lower test loss on the minority group, meaning that the regularization does mitigate some overfitting. However, the difference between the shortcut and leftover test losses shows that the model still relies on the shortcut.

Figure 5: Comparing log-loss with marg-ctrl as functions of the margin. Each marg-ctrl loss has a “bump” which characterizes the loss function’s transition from a decreasing function of the margin to an increasing one. These bumps push models to have uniform margins because the loss function’s derivative after the bump is negative which discourages large margins. The hyperparameters (temperature in \(\)-damp or function output target in marg-log.) affect the location of the bump and the slopes of the function on either side of the bump.

in training; namely, the margins on the training data do not go to \(\), evidenced by the training loss being bounded away from \(0\). Depending on the shortcut feature leads to different margins and therefore test losses between the shortcut and leftover groups; the right panel in each plot shows that the the test losses on both groups reach similar values, meaning marg-ctrl mitigates dependence on the shortcut. While default-erm fails to perform better than chance (\(50\%\)) even after \(100,000\) epochs (see fig. 1), marg-ctrl mitigates shortcut learning within \(5000\) epochs and achieves \(100\%\) test accuracy.

### marg-ctrl vs. default-erm with a neural network

With \(d=100\) and \(B=10\) in eq. (1), we train a two layer neural network on \(3000\) samples from the training distribution. The two layer neural network has a \(200\) unit hidden layer that outputs a scalar. Figure 9 shows that a neural network trained via default-erm fails to cross \(50\%\) test accuracy even after \(40,000\) epochs, while achieving less than \(10^{-10}\) in training loss.

Figure 8: A linear model trained with marg-log depend on the perfect stable feature to achieve perfect test accuracy whereas default-erm performs worse than random chance. The middle panel shows that marg-log does not let the loss on the training shortcut group to go to zero, unlike default-erm, and the right panel shows the test-loss is better for the leftover group.

Figure 6: A linear trained with \(\)-stitch depend on the perfect stable feature to achieve perfect test accuracy, unlike default-erm. The middle panel shows that \(\)-stitch does not let the loss on the training shortcut group to go to zero, unlike default-erm, and the right panel shows the test leftover group loss is better.

Figure 7: A linear model trained with sd depend on the perfect stable feature to achieve perfect test accuracy whereas default-erm performs worse than random chance. The middle panel shows that sd does not let the loss on the training shortcut group to go to zero, unlike vanilla default-erm, and the right panel shows the test-loss is better for the leftover group.

In fig. 10, we compare default-erm to \(\)-stitch. In fig. 12 and fig. 13, compare sd and marg-log respectively to default-erm. The left panel of all figures shows that marg-ctrl achieves better test accuracy than default-erm, while the right most panel shows that the test loss is better on the leftover group using marg-ctrl. Finally, the middle panel shows the effect of controlling margins in training; namely, the margins on the training data do not go to \(\), evidenced by the training loss being bounded away from \(0\).

### Spectral decoupling for a linear model on the linear dgp in eq. (1).

We first show that a linear classifier trained with sd achieves \(100\%\) test accuracy while default-erm performs worse than chance on the test data; so, sd builds models with more dependence on the stable perfect feature, compared to Empirical Risk minimization (erm). Next, we outline the assumptions for the gradient starvation (GS) regime from Pezeshki et al. (2021) and then instantiate it for a linear model under the data generating process in eq. (1), showing that the assumptions for the GS-regime are violated.

Figure 7 shows the results of training a linear model with sd on training data of size \(1000\) sampled as per eq. (1) from \(p_{=0.9}\) with \(d=300\); the test data also has a \(1000\) samples but comes from \(p_{=0.1}\)

Figure 10: A neural network trained with \(\)-stitch depend on the perfect stable feature to achieve perfect test accuracy, unlike default-erm. The middle panel shows that \(\)-stitch does not let the loss on the training shortcut group to go to zero, unlike default-erm, and the right panel shows the test leftover group loss is better.

Figure 9: Training a two-layer neural network with default-erm on data from eq. (1). The model achieves \(100\%\) train accuracy but \(<40\%\) test accuracy even after \(40,000\) epochs. The plot below zooms in on the first \(4000\) epochs and shows that the model drives down loss on the test shortcut groups but not on the test leftover group. This shows that the model uses the shortcut to classify the shortcut group and noise for the leftover.

Figure 7 shows that SD builds models with improved dependence on the perfect stable feature, as compared to \(}\), to achieve \(100\%\) test accuracy.

#### b.5.1 The linear example in Equation (1) violates the gradient starvation regime.

Background on Pezeshki et al. (2021).With the aim of explaining why \(}\)-trained neural networks depend more on one feature over a more informative one, Pezeshki et al. (2021) derive solutions to \(_{2}\)-regularized logistic regression in the ntk; they let the regularization coefficient be small enough for the regularized solution to be similar in direction to the unregularized solution. Given \(n\) samples \(^{i},^{i}\), let \(\) be a diagonal matrix with the labels on its diagonal, \(\) be a matrix with \(^{i}\) as its rows, and \(}(,)=f_{}()\) be the \(n\)-dimensional vector of function outputs where each element is \(}^{i}=f_{}(^{i})\). In gradient-based training in the ntk regime, the vector of function outputs of the network with parameters \(\) can be approximated as \(}=_{0}\), where \(_{0}\) is the neural-tangent-random-feature (ntf) matrix at initialization:

\[_{0}=}(,_{0})}{ _{0}}\]

To define the features, the strength (margin) of each feature, and how features appear in each sample, Pezeshki et al. (2021) compute the singular value decomposition (svd) of the ntf\(_{0}\) multiplied by the diagonal-label matrix \(\):

\[_{0}=^{}.\] (46)

The rows of \(\) are features, the diagonal elements of \(\) are the strengths of each feature and the \(i\)th row of \(\) denotes how each feature appears in the ntf representation of the \(i\)th sample.

To study issues with the solution to \(_{2}\)-regularized logistic regression, Pezeshki et al. (2021) define the gradient starvation (GS) regime. Under the GS regime, they assume \(\) is a perturbed identity matrix that is also unitary: for a small constant \(<<1\), such a matrix has all diagonal elements \(}\) and the rest of the elements are of the order \(\) such that the rows have unit \(_{2}\)-norm.

Figure 11: A neural network trained with \(\)-damp depend on the perfect stable feature to achieve perfect test accuracy whereas default-\(}\) performs worse than random chance. The middle panel shows that \(\)-damp does not let the loss on the training shortcut group to go to zero, unlike vanilla default-\(}\), and the right panel shows the test-loss is better for the leftover group.

Figure 12: A neural network trained with SD depend on the perfect stable feature to achieve perfect test accuracy whereas default-\(}\) performs worse than random chance. The middle panel shows that SD does not let the loss on the training shortcut group to go to zero, unlike vanilla default-\(}\), and the right panel shows the test-loss is better for the leftover group.

The GS regime is violated in eq. (1).When \(f_{}\) is linear, \(f_{}()=^{}\), the ntrf matrix is

\[}(,_{0})}{_{0}}= _{0}}{_{0}}=.\]

In this case, let us look at an implication of \(\) being a perturbed identity matrix that is also unitary, as Pezeshki et al. (2021) assume. With \((^{i})^{}\) as the \(i\)th row of \(\), the transpose of \(i\)th sample can be written as \((^{i})^{}=(^{i})^{}\). Pezeshki et al. (2021) assume that \(<<1\) in that the off-diagonal terms of \(\) are small perturbations such that off-diagonal terms of \((^{2}+)^{}\) have magnitude much smaller than \(1\), meaning that the terms \(|(^{i})^{}^{2}(^{j})+|<<1\) for \(i j\) and positive and small \(<<1\).

Then,

\[|^{i}^{j}(^{i})^{}^{j}| =|(^{i})^{}^{j}|\] (47) \[=|(^{i})^{}^{} ^{j}|\] (48) \[=|(^{i})^{}^{2}^{j}|\] (49) \[<<1\] (50)

In words, this means that any two samples \(^{i},^{j}\) are nearly orthogonal. Now, for samples from eq. (1), for any \(i,j\) such that \(^{j}=^{i}\) and \(^{i}=^{j}\),

\[|(^{i})^{}^{j}|=|B^{2}^{i} ^{j}+^{i}^{j}+(^{i})^{} ^{j}||100+1+(^{i})^{} ^{j}|\] (51)

As \(\) are isotropic Gaussian vectors, around half the pairs \(i,j\) will have \((^{i})^{}^{j}>0\) meaning \(|(^{i})^{}^{j}|>101\). This lower bound implies that \(\) is not a perturbed identity matrix for samples from eq. (1). This violates the setup of the gradient starvation regime from Pezeshki et al. (2021).

Thus, the linear dgp in eq. (1) does not satisfy the conditions for the GS regime that is proposed in Pezeshki et al. (2021). The GS regime blames the coupled learning dynamics for the different features as the cause for default-erm-trained models depending more on the less informative feature. Pezeshki et al. (2021) derive spectral decoupling (sd) to avoid coupling the training dynamics, which in turn can improve a model's dependence on the perfect feature. sd adds a penalty to the function outputs which Pezeshki et al. (2021) show decouples training dynamics for the different features as defined by the ntrf matrix:

\[_{}(,f_{}())=(1+(f _{}))+|f_{}()|^{2}\]

As eq. (1) lies outside the GS regime, the success of sd on data from eq. (1) cannot be explained as a consequence of avoiding the coupled training dynamics in the GS regime Pezeshki et al. (2021). However, looking at sd as marg-ctrl, the success of sd, as in fig. 7, is explained as a consequence encouraging uniform margins.

An example of perfect test accuracy even with dependence on the shortcut.In fig. 14, we train a linear model with default-erm on data from eq. (1), showing that even when shortcut dependence

Figure 13: A neural network trained with marg-log depend on the perfect stable feature to achieve perfect test accuracy whereas default-erm performs worse than random chance. The middle panel shows that marg-log does not let the loss on the training shortcut group to go to zero, unlike default-erm, and the right panel shows the test-loss is better for the leftover group.

is non-zero, test leftover group accuracy can be \(100\%\). Nagarajan et al. (2021) consider linearly separable data and formalize geometric properties of the data that make max-margin classifiers give non-zero weight to the shortcut feature \((_{z}>0)\). In their example, it is unclear when \(_{z}>0\) leads to poor accuracy in the leftover group because Nagarajan et al. (2021) do not separate the model's dependence on the stable feature from the dependence on noise. The example in fig. 14 gives an example where \(_{z}>0\) but test accuracy is \(100\%\), demonstrating that guarantees on test leftover group error require comparing \(_{y}\) and \(_{z}\); the condition \(_{z}>0\) alone is insufficient. In contrast, theorem 1 characterizes cases where leftover group accuracy is worse than random even without overparameterization.

### Experimental details

#### b.6.1 Background on Just Train Twice (jtt) and Correct-n-Contrast (cnc)

jttLiu et al. (2021) develop jtt with the aim of building models robust to subgroup shift, where the mass of disjoint subgroups of the data changes between training and test times. To work without training group annotations, jtt assumes rem builds models with high worst-group error. With this assumption, jtt first builds an "identification" model via rem to pick out samples that are misclassified due to model's dependence on the shortcut. Then, jtt trains a second model again via rem on the same training data with the loss for the misclassified samples upweighted (by constant \(\)). As Liu et al. (2021) point out, the number of epochs to train the identification model and the upweighting constant are hyperparameters that require tuning using group annotations. As Liu et al. (2021); Zhang et al. (2022) show that jtt and cnc outperforms lff and other two-stage shortcut-mitigating methods (Zhang et al., 2022), so we do not compare against them.

Correct-n-Contrast (cnc)In a fashion similar to jtt, the first stage of cnc is to train a model with regularized rem to predict based on spurious attributes, i.e. shortcut features. Zhang et al. (2022) develop a contrastive loss to force the model to have similar representations across samples that share a label but come from different groups (approximately inferred by the first-stage rem model). Formally, the first-stage model is used to approximate the spurious attributes in one of two ways: 1) predict the label with the model, 2) cluster the representations into as many clusters as there are classes, and then use the cluster identity. The latter technique was first proposed in (Sohoni et al., 2020). For an anchor sample \((^{i},^{i})\) of label \(=y\), positive samples \(P_{i}\) are those than have the same label but have the predicted spurious attribute is a different value: \( y\). Negatives \(N_{i}\) are those that have a different label but the spurious attribution is the same: \(=y\). For a temperature parameter \(\) and representation function \(r_{}\), the per-sample contrastive loss for cnc is:

\[_{cont}(r_{},i)=_{^{p} P_{i}}[- (^{i})^{}r_{}(^{p})/ )}{_{n N_{i}}(r_{}(^{i})^{}r_{ }(^{n})/)+_{p P_{i}}(r_{}( ^{i})^{}r_{}(^{p})/)}].\]

Figure 14: With \(d=200\) and \(n=1000\), a linear classifier can still depend on the shortcut feature and achieve \(100\%\) test accuracy. Nagarajan et al. (2021) consider linearly separable data and formalize geometric properties of the data that make max-margin classifiers give non-zero weight to the shortcut feature \((_{z}>0)\). In their example, it is unclear when \(_{z}>0\) leads to poor accuracy in the leftover group because Nagarajan et al. (2021) do not separate the model’s dependence on the stable feature from the dependence on noise. The example here gives an example where \(_{z}>0\) but test accuracy is \(100\%\). demonstrating that guarantees on test leftover group error require comparing \(_{y}\) and \(_{z}\); the condition \(_{z}>0\) alone is insufficient.

The samples \(i\) are called _anchors_. For a scalar \(\) to trade off between contrastive and predictive loss, the overall per-sample loss in the second-stage in cnc is

\[_{cont}(r_{},i)+(1-)_{log-loss}(^{i}w^{ }r_{}(^{i})).\]

**Cnc uses hyperparameters informed by dataset-specific empirical results from prior work.** The original implementation of cnc from Zhang et al. (2022) uses specific values of first-stage hyperparameters like weight decay and early stopping epoch for each dataset by using empirical results from prior work (Sagawa et al., 2020; Liu et al., 2021). The prior work finds weight-decay and early stopping epoch which lead default-erm models to achieve low test worst-group accuracy, implying that the model depends on the spurious attribute. This means the first-stage models built in cnc are pre-selected to pay attention to the spurious attributes. For example, (Zhang et al., 2022) point out that the first-stage model they use for Waterbirds predicts the spurious feature with an accuracy of \(94.7\%\).

Without using dataset-specific empirical results from prior work, choosing lr and wd requires validating through the whole cnc procedure. We let cnc use the same lr and wd for both stages and then validate the choice using validation performance of the second-stage model. This choice of hyperparameter validation leads to a similar number of validation queries for all methods that mitigate shortcuts.

#### b.6.2 Training details

Variants of marg-ctrl to handle label imbalance.The three datasets that we use in our experiments -- Waterbirds, CelebA, and Civilcomments -- all have an imbalanced (non-uniform) marginal distribution over the label; for each dataset,

\[_{\{-1,1\}}p(=)>0.75.\]

When there is sufficiently large imbalance, restricting the margins on all samples could bias the training to reduce loss on samples in the most-frequent class first and overfit on the rest of the samples. This could force a model to predict the most frequent class for all samples, resulting in high worst-group error.

To prevent such a failure mode, we follow (Pezeshki et al., 2021) and define variants of \(\)-damp, marg-log, and \(\)-stitch that have either 1) different maximum margins for different classes or 2) different per-class loss values for the same margin value. Mechanically, these variants encourage uniform margins within each class, thus encouraging the model to rely less on the shortcut feature. We give the variants here for labels taking values in \(\{-1,1\}\):

1. With per-class temperatures \(T_{-1},T_{1}>0\) the variant of \(\)-damp is \[f_{}=w_{f}{}^{}r_{}(),\] \[_{}(,f_{})=_{log}[T _{}*1.278f_{}(1-(1.278*f_ {}))]\] The \(1.278\) comes in to make sure the maximum input to log-loss occurs at \(f_{}=1\). However, due to the different temperatures \(T_{1} T_{-1}\), achieving the same margin on all samples produces lower loss on the class with the larger temperature.
2. With per-class temperatures \(T_{-1},T_{1}>0\) the variant of \(\)-stitch is \[f_{}=w_{f}{}^{}r_{}(),\] \[_{}(f_{})=_{log}(T _{}[[f_{}()<1] f_{}().+[f_{}( )>1](2-f_{}())\ ])\]
3. With per-class function output targets \(_{-1},_{1}>0\) the variant of marg-log is \[f_{}=w_{f}{}^{}r_{}(),\] \[_{}}(f_{})=_{log}( f_{})+(1+|f_{}-_{}|^{2}).\]

These per-class variants are only for training; at test time, the predicted label is \((f_{})\).

Details of the vision and language experiments.We use the same datasets from Liu et al. (2021), downloaded via the scripts in the code from (Idrissi et al., 2022); see (Idrissi et al., 2022) for sample sizes and the group proportions. For the vision datasets, we finetune a resnet50 from Imagenet-pretrained weights and for Civilcomments, we finetune a BERT model.

Optimization details.For all methods and datasets, we tune over the following weight decay (wd) parameters: \(10^{-1},10^{-2},10^{-3},10^{-4}\) For the vision datasets, we tune learning rate (lr) over \(10^{-4},10^{-5}\) and for CivilComments, we tune over \(10^{-5},10^{-6}\). For CivilComments, we use the AdamW optimizer while for the vision datasets, we use the Adam optimizer; these are the standard optimizers for their respective tasks (Puli et al., 2022; Gulrajani and Lopez-Paz, 2021). We use a batch size of \(128\) for both CelebA and Waterbirds, and train for \(20\) and \(100\) epochs respectively. For CivilComments we train for \(10\) epochs with a batch size of \(16\).

Per-method Hyperparameters.Like in (Pezeshki et al., 2021), the per-class temperatures \(T_{-1},T_{1}\) for \(\)-damp and \(\)-stitch, and the function output targets \(_{-1},_{1}\) for marg-log are hyper-parameters that we tune using the worst-group accuracy or label-balanced average accuracy computed on the validation dataset, averaged over \(2\) seeds.

1. For \(\)-stitch, we select from \(T_{-1}\{1,2\}\) and \(T_{1}\{2,4,8,12\}\) such that \(T_{1}>T_{-1}\).
2. For \(\)-damp, we search over \(T_{-1}\{1,2\}\) and \(T_{1}\{2,4\}\) such that \(T_{1}>T_{-1}\).
3. For sd and marg-log, we search over \(_{-1}\{-1,0,1\}\) and \(_{1}\{1,2,2.5,3\}\) for the image datasets and \(_{1}\{1,2\}\) for the text dataset, and the penalty coefficient is set to be \(=0.1\)

Figure 15: Images mis-classified by a model trained on CelebA data with equal group sizes, i.e. without a shortcut. Samples with blonde as the true label have a white strip at the bottom while samples with non-blonde as the true label have a black strip at the bottom. The figure demonstrates that many images with blonde people in the image have the non-blonde label, thus demonstrating label noise. For example, see a blonde man in the first row that is labelled non-blonde and a non-blonde lady in the third row that is lablled blonde. Yet, marg-ctrl improves over em for many lr and wd combinations; see fig. 16.

4. For jtt, we search over the following parameters: the number of epochs \(T\{1,2\}\) for CelebA and Civilcomments and \(T\{10,20,30\}\) for Waterbirds, and the upweighting constant \(\{20,50,100\}\) for the vision datasets and \(\{4,5,6\}\) for Civilcomments.
5. For cnc, we search over the same hyperparameter as [Zhang et al., 2022] : the temperature in \(\{0.05,0.1\}\), the contrastive weight \(\{0.5,0.75\}\), and the gradient accumulation steps \(s\{32,64\}\). For the language task in Civilcomments, we also try one additional \(s=128\).

### Marg-ctrl improves over default-erm on CelebA even without the stable feature being perfect.

CelebA is a perception task in that the stable feature is the color of the hair in the image. But unlike the synthetic experiments, marg-ctrl does not achieve a \(100\%\) test accuracy on CelebA. We investigated this and found that CelebA in fact has some label noise.

We trained a model via the marg-ctrl method \(\)-damp on CelebA data with no shortcut; this data is constructed by subsampling the groups to all equal size, (\(5000\) samples). This achieves a test worst-group accuracy of \(89\%\). We visualized the images that were misclassified by this model and found that many images with blond-haired people were classified as having non-blonde hair. Figure 15 shows \(56\) misclassified images where samples with blonde as the true label have a white strip at the bottom while samples with non-blonde as the true label have a black strip at the bottom. The figure shows that images with blonde people can have the non-blonde label, thus demonstrating label noise. Thus, marg-ctrl improves over em even on datasets like CelebA where the stable features do not determine the label.

### Sensitivity of em and marg-ctrl to varying lr and wd

In fig. 16, we compare the test worst-group accuracy of default-erm and marg-ctrl on CelebA, for different values of lr and wd. There are \(8\) combinations of lr and wd for which em is run. For each combination of lr and wd, the hyperparameters of the marg-ctrl method (values of \(,T,v\)) are tuned using validation group annotations, and the test worst-group accuracy corresponds to the best method hyperparameters. Default-erm's performance changes more with lr and wd than marg-ctrl, which shows that default-erm is more sensitive than marg-ctrl. Only 2 combinations of lr and wd improve em beyond a test worst-group accuracy of \(60\%\), while every marg-ctrl method achieves more than \(70\%\) test worst-group accuracy for every combination of lr and wd.

Figure 16: Test worst-group accuracy on CelebA of default-erm and marg-ctrl for different values of lr and wd. Default-erm’s performance changes more with lr and wd than marg-ctrl, which shows that default-erm is more sensitive than marg-ctrl. Only 2 combinations of lr and wd improve em beyond a test worst-group accuracy of \(60\%\), while every marg-ctrl method achieves more than \(70\%\) test worst-group accuracy for every combination of lr and wd.