# \(Se(3)\) Equivariant Ray Embeddings for

Implicit Multi-View Depth Estimation

 Yinshuang Xu

University of Pennsylvania

xuyin@seas.upenn.edu &Dian Chen

Toyota Research Institute

dian.chen@tri.global &Katherine Liu

Toyota Research Institute

katherine.liu@tri.global &Sergey Zakharov

Toyota Research Institute

sergey.zakharov@tri.global &Rares Ambrus

Toyota Research Institute

rares.ambrus@tri.global &Kostas Daniilidis

University of Pennsylvania

kostas@cis.upenn.edu &Vitor Guizilini

Toyota Research Institute

vitor.guizilini@tri.global

###### Abstract

Incorporating inductive bias by embedding geometric entities (such as rays) as input has proven successful in multi-view learning. However, the methods adopting this technique typically lack equivariance, which is crucial for effective 3D learning. Equivariance serves as a valuable inductive prior, aiding in the generation of robust multi-view features for 3D scene understanding. In this paper, we explore the application of equivariant multi-view learning to depth estimation, not only recognizing its significance for computer vision and robotics but also addressing the limitations of previous research. Most prior studies have either overlooked equivariance in this setting or achieved only approximate equivariance through data augmentation, which often leads to inconsistencies across different reference frames. To address this issue, we propose to embed \(SE(3)\) equivariance into the Perceiver IO architecture. We employ Spherical Harmonics for positional encoding to ensure 3D rotation equivariance, and develop a specialized equivariant encoder and decoder within the Perceiver IO architecture. To validate our model, we applied it to the task of stereo depth estimation, achieving state of the art results on real-world datasets without explicit geometric constraints or extensive data augmentation.

## 1 Introduction

Equivariance is a valuable property in computer vision, leveraging various symmetries to reduce sample and model complexity while boosting generalization. It has seen broad application in fields such as 3D shape analysis , panoramic image prediction , and robotics . In particular, there is an increasing interest in equivariant scene representation from multiple viewpoints , as the multi-view setting is a fundamental challenge in the field and equivariant representations are desirable for their robustness and efficiency.

Meanwhile, multi-view depth estimation has always been a core topic in computer vision. Previous works  leverage the explicit geometric constraint to construct the feature cost-volume for depth prediction. Recently, the paradigm of combining implicit representations with generalistarchitectures has been widely adopted and gaining success. Inserting inductive bias via the embedding of geometric entities (rays) in the multi-view setting [58; 47; 44] has become popular. Notably, in multi-view depth estimation, Yifan et al.  effectively combined geometric epipolar embeddings with image features for stereo depth estimation, outperforming traditional methods that depend on explicit geometric constraints. State-of-the-art work by  integrated multi-view geometric embeddings with image features for video depth estimation. These methods show that the implicit multi-view geometry learned by the Perceiver IO architecture, which is a more efficient general architecture compared to the vision transformer , can improve upon approaches that rely on traditional explicit geometric constraints, such as cost volumes, bundle adjustment, and projective geometry. However, the implicit multi-view geometry promoted by the Perceiver IO architecture lacks equivariance. This limitation becomes apparent when transforming the coordinate frame representing input geometry, such as camera poses, ray directions, or 3D coordinates. These transformations change the input in such a way that non-equivariant architectures are unable to achieve the same results, as shown in Figure 1.

Although  have tried to approximate equivariance through extensive data augmentation, achieving true equivariance at an architectural level remains an ongoing challenge. In this paper, we propose to embed equivariance with respect to \(SE(3)\) transformation of the global coordinate frame, i.e., gauge equivariance, to the Perceiver IO model. We substitute traditional Fourier positional encodings for the ray embedding with Spherical Harmonics, which are more suitable to represent 3D rotations. We custom-develop a \(SE(3)\) equivariant attention module to seamlessly interact with different types of equivariant features. This is achieved using a combination of invariant attention weights and equivariant fundamental layers. During the decoding stage, this equivariant latent space is disentangled into the equivariant frame and invariant global features. Our approach not only simplifies the integration of existing modules without requiring a specialized design, but also allows the network to focus on effective scene analysis via an invariant latent space, reducing the effects of global transformations. The equivariant frame is used to "standardize" the query ray, serving as an invariant input for the decoder. This method ensures that both sets of inputs for the decoder are invariant, leading to an invariant output regardless of the decoder used. Consequently, we can employ the conventional Perceiver IO decoder in our equivariant framework. In summary, our key contributions are as follows:

* We integrate \(SE(3)\) equivariance into a multi-view depth estimation model by design, using spherical harmonics as positional encodings for ray embeddings, as well as a specialized equivariant encoder.
* By leveraging the equivariant learned latent space, we introduce a novel scene representation scheme for multi-view settings, featuring a disentangled equivariance frame and an invariant scene representation.
* We assess our model's ability to learn 3D structures through wide-baseline stereo depth estimation. Our model delivers state-of-the-art results in this task, significantly surpassing the non-equivariant baseline.

## 2 Related Work

Equivariant NetworksEquivariant Networks are garnering interest in vision for their efficiency and powerful inductive bias. These networks can be categorized by the data structures over which they operate, spanning 2D images [15; 39], graphs [45; 38], 3D point clouds [60; 5], manifolds [9; 37], and spherical images [17; 7]. From an architectural perspective, methods can also be classified by the

Figure 1: Given a sparse set of posed images (red), the task is to estimate depth for a novel viewpoint (blue). The Perceiver IO struggles to accurately predict depth when the reference frame (gray) changes, equivalent to an inverse transformation applied to the object and cameras. In contrast, our model delivers the consistent result due to its equivariant design.

tools they rely on, such as group convolution [8; 16; 36; 20], steerable convolution on homogeneous spaces [53; 51; 52; 11; 54], and recently transformers [41; 40; 33; 24]. In the context of this paper, we highlight significant \(SE(3)\) equivariant transformer works. Fuchs et al.  first introduced an SE(3) equivariant transformer for point clouds, using steerable kernels for transformers and focusing on local features in point clouds. Liao and Smidt , Liao et al.  adopted a message-passing architecture for 3D equivariant transformers in point clouds. Xu et al.  applied similar techniques for ray space. Our approach differs by using direct input-level positional encodings, rather than modifying the kernel with relative poses. We learn a global, non-hierarchical representation. Safin et al.  inserts pairwise relative poses in self-attention with a conventional attention module, requiring quadratic computation and lacking compact scene representation, unlike our method. Closely related to our work, Assaad et al.  proposed vector neuron transformers embedded in the Perceiver IO encoder for point clouds. However, they replace the original latent array with a learnable transformation, did not use spherical harmonics for equivariant positional encoding, or design an equivariant decoder within the Perceiver IO framework. We treat the original latent array as invariant, and learn a disentangled representation for the decoder with versatile queries. Esteves et al.  uses spherical harmonics for positional encoding, primarily to enhance spherical function learning, not for equivariance.

Implicit Multi-View GeometryEven in the age of deep learning, traditional multi-view stereo methods like COLMAP  are still widely used for structure-from-motion. These methods are accurate but slow due to complex post-processing steps. To speed things up while maintaining accuracy, learning-based methods adapt traditional cost volume-based techniques for depth estimation [29; 3; 26; 27]. Recently, transformers  have become prevalent approaches, replacing CNNs in terms of popularity and performance. The Stereo Transformer  replaces cost volumes with an attention-based matching procedure inspired by sequence modeling. IIB  leverages Perceiver IO  for generalized stereo estimation by incorporating the epipolar geometric bias into the model. Liu et al. , Chen et al.  inject 3D geometry into the transformer akin to IIB for object detection, while Chen et al.  learns equivariance in a data-driven way. A closely related study to ours is DeFiNe , in which camera information is incorporated into Perceiver IO and used to decode predictions from arbitrary viewpoints. However, their approach relies on data augmentation to approximate equivariance in the Perceiver IO, whereas our design inherently ensures equivariance at an architectural level.

## 3 Method

In this section we start with some preliminaries about Perceiver IO and our baseline, Depth Field Networks (DeFiNe) , a state-of-the-art method integrating camera geometries into Perceiver IO for multi-view depth estimation. We then outline the concept of equivariance in multi-view scenarios in Section 3.2. Given these preliminaries, in Section 3.3 we delve into the details of our proposed equivariant positional encoding for rays, in Section 3.4 we elaborate on the attention mechanisms used in our model, and in Section 3.5 we describe our choice of encoder parameterization. Finally,

Figure 2: Our proposed Equivariant Perceiver IO (EPIO) architecture. (a) We take as input the concatenation of per-pixel image, ray, and camera embeddings, the latter two calculated using spherical harmonics. (b) The output of our equivariant encoder is a global latent code, including both global invariant and equivariant components. From those, we extract an equivariant reference frame through an equivariant MLP, while simultaneously obtaining invariant latents through inner product. (c) When a query camera is positioned in this equivariant reference frame, its pose becomes invariant, which enables the use of conventional Fourier basis to encode it. (d) Given an invariant latent and invariant pose, we use a conventional Perceiver IO decoder to generate predictions for each query ray.

in Section 3.6 we describe our decoder procedure, focusing on the task of depth estimation. The pipeline of our proposed \(SE(3)\) equivariant model in multi-view context is shown in Figure 2.

### Preliminaries: Input-level Inductive Biases to Perceiver IO

The Perceiver IO  is a generalist transformer architecture that encodes input data \(^{N_{i} C_{i}}\) into a latent space \(^{N_{R} C_{R}}\) by cross-attending \(\) with \(\). Further refinement of this latent space \(\) is achieved using self-attention layers, followed by cross-attention to decode predictions \(^{N_{o} C_{o}}\) using queries \(^{N_{o} C_{q}}\). Many works exploit its generic nature by introducing inductive biases at an input level, namely, providing prior knowledge about the data for implicit reasoning. Specifically, DeFiNe  uses camera geometries to construct 3D positional encodings for the multi-view problem. Given \(N\) images \(\{I_{i}\}_{i=1}^{N}\) from a set of cameras with poses \(\{T_{i}\}_{i=1}^{N}\) and intrinsics \(\{K_{i}\}_{i=1}^{N}\), DeFiNe calculates 3D rays \(\{r_{uv}^{i}\}_{uv=(1,1)}^{(H,W)}\) from each camera center \(t_{i}\) to each pixel \((u,v)\) on image \(I_{i}\), and obtains positional encodings \(PE(r_{uv}^{i},t_{i})\) with a mapping \(PE()\). These positional encodings are combined with image embeddings \(=\{f_{uv}^{i}\}\) from a visual feature extractor to be encoded by \(\) such that:

\[_{1} =(_{0},\{f_{uv}^{i} PE(r_{uv}^ {i},t_{i})\})\] \[_{k} =(_{k-1}), k=2,,K\]

To obtain predictions for a set of \(M\) novel viewpoints, we can similarly calculate 3D query rays from poses \(\{T_{j}^{}\}_{j=1}^{M}\) and intrinsics \(\{K_{j}\}_{j=1}^{M}\) and map them to query positional encodings \(=\{PE(r_{uv}^{j},t_{j})\}\), which will be used to decode the latent space \(\) via cross attention: \(=(,_{K})\). In this way, prior knowledge, i.e., 3D camera geometries, is directly fed into the model as additional input features for the implicit learning of multi-view geometry.

### Equivariance Definition in Multiview Context

After introducing the input-level inductive bias framework, it is worth noting that the poses of the encoding cameras, as well as the query viewpoints, are defined in a global reference frame \(T_{}\). However, this choice of global reference frame is subject to change, and the property of equivariance ensures that predictions remain identical under these changes. Assuming the global reference frame undergoes a transform \(T^{-1} SE(3)\) to \(T_{}^{}=T^{-1}T_{}\), the ray representations become \((Rr_{uv}^{i(j)},Rt_{i(j)}+t)\) when representing \(T=(R,t)\). The equivariant model \(\) should satisfy

\[(\{f_{uv}^{i} PE(Rr_{uv}^{i},Rt_{i}+t)\},\{PE(Rr_{uv}^ {j},Rt_{j}+t)\})\] \[=(\{f_{uv}^{i} PE(r_{uv}^{i},t_{i})\},\{PE(r_{uv}^{j},t_ {j})\}).\]

For further details on the definition of the equivariance, please see Appendix A.2.

### Equivariant Positional Encoding

To ensure the positional encoding process is equivariant w.r.t a transformation group \(G\), we would like to enforce that \((,PE(_{g}^{}x))=(,_{g}^{}PE(x))\) for any \(g G\), where \(^{}\) is the group representation on coordinate space, and \(^{}\) is the group representation on the positional encoding space. The traditional Fourier basis is translationally equivariant, as detailed in Appendix B. Kitaev et al.  used this to attain translational equivariance, employing a conjugate product for invariant attention. However, this approach lacks rotational equivariance.

This raises a key question: Are there any basis functions equivariant to both 3D translations and rotations? Unfortunately, none exist. However, a common method in equivariant works for translational equivariance is to subtract the center point, a technique we apply in our context as illustrated in Figure 3. This enables translational invariance, leaving the model to focus solely on achieving rotational equivariance. To address the 3D rotational equivariance, we turn to spherical harmonics (SPH), known for their inherent rotation-equivariant properties. They offer a way to accommodate 3D rotational changes, thereby achieving \(SE(3)\) equivariance.

#### 3.3.1 Spherical Harmonics

We provide a detailed introduction to Spherical Harmonics in Appendix A.4, where we also discuss how their application in previous equivariant transformers differs from their use in our model. Below,we present a brief overview of Spherical Harmonics for clarity. Similar to the varying frequencies of sines and cosines in Fourier series, spherical harmonics are characterized by different degrees (orders), denoted as \(l\). An order-\(l\) spherical harmonics, denoted as \(Y^{l}:^{3}^{2l+1}\), follows the transformation rule: \(Y^{l}(Rr)=D^{l}(R)Y^{l}(r),Y^{l}(r)=\|r\|^{l}Y^{l}()\), where \(R SO(3)\), \(\) is the unit vector, \(D^{l}:SO(3)^{(2l+1)(2l+1)}\) is called the Wigner-D matrix, serving as the irreducible representation of \(SO(3)\) corresponding to the order \(l\). The Wigner-D matrix is an orthogonal matrix, that is \(D^{l}(R)(D^{l}(R))^{T}=I\). These important properties allow us to achieve equivariance in the Perceiver IO transformer architecture.

#### 3.3.2 Equivariant Hidden features

In our model, we embed both camera centers and viewing rays using spherical harmonics. The embedding is given by: \(PE(r^{i}_{uv},t_{i})=_{l L}(Y^{l}(r^{i}_{uv}) Y^{l}(t_{i}- ))\), where each part corresponds to the same order of spherical harmonics (Figure 3). Here, \(L=\{1,2,,l_{max}\}\) and \(=_{i=1}^{N}t_{i}\), highlighting the extraction of the cameras' central position for translational invariance. Due to the properties of spherical harmonics, the positional encoding of transformed input, \(PE(Rv^{i}_{uv},Rt_{i}+t)\), is equal to \(_{l L}(D^{l}(R)(Y^{l}(r^{i}_{uv}) Y^{l}(t_{i}-)))=R  PE(r^{i}_{uv},t_{i})\), for any rotation \(R SO(3)\) and translation \(t^{3}\). In other words, it guarantees that these embeddings are both _rotationally equivariant_ and _translationally invariant_. The transformation of these embeddings operates by multiplying each block with its respective Wigner-D matrix.

The image remains unchanged when the reference frame is transformed, as its contents are unaffected. Mathematically, this property is akin to multiplying by a \(0\)-order Wigner Matrix, equivalent to an identity. Thus, combining image features with positional encodings (Figure 3) transform as:

\[(f^{i}_{uv},PE(Rv^{i}_{uv},Rt_{i}+t)) =(D^{l}(R)f^{i}_{uv},R PE(r^{i}_{uv},t_{i}))\] \[=R(f^{i}_{uv},PE(r^{i}_{uv},t_{i}))\]

In our model, the equivariant hidden features mirror the structure of our embeddings, composed as \(_{l L}H_{l}\) with subscripts indicating the feature type and \(L=\{0,1,,l_{max}\}\). The size for each feature type \(H_{l}\) follows \((2l+1,C_{l})\), where \(2l+1\) is the intrinsic dimension and \(C_{l}\) is the number of channels. For more a intuitive understanding, we visualize these embeddings in Appendix C (Figure 10). Similar to the input embeddings, any rotation \(R\) in \(SO(3)\) rotates the hidden features as

Figure 3: Comparison between an equivariant input embedding in our model (left) and the conventional input embedding in DeFiNe (right). (a) Pipeline used to generate input embeddings for the encoder, resulting in cross-attention keys and values. (b) To generate geometric information, we calculate embeddings for each ray \(r^{i}_{uv}\) and camera relative position \(t_{i}-\); (c) The final composed embedding format includes both image embeddings, which are invariant, and geometric embeddings, which are equivariant. In contrast, the conventional approach by Perceiver IO, as highlighted in parts (a) and (c), integrates Fourier positional encodings with image embeddings to form the input embeddings. Furthermore, as indicated in (b), Perceiver IO utilizes each ray \(r^{i}_{uv}\) and the absolute translation \(t_{i}\) for positional encoding purposes.

\[R_{l L}H_{l}=_{l L}D^{l}(R)H_{l}\]

where \(D^{l}\) are the Wigner-D matrices. We disregard any translation action since the input and queries become translation-invariant after center subtraction.

### Basic Attention Modules

This section highlights the equivariant attention module, fundamental to ensure encoder equivariance as depicted in Figure 3(a). It consists of basic equivariant layers and an invariant multi-head inner product. Our architecture, unlike typical equivariant transformers , does not enforce geometric constraints in the equivariant kernel. Instead, it incorporates all geometric features at an input-level. Our module, utilizing the Perceiver IO structure, learns global latent representations, in contrast to other methods that emphasize the hierarchical learning of local features.

Equivariant Foundamental LayersFor the fundamental layers, we use the equivariant linear layer and layer normalization commonly used in previous works , and provide additional details in Appendix A.3. For equivariant nonlinear layers, there have been multiple proposed methods for features with the same format as ours: Norm-based Nonlinearity, Gate Nonlinearity, and Fourier-based Nonlinearity. Here, we take inspiration from the nonlinearity of Vector Neuron  and adapt a similar vector operation to higher-order features. Please see Appendix E for details of equivariant nonliearity. To better understand the differences between the basic layers in equivariant attention module and those in conventional one, we have visualize them in Appendix A.3 and Appendix E.

Multi-Head Attention Inner ProductAs done in previous equivariant transformer works , we can obtain the invariant attention matrix through inner product of the same types of features. These transformers that emphasize the hierarchical learning of local features suggest using tensor products of edge feature and node feature to mix different feature types, which is computationally demanding. We discard the tensor product and only calculate attention weights using various feature types and then multiply these weights with multi-type features to efficiently integrate different types of feature. Please see Appendix F for more details. Alternatively, we can mix feature types by treating them as Fourier coefficients for spheres, apply transformers on the sphere, and use Fourier Transform to obtain new coefficients. Please refer to Appendix H for details.

Figure 4: Left: Our equivariant module is distinct from traditional implementations  in its fundamental layers and the key-query product, that are crafted to be respectively equivariant and invariant. Right: Equivariant latent array used as additional input to the encoder. We apply equivariant positional encoding to each camera rotation, which is then averaged. We leverage an equivariant linear layer to get a global geometric latent \(_{l}G_{l}\), which is concatenated with the conventional latent array \(_{0}\) to compose our proposed equivariant latent array \(_{0}^{}\).

### Equivariant Encoder

#### 3.5.1 Equivariant Cross-attention

As shown in the left part in Figure 3 and Section 3.3.2, the cross-attention input is in the format \(_{l L}H_{l}\) with \(C_{l}=2\) for \(l 1\) and \(C_{0}\) being the channel number of \(f^{i}_{uv}\). To facilitate a clearer understanding, a comparison of this input embedding with the one used in DeFiNE is also depicted in Figure 3. The latent array \(_{0}=((R_{0})_{1},(R_{0})_{2},,(R_{0})_{N_{R}})^ {N_{R} C_{R}}\) can be treated as a scalar (\(0\)-order) feature, remaining constant during transformations in the reference frame. To make the latent array also learn the geometric information, we apply a technique similar to , learning equivariant features from the input's averaged geometric information. Specifically, we apply the positional encoding (PE) for each camera rotation, with each order being the concatenation of embeddings of the rotation matrix's three columns. The PE is then averaged over cameras. For the specific formulation please see Appendix I.

We obtain a global geometric latent \(\) using an equivariant linear layer, where the size of the weight matrix \(W_{l}\) for each type \(l\) is \((3,N_{R}C^{l})\), with \(C^{l}\) being the channel count for type-\(l\) feature in each latent. We then append this equivariant feature to the latent \(_{0}\), forming a new latent array \(^{}_{0}=((R^{}_{0})_{1},(R^{}_{0})_{2},,( R^{}_{0})_{N_{R}})\), where \((R^{}_{0})_{i}=(R_{0})_{i}_{l L}(G_{l})_{i}\) with \(L=\{1,2, l_{max}\}\) and \((G_{l})_{i}^{(2l+1) C^{l}}\). Figure 4b illustrates the construction of this equivariant latent array. With both an equivariant input embedding and latent array, we apply equivariant cross-attention to get the equivariant latent output.

#### 3.5.2 Equivariant Self-Attention

We apply a self-attention equivariant attention mechanism to the equivariant output of cross-attention, producing a conditioned equivariant latent code. For visualization purposes (Figure 5), we can treat the equivariant latent code as the Fourier coefficients of spherical functions. Note that we do not map the features to a 2D color image. Since we have features with type-0, type-1, and type-2, etc, but we randomly select each channel from different types of feature and apply the inverse Fourier transform to get a spherical function and visualize it on a 3D sphere. For a proof of this result (i.e., the visualized sphere is also rotated when the latent code is rotated), please see Appendix D.

### Decoder

In Figure 2 we show that, before inputting the equivariant latent space and geometric query to the decoder, they are converted into an invariant form by establishing an equivariant frame. Specifically, the equivariant latent space \(_{K}\) is represented as \(_{l L}(_{K})_{l}\). From its type-1 feature \((_{K})_{1}\), we employ an equivariant MLP and the Gram-Schmidt orthogonalization  to derive an equivariant frame, represented by a rotation matrix \(R\). As depicted in Figure 5, the equivariant frame's rotation aligns with that of both the equivariant latent and the \(3D\) scene. Applying the inverse of \(R\) to \(_{K}\) results in a rotation-invariant latent code \(_{l L}D^{l}(R)^{T}(_{K})_{l}\), obtaining an invariant representation. See Appendix J for a proof.

Figure 5: Equivariant latent code and predicted frame. For simplicity, we use object rotation to denote the inverse rotation of the reference frame. When the object is rotated, our latent code and predicted canonical frame are also rotated.

For the embedding of camera center and viewing rays, we apply \(R^{T}\) to \(r^{j}_{uv}\) and \(t_{j}-\) to obtain invariant coordinates (see Appendix J for a proof), denoted as \(R^{T}r^{j}_{uv}\) and \(R^{T}(t_{j}-)\). We then use traditional sine and cosine positional encoding for these invariant coordinates, which allows us to leverage higher frequency information beyond the dimensional constraints of SPH. Since both the latent hidden state and the query are invariant to the transformation, this enables us to apply conventional cross-attention mechanisms to obtain invariant outputs and predictions, capturing higher frequency details and improving expressiveness.

## 4 Experimental Results

### Datasets and Implementation

We use **ScanNet** and **DeMoN** to validate our model on the task of stereo depth estimation. For ScanNet, we use the same setting as , which downsamples scenes by a factor of \(20\) and splits them to obtain \(94212\) training and \(7517\) test pairs. The DeMoN dataset includes the SUN3D, RGBD-SLAM and Scenes11 datasets, where SUN3D and RGBD-SLAM are real world datasets and Scenes11 is a synthetic dataset. There are a total of \(166285\) training image pairs from \(50420\) scenes, and we use the same test split as  (\(80\) pairs in SUN3D, \(80\) pairs in RGBD and \(168\) pairs in Scenes11). We include details on the network architecture and implementation in Appendix L.

### Stereo Depth Estimation

We compare our equivariant model with other state-of-the-art methods on stereo depth estimation, and report quantitative results in Table 1. As we can see, it significantly outperforms competing methods on all real-world datasets and shows comparable results to the state-of-the-art on Scenes11, a synthetic dataset. This superior performance on real-world datasets is evidence of the benefits of using equivariance in multi-view scene representation. Synthetic datasets, unaffected by real-world lighting conditions, camera miscalibration and view-dependent artifacts, might benefit approaches such as DPSNet  and NAS  that use cost volume to achieve view consistency. It's also noteworthy that NAS  uses additional ground truth surface normals as supervision.

We denote our model with "Equi" and our baseline, DeFiNe, with "Nonequi" to highlight that both use the same architecture, Perceiver IO, with the key difference being the presence of equivariance in our model.

Additionally, to assess the advantages of incorporating equivariance into our model, we conducted a comparative analysis of our model against our nonequivariant DeFiNe baseline , both with and without

   Method & Abs.Rel. \(\) & RMSE \(\) & \(<1.25\) \\  DeFiNe (w/o VCA) & 0.117 & 0.291 & 0.870 \\ Ours (w/o VCA) & 0.104 & 0.247 & 0.893 \\  DeFiNe (w/o jitter) & 0.099 & 0.261 & 0.891 \\ DeFiNe & 0.093 & 0.246 & 0.911 \\ Ours (w/o jitter) & **0.086** & **0.229** & **0.923** \\   

Table 2: Comparison of our EPIO model and DeFiNe on ScanNet regarding the use of data augmentation. _VCA_ stands for _virtual camera augmentation_, and _jitter_ stands for _canonical camera jittering_.

Figure 6: Stereo depth estimation results on ScanNet, using our proposed EPIO architecture.

data augmentation. We explore two kinds of data augmentation: virtual camera augmentation (VCA), in which novel viewpoints are generated via pointcloud reprojection; and canonical camera jittering (CCJ), in which the reference frame is perturbed with random rotation and translation, reported in Table 2.

To further showcase our equivariant properties, we visualize the predicted canonical frame and reconstructed \(3D\) point clouds from depth maps. In Figure 6(a), we see that, for the same scene, when we switch the reference frame (in red) between cameras, the output point clouds change when using the standard Perceiver IO architecture, while ours remain constant, since the predicted depth is equivariant to transformations. Furthermore, even when we use different image pairs within the same scene, which theoretically cannot be guaranteed equivariant due to changes in image content, our model still predicts near-consistent canonical frames and point clouds, as illustrated in Figure 6(b).In the meanwhile, we compare our model with current state-of-art depth estimation model Depth anything , and we provide the results in Appendix O.1.

### Ablation Study

We performed an ablation study on the geometric positional encodings, spherical harmonics encoding, equivariant attention, and the decoder architecture, and report the quantitative results in Table 3. As expected, when positional encoding is not used, results are significantly degraded due to missing geometric information. Our results demonstrate that the model leverages geometric information to learn implicit multi-view geometry. Although using Fourier positional encodings with our method breaks the equivariant properties, we conducted ablations by replacing spherical harmonics with Fourier encodings to assess the specific contribution of spherical harmonics.

  Dataset & Method & Abs.Rel. \(\) & RMSE \(<1.25\) & Dataset & Method & Abs.Rel. \(\) & RMSE \(<1.25\) \\   & DPSNet & 0.126 & 0.315 & - & & DeMoN & 0.157 & 1.780 & 0.801 \\  & NAS & 0.107 & 0.281 & - & & DeepMVS & 0.294 & 0.868 & 0.549 \\  & IIB & 0.116 & 0.281 & 0.908 & & & DPSNet & 0.151 & 0.695 & 0.804 \\  & DeFiNe & 0.093 & 0.246 & 0.911 & RGBD-SLAM & NAS & 0.131 & 0.619 & 0.857 \\  & **Ours** & **0.086** & **0.229** & **0.923** & & & IIB & 0.095 & 0.550 & 0.907 \\  & & & & & & **Ours** & **0.080** & **0.433** & **0.912** \\   & DeMoN & 0.214 & 2.421 & 0.733 & & & DeMoN & 0.556 & 2.603 & 0.496 \\  & DeepMVS & 0.282 & 0.944 & 0.562 & & & DeepMVS & 0.210 & 0.891 & 0.688 \\  & DPSNet & 0.147 & 0.449 & 0.781 & & & DPSNet & 0.050 & 0.466 & 0.961 \\  & NAS & 0.127 & 0.378 & 0.829 & Scenes11 & NAS & **0.038** & **0.371** & **0.975** \\  & IIB & 0.099 & 0.293 & 0.902 & & & IIB & 0.055 & 0.523 & 0.963 \\  & **Ours** & **0.090** & **0.260** & **0.912** & & & **Ours** & 0.069 & 0.617 & 0.965 \\  

Table 1: Stereo depth estimation results compared with the state-of-the-art: DPSNet , NAS , IIB , DeFine , DeMoN , DeepMVS .

Figure 7: The figure (a) shows the equivariance of changing reference frame (Red: reference frame): For the same input and varying camera frames as reference, the Perceiver IO’s predictions change, but our model’s predictions stay consistent and the predicted frame is equivariant to the reference frame transformation. The figure (b) shows the approximate equivariance for different camera sets.

As shown in the table, Fourier encodings, which are not equivariant, are incompatible with an equivariant architecture, resulting in significantly worse performance. Additionally, we replaced the equivariant attention module with a conventional one in another ablation study. Removing the equivariant attention layers also disrupts the equivariance of our architecture, leading to a substantial drop in performance since the model loses its theoretical equivariance. We also explore the impact of the maximum order of spherical harmonics in the positional encodings, indicated by \(l_{max}\). For network architecture, we evaluated the impact of not learning the canonical frame, that is, we use the equivariant attention module in the decoder followed by transferring the equivariant output to invariant output via inner product, see Appendix K for details. We noticed that higher order of spherical harmonics improve depth estimation, since high frequency promotes fine-grained learning and differentiate positions in a higher-dimensional space.

Unlike Fourier basis, the dimension of the spherical harmonics grows two times linearly with increasing order, which is a limitation of our method, and therefore we keep the highest SPH order as \(8\) in our final model. This is also a reason why learning a equivariant canonical frame for invariant decoding with Fourier basis and a conventional decoder is a better approach than directly using an equivariant decoder. Another factor is that learning a canonical frame enforces all inputs to the decoder to be invariant, which should facilitate 3D reasoning. Moreover, we performed additional small-scale experiments to study the impact of the number of available views, see Appendix O.2.

## 5 Conclusion and Discussion

We introduce an \(SE(3)\) equivariant model designed to learn the equivariant \(3D\) scene prior across multiple views, utilizing spherical harmonics for positional encoding and specialized equivariant attention mechanisms within the Perceiver IO architecture. Additionally, our design exploits its equivariant latent space to disentangle equivariant frames and invariant scene details, enabling the seamless integration of various existing decoders in conjunction with our specialized encoder. our model's capability in 3D structure comprehension is showcased through its superior performance in stereo depth estimation, significantly exceeding that of non-equivariant models. Our architecture can be modified to accommodate a wider range of vision tasks, which we leave to future work (for a more detailed discussion please see Appendix M).

LimitationAs discussed in Section 4.3, unlike the Fourier basis, the dimension of spherical harmonics increases linearly at twice the rate with each order. This limits the number of spherical harmonics and the maximum frequency utilized, resulting in an inability to preserve detailed features in cameras and images. Additionally, the presence of different types of features, each with its own linear and nonlinear layers, slightly slows down the forward process compared to traditional methods. Moreover, we observe instability in training the equivariant network, which may be due to the magnitude explosion of high-order spherical harmonics.

   Variation & Abs.Rel.\(\) & RMSE\(\) & \(<1.25\) \\  w/o camera information & 0.229 & 0.473 & 0.661 \\ w/ Fourier & 0.131 & 0.318 & 0.843 \\ w/o equi-attention & 0.127 & 0.314 & 0.851 \\ Type \(l_{max}=1\) & 0.134 & 0.310 & 0.869 \\ Type \(l_{max}=2\) & 0.125 & 0.302 & 0.875 \\ Type \(l_{max}=4\) & 0.116 & 0.283 & 0.898 \\ EquiDecoder & 0.128 & 0.317 & 0.857 \\ 
**Full Model** & **0.086** & **0.229** & **0.923** \\   

Table 3: Ablation study on the choice of positional encoding frequency and decoder architecture.