# Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models

**Sangwon Jang\({}^{*,1}\),** **Jaehyeong Jo\({}^{*,1}\),** **Kimin Lee\({}^{,1}\),** **Sung Ju Hwang\({}^{,1,2}\)**

\({}^{*}\)Equal contribution \({}^{}\)Equal advising KAIST\({}^{1}\), DeepAuto.ai\({}^{2}\)

{ sangwon.jang, harryjo97, kiminlee, sjhwang82 }@kaist.ac.kr

Text-to-image diffusion models have shown remarkable success in generating personalized subjects based on a few reference images. However, current methods often fail when generating multiple subjects simultaneously, resulting in mixed identities with combined attributes from different subjects. In this work, we present MuDI, a novel framework that enables multi-subject personalization by effectively decoupling identities from multiple subjects. Our main idea is to utilize segmented subjects generated by a foundation model for segmentation (Segment Anything) for both training and inference, as a form of data augmentation for training and initialization for the generation process. Moreover, we further introduce a new metric to better evaluate the performance of our method on multi-subject personalization. Experimental results show that our MuDI can produce high-quality personalized images without identity mixing, even for highly similar subjects as shown in Figure 1. Specifically, in human evaluation, MuDI obtains twice the success rate for personalizing multiple subjects without identity mixing over existing baselines and is preferred over 70% against the strongest baseline. Our project page is at https://mudi-t2i.github.io/.

Figure 1: Given a few images of multiple subjects (red boxes), MuDI can personalize a text-to-image model (e.g., SDXL ) to generate multi-subject images without identity mixing. Some reference images (e.g., Cloud Man and Blue Alien) are created by Sora , introducing novel concepts not previously encountered by SDXL.

## 1 Introduction

Text-to-image diffusion models, trained on large datasets of image and text pairs, have shown great success in generating high-quality images for given text prompts [39; 43; 40; 5]. Building on this success, there is a growing interest in personalizing these text-to-image models. Specifically, given a few images of a single user-defined subject, several methods have been developed to enable these models to generate images of the subject in novel contexts [13; 42; 50; 21]. Furthermore, these personalizing methods have been expanded to include the customization of style , background , and activity , offering even greater flexibility and creativity in image generation.

Despite significant progress in personalizing text-to-image models for single subjects, current methods often struggle to handle multiple subjects simultaneously [21; 15]. While successful in rendering each subject individually, these methods suffer from identity mixing during the composition of subjects. For instance, as shown in Figure 2(a), recent works, such as DreamBooth , generate images with mixed identities when applied to two dogs. The problem of identity mixing becomes more pronounced with semantically similar subjects that share attributes, such as colors or textures, which leads to greater confusion in maintaining distinct identities.

To address identity mixing in multi-subject personalization, Han et al.  proposed to use CutMix , an augmentation technique that presents the models with cut-and-mixed images of the subjects during personalization. However, using Cut-Mix-like images inevitably often results in the generation of unnatural images with stitching artifacts, such as vertical lines that separate the subjects. Moreover, Cut-mix remains unsuccessful in decoupling similar subjects (see Figure 2(b)). There are alternative approaches [8; 25; 14] that rely on pre-defined conditioning, e.g., bounding boxes or ControlNet  to separate the identities spatially. However, such auxiliary inputs like sketch  could be difficult to obtain, and we have observed that the layout conditioning is ineffective for recent diffusion models such as SDXL  (see Figure 2(c) and Appendix B.11).

In this work, we propose MuDI, a multi-subject personalization framework that effectively addresses identity mixing, even for highly similar subjects. Our key idea is to leverage the segmented subjects obtained by a foundation model for image segmentation (Segment Anything Model (SAM) ), enabling the decoupling of the identities among different subjects. Specifically, we extract segmentation maps of the user-provided subjects using SAM and utilize them for both training and inference. For training, we introduce a data augmentation method that randomly composes segmented subjects, which allows efficient personalization by removing identity-irrelevant information. Additionally, we utilize the segmented subjects to initialize the generation process. Instead of starting from Gaussian noise, we begin with a mean-shifted random noise created from segmented subjects. We find that this provides a helpful hint for the model to separate the identities and further reduces subject missing during generation. Notably, our approach significantly mitigates identity mixing as shown in Figure 2, without relying on preset auxiliary conditions such as bounding boxes or sketches.

We evaluate the effectiveness of the proposed framework using a new dataset composed of subjects prone to identity mixing, which includes a diverse range of categories from animals to objects and scenes. To facilitate this evaluation, we introduce a new metric specifically designed to assess the

Figure 2: **Comparison of multi-subject personalization methods using Corgi and Chow Chow images (red boxes) using SDXL . DreamBooth  produces mixed identity dogs, such as a Corgi with Chow Chow ears1. Cut-Mix  often generates artifacts like unnatural vertical lines. Additionally, using layout conditioning like region control  proves ineffective in preventing identity blending in recent advanced diffusion models such as SDXL. In contrast, ours successfully personalizes each dog, avoiding identity mixing and artifacts observed in prior methods.**fidelity of multiple subjects in the images, taking into account the degree of identity mixing. In our experiments, MuDI successfully personalizes the subjects without mixed identities, significantly outperforming DreamBooth , Cut-Mix , and Textual Inversion , in both qualitative and quantitative comparisons. Further human study with side-by-side comparisons of MuDI over other methods shows that human raters prefer our method by more than 70% over the strongest baseline.

## 2 Related work

Text-to-image personalizationPersonalized text-to-image diffusion models have shown impressive abilities to render a single user-specific subject in novel contexts from only a few images. Two representative classes of personalization methods have been proposed by Gal et al.  and Ruiz et al. . Textual Inversion  optimizes new text embedding for representing the specified subjects and has been improved for learning on extended embedding spaces [50; 1]. On the other hand, DreamBooth  fine-tunes the weights of the pre-trained model to bind new concepts with unique identifiers and has been developed by recent works [21; 15; 48] for efficiently fine-tuning the models.

However, existing methods fall short of synthesizing multiple user-defined subjects together, suffering from identity mixing. Han et al.  introduce Cut-Mix to address identity mixing by augmenting Cut-Mix-like images during training but fail to separate similar subjects and generates stitching artifacts. Other lines of work [25; 14] compose personalized subjects using layout conditioning, which manipulates cross-attention maps with user-defined locations. Yet such conditioning based on cross-attention maps is ineffective for recent diffusion models such as SDXL . In this work, we develop a novel framework that allows the personalization of multiple subjects without identity mixing even for subjects with similar appearances.

Modular customizationRecent works [21; 14; 35] explore a different scenario for personalizing multiple subjects, namely _modular customization_, where the subjects are independently learned by models and users mix and match the subjects during inference to compose them. Custom Diffusion  merges individually fine-tuned models by solving constrained optimization and Mix-of-Show  introduces gradient fusion to merge single-concept LoRAs . When handling multiple subjects, these works also suffer from identity mixing, and they rely on preset spatial conditions such as ControlNet  and region control  to address the problem. Notably, our method can be applied to this scenario to decouple subjects' identities without using such conditions.

## 3 Preliminaries

Text-to-image diffusion modelsDiffusion models [17; 46] generate samples from noise by learning to reverse the perturbation, i.e., denoise, which can be modeled by a diffusion process. To be specific, at each step of the diffusion, the model predicts the random noise \((0,)\) that has been used to corrupt the sample. Text-to-image diffusion models [43; 40] incorporate text conditions for the generation. Given the dataset \(\) consisting of the image-text pairs \((,)\), text-to-image diffusion models parameterized by the noise prediction model \(_{}\) can be trained with the following objective:

\[_{DM}(;)=_{(,),(0,),t(0,T)}[ \|_{}(_{t};,t)-\|_{2} ^{2}],\] (1)

where \(\) is the random noise, time \(t\) is sampled from the uniform distribution \((0,T)\), and \(_{t}=_{t}+_{t}\) for the coefficients \(_{t}\) and \(_{t}\) that determine the noise schedule of the diffusion process.

Personalizing text-to-image modelsGiven a few images of a single specific subject, DreamBooth  fine-tunes the weights of the diffusion model with a unique identifier for the subject, i.e., "a [identifier] [class noun]". The model weights are updated to learn the subject while preserving the visual prior, which can be achieved by minimizing the objective:

\[_{DB}()=_{DM}(;_{ref })}_{}+_{DM}(; _{prior})}_{},\] (2)

where \(_{DM}\) is the loss defined in Eq. (1), \(_{ref}\) is the dataset consisting of reference images of the subject, \(_{prior}\) is the dataset consisting of class-specific prior images, and \(\) is a coefficient for the prior preservation loss. Similar to personalizing a single subject, existing works [21; 15] jointly train for multiple subjects by combining the images from the set of user-specified subjects to construct \(_{ref}\) and using different identifiers for each subject.

## 4 MuDI: Multi-subject personalization for decoupled identities

In this section, we present MuDI: **M**ulti-subject personalization for **D**ecoupled **I**dentities, which leverages segmented subjects to separate identities. In Section 4.1, we describe our training method, which augments training data through random compositions of segmented subjects. We also introduce a simple inference method that initializes noise for sample generation based on subject segmentation in Section 4.2. Finally, we present a new metric to evaluate the multi-subject fidelity in Section 4.3.

### Training

Personalization with augmentationTo address identity mixing in multi-subject personalization, we introduce a new data augmentation method for training the pre-trained text-to-image model called _Seg-Mix_. We aim to mitigate identity mixing by leveraging segmented subjects during personalizing text-to-image models. By isolating each subject from the background, Seg-Mix enables the model to learn to distinguish between different identities effectively. We integrate Seg-Mix with DreamBooth , which personalizes text-to-image models using unique identifiers (see Eq. (2)).

To implement Seg-Mix, we preprocess reference images by automatically extracting segmentation maps of user-provided subjects using the Segment Anything Model (SAM) . Specifically, this process begins with the extraction of subject bounding boxes using the OWLv2 , an object detection model with an open vocabulary. Subsequently, SAM segments the subjects based on these bounding boxes, as illustrated in Figure 3(a). After the preprocessing step, we create augmented images by randomly positioning the resized segmented subjects, as illustrated in Figure 3(b). We provide the detailed procedures of our method in Algorithm 1. These augmented images are paired with a simple prompt "A photo of [\(V_{1}\)] and [\(V_{2}\)], simple background.", which is designed to explicitly remove identity-irrelevant information. We also apply this augmentation to the prior dataset by creating images from segmented prior subjects. Using augmented datasets, we fine-tune text-to-image models based on the DreamBooth objective function in Eq.(2).

One of the key advantages of Seg-Mix is its ability to train models without identity-irrelevant artifacts, due to the removal of backgrounds. This process also mitigates unnatural artifacts, such as stitching artifacts observed in previous methods like Cut-Mix . Moreover, by allowing subjects to overlap during Seg-Mix, which is different from Cut-Mix, we prevent attributes from leaking to neighboring identities and enhance interactions among the subjects, which we analyze in Appendix B.2.

Descriptive classIntuitively, for two similar subjects in the same category, separating them solely with unique identifiers is a challenging task and prone to identity mixing. In single-subject personalization, Chae et al.  observed that adding detailed descriptions in front of the class nouns helps in capturing the visual characteristics of rare subjects. Inspired by this observation, we adopt

Figure 3: **Overview of MuDI**. (a) We automatically obtain segmented subjects using SAM  and OWLv2  in the preprocessing stage. (b) We augment the training data by randomly positioning segmented subjects with controllable scales to train the diffusion model \(_{}\). We refer to this data augmentation method as Seg-Mix. (c) We initialize the generation process with mean-shifted noise created from segmented subjects, which provides a signal for separating identities without missing.

specific class nouns (e.g., Weimaraner instead of dog) or add detailed descriptions in front of general class nouns (e.g., white robot toy instead of toy). Instead of manually selecting appropriate classes, we leverage GPT4-v  to automatically obtain these specific class nouns or descriptions. We empirically validate that this simple modification improves the preservation of the details for multiple subjects leading to the decoupling of the identities of highly similar subjects.

### Inference

It has been observed that initial noises for the generation affect the overall quality of generated images [27; 44], a finding that holds for personalized models with Seg-Mix as well. Motivated by this observation, we propose a novel inference method to improve identity decoupling without additional training or computational overhead. As illustrated in Figure 3(c), we first create an image \(_{init}\) of segmented subjects following Seg-Mix and extract its latent embedding from VAE encoder \(\). We then add this latent embedding to a random Gaussian noise \(\), scaled by a coefficient \(\) as \(_{T}=((_{init}) Resize(_{init}))*+ {}\) where \(Resize(_{init})\) denotes the resized version of segmentation mask \(_{init}\). This _mean-shifted_ noise \(_{T}\) encodes coarse information about the subjects and their layout, serving as a good starting point in sample generation. We analyze the effect of coefficient \(\) in Appendix B.3 and validate the diversity of generated images from the initialization in Appendix B.4. The proposed inference method is summarized in Algorithm 2. Additionally, instead of using randomly composed initial latent, we explore utilizing Large Language Models (LLMs)  to generate the layouts of bounding boxes for each subject aligned with the given prompt. Such an LLM-guided initialization enhances the ability to render complex interactions between subjects (see Figure 24 in Appendix B.5 for supporting results).

We remark that our initialization method also addresses the issue of subject dominance , where certain subject dominates the generation while other subjects are ignored. By providing information through the initial composition, our inference method guides the model to consider all subjects without additional computation. In Section 5.3 and Appendix B.6, we validate that our inference method alleviates subject dominance, playing a crucial role when rendering many subjects simultaneously.

### New metric for multi-subject fidelity

Existing metrics designed for measuring subject fidelity, such as CLIP-I  or DINOv2 , are not suitable for evaluating multiple subjects because they do not account for identity mixing. Therefore, we introduce a new metric, called _Detect-and-Compare_ (D&C), for evaluating multi-subject fidelity.

First, we utilize OWLv2  to detect the subjects in the generated image, with text queries as the supercategories of the subjects. For the detected subjects \(\{B_{i}\}_{i=1}^{N}\) and the reference subjects \(\{R_{j}\}_{j=1}^{M}\), we construct similarity matrices by measuring the similarities between the subjects using subject fidelity metrics such as DreamSim  or DINOv2 . Specifically, we first construct the _D&C similarities_ matrix \(^{DC}\), where \(ij\)-th entry represents the similarity between detected subject \(B_{i}\) and reference \(R_{j}\) (see Figure 4). Similarly, we construct the _ground-truth similarities_\(^{GT}\), where \(ij\)-th entry represents the similarity between reference objects \(R_{i}\) and \(R_{j}\). Since a mixed-identity subject yields high similarities to multiple references, we compare \(^{DC}\) and \(^{GT}\) to account for identity mixing. Notably, the difference between \(^{DC}\) and \(^{GT}\) yields a matrix where diagonal entries denote similarities to the corresponding subject, while off-diagonal entries indicate similarities to other subjects which represent identity mixing. The closer \(^{DC}\) is to \(^{GT}\), the more accurately

Figure 4: **(Left) Overview of Detect-and-Compare**. We calculate the mean similarities between detected subjects and reference images to evaluate multi-subject fidelity. Specifically, we compare \(^{GT}\) and \(^{DC}\). We provide pseudo-code in Algorithm 3. **(Right) Correlation between metrics and human evaluation**. We report the Spearman’s rank correlation coefficient and AUROC.

[MISSING_PAGE_FAIL:6]

Implementation detailsFor all experiments, we use Stable Diffusion XL (SDXL)  as the pre-trained text-to-image diffusion model and employ a LoRA  with a rank of 32 for U-Net  module. We also present experiments with other Stable Diffusion models  in Appendix B.12 and B.13. For all methods, we pair the reference images with comprehensive captions obtained through GPT-4v  which effectively mitigates overfitting to the background and shows better text alignment. We evaluate 400 generated images for each method, across 8 combinations with 5 evaluation prompts and 10 images of fixed random seeds. We provide more details in Appendix A.1.

BaselinesWe evaluate our method against multi-subject personalization methods: _DreamBooth_, DreamBooth with region control , DreamBooth using Cut-Mix  augmentation, namely _Cut-Mix_, and _Textual Inversion_. Note that we exclude Custom Diffusion  from the baselines due to its low quality when applied to SDXL (see Appendix B.1). For both Cut-Mix and Seg-Mix, we use a fixed augmentation probability of 0.3, and we do not use Unmix regularization  as it degrades the image quality for SDXL (see Appendix B.10). We describe further details in Appendix A.1.

### Main results

Qualitative comparisonAs shown in Figure 5, our approach successfully generates the subjects avoiding identity mixing, even for similar subjects such as two dogs (2nd column). On the contrary, DreamBooth results in mixed identities, and using region control proves ineffective for separating identities, as it seldom succeeds and frequently fails. Cut-Mix also falls short of decoupling the identities while producing stitching artifacts. Textual Inversion fails to preserve the subjects' details.

Human evaluationWe conduct human evaluations to assess the quality of images generated by the baselines and our method. We first ask human raters to evaluate the multi-subject fidelity via binary feedback. Additionally, we provide reference images of each subject along with two anonymized images: one from MuDI and the other from Cut-Mix. Human raters are asked to indicate which one is better, or tie based on three criteria: (1) similarity to the subjects in the reference images, (2) alignment with the given text, and (3) overall image fidelity. We provide more details in Appendix A.4.

Figure 6: **(Left) Human evaluation results on multi-subject fidelity and overall preference. (Right) Quantitative results on multi-subject fidelity and text fidelity. \(\) denotes the text fidelity score considering the permutation of the subjects in the prompt to avoid position bias.**

Figure 7: **Ablation Studies** on MuDI. While our method successfully personalizes Corgi and Chow Chow, ablating Seg-Mix results in mixed identity dogs. Inference without our initialization generates images of the subject missing. Training without descriptive class fails to catch subject details.

As shown in Figure 6 (Left), MuDI significantly outperforms prior works in multi-subject fidelity, achieving twice the success rate in preventing identity mixing compared to Cut-Mix. Due to this, raters strongly prefer images generated by MuDI in side-by-side evaluations. These results confirm that MuDI effectively decouples the identities of highly similar subjects without stitching artifacts.

Quantitative resultsWe evaluate multi-subject personalization methods on two key aspects: _multi-subject fidelity_, which measures the preservation of subject details for multiple subjects, and _text fidelity_, which assesses how well the generated images align with the given text prompt. We use our D&C scores to evaluate multi-subject fidelity. For text fidelity, we report the results of _ImageReward_ and CLIP score (_CLIPs_) . To avoid position bias, we calculate scores for the two different orders and average them, for example "[\(V_{1}\)] and "[\(V_{2}\)] and "[\(V_{1}\)]."

As shown in the Table of Figure 6 (Right), our framework achieves the highest scores in both multi-subject and text fidelity, significantly outperforming previous methods. These results are consistent with qualitative assessments and human evaluations, where MuDI preserves subject details effectively without identity mixing, unlike prior methods. The superior text fidelity also indicates that our method generates images that closely follow the given prompt without mixing the subjects.

### Ablation studies

Necessity of Seg-MixTo validate that Seg-Mix is crucial for decoupling the subjects' identities, we compare MuDI against its variant without it. As shown in Table 1, ablating Seg-Mix results in low multi-subject fidelity due to identity mixing. Figure 7 demonstrates that the attributes of the Corgi and Chow Chow are completely mixed without Seg-Mix. In particular, we show in Figure 26 that using additional spatial conditioning, e.g., ControlNet , without Seg-Mix still suffers from identity mixing.

    &  \\  Method & D\&C-DS\(\) & D\&C-DINO\(\) \\  w/o Seg-Mix & 0.475 & 0.481 \\ w/o Initialization & 0.477 & 0.480 \\ w/o Desc. Class & 0.556 & 0.558 \\  MuDI (Ours) & **0.637** & **0.610** \\   

Table 1: Results on ablation studies.

Figure 8: **Personalizing more than two subjects.** (a) MuDI successfully personalizes more than two subjects without identity mixing. (b) Success rates when varying the number of subjects.

Figure 9: **Diverse backgrounds generated by MuDI. Our Seg-Mix does not have a bias with backgrounds due to the training prompt ”A photo of [\(V_{1}\)] and [\(V_{2}\)], simple background”. (a) Inference with simple prompts. (b) Inference with various places. (c) Inference with various styles.

Importance of our initializationWe show in Figure 7 that the inference initialization improves identity separation and alleviates subject dominance. Table 1 validates that images generated without initialization result in lower subject fidelity. We empirically find that our initialization provides significant benefits in three scenarios: (1) personalizing unusual subjects that pre-trained models struggle to generate (e.g., the cloud man of Figure 1 bottom-right), (2) personalizing more than two subjects, and (3) using complex prompts, which we explain in detail in Appendix B.6.

Descriptive classWe show in Figure 7 that using descriptive classes to represent the subjects improves the preservation of the subjects' detail, and Table 1 further shows that this method enhances subject fidelity. Despite the improvement, relying only on descriptive classes may occasionally lead to some subjects being ignored. This is effectively addressed by applying our initialization which results in significantly improved outcomes.

More than two subjectsFigure 8(b) shows the success rates of MuDI, DreamBooth , and Cut-Mix  as the number of subjects varies. Our method achieves significantly high success rates, while previous approaches  fail to personalize even two subjects effectively. In particular, our method shows over 50% success for generating four objects together (see Figure 8(a)). However, we observe that the performance of MuDI decreases as the number of personalized subjects increases, particularly for highly similar subjects. We provide further details in Appendix B.9.

Diverse backgroundAs shown in Figure 9, our Seg-Mix does not have a bias with white backgrounds and can generate diverse backgrounds. This is because the prompt "A photo of [\(V_{1}\)] and [\(V_{2}\)], simple background" is used during training for the image of segmented subjects composed on a white background. This effectively disentangles the background from the identities through the text "simple background", preventing overfitting.

Model agnosticNotably, MuDI is a model-agnostic personalization method as it is based on data augmentation during training that does not require model-specific techniques, such as utilizing attention maps  or choosing where to fine-tune . We validate this by using FLUX  as the pre-trained text-to-image model based on DiT , which is different from SDXL  based on UNet  backbone. As shown in Figure 10 top row, DreamBooth produces mixed identity teddy bears while MuDI successfully generates distinct bears without identity mixing. We show in Figure 10 bottom row that MuDI can personalize multiple subjects using FLUX in diverse backgrounds.

### Other use cases

Controlling relative sizeOur framework offers an intuitive way to control the relative size between the personalized subjects. By resizing the segmented subjects according to user intents in Seg-Mix,

Figure 10: **Images generated using FLUX  as a pre-trained text-to-image diffusion model**. (Top row) DreamBooth produces mixed-identity teddy bears while MuDI generates distinct bears. (Bottom row) MuDI can personalize many subjects without identity mixing on diverse backgrounds.

we find that personalized models generate subjects with the desired relative sizes. This showcases another benefit of our method unlike previous methods [42; 21; 15], which often result in inconsistent relative sizes due to a lack of size information during fine-tuning. As shown in Figure 11(a), our method allows the model to be personalized to generate either a larger dog compared to the toy or vice versa, by setting their relative sizes during Seg-Mix. The generated images show a consistent relative size which we provide more examples in Figure 34. Additionally, controlling the relative size of the segmented subjects during inference initialization can further improve the size consistency.

Modular customizationThe proposed Seg-Mix can also be applied to modular customization, where the subjects are independently learned in advance by single-subject LoRAs . We then efficiently combine these LoRAs to generate multi-subject images. To integrate Seg-Mix with modular customization, we first generate images for each subject using their respective single-subject LoRA, which serve as reference images. Next, we merge the single-subject LoRAs using an existing method such as gradient fusion . After merging, we apply Seg-Mix with the generated single-subject images for 200-300 iterations. This approach effectively reduces identity mixing and avoids the need for training from scratch by reusing the single-subject LoRAs. We illustrate the process of using Seg-Mix in Figure 35.

Figure 11(b) shows samples generated by gradient fusion , a modular customization method, applied to two-subject personalization. Without spatial conditioning, it produces mixed identities for the characters of the otter and the monster (left). However, if we fine-tune fused model with Seg-Mix only for a few iterations, the fine-tuned model produces a high-quality image of clearly separated subjects (Figure 11(b), right). We note that it is important to incorporate Kullback-Leibler (KL) divergence as regularization  in fine-tuning in order to prevent saturation and overfitting.

### Iterative training

To further improve the quality, we investigate a fully automatic iterative training (IT) method , which fine-tunes the personalized model using high-quality samples obtained from an earlier training stage. Specifically, we first generate multi-subject images with MuDI and select high-quality images based on the D&C score, which closely aligns with the human evaluation. These selected images are then used to fine-tune the personalized model, with KL regularization  added to Eq. (2). By applying IT to the images of Corgi and Chow Chow, the D&C-DS score is improved from 0.613 to 0.672, achieving a higher success rate (see Figure 37). We provide further details in Appendix C.3.

## 6 Conclusions

In this work, we present MuDI, a novel personalizing framework for multiple subjects that addresses identity mixing. We leverage segmented subjects automatically obtained from the foundation model for image segmentation for both training and inference, through data augmentation for training pre-trained models and initializing the generation process. We experimentally validate our approach on a new dataset comprising combinations of subjects prone to identity mixing, for which ours successfully prevents mixing even for highly similar subjects. We describe the limitations and societal impacts of our work in Appendix D. We hope that our work can serve as a starting point to develop personalizing methods for multiple concepts in more challenging scenarios.

Figure 11: Examples of other use cases of our method. **(a) Controlling relative size with Seg-Mix.** We visualize samples generated by MuDI using size-controlled Seg-Mix. **(b) Modular customization.** Applying Seg-Mix after merging LoRAs significantly improves identity decoupling.

Acknowledgements

We thank Juyong Lee, and Jaewoo Lee for providing valuable feedback. This work was supported by National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00256259), Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (No.RS-2019-III90075 Artificial Intelligence Graduate School Program(KAIST)), Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No. RS-2024-00509279 Global AI Frontier Lab), Artificial intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT(MSIT, Korea)&Gwangju Metropolitan City, and KAIST-NAVER Hypercreative AI Center.