# gimlet: A Unified Graph-Text Model for

Instruction-Based Molecule Zero-Shot Learning

 Haiteng Zhao\({}^{1}\), Shengchao Liu\({}^{2}\), Chang Ma\({}^{3}\), Hannan Xu\({}^{4}\),

**Jie Fu\({}^{5}\), Zhi-Hong Deng\({}^{1}\), Lingpeng Kong\({}^{3}\), Qi Liu\({}^{3}\)**

\({}^{1}\) Peking University \({}^{2}\) Mila \({}^{3}\) The University of Hong Kong

\({}^{4}\) University of Oxford \({}^{5}\) Hong Kong University of Science and Technology

###### Abstract

Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose gimlet, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. gimlet also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset consisting of more than two thousand molecule tasks with corresponding instructions derived from task descriptions. We pretrain gimlet on the molecule tasks along with instructions, enabling the model to transfer effectively to a broad range of tasks. Experimental results demonstrate that gimlet significantly outperforms molecule-text baselines in instruction-based zero-shot learning, even achieving closed results to supervised GNN models on tasks such as toxcast and muv. 1

## 1 Introduction

Molecule machine learning has gained significant attention in recent years [10; 52; 85; 77], including tasks like property prediction [14; 21], molecule design [81; 35; 28; 27], and others, which have broad applications in biomedical research. The primary method of molecule tasks is graph machine learning , where graphs are employed to represent molecule topology. Presently, graph machine learning approaches for molecules mainly follow the pretraining and finetuning paradigm [26; 60; 40; 84]. After the pretraining on large molecule corpus, models are able to encode informative molecule representations and generalize to downstream tasks by supervised finetuning.

One limitation of the supervised finetuning approach is the requirement on labeled data to acquire task-specific features for prediction on downstream tasks. This requirement poses a challenge in scenarios where obtaining labeled data is arduous or costly, especially in the field of molecules, and the training cost also restricts the efficiency of downstream tasks. Additionally, the supervised method exclusively relies on labeled data for acquiring task information, and is therefore unable to incorporate other additional information about the task. For instance, there exists a wealth of information regarding molecule assay tasks often provided in textual descriptions. Unfortunately, the supervised method fails to leverage such information, thereby restricting its flexibility.

In this work, we propose to investigate the feasibility of employing instructions to accomplish molecule-related tasks in a zero-shot setting. The instructions, also referred to as prompts , constitute natural language descriptions of the task to be executed. This is inspired by recent progress in natural language processing, where the large language models possess strong generalization performance to unseen tasks by following instructions . This approach eliminates the need for labeled data and leverages the textual knowledge available for downstream tasks.

Several studies have explored the molecule-text language models, employing either autoregressive pretraining using language models on both the text and molecule strings (SMILES) , or contrastive pretraining of molecule and text where molecules are encoded by Graph Neural Networks (GNN) . However, these works lack the investigation into instruct-based zero-shot for downstream tasks, and our experiments have empirically demonstrated their limited performance.

We conjecture that the constraints of current molecule-text language models are mainly imposed by their inadequate treatment of instructions during pretraining and their limited capacity to represent molecule structures. First, the pretraining corpora of these models lack task description about descriptions for abundant molecule tasks as well as the supervised signal. They mainly address the capacity of general molecule-related text understanding, which may be insufficient to capture details of complex instructions to molecule properties. Second, these models' capacity is limited in representing graph structures of molecules. Molecular tasks heavily rely on understanding the structure information, while existing multimodal methods either encode SMILES sequence representation of molecules by language model or encode molecular graphs into dense vector representation using GNN, which are not conducive for language models to gain a deep understanding of graph features.

To facilitate natural language instruction-based graph learning for zero-shot molecule tasks, we propose an innovative structured language model called gimlet (**G**raph **I**nstruction based **M**olec**u**Le **z**E**ro-**sho**T** learning). First, gimlet extends large language models for both graph and text data by applying transformer mechanisms with generalized position embedding, ensuring strong capacity for both the learning of graph structure representations and the executing of instruction texts without introducing additional graph encoding modules. We further enhance the model by decoupling the encoding of the graph from specific tasks via employing attention masks. This allows for improved generalization of graph features across novel tasks.

In addition to the advanced model architecture, our approach incorporates instruction-based pretraining on an extensive dataset of molecule tasks with textual instructions. We construct a dataset comprises of 2K tasks accompanied by corresponding instructions tailored for instruction-based molecule zero-shot learning framework. Throughout the pretraining process, molecule graphs are paired with natural language instructions for molecule-related tasks, and the supervision signal is provided to train the model in executing various molecule tasks. This methodology empowers gimlet to comprehend specific instructions for downstream tasks expressed in natural language and transfer acquired knowledge to a broad range of tasks effectively.

Our experimental results show that gimlet outperforms molecule-text baselines by a substantial margin in instruction-based zero-shot learning, which is closed to supervised GNN on tasks like muv and toxcast. Additionally, gimlet also exhibits impressive performance in few-shot learning and demonstrates robustness to instruction. In summary, our contributions are as follows:

* We present comprehensive investigations of natural language instruction-based graph zero-shot learning for molecule tasks, to reduce reliance on labeled data and leverage task textual knowledge. To accomplish this framework, we construct a molecule dataset consisting of two thousand tasks with instructions derived from task descriptions, which is open-sourced.
* We propose gimlet, which extends language models to handle graph and text data. By applying the transformer mechanism with generalized position embedding and decoupled attention, our model learns graph structure representations and executes instruction texts without additional graph encoding modules. Instruction-based pretraining is applied for gimlet to comprehend specific instructions expressed in natural language and transfer to a broad range of zero-shot tasks.
* Our experiment results outperform other molecule-text models by a substantial margin and demonstrate promising results in instruction-based zero-shot molecule tasks, demonstrating the viability of this framework. Additionally, gimlet exhibits impressive performance in few-shot learning and demonstrates robustness to instruction.

## 2 Related Work

**Molecule Representation Learning** One approach for molecular representation learning is utilizing language models to acquire molecular representations based on Simplified Molecular Input Line Entry System (SMILES) strings [68; 10]. Toward stronger capability to incorporate pertinent substructure information, Graph Neural Networks (GNNs) are proposed to model molecules as graphs [21; 83; 26; 36]. Existing GNNs follow the message-passing paradigm and suffer from problems like long-range dependency vanishing and over-smoothing. Recently, graph transformer [52; 80] has been proposed to better encode structures of graphs, illustrating effectiveness in molecule tasks [42; 31; 8; 88].

**Molecule Pretraining** To fully explore the inherent structural information of molecules on a large scale, significant efforts have been made to molecular pretraining. Supervised pretraining is commonly used for learning useful representations [26; 80; 62]. As for unsupervised pretraining, one approach involved the generative strategy on molecular SMILES strings [68; 25; 10; 3; 53] and graph [26; 37; 38; 52; 87], which was followed by recent works adopting the contrastive paradigm to distinguish augmented views of the same graph and other graphs [67; 60; 24; 83; 82; 63; 78; 19; 61; 70; 76; 69; 40].

Besides the structure-only pretraining, a few recent works incorporate natural language into molecule pretraining. One class of method is the SMILES-based language model, including KVPLM  and MoIT5 , which use SMILES strings and text for joint representation and translation. Another work Galactic  explored the multi-task molecule task learning with instruction. Some other works acquire advanced representations for molecules by GNN, such as Text2Mol , MoMu , MoleculeSTM , and CLAMP , trained by contrastive learning between molecule graph and text description for molecule retrieval and caption tasks. MoleculeSTM and CLAMP explored molecule editing and property prediction with text. However, previous works lack the investigation into instruct-based zero-shot scenarios for complex molecule tasks like property prediction.

**Instruction-based Zero-Shot Learning** Instruction-based zero-shot learning is an innovative approach that leverages natural language instructions to enable neural models to solve a variety of tasks [50; 6; 55; 18; 89; 44; 45; 49]. To enhance the model's ability to follow instructions, some researchers have employed instruction-based pretraining techniques [54; 71; 12; 47], which explicitly train language models to solve tasks with instructions. Besides natural language processing, instruction-based zero-shot learning is also studied in multimodal domains like images [4; 9; 1; 32].

## 3 Method

### Problem Formulation and Framework Overview

The molecule task is denoted as \(\) and consists of a set of graph data and their corresponding labels \((G_{i},y_{i}^{})\). A molecule graph \(G=(N,E,v,e)\) includes nodes \(N=\{1,,n\}\) and edges \(E N N\), while \(v\) and \(e\) represent node features and edge features, respectively. The label \(y^{}\) can take various

Figure 1: Our framework handles molecule tasks in the zero-shot fashion by natural language instruction. Within gimlet, we employ distance-based joint position embedding to encode graphs and instruction texts. Additionally, we utilize attention masks to decouple the graph encoding process.

forms, such as class labels or numerical values, depending on the type of task. The instruction of \(\) is denoted as \(T^{}\), which is a sentence \([o_{1},,o_{m}]\) describing the task.

In the supervised approach, a model \(F_{}\) predicts the label \(\) given a graph \(G\), i.e., \(=F_{^{}}(G)\), by supervised finetuning individually for each downstream task \(\). The limitation is that it relies on labeled data to learn the corresponding parameters and output modules, and finetuning does not enable the model to effectively utilize extra task-related knowledge.

To overcome these limitations, our framework leverages natural language instructions \(T^{}\) to provide the model with task information, as illustrated in Figure 1. Our zero-shot graph learning framework incorporates molecule graphs \(G\) and task instructions \(T^{}\) into a graph-text language model gimlet and decodes the output as text uniformly for different tasks, i.e. \(_{}=(G,T^{})\), where \(_{}\) is the label string. This description-based instruction framework empowers the model to handle a wide range of molecule tasks as long as they can be described in natural language, and the uniform text decoding approach accommodates various types of tasks including classification or regression.

Previous pretrained molecule-text models perform poorly in our framework due to the inadequate treatment of instructions and limited capacity for graphs. Our method addresses these from two aspects: First, we propose a unified language model gimlet for both graph and text, towards the stronger capacity to represent molecules and instructions; Second, we adopt instruction-based pretraining to gimlet, enabling generalization to new tasks based only on the instructions.

### Unified Graph-Text Transformer

The common method for multimodal language models is obtaining the feature of the other modality by applying an additional encoding module, then integrating it into the language model, as in other molecule language models with GNN encoder [17; 59; 56]. The individual encoding module benefits for decoupling the graph feature encoding from instructions, i.e., the features of graph data can be independent of task instructions in the early encoding stage, helping for generalization when the distribution of tasks changes.

However, for the molecule-text model, individual pre-encoding modules present problems. First, graph learning relies on structure information, but the dense vectors encoded by GNN have a limited capacity to carry structure information. Furthermore, training the additional module is difficult due to the increased layers, since deep transformers have vanishing gradients in early layers [34; 2]. Lastly, the additional modules increase parameters and training costs.

To overcome these issues, we propose a novel approach gimlet which not only directly unifies the standard language model for graph and text _without_ introducing additional graph encoder module, but also remains the decoupled graph encoding for better generalization.

Given graph \(G\) and text input \(T\), to utilize gimlet, we represent the graph nodes and text tokens as tokens. The resulting hidden states are denoted as \(H=[h_{1},,h_{n},h_{n+1},,h_{n+m}]^{T}^{(n+m) d_{h}}\) for corresponding \(n\) graph nodes and \(m\) text tokens.

In this study, we choose T5  as the backbone language model, due to the encoder-decoder architecture suitable for non-sequential encoding and text output. It utilizes the relative position embedding method  to represent sequential structure. In attention layer \(\) with parameter \(W^{V},W^{Q},W^{K}^{d_{h} d_{k}}\) and \(W^{O}^{d_{k} d_{h}}\), relative position embedding for i-th and j-th token is formalized as

\[_{ij}=W^{Q})(h_{j}W^{K})^{T}}{ {d_{k}}}+b(i,j),A=(),(H)= AHW^{V}W^{O},\] (1)

where \(b(i,j)\) is embedding of the relative distance between \(i\) and \(j\), i.e. \(i-j\) for sequence. For graph-text joint data, we construct the position embedding \(b(i,j)\) in Eq. 1 by the conjunction of different types of distances. For the relative position of graph nodes, we adopt the graph shortest distance, which has been widely used in the literature of graph transformer [80; 11; 48]. We also use unidirectional constraint in attention to decouple the graph encoding from the instruction. The overall form of position embedding for gimlet is:

\[b(i,j)=b^{D}_{(i,j)}+b^{M}_{i,j}+*{Mean}_{k(i,j)}b^{E}_{e_{k}},\] (2)

where \(b^{D}\), \(b^{M}\), and \(b^{E}\) are position bias, masking, and path embedding, individually. The relative position POS in \(b^{D}_{(i,j)}\) is defined as the conjunction of different types of distances between tokens,which allows for the effective encoding of both graph and text data, as well as their interaction:

\[(i,j)=i-j&n+1 i,j n+m\\ (i,j)&1 i,j n\\ <&,\] (3)

where <Cross> is a special distance token held out for cross distance between graph and text tokens. \(b^{M}_{i,j}\) aims to represent the cross mask used to decompose graph encoding from text instructions. It imposes a unidirectional constraint from the graph to the text:

\[b^{M}_{i,j}=-i nj>n0\] (4)

With the unidirectional constraint, graph tokens are limited to attending only to other graph tokens. On the other hand, instruction tokens have the ability to receive information from both instructions and graphs. This approach allows us to separate the encoding of graphs from instructions, enabling instructions to selectively utilize graph features for various downstream tasks.

Finally, \(_{k(i,j)}b^{E}_{e_{k}}\) is the mean pooling of the edge features \(b^{E}_{e_{k}}\) in the shortest path \((i,j)\) between node \(i\) and \(j\), which is only defined between graph node tokens, as used along with graph shortest distance in .

The generalized position embedding is applied to the encoder transformer. During decoding, the decoder generates the answer based on the text features outputted by the encoder.

gimlet unifies graph and text data by a single language model, which has the following merits: **(i)** In comparison to additional encoding module methods, gimlet not only avoids the challenges of training additional front layers but also provides a stronger capacity for handling graph data by introducing graph inductive bias to the whole transformer. **(ii)** Our method leverages both the existing knowledge within the language model and facilitates learning to execute instruction-based graph tasks through standard instruction-based learning on the language model. This approach eliminates the need for additional modeling costs. **(iii)** The decomposed encoding of graph data retains the advantages of the individual encoding module, reducing task disturbance and ensuring that data features remain independent of task instructions. This enhances generalization for novel instructions. We validate these claims in experiments Subsection 4.3.

### Pretraining and Datasets

**Pretraining** Our approach leverages the generalization capabilities acquired through learning from the instructions provided during pretraining, where comprehensive linguistic information and knowledge related to molecular tasks are learned from the provided instructions. The pretraining process is

Figure 2: (Left) Illustration of datasets. Circle size corresponds to task number. Tasks are organized by category. Tasks on the top are more related to biological assay, on the bottom need more chemical and physical properties. gimlet is trained on pretraining tasks, then tested on downstream tasks in the zero-shot setting. (Right) Our task instructions contain task explanations and questions.

conducted in a supervised manner using task labels. The loss function for supervised pretraining can be formalized as follows:

\[L=||}_{}_{(G_{i},y^{}_{str,i})}- _{str,i}|} P_{}(y^{}_{str,i}|G_{i},T^{ }),\] (5)

where \(P_{}\) is the model likelihood for the label string, \(|y^{}_{str,i}|\) represents the label string token number aiming to normalize the loss of long sequences, and \(||\) is the task sample number. The details of pretraining and downstream zero-shot testing are in Appendix.

**Pretraining Dataset** To effectively pretrain gimlet for downstream tasks, it is crucial to include a large number of tasks to provide the model with an extensive task corpus. To this end, we select Chembl  as the pretraining dataset, which is widely used for supervised graph pretraining [26; 62]. It consists of 1,310 prediction target labels from biological assays for drug discovery. We divided \(80\%\) of the tasks and \(80\%\) of the molecules in random for pretraining, while the remaining non-overlapping tasks were reserved for zero-shot testing. Additionally, we constructed the Chembl-property dataset, which encompasses various physical and chemical properties available in the Chembl database  like molecule weights, log p values, hydrogen bound acceptor numbers, etc. Full details are in Appendix. We validate the effect of Chembl biological tasks and Chembl-property physico-chemical tasks in pretraining in Subsection 4.3.

**Downstream Dataset** We target a large range of molecule tasks, as shown in Figure 2. First, we include large-scale datasets PCBA , which contains 128 biological assay tasks with abundant molecules. We also include the hold-out zero-shot set of Chembl, noted as Chembl Zero-Shot. These two datasets form a large-scale dataset. We also target tasks from MoleculeNet , a popular benchmark for molecule properties prediction. We adopt the dataset categorization method in  which classifies MoleculeNet datasets into four categories: Physico-chemical tasks, Bio-activity tasks, Toxicity tasks, and Pharmacokine tasks. Additional dataset CYP450  is also included in the Pharmacokine tasks. These tasks cover diverse aspects of molecule properties, posing a significant challenge for a unified model to simultaneously handle them in a zero-shot fashion.

**Instructions** To provide essential background information and context for each task, we include task explanations and descriptions in our instructions. The description covers a wide range of aspects, including the family, function, and mechanism of the assay target, the assay experiment setting, the approximation method used for determining the property, and others. See examples in Figure 2. Instructions for all tasks are available in the code file. The task explanation is primarily sourced from websites and databases that introduce and compile the respective datasets, or relevant papers. Details are in Appendix. The descriptions are then concatenated with relevant questions as instructions. These instructions are subsequently reviewed and validated by a professional biology Ph.D. student.

## 4 Experiment

In the experiments, we investigate the following inquiries: (**i**) Can gimlet effectively handle zero-shot molecule property tasks by instructions? (**ii**) Can gimlet performs better by few-shot learning? (**iii**) What impact does model architecture have on the performance of gimlet? (**iv**) How does pretraining affect the performance of gimlet? (**v**) How does the form of instruction influence gimlet for molecule zero-shot learning?

### Instruction-Based Zero-Shot Learning

**Baselines** In the zero-shot setting, we compare gimlet with three molecule-text models: SMILES-base language model KVPLM , Galactica , and GNN-language model MoMu . Other molecule-text models [16; 17; 39] either haven't released parameters or are difficult to handle zero-shot setting. Notably, the pretraining of Galactica includes the MoleculeNet datasets, which is thus not strictly zero-shot on some of our tasks. We report the result of all the baselines with our zero-shot learning framework and instructions. The details of baseline evaluation are in Appendix.

To establish upper bounds for task performance, we also illustrate the supervised results of popular graph models. For GNNs, we includes GCN , GAT , and GIN . We also include the graph transformer Graphormer . To mitigate the impact of our pretraining, we additionally perform supervised pretraining on Graphormer using our pretraining datasets, referred to as Graphormer-p.

**Settings** Following the standard supervised setting in previous studies , we adopt the Scaffold split  with a ratio of 0.8, 0.1, 0.1 for all the datasets, and report results on the testing sets, ensuring the comparability of our results to previous works. For classification tasks, we employ ROC-AUC as the evaluation metric, while for regression tasks, we utilize RMSE.

**Results** We report the result of different types of downstream tasks in Table 1. The result of GIN, GCN and GAT for MoleculeNet are from  which we mark by italic. We observe that in the zero-shot setting, gimlet outperforms most baselines on the majority of datasets, except for bbbp where gimlet also performs comparably to the baselines. In terms of the average performance across task categories, gimlet outperforms all baselines, demonstrating the effectiveness of our method for instruction-based zero-shot molecule tasks. It is worth noting that some of the baselines also achieve results on certain tasks. For example, KVPLM works on hiv, muv and bbbp, and MoMu works on tox21 and bace, showing our instruction-based molecule zero-shot learning method is a general framework to probe knowledge in molecule-text models.

Comparing our zero-shot performance to the supervised results, we observe that gimlet achieves performance close to those of GNNs on several datasets, like bace, muv, toxcast, and bbbp. This demonstrates that gimlet is able to solve molecule tasks in the zero-shot fashion nicely.

The results for large-scale molecule tasks are presented in Table 2. As depicted, the baselines struggle to handle these tasks. Our gimlet not only successfully transfers to the Chembl Zero-Shot splits, where both the tasks and graphs were unseen during pretraining, but also demonstrates strong generalization performance on the PCBA benchmark.

The results of regression tasks are shown in Table 3. The scatter plots of the regression are in Appendix. It is worth noting that regression tasks pose greater challenges in the zero-shot setting than classification tasks, because it is difficult to determine unseen physico-chemical properties using only natural language, and the output space is also vast. Notably, the zero-shot baselines fail to perform the regression tasks due to their inability to output correctly formatted numbers. In contrast, gimlet generates correctly formatted numbers for over 98% regression testing samples in all the tasks and showcases the potential of zero-shot regression tasks.

### Instructions-Based Few-Shot Finetuning

We apply few-shot instruction-based tuning on the downstream tasks, to examine whether gimlet exhibits improved performance in the presence of low-resource data. Notably, for datasets with more than one task, we do few-shot learning for every single task individually. We first split datasets into training, validation, and testing sets in the same as the zero-shot setting. Then \(K\) samples for each class are randomly sampled from the training set as the few-shot examples, where \(K\) is the few-shot number. We report the result of the best validation model on the testing set. The input is the

  Method & \#Param & Type & bace & hiv & muv & Avg. bio & tox21 & toxcast & Avg. tox & bbbp & cy450 & Avg. pha \\  KVPLM & 110M & & 0.5126 & 0.6120 & 0.6172 & 0.5806 & 0.4917 & 0.5096 & 0.5007 & 0.6020 & 0.5922 & 0.5971 \\ MoMu & 113M & & 0.6656 & 0.5026 & 0.6051 & 0.5911 & 0.5757 & 0.5238 & 0.5498 & 0.4981 & 0.5798 & 0.5390 \\ Galactica-125M & 125M & Zero Shot & 0.4451 & 0.3671 & 0.4986 & 0.4369 & 0.4964 & 0.5106 & 0.5035 & **0.6052** & 0.5369 & 0.5711 \\ Galactica-13B & 1.3B & & 0.5648 & 0.3385 & 0.5715 & 0.4916 & 0.4946 & 0.5123 & 0.5035 & 0.5394 & 0.4686 & 0.5040 \\ gimlet(Ours) & 64M & & **0.6957** & **0.6624** & **0.6439** & **0.6673** & **0.6119** & **0.5904** & **0.6011** & 0.5939 & **0.7125** & **0.6532** \\  GCN & 0.5M & & _0.736_ & _0.757_ & _0.732_ & _0.742_ & _0.749_ & _0.633_ & _0.691_ & _0.649_ & 0.8041 & 0.7266 \\ GAT & 1.0M & & _0.697_ & _0.729_ & _0.666_ & _0.697_ & _0.754_ & _0.646_ & _0.700_ & _0.662_ & 0.8281 & 0.7451 \\ GIN & 1.8M & Supervised & _0.701_ & _0.753_ & _0.718_ & _0.724_ & _0.740_ & _0.634_ & _0.687_ & _0.658_ & 0.8205 & 0.7392 \\ Graphormer & 48M & & 0.7760 & 0.7452 & 0.7061 & 0.7424 & 0.7589 & 0.6470 & 0.7029 & 0.7015 & 0.8436 & 0.7725 \\ Graphormer-p & 48M & & 0.8575 & 0.7788 & 0.7480 & 0.7948 & 0.7729 & 0.6649 & 0.7189 & 0.7163 & 0.8877 & 0.8020 \\  

Table 1: Zero-shot performance (ROC-AUC) over Bio-activity, Toxicity, and Pharmacokinetic tasks.

  Method &  &  &  &  &  \\  KVPLM & & - & - & - & - \\ MoMu & Zero Shot & & - & - & - \\ gimlet(Ours) & & 1.132 & 1.345 & 5.103 & **2.527** \\  GCN & & 1.331 & 0.760 & 2.119 & 1.403 \\ GAT & & 1.253 & 0.770 & 2.493 & 1.505 \\ GIN & & 1.243 & 0.781 & 2.871 & 1.632 \\ Graphormer & & 0.901 & 0.740 & 2.210 & 1.284 \\ Graphormer-p & & 0.804 & 0.675 & 1.850 & 1.110 \\  

Table 3: Zero-Shot performance (RMSE) on Physical-chemical datasets.

same as the zero-shot setting, including molecule data and instructions. We only tune the last linear layer, to inspect whether the feature is discriminative and linear separable. Linear tuning is also low resource costing and avoids overfitting. The last linear mapping to vocabulary is tuned for gimlet and KVPLM. For MoMu, we tune the last linear layer of the projection head for features.

The result is shown in Figure 3, and plots for each dataset are in Appendix. gimlet outperforms other baselines consistently, exhibiting improved performance with an increasing number of few-shot samples. The performance of gimlet also approaches the supervised GIN with only limited samples. Baselines also achieve some performance gain in the few-shot training but remain beaten by gimlet, and the improvements are not stable, fluctuating with the few-shot number. The few-shot results demonstrate the high-quality representation learned by gimlet, which is well-suited for specific tasks, highlighting the potential of gimlet in scenarios beyond zero-shot learning.

### Ablation and Exploration Studies

**Effect of Model Design** We investigate components of gimlet to highlight their benefits. Specifically, we focus on two aspects: **(a)** To measure the effectiveness of the unified transformer method, we compare it to the variant version which obtains graph embedding by an individual GIN as a token in text, a common method in multimodal transformers. **(b)** We ablate graph decoupling encoding by complete global attention. We conduct pretraining and downstream zero-shot testing in the same setting as our method.

The comparison is shown in Table 4. Compared to gimlet, w.o. unifying perform worse on all the datasets, especially on Bio-activity and Pharmacokine Tasks. Next, the w.o. decoupling variation performs worse than gimlet on most datasets. The decoupling significantly improves performance on Bio-activity and Pharmacokine tasks and is also comparable on Toxicity tasks. This proves our claims that the unified graph-text transformer not only avoids additional modules but also has a strong capacity for encoding graph data and generalizing across tasks.

**Effect of Pretraining** Our pretraining includes two large types of pretraining tasks, including Chembl bioactivity tasks and Chembl Property physico-chemical tasks. To validate the influence of each task type, we performed ablation experiments where each of them was excluded separately. The results are shown in Table 5, and the detailed result is in Appendix. Unsurprisingly, Chembl is essential for downstream molecule assay tasks, and Chembl property plays a crucial role in Physico-chemical tasks. However, the results also reveal that Chembl positively affects downstream Physico-chemical tasks, and Chembl property benefits Bio-activity tasks, Toxicity tasks, and Pharmacokine tasks. The results demonstrate the positive transfer of pretraining tasks on a diverse range of downstream tasks, spanning various types and domains.

**Robustness to Instruction** We explore whether gimlet is robust to the instructions. We rephrase our instructions by GPT-3.5-turbo for testing. Each instruction is rephrased using four types of requests:

  Method & bace & hiv & muv & Avg. bio & tox21 & toxcast & Avg. tox & bbbp & cyp450 & Avg. pha \\  w.o. unifying & 0.4319 & 0.6133 & 0.6067 & 0.5506 & 0.5922 & 0.5537 & 0.5730 & 0.5309 & 0.6206 & 0.5758 \\ w.o. decoupling & 0.6458 & 0.6406 & 0.5421 & 0.6095 & **0.6306** & **0.5954** & **0.6130** & 0.5666 & 0.6320 & 0.5993 \\ gimlet & **0.6957** & **0.6624** & **0.6439** & **0.6673** & 0.6119 & 0.5904 & 0.6011 & **0.5939** & **0.7125** & **0.6532** \\  

Table 4: Ablation study on gimlet module.

Figure 3: Few shot performance. Higher is better for bio, tox, and pha, and lower is better for phy.

rewriting, detailing, expanding, and shortening. The prompts and examples are provided in Appendix. We plot the performance for each type of augmentation, as well as the average standard variation for each task in Figure 4. As shown, gimlet is more robust than baselines on most tasks. This shows that our instruction-based pretaining enables gimlet to focus on the task rather than the specific language form.

**Instruction Interpretability** We ablate the explanation of the instructions in downstream tasks, to validate whether the explanation in instructions helps gimlet perform downstream tasks. Without explanation, only the task name and question are provided to the model. The examples of ablated instructions are available in Appendix. The results presented in Table 6 demonstrate a significant drop in performance when task explanations are not included. This finding supports the effectiveness of the explanation and highlights the model's ability to comprehend the explanation of tasks.

## 5 Conclusion and Discussion

In this work, we propose nature language instruction-based graph zero-shot learning for molecule tasks, and construct a molecule dataset consisting of two thousand tasks with instructions derived from task descriptions. We propose gimlet, which extends large language models to handle graph and text data by applying the transformer mechanism with generalized position embedding and decoupled attention. Instruction-based pretraining is applied for gimlet. Experiments demonstrate promising results in zero-shot learning, exhibit strong robustness to the instruction, and can be further improved by few-shot tuning. We do not consider the tasks with structured output like molecule generation in this study, which is left for further work.

  Method & bace & hiv & muv & Avg. bio & tox21 & toxcast & Avg. tox & bbbp & cyp450 & Avg. pha \\  name only & 0.5416 & 0.6132 & **0.6441** & 0.5996 & 0.5809 & 0.5279 & 0.5544 & 0.4871 & 0.6669 & 0.5770 \\ \(\) explanation & **0.6957** & **0.6624** & 0.6439 & **0.6673** & **0.6119** & **0.5904** & **0.6011** & **0.5939** & **0.7125** & **0.6532** \\  

Table 6: Ablation study on gimlet instructions.

Figure 4: Robustness to instruction.

  Method & Avg. bio \(\) & Avg. pha \(\) & Avg. tox \(\) & Avg. phy \(\) \\  bioactivity assay only & 0.6402 & 0.6071 & 0.5676 & - \\ physico-chemical only & 0.4894 & 0.5454 & 0.4748 & 2.6178 \\ both & **0.6673** & **0.6532** & **0.6011** & **2.5266** \\  

Table 5: Ablation study on gimlet pretraining.

Ethical Consideration

The work presented here is centered on a paradigm shift in molecule property prediction tasks, transitioning from traditional supervised learning to instruction-based zero-shot learning. Due to the fact that molecule property prediction has been widely explored in prior research, the direct societal impacts may appear limited. However, indirect negative impacts could be caused by excessive reliance on algorithms. We contend that a combination of model predictions and experimental validation is essential for practical implementation.