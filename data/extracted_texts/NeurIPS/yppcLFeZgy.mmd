# MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering

Yizhen Luo\({}^{1,2,}\), Zikun Nie\({}^{1,2,}\), Massimo Hong\({}^{1,2}\), Suyuan Zhao\({}^{1,2}\), Hao Zhou\({}^{1,*}\), Zaiqing Nie\({}^{1,3,*}\)

\({}^{1}\)Institute of AI Industry Research (AIR), Tsinghua University

\({}^{2}\)Department of Computer Science and Technology, Tsinghua University

\({}^{3}\)Pharmolix Inc.

{yz-luo22,nzk24,hongcd21,zhaosy23}@mails.tsinghua.edu.cn

{zhouhao,zaiqing}@air.tsinghua.edu.cn

Equal contribution

###### Abstract

Studying protein mutations within amino acid sequences holds tremendous significance in life sciences. Protein language models (PLMs) have demonstrated strong capabilities in broad biological applications. However, due to architectural design and lack of supervision, PLMs model mutations implicitly with evolutionary plausibility, which is not satisfactory to serve as explainable and engineerable tools in real-world studies. To address these issues, we present MutaPLM, a unified framework for interpreting and navigating protein mutations with protein language models. MutaPLM introduces a protein _delta_ network that captures explicit protein mutation representations within a unified feature space, and a transfer learning pipeline with a chain-of-thought (CoT) strategy to harvest protein mutation knowledge from biomedical texts. We also construct MutaDescribe, the first large-scale protein mutation dataset with rich textual annotations, which provides cross-modal supervision signals. Through comprehensive experiments, we demonstrate that MutaPLM excels at providing human-understandable explanations for mutational effects and prioritizing novel mutations with desirable properties. Our code, model, and data are open-sourced at https://github.com/PharMolix/MutaPLM.

## 1 Introduction

Studying protein evolution through mutations within amino acid sequences is a central research topic in life sciences . Despite immense research efforts, a large number of protein mutations with biological significance remain under-explored, highlighting the demand for in-silico tools to model these mutations. Practically, the tool should meet two requirements. First, it should be **explainable**, providing insightful and human-understandable interpretations for mutational effects. This is crucial for broad biological applications ranging from identifying immune-escape pathogens  to interpreting the mechanisms of human diseases . Additionally, the tool should be **engineerable**, proposing protein mutations that satisfy desirable properties such as catalytic activity and thermostability. This process is known as directed evolution , the most prevailing approach for protein design in the laboratory, which offers substantial benefits across various application fields, including industry , biotechnology , and therapeutics .

To achieve these goals, deep learning models  have emerged to capture evolutionary information from protein sequences. Recently, the development of protein language models (PLMs)[16; 17; 18; 19; 20] has brought a paradigm shift in computational biology. By self-supervised learning  on evolutionary-scale databases [22; 23], PLMs have achieved great success in various biological applications, including structure prediction [19; 24] and protein design [18; 25]. Additionally, PLMs have demonstrated zero-shot capabilities in predicting and optimizing evolutionary plausibility [26; 27; 28], a continuous value indicating whether a mutation is favored by natural selection.

Despite their promising advancements, we argue that existing PLMs are not yet satisfactory as explainable and engineerable tools for handling protein mutations. Regarding mutation explanation, PLMs' implicit interpretation with evolutionary plausibility is overly vague, lacking detailed information for mutational effects such as specific alterations in protein functions and impacts on organisms. Regarding mutation engineering, PLMs can only propose evolutionary-plausible mutations, which may be misaligned with human preferences in real-world practices of directed evolution. For example, enhancing the catalytic activity of an enzyme from a bacterium could be detrimental to its survival due to increased energy costs but beneficial for industrial applications. In such scenarios, the utility of PLMs in assisting protein engineering is significantly compromised.

In this paper, we aim to develop explainable and engineerable PLMs by explicitly modeling protein mutations. However, conventional PLMs based on the Transformers  architecture provide context-aware representations for each amino acid, which are inadequate for capturing the discrepancies between the wild-type and its mutant within a unified feature space. Besides, there is a lack of supervision signals necessary for comprehending the intricate impacts of protein mutations, which require extensive background knowledge, including protein structures, protein functions, and mechanisms of biological processes.

To address these issues, we envision that (1) mutation representations could be captured from the variations of PLM representations between the wild-type and its mutant with appropriate architecture, and (2) expert-written texts from protein databases and biomedical publications provide rich cross-modal supervision for learning protein mutations. Specifically, we propose **MutaPLM**, a unified framework for interpreting and navigating **Muta**tions with **P**rotein **L**anguage **M**odels. We introduce a protein _delta_ network that translates between mutations and protein _delta_ features, formulating a unified feature space aligned with textual semantics. We develop a transfer learning pipeline with a chain-of-thought (CoT) strategy  to harvest protein mutation knowledge from biomedical texts. Additionally, we construct MutaDescribe, the first large-scale dataset containing diverse protein mutations and rich textual annotations of their effects. Using natural language as a friendly interface, the dataset facilitates the training and evaluation of mutation explanation and engineering.

Through comprehensive experiments, we demonstrate that MutaPLM is a versatile, explainable, and engineerable tool for assisting protein mutation studies. In mutation explanation, MutaPLM outperforms the strongest baseline model by 6.5% in ROUGE-L, and 19.4% of the predicted mutational effects are regarded as accurate and insightful by human experts. In mutation engineering, our model achieves an average of 0.409 recall scores on top-50 mutation proposals navigated by free-text instructions, improving ESM-2  by 1.6-fold.

Our contributions are summarized as follows:

* We propose MutaPLM, a unified framework that enables protein language models to capture mutations explicitly using a protein _delta_ network and cross-modal supervision.
* We build MutaDescribe, the first dataset with detailed textual annotations for protein mutations.
* We validate the effectiveness of MutaPLM in explaining and engineering protein mutations through comprehensive experiments.

## 2 Related Work

### Protein Language Models

In analogy to large language models (LLMs) [31; 32; 33; 34] in natural language processing (NLP), protein language models (PLMs) such as ProteinBERT , ProtTrans , ProtGPT2 , and ESM series [36; 19; 37] have surged in modeling protein sequences. Pre-trained by masked language modeling  or auto-regressive language modeling  on evolutionary-scale protein databases, PLMs have demonstrated outstanding predictive power on protein secondary and tertiary structures , protein functions  and protein-protein interactions . More recently, explorations on PLMs unifyingprotein sequences and natural language [42; 43; 44; 45] have attracted rising research interest, as texts provide unstructured knowledge and a friendly user interface for studying proteins. Notably, a contemporary work  proposes to perform text-based protein editing by directly generating the mutated protein sequence. Unfortunately, none of the existing PLMs qualifies as an explainable and engineerable tool in modeling protein mutations, mainly owing to architectural design and lack of supervision.

### Protein Mutation Modeling

Previous works formulate mutation explanation as learning the 'local fitness landscape', a mapping from protein sequences to specific functional activity scores . Models for protein fitness prediction could be categorized as (1) alignment-based models [48; 49] trained on multiple sequence alignments (MSAs) , (2) PLM models [18; 19] trained on large-scale unaligned sequences, (3) inverse-folding models [51; 27] that learn protein fitness through structure-conditioned sequence distributions, and (4) hybrid models [52; 53] that combine both PLMs and MSAs. The evaluations are performed as per wild-type protein on deep mutation scanning (DMS)  or clinical variant  benchmarks. In this work, we formulate mutation explanation as a more challenging task that aims at providing textual descriptions of mutational effects for arbitrary wild-type protein and mutation.

The traditional mutation engineering [8; 9] task aims at generating protein mutants with high fitness scores. One line of work leverages generative models including variational auto-encoders (VAEs) , generative language models  and diffusion models  to directly generate the protein sequence conditioned on fitness scores. Another line attempts to propose mutations iteratively by greedy sampling , reinforcement learning , or proximal gradients  on the learned fitness landscape. Differing from prior studies, MutaPLM incorporates textual instructions instead of fitness scores as navigation and proposes mutations satisfying human preferences.

## 3 Methods

The main goal of our work is to develop explainable and engineerable PLMs by explicitly modeling protein mutations. To achieve this goal, we elaborate on the proposed MutaPLM framework, highlighting three design components: (1) a protein _delta_ network that translates between mutations and protein _delta_ features \(z_{}\) (Sec. 3.1, detailed in Appendix A.1), (2) a transfer learning pipeline with a chain-of-thought strategy that harvests protein mutation knowledge from cross-modal supervision (Sec. 3.2, detailed in Appendix A.3), and (3) a specifically constructed dataset with diverse proteins and rich textual annotations of mutation effects (Sec. 3.3, detailed in Appendix B.2).

### Protein _Delta_ Network for Explicit Mutation Modeling

The protein _delta_ network follows an encoder-decoder architecture, utilizing textual semantics as the latent feature space for protein mutations. As illustrated in Fig. 1, the protein _delta_ network is

Figure 1: **Model architecture of MutaPLM. (a) The encoding branch of the protein _delta_ network.** The _delta_ encoder takes the subtraction of the PLM representations of the mutant and wild-type as inputs to generate \(z_{}\). (b) **The decoding branch of the protein _delta_ network.** The key components involve a _delta_ decoder that reconstructs mutant features and two prediction heads deciding the position and amino acid of the mutation.

composed of a protein language model (PLM), a large language model (LLM), a wild-type encoder, a _delta_ encoder, a _delta_ decoder, and two mutation prediction heads. We leverage ESM-2 (650M) , a powerful PLM pre-trained on evolutionary-scale databases, to encode protein sequences. We initialize the LLM with BioMedGPT-LM , a scientific language model built on LLaMA2-7B  through continual pre-training  on large-scale biomedical corpora.

**Formulation of protein _delta_ features.** We speculate that the subtraction of PLM representations between the mutant and wild-type, denoted as \(h_{}\), contains rich mutation information, making it suitable for extracting protein _delta_ features \(z_{}\). Specifically:

\[h_{}=h_{}-h_{}=f_{}(x_{})-f_{ {PLM}}(x_{}),\] (1)

where \(x_{}\) and \(x_{}\) are the amino acid sequences of the mutant and wild-type protein, \(h_{}\) and \(h_{}\) are their sequence representations, and \(f_{}\) is the protein language model.

The _delta_ encoder \(f_{}\) and _delta_ decoder \(f_{}\) facilitates bi-directional transformations between \(h_{}\) and \(z_{}\) as follows:

\[z_{}=f_{}(h_{}), h_{}=f_{}(z_{ }).\] (2)

**Encoding protein _delta_ features.** Given \(h_{}\), the _delta_ encoder is expected to extract information-preserving protein _delta_ features \(z_{}\) within a unified feature space. However, protein sequences vary in length, ranging from several tens to thousands of amino acids. To address this issue, we adopt a cross-attention module  to transform the sequential representations into a fixed number of latent features. The module, partly inspired by BLIP series [64; 65], maintains \(K\) trainable features that serve as queries and takes the sequence representations as keys and values to generate outputs. We employ two parallel modules for encoding the wild-type features \(h_{}\) and mutational features \(h_{}\).

**Decoding protein _delta_ features.** Drawing inspirations from LM-DESIGN , we introduce a cross-attention module that takes a symmetrical form of the _delta_ encoder. Specifically, it treats the wild-type protein representations \(h_{}\) as queries and protein _delta_ features \(z_{}\) as keys and values. The outputs are then processed by a two-layer feed-forward network (FFN) to reconstruct \(h_{}\). The mutant representations \(h_{}\) are obtained by combining \(h_{}\) with \(h_{}\), and fed into a position head and a language modeling head to predict the mutation. The position head is a fully-connected layer that predicts whether the amino acid should be substituted. The language modeling head is initialized from the PLM and predicts the type of the mutated amino acid. To facilitate text-based protein engineering, we maintain \(K\) trainable soft tokens, which are appended to the input token embeddings of the LLM to summarize textual semantics. The output representations of the soft tokens are processed by the _delta_ decoder to generate mutations.

Compared with previous works that connect protein sequences with LLMs [67; 44; 45], the proposed protein _delta_ network exhibits the following advantages:

Figure 2: **Training pipeline of MutaPLM. (a) Workflow of pre-training on protein-related literature. We perform next token prediction for the encoding workflow and conditional masked language modeling for the decoding workflow. (b) Workflow of fine-tuning with chain-of-thought (CoT). We employ a two-round dialog that involves describing the functions of a wild-type protein, explaining the effects of its mutation, and predicting the mutation based on the mutational effects.**

* Explicit modeling of protein mutations. Prior models are designed for static protein sequences, while MutaPLM models the alterations introduced by mutations with protein _delta_ features \(z_{}\).
* Encoder-decoder architecture. Prior works adopt either an encoder or a decoder architecture for protein sequences, while MutaPLM incorporates both encoding and decoding components.

### Transfer Learning with Cross-modal Supervision

Biomedical texts contain rich expert-annotated information on protein properties and mutational effects. As depicted in Fig. 2, MutaPLM harvests these cross-modal supervision signals through a transfer learning pipeline, which we detail as follows:

**Pre-training on protein literature.** In this stage, we aim to incorporate general protein knowledge from scientific publications with language modeling objectives, as shown in Fig. 2(a). (1) For the encoding workflow, we take the output representations of the wild-type encoder as LLM inputs and calculate the next-token prediction objective  for generating descriptive texts. (2) For the decoding workflow, we employ the conditional masked language modeling (CMLM) objective  on the protein sequence. Specifically, we mask 15% amino acids and task the PLM to recover the masks based on the remaining amino acid sequence and the LLM-summarized textual representations. It is worth noting that in this stage, the _delta_ decoder acts as a modality translator, generating bias terms that help reconstruct the original sequence instead of capturing protein mutation information. Overall, we optimize the summation of these two language modeling objectives.

**Fine-tuning on protein mutations with chain-of-thought (CoT).** As depicted in Fig. 2(b), we fine-tune MutaPLM on textual annotations of mutational effects to facilitate mutation explanation and engineering. Since mutational effects typically involve the enhancement or attenuation of protein functions, we adopt a chain-of-thought (CoT) strategy  that seamlessly connects protein functions and mutational effects within a two-round dialogue. In the first round, we prompt the LLM to describe the functions of the wild-type protein using the encoding workflow. In the second round, we introduce two tasks, namely describing the mutational effects with the encoding workflow, and predicting the mutation based on textual instructions with the decoding workflow. Both tasks utilize the latent wild-type representations and the predicted functions from the first round dialogue as additional inputs. Formally, the overall objective of fine-tuning is the summation of three parts: (1) next token prediction on protein function descriptions, (2) next token prediction on mutational effects, and (3) weighted cross-entropy between the predicted mutation and the ground-truth mutation.

### MutaDescribe: A Diverse Protein Mutation Dataset with Textual Annotations

We build MutaDescribe, a large-scale dataset comprising 20.9K wild-type proteins and 171.1K single-site mutations, to facilitate fine-tuning and evaluation. We provide an overview of our dataset in Tab. 1. The construction process involves the following steps:

**Raw data collection.** The primary source of MutaDescribe is UniProtKB/SwissProt , a widely adopted protein database that contains 106.6K single-site substitutions. We collect expert-reviewed descriptions of mutational effects from the _Phenotypes & Variants_ entry and retrieve the abstract of the corresponding publications on PubMed  based on available reference information.

**Quality control.** We prompt GPT-3.5-turbo  to filter out low-quality descriptions such as those that only mention the originating species. This step helps ensure that the dataset contains high-quality and informative annotations.

   Split & \# Proteins & \# Samples & Avg. sequence length & Avg. words \\  Train & 20,553 & 165,236 & 516.1 & 28.3 \\ Valid & 2,207 & 4,663 & 524.8 & 28.3 \\ Test (Easy) & 429 & 460 & 518.1 & 27.3 \\ Test (Medium) & 68 & 384 & 669.6 & 31.6 \\ Test (Hard) & 81 & 404 & 530.0 & 31.8 \\   

Table 1: **Statistics of the MutaDescribe dataset.** We report the number of proteins and samples, the average protein sequence length, and the average number of words for mutational effects.

**Data enrichment.** Given that the descriptions in UniProtKB are generally short and homogeneous, we utilize GPT-3.5-turbo to enrich the textual annotations by retrieving relevant descriptions from the original PubMed abstract. Additionally, we balance the number of benign and malignant mutations by constructing reversed samples. Specifically, for each mutation, we attempt to exchange the wild-type and the mutant and prompt GPT-3.5-turbo to write a description opposite to the original mutational effect. For example, if the mutational effect of an A89H mutation is _"Increased catalytic activity"_, we will create a reversed sample with an H89A mutation and _"Decreased catalytic activity"_.

**Data splitting.** We first randomly split our dataset into training, validation, and test sets. To evaluate models' generalization capabilities on novel proteins, we further partition the test set into three subsets based on the wild-type sequence homology with training sequences. We adopt MMSeqS2 , a widely-adopted tool to calculate sequence homology. The _Easy, Medium_ and _Hard_ test subsets comprise samples whose sequence homology are between \([0.95,1],[0.5,0.95)\), and \([0,0.5)\) respectively. We also implement a temporal split based on the publication date of the mutation, and we defer readers to Appendix B for details and Appendix D.1 for evaluation results.

Compared with prior mutation benchmarks [55; 72; 73], MutaDescribe is the first to incorporate textual annotations for facilitating mutation explanation and engineering. Besides, MutaDescribe contains a wider variety of wild-type proteins, surpassing ProteinGym  by 6 times in quantity.

## 4 Experiments

In this section, we demonstrate that MutaPLM is adept at interpreting and engineering mutations through comprehensive experiments. We start with a brief introduction of our training setups (Sec. 4.1), followed by detailed evaluations on two core tasks: mutation explanation (Sec. 4.2) and mutation engineering (Sec. 4.3). We also present an in-depth analysis of our design components (Sec. 4.4), including pre-training and the CoT strategy.

   Model &  &  &  &  \\  & R-L & BL-2 & R-L & BL-2 & R-L & BL-2 & R-L & BL-2 \\  ProLLaMA  & 1.02 & 0.64 & 1.00 & 0.91 & 1.03 & 0.70 & 1.02 & 0.74 \\ Mol-Instructions  & 5.10 & 0.65 & 5.19 & 0.65 & 5.56 & 0.90 & 5.27 & 0.73 \\ Galactica-6.7B  & 6.53 & 3.52 & 7.64 & 3.58 & 7.33 & 2.88 & 7.13 & 3.33 \\  GPT-4-0613 (1-shot)  & 8.04 & 2.93 & 9.96 & 3.42 & 9.62 & 2.69 & 9.14 & 3.00 \\ GPT-4-0613 (5-shot)  & 10.46 & 2.51 & 10.31 & 2.81 & 10.79 & 1.88 & 10.52 & 2.40 \\ GPT-4-0613 (5-shot, kNN)  & 11.63 & 9.63 & 12.98 & 10.88 & 12.46 & 8.63 & 12.31 & 9.69 \\ GPT-4 + ESM-2  & 11.69 & 11.09 & 13.02 & 11.50 & 12.77 & 8.48 & 12.45 & 10.37 \\ GPT-4 + OntoProtein  & 11.84 & 10.93 & 12.69 & 11.22 & 12.81 & 8.17 & 12.42 & 10.13 \\  AugmentedESM  & 11.60 & 8.33 & 11.40 & 7.46 & 10.73 & 6.95 & 11.26 & 7.62 \\ Fine-tuned ESM-2  & 20.49 & 9.37 & 11.87 & 5.95 & 11.34 & 3.32 & 14.88 & 6.36 \\  MutaPLM & **25.80** & **18.77** & **21.07** & **12.59** & **16.51** & **8.69** & **21.34** & **13.61** \\   

Table 2: **Performance evaluation for mutation explanation on the test sets of MutaDescribe. R-L: ROUGE-L. BL-2: BLEU-2.**

Figure 3: **Human-AI collaborative evaluation results for mutation explanation on the test sets of MutaDescribe. We show the number of accurate, relevant, opposite, and irrelevant predictions.**

### Training Setup

To alleviate catastrophic forgetting  and save computational costs, we train MutaPLM in a parameter-efficient way. We apply low-rank adaptation (LoRA)  on the LLM with a rank of 16. The number of query embeds and soft tokens is set as \(K=32\). We optimize the LoRA modules, the wild-type encoder, the _delta_ encoder, the _delta_ decoder, the soft tokens, the position head, and the language modeling (LM) head, which comprises a total of 75.0M parameters. The remaining 7.4B parameters are kept frozen.

We pre-train MutaPLM for 200K steps with a batch size of 32 on 1.1M protein-text data collected from biomedical publications (detailed in Appendix B.1) and fine-tune it for 70K steps with a batch size of 24 on MutaDescribe. For both stages, we use the AdamW optimizer  with a learning rate that is linearly warmed up to \(10^{-4}\) for the first 1K steps and decreases to \(10^{-5}\) following a cosine annealing strategy. The overall training process takes 10 days on 4 NVIDIA A100 GPUs.

### Performance Evaluation on Mutation Explanation

Differing from existing studies that interpret mutational effects with protein fitness [26; 28], we formulate mutation explanation as providing detailed textual descriptions for protein mutations.

**Baselines.** While no prior work is specifically designed for this task, we perform zero-shot analysis on popular LLMs with various zero-shot or few-shot prompts and implement supervised models for comparison. Our baselines include (1) Text-based LLMs. We perform in-context learning  by providing 1-shot and 5-shot demonstrations to GPT-4 , the most advanced model in NLP. Additionally, we implement a k-nearest neighbor (kNN) strategy  that selects the top-k homologous proteins from the training set as few-shot examples. (2) LLM-assisted PLMs, including ESM-2  and OntoProtein . In addition to kNN-based 5-shot samples for GPT-4, we leverage PLMs to provide additional information by predicting the evolutionary plausibility of the mutation. (3) LLMs trained on protein sequences, including Galactica-6.7B , Mol-Instructions , and ProtLLM . We feed the wild-type and mutated protein sequences into these models and instruct them to provide mutation explanations. (4) Fine-tuned LLMs. We fine-tune BioMedGPT-LM by feeding the ESM-2 representations of the wild-type and mutant (Fine-tuned ESM-2) or the wild-type sequence and evolutionary plausibility (AugmentedESM ) into the LLM and performing casual generation. Notably, for all ESM-2 models used in our baselines, we adopt the model with 650M parameters for fair comparison. We defer readers to Appendix C.1 for more implementation details.

**Evaluation.** We adopt BLEU  and ROUGE  scores to assess the quality of the generations by comparing them with ground-truth annotations. To further investigate whether the predictions are truly insightful and helpful in studying protein mutations, we perform a human-AI collaborative evaluation. Specifically, we first utilize GPT-4 as a proxy of human experts to categorize the predictions into _Accurate_, _Relevant_, _Opposite_, and _Irrelevant_, based on the relevance between the predictions and ground truth. Then, we recruit a postgraduate from a top university who majors in biology to assess and rectify GPT-4 evaluation results on mutation explanations following the same categorization protocol. The prompt and detailed evaluation results are displayed in Appendix C.3.

**Results and analysis.** We present performance comparisons on the test sets of MutaDescribe in Tab. 2 and Fig. 3. We observe that: (1) MutaPLM achieves state-of-the-art performance across various evaluation metrics, outperforming fine-tuned ESM-2 by 6.46% in ROUGE-L and GPT-4 + ESM-2 by 3.24% in BLEU-2. Additionally, more than 40.22% of MutaPLM predictions are regarded as _Accurate_ or _Relevant_ with ground-truth labels, which showcases our model's helpfulness in real-world research scenarios. (2) While the performance on the _Medium_ and _Hard_ sets is not as promising as in the easy set, MutaPLM shows generalization capabilities on novel proteins, as validated by 6.44%

Figure 4: **Case study for a mutation from A (Alanine) to D (Aspartic) at the 205-th position of _m7GppX diphosphate_. MutaPLM provides accurate explanations and insights, while GPT-4 generates irrelevant results.**accurate and 19.80% relevant predictions on the hard set. (3) The evolutionary plausibility values are beneficial for elucidating mutational effects, as demonstrated by the slightly improved results of LLM-assisted PLMs against the plain GPT-4 counterpart. However, the superior performance of fine-tuned ESM-2 and MutaPLM indicates that integrating the mutant sequence provides richer mutational information. (4) Supervised baselines underperform few-shot GPT-4 models, especially on _Medium_ and _Hard_ sets and BLEU-2 scores. We observe that supervised models tend to randomly combine short textual segments from the training set, indicating overfitting problems. (5) LLMs trained on protein sequences perform poorly, as they are solely instruction-tuned on single protein sequences. Hence, we emphasize the significance of knowledge transfer from protein functions to mutational effects and their basic properties.

**Case study.** Additionally, we present a case study in Fig. 4 for a mutation from _m7GpppX diphosphate_. Our model accurately identifies the increased decapping activity and provides novel insights beyond the ground truth. In contrast, the GPT-4 model mistakenly identifies the mutational effects as decreases in enzymic activity. More cases are available in Appendix D.3.

### Performance Evaluation on Mutation Engineering

Differing from prior works [59; 60; 61] that perform mutation engineering with an active learning paradigm , we challenge models to directly propose protein mutations based on the wild-type sequence and textual instructions. As we primarily focus on single-site mutations, we formulate this as a retrieval task from \(19 L\) possible mutants for a protein sequence of length \(L\).

**Baselines.** We adopt four groups of baselines including: (1) Few-shot LLMs. Similar to mutation explanation, we prompt GPT-4 to suggest single-site mutations through in-context few-shot learning. (2) Zero-shot PLMs including ESM-2  and OntoProtein . We calculate the evolutionary plausibility scores following  for each amino acid and derive the best mutant. (3) A retrieval-based model, namely ProtST (ESM-2) . We calculate the cosine similarity between PLM and textual representations of mutational effects to score and rank mutations. (4) Fine-tuned models. We fine-tune BioMedGPT  to directly propose a mutation based on the protein sequence and instruction. We also fine-tune ESM-2 by combining its wild-type sequence representations with BioMedBERT  encodings of textual instructions by a cross-attention layer. Please refer to Appendix C.1 for details of our baselines.

**Evaluation.** We report the average accuracy of the mutated amino acid on the ground-truth mutational position. We also report top-50 recall scores on all possible mutations.

**Results and analysis.** Comparisons between MutaPLM and baselines on the test sets of MutaDescribe are presented in Tab. 3. We observe that: (1) MutaPLM achieves an average of 53.51% in accuracy and 40.94% in top-50 recall, improving the original ESM-2 model by 1.6-fold. The substantial gains

   Model &  &  &  &  \\  & Acc & Rec@50 & Acc & Rec@50 & Acc & Rec@50 & Acc & Rec@50 \\  Random & 5.23 & 0.83 & 4.94 & 0.52 & 5.20 & 1.24 & 5.13 & 0.87 \\  ProtST (ESM-2)  & 5.86 & - & 6.51 & - & 7.18 & - & 6.49 & - \\  GPT-4-0613 (1-shot)  & 10.83 & 5.00 & 10.77 & 6.92 & 12.09 & 8.79 & 11.21 & 6.81 \\ GPT-4-0613 (5-shot)  & 14.84 & 4.68 & 9.32 & 6.78 & 13.33 & 5.62 & 12.65 & 5.63 \\ GPT-4-0613 (5-shot, kNN)  & 15.97 & 7.56 & 14.29 & 7.14 & 14.77 & 7.95 & 15.06 & 7.56 \\  ESM-2  & 35.21 & 23.91 & 34.63 & 22.91 & 37.87 & 28.71 & 35.84 & 25.15 \\ OntoProtein  & 39.78 & 28.91 & 36.45 & 26.04 & 38.61 & 29.20 & 38.37 & 28.12 \\  Fine-tuned BioMedGPT  & 35.21 & 7.82 & 32.29 & 5.72 & 39.60 & 12.62 & 35.73 & 8.72 \\ Fine-tuned ESM-2 [19; 83] & 52.17 & 35.65 & **52.08** & 30.60 & 50.00 & 34.65 & 51.43 & 33.77 \\  MutaPLM & **56.08** & **43.47** & 48.69 & **34.89** & **55.19** & **43.81** & **53.51** & **40.94** \\   

Table 3: **Performance evaluation for mutation engineering on the test sets of MutaDescribe.** Acc: prediction accuracy of the amino acid given the position of the mutation. Rec@50: top 50 recall of the desired mutant. -: not reported due to unaffordable computation costs (requires \(\) 1M forward passes).

of MutaPLM underscore the significance of textual navigation in mutation engineering. (2) MutaPLM outperforms the fine-tuned ESM-2 model by an average of 2.09% in accuracy and 6.17% in top-50 recall, benefiting from our architectural design and pre-training. (3) The overall performance of MutaPLM on the _Easy_ and _Hard_ sets are similar but significantly higher than on the _Medium_ set. We attribute this to data distribution: protein sequences in the _Medium_ set are longer (see Tab. 1), and the distribution of the mutated amino acids differs (see Fig. A1). Besides, the PLM may have witnessed the wild-type protein during pre-training, which mitigates the overfitting problem. (4) Compared to LLMs, both zero-shot and fine-tuned PLMs achieve superior performance, thanks to their evolutionary knowledge attained from pre-training on large-scale protein sequences. (5) Aligning the representations of protein sequences and texts cannot benefit mutation modeling, as evidenced by the poor performance of ProtST (ESM-2).

**Visualization of protein fitness on multi-round optimization.** In addition to single-site mutations, we employ a beam-search algorithm  to obtain multi-point substitutions iteratively. We manually write the optimization objective for 6 representative benchmarks, set the number of beams as 20, perform 20 independent runs, and visualize the fitness scores predicted by ESM landscape models . We compare MutaPLM with EvoProtGrad , a gradient-based strategy that leverages PLMs for multi-round optimization, as well as with random sampling. More details are presented in Appendix C.4. As shown in Fig. 5, our model consistently yields higher-fitness mutants across 6 proteins with varying objectives, especially in the initial rounds of optimization. These results highlight MutaPLM's potential in assisting real-world mutagenesis applications.

### In-depth Analysis

**Impacts of transfer learning.** We show the impacts of pre-training and fine-tuning in Fig. 6. As the fine-tuning proceeds, the performance of MutaPLM continues to improve on the _Easy_ set but deteriorates on the _Medium_ and _Hard_ sets, indicating overfitting problems on out-of-domain samples. Besides, without pre-training, MutaPLM achieves higher performance for the initial steps, which we attribute to the adaptation cost from pre-training texts to fine-tuning texts. However, the overall ROUGE-L scores decline by 1.56% for mutation explanation and 1.18% for mutation engineering as the fine-tuning finalizes. Overall, these results validate our transfer learning design.

**Impacts of chain-of-thought (CoT).** To investigate the impacts of the chain-of-thought strategy, we perform ablation studies by (1) replacing the predicted function with the ground truth description, (2) replacing the predicted function with _'Unknown function'_, (3) removing the _delta_ features for mutation explanation, and (4) removing the mutational effects for mutation engineering. As shown in Tab. 4, removing protein functions leads to a performance drop of 2.80% for mutation explanation and 1.13% for mutation engineering. Conversely, using the ground-truth function results in notable improvements, particularly for mutation explanation. Besides, the _delta_ features and mutational effects within the second-round dialog play more significant roles in MutaPLM. These findings highlight the significance of jointly incorporating protein function and mutation information in explaining and navigating protein mutations.

Figure 5: **Visualization of fitness scores for multi-round protein optimization.** The curves indicate the average results, and the shaded regions indicate the standard deviation.

## 5 Limitations and Broader Impacts

MutaPLM pioneers as the first attempt in the explicit modeling of protein mutations with natural language, and we expect future endeavors on (1) expanding the scale and diversity of the MutaDescribe dataset by integrating multi-point mutations and indels , (2) analyzing the alterations of protein 3D structures  to deepen the understanding of mutations, and (3) developing active learning  pipelines to harness feedbacks from wet-lab experiments in real-world mutagenesis studies.

While MutaPLM bears promise in mutation explanation and engineering, we emphasize safety concerns that it can be misused to generate pathogenic mutations and harmful bio-agents. Hence, we declare that MutaPLM, upon public release, should be restricted to research purposes, and any further applications should undergo comprehensive experiments and human inspections.

## 6 Conclusions

In this work, we present MutaPLM, a unified framework harvesting protein language models for mutation explanation and engineering. We propose a protein _delta_ network to model mutations explicitly with protein _delta_ features and develop a transfer learning pipeline with a chain-of-thought strategy to integrate protein mutation knowledge from biomedical texts. Additionally, we construct MutaDescribe, the first large-scale dataset containing diverse proteins and detailed textual annotations for mutations. Our experiments demonstrate that MutaPLM offers insightful explanations for mutational effects and proposes desirable mutants based on textual instructions. We anticipate that the proposed MutaPLM framework and our publicly released dataset will pave the way for novel research avenues and applications in studying proteins.