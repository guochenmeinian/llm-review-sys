# Safety through feedback in Constrained RL

Shashank Reddy Chirra\({}^{1}\), Pradeep Varakantham\({}^{1}\), Praveen Paruchuri\({}^{2}\)

\({}^{1}\)Singapore Management University, \({}^{2}\)IIIT Hyderabad

{shashankc,pradeepv}@smu.edu.sg, praveen.p@iiit.ac.in

Corresponding Author

###### Abstract

In safety-critical RL settings, the inclusion of an additional cost function is often favoured over the arduous task of modifying the reward function to ensure the agent's safe behaviour. However, designing or evaluating such a cost function can be prohibitively expensive. For instance, in the domain of self-driving, designing a cost function that encompasses all unsafe behaviours (e.g., aggressive lane changes, risky overtakes) is inherently complex, it must also consider all the actors present in the scene making it expensive to evaluate. In such scenarios, the cost function can be learned from feedback collected offline in between training rounds. This feedback can be system generated or elicited from a human observing the training process. Previous approaches have not been able to scale to complex environments and are constrained to receiving feedback at the state level which can be expensive to collect. To this end, we introduce an approach that scales to more complex domains and extends beyond state-level feedback, thus, reducing the burden on the evaluator. Inferring the cost function in such settings poses challenges, particularly in assigning credit to individual states based on trajectory-level feedback. To address this, we propose a surrogate objective that transforms the problem into a state-level supervised classification task with noisy labels, which can be solved efficiently. Additionally, it is often infeasible to collect feedback for every trajectory generated by the agent, hence, two fundamental questions arise: (1) Which trajectories should be presented to the human? and (2) How many trajectories are necessary for effective learning? To address these questions, we introduce a _novelty-based sampling_ mechanism that selectively involves the evaluator only when the the agent encounters a _novel_ trajectory, and discontinues querying once the trajectories are no longer _novel_. We showcase the efficiency of our method through experimentation on several benchmark Safety Gymnasium environments and realistic self-driving scenarios. Our method demonstrates near-optimal performance, comparable to when the cost function is known, by relying solely on trajectory-level feedback across multiple domains. This highlights both the effectiveness and scalability of our approach. The code to replicate these results can be found at https://github.com/shshnkreddy/RLSF

## 1 Introduction

Reinforcement Learning (RL) is known to suffer from the problem of reward design, especially in safety-related settings . In such a case, constrained RL settings have emerged as a promising alternative to generate safe policies[3; 16; 34]. This framework introduces an additional cost function to split the task related information (rewards) from the safety related information (costs). In this paper, we address a scenario where the reward function is well-defined, while the cost function remains unknown _a priori_ and requires inference from feedback. This arises in cases where the cost function is _expensive_ to design or evaluate. For instance, consider the development of an autonomousdriving system, where the reward function may be defined as the time taken to reach a destination which is easy to define. However, formulating the cost function presents significant challenges. Designing a comprehensive cost function that effectively penalizes all potential unsafe behaviors is non-trivial, and ensuring safety often involves subjective judgments, which can vary based on individual preferences. Even if one succeeds in devising such a function, it must account for all environmental factors, such as neighboring vehicles and pedestrians. The evaluation of such a cost function in high-fidelity simulators can be prohibitively expensive.

Feedback can be collected from either a human observer, who monitors the agent's training process and periodically provides feedback on presented trajectories, or from a system that computes the cost incurred on selected trajectories. Throughout this paper, we use the term _evaluator_ to refer to the entity providing feedback, whether human or system-generated.

Cost inference in Constrained RL settings has gained recent attention. One key thread of research has focused on learning cost from from constraint abiding expert demonstrations [28; 32]. However, these expert demonstrations are not easily available in all settings. For example, consider robotic manipulation tasks where the human and the robot have different morphologies. This paper focuses on the second line of research in this area, where we generate trajectories and gather feedback from an evaluator to infer the underlying cost function. Prior works in this thread make limiting assumptions on the nature of the cost function such as _smoothness_ or _linearity_[28; 32; 11; 5] and are limited to obtaining feedback at the state level  which is expensive to collect from human evaluators. We do not make such assumptions and can take feedback provided at over longer horizons, in some cases the entire trajectory.

Frameworks for learning from feedback must exhibit the following properties as emphasized in [8; 18; 12]: 1) Feedback must be collected _offline_ in between rounds, since the agent may need to act in real time. 2) The amount of feedback collected must be _minimized_. 3) _Binary_ feedback is more suitable as compared to _numeric_ feedback as it is more intuitive to provide for humans. It has also been shown that humans provide less consistent feedback if it is _numeric_. 4) Each state is assigned a binary cost value, indicating that it is inherently _safe_ or _unsafe_, which is more intuitive for humans when assessing the safety of policies 1.

To this end we propose the **R**einforcement **L**earning from **S**afety **F**eedback (**RLSF**) algorithm, which embodies all the aforementioned properties. The key contributions of this algorithm include:

* Extends prior work to collect feedback over longer horizons. This is done by presenting the evaluator with the entire trajectory, breaking the trajectory into segments and eliciting feedback at the segment level. Inferring the costs directly by _maximizing likelihood_ presents new challenges due to the problem of credit assignment over longer horizons. To tackle this, we present a surrogate loss that converts the problem from trajectory level cost inference to a supervised binary classification problem with _noisy_ labels.
* Introduces a _novelty based sampling_ mechanism that reduces the number of queries by sampling _novel_ trajectories for feedback.
* Learns safe policies across diverse benchmark safety environments. We also show that the learnt cost function can be _transferred_ to train an agent with different dynamics/morphology from scratch without collecting additional feedback.

## 2 Preliminaries

Markov Decision ProcessA Markov Decision Process (MDP) \(\) is defined by the tuple \((,,,r,,)\), where \(\) denotes the set of states, \(\) is the set of actions, \((s^{}|s,a)\) is the transition probability, \(r(s,a)\) is the reward function, \(\) is the discount factor and \((s)()\) is the initial state distribution. A policy \((.|s)()\) is a distribution over the set of valid actions for state \(s\). We denote the set of all stationary policies as \(\). A _trajectory_\(=\{(s_{t},a_{t})\}\) denotes the state-action pairs encountered by executing \(\) in \(\). We use the short hand \(_{i:j}\) to denote a _trajectory segment_, i.e, the subsequence of \((s_{t},a_{t})\) pairs encountered from timestep \(i\) to \(j\). The _expected value_ of a function \(f\) under \(\) as \(^{f}() E_{}[_{t=0}^{}^{t}f(s _{t},a_{t})]\). We also employ the shorthand \(f()\) to represent the discounted sum of \(f\) along the trajectory \(\). The occupancy measure of a policy is define as \((s,a)=E_{}[_{t=0}^{}^{t}[(s_{t},a_{t})=(s,a)]]\), where \([.]\) denotes the indicator function. \(\) describes the frequency with which a state-action pair is visited by \(\).

Constrained Markov Decision ProcessA Constrained MDP  introduces a function \(c(s,a)\) and a cost threshold \(c_{max}\) that defines the maximum cost that can be accrued by a policy. The set of feasible policies is defined as \(_{c}=\{:^{c}() c_{max}\}\). A policy is considered to be _safe_ w.r.t \(c\) if it belongs to \(_{c}\).

## 3 Problem Definition

In this paper, we consider the constrained RL problem defined as,

\[^{*}=}{}\ ^{r}()\] (1)

We assume the threshold \(c_{max}\) is known, but the cost function is not known and must be inferred from feedback collected from an external evaluator. In many scenarios \(c_{max}\) is typically known, representing a predefined limit on acceptable costs or risks in the environment. However, crafting the cost function \(c(s,a)\) such that it penalizes all _unsafe_ behaviour can be infeasible.

We incorporate an additional constraint enforcing the cost function to be binary, i.e, \(c(s,a)\{0,1\}\). This ensures that each state-action pair is inherently categorized as either _safe_ or _unsafe_. We opt for this approach because it is simpler for human evaluators to assign a binary safety value to state-actions when assessing policy safety, as emphasized in .

## 4 Method

In this section, we introduce **R**einforcement **L**earning from **S**afety **F**eedback (**RLSF**), an on-policy algorithm that consists of two alternating stages: 1) Data/Feedback collection and 2) Constraint inference/Policy improvement. In the first stage, data is collected via rollouts of the current policy for a fixed number of trajectories. Next, a subset of these trajectories is presented for feedback from evaluator, which are then stored in a separate buffer. The second stage consists of two parts: i) Estimation of the cost function from the feedback data and ii) Improvement of the policy using the collected trajectories and their inferred costs. We repeat stages (1) and (2) until convergence.

First, we highlight how the feedback is collected and propose a method to infer the constraint function using this data. Next, we recognize the practical limitations of acquiring feedback for every trajectory during training and detail our approach to sampling a subset of trajectories for efficient cost learning of the cost function. Finally, we detail how the inferred cost function is used to improve the policy.

### Nature of the Feedback

In the feedback process, the evaluator is first presented with the entire trajectory \(_{0:T}\). Afterward, the trajectory is divided into contiguous segments \(_{i:j}\) of length \(k\), and feedback is collected for each segment. The segment length can be adjusted based on the complexity of the environment: in simpler environments, feedback can be gathered for the entire trajectory, whereas for environments with long horizons and sparse cost violations, shorter segments may be used. This approach simplifies the challenge of assigning credit to individual states. Similar methods have been adopted in other works that rely on human feedback [12; 18]. However, reducing segment length comes at an increased cost of obtaining feedback from the evaluator.

The evaluator is tasked with classifying a segment as _unsafe_ if the agent encounters an _unsafe_ state at any point within the segment. This decision was made to ensure consistent feedback from the evaluator. Alternative approaches--such as marking a segment _unsafe_ based on the number of unsafe states visited or leaving the classification to the evaluator's subjective judgment are more prone to generating inconsistent feedback in the case of human evaluators .

### Inferring the Cost Function

Let \(P=\{_{i:j},y^{safe}\}\) represent the feedback collected from the evaluator, where \(y^{safe}=1\) if the segment was labelled _safe_ and \(y^{safe}=0\) otherwise. We assume there exists an underlying ground truth cost function \(c_{gt}(s,a)\) based on which the evaluator provides feedback. Then, the probability that a state is _safe_ is defined as \(p_{gt}^{safe}(s,a)=[c_{gt}(s,a)=0]\).

Now, let \(p^{safe}(s,a)\) represent our estimate of \(p_{gt}^{safe}(s,a)\) that we intend to estimate from the collected feedback. Then, by definition of how the feedback is collected, the probability that a trajectory segment \(_{i:j}\) is labelled as _safe_ is given by,

\[p^{safe}(_{i:j})=_{t=i}^{j}p^{safe}(s_{t},a_{t})\] (2)

We can infer \(p^{safe}(s,a)\) by minimizing the likelihood loss,

\[L^{mle} =-E_{(_{i:j},y^{safe}) P}[y^{safe} p^{safe}(_ {i:j})+(1-y^{safe}) 1-p^{safe}(_{i:j})]\] \[=-E_{(_{i:j},y^{safe}) P}[y^{safe}_{t=i}^{j}  p^{safe}(s_{t},a_{t})+(1-y^{safe})(1-_{t=i}^{j}p^{safe}(s_ {t},a_{t}))]\] (3)

Directly minimizing Eq 3 is challenging as the term \(_{t=i}^{j}p^{safe}(s_{t},a_{t})\) would collapse to \(0\) when the segment length is long, causing unstable gradients. To address this issue, we propose using a surrogate loss function where we replace \(1-_{t=i}^{j}p^{safe}(s_{t},a_{t})\) by \(_{t=i}^{j}(1-p^{safe}(s_{t},a_{t}))\).

\[L^{sur} =-E_{(_{i:j},y^{safe}) P}[y^{safe}_{t=i}^{j} p ^{safe}(s_{t},a_{t})+(1-y^{safe})_{t=i}^{j}(1-p^{safe}(s_{t},a_{ t}))]\] \[=-E_{(_{i:j},y^{safe}) P}_{t=i}^{j}_{s,a}[(s_{t},a_{t})=(s,a)][y^{safe}=1] p^{safe}(s,a)\] \[+[y^{safe}=0](1-p^{safe}(s,a)) \] (4) \[=-[E_{(s,a) d_{g}} p^{safe}(s,a)+E_{(s,a) d_{b}} (1-p^{safe}(s,a))]\]

where \(d_{g}(s,a)=E_{(_{i:j},y_{safe}) P}[_{t=i}^{j}[(s_{t },a_{t})=(s,a) y^{safe}=1]]\) and \(d_{b}(s,a)=E_{(_{i:j},y_{safe}) P}[_{t=i}^{j}[(s_{t },a_{t})=(s,a) y^{safe}=0]]\) represent the densities with which states occur in _safe_ and _unsafe_ segments respectively.

The surrogate loss reformulates the objective from the segment level--where collapsing probabilities over long segments can be problematic--to the state level, where this issue does not occur. Optimizing Eq 4 involves breaking down the segments into individual states and assigning each state the label of the segment. Subsequently, states are uniformly sampled at random, and the binary cross-entropy loss is minimized. Consequently, \(L^{sur}\) represents a binary classification problem in which one class contains _noisy_ labels. While _unsafe_ states are always accurately labeled--since their presence necessitates an _unsafe_ classification for the segment--a _safe_ state may receive conflicting labels based on the status of the segment it belongs to. Thus, with a sufficient number of samples, we believe it becomes feasible to reliably differentiate between _safe_ and _unsafe_ states.

**Proposition 1**.: _The surrogate loss \(L^{sur}\) is an upper bound on the likelihood loss \(L^{mle}\)._

Thus, minimizing \(L^{sur}\) guarantees an upper bound on the likelihood loss of the estimated cost function.

Having discussed the surrogate loss, we now examine the characteristics of its optimal solution.

**Proposition 2**.: _The optimal solution to Eq 4 yields the estimate,_

\[p_{*}^{safe}(s,a)=(s,a)}{d_{g}(s,a)+d_{b}(s,a)}\] (5)

Subsequently, we define the inferred cost function as \(c_{*}(s,a)[p_{*}^{safe}(s,a)<]\). Employing \(c_{*}(s,a)\) in policy updates instead of \(c_{gt}(s,a)\) introduces a bias in estimating the cost accrued by the policy, that we analyse below. For this analysis, we assume that the feedback is _sufficient_, i.e, the density \(d(s,a)\) is greater than zero for every state, otherwise \(p^{*}(s,a)\) is not defined.

**Proposition 3**.: _For a fixed policy \(\), the bias in the estimation of the incurred costs is given by,_

\[E_{}[^{t}c_{*}(s,a)]-E_{}[^{t}c_{gt}(s,a)]=E_{(s,a)_ {g}^{}}[[d_{b}(s,a)>d_{g}(s,a)]]\] (6)

_where \(_{g}^{}(s,a)=E_{}[_{t=0}^{T}^{t}[[(s_{t},a_{t}) =(s,a) c_{gt}(s,a)=0]]\) is the occupancy measure of safe states visited by \(\)._

Proposition 3 illustrates that \(c^{*}(s,a)\) misclassifies certain _safe_ states as _unsafe_ when their frequency in segments labeled _unsafe_ exceeds that in segments labeled _safe_ by the evaluator. We contend that this misclassification is likely to diminish with increased data collection or shorter segment lengths. However, it is important to note that this misclassification is guaranteed to be zero only when the segment length is reduced to one, meaning feedback is provided at the state level.

Additionally, note that the bias is non-negative, meaning that the expected cost \(E_{}[c_{*}(s,a)]\) acts as an upper bound on the true cost incurred by \(\). Therefore, ensuring that the policy does not exceed the threshold \(c_{max}\) on \(c_{*}\) guarantees that it adheres to the threshold on \(c_{gt}\).

**Corollary 1**.: _Any policy \(\) that is safe w.r.t \(c_{*}\) is guaranteed to be safe w.r.t \(c_{gt}\)._

In practice, we represent \(p_{}^{safe}(s,a)\) using a neural network with parameters \(\). The resulting cost function is defined as \(c_{}(s,a)[p^{safe}(s,a)]<\).

### Efficient Subsampling of Trajectories

To reduce the burden on the evaluator and minimize the cost of feedback, we present a subset of the trajectories collected by the policy for feedback. The common approach is to break the problem into two parts: (1) define a schedule \(N_{queries}(i)\) that determines the number of trajectories to be shown to the user at the end of each data collection round \(i\). Subsequently, \(N_{queries}(i)\) trajectories are sampled from those collected by the policy at data collection round \(i\). While the ideal goal is to sample a subset of trajectories that maximizes the _expected value of information_, achieving this is computationally intractable . To address this challenge, various sampling methods have been employed, seeking to maximize a surrogate measure of this value. Among these, _uncertainty sampling_ stands out as the most prominent approach, wherein trajectories are sampled based on the estimator's uncertainty about their predictions [12; 18; 20]. However, quantifying the uncertainty is challenging given the lack of calibration in neural network predictions. To address this challenge, ensemble methods are frequently employed where the disagreement among the models is used as an uncertainty measure. However, the training of \(n\) distinct neural networks can exact substantial resource costs, prompting consideration for alternative approaches.

In light of this, we introduce a new form of _uncertainty sampling_ called _novelty sampling_. With _novelty sampling_, we gather all the _novel_ trajectories after each round and present them to the evaluator for feedback. Formally, we define a state as novel if its density in the feedback data collected so far \(d(s)=_{a}d(s,a)\) is 0. A trajectory is deemed _novel_ if it comprises of at least \(e\) novel states. This can be interpreted as ensuring that the _edit distance_--a well-known measure of trajectory distance --between the current trajectory and previously seen trajectories exceeds a threshold \(e\). We do not consider novelty for state-action pairs as we found that extending to this case adversely impacted the performance.

The central notion is that the model is prone to errors on _novel_ states- those it has not encountered during training. This arises because the policy evolves over time, venturing into states that were not previously encountered during data collection rounds. Therefore, this sampling strategy effectively reduces the _epistemic uncertainty_ of the model--error arising from insufficient training data--thereby making it a form of _uncertainty sampling_. Furthermore, this sampling method offers the advantage of implicitly establishing a decreasing querying schedule as novelty of trajectories reduces over time as shown in Figure 6 in the Appendix.

We compute the density \(d(s)\) through a count-based method, utilizing a hashmap to track the frequency of state occurrences in trajectories presented to the evaluator. Employing SimHash , we discretize the state space using a hash function \(:S\{-1,1\}^{n}\), which maps _locally_ similar states (measured by angular distance) to a binary code as:

\[(s)=sgn(Ag(s))\] (7)where \(g:S^{d}\) is an optional prepossessing function and \(A\) is an \(n d\) matrix with i.i.d entries drawn from a standard normal distribution. Also, note that \(n\) controls the granularity of the hash function, i.e, the number of states mapped to the same value. In our setting, \(g(s)=s\) as we observed no improvement when employing functions like autoencoders for feature extraction.

### Policy Optimization

After the data collection round, where we sample trajectories \(\{\}\) and their corresponding rewards \(\{r()\}\) using \(\), we estimate their costs \(c()\) using the inferred cost function. Our proposed method allows for the policy to be updated utilizing any on-policy constrained RL algorithm. In this study, we employ the PPO-Lagrangian algorithm that combines the PPO algorithm  with a lagrangian multiplier to ensure safety.

A detailed description of the proposed method can be found in Algorithm 1. Lines [6-17] describe the data and feedback collection stage, and Lines [18-23] describe the cost inference and policy improvement stage.

```
1:Input: cost threshold \(c_{max}\), segment length \(k\), novelty criterion \(e\)
2:Initialize: policy \(_{0}\)
3:Initialize: classifier \(c_{}\), learning rate \(lr_{}\) and feedback buffer \(D\)
4:Initialize:\(A^{n d}\) with entries sampled i.i.d from \((0,1)\), \((.)=sgn(A^{T}(.))\), density map \(d(.) 0\).
5:while not converged do
6: Collect trajectories \(\{\},\{r()\}\)\(\) Data Collection
7:for each trajectory \(^{i}\{\}\)do\(\) Feedback Collection
8: novel \(\) True if \(\) e states \(\{s_{e}\}^{i}\) such that \(d((s_{e}))>0\)
9:if novel then
10: Show \(^{i}\) to the evaluator
11:for each segment \(_{j:j+k-1}^{i}\)do
12: Obtain feedback \(y^{safe}\) for the segment \(^{i}_{j:j+k-1}\).
13:\(D D\{((s,a),y^{safe})\ \ (s,a)^{i}_{j:j+k-1}\}\)
14:\(d((s)) d((s))+1\ \  s^{i}_{j:j+k-1}\)\(\) Update the densities
15:endfor
16:endfor
17:for each gradient step do\(\) Update cost estimates
18: Sample random minibatch \(b\{(s,a),y_{safe}\} D\)
19:\(-lr_{} L^{sur}(b)\)
20:endfor
21: Infer costs \(\{c_{}(_{i})\}\) for all \(^{i}\{\}\).
22: Update \(\) using \(\{r(_{i})\}\) and \(\{c_{}(_{i})\}\). \(\) Policy Improvement
23:endwhile ```

**Algorithm 1** Reinforcement Learning from Safety Feedback (RLSF)

## 5 Experiments

We investigate the following questions in our experiments:

1. Does RLSF succeed in effectively learning safe behaviours?
2. Can the inferred cost function be transferred across agents in the same task?
3. How does the proposed novelty based sampling scheme compare with other methods used in the literature?
4. How accurate is inferred cost function compared to the true cost function?
5. How can we address the overestimation bias of the inferred cost function as described in Section 4.2?

### Experiment Setup

We evaluate RLSF on multiple continuous control benchmarks in the Safety Gymnasium environment  and Mujoco  based environments introduced in . The _Circle_, _Blocked Swimmer_ and _Biased Pendulum_ environments constrain the position of the agent whereas the _Half Cheetah_, _Hopper_ and _Walker-2d_ environments constrain the velocity of the agent. The _Goal_ and _Push_ tasks are the most challenging as they contain static and dynamic obstacles that the agent must avoid while completing the task.All of the above environments reflect safety challenges that an agent could potentially face in real-world scenarios. Additionally, we conduct experiments using the Driver simulator introduced in , which presents two scenarios that an autonomous driving agent is likely to encounter on the highway: lane changes and blocked paths. In this setup, the cost function is based on multiple variables, including speed, position, and distance to other vehicles. Additionally, we introduce a third scenario: overtaking on a two-lane highway. A detailed description of the tasks can be found in the Appendix B.

We split the environments into two settings: (1) _hard constraint_ setting (\(c_{max}=0\)) where the safety of the policy is measured in terms of the cost violation (CV) rate defined as the number of cost violations divided by the length of the episode and (2) _soft_ constraint setting where \(c_{max}>0\). We utilize an automated script that leverages the underlying cost function to simulate the feedback provided by the evaluator.

We compare the performance of our algorithm against the following baselines: **Self Imitation Safe Reinforcement Learning (SIM)**: SIM is a state-of-the-art method in constrained RL that also supports the case where feedback is elicited from an external evaluator. Similar to RLSF, the method consists of two stages, a data collection/feedback stage and a policy optimization stage. In the first stage, a trajectory \(\) is labelled as _good_ if \([r() r_{good}\ \ \ \ [c() c_{max}]\), and labelled as _bad_ if \([r() r_{bad}\ \ \ \ [c() c_{max}]\), where \(r_{good}\) and \(r_{bad}\) are predefined thresholds on the reward. The information \([c() c_{max}]\) is received from feedback. The idea is then to imitate the _good_

    &  &  \\   & &  & SIMKC & SDM & SIM & RLSF (Ours) \\   & Return & \(45.26\) & \(46.09\) & \(36.20 3.95\) & \(22.26 9.59\) & \(\) \\  & C.V Rate (\(\%\)) & \(0.4\) & \(0.43\) & \(11.43 0.69\) & \(35.21 10.09\) & \(\) \\   & Return & \(14.34\) & \(15.21\) & \(5.18 2.48\) & \(6.34 2.87\) & \(\) \\  & C.V Rate (\(\%\)) & \(0.84\) & \(5.4\) & \(6.2 6.18\) & \(4.53 4.00\) & \(\) \\   & Return & \(717.43\) & \(983.27\) & \(495.58 160.84\) & \(577.15 184.31\) & \(\) \\  & C.V Rate (\(\%\)) & \(0.0\) & \(0.1\) & \(39.91 17.05\) & \(48.58 21.67\) & \(\) \\   & Return & \(22.62\) & \(21.05\) & \(86.96 10.69\) & \(21.15 8.58\) & \(\) \\  & C.V Rate (\(\%\)) & \(3.91\) & \(0.01\) & \(92.8 1.65\) & \(13.33 12.11\) & \(\) \\   & Return & \(2786.71\) & \(2497.82\) & \(3031.7 336.48\) & \(257.34 147.35\) & \(\) \\  & C.V Rate (\(\%\)) & \(0.42\) & \(0.06\) & \(59.4 8.28\) & \(0.0 0.0\) & \(\) \\   & Return & \(1705.00\) & \(1555.25\) & \(1097.57 56.35\) & \(990.08 8.66\) & \(\) \\  & C.V Rate (\(\%\)) & \(0.19\) & \(0.02\) & \(0.0 0.0\) & \(0.0 0.0\) & \(\) \\   & Return & \(2947.25\) & \(2925.23\) & \(2195.94 134.21\) & \(993.38 17.69\) & \(\) \\  & C.V Rate (\(\%\)) & \(0.16\) & \(0.0\) & \(1.58 1.53\) & \(0.0 0.0\) & \(\) \\   & Return & \(26.16\) & \(26.10\) & \(1.61 1.8149\) & \(10.86 4.1\) & \(\) \\  & Cost (\(40.0\)) & \(34.19\) & \(31.83\) & \(30.57 13.29\) & \(52.76 12.85\) & \(\) \\   & Return & \(27.37\) & \(26.44\) & \(1.05 2.83\) & \(\) & \(24.28 2.1\) \\  & Cost (\(40.0\)) & \(41.67\) & \(35.41\) & \(34.71 9.87\) & \(33.33 11.26\) & \(41.25 2.27\) \\   & Return & \(6.00\) & \(10.84\) & \(0.16 0.14\) & \(3.63 1.77\) & \(\) \\  & Cost (\(35.0\)) & \(26.08\) & \(26.96\) & \(22.89 5.95\) & \(45.43 3.86\) & \(\) \\   & Return & \(3.07\) & \(2.68\) & \(-3.04 3.3\) & \(1.56 0.46\) & \(\) \\  & Cost (\(35.0\)) & \(20.53\) & \(20.95\) & \(23.25 7.78\) & \(36.55 1.48\) & \(\) \\   

Table 1: Performance of different algorithms on the Safety Benchmarks. The first \(7\) environments represent the _hard_ constraint case. The remaining environments illustrate the _soft_ constraint case, with values in brackets indicating the cost threshold. Each algorithm is run for \(6\) independent seeds. (orange) and (blue) indicate the best performance in the known costs and inferred costs settings, respectively. Algorithms with a cost violation (C.V) rate below \(1\%\) are deemed to have equal performance in terms of safety.

trajectories and stay away from the _bad_ trajectories. If \(^{}\) is the occupancy measure of the current policy \(\) and \(^{G},^{B}\) are the occupancy measures of the _good_ and _bad_ trajectories respectively, then, \(\) is optimized as,

\[^{*}=}\ \ (^{,G}||^{B})\] (8)

where \(^{,G}=(^{}+^{G})/2\) and KL denotes the Kullback-Leibler divergence.

**Safe Distribution Matching (SDM)**: SIM combines rewards with safety feedback into a joint notion of \(good\) and \(bad\) which may not be desirable when the cost function is unknown. Thus, we introduce an additional baseline that keeps these two signals separate by labelling the trajectory as _good_ if \(c() c_{max}\) and _bad_ otherwise. The policy is then updated as,

\[^{*}=}\ \ r+(^{,G}|| ^{B})\] (9)

where \(\) controls the tradeoff between the two objectives.

As an upper bound, we compare the performance of our algorithm to scenarios where the cost function is known: PPO-Lagrangian (PPOLag)  and SIM with known costs (SIMKC) . The objective is not to surpass their performance, but to match it. Therefore, when reporting the results of these algorithms, we present the best-performing seed across multiple runs.

All results presented use novelty-based sampling unless stated otherwise. We use a segment length of \(1\) in the _Driver_, _Goal_ and _Push_ environments. This is because the Safety Goal and Push environments contain small obstacles that the agent interacts with for very brief periods of time, hence requiring more fine-grained feedback. An example of this is present in Figure 11 in the Appendix. In the _Driver_ environments, a randomly initialized policy was _highly unsafe_. Thus a long segment length would force the evaluator to label every segment _unsafe_, making cost inference infeasible. **In the remaining environments, the segment length corresponds to the length of the episode**. Details on the number of queries generated for feedback can be found in Table 4 in the Appendix. We grant the two baseline methods (with unknown costs) an advantage by providing feedback for every trajectory generated.

### Cost Inference across various tasks

Benchmark EnvironmentsTable 1 presents the performance of the various algorithms on the benchmark environments. RLSF significantly outperforms the two baselines in terms of reward and safety in _all_ of the environments 2. RLSF comes to within \( 80\%\) of the performance of the best run of PPOLag in \(7/11\) environments thereby underscoring its effectiveness in learning safe policies.

Driving ScenariosIn the driving environments, we evaluate the performance of our algorithm against two baseline methods: a naive PPO policy that solely maximizes reward and the PPO-Lag algorithm. Figure 1 presents these results. Notably, our method demonstra

Figure 1: Cost Violation rate of different algorithms in the Driver environment. Each algorithm is run for \(6\) independent seeds, with the curves representing the mean and the shaded regions indicating the standard error.

performance to PPO-Lag while significantly outperforming PPO in terms of cost violations. This highlights the effectiveness of our approach in learning safe autonomous driving policies. Video clips showcasing the learned policies in the \(3\) scenarios are included in the supplementary material.

### Cost Transfer

We also showcase the potential of using the inferred cost function to train an agent with a different embodiment or morphology to solve the same task from scratch, without requiring any additional feedback. We utilize the inferred cost function obtained during the training of a _Point_ agent to train the _Doggo_ agent for the _Circle_ and _Goal_ tasks. The observation space in the environments consists of agent-related telemetry (acceleration, velocity) and task-related information (lidar sensors for goal/boundary/obstacle detection). In these experiments, we do not incorporate agent-related information when learning the cost function, ensuring that it can be transferred to a new agent. Next, the inferred cost function remains fixed during the training of the second agent, and the task-related features are utilized for cost inference. We compare the performance of this approach with that of the PPO-Lagrangian algorithm that utilizes the underlying cost function of the environment. From Table 2 we can infer that agents trained using transferred cost function are comparable in performance to agents trained using the _true_ underlying cost function.

### Effect of Novelty Sampling

Next, we demonstrate the effectiveness of the proposed _novelty-based sampling_ mechanism by comparing it with other popular querying methods from the literature . These methods require a predefined budget for the number of trajectories presented to the evaluator. We evaluate two schedules: (1) _Uniform schedule_, where a fixed number of trajectories is shown for feedback after each round, and (2) _Decreasing schedule_, where the number of trajectories decreases in proportion to \(\) each round. For each schedule, we test the following strategies for sampling the subset of trajectories to be presented to the evaluator: (a) _Random sampling_, which selects a random subset of trajectories, and (b) _Entropy sampling_, where trajectories are sampled in descending order of the average entropy \((p_{}^{safe}(s_{t},a_{t}))\) of the estimator. As shown in Figure 2, entropy based sampling outperforms random sampling, and a decreasing schedule outperforms a uniform schedule. However, all the methods fall short compared the proposed _novelty based sampling_ mechanism.

   Source Env & Target Env &  & PPOLag with transferred cost \\   &  & Return & \(2.69 0.24\) & \(2.00 0.27\) \\  & & C.V Rate (\(\%\)) & \(0.63 0.09\) & \(0.18 0.05\) \\   &  & Return & \(1.45 0.06\) & \(1.20 0.26\) \\  & & Cost (40.0) & \(40.86 4.80\) & \(37.31 6.77\) \\   

Table 2: Comparison of PPOLag performance when trained with the underlying task cost function versus the transferred cost function. Results are averaged over three independent seeds.

Figure 2: Comparison of different sampling and scheduling schemes. Results are averaged over \(3\) independent seeds. The proposed sampling method generates on average \( 1950\) queries, hence for fair comparison the other methods were given a budget of \(2000\) queries.

### Analyzing the Inferred Cost function

To further evaluate the quality of the inferred reward function, we plot it alongside the true underlying cost function of the environment as presented in Figure 3. In the Point Goal environment, where feedback is collected for each state, the inferred cost function closely aligns with the true cost given sufficient data. However, in the Point Circle environment, where feedback is collected at the trajectory level, the inferred cost function exhibits a slight overestimation bias, as discussed in Section 4.2. Nevertheless, from Figure 3, we can conclude that our method learns cost functions that strongly correlate with the true costs.

### Bias Correction

In Section 4.2, we established that when feedback is collected over segments, the inferred cost function tends to exhibit an overestimation bias. As a result, optimizing for a policy that is deemed _safe_ with respect to \(c_{max}\) using the inferred cost function can lead to overly conservative policies. If the bias \(b\), could be calculated apriori, then adjusting the safety threshold to \(c_{max}+b\) would still satisfy the safety guarantees outlined in Corollary 1. However, since estimating \(b\) in advance is infeasible, we propose a heuristic approach where a constant \(^{+}\) is added to \(c_{max}\) to account for the bias. It's important to note that Corollary 1 only holds if \( b\), meaning this heuristic doesn't guarantee that the resulting policy will always be safe. Nevertheless, we tested this approach with different \(\) values in the _Car Circle_ environment, as this setting exhibited a higher overestimation bias (as reflected in Table 1). The results, shown in Figure 4, demonstrate that adding this bonus does improve performance (with higher \(\) values yielding better results), though it introduces the additional challenge of tuning \(\).

## 6 Limitations, Future Work and Broader Impact

In this work, we consider the problem of learning cost functions from feedback in scenarios where evaluating or defining such functions is either costly or infeasible. We present a novel framework which significantly reduces the burden on the evaluator by eliciting feedback over extended horizons, a challenge that has not been addressed in previous research. To further reduce their load, we propose a _novelty-based sampling_ method that only presents previously unseen trajectories to the evaluator. Through experiments on multiple benchmark environments, we demonstrate the effectiveness of our framework and the proposed sampling algorithm in learning safe policies. However, there are a few limitations we would like to acknowledge. First, our method relies on state-level feedback in some environments, which can be expensive to obtain. Second, while we simulate feedback using the true cost function, real-world feedback is often noisy, especially when provided by human evaluators. It would be valuable to conduct experiments involving real human subjects to validate the approach.

By enabling safer autonomous systems, our approach has the potential to enhance safety across a range of applications, including autonomous vehicles, drones used for delivery, and industrial robots operating in environments like warehouses or manufacturing plants. These improvements could significantly reduce accidents, protect human operators, and increase the overall reliability of automated systems.

Figure 4: Increasing \(c_{max}\) by \(\) to correct for the overestimation bias.

Figure 3: Comparing the inferred cost to the true cost.

Acknowledgments

This research/project is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG2-RP-2020-016) and Lee Kuan Yew Fellowship awarded to Pradeep Varakantham.