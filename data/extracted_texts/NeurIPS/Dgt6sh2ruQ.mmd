# Contextual Decision-Making with Knapsacks

Beyond the Worst Case

 Zhaohua Chen

School of Computer Science

Peking University

Haidian, Beijing, China

chenzhaohua@pku.edu.cn

&Rui Ai

IDSS & LIDS

Massachusetts Institute of Technology

Cambridge, MA 02139, USA

ruiai@mit.edu

&Mingwei Yang

Dept. of Management Science and Engineering

Stanford University

Stanford, CA 94305, USA

mwyang@stanford.edu

&Yuqi Pan

School of Engineering and Applied Sciences

Harvard University

Cambridge, MA 02138, USA

yuqipan@g.harvard.edu

&Chang Wang

Dept. of Computer Science

Northwestern University

Evanston, IL 60208, USA

wc@u.northwestern.edu

&Xiaotie Deng

School of Computer Science

Institute for Artificial Intelligence

Peking University

Haidian, Beijing, China

xiaotie@pku.edu.cn

###### Abstract

We study the framework of a dynamic decision-making scenario with resource constraints. In this framework, an agent, whose target is to maximize the total reward under the initial inventory, selects an action in each round upon observing a random request, leading to a reward and resource consumptions that are further associated with an unknown random external factor. While previous research has already established an \(()\) worst-case regret for this problem, this work offers two results that go beyond the worst-case perspective: one for the worst-case gap between benchmarks and another for logarithmic regret rates. We first show that an \(()\) distance between the commonly used fluid benchmark and the online optimum is unavoidable when the former has a degenerate optimal solution. On the algorithmic side, we merge the re-solving heuristic with distribution estimation skills and propose an algorithm that achieves an \((1)\) regret as long as the fluid LP has a unique and non-degenerate solution. Furthermore, we prove that our algorithm maintains a near-optimal \(()\) regret even in the worst cases and extend these results to the setting where the request and external factor are continuous. Regarding information structure, our regret results are obtained under two feedback models, respectively, where the algorithm accesses the external factor at the end of each round and at the end of a round only when a non-null action is executed.

## 1 Introduction

In online contextual decision-making problems with knapsack constraints (CDMK for short), an agent is required to make sequential decisions over a finite time horizon to maximize the accumulatedreward under initial resource constraints [14; 15]. To be more specific, in each round \(t=1,,T\), a request \(_{t}\) and an external factor \(_{t}\) are independently generated from two distributions, and only \(_{t}\) is revealed to the agent. Based on the request, the agent should irrevocably choose an action \(a_{t}\), which results in a reward \(r(_{t},a_{t},_{t})\) and a consumption vector \((_{t},a_{t},_{t})\) of resources. The agent's target is to optimize the sum of rewards \(_{t=1}^{T}r(_{t},a_{t},_{t})\) before the resources are depleted.

The contextual decision-making with knapsacks problem presents two key challenges when compared to closely related problems (e.g., the network revenue management problem): (1) choices are made without observing the _external factor_, and (2) distributions of requests and external factors are _unknown_. However, the complexity of CDMK makes it a suitable mathematical abstraction for many real-life scenarios, and there are extensive application scenarios with this kind of information structure. We use the following examples as illustrations and motivation.

**Example 1.1** (Supply chain management).: In supply chain management, a factory needs to allocate among \(T\) repositories consecutively and is constrained by the total inventory. Given the request \(_{t}\) for each repository, the factory chooses the number of goods transported to it as action \(a_{t}\). However, the factory has randomized transportation costs for different locations and traffic conditions denoted by an external factor \(_{t}\) for the \(t\)-th repository, which finally influences rewards. The factory needs to form an optimal scheme to allocate its goods under uncertainty to all these repositories.

**Example 1.2** (Dynamic bidding in repeated auctions with budgets [9; 10]).: In this circumstance, an advertiser acquires the value of the ad slot \(_{t}\) at the start of each auction and chooses a bid \(a_{t}\) accordingly. The agent's gain in this auction, as a consequence, is collaboratively determined by the value, the bid, and the highest competing bid \(_{t}\), and has a form of \(_{t}(a_{t}>_{t})\). Additionally, the payment is \(a_{t}(a_{t}>_{t})\) for the first-price auction and \(_{t}(a_{t}>_{t})\) for the second-price auction, respectively. It is to be noted that the highest competing bid is inaccessible to the agent before committing to the bid, as all advertisers bid simultaneously. Meanwhile, its distribution is decided by other advertisers, which is also unknown to the agent before the auctions.

The CDMK model can also capture other well-studied problems, including multi-secretary, online linear programming, online matching, etc., as discussed in Balseiro et al. . Previous studies of the CDMK problem have shown that the worst-case regret is \(()\) when the initial resources are linearly proportional to the horizon length \(T\)[36; 24]. However, it is still unclear whether we can achieve a better regret guarantee for the CDMK problem beyond worst-case scenarios. In particular, can we design algorithms to obtain an \(o()\) regret only under mild assumptions that hold for almost all possible CDMK instances? Meanwhile, can these algorithms still obtain good regret guarantees even in the worst cases? At last, previous works would adopt specific benchmarks to measure the regret of algorithms, but how are these benchmarks close to the rewards that the optimal online algorithm can achieve? This work widely addresses these questions.

### Our Contributions

This work makes three main contributions, summarized as follows.

The fluid optimum can be \(()\) away from the online optimum.Since the online optimum is hard to characterize, previous works always use an alternative benchmark to measure the performance of any online algorithm, and the fluid optimum (also known as the deterministic LP) is a common choice [24; 35]. However, we demonstrate that when the fluid benchmark has a unique and degenerate solution, then an \(()\) gap is unavoidable between these two optima (cf. Theorem 2.1). While Han et al.  has also provided a similar lower bound result for the related contextual bandits with knapsacks (CBwK) problem, their condition depends on the inseparability of the possible expected reward/consumption function set. In other words, their condition may not perform well when this feasible set is small. Furthermore, their condition is rather complicated to verify. In contrast, our condition only depends on the underlying problem instance and is concise and easy to check. The proof of our result extends the approach of Vera and Banerjee  to the CDMK problem.

An \((1)\) regret via re-solving under mild assumptions with full/partial information feedback.Since an \(()\) worst-case regret is already known , we investigate how well an online algorithm can perform beyond worst cases by applying the re-solving heuristic in conjunction with distribution estimation techniques, as given in Algorithm 1. This method has been considered in the problemsof network revenue management (NRM) and bandits with knapsacks (BwK). (See Section 1.2 for a literature review.) However, to our knowledge, we are the first to extend this method to the CDMK problem, which poses new challenges as decisions should be made according to the request. To avoid worst cases, we explicitly suppose that the fluid problem has a unique and non-degenerate solution (cf. Assumption 3.1). This assumption is mild in three aspects: (1) it captures almost all CDMK problem instances, as slightly perturbing any LP can help it satisfy the unique optimality and non-degeneracy conditions; (2) it is less restrictive than the assumptions given in Sankararaman and Slivkins , which require that there are at most two resources; and (3) it is almost necessary for an \(o()\) regret bound to be established by Theorem 2.1, when using the fluid optimum as the benchmark. Under the assumption, our main results show that the re-solving heuristic reaches an \(O(1)\) regret with full information (cf. Theorem 3.1) and an \(O( T)\) regret with partial information (cf. Theorem 4.1). To our knowledge, these are the first \((1)\) regret results in the CDMK problem beyond the worst case with only mild assumptions. Importantly, unlike previous results, these regret bounds are also independent of the number of actions.

Within our results, the full information model assumes that the agent sees the external factor at the end of each round. In contrast, in the partial information model, the agent acquires the external factor only when a non-null action is adopted. In Example 1.1, if the factory can learn the road condition via map services, it then observes the external factor no matter its chosen action, reflecting the full information feedback. However, it can sometimes only observe transportation costs when it transports goods, resembling a non-null action. This is a case of partial information feedback. In the auction market illustrated in Example 1.2, agents might also face these two kinds of information models. In some situations, bidders can always view others' bids after the auction, while in other cases, only those who bid non-zero values can observe others' bids. Non-zero bidding here reflects a non-null action. Therefore, these two information models hold strong practical significance.

Other state-of-the-art results consider bandit information feedback, in which the agent only sees the reward and the consumption rather than the external factor. However, they explicitly assume a specific (e.g., linear) relationship between the conditional expected reward-consumption pair and the request , whereas our results do not impose any underlying distribution structures, bypassing realizability issues . On this side, our information model is comparable to those in existing work.

A near-optimal regret even in worst cases with full/partial information feedback and an extension to continuous randomness.We further explore how well our Algorithm 1 performs even in worst-case scenarios. With full information feedback, we show that an \(O()\) regret is achieved (cf. Theorem 5.1). This bound is asymptotically equal to the state-of-the-art with this information model . Even with partial information, we can still guarantee a universal \(O( T)\) regret (cf. Theorem 5.2), which is optimal up to a logarithmic factor. These results demonstrate the applicability and robustness of the re-solving heuristic in CDMK problems, regardless of some specific instances. For completeness, we extend our algorithm and analysis to the situation in which the distributions of request and external factor are continuous and derive corresponding regret results (cf. Theorems A.1 and A.2).

We summarize our algorithmic results on Algorithm 1 in Table 1.

    & 
 Beyond the Worst Case \\  &  \\   & Uniq., Non-Degen. LP & Discrete & Continuous \\  Full-Info. & \(O(1)\) & \(O()\) & \(O((T^{_{u}}+T^{_{v}}+T^{1/2}))\) \\ Part.-Info. & \(O( T)\) & \(O( T)\) & \(O((T^{_{u}}+T^{1/2})+T^{_{v}}( T)^{3/2-_{ v}})\) \\    \(u,v\): the mass/density function of the context and the external factor.

\(_{p\{u,v\}}\): \((+d)/(2+d)\) if \(p\) is a \(d\)-dimension distribution and \(p(,L)\). (See Appendix A.)

Table 1: A summary of our algorithmic results on Algorithm 1. Constants are omitted.

### Related Work

Contextual decision-making/bandits with knapsacks.The issue most closely related to the CDMK problem is the problem of contextual bandits with knapsacks (CBwK), introduced by Agrawal and Devanur . The main difference between the CDMK problem and the CBwK problem is that in the latter, an explicit model of the external factor is missed, and the bandit information feedback is considered. That is, only the consumption and the reward are revealed to the agent at the end of a round rather than the external factor. Along this research line, two primary methodologies have been proposed to solve the problem. The first approach aims to select the best probabilistic strategy within the policy set , and Agrawal et al.  adopts this approach to achieve an \(()\) regret. This heuristic originates from the subject of contextual bandits [17; 2], and requires a cost-sensitive classification oracle to achieve computation efficiency.

On the other hand, another approach views the problem from the perspective of the Lagrangian dual space. It uses a dual update method that reduces the CBwK problem to the online convex optimization (OCO) problem. In particular, some work [3; 32; 35; 30] assumes a linear relationship between the conditional expectation of the reward-consumption pair and the request-action pair. This line adopts techniques for estimating linear function classes [1; 6; 34; 18] and combines them with OCO methods to achieve sub-linear regret.

Apart from the above studies, some results [24; 36; 37] are not restricted to linear expectation functions. To deal with more general problems with bandit feedback, they plug model-reliable online regression methods [22; 21] into the dual update framework. As a result, their algorithms' regret is the sum of the regret on online regression and online convex optimization, respectively. Nevertheless, the online regression technique still limits the conditionally expected reward-consumption functions.

In the CDMK literature, Liu and Grigas  have considered full information feedback, where the agent sees the external factor at the end of each round. Motivated by practice, our work further considers a partial feedback model, in which the agent observes the external factor when a non-null action is chosen (cf. Section 2).

The re-solving heuristic and related problems.Unlike the above approaches, our work adopts the re-solving method, also known as the "certainty equivalence" (CE) heuristic. Under this approach, the agent (in)frequently solves the fluid optimization problem with the remaining resources to obtain a probability control in each round. This method comes from the literature on the network revenue management (NRM) problem, which can be seen as a simplification of the CDMK problem without the existence of external factors or the external factor not getting involved in the resource consumption . Some researches in this setting also assumes known request distributions [26; 5; 25; 19; 13; 28; 16; 11; 39; 12; 27]. These works show that the re-solving-based method can obtain a constant regret under certain non-degeneracy assumptions and can generally obtain a square-root regret . Recently, the re-solving method is also extended to the general dynamic resource-constrained reward collection problem in Balseiro et al. , which assumes the knowledge of request and external factor distributions and achieves \(O(1)\) to \(O( T)\) regret for different action space cardinalities.

We should mention that the re-solving technique, together with other methods, has also been adopted for the bandits with knapsacks (BwK) problem [23; 20; 42; 39; 29; 32] to achieve an \(O( T)\) regret under different assumptions. For example, an essential result by Sankararaman and Slivkins  achieves \(O( T)\) regret in BwK under the best-arm assumption and two resources. However, CDMK is a more challenging problem than BwK in that the decision has to be based on the received request. Thus, no optimal static action mode is irrelevant to the round, which adds a layer of complexity to the re-solving method.

## 2 Preliminaries

We consider an agent interacting with the environment for \(T\) rounds. There are \(n\) kinds of resources, with an average amount of \(^{i}\) for resource \(i\) in each round, resulting in a total of \(^{i}T\) amount of resource \(i\). We suppose that \(<=_{1}=(^{i})_{i[n]}\) is independent of \(T\), with a maximum entry of \(^{} 1\) and a minimum entry of \(^{}>0\).

At the beginning of each round \(t 1\), the agent observes a request \(_{t}\) drawn i.i.d. from a distribution \(\) and should choose an action \(a_{t}\) from a set of actions \(A\). Given the request \(_{t}\) and the action \(a_{t}\), the agent receives a random reward \(r_{t}\) and a consumption vector of resources \(_{t}^{n}\), both of which are related to an external factor \(_{t}\) drawn i.i.d. from a distribution \(\). In other words, there is a reward function \(r: A\) and a consumption vector function \(: A^{n}\), such that \(r_{t}=r(_{t},a_{t},_{t})\) and \(_{t}=(_{t},a_{t},_{t})\). We suppose these two functions are pre-known to the agent. We further define \(R(,a)_{}[r(,a,)]\), and \((,a)_{}[(,a,)]\).

We impose minimum restrictions on the distributions \(\) and \(\). In the main body of this work, we only suppose that the support sets of both distributions are finite. Specifically, we let \(k=||\) be the size of the request set. We denote the mass function of \(\) and \(\) by \(u()\) and \(v()\), respectively. We will extend to the situation that these two distributions can be continuous in Appendix A.

The agent's objective is to maximize the cumulative rewards over the period under initial resource constraints, which is a sequential decision-making problem. To ensure feasibility, we assume the existence of a null action (denoted by \(0\)) in the action set \(A\). Under the null action, the reward and the consumption of any resource are zero, regardless of the request and the external factor. In other words, we have \(r(_{t},0,_{t})=0\) and \((_{t},0,_{t})=\) for any \((_{t},_{t})\). We use \(A^{+} A\{0\}\) to denote the set of non-null actions and let \(m|A^{+}|\) be its size.

We would like to discuss here the necessity of the null action, which is widely used in related works . An alternative common choice for the null action is the so-called "early stop when resource exhausted"  in the BwK problem with no contexts. In reality, when the agent faces some "bad" contexts, a better choice is not "entering the market" to avoid, for example, small rewards but large consumption. As a comparison, struggling to come up with a non-null action here could occupy the space for serving those "good" contexts, and stopping before these contexts arrive may inevitably cause an \((T)\) regret. This illustrates that introducing a null action is necessary in the CDMK problem. In fact, in this problem, contexts play the role of revealing information and deterring unreasonable deals.

We consider the set of _non-anticipating_ strategies \(\). In particular, let \(_{t}\) be the history the agent could access at the start of round \(t\). Then, for any non-anticipating strategy \(\), \(a_{t}\) should depend only on \((_{t},_{t})\), that is, \(a_{t}=a_{t}^{}(_{t},_{t})\). For abbreviation, we write \(a_{t}^{}=a_{t}^{}(_{t},_{t})\) when there is no confusion.

Therefore, we can define the agent's optimization problem as below:

\[V^{}_{}\,_{^{T},^{T}}[_{t=1}^{T}r(_{t},a_{t}^{ },_{t})],\ _{t=1}^{T}(_{t},a_{t}^{},_{t})T,\, ^{T},^{T}.\]

The fluid benchmark.In practice, however, computing the expected reward of the optimal online strategy would require solving a high-dimension (probably infinite) dynamic programming, which is intractable. Hence, we turn to consider the fluid benchmark to measure the performance of a strategy, which is defined as follows:

\[V^{} T_{: A^{+} }\,_{}[_{a A^ {+}}R(,a)(,a)],\] \[\ \ _{}[_{a A^ {+}}(,a)(,a)];\,_{a A^{+}} (,a) 1,;\,(,a) 0,( ,a) A^{+}.\]

For a better understanding, \(V^{}\) reflects the maximum expected total rewards an agent can win when a static strategy is adopted and the resource constraints are only to be satisfied in expectation. Therefore, this optimization problem is a linear program in which the decision variable \((,a)\) represents the probability that the agent chooses action \(a\) upon seeing request \(\). It is a well-known result that \(V^{}\) gives an upper bound on \(V^{}\).

**Proposition 2.1** (Balseiro et al. ).: \(V^{} V^{}\)_._

Thus, we evaluate the performance of a non-anticipating strategy \(\) by comparing its expected accumulated reward \(Rew^{}\) with the fluid benchmark \(V^{}\), which is a common choice in literature . However, we prove that such a benchmark choice may lead to a \(()\) gap as long as \(V^{}\) is degenerate.

**Theorem 2.1** (Worst-case gap).: _When \(V^{}\) has a unique and degenerate optimal solution, \(V^{}-V^{}=()\)._

Despite the worst-case lower bound, we prove in this work that for any CDMK instance in which \(V^{}\) has a _unique non-degenerate_ optimal solution (cf. Assumption 3.1), we can obtain an \((1)\) gap compared to the fluid benchmark. Thus, it is still a good choice in most cases.

Information feedback model.In this work, we consider two types of information feedback models, with increasing difficulty obtaining a sample of the external factor \(\).

* [Full information feedback.] The agent is able to observe \(_{t}\) at the end of each round \(t\).
* [Partial information feedback.] The agent can observe \(_{t}\) at the end of round \(t\)_only if_\(a_{t} 0\).

In general, with full information feedback, the agent can observe an i.i.d. sample from \(\) each round, which is the optimal scenario for learning the distribution. Nevertheless, such an assumption could be strong since the reward and consumption vector are irrelevant to the external factor when the agent chooses the null action \(a=0\). Thereby, a more realistic information model is the partial feedback one, where the external factor is only accessible when \(a 0\). This limitation also increases the difficulty of learning the distribution \(\) since the agent observes fewer samples under this model than under full information feedback. It is important to note that the partial information model represents a transition from full to bandit information feedback, under which only the reward and consumption vector are accessible in each round, rather than the external factor. Real-life instances of partial information feedback include Examples 1.1 and 1.2, as we have discussed in the introduction.

## 3 The Re-Solving Heuristic

In this work, we introduce the re-solving heuristic to the CDMK problem. The resulting algorithm is presented in Algorithm 1.

To briefly describe the algorithm, we start by defining an optimization problem that captures the optimal fluid control for each round, assuming complete knowledge of \(\) and \(\). For any \(^{n}\), we define \(J()\) as the following optimization problem:

\[J()_{: A^{+} }_{}[_{a A^{+}}R(, a)(,a)],\] \[_{}[_{a A^{+}} (,a)(,a)];\;_{a A^{+}}( ,a) 1,;\,(,a) 0,( ,a) A^{+}.\]

Evidently, we have \(V^{}=T J()=T J(_{1})\) by definition. Intuitively, in each round \(t\), the best fluid choice of the agent is given by the optimal solution \(^{*}_{t}\) of LP \(J(_{t})\), where \(_{t}\) is the average budget of the remaining rounds, including round \(t\). Nevertheless, since full knowledge of the exact distributions \(\) and \(\) is lacking, the agent can only solve an estimated programming \((_{t},_{t})\) as outlined in Algorithm 1, with the following realization:

\[(_{t},_{t})_{:  A^{+}}_{}_{ t}}[_{a A^{+}}_{}_{t}} [r(,a,)](,a)],\] \[_{}_{t}}[_{a A^{+}} _{}_{t}}[(,a,) ](,a)]_{t};\;_{a A^{+}}(,a ) 1,;\,(,a) 0,( ,a) A^{+}.\]

Here, \(}_{t}\) and \(}_{t}\) represent the empirical distribution of \(\) and \(\), respectively, according to the sample history given by \(_{t}\). Specifically, let \(_{t}\) be the set of rounds that the agent accesses the external factor. The mass functions of these two estimated distributions are standard as follows: \(_{t}()t-1]}}{{t}-1};\;_{t}()_{t}]}}{{|_{t}|}}\).

It is worth noting that the estimated distribution of \(\), \(}_{t}\), is always based on \(t-1\) samples since the agent received an independent sample from \(\) at the beginning of each round. On the other hand, the empirical distribution of the external factor \(\), \(}_{t}\), is estimated from \(|_{t}|\) independent samples. With full information feedback, \(|_{t}|=t-1\); whereas with partial information feedback, \(|_{t}| t-1\) equals the number of times the agent chooses an action \(a 0\) before round \(t\). For brevity, for the estimated programming, we write \(}_{t}(,a)_{}_{t}}[(,a,)]\) and \(_{t}(,a)_{} _{t}}[r(,a,)]\).

As per Algorithm 1, the agent's decision mode in round \(t\) is given by the optimal solution \(_{t}^{*}\) of programming \((_{t},_{t})\). The algorithm stops when the resources are near depletion, that is, \(^{i}<1\) for some resource \(i[n]\), and we use \(T_{0}\) to denote the stopping time of Algorithm 1, i.e., \(T_{0}\{T,\{t: i[n],^{i}_{t+1}<1\}\}\).

For an analysis beyond the worst-case scenario, a crucial assumption we will make is that the fluid problem possesses good regularity properties, i.e., it is an LP with a unique and non-degenerate solution.

**Assumption 3.1**.: The optimal solution to \(J(_{1})\) is unique and non-degenerate.

As pointed out by Bumpensanti and Wang , uniqueness and non-degeneracy are a critical factor for an \(o()\) regret bound to hold in the CDMK problem, at least for the frequent re-solving technique we use in this work . Intuitively, if \(J(_{1})\) is degenerate, then with any minor error on the estimation, the optimal solution to \((_{t},_{t})\) can have a major different landscape with the optimal solution to \(J(_{1})\) in the sense of basic variables and binding constraints, and this will lead to an \(O()\) accumulated regret. For completeness, we formally define the above concepts.

**Definition 3.1**.: A context-action pair \((,a)\) is a _basic variable_ for \(J(_{1})\) if \(_{1}^{*}(,a)>0\), or else, it is a non-basic variable. Similarly, define basic/non-basic variables for \((_{t},_{t})\).

**Definition 3.2**.: \(i[n]\) is a _binding constraint_ for \(J(_{1})\) if \(_{,a A^{+}}u()^{i}(,a)_{1}^{*}( ,a)=_{1}^{i}\), or else it is a non-binding constraint. We let \(\{iJ(_{1})\}\), \(=[n]\), and we use \(|_{}\) or \(|_{}\) to define the sub-vector of \(\) confined on \(\) or \(\), respectively. Further, \(\) is a binding constraint for \(J(_{1})\) if \(_{a A^{+}}_{1}^{*}(,a)=1\), or else it is a non-binding constraint. Similarly, define binding/non-binding constraints for \((_{t},_{t})\).

As stated above, we want to guarantee that when the "distance" between \((_{t},_{t})\) and \(J(_{1})\) is sufficiently small, the optimal solution to these two programmings have the same landscapes. In this sense, we consider a _stability factor_\(D\) to measure such a threshold, as presented by Mangasarian and Shiau .

**Proposition 3.1** (Stability).: _Under Assumption 3.1, there is a maximum \(D>0\), such that when the following holds:_

\[\{\|(u()-_{t}())_{} \|_{},\|(v()-_{t}())_{}\|_{1 }\} D,\] (1) \[\{\|_{1}|_{S}-_{t}|_{ S}\|_{},\{_{1}|_{}-_{t}|_{}\}\} D,\]

\(J(_{1})\) _and \((_{t},_{t})\) share the same sets of basic/non-basic variables and binding/non-binding constraints._

With the assumption, below we present the main result of this work, which is proved in Appendix C.1.

**Theorem 3.1**.: _Under Assumption 3.1, with full information feedback, the expected accumulated reward \(Rew\) brought by Algorithm 1 when \(T\) satisfies:_

\[V^{}-Rew=O(+k}{D^{2}}),\]

_which is independent of \(T\)._

The intuition behind Theorem 3.1 is to conduct a regret decomposition in a Lagrangian manner, motivated by Chen et al. . This leads to three remaining terms (cf. Appendix C). For the first two Lagrangian product terms, thanks to Proposition 3.1, they equal 0 as long as the estimates of the distributions are sufficiently accurate with an error of \(O(D)\), which will happen with high probability after a constant number of rounds. The last term reflects how the stopping time of Algorithm 1 is close to the total time-span \(T\). On this front, we are left to demonstrate that the resources are spent smoothly. Intuitively, this property is guaranteed by combining two observations: (1) In each round, the action mode ensures that resources are spent evenly in expectation in the estimation world due to the re-solving step, and (2) the distance between the estimation world and the real world diminishes to zero, with the accumulation of samples. The complete reasoning is much more detailed.

We now compare Theorem 3.1 with results in prior work. We first mention that our benchmark \(V^{}\) is larger than the benchmark used in Slivkins and Foster  and Slivkins et al. , as proved in Appendix B. Thus, our result provides a stronger regret upper bound. As for the constants in the regret bound, first, our regret does not involve \(m\) explicitly. This is superior to existing results, which report an \(()\) reliance [8; 4; 36; 24]. As an intuitive reason, the number of actions does not explicitly appear in our Algorithm 1, but only contributes to the dimension of the linear program. However, \(m\) could appear in some complex and problem-specific constants that we omit in the bound. Interested readers can refer to Appendix C for more details. Second, although \(k\) does not always appear in previous works, this is inevitable in our bound, brought by the estimation error of the context distribution. Third, for the BwK problem, the well-known \(O( T)\) result given by Sankararaman and Slivkins  supposes that \(n 2\), and it is still unclear whether their analysis can be extended to an arbitrary number of resources. Our result does not suffer from such a limit. Finally, we remark that in the absence of resource constraints, \(D\) is precisely half the gap between the mean rewards of the best and second-best arms. Thus, \(D\) resembles the _reward-gap-like parameter_ in the multi-armed bandit literature. The dependence on \(D\) of our result is similar to the _first_ result in Sankararaman and Slivkins  and is the same with Chen et al. , and it is still unclear whether the dependence can be improved. We should also note that we omit the dependence of our regret bound on the unknown size of the external factor set in all our results, which could be improved via parameterized estimation techniques.

One key implication of Theorem 3.1 is that the re-solving heuristic's regret is independent of the number of rounds beyond the worst-case with full information. This result significantly improved over previous state-of-the-art results under mild assumptions, surpassing the solutions proposed by Han et al.  and Slivkins and Foster . In particular, their solutions come from the BwK literature and rely on dual update and upper confidence bound (UCB) heuristics, which only provide a worst-case regret of \(O()\).

## 4 Partial Information Feedback

We now shift to consider the re-solving method's performance with partial information feedback, under which the agent only sees the external factor \(_{t}\) when her choice is non-null in round \(t\), i.e.,\(a_{t} 0\). Apparently, with less information, the learning speed of the distribution \(\) decreases, hindering the re-solving procedure's quick convergence to an optimal solution. Nevertheless, we demonstrate that the performance of the re-solving method only faces an \(O( T)\) multiplicative degradation under partial information feedback. Our primary theorem in this section is as follows:

**Theorem 4.1**.: _Under Assumption 3.1, with partial information feedback, the expected accumulated reward \(Rew\) brought by Algorithm 1 when \(T\) satisfies:_

\[V^{}-Rew=O(+k+ T}{D^{2}}).\]

Before we come to the technical parts, we first place Theorem 4.1 within the literature. As previously mentioned, \(()\) is a worst-case lower bound on the regret even with full information feedback and thus also extends as a lower bound with partial information feedback. However, Theorem 4.1 steps beyond the worst case by providing an \(O( T)\) upper bound for regular problem instances. This result outperforms the universal \(O()\) regret by Han et al.  and Slivkins and Foster . While the result is asymptotically equivalent to that of Sankararaman and Slivkins , it imposes fewer restrictions on the problem structure, as previously discussed.

We now provide an intuitive understanding of the proof of Theorem 4.1. The crux lies in analyzing the frequency with which Algorithm 1 can access an independent sample of the external factor. To this end, we use \(Y_{t}=|_{t}| t-1\) to denote the number of times a non-null action is chosen before time \(t\), or equivalently, the number of i.i.d. samples from \(\) observed by the agent before time \(t\) under partial information feedback. The following crucial technical lemma provides a lower bound on \(Y_{t}\).

**Lemma 4.1**.: _There is a constant \(0<C_{b}<1/2\), such that with probability \(1-O(1/T)\), the following hold for Algorithm 1:_

1. _For any_ \(( T) t C_{b} T\)_,_ \(Y_{t} C_{f}(t-1)/ T\) _for some constant_ \(C_{f}\)_;_
2. _For any_ \(t>C_{b} T\)_,_ \(Y_{t} C_{r} T\) _for some constant_ \(C_{r}\)_._

The proof of Lemma 4.1 is deferred to Appendix D.2. In simple terms, during the initial \(( T)\) rounds (the shaded segment), the re-solving method cannot guarantee the accessing frequency since the learning of the request distribution \(\) has yet to converge sufficiently. However, after \(( T)\) rounds, Algorithm 1 ensures a constant probability of obtaining a new example in each round, provided that the remaining resources are sufficient. As a consequence, before \((T)\) rounds, we can guarantee an \((1/ T)\) accessing frequency at any time step and an overall \((1)\) frequency with high probability, as established by a concentration inequality. The remaining proof of Theorem 4.1 is provided in Appendix D.1.

## 5 Relaxing the Regularity Assumption - A Worst-Case Guarantee

In Sections 3 and 4, we have proved that Algorithm 1 can achieve an \((1)\) regret for CDMK problems under full and partial information feedbacks, assuming certain regular conditions (cf. Assumption 3.1). Put differently, the re-solving heuristic nicely deals with regular scenarios. In this section, we complement the above by showing that this method can attain nearly optimal regret in the worst cases. Furthermore, in Appendix A, we extend our analysis to cases where the context and external factor distributions can be continuous.

Our main results are given below, and their proofs are provided in Appendices E.1 and E.2, respectively.

**Theorem 5.1**.: _With full information feedback, the expected accumulated reward \(Rew\) brought by Algorithm 1 satisfies: \(V^{}-Rew=O(k+n),\) as \(T\)._

**Theorem 5.2**.: _With partial information feedback, the expected accumulated reward \(Rew\) brought by Algorithm 1 satisfies: \(V^{}-Rew=O(k T+n),\) as \(T\)._

As given by Theorem 2.1, the worst-case regret of any online CDMK algorithm is \(()\), while Theorems 5.1 and 5.2 indicate that the re-solving heuristic reaches near-optimality in such cases. Further, state-of-the-art algorithms  can at most obtain an \(()\) regret even with full/partial information feedback. Our algorithm also achieves this near-optimal regret bound in worst cases.

It is worth noticing that we omit some problem-specific constants in the previous bounds, e.g., the fluid optimum \(J(_{1})\), which could be related to the number of non-null arms \(m\). Therefore, our results do not conflict with the well-known \(()\) regret lower bound as given by Auer et al. .

## 6 Numerical Validations

In this section, we use numerical experiments to verify our analysis. We perform our simulation experiments with either full or partial information feedback under two cases. The first is with a degenerate optimal solution, and the second is with a unique and non-degenerate optimal solution. We delay more details, including the choice of the problem instances, to Appendix F.

Figure 1 describes the relationship between the regret and the number of total rounds \(T\) under all four settings. We set the horizon \(T\) to be \(2000 2^{k}\) for integer \(0 k 5\). The figure displays both the sample mean (the line) and the 99%-confidence interval (the light color zone) calculated by the results of \(50\) estimations for the regret, where each estimation comprises \(400\) independent trials. Observe that when the LP \(J()\) is degenerate, the regret grows on the order of \(()\) under both full information and partial information settings. Further, when the underlying LP \(J()\) has a unique and non-degenerate optimal solution, the regret does not scale with \(T\) under the full information setting. In the partial information setting, the regret slowly grows with \(T\), which matches our \((1)\) theoretical guarantee.

## 7 Concluding Remarks

This work establishes the effectiveness of the re-solving heuristic in the contextual decision-making problem with knapsack constraints. We first prove that the gap between the fluid optimum and online optimum is \(()\) when the fluid LP has a unique and degenerate optimal solution. Further, we show that the re-solving method reaches an \(O(1)\) regret with full information and an \(O( T)\) regret with partial information when the fluid LP has a unique and non-degenerate optimal solution, even compared to the fluid benchmark. Considering the sufficient condition for the \(()\) lower bound, our non-degeneracy assumption is mild, especially when comparing with the two-resource condition required in Sankararaman and Slivkins .

Further, we show that even in worst cases, the re-solving method achieves an \(O()\) regret with full information feedback and an \(O( T)\) regret with partial information feedback. These results are comparable to start-of-the-art results [24; 36]. We also extend our analysis to the continuous randomness case for completeness.