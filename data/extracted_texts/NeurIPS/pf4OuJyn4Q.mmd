# Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms

Rafael Rafailov

Stanford University

rafailov@cs.stanford.edu

&Yaswanth Chittepu1

UMass Amherstychittepu@umass.edu

&Ryan Park1

Stanford University

rypark@stanford.edu

Harshit Sikchi1

UT Austin

hsikchi@utexas.edu

&Joey Hejna1

Stanford University

jhejna@cs.stanford.edu

&W. Bradley Knox

UT Austin

bradknox@cs.utexas.edu

&Chelsea Finn

Stanford University

cbfinn@cs.stanford.edu

&Scott Niekum

UMass Amherst

sniekum@cs.umass.edu

Equal Contribution, Dice Rolling

###### Abstract

Reinforcement Learning from Human Feedback (RLHF) has been crucial to the recent success of Large Language Models (LLMs), however, it is often a complex and brittle process. In the classical RLHF framework, a reward model is first trained to represent human preferences, which is in turn used by an online reinforcement learning (RL) algorithm to optimize the LLM. A prominent issue with such methods is _reward over-optimization_ or _reward hacking_, where performance as measured by the learned proxy reward model increases, but true quality plateaus or even deteriorates. Direct Alignment Algorithms (DAAs) like Direct Preference Optimization have emerged as alternatives to the classical RLHF pipeline by circumventing the reward modeling phase. However, although DAAs do not use a separate proxy reward model, they still commonly deteriorate from over-optimization. While the so-called reward hacking phenomenon is not well-defined for DAAs, we still uncover similar trends: at higher KL budgets, DAA algorithms exhibit similar degradation patterns to their classic RLHF counterparts. In particular, we find that DAA methods deteriorate not only across a wide range of KL budgets but also often before even a single epoch of the dataset is completed. Through extensive empirical experimentation, this work formulates and formalizes the reward over-optimization or hacking problem for DAAs and explores its consequences across objectives, training regimes, and model scales.

## 1 Introduction

Recent advancements in Large Language Models (LLMs) have broadened their capabilities significantly, enabling applications in code generation, mathematical reasoning, tool use, and interactive communication. These improvements have popularized LLMs across various domains. Reinforcement Learning from Human Feedback (RLHF) has been instrumental in these advances and is now integral to sophisticated LLM training regimes . Before alignment, LLMs, trained on vast text corpses to predict subsequent tokens  are often unwieldy and hard to use. Today, leading LLMs incorporate variants of the RLHF framework  to align them with human intent, whichgenerally involves a multi-stage process. Specifically, users evaluate model responses to assorted prompts in order to train a reward model that encapsulates human preferences [10; 55; 72; 5; 62]. Then, the refined LLM maximizes the expected learned reward function using a reinforcement learning (RL) algorithm [50; 1; 65]. Despite its efficacy, this procedure is complex and computationally intensive, particularly in its latter stages.

Goodhart's Law [25; 11], that "when a measure becomes a target, it ceases to be a good measure", has often been cited as a core shortcoming of RLHF. Standard RLHF methods optimize a learned, but imperfect reward function which ends up amplifying the reward model's shortcomings. Empirically, this phenomenon was first extensively characterized by Gao et al. , who coined the term "reward over-optimization", and has been seen consistently in recent findings [62; 16; 14]. While reward over-optimization has been studied in the context of the aforementioned RLHF procedure, recent contemporary methods for aligning LLMs circumvent the reward learning procedure, necessitating a new characterization of the over-optimization phenomena.

This new broad class of algorithms, which we refer to as Direct Alignment Algorithms (DAAs), bypass the traditional RLHF pipeline by re-parameterizing the reward model directly through the optimal policy derived during the reinforcement learning phase. DAA methods, like Direct Preference Optimization , have gained popularity [14; 28] as they often reduce computational demands. Yet, despite not fitting a reward function, DAAs still exhibit over-optimization trends similar to those of traditional RLHF methods using a learned reward function. In some sense, this is puzzling: DAAs can be viewed as simply learning a reward function with supervised learning from which the optimal policy is deterministically mapped, however more seems to be at play than simple supervised learning.

In this work, we investigate the over-fitting phenomena present in DAA algorithms through extensive experimentation. First, we unify a number of different recent methods [46; 68; 4] under the DAA framework. Then, across different model scales and hyper-parameters, we show that DAAs exhibit a type of reward over-optimization consistent with that previously observed in RLHF . Specifically, we find that at different KL-divergence budgets DAAs exhibit degradation patterns similar to those found in RLHF. Interestingly, we also find that performance within a single epoch is not always as consistent as expected for DAAs. Finally, we explain why this happens by appealing to the under-constrained nature of the optimization problem used in DAAs.

## 2 Preliminaries

In this section, we first outline the core components of the standard RLHF pipeline [72; 55; 5; 41]). Then, we examine prior literature to characterize the reward over-optimization exhibited by standard RLHF methods. Finally, we provide a unifying view of direct alignment algorithms (DAAs) which will guide our analysis of their training dynamics in the next section.

### Reinforcement Learning From Human Feedback

The standard RLHF pipeline consists of three distinct stages with the goal of aligning the LLM with human preferences.

**Supervised Fine Tuning (SFT)**: First, a dataset of prompts \(x\) and high-quality answers \(y\) are used to train an LLM for instruction following via maximum likelihood estimation over next-tokens. We refer to the resultant model as \(_{}(y|x)\) and consider the entire prompt and answer strings to be single variables.

**Reward Modeling**: Second, the SFT model \(_{}(y|x)\) is used to learn a reward function over human preferences. Specifically, the SFT model is queried to produce pairs of answers \((y_{1},y_{2})_{}(y|x)\), for every prompt \(x\) in a dataset. Then, users select their preferred answers, resulting in ranking \(y_{w} y_{l} x\) where \(y_{w}\) and \(y_{l}\) are the preferred and dispreferred answers respectively. Typically, user rankings are assumed to be distributed according to the Bradley-Terry (BT) model 

\[p(y_{1} y_{2} x)=))}}{))}+))}}=(r(x,y_{1})-r(x,y_{2}))\] (1)

where the preference distribution \(p\) results from an unobserved latent reward \(r(x,y)\), and \(\) is the logistic function. Given this model and a dataset of rankings, denoted \(=\{x^{(i)},y_{w}^{(i)},y_{l}^{(i)}\}_{i=1}^{N}\), we can train a parameterized model \(r_{}(x,y)\) to predict the unobserved reward using maximum likelihood estimation. This yields the following loss function,

\[_{}(r_{})=-_{(x,y_{w},y_{l})} (r_{}(x,y_{w})-r_{}(x,y_{l})).\] (2)

**Reinforcement Learning (RL)**: The final stage of the standard RLHF pipeline uses the learned reward model \(r_{}(x,y)\) to update the LLM \(_{}\) with an on-policy RL algorithm like PPO , optimizing the model to provide responses more preferred by human raters. The most common objective is

\[_{_{}}_{x,y_{}(|x)} r_{}(x,y)-_{}_{}(y  x)_{}(y|x)\] (3)

which enforces a Kullback-Leibler (KL) divergence penalty with a reference distribution \(_{}(y|x)\) (usually taken to be \(_{}(y|x)\)) to prevent the LLM \(_{}\) from straying too far from its initialization. Thus, the hyper-parameter \(\) directly trades off exploiting the reward function and deviating from \(_{}(y|x)\).

### Reward Exploitation in RLHF

Unfortunately, repeating the above procedure without careful tuning of the RL phase can lead to disastrous performance. This is because in the context of RLHF the LLM policy is optimizing the surrogate reward estimate \(r_{}(x,y)\) and not the true reward function as is often the case in other domains. Thus, prior works have observed that while the LLM's expected reward according to eq. (3) increases the actual quality of the model's outputs can decrease [54; 43; 9; 34]. This particular instantiation of the reward exploitation or hacking problem  is often referred to as reward "over-optimization" in RLHF literature and has been studied empirically in both controlled experiments  and user studies . There are two prevailing explanations for why this phenomenon occurs.

**1. OOD Robustness:** In the classical RLHF pipeline, the RL objective (eq. (3)) is optimized using on-policy samples from \(_{}\). This means that the reward function is continuously queried using unseen model samples which are potentially out-of-distribution. Beyond the support of the reward modeling distribution, \(r_{}\) may assign high rewards to sub-par responses, leading the policy to believe it is doing well when it may not be. While the KL-regularization term is designed to prevent the model from drifting too far out of distribution, this term alone has proven inadequate to prevent reward hacking .

**2. Reward Mis-specification.** Learned reward functions may exhibit spurious correlations that cause them to prefer unintended behaviors. While this issue is not at the forefront of LLM research, it is known to be pervasive in RL [43; 34]. Most efforts to address these problems exist at the intersection of robustness and offline RL literature [13; 67; 16] and use measures of epistemic uncertainty to penalize the predicted reward.

### Direct Alignment Algorithms

Due to its complex multi-step nature, recent works have sought alternatives to the classic RLHF pipeline. A new class of algorithms, which we broadly classify as Direct Alignment Algorithms (DAAs), directly update the LLM's policy \(_{}\) using user feedback instead of fitting a reward function to it and then employing an RL algorithm. Perhaps the most known example is Direct Preference Optimization (DPO). DPO, as well as other DAAs, are derived using the closed form solution to the RLHF objective in eq. (3) , \(^{*}(y|x)_{}(y|x)e^{r(x,y)/}\), where \(r(x,y)\) is the ground-truth reward. By isolating \(r(x,y)\) in this relationship and substituting it into the reward optimization objective in eq. (2), we arrive at a general objective that allows us to train the LLM directly using feedback data:

\[_{}(_{};_{})= _{(x,y_{w},y_{l})}g (y_{w} x)}{_{}(y_{w} x)}- (y_{l} x)}{_{}(y_{l} x )}\] (4)

where \(g\) is a convex loss function. Using \(g(x)=-(x)\) coincides with the standard Bradley-Terry model and the original DPO objective. Other methods choose different loss functions: IPO  uses the quadratic objective \(g(x)=(x-1)^{2}\) and SLiC-HF [68; 38] uses the hinge loss \(g(x)=(0,1-x)\). Additional objectives were also considered in , but due to limited computational resources, we focus on the three objectives outlined above.

Crucially, the DAA approach allows us to recover the optimal policy using a straightforward classification loss without the need for learning a reward function, on-policy sampling, or RL, which can be notoriously difficult to tune and computationally expensive. Because of this, DAAs have emerged as a popular alternative. However, just like classical RLHF methods, DAAs exhibit strong over-fitting and even reward-hacking like behaviors. For example, Park et al.  show that LLMs trained with DPO generate responses with increasing length throughout the course of training, but do not improve in ground-truth win-rate after a certain point. Since DAAs do not explicitly learn a reward function, it is unclear how "reward-overoptimization" fits into the picture. In this work, we aim to shed some light on this phenomenon in DAAs.

## 3 Empirical Analysis of Overoptimization in DAAs

First, we examine the over-optimization problem in DAAs and compare it to those observed in traditional RLHF methods. All our experiments are carried out using the Reddit TL;DR summarization dataset  and the Pythia family of Large Language Models . Additional plots illustrating similar over-optimization trends for Direct Alignment Algorithms on the Gemma2-2b model  and the Anthropic Helpfulness-Harmlessness dataset  are provided in Appendix F

### Evaluating Model-Overoptimization

In our first set of experiments, we evaluate the reward model over-optimization phenomenon. We evaluate three training objectives DPO, IPO, and SLiC using seven \(\) parameters, representing different KL budgets at three model sizes - 1B, 2.8B, and 6.9B. Our main results are shown in Fig. 1 which presents results for different configurations after 1 epoch of training (row 1) and including 4 uniform intermediate checkpoints (row 2). We include additional results on the training dynamics in Fig. 2, which shows win rates and KL bounds for intra-epoch training. We present our findings below.

**Model Over-Optimization:** We see clear over-optimization for all objectives as performance exhibits a hump-shaped pattern, where an additional increase in the KL budget leads to decreasing model performance. Moreover in Fig. 2 we observe similar intra-epoch training dynamics patterns as configurations with wider KL budgets achieve their best performance after training on only 25% of the data, after which performance starts decreasing in conjunction with increasing KL divergence metrics.

Figure 1: Results on over-optimization in Direct Alignment Algorithms for DPO, IPO and SLiC. Results show model win-rates over the dataset summary on an evaluation set of prompts as judged by GPT-4. The top row shows the final performance after 1 epoch of training, while the second row also includes 4 intermediate checkpoints as well. The fitted dotted curves utilize scaling laws from  applied to direct alignment, with GPT4 winrates taking the place of the gold reward model score.

**Effect of Training Objective:** In the IPO work  the authors present theoretical arguments that due to the monotone sigmoid objective in the DPO formulation, the KL constraint is not effectively enforced and propose the quadratic fixed-margin loss as an alternative. Across all objectives, there are clear dependencies between the \(\) parameter and the corresponding KL achieved at the end of training. While DPO and SLiC exhibit similar performance, IPO indeed seems to be less prone to over-optimization and in general, achieve lower KLs under the same constraint. Our observations with IPO also align with prior works in preference-based RL and imitation learning where imposing a fixed margin led to more stable and performant methods [48; 51].

**Effect of Model Size:** The results also show a strong parameter count scaling effect. The Pythia 1B model achieves low performance under the same set of constraints it reaches much higher KL values, while almost immediately exhibiting signs of over-optimization. This behavior holds under all three objectives. At larger scales, the 6.9B Pythia model tends to exhibit more win-rate - KL trade-offs and be less prone to over-optimization, with both models significantly outperforming the 1B model. In the case of the IPO objective, the 6.9B also exhibits significantly better control over the KL objective and shows little to no over-optimization behavior.

### Scaling Law Fits

Given we have established a framework for evaluating over-optimization in DAAs and empirically validated it (section 3.1), we now develop scaling laws for this phenomenon. Previous work in classical RLHF has established such scaling laws for reward model scores as a function of the KL divergence between initial and optimized policies . The relevant functional of the reward \(R(d)\) is

\[R(d)=d(- d)\] (5)

where \(,\) are constants dependent on the size of the reward model dataset and parameter count, and \(d=}(||_{})}\). As DAAs do not train a proxy reward model, we treat GPT4 wirates over dataset completions as a proxy for gold reward. Somewhat surprisingly, we find that this scaling law accurately relates \(d\) and wirates for DAAs. Compared to a quadratic fit between \(D_{}(||_{})\) and wirates, this scaling law halves the RMSE. It is worth noting, however, that a quadratic fit between \(d\) and wirates yields a similar error compared to Equation 5.

Figure 2: Results on intra-epoch optimization dynamics. The top row shows win-rates against the fraction of an epoch so far, while the bottom row shows the corresponding KL values. Under a lower KL constraint, most experiments reach their best performance in the first 25% of the epoch and degrade with additional training, while the model deviates from the reference under increasing KL. All models are 6.9B and vary across DPO, SLiC, and IPO loss formulations.

### Length Correlations

Prior work  has shown that the DPO algorithm is prone to length exploitation as it amplifies verbosity biases in preference datasets. Here we show that length is not the only dimension on which exploitation can occur. Our experimental results are shown in Fig. 3. On the left, we show results for the 2.8B Pythia model with standard training plus the length-regularization approach from . Both approaches suffer from over-optimization, but the dynamics differ depending on the KL budget. Moreover, even though the regularized model achieves higher win rates on a length-correct basis, it under-performs the model trained with the standard objective in the lower KL constraint region.

Recent work  has also shown that DAAs prioritize features of the data based on their complexity and prevalence (with length a clear example of human datasets).  further showed that models trained with the DPO algorithm extrapolate significantly based on length. We extend this analysis in Fig, 3 (right). We consider a linear regression of the form

\[(y^{(i)}|x^{(i)})}{_{ref}(y^{(i)}|x^{(i)})}=|y^{(i)}|+^{(i)}\] (6)

where \(x^{(i)}\) are held-out prompts and \(y^{(i)}\) are samples from the corresponding model between the DPO implicit reward and length. We fit a different regression for each model size and checkpoint and plot the corresponding \(R^{2}\) values. We observe two main effects; first, there is a clear scaling law behavior. Weaker models extrapolate across the simple length feature to a much higher degree than stronger ones. This is especially clear when comparing the behavior of the Pythia 1B versus the 2.8B and 6.9B models. Second, we see significant effects based on the KL budget - under a smaller budget all model sizes exhibit higher extrapolation behavior. Based on these results we formulate the hypothesis that under limited capacity, either from model capability or limited KL budgets, the model will extrapolate more strongly based on simpler features, which can lead to OOD issues.

### Reward Metrics Correlations

Prior works have measured reward model quality in ranking settings by classification accuracy. We evaluate the relationship between the DAA implicit reward model accuracy and policy performance in Figure 4. The DPO and SLiC algorithms exhibit little to no correlation between reward model accuracy and downstream model performance. The IPO model shows a weak positive relationship, but upon further examination, this is entirely due to model size scaling - stronger models both fit the data better and produce better generations as well, however within each particular model size, there is no discernible relationship between the DAA implicit reward accuracy and the actual policy performance. Similar observations hold when comparing the empirical DAA loss with model performance, which is contrary to observations in supervised pre-training and instruction tuning .

Figure 3: **Left:** KL budget versus win-rates (over dataset human answer) with and without length-regularization . While including a length correction in the optimization objective changes the KL-win-rate Pareto Frontier, it does not alleviate reward over-optimization and might even exacerbate it. **Right:** Scaling behavior for length extrapolation - smaller capacity models (either by size or KL budget) extrapolate more strongly on simpler features such as length.

### Decreasing Likelihoods and Model Performance

A number of recent works have observed that the implicit DAA rewards of both preferred and dis-preferred responses decrease during training, which may be counter-intuitive. In  the authors make a counter-point that in offline training of DAAs \(_{}\) is usually pre-trained with SFT on the preferred response and thus

\[_{p_{}(y_{w}|x)}[(y_{w}|x)}{ _{}(y_{w}|x)}]_{_{}(y_{w}|x)} [(y_{w}|x)}{_{}(y_{w}|x)}]=- _{}_{}(y|x)\ \|\ _{}(y\ |\ x)\] (7)

where \(p_{}(y^{w}|x)\) is the dataset distribution of preferred answers. That is the expected implicit reward represents a forward KL divergence between the reference policy and the optimization policy, thus it is expected to be negative and decrease with training as the optimization model moves away from the reference. In this section, we study whether this empirical phenomenon presents a challenge for DAA learning. Similar to Fig. 1 we plot the win rates against the square-root-transformed (negative) expected implicit reward of the preferred response (evaluated on a held-out evaluation dataset), which as stated above approximates the (square-root-transformed) forward KL \(_{}_{}(y|x)\ \|\ _{}(y\ |\ x)\). Results are included in Fig. 5, which follow closely the pattern in Fig. 1 with performance initially increasing before it starts dipping down after a certain threshold. This indicates that under the standard DAA training pipeline decreasing likelihoods are not necessarily an issue for performance, and are even necessary for improvement, but exhibit non-linear over-optimization dynamics.

## 4 Reward Exploitation in Direct Alignment Algorithms

While the phenomena observed in the previous section echo those observed in classical RLHF, their underlying causes may be distinct. Reward over-optimization in classical RLHF is largely attributed to querying a proxy reward function that is potentially OOD, while DAAs do not train a separate reward model. Instead, DAAs are generally understood as fitting an "implicit" reward model to preference data with the parameterization \(r_{}(x,y)=(y|x)}{_{}(y|x)}\) using the objective in eq. (2). Therefore, the OOD behavior of the policy is inextricably linked to the OOD behavior of the implicit reward model. We demonstrate below that the reward modeling objective used is heavily under-constrained, allowing for a potentially large number of solutions that can place weight on OOD responses. This is especially problematic for DAAs which deterministically map the optimal policy from the "implicit" reward.

**Rank Deficiency with Finite Preferences.** In DAAs, the language modeling problem is treated as a contextual bandit. However, the space of possible prompts \(x\) and answers \(y\) are both

Figure 4: **Top:** We plot the DAA implicit reward accuracy in preference classification versus win rates. **Bottom:** DAA optimization loss versus checkpoint win rates. Model training statistics, do not exhibit a strong relationship with downstream performance.

exponentially large in sequence length. However, as highlighted by Tang et al. , DAAs often assume full support of the reference distribution when mapping from the implicit reward to the optimal policy \(\) by eq.10. However, in practice such coverage is impossible. Instead, preference datasets cover a minuscule portion of the prompt-response space. Unfortunately, as DAA objectives are not strictly convex, their loss functions (eq.4) can have multiple global optimas, which may be undesirable. We demonstrate this below, using the regression interpretation from Hejna et al. .

First, we re-write the DAA objective from eq.4 using vectors in the prompt-response space \(\). Each preference query in the comparison dataset can be written as difference between indicator vectors, specifically \(q_{i}=\{(x,y)=(x^{(i)},y_{w}^{(i)})\}-\{(x,y)=(x^{(i)},y_ {l}^{(i)})\}\). This "query" vector simply selects the comparison from the prompt response space, with the entree corresponding to \((x,y^{w})\) being +1 and the entree corresponding to \((x,y^{l})\) being -1. Similarly, we can consider the learned policy to be a vector \(-_{}\), to which the distributional constraint also applies in practice. Our generalized DAA loss function can then be re-written as

\[_{}(_{},)=_{i=1}^{||}g ( q_{i}^{}((y|x)-_{}(y|x)) ),q_{i}[x,y]=1&(x,y)=(x^{(i)},y_{l}^{(i)})\\ -1&(x,y)=(x^{(i)},y_{l}^{(i)})\\ 0&\]

with finite data. Choosing \(g\) to be the negative log sigmoid above recovers DPO with finite preferences, but also logistic regression with a data matrix \(Q\) of shape \(||\) by \(||\) constructed by stacking the aforementioned query vectors \(q\). As \(||>>||\), this matrix is likely to have a non-trivial null space, making the problem not strictly convex. Thus, there are many possible policies \(\) that can achieve the same optima, some of which will place a high weight on out-of-distribution responses due to the distributional constraint of policy . To demonstrate this, we formalize the construction below.

**Proposition 1**: _(Adapted from Hejna et al. ) Let \(S\) be the set of win-or-lose prompt-response vectors \((x,y)\) in \(\). Provided:_

1. _The intersection of the null space of_ \(Q\)_,_ \(N(Q)\)_, and the span of_ \(S\)_,_ \((S)\)_, is non-trivial._
2. _For every_ \(x\) _there exists a response_ \(y_{}\) _that is not in the data,_ \((x,y_{}) S\)_._

_Then, there are infinite number of minima to eq.4 which place weight on out-of-distribution responses \(y\)._

Figure 5: Over-optimization results for \(\) be the minima of the DAA loss function. Choose a vector \(u\) such that \(u N(Q)\), \(u(S)\), and \(u\) has at least one negative component. Modifying the log policy vector as \(+u\) will not affect the DAA loss, as \(u\) is in the null space of \(Q\), but the log-probability of the policy will decrease for least one prompt-response pair in \(S\) by construction. However, \(e^{+u}\) may not integrate to one. To fix this, we can construct a second vector \(v N(Q)\) using the \(y_{}\) at each \(x\) such that \(e^{+u+v}\) integrates to one. For more details, we refer the reader to Hejna et al.  Appendix A.3.

The second constraint of proposition 1 is often trivially satisfied by the dimension of the response space as we are unlikely to see _every_ response to a prompt. The first constraint is harder, but can be satisfied by conflicting preferences. A trivial example which satisfies these constraints is a simple MDP in which there is only a single state (or prompt \(x\)), but three possible actions (or responses), \(y_{1}\), \(y_{2}\), and \(y_{3}\). If we construct the preference dataset \(=\{(y_{1} y_{2}),(y_{2} y_{1})\}\), omitting \(x\) for brevity, then we satisfy the above conditions: the null space of \(Q\) is non trivial in span of \(y_{1}\) and \(y_{2}\) and there is an out-of-distribution action \(y_{3}\). In this setting, the DPO loss is minimized by both \((y|x)=(0.5,0.5,0)\) and \((y|x)=(0.0,0.0,1.0)\). In fact, it is minimized by infinitely many policies which place equal weight on \(y_{1}\) and \(y_{2}\). To demonstrate this effect in higher dimensions across a number of different DAA methods, we conduct experiments in a Toy MDP which bears resemblance to the language modeling setting.

**Understanding OOD behavior for DAA algorithms with a Toy MDP**: To illustrate that DAA algorithms, in general and not an artifact of training LLM's, end up placing probability mass on OOD sequences during training we design a simple Tree MDP (shown in Figure 6) to mimic the token-level MDP in LLMs. We use a dataset containing a single preference between two trajectories and follow the standard procedure of running SFT on preferred responses before updating an RNN policy using a DAA. Figure 7 shows that even in this simple setup, popular DAAs (DPO/IPO/SLiC) end up extrapolating incorrectly out of distribution revealing a fundamental shortcoming. Unlike in standard RLHF, the non-strict convexity of the reward function in DAAs ends up directly affecting the policy. Detailed experimental details can be found in Appendix E.

## 5 Related Work

Broadly, over-optimization has been a widely studied phenomenon across different settings [60; 18]. Over-fitting can be characterized as over-optimization in the supervised learning setting [39; 32], which can harm generalization [19; 12; 24] or lead to susceptibility to adversarial attacks [56; 37; 15]. Reward hacking in reinforcement learning (RL) , where an agent maximizes its reward through

Figure 7: (Top row) Probability of OOD trajectories. DAA algorithms end up placing a substantial probability mass of some of the OOD trajectories during training. (Bottom row) Probability of in-distribution (preference-pair) trajectories decreases during training.

behavior that deviates from the intended goal, can be viewed as a different type of over-optimization, commonly observed in prior work [43; 3; 22].

We study over-optimization in the context of aligning LLMs with human feedback, for which the most common approach is RLHF as outlined in section 2.1. Similar RLHF techniques were originally pioneered for control [31; 2; 10]. Standard RLHF methods suffer from both potential over-fitting of the reward function and reward exploitation by the RL algorithm. Several works have considered how to reduce over-fitting or increase the robustness of learned reward functions using ensembles [13; 67; 16] or data smoothing . Other approaches, like Moskovitz et al.  consider how reward exploitation can be reduced by using different optimization techniques in the RL stage. Much of this work is motivated by Gao et al. , which first characterized and provided scaling laws for over-optimization in RLHF.

Unlike Gao et al. , we consider the over-optimization problem in DAAs, which differs significantly from the standard RLHF pipeline.

Different DAAs have been derived theoretically [47; 46; 68; 4; 64], and applied to problems beyond language modeling like image generation  and control . In all of these scenarios, over-optimization problems have persisted. Park et al.  show that DAAs commonly over-fit to length and the expense of performance, which has been linked to inherent bias in training data [53; 29]. Other works have tried to allow DAAs to use more types of data like demonstrations  or ratings  to get better performance. Recently, incorporating online data has proven critical to improving performance [66; 26; 57]. Concurrent to our work, Tang et al.  study the differences between offline DAAs and standard RLHF methods. Unlike us, they focus on comparisons with online sampling whereas we focus on the purely offline setting.

## 6 Conclusion

In this work, we present an analysis of the over-optimization problem in Direct Alignment Algorithms. Through extensive experimentation on different algorithms (DPO, IPO, SLIC) and at different model scales (1B, 2.8B, 6.9B), we observe consistent over-optimization trends at different KL-divergence budgets. While our analysis is a first step, it is not a complete picture of understanding the over-optimization phenomena. More work can be done characterizing this effect at larger model scales, which we were unable to do due to computational limitations. Nevertheless, we believe our work sheds light on important problems in Direct Alignment Algorithms that can spur future research.

#### Acknowledgments

This work has taken place in part in the Safe, Correct, and Aligned Learning and Robotics Lab (SCALAR) at The University of Massachusetts Amherst. SCALAR research is supported in part by the NSF (IIS-2323384), AFOSR (FA9550-20-1-0077), and the Center for AI Safety (CAIS). This work has taken place in part in the Rewarding Lab at UT Austin. The Rewarding Lab is supported by NSF (IIS-2402650), ONR (N00014-22-1-2204), EA Ventures, Bosch, UT Austin's Good Systems grand challenge, and Open Philanthropy.