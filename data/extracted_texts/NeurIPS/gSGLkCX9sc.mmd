# Automated Label Unification for Multi-Dataset

Semantic Segmentation with GNNs

Rong Ma, Jie Chen, Xiangyang Xue, and Jian Pu

Fudan University

rma22@m.fudan.edu.cn, {chenj19,xyxue,jianpu}@fudan.edu.cn

These authors contributed to the work equally and should be regarded as co-first authors.Corresponding author.

###### Abstract

Deep supervised models possess significant capability to assimilate extensive training data, thereby presenting an opportunity to enhance model performance through training on multiple datasets. However, conflicts arising from different label spaces among datasets may adversely affect model performance. In this paper, we propose a novel approach to automatically construct a unified label space across multiple datasets using graph neural networks. This enables semantic segmentation models to be trained simultaneously on multiple datasets, resulting in performance improvements. Unlike existing methods, our approach facilitates seamless training without the need for additional manual reannotation or taxonomy reconciliation. This significantly enhances the efficiency and effectiveness of multi-dataset segmentation model training. The results demonstrate that our method significantly outperforms other multi-dataset training methods when trained on seven datasets simultaneously, and achieves state-of-the-art performance on the WildDash 2 benchmark. Our code can be found in https://github.com/Mrhonor/AutoUniSeg.

## 1 Introduction

Recent advances in computer vision  have shown the advantages of large datasets in training robust visual models . However, for deep supervised visual models that rely on annotated data, the collection of such extensive annotated datasets can be prohibitively costly . To address this expense and expand the data available for training, several efforts  focus on the challenges of multi-dataset training, enabling the use of diverse datasets to train more robust and generalizable models.

Models trained on multiple datasets must confront the challenge of reconciling conflicting annotation standards and label spaces. For example, the class _road_ in the BDD dataset  can be further divided into several classes in the Mapillary dataset : _road_, _lane marking_ and _crosswalk_. Similarly, the Mapillary dataset labels both _barrier_ and _curb_ as distinct classes, while in the IDD dataset , they are combined under the single label _curb_. These conflicts impact the supervised learning of models, as they may be incorrectly penalized for predicting finer-grained classes from other datasets.

Another challenge is the task of unifying diverse dataset labels to produce outputs in a standardized format . Several methods  attempt to address this by concatenating the label spaces of all datasets and using language models to encode label names into a text embedding space. However, there approaches introduce redundancy and fail to handle issues where labels share names but differ in annotation granularity. Other methods involve manually constructing universal taxonomies  or relabeling , both of which are time-consuming and labor-intensive. Recent approaches aim toautomatically construct universal taxonomies [6; 41], but these methods typically identify inter-label relations between only two datasets, involving a time-consuming iterative training process.

In this paper, we propose a novel approach leveraging Graph Neural Networks (GNNs)  to automatically construct unified label space, enabling segmentation model to be trained simultaneously on multiple datasets. In contrast to previous approaches [11; 4; 6], our approach eliminates the need for manual re-annotation or iterative training procedures to construct universal taxonomies, while also addressing the limitation of language-based methods in distinguishing categories with identical semantics. As depicted in Figure 1, we utilize a language model to convert the dataset labels into text features. Then, we apply GNNs to learn the relationships and associations among these labels. The process creates a unified label embedding space and dataset label mappings. The output head of the segmentation network incorporates the unified label embedding space to generate unified segmentation results within this unified space. The dataset label mappings are subsequently utilized to align the unified segmentation results with the label spaces relevant to each dataset. This enables the training of segmentation models and graph neural networks with dataset-specific annotations.

## 2 Related work

**Multi-datasets Semantic Segmentation.** In recent years, numerous studies [52; 29] focused on training semantic segmentation models on multiple datasets. A simple approach involved incorporating dataset-specific modules in the model, such as dataset-specific output heads  or dataset-specific batch normalization layers , to produce predictions tailored to each dataset domain. While these methods effectively avoided issues of dataset label space conflicts, they offered limited applicability in real-world scenarios as they could not deliver unified predictions. MSeg  addressed the problem at the data level via manual re-annotation processes to resolve label conflicts. However, this approach was time-consuming, error-prone and not easily scalable. Recent methods employed manual  or automatic techniques  to construct universal taxonomies and establish label relationships between datasets label space and the unified label space. These methods, leveraging partial label learning approaches [48; 12], enabled training with dataset-specific annotations and producing unified predictions.

**Construct Universal Taxonomies.** Each dataset has its unique domain, necessitating the construction of universal taxonomies to enable the model to cover all domains. Several approaches [11; 30] attempted to concatenate dataset label spaces and differentiated similar classes by aligning text embedding encoded by language model. However, this approach struggled with classes that had the same name but different levels of annotation granularity, and direct concatenation of label spaces could lead to semantic conflicts. Other research [24; 29] attempted to establish a unified label space

Figure 1: Our method consists of three modules. The label encoding provides the semantic text features of the dataset labels. The GNNs learn the unified label embedding space and dataset label mappings based on the textual features and input images. The segmentation network leverages the unified label embedding space to produce segmentation results in the unified label space.

through the expertise of human annotators. Additionally, other studies [41; 53] aimed to address this issue by developing automated mechanisms to create universal taxonomies. However, these methods either could only construct a unified label space between two datasets at a time  or could not handle complex class relationships . Our approach stands out by automatically constructing universal taxonomies in a single training session with multiple datasets, resulting in significant time savings compared to methods that require multiple training iterations.

**Graph Neural Networks** demonstrated exceptional effectiveness in dealing with complex topological data structures, as highlighted in recent research . The applicability extended across various domains, including recommendation systems , knowledge graph construction , and skeletal action recognition . In the context of our problem, discovering label relations can be conceptualized as a link prediction task [49; 22]. However, conventional approaches for graph link prediction [9; 10] are not suitable for our model since we do not possess ground truth links. We use the values of the learnable adjacency matrix as predictions of whether nodes are linked. Supervision of the linked predictions relys on the segmentation results of the segmentation network and the corresponding image annotations.

## 3 Proposed Method

The comprehensive framework is depicted in Figure 2. We first define the unified representation of the multi-dataset label space. Utilizing this representation, we build a graph neural network to learn the unified label space. Finally, leveraging the unified label space, we train our segmentation network and the graph neural network using the dataset annotations.

### Unification of Multi-dataset Label Space

**Unified Label Space.** Given \(K\) datasets with their respective label space \(\{L_{1},L_{2},,L_{K}\}\), multi-dataset semantic segmentation requires a model to predict within a consistent label space that

Figure 2: Illustration of our method that training with dataset-specific annotations through label mappings constructed by GNNs. We leverage a unified segmentation head (UniSegHead) to enable simultaneous training on multiple datasets. In the UniSegHead, we compute the matrix product between pixel embedding and augmented unified node features output by the GNNs, resulting in predictions for the unified label space. We finally utilize the label mappings constructed by GNNs to map the unified predictions to dataset-specific prediction for training.

encompasses all dataset label spaces \(L_{pred}=_{i=1}^{K}L_{i}\). Each pixel must be assigned to a label in this unified label space. We define \(N\) unified label nodes \(=\{_{1},_{2},,_{N}\}\), serving as the nodes in the graph neural networks. Their corresponding \(D\)-dimensional learnable embedding \(\{_{1},_{2},,_{N}\}\) represent the unified label embedding space. The number of unified label nodes \(N\) is often smaller than the total number of dataset classes \(|L|\), because we aim to merge multiple identical classes into a single unified label. The image is first encoded into pixel embedding \(\) by the segmentation network, which is then projected into the unified label embedding space to assign a unified label to each pixel.

**Label Mappings.** We define a mapping from the unified label space to the dataset-specific label space \(_{i}: L_{i}\), which is used to train the model with dataset-specific annotations. Mathematically, \(_{i}\{0,1\}^{N|L_{i}|}\) is a boolean linear transformation. Each unified class \(\) is at most linked to a class \(c\) within a specific dataset \(i\) to prevent label conflicts: \(_{}\). To handle different annotation granularities, we use label mappings to merge multiple unified label nodes representing fine-grained classes into one super-class \(c\): \(_{c}^{}\). For example, the _curb_ from IID can be simultaneously mapped by unified label nodes represented _curb_ and _barrier_. We use unified label nodes as input nodes for the GNNs, which learn the label mappings and unified label embedding space, thus enabling the automated unification of multi-dataset label spaces.

### Learning Unified Label Space with Graph Neural Networks

Learning the label mappings between dataset label spaces and the unified label space can be viewed as a bipartite graph matching problem. This makes graph neural networks well-suited to address this issue. Below, we detail our approach of constructing GNNs for learning a unified label space.

**Input Nodes.** To construct the input feature of dataset-specific label nodes, as illustrated in Figure 2, we used the dataset labels in the template "An image of <label> from the dataset <dataset>" as plain text input and employed ChatGPT to complete the detailed description of each label. Then, we employ Ilama-2  to encode these label descriptions and generate text features. To further distinguish nodes from different datasets, we introduce learnable dataset embedding for each dataset \(\{_{1},_{2},,_{K}\}\). The dataset embedding is combined with the text features to form the input features for dataset-specific label nodes:

\[_{i,m}=f_{t}(l_{i,m})+_{i},\] (1)

where \(_{i,m}\) is the input feature of the \(m\)-th label from dataset \(i\), and \(f_{t}(l_{i,m})\) is the text feature of the label description \(l_{i,m}\) encoded by language model. The input features of unified label nodes are randomly initialized to the same dimension as the dataset-specific label node. To determine the appropriate number of unified label nodes, inspired by the approach in , we used cross-validation results across different datasets to identify the number of mergeable categories, which served as the initial selection for the unified label nodes. The specific algorithm can be found in Appendix B. Together, dataset-specific label nodes and unified label nodes constitute the input nodes of GNNs. During the training process, we maintain a constant number of nodes. After the training is completed, we will remove inactive unified label nodes, meaning those that were not assigned to any dataset-specific label.

**Learnable Adjacency Matrix.** To enable label mappings to be updated via gradient descent, we embed the label mappings as a continuous, learnable graph adjacency matrix \(_{a}\). Values in the adjacency matrix represent the weights of corresponding edges. Only the edges between unified label nodes and dataset-specific nodes are learnable, while others are fixed to zero. We apply a softmax operation to the edges connecting each unified label node with nodes of a particular dataset, ensuring that the sum of the edge weights \(w\) between each unified label node and nodes of this dataset equals one. The element at the intersection of the \(r\)-th row and the \(c\)-th column in the lower triangular portion of \(_{a}\) is formulated as:

\[_{r,c}=\{}}{_{c^{}  L_{i},r^{}}e^{w_{r^{},c^{}}}}&\\ 0&.\] (2)

**Unified Label Embedding.** The forward propagation of our graph model follows the GraphSAGE framwork  as formulated in Equation 3. \(^{k}\) and \(^{k}\) are the weight and feature of the \(k\)-th GNNs layer. The \(\) indicates nonlinear activation function, implemented as the tanh in this work. The output features of the unified label nodes from the final layer serve as the unified label embedding space, \(_{u}=[_{1},_{2},,_{N}]^{}\).

\[^{k+1}=(^{k}[^{k}\|_{u} ^{k}]).\] (3)

To obtain the dataset label mappings, we partition the adjacency matrix into submatrices, each corresponding to a specific dataset. Each submatrix contains only the unified label nodes and the label nodes specific to that dataset. We compute the label mappings for each dataset based on the values in its corresponding submatrix, divided into the following two cases. During training, we will alternately train the segmentation network and the GNNs. When training the GNNs, we directly use the value of the learnable adjacency matrix to establish label mappings, thereby facilitating weight updates through gradient descent. When training the segmentation network, we utilize the unbalanced optimal transport algorithm to solve for the boolean label mappings that satisfies the many-to-one mapping constraints. This algorithm detailed in Appendix C effectively handles the conversion of the continuous adjacency matrix into a discrete dataset label mappings required for training segmentation network.

### Training a Universal Model with Dataset-specific Annotations

Training a universal model is divided into two steps. The first step involves training a robust encoder-decoder to provide the pixel embedding for each pixel position in the image, where similar objects should have similar features. The second step focuses on learning label mappings and unified label embedding space by GNNs. During the training phase, we alternate between these two steps, freezing one network while training another network. Both of these steps require supervised training using dataset-specific annotations. Here, we primarily focus on the training of GNNs, while further details of the training strategies are provided in the Appendix A.

**Training with Dataset-specific Annotations.** During training, an image is randomly sampled from dataset \(i\) and fed into an segmentation network, which provides embedding for each pixel position \(=\{_{1},_{2},,_{j}\}\), where \(_{k}\) is \(D\)-dimensional vectors. We utilize a unified segmentation head (UniSegHead) to assign a dataset label to each pixel. In the UniSegHead, We first project the pixel embedding into the unified label embedding space by multiplying the pixel embedding by the output feature of unified label node \(_{u}\), as shown in Equation 4. Then, to train the universal model with dataset-specific annotations, we need to map the predictions in unified label space to dataset-specific label space to obtain the probabilities of dataset-specific classes. This is achieved by computing per-pixel dataset-specific logits \(\) by multiplying the dataset-specific label mappings \(_{i}\) at each pixel:

\[_{k}=_{u}_{k},\] (4) \[_{k}=_{i}_{k}.\] (5)

Finally, probabilities of dataset-specific classes are computed by per-pixel softmax operation over the logits \(s\). This allows us to use dataset-specific annotations to compute the pixel-wise loss function, train the network, and update the label mappings. We formulate the cross-entropy loss for a specific pixel to train the segmentation network:

\[_{ce}(,)=-_{c=1}^{|L_{i}|}y_{c}(()_{c}),\] (6)

where \(\) is the pre-pixel annotation, \(|L_{i}|\) represents the total number of classes in the dataset \(i\) from which the image originates.

**Orthogonality Loss.** To achieve a conflict-free unified label space and avoid redundant unified label nodes that represent the same class or have overly similar features, we introduce soft constraints to promote orthogonality among the unified label node features, inspired by . This orthogonality loss encourages unified label node embedding to be mutually orthogonal. It not only aligns the unified

  Dataset Domain & Training and Validation datasets &  \\  Driving scene & CityScapes , Mapillary , BDD , IDD  & WildDash 2 , KITTI , CamVid , SUN RGBD  & ScanNet-20  \\ Everyday objects & ADE20K , COCO  & PASCAL VOC , PASCAL Context  \\  

Table 1: Training and test datasets in our experiments.

label nodes with annotation standards for practical use but also enhances the diversity of the model and helps in finding a better label mappings:

\[_{orth}=-_{i=1}^{N}(_{u}_{i})_ {i}((_{u}_{i})_{i}).\] (7)

The final loss function used to train the GNNs is represented as follows, with \(\)s as hyperparameters to adjust the weights of different loss functions:

\[=_{1}_{ce}+_{2}_{orth}.\] (8)

## 4 Experiments

**Datasets.** We list the semantic segmentation datasets used for training and testing in Table 1. Our training datasets cover a wide range of scenarios, from indoor scenes to driving scenes. We also introduce corresponding test datasets, which are not used in the training process, for the respective scenes to evaluate our generalization capability.

**Implementaion Details.** Our segmentation model is based on the HRNet-W48 architecture , while the GNN model is a three-layer GraphSAGE . We utilize the llama-2-7B model to encode label descriptions into 4096-dimensional text features. These text features, augmented with dataset embedding of the same dimensionality, are then employed as node features input into the GraphSAGE. When forming a minibatch from multiple datasets, we evenly sample 3 images per dataset within a batch for each GPU. For all images, We first apply random resizing with a ratio ranging from 0.5 to 2, followed by a random crop operation to achieve a final image size of 768 \(\) 768 pixels. We use AdamW optimizer  with warmup and polynomial learning rate decay, starting with a learning rate of 0.0001. We train our model for 300k iterations on four 80G A100 GPUs.

### Comparison on Multiple Datasets

In Table 2, we present the accuracy of our methods and compare them to other approaches on the seven training datasets. We use mean Intersection over Union (mIoU) to quantify the performance of models, a common metric used to evaluate the performance of segmentation models. Different methods adopt various approaches to construct their label spaces: _Dataset-Specific_ represents a lack of a unified label space, where the model outputs a separate label space for each dataset. _Manually

  Methods & Backbone & Venue & Label space1  & CS & MPL & SUN & BDD & IDD & ADE & COCO & Mean \\  MSeg  & HRNet-W48 & CVPR 20 & MR & 76.3 & 51.9\({}^{*}\) & 46.1 & 63.5 & 61.8 & 24.8\({}^{*}\) & 48.6\({}^{*}\) & 55.9 \\ NLL+  & SPN-RN18 & WACV 22 & MC & 72.6 & 39.1 & 41.7 & 58.5 & 54.4\({}^{*}\) & 31.0 & 35.4 & 47.5 \\ Uni NLL+  & SPN-DN161 & IJCV 24 & MC & 76.1 & **44.2** & 46.9 & 60.4 & 56.7\({}^{*}\) & 35.6 & 39.3 & 51.3 \\  Single dataset & HRNet-W48 & - & DS & 77.7 & 30.2 & 43.9 & 62.4 & 66.8 & 34.5 & 38.0 & 50.4 \\ Multi-SegHead & HRNet-W48 & - & DS & 79.5 & 36.1 & 47.3 & **65.6** & 67.0 & 30.5 & 36.7 & 51.8 \\ Auto univ.  & SPN-RN18 & BMVC 22 & Auto & 72.7 & 35.8 & 42.3 & **59.6** & 55.2\({}^{*}\) & 30.7 & 35.6 & 47.4 \\ Ours & HRNet-W48 & - & Auto & **80.7** & 43.7 & **47.5** & 65.5 & **68.6** & **42.0** & **46.7** & **56.4** \\  

* Approach to construct label space. MC-Manually Construct, MR-Manually Relabel, DS:Dataset-specific, Auto-Automatically Construct.[ENDFOOTNOTE]
* MSeg train and evaluate on 43 of 65 class in Mapillary dataset, 117 of 150 class in ADE dataset, 122 of 133 class in COCO dataset.
* These methods were trained and evaluated using 30 classes from the IDD dataset, while we trained and evaluated using the officially recommended 26 classes.

Table 2: Multi-dataset performance compared with other methods.

  Trained dataset & Mean results across training datasets & Mean results across unseen datasets \\ or label space & Single dataset & Multi-SegHead & Single dataset & Multi-SegHead \\  CS & 23.9 & 30.9 & 31.4 & 37.9 \\ MPL & 30.6 & 36.1 & 36.9 & 41.0 \\ SUN & 12.9 & 22.0 & 16.7 & 28.9 \\ BDD & 24.8 & 29.1 & 30.4 & 35.7 \\ IDD & 26.0 & 34.0 & 28.9 & 36.4 \\ ADE & 28.5 & 40.3 & 37.2 & 49.8 \\ COCO & 30.5 & 38.8 & 45.1 & 53.2 \\  Ours & **56.4** &  \\  

Table 3: Performance comparison with two baselines on training and unseen datasets.

_Relabel_ means manually re-annotating each image. _Manually Construct_ means the label space is constructed through human expertise. _Automatically Construct_ includes methods where the unified label space is automatically constructed by the model. We also establish two baseline methods: _Single dataset_ and _Multi-SegHead_. _Single dataset_ demonstrates the results of training on individual dataset only, while _Multi-SegHead_ trains on multiple datasets by using dataset-specific segmentation heads.

The results demonstrate that our method achieves the best average performance in multi-dataset training, while also achieving significant performance improvements on datasets with a large number of classes such as the ADE and COCO datasets. We attribute this to the construction of a robust unified label space. Leveraging visual connections from the samples, our unified label space can discover label relationships beyond textual similarities. For instance, the visual appearance of the _fireplace_ in ADE is similar to the _tunnel_ in Mapillary. Despite their different semantic meanings, our model merges these labels for prediction, as detailed in subsection 4.5. This approach saves model capacity and facilitates knowledge transfer across datasets for improved prediction.

In Table 3, as a supplement to Table 2, we compare our model with various models trained on single dataset, as well as each segmentation head output of the _Multi-SegHead_. Detailed data for each dataset can be found in the Appendix D. From the table, it can be observed that training with multiple datasets helps improve the model's generalization performance. However, the performance of different segmentation head outputs in _Multi-SegHead_ model shows significant differences due to the lack of a unified form of output that performs well across all datasets. In contrast, our approach provides a unified label space covering all datasets, resulting in a significant advantage in average performance. We also list the performance on the five unseen test datasets mentioned in Table 1. To evaluate the performance of our model on unseen datasets, we first evaluate the model results on its training dataset. We search for the optimal label mappings based on the accuracy of label predictions. There are no updates to any model parameters except the label mappings in this process. This process can actually be done manually without any annotation information. The results indicate that our model exhibits better generalization performance. It can handle various scenarios and consistently achieve excellent performance on the test set. Compared to _Multi-SegHead model_, our automatically constructed label space has advantages over dataset label spaces. The unified label space can integrate the semantic information from multiple dataset label spaces.

Figure 3 presents the segmentation results on multiple datasets predicted in unified label space. Compared to models trained only on _Single dataset_, our model successfully provides consistent predictions on all datasets. It's worth noting that in the BDD dataset, annotations are not provided for _lane marking_, _crosswalk_ and _manhole_, which are only annotated in the Mapillary dataset. Our model successfully integrates the label space of the Mapillary dataset, thereby predicting these classes in the BDD dataset. More results are presented in Appendix F.

### Results on WildDash 2 Benchmark.

WildDash 2  provides a benchmark for semantic segmentation, designed to test the robustness of algorithms in real-world driving scenarios. Due to the insufficient number of training samples

Figure 3: Visual comparisons with _Single dataset_ model on different training datasets.

provided by this dataset, the official recommendation is to use multiple datasets for training. Therefore, this benchmark is well-suited to evaluate the effectiveness of multi-dataset training methods. The WildDash 2 dataset includes negative test cases to challenge the robustness of the model. These negative test cases mainly consist of unconventional driving scenarios, and even non-driving scenarios. Across all pixels within negative test images, a robust model is expected to predict the _void_ label for open-set classes and anomalous objects. The WildDash 2 benchmark refers to the metric named Meta Avg mIoU Class, which calculates the mean Intersection over Union for each class by weighting negative and positive test cases according to their occurrence in the benchmark dataset.

Table 4 presents the current results on the WildDash 2 leaderboard. We present results for zero-shot generalization using our 7ds model. To evaluate in an unseen setting, we map the non-evaluated classes in the unified label space to a _void_ label for both positive and negative test frames. To ensure a fair comparison with other works, we also provide evaluation results for models trained using the training datasets from the Robust Vision Challenge 2022, which include CityScapes, ADE20K, Vistas, VIPER , ScanNet, and WildDash 2. The results indicate that our method achieves state-of-the-art performance in both zero-shot and trained settings. Our method exhibits significant performance improvements compared to other methods on negative test cases. We attribute this to the robustness of our model, which has been trained on diverse datasets, enabling it to perform well in unconventional scenarios. Our zero-shot model has been trained on a wider range of datasets compared to the trained model. Therefore, even without training on the WildDash 2 dataset, it achieves similar performance on negative test cases.

### Ablation Study

To further explore the ability of our GNNs to construct a unified label space, we compared it with four alternative methods. The first method concatenates all dataset label spaces into a single unified space \(_{i=1}^{D}L_{i}\). The second approach constructs a unified label space by clustering text features based on cosine similarity using the DBSCAN  algorithm. In the third method, we train a segmentation network using an initial adjacency matrix, as outlined in Appendix B, without incorporating GNN training. The final method involves an ablation experiment, removing the label description module to observe its impact. Experimental results, presented in Table 5, demonstrate that the label space constructed based on GNNs can better assist in the learning of segmentation models. Unlike the first approach, our method optimizes model capacity by focusing on label relationships rather than dataset recognition. Compared to the second approach, our method can differentiate between classes with identical names but differing levels of granularity. By leveraging label descriptions to enrich semantic context, our approach constructs a more refined and functional label space.

   &  &  & Meta Avg & Classic & Classic & Classic & Classic & Negative \\  & & mIoU Class & mIoU Class & iIoU Class & mIoU Cat. & iIoU Cat. & mIoU class \\  EffPS  & DCV21 & ✗ & 32.2 & 35.7 & 24.4 & 63.8 & 56.0 & 20.4 \\ MSeg  & CVPR 20 & ✗ & 35.2 & 38.7 & 35.4 & 65.1 & 50.7 & 24.7 \\ SeamSeg  & CVPR 19 & ✗ & 37.9 & 41.2 & **37.2** & 63.1 & **58.1** & 30.5 \\ UniSeg  & ECCV 22 & ✗ & 39.4 & **41.7** & 35.3 & **65.8** & 57.4 & 34.8 \\ Ours & - & ✗ & **40.5** & 41.2 & 33.7 & 65.4 & 54.2 & **43.4** \\  SNPRN152  & arXiv 20 & ✓ & 45.4 & 48.9 & 42.7 & 70.1 & 64.8 & 32.5 \\ NLRL+  & WACV 22 & ✓ & 46.8 & 51.0 & 43.9 & 71.4 & 65.5 & 32.6 \\ Uni NLL+  & IJCV 24 & ✓ & 46.9 & 51.6 & 45.9 & 72.8 & 67.5 & 29.0 \\ FAN  & arXiv 22 & ✓ & 47.5 & 50.8 & 44.0 & **74.2** & 67.5 & 34.4 \\ MIX6D  & arXiv 22 & ✓ & 48.5 & 51.2 & 46.5 & 72.4 & 66.1 & 40.8 \\ Ours & - & ✓ & **50.0** & **52.2** & **47.5** & 72.4 & **68.6** & **44.6** \\  

Table 4: Performance comparison on WildDash 2 benchmark.

  Methods & \(|L|\) & CS & MPL & SUN & BDD & IDD & ADE & COCO & Mean \\  Direct concatenation & 448 & 79.2 & 38.8 & 46.9 & 64.3 & 66.7 & 33.3 & 38.2 & 52.5 \\ Clustered by text features & 329 & 79.9 & 40.8 & 47.3 & 65.6 & 68.6 & 33.8 & 38.0 & 53.4 \\ Without GNN training & 231 & 79.8 & 39.7 & **47.7** & **66.1** & 68.1 & 37.8 & 45.3 & 54.9 \\ Without GPT’s label description & 226 & 80.2 & 43.4 & 47.2 & 64.7 & **68.8** & 40.1 & 46.0 & 55.8 \\ Our proposed method & 217 & **80.7** & **43.7** & 47.5 & 65.5 & 68.6 & **42.0** & **46.7** & **56.4** \\  

Table 5: Comparison of Different Methods of Construct Label Spaces.

### Exploring the Impact of Training Datasets

To explore the impact of training dataset selection on model performance, we train a domain-specific model focusing on road driving scenes and a domain-general model on more datasets, as shown in Figure 4. We conduct four sets of comparisons to evaluate the performance across datasets that were trained on both models, trained on one model and not on the other, and not trained on either model. As shown in Table 6 and Appendix E, the domain-specific model can focus more on learning features specific to the particular scene, resulting in slightly better performance on both trained and unseen driving scene datasets compared to domain-general model. On the other hand, domain-general model trained on more scenes and more data exhibit better generalization performance. Therefore, while it does not lag too far behind in performance on driving scene datasets, it demonstrates overwhelming advantages on other scene datasets.

### Qualitative Results

Figure 5 presents the qualitative analysis results of the label space learned by our model. We compare the label space learned by our model with the label space constructed using text features. The class _curb_ in the IDD dataset actually encompasses both the classes _curb_ and _barrier_ in Mapillary, whereas the constructed label space by text features cannot handle such subclass/superclass relations. In contrast, our GNNs learned label space splits the IDD _curb_ into two classes for prediction, effectively handling such label relationships. Similar situations also include the class _tunnel or bridge_ in the IDD dataset. However, since the proportion of bridge pixels is orders of magnitude greater than that of tunnels (\(10^{8}\) pixels for _bridge_ and \(10^{4}\) pixels for _tunnel_), a more reasonable approach is to merge it with the class _bridge_, as our GNNs have done. Additionally, it is worth noting that due to visual similarities, the Mapillary _tunnel_ has been merged with the ADE _fireplace_. This does not actually introduce any conflict because no dataset simultaneously annotates both the _tunnel_ and _fireplace_. The model will construct the label space in a way that facilitates its learning process.

## 5 Conclusion

We propose a novel approach that leverages graph neural networks to construct a unified label space for training semantic segmentation models across multiple datasets. Our method addresses the challenge of label conflicts in multi-dataset semantic segmentation and demonstrates performance improvements across various datasets. The unified label space generated during training, generalizes well to unseen datasets, showcasing the effectiveness of our approach.

**Broader Impact.** Our work explores the use of graph neural networks to unify label spaces between datasets, providing a new direction for achieving robust and efficient multi-dataset training. By enabling semantic segmentation models to be trained on multiple datasets with a unified label space, our method can potentially reduce human effort required for re-labeling images and facilitate the expansion of training datasets. This can lead to the development of models that are more universally applicable across various datasets, benefiting a wide range of applications.

**Limitations.** Although our approach does not require manual relabeling efforts, it still relies on fully annotated datasets for training, in contrast to weakly-supervised and unsupervised methods. We aim to explore ways to integrate these alternative methods in future research. Errors in the fully automated construction of a unified label space do present some safety risks for autonomous driving tasks. Therefore, we recommend introducing a manual review mechanism to address these issues. Additionally, using ChatGPT may generate inaccurate label descriptions, which could affect the

   &  &  \\  & CV & KT & VOC & SN \\  General & 71.1 & 65.5 & **69.2** & **40.6** \\ Specific & **74.4** & **67.4** & 28.7 & 8.1 \\ 

Table 6: Performance on unseen dataset.

Figure 4: The composition of the training datasets.

prediction of label relationships. Therefore, we aim to improve the accuracy of label descriptions by incorporating label descriptions provided by official datasets as prompts.

## 6 Acknowledgments

This work was supported in part by NSFC Project (62176061), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0103). The computations in this research were performed using the CFFF platform of Fudan University.