# Consent in Crisis:

The Rapid Decline of the AI Data Commons

Shayne Longpre

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Robert Mahari

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Ariel Lee

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Campbell Lund

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Hamidah Oderinwale

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

William Brannon

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Nayan Saxena

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Naana Obeng-Marun

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Tobin South

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Cole Hunter

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Kivman

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Christopher Klamm

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Hailey Schoelkopf

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Nikhil Singh

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Manuel Cherep

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Ahmad Mustafa Anis

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

An Dinh

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Caroline Chitongo

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Da Yin

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Damien Sileo

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Devidas Mataciannas

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Diganta Misra

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Emado Shippole

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Jianguo Zhang

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Joanna Materzynska

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Kun Qian

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Kush Tiwary

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Lester Miranda

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Manan Dey

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Minnie Liang

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Mohammed Hamdy

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Niklas Muennighoff

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Seonghyeon Ye

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Seonghyeon Ye

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Seonghyeon Ye

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Seonghyeon Ye

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

Steffen-Minnie Liang

Team Leads 2Top Contributors 3Contributors (alphabetized) 4Advisors

###### Abstract

General-purpose artificial intelligence (AI) systems are built on massive swathes of public web data, assembled into corpora such as C4, RefinedWeb, and Dolma. To our knowledge, we conduct the first, large-scale, longitudinal audit of the consent protocols for the web domains _underlying_ AI training corpora. Our audit of \(14,000\) web domains provides an expansive view of crawlable web data and how codified data use preferences are changing over time. We observe a proliferation of AI-specific clauses to limit use, acute differences in restrictions on AI developers, as well as general inconsistencies between websites' expressed intentions in their Terms of Service and their robots.txt. We diagnose these as symptoms of ineffective web protocols, not designed to cope with the widespread re-purposing of the internet for AI. Our longitudinal analyses show that in a single year (2023-2024) there has been a rapid crescendo of data restrictions from web sources, rendering ~5%+ of all tokens in C4, or 28%+ of the most actively maintained, critical sources in C4, fully restricted from use. For Terms of Service crawling restrictions, a full 45% of C4 is now restricted. If respected or enforced, these restrictions are rapidly biasing the diversity, freshness, and scaling laws for general-purpose AI systems. We hope to illustrate the emerging crises in data consent, for both developers and creators. The foreclosure of much of the open web will impact not only commercial AI, but also non-commercial AI and academic research.

## 1 Introduction

The web has become the primary communal source of data, or "data commons", for general-purpose and multi-modal AI systems. The scale and heterogeneity of web-sourced training datasets provide the foundation for both open and closed AI systems, such as OLMo , GPT-4o , and Gemini . However, the use of web content for AI poses ethical and legal challenges to data consent, attribution, copyright, and the potential impact on creative industries . This has spurred new initiatives to better verify data quality and provenance , isolate public domain and permissivelylicensed data , and integrate new infrastructure to signal , detect , and even evade the use of data for AI training .

The focus of this work is to understand the evolving role of the internet as a primary ingredient to AI, and how AI has collided with the limited protocols that govern data use. Web data is traditionally collected using _web crawlers_--automatic bots that systematically explore the internet and record what they see. However, the mechanisms for indicating restrictions to web crawlers, such as the Robots Exclusion Protocol (REP), were not designed with AI in mind . (The REP is referred to as robots.txt in practice.) As such, we examine their (in)ability to communicate the nuances in how content creators wish their work to be used, if at all, for AI. And more broadly, we analyze how AI is already re-shaping the culture of web consent, and how this is shifting the landscape for AI training data. Our results foretell significant changes not only to AI data collection practices and data scaling laws, but also the structure of consent on the open web, which will impact more than AI developers.

To this end, we present a large-scale audit of the web sources underlying three open AI training corpora: C4 , RefinedWeb , and Dolma . In contrast to prior audits that assess datasets--curated snapshots of data--this work looks _beneath_ the datasets at the web domains they were derived from, and traces the temporal evolution of these sources. We are, to our knowledge, the first to systematically measure detailed provenance, crawler consent mechanisms, and content monetization factors, all relevant to the responsible downstream use of this data. These analyses enable us to trace fundamental distribution shifts in how preference signals are expressed and the inadequacy of existing tools. Our work has several key findings:

1. **A proliferation of restrictions on the AI data commons.** We find a rapid proliferation of restrictions on web crawlers associated with AI development in both websites' robots.txt and Terms of Service. We estimate that in one year (Apr 2023 to Apr 2024), ~25%+ of tokens from the domains most critical to model training, and ~5%+ of tokens from the entire corpora of C4, RefinedWeb, and Dolma have since become restricted by robots.txt. Forecasting these trends forward shows a decline in unrestricted, open web data year-over-year.
2. **Consent asymmetries & inconsistencies.** OpenAI's crawlers are significantly more restricted than those of other AI developers. More broadly, preference signaling mechanisms like robots.txt see errors and omissions in their coverage across AI developers, as well as contradictions with their terms of service--indicating inefficiencies in the tools used to communicate data intentions.
3. **A divergence in content characteristics between the head and tail of public web-crawled training corpora.** We find the largest web-based sources of public training data have significantly higher rates of user content, multi-modal content, and monetized content, but only slightly less sensitive/explicit content. Top web domains comprise news, encyclopedias, and social media sites, as compared to the many organization websites, blogs, and e-commerce websites in the long tail of web sources.

## 2 Methodology

AI models that are highly performant on tasks in language , images [21; 22; 23], video [24; 25; 26], and even audio [27; 28] increasingly depend on massive web-sourced training datasets. These datasets are collected using web crawlers--agents that navigate the web, accessing and retrieving web pages without human intervention. While these robots are essential for a variety of applications, including search engines, studying the internet (i.e., archiving), and link verification tools; recently they have also become the backbone of AI training data collection [29; 30].

In our study, we focus on three popular, open-source, and permissively licensed data sources which are derived from Common Crawl, the largest publicly available crawl of the web, which has collected and stored hundreds of billions of web pages since 2008. For each web-based data source, we sample the web domains from which it was created, and extensively human-annotate their properties. Our analysis examines a snapshot of the present, as well as longitudinal changes across time, to understand how ecosystem norms have evolved.

Data sourcesThe data sources used for our study are C4 , RefinedWeb , and Dolma . These data sources each have 100k-1M+ downloads, are the primary component in most modern foundation models [30; 1; 31], and are also widely used to derive other popular datasets [32; 33; 34]. Common Crawl is released on a monthly basis, and, as seen in Table 1, each data source is based on a different set of monthly snapshots. Each of these corpora apply various automatic filtering techniques, including removing duplicative pages, low-quality content, and personally identifying information such as addresses.

Head sample and random sampleFor each data source, we identified and selected the top 2k web domains ranked by their number of tokens. We refer to the resulting 3.95k union of these web domains as HeadAll. This sample represents the largest, most actively maintained, and critical domains for AI training. For certain analyses, we consider only the head of C4, which we will refer to as HeadC4.

We are also interested in how consent preferences have evolved within a wider sample of internet domains. To capture this, we randomly sampled 10K domains (Random10k) from the intersection of the three corpora (itself totalling 10,136,147 domains). From the 10k sample, we selected a random subset of 2K for human annotation (Random2k). Random10k was sampled from the intersection of domains listed across all three datasets, which means this subset may skew towards more widely-used or high-quality domains.

Human annotationsWe trained annotators to manually label the websites for their content modalities (e.g. video, text); website purpose(s) (e.g. news, e-commerce); presence of paywalls and embedded advertisements; the text of the terms of service, if any; and other metadata detailed in Table 2. Annotators received individual instructions, frequent quality calibration, and were compensated well above industry standards at $25-$30 per hour. We collected annotations for the entirety of HeadAll as well as from the random sample Random2k. More details on our annotation process are available in Appendix C, and all annotations will be made publicly available for reproducibility and future research.

Measuring website administrators' intentionsA goal of our audit is to measure website administrators' intentions for how their sites can be crawled and their content used--including for training AI models. We used the Wayback Machine1, a digital archive of 835 billion web pages, to collect historical versions of each website's homepage, its Robots Exclusion Protocol (REP), commonly

   Data Source & Crawl Dates & Web Domains \\  C4 & 4/2019 & 15,928,138 \\ RefinedWeb & 2008 to 2/2023 & 33,210,738 \\ Dolma & 5/2020 to 6/2023 & 45,246,789 \\  Intersection & & 10,136,147 \\   

Table 1: Statistics on audited data sources.

   Attribute & Details & Collect \\ 
**Content Modalities** & Whether the web domain has images, videos, and standalone audio in addition to text. & \\ 
**User Content** & Whether the web domain hosts primarily content provided by users, such as forums, blog hosting, and social media websites. & \\
**Sensitive Content** & Whether explicit, illicit, pornographic, or hate speech content is clearly present. & \\
**Paywall** & Whether the web domain has use limits or any access gating behind a paywall. & \\
**Advertisements** & Whether the web domain has automatic advertisements embedded into any of its pages. & \\ 
**Purpose \& Service** & The purpose or service(s) of a website? Optience: E-commerce, Social MediaForum, Encyclopedia, Academic, Government, Organization site, News, or Other. & \\   \\ 
**Robots.txt** & A web domain’s robots.txt restrictions on crawler agents. We use Google’s crawler rules. & \\ 
**Terms \& Policies** & The terms, content, copyright, and privacy policy pages found for a web domain. & \\ 
**Crawling \& AI Policy** & Do terms restrict both crawling and AI, restrict crawling, restrict only AI, conditionally restricting & \\ 
**Content Use Policy** & Are there content use restrictions. Options: restricted to personal, academic, or non-commercial use, conditionally restricted, or unrestricted. & \\ 
**Non-Compete Policy** & Is content use prohibited for developing competing services? & \\   

Table 2: The **list of attributes collected for each web domain**, as sampled from C4, Dolma, and RefinedWeb. \(\) denotes automatic collection, \(\) human annotation, and \(\) information collected statically and historically from 2016. Full annotation guidelines are given in Appendix C.2.2.

referred to as its robots.txt file, and its terms of service page. This was collected at monthly intervals, from January 2016 to April 2024.

The REP, first introduced in 1995 and codified in 2022, has become the default mechanism for website owners to indicate to web crawlers what parts of their website, if any, they consent to have crawled . While it is not legally enforceable, it is respected by all major search engines, as it prevents website servers from getting overloaded by crawlers, it allows websites to signal pages that are undesirable to crawl (for example, calendar sites that could lead to infinite loops), and by respecting it, crawlers disincentivize adversarial tactics designed to impede crawlers. Website creators are able to set one set of instructions for all web crawlers or a different instructions for each web crawler. For instance, Google Search respects instructions which specifies the user-agent string "Googlebot" while Common Crawl looks for the user-agent "CCBot."

In our audit, we record the robots.txt instructions for a range of crawlers, but focus our analysis on five AI developers, Google, OpenAI, Anthropic, Cohere, and Meta, as well as non-profit web archival organizations such as Common Crawl and the Internet Archive, which have seen their data taken for AI training. Collectively, we refer to these as "AI Organizations". We classify robots.txt for each crawler in ascending order of restrictions, from no robots.txt present, to sitemaps which support crawlers without limitations, to basic restrictions on a subset of directories, to full restrictions on any crawling of the website. For each corpus, we measure the percentage of "restricted tokens" as the portion of tokens from web domains that fully restrict one or more of the AI Organizations's crawlers. For Terms of Service analysis, we define restricted tokens to simply mean the portion of unusable tokens due to terms that preclude crawling or AI. See Appendix D.2 for the full list of agents and Appendix D.1 for the robots.txt restriction classification taxonomy.

In addition to robots.txt, we recorded the Terms of Service (ToS) and other content and copyright policies for each website. These documents support more nuanced preferences than the REP, and allow for blanket bans on downstream use cases rather than just specification of what data agents are allowed to collect. We used an automatic annotation pipeline (see Appendix D for details) to categorize ToS agreements according to stance towards use of web crawlers and AI training, content use restrictions, and non-compete clauses, in ascending degrees of restrictiveness.

## 3 Findings

### The Rise of Restrictions on Open Web Data

To understand the web sources underlying foundation models, we analyze the longitudinal changes in robots.txt and Terms of Service restrictions between January 2016 and April 2024. In Figure 1 the plots depict the percent of tokens present in each category of restriction over time, for the AI Organizations in HeadC4--the largest, most actively maintained, and critical domains for AI training. The fine-grained longitudinal analysis of robots and Terms of Service trends allows us to estimate this time series into the future. We apply seasonal autoregressive integrated moving average (SARIMA) models to generate forecasts of future trends for both the head sample and random subset, the details of which can be found in Appendix F along with the coefficients, tests, and limitations.

In Figure 2 we measure the restricted tokens, or how many tokens fall into the most restrictive settings for each of robots.txt and Terms of Service, as a portion of the Full Corpus, or HeadAll. The intermittent lack of smoothness for Figures 1(c) and 1(d) is mainly due to temporal gaps in the Wayback Machine; however the main trends remain visible. We point to Appendix E for additional details regarding methodology. In all analyses we exclude web domains which could not be retrieved from the Wayback Machine, and all proportions are based on the set of web domains which existed in that time period.

These analyses show a clear and systematic rise in restrictions to crawl and train on data, from across the web. We make no assertion regarding whether the prior omission of a robots.txt or restrictions implies consent to use data. To the degree these restrictions are respected, it also foretells a decline in open data, which may impact more than commercial AI developers, or even AI organizations in general. We break down and discuss the findings of this temporal analysis below.

**Web domains are adopting robots.txt and Terms of Service pages to signal preferences.** Figure 1 (Top & Middle) shows from 2016, the portion of web domains in HeadC4 without a robots.txt and Terms of Service has gone from 20% and 80% respectively, to near zero.2 This reflects an emerging adoption of these practices to signal and protect data intentions.

**Robots.txt crawling restrictions have risen precipitously since mid-2023.** Figure 1(Top) shows the rapid re-distribution of robots.txt restrictions, directly after the introduction of GPTBot and Google-Extended crawler agents. This re-distribution to full restrictions mainly comes from websites with previously moderate restrictions, such as disallowed directories, pattern-based or search page restrictions, and partly from websites with no prior restrictions in their robots.txt.

Across the complete corpora, ~1% of C4, RefinedWeb, and Dolma tokens were restricted in mid 2023, as compared to 5-7% of tokens in April 2024. Among the most critical domains (HeadAll), 20-33% of all tokens are restricted, as compared to <3% one year prior (Figure 2a). From a relative perspective, from Apr 2023 to Apr 2024 these restrictions have risen 500%+ for both C4 and RefinedWeb's full corpus, and 1000%+ for both C4 and RefinedWeb's head sets. Note that these measurements only capture _fully_ restricted domains, and the numbers are higher for partially restricted domains.

**AI developers are restricted to widely varying degrees.** Figure 1 (Lower) breaks down the restrictions by AI developers and non-profit organizations. OpenAI crawlers are restricted for 25.9% of tokens in HeadC4, followed by Anthropic and Common Crawl (13.3%), Google's AI crawler (9.8%),

Figure 1: A temporal analysis, from 2016 to April 2024, of the web consent signals in HeadC4, a sample of the largest and most critical web domains. The colored regions represent the restriction categories as a portion of the total tokens in HeadC4. We also use SARIMA methods to forecast trends a year into the future. Top: Ascending categories of robots.txt restrictions for the AI Organizations: Google, OpenAI, Anthropic, Cohere, Meta, Common Crawl, and the Internet Archive. Middle: Ascending categories of Terms of Service restrictions (taxonomies described in Table 2). Bottom: A breakdown of robots.txt restrictions by organization, with the April 2024 restriction rates listed in the legend.

and more distantly Cohere (4.9%), Meta (4.1%), the Internet Archive (3.2%), and lastly Google Search's crawler (1.0%). These asymmetries in restrictions have significant differences, and tend to advantage less widely known AI developers. In Subsection 3.2 we discuss these asymmetries and their consequences in more depth.

**Terms of service pages have imposed more anti-crawling and now anti-AI restrictions.** Figure 1 (Middle) illustrates this gradual reformulation of terms pages--with web domains shifting from no terms pages, to those with restrictions on crawling, commercial use, using the data for competing services, or re-distribution. Only in 2024 do we see the wider emergence of terms which specifically mention and restrict the use of their data for generative AI. In the last year, we've seen a 26-53% relative increase in terms-of-service crawling restrictions across C4, RefinedWeb, and Dolma. Figure 1(c) shows 45-55% of all tokens in these three corpora have a form of data use restriction in their Terms pages. In practice, most automatic crawlers do not need these terms, though they may provide some avenue for subsequent legal enforcement.3

**AI restrictions are driven primarily by news, forums, and social media websites.** For robots.txt, Figure 1(b) shows nearly 45% of all News website tokens are fully restricted in HeadAll, as compared to 3% in 2023. For Terms of Service, Figure 1(d) shows News website tokens have had a 6% rise in the restricted portion since 2023. Paired with the findings in Table 2, this suggests that the composition of tokens in crawls respecting robots.txt may shift away from news, social media, and forums, and towards organization and e-commerce websites.

**Forecasting trends in the future suggest a continued and significant decline in open and consenting web data sources.** SARIMA forecasts suggest that for just the next year (by April 2025) an additional absolute 2-4% of C4, RefinedWeb, and Dolma tokens will be fully restricted by robots.txt. Equivalently, an additional 7-11% of the highest quality tokens in the head distribution will become restricted. The forecasts for Terms of Service are even starker, with the restricted tokens in the full corpus expected to rise an absolute 6-10% by April 2025. These trends illustrate a systematic rise in restrictions on data sources, which, where enforced or respected, will severely hamper data

Figure 2: A temporal depiction of the percentage of restricted tokens across both the full corpus and the HeadAll sample, which consists of the largest and most critical data sources. The robots.txt analysis (top) and terms of service analysis (bottom) are each broken down by corpus—C4, RefinedWeb, and Dolma (left)—and by domain type, averaged across corpora (right).

scaling practices in the coming years--practices which have thus far been responsible for remarkable capability improvements.

### Inconsistent and Ineffective Communication on AI Consent

In many cases, data providers and rightsholders fail to effectively communicate their preferences on how their data is used by AI systems. We observe robots.txt instructions which allow some AI organizations to crawl while restricting others, references to non-existent crawlers, and contradictions between the robots.txt and Terms of Service. Together, these issues point to the need for better preference signaling protocols.

Some AI crawlers are allowed, while others are not.We find not all AI agents are disallowed equally. In Table 3 we estimate the conditional probabilities of each organization's crawler being restricted, conditioned on whether any other AI organization is restricted. Whereas OpenAI and Common Crawl agents are frequently disallowed (in 91.5% and 83.4% of cases where _any_ of the organizations are disallowed), the agents of other AI companies, such as Google, Cohere, and Meta are often omitted from robots.txt. The omissions of Cohere, Meta, and other small AI organizations are likely because website administrators are unaware or unable to update their robots.txt to reflect the full list of AI developers. On the other hand, the particularly high omission rates of Internet Archive and Google Search suggest web administrators may be open to more traditional crawler uses like archiving and search engines, even as they seek to restrict AI usage. A full confusion matrix showing the correlation between restrictions for each user agent is provided in Appendix Figure 5.

Unrecognized crawler agents cause incorrect specifications.We find several instances where robots.txt refers to user agents that the companies do not recognize. For instance, 4.5% of websites disallowed the unrecognized user agents anthropic-ai or Claude-Web (documented as False Anthropic), but not the documented agent for Anthropic's crawler, ClaudeBot. The origin and reason for these unrecognized user agents remains unclear--Anthropic reports no use of them. These inconsistencies and omissions across AI agents suggest that a significant burden is placed on the domain creator to understand evolving agent specifications across (a growing number of) developers. AI crawler standardization could address these challenges in consent/preference signaling.

Contradictions exist between robots.txt and ToS.The Robots Exclusion Protocol (REP) is a guideline for web crawlers, while a website's terms of service is a legal agreement between the website and users of the site. The benefit of the REP is its machine-readability. However, its rigid structure, created in 1995, limits what signals it can convey. In contrast, a ToS can communicate

Figure 3: A cross-tabulation of the terms of service policies and robots.txt restrictions for HeadC4, measured in percentage of tokens. **We find that these two ways of expressing restrictions on data use for AI**_frequently disagree_, in both what they express and what they can express.

   Organization & Rest. (\%) \\  OpenAI & 91.5 \\ Common Crawl & 83.4 \\ Anthropic & 83.4 \\ Google Extended & 72.0 \\ False Anthropic & 61.6 \\ Cohere & 52.3 \\ Meta & 52.2 \\ Internet Archive & 32.3 \\ Google Search & 17.1 \\   

Table 3: The % each org’s crawler agents are **restricted** if at least one other org in this pool is restricted. Gray indicates crawlers with a primary purpose other than AI training data.

rich and nuanced policies in natural language. Without a robots.txt, a ToS lacks practical deterrence of unwanted crawling. Inversely, without a ToS, a robots.txt may lack any plausible means of enforcement . We found that in many cases, websites' robots.txt implementations fail to capture the intentions specified in their terms of service.

In Figure 3, we illustrate the distribution of terms and REP use criteria (the taxonomy is defined in Table 2 and broken down in detail in Appendix D). Common use criteria expressed in modern ToS pages include prohibitions specifically on commercial use, conditional use limiting actions such as third-party re-posting, non-compete criteria, or specific prohibitions only against "AI", but not against crawling for search engines. We also see many websites write anti-crawling terms but have no robots.txt file (35.1%), or have no ToS but a restrictive robots.txt (20.3%) that disallows at least some crawlers. Terms specifying only non-commercial uses are also often paired with fully or partially restrictive robots.txt files, which may unintentionally limit academic web crawlers, as a side effect of deterring corporate use. Another formidable challenge is that websites currently have to list every search engine or AI user agent they want to restrict. Empirical evidence from both Figure 5 and Figure 3 suggests the absence of REP expressivity and standardization for AI is leading to inconsistent or unintended signals that fail to reflect intended preferences.

### Correlating Features of Web Data

What does web data actually look like? Prior work has measured the characteristics of web-derived datasets, for the presence of artifacts [8; 11], undesirable text and images [37; 12], demographic biases , and quality discrepancies across languages . We expand upon these analyses by measuring what web data sources look like _before_ they have been neatly processed into AI training datasets. We measure the presence of multi-modal content, user-derived content, website monetization schemes, and sensitive content on the most well-represented web domains on the internet (HeadAll) and on a

   Variable &  &  &  \\  & Top 100 & Top 500 & Top 2000 & Random & Diff & C4 & RW & Dolma \\  Restrictive Robots.txt & **38.4** & **35.0** & **26.5** & 3.4 & +23.1 & 5.0\(\)1.5 & 6.6\(\)2.3 & 5.6\(\)1.9 \\ Restrictive Terms & **64.1** & **61.0** & **51.2** & 15.7 & +35.5 & 43.2\(\)15.5 & 52.8\(\)30.3 & 52.3\(\)15.4 \\  User Content & **21.3** & 19.1 & **19.4** & 15.1 & +4.4 & 27.9\(\)12.3 & 39.8\(\)32.8 & 37.3\(\)16.7 \\ Paywall & **31.8** & **31.3** & **24.6** & 1.6 & +23.0 & 4.1\(\)1.1 & 4.9\(\)0.4 & 10.8\(\)1.2 \\ Ads & **54.6** & **61.4** & **53.2** & 5.4 & +47.9 & 23.5\(\)12.6 & 44.8\(\)34.4 & 34.8\(\)18.1 \\ Modality: Image & 96.8 & 97.0 & 96.7 & 95.0 & +1.7 & 97.7\(\)2.3 & 98.6\(\)0.9 & 97.5\(\)1.9 \\ Modality: Video & **87.0** & **78.8** & **58.7** & 18.9 & +39.8 & 32.9\(\)14.2 & 27.0\(\)14.7 & 35.4\(\)10.6 \\ Modality: Audio & **80.7** & **68.3** & **41.8** & 3.4 & +38.4 & 21.2\(\)14.7 & 12.5\(\)6.3 & 20.5\(\)6.7 \\ Sensitive Content & 0.0 & 0.4 & 1.1 & 0.6 & +0.5 & 0.8\(\)1.0 & 0.2\(\)0.4 & 1.8\(\)3.0 \\    \\  Academic & **14.1** & **10.1** & **9.8** & 3.8 & +6.0 & 3.1\(\)1.6 & 2.6\(\)1.2 & 3.0\(\)0.7 \\ Blogs & **2.6** & **2.9** & **3.9** & 15.1 & -11.2 & 23.2\(\)11.3 & 16.3\(\)16.0 & 20.1\(\)11.9 \\ E-Commerce & 8.4 & 9.9 & 10.1 & 10.6 & -0.5 & 20.0\(\)17.8 & 32.6\(\)37.6 & 17.7\(\)19.1 \\ Encyclopedia/Database & **20.5** & **13.2** & **11.1** & 0.4 & +10.7 & 3.5\(\)3.4 & 5.8\(\)9.8 & 5.1\(\)5.8 \\ Government & **3.2** & **2.8** & **2.8** & 1.1 & +1.7 & 0.9\(\)0.9 & 0.9\(\)0.8 & 0.8\(\)0.6 \\ News/Periodicals & **45.6** & **53.3** & **50.0** & 5.3 & +44.7 & 11.5\(\)3.9 & 16.8\(\)10.8 & 22.9\(\)10.9 \\ Org/Personal Website & **15.3** & **13.2** & **12.7** & 71.2 & 58.5\(\)13.3 & 57.3\(\)24.2 & 46.3\(\)14.2 \\ Social Media/Forums & **9.4** & **9.3** & **11.8** & 1.6 & +10.1 & 5.1\(\)4.8 & 5.4\(\)8.9 & 14.9\(\)8.3 \\ Other & **15.0** & **10.9** & **11.8** & 4.3 & +7.4 & 4.7\(\)2.7 & 2.8\(\)1.3 & 3.7\(\)2.0 \\   

Table 4: **Mean incidence rates of web source features across C4, RefinedWeb, and Dolma.** We measure incidence rates for the top 100, 500, and 2000 URLs, ranked by number of tokens, as well as the random sample. The ‘Diff’ column reports the % difference between the top 2k and random samples. We test for significant differences between the overall corpus and each of the top-100, top-500 and top-2000 sets with a Bonferroni-corrected two-sided permutation test, where differences significant at the Bonferroni-corrected \(5\) level are indicated in bold. We also estimate the percentage of tokens in each corpus, C4, RefinedWeb, and Dolma, for which the web feature is present (\(\) 95% bootstrap CI shown in gray), by computing the final percentage of tokens based on the estimate for the unobserved population (from the random sample), and the observed head sample.

random sample of domains (Random2k). We also annotate the services provided and purpose of each web domain.

**Most of the web is composed of organizational/personal websites, and blogs, however the head distribution is disproportionately news, forums, and encyclopedias.** Table 4 shows several notable and statistically significant differences between head distribution (HeadAll) and tail distribution (Random2k) of web domains. HeadAll comprises mostly news, social media/forums, and encyclopedias (72.9%), in contrast to the long tail data in Random2k, which is dominated by personal or organization websites, blogs and E-commerce sites (97%). Academic and government content is also proportionately more common in the head distribution. Note however that though they are all derived from Common Crawl snapshots, C4, RefinedWeb, and Dolma all show variations in their source compositions--highlighting the importance of curation choices.

**The head distribution of domains is more multimodal, and heavily monetized.** We observe that HeadAll web domains are much more heavily monetized through ads (+47.5%) and paywalls (+24.1%). Accordingly, they also have significantly greater restrictions from both robots.txt (+22.5%) and terms of service (+35.3%). This greater prevalence of monetization and restrictions likely corresponds to the higher quality and heterogeneity of content usually produced by news, periodicals, forums, and databases, which are more common in HeadAll. This is reflected by the higher proportions of image (+4.4%), video (+39.8%), and audio content (+38.4%) than the rest of the web. Interestingly, the fraction of user-generated content and sensitive content between the head and tail distributions is less pronounced. Crawlers which respect the restrictions that occur far more frequently in HeadAll will increasingly lose access to the most multimodal, highly curated, and up-to-date content sources.

## 4 Discussion

Contextual Background and MotivationWebsite content permissions are rapidly changing within a broader context. Dataset creators and model developers have conflicting interests, particularly financial ones, as creators seek credit and compensation for their work . AI systems' potential to replace human labor  leads many creators to oppose using their data in potentially competing systems . On top of these considerations, copyright law remains unclear on AI and training data issues, including generated text authorship (because a model is not a legal person), fair use boundaries, and infringement thresholds . Legal clarification will require extensive work from regulatory agencies, courts and legislatures. All parties lack the certainty and protection that well established law provides. This uncertainty is amplified by the abandonment of pre-AI conventions for scraped data use  in current AI applications . These legal and economic uncertainties drive creators toward increased data restrictions.

The web-sourced AI data commons is rapidly becoming more restricted.The web has acted as the primary "data commons" for general-purpose AI. Its scale and heterogeneity have become fundamental to advances in capabilities. However, our results show web domains are rapidly restricting crawling and use of their content for AI. In less than a year, ~5% of the tokens in C4 and other major corpora have recently become restricted by robots.txt. And nearly 45% of these tokens now carry some form of restrictions from the domain's terms of service. If these rising restrictions are respected by model developers (as many developers claim) or are legally enforced, the availability of high-quality pretraining sources will rapidly diminish.

The rise in restrictions will skew data representativeness, freshness, and scaling laws.Prior work has emphasized scaling data as essential to improving frontier model capabilities . While the trend toward increasingly restricted data will respect content creators' intentions, it will also challenge these data scaling laws . Not only do these restrictions reduce the scale of available data, they also change the composition (away from news and forums), diversity, and representativeness of training data--biasing this data toward older and less fresh content.

Recently, multiple AI developers have been accused of bypassing robots.txt opt-outs to scrape publisher websites . While it is not possible to confirm, in each case it appears AI systems may be distinguishing between crawling data for training, and crawling data to retrieve information for user questions at inference time. One of the few, OpenAI has two crawler agents, GPTBot for training, and ChatGPT-User for live browsing plugins (see Table 5). Other companies may simply not be registering their inference time crawlers for opt-outs. This circumvention may allow developers to directly attribute the retrieved web pages, as well as better achieve data representativeness, freshness, and approximate the scaling laws had they trained on it. However, creators may feel this violates the spirit of the opt-outs, especially if the opportunity to attribute sources is not taken.

The web needs better protocols to express intentions and consent.The REP places an immense burden on website owners to correctly anticipate all agents who may crawl their domain for undesired downstream use cases. We consistently find this leads to protocol implementations that don't reflect intended preferences. An alternative scheme might give website owners control over _how_ their webpages are used rather than _who_ can use them. This would involve standardizing a taxonomy that better represents downstream use cases, e.g. allowing domain owners to specify that web crawling only be used for search engines, or only for non-commercial AI, or only for AI that attributes outputs to their source data. New commands could also set extended restriction periods, because dynamic sites may want to block crawlers for extended periods of time (e.g. for news organizations to protect their most recent work). Ultimately, a new protocol should lead to website owners having greater capacity to self-sort consensual from non-consensual uses, implementing machine-readable instructions that approximate the natural language instructions in their terms of service.

Rising expressions of non-consensual will affect non-profits, archives, and academic researchers.A new wave of robots.txt and terms of service pages have notdistinguished (or cannot distinguish) the various uses of their data. For instance, having to individually prohibit a plethora of AI crawlers has motivated many domains to switch to a blanket prohibition of any crawling with the wildcard "*" marker. Domains have also limited crawlers from non-profit archives such as the Common Crawl Foundation or Internet Archive, in order to prevent other organizations from downloaded their data for training. However, these archives are also used for non-commercial uses of AI, as well as academic research, knowledge, and accountability, well beyond the scope of AI. For instance, the Common Crawl is reported to be cited in 10,000+ research articles from varying fields.4 This tension between data creators and, predominantly, commercial AI developers has left academic and non-commercial interests as secondary victims. As web consent continues to evolve, we believe it is essential that these widely used facilities not be marginalized or severely hampered.

## 5 Related Work

Prior work has conducted large scale audits of the provenance, quality, biases, and characteristics of AI training data, for pretraining text [9; 10; 8; 49], finetuning text , as well as multimodal datasets [12; 50; 51; 52; 53], and challenges in data development . Recent work has looked at collecting non-copyrighted data , interpreting the legal implications of fair use for AI data [55; 56], and forecasting future data constraints . However, there is little work inspecting the evolution of consent signals on AI data. Prior research has attempted to understand link decay on the web , Common Crawl's collection process , and web crawlers' evolving behavior and implications [59; 60; 61; 62; 63]. Initial news reports have begun to investigate the rates of blocking AI web crawlers for general websites  and news publishers , setting the stage for our more rigorous analysis. The dearth of AI dataset documentation [66; 67; 68; 69] has been highlighted as a challenge for understanding model behavior [70; 71; 72; 73; 74], reproducibility, consent, and authenticity .

## 6 Conclusion

In this work, we presented the first large-scale audit of the web sources underlying the massive training corpora for modern, general-purpose AI. Our audit of \(14,000\) web domains provides a view the changing nature of crawlable content, consent norms, and points to daunting trends for the future openness of the highest quality data used to train AI. The inconsistencies and omissions between robots.txt and terms of service pages suggest a data ecosystem ill-equipped to signal or enforce preferences. Lastly, we uncover distributional mismatches in the documented real uses of AI systems and their underlying data. We release all our collected annotations and analysis, with the hope that future work will further investigate the provenance, consent, and composition of these fundamental ingredients in AI systems.5