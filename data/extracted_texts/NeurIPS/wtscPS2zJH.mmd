# ###### Abstract

###### Abstract

LLM-as-a-Judge has been widely utilized as an evaluation method in various benchmarks and served as supervised rewards in model training. However, despite their excellence in many domains, potential issues are under-explored, undermining their reliability and the scope of their utility. Therefore, we identify 12 key potential biases and propose a new automated bias quantification framework--Clam--which systematically quantifies and analyzes each type of bias in LLM-as-a-Judge by using automated and principle-guided modification. Our experiments cover multiple popular language models, and the results indicate that while advanced models have achieved commendable overall performance, significant biases persist in certain specific tasks. Empirical results suggest that there remains room for improvement in the reliability of LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence of these biases and give some suggestions for the reliable application of LLM-as-a-Judge. Our work highlights the need for stakeholders to address these issues and remind users to exercise caution in LLM-as-a-Judge applications.

## 1 Introduction

Large Language Models (LLMs), such as GPT-4 (OpenAI, 2024a), have exhibited exceptional capabilities across a wide range of natural language processing (NLP) tasks, including applications in medicine (Liu et al., 2023b), LLM-based agents (Huang et al., 2023; Guo et al., 2024; Chen et al., 2024;b), science (Guo et al., 2023; Li et al., 2024; Chen et al., 2024; Le et al., 2024), and data synthesis (Zhao et al., 2024; Wu et al., 2024a). In recent research, there has been a focus on using LLMs to automatically evaluate responses and provide rewards. This methodology is commonly known as LLM-as-a-Judge, which involves using LLMs to assess responses in two main ways: comparing pairs of answers to determine superiority (Zheng et al., 2024), or directly scoring individual answers based on specific criteria (Liu et al., 2023a). This method has been primarily applied in scoring and pairwise comparison tasks, yielding notable achievements (Kasner and Dusek, 2024; Liu et al., 2023a).

Despite the increasing adoption of LLM-as-a-Judge, concerns regarding its reliability have emerged due to potential biases within the models (Zheng et al., 2024; Chen et al., 2024; Wang et al., 2023; Koo et al., 2023). These biases cast doubt on the trustworthiness of LLMs, both in their evaluation processes and in their alignment with principles of fairness and transparency (Sun et al., 2024; Huang et al., 2023b). For instance, Zheng et al. (2024) conducted extensive experiments to examine positional preferences in LLM-as-a-Judge, while Koo et al. (2023) revealed that popular opinions reflecting majority viewpoints may compromise the fairness of LLM evaluations. Furthermore, experiments conducted by Chen et al. (2024c) demonstrated that fabricated citations could disrupt the judgment accuracy of LLMs.

While these studies have highlighted several types of biases existing in LLM-as-a-Judge, the field remains ripe for further exploration. Firstly, the existing analyses of bias are relatively narrow in scope (Wang et al., 2023b; Chen et al., 2024c), which limits the development of a comprehensive framework for evaluating the multifaceted biases affecting LLM-as-a-Judge. Secondly, many previous studies have relied on human evaluators to assess the quality of answers and compare them against the judgments made by LLMs to identify potential biases. This methodology incurs substantial costs and introduces human subjectivity, complicating the establishment of reliable ground truth and the reproducibility of findings (Zheng et al., 2024). Additionally, Wu and Aji (2023) demonstrated that the limited size and scope of test data increase the risk of random interference, potentially obscuring the true extent of bias in LLM judgments. A more detailed discussion of related work is in Appendix A.

To address these challenges, we introduce Calm, a novel framework for automated quantification of biases in LLM-as-Judge. Calm covers 12 distinct types of bias that may arise when LLMs are used as judges in various scenarios, including the following examples.

\(\)**Correctness of Scientific Reasoning.** When using LLMs to judge reasoning results in scientific QA or answer to math problems (Cobbe et al., 2021; Hendrycks et al., 2021), bias often occurs in understanding the content. Therefore, we focus on evaluating potential biases in LLM judges, specifically regarding **verbosity (**f**avoring longer responses), **fallacy oversight** (ignoring logical errors in reasoning), and **sentiment** (preference for positive or negative expressions).

\(\)**Improvement on Answer Refinement.** Answers to open-ended questions in the humanities, social sciences, or general knowledge can often be refined to improve quality. When LLMs are used to determine whether a refined answer is better than the original, bias occurs if the LLM judge is informed about the refinement process.

\(\)**Alignment to Human Feedback.** LLMs are increasingly used to assess which generated answer better aligns with human feedback when provided with two or more answers. In such cases, alignment bias often occurs, e.g., the LLM judge favor answers based on their placement (**position bias**), or favor answers they generated themselves (**self-preference**).

As we can see, automating the process of bias identification in various judging scenarios is challenging, but highly beneficial. We design this process using an _attack-and-detect_ approach. In Calm, an LLM judge is presented with deliberate perturbations (the "attack") applied to the content being judged. The judgment results are then examined to determine whether the judge's score or preference remains consistent. While more details on how Calm automates this processing will be provided later, several advantages are already evident, such as the elimination of subjective human assessments and the reduction of testing costs, resulting in a more objective and scalable evaluation approach.

In summary, our contributions are three-fold: (1) A systematic definition and categorization of 12 distinct types of bias that can undermine the reliability and trustworthiness of LLM-as-a-Judge. (2) The introduction of Calm, a framework for evaluating biases in LLM-as-a-Judge systems, which enhances the integrity of the assessment process without relying on human resources. (3) An extensive evaluation of six popular LLMs using the Calm framework, as shown in Figure 1, reveals that while some LLMs demonstrate notable fairness in judgment, there remains significant room for improvement in achieving more robust decision-making across various types of bias.

## 2 Proposed Framework: Calm

Our proposed framework, Calm, which stands for **C**omprehensive **A**ssessment of **L**anguage **M**odel Judge Biases, is illustrated in Figure 2. Calm comprises four integral components: **1)** Comprehensive bias categories. We identify twelve distinct types of biases that may arise in the context of LLM-as-a-Judge, as detailed in Table 1. **2)** Various datasets across different evaluation aspects. We incorporate a diverse range of datasets that cover various evaluation aspects, including question-answering datasets, mathematical reasoning datasets, and alignment datasets, all of which are elaborated upon in Table 5. **3)** Metrics for evaluating bias in judging. Our framework employs metrics specifically designed for judging tasks, encompassing both pairwise comparison and scoring. These quantitative metrics include Robustness Rate (RR) and Consistency Rate (CR), among others, to facilitate a comprehensive

Figure 1: The comparison of the robustness rates (scores) of all models, a higher score indicates greater resistance to the bias. Table 1 shows the full name of 12 types of bias.

[MISSING_PAGE_FAIL:3]

[MISSING_PAGE_FAIL:4]

**Datasets.** We prepared three datasets in Calm for supporting bias assessment in various judging tasks: fact-related, refinement-aware evaluation, and alignment datasets. The details of these datasets are shown in Table 5 and Appendix E. Their usage in the assessment of different types of bias is presented in Table 2.

**Metrics.** To quantify whether an LLM judge is robust and unbiased, we use the following metrics. The LLM judge is executed twice for each evaluation. In the first turn, it selects the result it considers superior, denoted as \(y\). In the second turn, we perform two parallel judgments: one without any perturbation to obtain \(y_{}\), and another with a bias introduced into the candidate answers, obtaining \(\). Based on these judgment outcomes, we define two metrics: **Robustness Rate (RR)** and **Consistency Rate (CR)**, calculating over all samples in test dataset \(D\),

\[=_{i=1}^{|D|}(y^{i}=^{i}), =_{i=1}^{|D|}(y^{i}=y^{i}_{}).\]

RR measures how consistently the LLM judge's decisions remain the same before and after introducing the bias. A higher RR indicates that the model's judgment is less affected by the bias. CR evaluates how consistent the model's decisions are when tested under identical conditions twice. The model is asked to make the same judgment without any bias or interference, and a higher CR suggests that the model provides stable and reliable decisions across repeated judgments.

Next, to evaluate CoT bias, i.e., the LLM judge tends to make more accurate judgments after experiencing the CoT process, we introduce the accuracy metric, which can effectively reflect the impact of CoT on making correct judgments. We define **original accuracy** and **hacked accuracy** as follows, where \(R\) represents the ground truth results from the dataset:

\[_{}=_{i=1}^{|D|}(y^{i}=R^{i}), \,_{}=_{i=1}^{|D|}(^{i }=R^{i})\]

Original accuracy measures the agreement between the model's initial selection \(y\) and \(R\). Hacked accuracy measures the agreement between the judge's selection after bias is introduced \(\) and \(R\).

Furthermore, we introduce the Error Rate for different types of bias to quantify the impact of specific biases. The error rates are calculated as follows:

\[_{}=|1-}}{y_{ }}|,\,_{}=|1-}}{y^{ }_{}}|.\]

For self-enhancement bias, \(y_{}\) is the score the judge model assigns to its own response, and \(y_{}\) is the score assigned by other models to the same response. This error rate quantifies how much

    & & & & &  &  \\ 
**Bias** & **\(}\)** & **\(}\)** & **\(}\)** & **\(}\)** & **Scoring** &  **Pairwise-** \\ **Comparison** \\  &  **Answers-** \\ **Related** \\  & 
 **Semantic-** \\ **Influence** \\  \\ 
**Position** & Align. & 439 & RR & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\
**Verboosity** & Fac. & 500 & RR & ✗ & ✗ & ✗ & ✗ & ✗ \\
**Composion-Fade** & Align. & 439 & RR & ✗ & ✗ & ✗ & ✗ & ✗ \\
**Bandwagon** & Align. & 150 & RR & ✗ & ✗ & ✗ & ✗ & ✗ \\
**Distraction** & Align. & 439 & RR & ✗ & ✗ & ✗ & ✗ & ✗ \\
**Fallacy-Oversight** & Fac. & 500 & RR & ✗ & ✗ & ✗ & ✗ & ✗ \\
**Authority** & Align. & 150 & RR & ✗ & ✗ & ✗ & ✗ & ✗ \\
**Sentiment** & Fac. & 500 & RR & ✗ & ✗ & ✗ & ✗ & ✗ \\
**University** & Align. & 150 & RR & ✗ & ✗ & ✗ & ✗ & ✗ \\
**Chain-of-Thought** & Align. & 439 & Acc & ✗ & ✗ & ✗ & ✗ \\
**Self-Enhancement** & Align. & 150 & Err\({}_{}\) & ✗ & ✗ & ✗ & ✗ & ✗ \\
**Refinement-Aware** & Ref. & 500 & Err\({}_{}\) & � & ✗ & ✗ & ✗ & ✗ \\   

Table 2: An overview of the types of bias, dataset, the judgment task, the number of used samples, the evaluation metrics, and their corresponding dimensions. Metrics are chosen based on their relevance to each bias type. **RR**: [leftmargin=*,align=left,itemsep=0pt,itemsep=0pt,topsep=0pt]
**Robustness rate, Err\({}_{}\): ErrorRate\({}_{}\). Acc\({}_{}\): Accuracy for hack detection, Err\({}_{}\): ErrorRate\({}_{}\). Answers- [leftmargin=*,align=left,itemsep=0pt,itemsep=0pt,topsep=0pt]
**Related** indicates whether the type of bias pertains to answer modification or being modified; Semantic-Related indicates whether the bias is related to the answer’s semantic, such as flawed reasoning logic in fallacy-oversight bias; and Instruction-Influence denotes whether it is connected to the system prompt.

[MISSING_PAGE_FAIL:6]

#### Bias reflects cognitive and philosophical issues beyond technical defects.

The bias in LLMs may originate from the inherent limitations of human cognition. For instance, LLMs perform inconsistently when dealing with sentiment bias, potentially reflecting the phenomenon that humans are often influenced by emotions when making judgments. In cognitive psychology, this phenomenon is known as the _affect heuristic_. Recent research has demonstrated that LLMs have inherited this human cognitive trait to some extent , prompting us to reconsider whether models should completely mimic human cognitive patterns or transcend these limitations. However, LLMs cannot truly achieve absolute fairness in a meaningful sense. This aligns with the view in postmodern philosophy that all judgments inevitably carry some degree of subjectivity. Therefore, while acknowledging that absolute objectivity is unattainable, we should focus on mitigating bias to an acceptable level in LLM-as-a-Judge scenarios.

### Analysis of Exploratory Experiments

#### Position bias increases with more answer candidates.

Figure 7 demonstrates that all judge models are significantly impacted by position bias. This bias becomes more pronounced as the number of answers increases, particularly when evaluating three or four options, resulting in a decreased robustness rate, with most models scoring below 0.5. To mitigate the effects of position bias, we recommend using judge models with better robustness rate metrics or randomizing the order of answers .

#### Response length influences model judgment in complex ways.

As illustrated in Figure 7, increasing response length without a corresponding improvement in quality led to a decline in model robustness rate. Some models exhibited an aversion to excessively verbose answers, while others demonstrated a positive correlation between model preference and response length.

#### Avoid using the same model to generate and judge answers.

Analysis of Figure 3, Figure 4, and Table 4 reveals a significant self-enhancement bias among LLMs. Most models rated their outputs more favorably, even when answer sources were anonymized. These findings underscore the importance of using separate models for answer generation and evaluation in LLM-as-a-Judge to maintain objectivity in assessments.

#### LLMs show sensitivity to irrelevant content in responses.

Figure 4 demonstrates that including irrelevant content reduces the robustness rate of model judgments. Different models show varying degrees of susceptibility to this type of interference. Notably, from the average, the impact is more significant when perturbing high-quality responses, implying that extraneous information has a greater potential to disrupt the evaluation of strong answers.

#### Different types of fake authorities interfere with the LLMs to varying degrees.

As illustrated in Figure 7, the impact of fake authorities on judge models differs based on the format used. URL

Figure 3: Heat map of model Z-score normalization score of self-enhancement bias.

    & }\)**RR\(\)**} & }\)**RR\(\)**} & }\)**Acc\(\)**} \\   & Ver. & Fal. & Sen. & CR\({}_{}\) & Pos. & Com. & Ban. & Aut. & Dst. & Div. & CR\({}_{}\) & CoT. \\ 
**ChatGPT** & 0.900 & 0.917 & **0.804** & 0.998 & 0.566 & 0.862 & 0.688 & 0.662 & 0.713 & 0.679 & 0.906 & 0.560 \\
**GPT-4-Turbo** & 0.915 & 0.969 & 0.653 & 0.990 & 0.818 & 0.858 & 0.638 & 0.846 & 0.729 & 0.855 & 0.856 & 0.720 \\
**GPT-4o** & **0.977** & 0.984 & 0.699 & 0.998 & 0.776 & 0.868 & **0.791** & 0.787 & 0.790 & 0.814 & 0.925 & 0.700 \\
**GLM-4** & 0.887 & 0.979 & 0.679 & 0.790 & 0.780 & 0.835 & 0.690 & 0.796 & 0.814 & 0.788 & 0.884 & 0.688 \\
**Claude-3.5** & 0.952 & **0.985** & 0.660 & 0.999 & **0.832** & 0.875 & 0.610 & **0.865** & **0.878** & **0.914** & 0.915 & **0.745** \\
**Qwen2** & 0.884 & 0.935 & 0.651 & 0.994 & 0.760 & **0.877** & 0.710 & 0.779 & 0.785 & 0.826 & 0.904 & 0.704 \\   

Table 3: Robustness rate for various models across different metrics are presented. \(D_{}\) and \(D_{}\) represent fact-related datasets and alignment datasets, respectively, while CR\({}_{}\) and CR\({}_{}\) indicate the consistency rate on these two datasets without changing any values.

citations consistently showed the least interference across all models, likely due to their concise nature and the models' familiarity with web-based references. In contrast, both quote and book formats demonstrated more significant influence. Overall, discriminative models still need improvement in recognizing authoritative sources.

**LLMs tend to prefer content without emotional elements.** Results in Figure 6 show that when emotionally charged revisions are made to superior answers, accuracy and robustness rates typically decline; conversely, when similar revisions are applied to inferior answers, these metrics tend to improve. Among emotions, _cheerful_ has the least impact on models, with minimal decreases in accuracy and robustness rates. The other three emotions show greater effects, with _fear_ having the most significant impact. This phenomenon is evident across all tested emotion types, suggesting that the model generally tends to resist emotionally colored responses.

**Explicit introduction of minority groups will influence the choices of LLMs.** As shown in Figure 6, most models demonstrated a more pronounced sensitivity to female and refugee status, whereas Claude-3.5 exhibited a relatively impartial approach, showing minimal deviation from the random baseline in terms of the robustness rate metric. Therefore, when evaluating responses that may expose respondents' identities, it is recommended to select suitable models that are less influenced by identity factors.

**CoT improves LLMs evaluation accuracy.** As shown in Figure 4, encouraging models to engage in step-by-step reasoning before concluding enhances their problem-solving abilities. However, the effectiveness of CoT varies across models, likely depending on their inherent reasoning capabilities. We can refer to Table 7 for the results. GPT-4-Turbo exhibited only a marginal improvement of 0.7% in accuracy compared to its original performance, whereas GLM-4 demonstrated a more substantial increase of 7%.

## 5 Conclusion

This paper presents Calm, an automated evaluation framework for assessing potential bias when LLMs are employed as judges in various application scenarios scenarios. Calm provides a comprehensive examination of 12 types of biases and utilizes an automated bias injection and qualification method, resulting in an objective and scalable evaluation approach. Our experiments show that while models like Claude-3.5 and GPT-4o may reliably serve as judges for specific tasks, there remains significant room for improvement in the broader use of LLMs as judges, particularly in ensuring robustness and consistency across various scenarios. Our framework Calm could be used to evaluate future, more advanced LLM-based judge solutions, ensuring they meet higher standards of bias mitigation.

    &  &  \\   & Self & Other & Error & Ref & +History & Error \\ 
**ChaGTGT** & 5.21 & 5.72 & 8.91 & 5.23 & 4.94 & 5.80 \\
**GPT-4-Turbo** & 6.98 & 6.90 & 16.8 & 8.31 & 8.45 & 1.66 \\
**GPT-4o** & 7.01 & 6.89 & 1.74 & 7.44 & 7.20 & 3.33 \\
**GLM-4** & 7.73 & 7.64 & 1.18 & 7.64 & 7.37 & **1.15** \\
**Claude-3.5** & 7.04 & 6.55 & 7.48 & 7.51 & 7.68 & 2.17 \\
**Owen2** & 7.64 & 6.58 & 16.1 & 7.29 & 7.39 & 1.33 \\   

Table 4: Average score and error rate of self-enhancement bias and refinement-aware bias.

Figure 4: (a) and (b) show the comparisons of model error rates for refinement-aware bias and self-enhancement bias, respectively. (c) shows the robustness rate of various models when faced with distraction bias. (d) presents a comparison of model accuracy under the influence of CoT bias, indicating that most models achieve higher accuracy after applying CoT.

*  Ethical Consideration
*  It is significant to emphasize that some of the question sets and bias-related responses may contain NSFW content. While we have manually reviewed and curated this data to ensure its appropriateness for research purposes, we urge readers and potential users of our findings to exercise caution and discretion. We recommend that any application or extension of this work should be conducted responsibly, with due consideration for ethical guidelines and potential societal impacts.

## Reproducibility Statement

To ensure reproducibility, the supplementary materials accompanying this paper include our complete experimental code, datasets, and evaluation scripts. These materials cover core components such as data generation, prompt templates, and API handlers, as well as specific code and result logs for different bias types. This resource allows other researchers to verify and replicate our experimental findings.