# Non-autoregressive Machine Translation with Probabilistic Context-free Grammar

Shangtong Gui\({}^{1,2,3}\),Chenze Shao\({}^{2,3}\),Zhengrui Ma\({}^{2,3}\),Xishan Zhang\({}^{1,4}\),Yunji Chen\({}^{1,3}\),Yang Feng\({}^{2,3}\)

\({}^{1}\)State Key Lab of Processors,

Institute of Computing Technology, Chinese Academy of Sciences

\({}^{2}\)Key Laboratory of Intelligent Information Processing,

Institute of Computing Technology, Chinese Academy of Sciences

\({}^{3}\)University of Chinese Academy of Sciences

\({}^{4}\)Cambricon Technologies

Corresponding author: Yang Feng

\({}^{1}\)State Key Lab of Processors,

Institute of Computing Technology, Chinese Academy of Sciences

\({}^{2}\)Key Laboratory of Intelligent Information Processing,

Institute of Computing Technology, Chinese Academy of Sciences

\({}^{3}\)University of Chinese Academy of Sciences

\({}^{4}\)Cambricon Technologies

###### Abstract

Non-autoregressive Transformer(NAT) significantly accelerates the inference of neural machine translation. However, conventional NAT models suffer from limited expression power and performance degradation compared to autoregressive (AT) models due to the assumption of conditional independence among target tokens. To address these limitations, we propose a novel approach called PCFG-NAT, which leverages a specially designed Probabilistic Context-Free Grammar (PCFG) to enhance the ability of NAT models to capture complex dependencies among output tokens. Experimental results on major machine translation benchmarks demonstrate that PCFG-NAT further narrows the gap in translation quality between NAT and AT models. Moreover, PCFG-NAT facilitates a deeper understanding of the generated sentences, addressing the lack of satisfactory explainability in neural machine translation.2

## 1 Introduction

Autoregressive machine translation models rely on sequential generation, leading to slow generation speeds and potential errors due to the cascading nature of the process. In contrast, non-autoregressive Transformer (NAT) models  have emerged as promising alternatives for machine translation. The Vanilla NAT model assumes that the target tokens are independent of each other given the source sentence. This assumption enables the parallel generation of all tokens. However, it neglects the contextual dependencies among target language words, resulting in a severe **multi-modality problem** caused by the rich expressiveness and diversity inherent in natural languages. Consequently, Vanilla NAT models exhibit significant performance degradation compared to their autoregressive counterparts.

Several approaches have been proposed to alleviate the multi-modality problem in NAT models while maintaining their computational efficiency. CTC-based NAT  depicted in Figure 1(b) addresses this issue by implicitly modeling linear adjacent dependencies through token repetition and blank probability. This approach helps alleviate the multi-modality of token distributions at each position compared to Vanilla NAT. Similarly, DA-Transformer  shown in Figure 1(c) introduces a hidden Markov model and employs a directed acyclic graph to model sentence probability, enabling the model to capture dependencies between adjacent tokens and reduce the multi-modal nature of the distribution.

However, despite these enhancements, the modeling of only adjacent and unidirectional dependencies limits the model's ability to capture the rich semantic structures with non-adjacent and bidirectional dependencies present in natural language. For example, considering the token _need_ as the previous token, the distribution of the next token is still affected by the issue of multi-modality, as the word _boat_ can be modified by various rich prefixes in the corpus.

To overcome this constraints, we propose PCFG-NAT, a method that leverages context-free grammar to endow the model with the capability to capture the intricate syntactic and semantic structures present in natural language. For instance, as illustrated in Figure 1(d), PCFG-NAT can directly model the robust and deterministic correlation between non-adjacent tokens, such as _You need boat_. In the parse tree generated by PCFG-NAT for the target sentence, the nodes connected to the tokens _You need boat_ constitute the main chain of the tree, capturing the left-to-right semantic dependencies through linear connections. Additionally, PCFG-NAT can learn relationships between modifiers, such as _are gonna_ and _a bigger_, and the words they modify by constructing a local prefix tree for the nodes on the main chain. This allows for the modeling of right-to-left dependencies between the modified words and their modifiers.

We conduct experiments on major WMT benchmarks for NAT (WMT14 En+De, WMT17 Zh+En, WMT16 En+Ro), which shows that our method substantially improves the translation performance and achieves comparable performance to autoregressive Transformer  with only one-iteration parallel decoding. Moreover, PCFG-NAT allows for the generation of sentences in a more interpretable manner, thus bridging the gap between performance and explainability in neural machine translation.

## 2 Background

In this section, we will present the necessary background information to facilitate readers' comprehension of the motivation and theoretical underpinnings of PCFG-NAT. In Section 2.1, we will provide a formalization of the machine translation task and introduce the concepts of autoregressive Transformer  and Vanilla NAT , which will serve as the basis for our proposed approach. Subsequently, in Section 2.2, we will provide a precise and rigorous definition of PCFG, accompanied by illustrative examples aimed at enhancing clarity and comprehension.

### Non-autoregressive Translation

Machine translation can be formally defined as a sequence-to-sequence generation problem. Given source sentence \(X=\{x_{1},x_{2},...,x_{S}\}\), the translation model is required to generate target sentence \(Y=\{y_{1},y_{2},...,y_{T}\}\). The autoregressive translation model predicts each token on the condition of all previous tokens:

\[P(Y|X)=_{t=1}^{T}p(y_{t}|y_{<t},X).\] (1)

Figure 1: Illustration of Vanilla NAT, CTC-NAT, DA-Transformer, and PCFG-NAT

The autoregressive inference is time-consuming as it requires generating target tokens from left to right. To reduce the decoding latency, Gu et al.  proposes non-autoregressive Translation, which discards dependency among target tokens and predicts all the tokens independently:

\[P(Y|X)=_{t=1}^{T}p(y_{t}|X).\] (2)

Since the prediction does not rely on previous translation history, the decoder inputs of NAT are purely positional embeddings or the copy of source embeddings. Vanilla NAT trains a length predictor to determine the sentence length when decoding. During the training, the golden target length is used. During the inference, the predictor predicts the target length and the translation of the given length is obtained by argmax decoding.

### Probabilistic Context-free Grammar

Probabilistic Context-free Grammar is built upon Context-Free Grammar (CFG). A CFG is defined as a 4-tuple:

\[G=(,,,S),\]

where \(\) is the set of non-terminal symbols, \(\) is the set of terminal symbols, \(\) is the set of production rules and \(S\) is the start symbol. PCFG extends CFG by associating each production rule \(r R\) with a probability \(P(r)\). Production rules are in the form:

\[A,\] (3)

where \(A\) must be a single non-terminal symbol, and \(\) is a string of terminals and/or nonterminals, \(\) can also be empty. Starting with the sequence that only has start symbol \(S\), we can replace any non-terminal symbol \(A\) with \(\) based on production rule \(A\). The derivation process repeats until the sequence only contains terminal symbols and the derived hierarchical structure can be seen as a **Parse Tree**. Those production rules and their probabilities can be applied regardless of the contexts, which provides Markov property for the joint probability of the parse tree. For every rule, \(A\) applied in a derivation, all of the symbols in \(\) are the children nodes of \(A\) in order. The probability of a parse tree \(t\) is formulated as:

\[P(t)=_{r R(t)}(P(r))^{n_{r}},\] (4)

where \(R(t)\) is the set of production rules applied in \(t\) and \(n_{r}\) is the number of times that \(r\) is applied. For sentence _You are gonna need a bigger boat._, Figure 2 shows an example parse tree and how the sentence is derived from the start symbol \(V_{1}\).

## 3 The Proposed Method

In the following sections, we present the formal definition of a novel variant of PCFG designed for PCFG-NAT in Section 3.1 and the architecture of the PCFG-NAT model in Section 3.2. Finally, we describe the training algorithm and decoding algorithm for PCFG-NAT in Section 3.3 and Section 3.4.

   Applied Rules & Sequence \\  Start & \(V_{1}\) \\ \(V_{1}-V_{0}\)You \(V_{1},V_{0}\) & You \(V_{5}\) \\ \(V_{3} V_{5}\) need \(V_{9}\) & You \(V_{5}\) need \(V_{9}\) \\ \(V_{3}\)V\(\)are \(V_{4},V_{0}\) & You are \(V_{4}\) need \(V_{9}\) \\ \(V_{4}-\) gonna & You are gonna need \(V_{9}\) \\ \(V_{9}\)V\(\)but \(V_{10}\) & You are gonna need \(V_{7}\) boat \(V_{10}\) \\ \(V_{7}\)\(V_{0}\)\(V_{8},V_{0}\) & You are gonna need a \(V_{8}\) boat \(V_{10}\) \\ \(V_{8}\) bigger & You are gonna need a bigger boat \(V_{10}\) \\ \(V_{10}.\) & You are gonna need a bigger boat. \\   

Table 1: Derivation of the parse tree. Used production rules for every step are listed, where \(\) stands for an empty string.

Figure 2: An example parse tree for **You are gonna need a bigger boat.**

### Right Heavy PCFG

Despite the strong expressive capabilities of PCFGs, the parameter induction process becomes challenging due to the vast latent variable space encompassing the possible parse tree structures of a single sentence. Moreover, training PCFGs incurs significant time complexity . To address these challenges, we propose a novel variant of PCFG called Right-Heavy PCFG (RH-PCFG). RH-PCFG strikes a balance between expressive capabilities and computational complexity, mitigating the difficulties associated with parameter induction and reducing training time.

#### 3.1.1 Derived Parse Tree of RH-PCFG

RH-PCFG enforces a distinct right-heavy binary tree structure in the generated parse tree. Specifically, for each non-terminal symbol in the tree, the height of the right subtree increases proportionally with the length of the derived sub-string of the non-terminal symbol, while the height of the left subtree remains constant. This characteristic significantly reduces the number of possible parse trees for a given sentence, thereby reducing training complexity.

In the context of a right-heavy binary tree, the traversal process begins with the root node and proceeds recursively by selecting the right child of the current node as the subsequent node to visit. This sequential path, known as the **Main Chain**, is guided by a design principle inspired by linguistic observations in natural language. Specifically, it reflects the tendency for sentence structures to display left-to-right connectivity, with local complements predominantly positioned to the left of the central core structure. Within this framework, the left subtree of each node along the main chain is referred to as its **Local Prefix Tree**. The parse tree in Figure 2 represents a right heavy tree. In this tree, \(V_{1},V_{5},V_{9},V_{13}\) form the main chain, and their left subtrees correspond to their respective local prefix trees.

#### 3.1.2 Support Tree of RH-PCFG

To make sure that all the parse trees are the right heavy tree, we first build up a **Support Tree** as the backbone for the parse trees derived from RH-PCFG. The construction of the support tree follows two steps: At first, given a source sentence with a length of \(L_{x}\) and an upsample ratio of \(\), we sequentially connect \( L_{x}+1\) nodes with right links, which become the candidates for the main chain nodes in parse trees. Next, excluding the root node, we append a complete binary tree of depth \(l\) as the left subtree to each node. For the root node, only one left child is added. The added nodes represent local prefix tree candidates to the attached main chain candidates. Consequently, for a source sentence of length \(L_{x}\), the number of non-terminal symbols \(m\) of the RH-PCFG is

\[m= L_{x} 2^{l}+2.\] (5)

The nodes within the support tree are assigned sequential indexes through an in-order traversal, and these indexes correspond to the corresponding non-terminal symbols sharing the same index. Detailed construction algorithm can be found in Appendix A.1. Figure 3 shows an example of PCFG parse tree with \(L_{x}=3,=1,l=2\).

In RH-PCFG, the set of non-terminals is \(=\{V_{0},...,V_{m-1}\}\) and the set of terminals \(\) is equivalent to the vocabulary. The collection of production rules is represented by \(R\), and the starting symbol is \(V_{1}\). The production rules in RH-PCFG have the following form:

\[V_{0},\] (6) \[V_{i} a,a,i 0,(i)\] (7) \[V_{i} V_{j}aV_{k},V_{i},V_{j},V_{k},a ,((j,i) j=0)(k,i)\] (8)

Figure 3: An example support tree of Right Heavy PCFG\(\) stands for an empty string. \((i)\) indicates whether the non-terminal symbol \(V_{i}\) corresponds to a leaf node in the support tree. \((j,i)\) denotes that, in the support tree, the node corresponding to \(V_{j}\) is in the left subtree of the node corresponding to \(V_{i}\). \((k,i)\) signifies that the node corresponding to \(V_{k}\) is in the right subtree of the node corresponding to \(V_{i}\) in the support tree, and if \(V_{i}\) corresponds to a main chain node, \(V_{k}\) has to correspond to a main chain node at the same time.

### Architecture

The main architecture of PCFG-NAT is identical to that of the Transformer , with the additional incorporation of structures to model the probabilities of the PCFG. The input sentence to be translated is fed into the encoder of the model. Additionally, based on the length \(L_{x}\) of the source sentence, we compute the number of corresponding non-terminal symbols \(m\) in the RH-PCFG. Then \(m\) positional embeddings are used as inputs to the decoder. Through multiple layers of self-attention modules with a global attention view, cross-attention modules that focus on distant information, and subsequent feed-forward transformations, we obtain \(m\) hidden states \(h_{0},...,h_{m-1}\). These hidden states are associated with non-terminal symbols of the same index and are utilized to calculate the rule probabilities related to each corresponding non-terminal symbol.

The probability of the rule \(P(V_{i} a)\) can be regarded as the word prediction probability at each position, and we can directly calculate it with the softmax operation. We use \(h_{i}\) to denote the hidden state representation of the non-terminal symbol \(V_{i}\) and use \(W_{o}\) to denote the weight matrix of the output layer. Formally, we have:

\[P(V_{i} a)=(W_{o}h_{i}).\] (9)

For the rule \(P(V_{i} V_{j}aV_{k})\), we can independently consider the prediction probability of word \(a\) and the transition probability between the three non-terminal symbols. Formally, we decompose it into two parts:

\[P(V_{i} V_{j}aV_{k})=P(<\!V_{j},V_{k}\!>|V_{i}) P(a|V_{i}).\] (10)

The prediction probability \(P(a|V_{i})\) can be calculated with the softmax operation like in Equation 9. To normalize the distribution of \(P(<\!V_{j},V_{k}\!>|V_{i})\), we have to enumerate all valid combinations of \(j,k\). Let \((i)=\{<o,u>|V_{i} V_{o}aV_{u}\}\) denotes the set of children combinations of the non-terminal symbol \(V_{i}\), we can formally define the transition probability as:

\[q_{i}=W_{q}h_{i}, q_{j}=W_{l}h_{j}, q_{k}=W_{r}h_{k},\] (11) \[P(<\!V_{j},V_{k}\!>|V_{i})=^{T}q_{j}+q_{i}^{T}q _{k}+q_{j}^{T}q_{k})}{_{<o,u>(i)}(q_{i}^{T}q_{o}+q_{ i}^{T}q_{u}+q_{o}^{T}q_{u})},\] (12)

where \(W_{q},W_{l},W_{r}\) are learnable model parameters.

Figure 4: The architecture of PCFG-NAT model. A PCFG is inserted between the decoder layers and the output layer, which captures the dependency between different tokens. A parse tree is derived to generate the sentence _You are gonna need a bigger boat_. and is highlighted with bold black lines.

### Training

#### 3.3.1 CYK Algorithm for RH-PCFG

We train PCFG-NAT with the maximum likelihood estimation, which sums up all the possible parse trees for the given sentence \(Y\) conditioned on source sentence \(X\) and model parameters \(\):

\[()=- P(Y|X,)=- P(V_{1} y_{0}y_{1}...y _{n-1}).\] (13)

In PCFG, the CYK algorithm  is employed to compute the probability of a given sentence derived from the start symbol. The CYK algorithm follows a bottom-up dynamic programming approach to fill in the table \(S\). Each element \(S^{a}_{i,j}\) in the table represents the sum of probabilities of that \(V_{a}\) derives the consecutive sub-string \(y_{i}...y_{j}\) of the target sentence, denoted as \(P(V_{a} y_{i}...y_{j})\). Therefore the size of table \(S\) is \((mn^{2})\). Based on the definition of \(S^{a}_{i,j}\), we have recursion formula

\[S^{a}_{i,j}=_{k=i+1}^{j-1}_{<b,c>(a)}P(V_{a} V_{b }y_{k}V_{c})S^{b}_{i,k-1}S^{c}_{k+1,j}\] (14)

For every non-terminal symbol \(V_{a}\), the complexity filling all the \(S^{a}_{i,j}\) is the product of the choice of span \(<i,j>\), splitting point \(k\), and children symbols \(<b,c>\).

**Local Prefix Tree** For the non-terminal symbol \(V_{a}\) in a local prefix tree, the rules specify that all values of \(b\) and \(c\) are strictly fewer than the size of the left attached tree in the support structure, denoted as \(2^{l}-1<d=2^{l}\). Similarly, the variables \(i\), \(j\), and \(k\) are also strictly limited to values fewer than \(d\), since it is the max length of sub-strings that \(V_{a}\) can derives. Please note that \(l\) represents the max depth of the complete binary tree added as the left subtree to each node. Therefore the complexity filling all the \(S^{a}_{i,j}\) is \((d^{5})\) for \(V_{a}\) and all \(V_{a}\) in Local Prefix Tree should be \((d^{5}m)\) where \(m\) is the number of non-terminal symbols.

**Main Chain** For the non-terminal symbol \(V_{a}\) that serves as a main chain, based on the rules of the model, the variable \(b\) has strictly fewer than \(d\) choices, while \(c\) has a maximum of \(m/d\) choices. In a right-heavy parse tree, the symbols in the main chain are responsible for deriving a complete suffix of the target sentence as they are the right offspring of the root. Consequently, \(j\) is always equal to \(n-1\), and variable \(i\) has \(n\) choices. However, \(k\) has only \(d\) choices since the the length of derived sub-string of \(V_{b}\) is less than \(d\). Considering the filling of all \(S^{a}_{i,j}\) entries for the main chain symbols, the time complexity is \((mdn)\). Summing up the complexities for all \(m/d\) main chain symbols, the total complexity is \((m^{2}n)\).

Based on the above analysis, the overall time complexity of the CYK algorithm for RH-PCFG is \((d^{5}n+m^{2}n)\).Such complexity is within the acceptable range, while the training complexity of general form PCFG, such as Chomsky normal form , is \((n^{3}||)\). Detailed pseudocode for the training algorithm can be found in the Appendix A.2.

#### 3.3.2 Glancing Training

Glancing training, as introduced in the work of Qian et al. , has been successfully applied in various NAT architectures [12; 14]. In the case of PCFG-NAT, we also incorporate glancing training. This training approach involves replacing a portion of the decoder inputs with the embeddings of the target sentence \(Y\). To determine the ratio of decoder inputs to be replaced with target embeddings, we employ the masking strategy proposed by Qian et al. . There is a mismatch between the decoder length \(m\) and target length \(n\), so we have to derive an alignment between them to enable the glancing. To incorporate Glancing Training into PCFG-NAT, we adapt the approach by finding the parse tree \(T^{*}_{Y}=_{T_{Y}(Y)}P(Y,T_{Y}|X)\) with the highest probability that generates the target sentence \(Y\), where \((Y)\) stands for all parse trees of \(Y\). \(T^{*}_{Y}\) can be traced by repalcing the **sum** opearation with **max** in the CYK algorithm. We provide the pseudocode in the Appendix A.3.

### Inference

We propose a Viterbi  decoding framework for PCFG-NAT to find the optimal parse tree \(T^{*}=_{T}P(T|X,L)\) under the constraint of target length \(L\). For each non-terminal symbol \(V_{a}\), wemaintain a record \(M_{L}^{a}\) that represents the highest probability among sub-strings of length \(L\) that can be derived by \(V_{a}\). Similar to the CYK algorithm, we can compute \(M_{L}^{a}\) in a bottom-up manner using dynamic programming, considering all non-terminal symbols \(V_{a}\) and lengths \(L\).

\[M_{L}^{a}=_{<b,c>(a),o,k=1..L-1}P(V_{a} V _{b}oV_{c})M_{L-1-k}^{b}M_{k}^{c}\] (15)

To recover the parse tree with the maximum probability given \(L\), we need to keep track of the values of \(b\), \(c\), and \(k\) that lead to \(M_{L}^{a}\) and start the construction from \(M_{L}^{1}\). The detailed pseudocode is provided in the Appendix A.4. According to Shao et al. , we rerank the output sentences with probability and length factors and then output the sentence with the highest score.

## 4 Experiments

### Setup

DatasetWe conduct our experiments on WMT14 English\(\)German (En-De, 4.5M sentence pairs),WMT17 Chinese\(\)English (Zh-En, 20M sentence pairs) and WMT16 English\(\)Romanian (En-Ro, 610k sentence pairs). We use the same preprocessed data and train/dev/test splits as Kasai et al. . For all the datasets, we learn a joint BPE model  with 32K merge operations to tokenize the data and share the vocabulary for source and target languages. The translation quality is evaluated with sacreBLEU  for WMT17 En-Zh and tokenized BLEU  for other benchmarks.

Implementation DetailsFor our method, we choose the \(l=1,=4\) settings for Support Tree of RH-PCFG and linearly anneal \(\) from 0.5 to 0.1 for the glancing training. For DA-Transformer , we use \(=8\) for the graph size, which has comparable hidden states to our models. All models are optimized with Adam  with \(=(0.9,0.98)\) and \(=10^{-8}\). For all models, each batch contains approximately 64K source words. All models are trained for 300K steps. For WMT14 En-De and WMT17 Zh-En, we measure validation BLEU for every epoch and average the 5 best checkpoints as the final model. For WMT16 En-Ro, we just use the best checkpoint on the valid dataset. We use the NVIDIA Tesla V100S-PCIE-32GB GPU to measure the translation latency on the WMT14 En-De test set with a batch size of 1. We implement our models based on the open-source framework of fairseq.

### Main Results

Translation QualityAs demonstrated in Table 2, PCFG-NAT outperforms CTC-GLAT  and DA-Transformer  in terms of translation quality, effectively reducing the performance gap between NAT and AT models. This indicates its ability to capture the intricate translation distribution inherent in the raw training data and efficiently learn from dependencies with reduced multi-modality. FA-DAT  builds upon

   &  &  &  &  \\   & & **En-De** & **De-En** & **Zh-En** & **Zh-En** & **En-Ro** & **Re-En** & **Average Gap** & **Speedup** \\  Transformer  & N & 27.66 & 31.59 & 34.89 & 23.89 & 34.26 & 33.87 & 0 & \(1.0\) \\  CMLM  & 10 & 24.61 & 29.40 & - & - & 32.86 & 32.87 & - & \(2.2\) \\ SMART  & 10 & 25.10 & 29.58 & - & - & 32.71 & 32.86 & - & \(2.2\) \\ Disc(17) & \( 4\) & 25.64 & - & - & - & 32.25 & - & \(3.5\) \\ Imputer  & 8 & 25.0 & - & - & - & - & - & \(2.7\) \\ CMLMC  & 10 & 26.40 & 30.92 & - & - & 34.14 & 34.13 & - & \(1.7\) \\  Vanilla NAT  & 1 & 11.79 & 16.27 & 18.92 & 8.69 & 19.93 & 24.71 & 14.31 & \(15.3\) \\ CTC  & 1 & 17.68 & 19.80 & 26.84 & 12.23 & - & - & - & \(14.6\) \\ FlowSeq & 1 & 18.55 & 23.36 & - & - & 29.26 & 30.16 & - & \(1.1\) \\ AXE  & 1 & 20.40 & 24.90 & - & - & 30.47 & 31.42 & - & \(14.2\) \\ OAXE  & 1 & 22.42 & 26.8 & - & - & - & - & \(14.2\) \\ CTC + GLAT  & 1 & 25.02 & 29.14 & 30.65 & 19.2 & - & - & - & \(14.6\) \\ DA-Transformer + Lookahead  & 1 & 26.55 & 30.81 & 33.54 & 22.68 & 32.31* & 32.73* & 1.27 & \(14.0\) \\ DA-Transformer + Joint-Viterbi  & 1 & 26.89 & 31.10 & 33.65 & 23.24 & 32.46* & 32.84* & \(1.00\) & \(13.2\) \\ FA-DAT  & 1 & **27.47** & **31.44** & **34.49** & **24.22** & - & - & \(0.41\) & \(13.2\) \\  PCFG-NAT & 1 & 27.02 & 31.29 & 33.60 & 23.40 & **32.72** & **33.07** & \(0.84\) & \(12.6\) \\  

Table 2: Results on raw WMT14 En-De, WMT17 Zh-En and WMT16 En-Ro. ‘Iter’ means the number of decoding iterations, and \(N\) is the length of the target sentence. The speedup is evaluated on the WMT14 En-De test set with a batch size of 1. ‘\(*\)’ indicates the results of our re-implementation.

DA-Transformer and trains the model to maximize a fuzzy alignment score between the graph and reference, taking captured translations in all modalities into account. The same training objective can also be implemented in PCFG-NAT. Experimental results show that FA-DAT  achieves state-of-the-art NAT performance. In the future, we plan to incorporate the ideas from FA-DAT  into PCFG-NAT, enabling a fairer comparison and demonstrating the superiority of PCFG modeling.

**Inference Latency**

As shown Table 2, PCFG-NAT demonstrates a decoding speedup that is slightly lower than Vanilla NAT but significantly higher than both AT models and iterative NAT models. This observation indicates that PCFG-NAT effectively strikes a favorable balance between translation quality and decoding latency, achieving a desirable trade-off between the two factors.

### Ablation study

#### 4.3.1 Structure of Support Tree

In this section, we examine the impact of the max depth \(l\) of the local prefix tree and the upsample ratio \(\) in the support tree of RH-PCFG. Figure 5 presents the results. When the total number of non-terminal symbols is kept constant, incorporating a local prefix tree with max two layers leads to a significant decline in performance. We posit that an excessively large value of \(l\) leads to a significant increase in the number of potential parse trees, thereby making it more challenging for the model to learn complex structures from the data. As a result, the translation performance of the model may decline.

### Decoding Strategy

To determine the optimal parse tree for a given sentence length, we employ Viterbi decoding. However, this approach incurs a certain computational cost in terms of decoding speed. To demonstrate the necessity of Viterbi decoding, we compare it with a greedy decoding strategy that always selects the rule with the highest probability. The experimental results, presented in Table 3, indicate that while Viterbi decoding is only slightly slower than greedy decoding, it yields significantly better performance. Based on these findings, we assert that Viterbi decoding is necessary and do not recommend the use of greedy decoding.

### Ablation Study for Distilled Data and GLAT

We conducted ablation studies to examine the impact of Glancing Training and Knowledge Distillation on our model's performance.GLAT consistently improves the translation performance of all models, while the gains from KD are more limited.

Figure 5: Ablation study on the max depth \(l\) of local prefix tree and upsample ramp ration \(\). Notice the number of the non-terminal symbols \(m\) equals to \( L_{x} 2^{l}+2\), where \(L_{x}\) is the length of the source sentence.

   &  &  &  \\   & **En-De** & **De-En** & **En-Zh** & **Zh-En** & **En-Ro** & **Ro-En** & **Speedup** \\ 
**Greedy Decoding** & 26.01 & 31.38 & 32.82 & 22.50 & 32.05 & 32.65 & 1.00\(\) \\
**Viterbi Decoding** & 27.02 & 32.29 & 33.60 & 23.40 & 32.72 & 33.07 & 0.91\(\) \\  

Table 3: Ablation Study on the effect of decoding strategies. We use the NVIDIA Tesla V100S-PCIE-32GB GPU to measure the speedup on the WMT14 En-De test set with a batch size of 1.

### Syntax Analysis for Generated Parse Tree

To analyze the syntax information captured by PCFG-NAT, we conducted an analytical experiment on the WMT14 De-En test set. We utilized the averaged perceptron tagger, a part-of-speech tagging tool available in the nltk library , to assign part-of-speech tags to the translations. Subsequently, we calculated the frequency of part-of-speech tags for tokens directly generated from non-terminal symbols in both the main chain and the local prefix tree. Additionally, we counted the frequency of unfinished subwords ending with the symbol @0. To ensure comparability, the frequency scores were normalized by the number of tokens in the main chain and local prefix tree, respectively. The results are presented in Figure 6.

As depicted in Figure 6, the majority of tokens generated by non-terminal symbols in the local prefix tree correspond to the part-of-speech tags **RB** (adverb), **JJ** (adjective), **NNP** (proper noun, singular), and **comma**. Furthermore, the frequency of unfinished subwords in the local prefix tree symbols is notably higher compared to that in the main chain. It is well-known that **RB**, **JJ**, **NNP**, **comma**, and unfinished subwords are closely related to the local adjacent tokens. Hence, PCFG-NAT effectively captures the connection between these elements with the local prefix tree serving as complementary information to the main chain.

### Probability Analysis of the Maximum Likelihood Path

We calculated the proportion of the maximum probability parse tree (path) that generates the references among all parse trees (paths) to investigate the benefits of the syntax tree learned by the RH-PCFG in comparison to the path in the Directed Acyclic Graph (DAG). We observed that the proportion of the highest probability parse tree for PCFG-NAT is generally higher than the proportion

    &  \\   & Baseline & 34.26 \\  & +KD & 34.72 \\   & Baseline & 31.98 \\  & +GLAT & 32.46 \\  & +GLAT \& KD & 32.40 \\   & Baseline & 31.94 \\  & +GLAT & 32.72 \\   & +GLAT \& KD & 32.75 \\   

Table 4: Comparison of WMT16 En-Ro translation performance (BLEU scores) for Transformer, DA Transformer, and PCFG-NAT models with and without Glancing (GLAT)  and Knowledge Distillation (KD) techniques. Baseline means the models are trained on raw train sets without GLAT .

Figure 6: The frequency of part-of-speech and subwords generated from the main chain and local prefix tree respectively.

of the highest probability path for DA-Transformer . This demonstrates that in the trained RH-PCFG, the probabilities of parse trees are more concentrated. Under the same training set and learnable parameters, this indicates that the modeling approach of RH-PCFG is more in line with the intrinsic structure of sentences compared to the flattened assumption of Directed Acyclic Graph.

## 5 Related Works

Gu et al.  accelerates neural machine translation with a non-autoregressive Transformer, which comes at the cost of translation quality. The major reason for the performance degradation is the multi-modality problem that there may exist multiple translations for the same source sentence. Many efforts have been devoted to enhancing the representation power of NAT. One thread of research introduces latent variables to directly model the uncertainty in the translation process, with techniques like vector quantization [16; 33; 3; 4], generative flow , and variational inference [38; 12]. Another thread of research explores some statistical information for intermediate prediction, including word fertility , word alignment [31; 39], and syntactic structures [2; 23]. Zhang et al.  explores the syntactic multi-modality problem in non-autoregressive machine translation The most relevant works to PCFG-NAT are CTC-based NAT [11; 22] and DA-Transformer [14; 37; 26], which enriches the representation power of NAT with a longer decoder. They can simultaneously consider multiple translation modalities by ignoring some tokens  or modeling transitions . Fang et al.  proposes a non-autoregressive direct speech-to-speech translation model, which achieves both high-quality translations and fast decoding speeds by decomposing the generation process into two steps. Compared to previous NAT approaches, PCFG-NAT mitigates the issue of multi-modality in NAT by capturing structured non-adjacent semantic dependencies in the target language, leading to improved translation quality. Our work is also related to the line of work on incorporating structured semantic information into machine translation models[42; 19; 32; 1]. In contrast to these works, PCFG-NAT learns structured information unsupervised from data and can generate target tokens in parallel.

## 6 Conclusion

Non-autoregressive Transformer has higher inference speed but weaker expression power due to the lack of dependency modeling. In this paper, we propose PCFG-NAT to model target-side dependency by capturing the hierarchical structure of the sentence. Experimental results show that PCFG-NAT further enhances the translation performance of NAT models while providing a more interpretable approach for generating translations.

## 7 Limitations

Further research and exploration are needed to overcome the limitations encountered when attempting to induce complex PCFGs from data. Developing more advanced learning algorithms or exploring alternative approaches for modeling and capturing the rich syntactic structures of language may be necessary to unlock the full potential of PCFG-NAT and further enhance its translation capabilities.

    &  &  &  &  \\    & **En-De** & **De-En** & **En-Zh** & **Zh-En** & **En-Ro** & **Ro-En** & \\ 
**DA-Transformer** & 0.6026 & 0.7565 & 0.4899 & 0.5611 & 0.7948 & 0.7124 & 0.6529 \\
**PCFG-NAT** & 0.6421 & 0.7718 & 0.4681 & 0.5886 & 0.7764 & 0.7160 & 0.6605 \\   

Table 5: In the Valid Set, for the DA-Transformer model, we calculated \(P(Y|X,A^{*})/P(Y|X)\), where \(X,Y\) represents the source text and the target text in the dataset, \(A^{*}=argmax_{A}P(A|X,Y)\), and \(A\) represents a hidden state path. For the PCFG-NAT model, we calculated \(P(Y|X,T^{*})/P(Y|X)\), where \(T^{*}=argmax_{T}P(T|X,Y)\), and T represents a parse tree.

Acknowledgement

This work is partially supported by the National Key R&D Program of China(under Grant 2021ZD0110102), the NSF of China(under Grants 61925208), CAS Project for Young Scientists in Basic Research(YSBR-029) and Xplore Prize. We thank all the anonymous reviewers for their insightful and valuable comments.