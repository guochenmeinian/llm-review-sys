# GAUCHE: A Library for

Gaussian Processes in Chemistry

 Ryan-Rhys Griffiths\({}^{1*}\)  Leo Klarner\({}^{2*}\)  Henry Moss\({}^{3*}\)  Aditya Ravuri\({}^{3*}\)  Sang Truong\({}^{4*}\)  Samuel Stanton\({}^{5*}\)  Gary Tom\({}^{6,7*}\)  Bojana Rankovic\({}^{8,9*}\)  Yuanqi Du\({}^{10*}\)  Arian Jamasb\({}^{3*}\)  Aryan Deshwal\({}^{11}\)  Julius Schwartz\({}^{3}\)  Austin Tripp\({}^{3}\)  Gregory Kell\({}^{12}\)  Simon Frieder\({}^{2}\)  Anthony Bourached\({}^{13}\)  Alex J. Chan\({}^{3}\)  Jacob Moss\({}^{3}\)  Chengzhi Guo\({}^{3}\)  Johannes Durholt\({}^{14}\)  Saudamini Chaurasia\({}^{15}\)  Ji Won Park\({}^{5}\)  Felix Strieth-Kalthoff\({}^{5}\)  Alpha A. Lee\({}^{3}\)  Bingqing Cheng\({}^{16}\)  Alan Aspuru-Guzik\({}^{6,7,17}\)  Philippe Schwaller\({}^{8,9}\)  Jian Tang\({}^{18,19,17}\)

\({}^{1}\)Meta \({}^{2}\)University of Oxford \({}^{3}\)University of Cambridge \({}^{4}\)Stanford University \({}^{5}\)Genentech \({}^{6}\)University of Toronto \({}^{7}\)Vector Institute \({}^{8}\)EPFL \({}^{9}\)NCCR Catalysis \({}^{10}\)Cornell University \({}^{11}\)Washington State University \({}^{12}\)King's College London \({}^{13}\)University College London \({}^{14}\)Evonik Industries AG \({}^{15}\)Syracuse University \({}^{16}\)IST Austria \({}^{17}\)CIFAR AI Research Chair \({}^{18}\)MILA Quebec AI Institute \({}^{19}\)HEC Montreal \({}^{*}\) Equal contributions

###### Abstract

We introduce GAUCHE, an open-source library for GAUssian processes in CHEmistry. Gaussian processes have long been a cornerstone of probabilistic machine learning, affording particular advantages for uncertainty quantification and Bayesian optimisation. Extending Gaussian processes to molecular representations, however, necessitates kernels defined over structured inputs such as graphs, strings and bit vectors. By providing such kernels in a modular, robust and easy-to-use framework, we seek to enable expert chemists and materials scientists to make use of state-of-the-art black-box optimization techniques. Motivated by scenarios frequently encountered in practice, we showcase applications for GAUCHE in molecular discovery, chemical reaction optimisation and protein design.

The codebase is made available at https://github.com/leojklarner/gauche.

## 1 Introduction

Early-stage scientific discovery is often characterised by the limited availability of high-quality experimental data , meaning that there is much knowledge to gain from targeted experiments. As such, machine learning methods that facilitate discovery in low data regimes, such as Bayesian optimisation (BO)  and active learning (AL) , have great potential to expedite the rate at which useful molecules, materials, chemical reactions and proteins can be discovered.

At present, Bayesian neural networks (BNNs) and deep ensembles are typically the method of choice to generate uncertainty estimates for molecular BO and AL loops . For small datasets, however, Gaussian processes (GPs) may often be a preferable and more appropriate choice . Furthermore, GPs possess particularly advantageous properties for BO; first, they admit exact as opposed to approximate Bayesian inference and second, few of their parameters need to be determined by hand. In the words of Sir David MacKay ,

"Gaussian processes are useful tools for automated tasks where fine tuning for each problem is not possible. We do not appear to sacrifice any performance for this simplicity."The iterative model refitting required in BO makes it a prime example of such an automated task. However, canonical GPs typically assume continuous input spaces of low and fixed dimensionality, hindering their application to standard molecular representations such as SMILES/SELFIES strings [18; 19; 20], topological fingerprints [21; 22; 23] and discrete graphs [24; 25].

With GAUCHE, we provide a modular, robust and easy-to-use framework to rapidly prototype GPs with 30+ GPU-accelerated string, fingerprint and graph kernels that operate on a range of molecular representations (see Figure 1). Furthermore, GAUCHE interfaces with the GPYTorch  and BoTorch  libraries and contains an extensive set of tutorial notebooks to make state-of-the-art probabilistic modelling and black-box optimization techniques more easily accessible to scientific experts in chemistry, materials science and beyond.

## 2 Background

We briefly recall the fundamentals of Gaussian processes and Bayesian optimisation in Sections 2.1 and 2.2, respectively, and refer the reader to  and [29; 30; 31] for a more comprehensive treatment.

### Gaussian Processes

Notation\(^{n d}\) is a design matrix of \(n\) training examples of dimension \(d\). A given row \(i\) of the design matrix contains a training molecule's representation \(_{i}\). A GP is specified by a mean function, \(m()=[f()]\) and a covariance function \(k(,^{})=[(f()-m())(f( ^{})-m(^{}))]\). \(K_{}(,)\) is a kernel matrix, where entries are computed by the kernel function as \([K]_{ij}=k(_{i},_{j})\) and \(\) represents the set of kernel hyperparameters. The GP specifies the full distribution over the function \(f\) to be modelled as

\[f()m(),k(,^{ }).\]

TrainingHyperparameters for GPs comprise kernel hyperparameters, \(\), in addition to the likelihood noise, \(_{y}^{2}\). These hyperparameters are chosen by optimising an objective function known as the negative log marginal likelihood (NLML)

\[ p(|,)=^{ }(K_{}(,)+_{y}^{2}I)^{-1}}_{}-|K_{}(,)+_{y}^{2}I|}_{ }-(2),\]

Figure 1: An overview of the applications and representations available in GAUCHE.

where \(_{y}^{2}I\) represents the variance of i.i.d. Gaussian noise on the observations \(\). The NLML embodies Occam's razor for Bayesian model selection  in favouring models that fit the data without being overly complex.

PredictionAt test locations \(_{*}\), assuming a zero mean function obtained following the standardization of the outputs \(\), the GP returns a predictive mean, \(}_{*}=K(_{*},)[K(,)+ _{y}^{2}I]^{-1}\), and a predictive uncertainty \((_{*})=K(_{*},_{*})-K(_{*},)[K(,)+_{y}^{2}I]^{-1}K(,_{*})\).

### Bayesian Optimisation

In molecular discovery campaigns, we are typically interested in solving problems of the form

\[^{}=_{}f(),\]

where \(f():\) is an expensive black-box function over a structured input domain \(\). In our setting the structured input domain consists of a set of molecular representations (graphs, strings, bit vectors) and the expensive black-box function is an experimentally determined property of interest that we wish to optimise. Bayesian optimisation (BO) [32; 33; 34; 35; 29; 36] is a data-efficient methodology for determining \(^{}\). BO operates sequentially by selecting input locations at which to query the black-box function \(f\) with the aim of identifying the optimum in as few queries as possible. Evaluations are focused on promising areas of the input space as well as areas with high uncertainty--a balancing act known as the exploration/exploitation trade-off.

The two components of a BO scheme are a probabilistic surrogate model and an acquisition function. The surrogate model is typically chosen to be a GP due to its ability to maintain calibrated uncertainty estimates through exact Bayesian inference. The uncertainty estimates of the surrogate model are then leveraged by the acquisition function to propose new input locations to query. The acquisition function is a heuristic that trades off exploration and exploitation, well-known examples of which include expected improvement (EI) [33; 35] and entropy search [37; 38; 39; 40]. After the acquisition function proposes an input location, the black-box is evaluated at that location, the surrogate model is retrained and the process is repeated until a solution is obtained.

## 3 Molecular Representations

We review commonly used representations for molecules (Section 3.1), chemical reactions (Section 3.2) and proteins (Section 3.3), before describing the kernels that operate on them in Section 4. An overview of the representations considered by GAUCHE is provided in Figure 1.

### Molecules

GraphsA molecule can be represented as an undirected, labelled graph \(=(,)\) where vertices \(=\{v_{1},,v_{N}\}\) represent the atoms of an \(N\)-atom molecule and edges \(\) represent covalent bonds between these atoms. Additional information may be incorporated in the form of vertex and edge labels \(:_{V}_{E}\) by specifying e.g. atom types or bond orders.

FingerprintsMolecular fingerprints enumerate sets or bags of subgraphs \(^{}=(^{},^{ })\) of a certain type and then hash them into machine-readable bit or count vectors. Extended connectivity fingerprints (ECFPs) , for example, enumerate all circular subgraphs up to a pre-specified radius parameter by assigning initial numeric identifiers to each atom in a molecule and iteratively updating them based on the identifiers of their neighbours. Each level of iteration appends substructural features of increasing non-locality to an array, which is then hashed to a bit vector reflecting the presence or absence of those substructures in the molecule (see Figure 2). We choose a radius of 3 for all experiments in the main text and provide a more detailed ablation of the radius parameter in Appendix E.3.

Figure 2: Visualisation of the ECFP subgraph enumeration and hashing procedures.

Additionally, we make use of fragment descriptors, which are count vectors in which each component indicates the count of a particular functional group present in a molecule, as well as the concatenation of the fingerprint and fragment feature vectors, a representation termed fragprints , which has shown strong empirical performance. Example representations of \(}\) for fingerprints, \(}\) for fragments and \(}\) for fragrings are given as

\[}=1&0&&1^{}\;} =3&0&&2^{}\;}= 1&0&&1&3&0&&2^{}\]

StringsThe Simplified Molecular-Input Line-Entry System (SMILES) is a text-based representation of molecules [18; 19], examples of which are given in Figure 3. Self-Referencing Embedded Strings (SELFIES)  is an alternative string representation to SMILES such that a bijective mapping exists between a SELFIES string and a molecule.

### Reaction Representations

Chemical reactions consist of (multiple) reactants and reagents that react to form one or more products. The choice of reactant/reagent typically constitutes a categorical design space. Taking as an example the high-throughput experiments by  on Buchwald-Hartwig reactions, the reaction design space consists of 15 aryl and heteroaryl halides, 4 Buchwald ligands, 3 bases, and 23 isoxazole additives.

Concatenated Molecular RepresentationsIf the number of reactants and reagents is constant, the molecular representations discussed in Section 3.1 may be used to represent them, and the vectors for the individual reaction components can be concatenated to construct a representation of the reaction [41; 42]. An additional and commonly used concatenated representation is the one-hot-encoding (OHE) of the reaction categories where bits specify which of the components in the different reactant and reagent categories is present. In the Buchwald-Hartwig example, the OHE would describe which of the aryl halides, Buchwald ligands, bases and additives are used in the reaction, resulting in a 44-dimensional bit vector .

Differential Reaction FingerprintsInspired by the hand-engineered difference reaction fingerprints by  and  recently introduced the differential reaction fingerprint (DRFP). This reaction fingerprint is constructed by taking the symmetric difference of the sets containing the molecular substructures on both sides of the reaction arrow. Reagents are added to the reactants. The size of the reaction bit vector generated by DRFP is independent of the number of reaction components.

Data-Driven Reaction Fingerprints described data-driven reaction fingerprints using Transformer models such as BERT  trained in a supervised or an unsupervised fashion on reaction SMILES. Those models can be fine-tuned on the task of interest to learn more specific reaction representations  (RXNFP). Similar to the DRFP, the size of the data-driven reaction fingerprints is independent of the number of reaction components.

### Protein Representations

Proteins are large macromolecules that adopt complex 3D structures. They can be represented in string form describing the underlying amino acid sequence. Graphs at varying degrees of coarseness may also be used for structural representations that capture spatial and intramolecular relationships between structural elements, such as atoms, residues, secondary structures and chains. GAUCHE interfaces with Graphein , a library for pre-processing and computing graph representations of structural biological data, thereby enabling the application of graph kernel-based methods to protein structure. We provide experiments on protein fitness prediction in Appendix E.1.

Figure 3: SMILES strings for structurally similar molecules. Similarity is encoded in the string through common contiguous subsequences (black). Local differences are highlighted in red.

## 4 Molecular Kernels

The choice of kernel is an important inductive bias for the properties of the function being modelled. A common choice for continuous input domains is the radial basis function kernel

\[k_{}(,^{})=_{f}^{2}(-^{}||_{2}^{2}}{2^{2}}),\]

where \(_{f}^{2}\) is the signal amplitude hyperparameter (vertical lengthscale) and \(\) is the (horizontal) lengthscale hyperparameter. However, in order to train GPS on the molecular representations covered in Section 3, bespoke kernel functions that are able to operate non-continuous input spaces are needed.

### Fingerprint Kernels

Scalar Product KernelThe simplest kernel to operate on fingerprints is the scalar product or linear kernel defined for vectors \(,^{}^{d}\) as

\[k_{}(,^{})_{f}^{2 },^{},\]

where \(_{f}\) is a scalar signal variance hyperparameter and \(,\) is the Euclidean inner product.

Tanimoto KernelIntroduced as a general similarity metric for binary attributes , the Tanimoto kernel was first used in chemoinformatics in conjunction with non-GP-based kernel methods . It is defined for binary vectors \(,^{}\{0,1\}^{d}\) for \(d 1\) as

\[k_{}(,^{})_{f}^{2} ,^{}}{\| \|^{2}+\|^{}\|^{2}-,^{}},\]

where \(||||\) is the Euclidean norm.

In addition to the Tanimoto kernel, GAUCHE provides parallelisable and batch-GP-compatible implementations of 12 other bit and count vector kernels that are presented in Appendix G.

### String Kernels

String kernels [52; 53] measure the similarity between strings by examining the degree to which their sub-strings differ. In GAUCHE, we implement the SMILES string kernel  which calculates an inner product between the occurrences of sub-strings, considering all contiguous sub-strings made from at most \(n\) characters (we set \(n=5\) in our experiments). Therefore, for the sub-string count featurisation \(:^{p}\), also known as a bag-of-characters representation , the SMILES string kernel between two strings \(\) and \(^{}\) is given by

\[k_{}(,^{})_{f}^{2} (),(^{}).\]

More complicated string kernels do exist in the literature, for example, GAUCHE also provides an implementation of the subset string kernel  which allows non-contiguous matches. However, we found that the significant added computational cost of these methods did not provide improved performance over the more simple SMILES string kernel in the context of molecular data. Note that although named the SMILES string kernel, this kernel can also be applied to any other string representation of molecules e.g. SELFIES or protein sequences.

### Graph Kernels

Graph kernels define a mapping \(_{}:\) from a graph domain \(\) to a feature space \(\), in which the inner product between a pair of graphs \(g,g^{}\) serves as a similarity measure

\[k_{}(g,g^{})_{f}^{2}_{ }(g),_{}(g^{})_{},\]

where \(\) denotes kernel-specific hyperparameters. Depending on how \(_{}\) is defined, the kernel captures different substructural motifs and is characterised by different hyperparameters. The Weisfeiler-Lehman (WL) kernel , for instance, is given by the inner products of label count vectors over \(\) iterations of the Weisfeiler-Lehman algorithm .

To maximise the number of graph kernels available in GAUCHE we implemented the SIGP class, which enables PyTorch-based GPS to be trained on non-tensorial inputs with any kernel from the GraKel library  (see Appendix F for more details).

## 5 Experiments

We evaluate GAUCHE on a range of regression, uncertainty quantification (UQ) and Bayesian optimisation (BO) tasks. The principle goal in conducting regression and UQ benchmarks is to gauge whether performance on these tasks may be used as a proxy for BO performance. BO is a powerful tool for automated scientific discovery but one would prefer to avoid model misspecification in the surrogate when deploying a scheme in the real world. We make use of the following datasets with experimentally determined labels:

* **Photoswitch** The labels \(y\) are the values of the \(E\) isomer \(-^{*}\) transition wavelength for 392 photoswitch molecules .
* **ESOL** The labels \(y\) are the logarithmic aqueous solubility values for 1128 organic small molecules .
* **FreeSolv** The labels \(y\) are the hydration free energies for 642 molecules .
* **Lipophilicity** The labels \(y\) are the octanol/water distribution coefficient (log D at pH 7.4) of 4200 compounds curated from the ChEMBL database [61; 62].
* **Buchwald-Hartwig reactions** The labels \(y\) are the yields for 3955 Pd-catalysed Buchwald-Hartwig C-N cross-couplings .
* **Suzuki-Miyaura reactions** The labels \(y\) are the yields for 5760 Pd-catalysed Suzuki-Miyaura C-C cross-couplings .

### Regression and Uncertainty Quantification (UQ)

Experimental setupFor the regression and uncertainty quantification experiments, all datasets were randomly split into training and test sets with a ratio of 80/20. (Note that validation sets are not required for the GP models, since hyperparameters are chosen using the marginal likelihood objective on the train set). All GP models were trained using the L-BFGS-B optimiser  and, if not stated otherwise, the default settings in the GPTorch and BoTorch libraries apply.

To quantify model performance, the predictive accuracy and the calibration of the predictive uncertainty estimates of the fitted models were evaluated on the held-out test set and summarised as the root-mean-square error (RMSE) and the negative log predictive density (NLPD), respectively. The mean and standard error of these metrics over 20 different random splits are reported in Table 1.

    &  &  &  &  &  &  \\  & & & RMSE & NLPD & RMSE & NLPD & RMSE & NLPD & RMSE & NLPD \\    } &  & fragments & \(\) & \(\) & \(0.71 0.01\) & \(0.33 0.01\) & \(1.31 0.06\) & \(0.28 0.02\) & \(\) & \(\) \\  & & fingerprints & \(\) & \(0.33 0.03\) & \(1.01 0.01\) & \(0.71 0.01\) & \(1.93 0.09\) & \(0.58 0.03\) & \(0.76 0.01\) & \(0.85 0.01\) \\  & & fragments & \(26.3 0.8\) & \(0.50 0.04\) & \(0.91 0.01\) & \(0.57 0.01\) & \(1.49 0.05\) & \(0.44 0.03\) & \(0.50 0.01\) & \(0.94 0.02\) \\   &  & fragments & \(22.5 0.7\) & \(\) & \(0.88 0.01\) & \(0.53 0.01\) & \(1.27 0.02\) & \(0.25 0.02\) & \(0.77 0.01\) & \(0.92 0.01\) \\  & & fingerprints & \(24.8 0.8\) & \(0.33 0.03\) & \(1.17 0.01\) & \(0.84 0.01\) & \(1.93 0.07\) & \(0.64 0.03\) & \(0.84 0.01\) & \(1.03 0.01\) \\  & & fragments & \(36.6 1.0\) & \(0.80 0.03\) & \(1.15 0.01\) & \(0.82 0.01\) & \(1.63 0.03\) & \(0.54 0.02\) & \(0.97 0.01\) & \(0.88 0.10\) \\   &  & SMILES & \(24.8 0.7\) & \(0.30 0.04\) & \(\) & \(\) & \(1.31 0.01\) & \(\) & \(\) & \(\) \\  & & WL Kernel & graph & \(22.4 1.4\) & \(0.39 0.11\) & \(1.04 0.02\) & \(0.76 0.00\) & \(1.47 0.06\) & - & - \\    } &  & fragments & \(\) & \(1.63 0.44\) & \(0.88 0.01\) & \(1.70 0.11\) & \(1.39 0.03\) & \(1.41 0.38\) & \(0.75 0.01\) & \(3.82 0.12\) \\  & & fingerprints & \(22.4 0.7\) & \(2.22 0.56\) & \(1.08 0.02\) & \(2.59 0.40\) & \(1.93 0.07\) & \(2.65 0.72\) & \(0.81 0.01\) & \(3.74 0.10\) \\   & & fragments & \(25.8 0.7\) & \(0.69 0.09\) & \(1.03 0.01\) & \(1.93 0.28\) & \(1.48 0.02\) & \(0.89 0.12\) & \(0.87 0.01\) & \(5.52 0.32\) \\    &  & graph & \(28.5 1.2\) & \(1.00 0.13\) & \(0.88 0.01\) & \(1.70 0.11\) & \(\) & \(1.01 0.02\) & \(0.73 0.02\) & \(1.14 0.01\) \\   & & CNN Ensemble & SELFIES & \(26.4 1.0\) & \(4.34 0.55\) & \(\) & \(2.91 0.14\) & \(1.29 0.04\) & \(2.24 0.21\) & \(0.75 0.01\) & \(2.60 0.06\) \\   & & CNN DKL GP & SELFIES & \(25.1 0.8\) & \(0.48 0.05\) & \(0.94 0.04\) & \(0.90 0.15\) & \(1.41 0.11\) & \(0.33 0.04\) & \(0.91 0.01\) & \(1.46 0.03\) \\   

Table 1: Predictive accuracy and calibration of Gaussian process and probabilistic deep learning models across four different molecular property prediction benchmarks. RMSE (\(\)) and NLPD (\(\)) are reported as mean and standard error over 20 random 80/20 train/test splits. The best models up to statistical significance are highlighted in bold. Numerical issues were encountered with the WL kernel on the large lipophilicity dataset and the corresponding entries are left blank.

Alternative metrics to measure the quality of the predictive uncertainty estimates such as the mean standardised log loss (MSLL) and the quantile coverage error (QCE) are reported in Appendix A, while additional results for the reaction yield prediction datasets and scaffold splits are presented in Appendices B and C. We also benchmark the performance of GP models against a range of Bayesian neural network- (BNN) and deep ensemble-based methods that are detailed in Appendix D.

ResultsSummarising our results in Table 1, we find that Tanimoto-based GPs generally outperform Scalar Product ones in terms of RMSD and NLDP, while string kernel-based GPs often yield even better performance. We additionally note that the quality of the predictive uncertainty estimates roughly correlates with predictive accuracy in the case of GP-based models.

While deep probabilistic models attained competitive results in terms of RMSD, we found their uncertainty estimates to be consistently less reliable than those of GP-based models with discrete string kernels or shallow continuous kernels on hand-crafted features (e.g. fragprints), limiting their suitability for Bayesian optimisation and active learning. Our results suggest that for small to mid-sized molecular datasets the Tanimoto kernel combined with fragprint representations in particular is a very compelling option, with good accuracy, calibration, and runtime across all tasks.

### Bayesian Optimisation

Building on these results, we employed the two best-performing kernels, namely the Tanimoto-fragprint kernel and the SMILES string kernel, to undertake Bayesian optimization BO over the photoswitch and ESOL datasets. BO is run for 20 iterations of sequential candidate selection (EI acquisition) where candidates are drawn from 95% of the dataset. The models are initialised with 5% of the dataset. In the case of the photoswitch dataset, this corresponds to just 19 molecules. The results are provided in Figure 4. In this ultra-low data setting--common to many areas of synthetic chemistry --both models significantly outperform the random search baseline, highlighting the real-world use-case for such models in supporting human chemists to prioritise candidates for synthesis.

Furthermore, one may observe that BO performance is tightly coupled to regression and UQ performance. In the case of the photoswitch dataset, the better-performing Tanimoto model on regression and UQ also achieves better BO performance, while on the ESOL dataset, the string kernel performs best. Additionally, we run BO on the Buchwald-Hartwig dataset using the Tanimoto kernel for the bit-vector representations DRFP and OHE, and the RBF kernel for RXNFP. All three representations perform similarly and outperform the random search.

Finally, we also investigate the performance of GP-based models for preferential BO--a setting in which the acquisition strategy only requires rank-based preferences of candidates, as opposed to their absolute objective function values [65; 66; 67; 68]. We use a Tanimoto-fragprint kernel GP model to perform molecular Bayesian optimization on the photoswitch dataset using binary preference data alone and present the full results in Appendix E.4.

Figure 4: BO performance reporting the standard error from 50 randomly initialised trials (20 for Buchwald-Hartwig). A kernel density estimate over the trials is shown on the right axis. El fragprints results use the Tanimoto kernel. The random search baseline is indicated in blue.

## 6 Related Work

General-purpose GP and BO libraries do not cater for molecular representations. Likewise, general-purpose molecular machine learning libraries do not consider GPs and BO. Here, we review existing libraries, highlighting the niche GAUCHE fills in bridging the GP and molecular machine learning communities. The closest work to ours is FlowMO , which introduces a basic molecular GP library in the GPflow framework. In this project, we extend the scope of the library to a broader class of molecular representations (graphs), problem settings (BO) and applications (reaction optimisation and protein engineering). An overview of how GAUCHE fits into the existing open-source GP, BO and molecular machine learning stack is presented in Table 2.

Gaussian Process LibrariesGP libraries include GPy (Python) , GPflow (TensorFlow) [69; 70], GPyTorch (PyTorch)  and GPJax (Jax)  while examples of recent BO libraries include BoTorch (PyTorch) , Dragonfly (Python) , HEBO (PyTorch)  and Trieste (Tensorflow) . The aforementioned libraries do not explicitly support molecular representations. Extending them to cover molecular representations, however, requires implementations of bespoke GP kernels for bit vector, string and graph inputs together with modifications to BO schemes to consider acquisition function evaluations over a discrete set of held-out molecules, a setting commonly encountered in virtual screening [78; 79].

Molecular Machine Learning LibrariesMolecular machine learning libraries include DeepChem , DGL-LifeSci  and TorchDrug . DeepChem features a broad range of model implementations and tasks, while DGL-LifeSci focuses on graph neural networks. TorchDrug caters for applications including property prediction, representation learning, retrosynthesis, biomedical knowledge graph reasoning and molecule generation. However, none of the aforementioned libraries includes GP implementations. In terms of atomistic systems, DScribe  features, amongst other methods, the Smooth Overlap of Atomic Positions (SOAP) representation , which is typically used in conjunction with a GP model to learn atomistic properties. Automatic Selection And Prediction (ASAP)  also principally focuses on atomistic properties as well as dimensionality reduction and visualisation techniques for materials and molecules. Lastly, the Graphein library focuses on graph representations of proteins .

Graph Kernel LibrariesGraph kernel libraries include GraKel , graphkit-learn , graphkernels , graph-kernels , pykernels (https://github.com/gmm/pykernels) and ChemoCKernel . The aforementioned libraries focus on CPU implementations in Python. Extending graph kernel computation to GPUs has been noted as an important direction for future research . In our work, we build on the GraKel library to construct GPyTorch-based GPs that can be trained on non-tensorial, graph-structured inputs. It is worth noting that GAUCHE extends the applicability of GPU-enabled GPs to general graph-structured inputs beyond just molecules and proteins.

   Library & Gaussian & Bayesian & Molecular & Chemistry & Graph & Bit Vector & String \\ Processes & Optimisation & Representations & Tutorials & Kernels & Kernels & Kernels \\  GPyTorch  & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ GPflow [69; 70] & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ BoTorch  & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ \\ DeepChem  & ✗ & ✗ & ✓ & ✓ & ✗ & ✗ & ✗ \\ GraKel  & ✗ & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ \\ FlowMO  & ✓ & ✗ & ✓ & ✓ & ✗ & ✓ & ✓ \\ GAUCHE (ours) & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 2: An overview of existing open-source GP, BO and molecular machine learning libraries. With GAUCHE, we provide a modular, robust and easy-to-use framework that combines state-of-the-art probabilistic modelling and black-box optimization techniques with bespoke molecular representations and kernels to make them more easily accessible to the broader scientific community.

Molecular Bayesian OptimisationBO over molecular space can be divided into two classes. In the first class, molecules are encoded into the latent space of a variational autoencoder (VAE) . BO is then performed over the continuous latent space and queried molecules are decoded back to the original space. Much work on VAE-BO has focussed on improving the synergy between the surrogate model and the VAE [90; 5; 91; 92; 93; 94; 95; 96]. One of the defining characteristics of VAE-BO is that it enables the generation of new molecular structures. In the second class of methods, BO is performed directly over the original discrete space of molecules. In this setting it is not possible to generate new structures and so a candidate set of queryable molecules is defined. The inability to generate new structures however, is not a bottleneck to molecule discovery as the principle concern is how best to explore existing candidate sets. These candidate sets are also known as molecular libraries in the virtual screening literature . To date, there has been little work on BO directly over discrete molecular spaces. In , the authors use a string kernel GP trained on SMILES to perform BO to select from a candidate set of molecules. In , an optimal transport kernel GP is used for BO over molecular graphs. In  a surrogate based on the Nadarya-Watson estimator is defined such that the kernel density estimates are inferred using BNNs. The model is then trained on molecular descriptors. Lastly, in  and  a BNN and a sparse GP respectively are trained on fingerprint representations of molecules. In the case of the sparse GP the authors select an ArcCosine kernel. It is a longstanding aim of the GAUCHE Project to compare the efficacy of VAE-BO against vanilla BO on real-world molecule discovery tasks.

Chemical Reaction OptimisationChemical reactions describe how reactants transform into products. Reagents (catalysts, solvents, and additives) and reaction conditions heavily impact the outcome of chemical reactions. Typically the objective is to maximise the reaction yield (the amount of product compared to the theoretical maximum) , in asymmetric synthesis, where the reactions could result in different enantiomers, to maximise the enantiomeric excess , or to minimise the E-factor, which is the ratio between waste materials and the desired product . A diverse set of studies have evaluated the optimisation of chemical reactions in single and multi-objective settings [103; 104].  and  benchmarked reaction optimisation algorithms in low-dimensional settings including reaction conditions, such as time, temperature, and concentrations.  suggested BO as a general tool for chemical reaction optimisation and benchmarked their approach against human experts.  compared the yield prediction performance of different kernels and  the impact of various molecular representations. In all reaction optimisation studies above, the representations of the different categories of reactants and reagents are concatenated to generate the reaction input vector, which could lead to limitations if another type of reagent is suddenly considered. Moreover, most studies concluded that simple one-hot encodings (OHE) perform at least on par with more elaborate molecular representations in the low-data regime [6; 108; 109]. In GAUCHE, we introduce reaction fingerprint kernels, based on existing reaction fingerprints [46; 45] and work independently of the number of reactant and reagent categories.

## 7 Limitations

One potential limitation of GAUCHE is the focus on core implementations that are likely to remain robust across as many applied problems as possible which will enable the library to have the most impact. As such, there is less of a focus on bespoke GP implementations for more targeted problems [110; 111; 112; 113]. Nonetheless, we hope that GAUCHE will function as an active development platform for such implementations.

## 8 Conclusion

We have introduced GAUCHE, a library for Gaussian Processes in Chemistry, with the aim of providing a user-friendly and robust library of state-of-the-art uncertainty quantification and Bayesian optimisation tools that may hopefully be deployed for screening in laboratory settings. Our aim is to maintain a lean, well-tested and up-to-date codebase and invite community-driven contributions principally as pull requests in the form of notebooks that reflect the needs and considerations that researchers come across in practice. In this fashion, we may support more advanced features without bloating the codebase and increasing maintenance requirements.

Acknowledgements

BR and PS acknowledge support from the NCCR Catalysis (grant number 180544), a National Centre of Competence in Research funded by the Swiss National Science Foundation. ARJ is funded by a Biotechnology and Biological Sciences Research Council (BBSRC) DTP studentship (BB/M011194/1). GT acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), and the Vector Institute. FS-K is a postdoctoral fellow in the Eric and Wendy Schmidt AI in Science Postdoctoral Fellowship Program, a program by Schmidt Futures. AA-G acknowledges the generous support of Anders G. Forseth, the Canadian Institute for Advanced Research (CIFAR), and the Canada 150 Research Chair program. LK acknowledges support from the University of Oxford's Clarendon Fund.

R-RG declares that this work was done exclusively at the University of Cambridge. Code/data was not created/accessed by Meta.