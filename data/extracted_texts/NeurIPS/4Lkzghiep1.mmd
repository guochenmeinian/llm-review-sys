# Strategic Littlestone Dimension:

Improved Bounds on Online Strategic Classification+
Footnote â€ : Authors are ordered alphabetically.

Saba Ahmadi\({}^{,}\)

Authors are ordered alphabetically.

Kunhe Yang\({}^{,*}\)

The following article (among others) discusses some well-known tricks for this: [https://www.nerdwallet.com/article/finance/raise-credit-score-fast](https://www.nerdwallet.com/article/finance/raise-credit-score-fast).

Hanrui Zhang\({}^{@sectionsign,*}\)

The following article (among others) discusses some well-known tricks for this: [https://www.nerdwallet.com/article/finance/raise-credit-score-fast](https://www.nerdwallet.com/article/finance/raise-credit-score-fast).

However, the situation becomes subtler in _online_ environments, where the decision maker has little or no prior knowledge about the population being classified, and must constantly adjust the decision-making policy (i.e., the _classifier_) through trial and error. This is particularly challenging in the presence of strategic behavior, because often the decision maker can only observe the agent's features _after manipulation_. In such online environments, the performance of a learning algorithm is often measured by its _regret_, i.e., how many more mistakes it makes compared to the best classifier within a certain family _in hindsight_. For online strategic classification, while progress has been made in understanding the optimal regret in several important special cases, a full instance-wise characterization has been missing, even in the seemingly basic realizable setting (meaning that there always exists a perfect classifier in hindsight). This salient gap is the starting point of our investigation in this paper -- which turns out to reach quite a bit beyond the gap itself.

Following prior work (Ahmadi et al., 2023; Cohen et al., 2024), we study the following standard and general model of online strategic classification: we have a (possibly infinite) feature space, equipped with a manipulation graph defined over it. An edge between two feature vectors \(x_{1}\) and \(x_{2}\) means that an agent whose true features are \(x_{1}\) can pretend to have features \(x_{2}\), and in fact, the agent would have incentives to do so if the label assigned to \(x_{2}\) by the classifier is better than that assigned to the true features \(x_{1}\). At each time step, the decision maker commits to a classifier (which may depend on observations from previous interactions), and an agent arrives and observes the classifier. The agent then responds to the classifier by reporting (possibly nontruthfully) a feature vector that leads to the most desirable label subject to the manipulation graph, i.e., the reported feature vector must be a neighbor of the agent's true feature vector. The decision maker then observes the reported feature vector, as well as whether the label assigned to that feature vector matches the agent's true label.

### Our Results and Techniques

An instance-optimal regret bound through the strategic Littlestone dimension.Our first main finding is an instance-optimal regret bound for online strategic classification in the realizable setting, when randomization is not allowed (we will discuss the role of randomization in Section 6). In this setting, there is a predefined hypothesis class of classifiers, in which there must exist one classifier that assigns all agents their true labels under manipulation. The decision maker, knowing that a perfect classifier exists in this class, tries to learn it on the fly while making as few mistakes as possible in the process. Naturally, the richer this hypothesis class is, the harder the decision maker's task will be (e.g., one extreme is when the hypothesis class contains only one classifier, and the decision maker knows a priori that that classifier must be perfectly correct, and the optimal regret is \(0\)). Thus, the optimal regret must depend on the richness of the hypothesis class. Similarly, one can imagine that the optimal regret must also depend on the manipulation graph.

Previous work (Ahmadi et al., 2023; Cohen et al., 2024) has established regret bounds for this setting based on various complexity measures of the hypothesis class and the manipulation graph, including the size and the (classical) Littlestone dimension (Littlestone, 1988) of the hypothesis class, as well as the maximum out-degree of the manipulation graph. However, the optimality (when applicable) of these bounds only holds under the assumption that the mistake bound must be parametrized as a function on the classical Littlestone dimension and/or the graph's out-degree. However, these bounds are not tight for all instances, as there exist problem instances that are learnable where all these parameters are infinite.

To address the above issue, we introduce a new combinatorial complexity measure that generalizes the classical Littlestone dimension into strategic settings. Conceptually, the new notion also builds on the idea of "shattered trees", which has proved extremely useful in classical settings. However, the asymmetry introduced by strategic behavior3 demands a much more delicate construction of shattered trees (among other intriguing implications to be discussed in Section 3). We show that the generalized Littlestone dimension captures precisely the optimal regret of any deterministic learning algorithm given a particular hypothesis class and a manipulation graph, thereby providing a complete characterization of learnability in this setting. Being instance-optimal, our bound strengthens and unifies all previous bounds for online strategic classification in the realizable setting.

An improved regret bound for the agnostic case.We then proceed to the agnostic setting, where no hypothesis necessarily assigns correct labels to all agents. The regret is defined with respect to the best hypothesis in hindsight. Compared to the classical (i.e., non-strategic) setting, the main challenge is incomplete information: since the learner cannot observe original features, upon observing the behavior of an agent under one classifier, it is not always possible to counterfactually infer what would have been observed if the learner used another classifier.

To understand why this can be a major obstacle, recall some high-level ideas behind the algorithms for classical agnostic online classification (see, e.g., (Ben-David et al., 2009)). The key is to construct a finite set of "representative" experts out of the potentially infinite set of hypotheses, such that the best expert performs almost as well as the best hypothesis in hindsight. An agnostic learner then runs a no-regret learning algorithm (such as multiplicative weights) on the expert set, which in the long run matches the performance of the best expert, and in turn of the best hypothesis.

The partial information challenge appears in both steps of the above approach. First, to construct the set of representative experts, the learner needs to simulate the observation received by each hypothesis, had that hypothesis been used to label the strategic agents. Second, the no-regret algorithm on the expert set also needs to counterfacturally infer the agent's response to each expert. To circumvent both issues, we design a nuanced construction of the representative set of experts that effectively "guesses" each potential direction of the agent's manipulation. We then run the biased voting approach introduced in (Ahmadi et al., 2023) on the finite set of experts, which enjoys regret guarantees in the strategic setting even with partial information. Combined with our regret bound for the realizable setting, this approach yields an improved bound for the agnostic setting.

Learning with unknown manipulation graphs.Our last result focuses on relaxing the assumption that the learner has perfect knowledge about the manipulation graph structure. Instead, following previous works (Lechner et al., 2023; Cohen et al., 2024), we model their knowledge about the manipulation graph using a pre-defined graph class, which to some degree reflects the true set of feasible manipulations. In this setting, our work is the first that provides positive results when the learner only observes features after manipulation. We start with the realizable setting where the manipulation graphs are consistently modeled by the same (unknown) graph in the class. In this setting, we provide a more careful construction of the representative experts to account for the additional challenge of unknown graphs. Combing this construction with re-examining the effectiveness of the biased voting approach (Ahmadi et al., 2023) when the input graph is a overly-conservative estimate of the true graph, we obtain the first regret bound in this setting that is approximately optimal (up to logarithmic factors) in certain instances. We also extend our results to fully agnostic settings where the agents in each round manipulates according to a potentially different graph, and the best graph in the class has nonzero error in modeling all the manipulations.

### Further Related Work

There is a growing line of research that studies learning from data provided by strategic agents (Dalvi et al., 2004; Dekel et al., 2008; Bruckner and Scheffer, 2011). The seminal work of Hardt et al. (2016) introduced the problem of _strategic classification_ as a repeated game between a mechanism designer that deploys a classifier and an agent that best responds to the classifier by modifying their features at a cost. Follow-up work studied different variations of this model, in an online learning setting (Dong et al., 2018; Chen et al., 2020; Ahmadi et al., 2021), incentivizing agents to take improvement actions rather than gaming actions (Kleinberg and Raghavan, 2020; Haghtalab et al., 2020; Alon et al., 2020; Ahmadi et al., 2022), causal learning (Bechavod et al., 2021; Perdomo et al., 2020), screening processes (Cohen et al., 2023), fairness (Hu et al., 2019), etc.

Two different models for capturing the set of plausible manipulations have been considered in the literature. The first one is a geometric model, where the agent's best-response to the mechanism designer's deployed classifier is a state within a bounded distance (with respect to some \(_{p}\) norm) from the original state, i.e. feature set (Dong et al., 2018; Chen et al., 2020; Shao et al., 2024; Sundaram et al., 2023; Ghalme et al., 2021; Haghtalab et al., 2020). In the second model, introduced by Zhang and Conitzer (2021) there is a combinatorial structure, i.e. manipulation graph, that captures the agent's set of plausible manipulations. This model has been studied in both offline PAC learning (Lechner and Urner, 2022; Zhang and Conitzer, 2021; Lechner et al., 2023) and online settings (Ahmadi et al., 2023). In a recent work, Lechner et al. (2023) consider this problem in an offline setting where the underlying manipulation is unknown and belongs to a known family of graphs. Our work improves the results given by (Ahmadi et al., 2023) in the online setting and also extends their results to the setting where the underlying manipulation is unknown and belongs to a known family of graphs.

Our work is also closely related to that of Cohen et al. (2024), with two main points of distinction. First, in the realizable setting, their bound is shown to be optimal for a specific instance, whereas our bound is instance-wise optimal. Second, in both the agnostic and the unknown graph settings, they assume that agents' original features are observable before the learner makes decisions, whereas our algorithm only requires access to post-manipulation features.

Finally, our work is also tangentially connected to several recent advances in understanding multi-class classification under bandit feedback, e.g., (Raman et al., 2024; Filmus et al., 2024), as the false-positive mistake types can be treated as multiple labels at a very abstract level. However, an additional challenge in the strategic setting is that the learner needs to choose a classifier without observing the original instance to be labeled.

## 2 Model and Preliminaries

### Strategic classification.

Let \(\) be a space of feature vectors, \(=\{-1,1\}\) be the binary label space, and \(:\) be a hypothesis class that is known to the learner (also referred to as "decision-makers"). In the strategic classification setting, agents prefer positive labels over negative labels, and they may manipulate their features within a predefined range to receive a positive label. We use the _manipulation graphs_ introduced by (Zhang and Conitzer, 2021; Lechner and Urner, 2022) to model the set of feasible manipulations. The manipulation graph \(G(,)\) is a directed graph in which each node corresponds to a feature vector in \(\), and each edge \((x_{1},x_{2})^{2}\) represents that an agent with initial feature vector \(x_{1}\) can modify their feature vector to \(x_{2}\). For each \(x\), we use \(N_{G}^{+}(x)\) to denote the set of out-neighbors of \(x\) in \(G\), excluding \(x\) itself, and \(N_{G}^{+}[x]\) to denote the out-neighbors including \(x\). Formally, \(N_{G}^{+}(x)=\{x^{}\{x\}(x,x^{}) \}\) and \(N_{G}^{+}[x]=\{x\} N_{G}^{+}(x)\). Similarly, we use \(N_{G}^{-}(x)\) and \(N_{G}^{-}[x]\) to denote respectively the exclusive and inclusive in-neighborhood of \(x\).

Agents' utility and the manipulation rule.Given a manipulation graph \(G(,)\) and a classifier \(h^{}\), an agent with initial features \(x\) aims to maximize their utility by potentially manipulating their features to a different \(x^{}\). The agent's utility function \(_{G,h}(x,x^{})\) is defined as

\[_{G,h}(x,x^{})=h(x^{})- 1\{(x,x^{ })\},\]

where the agent's utility is the classification outcome \(h(x^{})\) minus the manipulation cost associated with changing features from \(x\) to \(x^{}\). For the classification outcome, agents receive utility \(+1\) if the classifier \(h\) labels \(x^{}\) as positive and \(-1\) otherwise. For the manipulation cost, moving from \(x\) to \(x^{}\) incurs no cost if the two features are connected by an edge in \(G\), but when \(x\) and \(x^{}\) are not connected (i.e., \((x,x^{})\)), the manipulation incurs an infinite cost, effectively making such a manipulation infeasible. As a result, an agent with initial features \(x\) would move to some \(x^{}\) in the inclusive neighborhood \(N_{G}^{+}[x]\) that is labeled as positive by \(h\), if such a node exists.

Formally, the set of best response features from \(x\), denoted by \(_{G,h}(x)\), is defined as

\[_{G,h}(x)\{x^{}_{G,h}(x,x^{ })=+1\}=N_{G}^{+}[x]\{x h(x)=+1\}.\]

If, however, the set \(_{G,h}(x)\) is empty -- indicating that the entire out-neighborhood \(N_{G}^{+}[x]\) is labeled as negative by \(h\) -- then the agent is assumed to _not manipulate_ their features and remain at \(x\). In addition, when there are multiple nodes in \(_{G,h}(x)\), the agent may select any node arbitrarily. We do not require the selection to be consistent across rounds or to follow a pre-specified rule. Our results specifically focus on the learner's mistakes against worst-case (adversarial) selections by the agent.4 Finally, the manipulated feature vector is denoted by \(_{G,h}(x)\).

The labels induced by the manipulated feature vectors \(_{G,h}(x)\) are captured by _effective classifiers_\(_{G}\), formally defined as

\[_{G}(x) h(_{G,h}(x))=+1,& N_{G}^{+}[x],h()=+1;\\ -1,&.\]

Online learning.We consider an _online_ strategic classification setting modeled as a repeated game between the learner (aka the decision maker) and an adversary over \(T\) rounds, where the learner make decisions according to an online learning algorithm \(\). At each round \(t[T]\), the learner first commits to the classifier \(h_{t}^{}\) (not necessarily restricted to \(\)) that is generated by \(\). The adversary then selects an agent \((x_{t},y_{t})\) where \(x_{t}\) is the original feature vector and \(y_{t}\) is the true label. In response to \(h_{t}\), the agent manipulates their features from \(x_{t}\) to \(v_{t}=_{G,h}(x_{t})\). Consequently, the learner observes the manipulated features \(v_{t}\) (instead of \(x_{t}\)), and incurs a mistake if \(y_{t} h_{t}(v_{t})\). We use \(S=(x_{t},y_{t})_{t[T]}\) to denote the sequence of agents.

The learner aims to minimize the Stackelberg regret on \(S\) with respect to the optimal hypothesis \(h^{}\) had the agents responded to \(h^{}\):

\[_{}(S,,G)_{t=1}^{T}\{h_{t}(_{G,h_{t}}(x_{t})) y_{t}\}-_{h^{} }_{t=1}^{T}\{h^{}(_{G,h^{ }}(x_{t})) y_{t}\}.\]

We call a sequence \(S\)_realizable_ with respect to \(\) if the optimal-in-hindsight hypothesis \(h^{}\) achieves zero mistakes on \(S\). Specifically, this means that for all \((x_{t},y_{t})\) in the sequence \(S\), we have \(y_{t}=h^{}(_{G,h^{}}(x_{t}))=_{} ^{}(x_{t})\). In such cases, the learner's regret coincides with the number of mistakes made. We use \(_{}(,G)\) to denote the _maximal number of mistakes_ that \(\) makes against any realizable sequence with respect to class \(\) and graph \(G\). A deterministic algorithm is called _min-max optimal_ or _instance-optimal_ if achieves the minimal \(_{}(,G)\) across all deterministic algorithms5. We denote this optimal mistake bound by \((,G)_{}_{}(,G)\).

### Classical Littlestone Dimension

In this section, we revisit the classical online binary classification setting where the agents are unable to strategically manipulate their features. This setting can be viewed as a special case of strategic classification where the manipulation graph \(G\) consists solely of isolated nodes. We will introduce the characterization of the optimal mistake in this classical setting -- known as the _Littlestone Dimension_ -- which inspires our analysis in the strategic setting.

**Definition 2.1** (\(\)-Shattered Littlestone Tree).: _A Littlestone tree shattered by hypothesis class \(\) of depth \(d\) is a binary tree where:_

* _(Structure) Nodes are labeled by_ \(\) _and each non-leaf node has exactly two outgoing edges that are labeled by_ \(+1\) _and_ \(-1\)_, respectively._
* _(Consistency) For every root-to-leaf path_ \(x_{1}}x_{2}} x_{d}}x_{d+1}\) _where_ \(x_{1}\) _is the root node and each_ \(y_{t}\) _is the edge connecting_ \(x_{t}\) _and_ \(x_{t+1}\)_, there exists a hypothesis_ \(h\) _that is consistent with the entire path, i.e.,_ \( t d,\;h(x_{t})=y_{t}\)_._

The above tree structure intuitively models an adversary's strategy to maximize the learner's mistakes, where each node \(x_{t}\) represents the unlabeled instance to be presented to the learner, and \(y_{t}\) represents the type of mistake (either a false positive or a false negative) that the adversary aims to induce. For example, if the learner predicts the label of \(x_{t}\) to be \(_{t}=+1\), then the adversary will declare \(y_{t}=-1\), enforce a false positive mistake, and choose the next instance \(x_{t+1}\) as the children of the current node along the \(-1\) edge. In addition, the _consistency_ requirement guarantees that the resulting input sequence is realizable by some classifier in \(\).

**Definition 2.2** (Littlestone Dimension).: _The Littlestone dimension of class \(\), denoted as \(()\), is the maximum integer \(d\) such that there exists an \(\)-shattered Littlestone of depth \(d\)._

The interpretation of the true structure immediately implies that the mistake of any algorithm should be lower bounded by \(()\). Moreover, a seminal result by Littlestone (1988) also showed that an online learning algorithm known as the Standard Optimal Algorithm (SOA, see Algorithm 2 in Appendix A) can achieve this lower bound. Together, they form a complete characterization of the optimal mistake bound in the classical setting, which we summarize in the following proposition.

**Proposition 2.1** (Optimal Mistake Bound ).: _Let \(()\) be the optimal mistake in the classical online learning setting, then \(()=()\)._

In the next section, we will discuss the challenges of extending Littlestone's characterization to the strategic setting, and present our solution.

## 3 The Strategic Littlestone Dimension

In this section, we will introduce a new combinatorial dimension called the _Strategic Littlestone Dimension_, and show that it characterizes the minmax optimal mistake bound for strategic classification.

Inspired by the classical Littlestone dimension, we hope to use a tree structure to model an adversary's strategy for selecting agents \((x_{t},y_{t})\), where nodes serve as (proxies of) the initial feature vector of each agent, and edges represent the types of mistakes that the adversary can induce. However, since agents can strategically manipulate their features, the potential mistakes associated with the same initial feature vector could manifest in many more types depending on the learner's choice of classifiers. Specifically, let \(x\) be the initial feature vector. Then a mistake associated with \(x\) might be observed as a false negative mistake at node \(x\) (denoted as \((x,+1)\) where \(x\) is the observable node and \(+1\) is the true label), or as a false positive mistake at any outgoing neighbor \(v\) of \(x\) (denoted as \((v,-1)\) accordingly). Therefore, an adversary's strategy should accommodate all such possibilities, which necessitates the strategic Littlestone tree to contain branches representing all potential mistake types.

Another challenge is caused by the mismatch of the information available to the learner and the adversary. Since the learner only observes manipulated features instead of the true ones, the amounts of information carried by false positive and false negative mistakes are inherently asymmetric. False negatives provide full-information feedback as the manipulated and original features are identical. However, false positives introduce uncertainty about the original features, which could potentially be any in-neighbor of the observed one. As a result, a hypothesis is deemed "consistent" with a false positive observation as long as it can correctly label any one of the potential original nodes.

Now we formally introduce the _strategic Littlestone tree_ with adapted branching and consistency rules tailored for strategic classification. See Figure 1 for a pictorial illustration.

**Definition 3.1** (\(\)-Shattered Strategic Littlestone Tree).: _A Strategic Littlestone tree for hypothesis class \(\) under graph \(G\) with depth \(d\) is a tree where:_

* _(Structure) Nodes are labeled by_ \(\)_. The set of outgoing edges from each non-leaf node_ \(x\) _are: one false negative edge_ \((x,+1)\)_, and a set of false positive edges_ \(\{(v,-1) v N_{t}^{+}[x]\}\)_._
* _(Consistency) For every root-to-leaf path_ \(x^{}_{1},y_{1})}x^{}_{2},y_{ 2})} x^{}_{d},y_{d})}x^{}_{d+1}\) _where_ \(x^{}_{1}\) _is the root node and_ \((v_{t},y_{t})\{ 1\}\) _is the edge that connects_ \(x^{}_{t}\) _and_ \(x^{}_{t+1}\)_, there exists a hypothesis_ \(h\) _that is consistent with the entire path. Specifically,_ \( t d\)_,_ \( x_{t}\) _s.t._ \(_{G}(x_{t})=y_{t}\)_, where_ \(x_{t}\) _satisfies_ \(x_{t}=v_{t}\) _if_ \(y_{t}=+1\)_, and_ \(x_{t} N_{G}^{-1}[v_{t}]\) _if_ \(y_{t}=-1\) _._

**Definition 3.2** (Strategic Littlestone Dimension).: _The Strategic Littlestone Dimension of a hypothesis class \(\) under graph \(G\), denoted with \((,G)\), is defined as the largest nonnegative integer \(d\) for which there exists a Strategic Littlestone tree of depth \(d\) shattered by \(\) under graph \(G\)._

**Theorem 3.1** (Minmax optimal mistake for strategic classification).: _For any hypotheses class \(\) and manipulation graph \(G\), the minmax optimal mistake in the realizable setting is captured by the strategic Littlestone dimension, i.e., \((,G)=(,G)\)._

We divide the proof of Theorem 3.1 into two parts: the lower bound direction is established in Theorem 3.2, and the upper bound direction is established in Theorem 3.3.

**Theorem 3.2** (Lower bound part of Theorem 3.1).: _For any pair of hypothesis class \(\) and manipulation graph \(G\), any deterministic online learning algorithm \(\) must suffer a mistake lower bound of \(}}(,G)(,G)\)._

Proof sketch of Theorem 3.2.: Let \(\) be a strategic Littlestone tree for \((,G)\) with depth \(d\). We will show that for any deterministic algorithm \(\), there exists an adversarial sequence of agents\(S=(x_{t},y_{t})_{t[d]}\) such that \(\) is forced to make a mistake at every round. We construct the sequence \(S\) by first finding a path \(x^{}_{1},y_{1})}x^{}_{2},y_{2 })} x^{}_{d},y_{d})}x^{}_{d+1}\) in tree \(\) which specifies the types of mistakes that the adversary wishes to induce, then reverse-engineering this path to obtain the sequence of initial feature vectors before manipulation that is realizable under \(\). We remark that the need for reverse-engineering is unique to the strategic setting, which is essential in resolving the information asymmetry between the learner and the adversary regarding the true features. We formally prove this theorem in Appendix B.1. 

In the remainder of this section, we present an algorithm called the Strategic Standard Optimal Algorithm (SSOA) that achieves the instance-optimal mistake bound of \((,G)\). We first define some notations. For any hypothesis sub-class \(\) and an observable labeled instance \((v,y)\), we use \(^{(v,y)}_{G}\) to denote the subset of \(\) that is consistent with \((v,y)\) under manipulation graph \(G\). We refer to the consistency rule defined in Definition 3.1, i.e., for all \(v\), \(^{(v,+1)}_{G}\) and \(^{(v,-1)}_{G}\) are defined respectively as:

\[^{(v,+1)}_{G}\{h_{G}(v)=+1 \};^{(v,-1)}_{G}\{h x N ^{-}_{G}[v]_{G}(x)=-1\}.\]

We present \(\) in Algorithm 1 and prove its optimality in Theorem 3.3. The high-level idea of \(\) is similar to the classical SOA algorithm (Algorithm 2): it maintains a version space of classifiers consistent with the history, and chooses a classifier \(h_{t}\) in a way that guarantees the "progress on mistakes" property. This means that the (strategic) Littlestone dimension of the version space should decrease whenever a mistake is made.

However, designing \(h_{t}\) to satisfy this property in a strategic setting is more challenging because the potential types of mistakes depend on \(h_{t}\)'s labeling in the neighborhood \(N^{+}_{G}[x_{t}]\), where \(x_{t}\) is unobservable. If we directly optimize the labelings on \(N^{+}_{G}[x]\) for each \(x\) independently, the resulting classifier may suggest self-contradictory labelings to the nodes in the overlapping parts of \(N^{+}_{G}[x]\) and \(N^{+}_{G}[x^{}]\) for different \(x\) and \(x^{}\). Instead, the learner needs to choose a single \(h_{t}\) that simultaneously guarantees the "progress on mistakes" property for all possible \(x_{t}\) and their neighborhoods.

In the following, we will show that this challenge can be resolved by choosing a classifier \(h_{t}\) that labels each node \(x\) only based on whether a false positive observation \((x,-1)\) can decrease the strategic Littlestone dimension, as described in Line 3 of Algorithm 1.

**Theorem 3.3** (Upper bound part of Theorem 3.1).: _The \(\) algorithm (Algorithm 1) achieves a maximal mistake bound of \((,G)(,G)\)._

**Remark 3.4** (Comparison with previous results).: _Since the mistake bound of \(\) is shown to be instance-optimal across all deterministic algorithms, it improves upon the bounds established by Ahmadi et al. (2023); Cohen et al. (2024), which both depend on the maximum out-degree of the graph \(G\). Furthermore, we show in Appendix B.2 that the gap between their bounds and ours could be arbitrarily large. An extreme example is the complete graph \(G\) supported on an unbounded domain, where \((,G)=1\) but both previous bounds are \(\)._

Proof of Theorem 3.3.: It suffices to prove that if \(\) makes a mistake at round \(t\), then the strategic Littlestone dimension of version space \(_{t}\) (maintained by the \(\) algorithm in Line 5) must

Figure 1: A Strategic Littlestone Tree with depth \(2\). False negative edges are marked red, whereas false positive edges are marked blue. The highlighted path \(x^{}_{1},y_{1})}x^{}_{2},y_ {2})}x^{}_{3}\) is an example root-to-leaf path. In this path, the first observation \((v_{1},y_{1})\) is a false positive, which satisfies \(v_{1} N^{+}_{G}[x^{}_{1}]\) and \(y_{1}=-1\); the second observation \((v_{2},y_{2})\) is a false negative, which satisfies \(v_{2}=x^{}_{2}\) and \(y_{2}=+1\).

decrease by at least 1, namely \((_{t},G)(_{t-1},G)-1\). For notational convenience, let \(d=(_{t-1},G)\).

False positives.We start with the case where \(\) makes a false positive mistake, i.e., \(h_{t}(v_{t})=+1\) but \(y_{t}=-1\). According to the definition of classifier \(h_{t}\) and the update rule of version space \(_{t}\), we immediately obtain

\[(_{t},G)=((_{t-1})_{G}^ {(v_{t},-1)},G)<(_{t-1},G)\;\; (_{t},G) d-1.\]

False negatives.Then we consider the case where \(\) makes a false negative mistake, i.e., \(h_{t}(v_{t})=-1\) but \(y_{t}=+1\). For the sake of contradiction, assume that the strategic Littlestone dimension does not decrease, i.e., \((_{t},G)=((_{t-1})_{G} ^{(v_{t},+1)},G)=d\). This assumption implies that there exists a strategic Littlestone tree \(\) that is shattered by \((_{t-1})_{G}^{(v_{t},+1)}\) and of depth \((_{t-1},G)\).

Since the agent is classified as negative, it must be the case that the agent has not manipulated (i.e., \(x_{t}=v_{t}\)), and the entire outgoing neighborhood \(N_{G}^{+}[x_{t}]\) is labeled as negative by \(h_{t}\). Therefore, according to the definition of \(h_{t}\), for all \(v N_{G}^{+}[x_{t}]\), we have \(((_{t-1})_{G}^{(v,-1)},G)=d\), which implies that there also exists a strategic Littlestone tree \(_{v}\) of depth \(d\) that is shattered by \((_{t-1})_{G}^{(v,-1)}\).

Now consider the tree \(^{}\) with root \(x_{t}\), subtree \(\) on the false negative edge \((x_{t},+1)\), and subtree \(_{v}\) on each false positive edge \((v,-1)\) for all \(v N_{G}^{+}[x_{t}]\). Since we have argued that each subtree has depth \(d\), the overall depth of \(^{}\) is \(d+1\). We claim that \(^{}\) is shattered by \(_{t-1}\). In fact, for all root-to-leaf paths in \(^{}\), the first observation is guaranteed to be consistent with all hypotheses in the subclass for the subtree, and the consistency of each subtree ensures the existence of a hypothesis that makes all subsequent observations realizable.

We have thus constructed a strategic Littlestone tree \(^{}\) that is shattered by \(_{t-1}\) and of depth \(d+1\). However, this contradicts with the assumption that \((_{t-1},G)=d\). Therefore, it must follow that \((_{t},G) d-1=(_{t-1},G)-1\), which in turn proves \(}}(,G)(,G)\). 

## 4 Agnostic Setting

In this section, we study the regret bound in the agnostic setting. Recall that benchmark is defined as the minimum number of mistakes that the best hypothesis in \(\) makes, i.e., \(_{h^{}}_{t[T]}\{h^{}(_{G,h^{}}(x_{t})) y_{t}\}\). We will present an algorithm that has vanishing regret compared to \(_{G}^{+}\) whenever the strategic Littlestone dimension is bounded, where \(_{G}^{+}\) is the maximum out-degree of \(G\). Inspired by the classical reduction framework proposed by Ben-David et al. (2009), our algorithm aims to reduce the agnostic problem to that of strategic online learning with expert advice by constructing a finite number of representative experts that performs almost as well as the potentially unbounded hypothesis class. The problem with a finite expert set can then be solved using the biased weighted voting algorithm proposed by Ahmadi et al. (2023). However, establishing the reduction turns out to be more challenging in the strategic setting, as the learner can only observe manipulated features instead of the original ones. We address this problem by designing the experts to "guess" every possibile direction the original node could have come. We present our algorithms (Algorithms 3 and 4) in Appendix C and analyze their regret in Theorem 4.1.

**Theorem 4.1**.: _For any adaptive adversarial sequence \(S\) of length \(T\), the Agnostic Online Strategic Classification algorithm (Algorithm 3) has regret bound_

\[(S,,G) O(_{G}^{+}+ _{G}^{+}(,G)( T+_{G}^{- })),\]

_where \(_{G}^{+}\) (resp. \(_{G}^{-}\)) denotes the maximum out-degree (resp. in-degree) of graph \(G\)._

**Remark 4.2**.: _Ahmadi et al. (2023) showed that there exists instances in which all deterministic algorithms must suffer regret \((_{G}^{+})\), which means the first term in the above bound is necessary. The second term connects to our instance-wise lower bound of \((,G)\) in Theorem 3.2._

Proof sketch of Theorem 4.1.: We use \(\) to denote the set of experts constructed in Algorithm 4, and define \(^{}\) as the minimum number of mistakes made by the best expert \(^{}\), had the agents responded to \(^{}\). Then the _Biased Weighted Majority Vote_ algorithm from Ahmadi et al. (2023) guarantees that the number of mistakes made by Algorithm 3 is at most \(_{G}^{+}^{}+_{G}^{+}| |\). According to our construction of experts, the total number of experts satisfies \(||_{m d}(_{G}^{- })^{m} O(d( T+_{G}^{-}))\), where \(d=(,G)\) is the strategic Littlestone dimension. Therefore, it suffices to show that \(^{}\) is not too much larger than \(\)--in other words, the set of experts \(\) are _representative_ enough of the original hypothesis class \(\) in their ability of performing strategic classification. We use the following lemma, which we prove in Appendix C by establishing the equivalence between the \(\) instance running on the sequence labeled by the effective classifier and the \(\) instance simulated by a specific expert.

**Lemma 4.3** (Experts are representative).: _For any hypothesis \(h\) and any sequence of agents \(S\), there exists an expert \(_{h}\) that makes at most \((,G)\) more mistakes than \(h\)._

## 5 Unknown Manipulation Graph

In this section, we generalize the main settings to relax the assumption that the learner has full knowledge about the underlying manipulation graph \(G\). Instead, we use a graph class \(\) to capture the learner's knowledge about the manipulation graph. In Section 5.1, we begin with the _realizable graph class_ setting, where the true manipulation graph remains the same across rounds and belongs to the family \(\). We then study the _agnostic graph class_ setting in Section 5.2, where we drop both assumptions and allow our regret bound to depend on the "imperfectness" of \(\). In both cases, we assume the hypothesis class \(\) is also agnostic, which encompasses the setting where \(\) is realizable.

### Realizable graph classes

In this section, we assume that there exists a perfect (but unknown) graph \(G^{}\), such that each agent \((x_{t},y_{t}) S\) manipulates according to \(G^{}\). We define the benchmark \(_{}\) to be the optimal number of mistakes made by the best \(h^{}\) assuming that each agent best responds to \(h^{}\) according to \(G^{}\). Formally, \(_{}_{h^{}} \{h^{}(_{G^{},h^{}}(x_{t})) y_{t}\}\). Same to our main setting, we assume that the learner only observes the post-manipulation features \(v_{t}=_{G^{},h_{t}}(x_{t})\) after they commit to classifier \(h_{t}\), but cannot observe the original features \(x_{t}\).

Our algorithm (Algorithm 6) for this setting leverages two main ideas. First, to overcome the challenge that \(G^{}\) is unknown to the experts, we blow up the number of experts by a factor of \(||\) and let each expert simulate their own \(\) instance according to some internal belief of \(G^{}\). Since the regret bound depends logarithmic on the number of experts, this only introduces an extra \(||\) term, which has been shown by Cohen et al. (2024) to be unavoidable even when the learner has access to the original features.

Our second idea involves re-examining the correctness of Algorithm 5 for bounded expert class to the scenario where the input \(G\) is a pessimistic estimate of the true graph \(G^{}\), i.e., \(G\) contains all the edges in \(G^{}\) but potentially some extra edges. This allows us to use \(G_{}\) whose edge set is taken to be the union of all edges in \(\). Combining these two ideas, we present our algorithm and establish its regret bound (Theorem 5.1) in Appendix D.

**Theorem 5.1**.: _For any realizable graph class \(\) and any adaptive adversarial sequence \(S\) of length \(T\), Algorithm 6 has regret bound_

\[(S,,G) O(_{}^{+}( _{}+d_{}( T+_{ }^{-})+||)),\]

_where \(d_{}_{}( ,G)\) is the maximum strategic Littlestone dimension for all graphs in \(\), \(_{}^{+}_{}}^{+}\) is the maximum out-degree of \(G_{}\) (i.e., the union of graphs in \(\)), and \(_{}^{-}_{G}_{G}^{-}\) is the maximum max in-degree over graphs in \(G\)._

**Remark 5.2** (Implications in the realizable setting).: _In the realizable setting where \(_{}=0\), Theorem 5.1 implies a mistake bound of \((_{}^{+} d_{}+||)\). This bound is optimal up to logarithmic factors due to a lower bound proved by Cohen et al. (2024, Proposition 14). They constructed an instance with \(||=||=(n)\) in which any deterministic algorithm makes \((n)\) mistakes. In this instance, our bound evaluates to be \((n)\) since \(_{}^{+}=(n)\) and \(d_{}=1\)._

### Agnostic graph classes

In this section, we consider a fully agnostic setting where each agent \((x_{t},y_{t})\) may behave according to a different manipulation graph \(G_{t} G_{}\). We define the benchmark \(_{}\) to count the number of times that the best graph \(G^{}\) fails to model the local manipulation structure under \(G_{t}\), and \(_{}\) is defined as in Section 5.1, using the graph \(G^{}\) that achieves \(_{}\).

Assuming access to an upper bound \(N\) of \(_{}\), we present Algorithm 7 that achieves a regret bound of \((_{}^{+}(N+_{}+d_{ }))\), as shown in Theorem D.2. We additionally apply the standard doubling trick to remove the requirement of knowing \(N\). More details can be found in Appendix D.2.

## 6 Discussion and Future Research

Improved bounds for the agnostic setting.An immediate direction for future research is tightening our bounds in the agnostic setting under known manipulation graph. Note that our upper bound is \((_{G}^{+}(+(,G))\) whereas the lower bounds are \((_{G}^{+})\) from Ahmadi et al. (2023) and \(((,G))\) from Theorem 3.2. The extra \(_{G}^{+}\) factor is introduced by the strategic learning-with-expert-advice algorithm, for which all known results have the dependency on \(_{G}^{+}\).

Randomized learners.Our results mainly focus on deterministic learners. It is an important open problem to find the corresponding characterizations for randomized learners. In Appendix E, we provided a family of realizable instances that witnesses a super-constant gap between the optimal mistake of deterministic and randomized algorithms. This is in contrast to their classical counterparts which are always a factor of \(2\) within each other. One challenge (among others) of proving a tight lower bound in the randomized setting is controlling the learner's information about the agents' original features, as the adversary can no longer "look-ahead" at an algorithm's future classifiers.

**Acknowledgments.** We thank Avrim Blum for the helpful comments and discussions. This work was done while Hanrui Zhang was in residence at the Simons Laufer Mathematical Sciences Institute (formerly MSRI) in Berkeley, California, during the Fall 2023 semester. This work was supported in part by the National Science Foundation under grants DMS-1928930, CCF-2212968, and ECCS-2216899, by the Alfred P. Sloan Foundation under grant G-2021-16778, by the Simons Foundation under the Simons Collaboration on the Theory of Algorithmic Fairness, and by the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003. The views expressed in this work do not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. Approved for public release; distribution is unlimited.