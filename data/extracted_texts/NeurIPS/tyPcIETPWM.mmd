# Conditional Outcome Equivalence: A Quantile Alternative to CATE

Josh Givens

University of Bristol

josh.givens@bristol.ac.uk

Henry W J Reeve

University of Bristol

henry.reeve@bristol.ac.uk

Song Liu

University of Bristol

song.liu@bristol.ac.uk

Katarzyna Reluga

University of Bristol

katarzyna.reluga@bristol.ac.uk

###### Abstract

The conditional quantile treatment effect (CQTE) can provide insight into the effect of a treatment beyond the conditional average treatment effect (CATE). This ability to provide information over multiple quantiles of the response makes the CQTE especially valuable in cases where the effect of a treatment is not well-modelled by a location shift, even conditionally on the covariates. Nevertheless, the estimation of the CQTE is challenging and often depends upon the smoothness of the individual quantiles as a function of the covariates rather than smoothness of the CQTE itself. This is in stark contrast to the CATE where it is possible to obtain high-quality estimates which have less dependency upon the smoothness of the nuisance parameters when the CATE itself is smooth. Moreover, relative smoothness of the CQTE lacks the interpretability of smoothness of the CATE making it less clear whether it is a reasonable assumption to make. We combine the desirable properties of the CATE and CQTE by considering a new estimand, the conditional quantile comparator (CQC). The CQC not only retains information about the whole treatment distribution, similar to the CQTE, but also having more natural examples of smoothness and is able to leverage simplicity in an auxiliary estimand. We provide finite sample bounds on the error of our estimator, demonstrating its ability to exploit simplicity. We validate our theory in numerical simulations which show that our method produces more accurate estimates than baselines. Finally, we apply our methodology to a study on the effect of employment incentives on earnings across different age groups. We see that our method is able to reveal heterogeneity of the effect across different quantiles.

## 1 Introduction

In many real world scenarios such as personalised treatment allocation and individual level policy decisions, understanding the effect of a treatment/intervention at an individual level is invaluable in providing bespoke care. The field which aims to understand a treatment's effect given certain covariates is referred to as heterogeneous treatment effect (HTE) estimation and has seen popularity across many applications [11; 10; 24; 20]. Within HTE, the conditional average treatment effect (CATE) has proved itself to be a popular target of study in this area due to its simplicity and interpretability [2; 13; 28]. A key limitation of the CATE however, is that it fails to paint a full picture of the differences between distributions of the two responses. In addition, it can be sensitive to outliers, with extreme values leading to a biased outcome. As such, the conditional quantile treatment effect (CQTE), an estimand which compares the conditional quantiles of the distributions in the treated and untreated populations, has established itself as a popular alternative [1; 5; 26].

While the CQTE offers more information than the CATE and is more robust to outliers, it lacks some of the CATE's desirable estimation properties. Specifically, CQTE estimation involves estimating the quantile functions for the two marginal outcomes. This harms the estimation procedure in cases where estimation of marginal quantile functions is more challenging than estimation of the CQTE itself. An example of this is when the marginal quantile functions are less smooth as a function of the covariates than the CQTE. This aligns with a recurring idea within HTE estimation that the effect of a treatment may be simpler than the marginal outcomes. In contrast to the CQTE, there are many CATE estimators which aim to learn the CATE directly allowing them to exploit its relative simplicity. These include the X-learner , R-learner , and Doubly Robust (DR) learner . Before estimating the CATE, these procedures require estimation of intermediary estimands (nuisance parameters) which condition on the covariates such as the average marginal outcomes and the propensity score (the probability of being assigned to treatment group). These nuisance parameters are then used to aid the estimation of the CATE. With the DR learner specifically, it has been shown that it can still achieve optimal convergence rates even when estimation of the nuisance parameters is worse than estimation of the CATE itself. This notion is referred to as **double robustness**, as our estimation is robust to sub-optimal estimation of both of the nuisance parameters.

Some attempts have been made to improve CQTE estimation [34; 33] with a key work being that of Kallus and Oprescu . In this they provide an extension of the double robustness property to the CQTE, creating an estimation procedure that can achieve strong convergence even when nuisance parameters are more difficult to estimate. Unfortunately, one of the nuisance parameters which must be estimated is the reciprocal of the conditional densities over the response. These are highly difficult to estimate and risk the errors blowing up in low density regions which could potentially nullify the desirable estimation rates they achieve even with the dependence on the estimation accuracy of these nuisance parameters being less strong. Furthermore it is still unclear how one can interpret relative smoothness in the CQTE compared to the individual quantiles with their being relatively little discussion of this within the literature. In general there is a distinct lack of illustrative examples; which are present for the CATE. To our knowledge no other works specifically aim to tackle this double robustness phenomenon for the CQTE or other quantile based treatment effect estimands.

We introduce a novel estimand called the "conditional quantile comparator" (CQC). The CQCE gives the outcome for a treated individual in the same quantile as a given outcome for an untreated individual, conditional on covariates. This relates to the conditional Quantile-Quantile (QQ) plot for the treated and untreated outcomes as demonstrated in Figure 1.

Similarly to the CQTE, our new estimand, the CQCE, allows us to compare equivalent quantiles while working exclusively in the response landscape, making it a more interpretable tool. This allows us to construct canonical examples of the CQCE being smoother than various nuisance parameters, the CQTE, and the CATE; adding to this interpretability. In addition, using the pseudo-outcome framework presented in Kennedy , we can leverage CATE estimation procedures to estimate the CQC in a doubly robust way, as mentioned

Figure 1: The left plot gives the CQC surface which takes in covariates (\(x\)) and an untreated response (\(y\)) and returns the treated response of the equivalent quantile (\(g^{*}(y|x)\)). The right plot is a QQ-plot of the responses (\(Y\)) in the untreated (\(A=0\)) vs treated (\(A=1\)) population conditional on various covariates (\(X=0,0.5,1\)). These conditional QQ-plots correspond to “slices” of the CQC surface, as shown by the coloured lines in the left plot. The plot is best viewed in colour.

above. Crucially, the CQC can keep the valuable quantile-level information previously offered by the CQTE while building on much of the CATE literature to acquire its desirable robustness properties and interpretability. Our contributions are as follows:

* Introduce a new estimand for HTE analysis: the conditional quantile comparator (CQC).
* Propose an estimation procedure which we prove to be doubly robust.
* Demonstrate better estimation accuracy especially when the CQC is smooth but individual conditional cumulative distribution functions are not.
* Provide insights into real-world datasets on employment intervention and medical treatment.

## 2 Set-up

We now introduce the standard HTE set-up in our notation. Let \(Z\) denote the random triple \((Y,X,A)\) with \(Y\) a random variable (RV) on \(\), \(X\) a RV on \(^{d}\), and \(A\) a RV on \(\{0,1\}\). We treat \(Z\) as representing an individual and interpret the components as

\[Y: X: A: \]

**Remark 1**.: _We could view our setting as coming from a potential outcome framework . Under this framework we assume there exists RVs \(Y_{0},Y_{1}\) on \(\) representing the outcome with and without treatment and that \(Y Y_{A}\). \(Y_{1-A}\) would then be unobserved/unknown for each individual._

We define the _propensity score_\(:(0,1)\) by

\[()(A=1|X=),\]

in other words, \(\) denotes the conditional probability of being assigned to treatment given the covariates. We shall assume that \(\) is continuous and bounded away from \(0\) and \(1\). From a potential outcomes perspective, this means that each individual could potentially be assigned to either treatment. We also define

\[F_{a}(y|) (Y y|X=,A=a),\] (1) \[F_{a}^{-1}(|) \{y|F_{a}(y|)\},\] (2)

and refer to them as the _conditional cumulative distribution function (CCDF)_ and the _quantile function_ respectively. We also refer to \(F_{a}^{-1}\) in (2) as the _generalised inverse_ of \(F_{a}\).

We can now define the CATE and CQTE to be given by \(:\) and \(_{q}:\) with

\[() [Y|X=,A=1]-[Y|X=,A=0],\] \[_{q}(|)  F_{1}^{-1}(|)-F_{0}^{-1}(|).\]

We let \(D\{Z_{i}\}_{i=1}^{2n}\{(Y_{i},X_{i},A_{i})\}_{i=1}^{2n}\) for \(n\) be IID copies of \(Z\) representing our data sample with \(i\) indexing the individual. We assume an even number of samples for notational convenience. For \(a\{0,1\}\), we take \(I_{a}\{i|A_{i}=1\}\), the indices of individuals on treatment \(a\). We can then define \(D_{a}\{Z_{i}\}_{\{i I_{a}\}}\) and \(n_{a}|I_{a}|\) as the dataset and sample size of those on treatment \(a\).

For \(n\), let \([n]\{1,,n\}\). For a vector \(^{p}\) let \(w_{j}\) to represent the \(j^{}\) component of \(\) and let \(\|\|\) be the Euclidean norm unless otherwise specified. We also take \(\|\|_{1}\) as the \(1\)-norm and \(\|\|_{}_{j[p]}|w_{j}|\). We keep a summary table of all notation used in Appendix A.1.

### Introducing the quantile comparator

Our aim is to find "equivalent quantiles" between the treated and non-treated distributions conditional on the covariates. Specifically, for each \(y_{0}\), \(\) we aim to find \(y_{1}\) such that

\[F_{1}(y_{1}|)=F_{0}(y_{0}|).\]

This now allows us to define our primary estimand of interest, the _conditional quantile comparator_.

**Definition 1** (Conditional quantile comparator (CQC)).: _For our triple \((Y,X,A)\), the conditional quantile comparator is the measurable function \(g^{*}:\) such that, for all \(y,\ \),_

\[F_{1}(g^{*}(y|)|)=F_{0}(y|).\]We then simply define \(y_{1}\) as \(g^{*}(y_{0}|)\). The name conditional quantile comparator derives from the fact that it returns the value of \(y_{1}\) in the equivalent quantile of \(Y|X=,A=1\) as the quantile of \(Y|X=,A=0\) that \(y_{0}\) is in.

**Remark 2**.: _For simplicity and to ensure such a function is well defined, we will assume that \(Y|X=,A=a\) is a continuous RV for any given \(,\ a\{0,1\}\) with strictly positive density on its support. We will however allow the support of \(Y|X=x,A=a\) to vary in both \(\) and \(a\)._

We now introduce another estimand which will serve as a useful stepping stone in our estimation.

**Definition 2** (CCDF contrasting function).: _The CCDF contrasting function is defined to be \(h^{*}:[-1,1]\) given by_

\[h^{*}(y_{0},y_{1}|) F_{1}(y_{1}|)-F_{0}(y_{0}|).\] (3)

This estimand allows following alternative definitions for \(g^{*}\) which help its interpretation and estimation (detailed later). We take \(h^{*-1}\) representing the inverse of \(h^{*}\) with respect to the \(2^{}\) argument.

\[g^{*}(y_{0}|)=h^{*-1}(y_{0},0|)=F_{1}^{-1}(F_{0}(y_{0}|)| ).\] (4)

The second equality still holds if we replace the inverses in the above with generalised inverses. The equality (4) falls straight from the definition of each object and shows how we can use \(h^{*}\) to estimate \(g^{*}\). Moreover, the equality (4) allows us to generalise \(g^{*}\) to discontinuous \(Y\) (or pdfs with non-trivial \(0\) density regions inside the support).

### Exploring the CQC

We specifically focus on the CQC, \(g^{*}\), as we feel it gives insightful information allowing comparison of the two distributions (\(Y|X,A=1\)) and (\(Y|X,A=0\)).

The CQC allows us to compare the two distributions beyond simply a single point estimate such as that given by the CATE. This is especially valuable in cases where the two distributions differ beyond just a shift. For example, the effect of some treatments varies greatly between individuals with the same or similar covariates. An example of this is antidepressants, where some patients respond positively while others may have adverse reactions leading to a worse outcome than no treatment whatsoever. Another example is the use of opioids as painkillers where some patients have an increased tolerance making them less effective [12; 22].

As well as being of interest on its own, the quantile comparator relates closely to other estimands of interest. For example, if we take \(^{*}(y_{0}|) g^{*}(y_{0}|)-y_{0}\) then \(^{*}\) tells us whether the equivalent quantile in the treated distribution is higher or lower. This estimand then serves as a heuristic for whether the treatment is beneficial at that untreated response value.

Furthermore, CQTE can be written as

\[_{q}(|)=^{*}(F_{Y|X,A=0}^{-1}(|) )=g^{*}(F_{Y|X,A=0}^{-1}(|) )-F_{Y|X,A=0}^{-1}(|),\] (5)

linking the quantile comparator back to the CQTE. This equivalence highlights the perspective that the CQC can be seen as rephrasing the input of the CQTE in terms of the outcome space.

A key idea within CATE literature is the notion that the CATE itself may be a simpler estimand to study than the marginal treatment outcomes (\([Y|X=,A=a]\)) may be individually. One can exploit this feature to improve the CATE's estimation. A similar concept exists with the CQC as we will see in the example below.

**Example 1** (Illustrative Example).: _Suppose that_

\[Y|X=x,A=0((10x),\ 1^{2}),\ \ \ \ \ \ \ \ Y|X=x,A=1(2(10x),\ 2^{2}).\]

_Then we have \(g^{*}(y|x)=2y\) which does not depend on \(x\) and does not include the sine term present in the individual CDFs. Interestingly, \([Y|X,A=1]-[Y|X,A=0]=(x)\) hence the CATE is still non-constant in this case (the same also holds for the CQTE). Additionally, we have \(^{*}(y|x)=g^{*}(y|x)-y=y\) suggesting the intervention is beneficial for positive \(y\) and detrimental for negative \(y\). We now show 3D plots of a CCDF, the CQC, and the CQTE in Figure 2._

**Example 2** (General Smoothness Case).: _Suppose we are in the potential outcomes framework so that \(Y_{0},Y_{1}\) exist with \(Y Y_{A}\). Now also suppose that \(Y_{1}=(Y_{0},X)\) for some transformation \(\) increasing in \(Y_{0}\) for each \(X\). Then \(\) gives the CQC (i.e. \(=g^{*}\)) meaning that smoothness of the CQC can be seen as smoothness of \(\). This gives a generalisation of the CATE case where smoothness is present when \(Y_{1}=Y_{0}+(X)\) with \(\) smooth_

_A specific example could be a treatment which halves all individuals blood pressure. In this case \(^{*}(y,)=g^{*}(y|)=y\) and so the CQC is smooth but the CQTE and CATE would not be if the individual responses are non-smooth._

## 3 Estimation procedure

We now describe our estimation procedure for the CQC which is motivated by equation (4). At a high level our approach for estimating \(g^{*}(y_{0}|)\) will be the following:

* a specified proxy response computed from \((Y,X,A)\) which we will regress against.
* Find the value \(_{1}\) which makes our estimate of \(h^{*}(y_{0},|)\) closest to \(0\).

Section 3.1 and Algorithm 1 give our \(h^{*}\) estimation procedure while Section 3.3 and Algorithm 2 give our \(g^{*}\) estimation procedure.

### Estimating the CCDF contrasting function \(h^{*}\)

We focus on estimating \(h^{*}\) primarily for its two nice properties:

1. Similar to \(g^{*}\), \(h^{*}\) can exhibit smoothness even when the individual CCDFs are not smooth.
2. The estimation of \(h^{*}\) can be re-framed as a CATE problem.

The first property is important as the smoothness of \(h^{*}\) determines the best estimation rate that can be achieved when using non-parametric regression, setting a target for our approach. In particular, smoother functions have better estimation rates. The second property is important as it gives us a method for attaining this target rate. By re-framing the estimation as a CATE problem, we can leverage existing results to build a robust estimator which achieves the target estimation accuracy rate even when the rate of estimating nuisance parameters is sub-optimal. We demonstrate this robustness later using finite sample bounds on the estimation accuracy (Proposition 1 & Theorem 2).

First, we show how the estimation of \(h^{*}\) can be solved using a CATE estimator. Note that

\[h^{*}(y_{0},y_{1}|)=[\{Y y_{1}\}|X=,A=1]- [\{Y y_{0}\}|X=,A=0].\]

Hence, for a given \(y_{0},y_{1}\), if we define the RV \(W_{y_{0},y_{1}}:=\{Y y_{A}\}\), then estimating \(h^{*}(y_{0},y_{1}|.)\) is equivalent to estimating the CATE with \(W_{y_{0},y_{1}}\) replacing \(Y\) as the response. To perform this estimation, we turn to a recent method developed by Kennedy . They propose to write the CATE as a conditional expectation of a function of \(Z\) called a pseudo-outcome. A robust estimator

Figure 2: Surface plots for CCDF (panel (a)), CQC (panel (b)) and CQTE (panel (c)). We can see that CCDF, and CQTE have high-frequency change in \(x\) while the CQC does not depend on \(x\).

is then obtained by regressing this pseudo-outcome against \(X\). In our setting, the pseudo-outcome with the new response \(W_{y_{0},y_{1}}\), for a sample \((y,,a)\) is given by

\[_{y_{0},y_{1}}(y,,a): =)}{()(1-())}\{\{y y_{a}\}-F_{a}(y_{a}|)\}+F_{1}(y_{1}|)-F_{0}(y_{0}| )\] (6) \[=)}{()(1-())}\{\{y y_{a}\}-F_{a}(y_{a}|)\}+h^{*}(y_{0},y_{1}|).\]

Since \(h^{*}(y_{0},y_{1}|)=[_{y_{0},y_{1}}(Z)|X=]\) (Proposition 5, Appendix C.1), regressing \(_{y_{0},y_{1}}(Z)\) on \(X\) provides an estimate for \(h^{*}\).

As we do not know the CDFs nor the propensity score, we need to replace them in (6) with estimates. We define \(,_{0},_{1}\) to be estimates of \(,F_{0},F_{1}\) respectively. We then construct \(_{y_{0},y_{1}}\) in the same way as \(_{y_{0},y_{1}}\), but using estimated quantities \(,\ _{a}\) instead. \(_{y_{0},y_{1}}\) can now serve as the pseudo-outcome in our regression. We also use sample splitting to de-correlate the propensity score and CDF estimates from the \(h^{*}\) estimate. This helps make our estimator doubly robust, as we will see in the following theory.

We are now ready to define our Doubly Robust (DR)-learner to estimate \(h^{*}\) in Algorithm 1.

```
0:\(y_{0},y_{1}\), Data \(D\), a regressor (e.g. linear smoother)
1: Define \([n]\), \(\{n+1,,2n\}\) and split \(D\) into \(D_{}\{Z_{i}\}_{i},D_{}\{Z _{j}\}_{j}\).
2: Using \(D_{}\) to estimate \(,_{0}(y_{0}|.),_{1}(y_{1}|.)\) by regressing \(\{A=1\},\{Y y_{0}\},\{Y y_{1}\}\) respectively against \(X\).
3: Use these estimates to obtain \((Z_{j})\) for \(j D_{}\) using equation (6)
4: Using \(D_{}\) to regress \(\) against \(X\) to obtain estimate \((y_{0},y_{1}|.)\) of \(h^{*}(y_{0},y_{1}|.)\). ```

**Algorithm 1** DR estimation procedure for the CCDF contrasting function \(h^{*}\)

**Remark 3**.: _Cross-fitting can be implemented by repeating the procedure with the roles of \(\) and \(\) switched and then averaging the two estimates of \(h^{*}\). We could also perform this procedure multiple times with different random splits of the data to improve our estimator's potential stability._

Note that our algorithm is not specific on which form of regression to use allowing for any parametric or non-parametric procedure. Further to this, it can also be easily adapted to use other pseudo-outcome procedures such as the R-learner of Nie and Wager  or a standard inverse propensity weighting approach which we describe in Appendix A.3.

### Finite sample bound of \(h^{*}\) estimator

In this section, we prove the estimation accuracy of \(\), which will play important roles in the following notation and assumptions we need for estimating \(g^{*}\). These accuracy statements will be made for an arbitrarily fixed \(\). For our theoretical and experimental results we use linear smoothers for the final regression. This means our estimate \(\) and oracle estimate \(h^{}\) are of the form

\[(y_{0},y_{1}|) =_{j}w_{j}_{y_{0},y_{1}}(Z_{j}) h^{}(y_{0},y_{1}|) =_{j}w_{j}_{y_{0},y_{1}}(Z_{j})\]

where the weights \(w_{j} w_{j}(,X_{})\) are constructed using \(X_{}\{X_{j}\}_{j}\) with \(w_{j} 0\) and \(\|\|_{1}\). Linear smoothers encompass a broad class of estimation techniques used in both low and high-dimensional settings. Examples include k-NN regression [8; 9], kernel ridge regression , generalised forests , and Mondrian forests . Additionally linear smoothers have been shown to adapt to intrinsic low dimensionality in regression problems in higher dimensions , making them an apt estimator for our purposes.

For \(:\) treated as deterministic and \(p>1\), we also define the norms

\[\|\|_{^{*}}\!^{s}\|_{1}}_{i }w_{j}^{s}\,[(Z)^{2}|X=X_{i}]}\]

and take \(\|\|_{}\!\|\|_{^{1}}\). Note that this norm is random as the weights depend upon \(X_{}\).

We now aim to show that we are able to exploit smoothness in \(h^{*}\) even when the CCDFs and propensity score are less smooth. We introduce the notion of smoothness through Holder functions.

**Definition 3** (Holder functions).: _We say that a function \(f:\) is \((,C)\)-Holder for \((0,1],C 1\) if for any \(^{},^{}\),_

\[|f(^{})-f(^{})| C\|^{}-^{ }\|^{}.\]

Here, larger \(\) represents a smoother function which can be estimated at faster rates.

**Assumption 1**.: _For any \(y_{0},y_{1},\;>0,a\{0,1\}\):_

1. _[label=()]_
2. _There exists_ \((0,1/2]\) _such that_ \((),(^{})[,1-]\) _for all_ \(^{}\)_._
3. _With probability at least_ \(1-\)_,_ \(\|_{y_{0},y_{1}}-_{y_{0},y_{1}}\|_{^{2}}\! _{}(n,)\)_._
4. _With probability at least_ \(1-\)_,_ \(\|-\|_{}\!\!_{}(n,)\) _and_ \(\|F_{a}(y_{a}|.)-_{a}(y_{a}|.)\|_{}\!_{}(n, )\)_._
5. _For_ \((0,1],\;C 1\)_,_ \(h^{*}(y_{0},y_{1}|.)\) _is_ \((,C)\)_-Holder._

Assumption 1(a) exists to ensure for any covariate value, neither treatment assignment has too low a probability. Assumption 1(b) controls the convergence of the estimated pseudo-outcome to the true pseudo-outcome. Assumption 1(c) sets up the smoothness of the propensity score and CCDFs alongside the convergence rates of their estimators as \(_{},_{}\) respectively. Assumption 1(d) sets up the smoothness of \(h^{*}\) which will control the convergence rate of the oracle estimation procedure. Assumptions 1 (c) and (d) control the accuracy of our estimator in the following result.

**Proposition 1**.: _Suppose that Assumption 1 holds and let \(\) be a linear smoother estimated as in Algorithm 1. Then for any \(y_{1},y_{0},\;(0,2/e]\) and our \(\), with probability at least \(1-\),_

\[|(y_{0},y_{1}|)-h^{*}(y_{0},y_{1}|)| _{h}(n,).\]

_Here, for each \((0,2/e]\) we have_

\[_{h}(n,) \,_{}(n, /4)+_{}(n,/4)_{}(n,/4)+ _{}(n,/4),\] \[_{}(n,) |[h^{}(y_{0},y_{1}|)-h^{*}(y_{0},y _{1}|)|X_{},D_{I}]\] \[+\,\|\|\|-h(y_{0},y_{1} |.)\|_{^{2}}\!+\!2\|\|_{}\!(2/)/(3).\]

We have that \(_{}\) gives an upper bound on the accuracy of the oracle estimation and so acts as a target for our estimation procedure. If \(_{}(n,) 0\) then the first term in \(_{h}\) is guaranteed to be \(o(_{}(n,))\) for fixed \(\). Hence the first and last terms converge at oracle rates with respect to \(n\). As the \(_{}\) and \(_{}\) terms are multiplied together we can obtain better rates than either of them individually have. This is because both \(_{}\) and \(_{}\) can converge to \(0\) slower than \(_{}\) while their product \(_{}_{}\) converges quicker. This provides the desired double robustness as our estimation can converge at oracle rates even when convergence for the nuisance parameters is slower. Now that we have this we can convert our estimate of \(h^{*}\), into an estimate of \(g^{*}\).

### Estimating the conditional quantile comparator \(g^{*}\)

In order to obtain an estimate of \(g^{*}(y_{0}|)\) at a fixed \(y_{0},\), we need to obtain estimates of \(h^{*}(y_{0},y_{1}|)\) at various values of \(y_{1}\). As \(h^{*}\) is monotonic, we would then like to search for a monotonic function which aligns with these estimates. This monotonicity is especially important because it allows us to bound the estimation accuracy of \(\) uniformly over all \(y_{1}\) at a similar rate to our pointwise accuracy. With this, we can easily translate the estimation accuracy in \(\) (obtained in proposition 1) into the accuracy in \(\). Additionally, it simplifies the process of inverting \(\). In general our method in Algorithm 1 will not produce monotonic \(\) (this is in contrast to some other approaches such as an IPW pseudo-outcome, see Appendix A.3), or separately estimating the CCDFs). We can however obtain a monotonic estimate of \(h^{*}\) using isotonic projection.

**Definition 4** (Isotonic Projection).: _We define the isotonic projection of \(^{}^{p}\) as follows:_

\[P(^{})*{argmin}_{(p)}\|-^{}\|\]

_where \((p)\{^{p}|_{j}_{l+ 1}\; l[p-1]\}\), the set of all isotonic vectors in \(^{p}\)._

**Remark 4**.: _We can use the Pool Adjacent Violators Algorithm (PAVA)  which performs isotonic projection and is implemented in the IsotonicRegression class of sci-kit learn in Python ._

Hence, for a fixed value of \((y_{0},)\) and a set of predictions \(_{l}=(y_{0},y_{1}^{(l)}|)\) with \(y^{(l)} y^{(l+1)}\), we can take \(}:=P_{(p)}(})\) and use these to obtain a new monotonic estimate of \(h^{*}\). Furthermore, by a result in Yang and Barber , \(}\) will be at least as accurate as \(}\) in the worst case. We now describe our approach for estimating the CQC using this projection approach in Algorithm 2.

```
0: Data \(D\); test point \((y_{0},)\); sorted evaluation points \(\{y^{(l)}\}_{l=1}^{p}\)
1: Apply Algorithm 1 to obtain estimate of \(h^{*}\) given by \(\).
2: Define \(_{l}(y_{0},y^{(l)}|)\) for \(l[p]\).
3: Isotonically project \(}\) using PAVA to obtain \(}\) with \(_{i}_{i+1}\).
4: Take \((y_{0}|) y^{(l^{*})}\) with \(l^{*}*{argmin}_{l[p]}|_{l}|\). ```

**Algorithm 2** DR estimation procedure for the CQC \(g^{*}\)

**Remark 5**.: _For the case where \(\) is a step function, these steps can serve as our evaluation points while for continuous \(\) one could take these candidate \(y_{1}\) points at small evenly spaced intervals. Empirically we also find that \(\) is already close to isotonic and so step 3. of the algorithm is mostly for the theoretical justification of our approach._

While it may seem inefficient to be estimating the CQC via the CCDF contrasting function and then inverting, due to the monotonicity of \(h^{*}\), we actually pay a very small cost in estimation accuracy for having to estimate the CCDF contrasting function over all \(y_{1}\). We make this notion more explicit in the following section.

### Finite sample bound of the CQC estimator

We now provide the accuracy of our estimate \(\) obtained by Algorithm 2 when used in conjunction with linear smoothers. We assume that \(_{a}\) are also fit using linear smoothers of the form

\[_{a}(y|^{},a)=_{i}w_{F_{a};i}(^{ };X_{},A_{})\{Y_{i} y\}\]

with \(w_{j}(^{}) w_{j}(^{},X_{},A_{})>0,\|(^{})\|_{1}=1\). We will also require the following assumptions.

**Assumption 2**.: _For our RV \(X\), any \(y,\ ^{}\), \(<e^{-1}\):_

1. _There exists some_ \(s,>0\) _such that_ \(F_{1}(y^{}|)\) _for all_ \(y^{} B_{s}(g^{*}(y|))\)_._
2. _W.p. at least_ \(1-\)_,_ \(_{j}w_{j}_{}(n,)\) _and_ \(_{i}w_{F_{a};i}(X)_{}(n,)\)_._

Assumption 2(a) is a mild assumption which allows us to convert the \(\) accuracy into \(\) accuracy while Assumption 2(b) bounds the rates of decay of the weights in our linear smoothers.

**Theorem 2**.: _Let \(\) be estimated as using Algorithm 2 with \(\{Y_{i}\}_{i I_{1}}\), sorted and then used as our evaluation points and linear smoothers used for regressions in Algorithm 1. Then provided Assumptions 1 & 2 hold we have that for \((0,e^{-1})\) and sufficiently large \(n\), w.p. at least \(1-\),_

\[|(y|)-g^{*}(y|)| 2(^{-1}_{h}(n, /(2n))+^{-1}_{}(n,/(2n))).\]

From this result we see that if our weights decay at rate faster than \(_{h}(n,)\) then this error will be dominated by the \(_{h}\) term. We believe this to hold in most cases and show that it does comfortably when using Nadaraya-Watson (NW) estimation  with a box kernel in Appendix C.4. Furthermore, if the dependence on \(\) in both terms is of the form \(^{c}(1/)\) for some \(c>0\) then we obtain the same rate of estimation as for \(h^{*}\) up to polylog factors. This means we translate our desirable double robustness \(\) over to \(\). We also obtain finite sample bounds on \([|(Y|)-g^{*}(Y|)|\  A=0,]\) with high probability and present this in Appendix C.3.

## 4 Numerical experiments

We now apply our approach to a series of simulated and real data scenarios in order to demonstrate the utility of our estimand and the effectiveness of our estimation procedure. For these, we use NW estimation as our regression procedure throughout. See appendix A.2 for details on NW estimation.

### Simulated experiment

In this section, we test our method's performance in terms of our estimator's mean absolute error under simulated scenarios.1 In each scenario we test against a separate estimator which estimates the two CCDFs separately and simply takes their difference, an IPW pseudo-outcome estimator detailed in Appendix A.3, the CQTE estimator of Kallus and Oprescu , and the oracle DR estimator where \(\) is replaced with \(\) (i.e. exact \(,F_{a}\) are used). In this experiment, we return back to the set-up of example 1. We now change the frequency of the sine term by taking

\[Y|X=x,A=0 (( x),\;1^{2}), 28.452756ptY|X=x,A=1 (2( x),\;2^{2}),\] \[(x) =0.4( x)+0.5.\]

for \(\) so that increasing \(\) imitates decreasing smoothness of our nuisance parameters. In our experiments half the samples are used to estimate the propensity score and CCDFs and the other half are used to regress against the pseudo-outcome. Our estimate \(\) is then compared against \(g^{*}\) using a hold-out testing set. This process is repeated 500 times with new training data on each run. From this, a Monte-Carlo estimate of \(_{}[_{X}|_{Y|X,A=0}[|(Y|X)-g^{*}( Y|X)|]]\) is produced alongside 95% confidence intervals (CIs). In our first experiment, we let \(2n=1000\) and vary \(\) in \(\). In our second experiment, we let \(=6\) and \(2n\) vary in \(\). The results of this are shown in Figure 3.

We can see that the average error decreases as the sample size \(2n\) grows and mildly increases as \(\) grows. This is expected as increasing \(\) makes estimating the nuisance parameters more challenging. The result shows that the proposed DR method achieves the best performance compared with the Separate and IPW estimators and is only marginally outperformed by the oracle estimator. Additionally we see that the method of Kallus and Oprescu  is much more affected by the increase in \(\). This is because unlike the CQC, the CQTE has a complexity that depends on the frequency term (\(\)). We also observe much better performance as the sample size increases. We hypothesise that the plateau in the CQTE approach is due to the difficulty of estimating the reciprocal of the PDF, which causes the estimation to be unstable irrespective of sample size. Further simulated experiments including 10-dimensional \(\) and linear CQC are given in Appendix B.1.

### Real world employment example

To show the performance in the real-world scenarios, we use a dataset on an employment programme which has been studied in various prior works [4; 5; 26]. Within the programme, some participants were given job placements or temporary help jobs while others received no intervention. Participants' earnings were then monitored over the next 8 quarters following their enrollents. We take their net earnings as our response (\(Y\)) and the employment intervention as the treatment (\(A=1\)). We use each participant's age at their entry to the study as our covariate (\(X\)). We fit our quantile comparator function on 2,000 participants. Figure 4 shows our estimate of \(^{*}(y|)=g^{*}(y|)-y\) for various values of \((y,)\).

We see that participants around age 23 and between ages 32-37 benefit most from this scheme, as indicated by the darker colour on the heat map. Additionally, the lower quantile of the income distribution (wage \(\) $7500) shows the least change, indicating that wage improvements primarily occur

Figure 3: Mean absolute error with 95% CIs for various estimators. The left plot has fixed sample size (\(2n=1000\)) and increasing \(\). The right plot has \(=6\) and increasing sample size.

for the higher income group. For participants aged 40, there appears to be little change in outcomes overall. To demonstrate our approach in a medical setting, we apply our method to evaluate the effectiveness of a colon cancer treatment, as detailed in Appendix B.3. For comparative purposes, we also provide an estimate of the CQTE for this data in Appendix B.2.

## 5 Limitations

The theory provided here gives a strong foundation and motivation for framing our problem in this particular manner. There is however a great deal more to be explored in this area from a theoretical perspective. For example, one immediate improvement would be to give a more general case where our weight decay (in Assumption 2 (b) for Theorem 2) is sufficiently fast. In addition more work needs to be done exploring the relationship between the smoothness of \(g^{*}\) and the smoothness \(h^{*}\). Smoothness in \(h^{*}\) appears to be a stronger condition so ideally we would like to make theoretical statements directly on the smoothness of \(g^{*}\). Interestingly our experiments on synthetic data do seem to suggest that it is the complexity of \(g^{*}\) which drives the estimation rate as in these experiments \(h^{*}\) increases in complexity while \(g^{*}\) remains constant. Additionally, changing our experiment in Section 4.1 to have uniform response so that both \(h^{*},g^{*}\) are constant rather than just \(g^{*}\), seems to give no material improvement to the performance of our estimator (see Appendix B.1.3).

Another limitation of our current estimation procedure is that it requires learning an estimand and then inverting it to obtain our final estimator. While this process is relatively simple, it could be streamlined and made more computationally efficient if we could produce a more direct estimator similar to the DR-Learner for CATE  or the CQTE estimator in Kallus and Oprescu .

Finally, while we have been able to provide more concrete examples of smoothness for the CQC these are still limited to the case of a deterministic treatment effect which we would like to expand upon. This is closely related to a more general limitation with quantile-based estimands, the CQC included, in that they lack meaningful interpretability for the individual. While the CATE can be viewed as the expected difference in an individual's outcome on and off the treatment, no such individual-level interpretation exists for the CQC or indeed any other estimand trying to learn higher level distributional information than the mean. As a result the CATE is still a more naturally interpretable estimand. To facilitate this interpretation however, one still needs to make assumptions about a lack of confounding between the treatment assignment and the potential outcomes, which are only verifiable in certain restrictive scenarios .

## 6 Conclusion

In this paper we have introduced a new treatment effect estimand, the conditional quantile comparator and demonstrated its efficacy both in terms of its doubly robust estimation, and its ability to provide valuable data insights. This is a promising direction as it allows quantile-based treatment effect exploration to "keep up" with the CATE in terms of estimation quality offering more flexibility as to which estimand can be used to best describe the data. For these reasons, we see the CQC as an exciting and worthwhile new direction within the HTE framework.

Figure 4: Surface and heat plot of \(^{*}(y|)\) for our employment data with \(X=\)Age, \(Y\)=Income.