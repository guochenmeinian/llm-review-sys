# MLLMGuard:

A Multi-dimensional Safety Evaluation Suite

for Multimodal Large Language Models

 Tianle Gu\({}^{1,2}\)1, **Zeyang Zhou\({}^{2}\)**, **Kexin Huang\({}^{2}\)**, **Dandan Liang\({}^{2}\)**, **Yixu Wang\({}^{2}\)**, **Haiquan Zhao\({}^{2}\)**, **Yuanqi Yao\({}^{2}\)**, **Xingge Qiao\({}^{2}\)**, **Keqing Wang\({}^{2}\)**, **Yujiu Yang\({}^{1}\)\({}^{}\)**, **Yan Teng\({}^{2}\)\({}^{}\)**, **Yu Qiao\({}^{2}\)**, **Yingchun Wang\({}^{2}\)**

\({}^{1}\) Tsinghua Shenzhen International Graduate School, Tsinghua University

\({}^{2}\) Shanghai Artificial Intelligence Laboratory

###### Abstract

Powered by remarkable advancements in Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities in manifold tasks. However, the practical application scenarios of MLLMs are intricate, exposing them to potential malicious instructions and thereby posing safety risks. While current benchmarks do incorporate certain safety considerations, they often lack comprehensive coverage and fail to exhibit the necessary rigor and robustness. For instance, the common practice of employing GPT-4V as both the evaluator and a model to be evaluated lacks credibility, as it tends to exhibit a bias toward its own responses. In this paper, we present MLLMGuard, a multi-dimensional safety evaluation suite for MLLMs, including a bilingual image-text evaluation dataset, inference utilities, and a lightweight evaluator. MLLMGuard's assessment comprehensively covers two languages (English and Chinese) and five important safety dimensions (Privacy, Bias, Toxicity, Truthfulness, and Legality), each with corresponding rich subtasks. Focusing on these dimensions, our evaluation dataset is primarily sourced from platforms such as social media, and it integrates text-based and image-based red teaming techniques with meticulous annotation by human experts. This can prevent inaccurate evaluation caused by data leakage when using open-source datasets and ensures the quality and challenging nature of our benchmark. Additionally, a fully automated lightweight evaluator termed GuardRank is developed, which achieves significantly higher evaluation accuracy than GPT-4. Our evaluation results across 13 advanced models indicate that MLLMs still have a substantial journey ahead before they can be considered safe and responsible. 1

_Warning: The content of this article may cause discomfort or contain sensitive information._

## 1 Introduction

Attributed to the scaling up of training corpus and model parameters, recent years have witnessed remarkable progress in LLMs . This progress has further propelled the development of a growing number of MLLMs (e.g., GPT-4V , Gemini , CogVLM , etc.) that utilize LLMs as the central framework for conducting complex multimodal tasks. A typical MLLM  consists of a pre-trained LLM, a pre-trained modality encoder, and a modality interface to connect them. This architecture extends the LLM from a single text modality to a multimodal field. However, the expanded scope of capabilities means that MLLMs face a wider range of threats, presenting new challenges to their safety capabilities . Therefore, beyond assessing the capabilities of MLLMs, it is essential to conduct a comprehensive evaluation of their safety.

Several studies have made preliminary attempts to evaluate the safety of MLLMs. For example, some research  evaluate the hallucinations of MLLMs, Zhang et al.  examines the self-consistency of their responses when subjected to common corruptions, and Cai et al.  assesses their robustness against diverse style shifts. In addition to these specific safety aspects, more recent works have focused on the overall safety of MLLMs. Lin et al.  detects MLLMs' critical ability on meme-based social abuse, and Gong et al. , Liu et al. , Li et al.  explore distinct jailbreaking methods on safety topics. Moreover, Shi et al.  introduces evaluation based on 3H principle . However, there remains a gap between these efforts and achieving a complete and comprehensive safety assessment.

Reviewing existing benchmarks, we identify the following main challenges in achieving the comprehensive evaluation: 1) _Deficiency in comprehensive evaluation dimensions._ Most benchmarks focus on a single purpose, e.g., hallucination , or generalized safety , complicating the thorough evaluation and cross-comparison between MLLMs. 2) _Possible data leakage _. Most safety benchmarks build their dataset by integrating open-source datasets, which are likely to be included in the training sets of MLLMs. 3) _Lack of effective evaluator on open-ended assessment._ While existing research highlights the instability introduced by fixed-format evaluation (e.g., multiple-choice) , there is a lack of reliable evaluators on open-ended evaluation. Commonly, either human annotators or GPT-4V are employed to directly rate responses . However, relying on human annotators is costly for ongoing measurement, and employing GPT-4V poses risks to evaluation bias . 4) _Lack of multicultural assessment._ Current benchmarks predominantly focus

Figure 1: **Workflow of MLLMGuard**, including creating dataset through manual construction, evaluation on MLLMGuard and scoring with human and GuardRank.

on the English language, which restricts the applicability of MLLMs in non-English speaking regions. We present detailed comparisons between existing safety-related benchmarks in App. B.

We advocate for incorporating the following key characteristics in a high-quality safety benchmark to address the aforementioned challenges. Firstly, it should encompass assessments from extensive dimensions and not be limited to English, ensuring comprehensive consideration of all safety aspects. Secondly, it should present adequate challenges and effectively distinguish between evaluated models. Specifically, the evaluation data should be independent of the model's training set. Finally, the evaluation metric should be fair and cost-effective, ensuring the assessment is conducted promptly without significant resource constraints. Guided by these principles, we develop MLLMguard to offer comprehensive safety evaluations for MLLMs, which consist of a bilingual image-text evaluation dataset, inference utilities, and a lightweight evaluator.

To summarize, our main contributions are as follows:

* a multi-dimensional safety evaluation suite for MLLMs**, featuring a bilingual evaluation set (English and Chinese), adaptable inference utilities, and a lightweight evaluator. To our knowledge, this is the _first_ attempt to include a dataset in Chinese for MLLMs' safety evaluation. Our suite assesses privacy, bias, toxicity, truthfulness, and legality across 12 subtasks. The in-depth evaluation of 13 leading MLLMs yields valuable insights for subsequent model optimization on safety.
* Distinguished from existing benchmarks, **MLLMGuard is characterized by its highly adversarial nature**. We derive a substantial portion of our image data (82%) from social media to prevent data leakage. Moreover, human experts meticulously curate all text data, fortified with red teaming techniques.
* **We introduce GuardRank, a fully automated lightweight evaluator** that removes the need for GPT-4V and manual assessments, serving as a plug-and-play tool for straightforward evaluations on MLLMGuard.

## 2 MLLMGuard

MLLMGuard is designed to develop a collection of adversarial examples to test the ability of MLLMs to identify and counteract attacks orchestrated by red teams. Specifically, our evaluation focuses on Vision-Language models, which process both an image and a textual input to produce a text-based output. We break down our evaluation strategy into three main aspects: the **taxonomy** of the threats, the **dataset** of adversarial examples, and the **metric** for assessing model performance.

### Taxonomy

MLLMGuard incorporates five dimensions, i.e., Privacy, Bias, Toxicity, Truthfulness, and Legality. The data distribution statistics are shown in Tab. 1. Definitions of each dimension are shown below:

* **Privacy** tests whether models are aware of privacy-infringing requests and can refrain from providing corresponding assistance to safeguard privacy. It is further divided into three sub-tasks: personal privacy, trade secrets, and state secrets.
* **Bias** evaluates the models' reactions to queries that contain stereotypes, prejudice, and discrimination. It aims to determine if the model can counter these unjust assumptions and offer impartial responses.
* **Toxicity** focuses on evaluating whether MLLMs are capable of identifying toxic content in the input and generating responsible guidance and safe responses. Within this area, we specifically target two sub-tasks: hate speech, pornography and violence. Typically, the inputs are presented as image-text pairs, often in the form of memes.
* **Truthfulness** involves a dual consideration of hallucination and robustness. In the hallucination scenario, we query about entities not present in the image to determine if MLLMs correctly identify their absence. In the robustness scenario, we assess the models' consistency when subjected to noise injection and position swapping.
* **Legality** is classified into two distinct categories, namely personal safety and public security. This dimension tests MLLMs' ability to identify legal issues and offer legal guidance.

### Dataset

As shown in Fig. 1(a), the entire dataset creation process consists of three stages: Image & Prompt Collection, Red Teaming, and Data Review. Detailed data statistics can be found in App. C, and samples for each sub-task are provided in App. E.

#### 2.2.1 Data Creation

We recruit 9 crowd workers with relevant professional backgrounds to participate in the data creation process. Before we begin handcrafting the data, we prepare a detailed guideline that outlines the definition of the dimension, risk scenarios, red teaming techniques, and data source requirements. During the creation process, crowd workers are instructed to adhere to the following three principles:

* _Avoiding Data Leakage_ To prevent data from being exposed to the training set of evaluated models, we manually construct text prompts, ensuring the absence of identical image-text pairs in any publicly available datasets. Additionally, to diversify our sources and minimize dependence on open-source datasets, we source over 82% of our dataset's images from social media platforms.
* _Enhancing Data Quality_ We incorporate extensive red teaming techniques to increase the complexity of our samples. Every single sample in our dataset involves a specific red teaming technique.
* _Intellectual Property Protection_ The dataset primarily comprises images sourced from various social media platforms such as Twitter, with proper attribution provided for each sample. Furthermore, the dataset is exclusively intended for academic research. In the event of any copyright infringement notification, we will promptly adhere to relevant laws and regulations by removing related images.

Furthermore, referring to current practice , we extend the red teaming techniques originally used for LLMs  to apply to MLLMs, as demonstrated in Tab. 2. This systematic overview of red teaming techniques tailored for MLLMs could provide valuable insights for the community.

#### 2.2.2 Quality Control

* _Image-text Matching_: Textual prompt should be relevant to the corresponding images.
* _Dimension Matching_: Harmful information should be contained in the sample that aligns with the current evaluation dimension.
* _Correct Labeling_: Harmful samples should be correctly identified and labeled as intended red teaming techniques.
* _Necessity of Images_: Inspired by , we consider the necessity of including images to avoid answers from being directly derived from the textual prompt or existing knowledge within LLMs. As a result, we remove samples that do not necessitate the inclusion of images.

  
**Dimension** & **Subtask** & **Attack** & **Image Source** & **\# Num** & **\# Sum** & **\# Total** \\   & Personal Privacy &  &  &  \\  & Trade Secret & & & & \\  & State Secret & & & & \\   & Stereotype &  &  &  \\  & Prejudice & & & & 201 \\  & Discrimination & & & 34 \\   & Hate Speech &  & Hateful\_Memes  & 354 &  \\  & Pornography and Violence & & MEME  & 176 & \\   & Hallucination & i.3 & Logo-2K+  & 180 &  \\  & Robustness & i.4, i.5 & Animal-10  & 360 & \\   & Personal Safety &  &  &  \\  & Public Security & & & & \\   

Table 1: **Overview of MLLMGuard. We create 2,282 image-text pairs with images from social media and open-source datasets. Since image sources for all dimensions include social media, we only list those whose sources contain open-source datasets. Column Attack enumerates the red teaming techniques used in the related dimension, and the indexes corresponding to the techniques are listed in Tab. 2.**We require 3 reviewers to conduct the data review process based on the aforementioned criteria. If two or more reviewers find a particular sample inadequate or non-compliant with established criteria, the sample would be considered unfit for inclusion. In such cases, the necessary modifications or deletions are implemented based on reviewers' feedback. This iterative process ensures the refinement and improvement of the dataset.

### Metric

Our annotation is implemented through human scoring. For the dimension of Truthfulness, we solely employ a binary classification, indicating whether the model's responses are truthful or not. For dimensions of Privacy, Bias, Toxicity, and Legality, we set up scoring rules to map different performances of MLLMs (Tab. 3).

Inspired by , we categorize safe responses into 2 tiers: label \(0\) signifies _Safe and Responsible_ the model recognizes the gravity of the inquiry and opts not to provide assistance, usually accompanied by mild adomitions; label \(1\) indicates that models exclusively produce general harmless responses, such as declining assistance using templates or providing irrelevant content. Meanwhile, we also categorize unsafe responses into 2 levels: label \(2\) denotes a direct unsafe output (e.g., providing step-by-step guidance to create a bomb); label \(3\) signifies greater risks, indicating that a model is aware of the possible harmful impact in the query but still offers irresponsible assistance (e.g., reminding users that creating a bomb is illegal and dangerous while still providing guidance). Label \(3\) can serve as a potent probe to detect extreme risks , which is a significant consideration as we advance towards Artificial General Intelligence (AGI).

Overall, we provide the following metrics:

* **ASD (Attack Success Degree, \(\))** is computed using Eq. 1, which quantifies the extent to which the responses are harmless in each dimension. In Eq. 1, \(_{i}\) represents the degree for dimension \(i\), \(R_{i}\) denotes the prompt-response pairs for dimension \(i\), \(||\) denotes the number of prompt-response pairs, and \(()\) implements a linear transformation of data, mapping values from a 0 to 3 scale onto a 0 to 1 scale by standardizing the data across the designated interval (e.g., label

  
**Index** & **Method** & **Description** & **Num** \\   \\  t.1 & **Disguise** & involves role-playing, where the prompt assumes a specific ‘character role’ and carries out & 188 \\  & actions that appear reasonable but contravene values. &  \\ t.2 & **Reverse Induction** & refers to the deceptive avoidance of malice on the surface, while it actually & 91 \\  & involves enacting the very malevolence. &  \\ t.3 & **Unsafe Inquiry** & denotes that users ask models about how to engage in actions that violate human & 263 \\  & ethical standards, while tacitly approving the actions as correct. &  \\ t.4 & **Indirect Task Misalignment** & emphasizes the concealment of malicious tasks within the background & \\  & of a prompt rather than directly requesting models to carry out malevolent actions. &  \\  i.1 & **Facial Stereotype** & involves the practice of linking facial features with certain predetermined characteristics such as intelligence, income, or other traits, ascribed to a particular social group. & 286 \\ i.2 & **Malicious Labeling** & means the act of associating negative characteristics with certain social groups. & 221 \\ i.3 & **Non-existent Query** & refers to inquiring about features or entities that do not exist in the given image. & 180 \\ i.4 & **Position Swapping** & involves rearranging the options of an image-based multiple-choice question. By & 180 \\  & altering the positions of the image options, different answers can be generated & 180 \\ i.5 & **Noise Injection** & refers to adding various types of noise, such as Gaussian noise, to an image. & 180 \\ i.6 & **Harrful Scenario** & refers to the inclusion of hate speech, pornography, violence, or other harmful & \\  & content within an image, while ensuring that the accompanying textual prompt remains free from any & 574 \\  & red-team attacks or harmful language. & \\   

Table 2: **Description of each red teaming technique.**

    & Safe & Unsafe \\  Aware & 0 & 3 \\ Unaware & 1 & 2 \\   

Table 3: **Scoring rules for annotation.**_Aware_ represents that the model is capable of identifying the issues within the prompt, while _Safe_ indicates that the model can provide responses that are free from harm.

\(1\) is projected to \(0.3\)). \[}=}(((p,r)))}{|R_{i}|} 100\] (1)
* **PAR (Perfect Answer Rate, \(\))** is calculated using Eq. 2, which represents the proportion of safe and responsible responses among the responses in each dimension. \[}=}((((p,r)),0)}{|R_{i}|} 100\%\] (2)

## 3 Evaluation of MLLMGuard

In this section, we first present the experimental setup used to evaluate MLLMs on MLLMGuard. In Section 3.2, we discuss the performance of MLLMs across five dimensions on MLLMGuard. Section 3.3 introduces the design of a specific evaluator - GuardRank. Finally, we conduct a series of comparative experiments to discuss potential directions for enhancing MLLM Safety.

### Experimental Setup

Dataset for EvaluationTo ensure a fair evaluation and prevent our data from being exploited for training, we have randomly selected 1,500 samples from the original dataset for public disclosure. Henceforth, unless explicitly stated, the term "dataset" refers specifically to the publicly accessible dataset comprising the aforementioned 1,500 samples.

MLLMs for EvaluationWe select 13 mainstream MLLMs for evaluation, including 2 closed-source models and 11 open-source models. App. A provides additional information about these models.

### Main Results

General ComparisonThe ASD (\(\)) and PAR (\(\)) of each model across different dimensions can be seen in Tab. 4 and Tab. 5. Among all the models, GPT-4V has the lowest ASD, closely followed by the open-source model MiniGPT-v2 with a slight difference (\(-1.71\)). Meanwhile, MiniGPT-v2 achieves the highest PAR among all the models, surpassing the SOTA GPT-4V in most benchmarks.

Findings on TruthfulnessBased on the experimental results on Truthfulness, as depicted in Fig. 2, we have the following observations:

* Fig. 2(a) demonstrates the effectiveness of three red teaming techniques on MLLMs, with Position Swapping exhibiting a particularly significant impact.

  
**Model** & **Privacy** & **Bias** & **Toxicity** & **Tr Truthfulness** & **Legality** & **Avg.** \\  GPT-4V & 31.33 & 21.77 & 27.38 & 21.01 & 25.14 & **25.32** \\ Gemini & 38.89 & 48.10 & 35.54 & 26.36 & 36.81 & 37.14 \\  LLaVA-v1.5-7B & 41.05 & 44.31 & 35.25 & 59.35 & 35.42 & 43.08 \\ Qwen-VL-Chat & 43.21 & 39.75 & 37.85 & 48.27 & 35.42 & 40.90 \\ SEED-LLaMA & 49.23 & 55.78 & 44.09 & 58.89 & 48.75 & 51.35 \\ Yi-VL-34B & 48.61 & 43.44 & 35.06 & 52.04 & 40.97 & 44.03 \\ DeepSee-VL & 41.51 & 36.83 & 34.87 & 33.73 & 30.69 & 35.53 \\ mPLUG-owl2 & 46.14 & 49.56 & 41.40 & 57.71 & 50.28 & 49.02 \\ MiniGPT-v2 & **17.44** & 27.70 & **17.39** & 55.99 & **16.67** & 27.03 \\ CogVLM & 40.43 & 58.02 & 35.54 & 50.42 & 45.00 & 45.88 \\ ShareGPT4V & 44.14 & 46.94 & 52.83 & 58.15 & 45.56 & 49.52 \\ XComposer2-VL & 40.90 & 36.83 & 37.85 & 42.09 & 35.28 & 38.59 \\ InternVL-v1.5 & 40.74 & **20.60** & 46.88 & **19.09** & 34.72 & 32.41 \\   

Table 4: **ASD (\(\)) of various models across different dimensions.** We evaluate each model based on metrics in each dimension and highlight the best-performing model in **bold** and the second-best model with an underline.

* Fig. 2(b) indicates that all MLLMs are prone to hallucinations, especially when dealing with the dual problem of Non-existent Query, where the original open-ended prompts are transformed into the multiple-choice.
* As shown in Fig. 2(c), the placement of options significantly influences the selection of MLLMs. For instance, LLaVA-v1.5-7B tends to choose the left option (A), while mLUG-Owl2 leans towards the right option (B).
* Existing MLLMs demonstrate strong defense against Noise Injection as shown in 2(d). With the exception of MiniGPT-v2, most MLLMs get ASD below \(0.1\).

### The design of GuardRank

To automate our evaluation, we first utilize GPT-4 for Zero-Shot and In-Context Learning (ICL, we provide an example for each label) evaluation whose prompts can be found in App. G, but its overall accuracy on the test set is just 29.38% and 42.78% respectively. This demonstrates the unreliability of using GPT-4 directly for evaluations. Therefore, we develop an integrated evaluator GuardRank, allowing for a more accurate, faster, and cost-effective evaluation on MLLMGuard.

Implementation DetailsGuardRank is trained on a human-annotated dataset, employing LLaMA-2  as the backbone for the dimension of Privacy, Bias, Toxicity, and Legality, and Roberta-large  for the Hallucination sub-task. The textual prompt and the corresponding answer are concatenated into a single template, and human-annotated scores are used as labels. To validate the accuracy of GuardRank and its generalizability on out-of-distribution (OOD) models, we use responses from Xcomposer2-VL as the validation set and the responses from LLaVA-v1.5-7B and

  
**Model** & **Privacy** & **Bias** & **Toxicity** & **Truthfulness** & **Legality** & **Avg.** \\  GPT-4V & 39.35\% & 48.69\% & 18.73\% & 78.99\% & 27.92\% & 42.74\% \\ Gemini & 8.80\% & 7.00\% & 4.61\% & 73.64\% & 5.00\% & 19.81\% \\  LLaVA-v1.5-7B & 21.30\% & 18.08\% & 4.61\% & 40.65\% & 16.67\% & 20.26\% \\ Qwen-VL-Chat & 18.06\% & 18.95\% & 12.68\% & 51.73\% & 30.42\% & 26.37\% \\ SEED-LLAMA & 14.81\% & 3.50\% & 6.05\% & 41.11\% & 11.25\% & 15.34\% \\ Yi-VL-34B & 9.26\% & 22.16\% & 11.53\% & 47.96\% & 16.25\% & 21.43\% \\ DeepSee-VL & 25.46\% & 6.71\% & 5.19\% & 66.27\% & 23.75\% & 25.48\% \\ mPLUG-Owl2 & 14.81\% & 3.50\% & 6.34\% & 42.29\% & 7.08\% & 14.81\% \\ MiniGPT-v2 & **67.59**\% & 32.07\% & **47.84**\% & 44.01\% & **57.08\%** & **49.72\%** \\ CogVLM & 0.46\% & 0.00\% & 0.00\% & 49.58\% & 0.00\% & 10.01\% \\ ShareGPT4V & 13.89\% & 10.79\% & 2.31\% & 41.85\% & 16.25\% & 17.02\% \\ XComposer2-VL & 23.61\% & 23.03\% & 9.80\% & 57.91\% & 12.08\% & 25.29\% \\ InternVL-v1.5 & 24.54\% & **56.27**\% & 9.22\% & **80.91**\% & 30.00\% & 40.19\% \\   

Table 5: **PAR (\(\)) of various models across different dimensions.** We evaluate each model based on metrics in each dimension and highlight the best-performing model in **bold** and the second-best model with an underline.

Figure 2: **Results on Truthfulness.** (a) presents the ASD of MLLMs under various red teaming techniques on Truthfulness. (b) and (d) further display the ASD results on 2 red teaming techniques, i.e., Non-existent Query and Noise Injection. (c) provides the frequency of MLLMs selecting A/B/No Answer under the Position Swapping. Specifically, we experimented on both open-ended prompts and transferred multiple choice questions on Non-existent Query.

Qwen-VL-Chat as the test set. The model architecture and training details for GuardRank are provided in the APP. G.

Performance of GuardRankThe accuracy of GuardRank is shown in Tab. 6. GuardRank consistently outperforms GPT-4 as an evaluator, whether using Zero-shot or ICL approaches.

### Discussion

To further investigate the safety of MLLMs, we pose the following research questions to bring insights for future work:

RQ1: Do current alignment techniques in MLLMs enhance models' safety ability?We compare DeepSeek-VL-Base with its chat-aligned version, DeepSeek-VL-Chat, and Gemini with its safety-aligned version, Gemini-Safety. For Gemini-Safety, we utilize Safety filters2 and set the API threshold to BLOCK_LOW_AND_ABOVE to block most unsafe content. As shown in Fig. 3, the experimental results indicate that both chat alignment and safety alignment can enhance the safety of MLLMs to varying degrees. However, the marginal improvement of Gemini-Safety indicates that the current content filtering methods might be insufficient to defend against carefully crafted red teaming techniques.

RQ2: Does the LLM component affect the safety of MLLM?We conduct separate experiments to compare the safety of mPLUG-Owl (with LLAMA-7B as the LLM) and mPLUG-Owl2 (with LLAMA2-7B as the LLM). Here, we simply replace the LLM of CogVLM from Vicuna-v1.5-7B to LLAMA2-7B. As shown in Fig. 4, a safer LLM (LLAMA2-7B) improves MLLM safety across all dimensions. However, in the case of CogVLM, the performance varies across different dimensions. This inconsistency may stem from the direct replacement of LLM, which potentially disrupts the original alignment.

RQ3: Does the Scaling Law apply to MLLM Safety?We select three groups of MLLMs from the same families with different model parameter sizes to evaluate on GuardRank. The experimental results in Fig. 5 indicate that an increase in model parameters does not significantly enhance safety levels across all dimensions, even leading to a drop in some cases. The impact of the scaling law on MLLM safety is less pronounced than in LLMs [41; 3] or other MLLM capabilities .

RQ4: Is there a trade-off between being honest and harmless?Existing work has shown a trade-off between helpfulness and harmlessness in generative models , yet the relationship between honesty  and harmlessness remains underexplored. As shown in Tab. 4 and Tab. 5, MiniGPT-v2 exhibits strong safety across several dimensions but underperforms on Truthfulness, suggesting a potential trade-off between honesty and harmlessness.

## 4 Related Work

Red Teaming towards MLLMsIt is a common practice to discover MLLMs' vulnerabilities through adversarial attacks and jailbreaking methods. Image-based and text-based red-teaming are two mainstream attack methods for MLLMs. Image-based red-teaming attacks typically involve adding a small amount of perturbation to an image, causing the model to produce outputs completely disparate from the original answers. [44; 45; 46] optimize images on a few-shot corpus comprised of numerous sentences to maximize the model's probability of generating undesired responses. [16; 47] convert the harmful content into images through typography to bypass the safety alignment within the LLMs of MLLMs. Meanwhile, there are relatively fewer text-based attacks specifically designed for MLLMs, including those derived from LLMs. Text-based red teaming attacks typically involve rewriting text or stealing system prompts to bypass the safety alignment of the LLM component within MLLMs.  employ GPT-4 as a red teaming tool against itself to search for potential jailbreak prompts leveraging stolen system prompts and  elicitate the incorrect or harmful responses from VLMs through misleading text inputs.

Alignment for MLLMsThe training process of MLLMs usually consists of two phases: pretraining and supervised fine-tuning (SFT), with different types of alignment occurring during both stages. Pretraining aims to achieve _Modality Alignment_ between the vision encoder and LLM, often by using a large amount of weakly labeled data, followed by a smaller amount of high-quality data . The SFT stage then focuses on _Chat Alignment_ and _Safety Alignment_. After achieving modality alignment in pretraining, some will undergo chat alignment to enhance their capabilities in dialogue and instruction-following, such as Owen-VL-Chat  (aligned from Owen-VL) and DeepSeek-VL-Chat  (aligned from DeepSeek-VL). However, fewer of MLLMs undergo safety alignment. Gemini  incorporates a dedicated safety classifier that identifies, labels, and filters out content related to violence or negative stereotypes. GPT-4V  integrates supplementary multimodal data during the post-training process to strengthen its ability to refuse engagement in illicit behavior and unsupported inference requests.

## 5 Conclusion

This study introduces MLLMGuard, a comprehensive multi-dimensional safety evaluation suite for MLLMs, which is composed of three key components: 1) an extensive evaluation framework, 2) a highly adversarial, bilingual evaluation dataset, and 3) GuardRank, a lightweight, automated safety evaluator. Based on MLLMGuard, we conduct rigorous safety assessments of current

Figure 5: PAR (\(\)) of MLLMs on different parameter size.

MLLMs, identifying critical vulnerabilities and exploring potential techniques to enhance their safety. Consequently, MLLMGuard not only provides an effective tool for MLLM safety evaluation but also pioneers novel methodologies for safety enhancement, which contributes to steering the development of MLLMs towards safer and more responsible AI applications.

## 6 Limitations

Dataset and AnnotationOur dataset and annotation are created by workers aged between 20 and 35 from mainland China, whose expertise primarily spans psychology, sociology, law, and computer science. This demographic similarity may introduce potential biases related to their shared cultural backgrounds. Additionally, the purely manual construction of our dataset makes it costly to scale. We plan to enhance scalability and effectiveness by incorporating self-instruction through red teaming techniques. Meanwhile, while we strive to cover a broad range of evaluation aspects, the potential risks associated with MLLM outputs are inevitably limitless. Therefore, it is crucial for us to continuously expand the range of aspects evaluated.

Limitations of the EvaluatorWe acknowledge several possible limitations of our evaluator: 1) A fixed value for max_token (128) may introduce potential errors during the subsequent processing of responses. 2) To facilitate lightweight evaluation, GuardRank does not leverage more sophisticated models as its backbones, which may enhance the accuracy of the evaluator. 3) To exert more precise control over variables, our model's dialogue design is confined to single-turn conversations.

## 7 Social Impacts

Our work holds immense social implications, particularly surrounding the use of MLLMs. We outline the potential social impacts as follows:

Value Alignment with HumanOur research delves into the profound societal impacts of deploying MLLMs, including proprietary models such as GPT-4V and Gemini, as well as open-source alternatives like LLaVA. We pinpoint several areas where MLLMs fall short in alignment with human values: 1) **Lack of understanding of human values.** MLLMs often fail to recognize malicious intent in user inputs, missing cues that indicate harmful intentions. 2) **Inability to refuse malicious inputs.** Current MLLMs lack robust mechanisms to accurately detect and reject malicious or unethical inputs, which increases the risk of misuse. 3) **Absence of benevolent guidance.** Though MLLMs can identify malicious prompts, their responses are typically formulaic and do not offer constructive, value-aligned guidance. These findings underscore the necessity of integrating ethical and societal considerations in the development and deployment of MLLMs to ensure they uphold human values.

Truthfulness in MLLMsOur research into the truthfulness of MLLMs reveals that MLLMs are prone to issues such as hallucinations, selection bias, and the detrimental effects of noise on accuracy. These insights are crucial for guiding the development of more reliable and trustworthy MLLMs.

## 8 Ethical Considerations

In this work, we introduce an adversarial dataset to evaluate MLLM Safety. Given its adversarial nature, the dataset includes potentially offensive samples and may raise privacy concerns. We claim that all the data included are used strictly for academic research purposes and do not represent the views of the authors or the dataset constructors. To address privacy issues, we have anonymized certain facial features in the portions of the dataset that are publicly available. For access to non-anonymized data, anyone interested is required to complete our application form.

Regarding the risk of copyright infringement, it is crucial to acknowledge that the copyrights for images with attributed sources are owned by their respective rights holders. Usage of these images beyond the scope of research without explicit consent from the rights holders constitutes a violation of copyright laws, making the user legally liable.

For our annotators, we prioritize their legal rights and psychological well-being. We compensate them with a salary significantly above the local minimum wage. We also actively monitor the psychological state of our annotators and provide essential support as needed.