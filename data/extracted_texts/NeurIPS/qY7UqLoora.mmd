# Are GATs Out of Balance?

Nimrah Mustafa\({}^{*}\)

nimrah.mustafa@cispa.de

&Aleksandar Bojchevski\({}^{}\)

a.bojchevski@uni-koeln.de

&Rebekka Burkholz\({}^{*}\)

burkholz@cispa.de

\({}^{*}\)CISPA Helmholtz Center for Information Security, 66123 Saarbrucken, Germany

\({}^{}\)University of Cologne, 50923 Koln, Germany

###### Abstract

While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms.

## 1 Introduction

A rapidly growing class of model architectures for graph representation learning are Graph Neural Networks (GNNs)  which have achieved strong empirical performance across various applications such as social network analysis , drug discovery , recommendation systems , and traffic forecasting . This has driven research focused on constructing and assessing specific architectural designs  tailored to various tasks.

On the theoretical front, mostly the expressive power  and computational capabilities  of GNNs have been studied. However, there is limited understanding of the underlying learning mechanics. We undertake a classical variant of GNNs, the Graph Attention Network (GAT) . It overcomes the limitation of standard neighborhood representation averaging in GCNs  by employing a self-attention mechanism  on nodes. Attention performs a weighted aggregation over a node's neighbors to learn their relative importance. This increases the expressiveness of the neural network. GAT is a popular model that continues to serve as strong baseline.

However, a major limitation of message-passing GNNs in general, and therefore GATs as well, is severe performance degradation when the depth of the network is even slightly increased. SOTA results are reportedly achieved by models of 2 or 3 layers. This problem has largely been attributed to over-smoothing , a phenomenon in which node representations become indistinguishable when multiple layers are stacked. To relieve GNNs of over-smoothing, practical techniques inspired by classical deep neural networks tweak the training process, e.g. by normalization  or regularization , and impose architectural changes such as skip connections (denseand residual) [34; 53] or offer other engineering solutions  or their combinations . Other approaches to overcome over-smoothing include learning CNN-like spatial operators from random paths in the graph instead of the point-wise graph Laplacian operator  and learning adaptive receptive fields (different 'effective' neighborhoods for different nodes) [63; 36; 52]. Another problem associated with loss of performance in deeper networks is over-squashing , but we do not discuss it since non-attentive models are more susceptible to it, which is not our focus.

Alternative causes for the challenged trainability of deeper GNNs beyond over-smoothing is hampered signal propagation during training (i.e. backpropagation) [25; 62; 13]. To alleviate this problem for GCNs, a dynamic addition of skip connections to vanilla-GCNs, which is guided by gradient flow, and a topology-aware isometric initialization are proposed in . A combination of orthogonal initialization and regularization facilitates gradient flow in . To the best of our knowledge, these are the only works that discuss the trainability issue of deep GNNs from the perspective of signal propagation, which studies a randomly initialized network at initialization. Here, we offer an insight into a mechanism that allows these approaches to improve the trainability of deeper networks that is related to the entire gradient flow dynamics. We focus in particular on GAT architectures and the specific challenges that are introduced by attention.

Our work translates the concept of neuronwise balancedness from traditional deep neural networks to GNNs, where a deeper understanding of gradient dynamics has been developed and norm balancedness is a critical assumption that induces successful training conditions. One reason is that the degree of initial balancedness is conserved throughout the training dynamics as described by gradient flow (i.e. a model of gradient descent with infinitesimally small learning rate or step size). Concretely, for fully connected feed-forward networks and deep convolutional neural networks with continuous homogenous activation functions such as ReLUs, the difference between the squared \(l2\)-norms of incoming and outgoing parameters to a neuron stays approximately constant during training [15; 32]. Realistic optimization elements such as finite learning rates, batch stochasticity, momentum, and weight decay break the symmetries induced by these laws. Yet, gradient flow equations can be extended to take these practicalities into account .

Inspired by these advances, we derive a conservation law for GATs with positive homogeneous activation functions such as ReLUs. As GATs are a generalization of GCNs, most aspects of our insights can also be transferred to other architectures. Our consideration of the attention mechanism would also be novel in the context of classic feed-forward architectures and could have intriguing implications for transformers that are of independent interest. In this work, however, we focus on GATs and derive a relationship between model parameters and their gradients, which induces a conservation law under gradient flow. Based on this insight, we identify a reason for the lack of trainability in deeper GATs and GCNs, which can be alleviated with a balanced parameter initialization. Experiments on multiple benchmark datasets demonstrate that our proposal is effective in mitigating the highlighted trainability issues, as it leads to considerable training speed-ups and enables significant parameter changes across all layers. This establishes a causal relationship between trainability and parameter balancedness also empirically, as our theory predicts. The main theorem presented in this work serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms such as transformers  and in particular, vision transformers which require depth more than NLP transformers. Our contributions are as follows.

* We derive a conservation law of gradient flow dynamics for GATs, including its variations such as shared feature weights and multiple attention heads.
* This law offers an explanation for the lack of trainability of deeper GATs with standard initialization.
* To demonstrate empirically that our theory has established a causal link between balancedness and trainability, we propose a balanced initialization scheme that improves the trainability of deeper networks and attains considerable speedup in training and convergence time, as our theory predicts.

## 2 Theory: Conservation laws in GATs

PreliminariesFor a graph \(G=(V,E)\) with node set \(V\) and edge set \(E V V\), where the neighborhood of a node \(v\) is given as \((v)=\{u|(u,v) E\}\), a GNN layer computes each node's representation by aggregating over its neighbors' representation. In GATs, this aggregation is weighted by parameterized attention coefficients \(_{uv}\), which indicate the importance of node \(u\) for \(v\)Given input representations \(h_{v}^{l-1}\) for all nodes \(v V\), a GAT 1 layer \(l\) transforms those to:

\[h_{v}^{l} =(_{u(v)}_{uv}^{l} W_{s}^{l}h_{u }^{l-1}),\ \ \] (1) \[_{uv}^{l} =((a^{l})^{}(W_{s}^{l}h_ {u}^{l-1}+W_{t}^{l}h_{v}^{l-1}))}{_{u^{}(v)} ((a^{l})^{}(W_{s}^{l}h_{u^{}}^{l-1}+W_{t}^{l}h_{ v}^{l-1})))}.\] (2)

We consider \(\) to be a positively homogeneous activation functions (i.e \((x)=x^{}(x)\) and consequently, \((ax)=a(x)\) for positive scalars \(a\)), such as a ReLU \((x)=\{x,0\}\) or LeakyReLU \((x)=\{x,0\}+-\{-x,0\}\). The feature transformation weights \(W_{s}\) and \(W_{t}\) for source and target nodes, respectively, may also be shared such that \(W=W_{s}=W_{t}\).

**Definition 2.1**.: Given training data \(\{(x_{m},y_{m})\}_{m=1}^{M}^{d}^{p}\) for \(M V\), let \(f:^{d}^{p}\) be the function represented by a network constructed by stacking \(L\) GAT layers as defined in Eq. (1) and (2) with \(W=W_{s}=W_{t}\) and \(h_{m}^{0}=g(x_{m})\). Each layer \(l[L]\) of size \(n_{l}\) has associated parameters: a feature weight matrix \(W^{l}^{n_{l} n_{l-1}}\) and an attention vector \(a^{l}^{n_{l}}\), where \(n_{0}=d\) and \(n_{L}=p\). Given a differentiable loss function \(:^{d}^{p}\), the loss \(=}{{M}}_{i=1}^{M}(f(x_{m}),y_{m})\) is used to update model parameters \(w\{W^{l},a^{l}\}_{l=1}^{L}\) with learning rate \(\) by gradient descent, i.e., \(w^{t+1}=w^{t}-_{w}\), where \(_{w}=[}}{{ w_{ 1}}},,}}{{ w_{|w|}}}]\) and \(w^{0}\) is set by the initialization scheme. For an infinitesimal \( 0\), the dynamics of gradient descent behave similarly to gradient flow defined by \(}{{ t}}=-_{w}\), where \(t\) is the continuous time index.

We use \(W[i,:]\), \(W[:,i]\), and \(a[i]\) to denote the \(i^{th}\) row of \(W\), column of \(W\), and entry of \(a\), respectively. Note that \(W^{l}[i,:]\) is the vector of weights incoming to neuron \(i[n_{l}]\) and \(W^{l+1}[:,i]\) is the vector of weights outgoing from the same neuron. For the purpose of discussion, let \(i[d_{l}],j[d_{l-1}]\), and \(k[d_{l+1}]\), as depicted in Fig. 1. \(,\) denotes the scalar product.

Conservation LawsWe focus the following exposition on weight-sharing GATs, as this variant has the practical benefit that it requires fewer parameters. Yet, similar laws also hold for the non-weight-sharing case, as we state and discuss in the supplement. For brevity, we also defer the discussion of laws for the multi-headed attention mechanism to the supplement.

**Theorem 2.2** (Structure of gradients).: _Given the feature weight and attention parameters \(W^{l}\) and \(a^{l}\) of a layer \(l\) in a GAT network as described in Def. 2.1, the structure of the gradients for layer \(l[L-1]\) in the network is conserved according to the following law:_

\[ W^{l}[i,:],_{W^{l}[i,:]}- a^{l}[i], _{a^{l}[i]}= W^{l+1}[:,i],_{W^{l+1}[:,i ]}.\] (3)

**Corollary 2.3** (Norm conservation of weights incoming and outgoing at every neuron).: _Given the gradient structure of Theorem 2.2 and positive homogeneity of the activation \(\) in Eq. (1), gradient flow in the network adheres to the following invariance for \(l[L-1]\):_

\[}{t}(\|W^{l}[i,:]\|^{2}-\|a^{l} [i]\|^{2})=}{t}(\|W^{l+1}[: \,i]\|^{2}),\] (4)

   Init. & Feat. & Attn. & Bal.? \\  Xav & Xavier & Xavier & No \\ Xav\({}_{Z}\) & Xavier & Zero & No \\ Bal\({}_{X}\) & Xavier & Zero & Yes \\ Bal\({}_{O}\) & LLortho & Zero & Yes \\   

Table 1: Attributes of init. schemes

Figure 1: Params. to balance for neuron \(i[n_{l}]\)

_It then directly follows by integration over time that the \(l2\)-norms of feature and attention weights incoming to and outgoing from a neuron are preserved such that_

\[\|W^{l}[i;]\|^{2}-\|a^{l}[i]\|^{2}-\|W^{l+1}[:i] \|^{2}=c,\] (5)

_where \(c=}{t}(\|W^{l}[i;]\|^{2}-\|a^{l }[i]\|^{2})\) at initialization (i.e. \(t=0\))._

We denote the 'degree of balancedness' by \(c\) and call an initialization balanced if \(c=0\).

**Corollary 2.4** (Norm conservation across layers).: _Given Eq. (4), the invariance of gradient flow at the layer level for \(l[L-1]\) by summing over \(i[n_{l}]\) is:_

\[}{t}(\|W^{l}\|_{F}^{2}-\|a^{l} \|^{2})=}{t}(\|W^{l+1}\| _{F}^{2}).\] (6)

_Remark 2.5_.: Similar conservation laws also hold for the original less expressive GAT version  as well as for the vanilla GCN  yielded by fixing \(a^{l}=\).

InsightsWe first verify our theory and then discuss its implications in the context of four different initializations for GATs, which are summarized in Table 1. Xavier (Xav.) initialization  is the standard default for GATs. We describe the Looks-Linear-Orthogonal (LLortho) initialization later.

In order to empirically validate Theorem 2.2, we rewrite Equation 3 to define \(\) as:

\[= W^{l}[i,:],_{W^{l}[i,:]}- a^{l}, _{a^{l}}- W^{l+1}[:,i],_{W^{l+1}[:,i]} =0.\] (7)

We observe how the value of \(\) varies during training in Fig. 2 for both GD and Adam optimizers with \(=0.1\). Although the theory assumes infinitesimal learning rates due to gradient flow, it still holds sufficiently (\( 0\)) in practice for normally used learning rates as we validate in Fig. 2. Furthermore, although the theory assumes vanilla gradient descent optimization, it still holds for the Adam optimizer (Fig. 1(b)). This is so because the derived law on the level of scalar products between gradients and parameters (see Theorem 2.2) still holds, as it states a relationship between gradients and parameters that is independent of the learning rate or precise gradient updates.

Having verified Theorem 2.2, we use it to deduce an explanation for a major trainability issue of GATs with Xavier initialization that is amplified with increased network depth.

The main reason is that the last layer has a comparatively small average weight norm, as \(\|W^{L}[:,i]\|^{2}=2n_{L}/(n_{L}+n_{L-1})<<1\), where the number of outputs is smaller than the layer width \(n_{L}<<n_{L-1}\). In contrast, \(\|W^{L-1}[i,:]\|^{2}=2n_{L-1}/(n_{L-1}+n_{L-2})=1\) and \(a^{L-1}[i]^{2}=2/(1+n_{L-1})\). In consequence, the right-hand side of our derived conservation law

\[_{j=1}^{n_{L-2}}(W_{ij}^{L-1})^{2}^{L-1}}} {W_{ij}^{L-1}}-(a_{i}^{L-1})^{2}^{L-1}}}{a_{i}^{ L-1}}=_{k=1}^{n_{L}}(W_{ki}^{L})^{2}^{L}}}{W_{ ki}^{L}}\] (8)

Figure 2: Visualizing conservation laws: (a),(b) show \( 0\) (can not empirically be exactly zero due to finite \(=0.1\)) from Eq. (7) for hidden layers when \(L=3\). (c),(d) show the value of \(c\) from Eq. (5), which is determined by the initialization and should be \(0\) when the network is balanced, for \(L=10\) trained with GD. With Xav., the network is unbalanced and hence \(c 0\) for all neurons. With \(_{O}\), \(c=0\) exactly at initialization for all neurons by design but during training, it varies slightly and \(c 0\) as the network becomes slightly unbalanced due to the finite \(=0.1\). As both \(\) and \(c\) approximately meet their required value, we conclude that the derived law holds for practical purposes as well.

is relatively small, as \(_{k=1}^{n_{L}}(W_{ki}^{L})^{2}<<1\), on average, while the weights on the left-hand side are orders of magnitude larger. This implies that the relative gradients on the left-hand side are likely relatively small to meet the equality unless all the signs of change are set in such a way that they satisfy the equality, which is highly unlikely during training. Therefore, relative gradients in layer \(L-1\) and accordingly also the previous layers of the same dimension can only change in minor ways.

The amplification of this trainability issue with depth can also be explained by the recursive substitution of Theorem 2.2 in Eq. (8) that results in a telescoping series yielding:

\[_{j=1}^{n_{1}}_{m=1}^{n_{0}}W_{jm}^{(1)^{2}}^{(1)} }}{W_{jm}^{(1)}}-_{l=1}^{L-1}_{o=1}^{n_{l}}a_{o}^{(l)^{2}} ^{(l)}}}{a_{o}^{(l)}}=_{i=1}^{n_{L-1}}_{ k=1}^{n_{L}}W_{ki}^{(L)^{2}}^{(L)}}{W_{ki}^{(L)}}\] (9)

Generally, \(2n_{1}<n_{0}\) and thus \(\|W^{1}[j\,:]\|^{2}=2n_{1}/(n_{1}+n_{0})<1\). Since the weights in the first layer and the gradients propagated to the first layer are both small, gradients of attention parameters of the intermediate hidden layers must also be very small in order to satisfy Eq. (9). Evidently, the problem aggravates with depth where the same value must now be distributed over the parameters and gradients of more layers. Note that the norms of the first and last layers are usually not arbitrary but rather determined by the input and output scale of the network.

Analogously, we can see see how a balanced initialization would mitigate the problem. Equal weight norms \(\|W^{L-1}[i\,:]\|^{2}\) and \(\|W^{L}[:,i]\|^{2}\) in Eq. (8) (as the attention parameters are set to \(0\) during balancing, see Procedure 2.6) would allow larger relative gradients in layer \(L-1\), as compared to the imbalanced case, that can enhance gradient flow in the network to earlier layers. In other words, gradients on both sides of the equation have equal room to drive parameter change.

Figure 4: For \(L=10\), unbalanced initialization is unable to train the model as relative gradients are extremely small, and the parameters (and loss) do not change. However, the balanced initialization is able to propagate gradients, change parameters, and thus attain training loss \( 0\). Xav. and \(_{O}\) achieve \(39.3\%\) and \(80.2\%\) test accuracy, respectively. Note that the balanced initialization in (b) is also able to train faster than the \(5\) layer network with Xavier initialization in Fig. 2(b).

Figure 3: For \(L=5\), a high fraction of parameters change, and gradients propagate to earlier layers for both the unbalanced and balanced initialization Xav. and \(_{O}\) initialization achieve \(75.5\%\) and \(79.9\%\) test accuracy, respectively. Note that the balanced initialization in (b) is able to train faster.

To visualize the trainability issue, we study the relative change \(|{(w^{*}-w_{o})}/_{w^{*}}|\). of trained network parameters \((w^{*})\) w.r.t. their initial value. In order to observe meaningful relative change, we only consider parameters with a significant contribution to the model output (\(w^{*} 10^{-4}\)). We also plot the relative gradient norms of the feature weights across all layers to visualize their propagation. We display these values for \(L=5\) and \(L=10\) in Fig. 3 and 4, respectively, and observe a stark contrast, similar to the trends in relative gradient norms of selected layers of a \(10\) layer GAT in Fig. 5. In all experiments, GAT was trained with the specified initialization on Cora using GD with \(=0.1\) for \(5000\) epochs.

Interestingly, the attention parameters change most in the first layer and increasingly more towards the last layer, as seen in Fig. 2(a) and 3(a). This is consistently observed in both the \(5\) and \(10\) layer networks if the initialization is balanced, but with unbalanced initialization, the \(10\) layer network is unable to produce the same effect. As our theory defines a coarse-level conservation law, such fine-grained training dynamics cannot be completely explained by it.

Since the attention parameters are set to \(0\) in the balanced initializations (see Procedure 2.6), as ablation, we also observe the effect of initializing attention and feature parameters with zero and Xavier, respectively. From Eq.(2), \(a^{l}=0\) leads to \(_{uv}=}{{|(v)|}}\), implying that all neighbors \(u\) of the node \(v\) are equally important at initialization. Intuitively, this allows the network to learn the importance of neighbors without any initially introduced bias and thus avoids the 'chicken and egg' problem that arises in the initialization of attention over nodes. Although this helps generalization in some cases (see Fig. 6), it alone does not help the trainability issue as seen in Fig. 3(a). It may in fact worsen it (see Fig. 5), which is explained by Eq. 8. These observations underline the further need for parameter balancing.

**Procedure 2.6** (Balancing).: Based on Eq. (5) from the norm preservation law 2.3, we note that in order to achieve balancedness, (i.e. set \(c=0\) in Eq.(5)), the randomly initialized parameters \(W^{l}\) and \(a^{l}\) must satisfy the following equality for \(l[L]\):

\[\|{W^{l}[i,:]}\|^{2}-\|{a^{l}[i]}\|^{2}=\|{W^{l+1}[ :i]}\|^{2}\]

This can be achieved by scaling the randomly initialized weights as follows:

1. Set \(a^{l}=\) for \(l[L]\).
2. Set \(W^{1}[i,:]=[i,:]}{\|{W^{1}[i,:]}\|}}\), for \(i[n_{1}]\) where \(_{i}\) is a hyperparameter
3. Set \(W^{l+1}[:,i]=[:,i]}{\|{W^{l+1}[:,i]}\|}\|{W^{l}[ i,:]}\|\) for \(i[n_{l}]\) and \(l[L-1]\)

In step 2, \(_{i}\) determines the initial squared norm of the incoming weights to neuron \(i\) in the first layer. It can be any constant thus also randomly sampled from a normal or uniform distribution. We explain why we set \(_{i}=2\) for \(i[n_{1}]\) in the context of an orthogonal initialization.

_Remark 2.7_.: This procedure balances the network starting from \(l=1\) towards \(l=L\). Yet, it could also be carried out in reverse order by first defining \(\|{W^{L}[:,i]}\|^{2}=_{i}\) for \(i[n_{L-1}]\).

Balanced Orthogonal InitializationThe feature weights \(W^{l}\) for \(l[L]\) need to be initialized randomly before balancing, which can simply be done by using Xavier initialization. However, existing works have pointed out benefits of orthogonal initialization to achieve initial dynamical

Figure 5: Relative gradient norms of feature (left axis, solid) and of attention (right axis, stylized) parameters for \(l\) and \(L=10\), sampled every \(25\) epochs. Test accuracy is at the top. Both attention and feature gradients at the first, middle, and last layer of the network with both balanced initializations are much larger than with unbalanced initialization (note axis scales).

isometry for DNNs and CNNs [37; 44; 23], as it enables training very deep architectures. In line with these findings, we initialize \(W^{l}\), before balancing, to be an orthogonal matrix with a looks-linear (LL) mirrored block structure [46; 6], that achieves perfect dynamical isometry in perceptrons with ReLU activation functions. Due to the peculiarity of neighborhood aggregation, the same technique does not induce perfect dynamical isometry in GATs (or GNNs). Exploring how dynamical isometry can be achieved or approximated in general GNNs could be an interesting direction for future work. Nevertheless, using a (balanced) LL-orthogonal initialization enhances trainability, particularly of deeper models. We defer further discussion on the effects of orthogonal initialization, such as with identity matrices, to the appendix.

We outline the initialization procedure as follows: Let \(\) and \(\) denote row-wise and column-wise matrix concatenation, respectively. Let us draw a random orthonormal submatrix \(U^{l}\) and define \(W^{1}=[U^{1}-U^{1}]\) where \(U^{1}^{}{2} n_{0}}\), \(W^{l}=[[U^{l}-U^{l}][-U^{l} U^{l}]]\) where \(U^{l}^{}{2}}{2}}\) for \(l=\{2,,L-1\}\), and \(W^{L}=[U^{L}-U^{L}]\) where \(U^{L}^{n_{L}}{2}}\). Since \(U^{L}\) is an orthonormal matrix, \(\|W^{L}[:,i]\|^{2}=2\) by definition, and by recursive application of Eq. (2.3 (with \(a^{l}=\)), balancedness requires that \(\|W^{1}[i:,]\|^{2}=2\). Therefore, we set \(_{i}=2\) for \(i n_{1}\).

## 3 Experiments

The main purpose of our experiments is to verify the validity of our theoretical insights and deduce an explanation for a major trainability issue of GATs that is amplified with increased network depth. Based on our theory, we understand the striking observation in Figure 3(a) that the parameters of default Xavier initialized GATs struggle to change during training. We demonstrate the validity of our theory with the nature of our solution. As we balance the parameter initialization (see Procedure 2.6), we allow the gradient signal to pass through the network layers, which enables parameter changes during training in all GAT layers. In consequence, balanced initialization schemes achieve higher generalization performance and significant training speed ups in comparison with the default Xavier initialization. It also facilitates training deeper GAT models.

To analyze this effect systematically, we study the different GAT initializations listed in Table 1 on generalization (in % accuracy) and training speedup (in epochs) using nine common benchmark datasets for semi-supervised node classification tasks. We defer dataset details to the supplement. We use the standard provided train/validation/test splits and have removed the isolated nodes from Citeseer. We use the Pytorch Geometric framework and run our experiments on either Nvidia T4 Tensor Core GPU with 15 GB RAM or Nvidia GeForce RTX 3060 Laptop GPU with 6 GB RAM. We allow each network to train, both with SGD and Adam, for 5000 epochs (unless it converges earlier, i.e. achieves training loss \( 10^{-4}\)) and select the model state with the highest validation accuracy. For each experiment, the mean \( 95\%\) confidence interval over five runs is reported for accuracy and epochs to the best model. All reported results use ReLU activation, weight sharing and no biases, unless stated otherwise. No weight sharing leads to similar trends, which we therefore omit. Our experimental code is available at https://github.com/RelationalML/GAT_Balanced_Initialization.

SgdWe first evaluate the performance of the different initialization schemes of GAT models using vanilla gradient descent. Since no results have been previously reported using SGD on these datasets, to the best of our knowledge, we find that setting a learning rate of \(0.1\), \(0.05\) and \(0.005\) for \(L=\), \(L=\), and \(L=40\), respectively, allows for reasonably stable training on Cora, Citeseer, and

Figure 6: GAT trained on Cora using SGD

Pubmed. For the remaining datasets, we set the learning rate to \(0.05,0.01\), \(0.005\) and \(0.0005\) for \(L=,L=10\), \(L=20\), and \(L=40\), respectively. Note that we do not perform fine-tuning of the learning rate or other hyperparameters with respect to performance and use the same settings for all initializations to allow fair comparison.

Figure 6 highlights that models initialized with balanced parameters, \(_{O}\), consistently achieve the best accuracy and significantly speed up training, even on shallow models of \(2\) layers. In line with our theory, the default Xavier (Xav) initialization hampers model training (see also Figure 4a). Interestingly, the default Xavier (Xav) initialized deeper models tend to improve in performance with width but cannot compete with balanced initialization schemes. For example, the accuracy achieved by Xav for \(L=10\) increases from \(24.7\%\) to \(68.7\%\) when the width is increased from \(64\) to \(512\). We hypothesize that the reason why width aids generalization performance is that a higher number of (almost random) features supports better models. This would also be in line with studies of overparameterization in vanilla feed-forward architectures, where higher width also aids random feature models .

The observation that width overparameterized models enable training deeper models may be of independent interest in the context of how overparameterization in GNNs may be helpful. However, training wider _and_ deeper (hence overall larger) models is computationally inefficient, even for datasets with the magnitude of Cora. In contrast, the \(_{O}\) initialized model for \(L=10\) is already able to attain \(79.7\%\) even with width\(=64\) and improves to \(80.9\%\) with width\(=512\). Primarily the

   &  &  &  &  &  &  \\  _{O}\)} & Xav & \(71.82 2.73\) & \(58.40 2.25\) & \(24.70 8.90\) & \(19.23 1.54\) & \(18.72 1.15\) \\  & \(_{X}\) & \(71.62 0.80\) & \(68.83 1.62\) & \(64.13 1.57\) & \(54.88 7.95\) & \(42.63 17.47\) \\  & \(_{O}\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  _{O}\)} & Xav & \(77.26 1.39\) & \(70.68 2.16\) & \(67.32 10.70\) & \(36.52 11.50\) & \(27.20 13.99\) \\  & \(_{X}\) & \(\) & \(75.66 1.81\) & \(\) & \(76.44 1.70\) & \(75.74 2.94\) \\  & \(_{O}\) & \(77.68 0.45\) & \(\) & \(77.04 2.14\) & \(\) & \(\) \\  _{O}\)} & Xav & \(27.32 0.59\) & \(24.60 0.93\) & \(24.08 0.80\) & \(22.29 3.26\) & \(19.46 5.75\) \\  & \(_{X}\) & \(26.00 0.59\) & \(23.93 1.42\) & \(\) & \(\) & \(23.88 0.97\) \\  & \(_{O}\) & \(\) & \(\) & \(24.17 0.62\) & \(24.24 1.05\) & \(\) \\  _{O}\)} & Xav & \(\) & \(54.21 1.05\) & \(30.31 5.96\) & \(22.19 2.04\) & \(22.28 3.15\) \\  & \(_{X}\) & \(51.18 1.94\) & \(\) & \(\) & \(51.89 1.89\) & \(38.64 10.31\) \\   & \(_{O}\) & \(50.00 3.07\) & \(53.95 1.81\) & \(51.84 3.21\) & \(\) & \(\) \\  _{O}\)} & Xav & \(\) & \(41.08 2.51\) & \(\) & \(25.41 14.64\) & \(22.70 13.69\) \\  & \(_{X}\) & \(41.08 6.84\) & \(35.14 11.82\) & \(41.08 2.51\) & \(40.00 4.93\) & \(\) \\   & \(_{O}\) & \(42.16 1.64\) & \(\) & \(36.76 5.02\) & \(\) & \(36.22 3.42\) \\  _{O}\)} & Xav & \(35.20 0.44\) & \(40.96 0.92\) & \(21.65 1.52\) & \(20.23 1.69\) & \(19.67 0.29\) \\  & \(_{X}\) & \(\) & \(40.98 0.87\) & \(\) & \(38.35 1.07\) & \(25.38 4.62\) \\   & \(_{O}\) & \(35.83 0.92\) & \(\) & \(38.85 1.36\) & \(\) & \(\) \\  _{O}\)} & Xav & \(60.00 1.34\) & \(60.54 3.42\) & \(58.92 1.34\) & \(49.73 20.97\) & \(17.84 26.98\) \\   & \(_{X}\) & \(\) & \(\) & \(\) & \(\) & \(56.22 1.34\) \\   & \(_{O}\) & \(60.00 1.34\) & \(57.30 1.34\) & \(56.76 0.00\) & \(58.38 4.55\) & \(\) \\  _{O}\)} & Xav & \(\) & \(51.37 8.90\) & \(51.76 3.64\) & \(43.14 25.07\) & \(31.76 31.50\) \\   & \(_{X}\) & \(49.80 8.79\) & \(\) & \(4parameter balancing must be responsible for the improved performance, as it is not sufficient to simply initialize the attention weights to zero, as shown in Figure 3(a) and 5.

For the remaining datasets, we report only the performance of networks with \(64\) hidden dimension in Table 2 for brevity. Since we have considered the \(_{Z}\) initialization only as an ablation study and find it to be ineffective (see Figure 5(a)), we do not discuss it any further.

Note that for some datasets (e.g.Pubmed, Wisconsin), deeper models (e.g. \(L=40\)) are unable to train at all with Xavier initialization, which explains the high variation in test accuracy across multiple runs as, in each run, the model essentially remains a random network. The reduced performance even with balanced initialization at higher depth may be due to a lack of convergence within \(5000\) epochs. Nevertheless, the fact that the drop in performance is not drastic, but rather gradual as depth increases, indicates that the network is being trained, i.e. trainability is not an issue.

As a stress test, we validate that GAT networks with balanced initialization maintain their trainability at even larger depths of 64 and 80 layers (see Fig. 9 in appendix). In fact, the improved performance and training speed-up of models with balanced initialization as opposed to models with standard initialization is upheld even more so for very deep networks.

Apart from Cora, Citeseer and Pubmed, the other datasets are considered heterophilic in nature and standard GNN models do not perform very well on them. State-of-the-art performance on these datasets is achieved by specialized models. We do not compare with them for two reasons: i) they do not employ an attention mechanism, which is the focus of our investigations, but comprise various special architectural elements and ii) our aim is not to outperform the SOTA, but rather highlight a mechanism that underlies the learning dynamics and can be exploited to improve trainability. We also demonstrate the effectiveness of a balanced initialization in training deep GATs in comparison to LipschitzNorm , a normalization technique proposed specifically for training deep GNNs with self-attention layers such as GAT (see Table 6 in appendix).

AdamAdam, the most commonly used optimizer for GNNs, stabilizes the training dynamics and can compensate partially for problematic initializations. However, the drop in performance with depth, though smaller, is inevitable with unbalanced initialization. As evident from Figure 7, \(_{O}\) initialized models achieve higher accuracy than or at par with Xavier initialization and converge in fewer epochs. As we observe with SGD, this difference becomes more prominent with depth, despite the fact that Adam itself significantly improves the trainability of deeper networks over SGD. Our argument of small relative gradients (see Eq.(8)) also applies to Adam. We have used the initial learning rates reported in the literature : \(0.005\) for Cora and Citeseer, and \(0.01\) for Pubmed for the \(2\) and \(5\) layer networks. To allow stable training of deeper networks, we reduce the initial learning rate by a factor \(0.1\) for the \(10\) and \(20\) layer networks on all three datasets.

Architectural variationsWe also consider other architectural variations such as employing ELUs instead of ReLUs, using multiple attention heads, turning off weight sharing, and addition of standard elements such as weight decay and dropout. In all cases, the results follow a similar tendency as already reported. Due to a lack of space, we defer the exact results to the appendix. These variations further increase the performance of networks with our initialization proposals, which can therefore be regarded as complementary. Note that residual skip connections between layers are also supported

Figure 7: GAT with \(64\) hidden dimensions trained using Adam.

in a balanced initialization provided their parameters are initialized with zero. However, to isolate the contribution of the initialization scheme and validate our theoretical insights, we have focused our exposition on the vanilla GAT version.

LimitationsThe derived conservation law only applies to the self-attention defined in the original GAT and GATv2 models, and their architectural variations such as \(\)GAT (see Fig. 10 in appendix). Note that the law also holds for the non-attentive GCNs (see Table 7 in appendix) which are a special case of GATs (where the attention parameters are simply zero). Modeling different kinds of self-attention such as the dot-product self-attention in  entails modification of the conservation law, which has been left for future work.

## 4 Discussion

GATs  are powerful graph neural network models that form a cornerstone of learning from graph-based data. The dynamic attention mechanism provides them with high functional expressiveness, as they can flexibly assign different importance to neighbors based on their features. However, as we slightly increase the network depth, the attention and feature weights face difficulty changing during training, which prevents us from learning deeper and more complex models.

We have derived an explanation of this issue in the form of a structural relationship between the gradients and parameters that are associated with a feature. This relationship implies a conservation law that preserves a sum of the respective squared weight and attention norms during gradient flow. Accordingly, if weight and attention norms are highly unbalanced as is the case in standard GAT initialization schemes, relative gradients for larger parameters do not have sufficient room to increase.

This phenomenon is similar in nature to the neural tangent kernel (NTK) regime [24; 54], where only the last linear layer of a classic feed-forward neural network architecture can adapt to a task. Conservation laws for basic feed-forward architectures [15; 3; 32] do not require an infinite width assumption like NTK-related theory and highlight more nuanced issues for trainability. Furthermore, they are intrinsically linked to implicit regularization effects during gradient descent . Our results are of independent interest also in this context, as we incorporate the attention mechanism into the analysis, which has implications for sequence models and transformers as well.

One of these implications is that an unbalanced initialization hampers the trainability of the involved parameters. Yet, the identification of the cause already contains an outline for its solution. Balancing the initial norms of feature and attention weights leads to more effective parameter changes and significantly faster convergence during training, even in shallow architectures. To further increase the trainability of feature weights, we endow them with a balanced orthogonal looks-linear structure, which induces perfect dynamical isometry in perceptrons  and thus enables signal to pass even through very deep architectures. Experiments on multiple benchmark datasets have verified the validity of our theoretical insights and isolated the effect of different modifications of the initial parameters on the trainability of GATs.

## 5 Acknowledgements

We gratefully acknowledge funding from the European Research Council (ERC) under the Horizon Europe Framework Programme (HORIZON) for proposal number 101116395 SPARSE-ML.