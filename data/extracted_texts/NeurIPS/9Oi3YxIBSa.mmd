# Loss Decoupling for Task-Agnostic

Continual Learning

 Yan-Shuo Liang and Wu-Jun Li

National Key Laboratory for Novel Software Technology,

Department of Computer Science and Technology, Nanjing University, P. R. China

liangys@smail.nju.edu.cn,liwujun@nju.edu.cn

Wu-Jun Li is the corresponding author.

###### Abstract

Continual learning requires the model to learn multiple tasks in a sequential order. To perform continual learning, the model must possess the abilities to maintain performance on old tasks (stability) and adapt itself to learn new tasks (plasticity). Task-agnostic problem in continual learning is a challenging problem, in which task identities are not available in the inference stage and hence the model must learn to distinguish all the classes in all the tasks. In task-agnostic problem, the model needs to learn two new objectives for learning a new task, including distinguishing new classes from old classes and distinguishing between different new classes. For task-agnostic problem, replay-based methods are commonly used. These methods update the model with both saved old samples and new samples for continual learning. Most existing replay-based methods mix the two objectives in task-agnostic problem together, inhibiting the models from achieving a good trade-off between stability and plasticity. In this paper, we propose a simple yet effective method, called loss decoupling (LODE), for task-agnostic continual learning. LODE separates the two objectives for the new task by decoupling the loss of the new task. As a result, LODE can assign different weights for different objectives, which provides a way to obtain a better trade-off between stability and plasticity than those methods with coupled loss. Experiments show that LODE can outperform existing state-of-the-art replay-based methods on multiple continual learning datasets.

## 1 Introduction

Continual learning requires the model to learn multiple tasks in a sequential order . However, neural network models suffer from a phenomenon called catastrophic forgetting (CF) , in which the performance of the network on the old tasks degrades significantly after it learns a new task. To address this challenge and enable continual learning, the model must possess the abilities to maintain performance on old tasks (stability) and adapt itself to learn new tasks (plasticity). Nevertheless, an excess of stability or plasticity can interfere with the other , and hence the model needs to make a trade-off between stability and plasticity .

There exist two different kinds of problems, task-agnostic problem and task-aware problem, in continual learning. Task-agnostic problem  in continual learning is a challenging problem, in which task identities are not available in the inference stage and hence the model must learn to distinguish all the classes in all the tasks. In contrast, task-aware problem  in continual learning enables the model to get task identities in the inference stage. Therefore, the model for task-aware problem only needs to distinguish the classes belonging to the same task. The difference between task-agnostic problem and task-aware problem  shows that the model in task-agnostic problemneeds to learn two objectives for learning a new task, including distinguishing new classes from old classes (called new/old class distinction) and distinguishing between different new classes (called new class distinction). Figure 1 (a) illustrates these two learning objectives.

Many methods have been proposed for continual learning, including regularization-based methods [45; 20; 2], expansion-based methods [34; 18; 23] and replay-based methods [3; 10; 4]. For task-agnostic problem, replay-based methods are commonly used. These methods use a memory buffer to maintain a small portion of samples from the old classes. When learning a new task, the model retrieves old samples from memory and updates the parameters with both new and old samples. As illustrated in Figure 1 (b) from the methodological perspective, most existing continual learning methods [9; 7; 43] mix the two objectives in task-agnostic problem together (discussed later in Section 3.1). But these two learning objectives may cause different degrees of forgetting in continual learning and thus different trade-off strategies between stability and plasticity are required for these two learning objectives. More specifically, if a new learning objective leads to more forgetting, a good continual learner should pay more attention to the model's stability. On the contrary, if a new learning objective leads to less forgetting, a good continual learner should pay more attention to the model's plasticity for this objective. However, when the model mixes different learning objectives together, adjusting one of the learning objectives may influence others, inhibiting the model from achieving a good trade-off between stability and plasticity.

In this paper, we propose a simple yet effective method, called loss decoupling (LODE), for task-agnostic continual learning. The main contributions of this paper are listed as follows:

* By deeply analyzing the impacts of new/old class distinction and new class distinction, we find that these two learning objectives cause different degrees of forgetting. Therefore, mixing these two objectives together is detrimental for the model to make a good trade-off between stability and plasticity.
* LODE separates the two objectives for the new task by decoupling the loss of the new task. As a result, LODE can assign different weights for different objectives, which provides a way to obtain a better trade-off between stability and plasticity than those methods with coupled loss.
* Experiments show that LODE can outperform existing state-of-the-art replay-based methods on multiple continual learning datasets.

## 2 Related Work

Continual learning can be offline or online. In offline continual learning setting [17; 32; 44; 24], the model receives the entire dataset of a new task and updates its parameters multiple times over this dataset. In online continual learning setting [11; 4; 10], the data from each task is sequentially concatenated as a non-stationary data stream and each data of each task can only appear once in the

Figure 1: (a) shows two different learning objectives for learning the new task. The left side of (b) shows the trade-off between stability and plasticity in most existing methods, which mix different learning objectives. The right side of (b) shows that our method separates the learning of a new task into two objectives and hence separately considers the trade-off between stability and plasticity for these two learning objectives. y-axis in (b) represents the model’s abilities, including plasticity and stability.

data stream. Hence, the model can only receive a mini-batch of samples from the data stream at a time and update its parameters based on this mini-batch.

Different types of methods have been proposed for continual learning, including regularization-based methods [45; 20; 2], expansion-based methods [34; 18; 23] and replay-based methods [3; 10; 4]. For task-agnostic problem, replay-based methods are commonly used. Some replay-based methods [40; 42] achieve replay by learning a generative model for generating old samples. However, learning a good generative model is challenging in some settings, like online continual learning . Some replay-based methods like experience replay (ER)  maintain a memory buffer to save old samples and replay them with new samples. Some methods combine experience replay with knowledge distillation . Specifically, these methods store either a old model  or the outputs of the old models . Some methods  like experience replay with asymmetric cross entropy (ER-ACE)  improve ER by changing the loss of the new task so that the model can avoid large representation drift. Some methods like error-sensitive modulation experience replay (ERMER)  and complementary learning system experience replay (CLS-ER)  use an extra set of model's parameters to aggregate the knowledge of different tasks. Other methods try to search for valuable samples  or optimize the distribution of the memory [19; 41] to overcome forgetting. However, all these methods mix two new learning objectives in Figure 1 (a), inhibiting the models from achieving a good trade-off between stability and plasticity.

## 3 Methodology

In this section, we first formulate the problem in continual learning. Then, we deeply analyze the problem of mixing different learning objectives together in existing methods. After that, we propose our method called loss decoupling (LODE), which can be used in both offline and online continual learning settings. Finally, we discuss the relation between LODE and existing methods.

### Problem Formulation

Continual learning requires the model to learn from multiple tasks in a sequential order. We use \(_{t}=\{_{i}^{t},y_{i}^{t}\}^{N_{t}}\) to denote the dataset of the \(t\)-th task, where \(_{i}^{t}\) denotes an input sample and \(y_{i}^{t}\) denotes its label. \(N_{t}\) denotes the number of samples for task \(t\). The total number of tasks is denoted by \(T\). When the model learns a new task, the model can obtain limited or no data from the old tasks, potentially leading to catastrophic forgetting. In this work, we consider a challenging continual learning problem called task-agnostic problem, in which task identities are not available in the inference stage. In the task-agnostic problem, when the model learns on a new task \(t\), some new classes are presented to the model. The model must possess the abilities to maintain performance on old classes (stability) and adapt itself to learn new classes (plasticity).

Replay-based methods [16; 7] maintain a memory buffer \(\) with limited size to store a small portion of old samples. When receiving a mini-batch of new samples \(_{t}\) from a new task \(t\), the model retrieves a mini-batch of samples \(_{}\) from \(\) and replays them with the new samples \(_{t}\) to achieve a trade-off between stability and plasticity. The losses used in most existing replay-based methods can be written as follows:

\[=_{t}|}_{i=1}^{|_{t}|}_{new}(f_{}(_{i}^{t}),y_{i}^{t})+_{ }|}_{i=1}^{|_{}|}_{rep}(f_{ }(_{i}^{}),y_{i}^{}).\] (1)

Here, \(_{new}\) is the loss for the new task and is mainly for the plasticity of the model. In contrast, \(_{rep}\) is the replay loss and is mainly for the stability of the model. We follow most existing works [7; 43] and assume that \(_{new}\) is a cross-entropy loss. \(_{rep}\) usually varies for different methods. For example, \(_{rep}\) can be a cross-entropy loss [3; 9] or a combination of cross-entropy and regularization losses [7; 5; 36]. Based on Figure 1 (a), we can find that \(_{new}\) is not only related to new/old class distinction, but also related to new class distinction in the task-agnostic problem. Hence, existing methods with the loss in (1) mix the two different learning objectives (new/old class distinction and new class distinction) together.

Note that there exists a type of replay-based methods [43; 17; 14; 38] directly sampling training data from \(_{t}\), which makes their losses can not be written in the form shown in (1). In some challenging continual learning settings like online continual learning setting, the model cannot access the entire dataset of a new task. At this time, it is impossible for the model to sample from \(_{t}\). On the contrary, loss in (1) can be adapted to online continual learning by sampling \(_{t}\) from the data stream \(\) and sampling \(_{}\) from \(\) separately. Hence, in this work, we focus on the methods whose loss function has the form in (1). In Section 4 and Appendix, we also compare our method with the methods that sample training data from \(_{t}\).

### Analyzing Learning Objectives by Decoupling Loss

When learning the \(t\)-th task, we assume there are \(m\) old classes, which are denoted as \(_{o}=\{1,2,...,m\}\). We also assume there are \(n\) new classes, which are denoted as \(_{n}=\{m+1,m+2,...,m+n\}\). Then, for a given new sample \((,y)\) where \(y_{n}\), we can compute its loss \(_{new}\) as

\[_{new}(f_{}(),y)=_{ce}(f_{}(),y)=-()}{_{i=1}^{m+n}(o_{i})} ).\] (2)

Here, \(_{ce}()\) denotes the cross-entropy loss. \([o_{1},o_{2},...,o_{m},o_{m+1},...,o_{m+n}]\) denotes the logits outputed by \(f_{}()\). We decouple the loss \(_{new}(f_{}(),y)\) according to the two learning objectives in Figure 1 (a):

\[_{new}(f_{}(),y)= -()}{_{i=m+1}^{m+n}(o_{i})} )-(^{m+n}(o_{i})}{_{i=1}^{m+n}(o_ {i})})\] (3) \[= _{ce}(f_{}(),y;_{n})+_{n}(f_{ }()).\]

Here, we use \(_{ce}(;_{n})\) to denote the cross-entropy loss restricted to new classes \(_{n}\). Obviously, \(_{ce}(f_{}(),y;_{n})\) is related to new class distinction. \(_{n}(f_{}())\) is related to new/old class distinction. Note that both \(_{ce}(f_{}(),y;_{n})\) and \(_{n}(f_{}())\) are for the plasticity of the model and may cause catastrophic forgetting. Furthermore, loss \(_{ce}(f_{}(),y;_{n})\) and loss \(_{n}(f_{}())\) have the same weight in (1) due to the coupling property.

We use experience replay (ER) , which is one of the most popular replay-based methods and can be expressed in the form of (1), to evaluate the impact of \(_{ce}(;_{n})\) and \(_{n}()\). Specifically, we first let the model learn on the first task through valina stochastic gradient descent . Then, before learning the subsequent tasks, we remove one of the two losses in (3) from ER and analyze the forgetting of the first task. The experiments are conducted on two datasets Seq-CIFAR10 and Seq-CIFAR100, which will be introduced in Section 4. The experimental settings also follow the descriptions in Section 4.1. Figure 2 (a) and Figure 2 (b) show the accuracy of the first task when the model learns subsequent tasks. Here, 'Remove \(_{ce}(;_{n})\)' means that we remove \(_{ce}(;_{n})\) from (1) after learning the first task. 'Remove \(_{n}()\)' means that we remove \(_{n}()\) from (1) after learning the first task. The results show that removing \(_{n}()\) results in less forgetting of the first task than removing \(_{ce}(;_{n})\). In other words, \(_{n}()\) leads to more forgetting than \(_{ce}(;_{n})\).

It is intuitively reasonable that \(_{n}()\) leads to more forgetting than \(_{ce}(;_{n})\). First, since replay-based methods usually keep limited samples in memory, when the model learns a new task, it has access to much fewer samples from the old classes than from the new classes. Therefore, utilizing loss \(_{n}()\) to learn to distinguish between new classes and old classes introduces a risk of biasing the model towards the new classes, potentially leading to serious catastrophic forgetting. In contrast, loss

Figure 2: (a) and (b) show the variation of the first task’s accuracy on different datasets. (c) and (d) show the distribution of different losses over different datasets before learning the second task. Here, the frequency corresponding to each loss value represents the number of samples with this loss value.

\(_{ce}(;_{n})\) is independent of the old classes, thereby avoiding introducing a risk of biasing the model towards the new classes. Second, in Figure 2 (c) and Figure 2 (d), we also show the value of the losses \(_{ce}(;_{n})\) and \(_{n}()\) before using ER to learn task 2. We can find that the value of \(_{n}()\) is larger than the value of \(_{ce}(;_{n})\) on both Seq-CIFAR10 and Seq-CIFAR100. This finding is consistent with existing works [8; 36], which suggest that larger losses for the new task may result in larger feature drift, leading to more forgetting. In Appendix, we also provide the TSNE visualizations when either \(_{n}()\) or \(_{ce}(;_{n})\) is removed, further confirming that \(_{n}()\) leads to more forgetting than \(_{ce}(;_{n})\).

Since \(_{n}()\) leads to more forgetting than \(_{ce}(;_{n})\), treating these two losses equally is not reasonable. In particular, based on the above analysis, we can find that a good continual learner should assign a larger weight to \(_{ce}(;_{n})\) and a smaller weight to \(_{n}()\). However, loss in (1) fails to achieve this goal due to the coupling property.

### Loss Decoupling for Continual Learning

Section 3.2 has demonstrated the impact of different learning objectives on the model's forgetting and the issue of the coupling property in existing replay-based methods. To address this issue, we propose a new method called loss decoupling (LODE), which removes the coupling property present in existing methods.

Specifically, our LODE uses the following loss to perform continual learning:

\[=_{t}|}_{i=1}^{|_{t}|}( _{1}_{ce}(f_{}(_{i}^{t}),y_{i}^{t};_{n}) +_{2}_{n}(f_{}(_{i}^{t}),y_{i}^{t}))+_{}|}_{i=1}^{|_{}|}_{rep}(f_{}(_{i}^{}),y_{i}^{}).\] (4)

Here, \(_{1}\) and \(_{2}\) are two coefficients that control the weight of the two different learning objectives. The finding in Section 3.2 shows that LODE should set \(_{2}\) to be smaller than \(_{1}\), to make the model achieve a better trade-off between stability and plasticity than existing methods usually with coupled loss. Furthermore, since \(_{n}()\) is for new/old class distinction, LODE sets \(_{2}\) proportional to the ratio \(_{n}|}{|_{n}|}\) to make the model not bias toward old or new classes. In contrast, since \(_{ce}(;_{n})\) is only related to the new classes, LODE sets \(_{1}\) to be a constant value. More specifically, LODE sets \(_{1}\) and \(_{2}\) as

\[_{1}=C,\ \ _{2}=_{n}|}{|_{o}|}.\] (5)

Here, \(C\) and \(\) are two hyperparameters. Note that when the number of tasks increases, the number of old classes also increases. In particular, when the number of old tasks is large, the number of old classes \(|_{o}|\) is usually much larger than the number of new classes \(|_{n}|\). At this time, \(_{2}\) is much smaller than \(_{1}\). Setting \(_{2}\) to be as large as \(_{1}\), or setting \(_{1}\) to be as small as \(_{2}\) fails to make the model achieve a good trade-off between stability and plasticity, which will be verified in the experiment.

Note that we do not specify the form of \(_{rep}\) in (4). Therefore, our method can be combined with many replay-based methods with form (1) and improve these methods. Here, we give some examples which combine LODE with different state-of-the-art continual learning methods.

Combining LODE with ER and DER++Both experience replay (ER)  and dark experience replay++ (DER++)  can be written in the form of (1). Therefore, the combinations of our LODE with these two methods are direct. Specifically, given a new batch of samples \(_{t}\), the model compute \(_{ce}(;_{n})\) and \(_{n}()\) according to (3). Then, the model computes \(_{rep}\) with the old samples \(_{}\) through the specific formulation in ER or DER++. Finally, the model can get the loss (4).

Combining LODE with ESMERA recent method called error sensitive modulation experience replay (ESMER)  suggests that the model should learn more from smaller losses to avoid large feature drift. The loss in ESMER is slightly different from the loss in (1). Specifically, ESMER assigns different weights to different new samples in \(_{t}\) according to their loss values. Although the loss in ESMER is slightly different from the loss in (1), it still mixes the two different learning objectives together. Therefore, we can combine ESMER with our LODE through a similar form to (4). The decoupled loss for ESMER can be written as follows:

\[=_{t}|}_{i=1}^{|_{t}|}\!w_{i}( _{1}_{ce}(f_{}(_{i}^{t}),y_{i}^{t};_{n})+ _{2}_{n}(f_{}(_{i}^{t}),y_{i}^{t}))+_{}|}_{i=1}^{|_{}|}_{ rep}(f_{}(_{i}^{}),y_{i}^{}).\] (6)

Here, \(w_{i}\) is the weight assigned to new sample \(i\) in ESMER. We can find that the loss in (6) not only modulates weights for different new samples but also modulates weights for the two different learning objectives.

Algorithm 1 gives the whole learning process of LODE.

```
1:Input: a sequence of tasks with datasets \(\{_{1},...,_{T}\}\), a neural network model \(f_{}()\).
2:Output: a learned neural network model \(f_{}()\).
3:while Get a mini-batch of samples \(_{t}\) from a task \(t\)do
4: Sample a mini-batch \(_{}\) from memory \(\);
5: Specify the weights for the two different learning objectives by (5);
6: Get the losses for learning objective through (3);
7: Compute the final loss through (4).
8: Perform backward propagation and update the model \(f_{}()\) through SGD;
9: Update memory \(\) with \(_{t}\) through some memory update methods;
10:endwhile ```

**Algorithm 1** Loss Decoupling (LODE) for Continual Learning

### Relation with Existing Methods

The loss in many existing methods has the coupling property like the loss in (1). However, some methods, from the perspective of this work, use the losses that show a certain degree of decoupling property. For example, the loss in ER-ACE  can be written as

\[=_{t}|}_{i=1}^{|_{t}|}_{ce}(f_{}(_{i}^{t}),y_{i}^{t};_{n})+_{}|}_{i=1}^{|_{}|} _{rep}(f_{}(_{i}^{}),y_{i}^{}).\] (7)

It can be seen from (7) that this method removes the term \(_{n}()\) from (3) and only retains \(_{ce}(;_{n})\). In particular, loss in (7) is a special case of the loss in (4). More specifically, we can get the loss in (7) by setting \(_{1}=1\) and \(_{2}=0\) in (4). SSIL  has a similar form to that in (7) but uses a different \(_{rep}\) compared to ER-ACE. Due to the lack of \(_{n}()\), the loss in (7) can only leverage the new classes' samples kept in memory to learn the objective of new/old class distinction. In experiments, we will show that setting \(_{2}=0\) in (4) performs worse than setting \(_{2} 0\) in (4).

Some existing methods also incorporate the idea of separating objectives in continual learning. However, these methods are primarily designed for task-aware problem. For instance, bilevel memory system with knowledge projection (BMKP)  requires the task identities to choose the corresponding knowledge representations during testing, making it unsuitable in the task-agnostic problem. Space decoupling (SD)  does not explicitly mention that it only considers task-aware problem, but its experiments completely follow some task-aware methods [35; 26], indicating its focus on task-aware problem.

## 4 Experiments

### Experimental Settings

DatasetsWe use three popular datasets for evaluation, including Seq-CIFAR10 , Seq-CIFAR100 , and Seq-TinyImageNet . Seq-CIFAR10 consists of 5 disjoint tasks with each task having 2 classes and 10k training samples. Seq-CIFAR100 consists of 5 disjoint tasks with each task having 20 classes and 10k training samples. Seq-TinyImageNet consists of 10 disjoint tasks with each task having 20 classes and 10k training samples. The statistics of different datasets are given in Appendix. All the experiments are for the task-agnostic problem.

BaselinesWe compare our method with many state-of-the-art replay-based continual learning methods, including incremental classifier and representation learning (iCaRL) , bias correction (BIC) , separated softmax for incremental learning (SSIL) , experience replay (ER) , maximally interfere retrieval (MIR) , dark experience replay++ (DER++) , supervised contrastive replay (SCR) , proxy-based contrastive replay (PCR) , experience replay with asymmetric cross entropy (ER-ACE) , error sensitive modulating experience replay (ESMER) , complementary learning system experience replay (CLS-ER) , and task-specific attention modules in lifelong learning (TAMiL) . For CLS-ER, we follow the existing method  and implement it with a single exponential moving average model. We also include two methods without continual learning, _joint_ and _finetune_, in the comparison. Here, _joint_ denotes the method which learns all the tasks jointly while _finetune_ denotes the method which learns all the tasks sequentially without any memory. The accuracy of _joint_ can be treated as the accuracy upper-bound and the accuracy of _finetune_ can be treated as the accuracy lower-bound. Among the methods we mentioned above, some methods only maintain a single learning model to perform continual learning, while others require an extra model in memory for knowledge integration or distillation. Since keeping more models requires more memory, and memory cost is an important metric in continual learning , we group different methods by whether keeping extra model (refer to Table 1) to make a more fair comparison.

Architecture and Training DetailsWe follow existing continual learning works [7; 6] and use standard ResNet18 as the neural network architecture in all the experiments unless otherwise stated. The experiments are built on top of the mammoth  continual learning repository in PyTorch like many existing works [7; 6]. We use stochastic gradient descent (SGD) to optimize the parameters. The batch size and replay size are set to 32 to follow the existing continual learning works [7; 6]. We also follow existing methods [7; 5] to set memory as 500 and 5120 for all the datasets. The hyperparameters are selected through a small validation set. For the experiments of all the methods on all the datasets, we apply random crops and horizontal flips to both newly coming samples and buffered (saved) samples like existing works [7; 5]. For each of our experiments, we report the average and standard deviation of the mean test accuracy of all the tasks across 5 runs with different seeds. More details of training and hyperparameters for different methods are given in Appendix.

### Experimental Results

#### 4.2.1 Accuracy

Table 1 shows the results of different methods on different datasets. Here, 'Keeping Extra Model' represents whether a method needs to keep an extra model for continual learning, as described

    &  &  &  \\  & & & & & \\   & _joint_ & \(91.86 0.26\) & \(70.10 0.60\) & \(59.82 0.31\) \\  & _finetune_ & \(19.65 0.03\) & \(17.41 0.09\) & \(8.13 0.04\) \\   & **Buffer Size** & \(500\) & \(5120\) & \(500\) & \(5120\) & \(500\) & \(5120\) \\   & SCR  & \(57.95 1.57\) & \(82.47 0.44\) & \(23.06 0.22\) & \(45.02 0.67\) & \(8.37 0.26\) & \(18.20 0.48\) \\  & PCR  & \(65.74 3.29\) & \(82.58 0.42\) & \(28.38 0.46\) & \(52.51 1.61\) & \(11.88 1.61\) & \(26.39 1.64\) \\  & MIR  & \(63.93 0.39\) & \(83.73 0.97\) & \(27.80 0.52\) & \(53.73 0.82\) & \(11.22 0.43\) & \(30.60 0.40\) \\  & ER-ACE  & \(68.45 1.78\) & \(83.49 0.40\) & \(40.67 0.06\) & \(58.56 0.91\) & \(17.73 0.36\) & \(37.99 0.17\) \\  & ER  & \(61.78 0.72\) & \(83.64 0.95\) & \(27.69 0.58\) & \(53.86 0.57\) & \(10.36 0.11\) & \(27.54 0.30\) \\  & LODE (ER) & \(68.87 0.71\) & \(83.73 0.48\) & \(41.52 1.22\) & \(58.59 0.48\) & \(17.77 1.03\) & \(38.34 0.04\) \\  & DER++  & \(73.29 0.96\) & \(85.66 0.14\) & \(42.08 1.71\) & \(62.73 0.58\) & \(19.28 0.61\) & \(39.72 0.47\) \\  & LODE (DER++) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & CLS-ER  & \(70.73 0.54\) & \(\) & \(51.21 0.84\) & \(60.17 0.38\) & \(29.44 1.66\) & \(45.66 0.47\) \\  & TAMiL  & \(74.25 0.31\) & \(84.82 1.77\) & \(50.62 0.23\) & \(63.77 0.43\) & \(27.83 0.41\) & \(43.00 0.56\) \\  & iCaRL  & \(61.60 2.03\) & \(72.01 0.62\) & \(95.09 0.95\) & \(54.23 0.28\) & \(20.01 0.50\) & \(30.34 0.18\) \\  & BIC  & \(52.63 2.46\) & \(79.89 1.49\) & \(37.06 0.60\) & \(60.43 0.61\) & \(29.82 0.03 Section 4.1. We combine our LODE with ER, DER++ and ESMER, which can be written in the form of (1). We use LODE (ER), LODE (DER++) and LODE (ESMER) to denote them, respectively. Our experimental results confirm that the integration of our LODE method improves the performance of each combined method. For example, when compared to DER++ with buffer size 500, LODE (DER++) exhibits a \(2.16\%\) improvement on Seq-CIFAR10, a \(4.23\%\) improvement on Seq-CIFAR100 and a \(1.87\%\) improvement on Seq-TinyImageNet. Notably, LODE (DER++) achieves the best performance on Seq-CIFAR10. LODE (ESMER) achieves the best performance on Seq-CIFAR100 and Seq-TinyImageNet.

Figure 3 (a) and Figure 3 (b) show the variation of accuracy on Seq-CIFAR10 and Seq-CIFAR100 after the learning of each task. As we can see, LODE improves DER++ and ESMER at the end of each task. Figure 3 (c) and Figure 3 (d) show the results on Seq-CIFAR100 with different numbers of tasks. When there are 4 tasks in Seq-CIFAR100, each task consists of 25 exclusive classes. Similarly, when there are 10 tasks in Seq-CIFAR100, each task consists of 10 exclusive classes. As we can see, when the number of tasks varies, LODE still gives improvements on different methods. In Appendix, we show the results of more methods on Seq-CIFAR100 with different numbers of tasks.

#### 4.2.2 Hyperparameter Analysis

We analyze the hyperparameters in LODE. We choose LODE (DER++) and LODE (ESMER) to analyze hyperparameters, as they represent the best performance of methods that retain or do not retain an extra model for continual learning, respectively.

We first vary the value of \(\) in (5) to show its impact on the performance of the model. Figure 4 (a) and Figure 4 (b) give the analysis on Seq-CIFAR10 and Seq-CIFAR100. Note that when \(=0\), \(_{2}=0\) and the weight of \(_{n}()\) in (4) is always zero. At this time, the loss in (4) degenerates to the loss in (7) and the model fails to get satisfactory performance. When the value of \(\) increases, \(_{2}\) also increases and the performance of the model first increases and then decreases. This phenomenon is intuitively reasonable since a larger weight for \(_{n}()\) leads to more forgetting and thus influences the overall model performance.

We also vary the value of \(C\) in (5) to show its impact on the performance of the model. Figure 4 (c) and Figure 4 (d) show the results of varying \(C\) on Seq-CIFAR10 and Seq-CIFAR100, respectively. We can find that the performance of the model decreases significantly when \(C\) is close to zero. In

Figure 4: (a) and (b) show the variation of the accuracy for different \(\). (c) and (d) show the variation of the accuracy for different \(C\).

Figure 3: (a) and (b) show the variation of the accuracy for different methods on Seq-CIFAR10 and Seq-CIFAR100. (c) and (d) show the variation of accuracy on Seq-CIFAR100 with different number of tasks.

particular, the model gets the best performance when the value of \(C\) is between 1 and 5. Since \(\) and \(C\) are highly different when the model achieves the best performance, decoupling the loss through (3) is necessary for continual learning.

#### 4.2.3 Ablation Study

We change the value of \(_{1}\) and \(_{2}\) to show the effectiveness of setting \(_{1}\) and \(_{2}\) through (5). We first set the value of \(_{1}=_{2}\) in (4) to remove the decoupling property. There are two possibilities to set \(_{1}=_{2}\). The first possibility is to set \(_{1}=_{2}=C\) and the second possibility is to set \(_{1}=_{2}=_{n}|}{|_{o}|}\). Table 2 shows the results of these two possibilities, which are significantly inferior to our method. This means that separating the two different objectives by decoupling the loss of the new task is necessary for the model to achieve good performance. In Table 2, we also show the result of a variant by exchanging the value of \(_{1}\) and \(_{2}\), which means \(_{1}=_{n}|}{|_{o}|}\) and \(_{2}=C\). We can find that the performance of this variant is still significantly inferior to our method.

#### 4.2.4 Online Continual Learning

We also perform experiment for the online continual learning setting  where the datasets of different tasks are concatenated to a non-stationary data stream. Since online continual learning is more challenging than offline continual learning, existing methods  usually use larger memory in online continual learning, especially for challenging datasets such as Seq-CIFAR100 and Seq-TinyImageNet. Hence, we set the memory size to 5120 for all the datasets. To follow existing online continual learning methods , the experimental settings we use here are different from those introduced in Section 4.1. These settings are introduced in Appendix.

Table 3 shows the results of different methods. We exclude those methods that have been implemented only in offline continual learning or those that have demonstrated lower performance in online continual learning in previous works . Similar to that in the offline continual learning setting, we can find that the integration of our LODE method also significantly improves the overall accuracy of each combined method. For example, when compared with DER++, LODE (DER++) exhibits a \(1.70\%\) improvement on Seq-CIFAR10, a \(3.1\%\) improvement on Seq-CIFAR100 and a \(4.9\%\) improvement on Seq-TinyImageNet.

   } &  &  \\   & Seq-CIFAR10 & Seq-CIFAR100 & Seq-CIFAR100 & Seq-CIFAR100 \\  \(_{1}=C,_{2}=_{n}|}{|_{o}|}\) (Ours) & **75.45\(\)9.90** & **46.31\(\)1.01** & **74.53\(\)9.95** & **55.06\(\)0.35** \\ \(_{1}=_{2}=_{n}|}{|_{o}|}\) & \(71.18\)0.80 & \(37.49\)1.79 & \(73.41\)0.40 & \(45.64\)0.87 \\ \(_{1}=_{2}=C\) & \(73.80\)0.72 & \(42.08\)1.71 & \(73.08\)0.81 & \(52.37\)0.87 \\ \(_{1}=_{n}|}{|_{o}|},_{2}=C\) & \(73.19\)0.15 & \(40.79\)0.12 & \(72.38\)0.24 & \(51.86\)0.35 \\   

Table 2: Ablation study on Seq-CIFAR10 and Seq-CIFAR100.

    &  &  &  \\  & & & \\   & SCR  & \(69.49\)3.02 & \(36.09\)0.82 & \(20.04\)1.24 \\  & PCR  & \(73.28\)1.83 & \(34.89\)0.67 & \(23.84\)0.60 \\   & ER-ACE  & \(69.17\)1.64 & \(35.24\)0.51 & \(23.42\)0.34 \\   & MIR  & \(71.10\)1.59 & \(35.08\)1.32 & \(20.64\)1.17 \\   & ER  & \(67.93\)2.04 & \(34.40\)1.13 & \(21.14\)0.72 \\   & LODE (ER) & \(69.63\)1.41 & \(36.91\)1.38 & \(24.31\)0.82 \\   & DER++  & \(72.30\)0.99 & \(34.72\)1.51 & \(20.40\)1.02 \\   & LODE (DER++) & **74.00\(\)0.08** & **37.82\(\)1.16** & **25.30\(\)1.80** \\   

Table 3: Classification results which are averaged across \(5\) runs in the online continual learning setting.

Conclusion

In this work, we propose a new method called loss decoupling (LODE) for continual learning. Different from most existing replay-based methods which mixes the two different learning objectives together to learn the new task, LODE separates the two learning objectives for the new task by decoupling the loss of the new task. Experiments show that LODE can achieve a better trade-off between stability and plasticity than other baselines, and thus outperform other state-of-the-art replay-based methods across multiple datasets. Future work will extend LODE to other continual learning problem like task-aware problem and study the effectiveness of LODE for other types of tasks.