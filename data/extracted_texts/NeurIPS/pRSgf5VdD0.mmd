# Learning Partitions from Context

Simon Buchholz

Department for Empirical Inference

Max Planck Institute for Intelligent Systems

Tubingen AI Center

Tubingen, Germany

sbuchholz@tue.mpg.de

###### Abstract

In this paper, we study the problem of learning the structure of a discrete set of \(N\) tokens based on their interactions with other tokens. We focus on a setting where the tokens can be partitioned into a small number of classes, and there exists a real-valued function \(f\) defined on certain sets of tokens. This function, which captures the interactions between tokens, depends only on the class memberships of its arguments. The goal is to recover the class memberships of all tokens from a finite number of samples of \(f\). We begin by analyzing this problem from both complexity-theoretic and information-theoretic viewpoints. We prove that it is NP-complete in general, and for random instances, we show that on the order of \(N(N)\) samples, implying very sparse interactions, suffice to identify the partition. We then investigate the conditions under which gradient flow dynamics of token embeddings can reveal the class structure, finding that this is achievable in certain settings when given on the order of \(N^{2}^{2}(N)\) samples.

## 1 Introduction

Modern machine learning systems are able to learn extremely complicated relations from data. They often rely on learned embeddings of discrete tokens in a continuous space. This is notably true for Large Language Models (LLMs) [9; 17; 25; 23] which encode their input by converting text into a sequence of discrete tokens that are embedded in a high dimensional embedding space and those embeddings are fed into, e.g., a transformer architecture  which allows predicting the next token. But also in the other domains, e.g., in vision, discrete embeddings are frequently used as a component of deep learning architectures [27; 10] as this enables capturing complex concepts that are often discrete.

It was observed that after training, these word embeddings exhibit many interesting structures. The most prominent example probably is the observation that the difference of the Word2Vec embedding vectors of the nouns 'king' and 'queen' approximately equals the difference of the embeddings of'man' and 'woman' [21; 22]. Similarly, it was found that using word similarity as an inductive bias to structure latent spaces helps downstream performance . Thus, a properly structured latent space seems to be an important ingredient to capture the intricate correlations in complex data.

A proper theoretical understanding of such complex models currently remains an elusive goal. However, there have been various attempts to understand various components of deep learning models. Many works investigated the behavior of feedforward-networks in particular focusing on shallow networks [2; 8] and asymptotic regimes [15; 29]. More recently, several works investigated transformer architectures (often focusing on the one layer case with linearized attention mechanism) [16; 1; 30; 12]. On a technical level  is closely related as they study the large depth limit of transformers through the lens of particle systems (however in their case time corresponds to depth, while in our case it corresponds to training time). A feature shared by many of those works is thatlittle structure is assumed on the input, i.e., fixed token embeddings are assumed and often those are even assumed to be isotropic Gaussian.

Here we instead focus on the dynamics of the token embeddings and study how these can recover structure present in the data. There is no ground truth target for the embeddings for large scale models used in deep learning and their embeddings need to capture a variety of nuanced correlations and relations that are hard to formalize. Therefore, we focus on a simplified problem that nevertheless shares important features with more complex real world settings. One general heuristic is that the embeddings contain information about the similarity of tokens. We focus here on the strongest form of similarity, namely equality. Indeed, our central assumption is that tokens can be clustered in a small number of groups such that tokens within a cluster behave exactly the same, i.e., they interact in the same manner with other tokens.

Then the central questions is what assumptions allow us to recover a hidden structure. The crucial feature of this setting is that we only get information about a token by its interaction with other tokens about which we also only learn through their interaction behavior. Moreover, those interactions are typically sparse, i.e., we observe only a small subset of all possible interactions. Note that this setting resembles observations made in the context of collective behavior where a global structure emerges from local interactions [13; 24; 3]. A related question was investigated in  where they study associative memory and also want to identify a hidden structure, however, they learn the class memberships directly through the interaction with a class embedding (and they train the interaction instead of the embedding). In  the dynamics of word embeddings and transformer layers was investigated when the data follows a topic model. This work shares the crucial feature that membership of a word in a certain topic is only transmitted through the co-occurrence with other words from the same topic. In contrast to their work, we here do not focus on learning class membership from token frequencies and in fact consider uniform token distribution. Instead, we view this problem as a logical inference problem: Given a set of facts about a set of tokens can the hidden structure of the tokens discovered.

We summarize the main contributions of the paper as follows:

* We introduce a learning problem that shares important features with learning through the interaction behavior that is crucial for LLMs and complex systems.
* We analyze this problem from a complexity-theoretic viewpoint where we show that it is in general hard, and from an information-theoretic viewpoint where we show, roughly, that for \(N\) different tokens in the alphabet order \(N(N)\) samples are sufficient to identify the latent structure.
* We then carefully investigate the gradient dynamics of token embeddings, finding local recovery of the cluster structure and global recovery for tensor-product functions on the tokens if we have more than \(N^{2}(N)\) samples for an alphabet with \(N\) tokens.

Notation.We write \([N]=\{1,,N\}\) for the set of the first \(N\) integers. The cardinality of a finite set \(A\) is denoted by \(|A|\) and we also denote the standard Euclidean norm of any vector \(v^{d}\) by \(|v|\). The expressions \(_{}(A)\) and \(_{}(A)\) denote the largest and smallest eigenvalue of a symmetric matrix \(A\) respectively. We denote the uniform distribution over a set \(A\) by \((A)\). For two subsets \(A,B^{d}\) we denote by \(A+B=\{a+b:\,a A,b B\}\) their Minkowski sum. We denote the permutation group on \(k\) elements by \(_{k}\). An overview of the used variable names can be found in Appendix A.

## 2 Setting and Motivation

In this section, we illustrate our problem with an example and define the setup more formally. Consider the set of all animals. Those can be grouped into classes such as mammals, birds, or reptiles (in fact there is a rich hierarchical structure which we ignore here). Those groups were conceived by findings sets of animals that share many properties. Once these groups are found, we can predict unobserved properties by first identifying the cluster to which an animal belongs and then predict that the property is shared with animals in the same cluster. Note that this is a specific instance of a general problem in scientific inference, where we want to uncover a hidden grouping of similar entities from sparse observations about these entities.

Here our main motivation, however, stems from the analysis of large language models where a similar problem arises implicitly during training. They are trained by next token prediction, so we do not expect them to learn structure by deductive reasoning such as cows are mammals, and mammals have lungs, so cows have lungs. Instead, their learning signal is whether a token can be replaced by another token for a given context. Thus, it is a natural question whether gradient descent-based training on token embeddings can uncover a hidden cluster structure of the data. Note that if the hidden structure is recovered, then generalization to unseen prompts is possible.

We now introduce our formal setup that captures key aspects of the discussion. We consider \(I\) sets of \(N_{1},N_{2},,N_{I}\) tokens or entities (such as words). For simplicity, we identify these with tokens from the set \([N_{i}]\). For each of the sets \([N_{i}]\) there is a partition \(_{i}\) in \(K_{i}\) classes which we can identify with the set \([K_{i}]\). Then we can encode the partitions through maps \(_{i}:[N_{i}][K_{i}]\) so that the partition is given by \(_{i}=(_{i}^{-1}(k_{i}))_{k_{i}[K_{i}]}\), i.e., \(_{i}\) encode the class membership. We consider the map

\[=_{1}_{I}(n_{1},,n_{I})=(_{1}(n_{1}),,_{I}(n_{I})).\] (1)

This structure is illustrated in Figure 1 Now we assume that there is a function \(g:[K_{1}][K_{2}][K_{I}]\) which depends only on the classes. The map \(f=g\) extends this map to the tokens such that it only depends on the group a token belongs to. In the case where \(g\) (and thus \(f\)) maps to \(\{0,1\}\) this can be interpreted as truth assignments, i.e., a statement consisting of a sequence \((n_{1},,n_{I})\) is true if \(f(n_{1},,n_{I})=1\) and false otherwise and this case is the main motivation of our work. More generally, \(f\) could output the index of a suitable next token where in the \(0\), \(1\) case \(0\) could correspond to a negation while \(1\) to an end of sentence token. Our goal is to learn the partitions \(_{i}\) or, equivalently, \(\) up to a permutation and thereby identify the hidden structure.

We assume that we are given data in the form of samples \((^{s},f(^{s})\) where \(^{s}=(n_{1}^{s},,n_{I}^{s})\) and \(n_{i}^{s}[N_{i}]\). In other words, we try to learn the underlying structure from the interactions of a token with the other tokens, which is the same for every element of the partition. To simplify the notation and statements, we assume in the following that \(N_{i}=N\) and \(K_{i}=K\) for some \(N\), \(K\) and all \(1 i I\). Our main interest concerns the case where \(N\) is large, i.e., there are many entities and \(K\) and \(I\) are small, i.e., the number of groups is small.

Let us summarize several features that this model shares with real world problem, such as learning suitable embeddings in latent space.

* Hierarchical structures, i.e., groups of objects that share certain features, as discussed here, are abundant in language and science.
* We only receive an indirect learning signal for the value of \(_{i}(n)\) through its interaction with other tokens.
* Interactions can be very complex, i.e., here the output depends on the interaction of \(I\) different tokens and ignoring parts of the context makes learning infeasible.

On the other hand, many important features are abstracted away, e.g.:

* Here we assume that tokens from the same element of the partition interact in exactly the same way with other tokens while in reality there are many different partitions of the tokens depending on the broader context (e.g., we can group species by habitat, color, or, size each

Figure 1: Illustration of the setting for \(I=3\) different groups clustered in 3, 2, and 3 subgroups respectively. Samples consist of one element of each group, the dashed lines indicate samples \((1,3,1)\) and \((3,7,6)\).

resulting in different partitions) or there are exceptions, e.g., mammals generally do not lay eggs but the platypus does.
* Many more complex notions of similarity or further properties of embeddings such as a vector space structure are not covered. Also, there can be many uninformative features.
* We do not consider noisy data or errors in this work, which is crucial for real world applications.

## 3 Complexity-Theoretic and Information-Theoretic Analysis

We now study this learning problem in different settings. Let us first briefly discuss complexity-theoretic and information-theoretic properties of the learning problem to understand the general boundaries of this learning task. We first study the information-theoretic viewpoint, i.e., the question of how many samples are necessary to identify \(\) (and potentially \(g\)). We focus on the case where we sample \(\) and the data-samples uniformly at random. To learn an unstructured map \([N]^{I}\) we generally need of the order \(N^{I}(N^{I})\) independent samples (\(N^{I}\) when sampling without replacement).

For the structured setting we show that if \(\) is drawn uniformly at random then generally order \(K^{I}N(N)\) samples are sufficient to learn \(f\) and the partition induced by \(\). In other words, for every token \(n_{i}\) and each of the \(K^{I}\) classes \(^{-1}()\) we need of the order of \((N)\) samples \(\) such that \(()=\) and \(_{i}=n_{i}\). In particular, for \(N K^{I}\) any token will interact only with \(K^{I}(N) N\) other tokens, i.e., a very sparse subset of the other tokens.

We require the following necessary condition for identifiability: For every \(k_{i} k_{i}^{}\) there are \(k_{1},,k_{i-1},k_{i+1},,k_{I}[K]\) such that

\[g(k_{1},,k_{i-1},k_{i},k_{i+1},,k_{I}) g(k_{1}, ,k_{i-1},k_{i}^{},k_{i+1},,k_{I}).\] (2)

Note that if this condition is indeed necessary because if it is not satisfied then it is not possible to distinguish \(_{i}^{-1}(k_{i})\) and \(_{i}^{-1}(k_{i}^{})\). Clearly, we can generally only identify \(\) up to a permutation symmetry, i.e., we can only find \(\) such that there are permutations \(_{i}_{K}\) such that \(_{i}=_{i}_{i}\). We have the following result.

**Theorem 1**.: _Assume that \(g:[K]^{I}\) is a function satisfying the assumption (2). Assume we randomly sample maps \(_{i}\) such that \(_{i}(n_{i})=k_{i}\) with probability \(K^{-1}\) for all \(i\), \(n_{i}\), and \(k_{i}\) and such that \((_{i}(n_{i}))_{i I,n_{i}[N]}\) are independent. Assume we are given \(S\) samples \((,g())\) where \(([N]^{I})\). Then there is a constant \(N_{0}(I,K,)\) such that with probability at least \(1-2e^{-}\) for \(N N_{0}(I,K,)\) and_

\[S 2^{2I+3}IK^{I}N(N)\] (3)

_we can recover \(\) and \(g\) up to permutations of \([K]\)._

This result is a special case of Theorem 6 in Appendix B which shows similar bounds for arbitrary maps \(\) that are not necessarily random. In the more general setting, there are additional dependencies on the size of the preimages \(_{i}^{-1}(k)\). Note that this dependency cannot be avoided because if there is a \(\) such that \(|^{-1}()|=1\) and \(g()=1\) and \(g(^{})=0\) for \(^{}\) then order \(N^{I}\) samples are necessary to find \(\) such that \(()=\) and thus \(\). The general proof idea is to bound the probability that any fixed \(^{}\) is compatible with the dataset. It turns out that it is possible to bound this probability in terms of the partitions distance of the partitions induced by \(\) and \(^{}\). Then we are left with bounding the number of partitions, and we conclude with the union bound. We now show that this bound is essentially tight.

**Theorem 2**.: _Let \(g:[K]^{I}\) be a function such that \(g(1,k_{2},,k_{I})=g(2,k_{2},,k_{I})\) for all \(k_{2},,k_{I}[K]\) except when \(k_{2}=k_{3}==k_{I}=1\). Assume that \(N\) is divisible by \(K\) and that \(|_{i}^{-1}(k)|=N/K\) for all \(i[I]\) and \(k[K]\). Given \(3 S NK^{I-1}(N/K)/4\) samples \((^{s},g(^{s}))\) where \(([N]^{I})\) i.i.d. Then the function \(\) is identifiable with probability at most \(2e^{-}\)._

The proof of this result can be found in Appendix B. Next, we emphasize that while typically a rather small number of samples is sufficient to learn \(\) it can generally be very hard to do this in practice. More concretely, we show that for \(I 3\) even deciding whether there is a map \(=_{1}_{I}:[N]^{I}[K]^{I}\) such that \(f=g\) given access to samples of the form \((^{s},t^{s})\) is NP-complete. We show that this is true even if \(g\) is known, \(I=3\) and \(K=2\).

**Theorem 3**.: _Consider the map \(g:\{0,1\}^{3}\{0,1\}\) given by_

\[g(k_{1},k_{2},k_{3})=_{k_{1}+k_{2}+k_{3}=2}.\] (4)

_Then it is an NP-complete problem to decide given samples of the form \((n_{1}^{s},n_{2}^{s},n_{3}^{s},t^{s})[N]^{3}\{0,1\}\) whether there is a map \(=_{1}_{2}_{3}\) with \(_{i}:[N]\{0,1\}\) such that \(t^{s}=g(n_{1}^{s},n_{2}^{s},n_{3}^{s})\) for all samples._

The proof of this result can be found in Appendix C. Now that we established under what the conditions \(\) can in principle be learned, and clarified that this might be hard in general, we next discuss how we can find \(\) in practice. First, we remark that Theorem 3 rules out the existence of any general fast algorithms to learn \(\). Given the combinatorial nature and the hardness of the problem, it is natural to reformulate the task as a constraint satisfaction problem which can then be solved using standard SAT solvers (see, e.g.,  for a review). Indeed, we can introduce Boolean variables \(t^{i}_{kn}\) for \(i[I]\), \(k[K]\), and \(n[N]\) which encode whether \(_{i}(n)=k\) and \(_{v}\) for every \([K]^{I}\) and \(v(f)\) in the (finite) image of \(f\) that encode whether \(g()=v\). It is then relatively straightforward to then express the conditions for the map \(\) as a constraint satisfaction problem which is satisfiable if and only if there are maps \(\) and \(g\) such that \(t^{s}=g((^{s}))\) holds for all samples. We outline the construction in more detail in Appendix C. We leave the task of developing and studying efficient algorithms for the considered problem for future work because the main motivation of this paper is rather to understand how the complex statistical patterns can be extracted using simple gradient based algorithms. This will be investigated in the next section.

## 4 Analysis of Gradient Descent Dynamics

In this section, we investigate under what conditions the clustering induced by \(\) can be learned using gradient descent on token embeddings. Our main finding is that for uniformly random \(\) and \(S\) sufficiently large gradient descent can be used to uncover or at least preserve the cluster structure of the embeddings. This shows that while the general problem is NP hard typical random instances with sufficiently many samples can be solved with straightforward algorithms quickly. This is in spirit similar to the results found in, e.g., . Let us start by introducing the setting.

Setting.We assume that we have token embeddings for each of the \(I\) sets \([N_{1}]\) to \([N_{I}]\), i.e., we assume that there are vectors \(v(i,n)^{D}\) for some \(D\) and all \(i[I]\), \(n[N]\). Based on these embeddings, we assume that we are given a function \(:^{ID}\) that transforms the embeddings into a prediction. We will abuse notation and write for \([N]^{I}\)

\[()=(v(1,_{1}),,v(I,_{I})),\] (5)

i.e., we will suppress the map from tokens to embeddings in the notation.

Now we consider gradient descent for the embedding vectors using the least square loss on training samples, i.e., the loss of a sample \((,t=f())\) is \((()-f())^{2}\).

We assume that we are given a dataset \(=\{^{1},,^{S}\}([N]^{I})^{S}\). Then the empirical loss reads (the division by 2 is convenient for the gradient evaluation later)

\[}((v(i,n))_{i[I],n[N]})=_{s=1}^{S}((^{s})-f(^{s}))^{2}.\] (6)

We also define shorthands for certain concatenations of embeddings. For a sample \([N]^{I}\) we denote the collection of embeddings by \(()=(v(1,_{1}),,v(I,_{I}))\). Using the convention (5) we can then write \((())=()\).

Moreover, we define \(}(i)^{DN}\) as the concatenation of the vectors \(v(i,1),,v(i,n)\), i.e., the combined embedding for the \(i\)-th slot and \(}^{DNI}\) as the concatenation of the vectors \(}(i)\) for \(1 i I\), i.e., all token embeddings concatenated. We consider the regularized loss given by

\[}^{}(})=}(})+|}|^{2}.\] (7)Note that the scaling by \(N/S\) (instead of usual \(1/S\)) is natural because every token embedding \(v(i,n)\) occurs in approximately \(S/N\) of the samples and so the scaling ensures that the gradient of \(}^{}\) with respect to the token embedding \(v(i,n)\) is of order one. Now, we consider the continuous gradient descent of the loss with respect to the embeddings, i.e., we consider (omitting the time variable from the notation)

\[(i,n)=-}{v(i,n)}}^{}( )=-}{v(i,n)}}( )- v(i,n).\] (8)

This introduces a time dynamics on the token embeddings. We indicate the time dependence by \(v(i,n,t)\) but we drop \(t\) if not necessary. Our main goal is to understand which conditions ensure that the token embeddings \(v(i,n,t)\) and \(v(i,n^{},t)\) converge to each other if \(_{i}(n)=_{i}(n^{})\) as \(t\). To investigate this we define the center of the class embeddings by

\[w(i,k,t)=^{-1}(k)|}_{n_{i}^{-1}(k)}v(i,n,t)\] (9)

and we consider the deviations from the class centers given by

\[(i,n,t)=v(i,n,t)-w(i,_{i}(n),t).\] (10)

Thus the vectors \((i,n,t)\) capture whether we recover the cluster structure, in particular if all norms \(|(i,n,t)|\) are small then we essentially recovered the hidden structure. Therefore, we define

\[_{}(t)=_{i[I]}_{n[N]}|(i,n,t)|.\] (11)

Similarly, to the notation introduced before we consider for \([K]^{I}\) the vector \(()=(w(1,_{1}),,w(I,_{I}))\). As in (5) we abuse notation and write

\[()=(())=(w(1,_{1}),,w(I, _{I})).\] (12)

Similar to \(}(i)\) and \(}\) we introduce \(}(i)\) as the concatenation of \((w(k,i))_{k[K]}\) and \(}\) as the concatenation of \(}(i)\).

Assumptions.Our first result for the gradient dynamics states that clusters are stable under the dynamics if the initial loss is sufficiently small. More precisely, this means that we assume that \(v(i,n)\) and \(v(i,n^{})\) are close initially whenever \(_{i}(n)=_{i}(n^{})\), i.e., \(_{}(0)\) is small. In addition, we assume that \(|()-g()|\) is small. To capture this we define

\[r_{}(t)=_{[K]^{I}}|((,t))-g()|.\] (13)

Then the result shows that \(_{}\) stays small for all times if \(S N^{2}\) under mild additional assumptions. In other words, if we start from the correctly learned substructures and \(() g()\) for all \([K]^{I}\) then this remains true for all times. Note that while we phrase smallness as an assumption on the mean embeddings \(w(i,k)\) this is generally a consequence of \(_{}\) small and a small empirical loss \(}(})\). Let us now state the required assumptions.

**Assumption 1**.: _We assume that the map \(:[N]^{I}[K]^{I}\) is approximately balanced which means that for all \(i[I]\), \(k[K]\)_

\[_{i}^{-1}(k).\] (14)

This assumption ensures that clusters are of approximately equal size. We have already seen in Section 3 that different cluster sizes increase the sample complexity of learning \(\).

**Assumption 2**.: _We assume that there is a convex set \(^{D}\) and a constant \(M^{} 1\) such that the following bound holds_

\[_{^{I}}_{i_{1},i_{2},i_{3}[DI]}(|( )|,|_{i_{1}}()|,|_{i_{1}}()|, |_{i_{1}}_{i_{2}}_{i_{3}}()|) M ^{}=}{4}.\] (15)

_Here it is convenient to introduce \(M=16M^{ 2}\) so that later certain errors in a Taylor approximation are bounded by \(M\). We also assume that_

\[_{[K]^{I}}|g()| M^{}=}{4}.\] (16)This is a rather mild assumption. For \(C^{3}\) functions \(\) and \(\) bounded this is always true. The next assumption entails a rigidity of approximate minimizers of the loss.

**Assumption 3**.: _We assume that for all embeddings \(w(i,k)^{D}\) for \(i[I]\), \(k[K]\) that satisfy_

\[r_{}=_{[K]^{I}}|(())-g()| 1\] (17)

_the bound_

\[_{0}=_{k,i}_{}(_{[K]^{I}, _{i}=k}_{w(i,k)}(())_{w(i,k)} {f}(()))>0\] (18)

_holds for some positive constant \(_{0}\). Of course, the bound \(r_{} 1\) could be replaced by any other constant._

Note that this condition can only hold if \(D K^{I-1}\), i.e., the latent space dimension cannot be too large. The high level intuition of this assumption is essentially that (at least if \(_{}((())-g())^{2}\) is small) there is no direction \(v^{D}\) such that \(v_{w(i,k)}(()) 0\) for all \(\) such that \(_{i}=k\), i.e., we cannot move one single embedding without changing the output \((())\) for at least one \(\). If this condition does not hold, then we cannot guarantee that \(_{}((())-f())^{2}\) is minimized for a unique \(w(i,k)\) (for all other embeddings fixed). This generally prevents concentration of \(v(i,n)\). Note that this condition does not ensure that there is a unique minimizer \(}\), in particular there could still be a rotationally invariant family of embeddings \(w(i,k)\) such that \((())=g()\) for all \([K]^{I}\). Finally, we need a further mild assumption that ensures that mean token embeddings \(w(i,k)\) stay bounded in some set if the loss is small. This can be achieved, e.g., if \(\) if \(|()|\).

**Assumption 4**.: _We assume that for all collections of mean embeddings \(w(i,k)^{D}\) for \(i[I]\), \(k[K]\) that satisfy_

\[r_{}=_{[K]^{I}}|(())-g()| 1\] (19)

_there is a convex set \(_{0}^{d}\) such that \(w(i,k)_{0}\) for all \(i[I]\), \(k[K]\). Again, the right-hand side of the bound \(r_{} 1\) could be replaced by any other constant._

Results.The first stability theorem can then be stated as follows.

**Theorem 4**.: _Let \(:[N]^{I}[K]^{I}\) be approximately balanced as stated in Assumption 1 Assume that the functions \(g:[K]^{I}\) and \(:^{ID}\) satisfy Assumption 3 for some \(_{0}>0\) and Assumption 4 for some convex set \(_{0}\). Assume that Assumption 2 holds for some \(M\) and the set \(=_{0}+B_{2}(0)\). Then there are constants \(c_{1},C_{2},C_{3},C_{4}>0\) depending on \(I\), \(M\), \(D\), and \(_{0}\) such that for all initial embeddings \(v(i,n,t=0)^{D}\) for \(i[I]\) and \(n[N]\) satisfying_

\[_{}(0) C_{2}K^{-3I/2}, r_{}(0) C_{3}K^{-3I/2}\] (20)

_and sample size_

\[S c_{1}(K^{3I}N^{2}^{2}(N),N(N)K^{9I/2})\] (21)

_the following holds with probability at least \(1-S^{-1}\) over the randomness of the dataset. When considering the gradient dynamics of the embeddings given by (8) the bound_

\[R=_{t}r_{}(t) 1\] (22)

_holds and moreover_

\[_{t}_{}(t) C_{4}K^{3I/2}}R.\] (23)

_In particular \(_{}(t) 0\) if \(r_{}(t) 0\), i.e., all token embeddings for one fixed class converge to the same point._This result shows that for order \(N^{2}(N)\) samples and initialization sufficiently close to a global minimum the cluster structure remains stable. We do not provide conditions that ensure \(r_{}(t) 0\) which guarantees convergence to 0 loss and perfect recovery of the clusters.

Next we note that we cannot expect the clustering to be stable in general even if \(_{}(0)\) is arbitrarily small if initialization is not close to a minimum. This is true even in the simplest case where \(I=K=1\), i.e, we consider gradient descent for a single function value. Then gradient descent does not necessarily converge to a global minimum and close by points do not necessarily stay close because gradient descent is not well posed. Let us clarify this by an example.

**Example 1**.: Assume \(I=K=D=1\), \(N>1\), \((x)=-x^{2}+x^{3}\) and \(f(n)=-2\). Consider the dataset \(=\{1,,n\}\). Assume that \(v(1,n,t=0) N(0,^{2})\) for any \(^{2}>0\). Then the gradient dynamics introduced in (8) reads

\[(1,n,t)=^{}(v(1,n,t))((v(1,n,t))-f(n)) 4v(1,n,t)|v(1,n,t)|\] (24)

We find that \(v(1,n,t) 2/3\) (which is a local minimum of \(\) as \(t\) if \(v(1,n,t=0)>0\). On the other hand \(v(1,n,t)-1\) if \(v(1,n,t=0)<0\). So in this case \(_{}(0)=O(^{2})\) but \(_{}(t) 5/3\) as \(t\). Slight modifications show that also \(_{}\) is possible.

The previous example shows that without additional assumption, we cannot expect to recover the structure of the data. Therefore, we impose additional restrictions on the function \(\). As apparent from Example 1 and also from the bound in Lemma 1 in Appendix E, it is the curvature of the function \(\) that can push token embeddings of the same class apart. We therefore consider the function class of slow-wise linear functions defined as follows.

**Definition 1**.: We call \(\)_slot-wise linear_ if for every \(=(v(1),,v(I))^{D I}\) and any \(i[I]\), \(,[D]\) the relation

\[}{v_{}(i)}}{v_{ }(i)}()=0\] (25)

holds.

Let us denote for \(v^{D}\) by \(^{D+1}\) the vector \(v\) where we append a 1. The most general slot-wise linear function is then of the form

\[f()=T((1)(2)(I))\] (26)

Figure 2: Simulation of the setting in Theorem 5 with \(N=1000\), \(K=I=3\), \(D=2\), \(=0\), \(S=100.000\). (left) trajectories of 50 randomly sampled tokens from 6 different classes. (right) Average distance of token embeddings within a class for different classes (colored) and average distance between all pairs of embeddings (black).

where \(T:^{(D+1)^{}}\) is a linear map. Note that this class covers linearized attention where the embeddings \(v(i,n)\) are split in three separate parts that are used to form key, query, and value vectors. For this function class we can show stronger clustering results.

**Theorem 5**.: _Let \(\) be approximately balanced, i.e., assume that Assumption 1 holds. Let \(:^{ID}\) be a slot-wise linear. Assume that \(v(i,n,t)\) follow the gradient dynamics (8) and that \(v(i,n,t)\) for all \(i[I]\), \(n[N]\) and \(t>0\) for some convex set \(\). Assume that Assumption 2 holds with constant \(M\) for the set \(\). Let \(C_{1}\) be the constant from Lemma 1 (which depends on \(I\), \(D\), \(M\)). Assume that at initialization_

\[|v(i,n,t=0)|}.\] (27)

_Then there are constant \(C_{2},C_{3} 0\) depending on \(M\), \(I\), \(D\), such that for_

\[S(C_{2}K^{I-1}}{^{2}}^{2}(N/ ),C_{3}}{^{4}}(N/))\] (28)

_the bound_

\[_{}(t)(_{}(0)e^{- t/8}, K^{(I-1)/2}}{}})\] (29)

_holds for all \(t 0\)._

The high level summary of this result is that for order \(N^{2}(N)\) samples the clusters can be recovered up to an error of order \(^{-1}\). Note that the a-priori assumption that the gradient flow is restricted to the set \(\) might appear difficult to guarantee in practice. However, we conjecture that the results extend to gradient dynamics clamped at the boundary. Moreover, in Lemma 2 we prove that the mean embeddings \(w(i,k,t)\) stay within a ball of radius \(R=O(})\) for all times. This allows us to prove Theorem 8 which does not require any a-priori bounds on the evolution of \(v(i,n,t)\) but comes at the price that the constants \(C_{1}\), \(C_{2}\), and \(C_{3}\) depend on \(\) so we cannot infer the explicit \(\) dependence. Let us make an important remark.

_Remark 1_.: While we state our results for a fixed function \(\) this function could in principle be time-dependent, e.g., \(\) could be given by a neural network, and we consider gradient descent not only on the token embeddings but also on the network parameters. The only requirement is that the assumptions hold uniformly for all times \(t\). In particular, for Theorem 5 we only need to ensure that the derivatives of \(\) stay uniformly bounded in time. This can, e.g., be guaranteed by clipping the parameters of the slot-wise linear map \(\).

Our results so far show that we can recover the right cluster structure in the sense that the embeddings from the same group cluster. However, this leaves open whether there is any non-trivial dynamics at all, i.e., all embeddings could cluster at the same point. This is in general not the case as can be seen from Corollary 1 which states that in the setting of Theorem 5 and for large times the dynamics of the cluster-means follow the equation

\[(i,k)=-_{[K]^{I},_{i}=k},i}}{2}_{w(i,_{i})}(()-g())^{2}- w (i,k)+O(})\] (30)

where \((2K)^{-I}_{,i}(2/K)^{I}\) are positive numbers. This shows that \(w(i,k)\) follow generally a non-trivial dynamic (and this also justifies the scaling as this expression is of order 1). So in the generic case the cluster structure will be revealed if the numbers of samples is sufficiently large, however, there is no general guarantee that the clusters are well separated. As an illustration of this result we refer to Figure 2 where the clustering of the embeddings becomes apparent.

Proof idea and overview.Let use here give a quick summary of the main steps of the proof and where to find them. The first important ingredient is an expansion of the loss gradient. We Taylor expand the loss of a sample \(()\) around the point \((())\) to second order with remainder terms, the relevant calculations can be found in Appendix D (see Proposition 1 for the outcome). A second ingredient are concentration bounds for certain datapoint-statistics random variables and random matrices. Those are derived in Section G with the necessary results collected in Theorem 9. Combining the Taylor expansion with the concentration result, we can extract the dominant terms of the expansion (see Appendix E). Moreover, we obtain such an expansion for \((i,k)\) (see Corollary 1) and thus the displacements \((i,n)\) (see Corollary 2). This expansion can then be used to control \(_{t}|(i,n)|^{2}\) (see Lemma 1) which is sufficient to control \(_{}\).

Discussion of assumptionsLet us contemplate the differences and similarities to training token embeddings in neural networks.

* For the first main result Theorem 4 we make minimal assumptions on \(\) so this could in principle be a neural network applied to the token embeddings. The second result, Theorem 5, is more restrictive but covers subclasses of linearized attention.
* An important feature of the results is that \(\) itself could be time dependent (see Remark 1).

Differences to standard training of neural networks are:

* We use continuous time gradient descent instead of stochastic gradient descent. This is a frequently used modification and in suitable limits those converge (see, e.g., ).
* We use mean squared error, while sequence modelling usually relies on cross entropy loss. This simplification is frequently used in theoretical analysis, but it is expected that results generally extend to the non-convex cross entropy loss.
* A more crucial difference is that the embedding space dimension in practice is usually chosen large to provide large representation capacity. Here \(D\) has to be rather small to allow a unique optimal solution of the token embeddings that allows us to recover the cluster structure.

## 5 Conclusion

In this paper, we considered a learning problem where we try to recover a partition of tokens from their interaction with other tokens. This can be seen as a toy problem for next token prediction in LLMs, but also more broadly as a problem in scientific inference. We studied this problem from different perspectives, namely from an information-theoretic, complexity-theoretic, and gradient descent based viewpoint. We found that order \(N(N)\) samples are sufficient to recover the partition for \(N\) tokens, while we showed that \(N^{2}(N)\) samples are sufficient for gradient based methods. There are several natural open follow-up questions. First, there are some open questions regarding the tightness of our analysis of the gradient descent. In particular, it is a natural question whether already \((N(N))\) samples are sufficient to control \(_{}\) which is the information-theoretic threshold and would be similar to the optimal results for matrix completion [18; 7]). Another interesting question for future research is whether Theorem 5 holds for standard initialization schemes for the token embeddings. Secondly, it is of interest to relax the notion of clustering of embeddings to more general notions that still allow recovering some structure but are also applicable to high dimensional latent spaces and potentially to multiple partitions and relations on the same tokens (e.g., tokens belonging to different clusters). Thirdly, it is a natural question whether this work can be connected more closely to empirical findings.