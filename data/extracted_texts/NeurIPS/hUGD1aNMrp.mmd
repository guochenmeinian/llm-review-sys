# Assouad, Fano, and Le Cam with Interaction:

A Unifying Lower Bound Framework

and Characterization for Bandit Learnability

 Fan Chen

MIT

fanchen@mit.edu &Dylan J. Foster

Microsoft Research

dylanfoster@microsoft.com &Yanjun Han

New York University

yanjunhan@nyu.edu &Jian Qian

MIT

jianqian@mit.edu &Alexander Rakhlin

MIT

rakhlin@mit.edu &Yunbei Xu

National University of Singapore

yunbei@nus.edu.sg

###### Abstract

We develop a unifying framework for information-theoretic lower bound in statistical estimation and interactive decision making. Classical lower bound techniques--such as Fano's method, Le Cam's method, and Assouad's lemma--are central to the study of minimax risk in statistical estimation, yet are insufficient to provide tight lower bounds for _interactive decision making_ algorithms that collect data interactively (e.g., algorithms for bandits and reinforcement learning). Recent work of Foster et al.  provides minimax lower bounds for interactive decision making using seemingly different analysis techniques from the classical methods. These results--which are proven using a complexity measure known as the _Decision-Estimation Coefficient_ (DEC)--capture difficulties unique to interactive learning, yet do not recover the tightest known lower bounds for passive estimation. We propose a unified view of these distinct methodologies through a new lower bound approach called _interactive Fano method_. As an application, we introduce a novel complexity measure, the _Fractional Covering Number_, which facilitates the new lower bounds for interactive decision making that extend the DEC methodology by incorporating the complexity of estimation. Using the fractional covering number, we (i) provide a unified characterization of learnability for _any_ stochastic bandit problem, (ii) close the remaining gap between the upper and lower bounds in Foster et al.  (up to polynomial factors) for any interactive decision making problem in which the underlying model class is convex.

## 1 Introduction

The minimax criterion is a standard approach to studying the intrinsic difficulty of problems in statistics and machine learning. For an algorithm ALG that collects data (either passively or interactively) from the model \(M\), the minimax criterion (stated somewhat informally here) is

\[_{}\ _{M}\ (,M).\] (1)

The expression reflects the best cost that can be achieved by an algorithm ALG for a worst-case problem instance in a collection \(\), measured according to an appropriate cost function Cost. In statistics, the minimax approach was pioneered by A. Wald , who made the connection to von Neumann's theory of games  and unified statistical estimation and hypothesis testing under the umbrella of _statistical decision theory_. Minimax optimality and minimax rates of convergence ofestimators have since become a central object in the modern non-asymptotic statistics [82; 89]; here, for instance, ALG is an estimator of an unknown parameter based on noisy observations.

Upper bounds on the minimax value (1) are typically achieved by choosing a particular algorithm, while lower bounds often require specialized techniques. In statistics, three such techniques are widely used: Le Cam's two-point method, Fano's method, and Assouad's method. These techniques entail constructing "difficult" subsets of the class \(\). Le Cam's method focuses on two hypotheses, while Assouad's method and Fano's method involve multiple hypotheses. The relationships between these methods are explored in Yu .

Classical statistical estimation is a purely passive task. A parallel line of research  considers the task of _interactive decision making_, where ALG is a multi-round procedure that directly interacts with the data generating process and iteratively makes decisions with the (often contradictory) aims of minimizing cost and collecting information. Proving minimax lower bounds for interactive decision making problems presents unique challenges. The aforementioned lower bound techniques for estimation require quantifying the amount of information that can be gained from passively acquired data from a hard problem instance, but the amount information acquired by an _interactive_ algorithm is harder to quantify [69; 4; 68], since it depends on the decisions made by the algorithm itself over multiple rounds.

In spite of the challenges, recent work of Foster et al. [40; 42] shows that a complexity measure known as the _Decision-Estimation Coefficient_ (DEC) leads to both lower and upper bounds on the minimax rates for a general class of interactive decision making problems. Interestingly, the lower bound techniques in Foster et al.  proceed in a seemingly different fashion from classical lower bounds for statistical estimation; most notably, their techniques involve an _algorithm-dependent_ (as opposed to oblivious) choice of a hard-to-distinguish alternative problem instance.

Given the differences between the classical Assouad, Fano, and Le Cam methods, and the even larger disparity between these methods and the interactive decision making techniques of Foster et al. [40; 42], it is natural to ask whether there is a hope of unifying these lower bounds techniques. Beyond the fundamental nature of this question, there is hope that a unified understanding might lead to tighter lower bounds, or even inspire new algorithms and upper bounds; of particular interest is to close the remaining (estimation-based) gaps between the upper and lower bounds on the minimax rates for interactive decision making left open by Foster et al. .

Contributions.We present a new framework for information-theoretic lower bounds which allows for a unifying presentation of classical lower bounds in statistical estimation (Assouad, Fano, and Le Cam) and recent DEC-based lower bounds for interactive decision making [40; 42].

* **Interactive lower bound framework (Section 3).** Our main result is to introduce a new lower bound technique, the _interactive Fano method_. The interactive Fano method generalizes the stringent separation condition in the classical Fano inequality to a novel algorithm-dependent condition by introducing the concept of "ghost data" generated from a reference distribution. This technique recovers the Le Cam two-point method (and convex hull method), Assouad method, and Fano method as special cases. By virtue of being algorithm-dependent in nature, the interactive Fano method seamlessly recovers DEC-based lower bounds for interactive decision making as a special case, and leads to refined quantile-based variants.
* **Fractional covering number and bandit learnability (Section 4).** As an application of the interactive Fano method, we derive lower bounds for interactive decision making based on a new complexity measure, the _fractional covering number_, which quantifies the difficulty of _estimating_ a near-optimal policy/decision, and complements the original DEC lower bounds (which reflect difficulty of exploration as opposed to difficulty of estimation). As an application, the fractional covering number provides both lower and upper bound for learning any structured bandit problem, up to an exponential gap. In particular, finiteness of the fractional covering number is the first necessary and sufficient condition for finite-time learnability of any structured bandit problem. As a secondary result, we use the fractional covering number to close the remaining gap between the upper and lower bounds in Foster et al. [40; 42] (up to polynomial factors) for any interactive decision making problem in which the underlying model class is convex.

Related work.Due to space limitations, we discuss the related work in Appendix A.

### Preliminaries

Let \(P\) and \(Q\) be two distributions over a space \(\) such that \(P\) is absolutely continuous with respect to \(Q\). Then, for a convex function \(f:[0,+)(-,+]\) such that \(f(x)\) is finite for all \(x>0\), \(f(1)=0\), and \(f(0)=_{x 0^{+}}f(x)\), the \(f\)-divergence of between \(P\) and \(Q\) is defined as

\[D_{f}(P,Q)}=_{}f()\,dQ.\]

Concretely, we make use of three well-known \(f\)-divergences: the KL-divergence \(D_{}\), the squared Hellinger distance \(D_{}^{2}\), and the total variation distance \(D_{}\), for which the function \(f(x)\) is chosen to be \(x x\), \((-1)^{2}\), and \(|x-1|\) respectively. For a pair of random variables \((X,Y)\) with joint distribution \(P_{X,Y}\), the mutual information is defined as

\[I(X;Y)=_{X}D_{}P_{Y|X}\,\|\,P_{Y} ,\]

where \(P_{Y|X}\) is the conditional distribution of \(Y|X\) and \(P_{Y}\) is the marginal distribution of \(Y\).

## 2 Statistical Estimation and Interactive Decision Making

We work in a general framework we refer to as _Interactive Statistical Decision Making_ (ISDM). We adopt this framework as a convenient formalism which encompasses statistical estimation and interactive decision making in a unified fashion. We first introduce the framework and show how it subsumes statistical estimation (Section 2.1) and interactive decision making (Section 2.2), then give brief background on existing lower bound techniques and gaps in understanding (Appendix A).

Interactive Statistical Decision Making.An ISDM problem is specified by \((,,,L)\), where \(\) is the space of outcomes, \(\) is a model class (parameter space), \(\) is the space of algorithms, and \(L\) is a non-negative risk function. For an algorithm \(\) chosen by the learner and a model \(M\) specified by the environment, an observation \(X\) is generated from a distribution induced by \(M\) and \(\): \(X^{M,}\). The performance of the algorithm \(\) on the model \(M\) is then measured by the risk function \(L(M,X)\). The learner's goal is to minimize the risk by choosing the algorithm \(\). As described in the Introduction, the best possible expected risk the learner may achieve is the following _minimax risk_:

\[_{}_{M}^{M, }[L(M,X)].\] (2)

While our main results concern the general problem formulation in Eq. (2), we focus on applications to statistical estimation and interactive decision making throughout. Below, we give additional background on these settings and show how to view them as special cases.

### Statistical estimation

In statistical decision theory [90; 13; 12], the learner is given the parameter space \(\), observation space \(\), decision space \(\), and a loss function \(L\). For an underlying parameter \(^{}\), \(n\) i.i.d. samples \(Y_{1},...,Y_{n} P_{^{}}\) are drawn and observed by the learner. The learner then chooses a decision \(A=A(Y_{1},,Y_{n})\) based on the observations, and incurs the loss \(L(^{},A)\).

Any general statistical estimation problem can be trivially viewed as a ISDM instance, by choosing the model class as \(=\{P_{}:\}\) and the algorithm space as \(=\{:^{ n}\}\). For model \(M=P_{}\) and algorithm \(\), the distribution of the whole observation \(X^{M,}\) is given by

\[X=(Y_{1},,Y_{n},A), Y_{1},,Y_{n}}}{{}}P_{},\ A=(Y_{1},,Y_{n}).\]

The loss under model \(M\) is then measured by the loss of the decision \(A\), i.e., \(L(M,X)}=L(,A)\).

### Interactive decision making

For interactive decision making, we consider the following variant of the Decision Making with Structured Observations (DMSO) framework , which subsumes bandits and reinforcement learning. The learner interacts with the environment (described by an underlying model \(M^{}:()\), unknown to the learner) for \(T\) rounds. For each round \(t=1,...,T\):

* The learner selects a decision \(^{t}\), where \(\) is the decision space.
* The learner receives an observation \(o^{t}\) via \(o^{t} M^{}(^{t})\), where \(\) is the observation space.

The underlying model \(M^{}\) is formally a conditional distribution, and the learner is assumed to have access to a known model class \((())\) with the following property.

**Assumption 1** (Realizability).: _The model class \(\) contains \(M^{}\)._

The model class \(\) represents the learner's prior knowledge of the structure of the underlying environment. For example, for structured bandit problems, the models specify the reward distributions and hence encode the structural assumptions on the mean reward function (e.g. linearity, smoothness, or concavity). For a more detailed discussion, see Appendix B.

To each model \(M\), we associate a _risk_ function \(L(M,):_{ 0}\), which measures the performance of a decision in \(\) under \(M\). We consider two types of learning goals under the DMSO framework:

* Generalized no-regret learning: The goal of the agent is to minimize the _cumulative_ sub-optimality during the course of the interaction, given by \[_{}(T):=_{t=1}^{T}L(M^{},^{t}),\] (3) where \(^{t}\) can be randomly drawn from a distribution \(p^{t}()\) chosen by the learner at step \(t\).
* Generalized PAC (Probably Approximately Correct) learning: the goal of the agent is to minimize the sub-optimality of a final output decision \(\) (possibly randomized), which is selected by the learner once all \(T\) rounds of interaction conclude. We measure performance via \[_{}(T):=L(M^{},).\] (4) With an appropriate choice for \(L\), the setting captures reward maximization (regret minimization) [40; 42], model estimation and preference-based learning , multi-agent decision making and partial monitoring , and various other tasks. In the main text, we focus on reward maximization and defer the results for more general choices \(L\) to the appendices (cf. Appendix B).

**Example 1** (Reward maximization).: In the reward-maximization task, \(R:\) is a known reward function.1 For a model \(M\), \(^{M,}[]\) denotes expectation under the process \(o M()\), and \(f^{M}():=^{M,}[R(o)]\) denotes the expected value function. For any \(M\), we let \(_{{}_{M}}_{}f^{M}()\) be an optimal decision under \(M\), and the risk function is defined by \(L(M,)=f^{M}(_{{}_{M}})-f^{M}()\), measuring the sub-optimality of the decision \(\) under model \(M\).

**DMSO as an instance of ISDM.** Any DMSO class \((,)\) induces an ISDM as follows. For any \(t[T]\), denote the full history of decisions and observations up to time \(t\) by \(^{t-1}=(^{s},o^{s})_{s=1}^{t-1}\). The space of observations \(\) consists of all \(X\) of the form \(X=(^{T},)\), where \(\) is a final decision. An algorithm \(=\{q^{t}\}_{t[T]}\{p\}\) is specified by a sequence of mappings, where the \(t\)-th mapping \(q^{t}(^{t-1})\) specifies the distribution of \(^{t}\) based on \(^{t-1}\), and the final map \(p(^{T})\) specifies the distribution of the _output decision_\(\) based on \(^{T}\). The algorithm space \(\) consists of all such algorithms. The loss function is chosen to be \(L(M^{},X)=L(M^{},)\) for PAC learning (4), and \(L(M^{},X)=_{t=1}^{T}L(M^{},^{t})\) for no-regret learning (3). For any algorithm \(\) and model \(M\), \(^{M,}()\) is the distribution of \(X=(^{T},)\) generated by the algorithm \(\) under the model \(M\), and we let \(^{M,}[]\) to be the corresponding expectation.

## 3 A General Lower Bound

In this section, we introduce our general lower bound technique, the interactive Fano method, and use it to provide minimax lower bounds for the ISDM framework.

**Theorem 1** (Interactive Fano method).: _Fix a \(f\)-divergence \(D_{f}\). Let \(\) be a given algorithm, \(()\) be a given prior distribution over models, and \(>0\) be a given risk level. For any reference distribution \(()\), we define_

\[_{,}=_{M,X}(L(M,X)<).\] (5)

_Then, the following lower bound holds:_

\[_{M}_{X^{M,}}[L(M,X)] _{(),}\{ :_{M}[D_{f}(^{M,},)]< _{f,}(_{,})\},\]_where we denote \(_{f,}(p)=D_{f}((1-),(p))\) if \(p 1-\), and \(_{f,}(p)=0\) otherwise._

This result generalizes the classical Fano method in the prequel (as well as more sophisticated variants ) in multiple ways:

* It encompasses general interactive learning/estimation problems in the ISDM framework, as opposed to purely passive estimation. This is reflected in the fact that the distribution over the outcome \(X\) is allowed to depend on \(\) itself.
* The most important and novel change is that Theorem 1 generalizes the "hard" separation condition required in the classical Fano method to a "soft" notion of separation captured by the quantile \(_{,}\) in Eq.5. The quantile \(_{,}\) reflects the average separation under "ghost data" \(X\) generated from an arbitrary reference distribution \(\), which is independent of the true model \(M\).
* In addition, instead of relying on mutual information, which is can difficult to quantify for interactive problems, we use divergence with respect to the reference distribution \(\), generalizing a central idea in Foster et al. .

In what follows, we will show that these generalizations allow the Interactive Fano method to achieve two important desiderata: (1) unifying the methods of Fano, Le Cam, and Assouad (Section3.1), and (2) integrating these traditional lower bound techniques with the DEC approach  to derive new lower bounds (see Section3.2).

### Recovering non-interactive lower bounds

We begin by applying Theorem1 to recover classical non-interactive lower bounds for statistical estimation. Since a goal of our paper is to integrate the Fano and Assouad methods with the DEC framework, this serves as an important sanity check to demonstrate that our framework can recover the non-interactive versions of these methods.

**Fano method.** We specialize Theorem1 to the KL divergence. Observe that for any reference distribution \(\),

\[_{M,X}(L(M,X)<)_{x}(M:L(M,x)< ).\]

By choosing \(=_{M}^{M,}\) in Theorem1, we obtain the following proposition, which encompasses prior generalizations of Fano's inequality  developed in statistical estimation.

**Proposition 2** (Recovering the generalized Fano method).: _Fix an algorithm \(\) and prior distribution \(()\), and let \(I_{,}(M;X)\) be the mutual information between \(M\) and \(X\) under \(M\) and \(X^{M,}\). The following Bayes risk lower bound holds for all \( 0\):_

\[_{M}_{X^{M,}}[L(M,X) ](1+}(M;X)+ 2}{_{x}(M :L(M,x)<)}).\] (6)

When applied to the statistical estimation setting (Section2.1), the classical Fano method corresponds to the special case of Proposition2 where \(==\{1,2,,m\}\), \(L(,a)=( a)\) is the indicator loss, \(=()\) is the uniform prior, and \(=1\).

Note that in Proposition2, the term \(_{x}(M:L(M,x)<)\) in the denominator of Eq.6 takes the supremum over the outcome \(x\), resulting in a simplified expression that removes the role of the algorithm \(\). This simplification is often sufficient to derive tight guarantees for estimation, but is insufficient for interactive decision making in general. The DEC, which we define in Section3.2, more precisely accounts for the role of decisions selected by the algorithm.

**Le Cam's method and Assouad's method.** To recover Le Cam's two-point method and Assouad's method from Theorem1, we appeal to the following result, which recovers a lower bound known as the Le Cam convex hull method  which generalizes both approaches.

**Proposition 3** (Recovering Le Cam's convex hull method).: _For a parameter space \(\) and observation space \(\), consider a class of distributions \(=\{P_{}\}()\) indexed by \(\). Let \(L:_{+}\) be a loss function. Suppose \(_{0}\) and \(_{1}\) satisfy the separation condition_

\[L(_{0},a)+L(_{1},a) 2, a,_{ 0}_{0},_{1}_{1}.\] (7)

_Then_

\[_{}_{}_{Y P_{}}L(,(Y))_{_{0}(_{0}),_{1 }(_{1})}1-D_{}(P_{_{0}}^{ n},P_{ _{1}}^{ n}),\]_where the infimum is taken over all algorithms \(:^{ n}\), and \(P_{_{i}}^{ n}\) is the distribution on \(^{ n}\) induced by \(_{i},Y=(Y_{1},,Y_{n})}} {{}}P_{}\) for \(i\{0,1\}\)._

Le Cam's convex hull method is the most general formulation of the Le Cam two-point method, which--in its most basic form--corresponds to the case in which \(_{0}\) and \(_{1}\) are singletons. The convex hull method is also capable of recovering Assouad's method . It is important to note that the classical Fano's method, e.g. in the form of Proposition 2, cannot recover Proposition 3. This is because of fundamental differences between the divergences (KL versus TV) used in the traditional Fano method and the convex hull method.

### Recovering DEC-based lower bounds for interactive decision making

Within the DMSO framework (Section 2.2), Foster et al.  introduced the _Decision-Estimation Coefficient_ (DEC) as a complexity measure, providing both upper and lower bounds for any model class \(\). We now show how to recover the lower bounds of Foster et al.  through Theorem 1. We focus on the lower bounds from Foster et al. , which are based on a variant of the DEC called the _constrained DEC_, and provide the tightest guarantees from prior work.

Background on the Decision-Estimation Coefficient.Consider the reward maximization setting (Example 1) under DMSO.For a model class \(\) and a reference model \(:()\) (not necessarily in \(\)), we define the constrained regret-DEC via

\[^{}_{}(,):=_{p()}_{M}\{\,_{ p }[L(M,)]_{ p}D_{}^{2}(M(),( ))^{2}\},\] (8)

and define the constrained PAC-DEC via

\[^{}_{}(,):=_{p,q()}_{M}\{\,_{ p }[L(M,)]_{ q}D_{}^{2}(M(),( ))^{2}\}.\] (9)

Here, the superscript "\(\)" indicates "constrained", and the superscript "\(\)" (resp. "\(\)") indicates "regret" (resp. "\(\)"). We further define

\[^{}_{}()=_ {()}^{}_{}(,),^{ }_{}()=_{()}^{}_{}( \{\},),\]

where \(()\) denotes the convex hull of the model class \(\).

Based on these complexity measures, Foster et al.  (see also Glasgow and Rakhlin ) provide the following lower and upper bounds on optimal risk and regret, under mild growth conditions on the DECs.

**Theorem 4** (Informal; Foster et al. ).: _Consider the reward maximization variant of the DMSO setting (Example 1). For any model class \(\) and \(T\), the following lower and upper bounds hold: (1) For PAC learning,_

\[^{}_{(T)}() _{}_{M}^{M,}[ _{}(T)]^{ }_{(T)}(),\]

_where \((T)\) and \((T)|/T}\) (up to logarithmic factors). (2) For no-regret learning,_

\[^{}_{(T)}()  T_{}_{M}^{M, }[_{}(T)]^{}_{(T)}() T+T(T).\]

Therefore, up to the \(||\)-gap between the parameters \((T)\) and \((T)\) appearing in the lower and upper bounds, the constrained PAC-DEC tightly captures the minimax risk of PAC learning, and the constrained regret-DEC captures the minimax regret of no-regret learning.

A new complexity measure: The quantile Decision-Estimation Coefficient.We recover the DEC-based lower bounds from Foster et al.  through a new variant we refer to as the _quantile DEC_. To do so, we briefly recount the proof technique used by Foster et al. .

Given an algorithm \(\), the proof strategy is to first fix an arbitrary _reference model_\(\), then adversarially choose a hard _alternative model_\(M\) (in a way that is guided by the DEC and the algorithm \(\) itself) such that \(D_{}(^{M,},^{M,})\) is small, yet \(\) cannot achieve low risk on model \(M\). This lower bound technique does not explicitly require a separation condition between \(M\) and \(\), which is a departure from the classical Fano and two-point methods. Thus to recover it, the lack of an explicit separation condition in Theorem 1 will be critical. More precisely, for any model \(M\), we consider the following distributions over decisions:

\[q_{{M,}}=^{{M,}}_{t=1}^{T}q^{t}(^{t-1})(), p _{{M,}}=^{{M,}}p( ^{T})().\] (10)

That is, \(q_{{M,}}\) is the expected empirical distribution over the decisions \((^{1},,^{T})\) played by the algorithm under \(M\), and \(p_{{M,}}\) is the expected distribution of the final decision \(\).

With these definitions, we instantiate Theorem 1 with the Hellinger distance. We will use the sub-additivity of Hellinger distance (Lemma C.1), which allows us to bound

\[D_{}^{2}(^{{M,}},^{{M, }}) 7T_{ p_{{, }}}D_{}^{2}(M(),( )).\] (11)

Theorem 1 then yields the following intermediate result.

**Lemma 5** (Recovering interactive two-point method).: _Let \(\) be given, and consider an algorithm \(\). Define_

\[^{}_{,}:=_{( )}_{M}_{ 0}\{:,}}(:L(M,))}>+_{ q_{{,}}}D_{}^{2} (M(),())}\}.\]

_Then there exists \(M\) such that \(^{{M,}}L(M,)^{ }_{,}\)._

Using Lemma 5, as a starting point, we derive a new quantile-based variant of the DEC, which we will show can be viewed as a slight generalization of the original PAC DEC of Foster et al. .

For any model \(M\) and any parameter \(\), we define the \(\)-quantile risk as follows:

\[_{}(M,p)=_{ 0}\{:_{ p}(L (M,))\};\]

this serves as a measure of the sub-optimality of the distribution \(p()\) in terms of \(\)-quantile. We now define the quantile PAC DEC as follows:

\[^{}_{,}(, {M}):=_{p,q()}_{M}\{\,_{ }(M,p)\,\,\,_{ q}D_{}^{2}(M(), ())^{2}\},\] (12)

and define \(^{}_{,}():=_ {()}^{}_{ ,}(,)\). Applying Lemma 5, it is immediate to see that the quantile PAC-DEC provides a lower bound on the PAC risk.

**Theorem 6** (Quantile DEC lower bound).: _Let any \(T 1\) and \([0,1)\) be given, and define \(_{}(T):=}\). Then, for any algorithm \(\), there exists \(M^{}\) such that_

\[^{{M^{},}}_{}(T)^{}_{_{ }(T),}().\]

Unlike the original constrained DEC lower bounds (Theorem 4), which are restricted to the reward maximization variant of the DMSO setting (Example 1), the quantile DEC lower bound in Theorem 6 holds for _any risk function_\(L\). As a result, the lower bound applies to a broader range of generalized PAC learning tasks, including model estimation  and multi-agent decision making , where DEC-based lower bounds from prior work are loose in general; as a concrete application, we derive a new lower bound for _interactive estimation_ (Example 3) in Appendix E.2.

**Recovering DEC-based lower bounds using the quantile DEC.** At first glance, Theorem 6 might appear to be weaker than the constrained PAC-DEC lower bound in Theorem 4 due to the loose conversion from quantile risk to expected risk. However, by specializing to reward maximization (Example 3) and leveraging the structure of the risk function \(L\), we show that quantile PAC-DEC is equivalent to its constrained counterpart for this setting, leading to a tight lower bound.

**Proposition 7** (Recovering the PAC DEC lower bound).: _Under the reward maximization setting (Example 1), for any \(>0\) and \([0,1)\) it holds that_

\[^{}_{}() ^{}_{,}()+.\]

As a corollary, we may choose \(=\) and \((T)=}\) in Theorem 6, so that

\[_{M}^{{M,}}[_{ }(T)]^{}_{(T),1/2}()^ {}_{(T)}()-8(T).\]

Thus, the quantile PAC-DEC lower bound indeed recovers the constrained PAC-DEC lower bound in Theorem 4.

Our quantile DEC lower bound extends to regret with minor modifications, allowing us to recover the regret lower bounds in Theorem 4. We defer the details to the Appendix E.1 (Theorem E.1).

### Recovering mutual information-based lower bounds for interactive decision making

The following result uses Theorem 1 to extend classical Fano method to interactive decision making and achieves tight dependence on the problem dimension that is not recovered by the standard DEC lower bound in Foster et al. .

**Proposition 8** (Mutual information-based lower bound).: _Consider the DMSO setting. For any \(T 1\) and prior \(()\), we define the maximum \(T\)-round mutual information as_

\[I_{}(T):=_{}I_{,}(M;^{T}),\]

_where we recall that \(I_{,}(M;^{T})\) is the mutual information between \(M\) and \(^{T}\) under \(M\) and \(^{T}^{M,}\), and the supremum is taken over all possible DMSO algorithms \(\). Then for any algorithm \(\),_

\[_{M}^{M,}[L(M,)] _{()}_{>0}\{\,|\, _{}(M:L(M,))(-2I_{}(T))\}.\]

Using Proposition 8, along with mutual information bounds from Rajaraman et al. , we recover a \((d/)\) PAC lower bound for \(d\)-dimensional linear bandits, which in turn recovers the \((d)\) regret lower bound [30, 73, 57, etc.].

**Corollary 9**.: _For \(d 2\), consider the \(d\)-dimensional linear bandit problem with decision space \(=\{^{d}:\|\|_{2} 1\}\), parameter space \(=\{^{d}:\|\|_{2} 1\}\), and Gaussian rewards. The model class is \(=\{M_{}\}_{}\), where for each \(\), the model \(M_{}\) is given by \(M_{}()=(,,1)\). Then Proposition 8 implies a minimax risk lower bound:_

\[_{}_{M}^{M,}[_{}(T)](\{d/,1\}).\] (13)

In Section 4, we also instantiate Proposition 8 to derive a new complexity measure for DMSO.

## 4 Application to Interactive Decision Making: Bandit Learnability and Beyond

In this section, we focus on the DMSO setting and apply our general results (Theorem 1) to derive new lower and upper bounds for interactive decision making that go beyond the previous results based on the Decision-Estimation Coefficient  by incorporating hardness of estimation.

**Background: Gaps between DEC-based and upper and lower bounds.** A fundamental open question of the DEC framework is whether the \(||\)-gap between DEC lower and upper bounds in Theorem 4 can be closed. To highlight this gap in a more interpretable fashion, we re-state Theorem 4 in terms of a quantity we refer to as the _minimax sample complexity_. Let us focus on regret. Recall that for a fixed model class \(\), the following notion of minimax regret (2) is the central objective of interest:

\[^{}(,T):=_{}_{M }^{M,}[_{}(T)].\]

Given a parameter \(>0\), we define the _minimax sample complexity_

\[T^{}(,):=_{T 1}\{T:^{}( ,T) T\}\] (14)

as the least value \(T\) for which there exists an algorithm that achieves \( T\) regret. Clearly, characterizing \(T^{}(,)\) is equivalent to characterizing the minimax regret \(^{}(,T)\).

Consider the following quantity induced by DEC for a class \(\) and parameter \(>0\):

\[T^{}(,)=_{(0,1)}\{^{- 2}:^{}_{e}()}\}.\] (15)

With this definition, Theorem 4 is equivalent to the following characterization of the minimax sample complexity \(T^{}(,)\):

\[T^{}(,) T^{}(,)  T^{}(,)||.\] (16)

That is, Theorem 4 characterizes the minimax sample complexity up to a multiplicative \(||\) factor. Our main result in this section will be to use the fractional covering number and interactive Fano method (Theorem 1), to (i) tighten the above characterization (16) for various special cases of interest, and (ii) give a new characterization for \(T^{}(,)\) in structured bandit problems which avoids spurious parameters such as \(||\) altogether.

### New upper and lower bounds through the fractional covering number

For the a model class \(\) and parameter \(>0\), we define the _fractional covering number_ as follows:

\[_{}(,):=_{p()}_{M }\ .\] (17)

Informally, the fractional covering number \(_{}(,)\) represents the best possible coverage over \(\)-optimal decisions that can be achieved through a single exploratory distribution in the face of an unknown model \(M\). As we will now show, this quantity naturally arises as a lower bound on optimal risk through the interactive Fano method. We begin with the following assumption.

**Assumption 2** (Regular model class).: _There exists a constant \(C_{}>0\) and a reference model \(\) such that \(D_{}(M()()) C_{}\) for all \(M\) and \(\)._

Assumption 2 is a mild assumption on the boundedness of KL divergence. As an example, for structured bandits with means in \(\) and Gaussian rewards, Assumption 2 holds with \(C_{}=\). Details and more examples are provided in Appendix B.2. Our main lower bound based on the fractional covering number follows by specializing Theorem 1 to KL divergence.

**Theorem 10** (Fractional covering number lower bound).: _Suppose that \(\) satisfies Assumption 2 with parameter \(C_{}>0\). Then for any algorithm \(\) and \(>0\), unless \(T_{}(,)-2}{2C_{}}\), there exists \(M^{}\) such that \(^{M^{},}[L(M^{},)] \)._

In particular, for (generalized) no-regret learning, fractional covering number also implies a regret lower bound through Theorem 10. That is, for any algorithm to achieve \( T\)-regret, it is necessary to have \(T=(_{}(,2))\). Combining this with Theorem 4, we conclude that boundedness of both the DEC and the fractional covering number is necessary for learning with any model class \(\).

Upper bounds based on the fractional covering number.We now complement Theorem 10 by showing that for any reward maximization instance of the DMSO setting (Example 1), boundedness of the fractional covering number alone is also sufficient to derive _upper bounds_ on the sample complexity of learning. The caveat is that while the lower bound is logarithmic in \(_{}(,)\), the upper bound will be polynomial.

**Theorem 11** (Fractional covering number upper bound).: _Consider the reward maximization task (Example 1). There exists an algorithm that for any class \(\) and \(>0\), ensures that with probability at least \(1-\),_

\[_{}(T) T+O((T/))_{}(,)}.\]

Combining Theorem 10 and Theorem 11 yields the following bounds on \(T^{}(,)\) (omitting polylogarithmic factors):

\[_{}(,2)}{C_{}}  T^{}(,)_{} (,/2)}{^{2}}.\] (18)

The gap between the lower and upper bounds of (18) is exponential; However, for model classes with \(C_{}=O(1)\), (18) suffices to characterize _finite-time learnability_. As a special case, we now show that fractional covering number characterizes the learnability of any structured bandit problem.

### Application: Bandit learnability

We consider a structured bandit setting given by a reward function class \(()\). The protocol is as follows: For each round \(t[T]\), the learner chooses a decision \(^{t}\), then receives a reward \(r^{t}(h_{}(^{t}),1)\) in response, where the mean reward function \(h_{}\). This corresponds to an instance of the DMSO framework with induced model class \(_{}=\{(h(),1) h\}\). We define the fractional covering number for \(\) via

\[_{}(,):=_{}( _{},)=_{p()}_{h} \ )-h())},\] (19)

where we denote \(_{h}:=_{}h()\). This exactly coincides with the notion of _maximin volume_ of Hanneke and Yang , which was shown to give a tight characterization of learnability for the special case of _noiseless binary-valued_ structured bandits. We discuss the connection to Hanneke and Yang  in more detail in Appendix G.2.

It is straightforward to show that for any structured bandit problem, the induced class \(_{}\) satisfies Assumption 2 with \(C_{}=\) (Example 7). This leads to the following lower bound.

**Corollary 12** (Lower bound for stochastic bandits).: _For the bandit model class \(_{}\) defined as above, it holds that \(T^{}(_{},) 2_{}(,)-2\)._

Combining this result with the upper bound in Theorem 11, we obtain the following bounds on the minimax-optimal sample complexity for the structure bandit problem with class \(\):

\[_{}(,2) T^{}( _{},)_{}( ,/2)}{^{2}}.\] (20)

This implies that \(_{}(,)\) characterizes learnability for structured bandits.

**Theorem 13** (Structured bandit learnability).: _For a given reward function class \(\), the bandit problem class \(_{}\) is learnable for finite \(T\) if and only if \(_{}(,)<+\) for all \(>0\)._

We remark that the lower and upper bound in Eq. (20) cannot be improved in terms of the fractional covering number alone: (1) For \(K\)-armed bandits, we have \(_{}(,) K\), meaning the upper bound is tight. (2) For \(d\)-dimensional linear bandits, we have \(_{}(,)=(d)\), meaning the lower bound is nearly tight. Nevertheless, the exponential gap in Eq. (20) can be partly mitigated by combining the fractional covering number with the DEC, as we will show in Section 4.3.

Our characterization bypasses the impossibility results of Hanneke and Yang . Specifically, Hanneke and Yang  show that for _noiseless_ structured bandit problems, there exist classes \(\) for which bandit learnability is independent of the axioms of ZFC. Therefore, their results rule out the possibility of a characterization of noiseless bandit learnability through any _combinatorial dimension_ for the problem class. Our characterization is compatible with this result because the argument of Hanneke and Yang  relies on the noiseless nature of the bandit problem, and hence does not preclude a characterization for the noisy setting. Additional discussion is deferred to Appendix G.2.

### Improved upper bounds for general decision making

To close this section, we derive tighter upper bounds that scale with \(_{}(,)\) by combining the fractional covering number with the Decision-Estimation Coefficient. For simplicity of presentation, we focus on regret minimization under the setting of Example 1, and we assume the following condition to simplify our bounds (the fully general upper bound is detailed in Appendix G.3).

**Assumption 3** (Regularity of constrained DEC).: _A function \(:\) is said to have moderate decay if \(() 10\;\), and there exists a constant \(c 1\) such that \(c()}{}( ^{})}{^{}}\) for all \(^{}\). We assume the function \(^{}_{}(())\), as a function of \(\), satisfies moderate decay for a constant \(c_{} 1\)._

This condition essentially requires that the DEC for \(()\) exhibits moderate growth, which means that learning with \(()\) is not "too easy".We now state our upper bound, which tightens Theorem 4 by replacing the \(||\) dependence in the upper bound with \(_{}(,)\) (with the caveat that the upper bound scales with the DEC for the _convexified_ model class \(()\)).

**Theorem 14** (Upper bound with DEC and fractional covering number).: _Consider the reward maximization variant of the DMSO setting. Let \(\) be any class for which Assumption 3 holds, and assume that \(\) is finite. Let \((T)_{}(,)/T}\). Then for any \(>0\), Algorithm 1 (see Appendix G.3) ensures that with high probability,_

\[_{} T+Oc_{}T^{}_{(T)}( ()).\]

Restating this upper bound in terms of minimax sample complexity and combining it with the preceding lower bounds yields the following result.

**Theorem 15**.: _For any class \(\) that satisfies Assumption 2 and 3, we have_

\[\{T^{}(,),_{ }(,2)}{C_{}}\} T^{}( ,) T^{}((),) _{}(,/2),\] (21)

_up to dependence on \(c_{}\) and logarithmic factors._

In particular, when the model class \(\) is convex (i.e. \(()=\)) and \(C_{}=O(1)\), Theorem 15 provides lower and upper bounds for learning with \(\) that match up to a quadratic factor. Indeed, for convex model classes, the upper bound of (21) is always tighter than (16) (and also tighter than the result in Foster et al. ), as by definition we have

\[_{}(,)_{}(,0)\{||,||\}, >0.\]

As applications, we apply Theorem 15 to structured bandits and contextual bandits (Appendix G).