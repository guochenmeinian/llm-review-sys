# FOCAL: Contrastive Learning for Multimodal

Time-Series Sensing Signals in

Factorized Orthogonal Latent Space

 Shengzhong Liu\({}^{*}\), Tomoyoshi Kimura\({}^{}\), Dongxin Liu\({}^{}\), Ruijie Wang\({}^{}\), Jinyang Li\({}^{}\),

**Suhas Diggavi\({}^{@sectionsign}\), Mani Srivastava\({}^{@sectionsign}\), Tarek Abdelzaher\({}^{}\)**

\({}^{*}\)Shanghai Jiao Tong University, \({}^{}\)University of Illinois at Urbana-Champaign

\({}^{}\)Meta, \({}^{@sectionsign}\)Univeristy of California, Los Angeles

shengzhong@sjtu.edu.cn, {tkimura4, ruijiew2, jinyang7, zaher}@illinois.edu

dxliu@meta.com, {suhas, mbs}@ee.ucla.edu

###### Abstract

This paper proposes a novel contrastive learning framework, called FOCAL, for extracting comprehensive features from multimodal time-series sensing signals through self-supervised training. Existing multimodal contrastive frameworks mostly rely on the shared information between sensory modalities, but do not explicitly consider the exclusive modality information that could be critical to understanding the underlying sensing physics. Besides, contrastive frameworks for time series have not handled the temporal information locality appropriately. FOCAL solves these challenges by making the following contributions: First, given multimodal time series, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective. In contrast, the private space extracts modality-exclusive information through a transformation-invariant objective. Second, we propose a temporal structural constraint for modality features, such that the average distance between temporally neighboring samples is no larger than that of temporally distant samples. Extensive evaluations are performed on four multimodal sensing datasets with two backbone encoders and two classifiers to demonstrate the superiority of FOCAL. It consistently outperforms the state-of-the-art baselines in downstream tasks with a clear margin, under different ratios of available labels. The code and self-collected dataset are available at https://github.com/tomoyoshki/focal.

## 1 Introduction

As a representative self-supervised learning (SSL) paradigm, contrastive learning (CL) has achieved unprecedented success in vision tasks  and are increasingly leveraged in learning from time series . However, many IoT applications  rely on heterogeneous sensory modalities to collaboratively perceive the physical surroundings and lead to a more complicated learning space. This paper aims to build a contrastive learning framework that maximally extracts complementary information from multimodal time-series sensing signals ,. The key challenge is to define appropriate similarity/distance measures (_i.e._, who should be close to whom) in the joint multimodal embedding space that facilitates the downstream tasks .

Existing contrastive frameworks for time series either ignore the heterogeneity among sensory modalities and are limited to instance-level discrimination , or are designed to extract only shared information across sensory modalities . For instance, as a representative framework, CMC  builds on the hypothesis that only information shared across views corresponds to theactual physical target. We argue that the strength of multimodal sensing lies in that the collaborating modalities not only share information but also enhance each other by providing complementary information exclusive to the modalities. The pretraining objective in contrastive learning should be calibrated to extract all semantically meaningful and generalizable patterns.

In addition, we observe that existing contrastive frameworks for time series do not handle the temporal information locality in a proper way. Temporal contrastive works (_e.g._, TNC ) force the temporally close samples to be positive pairs of similarity 1 and force the temporally distant samples to be negative pairs of similarity 0, which may contradict long-term seasonality patterns. For example, a circling motorcycle may cause periodical vibration patterns to nearby microphone arrays, violating the above strict contrastive objective. Alternatively, TFC  optimizes the consistency between time-domain and frequency-domain representations, preventing neither encoder from simultaneously extracting features from the time-frequency spectrogram, which are known to achieve superior performance in learning from sensing signals .

To overcome these limitations, we propose a novel contrastive framework, FOCAL, for self-supervised representation learning from multimodal time-series sensing data. An overview is presented in Figure 1. Motivated by , we encode the input (_i.e._, fixed-length window of sensor readings) of each modality into a factorized orthogonal latent space, composed of a shared space and a private space. The shared features and private features of the same modality, as well as the private features between different modalities, are mutually orthogonal to emphasize their semantical independence. The shared space is designed to capture information consistency across modalities, while the private space captures modality-exclusive but transformation-invariant information. As we show in Figure 2, FOCAL outperforms CMC  by further emphasizing modality-exclusive sectors of target information. Besides, we define a temporal structural constraint to the distance measure of modality embeddings through a loose ranking constraint between temporally "close sample pairs" and "distant sample pairs". During the pertaining, we use a sampling strategy that randomly selects multiple short sequences with fixed length (_e.g._, 4 samples per sequence) to constitute training batches. Instead of performing temporal contrastive tasks as , we confine the average intra-sequence distance between temporally neighboring samples (_i.e._, \(\) in Figure 1) to be no larger than the average inter-sequence distances between temporally distant samples (_i.e._, \(\) in Figure 1). It simultaneously addresses overall temporal information locality and tolerates occasional violations of locality caused by periodicity, and it turns out to accelerate the pretraining convergence in practice.

On the one hand, we justify the feature orthogonality constraints from two aspects. First, the shared features and private features of a modality should be orthogonal, such that the private space can avoid reusing the shared information but instead exploit modality-exclusive discriminative information. Second, the private features of different modalities should be orthogonal to each other. Otherwise, the overlapped semantics should be included in their shared space. In summary, the orthogonality

Figure 1: Overview of the FOCAL framework. Best viewed in color.

Figure 2: Information diagram between CMC and the proposed FOCAL. Figure adapted from . Blue color denotes used information sectors.

constraint is imposed to refine the heterogeneous modality-exclusive information that is undervalued in existing cross-modal contrastive tasks.

On the other hand, the temporal information locality within time series is defined through coarse-grained distance orders for two reasons. First, we do not always regard temporally close samples as positive pairs (with similarity 1), such that fine-grained differences between neighboring samples can be included. Second, we only enforce the temporal constraint at the statistical average scale by comparing the average intra-sequence distances to the average inter-sequence distances to fit potential exceptions caused by long-term seasonal signal patterns and significantly reduce the computational complexity because we avoid traversing the ranking losses within all sample triplets .

In summary, we define the following four perspectives of pretraining objectives in FOCAL.

* **Modality Consistency in Shared Space:** In the shared space, we push the features of different modalities of the same sample to be similar to each other compared to randomly mismatched modality features from two temporally distant samples.
* **Transformation Consistency in Private space:** In the private space, we push the modality features under two random augmentations to be similar to each other compared to modality features from two samples.
* **Orthogonality Constraint:** We enforce the shared feature and private feature of the same modality, as well as private features of different modalities, to be orthogonal to each other.
* **Temporal Locality Constraint:** We restrict the average distance of samples within a short time sequence to be no larger than the average distance between two random sequences.

We extensively evaluate FOCAL against eleven state-of-the-art baselines on four multimodal sensing datasets with two different backbone encoders (DeepSense  and Swin-Transformer ). The finetuning performance is evaluated with two light-weight classifiers (_i.e.,_ linear and KNN classifier). It consistently achieves higher accuracy and F1 scores than the baselines. We also break down the contribution of individual components through step-by-step ablation studies.

## 2 Problem Formulation

Assume we have a set of \(P\) sensory modalities \(=\{M_{1},M_{2},,M_{P}\}\), and a large set of \(N\) unlabeled pretraining data from all sensory modalities \(=\{_{1},_{2},,_{N}\}\). Each sample is a fixed-length window of signals from all sensory modalities. For each sample \(_{i}\), we use \(_{ij}\) to denote the input from sensory modality \(M_{j}\). The original input of each sensory modality is a multivariate time series, and we apply Short-Time Fourier Transform (STFT) as the preprocessing procedure to extract the sample time-frequency representation. As we introduce in Appendix B, the processed modality input is \(_{ij}^{ I S}\), where \(C\) denotes the number of input channels, \(I\) denotes the number of time intervals within a sample window, and \(S\) denotes the spectrum length after applying the Fourier transform to each interval1. We have a set of backbone encoders \(=\{E_{1},E_{2},,E_{P}\}\) such that the encoder \(E_{i}\) of modality \(M_{i}\) can encode the modality input \(_{ij}\) into an embedding vector \(_{ij}=E_{j}(_{ij}))^{K}\), where \(K\) is the unified dimension of modality embedding vectors. We use \(_{ij},_{i^{}j^{}}\) to denote the inner product between two embedding vectors \(_{ij}\) and \(_{i^{}j^{}}\). The encoders extract both the time and frequency patterns from the preprocessed input. We also have a small set of \(N^{}\) supervised samples for finetuning \(^{s}=\{_{1}^{s},_{2}^{s},,_{N^ {}}^{s}\}\), where each sample \(_{i}^{s}\) is associated with a label \(y_{i}^{s}\). Please note \(^{s}\) is not necessarily a subset of the pretraining set \(\) due to potential domain adaptations, _i.e._, \(^{s}\). The objective of self-supervised pretraining is to use the unlabeled dataset \(\) to optimize the parameters of modality encoders \(\) such that the model accuracy finetuned on the supervised dataset \(^{s}\) is maximized.

## 3 FOCAL Framework

In this section, we start with an overview of the framework and then separately introduce the two main functional components: (1) contrastive learning in factorized orthogonal latent space, and (2) temporal structural constraint.

### Overview

The key questions to answer in designing a contrastive learning framework include the _selection of positive/negative pairs_ and the _contrastive loss functions_. Compared to image input about which we have almost no knowledge without human annotations, the meta-level information of multimodal time-series input, _i.e._, _cross-modal correspondence_ and _temporal information locality_, can be effectively leveraged during the pretraining to shape the learned joint embedding space. As we show in Figure 1, FOCAL groups multiple randomly sampled fixed-length (_i.e._, \(L\) samples) short sequences of samples into a batch \(\), with cardinality \(||=B\), and only conducts contrastive comparisons within the batch without any memory banks. Each modality input \(_{ij}\) goes through two randomly selected augmentations to generate two augmented versions, and each augmented input is separately encoded by the modality encoder \(E_{j}\) to get two versions of modality embeddings \(_{ij}\) and \(_{ij}\). The encoder network structures are not this paper's original contribution, so we leave them in Appendix D.

As the output of modality encoding, each modality embedding \(_{ij}\) is projected through a non-linear multilayer perceptron (MLP) projector into two orthogonal embeddings, a shared embedding \(_{ij}^{shared}\) and a private embedding \(_{ij}^{private}\). The two embeddings share all encoder layers except the MLP projector. Separate contrastive learning tasks are applied to each embedding to capture different aspects of information. At the same time, shared-private and private-private modality features of the same sample are mutually orthogonal. In addition, we apply a temporal structural constraint between intra-sequence distances and inter-sequence distances to both projected modality embeddings.

### Multimodal Contrastive Learning in Factorized Orthogonal Space

In multimodal collaborative sensing, information from different sensory modalities is not fully overlapped, thus extracting modality-exclusive discriminative information can reinforce the shared information during the downstream task finetuning. Projecting each modality embedding into separate shared space and private space while applying the orthogonality constraints avoids the shared information being reused in the private space optimization.

**Contrastive Task for Shared Space:** We use the shared feature space to learn modality consistency information. Specifically, we only consider samples within a batch \(\) but across different short sequences, and assume the same random augmentation is applied. We iterate over all pairs of modalities \(M_{j}\) and \(M_{j^{}}\), regarding different modality embeddings of the same sample (\(_{ij}^{shared}\), \(_{ij^{shared}}^{)}\) (_e.g._, 1 in Figure 1) as positive pairs, and regard two random modality embeddings from different samples (\(_{ij}^{shared}\), \(_{i^{}j^{}}^{shared}\)) as negative pairs. We calculate the following InfoNCE loss ,

\[_{shared}=-_{i}_{M_{j},M_{j^{}},j j ^{}}_{ij}^{shared},_{ij^{ }}^{shared}/)}}{_{i^{}}_{ij}^{shared},_{i^{}j^{}}^{shared}/ )}},\] (1)

where \(\) is a temperature parameter that controls the penalties on hard negative samples .

**Contrastive Task for Private Space:** We use the private feature space to learn modality-exclusive information that is useful in discriminating different sequences within a batch \(\), through capturing transformation consistency information. Specifically, for each modality \(M_{j}\), we consider the encoded embeddings of two randomly augmented versions of the same samples as positive pairs (_e.g._, 1 in Figure 1), _i.e._, \((_{ij}^{private}\), \(}_{ij}^{private})\), where we use \(}_{ij}^{private}\) to denote a differently augmented variant. The remaining 2B-2 modality embeddings in the batch are considered negative pairs to \(_{ij}^{private}\). We use the NT-Xent  loss for the private space contrastive task,

\[_{private}=-_{i}_{M_{j}}_{ij}^{private},}_{ij}^{private}/ )}}{_{i^{},i^{} i}_{ij}^{private},_{i^{}j}^{private}/)}+_{i^{ }}_{ij}^{private},}_{i^{}j}^{private}/)}}.\] (2)

**Orthogonality Constraint:** To enforce the orthogonality constraint between the shared feature and private feature of the same modality, as well as the private features between different modalities, such that they can capture independent semantic information in the factorized space, we apply a cosineembedding loss that directly minimizes their angular similarities,

\[_{orthogonal}=_{i}_{M_{j}}_{ij}^{ shared},_{ij}^{private}+_{i}_{M_{j},M_{j}^{} ,j j^{}}_{ij}^{private},_{ij^ {}}^{private}.\] (3)

### Temporal Structural Constraint

Appropriately shaping the temporal information locality in latent modality embedding space is challenging. First, simply considering temporally close samples as positive pairs and pushing their semantical similarities to 1 as [63; 49] can be problematic because they ignore the continuously evolving factors (_e.g._, distance) that make differences between temporally neighboring samples. Second, it is also impractical to predefine a fixed similarity curve with respect to the time difference between two samples, which is highly context-dependent and can not be known in advance.

Considering the complexity of temporal information correlations, we stop defining positive versus negative pairs in the temporal dimension. Instead, we only apply a loose ranking loss to specify the relationships of distances at the coarse-grained sequence level (intra-sequence distance vs. inter-sequence distance) as an information regularization . Given a sequence, we restrict the average distances between samples within the sequence (_e.g._, 1 in Figure 1) to be no larger than their average distance to samples from other sequences (_e.g._, 1 in Figure 1). To accommodate occasional violations of temporal locality caused by long time periodicity (such that temporally distant samples could be more similar than neighboring samples), and further reduce the computational complexity caused by multiplicative traverse of sample triplets, we restrict the average intra-sequence distances to be no larger than average inter-sequence distances.

We calculate the distance in the Euclidean space, to facilitate the downstream Euclidean classifiers. Given the sample-level distance matrix \(^{BL BL}\), we first compute the sequence-level mean distance matrix \(}^{B B}\) through aggregating sample-level distances2, such that the average distance between sequence \(s\) and sequence \(s^{}\) is \(_{ss^{}}=1/L^{2}_{i s,i^{} s^{}}D_{pq}\), then the temporal locality loss is defined by,

\[_{temporal}=_{s}_{s^{} s}(_{ss}- _{ss^{}}+,0),\] (4)

where the margin is the predefined degree of separation. It is worth noting that the temporal structural constraint is applied to holistic modality embeddings, including both the shared and private parts.

### Overall Training Objective

In summary, FOCAL pretraining simultaneously considers latent correlations between (1) synchronized embeddings of different modalities; (2) differently augmented modality embeddings; and (3) temporally neighboring sample embeddings. It minimizes the following overall loss,

\[=_{shared}+_{p}_{private}+ _{o}_{orthogonal}+_{t}_{temporal},\] (5)

where \(_{p}\), \(_{o}\), and \(_{t}\) are hyperparameters that control the weights of each loss component.

## 4 Evaluation

In this section, we evaluate FOCAL by comparing it with 11 popular SOTA self-supervised learning frameworks on four different datasets. We start with the experimental setups and then introduce the main evaluation results, followed by comprehensive ablation studies.

### Experimental Setup

**Datasets:** (1) **MOD** is a self-collected dataset using acoustic (8000Hz) and seismic (100Hz) signals to classify moving vehicle types. It includes 6 different vehicle types and 1 class of human walking. (2) **ACIDS** is an ideal dataset for vehicle classification using acoustic signals and seismic signals(both in 1025Hz). It includes data on 9 types of ground vehicles in 3 different terrains. (3) **RealWorld-HAR ** is a public dataset using accelerometer, gyroscope, magnetometer, and light signals (all in 50Hz) to recognize 8 common human activities. (4) **PAMAP2 ** is another public dataset using accelerometer, gyroscope, and magnetometer signals (all in 100Hz) to recognize 18 different physical activities. The length of the samples and the intervals, as well as the time overlap ratios between intervals within samples of each dataset, as listed in Table 1, are configured to achieve the best-supervised classification performance.

**Data Augmentations:** Candidate augmentations are defined in both the time domain before STFT and the frequency domain after STFT, where only one out of them is randomly selected in each forward pass. Time-domain augmentations include scaling, permutation, negation, time warp, magnitude warp, horizontal flip, jitter, channel shuffle, and time masking; frequency-domain augmentations include phase shift and frequency masking. Details can be found in Appendix B.

**Baselines:** We consider 12 baselines in total, including a supervised benchmark, three representative self-supervised learning frameworks from vision tasks that perform instance discrimination (SimCLR , MoCoV3 , and MAE ), four modality-matching contrastive frameworks for multimodal input (CMC , Cosmo , Cocoa , GMC ), three SOTA contrastive frameworks for time series (TS2Vec , TNC , TS-TCC ), and one predictive baseline (MTSS ). Their detailed introductions can be found in Appendix C. All compared frameworks use the same encoder structures (except for minor differences in module orders). The evaluation metrics are accuracy and (macro) F1 score.

**Backbone Encoders:** We apply two different modality encoders in this paper. (1) **DeepSense** uses convolutional layers to extract localized feature patterns within each time interval, and then uses recurrent layers to aggregate information across all intervals within a sample window. (2) **Swin-Transformer (SW-T)** uses stacked Transformer blocks to extract local information from shifted windows in a hierarchical manner. Their details and configurations can be found in Appendix D.

   Dataset & Classes & Modalities (Freq) & Sample Length & Interval (Overlap) & \#Samples & \#Labels \\  MOD & 7 & acoustic (8000Hz), seismic (100Hz) & 2 sec & 0.2 sec (0\%) & 39,609 & 7,335 \\ ACIDS & 9 & acoustic, seismic (both 1025Hz) & 1 sec & 0.25 sec (50\%) & 27,597 & 27,597 \\ RealWorld-HAR & 8 & acc, gyro, mag, (all 50Hz) & 5 sec & 1 sec (50\%) & 12,887 & 12,887 \\ PAMAP2 & 18 & acc, gyr, mag (all 100Hz) & 2 sec & 0.4 sec (50\%) & 9,611 & 9,611 \\   

Table 1: Statistical Summaries of Evaluated Datasets.

    &  &  &  &  \\  Encoder & Framework & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\   & Supervised & 0.9404 & 0.9399 & **0.9566** & 0.8407 & 0.9348 & **0.9388** & **0.8849** & **0.8761** \\   & SimCLR & 0.8855 & 0.8855 & 0.7438 & 0.6101 & 0.7138 & 0.6841 & 0.6802 & 0.6583 \\  & MoCo & 0.8808 & 0.8812 & 0.7717 & 0.6205 & 0.7859 & 0.7708 & 0.7559 & 0.7387 \\  & OMC & 0.9196 & 0.9186 & 0.8443 & 0.7244 & 0.7975 & 0.8116 & 0.7906 & 0.7706 \\  & MAE & 0.5981 & 0.5993 & 0.6644 & 0.5618 & 0.7565 & 0.7515 & 0.7114 & 0.6158 \\  & Cosmo & 0.8989 & 0.8998 & 0.8511 & 0.6929 & 0.8956 & 0.8888 & 0.8356 & 0.8135 \\  & Cocoa & 0.8774 & 0.8764 & 0.6644 & 0.5359 & 0.8465 & 0.8488 & 0.7603 & 0.7187 \\  & MTSS & 0.4153 & 0.3582 & 0.4352 & 0.2441 & 0.2989 & 0.1405 & 0.3541 & 0.1795 \\  & T2Vec & 0.7669 & 0.7648 & 0.5224 & 0.3587 & 0.6595 & 0.5984 & 0.5729 & 0.4715 \\  & GMC & 0.9257 & 0.9267 & 0.9096 & 0.7929 & 0.8869 & 0.8948 & 0.8119 & 0.7860 \\  & TNC & 0.9518 & 0.9528 & 0.8237 & 0.6936 & 0.8892 & 0.8971 & 0.8387 & 0.8143 \\  & TS-TCC & 0.8707 & 0.8735 & 0.7667 & 0.6164 & 0.8073 & 0.8010 & 0.7776 & 0.7250 \\   & FOCAL & **0.9732** & **0.9729** & 0.9516 & **0.8580** & **0.9382** & 0.9290 & 0.8588 & 0.8463 \\   & Supervised & 0.8948 & 0.8931 & 0.9137 & 0.7770 & 0.9313 & 0.9278 & **0.8612** & 0.8384 \\   & SimCLR & 0.9250 & 0.9247 & 0.9128 & 0.8144 & 0.7046 & 0.7220 & 0.7705 & 0.7424 \\  & MoCo & 0.9390 & 0.9384 & 0.9174 & 0.8100 & 0.7813 & 0.8024 & 0.7717 & 0.7313 \\  & CMC & 0.9129 & 0.9105 & 0.8128 & 0.6857 & 0.8840 & 0.8955 & 0.8080 & 0.7901 \\  & MAE & 0.7083 & 0.7772 & 0.8516 & 0.7023 & 0.8829 & 0.8813 & 0.7190 & 0.7606 \\  & Cosmo & 0.3429 & 0.3378 & 0.7110 & 0.6086 & 0.8664 & 0.8169 & 0.7741 & 0.7366 \\  & Cocoa & 0.7040 & 0.7038 & 0.7096 & 0.5794 & 0.8892 & 0.8861 & 0.7689 & 0.7317 \\  & MTSS & 0.4206 & 0.4163 & 0.4349 & 0.2250 & 0.5136 & 0.4370 & 0.2847 & 0.1714 \\  & TS2Vec & 0.7254 & 0.7174 & 0.7183 & 0.5748 & 0.6151 & 0.5955 & 0.6195 & 0.5426 \\  & GMC & 0.8640 & 0.8611 & 0.9402 & 0.7766 & 0.9319 & 0.9379 & 0.8312 & 0.8083 \\  & TNC & 0.8533 & 0.8539 & 0.8352 & 0.7372 & 0.8817 & 0.8784 & 0.8013 & 0.7506 \\  & TS-TCC & 0.8734 & 0.8735 & 0.9041 & 0.7547 & 0.8731 & 0.8454 & 0.7997 & 0.7260 \\   & FOCAL & **0.9805** & **0.9800** & **0.9489** & **0.8262** & **0.9451** & **0.9503** & 0.8580 & **0.8401** \\   

Table 2: Finetune Results with Linear ClassifierBoth models are implemented in PyTorch 1.14 and optimized with an AdamW  optimizer under cosine learning rate schedules . Without special specifications, we report the results on SW-T backbone. Detailed training configurations are introduced in Appendix E.

**Finetune Classifiers:** We evaluate with two lightweight classifiers during the finetune stage. **Linear profiling** adds a linear layer on top of the pretrained sample features (or concatenated modality features) with the encoder fixed during fine-tuning. **KNN classifier** directly uses the voting of 5 nearest neighbors (based on the Euclidean distances between samples) in the finetuning training samples to generate the predicted labels for the testing samples.

### Finetune Results

**Linear Probing:** Table 2 presents the finetune results using a linear classifier (The complete results can be found in Appendix F). FOCAL consistently achieves the best accuracy and F1 score across different datasets and backbone encoders. It outperforms CMC by 4.48% to 18.01% in accuracy, demonstrating the importance of extracting private modality information in addition to the shared information. SimCLR and MoCo achieved suboptimal performance on HAR datasets (_i.e._, RealWorld-HAR and PAMAP2) which have more than two modalities, showing that instance discrimination alone as the pretext task can not learn comprehensive feature patterns from multiple modalities. GMC, as the SOTA framework for multi-modal time series, beats the contrastive frameworks for single-modal time series (TS2Vec, TNC, and TS-TCC) in most cases, but is secondary to FOCAL. On MOD dataset, FOCAL achieves higher accuracy and F1 score than the supervised model by taking advantage of massive unlabeled samples.

Besides, we plot the finetune results on the MOD dataset under three different label ratios (100%, 10%, and 1%) in Figure 3. FOCAL overall achieves better label efficiency during the finetuning. It attains 3.51% relative improvement with 100% labels and 10.56% relative improvement with 1% labels, compared to the best-performing baseline. Among the baselines, TNC performs best with 1% label but does not generalize well when more labels are available (_e.g._, 100%).

**KNN Evaluation:** The evaluation results with a KNN classifier (K=5) using all available labels3 are plotted in Figure 4 (See Appendix F for complete results). KNN operates in a non-parametric way and can be expressive if the learned latent space aligns well with the underlying semantics. For multi-modal contrastive frameworks (_i.e._, FOCAL, CMC, Cosmo, Cocoa, GMC), we concatenate the modality features as the sample feature. FOCAL consistently performs better than the baselines across all datasets, proving its representational power in the non-parametric classification setting.

### Additional Downstream Tasks

**Clustering:** In multi-modal learning, the clusterability of the learned modality representations is preferable. Meaningful representation clusters should be coherent with the underlying semantic labels. For each modality, we first use K-means clustering with pretrained modality embeddings to get the cluster labels and measure the consistency between the cluster labels and the ground-truth category labels, with two common clustering metrics adjusted rand score (ARI)  and normalized mutual information score (NMI) . The results for five multi-modal frameworks are presented in Figure 5.

Figure 4: Finetuning results with KNN classifiers (K=5), with SW-T encoder.

Figure 3: Linear probing under different label ratios on MOD with SW-T encoder.

We separately evaluate the clusterability of each modality and report the mean and standard deviation among modalities. FOCAL is superior in learning representations better aligned with the target labels. Although CMC performs closely to FOCAL on MOD and RealWorld-HAR datasets, it performs poorly on ACIDS and PAMAP2 datasets. Besides, we qualitatively visualize the concatenated sample embeddings of the multi-modal contrastive frameworks after t-SNE  dimension reduction on MOD dataset in Figure 6. We can see that FOCAL achieved better separation among different classes than the compared baselines.

**Distance and Speed Classification:** In the MOD dataset, we separately collect new data samples recording the distance and speed of the vehicles. Data in this experiment is collected in a different environment with different vehicles from the pretraining data, thus accounting for potential domain shifts. The same pretrained models in previous experiments are used, and the results are summarized in Figure 7. The instance discrimination frameworks (_i.e._, SimCLR and MoCo) are relatively more resilient than the modality matching frameworks (_i.e._, CMC, Cocoa, Cosmo, and GMC), while FOCAL fills this gap by capturing the transformation consistency information in private modality spaces.

### Ablation Studies

Figure 5: Clustering evaluation results. We use SW-T as the backbone encoder. The mean and standard deviation among sensory modalities are reported. Higher scores are better.

Figure 6: t-SNE visualization of the concatenated modality features (DeepSense encoder, MOD dataset). Different colors represent different object classes.

    &  &  &  &  \\   & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\  FOCAL-noPrivate & 0.9296 & 0.9284 & 0.7981 & 0.7100 & 0.8869 & 0.8768 & 0.7938 & 0.7787 \\  FOCAL-noOrth & 0.9705 & 0.9692 & 0.9311 & 0.8261 & 0.9186 & 0.9257 & 0.8371 & 0.8233 \\ FOCAL-wDstInd & 0.5773 & 0.5502 & 0.4926 & 0.4157 & 0.9099 & 0.9084 & 0.6518 & 0.5503 \\  FOCAL-noTemp & 0.9671 & 0.9659 & 0.9456 & 0.8014 & 0.9361 & 0.9425 & 0.8367 & 0.8255 \\ FOCAL-wTempCon & 0.9363 & 0.9359 & 0.9287 & 0.7587 & 0.8793 & 0.8842 & 0.8391 & 0.8242 \\  FOCAL & **0.9805** & **0.9800** & **0.9489** & **0.8262** & **0.9451** & **0.9503** & **0.8580** & **0.8401** \\   

Table 3: Ablation Results with SW-T Encoder and Linear Classifier.

Figure 7: Additional downstream tasks on MOD dataset.

**Setup:** Here we compare with five variants of FOCAL, including **FOCAL-noPrivate** (no private space), **FOCAL-noOrth** (no orthogonality constraint), **FOCAL-wDistInd** (replace orthogonality with distributional independence), **FOCAL-noTemp** (no temporal structural constraint), **FOCAL-wTempCon** (replace temporal structural constraint with the temporal contrastive task).

**Analysis:** The ablation study results are summarized in Table 3. First, the accuracy decreases by 5.20%-5.90% after removing the private space and the related contrastive task, since only the shared information among modalities is leveraged. Second, on top of the private space task, both the orthogonality constraint and the temporal structural constraint further improve the accuracy by 1.39%-2.99%, which proves they both contribute positively to FOCAL. Third, replacing the geometrical orthogonality constraint with the distributional independence constraint causes significant degradation and may even cause the model training to collapse. Similarly, in our experiments, conducting temporal contrastive tasks leads to a noticeable accuracy decrease in three out of four datasets. To further demonstrate the contribution of our temporal structural constraint in accelerating the convergence during the pretraining, we use KNN classifier to periodically evaluate the quality of the learned representations. Concatenated modality embeddings are used as sample representations. We visualize the achieved KNN accuracy curves on MOD and PAMAP2 with and without the temporal constraint in Figure 8. The temporal constraint clearly improves the semantical structure of the learned embeddings in early epochs of pretraining, by rejecting obviously counter-intuitive parameter values that violate the constraint.

### General Applicability of the Temporal Constraint

To validate the general applicability of the proposed temporal constraint, we apply it to multiple contrastive learning baselines (_i.e._, SimCLR, MoCo, CMC, Cocoa, and GMC). Table 4 and 5 summarize the results on ACIDS and PAMAP2 respectively, and we have observed noticeable performance improvement in most cases (up to 18.99% on ACIDS and up to 8.39% on PAMAP2). It demonstrates that the temporal constraint can be used as a plugin to enhance existing contrastive learning frameworks for time-series data.

### Sensitivity Test on Loss Weights

We also perform a sensitivity test on the loss weight values and plot the performance of FOCAL against different hyperparameters in Figure 9. We observe that FOCAL is generally robust against the

    &  &  &  &  &  \\   & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\  wTemp & **0.7461** & **0.6938** & **0.7836** & **0.6618** & **0.8690** & 0.7090 & **0.8543** & **0.7665** & **0.9347** & **0.8109** \\ Vanilla & 0.7438 & 0.6101 & 0.7717 & 0.6205 & 0.8443 & **0.7244** & 0.6644 & 0.5359 & 0.9096 & 0.7929 \\   

Table 4: Benefits of Temporal Constraints to SOTA baselines on ACIDS

Figure 8: Convergence curves.

    &  &  &  &  &  \\   & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\  wTemp & **0.7129** & **0.6884** & **0.7800** & **0.7602** & 0.7804 & 0.7583 & **0.8442** & **0.8146** & **0.8253** & **0.8114** \\ Vanilla & 0.6802 & 0.6583 & 0.7559 & 0.7387 & **0.7906** & **0.7706** & 0.7603 & 0.7187 & 0.8119 & 0.7860 \\   

Table 5: Benefits of Temporal Constraints to SOTA baselines on PAMAP2

Figure 9: Loss weights sensitivity test.

hyperparameter selections, with less than 2% accuracy fluctuations in all cases. For this reason, we did not perform a comprehensive hyperparameter search in our experiment. Besides, combining our observations in the ablation study that the private space task, orthogonality constraint, and temporal constraint all contribute positively to the performance of FOCAL, we conclude that the competition between learning objectives does not happen in FOCAL.

## 5 Related Works

**Self-Supervised learning for multimodal data.** Self-supervised learning from multimodal data has been extensively studied in the literature, including contrastive learning [48; 35; 6; 37; 38; 25; 32; 2], masked autoencoders (MAE) [13; 11; 19], and variational autoencoder (VAE) [18; 50; 21], surpassing conventional generation-based semi-supervised learning approaches . As a leading paradigm in learning transferable and discriminative features [66; 24; 57] with a variety of successful applications [16; 29; 40; 30; 47], we mainly consider contrastive learning in this paper. On one hand, as we show in the experimental results, normal contrastive frameworks based on instance discriminations (SimCLR , MoCO , BYOL , SimSiam ) may lead to suboptimal results without accommodating the multimodal properties. On the other hand, existing contrastive learning frameworks for multimodal data [38; 48; 35; 6; 37] mostly focus on the consistency between modalities but ignore the information heterogeneity when they are collaboratively utilized in sensing tasks. CLIP , as a representative multimodal contrastive framework, mainly addresses the importance of using natural language to augment the visual models through learning their pairings. Similarly, both CMC  and GMC  highlight the information matching between modality embedding to another modality embedding or the joint embedding. Instead, in FOCAL, we simultaneously consider modality consistency and discriminative modality-exclusive information by designing corresponding contrastive tasks and imposing the information independence constraint. Besides, most existing multimodal contrastive learning frameworks are limited to vision and language modalities [62; 22; 47; 64; 31], while the domain knowledge might not be directly applicable to sensory modalities with time-series signals in IoT applications.

**Contrastive learning for time series.** There has been increasing interest in developing contrastive learning frameworks for time-series data [10; 63; 44; 7; 49; 22; 65; 36]. TS2Vec , TFC , TNC , and TS-TCC  were based on the time-series properties but did not consider the multimodal collaboration properties. Besides, TFC  and RF-URL  were designed from the consistency between time and frequency representations, or different time-frequency representations, while restricting the backbone encoder structures. Cosmo  and Cocoa  are the most recent attempts at contrastive learning from multi-modal time series, but they were not able to sufficiently utilize the complementary information from sensory modalities. CoST  proposed to separate the seasonal-trend representations for time series forecasting. Different from the existing works, the objective of FOCAL is to maximally extract complementary and discriminative features from multimodal sensory modalities, to facilitate the downstream recognition tasks. CLUDA  worked on the unsupervised domain adaptation (UDA) problem for time series, which is not directly comparable to our solution. In FOCAL, no expert features are assumed to be available as in ExpCLR .

## 6 Conclusion

We proposed a novel contrastive learning framework, FOCAL, for self-supervised learning from multimodal time-series sensing signals. FOCAL encodes each modality input into a factorized orthogonal latent space including shared features and private features. We learn each part by applying different contrastive learning objectives addressing the modality consistency in the shared space and the transformation invariance in the private space. Besides, we design a lightweight temporal structural constraint as an information regularization during the pretraining. Extensive evaluations on four multimodal sensing datasets with two encoder networks and two lightweight classifiers, demonstrate the superiority of FOCAL over 11 SOTA baselines, under different levels of supervision. Our future work would focus on extracting domain-invariant features in multi-vantage sensing applications to make the pretrained model resilient against task-unrelated environmental factors (_e.g._, terrain, wind, sensor-facing directions).