# Safe Exploitative Play with Untrusted Type Beliefs

Tongxin Li\({}^{1}\)

Correspondence to: Tongxin Li <litongxin@cuhk.edu.cn>.

Tinashe Handina\({}^{2}\)

Shaolei Ren\({}^{3}\)

Adam Wierman\({}^{2}\)

\({}^{1}\)School of Data Science

The Chinese University of Hong Kong, Shenzhen, China

litongxin@cuhk.edu.cn

\({}^{2}\)Computing + Mathematical Sciences

California Institute of Technology, Pasadena, USA

{thandina, adamw}@caltech.edu

\({}^{3}\)Electrical & Computer Engineering

University of California, Riverside, USA

shaolei@ucr.edu

###### Abstract

The combination of the Bayesian game and learning has a rich history, with the idea of controlling a single agent in a system composed of multiple agents with unknown behaviors given a set of types, each specifying a possible behavior for the other agents. The idea is to plan an agent's own actions with respect to those types which it believes are most likely to maximize the payoff. However, the type beliefs are often learned from past actions and likely to be incorrect. With this perspective in mind, we consider an agent in a game with type predictions of other components, and investigate the impact of incorrect beliefs to the agent's payoff. In particular, we formally define a tradeoff between risk and opportunity by comparing the payoff obtained against the optimal payoff, which is represented by a gap caused by trusting or distrusting the learned beliefs. Our main results characterize the tradeoff by establishing upper and lower bounds on the Pareto front for both normal-form and stochastic Bayesian games, with numerical results provided.

## 1 Introduction

_"The Chinese symbol for crisis is composed of two elements: one signifies danger and the other opportunity."_ -- Lewis Mumford, 1944

The famous interpretation of the Chinese word for 'crisisis' (known as \(\%\)), although based on mistaken etymology, captures the dual nature of risk and opportunity. It provides an interesting metaphor for the inherent complexities within real-world multi-agent systems. These systems, where risk and opportunity are often inextricably linked, play a pivotal role across diverse domains, ranging from human-AI collaboration  and cyber-physical systems , to highly competitive environments like real-time strategy games  and poker .

In conventional applied and theoretical frameworks, it is typically assumed that all agents either cooperate or adhere to pre-defined policies . However, real-world scenarios often defy these simplifications, presenting agents that display a spectrum of behaviors ranging from cooperative, to heterogeneous, irrational, or even adversarial . This deviation from expected behavior patternscomplicates the dynamics of multi-agent systems, as agents cannot reliably predict the actions of their counterparts. The uncertainty regarding whether to trust or distrust predictions naturally leads to a critical tradeoff between the coexisted risk and opportunity, reflecting the dual aspects highlighted in Mumford's remark.

As critical examples, Bayesian games  provide an approach for modeling differing types of agents in strategic environments. In these games, players form beliefs about others' types and update beliefs in response to observed actions and choose their actions accordingly. For example, in competitive settings like poker or even in simple games like matching pennies, deviating from the game theoretic optimal (GTO) strategy  to exploit weaker opponents can be beneficial, but this approach also relies on potentially flawed type beliefs , making it risky to take advantage of such side-information. Similarly, in real-world problems like security games, having a prior distribution of attackers' behavioral types in a Bayesian game setting leads to advantages. However, exploiting incorrect types can be risky compared to just using minimax strategies. Despite the ubiquity of incorrect type beliefs in practical scenarios, limited attention has been paid to explore such a tradeoff, with exceptions in designing heuristically safe and exploitative strategies in specific contexts, such as with Byzantine adversaries  and sequential games .

Inaccuracies in beliefs about others' types may arise from factors such as using out-of-distribution data to generate priors, changes in opponents' behaviors, or mismatches between hypothesized types and the optimal type space, etc. As noted in , it is evident that prior beliefs significantly affect the long-term performance of type-based learning algorithms like the Harsanyi-Bellman Ad Hoc Coordination (HBA). Although algorithms like HBA demonstrate asymptotic convergence to correct predictions or type distributions through various methods of estimating posterior beliefs, and methods exist for detecting inaccuracies in type beliefs using empirical behavioral hypothesis testing , the theoretical capabilities of general algorithms remain uncertain.

In general, relying on learned beliefs presents a fundamental tradeoff between the potential payoffs from exploitative play and the risk of incorrect type beliefs. Given the potential inaccuracies in these type beliefs, using them to exploit opponents could lead to high-risk strategies. Conversely, not exploiting these beliefs might result in overly cautious play. Therefore, it is natural to investigate the impact of incorrect beliefs on the agent's payoff, in terms of a tradeoff between trusting or distrusting the beliefs of types provided by type-based learning algorithms (e.g., Bayesian learning , best response dynamics , and policy iteration with neural networks , etc.) in multi-agent systems. To summarize the focus of this paper, we aim to address the following critical question:

_What is the fundamental tradeoff between trusting/distrusting type beliefs in games?_

**Contributions.** Motivated by the above question, we analyze the following _payoff gap_ that arises from erroneous type beliefs:

\[(;)_{d(,^{}) }(_{}(,^{})- ((),^{})),\ \ \ \] (1)

where \(\) and \(^{}\) are predicted and true type beliefs respectively; \(d(,)\) measures the distance between \(\) and \(^{}\) bounded from above by \(\) and will be formally specified with concrete model contexts in Section 3 and Section 4, together with the payoff, denoted by \((,)\) as a function of the true type \(^{}\) and the deployed strategy \(\). We denote the given strategy space by \(\). The agent's strategy \(()\) may depend on the type belief \(\). Overall, the payoff difference (1) above quantifies the worst-case gap between the optimal payoff (obtained by an optimal strategy in \(\) that maximizes \((,^{})\)), and the payoff corresponding to \(\).

In particular, when the agent's strategy \(\) trusts the belief \(\), it takes the opportunity to close the gap in (1) when the belief error \(\) is small. However, when \(\) increases, such a strategy incurs a high risk since it trusts the incorrect \(\). Evaluating the payoff gap in (1) naturally yields a tradeoff between opportunity and risk, as illustrated on the right of Figure 1. To be more precise, we measure the tradeoff between two important quantities:

_(Missed) Opportunity_: \((0;)\), corresponding to the case when the type beliefs are correct;

_Risk_: \(_{>0}(;)\) measuring the payoff difference incurred by worst-case incorrect beliefs.

In summary, the (missed) opportunity measures the discrepancy of a strategy \(\) from the optimal strategy, which is aligned with the ground truth belief \(^{}\) of other players in terms of the obtained payoff. Additionally, the risk quantifies how inaccurate beliefs impact the difference in terms of payoffs in the worst case. The goal of this paper is to investigate safe and exploitative strategies in Bayesian games that achieve near-optimal opportunity and risk.

Our main results are two-fold. Firstly, in normal-form Bayesian games, we characterize a tradeoff between the opportunity and risk. We consider a strategy as a convex combination of a safe strategy and the best response given type beliefs. Upper bounds on opportunity and risk are provided in Theorem 3.1. Conversely, lower bounds that hold for any mixed strategy are shown in Theorem 3.2. Notably, when the game is fair, and the hypothesis set \(\) is sufficiently large, these bounds tightly converge. Secondly, we explore a dynamic setting in stochastic Bayesian games, where an agent, provided with type beliefs about other players, engages in interactions over time, as illustrated in Figure 1. Unlike the normal-form approach, we utilize a value-based strategy that establishes upper bounds on opportunity and risk, as outlined in Theorem 4.1. Additionally, Theorem 4.2 provides lower bounds on opportunity and risk that differ from the upper bounds by multiplicative constants, yielding a characterization of the opportunity-risk tradeoff. Finally, a case study of a security game, simulating a defender protecting an elephant population from illegal poachers, is provided in Section 5.

## 2 Related Work

**Learning Types in Games.** The concept of type-based methods dates back to the development of Bayesian games, as first established by Harsanyi in the late 1960s [10; 11]. The concepts of learning and updating beliefs appear in pioneering works like the adaptive learning . While much game theory research, including work by Kalai et al. , focuses on equilibrium analysis through Bayesian belief-based learning, other studies, such as those by Nachbar et al. [21; 22] and [23; 24], reveal the challenges players face in making correct predictions while playing optimally under certain game conditions and assumptions. From an application perspective, Southey et al.  applied type-based methods to poker, where players' hands are partially hidden. They demonstrated how to maintain and use beliefs to determine the best strategies in this setting. Besides classic results exemplified above, closely related to our results, the recent work by Milec et al.  addresses the limitations of Nash equilibrium strategies in two-player extensive-form games, particularly their inability to exploit the weaknesses of sub-optimal opponents. They defined the exploitability of a strategy as the expected payoff that a fully rational opponent can gain beyond the game's base value and introduced a method that ensures safety, defined as an upper limit on exploitability compared to the payoff obtained using a Nash equilibrium strategy. However, the proposed tradeoff between the exploitation of the opponent given the correct model and safety against an opponent who can deviate arbitrarily from the predicted model is applicable specifically to an algorithm that employs a continual depth-limited restricted Nash response.

**Online Decision-Making with Predictions.** The tradeoff between opportunity and risk analyzed in this work is motivated by the recent progress in algorithms with predictions, also known as learning-augmented algorithms [26; 27] for online decision-making problems such as caching [28; 29; 30], bipartite matching , online optimization [32; 33], control [34; 35; 36; 37], valued-based reinforcement learning [38; 39; 40], and real-world applications [41; 42; 43; 44]. This line of work investigates the impact of untrusted predictions on two key metrics known as consistency and robustness, which are defined based on the competitive ratio of the considered contexts. It is also worth highlighting that previous works in decision-making often provide best-of-both-worlds guarantees for both stochastic and adversarial environments [45; 46], while the results in this work shed light on studying the intermediate regimes that do not fully align with either stochastic or

Figure 1: **Left**: A stochastic Bayesian game where an agent interacts with an environment and opponents, with a belief of their types \(\). **Right**: The tradeoff between trusting and distrusting type beliefs, with trust leading to higher risk and opportunity and distrust resulting in lower risk and opportunity, implying an opportunity-risk tradeoff with varying strategy \(\).

adversarial settings and delves into interactive environments formed by players whose behaviors may deviate from the type beliefs.

**Stochastic Bayesian Games.** Our results presented in Section 4 draw on foundational concepts from stochastic Bayesian games as outlined by Albrecht et al. [47; 15], which merge concepts of the Bayesian games [10; 11] and the stochastic games , with applications in cooperative multi-agent reinforcement learning . In , three methods--product, sum, and correlated--for integrating observed evidence into posterior beliefs have been explored. Specifically, it has been demonstrated that the Harsanyi-Bellman Ad Hoc Coordination (HBA) eventually make right future predictions under specific conditions using the product posterior beliefs. However, while HBA may eventually align with the correct type distribution, it is not guaranteed to learn it with the product posterior beliefs. Under certain conditions, HBA with the sum and correlated posterior converges to the correct type distribution. Nonetheless, even though methods like empirical behavioral hypothesis testing are available to detect inaccuracies in type beliefs, a comprehensive theoretical analysis is still lacking.

## 3 Normal-Form Bayesian Games with Untrusted Type Beliefs

Let \(\|\|_{1}\) denote the \(_{1}\)-norm and \(\|\|_{}\) denote the (element-wise) max norm.

### Problem Setting, Opportunity, and Risk

Consider a normal-form game (NFG) with two players: Player 1 possesses a payoff matrix \(A^{a b}\) with \(\|A\|_{}\), where the first player has \(a\) choices of the rows and the second player has \(b\) choices of the columns of \(A\). Player 1 forms a belief about Player 2's mixed strategy, denoted by a distribution \(\) over a set of hypothesized strategies \(\) that contains a ground truth strategy \(y^{}\).2

As a concrete example of the _payoff gap_ proposed in (1), the following benchmark characterizes the gap between payoffs obtained by a strategy with the machine-learned belief \(\) and an optimal strategy knowing \(y^{}\) beforehand:

\[_{}(;)_{d(,y^{}) }(_{x_{a}}x^{}Ay^{}-()^{ }Ay^{}),\ \ \] (2)

given a fixed policy \(:_{}_{a}\) that outputs a mixed strategy of the first player knowing \(\), where \(d(,y^{})\|_{}[y]-y^{}\|_{1}\), \(_{}\) and \(_{a}\) are the sets of probability distributions on \(\) and \(a\) different choices of rows respectively.

In particular, we focus on measuring a tradeoff between two important quantities, the _(missed) opportunity_\(_{}(0;)\) and the _risk_\(_{>0}_{}(;)\). The former measures how far the considered strategy \(\) is away from the optimal strategy knowing the ground truth type \(y^{}\) of Player 2 in terms of the payoff obtained; the latter quantifies the worst-case impact of inaccurate belief on the payoff difference.

### Motivating Example: Matching Pennies

Before presenting general results, we illustrate the underlying concepts through a simple normal-form game. Consider the classic matching pennies game as an example, where the mixed strategies for the players are defined as follows.

Suppose \(a=b=2\) and each of the two players has the option to choose either Heads (H) or Tails (T). Let \(y^{}\) be the true probability that Player 2 plays H and \(1-y^{}\) the probability of playing T. Suppose the hypothesis set \(\) contains all possible mixed strategies, each corresponding to a type of Player 2. Then, Player 1 receives a belief of \(y^{}\), denoted by \(y\), and chooses a strategy \(\) that depends on \(y\). Similarly, let \(x=(y)\) be the probability that Player 1 plays H and \(1-x\) the probability of playing T. If two players' actions match, Player 1 will receive 1 and \(-1\) otherwise. The problem setting is summarized in Figure 2. Given \(y^{}\) and \(x\), the expected payoff is therefore \((2y^{}-1)(2x-1)\). The best response of Player 1, depending on \(y\), is characterized by:

\[(y)=x=0&y<,\\ x&y=,\\ x=1&y>.\] (3)Given a strategy \(:\), the payoff gap in (2) for this example is instantiated as

\[_{}(;)_{|y-y^{}| }2(2y^{}-1)(y^{})-(2y^{}-1)(y) \ \ .\] (4)

**Impossibility.** We first consider an arbitrary strategy \(:\) that misses at most \((1-)\) opportunity. Suppose \(\) chooses \(\) with a probability \((y)\) and \(\) with a probability \(1-(y)\). Therefore, setting \(=0\), the payoff gap must satisfy

\[_{}(0;)=_{y}2(2y-1)((y)-(y) ) 1-,\]

which implies \((0)(1-)/2\) and \((1)(1+)/2\) by setting \(y=0\) and \(y=1\), respectively. Thus, plugging in \(y^{}=1\) and \(y=0\), we conclude that

\[_{}_{}(;)_{|y-y^{}|  1}2((y^{})-(y)) 2((1)-(0) ) 1+,\]

since \((1)=1\) by (3). This implies that \(\) has at least \((1+)\) risk. Note that this argument holds for any arbitrarily chosen \(\). In a word, any strategy misses at most \(1-\) opportunity must incur at least \(1+\) risk, in terms of the payoff.

**Mixed Strategy Existence.** Now, let us construct a concrete strategy to derive upper bounds on the (missed) opportunity and risk. The idea is to combine the strategy \((y)\), which exploits the belief \(y\) of types, and the Nash equilibrium strategy of this game, known as \(=1/2\), which is a solution of the minimax problem \(_{y}_{x}(2y-1)(2x-1)\) that enhances safety.

Fix \((y)(y)+(1-)\) a mixed strategy as a convex combination of the two strategies.

The payoff gap in (4) satisfies

\[_{}(;)_{|y-y^{}| }2(2y^{}-1)((y^{})- (y))+(1-)((y^{})-1/2).\]

_Opportunity:_ Therefore, there exists a mixed strategy \(\) such that when \(=0\) (i.e., \((y)=(y^{})\)), its (missed) opportunity is bounded by

\[_{}(0;)=_{y}2(2y-1)(1-)( (y)-1/2) 1-.\]

_Risk:_ Moreover, maximizing over \(\) such that \(y=0\), \(y^{}=1\), \((y^{})-(y)=1\), the risk for \(\) always satisfies \(_{}_{}(;) 1+\).

In a word, we find a strategy that misses \(1-\) opportunity and meanwhile has \(1+\) risk.

**Pareto Optimality** In conclusion, above construction shows that there is a mixed strategy \(\) for the matching pennies that misses \((1-)\) opportunity and incurs \((1+)\) risk. Conversely, any strategy that misses at most \((1-)\) opportunity must have at least \((1+)\) risk. The segment on the right of Figure 2 represents a Pareto front, and the strategy constructed as a convex combination is confirmed to be Pareto optimal following the arguments above. This motivating example indicates a tight tradeoff between opportunity and risk for matching pennies. In the sequel, we further generalize this result to normal-form games.

### Opportunity-Risk Tradeoff for Normal-Form Games

In general, the opportunity-risk tradeoff depends on the hypothesis set \(\) that contains a subset of candidate strategies and the payoff matrix of the game. We state useful definitions to characterize properties of the hypothesis set \(\) and the considered normal-form game.

**Definition 1**.: _Given a hypothesis set \(\), we let the diameter of \(\) with respect to the \(_{1}\)-norm be \(()_{y,z}\|y-z\|_{1}\). Define the following **type intensity**_

\[()_{y,z}(_{i:y_{i} z_{i}}z_{i} -_{i:y_{i}>z_{i}}z_{i})_{i:y_{i} z_{i}}y_{i}<_{i:y_{i}>z_{i}}y_{i}.\] (5)

_Furthermore, with fixed \(\) and \(A\), we define the maximum and value of the game by_

\[_{}(A)_{y}_{x_{a}}|x^{ }Ay|},_{}(A)_{y}_{x _{a}}x^{}Ay}.\] (6)

The type intensity, as defined in (5), quantifies the divergence between two distributions within the set \(\), specifically those that maximize the given objective. As the density of \(\) increases, the value of \(()\) approaches \(1\).

**Theorem 3.1** (Nfg Existence).: _Fix any \(\) and consider a general-sum normal-form game where Player 1 has a payoff matrix \(A^{a b}\) with \(_{}(A)\) and \(_{}(A)\). For any \(0 1\), there exists a mixed strategy \(:_{}_{a}\) for Player 1 that misses \((1-)(-)\) opportunity and has \((1-)(-)+()\) risk._

To show Theorem 3.1, we construct a mixed strategy as follows. Denote by \(\) as the following safe/minimax strategy of Player 1, which is the Nash equilibrium when the game is zero-sum:

\[_{y}^{}Ay=_{x_{a}}_{y }x^{}Ay.\] (7)

Motivated by the matching pennies example in Section 3.2, Player 1 implements a mixed strategy \(()+(1-)\) given the predicted belief \(\) as a distribution over types in \(\), where \(_{a}\) is a best response strategy given \(\) such that \(x^{}A_{}[y]\) is maximized. The detailed proof of Theorem 3.1 is provided in Appendix A.

Moreover, we further show the following impossibility result, indicating that the tradeoff in Theorem 3.1 is tight. We relegate the proof of Theorem 3.2 to Appendix B.

**Theorem 3.2** (Nfg Impossibility).: _For any \(\) satisfying \(() 0\), there is a payoff matrix \(A^{a b}\) with \(_{}(A)\) and \(_{}(A)\) such that for any \(0 1\), if any mixed strategy \(:_{}_{a}\) for Player 1 misses at most \((1-)(-)\) opportunity, then it incurs at least \((()-)(1+)\) risk._

In particular, Theorem 3.1 and 3.2 together imply the following special case when the hypothesis set \(\) contains all possible mixed strategies in \(_{b}\). This corollary highlights that for a fair game, the tradeoff is tight and the mixed strategy \(()+(1-)\) with \(\) is Pareto optimal when the hypothesis set contains all possible mixed strategies.

**Corollary 3.1** (Nfg Pareto Optimality).: _Suppose \(=_{b}\) and the game is fair, where Player 1 has a payoff matrix \(A^{a b}\) with \(\|A\|_{}\). For any \(0 1\), there exists a mixed strategy \(\) for Player 1 that misses \((1-)\!\) opportunity and has \((1+)\!\) risk. Furthermore, there is a payoff matrix \(A^{a b}\) with \(\|A\|_{}\) such that for any \(0 1\) if any mixed strategy \(\) for Player 1 misses at most \((1-)\!\) opportunity, then it incurs at least \((1+)\!\) risk._

It is worth noting that the requirement can be relaxed straightforwardly so that as long as the hypothesis set contains two mixed strategies such that \(()=1\) in (5), the statement holds. The proof of Corollary 3.1 can be found in Appendix C.

## 4 Stochastic Bayesian Games with Untrusted Type Beliefs

In many real-world applications, agents are coupled through a shared state in stochastic Bayesian games (SBG) [47; 15], which combines standard Bayesian games [10; 11] with the stochastic games . In this section, we consider an infinite-horizon time-varying discounted \(n\)-player stochastic Bayesian game, represented by a tuple \(,,,,p,r,\), and analyze the impact of beliefs on the opportunity-risk tradeoff.

### Preliminaries: MDP for Stochastic Bayesian Games

Let \(\) be a finite set of states. Write \(\{1,,n\}\). Let \((i)\) be the set of finitely many actions that player \(i\) can take at any state \(s\). Furthermore, \(:=(1)(n)\) denotes the set of action profiles \(a=(a(i):i)\) with \(a(i)(i)\). The types of other players are unknown to the \(i\)-th player. The set \(=_{j i}(j)\) is a product type space where each opponent \(j\) chooses types from \(_{j}\). We focus on the decision-making of the \(i\)-th player, considering stationary (Markov) strategies.3 At each time \(t 0\) each player \(j i\) uses a mixed strategy \(_{j}:(j)_{(j)}\) parameterized by a type that depends on the current state and the true type \(_{j}^{}(j)\). We use \(r:\) to denote the reward function for the \(i\)-th player, which is assumed to be bounded, as formally defined below.

**Assumption 1**.: _The reward \(r:\) satisfies that \(|r(s,a)| r_{}\) for all \(s\), \(a\)._

For any pair of states \((s,s^{})\) and action profile \(a\), we define \(p(s^{}|s,a)\) as a transition probability from \(s\) to \(s^{}\) given an action profile \(a\). Finally, let \((0,1)\) be a discount factor. We define the expected utility of the \(i\)-th player using a strategy \(_{i}\) as the expected discounted total payoff

\[J(_{i};_{-i}^{}):=[_{t=0}^{ }^{t}r(s_{t},a_{t})],\] (8)

where \(_{-i}^{}(_{j}^{}(j):j i)\), with respect to stochastic processes \((s_{t} p( s_{t-1},a_{t-1}))_{t>0}\) and \((a_{t}(i)_{i}(s_{t}))_{t 0}\), which generate the state and the action trajectories. The expectation in (8) is taken with respect to all randomness induced by the initial state distribution \(s_{0} p_{0}_{}\), the state transition kernel \(p\), and strategy profile \(\) (as well as opponents' types in \(\)).

### Opportunity, Risk, and Type Beliefs

Suppose the \(i\)-th player has beliefs of the other players' types, provided by a machine-learned forecaster, denoted by \(_{-i}\). For notational simplicity, we write \(_{i}\), the strategy for the \(i\)-th player as \(\), and the strategies \(_{-i}\) for the opponents as \(\) (parameterized by \(_{-i}\)). Furthermore, we omit the subscripts and denote the type beliefs and true types as \(\) and \(^{}\), and write the payoff in (8) as \(J()\) if there is no ambiguity. The \(i\)-th player uses a strategy \(:_{(i)}\), which is a function of the type beliefs of the other players. Given a strategy \(\) used by the agent and strategies by other players, denoted by \(\), we define the following useful value functions of the game:

\[V^{,}(s)_{p,,}[_{=t}^{ }^{-t}r(s_{t},a_{t})|s_{t}=s],\] (9)

which satisfies the Bellman equation \(V^{,}(s)=_{a(s),a^{}(s)}[ (r+V^{,})(s,(a,a^{})) ],\) where we define an operator \(V^{}(s,(a,a^{}))_{s^{} p(  s,(a,a^{}))}[V^{,}(s^{})]\).

The following worst-case payoff gap is defined similarly as the one in (2) for normal-form games, which in our context can be considered as the dynamic regret for a strategy \(_{i}\) against an optimal strategy. Note that an optimal strategy is also a best response strategy \(^{}\) knowing the true types of all players in hindsight maximizing (9). The optimal value function given \(\) and \(^{}\) is denoted by \(V^{,}(s)\), whose Bellman optimality equation is \(V^{,}(s)=_{a}(r+^{}V^ {,})(s,a)\).

**Definition 2** (SBG Payoff Gap).: _Given a stochastic Bayesian game \(=,,,,p,r,\), for a fixed strategy \(\), the payoff gap is defined as_

\[_{}(;)_{d(,^{}) }_{s_{0}}V^{^{},( ^{})}(s_{0})-V^{(),(^{})}(s_{0}), } {})}\] (10)

_where \(s_{0}\) denotes an initial state and \(d(,^{})_{s}\| (s;)-(s;^{})\|_{1}\)._

Similar to normal-form Bayesian games, we define the (missed) opportunity and risk below.

**Definition 3**.: _Given a payoff gap \(_{}(;)\) in (10), the (missed) opportunity of \(\) is \(()_{}(0;)\) and the risk is \(()_{>0}_{}(;)\)._Our goal is to characterize a tradeoff between the opportunity and risk for stochastic Bayesian games. Finally, we define the value of the game.

**Definition 4**.: _Given a hypothesis set \(\) and a stochastic Bayesian game \(=,,,,p,r,\), we denote the value of \(\) by_

\[_{}()_{}_{_{-i}}J (;_{-i})\ .\] (11)

### Opportunity-Risk Tradeoff for Stochastic Bayesian Games

Based on the definitions above, our first result states an upper bound on the opportunity and risk for stochastic Bayesian games. Proving the bounds in Theorem 4.1 is nontrivial because the players are coupled by a shared state. Consequently, the analysis used in Section 3 for a simple convex combination of the best response to type beliefs \(\) and a safe strategy in normal-form games cannot be directly applied to the stochastic setting.

**Theorem 4.1** (Shg Existence).: _Consider a stochastic Bayesian game \(=,,,,p,r,\) with \(_{}()\). For any \(0 1\), there exists a strategy \(\) whose (missed) opportunity and risk satisfy_

\[() (C_{1}()r_{}- )(1-),C_{1}()-3+6}{(1- )^{2}},\] \[() (C_{2}()r_{}- )(1+),C_{2}()(C_{3}(),C_{4}()),\] \[C_{3}()-3+2}{1-} { and }C_{4}()+1}{(1-)^{2}},\]

_where \(r_{}\) is defined in Assumption 1._

The proof of Theorem 4.1 is given in Appendix D, which constructs the following strategy.

Given type beliefs \(\), we denote a parameterized strategy \(()\), and let \(=()\) be the safe strategies of the opponents, with \(\) being an optimal solution of the minimax optimization in (11). Denote \( V^{,}\) and \( V^{,}\) the optimal value functions with the opponents' strategies being \(\) and \(\) respectively. Unlike the convex combination used to show Theorem 3.1 for normal-form Bayesian games, the strategy constructed for proving Theorem 4.1 is a value-based strategy below

\[(;s)*{arg\,max}_{a_{ (i)}}a^{}( R_{}(s)( s)+(1-)R_{}(s)(s)),\] (12)

where \(\) is a parameter that indicates the level of trusts on type beliefs, and for a fixed state \(s\) and a given value function \(V:\), \(R_{V}\) is an \((i)_{j i}(j)\) matrix defined as

\[R_{V}((a_{i},a_{-i})\,;s) r(s,(a_{i},a_{-i}))+ _{s^{}}p(s^{}|s,(a_{i},a_{-i})) V(s).\]

As indicated by Theorem 4.1, this design ensures safe and exploitative play, with its efficacy validated through the case studies presented in Section 5. Finally, the following lower bounds can be derived, implying that the upper bounds on \(()\) and \(()\) are tight and the value-based strategy formed in (12) are Pareto optimal up to multiplicative constants. Together, the bounds are illustrated in Figure 3.

Figure 3: Comparison of upper (Theorem 4.1) and lower (Theorem 4.2) bounds on (missed) opportunity and risk with a varying discount factor \(=0.0,0.3,0.5,0.8\), with \(=0\) and \(r_{}=1\).

**Theorem 4.2** (SBG Impossibility).: _There is a stochastic Bayesian game \(=,,,,p,r,\) whose value satisfies \(_{}() v\) such that for any \(0 1\), if any strategy \(\) misses at most \((r_{}-)(1-)\) opportunity, then it incurs at least \((r_{}-)(1+)\) risk._

In Theorem 4.2, the bounds on (missed) opportunity and risk differ from those in Theorem 4.1 by multiplicative constants. Theorem 4.2 can be derived by applying Theorem 3.2 for normal form games to a stateless MDP. We relegate the proof of Theorem 4.2 to Appendix E.

## 5 Case Studies

\(2 2\) **Game.** We provide a set of numerical evaluations to showcase the opportunity-risk tradeoff across a range of game theoretic scenarios. We begin with a focus on \(2 2\) games and to that end we sample games spanning the topology of \(2 2\) matrix games as shown in . This topology includes the comprehensive set of 78 strictly ordinal \(2 2\) games introduced by . We formulate our evaluation as a single state Stochastic Bayesian Game with the state being a specific \(2 2\) game. This evaluation is helpful in providing a concrete illustration of the tradeoff dynamics on a wide range of games within a particular topology given the comprehensive bench-marking done on the set of \(2 2\) games.

We make use of 3 broad type classes (Markovian, Leader-Follower-Trigger-Agents, and Co-evolved Neural Networks) inspired from work by . It is from these classes that we construct our types which are then used in simulations with a player leveraging a strategy that is a convex combination of a safe strategy and the best response give type beliefs or predictions. Figs 4 and 5 shows the tradeoff in one particular game as an agent moves from being fully robust as indicated by \(=0\) to fully trusting of the advice when \(=1\). In Appendix F, we provide further information on the particular types as well as illustrations on a couple of other games sampled from the topology of \(2 2\) games to showcase the range of tradeoffs which exist in this landscape.

**Security Game.** Extending our empirical study to real world settings, we also provide an evaluation of the opportunity-risk tradeoff using data from wildlife tracking studies. Security games are a clear domain wherein the tradeoff between opportunity and risk is very critical to the application. Building on a large body of work that has sought to understand the implications of game theoretic analysis in the environmental conservation domain, we formulate and explore the opportunity-risk tradeoff using data from real world wildlife movements. In particular, we formulate a green security game motivated by works such as  wherein we simulate a defender who is trying to protect an elephant population from attackers who are illegal poachers. The defender and attacker engage a multi-state Stochastic Bayesian Game where the states are constructed from historic elephant movement data sourced from . Our evaluation shows the practicality and wide ranging impact of this investigation. Figure 6

Figure 4: Comparison of average payoff for a player when varying values of \(\) and 6 potential discrete types for an instantiation of a \(2 2\) game.

shows the tradeoff for this setting evaluated for each particular type. We include implementation details on this game in Appendix F.

## 6 Concluding Remarks

In this study, we explored the fundamental limits of safe and exploitative strategies within both normal-form and stochastic Bayesian games, where pre-established type beliefs about opponents are considered. Given that these type beliefs may be inaccurate, relying on them to exploit opponents can result in high-risk strategies. Conversely, not leveraging these beliefs yields overly cautious play, leading to missed opportunities. We have quantified these dynamics by providing upper and lower bounds on the payoff gaps corresponding to different type belief inaccuracies, thereby characterizing the tradeoffs between opportunity and risk. These bounds are consistent up to multiplicative constants.

**Limitations and Future Directions.** In our current problem setting, the dynamics of a stochastic Bayesian game is assumed to be stationary, aligning with the canonical models in the related literature (for example ). To address this limitation, we plan to extend our framework to include time-varying type beliefs, addressing the absence of analysis for MDPs with dynamic transition probabilities and reward structures. Additionally, refining the opportunity and risk bounds to make them tighter would provide more precise strategic insights.

Figure 5: **Left**: \(2 2\) games considered in our case study; **Right**: Opportunity-risk tradeoff in the evaluation of a \(2 2\) game using an algorithm that has varying trust of type beliefs in 1,000 random runs. Fully trusting (\(=1\)) and distrusting (\(=0\)) type beliefs yield a best response strategy and a minimax strategy correspondingly.

Figure 6: Comparison of average payoff for the defender in a security game protecting an elephant population against illegal poachers when varying values of \(\) and 6 potential discrete attacker types.