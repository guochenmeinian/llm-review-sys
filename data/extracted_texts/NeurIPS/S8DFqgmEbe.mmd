# Learning nonparametric latent causal graphs

with unknown interventions

Yibo Jiang

University of Chicago

yiboj@uchicago.edu

&Bryon Aragam

University of Chicago

bryon@chicagobooth.edu

###### Abstract

We establish conditions under which latent causal graphs are nonparametrically identifiable and can be reconstructed from unknown interventions in the latent space. Our primary focus is the identification of the latent structure in measurement models without parametric assumptions such as linearity or Gaussianity. Moreover, we do not assume the number of hidden variables is known, and we show that at most one unknown intervention per hidden variable is needed. This extends a recent line of work on learning causal representations from observations and interventions. The proofs are constructive and introduce two new graphical concepts--_imaginary subsets_ and _isolated edges_--that may be useful in their own right. As a matter of independent interest, the proofs also involve a novel characterization of the limits of edge orientations within the equivalence class of DAGs induced by _unknown_ interventions. These are the first results to characterize the conditions under which causal representations are identifiable without making any parametric assumptions in a general setting with unknown interventions and without faithfulness.

## 1 Introduction

Among the many challenges in modern machine learning and artificial intelligence, learning and reasoning about causes and effects from data remains a key challenge. In practice, one of the hurdles that must be overcome is that we often do not have direct access to measurements on causally meaningful variables, and instead can only measure primitive, indirect measurements such as pixel intensities in vision, gene expression values in biology, letters and words in language, or frequency signals in audio. In these applications, it is necessary to first learn representations with meaningful causal signals, a problem known as causal representation learning . Besides learning representations or features of data, we are also interested in understanding what happens when we intervene on these learned features, which is essential for causal reasoning. As such, broadly speaking, causal representation learning can be broken down into two steps: (1) learning high-level features from raw data and (2) learning causal relations between these features.

Given the proliferation of recent work of identifying latent representations in the observational setting , in this paper we consider the case of interventions. Data arising from interventions opens the door for a causal interpretation of the learned representations, which is often described by a directed acyclic graph (DAG). This setting raises new challenges, namely that the interventions are both _latent_ and _unknown_. Moreover, in practical applications, flexible deep neural networks are used to learn nonlinear representations of the data, which necessitates the consideration of nonparametric assumptions. Motivated by these challenges, we seek to answer the following important question:

_Given a list of latent, unknown interventions, when is it possible to identify the underlying (latent) causal relationships without making parametric assumptions?_In particular, we focus on the problem of learning structure [47; 56], and leave the problem of learning the representations themselves to existing work, since one can then use deep latent variable models to infer the latent distributions from the latent structure, which is well-studied [e.g. 20; 28; 35; 46; 64; 65]. We adopt the measurement model [36; 43; 54] where there are no direct causal edges between observed covariates. This model has been used extensively to model causal representations [e.g. 1; 25; 32; 57; 61; 66; 67; 68]. Surprisingly, we show that it is possible to learn latent causal structure without learning the distributions of latent representations, unlike [1; 32; 57; 61]. Furthermore, unlike [1; 57; 61], we do not impose any parametric assumptions: We allow for general noisy, nonlinear transformations between latents and observed as well as arbitrary nonlinear relationships between the latents, which makes the problem significantly harder. Part of our motivation is to better understand the _minimal_ assumptions for learning causal structure from unknown interventions.

### Contributions

Our main contribution is a set of nonparametric assumptions under which the entire latent causal graph is identifiable given unknown interventions in the latent space, up to edges that we show cannot be oriented without additional assumptions (in a similar sense to reversible edges in a Markov equivalence class). To the best of our knowledge, structure identification in a nonparametric setting given _unknown_ and _latent_ interventions has not been considered in-depth in the literature.

More specifically, we make the following contributions:

1. We introduce two new graphical concepts--_imaginary subsets_ (Definition 3.2) and _isolated edges_ (Definition 3.3)--that are key to identifiability and orientability of edges in the true causal graph. These are illustrated in Figure 1 and discussed in detail in Sections 4-5.
2. We combine these concepts with nonparametric, graphical assumptions to show that the causal graph is identifiable up to isolated edges (Theorem 3.4).
3. We show the limitation of edge orientations using CI relations alone under unknown interventions even when there are no latent variables (Theorem 5.3).

The implications of these results are twofold: 1) It _is_ possible to learn the entire DAG without making parametric assumptions, albeit at the cost of nontrivial graphical assumptions, and 2) If we wish to relax these graphical conditions, alternative assumptions are needed. That is, in Appendix C (Examples 3-6), we prove that our assumptions are nearly necessary in the sense that if any individual assumption is relaxed, then identifiability fails. Figure 1 illustrates our graphical conditions in a simple example that will be referred back to throughout the paper. Finally, we verify our theoretical results in a simulation study.

### Related work

Learning the Markov equivalence class of causal graphs from observational distributions is a well-researched area . Although our focus is on learning measurement models with interventions, we note that the observational case has been extensively studied [25; 32; 36; 43; 54; 66; 67; 68].

Figure 1: Illustration of the main concepts used in this paper. \(G=G_{B} G_{H}\) is a measurement model with bipartite DAG \(G_{B}\) (blue edges) and latent DAG \(G_{H}\) (red edges), \(\{X_{5},X_{6}\}\) is an imaginary subset, \(\{X_{1},X_{2}\}\) is a replaceable subset while \(\{X_{1},X_{2},X_{5}\}\) is a non-replaceable subset, and \(H_{2} H_{4}\) is an isolated edge. In fact, \(\{X_{5},X_{6}\}\) is a non-replaceable imaginary subset. This is also not a maximal measurement model although it still illustrates the main concepts. For completeness, the undirected dependency graphs under different interventional targets are provided in Figure 2.

Intervention design.In the classical setting of known interventions on observables, Eberhardt et al.  show that for causal graphs with more than \(n>2\) variables, \(n-1\) single node interventions are sufficient, and in the worst case, they are also necessary. Kocaoglu et al.  consider intervention design in the presence of latents, and propose efficient algorithms. Hauser and Buhlmann  derive a notion of interventional equivalence for hard/structural interventions, while Tian and Pearl  and Yang et al.  do the same for soft/parametric interventions.

Unknown interventions.There has been growing literature on learning under unknown interventions recently as well. A recent line of work studies soft interventions where the unknown interventional targets are observed [7; 15; 19; 21; 27; 33; 49; 58]. In particular, Jaber et al.  consider the case where the causal graph consists of measured and unmeasured latent variables, but the intervention targets, though unknown, are from measured variables. Squires et al.  assume no hidden variables and use direct \(\)-faithfulness to identify the unknown intervention targets. Perry et al.  utilize independent causal mechanisms as a key assumption and measure the number of mechanism changes to identify the true DAG. Castelletti and Peluso , Eaton and Murphy , and Faria et al.  propose Bayesian methods to learn DAGs under unknown interventions.

Latent interventions.A very different setting arises when the interventions are both unknown _and_ latent. Perhaps the earliest approach to this general setting is _causal feature learning_, introduced in [8; 9]. More closely related to our paper are Liu et al. , Squires et al. , and Varici et al. , which consider unknown interventions on latent variables under parametric assumptions. Squires et al.  assume hard interventions on latent variables under a linear model. Varici et al.  allow nonlinearities in the latent space with a linear map between hidden and observed, using the score function for identification. Liu et al.  assume the existence of an auxiliary observed variable \(u\) that modulates the variant weights among latent causal variables where the setup is similar to that of iVAE . Ahuja et al.  assume a polynomial decoder and the intervention target on the latent is known. Brehmer et al.  consider the weakly supervised setting with paired pre- and post-intervention samples, where the interventions are random and unknown. Lippe et al. , on the other hand, work with latent interventions on temporal sequences.

While our work was under review, we were made aware of several concurrent works that study the same problem under different assumptions [5; 37; 72].

## 2 Preliminaries

Our basic setup is a standard graphical model with both observed and latent variables. Appendix A contains a comprehensive overview of all the necessary formal graphical concepts; we briefly outline the basics here. Let \(G=(V,E)\) be a directed acyclic graph (DAG) with \(V=(X,H)\) where \(X\) denotes the observed part and \(H\) denotes the hidden or latent part. Define \(n:=|X|\) and \(m:=|H|\). For a given node \(v\), we use standard notation such as \((v)\), \((v)\), \((v)\) and \((v)\) for parents, children, ancestors, and descendants respectively. Given a subset \(V^{} V\), \((V^{})_{v V^{}}(v)\), given two subsets \(A,B V\), \(_{B}(A)=(A) B\) and given a subgraph \(G^{} G\), \(_{G^{}}(V^{})(V^{}) G^{}\). Similar notation can be defined for children, ancestors, and descendants. For disjoint subsets \(A,B,C V\), define \(_{G}(AB\,|\,C)\) to mean that \(A,B\) are \(d\)-separated by \(C\) in the graph \(G\). A DAG encodes a set of \(d\)-separation relations, defined by

\[_{V^{}}(G)=\{ A,B,C:_{G}(AB\,|\,C)A,B,C V^{}\}\]

where \(V^{} V\). For simplicity, we write \((G)=_{V}(G)\). Recall that two DAGs \(G_{1}\) and \(G_{2}\) are Markov equivalent if \((G_{1})=(G_{2})\). The Markov equivalence relations define a set of equivalence classes of DAGs called the Markov equivalence class (MEC).

Every distribution \(P\) on \(V\) defines a collection of conditional independence (CI) relations:

\[_{V^{}}(P)=\{ A,B,C:A\!\!\! B\,|\,C PA,B,C V^{}\}\]

where \(V^{} V\), and as before \((P)=_{V}(P)\). We denote the marginal of \(V^{}\) by \(P_{V^{}}\).

_Remark 2.1_.: Throughout the paper, _unconditional_ (i.e. marginal) independence and \(d\)-separation will play a prominent role. Thus, unless otherwise stated, when we say \(A\) and \(B\) are dependent or \(d\)-connected, or that there is an active path between \(A\) and \(B\), without explicit mention of the separating set \(C\), we consider it implies that \(C=\).

Interventions.For background, Eberhardt and Scheines  provides a detailed account of the different types of interventions in causal systems. In this paper, we consider _hard_ (or _structural_) interventions on a _single node_. Let \(I V\) be a set of intervention targets. The **intervention graph** of \(G\) is the DAG \(G^{(I)}=(V,E^{(I)})\), where \(E^{I}\{(a,b)|(a,b) E,b I\}\). Similarly, let \(P^{(I)}\) be the interventional distribution. We further restrict intervention targets to be latent variables in this paper (\(I H\)), which necessitates considering _unknown_ intervention targets. Thus, we have access to a family of intervention targets \(=\{I_{0},...,I_{k}\}\) with \(|I_{j}| 1\). By convention, we let \(I_{0}=\) so that \(G^{(I_{0})}=G\) and \(P^{(I_{0})}=P\) is the observational distribution.

A subtle but important point is that different interventional distributions may be the same, i.e. \(P^{(I)}_{X}=P^{(I^{})}_{X}\) but \(I I^{}\). This implies, in particular, that the number of latent variables is unknown (see Remark C.1).

**Example 1**.: Known and unknown interventions have different implications in terms of edge orientations. To see this, consider the unoriented edge between two variables \(X_{1}\) and \(X_{2}\). If we know the intervention target is \(\{X_{1}\}\) and we observe that \(X_{1}\) and \(X_{2}\) become independent under this intervention, then we know \(X_{1} X_{2}\). However, if we do not know the intervention target is \(X_{1}\) and we observe the same independence between \(X_{1}\) and \(X_{2}\), then it is possible the true DAG is \(X_{1} X_{2}\) with intervention target \(X_{1}\) or the true DAG is \(X_{1} X_{2}\) and the intervention target \(X_{2}\).

Measurement models.Our main results apply to so-called _measurement models_ in which every observed variable only has incoming edges and no outgoing edges (i.e. \((X)=\)). This assumption cleanly encapsulates the problem of reconstructing latent causal structure and captures relevant applications where the relationships between raw observations are less relevant than causal features and is a standard model adopted in prior work [e.g. 25, 66, 67, 68, 43].

For any measurement model, \(G\) decomposes as the union of two subgraphs \(G=G_{B} G_{H}\) where \(G_{B}\) is a directed, bipartite graph pointing from \(H\) to \(X\), and \(G_{H}\) is a DAG over the latent variables \(H\). See Figure 1.

Following Markham and Grosse-Wentrup , for any distribution \(P\) over \(V\), define the undirected dependency graph (UDG), denoted \(D(P)\), to be the undirected graph over \(X\) in which there is an edge between \(X_{i}\) and \(X_{j}\) if and only if they are marginally dependent (i.e. given the empty set, cf. Remark 2.1). Clearly, \(D(P)\) is easily constructed from \(_{X}(P)\) by checking if each pair of observed variables is marginally independent or not.

Figure 2: UDG under different intervention targets for Figure 1.

Main result

We can now state our goal formally as follows:

_Given a set of interventional distributions \(\{P_{X}^{(I)}\}_{I}\), can we recover \(G_{B}\) and \(G_{H}\)?_

Here, \(\{P_{X}^{(I)}\}_{I}\) is a _set_ and not a tuple: If two different interventions lead to the same interventional distributions, then we only observe one copy of \(P_{X}^{(I)}\) in this set. As a result, we do not know the number of latent variables (see Remark C.1); we show how this is learned in the proof. Of course, in practice, we only have sample access to \(P_{X}^{(I)}\). In this paper, we focus on identifiability and leave estimation from finite samples to future work.

_Remark 3.1_.: Learning from the tuple \((P_{X}^{(I)})_{I}\) is easier than learning from the set \(\{P_{X}^{(I)}\}_{I}\), since one can trivially reduce the tuple problem to a set problem by removing duplicates. Thus, there is no loss of generality in our setting.

### Assumptions

To solve this problem, we make the following assumptions:

**Assumption 1** (Graphical conditions).: The DAG \(G\) satisfies the following conditions for every \(I\) and pair of hidden variables \(H_{i} H_{j}\):

* \(P^{(I)}\) is Markov with respect to \(G^{(I)}\), i.e. \((G^{(I)})(P^{(I)})\).
* \(X_{i}_{P^{(I)}}X_{j}_{G^{(I)}}\{X_{j} \}\,|\,\), i.e. marginal independence in \(X\) implies \(d\)-separation.
* \(_{G^{(I)}}\{\{H_{i}\}\{H_{j}\}\,|\,X_{i}_{X}(H_{i})X_{j}_{X}(H_{j}) _{G^{(I)}}\{\{X_{i}\}\{X_{j}\}\,|\,.\]

**Assumption 2** (Complete family of targets).: \(=\{,\{H_{1}\},,\{H_{m}\}\}\).

Intuitively, the graphical conditions in Assumption 1 ensure that hidden variables and their dependencies have detectable signatures in observed distributions. Of course, Assumption 1(a) is just the usual Markov assumption that relates the graph \(G\) to the distribution \(P\). Assumption 1(b) requires that marginal dependencies of observed variables are reflected in the underlying graph, and is much weaker than similar conditions in the graphical modeling literature. Assumption 2 ensures that the effect of each hidden variable is measured. Furthermore, with the exception of Assumption 1(c), which arises from our fully nonparametric setup, each of these assumptions has appeared previously in the literature . See also Remark 4.3. A detailed discussion of these assumptions is deferred to Appendix C. In particular, with the exception of the Markov property Assumption 1(a), we give counterexamples to show that when any _one_ of these assumptions is violated (but the rest continue to hold), there are two graphs that have the same set of observed distributions under different interventions.

_Remark 3.2_.: It is worth noting that the well-known subset condition  is implied by Assumption 1 and Assumption 2 (Lemma C.1 in Appendix C.4). However, this arises from the fact that Assumption 2 is needed for _exact_ recovery. If the main objective is instead partial recovery and Assumption 2 is relaxed, then one needs to additionally assume the subset condition. See Appendix C.4 for details.

_Remark 3.3_.: Assumption 1 and Assumption 2 also generalize the well-known _pure child_ condition, which is widely used and has applications in NLP and topic modeling [e.g. 3, 6, 45, 66]; see also Section 4.3. It is easy to see that the existence of pure children for each latent implies Assumption 1(c).

### Maximal measurement model

Under Assumptions 1-2, two different measurement models can still induce exactly the same interventional distributions of observed variables. Even without interventions, there may be ambiguities, an observation that dates back to  in the definition of a maximal ancestral graph and was more recently used in . Example 6 in Appendix C.5 gives a concrete example where two measurement models share the same set of observed distributions under interventions. Fortunately, the ambiguity is limited: There is always a _maximal_ measurement model that encodes as many non-redundant dependencies as possible, as defined below:1

**Definition 3.1** (Maximal measurement model).: A measurement model \(G\) is called **maximal** if it satisfies Assumption 1 and the following two conditions:

1. \((X_{i})\) for all \(i[n]\),
2. There is no DAG \(G^{}=(V,E^{})\) also satisfying Assumption 1 such that, \(\{_{X}(G^{(I)})\}_{I}=\{_{X}(G^{(I) })\}_{I}\), and \(E E^{}\).

Our definition of maximality here mirrors the concept of maximality introduced in , extended to include interventions. Throughout the paper, if not mentioned explicitly, the measurement model is considered to be maximal. More discussion on maximality is provided in Appendix C.5.

### Imaginary subsets and isolated edges

To identify \(G\), we will break the problem into two phases: First, we learn the bipartite graph \(G_{B}\), and then we use this to learn the latent DAG \(G_{H}\). Each of these phases introduces a new graphical concept, which are defined here.

#### 3.3.1 Imaginary subsets

Latent variables induce cliques in the UDG \(D(P_{X}^{(I)})\), which provide a way to identify the existence of a latent . Unfortunately, the identification of \(G_{B}\) is complicated by _imaginary subsets_: Intuitively, imaginary subsets are ambiguous subsets of observed variables that may not be the children of a single latent. Given \(D(P_{X}^{(I)})\), let \(_{P}^{(I)}\) be the set of maximal cliques in \(D(P_{X}^{(I)})\). We also let \(=_{I}_{P}^{(I)}\).

**Definition 3.2** (Imaginary subsets).: A subset \(X^{} X\) is a **valid subset** if for all \(I\), there exists a maximal clique \(C_{P}^{(I)}\) such that \(X^{} C\). A valid subset is **maximal** if there is no other valid subset \(X^{}\) such that \(X^{} X^{} C\) for every maximal clique \(C\) containing \(X^{}\). A maximal valid subset \(X^{} X\) is **imaginary** if there is no hidden variable \(H_{i}\) such that \(X^{}_{X}(H_{i})\).

Imaginary subsets (more precisely, the lack thereof) are crucial to the identification of \(G_{B}\), although at first glance they may seem a bit abstract. Indeed, handling imaginary subsets turns out to be a nontrivial issue, so we defer further discussion of this concept to Section 4, where several examples and intuitions are given.

#### 3.3.2 Isolated edges

Once we identify \(G_{B}\), we'd like to identify \(G_{H}\). Unfortunately, Example 1 gave an example where unknown interventions are incapable of orienting an edge using CI information only. This type of edge can be identified more broadly as follows:

**Definition 3.3** (Isolated edge).: We say an edge \(x y\) is **isolated** if \(x\) does not have any parent (\((x)=\)) and \(y\) only has \(x\) as its parent (\((y)=\{x\}\)).

The importance of isolated edges is that these are precisely the edges that cannot be oriented using CI information only (see Section 5.2).

### Identifiability of causal graph

We can now state the main result of this paper:

**Theorem 3.4**.: _Let \(G\) be a maximal measurement model satisfying Assumption 1. Then, given a complete family of interventions (Assumption 2), the following statements are true:_

1. _If there are no imaginary subsets (Definition_ 3.2_), then_ \(G\) _is identifiable from_ \(\{P^{(I)}\}_{I}\) _up to isolated edges (Definition_ 3.3_) in_ \(G_{H}\)2. Isolated edges in \(G_{H}\) cannot be oriented using CI information only.

In particular, the unknown number of latents \(m\) is also identified.

In other words, \(G\) can be maximally identified in the sense that any edge in the latent space that isn't oriented cannot be oriented from the given list of interventions using CI information only: Additional assumptions are needed (e.g. conditional invariances and direct \(\)-faithfulness as in ).

We devote a significant effort in the sequel to interpreting and understanding the assumption of no imaginary subsets, which turns out to be subtle and complicated. Thus, in Section 4, we provide several additional sufficient conditions as well as examples of imaginary subsets to help build intuition for this condition.

The proof of this result is broken down into two main steps:

1. Identifying the bipartite graph \(G_{B}\) (See Section 4);
2. Identifying the skeleton of the latent DAG \(G_{H}\) and orienting the edges in \(G_{H}\) (See Section 5);

Sections 4-5 outline the basic ideas behind these constructions. As the ideas are independently interesting and may be more broadly useful beyond just proving Theorem 3.4, we treat these independently.

## 4 Imaginary subsets and the bipartite graph

Theorem 3.4 indicates that as long as there are no imaginary subsets, \(G_{B}\) can be identified. In this section, we show how identifiability of \(G_{B}\) is related to the absence of imaginary subsets (Definition 3.2), and provide several different conditions for this to hold. Throughout this section, we assume as in Theorem 3.4 that \(G=G_{B} G_{H}\) is a maximal measurement model satisfying Assumption 1, and that Assumption 2 also holds.

### Identifying \(G_{b}\) with no imaginary subsets

The following proposition explains why maximal valid subsets (Definition 3.2) are useful:

**Proposition 4.1**.: _For any hidden variable \(H_{i}\), \(_{X}(H_{i})\) is a maximal valid subset._

Proposition 4.1 suggests that we assign a latent variable to each maximal valid subset. However, not every maximal valid subset corresponds to a single latent variable: For example, in Figure 1, \(\{X_{5},X_{6}\}\) is a maximal valid subset that does not correspond to any hidden variable, and hence is an imaginary subset. Unfortunately, these examples are not pathological; this turns out to be endemic and must be resolved carefully.

The first issue is that a maximal valid subset can be contained in another maximal valid subset. An example is \(\{X_{1},X_{2}\}\{X_{1},X_{2},X_{5}\}\) in Figure 1 (see also Example 7 in Appendix D). This typically happens when two such subsets appear in different cliques, which does not violate our definition of maximal valid subsets:

**Definition 4.2** (Replaceable subset).: A maximal valid subset \(X^{} X\) is **replaceable** if there exists another maximal valid subset \(X^{}\) such that \(X^{} X^{}\).

An example of a replaceable subset is in Figure 1. The advantage of replaceable subsets is that they can be identified and ignored. In particular, any replaceable imaginary subset is a non-issue. Thus, we have the following important result, which is proved in Appendix D.1:

**Theorem 4.3**.: _If there are no non-replaceable imaginary subsets, then a subset \(X^{}\) is a non-replaceable maximal valid subset if and only if there exists a hidden variable \(H_{i}\) such that \(_{X}(H_{i})=X^{}\). In particular, it follows that \(G_{B}\) is identifiable._

Since one can check if \(X^{}\) is replaceable, one might hope that simply eliminating replaceable subsets fixes the problem. Unfortunately, this is not enough: The devil is _non-replaceable imaginary subsets_; i.e. imaginary subsets that cannot be identified from the data. Moreover, non-replaceable imaginary subsets are a genuine phenomenon (\(\{X_{5},X_{6}\}\) in Figure 1). Thus, as stated, Theorem 4.3 has two drawbacks:1. Checking if non-replaceable imaginary subsets exist and getting rid of them is not easy; and
2. Even when there are imaginary subsets, \(G_{B}\) may still be identifiable.

Given the difficulties with non-replaceable imaginary subsets, we will provide two sufficient conditions to guarantee there are no imaginary subsets (Section 4.2) and also show how one can identify \(G_{B}\) even when there are imaginary subsets (Section 4.3).

### Sufficient conditions for no imaginary subsets

In this section, we provide two additional sufficient conditions to guarantee there are no imaginary subsets: The first--single source node--is intuitive and interpretable, but cannot always be checked. The second--no fractured subsets--is less intuitive but can be explicitly checked.

Single latent source.A latent source is any latent variable such that \((H_{i})=\). We can show that imaginary subsets do not exist if there is only one latent source. This still allows for arbitrarily many hidden variables \(m=|H|>1\) (i.e. the descendants of the latent source in \(G_{H}\)).

**Theorem 4.4**.: _Under Assumptions 1-2, if \(G_{H}\) has one latent source, there are no imaginary subsets._

Therefore, with only one latent source node, we can recover the bipartite graph by Theorem 4.3.

Fractured subset condition.A necessary condition for an imaginary subset is that the subset is _fractured_. The definition is somewhat complicated, but worth it since fractured subsets can be identified and checked in practice:

**Definition 4.5** (Fractured subset).: Given a collection of maximal valid subsets \(\), a clique \(C\) is called **shattered** by \(\) if there exists a subset \(^{}\) such that \(^{}=C\). A collection of maximal valid subsets \(\{S_{i}\}_{i=1}^{k}\) is **complete** if for every intervention target \(I\), all shattered cliques in \(D(P^{(I)})\) form an edge cover of \(D(P^{I})\). A maximal valid subset \(X^{} X\) is **fractured** if there exists a complete collection \(\{S_{i}\}_{i=1}^{k}\) such that \(S_{i} X^{}\) for all \(S_{i}\).

Intuitively, a fractured subset provides redundant information as every connection between nodes in the fractured subset can be explained by other maximal valid subsets. The aforementioned necessity is shown by the following lemma:

**Lemma 4.6**.: _If a subset \(X^{} X\) is imaginary, then \(X^{}\) is also fractured._

Therefore, we have the following identifiability corollary:

**Corollary 4.7**.: _Under Assumptions 1-2 and if there are no fractured subsets, then \(G_{B}\) is identifiable. Furthermore, the absence of fractured subsets can be checked and verified, and if it fails, a certificate is provided._

_Remark 4.1_.: Technically, we only need the condition that there are no _non-replaceable_ fractured subsets.

_Remark 4.2_.: One might suggest getting rid of all fractured subsets, however, even a non-imaginary subset can be a fractured subset (Example 9 in Appendix D). The no fractured subset condition naively captures such ambiguities of imaginary subsets, but is overkill. In fact, it is still possible to have identifiability with fractured subsets under different assumptions (Theorem 4.4, Theorem 4.8).

### Identifying \(G_{b}\) when there are imaginary subsets

Finally, we show that under the well-known pure child assumption, \(G_{B}\) can be identified even when there are imaginary subsets. The pure child assumption has been made in many existing works [e.g. 3, 6, 45, 66], typically along with additional parametric assumptions.

**Assumption 3** (Pure child).: For every \(H_{i} H,\) there exists at least one \(X_{i} X\) with \((X_{i})=\{H_{i}\}\), i.e. \(X_{i}\) only has one parent and that parent is \(H_{i}\).

_Remark 4.3_.: Assumption 1(c) is a much weaker assumption than Assumption 3. In particular, if a measurement model satisfies Assumption 3, it also satisfies Assumption 1(c).

**Theorem 4.8**.: _Under Assumptions 1-3, the complete collection (cf. Definition 4.5) with the smallest cardinality is exactly \(\{_{X}(H_{i})\}_{i=1}^{m}\) and thus \(G_{B}\) can be identified._

_Remark 4.4_.: Under Assumption 3, there still could be imaginary subsets (Example 11 in Appendix D).

Identifying the latent DAG

Once we have learned the bipartite graph \(G_{B}\), the next step is to learn the DAG \(G_{H}\) over the latent variables \(H\). Learning the skeleton of \(G_{H}\) turns out to be straightforward: Assumption 1(b-c) suggest that two hidden variables \(H_{i}\) and \(H_{j}\) are \(d\)-separated if and only if \(_{X}(H_{i})\) and \(_{X}(H_{j})\) are in different cliques (Lemma E.2). Therefore, the idea is to use unconditional \(d\)-separations of latent variables under interventions for identification which is harder than having access to all conditional \(d\)-separations in the fully observational case (see Appendix E for details).

_Remark 5.1_.: In fact, we do not have access to full conditional \(d\)-separation statements of latents because observed variables are descendants of latent nodes.

The more interesting question is how to orient the edges with _unknown_ interventions. Unlike known interventions, edge orientation might not always be possible even when there are no latent variables as shown in Example 1. At the same time, it is sometimes possible as demonstrated by Example 13 in Appendix F. This raises the question of which edges provably _cannot_ be oriented under our assumptions: It turns out these are precisely the _isolated_ edges (Definition 3.3).

**Theorem 5.1**.: _Let \(G\) be a maximal measurement model satisfying Assumption 1 and assume we are given a complete family of interventions (Assumption 2) as well as the bipartite DAG \(G_{B}\). Then the true latent DAG \(G_{H}\) is identifiable up to isolated edges. Moreover, isolated edges cannot be oriented without making additional assumptions._

Thus, as long as we identify \(G_{B}\) (cf. Section 4), we can identify \(G_{H}\) up to isolated edges.

_Remark 5.2_.: The proof of Theorem 5.1 in Appendix G provides a constructive algorithm. Pseudocode for the overall approach can be found in Algorithm 2 in Appendix G.

In fact, the non-orientability of isolated edges is not restricted to latent edges or even the measurement model; this fact applies to general, fully (or partially) observed DAGs.

### Isolated equivalence

We now introduce an equivalence relation on DAGs that refines the notion of Markov equivalence to account for the extra information conveyed by unknown interventions. Unlike known interventions, which suffice to identify the entire DAG, Example 1 shows that unknown interventions carry strictly less information vs. known interventions. _These definitions are purely graphical_ and can be studied in their own right.

For this result, we do not need the measurement model assumptions nor Assumption 1-2. So, for now, consider the case of an arbitrary, fully observed DAG (i.e. \(H=\)). By the transformational properties of MEC , we know that two Markov equivalent DAGs can be transformed into one another by a sequence of covered edge reversals (Definition B.3). Similarly, let's define the following:

**Definition 5.2**.: (Isolated equivalence class) Two DAGs \(G_{1}\) and \(G_{2}\) are isolated equivalent, denoted \(G_{1}_{E}G_{2}\), if there exists a sequence of isolated edge reversals to transform one into another.

An example of two isolated-equivalent DAGs can be obtained from Figure 1: Since the latent edge \(H_{2} H_{4}\) is isolated, reversing it yields a DAG that is isolated equivalent to \(G\).

_Remark 5.3_.: Despite what the name might suggest, an isolated edge \(X Y\) does not mean that \(X\) and \(Y\) are disconnected from all other nodes. In fact, \(X\) and \(Y\) can still have outgoing edges (Definition 3.3) and \(X Y\) is not just an isolated connected component. Therefore, the IEC is not just a union of disjoint edges.

_Remark 5.4_.: Since an isolated edge is covered by definition, \(G_{1}\) and \(G_{2}\) are Markov equivalent if they are isolated equivalent. It is easy to check that isolated equivalence is an equivalence relation. _Therefore, the isolated equivalence class (IEC) is a finer partition of the Markov equivalence class (MEC)_. An IEC can and often will be a singleton (i.e. a DAG that is not isolated equivalent to any other DAG in the MEC).

_Remark 5.5_.: This is also different from the interventional Markov equivalence class where the intervention target is known .

### (Non-)Orientability of isolated edges

The value of isolated equivalence is that it identifies which DAGs cannot be distinguished (from CI information alone) using unknown interventions. This is formalized in the following theorem:

**Theorem 5.3**.: _Suppose \(G_{1}\) and \(G_{2}\) are in the same IEC. If the tuple \((G_{1},_{1})\) induces \(\{(G_{1}^{(I)})\}_{I_{1}}\), then there exists a family of interventional targets \(_{2}\) (possibly the same as \(_{1}\)) such that the tuple \((G_{2},_{2})\) also induces \(\{(G_{1}^{(I)})\}_{I_{1}}\)._

Theorem 5.3 shows that it is impossible to distinguish DAGs in the same IEC by looking at \(d\)-separations only. While it is impossible to distinguish within an IEC, it is possible to do so between different IECs (see Theorem F.7 in the appendix). Together, Theorems 5.3 and F.7 establish that isolated edges are precisely those edges that cannot be oriented by unknown interventions under our assumptions. This does not imply, of course, that this is impossible in practice: We simply need to impose additional assumptions. For example, one could use conditional invariances to improve the identifiability of edge orientations under additional assumptions like direct \(\)-faithfulness , which we have not assumed in this paper.

## 6 Experiments

We test the theoretical results on simulated datasets under two settings: pure child and single latent source. Because this paper is primarily theoretical, the purpose of experiments is simply to verify the theory. In particular, for single latent source experiments, we still adopt the pure child structure but we explicitly test our identification strategy - no imaginary subset (Theorem 4.4). We generate random causal graphs under different settings of \(m\), \(n\). _We do not enforce Assumption 1 (b) nor maximality (Definition 3.1)_. For each variable \(V_{i}\) in the causal graph, the structural equation is simply \(V_{i}_{V_{j}(V_{i})}f(V_{j})+\), where \(\) is Gaussian noise, and \(f\) is a nonlinear function. We set \(f\) to be a quadratic function. To test independence, we adopt Chatterjee's coefficient . The metric we use is the Structural Hamming Distance (SHD) between the estimated DAG and the true DAG. The results show that even without the graphical assumptions, our method can be effective in recovering the DAG for nonlinear models.

## 7 Conclusion

Using unknown, latent interventions, we have provided nonparametric identifiability results for a class of graphical measurement models that are commonly used to learn causal representations. For this, we introduced two important graphical concepts: _Imaginary subsets_ and _isolated edges_. Our proofs are constructive and can be implemented on finite samples. Obvious relaxations of interest include finding better sufficient conditions for identifying bipartite graphs and extensions to multi-node and/or soft interventions.

## 8 Acknowledgments

The authors would like to acknowledge the support of the NSF via IIS-1956330, the NIH via R01GM140467, and the Robert H. Topel Faculty Research Fund at the University of Chicago Booth School of Business. This work was done in part while B.A. was visiting the Simons Institute for the Theory of Computing.

  (m,n) & (2, 5) & (3, 8) & (4, 7) & (4, 8) \\  Pure Child & 0.01\(\)0.01 & 0.54\(\)0.13 & 1.35\(\)0.17 & 2.35\(\)0.30 \\ Single Source & 0.02\(\)0.02 & 0.92\(\)0.18 & 1.52\(\)0.21 & 2.81\(\)0.30 \\  

Table 1: Experiments on simulated data show the effectiveness of our identification theory. The table shows SHD and standard errors are over \(100\) runs.