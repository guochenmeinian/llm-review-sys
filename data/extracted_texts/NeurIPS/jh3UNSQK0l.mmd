# Finite-Time Analysis of Single-Timescale Actor-Critic

Xuyang Chen

National University of Singapore

chenxuyang@u.nus.edu &Lin Zhao

National University of Singapore

elezhli@nus.edu.sg

Corresponding author

###### Abstract

Actor-critic methods have achieved significant success in many challenging applications. However, its finite-time convergence is still poorly understood in the most practical single-timescale form. Existing works on analyzing single-timescale actor-critic have been limited to i.i.d. sampling or tabular setting for simplicity. We investigate the more practical online single-timescale actor-critic algorithm on continuous state space, where the critic assumes linear function approximation and updates with a single Markovian sample per actor step. Previous analysis has been unable to establish the convergence for such a challenging scenario. We demonstrate that the online single-timescale actor-critic method provably finds an \(\)-approximate stationary point with \(}(^{-2})\) sample complexity under standard assumptions, which can be further improved to \((^{-2})\) under the i.i.d. sampling. Our novel framework systematically evaluates and controls the error propagation between the actor and critic. It offers a promising approach for analyzing other single-timescale reinforcement learning algorithms as well.

## 1 Introduction

Actor-critic (AC) methods have achieved great success in solving many challenging reinforcement learning (RL) problems . AC updates the actor (i.e., the policy) using the estimated policy gradient (PG), which is a function of the Q-value under the policy. Meanwhile, it employs a bootstrapping critic to estimate the Q-value, which often helps reduce variance and accelerates convergence in practice.

Despite the empirical success, the non-asymptotic convergence analysis of AC in the most practical single-timescale form remains underexplored. A large body of existing works consider the double-loop variants, where the critic runs many steps to accurately estimate the Q-value for a given actor . This leads to a decoupled convergence analysis of the critic and the actor, which involves a policy evaluation sub-problem in the inner loop and a perturbed gradient descent in the outer loop. Its finite-time convergence is relatively well understood . Nevertheless, the double-loop setting is mainly for ease of analysis, which is barely adopted in practice. Since it requires an accurate critic estimation, it is typically sample inefficient. In fact, it is unclear whether an inner loop of accurate policy evaluation is really necessary since it only corresponds to one transient policy among many iterations.

Another body of works considers the (single-loop) two-timescale variants , where the actor and the critic are updated simultaneously in each iteration with stepsizes of different timescales. The actor stepsize is typically smaller than that of the critic, with their ratio converging to zero as the iteration number goes to infinity. Hence, the actor is updated much slower than the critic. The two-timescale allows the critic to approximate the desired Q-value in an asymptotic way, which enables a decoupled convergence analysis of the actor and the critic. This variant is occasionallyadopted to improve learning stability. However, it is still considered inefficient as the actor update is artificially slowed down.

In this paper, we consider the more practical single-timescale AC algorithm, which is the one introduced in many works of literature as well as in  as a classic AC algorithm. In single-timescale AC, the stepsizes of the critic and the actor diminish at the same timescale. Unlike the aforementioned variants, which have specialized designs aimed at simplifying the convergence analysis, the analysis of the single-timescale AC presents a greater challenge. Due to the substantial errors in critic estimation and the close coupling between the parallel critic update and actor update, the algorithm is more prone to unstable error propagation. It remains unclear under what condition the errors will converge to zero. To study its finite-time convergence, we consider the challenging undiscounted time-average reward formulation [25; 31; 37], which consists of three parallel updates: the (time-average) reward estimator, the critic estimator, and the actor estimator. We keep track of the reward estimation error, the critic error, and the policy gradient norm (which measures the actor error) by deriving an implicit bound for each of them. They are then analyzed altogether as an interconnected system inspired by  to establish the convergence simultaneously. Specifically, we identify the (constant) ratio between the actor stepsize and the critic stepsize, below which all three errors will diminish to zero, despite the inaccurate estimation in all three updates (reward estimation, critic, actor).

### Main Contributions

We summarise our main contributions as follows:

\(\) We provide a finite-time analysis for the single-timescale AC under the Markovian sampling and prove an \(}(^{-2})\) sample complexity, where \(}()\) hides additional logarithmic terms. We further show that this sample complexity can be improved to \((^{-2})\) under i.i.d. sampling, which matches the state-of-the-art performance of SGD on general non-convex optimization problems. Our proof clearly shows that the additional logarithmic term under the Markovian sampling is introduced by the mixing time of the underlying Markov chain.

\(\) Our result compares favorably to existing works on single-timescale AC. To our knowledge, the only other results of single-timescale AC in the general MDP (Markov decision process) case are from  and , both of which obtain a sample complexity of \(O(^{-2})\) under discounted reward setting. However, both  and  considered the i.i.d. sampling, where the transition tuples are independently sampled from stationary distribution and discounted state-action visitation distribution. In this paper, we consider the more practical Markovian sampling, where the transition tuples are generated from a single trajectory (see Table 1).

Furthermore,  follows an explicit Lyapunov analysis, where they leave a biased term in the critic and eliminated in the actor. Therefore, their proof framework cannot show the convergence of the critic. We instead give a neat proof framework to guarantee convergence for both the critic and the actor. Additionally,  only considered the tabular case (finite state-action space) where we allow the state space to be infinite (see Table 1). It is worth emphasizing that moving from a finite state space to an infinite state space takes significantly non-trivial effort in analysis. The analysis in  concatenates all state-action pairs to create a finite-dimensional feature matrix, which however becomes impossible in the infinite state space scenario. Consequently, their analysis technique and established results are not applicable in our context.

\(\) Technically, we develop a new analysis framework that can establish the finite-time convergence for single-timescale AC under the general setting. The existing analysis for double-loop AC  and two-timescale AC  hinge on decoupling the analysis of actor and critic, which typically establishes the convergence of critic first and then actor [37; 31; 8]. We instead investigate the evolution of

   &  &  &  \\    & State Space & Reward & Actor & &  \\ 
 & Finite & Discounted & i.i.d. & i.i.d. & \((^{-2})\) \\ 
 & Infinite & Discounted & i.i.d. & i.i.d. & \((^{-2})\) \\  This Paper & Infinite & Average & Markovian & Markovian & \((^{-2})\) \\   

Table 1: Comparison with related single-timescale actor-critic algorithmsthe coupled estimation errors of the time-average reward, the critic, and the policy gradient norm altogether as an interconnected system in a much less conservative way. We emphasize that our analysis framework includes the discounted setting as a simple special case where the interconnected system is only two-dimensional without the time-average reward estimation.

### Related Work

**Policy gradient methods.** Policy gradient methods [26; 25] learn a parameterized policy, constituting a departure from the value-based approach [28; 32; 40]. The asymptotic convergence of policy gradient methods has been well established in [29; 26; 3; 14] via stochastic approximation methods . Some recent works have shown that PG methods can find the global optimum of some particular class of problems, such as LQR [11; 19] and tabular case problem . Under general function approximation setting, finite-time convergence of PG methods was analyzed in [1; 38; 33; 34]. Specifically,  established the finite-time convergence of PG methods under both tabular policy parameterizations and general parametric policy classes.  showed that a variant of PG methods can attain an \(\)-accurate stationary point at a sample complexity of \((^{-2})\), where they adopted Monte-Carlo sampling to find an unbiased estimation of policy gradient. [33; 34] studied the variance reduction PG and acceleration PG.

**Actor-Critic methods.** The AC algorithm was initially proposed by . Later,  extended it to the natural AC algorithm. The asymptotic convergence of AC algorithms has been well established in [14; 5; 7; 39] under various settings. Many recent works focused on the finite-time convergence of AC methods. Under the double-loop setting,  established the global convergence of AC methods for solving linear quadratic regulator (LQR).  studied the global convergence of AC methods with both the actor and the critic parameterized by neural networks.  studied the finite-time local convergence of a few AC variants with linear function approximation.

Under the two-timescale AC setting,  established the finite-time local convergence to a stationary point at a sample complexity of \(}(^{-2.5})\) under the undiscounted time-average reward setting.  studied both local convergence and global convergence for two-timescale (natural) AC, with \(}(^{-2.5})\) and \(}(^{-4})\) sample complexity, respectively, under the discounted accumulated reward. The algorithm collects multiple samples to update the critic.  proposed a two-timescale stochastic approximation algorithm for bilevel optimization and the algorithm was subsequently employed in the context of two-timescale AC.  established the global convergence of two-timescale AC methods for solving LQR, where only a single sample is used to update the critic in each iteration.

Under the single-timescale setting,  considered the least-squares temporal difference (LSTD) update for the critic and obtained the optimal policy within the energy-based policy class for both linear function approximation and nonlinear function approximation using neural networks.  studied single-timescale AC on LQR. In addition,  and  considered the single-timescale AC in general MDP cases, which have been reviewed and compared in Section 1.1.

**Notation.** We use non-bold letters to denote scalars and use lower and upper case bold letters to denote vectors and matrices respectively. Without further specification, we write \(x_{n}=(y_{n})\) if there exists an absolute positive constant \(C\) such that \(x_{n} Cy_{n}\), for two sequences \(\{x_{n}\}\) and \(\{y_{n}\}\). We use \(}()\) to hide logarithm factors. The total variation distance of two probability measure \(\) and \(v\) is defined by \(d_{TV}(,v):=_{}|(dx)-v(dx)|\). In addition, we use \(\) to denote a generic probability of some random event.

## 2 Preliminaries

In this section, we review the basics of the Markov decision process, policy gradient algorithm, and single-timescale AC with linear function approximation.

### Markov decision process

We consider the standard Markov Decision Process (MDP) characterized by \((,,,r)\), where \(\) is the state space and \(\) is the action space. We consider a finite action space \(||<\), whereas the state space can be either a finite set or an (unbounded) real vector space \(^{n}\). \((s_{t+1}|s_{t},a_{t})\) denotes the transition kernel. We consider a bounded reward \(r:[-U_{r},U_{r}]\), which is a function of the state \(s\) and action \(a\). A policy \(_{}(|s)^{||}\) parameterized by \(\) is defined as a mapping from a given state to a probability distribution over actions.

The RL problem of consideration aims to find a policy \(_{}\) that maximizes the infinite-horizon time-average reward [26; 25; 37; 31], which is given by

\[J():=_{T}_{}^{T-1}r(s_{t},a_{t})}{T}=}_{s_{},a _{}}[r(s,a)],\]

where the expectation \(_{}\) is over the Markov chain under the policy \(_{}\), and \(_{}\) denotes the stationary state distribution induced by \(_{}\). The existence of the stationary distribution can be guaranteed by the uniform ergodicity of the underlying MDP, which is a common assumption. Hereafter, we refer to \(J()\) as the time-average reward (or exchangeably, performance function), which can be evaluated by the expected reward over the stationary distribution \(_{}\) and the policy \(_{}\).

The state-value function is used to evaluate the overall rewards starting from a state \(s\) and following policy \(_{}\) thereafter, which is defined as

\[V_{}(s):=_{}[_{t=0}^{}(r(s_{t},a_{t} )-J())|s_{0}=s],\]

where the action follows the policy \(a_{t}_{}(|s_{t})\) and the next state comes from the transition kernel \(s_{t+1}(|s_{t},a_{t})\). Similarly, we define the action-value (Q-value) function to evaluate the overall rewards starting from \(s\), taking action \(a\), and following policy \(_{}\) thereafter:

\[Q_{}(s,a) =_{}[_{t=0}^{}(r(s_{t},a_{t})-J( ))|s_{0}=s,a_{0}=a]\] \[}}{{=}}r(s,a)-J()+ [V_{}(s^{})],\]

where the expectation in (i) is taken over \(s^{}(|s,a)\).

### Policy gradient theorem

The policy gradient theorem  provides an analytic expression for the gradient of the performance function \(J()\) with respect to the policy parameter \(\), which is given by:

\[_{}J()=_{s_{},a _{}}[Q_{}(s,a)_{}_{}(a|s)]. \]

Evaluating this gradient requires the Q-value corresponding to the current policy \(_{}\). The REINFORCE  is a Monte Carlo-based episodic algorithm, which uses all the rewards collected along the sample trajectory (that is, the differential return) as an approximation to the true Q-value.

Note that for any function \(b:\) that is independent of the action, we have

\[_{a}b(s)_{}(a|s)=b(s)(_{a }_{}(a|s))=b(s) 1=0.\]

Therefore, the policy gradient theorem can be written equivalently as:

\[ J()=_{s_{},a_{}}[(Q_{}(s,a)-b(s))_{}_{}(a|s )],\]

where \(b(s)\) is called the _baseline_ function. A popular choice of _baseline_ is the state-value function, which leads to the following advantage-based policy gradient

\[_{}J()=_{s_{},a _{}}[_{}(s,a)_{}_{}(a|s)],\]

where \(_{}=Q_{}(s,a)-V_{}(s)\) is known as the advantage function. This is the "REINFORCE with baseline" . The baseline function can help reduce variance. However, like all Monte Carlo-based methods, it can still suffer from high variance and thus learns slowly. In addition, it is inconvenient to implement the algorithm online for continuing tasks .

AC algorithm instead employs a bootstrapping critic to estimate the Q-value. We describe the classic single-timescale AC in the next subsection.

### The single-timescale actor-critic algorithm

We consider the classic single-sample single-timescale AC method, where the critic is bootstrapping and uses a single sampled reward to update in each iteration. This directly accommodates online learning for continuing tasks. We consider the following linear function approximation of the state-value function:

\[_{}(s;)=(s)^{},\]

where \(():^{d}\) is a known feature mapping, which satisfies \(\|()\| 1\). To drive \(_{}(s;w)\) towards its true value \(V_{}(s)\), the semi-gradient TD(0) update is applied to estimate the linear coefficient \(\) (hereafter referred to as the critic):

\[_{t+1}=_{t}+_{t}[(r_{t}-J()+(s_{ t+1})^{}_{t}-(s_{t})^{}_{t})](s_{t}), \]

where \(_{t}\) is the step size of the critic \(\) and \(r_{t}:=r(s_{t},a_{t})\). Since \(J()\) is unknown, the time-average reward setting introduces an additional estimator \(\) to estimate it. Hereafter, we simply refer to \(\) as the reward estimator. The temporal difference error can be defined as

\[_{t}:=r_{t}-_{t}+(s_{t+1})^{}_{t}-( s_{t})^{}_{t}.\]

Then, the update rule for the critic is given by

\[_{t+1} =_{t}+_{t}(r_{t}-_{t}),\] \[_{t+1} =_{t}+_{t}_{t}(s_{t}),\]

where \(_{t}\) is the step size of the reward estimator \(_{t}\).

Since \(_{t}\) is an approximation of the advantage function, similar to REINFORCE with baseline, the corresponding update rule for the actor can be written as:

\[_{t+1}=_{t}+_{t}_{t}_{} _{_{t}}(a_{t}|s_{t}),\]

where \(_{t}\) is the actor stepsize. The above-described AC is summarized in Algorithm 1, which is introduced in  as a classic online one-step AC algorithm. Algorithm 1 can be efficiently implemented for continuing tasks due to its online nature.

```
1:Input initial actor parameter \(_{0}\), initial critic parameter \(_{0}\), initial reward estimator \(_{0}\), stepsize \(_{t}\) for actor, \(_{t}\) for critic, and \(_{t}\) for reward estimator.
2:Draw \(s_{0}\) from some initial distribution
3:for\(t=0,1,2,,T-1\)do
4: Take action \(a_{t}_{_{t}}(|s_{t})\)
5: Observe next state \(s_{t+1}(|s_{t},a_{t})\) and reward \(r_{t}=r(s_{t},a_{t})\)
6:\(_{t}=r_{t}-_{t}+(s_{t+1})^{}_{t}-(s_ {t})^{}_{t}\)
7:\(_{t+1}=_{t}+_{t}(r_{t}-_{t})\)
8:\(_{t+1}=_{U_{}}(_{t}+_{t}_{t}(s_{t}))\)
9:\(_{t+1}=_{t}+_{t}_{t}_{} _{_{t}}(a_{t}|s_{t})\)
10:endfor
```

**Algorithm 1** Single-timescale Actor-Critic

Note that the "single-timescale" refers to the fact that the stepsizes \(_{t},_{t},_{t}\) are only constantly proportional to each other. In addition, this is a "single-sample" algorithm, since only one sample is needed for the update in each iteration. We remark that Algorithm 1 is more common in practice than double loop variants. In Line 8 of Algorithm 1, a projection (\(_{U_{}}\)) is introduced to keep the critic norm-bounded by \(U_{}\), which is widely adopted in the literature [31; 37; 36; 8] for analysis. Note that the projection can be handled easily, which is relaxed using its non-expansive property in our analysis.

## 3 Main Results

We first present several standard assumptions that are common in the literature of analyzing AC with linear function approximation [12; 35; 31; 8; 21]. Insights into these conditions and connections with relevant works are also discussed.

### Assumptions

By taking the expectation of \(_{t+1}\) in (2) with respect to the stationary distribution, we have for any given \(_{t}\)

\[_{}[_{t+1}|_{t}]=_{t}+_{t}(}+}_{t}), \]

where

\[} :=_{(s,a,s^{})}[(s)((s^{} )-(s))^{})], \] \[} :=_{(s,a)}[(r(s,a)-J())(s)], \]

and \(s_{}(),a_{}(|s),s^{} (|s,a)\) is the subsequent state of the \((s,a)\). It can be easily shown that  the TD limiting point \(^{*}()\) satisfies:

\[}+}^{*}()=0. \]

Note that \(}\) reflects the exploration of the policy. To see this, note that without sufficient exploration, \(}\) can be rank deficient and (6) can be unsolvable. Consequently, the critic update (2) will not converge. Hence, the following assumption is made to guarantee the problem's solvability.

**Assumption 3.1** (Exploration).: For any \(\), the matrix \(}\) defined in (4) is negative definite and its maximum eigenvalue can be upper bounded by \(-\).

Assumption 3.1 is commonly adopted in analyzing TD learning with linear function approximation [4; 42; 31; 23; 8; 21]. In particular, Assumption 3.1 holds if the policy \(_{}\) can explore all state-action pairs in the tabular case . In addition, with this assumption, we can choose \(U_{}=}{}\) so that all \(^{*}\) lie within the projection radius \(U_{}\) because \(\|}\| 2U_{r}\) and \(\|}^{-1}\|^{-1}\), which justifies the projection operator introduced in Line 8 of Algorithm 1.

**Assumption 3.2** (Uniform ergodicity).: For any \(\), denote \(_{}()\) as the stationary distribution induced by the policy \(_{}(|s)\) and the transition probability measure \((|s,a)\). For a Markov chain generated by the policy \(_{}\) and transition kernel \(\), there exists \(m>0\) and \((0,1)\) such that

\[d_{TV}((s_{}|s_{0}=s),_{}( )) m^{}, 0, s.\]

Assumption 3.2 assumes the Markov chain is geometrically mixing, which can be implied by the uniform ergodicity. It is commonly employed to characterize the noise induced by Markovian sampling in RL algorithms [4; 42; 31; 8; 21].

**Assumption 3.3** (Lipschitz continuity of policy).: Let \(_{}(a|s)\) be a policy parameterized by \(^{d}\). There exists positive constants \(B,L_{l}\) and \(L_{}\) such that for any \(,_{1},_{2}^{d}\), \(s\), and \(a\), it holds that:

* \(\|_{}(a|s)\| B\)
* \(\|_{_{1}}(a|s)-_{_{2}}(a|s)\|  L_{l}\|_{1}-_{2}\|\)
* \(|_{_{1}}(a|s)-_{_{2}}(a|s)| L_{}\|_{1}-_{2}\|\)

Assumption 3.3 is standard in the literature of policy gradient methods [22; 42; 38; 34; 31; 8; 21]. This assumption holds for many policy classes such as Gaussian policy , Boltzmann policy , and tabular softmax policy .

**Assumption 3.4**.: For any \(,^{}^{d}\), there exists constant \(L_{}\) such that \(\|_{}-_{^{}}\| L_{}\|-^{}\|\), where \(_{}(s)\) is the stationary distribution under the policy \(_{}\).

Assumption 3.4 is introduced in  to show the smoothness of the optimal critic \(^{*}()\), which is critical to guarantee the convergence of single-timescale AC. This assumption holds for the finite state-action space setting .

### Finite-Time Analysis

We define the following uniform upper bound for the linear function approximation error of the critic:

\[_{}:=_{}_{s _{}}((s)^{}^{*}()-V_{}(s))^{2}}.\]The error \(_{}\) is zero if \(V_{}\) is indeed a linear function for any \(\). Naturally, it can be expected that the learning errors of Algorithm 1 depend on \(_{}\).

We define the following integer \(_{T}\) that will be useful in the statement of the theorems, which depends on the number of total iterations \(T\):

\[_{T}:=\{i 0 m^{i-1}}\},\]

where \(m,\) are constants defined in Assumption 3.2. Therefore, we choose \(_{T}=}{^{-1}}+}= ( T)\) such that \(m^{_{T}-1}}\). The integer \(_{T}\) represents the mixing time of an ergodic Markov chain, which will be used to control the Markovian noise in the analysis.

We quantify the learning errors by defining \(y_{t}:=_{t}-J(_{t})\), which is the difference between the reward estimator and the true time-average reward \(J(_{t})\) at time \(t\). For the critic, we define, \(_{t}:=_{t}-_{t}^{*}\) with \(_{t}^{*}:=^{*}(_{t})\) to measure the error between the critic and its target value at iteration \(t\). The following two theorems summarize our main results.

**Theorem 3.5** (Markovian sampling).: _Consider Algorithm 1 with \(_{t}==},_{t}==},_ {t}==}\), where \(c\) is a constant depending on problem parameters. Suppose Assumptions 3.1-3.4 hold, we have for \(T 2_{T}\),_

\[}_{t=_{T}}^{T-1}y_{t}^{2} =(T}{})+(_{ }),\] \[}_{t=_{T}}^{T-1}\|_{t} \|^{2} =(T}{})+(_{ }),\] \[}_{t=_{T}}^{T-1}\| J( _{t})\|^{2} =(T}{})+(_{ }).\]

We defer the interpretation of the above results a bit and present below the analysis results under the i.i.d. sampling first for better comparison. The major difference of i.i.d. from Markovian sampling is that at the \(t\)-th iteration, the state \(s_{t}\) is sampled from the stationary distribution \(_{_{t}}\) instead of the evolving Markov chain (see Algorithm 2 in Appendix E). The i.i.d. sampling simplifies the analysis in the way that many Markovian noise terms reduce to zero effectively. This leads to a tighter sample complexity bound compared to the Markovian sampling by up to logarithmic factors.

**Theorem 3.6** (i.i.d. sampling).: _Consider Algorithm 2 (see Appendix E) with \(_{t}==},_{t}==}, _{t}==}\), where \(c\) is a constant depending on problem parameters. Suppose Assumptions 3.1-3.4 hold, we have for \(T 2_{T}\),_

\[}_{t=_{T}}^{T-1}y_{t}^{2} =(})+(_{}),\] \[}_{t=_{T}}^{T-1}\|_{t} \|^{2} =(})+(_{}),\] \[}_{t=_{T}}^{T-1}\| J (_{t})\|^{2} =(})+(_{}).\]

If the critic approximation error \(_{}\) is zero, we see that the reward estimator, the critic, and the actor estimation errors all diminish at a sub-linear rate of \(}(T^{-})\). The additional logarithmic term hidden by \(}()\) is incurred by the mixing time of the Markov chain, which can be get rid of under the i.i.d. sampling. It also hides the polynomials of all other problem parameters. They are explicitly characterized in the proofs up to the last step of analyzing the overall interconnected error propagation system. One can easily keep and get the dependence orders of all parameters if needed. Here we focus on the dependence of the iteration number for ease of presentation.

To put the results into perspective, note that \((T^{-})\) is the rate one would obtain from stochastic gradient descent (SGD) on general non-convex functions with unbiased gradient updates. In terms of sample complexity, to obtain an \(\)-approximate stationary point, it takes a number of \(}(^{-2})\) samples for Markovian sampling (Algorithm 1) and \((^{-2})\) for i.i.d. sampling (Algorithm 2), which matches the state-of-the-art performance of SGD on non-convex optimization problems.

The obtained sample complexities are superior to those of other AC variants. Notably,  provided finite-time convergence for double-loop variant with a \((^{-4})\) sample complexity and  analysed two-timescale variant, yielding a \(}(^{-2.5})\) sample complexity. The sample complexity gap is intrinsic to their inefficient usage of data. In the double-loop setting, the critic starts over to estimate the Q-value for an intermediate policy in the inner loop, ignoring the fact that the consecutive Q-values can be similar given a relatively minor policy update. The two-timescale setting artificially slows down the actor update by adopting an actor stepsize that decays faster than the critic. The single-timescale approach updates the critic and actor parallelly with proportional stepsizes and thus learns more efficiently.

Moreover, our result matches the \((^{-2})\) sample complexity of policy gradient methods such as REINFORCE [2; 22] under the i.i.d sampling. It is previously found in  that there is a sample complexity gap between Algorithm 1 adopting two-timescale stepsizes and (variance-reduced) REINFORCE . In this paper, we close this gap by providing a single-timescale analysis for Algorithm 1 which shows that this practical single-timescale AC can have the same sample complexity as REINFORCE.

### Proof Sketch

The main challenge in the finite-time analysis lies in that the estimation errors of the time-average reward, the critic, and the policy gradient are strongly coupled. To overcome this difficulty, we view the propagation of these errors as an interconnected system and analyze them holistically. To better appreciate the advantage of our analysis framework over the decoupled methods that are traditionally adopted in analyzing double-loop and two-timescale variants, we sketch the main proof steps of Theorem 3.5 in the following. We also highlight the key challenges and techniques developed correspondingly. All supporting lemmas mentioned below can be found in Appendix.

We first derive implicit (coupled) upper bounds for the reward estimation error \(y_{t}\), the critic error \(_{t}\), and the policy gradient \( J(_{t})\), respectively. Then, we solve a system of inequalities to establish finite-time convergence.

**Step 1: Reward estimation error analysis.** Using the reward estimator update rule (Line 7 of Algorithm 1), we decompose the reward estimation error into:

\[ y_{t+1}^{2}&=(1-2_{t})y_{t}^{2 }+2_{t}y_{t}(r_{t}-J(_{t}))\\ &+2y_{t}(J(_{t})-J(_{t+1}))+(J(_{t})-J(_{t+1})+_{t}(r_{t}-_{t}))^{2}. \]

The second term on the right-hand side of (7) is a bias term caused by the Markovian sample, which is characterized in Lemma C.1. As shown in Lemma E.1, this bias reduces to 0 under i.i.d. sampling after taking the expectation. The third term captures the variation of the moving targets \(J(_{t})\). The double-loop variant of AC runs a policy evaluation sub-problem in the inner loop for each target \(J(_{t})\) to estimate the policy gradient accurately. This easily ensures the monotonic decreasing of \(J(_{t})\) and consequently the convergence. The two-timescale variant utilizes the additional property of \(_{t}_{t}/_{t}=0\) to annihilate this term and consequently can have a decoupled analysis. In the case of single-timescale AC, we do not have the aforementioned special algorithm designs and properties to ease the analysis. Instead, we utilize the smoothness of \(J()\) (see Lemma B.2) and derive an implicit upper bound for this term as a function of the norm of \(y_{t}\) and \( J(_{t})\). This bound will be combined with the implicit bounds derived in Step 2 and Step 3 below to establish the non-asymptotic convergence altogether. The last term in (7) reflects the variance in reward estimation, which is bounded by \(}(_{t})\).

**Step 2: Critic error analysis.** Using the critic update rule (Line 8 of Algorithm 1), we decompose the squared error by (we neglect the projection for the time being for the ease of comprehension. The complete analysis can be found in the appendix.)

\[\|_{t+1}\|^{2}=&\|_{t}\|^ {2}+2_{t}_{t},(_{t},_{t}) +2_{t}(O_{t},_{t},_{t})+2_{t} _{t}, g(O_{t},_{t},_{t})\\ &+2_{t},_{t}^{*}-_{t+1}^ {*}+\|_{t}(g(O_{t},_{t},_{t})+ g(O_{t },_{t},_{t}))+_{t}^{*}-_{t+1}^{*}\|^{2},  \]where \(O_{t}:=(s_{t},a_{t},s_{t+1})\) is a tuple of observations and the definitions of \(g,, g\), and \(\) can be found in (12) and (13) in Appendix A. Without diving into the detailed definitions, here we focus on illustrating the high-level insights of our proof. First of all, the second term on the right-hand side of (8) can be bounded by \(-2_{t}\|_{t}\|^{2}\) under Assumption 3.1. It provides an explicit characterization of how sufficient exploration can help the convergence of learning. The third term is a Markovian noise, which is further bounded implicitly in Lemma C.3. For the i.i.d sampling case, as shown in Lemma E.1, this bias reduces to 0 after taking the expectation. The fourth term is caused by inaccurate reward and critic estimations, which can be bounded by the norm of \(y_{t}\) and \(_{t}\). The fifth term tracks both the critic estimation performance \(_{t}\) and the difference between the drifting critic targets \(_{t}^{*}\). Similar to the case of Step 1, the double-loop approach bounds this term relying on the accurate policy evaluation sub-problem in the inner loop for each target \(_{t}^{*}\), whereas the two-timescale approach ensures its convergence by additionally requiring \(_{t}_{t}/_{t}=0\). In contrast, we establish an implicit upper bound for this term as a function of \(y_{t}\) and \(_{t}\) by utilizing the smoothness of the optimal critic proved in Lemma B.4. Finally, the last term reflects the variances of various estimations, which is bounded by \((_{t})\).

**Step 3: Policy gradient norm analysis**. Using the actor update rule (Line 9 of Algorithm 1) and the smoothness property of \(J()\) (see Lemma B.2), we derive

\[\| J(_{t})\|^{2}& }(J(_{t+1})-J(_{t}))+(O_{t}, {}_{t})- J(_{t}), h(O_{t},_{t},_{t},_{t})\\ &- J(_{t}),_{O_{t}^{} }[ h^{}(O_{t}^{},_{t})]+}}{2}_{t}\|_{t}_{_{t}}(a_{t}|s_{t}) \|^{2}, \]

where \(O_{t}^{}\) is a shorthand for an independent sample from stationary distribution \(s_{_{t}},a_{_{t}},s^{}(|s,a)\), \(\) is defined in (13), and \(L_{J^{}}\) is a constant. The first term on the right-hand side of (9) compares the actor's performances between consecutive updates, which can be bounded via Abel summation by parts. The second term is a noise term introduced by Markovian sampling, which is characterized in Lemma C.6. Again, as proven in Lemma E.1, this bias reduces to 0 under i.i.d. sampling after taking the expectation. The third term is an error introduced by the inaccurate estimations of both the time-average reward and the critic. This term was directly bounded to zero under both the double-loop setting and the two-timescale setting due to their particular algorithm design, to enable a decoupled analysis. We control this term by providing an implicit bound depending on \(y_{t}\), \(_{t}\), and \( J(_{t})\). The fourth term comes from the linear function approximation error. The last term captures the variance of the stochastic gradient update, which is bounded by \((_{t})\).

**Step 4: Interconnected iteration system analysis.** Taking the expectation of and summing (7), (8), and (9) from \(_{T}\) to \(T-1\), respectively, we obtain the following system of inequalities in terms of \(Y_{T},\ Z_{T},\ G_{T}\):

\[ Y_{T}:=}_{t=_{T}}^{T-1} y_{t}^{2}&(T}{})+ l_{1}G_{T}},\\ Z_{T}:=}_{t=_{T}}^{T-1}\|_{t}\|^{2}&(T}{})+(_{})+l_{2}Z_{T}}+l_{3}(2Y_{T}+8Z_{T})},\\ G_{T}:=}_{t=_{T}}^{T-1}\|  J(_{t})\|^{2}&(T}{ })+(_{})+l_{4}(2Y_{T}+8Z_{T} )},\]

where \(l_{1},l_{2},l_{3},l_{4}\) are positive constants. By solving the above system of inequalities, we further prove that if

\[l_{1}(1+2l_{4}^{2}+8l_{4}^{2}(2l_{2}^{2}+l_{3})) 1 16l_{3}  1,\]

then \(Y_{T},Z_{T},G_{T}\) converge at a rate of \((T}{})\). This condition can be easily satisfied by choosing the stepsize ratio \(c\) to be smaller than a threshold identified in Equation (28). Thus, it completes the proof.

The above proof applies to i.i.d sampling straightforwardly, with the corresponding terms pointed out in the above steps reducing to 0 in the analysis. The additional proof can be found in Lemma E.1.

Conclusion and Discussion

In this paper, we establish the finite-time analysis for single-timescale AC with Markovian sampling. Our work compares favorably to existing works in terms of analyzing online learning and considering the continuous state space. We developed a series of lemmas that characterize the propagation of errors, and establish their convergence simultaneously by solving a system of nonlinear inequalities. The proposed framework is general and can be applied to analyze other single-timescale stochastic approximation algorithms. Our future work includes further considering the continuous action space problems and developing new proof techniques that require fewer assumptions.