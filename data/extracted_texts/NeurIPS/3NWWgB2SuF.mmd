# Undirected Probabilistic Model for Tensor Decomposition

Zerui Tao\({}^{1,2}\)   Toshihisa Tanaka\({}^{1,2}\)   Qibin Zhao\({}^{2,1}\)

zerui.tao@riken.jp   tanakat@cc.tuat.ac.jp   qibin.zhao@riken.jp

\({}^{1}\)Tokyo University of Agriculture and Technology  \({}^{2}\)RIKEN AIP

Corresponding author

###### Abstract

Tensor decompositions (TDs) serve as a powerful tool for analyzing multiway data. Traditional TDs incorporate prior knowledge about the data into the model, such as a directed generative process from latent factors to observations. In practice, selecting proper structural or distributional assumptions beforehand is crucial for obtaining a promising TD representation. However, since such prior knowledge is typically unavailable in real-world applications, choosing an appropriate TD model can be challenging. This paper aims to address this issue by introducing a flexible TD framework that discards the structural and distributional assumptions, in order to learn as much information from the data. Specifically, we construct a TD model that captures the joint probability of the data and latent tensor factors through a deep energy-based model (EBM). Neural networks are then employed to parameterize the joint energy function of tensor factors and tensor entries. The flexibility of EBM and neural networks enables the learning of underlying structures and distributions. In addition, by designing the energy function, our model unifies the learning process of different types of tensors, such as static tensors and dynamic tensors with time stamps. The resulting model presents a doubly intractable nature due to the presence of latent tensor factors and the unnormalized probability function. To efficiently train the model, we derive a variational upper bound of the conditional noise-contrastive estimation objective that learns the unnormalized joint probability by distinguishing data from conditional noises. We show advantages of our model on both synthetic and several real-world datasets.

## 1 Introduction

Tensor decompositions (TDs) serve as powerful tools for analyzing high-order and high-dimensional data, aiming to capture the inter-dependencies among different modes by utilizing multiple latent factors. TDs have demonstrated remarkable success in various machine learning tasks, including data imputation , factor analysis , time-series forecasting , model compression , generative models  among others.

Existing TDs typically incorporate predefined directed graphical models into the generative process. These models specify the priors of latent factors and the conditional probabilities of observations, following specific contraction rules associated with the latent factors. Traditional contraction rules predominantly employ multi-linear products, like CP , Tucker , tensor train  and other variants . However, selecting an appropriate contraction rule for specific datasets is often challenging in real-world applications. Recent research, known as tensor network structure search [TNSS, 22, 23], has demonstrated that selecting an appropriate TN contraction rule significantly enhances the factorization performance. Another promising approach involves learning non-linear mappings from the data, utilizing techniques like nonparametric models  and deep neural networks[25; 9]. Empirical results demonstrate that non-linear TDs exhibit superior performance compared to traditional multi-linear TDs in various applications, attributed to their enhanced expressive power.

Despite the success of non-linear TDs in reducing structural assumptions, they often rely on simplistic distributional assumptions. Typically, a specific directed graphical model is adopted to model the generative process from latent factors to tensor entries, represented as \(p(x)= p(x z)p(z)\,z\), where \(z\) denotes tensor factors and \(x\) represents observations. Additionally, the distributions are usually selected from exponential families for tractability, such as Gaussian and Bernoulli distributions. For instance, a Gaussian prior can be assigned to latent factors, and observed entries can be modeled using Gaussian distribution [32; 49] or Gaussian process [45; 53]. However, these prior assumptions regarding the probabilistic model can introduce model bias and reduce the effectiveness of TD models. In real-world applications, the latent factors might originate from unknown distributions, and the observations can exhibit complex multi-modal generative processes. Without knowing the underlying generative process, these simplistic assumptions can lead to inaccurate estimations.

To address these issues, this paper proposes to construct an undirected graphical model of TD. More specifically, a TD model that captures the joint probability of the data and latent tensor factors is constructed through a deep energy-based model (EBM), represented as \(p(x,z)(-f(x,z))\). Neural networks (NNs) are then employed to parameterize the joint energy function \(f(x,z)\). The flexibility of EBM and NNs facilitates the learning of underlying structures and distributions. Furthermore, our model unifies the learning process in the presence of side information, such as dynamic tensors with time stamps, by designing the energy function. The resulting model presents a doubly intractable nature due to the presence of latent tensor factors and the unnormalized probability density function (pdf). For efficient model training, we derive a variant of conditional noise-contrastive estimation  algorithm that learns the unnormalized joint probability by distinguishing data from conditional noises. The proposed model offers several advantages: (1) it features a flexible structure that can adapt to different distributions; (2) the undirected nature allows us to learn more general correlations than traditional directed TDs; (3) it can handle diverse tasks and encode auxiliary information by adjusting the energy function.

Experiments are conducted on synthetic and real-world datasets to showcase the advantages of our model. Through simulation studies, we demonstrate the capability of our model to handle data generated from diverse distributions, in contrast to traditional Gaussian-based models that yield unfaithful and biased estimates. Subsequently, experiments are performed on multiple real-world datasets to evaluate sparse and continuous-time tensor completion. Our model outperforms various baselines across multiple metrics and settings, highlighting the generality of the proposed model.

## 2 Backgrounds

NotationsWe adopt similar notations with . Throughout the paper, we use lowercase letters, bold lowercase letters, bold capital letters and calligraphic bold capital letters to represent scalars, vectors, matrices and tensors, _e.g._, \(x,,\) and \(}\). Tensors refer to multi-way arrays which generalize matrices. For a \(D\)-order tensor \(}^{I_{1} I_{D}}\), we denote its \((i_{1},,i_{D})\)-th entry as \(x_{}\).

### Tensor decomposition

Given a \(D\)-order tensor \(}^{I_{1} I_{D}}\), tensor decomposition (TD) aims to factorize \(}\) into \(D\) smaller latent factors \(^{d=1,,D}^{I_{d} R_{d}}\) by using some predefined tensor contraction rules. The classical Tucker decomposition  assumes \(}=}_{1}^{1}_{2}_{D }^{D}\), where \(}^{R_{1} R_{D}}\) is the coefficient and \(_{d}\) denotes the matrix-tensor contraction . Equivalently, each entry can be written as \(x_{}=_{r=1}^{R_{1}}_{r_{D}=1}^{R_{D}}w_{r_{1} r _{D}}z_{i_{1}r_{1}} z_{i_{D}r_{D}}^{D}\), where the tuple \((R_{1},,R_{D})\) is the Tucker rank of tensor \(}\). The latent factors \(^{d}\) can capture information of each tensor mode and \(}\) represents the weight of each factors. CP decomposition  is a restricted form of Tucker by assuming \(}\) is super-diagonal, _i.e._, \(x_{}=_{r=1}^{R}w_{r}z_{i_{1}r_{1}}^{} z_{i_{D}r_{1 }}^{D}\), where we simplify \(w_{r}=w_{r r}\). In this paper, we focus on probabilistic version of TDs, which serves as generalizations of traditional ones. The standard approach is to formulate TDs as a directed graphical model, \(p(})= p(})p()\, \), where \(\) denotes \(\{^{1},,^{D}\}\) for simplicity. For continuous data, the \(p(})\) is usually assumed to be Gaussian and TDs are used to parameterize the mean of corresponding Gaussian distribution [32; 49; 50].

Despite the elegant form of these multi-linear contraction rules, they have limited flexibility that can be mitigated by extending TDs to their non-linear counterparts. We can think of TD as a function that maps the multiway latent factors to tensor entries. One extension is to add Gaussian process (GP) priors on the function to obtain a nonparametric model, which resembles a GP latent variable model. In particular,  proposed to stack the latent factors as \(}}=[_{i_{1}}^{1},,_{i_{D}}^{D}]^ {DR}\) and then assign a GP prior on the functional mapping. In specific, for continuous data, it assumes \(x_{}(_{},^{2})\), where the mean function is a GP \(_{}=f(}})(0,k(} },))\) associated with kernel \(k(,)\). Since GP has large computational complexity and designing the kernel requires ad-hoc expert domain knowledge,  proposed to parameterize the function using neural networks (NNs), \(x_{}(_{},^{2})\) where \(_{}=f_{}(}})\). However, NNs easily overfit due to the high-dimensional and sparse nature of tensor data. To address this issue,  proposed to use Bayesian NN with spike-and-slab prior for sparse weights. All these models are based on directed graphical model, that assumes there exists a direct mapping from the latent factors to tensor entries and use simplistic distributions.

### Energy-based model

Energy-based model [EBM, 21] is a class of undirected probabilistic models, which uses an energy function to characterize the data distribution. Given observed data \(x\), the basic idea of EBM is to approximate the data distribution by a Boltzmann distribution, \(p_{}(x)\), where \(Z()=(-f(x;))\,x\) is the normalization constant (a.k.a., the partition function) and \(f(x;)\) is the energy function. One classical example of EBM is the restricted Boltzmann machine (RBM) where the energy function has bi-linear form for tractability. In deep EBMs, the energy function is typically parameterized by deep neural networks. The main difficulty for training EBMs is to deal with the intractable normalization constant . There are several ways to train EBMs, including contrastive divergence , score matching , noise-contrastive estimation [NCE, 11] and so on. In this paper, we focus on NCE, due to its efficiency and ability of tackling different data types.

We denote the unnormalized pdf as \((x;)=(-f(x;))\). NCE consider the normalization constant \(Z()\) as a trainable parameter. However, maximum likelihood estimation (MLE) does not work for this case since \(Z()\) can be arbitrarily small and the log-likelihood goes to infinity. Instead, the NCE can be obtained by maximizing the following objective,

\[_{}()=_{x} h(x;)+_{y}(1-h(y;)),\]

where \(x\) denotes observed data and \(y\) denotes noises generated from some known distribution \(p_{n}\), \(\) is the ratio between noise and sample sizes, _i.e._, \(=\#y/\#x\). And \(h()\) can be regarded as a classifier that distinguish data from noises, defined as follows, \(h(u;)=(u)}\). It has been shown that NCE is consistent with MLE .

Although the noise distribution is essential for training efficiency, the selection of noises is currently limited to heuristics. A common intuitive is that the noise should be similar with the data. Indeed,  proposed to use conditional noises \(y p_{c}(y x)\) and minimizing the following loss function,

\[_{}()=2_{xy}[1+(-G(x,y))],\] (1)

where \(G(u_{1},u_{2};)=;)p_{c}(u_{2}|u_{1})}{(u_{ 2};)p_{c}(u_{1}|u_{2})}\) with \(y\) drawn from \(p_{c}(y x)\).

## 3 Proposed model

### Energy-based tensor decomposition

Even though many non-linear tensor decompositions (TD) have been proposed to enhance flexibility, existing methods typically adopt simplistic distributions such as Gaussian. This can be problematic for complex real-world data. To address the issue, we propose energy-based tensor decomposition (EnergyTD), by integrating EBMs in TD framework. Given an order-\(D\) tensor \(}\) of shape \(I_{1} I_{D}\), we aim to factorize it into \(D\) smaller latent factors \(^{d}^{I_{d} R}, d=1,,D\). We denote the latent factor associated with the \((i_{1},,i_{D})\)-th entry as \(}}=[_{i_{1}}^{1},,_{i_{D}}^{D}] ^{DR}\), where \(_{i_{d}}^{d}^{R}\) is the \(i_{d}\)-th row of \(^{d}\). Unlike traditional directed TDs trying to parameterize the conditional expectation \([x_{}}}]=f(}})\), we model the joint distribution using an EBM,

\[p(x_{},}};)=},}};))}{Z()},\] (2)where \(f(,;)\) is the energy function and \(Z()=(-f(x_{},_{};))\,x_{ }\,_{}\) is the partition function to make it a valid pdf. We further assume the joint probability of all entries are independent, _i.e._, \(p(},)=_{i}p(x_{},_{{}_{i}} ^{},,_{{}_{i}}^{})\), where \(\) denotes the set of observed entries. This is a standard setting in TDs and the dependence of tensor entries can be captured by sharing latent factors.

The expressive nature of the energy function enables us to easily handle diverse data types. For example, we can deal with discrete data by plugging one-hot codings into Eq. (2) to represent categorical probabilities. Additionally, the flexibility of NNs allows us to model tensors with side information, where each tensor entry incorporates additional features . Specifically, in this paper, we focus on a particular case of dynamic tensors with continuous time stamps . In this case, we consider an observed tensor as a time series \(}_{t}\), where the time stamp \(t\) is continuous, and each entry \(\) has its own specific time stamp \(t_{}\). To model the tensor time series, we assume that each entry follows the same distribution and construct the time-dependent energy function, \(p(x_{},_{};,t_{})(-f(x_{ },_{},t_{};))\), where the time stamp \(t_{}\) is considered as an auxiliary feature. The flexibility of NNs allows this function to learn general patterns across continuous time stamps. Experimental results demonstrate that this simple treatment can achieve good performances.

Network architectureThe network architecture plays a crucial role in learning accurate probabilistic manifolds. Specifically, we define the energy function as \(f(x_{},_{})=g_{1}(g_{2}(g_{3}(x_{}),g_{4} (_{})))\), where \(g_{3}\) and \(g_{4}\) are MLP layers that encode information from \(x_{}\) and \(_{}\), respectively. \(g_{2}\) is a summation or concatenation layer that induce coupling between tensor values and latent factors, and \(g_{1}\) is the output layer. Although we currently utilize only MLPs, it is worth noting that convolutional architectures, as demonstrated by [39; 25], can also be employed, which is a topic for future research. To handle dynamic tensors, we incorporate an extra sinusoidal positional encoding layer  denoted as \(g_{5}(t)\) to capture temporal information. This embedding utilizes random Fourier features as proposed by . Consequently, the energy function can be expressed as \(f(x_{},_{},t_{})=g_{1}(g_{2}(g_{3}(x_{ }),g_{4}(_{}),g_{5}(t_{})))\). This architecture is commonly employed to capture temporal information and has been demonstrated to effectively learn high-frequency information when combined with MLPs .

Posterior samplingA significant application of TDs is to estimate the posterior of missing entries. Unlike traditional TDs, direct predictions cannot be obtained even after learning the latent factors due to the utilization of an undirected probabilistic model. Instead, we need to seek for sampling methods of \(p(x_{}_{})\). One choice is score-based samplers by utilizing the score function \(_{x_{}} p(x_{}_{})= _{x_{}}},_{})}{p( _{})}=-_{x_{}}f(x_{},_{ })\), such as Langevin dynamics . Score-based samplers are not suitable for handling discrete data. However, in our case, we model the one-dimensional pdf for each entry, enabling us to directly sample the discrete data. Consequently, for continuous data, the use of grid search is a viable approach to obtain maximum a posteriori (MAP) estimations.

### Learning objective

Despite the flexibility of the proposed model in Eq. (2), obtaining maximum likelihood estimation (MLE) becomes doubly intractable, as both the partition function \(Z()\) and the marginal distribution \(p(x_{})\) are intractable. Therefore, the CNCE loss Eq. (1) cannot be directly applied. In this section, we extend the variational approach  to construct a upper bound that addresses the challenge posed by intractable marginal distributions.

Denote the unnormalized pdf as \((x_{},_{};)=(-f(x_{}, _{};))\) and the unnormalized marginal pdf as \((x_{};)=(x_{},_{}; )\,_{}\). For clarity, we omit the index \(\) in the subsequent context of this subsection. We follow the idea of CNCE  to distinguish data \(x\) from conditional noises \(y p_{c}(y x)\). Firstly, Eq. (1) can be rewritten as

\[_{}()=2_{xy}[1+1/r(x,y;)],\] (3)

where

\[r(x,y;)=(y x)}{(y;)p_{c}(x y)}.\] (4)

However, for our problem, the unnormalized marginal probability \((x;)\) is unknown. An additional variational distribution \(q(;)\) is used to approximate the true posterior \(p(};)\). Note that in TDs where the data size is static, there is no need for amortized inference, which is different from previous ones like . Equipped with the variational distribution, the unnormalized marginal distribution can be computed using importance sampling,

\[(x;)=;)q(;)}{q(; )}\,=_{q(;)}[; )}{q(;)}].\] (5)

Plugging Eq. (5) into Eq. (4), we have

\[r(x,y;)=_{q(;)}[(x,;)/q( ;)]p_{c}(y x)}{(y;)p_{c}(x y)}.\] (6)

Since Eq. (3) is a convex function w.r.t. \(r(x,y;)\), plugging Eq. (6) into Eq. (3) and applying the Jensen's inequality, we have the upper bound,

\[_{}() =2_{xy}[1+1/r(x,y;)]\] \[ 2_{xy}_{q(;)}[1+ (x y)q(;)}{(x,;)p_{ c}(y x)}]_{(,)}.\] (7)

Following , we have the theorem about the tightness of the bound.

**Theorem 1**: _The difference between the VCNCE loss Eq. (7) and CNCE loss Eq. (1) is the expectation of the \(f\)-divergence,_

\[_{}(,)-_{}( )=_{xy}[_{f_{xy}}(p( x;)\|q( ;))],\]

_where \(f_{xy}(u)=(+u^{-1}}{_{xy}+1})\) with \(_{xy}=(y x)}{(y;)p_{c}(x y)}\)._

The proof can be found in appendix. Based on the theorem, we have the following corollaries to justify the optimization process.

**Corollary 1**: _When \(q(;)\) equals to the true posterior, the CVNCE bound is tight, i.e.,_

\[_{}=_{} q (;)=p( x;).\]

**Corollary 2**: _The following two optimization problems are equivalent,_

\[_{}_{}()=_{}_{q(; )}_{}(,).\]

In practice, we need to seek for the sampled version of Eq. (7). Supposing we have \(N\) observed samples \(\{x_{}\}_{i=1}^{N}\), \(\) noises \(\{y_{i,j}\}_{j=1}^{}\) for each sample \(x_{}\) and using importance sampling for \((y;)\), the sampled objective function is,

\[_{}(,)=_{= 1}^{N}_{j=1}^{}_{q(_{};)}[1+ _{q(_{};)}[, {m}_{};)}{q(_{};)}]p_{c}(x_{ } y_{,j})q(_{};)}{(x_{ },_{};)p_{c}(y_{,j} x_{})}].\]

Specifically, we formulate \(q(;)\) as a diagonal Gaussian and use reparameterization trick  to compute the expectation. When dealing with continuous data, we typically select conditional Gaussian noises, represented as \(p_{c}(y x)=(y x,^{2})\). This choice entails only one hyperparameter \(\) that needs to be tuned. Another benefit is the symmetry of the conditional distribution for Gaussian noise, expressed as \(p_{c}(y x)=p_{c}(x y)\). Hence, the objective function can be further reduced. For binary or categorical data, such symmetric noises can also be derived .

The time complexity of the proposed objective is \(( B(DRH+LH^{2}))\), where \(B\) is the batch size, \(\) is the number of conditional noises, \(H\) is the number of hidden units per layer, \(L\) is the number of layers and \(D\) is the tensor order. The time complexity of our model is \(\) times greater than traditional TDs, since we need to compute forward passes for \(\) particles. However, as we only use small networks, the computational speed is still very fast (See Appendix C.3 for an illustration).

Related work

Traditional tensor decompositions (TDs) are based on multi-linear contraction rules, such as CP , Tucker , tensor networks [29; 51] and their variations [19; 6]. In this paper, we mainly focus on probabilistic TDs, which extend traditional methods by providing uncertainty estimates about both observations and latent factors [32; 49; 50; 26; 38]. These models build directed mapping from latent factors to tensor entries using multi-linear contraction rules, resulting in limited flexibility when dealing with complex datasets. An alternative approach involves replacing the multi-linear relations with non-linear ones. [5; 45; 52] introduced the use of tensor-variate Gaussian processes (GPs) for achieving nonparametric factorization.  further expanded on this concept by incorporating a GP prior on the function that maps latent factors to tensor entries, resulting in a nonparametric TD for sparse tensors. GP-based TDs are further extended using hierarchical priors , stochastic processes [43; 8]. Despite the success of GP-based TDs, nonparametric approaches can encounter computational challenges and may still lack sufficient flexibility. Recently, neural networks (NNs) are also applied to TDs.  suggested the utilization of convolutional NNs to map latent factors to tensor entries. Besides,  built a hierarchical version of the Tucker model and introduced non-linear mappings within each hierarchy of latent factors. To mitigate overfitting,  suggested the adoption of deep kernels in GP-based TD rather than using NNs directly. On the other hand,  proposed to use Bayesian NN with spike-and-slab prior to prevent from overfitting and obtain probabilistic estimates. More recently,  adopt neural ODEs to capture dynamic tensor trajectories. Other works regarding more flexible exponential families  or mixture of Gaussians  employ linear structures. While all these methods using directed mapping from latent factors to tensor entries, our model is fundamentally different from them, in that we construct much more flexible undirected probabilistic model of TD that can deal with diverse distributions and structures.

Another related direction is the energy-based model (EBM). To address the intractable pdf of EBMs, various training methods have been proposed, including contrastive divergence [CD, 14], score matching [SM, 16], noise-contrastive estimation [NCE, 11]. CD requires large steps of Monte Carlo samples, which can be computationally expensive for high-dimensional tensors. SM cannot handle discrete data, and learning latent variables with SM requires complex bi-level optimization . Therefore, we focus on NCE in this paper. Learning energy-based TD is even more challenging because it involves multiple coupled latent factors that cannot be analytically marginalized.  proposed VNCE to handle unnormalized models with latent factors. We enhance their algorithm by using conditional noises  to improve the learning efficiency. One fundamental distinction between our model and traditional EBMs is that, through TD construction, we only need to learn one-dimensional distributions for each scalar entry, instead of the original high-dimensional tensor. Hence, our model avoids performance degradation when learning high-dimensional data using NCE.

## 5 Experiments

We demonstrate the proposed energy-based TD (EnergyTD) on synthetic data and several real-world applications. All the experiments are conducted on a Linux workstation with Intel Xeon Silver 4316 CPU, 256GB RAM and NVIDIA RTX A5000 GPUs (24GB memory each). The code is implemented based on PyTorch 1.12.1 . More experimental details can be found in the appendix. The code is available at https://github.com/taozerui/energy_td

### Simulation study

Tensors with non-Gaussian distributionsTraditional TDs commonly assume that tensor entries follow a Gaussian distribution. However, in real-world applications, we often encounter highly complex distributions. In this experiment, we evaluate the capability of our model to learn distributions that deviate from the Gaussian assumption.

We consider a two-mode tensor of shape \(I I\), where we set \(I=8\). Firstly, two latent factors of shape \(I R\) are generated, where the rank \(R\) is set to \(5\). Then, conditioned on the latent factors, we generated tensor observations from particular distributions. For each entry, we generate \(N=200\) samples. Three types of distributions are considered: (1) Beta distribution; (2) Mixture of Gaussians (MoG) and (3) Exponential distribution. For Beta distribution, we generate latent factors from uniform distribution \(^{i=1,2}}}{{}}Uni(0.0,1.1)\). Then, we sample the observed tensor from Beta distribution \(x_{ij}}{}Beta((^{1}^{2,})_{ij},1.2)\). For MoG distribution, we draw latent factors from uniform distribution \(^{i=1,2}}{}Uni(0.0,1.0)\). The tensor entries are then drawn from MoG \(x_{ij}}{}0.6(((^{1}^{2, })_{ij}),0.1^{2})+0.4(((^{1}^{2, })_{ij}),0.25^{2})\). For Exponential distribution, we generate latent factors from uniform distribution \(^{i=1,2}}{}Uni(0.0,1.0)\). The tensor entries are then sampled from Exponential distribution \(x_{ij}}{}Exp((^{1}^{2,})_{ij})\). We compare with GP tensor factorization [GPFT, 53], which assumes the entries follow Gaussian distribution.

The results of probability density function (pdf) estimation are presented in Fig. 1. We display the learned pdf of one single tensor entry. This reveals that GPTF is limited to capturing only the 1st moments (mean values) and overlooks higher-order information of more complex distributions. Our model exhibits greater flexibility in handling non-Gaussian distributions.

Continuous-time tensorsWe then consider a dynamic tensor, where each entry is a time series. We follow similar setting with  and use the same data size with the previous simulation, _i.e._, a two-mode tensor of shape \(8 8\) with each entry being a time series of length \(200\). We firstly generate latent factors of shape \(8 2\), with each rows drawn from \(_{i}^{1}(,2)\) and \(_{i}^{2}(,2)\). Then we generate \(N=200\) observed entries from time \(t\). The tensor entries are computed by \(x_{i}(t)=_{r_{1}=1}^{2}_{r_{2}=1}^{2}z_{i_{1}r_{1}}^{1}z_{i_{2}r_{2}}^ {2}_{r_{1}r_{2}}(t)\), where \(_{11}(t)=(2 t),_{12}(t)=(2 t),_{21}(t)=^{ 2}(2 t)\) and \(_{22}(t)=(5 t)^{2}(5 t)\). Finally, the data are normalized to range \(\). The synthetic data consist of low-frequency trends and high-frequency fluctuations. Apart from fully observed case, we also test with missing rates (MR) \(10\%\) and \(30\%\). In specific, for each entry, we randomly select a starting time and set the following consecutive \(10\%\) or \(30\%\) time stamps as missing.

We compare with two methods that are designed for dynamic tensors, the Bayesian continuous-time Tucker decomposition [BCTT, 8] and the nonparametric factor trajectory learning [NONFAT, 43].

Figure 1: Simulation results for different distributions. The blue line is the ground truth pdf. The yellow line is the kernel density estimation (KDE) plot of observed samples. The red line is the GPTF estimation, which is a Gaussian pdf. The green line is our method, computed by evaluating the unnormalized pdf on grids and calculating the partition function using Gaussian quadrature.

Figure 2: Simulation results for continuous-time tensor decomposition. The blue regions are observed and the red regions are missing. The trajectories of ground truth, BCTT, NONFAT and our model are drawn in black, blue, red and green lines, respectively.

BCTT treats Tucker core tensors as functions of time and NONFAT treats all GPTF factors as time series. Unlike BCTT with GP prior on the time domain, NONFAT uses GP prior on the frequency domain through inverse Fourier transform of original time series.

Fig. 2 displays the completion results. The learned trajectory of a single tensor entry is plotted. Higher missing rates result in the inability of BCTT to capture accurate trajectories, particularly in missing regions. NONFAT achieves more stable predictions, yet it tends to favor over-smoothed trajectories while disregarding high-frequency fluctuations. This behavior may be attributed to its unique construction, which introduces a GP prior in the frequency domain. Our utilization of flexible neural networks allows us to adapt to complex situations encompassing both low-frequency and high-frequency information.

### Tensor completion

We evaluate our model on two sparse tensor and two dynamic tensor completion applications. For real datasets, the energy function can be difficult to learn, when the pdf has a very sharp curve. Motivated by the idea of noise-perturbed score estimation , we add small i.i.d. Gaussian noises on the data during the training of EnergyTD as a form of smoothing technique. The results are reported on _clean_ test data. For EnergyTD, we use MAP estimates as described in Section 3.1.

#### 5.2.1 Sparse tensor completion

We test our model on two sparsely observed tensors: (1) _\(\)_, a file access log dataset  of shape _200 users \(\) 100 actions \(\) 200 resources_ with about 0.33% nonzero entries; (2) _ACC_, a three-way tensor generated from a code repository management system  of shape _3k users \(\) 150 actions \(\) 30k resources_ with about 0.009% nonzero entries. We use the same dataset split as in  and report the 5-fold cross validation results.

Competing methodsWe compare with five baselines: (1) CP-WOPT , CP decomposition with stochastic optimization; (2) GPTF , a GP-based tensor factorization using stochastic variational inference; (3) HGP-GPTF , a GPTF equipped with hierarchical Gamma process prior; (4) POND , a probabilistic non-linear TD using deep kernels with convolutional NNs (CNNs); (5) CoSTCo , a non-linear TD that uses CNNs to map latent factors to tensor entries. CP-WOPT is provided in Matlab Tensor Toolbox . We implement GPTF based on PyTorch by ourselves and use official implementations for HGP-GPTF2, POND3 and CoSTCo4.

Experimental settings and resultsWe set batch size 1000 and run 1000 epochs for _\(\)_, 100 epochs for _\(\)_. For our model, we use Adam  optimizer. Learning rates of all models are chosen from \(\{1,1,1\}\). For all methods, we evaluate with rank \(R\{3,5,8,10\}\). All methods are evaluated by 5 runs with different random seeds.

    &  &  \\  \(\) & Rank \(3\) & Rank \(5\) & Rank \(8\) & Rank \(10\) & Rank \(3\) & Rank \(5\) & Rank \(8\) & Rank \(10\) \\  CP-WOPT & 1.486 \(\) 0.282 & 1.386 \(\) 0.043 & 1.228 \(\) 0.063 & 1.355 \(\) 0.079 & 0.694 \(\) 0.098 & 0.664 \(\) 0.018 & 0.610 \(\) 0.027 & 0.658 \(\) 0.026 \\ GPTF & 0.911 \(\) 0.008 & 0.867 \(\) 0.008 & 0.878 \(\) 0.009 & 0.884 \(\) 0.009 & 0.511 \(\) 0.005 & 0.494 \(\) 0.004 & 0.530 \(\) 0.004 & 0.554 \(\) 0.006 \\ HGP-GPTF & 0.896 \(\) 0.011 & 0.867 \(\) 0.009 & 0.850 \(\) 0.011 & 0.844 \(\) 0.006 & 0.479 \(\) 0.007 & 0.473 \(\) 0.003 & 0.474 \(\) 0.004 & 0.480 \(\) 0.004 \\ POND & 0.885 \(\) 0.010 & 0.871 \(\) 0.013 & 0.858 \(\) 0.009 & 0.857 \(\) 0.011 & 0.463 \(\) 0.004 & 0.454 \(\) 0.005 & 0.444 \(\) 0.005 & 0.443 \(\) 0.006 \\ CoSTCo & 0.999 \(\) 0.007 & 0.936 \(\) 0.017 & 0.930 \(\) 0.024 & 0.909 \(\) 0.014 & 0.523 \(\) 0.006 & 0.481 \(\) 0.007 & 0.514 \(\) 0.031 & 0.481 \(\) 0.008 \\ EnergyTD & **0.864 \(\) 0.011** & **0.835 \(\) 0.011** & **0.840 \(\) 0.013** & **0.833 \(\) 0.016** & **0.450 \(\) 0.006** & **0.433 \(\) 0.006** & **0.424 \(\) 0.005** & **0.409 \(\) 0.004** \\  _ACC_ & & & & & & & & \\  CP-WOPT & 0.533 \(\) 0.039 & 0.592 \(\) 0.037 & 0.603 \(\) 0.028 & 0.589 \(\) 0.022 & 0.138 \(\) 0.004 & 0.147 \(\) 0.005 & 0.148 \(\) 0.003 & 0.147 \(\) 0.004 \\ GPTF & 0.367 \(\) 0.001 & 0.357 \(\) 0.001 & 0.359 \(\) 0.001 & 0.368 \(\) 0.001 & 0.152 \(\) 0.002 & 0.150 \(\) 0.001 & 0.167 \(\) 0.002 & 0.182 \(\) 0.001 \\ HGP-GPTF & 0.355 \(\) 0.001 & 0.344 \(\) 0.001 & 0.341 \(\) 0.001 & 0.338 \(\) 0.001 & 0.125 \(\) 0.003 & 0.129 \(\) 0.001 & 0.139 \(\) 0.000 & 0.145 \(\) 0.002 \\ CoSTCo & 0.385 \(\) 0.003 & 0.376 \(\) 0.018 & 0.363 \(\) 0.004 & 0.348 \(\) 0.002 & 0.117 \(\) 0.004 & 0.137 \(\) 0.020 & 0.107 \(\) 0.004 & **0.101 \(\) 0.004** \\ EnergyTD & **0.348 \(\) 0.005** & **0.336 \(\) 0.004** & **0.328 \(\) 0.003** & **0.328 \(\) 0.003** & **0.110 \(\) 0.008** & **0.101 \(\) 0.006** & **0.094 \(\) 0.006** & **0.101 \(\) 0.009** \\   

Table 1: Sparse tensor completion 

[MISSING_PAGE_FAIL:9]

Table 2 presents the completion results. It should be noted that the results presented here differ from those reported in  as we adhere to the standard definition of RMSE and MAE (see appendix for detail). Our model surpasses the baseline methods in almost all cases for both RMSE and MAE, with statistically significant improvements (\(p\) < 0.05) observed in most cases. Particularly, we observe that the improvements in MAE are notably more significant. One possible reason is that NONFAT is trained implicitly by minimizing the square loss (with regularization) as it adopts Gaussian assumption about the data. However, our model does not make such assumptions about the data distribution and the loss function. Hence, it can adapt to the data distribution more flexibly. Additionally, we observe that directly injecting time stamps into neural networks, as done in CTNN, is ineffective, thus highlighting the advantage of our model in learning more informative structures.

## 6 Conclusion

We introduce an innovative approach to undirected probabilistic tensor decomposition (TD), characterized by its exceptional flexibility in accommodating various structures and distributions. Specifically, our model integrates deep EBMs in TD to relax both structural and distributional assumptions, enabling it to handle complex real-world applications. To efficiently learn the doubly intractable pdf, we derive a VCNCE objective that is the upper bound of the CNCE loss. Experimental results demonstrate that our model can handle diverse distributions and outperforms baseline methods in multiple real-world applications. One limitation is that our final loss function is not a fully variational upper bound of CNCE, since we have to use importance samples to approximate the pdf of noise samples in Eq. (7). In the future, we aim to derive a fully variational bound as in . Finally, we did not delve into the interpretability of learned factors in this work. However, exploring the interpretability of these factors represents a promising avenue for future research in the realm of tensor decompositions.