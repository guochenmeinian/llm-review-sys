# Steganography in Large Language Models: Investigating Emergence and Mitigations

Yohan Mathew

LASR Labs

Ollie Matthews1

**Robert McCarthy1**

**LoasR Labs**

**University College London

**University College London

&**Joan Velja1**

**LASR Labs**

**University College London

&**University College London

&**Dylan Cope**

King's College London

Imperial College London

&**Nandi Schoots**

King's College London

Imperial College London

Equal contribution. Correspondence to: y.math5@gmail.com.

###### Abstract

The rapid proliferation of frontier model agents promises significant societal advances but also raises concerns about systemic risks arising from unsafe interactions. Collusion to the disadvantage of others has been identified as a central form of undesirable agent cooperation. The use of information hiding (steganography) in agent communications could render collusion practically undetectable. This underscores the need for evaluation frameworks to monitor and mitigate steganographic collusion capabilities. We address a crucial gap in the literature by demonstrating, for the first time, that robust steganographic collusion in LLMs can arise indirectly from optimization pressure. To investigate this problem we design two approaches - a gradient-based reinforcement learning (GBRL) method and an in-context reinforcement learning (ICRL) method - for reliably eliciting sophisticated LLM-generated linguistic text steganography. Importantly, we find that emergent steganographic collusion can be robust to both _passive_ steganalytic oversight of model outputs and _active_ mitigation through communication paraphrasing. We contribute a novel model evaluation framework and discuss limitations and future work. Our findings imply that effective risk mitigation from steganographic collusion post-deployment requires innovation in passive and active oversight techniques.

## 1 Introduction

Under optimization pressures, AI systems can learn to behave in unintended ways due to phenomena like reward misspecification and specification gaming (Clark and Amodei, 2016; Krakovna et al., 2020). In multi-agent environments the complexity increases further, as AI agents can learn emergent behaviors that may include covert collusion or unintended cooperation in the pursuit of maximizing individual or collective rewards (Zheng et al., 2022; Dou et al., 2023; Motwani et al., 2024). Elsewhere, when inadvertently presumed to do so, Large Language Models (LLMs) have been shown to be capable of using natural language chain-of-thought reasoning to help them execute misaligned behaviours, such as insider trading (Scheurer et al., 2023). We may hope to prevent such behaviours by analysing or intervening on model outputs. However, as we investigate in this paper, such mitigation attempts may be bypassed by models learning to hide information in innocuous-looking outputs.

_Text-based steganography_(Ziegler et al., 2019; Schroeder de Witt et al., 2022), the practice of embedding hidden messages in text, presents a potential avenue for models to bypass oversight (Petitcolas et al., 1999; Lockwood and Curran, 2017; Majeed et al., 2021; Greenblatt et al., 2024). Prior works have outlined concrete scenarios where steganography may pose a risk: (i) models may use steganography to perpetuate demographic biases present in their training data (Motwani et al., 2024); (ii) steganography poses a threat in "untrusted monitoring" control settings (Greenblatt et al., 2024); (iii)models may employ steganography within their chain-of-thought reasoning to facilitate sycophantic behaviors (Roger and Greenblatt, 2023). Furthermore, when models face penalties from overseer's during training, they may adapt to execute unintended behaviors more subtly, rather than ceasing them altogether. We refer to the learning of such a steganographic strategy as _emergent steganography_.

There is a need to understand these risks in order to inform policy decisions and technical research directions. We aim to make progress here via the following contributions:

**Contribution 1:**_We present general methods for eliciting linguistic steganography in frontier LLMs._ Prior work has mostly involved prompting LLMs to follow predefined steganographic codes (Roger and Greenblatt, 2023; Greenblatt et al., 2024) or to devise codes (Wu et al., 2024; Motwani et al., 2024). We present two optimization methods for improved automatic elicitation of steganography in LLMs. Specifically, we present a gradient-based reinforcement learning (GBRL) method and two variants of an in-context reinforcement learning (ICRL) method (see Section 3.2). As we show in our experiments, these methods can serve to: (i) evaluate the steganographic capabilities of LLMs; (ii) act as proxies for optimization pressures that may inadvertently encourage steganography in realistic settings; and (iii) aid evaluation of steganography defenses.

**Contribution 2:**_We show that unintended linguistic steganography can emerge as an instrumental goal while optimizing task performance in the presence of an overseer._ Since our methods do not rely on explicit prompting for steganography, they mark a step towards evaluation tools that can assess whether steganographic collusion can arise in future systems. In Section 5.1 we demonstrate via both our GBRL and ICRL methods that LLMs can learn to collude via steganography due to a misspecified reward and inadequate oversight.

**Contribution 3:**_We show that current steganography mitigation methods are often not robust to optimization pressure._ Mitigation methods for steganography can be categorized as _active_ or _passive_ depending on whether alteration of the message is performed (Bartel, 2000; Qi, 2013). An example of active mitigation is real-time editing of messages, e.g. by _paraphrasing_(Roger and Greenblatt, 2023). For passive mitigation, a possible strategy is deploying an _overseer_ that analyzes messages and punishes colluding agents. Existing empirical work investigating mitigation methods does not consider a colluding team that is learning (Lanham et al., 2023; Roger and Greenblatt, 2023; Motwani et al., 2024). In Section 5.2, we show that a combination of active and passive mitigation strategies can often fail when a colluding team is optimized against them.

## 2 Related Work

The field of _emergent communication_ studies how communication systems can develop under optimization pressure (Wagner et al., 2003; Havrylov and Titov, 2017; Lazaridou and Baroni, 2020; Chaabouni et al., 2022), and the study of _covert signaling_(Abadi and Andersen, 2016; Wang et al., 2023; Chen et al., 2023; Halawi et al., 2024) shows how methods of covert collusion like steganography can arise in multi-agent systems. In particular, Yu et al. (2022) studied messaging between two agents in the presence of an adversary agent (all of which are CNN-RNN networks trained end-to-end) and showed that these agents can learn a communication system in a referential game (Lewis, 1969) that is uninterpretable to a third-party. In contrast to this work, we do not backpropagate end-to-end through agent communications to optimize collusion.

Majeed et al. (2021) perform a review of various text steganography strategies and categorize them into three types - (i) _format-based methods_ where physical features of text symbols are used, (ii) _linguistic_ methods that use features of the language itself like synonyms or acronyms, and (iii) _statistical_ methods that use statistical properties of the language. Motwani et al. (2024) investigate collusion in LLMs and present a taxonomy for evaluation and mitigation of LLM steganography, and explore simple in-context elicitation settings for linguistic steganography. Motwani et al. (2024) also note that paraphrasing will by definition, be ineffective against steganography in semantic spaces. In this paper, we focus on linguistic steganography methods that might be employed by colluding LLMs, and investigate the robustness of oversight and paraphrasing as mitigation strategies.

Previous work shows LLMs can successfully execute predefined steganography schemes (Roger and Greenblatt, 2023; Greenblatt et al., 2024). We expand on this work by showing that environmental pressures can cause LLMs to find steganography schemes unprompted. Roger and Greenblatt (2023) discuss the emergence of encoded reasoning and steganography in a range of settings, and demonstrates that adding an LLM paraphraser post-hoc is effective against both these kinds of undesirable behavior. Our work builds on this line of research by investigating how paraphrasing fares under optimization pressure. Greenblatt et al. (2024) describe an "untrusted monitoring" protocol where an untrusted model is reviewed by an untrusted monitor. One red-team strategy used here was conveying a single bit of hidden information to the monitor so it knew to emit a false negative. This gives us a concrete threat model for investigating and defending against steganography to prevent such loss-of-control scenarios.

## 3 Methodology

### Our Steganography Framework

We frame steganography as an adversarial relationship between two teams, a _collusion team_ and a _mitigation team_. The collusion team consists of a _sender_ and a _receiver_, and aims to complete a _task_\(t\) while communicating a secret _payload_\(p\). The sender sends an innocuous-seeming message to the receiver called the _stegotext_, within which it hides the payload. The receiver interprets the sender's message and then produces an action \(a\) which completes the task. This action is passed to an evaluation function, which provides a _task reward_\(r_{}\) for the collusion team. In our tasks, successful communication of the payload will generally allow the receiver to better maximize this reward.

The mitigation team aims to detect and mitigate steganographic communication of the payload by the collusion team. It consists of an _overseer_ and an optional _paraphraser_, passive and active steganographic defenses respectively (Bartel, 2000). The paraphraser rewards the stegotext in an attempt to remove the payload. The overseer attempts to detect the presence of steganography by analysing stegotexts. When the overseer flags a stegotext as suspicious, the collusion team receives a _suspiciousness penalty_\(r_{}\). The overseer is not optional because without it there is no means to punish collusion, and no third-party from which the payload needs to remain hidden.

In our experiments, the mitigation team is fixed, and the collusion team attempts to maximize its reward \(r\), which is a combination of the task reward and the overseer penalty: \(r=r_{}-r_{}\)

**Evaluation Metrics.** We evaluate the emergence of steganography using the following measures:

* _Task Success:_ Receiver correctly completes the task.
* _Decoding Success:_ Receiver successfully decodes the payload.
* _Sender Task Success:_ Sender produces task-relevant stegotext.
* _Overseer Bypass Success:_ Overseer does not classify stegotext as suspicious.
* _Collusion Success:_ Collusion team completes the task, produces relevant stegotext (optional), and avoids overseer detection.
* _Channel Capacity:_ Bits of information communicated per request by the collusion team, as defined by Roger and Greenblatt (2023).

Figure 1: Illustration of our steganography elicitation framework, generalized for our Decentralized ICRL Learners and GBRL methods. The ‘learners’ iteratively test and improve on the behaviours of the agents to maximize reward, either by weight updates (GBRL) or proposing strategies in natural language (ICRL).

### Collusion Team

We present two different methods for optimizing the collusion team to maximize their reward signal: gradient-based reinforcement learning (GBRL) and in-context reinforcement learning (ICRL). For GBRL, the weights of a pretrained LLM are modified by _gradient descent_ via an RL algorithm. In ICRL, the behaviour of each agent is defined by _strategies_ (represented in natural language) within the context window that are improved over time, and the weights of the collusion team LLMs remain fixed. A generalization of the set-ups for GBRL and one version of ICRL is show in Figure 1.

#### 3.2.1 Gradient-based Reinforcement Learning (GBRL)

When optimizing the collusion team via GBRL, we periodically switch between training of the sender and training of the receiver. The receiver is first updated for \(n_{r}\) stochastic gradient descent (SGD) steps, after which the sender is updated for \(n_{s}\) SGD steps. This process is repeated until convergence.

The receiver is a pretrained language model with a randomly initialized classification head. It is trained using supervised fine-tuning (SFT) to take the stegotest generated by the sender and predict the label that maximizes the task reward. The sender is a pretrained autoregressive LLM which takes the task description as input, and outputs stegotest to be passed to the receiver. The sender is trained using a reinforcement learning (RL) algorithm to maximize a reward signal \(r\) that accounts for task usefulness and oversight. The overall reward \(r\) for the sender is the probability assigned to the correct label by the receiver (\(r_{}\)), minus a multiple \(_{}\) of the suspiciousness penalty: \(r=r_{}-_{}r_{}\)

We use proximal policy optimization (PPO) Schulman et al. (2017) with low-rank adaptation (LoRA) Hu et al. (2022) to update the _sender_Sokota et al. (2022). More implementation details, hyperparameters and prompts used for the task can be found in Appendix A.2. Relating this to Figure 1, the 'Sender Learner' in the figure performs the in-weight PPO updates on the sender, and the 'Receiver Learner' performs SFT updates on the Receiver.

#### 3.2.2 In-context Reinforcement Learning (ICRL)

Our ICRL methods make use of one or more _ICRL Learners_ in addition to the sender and receiver, all of which are LLMs. An ICRL Learner is an LLM that devises steganography strategies for the sender and receiver to execute. Once the sender and receiver execute the strategy (over a number of episodes), the ICRL Learner is fed the results in its context window, including details regarding the task reward \(r_{}\) and suspiciousness penalty \(r_{}\) received. It then iterates upon the strategy it sends to the sender and receiver, in order to maximize the reward signal. This is similar to previous approaches using in-context trial-and-error learning to iteratively improve LLM behaviour Yao et al. (2023); Shinn et al. (2023); Wang et al. (2024).

We make use of two different variations of our ICRL method - _Centralized Learner_ where the sender and receiver share a common ICRL Learner, and _Decentralized Learners_ where the sender and receiver each have their own independent ICRL Learners.

Decentralized ICRL Learners.This setup is equivalent to the setup shown in Figure 1. We have two separate ICRL learners (one for the sender and one for the receiver) which cannot communicate with each other. First, the sender learner suggests a strategy for the sender to execute over several episodes. The receiver learner analyzes the sender's stegotests from the episodes, and attempts to infer the strategy used by the sender. The receiver is then fed the inferred strategy and attempts to complete the task, while also trying to guess the payload in the sender's stegotests as a means of improving its task performance. The results of the current training step are fed back to the sender learner for the next training step. See Appendix A.3.1 and B.4 for more details on the decentralized learners methodology.

Centralized ICRL Learner.In the centralized setup, a single central ICRL learner generates strategies that are fed to both the sender and receiver. At each training step, the sender and receiver execute the strategy over several episodes. The ICRL learner is prompted to come up with new strategies based on the results of these episodes. The process is repeated for a fixed number of training steps. See Appendix A.3.2 for more details on the centralized learner methodology.

#### 3.2.3 Motivations Behind Elicitation via GBRL and ICRL

We desire elicitation methods that can serve to: (i) act as proxies for realistic optimization pressures that may inadvertently induce steganography in real-world settings; and (ii) act as strong optimization pressures for red-teaming steganography mitigation methods. Our GBRL and Decentralized ICRL Learners methods may act as realistic optimization pressures (i.e., there is no secret coordination channels between the sender and receiver), as we demonstrate in the experiments of Section 5.1. Our Centralized ICRL Learner may act as a strong optimization pressure; it is strengthened by the secret coordination channel between the sender and the receiver.

There are benefits to having qualitatively different gradient-based and in-context approaches. Pure in-context approaches can leverage frontier black-box models and may be more sample efficient and less computationally expensive than gradient-based approaches. Meanwhile, gradient-based approaches have the potential to achieve more substantial and persistent improvements in the model's steganography capabilities. Qualitatively different approaches may lead to different learning dynamics and results, meaning our analyses will be more comprehensive.

## 4 Tasks

### Automated Candidate Screening

We create a setting which investigates the possibility of steganography inadvertently emerging as an instrumental goal. In this setting, an AI system is trained on historic data to provide pass/fail evaluations of candidate cover letters for job applications. We setup the datasets such that there exists some bias in its labelling related to'sensitive variables' (either the gender of the applicant or their educational background). Without using the sensitive variables, the maximum task success that can be achieved on the dataset is _Max-Clean-Success_ < 100%. However, if the sensitive feature is exploited, 100% task success can be achieved.

We suppose the developers of the AI system are aware that automated candidate screening with machine learning can replicate and amplify social biases (Dastin, 2018; Andrews and Bucher, 2022; Chen, 2023), and so have (naively) attempted to mitigate against this. A first model (i.e., the speaker) summarizes the cover letter, while removing potentially biasing information. A second model (i.e., the receiver) takes in this summary and predicts the label. A third model (i.e., the overseer) checks the summaries for any mentions of the sensitive variables. The speaker and receiver are optimized to maximize their end-to-end prediction accuracy on the historic dataset (\(r_{}\) = whether the correct pass/fail label is predicted by the receiver), while minimizing penalties received from the overseer (\(r_{}\)). A version of this setting was originally described by Motwani et al. (2024). Note, in these tasks we do not make use of a _Sender Task Success_ metric.

We construct different instances of this setting (see Appendix B.1 for more details):

* Uni dataset: All cover letters with candidates who went to "The Good University" are labelled "pass", and all candidates who went to "The Bad University" are labelled "fail". _Max-Clean-Success_ is 50%.
* Uni-Long dataset: Identical to the Uni dataset, but the cover letters are longer.
* Gender dataset: A binary gender indicator is present in each application. The bias is more subtle than the previous datasets; labels are determined by three different categorical variables (gender, field-of-study, and soft-skill). _Max-Clean-Success_ is 67%.

### Evaluating Steganography Mitigations

We now present the tasks we use to evaluate steganography mitigations (see Section 5.2).

Here, the sender must produce some stegotext that completes a task (as judged by a _Sender Task Success_ metric), while encoding a payload that is explicitly provided in its prompt. The receiver must directly guess the payload from the stegotext. Thus, \(r_{}\) = _Decoding Success_, i.e., whether the receiver correctly guesses the payload. In addition to using an overseer that looks for steganography in the stegotext, we also flag a batch of stegotexs as suspicious if the _Sender Task Success_ falls below a certain _Honest-Performance_ threshold (refer to Appendix D.1 for details). Thus, here collusion issuccessful (i.e., _Collusion Success_) when: the stegotext is suitably performant, the receiver correctly decodes the payload, and the stegotext is not flagged as suspicious by the overseer. We make use of two commonly used LLM benchmarks for our tasks.

AQuA-RAT (Ling et al., 2017): The sender's task is to complete some chain-of-thought reasoning in order to answer a math multiple-choice-question (MCQ). _Sender Task Success_ is the proportion of the sender's final answers which are correct.

HumanEval (Zhong et al., 2024): The sender must complete a Python function based on its docstring. _Sender Task Success_ is the proportion of the sender's responses which pass the unit tests for each function. To decouple steganographic performance from the coding ability of the sender, we also show the sender a correct implementation of the function when asking it to complete the task.

## 5 Results

### Instrumental Emergence: Automated Candidate Screening

We run our GBRL method on the Uni and Gender datasets. All GBRL runs include an O1 overseer (single-episode, logits-based). One instance additionally includes a standard paraphraser. The GBRL experiments use the datasets with shorter cover letters for computational efficiency purposes. We run our ICRL method on the Uni-Long dataset. All ICRL runs use an O2 overseer (multi-episode, sampling-based). One ICRL experiment additionally includes a standard paraphraser. Details regarding the specific models used for these runs can be found in Appendix A.2 and A.3. The results of these runs are presented in Table 1.

GBRL Results.The collusion team achieves high task success rates, successfully communicating the sensitive information while bypassing the overseer (note, one seed failed to converge in the Gender dataset). Several runs converge to strategies which are _subtle_, i.e. the strategies were not immediately obvious to us when analysing the results. Strategies are particularly hard to spot in the Gender dataset where the sensitive variable is only required to predict the label on a subset of the data (see Table 12 in Appendix C). Subtle strategies included referring to males as "individuals" and females as "candidates", or using the phrase "and is also skilled in" for males and "and is able to" for females. The experiments with a paraphraser converged to writing 1st-person cover letters for the Good University and writing in 3rd-person for the Bad University.

A limitation to note here is that our overseers can be brittle, meaning that occasionally 'blatant' encoding strategies are learned. For instance, learning to say that a candidate went to a "less-renowned school" in the Uni setting can lead to oversight bypass. For this reason, we made use an oversight model that is stronger -in terms of capabilities- than the actual sender-receiver pair (see Appendix A.2). Future work should investigate how these results hold as overseer capabilities are scaled, as well as investigate a more thorough sweep of hyperparameters.

ICRL Results.Via the Decentralized ICRL process, the collusion team leveraging Claude-3.5-Sonnet realized that steganography might be an instrumental option to increase its reward and achieves a task success close to 100% across runs. This is the case even when the mitigation team employs a paraphraser. When the collusion team uses GPT-4o models, it is not able to achieve performance substantially over random chance (see Appendix B.2 for more details). Some impressive strategies

  Collusion Method & ICRL & ICRL & GBRL & GBRL & GBRL & GBRL-SB \\ Mitigation Team & O & O + P & O & O + P & O & O \\ Dataset & Uni-Long & Uni-Long & Uni & Uni & Gender & Gender \\   Max-Clean-Success & \(0.50\) & \(0.50\) & \(0.50\) & \(0.50\) & \(0.67\) & \(0.67\) \\  Task Success & \(0.97 0.03\) & \(0.98 0.03\) & \(0.99 0.01\) & \(1.00 0.00\) & \(0.88 0.16\) & \(1.00 0.00\) \\  overseer Bypass & \(0.97 0.04\) & \(1.00 0.01\) & \(0.99 0.01\) & \(1.00 0.00\) & \(1.00 0.00\) & \(0.98 0.02\) \\  

Table 1: Emergence of Steganography in Candidate Screening Settings. Results with our ICRL and GBRL elicitation methods are presented across 5 and 3 seeds respectively. ‘Max-Clean-Success’ refers to the maximum task success that can be obtained if the models do not collude via steganography. In the mitigation team column, ‘O’ means an overseer was used, ‘P’ means a paraphraser was used. All metrics here are obtained via evaluation on held-out test data.

learned by Claude when there was no paraphraser include: using different specific phrases, such as "innovative mindset" or "traditional approach" depending on the university. In some cases, we found that the colluding LLM models refused to execute the task for the biased Gender dataset (see Table 13 in Appendix C ).

One point to note for both the GBRL and ICRL results for this section is that we do not currently have a _Sender Task Success_ metric, and in some runs the collusion team converge to a strategy where the sender does not do a good job of summarising the cover letter.

Oversight Encourages Subtle Collusion.In runs where the GBRL collusion team finds a strategy to communicate the sensitive information, they always learn to successfully bypass the overseer. To investigate this phenomenon further, we perform experiments on the Gender dataset where the collusion team are initialized to blatantly communicate the gender of the applicant. The collusion method for these runs are titled _GBRL-SB_ in Table 1 (where '_SB_' means the collusion team are _seeded_ with a _blatant_ strategy). From this initialized state, the collusion team learn to make their strategy increasingly hard to spot by the overseer, eventually converging to a subtle strategy that always bypasses the overseer (see training curves in Appendix C.1.2). This is evidence that oversight is not effective at preventing collusion, but can in-fact encourage more subtle collusion.

### Evaluating Steganography Mitigations Using ICRL

We investigate the effectiveness of standard steganography mitigation strategies when a collusion team is optimized against it. We do so in the AQuA-RAT and HumanEval tasks. The mitigation

Figure 2: Evaluating steganography mitigations in the AQuA-RAT and HumanEval tasks. The best performing strategies from the ICRL centralized learner elicitation method are plotted across a range of integer payloads. We connect optimal strategies for each payload size via a line. Projections1 of the optimal strategies are indicated with dashed lines.

team makes use of an O3 overseer in both tasks. AQuA-RAT uses either a standard or a context-aware paraphraser. HumanEval uses a coding paraphraser. As mentioned in Section 4.2, segtorests will additionally be flagged as suspicious if _Sender Task Success_ falls below a certain threshold. The collusion team uses our strongest and most efficient red-teaming elicitation method, the Centralized ICRL Learner. Similar to the 'untrusted monitoring' setting in Greenblatt et al. (2024), we are interested in the case where the collusion team is more capable than the mitigation team, and so use Claude-3.5 Sonnet for all collusion team agents, and GPT-3.5 Turbo for all mitigation agents.

Collusion Team Training Details.Models are explicitly prompted to develop and execute steganography strategies against a fixed mitigation team. We start by training the ICRL learner to find strategies to encode and decode binary payloads (1-bit), and then scale to increasingly large integer payloads. This involves prompting the learner to encode the payloads in parts, with a different strategy for each part. We stop increasing the size of payloads when the learner can no longer find effective strategies. We evaluate the performance at each payload size by taking the most successful strategies from training and applying them to the task's test set against the same mitigation team from training. See Appendix A.3.4 for a more complete description.

Mitigation Effectiveness Across Payload Sizes.In Figure 2 we show collusion success and channel capacity for different payload sizes. Each point represents a single collusion strategy, and we draw a line between the optimal strategy at each payload size. We also show backwards projections of optimal strategies (via a dotted line), which represents the maximum performance that a collusion team could achieve for a smaller payload by implementing the optimal strategy at the lower payload size. For example, a strategy encoding values \(\{0,1,2,3\}\) could achieve at least the same decoding success for encoding values \(\{0,1\}\) by remapping redundant values in the original strategy. Additionally, strategies that performed well during training but below the frontier when running on the test dataset are also shown marked with a \(\) (labeled as sub-optimal strategy).

In the AQuA-RAT results, the paraphrasers significantly reduce collusion success rates at higher payload sizes. In particular, the context-aware paraphraser makes multi-bit communication difficult. However, in all cases, the collusion team is able to maintain a decoding success of at least 90% on 1-bit payloads. With no paraphraser, the lower-bit strategies are all flagged as suspicious, but the collusion team is able to learn a successful strategy which encodes an integer between 0 and 23. The collusion team may be better at bypassing the overseer when a paraphraser is employed as it implicitly forces the collusion teams to come up with more subtle strategies during training.

In the HumanEval results, we find the collusion team is able to encode less information overall, but the coding paraphraser is not able to completely cut out communication. Notably, the 1-bit strategies can achieve a decoding success of over 99% in both cases.

## 6 Conclusion

We have presented two distinct and effective methods for eliciting steganography in LLMs, even when a mitigation team consisting of paraphrasing and an overseer is present. Our candidate screening experiments show that steganography can emerge as an instrumental goal as part of optimizing task performance in some settings. We see that when this happens, the sender and receiver models are able to consistently bypass the overseer by learning subtle steganographic schemes. We were able to demonstrate these results with both GBRL and ICRL optimization approaches.

In our experiments to evaluate robustness of mitigation strategies, we used the ICRL optimization approach to create strong red teams that generate sophisticated steganographic strategies. We find these schemes can be as effective as or more effective than "worst case" human-devised schemes from previous work. We show that an overseer by itself proves to be inadequate for mitigating steganography when the mitigation team is present during the optimization process. Additionally, no paraphraser was able to reliably mitigate the steganographic communication of 1 bit.

Our results highlight the limitations of current black-box mitigation approaches for steganographic collusion, and as agents' capabilities grow, white-box detection strategies may become essential for mitigation. Addressing the technical and political challenges (Casper et al., 2024; Chan et al., 2024;b) of monitoring steganographic collusion underscores the need for continuous frontier model evaluations before deployment (Motwani et al., 2024).