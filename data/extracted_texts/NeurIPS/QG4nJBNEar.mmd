# CAr-Walk: Inductive Hypergraph Learning via Set Walks

Ali Behrouz

Department of Computer Science

University of British Columbia

alibez@cs.ubc.ca

&Farnoosh Hashemi

Department of Computer Science

University of British Columbia

farsh@cs.ubc.ca

&Sadaf Sadeghian

Department of Computer Science

University of British Columbia

sadafsdn@cs.ubc.ca

&Margo Seltzer

Department of Computer Science

University of British Columbia

mseltzer@cs.ubc.ca

These two authors contributed equally (ordered alphabetically) and reserve the right to swap their order.

###### Abstract

Temporal hypergraphs provide a powerful paradigm for modeling time-dependent, higher-order interactions in complex systems. Representation learning for hypergraphs is essential for extracting patterns of the higher-order interactions that are critically important in real-world problems in social network analysis, neuroscience, finance, etc. However, existing methods are typically designed only for specific tasks or static hypergraphs. We present CAr-Walk, an inductive method that learns the underlying dynamic laws that govern the temporal and structural processes underlying a temporal hypergraph. CAr-Walk introduces a temporal, higher-order walk on hypergraphs, SetWalk, that extracts higher-order causal patterns. CAr-Walk uses a novel adaptive and permutation invariant pooling strategy, SetMixer, along with a set-based anonymization process that hides the identity of hyperedges. Finally, we present a simple yet effective neural network model to encode hyperedges. Our evaluation on 10 hypergraph benchmark datasets shows that CAr-Walk attains outstanding performance on temporal hyperedge prediction benchmarks in both inductive and transductive settings. It also shows competitive performance with state-of-the-art methods for node classification. (Code)

## 1 Introduction

Temporal networks have become increasingly popular for modeling interactions among entities in dynamic systems . While most existing work focuses on pairwise interactions between entities, many real-world complex systems exhibit natural relationships among multiple entities . Hypergraphs provide a natural extension to graphs by allowing an edge to connect any number of vertices, making them capable of representing higher-order structures in data. Representation learning on (temporal) hypergraphs has been recognized as an important machine learning problem and has become the cornerstone behind a wealth of high-impact applications in computer vision , biology , social networks , and neuroscience .

Many recent attempts to design representation learning methods for hypergraphs are equivalent to applying Graph Neural Networks (GNNs) to the clique-expansion (CE) of a hypergraph . CE is a straightforward way to generalize graph algorithms to hypergraphs by replacing hyperedges with (weighted) cliques . However, this decomposition of hyperedges limits expressiveness,leading to suboptimal performance  (see Theorem 1 and Theorem 4). New methods that encode hypergraphs directly partially address this issue . However, these methods suffer from some combination of the following three limitations: they are designed for 1 learning the structural properties of _static hypergraphs_ and do not consider temporal properties, 2 the transductive setting, limiting their performance on unseen patterns and data, and 3 a specific downstream task (e.g., node classification , hyperedge prediction , or subgraph classification ) and cannot easily be extended to other downstream tasks, limiting their application.

Temporal motif-aware and neighborhood-aware methods have been developed to capture complex patterns in data . However, counting temporal motifs in large networks is time-consuming and non-parallelizable, limiting the scalability of these methods. To this end, several recent studies suggest using temporal random walks to automatically retrieve such motifs . One possible solution to capturing underlying temporal and higher-order structure is to extend the concept of a hypergraph random walk  to its temporal counterpart by letting the walker walk over time. However, existing definitions of random walks on hypergraphs offer limited expressivity and sometimes degenerate to simple walks on the CE of the hypergraph  (see Appendix C). There are two reasons for this: 1 Random walks are composed of a sequence of _pair-wise_ interconnected vertices, even though edges in a hypergraph connect _sets_ of vertices. Decomposing them into sequences of simple pair-wise interactions loses the semantic meaning of the hyperedges (see Theorem 4). 2 A sampling probability of a walk on a hypergraph must be different from its sampling probability on the CE of the hypergraph . However, Chitra and Raphael  shows that each definition of the random walk with edge-independent sampling probability of nodes is equivalent to random walks on a weighted CE of the hypergraph. Existing studies on random walks on hypergraphs ignore 1 and focus on 2 to distinguish the walks on simple graphs and hypergraphs. However, as we show in Table 2, 1 is equally important, if not more so.

For example, Figure 1 shows the procedure of existing walk-based machine learning methods on a temporal hypergraph. The neural networks in the model take as input only sampled walks. However, the output of the hypergraph walk  and simple walk on the CE graph are the same. This means that the neural network cannot distinguish between pair-wise and higher-order interactions.

We present Causal Anonymous Set Walks (CAr-Walk), an inductive hyperedge learning method. We introduce a hyperedge-centric random walk on hypergraphs, called SetWalk, that automatically extracts temporal, higher-order motifs. The hyperedge-centric approach enables SetWalks to distinguish multi-way connections from their corresponding CEs (see Figure 1, Figure 2, and Theorem 1). We use temporal hypergraph motifs that reflect network dynamics (Figure 2) to enable CAr-Walk to work well in the inductive setting. To make the model agnostic to the hyperedge identities of these motifs, we use two-step, set-based anonymization: 1 Hide node identities by assigning them new positional encodings based on the number of times that they appear at a specific position in a set of sampled SetWalks, and 2 Hide hyperedge identities by combining the positional encodings of the nodes comprising the hyperedge using a novel permutation invariant pooling strategy, called SetMixer. We incorporate a neural encoding method that samples a few SetWalks starting from nodes of interest. It encodes and aggregates them via MLP-Mixer and our new pooling strategy SetMixer, respectively, to predict temporal, higher-order interactions. Finally, we discuss how to extend CAr-Walk for node classification. Figure 3 shows the schematic of the CAr-Walk.

We theoretically and experimentally discuss the effectiveness of CAr-Walk and each of its components. More specifically, we prove that SetWalks are more expressive than existing random walk algorithms on hypergraphs. We demonstrate SetMixer's efficacy as a permutation invariant pooling strategy for hypergraphs and prove that using it in our anonymization process makes that process more expressive than existing anonymization processes [33; 45; 46] when applied to the CE of the hypergraphs. To the best of our knowledge, we report the most extensive experiments in the hypergraph learning literature pertaining to unsupervised hyperedge prediction with 10 datasets and eight baselines. Results show that our method produces 9% and 17% average improvement in transductive and inductive settings, outperforming all state-of-the-art baselines in the hyperedge prediction task. Also, CAr-Walk achieves the best or on-par performance on dynamic node classification tasks. All proofs appear in the Appendix.

## 2 Related Work

Temporal graph learning is an active research area [5; 47]. A major group of methods uses Gnns to learn node encodings and Recurrent Neural Networks (Rnns) to update these encodings over time [48; 49; 50; 51; 52; 53; 54; 55]. More sophisticated methods based on anonymous temporal random walks [33; 34], line graphs , GraphMixer , neighborhood representation , and subgraph sketching  are designed to capture complex structures in vertex neighborhoods. Although these methods show promising results in a variety of tasks, they are fundamentally limited in that they are designed for _pair-wise_ interaction among vertices and not the higher-order interactions in hypergraphs.

Representation learning on hypergraphs addresses this problem [17; 60]. We group work in this area into three overlapping categories:

1**Clique and Star Expansion**: CE-based methods replace hyperedges with (weighted) cliques and apply Gnns, sometimes with sophisticated propagation rules [21; 25], degree normalization, and nonlinear hyperedge weights [17; 18; 19; 20; 21; 39; 61; 62]. Although these methods are simple, it is well-known that CE causes undesired losses in learning performance, specifically when relationships within an incomplete subset of a hyperedge do not exist [6; 22; 23; 24]. Star expansion (SE) methods first use hypergraph star expansion and model the hypergraph as a bipartite graph, where one set of vertices represents nodes and the other represents hyperedges [25; 63; 64; 28]. Next, they apply modified heterogeneous GNNs, possibly with dual attention mechanisms from nodes to hyperedges and vice versa [25; 27]. Although this group does not cause as large a distortion as CE, they are neither memory nor computationally efficient. 2**Message Passing**: Most existing hypergraph learning methods, use message passing over hypergraphs [17; 18; 19; 20; 21; 25; 26; 27; 39; 61; 62; 65; 66]. Recently, Chien et al.  and Huang and Yang  designed universal message-passing frameworks that include propagation rules of most previous methods (e.g., [17; 19]). The main drawback of these two frameworks is that they are limited to node classification tasks and do not easily generalize to other tasks. 3**Walk-**based**: random walks are a common approach to extracting graph information for machine learning algorithms [32; 33; 34; 67; 68]. Several walk-based hypergraph learning methods are designed for a wide array of applications [69; 70; 71; 43; 68; 72; 73; 74; 75; 76; 77; 78]. However, most existing methods use simple random walks on the CE of the hypergraph (e.g., [26; 78; 43]). More complicated random walks on hypergraphs address this limitation [40; 41; 42; 79]. Although some of these studies show that their walk's transition matrix

Figure 3: **Schematic of the CAr-Walk.** CAr-Walk consists of three stages: (1) Causality Extraction via Set Walks (§3.2), (2) Set-based Anonymization (§3.3), and (3) Set Walk Encoding (§3.4).

[MISSING_PAGE_FAIL:4]

_where \(e_{i}\), \(t_{e_{i+1}}<t_{e_{i}}\), and the intersection of \(e_{i}\) and \(e_{i+1}\) is not empty, \(e_{i} e_{i+1}\). In other words, for each \(1 i-1\): \(e_{i+1}^{i}(e_{i})\). We use \([i]\) to denote the \(i\)-th hyperedge-time pair in the SetWalk. That is, \([i]=e_{i}\) and \([i]=t_{e_{i}}\)._

**Example 2**.: _Figure 1 illustrates a temporal hypergraph with two sampled SetWalks, hypergraph random walks, and simple walks. As an example, \((\{A,B,C\},t_{6})(\{C,D,E,F\},t_{5})\) is a SetWalk that starts from hyperedge \(e\) including \(\{A,B,C\}\) in time \(t_{6}\), backtracks overtime and then samples hyperedge \(e^{}\) including \(\{C,D,E,F\}\) in time \(t_{5}<t_{6}\). While this higher-order random walk with length two provides information about all \(\{A,B,C,D,E,F\}\), its simple hypergraph walk counterpart, i.e. \(A C E\), provides information about only three nodes._

Next, we formally discuss the power of SetWalks. The proof of the theorem can be found in Appendix E.1.

**Theorem 1**.: _A random SetWalk is equivalent to neither the hypergraph random walk, the random walk on the CE graph, nor the random walk on the SE graph. Also, for a finite number of samples of each, SetWalk is more expressive than existing walks._

In Figure 1, SetWalks capture higher-order interactions and distinguish the two nodes \(A\) and \(H\), which are indistinguishable via hypergraph random walks and graph random walks in the CE graph. We present a more detailed discussion and comparison with previous definitions of random walks on hypergraphs in Appendix C.

**Causality Extraction.** We introduce a sampling method to allow SetWalks to extract temporal higher-order motifs that capture causal relationships by backtracking over time and sampling adjacent hyperedges. As discussed in previous studies [33; 34], more recent connections are usually more informative than older connections. Inspired by Wang et al. , we use a hyperparameter \( 0\) to sample a hyperedge \(e\) with probability proportional to \(((t-t_{p}))\), where \(t\) and \(t_{p}\) are the timestamps of \(e\) and the previously sampled hyperedge in the SetWalk, respectively. Additionally, we want to bias sampling towards pairs of adjacent hyperedges that have a greater number of common nodes to capture higher-order motifs. However, as discussed in previous studies, the importance of each node for each hyperedge can be different [25; 27; 40; 65]. Accordingly, the transferring probability from hyperedge \(e_{i}\) to its adjacent hyperedge \(e_{j}\) depends on the importance of the nodes that they share. We address this via a temporal SetWalk sampling process with _hyperedge-dependent node weights_. Given a temporal hypergraph \(=(,,)\), a hyperedge-dependent node-weight function \(:^{ 0}\), and a previously sampled hyperedge \((e_{p},t_{p})\), we sample a hyperedge \((e,t)\) with probability:

\[[(e,t)|(e_{p},t_{p})]))} {_{(e^{},t^{})^{}(e_{p})}( (t^{}-t_{p}))}))} {_{(e^{},t^{})^{}(e_{p})}( (e^{},e_{p}))},\] (1)

where \((e,e^{})=_{ue e^{}}(u,e)(u,e^{ })\), representing the assigned weight to \(e e^{}\). We refer to the first and second terms as _temporal bias_ and _structural bias_, respectively.

The pseudocode of our SetWalk sampling algorithm and its complexity analysis are in Appendix D. We also discuss this hyperedge-dependent sampling procedure and how it is provably more expressive than existing hypergraph random walks in Appendix C.

Given a (potential) hyperedge \(e_{0}=\{u_{1},u_{2},,u_{k}\}\) and a time \(t_{0}\), we say a SetWalk, Sw, starts from \(u_{i}\) if \(u_{i}\). We use the above procedure to generate \(M\) SetWalks with length \(m\) starting from each \(u_{i} e_{0}\). We use \((u_{i})\) to store SetWalks that start from \(u_{i}\).

### Set-based Anonymization of Hyperedges

In the anonymization process, we replace hyperedge identities with position encodings, capturing structural information while maintaining the inductiveness of the method. Micali and Zhu  studied Anonymous Walks (AWs), which replace a _node's identity_ by the order of its appearance in each walk. The main limitation of AWs is that the position encoding of each node depends only on its specific walk, missing the dependency and correlation of different sampled walks . To mitigate this drawback, Wang et al.  suggest replacing node identities with the hitting counts of the nodes based on a set of sampled walks. In addition to the fact that this method is designed for walks on simple graphs, there are two main challenges to adopting it for SetWalks: 1 SetWalksare a sequence of hyperedges, so we need an encoding for the position of hyperedges. Natural attempts to replace hyperedges' identity with the hitting counts of the hyperedges based on a set of sampled \(\), misses the similarity of hyperedges with many of the same nodes. 2 Each hyperedge is a set of vertices and natural attempts to encode its nodes' positions and aggregate them to obtain a position encoding of the hyperedge requires a permutation invariant pooling strategy. This pooling strategy also requires consideration of the higher-order dependencies between obtained nodes' position encodings to take advantage of higher-order interactions (see Theorem 2). To address these challenges we present a set-based anonymization process for \(\)s. Given a hyperedge \(e_{0}=\{u_{1},,u_{k}\}\), let \(w_{0}\) be any node in \(e_{0}\). For each node \(w\) that appears on at least one \(\) in \(_{i=1}^{k}(u_{i})\), we assign a relative, node-dependent node identity, \((w,(w_{0}))^{m}\), as follows:

\[(w,(w_{0}))[i]=|\{|(w_{0}),w|i|\}|\;\; i\{1,2,,m\}.\] (2)

For each node \(w\) we further define \((w,e_{0})=\{(w,(u_{i}))\}_{i=1}^{k}\). Let \(_{()}:^{d d_{1}}^{d_{2}}\) be a pooling function that gets a set of \(d_{1}\)-dimensional vectors and aggregates them to a \(d_{2}\)-dimensional vector. Given two instances of this pooling function, \(_{()}\) and \(_{2}()\), for each hyperedge \(e=\{w_{1},w_{2},,w_{k}\}\) that appears on at least one \(\) in \(_{i=1}^{k}(u_{i})\), we assign a relative hyperedge identity as:

\[(e,e_{0})=_{1}(\{_{2}((w_{i},e_{0}))\}_{i=1}^{k^{}}).\] (3)

That is, for each node \(w_{i} e\) we first aggregate its relative node-dependent identities (i.e., \((w_{i},(u_{j}))\)) to obtain the relative hyperedge-dependent identity. Then we aggregate these hyperedge-dependent identities for all nodes in \(e\). Since the size of hyperedges can vary, we zero-pad to a fixed length. Note that this zero padding is important to capture the size of the hyperedge. The hyperedge with more zero-padded dimensions has fewer nodes.

This process addresses the first challenge and encodes the position of hyperedges. Unfortunately, many simple and known pooling strategies (e.g., \((.)\), \((.)\), \((.)\), etc.) can cause missing information when applied to hypergraphs. We formalize this in the following theorem:

**Theorem 2**.: _Given an arbitrary positive integer \(k^{+}\), let \(_{()}\) be a pooling function such that for any set \(S=\{w_{1},,w_{d}\}\):_

\[(S)=_{S^{} S\\ |v|+d}f(S^{}),\] (4)

_where \(f\) is some function. Then the pooling function can cause missing information, meaning that it limits the expressiveness of the method to applying to the projected graph of the hypergraph._

While simple concatenation does not suffer from this undesirable property, it is not permutation invariant. To overcome these challenges, we design an all-MLP permutation invariant pooling function, \(\), that not only captures higher-order dependencies of set elements but also captures dependencies across the number of times that a node appears at a certain position in \(\)s.

**SetMixer.** MLP-Mixer is a family of models based on multi-layer perceptrons, widely used in the computer vision community, that are simple, amenable to efficient implementation, and robust to over-squashing and long-term dependencies (unlike Rnns and attention mechanisms) . However, the token-mixer phase of these methods is sensitive to the order of the input (see Appendix A). To address this limitation, inspired by MLP-Mixer, we design \(\) as follows: Let \(S=\{_{1},,_{d}\}\), where \(_{i}^{d_{1}}\), be the input set and \(=[_{1},,_{d}]^{d d_{1}}\) be its matrix representation:

\[(S)=(_{}+((_{})_{s}^{(1)}) _{s}^{(2)}),\] (5)

where

\[_{}=+(( ()^{T}))^{T}.\] (6)

Here, \(_{s}^{(1)}\) and \(_{s}^{(2)}\) are learnable parameters, \((.)\) is an activation function (we use \(\) in our experiments), and \(\) is layer normalization . Equation 5 is the channel mixer and Equation 6 is the token mixer. The main intuition of \(\) is to use the \((.)\) function to bind token-wise information in a non-parametric manner, avoiding permutation variant operations in the token mixer. We formally prove the following theorem in Appendix E.3.

**Theorem 3**.: \(\) _is permutation invariant and is a universal approximator of invariant multi-set functions. That is, \(\) can approximate any invariant multi-set function._Based on the above theorem, SetMixer can overcome the challenges we discussed earlier as it is permutation invariant. Also, it is a universal approximator of multi-set functions, which shows its power to learn any arbitrary function. Accordingly, in our anonymization process, we use \((.)=(.)\) in Equation 3 to hide hyperedge identities. Next, we guarantee that our anonymization process does not depend on hyperedges or nodes identities, which justifies the claim of inductiveness of our model:

**Proposition 1**.: _Given two (potential) hyperedges \(e_{0}=\{u_{1},,u_{k}\}\) and \(e^{}_{0}=\{u^{}_{1},,u^{}_{k}\}\), if there exists a bijective mapping \(\) between node identities such that for each SetWalk like \(_{i=1}^{k}(u_{i})\) can be mapped to one SetWalk like \(^{}_{i=1}^{k}(u^{}_{i})\), then for each hyperedge \(e=\{w_{1},,w_{k}\}\) that appears in at least one SetWalk in \(_{i=1}^{k}(u_{i})\), we have \((e,e_{0})=((e),e^{}_{0})\), where \((e)=\{(w_{1}),,(w_{k^{}})\}\)._

Finally, we guarantee that our anonymization approach is more expressive than existing anonymization process [33; 45] when applied to the CE of the hypergraphs:

**Theorem 4**.: _The set-based anonymization method is more expressive than any existing anonymization strategies on the CE of the hypergraph. More precisely, there exists a pair of hypergraphs \(_{1}=(_{1},_{1})\) and \(_{2}=(_{2},_{2})\) with different structures (i.e., \(_{1}_{2}\)) that are distinguishable by our anonymization process and are not distinguishable by the CE-based methods._

### SetWalk Encoding

Previous walk-based methods [33; 34; 78] view a walk as a sequence of nodes. Accordingly, they plug nodes' positional encodings in a Rsn  or Transformer  to obtain the encoding of each walk. However, in addition to the computational cost of Rnn and Transformers, they suffer from over-squashing and fail to capture long-term dependencies. To this end, we design a simple and low-cost SetWalk encoding procedure that uses two steps: 1 A time encoding module to distinguish different timestamps, and 2 A mixer module to summarize temporal and structural information extracted by SetWalks.

**Time Encoding.** We follow previous studies [33; 84] and adopt random Fourier features [85; 86] to encode time. However, these features are periodic, so they capture only periodicity in the data. We add a learnable linear term to the feature representation of the time encoding. We encode a given time \(t\) as follows:

\[(t)=(_{l}t+_{i})\,\|(t_{p}),\] (7)

where \(_{l},_{l}\) and \(_{p}^{d_{2}-1}\) are learnable parameters and \(\|\) shows concatenation.

**Mixer Module.** To summarize the information in each SetWalk, we use a MLP-Mixer on the sequence of hyperedges in a SetWalk as well as their corresponding encoded timestamps. Contrary to the anonymization process, where we need a permutation invariant procedure, here, we need a permutation variant procedure since the order of hyperedges in a SetWalk is important. Given a (potential) hyperedge \(e_{0}=\{u_{1},,u_{k}\}\), we first assign \((e,e_{0})\) to each hyperedge \(e\) that appears on at least one sampled SetWalk starting from \(e_{0}\) (Equation 3). Given a SetWalk, \(:(e_{1},t_{e_{1}})(e_{m},t_{e_{m}})\), we let \(\) be a matrix that \(_{i}=(e_{i},e_{0})|(t_{e_{i}})\):

\[()=(_{}+ ((_{}) _{}^{(1)})_{}^{(2)} ),\] (8)

where

\[_{}=+_{}^{(2)} (_{}^{(1)}( )).\] (9)

### Training

In the training phase, for each hyperedge in the training set, we adopt the commonly used negative sample generation method  to generate a negative sample. Next, for each hyperedge in the training set such as \(e_{0}=\{u_{1},u_{2},,u_{k}\}\), including both positive and negative samples, we sample \(M\)SetWalks with length \(m\) starting from each \(u_{i} e_{0}\) to construct \((u_{i})\). Next, we anonymize each hyperedge that appears in at least one SetWalk in \(_{i=1}^{k}(u_{i})\) by Equation 3 and then use the Mixer module to encode each \(_{i=1}^{k}(u_{i})\). To encode each node \(u_{i} e_{0}\), we use Mean(.) pooling over SetWalks in \((u_{i})\). Finally, to encode \(e_{0}\) we use SetMixer to mix obtained node encodings. For hyperedge prediction, we use a 2-layer perceptron over the hyperedge encodings to make the final prediction. We discuss node classification in Appendix G.2.

## 4 Experiments

We evaluate the performance of our model on two important tasks: hyperedge prediction and node classification (see Appendix G.2) in both inductive and transductive settings. We then discuss the importance of our model design and the significance of each component in CAr-Walk.

### Experimental Setup

**Baselines.** We compare our method to eight previous state-of-the-art baselines on the hyperedge prediction task. These methods can be grouped into three categories: 1 Deep hypergraph learning methods including HyperSAGCN , NHP , and CHESHIRE . 2 Shallow methods including HPRA  and HPLSF . 3 CE methods: CE-CAW , CE-EvolveGCN  and CE-GCN . Details on these models and hyperparameters used appear in Appendix F.2.

**Datasets.** We use 10 available benchmark datasets  from the existing hypergraph neural networks literature. These datasets' domains include drug networks (i.e., NDC ), contact networks (i.e., High School  and Primary School ), the US. Congress bills network [94; 95], email networks (i.e., Email Enron  and Email Eu ), and online social networks (i.e., Question Tags and Users-Threads ). Detailed descriptions of these datasets appear in Appendix F.1.

**Evaluation Tasks.** We focus on Hyperedge Prediction: In the transductive setting, we train on the temporal hyperedges with timestamps less than or equal to \(T_{}\) and test on those with timestamps greater than \(T_{}\). Inspired by Wang et al. , we consider two inductive settings. In the **Strongly Inductive** setting, we predict hyperedges consisting of some unseen nodes. In the **Weakly Inductive** setting,we predict hyperedges with _at least_ one seen and some unseen nodes. We first follow the procedure used in the transductive setting, and then we randomly select 10% of the nodes and remove all hyperedges that include them from the training set. We then remove all hyperedges with seen nodes from the validation and testing sets. For dynamic node classification, see Appendix G.2. For all datasets, we fix \(T_{}=0.7\,T\), where \(T\) is the last timestamp. To evaluate the models' performance we follow the literature and use Area Under the ROC curve (AUC) and Average Precision (AP).

### Results and Discussion

**Hyperedge Prediction.** We report the results of CAr-Walk and baselines in Table 1 and Appendix G. The results show that CAr-Walk achieves the best overall performance compared to the baselines in both transductive and inductive settings. In the transductive setting, not only does our method outperform baselines in all but one dataset, but it achieves near perfect results (i.e., \(\) 98.0) on the NDC and Primary School datasets. In the Weakly Inductive setting, our model achieves high scores (i.e., \(>\) 91.5) in all but one dataset, while most baselines perform poorly as they are not designed for the inductive setting and do not generalize well to unseen nodes or patterns. In the Strongly Inductive setting, CAr-Walk still achieves high AUC (i.e., \(>\) 90.0) on most datasets and outperforms baselines on _all_ datasets. There are three main reasons for CAr-Walk's superior performance: 1 Our SetWalk's capture higher-order patterns. 2 CAT-Walk incorporates temporal properties (both from SetWalk's and our time encoding module), thus learning underlying dynamic laws of the network. The other temporal methods (CE-CAW and CE-EvolveGCN) are CE-based methods, limiting their ability to capture higher-order patterns. 3 CAr-Walk's set-based anonymization process that avoids using node and hyperedge identities allows it to generalize to unseen patterns and nodes.

**Ablation Studies.** We next conduct ablation studies on the High School, Primary School, and Users-Threads datasets to validate the effectiveness of CAr-Walk's critical components. Table 2 shows AUC results for inductive hyperedge prediction. The first row reports the performance of the complete CAr-Walk implementation. Each subsequent row shows results for CAr-Walk with one module modification: row 2 replace SetWalk by edge-dependent hypergraph walk , row 3 removes the time encoding module, row 4 replaces SetMixer with Mean(.) pooling, row 5 replaces the SetMixer with sum-based universal approximator for sets , row 6 replaces the MLP-Mixermodule with a Rnn (see Appendix G for more experiments on the significance of using MLP-Mixer in walk encoding), row 7 replaces the MLP-Mixer module with a Transformer , and row 8 replaces the hyperparameter \(\) with uniform sampling of hyperedges over all time periods. These results show that each component is critical for achieving CAr-Walk's superior performance. The greatest contribution comes from SetWalk, MLP-Mixer in walk encoding, \(\) in temporal hyperedge sampling, and SetMixer pooling, respectively.

**Hyperparameter Sensitivity.** We analyze the effect of hyperparameters used in CAr-Walk, including temporal bias coefficient \(\), SetWalk length \(m\), and sampling number \(M\). The mean AUC performance on all inductive test hyperedges is reported in Figure 4. As expected, the left figure shows that

    &  &  &  &  &  &  &  &  \\    &  &  &  &  &  &  &  &  &  &  &  &  &  \\  CE-GCN & 52.31 \(\) 2.99 & 60.54 \(\) 2.06 & 52.34 \(\) 2.75 & 49.18 \(\) 3.61 & 63.04 \(\) 1.80 & 52.76 \(\) 2.41 & 56.10 \(\) 1.88 & 57.91 \(\) 1.56 \\ CE-EvwGCN & 49.78 \(\) 3.13 & 46.12 \(\) 3.83 & 58.01 \(\) 2.56 & 54.00 \(\) 1.84 & 57.31 \(\) 4.19 & 44.16 \(\) 1.27 & 64.08 \(\) 2.75 & 52.00 \(\) 2.32 \\ CE-CAW & 76.45 \(\) 2.02 & 83.73 \(\) 1.42 & 80.31 \(\) 1.46 & 75.83 \(\) 1.25 & 70.81 \(\) 1.13 & 72.99 \(\) 0.20 & 70.14 \(\) 1.89 & 73.12 \(\) 1.06 \\ NHP & 70.43 \(\) 4.95 & 65.29 \(\) 1.80 & 70.86 \(\) 1.42 & 69.82 \(\) 2.19 & 47.11 \(\) 6.09 & 65.35 \(\) 2.07 & 68.23 \(\) 1.34 & 71.83 \(\) 2.64 \\ HvwrnSAGCN & 79.05 \(\) 2.48 & 88.12 \(\) 3.01 & 80.13 \(\) 1.38 & 79.51 \(\) 1.27 & 73.09 \(\) 2.60 & 78.01 \(\) 1.24 & 73.66 \(\) 1.95 & 73.94 \(\) 2.57 \\ CHESHIRE & 72.24 \(\) 2.63 & 82.54 \(\) 0.88 & 77.26 \(\) 1.01 & 79.43 \(\) 1.58 & 70.03 \(\) 2.55 & 69.98 \(\) 2.71 & N/A & 76.99 \(\) 2.82 \\ CA-Wax & **0.983\(\) 1.02** & **96.03\(\) 1.50** & **93.32\(\) 0.09** & **93.54\(\) 0.06** & **72.43\(\) 0.92** & 91.61 \(\) 2.78 & **78.01 \(\) 0.33** & **78.01 \(\) 0.35** & **79.84 \(\) 6.02** \\   &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \) to \(1.6 10^{5}\). Figure 5 (left) reports the runtimes of SetWalk sampling and Figure 5 (right) reports the runtimes of CAr-Walk for training one epoch using \(M=8\), \(m=3\) with batch-size \(=64\). Interestingly, our method scales linearly with the number of hyperedges, which enables it to be used on long hyperedge streams and large hypergraphs.

## 5 Conclusion, Limitation, and Future Work

We present CAr-Walk, an inductive hypergraph representation learning that learns both higher-order patterns and the underlying dynamic laws of temporal hypergraphs. CAr-Walk uses SetWalks, a new temporal, higher-order random walk on hypergraphs that are provably more expressive than existing walks on hypergraphs, to extract temporal higher-order motifs from hypergraphs. CAr-Walk then uses a two-step, set-based anonymization process to establish the correlation between the extracted motifs. We further design a permutation invariant pooling strategy, SetMixer, for aggregating nodes' positional encodings in a hyperedge to obtain hyperedge level positional encodings. Consequently, the experimental results show that CAr-Walk 1 produces superior performance compared to the state-of-the-art in temporal hyperedge prediction tasks, and 2 competitive performance in temporal node classification tasks. These results suggest many interesting directions for future studies: Using CAr-Walk as a positional encoder in existing anomaly detection frameworks to design an inductive anomaly detection method on hypergraphs. There are, however, a few limitations: Currently, CAr-Walk uses _fixed-length_ SetWalks, which might cause suboptimal performance. Developing a procedure to learn from SetWalks of varying lengths might produce better results.