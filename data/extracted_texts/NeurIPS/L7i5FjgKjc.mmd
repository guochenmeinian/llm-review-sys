# Distribution Learning with Valid Outputs Beyond the Worst-Case

Nick Rittler

University of California - San Diego

nrittler@ucsd.edu

&Kamalika Chaudhuri

University of California - San Diego

kamalika@cs.ucsd.edu

###### Abstract

Generative models at times produce "invalid" outputs, such as images with generation artifacts and unnatural sounds. Validity-constrained distribution learning attempts to address this problem by requiring that the learned distribution have a provably small fraction of its mass in invalid parts of space - something which standard loss minimization does not always ensure. To this end, a learner in this model can guide the learning via "validity queries", which allow it to ascertain the validity of individual examples. Prior work on this problem takes a worst-case stance, showing that proper learning requires an exponential number of validity queries, and demonstrating an improper algorithm which - while generating guarantees in a wide-range of settings - makes an atypical polynomial number of validity queries. In this work, we take a first step towards characterizing regimes where guaranteeing validity is easier than in the worst-case. We show that when the data distribution lies in the model class and the log-loss is minimized, the number of samples required to ensure validity has a weak dependence on the validity requirement. Additionally, we show that when the validity region belongs to a VC-class, a limited number of validity queries are often sufficient.

## 1 Introduction

When sampling from a generative model, it is highly desirable that its outputs meet some basic criteria of quality. In the case of text, this may mean that generated sentences respect grammar rules, or avoid the use of biased or offensive language . When generating code, a criterion may be that the generated code successfully compiles . In image generation, we might wish to avoid blurry outputs, or those possessing generation artifacts which clearly distinguish them from natural images .

In this paper, we examine the statistical cost of ensuring that learned distributions produce such "valid" outputs. To do so, we consider an elegant formulation of the problem of learning such valid models due to . In their work, training data are generated according to a probability distribution \(P\), and the binary "validity" of examples is determined by some unknown "validity function" \(v\). Given sample access to \(P\) and query access to \(v\), a learner attempts to identify a probability distribution which outputs invalid examples with probability at most \(_{2}\). At the same time, the distribution should have a loss which is at most \(_{1}\) worse than that of the minimum loss model in a class \(\) which outputs valid examples with probability 1. Here, query access to \(v\) captures the idea that collecting samples is often cheap, but verifying validity is often less so, possibly requiring a human-in-the-loop.

The initial work of  suggests that choosing such a low-loss, high-validity distribution \(\) may require a large number of validity queries. Under the assumption that \(P\) is "fully-valid", i.e. outputs a valid example with probability 1, they show that in the worst case, \(2^{(1/_{1})}\) validity queries are required to choose such a model \(\) from the class \(\). They follow this result with an improper learning algorithmfor choosing \(\) which, while achieving polynomial bounds on the number of validity queries, uses a relatively large number of validity queries \(((||)/_{1}^{2}_{2})\).

The somewhat pessimistic picture painted by these fascinating complexity-theoretic results can be tracked to their generality. Firstly, it's possible that \(\) and \(P\) are significantly "mismatched", i.e. the support of each model \(q\) has only a small overlap with the support of the distribution \(P\), in which case the validity information contained in valid training samples is unhelpful to a proper learner. Secondly, their improper learning algorithm is largely loss-agnostic, in that it generates guarantees for a wide class of bounded loss functions. Finally, nothing is assumed about the form of the validity function \(v\), precluding provable estimation.

In this work, we offer a counterbalance to this picture, beginning an investigation into learning settings where guaranteeing validity is cheaper than such results might indicate. We first consider learning under complete elimination of model class mismatch, where \(\) is rich enough to contain the fully-valid data distribution \(P\), and the loss is the log-loss \(l(f(x))=(1/f(x))\). It is intuitive that in this setting, loss minimization alone should guarantee validity. Somewhat less intuitively, we demonstrate an algorithm closely related to empirical risk minimization which uses just \(((||)/(_{1}^{2},_{2}))\) samples to guarantee its output meets loss and validity requirements - in other words, validity comes quickly under random sampling from \(P\) in this setting.

Secondly, we consider learning under a different realizability assumption, namely that the validity region is a member of a VC-class of dimension \(D\). In this setting, we provide an analysis of the natural scheme of restricting the empirical risk minimizer to an estimate of the valid part of space. We show that when small-loss models \(q\) have at least constant validity, this scheme uses \((D/_{2})\) validity queries, implying a query cost reduction over the general-purpose algorithm of . We also show that learning under the capped log-loss can be used to relax the assumption of constant validity at the cost of an extra factor of \(1/_{1}\).

Our results suggest the existence of a rich web of settings in which validity may be cheaper than in the general case. They also suggest that the choice of the loss plays an important roll in guaranteeing valid outputs, compelling further investigation of the log-loss in particular.

## 2 Related Work

The framing of learning distributiuons in terms of PAC guarantees similar to  dates back to , who consider the learnability of specific classes of discrete distributions under a realizability assumption. A significant body of work on distribution learning has been developed overtime, often focusing on algorithms for learning over parametric families or under specific "structural" assumptions [8; 9; 10; 11]. The only theoretical contribution to validity-constrained distribution learning under the formulation posed by  that we are aware of is that work itself.

The study of loss functions for the evaluation of probabilistic models has often been studied the lens of "scoring rules" in the forecasting literature [12; 13; 14]. There are some notable recent contributions towards expanding the understanding of when loss functions for distribution learning display desirable properties, e.g. "properness", which designates that the loss is minimized by the true data distribution [8; 15].

The first half of this paper draws on intuition from hypothesis testing to evaluate the performance of empirical risk minimization. Hypothesis testing is a major focus of the classical statistics literature . The bounds in the first half of the paper are due to analysis inspired by the Neyman-Pearson lemma [17; 18], and rely on the approximation of total variational distance between product measures .

The applied literature on generative modeling has consistently noted the problem of learned models producing "invalid" examples [20; 21; 22; 4]. Various techniques have been proposed for mitigating invalidity generally, and in domain specific settings [23; 20; 24]. While working under the assumption that the validity function lies in a VC-class, the strategy we introduce has some rough semblance to a "post-editing" procedure proposed by .

## 3 Preliminaries

### Problem Setup

Let \(\) be a subset of Euclidean space \(^{d}\) with finite Lebesgue measure \(\). Let \(\) denote the set of all probability distributions on the measurable space \((,_{})\), where \(_{}\) arises from Lebesgue measurable sets intersected with \(\). Let \(P\) be the data-generating distribution.

In the eyes of the learner, the function \(v:\{0,1\}\) is a fixed and unknown "validity function", measurable with respect to the relevant distributions. The validity function denotes whether or not an example \(x\) is considered a valid output for a learned approximation of \(P\). The learner is given a model class of \(\) of probability distributions on \(\), each with density \(f_{q}\) with respect to \(\), and afforded with the knowledge that at least one \(q\) is "fully-valid", i.e. that there is some \(q\) with invalidity \(V(q):=_{X q}(v(X)=1)=1\). We at times use the notion of "invalidity" of a model, by which we mean \(I(q)=1-V(q)\). Following the main exposition of , we assume \(\) is of finite cardinality.

The goodness-of-fit of a model \(q\) is governed by a decreasing "local" loss function \(l:^{ 0}\{\}\). Such a loss function gives rise to loss of model via \(L_{P}(q;l):=_{X P}[l(f_{q}(X))]\). Given an i.i.d sample \(S\) from \(P\), we let the empirical estimate of the loss of a model be \(L_{S}(q;l)=_{x_{i} S}l(f_{q}(x_{i}))/|S|\). We use the shorthand \(L_{P}(q)\) and \(L_{S}(q)\) to denote the true and empirical losses of \(q\) under the log-loss \(l(q)=(1/f_{q}(x))\), where \(\) denotes the natural logarithm. We take the log-loss to be infinite at points where \(f_{q}(x)=0\).

### Goal of Learning

The goal of the learner is to choose some \(\) which has a loss \(L_{P}(;l)\) similar to that of the lowest-loss fully-valid model in \(\), while simultaneously maintaining near full-validity. Explicitly, consider the model

\[q^{*}:=*{arg\,min}_{q:V(q)=1}L_{P}(q;l).\]

To describe the quality of an outputted model, we consider two learning parameters \(_{1}\) and \(_{2}\), where \(_{1}\) is used to control the loss sub-optimality, and \(_{2}\) to control the invalidity. Formally then, the goal of the learner is to output \(\) satisfying \(L() L(q^{*})+_{1}\) and \(I()_{2}\). To accomplish this goal, the learner has sample access to \(P\), and query access to \(v\), i.e. a learner can draw any finite number of \(i.i.d.\) samples from \(P\), and any request the value of the validity function \(v\) at any finite number of inputs in \(\).

At a minimum, we are interested in algorithms which require a number of samples from \(P\) and number validity queries that is polynomial in \((||)\), \(1/_{1}\) and \(1/_{2}\). Ideally, we would like to minimize the number of validity queries given some polynomial number of samples from \(P\). The motivation for this goal is similar to the minimization of label queries in active learning for classification , where samples from the marginal over instances are often cheap, but labeling such examples is assumed expensive.

### Full-Validity of \(P\)

We assume that all samples from the data-generating distribution \(P\) are valid, i.e. that \(V(P)=1\). Under such an assumption, the query demand of a learning algorithm can be conceptualized as the overhead number of queries sufficient for choosing a good model under the standard procedure of removing invalid examples from the training set.

If the data distribution is not fully-valid, and valid samples are required by an algorithm, the question of minimizing the overall number of queries is dependent on the sample complexity of learning - if one assumes that \(P\) has been constructed by "accepting" valid samples from some underlying distribution which outputs a valid sample with constant probability, then the overall query cost incurred by an algorithm is on the order of the larger of the number of samples and the number of "overhead" validity queries it uses.

In this paper, we are primarily interested in the "overhead" number of queries, which we refer to as the "number of validity queries" of a given scheme. In most cases, algorithm sample requirements are similar to \(O((||)/_{1}^{2})\), which allows for accurate loss estimation in many settings.

### Summary of Previous Results

The learning problem above is due to , who considered the possibility of specifying learning algorithms meeting the above bi-criteria objective for any choice of bounded, decreasing, local loss function.

This work gives some interesting insight into the difficulty of selecting such a low-loss, high-validity model. They begin by giving a negative result, namely that any proper learning algorithm outputting \(\), must make \(2^{(1/_{1})}\) validity queries in the worst case, regardless of the number of samples available from \(P\). This result arises from a specific problem instance wherein every \(q\) has a significant amount of mass outside of the support of \(P\), in which case samples from a fully-valid \(P\) do not give information about \(v\) in parts of space relevant to the choice of \(\).

On the other hand, they demonstrate an improper learning algorithm which achieves polynomial bounds on samples and validity queries for any choice of loss meeting the above criteria. Their algorithm harnesses a constrained ERM oracle, iteratively querying the validity of samples from the model \(q\) which is the empirical loss minimizer putting no mass on points known to be invalid. In particular, their scheme uses \(((||)/_{1}^{2})\) samples and \(((||)/_{1}^{2}_{2})\) validity queries.

## 4 Learning Without Model Class Mismatch Under the Log-Loss

We first consider the problem of selecting a low-loss, high-validity model under a relaxation of two of the main sources of difficulty in original problem formulation: the misalignment of the model class \(\) with the data distribution \(P\), and the lack of assumptions on the loss.

In particular, we consider the problem under a realizability assumption, namely that \(P\), further investigating the power of the log-loss. Such a setting is arguably more closely aligned with contemporary learning settings with rich model classes that appropriately capture features of the underlying data distribution, where the validity information contained in samples from \(P\) can be exploited by convergence to the best information-theoretic representation of \(P\) in \(\).

The log-loss is by far the most widely-used loss in practice . It is a classic result of the proper scoring rule literature that the log-loss is the unique strictly-proper local loss, i.e. the only local loss under which for all distributions \(q P\), it holds that \(L_{P}(P;l)<L_{P}(q;l)\). This highly desirable property - implying that convergence to the optimum over \(\) coincides with convergence to \(P\) - makes the choice of an alternative outside of capped variants preferable only under specialized circumstances.

### Towards Validity without Validity Queries

Given that samples are assumed to be valid, and the log-loss permits convergence to the data generating distribution, one would hope that simply selecting a model \(\) which is a sufficiently good representation of \(P\) under the log-loss would yield validity guarantees in this setting. Simply utilizing empirical risk minimization (ERM) is the canonical approach to this end, and one which, given sufficient data from \(P\), uses exactly zero validity queries.

Note that any model \(q\) with invalidity \(I(q)>_{2}\) necessary has \(d_{TV}(q,P)>_{2}\). In this case, \(q\) must have at least \(_{2}\) mass in the invalid part of space, where \(P\) has none. Thus, if one can guarantee that \(\) has \(d_{TV}(,P)_{2}\), the validity requirement is met. Recalling the Pinsker inequality \(d_{TV}(q,q^{}) O((q,q^{})})\) relating total variational distance and KL-divergence, it follows that obtaining a model \(\) which is at most \(_{2}^{2}\) sub-optimal in log-loss yields a model meeting the validity requirement.

While this illustrates useful intuition for the setting, it glosses over two main issues. Firstly, empirical estimates of the log-loss do not admit concentration guarantees - one can construct simple examples where \(_{X P}[(1/f_{q}(X))]\) is unbounded above, but with high probability, the empirical estimate \(L_{S}(q)=_{x S}(1/f_{q}(x))/|S|\) is approximately that of \(P\). Thus, selecting low-empirical loss models can never yield loss guarantees. Secondly, this application of Pinsker's inequality demands \(_{2}^{2}\) loss sub-optimality, suggesting that ensuring validity via the selection of a good model under the log-loss is even harder than guaranteeing a small loss.

We would hope that in the case that zero-query learning is possible, that guaranteeing validity arises somewhat coincidently with convergence to \(P\), meaning that the sample complexity is not much worse given a validity requirement than without one. Thus, the path towards satisfaction of the learning objectives requires subtle handling, and compels particular attention to the sample complexity dependence on the validity parameter \(_{2}\).

### Analysis of Empirical Risk Minimization

As indicated above, it is not possible to guarantee that empirical risk minimization (ERM) outputs a model with small log-loss. It is, however possible to guarantee that it outputs a model which closely resembles \(P\) and inherits validity guarantees with a small number of samples.

In particular, it's possible to show that given sufficient samples, ERM yields a model with small total variation to \(P\) when \(P\). This is due to the following folklore theorem , which we prove under assumption of density existence in the Appendix.

**Lemma 4**.: _Fix \(0<,<1\) arbitrarily, and let \(P,q\) be distributions with densities with respect to \(\). Then if \(d_{TV}(q,P)\), and \(S P^{n}\) for \(n((1/)/^{2})\), it holds with probability \( 1-\) that_

\[L_{S}(P)<L_{S}(q).\]

Thus, at the statistical cost of estimating a coin bias, any distribution \(q\) with total variation \(\) from the data distribution will reveal itself to be empirically inferior when the log-loss is used. This can be easily leveraged to generate guarantees for ERM over \(\) in terms of total variation.

It is tempting to think that this is the entire story when it comes to guaranteeing validity. After all, we argued above that small total variation from \(P\) is sufficient for \(_{2}\) invalidity. That said, simply looking at total variation ignores a particular structural feature of distributions \(q\) with \(I(q)>_{2}\) - in particular, such distributions have mass in parts of space in which \(P\) does not.

This observation can be used to construct tight lower bounds on the total variational distance between product measures arising from \(P\) and \(q\) with \(I(q)>_{2}\). This leads to the following result, which states that ERM yields a faithful representation of the data generating distribution that is at most \(_{2}\) invalid given a number of samples with a modest dependence on the validity parameter \(_{2}\).

**Lemma 5**.: _Fix \(0<,_{1},_{2}<1\) arbitrarily, and suppose \(P\). If \(P\) is fully-valid under \(v\), and \(S P^{n}\) for \(n(|)+(1/)}{(_{1}^ {2},_{2})})\), then with probability \( 1-\) over \(S P^{n}\), the ERM solution_

\[=*{arg\,min}_{q}_{x_{i} S}(1/q(x _{i})),\]

_satisfies both_

\[d_{TV}(,P)_{1}\;\;and\;\;I()_{2}.\]

Note that this guarantee is not redundant - having \(d_{TV}(,P)_{1}\) does not imply \(I(q)_{2}\) when \(_{2}<_{1}\).

### Attaining Log-Loss Guarantees

This result can be interpreted as a vote of confidence for the naive training of generative models under the log-loss. Nevertheless, from a learning-theoretic perspective, there is a question whether or not it is possible to guarantee low log-loss while maintaining validity with zero validity queries.

[MISSING_PAGE_EMPTY:6]

### Algorithm

A natural algorithm in this setting is to "correct" the invalidity of the empirical risk minimizer - to restrict the empirical risk minimizer to parts of space which are valid with respect to an estimate of the validity \(\). This is the precisely the idea formalized in Algorithm 2.

To generate guarantees for such a strategy, one must determine the distribution with respect to which the estimate \(\) should be accurate. In our case, we generate accuracy guarantees over both \(P\) and the ERM model \(_{}\) by selecting an estimate \(\) that has 0 empirical error over both distributions. The source of the query complexity of the algorithm comes from the fact that samples arising from \(_{}\) must be labeled by oracle calls to \(v\). Noting that samples from \(P\) can be automatically labeled as valid by the full-validity of \(P\) saves a constant factor over naively labeling all examples acquired in the second half of the algorithm.

Accuracy under samples from \(P\) allows one to control the loss of \(\) by invoking the boundedness of the loss in the disagreement region of \(\) and \(v\), and guarantees with respect to \(_{}\) allow us to bound the invalidity of the restriction. Because \(P\) is fully-valid, loss contributions from the agreement region of \(v\) and \(\) correspond to parts of space where \((x)=1\) - as the loss is non-increasing, placing more mass in such parts of space can never increase the loss contribution attributable to integrating over this region.

Algorithm 2 also requires a parameter \(>0\). This parameter should be a validity lower bound on the models \(q\), providing a safeguard on the possibility of an "invalidity blowup" when restricting the ERM output to a certain region of space - one must normalize the restriction to output a probability distribution, which in this case means increasing mass in parts of space that are estimated to be valid. An a priori lower bound on the validity allows for precise enough estimation of \(\) that increasing the mass in such regions is unlikely to lead to appreciable invalidity in the final model.

It's possible that the restriction of the ERM estimated valid region is undefined - this happens if and only if the estimated valid region has zero mass under the ERM. Given validity lower bounds for models \(q\), this is a low probability event which can occur only when estimation of the validity function is very poor relative to the query complexity. As one might imagine, the handling of this case is immaterial for PAC-guarantees. We choose to arbitrarily define behavior in this case by outputting the ERM model.

### Guarantees

The restricted output \(\) of Algorithm 2 admits the following guarantee over loss sub-optimality and invalidity.

**Theorem 3**.: _Suppose \(v\) with VC-dimension \(VC() D\), and that for each \(q\), the validity \(V(q)>0\). For all \(0<_{1},_{2}, 1\) and for all choices of non-increasing loss functions \(l:^{ 0}[0,M]\), Algorithm 2 requires a number of samples_

\[ O(((||)+(1/))}{ _{1}^{2}}+)+(1/))}{ _{1}}),\]

_and a number of validity queries_

\[ O()+(1/)}{_ {2}}),\]

_to ensure that with probability \( 1-\), its output enjoys_

\[L_{P}(;l) L_{P}(q^{*};l)+_{1}\;\;and\;\;I() _{2}.\]

Thus, in regimes where e.g. \((_{1})\), \(D=((||))\), this guarantee represents a reduced number of queries under the \((M^{2}(||)/_{1}^{2}_{2})\) bound of . It also implies a "decoupling" of the query complexity from \(_{1}\).

We note that the sample requirement from \(P\) is increased in certain regimes over the \(((|Q|)/_{1}^{2})\) requirement of . This is, however, not a concern in most settings where validity queries are expensive. If samples from a fully-valid \(P\) are readily obtainable, the setting is analogous to that of active learning, where focus is directed to the number of labels requested in training.

Even if \(P\) must be constructed by "accepting" valid samples from some unfiltered \(P^{}\), a comparison between the query complexity of Theorem 3 and the query bound of  is often still representative of the relative data costs of the schemes. Supposing \(P^{}\) produces valid samples with constant probability, the total number of validity queries made by each scheme is proportional to the scheme's sample requirements from \(P\), plus the number of validity queries used in its execution. Essentially, to yield validity query speedups, our scheme requires a VC bound on \(\) which does not dwarf \((||)\). Thus, in most cases of interest, the querying the validity of \((MD/_{1})\) extra samples is asymptotically inconsequential relative to \(O(M^{2}(|Q|)/_{1}^{2})\), and the \((D/_{2})\) query budget required to execute the algorithm given access to a fully-valid \(P\).

### Better Query Complexity Bounds

#### 5.3.1 Exploiting the Power of Active Learning

Theorem 3 presents a somewhat pessimistic view of the potential of such a "post-filtering" scheme.

Firstly, it ignores the potential of active learners to improve query complexities over passive sampling. Query complexities in active learning of classifiers are often expressed in terms of the "disagreement coefficient" , often denoted via \(\). In the realizable setting, query complexities of active learning look like \((D(1/))\). Defintionally, it can be shown that \( 1/\). Thus, proving the gains of active learning algorithms usually relies on bounding the disagreement coefficient non-trivially, i.e. showing \(<o(1/)\), or ideally, \( O(1)\).

While this is challenging, as \(\) is both a class and distribution-dependent quantity, there is a literature that addresses this potential in various settings - see the references in . In principle, one could use such an analysis to show that the query complexity of an active learning modification of Algorithm 2 is on a lower order than the guarantee of Theorem 3 when conditions are favorable. To this end, it may be useful to note that a modification of Algorithm 2 wherein \(\) is selected as the ERM on a dataset generated by a mixture of \(P\) and \(_{}\) admits guarantees as well.

#### 5.3.2 Only Low-Loss Models Need Appreciable Validity

Another source of potential looseness in the statement of Theorem 3 is that it phrases the query complexity in terms of the worst-case validity over models \(q\). This is unnecessary - with high probability, in the first step of the algorithm, one selects a model \(q\) with \(O(_{1})\) true loss. Thus, what really matters for such a strategy is that models that have relatively small loss \(l\) do not have invalidity nearing 1.

This is a realistic scenario in the case that the loss function \(l\) - despite possibly not being proper - prioritizes models which in some sense resemble the data generating distribution \(P\). Indeed, it's somewhat difficult to envision a situation where a loss would be chosen that prioritizes models with no relation to the data generating distribution. To this end, we give the following corollary to Theorem 3.

**Corollary 1**.: _Under the conditions of Theorem 3, if in addition it holds that all models \(q\) with loss sub-optimality \(_{1}\) have validity greater than some constant, then the query complexity of Algorithm 2 can be improved to_

\[ O()+(1/)}{_{2}}).\]

#### 5.3.3 Removing the Positive Validity Requirement

Using an idea found in the algorithm of , one can show that if a learner has access to single distribution \(\) with a density and at least some non-zero constant validity, and the densities \(f_{q}\) are bounded above, that Algorithm 2 can be modified so as to drop the requirement of positive validity over models when learning under the capped log-loss.

By mixing the \(_{}\) with \(\), giving mixture component \(O(_{1})\) to \(\), one can generate similar guarantees as those of Theorem 3. The modification can be found in the Appendix as Algorithm 3, and enjoys the following guarantee.

**Theorem 4**.: _Suppose \(v\) where \(VC() D\), and that for each \(q\), we have \(f_{q}(x)\). Suppose further that there is some known \(\) with density \(f_{}\) which has \(V() c>0\) for some constant \(c\). Then for all choices of \(0<_{1},_{2},<1/2\) and \(M>0\), Algorithm 3 requires a number of samples_

\[(((||)+(1/) )}{_{1}^{2}}+)+(1/) )}{_{1}}),\]

_and a number of validity queries_

\[ O(_{2})+(1/)}{_ {1}_{2}}),\]

_to ensure that with probability \( 1-\), its output enjoys_

\[_{X P}(M,\ (1/f_{}(X))) _{X P}(M,\ (1/f_{q^{*}}(X)))+_{1}\ \ and\ \ I() _{2}.\]

Here, the \(\) notation again hides factors polylogarithmic in \(1/\).

Note that the \(\) now appears in the sample complexity. This simply reflects the fact that the \(M\)-capped log-loss can range between gap \(M\) and \((1/)\) when working with densities bounded above by \( 1\). In the case that densities can be bounded above by \(1\), as in the discrete setting of , this dependence disappears.

## 6 Conclusion

This work is intended as a first-look into settings closer to the common-case, where ensuring validity may be relatively cheap.

A more thorough investigation of the log-loss, as well as capped variants, seems a very relevant line of further inquiry, given the widespread use of this family in practice and its useful information-theoretic properties. A natural extension to the first part of this work would be to consider learning in the agnostic case \(P\) under the log-loss, where one would hope to be able to exploit these properties and the validity of training samples to keep the number of validity queries low.

In general, characterizing the sample and query demands of validity-constrained distribution learning is challenging, given that proving lower bounds in general requires arguing against learners with two tools at their disposal - sampling and actively querying validity. Work in this direction will likely require some creative constructions.