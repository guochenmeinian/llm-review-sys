# TFG: Unified Training-Free Guidance

for Diffusion Models

 Haotian Ye\({}^{1}\)1 &Haowei Lin\({}^{2}\)1 &Jiaqi Han\({}^{1}\)1 &Minkai Xu\({}^{1}\) &Sheng Liu\({}^{1}\) &Yitao Liang\({}^{2}\) &Jianzhu Ma\({}^{3}\) &James Zou\({}^{1}\) &Stefano Ermon\({}^{1}\)

\({}^{1}\)Stanford University \({}^{2}\)Peking University \({}^{3}\)Tsinghua University

Equal contribution. Corresponding to mailto:haotianye@stanford.edu.Code is available at https://github.com/YWolfeee/Training-Free-Guidance.

###### Abstract

Given an unconditional diffusion model and a predictor for a target property of interest (_e.g._, a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training. Existing methods, though effective in various individual applications, often lack theoretical grounding and rigorous testing on extensive benchmarks. As a result, they could even fail on simple tasks, and applying them to a new problem becomes unavoidably difficult. This paper introduces a novel algorithmic framework encompassing existing methods as special cases, unifying the study of training-free guidance into the analysis of an algorithm-agnostic design space. Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy that can be readily applied to any downstream task. We systematically benchmark across 7 diffusion models on 16 tasks with 40 targets, and improve performance by 8.5% on average. Our framework and benchmark offer a solid foundation for conditional generation in a training-free manner.1

## 1 Introduction

Recent advancements in generative models, particularly diffusion models , have demonstrated remarkable effectiveness across vision , small molecules , proteins, audio , 3D objects , and many more. Diffusion models estimate the gradient of log density (i.e., Stein score, ) of the data distribution  via denoising learning objectives, and can generate new samples via an iterative denoising process. With impressive scalability to billions of data , future diffusion models have the potential to serve as _foundational_ generative models across a wide range of applications. Consequently, the problem of conditional generation based on these models, _i.e._, tailoring outputs to satisfy user-defined criteria such as labels, attributes, energies, and spatial-temporal information, is becoming increasingly important .

Conditional generation methods like classifier-based guidance  and classifier-free guidance  typically require training a specialized model for each conditioning signal (e.g., a noise-conditional classifier or a text-conditional denoiser). This resource-intensive and time-consuming process greatly limits their applicability. In contrast, _training-free guidance_ aims to generate samples that align with certain targets specified through an _off-the-shelf_ differentiable target predictor without involving any additional training. Here, a target predictor can be any classifier, loss function, probability function, or energy function used to score the quality of the generated samples.

In classifier-based guidance , where a noise-conditional classifier is specifically trained to predict the target property on both clean and noisy samples, incorporating guidance in the diffusionprocess is straightforward since the gradient of the classifier is an unbiased driving term. Training-free guidance, however, is fundamentally more difficult. The primary challenge lies in leveraging a target predictor trained solely on _clean_ samples to offer guidance on _noisy_ samples. Although various approaches have been proposed [18; 63; 6; 2; 78] and are effective for some individual tasks, theoretical grounding and comprehensive benchmarks are still missing. Indeed, existing methods fail to produce satisfactory samples for label guidance even on simple datasets such as CIFAR10 (Figure 1). Moreover, the lack of quantitative comparisons between these methods makes it difficult for practitioners to identify an appropriate algorithm for a new application scenario.

This paper proposes a novel and general algorithmic framework for (and also named as) **Training Free Guidance** (TFG). We show that existing approaches are special cases of the TFG as they correspond to particular hyper-parameter subspace in our unified space. In other words, TFG naturally simplifies and reduces the study of training-free guidance, as well as the comparisons between existing methods, into the analysis of hyper-parameter choices in our unified design space. Within our framework, we analyze the underlying theoretical motivation of each hyper-parameter and conduct comprehensive experiments to identify their influence. Our systematic study offers novel insights into the principles behind training-free guidance, allowing for a transparent and efficient survey of the problem.

Figure 1: **(a) Illustration of the unified search space of our proposed TFG, where the height (color) stands for performance. Existing algorithms search along sub-manifolds, while TFG results in improved guidance thanks to its extended search space. (b) The label accuracy (higher the better) and FrÃ©chet inception distance (FID, lower the better) of different methods for the label guidance task on CIFAR10 , averaged across ten labels. Ours (TFG-4) performs much closer to training-based methods. (c\(\)**h) TFG generated samples across various tasks in vision, audio, and geometry domains.**

Based on the framework, we propose a hyper-parameter searching strategy for general downstream tasks. We comprehensively benchmark TFG and existing algorithms across 16 tasks (ranging from images to molecules) and 40 targets. TFG achieves superior performance across all datasets, outperforming existing methods by 8.5% on average. In particular, it excels in generating user-required samples in various scenarios, regardless of the complexity of targets and datasets.

In summary, we (1) propose TFG that unifies existing algorithms into a design space, (2) theoretically and empirically analyze the space to propose an effective space-searching strategy for general problems, and (3) benchmark all methods on numerous qualitatively different tasks to present the superiority of TFG and the guideline for future research in training-free conditional generation algorithms. This advancement demonstrates the efficacy of TFG and establishes a robust and comprehensive benchmark for future research in training-free conditional generation algorithms.

## 2 Background

**Generative diffusion model.** A generative diffusion model is a neural network that can be used to sample from an unconditional distribution \(p_{0}()\) with the support on any continuous sample space \(\)[21; 62; 64; 27]. For instance, \(\) could be \([-1,1]^{d d 3}\) representing the RGB colors of \(d d\) images [4; 22], or \(^{3d}\) representing the 3D coordinates of molecules with \(d\) atoms [24; 74; 73]. Given a data \(_{0}\) sampled from \(p_{0}()\), a time step \(t[T]\{1,,T\}\), a corresponding noisy datapoint is constructed as \(_{t}=}_{0}+_{t}}\) where \((,)\) and \(\{_{t}\}_{t=1}^{T}\) is a set of pre-defined monotonically decreasing parameters used to control the noise level. Following , we further define \(_{t}=_{t}/_{t-1}\) for \(t>1\) and \(_{1}=_{1}\). The diffusion model \(_{}:[T]\) parameterized by \(\) is trained to predict the noise \(\) that was added on \(_{t}\) with p.d.f \(p_{t}(_{t})=_{_{0}}p_{0}(_{0})p_{t|0}(_{t}|_{ 0})_{0}\)2. In theory, this corresponds to learning the score of \(p_{t}()\), i.e.,

\[*{arg\,min}_{_{}}_{t=1}^{T}_{_{ 0} p_{0}(_{0}),(,)}\|_{ }(_{t},t)-\|=-_{t}} p_{t}.\] (1)

For sampling, we start from \(_{T}(,)\) and gradually sample \(_{t-1} p_{t-1|t}(_{t-1}|_{t})\). This conditional probability is not directly computable, and in practice, DDIM  samples \(_{t-1}\) via

\[_{t-1}=_{t-1}}_{0|t}+_{t-1}- _{t}^{2}}_{t}-_{t}}_{0|t}}{_{t}}}+_{t},\] (2)

where \(\{_{t}\}_{t=1}^{T}\) are DDIM parameters, \((,)\), and

\[_{0|t}=m(_{t})_{t}-_{t}} _{}(_{t},t)}{_{t}}}\] (3)

is the predicted sample given \(_{t}\). According to the Tweedie's formula [11; 51], \(_{0|t}\) equals to the conditional expectation \([_{0}|_{t}]\) under perfect optimization of \(_{}\) in Equation (1). It has been theoretically established that the above sampling process results in \(_{0} p()\) under certain assumptions.

**Target predictor.** For a user required target \(c\), we use a predictor \(f_{c}():_{+}\{0\}\)3 to represent how well a sample \(\) is aligned with the target (higher the better). Here \(f_{c}(x)\) can be a conditional probability \(p_{0}(c|)\) for a label \(c\)[62; 14], a Boltzmann distribution \(^{-e_{c}()}\) for any pre-defined energy function \(e_{c}\)[31; 63; 38], the similarity of two features , or even their combinations. The goal is to samples from the conditional distribution

\[p_{0}(|c)()f_{c}()}{_{ }}p_{0}(})f_{c}(})}}.\] (4)

**Training-based guidance for diffusion models.** proposes to train a time-dependent classifier to fit \(f_{c}(_{t},t)_{_{0} p_{0|t}(|_{t})} f_{c}(_{0})\). This can be regarded as a predictor over noisy samples. Since

\[_{_{t}} p_{t}(_{t}|c) =_{_{t}}_{_{0}}p_{t|0}(_{t}| _{0})p_{0}(_{0}|c)_{0}\] \[=_{_{t}}_{_{0}}p_{t}(_{t})p_{0|t} (_{0}|_{t})f_{c}(_{0})_{0}\] \[=_{_{t}} p_{t}(_{t})+_{_{t}}  f_{c}(_{t},t),\] (5)

if we denote the trained classifier as \(f(_{t})\) (that implicitly depends on \(c\) and model parameters), we can replace \(_{}(_{t},t)\) in Equation (3) by \(_{}(_{t},t)-_{t}}_{_{t}}  f(_{t})\) upon sampling to obtain unbiased sample \(_{0} p_{0}(_{0}|c)\). On the other hand,  proposes the classifier-free diffusion guidance approach. Instead of training a time-dependent predictor \(f\), it encodes conditions \(c\) directly into the diffusion model as \(_{}(,c,t)\) and trains this condition-aware diffusion model with sample-condition pairs. Both methods have been proven effective when training resources are available.

This paper in contrast focuses on conditional generation in a _training-free_ manner: given a diffusion model \(_{}(,t)\) and an off-the-shelf target predictor \(f()\) (we omit the subscript \(c\) below), we aim to generate samples from \(p_{0}(|c)\) without any additional training. Unlike training-based methods that can accurately estimate \(f(_{t},t)\), training-free guidance is significantly more difficult since it involves guiding a noisy data \(_{t}\) using \(f()\) defined over the _clean_ data space.

### Existing algorithms

Most existing methods take advantage of the predicted sample \(_{0|t}\) defined in Equation (3) and use the gradient of \(f()\) for guidance. We review and summarize five existing approaches below, and provide a schematic and a copy of pseudo-code in Appendix B for the sake of reference. Due to the variety in underlying intuitions and implementations, coupled with a lack of quantitative comparisons among these methods, it is challenging to discern which operations are crucial and which are superfluous, a problem we address in Section 3.

**DPS** was initially proposed to solve general noisy inverse problems for image generation: for a given condition \(\) and a transformation operator \(\), we aim to generate image \(\) such that \(\|()-\|_{2}\) is small. For instance, in super-resolution task , the operator \(\) is a down-sampling operator, and \(\) is a low-resolution image. DPS replaces \( f(_{t},t)\) in Equation (5) by \(_{_{t}} f(m(_{t}))\). As suggested in , this corresponds to a point estimation of the conditional density \(p_{0|t}(_{0}|_{t})\).

**LGD** replaces the point estimation in DPS and proposes to estimate \(f(_{t},t)\) with a Gaussian kernel \(_{(_{0|t},_{t}^{2})}f(,t)\), where the expectation is computed using Monte-Carlo sampling .

**FreeDoM** generalizes DPS by introducing a "recurrent strategy" (called "time-travel strategy" [39; 10; 70]) that iteratively denoises \(_{t-1}\) from \(_{t}\) and adds noise to \(_{t-1}\) to regenerate \(_{t}\) back and forth. This strategy empirically enhances the strength of the guidance at the cost of additional computation. FreeDoM also points out the importance of altering guidance strength at different time steps \(t\), but a comprehensive study on which schedule is better is not provided.

**MPGD** is proposed for manifold preserving tasks, e.g., the target predictor is supposed to generate samples on a given manifold. It computes the gradient of \( f(_{0|t})\) to \(_{0|t}\) instead of \(_{t}\), i.e., \(_{_{0|t}} f(_{0|t})\) to avoid the back-propagation through the diffusion model \(_{}\) that is highly inefficient. This strategy is effective in manifold-preserving problems, but whether it can be generalized to general training-free problems is unclear. In addition to the computation difference, theoretical understanding on the difference between gradients to \(_{0|t}\) and \(_{t}\) is missing.4

**UGD** builds on FreeDoM, with the difference that it additionally solves a backward optimization problem \(_{0}=_{}f(_{0|t}+)\) and guides \(_{0|t}\) and \(_{t}\) simultaneously. UGD also implements the "recurrent strategy" to further improve generation quality.

TFG: A Unified Framework for Training-free Guidance

Despite the array of algorithms available and their reported successes in various applications, we conduct a case study on CIFAR10  to illustrate the challenging nature of training-free guidance and the insufficiency of existing methods. Specifically, for each of the ten labels, we use the pretrained diffusion model and classifiers from [7; 9] to generate \(2048\) samples, where the hyper-parameters are selected via a grid search for the fairness of comparison. We compute the FID and the label accuracy evaluated by another classifier  and present results in Figure 1. Even in such a relatively simple setting, all training-free approaches significantly underperform training-based guidance, with a significant portion of generated images being highly unnatural (when guidance is strong) or irrelevant to the label (when guidance is weak). These findings reveal the fundamental challenges and highlight the necessity of a comprehensive study. Unfortunately, comparisons and analyses of existing approaches are missing or primarily qualitative, limiting deeper investigation in this field.

### Unification and extension

This sections introduces our unified framework for training-free guidance (TFG, Algorithm 1) and formally defines its design space in Definition 3.1. We demonstrate the advantage of TFG by drawing connections between TFG and other algorithms to show that existing algorithms are encompassed as special cases. Based on this, _all comparisons and studies of training-free algorithms automatically become the study within the hyper-parameter space of our framework_. This allows us to analyze the techniques theoretically and empirically, and choose an appropriate hyper-parameter for a specific downstream task efficiently and effectively, as shown in Section 4.

```
1:Input: Unconditional diffusion model \(_{}\), target predictor \(f\), guidance strength \(,,\), number of steps \(T,N_{},N_{}\)
2:\(_{T}(,)\)
3:for\(t=T,,1\)do
4: Define function \(()=_{(,)}f(+ _{t}})\)
5:for\(r=1,,N_{}\)do
6:\(_{0it}=(_{t}-_{t}}_{}(_ {t},t))/_{t}}\)\(\) Obtain the predicted data
7:\(_{t}=_{t}_{_{t}}(_{0|t})\)
8:\(_{0}=_{0}+_{t}_{_{0|t}}(_{0|t} +_{0})\)\(\) Iterate \(N_{}\) times starting from \(_{0}=\)
9:\(_{t-1}=(_{t},_{0|t},t)+_{t}/_{t}}+_{t-1}}_{0}\)\(\) Sample follows Equation (2)
10:\(_{t}(_{t}}_{t-1}, })\)\(\) Recurrent strategy
11:endfor
12:endfor
13:Output: Conditional sample \(_{0}\) ```

**Algorithm 1** Training-Free **Gu**idance

**Definition 3.1**.: Given a denoising step \(T\), the hyper-parameter space (design space) of Algorithm 1 is defined as

\[_{}=\{(N_{},N_{},, {},):N_{},N_{},  0,,_{+}\{0\}^{T}\}.\] (6)

We use \(_{}\) to represent the complete hyper-parameter space and \(_{}(N_{}=N_{0})\) to represent the subspace constrained on \(N_{}=N_{0}\).

Definition 3.1 defines the hyper-parameter space spanned by TFG, where one hyper-parameter in \(_{}\) is an instantiation of the framework. Intuitively, \(N_{recur}\) controls the recurrence of the algorithm, \(N_{iter}\) controls the iterating when computing \(_{0}\) (Line 8), \(\) controls the extent we smooth the original guidance function \(f\) (Line 4), and \(,\) control the strength of two types of guidance (Lines 7 and 8). A comprehensive explanation of the effect of each hyper-parameter can be found in Section 3.2.

Below is the major theorem showing that all algorithms presented in Section 2.1 correspond to special cases of TFG, thus unifying them into our framework and obviating the need for separate analyses.

**Theorem 3.2**.: _The hyper-parameter space of_

* _MPGD___\(_{}\) _is equivalent to_ \(_{}(N_{}=N_{}=1,=, =0)\)_._
* _LGD_ __\(_{}\) _is equivalent to_ \(_{}(N_{}=1,N_{}=0,=)\)_._* _UGD___\(_{}\) _is equivalent to_ \(_{}(=0)\)_._
* _DPS_ __\(_{}\) _is equivalent to_ \(_{}(N_{}=1,N_{}=0,= ,=0)\)_._
* _FreeDoM___\(_{}\) _is equivalent to_ \(_{}(N_{}=0,=, {}=0)\)_._

The complete analysis and proof of Theorem 3.2 is postponed to Appendix C. It implies that existing algorithms are limited in expressivity, covering only a subset of \(_{}\). In contrast, TFG covers the entire space and is guaranteed to perform better. In addition, TFG streamlines nuances between existing methods, allowing for a unified way to compare and study different techniques. Consequently, the versatile framework that TFG provides can simplify its adaptation to various applications.

### Algorithm and design space analysis

We now present a concrete analysis of TFG and its design space \(\) in detail. Similar to standard classifier-based guidance, TFG guides \(_{t}\) at each denoising step \(t\). To provide appropriate and informative guidance, TFG essentially leverages four techniques for guidance: Mean Guidance (Line 8) controlled by \(N_{},\), Variance Guidance (Line 7) controlled by \(\), Implicit Dynamic (Line 4) controlled by \(\), and Recurrence (Line 5) controlled by \(N_{}\).

**Mean Guidance** computes the gradient of \(()\) to \(_{0|t}\) and is the most straightforward approach. However, this method can yield inaccurate guidance. To show this, notice that under perfect optimization we have \(_{0|t}=[_{0}|_{t}]\), and when \(p_{0}([_{0}|_{t}])\) is close to zero, the predictor has rarely been trained on data from the region close to \(_{0|t}\), making the gradient unstable and noisy. To mitigate this, one can iteratively add gradients of \(()\) to \(_{0|t}\), encouraging \(_{0|t}\) to escape low-probability regions.

**Variance Guidance** provides an alternative approach for improving the gradient estimation. The reason why we dub it variance guidance might be ambiguous, as the only difference is that the gradient is taken with respect to \(_{t}\) (Line 7) instead of \(_{0|t}\) (Line 8). The lemma below demonstrates that this essentially corresponds to a covariance re-scaled guidance.

**Lemma 3.3**.: _If the model is optimized perfectly, i.e., \(_{}(,t)=-_{t}} p_{t }()\), we have_

\[_{t}=_{t}}}{1-_{t}}_{ 0|t}_{_{0|t}}(_{0|t}),\] (7)

_where \(_{0|t}_{}p_{0|t}(|_{t})(-[_{0}| _{t}])(-[_{0}| {x}_{t}])^{}\) is the covariance of \(_{0}|_{t}\)._

Lemma 3.3 suggests that variance guidance refines mean guidance by incorporating the second-order information of \(_{0}|_{t}\), specifically considering the correlation among components within \(_{0|t}\). Consequently, positively correlated components could have guidance mutually reinforced, while negatively correlated components could have guidance canceled. This also implies that mean guidance and variance guidance are intrinsically leveraging different orders of information for guidance. In TFG, variance guidance is controlled by \(_{t}\).

**Implicit Dynamic** transforms the predictor \(f\) into its convolution via a Gaussian kernel \((,(1-_{t}))\). This operation is initially introduced by LGD  to estimate \(p_{0|t}(_{0}|_{t})\). However, it is unclear why the form is pre-selected as a Gaussian distribution. We argue that this technique is effective because it creates an implicit dynamic on \(_{0|t}\). Specifically, starting from \(_{0|t}\), it iteratively adds noise to \(_{0|t}\), evaluates gradient, and moves \(_{0|t}\) based on the gradient. The repeating process converges to the density proportional to \(f()\) when \(N_{}\) goes to infinity, driving \(_{0|t}\) to high-density regions. This explanation is justified by Table 1: the performance remains nearly unchanged as we gradually decrease the number of Monte-Carlo samples in estimating the expectation (Line 4) down to \(1\), implying that the _preciseness_ of estimation is not essential, but adding noises is.

**Recurrence** helps strengthen the guidance by iterating the previous three techniques to obtain \(_{t-1}\) and resample \(_{t}\) back and forth. This can be understood as an _Ornstein-Uhlenbeck process_ on \(_{t-1}\) where Line 6\(\)9 corresponds to the drift term and \(_{t-1}_{t}\) (Line 10) the white noise term. Intuitively, it finds a trade-off between the error inherited from previous steps (the more you recur, the

   &  &  \\  & FID & Acc(\%) & FID & Acc(\%) \\ 
1 & 90.6 & 65.8 & 101 & 36.2 \\
2 & 91.0 & 65.2 & 100 & 35.6 \\
4 & 90.7 & 64.9 & 99.7 & 36.2 \\  

Table 1: Influence of the number of Monte-Carlo samples in estimating the expectation of Line 4. Both the FID and the accuracy remain unchanged when \(\#\)Samples varies, suggesting that the number of samples is less important. More details are in Appendix E.

less previous error stays) and the accumulated error in this step (the more you recur, the more error in the current guidance you suffer). Empirically, we also find that the generation quality improves and then deteriorates as we increase \(N_{}\).

## 4 Design Space of TFG: Analysis and Searching Strategy

Admittedly, a more extensive design space only yields a better performance if an effective and robust hyper-parameter searching strategy can be applied. For example, arbitrarily complex neural networks are guaranteed to have better optimal performance than simple linear models, but finding the correct model parameters is significantly more difficult. This section dives into this core problem by comprehensively analyzing the hyper-parameter space structure of \(_{}\), and further proposing a general searching algorithm applicable for any general downstream tasks.

The hyper-parameters of \(_{}\) can be categorized into two parts: time-dependent vectors \(,\), and time-independent scalars \(N_{},N_{},\). While a grid search can potentially result in the best performance, performing such an extensive search in \(_{}\) is highly impractical, especially considering the vector parameters \(,\). Fortunately, below we demonstrate that, if we decompose \(\) into \( s_{}(t)\) (same for \(\)) where \(\) is a scalar and \(s_{}(t)\) is a "structure" (a non-negative function) such that \(_{t}s_{}(t)=T\), then some structures are consistently better than others regardless of the other hyper-parameters. This allows us to pre-locate an appropriate structure for the given task and efficiently optimize the rest of the scalar hyper-parameters. Our analysis is conducted on the label guidance task on CIFAR-10  and ImageNet , with experimental settings identical to Section 3.

**Structure analysis.** Motivated by the default structure selected in UGD and LGD, we consider three structures for both \(s_{}(t)\) and \(s_{}(t)\) as

\[s(t)=}{_{t=1}^{T}_{t}}(),\;s(t)=)}{_{t=1}^{T}(1-_{t})}(),\;s(t)=1( ).\] (8)

These structures are selected to be qualitatively different, while each is justified to be reasonable under certain conditions [18; 78; 2]. We leave the study of more structures to future works. The rest of the parameters are grid-searched for the comprehensiveness of the analysis. For \(s_{}(t)\), we set \(N_{}=\{1,2,4\}\) and \(=\{0.25,0.5,1.0,2.0,4.0\}\); and for \(s_{}(t)\), we set \(N_{}=\{1,2,4\}\) and \(=\{0.25,0.5,1.0,2.0,4.0\}\). We run label guidance for each configuration and each of the ten labels on CIFAR10 (four labels on ImageNet, due to computation constraints).

As presented in Figure 2, the relationship between different structures remains unchanged when the rest of the parameters vary. For instance, on both datasets, the Validity-FID performance curves consistently move top-left (implying a better performance) when we switch from "decrease" structure (red lines) to "constant" structure (yellow lines) to "increase" structure (blue lines) for both \(,\) and different values of \(N_{}\) and \(N_{}\). This invariant relationship is essential as it allows for an efficient hyper-parameters search in \(_{}\) by first determining appropriate structures for \(s_{}(t),s_{}(t)\) under a simple subspace, and then selecting the rest scalar parameters.

Figure 2: Comparison of three structures in Equation (8) of \(\) and \(\) on CIFAR10 and ImageNet, under different choices of the rest hyper-parameters in \(_{}\). We set \(=,=0\) when studying structures of \(\), and similarly for \(\). Results are averaged across all labels. The comparative relationship between structures remains unchanged when the rest of the parameters vary.

**Computation cost analysis.** Among the scalars parameters, \(N_{}\) and \(N_{}\) directly influence the total computational cost, while \(,,\) do not. With a certain range, performance increases when the value of \(N_{},N_{}\) increase5, and the trade-off between generation quality and computation time is presented in Figure 3: recurrence leads to a \(N_{}\) times cost with clear performance gain; iteration (on \(_{|0|}\)) results in less increase of computation time, and its effect plateaus. In practice, users can determine their values based on computation resources, but an upper bound of \(4\) suffices to unlock a near-optimal performance.

**Searching strategy.** The above analysis successfully simplify the task-specific hyper-parameter search problem without significant performance sacrifice. It remains to be decided the scalar values \(,,\). Here we propose a strategy based on beam search to effectively and efficiently select their values. Specifically, our searching strategy starts with an initial set \(T=\{(_{},_{},_{})\}\), where these initial values are small enough to approximate TFG as an unconditional generation. At each searching step, for each tuple in \(T\), we separately double the values of \(\), \(\), and \(\) to generate up to \(3|T|\) new configurations. We conduct a small-sized generation trial for each new configuration and update \(T\) to be the top \(K\) configurations with the highest evaluation results that are determined by user requirements (e.g., accuracy, FID, or a combination). This iterative process is repeated until \(T\) stabilizes or the maximum number of search steps is reached. Notice that this process is conducted with a much smaller sample size, and consequently, the computation time is highly controllable.

## 5 Benchmarking

This section comprehensively benchmarks training-free guidance under the TFG framework and the design searching strategy in Section 4. We consider 7 datasets, 16 different tasks, and 40 individual targets with a total experimental cost of more than 2,000 A100 GPU hours. For comparison, we also run experiments for each of the existing methods (where the design searching is conducted in the corresponding subspace). All methods, tasks, search strategies, and evaluations are unified in our codebase, with details specified in Appendices D and E.

### Settings

**Diffusion models.** (1) **CIFAR10-DDPM** is a U-Net  model trained on CIFAR10  images. (2) **ImageNet-DDPM** is an larger U-Net model trained on ImageNet-1k  images. (3) **Cat-DDPM** is trained on Cat  images. (4) **CelebA-DDPM** is trained on CelebA-HQ dataset  that consists millions of human facial images. (5) **Molecule-EDM** is an equivariant diffusion

  
**Diffusion Model** & **Task-ID** & **Targets** & **Guidance Validity** & **Guidance fidelity** \\   & Gaussian deblur &  & LPIPS \(\) & FID \(\) \\  & Super-resolution & \(\) & LPIPS \(\) & FID \(\) \\   & Combined guidance (gender+age) & 2 genders \(\) 2 ages & Accuracy (\%) \(\) & KID (log) \(\) \\  & Combined guidance (gender+hair) & 2 genders \(\) 2 hair colors & Accuracy (\%) \(\) & KID (log) \(\) \\  CIFAR10-DDPM & Label guidance (CIFAR10) & 10 labels (0, \(\), 9) & Accuracy (\%) \(\) & FID \(\) \\   & Label guidance (ImageNet) & 4 labels (111, \(\), 444) & Accuracy (\%) \(\) & FID \(\) \\  & Fine-grained guidance & 4 labels (111, \(\), 444) & Accuracy (\%) \(\) & FID \(\) \\   & Style transfer & 4 styles & Style score \(\) & CLIP score \(\) \\   & Quantum Properties (\( 6\)) & Property distribution & MAE \(\) & Valid ratio \(\) \\   & Audio declipping &  & DTW (\%) \(\) & FAD \(\) \\  & Audio inpainting & & DTW (\%) \(\) & FAD \(\) \\   

Table 2: List of 14 task types we benchmark. Each task is run with multiple individual targets (38 in total). We evaluate the guidance validity (how well a sample is aligned with the target predictor) and the guidance fidelity (how well a sample is aligned with the unconditional distribution) according to the task type.

model pretrained on molecule dataset QM9  that performs molecule generation from scratch. (6) **Stable-Diffusion**\(( 1.5)\) is a latent text-to-image model that generate images with text prompts. (7) **Audio-Diffusion6** is a audio diffusion model based on DDPM trained to generate mel spectrograms of 256x256 corresponding to 5 seconds of audio.

**Tasks.** Our tasks (Table 2) cover a wide range of interests, including Gaussian deblur, super-resolution, label guidance, style transfer, molecule property guidance, audio declipping, audio inpainting, and guidance combination. Each task is run on multiple datasets or with multiple targets (, different labels, molecular properties, styles).

**Other settings.** We consistently set the time step \(T=100\) and the DDIM parameter \(=1\). We consider \(N_{}=1,N_{}=4\) and use a single sample for Implicit Dynamic (Line 4) throughout all experiments and methods for fair comparison. For TFG, the structures of \(\) and \(\) are set to "increase" and the scalars \(,,\) are determined via our searching strategy. We follow the setting in original papers if they specify their hyper-parameters. blueFor specific tricks in the code that are not mentioned in papers, we choose to align with original papers. Otherwise, values are determined via searching with \(1/8\) of the sample size and a maximum search step of \(6\). For fairness of comparison, we use accuracy as the metric during the search and compare different algorithms on the metric, but we report both accuracy and FID.

### Benchmarking results

We compare all six methods in Table 3. TFG outperforms existing algorithms in 13 over 14 settings, achieving an average guidance validity improvement of 7.4% compared to the best existing algorithm. Notice that we do not compare with the best algorithm in terms of generation fidelity because obtaining high realness samples is not our objective in training-free guidance, and an unconditional model suffices to generate high realness samples (with extremely low validity). Interestingly, different methods achieve the second best performance on different tasks, suggesting the variance of these methods, while TFG is consistent thanks to the unification.

We want to highlight that despite the superior performance of TFG, the key intention of our experiments is not restrained to comparing TFG with existing methods, but more importantly to systematically benchmark under the training-free guidance setting to see how much we have achieved in various tasks with different difficulties. Below we go through each task separately and conduct relevant ablation studies to provide a more fine-grained analysis.

**Fine-grained label guidance.** In addition to the standard label guidance, we for the first time study the _out-of-distribution_ fine-grained label guidance under the training-free setting, a problem where no existing training-based methods are available. We consider the bird-species guidance using an

 
**Task-ID** & **DPS** & **LGD** & **FreeDoM** & **MPGD** & **UGD** & **TFG** & **Rel. Improvement** \\  Deblur (\(\), \(\)) & 0.390 / 98.3 & 0.270 / 85.1 & 0.245 / 87.4 & 0.177 / 69.3 & 0.200 / 69.3 & **0.150** / 64.5 & +15.3\% \\ Super resolution (\(\), \(\)) & 0.420 / 109 & 0.360 / 96.7 & 0.191 / 74.5 & 0.283 / 82.0 & 0.249 / 75.9 & **0.190** / 65.9 & +0.524\% \\ Gender+Age (\(\), \(\)) & 71.6 / - 4.26 & 52.0 / 51.0 & 68.77 / 3.89 & 68.67 / 4.79 & 25.14 / 4.37 & **75.21** / -38.66 & +0.133\% \\ Gender+Hair (\(\), \(\)) & 73.0 / - 3.90 & 55.00 / 5.00 & 67.11 / 53.0 & 63.93 / 43.3 & 71.3 / -41.2 & **76.0** / -3.60 & +4.11\% \\ CIFAR10 (\(\), \(\)) & 50.1 / 172 & 32.2 / 102 & 34.8 / 135 & 30.80 / 88.3 & 45.9 / 94.2 & **52.0** / 917.7 & +3.59\% \\ ImageNet (\(\), \(\)) & 38.8 / 193 & 11.5 / 210 & 19.7 / 200 & 6.80 / 239 & 25.5 / 205 & **40.9** / 176 & +5.41\% \\ Fine-grained (\(\), \(\)) & 0.00 / 348 & 0.48 / 246 & 0.85 / 258 & 0.289 & 2.07 / 255 & **1.27** / 256 & +18.7\% \\ Style Transfer (\(\), \(\)) & 50.60 / 31.7 & 54.23 / 31.3 & 5.67 / 31.2 & 40.88 / 31.5 & 4.97 / 31.5 & 31.36 / 29.0 & +22.5\% \\  Polarizability \(\) (\(\), \(\)) & 511697 & 9.23 / 1.755 / 84.3 & 5.922 / 88.0 & 4.26 / 88.4 & 5.45 / 73.8 & **3.90** / 84.2 & +8.45\% \\ Dipole \(\) (\(\), \(\)) & 63.2 / 77.3 & 1.51 / 86.6 & 1.35 / 89.5 & 1.51 / 73.5 & 1.56 / 57.6 & **1.33** / 74.9 & +1.48\% \\ Heat capacity \(C_{}\) (\(\), \(\)) & 5.26 / 78.4 & 3.77 / 77.1 & 2.84 / 90.9 & 2.86 / 86.1 & 3.02 / 84.0 & **2.777** / 85.5 & +2.57\% \\ Highest M energy \(\)Hobulin (\(\), \(\)) & 0.744 / 83.8 & 0.664 / 66.4 & 0.623 / 6.23 & **0.554** / 53.4 & 0.582 / 58.2 & 0.568 / 77.3 & -2.53\% \\ Lowest MO energy \(\)Hobulin (\(\), \(\)) & 1.82 / 10.9 / 90.1 & 11.69 / 00.2 & 1.06 / 82.2 & 1.78 / **1.984** / 80.1 & +1.71\% \\ MO energy gap \(_{}\) (\(\), \(\)) & 1.38 / 75.7 & 1.19 / 85.3 & 1.17 / 88.5 & 1.072 / 72.5 & 1.15 / 75.7 & **0.893** / 62.5 & +16.7\% \\ Audio declipping (\(\), \(\)) & 633 / 3.60 & 157 / 2.33 & 126 / 0.173 & 178 / 0.402 & 150 / 0.262 & **101** / 0.172 & +19.8\% \\ Audio inpainting (\(\), \(\)) & 643 / 4.71 & 103 / 2.22 & 41.3 / 0.08 & 608 / 4.63 & 116 / 0.53 & **36.3** / 0.06 & +12.1\% \\  

Table 3: Benchmarking TFG and existing algorithms on 16 task types and 40 individual targets. Each cell presents the _guidance validity/generation fidelity_ averaged across multiple targets in the task (, labels, image styles). The best guidance validity is **bold**, and the second best underline. The relative improvement of guidance validity is computed between TFG and the existing method with the highest guidance validity.

EfficientNet trained to classify 525 fine-grained bird species. This problem remains highly difficult for leading text-to-image generative models such as DALLE. Under recurrence, TFG can generate at most 2.24% of accurate birds, compared with the unconditional generation rate of \(0\).

**Recurrence on label guidance.** We go back to the failure case we study in Section 3, _i.e._, the standard label guidance problem on CIFAR10 where the training-based method offers an 85% accuracy, while the accuracy of TFG without recurrence accuracy is 52% only. As presented in Table 4, increasing \(N_{}\) significantly closes the gap from 33% to 8%. Similar improvement is observed in other datasets as well.

**Multiple guidance and bias mitigation.** We next consider the scenario with multiple targets: control the generation of human faces based on gender and hair color (or age) using two predictors. It is well known that the label imbalance in CelebA-HQ causes classifiers to focus on _spurious correlations_, such as using hair colors to classify gender, a biased feature we aim to avoid. The stratified performance of TFG on "gender + age" and "gender + hair" guidance are presented in Table 5. Despite the highly disparate performance, training-free guidance largely alleviates the imbalance: only 1% of images in CelebA are "male + blonde hair", while the generated accuracy is 46.7%.

**Molecule property guidance.** To our knowledge, we are the first to study training-free guidance for molecule generation. We interestingly find in Table 3 that TFG is effective in guiding molecules towards desirable properties, yielding the highest guidance validity on 5 out of 6 targets with 5.64% MAE improvement over existing methods, verifying the generality of our approach as a unified framework in completely unseen domains. Notice that, unlike images, molecules with better validity usually have lower generation fidelity, a finding reflected in previous work .

**Audio Guidance.** We extend our investigation to the audio modality, where TFG achieves significant relative improvements over existing methods. Given that the audio domain is rarely explored in training-free guidance literature, our benchmarks will contribute to future research in this area.

## 6 Discussions and Limitations

Recently, training-free guidance for diffusion models has gained increasing attention and has been adopted in various applications. TFG is based on an extensive literature review over ten algorithmic papers for different purposes, including images, audio, molecules, and motions . While we incorporate several key algorithms into our framework, we acknowledge that encompassing all approaches is impossible, as it would make the unification bloated and less practical. We seek to find a balance point by unifying most representative algorithms while keeping the techniques clear and easily studied.

An often discussed problem is why we care about training-free guidance, given the ever-growing community of language-based generative models such as the image generator of GPT4. In practice, there are countless conditional generation tasks where the conditions are hard to accurately convey to or represent by language encoders. For instance, it can fail to under a complex property of a molecule or generate CelebA-style faces. We give an illustrative analysis in Appendix A.1. Despite that training-free guidance is important, this paper does not systematically analyze what types of conditional generation are, in general, more suitable for the framework and what types are for language-based models. That said, training-free guidance is fundamentally difficult due to the misalignment between the training objective of target predictors and the diffusion, with a more detailed discussion in Appendix A.2. This paper does not comprehensively analyze this misalignment, and the gap between training-based and TFG remains high in some tasks like molecule property guidance. We hope that future works can analytically dive into these problems.

   Recurrence & 1 & 2 & 4 \\  CIFAR10 & 52.0 / 91.7 & 66.8 / 88.7 & 77.1 / 73.9 \\ ImageNet & 40.9 / 177 & 52.3 / 163 & 59.8 / 165 \\ Fine-grained & 1.27 / 256 & 1.66 / 259 & 2.24 / 259 \\   

Table 4: The accuracy / FID for TFG with different recurrence step \(N_{}\) on three label guidance datasets, averaged across all labels.

   Target label & 0+0 & 0+1 & 1+0 & 1+1 \\  gender + hair & 92.2 & 72.7 & 89.8 & 46.7 \\ gender + age & 92.9 & 73.6 & 93.6 & 69.1 \\   

Table 5: The accuracy of multi-label guidance on CelebA, where labels 0 and 1 correspond to female and male (gender), non-blonde and blonde (hair color), and young and old (age). The accuracy is lower for minority groups, indicating an implicit bias in the generation process. Despite this, it is still much higher than unconditional generation.