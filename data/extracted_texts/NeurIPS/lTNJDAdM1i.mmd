# Out-of-Distribution Detection & Applications With Ablated Learned Temperature Energy

Will LeVine

Microsoft

Benjamin Pikus

Advex AI

Jacob Phillips

Andreessen Horowitz

Berk Norman

Anduril

Fernando Amat

Google

Sean Hendryx

Scale AI

Corresponding author email: levinewill@icloud.com

###### Abstract

As deep neural networks become adopted in high-stakes domains, it is crucial to identify when inference inputs are Out-of-Distribution (OOD) so that users can be alerted of likely drops in performance and calibration (Ovadia et al., 2019) despite high confidence (Nguyen et al., 2015) - ultimately to know when networks' decisions (and their uncertainty in those decisions) should be trusted. In this paper we introduce Ablated Learned Temperature Energy (or "AbeT" for short), an OOD detection method which lowers the False Positive Rate at 95% True Positive Rate (FPR@95) by \(43.43\%\) in classification compared to state of the art without training networks in multiple stages or requiring hyperparameters or test-time backward passes. We additionally provide empirical insights as to why our model learns to distinguish between In-Distribution (ID) and OOD samples while only being explicitly trained on ID samples via exposure to misclassified ID examples at training time. Lastly, we show the efficacy of our method in identifying predicted bounding boxes and pixels corresponding to OOD objects in object detection and semantic segmentation, respectively - with an AUROC increase of \(5.15\%\) in object detection and both a decrease in FPR@95 of \(41.48\%\) and an increase in AUPRC of \(34.20\%\) in semantic segmentation compared to previous state of the art. 2

## 1 Introduction

In recent years, machine learning models have shown impressive performance on fixed distributions. However, in cases where inference examples are far from the training set, not only does model performance drop, nearly all known uncertainty estimates also become miscalibrated - i.e. unreliable (Ovadia et al., 2019) - although there are exceptions (Rajendran and LeVine, 2019). Without OOD detection, users can therefore be fooled into false trust in model predictions due to high confidence on OOD inputs (Nguyen et al., 2015).

Aimed at OOD detection, existing methods have explored (among many other methods) modifying models via a learned temperature which is dynamic depending on input (Hsu et al., 2020) and an inference-time post-processing energy score (Liu et al., 2020). In this paper, we combine these methods and introduce an ablation, leading to our method deemed "AbeT." Due to these contributions, we demonstrate the efficacy of AbeT over existing OOD methods. We establish state of the artperformance in classification, object detection, and semantic segmentation on a suite of common OOD benchmarks spanning a variety of scales and resolutions. We also perform extensive visual and empirical investigations to understand our algorithm.

## 2 Preliminaries

Let \(X\) and \(Y\) be the input and response random variables with realizations \(x^{D}\) and \(y\{1,2,...,C-1,C\}\), respectively. Typically, we'd like to make inferences about \(Y\) given \(X\) using a learned model \(:^{D}^{C}\). In practice, a learner only has access to a limited amount of training examples in this data-set \(D_{in}^{train}=\{(x_{i},y_{i})\}_{i=1}^{N}\).

### Problem Statement

We define \(D_{in}^{test}\) identical to \(D_{in}^{train}\) but unseen at training time. And we define \(D_{out}^{test}\) as any dataset that has non-overlapping output classes with those of \(D_{in}^{train}\), as is standard in OOD detection evaluations (Huang et al., 2021; Hsu et al., 2020; Liu et al., 2020; Djurisic et al., 2022; Sun et al., 2021; Hendrycks and Gimpel, 2016; Liang et al., 2017; Sun et al., 2022; Katz-Samuels et al., 2022). The goal of Out-of-Distribution Detection is to define a score \(S\) such that \(S(x_{out})\) and \(S(x_{in})\) are far from each other \(\ x_{out} D_{out}^{test},x_{in} D_{in}^{test}\).

### Standard Classification Model Optimization

In OOD detection, \(\) serves a dual purpose: its outputs are optimized to classify among outputs \(\{1,2,...,C-1,C\}\) and functions of the network are used as inputs to \(S\). Though we use \(\) for both purposes, we aim for AbeT to neither have OOD data at training time nor significantly modify training to account for the ability to detect OOD data. Thus, we train our classification models in a standard way: to minimize cross-entropy loss \(=_{i=1}^{N}-_{y_{i}}(x_{i};)\), where \((x_{i},y_{i}) D_{in}^{train}\). Networks can be optimized towards other loss functions, but we did not test our method in conjunction with any other loss functions.

Figure 1: Histograms showing the separability between OOD scores on OOD inputs (red) and ID inputs (blue) for different methods. The goal is to make these red and blue distributions as separable as possible, with scores on OOD inputs (red) close to 0 and scores on ID inputs (blue) of high-magnitude (away from 0). (**Center**) Our first contribution is replacing the Scalar Temperature in the Energy Score (Liu et al., 2020) with a Learned Temperature (Hsu et al., 2020). This infusion leads to Equation 1, with the Learned Temperature showing up in the Exponential Divisor Temperature (overlined in Equation 1) and Forefront Temperature Constant (underlined in Equation 1) forms. (**Right**) The Forefront Temperature Constant contradicts the desired property of scores being close to 0 for OOD points (red) and of high magnitude for ID points (blue). (**Left**) Therefore, our second contribution is to ablate this Forefront Temperature Constant, leading to our final Ablated Learned Temperature Energy (AbeT) score. This ablation increases the separability of the OOD scores vs. ID scores, as can be seen visually and numerically (in terms of AUROC) comparing the center and left plots - where the only difference is this ablation of the Forefront Temperature Constant. Higher AUROC means more separability.

### Model Output

To estimate \(_{y_{i}}(x_{i};)\), models typically use logit functions per class which calculate the activation of class \(c\) on input \(x_{i}\) as \(L_{c}(x_{i};)=_{c}(x_{i};)\). Let \(w\) and \(b\) represent the weights and biases of the final layer of a network (mapping penultimate space to output space) respectively and \(f^{p}(x_{i})\) represent the penultimate representation of the network on input \(x_{i}\). Similar to Hsu et al. (2020), we found that a cosine-similarity-based logit function, \(_{c}(x_{i};)=^{T}f^{p}(x_{i})}{||w_{c}^{T}||f^{p}(x_{i} )||}\), was the most effective in our OOD evaluation paradigm. For more details, see Appendix Section A.3.

We now discuss tempering the logit function \(L\): often employed to increase calibration, Temperature Scaling (Guo et al., 2017) geometrically decreases the logit function \(L\) by a single scalar \(T_{}\). That is, \(\) has a logit function that employs a scalar temperature as \(L_{c}(x_{i};,T_{})=_{c}(x_{i};)/T_{}\). Introduced in Hsu et al. (2020), a learned temperature \(T_{}:(0,1)\) is a temperature that depends on input \(x_{i}\). That is, \(\) has a logit function that employs a learned temperature \(L_{c}(x_{i};,T_{})=_{c}(x_{i};)/T_{ }(x_{i})\). The softmax of this tempered logit serves as the final model prediction: \(_{y_{i}}(x_{i};)=_{y_{i}}(x_{i};)/ T_{}(x_{i}))}{_{c=1}^{C}(_{c}(x_{i}; )/T_{}(x_{i}))}\). This is input to the loss \(\) during training. In this formulation, \(\) and \(T_{}\) serve disjoint purposes: \(\) reduces loss by selecting \(_{y_{i}}(x_{i};)\) as highest among \(\{_{c}(x_{i};)\}_{c=1}^{C}\); and \(T_{}(x_{i})\) reduces loss by modifying the confidence (but not changing the classification) such that the confidence is high or low when the model is correct or incorrect, respectively.

We provide a visual architecture of a forward pass with a learned temperature in Appendix Figure 3. For more information on the details of our learned temperature, see Appendix Section A.1

## 3 Our Approach: AbeT

The following post-processing energy score was previously used for OOD detection in Liu et al. (2020): \((x_{i};L,T_{},)=-T_{}_ {c=1}^{C}e^{L_{c}(x_{i};,T_{})}\). This energy score was intended to be highly negative on ID input and close to \(0\) on OOD inputs via high logits on ID inputs and low logits on OOD inputs.

Our first contribution is replacing the scalar temperature with a learned one:

\[(x_{i};L,T_{},)=-}(x_{i})}_{}_{c=1}^{C}e^{L_{c}(x_{i};},}}^{ }\] (1)

By introducing this learned temperature, there become two ways to control the OOD score: by modifying the logits and by modifying the learned temperature. We note that there are two different operations that the learned temperature performs in terms of modifying the energy score. We deem these two operations the "Forefront Temperature Constant" and the "Exponential Divisor Temperature", which are underlined and overlined, respectively, in Equation 1. Our second contribution is noting that only the Exponential Divisor Temperature contributes to the OOD score being in adherence with this property of highly negative on ID inputs and close to \(0\) on OOD inputs, while the Forefront Temperature Constant counteracts that property - we therefore ablate this Forefront Temperature Constant. For a detailed explanation on this rationale, see Appendix Section A.2. Our final score (including this ablation) is as follows:

\[(x_{i};L,T_{},)=-_{c=1}^{C}e^{L_{c} (x_{i};,T_{})}\]

We visualize the effects of this ablation in Figure 1, using Places365 (Zhou et al., 2018) as the OOD dataset, CIFAR-100 (Krizhevsky, 2009) as the ID dataset, and a ResNet-20 (He et al., 2016) trained with learned temperature and a cosine logit head as the model.

For the limitations and failure cases of our method, see Appendix Section A.4.

## 4 Experiments

In Appendix Section D, we provide intuition-building evidence to suggest that our method is able to detect OOD samples effectively (as will be shown below) despite not being exposed to explicit OOD samples at training time as a result of being exposed to misclassified ID examples in low densities of the training data during training - and then treating OOD samples at test time as akin to misclassified ID examples in low-density regions of the training data.

### Classification Experiments

In Appendix Section B.1, we explain the experimental setup of our OOD evaluations in classification. This includes the datasets used, model architectures, training details, and hyperparameters - as well as our reasons for our choices of competitive methods.

In Table 1, we compare against competitive OOD methods outlined in Section B.1.3. All results are averaged across 4 OOD test datasets per ID dataset outlined in Section B.1.1. All OOD methods keep accuracy within \(1\%\) of their respective baseline methods without any modifications to account for OOD. We note that our method achieves an average reduction in FPR@95 of \(25.90\%\) on CIFAR-10, \(26.55\%\) on CIFAR-100, and \(77.84\%\) on ImageNet.

In Appendix Section C.2, we present evaluation studies in the presence of the inclusion of the Forefront Temperature Constant, Gradient Input Perturbation (Liang et al., 2017), the use of an Inner Product logit function instead of Cosine Similarity logit function, and an alternative architecture.

### Semantic Segmentation Experiments

In Appendix Table 11, we compare against competitive OOD Detection methods in semantic segmentation that predict which pixels correspond to object classes not found in the training set (i.e. which pixels correspond to OOD objects). To implement our method, we replace the Inner Product per-pixel in the final convolutional layer with a Cosine Logit head per-pixel and a learned temperature layer per-pixel - then compute OOD scores per-pixel. Further experimental details can be found in

  \(D_{thr}^{test}\) &  &  &  \\  Method & FPR@95 \(\) & AUROC \(\) & FPR@95 \(\) & AUROC \(\) & FPR@95 \(\) & AUROC \(\) \\  MSP & \(60.5 15\) & \(89.5 3\) & \(82.7 11\) & \(71.9 7\) & \(63.9 8\) & \(79.2 5\) \\ ODIN & \(39.1 24\) & \(92.4 4\) & \(73.3 31\) & \(75.3 13\) & \(72.9 7\) & \(82.5 5\) \\ Mahalanobis & \(37.1 35\) & \(91.4 6\) & \(63.9 16\) & \(85.1 5\) & \(81.6 19\) & \(62.0 11\) \\ Gradient Norm & \(28.3 24\) & \(93.1 6\) & \(56.1 38\) & \(81.7 13\) & \(54.7 7\) & \(86.3 4\) \\ DNN & \(49.0 11\) & \(83.4 5\) & \(66.6 13\) & \(78.6 5\) & \(61.9 6\) & \(82.9 3\) \\ GODIN & \(26.8 10\) & \(94.2 2\) & \(47.0 7\) & \(90.7 2\) & \(52.7 5\) & \(83.9 4\) \\ Energy & \(39.7 24\) & \(92.5 4\) & \(70.5 32\) & \(78.0 12\) & \(71.0 7\) & \(82.7 5\) \\ Energy + ReAct & \(39.6 15\) & \(93.0 2\) & \(62.8 17\) & \(86.3 6\) & \(31.4\)* & \(92.9\)* \\ Energy + DICE & \(20.8 1\) & \(95.2 1\) & \(49.7 1\) & \(87.2 1\) & \(34.7\)* & \(90.7\)* \\ Energy + ASH & \(20.0 21\) & \(95.4 5\) & \(37.6 34\) & \(89.6 12\) & \(16.7 13\) & \(96.5 2\) \\ LINE & \(14.71\)** & \(96.99\)** & \(35.67\)** & \(88.67\)** & \(20.7\)* & \(95.03\)* \\  AbeT & \(\) & \(\) & \(\) & \(\) & \(40.0 11\) & \(91.8 3\) \\ AbeT + ReAct & \(\) & \(\) & \(\) & \(\) & \(38.1 11\) & \(92.2 3\) \\ AbeT + DICE & \(\) & \(\) & \(\) & \(\) & \(30.7 15\) & \(93.2 3\) \\ AbeT + ASH & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  
* Results where a ResNet-50 is used as in their corresponding papers instead of ResNet-101 as in our experiments. This is due to our inability to reproduce their results with ResNet-101 ****** Results where a DenseNet is used as in their corresponding papers instead of ResNet-20 as in our experiments.

Table 1: **Comparison with other competitive OOD detection methods in classification.** OOD detection results on a suite of standard datasets compared against competitive methods which are trained with ID data only and require only one stage of training. All results are averaged across 4 OOD datasets, with the standard deviations calculated across these same 4 OOD datasets. \(\) means higher is better and \(\) means lower is better.

Appendix Section E. Notably, our method results in both a decrease in FPR@95 of \(41.48\%\) and an increase in AUPRC of \(34.20\%\) on average compared to previous state of the art.

We also present visualizations of pixel-wise OOD predictions for our method and methods against which we compare on a selection of images from OOD datasets in Figure 2.

### Object Detection Experiments

In Appendix Table 12, we compare against competitive OOD Detection methods in object detection. For our experiments with AbeT, the learned temperature and Cosine Logit Head are directly attached to a FasterRCNN classification head's penultimate layer as described Section 2.3 - OOD scores are then computed per-box. Further experimental details can be found in Appendix Section F.2.3. We note that our method shows improved performance on ID AP 3, AUROC, and AUPRC with comparable performance on FPR@95 4.

## 5 Conclusion

Inferences on examples far from a model's training set tend to be significantly less performant than inferences on examples close to its training set. Moreover, even if a model is calibrated on a holdout ID dataset, the confidence scores of these inferences on OOD examples are typically miscalibrated (Ovadia et al., 2019). In other words, not only does performance drop on OOD examples - users are often completely unaware of these performance drops. Therefore, detecting OOD examples in order to alert users of likely miscalibration and performance drops is one of the biggest hurdles to overcome in AI safety and reliability. Towards detecting OOD examples, we have introduced AbeT which mixes a learned temperature (Hsu et al., 2020) and an energy score (Liu et al., 2020) in a novel way with an effective ablation. We have established the superiority of AbeT in detecting OOD examples in classification, detection, and segmentation. We have additionally provided visual and empirical evidence as to why our method is able to achieve superior performance via exposure to misclassified ID examples during training time. Future work will explore if such exposure drives the performance of other OOD methods which do not train on OOD samples - as is suggested by our finding shown in Appendix Section A.4 that _all_ tested OOD detection methods' failures were concentrated on misclassified ID examples. Additional follow-up work includes extending our method to the LLM setting as in LeVine et al. (2023) and the VLM setting as in LeVine et al. (2023).

Figure 2: Qualitative comparison of OOD scores for semantic segmentation. The top row and bottom row contain examples from the datasets RoadAnomaly (Lis et al., 2019) and LostAndFound (Pinggera et al., 2016), respectively. Pixels corresponding to OOD objects are highlighted in red in each image in the leftmost column, which are cropped to regions where we have ID/OOD labels. Scores for each example (row) and technique (column) are thresholded at their respective 95% True Positive Rate and then normalized \(\) in the red channel, with void pixels (which have no ID/OOD label) set to 0. Bright red pixels represent high OOD scores, which should cover the same region as the pixels which correspond to OOD objects in the leftmost column. We invert the scores of Standardized Max Logit, Max Logit, and MSP to allow these methods to highlight OOD pixels in red.