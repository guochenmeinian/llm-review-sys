# A Computationally Efficient Sparsified Online Newton Method

Devvrit

Department of Computer Science

The University of Texas at Austin

devvrit.03@gmail.com

&Sai Surya Duvvuri

Department of Computer Science

The University of Texas at Austin

subramanyamdvss@gmail.com

&Rohan Anil

Google DeepMind

rohananil@google.com

&Vineet Gupta

Google

vineet@google.com

&Cho-Jui Hsieh

CS Department, UCLA & Google

chohsieh@cs.ucla.edu

&Inderjit Dhillon

Google

isd@google.com

equal contribution, \({}^{}\) Work done while at Google

###### Abstract

Second-order methods hold significant promise for enhancing the convergence of deep neural network training; however, their large memory and computational demands have limited their practicality. Thus there is a need for scalable second-order methods that can efficiently train large models. In this paper, we introduce the Sparsified Online Newton (SONew) method, a memory-efficient second-order algorithm that yields a sparsified yet effective preconditioner. The algorithm emerges from a novel use of the LogDet matrix divergence measure; we combine it with sparsity constraints to minimize regret in the online convex optimization framework. Empirically, we test our method on large scale benchmarks of up to 1B parameters. We achieve up to \(30\%\) faster convergence, \(3.4\%\) relative improvement in validation performance, and \(80\%\) relative improvement in training loss, in comparison to memory efficient optimizers including first order methods. Powering the method is a surprising fact - imposing structured sparsity patterns, like tridiagonal and banded structure, requires little to no overhead, making it as efficient and parallelizable as first-order methods. In wall-clock time, tridiagonal SONew is only about \(3\%\) slower per step than first-order methods but gives overall gains due to much faster convergence. In contrast, one of the state-of-the-art (SOTA) memory-intensive second-order methods, Shampoo, is unable to scale to large benchmarks. Additionally, while Shampoo necessitates significant engineering efforts to scale to large benchmarks, SONew offers a more straightforward implementation, increasing its practical appeal. SONew code is available at: https://github.com/devrrit/SONew

## 1 Introduction

Stochastic first order methods which use the negative gradient direction to update parameters have become the standard for training deep neural networks (DNNs). Gradient-based preconditioning involves finding an update direction, by multiplying the gradient with a preconditioner matrix carefully chosen from gradients observed in previous iterations, to improve convergence. (Fullmatrix) Adagrad , online Newton method  and natural gradient descent  use a full-matrix preconditioner, but computing and storing the full matrix is infeasible when there are millions of parameters. Thus, diagonal versions such as diagonal Adagrad, Adam , and RMSprop  are now widely used to train DNNs due to their scalability.

Several higher-order methods have previously been applied to deep learning ([24; 5; 23; 38]). All these methods use Kronecker product factorizations that reduce computational and storage costs to make them feasible for training neural networks. However, to precondition a \(d_{1} d_{2}\) parameter matrix, these methods require matrix inverse operations, which take \((d_{1}^{3}+d_{2}^{3})\) time and \((d_{1}^{2}+d_{2}^{2})\) space. In comparison, first-order methods use \((d_{1}d_{2})\) time and memory, which is linear in the number of parameters. For instance, when \(d_{1}=kd_{2}\), the memory used by Shampoo, \(d_{1}^{2}+d_{2}^{2}\) floating point numbers is \((k)\) times the number of parameters, which could be arbitrarily large depending on \(k\). This calls for further research in developing efficient second-order optimization techniques to train DNNs with memory and time complexity linear in the number of parameters.

In this paper, we present a novel Sparsified Online Newton (SONew) method, which only requires linear time and space complexity, to train large-scale DNNs. We derive the algorithm through two steps, classical regret analysis followed by a sparsification step. In more detail, regret analysis when using a preconditioner reveals that the error is bounded by two terms, the first depends on the change in the preconditioning matrix, while the second depends on the generalized gradient norm (see Section 3 for more details). We take a novel approach of minimizing the second term while regularizing two successive preconditioners to be close in the LogDet matrix divergence measure  (see Section 3 for the intuition behind choosing LogDet divergence). This analysis naturally yields us an Online Newton method . To make it computationally efficient, we further sparsify the preconditioner by finding a sparse approximation that is close in LogDet divergence. Thus we are consistent in using the same measure (LogDet divergence) in both the regularization and sparsification steps. This gives us our SONew method, which achieves linear complexity by leveraging structured sparsity patterns, such as tridiagonal and banded, in the preconditioner. This is unlike most existing online Newton methods that require quadratic space and cubic time complexity. By making each step linear time, the SONew method can be applied to train modern DNNs as efficiently as first order methods. Further, our method is embarrassingly parallelizable thus making negligible the overhead of computing the preconditioner. We also show that introducing sparsity allows us to reduce the condition number of the problem dynamically to improve numerical stability.

We strengthen the relationship between sparse LogDet divergence minimization and online convex optimization by establishing an optimal \(()\) regret upper bound for tridiagonal sparsity pattern. In our experiments on an MLP Autoencoder and Graph Neural Network (GNN), we found that our method outperformed first-order methods in terms of training loss within the same training time, while Shampoo (second-order method) takes significantly longer. In our experiments on Vision Transformers on Imagenet and GNN on OGBG-molpcba, we achieve a target validation performance using 10% and 30% fewer iterations respectively compared to Adam, the SOTA optimizer for both benchmarks. Furthermore, using the same number of iterations as Adam we observe 0.7% and 3.4% relative improvement for ViT and GNN respectively in validation performance. From an optimization point of view, SONew achieves 9% and 80% better relative training loss for ViT and GNN respectively. It is worth noting that Shampoo statistics required \( 7\#params\) for ViT whereas tridiag-SONew uses only \(2\#params\) for its statistics. We also test another recently proposed memory efficient second order optimizer, rfdSON , but found its performance suboptimal to the best performing first order method. Owing to SONew's scalability, we train a Large Language Model (LLM) with 1 billion parameters and compare it with AdaFactor , a popularly used first order optimizer to train LLMs . SONew achieves the same performance as AdaFactor using \(26\%\) fewer steps, resulting in a \(1.35\) faster training. When using the same number of steps, SONew obtained a \(1.7\%\) relative better train loss. In terms of implementation, SONew is just a few lines of code (Equation (13)) without complex engineering challenges, rendering it even more useful and practical.

## 2 Background

The inner product between matrices is defined as \( A,B=(A^{T}B)\), where \((.)\) denotes the matrix trace. The Frobenius norm of a matrix \(A\) is \(\|A\|_{F}=(A^{T}A)}\), while its spectral norm is \(\|A\|_{2}=_{x}\|Ax\|_{2}/\|x\|_{2}\). We use \(I_{n}^{n n}\) to denote an identity matrix. We use \(S_{n}\), \(S_{n}^{++}\) to denote the set of symmetric, and positive definite matrices respectively. The generalized norm of a vector \(x^{n}\) with respect to matrix \(A S_{n}^{++}\) is defined as \(\|x\|_{A}=Ax}\). We use \(\) to denote the determinant of matrix \(A\), and \((A)\) to denote the diagonal matrix with \((A)_{ii}=A_{ii}\). We use \(\) and \(}\) to denote a graph and its sub-graph with a vertex set \([n]=\{1,,n\}\). Let \(E_{}}\) denote set of edges in graph \(}\), and \(_{}(i)\) denote neighbours of vertex \(i\) in graph \(\). A sparse symmetric matrix \(A^{n n}\) follows a sparsity structure graph \(\) if \(A_{i,j}=0\)\((i,j) E_{}}\),. Note that set of all such matrices form a linear subspace. We use \(S_{n}()^{++}\) to denote the set of positive definite matrices with sparsity structure given by graph \(\), i.e, if \(X S_{n}()^{++}\), then \(X_{ij}=0\)\((i,j) E()\). \(S_{n}()^{++}\) is an open convex set. Given an index set \(I=\{i_{1},i_{2},..,i_{n}\}\), we use \(A_{II}\) to denote the corresponding principal sub-matrix of \(A\).

### LogDet matrix divergence

Let \(:S_{n}^{++}\) be a strictly convex, differentiable function. The Bregman matrix divergence between \(X,Y S_{n}^{++}\) is defined as [8; 34]: \(_{}(X,Y)=(X)-(Y)-((Y)^{ T}(X-Y))\). Since \(\) is convex, \(_{}(X,Y) 0\) for all \(X,Y S_{n}^{++}\). For example if \((X)=\|X\|_{F}^{2}\), the corresponding Bregman divergence \(_{}(X,Y)=\|X-Y\|_{F}^{2}\) is the squared Frobenius distance. In this paper, we extensively use the convex function \((X)=-\); the corresponding divergence measure \(_{}(X,Y)\) is called the _LogDet matrix divergence_:

\[_{}(X,Y)=-)}+ (XY^{-1})-n.\] (1)

The LogDet divergence is scale invariant to invertible matrices \(A\), i.e. \(_{}(A^{T}XA,A^{T}YA)=_{ }(X,Y)\). LogDet divergence can be written in terms of eigendecompositions of \(X=V V^{T}\) and \(Y=U U^{T}\):

\[_{}(Y)\!=\!_{i}\!_{j}\!(v_{i} ^{T}u_{j})^{2}\!(_{i}/_{j}\!-\!(_{i}/_{j})\!-\!1).\] (2)

These two properties are later used in Section 3 to highlight the significance of LogDet divergence in our algorithm.

## 3 SONew: Sparsified Online Newton Method

We now present our proposed algorithm SONew.

### Regret minimization via LogDet divergence

We set up our problem under the online convex optimization framework (OCO) [43; 26], where at each round the learner makes a prediction \(w_{t}\) and receives a convex loss \(f_{t}(w_{t})\) and gradient \(g_{t}= f_{t}(w_{t})\) as feedback. The goal of the learner is to reduce regret \(R_{T}\) by predicting \(w_{t}\) so that a low aggregate loss \(_{t=1}^{T}f_{t}(w_{t})\) is achieved compared to the best possible, \(w^{*}=_{w}_{t=1}^{T}f_{t}(w)\). Formally, regret is given by

\[R_{T}(w_{1},,w_{T})=_{t=1}^{T}f_{t}(w_{t})-_{t=1}^{T}f_{t}(w^{*}).\]

Using , \(R\) regret in online setting yields \(R/T\) convergence rate in the stochastic setting. To upper bound this regret, we proceed as in  by analyzing the error in the iterates for the update \(w_{t+1} w_{t}- X_{t}g_{t}\), where \(X_{t}^{n n}\). Then \(\|w_{t+1}-w^{*}\|_{X_{t}^{-1}}^{2}=\|w_{t}- X_{t}g_{t}-w^{ *}\|_{X_{t}^{-1}}^{2}=\|w_{t}-w^{*}\|_{X_{t}^{-1}}^{2}+ ^{2}g_{t}^{T}X_{t}g_{t}-2(w_{t}-w^{*})^{T}g_{t}\). The convexity of \(f_{t}\) implies that \(f_{t}(w_{t})-f_{t}(w^{*})(w_{t}-w^{*})^{T}g_{t}\) leading to \(f_{t}(w_{t})-f_{t}(w^{*})(\|w_{t}-w^{*}\|_{X_{t} ^{-1}}^{2}-\|w_{t+1}-w^{*}\|_{X_{t}^{-1}}^{2}+^{2}g_{t}^{T}X_{t}g _{t})\). Summing over all \(t[T]\) and rearranging reveals the following upper bound on overall regret:

\[R_{T}\|w_{1}-w^{*}\|_{X_{1}^{-1}}^{2}+_{t=1}^{T}g_{t}^{T}X_{t}g_{t}+_{t=2}^{T}(w_{t}-w^{*} )^{T}(X_{t}^{-1}-X_{t-1}^{-1})(w_{t}-w^{*}).\] (3)Since \(w^{*}\) is unknown, finding \(X_{t}\) which minimizes (3) is infeasible. So to minimize regret, we attempt to minimize the second term in (3) while regularizing \(X_{t}^{-1}\) to be "close" to \(X_{t-1}^{-1}\). The nearness measure we choose is the LogDet matrix divergence, thus leading to the following objective

\[X_{t}=*{arg\,min}_{X S_{}^{++}}g_{t}^{T}Xg_{t}, *{such\,\,that}\,_{}(X,X_{t-1} ) c_{t},\] (4)

where \(_{}\) is as in (1). Why do we use the LogDet divergence? From (2), due to the term \(_{i}/_{j}\), \(_{}(X,X_{t-1})\) prioritizes matching the smaller eigenvalues of \(X_{t-1}\) with those of \(X\), i.e., matching the larger eigenvalues of \(X_{t-1}^{-1}\) and \(X^{-1}\). As a consequence, LogDet divergence regularizes \(X\) by matching up its large eigenvalues with those of \(X_{t-1}\). For example if smallest and largest eigenvalue of \(X_{t-1}\) are \(_{n}\) and \(_{1}\), then for an eigenvalue \(\) of \(X\), when \(>_{n},\ _{1}\), the penalty from (2) for \(_{n}\) is higher than for \(_{1}\), \((/_{n}-(/_{n})-1)>(/_{1}-(/ _{1})-1)\). This intuition leads us to formulate (4) as our objective. We recall that there is precedence of using the LogDet divergence in the optimization literature; indeed the celebrated BFGS algorithm [9; 17; 22; 44] can be shown to be the unique solution obtained when the LogDet divergence between successive preconditioners, subject to a secant constraint, is minimized (as shown in the 4-page paper by ).

The optimization problem in (4) is convex in \(X\) since the LogDet divergence is convex in its first argument. The Lagrangian \((X,_{t})=g_{t}^{T}Xg_{t}+_{t}(_{ }(X,X_{t-1})-c_{t})=(Xg_{t}g_{t}^{T})+_{t}( -(XX_{t-1}^{-1})+(XX_{t-1}^{-1})-n))- _{t}c_{t}\). Setting \((X,_{t})=0\), and using the fact that \((X)=X^{-1}\) we get the following update rule:

\[X_{t}^{-1}=X_{t-1}^{-1}+g_{t}g_{t}^{T}/_{t}.\] (5)

We _emphasize_ that the update rule (5) arises naturally from our novel use of LogDet divergence to minimize the regret. Moreover, Equation (5) can be seen as a general update rule applicable to numerous existing optimizers. For example, setting \(c_{t}=0\) (equivalently \(_{t}=\)) \( t[n]\) in (4) results in no change to the preconditioner in any round. In this case, with \(X_{0}=I_{n}\), we get online gradient descent . On the other hand, setting \(_{t}=1\) gives the update rule of the online Newton method . Our update rule differs from (full-matrix) Adagrad  which has \(X_{t}^{-2}=X_{t-1}^{-2}+g_{t}g_{t}^{T}\).

Maintaining and updating \(X_{t}\) as in (5) is possible by using Sherman-Morrison formula but requires \((n^{2})\) storage and time complexity. This becomes impractical when \(n\) is in the order of millions which is typically the case in DNNs.

### Sparsifying the Preconditioner

To minimize the memory needed for maintaining and updating \(X_{t}\) using (5), we adopt the strategy of sparsifying the preconditioner. For existing optimizers such as (full-matrix) Adagrad or the Online Newton method, it is unclear how to sparsify a given preconditioner. Specifically, there is no intuitive approach to assessing the quality of a sparse preconditioner compared to a full-matrix preconditioner. However, since our update rule (5) originates from using LogDet divergence in the regret bound analysis, it gives us a natural metric to measure the quality of a sparse preconditioner. Let's consider the following problem: find a sparse positive definite \(X\) with \(\|X\|_{0} n\), \(>1\), such that the objective \(_{}(X,(X_{t-1}^{-1}+g_{t}g_{t}^{T}/_{t})^{-1})\) is minimized. Essentially, this problem imposes a sparsity constraint while requiring the sparse preconditioner to remain close to the full-matrix preconditioner in terms of LogDet divergence.

Due to the \(L_{0}\)-norm constraint, this is a non-convex problem, which makes it difficult to solve exactly. Since \(L_{1}\)-norm serves as a convex relaxation for the \(L_{0}\) norm, we could use it instead, resulting in the following optimization problem also known as graphical lasso estimator :

\[_{X S_{}^{++}}_{}(X,(X_{t-1} ^{-1}+g_{t}g_{t}^{T}/_{t})^{-1})+\|X\|_{1}.\]

However, the time taken to solve the above problem, even with the current best methods [7; 29; 16; 53], can still be too large (as these methods take several minutes for a matrix of size million), making it impractical to embed in DNN training.

In this paper, we take a different direction where we use fixed sparsity pattern constraints, specified by a fixed undirected graph \(\). To sparsify the solution in (5), we formulate the subproblem

\[X_{t}=*{arg\,min}_{X S_{n}()^{++}}_{}(X,(X_{t-1}^{-1}+g_{t}g_{t}^{T}/_{t})^{-1}),\] (6)where \(S_{n}()^{++}\) denotes the set of positive definite matrices with the fixed sparsity pattern corresponding to the adjacency matrix of graph \(\). Note that both steps (4) and (6) use the same LogDet measure.

Owing to the structure of LogDet divergence, (6) can be surprisingly solved in \((n)\) and easily parallelizable, for certain sparsity structures \(\). Algorithm 1 and 2 presents an instantiation of the proposed SONew method, which solves (6) using \((n)\) time and memory for banded matrices with band size \(b\). In particular a tridiagonal matrix, corresponding to a chain graph, is a banded matrix with bandsize 1.

``` Inputs:\(_{t}:=\) coefficient in the update (10), \(:=\) sparsity graph (banded/tridiagonal), \(:=\) damping parameter, \(T:=\) total number of iterations/mini-batches, \(_{t}:=\) step size/learning rate. Output:\(w_{T+1}\)
1:\(H_{0}= I_{d}\), \(w_{1}=0\)
2:for\(t\{1,,T\}\)do
3: compute \(g_{t}= f_{t}(w_{t})\)
4:\(H_{t} H_{t-1}+P_{}(g_{t}g_{t}^{T}/_{t}) S_{n}( )\) with \(P_{}\) as in (8). \(\ (n)\) time & memory
5: Get \(L,D=\) Sparsified_Inverse (\(H_{t},\)), where \(X_{t}=LDL^{T}\) solves (11).
6: Compute descent direction \(u_{t}=LDL^{T}g_{t}\).
7:\(w_{t+1}=w_{t}-_{t}u_{t}\)
8:endfor
9:return\(w_{T+1}\) ```

**Maintaining \(H_{t} S_{n}()\) in line 4**. Solving the subproblem in (6) naively is impractical since \(X_{t-1}^{-1}\) is a dense matrix. However, the structure of the LogDet divergence comes to the rescue; the optimization problem in (6) can be expanded as follows:

\[*{arg\,min}_{X S_{n}()^{++}}- (X)+(X(X_{t-1}^{-1}+g_{t}g_{t}^{T}/_{t})).\] (7)

Let us define the projection onto \(S_{n}()\), \(P_{}:^{n n}^{n n}\) as:

\[P_{}(M)_{ij}=M_{ij}&(i,j) E_{}, \\ 0&.\] (8)

Note that the \((.)\) term in (7) is dependent only on the non-zero elements of \(X S_{n}()^{++}\), since \((AB)= A,B\), for symmetric matrices \(A\) and \(B\). Hence, (7) can be written as

\[*{arg\,min}_{X S_{n}()^{++}}- (X)+ X,P_{}(X_{t-1}^{-1}+g_{t}g_{t}^{T}/ _{t}),\] (9)

Computing the entire matrix \(X_{t-1}^{-1}\) can be avoided by analyzing the optimality condition of (9). Let \(g(X)=-(X)+ X,P_{}(X_{t-1}^{-1}+g_{t}g_{t }^{T}/_{t})\) denote the objective function in (9), then the optimality condition of (9) is \(P_{}( g(X))=P_{}((-(X)+ X,P _{}(X_{t-1}^{-1}+g_{t}g_{t}^{T}/_{t}))=0\), since gradients with respective nonzero entries of \(X\) should be zero, \(}=(_{X}(g(X)))_{i,j}=0\), \((i,j) E_{}\). Using \((-(X))=-X^{-1}\), \(_{X}( X,Y)=Y\), and setting \(X=X_{t}\) gives:

\[P_{}(X_{t}^{-1})-P_{}(X_{t-1}^{-1}+g_{t}g_{t}^{T}/ _{t})=0,\]

\[H_{t}=H_{t-1}+P_{}(g_{t}g_{t}^{T}/_{t}),H_{t}=P_{ }(X_{t}^{-1})\] (10)

Thus we only need to maintain \(H_{t}=P_{}(X_{t}^{-1})\). This matrix is updated as \(H_{t}=H_{t-1}+P_{}(g_{t}g_{t}^{T}/_{t})\). Since \(H_{t} S_{n}()\), the update can be done in \((|E_{}|)\) memory and time, while computing the matrix \(X_{t}^{-1}\) would have cost \((n^{2})\). In SONew (Algorithm 1), this key observation is used to maintain \(H_{t}\) in line 4.

**Computing \(X_{t}\) in line 5**. Now that \(H_{t}\) is known at every round \(t\), we can replace \(P_{}(X_{t-1}^{-1}+g_{t}g_{t}^{T}/_{t})\) in (9) with \(H_{t}\) as:

\[X_{t}=*{arg\,min}_{X S_{n}()^{++}}-+ (XH_{t}).\] (11)

For an arbitrary graph \(\), solving (11) might be difficult. Theorems 3.1 and 3.2 show _embarrassingly parallelizable_ explicit solutions to the subproblem (11) for tridiagonal and banded sparsity patterns.

**Theorem 3.1** (Explicit solution of (11) for tridiagonal structures/chain graph).: _Let the sparsity structure \(\) be a chain with edges \(E_{}=\{(i,j):|i-j| 1,1 i,j n\}\). Also, let \(H S_{n}()\) be such that any submatrix of \(H\) corresponding to a complete subgraph of \(\) is positive definite, then the solution of (11) is given by \(=LDL^{T}\), where the unit lower bidiagonal matrix \(L\) and diagonal matrix \(D\) have the following non-zero entries:_

\[L_{jj}=1,\ L_{j+1j}=-}{H_{j+1j+1}},\ \ D_{jj}^{-1}=H_{jj}-^{2}}{H_{j+1j+1}}, j n-1\ \&\ D_{nn}^{-1}=H_{nn}\] (12)

Computing this explicit solution involves conducting paralellizable operations on \(2 2\) principle submatrices (highlighted in red) of the tridiagonal matrix \(H\) to find the \(\) as shown in the following \(3 3\) example:

\[H= (}&H_{12}\\ H_{21}&H_{22}\\  0&H_{32})=(_{11}& _{12}&0\\ _{21}&_{22}&_{23}\\ 0&_{32}&_{33})+(g_ {1}^{2}&g_{1}g_{2}&0\\ g_{1}g_{2}&g_{2}^{2}&g_{2}g_{3}\\ 0&g_{2}g_{3}&g_{3}^{2})\] (13) \[=(&0&0\\ }{H_{22}}&1&0\\  0&-}{H_{33}}&1)( -}{H_{22}}}&0&0\\  0&H_{22}-^{2}}{H_{33}}&0\\ 0&0&H_{33})(&- }{H_{22}}&0\\  0&1&-}{H_{33}}\\ 0&0&1)\]

Conducting these operations take \((n)\) time and memory complexity, and similarly the descent direction can be found sequentially by \(X_{t}g_{t}=L(D(L^{T}g_{t}))\), which can take \((n)\) time complexity, due to unit lower bidiagonal structure of \(L\), furthermore, these operations can be easily parallelized. We also generalize the explicit solution to banded sparsity structures with band size \(b\).

**Theorem 3.2** (Explicit solution of (11) for banded structures).: _Let the sparsity pattern \(\) be a banded matrix of band size \(b\), i.e. \(E_{}=\{(i,j):|i-j| b,1 i,j n\}\). For every vertex \(j\), let \(I_{j}=\{j+1,,j+b\}\). Then \(X_{t}=LDL^{T}\) is the solution of (11) with nonzero entries of \(L\) and \(D\) defined as follows :_

\[L_{jj}=1,\ L_{I_{j}j}=-H_{I_{j}I_{j}}^{-1}H_{I_{j}j}, D_{jj}^{-1}=(H_{jj} -H_{I_{j}j}^{T}H_{I_{j}I_{j}}^{-1}H_{I_{j}j}),\ 1 j n.\] (14)

_where, \(H S_{n}()\) any submatrix of \(H\) corresponding to a complete subgraph of \(\) is positive definite._

Note that Theorem 3.1 is a special case of Theorem 3.2 when \(b\) is set to \(1\), and the proof for Theorem 3.2 is given in Appendix A.1. Computing the above solution requires solving \(n\) linear systems of size \(b\) (which is small) as shown in Algorithm 2, and takes \(((n-b+1)b^{3})\) flops. Since \(b n\), the number of flops is \((n)\).

### Regret bound analysis of SONew

The following theorem establishes optimal regret guarantee  for SONew in the online convex optimization framework mentioned in Section 3.1.

**Theorem 3.3**.: _When \(=\) tridiagonal/chain graph as defined in Theorem 3.1, then setting \(= G_{}\), \(_{t}=G_{}\) and \(_{t}=}{}\) in Algorithm 1, where \(\|w_{t}-w^{*}\|_{2} D_{2}\), \(\|g_{t}\|_{} G_{}\) incurs a regret \(R_{T}=(G_{}D_{2})\)._

The proof sketch involves deriving an explicit expression for entries of \(X_{t}^{-1}\) in Lemma A.2, to upper bound the term \((w_{t}-w^{*})^{T}(X_{t}^{-1}-X_{t-1}^{-1})(w_{t}-w^{*})\) in regret upper bound (3). Upper bounding \(_{t=1}^{T}g_{t}^{T}X_{t}g_{t}\) involves using the Loewner order \(X_{t}\|X_{t}\|_{2}I_{n}\|X_{t}\|_{}I_{n}\). A detailed proof sketch and proof is given in Appendix A.2. We note that though the regret bound presented here is for convex losses, there are connections to non-convex convergence guarantees by using OCO (online convex optimization) learners, presented in Appendix A.2.5. While our main focus is on deep neural network training, which is typically non-convex, we also conducted convex experiments in Table 9.

### Numerical Stability of SONew

In Theorem 3.1 and Theorem 3.2, as mentioned, any submatrix of \(H_{t}\) corresponding to a complete subgraph of \(\) should be positive definite, however, in practice, due to finite precision, each entry of \(H\) is inherently perturbed with an error proportional to \((_{mach})\), where \(_{mach}\) is machine epsilon . We notice in practice that the subtraction operation in \(D_{jj}^{-1}=S_{jj}=H_{jj}-H_{j+1j}^{2}/H_{j+1j+1}\) (line 7 Algorithm 2), which has a condition number \(_{sub}=|H_{jj}|/|S_{jj}|\), can be high as \(S_{jj}\) can be arbitrarily low due to near singular submatrices \(H_{ii}&H_{ii+1}\\ H_{i+1i}&H_{i+1i+1}\). Thus small perturbation in \(H\) can lead to high perturbations in the preconditioner \(\). We formalize this notion by deriving an end-to-end componentwise condition number (pg. 135, problem 7.11 in ) of Sparsified_Inverse in Theorem A.10,Appendix A.3. To reduce this condition number upper bound and be robust to perturbations in \(H_{t}\) caused by finite precision, for a tridiagonal graph \(\), we can remove edges \((j,j+1)\) which correspond to low \(S_{jj}<\), where \( 0\) denotes a tolerance parameter. We show in Theorem A.11,Appendix A.3 that this reduces the condition number upperbound of Sparsified_Inverse. Furthermore, we generalize this to banded sparsity pattern in Algorithm 3,Appendix A.3.

## 4 Related Work

Online Newton method is a second order method in online convex optimization framework with properties such as scale invariance  and logarithmic regrets in exp-concave and strongly convex functions [25; 26]. However, it has a time complexity of \((n^{2})\), making it infeasible for large \(n\). However, introduction of LogDet divergence measure in SONew allows us to set different sparsity graphs as \(\) such as banded graph with band-size \(b\), for which our preconditioning process is more computationally efficient with a time complexity of \((b^{3}(n-b+1))\) compared to online-newton method \((n^{2})\).

Shampoo [24; 5] approximates full gradient statistics matrix using Kronecker factored preconditioners to reduce the memory and time complexity from \((n^{2})\) to \((d_{1}^{2}+d_{2}^{2})\) and \((d_{1}^{3}+d_{2}^{3})\) respectively. Here, \(n=d_{1}d_{2}\) denotes number of parameters for a linear layer of dimensions \(d_{1} d_{2}\). The time complexity of matrix inversion takes a heavy toll in Shampoo's compute time even with the Kronecker product assumption on the preconditioner, whereas, our method has a time complexity of \((b^{3}d_{1}d_{2})\) quadratic in dimensions of the linear layer (note that \(b=1\) for tridiagonal structure).

KFAC , similar to Shampoo, uses Kronecker factored preconditioning, but to approximate the Fisher-information matrix. FishLeg  instead approximates the inverse Fisher matrix directly by expressing it in terms of the solution to an optimisation problem. Both these methods have memory and time complexity similar to Shampoo. In this work, we compare with Shampoo among the class of Kronecker factored optimizers due to its widespread testing and adoption within the community . We also point the readers to Eva , a concurrent work aimed at devising memory efficient optimizer by maintaining rank one matrix approximation to the Kronecker factors of KFAC matrices. For completeness, we include comparison with KFAC, FishLeg, and Eva on Autoencoder benchmark.

There is prior work [35; 36] in reducing the complexity - \((n^{2})\) flops of Online Newton Step (ONS) to \((n)\) flops using sketching. These ONS variants maintain a low rank approximation of \(H_{t}\) (as in Algorithm 1) and updating it with a new gradient \(g_{t}\) at every iteration requires conducting SVD /orthonormalization  of a tall and thin matrix in \(^{n r}\), where \(r\) denotes the rank of approximation of \(H_{t}\). In Section 5, we conduct large scale experiments and compare SONew against rfdSON  as it's more stable than Oja-SON .

 
**Optimizer** & **Time complexity** & **Memory complexity** \\  Adam & \((d_{1}d_{2})\) & \(d_{1}d_{2}\) \\ rfdSON(m) & \((m^{2}d_{1}d_{2})\) & \(md_{1}d_{2}\) \\ Shampoo & \((d_{1}^{3}+d_{2}^{3})\) & \((d_{1}^{2}+d_{2}^{2})\) \\ tridiag-SONew & \((d_{1}d_{2})\) & \(2d_{1}d_{2}\) \\ band-4-SONew & \((d_{1}d_{2})\) & \(5d_{1}d_{2}\) \\  

Table 1: Consider preconditioning a \(d_{1} d_{2}\) parameter matrix. Time complexity of tridiag and banded SONew inverse scale linearly with number of parameters, but, Shampoo is cubic in the dimensions of the matrix. Memory used to store second-moments of gradients by tridiag-SONew can be significantly lower than Shampoo, for e.g. if \(d_{1}=4d_{2}\), then Shampoo takes \(>2\) times more memory.

LogDet problem in equation 11 is closely related to the Maximum Determinant Matrix Completion (MDMC) [4; 49]. The MDMC problem is the dual of LogDet problem (11), and has explicit solutions for chordal graphs . Thus the explicit solutions in (14) are the same as the ones proved in . Also, we noticed that the tridiagonal explicit solution has been used previously in KFAC  in the context of a gaussian graphical model interpretation of gradients, specifically, KFAC used a block-tridiagonal preconditioner to incorporate correlation within consecutive layers.

## 5 Experimental Results

We describe our experiments on standard Autoencoder benchmark  trained on MNIST dataset , Vision Transformer  on Imagenet training, GraphNetwork [6; 21] on OGBG-molpcha dataset , and a Large Language Model . For all second order optimizers, we use grafting , a technique used to transfer step size between optimization algorithms. Specifically, given an update \(v_{1}\) of Optimizer-1 and \(v_{2}\) of Optimizer-2, grafting allows to use the direction suggested by Optimizer-2 with step size suggested by Optimizer-1. The final update is given by \(\|}{\|v_{2}\|} v_{2}\). Grafting has been shown to take advantage of a tuned optimizer step size and improve performance. For SONew and rfdSON, we use Adam grafting - using Adam optimizer step size \(\|v_{1}\|\) with SONew/rfdSON direction \(}}{{\|v_{2}\|}}\). For Shampoo, we use its default RMSProp grafting. We couldn't find rfdSON official implementation, so we use our own implementation using which we reproduced the numbers on convex losses (Appendix A.4) reported in their paper .

### Autoencoder benchmark

**Setup:** We use three sparsity patterns for SONew - a) diagonal sparsity, resulting in a diagonal preconditioner similar to adaptive first order methods like Adam and Adagrad; b) tridiagonal sparsity, corresponding to a chain graph; and c) banded sparsity, represented by "band-\(k\)" in tables and figures for band size of \(k\). We compare SONew against widely used first order methods including SGD ), SGD with Momentum , Nesterov , Adagrad , Adam , and Rmsprop . We also compare with rfdSON , a recently proposed memory efficient second order optimizer and with Shampoo , a state of the art second-order optimizer used in practice, albeit with considerable memory and time requirements. Because of space constraint, we report only the best performing first order methods while include the entire set in the appendix. As previously mentioned, rfdSON maintains a low rank approximation of the Online Newton method's statistics matrix \(_{i}g_{i}g_{i}^{T}\). We observed rfdSON with adam grafting always performed better than without grafting, hence report the corresponding numbers. We evaluate rfdSON with rank \(m\) approximation, denoted as rfdSON(\(m\)), which requires \((m+1)*\#params\) space when using grafting. For a fair comparison with tridiag-SONew and band-4-SONew, we test rfdSON with \(m=1\) and \(m=4\), respectively. For shampoo, computing preconditioner at every step could be infeasible, instead it is computed every \(t\) steps - referred to as Shampoo(\(t\)). Section 3.3 compares time and memory complexities of rfdSON, Shampoo, tridiag-SONew, band-4-SONew. Note that \(d_{1}^{2}+d_{2}^{2} 2d_{1}d_{2}\  d_{1},d_{2}\), thus memory used by tridiag-SONew is never more than Shampoo. We use a \(2.72\)M parameters Autoencoder and each experiment is performed using one V100 GPU having \(16\) GB memory. Further setup details are given in Appendix A.4.

**Results:** In Table 2 we observe that among first order methods, diag-SONew performs the best while taking same amount of time. Increasing the number of edges in the sparsity graph to tridiag or banded sparsity with band size \(4\) enhances the performance further. Tridiag-SONew runs \(5\) faster than Shampoo at a marginal cost to the loss - even when Shampoo updates preconditioner once every 20 steps. Using same space, rfdSON performs considerably worse than SONew. To test the numerical stability and robustness of SONew, we reduce the precision to bfloat16 and conduct similar

  
**Optimizer** &  &  \\   & **Adagrad** & **RMSProp** & **Adam** & **diag-SONew** & **Shampoo(20)** & **rfdSON(1)** & **rfdSON(4)** & **tridiag-SONew** & **band-4-SONew** \\  Train CE loss & 54.393 & 53.330 & 53.591 & 53.025 & 50.702 & 56.21 & 55.55 & 51.723 & 51.357 \\  Time(s) & 62 & 62 & 62 & 63 & 371 & 85 & 300 & 70 & 260 \\   

Table 2: **float32 experiments on Autoencoder benchmark. We observe that diag-SONew performs the best among all first order methods while taking similar time. tridiag and band-4 SONew perform significantly better than first order methods while requiring similar linear space and time. Shampoo performs best but takes \((d_{1}^{3}+d_{2}^{2})\) time for computing preconditioner of a linear layer of size \(d_{1} d_{2}\), whereas our methods take \((d_{1}d_{2})\) time, as mentioned in Section 3.3. rfdSON takes similar space as SONew but performs considerably worse.**experiments in Appendix A.4.4 (Table 8 ). We notice that SONew undergoes the least degradation in performance compared to all other optimizers. We refer the reader to Appendix A.4.4 for a thorough comparison and study of bfloat16 experiments. In Figure 3 we plot the loss curves of all the baselines and SONew for float32 experiments. Moreover, in Appendix A.4.1 Table 4 we provide ablation on performance of SONew with varying batch sizes.

**Comparison with other baselines:** We further compare SONew with KFAC , FishLeg , and Eva  for completeness. Since these methods lack a JAX implementation, we adopted the authors' official Pytorch implementations. When we attempted to integrate their code with our Autoencoder benchmark, the results were unsatisfactory; for instance, FishLeg recorded a loss of approximately \( 60.0\). This was notably unexpected as it underperformed Adam, a benchmark that the authors themselves compared against. Given these results and to minimize modifications to the official code, we decided to test our optimizer, SONew, directly on their provided autoencoder architecture. We present the results in Appendix A.4.4 and notice that SONew outperforms these baselines as well by a large margin.

### VIT and GraphNetwork benchmark

**Setup:** We compare tridiag-SONew with Momentum, RMSProp, and Adam, on VIT (\(\)22M parameters) and GraphNetwork (\(\)3.5M parameters) benchmark. For each experiment, we search over \(200\) hyperparameters using \(4\)\(16\) GB TPUs (v2) for each run. In order to conduct a fair comparison of the running times, we executed the optimal hyperparameter configurations on \(4\) 32GB TPUs (v4) . This is because certain operations, including reshapes and transposes, are not optimized on TPUs (v2). Consequently, methods like rfdSON, Shampoo or SONew, which utilize these operations, could potentially be disadvantaged if TPUs (v2) were used, skewing the comparative results. All memory-efficient methods, including rfdSON, first-order methods, and SONew, exhibit similar runtimes, with differences of approximately \( 5\%\). For ViT, we evaluate their performance based on the same number of steps, as this also effectively compares wall-clock time. However, for GraphNetwork, we train Shampoo for 20% fewer steps to achieve a comparable wall-clock time.

**Results:** We plot the runs that give best validation error rate (for VIT) or validation average precision (for GraphNetwork) in Figure 1. tridiag-SONew requires \( 10\%\) less steps to reach the same performance as Adam for VIT, and \( 30\%\) less steps for GraphNetwork benchmark. Training for the same number of steps, we get \( 0.7\%\) better relative validation error for VIT and \( 3.4\%\) better relative validation average precision for GraphNetwork. On GraphNetwork benchmark, tridiag-SONew performs \(1.3\%\) relatively worse in average precision compared to Shampoo, while being \(1.25\) faster. On VIT benchmark, Shampoo doesn't fit in a 16 GB TPU v2. Its statistics require 155M entries (\( 7\#params\)) while tridiag-SONew requires only 44M entries (\(2\#params\)). Hence, we could not tune it. rfdSON takes same memory but slightly more time because of its SVD computations. We also notice rfdSON performs worse than Adam on both benchmarks; we leave a thorough investigation of this behavior as a future work.

We show in Appendix A.4 that corresponding to the best validation runs, tridiag-SONew optimizer's training loss is also less than that of Adam. Furthermore, from an optimization point of view we also show in Appendix A.4 that among all the \(200\) hyperparameter sweeps, the best training loss of tridiag-SONew is \(9\%\) relatively better on ViT and \(80\%\) relatively better on GraphNN than that of Adam. We further compare Adam and tridiag-SONew on a 248M parameter Transformer Model in Appendix A.4.4. In next subsection, we present results on a decoder only large scale Language Model.

### Experiments on Language Models

**Setup:** Owing to SONew's scalability, we test it on a Large Language Model (LLM)  with 1 billion parameters. We compare SONew with AdaFactor (without factoring), a commonly used first order optimizer for training LLMs [51; 11]. AdaFactor is similar to Adam except that in addition it offers benefits like "parameter scaling", which has an effect of layerwise damping of the learning rate. We defer the reader to  for more details. We trained the LLM for 5B tokens with a batch size of \(64k\) tokens. All experiments were performed on \(16\) TPU v4s. To support efficient training of large models, we implemented a sharded tridiag-SONew following model parallelism approach.

**Results:** We report the experiment in Figure 3 where we find that SONew beats Adafactor by a large margin. Specifically, SONew achieves the same log perplexity using \(26\%\) less steps. Moreover, using the same number of tokens, SONew achieves \(1.7\%\) relative better performance on train loss, leading to \(1.35\) speedup. This shows the potential of SONew as a scalable optimizer that can be used to train large models while using second order information.

## 6 Conclusions and Future Work

In this paper we have introduced a novel Sparsified Online Newtwon (SONew) method that yields a computationally efficient sparse preconditioner that can effectively train very large DNNs. The time and memory complexity of SONew is linear in the number of parameters, unlike current Kronecker-factorization based second-order methods for training deep networks. Our experimental results show that SONew uses similar time as first order methods and achieves much better validation and training performance in various benchmarks. In the future, we plan to explore different sparsity graphs for which efficient solutions exist for the LogDet subproblem (11) and develop corresponding regret bound analyses. Some of the limitations of SONew include: 1) explicit solutions akin to Theorem 3.1 & 3.2 need not exist for all sparsity graphs \(\); 2) Not all graphs allow for efficient optimizer implementation; 3) Among graphs permitting efficient optimizer implementation--like tridiagonal sparsity--the ordering of parameters remains unexplored. An alternative ordering might position closely related parameters adjacently, potentially enhancing performance; 4) Comprehensive exploration of methods to scale final updates is needed. While we employ grafting , other techniques, such as clipping [45; 38], merit investigation.