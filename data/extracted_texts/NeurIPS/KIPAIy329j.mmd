# SEGA: Instructing Text-to-Image Models

using Semantic Guidance

 Manuel Brack\({}^{1,2}\) Felix Friedrich\({}^{2,3}\) Dominik Hintersdorf\({}^{2}\) Lukas Struppek\({}^{2}\)

**Patrick Schramowski\({}^{1,2,3,4}\) Kristian Kersting\({}^{1,2,3,5}\)**

\({}^{1}\)German Research Center for Artificial Intelligence (DFKI),

\({}^{2}\)Computer Science Department, TU Darmstadt \({}^{3}\)Hessian.AI,

\({}^{4}\)LAION, \({}^{5}\)Centre for Cognitive Science, TU Darmstadt

brack@cs.tu-darmstadt.de

###### Abstract

Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions.

This semantic guidance (Sega) generalizes to any generative architecture using classifier-free guidance. More importantly, it allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate Sega's effectiveness on both latent and pixel-based diffusion models such as Stable Diffusion, Paella and DeepFloyd-IF using a variety of tasks, thus providing strong evidence for its versatility, flexibility and improvements over existing methods1.

https://huggingface.co/docs/diffusers/api/pipelines/semantic_stable_diffusion

## 1 Introduction

The recent popularity of text-to-image diffusion models (DMs) [28; 25; 27] can largely be attributed to their versatility, expressiveness, and--most importantly--the intuitive interface they provide to users. The generation's intent can easily be expressed in natural language, with the model producing faithful interpretations of a text prompt. Despite the impressive capabilities of these models, the initially generated images are rarely of high quality. Accordingly, a human user will likely be unsatisfied with certain aspects of the initial image, which they will attempt to improve over multiple iterations. Unfortunately, the diffusion process is rather fragile as small changes to the input prompt lead to entirely different images. Consequently, fine-grained semantic control over the generation process is necessary, which should be as easy and versatile to use as the initial generation.

Previous attempts to influence dedicated concepts during the generation process require additional segmentation masks, extensions to the architecture, model fine-tuning, or embedding optimization [1; 8; 13; 35]. While these techniques produce satisfactory results, they disrupt the fast, exploratory workflow that is the strong suit of diffusion models in the first place. We propose Semantic Guidance (Sega) to uncover and interact with semantic directions inherent to the model. Sega requires no additional training, no extensions to the architecture, nor external guidance and is calculated within a single forward pass. We demonstrate that this semantic control can be inferred from simple textual descriptions using the model's noise estimate alone. With this, we also refute previous researchclaiming these estimates to be unsuitable for semantic control . The guidance directions uncovered with Sega are robust, scale monotonically, and are largely isolated. This enables simultaneous applications of subtle edits to images, changes in composition and style, as well as optimizing the artistic conception. Furthermore, Sega allows for probing the latent space of diffusion models to gain insights into how abstract concepts are represented by the model and how their interpretation reflects on the generated image. Additionally, Sega is architecture-agnostic and compatible with various generative models, including latent[27; 26] and pixel-based diffusion.

In this paper, we establish the methodical benefits of Sega and demonstrate that this intuitive, lightweight approach offers sophisticated semantic control over image generations. Specifically, we contribute by (i) devising a formal definition of Semantic Guidance and discussing the numerical intuition of the corresponding semantic space, (ii) demonstrating the robustness, uniqueness, monotonicity, and isolation of semantic vectors, (iii) providing an exhaustive empirical evaluation of Sega's semantic control, and (iv) showcasing the benefits of Sega over related methods.

## 2 Background

**Semantic Dimensions.** Research on expressive semantic vectors that allow for meaningful interpolation and arithmetic pre-date generative diffusion models. Addition and subtraction on text embeddings such as word2vec [18; 19] have been shown to reflect semantic and linguistic relationships in natural language [20; 34]. One of the most prominent examples is that the vector representation of 'King - male + female' is very close to 'Queen'. Sega enables similar arithmetic for image generation with diffusion (cf. Fig. 0(b)). StyleGANs [11; 12] also contains inherent semantic dimensions that can be utilized during generation. For example, Patashnik et al.  combined these models with CLIP  to offer limited textual control over generated attributes. However, training StyleGANs at scale with subsequent fine-tuning is notoriously fragile due to the challenging balance between reconstruction and adversarial loss. Yet, large-scale pre-training is the base of flexible and capable generative models .

**Image Diffusion.** Recently, large-scale, text-guided DMs have enabled a more versatile approach for image generation [28; 25; 2]. Especially latent diffusion models  have been gaining much attention. These models perform the diffusion process on a compressed space perceptually equivalent to the image space. For one, this approach reduces computational requirements. Additionally, the latent representations can be utilized for other downstream applications [6; 17].

**Image Editing.** While these models produce astonishing, high-quality images, fine-grained control over this process remains challenging. Minor changes to the text prompt often lead to entirely different images. One approach to tackle this issue is inpainting, where the user provides additionally semantic masks to restrict changes to certain areas of the image [1; 21]. Other methods involve computationally expensive fine-tuning of the model to condition it on the source image before applying edits [13; 33]. In contrast, Sega performs edits on the relevant image regions through text descriptions alone and requires no tuning.

**Semantic Control.** Other works have explored more semantically grounded approaches for interacting with image generation. Prompt-to-Prompt utilizes the semantics of the model's cross-attention layers that attribute pixels to tokens from the text prompt . Dedicated operations on the cross

Figure 1: Semantic guidance (SEGA) applied to the image ‘a portrait of a king’ (Best viewed in color)

attention maps enable various changes to the generated image. On the other hand, Sega does not require token-based conditioning and allows for combinations of multiple semantic changes. Wu et al.  studied the disentanglement of concepts for DMs using linear combinations of text embeddings. However, for each text prompt and target concept, a dedicated combination must be inferred through optimization. Moreover, the approach only works for more substantial changes to an image and fails for small edits. Sega, in contrast, is capable of performing such edits without optimization.

**Noise-Estimate Manipulation.** Our work is closely related to previous research working directly on the noise estimates of DMs. Liu et al.  combine multiple estimates to facilitate changes in image composition. However, more subtle semantic changes to an image remain unfeasible with this method. In fact, Kwon et al.  argue that the noise-estimate space of DMs is unsuited for semantic manipulation of the image. Instead, they use a learned mapping function on changes to the bottleneck of the underlying U-Net. This approach enables various manipulations that preserve the original image quality. However, it does not allow for arbitrary spontaneous edits of the image, as each editing concept requires minutes of training. Sega, in comparison, requires no extension to the architecture and produces semantic vectors ad-hoc for any textual prompt. Lastly, Safe Latent Diffusion (SLD) uses targeted manipulation of the noise estimate to suppress the generation of inappropriate content . Instead of arbitrary changes to an image, SLD prevents one dedicated concept from being generated. Additionally, SLD is complex, and the hyperparameter formulation can be improved through a deeper understanding of the numerical properties of DMs' noise estimate space.

## 3 Semantic Guidance

Let us now devise Semantic Guidance for diffusion models.

### Guided Diffusion

The first step towards Sega is guided diffusion. Specifically, diffusion models (DM) iteratively denoise a Gaussian distributed variable to produce samples of a learned data distribution. For text-to-image generation, the model is conditioned on a text prompt \(p\) and guided toward an image faithful to that prompt. The training objective of a DM \(_{}\) can be written as

\[_{,_{p},,t}[w_{t}|| }_{}(_{t}+_{t},_{p})- ||_{2}^{2}]\] (1)

where \((,_{p})\) is conditioned on text prompt \(p\), \(t\) is drawn from a uniform distribution \(t()\), \(\) sampled from a Gaussian \((0,)\), and \(w_{t},_{t},_{t}\) influence the image fidelity depending on \(t\). Consequently, the DM is trained to denoise \(_{t}:=+\) yielding \(\) with the squared error loss. At inference, the DM is sampled using the prediction of \(=(_{t}-})\), with \(}\) as described below.

Classifier-free guidance  is a conditioning method using a purely generative diffusion model, eliminating the need for an additional pre-trained classifier. During training, the text conditioning \(_{p}\) drops randomly with a fixed probability, resulting in a joint model for unconditional and conditional objectives. During inference, the score estimates for the \(\)-prediction are adjusted so that:

\[_{}(_{t},_{p}):=_{}( _{t})+s_{g}(_{}(_{t},_{p})- _{}(_{t}))\] (2)

with guidance scale \(s_{g}\) and \(_{}\) defining the noise estimate with parameters \(\). Intuitively, the unconditioned \(\)-prediction is pushed in the direction of the conditioned one, with \(s_{g}\) determining the extent of the adjustment.

### Semantic Guidance on Concepts

We introduce Sega to influence the diffusion process along several directions. To this end, we substantially extend the principles introduced in classifier-free guidance by solely interacting with the concepts already present in the model's latent space. Therefore, Sega requires no additional training, no extensions to the architecture, and no external guidance. Instead, it is calculated during the existing diffusion iteration. More specifically, Sega uses multiple textual descriptions \(e_{i}\), representing the given target concepts of the generated image, in addition to the text prompt \(p\).

**Intuition.** The overall idea of Sega is best explained using a 2D abstraction of the high dimensional \(\)-space, as shown in Fig. 1. Intuitively, we can understand the space as a composition of arbitrary sub-spaces representing semantic concepts. Let us consider the example of generating an image of a king. The unconditioned noise estimate (black dot) starts at some random point in the \(\)-space without semantic grounding. The guidance corresponding to the prompt "a portrait of a king" represents a vector (blue vector) moving us into a portion of \(\)-space where the concepts'male' and royal overlap, resulting in an image of a king. We can now further manipulate the generation process using Sega. From the unconditioned starting point, we get the directions of'male' and 'female' (orange/green lines) using estimates conditioned on the respective prompts. If we subtract this inferred'male' direction from our prompt guidance and add the 'female' one, we now reach a point in the \(\)-space at the intersection of the 'royal' and 'female' sub-spaces, i.e., a queen. This vector represents the final direction (red vector) resulting from semantic guidance.

**Isolating Semantics in Diffusion.** Next, we investigate the actual noise-estimate space of DMs on the example of Stable Diffusion (SD). This enables extracting semantic concepts from within that space and applying them during image generation.

Numerical values of \(\)-estimates are generally Gaussian distributed. While the value in each dimension of the latent vector can differ significantly between seeds, text prompts, and diffusion steps, the overall distribution always remains similar to a Gaussian distribution (cf. App. B). Using the arithmetic principles of classifier-free guidance, we can now identify those dimensions of a latent vector encoding an arbitrary semantic concept. To that end, we calculate the noise estimate \(_{}(_{t},_{e})\), which is conditioned on a concept description \(e\). We then take the difference between \(_{}(_{t},_{e})\) and the unconditioned estimate \(_{}(_{t})\) and scale it. Again, the numerical values of the resulting latent vector are Gaussian distributed, as shown in Fig. 2. We will demonstrate that those latent dimensions falling into the upper and lower tail of the distribution alone encode the target concept. We empirically determined that using only 1-5% of the \(\)-estimate's dimensions is sufficient to apply the desired changes to an image. Consequently, the resulting concept vectors are largely isolated; thus, multiple ones can be applied simultaneously without interference (cf. Sec. 4).

We subsequently refer to the space of these sparse noise-estimate vectors as _semantic space_.

**One Direction.** Let us formally define the intuition for Sega by starting with a single direction, i.e., editing prompt. Again, we use three \(\)-predictions to move the unconditioned score estimate \(_{}(_{t})\) towards the prompt conditioned estimate \(_{}(_{t},_{p})\) and simultaneously away/towards the concept conditioned estimate \(_{}(_{t},_{e})\), depending on the editing direction. Formally, we compute \(_{}(_{t},_{p},_{e})=\)

\[_{}(_{t})+s_{g}_{}(_{t },_{p})-_{}(_{t})+( _{t},_{e})\] (3)

with the semantic guidance term \(\)

\[(_{t},_{e})=(;s_{e},)( _{t},_{e})\] (4)

where \(\) applies an edit guidance scale \(s_{e}\) element-wise, and \(\) depends on the edit direction:

\[(_{t},_{e})=_{}(_{t},_{e})-_{}(_{t})&\\ -_{}(_{t},_{e})-_{}( _{t})&\] (5)

Thus, changing the guidance direction is reflected by the direction between \(_{}(_{t},_{e})\) and \(_{}(_{t})\).

The term \(\) (Eq. 4) considers those dimensions of the prompt conditioned estimate relevant to the defined editing prompt \(e\). To this end, \(\) takes the largest absolute values of the difference between the unconditioned and concept-conditioned estimates. This corresponds to the upper and lower tail of the numerical distribution as defined by percentile threshold \(\). All values in the tails are scaled by an edit scaling factor \(s_{e}\), with everything else being set to 0, such that

\[(;s_{e},)=s_{e}&||_{ }(||)\\ 0&\] (6)

where \(_{}()\) is the \(\)-th percentile of \(\). Consequently, a larger \(s_{e}\) increases Sega's effect.

Figure 2: Numerical intuition of semantic guidance. The difference between the concept-conditioned and unconditioned estimates is first scaled. Subsequently, the tail values represent the dimensions of the specified concept. Distribution plots calculated using kernel-density estimates with Gaussian smoothing.

Sega can also be theoretically motivated in the mathematical background of DMs [9; 32; 14]. The isolated semantic guidance equation for the positive direction without the classifier-free guidance term can be written as

\[_{}(_{t},_{e}) _{}(_{t})+(_{}(_{t}, _{e})-_{}(_{t}))\] (7)

given Eqs. 3, 4, 5. Further, let us assume an implicit classifier for \(p(_{e}|_{t})_{t},|_{e })}{p(z_{t})}\), where \(p(z)\) is the marginal of \(z\) for the variance-preserving Markov process \(q(z|x)\) and \(x p(x)\). Assuming exact estimates \(^{*}(_{t},_{e})\) of \(p(_{t}|_{e})\) and \(^{*}(_{t})\) of \(p(z_{t})\) the gradient of the resulting classifier can be written as \(_{_{t}} p(_{e}|_{t})=-}^{*}(_{t},_{e})-(^{*}( _{e})\). Using this implicit classifier for classifier guidance  results in noise estimate \(^{*}(_{t},_{e})=^{*}(_{t})+w(^{*}(_{t},_{e})-^{*}(_{t}))\). This is fundamentally similar to Sega as shown in Eq. 7 with \(\) isolating the dimensions of the classifier signal with the largest absolute value (cf. Eq. 6). However, it should be noted that \(^{*}(_{t},_{e})\) is profoundly different from \(_{}(_{t},_{e})\) as the latter's expressions are outputs of an unconstrained network and not the gradient of a classifier. Consequently, there are no guarantees  for the performance of Sega. Nevertheless, this derivation provides a solid theoretical foundation and we are able to demonstrate the effectiveness of Sega empirically in Sec. 5.

To offer even more control over the diffusion process, we make two adjustments to the methodology presented above. We add warm-up parameter \(\) that will apply guidance \(\) after an initial warm-up period in the diffusion process, i.e., \((_{t},_{e})\!:=\!\) if \(t\!<\!\). Naturally, higher values for \(\) lead to less substantial adjustments of the generated image. If we aim to keep the overall image composition unchanged, selecting a sufficiently high \(\) ensures only altering fine-grained output details.

Furthermore, we add a momentum term \(_{t}\) to the semantic guidance \(\) to accelerate guidance over time steps for dimensions that are continuously guided in the same direction. Hence, \(_{t}\) is defined as:

\[(_{t},_{e})=(;s_{e},)(_ {t},_{e})+s_{m}_{t}\] (8)

with momentum scale \(s_{m}\) and \(\) being updated as

\[_{t+1}=_{m}_{t}+(1-_{m})_{t}\] (9)

where \(_{0}\!=\!\) and \(_{m}\!\![0,1)\). Thus, larger \(_{m}\) lead to less volatile changes in momentum. Momentum is already built up during the warm-up period, even though \(_{t}\) is not applied during these steps.

**Beyond One Direction.** Now, we are ready to move beyond using just one direction towards multiple concepts \(e_{i}\) and, in turn, combining multiple calculations of \(_{t}\).

For all \(e_{i}\), we calculate \(_{t}^{i}\) as described above with each defining their own hyperparameter values \(^{i}\), \(s_{e}^{i}\). The weighted sum of all \(_{t}^{i}\) results in

\[_{t}(_{t},_{e_{i}})=_{i I}g_ {i}_{t}^{i}(_{t},_{e_{i}})\] (10)

In order to account for different warm-up periods, \(g_{i}\) is defined as \(g_{i}\!=\!0\) if \(t<_{i}\). However, momentum is built up using all editing prompts and applied once all warm-up periods are completed, i.e., \(_{i}:_{i} t\). We provide a pseudo-code implementation of Sega in App. A and more detailed intuition of each hyper-parameter along with visual ablations in App. C.

Sega's underlying methodology is architecture-agnostic and applicable to any model employing classifier-free guidance. Consequently, we implemented Sega for various generative models of different architectures and make our code available online. If not stated otherwise the examples presented in the main body, are generated using our implementation based on SD v1.52 with images from other models and architectures included in App. F. We note that Sega can easily be applied to real images using reconstruction techniques for diffusion models, which we regard as future work.

## 4 Properties of Semantic Space

With the fundamentals of semantic guidance established, we next investigate the properties of Sega's semantic space. In addition to the following discussion, we present further examples in the Appendix.

**Robustness.** Sega behaves robustly for incorporating arbitrary concepts into the original image. In Fig. 2(a), we applied guidance for the concept '_glasses_' to images from different domains. Notably, this prompt does not provide any context on how to incorporate the glasses into the given image and thus leaves room for interpretation. The depicted examples showcase how Sega extracts best-effort integration of the target concept into the original image that is semantically grounded. This makes Sega's use easy and provides the same exploratory nature as the initial image generation.

**Uniqueness.** Guidance vectors \(\) of one concept are unique and can thus be calculated once and subsequently applied to other images. Fig. 2(b) shows an example for which we computed the semantic guidance for '_glasses_' on the left-most image and simply added the vector in the diffusion process of other prompts. All faces are generated wearing glasses without a respective \(\)-estimate required. This even covers significant domain shifts, as seen in the one switching from photo-realism to drawings.

However, the transfer is limited to the same initial seed, as \(\)-estimates change significantly with diverging initial noise latents. Furthermore, more extensive changes to the image composition, such as the one from human faces to animals or inanimate objects, require a separate calculation of the guidance vector. Nonetheless, Sega introduces no visible artifacts to the resulting images.

**Monotonicity.** The magnitude of a semantic concept in an image scales monotonically with the strength of the semantic guidance vector. In Fig. 2(c), we can observe the effect of increasing the strength of semantic guidance \(s_{e}\). Both for positive and negative guidance, the change in scale correlates with the strength of the smile or frown. Consequently, any changes to a generated image can be steered intuitively using only the semantic guidance scale \(s_{e}\) and warm-up period \(\). This level of control over the generation process is also applicable to multiple concepts with arbitrary combinations of the desired strength of the edit per concept.

**Isolation.** Different concepts are largely isolated because each concept vector requires only a fraction of the total noise estimate. Meaning that different vectors do not interfere with each other. Thus, multiple concepts can be applied to the same image simultaneously, as shown in Fig. 4. We can see, for example, that the glasses which were added first remain unchanged with subsequently added edits. We can utilize this behavior to perform more complex changes, best expressed using multiple concepts. One example is the change of gender by simultaneously removing the'male' concept and adding the 'female' one (cf. Figs. 5 and 6).

## 5 Experimental Evaluation

Next, we present exhaustive evaluation of semantic guidance on empirical benchmarks, as well as additional qualitative tasks. Furthermore, we report user preferences on direct comparisons with existing methods [16; 8; 35]. Our experiments refute claims of previous research, by demonstrating

Figure 3: Robustness, uniqueness and monotonicity of Sega guidance vectors. In a) and b) the top row depicts the unchanged image, bottom row depicts the ones guided towards ‘_glasses_’. (Best viewed in color)the suitability of noise estimates for semantic control  and its capability for small, subtle changes . The comparison to other methods diffusion-based approaches [8; 35; 16] indicate Sega's improved robustness and capability to cover a wider range of use cases.

**Empirical Results.** First, we performed an extensive empirical evaluation on human faces and respective attributes. This setting is inspired by the CelebA dataset  and marks a well-established benchmark for semantic changes in image generation. We generated 250 images with unique seeds using the prompt 'an image of the face of a random person' and manipulated ten facial attributes. These attributes are a subset of the CelebA labels. All attributes and respective examples of the corresponding manipulation using Sega are depicted in Fig. 5. In addition to additive image edits, we evaluated negative guidance of these attributes as well as two combinations of four simultaneous edits, as shown in Fig. 4. Therefore, our empirical evaluation spans 21 attribute changes and combinations in total with all images being generated using the Stable Diffusion implementation.

We evaluated the generated images with a user study and provide more details on its implementation in App. E. The results are shown in Tab. 1. For positive guidance, Sega faithfully adds the target concept to the image on average in 95% of the cases, with the majority of attributes exceeding 97%. We further manually investigated the two outliers, '_Bald_' and '_Bangs_'. We assume that many of the non-native English-speaking annotators were not familiar with the term 'bangs' itself. This assumption is based on correspondence with some of the workers, and the conspicuously low rate of annotator consensus. Consequently, the numbers for '_bangs_' should be taken with a grain of salt. For baldness, we found that long hair often makes up a large portion of a portrait and thus requires more substantial changes to the image. Consequently, such edits require stronger hyperparameters than those chosen for this study. Furthermore, we observe negative guidance to remove existing attributes from an image to work similarly well. It is worth pointing out that the guidance away from '_beard_', '_bald_' or '_gray hair_' usually resulted in a substantial reduction of the respective feature, but failed to remove it entirely for \(\)10% of the images. Again, this suggests that the hyperparameters were probably not strong enough.

Lastly, we looked into the simultaneous guidance of multiple concepts at once. The results in Tab. 2 empirically demonstrate the isolation of semantic guidance vectors. The per-attribute success rate remains similar for four instead of one distinct edit concept, suggesting no interference of guidance vectors. Consequently, the success of multiple edits only depends on the joint probability of the individual concepts. In comparison, if only two out of four applied concepts were interfering with each other to be mutually exclusive, the success rate of such a combination would always be 0%. Contrary to that, we successfully apply concurrent concepts in up to 91% of generated images.

Additionally, we investigated any potential influence of manipulations with Sega on overall image quality. Utilizing the facial images from the previous experiment, we calculated FID scores against a

Figure 4: Successive combination of concepts. From left to right an new concept is added each image. Concepts do not interfere with each other and only change the relevant portion of the image. (Best viewed in color)

Figure 5: Examples from our empirical evaluation benchmark, showing the 10 attributes edited with Sega. Original and edited images are evaluated by human users one feature at a time. (Best viewed in color)

reference dataset of FFHQ . FID scores for the original, unedited images are remarkable bad at 117.73, which can be attributed to small artifacts often present in facial images generated by Stable Diffusion. Editing images with SEGA significantly improved their quality resulting in an FID score on FFHQ of 59.86. Upon further investigation, we observed that the additional guidance signal for a dedicated portion of the faces frequently removed uncanny artifacts, resulting in overall better quality.

Further, we empirically evaluated Sega on the significantly different task of counteracting inappropriate degeneration [3; 4; 29]. For latent DMs, Schramowski et al.  demonstrated that an additional guidance term may be used to mitigate the generation of inappropriate images. We demonstrate that Sega's flexibility enables it to now perform this task on any diffusion architecture, both latent and pixel-based. To that end, we evaluated Stable Diffusion v1.5 and DeepFloyd-IF on the inappropriate-image-prompts (I2P) benchmark . Subsequently, we suppressed inappropriate content using Sega to guide the generation away from the inappropriate concepts as specified by Safe Latent Diffusion (SLD) . We empirically chose the Sega parameters to produce results similar to the 'Strong' configuration of SLD. The results in Tab. 3 demonstrate that Sega performs strong mitigation at inference for both architectures further highlighting the capabilities and versatility of the approach.

**Comparisons.** In addition to the isolated evaluation of Sega's capabilities, we conducted randomized user studies directly comparing Sega with related methods. This comparison investigates the performance of Composable Diffusion , Promp2Prompt , Disentanglement , and Sega on tasks from four types of manipulation categories, reflecting a broad range of operations. Specifically, we considered 1) composition of multiple edits, 2) minor changes, 3) style transfer and 4) removal of specific objects from a scene. For each method and task, we selected the best-performing hyperparameters on a set of seeds < 100 and subsequently generated non-cherry-picked edits on a fixed test set (i.e., first applicable seeds >= 100). As first evaluation step, we conducted a user study to assess the success of an edit based on the presence/absence of the target attribute(s) as shown in Tab. 4. The comparatively low scores on removal tasks result from the methods often reducing the presence of the targeted objects but not eliminating all of them in some cases. However, this peculiarity of the evaluation affects all methods similarly and does not influence the comparability of capabilities. The results demonstrate that Sega clearly outperforms Prompt2Prompt and Disentanglement on all

    &  &  &  \\   & Gender & 241 & 99.2 & 100.0 \\  & Glasses & 243 & 99.6 & 100.0 \\  & Smile & 146 & 100.0 & 99.3 \\  & Bald & 220 & 91.2 & 82.1 \\  & Beard & 135 & 97.8 & 97.0 \\  & Hat & 210 & 99.0 & 99.0 \\  & Curls & 173 & 95.4 & 97.0 \\  & Makeup & 197 & 99.5 & 99.0 \\  & Gray hair & 165 & 97.6 & 91.2 \\  & Bangs & 192 & 86.2 & 82.7 \\   & **Overall** & **1922** & **96.5** & **95.0** \\   & No Glasses & 6 & 98.9 & 100.0 \\  & No Smile & 93 & 100.0 & 94.4 \\  & No Bald & 18 & 94.8 & 83.3 \\  & No Beard & 111 & 100.0 & 89.9 \\  & No Hat & 31 & 100.0 & 90.3 \\  & No Curls & 50 & 98.0 & 98.0 \\  & No Makeup & 21 & 100.0 & 95.2 \\  & No Gray Hair & 52 & 94.6 & 82.7 \\  & No Bangs & 38 & 95.0 & 98.5 \\   & **420** & **91.9** & **92.1** \\   

Table 1: Empirical results of our user study conducted on face attributes. Sample sizes result from the portion of the 250 original images that did not contain the target attribute. Annotator consensus refers to the percentage of images for which the majority of annotators agreed on a label. Success rate is reported on those images with annotator consensus.

    &  &  &  \\   & \(\) 1 Attr. & & 100.0 \\  & \(\) 2 Attr. & & 100.0 \\  & \(\) 3 Attr. & 55 & 98.2 \\  & **All 4 Attr.** & & **90.7** \\   & Glasses & 96.4 & 100.0 \\  & Smile & 55 & 100.0 & 96.4 \\  & Curls & 100.0 & 98.2 \\  & Beard & 100.0 & 100.0 \\   & \(\) 1 Attr. & & 100.0 \\  & \(\) 3 Attr. & 45 & 81.8 & 100.0 \\  & **All 4 Attr.** & & **75.6** \\   & No Smile & 100.0 & 97.8 \\  & Makeup & 45 & 100.0 & 97.6 \\  & Hat & 100.0 & 88.6 \\  & Female & 100.0 & 100.0 \\   

Table 2: Results of user study on simultaneous combination of face attributes. Sample sizes result from the portion from the portion of the 250 original images that did not contain any target attribute. Annotator consensus refers to the percentage of images for which the annotator’s majority agreed on a label. Success rates for combinations on any combination of \(x\) attributes with per-attribute scores are reflecting the isolated success of that edit. Nonetheless, all scores are shown for images with all 4 edit concepts applied simultaneously.

examined editing tasks. Compared to Composable Diffusion, SEGA again has significantly higher success rates for multi-conditioning and minor changes while achieving comparable performance for style transfer and object removal.

Beyond the simple success rate of edits, we evaluated the faithfulness to the original image composition. To that end, users considered pair-wise comparisons of samples that both methods edited successfully and assessed the similarity with the original image. Importantly, users strongly prefer SEGA results over Composable Diffusion for 83.33% (vs. 13.33%) of samples. These two studies highlight that Sega is generally preferred in terms of its edit capabilities and perceived fidelity over other methods. We present examples and further details on comparisons to related diffusion and StyleGAN techniques in App. G.

**Qualitative Results.** In addition to the empirical evaluation, we present qualitative examples on other domains and tasks. We show further examples in higher resolution in the Appendix. Overall, this highlights the versatility of Sega since it allows interaction with any of the abundant number of concepts diffusion models are capable of generating in the first place. We performed a diverse set of style transfers, as shown in Fig. 6. Sega faithfully applies the styles of famous artists, as well as artistic epochs and drawing techniques. In this case, the entirety of the image has to be changed while keeping the image composition the same. Consequently, we observed that alterations to the entire output--as in style transfer--require a slightly lower threshold of \( 0.9\). Nonetheless, this still means that 10% of the \(\)-space is sufficient to change the entire style of an image. Fig. 6 also includes a comparison between outputs produced by Sega with those from simple extensions to the prompt text. Changing the prompt also significantly alters the image composition. These results further highlight the advantages of semantic control, which allows versatile and yet robust changes.

## 6 Broader Impact on Society

Recent developments in text-to-image models [25; 21; 28] have the potential for a far-reaching impact on society, both positive and negative, when deployed in applications such as image generation, image editing, or search engines. Previous research [3; 29] described many potential negative societal implications that may arise due to the careless use of such large-scale generative models. Many of these problems can be attributed to the noisy, large-scale datasets these models rely on. Since recent text-to-image models, such as SD, are trained on web-crawled data containing inappropriate content , they are no exception to this issue. Specifically, current versions of SD show signs of inappropriate degeneration . While Schramowski et al.  utilize the model's notion of inappropriateness to steer the model away from generating related content, it is noteworthy that we introduce an approach that could also be used to guide image generation toward inappropriate material. The limitations of Stable Diffusion also effect our editing of perceived gender which may

    & Multi- & Minor & Style & Concept &  &  \\  & Conditioning & (\%) & Changes (\%) & Transfer (\%) & & Removal (\%) \\  Composable Diffusion & \(35.00\) & \(\) & \(\) & \(\) & \(\) \\ Prompt2Prompt & \(35.00\) & \(68.00\) & \(65.00\) & \(5.00\) & \(43.25\) \\ Disentanglement & \(35.00\) & \(65.38\) & \(65.00\) & \(0.00\) & \(41.35\) \\ SEGA (Ours) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 4: User study results of success rates for different manipulation types and methods. Overall Sega outperforms all compared techniques and specifically improves on tasks with multiple edits and small changes. Success rate is reported on those images where the majority of annotator agree on a label.

  &  &  &  &  &  & Violence & Overall \\  SD & \(0.32_{0.92}\) & \(0.39_{0.96}\) & \(0.35_{0.94}\) & \(0.40_{0.97}\) & \(0.29_{0.86}\) & \(0.51_{1.00}\) & \(0.42_{0.98}\) & \(0.38_{0.97}\) \\ SD w/ Sega & \(0.12_{0.66}\) & \(0.15_{0.75}\) & \(0.09_{0.57}\) & \(0.07_{0.56}\) & \(0.04_{0.36}\) & \(0.19_{0.84}\) & \(0.14_{0.75}\) & \(0.11_{0.68}\) \\  IF & \(0.35_{0.98}\) & \(0.46_{10.00}\) & \(0.40_{0.99}\) & \(0.40_{10.00}\) & \(0.22_{0.91}\) & \(0.49_{10.00}\) & \(0.43_{10.00}\) & \(0.38_{0.99}\) \\ IF w/ Sega & \(0.16_{0.84}\) & \(0.20_{0.88}\) & \(0.15_{0.84}\) & \(0.13_{0.81}\) & \(0.06_{0.58}\) & \(0.21_{0.92}\) & \(0.18_{0.89}\) & \(0.15_{0.84}\) \\   

Table 3: Results of Stable Diffusion and IF on the I2P benchmark . Values reflect the probability of generating inappropriate content (the lower the better) with respect to the joint Q16/NudeNet classifier proposed by Safe Latent Diffusion. Subscript values denote the expected maximum inappropriateness over 25 prompts (the lower the better). For both architectures Sega performs strong mitigation at inference.

exhibit potential biases. Since Stable Diffusion's learned representation of gender are limited, we restrict ourselves to binary labels, although gender expression cannot be ascribed to two distinct categories. Furthermore, does Stable Diffusion contain severe biases in its attribution of gender and correlated features that may further reinforce pre-existing stereotypes. Since editing methods in general, and Sega in particular, rely on these representations of the underlying model, they will inevitably inherit them for related editing operations. However, on the positive side, Sega has the potential to mitigate bias. As demonstrated by Nichol et al., removing data from the training set has adverse effects, e.g., on a model's generalization ability. In contrast, Sega works at inference promoting fairness in the outcome. Therefore, we advocate for further research in this direction.

Another frequently voiced point of criticism is the notion that generative models like SD are replacing human artists and illustrators. Indeed, certain jobs in the creative industry are already threatened by generative image models. Furthermore, the collection of training data for (commercial) applications is often ethically questionable, building on people's artwork without their explicit consent. Nonetheless, we believe Sega to be promoting creating artwork in an interactive process that requires substantial amount of iterative human feedback.

## 7 Conclusions

We introduced semantic guidance (Sega) for diffusion models. Sega facilitates interaction with arbitrary concepts during image generation. The approach requires no additional training, no extensions to the architecture, no external guidance, is calculated during the existing generation process and compatible with any diffusion architecture. The concept vectors identified with Sega are robust, isolated, can be combined arbitrarily, and scale monotonically. We evaluated Sega on a variety of tasks and domains, highlighting--among others--sophisticated image composition and editing capabilities.

Our findings are highly relevant to the debate on disentangling models' latent spaces. So far, disentanglement as a property has been actively pursued . However, it is usually not a necessary quality in itself but a means to an end to easily interact with semantic concepts. We demonstrated that this level of control is feasible without disentanglement and motivate research in this direction.

Additionally, we see several other exciting avenues for future work. For one, it is interesting to investigate further how concepts are represented in the latent space of DMs and how to quantify them. Similarly, Sega could be extended to allow for even more complex manipulations, such as targeting different objects separately. More importantly, automatically detecting concepts could provide novel insights and toolsets to mitigate biases, as well as enacting privacy concerns of real people memorized by the model.

Figure 6: Qualitative examples using Sega. Top row demonstrates that the approach is architecture agnostic showing examples for for Stable Diffusion\({}^{3}\), Paella  and Deepfloyd-IF\({}^{4}\). Below we show style transfer using Sega. All images are generated from the same noise latent using the text prompt ‘a house at a lake’. Sega easily applies the characteristics of a specific artist, epoch or drawing/imaging technique to the original image while preserving the overall composition. In contrast, appending the prompt with a style instruction results in images that significantly change the composition. (Best viewed in color)

AcknowledgmentsThis research has benefited from the Hessian Ministry of Higher Education, Research, Science and the Arts (HMWK) cluster projects "The Third Wave of AI" and hessian.AI, from the German Center for Artificial Intelligence (DFKI) project "SAINT", the Federal Ministry of Education and Research (BMBF) project KISTRA (reference no. 13N15343), as well as from the joint ATHENE project of the HMWK and the BMBF "AVSV". Further, we thank Amy Zhu for her assistance in conducting user studies for this work.