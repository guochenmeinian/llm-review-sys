# DeepLag: Discovering Deep Lagrangian Dynamics

for Intuitive Fluid Prediction

 Qilong Ma, Haixu Wu, Lanxiang Xing, Shangchen Miao, Mingsheng Long

School of Software, BNRist, Tsinghua University, China

{mql22,wuhx23,xlx22,msc21}@mails.tsinghua.edu.cn, mingsheng@tsinghua.edu.cn

Equal Contribution

###### Abstract

Accurately predicting the future fluid is vital to extensive areas such as meteorology, oceanology, and aerodynamics. However, since the fluid is usually observed from the Eulerian perspective, its moving and intricate dynamics are seriously obscured and confounded in static grids, bringing thorny challenges to the prediction. This paper introduces a new Lagrangian-Eulerian combined paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose DeepLag to discover hidden Lagrangian dynamics within the fluid by tracking the movements of adaptively sampled key particles. Further, DeepLag presents a new paradigm for fluid prediction, where the Lagrangian movement of the tracked particles is inferred from Eulerian observations, and their accumulated Lagrangian dynamics information is incorporated into global Eulerian evolving features to guide future prediction respectively. Tracking key particles not only provides a transparent and interpretable clue for fluid dynamics but also makes our model free from modeling complex correlations among massive grids for better efficiency. Experimentally, DeepLag excels in three challenging fluid prediction tasks covering 2D and 3D, simulated and real-world fluids. Code is available at this repository: https://github.com/thuml/DeepLag.

## 1 Introduction

Fluids, characterized by a molecular structure that offers no resistance to external shear forces, easily deform even under minimal stress, leading to highly complex and often chaotic dynamics . Consequently, the solvability of fundamental theorems in fluid mechanics, such as Navier-Stokes equations, is constrained to only a limited subset of flows due to their inherent complexity and intricate multiphysics interactions . In practical applications, computational fluid dynamics (CFD) is widely employed to predict fluid behavior through numerical simulations, but it is hindered by significant computational costs. Accurately forecasting future fluid dynamics remains a formidable challenge. Recently, deep learning models  have shown great promise for fluid prediction due to their exceptional non-linear modeling capabilities. These models, trained on CFD simulations or real-world data, can serve as efficient surrogate models, dramatically accelerating inference.

A booming direction for deep fluid prediction is learning deep models to solve partial differential equations (PDEs) . However, most of these methods  attempt to capture fluid dynamics from the Eulerian perspective, which means modeling spatiotemporal correlations among massive grids unchanging over time. From this perspective, the complicated moving dynamics in fluids could be seriously obscured and confounded in static grids, bringing challenges in both computational efficiency and learning difficulties for accurately predicting future fluids.

In parallel to the Eulerian method, we notice another major approach for elucidating fluid dynamics, the Lagrangian method , also known as the particle tracking method. This method primarilyfocuses on tracing individual fluid particles by modeling the temporal evolution of their position and velocity. Unlike the Eulerian methods, which observe fluid flow at fixed spatial locations, the Lagrangian approach describes the fluid dynamics through the moving trajectory of individual fluid particles, offering a more natural and neat representation of fluid dynamics with inherent advantages in capturing intricate flow dynamics. Moreover, it allows a larger inference time step tha Eulerian methods while complying with the Courant-Friedrichs-Lewy condition  that guarantees stability. In Figure 1, we can find that fluid dynamics is much more visually apparent in Lagrangian trajectories on the left compared to the density changes observed on static Eulerian grids on the right.

Building on the two perspectives mentioned earlier, we propose DeepLag as a Eulerian-Lagrangian Recurrent Network. Our aim is to integrate Lagrangian tracking into the deep model, thereby enhancing the dynamics modeling in Eulerian fluid prediction. To achieve this, we present the EuLag Block, a powerful module that accomplishes Lagrangian tracking and Eulerian predicting at various scales. By leveraging the cross-attention mechanism, the EuLag Block assimilates tracked Lagrangian particle dynamics into the Eulerian field, guiding fluid prediction. It also forecasts the trajectory and dynamics of Lagrangian particles with the aid of Eulerian features. This unique Eulerian-Lagrangian design harnesses the dynamics information captured by Lagrangian trajectories and the fluid-structure features learned in the Eulerian grid. In our experiments, DeepLag consistently outperforms existing models, demonstrating state-of-the-art performance across three representative datasets, covering 2D and 3D fluid at various scales. Our contributions are as follows:

* Going beyond learning fluid dynamics at static grids, we propose DeepLag featuring the Eulerian-Lagrangian Recurrent Network, which integrates both Eulerian and Lagrangian frameworks from fluid dynamics within a pure deep learning framework concisely.
* Inspired by Lagrangian mechanics, we present EuLag Block, which can accurately track particle movements and interactively utilize the Eulerian features and dynamic information in fluid prediction, enabling a better dynamics modeling paradigm.
* DeepLag achieves consistent state-of-the art on three representative fluid prediction datasets with superior trade-offs for performance and efficiency, exhibiting favorable practicability.

## 2 Preliminaries

### Eulerian and Lagrangian Methods

Eulerian and Lagrangian descriptions are two fundamental perspectives for modeling fluid motion. The Eulerian view, commonly used in practical applications , observes fluid at fixed points and records physical quantities, such as density, as a function of position and time, \(=(,t)\). Thus, future fluid can be predicted by integrating velocity along the temporal dimension and interpolating the results to observed grid points . In contrast, the Lagrangian view focuses on the trajectory of individual particles, tracking a particular particle from initial position \(_{0}\) by its displacement \(=(_{0},t)\) at time \(t\). This approach reveals the intricate evolution of the fluid by following particle trajectories, making it convenient for describing complex phenomena like vortices, turbulence, and interface motions . Two perspectives are constitutionally equivalent, as bridged by the velocity:

\[((_{0},t),t)=}{ t}(_{0},t).\] (1)

Figure 1: Comparison between Lagrangian (left) and Eulerian (right) perspectives. The left depicts the learned trajectories of Lagrangian particles overlaid on the mean state, while the right displays the positions of tracked particles in successive Eulerian frames. Fluid motion is more visibly represented through the dynamic Lagrangian view compared to the density variations in static Eulerian grids.

Furthermore, the _material derivative_\(}{t}\) that describes the change rate of a physical quantity \(\) of a fluid parcel can be written as the sum of the two terms reflecting the spatial and temporal influence on \(\), which represent the derivatives on Eulerian domain and Lagrangian convective respectively:

\[}{t}}{ t}}_{}+ }_{}.\] (2)

This connection inspires us to incorporate Lagrangian descriptions into dynamics learning with Eulerian data, enabling a more straightforward decomposition of complex spatiotemporal dependencies.

While traditional particle-based (or mixed-representation) solvers demonstrate superior accuracy and adaptability in inferring small-scale phenomena and dealing with nonlinear and irregular boundary conditions, they require computing the acceleration of each particle through physical equations, followed by sequential updates of their velocity and position . This pointwise modeling approach often demands a significant number of points to fully characterize the dynamics of the entire field to meet accuracy requirements. Moreover, the irregular arrangement of particles and difficulty in parallelization result in higher computational costs and challenges with particle interpolation and gridding . This renders particle-based solvers suboptimal compared to Eulerian solvers, particularly in high-dimensional spaces and large-scale simulations. However, the proposed DeepLag leverages the strengths of both solvers, eschewing equations and instead utilizing Eulerian information to assist particle tracking directly. This greatly alleviates the pressure on Lagrangian representation, requiring significantly fewer representative particles to aggregate the dynamics of the entire field.

### Neural Fluid Prediction

As computational fluid dynamics (CFD) methods often require hours or even days for simulations , deep models have been explored as efficient surrogate models that can provide near-instantaneous predictions. These neural fluid prediction models approximate the solutions of governing fluid equations differently and can be categorized into three mainstream paradigms as in Figure 2(a-c).

Classical ML methodsAs depicted in Figure 2(a), these methods either replace part of a numerical method with a neural surrogate  or encode multivariate fields into a single-variable latent state \(\), on which they model an ODE governing the state function \(_{t}:^{d}\), representing the first-order time derivative, through neural networks. However, the absence of physical meaning and guidance for latent states in evolution leads to error accumulation and a generally short forecasting horizon. A detailed comparison between DeepLag and these models is provided in Appendix B.

Physics-Informed Neural Networks (PINNs)This branch of methods in Figure 2(b) adopts deep models to learn the mapping from coordinates to solutions and formalize PDE constraints along with initial and boundary conditions as the loss functions . Though this paradigm can explicitly approximate the PDE solution, they usually require exact formalization for coefficients and conditions, limiting their generality and applicability to real-world fluids that are usually partially observed . Plus, the Eulerian input disables them from handling Lagrangian descriptions.

Neural OperatorsRecently, a new paradigm, illustrated in Figure 2(c), has emerged where deep models learn the neural operators between input and target functions, e.g., past observations to future fluid predictions. Since DeepONet , various neural operators have significantly advanced fluid prediction, which directly approximates mappings between equation parameters and solutions. For Eulerian grid data, models based on U-Net  and ResNet  architectures have been proposed , as well as variants addressing issues like generalizing to unseen domains , irregular mesh , and uncertainty quantification . Transformer-based models  enhance modeling capabilities and efficiency by exploiting techniques like Galerkin attention , incorporating ensemble information from the grid , applying low-rank decomposition to the attention mechanism , and leveraging spectral methods in the latent space . Additionally, FNO  learns mappings in the frequency domain, and MP-PDE  utilizes the message-passing mechanism. For Lagrangian fluid particle data, some CNN-based methods  model particle interactions through redesigned basic modules, while GNN-based methods  update particle positions using the Encoder-Process-Decode paradigm. Despite the progress made by these methods, they are limited to one description and do not seamlessly combine Eulerian and Lagrangian views.

## 3 DeepLag

Following the convention of neural fluid prediction , we formalize the fluid prediction problem as learning future fluid given past observation, as shown in Figure 2(d). Given a bounded open subset of \(d\)-dimensional Euclidean space \(^{d}\) and a Eulerian space \(^{o}\) with \(o\) observed physical quantities, letting \(_{t}()\) and \(_{t+1}()\) represent the Eulerian fluid field observation at two consecutive time steps on a finite coordinate set \(\), we aim at fitting the mapping \(:_{t}()_{t+1}()\). Concretely, provided initial \(P\) step observations \(U_{P}=\{_{1},,_{P}\}\), the fluid prediction process can be written as the following autoregressive paradigm:

\[U_{t}=\{_{t-P+1},,_{t}\}_{ }}_{t+1},\] (3)

where \(t P\) and \(_{}\) represents the learned mapping between \(U_{t}\) and the predicted field \(_{t+1}\).

Inspired by the material derivative in Eq. (2), we present DeepLag as a Eulerian-Lagrangian Recurrent Network, which utilizes the EuLag Block to learn Eulerian features and Lagrangian dynamics interactively at various scales to address the complex spatiotemporal correlations in fluid prediction. Specifically, we capture the temporal evolving features at fixed points from the Eulerian perspective and the spatial dynamic information of essential particles from the Lagrangian perspective through their movement. By integrating Lagrangian pivotal dynamic information into Eulerian features, we fully model the spatiotemporal evolution of the fluid field over time and motion. Moreover, DeepLag can obtain critical trajectories within fluid dynamics with high computational efficiency by incorporating high-dimensional Eulerian space into lower-dimensional Lagrangian space.

### Overall Framework

It is widely acknowledged that fluids exhibit different motion characteristics across varying scales [3; 52]. In order to capture the intrinsic dynamics information at different scales, we track the trajectories of the key particles on \(L\) scales separately and propose a Eulerian-Lagrangian Recurrent Network to realize the interaction between Eulerian and Lagrangian information. For clarity, we omit the scale index \(l\) for primary physical quantities in this subsection, where \(l\{1,2,,L\}\).

Initializing Lagrangian particlesTo better capture complex dynamics in the fluid field, we sample by importance of the points from Eulerian perspective observations to determine initial positions for Lagrangian tracking. This is done by our dynamics sampling module. For the first predicting step \(t=P\), given observations \(_{t}=\{_{t}(_{k})|_{k}_{ l},1 k N_{l}\}^{N_{l} o}\) on all \(N_{l}\) points in observation domain \(_{l}^{d}\) of each scale, we extract their spatial dynamics by a convolutional network. We then calculate the probability matrix \(^{N_{l}}\) via softmax along spatial dimension:

\[=((_{t}) ),\] (4)

Figure 2: Three types of neural fluid prediction models (a-c) and overview of DeepLag (d). The EuLag Block accumulates the previous dynamics at each time and scale to guide the Eulerian field update and then evolves the particle movement and dynamics conditioned on the updated field.

where \(()\) consists of a convolutional layer and a linear layer, with activation function in-between. We then sample \(M_{l}\) Lagrangian tracking particles by the probability matrix at each scale:

\[_{t}=(\{_{k}\}_{k=1}^{N_{l}},),\] (5)

where \(_{t}=\{_{t,i}^{l}_{l}\}_{i=1}^{M_{l}} ^{M_{l} d}\) represents the set of sampled \(M_{l}\) particles.

Eulerian fluid prediction with Lagrangian dynamicsFor the \(t\)-th timestep, we adopt a learnable embedding layer with multiscale downsampling \(()\) to encode past observations or predictions \(U_{t}=\{_{t-P+1},,_{t}\}\) to obtain the Eulerian representations \(_{t}^{l}^{N_{l} C_{l}}\) at each scale.

We integrate the EuLag Block to fuse the Lagrangian dynamics at each scale and direct the evolution of Eulerian features. This interaction simultaneously enables Eulerian features to guide the progression of Lagrangian dynamics. For the \(l\)-th scale, we track \(M_{l}\) key particles over time, with \(_{t}=\{_{t,i}^{l}_{l}\}_{i=1}^{M_{l}}\) representing their positions and \(_{t}=\{_{t,i}^{l}^{C_{l}}\}_{i=1}^{M_{l}}\) denoting their learned particle dynamics. As shown in Figure 2(d), the positions and dynamics of the \(M_{l}\) particles at the \(l\)-th scale are learned autoregressively using the EuLag Block, which can be written as

\[_{t+1},\{_{t+1}\},\{_{t+1}\}=( _{t},\{_{t}\},\{_{t}\}),\] (6)

where the scale index \(l\) is omitted for notation simplicity. The EuLag Block learns to optimally leverage the complementary strengths of Eulerian and Lagrangian representations, facilitating mutual refinement between these two perspectives towards an exceeding performance. More details about the specific implementation of the EuLag Block are elaborated in the following subsection 3.2.

After evolving into new Eulerian features \(_{t+1}\), Lagrangian particle position \(_{t+1}\) and dynamics \(_{t+1}\) at the \(l\)-th scale, we further aggregate \(_{t+1}\) with the predicted Eulerian field at a coarser scale by upsampling \(()\). Eventually, the full-resolution prediction \(_{t+1}\) at step \(t+1\) is decoded from \(_{t+1}^{l}\) with a projection layer. We unfold the implementation of the overall architecture in Appendix A.2.

### EuLag Block

As stated in Eq. (6), we adopt a recurrent network to interactively exploit the information from two fluid descriptions, which consist of two main components: Lagrangian-guided feature evolving and Eulerian-conditioned particle tracking. The following quantities are all in the \(l\)-th scale.

Lagrangian-guided feature evolvingClassical theories  and numerical algorithms  show that fluid predictions can be solved by identifying the origin of the fluid parcel and interpolating the dependent variable from nearby grid points. However, without specifying certain PDEs, we cannot explicitly determine the former position of the particle on each Eulerian observed point. Thus, we first adaptively synthesize the Lagrangian dynamics of the tracked particles to guide the evolution of Eulerian features using a cross-attention mechanism. Formally, we adopt a Lagrangian-to-Eulerian cross-attention, where the Eulerian field \(_{t}\) serves as queries, and Lagrangian dynamics concatenated with particle position \(_{t}||_{t}^{M_{l}(C_{l}+d)}\) is used as keys and values:

\[(_{t},_{t}||_{t},_{t}||_{t})=(_{Q}_{t}( _{K}_{t}||_{t})^{}}{+ d}})_{V}_{t}||_{t},\] (7)

where \(_{Q}\), \(_{K}\) and \(_{V}\) stand for linear projections. We wrap the attention in a Transformer block with residual connections to implement the Lagrangian-dynamics-guided Eulerian evolution process:

\[_{t+1}=_{t}+(_{t},_{t }||_{t},_{t}||_{t}).\] (8)

Figure 3: Overview of the EuLag Block, which accumulates previous dynamics information to guide Eulerian evolution for predicting particle movement. Scale index \(l\) is omitted for simplicity.

Eulerian-conditioned particle trackingTraditional Lagrangian methods rely on interactions among vast quantities of particles to estimate the future fluid fields. However, the high computational cost hinders the application in deep surrogate models. In other data-driven approaches, the sparse sampling of particles is computational-friendly but cannot directly derive Lagrangian dynamics for other particles. Considering the equivalence of the Eulerian and Lagrangian representations indicated by the material derivative in Eq. (1), we propose to learn particle movements based on the Eulerian conditions. Concretely, we utilize another Eulerian-to-Lagrangian cross-attention, where the evolved dense Eulerian features are used to navigate the Lagrangian dynamics of sparse particles:

\[(_{t}||_{t},_{t+1}, _{t+1})=(_{Q}^{} _{t}||_{t})(_{K}^{}_{t+1})^{ }}{}})_{V}^{}_{t+1},\] (9)

where we use a different set of \(_{Q}^{}\), \(_{K}^{}\), and \(_{V}^{}\). Similarly, the Transformer block \(\) wrapping this attention produces the change of forecasted global Lagrangian dynamics \(_{,t}\) and movement of tracking particles \(_{t}\), which leads to the next step by residual connections:

\[_{,t+1}||_{t+1}=_{t}||_{t }+(_{t}||_{t},_{t+1}, _{t+1}).\] (10)

To better model the dynamic evolution of the particles, we gather local Lagrangian dynamic by employing bilinear interpolation to evolved Eulerian features \(_{t+1}\) on new particle position \(_{t+1}\), then use a linear function to aggregate it with global dynamics information \(_{,t+1}\):

\[_{t+1}=((_{t+1}, _{t+1}),_{,t+1}).\] (11)

Additionally, particles could move out of the observation domain as the input field may have an open boundary. We check the updated position of tracking particles and resample from the latest probability matrix \(\) to substitute the ones that exit, ensuring the validity of the Lagrangian information.

Overall, the EuLag Block can fully utilize the complementary advantages of Eulerian and Lagrangian perspectives in describing fluid dynamics, thereby being better suited for fluid prediction. For more implementation details of the EuLag Block, please refer to Appendix A.3.

## 4 Experiments

We evaluated DeepLag on three challenging benchmarks, including simulated and real-world scenarios, covering both 2D and 3D, as well as single and multi-physics fluids. Following the previous convention , we train DeepLag and the baselines for each task to predict ten future timesteps in an autoregressive fashion given ten past observations. Detailed benchmark information is listed in Table 1. We provide an elaborate analysis of the efficiency, parameter count, and performance difference in section 4.4 and Appendix E. Additionally, more detailed visualizations, besides in later this section, are provided in Appendix H. Furthermore, the trained models are engaged to perform 100 frames extrapolation to examine their long-term stability, whose results are in Appendix I.

BaselinesTo demonstrate the effectiveness of our model, we compare DeepLag with seven baselines on all benchmarks, including the classical multiscale model U-Net  and advanced neural operators for Navier-Stokes equations: FNO , Galerkin Transformer , Vortex for 2D image , GNOT , LSM  and FactFormer . U-Net has been widely used in fluid modeling, which can model the multiscale property precisely. LSM  and FactFormer  are previous state-of-the-art neural operators.

MetricsFor all three tasks, we follow the convention in neural fluid prediction [21; 52] and report relative L2 as the main metric. Implementations of the metrics are included in Appendix A.4.

ImplementationsAligned with convention and the baselines, DeepLag is trained with relative L2 as the loss function on all benchmarks. We use the Adam  optimizer with an initial learning rate of \(5 10^{-4}\) and StepLR learning rate scheduler. The batch size is set to 5, and the training process is stopped after 100 epochs. All experiments are implemented in PyTorch  and conducted on a single NVIDIA A100 GPU. Training curves are shown in Appendix D.

   Datasets & Type & \#Var & \#Dim & \#Space \\  Bounded N-S & Simulation & 1 & 2D & \(128 128\) \\ Ocean Current & Real World & 5 & 2D & \(180 300\) \\
3D Smoke & Simulation & 4 & 3D & \(32^{3}\) \\   

Table 1: Summary of the benchmarks. #Var refers to the number of observed physics quantities in fluid. #Space is the spatial resolution.

### Bounded Navier-Stokes

SetupsIn real-world applications, handling complex boundary conditions in predicting fluid dynamics is indispensable. Thus, we experiment with the newly generated Bounded Navier-Stokes, which simulates a scenario where some colored dye flows from left to right through a 2D pipe with several fixed pillars as obstacles inside. Details about this benchmark can be found in Appendix C.1.

Quantitive resultsAs shown in Table 2, DeepLag achieves the best performance on Bounded Navier-Stokes, demonstrating its advanced ability to handle complex boundary conditions. In comparison to the previous best model, DeepLag achieves a significant 13.8% (Relative L2: 0.0618 v.s. 0.0544) and 2.7% (Relative L2: 0.1020 v.s. 0.0993) relative promotion on short and long rollout. The timewise error curves of all the models are also included in Figure 4. We can find that DeepLag presents slower error growth and excels in long-term forecasting. This result may stem from the Lagrangian-guided fluid prediction, which can accurately capture the dynamics information over time, further verifying the effectiveness of our design.

ShowcasesTo intuitively present the forecasting skills of different models, we also provide showcase comparisons in Figure 4 and particle movements predicted by DeepLag in Appendix K. We can find that DeepLag can precisely illustrate the vortex in the center of the figure and give a reasonable motion mode of the Karman vortex phenomenon formed behind the upper left pillar. As for U-Net and LSM, although they successfully predicted the position of the center vortex, the error map shows that they failed to predict the density field as accurately as DeepLag. In addition, FactFormer deteriorates on this benchmark. This may be because it is based on spatial factorization, which is unsuitable for irregularly placed boundary conditions. These results further highlight the benefits of Eulerian-Lagrangian co-design, which can simultaneously help with dynamic and density prediction.

### Ocean Current

SetupsPredicting large-scale ocean currents, especially in regions near tectonic plate boundaries prone to disasters such as tsunamis due to intense terrestrial activities, plays a crucial role in various domains. Hence, we also explore this challenging real-world scenario in our experiments. More details about the source and settings of this benchmark can be found in Appendix C.2.

    &  \\   & 10 Frames & 30 Frames \\  U-Net  & 0.0618 & 0.1038 \\ FNO  & 0.1041 & 0.1282 \\ Galerkin Transformer  & 0.1084 & 0.1369 \\ Vortex  & 0.1999 & NAN \\ GNOT  & 0.1388 & 0.1793 \\ LSM  & 0.0643 & 0.1020 \\ FactFormer  & 0.0733 & 0.1195 \\  DeepLag (Ours) & **0.0543** & **0.0993** \\ Promotion & 13.8\% & 2.7\% \\   

Table 2: Performance comparison on Bounded Navier-Stokes. Relative L2 of 10 frames and 30 frames prediction are recorded. Promotion represents the relative promotion of DeepLag w.r.t the second-best (underlined).“NaN” refers to the instability during rollout.

Figure 4: Showcases (left) and timewise relative L2 (right) on Bounded Navier-Stokes dataset. Both predictions (upper row) and absolute error maps (lower row) are plotted for intuitive comparison.

Quantitative resultsWe report relative L2 for the Ocean Current dataset in Table 3, where DeepLag still achieves the best with 8.3% relative promotion w.r.t. the second-best model. Even in 30 days of forecasting, the number is 12.8%. These results show that DeepLag performs well in real-world, large-scale fluids, which usually involve more inherent stochasticity than simulated data. Moreover, we provide the ACC metric and timewise curve in Appendix F.

ShowcasesTo provide an intuitive comparison, we plotted different model predictions in Figure 5. In comparison to other models, DeepLag exhibits the most minor prediction error. It accurately predicts the location of the high-temperature region to the south area and provides a clear depiction of the Kuroshio pattern  bounded by the red box.

Learned trajectory visualizationTo reveal the effect of learning Lagrangian trajectories, we visualize tracked particles in Figure 5. We observe that all the particles move from west to east, consistent with the Pacific circulation. Additionally, the tracked particles show distinct moving patterns, confirming their ability to represent complex dynamics. The movement of upper particles matches the sinuous trajectory of the Kuroshio current, demonstrating the capability of DeepLag to provide interpretable evidence for prediction results. Visualizing the tracking points in Lagrangian space instills confidence in the reliability and interpretability of the predictions made by our model, which can provide valuable and intuitive insights for real-world fluid dynamics.

### 3D Smoke

Setups3D fluid prediction has been a long-standing challenge due to the tanglesome dynamics involved. Therefore, we generated this benchmark to describe a scenario in which smoke flows under the influence of buoyancy in a three-dimensional bounding box. For more details, please refer to Appendix C.3.

Quantitive resultsTable 4 shows that DeepLag still performs best in 3D fluid. Note that in this benchmark, the canonical deep model U-Net degenerates seriously, indicating that a pure Eulerian multiscale framework is insufficient to model complex dynamics. We also noticed that the Transformer-based neural operators, such as GNOT and Galerkin Transformer, failed in this task. This may be because both of them are based on the linear attention mechanism [19; 54], which may depreciate under massive tokens. These comparisons further highlight the capability of DeepLag to handle high-dimensional fluid.

   Model & Relative L2 (\(\)) \\  U-Net  & 0.0508 \\ FNO  & 0.0635 \\ Galerkin Transformer  & 0.1066 \\ GNOT  & 0.2100 \\ LSM  & 0.0527 \\ FactFormer  & 0.0793 \\  DeepLag (Ours) & **0.0378** \\ Promotion & 34.4\% \\   

Table 4: Performance comparison on the 3D Smoke dataset. Relative L2 with relative promotion w.r.t. the second-best model is recorded.

Figure 5: Showcase comparison and visualization of Lagrangian trajectories learned by DeepLag on Ocean Current. Notably, potential temperatures predicted by different models are plotted. Error maps of predictions are normalized to \((-4,4)\) for a better view.

    &  \\   & 10 Days & 30 Days \\  U-Net  & 0.0185 & 0.0297 \\ FNO  & 0.0246 & 0.0420 \\ Galerkin Transformer  & 0.0323 & 0.0515 \\ Vortex  & 0.9548 & NaN \\ GNOT  & 0.0206 & 0.0336 \\ LSM  & 0.0182 & 0.0290 \\ FactFormer  & 0.0183 & 0.0296 \\  DeepLag (Ours) & **0.0168** & **0.0257** \\ Promotion & 8.3\% & 12.8\% \\   

Table 3: Performance comparison on Ocean Current. We report the relative L2 of the short-term and long-term rollouts with their relative promotions.

ShowcasesIn Figure 6, we compared the prediction results on the 3D Smoke dataset. DeepLag demonstrates superior performance in capturing both the convection and diffusion of smoke within the bounding box. In contrast, the predictions made by U-Net tend to average value across various surfaces, resulting in blurred details, which also indicates its deficiency in dynamics modeling. Similarly, LSM and FactFormer exhibit more pronounced errors, particularly around the smoke boundaries, where complex wave interactions often occur. By comparison, our model significantly reduces both the overall and cross-sectional error, excelling in the prediction of fine-grained, subtle flow patterns and maintaining high accuracy even in challenging regions with intricate dynamics.

### Model Analysis

AblationsTo verify the effectiveness of detailed designs in DeepLag, we conduct exhaustive ablations in Table 5. In our original design, we track 512 particles (around 3% of Eulerian grids) in total on 4 hierarchical spatial scales with a latent dimension 64 in the Lagrangian space.

The experiments indicate that further increasing the number of particles, scales, or latent dimensions yields marginal performance improvements. Therefore, we opt for these values to strike a balance between efficiency and performance. In addition, we can conclude that all components proposed in this paper are indispensable. Especially, the lack of interaction between the Eulerian and Lagrangian space will cause a severe drop in accuracy, highlighting the dual cross-attention that exploits Lagrangian dynamics has a positive influence on the evolution of Eulerian features. Besides, rather than uniformly sampling particles, sampling from a learnable probability matrix also provides an upgrade (refer to Appendix G for visual results). When swapping the

Figure 6: Showcases comparison of the whole space (left part) and a cross-section (right part) on the 3D Smoke dataset. For better visualization, we present the absolute value of the prediction error and normalize the whole space error into \((0,0.12)\). As for the cross-section visualization, we choose the \(xOy\) plane in the middle 3D fluid and normalize error maps to \((-0.5,0.5)\).

Table 5: Ablations of **(a)** module removing and **(b)** hyperparameter sensitivity on Bounded Navier-Stokes, where (a) includes _w/o_\(\): removing Lagrangian-guided feature evolving, _w/o_\(\): removing Eulerian-conditioned particle tracking and _w/o Learnable Sampling_: removing learnable particle sampling strategy, and (b) includes _#Particle (\(M_{1}\))_: adjusting the number of tracking particles of the first layer, _#Scale (\(L\))_: adjusting the number of spatial scales and _#Latent (\(C_{1}\))_: adjusting the number of latent dimension of the first layer. We use the control variable method, when one hyperparameter is changed, the values of the others are all original (_ori_). Ablations of **(c)** attention swapping on Bounded Navier-Stokes (_2D_) and 3D Smoke (_3D_), where the order of \(\) and \(\) blocks is swapped. _Original_ and _Swapped_ Relative L2 are reported.

Figure 7: Efficiency comparison among all the models. Running time and Relative L2 are evaluated on the 3D Smoke benchmark.

positions of the \(\) and \(\) blocks, the minimal performance change in both 2D and 3D benchmarks suggests the equivalence of the two perspectives, and the flow of information between them is bidirectional and insensitive to the order, underscoring the robustness of our approach. The above results provide solid support to our motivation in tracking essential particles and utilizing Eulerian-Lagrangian recurrence, further confirming the merits of our model.

Efficiency analysisWe also include the efficiency comparison in Figure 7. DeepLag hits a favorable balance between efficiency and performance by simultaneously considering performance, model parameters, and running time, demonstrating superior performance with significantly less memory than GNOT and LSM, thereby minimizing storage complexity. Standard U-Net has a large number of parameters, and Transformers have quadratic memory, so in large-scale or complex fluid prediction scenarios, using linear complexity attention mechanisms like ours is necessary. This explains why, although U-Net and LSM are good at Bounded Navier-Stokes, they will worsen when it comes to complex fluid dynamics, such as the Ocean Current and the 3D Smoke benchmark.

Comparison under aligned efficiencyAs aforementioned, U-Net also presents competitive performance and efficiency in some cases. To carry out a fair comparison between baselines and DeepLag, we examine them on 3D Smoke, which scales up U-Net to have a running time similar to DeepLag as in Table 6(a). The results show that too many parameters overwhelm small baselines at the lower left corner in Figure 7 like U-Net, indicating that they have a shortcoming in scalability.

Generalization analysisTo verify the generalizing ability of DeepLag on larger and new domains, we ran tests on high-resolution (HR) data as in Table 6(b) and unseen boundary conditions (BC) as in Appendix J, respectively. The HR simulation on the \(256 256\) Bounded Navier-Stokes shows that the finer the data are, the more accurate our DeepLag is, which still has a 17% promotion w.r.t U-Net on relative L2 (DeepLag: 0.051, U-Net: 0.060). The comparison of GPU memory and running time per epoch also confirms that the increase of the space complexity is sublinear and the time complexity is minor, underscoring the scalability of DeepLag. For BCs, we ran a zero-shot test with the old model checkpoint on a newly generated Bounded N-S dataset, which has obstacles of different numbers, positions, and sizes. DeepLag still has a 7% promotion w.r.t the best baseline, U-Net, on relative L2 (DeepLag: 0.203, U-Net: 0.217). The visual comparison between the two models further shows that DeepLag adaptively generalizes well on unknown and more complex domains.

## 5 Conclusions and Limitations

To tackle intricate fluid dynamics, this paper presents DeepLag by introducing the Lagrangian dynamics into Eulerian fluid, which can provide clear and neat dynamics information for prediction. A EuLag Block is presented in the Eulerian-Lagrangian Recurrent Network to utilize the complementary advantages of Eulerian and Lagrangian perspectives, which brings better particle tracking and Eulerian fluid prediction. DeepLag excels in complex fluid prediction with an average improvement of nearly 10% across three carefully selected complex benchmarks, even in 3D fluid, and can also provide interpretable evidence by plotting learned Lagrangian trajectories. However, the number of tracking particles in DeepLag is appointed as a fixed hyperparameter that needs to be adjusted for specific scenarios, and missing Lagrangian supervision in the data leads to a lack of judgment on learned Lagrangian dynamics, which leave space for future exploration. That said, results show that the performance always tends to incline as we simply scale up. Therefore, the few hyperparameters for tuning depend more on the limitation of computing resources rather than blind searching.

Table 6: Experiments of **(a)** model efficiency alignment on 3D Smoke and **(b)** high-resolution data on Bounded Navier-Stokes. To compare with an aligned efficiency, we add more convolutional layers into the standard U-Net and increase the latent dimension. In the high-resolution simulation, we trained a new DeepLag model on a newly generated 256\(\)256 Bounded Navier-Stokes dataset. Model parameters (_#Param_), GPU memory (_Mem_) and running time per epoch (_Time_) are reported.