# DRAUC: An Instance-wise Distributionally Robust AUC Optimization Framework

Siran Dai\({}^{1,2}\) Qianqian Xu\({}^{3}\)   Zhiyong Yang\({}^{4}\)

**Xiaochun Cao\({}^{5}\)   Qingming Huang\({}^{4,3,6}\)1**

\({}^{1}\) SKLOIS, Institute of Information Engineering, CAS

\({}^{2}\) School of Cyber Security, University of Chinese Academy of Sciences

\({}^{3}\) Key Lab. of Intelligent Information Processing, Institute of Computing Tech., CAS

\({}^{4}\) School of Computer Science and Tech., University of Chinese Academy of Sciences

\({}^{5}\) School of Cyber Science and Tech., Shenzhen Campus of Sun Yat-sen University

\({}^{6}\) BDKM, University of Chinese Academy of Sciences

daisiran@iie.ac.cn xuqiangian@ict.ac.cn yangzhiyong21@ucas.ac.cn caoxiaochun@mail.sysu.edu.cn qmhuang@ucas.ac.cn

###### Abstract

The Area Under the ROC Curve (AUC) is a widely employed metric in long-tailed classification scenarios. Nevertheless, most existing methods primarily assume that training and testing examples are drawn i.i.d. from the same distribution, which is often unachievable in practice. Distributionally Robust Optimization (DRO) enhances model performance by optimizing it for the local worst-case scenario, but directly integrating AUC optimization with DRO results in an intractable optimization problem. To tackle this challenge, methodically we propose an instance-wise surrogate loss of Distributionally Robust AUC (DRAUC) and build our optimization framework on top of it. Moreover, we highlight that conventional DRAUC may induce label bias, hence introducing distribution-aware DRAUC as a more suitable metric for robust AUC learning. Theoretically, we affirm that the generalization gap between the training loss and testing error diminishes if the training set is sufficiently large. Empirically, experiments on corrupted benchmark datasets demonstrate the effectiveness of our proposed method. Code is available at: https://github.com/EldercatSAM/DRAUC.

## 1 Introduction

The Area Under the ROC Curve (AUC) is an essential metric in machine learning. Owing to its interpretation equivalent to the probability of correctly ranking a random pair of positive and negative examples , AUC serves as a more suitable metric than accuracy for imbalanced classification problems. Research on AUC applications has expanded rapidly across various scenarios, including medical image classification , abnormal behavior detection  and more.

However, current research on AUC optimization assumes that the training and testing sets share the same distribution , a challenging condition to satisfy when the testing environment presents a high degree of uncertainty. This situation is common in real-world applications.

Distributionally Robust Optimization (DRO) as a technique designed to handle distributional uncertainty, has emerged as a popular solution  in various applications, including machine learning , energy systems  and transportation . This technique aims to develop a model that performs well, even under the most adversarial distribution within a specified distance from the original training distribution. However, existing DRO methods primarily focus on accuracy as ametric, making it difficult to directly apply current DRO approaches to AUC optimization due to its pairwise formulation. Consequently, it prompts the following question:

_Can we optimize the Distributionally Robust AUC (DRAUC) using an end-to-end framework?_

This task presents three progressive challenges: **1**): The pairwise formulation of AUC necessitates simultaneous access to both positive and negative examples, which is computationally intensive and infeasible in online settings. **2)**: The naive integration of AUC optimization and DRO leads to an intractable solution. **3)**: Based on a specific observation, we find that the ordinary setting of DRAUC might lead to severe label bias in the adversarial dataset.

In this paper, we address the aforementioned challenges through the following techniques: For **1)**, we employ the minimax reformulation of AUC and present an early trail to explore DRO under the context of AUC optimization. For **2)**, we propose a tractable surrogate loss that is proved to be an upper bound of the original formulation, building our distribution-free DRAUC optimization framework atop it. For **3)**, we further devise distribution-aware DRAUC, to perform class-wise distributional perturbation. This decoupled formulation mitigates the label noise issue. This metric can be perceived as a class-wise variant of the distribution-free DRAUC.

It is worth noting that  also discusses the combination of DRO techniques with AUC optimization. However, the scope of their discussion greatly differs from this paper. Their approach focuses on using DRO to construct estimators for partial AUC and two-way partial AUC optimization with convergence guarantees, whereas this paper primarily aims to enhance the robustness of AUC optimization.

The main contributions of this paper include the following:

* **Methodologically**: We propose an approximate reformulation of DRAUC, constructing an instance-wise, distribution-free optimization framework based on it. Subsequently, we introduce the distribution-aware DRAUC, which serves as a more appropriate metric for long-tailed problems.
* **Theoretically**: We conduct a theoretical analysis of our framework and provide a generalization bound derived from the Rademacher complexity applied to our minimax formulation.
* **Empirically**: We assess the effectiveness of our proposed framework on multiple corrupted long-tailed benchmark datasets. The results demonstrate the superiority of our method.

## 2 Related Works

### AUC Optimization

AUC is a widely-used performance metric. AUC optimization has garnered significant interest in recent years, and numerous research efforts have been devoted to the field. The researches include different formulations of objective functions, such as pairwise AUC optimization , instance-wise AUC optimization [49; 26; 50], AUC in the interested range (partial AUC , two-way partial AUC ), and area under different metrics (AUPRC [35; 44; 45], AUTKC , OpenAUC . For more information, readers may refer to a review on AUC .

Some prior work investigates the robustness of AUC. For instance,  improves the robustness on noisy data and  studies the robustness under adversarial scenarios. In this paper, we further explore robustness under the local worst distribution.

### Distributionally Robust Optimization

DRO aims to enhance the robustness and generalization of models by guaranteeing optimal performance even under the worst-case local distribution. To achieve this objective, an ambiguity set is defined as the worst-case scenario closest to the training set. A model is trained by minimizing the empirical risk on the ambiguity set. To quantify the distance between distributions, prior research primarily considers \(-divergence\)[2; 16; 4; 31] and the Wasserstein distance [39; 28; 19; 3; 7] as distance metrics. For more details, readers may refer to recent reviews on DRO [28; 23].

DRO has applications in various fields, including adversarial training , long-tailed learning , label shift , etc. However, directly optimizing the AUC on the ambiguity set remains an open problem.

## 3 Preliminaries

In this subsection, we provide a brief review of the AUC optimization techniques and DRO techniques employed in this paper. First, we introduce some essential notations used throughout the paper.

We use \(z\) to denote the example-label pair, and \(f_{}:\) to represent a model with parameters \(\). This is typical when connecting a Sigmoid function after the model output. For datasets, \(\) denotes the nominal training distribution with \(n\) examples, while \(P\) represents the testing distribution. We use \(_{+}=\{x_{1}^{+},...,x_{n^{+}}^{+}\}\) and \(_{-}=\{x_{1}^{-},...,x_{n^{-}}^{-}\}\) to denote positive/negative training set, respectively. To describe the degree of imbalance of the dataset, we define \(=}{n^{+}+n^{-}}\) as the imbalance ratio of training set, and \(p=(y=1)\) as the imbalance ratio of testing distribution. The notation \(_{P}\) signifies the expectation on distribution \(P\). We use \(c(z,z^{})=||z-z^{}||_{2}^{2}\) to denote the cost of perturbing example \(z\) to \(z^{}\).

### AUC Optimization

Statistically, AUC is equivalent to the Wilcoxon-Mann-Whitney test , representing the probability of a model predicting a higher score for positive examples than negative ones

\[AUC(f_{})=}_{P_{+},P_{-}}[_{0,1}(f_{ {}}(^{+})-f_{}(^{-}))]\] (1)

where \(_{0,1}()\) denotes the 0-1 loss, i.e., \(_{0,1}(x)=1\) if \(x<0\) and otherwise \(_{0,1}(x)=0\). Based on this formulation, maximizing AUC is equivalent to the following minimization problem

\[_{}}_{P_{+},P_{-}}[(f_{} (^{+})-f_{}(^{-}))]\] (2)

where \(\) is a differentiable, consistent surrogate loss of \(_{0,1}\). However, the pairwise formulation of the above loss function is not applicable in an online setting. Fortunately,  demonstrates that using the square loss as a surrogate loss, the optimization problem (2) can be reformulated as presented in the following theorem.

**Theorem 1** ().: _When using square loss as the surrogate loss, the AUC maximization is equivalent to_

\[_{}}_{P_{+},P_{-}}[(f_{}(^{+})-f_{}(^{-}))]=_{,a,b}_{}\ \ }_{P}[g(a,b,,,)]\] (3)

_where_

\[ g(a,b,,,)&=(1 -p)(f_{}()-a)^{2}_{[y=1]}+p(f_{}()-b)^{2}_{[y=0]}\\ &+2(1+)(p f_{}() _{[y=0]}-(1-p) f_{}()_{[y=1]}- p(1-p)^{2}).\] (4)

_Moreover, with the parameter \(\) fixed, the optimal solution of \(a,b,\), denoted as \(a^{},b^{},^{}\), can be expressed as:_

\[a^{}=}_{P_{+}}[f_{}(^{+})],b^{}=}_{P_{-}}[f_{}(^{-}) ],^{}=b^{}-a^{}.\] (5)

**Similar results hold if the true distribution \(P_{+},P_{-}\) in the expressions are replaced with \(_{+},_{-}\).**

**Remark 1** (**The constraints on \(a,b,\)**).: _Given that the output of the model \(f_{}\) is restricted to \(\), \(a,b,\) can be confined to the following domains:_

\[&_{a,b}=\{a,b|0 a,b, 1\},\\ &_{}=\{|-1 1\}.\] (6)

_So that the minimax problem can be reformulated as:_

\[_{,(a,b)_{a,b}}_{_{}}\ \ }_{P}[g(a,b,,,)].\] (7)

### Distributionally Robust Optimization

Distributionally Robust Optimization (DRO) aims to minimize the learning risk under the local worst-case distribution. Practically, since we can only observe empirical data points, our discussion is primarily focused on empirical distributions. Their extension to population-level is straightforward

\[_{}_{:d(,)} }{}[(f_{},z)]\] (8)

where \(\) is the original empirical distribution, \(\) is the perturbed distribution and \(d\) is the metric of distributional distance. The constraint \(d(,)\) naturally expresses that the perturbation induced \(\) should be small enough to be imperceptible.

As demonstrated in , when employing the Wasserstein distance \(_{c}\) as the metric, a Lagrangian relaxation can be utilized to reformulate DRO into the subsequent minimax problem.

**Theorem 2** ().: _With \(_{}(z,)=_{z^{}}\{(f_{},z^{})- c(z,z^{})\}\), for all distribution \(\) and \(>0\), we have_

\[_{:_{c}(,)} }{}[(f(z))]=_{ 0}\{ +}{}[_{}(z,)]\}.\] (9)

With the theorem above, one can directly get rid of the annoying Wasserstein constraint in the optimization algorithms. We will use this technique to derive an AUC-oriented DRO framework in this paper.

## 4 Method

### Warm Up: A Naive Formulation for DRAUC

As a technical warm up, we first start with a straightforward approach to optimize AUC metric directly under the worst-case distribution. By simply incorporating the concept of the Wasserstein ambiguity set, we obtain the following definition of DRAUC in a pairwise style.

**Definition 1** (Pairwise Formulation of DRAUC).: _Let \(\) be a consistent loss of \(_{0,1}\), for any nominal distribution \(\) and \(>0\), we have_

\[DRAUC_{}(f_{},)=1-_{: _{c}(,)}}{} [(f_{}(^{+})-f_{}(^{-}) )].\] (10)

However, generating local-worst Wasserstein distribution \(\) is loss-dependent, implying that we need to know all the training details to deliver a malicious attack. In our endeavor to secure a performance guarantee for our model, we cannot limit the scope of information accessible to an attacker. This pairwise formulation elevates the computational complexity from \(O(n)\) to \(O(n^{+}n^{-})\), significantly increasing the computational burden. By a simple reuse of the trick in (7), one can immediately reach the following reformulation of the minimization of (10).

**Proposition 1** (A Naive Reformulation).: _When using square loss as the surrogate loss, The DRAUC minimization problem: \(_{}DRAUC_{}(f_{},)\), is equivalent to_

\[_{}_{:_{c}(,)}_{(a,b)_{a,b}}_{ _{}}\ }{}[g(a,b,,,z_{i}) ].\] (11)

Unfortunately, the optimization operators adhere to a min-max-min-max fashion. There is no known optimization algorithm can deal with this kind of problems so far. Hence, in the rest of this section, we will present two tractable formulations as proper approximations of the problem.

### DRAUC-Df: Distribution-free DRAUC

Let us take a closer look at the minimax problem \(()\). It is straightforward to verify that, fix all the other variables, \(g\) is convex with respect to \(a,b\) and concave with respect to \(\) within \(_{a,b},_{}\). Weare able to interchange the inner \(_{(a,b)_{,b}}\) and \(_{_{}}\) by invoking von Neumann's Minimax theorem , which results in

\[_{}_{:_{c}(,) }_{_{}}_{(a,b)_{,b}} [g(a,b,,,z)].\] (12)

Moreover, based on the simple property that \(_{x}_{y}f(x,y)_{y}_{x}f(x,y)\), we reach an upper bound of the objective function:

\[:_{c}(,) }_{_{}}_{(a,b)_{a,b}}[ g(a,b,,,z)]}_{DRAUC_{}(f_{},)} }_{_{}}_{ :_{c}(,)}[ g(a,b,,,z)]}_{D_{c}(f_{},)}\] (13)

From this perspective, if we minimize \(_{}(f_{},)\) in turn, we can at least minimize an **upper bound** of \(DRAUC_{}(f_{},)\). In light of this, we will employ the following optimization problem as a surrogate for **(Ori)**:

\[_{}_{_{}}_{ {Q}:_{c}(,)}[g(,,z)]\] (14)

where \(=,(a,b)_{a,b}\). Now, by applying the strong duality to the inner maximization problem

\[_{:_{c}(,)} [g(,,z)]\]

we have

\[_{}_{_{}}_{ 0 }\{+[_{,,}(z)]\}\] (15)

where \(_{,,}(z)=_{z^{}}[g( ,,z)- c(z,z^{})]\). This min-max-min formulation remains difficult to optimize, so we take a step similar to (13) that interchange the inner \(_{ 0}\) and outer \(_{_{}}\), resulting in a tractable **upper bound**

\[_{}_{ 0}_{_{ }}\{+[_{,,}(z)]\}.\] (16)

In this sense, we will use the \(\) as the final optimization problem for **DRAUC-Df**.

```
1:Input: the training data \(\), step number \(K\), step size for inner \(K\)-step gradient ascent \(_{z}\), learning rates \(_{}\), \(_{}\), \(_{}\) and maximal corrupt distance \(\).
2:Initialize: initialize \(a^{0},b^{0},^{0}=0,^{0}=_{0}\).
3:for\(t=1\)to\(T\)do
4: Sample a batch of example \(z\) from \(\).
5: Generate Local Worst-Case Examples:
6: Initialize \(z^{}=z\).
7:for\(k=1\)to\(K\)do
8:\(z^{}=_{Z}(z^{}+_{z}_{z}_{^{},a,b,}(,z^{}))\).
9:endfor
10: Update Parameters:
11: Update \(^{t+1}=_{_{}}(^{t}+_{}_{ }g^{t}(z^{}))\).
12: Update \(^{t+1}=_{_{}}(^{t}-_{}_{ }[+_{^{t},a,b,}(,z^{})])\).
13: Update \(^{t+1}=_{_{}}(^{t}-_{} _{}g^{t}(z^{}))\).
14:endfor ```

**Algorithm 1** Algorithm for optimizing DRAUC-Df:

### DRAUC-Da: Distribution-aware DRAUC

Though AUC itself is inherently robust toward long-tailed distributions, we also need to examine whether DRAUC shares this resilience. We now present an analysis within a simplified feature space on the real line, where positive and negative examples are collapsed to their corresponding clusters. The choice of the feature space is simple yet reasonable since it is a 1-d special case of the well-accepted neural collapse phenomenon [32; 10; 17; 57; 27].

Specifically, the following proposition states that the distributional attacker in DRAUC can ruin the AUC performance easily by merely attacking the tail-class examples.

**Proposition 2** (**Powerful and Small-Cost Attack on Neural Collapse Feature Space**).: _Let the training set comprises \(n^{+}\) positive examples and \(n^{-}\) negative examples in \(^{1}\), i.e., \(=\{x_{1}^{+},...,x_{n^{+}}^{+},x_{n^{+}+1}^{-},...,x_{n^{-}}^{-}\}\), with the empirical distribution \(=_{i=1}^{n}_{x_{i}}\) (\(_{z}\) represents the Dirac point mass at point \(z\).). According to the neural collapse assume, we have: \(x_{i}^{+}=x^{+}\), \(x_{j}^{-}=x^{-}\). Given a classifier \(f(x)=x\), we assume that the maximization of perturb distribution \(\) is further constrained on the subset:_

\[=\{:=_{i=1}^{n}_{x_ {i}^{}}\}\]

_where \(x_{i} x_{i}^{}\) forms a discrete Monge map. Then, we have:_

\[_{,AUC(f,)=0}_{c}( {P},)(1-)(x^{+}-x^{-})^{2}\]

_where \(=}{n}\) is the ratio of the positive examples in the dataset. Moreover, the cost \((1-)(x^{+}-x^{-})^{2}\) is realized by setting:_

\[{x^{+}}^{}={x^{-}}^{}= x^{+}+(1-) x ^{-}\]

_the barycenter of the two-bodies system \((x^{+},x^{-})\)._

It is noteworthy that \((1-)\) reflects the degree-of-imbalanceness, which is relatively small for long-tailed datasets. Moreover, the barycenter tends to be pretty close to the head-class examples. Therefore, only the tail-class examples are required to be revised heavily during the attack. In this sense, the attacker can always exploit the tail class examples as a backdoor to ruin the AUC performance with small Wasserstein cost. This is similar to the overly-pessimistic phenomena  in DRO. The following example shows how small such cost could be in a numerical sense.

**Example 1**.: _Consider a simplified setting in which the training set is comprised of only one positive example and 99 negative examples, i.e., \(=\{x_{1}^{+},x_{2}^{-},...,x_{100}^{-}\}\) with \(x^{+}=0.99\) and \(x^{-}=0.01\). The minimum distance required to perturb the AUC metric from \(1\) to \(0.009702\). This result is achieved by perturbing the positive example from \(0.99\) to \(0.0198\) and the negative examples from \(0.01\) to \(0.0198\), respectively._

This perturbation strategy indicates a preference towards strong attack on tail-class examples. The resulting distribution \(\) is always highly biased toward the original distribution, despite the small Wasserstein cost. In the subsequent training process, one has to minimize the expected loss over \(\), resulting to label noises.

Therefore, it is natural to consider perturbations on the positive and negative distributions separately to avoid such a problem. Accordingly, we propose here a distribution-aware DRAUC formulation:

**Definition 2** (**Distribution-aware DRAUC**).: _Let \(\) be a consistent loss of \(_{0,1}\), for any nominal distribution \(\) and \(_{+},_{-}>0\), we have_

\[DRAUC_{_{+},_{-}}^{Da}(f_{},)= 1-_{_{+},_{c}(_{+}, _{+})_{+}\\ _{-}:_{c}(_{-},_{-})_ {-}}_{_{+},_{-}}[(f_{ }(x_{i}^{+})-f_{}(x_{j}^{-}))].\] (17)

For simplicity, let us denote

\[}=\{|\;_{c}(_{+}, {P}_{+})_{+},_{c}(_{-},_{-}) _{-}\}\] (18)

Similar to **DRAUC-Df**, we construct our reformulation as follows:

\[_{}_{_{}}_{ }}}_{_{+}, _{-}}[g(a,b,,,z_{i})].\] (19)

Moreover, we conduct a similar derivation as **DRAUC-Df**, to construct a tractable upper bound:

\[_{}_{_{+},_{-} 0}_{ _{}}\{_{+}_{+}+_{-}_{-}+ }_{_{+}}[_{,_{+},}(z)]+(1-)}_{_{-}}[_{ ,_{-},}(z)]\}\] (20)

where \(_{,_{+},}(z)=_{z^{}}[g( ,,z)-_{+}c(z,z^{})]\) and \(_{,_{-},}(z)=_{z^{}}[g( ,,z)-_{-}c(z,z^{})]\). Please see Appendix A for the details.

### Algorithm

#### 4.4.1 DRAUC Optimization

Motivated by the above reformulation, we propose our DRAUC optimization framework, where we solve this optimization problem alternatively.

**Inner maximization problem :**\(K\)**-step Gradient Ascent**: Following , we consider accessing \(K\)-step gradient ascent with learning rate \(_{z}\) to solve the inner maximization problem, which is widely used in DRO and can be considered as a variance of PGM. For \(\), we use SGA with a step size \(_{}\).

**Outer minimization problem: Stochastic Gradient Descent**: On each iteration, we apply stochastic gradient descent over \(w\) with learning rate \(_{w}\) and over \(\) with learning rate \(_{}\).

See Algorithms 1,2 for more details.

```
1:Input: the training data \(\), step number \(K\), step size for inner \(K\)-step gradient ascent \(_{z}\), learning rates \(_{},_{w},_{}\) and maximal corrupt distance \(_{+},_{-}\).
2:Initialize: initialize \(a^{0},b^{0},^{0}=0,_{+}^{0}=_{-}^{0}=_{0}\).
3:for\(t=1\)to\(T\)do
4:Sample a batch of example \(z\) from \(\).
5:Generate Local Worst-Case Examples:
6:Initialize \(z^{}_{+}=z_{+},z^{}_{-}=z_{-}\).
7:for\(k=1\)to\(K\)do
8:\(z^{}_{+}=_{}(z^{}_{+}+_{z}_{z}_ {^{}_{+},a,b,}(,z^{}_{+}))\).
9:\(z^{}_{-}=_{}(z^{}_{-}+_{z}_{z}_ {^{}_{-},a,b,}(,z^{}_{-}))\).
10:endfor
11:Update Parameters:
12: Update \(^{t+1}=_{_{}}(^{t}+_{}(p_{a} g^{t}(z^{}_{+})+(1-p)_{a}g^{t}(z^{}_{-})))\).
13: Update \(^{t+1}_{+}=_{_{}}(^{t}_{+}-_{l}_ {_{+}}[_{+}_{+}+_{^{t}_{+},a,b,}(,z^{}_{+})])\).
14: Update \(^{t+1}_{-}=_{_{}}(^{t}_{-}-_{l} _{_{-}}[_{-}_{-}+_{^{t}_{-},a,b,}(,z^{}_{-})])\).
15: Update \(^{t+1}=_{_{}}(^{t}-_{} (_{}g^{t}(z^{}_{+})+(1-) _{}g^{t}(z^{}_{-})))\).
16:endfor ```

**Algorithm 2** Algorithm for optimizing DRAUC-Da:

### Generalization Bounds

In this section, we theoretically show that the proposed algorithm demonstrates robust generalization in terms of DRAUC-Da metric, even under local worst-case distributions. That is, we show that a model sufficiently trained under our approximate optimization (\(\)) enjoys a reasonable performance guarantee in DRAUC-Da metric. Our analysis based on the standard assumption that the model parameters \(\) are chosen from the hypothesis set \(\)(such as neural networks of a specific structure). To derive the subsequent theorem, we utilize the results analyzed in Section 4.3 and perform a Rademacher complexity analysis of DRAUC-Da. The proof for DRAUC-Df follows a similar proof and is much simpler, thus we omit the result here. For additional details, please refer to Appendix A.

**Theorem 3** (**Informal Version**).: _For all \(,_{+},_{-} 0,(a,b)_{a,b}, _{}\) and \(_{+},_{-}>0\), the following inequality holds with a high probability_

\[,_{-}}^{Da}(f_{},P)}_{( {})}}}_{()}+(})}_{()}\] (21)

_where \(\) is some normalized sample size and \(}=_{}_{_{+},_{-} 0} _{_{}}\{_{+}_{+}+_{-}_{ -}+}_{_{+}}[_{, _{+},}(z)]+(1-)}_{_{-}}[_{,_{-},}(z)]\}\)._

In Thm.3, **(a)** represents the robust AUC loss in terms of expectation, \(()\) denotes the training loss that we use to optimize our model parameters, and \(()\) is an error term that turns to zero when thesample size turns to infinity. In this sense, if we train our model sufficiently within a large enough training set, we can achieve a minimal generalization error.

## 5 Experiments

In this section, we demonstrate the effectiveness of our proposed framework on three benchmark datasets with varying imbalance ratios.

### Experiment Settings

We evaluate our framework using the following approach. First, we conduct a binary, long-tailed training set. Then, we proceed to train the model on the long-tailed training set with varying imbalance ratios, tune hyperparameters on the validation set, and evaluate the model exhibiting the highest validation AUC on the corrupted testing set. For instance, we train our model on binary long-tailed MNIST , CIFAR10, CIFAR100 , and Tiny-ImageNet , and evaluate our proposed method on the corrupted version of corresponding datasets [30; 13; 14]. Furthermore, we compare our results with multiple competitors including the baseline (CE), typical methods for long-tailed problems [24; 52; 56] and DRO methods [55; 20; 37; 34]. Please see Appendix B for more details.

### Results and Analysis

#### 5.2.1 Overall Performance

The overall performances on CIFAR10 and Tiny-ImageNet are presented in Table 1 and Table 2, respectively. We further compare model performances by altering the level of perturbation, with results displayed in Figure 1. Due to the space limitation, we attach results on MNIST and CIFAR100 in Appendix B. Based on these findings, we make the following observations:

**Effectiveness.** Our proposed method outperforms all competing approaches across Corrupted MNIST, CIFAR10, CIFAR100 and Tiny-ImageNet datasets for all imbalance ratios, thereby substantiating its effectiveness. Additionally, our approach exhibits enhanced performance as the level of perturbation intensifies, indicating its robustness in challenging testing scenarios.

**Ablation results.** Given that our method is modified on AUCMLoss , the results presented in Figure 1 can be treated as ablation results. Under the same hyperparameters of AUCMLoss, our method exhibits significant improvement over the baseline, indicating enhanced model robustness.

**Advantage of Distribution-awareness.** As presented in Table 1, DRAUC-Da attains higher scores than DRAUC-Df across almost all corrupted scenarios. This supports our hypothesis that a strong attack on tail-class examples can potentially compromise model robustness.

**Performances on non-corrupted data.** Within non-corrupted datasets, our approach continues to exhibit competitive performance under conditions of extreme data imbalance, specifically when the imbalance ratio equals to \(0.01\). However, with less imbalanced training data, our method may suffer performance degradation, attributable to the potential trade-off between model robustness and clean performance, which is an unavoidable phenomenon in Adversarial Training .

Figure 1: Overall Performance of ResNet32 Across Perturbation Levels on CIFAR10. This graph illustrates the performance of various methods at different corruption levels, with Level 0 indicating no corruption and Level 5 representing the most severe corruption. In each figure, the seven lines depict the test AUC for CE, AUCMLoss, FocalLoss, ADVShift, WDRO, DROIT, GLOT, AUCDRO, DRAUC-Da and DRAUC-Df, respectively. Best viewed in colors.

    &  &  &  \\   & & 0.01 & 0.05 & 0.10 & 0.20 & 0.01 & 0.05 & 0.10 & 0.20 \\   & CE & 62.48 & 75.87 & 83.13 & 86.20 & 65.43 & 84.12 & **92.32** & **95.68** \\  & AUCMLoss & 63.93 & 76.77 & 81.75 & 85.26 & **68.88** & 84.74 & 90.97 & 94.40 \\  & FocalLoss & 56.56 & 74.44 & 81.81 & 84.97 & 57.63 & 81.62 & 91.33 & 94.62 \\  & ADVShift & 61.36 & 75.97 & 83.78 & 87.35 & 64.97 & 82.91 & 87.87 & 95.46 \\  & WDRO & 63.19 & 78.90 & 80.59 & 86.02 & 68.80 & **88.54** & 91.04 & 94.04 \\  & DROLT & 59.92 & 77.51 & 81.09 & 86.46 & 60.99 & 85.76 & 91.35 & 95.17 \\  & GLOT & 63.98 & 77.19 & 83.33 & 87.57 & 65.95 & 88.37 & 90.51 & 94.62 \\  & AUCDRO & 63.35 & 76.19 & 81.82 & 85.96 & 67.14 & 84.00 & 90.92 & 94.88 \\   & DRAUC-Df & 65.58 & **80.18** & 85.71 & 88.83 & 68.12 & 86.47 & 90.57 & 94.17 \\  & DRAUC-Da & **66.06** & 80.13 & **85.91** & **89.51** & 68.71 & 84.43 & 90.30 & 93.76 \\   & CE & 64.43 & 78.79 & 83.12 & 86.89 & 66.05 & 84.40 & 90.44 & **95.61** \\  & AUCMLoss & 64.00 & 76.98 & 81.87 & 85.66 & **68.90** & 84.94 & **91.52** & 95.16 \\  & FocalLoss & 56.96 & 76.53 & 83.82 & 87.42 & 58.04 & 82.99 & 91.02 & 95.16 \\  & ADVShift & 55.74 & 72.42 & 83.47 & 88.32 & 66.73 & 79.36 & 87.88 & 94.95 \\  & WDRO & 64.51 & 78.45 & 83.87 & 88.03 & 68.16 & **86.48** & 90.11 & 95.23 \\  & DROLT & 63.66 & 76.71 & 83.93 & 88.42 & 65.40 & 84.68 & 90.11 & 95.51 \\  & GLOT & 62.59 & 77.21 & 83.67 & 87.30 & 64.53 & 82.62 & 89.59 & 94.62 \\  & AUCDRO & 65.10 & 71.23 & 81.45 & 86.23 & 68.69 & 78.51 & 90.67 & 95.07 \\   & DRAUC-Df & 65.44 & 80.27 & 85.70 & **90.62** & 67.11 & 85.03 & 90.63 & 94.86 \\  & DRAUC-Da & **65.50** & **80.57** & **86.25** & 90.15 & 68.51 & 85.03 & 90.98 & 94.27 \\   

Table 1: Overall Performance on CIFAR10-C and CIFAR10-LT with different imbalance ratios and different models. The highest score on each column is shown with **bold**, and we use darker color to represent higher performance.

    &  &  &  \\   & & Dogs & Birds & Vehicles & Dogs & Birds & Vehicles \\   & CE & 78.46 & 85.19 & 87.53 & 93.72 & 94.49 & 97.72 \\  & AUCMLoss & 77.35 & 85.98 & 82.37 & 93.35 & 94.11 & 97.34 \\  & FocalLoss & 78.34 & 81.48 & 86.55 & 93.25 & 92.87 & 97.66 \\  & ADVShift & 81.20 & 80.94 & 86.65 & 93.70 & 93.53 & 97.66 \\  & WDRO & 82.20 & 85.23 & 85.92 & 94.46 & 95.50 & **98.19** \\  & DROLT & 80.44 & 86.91 & 86.76 & 93.89 & **96.40** & 97.86 \\  & GLOT & 81.96 & 85.89 & 86.80 & **94.67** & 96.14 & 98.05 \\  & AUCDRO & 75.97 & 83.26 & 79.46 & 92.58 & 93.04 & 96.29 \\   & DRAUC-Df & **84.11** & 87.30 & 88.67 & 93.39 & 95.58 & 97.50 \\  & DRAUC-Da & 83.96 & **87.61** & **89.06** & 93.76 & 95.94 & 97.25 \\   & CE & 82.55 & 84.64 & 86.26 & 94.31 & 94.49 & 97.76 \\  & AUCMLoss & 77.25 & 85.20 & 81.12 & 93.19 & 95.19 & 97.57 \\  & FocalLoss & 77.96 & 79.80 & 85.33 & 93.41 & 92.85 & 97.78 \\  & ADVShift & 84.30 & 84.56 & 86.43 & 92.92 & 94.71 & 97.59 \\  & WDRO & 80.08 & 85.58 & 86.94 & 94.39 & 95.51 & 97.67 \\  & DROLT & 79.25 & 85.75 & 86.79 & 91.68 & **96.06** & 97.82 \\  & GLOT & 81.70 & 83.09 & 88.24 & 94.08 & 95.16 & **97.92** \\  & AUCDRO & 78.21 & 80.55 & 85.26 & 91.56 & 93.15 & 96.33 \\   & DRAUC-Df & **85.79** & **88.00** & 88.32 & **94.43** & 95.29 & 97.37 \\  & DRAUC-Da & 84.56 & 87.60 & **88.46** & 94.03 & 95.96 & 97.65 \\   

Table 2: Overall Performance on Tiny-ImageNet-C and Tiny-ImageNet-LT with different imbalance ratios and different models. The highest score on each column is shown with **bold**, and we use darker color to represent higher performance.

#### 5.2.2 Sensitivity Analysis

**The Effect of \(\).** In Figure 2-(a)-(d), we present the sensitivity of \(\). The results demonstrate that when the training set is relatively balanced (i.e., the imbalance ratio \(p 0.1\)), the average robust performance improves as \(\) increases. Nonetheless, when the training set is highly imbalanced, the trend is less discernible due to the instability of the training process in these long-tailed settings.

**The Effect of \(_{}\).** In Figure 2-(e)-(h), we present the sensitivity of \(_{}\). \(_{}\) governs the rate of change of \(\) and serves as a similar function to the warm-up epochs in AT. When \(_{}\) is small, \(\) remains large for an extended period, so the adversarial example is regularized to be less offensive. In cases where the training set is extremely imbalanced, a large \(_{}\) introduces strong examples to the model while it struggles to learn, increasing the instability of the training process and explaining why the smallest \(_{}\) performs best with an imbalance ratio of \(0.01\). Conversely, when the model does not face difficulty fitting the training data, an appropriately chosen \(_{}\) around 0.1 enhances the model's robustness.

## 6 Conclusion and Future Works

This paper presents an instance-wise, end-to-end framework for DRAUC optimization. Due to the pairwise formulation of AUC optimization, a direct combination with DRO is intractable. To address this issue, we propose a tractable surrogate reformulation on top of the instance-wise formulation of AUC risk. Furthermore, through a theoretical investigation on the neural collapse feature space, we find that the distribution-free perturbation is a scheme that might induce heavy label noise into the dataset. In this sense, we propose a distribution-aware framework to handle class-wise perturbation separately. Theoretically, we show that the robust generalization error is small if both the training error and \((1/})\) is small. Finally, we conduct experiments on three benchmark datasets employing diverse model structures, and the results substantiate the superiority of our approach.

Owing to space constraints, not all potential intersections between AUC optimization and distributionally robustness can be exhaustively explored in this paper. Numerous compelling aspects warrant further investigation. We offer a detailed, instance-wise reformulation of DRAUC, primarily evolving from an AUC optimization standpoint. Future discussions could benefit from initiating dialogue from the angle of DRO. Additionally, integrating various formulations of AUC such as partial AUC and AUPRC with distributional robustness presents a fertile ground for exploration. The existence of a potentially overly-pessimistic phenomenon is yet to be conclusively determined, which paves the way for future inquiries and discoveries.