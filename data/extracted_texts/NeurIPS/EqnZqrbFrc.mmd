# Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning

Arnaud Robert

Brain & Behaviour Lab

Dept. of Computing

Imperial College London, UK

a.robert20@imperial.ac.uk

&Ciara Pike-Burke

Dept. of Mathematics

Imperial College London, UK

c.pike-burke@imperial.ac.uk

&A. Aldo Faisal

Brain & Behaviour Lab

Depts. of Computing & Bioengineering

Imperial College London, UK

Chair in Digital Health & Data Science

University of Bayreuth, Germany

a.faisal@imperial.ac.uk

###### Abstract

Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains, nor any theoretically-grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical \(n\)-rooms, Gymnasium's Taxi). The hierarchical \(n\)-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.

## 1 Motivation

Hierarchical Reinforcement Learning (HRL) [27; 8; 9; 4] leverages the hierarchical decomposition of a problem to build algorithms that are more sample efficient. While there is significant empirical evidence that hierarchical implementations can drastically improve the sample efficiency of Reinforcement Learning (RL) algorithms [20; 21; 29; 8], there are also cases where temporal abstraction worsens the empirical sample complexity . Therefore, a natural question to ask is: when does HRL lead to improved sample complexity, and how much of an improvement can it provide?

Theoretical work on sample-complexity bound in Machine Learning has been integral to the development of the field. Moreover, theoretical results (e.g. [7; 18; 3; 15; 26]) often uncover interesting principles useful for improving algorithm design. For example, the Q-learning algorithm analysed in  improved our understanding of exploration strategies in model-free RL and the policy gradienttheorem  gave birth to a wide range of new RL methods. In contrast, there are few theoretical results in hierarchical RL and many key studies are empirical, e.g. hierarchies of states [8; 10], time , or action [28; 22; 2].

To address this gap in the literature, we consider a tabular version of the goal-based approach to HRL [20; 4], and we analyze the induced MDP decomposition to derive a lower bound on the sample complexity of this specific HRL framework. This lower bound allows us to understand when a hierarchical decomposition is beneficial and motivates a new hierarchical Q-learning algorithm that can leverage the hierarchical structure to improve its sample efficiency. In the goal-based HRL framework, a high-level policy and a low-level policy are jointly learned to solve an overarching goal. In such a goal-hierarchical RL system, the high-level policy chooses a sub-goal for the low-level policy, which in turn executes primitive actions to solve the sub-goal (Fig. 1, left diagram). This natural way to break down tasks is universal (i.e., it can be applied to a wide range of tasks) and it induces a decomposition of the original MDP into two sub-MDPs (detailed in Sec. 2.2).

This paper improves our understanding of HRL through the following contributions:

* We provide a lower bound on the sample complexity associated with the hierarchical decomposition (see Sec. 3). This lower bound allows practitioners to quantify the efficiency gain they might obtain from decomposing their task.
* We propose a simple, yet novel, Q-learning-type algorithm for goal-hierarchical RL, inspired by the type of decomposition considered (see Sec. 4).
* We empirically validate the theoretical findings using a synthetic task with hierarchical properties that can be scaled in complexity (see Sec. 5). This evidence confirms that the derived bound is able to successfully identify instances where a hierarchical decomposition could be beneficial (see Sec. 5).

## 2 Background

We consider a system where an agent needs to make a sequence of decisions in an uncertain environment to maximise the sum of cumulated rewards. Such problems are modelled as Markov Decision Processes (MDPs) and can be solved by RL algorithms . When a task is too complex, the number of interactions required to learn a near-optimal policy becomes prohibitive. The task complexity typically depends on the difficulty of temporal credit assignment (which is directly related to the episode length) and the size of the state and action spaces . To address this complexity, HRL leverages temporal abstractions  and state abstractions  to improve sample efficiency when learning an optimal policy. There exists a wide range of HRL frameworks; see  for a survey. In this paper, we focus on the goal-conditioned HRL framework [20; 4]. Of the other HRL frameworks, only the options framework  and the resulting semi-Markov Decision Process [12; 30; 5; 11] benefit from some theoretical understanding. However, in practice, the goal-conditioned hierarchical

Figure 1: The left block diagram depicts the interactions between the different components of our goal-conditioned hierarchical agent. The diagram suggests that the agent is composed of a low-level policy and a high-level policy that collaborate in order to solve a task. The high-level policy \(^{h}\) observes the pair \((s_{h},r_{h})\), which denotes the high-level state and reward. It then sends a sub-goal \(g_{sub}\) as an input to the low-level policy \(^{l}\). \(^{l}\) observes the pair \((s_{l},r_{l})\), which encodes the low-level state \(s_{l}\) and the low-level reward \(r_{l}\). To achieve the sub-goal \(g_{sub}\), the low-level policy \(^{l}\) interacts with the environment through primitive actions \(a\). The right diagram illustrates the decomposition of the original MDP \(_{O}\) into the low-level MDP \(_{l}\) and the high-level MDP \(_{h}\). A detailed description of this decomposition is given in Sec. 2.2

framework presented in Fig. 1 is often preferred. Unlike the options framework, the goal-conditioned HRL framework requires no prior knowledge about the task , and its ability to generalize over the goal space when function approximation is used leads to significant performance gains in benchmark tasks [29; 20; 13]. Existing theoretical work [30; 12; 11] on the options framework does not consider the case where all the hierarchy levels are jointly learned. The regret analysis proposed by  focuses on the benefit of leveraging repeating sub-structures in hierarchical MDPs. Other regret analyses [12; 11] highlight the efficiency gain of learning with temporally extended actions (such as options). However, they always assume the set of options is known, and the intra-option policies are not learned. Options are composed of an intra-option policy, which governs the agent's behaviour while the corresponding option is executed, making intra-option policies very similar to the low-level policy we consider. However, the goal-conditioned HRL setting considered in this article quantifies the benefits of state abstraction, action abstraction, and time abstraction while jointly learning all levels of the hierarchy (low-level and high-level policy) through interaction with the environment. A detailed description of the option framework and its connection to HRL is available in Appendix B.

For the remainder of this section, we define episodic finite-horizon MDPs and the hierarchical decomposition we consider.

### Episodic Finite-Horizon Markov Decision Process

An episodic finite-horizon MDP is defined by the following tuple: \(=,,r,P,p_{0},H\). Where \(\) is a finite state space of size \(||\) and \(\) is a finite action space of size \(||\). The goal of the task is encoded in a terminal state \(g\). We assume the reward function \(r(s,g)[-a,b]\) (for \(a,b 0\)) is known \( s,g\), the reward function penalises each step with a negative reward of at most \(-a\) and reward the completion of the task with a positive reward of \(b\). The initial state distribution \(p_{0}\) is a distribution over states that is used to determine in which state an episode starts. The learner interacts with the MDP in episodes of at most \(H\) time steps. The episode's starting state \(s_{0} p_{0}\) is drawn from the initial state distribution. In each time step \(t=0,,H-1\), the learner observes a state \(s_{t}\) and chooses an action \(a_{t}\). Given a state action pair \((s_{t},a_{t})\) the next state \(s_{t+1} P(|s_{t},a_{t})\) is drawn from the transition kernel. Eventually, the episode ends because the agent reaches the terminal state or has interacted with the environment for H time-steps.

The agent's objective is to select actions that maximize the expected return throughout an episode. We typically assume actions are chosen according to a policy, \(a_{t}(s_{t})\), where \(\) is a function that maps each state and time step pair to a distribution over actions \(:[H-1]_{}\), and \(_{}\) is the set of all probability distributions over \(\) and \([H]\) is the set of natural numbers up to \(H\). The agent aims to select a policy \(\) to maximize the sum of expected rewards, \([_{t=1}^{H}r_{t}|a_{t}(s_{t})]\), where the expectation is over the initial state distribution, the policy and the stochastic transitions. Note that it is usually the case for finite-horizon MDPs that the policy also depends on the current time step. However, to simplify notation, we do not make this relation explicit.

For a given policy \(\), we define the value function, \(V_{}^{}(s)\), and the Q-function, \(Q_{}^{}(s,a)\), at time step \([H-1]\) as follows:

\[V_{}^{}(s)=_{t=}^{H-1}r_{t}|s_{}=s,a_{ :H-1},\ \ \ Q_{}^{}(s,a)=_{t=}^{H-1}r_{t}|s_{}=s,a_{ }=a,a_{+1:H-1}\]

where \(s\) denotes the state, \(a\) is the action and the notation \(a_{:H-1}\) is used to specify that actions between time step \(\) and time step \(H-1\) were selected using \(\). The optimal policy \(^{*}\) is the policy with the highest value function for every time step and every state, \(V_{}^{^{*}}(s)=V_{}^{*}(s)=_{}V_{}^{}(s)\, [H-1], s\). There is always a deterministic Markov policy that maximizes the total expected reward in a finite-horizon MDP .

In this article, we assess the quality of a policy by its expected value at the beginning of an episode. To lighten the notation, we define \(V^{}=_{s_{0} p_{0}}[V_{0}^{}(s)]\) to be the expected value from the beginning of an episode where the expectation is taken over initial states.

### Episodic Finite-Horizon Hierarchical MDP

For a given episodic finite-horizon MDP \(_{o}\), we assume it can be hierarchically decomposed into a pair of MDPs \((_{l},_{h})\) as illustrated on right diagram of Fig. 1. To avoid ambiguity, we use the following notation: the subscript \(o\) denotes the original MDP, while subscripts \(l\) and \(h\) denote low-level and high-level MDPs, respectively.

The low-level and high-level MDPs consist of the following tuples \(_{l}=_{l}_{h},,r_{l}, P_{l},p_{0,l},H_{l}\) and \(_{h}=_{h},_{h},r_{h},P_{h},p_{0,h},H_{h}\), respectively. To be a valid hierarchical decomposition, we require that these MDPs satisfy the following set of conditions:

**Action space:** The low-level action space consists of the set of primitive actions that the agent can use to interact with the environment. It is equivalent to the original MDP action space \(\). The high-level action space \(_{h}\) is the set of the sub-goals the high-level agent can instruct to the low-level agent. We assume that the set of sub-goals encoded in \(_{h}\) is sufficient to solve the task for any state. Note that the set of available actions \(_{h}\) depends on the current high-level state \(s_{h}\). To simplify our notation, we do not make this relationship explicit.

**State spaces:** The low-level state \(s_{l}\) and the high-level state \(s_{h}\) contain all necessary information to reconstruct the corresponding state, \(s\), in the original MDP. States \(s^{d}\) are usually described as multi-dimensional vectors, where each dimension encodes a specific characteristic. For example, a state description can be factored in a tuple \((s_{l},s_{h})_{l}_{h}\) with a part of the state description that belongs to the low-level MDP and another part to the high-level MDP. Hence, in this work, we consider that any state \(s_{o}\) can be represented by a tuple \((s_{l},s_{h})_{l}_{h}\). Additionally, since the low-level policy is goal-conditioned, its state space also contains the goal description leading to the following state space for the low-level MDP: \(_{l}_{h}\), a complete low-level state consists of the concatenation of the low-level state description \(s_{l}\) and the sub-goal description \(a_{h}\).

**Initial state distribution:** The high-level initial state distribution \(p_{0,h}\) is a restriction of the original state distribution \(p_{0}\) on \(_{h}\). The low-level initial state distribution \(p_{0,l}(|s_{h,0})\) is conditioned on the initial high-level state \(s_{h,0}\) and spans the low-level space, ensuring that \(p_{0}(s)=p_{0,h}(s_{h})p_{0,l}(s_{l}|s_{h})\), where \(s_{l}\) and \(s_{h}\) are the decomposition of \(s\).

**Transition functions:** The low-level transition function \(P_{l}\) is the restriction of \(P\) on \(_{l}_{h}\). One challenge in HRL is that the high-level transition function, \(P_{h}\), depends on the low-level policy since the quality of the low-level policy influences the likelihood of reaching a sub-goal state. The high-level transition probability \(P_{h}(s^{}_{h}|s_{h},a_{h},_{l})\) is the probability that the agent transitions to \(s^{}_{h}\) given the current high-level state \(s_{h}\), the sub-goal \(a_{h}\) and low level policy \(_{l}\). Since \(P_{h}\) depends on the low-level policy, it is non-stationary, making the learning task more challenging.

**Reward functions:** Since the terminal states for the original MDP belong to \(\) and the sub-goals for the low-level MDP lie in \(_{l}\) the low-level reward function can be obtained from the original reward function, \(r_{l}(s_{l},g_{sub})=2r(s,g)\), where \(s\) and \(g\) are the reconstruction of the low-level state and the sub-goal in the original MDP, using the current high-level state. The high-level reward function is the sum of rewards obtained by the low level during the sub-episode, where the high-level action plays the role of a sub-goal: \(r_{h}(s_{h},a_{h})=_{t=1}^{H_{l}}r_{l}(s_{l,t},a_{h})\).

**Horizons:** The original MDP allows an episode to last at most \(H\) steps. Consequently, the horizons of the high-level, \(H_{h}\), and low-level, \(H_{l}\), MDPs must satisfy the following equality \(H=H_{h}H_{l}\).

Note that we can always find a decomposition that satisfies these assumptions; a naive way to decompose any MDP would be to consider a high-level agent whose only action encodes the end goal of the task and a low-level with complete state information (i.e. it does not use state abstraction). While this decomposition is valid, it is not necessarily beneficial. Here, our goal is to identify when a given decomposition is useful, specifically in terms of improvements in the sample efficiency.

We denote by \(_{l}\) a policy interacting with the low-level MDP \(_{l}\), and \(_{h}\) a policy interacting with the high-level MDP \(_{h}\). In goal-conditioned HRL, the low-level policy maps a low-level state and sub-goal pair to an action: \(_{l}:_{l}_{h}_{l}\) and the high-level policy maps a high-level state to a high-level action: \(_{h}:_{h}_{h}\). Each policy can be evaluated using the corresponding high and low-level value functions \(V_{l}^{_{l}}\) and \(V_{h}^{_{h}}\). Similar to the non-hierarchical case, we can define optimal high-level and low-level policies as \(_{l}^{*}=*{argmax}_{_{l}}V_{l}^{_{l}}\) for the low-level policy and \(_{l}^{*}=*{argmax}_{_{h}}V_{h}^{_{h}}\) for the high-level policy. Moreover, as shown below, every pair of policies \((_{l},_{h})\) can be combined to produce a policy \(\) that interacts with the original MDP \(_{o}\).

**Definition 2.1**.: A hierarchical policy consists of a pair \((_{l},_{h})\) that can be mapped to a policy \(\) in the original MDP \(_{o}\) as follows:

\[(a|s)=(a|s_{l},s_{h})=_{a_{h}_{h}}_{h}(a_{h}|s_{h}) _{l}(a|a_{h},s_{l}).\] (1)The optimal hierarchical policy is obtained when merging \((_{l}^{},_{h}^{})\). It is important to note that not all policies \(\) in the original MDP have a corresponding decomposition \((_{l},_{h})\), and in particular, there is no guarantee that the optimal policy in the original MDP can be decomposed.

We aim to understand when a hierarchical decomposition of the MDP allows us to learn a near-optimal policy faster. Therefore, we are interested in evaluating the performance of the combination of \(_{l}\) and \(_{h}\) while they interact with the original MDP \(_{o}\). To convey the fact that we are evaluating a hierarchical policy in the original MDP, we use the following notation: given a pair of policies \((_{l},_{h})\) and their associated policy in the original MDP, \(\), the value function of the hierarchical policy is denoted by \(V_{o}^{_{l},_{h}}=_{s_{0} p_{0}}[V_{o,0}^{}(s_{0})]\), where the subscript \(o\) is a reminder that we are evaluating a policy on the original MDP \(_{o}\).

When learning in a decomposed MDP, the learner has to learn two policies, the high-level policy, \(_{h}\), and the low-level policy, \(_{l}\). This is done in an episodic setting where an episode unfolds as follows. Firstly, the learner observes the initial state and uses the high-level policy to find the most appropriate sub-goal. For the next \(H_{l}\) time steps, the low-level policy attempts to solve the sub-goal. The low-level agent updates its policy at the end of each low-level step. Once the \(H_{l}\) time steps are over or if the sub-goal has been reached, the high-level agent observes a new high-level state and can finally perform an update to its policy. The high-level agent instructs a new sub-goal if the overall task is not completed. These interactions are repeated until the task is completed or the horizon \(H\) is reached. We can now think of HRL as two agents interacting with the environment. Often, each agent will try to find the policy that maximizes their value function, \(_{_{l}}V_{l}^{_{l}}\) and \(_{_{h}}V_{h}^{_{h}}\).

### Probably-Approximately Correct RL

We aim to find, in as few episodes as possible, a pair of policies \((_{l},_{h})\) with a near-optimal value. To formalize this, we introduce the Probably-Approximately Correct (PAC) RL notion. We denote by \(_{k}\) the sub-optimality gap, that is the difference between the optimal (non-hierarchical) policy \(^{*}\) and the current hierarchical policy \((_{l}^{k},_{h}^{k})\): \(_{k} V_{o}^{*}-V_{o}^{_{l}^{k},_{h}^{k}}\). Note that both policies are evaluated on the original MDP \(_{o}\). The PAC guarantee in this paper follows the definition in .

**Definition 2.2**.: An algorithm satisfies a PAC bound \(N\) if, for a given input \(,>0\), it satisfies the following condition for any episodic fixed-horizon MDP: with probability at least \(1-\), the algorithm plays policies that are at least \(\)-optimal after at most \(N\) episodes. That is, with probability at least \(1-\), \(\{k:_{k}>\} N,\) where \(N\) is a polynomial that can depend on the properties of the problem instance.

In Section 3, we will bound the sample complexity of HRL algorithms. In this context, the sample complexity refers to the number of episodes, \(N\), in the original MDP, during which the algorithm may not follow a policy that is at least \(\)-optimal with probability at least \(1-\).

### Running Example

We consider the following companion example. The original MDP describes the task of solving a maze in a grid-world environment. The state consists of a tuple \((R,C)\) that indicates in which room, \(R\), and which cell within that room, \(C\), the agent is currently in. The reward function incurs a small cost, \(-a\), at each time step unless the agent reaches the absorbing goal state. Once the goal state is reached, the agent stops receiving penalties and receives a reward of 0 for all the remaining time steps. Mathematically, \(r(s)=-a1\{s g\}\) where \(g\) is the goal state, and \(1\) is the indicator function.

We can decompose this MDP as follows. The high-level MDP describes a similar maze, but instead of moving from cell to cell, the agent moves from room to room, so the state is just the current room. The high-level agent aims to find the room sequence that leads to the goal. Hence, at each (high-level) time step, it indicates the most valuable exit the low-level agent should take from the room. As specified in Section 2.2, the high-level reward for a sub-goal is the sum of the rewards accumulated by the low-level agent during that sub-episode. The low-level agent is myopic to other rooms - it only sees the current room and the exit it has to reach, and it receives a penalty of \(-2a\) for each action it takes unless it reaches the sub-goal, in which case it does not receive any penalty. Hence, if \(g_{sub}\) is the sub-goal, it receives reward \(r(s)=-2a1\{s g_{sub}\}\).

We will return to this example throughout the paper, but it should be noted that the framework we consider is general enough to be applied to a wide range of tasks. One such example is robotics, where the low-level agent would be tasked with controlling the joints of the robot to produce movements selected by the high-level policy, whose goal is to perform tasks that require a sequence of distinct movements (i.e. navigational tasks, manipulation tasks or a combination of both).

## 3 Lower Bound on the Sample Complexity of HRL

It has been proven in  that, for any RL algorithm, the number of sample episodes necessary to obtain an \((,)\)-accurate policy (in the original MDP) is lower bounded by:

\[[N]=|||H^{2}}{^{ 2}},\] (2)

where \(c\) is a positive constant.

We now extend this result to hierarchical MDPs. Before doing so, it is essential to notice that even the best hierarchical policy (as constructed in Eq. (1)) might be sub-optimal. This is a direct consequence of the goal-conditioned architecture. If, while executing a sub-episode, it appears that another sub-goal becomes more valuable, the architecture proposed does not allow interruptions. The agent will first have to complete the current sub-episode before being able to adapt to the new circumstances. Let \(V_{o}^{_{l}^{*},_{h}^{*}}\) denote the value of the optimal hierarchical policy value function in the original MDP. Then, the sub-optimality gap is larger than the gap between the current policy pair and the optimal hierarchical policy \(_{k}=V_{o}^{*}-V_{o}^{_{l}^{*},_{h}^{*}} V_{o}^{_{l}^{*}, _{h}^{*}}-V_{o}^{_{l}^{*},_{h}^{*}}\). Therefore, if for some \(N\), \(V_{o}^{_{l}^{*},_{h}^{*}}-V_{o}^{_{l}^{*},_{h}^{*}}\) for at least \(N\) episodes, it must also be the case that \(_{k}\) for at least \(N\) episodes. Hence, \(N\) is a lower bound on the number of episodes where the algorithm must follow a sub-optimal policy.

In the following theorem, we lower bound the number of episodes required to learn a pair of policies \((_{l},_{h})\) which are \(\)-accurate with respect to the optimal hierarchical policy \((_{l}^{*},_{h}^{*})\). By the above argument, this will also be a lower bound on the number of episodes necessary to learn an \(\)-accurate policy with respect to the optimal policy \(^{*}\).

**Theorem 3.1**.: _There exist positive constants \(c_{l}\), \(c_{h}\) and \(_{0}\) such that for every \((0,_{0})\) and for every algorithm \(A\) that satisfies a PAC guarantee for \((,)\) and outputs a deterministic policy, there is a fixed horizon MDP such that \(A\) must interact for_

\[[N]=_{l}||_{ h}|||H_{l}^{2}}{^{2}}}, _{h}||_{h}|H_{h}^{2}}{^{2}} }\] (3)

_episodes, in the original MDP, until the policy is \((,)\)-accurate._

The complete proof is in Appendix A.1. In the following, we highlight the main steps.

**Sketch of the proof:** An \(\)-accurate pair of policies must satisfy the following inequality, \(|V_{o}^{_{l}^{*},_{h}^{*}}-V_{o}^{_{l},_{h}}|\). To find a lower bound on the number of episodes \(N\) before we obtain an \(\)-accurate pair of policies \((_{l},_{h})\) we used the following steps:

1. We decompose the objective using the triangle inequality, \(|V_{o}^{_{l}^{*},_{h}^{*}}-V_{o}^{_{l}^{*},_{h}}|+|V_{o}^{_{l}^ {*},_{h}}-V_{o}^{_{l},_{h}}|\).
2. We show that the number of samples required to guarantee \(|V_{o}^{_{l}^{*},_{h}^{*}}-V_{o}^{_{l}^{*},_{h}}|/2\) is bounded by \(_{h}||_{h}||H_{h}^{2}}{^{2} }}\)
3. We show that the number of samples required to guarantee \(|V_{o}^{_{l}^{*},_{h}}-V_{o}^{_{l},_{h}}|/2\) is bounded by \(_{l}||_{h}|||H_{l}^{2}}{ ^{2}}}\)

Combining these three steps gives us the result in Theorem 3.1; see A.1 for more details.

### Interpretation of the Sample Complexity Bound:

By comparing this lower bound1 to that in the original MDP, we can identify the problem characteristics that might lead to improved sample efficiency. In general, only one of the two MDP characteristics will dominate the overall sample complexity because of the _max_ operator in the bound in Eq. 3. To maintain the _max_ as small as possible, the complexity should be distributed between the low- and high-level MDP as evenly as possible. We discuss some of these key insights below:

**State abstraction:** Only one of the two-state space cardinalities will dominate the bound in Eq. 3. This suggests that an efficient decomposition tends to separate the original state space as evenly as possible between the two levels of the hierarchy. Another phenomenon at stake is the low-level re-usability. Due to the state abstraction, the low-level agent can re-use its learned policy in different states (i.e. different states \(s_{1},s_{2}\) whose low-level component \(s_{l}\) are the same). We rewrite the lower bound 3 in terms of the _re-usability index_\(=|}{|_{l}|}\).

\[[N]= _{h}|}{}||H_{l}^{2}}{^{2}} },_{h}||_{h}|H_{h}^{ 2}}{^{2}}}.\] (4)

Eq. 4 highlights that a large re-usability index improves the sample efficiency.

**Temporal abstraction:** Similarly, only one of the two-time horizons will dominate the bound, again suggesting a fair repartition of the load. The temporal abstraction (reducing \(H\) to \(H_{h}\) and \(H_{l}\)) simplifies the credit assignment problem for the high-level and the low-level policies by giving denser feedback. The low-level agent is rewarded for completing sub-tasks that are significantly shorter than the original task, and the high-level trajectory consists of significantly fewer (high-level) steps than a trajectory in the original MDP.

**High-level action space:** This is the only term that appears on both sides of the \((,)\) in Eq. 3. This highlights that both the high-level and the low-level benefit from a compact sub-goal representation.

It is interesting to note the contrast between the state space decomposition and the design of the high-level action space. To find efficient state decomposition, the amount of information available at each level must be distributed among each level of the hierarchy. In the case of the sub-goal space, it appears that both levels benefit from a compact representation.

The above discussion highlights properties of the hierarchical decomposition that could improve sample complexity. Note, however, that our bound also shows that a hierarchical decomposition does not always improve the sample efficiency. Indeed, there will be some settings where using a "bad" hierarchical decomposition does not improve the sample complexity. Our bound can, therefore, provide a sanity check to determine whether a hierarchical decomposition _could_ lead to an improved sample complexity. However, finding an algorithm that achieves this improved sample complexity can still be challenging. Nevertheless, the proposed Q-learning-based hierarchical algorithm empirically demonstrates the potential benefits of leveraging the considered decomposition. In Section 5, we consider several MDP decompositions and empirically validate that when our bound suggests the hierarchical decomposition is beneficial, our algorithm (see Sec. 4) leverages this to achieve lower sample complexity.

## 4 Stationary Hierarchical Q-Learning

Once we know that we are in an MDP where the hierarchical decomposition could lead to improved sample complexity, the next challenge is to design an algorithm to exploit this. This section proposes the _Stationary Hierarchical Q-learning_ algorithm (SHQL) for this purpose. One of the most challenging aspects of jointly learning a pair of policies is the non-stationarity of the high-level transition dynamics, \(P_{h}\). It was briefly mentioned (in Sec. 2.2) that the high-level transition function, \(P_{h}\), is non-stationary since it depends on the low-level policy, \(_{l}\) with the next high-level state depending on whether \(_{l}\) managed to reach the sub-goal. To address this issue, we leverage the fact that the algorithm knows what a successful sub-episode is, i.e. it knows if the low-level agent managed to

[MISSING_PAGE_FAIL:8]

learning the shortest sequence of rooms that lead the agent from the starting position (the top left room) to the goal room (the bottom right room). The low-level task is to learn how to navigate within each room and to reach the instructed hallway. To further modulate the task's difficulty (in addition to the maze size), we vary the room profiles used, as depicted in the rightmost plot of Fig. 4.

The set of MDPs generated by these environments are the following:

**The original MDP:** This is a standard grid-world MDP, where the state space indicates the cell where the agent is located, and the action space allows the agent to move one cell in any cardinal direction (North, South, East, West). To obtain stochastic environments, each action has a success probability of \(p_{success}=4/5\). In case of failure, the action will be chosen at random.

**The high-level MDP:** The high-level state space is restricted to the room where the agent is currently located, and the exact position of the agent within that room is abstracted away. The high-level actions instruct the low-level to reach one of the available hallways. Note that not all rooms have access to the four hallways.

**The low-level MDP:** The low-level agent only observes the agent's current location within a room and the goal instructed by the high-level agent (one of the reachable hallways). It then uses the primitive action space (the four cardinal directions) to reach the desired hallway. All required code to reproduce the experiments is made available online .

### Identical Rooms

We first introduce the experimental setting in its simplest form. The environments considered in this subsection are mazes built by assembling identical rooms without obstacles (i.e. the top room profile in Fig. 4). Fig. 2 illustrates the empirical performance of our SHQL algorithm against Q-learning in the original MDP. As expected for simple mazes (e.g. with 4 or 16 rooms), the hierarchical decomposition does not provide much improvement. Still, as the problems grow more complex, the empirical evaluation suggests a significant improvement in sample efficiency. This is also confirmed by our bound (yellow curve on the rightmost plot of Fig. 4), which highlights that the efficiency gain of HRL is mostly achievable in complex MDPs (i.e. MDPs with large state and action spaces). It is essential to notice that in this experiment, the low-level decomposition remains constant for a given set of room profiles. This is why the benefit of HRL increases with the number of rooms (i.e. the high-level state space) until a plateau is reached. Once the bound is dominated by the high-level MDP, the unchanging complexity of the low-level MDP causes the ratio between the RL bound (Eq. 2) and the high-level part of the HRL bound (Eq. 3), \(|H}{|S_{h}||_{h}|H_{h}}\), to remain constant (even though number of rooms might still grow).

### Different \(n\)-rooms & Gymansium Taxi Task

To make the task more challenging, we next increase the number of room profiles used to construct the mazes. As depicted in the rightmost plot of Fig. 4 we considered four different room profiles, each one with a different obstacle in the room. The low-level agent must now learn to navigate multiple

Figure 3: Those plots are similar to the ones shown in Fig. 2, showing the performance obtained on mazes built from four different room layouts.

types of rooms to reach the sub-goal sent by the high-level agent. The performance of the algorithms with four different rooms is shown in Fig. 3. The introduction of different room profiles allows us to modulate the complexity of the low-level MDP, in contrast to varying the number of rooms, which only affects the complexity of the high-level MDP. This additional complexity results in a larger state space \(_{l}\) but may also result in a longer horizon \(H_{l}\) as the optimal trajectory might require more time to navigate around obstacles to reach the desired hallway successfully. It also becomes evident that, as the number of rooms increases, the hierarchy's benefits become more significant. Nevertheless, comparing Fig. 2 and Fig. 3 we can observe that the introduction of various room layouts has little effect on the Q-learning curve (in red). At the same time, it makes the task slightly more challenging for the HSQL learning curve (in blue), especially when the number of rooms is small since it suffers from the increased complexity of the low-level MDP. But, when the number of rooms is sufficiently large for the high-level MDP complexity to dominate the bound, the benefit of hierarchical decomposition becomes evident. The evolution of the bound ratio (HRL/RL) for the various MDPs considered is shown in the leftmost plot of Fig. 4. It shows that the low-level MDP dominates the bound when the maze consists of a small number of rooms. However, the curves clearly indicate that the expected sample efficiency improves as the high-level MDP becomes more complex (i.e., balancing the complexity between the two levels of the hierarchy). This result is also supported by empirical evidence as illustrated in Figs. 2, 3, 5, and 6. The Gymnasium Taxi environment  experiments presented in Appendix A.2.2 further validate our approach and conclusion on an entirely different task.

## 6 Conclusion

In this work, we analysed the sample complexity of goal-conditioned HRL. To the best of our knowledge, we provide the first result that analyses the decomposition induced by goal-conditioned HRL. In particular, our lower bound offers a valuable tool for practitioners that could help them decide whether they should consider a hierarchical decomposition for their problem. We also designed a novel algorithm that can leverage the hierarchy to improve its sample efficiency and implemented this on a set of hierarchical tasks. These experimental results further emphasizes the usefulness of the proposed bound since our theoretical findings support empirical efficiency gains.

Although this paper has taken a significant first step in bettering our understanding of the benefits of hierarchical decomposition, there is still scope for further work in this area. Three immediate open questions are: (i) whether our lower bound could be refined by explicitly accounting for the interactions between the low-level and the high-level agent, (ii) is it possible to design an algorithm that can theoretically match the proposed lower bound, (iii) the current results only consider the cardinality of the sub-goal space because we assumed that the sub-goal-space was given and that all sub-goals where solvable. Methods that design efficient sub-goal spaces remain largely unexplored and are a critical aspect of the design of HRL algorithms. Moreover, the insights we proposed are framed in a tabular setting and do not yet extend to a continuous setting where function approximation could be leveraged to allow the low-level agent to generalise over sub-goals and to consider setting beyond the tabular case described in this article. Overcoming those limitations is an interesting direction for future work.

Figure 4: The left-hand plot shows the evolution of the ratio between the RL bound Eq. (2) and the HRL bound Eq. (3) for various mazes and different room profiles. The plateau is obtained when the high-level MDP dominates the bound, leading to the following ratio: \(|||H}{|_{h}||_{h}|H_{h}}\). The curves are colour-coded such that a darker curve indicates more room profiles were considered. The right-hand side of the plot shows the different room profiles available to build the mazes.