# CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based Multi-Entity Action Recognition

Yuhang Wen

Sun Yat-sen University

wenyh29@mail2.sysu.edu.cn &Mengyuan Liu

State Key Laboratory of General Artificial Intelligence

Peking University, Shenzhen Graduate School

nkliuyifang@gmail.com &Songtao Wu

Sony R&D Center China

Songtao.Wu@sony.com &Beichen Ding

Sun Yat-sen University

dingbch@mail.sysu.edu.cn

Corresponding Authors.

###### Abstract

Skeleton-based multi-entity action recognition is a challenging task aiming to identify interactive actions or group activities involving multiple diverse entities. Existing models for individuals often fall short in this task due to the inherent distribution discrepancies among entity skeletons, leading to suboptimal backbone optimization. To this end, we introduce a Convex Hull Adaptive Shift based multi-Entity action recognition method (CHASE), which mitigates inter-entity distribution gaps and unbiases subsequent backbones. Specifically, CHASE comprises a learnable parameterized network and an auxiliary objective. The parameterized network achieves plausible, sample-adaptive repositioning of skeleton sequences through two key components. First, the Implicit Convex Hull Constrained Adaptive Shift ensures that the new origin of the coordinate system is within the skeleton convex hull. Second, the Coefficient Learning Block provides a lightweight parameterization of the mapping from skeleton sequences to their specific coefficients in convex combinations. Moreover, to guide the optimization of this network for discrepancy minimization, we propose the Mini-batch Pair-wise Maximum Mean Discrepancy as the additional objective. CHASE operates as a sample-adaptive normalization method to mitigate inter-entity distribution discrepancies, thereby reducing data bias and improving the subsequent classifier's multi-entity action recognition performance. Extensive experiments on six datasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity and Volleyball, consistently verify our approach by seamlessly adapting to single-entity backbones and boosting their performance in multi-entity scenarios. Our code is publicly available at https://github.com/Necolizer/CHASE.

## 1 Introduction

Multi-entity action recognition, a challenging task derived from action recognition , aims to find the optimal estimator of the mapping from multi-entity motions to semantic labels, where entities involved can range from human bodies , hands  to various objects . Recent approaches predominantly rely on skeletal data for addressing this challenge , given that skeletons serve as a concise representation of spatiotemporal features . This task has broad applications in human-robot interaction , scene understanding , human motion analysis , etc.

Experiments have revealed that network architectures tailored for single-entity actions get unsatisfactory performance when confronted with multi-entity actions [10; 35]. This inadequacy can be attributed to a common practice [36; 37; 38; 39; 40] observed in treating interactions: each entity is encoded independently using the same single-entity backbone, and their features are averaged for recognition. This practice is based on an empirical assumption that each entity is independent and identically distributed (i.i.d.). But we demonstrate that different entities depicted by skeletons exhibit evident non-i.i.d. characteristics. Fig. 1 (a) Row 1 reveals significant inter-entity distribution discrepancies using estimated distributions of joints from distinct entities. Such discrepancies can introduce bias into the backbone models, leading to suboptimal optimization and performance. It explains why multi-entity action modeling usually diverges from the single-entity one.

Using local coordinates for each entity holds promise in rendering them i.i.d., achieved by shifting individual origins to the per-entity spatiotemporal centers of mass (S2CoM), as depicted in Fig. 1 (a) Row 2. S2CoM is a straightforward and intuitive baseline to address this problem. However, this approach exists a significant toll as it entails a complete loss of inter-entity information. Experimental results corroborate this notion, as illustrated in Fig. 1 (b), showcasing the detrimental impact of lacking inter-entity measurements on recognition performance. Nonetheless, this endeavor sparks an insightful realization: the potential for narrowing distribution gaps through origin shifts, thereby improving the performance of single-entity backbones in multi-entity scenarios. A natural question arises: Can we reduce the bias by finding the optimal sample-adaptive shift in \(^{3}\) that minimizes the distribution discrepancies among entities?

To address the inter-entity distribution discrepancy problem, we propose a Convex Hull Adaptive Shift based multi-Entity action recognition method (CHASE). Serving as an additional normalization step, CHASE aims to accompany other single-entity backbones for enhanced multi-entity action recognition. Our main insight lies in the adaptive repositioning of skeleton sequences to mitigate inter-entity distribution gaps, thereby unbiasing the subsequent backbone and boosting its performance. Specifically, CHASE consists of a learnable parameterized network and an auxiliary objective. The parameterized network can achieve plausible and sample-adaptive repositioning of skeleton sequences through two crucial components. First, the Implicit Convex Hull Constrained Adaptive Shift (ICHAS) ensures that the new origin of the coordinate system is within the skeleton convex hull. Second, the Coefficient Learning Block (CLB) provides a lightweight parameterization of the mapping from skeleton sequences to their specific coefficients in ICHAS. Moreover, to guide the optimization of this network for discrepancy minimization, we propose the Mini-batch

Figure 1: **Inter-entity distribution discrepancies in multi-entity action recognition task.** (a) We delineate three distinct settings: _Vanilla_ (a common practice), _S2CoM_ (an intuitive baseline approach), and _CHASE_ (our proposed method). Column 2 illustrates spatiotemporal point clouds defined by the skeletons over \(10^{4}\) sequences. Column 3-5 depict the projections of estimated distributions of these point clouds onto the x-y, z-x, and y-z planes. These projections reveal significant inter-entity distribution discrepancies when using _Vanilla_. (b) The discrepancies observed in _Vanilla_ introduce bias into backbone models, leading to unsatisfactory performance. Although _S2CoM_ can reduce these discrepancies, it makes the classifiers produce wrong predictions due to a complete loss of inter-entity information. With the lowest inter-entity discrepancy, our method unbiases the subsequent backbone to get the highest accuracy, underscoring its efficacy.

Pair-wise Maximum Mean Discrepancy (MPMMD) as the additional objective. This loss function quantifies pair-wise entity discrepancies using maximum mean discrepancy and integrates mini-batch sampling strategies to estimate the expectation. In conclusion, CHASE works as a sample-adaptive normalization method to mitigate inter-entity distribution discrepancies, which can reduce bias in the subsequent classifier and enhance its multi-entity action recognition performance.

The contributions of this paper are three-fold:

1. To the best of our knowledge, we are the first to investigate the issue of inter-entity distribution discrepancies in multi-entity action recognition. Our proposed method, Convex Hull Adaptive Shift for Multi-Entity Actions, effectively addresses this challenge. Our main idea is adaptively repositioning skeleton sequences to mitigate inter-entity distribution gaps, thereby unbiasing the subsequent backbones and boosting their performance.
2. Serving as an additional normalization step for backbone models, CHASE consists of a learnable network and an auxiliary objective. Specifically, this network is formulated by the Implicit Convex Hull Constrained Adaptive Shift, together with the parameterization of a lightweight Coefficient Learning Block, which learns sample-adaptive origin shifts within skeleton convex hull. Additionally, the Mini-batch Pair-wise Maximum Mean Discrepancy objective is proposed to guide the discrepancy minimization.
3. Experiments on NTU Mutual 11, NTU Mutual 26, H2O, Assembly101, Collective Activity Dataset and Volleyball Dataset consistently verify our proposed method by improving performance of single-entity backbones in multi-entity action recognition task.

## 2 Related Work

### Skeleton-based Action Recognition

**Datasets & Models**. Datasets [41; 42; 43; 44] differing annotated or estimated skeleton sequences support the development of skeleton-based action recognition. Based on these benchmarks, a significant body of works focus on the design of artificial neural network architecture for more effective skeleton-based action recognition. Early models rely on the basic architecture of Recurrent Neural Network to capture temporal motions [45; 46; 47; 48; 49; 50]. Graph Convolution Network (GCN) shows predominated popularity as various graph convolution operators being proposed [36; 37; 38; 51; 52; 53; 54; 55; 56; 57; 58]. Recent progress of the model design is largely driven by adopting self-attention mechanism and transformer architecture [39; 40; 59; 60; 61; 62; 63; 64].

**Optimization Objectives**. Several works have explored additional optimization objectives beyond the commonly used cross-entropy (CE) loss to ensure robust recognition [37; 65], address challenging open-set problems , or integrate supplementary natural language descriptions [66; 67].

However, existing methods are usually developed under the empirical assumption that entities are i.i.d. allowing the backbones to learn representations of actions concerning only one entity [36; 37; 38; 39; 40; 53]. However, when confronted with multi-entity interactions, their common practice of feeding the backbone separately often proves inadequate. Our proposed approach can seamlessly adapt to these existing methods, boosting their performance by minimizing the distribution discrepancies.

### Skeleton-based Multi-Entity Action Recognition

**Interactive Actions**. Addressing datasets featuring two-person actions [41; 42; 68; 69; 70; 71; 72] or egocentric hand-object interactions [12; 13; 73; 74] necessitates effective interaction modeling. This spurs the development of various interaction recognition models leveraging human body and hand graph priors [10; 75]. Notably, the introduction of the general interactive action recognition task  unifies diverse interactions across various entity types, including person-to-person [10; 11; 14; 35; 76; 77], hand-to-hand [12; 35; 78; 79] and hand-to-object [13; 35; 75; 79; 80; 81] interactions.

**Group Activities**. Another interesting area of study is group activities [82; 83; 84], which involve more entities and may include irrelevant individual motions [85; 86]. To this end, recent works usually leverage compositional reasoning from group skeletons, either alone or in combination with additional modalities, to achieve promising results [87; 88; 89; 90; 91; 92; 93; 94; 95; 96].

While these works demonstrate satisfactory performance through interaction modelling, some may encounter model scalability issues when confronted with the factorial growth of inter-entity interactions [10; 13; 35; 75]. Moreover, they usually lack sufficient justification for why multi-entity action modeling significantly diverges from the single-entity one [10; 35; 79; 81]. In this paper, we delve into the inter-entity distribution discrepancy problem and introduce CHASE as a solution to minimize discrepancies. Through our proposed method, we aim to demonstrate that single-entity backbones can work well in multi-entity settings.

## 3 Chase

Fig. 2 presents the framework of our proposed CHASE for skeleton-based multi-entity action recognition. We begin by presenting the formulation of the implicit convex hull constrained adaptive shift in Section 3.1, followed by the design of a lightweight Coefficient Learning Block in Section 3.2. In Section 3.3, we subsequently introduce an additional objective termed Mini-batch Pair-wise Maximum Mean Discrepancy to further mitigate inter-entity distribution discrepancies.

### Implicit Convex Hull Constrained Adaptive Shift

The observed inter-entity distribution discrepancy in multi-entity skeleton sequences stems from the initial configuration of the world coordinate system. To mitigate this discrepancy, we propose an adaptive shift mechanism for each multi-entity skeleton sequence. It guides the origin to a sample-adaptive location, aiming to render each entity approximately i.i.d.. Moreover, based on the empirical assumption that the origin should not be far away from the skeletons, we implicitly constrain the new origin to remain within the skeleton convex hull by proving a simple but crucial proposition.

Consider a scenario where \(E\) interactive entities (e.g. persons) engage in purposeful activities over a duration of \(T\), and the pose of each entity is indicated by \(J\) joints with \(C\) Cartesian coordinates. The skeleton sequence of a multi-entity action is defined as \(X^{C T J E}\). For clarity we denote \(U=T J E\). Given points \(}^{C 1}\) in \(X^{C U}\), the subtraction \(_{i}}=}-}(1 i U)\) defines a shift of origin for them, where \(_{i}},}^{C 1}\). This can be expressed in matrix form as:

\[=X-}J_{1,U},\] (1)

Figure 2: **The overall framework of the proposed CHASE for multi-entity action recognition.** Given a skeleton sequence of multi-entity action as input, CHASE executes an implicit convex hull constrained adaptive shift with the Coefficient Learning Block, implemented as a lightweight backbone wrapper. CHASE also collects pair-wise shifted skeletons within mini-batches, effectively alleviating inter-entity distribution discrepancies by introducing an additional objective.

where \(J_{1,U}^{1 U}\) is a matrix of ones, and \(\) is the shifted skeleton sequence. Now the problem is to make the shift vector \(}\) adaptive to \(X\). A naive implementation is the linear combination:

\[=X-}J_{1,U}=X(I-WJ_{1,U}),\] (2)

where \(I^{U U}\) and the weight matrix \(W^{U 1}\).

However, optimizing \(W\) can be challenging without constraints, as \(}\) could potentially be any point in \(^{3}\). It is therefore reasonable to constrain \(}\) by incorporating the definition of the **Convex Hull**.

**Definition 1** (Convex Hull ).: The convex hull \(S\) of a given set \(X\) can be defined as: 1) The (unique) minimal convex set containing \(X\). 2) The set of all convex combinations of points in \(X\). These definitions are equivalent.

We jump to the formulation of the implicit skeleton convex hull constrained adaptive shift vector by proving the following proposition:

**Proposition 1**.: _The implicit skeleton convex hull constrained adaptive shift vector is formulated as_

\[}=X{ softmax}(W),\] (3)

_where \(X^{C U}\), \(W^{U 1}\), and \(}^{C 1}\). \(}\) in Eq. 3 is an element in the set of all convex combinations of points in \(X\). It is also a point that lies in the minimal convex set containing \(X\)._

Proof.: The first half of this proposition is equivalent to show that the matrix product of \(X\) and \({ softmax}(W)\) is a convex combination of \(X\). \(X\) is a set of points \(_{1},,_{U}\) with \(C\) Cartesian coordinates. We denote \({ softmax}(W)\) as \(\) with component \(_{i}\), which is formulated as

\[_{i}=}}{_{j=1}^{U}e^{_{j}}}( 1 i U),\] (4)

where \(_{i}\) is a component of \(W\). By applying function \({ softmax}:^{U}(0,1)^{U}\), each component \(_{i}\) of \(\) will be in the interval \((0,1)\), and the components will add up to 1. Thus we have \(}=_{i=1}^{U}_{i}}\), where all \(_{i}\) satisfy \(_{i}>0\) and \(_{i=1}^{U}_{i}=1\). This is sufficient for the definition of a convex combination, which only requires \(_{i} 0\). Then the second half of this proposition is evident with the equivalence of definitions in Def. 1. 

Proposition 1 also implies that all possible \(}\) constitute a subset \(\) of the convex hull \(S\) defined by the skeleton joints for all entities during the action period:

\[=\{._{i=1}^{U}_{i}_{i}| _{i} X,_{i=1}^{U}_{i}=1,_{i}(0,1 )\} S,\] (5)

which specifically is the interior of \(S\) (i.e., the open convex hull of \(X\)). We provide an example of the feasible \(}\) in the interior of \(S\), marked by a green circle in Fig. 2. The center of mass (CoM) \(}\) is also in the set \(\), proven by simply taking all \(_{i}=1/U(1 i U)\).

With Eq. 1 and Eq. 3, we introduce Implicit Convex Hull Constrained Adaptive Shift as:

\[=X(I-{ softmax}(W)J_{1,U}),\] (6)

where \(W\) is coefficients needed to be optimized. In Eq. 2, the search space for \(}\) encompasses the entire \(R^{3}\). However, in Eq. 6, it's restricted to the open convex hull \(\). We optimize the weights for each point under the constraint of the skeleton convex hull, subsequently deriving the adaptive shift vector for each sample. Applying a softmax function implicitly constrains \(}\) to remain within the convex hull \(S\), while preserving inter-entity measurements. Consequently, the subtraction between the point set and the shift vector repositions the origin to a specific point in the open convex hull.

### Parameterized Mapping for Coefficients

In this section, a lightweight Coefficient Learning Block is introduced to parameterize the mapping from the input skeleton sequence to the weight matrix. This parameterization allows CHASE to achieve sample-adaptive coefficients beyond sample-adaptive shifts formulated in Section 3.1.

In Eq. 6, we note that the first-order partial derivative of \(\) with respect to \(X\) is

\[}{ X}=I-J_{U,1}(W^{T}),\] (7)

whose result is constant. This implies that the same learnt weight matrix \(W\) is applied to all different \(X\)s when getting adaptive \(}\)s. To make the coefficients \(W\) dependent on the input \(X\), a mapping \(:^{C U}^{C 1}\) is expected to map \(X\) to \(W\).

As depicted in Fig. 2, we parameterize the nonlinear mapping \(\) as a sequence of learnable layers, termed the Coefficient Learning Block. This lightweight CLB can be formulated as follows:

\[W=(X)=W_{3}(W_{2}(W_{1}X+b)),\] (8)

where \(W_{1}^{C_{1} C},W_{2}^{C_{2} C_{1}},W_{3 }^{U C_{2}}\) are weight matrices, \(b\) is a bias matrix, \(:^{C_{1} U}^{C_{1} 1}\) is a squeeze operator  and \(\) is an activation function. Using a dimensionality-reduction layer and a dimensionality-increasing layer around the non-linearity is a common gating mechanism parameterization [98; 99]. Hence, we ensure \(U C_{1}>C_{2}\).

### Objective for Inter-entity Distribution Discrepancy Minimization

To facilitate CHASE optimization, we introduce an additional objective aimed at minimizing the inter-entity distribution discrepancies of the shifted skeleton sequences. This objective quantifies the pair-wise discrepancies and employs mini-batch sampling strategies to estimate the expectation.

Maximum mean discrepancy is a metric used to measure the distance between distributions, defined as the distance between their embeddings in the reproducing kernel Hilbert space (RKHS) \(\):

\[(P,Q)=_{\|f\|_{} 1}([f(x)]- [f(y)]),\] (9)

where \(()\) denotes the supremum. It is equivalent to finding the RKHS function \(f\) that maximizes the difference in expectations between the two probability distributions \(P(x)\) and \(Q(y)\).

Suppose each entity distribution is denoted as \(P^{i}(1 i E)\) for \(E\) entities, we measure the distance of all pair-wise distributions using the empirical mean

\[_{r(z)}[(z)]=_{i=1}^{E-1}_{j=i+1}^{E}(P^ {i},P^{j})/(E,2),\] (10)

where \(z=(P^{i},P^{j})(1 i,j E,i j)\) with the probability density \(r(z)\), and \(C(E,2)\) denotes a combination of \(E\) things taken 2 at a time without repetition. We adopt two approximations for computational efficiency. The first involves estimating \([f(x)]\) in Eq. 9 using a mini-batch of \(x\). The second approximation concerns the right-hand side of Eq. 10, which is impractical due to its complexity of \(O(n!)\) in terms of the entity count. Instead, it can be approximated by uniformly sampling a mini-batch of \(M\) entity pairs from all possible \((E,2)\) combinations \(z\):

\[_{r(z)}[(z)]_{m=1}^{M}(z_ {m}).\] (11)

We denote Eq. 11 with the above two approximations to be the Mini-batch Pair-wise Maximum Mean Discrepancy Loss \(_{mpmmd}\), thereby we have the total loss function for training:

\[=_{CLS}+_{mpmmd},\] (12)

where \(_{CLS}\) is the classification loss and \(\) is the trade-off weight factor for \(_{mpmmd}\).

## 4 Experiments

### Datasets & Settings

We conduct experiments on six multi-entity action recognition datasets. Fig. 3 presents skeletal samples in these datasets and their skeleton convex hulls, showcasing their difficulties.

**NTU Mutual 11** and **NTU Mutual 26**, respectively subsets of **NTU RGB+D** and **NTU RGB+D 120**, consist of a variety of inter-person mutual actions. NTU Mutual 11 adopts the widely-used X-Sub and X-View criteria, while NTU Mutual 26 follows the X-Sub and X-Set criteria.

**H2O** proffers 3D poses of human hands and bounding boxes of the manipulated objects, facilitating both hand-to-hand and hand-to-object interactions learning. We follow the training, validation, and test splits outlined in  in our experiments.

**Assembly101 (ASB101)** is a large and challenging 3D manual procedural activity dataset, with 1,380 categories of interactive actions. We follow the training, validation, and test splits described in  for evaluations. Fine-grained actions (verb & noun) are adopted as labels in experiments.

**Collective Activity Dataset (CAD)** captures people and their behaviors in public using street cameras, categorizing pedestrian collective activities into 4 groups. We adopt the same categories, individual labels, train-test split in . Only 2D joint coordinates are used in our experiments.

**Volleyball Dataset (VD)** consists of video clips from volleyball tournaments and includes 8 group activity classes based on volleyball terminology. We follow the Original split described in  for evaluation. Only estimated 2D joint coordinates are used as input features.

**Settings.** Experiments are conducted on the GeForce RTX 3070 GPUs with PyTorch. CTR-GCN , InfoGCN , STSA-Net  and HD-GCN  are chosen as our baseline models. To ensure fair comparisons, we adopt single intra-skeleton modality without multi-modality fusion following . For CTR-GCN in NTU Mutual 26, we adopt input shape \(X^{3 64 25 2}\), segment size \((1,1,1)\) and \(=0.1\) in CHASE. SGD optimizer is used with Nesterov momentum of 0.9, a initial learning rate of 0.1 and a decay rate 0.1 at the 80th and 100th epoch. Batch size is set to 64. More detailed configurations for each model are provided in the Appendix.

### Experimental Results

Table 1 shows the experimental results on different benchmarks, reporting the averaged top-1 accuracy and its standard deviation in runs with several seed initializations. We compare CHASE with vanilla counterparts (light red background) and the state-of-the-art multi-entity action recognition methods (light yellow background). By adopting our proposed CHASE, we can boost the vanilla counterparts'

Figure 3: **Visualizations of multi-entity action samples and their skeleton convex hulls.**performance by a noticeable margin in most settings. It yields varying degrees of accuracy improvement across different baseline models and benchmarks, owing to differences in model parameter count, training objective, data scale, etc. Compared to models with complicated interaction designs, CHASE can help single action backbones achieve the state-of-the-art performance in interaction recognition by outperforming ISTA-Net , AHNet-Large , etc. In group activities recognition task, which is more challenging for single-entity backbones, CHASE can help achieve competitive performance. Fig. 4 visualizes that CHASE can effectively alleviate the potential inter-entity distribution discrepancies across a range of data scales, thereby ensuring robust backbone optimization and inference. UMAP  visualization in Fig. 5 demonstrates our proposed CHASE differentiate similar multi-entity actions better by assisting backbones to learn more distinctive representations.

### Ablation Study

In this section, we conduct ablation studies on the widely-adopted benchmarks NTU Mutual 26 and NTU Mutual 11 with only joint modality.

**Comparison with other alternatives**. We compare our proposed CHASE with several alternatives as follows: 1) Vanilla: Use the raw world coordinates or pixel coordinates. 2) S2CoM: Shift

   &  &  &  \\   & & X-Sub & X-Set & X-Sub & X-View \\  GDCN  & TPAMI’23 & \(85.80\) & \(92.10\) & - & - \\ Skele TR  & ICCV’23 & \(87.80\) & \(88.30\) & \(94.80\) & \(97.70\) \\ ISTA-Net  & IROS’23 & \(90.56_{( 0.08)}\) & \(91.72_{( 0.30)}\) & - & - \\ AHNet-Large  & PR’24 & \(86.43\) & \(86.64\) & \(90.85\) & \(93.38\) \\ me-GCN  & arXiv’24 & \(90.00\) & \(90.00\) & \(95.50\) & \(98.20\) \\  CTR-GCN  & ICCV’21 & \(89.32_{( 0.06)}\) & \(90.19_{( 0.17)}\) & \(95.94_{( 0.36)}\) & \(98.32_{( 0.29)}\) \\ 
**+ CHASE (Ours)** & - & \(_{( 0.22)}^{1.98}\) & \(_{( 0.10)}^{2.15}\) & \(_{( 0.05)}^{0.51}\) & \(_{( 0.13)}^{0.51}\) \\  InfoGCN (k=1) & CVPR’22 & \(90.22_{( 0.13)}\) & \(91.13_{( 0.16)}\) & \(95.51_{( 0.10)}\) & \(97.76_{( 0.22)}\) \\
**+ CHASE (Ours)** & - & \(_{( 0.05)}\) & \(_{( 0.34)}^{1.28}\) & \(_{( 0.18)}^{0.84}\) & \(_{( 0.25)}^{0.49}\) \\  STSA-Net  & Neuro.’23 & \(88.41_{( 0.01)}\) & \(90.19_{( 0.11)}\) & \(95.96_{( 0.09)}\) & \(98.47_{( 0.09)}\) \\
**+ CHASE (Ours)** & - & \(_{( 0.18)}^{1.36}\) & \(_{( 0.12)}^{1.35}\) & \(_{( 0.10)}^{0.68}\) & \(_{( 0.08)}^{0.26}\) \\  HD-GCN (CoM=1) & ICCV’23 & \(88.25_{( 0.44)}\) & \(90.08_{( 0.12)}\) & \(95.58_{( 0.10)}\) & \(97.93_{( 0.07)}\) \\
**+ CHASE (Ours)** & - & \(_{( 0.13)}^{2.56}\) & \(_{( 0.21)}^{1.97}\) & \(_{( 0.05)}^{0.64}\) & \(_{( 0.07)}^{0.38}\) \\  Method & Venue & H2O(\%) & ASB101(\%) & CAD(\%) & VD(\%) \\  AT  & CVPR’20 & - & - & - & 92.30 \\ ISTA-Net  & IROS’23 & \(89.09_{( 1.21)}\) & \(28.01_{( 0.06)}\) & \(87.16_{( 2.55)}\) & \(91.40_{( 0.23)}\) \\ H2OTR  & CVPR’23 & \(90.90\) & - & - & - \\ EffHandEgoNet  & arXiv’24 & \(91.32\) & - & - & - \\ AHNet-Large  & PR’24 & - & - & 89.32 & 84.31 \\  CTR-GCN  & ICCV’21 & \(81.68_{( 0.85)}\) & \(27.83_{( 0.45)}\) & \(80.45_{( 2.29)}\) & \(92.66_{( 0.21)}\) \\
**+ CHASE (Ours)** & - & \(_{( 1.98)}^{9.37}\) & \(_{( 0.30)}^{0.21}\) & \(_{( 0.20)}^{19.16}\) & \(_{( 0.15)}^{0.24}\) \\  InfoGCN (k=1) & CVPR’22 & \(76.24_{( 1.93)}\) & \(27.18_{( 0.10)}\) & \(83.07_{( 0.46)}\) & \(91.77_{( 0.15)}\) \\
**+ CHASE (Ours)** & - & \(_{( 2.89)}^{7.23}\) & \(_{( 0.12)}^{0.18}\) & \(_{( 2.91)}^{11.1}\) & \(_{( 0.15)}^{0.23}\) \\  STSA-Net  & Neuro.’23 & \(92.29_{( 0.52)}\) & \(27.70_{( 0.19)}\) & \(80.20_{( 3.60)}\) & \(92.52_{( 0.52)}\) \\
**+ CHASE (Ours)** & - & \(_{( 1.36)}^{2.48}\) & \(_{( 0.13)}^{0.11}\) & \(_{( 2.46)}^{5.73}\) & \(_{( 0.26)}^{0.26}\) \\  HD-GCN (CoM=1) & ICCV’23 & \(72.73_{( 0.41)}\) & \(27.31_{( 0.36)}\) & \(76.93_{( 4.38)}\) & \(91.32_{( 0.02)}\) \\
**+ CHASE (Ours)** & - & \(_{( 1.03)}^{78.88}\) & \(_{( 0.24)}^{0.19}\) & \(_{( 1.61)}^{5.46}\) & \(_{( 0.07)}^{0.08}\) \\  

Table 1: Comparisons with Skeleton-based Methods on Multi-Entity Action Datasets

  Method & Acc (\%) & \(\) (\%) \\  Vanilla & \(89.32_{( 0.06)}\) & - \\  S2CoM & \(88.66_{( 0.26)}\) & \(-0.67\) \\ BatchNorm & \(89.06_{( 0.16)}\) & \(-0.27\) \\ ER  & \(89.34_{( 0.15)}\) & \(+0.02\) \\ Aug & \(89.72_{( 0.04)}\) & \(+0.40\) \\ S2CoM\({}^{}\)/STD &the individual origins to the spatiotemporal centers of mass for each entity. 3) BatchNorm: Apply an additional BatchNorm operation immediately when batches of samples are fed into the model. 4) ER (Entity Rearrangement ): A technique aims to eliminate the orderliness of entities for interaction modelling. 5) Aug: Apply an additional data augmentation by randomly shifting the skeleton sequences. 6) S2CoM\(\): Shift the origin to the spatiotemporal center of mass. 7) S2CoM\(\)/STD: Scale according to the channel-wise standard deviations after applying S2CoM\(\). Results in Table 2 indicate that CHASE can outperform these alternatives by bringing the largest accuracy improvement to the vanilla CTR-GCN.

**Analysis of inter-entity distribution discrepancies.** Table 3 presents metrics evaluating the inter-entity distribution discrepancies on test sets, including Averaged Kullback-Leibler Divergence (Avg KLD), Jensen-Shannon Divergence (JSD), Bhattacharyya Distance (BD), Hellinger Distance (HD) and MMD. We measure the pair-wise distributions of sampled data points from different entities in test sets of NTU Mutual 11 X-Sub (I), X-View (II) and NTU Mutual 26 X-Sub (III), X-Set (IV). Table 3 demonstrates that CHASE significantly minimizes discrepancies across all evaluation metrics, thereby benefiting backbone learning for each entity in multi-entity actions.

   &  &  &  &  &  \\  AS & CHC &  &  &  &  &  &  &  \\  ✓ & ✓ & ✓ & ✓ & 0.1 & \(_{( 0.22)}\) & - \\  ✓ & & ✓ & ✓ & 0.1 & \(22.65_{( 0.35)}\) & \(-68.65\) \\ ✓ & & ✓ & ✓ & 0.01 & \(86.99_{( 0.16)}\) & \(-4.32\) \\ ✓ & ✓ & & ✓ & 0.1 & \(91.20_{( 0.13)}\) & \(-0.10\) \\ ✓ & ✓ & & ✓ & 0.1 & \(22.75_{( 0.12)}\) & \(-68.56\) \\ ✓ & & & ✓ & 0.01 & \(23.51_{( 0.38)}\) & \(-67.79\) \\ ✓ & & ✓ & ✓ & 0.1 & \(20.42_{( 0.09)}\) & \(-70.88\) \\ ✓ & ✓ & ✓ & & 0.1 & \(91.17_{( 0.18)}\) & \(-0.13\) \\  & & & & 0.1 & \(89.50_{( 0.10)}\) & \(-1.81\) \\  

Table 4: Analysis of Key Components in CHASE

Figure 4: **Qualitative results of CHASE**. Different entity distributions are denoted by blue and orange. CHASE effectively mitigates inter-entity distribution discrepancies, demonstrating its clear effectiveness across a range of data scales, from small to large.

  Set & Method & Avg KLD \(\) & JSD \(\) & BD \(\) & HD \(\) & MMD \(\) \\   & Vanilla & \(1.07_{( 0.25)}\) & \(0.19_{( 0.04)}\) & \(0.25_{( 0.06)}\) & \(0.46_{( 0.06)}\) & \(0.94_{( 0.54)}\) \\  & **CHASE (Ours)** & \(_{( 0.09)}\) & \(_{( 0.02)}\) & \(_{( 0.02)}\) & \(_{( 0.03)}\) & \(_{( 0.02)}\) \\   & Vanilla & \(1.00_{( 0.23)}\) & \(0.18_{( 0.04)}\) & \(0.23_{( 0.05)}\) & \(0.45_{( 0.05)}\) & \(1.03_{( 0.60)}\) \\  & **CHASE (Ours)** & \(_{( 0.08)}\) & \(_{( 0.02)}\) & \(_{( 0.02)}\) & \(_{( 0.03)}\) & \(_{( 0.02)}\) \\   & Vanilla & \(0.72_{( 0.14)}\) & \(0.14_{( 0.02)}\) & \(0.17_{( 0.03)}\) & \(0.39_{( 0.04)}\) & \(1.25_{( 0.60)}\) \\  & **CHASE (Ours)** & \(_{( 0.08)}\) & \(_{( 0.02)}\) & \(_{( 0.02)}\) & \(_{( 0.03)}\) & \(_{( 0.04)}\) \\   & Vanilla & \(0.75_{( 0.14)}\) & \(0.14_{( 0.03)}\) & \(0.17_{( 0.03)}\) & \(0.40_{( 0.04)}\) & \(1.15_{( 0.56)}\) \\  & **CHASE (Ours)** & \(_{( 0.07)}\) & \(_{( 0.01)}\) & \(_{( 0.02)}\) & \(_{( 0.03)}\) & \(_{( 0.03)}\) \\  

Table 3: Analysis of Inter-entity Distribution Discrepancies

**Evaluations on Mixed Recognition of Single- & Multi-Entity Actions**. Table 5 shows a 0.41% improvement in X-Sub accuracy on the entire NTU RGB+D 120. This implies that although CHASE is proposed for multi-entity actions, it is also effective in mixed recognition settings.

**Analysis of Efficiency**. As illustrated in Table 6, the number of trainable parameters of CHASE in NTU Mutual 26 configurations is about 26.37 k, resulting in a mere 1%-2% parameter increase. For computational complexity, FLOPs of CHASE is approximately 2.50 M. These metrics demonstrate that CHASE is both efficient and lightweight.

## 5 Conclusion

This paper proposes the Convex Hull Adaptive Shift for Multi-Entity Action Recognition (CHASE) to address the inter-entity distribution discrepancies. To the best of our knowledge, we are the first to investigate this problem and leverage discrepancy minimization to unbias the classifiers. Our approach can seamlessly adapt to existing backbone architectures and demonstrate performance improvements across six multi-entity action recognition datasets.