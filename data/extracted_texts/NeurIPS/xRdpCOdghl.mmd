# Enhancing Semi-Supervised Learning via Representative and Diverse Sample Selection

Qian Shao1,3\({}^{*}\), Jiangrui Kang2\({}^{*}\), Qiyuan Chen1,3\({}^{*}\), Zepeng Li4, Hongxia Xu1,3,

Yiwen Cao2, Jiaquan Liang2\({}^{}\), and Jian Wu1\({}^{}\)

\({}^{1}\)College of Computer Science \(\&\) Technology and Liangzhu Laboratory, Zhejiang University

\({}^{2}\)BNU-HKBU United International College \({}^{3}\)WebDoctor Cloud

\({}^{4}\)The State Key Laboratory of Blockchain and Data Security, Zhejiang University

{qianshao, qiyuanchen, lizepeng, einstein, wujian2000}@zju.edu.cn

{kangjiangrui, yiwencao, jiajuanliang}@uic.edu.cn

These authors contributed equally to this work.Corresponding authors.

###### Abstract

Semi-Supervised Learning (SSL) has become a preferred paradigm in many deep learning tasks, which reduces the need for human labor. Previous studies primarily focus on effectively utilising the labelled and unlabeled data to improve performance. However, we observe that how to select samples for labelling also significantly impacts performance, particularly under extremely low-budget settings. The sample selection task in SSL has been under-explored for a long time. To fill in this gap, we propose a Representative and Diverse Sample Selection approach (RDSS). By adopting a modified Frank-Wolfe algorithm to minimise a novel criterion \(\)-Maximum Mean Discrepancy (\(\)-MMD), RDSS samples a representative and diverse subset for annotation from the unlabeled data. We demonstrate that minimizing \(\)-MMD enhances the generalization ability of low-budget learning. Experimental results show that RDSS consistently improves the performance of several popular SSL frameworks and outperforms the state-of-the-art sample selection approaches used in Active Learning (AL) and Semi-Supervised Active Learning (SSAL), even with constrained annotation budgets. Our code is available at RDSS.

## 1 Introduction

Semi-Supervised Learning (SSL) is a popular paradigm which reduces reliance on large amounts of labeled data in many deep learning tasks . Previous SSL research mainly focuses on effectively utilising labelled and unlabeled data. Specifically, labelled data directly supervise model learning, while unlabeled data help learn a desirable model that makes consistent and unambiguous predictions . Besides, we also find that how to select samples for annotation will greatly affect model performance, particularly under extremely low-budget settings (see Section 7.2).

The prevailing sample selection methods in SSL have many shortcomings. For example, random sampling may introduce imbalanced class distributions and inadequate coverage of the overall data distribution, resulting in poor performance. Stratified sampling randomly selects samples within each class, which is impractical in real-world scenarios where the label for each sample is unknown. Existing researchers also employ representativeness and diversity strategies to select appropriate samples for annotation. Representativeness  ensures that the selected subset distributes similarly with the entire dataset, and diversity  is designed to select informative samples by pushing awaythem in feature space. And focusing on only one aspect presents significant limitations (Figure 0(a) and 0(b)). To address these issues, Xie et al.  and Wang et al.  employ a combination of the two strategies for sample selection. These methods set a fixed ratio for representativeness and diversity, restricting the ultimate performance through our empirical evidence (see Section 7.4). Fundamentally, they lack a theoretical basis to substantiate their effectiveness.

We observe that Active Learning (AL) primarily focuses on selecting the right samples for annotation, and numerous studies transfer the sample selection methods of AL into SSL, giving rise to Semi-Supervised Active Learning (SSAL) . However, most of these approaches exhibit several limitations: (1) They require randomly selected samples to begin with, which extends a portion of the labelling budget, making it difficult to work effectively with a very limited budget (_e.g._, 1% or even lower) ; (2) They involve human annotators in iterative cycles of labelling and training, leading to substantial labelling overhead ; (3) They are coupled with the model training so that samples for annotation need to be re-selected every time a model is trained . In summary, selecting the appropriate samples for annotation is challenging in SSL.

To address these challenges, we propose a Representative and Diverse Sample Selection approach (RDSS) that requests annotations only once and operates independently of the downstream tasks. Specifically, inspired by the concept of Maximum Mean Discrepancy (MMD) , we design a novel criterion named \(\)-MMD. It aims to strike a balance between representativeness and diversity via a trade-off parameter \(\) (Figure 0(c)), for which we find an optimal interval adapt to different budgets. By using a modified Frank-Wolfe algorithm called Generalized Kernel Herding without Replacement (GKHR), we can get an efficient approximate solution to this minimization problem.

We prove that under certain Reproducing Kernel Hilbert Space (RKHS) assumptions, \(\)-MMD effectively bounds the difference between training with a constrained versus an unlimited labelling budget. This implies that our proposed method could significantly enhance the generalization ability of learning with limited labels. We also give a theoretical assessment of GKHR with some supplementary numerical experiments, showing that GKHR performs well in learning with limited labels.

Furthermore, we evaluate our proposed RDSS across several popular SSL frameworks on the datasets CIFAR-10/100 , SVHN , STL-10  and ImageNet . Extensive experiments show that RDSS outperforms other sample selection methods widely used in SSL, AL or SSAL, especially with a constrained annotation budget. Besides, ablation experimental results demonstrate that RDSS outperforms methods using a fixed ratio.

The main contributions of this article are as follows:

* We propose RDSS, which selects representative and diverse samples for annotation to enhance SSL by minimizing a novel criterion \(\)-MMD. Under low-budget settings, we develop a fast and efficient algorithm, GKHR, for optimization.

Figure 1: Visualization of selected samples from a dog dataset. The red and grey circles respectively symbolize the selected and unselected samples. **a)** The selected samples often contain an excessive number of highly similar instances, leading to redundancy; **b)** The selected samples contain too many edge points, unable to cover the entire dataset; **c)** The selected samples represent the entire dataset comprehensively and accurately.

* We prove that our method benefits the generalizability of the trained model under certain assumptions and rigorously establish an optimal interval for the trade-off parameter \(\) adapt to the different budgets.
* We compare RDSS with sample selection strategies widely used in SSL, AL or SSAL, the results of which demonstrate superior sample efficiency compared to these strategies. In addition, we conduct ablation experiments to verify our method's superiority over the fixed-ratio approach.

## 2 Related Work

Semi-Supervised LearningSemi-Supervised Learning (SSL) effectively utilizes sparse labeled data and abundant unlabeled data for model training. Consistency Regularization [34; 20; 45], Pseudo-Labeling [21; 56] and their hybrid strategies [40; 63; 35] are commonly used in SSL. Consistency Regularization ensures the model's output stays stable even when there's noise or small changes in the input, usually from the data augmentation . Pseudo-labelling integrates high-confidence data pseudo-labels directly into training, adhering to entropy minimization . Moreover, an integrative approach that combines the aforementioned strategies can also achieve substantial results [53; 59]. Even though these approaches have been proven effective, they usually assume that labelled samples are randomly selected from each class (_i.e._, stratified sampling), which is not practical in real-world scenarios where the label for each sample is unknown.

Active LearningActive learning (AL) aims to optimize the learning process by selecting the appropriate samples for labelling, reducing reliance on large labelled datasets. There are two different criteria for sample selection: uncertainty and representativeness. Uncertainty sampling selects samples about which the current model is most uncertain. Earlier studies utilized posterior probability [22; 49], entropy [18; 26], and classification margin  to estimate uncertainty. Recent research regards uncertainty as training loss [17; 60], influence on model performance [11; 24] or the prediction discrepancies between multiple classifiers . However, uncertainty sampling methods may exhibit performance disparities across different models, leading researchers to focus on representativeness sampling, which aims to align the distribution of selected subset with that of the entire dataset [36; 39; 27]. Most AL approaches are difficult to perform well under extremely low-label settings. This may be because they usually require randomly selected samples to begin with and involve human annotators in iterative cycles of labelling and training, leading to substantial labelling overhead.

Model-Free SubsamplingSubsampling is a statistical approach which selects a subset with size \(m\) as a surrogate for the full dataset with size \(n m\). While model-based subsampling methods depend heavily on the model assumptions [1; 61], improper choice of the model could lead to bad performance of estimation and prediction. In that case, model-free subsampling is preferred in data-driven modelling tasks, as it does not depend on the model assumptions. There are mainly two kinds of popular model-free subsampling methods. The one is induced by minimizing statistical discrepancies, which forces the distribution of subset to be similar to that of full data, in other words, selects representative subsamples, such as Wasserstein distance , energy distance , uniform design , maximum mean discrepancy  and generalized empirical \(F\)-discrepancy . The other tends to select a diverse subset containing as many informative samples as possible . The above-mentioned methodologies either exclusively focus on representativeness or diversity, which are difficult to effectively apply to SSL.

## 3 Problem Setup

Let \(\) be the unlabeled data space, \(\) be the label space, \(_{n}=\{_{i}\}_{i[n]}\) be the full unlabeled dataset containing pairwise different data, and \(_{m}=\{i_{1},i_{2},,i_{m}\}[n](m<n)\) be an index set contained in \([n]\), our goal is to find an index set \(^{*}_{m}=\{i^{*}_{1},i^{*}_{2},,i^{*}_{m}\}[n](m<n)\) such that the selected set of samples \(^{*}_{^{*}_{m}}=\{_{i^{*}_{1}},_{i^ {*}_{2}},,_{i^{*}_{m}}\}\) is the most informative. After that, we can get access to the true labels of selected samples and use the set of labelled data \(S=\{(_{i},y_{i})\}_{i^{*}_{m}}\) and the rest of the unlabeled data to train a deep learning model.

Following the methodology of previous works, we use representativeness and diversity as criteria for evaluating the informativeness of selected samples. Representativeness ensures the selected samples distribute similarly to the full unlabeled dataset. Diversity is proposed to prevent an excessive concentration of selected samples in high-density areas of the full unlabeled dataset. Furthermore, the cluster assumption in SSL suggests that the data tend to form discrete clusters, in which boundary points are likely to be located in the low-density area. Therefore, under this assumption, selected samples with diversity contain more boundary points than the non-diversified ones, which is desired in training classifiers.

As a result, our goal can be formulated by solving the following problem:

\[_{_{m}[n]}(_{_{m}}, _{n})+(_{_{m}},_{n}),\] (1)

where \((_{_{m}},_{n})\) and \((_{_{m}},_{n})\) quantify the representativeness and diversity of selected samples respectively and \(\) is a hyperparameter to balance the trade-off representativeness and diversity.

Besides, we propose another two fundamental settings which are beneficial to the implementation of the framework: **(1) Low-budget learning.** The budget for many of the real-world tasks which require sample selection procedures is relatively low compared to the size of unlabeled data. Therefore, we set \(m/n 0.2\) in default in the following context, including the analysis of the sampling algorithm and the experiments; **(2) Sampling without Replacement.** Compared with the setting of sampling with replacement, sampling without replacement offers several benefits which better match our tasks, including bias and variance reduction, precision increase and representativeness enhancement [25; 46].

## 4 Representative and Diversity Sample Selection

The Representative and Diverse Sample Selection (RDSS) framework consists of two steps: (1) _Quantification._ We quantify the representativeness and diversity of selected samples by a novel concept called \(\)-MMD (6), where \(\) is replaced by \(\) as the trade-off hyperparameter; (2) _Optimization._ We optimize \(\)-MMD by GKHR algorithm to obtain the optimally selected samples \(_{_{m}^{*}}\).

### Quantification of Diversity and Representativeness

In classical statistics and machine learning problems, the inner product of data points \(,\), defined by \(,\), is employed to as a similarity measure between \(,\). However, the application of linear functions can be very restrictive in real-world problems. In contrast, kernel methods use kernel functions \(k(,)\), including Gaussian kernels (RBF), Laplacian kernels and polynomial kernels, as non-linear similarity measures between \(,\), which are actually inner products of the projections of \(k(,)\) in some high-dimensional feature space .

Let \(k(,)\) be a kernel function on \(\), and we employ \(k(,)\) to measure the similarity between any two points and the average similarity, denoted by

\[S_{k}(_{_{m}})=}_{i_{m}} _{j_{m}}k(_{i},_{j}),\] (2)

to measure the similarity between the selected samples. Obviously, \(S(_{_{m}})\) can evaluate the diversity of \(_{_{m}}\) since larger similarity implies smaller diversity.

As a statistical discrepancy which measures the distance between distributions, the maximum mean discrepancy (MMD) is introduced here to quantify the representativeness of \(_{_{m}}\) to \(_{n}\). Proposed by Gretton et al. , MMD is formally defined below:

**Definition 4.1** (**Maximum Mean Discrepancy**).: Let \(P,Q\) be two Borel probability measures on \(\). Suppose \(f\) is sampled from the unit ball in a reproducing kernel Hilbert space (RKHS) \(\) associated with its reproducing kernel \(k(,)\), _i.e._, \(\|f\|_{} 1\), then the MMD between \(P\) and \(Q\) is defined by

\[_{k}^{2}(P,Q):=_{\|f\|_{} 1}( fdP- fdQ )^{2}=[k(X,X^{})+k(Y,Y^{} )-2k(X,Y)],\] (3)

where \(X,X^{} P\) and \(Y,Y^{} Q\) are independent copies.

We can next derive the empirical version for MMD that is able to measure the representativeness of \(_{_{m}}=\{_{i}\}_{i_{m}}\) relative to \(_{n}=\{_{i}\}_{i=1}^{n}\) by replacing \(P,Q\) with the empirical distribution constructed by \(_{_{m}},_{n}\) in (3):

\[_{k}^{2}(_{_{m}},_{n}):=}_{i=1}^{n}_{j=1}^{n}k(_{i},_{j})+ }_{i_{m}}_{j_{m}}k( _{i},_{j})-_{i=1}^{n}_{j _{m}}k(_{i},_{j}).\] (4)

**Optimization objective.** Set \((,)=-_{k}^{2}(,)\) and \(()=-S_{k}()\) in (1), where \(k\) is a proper kernel function, our optimization objective becomes

\[_{_{m}[n]}_{k}^{2}(_{_ {m}},_{n})+ S_{k}(_{_{m}}).\] (5)

Set \(=\), since \(_{i=1}^{n}_{j=1}^{n}k(_{i},_{j})\) is a constant, the objective function in (5) can be rewritten by

\[\,_{k}^{2}(_{_{m}},_{n})+S_{k}(_{_{m}})+}_{i=1}^{n}_{j=1}^{n}k(_{i}, _{j})\] (6) \[= }{n^{2}}_{i=1}^{n}_{j=1}^{n}k( _{i},_{j})+}_{i_{m} }_{j_{m}}k(_{i},_{j})-_{i=1}^{n}_{j_{m}}k(_{i}, _{j})\] \[= _{\|f\|_{u} 1}(_{i_{m}}f( _{i})-_{j=1}^{n}f(_{j}))^{2},\]

which defines a new concept called \(\)-MMD, denoted by \(_{k,}(_{_{m}},_{n})\). This new concept distinguishes our method from those existing methods, which is essential for developing the sampling algorithms and theoretical analysis. Note that \(\)-MMD degenerates to classical MMD when \(=1\) and degenerates to average similarity when \(=0\). As \(\) decreases, \(\) increases, thereby encouraging the diversity for sample selection.

**Remark 1**.: In the following context, all the kernels are assumed to be characteristic and positive definite if not specified. The following illustrates the advantages of the two properties.

**Characteristics kernels.** The MMD is generally a pseudo-metric on the space of all Borel probability distributions, implying that the MMD between two different distributions can be zero. Nevertheless, MMD becomes a proper metric when \(k\) is a characteristic kernel, _i.e.,_\(P_{}k(,)dP\) for any Borel probability distribution \(P\) on \(\). Therefore, MMD induced by characteristic kernels can be more appropriate for measuring representativeness.

**Positive definite kernels.** Aronszajn  showed that for every positive definite kernel \(k(,)\), _i.e.,_ its Gram matrix is always positive definite and symmetric, it uniquely determines an RKHS \(\) and vice versa. This property is not only important for evaluating the property of MMD  but also required in optimizing MMD  by Frank-Wolfe algorithm.

### Sampling Algorithm

In the previous research [36; 27; 50; 38; 58], sample selection is usually modelled by a non-convex combinatorial optimization problem. In contrast, following the idea of , we regard \(_{_{m}[n]}_{k,}^{2}(_{_{m}},_{n})\) as a convex optimization problem by exploiting the convexity of \(\)-MMD, and then solve it by a fast iterative minimization procedure derived from Frank-Wolfe algorithm (see Appendix A for derivation details):

\[_{i_{p+1}^{*}}*{arg\,min}_{i[n]}f_{_{p} ^{*}}(_{i}),_{p+1}^{*}_{p}^{*}\{ i_{p+1}^{*}\},_{0}=,\] (7)

where \(f_{_{p}}(_{i})=_{j_{p}}k(_{i},_{j})- p_{l=1}^{n}k(_{i}, _{l})\). As an extension of kernel herding , its corresponding algorithm (see Algorithm 2) is called Generalized Kernel Herding (GKH). Note that \(f_{_{p}}(_{i})\) is iteratively updated in Algorithm 2, which can save a lot of running time. However, GKH can select repeated samples that contradict the setting of sampling without replacement. To address this issue, we propose a modified iterating formula based on (7):

\[_{i_{p+1}^{*}}*{arg\,min}_{i[n]_{p}^{*}}f_{_{p}^{*}}(_{i}),_{p+1}^{*} _{p}^{*}\{i_{p+1}^{*}\},_{0}^{*}=,\] (8)which admits no repetitiveness in the selected samples. Its corresponding algorithm (see Algorithm 1) is thereby named as Generalized Kernel Herding without Replacement (GKHR), employed as the sampling algorithm for RDSS.

**Computational complexity.** Despite the time cost for calculating kernel functions, the computational complexity of GKHR is \(O(mn)\), since in each iteration, the steps in lines 4 and 5 of Algorithm 2 respectively require \(O(n)\) computations. Note that GKH has the same order of computational complexity as GKHR.

## 5 Theoretical Analysis

### Generalization Bounds

Recall the core-set approach in , _i.e.,_ for any \(h\),

\[R(h)_{S}(h)+|R(h)-_{T}(h)|+|_{T}(h)- _{S}(h)|,\]

where \(T\) is the full labeled dataset and \(S T\) is the core set, \(R(h)\) is the expected risk of \(h\), \(_{T}(h),_{S}(h)\) are empirical risk of \(h\) on \(T,S\). The first term \(_{S}(h)\) is unknown before we label the selected samples, and the second term \(|R(h)-_{T}(h)|\) can be upper bounded by the so-called generalization bounds [3; 64] which do not depend on the choice of core set. Therefore, to control the upper bound of \(R(h)\), we only need to analyse the upper bound of the third term \(|_{T}(h)-_{S}(h)|\) called core-set loss, which requires several mild assumptions. Shalit, et al.  derived a MMD-type upper bound for \(|_{T}(h)-_{S}(h)|\) to estimate individual treatment effect, while our bound is generalized to a wider range of tasks.

Let \(_{1}=\{h|h:\}\) be a hypothesis set in which we are going to select a predictor and suppose that the labelled data \(T=\{(_{i},y_{i})\}_{i=1}^{n}\) are i.i.d. sampled from a random vector \((X,Y)\) defined on \(\). We firstly assume that \(_{1}\) is an RKHS, which is mild in machine learning theory [3; 5].

**Assumption 5.1**.: \(_{1}\) is an RKHS associated with bounded positive definite kernel \(k_{1}\) where the norm of any \(h_{1}\) is bounded by \(K_{h}\).

We further make RKHS assumptions on the functional space of \((Y|X)\) and \((Y|X)\) that are fundamental in the field of conditional distribution embedding [41; 43].

**Assumption 5.2**.: There is an RKHS \(_{2}\) associated with bounded positive definite kernel \(k_{2}\) such that \((Y|X)_{2}\) and the norm of any \((Y|X)\) is bounded by \(K_{m}\).

**Assumption 5.3**.: There is an RKHS \(_{3}\) associated with bounded positive definite kernel \(k_{3}\) such that \((Y|X)_{3}\) and the norm of any \((Y|X)\) is bounded by \(K_{s}\).

We next give a \(\)-MMD-type upper bound for the core-set loss by the following theorem:

**Theorem 5.4**.: _Take \(k=k_{1}^{2}+k_{1}k_{2}+k_{3}\), then under assumptions 1-3, for any selected samples \(S T\), there exists a positive constant \(K_{c}\) such that the following inequality holds:_

\[|_{T}(h)-_{S}(h)| K_{c}(_{k,}( _{S},_{T})+(1-))^{2},\]

_where \(0 1\), \(0_{}k(,)=K\) and \(_{S},_{T}\) are projections of \(S,T\) on \(\)._Therefore, minimizing \(\)-MMD can optimize the generalization bound for \(R(h)\) and benefit the generalizability of the trained model (predictor).

### Finite-Sample-Error-Bound for GKHR

The concept of convergence does not apply to analyzing GKHR. With \(n\) fixed, GKHR iterates for at most \(n\) times and then returns \(_{_{p}^{*}}=_{n}\). Consequently, we analyze the performance of GKHR by its finite-sample-error bound. Previous to that, we make an assumption on the mean of \(f_{_{p}^{*}}\) over the full unlabeled dataset.

**Assumption 5.5**.: For any \(_{p}^{*}\) returned by GKHR, \(1 p m-1\), there exists \(p+1\) elements \(\{_{j_{l}}\}_{l=1}^{p+1}\) in \(_{n}\) such that

\[f_{_{p}^{*}}(_{j_{1}}) f_{_{p}^{*}}( _{j_{p+1}})^{n}f_{_{p}^{*}}(_{i})}{n}.\]

When \(m\) is not relatively small, this assumption is rather unrealistic. Nevertheless, under our low-budget setting, especially when \(m n\), the assumption becomes an extension of the principle that "the minimum is never larger than the mean", which still probably makes sense. We can then show that the decaying rate for optimization error of GKHR can be upper bounded by \(O( m/m)\):

**Theorem 5.6**.: _Let \(_{_{m}^{*}}\) be the samples selected by GKHR, under assumption 4, it holds that_

\[_{k,}^{2}(_{_{m}^{*}},_ {n}) C_{}^{2}+B\] (9)

_where \(B=2K\), \(0_{}k(,)=K\), \(C_{}^{2}=(1-)^{2}\) where \(\) is defined in Lemma B.6._

## 6 Choice of Kernel and Hyperparameter Tuning

In this section, we make some suggestions for choosing the kernel and tuning the hyperparameter \(\).

**Choice of kernel.** Recall Remark 1 in Section 4.1, we only consider characteristic and positive definite kernels in RDSS. Since the Gaussian kernels are the most commonly used kernels in the field of machine learning and statistics [3; 15], we introduce Gaussian kernel as our choice, which is defined by \(k(,)=(-\|-\|_{2}^{2}/^{2})\). The bandwidth parameter \(\) is set to be the median distance between samples in the aggregate dataset , _i.e., \(=(\{\|-\|_{2}|, _{n}\})\)_, since the median is robust and also compromises between extreme cases.

**Tuning trade-off hyperparameter \(\).** According to Theorem 5.6 and Lemma B.3, by straightforward deduction we have

\[_{k}(_{_{m}^{*}},_{n})  C_{}+(})+(1-) \]

to upper bound the MMD between the selected samples and the full dataset under a low-budget setting. We can just set \([1-},1)\) so that the upper bound of the MMD would not be larger than the one of \(\)-MMD in the perspective of the order of magnitude.

## 7 Experiments

In this section, we first explain the implementation details of our method RDSS in Section 7.1. Next, we compare RDSS with other sampling methods by integrating them into two state-of-the-art (SOTA) SSL approaches (FlexMatch  and Freematch ) on five datasets (CIFAR-10/100, SVHN, STL-10 and ImageNet-1k) in Section 7.2. The details of the datasets, the visualization results and the computational complexity of different sampling methods are shown in Appendix D.2, D.3, and D.4, respectively. We also compare against various AL/SSAL approaches in Section 7.3. Lastly, we make quantitative analyses of the trade-off parameter \(\) in Section 7.4.

### Implementation Details of Our Method

First, we leverage the pre-trained image feature extraction capabilities of CLIP , a vision transformer architecture, to extract features. Subsequently, the [CLS] token features produced by the model's final output are employed for sample selection. During the sample selection phase, the Gaussian kernel function is chosen as the kernel method to compute the similarity of samples in an infinite-dimensional feature space. The value of \(\) for the Gaussian kernel function is set as explained in Section 6. To ensure diversity in the sampled data, we introduce a penalty factor given by \(=1-}\), where \(m\) denotes the number of selected samples. Concretely, we set \(m=\{40,250,4000\}\) for CIFAR-10, \(m=\{400,2500,10000\}\) for CIFAR-100, \(m=\{250,1000\}\) for SVHN, \(m=\{40,250\}\) for STL-10 and \(m=\{100000\}\) for ImageNet. Next, the selected samples are used for two SSL approaches, which are trained and evaluated on the datasets using the codebase Unified SSL Benchmark (USB) . The optimizer for all experiments is standard stochastic gradient descent (SGD) with a momentum of \(0.9\). The initial learning rate is \(0.03\) with a learning rate decay of \(0.0005\). We use ResNet-50  for the ImageNet experiment and Wide ResNet-28-2  for other datasets. Finally, we evaluate the performance with the Top-1 classification accuracy metric on the test set. Experiments are run on 8*NVIDIA Tesla A100 (40 GB) and 2*Intel 6248R 24-Core Processor. We average our results over five independent runs.

### Comparison with Other Sampling Methods

**Main results** We apply RDSS on Flexmatch and Freematch to compare with the following three baselines and two SOTA methods in SSL under different annotation budget settings. The baselines conclude **Stratified**, **Random** and \(k\)**-Means**, while the two SOTA methods are **USL** and **ActiveFT**. The results are shown on Table 1 from which we have several observations: (1) Our proposed RDSS achieves the highest accuracy, outperforming other sampling methods, which underscores the effectiveness of our approach; (2) USL attains suboptimal results under most budget settings yet exhibits a significant gap compared to RDSS, particularly under severely constrained ones. For instance, FreeMatch achieves a \(4.95\%\) rise on the STL-10 with a budget of \(40\); (3) In most experiments, RDSS either approaches or surpasses the performance of stratified sampling, especially on SVHN and STL-10. However, the stratified sampling method is practically infeasible given that the category labels of the data are not known a priori.

**Results on ImageNet** We also compare the second-best method USL with RDSS on ImageNet. Following the settings of FreeMatch , we select 100k samples for annotation. FreeMatch, using RDSS and USL as sampling methods, achieves \(58.24\%\) and \(56.86\%\) accuracy, respectively, demonstrating a substantial enhancement in the performance of our method over the USL approach.

### Comparison with AL/SSAL Approaches

First, we compare RDSS against various traditional AL approaches on CIFAR-10/100. AL approaches conclude **CoreSet**, **VAAL**, **LearnLoss** and **MCDAL**. For a fair comparison, we

exclusively use samples selected by RDSS for supervised learning compared to other AL approaches, considering that AL relies solely on labelled samples for supervised learning. The implementation details are shown in Appendix D.5. The experimental results are presented in Table 2, from which we observe that RDSS achieves the highest accuracy under almost all budget settings when relying solely on labelled data for supervised learning, with notable improvements on CIFAR-100.

Second, we compare RDSS with sampling methods used in SSAL when applied to the same SSL framework (_i.e.,_ FlexMatch or FreeMatch) on CIFAR-10. The sampling methods conclude **CoreSetSSL**, **MMA**, **CBSSAL**, and **TOD-Semi**. In detail, we tune recent SSAL approaches with their public implementations and run experiments under an extremely low-budget setting, _i.e.,_ 40 samples in a 20-random-and-20-selected setting. Table 3 illustrates that the performance of most SSAL approaches falls below that of random sampling methods under extremely low-budget settings. This inefficiency stems from the dependency of sample selection on model performance within the SSAL framework, which struggles when the model is weak. Our model-free method, in contrast, selects samples before training, avoiding these pitfalls.

Third, we directly compare RDSS with the above AL/SSAL approaches when applied to SSL, which may better reflect the paradigm differences. The experimental results and analysis are in the Appendix D.6.

### Trade-off Parameter \(\)

We analyze the effect of different \(\) with Freematch on CIFAR-10/100. The results are presented in Table 4, from which we have several observations: (1) Our proposed RDSS achieves the highest accuracy under all budget conditions, surpassing those that employ a fixed value; (2) The \(\) that achieve the best or the second best performance are within the interval we set, which is in line with our theoretical derivation in Section 6; (3) The experimental outcomes exhibit varying degrees of reduction compared to our approach when the representativeness or diversity term is removed.

   Dataset &  &  \\  Budget (\(m\)) & 40 & **250** & **4000** & **400** & **2500** & **10000** \\ 
0 & \(85.54 0.48\) & \(93.55 0.34\) & \(94.58 0.27\) & \(39.26 0.52\) & \(63.77 0.26\) & \(71.90 0.17\) \\
0.40 & \(92.28 0.24\) & \(93.68 0.13\) & \(94.95 0.12\) & \(42.56 0.47\) & \(65.88 0.24\) & \(71.71 0.29\) \\
0.80 & \(94.42 0.49\) & \(94.94 0.37\) & \(95.15 0.35\) & \(45.62 0.35\) & \(66.87 0.20\) & \(72.45 0.23\) \\
0.90 & **94.33\( 0.28\)** & \(95.03 0.21\) & \(95.20 0.42\) & \(48.12 0.50\) & \(67.14 0.16\) & \(72.15 0.23\) \\
0.95 & 94.44\( 0.64\) & \(95.07 0.26\) & \(95.45 0.38\) & **48.41\( 0.59\)** & \(67.11 0.29\) & \(72.80 0.35\) \\
0.98 & 94.51\( 0.39\) & 95.02\( 0.15\) & \(95.31 0.44\) & **48.33\( 0.54\)** & **67.40\( 0.23\)** & \(72.68 0.22\) \\
1 & 94.53\( 0.42\) & 95.01\( 0.23\) & **95.54\( 0.25\)** & 48.18\( 0.36\) & **67.20\( 0.29\)** & **73.05\( 0.18\)** \\  \(1-1/\) (Ours) & **95.05\( 0.13\)** & **95.50\( 0.20\)** & **95.98\( 0.28\)** & **48.41\( 0.59\)** & **67.40\( 0.23\)** & **73.13\( 0.19\)** \\   

Table 4: Effect of different \(\). The grey results indicate that the \(\) is outside the interval we set in Section 6, _i.e.,_\(<1-1/\), while the black results indicate that the \(\) is within the interval we set, _i.e.,_\(1-1/ 1\). Among them, \(=0\) and \(=1\) indicate the removal of the representativeness and diversity terms, respectively. The best performance is bold, and the second-best performance is underlined.

   Dataset &  &  \\  Budget & 7500 & 10000 & 7500 & 10000 \\  CoreSet & 85.46 & 87.56 & 47.17 & 53.06 \\ VAAL & 86.82 & 88.97 & 47.02 & 53.99 \\ LearnLoss & 85.49 & 87.06 & 47.81 & 54.02 \\ MCDAL & **87.24** & 89.40 & 49.34 & 54.14 \\ SL+RDSS (Ours) & 87.18 & **89.77** & **50.13** & **56.04** \\  Whole Dataset &  &  \\   

Table 2: Comparison with AL approaches under Supervised Learning (SL) paradigm. The best performance is bold and the second best performance is underlined.

   Method & FlexMatch & FreeMatch \\  Stratified & 91.45 & 95.05 \\ Random & 87.30 & 93.41 \\  CoreSetSSL & 87.66 \(\) 0.36 & 91.24 \(\) 2.17 \\ MMA & 74.61 \(\) 12.69 & 87.37 \(\) 6.04 \\ CBSSAL & 86.58 \(\) 0.72 & 91.68 \(\) 1.73 \\ TOD-Semi & 86.21 \(\) 1.09 & 90.77 \(\) 2.64 \\  RDSS (Ours) & **94.69**\(\) 7.39 & **95.05**\(\) 1.64 \\   

Table 3: Comparison with SSAL approaches. The green (red) arrow represents the improvement (decrease) compared to the random sampling method.

Conclusion

In this work, we propose a model-free sampling method, RDSS, to select a subset from unlabeled data for annotation in SSL. The primary innovation of our approach lies in the introduction of \(\)-MMD, designed to evaluate the representativeness and diversity of selected samples. Under a low-budget setting, we develop a fast and efficient algorithm GKHR for this problem using the Frank-Wolfe algorithm. Both theoretical analyses and empirical experiments demonstrate the effectiveness of RDSS. In future research, we would like to apply our methodology to scenarios where labelling is cost-prohibitive, such as in the medical domain.