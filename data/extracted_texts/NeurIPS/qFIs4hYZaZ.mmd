# Learning Interatomic Potentials at Multiple Scales

Xiang Fu

MIT CSAIL

xiangfu@mit.edu &Albert Musaelian

Harvard John A. Paulson

School of Engineering and Applied Sciences

albym@g.harvard.edu &Anders Johansson

Harvard John A. Paulson

School of Engineering and Applied Sciences

andersjohansson@g.harvard.edu &Tommi Jaakkola

MIT CSAIL

tommi@csail.mit.edu &Boris Kozinsky

Harvard John A. Paulson School of Engineering and Applied Sciences and

Robert Bosch Research and Technology Center

bkoz@g.harvard.edu

###### Abstract

The need to use a short time step is a key limit on the speed of molecular dynamics (MD) simulations. Simulations governed by classical potentials are often accelerated by using a multiple-time-step (MTS) integrator that evaluates certain potential energy terms that vary more slowly than others less frequently. This approach is enabled by the simple but limiting analytic forms of classical potentials. Machine learning interatomic potentials (MLPs), in particular recent equivariant neural networks, are much more broadly applicable than classical potentials and can faithfully reproduce the expensive but accurate reference electronic structure calculations used to train them. They still, however, require the use of a single short time step, as they lack the inherent term-by-term scale separation of classical potentials. This work introduces a method to learn a scale separation in complex interatomic interactions by co-training two MLIPs. Initially, a small and efficient model is trained to reproduce short-time-scale interactions. Subsequently, a large and expressive model is trained jointly to capture the remaining interactions not captured by the small model. When running MD, the MTS integrator then evaluates the smaller model for every time step and the larger model less frequently, accelerating simulation. Compared to a conventionally trained MLIP, our approach can achieve a significant speedup (\(\)3x in our experiments) without a loss of accuracy on the potential energy or simulation-derived quantities.

## 1 Introduction

The interatomic potential energy that governs the dynamics of a system of atoms has long been both understood and modeled as a combination of atomic interactions of various strengths and scales. In a system containing a comparatively stiff molecule in a soft fluid, for example, the intramolecular forces are much stronger than the intermolecular forces from the solvent. Classical potentials, such as the popular optimized potentials for liquid simulations (OPLS, [1; 2]), explicitly define the potential as such a sum over simple analytic terms:

\[E()=E_{}()+E_{}()+E_{}()+E_{}()\]where \(\) denotes the atom positions; \(E_{ bonds}(),E_{ angles}()\), and \(E_{ dihedrals}()\) are intramolecular (or "bonded") bond, angle, and torsional potentials; and \(E_{ nb}\) denotes the nonbonded, including intermolecular potential term. Rigid interactions such as bond vibrations, governed by \(E_{ bonds}\), occur at fast time scales, while the nonbonded interactions \(E_{ nb}\) are slower and smoother. Due to the greater number of atoms involved in nonbonded interactions, however, the computational expense of calculating the \(E_{ nb}\) term can be meaningfully larger.

Essentially, the intramolecular and intermolecular nonbonded forces are of different scales. While the intramolecular forces require a short time step, integrating the intermolecular forces at the same step size is often overkill. To harness this separation of both scales and computational cost, multiple-time-step (MTS) integrators [3; 4; 5; 6; 7] integrate the fast-evolving terms with a short time step and the slow-evolving terms with a long time step. MTS integrators are theoretically principled and have been widely used in various classical MD workflows [8; 9; 10; 11; 12], usually bringing a speedup of two to four times.

The approximations and limited functional forms of classical potentials are not sufficient for many applications. Ab initio molecular dynamics (AIMD) simulations--governed by a potential energy surface computed with electronic structure methods such as Density Functional Theory (DFT)--are widely used but suffer from dramatic computational limitations on time- and length-scale due to the expense and unfavorable scaling of the electronic structure calculations. The application of MTS schemes to AIMD has been limited by the difficulties in decomposing the ab initio potential into components of separate scales in the absence of a simple analytical form [13; 14].

Machine learning interatomic potentials (MLPs) [15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48] are increasingly used to run MD simulations orders of magnitude cheaper than AIMD while preserving near-AIMD accuracy. MLIPs also largely avoid classical potentials' assumption of an explicit discrete covalent bond topology, which greatly broadens their applicability, notably in materials science and reactive simulations. While MLIPs have enabled many previously impractical or impossible simulations, further improvements in the speed and cost of MLIP-driven MD are extremely valuable. MTS methods remain largely unexplored for MLIPs: although recent research  has combined an infrequently evaluated MLIP with a classical potential evaluated at every time step, a methodology for machine learning a scale-separated potential model from data is, to the best of our knowledge, lacking in the literature. This work presents an approach for learning complementary scale-separated MLIPs from data, yielding a \(\)3x speedup in MD simulations using MTS integration.

## 2 Preliminaries

**MD simulation** involves integrating a Newtonian equation of motion \(}=d^{2}/dt^{2}=^{-1}()\) with atomic positions \(\), masses \(\), and forces \(\). The forces are obtained by differentiating the potential energy of a molecular system \(E()\) with respect to the atomic positions \(\): \(()=- E/\).

**Allegro** is an \(E(3)\)-equivariant  neural network MLIP architecture for predicting \(E()\) that enforces strictly local interactions while achieving state-of-the-art performance. Its strict locality enables efficient parallel scaling across GPUs to reach larger length- and time-scales . In Allegro, every pair of atoms within a chosen fixed cutoff distance \(r\) are considered neighbors. For each ordered pair of neighboring atoms \(i\) and \(j\), an Allegro model produces a learnable, \(E(3)\)-invariant, many-body final latent representation \(^{ij}\) of the geometry of \(i\), \(j\), and all other neighbors of \(i\). An edge energy \(E_{ij}\) is then predicted from it via the output block, a multi-layer perception \(_{ out}\) without bias; the sum over edge energies gives the total potential energy:

\[E_{ ML}()=_{(i,j):||_{i}-_{j}|| r}E_{ij}=_{(i,j):||_{i}-_{j}|| r}_{ out}(^{ij})\] (1)

An Allegro model's predicted \(E_{ ML}()\) is trained to reproduce the potential energy and forces from a reference method such as DFT using a loss function like:

\[=_{E}\|-E}{N}\|^{2}+_{F} \|-}{}-\| ^{2}\] (2)

where \(_{E}\) and \(_{F}\) are loss coefficients for energy and forces and \(N\) is the number of atoms. For complete details on Allegro and its training see .

**MTS Integrators** accelerate MD simulations by propagating different parts of the dynamics with different time steps suited to their respective characteristic time scales. In this paper, we focus on the reversible reference system propagator algorithms (rRESPA [5; 52]), an MTS integrator popular for its rigorous derivation, time-reversibility, and symplectic properties. We show the rRESPA algorithm in Algorithm 1 and refer interested readers to  for detailed derivations.

```
1:Input: Inner time step \( t\), number of inner time steps per outer time step \(N_{}\), short-range force \(_{s}\), long-range force \(_{l}\), atom masses \(\), initial positions \(\), initial velocities \(}\)
2:\(}}+(N_{} t) ^{-1}_{l}()\)
3:for step \(i=1,,N_{}\)do
4:\(}}+ t^{-1}_ {s}()\)
5:\(}}+ t^{-1}_ {s}()\)
6:endfor
7:\(}}+(N_{} t) ^{-1}_{l}()\) ```

**Algorithm 1** An integration step of the MTS integrator (rRESPA)

## 3 Learning Scale Separation

To achieve scale separation and harness the MTS integrator, we need to enable efficient calculation of stiff and fast-evolving force terms. Meanwhile, we still need to capture the smooth and slow-evolving force terms so that the overall machine learning potential remains accurate. For many molecular systems of interest, short-range interactions, such as covalent bonds, are strong and induce stiff motions, while long-range interactions, such as non-bonded interactions, can be integrated with a longer time step.

The above observation motivates separating scales by combining MLIPs with different receptive fields. Allegro's unique combination of the leading accuracy of equivariant techniques with strict locality particularly lends itself to such a scale separation scheme. We train two models:

* **Inner model** (\(E_{}\)): An efficient model with fewer parameters and interaction layers and a small radial cutoff that captures short-time-scale interactions.
* **Outer model** (\(E_{}\)): An expressive model with a larger number of parameters and interaction layers and a larger radial cutoff that fits the remaining interactions not learned by the inner model.

We let the two models jointly predict the potential energy: \(E_{}()=E_{}()+E_{}()\). Training the two models together from scratch, however, may not induce scale separation: the outer model has sufficient capacity to learn the entire reference potential energy surface, and so the short-range interactions may not be attributed to the inner model. To avoid such degeneracy, at the beginning of training, we first freeze all parameters in the outer model to let the inner model fit the force and energy within its capacity. We then later start training the outer model with a zero-initialization over its output block \(_{}\). This zero initialization ensures \(E_{ij}^{}=0\) and \( E_{ij}^{}/=0\) for all atom pairs \((i,j)\), preventing the initial noise of the outer model from interfering with the interactions already learned by the inner model. Zero-initialization has been widely used in previous works for finetuning pretrained models . The training procedure is presented in Algorithm 2. We refer to our scale-separated Allegro model as **MTS-Allegro**.

## 4 Experiments

Our experiments consider an ab initio water system . This dataset contains 1593 reference calculations of bulk liquid water at the revPBE0-D3 level of accuracy. Each structure contains 192 atoms (64 water molecules). We randomly sample 1000 structures for training, 100 structures for validation, and the rest for testing1. Both the Allegro model and the outer model of MTS-Allegro havetwo interaction layers, a 6 A cutoff, and the same parameter count; the inner model of MTS-Allegro has one interaction layer, a 4 A cutoff, and half the width in each layer compared to the outer model. Detailed hyperparameters are included in Appendix A.

We compare the accuracy in recovering the reference potential and various simulation-derived quantities between (1) our proposed MTS-Allegro model with rRESPA integration, (2) a conventionally trained Allegro model with standard Velocity Verlet integration, and (3) the inner model of MTS-Allegro alone with Velocity Verlet integration. We use a time step of \(0.5\) fs for Velocity Verlet integration and the inner loop of MTS integration, which is standard for water simulations . For MTS-Allegro, we experiment with outer time steps of \([1.0,2.0,3.0,4.0]\) fs, corresponding to [2x, 4x, 6x, 8x] multiples of the inner time step.

**Force and energy prediction accuracy.** We report the force and energy prediction errors in Table 1. The MTS-Allegro model has a very similar performance to a conventionally trained Allegro model. Unsurprisingly, the inner model of MTS-Allegro alone obtains higher errors due to its limited receptive field and capacity.

**Structure and dynamics.** To investigate MTS-Allegro's ability to reproduce structural and dynamical observables, we simulate the water system in a canonical (NVT) ensemble (constant volume and temperature) at 300 K and compute the element-wise radial distribution functions (RDFs, structural) and the mean squared displacement (MSD, dynamical). We simulate for 400 picoseconds (ps) and remove the first 50 ps for equilibration when computing the observables. All models remain stable throughout the entire simulation. Figure 1 (a-d) shows the RDFs and MSD (in log-log space) for Allegro, the inner model of MTS-Allegro, and MTS-Allegro under different outer step sizes. MTS-Allegro simulations with 2x to 8x outer time steps all achieve excellent agreement with Allegro on the RDFs and MSD, while the inner model alone produces erroneous dynamics and observables due to its limited capacity and, thereby, accuracy in recovering the potential.

**Energy conservation under the microcanonical ensemble.** For a correct MD simulation in the microcanonical (NVE) ensemble, the total energy (sum of potential and kinetic energies) should remain constant over time. To evaluate the energy conservation property of the MLIPs, we initialize the water system using a structure from the test dataset and a temperature of 300 K, run energy minimization, and then simulate for 100 ps in the NVE ensemble. Figure 1 (e) shows the drift of total energy from the first frame (after removing the first 20 ps for equilibration). We observe that Allegro and MTS-Allegro with 2x and 4x outer time steps are energy-conserving. MTS-Allegro with 6x and 8x outer time steps yield drifts of \(0.12\) meV/(atom\(\)ps) and \(0.18\) meV/(atom\(\)ps), respectively.

    & Allegro & MTS-Allegro & MTS-Allegro, inner \\  Energy MAE [meV/Atom] & 2.4 & 1.6 & 12.5 \\ Forces MAE [meV/Å] & 40.3 & 35.5 & 72.5 \\ Forces RMSE [meV/Å] & 77.4 & 76.9 & 118.3 \\   

Table 1: Energy and Force prediction mean absolute error (MAE) and root mean square error (RMSE).

**Speedup.** Figure 2 (a) shows the relative speedup of MTS-Allegro compared to Allegro at two system sizes as measured in MD simulation2. MTS-Allegro with a 4x outer time step, which preserves simulation-derived quantities and maintains energy conservation, achieves an MD speedup of 2.6x for a system size of 192 atoms and 2.9x for a system size of 1536 atoms. MTS-Allegro 6x and 8x, which obtain a further speedup, remain reliably stable in our experiments and faithfully reproduce RDFs and MSDs but do not maintain energy conservation. Such larger outer time step simulations may still find use in preliminary or other calculations whose requirements are less stringent.

**Analysis of scale separation.** We investigate how well MTS-Allegro separates scales by inspecting the time-smoothness and strength of the learned intentions. Figure 2 (b) shows the average relative frame-to-frame force difference (defined as \(_{t}[\|_{t+}-_{t}\|/\|_{t}\|]\) for each component) in \(0.5\) fs time step Velocity Verlet MD for different models. Compared to the conventional Allegro model and the inner model, the outer model of MTS-Allegro outputs forces with a much lower \(_{t}[\|_{t+}-_{t}\|/\| _{t}\|]\) for all \(\), indicating that it learns interactions that change much more slowly in time. Figure 2 (c) shows the average force norm (defined as \(_{t}[\|_{t}\|]\)) of different models. The outer model learns interactions that are much weaker in magnitude than Allegro and the inner model of MTS-Allegro. Weak and slow-varying interactions are exactly what allows for a longer time step. This analysis confirms the effectiveness of MTS-Allegro in learning scale separation.

Figure 1: (a-c) O-O, H-H, and H-O RDFs of NVT simulations. (d) MSD of NVT simulations. (e) Total energy drift of NVE simulations. In the legend, MTS-Allegro is shortened for “MTS” along with the outer-inner time step ratio.

Figure 2: (a) Relative speedup factor compared to Allegro (1x in the plot). (b) Average relative frame-to-frame force difference in \(0.5\) fs time step Velocity Verlet MD for different models. (c) Average norm of forces for different models.

Conclusion

We have developed a method to accelerate MLIP-driven MD simulations by learning a scale separation and using an MTS integrator. In an ab initio water system, our approach achieves around three times speedup without loss in accuracy on the potential energy or simulation-derived quantities. MTS integrators can also be used with more than two levels, and learning finer-grained scale separations with more than two MLIPs is a direction for future work. It is also possible to co-train different model architectures, such as kernel-based methods  and message-passing MLIPs , for accuracy-speed trade-offs. The presented technique promises a direction for significant practical speed gains when running MLIP-driven MD, including in large and complex systems.