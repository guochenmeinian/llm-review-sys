# Single Image Reflection Separation via

Dual-Stream Interactive Transformers

Qiming Hu, Hainuo Wang, Xiaojie Guo

College of Intelligence and Computing, Tianjin University, Tianjin 300350, China

huqiming@tju.edu.cn hainuo@tju.edu.cn xj.max.guo@gmail.com

Corresponding Author

###### Abstract

Despite satisfactory results on "easy" cases of single image reflection separation, prior dual-stream methods still suffer from considerable performance degradation when facing complex ones, _i.e._, the transmission layer is densely entangled with the reflection having a wide distribution of spatial intensity. The main reasons come from the lack of concern on the feature correlation during interaction, and the limited receptive field. To remedy these deficiencies, this paper presents a Dual-Stream Interactive Transformer (DSIT) design. Specifically, we devise a dual-attention interactive structure that embraces a dual-stream self-attention and a layer-aware dual-stream cross-attention mechanism to simultaneously capture intra-layer and inter-layer feature correlations. Meanwhile, the introduction of attention mechanisms can also mitigate the receptive field limitation. We modulate single-stream pre-trained Transformer embeddings with dual-stream convolutional features through cross-architecture interactions to provide richer semantic priors, thereby further relieving the ill-posedness of the problem. Extensive experimental results reveal the merits of the proposed DSIT over other state-of-the-art alternatives. Our code is publicly available at https://github.com/mingcv/DSIT.

## 1 Introduction

When images are captured through glass-like mediums (semi-reflectors), the reflected scenes appear together with the transmitted ones to different degrees, influenced by many factors such as the material of medium and the illumination of both scenes, among others . This phenomenon poses significant challenges in various fields, like multi-view stereo imaging, mobile photography, security surveillance, and autonomous driving . Therefore, successfully separating the superimposed layers can, on the one hand, enhance the capability of models to serve downstream applications. On the other hand, it paves the way for tackling a broader spectrum of layer-decomposition tasks, such as image denoising and watermark/obstacle removal .

As a long-standing blind source separation problem, single image reflection separation (SIRS) has always been challenging, due to the severe ill-posedness of disentangling two natural image signals. Generally, the superimposed images \(\) can be formulated as follows:

\[=++(,).\] (1)

It consists of an additive combination of target transmission and reflection layers (\(\) and \(\), respectively) and a residual component, denoted by the mapping \(\) of the two layers. Note that \(\) is used to describe the non-linear/linear attenuation of the two layers, thus representing a group of reflection models . In the literature, two main routes of approaches have been delivered. One tendency (single-stream) is to treat the reflection layer as noise/degradation, merely modeling the transmission layer. Alternatively, the other rising trend (dual-stream) pays attention to the reconstruction qualityof both the transmission and reflection layers. This work follows the latter principle because the reflection layer may also contain valuable information  and imposing constraints from both perspectives can better regularize the decomposition [22; 23].

The dual-stream schemes, with IBCLN , YTMT , and DSRNet  as representatives, attempt to estimate both of the two layers with Siamese networks that employ two sub-networks with identical architecture and shared weights. Particularly, YTMT and DSRNet advocated dual-stream feature interactions to facilitate the information flow between the streams and finally reconstruct the decoupled layers. Though being effective, the interaction mechanisms used in these two methods do NOT explicitly assess the correlation between dual-stream features during interactions, as illustrated in Fig. 1 (a) and (b). More concretely, assuming we have dual-stream features at a certain stage in the networks, previous interaction strategies directly pass the undesired information at the current stage (may be required again at subsequent stages) from one branch to the other without checking if the passed information is needed by the sibling. Actually, in intermediate blocks, some information is very likely delivered back and forth, making the separation process ineffective and inefficient. In contrast, the attention mechanisms in Transformers assign small weights to token pairs with low similarities, which are further suppressed by the Softmax function. In other words, employing a cross-attention mechanism seems to have the potential for improving dual-stream interaction. Furthermore, due to the presence of correlated scenes in reflection superposition phenomena across entire images, the task demands a keen perception of global information in both streams. Motivated by the above analysis, we propose a strategy, called dual-attention interaction, consisting of a dual-stream self-attention and a dual-stream cross-attention, as depicted in Fig. 1 (c), to extract both the intra-layer and inter-layer feature correlations explicitly.

In addition, most of recent state-of-the-art methods [60; 48; 22; 23] adopted networks (_e.g._, HyperColumn [60; 48; 22] and Feature Pyramid ) pre-trained on high-semantic tasks to assist feature extraction. In this work, we argue that Transformers, thanks to their generalization and selective attention characteristics, should be more powerful to the target task. However, pre-trained Transformer models typically have trouble in dense prediction due to the lack of inductive biases . To make Transformers suitable, we develop a Dual-Architecture Interactive Encoder (DAIE), which enables the interaction between semantically rich features extracted by a pre-trained Transformer and local dual-stream features extracted by a CNN. By this means, the Cross-Architecture Interactions (CAI) can balance the global and local perspectives and combine high-semantic priors with the low-semantic demands of the reflection separation.

In summary, our primary contributions are as follows:

* We propose a novel Dual-Attention Interaction (DAI) mechanism to energize Dual-Stream Interactive Transformers. DAI introduces the explicit correlation assessment within dual streams to effectively address the challenge of reflection separation;

Figure 1: Schematic illustration of dual-stream interactive behaviors, including YTMT , MuGI , and our proposed Dual-Attention Interaction (DAI) mechanisms, where \(}\) and \(}\) represent the feature flows of transmission layer and reflection layer respectively. The superscript \(l\) of the feature flows denotes the number of building blocks traversed to derive the flows.

* We customize a bridge, namely the Dual-Architecture Interactive Encoder (DAIE), to connect the pre-trained Transformer model with the task of layer decomposition, which alleviates the inherent ill-posedness of the problem;
* Through extensive experiments on multiple datasets, we demonstrate the efficacy of our design with superior performance over other SOTA competitors, both quantitatively and qualitatively. Moreover, the better generalizability compared to previous methods is also verified.

## 2 Related Work

**Low-level Vision Transformers**. Building upon the attention mechanism , Transformers were initially developed by the community of natural language processing , and soon became popular across various domains because of their remarkable modeling [2; 17], scaling [24; 56; 36], and transferring [10; 6; 4] abilities. Introduced by , Vision Transformers (ViTs) have shown advantages on a large number of visual tasks [56; 34; 66; 57; 50; 8].

For low-level purposes, IPT  was developed to handle multiple restoration tasks with a shared standard Transformer body, which required a large number of parameters for good performance without suitable task-related priors. Swin Transformer  introduced the shifted window attention mechanism, which reduced the computational cost of attention while incorporating inductive biases for images, inspiring a series of subsequent works. SwinIR  equipped residual-in-residual structures [46; 61] with the Swin Transformer block, exploring its capability in low-level vision tasks. ELAN  performed multiple window attentions of varying sizes in parallel and fused them, using grouped four-directional offset convolution layers for local feature extraction and cross-window association. Chen _et al._ introduced an overlapping attention mechanism in their HAT model to establish cross-window connections and employed same-task pre-training for better performance. Zhang _et al._ proposed ART , which utilized a sparse window attention mechanism akin to dilated convolution, alternating it with window attention, thereby replacing the shifted window mechanism. UFormer  embedded window attention modules into a U-shaped network, which captured cross-window associations beyond the current scale. Moreover, Restormer  introduced a transposed attention mechanism, resembling channel attention. While faster, this approach somewhat neglected spatial correlations. Retinexformer  employed this design for low-light image enhancement. DAT  alternated between window self-attention and window transposed self-attention to address spatial correlation deficiencies. _Overall, these methods mostly opt to validate their designs on tasks like image super-resolution, which focus on reconstructing a single component, overlooking the intrinsic advantages of attention mechanisms in component decomposition tasks._

**Single Image Reflection Separation**. Single image reflection separation, with looser data assumptions, relies more heavily on priors to alleviate its inherent ill-posedness. Traditional methods developed priors like edge sparsity [28; 27], manual annotation , or relative smoothness , some of which were further leveraged by deep learning methods. Although multiple-image solutions [37; 1; 16; 39; 30; 18; 38; 51; 53; 19] have shown satisfactory performance facing weaker ill-posedness, _these methods typically rely on sequences of images captured with rotating polarizers or moving cameras, which limits their applicability._

In deep learning methods, CEILNet  applied the relative smoothness assumption to data synthesis and used an edge detection network to emphasize edge information. Zhang _et al._ proposed a gradient mutual exclusion loss to promote edge sparsity and introduced the HyperColumn and perceptual loss to incorporate high-level semantics. ERRNet  aimed to expand the receptive field and utilized non-aligned images for data augmentation. _These methods employed end-to-end single-branch networks for estimation, but the lack of interaction between layers led to inefficiency and untidy separations._ In another way, BDN  alternated between estimating transmission and reflection layers, considering their mutual dependency. RAGNet  estimated reflection first and then used its features to modulate the estimation of the transmission layer and mask. Dong _et al._ emphasized the importance of the reflection layer, using multi-scale Laplacian features with LSTM for iterative estimation. RRW  utilized a cascaded reflection detector and remover and proposed a more reasonable data acquisition scheme. Song _et al._ proposed a robust SIRR model based on a multi-scale Transformer architecture, but it only learned to restore the transmission layer, and the usage of the Transformer was not well motivated. IBCLN  introduced a convolutional LSTM network, using a dual-branch structure for reflection and transmission reconstruction but lacking interaction between branches for cross-verifying the accuracy of decoupling. Hu and Guo proposed a dual-streaminteractive reflection separation strategy , called YTMT, assessing information utility through activation functions and exchanging low-value information, facilitating information retention and efficient interaction in the high-dimensional feature space. Their subsequent DSRNet  used MuGI for efficient interaction and introduced a non-linear residual component to estimate the overexposure during the layer superimposition, achieving state-of-the-art performance. _However, neither the YTMT nor the MuGI mechanism explicitly assessed the correlation of exchanged information, which can introduce errors and lower the decoupling efficiency._ To further alleviate the ill-posedness of the problem, a recent concurrent work  utilized wavelet priors and diffusion models to guide frequency-domain-based reflection removal. The other work  used natural language prompts for reflection separation, which, however, required accurate paired prompts, incurring additional costs.

## 3 Methodology

Our overall architecture is illustrated in Fig. 2 (a), which comprises a Dual-Architecture Interactive Encoder (DAIE) and a Dual-Stream Interactive Decoder (DSID). The DAIE leverages both a pre-trained Transformer (Global Prior Extractor, GPE) and a task-specific dual-stream CNN (Local Prior Extractor, LPE), capturing global and local features through specialized extractors. The mixed global information is then injected into the dual-stream local flows via Cross-Architecture Interactions (CAI), ensuring comprehensive information utilization. Subsequently, the DSID separates and aggregates the embeddings hierarchically through our newly developed Dual-Attention Interactive Block (DAIB, illustrated in Fig. 2 (b)). These components are detailed in the following subsections.

### Dual-Attention Interactive Block

As depicted in Fig. 2 (b), our proposed DAIB embraces a dual-stream design, taking both transmission and reflection feature flows (\(_{}^{}\) and \(_{}^{}\), respectively) as inputs. After that, a layer normalization and two parallel attention mechanisms, namely dual-stream self-attention (DS-SA) and dual-stream cross-attention (DS-CA) are applied to the feature flows, capturing both inter- and intra-layer correlations. Subsequently, we derive the output feature flows \(_{}^{}\) and \(_{}^{}\) after passing the features through a layer normalization and a feed-forward network in the form of the dual-stream locality-preserving

Figure 2: (a) The overall architecture of our proposed Dual-Stream Interactive Transformer, which consists of a dual-architecture interactive encoder and a dual-stream interactive decoder, injecting the global prior into local features and aggregating them in dual-stream from bottom to up. (b) A visual illustration of our proposed dual-attention interactive block, which provides both intra-layer self-attention and inter-layer cross-attention, capturing holistic feature correlations.

block (DSLP Block). The detailed computation procedure is displayed in Alg. 1. We provide a detailed explanation of the dual attention mechanism in the remainder of this subsection.

**Efficient Dual-Stream Cross-Attention Mechanism**. We present a simple yet effective cross-attention mechanism for dual-stream Transformer models via an extension of the self-attention mechanism. Given the feature streams of transmission layer \(}^{N C}\) and reflection layer \(}^{N C}\), we concatenate them along the token dimension to form the input matrix \(_{}^{2N C}=} \\ }.\) We then compute the query \(_{}\), key \(_{}\), and value \(_{}\) matrices for cross-attention by applying the linear transformations:

\[_{}=_{}_{q},_{ }=_{}_{k},_{}= _{}_{v},\] (2)

where \(_{q},_{k},_{v}^{C D}\) denote the weight matrices that project the input features from \(C\)-dimensional channels into \(D\)-dimensional hidden representations. The cross-attention score matrix \(_{}^{2N 2N}\) are computed as:

\[_{}^{2N 2N}& =(_{}_{}^{ })=(}\\ }_{q}_{k}^{} _{T}^{}&_{R}^{})\\ &=(_{T}^{}_{q}_ {k}^{}_{T}^{}&_{T}^{}_{q}_ {k}^{}_{R}^{}),\] (3)

where the intra-layer terms \(}_{q}_{k}^{}_{T}^{}\) and \(}_{q}_{k}^{}_{R}^{}\) represent interactions within the transmission stream \(}\) and the reflection stream \(}\), respectively. The inter-layer terms \(}_{q}_{k}^{}_{R}^{}\) and \(}_{q}_{k}^{}_{T}^{}\) indicate interactions between \(}\) and \(}\). By denoting the Softmax function with a scaling factor \(}\) as \(()\), the output matrix \(_{}\) is then calculated as:

\[_{}=_{} _{}=(}_{q }_{k}^{}_{T}^{})}_{v}+ (}_{q}_{k}^{}_{R}^{ })}_{v}\\ (}_{q}_{k}^{}_{T}^{ })}_{v}+(}_{q} _{k}^{}_{R}^{})}_{v}. \] (4)

We further simplify the form of \(_{}\) by introducing \((_{1},_{2})=(_{1}_{q}_{k}^{}_{2}^{})_{2}_{v}\), where \(_{1}^{N C}\) and \(_{2}^{N C}\) can be chosen between \(}\) and \(}\), yielding the follows:

\[_{}=(},})+(},})\\ (},})+(},})=_{T}^{}\\ _{R}^{}.\] (5)

We finally obtain the output of the dual-stream cross-attention as \(^{}}=(},})+ (},})\) and \(^{}}=(},})+ (},})\), which are the combined effects of intra-layer and inter-layer interactions. Meanwhile, if we concatenate the dual-stream features along the batch dimension, obtaining the input matrix for the dual-stream self-attention mechanism \(_{}^{2 N C}\), we can further boost the parallelism of our model.

**Dual-Attention Design**. Based on the above analysis, we can define the following dual-attention mechanism:

\[_{}=(_{}, _{},_{})=(_{ }_{}^{}/+_{}) _{},\\ _{}=(_{},_{},_{})=(_{}_{}^{ }/+_{})_{},\] (6)

where \(_{}\), \(_{}\), and \(_{}\) are derived as in the DS-CA. Note that, the number of tokens is doubled in DS-CA compared to DS-SA. To reduce the computation burden, we employ a window-based attention mechanism for our dual-attention design. In this way, \(_{},_{},_{}^{ 2N_{T} N_{W} D}\), \(_{},_{},_{}^ {N_{T} 2N_{W} D}\), where \(N_{T}\) denotes the total number of windows, \(N_{W}\) stands for the window size. \(_{}^{N_{W} N_{W}}\) represents the relative position bias , which provides the same bias values with respect to the same distance between two tokens in a window. It is obtained by indexing a learnable lookup table \(_{}^{}^{(2}-1)(2}-1)}\) through the predefined relative indexes \(_{}^{N_{W} N_{W}}\). Each item of \(_{}\) is a mapped distance of two locations: \(u_{ij}=t(p_{i}-p_{j})\). \(p\) is a 2-D point in a window, the coordinates of which fall between \(0\) and \(}-1\), and each distinct \(p_{i}-p_{j}\) is mapped into a single index by \(t()\). For DS-CA, we propose the Layered Relative Position Biases (LRPB), \(_{}^{2N_{W} 2N_{W}}\), which are indexed from the extended lookup table \(_{}^{}^{(2-1})(2 -1}) 3}\) by the layered relative indexes \(_{}^{2N_{W} 2N_{W}}\). Each element is mapped by subtracting two 3-D points \(u^{}_{ij}=t^{}(v_{i}-v_{j})\). \(v\) is a 3-D point in a layered window, with an additional dimension representing to which layer the token belongs, and \(t^{}()\) maps 3-D locations into single indexes.

**Dual-Stream Locality-Preserving Block**. Since reflection separation is a dense prediction task, a primary consideration during architecture design is to maintain the local information. Therefore, we introduce the DSLP Block in our DSIT structure, which can be _any convolutional dual-stream_ network modules. To focus on the enhanced interaction capabilities achieved by our dual-attention design, we avoid introducing additional novel local modules, opting instead to employ the MuGI Block  as the implementation of the DSLP Block. This approach isolates the performance gains attributed solely to the dual-attention mechanism, as evidenced in comparisons with models like DSRNet. One could, of course, substitute our design with alternative specialized dual-stream modules, potentially achieving even better model performance.

### Dual-Architecture Interactive Encoder

As depicted in Fig. 2 (a), our proposed DAIE integrates both global and local prior extractors. The single-stream global features modulate the dual-stream local features hierarchically through cross-architecture interactions (CAI), which are implemented with our proposed dual-attention interactive blocks. Formally, we have \((^{},_{}^{})\) and \((^{},_{}^{})\), where \(^{}\) and \(_{}^{}\), \(_{}^{}\) represent global and local features respectively. \(_{}^{}\) denotes the transmission information flow and \(_{}^{}\) signifies the reflection stream. In an effort to provide an intuitive understanding of our DAIE design, we illustrate the feature visualization of DSIT in Fig. 3. As shown, the dual-stream local priors focus on different components of the inputs but lack precise layer-specific attention. After being modulated by the global priors \(^{}\) via our proposed CAI and aggregated with the lower stream, we obtain \(_{}^{}\) and \(_{}^{}\), which are significantly separated. Furthermore, the modulated dual-stream features are fed into a group of DAIBs, resulting in feature separations \(_{}^{},_{}^{}\) of higher quality.

### Loss Function

**Pixel reconstruction loss.** To compel the consistency of the restored layers \(}\) and \(}\) in the spatial domain with their ground-truth scenes **T** and **R** and the layer superimposition modeling, we introduce the following loss function:

\[_{pix}:=\|}-\|_{2}^{2}+\|}- \|_{2}^{2}+\|-(}+})- (},})\|_{1},\] (7)

where \(\) denotes a learnable term to constitute the reflection superposition. \(\|\|_{2}\) and \(\|\|_{1}\) represent the \(_{2}\) and \(_{1}\) norms, respectively. \(\) is a hyperparameter to balance the intra-layer and inter-layer fidelity. By enforcing the reconstruction loss with a learnable residual term, the restored layers appear to be cleaner and completed.

Figure 3: Visualization of extracted local priors, global priors, their cross-architecture-interacted dual-stream features and features after the DAIBs of two reflection-superimposed inputs. All the above features are from the second level of our DSIT model and are channel-wise averaged to display.

**Gradient reconstruction loss.** Considering the gradient independence, as a traditional prior in blind-source decomposition, we simultaneously encourage the models to restore the ground-truth gradient and penalize the intersected gradient as follows:

\[_{grad}:=&\|}-\|_{1}+\|}-\|_{1}+ _{n=0}^{N-1}\|(}^{ n},}^{ n})\|_{2}^{2},\\ (},}):=& (_{1}|}|)(_{2}| {}|),\] (8)

where \(\) denotes the difference operator of images. \(}^{ n},}^{ n}\) are \(2^{n}\) down-sampled version of \(}\) and \(}\). \(_{1}\) and \(_{2}\) are normalization factors. The exclusion term, introduced by , ensures the multi-scale exclusion of the two layers in the gradient domain.

**Feature reconstruction Loss.** To promote the perceived quality of decoupled layers, we harness the following feature reconstruction loss:

\[_{fea}:=_{i}_{i}\|_{i}(})-_{i}( )\|_{1},\] (9)

where \(_{i}()\) represents the intermediate feature of the pre-trained VGG-19 model, where \(i\{2,7,12,21,30\}\) tells the layer id. \(_{i}\) balance the weights of hierarchies.

**Total Loss.** The full training objectives \(_{total}\) is defined as follows:

\[_{total}:=_{1}_{pix}+_{2}_{ grad}+_{3}_{fea},\] (10)

where \(_{1}=1\), \(_{2}=1\), and \(_{3}=0.01\) are coefficients for balancing different loss terms.

## 4 Experimental Validation

### Implementation Details

**Datasets**. Our training datasets include both synthetic and real-world images. Following , we design two data settings for fair comparison: I. 7,643 synthesized pairs randomly sampled from the

    &  &  &  &  &  \\   & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\  Zhang _et al._ & 22.55 & 0.788 & 22.68 & 0.879 & 16.81 & 0.797 & 21.52 & 0.832 & 20.08 & 0.835 \\ BDN  & 18.41 & 0.726 & 22.72 & 0.856 & 20.71 & 0.859 & 22.36 & 0.830 & 21.65 & 0.849 \\ ERRNet  & 22.89 & 0.803 & 24.87 & 0.896 & 22.04 & 0.876 & 24.25 & 0.853 & 23.53 & 0.879 \\ IBCLN  & 21.86 & 0.762 & 24.87 & 0.893 & 23.39 & 0.875 & 24.71 & 0.886 & 24.10 & 0.879 \\ RAGNet  & 22.95 & 0.793 & 26.15 & 0.903 & 23.67 & 0.879 & 25.53 & 0.880 & 24.90 & 0.886 \\ DMGN  & 20.71 & 0.770 & 24.98 & 0.899 & 22.92 & 0.877 & 23.81 & 0.835 & 23.80 & 0.877 \\ Zheng _et al._ & 20.17 & 0.755 & 25.20 & 0.880 & 23.26 & 0.905 & 25.39 & 0.878 & 24.19 & 0.885 \\ YTMT  & 23.26 & 0.806 & 24.87 & 0.896 & 22.91 & 0.884 & 25.48 & 0.890 & 24.05 & 0.886 \\ RobustSIRR  & 23.30 & 0.827 & 24.90 & 0.917 & 19.91 & 0.868 & 23.67 & 0.884 & 22.59 & 0.889 \\ DSRNet  & 24.23 & 0.820 & 26.28 & 0.914 & 24.56 & 0.908 & 25.68 & 0.896 & 25.40 & 0.905 \\ PromptBR  & 24.11 & 0.813 & 24.17 & 0.859 & 23.03 & 0.895 & 26.43 & **0.930** & 23.95 & 0.880 \\ Ours & **25.06** & **0.836** & **26.81** & **0.919** & **25.63** & **0.924** & **27.06** & 0.910 & **26.27** & **0.917** \\  Dong _et al._ & 23.34 & 0.812 & 24.36 & 0.898 & 23.72 & 0.903 & 25.73 & 0.902 & 24.21 & 0.897 \\ DSRNet\({}^{}\) & 23.91 & 0.818 & 26.74 & 0.920 & 24.83 & 0.911 & 26.11 & 0.906 & 25.75 & 0.910 \\ RRW\({}^{}\) & 21.83 & 0.801 & 26.67 & 0.931 & 24.04 & 0.903 & 26.49 & 0.915 & 25.34 & 0.912 \\ Zhong _et al._ & 24.05 & 0.824 & 26.51 & 0.927 & 25.02 & 0.915 & 26.23 & **0.925** & 25.75 & 0.917 \\ Ours\({}^{}\) & **25.22** & **0.836** & **27.27** & **0.932** & **25.58** & **0.922** & **27.40** & 0.918 & **26.49** & **0.922** \\   

Table 1: Quantitative results on four real-world testing datasets of SIRS models. The best results are displayed in **bold**, while the second-best are underlined. \(\) means data setting II is employed to train the model. * represents additional prompts are introduced. \(\) reflects extra data pairs are involved.

   Metrics & ERRNet-F & IBCLN & YTMT & Dong _et al._ & DSRNet & RRW & Zhong _et al._ & Ours \\  PSNR & 22.18 & 23.57 & 23.85 & 23.45 & 25.22 & 26.04 & 23.87 & **26.77** \\ SSIM & 0.756 & 0.783 & 0.810 & 0.808 & 0.832 & 0.846 & 0.812 & **0.847** \\   

Table 2: Quantitative results on the “Nature” testings set SIRS methods trained under data setting II. The best results are shown in **bold**, and the second-best are underlined.

PASCAL VOC dataset  in each epoch and 90 real pairs from . II. 200 extra real pairs from the "Nature" dataset , and 13,700 synthesized pairs sampled from  instead. The training image size is fixed as \(384 384\). The window size of attention mechanisms, \(N_{W}\), is fixed to \(12 12\), and the number of windows, \(N_{T}\), varies depending on the spatial scale of the features.

**Training Strategy**. Our models are all implemented via the PyTorch framework and optimized with Adam optimizer for 20 or 80 epochs based on different data settings. The learning rate is fixed as \(10^{-4}\) with a batch size of 1 on a single RTX 3090 GPU. Given real-world data pairs are hard to acquire, we additionally propose a data augmentation operation Reflection Mixup (RefMix) for the training real pairs, formulated as \(_{}=+(1-)\), where \(\) is uniformly sampled.

### Performance Evaluation

**Quantitative comparison**. As shown in Tabs. 1 and 2, we make a comparison between ours and state-of-the-art methods on five real-world datasets, including Real20 , Nature20  and three subsets of the SIR\({}^{2}\) Dataset . It is noteworthy that our models trained on both data settings show superior performance over the alternatives on most testing sets, including those that involve extra real-world data  and language prompts . The superiority is attributed not only to the improved generalizability afforded by the hybrid Transformer architecture but also to the dual-attention interactive design that directly assesses intra-layer and inter-layer correlations, which shows impressive efficiency on SIRS tasks and has a high potential for other decomposition tasks.

**Qualitative comparison**. To evaluate our proposed model aesthetically, we first present a visual comparison of estimated transmission layers in Fig. 4. The two superimposed input images are

Figure 4: Visual comparison of transmission layer predictions between previous state-of-the-arts and ours on samples from Real20  and SIR\({}^{2}\) datasets. Please note the areas in the boxes.

sampled from Real20 and SIR\({}^{2}\) datasets, respectively. The two cases are representative since the first case is captured outdoors and contains both specular and weak reflections, while the second one is taken indoors with a relatively uniform reflection layer, which is highly entangled with the transmission structure. As can be seen, for the first case, ERRNet, YTMT, and Dong _et al._ cannot recognize the reflection regions successfully. Zhang _et al._ removes the reflections at the cost of introducing color bias and artifacts. IBCLN, DSRNet, and RRW separate either the specular or the weak reflection parts, lacking the ability to correlate the reflection components of different intensities. As for the second one, most alternatives fail to separate the majority of the reflection layer. Although Dong _et al._ shows an improvement over previous methods, it still leaves blurry reflection components in its result transmission layer. With a better layer modeling capability, our models conquer such a problem, providing strikingly clearer reconstructions.

Additionally, we specifically captured several in-the-wild test cases, as illustrated in Fig. 5. Unlike standard benchmarks that often incorporate artificial glass plates, these cases utilize real-world reflective surfaces, including reflections from accumulated water--conditions entirely absent from the training set. The superior performance of our approach, in comparison to previous state-of-the-art methods, underscores its robust generalization capacity and practical applicability. We further compare the reflection predictions between dual-stream reflection separation models in Fig. 6. Notably, our DSIT model yields significantly more plausible results, exhibiting superior content fidelity and color accuracy, attributed to the enhanced information selection capabilities of our design.

### Ablation Study

**Model selection for GPE**. Generally speaking, the design of our DAIE allows different choices of global prior extractors, which aim to provide semantic priors and/or non-local information. To elaborate on the efficacy of different GPE models, we compare the settings of using ResNet101 , FocalNet-L , PVTv2-b4 , and Swin-L  models, which are all pre-trained on image classification tasks and finetuned on object detection tasks. As shown in Tab. 3, the ResNet101 as a CNN backbone provides limited global priors and poorly generalizes to the Real20 dataset. PVTv2 comprises few inductive biases, thus exhibiting inferior performance to the FocalNet and Swin Transformer, which preserve local dependency.

**Design of CAI operation**. Our cross-architecture interaction mechanism is designed to exploit useful information from the priors extracted by GPE and LPE, and thus, various feature fusion operations can be taken. Here, we build a baseline "Add" that simply sums the information flow provided by the different prior extractors. Further, we propose two variants based on Cross Attention design ("CrossAttn") and DAIB ("DAIB") mechanisms, respectively. As Tab. 3 shown, cross-attention is not

Figure 5: Visual comparison of transmission predictions between previous state-of-the-arts and ours in real-world scenarios additionally captured in this paper. The broad advantages demonstrated by our method across these diverse conditions highlight its superior generalization capability.

capable of fusion cross-architecture information, leading to worse performance than simple addition, while our proposed DAIB bridges the local and global priors, obtaining superior results.

**Design of DAIB module**. To demonstrate the effectiveness of our DAIB design, we constructed three baseline models. The first baseline, "MLP FFN", replaces our DSLP Block with a standard MLP module. The second, "w/o DS-CA", omits the DS-CA mechanism, and the third, "w/o DS-SA", removes the DS-SA mechanism. As shown in Tab. 3, model performance degrades when substituting DSLP Block with MLP, primarily due to the reduced inductive biases and feature interactions. Moreover, the removal of either DS-CA or DS-SA results in inferior performance, particularly for DS-CA. This highlights the critical role of our proposed dual-attention interactive mechanism.

**Design of LRPB mechanism**. LRPB provides an initial bias to each attention point according to the spatial and layer locations, injecting a layer-aware prior into the attention mechanism. Through the ablation study of whether or not to equip with LRPB shown in Tab. 3 "LRPB", we show its merit in handling the layer decomposition problem.

**The usage of RefMix**. To evaluate the effectiveness of our proposed RefMix, a comparison of employing it or not is made in Tab. 3 "RefMix", showing that it aids the reflection separation.

Due to the page limitation, more visual analyses are organized in the appendix.

## 5 Concluding Remarks

In this study, a dual-stream interactive Transformer has been designed to address the challenge of single image reflection separation. To harness high-quality priors from pre-trained Transformer models, we developed a dual-architecture interactive encoder, which can effectively fuse multi-source information with adaptive emphases. Additionally, we introduced a novel dual-attention interactive block that utilizes both an effective dual-stream self-attention mechanism and a layer-aware dual-stream cross-attention module to separate the entangled features. Comprehensive experiments together with ablations have been conducted to verify the advances of our method. Looking forward, several interesting points deserve future exploration. For instance, larger vision foundation models can provide more substantial priors for low-level vision tasks. A relative position bias design tailored for a specific vision task will be also beneficial. Furthermore, our RefMix technique is likely to extend its benefits to other data-hungry visual tasks. These considerations may inspire future research on low-level Transformer designs.

    &  &  & ^{2}\) (454)} \\   & & PSNR & SSIM & PSNR & SSIM \\   & ResNet101 & 21.59 & 0.777 & 24.42 & 0.886 \\  & PVTV2-b4 & 23.16 & 0.793 & 24.44 & 0.890 \\  & FocalNet-L & 24.13 & 0.824 & 25.52 & 0.910 \\  & Swin-L & **25.06** & **0.836** & **26.32** & **0.920** \\   & Add & 24.62 & 0.825 & 25.79 & 0.917 \\  & CrossAttn & 24.50 & 0.819 & 24.76 & 0.896 \\  & DAIB & **25.06** & **0.836** & **26.32** & **0.920** \\   & MLP FFN & 23.65 & 0.817 & 25.38 & 0.909 \\  & w/o DS-CA & 24.48 & 0.823 & 25.89 & 0.919 \\  & w/o DS-SA & 24.47 & 0.827 & 26.12 & 0.920 \\  & DAIB & **25.06** & **0.836** & **26.32** & **0.920** \\   & w/o LRPB & 24.93 & 0.821 & 25.99 & 0.917 \\  & w/ LRPB & **25.06** & **0.836** & **26.32** & **0.920** \\   & w/o RefMix & 24.72 & 0.823 & 26.27 & 0.915 \\  & w/ RefMix & **25.06** & **0.836** & **26.32** & **0.920** \\   

Table 3: Ablation study on different factors of our design.

Figure 6: Visual comparison of reflection layer predictions between previous state-of-the-arts and ours on the \(^{2}\) dataset. Our method shows significant superiority over previous dual-stream arts.