# Computing Optimal Equilibria and Mechanisms via Learning in Zero-Sum Extensive-Form Games

Brian Hu Zhang

Carnegie Mellon University

bhzhang@cs.cmu.edu

&Gabriele Farina

MIT

gfarina@mit.edu

&Ioannis Anagnostides

Carnegie Mellon University

ianagnos@cs.cmu.edu

&Federico Cacciamani

DEIB, Politecnico di Milano

federico.cacciamani@polimi.it

&Stephen McAleer

Carnegie Mellon University

smcaleer@cs.cmu.edu

&Andreas Haupt

MIT

haupt@mit.edu

&Andrea Celli

Bocconi University

andrea.celli2@unibocconi.it

&Nicola Gatti

DEIB, Politecnico di Milano

nicola.gatti@polimi.it

&Vincent Conitzer

Carnegie Mellon University

conitzer@cs.cmu.edu

&Tuomas Sandholm

Carnegie Mellon University

Strategic Machine, Inc.

Strategy Robot, Inc.

Optimized Markets, Inc.

sandholm@cs.cmu.edu

Equal contribution.

###### Abstract

We introduce a new approach for _computing_ optimal equilibria and mechanisms via learning in games. It applies to extensive-form settings with any number of players, including mechanism design, information design, and solution concepts such as correlated, communication, and certification equilibria. We observe that _optimal_ equilibria are minimax equilibrium strategies of a player in an extensive-form zero-sum game. This reformulation allows us to apply techniques for learning in zero-sum games, yielding the first learning dynamics that converge to optimal equilibria, not only in empirical averages, but also in iterates. We demonstrate the practical scalability and flexibility of our approach by attaining state-of-the-art performance in benchmark tabular games, and by computing an optimal mechanism for a sequential auction design problem using deep reinforcement learning.

## 1 Introduction

What does it mean to _solve_ a game? This is one of the central questions addressed in game theory, leading to a variety of different solution concepts. Perhaps first and foremost, there are various notions of _equilibrium_, strategically stable points from which no rational individual would be inclined to deviate. But is it enough to compute, or indeed _learn_, just any one equilibrium of a game? In two-player zero-sum games, one can make a convincing argument that a single equilibrium in fact constitutes a complete solution to the game, based on the celebrated minimax theorem of von Neumann . Indeed, approaches based on computing minimax equilibria intwo-player zero-sum games have enjoyed a remarkable success in solving major AI challenges, exemplified by the recent development of superhuman poker AI agents .

However, in general-sum games it becomes harder to argue that _any_ equilibrium constitutes a complete solution. Indeed, one equilibrium can offer vastly different payoffs to the players than another. Further, if a player acts according to one equilibrium and another player according to a different one, the result may not be an equilibrium at all, resulting in a true _equilibrium selection problem_. In this paper, therefore, we focus on computing an _optimal_ equilibrium, that is, one that maximizes a given linear objective within the space of equilibria. There are various advantages to this approach. First, in many contexts, we would simply prefer to have an equilibrium that maximizes, say, the sum of the players' utilities--and by computing such an equilibrium we also automatically avoid Pareto-dominated equilibria. Second, it can mitigate the equilibrium selection problem: if there is a convention that we always pursue an equilibrium that maximizes social welfare, this reduces the risk that players end up playing according to different equilibria. Third, if one has little control over how the game will be played but cares about its outcomes, one may like to understand the space of all equilibria. In general, a complete picture of this space can be elusive, in part because a game can have exponentially many equilibria; but computing extreme equilibria in many directions--say, one that maximizes Player 1's utility--can provide meaningful information about the space of equilibria.

That being said, many techniques that have been successful at computing a single equilibrium do not lend themselves well to computing optimal equilibria. Most notably, while _no-regret_ learning dynamics are known to converge to different notions of _correlated equilibria_, little is known about the properties of the equilibrium reached. In this paper, therefore, we introduce a new paradigm of learning in games for _computing_ optimal equilibria. It applies to extensive-form settings with any number of players, including information design, and solution concepts such as correlated, communication, and certification equilibria. Further, our framework is general enough to also capture optimal mechanism design and optimal incentive design problems in sequential settings.

Summary of Our ResultsA key insight that underpins our results is that computing _optimal_ equilibria in multi-player extensive-form games can be cast via a Lagrangian relaxation as a two-player zero-sum extensive-form game. This unlocks a rich technology, both theoretical and experimental, developed for computing minimax equilibria for the more challenging--and much less understood--problem of computing optimal equilibria. In particular, building on the framework of Zhang and Sandholm , our reduction lends itself to mechanism design and information design, as well as an entire hierarchy of equilibrium concepts, including _normal-form coarse correlated equilibria (NFCCE)_, _extensive-form coarse correlated equilibria (EFCCE)_, _extensive-form correlated equilibria (EFCE)_, _communication equilibria (COMM)_, and _certification equilibria (CERT)_. In fact, for communication and certification equilibria, our framework leads to the first learning-based algorithms for computing them, addressing a question left open by Zhang and Sandholm  (_cf._, discussed in Appendix B).

We thus focus on computing an optimal equilibrium by employing regret minimization techniques in order to solve the induced bilinear saddle-point problem. Such considerations are motivated in part by the remarkable success of no-regret algorithms for computing minimax equilibria in large two-player zero-sum games (_e.g._, see ), which we endeavor to transfer to the problem of computing optimal equilibria in multi-player games.

In this context, we show that employing standard regret minimizers, such as online mirror descent  or counterfactual regret minimization , leads to a rate of convergence of \(T^{-1/4}\) to optimal equilibria by appropriately tuning the magnitude of the Lagrange multipliers (Corollary 3.3). We also leverage the technique of _optimism_, pioneered by Chiang et al. , Rakhlin and Sridharan  and Syrgkanis et al. , to obtain an accelerated \(T^{-1/2}\) rate of convergence (Corollary 3.4). These are the first learning dynamics that (provably) converge to optimal equilibria. Our bilinear formulation also allows us to obtain _last-iterate_ convergence to optimal equilibria via optimistic gradient descent/ascent (Theorem 3.5), instead of the time-average guarantees traditionally derived within the no-regret framework. As such, we bypass known barriers in the traditional learning paradigm by incorporating an additional player, a _mediator_, into the learning process. Furthermore, we also study an alternative Lagrangian relaxation which, unlike our earlier approach, consists of solving a sequence of zero-sum games (_cf._). While the latter approach is less natural, we find that it is preferable when used in conjunction with deep RL solvers since it obviates the need for solving games with large reward ranges--a byproduct of employing the natural Lagrangian relaxation.

Experimental resultsWe demonstrate the practical scalability of our approach for computing optimal equilibria and mechanisms. First, we obtain state-of-the-art performance in a suite of \(23\) different benchmark game instances for seven different equilibrium concepts. Our algorithm significantly outperforms existing LP-based methods, typically by more than one order of magnitude. We also use our algorithm to derive an optimal mechanism for a sequential auction design problem, and we demonstrate that our approach is naturally amenable to modern deep RL techniques.

### Related work

In this subsection, we highlight prior research that closely relates to our work. Additional related work is included in Appendix B.

A key reference point is the recent paper of Zhang and Sandholm , which presented a unifying framework that enables the computation via linear programming of various mediator-based equilibrium concepts in extensive-form games, including NFCCE, EFCCE, EFCE, COMM, and CERT.2 Perhaps surprisingly, Zhang et al.  demonstrated that computing optimal communication and certification equilibria is possible in time polynomial in the description of the game, establishing a stark dichotomy between the other equilibrium concepts--namely, NFCCE, EFCE, and EFCCE--for which the corresponding problem is NP-hard . In particular, for the latter notions intractability turns out to be driven by the imperfect recall of the mediator . Although imperfect recall induces a computationally hard problem in general from the side of the mediator , positive parameterized results have been documented recently in the literature .

Our work significantly departs from the framework of Zhang and Sandholm  in that we follow a learning-based approach, which has proven to be a particularly favorable avenue in practice; _e.g._, we refer to  for such approaches in the context of computing EFCE. Further, beyond the tabular setting, learning-based frameworks are amenable to modern deep reinforcement learning methods (see , and references therein). Most of those techniques have been developed to solve two-player zero-sum games, which provides another crucial motivation for our main reduction. We demonstrate this experimentally in large games in Section 4. For multi-player games, Marris et al.  developed a scalable algorithm based on _policy space response oracles (PSRO)_ (a deep-reinforcement-learning-based double-oracle technique) that converges to NFC(C)E, but it does not find an optimal equilibrium.

Our research also relates to computational approaches to static auction and mechanism design through deep learning . In particular, similarly to the present paper, Dutting et al.  study a Lagrangian relaxation of mechanism design problems. Our approach is significantly more general in that we cover both static and _sequential_ auctions, as well as general extensive-form games. Further, as a follow-up, Rahme et al.  frame the Lagrangian relaxation as a two-player game, which, however, is not zero-sum, thereby not enabling leveraging the tools known for solving zero-sum games. Finally, in a companion paper , we show how the framework developed in this work can be used to _steer_ no-regret learners to optimal equilibria via nonnegative vanishing payments.

## 2 Preliminaries

We adopt the general framework of _mediator-augmented games_ of Zhang and Sandholm  to define our class of instances. At a high level, a mediator-augmented game explicitly incorporates an additional player, the _mediator_, who can exchange messages with the players and issue action recommendations; different assumptions on the power of the mediator and the players' strategy sets induce different equilibrium concepts, as we clarify for completeness in Appendix A.

**Definition 2.1**.: A _mediator-augmented, extensive-form game_\(\) has the following components:

1. a set of players, identified with the set of integers \( n:=\{1,,n\}\). We will use \(-i\), for \(i n\), to denote all players except \(i\);
2. a directed tree \(H\) of _histories_ or _nodes_, whose root is denoted \(\). The edges of \(H\) are labeled with _actions_. The set of actions legal at \(h\) is denoted \(A_{h}\). Leaf nodes of \(H\) are called _terminal_, and the set of such leaves is denoted by \(Z\);3. a partition \(H Z=H_{} H_{0} H_{1} H_{n}\), where \(H_{i}\) is the set of nodes at which \(i\) takes an action, and **C** and \(0\) denote chance and the mediator, respectively;
4. for each agent3\(i[\![n]\!]\{0\}\), a partition \(_{i}\) of \(i\)'s decision nodes \(H_{i}\) into _information sets_. Every node in a given information set \(I\) must have the same set of legal actions, denoted by \(A_{I}\); 3. for each agent \(i\), a _utility function_\(u_{i}:Z\); and 5. for each chance node \(h H_{}\), a fixed probability distribution \(c(\,|h)\) over \(A_{h}\).

To further clarify this definition, in Appendix A we provide two concrete illustrative examples: a single-item auction and a welfare-optimal correlated equilibrium in normal-form games.

At a node \(h H\), the _sequence_\(_{i}(h)\) of an agent \(i\) is the set of all information sets encountered by agent \(i\), and the actions played at such information sets, along the \( h\) path, excluding at \(h\) itself. An agent has _perfect recall_ if \(_{i}(h)=_{i}(h^{})\) for all \(h,h^{}\) in the same infoset. We will use \(_{i}:=\{_{i}(z):z Z\}\) to denote the set of all sequences of player \(i\) that correspond to terminal nodes. We will assume that all _players_ have perfect recall, though the _mediator_ may not.4

A _pure strategy_ of agent \(i\) is a choice of one action in \(A_{I}\) for each information set \(I_{i}\). The _sequence form_ of a pure strategy is the vector \(_{i}\{0,1\}^{_{i}}\) given by \(_{i}[]=1\) if and only if \(i\) plays every action on the path from the root to sequence \(_{i}\). We will use the shorthand \(_{i}[z]=_{i}[_{i}(z)]\). A _mixed strategy_ is a distribution over pure strategies, and the sequence form of a mixed strategy is the corresponding convex combination \(_{i}^{_{i}}\). We will use \(X_{i}\) to denote the polytope of sequence-form mixed strategies of player \(i\), and use \(\) to denote the polytope of sequence-form mixed strategies of the mediator.

For a fixed \(\), we will say that \((,)\) is an _equilibrium_ of \(\) if, for each _player_\(i\), \(_{i}\) is a best response to \((,_{-i})\), that is, \(_{_{i}^{} X_{i}}u_{i}(,_{i}^{},_ {-i}) u_{i}(,_{i},_{-i})\). We do _not_ require that the mediator's strategy \(\) is a best response. As such, the mediator has the power to commit to its strategy. The goal in this paper will generally be to reach an _optimal (Stackelberg) equilibrium_, that is, an equilibrium \((,)\) maximizing the mediator utility \(u_{0}(,)\). We will use \(u_{0}^{*}\) to denote the value for the mediator in an optimal equilibrium.

Revelation principleThe _revelation principle_ allows us, without loss of generality, to restrict our attention to equilibria where each player is playing some fixed pure strategy \(_{i} X_{i}\).

**Definition 2.2**.: The game \(\) satisfies the _revelation principle_ if there exists a _direct_ pure strategy profile \(=(_{1},,_{n})\) for the players such that, for all strategy profiles \((,)\) for all players including the mediator, there exists a mediator strategy \(^{}\) and functions \(f_{i}:X_{i} X_{i}\) for each player \(i\) such that:

1. \(f_{i}(_{i})=_{i}\), and
2. \(u_{j}(^{},_{i}^{},_{-i})=u_{j}(,f_{i}( _{i}^{}),_{-i})\) for all \(_{i}^{} X_{i}\), and _agents_\(j[\![n]\!]\{0\}\).

The function \(f_{i}\) in the definition of the revelation principle can be seen as a _simulator_ for Player \(i\): it tells Player \(i\) that playing \(_{i}^{}\) if other players play \((,_{-i})\) would be equivalent, in terms of all the payoffs to all agents (including the mediator), to playing \(f(_{i}^{})\) if other agents play \((,_{-i})\). It follows immediately from the definition that if \((,)\) is an \(\)-equilibrium, then so is \((^{},)\)--that is, every equilibrium is payoff-equivalent to a direct equilibrium.

The revelation principle applies and covers many cases of interest in economics and game theory. For example, in (single-stage or dynamic) mechanism design, the direct strategy \(_{i}\) of each player is to report all information truthfully, and the revelation principle guarantees that for all non-truthful mechanisms \((,)\) there exists a truthful mechanism \((^{},)\) with the same utilities for all players.5 For correlated equilibrium, the direct strategy \(_{i}\) consists of obeying all (potentially randomized) recommendations that the mediator gives, and the revelation principle states that we can, without loss of generality, consider only correlated equilibria where the signals given to the players are what actions they should play. In both these cases (and indeed in general for the notions we consider in this paper), it is therefore trivial to specify the direct strategies \(\) without any computational overhead. Indeed, we will assume throughout the paper that the direct strategies \(\) are given. Further examples and discussion of this definition can be found in Appendix A.

Although the revelation principle is a very useful characterization of optimal equilibria, as long as we are given \(\), all of the results in this paper actually apply regardless of whether the revelation principle is satisfied: when it fails, our algorithms will simply yield an _optimal direct equilibrium_ which may not be an optimal equilibrium. Under the revelation principle, the problem of computing an optimal equilibrium can be expressed as follows:

\[_{}u_{0}(,)_{_{ i} X_{i}}u_{i}(,_{i},_{-i}) u_{i}(,)\ \  i\!n\!.\]

The objective \(u_{0}(,)\) can be expressed as a linear expression \(^{}\), and \(u_{i}(,_{i},_{-i})-u_{i}(,)\) can be expressed as a bilinear expression \(^{}_{i}_{i}\). Thus, the above program can be rewritten as

\[_{}^{}_{ {x}_{i} X_{i}}^{}_{i}_{i} 0\ \  i \!n\!.\] (G)

Zhang and Sandholm  now proceed by taking the dual linear program of the inner maximization, which suffices to show that (G) can be solved using linear programming.6

Finally, although our main focus in this paper is on games with discrete action sets, it is worth pointing out that some of our results readily apply to continuous games as well using, for example, the discretization approach of Kroer and Sandholm .

## 3 Lagrangian relaxations and a reduction to a zero-sum game

Our approach in this paper relies on Lagrangian relaxations of the linear program (G). In particular, in this section we introduce two different Lagrangian relaxations. The first one (Section 3.1) reduces computing an optimal equilibrium to solving a _single_ zero-sum game. We find that this approach performs exceptionally well in benchmark extensive-form games in the tabular regime, but it may struggle when used in conjunction with deep RL solvers since it increases significantly the range of the rewards. This shortcoming is addressed by our second method, introduced in Section 3.2, which instead solves a _sequence_ of suitable zero-sum games.

### "Direct" Lagrangian

Directly taking a Lagrangian relaxation of the LP (G) gives the following saddle-point problem:

\[_{}_{ R_{  0},\\ _{i} X_{i}:i\!n\!}^{} -_{i=1}^{n}^{}_{i}_{i}.\] (L1)

We first point out that the above saddle-point optimization problem admits a solution \((^{*},^{*},^{*})\):

**Proposition 3.1**.: _The problem (L1) admits a finite saddle-point solution \((^{*},^{*},^{*})\). Moreover, for all fixed \(>^{*}\), the problems (L1) and (G) have the same value and same set of optimal solutions._

The proof is in Appendix C. We will call the smallest possible \(^{*}\) the _critical Lagrange multiplier_.

**Proposition 3.2**.: _For any fixed value \(\), the saddle-point problem (L1) can be expressed as a zero-sum extensive-form game._

Proof.: Consider the zero-sum extensive-form game \(\) between two players, the _mediator_ and the _deviator_, with the following structure:

1. Nature picks, with uniform probability, whether or not there is a deviator. If nature picks that there should be a deviator, then nature samples, also uniformly, a deviator \(i\!n\!\). Nature's actions are revealed to the deviator, but kept private from the mediator.

2. The game \(\) is played. All players, except \(i\) if nature picked a deviator, are constrained to according to \(_{i}\). The deviator plays on behalf of Player \(i\).
3. Upon reaching terminal node \(z\), there are two cases. If nature picked a deviator \(i\), the utility is \(-2 n u_{i}(z)\). If nature did not pick a deviator, the utility is \(2u_{0}(z)+2_{i=1}^{n}u_{i}(z)\).

The mediator's expected utility in this game is

\[u_{0}(,)-_{i=1}^{n}u_{i}(,_{i}, _{-i})-u_{i}(,).\]

This characterization enables us to exploit technology used for extensive-form zero-sum game solving to compute optimal equilibria for an entire hierarchy of equilibrium concepts (Appendix A).

We will next focus on the computational aspects of solving the induced saddle-point problem (L1) using regret minimization techniques. All of the omitted proofs are deferred to Appendices D and E.

The first challenge that arises in the solution of (L1) is that the domain of the minimizing player is unbounded--the Lagrange multiplier is allowed to take any nonnegative value. Nevertheless, we show in Theorem D.1 that it suffices to set the Lagrange multiplier to a fixed value (that may depend on the time horizon); appropriately setting that value will allow us to trade off between the equilibrium gap and the optimality gap. We combine this theorem with standard regret minimizers (such as variants of CFR employed in Section 4.1) to guarantee fast convergence to optimal equilibria.

**Corollary 3.3**.: _There exist regret minimization algorithms such that when employed in the saddle-point problem (L1), the average strategy of the mediator \(}_{t=1}^{T}^{(t)}\) converges to the set of optimal equilibria at a rate of \(T^{-1/4}\). Moreover, the per-iteration complexity is polynomial for communication and certification equilibria (under the nested range condition ), while for NFCCE, EFCCE and EFCE, implementing each iteration admits a fixed-parameter tractable algorithm._

Furthermore, we leverage the technique of _optimism_, pioneered by Chiang et al. , Rakhlin and Sridharan , Syrgkanis et al. , to obtain a faster rate of convergence.

**Corollary 3.4** (Improved rates via optimism).: _There exist regret minimization algorithms that guarantee that the average strategy of the mediator \(}_{t=1}^{T}^{(t)}\) converges to the set of optimal equilibria at a rate of \(T^{-1/2}\). The per-iteration complexity is analogous to Corollary 3.3._

While this rate is slower than the (near) \(T^{-1}\) rates known for converging to some of those equilibria , Corollaries 3.3 and 3.4 additionally guarantee convergence to _optimal_ equilibria; improving the \(T^{-1/2}\) rate of Corollary 3.4 is an interesting direction for future research.

Last-iterate convergenceThe convergence results we have stated thus far apply for the _average_ strategy of the mediator--a typical feature of traditional guarantees in the no-regret framework. Nevertheless, an important advantage of our mediator-augmented formulation is that we can also guarantee _last-iterate convergence_ to optimal equilibria in general games. Indeed, this follows readily from our reduction to two-player zero-sum games, leading to the following guarantee.

**Theorem 3.5** (Last-iterate convergence to optimal equilibria in general games).: _There exist algorithms that guarantee that the last strategy of the mediator \(^{(T)}\) converges to the set of optimal equilibria at a rate of \(T^{-1/4}\). The per-iteration complexity is analogous to Corollaries 3.3 and 3.4._

As such, our mediator-augmented paradigm bypasses known hardness results in the traditional learning paradigm (Proposition D.2) since iterate convergence is no longer tied to Nash equilibria.

### Thresholding and binary search

A significant weakness of the above Lagrangian is that the multiplier \(^{*}\) can be large. This means that, in practice, the zero-sum game that needs to be solved to compute an optimal equilibrium could have a large reward range. While this is not a problem for most tabular methods that can achieve high precision, more scalable methods based on reinforcement learning tend to be unable to solve games to the required precision. In this section, we will introduce another Lagrangian-based method for solving the program (G) that will not require solving games with large reward ranges.

Specifically, let \(\) be a fixed threshold value, and consider the bilinear saddle-point problem

\[_{}_{^{n+1},\\ _{i} X_{i};i n}_{0}( ^{}-)-_{i=1}^{n}_{i}^{}_{i}_{i},\] (L2)

where \(^{k}:=\{_{ 0}^{k}:^{}=1\}\) is the probability simplex on \(k\) items. This Lagrangian was also stated--but not analyzed--by Farina et al. , in the special case of correlated equilibrium concepts (NFCCE, EFCCE, EFCE). Compared to that paper, ours contains a more complete analysis, and is general to more notions of equilibrium.

Like (L1), this Lagrangian is also a zero-sum game, but unlike (L1), the reward range in this Lagrangian is bounded by an absolute constant:

**Proposition 3.6**.: _Let \(\) be a (mediator-augmented) game in which the reward for all agents is bounded in \(\). For any fixed \(\), the saddle-point problem (L2) can be expressed as a zero-sum extensive-form game whose reward is bounded in \([-2,2]\)._

Proof.: Consider the zero-sum extensive-form game \(\) between two players, the _mediator_ and the _deviator_, with the following structure:

1. The deviator picks an index \(i n\{0\}\).
2. If \(i 0\), nature picks whether Player \(i\) can deviate, uniformly at random.
3. The game \(\) is played. All players, except \(i\) if \(i 0\) and nature selected that \(i\) can deviate, are constrained to play according to \(_{i}\). The deviator plays on behalf of Player \(i\).
4. Upon reaching terminal node \(z\), there are three cases. If nature picked \(i=0\), the utility is \(u_{0}(z)-\). Otherwise, if nature picked that Player \(i 0\) can deviate, the utility is \(-2u_{i}(z)\). Finally, if nature picked that Player \(i 0\) cannot deviate, the utility is \(2u_{i}(z)\).

The mediator's expected utility in this game is exactly

\[_{0}u_{0}(,)-_{i=1}^{n}_{i}[u_{i}( ,_{i},_{-i})-u_{i}(,)]\]

where \(^{n+1}\) is the deviator's mixed strategy in the first step. 

The above observations suggest a binary-search-like algorithm for computing optimal equilibria; the pseudocode is given as Algorithm 1. The algorithm solves \(O((1/))\) zero-sum games, each to precision \(\). Let \(v^{*}\) be the optimal value of (G). If \( v^{*}\), the value of (L2) is \(0\), and we will therefore never branch low, in turn implying that \(u v^{*}\) and \( v^{*}-\). As a result, we have proven:

**Theorem 3.7**.: _Algorithm 1 returns an \(\)-approximate equilibrium \(\) whose value to the mediator is at least \(v^{*}-2\). If the underlying game solver used to solve (L2) runs in time \(f(,)\), then Algorithm 1 runs in time \(O(f(,)(1/))\)._

``` input: game \(\) with mediator reward range \(\), target precision \(>0\) \( 0,u 1\) while\(u->\)do \((+u)/2\) run an algorithm to solve game (L2) until either (1) it finds a \(\) achieving value \(-\) in (L2), or (2) it proves that the value of (L2) is \(<0\) ifcase (1) happenedthen\(\) else\(u\) returnthe last \(\) found ```

**ALGORITHM 1**Pseudocode for binary search-based algorithm

The differences between the two Lagrangian formulations can be summarized as follows:1. Using (L1) requires only a single game solve, whereas using (L2) requires \(O((1/))\) game solves.
2. Using (L2) requires only an \(O()\)-approximate game solver to guarantee value \(v^{*}-\), whereas using (L1) would require an \(O(/^{*})\)-approximate game solver to guarantee the same, even assuming that the critical Lagrange multiplier \(^{*}\) in (L1) is known.

Which is preferred will therefore depend on the application. In practice, if the games are too large to be solved using tabular methods, one can use approximate game solvers based on deep reinforcement learning. In this setting, since reinforcement learning tends to be unable to achieve the high precision required to use (L1), using (L2) should generally be preferred. In Section 4, we back up these claims with concrete experiments.

## 4 Experimental evaluation

In this section, we demonstrate the practical scalability and flexibility of our approach, both for computing optimal equilibria in extensive-form games, and for designing optimal mechanisms in large-scale sequential auction design problems.

### Optimal equilibria in extensive-form games

We first extensively evaluate the empirical performance of our two-player zero-sum reduction (Section 3.1) for computing seven equilibrium solution concepts across 23 game instances; the results using the method of Section 3.2 are slightly inferior, and are included in Appendix H. The game instances we use are described in detail in Appendix F, and belong to following eight different classes of established parametric benchmark games, each identified with an alphabetical mnemonic: B - Battleship , D - Liar's dice , GL - Goofspiel , K - Kuhn poker , L - Leduc poker , RS - ridesharing game , S - Sheriff , TP - double dummy bridge game .

For each of the 23 games, we compare the runtime required by the linear programming method of Zhang and Sandholm  ('LP') and the runtime required by our learning dynamics in Section 3.1 ('Ours') for computing \(\)-optimal equilibrium points.

Table 1 shows experimental results for the case in which the threshold \(\) is set to be 1% of the payoff range of the game, and the objective function is set to be the maximum social welfare (sum of player utilities) for general-sum games, and the utility of Player 1 in zero-sum games. Each row corresponds to a game, whose identifier begins with the alphabetical mnemonic of the game class, and whose size in terms of number of nodes in the game trees is reported in the second column. The remaining columns compare, for each solution concept, the runtimes necessary to approximate the optimum equilibrium point according to that solution concept. Due to space constraints, only five out of the seven solution concepts (namely, NFCCE, EFCCE, EFCE, COMM, and CERT) are shown; data for the two remaining concepts (NFCCERT and CCERT) is given in Appendix G.

We remark that in Table 1, the column 'Ours' reports the minimum across the runtime across the different hyperparameters tried for the learning dynamics. Furthermore, for each run of the algorithms, the timeout was set at one hour. More details about the experimental setup are available in Appendix G, together with finer breakdowns of the runtimes.

We observe that our learning-based approach is faster--often by more than an order of magnitude--and more scalable than the linear program. Our additional experiments with different objective functions and values of \(\), available in Appendix G, confirm the finding. This shows the promise of our computational approach, and reinforces the conclusion that _learning dynamics are by far the most scalable technique available today to compute equilibrium points in large games_.

### Exact sequential auction design

Next, we use our approach to derive the optimal mechanism for a sequential auction design problem. In particular, we consider a two-round auction with two bidders, each starting with a budget of \(1\). The valuation for each item for each bidder is sampled uniformly at random from the set \(\{0,}{{4}},}{{2}},}{{4}},1\}\). We consider a mediator-augmented game in which the principal chooses an outcome (allocation and payment for each player) given their reports (bids). We use CFR+  as learning algorithm and a fixed Lagrange multiplier \( 25\) to compute the optimal communication 

[MISSING_PAGE_FAIL:9]

First, to verify that the deep learning method is effective, we replicate the results of the tabular experiments in Section 4.2. We find that PSRO achieves the same best response values and optimal equilibrium value computed by the tabular experiment, up to a small error. These results give us confidence that our method is correct.

Second, to demonstrate scalability, we run our deep learning-based algorithm on a larger auction environment that would be too big to solve with tabular methods. In this environment, there are four rounds, and in each round the valuation of each player is sampled uniformly from \(\{0,0.1,0.2,0.3,0.4,0.5\}\). The starting budget of each player is, again, \(1\). We find that, like the smaller setting, the optimal revenue of the mediator is \( 1.1\) (right-side of Figure 1). This revenue exceeds the revenue of every second-price auction (none of which have revenue greater than \(1\)).8

## 5 Conclusions

We proposed a new paradigm of learning in games. It applies to mechanism design, information design, and solution concepts in multi-player extensive-form games such as correlated, communication, and certification equilibria. Leveraging a Lagrangian relaxation, our paradigm reduces the problem of computing optimal equilibria to determining minimax equilibria in zero-sum extensive-form games. We also demonstrated the scalability of our approach for _computing_ optimal equilibria by attaining state-of-the-art performance in benchmark tabular games, and by solving a sequential auction design problem using deep reinforcement learning.

Figure 1: Exploitability is measured by summing the best response for both bidders to the mechanism. Zero exploitability corresponds to incentive compatibility. In a sequential auction with budgets, our method is able to achieve higher revenue than second-price auctions and better incentive compatibility than a first-price auction.