# Pretraining Codomain Attention Neural Operators

for Solving Multiphysics PDEs

Md Ashiqur Rahman\({}^{1}\), Robert Joseph George\({}^{2}\), Mogab Elleithy\({}^{2}\), Daniel Leibovici\({}^{2}\),

Zongyi Li\({}^{2}\), Boris Bonev\({}^{3}\), Colin White\({}^{2}\), Julius Berner\({}^{2}\), Raymond A. Yeh\({}^{1}\),

Jean Kossaifi\({}^{3}\), Kamyar Azizzadenesheli\({}^{3}\), Anima Anandkumar\({}^{2}\)

\({}^{1}\)Purdue University, \({}^{2}\)Caltech, \({}^{3}\)NVIDIA

###### Abstract

Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs) due to complex geometries, interactions between physical variables, and the limited amounts of high-resolution training data. To address these issues, we propose _Codomain Attention Neural Operator_ (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems. Specifically, we extend positional encoding, self-attention, and normalization layers to function spaces. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings. On complex downstream tasks with limited data, such as fluid flow simulations, fluid-structure interactions, and Rayleigh-Benard convection, we found CoDA-NO to outperform existing methods by over \(36\%\).

## 1 Introduction

Many science and engineering challenges involve solving partial differential equations (PDEs). A PDE can represent physical phenomena such as fluid dynamics, wave propagation, material deformation, etc., but to describe many real-world systems, multiple such PDEs must be coupled together, viz., multi-physics modeling . For instance, in subsurface engineering, equations of flow, thermodynamics, and microchemistry are coupled together ; in materials science, physics at multiple scales are involved in modeling , and in weather forecasting, atmospheric processes involve interactions of wave propagation and fluid dynamics .

Traditionally, numerical methods have been devised to solve PDEs. However, they typically require discretization of PDEs on fine grids to capture the physical phenomena accurately. Consequently, these computational requirements often exceed available memory and computational budgets for real-world applications. Beyond these obstacles present in individual PDE problems, the convergence of numerical solvers in multiphysics systems presents major difficulties arising from intricate interactions among multiple coupled PDEs.

Deep learning techniques have emerged as faster alternatives to numerical solvers for PDEs in many applications. They are typically trained using supervised learning with data obtained from solvers. This becomes a challenge when only limited data is available, especially in the case of multiphysics simulations, which are expensive and challenging for numerical solvers. Instead, obtaining data from simpler simulations where only a subset of the "physics" is incorporated is more convenient and less expensive. In other words, instead of getting data from coupled PDE systems, we can obtain data by solving individual PDEs. While the solutions of the two systems can be very different, they share common features and can benefit from a combined learning framework. _Can we design a systematic curriculum learning scheme for learning multiphysics systems?_More generally, a foundation model trained on different kinds of PDEs can learn representations across multiple domains and then transfer them to new problems. Such foundation models have found immense success in computer vision and natural language processing . Foundation models are first trained in a self-supervised manner on large and often unlabeled datasets. Then they can be efficiently adapted or fine-tuned to a broad range of downstream tasks with minimal to no additional data or training.

Recent works have attempted to train a foundation model for solving PDEs . However, these methods only work on predetermined PDEs with a fixed number of variables, and none of them consider multi-physics PDEs, and they are mostly restricted to uniform grids, limiting their applicability. For example, standard patching-based approaches used in Vision Transformers (ViTs)  often struggle with discontinuities in predicted functions and changing resolutions . Since they are limited to fixed uniform grids, they cannot generalize to resolutions different from the training resolutions.

To handle varying resolutions and grids, _neural operators_ have been introduced as a deep learning framework for learning mappings between function spaces. Neural operators are guaranteed to converge to a unique operator in the limit of increasingly fine discretizations (of the computational domain). This property is known as _discretization convergence_, making them agnostic to the discretization of the input and output functions and suitable for approximating solution operators of PDEs. Neural operators can replace numerical solvers while being significantly faster in several scenarios . While some of the previous PDE foundation models  use neural operators, they still cannot handle multiphysics or coupled PDEs. They also cannot adapt to new variables that are not predetermined at the beginning of training.

**Our Approach:** We propose a novel _transformer_ neural operator architecture with codomain attention (CoDA-NO) layers designed to handle varying combinations of physical phenomena modeled through coupled PDEs. We partition the input function codomain-wise into a set of token functions, each corresponding to distinct physical variables of the PDE. The CoDA-NO model processes this set of functions as input, extending the transformer architecture from a finite-dimensional vector space to an infinite-dimensional function space. This extension is achieved by carefully redesigning positional encodings, the self-attention mechanism, and normalization techniques.

In our architecture, each token is treated as a function, capturing cross-function dynamics through attention mechanisms while maintaining _discretization convergence_. This design empowers the architecture to handle functions discretized on grids of varying resolutions. Specifically, each token function is subjected to the following operations: (i) concatenation with a learned positional embedding, (ii) lifting to a higher-dimensional co-domain, and (iii) functional attention mechanisms to compute interactions. We use Fourier neural operators (FNOs)  rather than traditional multi-layer perceptrons (MLPs) to create the representations for keys, values, and queries, which helps maintain the functional nature of the input data. Details can be found in Sec. 3 and Alg. 1.

CoDA-NO can be applied to varying numbers of input functions (on different geometries) and adapt to novel PDEs with fewer or additional interacting variables, as illustrated in Fig. 1. This allows us to learn multiple PDE systems in one model.

To demonstrate CoDA-NO's generalizability across diverse physical systems, we examine two settings: multiphysics problems and a collection of single-physics problems.

Figure 1: **CoDA-NO adapts seamlessly to new multi-physics systems.** Pre-trained on fluid dynamics data (Navier-Stokes equation with \(u_{x},u_{y}\), and \(p\)) using the masked-reconstruction objective, CoDA-NO easily adapts to multi-physics fluid-solid interaction systems (new \(d_{x}\) and \(d_{y}\) variables) without any architectural changes.

For the multiphysics scenario, we examine two distinct systems. First, we consider a fluid-structure interaction problem  governed by the incompressible Navier-Stokes equation and the Elastic wave equation. The fluid-structure interaction problem is representative of the multi-physics behavior of various real-world problems, e.g., climate and atmosphere modeling. It also provides an additional challenge of irregular meshes on a complex geometry.

Instead of directly learning to solve the full multiphysics problem, we start with a curriculum where we first learn the basic fluid dynamics without the elastic wave equation, governed by the incompressible Navier-Stokes equation, with velocity and pressure as variables. We pre-train CoDA-NO in a self-supervised manner on snapshots of fluid flows by masking different parts of the velocity or pressure fields. Using few-shot supervised fine-tuning, we show that our model can adapt to unseen viscosities and additional displacement fields given by the elastic wave equation. We use graph neural operator (GNO) layers  as encoders and decoders to handle time-varying irregular meshes of the fluid-structure interaction problems. For the few-shot learning problem, our model achieves \(36.8\%\) lower errors on average compared to the best-performing baseline trained from scratch on the target problem.

The second system involves Rayleigh-Benard convection, where the Navier-Stokes and heat (energy) equations are coupled in a regular \(2D\) domain. Similar to the first case, we pre-train CoDA-NO with an incompressible Navier-Stocks equation involving just the velocity term. Then, we fine-tuned the model to predict velocity and temperature using few-shot training samples. Here, too, the pre-trained CoDA-NO significantly outperforms the baseline, reducing the prediction error by a factor of two.

We also train CoDA-NO on a diverse set of PDEs, which form a subset of PDEBench  and demonstrate superior performance and parameter efficiency over prior approaches in learning all of those PDE systems. CoDA-NO consistently outperforms the FNO architecture trained on the same set of PDEs, reducing test error by up to 43% while only requiring 2% of the parameters.

Figure 2: (a) CoDA-NO architecture. (b) Self-supervised pre-training and fine-tuning process with CoDA-NO.

Our contributions are as follows:

* We propose a co-domain attention neural operator that efficiently learns solution operators to PDEs by formulating transformer operations in function space and ensuring discretization convergence.
* The proposed architecture enables self-supervised learning in function space for diverse physical systems by handling varying numbers of input functions and geometries.
* CoDA-NO achieves state-of-the-art performance in generalizing to unknown physical systems with very limited data. That is, CoDA-NO can be viewed as the first foundation neural operator for multiphysics problems.

## 2 Related Works

**Transformers for solving PDEs.** Recent work  proposes a method to weight variables/codomains of the input function based on the weights calculated from the PDE parameters. Another study  proposes a scalable transformer architecture by combining a projection operator to a one-dimensional domain and a learnable factorized kernel. In contrast to these works, CoDA-NO provides a complete attention operator by considering each physical variable as a token function, i.e., an infinite-dimensional vector, extending traditional transformers that only operate on finite-dimensional tokens.

**Self-supervised learning.** Self-supervised learning (SSL) has been proposed to tackle the issue of limited labeled data [22; 23; 24]. It allows the training of large _foundation models_ on massive amounts of unlabeled data in the field of computer vision and natural language processing. Subsequently, these models can be successfully applied to a wide range of downstream tasks with minimal to no additional task-specific data [6; 25; 26; 27].

**Pre-training for PDE solving.** Models that are pre-trained in a self-supervised fashion have also gained traction in the domain of scientific computing. One recent study  proposes pretraining the models with autoregressive tasks on a diverse dataset of multiple PDEs. These models can then be fine-tuned for specific downstream PDEs. Several recent studies have investigated task-agnostic approaches through masking-and-reconstruction  and the consistency of representations under symmetry transformations [16; 28; 29]. Recent work  also sheds light on the transferability of these models between different systems of PDEs. While these methods achieve good performance, the target (downstream) PDE must maintain a strict resemblance to the ones used for pretraining. In addition, adapting these models for PDEs with new additional physical variables is not possible. Additionally, ViT-based patching approaches  disrupt the continuity and are not resolution-agnostic.

## 3 Method

Let us first define our setting and provide a brief introduction to neural operators. For further details, we refer to Sec. A in the appendix.

For an input function \(a^{d_{in}}\), we will denote the \(d_{in}\)-dimensional output space \(^{d_{in}}\) as the _codomain_. We consider the components of the codomain as different physical variables, given by real-valued functions over the input domain \(\), i.e., \(a=[a^{1},,a^{d_{in}}]\) with \(a^{i}:\). The same applies to the output function \(u^{d_{out}}\). We define the action of a _pointwise operator_\(:\{f:^{d_{f}}\}\{g: ^{d_{g}}\}\) given by a function \(h_{}:^{d_{f}}^{d_{g}}\) with parameters \(\) as

\[[f](x)=h_{}(f(x)).\] (1)

Moreover, we define an _integral operator_\(:\{f:^{d_{f}}\}\{g: ^{d_{g}}\}\) given by a kernel function \(k_{}\) with parameters \(\) as

\[[f](x)=_{}k_{}(x,y)f(y)\,y.\] (2)

**Problem Statement.** Our objective is to construct a general neural operator architecture that explicitly represents the interaction between the physical variables of PDE systems. Such an architecture should be able to learn and predict various systems without being constrained to a fixed number of variables.

Let's consider two input functions \(a^{d_{in}}\) and \(^{_{in}}\) of two different PDE with corresponding output functions \(u^{d_{out}}\) and \(^{_{out}}\). In general, the functions \(a\) and \(\)represent \(d_{in}\) and \(_{in}\) physical variables over the domain \(\) with \(d_{in}_{in}\). We aim to design neural operator architectures \(\) that can both be applied to \(a\) as well as \(\) despite the different codomains of the input as well as output functions.

Such property provides the possibility to evaluate or finetune the operator on PDEs with different numbers of variables than those on which it was trained. In particular, when the PDE systems have overlapping physical variables \(\{a^{i}\}_{i=1}^{d_{in}}\{^{i}\}_{i=1}^{_{in}}\), this naturally allows to transfer learned knowledge from one system to the other. We will next describe the details of the CoDA-NO layers and architecture to achieve this goal.

**Neural Operator on Sets.** As we consider the vector-valued input function \(a\) as a set of \(d_{in}\) functions \(\{a^{1},a^{2},,a^{d_{in}}\}\) that represents different physical variables of the PDE, we seek to construct operators that act on _sets_ of input functions with different cardinalities.

For an efficient implementation of operators on sets of functions, we mimic transformer architectures and share weights across different variables. Specifically, we can define the integral operator \(_{per}\) as

\[_{per}[a]=[a^{1}],,[a^{d_{in} }],\] (3)

where \(a=[a^{1},,a^{d_{in}}]\) and \(\) is a regular integral operator as described in Eq.2. Such construction makes the operator _permutation-equivariant_ with respect to the order of the variables in the set. Following the same mechanism, we can also define permutation-equivariant pointwise operator \(_{per}\) with a shared pointwise operator \(\) (see Eq.1). We will use \(_{per}\) and \(_{per}\) to denote permutation-equivariant operators using a shared GNO and FNO, respectively.

**CoDA-NO Layer.** To explain the CoDA-NO layer, let us assume the input function \(a\) has been processed into a latent function \(w:^{d}\). We partition the function into a set of so-called _token functions_\(w^{j}:^{d^{}}\) with \(w^{j}\) for \(j\{1, T\}\) along the codomain, such that \(w=w^{1}, w^{T}\) (and where each \(w^{j}\) is associated with precisely one of the physical input variables). That is, \(w\) represents the codomain-wise concatenation of the token functions \(w^{j}\) and \(d^{}=\). If no other value is specified, we assume that \(d^{}=1\). The CoDA-NO layer now processes the token functions using an extension of the self-attention mechanism to the function space (see Appendix Sec.B and Fig.2).

Let us begin by introducing a single-head CoDA-NO layer. Later, we will expand the concept to multi-head codomain attention. We extend the key, query, and value _matrices_ of the standard attention (see Appendix Sec.B for details) to _operators_ mapping token functions \(w^{j}^{d^{}}\) to key, query, and value functions. We define the key, query, and value operators as

\[:\{k^{j}:^{d_{k}}\},\ \ :\{q^{j}:^{d_{q}}\},\ \ :\{v^{j}:^{d_{v}}\}.\] (4)

Assuming \(d_{k}=d_{q}\), we denote by \(k^{j}=[w^{j}]\), \(q^{j}=[w^{j}]\), and \(v^{j}=[w^{j}]\) the key, query, and value functions of the token functions, respectively.

Next, we calculate the output (token) functions \(o^{j}:^{d_{v}}\) as

\[o^{j}=(,k^{1}}{ }\\ \\ ,k^{T}}{})[v^{1},,v^{T}]^ {},\] (5)

where \(\) is the _temperature_ hyperparameter. Here, \(.,.\) denotes a suitable dot product in the function space. We take the \(L^{2}(,^{d_{k}})\)-dot product given by \( q^{j},k^{m}=_{} q^{j}(x),k^{m}(x) \,x\), where the integral can be discretized using quadrature rules, similar to the integral operator in Eq.2.

To implement multi-head attention, we apply the (single-head) attention mechanism described above separately for multiple heads \(h\{1, H\}\) using \(^{h},^{h},^{h}\) to obtain \(o^{j,h}\). We then concatenate these outputs \(o^{j,h}\) along the codomain and get \(c^{j}:=[o^{j,1}, o^{j,H}]\). Finally, we use an operator

\[:\{c^{j}:^{H d_{v}}\}\{o^{j}: ^{d_{v}}\}\] (6)

to get the output function \(o^{j}\).

We obtain the output of the attention mechanism by concatenating \(o^{j}\)s as \(o=[o^{1},o^{2}, o^{}]\). Finally, we complete the CoDA-NO layer by applying a permutation-equivariant integral operator \(_{per}\) on \(o\). When CoDA-NO is acting on functions sampled on a uniform grid, the internal operators \(^{h},^{h},^{h},\), and \(\) are implemented as FNOs.

**Function Space Normalization.** Normalization is a vital aspect of deep learning architectures. However, when it comes to neural operators mapping infinite-dimensional functions, this topic remains largely unexplored. We now provide a natural extension. Given a function \(w\), let \(w^{j}:^{d^{}}\) be a token. Then we calculate the mean \(^{d^{}}\) and standard deviation \(^{d^{}}\) for this token as

\[^{j}=_{}w^{j}(x)\,x,\ \ \ ^{j}=_{ }(w^{j}(x)-^{j})^{ 2}\,x^{}.\] (7)

Here, \( r\) denotes the elementwise (Hadamard) \(r^{th}\)-power. The normalization operator can be written as

\[[w^{j}](x)=(\!\!^{j})(w^{j}(x)-^{j} )+.\]

Here \(^{d^{}}\) and \(^{d^{}}\) are learnable bias and gain vectors and \(\) and \(\) denote elementwise division and multiplication operation. This normalization can be seen as an extension of _instance normalization_ for function spaces. Similarly, normalization variants, such as _group norm_, _layer norm_, and _batch norm_, extend to operator learning with these definitions of statistics .

**Variable Specific Positional Encoding (VSPE).** We learn positional encoders \(e^{i}:^{d_{cn}}\) for each physical variable \(i\{1,,d_{in}\}\), for the given vector-valued input function \(a=[a^{1},,a^{d_{in}}]\). We concatenate each positional encoding \(e^{i}\) with the respective variable \(a^{i}:\) along the codomain to obtain extended input functions \(^{i}=[a^{i},e^{i}]\). Next, we apply a shared pointwise lifting operator \(:\{^{i}:^{d_{cn}+1}\} \{^{i}:^{D}\},\) typically with \(D>d_{en}+1\). Finally, we concatenate \(^{i}\), \(i\{1, d_{in}\}\), to get the lifted latent function

\[w=[^{1},,^{d_{in}}] ^{D d_{in}}.\] (8)

In the previous paragraphs, we used \(d=D d_{in}\) and, to maintain the permutation-equivariance property of the operator, \(d^{}\) must divide \(D\).

Algorithm 1 presents the pseudocode for the CoDA-NO architecture applied to input functions \([a^{1},a^{2}]\), mapping two different physical variables on a uniform grid in a 1D domain, to the solution functions \([u^{1},u^{2}]\). It assumes \(d^{}=D\) while designing the CoDA-NO layer. Notably, to incorporate another function \(a^{3}\), representing a new physical variable, it is only necessary to introduce a corresponding parameter for the new VSPE, denoted as \(^{3}\).

To effectively handle non-uniform complex geometries, we follow the GINO architecture , where a GNO is used as an encoding and decoding module. Given a set of evaluations of an input function \(a\) on a mesh, as represented by \(\{a(x_{i}^{in})\}_{i=1}^{n}\), where \(\{x_{i}^{in}\}_{i=1}^{n}_{in}\), our first step involves concatenation of each physical variables with respective VSPEs (see Fig. 1(a)).

Next, we use \(_{per}\) to transform the function \(a\) into a new function \(w_{0}\) on a uniform latent grid, represented by \(\{x_{i}^{grid}\}_{i=1}^{n^{}}\). Finally, we apply \(l\) stacked CoDA-NO layers to \(w_{0}\) to obtain the encoded function \(w_{l}\), which acts as a representation of the input function \(a\).

The decoding module is essentially a mirrored version of the encoding module. It starts by applying another block of \(l\) stacked CoDA-NO layers to the encoded function \(w_{l}\) to obtain \(w_{L}\). Subsequently, it uses another \(_{per}\) operator to transform \(w_{L}\) on a uniform grid to an approximation \(u\) of the solution function on an arbitrary output grid \(\{u(x_{i}^{out})\}_{i=1}^{n^{}}\). The architecture is summarized in Fig. 1(a).

**Model Training.** To seamlessly adapt to multi-physics PDEs with limited data, we propose a two-stage training process: Self-supervised pretraining is followed by a supervised fine-tuning stage. For a summary, we refer to Fig. 1(b).

_Pre-training._ In the context of self-supervised pretraining, the objective is to train the model to reconstruct the original input function from its masked version. Within this phase, the model's encoding component is denoted as the _Encoder_, while the decoding component comprises the _Reconstructor_. The values of the input function at a specific percentage of mesh points are randomly masked to zero, and certain variables (channels/co-domains) of the input function are entirely masked to zero. The model is then trained to reconstruct the original input from this masked version.

We emphasize that the self-supervised learning phase is agnostic of the downstream supervised task and only requires snapshots of simulations of the physical systems.

_Fine-tuning._ In the supervised fine-tuning phase, the _Reconstructor_ is omitted from the decoding module and replaced by a randomly initialized Predictor module. The parameters of the Encoder and VSPEs are copied from pre-trained weights. If the fine-tuning (target) PDE introduces variables that are not present in the pre-training PDE; we train additional variable encoders only for these newly introduced variables (see Fig. 1(b)). This ensures that the model adapts to the expanded set of variables needed for the fine-tuning task with minimal additional parameters.

## 4 Experiments

We conduct experiments on two coupled PDEs: fluid-structure interaction and Rayleigh-Benard convection system. We also test our model on a diverse set of PDEs from PDEBench . The code is available at https://github.com/neuraloperator/CoDA-NO.

**Modeling Fluid-Structure Interaction.** We consider the following problems: (a) a fluid dynamics problem, where a Newtonian, incompressible fluid impinges on a rigid object, and (b) a fluid-structure interaction problem between a Newtonian, incompressible fluid and an elastic, compressible solid object . We denote \(_{t}^{f}\) (resp. \(_{t}^{s}\)) as the domain occupied by the fluid (resp. the solid) at time \(t\). The dynamics of the fluid are governed by the Navier-Stokes equations

\[^{f}+^{f}(u u)= ^{f},\; u=0,\;\;\;\;_{t}^{f}\] (9)

where \(u\) and \(^{f}\) denote the fluid velocity and density, respectively. And \(^{f}\) denotes the Cauchy stress tensor, given by \(^{f}=-p+( u+ u^{T}),\) where \(\) is the identity tensor, \(p\) the fluid pressure, and \(\) the fluid dynamic viscosity.

For fluid-structure interaction, the deformable solid is governed by the elastodynamics equations

\[^{s}d}{ t^{2}}=.(J^{s}(^{-1})^{T})_{t}^{s}\] (10)

with \(=+ d\) and \(J=()\). Here \(d\), \(^{s}\), \(F\), and \(^{s}\) denote the deformation field, the solid density, the deformation gradient tensor, and the Cauchy stress tensor, respectively (see Eq. (18) in the Appendix). The fluid dynamics (resp. the fluid-structure interaction) problem considers a fluid flow past a fixed, rigid cylinder with a rigid (resp. elastic) strap attached. The details regarding the geometric setup (see Fig. 3), time-dependent inlet boundary condition, and the initial conditions are provided in the Appendix Sec. C.1.

**Modeling Rayleigh-Benard Convection.** The Rayleigh-Benard convection system governs the flow of a fluid layer heated from below and cooled from above. The governing equations for the Rayleigh-Benard system consist of the incompressible Navier-Stokes equations coupled with an energy equation for heat transfer. The system is modeled as follows:

\[}{ t}++ P-^{2}- g}=0\] (11) \[+- ^{2}=0\] (12)

**Dataset Description and Generation.** To study the fluid-structure interaction system, two datasets, the fluid-structure interaction (NS+EW dataset) and the fluid dynamics(NS dataset), are generated using the TurtleFSI package .

We simulate the fluid-structure interaction and the fluid dynamics test cases described above up to time \(T_{f}=10\), using a constant time-step \( t=}{n}\), where \(n=1000\). The data sets are composed of solution trajectories \([u_{t},p_{t},d_{t}]\) (resp. \([u_{t},p_{t}]\)), which denote the approximate solution of the fluid-structure interaction problem (resp. the fluid dynamics problem) at times \(t=i t,i\{0,,n\}\). These trajectories are generated on the basis \(3\) parameters \((,c_{1},c_{2})\) describing combinations of fluid viscosities \(\{0.5,1,5,10\}\) and inlet conditions, \((c_{1},c_{2})\).

For our setup, the fluid considered is water, with a density of \(1000kg.m^{-3}\) and a maximum inlet velocity of approximately \(4m.s^{-1}\), leading to Reynolds (\(Re\)) numbers in the range \(200-4000\) (for \(\) between \(10-0.5\)). Modeling fluid-solid interaction or only fluid motion with such high Reynolds numbers is challenging and serves as a benchmark problem  (See Sec. C.2 for a detailed explanation).

To study the Rayleigh-Benard convention system, we degenerate two different PDE datasets. Firstly, we generate Rayleigh-Benard convection system with \(Ra\) number \(12 10^{3}\) and \(20 10^{3}\). We set the temperature difference between the top (cold) and bottom (hot) boundaries to \(1\). We assume no-slip boundary conditions, and to start the convection process, we also add initial temperature perturbation. Additionally, we generate incompressible Navier-Stocks equations with Reynold number \(Re=500\) with cyclic boundary condition on a uniform \(2D\) grid  (for details, see Appendix Sec. C.3).

**Experiment Setup.** For the fluid-structure interaction system, we conduct two distinct pretraining procedures for CoDA-NO and obtain two pretrained models: \(^{}_{}\) and \(^{}_{}\). The former is pretrained on a fluid-structure interaction dataset that combines the Navier-Stokes equation and the elastic wave equation, denoted as \(^{}_{}\). The latter, \(^{}_{}\), is pretrained on a fluid motion dataset governed solely by the Navier-Stokes equation. In both scenarios, the pretraining involves utilizing 8000 snapshots of flow and displacement fields with \(Re\{200,2000\}\).

The supervised task involves training the model to predict the system's state at the subsequent time step based on its current state. For the fluid-structure interaction dataset, we train an operator \(_{}\) such that \(_{}:[u_{t},p_{t},d_{t}][u_{t+ t},p_{t+  t},d_{t+ t}],\) where \(u,p\), and \(d\) are the velocity, pressure, and mesh deformation fields (see Sec. 4). For the data with only fluid motion, we train the operator \(_{}\) which maps between the current and next time step velocity and pressure field as \(_{}:[u_{t},p_{t}][u_{t+ t},p_{t+ t }]\).

The pretrained model for both datasets is fine-tuned for unseen viscosity \(=5.0(Re=400)\) with different numbers of a few shot examples. The inlet conditions of these simulations are excluded from the pretraining data. So, the target PDEs' viscosity and inlet conditions are absent in the per-taining dataset. We test the model's adaptability on a more turbulent fluid-solid interaction dataset with \(Re=4000(=0.5)\) by finetuning both pretrained models \(^{}_{}\) and \(^{}_{}\) on each dataset.

For the Rayleigh-Benard convention system, we pretrain a CoDA-NO model, denoted as \(^{}_{}\), on the incompressible Navier-Stokes equations using \(40,000\) snapshots in a self-supervised manner. The supervised task for this system is to train an operator, \(_{}:[u_{t},_{t}][u_{t+ t}, _{t+ t}]\), where \(u\) represents velocity and \(T\) represents temperature. The pretrained model \(^{}_{}\) is fine-tuned for the supervised task of solving Rayleigh-Benard convection using different numbers of a few shot training samples.

**Baselines.** For comparison on the supervised tasks on fluid-structure interaction system, we train GINO , DeepONet , graph neural network (GNN) , vision transformer (ViT) , and the Unet  model from scratch. The mesh points of the NS and NS+EW datasets are irregular and change for each sample. So, to efficiently handle irregular mesh, in the _branch_ network of DeepONet, we use a GNN layer followed by MLPs. Also, as ViT and Unet can handle irregular mesh, we follow the architecture of GINO and use a GNN layer to query the latent function on a uniform grid. We then apply Unet and ViT to the uniform grid, followed by another GNN layer, to get the output at the desired query points. For the Rayleigh-Benard convection system, we train Unet  and FNO  from scratch and compare them against our proposed model.

It should be noted that employing the existing models for pertaining and subsequent finetuning on the target datasets is nontrivial due to complex geometry and the changes in the number of physical variables between the pertaining and target datasets. We report the \(L^{2}\) error between the predicted and target functions, which serves as a measure of model performance. Additional implementation details are provided in the Appendix Sec. H.

**Results.** In Tab. 1, we report the performance of our model and the baselines for modeling the fluid-structure interaction. We observe that the pretrained CoDA-NO model performs better than the baselines. Importantly, the performance gain is higher when the number of few-shot examples is very low. This demonstrates the sample efficiency and generalization capability of CoDA-NO to previously unseen physical systems.

Next, when CoDA-NO is pretrained solely on the NS dataset, it shows an impressive ability to adapt to the more challenging NS+EW dataset. Finally, when CoDA-NO is pretrained on the more intricate NS+EW dataset, it easily adapts to the simpler NS dataset through fine-tuning. This underscores the capability of the CoDA-NO to adjust between different PDEs with varying numbers of variables seamlessly.

Also, we notice that pretrained CoDA-NO performs better than CoDA-NO trained from scratch, demonstrating the effectiveness of the pretraining scheme. We also provide the energy spectra of the predicted fluid flow by the different models in Sec. F.4 where we observe that the energy spectrum remains closest to the ground truth.

In Tab. 2, we present the result on modeling the Rayleigh-Benard convention. We observe that pretrained CoDA-NO outperforms every other baseline and adapted to the new temperature variable, \(T\), of the Rayleigh-Benard system. Similar to the fluid-structure interaction problem, we also observe that the pretrained CoDA-NO outperforms CoDA-NO trained from scratch, which underlines the effectiveness of our pretraining and adaptation mechanism.

    &  Pretrain \\ Dataset \\  } &  &  \\   & &  \\   & &  &  &  &  &  &  \\   & &  &  \\  & & NS & NS+EW & NS & NS+EW & NS & NS+EW & NS+EW & NS+EW \\  GINO & - & 0.200 & 0.122 & 0.047 & 0.053 & 0.022 & 0.043 & 0.717 & 0.292 & 0.136 \\ DeepO & - & 0.686 & 0.482 & 0.259 & 0.198 & 0.107 & 0.107 & 0.889 & 0.545 & 0.259 \\ GNN & - & 0.038 & 0.045 & 0.008 & 0.009 & 0.008 & 0.009 & 0.374 & 0.310 & 0.132 \\ ViT & - & 0.271 & 0.211 & 0.061 & 0.113 & 0.017 & 0.021 & 0.878 & 0.409 & 0.164 \\ U-Net & - & 13.33 & 3.579 & 0.565 & 0.842 & 0.141 & 0.203 & 3.256 & 0.563 & 0.292 \\   & - & 0.182 & 0.051 & 0.008 & 0.084 & 0.006 & 0.004 & 0.326 & 0.264 & 0.070 \\  & NS & 0.025 & 0.071 & 0.007 & 0.008 & **0.004** & 0.005 & 0.366 & 0.161 & 0.079 \\   & NS+EW & **0.024** & **0.040** & **0.006** & **0.005** & 0.005 & **0.003** & **0.308** & **0.143** & **0.069** \\   

Table 1: Test \(L_{2}\) loss for fluid dynamics (NS) and fluid-solid interaction (NS+EW) datasets with viscosity \(Re=400\) and \(Re=4000\) for different numbers of few-shot training samples.

Additionally, we also conduct experiments on various PDEs from the PDEBench dataset , where we show superior performance and parameter efficiency (see Appendix Sec. G).

**Adaptation to More Turbulent Fluid-Structure Interaction.** We also test the adaptation capability of our pretrained model on a more turbulent fluid-solid interaction scenario with viscosity \(=0.5\) with a Reynolds number of \(4000\). From Tab. 1, we can observe that, even though the model is pretrained on data with lower Reynold's number (\(200-2000\)), it can seamlessly adapt to more turbulent flow and outperform baselines with a significant margin.

**Ablation Studies.** To demonstrate the effect of each of the proposed components, namely, codomain attention, normalization layer, VSPE, and pertaining, we present the result of a detailed ablation study in Sec. F.1. We observe that substituting the codomain attention with regular patch-based attention impacts the model's performance. In particular, removing the normalization layer prevents the model from converging.

We also provide an ablation study on fine-tuning methods. Instead of fine-tuning all the parameters, here, we freeze the parameters of the "Encoder" and only train the parameters of the "Predictor" and VSPEs. This minimized the number of trainable parameters during fine-tuning. Also, in this case, we performed significantly better than the other models (see Appendix Sec. F.5).

We also provide the results for the zero-shot super-resolution task, where we directly predict the output function on a much denser mesh than the training mesh. Our findings show that CoDA-NO outperforms other baselines significantly (see Appendix Sec. F.2).

Additionally, we have conducted a comparative analysis of the parameter count and computational cost for each model, which points to the overfitting problem of the baseline when learning complex multi-physics PDEs (see Appendix Sec. F.3).

**Limitations.** In general, CoDA-NO's performance on target PDEs is influenced by the number of training examples, and we highlight the potential for further enhancement through the integration of physics-informed approaches.

## 5 Conclusion

In this work, we introduce CoDA-NO, a versatile pre-trained model architecture designed for seamless adaptation to Partial Differential Equations (PDEs) featuring diverse variable compositions. Departing from conventional patch-based attention modules, CoDA-NO innovatively extends the transformer to function spaces by computing attention across co-domains. Leveraging a flexible variable encoding scheme and a graph-based neural operator module, CoDA-NO exhibits adaptability to any target PDE, accommodating new and previously unseen variables with arbitrary input-output geometries during fine-tuning. Our empirical evaluations demonstrate that CoDA-NO consistently outperforms baselines across varying amounts of training data and exhibits robustness in handling missing variables. Our findings on complex multiphysics simulations underscore the efficacy and adaptability of CoDA-NO, positioning it as a valuable tool for addressing challenges in machine learning for PDEs.

    & & \)} & \)} \\  Model &  Pretrain \\ dataset \\  } &  \\   & & 5 & 10 & 25 & 5 & 10 & 25 \\   & - & 0.049 & 0.025 & 0.013 & 0.126 & 0.083 & 0.075 \\ FNO & - & 0.119 & 0.070 & 0.044 & 0.491 & 0.166 & 0.127 \\   & - & 0.067 & 0.045 & 0.035 & 0.221 & 0.058 & 0.040 \\  & NS & **0.016** & **0.007** & **0.002** & **0.074** & **0.040** & **0.029** \\   

Table 2: Test \(L_{2}\) error for Rayleigh-BÃ©nard convection system with coupled Navier-Stokes and energy (heat) equation with Rayleigh number \(Ra=12 10^{3}\) and \(Ra=20 10^{3}\) for different few shot examples.