# DINTR: Tracking via Diffusion-based Interpolation

Pha Nguyen\({}^{1}\), Ngan Le\({}^{1}\), Jackson Cothren\({}^{1}\), Alper Yilmaz\({}^{2}\), Khoa Luu\({}^{1}\)

\({}^{1}\)University of Arkansas \({}^{2}\)Ohio State University

\({}^{1}\){panguyen, thile, jcothre, khoaluu}@uark.edu \({}^{2}\)yilmaz.15@osu.edu

###### Abstract

Object tracking is a fundamental task in computer vision, requiring the localization of objects of interest across video frames. Diffusion models have shown remarkable capabilities in visual generation, making them well-suited for addressing several requirements of the tracking problem. This work proposes a novel diffusion-based methodology to formulate the tracking task. Firstly, their conditional process allows for injecting indications of the target object into the generation process. Secondly, diffusion mechanics can be developed to inherently model temporal correspondences, enabling the reconstruction of actual frames in video. However, existing diffusion models rely on extensive and unnecessary mapping to a Gaussian noise domain, which can be replaced by a more efficient and stable interpolation process. Our proposed interpolation mechanism draws inspiration from classic image-processing techniques, offering a more interpretable, stable, and faster approach tailored specifically for the object tracking task. By leveraging the strengths of diffusion models while circumventing their limitations, our **D**iffusion-based **I**N**terpolation **T**ra**ck**R (**DINTR**) presents a promising new paradigm and achieves a superior multiplicity on seven benchmarks across five indicator representations.

Figure 1: Diffusion-based processes. (a) Probabilistic diffusion process , where \(q()\) is noise sampling and \(p_{}()\) is denoising. (b) Diffusion process in the 2D coordinate space . (c) A purely visual diffusion-based _data prediction_ approach reconstructs the subsequent video frame. (d) Our proposed _data interpolation_ approach **DINTR** interpolates between two consecutive video frames, indexed by timestamp \(t\), allowing a seamless temporal transition for visual content understanding, temporal modeling, and instance extracting for the object tracking task across various indications (e).

Introduction

Object tracking is a long-standing computer vision task with widespread applications in video analysis and instance-based understanding. Over the past decades, numerous tracking paradigms have been explored, including _tracking-by-regression_, _-detection_, _-segmentation_ and two more recent _tracking-by-attention_[8; 9], _-unification_ paradigms. Recently, generative modeling has achieved great success, offering several promising new perspectives in instance recognition. These include denoising sampling bounding boxes to final prediction [2; 3; 4], or sampling future trajectories . Although these studies explore the generative process in instance-based understanding tasks, they perform solely on coordinate refinement rather than performing on the visual domain, as in Fig. 0(b).

In this work, we propose a novel tracking framework solely based on _visual_ iterative latent variables of diffusion models [12; 13], thereby introducing the novel and true _Tracking-by-Diffusion_ paradigm. This paradigm demonstrates versatile applications across various indications, comprising points, bounding boxes, segments, and textual prompts, facilitated by the conditional mechanism (Eqn. (3)).

Moreover, our proposed Diffusion-based **IN**terpolation **T**rack**R (**DINTR**) inherently models the temporal correspondences via the diffusion mechanics, _i.e._, the denoising process. Specifically, by formulating the process to operate temporal modeling _online_ and _auto-regressively_ (_i.e._ next-frame reconstruction, as in Eqn. (4)), **DINTR** enables the capability for instance-based video understanding tasks, specifically the object tracking. However, existing diffusion mechanics rely on an extensive and unnecessary mapping to a Gaussian noise domain, which we argue can be replaced by a more efficient interpolation process (Subsection 4.3). Our proposed interpolation operator draws inspiration from the image processing field, offering a more direct, seamless, and stable approach. By leveraging the diffusion mechanics while circumventing their limitations, our **DINTR** achieves superior multiplicity on seven benchmarks across five types of indication, as elaborated in Section 5. Note that our Interpolation process does not aim to generate high-fidelity unseen frames [14; 15; 16; 17]. Instead, its objective is to seamlessly transfer internal states between frames for visual semantic understanding.

**Contributions.** Overall, _(i)_ this paper reformulates the _Tracking-by-Diffusion_ paradigm to operate on visual domain _(ii)_ which demonstrates broader tracking applications than existing paradigms. _(iii)_ We reformulate the diffusion mechanics to achieve two goals, including _(a)_ temporal modeling and _(b)_ iterative interpolation as a \(2\) faster process. _(iv)_ Our proposed **DINTR** achieves superior multiplicity and State-of-the-Art (SOTA) performances on _seven tracking benchmarks_ of _five representations_. _(v)_ Following sections including **Appendices** A elaborate on its formulations, properties, and evaluations.

## 2 Related Work

### Object Tracking Paradigms

**Tracking-by-Regression** methods refine future object positions directly based on visual features. Previous approaches [31; 45] rely on the regression branch of object features in nearby regions. CenterTrack  represents objects via center points and temporal offsets. It lacks explicit object identity, requiring the appearance , motion model , and graph matching  components.

**Tracking-by-Detection** methods form object trajectories by linking detections over consecutive frames, treating the task as an optimization problem. _Graph_-based methods formulate the tracking problem as a bipartite matching or maximum flow . These methods utilize a variety of techniques, such as link prediction , trainable graph neural networks [47; 34], edge lifting , weighted graph labeling , multi-cuts [52; 53], general-purpose solvers , motion information , learned models , association graphs , and distance-based [58; 59; 60]. Additionally, _Appearance_-based methods leverage robust image recognition frameworks to track objects. These techniques depend on similarity measures derived from 3D appearance and pose , affinity estimation , detection candidate selection , learned re-identification features [63; 64], or twin neural networks . On the other hand, _Motion_ modeling is leveraged for camera motion , observation-centric manner , trajectory forecasting , the social force model [68; 69; 70; 71], based on constant velocity assumptions [72; 73], or location estimation [74; 68; 75] directly from trajectory sequences. Additionally, data-driven motion  need to project 3D into 2D motions .

**Tracking-by-Segmentation** leverages detailed pixel information and addresses the challenges of unclear backgrounds and crowded scenes. Methods include cost volumes , point cloud representations , mask pooling layers , and mask-based  with 3D convolutions . However, its reliance on segmented multiple object tracking data often necessitates bounding box initialization.

**Tracking-by-Attention** applies the attention mechanism  to link detections with tracks at the feature level, represented as tokens. TrackFormer  approaches tracking as a unified prediction task using attention, during initiation. MOTR  and MOTRv2  advance this concept by integrating motion and appearance models, aiding in managing object entrances/exits and temporal relations. Furthermore, object token representations can be enhanced via memory techniques, such as memory augmentation  and memory buffer [80; 81]. Recently, MENDER  presents another stride, a transformer architecture with tensor decomposition to facilitate object tracking through descriptions.

**Tracking-by-Unification** aims to develop unified frameworks that can handle multiple tasks simultaneously. Pioneering works in this area include TraDeS  and SiamMask , which combine object tracking (SOT/MOT) and video segmentation (VOS/VIS). UniTrack  employs separate task-specific heads, enabling both object propagation and association across frames. Furthermore, UNICORN  investigates learning robust representations by consolidating from diverse datasets.

### Diffusion Model in Semantic Understanding

Generative models have recently been found to be capable of performing understanding tasks.

**Visual Representation and Correspondence.** Hedlin _et al._ establishes semantic visual correspondences by optimizing text embeddings to focus on specific regions. Diffusion Autoencoders  form a diffusion-based autoencoder encapsulating high-level semantic information. Similarly, Zhang _et al._ combine features from Stable Diffusion (SD) and DINOv2  models, effectively merging the high-quality spatial information and capitalizing on both strengths. Diffusion Hyperfeatures  uses feature aggregation and transforms intermediate feature maps from the diffusion process into a single, coherent descriptor map. Concurrently, DIFT  simulates the forward diffusion process, adding noise to input images and extracting features within the U-Net. Asyrp  employs the asymmetric reverse process to explore and manipulate a semantic latent space, upholding the original

    &  & ^{}\)**} &  \\   & & & Point & Pose & Box & Segment & Text \\  TAPIR  & & _Iter._ Refinement & TAP-Vid & ✗ & ✗ & ✗ \\ Tracktort* & & Regression Head & ✗ & ✗ & MOT & ✗ \\ CenterTrack  & & Offset Prediction & ✗ & ✗ & MOT & ✗ \\ GTI  & & _Rgn-Tpl Integ._ & ✗ & ✗ & **LaSOT** & ✗ & **LaSOT** \\  DeepSORT  & & Cascade _Assoc._ & ✗ & ✗ & MOT & ✗ & ✗ \\ GSDT  & & Relation Graph & ✗ & ✗ & MOT & ✗ & ✗ \\ JDE  & & Multi-Task & ✗ & ✗ & MOT & ✗ & ✗ \\ ByteTrack  & & Two-stage _Assoc._ & ✗ & ✗ & MOT & ✗ & ✗ \\  TrackR-CNN  & & 3D Convolution & ✗ & ✗ & MOTS & ✗ \\ MOTSNet  & & Mask-Pooling & ✗ & ✗ & MOTS & ✗ \\ CAMOT  & Segmentation & Hypothesis Select & ✗ & ✗ & KITTI & ✗ \\ PointTrack  & & _Seg._ as Points & ✗ & ✗ & **✗** & MOTS/KITTI & ✗ \\  MixFormerV2  & & Mixed Attention & ✗ & ✗ & **LaSOT** & ✗ \\ TransVLT  & Attention & \(X\)-Modal Fusion & ✗ & ✗ & **LaSOT** & ✗ & **LaSOT** \\ MeMOTR  & & Memory _Aug._ & ✗ & ✗ & MOT & ✗ & ✗ \\ MENDER  & & Tensor _Decomp._ & ✗ & ✗ & MOT & ✗ & GroOT \\  SiamMask  & & Variant Head & ✗ & **LaSOT** & **VOS** & ✗ \\ TraDeS  & & Cost Volume & ✗ & **✗** & MOT & VIS/MOTS & ✗ \\ UNICORN  & & Unified _Embed._ & ✗ & ✗ & **LaSOT**/MOT & **VOS**/MOTS & ✗ \\ UniTrack  & & Primitive Level & ✗ & PoseTrack & **LaSOT**/MOT & **VOS**/MOTS & ✗ \\  DiffusionTrack  & & Denoised _Coord._ & ✗ & ✗ & MOT & ✗ & ✗ \\ DiffMOT  & **Diffusion** & _Motion_ Predictor & ✗ & **✗** & MOT & ✗ & ✗ \\
**DINTR (Ours)** & & Visual _Interpolat._ TAP-Vid PoseTrack & **LaSOT**/MOT & **VOS**/MOTS & **LaSOT**/GroOT \\   

* _Iter._: Iterative. _Rgn-Tpl Integ._: Region-Template Integration. _Assoc._: Association. _X_: Cross. _Decomp._: Decomposition. _Embed._: Embedding. _Coord._: **2D** Coordinate. _Motion_: **2D** Motion. _Interpolat._: Interpolation.

Table 1: Comparison of paradigms, mechanisms of SOTA tracking methods. **Indication Types** defines the representation to indicate targets with their corresponding datasets: TAP-Vid , PoseTrack [19; 20], MOT [21; 22; 23], **VOS**, VIS , MOTS , KITTI , **LaSOT** , **GroOT**. **Methods** in color gradient support both types of **single-** and multi-target benchmarks.

performance, integrity, and consistency. Furthermore, DRL  introduces an infinite-dimensional latent code that offers discretionary control over the granularity of detail.

**Generative Perspectives in Object Tracking.** A straightforward application of generative models in object tracking is to augment and enrich training data [90; 91; 92]. For trajectory refinement, QuoVadis  uses the social generative adversarial network (GAN)  to sample future trajectories to account for the uncertainty in future positions. DiffusionTrack  and DiffMOT  utilize the diffusion process in the bounding box decoder. Specifically, they pad prior _2D coordinate_ bounding boxes with noise, then transform them into tracking results via a denoising decoder.

### Discussion

This subsection discusses the key aspects of our proposed paradigm and method, including the mechanism comparison of our **DINTR** against alternative diffusion approaches [2; 3; 4], and the properties that enable _Tracking-by-Diffusion_ on visual domain to stand out from the existing paradigms.

**Conditioning Mechanism.** As illustrated in Fig. 0(b), tracking methods performing diffusion on the 2D coordinate space [3; 4] utilize generative models to model 2D object motion or refine coordinate predictions. However, they fail to leverage the conditioning mechanism  of Latent Diffusion Models, which are principally capable of modeling unified conditional distributions. As a result, these diffusion-based approaches have a specified indicator representation limited to the bounding box, that cannot be expanded to other advanced indications, such as point, pose, segment, and text.

In contrast, we formulate the object tracking task as two visual processes, including one for diffusion-based Reconstruction, as illustrated in Fig. 0(c), and another \(2\) faster approach that is Interpolation, as shown in Fig. 0(d). These two approaches demonstrate their superior versatility due to the controlled injection \(p_{}(|)\) implemented by the attention mechanism  (Eqn. (3)) during iterative diffusion.

**Unification.** Current methods under _tracking-by-unification_ face challenges due to the separation of task-specific heads. This issue arises because single-object and multi-object tracking tasks are trained on distinct branches [7; 44] or stages , with results produced through a manually designed decoder for each task. The architectural discrepancies limit the full utilization of network capacity.

In contrast, _Tracking-by-Diffusion_ operating on the visual domain addresses the limitations of unification. Our method seamlessly handles diverse tracking objectives, including _(a) point and pose regression_, _(b) bounding box and segmentation prediction_, and _(c) referring initialization_, while remaining _(d) data- and process-unified_ through an iterative process. This is possible because our approach operates on the base core domain, allowing it to understand contexts and extract predictions.

**Application Coverage** presented in Table 1 validates the unification advantages of our approach. As highlighted, our proposed model **DINTR** supports unified tracking across _seven benchmarks_ of _eight settings_ comprising _five distinct categories of indication_. It can handle both **single-target** and multiple-target benchmarks, setting a new standard in terms of multiplicity, flexibility, and novelty.

## 3 Problem Formulation

Given two images \(_{t}\) and \(_{t+1}\) from a video sequence \(\), and an indicator representation \(L_{t}\) (_e.g._, point, structured points set for pose, bounding box, segment, or text) for an object in \(_{t}\), our goal is to find the respective region \(L_{t+1}\) in \(_{t+1}\). The relationship between \(L_{t}\) and \(L_{t+1}\) can encode semantic correspondences [87; 86; 94] (_i.e._, different objects with similar semantic meanings), geometric correspondence [95; 96; 97] (_i.e._, the same object viewed from different viewpoints) or temporal correspondence [98; 99; 100] (_i.e._, the location of a deforming object over a video sequence).

We define the object-tracking task as temporal correspondence, aiming to establish matches between regions representing the same real-world object as it moves, potentially deforming or occluding across the video sequence over time. Let us denote a feature encoder \(()\) that takes as input the frame \(_{t}\) and returns the feature representation \(^{t}\). Along with the region \(L_{t}\) for the initial indication, the _online_ and _auto-regressive objective_ for the tracking task can be written as follows:

\[L_{t+1}=_{L}dist(_{t})[L_{t}], (_{t+1})[L],\] (1)

where \(dist(,)\) is a semantic distance that can be cosine  or distributional softmax . A special case is to give \(L_{t}\) as textual input and return \(L_{t+1}\) as a bounding box for the _referring object_tracking_ task. In addition, the pose is treated as multiple-point tracking. The output \(L_{t+1}\) is then mapped to a point, box, or segment. We explore how diffusion models can learn these temporal dynamics end-to-end to output consistent object representations frame-to-frame in the next section.

## 4 Methodology

This section first presents the notations and background. Then, we present the deterministic frame reconstruction task for video modeling. Finally, our proposed framework **DINTR** is introduced.

### Notations and Background

**Latent Diffusion Models** (LDMs)  are introduced to denoise the latent space of an autoencoder. First, the encoder \(()\) compresses a RGB image \(_{t}\) into an initial latent space \(_{0}^{t}=(_{t})\), which can be reconstructed to a new image \((_{0}^{t})\). Let us denote two operators \(\) and \(_{_{}}\) are corresponding to the sampling noise process \(q(_{k}^{t}|_{k-1}^{t})\) and the denoising process \(p_{}(_{k-1}^{t}|_{k}^{t})\), where \(_{_{}}\) is parameterized by an U-Net \(_{}\) as a _noise prediction model_ via the objective:

\[_{}_{_{0}^{t},(0,1), k(1,T)}-_{_{ }}(_{0}^{t},k),k,_{2}^{2} ,=_{}(L_{t}).\] (2)

**Localization.** All types of localization \(L_{t}\), _e.g._, point, pose (_i.e._ set of structured points), bounding box, segment, and especially text, are unified as guided indicators. \(_{}()\) is the respective extractor, such as the Gaussian kernel for point, pooling layer for bounding box and segment, or word embedding model for text. \(_{k}^{t}\) is a noisy sample of \(_{0}^{t}\) at step \(k[1,,T]\), and \(T=50\) is the maximum step.

**The Conditional Process \(p_{}(_{0}^{t+1}|)\)**, containing cross-attention \(Attn(,)\) to inject the indication \(\) to an autoencoder with U-Net blocks \(_{}(,)\), is derived after noise sampling \(_{k}^{t}=(_{0}^{t},k)\):

\[_{_{}}(_{0}^{t},k),k,= }_{0}^{t}+} }{} W_{Q}( W_{K})^{}}_{Attn( ,)}( W_{V}),\] (3)

where \(W_{Q,K,V}\) are projection matrices, \(d\) is the feature size, and \(_{k}\) is a scheduling parameter.

### Deterministic Next-Frame Reconstruction by Data Prediction Model

The _noise prediction model_, defined in Eqn. (2), can not generate specific desired pixel content while denoising the latent feature to the new image. To effectively model and generate exactly the desired video content, we formulate a next-frame reconstruction task, such that \((_{_{}}(_{r}^{t},T,)) _{t+1}\). In this formulation, the denoised image obtained from the diffusion process should approximate the next frame in the video sequence. The objective for a _data prediction model_ (Fig. 1c) derives that goal as:

\[_{}_{_{0}^{t,t+1},k(1,T)} _{k}^{t+1}-_{_{}} (_{0}^{t},k),k,_{2}^{2}.\] (4)

```
0: Network \(_{}\), video sequence \(\), indication \(L_{t=0}\)
1: Sample \((t,t+1)(0,||-2)\)
2:\(_{}(L_{t})\)
3: Draw \(_{t,t+1}\) and encode \(_{0}^{t,t+1}=(_{t,t+1})\)
4: Sample \(k(1,T)\)
5: Optimize \(_{}_{k}^{t+1}-_{_{ }}((_{0}^{t},k),k,)_{2}^{2}\)
6: Optimize \(_{}_{t+1}-(_{ _{}}((_{0}^{t},k),k,))_{2}^{2} \) ```

**Algorithm 1** Inplace Reconstruction Finetuning

In layman's terms, the objective of the _data prediction model_ formulates the task of establishing temporal correspondence between frames by effectively capturing the pixel-level changes and reconstructing the real next frame from the current frame. With the pre-trained decoder \(()\) in place, the key optimization target becomes the denoising process itself. To achieve this, a combination of step-wise KL divergences is used to guide the likelihood of current frame latents \(_{k}^{t}\) toward the desired latent representations for the next frame \(_{k}^{t+1}\), as described in Alg. 1 and derived as:

\[=_{_{0}^{t,t+1},k(1, T)}_{k}^{t+1}-_{_{}} (_{0}^{t},k),k,_{2}^{2}= _{0}^{1}}D_{KL}q(_{k}^{t+1}|_{k-1}^{t+1})\|p_{}(_{k-1}^{t}|_{k}^{t}) \,d_{k}.\] (5)

[MISSING_PAGE_FAIL:6]

Interpolation Operator

is selected based on the theoretical properties between the equivalent variants , presented in Table 2 and derived in Section C. In this table, we define \(_{k}=\), then the selected operator (2d), which adds noise in offset form \((_{0}^{t+1},k-1)-(_{0}^{t},k)\), is derived as:

\[}_{k-1}^{t+1} =}_{k}^{t+1}+(_{k}-_{k-1})\;( _{k-1}^{t+1}-_{k}^{t})=}_{k}^{t+1}+ \;(_{k-1}^{t+1}-_{k}^{t}),\] (9) \[}_{k}^{t+1}+(_{k-1}^{t+1}- _{k}^{t})=}_{k}^{t+1}-(_{0}^ {t},k)+(_{0}^{t+1},k-1),\] (10)

Intuitively, the proposed interpolation process to generate the next frame takes the current frame as the starting point of the noisy sample. The internal states and intermediate features of the diffusion model transition from the current frame, resulting in a more stable prediction for video modeling.

**Correspondence Extraction via Internal States.** From Eqn. (3), we demonstrate that _the object of interest can be injected via the indication_. From the objectives in Eqn. (4) and Eqn. (6), we show that _the next frame \(_{t+1}\) can be reconstructed or interpolated from the current frame \(_{t}\)_. Subsequently, internal accumulative and stable states, such as the attention map \(Attn(,)\), which exhibit spatial correlations, can be used to identify the target locations and can be effortlessly extracted. To get into that, the self- and cross-attention maps (\(}_{S}\), \(}_{X}\)) over \(N\) layers and \(T\) time steps are averaged and performed element-wise multiplication:

\[}_{S}=_{l=1}^{N}_{k=0} ^{T}Attn_{[l,k]}(,),}_{X}=_{l=1}^{N}_{k=0}^{T}Attn_{[l,k]}(,),\] (11) \[}^{*}=}_{S} }_{X},}^{*}^{H W},(H W)_{t+1}.\]

Self-attention captures correlations among latent features, propagating the cross-attention to precise locations. Finally, as in Fig. 1e, different mappings produce desired prediction types:

\[L_{t+1}=(}^{*})=(}^{*}),&\\ }^{*}>0,&\\ (_{i},_{j},_{i},_{j}),&=(i,j )}^{*}_{i,j}>0},&\] (12)

In summary, the entire diffusion-based tracking process involves the following steps. First, the indication of the object of interest at time \(t\) is injected as a condition by \(p_{}(_{0}^{t}|)\), derived via Eqn. (3). Next, the video modeling process operates through the deterministic next-frame interpolation \(p_{}(_{0}^{t+1}|_{0}^{t})\), as described in Subsection 4.3. Finally, the extraction of the object of interest in the next frame is performed via a so-called "reversed conditional process" \(p_{}^{-1}(_{0}^{t+1}|)\), outlined in Alg. 3.

```
0: Internal \(Attn\)'s while processing \(_{_{0}}\)
1:for\(k[0,T 0.8]\)do
2:\(_{,X}\)\(+=_{l=1}^{N}Attn_{[l,k]}(,),Attn_{[l,k]}( ,)\)
3:endfor\(\) requires accumulativeness in Table 2
4:\(}_{S,X}_{k=0}^{T  0.8}_{,X}\)
5:\(}^{*}}_{S}}_{X}\)
6:\(L_{t+1}(}^{*})\)\(\) as described in Eqn. (12)
7:return\(L_{t+1}\) ```

**Algorithm 3** Correspondence Extraction

## 5 Experimental Results

### Benchmarks and Metrics

TAP-Vid  formalizes the problem of long-term physical **Point Tracking**. It contains 31,951 points tracked on 1,219 real videos. Three evaluation metrics are _Occlusion Accuracy (OA)_, \(<_{avg}^{x}\) averaging position accuracy, and _Jaccard @ \(\)_ quantifying occlusion and position accuracies.

PoseTrack21  is similar to MOT17 . In addition to estimating **Bounding Box** for each person, the body **Pose** needs to be estimated. Both keypoint-based and standard MOTA , IDF1 , and HOTA  evaluate the tracking performance for every keypoint visibility and subject identity.

DAVIS  and MOTS  are included to quantify the **Segmentation Tracking** performance. For the single-target dataset, evaluation metrics are Jaccard index \(\), contour accuracy \(\) and an overall \(\&\) score . For the multiple-target dataset, MOTSA and MOTSP  are equivalent to MOTA and MOTP, where the association metric measures the mask IoU instead of the bounding box IoU.

Finally, LaSOT  and GroOT  evaluate the **Referring Tracking** performance. The _Precision_ and _Success_ metrics are measured on LaSOT, while GroOT follows the evaluation protocol of MOT.

### Implementation Details

We fine-tune the Latent Diffusion Models  inplace, follow [109; 110]. However, different from offline fixed batch retraining, our fine-tuning is performed online and auto-regressively between consecutive frames when a new frame is received. Our development builds on LDM  for settings with textual prompts and ADM  for localization settings, initialized by their publicly available pre-trained weights. The model is then fine-tuned using our proposed strategy for 500 steps with a learning rate of \(3 10^{-5}\). The model is trained on 4 NVIDIA Tesla A100 GPUs with a batch size of 1, comprising a pair of frames. We average the attention \(}_{S}\) and \(}_{X}\) in the interval \(k[0,T 0.8]\) of the DDIM steps with the total timestep \(T=50\). For the first frame initialization, we employ YOLOX  as the detector, HRNet  as the pose estimator, and Mask2Former  as the segmentation model. We maintained a linear noise scheduler across all experiments, as it is the default in all available implementations and directly dependent on the number of diffusion steps, which is analyzed in the next subsection. Details for handling multiple objects are in Section D.

### Ablation Study

**Diffusion Steps.** We systematically varied the number of diffusion steps (50, 100, 150, 200, 250) and analyzed their impact on performance and efficiency. Results show that we can reconstruct an image close to the origin with a timestep bound \(T=250\) in the reconstruction process of **DINTR**.

**Alternative Approaches** to the proposed **DINTR** modeling are discusses in this subsection. To substantiate the discussions, we include all ablation studies in Table 4, comparing against our base setting. These alternative settings are different interpolation operators as theoretically analyzed in Table 2, and different temporal modeling, including the Reconstruction process as visualized in Fig. 0(c). Results demonstrate that our offset learning approach, which uses two anchor latents to deterministically guide the start and destination points, yields the best performance. This approach provides superior control over the interpolation process, resulting in more accurate and visually coherent output. For point tracking on TAP-Vid, **DINTR** achieves the highest scores, with AJ values ranging from 57.8 to 85.5 across different datasets. In pose tracking on PoseTrack, **DINTR** scores 82.5 mAP, significantly higher than other methods. For bounding box tracking on LaSOT, **DINTR** achieves the highest 0.74 precision and 0.70 success rate with text versus 0.60 precision and 0.58 success rate without text. In segment tracking on VOS, **DINTR** scores 75.7 for \(\&\), 72.7 for \(\), and 78.6 for \(\), consistently outperforming other methods.

    & Kinetics & Kubric & DAVIS & RGB-Stacking \\  & AJ & \(<\)\(_{}^{-1}\) & AJ & \(<\)\(_{}^{-1}\) & AJ & \(<\)\(_{}^{-1}\) & AJ & \(<\)\(_{}^{-1}\) \\ 
**DINTR** & **57.8** & **72.2** & **58.5** & **90.5** & **62.3** & **74.6** & **65.2** & **77.5** \\  _(lc) Recon._ & 53.6 & 64.3 & 80.5 & 86.4 & 62.0 & 66.9 & 62.3 & 71.0 \\  _(2a) Linear_ & 27.6 & 34.8 & 54.6 & 60.1 & 48.1 & 51.6 & 55.6 & 66.3 \\ _(2b)_\(_{0}^{s+1}\) & 34.1 & 43.3 & 64.9 & 63.9 & 51.6 & 54.8 & 59.7 & 60.3 \\ _(2c)_\(_{0}^{s}\) & 33.4 & 41.8 & 63.3 & 62.0 & 51.4 & 53.9 & 58.6 & 59.6 \\    
    & Precision & Success & Precision & Success \\ 
**DINTR** & **0.74** & **0.70** & **0.60** & **0.58** \\  _(lc) Recon._ & 0.66 & 0.64 & 0.52 & 0.50 \\  _(2a) Linear_ & 0.46 & 0.43 & 0.42 & 0.40 \\ _(2b)_\(_{0}^{s+1}\) & 0.52 & 0.49 & 0.46 & 0.45 \\ _(2c)_\(_{0}^{s}\) & 0.51 & 0.48 & 0.44 & 0.44 \\   

Table 4: Ablation studies of different temporal modeling alternatives (the second sub-block) and interpolation operators (the third sub-block) on point tracking (A), pose tracking (B), bounding box tracking with and without text (C), and segment tracking (D).

   \(T\) (steps) & 50 & 100 & 150 & 200 & 250 \\  MSE \(\) & 20.5 & 15.4 & 10.3 & 5.2 & **0.04** \\ \(\&\) \(\) & 75.4 & 75.8 & 76.0 & 76.3 & **76.5** \\  Reconstruction time (s) \(\) & 6.2 & 12.7 & 17.5 & 23.6 & 28.7 \\ Interpolation time (s) \(\) & **3.2** & 5.7 & 8.5 & 10.6 & 14.7 \\   

Table 3: The timestep bound \(T\) affects reconstruction quality.

The reconstruction-based method (1c) generally ranks second in performance across tasks. The decrease in performance for reconstruction is expected, as it does not transfer forward the final prediction to the next step. Instead, it reconstructs everything from raw noise at each step, as visualized in Fig. D.5. Although visual content can be well reconstructed, the lack of seamlessly transferred information between frames results in lower performance and reduced temporal coherence.

The performance difference between (2b) and (2c), which use a single anchor at either the starting latent point (\(_{0}^{t}\)) or destination latent point (\(_{0}^{t+1}\)) respectively, is minimal. However, we observed slightly higher effectiveness when controlling the destination point (2b) compared to the starting point (2b), suggesting that end-point guidance has a marginally stronger impact on overall interpolation quality. Linear blending (2a) consistently shows the lowest performance. Derivations of alternative operators blending (2a), learning from \(_{0}^{t+1}\) (2b), learning from \(_{0}^{t}\) (2c), and learning offset (2d) are theoretically proved to be equivalent as elaborated in Section C.

### Comparisons to the State-of-the-Arts

**Point Tracking.** As presented in Table 5, our **DINTR** point model demonstrates competitive performance compared to prior works due to its thorough capture of local pixels and high-quality reconstruction of global context via the diffusion process. This results in the best performance on DAVIS and Kinetics datasets (88.9 and 89.4 OA). TAPIR  extracts features around the estimations rather than the global context. PIPs  and Tap-Net  lose flexibility by dividing the video into fixed segments. RAFT  cannot easily detect occlusions and makes accumulated errors due to per-frame tracking. COTR  struggles with moving objects as it operates on rigid scenes.

**Pose Tracking.** Table 6 compares our **DINTR** against other pose-tracking methods. Classic tracking methods, such as CorrTrack  and Tracktor++ , form appearance features with limited descriptiveness on keypoint representation. We also include DiffPose , another diffusion-based performer on the specific keypoint estimation task. The primary metric in this setting is the average precision computed for each joint and then averaged over all joints to obtain the final mAP. DiffPose  employs a similar diffusion-based generative process but operates on a different heatmap domain, achieving a similar performance on the pixel domain of our interpolation process.

**Bounding Box Tracking.** Table 7 shows the performance of single object tracking using bounding boxes or textual initialization. Similarly, Table 8 presents the performance of MOT using bounding boxes (left), against DiffussionTrack  and DiffMOT  or textual initialization (right), against MENDER  and MDETR+TrackFormer [129; 8]. Unlike DiffussionTrack  and DiffMOT , which are limited to specific initialization types, our approach allows flexible indicative injection from any type, improving unification capability, and achieving comparable performance. Moreover,

   PoseTrack21 & **mAP** & **MOTA** & **IDFI** & **HOTA** \\  CorrTrack  & 72.3 & 63.0 & 66.5 & 51.1 \\ Tracktor++  w/ poses & 71.4 & 63.3 & 69.3 & 52.2 \\ CorrTrack  w/ RefID & 72.7 & 63.8 & 66.5 & 52.7 \\ Tracktor++  w/ corr. & 73.6 & 61.6 & 69.3 & 54.1 \\  DCPose  & 80.5 & ✗ & ✗ & ✗ \\ FAMI-Pose  & 81.2 & ✗ & ✗ & ✗ \\ DiffPose  & **83.0** & ✗ & ✗ & ✗ \\ 
**DINTR** & 82.5 & **64.9** & **71.5** & **55.5** \\   

Table 6: Pose tracking performance against several methods on PoseTrack21 .

    &  &  &  &  \\  & AJ & \(<_{avg}^{x}\) & OA & AJ & \(<_{avg}^{x}\) & OA & AJ & \(<_{avg}^{x}\) & OA \\  COTR  & 19.0 & 38.8 & 57.4 & 40.1 & 60.7 & 78.5 & 35.4 & 51.3 & 80.2 & 6.8 & 13.5 & 79.1 \\ Kubrie-VFS-Like  & 40.5 & 59.0 & 80.0 & 51.9 & 69.8 & 84.6 & 33.1 & 48.5 & 79.4 & 57.9 & 72.6 & 91.9 \\ RAFT  & 34.5 & 52.5 & 79.7 & 41.2 & 58.2 & 86.4 & 30.0 & 46.3 & 79.6 & 44.0 & 58.6 & 90.4 \\ PIPs  & 35.1 & 54.8 & 77.1 & 59.1 & 74.8 & 88.6 & 42.0 & 59.4 & 82.1 & 37.3 & 51.0 & 91.6 \\ TAP-Net  & 46.6 & 60.9 & 85.0 & 65.4 & 77.7 & 93.0 & 38.4 & 53.1 & 82.3 & 59.9 & 72.8 & 90.4 \\ TAPIR  & 57.1 & 70.0 & 87.6 & 84.3 & **91.8** & **95.8** & 59.8 & 72.3 & 87.6 & **66.2** & 77.4 & **93.3** \\ 
**DINTR** & **57.8** & **72.5** & **89.4** & **85.5** & 90.5 & 95.2 & **62.3** & **74.6** & **88.9** & 65.2 & **77.5** & 91.6 \\   

Table 5: Point tracking performance against several methods on TAP-Vid .

    &  &  &  &  \\  & AJ & \(<_{avg}^{x}\) & OA & AJ & \(<_{avg}^{x}\) & OA & AJ & \(<_{avg}^{x}\) & OA & AJ & \(<_{avg}^{x}\) & OA \\  COTR  & 19.0 & 38.8 & 57.4 & 40.1 & 60.7 & 78.5 & 35.4 & 51.3 & 80.2 & 6.8 & 13.5 & 79.1 \\ Kubrie-VFS-Like  & 40.5 & 59.0 & 80.0 & 51.9 & 69.8 & 84.6 & 33.1 & 48.5 & 79.4 & 57.9 & 72.6 & 91.9 \\ RAFT  & 34.5 & 52.5 & 79.7 & 41.2 & 58.2 & 86.4 & 30.0 & 46.3 & 79.6 & 44.0 & 58.6 & 90.4 \\ PIPs  & 35.1 & 54.8 & 77.1 & 59.1 & 74.8 & 88.6 & 42.0 & 59.4 & 82.1 & 37.3 & 51.0 & 91.6 \\ TAP-Net  & 46.6 & 60.9 & 85.0 & 65.4 & 77.7 & 93.0 & 38.4 & 53.1 & 82.3 & 59.9 & 72.8 & 90.4 \\ TAPIR  & 57.1 & 70.0 & 87.6 & 84.3 & **91.8** & **95.8** & 59.8 & 72.3 & 87.6 & **66.2** & 77.4 & **93.3** \\ 
**DINTR** & **57.8** & **72.5** & **89.4** & **85.5** & 90.5 & 95.2 & **62.3** & **74.6** & **88.9** & 65.2 & **77.5** & 91.6 \\   

Table 5: Point tracking performance against several methods on TAP-Vid .

capturing global contexts via diffusion mechanics helps our model outperform MENDER and TrackFormer relying solely on spatial contexts formulated via transformer-based learnable queries.

**Segment Tracking.** Finally, Table 9 presents our segment tracking performance against _unified_ methods [44; 10], _single-target_ methods [43; 131], and _multiple-target_ methods [37; 7; 8; 132]. Our **DINTR** achieves the best sMOTSA of 67.4, an accurate object tracking and segmentation. Unified methods perform the task separately, either using different branches  or stages . It leads to a discrepancy in networks. Our **DINTR** that is both data- and process-unified avoids this shortcoming.

## 6 Conclusion

In conclusion, we have introduced a _Tracking-by-Diffusion_ paradigm that reformulates the tracking framework based solely on visual iterative diffusion models. Unlike the existing denoising process, our **DINTR** offers a more seamless and faster approach to model temporal correspondences. This work has paved the way for efficient unified instance temporal modeling, especially object tracking.

**Limitations.** There is still a minor gap in performance to methods that incorporate _motion models_, _e.g._, DiffMOT  with 2D coordinate diffusion, as illustrated in Fig. 0(b). However, our novel visual generative approach allows us to handle multiple representations in a unified manner rather than waste \(5\) efforts on designing specialized models. As our approach introduces innovations from _feature representation_ perspective, comparisons with advancements stemming from _heuristic optimizations_, such as ByteTrack , are not head-to-head as these are narrowly tailored increments for a specific type rather than paradigm shifts. However, exploring integrations between core representation and advancements offers promising performance. Specifically, final predictions are extracted by the so-called "reversed conditional process" \(p_{}^{-1}(_{0}^{t+1}|)\) rather than sophisticated operations [133; 134]. Finally, time and resource consumption limit the practicality of Reconstruction. However, offline trackers continue to play a vital role in scenarios that demand comprehensive multimodality analysis.

**Future Work & Broader Impacts.****DINTR** is a stepping stone towards more advanced and real-time visual _Tracking-by-Diffusion_ in the future, especially to develop a new tracking approach that can manipulate visual contents  via the diffusion process or a foundation object tracking model. Specific future directions include formulating diffusion-based tracking approaches for open vocabulary , geometric constraints , camera motion [66; 137; 95], temporal displacement , object state , motion modeling [139; 6; 4], or new object representation  and management . The proposed video modeling approach can be exploited for unauthorized surveillance and monitoring, or manipulating instance-based video content that could be used to spread misinformation.

**Acknowledgment.** This work is partly supported by NSF Data Science and Data Analytics that are Robust and Trusted (DART), USDA National Institute of Food and Agriculture (NIFA), and Arkansas Biosciences Institute (ABI) grants. We also acknowledge Trong-Thuan Nguyen for invaluable discussions and the Arkansas High-Performance Computing Center (AHPCC) for providing GPUs.

   MOT17 & MOTA & IDF1 & HOTA & MT & ML & IDs \\  MORT  & 73.4 & 68.6 & 57.8 & 42.9\% & 19.1\% & 2439 \\ TransMOT  & 76.7 & 75.1 & 61.7 & 51.0\% & 16.4\% & **2346** \\ UNICORN  & 77.2 & 75.5 & 61.7 & **58.7\%** & **11.2**\% & 5379 \\ DiffusionTrack  & 77.9 & 73.8 & 60.8 & – & – & – \\ DiffMOT  & **79.8** & **79.3** & **64.5** & – & – & – \\   

Table 8: Multiple object tracking without (left) and with (right) textual prompt input.

   VOS & \(\) & \(\) & \(\) \\  SiamMask  & 56.4 & 54.3 & 58.5 \\ Siam R-CNN  & 70.6 & 66.1 & 75.0 \\  UniTrack  & – & 58.4 & – \\ UNICORN  & 69.2 & 65.2 & 73.2 \\ 
**DINTR** & **75.4** & **72.5** & **78.4** \\    
   MOTS & s MOTSA & IDF1 & MT & ML & IDSw \\  Track R-CNN  & 40.6 & 42.4 & 38.7\% & 21.6\% & 567 \\ TraDeS  & 50.8 & 58.7 & 49.4\% & 18.3\% & 492 \\ TrackFormer  & 54.9 & 63.6 & – & – & **278** \\ PointTrackV2  & 62.3 & 42.9 & 56.7\% & 12.5\% & 541 \\ UNICORN  & 65.3 & 65.9 & 64.9\% & 10.1\% & 398 \\ 
**DINTR** & **67.4** & **66.4** & **66.5**\% & **8.5**\% & 484 \\   

Table 9: Segment tracking performance on DAVIS  and MOTS .