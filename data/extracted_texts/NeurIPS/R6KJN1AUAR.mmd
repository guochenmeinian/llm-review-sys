# Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation

Sebastien Lachapelle\({}^{*,1}\)

Ioannis Mitliagkas\({}^{}\)

&Divyat Mahajan\({}^{*}\)

Correspondence to: {lachaseb, divyat.mahajan}@mila.quebec

Mila & DIRO, Universite de Montreal

\({}^{1}\)Samsung - SAIT AI Lab, Montreal

###### Abstract

We tackle the problems of latent variables identification and "out-of-support" image generation in representation learning. We show that both are possible for a class of decoders that we call _additive_, which are reminiscent of decoders used for object-centric representation learning (OCRL) and well suited for images that can be decomposed as a sum of object-specific images. We provide conditions under which exactly solving the reconstruction problem using an additive decoder is guaranteed to identify the blocks of latent variables up to permutation and block-wise invertible transformations. This guarantee relies only on very weak assumptions about the distribution of the latent factors, which might present statistical dependencies and have an almost arbitrarily shaped support. Our result provides a new setting where nonlinear independent component analysis (ICA) is possible and adds to our theoretical understanding of OCRL methods. We also show theoretically that additive decoders can generate novel images by recombining observed factors of variations in novel ways, an ability we refer to as _Cartesian-product extrapolation_. We show empirically that additivity is crucial for both identifiability and extrapolation on simulated data.

## 1 Introduction

The integration of connectionist and symbolic approaches to artificial intelligence has been proposed as a solution to the lack of robustness, transferability, systematic generalization and interpretability of current deep learning algorithms  with justifications rooted in cognitive sciences  and causality . However, the problem of extracting meaningful symbols grounded in low-level observations, e.g. images, is still open. This problem is sometime referred to as _disentanglement_ or _causal representation learning_. The question of _identifiability_ in representation learning, which originated in works on _nonlinear independent component analysis_ (ICA) , has been the focus of many recent efforts . The mathematical results of these works provide rigorous explanations for when and why symbolic representations can be extracted from low-level observations. In a similar spirit, _Object-centric representation learning_ (OCRL) aims to learn a representation in which the information about different objects are encoded separately . These approaches have shown impressive results empirically, but the exact reason why they can perform this form of segmentation without any supervision is poorly understood.

### Contributions

Our first contribution is an analysis of the identifiability of a class of decoders we call _additive_ (Definition 1). Essentially, a decoder \(()\) acting on a latent vector \(^{d_{z}}\) to produce an observation \(\) is said to be additive if it can be written as \(()=_{B}^{(B)}(_{B})\) where \(\) is a partition of \(\{1,,d_{z}\}\), \(^{(B)}(_{B})\) are "block-specific" decoders and the \(_{B}\) are non-overlapping subvectors of \(\). This class of decoder is particularly well suited for images \(x\) that can be expressed as a sum of images corresponding to different objects (left of Figure 1). Unsurprisingly, this class of decoder bears similarity with the decoding architectures used in OCRL (Section 2), which already showed important successes at disentangling objects without any supervision. Our identifiability results provide conditions under which exactly solving the reconstruction problem with an additive decoder identifies the latent blocks \(_{B}\) up to permutation and block-wise transformations (Theorems 1 & 2). We believe these results will be of interest to both the OCRL community, as they partly explain the empirical success of these approaches, and to the nonlinear ICA and disentanglement community, as it provides an important special case where identifiability holds. This result relies on the block-specific decoders being "sufficiently nonlinear" (Assumption 2) and requires only very weak assumptions on the distribution of the ground-truth latent factors of variations. In particular, these factors can be statistically dependent and their support can be (almost) arbitrary.

Our second contribution is to show theoretically that additive decoders can generate images never seen during training by recombining observed factors of variations in novel ways (Corollary 3). To describe this ability, we coin the term "Cartesian-product extrapolation" (right of Figure 1). We believe the type of identifiability analysis laid out in this work to understand "out-of-support" generation is novel and could be applied to other function classes or learning algorithms such as DALLE-2  and Stable Diffusion  to understand their apparent creativity and hopefully improve it.

Both latent variables identification and Cartesian-product extrapolation are validated experimentally on simulated data (Section 4). More specifically, we observe that additivity is crucial for both by comparing against a non-additive decoder which fails to disentangle and extrapolate.

**Notation.** Scalars are denoted in lower-case and vectors in lower-case bold, e.g. \(x\) and \(^{n}\). We maintain an analogous notation for scalar-valued and vector-valued functions, e.g. \(f\) and \(\). The \(i\)th coordinate of the vector \(\) is denoted by \(_{i}\). The set containing the first \(n\) integers excluding \(0\) is denoted by \([n]\). Given a subset of indices \(S[n]\), \(_{S}\) denotes the subvector consisting of entries \(_{i}\) for \(i S\). Given a function \((_{S})^{m}\) with input \(_{S}\), the derivative of \(\) w.r.t. \(_{i}\) is denoted by \(D_{i}(_{S})^{m}\) and the second derivative w.r.t. \(_{i}\) and \(_{i^{}}\) is \(D_{i,i^{}}^{2}(_{S})^{m}\). See Table 2 in appendix for more.

**Code:** Our code repository can be found at this link.

## 2 Background & Literature review

**Identifiability of latent variable models.** The problem of latent variables identification can be best explained with a simple example. Suppose observations \(^{d_{x}}\) are generated i.i.d. by first sampling a latent vector \(^{d_{z}}\) from a distribution \(_{}\) and feeding it into a decoder function \(:^{d_{z}}^{d_{x}}\)

Figure 1: **Left: Additive decoders model the additive structure of scenes composed of multiple objects. Right: Additive decoders allow to generate novel images never seen during training via Cartesian-product extrapolation (Corollary 3). Purple regions correspond to latents/observations seen during training. The blue regions correspond to the Cartesian-product extension. The middle set is the manifold of images of balls. In this example, the learner never saw both balls high, but these can be generated nevertheless thanks to the additive nature of the scene. Details in Section 3.2.**i.e. \(=()\). By choosing an alternative model defined as \(}:=\) and \(}:=^{-1}()\) where \(:^{d_{z}}^{d_{z}}\) is some bijective transformation, it is easy to see that the distributions of \(}=}(})\) and \(\) are the same since \(}(})=(^{-1}())=()\). The problem of identifiability is that, given only the distribution over \(\), it is impossible to distinguish between the two models \((,)\) and \((},})\). This is problematic when one wants to discover interpretable factors of variations since \(\) and \(}\) could be drastically different. There are essentially two strategies to go around this problem: (i) restricting the hypothesis class of decoders \(}\), and/or (ii) restricting/adding structure to the distribution of \(}\). By doing so, the hope is that the only bijective mappings \(\) keeping \(}\) and \(}\) into their respective hypothesis classes will be trivial indeterminacies such as permutations and element-wise rescalings. Our contribution, which is to restrict the decoder function \(}\) to be additive (Definition 1), falls into the first category. Other restricted function classes for \(\) proposed in the literature include post-nonlinear mixtures , local isometries , conformal and orthogonal maps  as well as various restrictions on the sparsity of \(\). Methods that do not restrict the decoder must instead restrict/structure the distribution of the latent factors by assuming, e.g., sparse temporal dependencies , conditionally independent latent variables given an observed auxiliary variable , that interventions targeting the latent factors are observed , or that the support of the latents is a Cartesian-product . In contrast, our result makes very mild assumptions about the distribution of the latent factors, which can present statistical dependencies, have an almost arbitrarily shaped support and does not require any interventions. Additionally, none of these works provide extrapolation guarantees as we do in Section 3.2.

**Relation to nonlinear ICA.** Hyvarinen and Pajunen  showed that the standard nonlinear ICA problem where the decoder \(\) is nonlinear and the latent factors \(_{i}\) are _statistically independent_ is unidentifiable. This motivated various extensions of nonlinear ICA where more structure on the factors is assumed . Our approach departs from the standard nonlinear ICA problem along three axes: (i) we restrict the mixing function to be additive, (ii) the factors do not have to be necessarily independent, and (iii) we can identify only the blocks \(_{B}\) as opposed to each \(_{i}\) individually up to element-wise transformations, unless \(=\{\{1\},...,\{d_{z}\}\}\) (see Section 3.1).

**Object-centric representation learning (OCRL).** Lin et al.  classified OCRL methods in two categories: _scene mixture models_ & _spatial-attention models_. Additive decoders can be seen as an approximation to the decoding architectures used in the former category, which typically consist of an object-specific decoder \(^{}\) acting on object-specific latent blocks \(_{B}\) and "mixed" together via a masking mechanism \(^{(B)}()\) which selects which pixel belongs to which object. More precisely,

\[()=_{B}^{(B)}()^{}(_{B})\;^{(B)}_{k}()=_{k}(_{B}))}{_{B^{ }}(_{k}(_{B^{}}))}\;,\] (1)

and where \(\) is a partition of \([d_{z}]\) made of equal-size blocks \(B\) and \(:^{|B|}^{d_{x}}\) outputs a score that is normalized via a softmax operation to obtain the masks \(^{(B)}()\). Many of these works also present some mechanism to select dynamically how many objects are present in the scene and thus have a variable-size representation \(\), an important technical aspect we omit in our analysis. Empirically, training these decoders based on some form of reconstruction objective, probabilistic or not, yields latent blocks \(_{B}\) that represent the information of individual objects separately. We believe our work constitutes a step towards providing a mathematically grounded explanation for why these approaches can perform this form of disentanglement without supervision (Theorems 1 & 2). Many architectural innovations in scene mixture models concern the encoder, but our analysis focuses solely on the structure of the decoder \(()\), which is a shared aspect across multiple methods. Generalization capabilities of object-centric representations were studied empirically by Dittadi et al.  but did not cover Cartesian-product extrapolation (Corollary 3) on which we focus here.

**Diagonal Hessian penalty .** Additive decoders are also closely related to the penalty introduced by Peebles et al.  which consists in regularizing the Hessian of the decoder to be diagonal. In Appendix A.2, we show that "additivity" and "diagonal Hessian" are equivalent properties. They showed empirically that this penalty can induce disentanglement on datasets such as CLEVR , which is a standard benchmark for OCRL, but did not provide any formal justification. Our work provides a rigorous explanation for these successes and highlights the link between the diagonal Hessian penalty and OCRL.

**Compositional decoders .** Compositional decoders were recently introduced by Brady et al.  as a model for OCRL methods with identifiability guarantees. A decoder \(\) is said to be _compositional_ when its Jacobian \(D\) satisfies the following property everywhere: For all \(i[d_{z}]\) and \(B\), \(D_{B}_{i}() D_{B^{c}}_{i}()=\), where \(B^{c}:=[d_{z}] B\). In other words, each \(_{i}\) can _locally_ depend solely on one block \(_{B}\) (this block can change for different \(\)). In Appendix A.3, we show that compositional \(C^{2}\) decoders are additive. Furthermore, Example 3 shows a decoder that is additive but not compositional, which means that additive \(C^{2}\) decoders are strictly more expressive than compositional \(C^{2}\) decoders. Another important distinction with our work is that we consider more general supports for \(\) and provide a novel extrapolation analysis. That being said, our identifiability result does not supersede theirs since they assume only \(C^{1}\) decoders while our theory assumes \(C^{2}\).

**Extrapolation.** Du and Mordatch  studied empirically how one can combine energy-based models for what they call _compositional generalization_, which is similar to our notion of Cartesian-product extrapolation, but suppose access to datasets in which only one latent factor varies and do not provide any theory. Webb et al.  studied extrapolation empirically and proposed a novel benchmark which does not have an additive structure. Besserve et al.  proposed a theoretical framework in which out-of-distribution samples are obtained by applying a transformation to a single hidden layer inside the decoder network. Krueger et al.  introduced a domain generalization method which is trained to be robust to tasks falling outside the convex hull of training distributions. Extrapolation in text-conditioned image generation was recently discussed by Wang et al. .

## 3 Additive decoders for disentanglement & extrapolation

Our theoretical results assume the existence of some data-generating process describing how the observations \(\) are generated and, importantly, what are the "natural" factors of variations.

**Assumption 1** (Data-generating process).: _The set of possible observations is given by a lower dimensional manifold \((^{})\) embedded in \(^{d_{x}}\) where \(^{}\) is an open set of \(^{d_{z}}\) and \(:^{}^{d_{x}}\) is a \(C^{2}\)-diffeomorphism onto its image. We will refer to \(\) as the ground-truth decoder. At training time, the observations are i.i.d. samples given by \(=()\) where \(\) is distributed according to the probability measure \(_{}^{}\) with support \(^{}^{}\). Throughout, we assume that \(^{}\) is regularly closed (Definition 6)._

Intuitively, the ground-truth decoder \(\) is effectively relating the "natural factors of variations" \(\) to the observations \(\) in a one-to-one fashion. The map \(\) is a \(C^{2}\)-diffeomorphism onto its image, which means that it is \(C^{2}\) (has continuous second derivative) and that its inverse (restricted to the image of \(\)) is also \(C^{2}\). Analogous assumptions are very common in the literature on nonlinear ICA and disentanglement [33; 36; 42; 1]. Mansouri et al.  pointed out that the injectivity of \(\) is violated when images show two objects that are indistinguishable, an important practical case that is not covered by our theory.

We emphasize the distinction between \(^{}\), which corresponds to the observations seen during training, and \(^{}\), which corresponds to the set of all possible images. The case where \(^{}^{}\) will be of particular interest when discussing extrapolation in Section 3.2. The "regularly closed" condition on \(^{}\) is mild, as it is satisfied as soon as the distribution of \(\) has a density w.r.t. the Lebesgue measure on \(^{d_{z}}\). It is violated, for example, when \(\) is a discrete random vector. Figure 2 illustrates this assumption with simple examples.

**Objective.** Our analysis is based on the simple objective of reconstructing the observations \(\) by learning an encoder \(}:^{d_{x}}^{d_{z}}\) and a decoder \(}:^{d_{z}}^{d_{x}}\). Note that we assumed implicitly that the dimensionality of the learned representation matches the dimensionality of the ground-truth. We define the set of latent codes the encoder can output when evaluated on the training distribution:

\[}^{}:=}((^{ }))\,.\] (2)

When the images of the ground-truth and learned decoders match, i.e. \((^{})=}(}^{})\), which happens when the reconstruction task is solved exactly, one can define the map \(:^{}^{}\) as

\[:=^{-1}}\,.\] (3)

This function is going to be crucial throughout the work, especially to define \(\)-disentanglement (Definition 3), as it relates the learned representation to the ground-truth representation.

Before introducing our formal definition of additive decoders, we introduce the following notation: Given a set \(^{d_{z}}\) and a subset of indices \(B[d_{z}]\), let us define \(_{B}\) to be the projection of \(\) onto dimensions labelled by the index set \(B\). More formally,

\[_{B}:=\{_{B}\}^{|B |}\,.\] (4)

Intuitively, we will say that a decoder is _additive_ when its output is the summation of the outputs of "object-specific" decoders that depend only on each latent block \(_{B}\). This captures the idea that an image can be seen as the juxtaposition of multiple images which individually correspond to objects in the scene or natural factors of variations (left of Figure 1).

**Definition 1** (Additive functions).: _Let \(\) be a partition of \([d_{z}]\)1. A function \(:^{d_{x}}\) is said to be **additive** if there exist functions \(^{(B)}:_{B}^{d_{x}}\) for all \(B\) such that_

\[,()=_{B}^{(B)} (_{B})\,.\] (5)

This additivity property will be central to our analysis as it will be the driving force of identifiability (Theorem 1 & 2) and Cartesian-product extrapolation (Corollary 3).

**Remark 1**.: _Suppose we have \(=(_{B}^{(B)}(_{B}))\) where \(\) is a known bijective function. For example, if \(():=()\) (component-wise), the decoder can be thought of as being multiplicative. Our results still apply since we can simply transform the data doing \(}:=^{-1}()\) to recover the additive form \(}=_{B}^{(B)}(_{B})\)._

**Differences with OCR in practice.** We point out that, although the additive decoders make intuitive sense for OCRL, they are not expressive enough to represent the "masked decoders" typically used in practice (Equation (1)). The lack of additivity stems from the normalization in the masks \(^{(B)}()\). We hypothesize that studying the simpler additive decoders might still reveal interesting phenomena present in modern OCRL approaches due to their resemblance. Another difference is that, in practice, the same object-specific decoder \(^{()}\) is applied to every latent block \(_{B}\). Our theory allows for these functions to be different, but also applies when functions are the same. Additionally, this parameter sharing across \(^{(B)}\) enables modern methods to have a variable number of objects across samples, an important practical point our theory does not cover.

### Identifiability analysis

We now study the identifiability of additive decoders and show how they can yield disentanglement. Our definition of disentanglement will rely on _partition-respecting permutations_:

**Definition 2** (Partition-respecting permutations).: _Let \(\) be a partition of \(\{1,...,d_{z}\}\). A permutation \(\) over \(\{1,...,d_{z}\}\) respects \(\) if, for all \(B,\;(B)\)._

Essentially, a permutation that respects \(\) is one which can permute blocks of \(\) and permute elements within a block, but cannot "mix" blocks together. We now introduce \(\)-disentanglement.

**Definition 3** (\(\)-disentanglement).: _A learned decoder \(}:^{d_{z}}^{d_{x}}\) is said to be \(\)**-disentangled** w.r.t. the ground-truth decoder \(\) when \((^{})=}(}^{})\) and the mapping \(:=^{-1}}\) is a diffeomorphism from \(}^{}\) to \(^{}\) satisfying the following property: there exists a permutation \(\) respecting \(\) such that, for all \(B\), there exists a function \(}_{(B)}:}^{}_{B}^{ }_{(B)}\) such that, for all \(}^{}\), \(_{(B)}()=}_{(B)}(_{B})\). In other words, \(_{(B)}()\) depends only on \(_{B}\)._

Thus, \(\)-disentanglement means that the blocks of latent dimensions \(_{B}\) are disentangled from one another, but that variables within a given block might remain entangled. Note that, unless the partition is \(=\{\{1\},,\{d_{z}\}\}\), this corresponds to a weaker form of disentanglement than what is typically seeked in nonlinear ICA, i.e. recovering each variable individually.

**Example 1**.: _To illustrate \(\)-disentanglement, imagine a scene consisting of two balls moving around in 2D where the "ground-truth" representation is given by \(=(x^{1},y^{1},x^{2},y^{2})\) where \(_{B_{1}}=(x^{1},y^{1})\) and \(_{B_{2}}=(x^{2},y^{2})\) are the coordinates of each ball (here, \(:=\{\{1,2\},\{3,4\}\}\)). In that case, a learned representation is \(\)-disentangled when the balls are disentangled from one another. However, the basis in which the position of each ball is represented might differ in both representations._Our first result (Theorem 1) shows a weaker form of disentanglement we call _local_\(\)-disentanglement. This means the Jacobian matrix of \(\), \(D\), has a "block-permutation" structure everywhere.

**Definition 4** (Local \(\)-disentanglement).: _A learned decoder \(}:^{d_{z}}^{d_{x}}\) is said to be **locally \(\)-disentangled** w.r.t. the ground-truth decoder \(\) when \((^{})=}(}^{})\) and the mapping \(:=^{-1}}\) is a diffeomorphism from \(}^{}\) to \(^{}\) with a mapping \(:}^{}^{}\) satisfying the following property: for all \(}^{}\), there exists a permutation \(\) respecting \(\) such that, for all \(B\), the columns of \(D_{(B)}()^{|B| d_{z}}\) outside block \(B\) are zero._

In Appendix A.4, we provide three examples where local disentanglement holds but not global disentanglement. The first one illustrates how having a disconnected support can allow for a permutation \(\) (from Definition 4) that changes between disconnected regions of the support. The last two examples show how, even if the permutation stays the same throughout the support, we can still violate global disentanglement, even with a connected support.

We now state the main identifiability result of this work which provides conditions to guarantee _local_ disentanglement. We will then see how to go from local to _global_ disentanglement in the subsequent Theorem 2. For pedagogical reasons, we delay the formalization of the sufficient nonlinearity Assumption 2 on which the result crucially relies.

**Theorem 1** (Local disentanglement via additive decoders).: _Suppose that the data-generating process satisfies Assumption 1, that the learned decoder \(}:^{d_{z}}^{d_{x}}\) is a \(C^{2}\)-diffeomorphism, that the encoder \(}:^{d_{x}}^{d_{z}}\) is continuous, that both \(\) and \(}\) are additive (Definition 1) and that \(\) is sufficiently nonlinear as formalized by Assumption 2. Then, if \(}\) and \(}\) solve the reconstruction problem on the training distribution, i.e. \(^{}||-}(}())||^{2}=0\), we have that \(}\) is locally \(\)-disentangled w.r.t. \(\) (Definition 4)._

The proof of Theorem 1, which can be found in Appendix A.5, is inspired from Hyvarinen et al. . The essential differences are that (i) they leverage the additivity of the conditional log-density of \(\) given an auxiliary variable \(\) (i.e. conditional independence) instead of the additivity of the decoder function \(\), (ii) we extend their proof techniques to allow for "block" disentanglement, i.e. when \(\) is not the trivial partition \(\{\{1\},,\{d_{z}\}\}\), (iii) the asssumption "sufficient variability" of the prior \(p()\) of Hyvarinen et al.  is replaced by an analogous assumption of "sufficient nonlinearity" of the decoder \(\) (Assumption 2), and (iv) we consider much more general supports \(^{}\) which makes the jump from local to global disentanglement less direct in our case.

**The identifiability-expressivity trade-off.** The level of granularity of the partition \(\) controls the trade-off between identifiability and expressivity: the finer the partition, the tighter the identifiability guarantee but the less expressive is the function class. The optimal level of granularity is going to dependent on the application at hand. Whether \(\) could be learned from data is left for future work.

**Sufficient nonlinearity.** The following assumption is key in proving Theorem 2, as it requires that the ground-truth decoder is "sufficiently nonlinear". This is reminiscent of the "sufficient variability" assumptions found in the nonlinear ICA litterature, which usually concerns the distribution of the latent variable \(\) as opposed to the decoder \(\). We clarify this link in Appendix A.6 and provide intuitions why sufficient nonlinearity can be satisfied when \(d_{x} d_{z}\).

**Assumption 2** (Sufficient nonlinearity of \(\)).: _Let \(q:=d_{z}+_{B}\). For all \(^{}\), \(\) is such that the following matrix has linearly independent columns (i.e. full column-rank):_

\[():=[[D_{i}^{(B)}(_{B})]_{i B}\, [D_{i,i^{}}^{2}^{(B)}(_{B})]_{(i,i^{}) B _{}^{2}}]_{B}^{d_{x} q}\,,\] (6)

_where \(B_{}^{2}:=B^{2}\{(i,i^{}) i^{} i\}\). Note this implies \(d_{x} q\)._

The following example shows that Theorem 1 does not apply if the ground-truth decoder \(\) is linear. If that was the case, it would contradict the well known fact that linear ICA with independent Gaussian factors is unidentifiable.

**Example 2** (Importance of Assumption 2).: _Suppose \(=()=\) where \(^{d_{x} d_{z}}\) is full rank. Take \(}():=\) and \(}():=^{-1}^{}\) where \(^{d_{z} d_{z}}\) is invertible and \(^{}\) is the left pseudo inverse of \(\). By construction, we have that \([-}(}())]=0\) and \(\) and \(}\) are \(\)-additive because \(()=_{B}_{,B}_{B}\) and \(}()=_{B}()_{,B}_{B}\). However, we still have that \(():=^{-1}}()=\) where \(\) does not necessarily have a block-permutation structure, i.e. no disentanglement. The reason we cannot apply Theorem 1 here is because Assumption 2 is not satisfied. Indeed, the second derivatives of \(^{(B)}(_{B}):=_{,B}_{B}\) are all zero and hence \(()\) cannot have full column-rank._

**Example 3** (A sufficiently nonlinear \(\)).: _In Appendix A.7 we show numerically that the function_

\[():=[_{1},_{1}^{2},_{1}^{3},_{1}^{4}]^{ }+[(_{2}+1),(_{2}+1)^{2},(_{2}+1)^{3},(_{2}+1)^{4} ]^{}\] (7)

_is a diffeomorphism from the square \([-1,0]\) to its image that satisfies Assumption 2._

**Example 4** (Smooth balls dataset is sufficiently nonlinear).: _In Appendix A.7 we present a simple synthetic dataset consisting of images of two colored balls moving up and down. We also verify numerically that its underlying ground-truth decoder \(\) is sufficiently nonlinear._

#### 3.1.1 From local to global disentanglement

The following result provides additional assumptions to guarantee _global_ disentanglement (Definition 3) as opposed to only local disentanglement (Definition 4). See Appendix A.8 for its proof.

**Theorem 2** (From local to global disentanglement).: _Suppose that all the assumptions of Theorem 1 hold. Additionally, assume \(^{}\) is path-connected (Definition 8) and that the block-specific decoders \(^{(B)}\) and \(}^{(B)}\) are injective for all blocks \(B\). Then, if \(}\) and \(}\) solve the reconstruction problem on the training distribution, i.e. \(^{}||-}(}())||^{2}=0\), we have that \(}\) is (globally) \(\)-disentangled w.r.t. \(\) (Definition 3) and, for all \(B\),_

\[}^{(B)}(_{B})=^{((B))}(}_{(B)}( _{B}))+^{(B)},\,_{B}}_{B}^{}\,,\] (8)

_where the functions \(}_{(B)}\) are from Defition 3 and the vectors \(^{(B)}^{d_{x}}\) are constants such that \(_{B}^{(B)}=0\). We also have that the functions \(}_{(B)}:}_{B}^{}_{ (B)}^{}\) are \(C^{2}\)-diffeomorphisms and have the following form:_

\[}_{(B)}(_{B})=(^{(B)})^{-1}(}^{(B)}( _{B})-^{(B)}),_{B}}_{B}^{ }\,.\] (9)

Equation (8) in the above result shows that each block-specific learned decoder \(}^{(B)}\) is "imitating" a block-specific ground-truth decoder \(^{(B)}\). Indeed, the "object-specific" image outputted by the decoder \(}^{(B)}\) evaluated at some \(_{B}}_{B}^{}\) is the same as the image outputted by \(^{(B)}\) evaluated at \((_{B})_{B}^{}\), _up to an additive constant vector \(^{(B)}\)_. These constants cancel each other out when taking the sum of the block-specific decoders.

Equation (9) provides an explicit form for the function \(}_{(B)}\), which is essentially the learned block-specific decoder composed with the inverse of the ground-truth block-specific decoder.

**Additional assumptions to go from local to global.** Assuming that the support of \(^{}\), \(^{}\), is **path-connected** (see Definition 8 in appendix) is useful since it prevents the permutation \(\) of Definition 4 from changing between two disconnected regions of \(}^{}\). See Figure 2 for an illustration. In Appendix A.9, we discuss the additional assumption that each \(^{(B)}\) must be injective and show that, in general, it is not equivalent to the assumption that \(_{B}^{(B)}\) is injective.

### Cartesian-product extrapolation

In this section, we show how a learned additive decoder can be used to generate images \(\) that are "out of support" in the sense that \((^{})\), but that are still on the manifold of "reasonable" images, i.e. \((^{})\). To characterize the set of images the learned decoder can generate, we will rely on the notion of "cartesian-product extension", which we define next.

Figure 2: Illustrating regularly closed sets (Definition 6) and path-connected sets (Definition 8). Theorem 2 requires \(^{}\) to satisfy both properties.

**Definition 5** (Cartesian-product extension).: _Given a set \(^{d_{z}}\) and partition \(\) of \([d_{z}]\), we define the Cartesian-product extension of \(\) as_

\[_{}():=_{B}_{ B}\,,_{B}:=\{_{B}\}.\]

_It is indeed an extension of \(\) since \(_{B}_{B}\)._

Let us define \(}:_{}(}^{}) _{}(^{})\) to be the natural extension of the function \(:}^{}^{}\). More explicitly, \(}\) is the "concatenation" of the functions \(}_{B}\) given in Definition 3:

\[}()^{}:=[}_{B_{1}}(_{^{-1}(B_{1})})^{ }}_{B_{}}(_{^{-1}(B_{})})^{}]\,,\] (10)

where \(\) is the number of blocks in \(\). This map is a diffeomorphism because each \(}_{(B)}\) is a diffeomorphism from \(}_{B}^{}\) to \(_{(B)}^{}\) by Theorem 2.

We already know that \(}()=}()\) for all \(}^{}\). The following result shows that this equality holds in fact on the larger set \(_{}(}^{})\), the Cartesian-product extension of \(}^{}\). See right of Figure 1 for an illustration of the following corollary.

**Corollary 3** (Cartesian-product extrapolation).: _Suppose the assumptions of Theorem 2 holds. Then,_

\[_{}(}^{ }),\;_{B}}^{(B)}(_{B})=_{B }^{((B))}(}_{(B)}(_{B}))\,.\] (11)

_Furthermore, if \(_{}(^{})^ {}\), then \(}(_{}(}^{})) (^{})\)._

Equation (11) tells us that the learned decoder \(}\) "imitates" the ground-truth \(\) not just over \(}^{}\), but also over its Cartesian-product extension. This is important since it guarantees that we can generate observations never seen during training as follows: Choose a latent vector \(^{}\) that is in the Cartesian-product extension of \(}^{}\), but not in \(}^{}\) itself, i.e. \(^{}_{}(}^{})}^{}\). Then, evaluate the learned decoder on \(^{}\) to get \(^{}:=}(^{})\). By Corollary 3, we know that \(^{}=}(^{})\), i.e. it is the observation one would have obtain by evaluating the ground-truth decoder \(\) on the point \(}(^{})_{}(^ {})\). In addition, this \(^{}\) has never been seen during training since \(}(^{})}(}^{})=^{}\). The experiment of Figure 4 illustrates this procedure.

**About the extra assumption "\(_{}(^{})^ {}\)".** Recall that, in Assumption 1, we interpreted \((^{})\) to be the set of "reasonable" observations \(\), of which we only observe a subset \((^{})\). Under this interpretation, \(^{}\) is the set of reasonable values for the vector \(\) and the additional assumption that \(_{}(^{})^ {}\) in Corollary 3 requires that the Cartesian-product extension of \(^{}\) consists only of reasonable values of \(\). From this assumption, we can easily conclude that \(}(_{}(}^{})) (^{})\), which can be interpreted as: "The novel observations \(^{}\) obtained via Cartesian-product extrapolation are _reasonable_". Appendix A.11 describes an example where the assumption is violated, i.e. \(_{}(^{}) ^{}\). The practical implication of this is that the new observations \(^{}\) obtained via Cartesian-product extrapolation might not always be reasonable.

**Disentanglement is not enough for extrapolation.** To the best of our knowledge, Corollary 3 is the first result that formalizes how disentanglement can induce extrapolation. We believe it illustrates the fact that disentanglement alone is not sufficient to enable extrapolation and that one needs to restrict the hypothesis class of decoders in some way. Indeed, given a learned decoder \(}\) that is disentangled w.r.t. \(\) on the training support \(^{}\), one cannot guarantee both decoders will "agree" outside the training domain without further restricting \(}\) and \(\). This work has focused on "additivity", but we believe other types of restriction could correspond to other types of extrapolation.

## 4 Experiments

We now present empirical validations of the theoretical results presented earlier. To achieve this, we compare the ability of additive and non-additive decoders to both identify ground-truth latent factors (Theorems 1 & 2) and extrapolate (Corollary 3) when trained to solve the reconstruction task on simple images (\(64 64 3\)) consisting of two balls moving in space . See Appendix B.1for training details. We consider two datasets: one where the two ball positions can only vary along the \(y\)-axis (**ScalarLatents**) and one where the positions can vary along both the \(x\) and \(y\) axes (**BlockLatents**).

**ScalarLatents:** The ground-truth latent vector \(^{2}\) is such that \(_{1}\) and \(_{2}\) corresponds to the height (y-coordinate) of the first and second ball, respectively. Thus the partition is simply \(=\{\{1\},\{2\}\}\) (each object has only one latent factor). This simple setting is interesting to study since the low dimensionality of the latent space (\(d_{z}=2\)) allows for exhaustive visualizations like Figure 4. To study Cartesian-product extrapolation (Corollary 3), we sample \(\) from a distribution with a L-shaped support given by \(^{}:=[0.5,1][0.5,1]\), so that the training set does not contain images where both balls appear in the upper half of the image (see Appendix B.2).

**BlockLatents:** The ground-truth latent vector \(^{4}\) is such that \(_{\{1,2\}}\) and \(_{\{3,4\}}\) correspond to the \(x,y\) position of the first and second ball, respectively (the partition is simply \(=\{\{1,2\},\{3,4\}\}\), i.e. each object has two latent factors). Thus, this more challenging setting illustrates "block-disentanglement". The latent \(\) is sampled uniformly from the hypercube \(^{4}\) but the images presenting occlusion (when a ball is behind another) are rejected from the dataset. We discuss how additive decoders cannot model images presenting occlusion in Appendix A.12. We also present an additional version of this dataset where we sample from the hypercube \(^{4}\) with dependencies. See Appendix B.2 for more details about data generation.

**Evaluation metrics:** To evaluate disentanglement, we compute a matrix of scores \((s_{B,B^{}})^{}\) where \(\) is the number of blocks in \(\) and \(s_{B,B^{}}\) is a score measuring how well we can predict the ground-truth block \(_{B}\) from the learned latent block \(}_{B^{}}=}_{B^{}}()\) outputted by the encoder. The final Latent Matching Score (LMS) is computed as \(=_{_{B}}_{B} s_{B,(B)}\), where \(_{}\) is the set of permutations respecting \(\) (Definition 2). When \(:=\{\{1\},,\{d_{z}\}\}\) and the score used is the absolute value of the correlation, LMS is simply the _mean correlation coefficient_ (MCC), which is widely used in the nonlinear ICA literature . Because our theory guarantees recovery of the latents only up to invertible and potentially nonlinear transformations, we use the Spearman correlation, which can capture nonlinear relationships unlike the Pearson correlation. We denote this score by \(_{}\) and will use it in the dataset **ScalarLatents**. For the **BlockLatents** dataset, we cannot use Spearman correlation (because \(_{B}\) are two dimensional). Instead, we take the score \(s_{B,B^{}}\) to be the \(R^{2}\) score of a regression tree. We denote this score by \(_{}\). There are subtleties to take care of when one wants to evaluate \(_{}\) on a non-additive model due to the fact that the learned representation does not have a natural partition \(\). We must thus search over partitions. We discuss this and provide further details on the metrics in Appendix B.3.

### Results

**Additivity is important for disentanglement.** Table 1 shows that the additive decoder obtains a much higher \(_{}\ \&\ _{}\) than its non-additive counterpart on all three datasets considered, even if both decoders have very small reconstruction errors. This is corroborated by the visualizations of Figures 4 & 5. Appendix B.5 additionally shows object-specific reconstructions for the **BlockLatents** dataset. We emphasize that disentanglement is possible even when the latent factors are dependent (or causally related), as shown on the **ScalarLatents** dataset (L-shaped support implies dependencies) and on the **BlockLatents** dataset with dependencies (Table 1). Note that prior works have relied on interventions  or Cartesian-product supports  to deal with dependencies.

    &  &  &  \\  & & & & & \))} & \))} \\  Decoders & RMSE & \(_{}\) & \(^{}\) & \(_{}^{}\) & RMSE & \(_{}\) & RMSE & \(_{}\) \\  Non-add. & 06 \(\).002 & 70.6\(\) 5.21 &.18\(\).012 & 73.7\(\) 4.64 &.02\(\).001 & 53.9\(\) 7.58 &.02\(\).001 & 78.1\(\).292 \\ Additive &.06\(\).002 & **91.5\(\).357** &.11\(\).018 & **89.5\(\).62** &.03\(\).012 & **92.2\(\).491** &.01\(\).002 & **99.9\(\).02** \\   

Table 1: Reporting reconstruction mean squared error (RMSE \(\)) and the Latent Matching Score (LMS \(\)) for the three datasets considered: **ScalarLatents** and **BlockLatents** with independent and dependent latents. Runs were repeated with 10 random initializations. \(^{}\) and \(_{}^{}\) are the same metric but evaluated out of support (see Appendix B.3 for details). While the standard error is high, the differences are still clear as can be seen in their box plot version in Appendix B.4.

**Additivity is important for Cartesian-product extrapolation.** Figure 4 illustrates that the additive decoder can generate images that are outside the training domain (both balls in upper half of the image) while its non-additive counterpart cannot. Furthermore, Table 1 also corroborates this showing that the "out-of-support" (OOS) reconstruction MSE and \(_{}\) (evaluated only on the samples never seen during training) are significantly better for the additive than for the non-additive decoder.

**Importance of connected support.** Theorem 2 required that the support of the latent factors, \(^{}\), was path-connected. Appendix B.6 shows experiments where this assumption is violated, which yields lower \(_{}\) for the additive decoder, thus highlighting the importance of this assumption.

## 5 Conclusion

We provided an in-depth identifiability analysis of _additive decoders_, which bears resemblance to standard decoders used in OCRL, and introduced a novel theoretical framework showing how this architecture can generate reasonable images never seen during training via "Cartesian-product extrapolation". We validated empirically both of these results and confirmed that additivity was indeed crucial. By studying rigorously how disentanglement can induce extrapolation, our work highlighted the necessity of restricting the decoder to extrapolate and set the stage for future works to explore disentanglement and extrapolation in other function classes such as masked decoders typically used in OCRL. We postulate that the type of identifiability analysis introduced in this work has the potential of expanding our understanding of creativity in generative models, ultimately resulting in representations that generalize better.

Figure 4: Figure (a) shows latent representation outputted by the encoder \(}()\) over the _training_ dataset, and the corresponding reconstructed images of the additive decoder with median \(_{}\) among runs performed on the **ScalarLatents** dataset. Figure (b) shows the same thing for the non-additive decoder. The color gradient corresponds to the value of one of the ground-truth factor, the red dots correspond to factors used to generate the images and the yellow dashed square highlights extrapolated images.

Figure 5: Latent responses for the case of independent latents in the **BlockLatent** dataset. In each plot, we report the latent factors predicted from multiple images where one ball moves along only one axis at a time. For the additive case, at most two latents change, as it should, while more than two latents change for the non-additive case. See Appendix B.5 for details.