# Contextual Active Model Selection

Xuefeng Liu\({}^{1}\)

Fangfang Xia\({}^{2}\)

Rick L. Stevens\({}^{1,2}\)

Yuxin Chen\({}^{1}\)

\({}^{1}\)Department of Computer Science

 University of Chicago

\({}^{2}\)Argonne National Laboratory

Correspondence to: Xuefeng Liu <xuefeng@uchicago.edu>.

###### Abstract

While training models and labeling data are resource-intensive, a wealth of pre-trained models and unlabeled data exists. To effectively utilize these resources, we present an approach to actively select pre-trained models while minimizing labeling costs. We frame this as an _online contextual active model selection_ problem: At each round, the learner receives an unlabeled data point as a context. The objective is to adaptively select the best model to make a prediction while limiting label requests. To tackle this problem, we propose CAMS, a contextual active model selection algorithm that relies on two novel components: (1) a contextual model selection mechanism, which leverages context information to make informed decisions about which model is likely to perform best for a given context, and (2) an active query component, which strategically chooses when to request labels for data points, minimizing the overall labeling cost. We provide rigorous theoretical analysis for the regret and query complexity under both adversarial and stochastic settings. Furthermore, we demonstrate the effectiveness of our algorithm on a diverse collection of benchmark classification tasks. Notably, CAMS requires substantially less labeling effort (less than 10%) compared to existing methods on CIFAR10 and DRIFT benchmarks, while achieving similar or better accuracy.

## 1 Introduction

As pre-trained models become increasingly prevalent in a variety of real-world machine learning applications [2; 11; 53], there is a growing demand for label-efficient approaches for model selection, especially when facing varying data distributions and contexts at run time. Oftentimes, no single pre-trained model achieves the best performance for every context, and a proper approach is to construct a policy for adaptively selecting models for specific contexts . For instance, in medical diagnosis and drug discovery, accurate predictions are of paramount importance. The diagnosis of diseases through pathologist or the determination of compound chemical properties through lab testing can be costly and time-consuming. Different models may excel in analyzing different types of pathological images [1; 3; 23] or chemical compounds [17; 32; 46]. Furthermore, in many real-world applications, the collection of labels for model evaluation can be expensive and data instances may arrive as a stream rather than all at once. This scenario necessitates _cost-effective_ and _robust_ online algorithms capable of determining the most efficient model selection policy even when faced with a limited supply of labels, a scenario not fully addressed by previous works that typically assume access to all labels [6; 7; 27; 54].

Recently, the problem of online model selection with the consideration of label acquisition costs was studied in a _context-free_ setting by Karimi et al. . However, this approach doesn't fully capture the dynamics of data contexts that are essential in many applications. Recognizing this gap, in this paper, we consider a more general problem setting that incorporates context information for adaptive model selection. We introduce CAMS, an algorithm for active model selection that dynamically adapts to the data context to choose the most suitable models for an arbitrary data stream. As highlighted inTable 1, CAMS aims to address the need for adaptive and effective model selection, by bridging the gap between contextual bandits, online learning, and active learning.

Our key contributions are summarized as follows:

* We investigate a novel problem which we refer to as _contextual active model selection_, and introduce a novel principled algorithm that features two key technical components: (1) a _contextual online model selection_ procedure, designed to handle both stochastic and adversarial settings, and (2) an _active query_ strategy. The proposed algorithm is designed to be robust to heterogeneous data streams, accommodating both stochastic and adversarial online data streaming scenarios.
* We provide rigorous theoretical analysis on the _regret_ and _query complexity_ of the proposed algorithms. We establish regret upper bounds for both adversarial and stochastic data streams under limited label costs. Our regret upper bounds are within constant factors of the existing lower bounds for online learning problems with expert advice under the full information setting.
* Empirically, we demonstrate the effectiveness and robustness of our approach on a variety of online model selection tasks spanning different application domains (from generic ML benchmarks such as CIFAR10 to domain-specific tasks in biomedical analysis), data scales (ranging from 80 to 10K), data modalities (i.e., tabular, image, and graph-based data), and label types (binary or multiclass labels). For the tasks evaluated, (1) CAMS outperforms all competing baselines by a significant margin. (2) Asymptotically, CAMS performs no worse than the best single model. (3) CAMS is not only robust to adversarial data streams but also can efficiently recover from "malicious experts" (i.e. inferior pre-trained models).

## 2 Related Work

Contextual bandits.Classical bandit algorithms (e.g., [6; 7]) aim to find the best arm(s) through a sequence of actions. When side information (e.g., user profile for recommender systems or environmental context for experimental design) is available, many bandit algorithms can be lifted to the contextual setting: For example, EXP4 [7; 9; 52] considers the bandit setting with expert advice: At each round, experts announce their predictions of which actions are the most promising for the given context, and the goal is to construct a expect selection policy that competes with the best expert from hindsight. In bandit problems, the learner only gets to observe the reward for each action taken. In contrast, for the online model selection problem considered in this work--where an action corresponds to choosing a model to make prediction on an incoming data point--we get to see the loss/reward of _all_ models on the labeled data point. By utilizing the information from unchosen arms, it could significantly reduce the cumulative regret. In this regard, this work aligns more closely with online learning with _full information_ setting, where the learner has access to the loss of all the arms at each round (e.g. as considered in the Hedge algorithm [13; 14; 27; 36]).

Online learning with full information.A clear distinction between our work and online learning is that we assume the labels of the online data stream are not readily available but can be acquired at each round with a cost. In addition, the learner only observes the loss incurred by all models on a data point when it decides to query its label. In contrast, in the canonical online learning setting, labels arrive with the data and one gets to observe the loss of all candidate models at each round. Similar setting also applies to other online learning problems, such as online boosting or bagging. A related work to ours is online learning with label-efficient prediction , which proposes an online learning algorithm with matching upper and lower bounds on the regret. However, they consider a fixed query probability that leads to a linear query complexity. Our algorithm, inspired by uncertainty sampling in active learning, achieves an improved query complexity with the adaptive query strategy while maintaining a comparable regret.

   & **Online bagging** & **Helge** & **EXP3** & **EXP4** & **Query by Committee** & **ModelPicker** & **CAMS** \\  &  &  &  &  &  &  & (ours) \\  & bagging & online learning & bandit & contextual bandits & active learning & model selection & (ours) \\  model selection\({}^{}\) & \(\) & ✓ & ✓ & \(\) & ✓ & ✓ & ✓ \\  full-information & ✓ & ✓ & \(\) & \(\) & ✓ & ✓ & ✓ \\  active queries & \(\) & \(\) & \(\) & \(\) & ✓ & ✓ & ✓ \\  context-aware & \(\) & \(\) & \(\) & ✓ & \(\) & \(\) & ✓ \\  

* We regard “arm” as “modroids” when comparing CAMS against bandit algorithms, such as EXP3EXP4.
* Online ensemble learning aims to build a competing model by aggregating multiple models rather than selecting the best model (for a given context).

Table 1: Comparing CAMS against related work in terms of problem setup.

Stream-based Active learning.Active learning aims to achieve a target learning performance with fewer training examples . The active learning framework closest to our setting is query-by-committee (QBC) , in particular under the stream-based setting . QBC maintains a committee of hypotheses; each committee member votes on the label of an instance, and the instances with the maximal disagreement among the committee are considered the most informative labels. Note that existing stream-based QBC algorithms are designed and analyzed assuming i.i.d. data streams. In comparison, our work uses a different query strategy as well as a novel model recommendation strategy, which also applies to the adversarial setting.

Active model selection.Active model selection captures a broad class of problems where model evaluations are expensive, either due to (1) the cost of evaluating (or "probing") a model, or (2) the cost of annotating a training example. Existing works under the former setting  and online setting  often ignore context information and data annotation cost, and only consider _partial_ feedback on the models being evaluated/ probed on i.i.d. data. The goal is to identify the best model with as few model probes as possible. This is quite different from our problem setting which considers the full information setting as well as non-negligible data annotation cost.  proposes that the optimal model choice is influenced by the sample size rather than any individual sample feature.  addresses the active model selection problem, however both works do not adopt a stream-based approach. For the later, apart from Karimi et al. , an online contextual-free model selection work, as shown in Table 1, most existing works assume a pool-based setting where the learner can choose among the pool of unlabeled data , and the goal is to identify the best model with a minimal set of labels.

## 3 Problem Statement

Notations.Let \(\) be the input domain and \(:=\{0,,c-1\}\) be the set of \(c\) possible class labels for each input instance. Let \(=\{f_{1},,f_{k}\}\) be a set of \(k\) pre-trained classifiers over \(\). A model selection policy \(:^{k-1}\) maps any input instance \(}\) to a distribution over the pre-trained classifiers \(\), specifying the probability \((})\) of selecting each classifier under input \(x\). Here, \(^{k-1}\) denotes the \(k\)-dimensional probability simplex \(\{}^{k}:|}|=1, } 0\}\). One can interpret a policy \(\) as an "expert" that suggests which model to select for a given _context_ \(x\).

Let \(\) be a collection of model selection policies. In this paper, we propose an _extended policy set_\(^{*}:=\{_{i}^{},,_{k}^{}\}\) which also includes constant policies that always suggest a fixed model. Here, \(_{j}^{}():=_{j}\), and \(}_{j}^{k-1}\) denotes the canonical basis vector with \(e_{j}=1\). Unless otherwise specified, we assume \(\) is finite with \(||=n\), and \(|^{*}| n+k\). As a special case, when \(=\), our problem reduces to the contextual-free setting.

The contextual active model selection protocol.Assume that the learner knows the set of classifiers \(\) as well as the set of model selection policies \(\). At round \(t\), the learner receives a data instance \(}_{t}\) as the context for the current round, and computes the predicted label \(_{t,j}=f_{j}(}_{t})\) for each pre-trained classifier indexed by \(j[k]\). Denote the vector of predicted labels by all \(k\) models by \(}}_{t}:=[_{t,1},,_{t,k}]^{}\). Based on previous observations, the learner identifies a model/classifier \(f_{j_{t}}\) and makes a prediction \(_{t,j_{t}}\) for the instance \(}_{t}\). Meanwhile, the learner can obtain the true label \(y_{t}\)_only if_ it decides to query \(}_{t}\). Upon observing \(y_{t}\), the learner incurs a _query cost_, and receives a (full) loss vector \(_{t}=_{\{_{t} y_{t}\}}\), where the \(j\)th entry \(_{t,j}:=_{\{_{t,j} y_{t}\}}\) corresponds to the 0-1 loss for model \(j[k]\) at round \(t\). The learner can then use the queried labels to adjust its model selection criterion for future rounds.

Performance metric.If \(}_{t}\) is misclassified by the model \(j_{t}\) selected by learner at round \(t\), i.e. \(_{t,j_{t}} y_{t}\), it will be counted towards the _cumulative loss_ of the learner, regardless of the learner making a query. Otherwise, no loss will be incurred for that round. For a learning algorithm \(\), its cumulative loss over \(T\) rounds is defined as \(L_{T}^{}:=_{t=1}^{T}_{t,j_{t}}\).

In practice, the choice of model \(j_{t}\) at round \(t\) by the learner \(\) could be random: For _stochastic_ data streams where \((},y)\) arrives i.i.d., the learner may choose different models for different random realizations of \((}_{t},y_{t})\). For the _adversarial_ setting where the data stream \(\{(}_{t},y_{t})\}_{t 1}\) is chosen by an adversary before each round, the learner may randomize its choice of model to avoid a constant loss at each round . Therefore, due to the randomness of \(L_{T}^{}\), we consider the _expected_ cumulative loss \([L_{T}^{}]\) as a key performance measure of the learner \(\). To characterize the progress of \(\), we consider the _regret_--formally defined as follows-- as the difference between the cumulative loss received by the learner and the loss if the learner selects the "best policy" \(^{*}^{*}\) in hindsight.

For stochastic data streams, we assume that each policy \(i\) recommends the _most probable_ model w.r.t. \(_{i}(_{t})\) for context \(_{t}\). We use \(():=_{j:w_{j}}w_{j}\) to denote the index of the maximal-value entry2 of \(\). Since \((,y)\) are drawn i.i.d., we define \(_{i}=_{t=1}^{T}_{_{t},y_{t}}_{t, (_{i}(_{t}))}\). This leads to the pseudo-regret for the stochastic setting over \(T\) rounds, defined as

\[}_{T}()=[L_{T}^{}]-T_{i[\|^{*}\|]}_{i}.\] (1)

In an adversarial setting, since the data stream (and hence the loss vector) is determined by an adversary, we consider the reference best policy to be the one that minimizes the loss on the adversarial data stream, and the expected regret

\[_{T}()=[L_{T}^{}]-_ {i[\|^{*}\|]}_{t=1}^{T}_{t,i},\] (2)

where \(_{t,i}:=_{i}(_{t}),_{t}\) denotes the expected loss if the learner commits to policy \(_{i}\), randomizes and selects \(j_{t}_{i}(_{t})\) (and receives loss \(_{t,j_{t}}\)) at round \(t\). Our goal is to devise a principled online active model selection strategy to minimize the regret as defined in (1) or (2), while maintaining a low total query cost. For convenience, we refer the readers to App. B for a summary of the notations used in this paper.

## 4 Contextual Active Model Selection

In this section, we introduce our main algorithm for both stochastic and adversarial data streams.

**Contextual model selection.** Our key insight underlying the contextual model selection strategy extends from the _online learning with expert advice_ framework . We start by appending the constant policies that always pick single pre-trained _models_ to form the extended policy set \(^{*}\) (Line 3, in Fig. 1). This allows CAMS to be at least as competitive as the best model. Then, at each round, CAMS maintains a probability distribution over the (extended) policy set \(^{*}\), and updates those according to the observed loss for each policy. We use \(_{t}:=(q_{t,i})_{i[^{*}]}\) to denote the probability distribution over \(^{*}\) at \(t\). Specifically, the probability \(q_{t,i}\) is computed based on the exponentially weighted cumulative loss, i.e. \(q_{t,i}(-_{t}_{t-1,i})\) where \(_{t,i}:=_{=1}^{t}_{,i}\) denotes the cumulative loss of policy \(i\).

Figure 1: The Contextual Active Model Selection (CAMS) algorithm

For adversarial data streams, it is natural for both the online learner and the model selection policies to randomize their actions to avoid linear regret . Following this insight, CAMS randomly samples a policy \(i_{t}_{t}\), and--based on the current context \(_{t}\)--samples a classifier \(j_{t}_{i_{t}}(_{t})\) to recommend at round \(t\).

Under the stochastic setting, CAMS adopts a weighted majority strategy  when selecting models. The vector of each model's weighted votes from the policies, \(_{t}=_{i\|^{*}\|}q_{t,i}_{i}(_{t})\), is interpreted as a distribution induced by the weighted policy. The model \(j_{t}=(_{t})\) which receives the highest probability becomes the recommended model at round \(t\). This deterministic model selection strategy is commonly used in stochastic online optimization . An alternative strategy is to take a randomized approach as in the adversarial setting, or take a Follow-the-Leader approach  and go with the most probable model recommended by the most probable policy (i.e. use \(_{t}=_{(_{t})}(_{t})\)).As shown in experimental results section and further discussed in Appendix (outperformance over the best policy/expert section), CAMS outperforms these policies in a wide range of practical applications. The model selection steps are detailed in Line 5-9 in Fig. 1.

**Active queries.** Under a limited budget, we intend to query the labels of those instances that exhibit significant disagreement among the pre-trained models \(\). To achieve this goal, we design an adaptive query strategy with query probability \(z_{t}\). Concretely, given context \(_{t}\), model predictions \(}_{t}\) and model distribution \(_{t}\), we denote by \(_{t}^{y}:=_{t},\{}_{t}  y\}\) as the expected loss if the true label is \(y\). We characterize the model disagreement as

\[(}_{t},_{t}):=_{y ,_{t}^{y}(0,1)}_{t}^{y}_{c} _{t}^{y}}.\] (3)

Intuitively, when \(_{t}^{y}\) is close to \(0\) or \(1\), there is little disagreement among the models in labeling \(_{t}\) as \(y\), otherwise there is significant disagreement. We capture this insight with function \(h(x)=-x x\). Since the label \(y_{t}\) is unknown upfront when receiving \(_{t}\), we iterate through all the possible labels \(y\) and take the average value as in Eq. (3). Note that \(\) takes a similar algebraic form to the entropy function, although it does not inherit the information-theoretic interpretation.

With the model disagreement term defined above, we consider an adaptive query probability3

\[z_{t}=_{0}^{t},(}_{t},_{t })},\] (4)

where \(_{0}^{t}=}(0,1]\) is an adaptive lower bound on the query probability to encourage exploration at an early stage. The query strategy is summarized in Line 10-14 in Fig. 1.

Model updates.Now define \(U_{t}(z_{t})\) as a binary query indicator that is sampled from a Bernoulli distribution parametrized by \(z_{t}\). Upon querying the label \(y_{t}\), one can calculate the loss for each model \(f_{j}\) as \(_{t,j}=\{_{t,j} y_{t}\}\). Since CAMS does not query all the i.i.d. examples, we introduce an unbiased loss estimator for the models, defined as \(_{t,j}=}{z_{t}}U_{t}\). The unbiased loss of policy \(_{i}^{*}\) can then be computed as \(_{t,i}=_{i}(_{t}),_{t,j}\). In the end, CAMS computes the (unbiased) cumulative loss of policy \(_{i}\) as \(_{T,i}=_{t=1}^{T}_{t,i}\), which is used to update the policy probability distribution in next round. Pseudocode for the model update steps is summarized in Line 15-21 in Fig. 1.

Remark.CAMS runs efficiently with time complexity \(O(nk)\) per round and space complexity \(O((n+k) k)\). At each round, each model selection policy specifies a probability distribution over the models for the given context. When these distributions correspond to constant Dirac delta distributions (regardless of the context), the problem reduces to the context-free problem investigated by Karimi et al. .

## 5 Theoretical Analysis

We now present theoretical bounds on the regret (defined in Eq. (1) and Eq. (2), respectively) and the query complexity of CAMS for both the stochastic and the adversarial settings.

### Stochastic setting

Under the stochastic setting, the cumulative loss of CAMS over T rounds--as specified by the Recommend procedure--is \(L_{T}^{}=_{t=1}^{T}_{t,(_{t})}\) where recall \(_{t}=_{i|^{*}|}q_{t,i}_{i}( _{t})\) is the probability distribution over \(\) induced by the weighted policy.

Let \(i^{*}=_{i||^{*}||}_{i}\) be the index of the best policy (\(_{i}\) denotes the expected loss of policy \(i\), as defined in problem statement section. The cumulative expected loss of policy \(i^{*}\) is \(T_{i^{*}}\); therefore the expected pseudo-regret (Eq. (1)) is \(}_{T}()=[_{t=1} ^{T}_{t,(_{t})}]-T_{i^{*}}\).

Define \(:=_{i i^{*}}(_{i}-_{i^{*}})\) as the minimal sub-optimality gap4 in terms of the expected loss against the best policy \(i^{*}\). Furthermore, let \(_{t}^{t}:=_{i^{*}}(_{t})\) be probability distribution over \(\) induced by policy \(i^{*}\) at round \(t\). We define \(:=_{_{t}}\{_{w_{j}_{t}^{*}}w_{j} -_{w_{j}_{t}^{*},j(_{t}^{ *})}w_{j}\}\) (5) as the minimal probability gap between the most probable model and the rest (assuming no ties) induced by the best policy \(i^{*}\). We further define \(b=p_{}_{c}(1/p_{})\), where \(p_{}=_{s,i}(_{s})\) denotes the minimal model selection probability by any policy5. As our first main theoretical result, we show that, without exhaustively querying the labels of the stochastic stream, CAMS achieves constant expected regret.

**Theorem 1**.: _(Regret) In the stochastic environment, with probability at least \(1-\), CAMS achieves constant expected pseudo regret \(}_{T}()=( |-1+| 2b^{2}}}{ |}})^{2}\)._

Note that in the stochastic setting, a lower bound of \(((^{*})/)\) was shown in Mourtada and Gaiffas  for online learning problems with expert advice under the full information setting (i.e. assuming labels are given for all data points in the stochastic stream). To establish the proof of Theorem 1, we consider a novel procedure to connect the weighted policy by CAMS to the best policy \(_{i^{*}}\). Conceptually, we would like to show that, after a _constant_ number of rounds \(_{}\), with high probability, the model selected by CAMS (Line 32) will be the same as the one selected by the best policy \(i^{*}\). In that way, the expected pseudo regret will be dominated by the maximal cumulative loss up to \(_{}\). Toward this goal, we first bound the weight of the best policy \(w_{t,i^{*}}\) as a function of \(t\), by choosing a proper learning rate \(_{t}(,23)\). Then, we identify a constant threshold \(_{}\), beyond which CAMS exhibits the same behavior as \(_{i^{*}}\) with high probability. Finally, we obtain the regret bound by inspecting the regret at the two stages separately. The formal statement of Theorem 1 and the detailed proof are deferred to App. E.1.

Next, we provide an upper bound on the query complexity in the stochastic setting.

**Theorem 2**.: _(Query Complexity). For \(c\)-class classification problems, with probability at least \(1-\), the expected number of queries made by CAMS over \(T\) rounds is upper bounded by \(((|-1}+ | 2b^{2}}}{|}})^{ 2}+T_{i^{*}})\)._

Theorem 2 is built upon Theorem 1, where the the key idea behind the proof is to relate the number of updates to the regret. When \(T_{i^{*}},_{T,*}\) are regarded as constants (given by an oracle), the query-complexity bound is then sub-linear _w.r.t._\(T\). Note that the number of class labels \(c\) affects the quality of the query complexity bound. The intuition behind this result is, with larger number of classes, _each query may carry more information upon observation_. For instance, in an extreme case where only one expert always recommends the best model and others gives random recommendations of models (and predicts random labels), having more classes lowers the chance of a model making the correct guess, and therefore helps to "filter out" those suboptimal experts in fewer rounds--hence being more query efficient. We defer the proof of Theorem 2 to App. E.2.

### Adversarial setting

Now we consider the adversarial setting. Let \(_{T,*}:=_{i[|^{*}|]}_{t=1}^{T} _{t,i}\) be the cumulative loss of the best policy. The expected regret (Eq. (2)) for CAMS equals to \(_{T}()=( |}})^{2}\).

\(_{t=1}^{T}_{t},}_{t} -_{T,*}\). We show that under the adversarial setting, CAMS achieves sub-linear regret in \(T\) without accessing all labels.

**Theorem 3**.: _(Regret) Let \(c\) be the number of classes and \(_{t}\) be specified as Line 26-27 in the SetRate procedure. Under the adversarial setting, the expected regret of CAMS is bounded by \(2c,\}}|}\)._

The proof is provided in App. F.1. Assuming \(_{t}\) to be a constant, our regret upper bound in Theorem 3 matches (up to constants) the lower bound of \((|})\) for online learning problems with expert advice under the full information setting [15; 63] (i.e. assuming labels are given for all data points). Hereby, the decaying learning rate \(_{t}\) as specified in Line 27 is based on two parameters, where \(1/\) corresponds to the lower bound \(_{0}^{t}\) on the query probability, and \(_{t} 1-_{[t-1]}_{},\{ }_{}=y_{}\}\)) (6) is a (data-dependent) term that is chosen to reduce the impact of the randomized query strategy on the regret bound (especially when \(t\) is large). Intuitively, \(_{t}\) relates to the skewness of the policy where the \(\) term corresponds to the maximal probability of most probable mispredicted label over \(t\) rounds. Note that in theory \(_{t}\) can be small (e.g. CAMS may choose a constant policy \(_{i}^{}^{*}\) that mispredict the label for \(_{t}\), which leads to \(_{t}=0\)); in such cases, our result still translates to a sublinear regret bound of \(O(c T^{}|})\). Furthermore, in practice, we consider to "regularize" the policies (App. D.4) to ensure that probability a policy selecting any model is bounded away from 0.

Finally, the following theorem (proof in App. F.2) establishes a query complexity bound of CAMS.

**Theorem 4**.: _(Query Complexity). Under the adversarial setting, the expected query complexity over \(T\) rounds is \(O( T(|}{\{_{T},\}}}+ _{T,*})).\)_

## 6 Experiments

Datasets.We evaluate our approach using five datasets: (1) CIFAR10  contains 60,000 images from 10 different balanced classes. (2) DRIFT  is a tabular dataset with 128-dimensional features, based on 13,910 chemical sensor measurements of 6 types of gases at various concentration levels. (3) VERTEBRAL  is a biomedical tabular dataset which classifies 310 patients into three classes (Normal, Spondylolisthesis, Disk Hernia) based on 6 attributes. (4) HIV  contains over 40,000 compounds annotated with molecular graph features and binary labels (active, inactive) indicating their ability to inhibit HIV replication. (5) CovType  has 580K samples and contains details including slope, aspect, elevation, measurements of area, and type of forest cover.

Policy sets.We construct the policy sets \(\) for each dataset following a procedure similar to Meta-selector . In this approach, a set of recommender algorithms is considered, and Meta-selector assigns varying ratings to these algorithms based on the specific user. Concretely, we first construct a set of models trained on different subsamples from each dataset. We then construct a set of policies, which include _malicious_, _normal_, _random_, and _biased_ policy types for each dataset

Figure 2: **Main results. Comparison of CAMS with 7 baselines across 4 diverse benchmarks in terms of cost effectiveness. We plot the cumulative loss as we increase the query cost for a fixed number of rounds \(T\) and maximal query cost \(B\) (from left to right: \(T=10000,3000,80,4000\), and \(B=1200,2000,80,2000\)). CAMS outperforms all baselines. Algorithms: 4 contextual {Oracle, CQBC, CIWAL, CAMS} and 4 non-contextual baselines {RS, QBC, IWAL, MP} are included (see Section ). 90% confident interval are indicated in shades.**

based on different models and features. Details on the classifiers and policies are provided in the supplemental materials. The _malicious_ policy provides contrary advice; the _random_ policy provides random advice; the _biased_ policy provides biased advice by training on a biased distribution for classifying specific classes. The _normal_ policy gives reasonable advice, being trained under a standard process on the training set. We represent the output of the \(i_{th}\) policy as \(_{i}(_{t})\), indicating the rewards distribution of all the base classifiers on \(_{t}\). In total, we create 80, 10, 6, 4 classifiers and 85, 11, 17, 20 policies for CIFAR10, DRIFT, VERTEBRAL, and HIV, respectively.

Baselines.We evaluate CAMS against both _contextual_ and _non-contextual_ active model selection baselines. We consider four _non-contextual_ baselines: (1) Random Query Strategy (RS) which queries the instance label with a fixed probability \(}{T}\); (2) Model Picker (MP)  that employs variance-based active sampling with a coin-flip query probability \(\{v(}_{t},_{t}),_{t}\}\), where the variance term is defined as \(v(}_{t},_{t})=_{y Y}_{t}^{y}( 1-_{t}^{y})\); (3) Query by Committee (QBC) implementing committee-based sampling ; and (4) Importance Weighted Active Learning (IWAL)  that calculates query probability based on labeling disagreements of surviving classifiers. Since no _contextual_ baselines exist yet, we propose contextual versions of QBC and IWAL as (5) CQBC and (6) CIWAL. Both extensions maintain their respective original query strategies but incorporate the context into the cumulative rewards. For _model selection_, CAMS, MP, CQBC, and CIWAL recommend the classifier with the highest probability. The other baselines use Follow-the-Leader (FTL), recommending the model with the minimum cumulative loss for past queried instances. Finally, we add (7) Oracle to represent the best single policy with the minimum cumulative loss, with the same query strategy as CAMS.

### Main results

Fig. 2 visualizes the _cost effectiveness_ of CAMS and the baselines. Here, we define _cost effectiveness_ as the measure of how quickly the cumulative loss decreases in response to an increase in query cost. Fig. 2 demonstrates that CAMS outperforms all the comparison methods across all benchmarks. Remarkably, it outperforms even the oracle on the VERTEBRAL (Fig. 1(c)) and HIV (Fig. 1(d)) benchmarks with fewer than 10 and 20 queries, respectively. In the case of the VERTEBRAL benchmark, CAMS outperforms the best baseline in query cost by a margin of \(20\%\), despite the fact that 11 out of the 17 experts provided malicious or random advice. This level of performance is attained by utilizing an active query strategy to retrieve highly informative data, thereby maximizing the differentiation between models and policies within the constraints of a limited budget. Additionally, the model selection strategy allows for effectively combining the expertise among the experts.

Figure 3: **Ablation studies.** (a) Comparing three query strategies {CAMS, variance-based, random} under same model selection policy. (b) Comparing the increasing rate of CAMS’ query cost over other baselines. (c) Comparing CAMS with MP in context-free environment. (d) Evaluating the performance of CAMS under a pure adversarial setting. (e) Large dataset. (f,g) Adjustable query probability. (h) CAMS outperforms the best single policy. The ablation study (a)-(d) is conducted on CIFAR10. For additional results on other benchmarks, please refer to the supplemental material.

### Ablation studies

**Effectiveness of active querying.** In Fig. 2(a) and Fig. 2(b), we perform ablation studies to demonstrate the effectiveness of our active query strategy. We fix the model recommendation strategy as the one used by CAMS, and compare three query strategies: (1) CAMS, (2) the state-of-the-art variance-based query strategy from Model Picker  (referred to as "variance"), and (3) a random query strategy. Figure 2(a) demonstrates that CAMS has the fastest convergence rate in terms of cumulative loss on CIFAR10, implying effective use of queried labels. Furthermore, CAMS not only achieves the minimum cumulative loss but also incurs significantly lower query costs, with reductions of 71% and 95% compared to the variance and random strategies respectively as showed in Fig. 2(b). This suggests that CAMS selectively queries data to optimize policy improvement, whereas the other strategies may query unnecessary labels, including potentially noisy or uninformative ones, which impede policy improvement and convergence.

**Robustness.** In Fig. 2, 2(c), 2(d), 2(e), 2(f), and 2(g), CAMS exhibits robustness in a variety of environmental settings. Firstly, As shown in Fig. 2, CAMS outshines other methods in a contextual environment, whereas in Fig. 2(c), a non-contextual (no experts) environment, it achieves comparable performance to the state-of-the-art Model Picker in identifying the best classifier. Secondly, CAMS is robust in both stochastic and adversarial environments. As demonstrated in Fig. 2, CAMS surpasses other methods in a stochastic environment. Additionally, as illustrated in Fig. 2(d), in a worst-case adversarial environment, CAMS effectively recovers from adversarial actions and approaches the performance of the best classifier (see App. G.5). We further observe that CAMS demonstrates robustness to varying scales of data, where the online stream sizes range from 80 to 10K (Fig. 2) to 100K (Fig. 2(e), where we randomly sample 100K samples from the CovType dataset ).

In Fig. 2, we assume that the stream length \(T\) is hidden and not used as input to CAMS. Under the stochastic setting, however, knowing \(T\) can provide additional information that one can leverage to optimize the query probability, thereby giving an advantage to some of the baseline algorithms (e.g. random). As an ablation study, in Fig. 2(f) and Fig. 2(g), we assume the stochastic setting where the total length \(T\) of the online stream is given. Given the stream length \(T\) and query budget \(b\), we may optimize each algorithm by scaling their query probabilities, so that each algorithm allocates its query budget to the top \(b\) informative labels in the entire online stream based on its own query criterion. CAMS still outperform the baselines under the setting.

**Improvement over the best classifier and policy.** Fig. 2(h) demonstrates that when provided with good policies, CAMS formulates a stronger policy which incurs no regret. CAMS has the potential to outperform an oracle, especially in rounds where the oracle does not make the optimal recommendation. For instance, in the stochastic version of CAMS (as shown in lines 22-23 and 30-32 of Fig. 1), CAMS recommends a model using a weighted majority vote among all policies, enabling the formation of a new policy in each round by amalgamating the strengths of each sub-optimal policy. This adaptive strategy can potentially outperform any single policy. Moreover, in most real-world scenarios and conducted experiments (as depicted in App. G.6), data streams may not be strictly stochastic, and therefore no single policy consistently performs the best. In such cases, CAMS's weighted policy may find an enhanced combination of "advices", leading to improved performance.

## 7 Conclusion

We introduced CAMS, an online contextual active model selection framework based on a novel model selection and active query strategy. The algorithm was motivated by many real-world use cases that need to make decision by taking both contextual information and the cost into consideration. We have demonstrated CAMS's compelling performance of using the minimum query cost to learn the optimal contextual model selection policy on several diverse online model selection tasks. In addition to the promising empirical performance, we also provided rigorous theoretical guarantees on the regret and query complexity for both stochastic and adversarial settings. We hope our work can inspire future works to handle more complex real-world model selection tasks (e.g. beyond classification or non-uniform loss functions, etc. where our analysis does not readily apply).