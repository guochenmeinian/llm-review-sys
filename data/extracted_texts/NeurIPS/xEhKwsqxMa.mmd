# Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning

Yingcong Li

University of California, Riverside

yli692@ucr.edu

&Kartik Sreenivasan &Angeliki Giannou

University of Wisconsin-Madison

{ksreenivasa2,giannou}@wisc.edu

&Dimitris Papailiopoulos

University of Wisconsin-Madison

dimitris@papail.io

&Samet Oymak

University of Michigan & UC Riverside

oymak@umich.edu

###### Abstract

Chain-of-thought (CoT) is a method that enables language models to handle complex reasoning tasks by decomposing them into simpler steps. Despite its success, the underlying mechanics of CoT are not yet fully understood. In an attempt to shed light on this, our study investigates the impact of CoT on the ability of transformers to in-context learn a simple to study, yet general family of compositional functions: multi-layer perceptrons (MLPs). In this setting, we find that the success of CoT can be attributed to breaking down in-context learning of a compositional function into two distinct phases: focusing on and filtering data related to each step of the composition and in-context learning the single-step composition function. Through both experimental and theoretical evidence, we demonstrate how CoT significantly reduces the sample complexity of in-context learning (ICL) and facilitates the learning of complex functions that non-CoT methods struggle with. Furthermore, we illustrate how transformers can transition from vanilla in-context learning to mastering a compositional function with CoT by simply incorporating additional layers that perform the necessary data-filtering for CoT via the attention mechanism. In addition to these test-time benefits, we show CoT helps accelerate pretraining by learning shortcuts to represent complex functions and filtering plays an important role in this process. These findings collectively provide insights into the mechanics of CoT, inviting further investigation of its role in complex reasoning tasks.

## 1 Introduction

The advent of transformers (Vaswani et al., 2017) has revolutionized natural language processing, paving the way for remarkable performance in a wide array of tasks. LLMs, such as GPTs (Brown et al., 2020), have demonstrated an unparalleled ability to capture and leverage vast amounts of data, thereby facilitating near human-level performance across a variety of language generation tasks. Despite this success, a deep understanding of their underlying mechanisms remains elusive.

Chain-of-thought prompting (Wei et al., 2022c) is an emergent ability of transformers where the model solves a complex problem (Wei et al., 2022b), by decomposing it into intermediate steps. Intuitively, this underlies the ability of general-purpose language models to accomplish previously-unseen complex tasks by leveraging more basic skills acquired during the pretraining phase. Compositional learning and CoT has enjoyed significant recent success in practical language modeling tasks spanning question answering, code generation, and mathematical reasoning (Perez et al., 2021; Imani et al., 2023; Yuan et al., 2023). In this work, we attempt to demystify some of the mechanics underlying this success and the benefits of CoT in terms of sample complexity and approximation power. Todo this we explore the role of CoT in learning multi-layer perceptrons (MLPs) in-context, which we believe can lead to a first set of insightful observations. Throughout, we ask:

_Does CoT improve in-context learning of MLPs, and what are the underlying mechanics?_

In this work, we identify and thoroughly compare three schemes as illustrated in Figure 1. (a) ICL: In-context learning from input-output pairs provided in the prompt, (b) CoT-I: Examples in the prompt are augmented with intermediate steps, (c) CoT-I/O: The model also outputs intermediate steps during prediction. Our main contributions are:

* **Decomposing CoT into filtering and ICL:** As our central contribution, we establish a rigorous and experimentally-supported abstraction that decouples CoT prompting into a _filtering phase_ and an _in-context learning (ICL) phase_. In _filtering_, the model attends to the relevant tokens within the prompt based on an instruction. In _ICL_, the model runs inference on the filtered prompt to output a _step_ and then moves onto the next _step_ in the chain. Our Theorem 1 develops a theoretical understanding of this two-step procedure and formalizes how filtering and ICL phases of CoT can be implemented via the self-attention mechanism to learn MLPs.
* in contrast to the \((kd)\) lower bound without step-augmented prompt. As predicted by our theory, our experiments (see Sec. 4) identify a striking universality phenomenon (as \(k\) varies) and also demonstrate clear approximation benefits of CoT compared to vanilla ICL.
* **Accelerated pretraining via learning shortcuts:** We construct deep linear MLPs where each layer is chosen from a discrete set of matrices. This is in contrast to the above setting, where MLP weights can be arbitrary. We show that CoT can dramatically accelerate pretraining by memorizing these discrete matrices and can infer all layers correctly from a _single_ demonstration. Notably the pretraining loss goes to zero step-by-step where each step _"learns to filter a layer"_. Together, these showcase how CoT identifies composable shortcuts to avoid the need for solving linear regression. In contrast, we show that ICL (without CoT) collapses to linear regression performance as it fails to memorize exponentially many candidates (due to lack of composition).

The paper is organized as follows. In Section 2, we introduce the problem setup. Section 3 states our main theoretical results which decouple CoT into filtering and ICL. Section 4 provides empirical investigations of CoT with 2-layer MLPs, which validates our theoretical findings. Finally, we

Figure 1: An illustration of ICL, CoT-I and CoT-I/O methods, using a 3-layer MLP as an example (top left, where \(\), \(\), \(^{1}\), \(^{2}\) denote input, output and hidden features respectivaly). The ICL method utilizes in-context examples in the form of \((,)\) and makes predictions directly based on the provided \(_{}\). Both CoT-I and CoT-I/O methods admit prompts with samples formed by \((,^{1},^{2},)\). However, CoT-I/O uniquely makes recurrent predictions by re-inputting the intermediate output (as shown on the right). The performance of these methods is shown on the bottom left, with a more detailed discussion available in Section 4.

elucidate the benefits of CoT during pretraining via experiments on deep linear MLPs in Section 5. Related work and discussion are provided in Sections 6 and 7, respectively.

## 2 Preliminaries and Setup

_Notation._ We denote the set \(\{1,2,,n\}\) as \([n]\). Vectors and matrices are represented in bold text (e.g., \(,\)), while scalars are denoted in plain text (e.g., \(y\)). The input and output domains are symbolized as \(\) and \(\) respectively (unless specified otherwise), and \(\), \(\) denote the input and output. Additionally, let \(\) be a set of functions from \(\) to \(\). Consider a transition function \(f\) where \(=f()\). In this section, we explore the properties of learning \(f\), assuming that it can be decomposed into simpler functions \((f_{})_{=1}^{L}\), and thus can be expressed as \(f=f_{L} f_{L-1} f_{1}\).

### In-context Learning

Following the study by Garg et al. (2022), the fundamental problem of vanilla in-context learning (ICL) involves constructing a prompt with input-output pairs in the following manner:

\[_{n}(f)=(_{i},_{i})_{i=1}^{n}_{ i}=f(_{i}).\] (P-ICL)

Here the transition function \(f:\) remains constant within a single prompt but can vary across prompts, and the subscript \(n\) signifies the number of in-context samples contained in the prompt. Considering language translation as an example, \(f\) is identified as the target language, and the prompt can be defined as \(()\) = ((_apple, manzana_), (_ball, pelota_), \(\)) or \(()\)=((_cat, chat_), (_flower, fleur_), \(\)). Let TF denote any auto-regressive model (e.g., Decoder-only Transformer). The aim of in-context learning is to learn a model that can accurately predict the output, given a prompt \(\) and the test input \(_{}\), as shown in the following equation:

\[(_{n}(),_{})(_{})\] (2.1)

where \(\) is the test function which may differ from the functions used during training. Previous work (Zhou et al., 2022; Li et al., 2023; Li et al., 2023) has demonstrated that longer prompts (containing more examples \(n\)) typically enhance the performance of the model.

### Chain-of-thought Prompt and Prediction

As defined in (P-ICL), the prompt in vanilla ICL only contains input-output pairs of the target function. This demands that the model learns the function \(f\) in one go, which becomes more challenging as \(\) grows more complex, since larger models and increased prompt length (\(n\)) are needed to make correct predictions (as depicted by the green curves in Figures 5 and 6). Existing studies on chain-of-thought methods (e.g., (Wei et al., 2022)) observed that prompts containing step-by-step instructions assist the model in decomposing the function and making better predictions. Specifically, consider a function composed of \(L\) subfunctions, represented as \(f:=f_{L} f_{L-1} f_{1}\). Each intermediate output can be viewed as a step, enabling us to define a length-\(n\) CoT prompt related to \(f\) with \(L\) steps (expressed with \(^{},[L]\)) as follows:

\[_{n}(f)=(_{i},_{i}^{1},_{i}^{L-1},_{i}^{L })_{i=1}^{n}_{i}^{}=f_{}(_{i}^{- 1}),\;[L].\] (P-CoT)

Here \(_{i}=_{i}^{0}\), \(_{i}=_{i}^{L}\) and \(f_{}_{}\), which implies that \(f_{L}_{1}:=\).

Next we introduce two methodologies for making predictions within the CoT framework:

**CoT over input only (CoT-I).** Contrasted with ICL, CoT-I considers step-by-step instructions as inputs, nonetheless, the prediction for the last token is performed as a single entity. Our experiments indicate that this approach lowers the sample complexity for TF to comprehend the function \(\) being learned (see the orange curves in Figures 5 and 6). The CoT-I prediction aligns with Eq. (2.1) as follows, while the prompt is determined by (P-CoT).

\[(_{n}(),_{}) (_{}).\] (2.2)

**CoT over both input and output (CoT-I/O).** Despite the fact that CoT-I improves the sample complexity of learning \(\), the TF must still possess the capacity to approximate functions from the function class \(\), given that the prediction is made in one shot. To mitigate this challenge, we consider a scenario where in addition to implementing a CoT prompt, we also carry out CoT predictions. Specifically, for a composed problem with inputs formed via (P-CoT), the model recurrently makes \(L\)-step predictions as outlined below:

Step 1: \[(_{n}(),_{}):=}^{1}\] Step 2: \[(_{n}(),_{},}^{1 }):=}^{2}\] \[\] Step \[L\] : \[(_{n}(),_{},}^ {1},}^{L-1})(_{}),\] (2.3)

where at each step (step \(\)), the model outputs an intermediate step (\(}^{}\)) which is then fed back to the input sequence to facilitate the next-step prediction (\(}^{+1}\)). Following this strategy, the model only needs to learn the union of the sub-function sets, \(_{=1}^{L}_{}\), whose complexity scales linearly with the number of steps \(L\). Empirical evidence of the benefits of CoT-I/O over ICL and CoT-I in enhancing sample efficiency and model expressivity is reflected in the blue curves shown in Figures 5 and 6.

### Model Training

In Figure 1 and Section 2, we have discussed vanilla ICL, CoT-I and CoT-I/O methods. Intuitively, ICL can be viewed as a special case of CoT-I (or CoT-I/O) if we assume only one step is performed. Consequently, we will focus on implementing CoT-I and CoT-I/O for model training in the following.

Consider the CoT prompt as in (P-CoT), and assume that \(_{}\), and \(f_{}_{},[L]\), where \(L\) denotes the number of compositions/steps, such that the final prediction should approximate \(f()=f_{L}(f_{L-1} f_{1}()):=\). We define \((},):\) as a loss function. For simplicity, we assume \(f_{}( f_{1}())\), \([L]\). Let \(N\) represent the in-context window of \(\), which implies that \(\) can only admit a prompt containing up to \(N\) in-context samples. Generally, our goal is to ensure high prediction performance given any length-\(n\) prompt, where \(n[N]\). To this end, we train the model using prompts with length from \(1\) to \(N\) equally and aim to minimize the averaged risk over different prompt size. Assuming the model TF is parameterized by \(\) and considering meta learning problem, the objective functions for CoT-I and CoT-I/O are defined as follows.

\[}^{}=_{}_{(_{n })_{n=1}^{N},(f_{})_{=1}^{L}}[_{n=1}^{N}(}_{n},f(_{n}))]\]

and

\[}^{}=_{}_{(_{ n})_{n=1}^{N},(f_{})_{=1}^{L}}[_{n=1}^{N}_{ =1}^{L}(}^{}_{n},^{}_{n})]\]

where \(}_{n}=(_{n}(f),_{n})\) and \(}^{}_{n}=(_{n}(f),_{n}^{ -1}_{n})\). \(_{n}(f)\) is given by (P-CoT), and as mentioned previously, \(^{0}=\) and \(^{L}=\). All \(\) and \(f_{}\) are independent, and we take the expectation of the risk over their respective distributions.

## 3 Provable Approximation of MLPs via Chain-of-Thought

In this section, we present our theoretical findings that demonstrate how CoT-I/O can execute filtering over the CoT prompt, thereby learning a 2-layer MLP with input dimension of \(d\) and hidden dimension of \(k\), akin to resolving \(k\)\(d\)-dimensional ReLU problems and \(1\)\(k\)-dimensional linear regression problem. Subsequently, in Section 4.1, we examine the performance of CoT-I/O when learning 2-layer random MLPs. Our experiments indicate that CoT-I/O needs only \(O((d,k))\) in-context samples to learn the corresponding MLP.

We state our main contribution of establishing a result that decouples CoT-based in-context learning (CoT-I/O) into two phases: (1) _Filtering Phase:_ Given a prompt that contains features of multiple MLP layers, retrieve only the features related to a target layer to create an ICL prompt. (2) _ICL Phase:_ Given filtered prompt, learn the target layer weights through gradient descent. Combining these two phases, and looping over all layers, we will show that there exists a transformer architecture such that CoT-I/O can provably approximate a multilayer MLP up to a given resolution. An illustration is provided in Figure 2. To state our result, we assume access to an oracle that performs linear regression and consider the consider the condition number of the data matrix.

**Definition 1** (MLP and condition number): _Consider a multilayer MLP defined by the recursion \(_{i}^{}=(_{}_{i}^{-1})\) for \([L]\), \(i[n]\) and \(_{i}^{0}=_{i}\). Here \((x):=( x,x)\) is a Leaky-ReLU activation with \(1>0\). Define the feature matrix \(_{}=[_{1}^{}\ \ _{n}^{}]^{}\) and define its condition number \(_{}=_{}(_{})/_{}(_{})\) (with \(_{}:=0\) for fat matrices) and \(_{}=_{0<L}_{}\)._

**Assumption 1** (Oracle Model): _We assume access to a transformer \(_{}\) which can run \(T\) steps of gradient descent on the quadratic loss \(()=_{i=1}^{n}(y_{i}-^{}_{i})^{2}\) given a prompt of the form \((_{1},y_{1},,_{n},y_{n})\)._

We remark that this assumption is realistic and has been formally established by earlier work (Giannou et al., 2023; Akyurek et al., 2022). Our CoT abstraction builds on these to demonstrate that CoT-I/O can call a blackbox TF model to implement a compositional function when combined with filtering.

We now present our main theoretical contribution. Our result provides a transformer construction that first filters a particular MLP layer from the prompt through the attention mechanism, then applies in-context learning, and repeats this procedure to approximate the MLP output. The precise statement is deferred to the supplementary material.

**Theorem 1** (CoT\(\)Filtering+ICL): _Consider a CoT prompt \(_{n}(f)\) generated from an \(L\)-layer MLP \(f()\) as described in Definition 1, and assume given test example \((_{},_{}^{1},_{} ^{L})\). For any resolution \(>0\), there exists \(=()\), iteration choice \(T=(_{}^{2}(1/))\), and a backend transformer construction \(_{}\) such that the concatenated transformer \(=_{}_{}\) implements the following: Let \((}^{i})_{i=0}^{-1}\) denote the first \(-1\) CoT-I/O outputs of TF where \(}^{0}=_{}\) and set \([]=(_{n}(f),_{},}^{1}}^{-1})\). At step \(\), TF implements_

1. **Filtering.** _Define the filtered prompt with input/output features of layer_ \(\)_,_ \[_{n}^{}=,_{1}^{-1},\ \ ,_{n}^{-1},\ \ ,}^{-1}\\ ,\ ,\ _{1}^{},\ ,\ _{n}^{} ,\ .\] _There exists a fixed projection matrix_ \(\) _that applies individually on tokens such that the backend output obeys_ \(\|(_{}([]))-_{n}^{} \|\)_._
2. **Gradient descent.** _The combined model obeys_ \(\|([])-_{}^{}\|/L\)_._

\(_{}\) _has constant number of layers independent of_ \(n\) _and_ \(T\)_. Consequently, after_ \(L\) _rounds of CoT-I/O, TF outputs_ \(f(_{})\) _up to_ \(\) _accuracy._

Figure 2: Depiction of Theorem 1 that formalizes CoT as a Filtering \(+\) ICL procedure. In this illustration, we utilize in-context samples as depicted in Figure 1, characterizing the CoT prompt by \((,^{1},^{2},)\). We show a transformer TF, composed of \(_{}_{}\). \(_{}\) facilitates the implementation of filtering on the CoT prompt (refer to (P-CoT), illustrated on the left side of the figure), subsequently generating vanilla ICL prompts (refer to (P-ICL), illustrated on the right side of the figure). During each step of the ICL process, \(_{}\) employs gradient descent techniques to address and solve sub-problems, exemplified by linear regression in the context of solving MLPs.

**Remark 1**: _Note that, this result effectively shows that, with a sufficiently good blackbox transformer \(_{}\) (per Assumption 1), CoT-I/O can learn an \(L\)-layer MLP using in-context sample size \(n>_{[L]}d_{}\) where \(d_{}\) is the input dimension of \(\)th layer. This is assuming condition number \(_{}\) of the problem is finite as soon as all layers become over-determined. Consequently, CoT-I/O needs \((k,d)\) sample complexity to learn a two layer MLP. This provides a formal justification for the observation that empirical CoT-I/O performance is agnostic to \(k\) as long as \(k d\)._

We provide the concrete filtering statements based on the transformer architecture in Appendix A, and the key components of our construction are the following: (i) Inputs are projected through the embedding layer in which a set of encodings, an enumeration of the tokens (\(1,2,,N\)), an enumeration of the layers (\(1,2,,L\)) and an identifier for each layer already predicted are all attached. Notice that this "modification" to the input only depends on the sequence length and is agnostic to the token to-be-predicted. This allows for an automated looping over \(L\) predictions. (ii) We use this information to extract the sequence length \(N\) and the current layer \(\) to-be-predicted. (iii) With these at hand, we construct an 'if-then' type of function using the ReLU layers to filter out the samples that are not needed for the prediction.

## 4 Experiments with 2-layer Random MLPs

For a clear exposition, we initially focus on two-layer MLPs, which represent 2-step tasks (e.g., \(L=2\)). We begin by validating Theorem 1 using the CoT-I/O method, demonstrating that in-context learning for a 2-layer MLP with \(d\) input dimensions and \(k\) hidden neurons requires \(O((d,k))\) samples. The results are presented in Section 4.1. Subsequently, in Section 4.2, we compare three different methods: ICL, CoT-I, and CoT-I/O. The empirical evidence highlights the advantages of CoT-I/O, showcasing its ability to reduce sample complexity and enhance model expressivity.

**Dataset.** Consider 2-layer MLPs with input \(^{d}\), hidden feature (step-1 output) \(^{k}\), and output \(y\). Here, \(=f_{1}():=()_{+}\) and \(y=f_{2}():=^{}\), with \(^{k d}\) and \(^{k}\) being the parameters of the first and second layer, and \((x)_{+}=(x,0)\) being ReLU activation. The function is composed as \(y=^{}()_{+}\). We define the function distributions as follows: each entry of \(\) is sampled via \(_{ij}(0,)\), and \((0,_{k})\), with inputs being randomly sampled through \((0,_{d})\)1. We apply the quadratic loss in our experiments. To avoid the implicit bias due to distribution shift, both training and test datasets are generated following the same strategy.

Figure 4: We decouple the composed risk of predicting 2-layer MLPs into risks of individual layers.

Figure 3: Solving 2-layer MLPs with varying input dimension \(d\) and hidden neuron size \(k\).

[MISSING_PAGE_EMPTY:7]

### Comparative Analysis of ICL, CoT-I and CoT-I/O

**Varying model sizes (Figure 5).** We initially assess the benefits of CoT-I/O over ICL and CoT-I across different TF models. With \(d=10\) and \(k=8\) fixed, we train three different GPT-2 models: standard, small and tiny GPT-2. The small GPT-2 has \(6\) layers, \(4\) attention heads per layer and \(128\) dimensional embeddings. The standard GPT-2 consists of twice the number of layers, attention heads and embedding dimensionality compared to the small GPT-2, and tiny GPT-2, on the other hand, possesses only half of these hyperparameters compared to the small GPT-2. We evaluate the performance using prompts containing \(n\) in-context samples, where \(n\) ranges from \(1\) to \(N\) (\(N=100\)). The associated test risks are displayed in Figs. 5(b), 5(c) and 5(d). The blue, orange and green curves correspond to CoT-I/O, CoT-I and ICL, respectively. In Fig. 5(a), we present the averaged risks. The results show that using CoT-I/O, the small GPT-2 can solve 2-layer MLPs with approximately \(60\) samples, while CoT-I requires the standard GPT-2. Conversely, ICL is unable to achieve zero test risk even with the standard GPT-2 model and up to \(100\) samples. This indicates that to learn 2-layer MLPs in a single shot, ICL requires at least \((dk+d)\) samples to restore all function parameters. Conversely, CoT-I and CoT-I/O can leverage implicit samples contained in the CoT prompt. Let \(f_{1}_{1}\) (first layer) and \(f_{2}_{2}\) (second layer). By comparing the performance of CoT-I and CoT-I/O, it becomes evident that the standard GPT-2 is capable of learning the composed function \(f=f_{2} f_{1}\), which the small GPT-2 cannot express.

**Varying MLP widths (Figure 6).** Next, we explore how different MLP widths impact the performance (by varying the hidden neuron size \(k\{4,8,16\}\)). The corresponding results are depicted in Figure 6. The blue, orange and green curves in Fig. 6(b), 6(c) and 6(d) correspond to hidden layer sizes of \(k=4\), \(8\), and \(16\), respectively. Fig. 6(a) displays the averaged risks. We keep \(d=10,\ N=100\) fixed and train with the small GPT-2 model. As discussed in Section 4.1, CoT-I/O can learn a 2-layer MLP using around \(60\) samples for all \(k=4,8,16\) due to its capability to deconstruct composed functions. However, CoT-I can only learn the narrow MLPs with \(k=4\), and ICL is unable to learn any of them. Moreover, we observe a substantial difference in the performance of ICL and CoT-I with varying \(k\) (e.g., see averaged risks in Fig. 6(a)). This can be explained by the fact that enlarging \(k\) results in more complex \(_{1}\) and \(_{2}\), thus making the learning of \(=_{2}_{1}\) more challenging for ICL and CoT-I.

## 5 Further Investigation on Deep Linear MLPs

In Section 4, we have discussed the approximation benefits of CoT-I/O and how it in-context learns 2-layer random MLPs by parallel learning of \(k\)\(d\)-dimensional ReLU and 1 \(k\)-dimensional linear regression. In this section, we investigate the capability of CoT-I/O in learning longer compositions. For brevity, we will use CoT to refer to CoT-I/O in the rest of the discussion.

**Dataset.** Consider \(L\)-layer linear MLPs with input \(^{d}(0,_{d})\), and output generated by \(=_{L}_{L-1}_{1}\), where the \(\)th layer is parameterized by \(_{}^{d d}\), \([L]\). In this work, to better understand the emerging ability of CoT, we assume that each layer draws from the same discrete sub-function set \(}=\{_{k}:}_{k}^{}}_{k }=,k[K]\}\)3. Therefore, to learn the \(L\)-layer neural net, CoT only needs to learn \(}\) with \(|}|=K\), whereas ICL needs to learn the function set \(}^{L}\), which contains \(K^{L}\) random matrices.

Figure 7: Evaluations over deep linear MLPs using CoT-I/O and ICL where CoT-\(X\) represents the \(X\)-step CoT-I/O. Fig. 7(a) illustrates point-to-point meta results where the model is trained with substantial number of samples. In contrast, Fig. 7(b) displays the one-shot performance (with only one in-context sample provided) when making predictions during training.

Composition ability of CoT (Figures 7).Set \(d=5\), \(L=6\) and \(K=4\). At each round, we randomly select \(L\) matrices \(_{}\), \([L]\) from \(\) so that for any input \(\), we can form a chain

\[^{1}^{2} {s}^{6}:=,\]

where \(^{}=_{}^{-1}\), \([L]\) and \(^{0}:=\). Let CoT-\(X\) denote \(X\)-step CoT-I/O method. For example, the in-context sample of CoT-6 has form of \((,^{1},^{2},^{5},)\), which contains all the intermediate outputs from each layer; while CoT-3, CoT-2 have prompt samples formed as \((,^{2},^{4},)\) and \((,^{3},)\) respectively. In this setting, ICL is also termed as CoT-1, as its prompt contains only \((,)\) pairs. To solve the length-6 chain, CoT-\(X\) needs to learn a model that can remember \(4^{6/X}\) matrices. Therefore, ICL is face a significantly challenge sincd it needs to remember \(4^{6}=4,096\) matrices (all combinations of the 4 matrices used for training and testing) compared to just 4 for CoT-6.

We train small GPT-2 models using the CoT-2/-3/-6 and ICL methods, and present the results in Fig. 7(a). As evident from the figure, the performance curves of CoT-2 (orange), CoT-3 (green) and CoT-6 (red) overlap, and they can all make precise predictions in one shot (given an in-context example \(n=1\)). It seems that TF has effectively learned to remember up to \(64\) matrices (for CoT-2) and compose up to \(6\) layers (for CoT-6). However, ICL (blue) struggles to learn the 6-layer MLPs in one shot. The black dashed curve shows the solution for linear regression \(y=^{}\) computed directly via least squares given \(n\) random training samples, where \(\) is the input and \(y\) is from the output of the 6-layer MLPs (e.g., \(\)). The test risks for \(n=1, 10\) are plotted (in Fig. 7(a)), which show that the ICL curve aligns with the least squares performance. This implies that, instead of remembering all \(4,096\) matrices, ICL solves the problem from the linear regression phase.

In addition to the meta-learning results which highlight the approximation benefits of multi-step CoT, we also investigate the convergence rate of CoT-2/-3/-6 and ICL, with results displayed in Fig. 7(b). We test the one-shot performance during training and find that CoT-6 converges fastest. This is because it has the smallest sub-function set, and given the same tasks (e.g., deep neural nets), shortening the chain leads to slower convergence. This supports the evidence that taking more steps facilitates faster and more effective learning of complex problems.

Evidence of Filtering (Figure 8).As per Theorem 1 and the appendix, transformers can perform filtering over CoT prompts, and the results from 2-layer MLPs align with our theoretical findings. However, can we explicitly observe filtering behaviors? In Fig. 8(a), we display the results of the first 50k iterations from Fig. 7(b), and observe risk drops in CoT-6 (red) at the 15k and 25k iteration (shown as grey dotted and dashed lines). Subsequently, in Fig. 8(b), we plot the test risk of each layer prediction (by feeding the model with correct intermediate features not the predicted ones), where CoT-6 (red) predicts the outputs from all 6 layers (\(^{1},,^{L}\)). From these figures, we can identify risk drops when predicting different layers, which appear at either 15k (for layer 2, 3, 4, 5, 6) or 25k (for layer 1) iteration. This implies that the model learns to predict each step/layer function independently. Further, we test the filtering evidence of the \(\)th layer by filling irrelevant positions with random features. Specifically, an in-context example is formed by

\[(^{0},,^{-1},^{},^{+1},^ {L}),\ \ \ \ ^{}=_{}(^{-1})\ \ \ \ (0,_{d}).\]

The test risks are represented by black dotted curves in Fig. 8(b), which aligned precisely with the CoT-6 curves (red). This signifies that each layer's prediction concentrate solely on the corresponding intermediate steps in the prefix, while disregarding irrelevant features. This observation provides evidence that the process of filtering is indeed performed.

Figure 8: Fig. 8(a) is generated by magnifying the initial 50k iterations of Fig. 7(b), and we decouple the composed risks from predicting \(6\)-layer linear MLPs into predictions for each layer, and the results are depicted in Fig. 8(b). Additional implementation details can be found in Section 5.

## 6 Related Work

With the success of LLMs and prompt structure (Lester et al., 2021), there is growing interest in in-context learning (ICL) from both theoretical and experimental lens (Garg et al., 2022, Brown et al., 2020, von Oswald et al., 2022, Dai et al., 2022, Min et al., 2022, Lyu et al., 2022, Li et al., 2023c, Balim et al., 2023, Xie et al., 2021, Min et al., 2021, Wei et al., 2023, Li et al., 2023a). As an extension, chain-of-thought (CoT) prompts have made impressive improvements in performing complex reasoning by decomposing it into step-by-step intermediate solutions (Wei et al., 2022c, Narang et al., 2020, Lampinen et al., 2022, Wei et al., 2022b, Zhou et al., 2022, Nye et al., 2021, Velickovic and Blundell, 2021, Lanchantin et al., 2023), which in general, shows the ability of transformer in solving compositional functions. Lee et al. (2023), Dziri et al. (2023) study the problem of teaching arithmetic to small transformers and show that breaking the task down into small reasoning steps allows the model to learn faster. Li et al. (2023b) show that small student transformer models can learn from rationalizations sampled from significantly larger teacher models. The idea of learning how to compose skills has been well studied in other literatures (Sahni et al., 2017, Liska et al., 2018). More specifically, for the problem of learning shallow networks, there are several well known hardness results Goel et al. (2017, 2020), Zhang et al. (2019). In particular, Hahn and Goyal (2023) shows a formal learnability bound which implies that compositional structure can benefit ICL. However, most of the work focuses on investigating empirical benefits and algorithmic designing of CoT, and there exists little effort studying the underlying mechanisms of CoT.

Considering the expressivity of the transformer architecture itself, Yun et al. (2019) showed that TFs are universal sequence to sequence approximators. More recently, Giannou et al. (2023) use an explicit construction to show that shallow TFs can be used to run general purpose programs as long as we loop them. Other works have also shown the turing-completeness of the TF architecture but these typically require infinite/high precision and recursion around attention layers (Wei et al., 2022a, Perez et al., 2019, 2021, Liu et al., 2022). Closer to our work, Akyurek et al. (2022), Von Oswald et al. (2023), von Oswald et al. (2023) prove that a transformer with constant number of layers can implement gradient descent in solving linear regression, and Giannou et al. (2023) introduce similar results by looping outputs back into inputs. Ahn et al. (2023) prove this result from an optimization perspective and show that the global minimum of the training objective implements a single iteration of preconditioned gradient descent for transformers with a single layer of linear attention. Zhou et al. (2023) introduce the RASP-Generalization Conjecture which says that Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths. In this work, we prove CoT can be treated as: first apply filtering on the CoT prompts using special construction, and then in-context learn the filtered prompt.

## 7 Conclusion, Limitations, and Discussion

In this work, we investigate chain-of-thought prompting and shed light on how it enables compositional learning of multilayer perceptrons step-by-step. Specially, we have explored and contrasted three methods: ICL, CoT-I and CoT-I/O, and found that CoT-I/O facilitates better approximation and faster convergence through looping and sample efficiency. Additionally, we empirically and theoretically demonstrated that to learn a 2-layer MLP with \(d\)-dimensional input and \(k\) neurons, CoT-I/O requires \(((d,k))\) in-context samples whereas ICL runs into approximation error bottlenecks.

While we have provided both experimental and theoretical results to validate the advantages of CoT, it is important to note that our analysis in the main text pertains to in-distribution scenarios. In an effort to address this limitation and demonstrate the robustness of CoT, we have conducted additional simulations, as detailed in Appendix C.1, where the test samples follow a different distribution than the training examples. Also, we note that our focus has been primarily on MLP-based tasks, where the subproblems are essentially instances of simple linear regression. It would be valuable to explore how CoT might influence the training of tasks characterized by more complex structures, longer compositional chains, and a broader variety of subproblems.

There are several interesting avenues for future research to build on our findings. To what extent does our decoupling of CoT (filtering followed by ICL) align with the empirical evidence in practical problems such as code generation and mathematical reasoning? We have shown that CoT-I/O can rely on a linear regression oracle to learn an MLP. To what extent can transformers approximate MLPs without CoT-I/O (e.g. with CoT-I) and what are the corresponding lower/upper bounds?

## 8 Acknowledgements

This work was supported by the NSF CAREER awards #2046816 and #1844951, NSF grant #2212426, AFOSR & AFRL Center of Excellence Award FA9550-18-1-0166, a Google Research Scholar award, an Adobe Data Science Research award, an Army Research Office grant W911NF2110312, and an ONR Grant No. N00014-21-1-2806.