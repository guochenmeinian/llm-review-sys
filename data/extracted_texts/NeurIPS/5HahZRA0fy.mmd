# Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations

**Tomas Vaskevicius and Lenaic Chizat**

Institute of Mathematics

Ecole Polytechnique Federale de Lausanne (EPFL) Station Z

CH-1015 Lausanne

Switzerland

## 1 Introduction

The Wasserstein distance between two probability distributions measures the least amount of effort needed to reconfigure one measure into the other. Unlike other notions of distances based solely on the numerical values taken by the distribution functions (e.g., the Kullback-Leibler divergence), the Wasserstein distance incorporates an additional layer of complexity by considering pairwise distances between distinct points, measured by some predetermined cost function. As a result, the Wasserstein distances can be seen to lift the geometry of the underlying space where the probability measures are defined to the space of the probability measures itself. This allows for a more thorough and geometrically nuanced understanding of the relationships between different probability measures, which proved to be a versatile tool of increasing importance in a broad spectrum of areas.

Given a collection of probability measures and an associated set of positive weights that sum to one, the corresponding Wasserstein barycenter minimizes the weighted sum of Wasserstein distances to the given measures. In the special case of two measures and the squared Euclidean cost function, Wasserstein barycenters concide with the notion of McCann's displacement interpolation introduced in the seminal paper . The general case, encompassing an arbitrary number of measures, was first studied by Agueh and Carlier , where they also demonstrated a close link between Wasserstein barycenters and the multi-marginal optimal transport problem . Recent years have witnessed an increasing number of applications of Wasserstein barycenters across various scientific disciplines. See, for instance, the following sample of works in economics [16; 11], statistics , image processing , and machine learning , among other areas. For further background and references we point the interested reader to the introductory surveys [47; 45] and the textbooks [58; 59; 54; 46; 28].

Despite their compelling theoretical characteristics, the computation of Wasserstein barycenters poses significant computational challenges, particularly in large-scale applications. While Wasserstein barycenters can be computed in polynomial time for fixed dimensions , the approximation of Wasserstein barycenters is known to be NP-hard . Currently employed methods for approximating Wasserstein barycenters are predominantly based on space discretizations. Unfortunately, such strategies are only computationally practical for problems of relatively modest scale. Although there are a handful of grid-free techniques available for approximating Wasserstein barycenters (e.g., [18; 35; 23; 40]), we are not aware of any existing methods that provide bounds on computational complexity. One contribution of the present paper is to introduce a method that in some regimes can provably approximate Wasserstein barycenters without relying on space discretizations, but instead employing approximate Monte Carlo sampling.

More broadly, the difficulties associated with computation of the optimal transport cost has prompted the exploration of computationally efficient alternatives, leading to the consideration of regularized Wasserstein distances. Among these, the entropic penalty has emerged as one of the most successful in applications. The practical success of entropic penalization can be attributed to Sinkhorn's algorithm , which enables efficient and highly parallelizable computation, an algorithm that gained substantial traction in the machine learning community following the work of Cuturi . It is worth noting that entropic Wasserstein distances are of intrinsic interest, beyond their approximation capabilities. Indeed, they hold a rich historical connection to the Schrodinger bridge problem [55; 60; 26], as highlighted in the recent surveys [39; 14]. Furthermore, they increasingly serve as an analytically convenient tool for studying the unregularized optimal transport problem (see, e.g., [38; 31; 27; 15]) and they underlie some favorable statistical properties that are currently under active investigation; see the works [43; 30; 24; 48; 52; 49] and the references therein.

Let us now define the entropic optimal transport cost. Consider two probability measures, \(\) and \(\), both supported on \(\), and let \(c:[0,)\) be a cost function. The entropic Wasserstein distance with a regularization level \(>0\) is defined as

\[T_{}(,)=_{(,)}_{(X,Y)}[ c(X,Y)]+(,),\] (1)

where \((,)\) denotes the set of probability measures on \(\) with marginal distributions equal to \(\) and \(\), and \((,)\) is the Kullback-Leibler divergence. When \( 0\), the regularized cost \(T_{}(,)\) converges to the unregularized Wasserstein distance. Various properties of entropic optimal transport can be found in the recent lecture notes by Leonard .

To develop efficiently computable approximations for Wasserstein barycenters, a natural approach is to replace the unregularized Wasserstein cost with the minimizer of the weighted sum of entropy-regularized costs. This method was first explored by Cuturi and Doucet  and it has gained additional traction in the recent years. There is some flexibility in the definition of (1), which arises from substituting the reference product measure \(\) with alternatives such as the Lebesgue measure. Consequently, various notions of entropic barycenters have emerged in the literature, which can be unified through the following optimization problem:

\[_{}_{j=1}^{k}w_{j}T_{}(,^{j})+(, _{}).\] (2)

Here \(^{1},,^{k}\) are the probability measures whose barycenter we wish to compute and \(w_{1},,w_{k}\) are positive weights that sum to one. The inner regularization strength is denoted by \(>0\) while \(>0\) is the outer regularization strength. The measure \(_{}\) is an arbitrary reference measure, the support of which dictates the support of the computed barycenter. For instance, if we take \(_{}\) to be a uniform measure on a particular discretization of the underlying space, we are dealing with a fixed-support setup. On the other hand, letting \(_{}\) be the Lebesgue measure puts us in the free-support setup. We shall refer to the minimizer of (2) as the \((,)\)-barycenter, which exists and is unique due to the strict convexity of the outer regularization penalty; however, uniqueness may no longer holds when \(=0\).

The objective (2) was recently studied in ; it also appeared earlier in  for the special case \(\), where stochastic approximation algorithms were considered for the computation of fixed-support barycenters. In [17, Section 1.3], it is discussed how various choices of \((,)\) relate to Barycenters previously explored in the literature. To provide a brief overview, \((0,0)\) are the unregularizedWasserstein barycenters studied in . Inner-regularized barycenters \((,0)\) introduce a shrinking bias; this can be seen already when \(k=1\), in which case the solution computes a maximum-likelihood deconvolution . The \((,)\)-barycenters were considered in [21; 7; 22; 10; 36]; they introduce a blurring bias. Likewise, blurring bias is introduced by the outer-regularized barycenters \((0,)\), studied in [9; 12]. The only case not covered via the formulation (2) appears to be the one of debiased Sinkhorn barycenters [51; 33], for which an algorithm exists but without computational guarantees. Of particular interest are the \((,/2)\) barycenters: the choice \(=/2\) for smooth densities yields approximation bias of order \(^{2}\), while the choice \(=\) results in bias of order \(\), which is significantly larger than \(^{2}\) in the regimes of interest. This is a new notion of entropic barycenters that was unveiled in the analysis of . We provide the first convergence guarantees for this type of barycenters.

The regularity, stability, approximation, and statistical sample complexity properties of \((,)\)-barycenters were investigated in . However, the question of obtaining non-asymptotic convergence guarantees for the computation of \((,)\)-barycenters with arbitrary regularization parameters was not addressed therein. In particular, the \((,/2)\) case, which has stood out due to its compelling mathematical features, has not yet been addressed in the existing literature. This gap is addressed by the present paper; we summarize our contributions in the following section.

### Contributions

The remainder of this paper is organized as follows: Section 2 provides the necessary background on entropic optimal transport and a particular dual problem of the doubly regularized entropic objective (2). Section 3 introduces a damped Sinkhorn iteration scheme and complements it with convergence guarantees. An approximate version of the algorithm together with convergence results and implementation details is discussed in Section 4. We summarize our key contributions:

1. Lemma 1, presented in Section 3, demonstrates that bounds on the dual suboptimality gap for the dual problem (8), defined in Section 2.2, can be translated into Kullback-Leibler divergence bounds between the \((,)\)-barycenter and the barycenters corresponding to dual-feasible variables. This translation enables us to formulate all our subsequent results in terms of optimizing the dual objective (8).
2. In Section 3, we introduce a damped Sinkhorn scheme (Algorithm 1) that can be employed to optimize \((,)\)-barycenters for any choice of regularization parameters. The damping factor \((1,/)\) accommodates the degrading smoothness properties of the dual objective (8) as a function of decreasing outer regularization parameter \(\). The introduced damping of the Sinkhorn iterations is, in fact, necessary and it is one of our core contributions: undamped exact scheme can be experimentally shown to diverge as soon as \(</2\).
3. The main result of this paper is Theorem 1 proved in Section 3. It provides convergence guarantees for Algorithm 1 with arbitrary choice of regularization parameters \(,>0\). This, in particular, results in the first algorithm with guarantees for computing \((,/2)\) barycenters. For smooth densities, these barycenters incur a bias of order \(^{2}\) in contrast to the predominantly studied \((,)\) barycenters that incur bias of order \(\).
4. In Section 4, we describe Algorithm 2, an extension of Algorithm 1 that allows us to perform inaccurate updates. We formulate sufficient conditions on the inexact updates oracle under which the errors in the convergence analysis do not accumulate. Section 4.1 details an implementation of this inexact oracle, based on approximate Monte Carlo sampling.
5. Theorem 2 proved in Section 4 furnishes convergence guarantees for Algorithm 2. When combined with the implementation of the inexact oracle described in Section 4.1, this yields a provably convergent scheme for a grid-free computation of entropic Wasserstein barycenters between discrete distributions, provided sufficient regularity on the domain \(\) and the cost function \(c\).
6. Appendix F complements our theoretical results with numerical experiments. Our simulations experimentally confirm the necessity of damping when \(</2\). They also provide experimental support for the suggested damping factor in Algorithms 1 and 2.

## 2 Background and Notation

This section provides the background material on doubly regularized entropic Wasserstein barycenters and introduces the notation used throughout the paper. In the remainder of the paper, let \(\) be a compact and convex subset of \(^{d}\) with a non-empty interior. Let \(()\) denote the set of probability measures on \(\) endowed with Borel sigma-algebra. Let \(c:[0,)\) be a cost function such that \(c_{}()=_{x,x^{}}c(x,x^{})<\). We denote by \((,)\) the Kullback-Leibler divergence, \(\|\|_{}\) is the total-variation norm, and \(\|f\|_{}=_{x}f(x)-_{x^{}}f(x^{})\) is the oscillation norm. Given two measures \(,^{}\), the notation \(^{}\) denotes that \(\) is absolutely continuous with respect to the measure \(^{}\); in this case \(d/d^{}\) denotes the Radon-Nikodym derivative of \(\) with respect to \(^{}\). Finally, throughout the paper \(w\) denotes a vector of \(k\) strictly positive elements that sum to one.

### Entropic Optimal Transport

For any \(,()\) define the entropy regularized optimal transport problem by

\[T_{}(,)=_{(,)}_{(X,Y)}[c (X,Y)]+(,),\] (3)

where \(\) is the Kullback-Leibler divergence and \((,)()\) is the set of probability measures such that for any \((,)\) and any Borel subset \(A\) of \(\) it holds that \((A)=(A)\) and \(( A)=(A)\).

Let \(E_{}^{,}:L_{1}() L_{1}()\) be the function defined by

\[E_{}^{,}(,) =_{X}[(X)]+_{Y}[(Y)]\] \[+(1-_{}_{} ()(dy)(dx)).\]

The entropic optimal transport problem (3) admits the following dual representation:

\[T_{}(,)=_{,}E_{}^{,}(,).\] (4)

For any \(\) define

\[_{}_{ L_{1}()}E_{}^{,}(,).\]

The solution is unique \(\)-almost everywhere up to a constant; we fix a particular choice

\[_{}(x)=-(_{}()(dy)).\]

Likewise, we denote \(_{}=_{ L_{1}()}E_{}^{,}(,)\) with the analogous expression to the one given above, interchanging the roles of \(\) and \(\). Then, the maximum in (4) is attained by any pair \((^{*},^{*})\) such that \(^{*}=_{^{*}}\) and \(^{*}=_{^{*}}\); such a pair is said to solve the Schrodinger system and it is unique up to translations \((^{*}+a,^{*}-a)\) by any constant \(a\). The optimal coupling that solves the primal problem (3) can be obtained from the pair \((^{*},^{*})\) via the primal-dual relation

\[^{*}(dx,dy)=((x)+^{*}(y)-c(x,y)}{} )(dx)(dy).\]

We conclude this section by listing two properties of functions of the form \(_{}\). These properties will be used repeatedly throughout this paper. First, for any \(\) we have

\[_{}_{}((x)+(y)-c( x,y)}{})(dy)(dx)=1,\]

which means, in particular, that for any \(\) we have

\[E_{}^{,}(_{},)=_{X}[_{}(X)]+ _{Y}[(Y)].\] (5)The second property of interest is that for any \(\) and any \(x,x^{}\) it holds that

\[_{}(x)-_{}(x^{}) =- )(dy)}{(,y)}{}) (dy)}\] \[=-,y)+c( x^{},y)-c(x,y)}{})(dy)}{(,y)}{})(dy)}\] \[_{y}c(x^{},y)-c(x,y) c_{}( ).\]

In particular, for any \(\) we have

\[\|_{}\|_{}=_{x}_{}(x)-_{x^{}}_{ }(x^{}) c_{}().\] (6)

### Doubly Regularized Entropic Barycenters

Let \(=(^{1},,^{k})()^{k}\) be \(k\) probability measures and let \(w^{k}\) be a vector of positive numbers that sum to one. Given the inner regularization strength \(>0\) and the outer regularization strength \(>0\), the \((,)\) barycenter \(_{,}()\) of probability measures \(\) with respect to the weights vector \(w\) is defined as the unique solution to the following optimization problem:

\[_{,}=_{()}\, _{j=1}^{k}w_{j}T_{}(,^{j})+(,_{}),\] (7)

where \(_{}()\) is a reference probability measure.

We will now describe how to obtain a concave dual maximization problem to the primal problem (7), following along the lines of Chizat [17, Section 2.3], where the interested reader will find a comprehensive justification of all the claims made in the rest of this section.

First, using the semi-dual formulation of entropic optimal transport problem (5), we have, for each \(j\{1,,k\}\)

\[T_{}(,^{j})=_{^{j} L_{1}(^{j})}_{X }[_{^{j}}(X)]+_{Y^{j}}[^{j}(Y)].\]

Denote \(=(^{1},,^{j}) L_{1}()\). Then, we may rewrite the primal problem (7) by

\[_{(X)}_{ L_{1}()}_{j=1}^{k}w _{j}_{Y^{j}}^{j}(Y)+_{X} _{j=1}^{k}w_{j}_{^{j}}(X)+(,_{ }).\]

Interchanging \(\) and \(\), which is justified using compactness of \(\) as detailed in , we obtain the dual optimization objective \(E^{,w}_{,}:L_{1}()\) defined by

\[ E^{,w}_{,}()& =_{(X)}_{j=1}^{k}w_{j}_{Y ^{j}}^{j}(Y)+_{X}_{j=1}^{k}w _{j}_{^{j}}(X)+(,_{}).\\ &=_{j=1}^{k}w_{j}_{Y^{j}}^{j}(Y) -(^{k}w_{j}_{^{j}}(x)}{ })_{}(dx).\] (8)

The infimum above is attained by the measure

\[_{}(dx)=Z_{}^{-1}(^{k}_{ ^{j}}(x)}{})_{}(dx), Z_{}= (^{k}_{^{j}}(x)}{})_{}(dx).\]

To each dual variable \(\) we associate the marginal measures \(^{j}_{}(dy)\) defined for \(j=1,,k\) by

\[^{j}_{}(dy)=^{j}(dy)(}(x)+^ {j}(y)-c(x,y)}{})_{}(dx).\] (9)Finally, we mention that the objective \(E^{,w}_{,}\) is concave and for any \(,^{}\) it holds that

\[_{h 0},w}_{,}(+h^{}) -E^{,w}_{,}()}{h}=_{j=1}^{k}w_{j}(_{^{j}}[(^{})^{j}]-_{^{j}_{}}[( ^{})^{j}])\.\]

In particular, fixing any optimal dual variable \(^{*}\), for any \(\) it holds using concavity of \(E^{,w}_{,}\) that

\[0 E^{,w}_{,}(^{*})-E^{,w}_{, }()_{j=1}^{k}w_{k}(_{^{j}}[( ^{*})^{j}-^{j}]-_{^{j}_{}}[( ^{*})^{j}-^{j}]).\] (10)

This concludes our overview of the background material on \((,)\)-barycenters.

## 3 Damped Sinkhorn Scheme

This section introduces a damped Sinkhorn-based optimization scheme (Algorithm 1) and provides guarantees for its convergence (Theorem 1). Before describing the algorithm, we make a quick detour to the following lemma, proved in Appendix A, which shows that the sub-optimality gap bounds on the dual objective (8) can be transformed into corresponding bounds on relative entropy between the \((,)\)-barycenter and the barycenter associated to a given dual variable.

**Lemma 1**.: _Fix any \(,>0\) and \(,w\). Let \(^{*}\) be the maximizer of dual problem \(E^{,w}_{,}\) and let \(_{^{*}}\) be the corresponding minimizer of the primal objective (7). Then, for any \( L_{1}()\) we have_

\[(_{^{*}},_{})^{-1}(E^{,w}_ {,}(^{*})-E^{,w}_{,}()).\]

We now turn to describing an iterative scheme that ensures convergence of the dual suboptimality gap to zero. Let \(_{t}\) be an iterate at time \(t\). Then, we have

\[E^{,w}_{,}(_{t})=L(_{t},_{t}, _{t})=_{j=1}^{k}w_{j}_{^{j}}[_{t}^{j}]- _{_{t}}[_{t}^{j}]+(_{t},_{}),\]

where

\[^{j}=*{argmax}_{}E^{_{t-1},^{j}}_{}(, _{t}^{j})_{t}=*{argmin}_{} _{}_{j}w_{j}_{t}^{j}+(, _{})}.\] (11)

In particular, when optimizing the dual objective \(E^{,w}_{,}\), every time the variable \(_{t}\) is updated, it automatically triggers the exact maximization/minimization steps defined in (11). It is thus a natural strategy to fix \(_{t}\) and \(_{t}\) and perform exact minimization on \(\), which can be done in closed form:

\[_{t+1}^{j}=*{argmax}_{}E^{_{t},^{j}}_{}( _{t}^{j},)=_{t}^{j}-^{j}}{d^{j}},\] (12)

where \(_{t}^{j}\) denotes the marginal distribution \(_{_{t}}^{j}\) defined in (9). The update (12) performs a Sinkhorn update on each block of variables \(^{j}\). Together, the update (12) followed by (11) results in the iterative Bregman projections algorithm introduced in . In , it was shown that this scheme converges for the \((,)\)-barycenters. The analysis of  is built upon a different dual formulation from the one considered in our work; this alternative formulation is only available when \(=\)[17, Section 2.3] and thus excludes the consideration of debiased barycenters \((,/2)\).

We have observed empirically (see Appendix F) that the iterates of the iterative Bregman projections (i.e., the scheme of updates defined in (12) and (11)) diverge whenever \(</2\). Indeed, decreasing the outer regularization parameter \(\) makes the minimization step in (11) less stable. As a result, the cumulative effect of performing the updates (12) and (11) may result in a decrease in the value of the optimization objective \(E^{,w}_{,}\).

One of the main contributions of our work is to show that this bad behaviour can be mitigated by damping the exact Sinkhorn updates (12). This leads to Algorithm 1 for which convergence guarantees are provided in Theorem 1 stated below.

**Theorem 1**.: _Fix any \(,>0\) and \(,w\). Let \(^{*}\) be the maximizer of dual problem \(E^{,w}_{,}\). Let \((_{t})_{t 0}\) be the sequence of iterates generated by Algorithm 1. Then, for any \(t 1\) it holds that_

\[E^{,w}_{,}(^{*})-E^{,w}_{,}(_{t})()^{2}}{(,)}\,.\]

Our convergence analysis draws upon the existing analyses of Sinkhorn's algorithm , which in turn are based on standard proof strategies in smooth convex optimization (e.g., [44, Theorem 2.1.14]). Concerning the proof of Theorem 1, the main technical contribution of our work lies in the following proposition proved in Appendix B.

**Proposition 1**.: _Consider the setup of Theorem 1. Then, for any integer \(t 0\) it holds that_

\[E^{,w}_{,}(_{t+1})-E^{ {},w}_{,}(_{t})_{j =1}^{k}w_{j}(^{j},_{t}^{j}).\]

With Proposition 1 at hand, we are ready to prove Theorem 1.

Proof of Theorem 1.: Denote \(_{t}=E^{,w}_{,}(^{*})-E^{ ,w}_{,}(_{t})\). We would like to relate the suboptimality gap \(_{t}\) to the increment \(_{t}-_{t+1}\). To do this, we will first show that the iterates \(_{t}\) have their oscillation norm bounded uniformly in \(t\). Indeed, for any \(j\{1,,k\}\), any \(t 1\), and any \(y\) we have

\[_{t}^{j}(y)=(1-)_{t-1}^{j}(y)+_{_{t}^{j}}(y).\]

By (6), \(_{_{t}^{j}}\) has oscillation norm bounded by \(c_{}()\). Because \(_{0}^{j}=0\) and \((0,1]\), by induction on \(t\) it follows that \(\|_{t}\|_{} c_{}()\) for any \(t 0\). Combining the bound on the dual sub-optimality gap (10) with Pinsker's inequality yields

\[_{t} 2c_{}()_{j=1}^{k}w_{j}\|^{j}-_{t}^{j} \|_{}c_{}_{j=1}^{k}w_{j}( ^{j},_{t}^{j})}.\]

Using concavity of the square root function, Proposition 1 yields for any \(t 0\)

\[_{t}-_{t+1}(,)_{j=1}^{k}w_{j}( ^{j},_{t}^{j})()^{2 }}_{t}^{2}.\]

By Proposition 1, the sequence \(_{t}\) is non-increasing. Hence, dividing the above equality by \(_{t}_{t+1}\) yields

\[}-}()^{2}}.\]

Telescoping the left hand side completes the proof.

Approximate Damped Sinkhorn Scheme

In this section, we extend the analysis of Algorithm 1 to an approximate version of the algorithm. Then, in Section 4.1, we describe how inexact updates may be implemented via approximate random sampling, thus enabling the computation of \((,)\)-barycenters in the free-support setting with convergence guarantees.

Algorithm 2 describes an inexact version of Algorithm 1. It replaces the damped Sinkhorn iterations of Algorithm 1 via approximate updates computed by an approximate Sinkhorn oracle - a procedure that satisfies the properties listed in Definition 1.

**Definition 1** (Approximate Sinkhorn Oracle).: An \(\)-approximate Sinkhorn oracle is a procedure that given any \(\) and any index \(j\{1,,k\}\), returns a Radon-Nikodym derivative \(_{}^{j}}{d j}\) of a measure \(_{}^{j}^{j}\) that satisfies the following properties:

1. \(_{}^{j}}{d^{j}}\) is strictly positive on the support of \(^{j}\);
2. \(\|_{}^{j}-_{}^{j}\|_{} /(2c_{}())\);
3. \(_{Y^{j}}|}{d_{}^{j}}(Y )| 1+^{2}/(2c_{}()^{2})\);
4. For any \(\) and any \(j\{1,,k\}\) it holds that \(\|^{j}+(d_{}^{j}/d^{j})\|_{}(1-)\|^{j}\|_{}+ c_{}()\).

``` error tolerance parameter \(>0\), a function "ApproximateSinkhornOracle" satisfying properties listed in Definition 1, regularization strengths \(,>0\), reference measure \(_{}\),number of iterations \(T\) and \(k\) marginal measures \(^{1},,^{k}\) with positive weights \(w_{1},,w_{k}\) such that \(_{j=1}^{k}w_{j}=1\).
1. Set \(=(1,/)\) and initialize \((_{0}^{j})=0\) for \(j\{1,,k\}\).
2. For \(t=0,1,T-1\) do 1. \(_{t}^{j}}{d j}(y)(,,,_{t},,j)\) for \(j\{1,,k\}\). 2. \(_{t+1}^{j}(y)_{t}^{j}(y)-_{t}^{j}}{d^{j}}(y)\) for \(j\{1,,k\}\).
3. Return \((_{T}^{j},_{T}^{j})_{j=1}^{k}\). ```

**Algorithm 2**Approximate Damped Sinkhorn Scheme

The following theorem shows that Algorithm 2 enjoys the same convergence guarantees as Algorithm 1 up to the error tolerance of the procedure used to implement the approximate updates. A noteworthy aspect of the below theorem is that the error does not accumulate over the iterations.

**Theorem 2**.: _Fix any \(,>0\) and \(,w\). Let \(^{*}\) be the maximizer of dual problem \(E_{,}^{,w}\). Let \((}_{t})_{t 0}\) be the sequence of iterates generated by Algorithm 2 with the accuracy parameter \( 0\). Let \(T=\{t:E_{,}^{,w}(^{*})-E_{,}^{,w}(}_{t}) 2\}\). Then, for any \(t T\) it holds that_

\[E_{,}^{,w}(^{*})-E_{,}^{,w}( }_{t}) 2+()^{2}}{ (,)}\,.\]

The proof of the above theorem can be found in Appendix C.

### Implementing the Approximate Sinkhorn Oracle

In this section, we show that the approximate Sinkhorn oracle (see Definition 1) can be implemented using approximate random sampling when the marginal distributions \(^{j}\) are discrete. To this end, fix the regularization parameters \(,>0\), the weight vector \(w\), and consider a set of \(k\) discrete marginal distributions

\[^{j}=_{l=1}^{m_{j}}^{j}(y_{l}^{j})_{y_{l}^{j}},\]

where \(_{x}\) is the Dirac measure located at \(x\) and \(^{j}(y_{l}^{j})\) is equal to the probability of sampling the point \(y_{l}^{j}\) from measure \(^{j}\). We denote the total cardinality of the support of all measures \(^{j}\) by

\[m=_{j=1}^{m}m_{j}.\]

Fix any \( L_{1}()\). Suppose we are given access to \(n\) i.i.d. samples \(X_{1},,X_{n}\) from a probability measure \(_{}^{}\) that satisfies

\[\|_{}-_{}^{}\|_{}_{}.\]

Then, for \(j=1,,k\) and \(l=1,,m_{j}\) consider

\[^{j}(y_{i}^{j})=^{j}(y_{i}^{j})_{i=1}^{n} (}(X_{i})+^{j}(y)-c(x,y)}{})\]

and for any parameter \((0,1/2]\) define

\[^{j}=(1-)^{j}+^{j}.\] (13)

We claim that \(^{j}\) implements the approximate Sinkhorn oracle with accuracy parameter of order \(O(_{}^{1/4})\) provided that \(n\) is large enough. This is shown in the following lemma, the proof of which can be found in Appendix D.

**Lemma 2**.: _Fix any \((0,1)\) and consider the setup described above. With probability at least \(1-\), for each \(j\{1,,k\}\) it holds simultaneously that the measure \(^{j}\) defined in (13) satisfies all the properties listed in Definition 1 with accuracy parameter_

\[_{j} c_{}()2+m_{ j}_{}+m_{j}}{n}}^{1/2}.\]

The above lemma shows that a step of Algorithm 2 can be implemented provided access to i.i.d. sampling from some measure \(_{}^{}\) close to \(_{}\) in total variation norm, where \(\) is an arbitrary iterate of Algorithm 2. The remainder of this section is dedicated to showing that this can be achieved by sampling via Langevin Monte Carlo.

Henceforth, fix \(_{}\) to be the Lebesgue measure on \(\), which corresponds to the free-support barycenters setup. Then, for any \(\) we have

\[_{}(dx)_{}(-V_{}(x)/) dx, V_{}(x)=_{j=1}^{k}w_{j}_{^{j}}^{j},\]

where \(_{}\) is equal to one on \(\) and zero everywhere else. It follows by (6) that \(\|V_{}\|_{} c_{}()/\). Further, let \(\!=_{x,x^{}}\|x-x^{ }\|_{2}\). By the convexity of \(\), the uniform measure on \(\) satisfies the logarithmic Sobolev inequality (LSI) with constant \(()^{2}/4\) (cf. ). Hence, by the Holley-Stroock perturbation argument , the measure \(_{}\) satisfies LSI with constant at most \((2c_{}()/)()^{2}/4<\).

It is well-established that Langevin Monte Carlo algorithms offer convergence guarantees for approximate sampling from a target measure subject to functional inequality constraints provided additional conditions hold such as the smoothness of the function \(V_{}\). However, such guarantees do not directly apply to the measure \(_{}\) due to its constrained support. Instead, it is possible to approximate \(_{}\) arbitrarily well in total variation norm by a family of measures \((_{,})_{>0}\) (see Appendix E for details) supported on all of \(^{d}\). Tuning the parameter \(\) allows us to trade-off between the approximation quality of \(_{,}\) and its LSI constant. Crucially, standard sampling guarantees for Langevin Monte Carlo (e.g., ) apply to the regularized measures \(_{,}\), which leads to provable guarantees for an implementation of Algorithm 2, thus furnishing the first convergence guarantees for computation of Wasserstein barycenters in the free support setup; see Theorem 3 stated below.

The above approximation argument applies to any cost function \(c\) that is Lipschitz on \(\) and exhibits quadratic growth at infinity. For the sake of simplicity, we consider the quadratic cost \(c(x,y)=\|x-y\|_{2}^{2}\). The exact problem setup where we are able to obtain computational guarantees for free-support barycenter computation via Langevin Sampling is formalized below.

**Problem Setting 1**.: Consider the setting described at the beginning of Section 4.1. In addition, suppose that

1. the reference measure \(_{}(dx)=_{}dx\) is the Lebesgue measure supported on \(\) (free-support setup);
2. it holds that \(_{R}=\{x^{d}:\|x\|_{2} R\}\) for some constant \(R<\);
3. the cost function \(c:^{d}^{d}[0,)\) is defined by \(c(x,y)=\|x-y\|_{2}^{2}\);
4. for any \(\) we have access to a stationary point \(x_{}\) of \(V_{}\) over \(\).

The last condition can be implemented in polynomial time using a first order gradient method. For our purposes, this condition is needed to obtain a good initialization point for the Unadjusted Langevin Algorithm following the explanation in [57, Lemma 1]; see Appendix E for further details.

We now proceed to the main result of this section, the proof of which can be found in Appendix E. The following theorem provides the first provably convergent method for computing Wasserstein barycenters in the free-support setting. We remark that a stochastic approximation argument of a rather different flavor used to compute fixed-support Wasserstein barycenters (for \(\)) has been previously analyzed in .

**Theorem 3**.: _Consider the setup described in Problem Setting 1. Then, for any confidence parameter \((0,1)\) and any accuracy parameter \(>0\), we can simulate a step of Algorithm 2 with success probability at least \(1-\) in time polynomial in_

\[^{-1},d,R,(R^{2}/),(Rd^{-1/4})^{d},^{-1},^{-1},d,m,(m/).\]

_In particular, an \(\)-approximation of the \((,)\)-Barycenter can be obtained within the same computational complexity._

Comparing the above guarantee with the discussion following the statement of Lemma 2, we see an additional polynomial dependence on \((Rd^{-1/4})^{d}\) (note that for \(R d^{1/4}\) this term disappears). We believe this term to be an artefact of our analysis appearing due to the approximation argument described above. Considering the setup with \(R d^{1/4}\), the running time of our algorithm depends exponentially in \(R^{2}/\).

We conclude with two observations. First, since approximating Wasserstein barycenters is generally NP-hard , an algorithm with polynomial dependence on all problem parameters does not exist if \(\). Second, notice that computing an \(\) approximation of \((,)\)-Barycenter can be done in time polynomial in \(^{-1}\). This should be contrasted with numerical schemes based on discretizations of the set \(\), which would, in general, result in computational complexity of order \((R/)^{d}\) to reach the same accuracy.

## 5 Conclusion

We introduced algorithms to compute doubly regularized entropic Wasserstein barycenters and studied their computational complexity, both in the fixed-support and in the free-support settings. Although a naive adaptation of the usual alternate maximization scheme from  to our setting leads to diverging iterates (at least for small values of \(\)), our analysis shows that it is sufficient to damp these iterations to get a converging algorithm.

While we have focused on the problem of barycenters of measures, we note that the idea of entropic regularization is pervasive in other applications of optimal transport. There, the flexibility offered by the double entropic regularization may prove to be useful as well, and we believe that our damped algorithm could be adapted to these more general settings.