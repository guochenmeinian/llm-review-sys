# Efficient Neural Music Generation

Max W. Y. Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan Feng, Ming Tu, Yuliang Ji,

**Rui Xia, Mingbo Ma, Xuchen Song, Jitong Chen, Yuping Wang, Yuxuan Wang**

Speech, Audio & Music Intelligence (SAMI), ByteDance

###### Abstract

Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present **MeLoDy** (**M** for music; **L** for LM; **D** for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% to 99.6% forward passes in MusicLM, respectively, for sampling 10s to 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.

Our samples are available at https://Efficient-MeLoDy.github.io/.

## 1 Introduction

Music is an art composed of harmony, melody, and rhythm that permeates every aspect of human life. With the blossoming of deep generative models , music generation has drawn much attention in recent years . As a prominent class of generative models, language models (LMs)  showed extraordinary modeling capability in modeling complex relationships across long-term contexts . In light of this, AudioLM  and many follow-up works  successfully applied LMs to audio synthesis. Concurrent to the LM-based approaches, diffusion probabilistic models (DPMs) , as another competitive class of generative models , have also demonstrated exceptional abilities in synthesizing speech , sounds  and music .

However, generating music from free-form text remains challenging as the permissible music descriptions can be very diverse and relate to any of the genres, instruments, tempo, scenarios, or even some subjective feelings. Conventional text-to-music generation models are listed in Table 1, where both MusicLM  and Noise2Music  were trained on large-scale music datasets and demonstrated the state-of-the-art (SOTA) generative performances with high fidelity and adherence to various aspects of text prompts. Yet, the success of these two methods comes with large computational costs, which would be a serious impediment to their practicalities. In comparison, Mousai  building upon DPMs made efficient samplings of high-quality music possible. Nevertheless, the number of their demonstrated cases was comparatively small and showed limited in-sample dynamics. Aiming for a feasible music creation tool, a high efficiency of the generative model is essential since it facilitates interactive creation with human feedback being taken into account as in .

While LMs and DPMs both showed promising results, we believe the relevant question is not whether one should be preferred over another but whether we can leverage both approaches with respect to their individual advantages, e.g., . After analyzing the success of MusicLM, we leverage the highest-level LM in MusicLM, termed as _semantic LM_, to model the semantic structure of music, determining the overall arrangement of melody, rhythm, dynamics, timbre, and tempo. Conditional on this semantic LM, we exploit the non-autoregressive nature of DPMs to model the acoustics efficiently and effectively with the help of a successful sampling acceleration technique . All in all, in this paper, we introduce several novelties that constitute our main contributions:

1. We present **MeLoDy** (**M** for music; **L** for LM; **D** for diffusion), an LM-guided diffusion model that generates music of competitive quality while reducing 95.7% and 99.6% iterations of MusicLM to sample 10s and 30s music, being faster than real-time on a V100 GPU.
2. We propose the novel dual-path diffusion (DPD) models to efficiently model coarse and fine acoustic information simultaneously with a particular semantic conditioning strategy.
3. We design an effective sampling scheme for DPD, which improves the generation quality over the previous sampling method in  proposed for this class of LDMs.
4. We reveal a successful audio VAE-GAN that effectively learns continuous latent representations, and is capable of synthesizing audios of competitive quality together with DPD.

## 2 Related Work

Audio GenerationApart from the generation models shown in Table 1, there are also music generation models [28; 29] that can generate high-quality music samples at high speed, yet they cannot accept free-form text conditions and can only generate single-genre music, e.g., techno music in . There also are some successful music generators in the industry, e.g. Mubert  and Riffusion , yet, as analyzed in , they struggled to compete with MusicLM in handling free-form text prompts. In a more general scope of audio synthesis, some promising text-to-audio synthesizers [12; 21; 22] trained with AudioSet  also demonstrated the ability to generate music from free-form text, but the musicality of their samples is limited.

Acceleration of Autoregressive ModelsWaveNet  is a seminal work that demonstrates the capability of autoregressive (AR) models in generating high-fidelity audio. It comes with the drawback of extremely high computational cost in sampling. To improve its practical feasibility, Parallel WaveNet  and WaveRNN  were separately proposed to accelerate WaveNet. With a similar goal, our proposed MeLoDy can be viewed as an accelerated variant of MusicLM, where we replace the last two AR models with a dual-path diffusion model. Parallel to our work, SoundStorm  also exceedingly accelerates the AudioLM with a mask-based non-AR decoding scheme . While it is applicable to MusicLM, the sound quality of this model is still limited by the bitrate of the neural codec. In comparison, the proposed diffusion model in MeLoDy operates with continuous-valued latent vectors, which by nature can be decoded into music audios of higher quality.

  
**Model** & **Prompts** & **Training Data** & **AC** & **FR** & **VT** & **MP** \\  Moüsai  & Text & 2.5k hours of music & ✓ & ✓ & ✗ & ✗ \\ MusicLM  & Text, Melody & 280k hours of music & ✓ & ✗ & ✓ & ✗ \\ Noise2Music  & Text & 340k hours of music & ✗ & ✗ & ✓ & ✗ \\ 
**MeLoDy** (Ours) & Text, Audio & 257k hours of music1 & ✓ & ✓ & ✓ & ✓ \\   

Table 1: A comparison of MeLoDy with conventional text-to-music generation models in the literature. We use **AC** to denote whether audio continuation is supported, **FR** to denote whether the sampling is faster than real-time on a V100 GPU, **VT** to denote whether the model has been tested and demonstrated using various types of text prompts including instruments, genres, and long-form rich descriptions, and **MP** to denote whether the evaluation was done by music producers.

Network ArchitectureThe architecture designed for our proposed DPD was inspired by the dual-path networks used in the context of audio separation, where Luo et al.  initiated the idea of segmentation-based dual-path processing, and triggered a number of follow-up works achieving the state-of-the-art results [39; 40; 41; 42; 43]. Noticing that the objective in diffusion models indeed can be viewed as a special case of source separation, this kind of dual-path architecture effectually provides us a basis for simultaneous coarse-and-fine acoustic modeling.

## 3 Background on Audio Language Modeling

This section provides the preliminaries that serve as the basis for our model. In particular, we briefly describe the audio language modeling framework and the tokenization methods used in MusicLM.

### Audio Language Modeling with MusicLM

MusicLM  mainly follows the audio language modeling framework presented in AudioLM , where audio synthesis is viewed as a language modeling task over a hierarchy of coarse-to-fine audio tokens. In AudioLM, there are two kinds of tokenization for representing different scopes of audio:

* **Semantic Tokenization**: K-means over representations from SSL, e.g., w2v-BERT ;
* **Acoustic Tokenization**: Neural audio codec, e.g., SoundStream .

To better handle the hierarchical structure of the acoustic tokens, AudioLM further separates the modeling of acoustic tokens into coarse and fine stages. In total, AudioLM defines three LM tasks: (1) semantic modeling, (2) coarse acoustic modeling, and (3) fine acoustic modeling.

We generally define the sequence of conditioning tokens as \(_{1:T_{}}:=[_{1},,_{T_{}}]\) and the sequence of target tokens as \(_{1:T_{}}:=[_{1},,_{T_{}}]\). In each modeling task, a Transformer-decoder language model parameterized by \(\) is tasked to solve the following autoregressive modeling problem:

\[p_{}(_{1:T_{}}|_{1:T_{}})= _{j=1}^{T_{}}p_{}(_{j}|[_{1},,_{T_{}},_{1},,_{j-1}]),\] (1)

where the conditioning tokens are concatenated to the target tokens as prefixes. In AudioLM, semantic modeling takes no condition; coarse acoustic modeling takes the semantic tokens as conditions; fine acoustic modeling takes the coarse acoustic tokens as conditions. The three corresponding LMs can be trained in parallel with the ground-truth tokens, but need to be sampled sequentially for inference.

#### 3.1.1 Joint Tokenization of Music and Text with MuLan and RVQ

To maintain the merit of audio-only training, MusicLM relies on joint audio-text embedding model, termed as MuLan , which can be individually pre-trained with large-scale music data and weakly-associated, free-form text annotations. This MuLan model is learned to project the music audio and its corresponding text description into the same embedding space such that the paired audio-text embeddings can be as close as possible. In MusicLM, the embeddings of music and text are tokenized using a separately learned residual vector quantization (RVQ)  module. Then, to generate music from a text prompt, MusicLM takes the MuLan tokens from the RVQ as the conditioning tokens in the semantic modeling stage and the coarse acoustic modeling stage, following Eq. (1). Given the prefixing MuLan tokens, the semantic tokens, coarse acoustic tokens, and fine acoustic tokens can be subsequently computed by LMs to generate music audio adhering to the text prompt.

## 4 Model Description

The overall training and sampling pipelines of MeLoDy are shown in Figure 1, where, we have three modules for representation learning: (1) MuLan, (2) Wav2Vec2-Conformer, and (3) audio VAE, and two generative models: a language model (LM) and a dual-path diffusion (DPD) model, respectively, for semantic modeling and acoustic modeling. In the same spirit as MusicLM, we leverage LM to model the semantic structure of music for its promising capability of modeling complex relationships across long-term contexts [9; 10; 11]. We also similarly pre-train a MuLan model to obtain the conditioning tokens. For semantic tokenization, after empirically compared against w2v-BERT , we employ a Wav2Vec2-Conformer model, which follows the same architecture as Wav2Vec2  but employs the Conformer blocks  in place of the Transformer blocks. The remainder of this section presents our newly proposed DPD model and the audio VAE-GAN used for DPD model, while other modules overlapped with MusicLM are described in Appendix B regarding their training and implementation details.

### Audio VAE-GANs for Latent Representation Learning

To avoid learning arbitrarily high-variance latent representations, Rombach et al.  examined a KL-regularized image autoencoder for latent diffusion models (LDMs) and demonstrated extraordinary stability in generating high-quality image , igniting a series of follow-up works . Such an autoencoder imposes a KL penalty on the encoder outputs in a way similar to VAEs [51; 52], but, different from the classical VAEs, it is adversarially trained as in generative adversarial networks (GANs) . In this paper, this class of autoencoders is referred to as the _VAE-GAN_. Although VAE-GANs are promisingly applied in image generation, there are limited comparable attempts in audio generation. In this work, we propose to use a similarly trained VAE-GAN for raw audio.

Specifically, the audio VAE-GAN is trained to reconstruct 24kHz audio with a striding factor of 96, resulting in a 250Hz latent sequence. The architecture of the decoder is the same as that in HiFi-GAN . For the encoder, we basically replace the up-sampling modules in the decoder with convolution-based down-sampling modules while other modules stay the same. For adversarial training, we use the multi-period discriminators in  and the multi-resolution spectrogram discriminators in . The implementation and training details are further discussed in Appendix B.

### Dual-Path Diffusion: Angle-Parameterized Continuous-Time Latent Diffusion Models

The proposed dual-path diffusion (DPD) model is a variant of diffusion probabilistic models (DPMs) [1; 56; 15] in continuous-time [57; 58; 16; 59]. Instead of directly operating on raw data space \( p_{}()\), with reference to LDMs , DPD operates on a low-dimensional latent space \(_{0}=_{}()\), such that the audio can be approximately reconstructed from the latent vectors: \(_{}(_{0})\), where \(_{}\) and \(_{}\) are the encoder and the decoder in VAE-GAN, respectively. Diffusing the latent space could significantly relieve the computational burden of DPMs . Also, sharing a similar observation with , we find that audio VAE-GAN performed more stabler than other VQ-based autoencoders [45; 60] when working with the outputs from diffusion models.

Formally speaking, DPD is a Gaussian diffusion process \(_{t}\) that is fully specified by two strictly positive scalar-valued, continuously differentiable functions \(_{t},_{t}\): \(q(_{t}|_{0})=(_{t};_{t} _{0},_{t}^{2})\) for any \(t\). In the light of , we define \(_{t}:=( t/2)\) and \(_{t}:=( t/2)\) to benefit from some nice trigonometric properties, i.e., \(_{t}=^{2}}\) (a.k.a. variance-preserving ). With this definition, the forward diffusion process of \(_{t}\) can be re-parameterized in terms of angle \([0,/2]\):

\[_{}=()_{0}+(), (,),\] (2)

which implies \(_{}\) gets noisier as the angle \(\) increases from \(0\) to \(/2\).

To create a generative process, a \(\)-parameterized variational model \(p_{}(_{-}|_{})\) is trained to reverse the diffusion process by enabling taking any step \((0,]\) backward in angle. By discretizing \(/2\)

Figure 1: The training and sampling pipelines of MeLoDy

into \(T\) finite segments, we can generate \(_{0}\) from \(_{/2}(,)\) in \(T\) sampling steps:

\[p_{}(_{0}|_{/2})=_{_{ _{1,T-1}}}_{t=1}^{T}p_{}(_{_{t}-_{t}}| _{_{t}})\,d_{_{1,T-1}},_{t}= -_{i=t+1}^{T}_{i},&1 t<T;\\ ,&t=T,\] (3)

where \(_{1},,_{T}\), termed as the _angle schedule_, satisfy \(_{t=1}^{T}_{t}=/2\). Regarding the choice of angle schedule, Schneider et al.  proposed a uniform one, i.e., \(_{t}=\) for all \(t\). Yet, we observe that noise scheduling in the previous works  tend to take larger steps at the beginning of the sampling followed smaller steps for refinement. With a similar perspective, we design another linear angle schedule, written as

\[_{t}=+,\] (4)

which empirically gives more stable and higher-quality results. Appendix D presents the comparison results of this linear angle schedule against the uniform schedule used in .

#### 4.2.1 Diffusion Velocity Prediction

In DPD, we model the diffusion velocity at \(\), defined as \(_{}:=_{}}{d}\). It can be simplified as:

\[_{}=_{0}+ =()-( )_{0}.\] (5)

When \(_{}\) is given, we can easily remedy the original sample \(_{0}\) from a noisy latent \(_{}\) at any \(\), since \(_{0}=_{}-()_{}\). This suggests \(_{}\) a feasible target for neural network prediction \(}_{}(_{};)\), where \(\) generally denotes the set of conditions controlling the music generation. In MeLoDy, as illustrated in Figure 1, the semantic tokens \(_{1},,_{T_{}}\), which are obtained from the SSL model during training and generated by the LM at inference time, are used to condition the DPD model. In our experiments, we find that the stability of generation can be significantly improved if we use token-based discrete conditions to control the semantics of the music and let the diffusion model learn the embedding vector for each token itself. As in , this velocity prediction network \(\) can be effectively trained with a mean squared error (MSE) loss:

\[:=_{_{0}_{}( _{0}),(,),}[\|()-()_{0}- }_{}(()_{0}+();)\|_{2}^{2}],\] (6)

which forms the basis of DPD's training loss.

#### 4.2.2 Multi-Chunk Velocity Prediction

With reference to , for long-context generation, we can incrementally appending new chunks of random noise to infinitely continue audio generation. To achieve this, the velocity prediction

Figure 2: The proposed dual-path diffusion (DPD) model

network needs to be trained to handle the chunked input, where each chunk exhibits a different scale of noisiness. In particular, we define the multi-chunk velocity target \(_{}\) that comprises \(M\) chunks of velocities. Given \(_{0},_{},^{L D}\) with \(L\) representing the length of latents and \(D\) representing the latent dimensions, we have \(_{}:=_{1}_{M}\), where \(\) is the concatenation operation and

\[_{m}:=(_{m})[L_{m-1}:L_{m},:]-( _{m})_{0}[L_{m-1}:L_{m},:], L_{m}:=.\] (7)

Here, we use the NumPy slicing syntax (0 as the first index) to locate the \(m\)-th chunk, and we draw \(_{m}[0,/2]\) for each chunk at each training step to determine the noise scale. The MSE loss in Eq. (6) is then extended to

\[_{} :=_{_{0},,_{1}, ,_{M}}[\|_{}-}_{ }(}_{_{1}}}_{ _{M}};)\|_{2}^{2}],\] (8) \[}_{_{m}} :=(_{m})_{0}[L_{m-1}:L_{m},:]+(_{m })[L_{m-1}:L_{m},:].\] (9)

Different from the original setting where we use a global noise scale for the network input , in the case of multi-chunk prediction, we need to specifically inform the network what the noise scales are for all \(M\) chunks. Therefore, we append an angle vector \(\) to the set of conditions \(:=\{_{1},,_{T_{}},\}\) to record the angles drawn in all \(M\) chunks aligned with the \(L\)-length input:

\[:=[_{1}]_{r=1}^{L_{1}} [_{M}]_{r=1}^{L_{M}}^{L},\] (10)

where \([a]_{r=1}^{B}\) denotes the operation of repeating a scalar \(a\) for \(B\) times to make a \(B\)-length vector.

#### 4.2.3 Dual-Path Modeling for Efficient and Effective Velocity Prediction

To predict the multi-chunk velocity with \(}_{}\), we propose a dual-path modeling mechanism, which plays a prime role in DPD for efficient parallel processing along coarse and fine paths and effective semantic conditioning. Figure 2 presents the computation procedures of \(}_{}\), which comprises several critical modules that we present one by one below.

To begin with, we describe how the conditions \(\{_{1},,_{T_{}},\}\) are processed in DPD:

Encoding Angle VectorFirst, we encode \(^{L}\), which records the frame-level noise scales of latents. Instead of using the classical positional encoding , we use a spherical interpolation  between two learnable vectors \(_{},_{}^{256}\) using broadcast multiplications, denoted by \(\):

\[_{}:=^{(1)}(()_{}+() _{})^{L D_{}},\] (11)

where, for all \(i\), \(^{(i)}():=(_{2}^{(i)}( _{1}^{(i)}+_{1}^{(i)})+_{2}^{(i)})\) projects an arbitrary input \(^{D_{}}\) to \(^{D_{}}\) using RMSNorm  and GELU activation  with learnable \(_{1}^{(i)}^{D_{} D_{}}\), \(_{2}^{(i)}^{D_{} D_{}}\), \(_{1}^{(i)},_{2}^{(i)}^{D_{}}\), and \(D_{}\) is hidden dimension.

Encoding Semantic TokensThe remaining conditions are the discrete tokens representing semantic information \(_{1},,_{T_{}}\). Following the typical approach for embedding natural languages , we directly use a lookup table of vectors to map any token \(_{t}\{1,,V_{}\}\) into a real-valued vector \(E(_{t})^{D_{}}\), where \(V_{}\) denotes the vocabulary size of the semantic tokens, i.e., the number of clusters in k-means for \(\)-Conformer. By stacking the vectors along the time axis and applying another MLP block, we obtain \(_{}:=^{(2)}([E(_{1}),,E( _{T_{}})])^{T_{} D_ {}}\).

Conditional on the computed embeddings \(_{}\) and \(_{}\), we next show how the network input, i.e., \(_{_{t}}\) for the case of having same noise scale \(_{t}\) for all chunks and \(}_{_{1}}}_{_{M}}\) for the case of having different noise scales, is processed in DPD for velocity prediction. For the simplicity of notation, we use \(_{_{t}}\) to denote the network input here and below. \(_{_{s}}\) is first linearly transformed and added up with the angle embedding of the same shape: \(:=(_{_{s}}_{}+ }_{}),\) where \(_{}^{D D_{}}\) is learnable. Then, a crucial segmentation operation is applied for dual-path modeling.

SegmentationAs illustrated in Figure 3, the segmentation module divides a 2-D input into \(S\) half-overlapping segments each of length \(K\), represented by a 3-D tensor \(:=[,_{1},,_{S}, ]^{S K D_{}}\), where \(_{s}:=[:+K,:],\) and \(\) is zero-padded such that we have \(S=+1\). By choosing a segment size \(K\), the costs of sequence processing become sub-linear (\(()\)) as opposed to (\((L)\)). This greatly reduces the difficulty of learning a very long sequence and permits MeLoDy to use higher-frequency latents for better audio quality. In this work, 250Hz latent sequences was used. In comparison, MusicLM  was built upon 50Hz codec.

Dual-Path BlocksAfter the segmentation, we obtain a 3-D tensor input. As shown in Figure 2, the tensor is subsequently passed to \(N\) dual-path blocks, where each block contains two processing stages corresponding to coarse-path (i.e., inter-segment) and fine-path (i.e., intra-segment) processing, respectively. Similar to the observations in , we find it superior to use an attention-based network for coarse-path processing and to use a bi-directional RNN for fine-path processing. The goal of fine acoustic modeling is to better reconstruct the fine details from the roughly determined audio structure . At a finer scope, only the nearby elements matter and contain the most information needed for refinement, as supported by the modeling perspectives in neural vocoding . Specifically, we employ the Roformer network  for coarse-path processing, where we use a self-attention layer followed by a cross-attention layer to be conditional on \(_{}\) with rotary positional embeddings. On the other hand, we use a stack of 2-layer simple recurrent units (SRUs)  for fine-path processing. A feature-wise linear modulation (FiLM)  layer is applied to the output of SRUs to assist the denoising with the angle embedding \(_{}\) and the pooled \(_{}\). The details of inner mechanism in each dual-path block is presented in Appendix B.

#### 4.2.4 Music Generation and Continuation

Suppose we have a well-trained multi-chunk velocity model \(}_{}\), we begin with a \(L\)-length latent generation, where \(L\) is the length of latents we used in training. According to Appendix A, the DDIM sampling algorithm  can be re-formulated by applying the trigonometric identities:

\[_{_{t}-_{t}}=(_{t})_{_{t}}- (_{t})}_{}(_{_{t}};),\] (12)

which, by running from \(t=T\) to \(t=1\) using the defined \(_{t}\) in Eq. (4), we can generate a sample of \(_{0}\) of length \(L\). To continue generation, we append a new chunk composed of random noises to the generated \(_{0}\) and drop the first chunk in \(_{0}\). Recall that the inputs to \(}_{}\) are the \(M\) concatenated noisy latents of different noise scales. The continuation of generation is feasible since the conditions (i.e., the semantic tokens and the angle vector) defined in DPD have an autoregressive nature at inference time. On one hand, the semantic tokens are generated by the semantic LM in an autoregressive manner, therefore we can continue the generation of semantic tokens for the new chunk. On the other hand, since the multi-chunk model \(}_{}\) is trained to tackle chunks of different noise scales with respect to the angle vector, we can simply ignore the generated audio (on the first \(M-1\) chunks) by zeroing the respective values and setting ones for the newly appended chunk, i.e., \(_{}:=_{r=1}^{L- L/M}[_{t}]_{r=1}^{ L/M}\). Then, the newly appended noise chunk can be transformed to meaningful music audio after \( T/M\) step of DDIM sampling. For more details of generation, we present the corresponding algorithms in Appendix C. Besides music continuation, based on MuLan, MeLoDy also supports music prompts to generate music of a similar style, as shown in Figure 1. Examples of music continuation, and music prompting are shown on our demo page.

Figure 3: Diagram for visually understanding the segmentation operation for dual-path modeling

## 5 Experiments

### Experimental Setup

Data PreparationAs shown in Table 1, MeLoDy was trained on 257k hours of music data (6.4M 24kHz audios), which were filtered with  to focus on non-vocal music. Additionally, inspired by the text augmentation in , we enriched the tag-based texts to generate music captions by asking ChatGPT . This music description pool is used for the training of our 195.3M MuLan, where we randomly paired each audio with either the generated caption or its respective tags. In this way, we robustly improve the model's capability of handling free-form text.

Semantic LMFor semantic modeling, we trained a 429.5M LLaMA  with 24 layers, 8 heads, and 2048 hidden dimensions, which has a comparable number of parameters to that of the MusicLM . For conditioning, we set up the MuLan RVQ using 12 1024-sized codebooks, resulting in 12 prefixing tokens. The training targets were 10s semantic tokens, which are obtained from discretizing the 25Hz embeddings from a 199.5M Wav2Vec2-Conformer with 1024-center k-means.

Dual-Path DiffusionFor the DPD model, we set the hidden dimension to \(D_{}=768\), and block number to \(N=8\), resulting in 296.6M parameters. For the input chunking strategy, we divide the 10s training inputs in a fixed length of \(L=2500\) into \(M=4\) parts. For segmentation, we used a segment size of \(K=64\) (i.e., each segment is 256ms long), leading to \(S=80\) segments. In addition, we applied the classifier-free guidance (CFG)  to DPD to improve the correspondence between samples and conditions. During training, the cross-attention to semantic tokens is randomly replaced by self-attention with a probability of \(0.1\). For sampling, the unconditional prediction \(_{}\) and the conditional prediction \(_{}\) are linearly combined: \(_{}+(1-)_{}\) with a scale of \(=2.5\).

Audio VAE-GANFor audio VAE-GAN, we used a hop size of 96, resulting in 250Hz latent sequences for encoding 24kHz music audio. The latent dimension \(D=16\), thus we have a total compression rate of 6\(\). The hidden channels used in the encoder were 256, whereas that used in the decoder were 768. The audio VAE-GAN in total contains 100.1M parameters.

### Performance Analysis

Objective MetricsWe use the VGGish-based  Frechet audio distance (FAD)  between the generated audios and the reference audios from MusicCaps  as a rough measure of generation fidelity.2 To measure text correlation, we use the MuLan cycle consistency (MCC) , which calculates the cosine similarity between text and audio embeddings using a pre-trained MuLan.3

Inference SpeedWe first evaluate the sampling efficiency of our proposed MeLoDy. As DPD permits using different numbers of sampling steps depending on our needs, we report its generation speed in Table 2. Surprisingly, MeLoDy steadily achieved a higher MCC score than that of the reference set, even taking only 5 sampling steps. This means that (i) the MuLan model determined that our generated samples were more correlated to MusicCaps captions than reference audios, and (ii) the proposed DPD is capable of consistently completing the MuLan cycle at significantly lower costs than the nested LMs in .

  
**Steps (\(T\))** & **Speed on CPU (\(\))** & **Speed on GPU (\(\))** & **FAD (\(\))** & **MCC (\(\))** \\  (MusicCaps) & - & - & - & 0.43 \\ 
5 & **1472Hz (0.06\(\))** & **181.1kHz (7.5\(\))** & 7.23 & 0.49 \\
10 & 893Hz (0.04\(\)) & 104.8kHz (4.4\(\)) & 5.93 & 0.52 \\
20 & 498Hz (0.02\(\)) & 56.9kHz (2.4\(\)) & **5.41** & **0.53** \\   

Table 2: The speed and the quality of our proposed MeLoDy on a CPU (Intel Xeon Platinum 8260 CPU @ 2.40GHz) or a GPU (NVIDIA Tesla V100) using different numbers of sampling steps.

Comparisons with SOTA modelsWe evaluate the performance of MeLoDy by comparing it to MusicLM  and Noise2Music , which both were trained large-scale music datasets and demonstrated SOTA results for a wide range of text prompts. To conduct fair comparisons, we used the same text prompts in their demos (70 samples from MusicLM; 41 samples from Noise2Music),4 and asked seven music producers to select the best out of a pair of samples or voting for a tie (both win) in terms of musicality, audio quality, and text correlation. In total, we conducted 777 comparisons and collected 1,554 ratings. We detail the evaluation protocol in Appendix F. Table 3 shows the comparison results, where each category of ratings is separated into two columns, representing the comparison against MusicLM (MLM) or Noise2Music (N2M), respectively. Finally, MeLoDy consistently achieved comparable performances (all winning proportions fall into [0.4, 0.6]) in musicality and text correlation to MusicLM and Noise2Music. Regarding audio quality, MeLoDy outperformed MusicLM (\(p<0.05\)) and Noise2Music (\(p<0.01\)), where the \(p\)-values were calculated using the Wilcoxon signed-rank test. We note that, to sample 10s and 30s music, MeLoDy only takes 4.32% and 0.41% NFEs of MusicLM, and 10.4% and 29.6% NFEs of Noise2Music, respectively.

Diversity AnalysisDiffusion models are distinguished for its high diversity . We conduct an additional experiment to study the diversity and validity of MeLoDy's generation given the same text prompt of open description, e.g., feelings or scenarios. The sampled results were shown on our demo page, in which we obtained samples with diverse combinations of instruments and textures.

Ablation StudiesWe also study the ablation on two aspects of the proposed method. In Appendix D, we compared the uniform angle schedule in  and the linear one proposed in DPD using the MCC metric and case-by-case qualitative analysis. It turns out that our proposed schedule tends to induce fewer acoustic issues when taking a small number of sampling steps. In Appendix E, we showed that the proposed dual-path architecture outperformed other architectures [23; 31] used for LDMs in terms of the signal-to-noise ratio (SNR) improvements using a subset of the training data.

## 6 Discussion

LimitationWe acknowledge the limitations of our proposed MeLoDy. To prevent from having any disruption caused by unnaturally sound vocals, our training data was prepared to mostly contain non-vocal music only, which may limit the range of effective prompts for MeLoDy. Besides, the training corpus we used was unbalanced and slightly biased towards pop and classical music. Lastly, as we trained the LM and DPD on 10s segments, the dynamics of a long generation may be limited.

Broader ImpactWe believe our work has a huge potential to grow into a music creation tool for music producers, content creators, or even normal users to seamlessly express their creative pursuits with a low entry barrier. MeLoDy also facilitates an interactive creation process, as in Midjourney , to take human feedback into account. For a more precise tune of MeLoDy on a musical style, the LoRA technique  can be potentially applied to MeLoDy, as in Stable Diffusion .

  
**Model** & **NFE** (\(\)) &  &  &  \\   & MLM & N2M & MLM & N2M & MLM & N2M \\  MusicLM  & \((25+200+400)T\) & **0.541** & - & 0.465 & - & **0.548** & - \\ Noise2Music  & \(1000+800+800\) & - & **0.555** & - & 0.436 & - & **0.572** \\ 
**MeLoDy** (20 steps) & \(25T+20\) & 0.459 & 0.445 & **0.535** & **0.564** & 0.452 & 0.428 \\   

Table 3: The comparison of MeLoDy with the SOTA text-to-music generation models. **NFE** is the number of function evaluations  for generating \(T\)-second audio.5** **Musicality**, **Quality**, and **Text Corr.** are the winning proportions in terms of musicality, quality, and text correlation, respectively.