# Convergent Bregman Plug-and-Play Image Restoration for Poisson Inverse Problems

Samuel Hurault

Univ. Bordeaux, CNRS, INRIA, Bordeaux INP, IMB, UMR 5251

samuel.hurault@math.u-bordeaux.fr

Ulugbek Kamilov

Washington University in St. Louis

kamilov@wustl.edu

Arthur Leclaire

Univ. Bordeaux, CNRS, INRIA, Bordeaux INP, IMB, UMR 5251

LTCI, Telecom Paris, IP Paris

arthur.leclaire@telecom-paris.fr

Nicolas Papadakis

Univ. Bordeaux, CNRS, INRIA, Bordeaux INP, IMB, UMR 5251

nicolas.papadakis@math.u-bordeaux.fr

###### Abstract

Plug-and-Play (PnP) methods are efficient iterative algorithms for solving ill-posed image inverse problems. PnP methods are obtained by using deep Gaussian denoisers instead of the proximal operator or the gradient-descent step within proximal algorithms. Current PnP schemes rely on data-fidelity terms that have either Lipschitz gradients or closed-form proximal operators, which is not applicable to Poisson inverse problems. Based on the observation that the Gaussian noise is not the adequate noise model in this setting, we propose to generalize PnP using the Bregman Proximal Gradient (BPG) method. BPG replaces the Euclidean distance with a Bregman divergence that can better capture the smoothness properties of the problem. We introduce the Bregman Score Denoiser specifically parametrized and trained for the new Bregman geometry and prove that it corresponds to the proximal operator of a nonconvex potential. We propose two PnP algorithms based on the Bregman Score Denoiser for solving Poisson inverse problems. Extending the convergence results of BPG in the nonconvex settings, we show that the proposed methods converge, targeting stationary points of an explicit global functional. Experimental evaluations conducted on various Poisson inverse problems validate the convergence results and showcase effective restoration performance.

## 1 Introduction

Ill-posed image inverse problems are classically formulated with a minimization problem of the form

\[*{arg\,min}_{x^{n}} f(x)+g(x) \]

where \(f\) is a data-fidelity term, \(g\) a regularization term, and \(>0\) a regularization parameter. The data-fidelity term is generally written as the negative log-likelihood \(f(x)=- p(y|x)\) of theprobabilistic observation model chosen to describe the physics of an acquisition \(y^{m}\) from linear measurements \(Ax\) of an image \(x^{n}\). In applications such as Positron Emission Tomography (PET) or astronomical CCD cameras (Bertero et al., 2009), where images are obtained by counting particles (photons or electrons), it is common to use the Poisson noise model \(y( Ax)\) with parameter \(>0\). The corresponding negative log-likelihood corresponds to the Kullback-Leibler divergence

\[f(x)=_{i=1}^{m}y_{i}(}{(Ax)_{i}})+( Ax)_{i}-y_{i}. \]

The minimization of (1) can be addressed with proximal splitting algorithms (Combettes and Pesquet, 2011). Depending on the properties of the functions \(f\) and \(g\), they consist in alternatively evaluating the proximal operator and/or performing a gradient-descent step on \(f\) and \(g\).

Plug-and-Play (PnP) (Venkatakrishnan et al., 2013) and Regularization-by-Denoising (RED) (Romano et al., 2017) methods build on proximal splitting algorithms by replacing the proximal or gradient descent updates with off-the-shelf Gaussian denoising operators, typically deep neural networks trained to remove Gaussian noise. The intuition behind PnP/RED is that the proximal (resp. gradient-descent) mapping of \(g\) writes as the maximum-a-posteriori (MAP) (resp. posterior mean) estimation from a Gaussian noise observation, under prior \(p=(-g)\). A remarkable property of these priors is that they are decoupled from the degradation model represented by \(f\) in the sense that one learned prior \(p\) can serve as a regularizer for many inverse problems. When using deep denoisers corresponding to exact gradient-step (Hurault et al., 2021; Cohen et al., 2021) or proximal (Hurault et al., 2022) maps, RED and PnP methods become real optimization algorithms with known convergence guarantees.

However, for Poisson noise the data-fidelity term (2) is neither smooth with a Lipschitz gradient, nor proximable for \(A\) (meaning that its proximal operator cannot be computed in a closed form), which limits the use of standard splitting procedures. Bauschke et al. (2017) addressed this issue by introducing a Proximal Gradient Descent (PGD) algorithm in the Bregman divergence paradigm, called Bregman Proximal Gradient (BPG). The benefit of BPG is that the smoothness condition on \(f\) for sufficient decrease of PGD is replaced by the "NoLip" condition "\(Lh-f\) is convex" for a convex potential \(h\). For instance, Bauschke et al. (2017) show that the data-fidelity term (2) satisfies the NoLip condition for the Burg's entropy \(h(x)=-_{i=1}^{n}(x_{i})\).

Our primary goal is to exploit the BPG algorithm for minimizing (1) using PnP and RED priors. This requires an interpretation of the plug-in denoiser as a Bregman proximal operator.

\[^{h}_{g}(y)=*{arg\,min}_{x}g(x)+D_{h}(x,y), \]

where \(D_{h}(x,y)\) is the Bregman divergence associated with \(h\). In Section 3, we show that by selecting a suitable noise model, other than the traditional Gaussian one, the MAP denoiser can be expressed as a Bregman proximal operator. Remarkably, the corresponding noise distribution belongs to an exponential family, which allows for a closed-form posterior mean (MMSE) denoiser generalizing the Tweedie's formula. Using this interpretation, we derive a RED prior tailored to the Bregman geometry. By presenting a prior compatible with the noise model, we highlight the limitation of the decoupling between prior and data-fidelity suggested in the existing PnP literature.

In order to safely use our MAP and MMSE denoisers in the BPG method, we introduce the Bregman Score denoiser, which generalizes the Gradient Step denoiser from (Hurault et al., 2021; Cohen et al., 2021). Our denoiser provides an approximation of the log prior of the noisy distribution of images. Moreover, based on the characterization of the Bregman Proximal operator from Gribonval and Nikolova (2020), we show under mild conditions that our denoiser can be expressed as the Bregman proximal operator of an explicit nonconvex potential.

In Section 4, we use the Bregman Score Denoiser within RED and PnP methods and propose the B-RED and B-PnP algorithms. Elaborating on the results from Bolte et al. (2018) for the BPG algorithm in the nonconvex setting, we demonstrate that RED-BPG and PnP-BPG are guaranteed to converge towards stationary points of an explicit functional. We finally show in Section 5 the relevance of the proposed framework in the context of Poisson inverse problems.

## 2 Related Works

**Poisson Inverse Problems** A variety of methods have been proposed to solve Poisson image restoration from the Bayesian variational formulation (1) with Poisson data-fidelity term (2). Although\(f\) is convex, this is a challenging optimization problem as \(f\) is non-smooth and does not have a closed-form proximal operator for \(A\), thus precluding the direct application of splitting algorithms such as Proximal Gradient Descent or ADMM. Moreover, we wish to regularize (1) with an explicit denoising prior, which is generally nonconvex. PIDAL (Figueiredo and Bioucas-Dias, 2009, 2010) and related methods (Ng et al., 2010; Setzer et al., 2010) solve (2) using modified versions of the alternating direction method of multipliers (ADMM). As \(f\) is proximable when \(A=\), the idea is to add a supplementary constraint in the minimization problem and to adopt an augmented Lagrangian framework. Figueiredo and Bioucas-Dias (2010) prove the convergence of their algorithm with convex regularization. However, no convergence is established for nonconvex regularization. Boulanger et al. (2018) adopt the primal-dual PDHG algorithm which also splits \(A\) from the \(f\) update. Once again, there is not convergence guarantee for the primal-dual algorithm with nonconvex regularization.

**Plug-and-Play (PnP)** PnP methods were successfully used for solving a variety of IR tasks by including deep denoisers into different optimization algorithms, including Gradient Descent (Romano et al., 2017), Half-Quadratic-Splitting (Zhang et al., 2017, 2021), ADMM (Ryu et al., 2019; Sun et al., 2021), and PGD (Kamilov et al., 2017; Terris et al., 2020). Variations of PnP have been proposed to solve Poisson inverse problems. Rond et al. (2016) use PnP-ADMM and approximates, at each iteration, the non-tractable proximal operator of the data-fidelity term (2) with an inner minimization procedure. Sanghvi et al. (2022) propose a PnP version of the PIDAL algorithm which is also unrolled for image deconvolution. Theoretical convergence of PnP algorithms with deep denoisers has recently been addressed by a variety of studies (Ryu et al., 2019; Sun et al., 2021; Terris et al., 2020) (see also a review in Kamilov et al. (2023)). Most of these works require non-realistic or sub-optimal constraints on the deep denoiser, such as nonexpansiveness. More recently, convergence was addressed by making PnP genuine optimization algorithms again. This is done by building deep denoisers as exact gradient descent operators (_Gradient-Step denoiser_) (Cohen et al., 2021; Hurault et al., 2021) or exact proximal operators (Hurault et al., 2022, 2023). These PnP algorithms thus minimize an explicit functional (2) with an explicit (nonconvex) deep regularization.

**Bregman optimization**Bauschke et al. (2017) replace the smoothness condition of PGD by the NoLip assumption (4) with a Bregman generalization of PGD called Bregman Proximal Gradient (BPG). In the nonconvex setting, Bolte et al. (2018) prove global convergence of the algorithm to a critical point of (1). The analysis however requires assumptions that are not verified by the Poisson data-fidelity term and Burg's entropy Bregman potential. Al-Shabili et al. (2022) considered unrolled Bregman PnP and RED algorithms, but without any theoretical convergence analysis. Moreover, the interaction between the data fidelity, the Bregman potential, and the denoiser was not explored.

## 3 Bregman denoising prior

The overall objective of this work is to efficiently solve ill-posed image restoration (IR) problems involving a data-fidelity term \(f\) verifying the NoLip assumption for some convex potential \(h\)

\[L>0Lh-finth. \]

PnP provides an elegant framework for solving ill-posed inverse problems with a denoising prior. However, the intuition and efficiency of PnP methods inherit from the fact that Gaussian noise is well suited for the Euclidean \(L_{2}\) distance, the latter naturally arising in the MAP formulation of the Gaussian denoising problem. When the Euclidean distance is replaced by a more general Bregman divergence, the noise model needs to be adapted accordingly for the prior.

In Section 3.1, we first discuss the choice of the noise model associated to a Bregman divergence, leading to Bregman formulations of the MAP and MMSE estimators. Then we introduce in Section 3.2 the Bregman Score Denoiser that will be used to regularize the inverse problem (1).

**Notations** For convenience, we assume throughout our analysis that the convex potential \(h:C_{h}^{n}\{+\}\) is \(^{2}\) and of Legendre type (definition in Appendix A.1). Its convex conjugate \(h^{*}\) is then also \(^{2}\) of Legendre type. \(D_{h}(x,y)\) denotes its associated Bregman divergence

\[D_{h}:^{n} inth[0,+]:(x,y) \{h(x)-h(y)- h(y),x-y& x(h)\\ +&. \]

### Bregman noise model

We consider the following observation noise model, referred to as _Bregman noise_1,

\[\,x,y(h) int(h) p (y|x):=(- D_{h}(x,y)+(x)). \]

We assume that there is \(>0\) and a normalizing function \(:(h)\) such that the expression (6) defines a probability measure. For instance, for \(h(x)=||x||^{2}\), \(=}\) and \(=0\), we retrieve the Gaussian noise model with variance \(^{2}\). As shown in Section 5, for \(h\) given by Burg's entropy, \(p(y|x)\) corresponds to a multivariate Inverse Gamma (\(\)) distribution.

Given a noisy observation \(y int(h)\), _i.e._ a realization of a random variable \(Y\) with conditional probability \(p(y|x)\), we now consider two optimal estimators of \(x\), the MAP and the posterior mean.

**Maximum-A-Posteriori (MAP) estimator** The MAP denoiser selects the mode of the a-posteriori probability distribution \(p(x|y)\). Given the prior \(p_{X}\), it writes

\[_{MAP}(y)=*{arg\,min}_{x}- p(x|y)=*{arg\, min}_{x}- p_{X}(x)- p(y|x)=_{-( + p_{X})}^{h}(y). \]

Under the Bregman noise model (6), the MAP denoiser writes as the _Bregman proximal operator_ (see relation (3)) of \(-( p_{X}+)\). This acknowledges for the fact that the introduced Bregman noise is the adequate noise model for generalizing PnP methods within the Bregman framework.

**Posterior mean (MMSE) estimator** The MMSE denoiser is the expected value of the posterior probability distribution and the optimal Bayes estimator for the \(L_{2}\) score. Note that our Bregman noise conditional probability (6) belongs to the regular _exponential family of distributions_

\[p(y|x)=p_{0}(y)( x,T(y)-(x)) \]

with \(T(y)= h(y)\), \((x)= h(x)-(x)\) and \(p_{0}(y)=( h(y)- h(y),y)\). It is shown in  (for \(T=\) and generalized in  for \(T\)) that the corresponding posterior mean estimator verifies a generalized Tweedie formula \( T(y)._{MMSE}(y)=- p_{0}(y)+ p_{Y}(y)\), which translates to (see Appendix B for details)

\[_{MMSE}(y)=[x|y]=y-(^{2}h(y))^{-1} (- p_{Y})(y). \]

Note that for the Gaussian noise model, we have \(h(x)=||x||^{2}\), \(=1/^{2}\) and (9) falls back to the more classical Tweedie formula of the Gaussian posterior mean denoiser \(=y-^{2}(- p_{Y})(y)\). Therefore, given an off-the-shelf "Bregman denoiser" \(_{}\) specially devised to remove Bregman noise (6) of level \(\), if the denoiser approximates the posterior mean \(_{}(y)_{MMSE}(y)\), then it provides an approximation of the score \(- p_{Y}(y)^{2}h(y).(y-_{}( y))\).

### Bregman Score Denoiser

Based on previous observations, we propose to define a denoiser following the form of the MMSE (9)

\[_{}(y)=y-(^{2}h(y))^{-1}. g_{}(y), \]

with \(g_{}:^{n}\) a nonconvex potential parametrized by a neural network. Such a denoiser corresponds to the Bregman generalization of the Gaussian Noise Gradient-Step denoiser proposed in .

When \(_{}\) is trained as a denoiser for the associated Bregman noise (6) with \(L_{2}\) loss, it approximates the optimal estimator for the \(L_{2}\) score, precisely the MMSE (9). Comparing (10) and (9), we get \( g_{}- p_{Y}\), _i.e._ the score is properly approximated with an explicit conservative vector field. We refer to this denoiser as the _Bregman Score denoiser_.

**Is the Bregman Score Denoiser a Bregman proximal operator?** We showed in relation (7) that the optimal Bregman MAP denoiser \(_{MAP}\) is a Bregman proximal operator. We want to generalize thisproperty to our Bregman denoiser (10). When trained with \(L_{2}\) loss, the denoiser should approximate the MMSE rather than the MAP. For Gaussian noise, Gribonval (2011) re-conciliates the two views by showing that the Gaussian MMSE denoiser actually writes as an Euclidean proximal operator.

Similarly, extending the characterization from (Gribonval and Nikolova, 2020) of Bregman proximal operators, we now prove that, under some convexity conditions, the proposed Bregman Score Denoiser (10) explicitly writes as the Bregman proximal operator of a nonconvex potential.

**Proposition 1** (Proof in Appendix C).: _Let \(h\) be \(^{2}\) and of Legendre type. Let \(g_{}:^{n}\{+\}\) proper and differentiable and \(_{}(y):int(h)^{n}\) defined in (10). Assume \((_{}) int(h)\). With \(_{}:^{n}\{+\}\) defined by_

\[_{}(y)=\{-h(y)+ h(y),y-g _{}(y)&y int(h)\\ +&.. \]

_suppose that \(_{} h^{*}\) is convex on \(int(h^{*})\). Then for \(_{}:^{n}\{+\}\) defined by_

\[_{}(x):=\{g_{}(y)-D_{h}(x,y)&y_{}^{-1}(x)&\ x(_{})\\ +&. \]

_we have that for each \(y int(h)\)_

\[_{}(y)*{arg\,min}_{x^{n}}\{D_{h }(x,y)+_{}(x)\} \]

**Remark 1**.: _Note that the order in the Bregman divergence is important in order to fit the definition of the Bregman proximal operator (3). In this order, (Gribonval and Nikolova, 2020, Theorem 3) does not directly apply. We propose instead in Appendix C a new version of their main theorem._

**Remark 2**.: _As shown in Appendix C (equation (64)), \(_{}(y)\) can be written as \(_{}(y)=(_{} h^{*}) h(y)\) so that the Jacobian \(J_{_{}}(y)=^{2}h(y).^{2}(_{} h ^{*})( h(y))\). The hypothesis of convexity of \(_{} h^{*}\) on \(}(h^{*})\) is thus equivalent to the fact that the Jacobian of \(_{}\) is positive semi-definite on \(}(h)\)._

This proposition generalizes the result of (Hurault et al., 2022, Prop. 1) to any Bregman geometry, which proves that the Gradient-Step Gaussian denoiser writes as an Euclidean proximal operator when \(_{} h^{*}(x)=||x||^{2}-g_{}(x)\) is convex. More generally, as exhibited for Poisson inverse problems in Section 5, such a convexity condition translates to a constraint on the deep potential \(g_{}\).

To conclude, the Bregman Score Denoiser provides, via _exact_ gradient or proximal mapping, two distinct explicit nonconvex priors \(g_{}\) and \(_{}\) that can be used for subsequent PnP image restoration.

## 4 Plug-and-Play (PnP) image restoration with Bregman Score Denoiser

We now regularize inverse problems with the explicit prior provided by the Bregman Score Denoiser (10). Properties of the Bregman Proximal Gradient (BPG) algorithm are recalled in Section 4.1. We show the convergence of our two B-RED and B-PnP algorithms in Sections 4.2 and 4.3.

### Bregman Proximal Gradient (BPG) algorithm

Let \(F\) and \(\) be two proper and lower semi-continuous functions with \(F\) of class \(^{1}\) on \(int(h)\). Bauschke et al. (2017) propose to minimize \(=F+\) using the following BPG algorithm:

\[x^{k+1}*{arg\,min}_{x^{n}}\{(x)+  x-x^{k}, F(x^{k})+D_{h}(x,x^{k})\}. \]

Recalling the general expression of proximal operators defined in relation (3), when \( h(x_{k})- F(x_{k})(h^{*})\), the previous iteration can be written as (see Appendix D)

\[x^{k+1}^{h}_{} h^{*}( h - F)(x_{k}). \]

With formulation (15), the BPG algorithm generalizes the Proximal Gradient Descent (PGD) algorithm in a different geometry defined by \(h\).

**Convergence of BPG** If \(F\) verifies the NoLip condition (4) for some \(L_{F}>0\) and if \(<}\), one can prove that the objective function \(\) decreases along the iterates (15). Global convergence of the iterates for nonconvex \(F\) and \(\) is also shown in (Bolte et al., 2018). However, Bolte et al. (2018) take assumptions on \(F\) and \(h\) that are not satisfied in the context of Poisson inverse problems. For instance, \(h\) is assumed strongly convex on the full domain \(^{n}\) which is not satisfied by Burg's entropy. Additionally, \(F\) is assumed to have Lipschitz-gradient on bounded subset of \(^{n}\), which is not true for \(F\) the Poisson data-fidelity term (2). Following the same structure of their proof, we extend in Appendix D.1 (Proposition 3 and Theorem 3) the convergence theory from (Bolte et al., 2018) with the more general Assumption 1 below, which is verified for Poisson inverse problems (Appendix E.4).

**Application to the IR problem** In what follows, we consider two variants of BPG for minimizing (1) with gradient updates on the data-fidelity term \(f\). These algorithms respectively correspond to Bregman generalizations of the RED Gradient-Descent (RED-GD) (Romano et al., 2017) and the Plug-and-Play PGD algorithms. For the rest of this section, we consider the following assumptions

**Assumption 1**.:
1. \(h:C_{h}\{+\}\) _is of class_ \(^{2}\) _and of Legendre-type._
2. \(f:^{n}\{+\}\) _is proper, lower-bounded, coercive, of class_ \(^{1}\) _on_ \(int(h)\)_, with_ \((h)(f)\)_, and is subanalytic._
3. _NoLip :_ \(L_{f}h-f\) _is convex on_ \(int(h)\)_._
4. \(h\) _is assumed strongly convex on any bounded convex subset of its domain and for all_ \(>0\)_,_ \( h\) _and_ \( f\) _are Lipschitz continuous on_ \(\{x(h),(x)\}\)_._
5. \(g_{}\) _given by the Bregman Score Denoiser (_10_) and its associated_ \(_{}\) _obtained from Proposition_ 1 _are lower-bounded and subanalytic._

Even though the Poisson data-fidelity term (2) is convex, our convergence results also hold for more general nonconvex data-fidelity terms. Assumption (iv) generalizes (Bolte et al., 2018, Assumption D) and allows to prove global convergence of the iterates. The subanalytic assumption, defined in Appendix A.3, is a sufficient condition for the Kurdyka-Lojasiewicz (KL) property to be verified (Bolte et al., 2007). The latter, also defined in Appendix A.3, can be interpreted as the fact that, up to a reparameterization, the function is sharp. The KL property is widely used in nonconvex optimization (Attouch et al., 2013; Ochs et al., 2014; Bolte et al., 2018). As detailed in Appendix A.3, the subanalytic assumption of \(f\), \(g_{}\) and \(_{}\) allows for the Kurdyka-Lojasiewicz (KL) property to be verified by the objective functions \(f+g_{}\) and \(f+_{}\).

### Bregman Regularization-by-Denoising (B-RED)

We first generalize the RED Gradient-Descent (RED-GD) algorithm (Romano et al., 2017) in the Bregman framework. Classically, RED-GD is a simple gradient-descent algorithm applied to the functional \( f+g_{}\) where the gradient \( g_{}\) is assumed to be implicitly given by an image denoiser \(_{}\) (parametrized by \(\)) via \( g_{}=-_{}\). Instead, our Bregman Score Denoiser (10) provides an explicit regularizing potential \(g_{}\) whose gradient approximates the score via the Tweedie formula (9). We propose to minimize \(F_{,}= f+g_{}\) on \((h)\) using the Bregman Gradient Descent algorithm

\[x_{k+1}= h^{*}( h- F_{,})(x_{k}) \]

which writes in a more general version as the BPG algorithm (14) with \(=0\)

\[x_{k+1}=*{arg\,min}_{x^{n}}\{ x-x_{k},  f(x_{k})+ g_{}(x_{k})+D_{h}(x,x_{k})\}. \]

As detailed in Appendix E.4, in the context of Poisson inverse problems, for \(h\) being Burg's entropy, \(F_{,}= f+g_{}\) verifies the NoLip condition only on _bounded_ convex subsets of \(dom(h)\). Thus we select \(C\) a non-empty closed bounded convex subset of \((h)}\). For the algorithm (17) to be well-posed and to verify a sufficient decrease of \((F_{,}(x^{k}))\), the iterates need to verify \(x_{k} C\). We propose to modify (17) as the Bregman version of Projected Gradient Descent, which corresponds to the BPG algorithm (14) with \(=i_{C}\), the characteristic function of the set \(C\):

\[ x^{k+1} T_{}(x_{k})=*{arg\,min}_{x ^{n}}\{i_{C}(x)+ x-x^{k}, F_{,}(x^{k} )+D_{h}(x,x^{k})\}. \]

For general convergence of B-RED, we need the following assumptions

**Assumption 2**.:
1. \(=i_{C}\)_, with_ \(C\) _a non-empty closed, bounded, convex and semi-algebraic subset of_ \((h)}\) _such that_ \(C int(h)\)_._
2. \(g_{}\) _has Lipschitz continuous gradient and there is_ \(L_{}>0\) _such that_ \(L_{}h-g_{}\) _is convex on_ \(C int(h)\)_._

In (Bolte et al., 2018; Bauschke et al., 2017), the NoLip constant \(L\) needs to be known to set the stepsize of the BPG algorithm as \( L<1\). In practice, the NoLip constant which depends on \(f\), \(g_{}\) and \(C\) is either unknown or over-estimated. In order to avoid small stepsize, we adapt the **backtracking** strategy of (Beck, 2017; Chapter 10) to automatically adjust the stepsize while keeping convergence guarantees. Given \((0,1)\), \([0,1)\) and an initial stepsize \(_{0}>0\), the following backtracking update rule on \(\) is applied at each iteration \(k\):

\[\ \ F_{,}(x_{k})-F_{,}(T_{}(x_{k})) <D_{h}(T_{}(x_{k}),x_{k}),. \]

Using the general nonconvex convergence analysis of BPG realized in Appendix D.1, we can show sufficient decrease of the objective and convergence of the iterates of B-RED.

**Theorem 1** (Proof in Appendix D.2).: _Under Assumption 1 and Assumption 2, the iterates \((x_{k})\) given by the B-RED algorithm (18) with the backtracking procedure (19) decrease \(F_{,}\) and converge to a critical point of \(=i_{C}+F_{,}\) with rate \(_{0 k K}D_{h}(x^{k+1},x^{k})=O(1/K)\)._

### Bregman Plug-and-Play (B-PnP)

We now consider the equivalent of PnP Proximal Gradient Descent algorithm in the Bregman framework. Given a denoiser \(_{}\) with \((_{})(h)\) and \(>0\) such that \(( h- f)( h ^{*})\), it writes

**(B-PnP)**\(x^{k+1}=_{} h^{*}( h- f)(x_{k})\). (20)

We use again as \(_{}\) the Bregman Score Denoiser (10). With \(_{}\) defined from \(g_{}\) as in (11) and assuming that \(_{} h^{*}\) is convex on \(int(h^{*})\), Proposition 1 states that the Bregman Score denoiser \(_{}\) is the Bregman proximal operator of some nonconvex potential \(_{}\) verifying (12). The algorithm B-PnP (20) then becomes \(x^{k+1}_{_{}}^{h} h^{*}( h-  f)(x_{k})\), which writes as a Bregman Proximal Gradient algorithm, with stepsize \(=1\),

\[x^{k+1}*{arg\,min}_{x^{n}}\{_{}(x)+  x-x^{k}, f(x^{k})+D_{h}(x,x^{k})\}. \]

With Proposition 1, we have \(_{}(y)_{_{}}^{h}(y)\)_i.e._ a proximal step on \(_{}\)_with stepsize_\(1\). We are thus forced to keep a fixed stepsize \(=1\) in the BPG algorithm (21) and no backtracking is possible. Using Appendix D.1, we can show that B-PnP converges towards a stationary point of \( f+_{}\).

**Theorem 2** (Proof in Appendix D.3).: _Assume Assumption 1 and \(_{} h^{*}\) strictly convex on \(int(h^{*})\). Then for \(( h- f)(  h^{*})\), \((_{})(h)\) and \( L_{f}<1\) (with \(L_{f}\) specified in Assumption 1), the iterates \(x_{k}\) given by the B-PnP algorithm (20) decrease \( f+_{}\) and converge to a critical point of \( f+_{}\) with rate \(_{0 k K}D_{h}(x^{k+1},x^{k})=O(1/K)\)._

**Remark 3**.: _The condition \((_{})(h)\) and the required convexity of \(_{} h^{*}\) come from Proposition 1 while the condition \(( h- f)(  h^{*})\) allows the algorithm B-PnP (20) to be well-posed. These assumptions will be discussed with more details in the context of Poisson image restoration in Section 5._

## 5 Application to Poisson inverse problems

We consider ill-posed inverse problems involving the Poisson data-fidelity term \(f\) introduced in (2). The Euclidean geometry (_i.e._\(h(x)=||x||^{2}\)) does not suit for such \(f\), as it does not have a Lipschitz gradient. In (Bauschke et al., 2017; Lemma 7), it is shown that an adequate Bregman potential \(h\) (in the sense that there exists \(L_{f}\) such that \(L_{f}h-f\) is convex) for (2) is the Burg's entropy

\[h(x)=-_{i=1}^{n}(x_{i}), \]for which \((h)=_{++}^{n}\) and \(L_{f}h-f\) is convex on \(int\,(h)=_{++}^{n}\) for \(L_{f}||y||_{1}\). For further computation, note that the Burg's entropy (22) satisfies \( h(x)= h^{*}(x)=-\) and \(^{2}h(x)=}\). The Bregman score denoiser associated to the Burg's entropy is presented in Section 5.1. The corresponding Bregman RED and PnP algorithms are applied to Poisson Image deblurring in Section 5.2.

### Bregman Score Denoiser with Burg's entropy

We now specify the study of Section 3 to the case of the Burg's entropy (22). In this case, the Bregman noise model (6) writes (see Appendix E.1 for detailed calculus)

\[p(y|x)=((x)+n)_{i=1}^{n}(}{y_{i}})^{ }(-}{y_{i}}). \]

For \(>1\), this is a product of _Inverse Gamma_ (\((,)\)) distributions with parameter \(_{i}= x_{i}\) and \(_{i}=-1\). This noise model has mean (for \(>2\)) \(x\) and variance (for \(>3\)) \(}{(-2)^{2}(-3)}x^{2}\). In particular, for large \(\), the noise becomes centered on \(x\) with signal-dependent variance \(x^{2}/\). Furthermore, using Burg's entropy (22), the optimal posterior mean (9) and the Bregman Score Denoiser (10) respectively write, for \(y_{++}^{n}\),

\[_{MMSE}(y) =y-y^{2}(- p_{Y})(y) \] \[_{}(y) =y-y^{2} g_{}(y). \]

**Denoising in practice** As Hurault et al. (2021), we chose to parametrize the deep potential \(g_{}\) as

\[g_{}(y)=||x-N_{}(x)||^{2}, \]

where \(N_{}\) is the deep convolutional neural network architecture DRUNet (Zhang et al., 2021) that contains Softplus activations and takes the noise level \(\) as input. \( g_{}\) is computed with automatic differentiation. We train \(_{}\) to denoise images corrupted with random Inverse Gamma noise of level \(\), sampled from clean images via \(p(y|x)=_{i=1}^{n}(_{i},_{i})(x_{i})\). To sample \(y_{i}(_{i},_{i})\), we sample \(z_{i}(_{i},_{i})\) and take \(y_{i}=1/z_{i}\). Denoting as \(p\) the distribution of a database of clean images, training is performed with the \(L^{2}\) loss

\[()=_{x p,y_{}(x)}[|| _{}(y)-x||^{2}]. \]

**Denoising performance** We evaluate the performance of the proposed Bregman Score DRUNet (B-DRUNet) denoiser (26). It is trained with the loss (27), with \(1/\) uniformly sampled in \((0,0.1)\). More details on the architecture and the training can be found in Appendix E.2. We compare the performance of B-DRUNet (26) with the same network DRUNet directly trained to denoise inverse Gamma noise with \(L_{2}\) loss. Qualitative and quantitative results presented in Figure 1 and Table 1 show that the Bregman Score Denoiser (B-DRUNet), although constrained to be written as (25) with a conservative vector field \( g_{}\), performs on par with the unconstrained denoiser (DRUNet).

**Bregman Proximal Operator** Considering the Burg's entropy (22) in Proposition 1 we get that, if \(_{}:x_{} h^{*}(x)=_{}(- )\) is convex on \((h^{*})=_{--}\), the Bregman Score Denoiser (25) satisfies \(_{}(y)=_{_{}}^{h}\), for the nonconvex potential \(_{}(y)=_{i=1}^{n}(y_{i})-g_{}(y)-1\). The convexity of \(_{} h^{*}\) can be verified using the following characterization, which translates to a condition on the deep potential \(g_{}\) (see Appendix E.2 for details)

\[ x(h^{*}),\ \  d^{n},\ \  ^{2}_{}(x)d,d 0 \] \[\  y_{++}^{n},\ \  d^{n},\ \  y^{4}^{2}g_{}(y)d,d_{i=1}^{n}(y^{2}(1-2y g_{}(y)))_{i}d_{i}^{2} \]

 \(\) & \(10\) & \(25\) & \(50\) & \(100\) & \(200\) \\  DRUNet & \(28.42\) & \(30.91\) & \(32.80\) & \(34.76\) & \(36.79\) \\ B-DRUNet & \(28.38\) & \(30.88\) & \(32.76\) & \(34.74\) & \(36.71\) \\ 

Table 1: Average denoising PSNR performance of Inverse Gamma noise denoisers B-DRUNet and DRUNet on \(256 256\) center-cropped images from the CBSD68 dataset, for various noise levels \(\).

It is difficult to constrain \(g_{}\) to satisfy the above condition. In Appendix E.2, we proceed to a detailed experimental validation of this assumption. We empirically observe on images that the trained deep potential \(g_{}\) satisfies the above inequality. This suggests that the convexity condition for Proposition 1 is true at least locally, on the image manifold. Indeed, we prove in Appendix E.3 that the convexity condition holds for the theoretical MMSE denoiser (24). As explained below, our denoiser is trained by minimizing the \(L^{2}\) cost for which the optimal Bayes estimator is the MMSE. The MMSE is then a theoretical denoiser that our denoiser tries to approximate. This explains why our denoiser, after training, naturally satisfies this condition without necessitating supplementary constraints.

### Bregman Plug-and-Play for Poisson Image Deblurring

We now derive the explicit B-RED and B-PnP algorithms in the context of Poisson image restoration. Choosing \(C=[0,R]^{n}\) for some \(R>0\), the B-RED and B-PnP algorithms (18) and (20) write

**(B-RED)**: \[x_{i}^{k+1} =\{x F_{,}(x^{k})_{i}+ (^{k}}-^{k}}):x[0,R]\} 1  i n\] (30) \[=&^{k}}{1+ x_{i}^{k} F_{ ,}(x^{k})_{i}}\ \ 0^{k}}{1+ x_{i}^{k}  F_{,}(x^{k})_{i}} R&1 i n\\ &R\ \] (31)
**(B-PnP)**: \[x^{k+1} =_{}(}{1+ x^{k} f(x^{k })}).\] (32)

Verification of the assumptions of Theorems 1 and 2 for the convergence of both algorithms is discussed in Appendix E.4.

**Poisson deblurring** Equipped with the Bregman Score Denoiser, we now investigate the practical performance of the plug-and-play B-RED and B-PnP algorithms for image deblurring with Poisson noise. In this context, the degradation operator \(A\) is a convolution with a blur kernel. We verify the efficiency of both algorithms over a variety of blur kernels (real-world camera shake, uniform and Gaussian). The hyper-parameters \(\), \(\) are optimized for each algorithm and for each noise level \(\) by grid search and are given in Appendix E.5. In practice, we first initialize the algorithm with \(100\) steps with large \(\) and \(\) so as to quickly initialize the algorithm closer to the right stationary point.

Note that the constraint \( L_{f}<1\) for convergence of the B-PnP algorithm (Theorem 1) may not be respected. The global NoLip constant \(L_{f}\) can indeed be locally very lose. As explained in the Appendix (32), we can adopt a backtracking-like strategy on the regularization parameter \(\) to ensure convergence. Nevertheless, with the proposed default value of \(\), this backtracking algorithm was never activated over the variety of blur kernels and noise levels experimented.

We show in Figures 2 and 3 that both B-PnP and B-RED algorithms provide good visual reconstruction. Moreover, we observe that, in practice, both algorithms satisfy the sufficient decrease property of the objective function as well as the convergence of the iterates. Additional quantitative performance and comparisons with other Poisson deblurring methods are given in Appendix E.4.

of the problem, we propose a new deep denoiser, parametrized by \(h\), which provably writes as the Bregman proximal operator of a nonconvex potential. We argue that this denoiser should be trained on a particular noise model, called Bregman noise, that also depends on \(h\). By plugging this denoiser in the BPG algorithm, we propose two new plug-and-play algorithms, called B-PnP and B-RED, and show that both algorithms converge to stationary points of explicit nonconvex functionals. We apply this framework to Poisson image inverse problem. Experiments on image deblurring illustrate numerically the convergence and the efficiency of the approach.

The central significance of our work stems from its theoretical study but we recognize certain limits within our experimental results. First, when applied to deblurring with Poisson noise, our proposed algorithms do not outperform existing methods in terms of PSNR. Second, while we prove that B-RED is convergent without restriction, the convergence of B-PnP depends on a specific convexity condition. Despite being confirmed with experiments and having robust theoretical foundations, this assumption could potentially be not verified when applied to non-natural images that significantly differ from those in the training dataset. Finally, due to the nonconvex nature of our proposed prior, the practical performance of the algorithms can be sensitive to their initialization.

AcknowledgementsThis work was funded by the French ministry of research through a CDSN grant of ENS Paris-Saclay. It has also been carried out with financial support from the French Research Agency through the PostProdEAP and Mistic projects (ANR-19-CE23-0027-01 and ANR-19-CE40-005). It has also been supported by the NSF CAREER award under grant CCF-2043134.

Figure 3: Deblurring from the indicated Gaussian blur kernel and Poisson noise with \(=60\).

Figure 2: Deblurring from the indicated motion kernel and Poisson noise with \(=40\).