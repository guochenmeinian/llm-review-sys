# General Articulated Objects Manipulation in Real Images via Part-Aware Diffusion Process

Zhou Fang Yong-Lu Li Lixin Yang Cewu Lu

Shanghai Jiao Tong University

{joefang, yonglu_li, siriusyang, lucewu}@sjtu.edu.cn

Corresponding authors.

###### Abstract

Articulated object manipulation in real images is a fundamental step in computer and robotic vision tasks. Recently, several image editing methods based on diffusion models have been proposed to manipulate articulated objects according to text prompts. However, these methods often generate weird artifacts or even fail in real images. To this end, we introduce the Part-Aware Diffusion Model to approach the manipulation of articulated objects in real images. First, we develop Abstract 3D Models to represent and manipulate articulated objects efficiently. Then we propose dynamic feature maps to transfer the appearance of objects from input images to edited ones, meanwhile generating the novel-appearing parts reasonably. Extensive experiments are provided to illustrate the advanced manipulation capabilities of our method concerning state-of-the-art editing works. Additionally, we verify our method on 3D articulated object understanding for embodied robot scenarios and the promising results prove that our method supports this task strongly. The project page is at https://mvig-rhos.com/pa_diffusion.

## 1 Introduction

Image editing is a long-standing popular computer vision task. Specifically, manipulating articulated objects has garnered significant attention owing to its application in various fields, such as image augmentation for downstream tasks , building goal conditions to train reinforcement learning models for robotic manipulation , creating videos with extra supervision information , detecting human-object interactions , reasoning object affordance , _etc_. Thanks to the large-scale training data and immense computing power, diffusion-based  generative models have achieved surprising results in the field of image and video generation.

Inspired by these successes, several recent works have adopted diffusion models as the backbone and implemented text-guided object manipulation . We can properly divide these studies into a couple of groups. The first one is to directly edit 2D images by transferring the feature/attention maps from original images to edited ones such as . However, weird artifacts are prone to appear when the objects are rotated and deformed, or novel views appear. Consequently, these methods are restricted to structure-preserving image editing. Another group relies on reconstructing 3D object models. As the most related work to ours,  reconstructed 3D object models for manipulation and projected them back to images later. Nevertheless, this approach depends on the quality of reconstructed 3D models heavily. And the reconstruction model has to be fine-tuned when dealing with new categories. Moreover, manipulation has to be done manually which is laborious and impractical to support editing large quantities of images.

To address these problems, we propose the Part-Aware Diffusion Model (PA-Diffusion model) for articulated object manipulation in real images, as illustrated in Fig. 1. Firstly, we introduce theconcept of Abstract 3D Model and build a Primitive Prototype Library to represent articulated objects in the 3D space, so that our method can not only cover many common objects but also handle novel categories without extra training data or fine-tuning processes. Besides, various manipulation can be done efficiently. Second, we proposed dynamic feature maps to assist generation models in accurately transferring object appearances to accurate locations in edited images. As a result, weird artifacts are eliminated, and meanwhile, the novel-appearing parts are generated more reasonably. Finally, owing to the simple manipulation and editing process, the procedure is automatic and the model can strongly support other tasks by editing a large volume of images.

Our main contributions are summarized as follows:

(1) We introduce the concept of the Abstract 3D Model which accurately and robustly represents various articulated object categories with primitive prototypes. Meanwhile, novel categories can also be incorporated quickly. In addition, the articulated objects can be efficiently manipulated with text instructions or human interactions in the 3D space.

(2) We propose dynamic feature maps that let the diffusion model comprehend the object structure. Consequently, the diffusion model can generate novel-appearing parts of objects reasonably and preserve the appearance of the seen parts simultaneously.

(3) We present comprehensive experiments to highlight the advantages of our PA-Diffusion model including comparing with state-of-the-art editing methods both qualitatively and quantitatively, choosing a 3D articulated object understanding experiment to demonstrate how our method supports the tasks in embodied robot scenarios.

## 2 Related Work

### Diffusion Model for Image Generation

In recent years, diffusion models [40; 39; 11] have achieved great success in the fields of image/video generation [7; 17], segmentation [5; 50], and many downstream computer vision tasks. To make the generation results controllable,  first proposed to extract and incorporate text features into the denoising process. Following this concept, [41; 12; 43; 15; 9] improved the performance of text-guided diffusion models with more effective text embedding methods.

However, as an implicit instruction, text guidance is still not strong enough to finish fine-grained image control such as determining the image layouts, objects' shape and texture, and so on. To make up this gap,  provided structural guidance by enhancing the similarity between the features of other conditions and the text guidance. [14; 6] proposed to modify the cross-attention maps and then guide the denoising process. To handle more complex scenarios and achieve more precise control,  and  proposed adding an extra module to the diffusion model. Then extra condition information can be imported to guide the denoising process.

Figure 1: We propose the **Part-Aware Diffusion Model**: Abstract 3D model of the articulated object is constructed referring to the input 2D real image. Various manipulations can be done based on the text instruction or human interaction in the 3D space, the generation model then creates the edited image according to the manipulation.

### Diffusion Based Image Editing

Considering the remarkable capability of understanding images, several recent works have also reported editing real and synthetic images with using diffusion models as the backbone. These methods can generally be summarized into two groups: Inversion-Based and Feature-Sharing Based.

The first group is primarily based on adding extra control to the inverted noise maps of images, then re-generating the image such as . However, because the deterministic DDIM sampling process cannot be reversed perfectly, these methods struggle to preserve the appearance of original objects and backgrounds precisely. The second group attempts to maintain the appearance of objects by transferring the feature/attention/activation maps between guidance and generation branches or by adding extra loss items during the denoising process, as seen in [8; 32; 13]. Recent approaches like DragGAN  and DragDiffusion  propose to utilize a point-to-point dragging scheme, which can achieve refined content dragging. Nonetheless, these approaches often perform poorly on articulated object manipulation in real images, resulting in weird and blurry artifacts in edited images.

2D-3D-2D is another promising way of image editing, the recent work  introduced reconstructing 3D models from 2D images and projecting them back after manipulation. However, this approach highly relies on the quality of 3D reconstructed models, and reconstructing 3D models from a single 2D image is still a challenging task.

In contrast to the aforementioned approaches, our method demonstrates advantages when manipulating articulated objects in real images - high fidelity edited images, easy and various manipulations, covering multiple categories, and incorporating novel categories quickly.

## 3 Method

### Overview

In this session, we go through the proposed PA-Diffusion model in detail. The overall architecture is demonstrated in Fig. 2. Initially, we reconstruct abstract 3D models for articulated objects with the Primitive Prototype Library. Then various manipulations can be done according to text instructions or human interactions. Next, leveraging DDIM Inversion [45; 31], initial inverted noise maps are created and manipulated following the previous actions. During the generation stage, we introduce dynamic feature maps, including manipulated inverted noise maps and compositional activation maps and images. These ensure that the appearance of seen parts of objects can be preserved accurately and that novel-appearing parts are generated reasonably. Besides, Texture and Style Consistency Score Loss are introduced to alleviate the blurry and style mismatch problems.

### Preliminary

Diffusion models aim to convert random Gaussian noise into high-resolution images through a sequential denoising and sampling process . Given the conditioning \(y\), we start from the initial Gaussian noise map \(z_{t}\), and then iteratively estimate the reduced noise \(_{t}\) at each time step \(t\):

\[_{t} =_{}(z_{t};t,y),\] (1) \[z_{t-1} =update(z_{t},_{t},t,t-1,_{t-1}),\]

The update function could be DDPM , DDIM , or other sampling methods. Nevertheless, conventional sampling from conditional diffusion models often fails to produce high-quality images that align well with the condition \(y\). To enhance the effect of the desired condition, extra class loss guidance is added to the reduced noise during the sampling process such as Classifier or Classifier-free guidance [46; 19].

Classifier guidance is introduced to generate conditional samples from an unconditional model by combining the unconditional score \(_{t}\) with a classifier \(p(y|z_{t})\), where \(p(y|z_{t})\) is the probability distribution of condition \(y\) based on the noise at time step \(t\):

\[_{t}=_{}(z_{t};t,y)+_{z_{t}} p(y|z_{t}),\] (2)

Classifier-free guidance eliminates the need for a separate classifier by incorporating the class information directly into the generative model as follows:

\[_{t}=(1+)_{}(z_{t};t,y)-_{ }(z_{t};t),\] (3)Following these concepts, custom energy functions can also be utilized to guide the denoising process, instead of the probability function. In , various energy functions \(g\) are incorporated alongside classifier-free guidance to obtain high-fidelity samples as follows:

\[}=(1+)_{}(z_{t};t,y)-_{ }(z_{t};t)+_{z_{t}}g(z_{t};t,y),\] (4)

Our proposed PA-Diffusion model is built on the diffusion model with classifier-free guidance. Extra energy functions are employed during the image editing process.

### Various Manipulations in the 3D Space

As a promising workaround to the methods of dealing with images directly, the 2D-3D-2D pipeline has successfully handled many articulated objects with precise 3D models. Unfortunately, creating 3D models for various categories from a single image remains challenging, particularly for novel categories or instances. In this work, we introduce the concept of Abstract 3D Model that reconstructs accurate 3D models, supports efficient object manipulation, and incorporates novel objects easily.

**Abstract 3D Model.** Unlike previous methods, there is no need for precise 3D models of our method, the conditional information we have to provide to the diffusion model is coarse sketch maps and part-level masks. Therefore, we introduce the use of an abstract 3D model to represent the articulated object. As an abstract 3D model, the object is represented by combining several basic prototypes. As depicted at the bottom of Fig. 2, the laptop can be represented by two planes, storage furnitures and microwaves can be represented by a plane and a box. Primitive Prototype Library, which includes basic 3D prototypes such as planes, cubes, and boxes, can support common articulated object categories involving both rotation and translation joint types.

**Camera alignment.** Next, we compute the camera pose in the 3D space and align the 2D real image view with the 3D space camera view. The pose computation problem is to calculate the intrinsic and extrinsic matrices for the camera that minimize the reprojection error from 3D-2D point correspondences . Thus, in this work, we first employ Large-scale Segmentation Models to obtain the initial part-level segmentation masks of articulated objects \(M^{Init}\) and then detect the extreme corner points \(A,B,C,D\) (\(pts_{1}\)) with simple corner detection functions. These 2D extreme points are aligned with their 3D counterparts \(A^{{}^{}},B^{{}^{}},C^{{}^{}},D^{{}^{}}\) (\(pts_{2}\), pre-defined in Primitive Prototype Library) as shown in Fig. 3. Finally, based on Perspective n-Points 2D-3D method , the camera matrices can be extracted, and 2D-3D views are aligned.

**Manipulation.** By representing objects with primitive prototypes, multiple types of manipulations can be implemented in the 3D space efficiently with the assistance of 3D computer graphics software.

Figure 2: The overall image editing process. (1) In the Pre-Process stage, articulated objects in 2D images are part-level segmented and reconstructed to abstract 3D models. Meanwhile, inverted noise maps of input images are created with DDIM Inversion. (2) In the Manipulation stage, various manipulations can be implemented based on text instructions or human interaction in the 3D space. (3) After manipulation, part-level masks and sketches are rendered and exported. The inverted noise maps are transformed according to these masks. (4) Finally, with the transformed inverted noise maps, sketch maps, and part-level masks, the generation model creates the edited images.

As shown in Fig. 2, manipulating objects through text instructions is a straightforward approach. For example, the instruction \(opening\)\(the\)\(laptop\)\(120^{}\) is converted to a script, then the manipulation will be done by running the script in 3D software. Our PA-Diffusion model also supports human interaction, which could be even more efficient. Additional manipulation guidance is provided in the Appendix. Contrary to the tedious manipulation experience of previous SOTA works, our proposed PA-Diffusion model offers a more flexible approach to editing articulated objects.

**Structure disentangle.** Some object parts could still be seen after manipulation, and some unseen parts would appear. As shown in the top right part in Fig. 3, the laptop shell can be seen in the input image. After opening the laptop, the shell can still be seen, however, the keyboard is newly revealed. Therefore, to distinguish them and implement part-aware diffusion, we disentangle articulated objects into **seen parts** and **novel-appearing parts**. The appearance of seen parts should be consistent between input and edited images, and the style of novel-appearing parts should be consistent with the objects' overall appearance. In this work, we regard the initial part-level masks \(M^{Init}\) as seen. Then after manipulation, we export manipulated part-level masks from the 3D software. And then manipulated seen parts mask \(M_{s}^{Gen}\) and manipulated novel-appearing parts mask \(M_{n}^{Gen}\) can be calculated.

### Dynamic Feature Maps

To maintain the object's appearance including color and texture, previous editing methods introduced a **guidance branch** to invert and re-generate the input image, and a **generation branch** to create the edited image, the attention/feature/activation maps are transferred from the guidance to the generation branch directly . However, when changing the object location or shape during manipulation, directly sharing these maps would transfer the feature from the input image to undesired locations in the edited image. Furthermore, these methods cannot reasonably generate novel views or novel-appearing parts.

To overcome these problems, we propose dynamic feature maps including manipulated inverted noise maps and compositional activation maps and images. To keep appearance accurate, manipulated inverted noise maps transfer the feature of seen parts in input images to the manipulated seen parts in edited images. Simultaneously, to make novel-appearing parts reasonably, compositional images let the diffusion model create these parts from random noise. The following content describes how to manipulate the noise maps, how to construct compositional maps, and how they work. For clarity, the process is presented in Fig. 3.

**Manipulated inverted noise map.** As shown at the top of Fig. 3, we firstly reverse the input image to the initial inverted noise map \(z_{T}^{Init}\) with DDIM inversion. After 3D manipulation, we calculate the transform function \(T\) based on the initial \(M^{Init}\) and manipulated seen part-level masks \(M_{s}^{Gen}\) and then compute the transformed inverted noise map \(z_{T}^{Train}\) with \(T\). The manipulated inverted noise map is created as the following equation:

\[z_{T}^{Man}=z_{T}^{Tran} M_{s}^{Gen}+z_{T}^{Tran} M_{s}^{Make}+z_{ T}^{Init} M_{bg},\] (5)

Figure 3: Algorithm pipeline of the PA-Diffusion model. Symbols and procedures in the figure are the same as those in the content.

where \(M_{s}^{Make}\) is the makeup mask generated by \(XOR(M^{Init}\), \(M^{Init} M_{s}^{Gen})\). \(M_{bg}\) is the background mask created by \(1-M_{s}^{Gen}\). In addition, we also create a random noise map \(z_{T}^{Ran}\). The three noise maps \(z_{T}^{Init}\), \(z_{T}^{Ran}\), and \(z_{T}^{Man}\) will be sent to the denoising UNet in the next step.

**Compositional activation map and image.** As shown at the bottom of Fig.3, the diffusion model runs a sequential denoising process with the three noise maps as a batch. The activation map \(A_{t}^{Gen\_{bg}}\) and \(A_{t}^{Gen\_{bg}}\) for seen parts and background are generated separately with the supervision of the guidance branch, and then merged to build \(A_{t}^{Gen}\) according to the seen part mask \(M_{s}^{Gen}\). After the decoder, the initial edited images \(I^{Gen}\) are created. Owing to the above steps, we transfer the feature of seen parts in input images to the accurate location in edited images. The other contents and the background can also be preserved, as the highlighted yellow part in Fig. 3. Besides, as the highlighted blue part in Fig. 3, an extra image \(I^{Ran}\) is synthesized with random noise map \(z_{T}^{Ran}\), the novel-appearing parts are cropped and pasted to edited images \(I^{Gen}\) as follows, which makes these parts more reasonable and consistent with the original inputs.

\[I^{Gen}=I^{Ran} M_{n}^{Gen}+I^{Gen}(1-M_{n}^{Gen}),\] (6)

### Score Function

**Texture Consistency Score Loss**. However, simply manipulating the inverted noise map will lead to a serious blurry problem. This is due to the denoising process includes several convolution steps. As the initial inverted noise map \(z_{T}^{Init}\) is not rotation invariant, manipulating \(z_{T}^{Init}\) will disturb the original distribution and make the denoising process fail. To alleviate this limitation, we construct Texture Consistency Score Loss (TCSL)  as an extra supervision that lets the specific region in the generation branch match with the one in the guidance branch,

\[Loss_{t}^{fg} =^{Gui}[M^{Init}],F_{t}^{Gen}[M_{s}^{Gen}])}\] (7) \[Loss_{t}^{bg} =^{Gui}[1-M^{Init}],F_{t}^{Gen}[1-M_{s}^{Gen}] )},\]

where \(F_{t}^{Gui}\) and \(F_{t}^{Gen}\) are feature maps of the guidance and generation branch. We add these items as extra losses in classifier guidance in each denoising iteration step to calibrate the appearance of objects and background.

**Style Consistency Score Loss**. For novel-appearing parts, the diffusion model is prone to randomly select a style to generate them with text guidance or sketch maps. As a result, the texture and style could be different from the objects in input images. Therefore, we introduce Style Consistency Score Loss (SCSL) to calibrate the style of seen parts and the novel-appearing parts.

Different from Texture Consistency Score Loss, there is no need to match every pixel in input images and edited images. Thus we calculate L1 loss between the feature maps \(F_{t}^{Ran}\) of the random branch and \(F_{t}^{Gui}\). The loss function is as follows:

\[Loss_{s}=|sum(F_{t}^{Gui}[M^{Init}])-sum(F_{t}^{Ran}[M_{n}^{Gen}])| _{1},\] (8)

Briefly, the reduced noise \(_{t}^{Gen\_{fg}}\), \(_{t}^{Gen\_{bg}}\), and \(_{t}^{Ran}\) in each denoising iteration can be presented as follows, where \(_{1}\), \(_{2}\), and \(_{3}\) are the hyper-parameters.

\[_{t}^{Gen\_{fg}} =_{t}^{Gen\_{fg}}+_{1}_{z_{t }}Loss_{t}^{fg} M_{s}^{Gen}\] (9) \[_{t}^{Gen\_{bg}} =_{t}^{Gen\_{bg}}+_{2}_{z_{t }}Loss_{t}^{bg}(1-M_{s}^{Gen})\] \[_{t}^{Ran} =_{t}^{Ran}+_{3}_{z_{t}}Loss_{s } M_{n}^{Gen}\]

## 4 Experiment

In this section, we provide two kinds of experiments to prove the advantages of our proposed PA-Diffusion model. First, various image editing tasks are conducted to showcase the model's image editing capabilities. To highlight the superiority of our model compared with state-of-the-art methods, we collect a testbench and evaluate all the methods both qualitatively and quantitatively. Second, we create a synthetic training set to support the challenging 3D articulated object understanding task in the robotic scenarios.

### Implementation

In this work, we select Grounded Segment Anything  to obtain the initial part-level object segmentation masks. T2I Adapter  is chosen as the conditional generation model, and the condition we used is the sketch map. The fundamental diffusion model is Stable Diffusion V1-5. All experiments run on a single NVIDIA A100 GPU. Notably, **NO** models need to be trained or fine-tuned in the image editing process.

Primitive Prototype Library is built within Blender . 3D planes, cubes, boxes, and other 3D primitive shapes are created and combined to represent different objects. In this work, 5 primitive shapes are collected and combined to represent 6 categories of articulated objects. The ease of creating prototypes allows for the quick incorporation of novel categories or instances. Various manipulations can be implemented in Blender efficiently.

### Results

Fig. 4 demonstrates the editing results of some basic manipulations and a sequential manipulation process. As shown in the left part, our PA-Diffusion model naturally moves, scales/shears, rotates and opens articulated objects with rotation or translation joint types. The edited objects blend seamlessly with other contents and backgrounds in the original images. When we move or rotate the objects, the blank regions in the background are in-painted semantically according to the surroundings. Moreover, in-painting and editing are completed in a single denoising process by the PA-Diffusion model, no extra in-paint model or process is required. Last but not least, novel-appearing parts of objects are generated reasonably, and the style matches with the object. For example, the storage furniture is empty after opening, and the keyboard of the laptop is designed logically.

The right part of Fig. 4 presents a complete operation process of opening articulated objects, from the initial closed state to the open state with 4 steps progressively. The appearance of objects' seen parts is transferred from the original input to different states accurately. The point that needs to be mentioned is that along with the operation progress, more novel-appearing parts of objects appear, our proposed PA-Diffusion model keeps the style and texture of these novel-appearing parts being consistent throughout the process. This capability potentially allows our method to generate a

Figure 4: Results of basic manipulations: move, scale/shear. rotate, and manipulate. Blank regions caused by the manipulation are in-painted automatically. The novel views and novel-appearing parts match with the style of the seen parts (left). Articulated objects are opened from the initial close state with 4 steps. The appearance of the increasing novel-appearing parts keeps being consistent throughout the whole process (right).

[MISSING_PAGE_FAIL:8]

### Quantitative Evaluation

To quantitatively evaluate our method, we built an articulated object manipulation testbench. The testbench comprises 6 object categories including storage furniture, laptop, trashcan, microwave, drawer, and refrigerator, which covers both rotation and translation joint types. In total, 660 real images are collected from the website. Considering articulated objects are typically rigid with uniform shapes, this testbench can represent the characteristics of common articulated object categories.

The comparison methods we select are Imagic and MasaCtrl (with T2I Adapter). DragDiffusion is excluded here, as it simply reconstructs the original image and cannot complete the manipulation tasks. Due to the long manual processing time and frequent failures in generating 3D models, Image Sculpting is also excluded here. To assess the realism of the edited images, the evaluation metric used is the Frechet Inception Distance (FID) score. The quantitative evaluation results are summarized in Tab. 1.

Since Imagic relies solely on text instructions, the edited images often do not align well with the original inputs, resulting in poor scores. Due to previously discussed reasons, the edited images of MasaCtrl are confusing and lack coherence. Sequentially, the FID score is not satisfying. In comparison, the PA-Diffusion model outperforms other methods with over 40.1% improvement.

### Articulated Object Understanding

In this session, we demonstrate how our proposed method supports the task of 3D articulated object understanding. As one of the fundamental steps to understanding 3D articulated objects, estimating the axes and surface normal is still challenging because of the lack of data. To release the data limitation, we create a synthetic dataset with the PA-Diffusion model. The dataset includes 660 sequential samples, each sample includes the sequence of opening objects from the close state with 4 steps.  introduced a 3-step training process to develop the object understanding model including

Figure 6: TCSL and SCSL are employed to release the blurry and style mismatch problem. The model is required to move and open the object (left). Comparison of Imagic, DragDiffusion, MasaCtrl (with T2I adapter), Image Sculpting, and our PA-Diffusion model. The target state is _‘a photo of an opened object’_ (right).

BBox detection, axis prediction, and plane normal estimation. Following this schedule, we evaluate the feasibility of edited images by two kinds of experiments.

First, the generated sequential samples are divided into training/testing sets (612/48). Specifically, we follow the 3-step to develop the model with half and full training samples separately, and then evaluate the model with three matrices, BBox IoU, Axes EA-score, and surface normal error smaller than \(30^{}\). Fig. 7 demonstrates the prediction results, the model can understand the structures of articulated objects after training with edited images, including moving plane, joint types, axis and surface normal. Quantitative evaluation results in Tab. 2 indicate that the prediction accuracy improves with more training samples, illustrating that the edited images are comparable to the real ones.

Second, to further evaluate the edited images, we merge them with the original training set of Internet Video dataset  and fine-tune the pre-trained model. The fine-tuned model is evaluated on the testing set (6,231 real images) of the InternetVideo Dataset. \(Baseline\) refers to the model trained on the InternetVideo dataset only. Here, we select to use the same evaluation matrices as , surface normal accuracy is multiplied with the accuracy of BBox and axis, other evaluation metrics are the same as above. The evaluation results are summarized in Tab. 3. Compared with the baseline, the overall performance has been improved by enlarging the training set with edited images. The above two experiments illustrate how our PA-Diffusion model can benefit robotic vision tasks.

## 5 Limitations

Even though our method can handle common articulated objects, there are still some limitations. First, as edited images are generated from inverted noise maps, the quality of the original input images significantly affects the editing outcomes. Blurry or low-resolution inputs will degrade the edited images. Second, when the object undergoes substantial deformation or movement, this editing method is likely to fail. Besides, manipulating deformable objects and fluids remains challenging with this approach. Further explanation is provided in the Appendix.

## 6 Conclusion

This work introduces the PA-Diffusion model, a novel articulated object manipulation method that covers common object categories and supports various manipulations. Both the qualitative and quantitative experiments have proven the feasibility and effectiveness of our method. Besides, the 3D articulated object understanding experiment illustrates that the PA-Diffusion model has positive impacts on helping build robots that interact with the real world smartly.

  Category & Imagic \(\) & MasaCtrl \(\) & **Ours \(\)** \\  Storage. & 5.85 & 2.27 & 0.81 \\ Laptop & 9.48 & 1.17 & 1.94 \\ Microwave & 1.34 & 3.15 & 0.87 \\ Trashcan & 5.62 & 1.59 & 0.96 \\ Refrigerator & 7.75 & 1.76 & 1.38 \\ Drawer & 30.7 & 0.98 & 0.60 \\ Avg. & 10.1 & 1.82 & **1.09** \\   
  Category & bbox \(\) & bbox+axis \(\) & normal \(<30^{}\) \\  Storage. & 67.5/65.0 & 65.0/65.0 & 59.9/61.5 \\ Laptop & 87.5/90.0 & 64.2/75.2 & 22.8/35.7 \\ Microwave & 80.0/85.0 & 80.0/85.0 & 72.4/70.7 \\ Trashcan & 95.0/92.5 & 74.4/84.9 & 40.5/49.2 \\ Refrigerator & 70.0/80.0 & 70.0/80.0 & 63.2/80.0 \\ Drawer & 70.0/82.5 & 70.0/78.8 & 67.5/82.5 \\ Avg. & 78.3/**82.5** & 70.6/**78.2** & 54.1/**63.2** \\  

Table 1: FID Score of edited images with Imagic, MasaCtrl (with T2I adapter) and ours.

  Dataset & AUROC \(\) & bbox \(\) & bbox+axis(rot) \(\) & bbox+axis(rot) &  & bbox+axis(train) \\  & & & normal \(\) & normal \(\) & normal \(\) \\  InternetVideo & 74.0 & 62.1 & 28.5 & 16.5 & 32.0 & 26.2 & 14.3 \\
**Mixed** & **74.7** & **66.1** & **29.2** & **18.3** & **38.1** & **30.0** & **19.3** \\  

Table 3: Mix the edited images with the training set of the InternetVideo dataset, then evaluate the fine-tuned model on the testing set of the InternetVideo dataset.

Figure 7: Detection results on the testing samples, including rotation and translation joint types.