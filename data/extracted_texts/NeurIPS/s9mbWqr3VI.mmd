# A Compact Representation for Bayesian Neural Networks By Removing Permutation Symmetry

Tim Z. Xiao1,2

zhenzhong.xiao@uni-tuebingen.de

&Weiyang Liu34

wl396@cam.ac.uk

Robert Bamler1

robert.bamler@uni-tuebingen.de

1University of Tubingen

1IMPRS-IS 3University of Cambridge

4Max Planck Institute for Intelligent Systems, Tubingen

###### Abstract

Bayesian neural networks (BNNs) are a principled approach to modeling predictive uncertainties in deep learning, which are important in safety-critical applications. Since exact Bayesian inference over the weights in a BNN is intractable, various approximate inference methods exist, among which sampling methods such as Hamiltonian Monte Carlo (HMC) are often considered the gold standard. While HMC provides high-quality samples, it lacks interpretable summary statistics because its sample mean and variance is meaningless in neural networks due to permutation symmetry. In this paper, we first show that the role of permutations can be meaningfully quantified by a number of transpositions metric. We then show that the recently proposed rebasin method  allows us to summarize HMC samples into a compact representation that provides a meaningful explicit uncertainty estimate for each weight in a neural network, thus unifying sampling methods with variational inference. We show that this compact representation allows us to compare trained BNNs directly in weight space across sampling methods and variational inference, and to efficiently prune neural networks trained without explicit Bayesian frameworks by exploiting uncertainty estimates from HMC.

## 1 Introduction and Background

When training a neural network on a data set \(\), one minimizes a loss function \((,)\) over weights and biases (collectively referred to as "weights" and denoted as boldface \(\) in the following). Yet, when comparing two trained networks (e.g., to choose a training algorithm or learning rate), one rarely compares the trained weights directly. Instead, one compares various performance metrics of the two trained networks on a held-out data set. Direct comparisons in weight space are difficult, in part because standard distance metrics (e.g., the euclidean distance) are not meaningful in weight space due to a permutation symmetry of neural networks [7; 3; 4; 1]: consider two consecutive layers of a neural network, which represent a function \(x_{2}(W_{2}_{1}(W_{1}x+b_{1})+b_{2})\), where \(W_{1,2}\), \(b_{1,2}\), and \(_{1,2}\) are weight matrices, bias vectors, and (componentwise applied) activation functions, respectively. It is easy to see that this function does not change if we consider an arbitrary permutation matrix \(P\) and replace the weights and biases by \(W^{}_{1}:=PW_{1}\), \(b^{}_{1}:=Pb_{1}\), and \(W^{}_{2}:=W_{2}P^{-1}\). Thus, euclidean distances in weight space like \(||W_{1}-W^{}_{1}||^{2}_{2}\) are not meaningful.

Recent works [1; 4] propose and analyze a method called rebasin, which efficiently finds a permutation that brings the weights \(_{1}\) of one neural network as close as possible to the weights \(_{0}\) of an independently trained reference network with the same architecture. The authors show that ap

Figure 1: Training dynamics for models with \(_{0}\) and \(_{1}\), and their interpolations \(_{}\).

plying this permutation essentially removes the loss barrier  when linearly interpolating between the weights of the two neural networks (see Figure 1 for before and after rebasin, where \(\) controls the interpolation \(_{}:=(-1)_{0}+_{1}\)). The authors conjecture that the loss landscape is quasi-convex up to permutations. In this paper, we build on these findings in three ways

1. We propose to quantify rebasin permutations by their number of transpositions (NoT), see below. We show empirically that NoT is a valuable metric for analyzing weight-space geometry as it is remarkably stable over training and correlates strongly with the loss barrier.
2. We argue that a quasi-convex loss landscape is particularly useful for Bayesian neural networks (BNNs), which aim to find (an approximation of) the posterior distribution \(p()\) over weights \(\) that are consistent with the data \(\). We show empirically that rebasin allows us to summarize the approximate posterior of sampling based inference methods like Hamiltonian Monte Carlo (HMC) in the same compact representation that variational inference (VI) uses, thus enabling direct comparisons across inference methods.
3. We show that the proposed unifying compact representation of BNNs is interpretable. For example, unusual for sampling methods in BNNs, we obtain meaningful explicit uncertainty estimates in weight space from HMC. We show that we can use these uncertainty estimates from HMC to efficiently prune a neural network trained with deep ensembles .

The paper is structured as follows: we introduce and empirically analyzes the NoT metric (item 1 above) in Section 2, discuss BNNs (items 2 and 3 above) in Section 3, and conclude in Section 4.

## 2 Quantifying Permutations in Weight Space by Number of Transpositions

We briefly introduce and analyze a metric to quantify the "magnitude" of permutations obtained from rebasin , which proved useful for building up intuition and for debugging implementations of experiments discussed in Section 3. Readers only interested in BNNs may opt to skip this part.

We propose to measure the magnitude of a permutation by its number of transpositions (NoT), i.e., the minimal number of pairwise swaps whose consecutive execution results in the given permutation. It is a well-known result from algebra  that this is always possible, and that NoT can be calculated efficiently by first factorizing a permutation into non-overlapping cycles and then expressing each cycle of length \(k\) as a product of \((k-1)\) transpositions. For example, the permutation \(P\) that maps \(1 4\), \(2 1\), \(3 5\), \(4 2\), and \(5 3\) can be written as \(P=(1\;4\;2)\,(3\;5)=(1\;4)\,(4\;2)\,(3\;5)\), where \((a_{1},a_{2},,a_{k})\) denotes a cycle \(a_{1} a_{2} a_{k} a_{1}\). Thus, \((P)=3\).

We find empirically that NoT is a meaningful metric for analyzing weight space geometry. We consider a neural network for classification of MNIST digits with a single hidden layer and a total of 512 hidden activations that can be permuted. We trained a network with randomly initialized weights \(_{}\) and a set of networks whose weights were initialized as \(P_{}\) where \(P\) is a random permutation with \((P)\) ranging over all values from zero to 511. After training both neural networks, we obtained a permutation \(P^{}\) by matching the two trained networks using rebasin.

As a first high-level check, Figure 2 (left) shows \((P^{})\) as a function of \((P)\) for three random \(_{}\). Even though the permuted and unperturbed networks were trained with different ran

Figure 2: Left three: effect of permuting initial weights by different Number of Transpositions (NoT) on NoT after training, weight-space distance, and loss barrier (shaded regions: \( 1\) over 5 runs). Right: NoT changes monotonically along the interpolation \(_{}\) between two models \(_{0}\) and \(_{1}\).

dom seeds for the sampling of minibatches, NoT remains almost exactly unaffected by training. Further, Figure 2 (center two) show that both the euclidean distance in weight space and the barrier of the loss function after training correlate strongly with NoT. Here, the barrier is defined as in  as \(_{}(_{},)- (_{0},)+(_{1}, )\), where \(_{}:=(-1)_{0}+_{1}\), and \(_{0}\) and \(_{1}\) are the weights of the two trained models. Finally, Figure 2 (right) analyzes \(()\) both over training epochs and along the linear interpolation \(\), where \(\) is obtained by matching \(_{}\) to \(_{0}\) using rebasin. We observe that the \(()\) remains flat in the vicinity of either trained model, and changes smoothly and monotonically in between.

From these observations, we conclude that, while comparing neural network weights by euclidean distance alone is not meaningful (see Section 1), we can meaningfully quantify weight-space distances by a pair \(||_{0}-P_{1}||_{2}^{2},(P)\) where the permutation \(P\) matches \(_{1}\) to \(_{0}\) by rebasin.

## 3 A Unifying Compact Representation for Bayesian Neural Networks

Building on the argument [14; 11; 18] that removing permutation degrees of freedom is particularly useful for _Bayesian_ Neural Networks (BNNs), we propose a framework to combine the strengths of two classes of inference algorithms in BNNs. Training a BNN amounts to finding (an approximation of) the so-called posterior distribution \(p(\,|\,)\) of all weights \(\) that are consistent with the training data \(\), and thus involves more than a single set of weights. We show below that being able to meaningfully compare weights makes approximate posteriors of BNNs more interpretable.

The exact posterior distribution is \(p(\,|\,)=p()p(\,|\,)/p( )\), where \(p()\) is a prior distribution that acts like a regularizer (often an isotropic Gaussian), \(p(\,|\,)=[-(,)]\), and \(p()= p()p(\,|\,)\, \). The exact posterior is usually intractable in BNNs, but various efficient approximation methods have been developed. We group them into two categories:

* **Parametric methods**, such as variational inference (VI; ) and Laplace approximation [15; 10] approximate the posterior \(p(\,|\,)\) explicitly with a simpler distribution \(q()\), e.g., a fully factorized normal distribution with fitted means and variances;
* **Sampling methods**, such as Hamiltonian Monte Carlo (HMC; ), stochastic gradient Langevin dynamics , and MCDropout  draw a set of \(K\) samples \(\{^{(k)}\}_{k=1}^{K}\) directly from \(p(\,|\,)\) without explicit representing their distribution; deep ensembles  is sometimes also considered in this context despite not following the Bayesian framework.

As sampling methods (b) lack explicit uncertainty information, one might be tempted to fit samples with a parametric distribution, e.g., a Gaussian \(q_{}()=(_{},(_{}^{2}))\), where \(_{}\) and \(_{}^{2}\) are the sample mean and variance, and the subscript 'd' is for 'direct'. However, we show in Section 3.1 that \(q_{}\) is a poor posterior approximation, likely because the posterior is multimodal due to the permutation symmetry. However, the quasi-convexity conjecture [1; 4] suggests that the posterior is unimodal once we remove the permutation degrees of freedom. Therefore, we propose to summarize samples from, e.g., HMC, with a diagonal Gaussian \(q_{}()=(_{},(_{}^{2}))\) where \(_{}\) and \(_{}^{2}\) are the sample mean and variance after using rebasin  to match each sample to an arbitrary shared reference sample. We show in Section 3.1 that \(q_{}\) approximates the posterior well despite its compactness, and in Section 3.2 that having a unifying compact representation allows us to combine the respective strengths of different inference methods, thus going beyond the findings in .

All experiments were done with a simple fully connected network for MNIST classification with a single hidden layer of size 512. We compare \(q_{}\) and \(q_{}\) across HMC, deep ensemble, and VI.

### Compact Representation as an Approximate Posterior

Before we use \(q_{}()\) to compare models directly in weight space, we first evaluate whether it provides a good approximation of the posterior despite radically reducing the amount of information available in the samples. Evaluations of BNNs are typically done by comparing the predictive distribution \(p(^{*}\,|\,^{*},)= p(^ {*}\,|\,^{*},)p(\,|\,)\, \) on a test set \(_{}\), where \(^{*}\) is the input, \(^{*}\) is the prediction, and we assume that \(p(^{*}\,|\,^{*},)\) is described by the neural network. There are two popular metrics to compare predictive distributions between a method \(p\) and HMC (which is often considered the most precise approximation of a BNN posterior ): _agreement_ and total variation_ (TV)  (in the following, \(I[\,\,]\) is the indicator function),

\[(p,p_{}) =_{}|}_{^{*} _{}}\!\!I*{arg\,max}_{^{*}}p (^{*}\,|\,^{*},)=*{arg\,max}_{^{*}}p _{}(^{*}\,|\,^{*}_{i},);\] (1) \[(p,p_{}) =_{}|}_{^{*} _{}}\!\!_{^{*}}p(^{* }\,|\,^{*}_{i},)-p_{}(^{*}\,|\,^{*}_{ i},).\] (2)

Results in Table 1 show that: (i) \(q_{}()\) has much better performance than \(q_{}()\) for both HMC and ensemble; thus, rebasin is crucial for obtaining a accurate compact representation; (ii) ensemble outperforms HMC in \(q_{}()\), which could indicate issues of the activation matching algorithm  when applied between networks with different loss levels, e.g., within HMC samples; and (iii) \(q_{}()\) provides a parameter efficient representation for ensemble with competitive performance.

### Comparing and Merging BNNs in Weight Space

Figure 3 (left) shows histograms of the variances \(_{}\) and \(_{}\) for HMC and ensemble, and the variances fitted by VI. We observe that (i) permuting all samples into the same basin reduces variances overall, as expected; and (ii) the two Bayesian methods (HMC and VI) have lots of weights with a variance close to one, which is not affected by rebasin in HMC. This indicates mode collapse since we used a standard Gaussian prior, i.e., the Bayesian methods identify these weights as unnecessary.

The proposed unifying compact representation allows us to merge different BNNs by stitching the means \(_{}\) from one model with the variances \(_{}^{2}\) from another. Figure 3 (right) shows test accuracies of neural networks after pruning weights with high \(_{}^{2}\) (a simplified variant of the compression method in [20; 16]). The purple curve uses weights \(_{}\) from ensemble but \(_{}^{2}\) from HMC, thus combining the predictive strength of ensemble with the accurate uncertainty estimates of HMC. It significantly outperforms both variants that use only the ensemble or only HMC (green curves).

## 4 Conclusion

When doing Bayesian inference in BNNs, it is straightforward to go from parametric based to sampling based inference, e.g., one can easily draw samples from a variational distribution. But permutation symmetry makes it difficult to go in the reverse direction. In this work, we use the recently proposed rebasin method to remove the permutation symmetry. We propose a unifying compact representation for Bayesian inference in BNNs, which allows us to go from sampling based inference to parametric based inference, and to combine the respective strengths of different inference methods.

  &  &  &  \\  & Sample & \(q_{}()\) & \(q_{}()\) & Sample & \(q_{}()\) & \(q_{}()\) & \(q()\) \\  (\(\)) Agreement with HMC samples & 1. & 0.1212 & 0.8249 & 0.9931 & 0.5239 & 0.9868 & 0.9885 \\ (\(\)) TV to HMC samples & 0. & 0.8641 & 0.6570 & 0.0229 & 0.7210 & 0.0495 & 0.0235 \\  Test Accuracy (\%) of Samples & \(98.43\) & \(11.11\) & 82.34 & 98.66 & 52.25 & 97.72 & 98.11 \\ Test Accuracy (\%) of \(_{}\) and \(_{}\) & N/A & \(28.06\) & 92.25 & N/A & \(86.40\) & 97.97 & 98.04 \\ 

Table 1: Performance of different BNNs (\(q_{}\): before rebasin; \(q_{}\): after rebasin) on their agreement (Eq. (1)) and total variation (TV; Eq. (2)) to HMC samples, and on their test set accuracy.

Figure 3: Left: histograms of the standard deviation \(\) of weights before (\(_{}\)) and after (\(_{}\)) rebasin. Right: test accuracy vs. various levels of weight pruning (retaining only weights with lowest \(\)).