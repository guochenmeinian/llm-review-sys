# Measuring Mutual Policy Divergence for Multi-Agent Sequential Exploration

Haowen Dou\({}^{1,2,3}\) Lujuan Dang\({}^{1,2,3,}\) Zhirong Luan\({}^{4}\) Badong Chen\({}^{1,2,3,}\)

\({}^{1}\)National Key Laboratory of Human-Machine Hybrid Augmented Intelligence,

\({}^{2}\)National Engineering Research Center for Visual Information and Applications,

\({}^{3}\)Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University,

\({}^{4}\)School of Electrical Engineering, Xi'an University of Technology

douhaowen@stu.xjtu.edu.cn, danglj@xjtu.edu.cn, luanzhirong@xaut.edu.cn

chenbd@mail.xjtu.edu.cn

Corresponding authors.

###### Abstract

Despite the success of Multi-Agent Reinforcement Learning (MARL) algorithms in cooperative tasks, previous works, unfortunately, face challenges in heterogeneous scenarios since they simply disable parameter sharing for agent specialization. Sequential updating scheme was thus proposed, naturally diversifying agents by encouraging agents to learn from preceding ones. However, the exploration strategy in sequential scheme has not been investigated. Benefiting from updating one-by-one, agents have the access to the information from preceding agents. Thus, in this work, we propose to exploit the preceding information to enhance exploration and heterogeneity sequentially. We present Multi-Agent Divergence Policy Optimization (MADPO), equipped with mutual policy divergence maximization framework. We quantify the discrepancies between episodes to enhance exploration and between agents to heterogenize agents, termed intra-agent divergence and inter-agent divergence. To address the issue that traditional divergence measurements lack stability and directionality, we propose to employ the conditional Cauchy-Schwarz divergence to provide entropy-guided exploration incentives. Extensive experiments show that the proposed method outperforms state-of-the-art sequential updating approaches in two challenging multi-agent tasks with various heterogeneous scenarios. Source code is available at https://github.com/hwdou6677/MADPO.

## 1 Introduction

Multi-Agent Reinforcement Learning (MARL) plays an increasingly important role in numerous real-world cooperative problems, such as smart grid management , autonomous driving , unmanned system control  and games . Centralized and decentralized MARL methods have been investigated as the first two extensions from single-agent to multi-agent systems. However, challenges have arisen regarding the curse of dimensionality and non-stationary training as the number of agents increasing . To address this issue, Centralized Training with Decentralized Execution (CTDE) was developed to disentangle training and execution phases . In the CTDE scheme,the centralized critic provides global information, guiding agents during training but not during execution. CTDE significantly simplifies and stabilizes the training process, providing an effective and efficient paradigm for policy-gradient cooperative MARL.

In CTDE, agents share parameters for homogeneous tasks, such as multi-particle coordination, and then take actions sampled from the same policies. For heterogeneous tasks, such as multi-joint coordination in robotic control, they learn distinct policies without sharing parameters and exhibit different behaviors. However, in these scenarios, relying solely on the non-parameter-sharing setting to achieve cooperation is an oversimplification (Bhattacharya et al., 2023). This is because agents can never learn optimal policies that depend on trajectories from other agents when updating simultaneously. To tackle this problem, sequential updating (Bertsekas, 2021) has been proposed to improve heterogeneity and collaboration. This updating scheme originates from the insight that agents in one rollout update their policies one-by-one, rather than simultaneously, to retain preceding agent information. Several sequential methods have been proposed by leveraging the multi-agent advantage decomposition lemma(Kuba et al., 2022), the multi-agent performance difference lemma (Wang et al., 2023), and rollout policy iteration (Bertsekas, 2021), to not only adapt the sequential updating scheme but also maintain the monotonic improvement property.

Despite the success of sequential policy updating, the exploration towards further heterogeneity improvement remains unexplored and challenging (Zhang et al., 2022). In MARL, agents struggle to learn globally optimal policy due to the huge exploration space complexity, which is, unfortunately, further amplified in heterogeneous tasks. Existing multi-agent exploration strategies typically require parameter sharing in homogeneous scenarios. However, when applied to heterogeneous scenarios, they suffer from performance degeneration despite employing the non-parameter sharing setting. This is because these methods fail to fully leverage the main advantage of sequential updating, _i.e._ the preceding information. To the best of our knowledge, there is no exploration method that can adapt to both heterogeneous scenarios with sequential updating and homogeneous scenarios with simultaneous updating.

To this end, this paper presents a novel sequential MARL framework, termed **M**ulti-**A**gent **D**ivergence **P**olicy **O**ptimization (MADPO), where a simple yet efficient exploration strategy is equipped to enhance sample efficiency, particularly in heterogeneous scenarios. In MADPO, we first propose a Mutual Policy Divergence Maximization (Mutual PDM) strategy to heterogenize agents. Specifically, mutual PDM consists of the intra-agent divergence and the inter-agent divergence. The intra-agent divergence measures the policy discrepancy between episodes, encouraging agents to learn diversified policies. The inter-agent divergence measures the policy discrepancy between agents, enhancing heterogeneity and promoting greater diversity. However, simply applying classical divergence measures to the proposed framework may trap the exploration in local optima due to the lack of positive incentives. To address this issue, we propose to employ conditional Cauchy-Schwarz (CS) divergence to provide entropy-guided incentives. Compared to the famous Kullback-Leibler (KL) divergence, the conditional CS divergence implicitly maximizes the entropy of current policy and is more stable. The main contributions can be summarized as follows:

1. We develop a novel multi-agent divergence reinforcement learning model equipped with mutual policy divergence maximization, termed MADPO, to enhance exploration and heterogenize agents in heterogeneous scenarios. To the best of our knowledge, we are the first to demonstrate the efficacy of policy divergence maximization in sequential MARL.
2. We propose to maximize conditional Cauchy-Shwarz policy divergence to provide entropy-guided incentive and stabilize multi-agent sequential exploration.
3. We evaluate the proposed method through extensive experiments. The results show that MADPO outperforms state-of-the-art sequential methods in two multi-joint coordination tasks with various heterogeneous scenarios.

## 2 Related Works

### Multi-Agent Reinforcement Learning with CTDE

Benefiting from the CTDE framework, multi-agent policy gradient algorithms have paved a promising path for cooperative games (Chai et al., 2021; Qiu et al., 2021; Li et al., 2021). For example, Wu et al. (2021) proposed CoPPO which guarantees the joint policy improvement by adapting the step size dynamically. Yu et al. (2022) proposed Multi-Agent Proximal Policy Optimization (MAPPO) which applies PPO to multi-agent scenarios without violating the guarantee of monotonic improvement in the individual policy level. Policy entropy incentive in MAPPO is one of the most related parts to our method, providing diversified policy learning from an information theory perspective. Li and He (2023) proposed MATRPO to extend Trust Region Policy Optimization (TRPO) to multi-agent tasks through a fully decentralized setting and distributed optimization. However, when the number of agents becomes large, MATRPO may encounter the challenge of the connecting link dimension curse. This is because it relies on communication rather than global information to facilitate cooperation. Guo et al. (2024) proposed MASPG, a trust region-based MARL algorithm in the off-policy manner, to enhance the sample efficiency of trust region methods. However, these methods require homogeneity of agents, _i.e._ parameter sharing, to ensure monotonic improvement. This homogeneity assumption can impose significant restrictions on agents, limiting their ability to explore the joint policy space adequately (Ding et al., 2022). Consequently, if the sharing of parameters is canceled, it can lead to violations of the monotonicity guarantee and result in performance degradation (Zhan et al., 2023).

### Sequential Updating MARL

The sequential updating scheme originates from single-agent rollout and policy iteration (Bertsekas, 2021), aiming to update policies of agents one by one, as shown in Fig. 1a. This structure encourages agents to learn different policies based on information from preceding ones, thereby naturally generalizing the homogeneous MARL to heterogeneous MARL. To build the multi-agent sequential updating scheme, attempts have been addressed from both joint and individual policy perspectives. For example, Bertsekas (2021) proposed rollout and policy iteration method, which was the first to consider sequential updating in MARL. Kuba et al. (2022) observed the multi-agent advantage decomposition lemma and proposed HAPPO. Leveraging this powerful lemma, HAPPO estimates and decomposes the joint advantage function to implement sequential updating and ensure the joint monotonic improvement. On the other hand, A2PO proposed by Wang et al. (2023) focuses on individual policy improvement by leveraging the multi-agent policy difference lemma. A2PO maintains distribution invariance during each agent's advantage estimation process and consider a more refined updating order. Zhao et al. (2023) introduced a localized action value function as the surrogate optimization objective, offering a provable convergence guarantee for multi-agent PPO.

### Information Theory Induced RL

Information-theoretic principles serve as a powerful regularization technique for providing valuable guidance in intrinsic reward-driven RL (Liu and Zhang, 2023; Subramanian et al., 2022; Russo and Proutiere, 2024), including both policy and state exploration (Cen et al., 2021; Jacob et al., 2022). For example, the Soft Actor-Critic (SAC) is the first to maximize the Shannon entropy of policies, promoting randomness and encouraging exploration (Haarnoja et al., 2018). On-policy methods, such as PPO, MAPPO and HAPPO, embrace the same concept by incorporating entropy regularization into the optimization objective. Additionally, recent advancements have explored the utilization of various entropy forms, such as encoder estimated stable entropy (Liu and Abbeel, 2021), value conditional entropy (Kim et al., 2023) and Renyi entropy (Yuan et al., 2023) to model environmental dynamics and accelerate novel state discovery. However, maximizing entropy only introduces stochasticity for measuring uncertain dynamics. To address this limitation, policy divergence regularization between episodes (Su and Lu, 2022; Xu et al., 2023) has been proposed. This regularization method calculates the policy divergence based on a fixed policy and offers more directed guidance compared to entropy alone. Furthermore, the efficacy of state divergence in combating local optima and fostering state novelty has been demonstrated (Hong et al., 2018; Yang et al., 2021). However, the existing divergence RL methods cannot be effectively extended to the sequential updating paradigm.

In this work, we pursue an on-policy method to enhance exploration and heterogenize agents in a sequential updating paradigm, termed **M**ulti-**A**gent **D**i**vergence **P**olicy **O**ptimization (MADPO). In contrast to the aforementioned policy divergence-based methods, we introduce a novel approach that maximizes inter- and intra-agent policy divergence, thereby incorporating policy information. To further address the deficiency of exploration direction in the traditional divergence RL, we propose to employ the conditional Cauchy-Schwarz divergence to provide an entropy-guided incentive.

## 3 Preliminaries

### MARL Problem Formulation

In this paper, we consider a multi-agent sequential decision-making problem, which can be described as a decentralized Markov decision process (DEC-MDP). A DEC-MDP with \(n\) agents can be formulated as the tuple: \(,},r,,\), where \(\) represents the state space. We denote \(N=\{1,,n\}\) as the set of finite agents. \(}=^{1}^{n}\) is the joint action space by taking the product of actions spaces of \(n\) agents. \(:}\) is the state transition function of the environment dynamics. \(r:}\) is the reward function. \(\) is the discount factor. At time step \(t\), to interact with the environment, each agent at state \(s_{t}\) takes an action \(a_{t}^{i}\) from its own policy \(^{i}(|s_{t})\) to form a joint action \(_{t}=\{a_{1}^{1},,a_{t}^{n}\}\) and a joint policy \((|s_{t})=^{1}^{n}\). The agents then receive a joint reward \(r(s_{t},_{t})\) and step to the new state \(s_{t+1}\) with the probability \((s_{t+1}|s_{t},_{t})\). The objective is to learn an optimal joint policy by maximizing the expected cumulative reward: \(}=*{arg\,max}_{}_{t=0}^{}_{s_{t}_{},_{t}}[^{t}r(s_{t},_{t})]\), where \(_{}\) is the marginal state distribution. Following Bellman Equations, the state-action value function and the state function of state \(s_{t}\) are defined as \(Q^{}(s_{t},_{t})=r(s_{t},_{t})+_{i>t}^{}_{s_{t}_{},_{i}}[^{i-t}r(s_{i},_{i })]\), and \(V^{}(s_{t})=_{i>t}^{}_{s_{t}_{}, _{i}}[^{i-t}r(s_{i},_{i})|s_{0}=s_{t}]\). And the advantage function is defined as \(A^{}(s_{t},_{t})=Q^{}(s_{t},_{t})-V^{}(s_{t})\).

### Multi-Agent Sequential Policy Updating Paradigm

Sequential updating paradigm was introduced to alleviate homogeneity in multi-agent reinforcement learning. The overview of sequential updating with a three-agent setting is shown in Fig. 0(a). For instance, Heterogeneous-Agent Proximal Policy Optimisation (HAPPO) takes preceding agent information into account by employing the multi-agent advantage decomposition lemma and the joint advantage estimator (Kuba et al., 2022). At episode \(k\), agent \(m\) in HAPPO maximizes the extrinsic

Figure 1: A three-agent example of traditional sequential updating MARL and our MADPO. The white boxes represent the policies to be updated \(^{i}\), and the orange boxes represent the updated policies \(^{i}\). The white boxes with dashed lines represent the joint policies to be updated \(\), and the orange ones represent the updated joint policy \(}\). Compared to the traditional sequential updating MARL, our method takes the intra-agent and inter-agent divergence into account, as shown in the blue boxes. The intra-agent divergence directs agents to explore novel policies based on their former policies, while the inter-agent divergence heterogenizes agents sequentially.

multi-agent clipping objective as formulated in Eq. 1,

\[r^{E}=_{s_{_{_{k}}},_{k}}[ (}(a^{i}|s)}{_{k}^{i_{m}}(a^{i}|s)})M^{i _{1:m}}(s,),clip(}(a^{i}|s)}{_{k}^{i_{m}}(a ^{i}|s)},1)M^{i_{1:m}}(s,)],\] (1)

where \(_{k}^{i_{m}}(a^{i}|s)\) is the policy of the \(m^{th}\) agent updated at episode \(k-1\), the superscript of \(r^{E}\) represents _Extrinsic_, and \(M^{i_{1:m}}(s,)\) is the joint advantage estimator of the first to \(m^{th}\) agents, which is defined as follows,

\[M^{i_{1:m}}=^{i_{1:m-1}}}{^{i_{1:m-1}}}(s,),\] (2)

where \(^{i_{1:m-1}}=_{p=1}^{m-1}^{i_{p}}\) is the joint policy of the first to \(m^{th}\) agents, and \((s,)\) is an individual advantage estimator, such as Generalized Advantage Estimation (GAE). Additionally, Eq. 1 is also incorporated with an intrinsic reward term, _i.e._ the policy entropy, defined as \(r^{I}=(^{i_{m}}(a^{i}|s))\).

## 4 Method

### Mutual Policy Divergence Maximization

Most existing divergence RL methods only consider state or policy divergence between episodes in a simultaneous updating scheme, which lacks practicality in heterogeneous scenarios. To address this issue, We introduce the main framework of MADPO in this section, _i.e._ Mutual Policy Divergence Maximization (Mutual PDM), and the framework is shown in Fig 1b. Specifically, we consider a mutual intrinsic reward which consists of two types of policy divergence: inter-agent and intra-agent policy divergence. At episode \(k\), agent \(i\) maximize mutual policy divergence as follows,

\[r^{I}_{mutual}= Div(^{i}_{k}|^{i-1}_{k})+(1-)Div( ^{i}_{k}|^{i}_{k-1}),\] (3)

where \(Div()\) is one divergence measurement, \(\) is the coefficient to control the influence of the two divergence, and the superscript \(I\) represents _Intrinsic_.

The first term in Eq. 3 is the inter-agent policy divergence, quantifying the discrepancy between policies of the current agent and the preceding agent. In heterogeneous tasks, such as multi-joint control in robotics, each agent has its own specialization. Hence, learning diversified policies for different agents is more desirable in these scenarios. By maximizing the inter-agent divergence, agents are provided with a novel optimization direction towards heterogeneity, resulting in significant diversification. Note that in the simultaneous updating manner, the inter-agent divergence maximization becomes theoretically challenging, since these methods lack access to the information from the preceding agents.

The second term in Eq. 3 is the intra-agent policy divergence, which measures the difference between the current policy and the former policy of an agent. The intra-agent policy divergence encourages agents to learn new and diverse policies based on their previous policies. Consequently, we provide agents an optimization direction towards policy novelty, greatly enhancing exploration.

### Conditional Cauchy-Schwarz Policy Divergence

To measure the discrepancy between policies, a natural idea is to use the KL-divergence. At episode \(k\), agent \(i\) optimizes the KL-divergence between the current policy \(^{i}_{k}\) and a fixed policy \(\), which can be defined as follows,

\[D_{KL}(^{i}_{k}||) = _{s_{j}_{_{_{k}}},a_{j}^{i}_ {k}}(_{j}^{i}_{k}(a_{j}|s_{j})_{k}(a_{j}|s_{j})}{ (a_{j}|s_{j})})\] (4) \[= (^{i}_{k},)-(^{i}_{k}),\] (5)

where \((^{i}_{k},)\) is the cross entropy between \(^{i}_{k}\) and \(\), \((^{i}_{k})\) is the policy entropy. However, optimizing KL-divergence between policies raises problems regarding instability and inhibition of exploration in MARL. Specifically, when approaching \(0\), the fixed policy \((a_{j}|s_{j})\) in Eq. 4 may lead to uncontrollability and unreliability of the log term, which is common in initialization and converged phrase of MARL. Moreover, the second term in Eq. 5 minimizes the entropy of the current policy, which brings an opposite optimization direction, thus adversely affecting exploration. To address these issues, we introduce Conditional Cauchy-Schwarz Divergence for policy divergence maximization.

The Conditional CS divergence, recently proposed by Yu et al. (2023), is an extension from classic CS divergence for quantifying the discrepancy between two conditional distributions. Formally, given random variable \(\) and \(\) with a finite data set, the CS inequality is defined as follows,

\[| p(a)q(a)da|^{2}|p(a)|^{2}da|q(a)|^{2}da,\] (6)

where \(p(a)\) and \(q(a)\) are the probability density functions. By leveraging Eq. 6, we can obtain the CS divergence, defined as \(D_{CS}=-}{ p^{2}(a)  q^{2}(a)}\). Similarly, we can derive the conditional CS divergence of two policies, _i.e._ the action distributions conditioned by the states, defined as follows,

\[D_{CS}((a|s)||(a|s))\] \[=-}_{}(a|s) (a|s)d)^{2}}{(_{}_{}^{2}(a|s)d)(_{}_{}^{2}(a|s)d)}\] \[=-2(_{}_{}(a|s)(a|s)d )+(_{}_{}^{2}(a|s)d )+(_{}_{}^{2}(a|s)d ).\] (7)

We then present some desirable properties of Eq 7.

**Proposition 1**.: _Given a policy to be updated \(\) and a fixed policy \(\), and \(\)-order Renyi policy entropy \(_{}()=_{}^{}(a|s) da=_{a}^{-1}(a|s)\), then we have:_

\[_{2}()+_{2}() D_{ CS}(|).\] (8)

Proofs can be found in Appendix A.1. Proposition. 1 is motivated by (Li et al., 2017) and indicates that the CS divergence between distributions is a lower bound of the sum of 2nd-Renyi entropy of distributions. Consequently, in MARL, maximizing the CS divergence between a target policy and a fixed policy behaves like maximizing the 2nd-Renyi entropy of the target policy, which is a generalized form of Shannon policy entropy (Yuan et al., 2023). In this way, by taking the conditional CS divergence into account, agents are encouraged to enhance their policy entropy while diversifying their policies. Thus, maximizing the CS policy divergence can provide agents with 2nd-Renyi entropy-guided exploration incentives.

**Proposition 2**.: _Given a policy to be updated \(\) and a fixed policy \(\) with a finite action set \(=\{a_{0},...,a_{n}\}\) at state \(s\), then the CS divergence is lower bounded by:_

\[D_{CS}(||)- n.\] (9)

Proofs can be found in Appendix A.2. Recall that in Eq. 4, it is obvious that the KL-divergence is unstable when the probability of one action approaching \(0\), a common occurrence in MARL. In contrast, Proposition. 2 demonstrates that the CS divergence has a deterministic lower bound unless the number of actions approaches infinity, which is not feasible in practical MARL. Even if in continuous action tasks, the trajectories sampled from policies have finite actions. Thus, maximizing the CS divergence can provide a more stable guidance for policy optimization.

### Multi-Agent Divergence Policy Optimization

We first present the overall optimization objective of MADPO in this section. At episode \(k\), agent \(i\) in MADPO maximizes the practical objective as follows,

\[(^{i}(a^{i}|s))=r^{E}(^{i}_{k}(a^{i}|s))+_{CS}(^{i}_{k}||^{i-1}_{k};)+_{CS}(^{i}_{k}||^{i}_{k-1};),\] (10)where \(_{CS}(||)\) is the estimator of the conditional CS divergence, and \(\) is the parameter of the estimator. Given trajectories \(^{}=\{s_{1}^{},a_{1}^{},...,s_{n}^{},a_{n}^{}\}\) and \(^{}=\{s_{0}^{},a_{0}^{},...,s_{n}^{},a_{n}^{}\}\) sampled from a fixed policy \(\) and the current policy \(\). The empirical estimator of Eq. 7 can be formulated by using kernel density estimation:

\[_{CS}((a|s)||(a|s))=(_{i=1}^{n} (^{n}_{ij}^{}_{ij}^{}}{(_{j=1}^{n}_{ij}^{})^{2}}))+ (_{i=1}^{n}(^{n}_{ij}^{} _{ij}^{}}{(_{j=1}^{n}_{ij}^{}) ^{2}}))\] \[-(_{i=1}^{n}(^{n}_{ij }^{}_{ij}^{}}{_{j=1}^{n}_ {ij}^{}}))-(_{i=1}^{n}(^ {n}_{ij}^{}_{ij}^{}}{_{j=1}^ {n}_{ij}^{}})),\] (11)

where \(^{}\) and \(^{}\) represent the Gram matrices of states and actions sampled from the policy \(\): \(_{ij}^{}=(s_{i}^{}-s_{j}^{})\), \(_{ij}^{}=(a_{i}^{}-a_{j}^{})\), where \(()\) is a Gaussian kernel denoted as \(()=(-}{2^{2}})\), and \(\) is the parameter of \(()\). Moreover, \(^{}\) and \(^{}\) represent the Gram matrices from distribution (_i.e._ policy) \(\) to distribution \(\), formulated as \(_{ij}^{}=(s_{i}^{}-s_{j}^{})\). Detailed proofs can be found in Yu et al. (2023).

In contrast to existing sequential methods, starting from the second agent in the first episode, MADPO maintains the buffer data for more time. Specifically, for mutual policy divergence maximization, when finished training in episode \(k\), MADPO maintains the minibatch of the updated \(k\)-th policies for one more episode. We summarize the whole algorithm in Algo. 1.

```
0: Initial joint policy \(_{0}=_{0}^{1}..._{0}^{n}\), parameters of Mutual PDM, \(\) and \(\).
1:for iteration \(k=1,...,K\)do
2: Collection trajectories \(_{k}=\{_{k}^{1},...,_{k}^{n}\}\) by running \(}_{k}=_{k}^{1}..._{k}^{n}\).
3: Restore \(_{k}\) into the buffer.
4: Compute the advantage \((s,)\) by using the V network.
5:for agent \(i=1,...,n\)do
6:if not \(i=1\)then
7: Compute the inter-agent divergence \(_{CS}(_{k}^{i}||_{k}^{i-1};)\) via trajectories \(_{k}^{i-1}\), \(_{k}^{i}\) and Eq. 11.
8:endif
9: Compute the intra-agent divergence \(_{CS}(_{k}^{i}||_{k-1}^{i};)\) via trajectories \(_{k-1}^{i}\), \(_{k}^{i}\) and Eq. 11.
10: Compute \(=r^{E}+r_{mutual}^{I}\) and update the actor network by maximizing Eq. 10.
11: Compute the joint advantage via Eq. 2.
12:endfor
13: Update the V network.
14: Delete \(_{k-1}\) from the buffer.
15:endfor ```

**Algorithm 1** Multi-Agent Divergence Policy Optimization

## 5 Experiments

We evaluate the proposed MADPO on two challenging multi-agent heterogeneous environments, **Multi-Agent Mujoco (MA-Mujoco)**(de Witt et al., 2020) and **Bi-DexHands**(Chen et al., 2022). Multi-Agent Mujoco is a complex and widely used task which necessitates up to \(17\) different joints of one robot to coordinate for human-like behavior imitation, such as running and walking. Bi-DexHands is a bimanual dexterous manipulation environment, where agents are in control of fingers, hands or joints. Sub-scenarios in Bi-DexHands require agents to collaborate for more complex bimanual tasks, such as opening a door inward and outward, passing an item from one hand to another. We compare MADPO with state-of-the-art MARL algorithms, including one simultaneous method MAPPO(Yu et al., 2022) and sequential methods, such as HATRPO(Kuba et al., 2022), HAPPO(Kuba et al., 2022) and A2PO(Wang et al., 2023). Clearly, different agents in the two benchmarks should learn diversified policies. Hence, we switch off the parameter sharing setting for HAPPO, HATRPO, A2PO and our MADPO, and keep sharing parameter in MAPPO. In this work, we conduct experiments of \(5\) random seeds on \(10\) scenarios of MA-Mujoco and \(10\) scenarios of Bi-DexHands.

We also conduct statistical testing experiments by using **rliable**(Agarwal et al., 2021). Since the environments we used in this work do not have a round end score, we choose the aggregate interquartile mean (IQM) sample efficiency test of reliable for evaluation. The interquartile mean (IQM) computes the mean scores of the middle \(50\%\) runs, while discarding the bottom and top \(25\%\). Here, we evaluate the performance across multiple tasks, and the total number of runs is \(n m\), where \(n\) is the number of trials for one task, and \(n=5\) in this paper. \(m\) is the number of tasks. IQM test is more robust than the mean and has less bias than the median. The experimental details can be found in Appendix B.

### Results on MA-Mujoco and Bi-DexHands

Fig. 2 and Fig. 3 show the results on MA-Mujoco and Bi-DexHands. The shaded areas represent the \(95\%\) confidence interval. we observe that MADPO consistently outperforms all baselines in MA-Mujoco, especially when the number of agents is large, indicating its efficiency in highly complex scenarios. Additionally, MADPO shows superior in challenging bimanual coordination tasks in Bi-DexHands, while other methods like HAPPO suffer from local optima due to insufficient exploration.

Fig. 4 shows the IQM rewards comparison against other baselines. The lines in the figures represent the IQM, while the shaded areas indicate the confidence intervals. The _10 tasks in MA-Mujoco_ include all the tasks of MA-Mujoco used and _10 tasks in Bi-Dexhands_ include all the tasks of Bi-Dexhands used. The _3 tasks of MA-Mujoco Ant_ include Ant-v2-2x4, 4x2, and 8x1. The _3 tasks of MA-Mujoco Halfcheetah_ include Halfcheetah-v2-2x3, 3x2, and 6x1. The _3 tasks of MA-Mujoco Walker2d_ include Walker2d-v2-2x3, 3x2, and 6x1. The results in Fig 4 indicates that the proposed MADPO consistently outperforms the state-of-the-art MARL methods in terms of best episodic reward across multiple tasks. The results also show that, MADPO has higher sample efficiency compared to other methods and achieves an improvement gap in most tasks.

### Ablation Study

We also investigate the efficiency of conditional CS policy divergence compared to other widely used exploration incentives as shown in Fig. 4(a) and Fig 4(b). Here, _no incentive_ presents disabling the intrinsic reward for training. We can clearly observe in Fig. 4(a) that the conditional CS policy divergence and KL-divergence achieve significant improvements compared to the popular policy entropy. These results indicate the effectiveness of mutual PDM framework, which takes the information from preceding agent into account. Additionally, the conditional CS policy divergence outperforms

Figure 2: Performance comparison against baseline methods on Multi-Agent Mujoco. Benefiting from the heterogeneity and exploration enhanced by mutual policy divergence maximization, the proposed MADPO consistently outperforms all baselines.

the famous KL-divergence empirically, particularly in MA-Mujoco tasks. This is because the KL-divergence implicitly minimizes the entropy of the current policy when maximizing the divergence, which can be detrimental to exploration. In contrast, the CS policy divergence maximizes policies' novelty and, more importantly, the Renyi entropy for efficient exploration. Fig. 4(b) demonstrates that in the aggregate evaluation, the CS-divergence outperforms other incentives with narrower confidence interval, indicating its better stability than KL-divergence.

Fig. 6 shows the experiments of parameter sensitivity. In this experiment, \(\) controls the influences of inter- and intra-agent policy divergence. When \(=0\), the inter-agent policy divergence is disabled, and when \(=1\), the intra-agent policy divergence is disabled. We can observe a consistent degradation in performance when any one aspect of the mutual policy divergence is turned off, thus confirming the significance of our method. When \(=0.2\), the proposed method achieves the highest reward, whereas excessive influence from inter-agent divergence with \(=0.5\) is harmful. Parameter \(\) controls the kernel width in Cauchy-Schwarz divergence, impacting the influence of the mutual PDM. We find that MADPO is slightly sensitive to \(\), behaving similarly to the entropy coefficient in MAPPO.

## 6 Conclusion

In this work, we present MADPO, a sequential updating MARL method equipped with mutual policy divergence maximization for efficient exploration in heterogeneous tasks. By leveraging the sequential updating paradigm, MADPO maximizes intra-agent policy divergence to enhance exploration and inter-agent policy divergence to promote heterogeneity. However, maximizing traditional divergence measurements can lead to instability and lack of direction in MARL. To tackle this issue, we propose conditional Cauchy-Schwarz policy divergence to quantify the distance between policies. The

Figure 4: IQM performance comparison against baseline methods on 10 tasks of Bi-DexHands and 9 tasks of MA-mujoco.

Figure 3: Performance comparison against baseline methods on Bi-DexHands. The proposed MADPO achieves superior performance compared to other MARL methods.

conditional Cauchy-Schwarz policy divergence possesses favorable properties and provides a stable entropy-guided incentive for sequential exploration. We evaluate the performance of MADPO on two challenging heterogeneous tasks, MA-Mujoco and Bi-Dexhands. We observe that the proposed Mutual PDM outperforms entropy-based methods since it consider both previous and preceding information. Moreover, we verify the efficiency of the conditional Cauchy-Schwarz policy divergence in terms of stabilizing and guiding the exploration. Totally, the results demonstrate the effectiveness of MADPO, achieving state-of-the-art performance in complex multi-agent scenarios. The main limitation of this work is that when the number of agent increases, the proposed method may require more ram to restore previous information. We will investigate effective representation methods for previous information in the future.