# Searching for Optimal Per-Coordinate Step-sizes

with Multidimensional Backtracking

 Frederik Kunstner Victor S. Portella Mark Schmidt Nick Harvey

{kunstner,victorsp,schmidtm,nickhar}@cs.ubc.ca

University of British Columbia Canada CIFAR AI Chair (Amii)

###### Abstract

The backtracking line-search is an effective technique to automatically tune the step-size in smooth optimization. It guarantees similar performance to using the theoretically optimal step-size. Many approaches have been developed to instead tune per-coordinate step-sizes, also known as diagonal preconditioners, but none of the existing methods are provably competitive with the optimal per-coordinate step-sizes. We propose _multidimensional backtracking_, an extension of the backtracking line-search to find good diagonal preconditioners for smooth convex problems. Our key insight is that the gradient with respect to the step-sizes, also known as hyper-gradients, yields separating hyperplanes that let us search for good preconditioners using cutting-plane methods. As black-box cutting-plane approaches like the ellipsoid method are computationally prohibitive, we develop an efficient algorithm tailored to our setting. Multidimensional backtracking is provably competitive with the best diagonal preconditioner and requires no manual tuning.

## 1 Introduction

When training machine learning models, tuning the hyperparameters of the optimizer is often a major challenge. For example, finding a reasonable step-size hyperparameter for gradient descent typically involves trial-and-error or a costly grid search. In smooth optimization, a common approach to set the step-size without user input is a backtracking line-search: start with a large step-size, and decrease it when it is too big to make sufficient progress. For ill-conditioned problems, however, there are limits to the improvement achievable by tuning the step-size. Per-coordinate step-sizes--also known as diagonal preconditioners--can drastically improve performance. Many approaches have been developed to automatically tune per-coordinate step-sizes. Those are often described as "adaptive" methods, but the meaning of this term varies widely, from describing heuristics that set per-coordinate step-sizes, to ensuring performance guarantees as if a particular property of the problem were known in advance. Yet, even on the simplest case of a smooth and strongly convex deterministic problem where a good fixed diagonal preconditioner exists (i.e., one that reduces the condition number), none of the existing adaptive methods are guaranteed to find per-coordinate step-sizes that improve the convergence rate. We discuss approaches to adaptive methods in the next section.

**Contribution.** We propose _multidimensional backtracking_, an extension of the standard backtracking line-search to higher dimension, to automatically find good per-coordinate step-sizes. Our method recovers the convergence rate of gradient descent with the _optimal preconditioner_ for the problem, up to a \(\) factor where \(d\) is the number of coordinates. This is a direct generalization of the line-search guarantee, with a penalty depending on dimension due to the extra degrees of freedom, as expected.

### Adaptive step-sizes and preconditioning methods

**Adaptive and parameter-free methods in online learning** are an example where _adaptive methods_ have a well-defined meaning. AdaGrad (McMahan and Streeter, 2010; Duchi et al., 2011) and Coin Betting (Orabona and Pal, 2016; Orabona and Tommasi, 2017) can adapt to problem-specific constants without user input and have strong guarantees, even in the _adversarial_ setting. However,this resilience to adversaries is a double-edged sword; to satisfy this definition of adaptivity, AdaGrad uses monotonically decreasing step-sizes. While AdaGrad still converges at the desired asymptotic rate on smooth, Lipschitz functions (Ward et al., 2019; Li and Orabona, 2019), its performance can be worse than plain gradient descent. This motivated investigations of workarounds to avoid the monotonically decreasing updates, including augmenting the update with an increasing step-size schedule (Agarwal et al., 2020), a line-search (Vaswani et al., 2020), or modifying the update to the preconditioner (Defazio et al., 2022). Methods commonly used in deep learning, such as RMSProp and Adam (Hinton et al., 2012; Kingma and Ba, 2015), are often motivated as _adaptive_ by analogy to AdaGrad, but without decreasing step-sizes (e.g., Defossez et al., 2022, SS4.3). This change is crucial for their practical performance, but nullifies their online-learning adaptivity guarantees.

**Adaptive gain and hypergradient heuristics.** Many heuristics that tune the hyperparameters of the optimization procedure use the gradient with respect to the hyperparameters, or _hypergradients_(Maclaurin et al., 2015). Methods have been proposed to tune the step-size (Masse and Ollivier, 2015), a preconditioner (Moskovitz et al., 2019), any hyperparameter (Baydin et al., 2018), or to maintain a model of the objective (Bae et al., 2022). "Stacking" such optimizers recursively has been shown to reduce the dependency on user-specified hyperparameters in practice (Chandra et al., 2022). This idea pre-dates the hypergradient nomenclature; Kesten (1958) presents a method to update the step-size based on the sign of successive gradients, and Saridis (1970) presents a control perspective for per-coordinate step-sizes, which can be cast as a hypergradient update to a diagonal preconditioner.1 This approach has led to _adaptive gain_ methods such as Delta-Delta and variants (Barto and Sutton, 1981; Jacobs, 1988; Silva and Almeida, 1990; Sutton, 1992a,b), and further developed using the sign of the hypergradient (Riedmiller and Braun, 1993), full-matrix updates (Almeida et al., 1999), a larger history (Plagianakos et al., 2001), updates in log-space (Schraudolph, 1999; Schraudolph et al., 2005), heuristics to adjust the outer step-size (Mahmood et al., 2012), or multiplicative weight updates (Amid et al., 2022). While showing promising practical performance in some settings, existing methods are often motivated from intuition rather than a formal definition of adaptivity, giving no guarantee that the tuned method will converge faster, if at all. Indeed, hypergradient methods are often unstable, and may require as much manual tuning as the original optimizer they are intended to tune.

**Second-order methods.** A classical approach to preconditioning is to use second-order information, as in Newton's method or its regularized variants (e.g., Nesterov and Polyak, 2006). To avoid the load of computing and inverting the Hessian, quasi-Newton methods (Dennis and More, 1977) such as L-BFGS (Liu and Nocedal, 1989) fit an approximate Hessian using the secant equation. Variants using diagonal approximations have also been proposed, framed as Quasi-Cauchy, diagonal BFGS, or diagonal Barzilai-Borwein methods (Zhu et al., 1999; Andrei, 2019; Park et al., 2020), while other methods use the diagonal of the Hessian (LeCun et al., 2012; Yao et al., 2021). Some second-order and quasi-Newton methods converge super linearly (although not the diagonal or limited memory variants used in practice), but those guarantees only hold locally when close to the minimum. To work when far from a solution, those methods require "globalization" modifications, such as regularization or a line-search. Unfortunately, analyses of second-order methods do not capture the global benefit of preconditioning and instead lead to worse rates than gradient descent, as in the results of Byrd et al. (2016, Cor. 3.3), Bollapragada et al. (2018, Thm. 3.1), Meng et al. (2020, Thm. 1), Yao et al. (2021, Apx.), Berahas et al. (2022, Thm. 5.2), or Jahani et al. (2022, Thm. 4.9).

**Line-searches.** Adaptivity in smooth optimization is most closely related to line-searches. The standard guarantee for gradient descent on an \(L\)-smooth function requires a step-size of \(1/L\), but \(L\) is typically unknown. The backtracking line-search based on the Armijo condition (Armijo, 1966) approximately recovers this convergence guarantee by starting with a large step-size, and backtracking; halving the step-size whenever it does not yield sufficient improvement. However, line-searches are often overlooked in the discussion of adaptive methods, as they do not provide a way to set more than a scalar step-size. While line-searches can be shown to work in the stochastic overparameterized setting and have been applied to train neural networks (Vaswani et al., 2019), improvements beyond backtracking have been limited. Additional conditions (Wolfe, 1969), non-monotone relaxations (Grippo et al., 1986), or solving the line-search to higher precision (More and Thuente, 1994) can improve the performance in practice, but even an exact line-search cannot improve the convergence rate beyond what is achievable with a fixed step-size (Klerk et al., 2017).

### Summary of main result: adaptivity to the optimal preconditioner

Our approach is inspired by the work discussed above, but addresses the following key limitation: none of the existing methods attain better global convergence rates than a backtracking line-search. Moreover, this holds even on smooth convex problems for which a good preconditioner exists.

We generalize the backtracking line-search to handle per-coordinate step-sizes and find a good preconditioner. As in quasi-Newton methods, we build a preconditioner based on first-order information. However, instead of trying to approximate the Hessian using past gradients, our method searches for a preconditioner that minimizes the objective function at the next step. Our convergence result depends on the best rate achievable by an _optimal diagonal preconditioner_, similarly to how methods in online learning are competitive against the best preconditioner in hindsight. However, our notion of optimality is tailored to smooth strongly-convex problems and does not require decreasing step-sizes as in AdaGrad. Our update to the preconditioner can be interpreted as a hypergradient method, but instead of a heuristic update, we develop a cutting-plane method that uses hypergradients to guarantee a good diagonal preconditioner. Our main theoretical contribution is summarized below.

**Theorem 1.1** (Informal).: _On a smooth, strongly-convex function \(f\) in \(d\) dimensions, steps accepted by multidimensional backtracking guarantee the following progress_

\[f(_{t+1})-f(_{*})1-}}f(_{t})-f(_{*}),\]

_where \(_{*}\) is the condition number achieved by the optimal preconditioner defined in Section 2. The number of backtracking steps is at most linear in \(d\) and logarithmic in problem-specific constants._

Multidimensional backtracking finds per-coordinate step-sizes that lead to a provable improvement over gradient descent on badly conditioned problems that can be improved by diagonal preconditioning, i.e., if the condition number of \(f\) is at least \(_{*}\). Moreover, this guarantee is worst-case, and multidimensional backtracking can outperform the globally optimal preconditioner by finding a better _local_ preconditioner, as illustrated on an ill-conditioned linear regression problem in Figure 1.

To find a competitive diagonal preconditioner, we view backtracking line-search as a cutting-plane method and generalize it to higher dimensions in Section 3. In Section 4 we show how to use hypergradients to find separating hyperplanes in the space of preconditioners, and in Section 5 we develop an efficient cutting-plane methods tailored to the problem. In Section 6, we illustrate the method through preliminary experiments and show it has consistent performance across problems.

**Notation.** We use standard font weight \(d\), \(n\), \(\) for scalars, bold \(\), \(\) for vectors, and capital bold \(\), \(\) for matrices. We use \([i]\) for the \(i\)-th entry of \(\), use \(\) for element-wise multiplication, and define \(^{2}\). We use \(=()\) to denote the diagonal matrix with diagonal \(\), and \(=()\) to denote the vector of diagonal entries of \(\). We say \(\) is larger than \(\), \(\), if \(-\) is positive semidefinite. If \(=()\), \(=()\), the ordering \(\) is equivalent to \([i][i]\) for all \(i\), which we write \(\). We use \(\) for the identity matrix and \(\) for the all-ones vector.

Figure 1: **Multidimensional backtracking can find the optimal diagonal preconditioner. Example on a linear regression where the optimal preconditioner can be computed. Left: Performance of Gradient Descent (GD), optimally preconditioned GD (\(_{*}\)GD) with a line-search (+LS), and Multidimensional Backtracking (MB) with the strategies in Section 5. The ellipsoid variant can outperform the _globally_ optimal preconditioner by selecting preconditioners that leads to more _local_ progress. Right: Optimal per-coordinate step-sizes (\(\)) and the ones found by MB (box) across iterations.**

## 2 Optimal preconditioning and sufficient progress

Consider a twice-differentiable function \(f^{d}\) that is \(L\)-smooth and \(\)-strongly convex,2 i.e.,

\[\|-\|^{2} f()-f()-  f(),- L \|-\|^{2},,,\]

or \(^{2}f() L\) for all \(\). We measure the quality of a preconditioner \(\) by how well \(^{-1}\) approximates the Hessian \(^{2}f()\). We define an _optimal diagonal preconditioner_ for \(f\) as

\[_{}*{arg\,min}_{ 0, }^{-1} ^{2}f()^{-1},\] (1)

and denote by \(_{}\) the optimal \(\) above. A related and known measure for the convergence rate of preconditioned methods is \((^{1/2}^{2}f()^{1/2})\)(Bertsekas, 1999, SS1.3.2). Moreover, (1) reduces to the definition of optimal preconditioning for linear systems (Jambulapati et al., 2020; Qu et al., 2022) when \(f\) is quadratic. Alternatively, the optimal preconditioner can be viewed as the matrix \(_{}\) such that \(f\) is \(1\)-smooth and maximally strongly-convex in the norm \(\|\|_{_{}^{-1}}^{2}=,_{}^{-1}\),

\[}\|-\|_{_{ }^{-1}}^{2} f()-f()- f(), -\|-\|_{ _{}^{-1}}^{2},,.\] (2)

Similar definitions of smoothness and strong-convexity relative to a matrix are common in coordinate descent methods (e.g., Qu et al., 2016; Safaryan et al., 2021), where the matrices are assumed to be known a priori. If we knew \(_{}\), preconditioned gradient descent using \(_{}\) would converge at the rate

\[f(x-_{} f())-f(_{})1 -}(f()-f(_{})),\]

where \(_{}\) minimizes \(f\). We do not know \(_{}\) and will be searching for a good approximation.

For the standard backtracking line-search on \(L\)-smooth functions, the goal is to find a step-size that works as well as \(}{{L}}\) without knowledge of \(L\). To do so, we can start with a large step-size \(}{{L}}\) and check the _Armijo condition_: the step-size \(\) makes progress as if \(f\) were \(1/\)-smooth, that is,

\[f(- f()) f()- \| f()\|^{2}.\] (3)

If the condition is satisfied, we take the step \(- f()\). By the descent lemma, (Bertsekas, 1999, A.24), the condition is satisfied if \(}{{L}}\). So if the condition fails, we know \(\) is too large and can decrease \(\). For diagonal preconditioners, the Armijo condition checks whether the preconditioner makes sufficient progress in the norm induced by \(^{-1}\), as if \(f\) were \(1\)-smooth in Equation (2), that is,

\[f(- f()) f()- \| f()\|_{}^{2}.\] (4)

As with a scalar step-size, sufficient progress holds for any matrix \(\) that satisfies \(^{2}f()^{-1}\).

## 3 Multidimensional Backtracking

The typical presentation of the backtracking line-search maintains a step-size and decreases it when the Armijo condition fails (e.g., Nocedal and Wright, 1999, Alg 3.1). We instead take the following non-standard view, which generalizes more naturally to high dimension; as maintaining a set _containing_ the optimal step-size, and using bisection to narrow down the size of the set. Starting with an interval \(=[0,_{}]\) containing \(}{{L}}\), we pick a candidate step-size \(\) by "backtracking" by \(<1\) from the largest step-size in \(\), taking \(=_{}\) to balance two properties;

1. **Large progress:** If the candidate step-size satisfies the Armijo condition and the step is accepted, the value of \(f\) decreases proportionally to \(\) as in (3). To maximize the progress, \(\) should be large.
2. **Volume shrinkage:** If the candidate step-size fails the Armijo condition, we learn that \(>}{{L}}\) and can cut the interval to \(^{}=[0,_{}]\). To ensure the interval shrinks fast, \(\) should be small.

Taking \(=}{{2}}\) balances both properties; \(\) is at least \(}{{2}}\) as large as any step-size in \(\), and we can halve the interval if the Armijo condition fails. We do not use \(_{}\) as a candidate since, although the largest in \(\), it would give no information to update the interval in case it failed the Armijo condition.

For multidimensional backtracking, we can check whether a candidate preconditioner yields sufficient progress with Equation (4) instead of the Armijo condition, and replace the intervals by sets of diagonal preconditioners. The high-level pseudocode is given in Figure 2, where each iteration either leads to an improvement in function value or shrinks the sets of potential step-sizes/preconditioners.

To complete the algorithm, we need to define the steps marked as \(()\) to select preconditioners that lead to large progress when the step is accepted, while significantly reducing the search space when the preconditioner does not yield sufficient progress. For computational efficiency, we want methods that take \(O(d)\) time and memory like plain gradient descent.

### Guaranteed progress competitive with the optimal preconditioner

We start by formalizing the progress guarantee. If \(_{t}\) satisfies the Armijo condition (4) at \(_{t}\), the function value decreases by at least \(\| f(_{t})\|_{_{t}}^{2}\). If we can guarantee that \(\| f(_{t})\|_{_{t}}^{2}\| f(_{t})\|_{_{t}}^{2}\) for some \(>0\), we can recover the convergence rate of gradient descent preconditioned with \(_{t}\) up to a factor of \(\). However, we do not know \(_{*}\), but know a set \(_{t}\) that contains preconditioners we have not yet ruled out, including \(_{*}\). To guarantee that \(_{t}\) is competitive with \(_{t}\), we can enforce that \(_{t}\) is competitive with _all_ the preconditioners in \(_{t}\), as captured by the following definition.

**Definition 3.1** (\(\)-competitive candidate preconditioners).: _A matrix \(_{t}_{t}\) is \(\)-competitive in \(_{t}\), for a gradient \( f(_{t})\), if \(\| f(_{t})\|_{_{t}}^{2}\| f(_{t})\|_{}^{2}\) for any \(_{t}\)._

If \(_{t}\) is \(\)-competitive, then it is competitive with \(_{*}\) as \(_{_{t}}\| f(_{t})\|_{} ^{2}\| f(_{t})\|_{_{t}}^{2}\). However, this is a strong requirement. To illustrate what competitive ratios are attainable, we show in Appendix B that even the optimal preconditioner \(_{*}\) might only be \(}{{d}}\)-competitive, as other preconditioners can lead to more _local_ progress depending on \( f(_{t})\), whereas \(_{*}\) is a fixed _global_ optimal preconditioner. This also suggests that selecting a preconditioner that guarantees more local progress may lead to better performance, which we take advantage of to ensure a \(=}{{}}\) competitive ratio.

To see how to ensure a competitive ratio, consider the case where \(\) contains diagonal preconditioners whose diagonals come from the box \(():=\{\,_{ 0}^{d}: \}\). To select a candidate preconditioner that is \(\)-competitive in \(\), we can backtrack from the largest vector in \(()\) by some constant \(<1\), and take \(=()\). While a large \(\) leads to more progress when the step is accepted, we will see that we need a small \(\) to ensure the volume shrinks when the step is rejected.

We can obtain the convergence rate of Theorem 1.1 depending on \(\) and the optimal preconditioned condition number \(_{*}\) if we ensure \(_{*}_{t}\) and that \(_{t}\) is \(\)-competitive for all \(t\).

**Proposition 3.2**.: _Let \(_{*},_{*}\) be an optimal preconditioner and condition number for \(f\) (1). If the set \(_{t}\) from the algorithm in Figure 2 contains \(_{*}\), and \(_{t}_{t}\) is \(\)-competitive (Definition 3.1), then_

\[f(_{t+1})-f(_{*})1-} (f(_{t})-f(_{*}))\]

_whenever the candidate step leads to sufficient progress and is accepted._

Proof.: The proof relies on three inequalities. (a) The iterate \(_{t+1}\) yields sufficient progress (Eq. 4), (b) any accepted preconditioner \(_{t}\) is \(\)-competitive in \(_{t}\) and thus with \(_{*}\), and (c) \(f\) is \(}{{_{*}}}\)-strongly convex in \(\|\|_{^{-1}}\), which implies \(_{*}\| f(_{t})\|_{_{t}}^{2} f( _{t})-f(_{*})\). Combining those yields

\[f(_{t+1})}{{}}f(_{t})- \| f(_{t})\|_{_{t}}^{2}}{{}}f(_{t})-\| f(_{t}) \|_{_{*}}^{2}}{{}}f(_{t})- }(f(_{t})-f(_{*})).\]

Subtracting \(f(_{*})\) on both sides yields the contraction guarantee.

Figure 2: **Pseudocode for the backtracking line-search and multidimensional backtracking. We view backtracking as maintaining a set of step-sizes, testing one at each iteration that either make progress on \(f\) or reduce the size of the set. Steps marked by \(()\), are the subject of Sections 3–5.**

## 4 Separating hyperplanes in higher dimensions

In one dimension, if the step-size \(\) does not satisfy the sufficient progress condition (3), we know \(>}{{L}}\) and can rule out any \(^{}\). We are looking for a generalization to higher dimensions: if the queried preconditioner fails the sufficient progress condition, we should be able to discard all larger preconditioners. The notion of _valid_ preconditioners formalizes this idea.

**Definition 4.1** (Valid preconditioner).: _A preconditioner \(\) is valid if \(^{}{{2}}}^{2}f()^{}{{2}}}\) for all \(\), which guarantees that \(\) satisfies the sufficient progress (4) condition, and invalid otherwise._

Validity is a global property: a preconditioner \(\) might lead to sufficient progress locally but still be invalid. Using the partial order, if \(\) is invalid then any preconditioner \(^{}\) is also invalid. However, this property alone only discards an exceedingly small portion of the feasible region in high dimensions. Consider the example illustrated in Figure 2(a): if the diagonals are in a box \(()\), the fraction of volume discarded in this way if \((}{{2}})()\) is invalid is only \(}{{2^{d}}}\).

To efficiently search for valid preconditioners, we show that if \(f\) is convex, then the gradient of the sufficient progress condition gives a _separating hyperplane_ for valid preconditioners. That is, it gives a vector \(^{d}\) such that if \(^{d}_{ 0}\) satisfies \(,>1\), then \(()\) is invalid, as illustrated in Figure 2(b). We use the following notation to denote normalized half-spaces:

\[_{>}()\{\,^{d}_{ 0}: ,>1\}_{}()\{^{d}_{  0}:, 1\}.\]

**Proposition 4.2** (Separating hyperplane in preconditioner space).: _Suppose \(=() 0\) does not lead to sufficient progress (4) at \(\), and let \(h()\) be the gap in the sufficient progress condition,_

\[h() f(- f())-f( )+\| f()\|_{}^{2}>0.\]

_Then \(()\) for any \(\) in the following half-space satisfies \(h()>0\) and is also invalid,_

\[\{\,^{d}: h(), > h(),-h()\},\] (5)

_This half-space is equal to \(_{>}()\) with \(\) given by \(=)}}{{( h(), -h())}}\), or_

\[-^{+})}{f()-^{+},-f(^{+})}, \{ &^{+} - f(),\\ &(,^{+})( f(),  f(^{+}))..\] (6)

Proof idea.: If \(f\) is convex, then \(h\) also is. Convexity guarantees that \(h() h()+ h(),- \) for any \(\). A sufficient condition for \(h()>0\), which means \(\) is invalid, is whether \(h()+ h(),->0\) holds. Reorganizing yields Equation (5), and Equation (6) expresses the half-space in normalized form, \(_{>}()\), expanding \(h\) in terms of \(f\), its gradients, and \(\). 

The half-space in Proposition 4.2 is however insufficient to find good enough cutting-planes, as it uses convexity to invalidate preconditioners but ignores the ordering that if \(\) is invalid, any \(^{}\) is also invalid. If such preconditioners are not already ruled out by convexity, we can find a stronger half-space by removing them, as illustrated in Figure 2(c). We defer proofs to Appendix C.

**Proposition 4.3** (Stronger hyperplanes).: _If \(_{>}()\) is a half-space given by Proposition 4.2, then \(_{>}()\) where \(\{,0\}\) element-wise is a stronger half-space in the sense that \(_{>}()_{>}()\), and \(_{>}()\) contains only invalid preconditioners._

Figure 3: **Lack of information from the ordering and separating hyperplanes.**

[MISSING_PAGE_FAIL:7]

**Candidate preconditioner.** In the box example, we selected the candidate preconditioner by backtracking from the largest preconditioner in the box. With an ellipsoid, there is no _largest_ preconditioner. We need to choose where to backtrack from. To ensure the candidate preconditioner \(\) is competitive (Definition 3.1), we backtrack from the preconditioner that maximizes the progress \(\| f()\|_{}^{2}\),

\[()}{}\| f( )\|_{}^{2}=^{-1} f()^{2}}{\| f ()^{2}\|_{^{-1}}}, f()^{2}  f() f().\] (8)

This lets us pick the preconditioner that makes the most progress _for the current gradient_, and will let us improve the competitive ratio by allowing a backtracking coefficient of \(}{{}}\) instead of \(}{{d}}\).

**Cutting.** To complete the algorithm, we need to find a new set \((_{t+1})\) with smaller volume which contains the intersection of the previous set \((_{t})\) and the half-space \(_{}(_{t})\). Unlike the box approach, the minimum volume ellipsoid has no closed form solution. However, if we backtrack sufficiently, by a factor of \(<}{{}}\), we can find an ellipsoid guaranteed to decrease the volume.

**Lemma 5.2**.: _Consider the ellipsoid \(()\) defined by \(=()\) for \(_{ 0}^{d}\). Let \(()\) be a point sufficiently deep inside the ellipsoid, such that \(\|\|_{}}{{}}\), and \(_{>}()\) be a half-space obtained from Proposition 4.3 at \(\). The intersection \(()()_{}\) is contained in the new ellipsoid_

\[(^{+}(,)), ^{+}(,)=+(1-)^{2},=,=\|\|_{ ^{-1}}^{2},\] (9)

_which has a smaller volume, \(((^{+}(,))) c\, (())\), where \(c=}}{{}} 0.91\)._

Proof idea.: The new ellipsoid in (9) is a convex combination between \(()\) and the minimum volume ellipsoid containing the set \(\{\,^{d}:,|| 1\}\) where \(||\) is the element-wise absolute value of \(\). The choice of \(\) in (9) is not optimal, but suffices to guarantee progress as long as \(\|\|_{}\) is small. A similar approach was used by Goemans et al. (2009) to approximate submodular functions, although they consider the polar problem of finding a maximum-volume enclosed ellipsoid. The full proof and discussion on the connections to the polar problem are deferred to Appendix D. 

To improve the cuts, we can refine the estimate of \(\) in Lemma 5.2 by minimizing the volume numerically. We include this modification, detailed in Appendix D, in our experiments in Section 6.

**Overall guarantees.** We can now define the two subroutines for the ellipsoid method, and obtain the main result that we stated informally in Theorem 1.1, by combining the guarantees of the ellipsoid approach with the convergence result of Proposition 3.2.

**Theorem 5.3**.: _Consider the multidimensional backtracking from Figure 2 initialized with the set \(_{0}=\{\,():( _{0})\}\) containing \(_{*}\), given by some scaling \(_{0}>0\) of the uniform vector, \(_{0}=_{0}\). For \(_{t}\), let \(_{t}=(_{t})\). Define the subroutines_

\[_{t}=(_{t},,_{t}) _{t}^{-1} f(_{t})^{2}}{\|  f(_{t})^{2}\|_{_{t}^{-1}}},\ (_{t},_{t}) \{\,():(^{+} (_{t},_{t}))\},\]

_where \(_{t}\) is the vector given by Proposition 4.3 when \(_{t}\) fails the Armijo condition at \(_{t}\), and \(^{+}\) is computed as in (9). If \(=}{{}}\), then: (a) \(_{*}_{t}\) for all \(t\), (b) the candidate preconditioners \(_{t}\) are \(}{{}}\)-competitive in \(_{t}\), and (c) \(\) is called no more than \(12d(L/_{0})\) times._

## 6 Experiments

To illustrate that multidimensional backtracking finds good preconditioners and improves over gradient descent on ill-conditioned problems even when accounting for the cost of backtracking, we run experiments on small but very ill-conditioned and large (\(d 10^{6}\)) problems.

As examples of adaptive gain and hypergradient methods, we include RPROP (Riedmiller and Braun, 1993) and GD with a hypergradient-tuned step-size (GD-HD, Baydin et al. 2018, multiplicative update). As examples of approximate second-order methods, we include diagonal BB (Park et al., 2020) and preconditioned GD using the diagonal of the Hessian. We use default parameters, except for the hypergradient method GD-HD, where we use \(10^{-10}\) as the initial step-size instead of \(10^{-3}\) to avoid immediate divergence. We include AdaGrad (diagonal), but augment it with a line-search as suggested by Vaswani et al. (2020), to make it competitive in the deterministic setting.

**Line-searches and forward steps.** For all methods that use a line-search, we include a _forward_ step, a common heuristic in line-search procedures to allow for larger step-sizes when possible, although itcan increase the number of backtracking steps. When a step-size or preconditioner is accepted, we increase the size of the set, allowing for larger (scalar or per-coordinate) step-sizes by a factor of \(1.1\).

**Performance comparison.** To capture the overhead of backtracking and provide a fair evaluation, Figures 1 and 5 compare performance per function and gradient evaluations (see Appendix E.1).

**On a small but extremely ill-conditioned problems**, our method is the only one that gets remotely close to being competitive with preconditioning with the diagonal Hessian--while only using first-order information. The diagonal Hessian is very close to the optimal preconditioner for those problems. On the cpusmall dataset, it reduces the condition number from \( 5 10^{13}\) to \( 300\), while \(_{*} 150\). All other methods struggle to make progress and stall before a reasonable solution is achieved, indicating they are not competitive with the optimal preconditioner.

**On large regularized logistic regression on News20 (\(d 10^{6}\)), gradient descent performs relatively better, suggesting the problem is less ill-conditioned to begin with (the regularized data matrix has condition number \( 10^{4}\)). Despite the bound of \(O(d)\) backtracking steps, our methods finds a reasonable preconditioner within 100 gradient evaluations. Despite the high dimensionality, it improves over gradient descent when measured in number of oracle calls.**

Using plain gradient updates on the hyperparameters in GD-HD leads to unstable behavior, but diagonal BB and even RPROP, perform remarkably well on some problems--even outperforming preconditioning with the diagonal Hessian, which uses second-order information. However, they fail on other ill-conditioned problems, even when a good diagonal preconditioner exists. This pattern holds across other problems, as shown in Appendix E. Multidimensional backtracking demonstrates robust performance across problems, a clear advantage of having worst-case guarantees.

## 7 Conclusion

We designed _multidimensional backtracking_, an efficient algorithm to automatically find diagonal preconditioners that are competitive with the optimal diagonal preconditioner. Our work provides a definition of adaptive step-sizes that is complementary to the online learning definition. While online learning focuses on the adversarial or highly stochastic setting, we define and show how to find optimal per-coordinate step-sizes in the deterministic smooth convex setting. We show it is possible to build provably robust methods to tune a preconditioner using hypergradients. While our specific implementation uses cutting-planes, the general approach may lead to alternative algorithms, that possibly tune other hyperparameters, with similar guarantees.

The main limitation of our approach is its reliance on the convex deterministic setting. The results might transfer to the stochastic overparametrized regime using the approach of Vaswani et al. (2019), but the non-convex case seems challenging. It is not clear how to get reliable information from a cutting-plane perspective using hypergradients without convexity. As the first method to provably find competitive preconditioners, there are likely modifications that lead to practical improvements while preserving the theoretical guarantees. Possible ideas to improve practical performances include better ways to perform forward steps, using hypergradient information from accepted steps (which are currently ignored), or considering alternative structures to diagonal preconditioners.

Figure 5: **Multidimensional backtracking finds a good preconditioner, when there is one. Experiments on regularized linear and logistic regression on small but ill-conditioned datasets, cpusmall and breast-cancer (left, middle), and the large dataset News\(20\) (right, \(d 10^{6}\)). Methods used: Gradient Descent (GD) Multidimensional Backtracking (MB) with ellipsoids, diagonal Hessian, diagonal BB, and diagonal AdaGrad—all of which use a line-search (+LS)— RPROP, and GD with hypergradient-tuned step-size (GD-HD) using the multiplicative update. Details in Appendix E.**

[MISSING_PAGE_FAIL:10]

John C. Duchi, Elad Hazan, and Yoram Singer (2011). "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization". In: _Journal of Machine Learning Research_ 12, pp. 2121-2159.
* Flake and Lawrence (2002) Gary William Flake and Steve Lawrence (2002). "Efficient SVM Regression Training with SMO". In: _Mach. Learn._ 46.1-3, pp. 271-290.
* Goemans et al. (2009) Michel X. Goemans, Nicholas J. A. Harvey, Satoru Iwata, and Vahab Mirrokni (2009). "Approximating submodular functions everywhere". In: _ACM-SIAM SODA 2009_, pp. 535-544.
* Grippo et al. (1986) Luigi Grippo, Francesco Lampariello, and Stephano Lucidi (1986). "A Nonmonotone Line Search Technique for Newton's Method". In: _SIAM Journal on Numerical Analysis_ 23.4, pp. 707-716.
* Hinton et al. (2012) Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky (2012). _Neural Networks for Machine Learning_. lecture 6.
* Jacobs (1988) Robert A. Jacobs (1988). "Increased rates of convergence through learning rate adaptation". In: _Neural Networks_ 1.4, pp. 295-307.
* Jahani et al. (2022) Majid Jahani, Sergey Rusakov, Zheng Shi, Peter Richtarik, Michael W. Mahoney, and Martin Takac (2022). "Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information". In: _ICLR_.
* Jambulapati et al. (2020) Arun Jambulapati, Jerry Li, Christopher Musco, Aaron Sidford, and Kevin Tian (2020). _Fast and Near-Optimal Diagonal Preconditioning_. arXiv/2008.01722.
* Karimi et al. (2016) Hamed Karimi, Julie Nutini, and Mark Schmidt (2016). "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Lojasiewicz Condition". In: _ECML_, pp. 795-811.
* Keerthi and DeCoste (2005) S. Sathiya Keerthi and Dennis DeCoste (2005). "A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs". In: _Journal of Machine Learning Research_ 6.12, pp. 341-361.
* Kelley Pace and Barry (1997) R. Kelley Pace and Ronald Barry (1997). "Sparse spatial autoregressions". In: _Statistics & Probability Letters_ 33.3, pp. 291-297.
* Kesten (1958) Harry Kesten (1958). "Accelerated Stochastic Approximation". In: _The Annals of Mathematical Statistics_ 29.1, pp. 41-59.
* Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba (2015). "Adam: A Method for Stochastic Optimization". In: _ICLR_.
* Klerk et al. (2017) Etienne de Klerk, Francois Glineur, and Adrien B. Taylor (2017). "On the worst-case complexity of the gradient method with exact line search for smooth strongly convex functions". In: _Optimization Letters_ 11.7, pp. 1185-1199.
* Second Edition_. Vol. 7700, pp. 9-48.
* Lewis et al. (2004) David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li (2004). "RCV1: A New Benchmark Collection for Text Categorization Research". In: _Journal of Machine Learning Research_ 5, pp. 361-397.
* Li and Orabona (2019) Xiaoyu Li and Francesco Orabona (2019). "On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes". In: _AISTATS 2019_, pp. 983-992.
* Liu and Nocedal (1989) Dong C. Liu and Jorge Nocedal (1989). "On the limited memory BFGS method for large scale optimization". In: _Mathematical programming_ 45.1-3, pp. 503-528.
* Lojasiewicz (1963) S. Lojasiewicz (1963). "Une propriete topologique des sous-ensembles analytiques reels". In: _Les Equations aux Derivees Partielles_, pp. 87-89.
* Maclaurin et al. (2015) Dougal Maclaurin, David Duvenaud, and Ryan P. Adams (2015). "Gradient-based Hyperparameter Optimization through Reversible Learning". In: _ICML_, pp. 2113-2122.
* Mahmood et al. (2012) Ashique Rupam Mahmood, Richard S. Sutton, Thomas Degris, and Patrick M. Pilarski (2012). "Tuning-free step-size adaptation". In: _ICASSP_, pp. 2121-2124.
* Masse and Ollivier (2015) Pierre-Yves Masse and Yann Ollivier (2015). _Speed learning on the fly_. arXiv/1511.02540.
* McMahan and Streeter (2010) H. Brendan McMahan and Matthew J. Streeter (2010). "Adaptive Bound Optimization for Online Convex Optimization". In: _COLT_, pp. 244-256.
* McMahan et al. (2015)Si Yi Meng, Sharan Vaswani, Issam Hadj Laradji, Mark Schmidt, and Simon Lacoste-Julien (2020). "Fast and Furious Convergence: Stochastic Second Order Methods under Interpolation". In: _AISTATS_, pp. 1375-1386.
* More and Thuente (1994) Jorge J. More and David J. Thuente (1994). "Line search algorithms with guaranteed sufficient decrease". In: _ACM Transactions on Mathematical Software (TOMS)_ 20.3, pp. 286-307.
* Moskovitz et al. (2019) Ted Moskovitz, Rui Wang, Janice Lan, Sanyam Kapoor, Thomas Miconi, Jason Yosinski, and Aditya Rawal (2019). _First-Order Preconditioning via Hypergradient Descent_. arXiv/1910.08461.
* Nesterov and Polyak (2006) Yurii E. Nesterov and Boris T. Polyak (2006). "Cubic regularization of Newton method and its global performance". In: _Mathematical Programming_ 108.1, pp. 177-205.
* Nocedal and Wright (1999) Jorge Nocedal and Stephen J. Wright (1999). _Numerical Optimization_. Springer.
* Orabona and Pal (2016) Francesco Orabona and David Pal (2016). "Coin Betting and Parameter-Free Online Learning". In: _NeurIPS_, pp. 577-585.
* Orabona and Tommasi (2017) Francesco Orabona and Tatiana Tommasi (2017). "Training Deep Networks without Learning Rates Through Coin Betting". In: _NeurIPS_, pp. 2160-2170.
* Park et al. (2020) Youngsuk Park, Sauptik Dhar, Stephen P. Boyd, and Mohak Shah (2020). "Variable Metric Proximal Gradient Method with Diagonal Barzilai-Borwein Stepsize". In: _ICASSP_, pp. 3597-3601.
* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala (2019). "PyTorch: An Imperative Style, High-Performance Deep Learning Library". In: _NeurIPS_, pp. 8024-8035.
* Pedregosa et al. (2011) F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay (2011). "Scikit-learn: Machine Learning in Python". In: _Journal of Machine Learning Research_ 12, pp. 2825-2830.
* Plagianakos et al. (2001) Vassilis P. Plagianakos, George D. Magoulas, and Michael N. Vrahatis (2001). "Learning rate adaptation in stochastic gradient descent". In: _Advances in Convex Analysis and Global Optimization: Honoring the Memory of C. Caratheodory (1873-1950)_, pp. 433-444.
* Polyak (1963) Boris T. Polyak (1963). "Gradient methods for minimizing functionals". In: _Z. Vycisl. Mat i Mat. Fiz._ 3, pp. 643-653.
* Qu et al. (2022) Zhaonan Qu, Wenzhi Gao, Oliver Hinder, Yinyu Ye, and Zhengyuan Zhou (2022). _Optimal Diagonal Preconditioning: Theory and Practice_. arXiv/2209.00809.
* Qu et al. (2016) Zheng Qu, Peter Richtarik, Martin Takac, and Olivier Fercoq (2016). "SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization". In: _ICML_, pp. 1823-1832.
* Riedmiller and Braun (1993) Martin A. Riedmiller and Heinrich Braun (1993). "A direct adaptive method for faster backpropagation learning: the RPROP algorithm". In: _ICNN_, pp. 586-591.
* Safaryan et al. (2021) Mher Safaryan, Filip Hanzely, and Peter Richtarik (2021). "Smoothness Matrices Beat Smoothness Constants: Better Communication Compression Techniques for Distributed Optimization". In: _NeurIPS_, pp. 25688-25702.
* Saridis (1970) George N. Saridis (1970). "Learning Applied to Successive Approximation Algorithms". In: _IEEE Transactions on Systems Science and Cybernetics_ 6.2, pp. 97-103.
* Schraudolph (1999) Nicol N. Schraudolph (1999). "Local gain adaptation in stochastic gradient descent". In: _ICANN_, 569-574 vol.2.
* Schraudolph et al. (2005) Nicol N. Schraudolph, Douglas Aberdeen, and Jin Yu (2005). "Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation". In: _NeurIPS_, pp. 1185-1192.
* Shor (1977) Naum Z. Shor (1977). "Cut-off method with space extension in convex programming problems". In: _Cybernetics_ 13.1, pp. 94-96.
* Silva and Almeida (1990) Fernando M. Silva and Luis B. Almeida (1990). "Acceleration techniques for the backpropagation algorithm". In: _Neural Networks_, pp. 110-119.
* Sukhukhovsky et al. (2019)Richard S. Sutton (1992a). "Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta". In: _Proceedings of the 10th National Conference on Artificial Intelligence, San Jose, CA, USA, July 12-16, 1992_, pp. 171-176.
* Sutton (1992b) Richard S. Sutton (1992b). "Gain adaptation beats least squares". In: _Proceedings of the 7th Yale workshop on adaptive and learning systems_, p. 166.
* Tufekci (2014) Pinar Tufekci (2014). "Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods". In: _International Journal of Electrical Power & Energy Systems_ 60, pp. 126-140.
* Vaswani et al. (2020) Sharan Vaswani, Frederik Kunstner, Issam H. Laradji, Si Yi Meng, Mark Schmidt, and Simon Lacoste-Julien (2020). _Adaptive Gradient Methods Converge Faster with Over-Parameterization (and you can do a line-search)_. arXiv/2006.06835.
* Vaswani et al. (2019) Sharan Vaswani, Aaron Mishkin, Issam H. Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-Julien (2019). "Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates". In: _NeurIPS_, pp. 3727-3740.
* Virtanen et al. (2020) Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors (2020). "SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python". In: _Nature Methods_ 17, pp. 261-272.
* Ward et al. (2019) Rachel Ward, Xiaoxia Wu, and Leon Bottou (2019). "AdaGrad stepsizes: sharp convergence over nonconvex landscapes". In: _ICML_, pp. 6677-6686.
* Wolfe (1969) Philip Wolfe (1969). "Convergence conditions for ascent methods". In: _SIAM review_ 11.2, pp. 226-235.
* Yao et al. (2021) Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney (2021). "AdaHessian: An Adaptive Second Order Optimizer for Machine Learning". In: _AAAI_ 35.12, pp. 10665-10673.
* Yeh (1998) I.-Cheng Yeh (1998). "Modeling of strength of high-performance concrete using artificial neural networks". In: _Cement and Concrete Research_ 28.12, pp. 1797-1808.
* Yudin and Nemirovski (1976) David B. Yudin and Arkadi S. Nemirovski (1976). "Informational complexity and effective methods for the solution of convex extremal problems". In: _Ekonom. i Mat. Metody_ 12.2, pp. 357-369.
* Zhu et al. (1997) Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge Nocedal (1997). "Algorithm 778: L-BFGS-B: Fortran Subroutines for Large-Scale Bound-Constrained Optimization". In: _ACM Transactions on Mathematical Software_ 23.4, pp. 550-560.
* Zhu et al. (1999) M. Zhu, John L. Nazareth, and Henry Wolkowicz (1999). "The Quasi-Cauchy Relation and Diagonal Updating". In: _SIAM Journal on Optimization_ 9.4, pp. 1192-1204.

**Supplementary Material**

###### Contents

* A Full pseudocode of the algorithms
* A.1 Subroutines for standard backtracking line-search
* A.2 Separating hyperplanes used by multidimensional backtracking
* A.3 Multidimensional backtracking using boxes
* A.4 Multidimensional backtracking using ellipsoids
* A.5 Implementable pseudocode
* B Optimal preconditioners, valid preconditioners and competitive ratios
* B.1 Defining optimal preconditioners without twice-differentiability or strong-convexity
* B.2 Valid and optimal preconditioners with singular matrices
* B.3 Best competitive ratio achievable by the optimal preconditioner
* C Separating hyperplanes
* C.1 Stronger hyperplanes
* C.2 Separating hyperplanes for invalid preconditioners
* D Cutting-plane methods
* D.1 Boxes
* D.2 Axis-aligned ellipsoids
* E Experiments
* E.1 Performance comparison
* E.2 Additional results

Code available at https://github.com/fKunstner/multidimensional-backtrackingFull pseudocode of the algorithms

We first give with a generic version using the subroutines initialize, candidate and cut, to be specialized for the backtracking line-search (Figure 8), multidimensional backtracking using boxes (Figure 10), and ellipsoids (Figure 11). The generic pseudocode is written in terms of preconditioners, but also applies to the step-size version, which we can consider as looking for a preconditioner constrained to isotropic diagonal preconditioners, that is, preconditioners in the set \(\{:_{ 0}\}\).

Although we write the pseudocode maintaining at each iteration an abstract set of preconditioners \(\), the only information the algorithm needs to maintain on each iteration for the implementation in the different cases is

* **For the line-search:** the current maximum step-size \(_{}\) defining the interval of valid step-sizes, \([0,_{}]\) such that the set of preconditioners is \(=\{:[0,_{}]\}\);
* **For multidimensional backtracking with boxes:** the vector \(\) defining the maximum corner of the box \(()=\{_{ 0}^{d}: \}\) used to define the candidate diagonals preconditioners in the set \(=\{():()\}\);
* **For multidimensional backtracking with ellipsoids:** the vector \(\) defining the axis-aligned ellipsoid \(()=\{_{ 0}^{d}: ,() 1\}\) used to define the candidate diagonal preconditioners in the set \(=\{():()\}\).

The pseudocode in Figure 6 updates \((_{t},_{t})\) to \((_{t+1},_{t+1})\) at each iteration, and ensures that either the function value decreases, \(f(_{t+1})<f(_{t})\), or the volume decreases, \((_{t+1})<(_{t})\).

We give an alternative pseudocode in Figure 7, which defines iterations as updates to the iterates \(_{t}\) that decrease the function value, and uses a while-loop to backtrack. Since it more closely resemble standard ways backtracking line-search is described, some reader may find it easier to understand. We stress, however, that this is still the same algorithm as Figure 6 but written differently.

The pseudocode in Figures 6-11, are expressed in a modular form to highlight how the algorithm works and its similarity to a line-search. In Appendix A.5, we give a more directly implementable pseudocode of multidimensional backtracking in both box and ellipsoid variants solely relying on vector notation.

**Backtracking Preconditioner Search with Sets**

**Input:**

A starting point \(_{0}^{d}\);

A backtracking coefficient \(\);

A scalar \(c_{0}>0\) larger than the optimal preconditioner, i.e., such that \(_{*} c_{0}\).

\(_{0}=(c_{0})\)

Iterate for \(t\) in \(0,1,...,T-1\)

\(_{t}=(_{t},, f(_ {t}))\)

If \(f(_{t}-_{t} f(_{t})) f(_{t})- \| f(_{t})\|_{_{t}}^{2}\)//Armijo condition Equation (4)

\((_{t+1},_{t+1})=(_{t}-_{t} f( _{t}),_{t})\)

Otherwise,

\((_{t+1},_{t+1})=(_{t},(_ {t},_{t},_{t}))\)

**Output:**\(_{T}\)

**Input:**

A starting point \(_{0}^{d}\);

A backtracking coefficient \(\);

A scalar \(c_{0}>0\) larger than the best preconditioner, that is, \(_{*} c_{0}\).

Initialize the set \(=(c_{0})\)

Iterate for \(t\) in \(0,1,...,T-1\)

\(_{t}(_{b},, f( _{t}))\)

While \(f(_{t}-_{t} f(_{t})) f(_{t} )-\| f(_{t})\|_{_{t}}^{2}\)//Armijo condition Equation (4)

\((,_{t},_{t})\)

\(_{t}(,, f( _{t}))\)

\(_{t+1}=_{t}-_{t} f(_{t})\)

**Output:**\(_{T}\)

**Input:**

A starting point \(_{0}^{d}\);

A backtracking coefficient \(\);

A scalar \(c_{0}>0\) larger than the best preconditioner, that is, \(_{*} c_{0}\).

Initialize the set \(=(c_{0})\)

Iterate for \(t\) in \(0,1,...,T-1\)

\(_{t}(_{b},, f( _{t}))\)

While \(f(_{t}-_{t} f(_{t})) f(_{t} )-\| f(_{t})\|_{_{t}}^{2}\)//Armijo condition Equation (4)

\((,_{t},_{t})\)

\(_{t}(,, f( _{t}))\)

\(_{t+1}=_{t}-_{t} f(_{t})\)

**Output:**\(_{T}\)

**Output:**\(_{T}\)

### Subroutines for standard backtracking line-search

Implementation of the subroutines for the standard backtracking line-search. Although written in terms of sets, the algorithm only needs to maintain the maximum step-size in the interval \([0,_{}]\) at each iteration. The corresponding preconditioners are the matrices \(=\{:[0,_{}]\}\).

### Separating hyperplanes used by multidimensional backtracking

Both versions of multidimensional backtracking need a direction to update the set of preconditioners in the cut subroutine. We define the subroutine SeparatingHyperplane in Figure 9. The description of the separating hyperplane and their properties can be found in Section 4 and Appendix C.

### Separating hyperplanes used by both variants of multidimensional backtracking.

Figure 8: **Specialization of the subroutines for the backtracking line-search**

Figure 9: **Separating hyperplane used by both variants of multidimensional backtracking.**

### Multidimensional backtracking using boxes

The implementation of multidimensional backtracking with boxes only needs to maintain a vector \(\), representing the maximum step-size for each coordinate that has not been ruled out, in the box \(()\). The associated sets of preconditioners \(\) are

\[()=\{_{ 0}^{d}: \}, =\{():( )\}.\]

The description of boxes and the theoretical guarantees when using them in multidimensional backtracking can be found in Section 5 and Appendix D.1. The subroutines used by the algorithm with boxes are:

* initialize: initializes \(\) to \(c_{0}\) so that the diagonal preconditioner \(c_{0}\) is in \(_{0}\).
* candidate: backtracks from the largest diagonal in \(()\), returning \(()\).
* SeparatingHyperplane: computes the vector \(\) defining the half-space of invalid preconditioners \(_{>}()\) obtained when the preconditioner \(\) fails the Armijo condition at \(\) as described in Proposition 4.2 and Proposition 4.3.
* cut: returns the minimum volume box \((^{+})\) containing the intersection \(()_{}()\).

Figure 10: **Specialization of the subroutines for multidimensional backtracking with boxes**

### Multidimensional backtracking using ellipsoids

The implementation only needs to maintain a vector \(\) representing the diagonal of the matrix defining the (centered, axis-alligned) ellipsoid \(()\) and the associated set of preconditioners \(\) given by

\[()=\{_{ 0}^{d}: ,() 1\}, =\{():( )\}.\]

The description of the ellipsoids and their properties can be found in Section 5 and Appendix D.2. The subroutines used by the algorithm with boxes are:

* initialize: initializes \(\) to \((}{{d_{0}^{2}}})\) so that \(c_{0}()\), implying the diagonal preconditioner \(c_{0}\) is in \(\).
* candidate: backtracks from the diagonal preconditioner in \(\) that maximizes the gradient norm. Let \(()\) be the set of candidate diagonals and define \(=()\). The subroutine returns \(_{}\), where \[_{}*{arg\,max}_{}  f()_{}^{2}.\] Writing this in terms of the diagonal vector \(_{}(_{})\) yields \[_{} =*{arg\,max}_{()}  f()_{()}^{2},\] \[=*{arg\,max}_{} f()^{2}, :_{} 1=^{-1}  f()^{2}}{ f()_{^{-1}}},\] where \( f()^{2}= f() f()\).
* SeparatingHyperplane: computes the vector \(\) defining the half-space of invalid preconditioners \(_{>}()\) obtained when the preconditioner \(\) fails the Armijo condition at \(\) as described in Proposition 4.2 and Proposition 4.3.
* cut: returns an ellipsoid \((^{+})\) containing the intersection of \(()_{<}()\) with guaranteed volume decrease from \(()\). As there is no closed-form solution for the minimum volume ellipsoid, we set \(^{+}\) as a convex combination between the original ellipsoid \(()\) and the minimum volume axis-aligned ellipsoid containing \(_{}()\), given by \((^{2})\), that is, \[^{+}+(1-)^{2}, { where }_{^{-1}}^{2},\] (10) where \(()\). Although the above choice of \(\) has guaranteed volume decrease, we can find a better value of \(\) by solving the minimum volume ellipsoid as a function of \(\) numerically. Namely, approximating \[^{*}*{arg\,min}_{0<<1}-(( ()+(1-)(^{2} ))).^{}\] In our experiments, we start with \(\) as in (10) and, starting from it, we solve the above minimization problem numerically using L-BFGS-B (Zhu et al., 1997) in SciPy (Virtanen et al., 2020). This preserves the theoretical guarantee while improving empirical performance.

  \((c_{0})\) \\ 
**Input:** \\ >0\) such that \(c_{0}\) is larger than the optimal diagonal preconditioner, i.e., \(_{*} c_{0}\). \\
**Output:**\(=\{():()\}\) with \(()=\{_{ 0}^{d}:, () 1\}\) for \(=^{2}}\) \\   \((,,)\) \\ 
**Input:** \\ =\{():()\}\) where \(()=\{_{ 0}^{d}:, () 1\}\), and \(_{>0}^{d}\); \\  \\ ^{d}\).} \\  \((,,)\) \\ 
**Input:** \\ =\{():()\}\), where \(()=\{_{ 0}^{d}:, () 1\}\), and \(_{>0}^{d}\); \\ ^{d}\);} \\ _{}\) that failed the Armijo condition at \(\).} \\ =(_{t},_{})\)} \\ \|_{^{-1}}^{2}\)} \\ (=_{0<c<1}-((c\,()+(1-c)\, (^{2}))))\) \\ ^{+}=+(1-)^{2}\)} \\ (^{+})\) containing \(()_{}()\)} \\ =\{():(^{+})\}\)} \\  

Figure 11: **Specialization of the subroutines for multidimensional backtracking with ellipsoids**

### Implementable pseudocode

The pseudocode in Figures 6-11 are expressed in a modular form to highlight how the algorithm works and its similarity to a line-search. In this section, we give a more directly implementable pseudocode of multidimensional backtracking, in both the box and ellipsoid variants, using mostly vector notation. Scalar operations on vectors such as \(/\), \(}\), \(^{2}\) are understood to be taken element-wise.

``` Input:  Function to optimize \(f^{d}\) ;  Starting point \(_{0}^{d}\);  A scalar for the scale of initial set of preconditioners \(c_{0}>0\);  Backtracking coefficient \(<}{{d}}\). \(=c_{0}\)//Initialize box Iterate for \(t\) in \(0,1,...\) \(_{t}=\)//Get candidate preconditioner \(_{t}= f(_{t})\)//Get candidate point \(_{t}^{+}=_{t}-_{t}_{t}\)  While \(f(_{t}^{+})>f(_{t})-_{t }^{2},_{t}\)//Armijo condition fails \(_{t}^{+}= f(_{t}^{+})\)//Get next gradient to compute \(_{t}=(_{t}-_{t}^{+})_{t}\)//the separating hyperplane direction, \(c_{t}=f(_{t})-f(_{t}^{+})-_{t} _{t},_{t}^{+}\)//the normalization constant, \(_{t}=\{_{t}/c_{t},0\}\) (element-wise)//and truncate it \(=/\{/,\,_{t}\}\) (element-wise)//Find new minimum volume box. //(\(\)-free \(\{,/\}\)) \(_{t}=\)//Pick next candidate preconditioner \(_{t}^{+}=_{t}-_{t}_{t}\)//and next candidate point \(_{t+1}=_{t}^{+}\)//Accept new point Output:\(_{t}\) ```

**Algorithm 1**Multidimensional backtracking using boxes

### Implementable pseudocode

The pseudocode in Figures 6-11 are expressed in a modular form to highlight how the algorithm works and its similarity to a line-search. In this section, we give a more directly implementable pseudocode of multidimensional backtracking, in both the box and ellipsoid variants, using mostly vector notation. Scalar operations on vectors such as \(/\), \(}\), \(^{2}\) are understood to be taken element-wise.

``` Input:  Function to optimize \(f^{d}\) ;  Starting point \(_{0}^{d}\);  A scalar for the scale of initial set of preconditioners \(c_{0}>0\);  Backtracking coefficient \(<}{{d}}\). \(=c_{0}\)//Initialize box Iterate for \(t\) in \(0,1,...\) \(_{t}=\)//Get candidate preconditioner \(_{t}= f(_{t})\)//Get candidate point \(_{t}^{+}=_{t}-_{t}_{t}\)  While \(f(_{t}^{+})>f(_{t})-_{t }^{2},_{t}\)//Armijo condition fails \(_{t}^{+}= f(_{t}^{+})\)//Get next gradient to compute \(_{t}=(_{t}-_{t}^{+})_{t}\)//the separating hyperplane direction, \(c_{t}=f(_{t})-f(_{t}^{+})-_{t} _{t},_{t}^{+}\)//the normalization constant, \(_{t}=\{_{t}/c_{t},0\}\) (element-wise)//and truncate it \(=/\{/,\,_{t}\}\) (element-wise)//Find new minimum volume box. //(\(\)-free \(\{,/\}\)) \(_{t}=\)//Pick next candidate preconditioner \(_{t}^{+}=_{t}-_{t}_{t}\)//and next candidate point \(_{t+1}=_{t}^{+}\)//Accept new point Output:\(_{t}\) ```

**Algorithm 2**Multidimensional backtracking using boxes

**Multidimensional backtracking using ellipsoids**

**Direct implementation**

**Input:**

Function to optimize \(f^{d}\) ;

Starting point \(_{0}^{d}\);

A scalar for the scale of initial set of preconditioners \(c_{0}>0\);

Backtracking coefficient \(<}{{}}\)

\(=/(dc_{0}^{2})\)//Initialize ellipsoid

Iterate for \(t\) in \(0,1,...\)

\(_{t}= f(_{t})\)

\(_{t}=_{t}^{2}/\) (element-wise) //Get candidate preconditioner

\(_{t}=_{t}/_{t}^{2}/}\) (element-wise) //normalize it

\(_{t}^{+}=_{t}-_{t}_{t}\)//Get candidate point

While \(f(_{t}^{+})>f(_{t})-_{t} ^{2},_{t}\)//Armijo condition fails

\(_{t}^{+}= f(_{t}^{+})\)//Get next gradient to compute

\(_{t}=(_{t}-_{t}^{+})_ {t}\)//the separating hyperplane direction,

\(c_{t}=f(_{t})-f(_{t}^{+})-_{t} _{t},_{t}^{+}\)//the normalization constant,

\(_{t}=\{_{t}/c_{t},0\}\) (element-wise) //and truncate it

take \(=\) where \(=^{2},1/\)//Approx. min. vol. new ellipsoid

or

find \(\) by numerically minimizing \(()\) where //Find better approximation of min.

\(()=-_{i=1}^{d}([i]+(1-)_ {t}[i]^{2})\)//of volume of new ellipsoid

\(=+(1-)^{2}\)//New ellipsoid

\(_{t}=_{t}^{2}/\) (element-wise) //Get new candidate preconditioner,

\(_{t}=_{t}/_{t}^{2}/}\) (element-wise) //normalized,

\(_{t}^{+}=_{t}-_{t}_{t}\)//and new candidate point

\(_{t+1}=_{t}^{+}\)//Accept new point

**Output:**\(_{t}\)

## Appendix B Optimal preconditioners, valid preconditioners and competitive ratios

In Section 2, we defined the optimal preconditioner \(_{*}\) as the preconditioner that is the best overall approximation to the inverse Hessian. Formally, we define the optimal diagonal preconditioner \(_{*}\) as

\[_{*}*{arg\,min}_{^{-} 0,}^{-1} ^{2}f()^{-1}.\] (1)

One way to interpret this definition is that \(_{*}^{-1}\) is the tightest diagonal approximation to \(^{2}f()\).

We remark that we do not need \(f\) to be (strongly-)convex to define the theoretically optimal step-size of \(}{{L}}\) for gradient descent. Thus, one may wonder why we need strong-convexity (although we relax this to requiring \(f\) to be PL in Appendix B.1) to define what an optimal preconditioner is in (1).

The main difference between the scalar step-size and per-coordinate step-sizes settings is whether the "largest" step-size or preconditioner is well-defined. In the scalar setting, the largest step-size that is guaranteed to lead to progress everywhere (i.e., a step-size that satisfies the Armijo condition (3) for all \(\)) is well-defined and equal to \(_{*}}{{L}}\) for \(L\)-smooth function \(f\). Equivalently,

\[_{*}=>0:^{2}f() }=_{^{d}} _{}(^{2}f()),\]

where \(_{}(^{2}f())\) is the largest eigenvalue of \(^{2}f()\). But in the case of preconditioners, the ordering on positive definite matrices is not complete, so there is no single "largest" preconditioner \(\) that satisfies \(^{2}f()^{-1}\). We can still describe "good" preconditioners, that are guaranteed to satisfy the Armijo condition (Equation (4)) everywhere; this is the notion of valid preconditioners defined in Definition 4.1, which in set notation is \(\{ 0:^{2}f() ^{-1}\}\). With this definition, we can consider the set of valid preconditioners \(\) for which there are no bigger valid preconditioners, that is, \(\{:\,^{} ^{}\}\). However, \(\) contains incomparable preconditioners, that is, distinct matrices \(,\) that neither \(\) nor \(\) hold.

Let us look at an example with a quadratic function (illustrated in Figure 12)

\[f()=, =.5&.1\\.1&1.0.\] (11)

There are many preconditioners that are valid,3 for example using the per-coordinate step-sizes

\[_{L}.91&0\\ 0&.91,_{1}=2.0&0\\ 0&0.0,_{2}=0.0&0\\ 0&1.0,_{*}1.75&0\\ 0&0.87.\]

The preconditioner \(_{L}\) corresponds to the \(}{{L}}\) step-size, \(_{1}\) and \(_{2}\) take the largest possible step-size in each coordinate, and \(_{*}\) is the optimal preconditioner according to Equation (1). Those preconditioners are not comparable to each other, as neither \(_{L}_{*}\) nor \(_{*}_{L}\) hold. Instead of looking at the matrices themselves, we use in (1) the condition number4 of \(^{1/2}^{2}f()^{1/2}\) as a measure of quality of \(\). This allows for a well-defined optimal preconditioner as this condition number can be maximized.

Figure 12: Set of valid diagonal preconditioners (step-sizes \(_{1}\) and \(_{2}\)) for the quadratic in Equation (11). Preconditioned gradient descent can use a larger step-size in the first coordinate.

### Defining optimal preconditioners without twice-differentiability or strong-convexity

Although we used twice-differentiability of \(f\) to define the optimal preconditioner, this is not necessary. If \(f\) is not twice-differentiable but still strongly-convex, the definition in Equation (1) can be replaced by Equation (2), as finding the \(\)-norm under which the function is most strongly-convex.

\[_{*}=*{arg\,min}_{ 0,}\] \[\{\|-\|_{-1}^{2} f()-f()- f(),-,\\ f()-f()- f(),- \|-\|_{-1}^{2}, .,.\]

To avoid strong-convexity, we can instead use the PL inequality. A function \(f\) is \(\)-PL if

\[\| f()\|^{2} f()-f( _{*}).\] (12)

This property is implied by \(\)-strong convexity. We refer to the work of Karimi et al. (2016) for the properties of PL functions and its relation to other assumptions. To adapt Equation (12) to our results, we can measure the PL constant \(\) in the norm induced by \(\), and say that \(f\) is \(\)-PL in \(\|\|_{}\) if

\[\| f()\|_{}^{2} f( )-f(_{*}).\] (13)

We use this inequality in the convergence proof in Proposition 3.2 since it is a consequence of strong-convexity. As this property is the only property of strong-convexity needed for our results, we can adapt our results to be competitive with the optimal preconditioner defined using the PL inequality, using the definition

\[_{*}^{}:=*{arg\,min}_{  0,}\] \[\{\|  f()\|_{}^{2} f()-f(_{*}) \\ f()-f()- f(),- \|-\|_{-1}^{2}, .,.\] (14)

If \(f\) is \(\)-PL and \(L\)-smooth, Equation (14) has a feasible solution at \(=}{{L}}\) number \(=}{{}}\). The constraint based on the \(\)-PL condition in Equation (14) is weaker than the definition using strong-convexity, as strong-convexity implies the PL inequality. The optimal preconditioner defined using the PL inequality (14) might thus achieve a lower condition number than the one using strong-convexity (1). For example, the quadratic \(f()=(}{{2}}),\) with a positive semi-definite \(\) is not strongly convex if the smallest eigenvalue of \(\) is \(0\). The optimal preconditioner in Equation (1) is ill-defined (or has condition number \(_{*}=\)). In contrast, the optimal preconditioner defined using the PL inequality in Equation (14) has a finite condition number, as \(=}{{L}}\) is a feasible solution with condition number \(=}{{_{}^{+}()}}\) where \(_{}^{+}()\) is the smallest non-zero eigenvalue of \(\). As our proofs only use the properties guaranteed by Equation (14), our results also apply to PL functions.

### Valid and optimal preconditioners with singular matrices

In the main text, we defined valid preconditioners (Definition 4.1) only for positive definite matrices for ease of presentation. The notion of valid preconditioners can be extended to general positive semidefinite matrices. In the diagonal case, the convention \(1/0=+\) is a useful mental model but can cause inconsistencies (such as \( 0\)). To extend the notion of valid preconditioners to general positive semidefinite matrices, we can use the definition

**Definition B.1**.: _A preconditioner \( 0\) is valid if \(^{1/2}^{2}f()^{1/2} I\) for all \(^{d}\)._

The above is well-defined for all positive semidefinite matrices. An alternative to arrive at a definition closer to Definition 4.1 is to consider the projection matrix \(_{}\) onto the image of \(\), given by \(_{}=^{1/2}(^{1/2})^{}\) where \(^{}\) is the Moore-Penrose pseudo-inverse of \(\). Using that, one can show that \(\) is _valid_ (according to Definition B.1) if and only if

\[_{}^{2}f()_{}^{ }^{d}.\]

An example of a valid preconditioner that is covered by Definition B.1 but not 4.1 is the all-zeroes matrix. Definition B.1 can seamlessly replace 4.1, and all the results follow similarly. Moreover, notice that the optimization problem defining the optimal preconditioner (1) may not attain its minima on positive definite matrices when \(f\) is not strongly convex. In this case, we can define an optimalpreconditioner as a limit point of a sequence that attains in the limit the value in (1) by replacing the minimum with an infimum. In this case, an optimal preconditioner may be singular, but the results in the main body also follow seamlessly using this definition. We decided to restrict our attention to non-singular preconditioners in the main paper for ease of exposition, since when \(f\) is strongly-convex, an optimal preconditioner is always non-singular.

### Best competitive ratio achievable by the optimal preconditioner

In Section 3, we mentioned that the optimal preconditioner \(_{*}\) could be only \(}{{d}}\)-competitive. In fact, the competitive ratio of \(_{*}\) can be arbitrarily bad. The reason for this is that the competitive ratio \(\) does not compare against \(_{*}\), but rather against any \(\) in the set \(\) of potentially valid preconditioners. Moreover, this definition only takes into account the norm \(\| f()\|_{}\) at a _fixed_\(\), while the optimal preconditioner needs to have large norm _for all_\(\).

For example, consider the scalar step-size case. If our current interval of candidate step-sizes to try is \(=\) but the optimal step-size \(_{*}\) is small, let us say \(_{*}=}{{10}}\), then \(_{*}\) is only \(}{{10}}\)-competitive in \(\). The motivation for this definition of competitive ratio is that we cannot check whether \(\) is large compared to \(_{*}\) (as we do not know \(_{*}\)) but we can more easily ensure that a candidate step-size \(\) is \(\)-competitive in \(\) (for example \(=}{{2}}\) is \(}{{2}}\)-competitive in \(\)).

In the previous example, the bad competitive ratio of \(_{*}\) in \(\) was mostly due to the fact that \(\) was large and that, for some \(\), step sizes larger than \(_{*}\) could satisfy the Armijo condition (3). Even if \(_{*}\) is globally optimal, we could make more progress by using a larger step-size if they were to be accepted, and we have not yet ruled out those step-sizes. However, as \(\) shrinks, it may eventually converge to the interval \(\), in which case the optimal step-size \(_{*}\) would be \(1\)-competitive.

**In high dimensions however,** the optimal preconditioner can have a competitive ratio of \(}{{d}}\) even when comparing only against valid preconditioners.5 This is because the competitive ratio is defined using the \(\)-norm of the gradient, and we need to take the direction of the gradient into account. For example, consider the quadratic function (illustrated in Figure 13)

\[f()=,\] where \[=1&-1\\ -1&1,\] (15)

with eigenvalues \(\{2,0\}\) as \(=[-1,1]^{}[-1,1]\). The following three preconditioners are all valid:

\[_{1}=1&0\\ 0&0, _{2}=0&0\\ 0&1, _{*}=}{{2}}&0\\ 0&}{{2}}.\]

The preconditioner \(_{1}\) takes the largest possible step-size in the first coordinate and ignores the second, while \(_{2}\) does the opposite. They are not good global preconditioners, as each ignores one coordinate. Yet, they can make much more progress (i.e., the objective value may decrease more) than the optimal preconditioner \(_{*}\) if the gradient is very skewed towards one coordinate. This implies that

Figure 13: Set of valid diagonal preconditioners (step-sizes \(_{1}\) and \(_{2}\)) for the quadratic in Equation (15). The set of valid preconditioners (Definition 4.1) is the white region in the right figure.

\(_{*}\) may be only \(}{{2}}\)-competitive in \(\{_{1},_{2}\}\) for some \(\) since

\[ f() =1\\ 0,\| f()\|_{_{1}}^{2}=1, \| f()\|_{_{2}}^{2}=0,\| f()\|_{_{*}}^{2}=}{{2}},\] \[ f() =0\\ 1,\| f()\|_{_{1}}^{2}=0, \| f()\|_{_{2}}^{2}=1,\| f()\|_{_{*}}^{2}=}{{2}}.\]

The preconditioner \(_{*}\) is still a better choice globally (i.e, for all \(\)) since it ensures optimal worst-case linear rate in preconditioned gradient descent. But there are better preconditioners that depend on the current gradient. We exploit this in the ellipsoid variant of multidimensional backtracking to improve our competitive ratio. We backtrack from the preconditioner that maximizes the local progress guarantees to ensure a \(}{{}}\) competitive ratio, while ensuring volume shrinkage of the set of candidate preconditioners when we call cut, if the preconditioner fails the Armijo condition.

Separating hyperplanes

In this section, we prove Propositions 4.2 and 4.3 on existence and strengthening of separating hyperplanes for valid preconditioners.

**General idea.** Let us start with a summary of the separating hyperplanes used to search for good preconditioners as discussed in Sections 3 and 4. The goal of the separating hyperplanes is to give us ways to shrink the initial set of potential preconditioners \(\) to narrow in on valid preconditioners using the cutting-plane methods in Section 5. At each iteration we are looking for preconditioners \(\) that satisfy the Armijo condition at \(\) given by

\[f(- f()) f()-)\|}_{}^{2}.\]

If \(\) fails the Armijo condition, we conclude that \(\) is invalid. To obtain more information, we look at the condition as a function of the (diagonal of the) preconditioner, and define the gap function at \(\),

\[h() f(-() f( ))-f()+)\|}_{()}^{2},_{ 0}^{d}.\]

Then, \(h() 0\) if \(=()\) satisfies the Armijo condition at \(\), and \(h()>0\) otherwise. Any preconditioner \(()\) such that \(h()>0\) is guaranteed to be invalid. We can use the gradient of \(h\) at \(\) and convexity to find a half-space such that one side contains only preconditioners with \(h()>0\). In this section, we show how to construct such half-space, and strengthen them using the partial order on matrices, which is needed to ensure volume shrinkage of our cutting plane methods.

### Stronger hyperplanes

In the main body we presented the strengthening of separating hyperplanes via truncation (Proposition 4.3) after the result of existence of separating hyperplanes (Proposition 4.2). Here, we prove a more general lemma on strengthening half-spaces of invalid preconditioners first, as it is useful in simplifying the proof of Proposition 4.2. Proposition 4.3 follows directly from the following lemma.

**Lemma C.1**.: _Let \(_{,}\) be the intersection of the non-negative orthant \(_{ 0}^{d}\) and the half-space defined by the vector \(^{d}\) and coefficient \(>0\),_

\[_{,}\{\,_{ 0}^{d }:,>\}.\]

_Define \(\{,0\}\) and let \(_{,}\) be defined similarly as above, that is,_

\[_{,}\{\,_{ 0}^{d }:,>\}.\]

_If \(_{,}\) only contains diagonals of invalid preconditioners, that is, \(()\) is invalid for any \(_{}\), Then \(_{,}_{,}\) and \(_{,}\) only contains diagonals of invalid preconditioners._

Proof.: **Inclusion \(_{,}_{,}\).** We have that \(,>\) implies \(,>\) for any \(_{ 0}^{d}\) since

\[,=_{i:\;[i] 0}[i] [i]+_{i:\;[i]<0}[i][i]_{i: \;[i] 0}[i][i]=_{i:\;[i] 0} [i][i]=,.\]

\(_{,}\) **only contains invalid diagonals.** Let \(_{}_{,}\). We can show that \((_{})\) is invalid by finding \(_{}_{,}\) such that \((_{})(_{})\). Since \((_{})\) is invalid by assumption, this would imply that \((_{})\) is also invalid. To find \(_{}\), we can truncate the entries of \(_{}\) as

\[_{}[i]_{}[i]& [i] 0\\ 0&, i\{1,,d\}.\]

Then \(_{}_{,}\) since \(<,_{}=, _{}=,_{}\). 6 and \((_{})(_{})\), as desired. 

### Separating hyperplanes for invalid preconditioners

We are now in position to prove Proposition 4.2.

Proof of Proposition 4.2.: Throughout the proof, we shall denote by \(\) the matrix \(()\). If \(f\) is convex, then \(h\) also is since the map \(_{ 0}^{d} f(- f( ))\) is the composition of an affine transformation and a convex function, and \(\| f()\|_{}^{2}= f(), () f()\) is linear in \(\). Convexity of \(h\) yields the inequality

\[h() h()+ h(),- ,_{ 0}^{d}.\]

This implies that if \(\) is such that \(h()+ h(),->0\), then \(h()>0\), which implies that \(()\) is an invalid preconditioner. Rearranging we conclude that \(()\) is invalid for all \(\) in the set in (5), i.e., in

\[\{\,_{ 0}^{d}: h(), > h(),-h()\}\] (16)

We express the above half-space as

\[_{>}()=\{:,> 1\})}{( h(), -h())}.\]

Yet, for \(_{>}()\) to be equivalent to the set in (16) or even to be well-defined, we need to ensure \( h(),-h()>0\). To see that this holds, note first that by convexity of \(h\) and that fact that \(h(0)=0\) we have

\[h(0) h()+ h(),0-  h(),-0-h()-h(0)=0\]

To show that the last inequality is strict, assume that \( h(),-0-h()=0\) for the sake of contradiction. By Lemma C.1, the half-space \(\{\,_{ 0}^{d}:[  h()]_{+},>0\}\) contains only diagonals of invalid preconditioners, where \([ h()]_{+}\{ h(),0\}\) entry wise. However, \((}{{L}})\) as \([ h()]_{+} 0\) and should be invalid, which is a contradiction since \(f\) is \(L\)-smooth and \(}{{L}}\) is valid. Therefore, \( h(),-0-h()>0\).

Finally, we can write \(\) in terms of \(f\) and \(\). To do so, first define \(^{+}- f()\), and the gradients of \(f\) at different points by \( f()\) and \(^{+} f(^{+})\). Then, by the chain-rule,

\[ h()=- f(- f())  f()+ f() f( )=-^{+}+ ,\]

which implies

\[ h(),-h() =-^{+}, +,-f(^{+})+f ()+,\] \[=f()-^{+}, -f(^{+}).\]

Plugging these equations in the definition of \(\) yields

\[=)}{ h(), -h()}=-^{+}) }{f()-^{+},-f (^{+})}.\]

Remark on assumptions of Proposition 4.2.: One may have noticed that we never use the assumption that \(\) fails the Armijo condition (i.e., that \(h()>0\)) in the proof of the proposition. In fact, the proposition holds for any \(_{ 0}^{d}\). However, and crucially for our application, we have that \(\) is in the half-space \(_{>}()\) of invalid diagonals from Proposition 4.2. In multidimensional backtracking, \(\) is the diagonal of a preconditioner \(()\) that failed the Armijo condition \(h()>0\). Since \(\) is close to the origin in multidimensional backtracking, we can ensure the half-space \(_{>}()\) contains a significant portion of our current set of candidate preconditioners, leading to significant shrinkage of the set of candidate preconditioners whenever cut is invoked.

Cutting-plane methods

### Boxes

Given a box \(()\) for some \(_{ 0}^{d}\) and a vector \(_{ 0}^{d}\), our cutting plane method needs to find a box \((^{+})\) that contains \(()_{>}()\) which, hopefully, has smaller volume than \(()\).

The next lemma gives a formula for the _minimum volume_ box for any \(\), which is used in the main text to define \(\) in Equation (7). Moreover, we show that if the half-space \(_{>}()\) is close enough to the origin (since otherwise we might have \(^{+}=\)), then we have a significant volume decrease.

**Lemma D.1**.: _Let \(_{ 0}^{d}\) and \(()\). Let \(_{ 0}^{d}\). Then the box \((^{+})\) with minimum volume that contains \(()_{}()\) is given by (using the convention that \(1/[i]=+\) if \([i]=0\))_

\[^{+}[i]\{[i],}{{[i] }}\}, i\{1,,d\},\] (17)

_Moreover, if \(}{{2d}})}}{{}}\) is excluded by the half-space, that is, \(_{>}()\), then \(((^{+})) }{{d}}+1)}}{{(}{{d}}+1)}}((^{+}))\)._

Proof.: **Formula for \(^{+}\).** Finding the minimum volume box containing \(()_{}()\),

\[^{+}=*{arg\,min}_{^{d}} (())()_{}()(),\]

is equivalent to finding the solution to the following optimization problem:

\[^{+}=*{arg\,min}_{^{d}}_{ i}[i]_{() _{}()}[i][i]i\{1,,d\}.\]

As the constraints separate over the coordinates, the minimization can be done for each coordinate separately. As the function is increasing in \([i]\), the minimum is achieved by making all the constraints tight, which give the formula for \(^{+}\) in the statement of the lemma.

**Volume decrease.** Let us prove the second part of the statement. Thus, assume for the remainder of the proof that \(}{{2d}})}}{{}}_{>}( )\). We first show that \(((^{+}))}{{ d}}+1)}}{{(d+1)}}((^{+}))\) if we assume that the update from \(()\) to \((^{+})\) shrinks the box in only one coordinate, i.e.,

\[\{\,i[d]:\,[i]>}{{[i ]}}=\{\,i[d]:\,^{+}[i][i]\}.\] (18)

Assume the above holds and \(=\{j\}\). Then, as \(}{{2d}})}}{{}}_{>}( )\) implies \(,}{{2d}})}}{{}}>1\),

\[1<,}{{2d}})}}{{}} ([j][j]+d-1)(d+1)[j]}[j].\]

This together with the fact that \(^{+}[i]=[i]\) for all \(i j\) and \(^{+}[j]=}{{[j]}}\) yields

\[((^{+}))=_{i=1}^{d}^{+}[ i]=[j]}_{i j}[i] _{i=1}^{d}[i]=(( )).\]

To complete the proof, we only need to show we may assume (18) holds. Assume the opposite, that is, that there are two distinct coordinates that shrink from \(^{+}\) to \(\). We will show that the volume shrinks more, meaning the above bound also applies. Formally, assume there are \(j,k\) that are distinct. For this part, it will be useful to denote by \(^{+}()\) the point defined in Equation (17) for a given vector \(\). We will show we can construct \(^{}_{ 0}^{d}\) such that \((^{+}())( ^{+}(^{}))\) while maintaining the property \(}{{2d}})}}{{}}_{>}(^{})\) and such that \(^{+}(^{})[i][i]\) for all \(i\{j\}\), which makes (18) follow by induction. Indeed, define \(^{}_{ 0}^{d}\) by

\[^{}[i][i]i\{j,k\}, ^{}[j][j]},^{}[k][k]+[j]}{[ k]}[j]-[j]}.\] (19)

First, note that \(}{{2d}})}}{{}}_{>}(^{})\) since

\[^{}-, =[j](^{}[j]-[j])+( ^{}[k]-[k])[k]\] \[=[j][j]}-[j] +[j]}{[k]}[j]- [j]}[k]=0\]and, thus, \(1<,(}{{2d}})=^{ },(}{{2d}})\). Let us now show that \(((^{+}()))((^{+}(^{})))\). Since \(^{+}()[i]=^{+}(^{})[i]\) for \(i\{j,k\}\), we have

\[((^{+}()))}{ ((^{+}(^{})))} =^{+}()[j]}{^{+}(^ {})[j]}^{+}()[k]}{^{+}( ^{})[k]}\] \[=[j],}{{u}[j]})}{( [j],}{{u}[j]})}[k],}{{u}[k]})}{([k],}{{u}^{}[k]})}\] \[=}{{u}[j]}}{[j]}}{{u}[k]}}{}{{u}^{}[k]}}\] (since \[j,k\] and by (19)) \[=[j][j]}[k]} [k]+[j]}{[k]}[j] -[j]}\] \[=[j][j]}[k] [k]}([k][k]+[j][j]-1).\]

To get that \((^{+}())(^{+}(^{}))\), we can show that last line is bounded by \(<1\). Using the substitution \([j][j]\) and \([k][k]\), we want to show that

\[<1--+1>0( -1)(-1)>0.\]

This holds if \(>1\) and \(>1\), is implied by \(j,k\) since \(=[j][j]>1\) and \(=[k][k]>1\). A simple induction shows we may assume (18) holds. To see that \((+-1)/<1\), note that 

Equipped with the above lemma, we are in position to prove Theorem 5.1.

Proof of Theorem 5.1.: Property (a), holds by induction because, for any \(_{t}\) used in a call to cut, we have \(^{*}_{}(_{t})\) since \(^{*}\) is valid and since by Proposition 4.2 the half-space \(_{}(_{t})\) contains only diagonals of invalid preconditioners. For (b), fix \(t\{1,,T\}\) and recall that in this case we have \(_{t}=\{():( _{t})\}\) and \(_{t}=(}{{2d}})(_{t})\). The competitive ratio of \(}{{2d}}\) follows since \((_{t})\) is the preconditioner that maximizes \(\| f(_{t})\|_{}\) for \((_{t})\). Finally, for (c) by Lemma D.1 we have that every call to cut makes the volume of the set decrease by \(1/c 1/(d+1)\). Moreover, one can easily verify that \(_{t}[i]\{}{{L}},_{0}[i]\}\) for all \(i\{1,,d\}\) since \((}{{L}})1)\) contains only diagonals of valid preconditioners. Therefore, for \(_{}[i]\{}{{L}},_{0}[i]\}\), the volume of \((_{t})\) cannot be smaller than \((_{})\) for all iteration \(t\). Therefore, the number of times cut is invoked is no more than

\[_{c}((_{0}))}{ ((_{}))}=_{c}( _{i=1}^{d}_{0}[i]}{_{}[i]})_{c }((_{0}_{}L)^{d})=d_{c}( _{0}_{}L).\]

as desired. 

### Axis-aligned ellipsoids

We now analyze the cutting-plane method using axis-aligned ellipsoids. Interestingly, the results that we prove in this sections are connected to some of the results from Goemans et al. (2009) via polarity theory. We defer a discussion on this connection to the end of this section.

Different from the main body, it will be helpful for the analysis of the method and proofs of the results to not restrict ellipsoids to the non-negative orthant, as was done in the main text for ease of exposition. For any symmetric positive definite matrix \(^{d d}\), define the _ellipsoid_ given by \(\) by

\[()\{^{d} , 1\}.\]

When \(\) is diagonal, we say that \(()\) is _axis-aligned_. Moreover, we may slightly overload our notation by defining \(()(())\).

**General ellipsoids.** Although we are ultimately interested in working solely with ellipsoids defined by diagonal matrices, we will start by looking at more general ellipsoids, and then exploit symmetry in our case to derive the result in Lemma 5.2. We start with an ellipsoid \(()\) where \(\) is a positive definite matrix. Then, given a vector \(^{d}\), we are interested in finding an ellipsoid the intersection of \(()\) with the half-spaces defined by \(\) and \(-\) that contain the origin, that is, the set

\[()\{\,^{d}:, <1\}\{\,^{d}:-, <1\}=()\{\,^{ d}:|,|<1\}.\]

The following theorem shows how to find an ellipsoid that contains the above intersection, and how to guarantee its volume is smaller than \(()\) if \(\) is large enough. Interestingly, note that

\[\{\,^{d}:|,|<1\}=\{ \,^{d}:(,)^{2}<1\}= (^{}).\]

The set \((^{})\) is a degenerate ellipsoid, in the sense that it is not a compact set, and any \(\) orthogonal to \(\) is contained in \((^{})\). Still, the next theorem shows how to find a convex combination of \(()\) and \((^{})\)--which always contains \(()(^{})\)--that is guaranteed to have volume smaller than \(()\) if \(\) is large enough. The following result can be seen as the polar result of Goemans et al. (2009, Lemma 2).

**Theorem D.2**.: _Let \(^{d d}\) be positive definite and let \(^{d}\). Let \((0,1)\) and define_

\[L(,)+(1-)^{ }.\]

_Then \(()(^{}) (L(,))\) and_

\[((L(,)))=}}(())\]

_In particular, if \(\|\|_{^{-1}}^{2}>d\) and_

\[=,\] (20)

_then \((0,1)\) and \(((L(,)))=_{d}() (())\) where_

\[_{d}()=}}=( )^{d/2}()^{(d-1)/2}(0,1).\] (21)

Proof.: First, note that for any \(()(^{})\) and any \((0,1)\) we have

\[,L(,)= ,+(1-),+(1-)=1.\]

Thus, \((L(,))() (^{})\). For the volume decrease, recall that for ellipsooids \(()\) we have \((())=}}{{)}}}\) where \(V_{d}\) is the volume of the unit sphere in \(^{d}\). By the matrix-determinant lemma, we have

\[(L(,))=(1+ ,^{-1})()= (1+)^{d}().\]

Figure 14: Illustration of the ellipsoids in Theorem D.2 in the left. In the right an illustration of the symmetrized intersection of halfspaces \(()\) used in the proof of Lemma 5.2 together with the ellipsoid \(()\) used in the convex combination in the lemma.

Therefore,

\[((L(,)))=)}}}( ())=} }(()).\]

Finally, for \(\) defined as in (20) we have

\[1+ =1+(1-)\! =1+(-1),\] \[=1+() =1+=,\]

which yields the desired formula for \(_{d}()\). 

On the norm of \(\).The above theorem has a requirement on the norm of the vector \(\) that defines the half-space \(_{}()\). However, in our cutting plane method we obtain \(\) from Proposition 4.2 and Proposition 4.3, which do not have any guarantees on the norm of \(\) explicitly. Crucially, at any given iteration \(t\) of multidimensional backtracking with ellipsoids, we select a candidate preconditioner \(_{t}=(_{t})\) such that \(\|_{t}\|_{}=}{{}}\). Then, if it fails the Armijo condition in (4) and \(_{t}\) is as given by Proposition 4.2, then we have \(_{t}_{>}(_{t})\), that is, the separating hyperplane excludes \(_{t}\). As we will show, this implies that \(\|\|_{^{-1}}\) is large.

**Lemma D.3**.: _Let \(^{d d}\) be positive definite and \(^{d}_{ 0}\) be such that \(\|\|_{}\) for some \(>0\). Let \(^{d}_{ 0}\) be such that \(_{>}()\). Then \(\|\|_{^{-1}}>}{{}}\)._

Proof.: For the sake of contradiction, assume \(\|\|_{^{-1}}}{{}}\). Then \(\|\|_{^{-1}}\|\|_{ } 1\). Thus, by the Cauchy-Schwartz inequality,

\[,=^{-1/2} ,^{1/2}\|^{-1/2 }\|\|^{1/2}\|=\| \|_{^{-1}}\|\|_{ } 1.\]

This is a contradiction since \(_{>}()\) and, therefore, \(,>1\). 

On the volume decrease.Although the formula \(_{d}()\) in Equation (21) can be hard to interpret, we show a simple bound when \(\|\|_{^{-1}}^{2} 2d\).

**Lemma D.4**.: _Let \(^{d d}\) be a positive definite matrix and \(^{d}\) be such that \(\|\|_{A^{-1}}^{2}>d\). For \(c:=d/(0,1)\) we have \(_{d}()}\), where \(_{d}\) is defined as in (21). In particular, if \(\|\|_{A^{-1}}^{2}>d\), then \(_{d}()}}{{}}\)._

Proof.: Define \(:=\|\|_{^{-1}}^{2}>d\) and \(c:=}{{}}(0,1)\). Then,

\[_{d}()^{2} =()^{d}\!() ^{(d-1)} =( )^{(d-1)} =c(c}{{c}}-1}{d-1})^{(d-1)}\] \[=c()^{(d-1)} =c(1+)^{(d-1)}  c e^{1-c},\]

where the last inequality follows since \(1+x e^{x}\) for all \(x\). In particular, note that \(c(0,1) c e^{1-c}\) is increasing since the derivative of the mapping is positive on \((0,1)\). Thus, if \(\|\|_{^{-1}} 2d\), then \(c\) and \(c e^{1-c}(}{{2}}) e^{1/2}\). 

Exploiting symmetry.Let us now exploit symmetry to avoid using non-diagonal matrices in our ellipsoids. We use the notion of _axis-aligned_ sets in the next few results. A set \(^{d}\) is _axis-aligned_ if for any point \(\), the reflections of \(\) along the axes are also contained in \(\). Formally, for any \(\{ 1\}^{d}\), we have that if \(\), then \(()\). Furthermore, with a slight abuse of notation define \(()(())\). That is, \(()\) is the diagonal matrix whose diagonal entries match those of \(\). The idea is that the set \(\{\,^{d}_{ 0}:()\}\) of diagonals of valid preconditioners is contained in the non-negative orthant. Yet, we can extend it by reflecting it over each of the axes. Although this may seem counter-intuitive, this translates the structure of our problem into symmetry among all orthant, and this can be exploited elegantly. Formally, the set of diagonals of valid preconditioners reflected over each axis is given by set

\[\{\,^{d}:(||) \},\]

where \(||\) is the entry-wise absolute value of \(^{d}\). The following lemma shows that when looking for low volume ellipsoids that contain an axis-aligned set, we can restrict out attention to axis-aligned ellipsoids, defined by a diagonal matrix. The following lemma can be seen as the polar statement of Goemans et al. (2009, Proposition 3.1), with the benefit of not requiring any matrix inversions.

**Lemma D.5**.: _Let \(^{d}\) be an axis-aligned convex set and let \(^{d d}\) be positive definite matrix such that \(()\). Then \((())\) and \(((()))( ())\)._

Proof.: Let us start by showing that \((())\). We use the notation \(()\) to denote the set \(()\{\,():\}\). Since \(\) is axis-aligned, we have

\[=()( )()=(() \,()),\{ 1\}^{d}.\]

Therefore, \(\) is contained in each of the \(2^{d}\) ellipsoids of the form \((()\,())\). Thus,

\[_{\{ 1\}^{d}}(()\,()) }_{\{ 1\}^{d}}() \,(),\]

where the last inclusion follows since, for any set of positive definite matrices \(\), one may verify that \(_{}() (}{{||}})_{ }\). Finally, note that

\[_{\{ 1\}^{d}}()\,()=().\]

Indeed, let \(i,j\{1,,d\}\). If \(i=j\), then \((()\,())_{i,j}= _{i,j}\) for any \(\{ 1\}^{d}\). If \(i j\), then

\[_{\{ 1\}^{d}}(() \,())_{i,j}\] \[=_{\{ 1\}^{d}:\,[i][j ]}(()\,())_{i,j}+ _{\{ 1\}^{d}:\,[i]=[j]}(( )\,())_{i,j}\] \[=2^{d-1}(-_{i,j})+2^{d-1}_{i,j}=0.\]

Let us now show that \(((()))( ())\). Note that \(((()))=((( )))-()\). Since \(()\) is concave over positive definite matrices, we have

\[(()) =}_{\{ 1\}^{d}} ()\,()\] \[}_{\{ 1\}^{d}} ()\,()=} 2^{d}()=().\]

Therefore,

\[(((()))) =((()))-( ())\] \[((()))-( )\] \[=((())),\]

which implies that \(((()))( ())\). 

We are now in position to prove Lemma 5.2, which follows directly from the previous two results.

Proof of Lemma 5.2.: By the assumptions in Proposition 4.2 we have that \(()\) fails the Armijo condition 4 condition and, thus, \(_{>}()\). This together with the assumption that \(\|\|_{}}{{}}\) imply via Lemma D.3 that \(_{^{-1}}\). This allows us to use Theorem D.2 to find a new ellipsoid containing \(()_{}()\) with the required volume decrease by Lemma D.4.

Yet, this ellipsoid may not be axis-aligned. We shall exploit the symmetry described in Lemma D.5 to show that the axis-aligned ellipsoid \((^{+}(,))\) enjoys the same guarantees.

Formally, we need \((^{+}(,))\) to contain \(()_{}()\). Since \( 0\), we have

\[_{}()()\{\, :() _{}()\{ 1\}^{d}\}.\]

Thus, it suffices for \((^{+}(,))\) to contain \(()()\). From Theorem D.2 we know that \(()()\) is contained in the ellipsoid given by the matrix \(()+(1-)^{ }\) for any \(\), in particular for \(\) as in (20) since \(\|\|_{^{-1}}>\). Since \(()\) is axis-aligned, we can exploit symmetry using Lemma D.5, which tells that \(()()\) is contained in the ellipsoid given by the matrix

\[(()+(1-) ^{})=(^{+}( ,)),\]

as desired. Finally, the bound on the volume follows by Theorem D.2 and the bound on \(_{d}()\) given by Lemma D.4 since \(\|\|_{^{-1}}\). 

Finally, we are in position to prove Theorem 5.3, which follows almost directly from Lemma 5.2.

Proof of Theorem 5.3.: Note that (a) holds by induction and since, by Proposition 4.2, we have \((^{*})_{}(_{t})\) for any \(_{t}\) used in a call to cut. For (b), fix \(t\{1,,T\}\) and recall that in this case we have \(_{t}=\{\,():( _{t})\}\). As described in (8), one may verify that \((_{t}^{*})\) for \(_{t}^{*}\) given by

\[_{t}^{*}_{t})^{2} \|_{_{t}^{-1}}}_{t}^{-1} f(_{t} )^{2}\]

maximizes \(\| f(_{t})\|_{}\) for \(_{t}\). Since

\[_{t}=(_{t},}{{}}, _{t})=}(_{t}^{*}),\]

we conclude that \(_{t}\) is \(}{{}}\)-competitive. For (c), first note that we may assume \((}{{L}})(_{0}I)\). To see that, assume \((}{{L}})(_{0}I)\), implying \(_{0}d>L^{2}\). In this case, any candidate preconditioner computed by candidate is always valid and, thus, we never call cut. To see this, let \(_{0}_{0}\) be the matrix defining the initial ellipsoid. Then, by the definition of candidate for ellipsoids we have that \(_{0}=(_{0})\) is such that

\[\|_{0}\|_{_{0}}^{2}=_{0}\|_{0} \|^{2}=<}{L^{2}}.\]

Therefore, \(_{0}[i]}{{L}}\) for all \(i\{1,,d\}\), which implies that \(_{0}\) is valid since \(_{0}\).

Let us look now at the case \((}{{L}})(_{0}I)\). Therefore, \((}{{L}})(_{t})\) for all iterations \(t\). Since the minimum volume ellipsoid containing the box \((}{{L}}))\) is the unit sphere of radius \(}{{L}}\), that is, \((}}{{d)}})\). Therefore, \(((_{t}))( (}}{{d)}}))\). Moreover, every time we call cut the volume of the ellipsoid goes down by \(1/c[d]{e}/[d]{e}/[d]{e}\). Therefore, the total number of calls to cut is no more than

\[_{c}\!(((_{0}))}{ (((}{{L}}/d)))})= _{c}\!(}{d_{0}^{d/2}}) \!(}) 12d\!(})\]

since \((c) 1/12\). 

**Refining the choice of \(\).** Although we have shown in Lemma 5.2 a choice a \(\) that guarantees volume decrease, it may be sub-optimal. The choice of \(\) in Equation (20) is inherited from the non-symmetric case in Theorem D.2. Although Lemma 5.2 and Theorem D.2 match when \(\) has only one non-zero entry, we should expect better choices of \(\), leading to more volume shrinkage, to be possible in Lemma 5.2. Although we have not found a choice of \(\) that is dependent on \(\) that generically improves upon (20), in practice we can solve for a better \(\) numerically, by directly minimizing the volume of the resulting ellipsoid,

\[_{0<<1}((+(1-) (^{})))=_{0<<1}-_ {i}([i]+(1-)[i]^{2}).\]As the problem is one-dimensional, numerical solvers can often find near-optimal solutions. By warm-starting a numerical solver with the \(\) defined in (20), we can guarantee that the resulting ellipsoid leads to a smaller volume and we do not lose our worst-case theoretical guarantees.

**Connection to the polar problem and related work.** Our results have an interesting connection to some of the results from Goemans et al. (2009), via the use of polarity theory. Here we give a quick overview of their work and the connection to our cutting plane methods. Goemans et al. (2009) shows techniques to approximate some polyhedron \(^{d}\) (a polymatroid being one of the main examples) _from inside_ by some ellipsoid \(()\). Their algorithm maintains an ellipsoid \(()\) and tries to iteratively enlarge it. They assume access to an oracle such that, at each iteration, either finds a point \(\) that is sufficiently far from \(()\), meaning \(\|\|_{}>+\) for some \(>0\), or guarantees that \(()\) "approximates well" \(\) from inside in the sense that \(\|\|_{}(+)/\) for all \(\), where \(>0\) is some approximation factor. In their algorithm, when the oracle finds a point \(\) such that \(\|\|_{}>+\) the algorithm needs to find an ellipsoid \((^{+})\) such that

\[(^{+})(() \{,-\}),\] (22)

where \(()\) is the convex hull of \(\). Interestingly, the polar problem is exactly what we need for out cutting plane method. More precisely, the polar set \(^{*}\) of a set \(\) is given by \(^{*}:=\{\,z^{d}: z,x 1\}\). Then, by taking polars and using that \(()^{*}=(^{-})\), we have that \(^{*}(^{-1})\). Moreover, taking polar on both sides of (22) yields that an equivalent problem is finding \((^{+})^{-1}\) such that

\[((^{+})^{-1})(^{-1}) \{-,\}^{*}=(^{-1})\{\,:|,| 1\}.\]

That is, the problem is the one of finding a smaller ellipsoid \(((^{+})^{-1})\) that contains \((^{-1})\{\,:|, | 1\}\), which is broadly the goal of the subroutine cut.

## Appendix E Experiments

### Objective functions

We use \(L_{2}\)-regularized linear regression \(_{}\) and \(L_{2}\)-regularized logistic regression \(_{}()\), with a regularization coefficient of \(1\). Given a data matrix \(^{n d}\), target \(y^{n}\) for regression tasks and \(y\{0,1\}^{n}\) for classification tasks, and parameters \(^{d}\),

\[_{}() =\|- \|^{2}+\|\|^{2}.\] \[_{}() =_{i=1}^{n}-[i](( _{i},))-(1-[i])(1-( _{i},))+\|\|^{2}.\]

where \(_{i}\) is the \(i\)th row of \(\) and \(\) is the sigmoid function, \((z)=}{{1+(-z)}}\). For all datasets, we add a bias term by prepending a feature column of ones to \(\).

### Datasets

We use the datasets listed in Table 1, made available by LIBSVM (Chang and Lin, 2011), Scikit-Learn (Pedregosa et al., 2011) and the UCI repository (Dua and Graff, 2017).

### Data rescaling

We do not rescale, standardize or otherwise change any of the datasets beyond adding a bias term, as our goal is to check whether preconditioned methods can handle badly scaled data.

#### Initializations

We consider two types of initializations. The first approximates a "best-case" scenario where we start from an estimate with a reasonable loss value despite the bad scaling of the data. We set \([i]=0\) except for the bias term \(\) which is set at the MLE of the non-regularized problem,

\[ = =_{i=1}^{n}[i] ,\] \[ =\!(}{1-}) =_{i=1}^{n}[i] .\]

The results in the main text use this initialization. The second initialization takes \((0,)\), giving a starting point with potentially large loss. We give results using both initializations in the appendix.

  
**Dataset** & Repository/Source & \(n\) & \(d\) & \(\) & \(_{*}\) \\ 
**cpusmall** & LIBSVM, Delve (comp-activ) & \(8\,192\) & \(12\) & \(10^{13}\) & \(10^{2}\) \\
**california-housing** & Scikit/StatLib, Kelley Pace and Barry (1997) & \(20\,640\) & \(8\) & \(10^{10}\) & \(10^{4}\) \\
**power-plant** & UCI, Tufekci (2014) & \(9\,568\) & \(4\) & \(10^{9}\) & \(10^{4}\) \\
**concrete** & UCI, Yeh (1998) & \(1\,030\) & \(8\) & \(10^{9}\) & \(10^{3}\) \\
**mg** & LIBSVM, Flake and Lawrence (2002) & \(1\,385\) & \(6\) & \(10^{3}\) & \(10^{3}\) \\
**breast-cancer** & UCI & \(569\) & \(32\) & \(10^{13}\) & \(10^{2}\) \\
**australian** & LIBSVM, Statlog & \(690\) & \(14\) & \(10^{9}\) & \(10^{2}\) \\
**heart** & LIBSVM, Statlog & \(270\) & \(13\) & \(10^{7}\) & \(10^{2}\) \\
**diabetes** & UCI & \(768\) & \(8\) & \(10^{6}\) & \(10^{2}\) \\
**ionosphere** & UCI & \(351\) & \(34\) & \(10^{3}\) & \(10^{2}\) \\
**news20** & LIBSVM, Keerthi and DeCoste (2005) & \(19\,996\) & \(1\,355\,191\) & \(10^{13}\) & NA \\
**rcv1** & LIBSVM, Lewis et al. (2004) & \(20\,242\) & \(47\,236\) & \(10^{13}\) & NA \\   

Table 1: **Datasets used in our experiments, including number of samples \(n\) and dimension \(d\), and order of magnitude of the condition number of the regularized system (\((^{}+}{{n}})\)) and condition number of the system when using the optimal diagonal preconditioner, \(_{*}\).**

#### Optimizers used

* For the small linear regression problems, we use preconditioned gradient descent with the optimal preconditioner, pre-computed using the semidefinite formulation of Qu et al. (2022), solved using CVXPY (Diamond and Boyd, 2016) based on the Matlab implementation of Qu et al. https://github.com/Gwzwpxz/opt_dpcond
* Gradient descent with a backtracking line-search with backtracking parameter \(=}{{2}}\).
* RPROP (Riedmiller and Braun, 1993) following the implementation and default hyperparameters in PyTorch (Paszke et al., 2019) (starting step-size of \(10^{-1}\), increase step-size factor \(^{+}=1.2\), decreasing step-size factor \(^{-}=0.5\), minimum step-size of \(10^{-6}\) and maximum step-size of \(50\)). https://github.com/pytorch/pytorch/blob/v2.0.1/torch/optim/rprop.py
* Hypergradient descent to set the step-size, using (S)GD-HD (the multiplicative variant, Baydin et al., 2018). The hypergradient step-size is set to the default \(=0.02\)(Baydin et al., 2018, footnote 7: To increase by a factor of \(1.1\) in the one-dimensional case, the update to the ellipsoid should be \(^{}=/1.1^{2}\).
* The diagonal Barzilai-Borwein method of Park et al. (2020), using their non-monotonic line-search. We use the default parameters suggested; a starting step-size of \(10^{-6}\), regularization factor on the previous diagonal approximation \(=10^{-6}\), a backtracking factor of \(}{{2}}\) for the backtracking line-search and a window of \(15\) steps for the non-monotone line-search. This line-search does not use a forward step as the update can increase the preconditioner.
* Preconditioned gradient descent using the diagonal Hessian, with a backtracking line-search.
* AdaGrad (Duchi et al., 2011) but augmented with a backtracking line-search as suggested by Vaswani et al. (2020) to make it competitive in the deterministic setting, following the PyTorch (Paszke et al., 2019) implementation. https://github.com/pytorch/pytorch/blob/v2.0.1/torch/optim/adagrad.py

#### Line-search and forward steps

For all methods, the backtracking line-search is augmented by a forward step. When a step-size is accepted, it is increased by a factor of \(1.1\) for the next step. For multidimensional backtracking, we increase the set uniformly, taking \(^{}=1.1\) for the box and \(^{}=/\) for the ellipsoid. The ellipsoid uses a slightly smaller increase factor.7

#### Hyperparameters for the line-search and multidimensional backtracking

For the backtracking line-searches used in gradient descent, preconditioned gradient descent and used to augment the other algorithms, we start the search at an initial step-size of \(10^{10}\) and backtrack by a factor of \(}{{2}}\) when failing the Armijo condition, implemented generically as

\[f(-) f()- f( ),\]

For multidimensional backtracking, we initialize the sets such that the first preconditioner is on the order of \(10^{10}\). Using the notation of Appendix A, we use the scaling factor \(c_{0}=d 10^{10}\) for the box variant and \(c_{0}= 10^{10}\) for the ellipsoid variant. The first preconditioner tried by the box variant with backtracking factor \(=}{{2d}}\) is then \(}{{2}} 10^{10}\), and the first preconditioner tried by the ellipsoid variant (assuming the gradient is uniform, \( f(_{0})\)) is \(}{{}} 10^{10}\).

[MISSING_PAGE_FAIL:38]

Figure 15: Runs on small linear regression datasets with Bias initialization

Figure 16: Runs on small linear regression datasets with Gaussian initialization

Figure 17: Runs on small logistic regression datasets with Bias initialization

Figure 18: Runs on small logistic regression datasets with Gaussian initialization

Figure 19: Runs on large logistic regression datasets with Bias initialization

Figure 20: Runs on large logistic regression datasets with Gaussian initialization