# Policy Aggregation

Parand A. Alamdari

University of Toronto & Vector Institute

parand@cs.toronto.edu

&Soroush Ebadian

University of Toronto

soroush@cs.toronto.edu

Equal contribution. Authors are listed alphabetically.

Ariel D. Procaccia

Harvard University

arielpro@seas.harvard.edu

###### Abstract

We consider the challenge of AI value alignment with multiple individuals that have different reward functions and optimal policies in an underlying Markov decision process. We formalize this problem as one of _policy aggregation_, where the goal is to identify a desirable collective policy. We argue that an approach informed by social choice theory is especially suitable. Our key insight is that social choice methods can be reinterpreted by identifying ordinal preferences with volumes of subsets of the _state-action occupancy polytope_. Building on this insight, we demonstrate that a variety of methods -- including approval voting, Borda count, the proportional veto core, and quantile fairness -- can be practically applied to policy aggregation.

## 1 Introduction

Early discussion of AI value alignment had often focused on learning desirable behavior from an individual teacher, for example, through inverse reinforcement learning . But, in recent years, the conversation has shifted towards aligning AI models with large groups of people or even entire societies. This shift is exemplified at a policy level by OpenAI's "democratic inputs to AI" program  and Meta's citizens' assembly on AI governance , and at a technical level by the ubiquity of reinforcement learning from human feedback  as a method for fine-tuning large language models.

We formalize the challenge of value alignment with multiple individuals as a problem that we view as fundamental -- _policy aggregation_. Our starting point is the common assumption that the environment can be represented as a _Markov decision process (MDP)_. While the states, actions and transition functions are shared by all agents, their reward functions -- which incorporate values, priorities or subjective beliefs -- may be different. In particular, each agent has its own optimal policy in the underlying MDP. Our question is this: _How should we aggregate the individual policies into a desirable collective policy?_

A naive answer is to define a new reward function that is the sum of the agents' reward functions (for each state-action pair separately) and compute an optimal policy for this aggregate reward function; such a policy would guarantee maximum _utilitarian social welfare_. This approach has a major shortcoming, however, in that it is sensitive to affine transformations of rewards, so, for example, if we doubled one of the reward functions, the aggregate optimal policy may change. This is an issue because each agent's individual optimal policy is invariant to (positive) affine transformations of rewards, so while it is possible to recover a reward function that induces an agent's optimal policy byobserving their actions over time,2 it is impossible to distinguish between reward functions that are affine transformations of each other. More broadly, economists and moral philosophers have long been skeptical about _interpersonal comparisons of utility_ due to the lack of universal scale -- an issue that is especially pertinent in our context. Therefore, aggregation methods that are invariant to affine transformations are strongly preferred.

**Our approach.** To develop such aggregation methods, we look to social choice theory, which typically deals with the aggregation of _ordinal_ preferences. To take a canonical example, suppose agents report _rankings_ over \(m\) alternatives. Under the _Borda count_ rule, each voter gives \(m-k\) points to the alternative they rank in the \(k\)'th position, and the alternative with most points overall is selected.

The voting approach can be directly applied to our setting. For each agent, it is (in theory) possible to compute the value of every possible (deterministic) policy, and rank them all by value. Then, any standard voting rule, such as Borda count, can be used to aggregate the rankings over policies and single out a desirable policy. The caveat, of course, is that this method is patently impractical, because the number of policies is exponential in the number of states of the MDP.

The main insight underlying our approach is that ordinal preferences over policies have a much more practical volumetric interpretation in the _state-action occupancy polytope_\(\). Roughly speaking, a point in the state-action occupancy polytope represents a (stochastic) policy through the frequency it is expected to visit different state-action pairs. If a policy is preferred by an agent to a subset of policies \(^{}\), its "rank" is the volume of \(^{}\) as a fraction of the volume of \(\). The "score" of a policy under Borda count, for example, can be interpreted as the sum of these "ranks" over all agents.

**Our results.** We investigate two classes of rules from social choice theory, those that guarantee a notion of fairness and voting rules. By mapping ordinal preferences to the state-action occupancy polytope, we adapt the different rules to the policy aggregation problem.

The former class is examined in Section 5. As a warm-up we start from the notion of _proportional veto core_; it follows from recent work by Chaudhury et al.  that a volumetric interpretation of this notion is nonempty and can be computed efficiently. We then turn to _quantile fairness_, which was recently introduced by Babichenko et al. ; we prove that the volumetric interpretation of this notion yields guarantees that are far better than those known for the original, discrete setting, and we design a computationally efficient algorithm to optimize those guarantees.

The latter class is examined in Section 6; we focus on volumetric interpretations of \(\)_-approval_ (including the ubiquitous _plurality_ rule, which is the special case of \(=1\)) and the aforementioned Borda count. In contrast to the rules studied in Section 5, existence is a nonissue for these voting rules, but computation is a challenge, and indeed we establish several computational hardness results. To overcome this obstacle, we implement voting rules for policy aggregation through mixed integer linear programming, which leads to practical solutions.

Finally, our experiments in Section 7 evaluate the policies returned by different rules based on their fairness; the results identify quantile fairness as especially appealing. The experiments also illustrate the advantage of our approach over rules that optimize measures of social welfare (which are sensitive to affine transformations of the rewards).

## 2 Related Work

Noothigattu et al.  consider a setting related to ours, in that different agents have different reward functions and different policies that must be aggregated. However, they assume that the agents' reward functions are noisy perturbations of a ground-truth reward function, and the goal is to learn an optimal policy according to the ground-truth rewards. In social choice terms, our work is akin to the typical setting where subjective preferences must be aggregated, whereas the work of Noothigattu et al.  is conceptually similar to the setting where votes are seen as noisy estimates of a ground-truth ranking [39; 9; 6].

Chaudhury et al.  study a problem completely different from ours: fairness in federated learning. However, their technical approach served as an inspiration for ours. Specifically, they consider the proportional veto core and transfer it to the federated learning setting using volumetric arguments, by considering volumes of subsets in the space of _models_. Their proof that the proportional veto core is nonempty carries over to our setting, as we explain in Section 5.

There is a body of work on multi-objective reinforcement learning (MORL) and planning that uses a scalarization function to reduce the problem to a single-objective one [32; 21]. Other solutions to MORL focus on developing algorithms to identify a set of policies approximating the problem's Pareto frontier . A line of work more closely related to ours focuses on fairness in sequential decision making, often taking the scalarization approach to aggregate agents' preferences by maximizing a (cardinal) social welfare function, which maps the vector of agent utilities to a single value. Ogryczak et al.  and Siddique et al.  investigate generalized Gini social welfare, and Mandal and Gan , Fan et al.  and Ju et al.  focus on Nash and egalitarian social welfare. Alamdari et al.  study this problem in a non-Markovian setting, where fairness depends on the history of actions over time, and introduce concepts to assess different fairness criteria at varying time intervals. A shortcoming of these solutions is that they are not invariant to affine transformations of rewards -- a property that is crucial in our setting, as discussed earlier.

Our work is closely related to the pluralistic alignment literature, aiming to develop AI systems that reflect the values and preferences of diverse individuals [35; 10]. Alamdari et al.  propose a framework in reinforcement learning in which an agent learns to act in a way that is considerate the values and perspectives of humans within a particular environment. Concurrent work explores reinforcement learning from human feedback (RLHF) from a social choice perspective, where the reward model is based on pairwise human preferences, often constructed using the Bradley-Terry model . Zhong et al.  consider the maximum Nash and egalitarian welfare solutions, and Swamy et al.  propose a method based on maximal lotteries due to Fishburn .

## 3 Preliminaries

For \(t\), let \([t]=\{1,2,,t\}\). For a closed set \(S\), let \((S)\) denote the probability simplex over the set \(S\). We denote the dot product of two vectors as \( x,y=_{i=1}^{d}x_{i} y_{i}\) for \(x,y^{d}\). A _halfspace_ in \(^{d}\) determined by \(w^{d}\) and \(b\) is the set of points satisfying \(\{x^{d} x,w b\}\). A _polytope_\(^{d}\) is the intersection of a finite number of halfspaces, i.e., a convex subset of the \(d\)-dimensional space \(^{d}\) determined by a set of linear constraints \(\{x Ax b\}\) where \(A^{k d}\) is a matrix of coefficients of \(k\) linear inequalities and \(b^{k}\).

### Multi-Objective Markov Decision Processes

A multi-objective Markov decision process (MOMDP) is a tuple defined as \(M=,,,R_{1},,\)\(R_{n}\) for the average-reward case and \(,d_{},,,R_{1},,R_{n},\) for the discounted-reward case, where \(\) is a finite set of states, \(\) is a finite set of actions, and \(:()()\) is the transition probability distribution. \((s_{t},a_{t},s_{t+1})\) is the probability of transitioning to state \(s_{t+1}\) by taking action \(a_{t}\) in \(s_{t}\). For \(i[n]\), \(R_{i}:\) is the reward function of the \(i\)th agent, the initial state is sampled from \(d_{}()\), and \((0,1]\) is the discount factor.

A _(Markovian) policy_\((a|s)\) is a probability distribution over the actions \(a\) given the state \(s\). A policy is _deterministic_ if at each state \(s\) one action is selected with probability of \(1\), and otherwise it is _stochastic_. The expected _average_ return of agent \(i\) for a policy \(\) and the expected _discounted_ return of agent \(i\) for a policy \(\) are defined over an _infinite time horizon_ as

\[J_{i}^{}()=_{T}\,_{ ,}[_{t=1}^{T}R_{i}(s_{t},a_{t})],J_{i}^{ }()=(1-)\,_{,}[_{t=1} ^{}^{t}R_{i}(s_{t},a_{t})|_{s_{1} d_{}}]\]

where the expectation is over the state-action pairs at time \(t\) based on both the policy \(\) and the transition function \(\).

**Definition 1** (state-action occupancy measure).: _Let \(_{}^{t}\) be the probability measure over states at time \(t\) under policy \(\). The state-action occupancy measure for state \(s\) and action \(a\) is defined as_

\[d_{}^{}(s,a)=_{T}\,\, [_{t=1}^{T}_{}^{t}(s)(a|s)],\,d_{}^{ }(s,a)=(1-)\,[_{t=1}^{}^{ t}_{}^{t}(s)(a|s)].\]

For both the average and discounted cases, we can rewrite the expected return as the dot product of the state-action occupancy measures and rewards, that is,\( d_{},R_{i}\). In addition, the policy can be calculated given the occupancy measure as \((a|s)=d_{}(s,a)/(_{a}d_{}(s,a))\) if \(_{a}d_{}(s,a)>0\), and \((a|s)=1/||\) otherwise.

**Definition 2** (state-action occupancy polytope ).: _For a MOMDP \(M\) in the average-reward case, the space of valid state-action occupancies is the polytope_

\[^{}=d_{}^{} d_{}^{ } 0,_{s,a}d_{}^{}(s,a)=1,_{a}d_{}^{ }(s,a)=_{s^{},a^{}}(s^{},a^{ },s)d_{}^{}(s^{},a^{}), s}.\]

We similarly define this polytope for the discounted-reward case in Appendix A.

A _mechanism_ receives a MOMDP and aggregates the agents' preferences into a policy. An economical efficiency axiom in the social choice literature is that of Pareto optimality.

**Definition 3** (Pareto optimality).: _For a MOMDP \(M\) and \( 0\), a policy \(\) is \(\)-Pareto optimal if there does not exist another policy \(^{}\) such that \(J_{i}(^{}) J_{i}()+\) for all \(i N\), with strict inequality for at least one agent. For \(=0\), we simply call such policies Pareto optimal._

We call a mechanism Pareto optimal if it always returns a Pareto optimal policy. In special cases where all agents unanimously agree on an optimal policy, Pareto optimality implies that the mechanism will return one such policy. We discuss Pareto optimal implementations of all mechanisms in this work.

### Voting and Social Choice Functions

In the classical social choice setting, we have a set of \(n\) agents and a set \(C\) of \(m\) alternatives. The preferences of voter \(i[n]\) is represented as a _strict ordering_ over the alternatives \(_{i}:[m] C\) equal to \(_{i}(1)_{i}_{i}(2)_{i}_{i}_{i}(m)\), where \(c_{1}_{i}c_{2}\) denotes agent \(i\) prefers \(c_{1}\) over \(c_{2}\) for \(c_{1},c_{2} C\). A (possibly randomized) voting rule aggregates agents' preferences and returns an alternative or a distribution over the alternatives as the collective decision.

**Positional Scoring Rules.** A positional scoring rule with scoring vector \(=(s_{1},,s_{m})\) such that \(s_{1} s_{2} s_{m}\) works as follows. Each agent gives a score of \(s_{1}\) to their top choice, a score of \(s_{2}\) to their second choice, and so on. The votes are tallied and an alternative with the maximum total score is selected. A few of the well-known positional scoring rules are: Plurality: \((1,0,,0)\), Borda: \((m-1,m-2,,1,0)\), \(k\)-approval: \((1,,1,0,,0)\) with \(k\) ones.

## 4 Occupancy Polytope as the Space of Alternatives

In a MOMDP \(M\), each agent \(i\) incorporates their values and preferences into their respective reward function \(R_{i}\). Agent \(i\) prefers \(\) over \(^{}\) if and only if \(\) achieves higher expected return, \(J_{i}()>J_{i}(^{})\), and is indifferent between two policies \(\) and \(^{}\) if and only if \(J_{i}()=J_{i}(^{})\). As discussed before, given a state-action occupancy measure \(d_{}\) in the state-action occupancy polytope \(\), we can recover the corresponding policy \(\). Therefore, we can interpret \(\) as the domain of all possible alternatives over which the \(n\) agents have heterogeneous _weak_ preferences (with ties). Agent \(i\) prefers \(d_{}\) to \(d_{^{}}\) in \(\) if and only if they prefer \(\) to \(^{}\). We study the policy aggregation problem through this lens; specifically, we design or adapt voting mechanisms where the (continuous) space of alternatives is \(\) and agents have weak preferences over them determined by their reward functions \(R_{i}\).

**Affine transformation and reward normalization.** A particular benefit of this interpretation, as mentioned before, is that all positive affine transformations of the reward functions, i.e., \(aR_{i}+b\) for all \(a_{ 0}\) and \(b\), yield the same weak ordering over the polytope. Hence, we can assume without loss of generality that \(J_{i}()\). Further, we can ignore agents that are indifferent between all policies, i.e., \(_{}J_{i}()=_{}J_{i}()\), and normalize reward functions \(R_{i}-_{}J_{i}()}{_{}J_{i}()-_{} J_{i}()}\) such that \(_{}J_{i}()=0\) and \(_{}J_{i}()=1\). The relative ordering of the policies does not change since for all points \(d_{}\) we have \(_{s,a}d_{}(s,a)=1\).

**Volumetric definitions.** A major difference between voting over a continuous space of alternatives and the classical voting setting is that the domain of alternatives is infinite and not all voting mechanisms can be directly applied to the policy aggregation problem. In particular, various voting rules require reasoning about the rank of an alternative or the size of some subset of alternatives. For instance, the Borda count of an alternative \(c\) over a finite set of alternatives is defined as the number (or fraction) of candidates ranked below \(c\). In the continuous setting, for almost all of the mechanisms that we discuss later, we use the _measure_ or _volume_ of a subset of alternatives to refer to their size. For a measurable subset \(^{}\), let \((^{})\) denote its measure. The ratio \((^{})/()\) is the fraction of alternatives that lie in \(^{}\). A probabilistic interpretation is that for a uniform distribution over the polytope \(\), \((^{})/()\) denotes the probability that a policy uniformly sampled from \(\) lies in \(^{}\). We can also define the _expected return distribution_ of an agent over \(\) as a random variable that maps a policy to its expected return, i.e., one that maps \(d_{}\) to \( d_{},R_{i}\). The pdf and cdf of this r.v. is defined below.

**Definition 4** (expected return distribution).: _For a MOMDP \(M\) and \(v\), the expected return distribution of agent \(i[n]\) is defined as_

\[f_{i}(v)=()}_{x} (v- x,R_{i})\;dx, F_{i}(v)=_{x=-}^{v}f_{i}( x)\,dx=(\{ x,R_{i} v\})}{ ()},\]

_where \(f_{i}\) and \(F_{i}\) are the pdf and cdf of the expected return distribution and \(()\) is the Dirac delta function indicating \(v= x,R_{i}\)._

A useful observation about \(f_{i}\), the pdf, is that it is unimodal, i.e., increasing up to its mode\((f_{i})_{v}f_{i}(v)\) and decreasing afterwards, which follows from the Brunn-Minkowski inequality . Since \(f_{i}\) (pdf) is the derivative of \(F_{i}\) (cdf), the unimodality of \(f_{i}\) implies that \(F_{i}\) is a convex function in \((-,(f_{i})]\) and concave in \([(f_{i}),)\).

In our algorithms, we use a subroutine that measures the volume of a polytope, which we denote by \(\)-\((\{Ax b\})\). Dyer et al.  designed a fully polynomial time randomized approximation scheme (FPRAS) for computing the volume of polytopes. We report the running time of algorithms in terms of the number of calls to this oracle.

## 5 Fairness in Policy Aggregation

In this section, we utilize the volumetric interpretation of the state-action occupancy polytope to extend fairness notions from social choice to policy aggregation, and we develop algorithms to compute stochastic policies provably satisfying these notions.

### Proportional Veto Core

The proportional veto core was first proposed by Moulin  in the classical social choice setting with a finite set of alternatives where agents have full (strict) rankings over the alternatives. For simplicity, suppose the number of alternatives \(m\) is a multiple of \(n\). The idea of the proportional veto core is that \(x\%\) of the agents should be able to veto \(x\%\) of the alternatives. More precisely, for an alternative \(c\) to be in the proportional veto core, there should not exist a coalition \(S\) that can "block" \(c\) using their proportional veto power of \(}{{n}}\). \(S\) blocks \(c\) if they can unanimously suggest \(m(1-}{{n}})\) candidates that they prefer to \(c\). For instance, if \(c\) is in the proportional veto core, it cannot be the case that a coalition of \(60\%\) of the agents unanimously prefer \(40\%\) of the alternatives to \(c\).

Chaudhury et al.  extended this notion to a continuous domain of alternatives in the federated learning setting. We show that such an extension also applies to policy aggregation.

**Definition 5** (proportional veto core).: _Let \((0,}{{n}})\). For a coalition of agents \(S[n]\), let \((S)=}{{n}}\) be their veto power. A point \(d_{}\) is blocked by a coalition \(S\) if there exists a subset \(^{}\) of measure \((^{})/()  1-(S)+\) such that all agents in \(S\) prefer all points in \(^{}\) to \(d_{}\), i.e., \(d_{^{}}_{i}d_{}\) for all \(d_{^{}}^{}\) and \(i S\). A point \(d_{}\) is in the \(\)-proportional veto core if it is not blocked by any coalition._

A candidate in the proportional veto core satisfies desirable properties that are extensively discussed in prior work [26; 7; 24; 22]. It is worth mentioning that any candidate in the \(\)-proportional veto core, besides the fairness aspect, is also economically efficient as it satisfies \(\)_-Pareto optimality_. This holds since the grand coalition \(S=[n]\) can veto any \(\)-Pareto dominated alternative.

Moulin  proved that the proportional veto core is nonempty in the discrete setting and Chaudhury et al.  proved it for the continuous setting. The following result is a corollary of Theorem 1 of Chaudhury et al. .

**Theorem 1**.: _Let \((0,1/n)\). For a policy aggregation problem, the \(\)-proportional veto core is nonempty. Furthermore, such policies can be found in polynomial time using \(O((1/))\) many calls per agent to \(\)._

We refer the reader to the paper of Chaudhury et al.  for the complete proof, and provide a high-level description of Algorithm 1, which finds a point in the proportional veto core. Algorithm 1 iterates over the agents and lets the \(i\)th agent "eliminate" roughly \(}{{n}}()\) of the remaining space of alternatives. That is, agent \(i\) finds the hyperplane \(H_{i}=\{ x,R_{i} v_{i}^{*}\}\) such that its intersection with \(O_{i-1}\) (the remaining part of \(\) at the \(i\)th iteration) has a volume of approximately \(()/n\). This value, for each agent, can be found by doing a binary search over \(v_{i}^{*}\) to a precision of \([v_{i}^{*}-,v_{i}^{*}]\) by \(O((1/))\) calls to the volume estimating subroutine \(\).3

**Pareto optimality.** We briefly discuss why Algorithm 1 is Pareto optimal. During the \(i\)th iteration, the space of policies is contracted by adding a linear constraint of the form \(J_{i}() v_{i}^{*}\). If the returned welfare-maximizing policy \(\) (derived from \(d_{}\)) is Pareto dominated by another policy \(^{}\), then \(^{}\) would satisfy all these linear constraints as \(J_{i}(^{}) J_{i}() v_{i}^{*}\) with the earlier inequality being strict for at least one agent. Therefore, \(d_{^{}}_{n}\) and \(^{}\) achieves a higher social welfare, which is a contradiction. The same argument can be used to establish the Pareto optimality of other mechanisms discussed later; each of these mechanisms searches for a policy that satisfies certain lower bounds on agents' utilities, from which a welfare-maximizing, Pareto optimal policy can be selected.

### Quantile Fairness

Next, we consider an egalitarian type of fairness based on the occupancy polytope, building on very recent work by Babichenko et al.  in the discrete setting; surprisingly, we show that it is possible to obtain stronger guarantees in the continuous setting.

Babichenko et al.  focus on the fair allocation of a set of \(m\) indivisible items among \(n\) agents, where each item must be fully allocated to a single agent. They quantify the extent an allocation \(A\) is _fair_ to an agent \(i\) by the fraction of allocations over which \(i\) prefers \(A\) (note that the number of all discrete allocations is \(n^{m}\)). In other words, if one randomly samples an allocation, the fairness is measured by the probability that \(A\) is preferred to the random allocation. An allocations is _\(q\)-quantile fair_ for \(q\) if _all_ agents consider this allocation among their top \(q\)-quantile allocations. Babichenko et al.  aim to find a universal value of \(q\) such that for any fair division instance, a \(q\)-quantile fair allocation exists. They make an interesting connection between \(q\)-quantile fair allocations and the Erdos Matching Conjecture, and show under the assumption that the conjecture holds, \((1/2e)\)-quantile fair allocations exist for all instances.4

We ask the same question for policy aggregation, and again, a key difference is that our domain of alternatives is continuous. The notion of \(q\)-quantile fairness extends well to our setting. Agents assess the fairness of a policy \(\) based on the fraction of the occupancy polytope (i.e., the set of all policies) to which they prefer \(\), or equivalently, the probability that they prefer the chosen policy to a randomly sampled policy.

**Definition 6** (\(q\)-quantile fairness).: _For a MOMDP \(M\) and \(q\), a policy \(\) is \(q\)-quantile fair if for every agent \(i[n]\), \(\) is among \(i\)'s top \(q\)-fraction of policies, \((\{ x,R_{i} J_{i}()\})  q()\)._We show that a \((1/e)\)-quantile fair policy always exist and that this ratio is tight; note that this bound is twice as good as that of Babichenko et al. , and it is unconditional. We prove this by making a connection to an inequality due to Grunbaum . The _centroid_ of a polytope \(P\) is defined as \((P)=x\ d\ x}{_{x P}1\ d\ x}\).

**Lemma 1** (Grunbaum's Inequality).: _Let \(P\) be a polytope and \(w\) a direction in \(^{d}\). For the halfspace \(H=\{x w,x-(P) 0\}\), it holds that_

\[()^{d} (P H)}{(P)} 1-()^{d}  1-,\]

_Furthermore, this is tight for the \(n\)-dimensional simplex._

**Theorem 2**.: _For every MOMDP \(M\), there always exist \(q\)-quantile fair policy where \(q=()^{-1}\) and \(=||||\). Note that \(q 36.7\%\). Furthermore, this bound is tight: For any \(\), there is an instance with a single state and \(\) actions where no \(q\)-quantile fair policy exists for any \(q>()^{-1}\)._

Proof.: First, we show that the centroid of the occupancy polytope \(c=()\) is \(q\)-quantile fair policy for the aformentioned \(q\). Since \(\) is a subset of the \((n-1)\)-simplex (see Definition 2), \(\) has a nonzero volume in some lower dimensional space \(^{}||||-1\). By invoking Grunbaum's inequality (Lemma 1) with \(w_{i}\) being equal to \(R_{i}\) projected to the \(^{}\)-dimensional subspace for all agents \(i[n]\), we have that \(( H_{i})(}{^{ }+1})^{^{}}()\) where \(H_{i}=\{ x-c,w_{i} 0\}=\{ x,R_{i}  J_{i}(c)\}\). Since \(^{}-1\), we have \((}{^{}+1})^{^{}}()^{-1}\), which completes the proof.

To show tightness, take a MOMDP with a single state \(=\{s\}\) -- hence a constant transition function \(()=s\) -- and \(\) actions \(=\{a_{1},,a_{}\}\) and \(\) agents. The reward function of agent \(i\) is \(R_{i}(s,a_{i})=1\) and \(0\) otherwise. The state-action occupancy polytope of this MOMDP is the \((-1)\)-dimensional simplex \(=\{_{a}d_{}(s,a)=1,d_{}_{ 0}^{1 }\}\). Take any point in \(d_{}\). There exists at least one agent \(i\) that has \(J_{i}(d_{})=d_{}(s,a_{i})\). Take the halfspace \(H_{i}=\{ x,R_{i}\}\). Observe that \(H_{i}\) is equivalent to a smaller \((-1)\)-dimensional simplex \(\{_{a}d_{}(s,a)=1-\}\) which has volume of \(((1-))=()^{-1}()\). Therefore, \(( H_{i})}{()}=( )^{-1}\). 

The centroid of the occupancy polytope, as per Theorem 2, attains a worst-case measure of quantile-fairness. However, the centroid policy can be highly suboptimal as it disregards the preferences of the agents involved. For instance, there could exist a \(99\%\)-quantile fair policy. To this end, we take an egalitarian approach and aim to find a \(q\)-quantile fair policy with the maximum \(q\).

**Max quantile fair algorithm.** Algorithm 2 searches for the optimal value \(q^{*}\), for which a \(q^{*}\)-quantile fair policy exists, and gets close to \(q^{*}\) by a binary search. To perform the search, we need a subroutine that checks, for a given value of \(q\), if a \(q\)-quantile fair policy exists. Suppose we have the \(q\)-quantile expected return \(F_{i}^{-1}(q)\) for all \(i\), that is, the expected return amount \(v_{i}\) such that \(F_{i}(v_{i})=q\). Then, the problem of existence of a \(q\)-quantile fair policy is equivalent to the feasibility of the linear program \(\{x x,R_{i} F_{i}^{-1}(q),i[n]\}\), which can be solved in polynomial time. Importantly, after finding a good approximation of \(q\), there can be infinitely many policies that are \(q\)-quantile fair and there are various ways to select the final policy after finding the value \(q\). As mentioned earlier, a desirable efficiency property is Pareto optimality; to satisfy it, one can return the policy that maximizes the sum of agents' expected returns among the ones that are \(q\)-quantile fair. Finally, to calculate \(F_{i}^{-1}(q)\), we can again use binary search to get \(\) close to the value \(v_{i}\) for which \(F_{i}(v_{i})=q\) using \(O( 1/)\) calls to \(comp}\). The discussion above is summarized below.

**Proposition 3**.: _Assuming an optimal oracle for \(F_{i}^{-1}\), Algorithm 2 finds a \(q\)-quantile fair policy that is \(\) close to the optimal value in polynomial time with \(O((1/))\) per agent calls to the oracle. A \(\)-approximation to \(F_{i}^{-1}(q)\) can be computed using \(O((1/))\) calls to \(comp}\)._

## 6 Policy Aggregation with Voting Rules

In this section, we adapt existing voting rules from the discrete setting to policy aggregation and discuss their computational complexity.

**Plurality.** The plurality winner is the policy that achieves the maximum number of plurality votes or "approvals," where agent \(i\) approves a policy \(\) if it achieves their maximum expected return \(J_{i}()=_{^{}}J_{i}(^{})=1\). Hence the plurality winner is a policy in \(_{}_{i[n]}[J_{i}()=1]\). This formulation does not require the volumetric interpretation. However, in contrast to the discrete setting where one can easily count the approvals for all candidates, we show that solving this problem in the context of policy aggregation is not only \(\)-hard, but hard to approximate up to factor of a \(1/n^{1/2-}\). We establish the hardness of approximation by a reduction from the _maximum independent set_ problem ; we defer the proof to Appendix B.

**Theorem 4**.: _For any fixed \((0,1)\), there is no polynomial-time \(}\)-approximation algorithm for the maximum plurality score unless \(=\)._

Nevertheless, we can compute plurality in practice, as we discuss below.

\(\)**-approval.** We extend the \(k\)-approval rule using the volumetric interpretation of the occupancy polytope, similarly to the \(q\)-quantile fairness definition. For some \(\), agents approve a policy \(\) if its return is among their top \(\) fraction of \(\), i.e., \(F_{i}(J_{i}())\). The \(\)-approval winner is a policy that has the highest number of \(\)-approvals, so it is in \(_{}_{i[n]}[F_{i}(J_{i}())]\). Note that plurality is equivalent to \(1\)-approval. It is worth mentioning that there can be infinitely many policies that have the maximum approval score and, to avoid a suboptimal decision, one can return a Pareto optimal solution among the set of \(\)-approval winner policies.

Theorem 2 shows that for \( 1/e\), there always exists a policy that all agents approve, and by Proposition 3 such policies can be found in polynomial time, assuming access to an oracle for volumetric computations. Therefore, the problem of finding an \(\)-approval winner is "easy" for \((0,1/e)\). In sharp contrast, for \(=1\) -- namely, plurality -- Theorem 4 gives a hardness of approximation. The next theorem shows the hardness of computing \(\)-approval for \((7/8,1]\) via a reduction from the MAX-2SAT problem. We defer the proof to Appendix B.

**Theorem 5**.: _For \((7/8,1]\), computing a policy with the highest \(\)-approval score is \(\)-hard. This even holds for binary reward vectors and when every \(F_{i}\) has a closed form._

Given the above hardness result, to compute the \(\)-approval rule, we turn to _mixed-integer linear programming (MILP)_. Algorithm 3 simply creates \(n\) binary variables for each agent \(i\) indicating whether \(i\)\(\)-approves the policy, i.e., \(F_{i}(J_{i}())\) which is equivalent to \(J_{i}() F_{i}^{-1}()\). To encode the expected return requirement for agent \(i\) to approve a policy as a linear constraint, we precompute \(F_{i}^{-1}()\). This can be done by a binary search similar to Algorithm 2. Importantly, Algorithm 3 has one binary variable per agent and only \(n\) constraints which is key to its practicability.

**Borda count.** The Borda count rule also has a natural definition in the continuous setting. In the discrete setting, the Borda score of agent \(i\) for alternative \(c\) is the number of alternatives \(c^{}\) such that \(c_{i}c^{}\). In the continuous setting, \(F_{i}(J_{i}())\) indicates the volume of the occupancy polytope to which agent \(i\) prefers \(\). The Borda count rule then selects a policy among \(_{}_{i[n]}F_{i}(J_{i}())\).

The computational complexity of the Borda count rule remains an interesting open question, though we make progress on two fronts.5 First, we identify a sufficient condition under which we can find an approximate max Borda count policy using convex optimization in polynomial time. Second, similar to Algorithm 3, we present a MILP to approximate the Borda count rule in Algorithm 4.

The first is based on the observation in Section 4 that \(F_{i}\) is concave in range \([(f_{i}),)\). We assume that the max Borda count policy \(\) appears in the concave portion of all agents, i.e., \(J_{i}()(f_{i})\) for all \(i[n]\). Then, the problem becomes a maximization of the concave objective \(_{i}F_{i}( d_{},R_{i})\) over the convex domain \(\{d_{}\ |\  d_{},R_{i}(f_{i}),  i[n]\}\).

Second, Algorithm 4 is a MILP that finds an approximate max Borda count policy. As a pre-processing step, we estimate \(F_{i}\) for each agent \(i\) separately. We measure \(F_{i}\) for the fixed expected return values of \(\{,2,,1-,1\}\). This accounts for \(1/\) oracle calls to \(\) per agent. Then, for the MILP, we introduce \(1/\) binary variables for each agent indicating their \(\)-rounded return levels, i.e., \(a_{i,k}=1\) iff \( d_{},R_{i} k\) for \(k[1/]\). The MILP then searches for an occupancy measure \(d_{}\) with maximum total Borda score among the \(\)-rounded expected return vectors (see Appendix C for more details).

Finally, we make a novel connection between \(q\)-quantile fairness and Borda count in Theorem 6. We defer the proof to Appendix B. A corollary of Theorems 2 and 6 is that the policy returned by \(\)-max quantile fair algorithm (Algorithm 2) achieves a \((1/e-)\) multiplicative approximation of the maximum Borda score.

**Theorem 6**.: _A \(q\)-quantile fair policy is a \(q\)-approximation of the maximum Borda score._

## 7 Experiments

**Environment.** We adapt the dynamic attention allocation environment introduced by D'Amour et al. . We aim to monitor several sites and prevent potential incidents, but limited resources prevent us from monitoring all sites at all times; this is inspired by applications such as food inspection and pest control 6. There are \(m=5\) warehouses and each can be in 3 different stages: normal (\(\)), risky (\(\)) and incident (\(\)). There are \(||=3^{m}\) states containing all possible stages of all warehouses. In each step, we can monitor at most one site, so there are \(m+1\) actions, where action \(m+1\) is no operation and action \(i m\) is monitoring warehouse \(i\). There are \(n\) agents; each agent \(i\) has a list \(_{i}\) of warehouses that they consider valuable and a reward function \(R_{i}\). In each step \(t\), \(R_{i}(s_{t},a_{t})=-[a_{t} m]-_{j_{i}}_{i}w _{j}[s_{t,j}= a_{t} j]\), where \(w_{j}\{100,150,,250\}\) denotes the penalty of an incident occurring in warehouse \(j\), \(_{i}\) is the scale of penalties for agent \(i\) which is sampled from \(\{0.25,0.5,,n\}\), and \(-1\) is the cost of monitoring. In each step, if we monitor warehouse \(j\), its stage becomes normal. If not, it changes from \(\) to \(\) and from \(\) with probabilities \(p_{j,}\) and \(p_{j,}\), and stays the same otherwise. Probabilities are sampled i.i.d. uniformly from \([0.5,0.8]\). The state transitions \(\) is the product of the warehouses' stage transitions.

**Rules.** We compare the outcomes of policy aggregation with different rules: _max-quantile, Borda, \(\)-approval_ (\(=0.9,0.8\)), _egalitarian_ (maximize minimum return) and _utilitarian_ (maximize sum of returns). We sample \(5 10^{5}\) random policies based on which we fit a generalized logistic function to estimate the cdf of the expected return distribution \(F_{i}\) (Definition 4) for every agent. The policies for \(\)-approval voting rules are optimized with respect to maximum utilitarian welfare. The egalitarian rule finds a policy that maximizes the expected return of the worst-off agent, then optimizes for the second worst-off agent, and so on. The implementation details of Borda count are in Appendix D.

**Results.** In Figure 1, we report the normalized expected return of agents as \(()-_{}J_{i}()}{_{^{}}J_{i}(^{})- _{^{}}J_{i}(^{})}\) (sorted from lowest to highest) which are averaged over \(10\) different environment and agents instances. We observe that the utilitarian and egalitarian rules are sensitive to the different agents' reward scales and tend to perform unfairly. The utilitarian rule achieves the highest utilitarian welfare by almost ignoring one agent. The egalitarian rule achieves higher return for the worst-off agents compared to the utilitarian rule, but still yields an inequitable outcome. The max-quantile rule tends to return the fairest outcomes with similar normalized returns for the agents. The Borda rule, while not a fair rule by design, tends to find fair outcomes which are slightly worse than the max-quantile rule. The \(\)-approval rule with max utilitarian completion tends to the utilitarian rule as \( 0\) and to plurality as \( 1\). Importantly, although not shown in the plots, the plurality rule ignores almost all agents and performs optimally for a randomly selected agent.

In addition to the fine-grained utility distributions, in Table 1, we report two aggregate measures based on agents' utilities: (i) the Gini index, a statistical measure of dispersion defined as \(_{j N}|J_{i}()-J_{j}()|}{2n_{i N} J_{i}()}\) -- where a lower Gini index indicates a more equitable distribution, and (ii) the Nash welfare, defined as the geometric mean of agents' utilities \((_{i N}J_{i}())^{1/n}\) -- where a higher Nash welfare is preferable. We observe a similar trend as above, where utilitarian and egalitarian rules perform worse across both metrics. For the other four rules, the Nash welfare scores are comparable in both scenarios, with Borda showing slightly better performance. The Gini index, however, highlights a clearer distinction among the rules, with max-quantile performing better.

## 8 Discussion

We conclude by discussing some of the limitations of our approach. A first potential limitation is computation. When we started our investigation of the policy aggregation problem, we were skeptical that ordinal solutions from social choice could be practically applied. We believe that our results successfully lay this initial concern to rest. However, additional algorithmic advances are needed to scale our approach beyond thousands of agents, states, and actions. Additionally, an interesting future direction is to apply these rules within continuous state or action spaces, as well as in online reinforcement learning setting where the environment remains unknown.

A second limitation is the possibility of strategic behavior. The Gibbard-Satterthwaite Theorem [16; 33] precludes the existence of "reasonable" voting rules that are strategyproof, in the sense that agents cannot gain from misreporting their ordinal preferences; we conjecture that a similar result holds for policy aggregation in our framework. However, if reward functions are obtained through inverse reinforcement learning, successful manipulation would be difficult: an agent would have to act in a way that the learned reward function induces ordinal (volumetric) preferences leading to a higher-return aggregate stochastic policy. This separation between the actions taken by an agent and the preferences they induce would likely alleviate the theoretical susceptibility of our methods to strategic behavior.

   &  &  \\ 
**Rules** & **Gini index** & **Nash welfare** & **Gini index** & **Nash welfare** \\  egalitarian & \(0.2864 0.0295\) & \(0.2208 0.0717\) & \(0.2126 0.0209\) & \(0.4655 0.0581\) \\ utilitarian & \(0.4392 0.0094\) & \(0.0502 0.0174\) & \(0.2020 0.0182\) & \(0.5736 0.0471\) \\
80\%-approvals & \(0.1233 0.0047\) & \(0.5186 0.0051\) & \(0.1352 0.0037\) & \(0.6741 0.0200\) \\
90\%-approvals & \(0.0793 0.0056\) & \(0.5286 0.0053\) & \(0.1257 0.0034\) & \(0.6746 0.0211\) \\ Borda & \(0.0225 0.0024\) & \(0.5356 0.0062\) & \(0.1029 0.0083\) & \(0.6801 0.0261\) \\ max-quantile & \(0.0188 0.0022\) & \(0.5355 0.0062\) & \(0.0625 0.0067\) & \(0.6474 0.0232\) \\  

Table 1: Comparison of policies optimized by different rules in two scenarios based on Gini index and Nash welfare based on their normalized expected return averaged. We report the mean and the standard error.

Figure 1: Comparison of policies optimized by different rules in two different scenarios based on the normalized expected return for agents. The bars, grouped by rule, correspond to agents sorted based on their normalized expected return. The error bars show the standard error of the mean.