# Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need

Xianlong Wang\({}^{1,2,4,5}\), Minghui Li\({}^{}\), Wei Liu\({}^{1,2,4,5}\), Hangtao Zhang\({}^{4,5}\),

**Shengshan Hu\({}^{1,2,4,5}\), Yechao Zhang\({}^{1,2,4,5}\), Ziqi Zhou\({}^{1,2,3}\), Hai Jin\({}^{1,2,3}\)**

\({}^{1}\) National Engineering Research Center for Big Data Technology and System

\({}^{2}\) Services Computing Technology and System Lab \({}^{3}\) Cluster and Grid Computing Lab

\({}^{4}\) Hubei Engineering Research Center on Big Data Security

\({}^{5}\) Hubei Key Laboratory of Distributed System Security

\(\) School of Cyber Science and Engineering, Huazhong University of Science and Technology

\(\) School of Software Engineering, Huazhong University of Science and Technology

\(@sectionsign\) School of Computer Science and Technology, Huazhong University of Science and Technology

(wx199,minghuili,weiliu73,hangt_zhang,hushengshan,ycz,zhouziqi,hjin)@hust.edu.cn

Minghui Li is the corresponding author.

###### Abstract

Traditional unlearnable strategies have been proposed to prevent unauthorized users from training on the 2D image data. With more 3D point cloud data containing sensitivity information, unauthorized usage of this new type data has also become a serious concern. To address this, we propose the first integral unlearnable framework for 3D point clouds including two processes: (i) we propose an unlearnable data protection scheme, involving a class-wise setting established by a category-adaptive allocation strategy and multi-transformations assigned to samples; (ii) we propose a data restoration scheme that utilizes class-wise inverse matrix transformation, thus enabling authorized-only training for unlearnable data. This restoration process is a practical issue overlooked in most existing unlearnable literature, _i.e._, even authorized users struggle to gain knowledge from 3D unlearnable data. Both theoretical and empirical results (including 6 datasets, 16 models, and 2 tasks) demonstrate the effectiveness of our proposed unlearnable framework. Our code is available at https://github.com/CGCL-codes/UnlearnablePC.

## 1 Introduction

Recently, 3D point cloud deep learning has been making remarkable strides in various domains, _e.g._, self-driving  and virtual reality [1; 46]. Specifically, numerous 3D sensors scan the surrounding environment and synthesize massive 3D point cloud data containing sensitive information such as pedestrian and vehicles  to the cloud server for deep learning analysis [12; 23]. However, the raw point cloud data can be exploited for point cloud unauthorized deep learning if a data breach occurs, posing a significant privacy threat. Fortunately, the privacy protection approaches for preventing unauthorized training have been extensively studied in the 2D image domain [9; 19; 28; 29; 39; 42]. They apply elaborate perturbations on images such that trained networks over them exhibit extremely low generalization, thus failing to learn knowledge from the protected data, known as "making data unlearnable". Nonetheless, the stark disparity between 2D images and 3D point clouds poses significant challenges for drawing lessons from existing 2D solutions.

Specifically, migrating 2D unlearnable schemes to 3D suffers from following challenges: **(i) Incompatibility with 3D data.** Numerous model-agnostic 2D image unlearnable schemes operatein the pixel space, such as convolutional operations [39; 42], making them fail to be directly transferred to the 3D point space. **(ii) Poor visual quality.** Migrating model-dependent 2D unlearnable methods [5; 9; 19; 28] to 3D point clouds requires perturbing substantial points, leading to irregular three-dimensional shifts which may significantly degrade visual quality. Hence, these challenges spur us to start directly from the characteristics of point clouds for proposing 3D unlearnable solutions.

Recent works observe that 3D transformations can alter test-time results of models [7; 17; 44]. To explore this, we conduct an in-depth investigation into the properties of seven 3D transformations as shown in Fig. 1 and reveal the mechanisms by which transformations employed in a certain pattern serve as unlearnable schemes (Sec. 3.2). In light of this, we propose the first unlearnable approach in 3D point clouds via multi class-wise transformation (UMT), transforming samples to various forms for privacy protection. Concretely, we newly propose a category-adaptive allocation strategy by leveraging uniform distribution sampling and category constraints to establish a class-wise setting, thereby multiplying multi-transformations to samples based on categories. To theoretically analyze UMT, we define a binary classification setup similar to that used in [20; 33; 39]. Meanwhile, we employ a _Gaussian Mixture Model_ (GMM)  to model the clean training set and use the Bayesian optimal decision boundary to model the point cloud classifier. Theoretically, we prove that there exists a UMT training set follows a GMM distribution and the classification accuracy of UMT dataset is lower than that of the clean dataset in a Bayesian classifier.

Moreover, an incompatible issue in existing unlearnable works [9; 19; 28; 29; 39; 43] is identified , _i.e._, these approaches prevent unauthorized learning to protected data, but they also impede authorized users from effectively learning from unlearnable data. To address this, we propose a data restoration scheme that applies class-wise inverse transformations, determined by a lightweight message received from the protector. Our proposed unlearnable framework including UMT approach and data restoration scheme is depicted in Fig. 3.

Extensive experiments on 6 benchmark datasets (including synthetic and real-world datasets) using 16 point cloud models across CNN, MLP, Graph-based Network, and Transformer on two tasks (classification and semantic segmentation), verified the effectiveness of our proposed unlearnable scheme. We summarize our main contributions as follows:

* **The First Integral 3D Unlearnable Framework.** To the best of our knowledge, we propose the first integral unlearnable 3D point cloud framework, utilizing class-wise multi-transformation as its unlearnable mechanism (effectively safeguarding point cloud data against unauthorized exploitation) and proposing a novel data restoration approach that leverages class-wise reversible 3D transformation matrices (addressing an incompatible issue in most existing unlearnable works, where even authorized users cannot effectively learn knowledge from unlearnable data).
* **Theoretical Analysis.** We theoretically indicate the existence of an unlearnable situation that the classification accuracy of the UMT dataset is lower than that of the clean dataset under the decision boundary of the Bayes classifier in Gaussian Mixture Model.
* **Experimental Evaluation.** Extensive experiments on 3 synthetic datasets and 3 real-world datasets using 16 widely used point cloud model architectures on classification and semantic segmentation tasks verify the superiority of our proposed schemes.

Figure 1: An overview of existing seven types of 3D transformations. “*” denotes _rigid transformations_ that do not alter the shape of the point cloud samples, while the remaining transformations are non-rigid transformations.

## 2 Preliminaries

**Notation.** Considering the raw point cloud data \((,)\) sampled from a clean distribution \(\) for training a point cloud network, the user's goal is to obtain a model \(:\) by minimizing the loss function (_e.g._, cross-entropy loss) \(((),)\). Let \(\) be a 3D transformation matrix that does not seriously damage the visual quality of point clouds. Note that \(^{3 p}\), \(^{3 3}\), and \(p\) represents the number of points. In theoretical analysis, following [20; 33], we simplify a training dataset \(_{k}\) to a _Gaussian Mixture Model_ (GMM) \((y,)\), where \(y\{ 1\}\) denotes the class labels, \(^{d}\) denotes the mean value, and \(^{d d}\) denotes the identity matrix. Thus the Bayes optimal decision boundary for classifying \(_{k}\) is defined by \(P(x)^{T}x=0\). The accuracy of the decision boundary \(P\) on \(_{k}\) is equal to \((||||_{2})\), where \(\) denotes the _Cumulative Distribution Function_ (CDF) of the standard normal distribution.

**Data protector \(_{p}}\).**\(_{p}\) aims to protect the knowledge from the clean training set (with size of \(n\)) \(_{c}=\{_{i},_{i}\}_{i=1}^{n}\) by compromising the unauthorized models who train on the unlearnable point cloud data \(\{_{i}(_{i}),_{i}\}_{i=1}^{n}\), resulting in extremely poor generalization on the clean test distribution \(_{t}\). This objective can be formalized as:

\[_{(,)_{t}}( (;_{u}),),_{u}=*{arg\,min}_{}_{(_{i}, _{i})_{c}}((_{i}( _{i});),_{i})\] (1)

where \(_{p}\) assumes that training samples are all transformed into unlearnable ones while maintaining normal visual effects, in line with previous unlearnable works [19; 28; 39; 42]. By the way, solving Eq. (1) directly is infeasible for neural networks because it necessitates unrolling the entire training procedure within the inner objective and performing backpropagation through it to execute a single step of gradient descent on the outer objective .

**Authorized user \(_{a}}\).**\(_{a}\) aims to apply another transformation \(^{}\) on the unlearnable sample, making the protected data learnable. This is formally defined as:

\[_{(,)_{t}}( (;_{r}),),_{r}=*{arg\,min}_{}_{(_{i}, _{i})_{c}}((^{}_ {i}(_{i}(_{i}));),_{i})\] (2)

where \(_{a}\) assumes that, without access to any clean training samples, \(^{}\) can be constructed by utilizing a lightweight message \(M\) received from data protectors.

## 3 Our Proposed Unlearnable Schemes

### Key Intuition

Several recent works [7; 13; 44] reveal employing 3D transformations can mislead the model's classification results. Such a phenomenon implies that there might be some defects in point cloud classifiers when processing transformed samples, leading us to infer that 3D transformations are probable candidates for data protection against unauthorized training. If the transformed point cloud data are used to train unauthorized DNNs, only simple linear features inherent in 3D transformations (at which transformations may act as shortcuts ) are captured by the DNNs, successfully protecting point cloud data privacy.

Figure 2: (a) Training on the transformed ModelNet10 dataset (employing sample-wise, dataset-wise, and class-wise patterns) using PointNet classifier; (b) The high-level overview of the class-wise setting

### Exploring the Mechanism

We summarize existing 3D transformations in Fig. 1 and formally define them in Appendix A. To seek clarity on the application and selection of transformations, we explore three aspects: **(i) execution mechanism**, **(ii) exclusion mechanism**, and **(iii) working mechanism** as follows.

**(i) Which execution mechanism successfully satisfy Eq. (1)?** The extensively employed execution patterns in 2D unlearnable approaches are sample-wise [9; 19] and class-wise [19; 39] settings. We further complement the dataset-wise setting (using universal transformation) and implement the above execution mechanisms for training a PointNet classifier  on the transformed ModelNet10 , obtaining test accuracy results in Fig. 2 (a). We discover that model achieves considerably low test accuracy under the class-wise setting, satisfying Eq. (1). Sample-wise and dataset-wise settings do not obviously compromise model performance, which cannot serve as promising unlearnable routes. Moreover, we note that sample-wise transformation is often considered as a data augmentation scheme to improve generalization, which contradicts our aim of using class-wise transformation to lower model generalization.

**(ii) Which transformations need to be excluded?** Not all transformations are suitable candidates. We exclude three transformations, tapering, reflection, and translation. (1) The tapering matrix may cause point cloud samples to become a planar projection when \( z\) defined in Eq. (18) equals to -1, rendering the tapered samples meaningless; (2) The reflection matrix has only three distinct transformation matrices, rendering it incapable of assigning class-wise transformations when the number of categories exceeds three; (3) The translation transformation is a straightforward and simple additive transformation that is too easily defeated by point cloud data pre-processing approaches.

**(iii) Why does class-wise transformation work?** We conduct experiments using class-wise transformations (see Tab. 6), indicating that the model training on the class-wise transformed training set achieves a relatively high accuracy on the class-wise transformed test set (using the same transformation process as the training set). Besides, if we permute the class-wise transformation for the test set, we obtain a significant low accuracy on the test set. Therefore, we conclude that the reason why class-wise transformation works is that the model learns the mapping between class-wise transformations and corresponding category labels as shown in Fig. 2 (b), which results in the model being unable to predict the corresponding labels on a clean test set lacking transformations. This analytical process yields conclusions that are in agreement with prior research [39; 44].

### Our Design for UMT

#### 3.3.1 Category-Adaptive Allocation Strategy

We assign transformation parameters based on categories to realize class-wise setting. For rotation transformation \(^{3 3}\), we refer to \(\) and \(\) as slight angles imposed on the \(x\) and \(y\) axes, \(\) as the primary angle for \(z\) axis. We generate random angles for \(_{N}\) times in three directions:

\[,(0,r_{s}),(0,r_{p}),_{N}=\] (3)

where \(\) denotes uniform distribution, \(N\) denotes the number of categories, \(r_{s}\) is a small range that controls \(\) and \(\), while \(r_{p}\) is a large range that controls \(\). \(_{N}\) is computed in such a way to ensure that the number of combinations of three angles is greater than or equal to \(N\). Concretely, in the rotation operation, each of the three directions has \(_{N}\) distinct angles, which means that the final rotation matrix has \(_{N}^{3}\) possible combinations. To satisfy the class-wise setup, \(_{N}^{3}\) must be at least \(N\), requiring \(_{N}\) to be no less than \(\). Finally, we randomly select \(N\) combinations of angles for the allocation. The scaling transformation \(^{3 3}\) resizes the position of each point in the 3D point cloud sample by a certain scaling factor \(\), which is sampled \(N\) times from a uniform distribution \(\):

\[(b_{l},b_{u})\] (4)

where \(b_{l}\) and \(b_{u}\) represent the lower bound and upper bound of the scaling factor, respectively. For shear \(^{3 3}\) defined in Eq. (13), twisting \(^{3 3}\) defined in Eq. (16), the process of generating parameters within ranges \((_{l},_{u})\) and \((h_{l},h_{u})\) is consistent to scaling. The range of these parameters ensures the visual effect of the sample.

**Property 1**.: _Since rotation matrices \(_{}\), \(_{}\), and \(_{}\) around three directions are all orthogonal matrices, \(\) is also an orthogonal matrix, which can be defined as:_

\[\{_{},_{}, _{},\},^{T}= \] (5)

where we can determine the orthogonality by matrix multiplication through the definitions of Eq. (11). Since \(=_{}_{}_{}\) and \(^{T}=_{}_{}_{} _{}^{T}_{}^{T}_{}^{T}= \), so \(\) is also an orthogonal matrix.

**Property 2**.: _All four transformation matrices we employ, \(\), \(\), \(\), and \(\), and the multiplicative combinations of any these matrices are all invertible matrices, which can be formally defined as:_

\[\{f()f()f()f( ) f(x)\{x,\}\}, ==\] (6)

where the inverse matrices of \(\), \(\), \(\), and \(\) are given in Appendix A. This property allows the authorized users to normally train on the protected data due to that multiplying a matrix by its inverse results in the identity matrix, leading us to propose a data restoration scheme in Sec. 3.4.

#### 3.3.2 Employing Class-wise Transformations

Assuming the point cloud training set \(_{c}\) is defined as \(\{(_{c1},_{i}),(_{c2},_{i}),...,( _{cn_{i}},_{i})\}_{i=1}^{N}\), where \(n_{1},n_{2},...,n_{N}\) represent the number of samples in the 1st, 2nd,..., \(N\)-th category, respectively. We formally define the spectrum of transformations as \(k\) to indicate the number of transformations involved. Thus the ultimate unlearnable transformation matrix \(_{k}\) is defined as:

\[_{k}=_{i=1}^{k}_{i}, i j, _{i},_{j}\{,, ,\},_{i}_{j}\] (7)

Once we employ the proposed category-adaptive allocation strategy to \(_{k}\), the unlearnable point cloud dataset \(_{u}\) is constructed as:

\[_{u}=\{(_{k_{i}}(_{c1}),_{i}),( _{k_{i}}(_{c2}),_{i}),...,(_{k_{i}}( _{cn_{i}}),_{i})\}_{i=1}^{N}\] (8)

Our proposed UMT scheme is described in Algorithm 1. We enumerate possible transformations in Eq. (7) to obtain the unlearnability in Tab. 7 and select one type of class-wise transformation for each \(k\) for more comprehensive experiments in Tab. 1. To facilitate the theoretical study of UMT2, we opt for \(\) as the transformation matrix \(\), which achieves the best unlearnable effect as suggested in Tabs. 1 and 7. Thus, in the GMM scenario, the class-wise transformation matrix is defined as \(_{y}=_{y}_{y}=_{y}_{y} ^{d d}\), where \(_{y}\) is the scaling factor.

**Lemma 3**.: _The unlearnable dataset \(_{u}\) generated using UMT on \(_{c}\) can also be represented using a GMM, i.e., \(_{u}(y_{y},_{y}^{2})\)._

Proof.: See Appendix D.1. Lemma 3 demonstrates that the unlearnable dataset \(_{u}\) can also be represented as a GMM, which is derived from Property 1.

**Lemma 4**.: _The Bayes optimal decision boundary for classifying \(_{u}\) is given by \(P_{u}(x)x^{}x+^{}x+=0\), where \(=_{-1}^{-2}-_{1}^{-2}\), \(=2(_{-1}^{-2}_{-1}+_{1}^{-2}_{1})\), and \(=^{2}|}{|_{1}^{2} |}\)._

Figure 3: An overview of our proposed integral unlearnable pipeline_Proof:_ See Appendix D.2. Lemma 4 reveals that the Bayesian decision boundary for classifying \(_{u}\) is a quadratic surface based on the GMM expression of \(_{u}\).

**Lemma 5**.: _Let \(z(0,)\), \(Z=z^{}z+b^{}z+c\), where \(b=}{A},c=}{A}\), and \(_{2}\) denote 2-norm of vectors. For any \(t 0\) and \(\), we employ Chernoff bound to have:_

\[\{Z[Z]+\}}{2(1-2 t)}||b||_{2}^{2}-t(+d)\}}{|(1-2t)|^{}}\]

_Proof:_ See Appendix D.3. Lemma 5 enables us to establish an upper bound on the accuracy of the unlearnable decision boundary \(P_{u}\) applied to the clean dataset \(_{c}\), denoted as \(_{_{c}}(P_{u})\), as presented in Theorem 6 below.

**Theorem 6**.: _For any constant \(t_{1}\) and \(t_{2}\) satisfying \(0 t_{1}<\) and \(0 t_{2}<\), the accuracy of the unlearnable decision boundary \(P_{u}\) on \(_{c}\) can be upper-bounded as:_

\[_{_{c}}(P_{u}) ^{2}}{2(1-2t_{1})}||b+2||_{2}^ {2}+t_{1}(^{}+b^{}+c)\}}{2|(1-2t_{1})|^{}}\] \[+^{2}}{2(1-2t_{2})}||b-2||_{2}^{2}- t_{2}(^{}-b^{}+c+2d)\}}{2|(1-2t_{2})|^{}}\] \[:=p_{1}+p_{2}\]

_Furthermore, if \(^{}+b^{}+c+d<0\) and \(-^{}+b^{}-c-d<0\), we have \(_{_{c}}(P_{u})<1\). Moreover, for any \( 0\), \(\) matrix \(_{i}\) such that \(_{_{c}}(P_{u})<_{_{c}}(P)\), where \(P\) is the Bayes optimal decision boundary for classifying \(_{c}\)._

_Proof:_ See Appendix D.4. The unlearnable effect takes place when \(_{_{c}}(P_{u})<_{_{c}}(P)\). To achieve this, we elaborately choose \(_{y}\), which is formalized as \(^{}^{-2}_{1}^{-1}+_{1}^{-2}_{1}^{}}{_{-1}^{-2}-_{1}^{-2}} 0\). Therefore, Theorem 6 theoretically explains why UMT is effective in generating unlearnable point cloud data.

### Data Restoration Scheme

To ensure that authorized users can achieve better generalization after training on unlearnable data, _i.e._, satisfying Eq. (2), we exploit the inverse properties of 3D transformations, presented in Property 2, to calculate the inverse matrix of \(_{k}\) as:

\[_{k}{}^{-1}=_{i=k}^{1}_{i}{}^{-1}, i j,_{i}{}^{-1},_{j}{}^{-1}\{ ^{-1},^{-1},^{-1},^{-1}\},_{i}{}^{-1}_{j}{}^{-1}\] (9)

In particular, we note that \(^{-1}=^{T}\), \(^{-1}=\). Afterwards, the authorized user receives a lightweight message \(M\) containing class-wise parameters from the data protector through a secure channel, thereby assigning \(M\) to the inverse transformation matrix in Eq. (9) for multiplying the unlearnable samples. Our proposed integral unlearnable process is illustrated in Fig. 3.

## 4 Experiments

### Experimental Details

**Datasets and Models.** Three synthetic 3D point cloud datasets, ModelNet40 , ModelNet10 , ShapeNetPart , and three real-world datasets including autonomous driving dataset KITTI  and indoor datasets ScanObjectNN , S3DIS  are used. We choose 16 widely used 3D point cloud models PointNet , PointNet++ , DGCNN , PointCNN , PCT , PointConv , CurveNet , SimpleView , 3DGCN , LGR-Net , RIConv , RIConv++ , PointMLP , PointNN , PointTransformerV3 , and SegNN  for evaluation of classification and semantic segmentation tasks.

**Experimental Setup.** The training process involves Adam optimizer , CosineAnnealingLR scheduler , initial learning rate of 0.001, weight decay of 0.0001. We empirically set \(r_{s}\), \(r_{p}\), \(b_{l}\), \(b_{u}\), \(_{l}\), \(_{u}\), \(h_{l}\), and \(h_{u}\) to 15\({}^{}\), 120\({}^{}\), 0.6, 0.8, 0\({}^{}\), 20\({}^{}\), 0, and 0.4 respectively. The main results of different 

[MISSING_PAGE_FAIL:7]

knowledge about the \(_{p}\)'s use of \(\). Thus we propose random rotation & scaling as an adaptive scheme. In Tab. 2, the adaptive scheme exhibits a higher accuracy than other schemes, confirming its effectiveness. Nonetheless, it remains 28.67% lower than clean baseline, revealing the robustness of UMT against adaptive attack. More results of adaptive attacks are provided in Appendix C.3.

**Visual Effect.** We visualize UMT samples in Figs. 7 to 10, indicating that the unlearnable point cloud samples still retain their normal feature structure with visual rationality.

**Evaluation of Semantic Segmentation.** We evaluate UMT using common metrics for point cloud semantic segmentation tasks in Tab. 3. As can be seen, the performance of semantic segmentation of data protected by UMT significantly decreases. The underlying reason is that the DNNs learn the features of class-wise transformations and establish a new mapping, which leads to the inability of test samples without transformations to be correctly segmented by the segmentation model.

**Evaluation of Data Restoration.** We multiply UMT samples by the transformation matrix in Eq. (9). The data becomes learnable after the restoration process, with test accuracy reaching a level comparable to the clean baseline as shown in Fig. 4. This strongly validates the effectiveness of the proposed data restoration scheme.

### Ablation Study and Hyper-Parameter Sensitivity Analysis

**Ablation on Rotation Module.** As shown in Tab. 4, the average accuracy increases by 15.18% and 21.65%, respectively, when only using \(\). This suggests the importance of class-wise rotation module. The high test accuracy demonstrated by 3DGCN  can be attributed to its scaling invariance, which endows it with robustness against scaling transformation.

**Ablation on Scaling Module.** As also shown in Tab. 4, the average accuracy increases by 18.80% and 16.88% when only using the rotation module, respectively. The high test accuracy achieved on RIConv  and LGR-Net  is due to the fact that both networks are rotation-invariant, thus providing resistance against rotation transformations. These ablation results furthermore emphasize the importance of incorporating more non-rigid transformations.

**Hyper-Parameter Analysis.** We analyze four hyperparameters \(r_{s}\), \(r_{p}\), \(b_{l}\), and \(b_{u}\) in Fig. 5. The influence of \(r_{s}\) and \(r_{p}\) on the accuracy remains relatively small, exhibiting their best unlearnable effect when set to 15\({}^{}\) and 120\({}^{}\), respectively. We attribute this to the crucial role played by the class-wise setting, while it is not highly sensitive to the size of specific values. The unlearnable effect is the best when \(b_{l}\) and \(b_{u}\) are set to 0.6 and 0.8, respectively. Similarly, the variations in \(b_{l}\) and \(b_{u}\) do not significantly alter the effect due to the class-wise setting.

### Insightful Analysis Into UMT

We formalize \(()=_{(,)}[ _{c}((;),)]\), where \(_{c}\) is the cross-entropy loss, \(,\) is the point cloud data sampled from dataset \(\). We define \(_{tr}\), \(_{u}\), and \(_{c}\) as the training set, unlearnable test set (_i.e._, test set transformed by UMT), and clean test set, respectively. Thus we have the _training loss_\(_{train}=(_{tr})\), _unlearnable test loss_\(_{u}=(_{u})\), and _clean test loss_\(_{c}=(_{c})\).

The models trained on both clean and UMT training sets exhibit low \(_{train}\) as shown in Fig. 6 (_yellow ellipses_), indicating the models converge well during training. Furthermore, when tested on \(_{c}\) as shown in Fig. 6 (a), the clean model (trained with clean training set) achieves a low \(_{c}\) (_blue ellipse_), while the UMT model (trained with UMT training set) exhibits a high \(_{c}\) (_red ellipse_), also supporting the unlearnable effectiveness of UMT. Fig. 6 (b) reveals that the clean model and UMT model both exhibit low \(_{u}\) (_green ellipse_), suggesting that they can classify UMT samples. But the mechanisms underlying the two cases of low \(_{u}\) differ. The clean one is due to that the semantics of samples can be remained by UMT and thus normally classified. The UMT one is that the UMT model learns the mapping between transformations and labels, thereby correctly predicting the samples containing the same transformations. We conclude that the clean model effectively classifies both clean and UMT samples, while the UMT model successfully classifies UMT samples (the UMT process is the same for both training and test samples) but fails to classify clean samples.

## 5 Related Work

### 2D Unlearnable Schemes

The development of 2D unlearnable schemes [19; 29; 37; 39; 42; 55] has been booming. Specifically, model-dependent methods are initially proposed in abundance [9; 19; 29]. Afterwards, numerous model-agnostic methods that significantly improve the generation efficiency have surfaced [39; 42; 47]. However, due to the structural disparities between 3D point cloud data and 2D images, applying unlearnable methods directly from 2D to 3D reveals significant challenges.

### Protecting 3D Point Cloud Data Privacy

Some works proposed an encryption scheme based on chaotic mapping  or optical chaotic encryption , and a 3D object reconstruction technique was introduced , both achieving privacy protection for individual 3D point cloud data. Nevertheless, no privacy-preserving solution has been proposed specifically for the scenario of unauthorized DNN learning on abundant raw 3D point cloud data. It is worth mentioning that both parallel works [44; 63] study availability poisoning attacks against 3D point cloud networks, which largely reduce model accuracy, and both have the potential to be applied as unlearnable schemes. However, the feature collision error-minimization poisoning scheme proposed by Zhu _et al._ overlooks the problem of effective training for authorized users, which limits its practical use in real-world applications. The rotation-based poisoning approach proposed by Wang _et al._ is easily defeated by rotation-invariant networks [57; 58; 59], as revealed in Tabs. 1 and 4.

Figure 6: UMT in weight space. The blue arrow represents the clean training trajectory of the weights \(_{i}\) at step \(i\), while the red arrows denote the UMT training trajectory. The values for plotting this figure are provided in Appendix C.6. (a) Testing on clean test set (_blue and red ellipses_); (b) Testing on UMT test set (_green ellipse_)Conclusion, Limitation, and Broader Impacts

In this research, we propose the first integral unlearnable framework in 3D point clouds, which utilizes class-wise multi transformations, preventing unauthorized deep learning while allowing authorized training. Extensive experiments on synthetic and real-world datasets and theoretical evidence verify the superiority of our framework. The transformations include rotation, scaling, twisting, and shear, which are all common 3D transformation operations. If unauthorized users design a network that is invariant to all these transformations, they could potentially defeat our proposed UMT. So far, only networks invariant to rigid transformations like rotation and scaling have been proposed, while networks invariant to non-rigid transformations like twisting and shear, have not yet been introduced. Therefore, our research also contributes to the design of more transformation-invariant networks.

Our research calls for the design of more robust point cloud networks, which helps improve the robustness and security of 3D point cloud processing systems. On the other hand, if our proposed UMT scheme is maliciously exploited, it may have negative impacts on society, such as causing a sharp decline in the performance of models trained on it, affecting the security and reliability of technologies based on 3D point cloud networks. More transformation-invariant 3D point cloud networks need to be proposed in the future to avoid potential negative impacts.