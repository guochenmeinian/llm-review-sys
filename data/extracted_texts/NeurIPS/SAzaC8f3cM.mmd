# Towards Self-Interpretable

Graph-Level Anomaly Detection

 Yixin Liu\({}^{1}\), Kaize Ding\({}^{2}\), Qinghua Lu\({}^{3}\), Fuyi Li\({}^{4,5}\), Leo Yu Zhang\({}^{6}\), Shirui Pan\({}^{6}\)

\({}^{1}\)Monash University, \({}^{2}\)Northwestern University, \({}^{3}\)Data61, CSIRO,

\({}^{4}\)Northwest A&F University, \({}^{5}\)The University of Adelaide, \({}^{6}\)Griffith University

yixin.liu@monash.edu, Kaize.ding@northwestern.edu, qinghua.lu@data61.csiro.au,

fuyi.li@nwsuaf.edu.cn, leo.zhang@griffith.edu.au, s.pan@griffth.edu.au

Corresponding Author.

###### Abstract

Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit notable dissimilarity compared to the majority in a collection. However, current works primarily focus on evaluating graph-level abnormality while failing to provide meaningful explanations for the predictions, which largely limits their reliability and application scope. In this paper, we investigate a new challenging problem, _explainable GLAD_, where the learning objective is to predict the abnormality of each graph sample with corresponding explanations, i.e., the vital subgraph that leads to the predictions. To address this challenging problem, we propose a Self-Interpretable Graph aNomaly dETection model (SIGNET for short) that detects anomalous graphs as well as generates informative explanations simultaneously. Specifically, we first introduce the multi-view subgraph information bottleneck (MSIB) framework, serving as the design basis of our self-interpretable GLAD approach. This way SIGNET is able to not only measure the abnormality of each graph based on cross-view mutual information but also provide informative graph rationales by extracting bottleneck subgraphs from the input graph and its dual hypergraph in a self-supervised way. Extensive experiments on 16 datasets demonstrate the anomaly detection capability and self-interpretability of SIGNET.

## 1 Introduction

Graphs are ubiquitous data structures in numerous domains, including chemistry, traffic, and social networks . Among machine learning tasks for graph data, graph-level anomaly detection (GLAD) is a challenge that aims to identify the graphs that exhibit substantial dissimilarity from the majority of graphs in a collection . GLAD presents great potential for various real-world scenarios, such as toxic molecule recognition  and pathogenic brain mechanism discovery . Recently, GLAD has drawn increasing research attention, with advanced techniques being applied to this task, e.g., knowledge distillation  and one-class classification .

Despite their promising performance, existing works  mainly aim to answer **how** to predict abnormal graphs by designing various GLAD architectures; however, they fail to provide explanations for the prediction, i.e., illustrating **why** these graphs are recognized as anomalies. In real-world applications, it is of great significance to make anomaly detection models explainable . From the perspective of models, valid explainability makes GLAD models trustworthy to meet safety and security requirements . For example, an explainable fraud detection model can pinpoint specific fraudulent behaviors when identifying defrauders, which enhances the reliability of predictions. From the perspective of data, an anomaly detection model with explainability canal equations of the dataset, which further supports human experts in data understanding . For instance, an explainable GLAD model for molecules can summarize the functional groups that cause abnormality, enabling researchers to deeply investigate the properties of compounds. Hence, the broad applications of interpreting anomaly detection results motivate us to investigate the problem of **Explainable GLAD** where the GLAD model is expected to measure the abnormality of each graph sample as well as provide meaningful explanations of the predictions during the inference time. As an example shown in Fig. 1, the GLAD model also extracts a graph rationale  corresponding to the predicted anomaly score. Although there are a few studies  proposed to explain anomaly detection results for visual or tabular data, explainable GLAD remains underexplored and it is non-trivial to apply those methods to our problem due to the discrete nature of irregular graph-structured data .

Towards the goal of designing an explainable GLAD model, two essential challenges need to be solved with careful design: _Challenge 1 -- how to make the GLAD model self-interpretable2?_ Even though we can leverage existing post-hoc explainers  for GNNs to explain the predictions of the GLAD model, such post-hoc explainers are not synergistically learned with the detection models, resulting in the risk of wrong, biased, and sub-optimal explanations . Hence, developing a self-interpretable GLAD model which detects graph-level anomalies with explanations inherently is more desirable and requires urgent research efforts. _Challenge 2 -- how to learn meaningful graph explanations without using supervision signals?_ For the problem of GLAD, ground-truth anomalies are usually unavailable during training, raising significant challenges to both detecting anomalies and providing meaningful explanations. Since most of the existing self-interpretable GNNs  merely focus on the (semi-)supervised setting, in particular the node/graph classification tasks, how to design a self-interpretable model for the explainable GLAD problem where ground-truth labels are inaccessible remains a challenging task.

To solve the above challenges, in this paper, we develop a novel Self-Interpretable Graph aNomaly dETection model (SIGNET for short). Based on the information bottleneck (IB) principle, we first propose a multi-view subgraph information bottleneck (MSIB) framework, serving as the design basis of our self-interpretable GLAD model. Under the MSIB framework, the instantiated GLAD model is able to predict the abnormality of each graph as well as generate corresponding explanations without relying on ground-truth anomalies simultaneously. To learn the self-interpretable GLAD model without ground-truth anomalies, we introduce the dual hypergraph as a supplemental view of the original graph and employ a unified bottleneck subgraph extractor to extract corresponding graph rationales. By further conducting multi-view learning among the extracted graph rationales, SIGNET is able to learn the feature patterns from both node and edge perspectives in a purely self-supervised manner. During the test phase, we can directly measure the abnormality of each graph sample based on its inter-view agreement (i.e., cross-view mutual information) and derive the corresponding graph rationales for the purpose of explaining the prediction. To sum up, our contribution is three-fold:

* **Problem.** We propose to investigate the explainable GLAD problem that has broad application prospects. To the best of our knowledge, this is the _first_ attempt to study the explainability problem for graph-level anomaly detection.
* **Algorithm.** We propose a novel self-interpretable GLAD model termed SIGNET, which infers graph-level anomaly scores and subgraph-level explanations simultaneously with the multi-view subgraph information bottleneck framework.

Figure 1: A toy example to illustrate (a) GLAD problem and (b) explainable GLAD problem.

* **Evaluation.** We perform extensive experiments to corroborate the anomaly detection performance and self-interpretation ability of SIGNET via thorough comparisons with state-of-the-art methods on 16 benchmark datasets.

## 2 Preliminaries and Related Work

In this section, we introduce the preliminaries and briefly review the related works. A more comprehensive literature review can be found in Appendix B.

**Notations.** Let \(G=(,,)\) be a simple graph with \(n\) nodes and \(m\) edges, where \(\) is the set of nodes and \(\) is the set of edges. The node features are included by feature matrix \(^{n d_{f}}\), and the connectivity among the nodes is represented by adjacency matrix \(^{n n}\). Unlike simple graphs where each edge only connects two nodes, "hypergraph" is a generalization of a traditional graph structure in which hyperedges connect more than two nodes. We define a hypergraph with \(n^{*}\) nodes and \(m^{*}\) hyperedges as \(G^{*}=(^{*},^{*},^{*})\), where \(^{*}\), \(^{*}\), and \(^{*}^{n^{*} d_{f}^{*}}\) are the node set, hyperedge set, and node feature matrix respectively. To indicate the higher-order relations among arbitrary numbers of nodes within a hypergraph, we use an incidence matrix \(^{*}^{n^{*} m^{*}}\) to represent the interaction between \(n^{*}\) nodes and \(m^{*}\) hyperedges. Alternatively, a simple graph and a hypergraph and be represented by \(G=(,)\) and \(G^{*}=(^{*},^{*})\), respectively. We denote the Shannon mutual information (MI) of two random variables \(A\) and \(B\) as \(I(A;B)\).

**Graph Neural Networks (GNNs).** GNNs are the extension of deep neural networks onto graph data, which have been applied to various graph learning tasks [1; 2; 23; 24; 25; 26; 27; 28]. Mainstream GNNs usually follow the paradigm of message passing [2; 23; 24; 26]. Some studies termed hypergraph neural networks (HGNNs) also apply GNNs to hypergraphs [29; 30; 31]. The formulations of GNN and HGNN are in Appendix C. To make the predictions understandable, some efforts try to uncover the explanation for GNNs [18; 32]. A branch of methods, termed post-hoc GNN explainers, use specialized models to explain the behavior of a trained GNN [19; 20; 33]. Meanwhile, some self-interpretable GNNs can intrinsically provide explanations for predictions using interpretable designs in GNN architectures [13; 21; 22]. While these methods mainly aim at supervised classification scenarios, how to interpret unsupervised anomaly detection models still remains open.

**Information Bottleneck (IB).** IB is an information theory-based approach for representation learning that trains the encoder by preserving the information that is relevant to label prediction while minimizing the amount of superfluous information [34; 35; 36]. Formally, given the data \(X\) and the label \(Y\), IB principle aims to find the representation \(Z\) by maximizing the following objective: \(_{Z}I(Z;Y)- I(X;Z)\), where \(\) is a hyper-parameter to trade off informativeness and compression. To extend IB onto unsupervised learning scenarios, Multi-view Information Bottleneck (MIB)  provides an optimizable target for unsupervised multi-view learning, which alleviates the reliance on label \(Y\). Given two different and distinguishable views \(V_{1}\) and \(V_{2}\) of the same data \(X\), the objective of MIB is to learn sufficient and compact representations \(Z_{1}\) and \(Z_{2}\) for two views respectively. Taking view \(V_{1}\) as an example, by factorizing the MI between \(V_{1}\) and \(Z_{1}\), we can identify two components: \(I(V_{1};Z_{1})=I(V_{1};Z_{1}|V_{2})+I(V_{2};Z_{1})\), where the first term is the superfluous information that is expected to be minimized, and the second term is the predictive information that should be maximized. Then, \(Z_{1}\) can be learned using a relaxed Lagrangian objective:

\[_{Z_{1}}I(V_{2};Z_{1})-_{1}I(V_{1};Z_{1}|V_{2}),\] (1)

where \(_{1}\) is a trade-off parameter. By optimizing Eq. (1) and its counterpart in view \(V_{2}\), we can learn informative and compact \(Z_{1}\) and \(Z_{2}\) by extracting the information from each other.

IB principle is also proven to be effective in graph learning tasks, such as graph contrastive learning [38; 39], subgraph recognition [17; 40], graph-based recommendation , and robust graph representation learning [22; 42; 43]. Nevertheless, how to leverage the idea of IB on graph anomaly detection tasks is still an open problem.

**Graph-level Anomaly Detection (GLAD).** GLAD aims to recognize anomalous graphs from a set of graphs by learning an anomaly score for each graph sample to indicate its degree of abnormality [4; 7; 8; 9]. Recent studies try to address the GLAD problem with various advanced techniques, such as knowledge distillation , one-class classification , transformation learning ,and deep graph kernel . However, these methods can only learn the anomaly score but fail to provide explanations, i.e., the graph rationale causing the abnormality, for their predictions.

**Problem Formulation.** Based on this mainstream unsupervised GLAD paradigm [4; 7; 8], in this paper, we present a novel research problem termed _explainable GLAD_, where the GLAD model is expected to provide the anomaly score as well as the explanations of such a prediction for each testing graph sample. Formally, the proposed research problem can be formulated by:

**Definition 2.1** (Explainable graph-level anomaly detection).: Given the training set \(_{tr}\) that contains a number of normal graphs, we aim at learning an explainable GLAD model \(f:(,)\) that is able to predict the abnormality of a graph and provide corresponding explanations. In specific, given a graph \(G_{i}\) from the test set \(_{te}\) with normal and abnormal graphs, the model can generate an output pair \(f(G_{i})=(s_{i},G_{i}^{(es)})\), where \(s_{i}\) is the anomaly score that indicates the abnormality degree of \(G_{i}\), and \(G_{i}^{(es)}\) is the subgraph of \(G_{i}\) that explains why \(G_{i}\) is identified as a normal/abnormal sample.

## 3 Methodology

This section details the proposed model SIGNET for explainable GLAD. Firstly, we derive a multi-view subgraph information bottleneck (MSIB) framework (Sec. 3.1) that allows us to identify anomalies with causal interpretations provided. Then, we provide the instantiation of the components in MSIB framework, including view construction (Sec. 3.2), bottle subgraph extraction (Sec. 3.3), and cross-view mutual information (MI) maximization (Sec. 3.4), which compose the SIGNET model. Finally, we introduce the self-interpretable GLAD inference (Sec. 3.5) of SIGNET. The overall learning pipeline of SIGNET is demonstrated in Fig. 2(a).

### MSIB Framework Overview

To achieve the goal of self-interpretable GLAD, an unsupervised learning approach that can jointly predict the abnormality of graphs and yield corresponding explanations is required. Inspired by the concept of information bottleneck (IB) and graph multi-view learning [36; 37; 45], we propose multi-view subgraph information bottleneck (MSIB), a self-interpretable and self-supervised learning framework for GLAD. The learning objective of MSIB is to optimize the "bottleneck subgraphs", the vital substructure on two distinct views of a graph, by maximizing the predictive structural information shared by both graph views while minimizing the superfluous information that is irrelevant to the cross-view agreement. Such an objective can be optimized in a self-supervised manner, without the guidance of ground-truth labels. Due to the observation that latent anomaly patterns of graphs can be effectively captured by multi-view learning [8; 9], we can directly use the cross-view agreement, i.e., the MI between two views, to evaluate the abnormality of a graph sample. Simultaneously, the extracted bottleneck subgraphs provide us with graph rationales to explain the anomaly detection predictions, since they contain the most compact substructure sourced from the original data and the most discriminative knowledge for the predicted abnormality, i.e., the estimated cross-view MI.

Formally, in the proposed MSIB framework, we assume each graph sample \(G\) has two different and distinguishable views \(G^{1}\) and \(G^{2}\). Then, taking view \(G^{1}\) as an example, the target of MSIB is to learn a bottleneck subgraph \(G^{1(s)}\) for \(G^{1}\) by optimizing the following objective:

\[_{G^{1(s)}}I(G^{2};G^{1(s)})-_{1}I(G^{1};G^{1(s)}|G^{2}).\] (2)

Similar to MIB (Eq. (1), the optimization for \(G^{2(s)}\), the bottleneck subgraph of \(G^{2}\), can be written in the same format. Then, by parameterizing bottleneck subgraph extraction and unifying the domain of bottleneck subgraphs, the objective can be transferred to minimize a tractable loss function:

\[_{MSIB}=-I(G^{1(s)};G^{2(s)})+ D_{SKL}(p_{}(G^{1(s)} |G^{1})\|p_{}(G^{2(s)}|G^{2})),\] (3)

where \(p_{}(G^{1(s)}|G^{1})\) and \(p_{}(G^{2(s)}|G^{2})\) are the bottleneck subgraph extractors (parameterized by \(\) and \(\)) for \(G^{1}\) and \(G^{2}\) respectively, \(D_{SKL}()\) is the symmetrized Kullback-Leibler (SKL) divergence, and \(\) is a trade-off hyper-parameter. Detailed deductions from Eq. (2) to Eq. (3) are in Appendix D.

MSIB framework can guide us to build a self-interpretable GLAD model. The first term in Eq. (3) tries to maximize the MI between the bottleneck subgraphs from two views, which not only prompts the model to capture vital subgraphs but also helps capture the cross-view matching patterns for anomaly detection. The second term in Eq. (3) is a regularization term to align the extractors, which ensures the compactness of bottleneck subgraphs. During inference, \(-I(G^{1(s)};G^{2(s)})\) can be regarded as a measure of abnormality. Meanwhile, the bottleneck subgraphs extracted by \(p_{}\) and \(p_{}\) can serve as explanations. In the following subsections, we take SIGNET as a practical implementation of MSIB framework. We illustrate the instantiations of view construction (\(G^{1}\) and \(G^{2}\)), subgraph extractors (\(p_{}\) and \(p_{}\)), MI estimation (\(I(G^{1(s)};G^{2(s)})\)), and explainable GLAD inference, respectively.

### Dual Hypergraph-based View Construction

To implement MSIB framework for GLAD, the first step is to construct two different and distinguishable views \(G^{1}\) and \(G^{2}\) for each sample \(G\). In multi-view learning approaches [37; 46; 47; 48], a general strategy is using stochastic perturbation-based data augmentation (e.g., edge modification  and feature perturbation ) to create multiple views. Despite their success in graph representation learning [46; 47; 48], we claim that perturbation-based view constructions are not appropriate in SIGNET for the following reasons. 1) Low sensitivity to anomalies. Due to the similarity of normal and anomalous graphs in real-world data, perturbations may create anomaly-like data from normal data as the augmented view . In this case, maximizing the cross-view MI would result in reduced sensitivity of the model towards distinguishing between normal and abnormal data, hindering the performance of anomaly detection . 2) Less differentiation. In the principle of MIB, two views should be distinguishable and mutually redundant . However, the views created by graph perturbation from the same sample can be similar to each other, which violates the assumption of our basic framework. 3) Harmful instability. The MI \(I(G^{1(s)};G^{2(s)})\) for abnormality measurement is highly related to the contents of two views. Nevertheless, the view contents generated by stochastic perturbation can be quite unstable due to the randomness, leading to inaccurate estimation of abnormality.

Considering the above limitations, a perturbation-free, distinct, and stable strategy is required for view construction. To this end, we utilize _dual hypergraph transformation_ (DHT)  to construct the opposite view of the original graph. Concretely, for a graph sample \(G\), we define the first view as itself (i.e., \(G^{1}=G\)) and the second view as its dual hypergraph \(G^{*}\) (i.e., \(G^{2}=G^{*}\)). Based on the hypergraph duality [52; 53], dual hypergraph can be acquired from the original simple graph with DHT: each edge of the original graph is transformed into a node of the dual hypergraph, and each node of the original graph is transformed into a hyperedge of the dual hypergraph . As the example shown in Fig. 2(b), the structural roles of nodes and edges are interchanged by DHT, and the incidence matrix \(^{*}\) of the dual hypergraph is the transpose of the incidence matrix of the original graph. To initialize the node features \(^{*}^{m d_{f}^{*}}\) of \(G^{*}\), we can either use the original edge features (if available), or construct edge-level features from the original node features or according to edge-level geometric property.

The DHT-based view construction brings several advantages. Firstly, the dual hypergraph has significantly distinct contents from the original view, which caters to the needs for differentiation in

Figure 2: (a) The overall pipeline of the proposed model SIGNET, consisting of (i) view construction, (ii) bottleneck subgraph extraction, and (iii) cross-view MI maximization. (b) An illustration of dual hypergraph transformation (DHT), where the nodes () and edges (â€”) in the original graph correspond to hyperedges (\(\)) and nodes () in its dual hypergraph, respectively.

MIB. Secondly, the dual hypergraph pays more attention to the edge-level information, encouraging the model to capture not only node-level but also edge-level anomaly patterns. Thirdly, DHT is a bijective mapping between two views, avoiding confusion between normal and abnormal samples. Fourthly, DHT is randomness-free, ensuring the stable estimation of MI.

### Bottleneck Subgraph Extraction

In MSIB framework, bottleneck subgraph extraction is a key component that learns to refine the core rationale for abnormality explanations. Following the procedure of MSIB, we need to establish two bottleneck subgraph extractors for the original view \(G\) and dual hypergraph view \(G^{*}\) respectively. To model the discrete subgraph extraction process in a differentiable manner with neural networks, following previous methods [14; 17; 22], we introduce continuous relaxation into the subgraph extractors. Specifically, for the original view \(G\), we model the subgraph extractor with \(p_{}(G^{(s)}|G)=_{v}p_{}(v^{(s)}|G)\). In practice, the GNN-based extractor takes \(G\) as input and outputs a node probability vector \(^{n 1}\), where each entry indicates the probability that the corresponding node belongs to \(G^{(s)}\). Similarly, for the dual hypergraph view \(G^{*}\), an edge-centric HGNN serves as the subgraph extractor \(p_{}\). It takes \(G^{*}\) as input and outputs an edge probability vector \(^{*}^{m 1}\) that indicates if the dual nodes (corresponding to the edges in the original graphs) belong to \(G^{*(s)}\). Once the probability vectors are calculated, the subgraph extraction can be executed by:

\[G^{(s)}=(,^{(s)})=(,),  G^{*(s)}=(^{*},^{*(s)})=(^{*},^ {*}^{*}),\] (4)

where \(\) is the row-wise production. Then, to implement the second term in Eq. (3), we can lift the node probabilities \(\) to edge probabilities by \(^{}\) by \(^{}_{i(e_{ij})}=_{i}_{j}\), where \((e_{ij})\) is the index of edge connecting node \(v_{i}\) and \(v_{j}\). After re-probabilizing \(^{}\), the SKL divergence between \(^{}\) and \(^{*}\) can be computed as the regularization term in MSIB framework.

Although the above "two-extractor" design correlates to the theoretical framework of MSIB, in practice, it is non-trivial to ensure the consistency of two generated subgraphs only with an SKL divergence loss. The main reason is that the input and architectures of two extractors are quite different, leading to the difficulty in output alignment. However, the consistency of two bottleneck subgraphs not only guarantees the informativeness of cross-view MI for abnormality measurement, but also affects the quality of explanations. Considering the significance of preserving consistency, we use a single extractor to generate bottleneck subgraphs for two views. In specific, the bottleneck subgraph extractor first takes the original graph \(G\) as its input and outputs the node probability vector \(\) for the bottleneck subgraph extraction of \(G^{(s)}\). Then, leveraging the node-edge correspondence in DHT, we can directly lift the node probabilities to edge probability vector \(^{*}\) via \(^{*}_{(e_{ij})}=_{i}_{j}\) and re-probabilization operation. \(^{*}\) can be used to extract subgraph for the dual hypergraph view. In this way, the generated bottleneck subgraphs in two views can be highly correlated, enhancing the quality of GLAD prediction (MI) and its explanations. Meanwhile, such a "single-extractor" design further simplifies the model architecture by removing extra extractor and loss function (i.e., the \(D_{SKL}\) term in Eq. (3)), reducing the model complexity. Empirical comparison in Sec. 4.4 also validates the effectiveness of this design.

### Cross-view MI Maximization

After bottleneck subgraph extraction, the next step is to maximize the MI \(I(G^{(s)};G^{*(s)})\) between the bottleneck subgraphs from two views. The estimated MI, in the testing phase, can be used to evaluate the graph-level abnormality. Owing to the discrete and complex nature of graph-structured data, it is difficult to directly estimate the MI between two subgraphs. Alternatively, a feasible solution is to obtain compact representations for two subgraphs, and then, calculate the representation-level MI as a substitute. In SIGNET, we use message passing-based GNN and HGNN with pooling layer (formulated in Appendix C) to learn the subgraph representations \(_{G^{(s)}}\) and \(_{G^{*(s)}}\) for \(G^{(s)}\) and \(G^{*(s)}\), respectively. In this case, \(I(G^{(s)};G^{*(s)})\) can be transferred into a tractable term, i.e., the MI between subgraph representations \(I(_{G^{(s)}};_{G^{*(s)}})\).

After that, the MI term \(I(_{G^{(s)}};_{G^{*(s)}})\) can be maximized by using sample-based differentiable MI lower bounds , such as Jensen-Shannon (JS) estimator , Donsker-Varadhan (DV) estimator , and Info-NCE estimator . Due to its strong robustness and generalization ability [37; 57], we employ Info-NCE for MI estimation in SIGNET. Specifically, given a batch of graph samples \(=\{G_{1},,G_{B}\}\), the training loss of SIGNET can be written by:

\[=-|}_{G_{i} }I(_{G_{i}^{(s)}};_{G_{i}^{(s)}})& =-|}_{G_{i}}(( _{G_{i}^{(s)}},_{G_{i}^{(s)}})+(_{G_{i}^{(s )}},_{G_{i}^{(s)}})),\\ (_{G_{i}^{(s)}},_{G_{i}^{(s)}})& =f_{k}(_{G_{i}^{(s)}},_{G_{i}^{(s)}})/ }{_{G_{j} G_{i}}expf_{k}( _{G_{i}^{(s)}},_{G_{j}^{(s)}})/},\] (5)

where \(f_{k}(,)\) is the cosine similarity function, \(\) is the temperature hyper-parameter, and \((_{G_{i}^{(s)}},_{G_{i}^{(s)}})\) is calculated following \((_{G_{i}^{(s)}},_{G_{i}^{(s)}})\).

### Self-Interpretable GLAD Inference

In this subsection, we introduce the self-interpretable GLAD inference protocol with SIGNET (marked in red in Fig. 2(a)) that is composed of two parts: anomaly scoring and explanation.

**Anomaly scoring.** By minimizing Eq. (5) on training data, the cross-view matching patterns of normal samples are well captured, leading to a higher MI for normal data; on the contrary, the anomalies with anomalous attributal and structural characteristics tend to violate the matching patterns, resulting in their lower cross-view MI in our model. Leveraging this property, during inference, the negative of MI can indicate the abnormality of testing data. For a testing sample \(G_{i}\), its anomaly score \(s_{i}\) can be calculated by \(s_{i}=-I(_{G_{i}^{(s)}};_{G_{i}^{(s)}})\), where the MI is estimated by Info-NCE.

**Explanation.** In SIGNET, the bottleneck subgraph extractor is able to pinpoint the key substructure of the input graph under the guidance of MSIB framework. The learned bottleneck subgraphs are the most discriminative components of graph samples and are highly related to the anomaly scores. Therefore, we can directly regard the bottleneck subgraphs as the explanations of anomaly detection results. In specific, the node probabilities \(\) and edge probabilities \(^{*}\) can indicate the significance of nodes and edges, respectively. In practical inference, we can pick the nodes/edges with top-k probabilities or use a threshold-based strategy to acquire a fixed-size explanation subgraph \(G^{(es)}\).

More discussion about methodology, including the pseudo-code algorithm of SIGNET, the comparison between SIGNET and existing method, and the complexity analysis of SIGNET, is illustrated in Appendix E.

## 4 Experiments

In this section, extensive experiments are conducted to answer three research questions:

* **RQ1:** Can SIGNET provide informative explanations for the detection results?
* **RQ2:** How effective is SIGNET on identifying anomalous graph samples?
* **RQ3:** What are the contributions of the core designs in SIGNET model?

### Experimental Setup

**Datasets.** For the explainable GLAD task, we introduce 6 datasets with ground-truth explanations, including three synthetic datasets and three real-world datasets. Details are demonstrated below. We also verify the anomaly detection performance of SIGNET on 10 TU datasets , following the setting in . Detailed statistics and visualization of datasets are demonstrated in Appendix F.1.

* **BM-MT, BM-MN, and BM-MS** are three synthetic dataset created by following [13; 19]. Each graph is composed of one base (Tree, Ladder, or Wheel) and one or more motifs that decide the abnormality of the graph. For BM-MT (motif type), each normal graph has a house motif and each anomaly has a 5-cycle motif. For BM-MN (motif number), each normal graph has 1 or 2 house motifs and each anomaly has 3 or 4 house motifs. For BM-MS (motif size), each normal graph has a cycle motif with 3-5 nodes and each anomaly has a cycle motif with 6-9 nodes. The ground-truth explanations are defined as the nodes/edges within motifs.

* **MNIST-0 and MNIST-1** are two GLAD datasets derived from MNIST-75sp superpixel dataset . Following , we consider a specific class (i.e., digit 0 or 1) as the normal class, and regard the samples belonging to other classes as anomalies. The ground-truth explanations are the nodes/edges with nonzero pixel values.
* **MUTAG** is a molecular property prediction dataset . We set nonmutagenic molecules as normal samples and mutagenic molecules as anomalies. Following , -NO2 and -NH2 in mutagenic molecules are viewed as ground-truth explanations.

**Baselines**. Considering their competitive performance, we consider three state-of-the-art deep GLAD methods, i.e., OCGIN , GLocalKD , and OCGTL , as baselines. To provide explanations for them, we integrate two mainstream post-hoc GNN explainers, i.e., GNNExplainer  (GE for short) and PGExplainer  (PG for short) into the deep GLAD methods. For GLAD tasks, we further introduce the baselines composed of a graph kernel (i.e., Weisfeiler-Lehman kernel (WL)  or Propagation kernel (PK) ) and a detector (i.e., iForest (iF)  or one-class SVM (OCSVM) ).

**Metrics and Implementation.** For interpretation evaluation, we report explanation ROC-AUCs at node level (NX-AUC) and edge level (EX-AUC) respectively, similar to [19; 20]. For GLAD performance, we report the ROC-AUC w.r.t. anomaly scores and labels (AD-AUC) . We repeat 5 times for all experiments and record the average performance. In SIGNET, we use GIN  and Hyper-Conv  as the GNN and HGNN encoders. The bottleneck subgraph extractor is selected from GIN  and MLP. We perform grid search to pick the key hyper-parameters in SIGNET and baselines. More details of implementation and infrastructures are in Appendix F. Our code is available at https://github.com/yixinliu233/SIGNET.

### Explainability Results (RQ1)

**Quantitative evaluation.** In Table 1, we report the node-level and edge-level explanation AUC  on 6 datasets. Note that PGExplainer  can only provide edge-level explanations natively. We have the following observations: 1) _SIGNET achieves SOTA performance in almost all scenarios._ Compared to the best baselines, the average performance gains of SIGNET are \(27.89\%\) in NX-AUC and \(8.99\%\) in EX-AUC. The superior performance verifies the significance of learning to interpret and detect with a unified model. 2) _The post-hoc explainers are not compatible with all GLAD models_. For instance, PGExplainer works relevantly well with GLocalKD but cannot provide informative explanations for OCGIN. The GNNExplainer, unfortunately, exhibits poor performance in most scenarios. 3) _SIGNET has larger performance gains on real-world datasets_, which illustrates the potential of SIGNET in explaining real-world GLAD tasks. 4) Despite its superior performance, _the stability of SIGNET is relevantly average_. Especially on the synthetic datasets, we can find that the standard deviations of

  
**Dataset** & **Metric** & **OCGIN-FG** & **GLocalKD-GE** & **OCGIL-EG** & **OCGIN-PG** & **GLocalKD-PG** & **OCGTL-PG** & **SIGNET** \\   & _NX-AUC_ & 48.26\(\)1.38 & 49.67\(\)0.58 & 45.75\(\)2.53 & - & - & 78.41\(\)6.88 \\  & _EX-AUC_ & 520.43\(\)3.28 & 49.11\(\)2.77 & 49.80\(\)2.88 & 64.08\(\)12.23 & 74.59\(\)57.66 & 72.72\(\)10.19 & 77.69\(\)13.14 \\   & _NX-AUC_ & 46.25\(\)4.60 & 49.10\(\)0.71 & 40.53\(\)3.18 & - & - & 76.57\(\)6.62 \\  & _EX-AUC_ & 60.20\(\)2.90 & 50.71\(\)3.14 & 56.34\(\)3.10 & 50.18\(\)7.88 & 76.85\(\)3.53 & 74.36\(\)12.78 & 83.45\(\)3.93 \\   & _NX-AUC_ & 52.43\(\)1.79 & 50.43\(\)0.62 & 54.31\(\)1.55 & - & - & - & 76.45\(\)4.81 \\  & _EX-AUC_ & 54.31\(\)9.61 & 49.10\(\)2.99 & 68.87\(\)1.44 & 43.67\(\)12.60 & 82.53\(\)18.56 & 77.45\(\)30.70 & 70.45\(\)5.07 \\   & _NX-AUC_ & 49.48\(\)0.58 & 50.11\(\)0.50 & 38.87\(\)3.21 & - & - & 70.83\(\)5.64 \\  & _EX-AUC_ & 50.55\(\)4.77 & 49.55\(\)0.45 & 41.42\(\)2.49 & 39.55\(\)15.31 & 54.69\(\)17.99 & 59.25\(\)4.60 & 72.78\(\)7.25 \\   & _NX-AUC_ & 48.21\(\)2.01 & 49.50\(\)0.50 & 47.04\(\)1.66 & - & - & - & 68.44\(\)4.37 \\  & _EX-AUC_ & 48.60\(\)3.28 & 49.78\(\)0.26 & 45.24\(\)1.11 & 47.98\(\)2.44 & 49.24\(\)1.56 & 57.93\(\)5.51 & 74.83\(\)5.24 \\   & _NX-AUC_ & 48.99\(\)1.50 & 49.70\(\)1.19 & 49.31\(\)0.44 & - & - & - & 75.60\(\)5.94 \\  & _EX-AUC_ & 51.29\(\)0.65 & 47.65\(\)1.19 & 45.80\(\)2.81 & 46.22\(\)7.90 & 70.47\(\)15.26 & 65.03\(\)16.90 & 78.05\(\)9.19 \\   

Table 1: Explanation performance in terms of _NX-AUC_ and _EX-AUC_ (in percent, mean \(\) std). The best and runner-up results are highlighted with bold and underline, respectively.

Figure 3: Visualization of explanation results w.r.t. node and edge probabilities.

[MISSING_PAGE_FAIL:9]

the original SIGNET with one extractor. Meanwhile, we can witness that compared to JS  and DV  MI estimators, Info-NCE estimator can lead to superior performance, especially for anomaly detection.

## 5 Conclusion

This paper presents a novel and practical research problem, explainable graph-level anomaly detection (GLAD). Based on the information bottleneck principle, we deduce the framework multi-view subgraph information bottleneck (MSIB) to address the explainable GLAD problem. We develop a new method termed SIGNET by instantiating MSIB framework with advanced neural modules. Extensive experiments verify the effectiveness of SIGNET in identifying anomalies and providing explanations. A limitation of our paper is that we mainly focus on purely unsupervised GLAD scenarios where ground-truth labels are entirely unavailable. As a result, for few-shot or semi-supervised GLAD scenarios  where a few labels are accessible, SIGNET cannot directly leverage them for model training and self-interpretation. We leave the exploration of supervised/semi-supervised self-interpretable GLAD problems in future works.