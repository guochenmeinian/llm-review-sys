# Identifying Selections

for Unsupervised Subtask Discovery

 Yiwen Qiu

Carnegie Mellon University

Pittsburgh, PA 15213

yiwenq@andrew.cmu.edu

&Yujia Zheng

Carnegie Mellon University

Pittsburgh, PA 15213

yujiazh2@andrew.cmu.edu

&Kun Zhang

Carnegie Mellon University, MBZUAI

Pittsburgh, PA 15213

kunzl@andrew.cmu.edu

Corresponding author.

###### Abstract

When solving long-horizon tasks, it is intriguing to decompose the high-level task into subtasks. Decomposing experiences into reusable subtasks can improve data efficiency, accelerate policy generalization, and in general provide promising solutions to multi-task reinforcement learning and imitation learning problems. However, the concept of subtasks is not sufficiently understood and modeled yet, and existing works often overlook the true structure of the data generation process: subtasks are the results of a _selection_ mechanism on actions, rather than possible underlying confounders or intermediates. Specifically, we provide a theory to identify, and experiments to verify the existence of selection variables in such data. These selections serve as subgoals that indicate subtasks and guide policy. In light of this idea, we develop a sequential non-negative matrix factorization (seq-NMF) method to learn these subgoals and extract meaningful behavior patterns as subtasks. Our empirical results on a challenging Kitchen environment demonstrate that the learned subtasks effectively enhance the generalization to new tasks in multi-task imitation learning scenarios. The codes are provided at this link.

## 1 Introduction

Being able to reuse learned skills from past experiences and conduct hierarchical planning (LeCun (2022); Hafner et al. (2022); Gehring et al. (2021)) is crucial for tackling real-world challenging tasks such as driving: it is meaningful to let higher levels of abstractions perform longer-term prediction (of subgoals), while lower levels of policy perform shorter-term actions. The concept of breaking down a task into subtasks1 can be illustrated through the example of commuting to New York. You are at home, and the overall task, commuting to New York, can be decomposed into smaller, manageable subtasks. This includes subtasks like walking out of the house, getting into the car, driving and catching an airplane. Even more granular, each of these subtasks can be further broken down into smaller actions: walking out of the house involves standing up, grabbing the luggage and walking to the door. This method of decomposing a task into subtasks fits our intuition of how humans perform actions and helps to simplify complex tasks, making them more manageable.

For artificial intelligence (AI) to match the ability of humans in terms of understanding the event structures (Zacks et al. (2001), Baldassano et al. (2017)) and learning to perform complex and long-horizon tasks by reinforcement learning (RL) (Sutton and Barto (2018)) or imitation learning (IL) (Hussein et al. (2017)), it is natural to ask this question: with an abundance of past collected either human or robotics experiences, how can one extract reusable subtasks, such that we can use them to solve future unseen new complex tasks? Current RL is well known for its sample inefficiency, and learning decomposition of subtasks serves as the basis for perform complex tasks via planning. The benefit is straightforward: extracting reusable disentangled (Denil et al. (2017)) temporal-extended common structure can enhance data efficiency and accelerate learning of new tasks (Thrun and O'Sullivan (1996), Florensa et al. (2017), Griffiths et al. (2019), Jing et al. (2021)). This is the main motivation for subtask discovery, the problem we aim to thoroughly investigate in this paper.

In subtask discovery, the criterion to segment these subtasks is vital, and should be consistent with the true generating process. However, most prior works did not explore the ground-truth generative structure to identify that criterion-instead, they simply focus on designing heuristic algorithms (McGovern and Barto (2001), Simsek and Barto (2004), Wang et al. (2014), Paul et al. (2019)) or maximizing the likelihood of the data sequences that is built upon intuitive graphical models (Krishnan et al. (2017), Kipf et al. (2019), Sharma et al. (2019), Yu et al. (2019)). Relying on a structure that conflicts with the true generating process may mislead the segmentation as well as worsen the performance of those downstream IL tasks. Thus, we argue that it is helpful to consider the causal structure underlying the sequence, specifically, the _selection_ variables as subgoals to discover subtasks.

Selective inclusion of data points is prevalent in practical scenarios. We provide a short illustration of selection in Fig.1 to help distinguish the confounder and the selection case. We are at home, and have two subgoals to select from: "picnicking" and "movie", and go back home again. One way to look at selection is recognizing its preferential nature of including certain data (Heckman (1979)): achieving a subgoal involves performing a subtask, which consists of a sequence of states and actions that must follow a unique pattern. Another way to look at selection is by considering interventions (Eberhardt and Scheines (2007)) which is a widely used concept in the literature on causal discovery. Intervention involves assigning values to a single variable: in our case, the actions lead us to achieve a subgoal, and intervening on the actions would lead us to achieve others. In other words, the subgoal functions as a _selection_ for the actions. Overlooking the selection structure is adopting an inappropriate inductive bias, and it can distort our understanding of the data. Therefore, the incorporation of identifying and modeling the selection structure to uncover the true causal process is crucial for understanding subtasks. A more comprehensive literature review on recent advances in understanding selections and subtask discovery can be found in Appx. A.

The main contributions of this paper are as follows: (1) We show that one can identify the selection structure without interventional experiments, and confirm its existence through our experiments. (2) According to the identified structure, and based on the formal definition of subtasks, we propose a novel sequential non-negative matrix factorization (seq-NMF) method to learn subgoals as selections. (3) We demonstrate that the learned subtasks can be leveraged to improve the performance of imitation learning in solving new tasks.

## 2 Preliminary

Imitation Learning.In standard Imitation Learning, we collected a set of trajectories \(=\{_{n}\}_{n=1}^{N}\) from an expert in a Markov Decision Process (MDP). An MDP is defined by \(,,,,_{0},\), with \(\) as the state space, \(\) as the action space, \(:\) as the transition probability, \(:\) as the reward function, \(_{0}\) as the initial state distribution, and \(\) as the discount factor.

Figure 1: Example of subgoals as selections. One subgoal is to "go picinicking", another subgoal is to "go to a movie". In order to “go picinicking”, you need to go shopping first and then drive to the park; in order to “go to a movie”, you need to check the movie information online first and then get the tickets. The actions _caused_ us to accomplish the subtasks, and we essentially select the actions based on (conditioned on) the subgoals we want to achieve. On the contrary, weather is a confounder of the states and actions: changing our actions would not influence the weather, but actions influence whether we can achieve the subgoals.

Suppose we are given \(N\) expert trajectories \(_{n}\) generated in an MDP with unknown reward. Each trajectory \(_{n}=\{s_{t},a_{t},\}_{t=1}^{T^{n}}\) is composed of a series of states \(s_{t}\) and actions \(a_{t}\). The goal of the agent is to learn a policy \((a_{t} s_{t}):\) that mimics the expert's behavior.

Option Framework.For multi-task settings, learning a hierarchical policy that extracts basic skills has been proven useful in solving tasks composed of multiple subtasks. Sutton et al. (1999) proposed the option framework, which is a hierarchical reinforcement learning framework that decomposes a task into a set of temporally extended options. An option \(\) is defined as a tuple \(,,\), where \(\) is the initiation set, \(\) is the policy, and \(:\) is the termination function deciding whether the current option terminates. The execution of a policy within an option is a semi-Markov decision process (SMDP). By sequentially performing SMDPs, that is, taking an option \(_{a}\) until it terminates, and then taking the next option \(_{b}\) until it terminates, the agent can learn a hierarchical policy that executes in the overall MDP.

Causal Graph and Common Assumptions.In a Bayesian network, the distribution \(\) over a set of variables is assumed to be _Markov_ w.r.t. to a directed acyclic graph (DAG) \(=\{,\}\) where \(\) is the set of vertices and \(\) is the set of edges, and the DAG \(\) is _faithful_ to the data. The Markov condition, and faithfulness assumption are defined in Def. 1 and Def. 2, respectively.

**Definition 1**.: (Markov Condition (Spirtes et al. (2001), Pearl (2009))) Given a DAG \(\) and distribution \(\) over the variable set \(\), every variable X in \(\) is probabilistically independent of its non-descendants given its parents in \(\).

**Definition 2**.: (Faithfulness Assumption (Spirtes et al. (2001), Pearl (2009))) There are no independencies between variables that are not entailed by the Markov Condition.

Combining the Markov Condition and Faithfulness Assumption, we can use _d-separation_ as a criterion (denoted as \(_{d}\)) to read all the conditional independencies from a given DAG \(\):

**Definition 3**.: (d-separation (Spirtes et al. (2001), Pearl (2009))) Two sets of nodes \(\) and \(\) in \(\) is said to be _d-separated_ by a set of nodes \(\) if and only if: for every path \(p\) that connects one node \(i\) in \(\) to one node \(j\) in \(\), either (1) \(p\) contains \(i m j\) or \(i m j\) such that \(m\) is in \(\), or (2) \(p\) contains a collider \(m\), i.e. \(i m j\) such that \(m\) and all descendants of \(m\) are not in \(\).

Problem FormulationGiven the above context, we formulate the considered imitation learning problem as follows: we have a distribution of tasks \(_{e}()\) and a corresponding set of expert trajectories \(=\{_{n}\}_{n=1}^{N}\), and we aim to let the imitater learn a policy that can be transferred to new tasks that follow a different distribution \(_{i}()\). Each task sampled from \(_{}()\) should be generated by a MDP and composed of a sequence of option \(\{_{j},\}\), where \(_{j}=_{j},_{j},_{j}_{j}\). We use \(=_{j=1}^{J}_{j}\) to denote all J options, and \(_{p}=\{},},...\}_{t=1}^{ L}\) as a sub-sequence of states and actions \((},})\) from any trajectory \(_{n}\). Each trajectory can be partitioned into sub-sequences \(_{-}p\) with maximum length \(\). Unlike traditional MIL (Seyed Ghasemipour et al. (2019), Yu et al. (2019)), we assume a shift in the task distribution, i.e. \(_{i}()_{e}()\), but they only share the same option set, and we expect the agent to leverage the past learned subtasks to carry out new tasks efficiently.

Roadmap for solving the "subtask discovery" problemThe question raised above on how to discover useful subtasks for policy generalization on new tasks can be answered two-fold. First, it is essential to adopt an accurate understanding of subtasks that aligns with a true data generation process: it is a matter of comprehending temporal data dynamics (Matsubara et al. (2014), Chen et al. (2018), Dyn), to which we provide an answer in Sec. 3.1. Second, based on that understanding, we need to design a learning algorithm that can extract subtasks from expert demonstrations, as discussed in Sec. 3.2. Finally, the learned subtasks should be used to facilitate policy training so that the policy can quickly adapt to new tasks by alternating between subtasks, which we address in Sec. 4.

## 3 Subgoal as Selection

OverviewThe first step is to understand _what_ a subtask is. We propose to understand subtask by building the causal graph and distinguishing different potential structures. We assert that a subtask is indicated by a selection variable denoted as \(}\) (subgoal), and will distinguish it from confounder and intermediate node (Sec. 3.1). Then, we give a formal definition of subtasks as sub-sequences that can be representative of common behavior patterns and avoid uncertainties in policy (Sec. 3.2). With the understanding of selection (Sec. 3.1) and formal definition (Sec. 3.2), we propose a novel sequential non-negative matrix factorization (seq-NMF) method that aligns with these two ideas to learn subtasks from multi-task expert demonstrations (Sec. 3.3).

### Identifying Selections in Data

To better understand subtasks, we build a DAG \(=\{,\}\) to represent the generation process of a trajectory by setting each \(}\), \(}\) as vertices (\(\)), and add edges (\(\)) by connecting \(}},}}\) to represent the transition function \(\). There are three potential patterns as follows:

**Definition 4**.: (Confounder, selection and intermediate node) \(}\) is a _confounder_ if \(}}}\), \(}\) is a _selection_ if \(}}}\), \(}\) is an _intermediate node_ if \(}}}\).

In other words, the causal Dynamic Bayesian Network (DBN) (Murphy (2002)) of a series of \(\{},},\}\) is one of the following three scenarios:

Selection implies that we can only observe the data points for which the selection criterion is met, such as reaching some subgoal \(j\), e.g. \(}^{(j)}=1\). As a consequence, the trajectory distribution \(p(},})\) is actually the conditional one: \(p(},}})\). Understanding subgoals as selections that are always _conditioned on_ enables us to explain the relationship between the states and actions: the dependency between \(}\) and \(}\) is the result of the given selection, i.e. \(}}}\). We argue that the policy \((}})\) does not indicate a direct causal relation between states and action, but rather an inference from states to actions, and the dependency is built by subgoal as selections.

We start by proposing a conditional independence test (CI test) based condition in Prop. 1 as a _sufficient_ condition for recognizing selections:

**Proposition 1**.: (Sufficient condition) Assuming that the graphical representation is Markov and faithful to the measured data, if \(}}}\), then \(}\) is a selection variable, i.e., \(}}\), under the assumption that:

1. (confounder, selection, and intermediate nodes can not co-exist) At each time step, \(}\) can only be one of \(\{},},}\}\). (For a relaxation of this assumption, see Appx. B.4).
2. (consistency in a time series) At every time step, \(}\) plays the same role as one of \(\{},},}\}\).

For a _necessary and sufficient_ condition, we have the following proposition:

**Proposition 2**.: (Necessary and sufficient condition) \(}\) is a selection variable (\(}}\)) if and only if \(}}}\) and \(}}}\).

Since our goal is to identify the presence of selection, the necessity aspect is not our primary focus, but it is still worthwhile to consider. For a condition that is weaker but has real-world implications, we have the following _necessary_ condition:

**Proposition 3**.: (Necessary condition) If \(}\) is a selection variable (\(}}\)), then \(}}},}\). (Such independency does not hold true for confounders case which is discussed in Appx. B.3)

This proposition implies that \(s_{t+1}\) is only determined by \(},}\) and solely relies on the transition function in the environment. The hidden variable \(}\) would not provide much additional information for the prediction of \(}\). On the other hand, if we consider the confounder case, it is unrealistic that changing the subtask would result in changing the transition function, as the confounder case entails.

Figure 2: Three kinds of dependency patterns of DAGs that we aim to distinguish. Structure (1) models the confounder case \(}}}\), structure (2) models the selection case \(}}}\), and structure (3) models the mediator case \(}}}\). In all three scenarios, the solid black arrows (\(\)) indicate the transition function that is invariant across different tasks. The dashed arrows (\(\)) indicate dependencies between nodes \(}\) and \(}\). We take them to be direct adjacencies in the main paper, and for potentially higher-order dependencies, we refer to Appx. B.4.

Such is strong evidence for us to assert that subtasks should be considered as selections. The proof of Prop. 1, Prop. 2 and Prop. 3 are provided in Appx. B.1, B.2. B.3, respectively.

**Remark 1**.: As a relaxation of Prop. 1, we do not assert \(},}\) and \(}\) to be mutually exclusive. As long as we have \(}}}\) and \(}}}\), then the selection mechanism holds. There can also be confounders and intermediate nodes, but for simplicity, we do not consider them in the main paper and leave the combination of multiple \(},}\) and \(}\) to Appx. B.4.

**Remark 2**.: There is a parallel between the selection variable and the reward signal in reinforcement learning from a probabilistic inference view (Levine (2018)). See discussion in Appx. E.

Based on Prop. 2, we verify that there is indeed selection in the trajectories, as indicated by experiments (Sec. 5.1). These selection variables serve as subgoals that can facilitate imitation learning.

### Definition of the Subtask

In Prop. 1, we provide the sufficient condition to identify selection (subgoal), that is \(}}}\). In other words, the current action \(}\) is affected by both the current state \(}\) and the current subgoal \(}\). Only \(}\) alone is not sufficient to determine action, but also the current subgoal \(}\) guides the agent's action. e.g. When arriving at a crossroad, and you are deciding whether to turn left or right, it is only when a subgoal (the left road or the right road) is selected, then the subgoal-conditioned policy \(_{g}(}},})\) is uniquely determined. Therefore, we can define subtasks as sub-sequences that can: (1) be representative of common behavior patterns (because \(}\) guided the selection of \(}\)) (2) avoid uncertainties in policy \((}})\), that is, different distributions of action predictions given a state. The formal definition goes as in Def. 5.

**Definition 5**.: _Subtask_ is defined as a set of \(J\) options \(=\{_{j}\}_{j=J}^{J}\), s.t. for some partition of trajectories, \(_{p}=\{},},...\}_{t=1}^{ L}\) as a sub-sequence of \((},})\) from any trajectory \(_{n}\) and \(L\) is the maximum length of the sub-sequence, and also the maximum lag of each subtask:

\[ J,\,\] (1) \[(_{p})\;(_{j})\;_{p} _{j}\;,\;\;(} _{p},_{p}(p p))\] \[\;_{j}(}}=})_{j^{}}(}}=}), \;}_{j},_{j^{}},j j ^{}\]

We require every sub-sequence to be generated from some option \(_{j}\), which means that its first state \(_{p}(0)_{j}\) and the last state is a termination state \(_{j}(_{p}(-1))=1\), and the actions in \(_{p}\) are generated by \(_{j}(}})\). Importantly, when there are multiple policies available at one state \(}\), i.e. \(_{j}(}}=})_{j^{}}( }}=})\), then these policies should correspond to different options, \(_{j}\) and \(_{j^{}}\) (\(j j^{}\)), in order to avoid unmodeled uncertainty for the imitator.

By definition, subtasks are our way of recovering a minimal number of options from trajectories, and we view sub-sequences in the data as instantiations of options, because they are generated from corresponding SMDPs. Then, each \(_{n}\) should be _some_ combination of those sub-sequences. On the one hand, we aim to avoid excessively granular partitions that result in a large number of one-step options, because it is essential to capture long-term patterns. On the other hand, we seek a sufficient number of subtasks to avoid policy ambiguity; when multiple options are available for a single state, we should be able to differentiate them by setting distinct subgoals. Consequently, in situations with varying distributions of action predictions, we employ different subtasks to capture these differences. This involves selecting subgoals to predict actions that a single policy cannot suffice. For example, determining to turn left at a crossroad as one subgoal and turning right as another.

**Justification of necessity** By this definition of subtasks, we reinforce the necessity of learning subtasks from the perspective of avoiding policy uncertainties in multiple trajectories. Since the trajectories are collected by a variety of human or robot experts under different tasks, they are likely to exhibit different optimal policies. If there is an ambiguity about which policy to imitate, i.e., there are multiple optimal action predictions at hand, we can comprehend it as there is a latent variable \(}\) affecting the prediction of \(}\), namely a subgoal that helps to select action. Learning one policy distribution \(p(}})\) from a mixture of different policies ignores the variety of behaviors, and focuses merely on the marginal policy \((}})\) rather than a subgoal conditioned policy \(_{g}(}},})\), leading to unsatisfactory results. To our knowledge, we are the first ones to give a definition that eliminates the ambiguity in the definition of subtasks. While previous works provide only general and vague definitions by human intuition (McGovern and Barto (2001), Simsek and Barto (2004), Wang et al. (2014), Paul et al. (2019), Krishnan et al. (2017), Kipf et al. (2019), Sharmaet al. (2019); Yu et al. (2019)), e.g. semantically meaningful sub-trajectories, we explicitly express subtasks as patterns that capture _all policy uncertainties_ exhibited in the dataset. These uncertainties are then mitigated by policy selections, i.e. the subgoal-conditioned policy.

### Learning Subtasks with Seq-NMF

In Sec. 3.1, we give the conditions to identify subgoals as selections (verified in experiments in Sec. 5.1). In Sec. 3.2, we provide a formal definition of subtasks. Combining these concepts, we connect subgoals to subtasks: _subgoals_ are multi-dimensional binary variables \(_{t}\{0,1\}^{J}\), while _subtasks_ are sets of options \(\) that generate sub-sequences \(_{p}=\{},},...\}_{t=1}^{ L}\). In particular, we use \(_{t}^{(j)}(_{p})=1(_{p}_{j})\) to indicate whether the subgoal of a sub-sequence is generated from an SMDP captured by \(_{j}\), where \(()\) is the identify function. Similarly, we use \(_{t}^{(j)}([},}])=(}_{j})\) to indicate whether \(}\) is an initial state of \(_{j}\).

This formulation can be intuitively understood as follows: subtasks represent the behavior patterns one might select to perform and are thus exhibited in the expert trajectories. In contrast, the subgoal is the selection variable itself, indicating whether or not this behavior pattern has been executed.

Method: Sequential Non-Negative Matrix FactorizationLearning such a binary coefficient matrix and feature pattern is closely related to the non-negative matrix factorization (NMF) (Lee and Seung (1999)), which focuses on decomposing a complex dataset into a set of simpler, interpretable components while maintaining non-negativity constraints. A thorough review of NMF is in Appx. G.

In our setting, instead of using a vector to represent a pattern, we want the pattern to be temporally extended, sharing the merit of those works of extensions of NMF (Smaragdis (2004, 2006); Mackevicius et al. (2019)). We set \(}=(};})\) and concatenate \(}\) across time to form a data matrix \(X\). We then transform the problem of learning subgoals into a matrix factorization problem: identifying the repeated patterns within sub-sequences. The entire data matrix (trajectories) can be reconstructed with _subtasks_ representing temporally extended patterns, and binary indicator representing which option is selected at each time step. We define \(=[_{1}&_{2}&& _{J}]^{D J L}\) as a three-dimensional tensor, with \(J\) subtask patterns \(_{j}^{D L}\), and \(=[_{1}&_{2}&&_{T}]\{0,1\}^{J T}\) as corresponding indicator binary matrix, where \(D=d_{s}+d_{a}\) is the dimension of \(}\), and \(L\) is the maximum length of each subtask pattern. We construct the decomposition as:

\[,( )_{dt}=_{j=1}^{J}_{=0}^{L-1}_{dj}_{j(t-)},\] (2)

and \(\) is a convolution operator that aggregates the patterns across time lag \(L\). Then the optimization problem is transformed into:

Figure 3: Figure (a) is the causal model for expert trajectories, which is further abstracted as the matrices in Figure (b), which can be learned by a seq-NMF algorithm. In both figures, data matrix \(X\) is the aggregated \(\{};}\}_{t=1}^{T}\), and \(\{0,1\}^{J T}\) represents the binary subgoal matrix.

\[(^{*},^{*}) =*{arg\,min}_{,}(\|}-\|_{F}^{2}+)\] (3) \[s.t. }_{dt} =_{j=1}^{J}_{=0}^{L-1}_{dj}_{ j(t-)}.\]

\(\|\|_{F}\) is the Frobenius norm, and \(\) is the regularizor. In order for it to fit our framework proposed in Sec. 3.1 and Sec. 3.2, we need three terms in the regularizor: \(=_{}+_{1}+_{}\) with corresponding learning rates \(_{}\), \(_{1}\) and \(_{}\).

The first term \(_{}\) corresponds to _the binary nature of selection_. Because the set \(\{0,1\}\) is not convex, we remove the constraint \(_{t}^{(j)}\{0,1\}\) to a regularizer \(_{}=_{}\|(1- )\|_{2}^{2}\), forcing the subgoal to be binary, where \(\) is the element-wise product. Because of _sparsity of subtasks_ in its definition, we require a minimal number of subgoals, which should be an L0 penalty term. We use the L1 penalty \(_{1}=_{1}\|\|_{1}\) to approximate such sparsity since solving the L0 regularized problem is NP-hard. Finally, we should have _distinct features of subtasks_: distinct common patterns should be distinguished as the same subtask, i.e. there should not be similar or duplicated patterns between any two different subtasks \(_{j}\) and \(_{j^{}}\). We use \(_{}=_{}\|( }{{*}})^ {}\|_{1,i j}\) to avoid such redundancy, where \(\) is a \(T T\) smoothing matrix: \(_{ij}=1\) when \(|\)\(i-j\)\(|<L\) and \(_{ij}=1\) otherwise. Specifically, the first term \((}{{*}})\) calculates the overlap of data \(X\) with subtask pattern \(j\) at each time step \(t\), where \((}{{*}})_{jt}= _{=0}^{L-1}_{d=1}^{D}_{dj}_{j(t+)}\) and \(}{{*}}\) is the transposed convolution operator. Then by multiplying the loadings \(^{}\) within the time shift of \(L\), we obtain the correlation between different patterns' overlapping with the data. If the correlation is high, it means that the two patterns have similar power in explaining the data at time \(t\). Diagonal entries are omitted. We provide the detailed discussion on subtasks ambiguity in Appx. F.

Optimization. In terms of optimization, we derive multiplication rules which have been proven to be more efficient in solving problems with non-negative constraints ( Lee and Seung (1999)), rather than relying on standard gradient descent. The detailed derivation is provided in Appx. G and the overall algorithm of seq-NMF is described in Appx. C.1.

\[_{..}_{..}* ^{}}{}*^{}+}{_{}}}, }{{*}}}{ *}+}{ }}\] (4)

## 4 Transfering to New Tasks

After learning subtasks from demonstrations, it is intuitive to utilize them by augmenting the action space with the subgoal selection. We learn a new policy that takes in both the state and subgoal as input, same as in other literature ( Sharma et al. (2019); Kipf et al. (2019); Jing et al. (2021); Jiang et al., Chen et al. (2023)).

Among all the different ways to perform IL, such as Behavioral Cloning (BC) (Bain and Sammut (1995)), Inverse Reinforcement Learning (IRL) (Abbeel and Ng (2004); Ng et al. (2000); Ziebart et al. (2008); Finn et al. (2016)), and Generative Adversarial Imitation Learning (GAIL) (Ho and Ermon (2016); Fu et al. (2018)), we adopt a GAIL-based approach that matches the occupancy measure between the learned policy and the demonstrations through adversarial learning to seek the optimal policy. The overall objective is:

\[_{_{g}}_{}_{(s_{},a_{t},g_{t})}(1-D_ {}(s_{t},a_{t},g_{t}))+_{(_{t},_{t},_{t})_{g}}(D_{}(_{t},_{t},_{t})),\] (5)

where \(_{g}\) is the augmented policy, and \(D_{}: J\) is a parametric discriminator that aims at distinguishing between the samples generated by the learned policy and the demonstrations. The policy is trained via PPO ( Schulman et al. (2017)). Algorithms for the overall training procedure and the execution of policy can be found in Appx. C.2. Comparison with other IL algorithms is provided in our experiment in Sec. 5.3.

Experiments

The goals of our experiments are closely tied to our formulation in Sec. 3. In Sec. 5.1, we verify the existence of selection in data. In Sec. 5.2, we evaluate the effectiveness of seq-NMF in recovering subgoals as selections. In Sec. 5.3, we demonstrate the learned subgoals and subtasks are transferable to new tasks by re-composition.

### Verifying Subgoals as Selections

We first verify the theory in Sec. 3 that selections can be identified in data by the following CI tests: (1) whether \(}}}\) holds, (2) whether \(}_{t+1}}\) holds, and (3) whether \(_{t+1}}_{t},_{t}\) holds. Our empirical results provide an affirmative answer to all these questions, suggesting that selections do exist, and they can serve as subgoals.

Synthetic Color DatasetWe follow the didactic experiment in Jiang et al. and construct color sequences similarly. The dataset consists of repeating patterns of repetitive color, either with \(3\) or \(10\) steps of time lag in each pattern, of which Fig. 4 is an illustration. Details about the construction and the CI test results are elaborated in Appx. D.1.

Driving DatasetIn the driving environment, there are two tasks to finish. As shown in Fig. 5, both cars start at the left end, either facing up or down. The first task is to drive to the right end following the yellow path, while the second task is to follow the blue path. Each state is represented by a tuple \((x,y,)^{3}\) (coordinates and orientation), and each action is the angular velocity \(\). We collected \(100\) trajectories in total, \(50\) for each task.

CI test resultsWe show our results in Tab. 1 for the Driving dataset. In short, the answer for (1) whether \(}}}\), (2) whether \(}_{t+1}}\), and (3) \(_{t+1}}_{t},_{t}\) are all yes. For Driving Dataset, as is shown in Tab. 1, single-step denotes that we are treating \(s_{t}\) at different time steps as different variables, and calculate the mean of p-values across time. Multi-step denotes that we are aggregating \(s_{t}\) at different times steps together as a single variable, and sample a subset of data every time, calculate the mean of the p-values across subsets. Note that all the variance of p-values in Tab. 1 are less than \(0.001\), so we only record the mean here. By testing CI condition (1) as sufficient, and CI conditions (1) and (2) together as both necessary and sufficient, we verify that subgoals are essentially selection variables. Moreover, by CI condition (3), we also validate that the next state should be determined solely by the previous state and action, and particularly depends on the physical transition, but not influenced by the subgoals.

### Evaluating the Effectiveness of Seq-NMF

Next, we evaluate the seq-NMF in recovering selections and discovering subtasks. Results in the Driving dataset are in Fig. 6 while those for \(Color\) are in Appx. D.4. The y-axis represents the dominance of each subtask in explaining the whole sequence. We plot two sequences and each lasts for around \(110\) steps. Our algorithm finds the "crossing point" and automatically partitions every trajectory into three subtasks (before reaching the first crossing point, in the middle, and after reaching the second crossing point).

  
**CI test** & Single-step & Multi-step \\  \((1)\)\(}}}\) & \(0.00\) & \(0.00\) \\ \((2)\)\(}_{t+1}}\) & \(1e^{-7}\) & \(0.008\) \\ \((3)\)\(_{t+1}}_{t},_{t}\) & \(0.40\) & \(0.65\) \\   

Table 1: P-values for CI tests in Driving.

Figure 4: Patterns in \(Color\)-\(3\) and -\(10\).

Figure 5: Two tasks in Driving environment.

Figure 6: seq-NMF result on Driving.

### Transfering to New Tasks

Kitchen DatasetThe evaluation of the imitation learning is performed on a challenging Kitchen environment from D4RL(Fu et al. (2020)) benchmark. Each agent is required to perform a sequence of subtasks including manipulating the microwave, kettle, cabinet, etc. Each task is composed of \(4\) subtasks and the composition is unknown to the agent. We use the demonstrations provided by (Gupta et al. (2019)) for reproducibility, which only contain state and action pairs but not reward. We use the demonstrations of two tasks for training, and require the agent to accomplish a target task that has a different composition of subtasks than any one of the demonstrations, but the units of subtasks have been performed. Detailed description is included in Appx. D.2.

Baseline methodsWe compare our method with the following state-of-the-art (SOTA) hierarchical imitation learning methods to prove its efficacy: (1) **H-AIRL**(Chen et al. (2023)) is a variant of the meta-hierarchical imitation learning method proposed in Chen et al. (2023) that doesn't incorporate the task context, and is learning an option-based hierarchical policy just as in our setting. (2) **Directed-info GAIL**(Sharma et al. (2019)) and (3) **Option-GAIL**(Jing et al. (2021)) are two other competitive baselines that are proven to be effective in solving multi-task imitation learning.

Results on new tasks with \(4\)-subtasksWe show the results on new tasks with \(4\)-subtasks in Fig. 8. We use the episodic accumulated return as the metric. Training is repeated \(5\) times randomly for each algorithm, with the mean shown as a solid line and the standard deviation as a shaded area. We observe that our method outperforms all the baselines in both tasks. The agent trained with the selection-based subtasks can quickly adapt to the new task with different subtask compositions, achieving a higher return. The results show that our method can effectively transfer the subtask knowledge learned from the demonstrations to new tasks.

Results on new tasks with \(5\)-subtasks (generalized)We conduct additional experiments by considering a distribution shift problem that involves longer-horizon tasks, and plot the results in Fig. 9. Specifically, under the Kitchen environment, we keep the same training set of tasks (each task is composed of \(4\) sequential manipulation subtasks), and tests the method's generalizability to a new task with different permutation of one more subtask, i.e. \(5\) sequential manipulation subtasks. Such generalization to longer-horizon tasks is not taken into consideration by some of the existing works (Chen

Figure 8: Results for solving new tasks in the Kitchen environment w.r.t. training steps.

Figure 7: Kitchen environment

Figure 9: (Generalized) Transfering to microwave \(\) bottom burner \(\) top burner \(\) slide cabinet \(\) hinge cabinet.

et al. (2023)), and our empirical results show that our formulations are able to deal with such a more challenging distribution shift problem.

By detecting the existence of selections in data, and recognizing them as subgoals, we can effectively learn the subtasks that are useful for future use-subgoals are the selection indicators for the subtask patterns. Such a procedure is in contrast to other methods that neglect the real structure in data and merely adopt a maximization of the likelihood objective, which might be misleading for recovering meaningful patterns. Also, their joint optimization of up to five networks (Chen et al. (2023)) makes model instability a major concern. Our method, on the other hand, learns subgoals directly from demonstrations, disentangling it from the policy training. This makes the learning more stable and interpretable, and the learned subtasks can easily adapt to new scenarios.

## 6 Conclusion

In short, we target at a subtask decomposing problem. While previous research has not sufficiently analyze the concept of subtasks, which might lead to an inappropriate inference of subgoals, we propose to view subtasks as outcomes of selections. We first verify the existence of selection variables in the data based on our theory. From this perspective, we recognize subgoals as selections and develop a sequential non-negative matrix factorization (seq-NMF) method for subtask discovery. We rigorously evaluate the algorithm on various tasks and demonstrate the existence of selection and the effectiveness of the method. Finally, our empirical results in a challenging multi-task imitation learning setting further show that the learned subtasks significantly enhance generalization to new tasks, suggesting exciting directions on uncovering the causal process in the data, also showing a new perspective on improving the transferability of policy.

The main limitation in this work lies in that we are not yet able to deal with the scenarios where there might be multiple factors at work, such as the case where there are both underlying confounders and selections. Confounders might be related to other types of distribution shift, e.g. change in the system dynamics, robot embodiment, etc. In future work, we will investigate the causal process in other contexts, and aiming at providing a more general framework for subtask discovery.