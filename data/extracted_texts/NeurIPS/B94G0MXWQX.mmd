# CamoPatch: An Evolutionary Strategy for Generating Camouflaged Adversarial Patches

Phoenix Neale Williams

Department of Computer Science

University of Exeter

Exeter, EX4 4RN

pw384@exeter.ac.uk

&Ke Li

Department of Computer Science

University of Exeter

Exeter, EX4 4RN

k.li@exeter.ac.uk

###### Abstract

Deep neural networks (DNNs) have demonstrated vulnerabilities to adversarial examples, which raises concerns about their reliability in safety-critical applications. While the majority of existing methods generate adversarial examples by making small modifications to the entire image, recent research has proposed a practical alternative known as adversarial patches. Adversarial patches have shown to be highly effective in causing DNNs to misclassify by distorting a localized area (patch) of the image. However, existing methods often produce clearly visible distortions since they do not consider the visibility of the patch. To address this, we propose a novel method for constructing adversarial patches that approximates the appearance of the area it covers. We achieve this by using a set of semi-transparent, RGB-valued circles, drawing inspiration from the computational art community. We utilize an evolutionary strategy to optimize the properties of each shape, and employ a simulated annealing approach to optimize the patch's location. Our approach achieves better or comparable performance to state-of-the-art methods on ImageNet DNN classifiers while achieving a lower \(l_{2}\) distance from the original image. By minimizing the visibility of the patch, this work further highlights the vulnerabilities of DNNs to adversarial patches.

## 1 Introduction

Deep neural networks (DNNs) have revolutionized the field of computer vision, demonstrating significant progress in several tasks . Nevertheless, they are not without vulnerabilities. Recent studies highlight a critical weakness: susceptibility to adversarial examples, where subtle, intentionally designed perturbations to input images result in the DNNs misclassification . The existence of these adversarial examples in the physical world poses a significant threat to security-critical applications such as autonomous vehicles and medical imaging . As a result, developing methods to generate adversarial images has emerged as a critical research area for assessing the robustness of DNNs .

While initial studies emphasized the creation of adversarial examples with \(l_{p}\)-norm (\(p\) can be \(1\), \(2\), or \(\)) constrained perturbations, current research has shifted towards generating sparse perturbations, which alter only a small portion of the original image . These sparse perturbations have proven to be as effective as traditional \(l_{2}\) or \(l_{}\)-constrained perturbations. Adversarial patches, localized perturbations affecting a small portion of the image, have garnered significant interest as a type of sparse perturbation . Various methods for generating adversarial patches have been proposed for both white-box (where complete information about the model is known) and black-box (where only the input-output pairs are accessible) scenarios .

However, a significant challenge remains: the unbounded magnitudes of adversarial patches often lead to noticeable distortions in the original image, as depicted in Figure 1.

A contrasting approach to adversarial attacks is embodied by minimum-norm attacks, a class of strategies that generate adversarial examples through minimizing a particular norm of the adversarial perturbation [40; 6; 13; 58]. Due to their ability to measure adversarial accuracy under a range of perturbation budgets, these attacks serve as valuable tools for assessing DNN robustness . However, these methods come with a notable drawback: they rely heavily on numerous DNN queries to substantially decrease the perturbation size. This dependence becomes particularly problematic in black-box scenarios, where the number of queries is often restricted, thus making it more difficult to reduce the perturbation size effectively .

Although adversarial patches have proven effective in causing DNN misclassification, existing methods often overlook the necessity of minimizing the visual impact of the patch on the original image. This oversight leads to patches that are easily detectable. To address this shortcoming, we introduce a novel method for generating adversarial patches. Our approach utilizes semi-transparent circles with RGB values, which blend into the original image. We employ simulated annealing for optimal location selection and implement an evolutionary strategy to fine-tune each circle's properties. The goal of our method is to induce the desired misclassification while minimizing the \(l_{2}\) distance between the patch and the original image, thereby camouflaging the patch effectively.

The rest of this paper is organized as follows. Section 2 overviews some related works, underscoring their contributions and limitations. In Section 3, we outline our proposed attack scenario and provide an in-depth explanation of our method's implementation. Empirical results are presented and analyzed in Section 4. Section 5 concludes this paper and sheds some light on future directions.

## 2 Related Works

Adversarial attacks on DNNs have been one of most active fields in the machine learning community. A common strategy in these attacks is to create imperceptible perturbations, often achieved by constraining the perturbations using the \(l_{p}\)-norm. These perturbations are typically generated differently depending on the access level to the targeted DNN's information. In a white-box scenario, where the attacker has full access to the DNN's details, approaches often leverage the gradient information of the DNN's loss function. This gradient is used within an optimization algorithm to generate adversarial perturbations [54; 22; 39]. On the other hand, in a black-box scenario, where the attacker's access is limited to the DNN's output probabilities, various attack methods estimate the loss function's gradient and use it within a gradient-based optimization technique [26; 4; 59]. Some researchers have also proposed heuristic methods for the black-box scenario that do not rely on gradient information [1; 2; 27; 28; 11].

Figure 1: This illustration shows adversarial images generated by two algorithms, the proposed method and Patch-RS , both attacking a conventionally trained (left) and adversarially trained (right) ImageNet classifiers. While both images are adversarial, the adversarial patch generated by the state-of-the-art Patch-RS algorithm visibly distorts the image, whereas the proposed methodâ€™s adversarial patch remains more similar to the original image. This similarity is demonstrated by calculating the \(l_{2}\) distance between the adversarial patches and the area of the original image they are placed upon.

Our work fits into this landscape by focusing on the black-box scenario. However, unlike many existing methods, we aim to create adversarial patches that are not only effective but also visually blend into the original image.

### Adversarial Patches

Adversarial patches, designed to cause image misclassification, represent a unique approach to adversarial attacks. These patches are typically small, visible, and often square-shaped, strategically applied to the targeted image [15; 47; 66]. The pioneering work by Brown _et al._ introduced the concept of universal adversarial patches, which cause misclassification when applied to a variety of images. Using the gradient information of the DNN, they utilized stochastic gradient descent to optimize the patch pattern, which was subsequently superimposed onto a set of target images at predetermined locations. Following this, Karmon _et al._ proposed LaVAN that also generates universal patches. However, they used random search to optimize the patch location. In black-box scenarios, Brown _et al._ produced universal adversarial patches by executing white-box attacks on four DNN classifiers and transferring the results to an unseen classifier. Croce _et al._ proposed the Patch-RS method, which generates adversarial patches by minimizing the aggregated loss of a set of images, using random search for patch location optimization and the Square-Attack method of Andriushchenko _et al._ for patch pattern optimization.

In contrast to universal adversarial patch approaches, other researchers have focused on generating image-specific patches. Fawzi and Frossard created patches for individual images by optimizing the position and shape of rectangular patches with a predefined monochrome pattern. Built upon the LaVAN concept, Rao _et al._ proposed alternative techniques for patch location optimization using random search variants. Yang _et al._ and Croce _et al._ further advanced the image-specific scenario, with the former using reinforcement learning for patch generation and the latter applying the Patch-RS method to minimize the loss of a single image.

Despite recent advancements in creating both universal and image-specific adversarial patches, the glaring distortions from significant modifications to input images raise practical concerns. This issue also impacts the accurate assessment of DNN robustness against adversarial patches.

### Minimum-Norm Attacks

Minimum-norm attacks diverge from the traditional adversarial attacks by focusing on finding the smallest perturbation that can lead to misclassification under a specific norm. These attacks offer a more comprehensive assessment of DNN robustness . Although white-box attacks have made significant progress in enhancing the query efficiency of minimum-norm attacks [45; 48; 45], black-box attacks still demand a substantial query budget to achieve effectiveness. The ZOO algorithm  constructs the problem as an aggregated function of the perturbations \(l_{2}\)-norm and weighted loss function. Estimating its gradient using finite-differences, the authors make use of coordinate descent to minimize the formulated problem. Tu _et al._ addressed the query inefficiency of ZOO by reducing the size of the perturbation using a trained Auto-Encoder DNN. Ilyas _et al._ remove the need for estimating coordinate-specific gradients by making use of natural evolutionary strategies, reducing the \(l_{}\)-norm of the perturbation by iteratively maximizing the adversarial criterion within a given norm constraint, then reducing the norm. Despite the efficiency improvement of gradient estimation, existing black-box methods still require large query budgets. The SimBA method of  incrementally adds scaled orthogonal vectors to the current perturbation, increasing its \(l_{2}\)-norm, but is unable to reduce the \(l_{2}\)-norm of the perturbation once the desired misclassification has been achieved.

Therefore, while recent works have achieved large performance gains within the white-box scenario, black-box methods suffer from query inefficiency which restricts their applicability to real-world scenarios, particularly when the query budget is limited.

### Evolutionary Strategies for Adversarial Attacks

Many existing studies employ evolutionary strategies (ES) for non-patch-based adversarial attacks. In the black-box scenario, ES has gained popularity due to its independence from gradient information. Notable examples include the works of [1; 46; 36; 63], who utilize evolutionary algorithms to create \(l_{}\) constrained adversarial perturbations. For conducting sparse adversarial images Williams and Limake use of a multi-objective evolutionary algorithm to minimize both the number of modified pixels and magnitude of the pixels modification. ESs have also been used to construct adversarial examples within other domains such as natural language processing [68; 67].

The use of evolutionary algorithms has also been explored for constructing adversarial patches. Chen _et al._ addressed the more-limited decision-only setting (where only the predicted label is accessible to the attacker) by placing localised regions of an image from the target class onto the attacked image. Under the constraint of the patch causing misclassification, the authors optimised the coordinates of the patch by using an adapted differential evolution algorithm to minimise the patch's \(l_{0}\) norm.

## 3 Proposed Method

In essence, our method strives to generate adversarial patches that seamlessly blend into the targeted image by modeling the superimposed area with semi-transparent, RGB-valued circles. We adopt an approach akin to existing works that generate adversarial patches by iteratively optimizing both the patch and its position on the image. The balance between these steps is managed by a location schedule parameter, \(li\). In this section, we start by defining the problem formulation, followed by a detailed description of our proposed method. The overarching structure of our method is summarized in Algorithm 1.

``` Input: Margin loss \(\), input \(^{h w 3}\), query budget \(K\), sparsity \(\), initial temperature \(t\), number of circles \(N\), location schedule \(li\), evolutionary step-size \(\)
1\(s\)// Patch Side Length
2\( InitialPatch(N,s)\)
3\(i(\{0,,w-s\})\)
4\(j(\{0,,h-s\})\)
5\(^{*}\)
6\(^{*}_{i:i+s,j:j+s}\)// Apply patch
7\(L(^{*})\)
8\(norm||_{ii:i+s,j:j+s}-||_{2}\)
9for\(k 1;k<K\); \(k k+1\)do
10if\(mod(k,li+1)=0\)then
11\(i,j,L,norm LocationUpdate()\)// see Algorithm 4
12
13else
14\(,L,norm PatchUpdate()\)//see Algorithm 3
15
16return\(^{*}\) ```

**Algorithm 1**Evolutionary Strategy for Generating Disguised Adversarial Patches (CamoPatch)

### Problem Formulation

Consider a trained DNN image classifier \(f:^{h w 3}^{P}\) which takes a benign RGB image \(\) of height \(h\) and width \(w\) and outputs a label \(y=*{argmax}_{p\{1,,P\}}f_{p}()\), with \(P\) representing the total number of class labels. A non-targeted attack seeks a perturbation \(\) satisfying:

\[*{argmax}_{p\{1,,P\}}f_{p}(+)=y_{q},\] (1)

where \(y\) is the original class label for \(\) and \(y_{q}=*{argmax}_{q y}f_{p}()\) is a label corresponding to a class other than the true class \(y\). For targeted attacks \(y_{q}\) is assigned a target label \(y_{t}\), where \(y_{t} y\). In the adversarial patch scenario, the number of modified pixels is limited to maintain the semantic content of the image. Hence, the problem is cast as:

\[*{minimize}_{} (f;+,y_{q})\] (2) \[*{subject\ to} ||||_{0},\ \ 0 + 1,\]

[MISSING_PAGE_FAIL:5]

Addressing the discrete nature of pixel locations, many existing methods have employed random search to optimize the position of the patch within the image [15; 29; 47]. However, random search methods often falter when encountering local optima. To mitigate this, Skiscim and Golden introduced simulated annealing, a method that probabilistically accepts worse solutions based on the search _temperature_ and the performance difference between current and new solutions. This approach promotes exploration of the search space in the early stages of optimization and gradually becomes more selective, favoring solutions with better quality in the later stages.

In our work, we leverage the fast simulated annealing approach proposed by Szu and Hartley to optimize the location of a patch. During each iteration \(k\), we uniformly sample a single location (denoted as (\(i^{*}\), \(j^{*}\))) from the location space. Then, we apply the patch to the new location on the input image \(\), and construct an updated adversarial image \(^{**}\). The new solution \(^{**}\) is then evaluated using the loss function \(\). If both \(^{*}\) and \(^{**}\) satisfy the loss \(\) constraint as per (3), we retain the solution with the lowest \(l_{2}\)-norm from the original image. Otherwise, simulated annealing is employed to probabilistically decide the acceptance of the new solution. Specifically, the acceptance probability is defined as \((-d/t_{curr})\), where \(d\) is the loss difference between the current and new solution, and \(t_{curr}=t/k\) follows an exponentially decreasing schedule. This formulation ensures better solutions are always selected, while solutions with relatively poor quality are more likely to be accepted in the early search stages for enhanced exploration. The parameter \(t\) is predefined, with larger values promoting exploration during a longer portion of the attack process. The detailed location update method can be found in Algorithm 4 in the supplementary document.

## 4 Empirical Study

In this section, we empirically evaluate our proposed method's effectiveness by attacking classifiers trained on the ImageNet dataset . The experimental setup is outlined in Section 4.1, followed by a comparative analysis with state-of-the-art adversarial patch methods, including Patch-RS , TPA , OPA , Adv-Watermark  and a black-box adaptation of LOAP  in Section 4.2. Last but not the least, Section 4.3 offers an ablation study that scrutinizes the significance of various components and parameters within our proposed method.

### Experimental Setup

Dataset and Classifiers Settings:For our experiments, we follow a similar setup to preceding works, conducting non-targeted and targeted attacks on DNN classifiers trained on the ImageNet dataset. We specifically target three adversarially trained and defended classifiers, namely AT-ResNet-50 , AT-WideResNet-50-2  and PatchGuard , along with three conventionally trained classifiers, VGG-16 , ResNet-50  and ViT-B/16 . A subset of \(1,000\) images, correctly classified by each classifier from the ImageNet validation set, is chosen and resized to dimensions of \(224 224 3\). For targeted attacks we randomly select \(y_{t}\) for each image ensuring it is different from the images true label \(y\). The adversarially trained and defended classifiers are implemented using the RobustBench library  and authors original implementations, respectively, while the three conventional classifiers are derived from their pre-trained versions available in the PyTorch library . All experiments were carried out on a system with an NVIDIA GeForce RTX 2080Ti GPU.

Parameter Settings:To select the value of \(\), we follow the approach of Croce _et al._, setting \(=1600\). This corresponds to a patch size of \(40 40\), which constitutes roughly \(3.2\%\) of the total pixel count. We assign a budget of \(10,000\) queries for each attack. As discussed in Section 3, our proposed method entails four free parameters: \(\), \(lit\), \(t\), and \(N\). For these parameters, we set \(=0.1\)

Figure 2: This illustration shows an adversarial image (left) with the adversarial patch outlined, and the magnified patch (right) for better visibility. This patch is generated by the proposed method using \(N=100\) overlapping circular shapes.

\(t=300\), \(lit=4\), and \(N=100\). We provide an empirical justification for these specific settings in Section 4.3.

Performance Metrics:We evaluate the performance of all considered algorithms by allowing each method to exhaust the allocated query budget while attacking each classifier. To evaluate the effectiveness of an attack we report the accuracy of the classifier on the generated adversarial images. For the successful adversarial images, we report two additional metrics: (1) the \(l_{2}\) distance between the adversarial patch and the corresponding area of the original image, and (2) the non-normalised residual (NNR) between the adversarial and original image, which measures the absolute difference between the pixel values of the constructed patch and the area of the original image it covers.

Given the stochastic nature of our proposed method and the comparison methods, we follow the setup of  and report the mean and variance of each metric over \(10\) independent runs with different random seeds. We additionally utilize the Wilcoxon signed-rank test  at a \(5\%\) significance level to statistically verify whether the improvements by our method over the compared algorithms across the \(10\) runs are significant.

### Comparison

For the adaptation of LOAP , we replace its gradient computation method with the estimation method of . The detailed description of this estimation method can be found in Algorithm 2 in the supplementary document. Additionally, we compare our method with an adapted version of Patch-RS, where we minimize the \(l_{2}\) distance of the constructed patch in a similar manner to our proposed method. For each compared algorithm, we utilize the authors' original implementation and recommended settings. In our black-box adaptation of LOAP , we set the number of iterations \(n=50\) and variance \(=0.001\) for the gradient estimation method of Ilyas _et al_..

   &  &  \\ 
**Attack** & Accuracy & \(l_{2}\) & NNR & Accuracy & \(l_{2}\) & NNR \\
**Method** & & & & & & \\   Cano-Patch \\ Patch-RS\({}^{}\) \\ Patch-RS\({}^{}\) \\ Patch-RS\({}^{}\) \\ Patch-RS\({}^{}\) \\  } & ^{}\)} & ^{}\)} & ^{}\)} & ^{}\)} & ^{}\)} & ^{}\)} \\    & & & & & & \\  TPA & \(51.65\%\) (\(0.13\))\({}^{}\) & \(0.82\) (\(1.21\))\({}^{}\) & \(0.82\) (\(0.07\))\({}^{}\) & \(34.82\%\) (\(1.14\))\({}^{}\) & \(0.92\) (\(0.05\))\({}^{}\) & \(0.87\)(\(0.09\))\({}^{}\) \\  OPA & \(36.88\%\) (\(0.1\))\({}^{}\) & \(0.76\) (\(0.20\))\({}^{}\) & \(0.74\) (\(0.05\))\({}^{}\) & \(24.83\%\) (\(1.12\))\({}^{}\) & \(0.77\) (\(0.14\))\({}^{}\) & \(0.75\) (\(0.04\))\({}^{}\) \\  LOAP & \(38.85\%\) (\(0.4\))\({}^{}\) & \(0.56\) (\(0.02\))\({}^{}\) & \(0.46\) (\(0.03\))\({}^{}\) & \(48.89\%\) (\(0.1\))\({}^{}\) & \(0.72\) (\(0.18\))\({}^{}\) & \(0.64\) (\(0.03\))\({}^{}\) \\  Adv-watermark & \(52.00\%\) (\(0.3\))\({}^{}\) & \(0.37\)(\(0.05\))\({}^{}\) & \(0.23\)(\(0.07\))\({}^{}\) & \(44.00\%\) (\(0.3\))\({}^{}\) & \(0.42\) (\(0.02\))\({}^{}\) & \(0.29\) (\(0.07\))\({}^{}\) \\   
   &  &  \\ 
**Attack** & Accuracy & \(l_{2}\) & NNR & Accuracy & \(l_{2}\) & NNR \\
**Method** & & & & & & & \\  - & \(77.91\%\) & - & - & - & \(55.1\%\) & - & \\   Cano-Patch \\ Patch-RS\({}^{}\) \\ Patch-RS\({}^{}\) \\ Patch-RS\({}^{}\) \\ Patch-RS\({}^{}\) \\ Patch-RS\({}^{}\) \\  } & ^{}\)} & ^{}\)} & ^{}\)} & ^{}\)} & ^{}\)} & ^{}\)} \\    & & & & & & \\  Patch-RS\({}^{}\) & \(19.00\%\) (\(0.10\))\({}^{}\) & \(0.71\) (\(0.12\))\({}^{}\) & \(0.41\) (\(0.09\))\({}^{}\) & \(5.80\%\) (\(0.02\))\({}

Results:Table 1 present the statistical results of non-targeted attacks conducted on the trained ImageNet classifiers. In the tables, "CamoPatch" denotes our proposed method, and "Patch-RS*" refers to the adapted Patch-RS algorithm.

These results demonstrate that the Patch-RS attack, along with our own method, achieves higher attack success rates compared to the other state-of-the-art methods. This result aligns with previous work , which demonstrated the superior performance of the Patch-RS algorithm. Despite Patch-RS outperforming our method when attacking the VGG-16 classifier, according to the Wilcoxon signed-rank test, there is no significant difference between the performance of both methods. Alternatively, when attacking the remaining five classifiers, the proposed method is able to significantly outperform Patch-RS and other compared methods according to the Wilcoxon signed-rank test.

Comparing the \(l_{2}\) distance and NNR of adversarial patches generated by the attack methods, the proposed method is able to construct adversarial patches that are far less invasive to the input image. This is supported by the proposed method significantly outperforming all other methods in terms of both \(l_{2}\) distance and NNR, according to the Wilcoxon signed-rank test. This result highlights that the effectiveness of our adversarial patches is not compromised by their perceptibility.

Despite the adapted Patch-RS* algorithm being able to generate patches with lower \(l_{2}\) distances from the original image compared to its original implementation, its use of Square-Attack  for patch pattern optimization results in the patch values taking the corners of the color cube \(\). Therefore, its ability for \(l_{2}\) minimization is significantly hampered. Alternatively, the proposed method is able to construct patches with any color, which allows for effective approximations of the original image area. Figure 3 provides a visual comparison of adversarial images generated by each method when attacking the VGG-16 and AT-ResNet-50 classifiers.

We report the results of the targeted attacks in Section 6.1 of the supplementary material.

Robustness Evaluation:Despite the assumption that adversarial trained classifiers have improved robustness compared to their conventionally trained counterparts, our experimental results reveal a different picture. The proposed method achieves higher success rates when attacking the adversarial trained classifier AT-ResNet-50 of Salman _et al._ compared to the conventionally trained VGG-16 and ResNet-50 classifiers.

However, the results also demonstrate that the adversarial patches generated by the proposed method, when attacking the AT-ResNet-50 classifier, exhibit larger \(l_{2}\) distances from the original image, resulting in larger non-normalised residuals between the entire adversarial image and the original image. This suggests that while the adversarial trained classifier is more susceptible to adversarial patches, these patches require larger distortions to cause the misclassification. On the other hand, conventionally trained classifiers are more susceptible to smaller changes in the original image. This behavior can be attributed to the general procedure of adversarial training, which introduce noise onto images during the training to enhance robustness. Consequently, patches with larger impact decrease the likelihood of the image being representative of the training data, as has been observed in other works . Furthermore, wee see the AT-WideResNet-50-2 classifier exhibits greater robustness to

Figure 3: Adversarial images generated by methods conducting non-targeted attacks on the conventionally trained VGG-16 (top) and adversarial trained WideResNet-50-2  (bottom) classifiers. Whereas adversarial patches generated by state-of-the-art methods are visibly clear, the patches generated by the proposed method are well camouflaged within the image.

our method. Since both AT-WideResNet-50-2 and AT-ResNet-50 models are trained using the same process, these results suggest that the WideResNet architecture is inherently more robust.

### Ablation Study

The proposed method consists of four tunable parameters: \(li,N,,\) and \(t\). To determine their optimal values, we conduct a grid search over the parameter space. Specifically, we explore \(li\{1,4\}\), \(N\{100,300\}\), \(t\{100,300\}\), and \(\{0.1,0.3\}\). The choice of \(li\) follows the recommendation of Croce _et al._, while the values of \(\), \(t\), and \(N\) are commonly used in the evolutionary and computational art  communities, respectively.

To evaluate the performance of each parameter configuration, we conduct non-targeted attacks on the VGG-16 ImageNet classifier using \(1000\) correctly classified images from the validation set. We measure the accuracy of the model on the generated adversarial images, \(l_{2}\) distance and NNR for each configuration over \(10\) independent runs with different random seeds. Additionally, we compare the computational time required for each configuration to complete an attack on a single image.

Configurations:Table 2 presents the four top performing configurations in terms of the attack accuracy. The results demonstrate that the performance of the proposed method heavily depends on the number of circles \(N\) used to construct the patch pattern. Increasing the number of circles allows for better detailed approximations but also introduces additional complexity. From the results in Table 2, we observe that the best performing configurations all use \(N=100\), suggesting that the patch optimizer, \((1+1)\)-ES, struggles with larger numbers of circles. Moreover, we observe longer runtimes for \(N=300\) due to the increased number of properties that need adjustment. Beyond the number of circles \(N\), the proposed method achieves improved performance with a larger budget for patch pattern optimization (\(li=4\)) and a larger exploration parameter for location optimization (\(t=300\)). Based on these findings, we set the optimal parameter configuration of the proposed method to \(li=4,t=300,N=100\), and \(=0.1\).

Simulated Annealing:To justify the use of simulated annealing for location optimization within the proposed method, we compare its performance with and without simulated annealing. Removing simulated annealing results in a pure random search method similar to existing works. We keep the other parameters of the proposed method constant with those outlined in Section 4.1. The results in Table 3 demonstrate the improved performance exhibited by the proposed method when the simulated annealing policy is employed for location optimization, particularly when attacking adversarial trained classifiers. Despite generating patches with a higher \(l_{2}\) distance, the proposed method with simulated annealing achieves higher success rates. This suggests that more challenging images require larger distortions to cause misclassification, increasing the average \(l_{2}\) distance of the generated successful adversarial patches.

## 5 Contributions, Limitations and Future Work

Contributions:In this work, we propose Campatch, a novel attack method for generating adversarial patches that can blend into the targeted image. We achieve this by constructing the patch

   & & &  \\  \(li\) & \(t\) & \(N\) & \(\) & Accuracy & \(l_{2}\) & NNR & Runtime(s) \\  - & - & - & - & 76.12\% & - & - & - \\ 
1 & \(300\) & \(100\) & \(0.1\) & \(12.88\%(1.0)\) & \(\) & \(0.14(0.01)\) & \(440.03(10.32)\) \\ 
4 & \(100\) & \(100\) & \(0.1\) & \(10.79\%(1.5)\) & \(\) & \(0.13(0.02)\) & \(440.01(10.05)\) \\ 
4 & \(300\) & \(100\) & \(0.1\) & \(\) & \(\) & \(\) & \(440.03(10.32)\) \\ 
4 & \(300\) & \(100\) & \(0.3\) & \(11.66\%(2.0)\) & \(\) & \(\) & \(\) \\  

* denotes the performance of the method significantly outperforms the compared methods according to the Wilcoxon signed-rank test  at the \(5\%\) significance level;
* denotes the corresponding method is significantly outperformed by the best performing method (shaded).

Table 2: Table presents the before and after-accuracy of each Campatch configuration along with the \(l_{2}\) distance of the adversarial patch and the non-normalised residual (NNR) between the adversarial and original image after conducting non-targeted attacks. We provide the mean and variance of each metric over \(10\) runs.

pattern using a combination of semi-transparent, RGB-valued circles, which are optimized to cause misclassification and approximate the covered area of the original image. By incorporating a simulated annealing policy for location optimization, our method generates adversarial patches with improved or comparable success rates, while minimizing the visual impact on the target image.

Ethical Considerations:Adversarial patches have gained attention due to their potential real-world applications, where attackers can print and physically place them to deceive real-world implemented DNNs [7; 18]. However, existing methods often generate patches that are visually clear and easily detectable to a human observer. Our work introduces the concept of camouflaged adversarial patches, which are difficult for both humans and computer vision systems to perceive. This raises further concerns about the robustness of DNN classifiers in safety-critical applications. Adversarial training has proven to be an effective method of improving the robustness of DNN classifiers to adversarial patches . Incorporating images with camouflaged adversarial patches into the training process of DNNs may be a promising avenue to enhance their robustness and mitigate the vulnerabilities demonstrated in this work.

Limitations and Future Work:It is important to acknowledge the limitations of the proposed method and identify potential areas for future research. One limitation is that our method assumes the attacker has access to the output probabilities of the targeted DNN, which may not always be the case in real-world scenarios. Future work could explore adapting the proposed method to scenarios where only the predicted label of the input is available, by utilizing estimated loss functions such as the one proposed by Ilyas _et al._. Furthermore, techniques from weakly supervised learning can be incorporated into the proposed method to address the label-only setting. Specifically, by using estimation techniques  to score constructed adversarial images, the use of weakly supervised image classification models  as surrogates could improve the efficiency of the proposed method in addition to providing a direction of the search for the label-only setting. Alternatively, utilizing the stochastic nature of the proposed method to generate a set of non-evaluated candidate solutions, the use of the weakly supervised learning techniques such as semi-supervised learning could be applied  to train a surrogate model on both evaluated and non-evaluated adversarial images. Thereby using the surrogate to select predicted-optimal solutions for evaluation.

Another limitation is the DNN query budget assumption in our experiments. In practice, the available query budget might be significantly lower. To address this limitation, future research could extend the proposed method to incorporate surrogates or approximation models that guide the attack process, using fewer DNN queries, similar to that of Williams _et al._ within the computational art field.

Lastly, the parameter tuning process in our work follows a conventional grid-search approach, which limits the exploration of parameter combinations. Bayesian optimization methods could be employed to automate the configuration of attack algorithms, leveraging Gaussian Process surrogate models to handle the stochastic nature of the proposed method and guide the parameter search [34; 10; 9; 37]. This would provide a more efficient approach for determining the optimal parameter configuration.

   &  & ^{}\)} \\ 
**Classifier** & ASR & \(l_{2}\) & \(\) & ASR & \(l_{2}\) & \(\) \\  VGG-16 & **90.03** & **0.003** & **0.002** & **0.11 (0.02)** & **0.01 (0.11)** & **0.00 (0.01)** & **0.11 (0.02)** \\  ResNet-50 & **90.00** & **0.02** & **0.08 (0.01)** & **0.08 (0.01)** & **0.000**(0.01)** & 0.00 (0.01) & 0.1 (0.02) \\  AT-WideResNet- & **87.02** & **(0.01)** & \(0.14\) (0.03)\({}^{}\) & **0.12 (0.07)** & 83.02 (0.02)\({}^{}\) & **0.11 (0.05)\({}^{}\)** & **0.09 (0.02)** \\  AT-ResNet-50 & **94.00** & **(0.03)** & \(0.15\) (0.03) & **0.13 (0.03)** & 90.00\% (0.01) & **0.14 (0.03)** & **0.13 (0.05)** \\   \({}^{}\) denotes the performance of the method significantly outperforms the compared methods according to the Wilcoxon signed-rank test  at the \(5\%\) significance level; \({}^{}\) denotes the corresponding method is significantly outperformed by the best performing method (shaded).

Table 3: Table presents the before and after-accuracy of the CamoPatch method with (CamoPatch) and without (CamoPatch*) the simulated annealing policy for location optimization, along with the \(l_{2}\) distance of the adversarial patch and the non-normalised residual (NNR) between the adversarial and original image after conducting non-targeted attacks. We provide the mean and variance of each metric over \(10\) runs.