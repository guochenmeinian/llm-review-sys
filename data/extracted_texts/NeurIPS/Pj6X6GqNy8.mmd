# Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos

via Temporal Alignment

 Zihui Xue1,2 Kristen Grauman1,2

1The University of Texas at Austin 2FAIR, Meta

###### Abstract

The egocentric and exocentric viewpoints of a human activity look dramatically different, yet invariant representations to link them are essential for many potential applications in robotics and augmented reality. Prior work is limited to learning view-invariant features from _paired_ synchronized viewpoints. We relax that strong data assumption and propose to learn fine-grained action features that are invariant to the viewpoints by aligning egocentric and exocentric videos in time, even when not captured simultaneously or in the same environment. To this end, we propose AE2, a self-supervised embedding approach with two key designs: (1) an object-centric encoder that explicitly focuses on regions corresponding to hands and active objects; and (2) a contrastive-based alignment objective that leverages temporally reversed frames as negative samples. For evaluation, we establish a benchmark for fine-grained video understanding in the ego-exo context, comprising four datasets--including an ego tennis forehand dataset we collected, along with dense per-frame labels we annotated for each dataset. On the four datasets, our AE2 method strongly outperforms prior work in a variety of fine-grained downstream tasks, both in regular and cross-view settings.1

## 1 Introduction

_Fine-grained video understanding_ aims to extract the different stages of an activity and reason about their temporal evolution. For example, whereas a coarse-grained action recognition task  might ask, "is this sewing or snowboarding or...?", a fine-grained action recognition task requires detecting each of the component steps, _e.g._, "the person threads the needle here, they poke the needle through the fabric here..." Such fine-grained understanding of video is important in numerous applications that require step-by-step understanding, ranging from robot imitation learning  to skill learning from instructional "how-to" videos .

The problem is challenging on two fronts. First, the degree of detail and precision eludes existing learned video representations, which are typically trained to produce global, clip-level descriptors of the activity . Second, in many scenarios of interest, the camera viewpoint and background will vary substantially, making the same activity look dramatically different. For example, imagine a user wearing AR/VR glasses who wants to compare the egocentric recording of themselves doing a tennis forehand with a third-person (exocentric) expert demonstration video on YouTube, or a robot tasked with pouring drinks from its egocentric perspective but provided with multiple third-person human pouring videos for learning. Existing view-invariant feature learning --including methods targeting ego-exo views --assume that training data contains synchronized video pairs capturing the same action _simultaneously in the same environment_. This is a strong assumption. It implies significant costs and physical constraints that can render these methods inapplicable in complex real-world scenarios.

Our goal is to learn fine-grained frame-wise video features that are invariant to both the ego and exo views,2 from _unpaired_ data. In the unpaired setting, we know which human activity occurs in any given training sequence (_e.g._, pouring, breaking eggs), but they need not be collected simultaneously or in the same environment. The main idea of our self-supervised approach is to infer the temporal alignment between ego and exo training video sequences, then use that alignment to learn a view-invariant frame-wise embedding. Leveraging unpaired data means our method is more flexible--both in terms of curating training data, and in terms of using available training data more thoroughly, since with our approach _any_ ego-exo pairing for an action becomes a training pair. Unlike existing temporal alignment for third-person videos [15; 54; 22; 23; 41], where canonical views are inherently similar, ego-exo views are so distinct that finding temporal visual correspondences is non-trivial (see Fig. 1).

To address this challenge, we propose two main ideas for aligning ego and exo videos in time. First, motivated by the fact that human activity in the first-person view usually revolves around hand-object interactions, we design an object-centric encoder to bridge the gap between ego-exo viewpoints. The encoder integrates regional features corresponding to hands and active objects along with global image features, resulting in more object-centric representations. Second, we propose a contrastive-based alignment objective, where we temporally reverse frames to construct negative samples. The rationale is that aligning an ego-exo video pair should be easier (_i.e._, incur a lower alignment cost) compared to aligning the same video pair when one is played in reverse. We bring both ideas together in a single embedding learning approach we call AE2, for "align ego-exo".

To evaluate the learned representations, we establish an ego-exo benchmark for fine-grained video understanding. Specifically, we assemble four datasets of atomic human actions, comprising ego and exo videos drawn from five public datasets and an ego tennis forehand dataset that we collected. In addition, we annotate these datasets with dense per-frame labels (for evaluation only). We hope that the enriched data and labels that we release publicly can support progress in fine-grained temporal understanding from the ego-exo perspective. This is a valuable dataset contribution alongside our technical approach contribution. Furthermore, we present a range of practical downstream tasks that demand frame-wise ego-exo view-invariant features: action phase classification and retrieval across views, progress tracking, and synchronous playback of ego and exo videos. Experimental results demonstrate considerable performance gains of AE2 across all tasks and datasets.

## 2 Related Work

Fine-grained Action UnderstandingTo recognize fine-grained actions, supervised learning approaches [82; 30; 59; 65] employ fine-grained action datasets annotated with sub-action boundaries. In contrast, our self-supervised approach requires much lighter annotations to pretrain for

Figure 1: Left: We propose AE2, a self-supervised approach for learning frame-wise action features that are invariant to the egocentric and exocentric viewpoints. The key is to find correspondences across the viewpoints via temporal alignment. Right: Before training, the frame embeddings are clustered by viewpoint (pink and green); after training, our learned AE2 embeddings effectively capture the progress of an action and exhibit viewpoint invariance. Shading indicates time.

action phase classification, namely, video-level action labels. Keystep recognition in instructional videos [86; 73; 47; 46; 2] is a related form of fine-grained action understanding, where the goal is to name the individual steps of some longer activity (_e.g._, making brownies requires breaking eggs, adding flour, etc.). In that domain, multi-modal representation learning is a powerful tool that benefits from the spoken narrations that accompany how-to videos [47; 72; 38; 84]. In any of the above, handling extreme viewpoint variation is not an explicit target.

**Self-supervised Video Representation Learning** Various objectives for self-supervised video features have been explored . This includes using accompanying modalities like audio [51; 29; 49] and text or speech [47; 72; 38; 37; 84] as training signals, as well as temporal coherence priors [28; 26; 63; 44; 10] or self-supervised tasks that artificially alter the temporal order of videos [48; 19; 35]. Alignment-based objectives have also been explored [15; 54; 22; 23; 41; 33], with ideas based on cycle consistency [15; 54], soft dynamic time warping (DTW) [22; 23], and optimal transport (OT) . Our model also has an alignment stage; however, unlike our work, all these prior methods are explored for third-person videos only (where views are generally more similar across videos than ego-exo) without attending to the viewpoint discrepancy problem. Frame-wise representations invariant to both ego and exo viewpoints remain unresolved.

**View-invariant Action Representation Learning** Many prior works have explored view-invariance in action recognition [76; 9; 53]. One popular approach is to employ multi-view videos during training to learn view-invariant features [76; 77; 27; 78; 45; 71]. Several works [17; 39; 83; 55] discover a latent feature space independent of viewpoint, while others use skeletons , 2D/3D poses [56; 71] and latent 3D representations  extracted from video to account for view differences.

As advances in AR/VR and robotics unfold, ego-exo viewpoint invariance takes on special interest. Several models relate coarse-grained action information across the two drastically different domains [70; 1; 80; 36], for action recognition using _both_ ego and exo cameras  or cross-viewpoint video recognition [80; 1; 36]. Closer to our work are Time Contrastive Networks (TCN)  and CharadesEgo [67; 66], both of which use embeddings aimed at pushing (ir)relevant ego-exo views closer (farther). However, unlike our approach, both employ videos that _concurrently_ record the same human actions from ego and exo viewpoints. In fact, except for , all the above approaches rely on simultaneously recorded, paired multi-view videos. However, even in the case of , the goal is a coarse, clip-wise representation. In contrast, our method leverages temporal alignment as the pretext task, enabling fine-grained representation learning from _unpaired_ ego and exo videos.

**Object-centric Representations** Several works advocate object-centric representations to improve visual understanding [40; 85; 50; 36], such as an object-centric transformer for joint hand motion and interaction hotspot prediction , or using a pretrained detection model for robot manipulation tasks [85; 50]. Hand-object interactions are known to dominate ego-video for human-worn cameras [11; 21], and recent work explores new ways to extract hands and active objects [64; 43; 12]. Our proposed object-centric transformer shares the rationale of focusing on active objects, though in our case as a key to link the most "matchable" things in the exo view and bridge the ego-exo gap.

## 3 Approach

We first formulate the problem (Sec. 3.1) and introduce the overall self-supervised learning objective (Sec. 3.2). Next we present our ideas for object-centric transformer (Sec. 3.3) and an alignment-based contrastive regularization (Sec. 3.4), followed by a review of our training procedure (Sec. 3.5).

### Learning Frame-wise Ego-Exo View-invariant Features

Our goal is to learn an embedding space shared by both ego and exo videos that characterizes the progress of an action, as depicted in Fig. 1 (right). Specifically, we aim to train a view-invariant encoder network capable of extracting fine-grained frame-wise features from a given (ego or exo) video. Training is conducted in a self-supervised manner.

Formally, let \((;)\) represent a neural network encoder parameterized by \(\). Given an ego video sequence \(\) with \(M\) frames, denoted by \(=[_{1},,_{M}]\), where \(_{i}\) denotes the \(i\)-th input frame, we apply the encoder to extract frame-wise features from \(\), _i.e._, \(_{i}=(_{i};), i\{1,,M\}\). The resulting ego embedding is denoted by \(=[_{1},,_{M}]\). Similarly, given an exo video sequence \(=[_{1},,_{N}]\) with \(N\) frames, we obtain the frame-wise exo embedding \(=[_{1},,_{N}]\), where \(_{i}=(_{i};), i\{1,,N\}\) using the same encoder \(\).

The self-supervised learning objective is to map \(\) and \(\) to a shared feature space where frames representing similar action stages are grouped together, regardless of the viewpoint, while frames depicting different action stages (_e.g._, the initial tilting of the container versus the subsequent decrease in the liquid stream as the pouring action nears completion) should be separated. We present a series of downstream tasks to evaluate the quality of learned feature embeddings \(\) and \(\) (Sec. 4).

An intuitive approach is to utilize paired ego-exo data collected simultaneously, adopting a loss function that encourages similarities between frame representations at the same timestamp while maximizing the distance between frame representations that are temporally distant [63; 66]. However, this approach relies on time-synchronized views, which is often challenging to realize in practice.

Instead, we propose to achieve view-invariance from _unpaired_ ego-exo data; the key is to adopt temporal alignment as the pretext task. The encoder \(\) learns to identify visual correspondences across views by temporally aligning ego and exo videos that depict the same action. Throughout the learning process, no action phase labels or time synchronization is required. To enforce temporal alignment, we assume that multiple videos portraying the same action (_e.g._, pouring, tennis forehand), captured from different ego and exo viewpoints (and potentially entirely different environments), are available for training.

### Ego-Exo Temporal Alignment Objective

To begin, we pose our learning objective as a classical dynamic time warping (DTW) problem . DTW has been widely adopted to compute the optimal alignment between two temporal sequences [8; 6; 23; 22; 79] according to a predefined cost function. In our problem, we define the cost of aligning frame \(i\) in ego sequence \(\) and frame \(j\) in exo sequence \(\) as the distance between features extracted by our encoder \(\), \(_{i}\) and \(_{j}\), so that aligning more similar features results in a lower cost.

Formally, given the two sequences of embeddings \(\) of length \(M\) and \(\) of length \(N\), we calculate a cost matrix \(^{M N}\), with each element \(c_{i,j}\) computed by a distance function \(\). We adopt the distance function from , and define \(c_{i,j}\) to be the negative log probability of matching \(_{i}\) to a selected \(_{j}\) in sequence \(\):

\[c_{i,j}=(_{i},_{j})=-}_{i}^{T}}_{j}/)}{_{k=1}^{N}(}_{i}^{T}}_{k}/)},}_{i} =_{i}/\|_{i}\|_{2}}_{i}=_ {i}/\|_{i}\|_{2}\] (1)

where \(\) is a hyper-parameter controlling the softmax temperature, which we fix at 0.1.

DTW calculates the alignment cost between \(\) and \(\) by finding the minimum cost path in \(\). This optimal path can be determined by evaluating the following recurrence :

\[(i,j)=c_{i,j}+[(i-1,j-1),(i-1,j), (i,j-1)]\] (2)

where \((i,j)\) denotes the cumulative distance function, and \((0,0)=0\). Given that the min operator is non-differentiable, we approximate it with a smooth differentiable operator, as defined in [34; 22]. This yields a differentiable loss function that can be directly optimized with back-propagation:

\[_{}(,)=(M,N).\] (3)

While DTW has been applied in frame-wise action feature learning [23; 22] with some degree of view invariance across exo views, it is inadequate to address the ego-exo domain gap. As we will show in results, directly adopting the DTW loss for learning ego-exo view-invariant features is sub-optimal since the ego and exo views, even when describing the same action, exhibit significant differences (compared to differences within ego views or within exo views). In Sec. 3.3-3.4, we propose two designs of AE2 that specifically account for the dramatic ego-exo view differences.

### Object-centric Encoder Network

The visual appearance in ego and exo views may look dramatically different due to different backgrounds, fields of view, and (of course) widely disparate viewing angles. Yet a common element shared between these two perspectives is the human action itself. In this context, _hand-object interaction regions_ are particularly informative about the progress of an action. Hand-object interactions are prominent in the first-person viewpoint; the wearable camera gives a close-up view of the person's near-field manipulations. In exo videos, however, these active regions may constitute only a small portion of the frame and are not necessarily centered, depending on the third-person camera position and angle. Inspired by these observations, we propose an object-centric encoder design that explicitly focuses on regions corresponding to hands and active objects to better bridge the ego-exo gap.

Our proposed AE2 encoder network \(\) consists of two main components: (1) a ResNet-50 encoder , denoted by \(_{}\), to extract features from the input video frame; (2) a transformer encoder , denoted by \(_{}\), to fuse global features along with regional features through self-attention.

As a preprocessing step, we employ an off-the-shelf detector to localize hand and active object regions in ego and exo frames . For frame \(i\), we select the top two hand and object proposals with the highest confidence scores, resulting in a total of four bounding boxes \(\{_{i,k}\}_{k=1}^{4}\).

First, given one input frame \(_{i}\) at timestamp \(i\), we pass it to \(_{}\) to obtain global features: \(_{i}=_{}(_{i})\). Next, we apply ROI Align  to intermediate feature maps produced by \(_{}\) for the input frame \(_{i}\). This yields regional features \(_{i,k}\) for each bounding box \(_{i,k}\): \(_{i,k}=(_{}^{}(_{i}),_{i,k}), k\).

Both the global and regional features are flattened as vectors and passed through a projection layer to unify their dimensions. We denote the projection layer to map global and local tokens as \(_{_{g}}\) and \(_{_{i}}\), respectively. The input tokens are then enriched with two types of embeddings: (1) a learnable spatial embedding, denoted by \(_{}\), which encodes the bounding box coordinates and confidence scores obtained from the hand-object detector and (2) a learnable identity embedding, denoted by \(_{}\), which represents the category of each feature token, corresponding to the global image, left hand, right hand, and object. This yields the tokens: \(_{i,}=_{_{g}}(_{i})+ _{}\) and \(_{i,k}=_{_{i}}(_{i,k})+ _{}+_{}, k\).

The transformer encoder processes one global token along with four regional tokens as input, leveraging self-attention to conduct hand-object interaction reasoning. This step enables the model to effectively capture action-related information within the scene, yielding the transformer outputs: \(_{i,k}=_{}(_{i,},\{ _{i,k}\}_{k})_{k})\). Finally, we average the transformer output tokens and adopt one embedding layer \(_{}\) to project the features to our desired dimension, yielding the final output embedding for the input frame \(_{i}\): \(_{i}=_{}(_{k}_{i,k})\).

Fig. 2 (left) showcases our proposed object-centric encoder. This design inherently bridges the ego-exo gap by concentrating on the most informative regions, regardless of the visual differences between the two perspectives. As a result, the object-centric encoder can better align features extracted from both ego and exo videos, enhancing the learned features in downstream tasks.

### Contrastive Regularization With Reversed Videos

As shown in previous studies [22; 23], directly optimizing the DTW loss can result in the collapse of embeddings and the model settling into trivial solutions. Moreover, aligning ego and exo videos

Figure 2: Overview of AE2. We extract frame-wise representations from the two video sequences (_i.e._, \(\) and \(\)) using the encoder \(\) and temporally align their embeddings (_i.e._, \(\) and \(\)) using DTW. Left: Our proposed encoder network design emphasizes attention to hand and active object regions, leading to more object-centric representations. Right: As a form of regularization, we reverse the video sequence \(\) and enforce that the cost of aligning \((,)\) is less than that of aligning \((},)\).

presents new challenges due to their great differences in visual appearance. Thus, a careful design of regularization, supplementing the alignment objective in Eqn. (3), is imperative.

We resort to discriminative modeling as the regularization strategy. The crux lies in how to construct positive and negative samples. A common methodology in previous fine-grained action approaches [63; 66; 23; 10], is to define negative samples on a frame-by-frame basis: temporally close frames are considered as positive pairs (expected to be nearby in the embedding space), while temporally far away frames are negative pairs (expected to be distant). This, however, implicitly assumes clean data and strict monotonicity in the action sequence: if a video sequence involves a certain degree of periodicity, with many visually similar frames spread out over time, these approaches could incorrectly categorize them as negative samples.

To address the presence of noisy frames in real-world videos, we introduce a global perspective that creates negatives at the video level. From a given positive pair of sequences \((,)\), we derive a negative pair \((},)\), where \(}=[_{M},,_{1}]\) denotes the _reversed_ video sequence \(\). The underlying intuition is straightforward: aligning an ego-exo video pair in their natural temporal order should yield a lower cost than aligning the same pair when one video is played in reverse. Formally, we employ a hinge loss to impose this regularization objective:

\[_{}(,)=(_{}(,)-_{}(}, ),0)\] (4)

where \(}\) denotes the embeddings of \(}\), which is essentially \(\) in reverse. Fig. 2 (right) illustrates the alignment cost matrices of \((,)\) and \((},)\). A lighter color indicates a smaller cost. Intuitively, \(_{}\) corresponds to the minimal cumulative cost traversing from the top-left to the bottom-right of the matrix. As seen in the figure, the alignment cost increases when \(\) is reversed, _i.e._, \(_{}(},)\) should be larger than \(_{}(,)\).

Compared with frame-level negatives, our formulation is inherently more robust to repetitive and background frames, allowing for a degree of temporal variation within a video. This principle guides two key designs: (1) In creating negative samples, we opt for reversing the frames in \(\) rather than randomly shuffling them. The latter approach may inadvertently generate a plausible (hence, not negative) temporal sequence, particularly when dealing with short videos abundant in background frames. (2) To obtain informative frames as the positive sample, we leverage the hand-object detection results from Sec. 3.3 to perform weighted sampling of the video sequences \(\) and \(\). Specifically, we sample frames proportionally to their average confidence scores of hand and object detections. The subsampled \(\) and \(\) thus compose a positive pair. This way, frames featuring clear hand and object instances are more likely to be included.

### Training and Implementation Details

Our final loss is a combination of the DTW alignment loss (Eqn. (3)) and the contrastive regularization loss (Eqn. (4)):

\[(,)=_{}(, )+_{}(,),\] (5)

where \(\) is a hyper-parameter controlling the ratio of these two loss terms.

During training, we randomly extract 32 frames from each video to construct a video sequence. The object-centric encoder network \(\), presented in Sec. 3.3, is optimized to minimize the loss above for all pairs of video sequences in the training set, including ego-ego, exo-exo, and ego-exo pairs3--all of which are known to contain the same coarse action. During evaluation, we freeze the encoder \(\) and use it to extract 128-dimensional embeddings for each frame. These representations are then assessed across a variety of downstream tasks (Sec. 4). See Supp. for full implementation details.

## 4 Datasets and Evaluation

DatasetsExisting datasets for fine-grained video understanding (_e.g._[82; 65; 3]) are solely composed of third-person videos. Hence we propose a new ego-exo benchmark by assembling ego and exo videos from five public datasets--CMU-MMAC , H2O , EPIC-Kitchens , HMDB51 and Penn Action --plus a newly collected ego tennis forehand dataset. Our benchmark consists of four action-specific ego-exo datasets: (A) **Break Eggs**: ego-exo videos of 44 subjects breaking eggs, with 118/30 train/val+test samples from CMU-MMAC ; (B) **Pour Milk**: ego-exo videos of 10 subjects pouring milk, with 77/35 train/val+test from H2O ; (C) **Pour Liquid**: we extract ego "pour water" clips from EPIC-Kitchens  and exo "pour" clips from HMDB51 , yielding 137/56 train/val+test. (D) **Tennis Forehand**: we extract exo tennis forehand clips from Penn Action  and collect ego tennis forehands by 12 subjects, totaling 173/149 train/val+test clips. We obtain **ground truth key event and action phase labels** for all of them (used only in downstream evaluation), following the labeling methodology established in . Fig. 3 illustrates examples of these labels. Altogether, these four datasets provide unpaired ego-exo videos covering both tabletop (_i.e._, breaking eggs, pouring) and physical actions (_i.e._, tennis forehand).4 While (A) and (B) are from more controlled environments, where exo viewpoints are fixed and both views are collected in the same environment, (C) and (D) represent in-the-wild scenarios, where ego and exo are neither synchronized nor collected in the same environment. Our model receives them all as unpaired.

EvaluationWe aim to demonstrate the practical utility of the learned ego-exo view-invariant features by addressing two questions: (1) how well do the features capture the progress of the action? (2) how view-invariant are the learned representations? To this end, we test four downstream tasks: (1) **Action Phase Classification**: train an SVM classifier on top of the embeddings to predict the action phase labels for each frame and report F1 score. We also explore few-shot and cross-view zero-shot settings (_i.e._, "exo2ego" when training the SVM classifier on exo only and testing on ego, and vice versa). (2) **Frame Retrieval**: parallels the classification task, but done via retrieval (no classifier), and evaluated with mean average precision (mAP)@\(K\), for \(K\)=5,10,15. We further extend this task to the cross-view setting to evaluate view-invariance. (3) **Phase Progression**: train a linear regressor on the frozen embeddings to predict the phase progression values, defined as the difference in time-stamps between any given frame and each key event, normalized by the number of frames in that video , evaluated by average R-square. (4) **Kendall's Tau**: to measure how well-aligned two sequences are in time, following .

Note that: (1) Departing from the common practice of splitting videos into train and validation only , our benchmark includes a dedicated test set to reduce the risk of model overfitting. (2) We opt for the F1 score instead of accuracy for action phase classification to better account for the label imbalance in these datasets and provide a more meaningful performance evaluation. (3) Phase progression assumes a high level of consistency in actions, with noisy frames diminishing the performance greatly. Due to the challenging nature of Pour Liquid data, we observe a negative progression value for all approaches. Thus, we augment the resulting embeddings with a temporal dimension, as 0.001 times the time segment as the input so that the regression model can distinguish repetitive (or very similar) frames that differ in time. We report modified progression value for all approaches on this dataset. (4) Kendall's Tau assumes that there are no repetitive frames in a video. Since we adopt in-the-wild videos where strict monotonicity is not guaranteed, this metric may not faithfully reflect the quality of representations. Nonetheless, we report them for completeness and for consistency with prior work. (5) With our focus being frame-wise representation learning, the total number of training samples equals the number of frames rather than the number of videos, reaching a scale of a few thousands (8k-30k per action class). See Supp. for more dataset and evaluation details.

## 5 Experiments

BaselinesWe compare AE2 with 8 total baselines of three types: (1) Naive baselines based on **random features** or **ImageNet features**, produced by a ResNet-50 model randomly initialized

Figure 3: Example labels for Pour Liquid (left) and Tennis Forehand (right). Key events are displayed in boxes below sequences, with the phase label assigned to each frame between two key events.

or pretrained on ImageNet ; (2) Self-supervised learning approaches specifically designed for learning ego-exo view-invariant features, **time-contrastive networks (TCN)** and **ActorObserverNet**. Both require synchronized ego-exo videos for training, and are thus only applicable to Break Eggs data. Additionally, TCN offers a single-view variant that eliminates the need for synchronized data, by taking positive frames within a small temporal window surrounding the anchor, and negative pairs from distant timesteps. We implement **single-view TCN** for all datasets. (3) Fine-grained action representation learning approaches, **CARL** which utilizes a sequence contrastive loss, and **TCC** and **GTA**, which also utilize alignment as the pretext task but do not account for ego-exo viewpoint differences.

Main ResultsIn Table 1, we benchmark all baseline approaches and AE2 on four ego-exo datasets. See Supp. for full results, including the F1 score for few-shot classification, and mAP@5,15 for frame retrieval. From the results, it is clear that AE2 greatly outperforms other approaches across all downstream tasks. For instance, on Break Eggs, AE2 surpasses the multi-view TCN , which utilizes perfect ego-exo synchronization as a supervision signal, by +6.32% in F1 score. Even when we adapt the TCN to treat all possible ego-exo pairs as perfectly synchronized, AE2 still excels, showing a considerable margin of improvement (see Supp. for a discussion). Pour Liquid poses the most substantial challenge due to its in-the-wild nature, as reflected by the low phase progression and Kendall's Tau values. Yet, AE2 notably improves on previous works, achieving, for instance, a +9.64% F1 score. Regarding Tennis Forehand, another in-the-wild dataset, we note that some methods like TCC perform well in identifying correspondences within either the ego or exo domain, but struggle to connect the two. Consequently, while it achieves an 80.24% mAP@10 for standard frame retrieval, it falls below 60% for cross-view frame retrieval. In contrast, AE2 effectively bridges these two significantly different domains, resulting in a high performance of over 81% mAP@10

    &  &  &  &  & Kendall’s \\  & & regular & ego2exo & exo2ego & regular & ego2exo & exo2ego & Progression & Tau \\   & Random Features & 19.18 & 18.93 & 19.45 & 47.13 & 41.74 & 38.19 & -0.0572 & 0.0018 \\  & ImageNet Features & 50.24 & 21.48 & 32.25 & 50.49 & 33.09 & 37.80 & -0.1446 & 0.0188 \\  & ActorObserverNet  & 36.14 & 36.40 & 31.00 & 50.47 & 42.70 & 41.29 & -0.0517 & 0.0024 \\  & single-view TCN  & 56.90 & 18.60 & 35.61 & 53.42 & 32.63 & 34.91 & 0.0051 & 0.1206 \\  & multi-view TCN  & 59.91 & 48.65 & 56.91 & 58.83 & 47.04 & 52.68 & 0.2669 & 0.2886 \\  & CARL  & 43.43 & 28.35 & 29.22 & 46.04 & 37.38 & 39.94 & -0.0837 & -0.0091 \\  & TCC  & 59.84 & 54.17 & 52.28 & 58.75 & 61.11 & 62.03 & 0.2880 & 0.5191 \\  & GTA  & 56.86 & 52.33 & 58.35 & 61.55 & 56.25 & 53.93 & 0.3462 & 0.4626 \\  & AE2 (ours) & **66.23** & **57.41** & **71.72** & **65.85** & **64.59** & **62.15** & **0.5109** & **0.6316** \\   & Random Features & 36.84 & 33.96 & 41.97 & 52.48 & 50.56 & 51.98 & -0.0477 & 0.0050 \\  & ImageNet Features & 41.59 & 39.93 & 45.52 & 54.09 & 27.31 & 43.21 & -2.6681 & 0.0115 \\  & single-view TCN  & 47.39 & 43.44 & 42.28 & 57.00 & 46.48 & 47.20 & -0.3238 & -0.0197 \\  & CARL  & 48.79 & 52.41 & 43.01 & 55.01 & 52.99 & 51.51 & -0.1639 & 0.0443 \\  & TCC  & 77.91 & 72.29 & 81.07 & 80.97 & 75.30 & 80.27 & 0.6665 & 0.7614 \\  & GTA  & 81.11 & 74.94 & 81.51 & 80.12 & 72.78 & 75.40 & 0.7086 & 0.8022 \\   & AE2 (ours) & **85.17** & **84.73** & **82.77** & **84.90** & **78.48** & **83.41** & **0.7634** & **0.9062** \\   & Random Features & 45.26 & 47.45 & 44.33 & 49.83 & 55.44 & 57.55 & -0.1303 & -0.0072 \\  & ImageNet Features & 53.13 & 22.44 & 44.61 & 51.49 & 52.17 & 30.44 & -1.6329 & -0.0053 \\  & single-view TCN  & 54.02 & 32.77 & 51.24 & 48.83 & 55.28 & 31.15 & -0.5283 & 0.0103 \\  & CARL  & 56.98 & 47.46 & 52.68 & 55.29 & 59.37 & 36.80 & -0.1176 & 0.0085 \\  & TCC  & 52.53 & 43.85 & 42.86 & 62.33 & 56.08 & **57.89** & 0.1163 & **0.1103** \\  & GTA  & 56.92 & 42.97 & 59.96 & 62.79 & 58.52 & 53.32 & -0.2370 & 0.1005 \\  & AE2 (ours) & **66.56** & **57.15** & **65.00** & **65.54** & **65.79** & 57.35 & **0.1380** & 0.0934 \\   & Random Features & 30.31 & 33.42 & **28.10** & 66.47 & 58.98 & 59.87 & -0.0425 & 0.0177 \\  & ImageNet Features & 69.15 & 42.03 & 58.61 & 76.96 & 66.90 & 60.31 & -0.4143 & 0.0734 \\  & single-view TCN  & 68.87 & 48.86 & 36.48 & 73.76 & 55.08 & 56.65 & -0.0602 & 0.0737 \\  & CARL  & 59.69 & 35.19 & 47.83 & 69.43 & 54.83 & 63.19 & -0.1310 & 0.0542 \\  & TCC  & 78.41 & 53.29 & 32.87 & 80.24 & 55.84 & 47.27 & 0.2155 & 0.1040 \\  & GTA  & 83.63 & 82.91 & 81.80 & 85.20 & 78.00 & 79.14 & 0.4691 & 0.4901 \\  & AE2 (ours) & **85.87** & **84.71** & **85.68** & **86.83** & **81.46** & **82.07** & **0.5060** & **0.6171** \\   

Table 1: Benchmark evaluation on four datasets: (A) Break Eggs; (B) Pour Milk; (C) Pour Liquid; (D) Tennis Forehand. Top results are highlighted in **bold**, while second-best results are underlined. AE2 performs best across all tasks and datasets, in both regular and cross-view scenarios.

in both standard and cross-view settings. These results demonstrate the efficacy of AE2 in learning fine-grained ego-exo view-invariant features.

AblationTable 2 provides an ablation of the two design choices of AE2. Starting with a baseline approach (denoted as "Base DTW" in the table), we optimize a ResNet-50 encoder with respect to a DTW alignment objective as defined in Eqn. (3). Next, we replace the feature encoder with our proposed object-centric encoder design and report its performance. Finally, we incorporate the contrastive regularization from Eqn. (4) into the objective. It is evident that object-centric representations are instrumental in bridging the ego-exo gap, leading to substantial performance improvements. Furthermore, incorporating contrastive regularization provides additional gains.

Negative SamplingWe conduct experiments to validate the efficacy of creating negative samples by temporally reversing frames, as opposed to random shuffling and report results in Table 3. In general, temporally reversing frames yields superior and more consistent performance than randomly shuffling. The inferior performance of random shuffling can be related to the abundance of similar frames within the video sequence. Even after shuffling, the frame sequence may still emulate the natural progression of an action, thereby appearing more akin to a positive sample. Conversely, unless the frame sequence is strictly symmetric (a scenario that is unlikely to occur in real videos), temporally reversing frames is apt to create a negative sample that deviates from the correct action progression order. To further illustrate this point, we visualize two video sequences in Fig. 5. It is evident that the randomly shuffled sequence seems to preserve the sequence of actions like breaking eggs or pouring liquid, thereby resembling a "positive" example. Consequently, incorporating such negative samples into training may confuse the model and lead to diminished performance.

Qualitative ResultsFig. 5 (left) shows examples of cross-view frame retrieval on Pour Liquid and Tennis Forehand datasets. Given a query image from one viewpoint in the test set, we retrieve its nearest neighbor among all test frames in another viewpoint. We observe that the retrieved frames closely mirror the specific action state of the query frame. For example, when the query is a frame

    &  \\  & (A) & (B) & (C) & (D) \\  Base DTW & 58.53 & 82.91 & 59.66 & 79.56 \\ + object & 62.86 & 84.04 & 63.28 & 84.14 \\ + object + contrast & **66.23** & **85.17** & **66.56** & **85.87** \\   

Table 2: Ablation study of AE2 on four datasets (F1 score). See Supp. for other metrics.

Figure 4: Visualization of (a) the original video sequence; (b) a randomly shuffled sequence; and (c) a temporally reversed sequence on Break Eggs (left) and Pour Liquid (right). The randomly shuffled sequence may still preserve the action progression order due to many similar frames in the video sequence, failing to be truly “negative”. In contrast, temporally reversing frames results in a more distinct and suitable negative example.

capturing the moment before the pouring action, where the liquid is still within the container (row 1), or when the query shows pouring actively happening (row 2), the retrieved exo frames capture that exact step as well. Fig. 5 (right) shows tSNE visualizations of embeddings for 4 test videos on Break Eggs. Our learned embeddings effectively capture the progress of an action, while remaining view-invariant. These learned embeddings enable us to transfer the pace of one video to others depicting the same action. We demonstrate this capability with examples of both ego and exo videos, collected in distinct environments, playing synchronously in the Supp. video.

LimitationsWhile AE2 has demonstrated its ability to handle in-the-wild data, there remains much room for improvement, particularly on challenging in-the-wild datasets such as Pour Liquid. Here, we observe instances of failure in cross-view frame retrieval and synchronizing the pace of two videos. Although we highlight successful examples in our discussion, it is important to note that some attempts have been unsuccessful, largely due to the inherent noise in real-world videos. This represents a key area for future refinement and development of AE2.

## 6 Conclusion

We tackle fine-grained temporal understanding by jointly learning from _unpaired_ egocentric and exocentric videos. Our core idea is to leverage temporal alignment as the self-supervised learning objective for invariant features, together with a novel object-centric encoder and contrastive regularizer that reverses frames. On four datasets we achieve superior performance compared to state-of-the-art ego-exo and alignment approaches. This fundamental technology has great promise for both robot learning and human skill learning in AR, where an ego actor needs to understand (or even match) the actions of a demonstrator that they observe from the exo point of view. Future work includes generalizing AE2 to train from pooled data of multiple actions, extending its applicability to more complex, longer action sequences and exploring its impact for robot imitation learning.

Figure 5: Qualitative results of AE2. Left: Cross-view frame retrieval results on Pour Liquid and Tennis Forehand. Our view-invariant embeddings can be effectively used for fine-grained frame retrieval; Right: tSNE trajectories of 4 test videos (2 ego and 2 exo) in the embedding space on Break Eggs. Shading indicates time. The learned embeddings successfully capture the progress of an action and maintain invariance across the ego and exo viewpoints. See Supp. for more examples.