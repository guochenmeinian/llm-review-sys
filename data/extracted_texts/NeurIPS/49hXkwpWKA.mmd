# AHA: Human-Assisted Out-of-Distribution Generalization and Detection

Haoyue Bai, Jifan Zhang, Robert Nowak

University of Wisconsin-Madison

{baihaoyue, jifan}@cs.wisc.edu, rdnowak@wisc.edu

###### Abstract

Modern machine learning models deployed often encounter distribution shifts in real-world applications, manifesting as covariate or semantic out-of-distribution (OOD) shifts. These shifts give rise to challenges in OOD generalization and OOD detection. This paper introduces a novel, integrated approach AHA (**A**daptive **H**uman-**A**ssisted OOD learning) to simultaneously address both OOD generalization and detection through a _human-assisted framework_ by labeling data in the wild. Our approach strategically labels examples within a novel maximum disambiguation region, where the number of semantic and covariate OOD data roughly equalizes. By labeling within this region, we can maximally disambiguate the two types of OOD data, thereby maximizing the utility of the fixed labeling budget. Our algorithm first utilizes a noisy binary search algorithm that identifies the maximal disambiguation region with high probability. The algorithm then continues with annotating inside the identified labeling region, reaping the full benefit of human feedback. Extensive experiments validate the efficacy of our framework. We observed that with only a few hundred human annotations, our method significantly outperforms existing state-of-the-art methods that do not involve human assistance, in both OOD generalization and OOD detection. Code is publicly available at https://github.com/HaoyueBaiZJU/aha.

## 1 Introduction

Modern machine learning models deployed in the real world often encounter various types of distribution shifts. For example, out-of-distribution (OOD) covariate shifts arise when the domain and environment of the test data differ from the training data. OOD semantic shifts occur when the model encounters novel classes during testing. This gives rise to two important challenges: OOD generalization , which addresses distribution mismatches between training and test data related to covariate shifts, and OOD detection , which aims to identify examples from semantically unknown categories that should not be predicted by the classifier, relating to semantic shifts. The natural coexistence of these different distribution shifts in real-world scenarios motivates the simultaneous handling of both tasks, a direction that has not been largely explored previously, as most existing approaches are highly specialized in one task.

Specifically, we consider a generalized characterization of the wild data setting  that naturally arises in the model's operational environment:

\[_{}:=(1-_{s}-_{c})_{}+_{c} _{}^{}+_{s}_{}^{ },\]

where \(_{}\) denotes the marginal distributions of in-distribution (ID) data, \(_{}^{}\) represents covariate-shifted OOD data, and \(_{}^{}\) indicates semantic-shifted OOD data. This is challenging as we lack access to both the category labels and distribution types of this wild mixture data, which is crucial for OOD learning. To tackle this challenge, it is natural to develop a human-assisted framework and

[MISSING_PAGE_FAIL:2]

domain adaptation tasks [12; 36; 89; 57; 32; 99; 74], OOD generalization is more critical as it focuses on generalizing to covariate-shifted data distributions that are unseen during training [8; 61; 114; 71; 40; 69; 9; 58]. A primary set of approaches to OOD generalization involves extracting domain-invariant representations. Strategies include invariant risk minimization [2; 79; 111; 1], domain adversarial learning [65; 115; 86; 37; 66], meta-learning [63; 76], and others [78; 16; 7]. Other sets of approaches for OOD generalization include single domain generalization [75; 92], test-time adaptation [52; 110], and model ensembles [3; 77]. SCONE  aims to enhance OOD generalization and detection by leveraging unlabeled data from the wild. Based on the problem setting of SCONE, we propose to integrate human assistance to enhance OOD robustness and improve OOD detection accuracy. The primary motivation is to identify the optimal labeling regions within the wild data. We find that even a few hundred human-labeled instances, chosen based on our selection criteria, can significantly enhance performance for both tasks.

**Out-of-distribution detection** has gained increasing attention in recent years. There are primarily two sets of approaches to OOD detection: post hoc methods and regularization-based methods. Post hoc methods involve designing OOD scores at test time, which include confidence-based methods , energy-based scores [68; 109], gradient-based scores [10; 26], and distance-based scores [62; 91]. Another set of approaches involves leveraging training-time regularization for OOD detection by relying on an additional clean set of semantic OOD data [47; 44; 100]. Some recent studies propose utilizing wild mixture data for OOD detection. For example, WOODS  considers a wild mixture of both unlabeled ID and semantic OOD data. SCONE  includes a wild mixture of unlabeled ID, semantic OOD, and covariate OOD data, which are suitable for real-world scenarios. In contrast to previous work, we propose a human-assisted approach for the wild mixture setting and observe that only a few hundred human annotations can significantly improve robustness and OOD detection. Unlike , which utilizes adaptive human review for OOD detection via a fixed false positive rate threshold, our approach is fundamentally different. We collect informative examples to finetune the model and OOD detector, simultaneously improving OOD detection and generalization.

**Noisy binary search.** Our algorithm utilizes a noisy binary search algorithm to find the threshold where the density difference between covariate OOD and semantic OOD examples flip from negative to positive. In traditional binary search, one simply shrinks the possible interval of the threshold by half based on the observation of either a negative or a positive signal. However, in noisy binary search, the observations are inherently noisy with some probabilities. As a result, one necessarily needs to maintain a _high probability confidence interval_ of where the threshold may be located. The noisy binary search problem has been primarily studied in combinatorial bandits [18; 34; 17; 15; 33; 54] and agnostic active learning [22; 41; 42; 23; 49; 53; 56]. We primarily utilize a version of the fix-budget algorithm from  as it is proven to be near instance-optimal. In the past, noisy binary search algorithms have been widely applied in applications such as text classification , wireless networks [87; 88] and training neural networks on in-distribution data [108; 73].

**Deep active learning** is a vital paradigm in machine learning that emphasizes the selection of the most informative data points for labeling, enabling efficient and effective model training with limited labeled data . There are two main groups of algorithms: uncertainty sampling and diversity sampling. Uncertainty sampling aims to identify and select data examples where model confidence is low in order to reduce uncertainty when labeled [35; 28; 11; 97]. Diversity sampling aims to query a batch of diverse examples that are representative of the unlabeled pool for the overall data distribution [85; 39; 38; 112; 20]. Recently, some hybrid methods have arisen that consider both uncertainty sampling and diversity sampling, which query a batch of informative and diverse examples [5; 4; 20; 70]. Another line of work is deep active learning with class imbalance [21; 59; 31]. Some recent advances consider distribution shifts in the context of deep active learning [105; 13]. In this work, we consider OOD robustness and tackle the challenging scenario of unlabeled wild distributions, training a robust multi-class classifier and an OOD detector simultaneously.

## 3 Problem Setup

**Labeled in-distribution data.** Let \(\) denote the input space and the label space \(:=[K]\) consists of \(K\) classes. We have access to an initial labeled training set of \(M\) examples \(_{}_{}^{M}\).

**Unlabeled wild data.** When a model is deployed into a wild environment, it encounters unlabeled examples that exhibit various distributional shifts. We consider a generalized characterization of the 

[MISSING_PAGE_FAIL:4]

Let \(_{}^{}\), \(_{}^{}\) and \(_{}^{}\) denote the sets of ID, covariate OOD and semantic OOD data in \(_{}\) respectively. When collecting human feedback, the ideal outcome is to label examples that best separate ID and covariate OOD examples from the semantic OOD ones. This may be achieved by labeling _the highest score covariate OOD examples_ and _the lowest score semantic OOD examples_. Let \(_{}:=_{_{}^ {}}g()\) and \(_{}:=_{_{}^{}}g()\) denote the scores of the most covariate and the least semantic OOD examples. For analysis purposes, we propose the following three oracular labeling regions with a labeling budget of \(k\):

* **Most covariate OOD**: Label the top-\(k\) OOD score examples from \(\{_{}:g()_{ }\}\).
* **Least semantic OOD**: Label the bottom-\(k\) OOD score examples from \(\{_{}:g()_ {}\}\).
* **Mixture of the two**: Allocate half of the budget \(\) to **most covariate OOD** and the remaining half \(\) to **least semantic OOD**, combining the two subsets.

In practice, since \(_{}\) and \(_{}\) are unknown, one may opt for the following two surrogate practical labeling regions:

* **Top-\(k\) most OOD examples**: As a surrogate to the **most covariate OOD** labeling region, we label the top-\(k\) OOD score examples from \(_{}\) (see Figure 1 (a)).
* **Near-boundary examples**: As a surrogate to the **least semantic OOD** labeling region, we label \(k\) examples closest to the \(95\%\) TPR threshold from both sides (see Figure 1 (b)). We choose the threshold based on the labeled ID data \(_{}\), which captures a substantial fraction of ID examples (e.g., 95%), and is commonly defined as the ID vs OOD boundary in OOD detection literature.

**Limitations in OOD Learning Performance of Baseline Labeling Regions.** We conducted a case study on the five novel baseline labeling regions listed in Table 2. Although our proposed novel oracle **Most covariate OOD region** targets selecting covariate OOD with the highest scores, it performs poorly in wild settings. Most selected examples turn out to be semantic OOD near \(_{}\), which does not aid in OOD generalization as expected. Similarly, the oracle **Least semantic OOD region** aims to identify semantic OOD examples with the lowest scores, and mostly ends up labeling ID and covariate OOD examples near \(_{}\). The **Mixed range** achieves performance somewhere in between the two. We observe a similar phenomenon for the practical **Top-\(k\) most examples region** and **Near-boundary region**. The above labeling regions are not as effective one might hope. This is primarily due to the dominant number of the other types of data around the most covariate OOD and least semantic OOD examples, which are not informative. This motivates us to label examples where the density of the two types of OOD examples roughly equalizes--the maximum disambiguation region. Empirically, as shown in Table 2, we observe that labeling within this region can significantly improve overall performance in both oracle and practical settings.

### Maximum Disambiguation Region

In this section, we formally introduce the maximum disambiguation region (see Figure 1(c)), centered around the _maximum ambiguity threshold_. While we hope to find the threshold where the densities of semantic and covariate OOD examples equalize, it is impossible to distinguish between covariate OOD examples from ID examples based on human labels. Therefore, we formally define the threshold as the OOD score where the weighted density of semantic OOD examples is equal to that of covariate OOD and ID examples combined.

Concretely, given the OOD scoring function \(g\), we let \(p_{}()\) be the probability density of \(g()\) when \(\) is drawn from the covariate OOD distribution. That is, \(_{0}^{}p_{}()d\) is the probability that an \(\) drawn from the covariate OOD distribution has a score less than or equal to \(\). Similarly, we define \(p_{}\) and \(p_{}\) as the probability densities of \(g()\) when \(\) is drawn from ID and semantic OOD distributions respectively. Recall \(_{c}\) and \(_{s}\) are the prior probabilities of x coming from the covariate and semantic OOD distributions, we define the maximum ambiguity threshold as follows.

**Definition 1** (Maximum Ambiguity Threshold).: _Given the OOD scoring for all wild data points, we define the maximum ambiguity threshold as the CDF of the two categories of examples is maximized:_

\[_{*}=*{arg\,max}_{}_{0}^{}((1-_{c }-_{s})p_{}()+_{c}p_{}())-_{s}p_{ }()d.\] (1)

_Ties are broken by choosing the \(\) value closest to the median of the OOD scores of the wild examples._

Note that under benign continuity assumptions, we necessarily have \((1-_{c}-_{s}) p_{}(^{*})+_{c}p_{}( ^{*})=_{s}p_{}(^{*})\), where the weighted densities of the two distributions equalize.

Through a different lens, the threshold \(_{*}\) also corresponds to where the current OOD detector is most _uncertain_ about its prediction. In fact, when we label around this threshold, we make the maximum number of corrections to the OOD detector's predictions, correcting at least half of the examples to their appropriate categories.

**Reduction to noisy binary search.** At the essence, the above is a noisy binary search problem. When labeling an examples \(\) with OOD score \(=g()\), the outcome is a Bernoulli-like random variable. Specifically, one observes a class label \(y[K]\) with probability \(p_{}()+p_{}()\), and an \(y=\) label with probability \(p_{}()\). When given a labeled set \(=\{(}_{i},_{i})\}_{i[n]}\) of size \(n\), by finite sample approximation, equation (1) can be further derived as

\[_{}_{0}^{}((1-_{c}-_{s})p_{ {in}}()+_{c}p_{}())-_{s}p_{}()d\] (2) \[ _{}|\{y_{i}:(_{i}, y_{i}),g(_{i})\}|-|\{y_{i}=:( _{i},y_{i}),g(_{i})\}|.\] (3)

### Algorithm

Our algorithm AHA consists of two main steps: (1) We propose identifying the maximum ambiguity threshold by leveraging an off-the-shelf adaptive labeling algorithm . This threshold is determined by equation 2, where the cumulative number of ID and covariate OOD examples most dominate that of semantic OOD examples. (2) We then annotate an equal number of examples on both sides of this identified maximum ambiguity threshold, establishing the maximum disambiguation region.

Specifically, as shown in Algorithm 1, AHA starts by initializing an empty set for labeled examples and a broad confidence interval for the maximum ambiguity threshold. During the first phase, the algorithm iteratively and adaptively labels more examples. Over the annotation period, our algorithm maintains a confidence interval \([,]\) with high probability, ensuring that the maximum ambiguity threshold \(_{*}[,]\) lies within this interval with high probability. During each iteration of the first phase, we uniformly at random label an example within this confidence interval. Upon obtaining the label, we update the confidence interval using a subprocedure called **ConfUpdate**. This subprocedure shrinks the interval based on the labeled examples, ensuring it converges to an accurate threshold over time with statistical guarantees. The detailed implementations of **ConfUpdate** and its theoretical foundations are discussed in Appendix A and  respectively. Overall, we spend half of our labeling budget during the first phase. During the second phase, we then spend the remaining half of the budget labeling examples around the identified threshold. Finally, the classifier and the OOD detector are trained on the combined set of initially labeled and newly annotated examples.

``` Input: OOD detector \(g\) trained on \(_{}\), wild set of examples \(_{}=\{_{i}\}_{i=1}^{N}\), budget \(k\) Initialize:\(_{}\{\}\), confidence interval \(,-,\) Spend half budget searching for maximum ambiguity threshold for\(t=1,...,\)do  Sample \(}_{t}\) uniformly at random from \(\{_{}_{}:  g()\}\)  Ask human for label on \(}_{t}\), observe \(_{t}\), and insert the example \((}_{t},_{t})\) into \(_{}\)  Update confidence interval \(,(_{}, _{},g,,)\) endfor Spend half budget labeling around identified threshold ```

**Algorithm 1** AHA: Adaptive Human Assisted labeling for OOD learning

### Learning Objective

Let \(_{}^{}\) denote the set of annotated covariate examples, and \(_{}^{}\) represent the set of annotated semantic examples from wild data. Our learning framework jointly optimizes two objectives: (1) multi-class classification of examples from \(_{}\) and covariate OOD \(_{}^{}\), and (2) a binary OOD

[MISSING_PAGE_FAIL:7]

analysis of different labeling budgets 100, 500, 1000, 2000 in Section 5.3. In our experiment, the output of \(g_{}\) is utilized as the score for OOD detection.

### Main Results and Discussion

**Results on benchmark for both OOD generalization and detection.** Table 3 provides a comparative analysis of various OOD generalization and detection methods on the CIFAR benchmark, evaluating their performance across different semantic OOD datasets including SVHN, LSUN-C, and Textures. AHA shows significant improvements for both OOD generalization and OOD detection tasks, suggesting a robust method for handling OOD scenarios.

Specifically, we compare AHA with three groups of methods: (1) methods developed for OOD generalization, including IRM , GroupDRO , Mixup , VREx , EQRM , and the more recent SharpDRO ; (2) methods tailored for OOD detection, including MSP , ODIN , Energy , Mahalanobis , ViM , KNN , and the more recent ASH ; and (3) methods that are trained with unlabeled data from the wild, including Outlier Exposure , Energy-based Regularized Learning , WOODS , and SCONE .

We highlight some key observations: (1) AHA achieves superior performance compared to specifically designed OOD generalization baselines. These baselines struggle to distinguish between ID data and semantic OOD data, leading to poor OOD detection performance. Additionally, our method selects the optimal region and involves human labeling to retrain the model using the selected examples, thus leading to better generalization performance compared to other OOD generalization baselines. (2) Our approach achieves superior performance compared to OOD detection baselines. Methods specifically designed for OOD detection, which aim to identify and separate semantic OOD, show suboptimal OOD accuracy. This demonstrates that existing OOD detection baselines struggle with covariate distribution shifts. (3) Compared with strong baselines trained with wild data, AHA consistently outperforms existing learning with wild data baselines. Specifically, our approach surpasses the current state-of-the-art (SOTA) method, SCONE, by 31.52% in terms of FPR95 on the Texture OOD dataset and simultaneously improves the OOD accuracy by 4.95%. This demonstrates the robust effectiveness of our method for both OOD generalization and detection tasks.

**Additional results on PACS.** Table 4 presents our results on the PACS dataset  from DomainBed . We compare AHA against various common OOD generalization baselines, including IRM , DANN , CDANN , GroupDRO , MTL , I-Mixup , MMD , VREx , MLDG , ARM , RSC , Mixstyle , ERM , CORAL , SagNet , SelfReg , GVRT , VNE , and the most recent baseline HYPO . Our method achieves an average accuracy of 92.7%, outperforming these OOD generalization baselines.

  
**Sampling score** & **OOD Acc.\(\)** & **ID Acc.\(\)** & **FPR\(\)** & **AUROC\(\)** \\ 
**Random** & 89.22 & 94.84 & 9.45 & 95.41 \\
**Least confidence** & 90.08 & 94.40 & 5.29 & 97.94 \\
**Entropy** & 89.99 & 94.50 & 5.35 & 97.75 \\
**Margin** & 90.10 & 94.55 & **4.15** & **98.53** \\
**Energy score** & 89.58 & 94.73 & 6.37 & 97.26 \\
**Gradient-based** & **90.1** & 94.54 & 5.63 & 97.95 \\   

Table 6: Ablation on labeling budget \(k\). We train on CIFAR-10 as ID, using wild data with \(_{c}=0.4\) (CIFAR-10-C) and \(_{s}=0.3\) (Texture).

  
**Algorithm** & **Art** & **Cartoon** & **Photo** & **Sketch** & **Average** \\ 
**IRM** & 84.8 & 76.4 & 96.7 & 76.1 & 83.5 \\
**DANN** & 86.4 & 77.4 & 97.3 & 73.5 & 83.7 \\
**CDANN** & 84.7 & 75.5 & 96.8 & 73.5 & 82.6 \\
**GroupDRO** & 83.5 & 79.1 & 96.7 & 78.3 & 84.4 \\
**MTL** & 87.5 & 77.1 & 96.4 & 77.3 & 84.6 \\
**1-Mixup** & 86.1 & 78.9 & 97.6 & 75.8 & 84.6 \\
**MMD** & 86.1 & 79.4 & 96.6 & 76.5 & 84.7 \\
**WREs** & 86.0 & 79.1 & 96.9 & 77.7 & 84.9 \\
**MLP** & 85.5 & 80.1 & 97.4 & 76.6 & 84.9 \\
**ARM** & 86.8 & 76.8 & 97.4 & 79.3 & 85.1 \\
**BSC** & 85.4 & 79.7 & 97.6 & 78.2 & 85.2 \\
**Mixstyle** & 86.8 & 79.0 & 96.6 & 78.5 & 85.2 \\
**ERM** & 84.7 & 80.8 & 97.2 & 79.3 & 85.5 \\
**CORAL** & 88.3 & 80.0 & 97.5 & 78.8 & 86.2 \\
**SagNet** & 87.4 & 80.7 & 97.1 & 80.0 & 86.3 \\
**SelfReg** & 87.9 & 79.4 & 96.8 & 78.3 & 85.6 \\
**GVRT** & 87.9 & 78.4 & 98.2 & 75.7 & 85.1 \\
**VNE** & 88.6 & 79.9 & 96.7 & 82.3 & 86.9 \\
**HYPO** & 90.5 & 84.6 & 97.7 & 83.2 & 89.0 \\
**AHA**(**Ours) & **92.6** & **93.5** & **98.7** & **86.1** & **92.7** \\   

Table 4: Comparison with domain generalization methods on the PACS benchmark. We followed the same leave-one-domain-out validation experimental protocol as in . All methods are trained on ResNet-50. The model selection is based on a training domain validation set.

  
**Algorithm** & **Art** & **Cartoon** & **Photo** & **Sketch** & **Average** \\ 
**IRM** & 84.8 & 76.4 & 96.7 & 76.1 & 83.5 \\
**DANN** & 86.4 & 77.4 & 97.3 & 73.5 & 83.7 \\
**CDANN** & 84.6 & 75.5 & 96.8 & 73.5 & 82.6 \\
**GroupDRO** & 83.5 & 79.1 & 96.7 & 78.3 & 84.4 \\
**MTL** & 87.5 & 77.1 & 96.4 & 77.3 & 84.6 \\
**1-Mixup** & 86.1 & 78.9 & 97.6 & 75.8 & 84.6 \\
**MMD** & 86.1 & 79.4 & 96.6 & 76.5 & 84.7 \\
**WREs** & 86.0 & 79.1 & 96.9 & 77.7 & 84.9 \\
**MLP** & 85.5 & 80.1 & 97.4 & 76.6 & 84.9 \\
**ARM** & 86.8 & 76.8 & 97.4 & 79.3 & 85.1 \\
**BSC** & 85.4 & 79.7 & 76.8 & 78.2 & 85.2 \\
**Mixstyle** & 86.8 & 79.0 & 96.6 & 78.5 & 85.2 \\
**ERM** & 84.7 & 80.8 & 97.2 & 79.3 & 85.5 \\
**CORAL** & 88.3 & 80.0 & 97.5 & 78.8 & 86.2 \\
**SagNet** & 87.4 & 80.7 & 97.1 & 80.0 & 86.3 \\
**SelfReg** & 87.9 & 79.4 & 96.8 & 78.3 & 85.6 \\
**GVRT** & 87.9 & 78.4 & 98.2 & 75.7 & 85.1 \\
**VNE** & 88.6 & 79.9 & 96.7 & 82.3 & 86.9 \\
**HYPO** & 90.5 & 84.6 & 97.7 & 83.2 & 89.0 \\
**AHA**(**Ours) & **92.6** & **93.5** & **98.7** & **86.1** & **92.7** \\   

Table 5: Impact of sampling scores with our selection strategy. We use budget \(k=1000\) for all methods. We train on CIFAR-10 as ID, using wild data with \(_{c}=0.5\) (CIFAR-10-C) and \(_{s}=0.1\) (Texture).

### Ablation Studies

**Effect of different scores.** Different OOD scores play a crucial role in identifying various distributions and impacting the selection process. To evaluate the effectiveness of different OOD scores within our framework, we conducted an ablation study (see Table 5). Detailed descriptions of the different OOD scores can be found in Appendix C. The scores include least-confidence [97; 46], entropy , margin , energy score , gradient-based . We also compared our approach with random sampling, which serves as a straightforward baseline method involving the random selection of \(k\) examples to query. We observe that AHA consistently achieves superior performance when combined with various sampling scores for OOD generalization and detection, and it consistently outperforms the random sampling baseline. The gradient-based score demonstrates the best overall performance in terms of OOD accuracy and FPR. This also shows that AHA can be easily integrated with existing sampling scores.

**Effect on different labeling budgets \(k\).** In Table 6, we provide ablations on different labeling budgets \(k\) from 100, 500, 1000, 2000. We observe that both OOD generalization and detection performance improve with an increasing labeling budget. For instance, our method's OOD accuracy increased from 79.77% to 90.46% when the budget increased from 100 to 2000. Simultaneously, the TPR decreased from 17.55% to 2.04%, which also indicates a significant improvement in OOD detection performance. Moreover, AHA consistently outperforms the practical top-\(k\) OOD example sampling strategy across different labeling budgets.

### Qualitative Analysis

**Visualization of OOD score distributions.** Figure 2 (a) and (b) present feature embedding visualizations using t-SNE  on the test data. The blue points represent the ID test data (CIFAR-10), green points represent OOD test examples from CIFAR-10-C, and gray points are from the Texture dataset. We observe that (1) the embeddings of the ID data \(_{}\) (CIFAR-10) and the covariate shift OOD data \(_{}^{}}\) (CIFAR-10-C) are more closely aligned, and (2) the embeddings of the semantic shift OOD data \(_{}^{}}\) (Texture) are better separated from the ID and covariate shift OOD data using our method. This contributes to enhanced OOD generalization and OOD detection performance.

**Visualization of OOD score distributions.** Figure 2 (c) and (d) visualize the score distributions using kernel density estimation (KDE) for the baseline and our method. The OOD score distributions between the ID data (\(_{}\)) and the semantic OOD data (\(_{}^{}}\)) are more separated using our method. This separation represents an improvement in OOD detection performance, demonstrating the effectiveness of AHA in identifying semantic OOD data.

## 6 Conclusion

In this study, we introduce the first human-assisted framework designed to simultaneously address OOD generalization and OOD detection by leveraging wild data. We propose a novel labeling strategy that selects the maximum disambiguation region, strategically utilizing human labels to maximize model performance amid covariate and semantic shifts. Extensive experiments demonstrate that AHA effectively enhances both OOD generalization and detection performance. This research establishes a solid foundation for further advancements in OOD learning within dynamic environments characterized by heterogeneous data shifts.

Figure 2: (a)-(b): T-SNE visualization of the image embeddings for ERM vs. AHA (ours). (c)-(d) Score distributions for ERM vs. AHA (ours). Different colors represent the different types of test data: CIFAR-10 as \(_{}\) (blue), CIFAR-10-C as \(_{}^{}}\) (green), and Textures as \(_{}^{}}\) (gray).