# Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandits

Tian Huang\({}^{1}\), Shengbo Wang\({}^{1}\), Ke Li\({}^{2}\)

\({}^{1}\) School of Computer Science and Engineering,

University of Electronic Science and Technology of China

\({}^{2}\) Department of Computer Science, University of Exeter

tianhuang.uestc@gmail.com shnbo.wang@foxmail.com k.li@exeter.ac.uk

Correspondence: k.li@exeter.ac.uk; \({}^{}\) Equal contributions.

###### Abstract

The ultimate goal of multi-objective optimization (MO) is to assist human decision-makers (DMs) in identifying solutions of interest (SOI) that optimally reconcile multiple objectives according to their preferences. Preference-based evolutionary MO (PBEMO) has emerged as a promising framework that progressively approximates SOI by involving human in the _optimization-cum-decision-making_ process. Yet, current PBEMO approaches are prone to be inefficient and misaligned with the DM's true aspirations, especially when inadvertently exploiting mis-calibrated reward models. This is further exacerbated when considering the stochastic nature of human feedback. This paper proposes a novel framework that navigates MO to SOI by _directly_ leveraging human feedback without being restricted by a predefined reward model nor cumbersome model selection. Specifically, we developed a clustering-based stochastic dueling bandits algorithm that strategically scales well to high-dimensional dueling bandits. The learned preferences are then transformed into a unified probabilistic format that can be readily adapted to prevalent EMO algorithms. This also leads to a principled termination criterion that strategically manages human cognitive loads and computational budget. Experiments on \(48\) benchmark test problems, including the RNA inverse design and protein structure prediction, fully demonstrate the effectiveness of our proposed approach.

## 1 Introduction

Multi-objective optimization (MO) represents a fundamental challenge in artificial intelligence , with profound implications that span virtually every sector--from scientific discovery  to engineering design , and societal governance . In MO, there is no single _utopian solution_ that optimizes all objectives; instead, the Pareto front (PF) comprises non-dominated solutions, each representing an efficient yet incomparable trade-off between objectives. The goal of MO is to assist human decision-makers (DMs) in identifying solutions of interest (SOI) that optimally reconcile these conflicting objectives according to their preferences. This field has been a subject of rigorous study within the multi-criterion decision-making (MCDM) community  for more than half a century. Over the past two decades, we have witnessed a seamless transition from purely analytical methodologies to a burgeoning interest in interactive evolutionary meta-heuristics, known as preference-based evolutionary MO (PBEMO) .

As in Figure 1(**a**), a PBEMO method involves three building blocks. The optimization module uses a population-based meta-heuristics to explore the search space. The preference information is progressively learned by periodically involving human DM in the consultation module to providepreference feedback. The learned preference representation is then transformed into the format that guides the evolutionary search progressively towards SOI in the preference elicitation module. The overall PBEMO process is DM-oriented and is an _optimization-cum-decision-making_ process. While PBEMO has been extensively studied in the literature in the past three decades (e.g., ), there are several fundamental issues unsolved, especially in the consultation and preference elicitation modules, that significantly hamper the further uptake in real-world problem-solving scenarios.

* The consultation module serves as the interface for DM interaction with the optimization module. It queries the DM about their preferences to effectively recommend solutions. This closely aligns with preference learning in the machine learning community. There are three strategies to accomplish this. The first, as shown in Figure 1(**b**-1), involves reward models  or ranking mechanisms . In this approach, DMs often provide scores or rankings for a large set of solutions. However, this overburdens the DM and risks introducing errors into the optimization process due to overlooked human cognitive limitations. The other two strategies are grounded in the dueling bandits settings , utilizing pairwise comparisons in consultation but with different assumptions about human feedback. Specifically, the second strategy relies on a parameterized transitivity model  or a structured utility function for dueling feedback, such as the Bradley-Terry-Luce model  in Figure 1(**b**-2). While theoretically appealing, this method faces a challenge in model selection that can be as complex as the original problem, posing difficulties in practical applications. Different from the previous two strategies, the last one tackles human feedback as stochastic events such as Bernoulli trials , and it learns DM's preferences from their feedback as shown in Figure 1(**b**-3). However, a key bottleneck here is the souring number of DM queries required when considering a population of solutions in PBEMO.
* The preference elicitation module acts as a catalyst, transforming the preference information learned in the consultation module--usually not directly applicable--into a format usable in the underlying EMO algorithm. The stochastic nature of human feedback  can lead to a misuse of learned preferences that adversely disturb search processes. This issue is pronounced in PBEMO contexts, where the number of consultations is constrained to reduce the human cognitive burden. Additionally, there is no thumb rule for determining the frequency of DM interactions or terminating such interactions, where current methods are often heuristics (e.g., setting a fixed number of interactions ).

In this paper, we propose a novel direct PBEMO framework (dubbed D-PBEMO) that directly leverages DM's feedback to guide the evolutionary search for SOI. Note that it neither relies on any reward model nor cumbersome model selection. Our D-PBEMO framework consists of two key features.

* Given the stochastic nature of human feedback, we develop a novel clustering-based stochastic dueling bandits algorithm in the consultation module. It is model-free and its regret

Figure 1: (**a**) Flow chart of a conventional PBEMO. (**b**) Conceptual illustration of reward-based, model-based, and direct preference learning strategies.

is \((K^{2} T)\), where \(K\) is the number of clusters and \(T\) is the number of rounds. This overcomes the challenge of substantial queries inherent in conventional dueling bandits .
* The preference elicitation module transforms the learned preferences from the consultation module into a unified probabilistic format, in which the associated uncertainty represents the stochasticity involved in preference learning. This not only streamlines the incorporation of learned preferences into the optimization module to guide EMO to search for SOI, but also constitutes a principled termination criterion that strategically manages human cognitive burden and the computational budget.

## 2 Preliminaries

### Multi-Objective Optimization Problem

The MO problem is formulated as: \(_{}()=(f_{1}(), ,f_{m}())^{}\), where \(=(x_{1},,x_{n})^{}\) is an \(n\)-dimensional decision vector and \(()\) is an \(m\)-dimensional objective vector whose \(i\)-th element is the objective mapping \(f_{i}:\), where \(\) is the feasible set in the decision space \(^{n}\). Without considering the DM's preference information, given \(^{1},^{2}\), \(^{1}\) is said to dominate \(^{2}\) (denoted as \(^{1}^{2}\)) iff \( i\{1,,m\}\) we have \(f_{i}(^{1}) f_{i}(^{2})\) and \((^{1})(^{2})\). A solution \(\) is said to be Pareto-optimal iff \(^{}\) such that \(^{}\). The set of all Pareto-optimal solutions is called the Pareto-optimal set (PS) and their corresponding objective vectors constitute the PF.

The ultimate goal of MO is to identify the SOI from the PS satisfying DM's preference. It consists of two tasks: 1 searching for Pareto-optimal solutions that cover SOI, and 2 steering these solutions towards the SOI. PBEMO addresses the task 1 by employing an EMO algorithm as a generator of an evolutionary population of non-dominated solutions \(=\{^{i}\}_{i=1}^{N}\), striking a balance between convergence and diversity for coverage. For the task 2, PBEMO actively queries DM for preference information regarding these generated solutions, then it leverages the learned preferences to guide the EMO algorithm to approximate the SOI.

### Preference Learning as Dueling Bandits

Since human feedback from relative comparisons is considerably more reliable than absolute labels , we focus on pairwise comparisons as a form of indirect preference information. In PBEMO, a DM is asked to evaluate pairs of solutions \(^{i},^{j}\) selected from \(\), where \(i,j\{1,,N\}\) and \(i j\). The DM's task is to decide, based on her/his preferences, whether \(^{i}\) is better, worse, or equivalent to \(^{j}\), denoted as \(^{i}_{}^{j}\), \(^{i}_{}^{j}\), or \(^{i}_{}^{j}\). Regarding stochastic preference, there is a preference matrix for \(K\)-armed dueling bandits defined as \(=[p_{i,j}]_{K K}\), where \(p_{i,j}\) is the winning probability of the \(i\)-th arm over the \(j\)-th arm . In particular, we have \(p_{i,j}+p_{j,i}=1\) with \(p_{i,i}=0.5\). The \(i\)-th arm is said to be superior to the \(j\)-th one iff \(p_{i,j}>0.5\). Simply considering each solution as an individual arm will yield \(K=N\), which suffers efficiency in targeting the SOI when the evolutionary population is large . The ranking of all arms is determined by their Copeland scores, where the SOI should be the Copeland winners that have the biggest Copeland scores.

**Definition 2.1** ().: The normalized Copeland score of the \(i\)-th arm, \(i\{1,,K\}\), is given by:

\[_{i}=_{j i,j\{1,,K\}}(p_{i,j }>0.5),\] (1)

where \(()\) is an indicator function. Arm \(k^{}\) satisfying \(k^{}=}{}\)\(_{i}\) is the Copeland winner.

The goal of dueling bandits algorithm is to identify the Copeland winner among all candidate arms with no prior knowledge of \(\). To this end, a winning matrix is introduced as \(=[b_{i,j}]_{K K}\) to record the pairwise comparison labels, where \(b_{i,j}\) is the number of time-slots when the \(i\)-th arm is preferred from pairs of \(i\)-th and \(j\)-th arms. Consequently, we can approximate the preference probability with mean \(_{i,j}=}{b_{i,j}+b_{j,i}}\), whose upper confidence bound \(u_{i,j}\) and lower confidence bound \(l_{i,j}\) can be quantified as:

\[u_{i,j}=}{b_{i,j}+b_{j,i}}++b_{j, i}}}, l_{i,j}=}{b_{i,j}+b_{j,i}}- +b_{j,i}}},\] (2)where \(>0.5\) controls the confidence interval, and \(t\) is the total number of comparisons so far. The performance of a dueling bandits algorithm is often evaluated by the regret defined as follows.

**Definition 2.2**.: The expected cumulative regret for a dueling bandits algorithm is given as:

\[R_{T}=_{t=1}^{T}-^{}(t))+(^{}- ^{}(t))}{2}=^{}T-_{t=1}^{T}(^{ }(t)+^{}(t)),\] (3)

where \(T\) is the total number of pairwise comparisons, \(^{}(t)\) and \(^{}(t)\) denote the pair to be compared at the \(t\)-th (\(1 t T\)) round, \(^{}\) represents the Copeland score of the Copeland winner.

## 3 Proposed Method

Our proposed D-PBEMO framework follows the conventional PBEMO flow chart as in Figure 1(**a**). In the following paragraphs, we mainly focus on delineating the design of D-PBEMO with regard to both consultation and preference elicitation modules, while leaving the design of the optimization module open.

### Consultation Module

As the interface by which the DM interacts with an EMO algorithm, the consultation module mainly aims to collect the DM's preference information from their feedback upon \(\) to identify the SOI. We employ the stochastic dueling bandits , to directly derive preferences from human feedback without relying on further assumptions such as contextual priors  or structured models . In this setting, a natural choice is to consider each candidate solution as an arm to play. However, since the size of \(\) is usually as large as over \(100\) in the context of EMO, the conventional dueling bandits algorithms will suffer from a large amount of preference comparisons to converge [73; 35]. This is impractical in PBEMO when involving DM in the loop. To address this problem, we propose clustering-based stochastic dueling bandits algorithm that consist of the following three steps.

Step \(1\): **Partition \(\) into \(K\) subsets \(\{}^{i}\}_{i=1}^{K}\) based on solution features in the context of EMO.** Such partitioning is implemented as a clustering method based on the Euclidean distances between solutions of \(\) in the objective space. Instead of viewing each solution as an individual arm, we consider each subset \(}^{i}\) as an arm in our proposed dueling bandits. We denote the solution-level preference matrix as \(_{s}=[p_{i,j}^{s}]_{N N}\), where \(p_{i,j}^{s}\) represents the probability that \(^{i}_{}^{j}\). Then, the preference matrix \(\) in the subset-level can be calculated by \(p_{i,j}=}^{i}||}^{j}|}_{ ^{u}}^{i}}_{^{v}}^{j}}p_{u,v}^{s}\), where \(|}^{i}|\) stands for the size of \(}^{i}\). This probability is well-defined since it satisfies \(p_{i,j}+p_{j,i}=1\). So far, we have reformulated a subset-level dueling bandits problem. Accordingly, the subset-level Copeland winner is the subset \(}^{}\) that beats others on average.

Step \(2\): **Subset-level dueling sampling and solution-level pairwise comparisons.** We employ the double Thompson sampling algorithm  to determine the subset pairs, and then select solutions from the pairs to query DM preferences. We introduce two vectors \(=(v_{1},,v_{N})^{}\) and \(=(_{1},,_{N})^{}\) to record the winning and losing times of each solution respectively, initialized by \(v_{i}=0\), \(_{i}=0\), \(i=\{1,,N\}\). We perform the following steps within a given budget \(T\).

Step \(2.1\): **Determine the subset \(}^{}\) that most likely covers the SOL.** We first narrow candidates to the subsets having the highest upper confidence Copeland scores, denoted as \(^{1}=\{}^{i}|i=*{argmax}_{i}_{i}\}\), where \(_{i}=_{j i}(u_{i,j}>0.5)\), \(i,j\{1,,K\}\). Then, \(}^{i}^{1}\), we apply Thompson sampling as \(_{i,j}^{(1)}*{Beta}(b_{i,j}+1,b_{j,i}+1)\) to sample the winning probability of \(}^{i}\) over other subsets \(}^{j}\), where \(j\{1,,K\}\) and \(j i\). Finally, we apply the majority voting strategy to determine the candidate by \(}^{}*{argmax}_{}^{i}^{1}}_{j i}(_{i,j}^{(1) }>0.5)\), where ties are broken randomly.

Step \(2.2\): **Select the subset \(}^{}\) that can be potentially preferred over \(}^{}\).** To promote exploration, we narrow candidates to the subsets whose lower-confident winning probability over \(}^{}\) is at most \(0.5\), denoted as \(^{2}=\{}^{i}|l_{i,t} 0.5\}\). Note that \(}^{}^{2}\) because \(l_{i,t} p_{i,t}=0.5\).

Then, \(}^{i}^{2}\), we apply Thompson sampling as \(^{(2)}_{i,}(b_{i,}+1,b_{,i}+1)\) to sample the winning probability of \(}^{i}\) over \(}^{}\), and fix \(^{(2)}_{,}=0.5\) according to the definition of preference matrix. Finally, the candidate is determined by \(}^{}_{}^{i}^{2}}\,^{(2)}_{i,}\).
3. **Select two representative solutions \(^{}}^{}\) and \(^{}}^{}\) to query DM.** We conduct uniform sampling to obtain solutions from the _least-informative_ perspective. The DM is asked to evaluate the pair of solutions \(^{},^{}\). If we observe \(^{}_{p}^{}\), we update \(b_{,} b_{,}+1\), \(v_{} v_{}+1\), and \(_{}_{}+1\), and vice versa. Note that other strategies to obtain solutions \(^{}\) and \(^{}\) can be used to improve the query efficiency.

Step \(3\): Output the learned preferences.The output is a triplet \(\{}^{},,\}\), where \(}^{}=_{}^{i}}\). \(_{i}\) is the optimal subset that most likely covers SOI, and candidate solutions are considered to be the SOI with uncertainty encoded by their winning and losing times.

The pseudo codes of the above algorithmic implementation are detailed in Appendix C. Figure 2 gives an illustrative example of the consultation process.

**Remark 1**.: _PBEMO is a optimization-cum-decision-making process. Instead of having a set of Pareto-optimal candidate solutions upfront, PBEMO starts with a coarse-grained representation of the PF. Then, it gradually steers incumbent solutions towards the learned SOI, which may be inaccurate initially. Subsequent consultations then serve as a refinement process. In this context, different from the dueling bandits, which are designed for identifying the single best solution in each round, our proposed method aims to recognize the SOI with a progressively refined fidelity._

**Remark 2**.: _Based on the Remark 1, we intend to explore the dependency among solutions . This involves performing pairwise comparisons for solution groups, rather than for a single candidate, to enable more efficient interaction. Consequently, in the preference elicitation stage, it becomes essential not only to rely on the learned preference but also to consider the uncertainty introduced by the coarse-grained representation._

**Theoretical Analysis.** We present a rigorous analysis of the regret bound of our proposed algorithm. To this end, we introduce the following two assumptions derived from the current literature.

**Assumption 3.1** ().: The winning probability between two arms satisfies \(p_{i,j} 0.5\), \( i j\).

**Assumption 3.2** (Tight clustering ).: All solutions in \(}^{}\) are the Copeland winners over other solutions in the sub-optimal subsets.

Figure 2: **(a)** The evolutionary population of an EMO algorithm is divided into three subsets, where \(}^{2}\) covers the SOI (denoted as a \(\)). **(b)** After a PBEMO round, in the next consultation session, all solutions are steered towards the SOI and their spreads become more tightened towards the SOI.

**Theorem 3.3**.: _Under the Assumptions 3.1 and 3.2, for any \((0,1]\) and \(>0.5\), the regret of our clustering-based stochastic dueling bandits algorithm is bounded by:_

\[R_{T}(T)_{i j,\ _{i,j}<0.5}((1+)}(_{i,j} 0.5)}+_{i,j}-0.5)^{2}})+(}{^{ 2}}).\]

Definitions of \(_{i,j}\) and proof can be found in Appendix B.1.

**Remark 3**.: _Theorem 3.3 reveals that the expected regret of our algorithm is bounded by \((K^{2} T)\). Compared with the dueling bandits algorithms based on Thompson sampling and their extensions to large arms , whose regret is bounded by \((N^{2} T)\), our proposed clustering-based stochastic dueling bandits algorithm is more efficient in searching for the SOI, given \(K N\)._

### Preference Elicitation Module

The preference elicitation module plays as a bridge that connects consultation and optimization. It transforms the preference learned from the consultation module into the configurations used in the underlying EMO algorithm, thus steering the EMO process progressively towards the SOI. In addition, this module maintains the preference information to inform a strategic termination criterion of consultation, thus reduces DM's workload. We present the design of this module by addressing the following three questions.

**How to leverage the preference learned from the consultation session?** Let us assume the DM's feedback collected in the consultation session is drawn from a preference distribution as the density ratio \(p_{v}(})/p_{}(})\), where \(p_{v}(})\) and \(p_{}(})\) is respectively the winning and losing probability of a solution \(}\) sampled from the PS, see an illustrative example in Figure 3(**a**). After the \(\)-th round of the consultation session, we perform density-ratio estimation based on \(\) and \(\). By using moment matching techniques (see Appendix B.2), we obtain a Gaussian distribution with mean \(}_{}^{*}\) and covariance \(_{}\). Let \(_{}\) take the largest value of diagonal elements of \(_{}\). The preference elicitation module maintains a Gaussian mixture distribution by a convex combination of Gaussian distributions from multiple consultation sessions:

\[()=_{=1}^{N_{}} _{}}(} _{}^{*},_{}),\] (4)

where \(\), \(N_{}\) is the total number of consultation sessions conducted so far, and \(Z=_{=1}^{N_{}}1/_{}\) is the normalization term. Figure 3(**b**) gives an illustrative example of a Gaussian distribution approximated by the DM's feedback collected at one consultation session.

**How to adapt \(()\) to EMO algorithms?** We believe \(()\) can be applied in any EMO algorithms with few adaptation in their environmental selection. For proof-of-concept purpose, this paper takes NSGA-II  and MOEA/D , two most popular algorithms in the EMO literature, as examples and we design two D-PBEMO instances, dubbed D-PBNSA-II and D-PBMOEA/D.

Figure 3: The density ratio between \(p_{}(})\) and \(p_{}(})\) is shaded in blue, while its estimation is shaded in red. The SOI falls within the estimated Gaussian distribution for \(95\%\) confidence interval.

* At each generation of the original NSGA-II, it first uses non-dominated sorting to divide the combination of parents and offspring into several non-domination fronts \(F_{1},,F_{l}\). Starting from \(F_{1}\), one front is selected at a time to construct a new population, until its size equals to or exceeds the limit. The exceeded solutions in the last acceptable front will be eliminated according to the crowding distance metric to maintain population diversity. In D-PBNGGA-II, we replace the crowding distance with \(()\). As a result, the solutions close to the SOI will survive to the next generation.
* The basic idea of the classic MOEA/D is to decompose the original MOP into a set of subproblems using weight vectors. Then, these subproblems are tackled collaboratively using population-based meta-heuristics. In D-PBMOEA/D, we progressively transform the originally uniformly distributed weight vectors \(W=\{^{i}\}_{i=1}^{N}\) used in the original MOEA/D to the preference distribution \(()\). In practice, the transformed weight vector is \(^{i}=^{-1}_{}(^{i})\), where \(^{-1}_{}()\) is the weighted sum of inverse Gaussian distribution, defined in equation (19).

Detailed implementation of D-PBNGGA-II and D-PBMOEA/D are in Appendix C.

**When to stop querying DM?** There is no principled termination criterion in exiting PBEMO, but is often set as a pre-defined number of consultation sessions. This is not rationale and likely to incur unnecessary workloads to DM, even when the evolutionary population is either converged to the SOI or being trapped by local optima. Under our D-PBEMO framework, we have the following theoretical result about the convergence property of \(()\).

**Theorem 3.4**.: _Assume the preference distribution \(()\) around the SOI follows a Gaussian mixture distribution. It becomes stable when \(N_{}\) increases._

The proof of Theorem 3.4 is in Appendix B.3. In D-PBEMO, we apply Theorem 3.4 to adaptively terminate the consultation session when \(()\) becomes stable. In practice, this happens when the Kullback-Leibler (KL) divergence of \(()\) between two consecutive consultation sessions is smaller than a threshold \(\):

\[D_{}(_{-1}_{} )=_{i=1}^{N_{}}_{-1}(}_{i})_{-1}(}_{i})}{_{}(}_{i})},\] (5)

where \(1< N_{}\), \(}_{i}\) is sampled from \(}^{}\). \(N_{}\) is the number of samples when calculating the KL divergence. Here we use \(=10^{-3}\), and its parameter sensitivity is studied in Section 4.4.

## 4 Experiments

### Experimental Setup

This section outlines some key experimental setup including benchmark test problems and performance metrics. More detailed information can be found in Appendix D.

Benchmark problemsOur experiments considers \(33\)_synthetic test instances_ including ZDT1 to ZDT4 and ZDT6  (\(m=2\)), DTLZ1 to DTLZ6  where \(m=\{3,5,8,10\}\), and WFG1, WFG3, WFG5, and WFG7  (\(m=3\)). These problems are with various PF shapes and challenges such as bias, plateau, and multi-modal. In addition, we also consider two _scientific discovery problems_ including \(10\) two-objective RNA inverse design tasks  and \(5\) four-objective protein structure prediction (PSP) task . The problem formulations are detailed in Appendix D.1.

Performance metricsAs discussed in , quality assessment of non-dominated solutions is far from trivial when considering DM's preference information regarding conflicting objective. In our experiments, we consider two metrics to serve this purpose. One is approximation accuracy \(^{}()\) that evaluates the closest distance of \(\) regarding the DM preferred solution in the objective space, denoted as 'golden point' \(^{}^{m}\), while the other is average accuracy \(()\) that evaluates the average distance to \(^{}\). Note that \(^{}\) is unknown to the underlying algorithm in practice, and the corresponding settings used in our experiments are in Appendix D.2. Due to the stochastic nature of evolutionary computation, each experiment is repeated \(20\) times with different random seeds. To derive a statistical meaning of comparison results, we consider Wilcoxon rank-sum test and Scott-Knott test in our experiments. They are briefly introduced in Appendix D.3.

### Comparison Results with State-of-the-art PBEMO algorithms

To validate the effectiveness of our proposed D-PBEMO framework, we first compare the performance of D-PBNSAG-II and D-PBMOEA/D against three state-of-the-art PBEMO algorithms, I-MOEAD-PLVF , I-NSGA2/LTR , IEMO/D .

For the synthetic benchmark test problems, the comparison results of \(^{}()\) and \(()\) in Tables A5 and A6 have demonstrated the competitiveness of our proposed D-PBEMO algorithm instances. In particular, D-PBNSAG-II and D-PBMOEA/D have achieved the best metric values in 77 out of 96 comparisons according to the Wilcoxon rank-sum test at the \(0.05\) significant level. In Figure 4, the box plots of the ranks derived from the Scott-Knott test of D-PBNSAG-II and D-PBMOEA/D compared against the other three peer algorithms further consolidate our observation about the effectiveness of our D-PBEMO framework for searching for SOI within a given computational budget. From the plots of the final non-dominated solutions obtained by different algorithms shown in Figures A8 to A9, we can see that the solutions found by our D-PBEMO algorithm instances are more concentrated on the 'golden point' while the others are more scattered. Further, the superiority of our proposed D-PBEMO algorithm instances becomes more evident when tackling problems with many objectives, i.e., \(m 3\). Similar observations can be found in the scientific discovery problems, as the results shown in Tables A22, A23, A24 and A25. For the RNA inverse design tasks, the sequences identified by our proposed D-PBEMO algorithm instances have a good match regarding the targets as the selected results shown in Figure 5 (full plots in Figures A14 and A15). As for the PSP problems, from the results in Figure 6, it is clear to see that D-PBEMO algorithm instances significantly outperform the other peers. The protein structures predicted by our algorithms are more aligned with the native protein structure. Full results can be found in Appendix F.

Figure 4: Box plot for the Scott-Knott test rank of D-PBEMO and peer algorithms achieved by \(33\) test problems running for \(20\) times. The index of algorithms are as follows: \(1\)D-PBNSAG-II, \(2\)D-PBMOEA/D, \(3\)I-MOEA/D-PLVF, \(4\)I-NSGA-II/LTR, \(5\)IEMO/D.

Figure 5: Comparison result of D-PBNSAG-II against the other three state-of-the-art PBEMO algorithms on a selected RNA inverse design task (Eterna ID: \(852950\)). The target structure is shaded in blue color while the predicted structures obtained by different optimization algorithms are highlighted in red color. In this experiment, the preference is set to \(=1\). The closer \(\) is to \(1\), the better performance achieved by the corresponding algorithm. When the \(\) shares the same biggest value, the smaller \(MFE\) the better the performance is. Full results can be found in Appendix F.

### Investigation of the Effectiveness of Our Consultation Module

The consultation module, which learns the DM's preferences from their feedback, is one of the most important building blocks of our proposed D-PBEMO framework. To validate its effectiveness, we designed a D-PBEMO variant (denoted as D-PBEMO-DTS) that uses the double Thompson sampling (DTS) widely used in conventional stochastic dueling bandits  as an alternative of our proposed clustering-based stochastic dueling bandit algorithm. From the results in Tables 12 and 13, it is clear to see that D-PBEMO-DTS is always outperformed by our three D-PBNSA-II and D-PBMOEA/D. This observation can be attributed to the ineffectiveness of the traditional dueling bandit algorithms for tackling a large number of arms. In our PBEMO context, DTS requires at least thousands comparisons when encountering more than \(100\) candidate solutions. This is not feasible under the limited amount of computational budget. We envisage the same results will be obtained when considering other dueling bandits variants such as [74; 34].

Further, we designed another variant (denoted as D-PBEMO-PBO) that uses a parameterized preference learning model in Bayesian optimization  as alternative in the consultation module. From the comparison results shown in Tables 12 and 13, we find that the performance of D-PBEMO-PBO is competitive on problems with a small number of objective (\(m=2\)). However, its performance degenerate significantly with the increase of the number of objectives. This can be attributed to the exponentially increased search that renders D-PBEMO-PBO ineffective by suggesting too many solutions outside of the region of interest. In contrast, the clustering strategy in our proposed method strategically and significantly narrow down the amount of comparisons without compromising the learning capability. Additionally, the enlarged search space also makes the model selection in D-PBEMO-PBO difficult. Our proposed method, on the other hand, learns human preferences directly from their feedback, thus is scalable to a high-dimensional space.

Figure 6: Experiments results for comparison results between D-PBEMO and the other three state-of-the-art PBEMO algorithms on the PSP problems. In particular, the native protein structure is represented in a blue color while the predicted one obtained by different optimization algorithms are highlighted in a red color. The smaller RMSD as defined in Equation (29) of appendix, the better performance achieved by the corresponding algorithm.

### Parameter Sensitivity Study

This section discusses the sensitivity study of two hyperparameters of the D-PBEMO framework.

\(K\) is the parameter that controls the number of subsets used in our clustering-based stochastic dueling bandits algorithm. From the results shown from Tables A11 and A12 considering \(K=\{2,5,10\}\), it is interesting to note that D-PBEMO is not sensitive to the setting of \(K\) when the dimensionality is low (i.e., \(m=2\)). This implies that we can achieve a reasonably good performance even when using a smaller \(K\), i.e., a coarser-grained approximation of SOI. By doing so, we can further improve the efficiency of our D-PBEMO framework.

As the dimensionality increases (i.e., \(m=\{3,5,8,10\}\)), the population size will increase. Generally, as is shown in Table A13 - A20, with larger populations, a higher \(K\) tends to yield better results, aligning with our intuition. Furthermore, our significance analysis across 20 repeated experiment (Figure 4) reveals that the optimal \(K\) does not show significant differences in performance. In summary, \(K\) does not significantly impact the performance of our proposed D-PBEMO framework. For most problems, we do not recommend choosing a very small/large \(K\) (e.g., \(K=2\), \(K=N\)), as it may inefficiently narrow down the ROI.

To a certain extent, the threshold \(\) plays an important role for controlling the budget of consulting the DM. From the results shown in Tables A9 and A10, we find that all three settings of \(=\{10^{-1},10^{-3},10^{-6}\}\) have shown comparable results. However, a too large \(=10^{-1}\) may lead to a premature convergence risk. On the other hand, a too small \(=10^{-6}\) may be too conservative to terminate. This renders more consultation iterations, thus leading to a larger amount of cognitive workloads to DMs. In contrast, we find that D-PBEMO algorithm instances can converge with less than \(10\) consultation iterations with \(=10^{-3}\).

## 5 Limitations

Our proposed D-PBEMO directly leverages DM's feedback to guide the evolutionary search for SOI, which neither relies on any reward model nor cumbersome model selection. However, there are several potential limitations of D-PBEMO that warrant discussion here:

* The regret analysis of our proposed clustering-based stochastic dueling bandits is for the optimal subset, i.e., the region of interest on the PF. It is not yet directly applicable to identify the exact optimal solution of interest. As part of our future work, we will work on efficient algorithms and theoretical study on the best arm identification in the context the preference-based EMO.
* This paper only analyzes the regret of the consultation module. How to further analyze the convergence of the D-PBEMOas a whole remains unknown. This will also lead to the next step of our research. In particular, if it is successful, we may provide a radically new perspective to analyze the convergence of evolutionary multi-objective optimization algorithms.

## 6 Conclusion

This paper introduced the D-PBEMO framework that is featured in a novel clustering-based stochastic dueling bandits algorithm. It enables learning DM's preferences directly from their feedback, neither relying on a predefined reward model nor cumbersome model selection. Additionally, we derived a unified probabilistic format to adapt the learned preference to prevelant EMO algorithms. Meanwhile, such probabilistic representation also contributes to a principled termination criterion of DM interactions. Experiments demonstrate the effectiveness of our proposed D-PBEMO algorithm instances. In future, we will investigate more principled approaches to obtain \(K\) subsets, such as fuzzy clustering with overlapping. Further, we plan to extend this current \(K\)-armed bandits setup to a best arm identification. By doing so, we can directly obtain the solution of interest, with a potential explainability for MCDM. Moreover, we will extend our theoretical analysis of the termination criterion to a convergence analysis of PBEMO, even generalizable to the conventional EMO. Last but not the least, we will collaborate with domain experts to promote a emerging 'expert-in-the-loop' scientific discovery platform, contributing to the prosperity of AI for science.