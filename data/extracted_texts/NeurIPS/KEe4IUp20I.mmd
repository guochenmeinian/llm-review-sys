# SpaceByte: Towards Deleting Tokenization

from Large Language Modeling

 Kevin Slagle

Rice University

kevin.slagle@rice.edu

###### Abstract

Tokenization is widely used in large language models because it significantly improves performance. However, tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character-level modeling performance, and increased modeling complexity. To address these disadvantages without sacrificing performance, we propose SpaceByte, a novel byte-level decoder architecture that closes the performance gap between byte-level and subword autoregressive language modeling. SpaceByte consists of a byte-level Transformer model, but with extra larger transformer blocks inserted in the middle of the layers. We find that performance is significantly improved by applying these larger blocks only after certain bytes, such as space characters, which typically denote word boundaries. Our experiments show that for a fixed training and inference compute budget, SpaceByte outperforms other byte-level architectures and roughly matches the performance of tokenized Transformer architectures.

## 1 Introduction

Most language models are trained using tokenization, which partitions text into tokens that typically consist of words or subwords. Tokenization is useful because it significantly decreases the inference and training computational costs of large language models. However, tokenization also imposes several disadvantages, including performance penalties for languages that are prioritizes less by the tokenizer ; increased vulnerability to adversarial attacks ; and worse character-level modeling performance , and additional model complexity.1

Recently, MegaByte , MambaByte , and more  have been proposed as new byte-level autoregressive language models that model bytes instead of tokens. (See  for encoder and encoder-decoder byte-level modeling.) To address the longer context size resulting from modeling bytes instead of tokens, MegaByte uses multiscale modeling , while MambaByte uses Mamba blocks  instead of Transformer blocks. But although MegaByte and MambaByte have been shown to perform better than a standard byte-level Transformer, to our knowledge, no byte-level autoregressive large language model architecture has been shown to match the performance of tokenized models when controlling for compute costs.

In this work, we study the performance of byte-level and subword-level autoregressive models when trained using a fixed compute budget. We measure the performance in terms of the cross entropy (measured in bits-per-byte), which has been shown to be a strong predictor of down-stream performance . In addition to controlling for training compute, we also control for inference compute costs (measured in FLOPs). We find that byte-level Transformer and MegaByte models can require roughly 10 times more training FLOPs to achieve the same performance as a subwordlevel Transformer. To close this substantial performance gap, we propose a new byte-level decoder architecture: SpaceByte.

SpaceByte also utilizes multiscale modeling to improve efficiency by grouping bytes into patches. But unlike MegaByte, which uses a fixed patch size, SpaceByte uses a simple rule to dynamically partition the bytes into patches that are aligned with word and other language boundaries. Our compute-controlled experiments show that this simple modification is crucial for performance, allowing SpaceByte to outperform other byte-level architectures and roughly match the performance of subword Transformers across a variety of text modalities.

Our experiments are performed on datasets consisting of English books, LaTeX formatted arXiv papers, and open-source code. For other data modalities, SpaceByte with our simple patching rule might not be as effective.

## 2 SpaceByte

The SpaceByte architecture is summarized in Figure 1. In a nutshell, SpaceByte can be thought of as a byte-level Transformer model, but with extra "global" transformer blocks (with a larger model dimension) inserted in the middle, which are only applied a fraction of the time. While the MegaByte architecture applies the global transformer blocks every \(P 8\) bytes, we hypothesize that this fixed spacing hinders performance. Our intuition is that the first character of a word is typically significantly harder to predict than the following characters. We therefore expect that performance can be improved by applying the global blocks primarily at word boundaries.

**Global Block Insertion Rule** In this work, we consider a very simple rule to dynamically decide when to apply the global blocks. We assume that the text bytes are encoded using the UTF-8 encoding. We define a byte to be _spacelike_ if the byte does not encode a letter, number, or UTF-8 continuation byte2. We apply the global blocks after any spacelike byte that is not preceded by another spacelike byte (and after any BOS token). See Figure 2 for examples.

The most common spacelike byte is the space character. Thus, the global blocks are applied most frequently to predict the first character of a word, which we expect is the hardest character to predict  in a given word. With fixed patch size (e.g. as in MegaByte), the global blocks are typically inserted in the middle a word, which we expect is inefficient because predicting the rest of the word could likely be more efficiently accomplished using the local blocks. We define continuation bytes to be spacelike so that languages that do not use spaces between words can still benefit from the global blocks between multi-byte characters (e.g. Chinese characters consists of three bytes in UTF-8).

Figure 1: An overview of the SpaceByte architecture. The embedding, local transformer blocks, and de-embedding (i.e. a layer norm and linear) are the standard Transformer decoder layers. SpaceByte modifies the standard transformer by applying “global” transformer blocks only after certain bytes, such as space characters. The intuition is that the first character of a word is typically the hardest to predict; thus this positioning of the global blocks should make the best use of the global blocks (which use a larger model dimension).

Although this very simple "spacelike" rule is likely not the optimal rule, we find that it works surprisingly well in practice for English text, LaTeX formatted papers, and code. Nevertheless, a critical future direction is to optimize [28; 14; 9] better rules using data rather than our simple heuristic.

**Important Details** Since the global blocks are not applied as often as the local transformer blocks, it is advantageous to use a larger model dimension for the global transformer blocks. To increase the dimensions of an activation vector before the global blocks, we simply pad the activation vector with zeros. To decrease the dimension, we truncate the activation vector.

In our experiments, we use a significantly larger context size than the model dimension \(D_{}\) of the local transformer blocks. To prevent the attention mechanism from dominating the compute costs for the local model, we use an attention window [29; 30; 31] of length \(D_{}\) for the local transformer blocks. The global blocks use a global attention that attends to all other global blocks.

See Appendix C for pseudocode. Additional details specific to our experiments are provided in Sections 4.1 and 4.2 and Appendix A.

## 3 Related Work

The most straight-forward consequence of modeling bytes instead of subword tokens is that the length of a sequence typically increases by about a factor of four. This increased sequence length increases the training and inference compute cost for modeling a given long sequence of text for a Transformer due to the quadratic scaling of attention.

**MegaByte** The MegaByte architecture strives to use multiscale Transformer modeling to lessen these performance issues. In particular, MegaByte groups bytes into patches of a fixed patch size \(P\). Each patch of bytes is vectorized and then fed into a "global" Transformer model. The output of the global model is then fed into a "local" Transformer model that autoregressively outputs byte-level logits. 

For a context size of \(T\) bytes, MegaByte's global Transformer model compresses the context into only \(T/P\) patches, which can significantly decrease the compute cost for modeling long sequences. Similar to Yu et al. , we also find that MegaByte outperforms a standard byte-level Transformer. However, we find that MegaByte's performance is remarkably close to a stronger byte-level Transformer baseline that simply uses a sliding window attention mechanism [29; 30; 31] to increase the context size without increasing the compute costs.

Figure 2: Examples of patch boundaries from datasets that we study. Spacelike bytes are underlined and colored blue. Patches boundaries are drawn above the text. Each patch ends after a spacelike byte that is not preceded by another spacelike byte. Consequently, each patch begins with zero or more spacelike bytes, followed by one or more non-spacelike bytes, and ends with a single spacelike byte. The global blocks predict the first character of each patch. The downward arrow (\(\)) denotes a newline byte. The left and right quotation characters, (") and (") in the PG-19 example, are encoded using three bytes in UTF-8. The first of the three bytes is spacelike, while the later two bytes are UTF-8 continuation bytes, which are not spacelike and are each denoted using a bullet point (\(\)) above.

Yu et al.  do not compare MegaByte to subword-level Transformer in compute controlled experiments. In our compute controlled experiments, we find that MegaByte's performance significantly lags behind a subword-level Transformer.

Compared to MegaByte, SpaceByte makes the crucial change that patches are dynamically sized to be commensurate with the text, e.g. with word boundaries. We also add an additional local model before the global model (while MegaByte only utilizes a single local model after the global model) to help the model deal with the dynamical patch sizes. We also use significantly longer attention windows for our local models. We find that these changes allow SpaceByte to significantly improve upon the performance of MegaByte and roughly match the performance of subword-level Transformers.

**MambaByte** The MambaByte architecture  takes an alternative approach to avoiding the quadratic compute scaling of the attention mechanism by replacing the Transformer block with a Mamba block . Wang et al.  find that their byte-level MambaByte models outperform byte-level Transformer and byte-level MegaByte models. They perform one controlled experiment with a subword model, where they find that MambaByte slightly outperforms Mamba (using tokens) when controlling for the amount of model parameters and training data (which was 14 epochs of the PG-19 dataset). But this experiment was not controlled for compute as MambaByte was trained using roughly four times as much compute than Mamba. We view the Mamba and MambaByte architectures as complementary to our work, as the Mamba block could be integrated into SpaceByte (or MegaByte) in place of Transformer blocks.

**Layer Skipping** SpaceByte could be though of as a Transformer that employs a novel kind of text-dependent layer skipping [32; 33; 34; 35; 36; 37; 38] on the middle layers.

**Word Boundary** Prior works have shown utility in using word boundaries to partition patches for autoregressive multi-scale byte-level modeling [9; 8; 11] (and also  for encoder-decoder modeling). However, these works did not compare autoregressive byte-level models to subword-level models, nor did they identity a patch partitioning rule that could generically be applied to UTF-8 encoded text. Our primary contributions beyond these prior works is to show how to scale word-boundary byte-level modeling to more diverse text modalities while roughly matching the performance of subword-level models in compute-controlled experiments.

Nawrot et al.  and Fleshman and Van Durme  make use of the Hourglass Transformer architecture . The SpaceByte architecture is similar to the Hourglass Transformer, except SpaceByte uses a simpler technique for shortening and upscaling the activations before and after the global blocks, and SpaceByte uses a sliding window attention [29; 30; 31] in the local blocks to improve performance for long context sizes.

## 4 Experiment Setup

Our experiments compare the performance of our byte-level SpaceByte architecture to subword-level Transformer and byte-level Transformer and MegaByte architectures. To fairly compare the performance between the byte and subword level models, we measure the cross-entropy of the test dataset in terms of bits-per-byte.3 Given the substantial variation in inference compute costs across the models we study, we also compare their inference compute costs to provide a more comprehensive evaluation. We use FLOPs-per-byte as a simple software and hardware-independent proxy for inference compute costs, which is the number of FLOPs (see Appendix A.1) required to model a byte of text.4

Note that by controlling for both total training compute and FLOPs-per-byte, we are also controlling for the amount of training data since \(()=(\,)/(\,)\). The FLOPs-per-byte during training is equal to three times the FLOPs-per-byte during inference (due to the backwards pass during training).

We therefore study the Pareto frontier of lowest bits-per-byte and lowest FLOPs-per-byte. We train all models using a compute-controlled setup, using either \(10^{18}\) or \(10^{19}\) FLOPs. In order to effectively explore this Pareto frontier, we train models using a grid of different model dimensions and numbers of layers, as specified in Appendix B.3.

**Datasets** Following the MegaByte  and MambaByte  experiments, we benchmark our models on a diverse range of long-form datasets: PG-19 (English-language books written before 1919) ; arXiv (papers from ArXiv written in LaTeX, extracted from the arXiv component of The Pile ); and Github (open-source code repositories, extracted from the Github component of The Pile ).

### Models

The models we study tend to perform best when the compute cost is roughly evenly split between the attention and feedforward layers. To ensure this, we fix the context size (or attention window) to be equal to the model dimension for every layer. We detail our model setup below.

**Notation** For all models, we use \(T\) to denote the context length, and \(D\) to be the model dimension (of the global model for SpaceByte and MegaByte).

For SpaceByte and MegaByte, \(D_{}\) is the dimension of the local model, and \(T_{}\) is the maximum context size for the global model. The patch size \(P\) is the number of bytes between global blocks. If the patch size is fixed (which is always the case for MegaByte), we naturally set the context size to be \(T=PT_{}\).

Below, we describe each of the model architectures that we compare in our experiments.

**SpaceByte** We fix the global context size and global model dimension to be equal, \(T_{}=D\), and we set the local attention window \(W_{}\) equal to the local model dimension, \(W_{}=D_{}\). For the PG-19 and arXiv datasets, the average patch size is roughly 6, so we take \(T=6T_{}\) for these datasets; for the Github dataset, the average patch size is roughly 8, so we instead take \(T=8T_{}\) for the Github dataset.

For simplicity, we fix the number of global transformer blocks \(L_{}\) to be equal to the total number of local blocks, \(L_{}^{(1)}+L_{}^{(2)}\), and we evenly split the number of local blocks before (\(L_{}^{(1)}\)) and after (\(L_{}^{(2)}\)) the global blocks, i.e. we fix \(L_{}^{(1)}=L_{}^{(2)}=L_{}\).

**SpaceByte (fixed patches)** To clearly demonstrate the utility of dynamically aligning patch boundaries in SpaceByte, we also train a simplified version SpaceByte where the patches all have a fixed size. In order to roughly match SpaceByte's average patch size, we take the fixed patch size to be \(P=6\) for all datasets except for the Github dataset, for which we use \(P=8\). We again use \(T_{}=D\) and \(T=PT_{}\).

**Byte-level Transformer** For a simple baseline comparison (following Yu et al. ), we train byte-level Transformer models. We take the context size to be equal to the model dimension, \(T=D\).

Note that in our setup, a Transformer with model dimension \(D\) only sees a context size of \(D\), which is significantly smaller than the context size of \(PD\) for SpaceByte (and MegaByte) with patch size \(P\).

**Byte-level Transformer (Window Attention)** Since a shorter context is a significant disadvantage for long-form datasets, we also compare against a stronger Transformer baseline that uses a sliding window attention  to efficiently increase the context size without increasing compute costs. We train each window attention enhanced Transformer using a context size \(T=PD\) and a sliding attention window size equal to \(D\), with \(P=6\) for all datasets except for the Github dataset for which \(P=8\).5

**MegaByte** We also compare to MegaByte . Although MegaByte was originally trained using a patch size of \(P=8\), we found that a patch size of \(P=4\) was often better in our setup. We thus include both of these patch sizes (4 and 8) in our hyperparameter grid for MegaByte. For simplicity, we fix the number of layers in the global and local blocks to be equal, \(L_{}=L_{}\), which is close to what was used by Yu et al. . Similar to SpaceByte, we set the context size to \(T=PD\), where \(D\) is the global model dimension.

**Subword Transformers** Our most important baseline is the standard subword Transformer. We train subword Transformers using two different tokenizers (both with a vocabulary size of 50,257): (1) the GPT2 tokenizer , and (2) a SentencePiece  tokenizer using a byte-pair-encoding model  that was separately trained for each dataset. As usual, we set the context size to be equal to the model dimension, \(T=D\).

### More Details

We use fairly standard Pre-LN [46; 30; 47] Transformer  blocks with no bias terms. Since MegaByte uses Rotary Position Embedding (RoPE) , we also use RoPE for all models (which slightly improves the loss). To prevent loss divergences during training, we use qk-layernorm [50; 51; 52] (which we strongly recommend) for all models; i.e. we add an extra layer-normalization to the query and key vectors in the self-attention layers.

All hyperparameters have been carefully tuned using grid and random searches. See Appendices A and B for more details.6

## 5 Results

We now present our experimental data comparing the different model architectures in compute-controlled settings. Figure 3 plots the Pareto frontier of lowest cross-entropy bits-per-byte and lowest FLOPs-per-byte (i.e. inference compute cost) for each architecture and training compute budget. We assume that the Pareto frontier is convex. Thus, for each architecture and compute budget, we perform a grid search over model dimension and number of layers; we then draw a piecewise-linear line connecting the best (i.e. minimal subset of) models such that all other models (not shown in figure) lie above and to the right of the line. Table 1 summarizes the results for the lowest overall bits-per-byte for each architecture.

Across all datasets, training compute budgets, and inference compute budgets (i.e. FLOPs-per-byte), SpaceByte significantly outperforms all other byte-level architectures. SpaceByte also consistently outperforms the subword Transformer when using GPT2 tokens, and by a wide margin on the arXiv and Github datasets. SpaceByte roughly matches the performance of the most competitive baseline, the subword Transformer using the SentencePiece tokenizer, with SpaceByte performing slightly better on the arXiv and Github datasets. Figure 3 also suggests that SpaceByte's performance improves faster than the subword Transformer as the training compute budget increases.

Byte-level architectures other than SpaceByte perform significantly worse than SpaceByte or the SentencePiece Transformer. For example, for PG-19, the next best byte-level architecture is

    & Model & PG-19 & arXiv & Github \\    } & Transformer (GPT2 tokenizer) & 1.013 & 0.796 & 0.554 \\  & Transformer (SentencePiece) & **0.989** & 0.768 & **0.508** \\    } & Transformer & 1.138 & 0.909 & 0.655 \\  & Transformer (Window Attention) & 1.089 & 0.818 & 0.560 \\  & MegaByte & 1.083 & 0.822 & 0.570 \\  & SpaceByte (fixed \(P\)) & 1.112 & 0.804 & 0.552 \\  & SpaceByte & **1.009** & **0.748** & **0.500** \\   

Table 1: **Best bits-per-byte.** Lowest bits-per-byte3 for each model architecture when trained using \(10^{19}\) FLOPs on different text modalities. The lowest bits-per-byte for each dataset are underlined; and the lowest within 2.5% are bolded. The largest statistical error (due to a finite number of evaluation samples) is 0.4%. SpaceByte significantly outperforms other byte-level architectures and performs on par with the SentencePiece subword Transformer.

Figure 3: Pareto frontier of the cross-entropy bits-per-byte\({}^{3}\) vs FLOPs-per-byte during inference (details in Appendix A.1) for each model architecture trained using \(10^{18}\) (connected by thin lines) or \(10^{19}\) (thick lines) FLOPs on different datasets (on a log-log scale). Each dot describes a model with a different number of layers and/or model dimension. Lower and to the left is better. SpaceByte (red) outperforms all other byte-level architectures across the entire Pareto frontier for all datasets. SpaceByte roughly matches the performance of the subword Transformer using SentencePiece tokens, and outperforms the subword Transformer using GPT2 tokens.

MegaByte; however, MegaByte trained using \(10^{19}\) FLOPs (thick green line in Figure 2(a)) performs worse across nearly the entire Pareto frontier than the SentencePiece Transformer trained using only 10% as many training FLOPs (thin black line). Although the standard byte-level transformer (which is the primary baseline used by Yu et al. , blue in Figure 3) performs significantly worse than the other byte-level models, we note that by simply using a sliding window attention mechanism to increase the context size to more closely match that of the other byte-level models, this stronger baseline (purple) performs almost as well as MegaByte. Nevertheless, SpaceByte still significantly outperforms this stronger baseline.

To verify the importance of dynamic patch sizes for SpaceByte's performance, we compare SpaceByte to a variant of SpaceByte with fixed patch sizes (orange in Figure 3). We observe that fixing the patch size significantly degrades the performance of SpaceByte.

Note that on the arXiv and Github datasets, the subword Transformer performs significantly worse when using GPT2 tokens (which were trained on WebText ) than SentencePiece tokens (which were trained using the specific dataset). This exemplifies the bias that tokenization can introduce on data distributions different from what the tokenizer was trained on.

## 6 Comparison with Other Works

We also compare SpaceByte performance to byte-level models trained in other works. Yu et al.  trained Transformer, PerceiverAR, and MegaByte models, each using the same amount of compute, FLOPs-per-byte, and data (80B bytes). Wang et al.  additionally trained a MambaByte model using the same FLOPs-per-byte but only 30B bytes of data. We train SpaceByte-793M+184M (\(D=1536\), \(D_{}=768\), \(L_{}=26\), \(L_{}=28\)) using roughly the same inference FLOPs-per-byte (728M) but also only 30B bytes of data (following Wang et al. ). Training these models thus requires roughly \(3 728\)M FLOPs-per-byte \(\) 30B bytes \( 6.5 10^{19}\) FLOPS, where the factor of three comes from converting inference FLOPs-per-byte to training FLOPs-per-byte (which additionally requires a backwards pass). For this experiment, we set the context size of SpaceByte to 8192 bytes to follow the prior works. See Appendix A for more details.

We also train subword Transformer-1B (\(D=1536\)) models using the SentencePiece tokenizer (except for the Stories dataset, for which we use the GPT2 tokenizer). The average number of bytes per token

   } &  Model \\ size \\  } &  Context \\ trained \\  } &  Data \\ trained \\  } &  \\    & & & 2048 & & & & \\   & & & tokens & \( 30^{*}\) & **0.908** & **0.809** & **0.666** & **0.400** \\   & & & & bytes & & & \\   Deep \\  } & Transformer-320M  & 1024 & 80B & 1.057 & 1.064 & \(0.816^{}\) & \(0.575^{}\) \\  & PerceiverAR-248M  & 8192 & 80B & 1.104 & 1.070 & \(0.791^{}\) & \(0.546^{}\) \\  & MegaByte-758M+262M  & 8192 & 80B & 1.000 & 0.978 & **0.678\({}^{}\)** & **0.411\({}^{}\)** \\  & MambaByte-353M  & 8192 & 30B\({}^{*}\) & **0.930** & 0.908\({}^{}\) & **0.663\({}^{}\)** & **0.396\({}^{}\)** \\  & SpaceByte-793M+184M & 8192 & 30B\({}^{*}\) & **0.918** & **0.833** & **0.663** & **0.411** \\  & & (bytes) & (bytes) & & & & \\   

Table 2: **Comparison with other works.** We compare SpaceByte to byte-level models trained in other works, along with a subword transformer that we train. All models are trained using roughly the same inference FLOPs-per-byte (\( 728\)M). The bits-per-byte for the Transformer, PerceiverAR, and MegaByte models are taken from Yu et al. , while MambaByte results are taken from Wang et al. . The best bits-per-byte for each dataset are underlined; and the lowest within 3% are bolded. The largest 1-sigma statistical error (due to a finite number of evaluation samples) for the models we train is less than 0.001. SpaceByte is the overall best performing byte-level model and consistently performs within a few percent of the subword Transformer.

\({}^{}\) These models used slightly different datasets for training and/or testing. For MambaByte-353M, we estimate that this difference very roughly amounts to an extra 3% statistical error.

for the PG-19, Stories, arXiv, and Github datasets are 4.05, 4.39, 3.73, and 3.31, respectively. To match the FLOPs-per-byte of the subword Transformer-1B models to the byte-level models, we set the number of layers to 40, 44, 37, or 31, for Transformer-1B on these four respective datasets.

Results are shown in Table 2. We show experiments for the PG-19 , Stories , arXiv (extracted from The Pile ), and Github (extracted from The Pile ) datasets.7 Yu et al.  used proprietary "arXiv" and "Code" datasets, which we do not have access to. Following Wang et al. , we compare Yu et al. 's results to the similar (but likely slightly different) arXiv and Github components of The Pile . However, Wang et al.  use their own test splits to evaluate MambaByte-353M on Stories, arXiv, and Github. Due to the rather small test splits (\( 100\)MB for the arXiv and Github datasets), this difference can be significant. For example, the validation (and test) bits-per-byte for SpaceByte-793M+184M on the Stories, arXiv, and Github datasets are 0.877 (0.833), 0.658 (0.663) and 0.397 (0.411), which differ by \(+5\%\), \(-1\%\), and \(-3\%\), respectively. Given this variation, the bits-per-byte of MambaByte-353M and SpaceByte-793M+184M are not statistically different on the arXiv or Github datasets.

Overall, we find that SpaceByte outperforms the byte-level models trained in other works. SpaceByte outperforms MegaByte, even though MegaByte was trained using 2.7 times as much compute and data. Moreover, SpaceByte's performance is competitive with the subword Transformer-1B.

## 7 Conclusion

We have proposed a new byte-level Transformer decoder architecture, SpaceByte. Our compute-controlled experiments show that SpaceByte outperforms all other byte-level architectures and roughly matches the performance of sub-word level Transformers.

**Limitations** SpaceByte uses a simple byte partitioning rule that relies on "spacelike" bytes, such as spaces which typically denote word boundaries. As such, SpaceByte should not be expected to perform well on arbitrary sequences of bytes, such as images or audio. Some languages, such as Chinese, do not use spaces between words. SpaceByte is somewhat robust to these languages, since e.g. Chinese characters are encoded using three bytes in UTF-8, which SpaceByte will group together. However, our preliminary experiments suggest that SpaceByte performs worse than subword transformers on Chinese text. It would therefore be desirable to improve upon and generalize SpaceByte's global block insertion rule.

The variable spacing between global blocks makes it more challenging to design and implement an efficient batched inference sampling algorithm for SpaceByte.

**Future Work** SpaceByte uses multiscale modeling where the local model operates on bytes while the global model typically operates on words. Another natural extension of our work is to try recursively applying multiscale modeling at even longer scales, such as the sentence or paragraph level. It would also be fruitful to investigate if Mamba blocks  could further improve SpaceByte's performance.