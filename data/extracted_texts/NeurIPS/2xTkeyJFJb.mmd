# Generative Retrieval Meets Multi-Graded Relevance

Yubao Tang\({}^{1,2}\) Ruqing Zhang\({}^{1,2}\) Jiafeng Guo\({}^{1,2}\)1 Maarten de Rijke\({}^{3}\)

Wei Chen\({}^{1,2}\) Xueqi Cheng\({}^{1,2}\)

\({}^{1}\)CAS Key Lab of Network Data Science and Technology, ICT, CAS

\({}^{2}\)University of Chinese Academy of Sciences

\({}^{3}\)University of Amsterdam

{tangyubao21b,zhangruqing,guojiafeng,chenwei2022,cxq}@ict.ac.cn m.derijke@uva.nl

Corresponding author.

###### Abstract

Generative retrieval represents a novel approach to information retrieval. It uses an encoder-decoder architecture to directly produce relevant document identifiers (docids) for queries. While this method offers benefits, current approaches are limited to scenarios with binary relevance data, overlooking the potential for documents to have multi-graded relevance. Extending generative retrieval to accommodate multi-graded relevance poses challenges, including the need to reconcile likelihood probabilities for docid pairs and the possibility of multiple relevant documents sharing the same identifier. To address these challenges, we introduce a framework called GRaded Generative Retrieval (GR\({}^{2}\)). GR\({}^{2}\) focuses on two key components: ensuring relevant and distinct identifiers, and implementing multi-graded constrained contrastive training. First, we create identifiers that are both semantically relevant and sufficiently distinct to represent individual documents effectively. This is achieved by jointly optimizing the relevance and distinctness of docids through a combination of docid generation and autoencoder models. Second, we incorporate information about the relationship between relevance grades to guide the training process. We use a constrained contrastive training strategy to bring the representations of queries and the identifiers of their relevant documents closer together, based on their respective relevance grades. Extensive experiments on datasets with both multi-graded and binary relevance demonstrate the effectiveness of GR\({}^{2}\).

## 1 Introduction

Generative retrieval (GR)  is a new paradigm for information retrieval (IR), where all information in a corpus is encoded into the model parameters and a ranked list is directly produced based on a single parametric model. In essence, a sequence-to-sequence (Seq2Seq) encoder-decoder architecture is used to directly predict identifiers (docids) of documents that are relevant to a given query. Recent studies have achieved impressive retrieval performance on many search tasks .

Current work on GR mainly focuses on binary relevance scenarios, where a binary division into relevant and irrelevant categories is assumed , and a query is usually labeled with a single relevant document  or multiple relevant documents that have the same relevance grade . The standard Seq2Seq objective, via maximizing likelihood estimation (MLE) of the output sequence with teacher forcing, has been used extensively in GR due to its simplicity. However, in real-world search scenarios, documents may have different degrees of relevance  as binary relevance may not be sufficiently represent fine-grained relevance. In traditional learning-to-rank (LTR), multi-graded relevance judgments  are widely considered, with nDCG  and ERR  being particularly popular. In modeling multi-graded relevance in LTR, a popular approach isthe pairwise method [10; 84] which involves weighting different documents based on their relevance grades and predicting the relative order of a document pair.

Compared to common LTR algorithms, the learning objective currently being used in GR differs significantly: the standard Seq2Seq objective emphasizes one-to-one associations between queries and docids, aiming to generate a single most relevant docid. Inspired by pairwise methods in LTR, a straightforward approach to extending GR to multiple grades, involves having the GR model generate the likelihood of docids with higher relevance grades being greater than that of lower relevance grades. The docid likelihood is the product of the likelihoods of each token in the generated docid. Docids commonly exhibit distinct lengths, as a fixed length might not adequately encompass diverse document semantics. However, the variation in docid lengths within the corpus may lead to smaller likelihood scores for longer docids. Although some GR work [44; 78; 92; 93; 102] use a pairwise or listwise loss for optimization, they still only consider binary relevance or require complex multi-stage optimization. Besides, essential topics in multi-graded relevant documents may be similar, emphasizing the need for a one-to-one correspondence between document content and its identifier to ensure distinctness. Consequently, harnessing a GR model's capabilities for multi-graded relevance ranking in a relatively succinct manner remains an non-trivial challenge.

To this end, we consider _multi-graded generative retrieval_ and propose a novel GRaded Generative Retrieval (GR\({}^{2}\)) framework, with three key features:

1. To enhance docid distinctness while ensuring its relevance to document semantics, we introduce a _regularized fusion_ approach with two modules: (i) a _docid generation module_, that produces pseudo-queries based on the original documents as the docids; and (ii) an _autoencoder module_, that reconstructs the target docids from their corresponding representations. We train them jointly to ensure that the docid representation is close to its corresponding document representation while far from other docid representations.
2. For the mapping from a query to its relevant docids, we design a _multi-graded constrained contrastive_ (MGCC) loss to capture the relationships between labels with different relevance grades. Considering the incomparability of likelihood probabilities associated with docids of varying lengths, we convert queries and docids into representations within the embedding space. The core idea is to pull the representation of a given query in the embedding space towards those of its relevant docids, while simultaneously pushing it away from representations of irrelevant docids in the mini-batch. To maintain the order between relevance grades in the embedding space, the strength of the pull is determined by the relevance grades of the docids. The distinction between MGCC and pairwise methods in LTR [9; 10; 84] lies in proposing more specific grade penalties and constraints to regulate the relative distances between query representations and docid representations of different grades.
3. We explore two learning scenarios, i.e., _supervised learning_ and _pre-training_, to learn generative retrieval models using our GR\({}^{2}\) framework. Importantly, our method for obtaining docids is applicable to both multi-graded and binary relevance data, and it can reduce to the supervised contrastive approach in binary relevance scenarios.

Our main contributions are: (i) We introduce a general GR\({}^{2}\) framework for both binary and multi-graded relevance scenarios, by designing relevant and distinct docids and using the information about the relationship between labels. (ii) Through experiments on 5 representative document retrieval datasets, GR\({}^{2}\) achieves 14% relative significant improvements for P@20 on Gov 500K dataset over the SOTA GR baseline RIPOR . (iii) Even in low-resource scenarios, our method performs well, surpassing BM25 on two datasets. On large-scale datasets, it achieves comparable results to RIPOR.

## 2 Related Work

**Learning to rank (LTR).** LTR ranks candidate documents using ranking functions for queries, employing pointwise, pairwise, and listwise approaches. In the pointwise approach, a query is modeled with a single document, similar to using MLE in GR, making it challenging to capture global associations. Pairwise LTR treats document pairs as instances, e.g., LambdaRank , LTRGR , and RIPOR . The MGCC loss proposed here aligns with a pairwise approach. The listwise method [78; 88] treats entire document lists as instances, involving high optimization costs.

**Generative retrieval.** GR has been proposed as a new paradigm for IR in which documents are returned using model parameters only . A single model can directly generate relevant documentsfor a query. Inspired by this blueprint, there have been several proposals [7; 14; 22; 40; 79; 85] to learn a Seq2Seq model by simultaneously addressing the two key issues below.

_Key issue 1: Building associations between documents and docids_. The widely-used model designs are pre-defined and learnable docids . Pre-defined docids are fixed during training, e.g., document titles , semantically structured strings [56; 79; 85], n-grams [7; 14; 44], pseudo-queries , URLs [68; 102; 104], product quantization code [13; 104]. Learnable docids are tailored to retrieval tasks, e.g., discrete numbers , important word sets [86; 96; 97] and residual quantization code [92; 93]. Though effective in some tasks, these docid designs have limitations. Titles and URLs rely on metadata, while n-grams demand storage of all n-grams. Quantization codes lack interpretability. Learning optimal learnable docids is challenging, involving a complex learning process. Considering performance, implementation complexity, and storage requirements, the pre-defined pseudo-query is a promising compromise choice. Unfortunately, semantically similar documents might have similar docids or even repetitions, making it challenging for the GR model to distinguish them in the both binary and multi-graded relevance scenarios. To generate diverse and relevant docids, we propose a docid fusion approach based on pseudo-queries.

_Key issue 2: Mapping queries to relevant docids_. Given a query, a GR model takes as input a query and outputs its relevant docids by maximizing the output sequence likelihood, which is only suitable for binary relevance. If a query has only one relevant document, it is paired with its relevant docid. If a query has multiple relevant documents at the same grade, it is paired with multiple relevant docids. The relative order of relevant docids in the returned list is random. Such a learning objective cannot handle search tasks with multi-graded relevance, which limits its efficacy for general IR problems. While certain GR studies [44; 92; 93; 102] employ a pairwise or listwise loss for optimization, they remain limited to binary relevance or necessitate intricate multi-stage optimization processes. In this work, we use all available relevance labels to enable multi-graded GR. For more related work, please refer to Appendix C.

## 3 Preliminaries

**Document retrieval.** Assume that \(L_{q}=[1,,l,,L]\) is the grade set representing different degrees of relevance. We assume that there exists a total order between the grades \(l>l-1>>1\), \( l L_{q}\). Let \(q\) be a query from the query set \(Q\), and \(D_{q}=\{d_{1},d_{2},,d_{N}\}\) be the set of \(N\) relevant documents for \(q\), which are selected from the large document collection \(D\). \(D_{Q}\) is the relevant document set for \(Q\). We write \(D_{q}^{l}\) for the documents with grade \(l\) for \(q\). The document retrieval task is to find a retrieval model \(f\) to produce the ranked list of relevant documents for the given query, i.e., \(_{f}(q):=[_{d}^{(1)}f(q,d),_{d}^{(2)}f(q,d),],\) where \(_{d}^{(i)}f(q,d)\) denotes the \(i\)-ranked document \(d\) for \(q\) over \(D\) given by \(f\) via matching the query and documents. The model \(f\) is optimized by minimizing its loss function over some labeled datasets, i.e., \(_{f}_{Q}_{D}(f;q,D_{q})\).

**Multi-graded generative retrieval.** In an end-to-end architecture, the GR process directly returns a ranked list for a given query, without a physical index component. Assume that the indexing mechanism to represent docids is \(I:D I_{D}\), where \(I_{D}\) is the corresponding docid set. For the observed relevant document set \(D_{q},q Q\), the indexing mechanism \(I\) maps them to the docid set \(I_{D_{q}}=\{I_{D_{q}^{l}} l=1,,L\}\), in which the docid \(id^{l} I_{D_{q}^{l}}\) is at grade \(l\). The GR model observes pairs of a query and docid with multi-graded relevance labels under the indexing mechanism \(I\), i.e., \(\{Q,I_{D_{Q}}\}\), where \(I_{D_{Q}}\) denotes \(\{I_{D_{q}^{l}} l=1,,L,q Q\}\). Given \(Q\), the model \(g:Q I_{D_{Q}}\) autoregressively generates a ranked list of candidate docids in descending order of output likelihood conditioned on each query. Mathematically, \(g(q;)=P_{}(id q)=_{t[1,|id|]}p_{}(w_{t} q,w _{<t})\), where \(w_{t}\) is the \(t\)-th token in the docid \(d I_{D}\) and \(w_{<t}\) represents all tokens before the \(t\)-th token in \(id\). \(\) is the model parameters. During inference, the GR model produces the ranked docid list via \(_{g}(q):=[g^{(1)}(q),g^{(2)}(q),]=[_{id}^{(1)}P_{}(id  q),_{id}^{(2)}P_{}(id q),]\), where \(_{id}^{(i)}P_{}(id q)\) denotes \(id\) for \(q\) whose generation likelihood is ranked at position \(i\).

## 4 Methodology

In this section, we develop a general GR framework called GR\({}^{2}\) to support multi-graded relevance learning. We address two main challenges: (i) how to generate relevant and distinct identifiers given the original documents (Section 4.1), and (ii) how to capture the interrelationship between docids in a ranking for a query (Section 4.2). Next, we introduce the optimization process (Section 4.3).

### Docid design: regularized fusion approach

A popular docid representation method is to employ a query generation (QG) technique  to generate a pseudo-query conditioned on the document as the docid . It is common for different documents to share identical or similar docids when they contain similar information . While this similarity can aid the GR model in recognizing the likeness, it also poses a challenge when the GR model needs to differentiate among multiple documents with varying relevance grades to a query. Therefore, as shown in Figure 5 in Appendix A, we propose a regularized fusion approach to optimize the trade-off between relevance and distinctness in docids.

The key idea is to jointly optimize the relevance and distinctness that fuses the latent space of a QG model, i.e., a docid generation model, and that of an autoencoder (AE) model. The AE model is used to reconstruct the target query, and both models are based on an encoder-decoder architecture. We share the same decoder for both QG and AE models as in . Specifically, we propose two simple yet effective auxiliary regularization terms, i.e., a relevance term and a distinctness term.

**Relevance regularization term.** To improve the relevance , we encourage the representation of a document and that of the corresponding docid (i.e., pseudo-query) to be close to each other in the shared latent space. We also aim to increase the distance between the representation of a document and that of irrelevant docids associated with other documents. This term is formalized as:

\[_{Rel}(Q,D_{Q};_{QG},_{AE})=-_{q Q,d D_{Q}}_{QG},e^{q}_{AE}))}{(sim(e^{d}_{QG},e^{q} _{AE}))+},\] (1)

where \(=_{d D_{Q}, Q, q}(sim(e^{d}_{ Q},e^{}_{AE}))\). For each query-document pair, \(e^{d}_{QG}\) is the document representation obtained by the encoder of the QG model, and \(e^{q}_{AE}\) is the query representation obtained by the encoder of the AE model; \(\) is one of the remaining queries except for \(q\) in the batch, and the batch size is \(|Q|\); \(sim(,)\) is the dot-product function; and \(_{QG}\) and \(_{AE}\) are model parameters of the QG model and AE model, respectively.

**Distinctness regularization term.** To enhance the distinctness between documents and between docids, we push away the representations of different documents in the document space and, simultaneously, push away the representations of different docids in the docid space. Additionally, to establish a connection between the two latent spaces of docids and documents, we ensure that the representation of a document and its corresponding docid are close in the same latent space. In a batch, the distinctness regularization term \(_{Div}(Q,D_{Q};_{QG},_{AE})\) is formalized as:

\[_{Div}()=_{d, D_{Q},d }_{QG},e^{}_{QG})}{|Q|(|Q|-1)}+_{ q, Q,q}_{AE},e^{}_{ AE})}{|Q|(|Q|-1)}-_{q Q,d D_{Q}}_{QG},e^{q}_{AE})}{|Q|},\] (2)

where \(\) is an irrelevant document with respect to \(q\) in the batch. We include a discussion on the difference between the two regularization terms in Appendix B.

**Jointly training the QG and AE model.** Both models use MLE to optimize their targets based on their inputs. Therefore, the overall optimization objective \(_{Docid}(Q,D_{Q};_{QG},_{AE})\) is:

\[_{Docid}()=^{QG}_{MLE}(Q,D_{Q};_{QG})+ ^{AE}_{MLE}(Q;_{AE})+_{Rel}()+ _{Div}(),\] (3)

where \(^{QG}_{MLE}()=-_{q Q,d D_{Q}} P_{_{QG}}(q |d)\), and \(^{AE}_{MLE}()=-_{q Q} P_{_{AE}}(q|e^{q}_{AE})\). \(P_{_{QG}}(q|d)\) and \(P_{_{AE}}(q|e^{q}_{AE})\) denote the output query likelihood conditioned on the document, and \(e^{q}_{AE}\). \(\) and \(\) are hyperparameters.

**Relevant and distinct docid generation.** During inference, following , we sample different latent vectors to generate docids. Given a document, we obtain its representation \(e^{d}_{QG}\) by the QG model's encoder. We introduce a random vector \(r\) that is uniformly sampled from a hypersphere of radius \(|r|\) centered at \(e^{d}_{QG}\), denoted as \(z_{d,r}=e^{d}_{QG}+r\). The value of \(|r|\) is tuned on the validation set to optimize the trade-off between relevance and distinctness. \(z_{d,r}\) is then used as the initial state for the decoder of QG model. We subsequently generate a list of pseudo-queries using beam search decoding. Initially, we choose the top 1 pseudo-query as the docid. If there are still duplicate docids,we select subsequent pseudo-queries from the list to replace these duplicates until all docids in the corpus are unique. According to our experimental analysis, selecting up to the top 2 can ensure docid uniqueness in the corpus. In this way, we first generate diverse docids \(id\) relevant to the original text, serving as the basis for GR model learning. Considering learning costs, the models for docid design and the GR model are distinct. Though not end-to-end, These fixed docids can guide the GR model towards appropriate optimization, whereas joint optimization increases the learning difficulty.

### Multi-graded constrained contrastive loss

After obtaining docids, we introduce the _multi-graded constrained contrastive_ (MGCC) loss for the GR model. Positive pairs and negative pairs are constructed by pairing each query with its relevant docids drawn from all grades, and with all docids relevant to the rest of queries in the mini-batch except it, respectively. As illustrated in Figure 1, the key idea is to force positive pairs closer together in the representation space, but the magnitude of the force is dependent on the relevance grade. The MGCC loss includes a grade penalty and constraint.

**Grade penalty.** To distinguish between multiple positive pairs, our key idea is to apply higher penalties to positive pairs constructed from higher grades, forcing them closer than negative pairs constructed from lower grades. We first define the loss \(_{Pair}(q,id^{l};)\) between a query \(q\) and its relevant docid at grade \(l\), as

\[_{Pair}(q,id^{l};)=_{q},_{id}^{l})/)}{_{a A_{q}}_{[_{q}_{a}]} (sim(_{q},_{a})/)},\] (4)

where \(A_{q}\) includes all positive query-docid pairs at different grades and other negative query-docid pairs for \(q\). \(_{q}\) and \(_{id}^{l}\) denote the representation of \(q\) and \(id^{l}\), respectively. They are computed based on the encoder and decoder hidden states, respectively, i.e.,

\[_{q} =(^{q};),\ \ \ _{id}^{l}=(^{l};),\] (5) \[([_{1},,_{T};]) =([_{1},,_{T}]),\] (6) \[_{t} =(_{t}+),\] (7)

where \(\) is the composition of affine transformation with the ReLU  and average pooling. \(^{l}=[_{1}^{l},,_{|id^{l}|}^{l}]\) is a concatenation of the decoder hidden states of \(id^{l}\). \(^{q}=[_{1}^{q},,_{|q|}^{q}]\) is the concatenation of the hidden representations generated by the encoder of \(q\). In this way, the loss for \(Q\) is: \(_{q Q}_{l=1}^{L}}{|I_{D_{q}^{l}}|} _{id^{l} I_{D_{q}^{l}}}_{Pair}(q,id^{l};)\), where \(id^{l} I_{D_{q}^{l}}\) is a docid at relevance grade \(l\) for \(q\); \(_{l}\) is a controlling parameter that applies a fixed penalty for each grade, contributing to preserving the relevance level explicitly.

**Grade constraint.** Inspired by the hierarchical constraint in classification [30; 98], where a class higher in the hierarchy cannot have a lower confidence score than a class lower in the ancestry sequence, for each \(q\), we propose to enforce a grade constraint \(_{Max}\), i.e., the maximum loss from all positive pairs at grade \(l\):

\[_{Max}(l,q,id^{l})=_{(q,id^{l};)}_{Pair}(q,id^ {l};).\] (8)

Figure 1: A Seq2Seq encoder-decoder architecture is used to consume queries and produce relevant docids for GR. We employ a multi-graded constrained contrastive loss (Section 4.2) to characterize the relationships among relevance labels based on the relevant and distinct docids (Section 4.1).

The loss between query-docid pairs constructed from a higher relevance grade will never be higher than that constructed from a lower relevance grade. The final MGCC loss \(_{MGCC}(Q,I_{D_{Q}};)\) is:

\[_{MGCC}()=_{q Q}_{l=1}^{L}}{|I_{D_{q}^{l}}|}_{id^{l} I_{D_{q}^{l}}}(_{Pair}(q, id^{l};),_{Max}(l+1,q,id^{l+1};)).\] (9)

For binary relevance datasets, i.e., where there is only a single level of relevance labels (i.e., \(L=1\)), the MGCC loss reduces to the supervised contrastive loss , which helps force the representation of the query close to that of its relevant documents, while far away from other irrelevant documents. In this way, GR\({}^{2}\) can also tackle the GR problem for the scenarios with binary relevance data, and this notably reduces complexity compared to multi-graded relevance.

### Learning and optimization

**Supervised learning.** Based on the docids, we directly supervise the GR model with \(_{MGCC}\), and we denote this version as GR\({}^{2S}\). To index all documents in a corpus, we adopt the MLE loss to learn document-docid pairs. To guarantee the generation of each relevant docid to a query, we adopt the MLE for query-docid pairs at different grades. The final supervised learning loss is:

\[_{total}(Q,D,I_{D};)=_{MGCC}(Q,I_{D_{Q}}; )+_{MLE}^{q}(Q,I_{D_{Q}};)+_{MLE}^{d}(D,I_{ D};),\] (10)

\[_{MLE}^{q}(Q,I_{D_{Q}};)=-_{q Q}_{l=1}^ {L}^{l}}|}_{id^{l} I_{D_{q}^{l}}} P_{}(id^ {l} q),\] (11)

and \(_{MLE}^{d}(D,I_{D};)=-_{d D} P_{}(id d)\), where \(\) is a hyperparameter, \(P_{}(id^{l} q)\) and \(P_{}(id d)\) denote the output docid likelihood conditioned on the query and document, respectively.

The learning objective currently being used in GR is usually defined as \(_{MLE}^{q}(Q,I_{D_{Q}};)+_{MLE}^{d}(D,I_{D};)\), which does not capture the relationships between labels.

**Pre-training and fine-tuning.** We also explore the use of GR\({}^{2}\) in a pre-training scenario. To construct pre-training data, we use the English Wikipedia  to build a set of pseudo-pairs of queries and docids. We use the unique titles of Wikipedia articles as the docids for pre-training and assume that a random sentence in the abstract can be viewed as a representative query of the article.

Then, for each query, we construct its relevant documents with 4 relevance grades as follows, and leave other grades as future work: (i) _grade 4_: the Wikipedia article from which the query is sampled, is regarded as the most relevant document. (ii) _grade 3_: We use the _See Also_ section of a Wikipedia article in which hyperlinks link to other articles with similar or comparable information, which is mainly written manually. If there exists no _See Also_ section, we use a similar section, i.e., the _Reference_ section. (iii) _grade 2_ and _grade 1_: Besides the _See Also_ section, some hyperlinks link to pages that describe the concept of some entities in detail. We randomly sample several anchor texts from the first section and other sections, respectively, and regard the linked target pages as grade 2 and grade 1 relevant documents, respectively.

In this way, a total of 1,180,131 query-docid pairs are obtained, and we pre-train an encoder-decoder architecture using \(_{total}\) as defined in Eq. (10). The architecture can be fined-tuned for downstream retrieval tasks using \(_{total}\), where docids are obtained via the fusion method. We denote this version as GR\({}^{2P}\). In the future, we could explore using large language models to automatically label data.

## 5 Experiments

### Experimental settings

**Datasets and evaluation metrics.** We select three widely-used multi-graded relevance datasets: Gov2 , ClueWeb09-B  and Robust04 . And we use the classic normalized discounted cumulative gain (nDCG@\(\{5,20\}\)), expected reciprocal rank (ERR@\(20\)) and precision (P@\(20\)) as metrics [12; 31; 49]. Furthermore, we consider two binary relevance datasets: MS MARCO Document Ranking  and Natural Questions (NQ 320K) . We take mean reciprocal rank (MRR@\(\{3,20\}\)) and hit ratio (Hits@\(\{1,10\}\)) as metrics following [7; 79; 85; 104]. Following existingworks [17; 74; 79; 86], for Gov2, ClueWeb09-B and MS MARCO, we primarily sampled subset datasets consisting of 500K documents for experiments, denoted as Gov 500K, ClueWeb 500K and MS 500K, respectively. For a detailed description of the datasets, please refer to Appendix D.

**Baselines.** We consider three types of baselines: sparse retrieval (SR), dense retrieval (DR), and GR models. The SR baselines include: BM25 , DocT5Query , Query Likelihood Model (QLM) , and SPLADE [24; 25]. The DR baselines include: RepBERT , DPR , PseudoQ , and ANCE . The GR baselines are DSI-Num , DSI-Sem , DSI-QG , NCI , SEAL , GENRE , Ultron-PQ , LTRGR , GenRRL , GenRet , NOVO , and RIPOR . Additionally, we compare our method with a full-ranking method, monoBERT . For a detailed description of the baselines, please refer to Appendix E.

**Model variants.** We consider two versions of GR\({}^{2}\): GR\({}^{2S}\) and GR\({}^{2P}\), for supervised learning and pre-training, respectively. Additional variants are: (i) GR\({}^{2S}_{-RF}\) and GR\({}^{2P}_{-RF}\) omit the regularized fusion approach, and directly use pseudo-queries generated by a single QG model as docids. (ii) GR\({}^{2S}_{-}\) and GR\({}^{2P}_{-}\) omit the grade penalty \(_{l}\) in \(_{MGCC}\) (Eq. (9)); (iii) GR\({}^{2S}_{-Max}\) and GR\({}^{2P}_{-Max}\) omit \(_{Max}\) in the MGCC loss; (iv) GR\({}^{2S}_{-ME}\) and GR\({}^{2P}_{MLE}\) only use \(^{q}_{MLE}\) (Eq. (11)) and \(^{d}_{MLE}\); (v) GR\({}^{2S}_{CE}\) and GR\({}^{2P}_{CE}\) use the weighted cross-entropy loss, where the relevance grades are the weights; it can be viewed as an adaption of the loss from [8; 9]; (vi) GR\({}^{2S}_{LR}\) and GR\({}^{2P}_{LR}\) directly use the LambdaRank loss .

**Implementation details.** For backbones, we choose the widely-used backbone in GR research, i.e., T5-base model  to implement the GR\({}^{2}\) and GR baselines. For docl generation, we use the docT5query model  as the QG model and a transformer autoencoder . For T5-base, the hidden size is 768, the feed-forward layer size is 12, the number of self-attention heads is 12, and the number of transformer layers is 12. GR\({}^{2}\) and the reproduced baselines are implemented with PyTorch 1.9.0 and HuggingFace transformers 4.16.2; we re-implement DSI-Num and DSI-Sem, and utilize open-sourced code for other baselines.

For hyperparameters, we use the Adam optimizer with a linear warm-up over the first 10% steps. The learning rate is 5e-5, label smoothing is 0.1, weight decay is 0.01, sequence length of documents is

    &  &  &  \\ 
**Methods** & **nDCG** & **P** & **ERR** & **nDCG** & **P** & **ERR** & **nDCG** & **P** & **ERR** \\   & \( 5\) & \( 20\) & \( 20\) & \( 50\) & \( 20\) & \( 20\) & \( 20\) & \( 55\) & \( 20\) & \( 20\) & \( 20\) \\  BM25 & 0.4984 & 0.4819 & 0.5374 & 0.1848 & 0.2579 & 0.2417 & 0.3471 & 0.1362 & - & 0.4193\({}^{}\) & **0.3657\({}^{}\)** & 0.1140\({}^{}\) \\ DocT5query & 0.3936 & 0.3861 & 0.4177 & 0.1258 & 0.2071 & 0.1631 & 0.2604 & 0.0821 & 0.3613 & 0.3229 & 0.3023 & 0.1075 \\ QLM & 0.4987 & 0.4822 & 0.5379 & 0.1851 & 0.2582 & 0.2423 & 0.3475 & 0.1365 & 0.4121 & 0.4195 & 0.3658 & 0.1143 \\ SPLADE & 0.4370 & 0.4146 & 0.4445 & 0.1575 & 0.2272 & 0.2155 & 0.3050 & 0.1109 & 0.4031 & 0.3640 & 0.3192 & 0.1088 \\  RepBERT & 0.3101 & 0.3351 & 0.4305 & 0.1446 & 0.2624 & 0.2431 & 0.3650 & 0.1663 & 0.2725 & 0.2212 & 0.1686 & 0.0812 \\ DPR & 0.3236 & 0.3408 & 0.4417 & 0.1597 & 0.2614 & 0.2576 & 0.3754 & 0.1737 & 0.2873 & 0.2316 & 0.1788 & 0.0873 \\ PseudoQ & 0.4168 & 0.4383 & 0.5134 & 0.1801 & 0.2752 & 0.2704 & 0.3926 & 0.1815 & 0.4072 & 0.3577 & 0.2823 & 0.0927 \\ ANCE & 0.4152 & 0.4379 & 0.5129 & 0.1794 & 0.2743 & 0.2696 & **0.3919** & 0.1809 & 0.4069 & 0.3573 & 0.2820 & 0.0921 \\  DSI-Num & 0.2484 & 0.2647 & 0.3237 & 0.1052 & 0.1942 & 0.1690 & 0.2520 & 0.1063 & 0.2699 & 0.2028 & 0.1524 & 0.0711 \\ DSI-Sem & 0.2497 & 0.2745 & 0.3392 & 0.1215 & 0.2004 & 0.1977 & 0.2669 & 0.1143 & 0.2711 & 0.2135 & 0.1649 & 0.0737 \\ SEAL & 0.3914 & 0.3255 & 0.4418 & 0.1592 & 0.2683 & 0.2293 & 0.2927 & 0.1305 & 0.2823 & 0.2827 & 0.1654 & 0.0885 \\ DSI-QG & 0.4566 & 0.4365 & 0.4602 & 0.1702 & 0.2722 & 0.2556 & 0.3625 & 0.1783 & 0.4089 & 0.3703 & 0.3267 & 0.1032 \\ NCI & 0.4635 & 0.4473 & 0.4722 & 0.1882 & 0.2783 & 0.2631 & 0.3734 & 0.1896 & 0.4096 & 0.3786 & 0.3349 & 0.1052 \\ Ultron-PQ & 0.4658 & 0.4496 & 0.4775 & 0.1911 & 0.2798 & 0.2652 & 0.3758 & 0.1904 & 0.4103 & 0.3797 & 0.3352 & 0.1063 \\ LTRGR & 0.4663 & 0.4517 & 0.4783 & 0.1923 & 0.2805 & 0.2664 & 0.3762 & 0.1916 & 0.4109 & 0.3805 & 0.3358 & 0.1071 \\ GenRRL & 0.

512, max training steps are 50K, and batch size is 60. We train GR\({}^{2}\) on eight NVIDIA Tesla A100 80GB GPUs. For more details, please see Appendix F.

### Experimental results

Note, _Appendix G contains additional experimental analyses_, i.e., comparisons with full-ranking baselines (Appendix G.1) and results on large-scale datasets (Appendix G.3).

#### 5.2.1 Comparison against baselines

**Results on multi-graded relevance.** Table 1 shows the performance of GR\({}^{2}\) and baselines on multi-graded relevance datasets. We find that: (i) QLM performs the best among sparse retrieval and dense retrieval baselines on Robust04 and Gov 500K, confirming prior work ; these multi-graded datasets have limited labeled training pairs, which may not be sufficient for learning semantic relationships between queries and documents. (ii) Existing GR baselines perform worse than QLM on Gov 500K and Robust04, indicating that developing an effective GR method remains an open challenge. (iii) RIPOR outperforms other GR baselines; the multi-stage training strategy appears to aid the effectiveness. (iv) By capturing the relationship between multi-graded relevance labels, GR\({}^{2}\) achieves significant improvements over GR baselines that only consider binary relevance based on the standard Seq2Seq objective. For example, GR\({}^{2P}\) and GR\({}^{2S}\) outperform RIPOR by about 11% and 14% on the Gov 500K dataset in terms of P@20, respectively. (v) Between our two methods, GR\({}^{2P}\) outperforms GR\({}^{2S}\), indicating that pre-training on large-scale elaborately constructed multi-graded relevance data, is better than training a single generation model from scratch.

**Results on binary relevance.** The performance on binary relevance datasets is shown in Table 2. We find that the relative order of different models on these datasets is almost consistent with that on the multi-graded relevance data. (i) PseudoQ outperforms QLM on these datasets. The number of labeled query-document pairs appears to be sufficient, contributing to the learning of semantic relationships. (ii) DocT5query outperforms some dense retrieval baselines on MS 500K, which may differ from

    &  &  \\   &  &  &  &  \\   & \(@33\) & \(@20\) & \(@1\) & \(@10\) & \(@3\) & \(@20\) & \(@1\) & \(@10\) \\  BM25 & 0.2171 & 0.2532 & 0.2385 & 0.3969 & 0.1456 & 0.1875 & 0.2927 & 0.6016 \\ DocT5query & 0.3378 & 0.3561 & 0.3489 & 0.5773 & 0.2612 & 0.2859 & 0.3913 & 0.697 \\ QLM & 0.2746 & 0.2805 & 0.2852 & 0.4593 & 0.2625 & 0.2864 & 0.3927 & 0.6979 \\ SPLADE & 0.3246 & 0.3483 & 0.3353 & 0.5637 & 0.3057 & 0.3404 & 0.4253 & 0.7146 \\  RepBERT & 0.3029 & 0.3382 & 0.3287 & 0.5233 & 0.3135 & 0.3421 & 0.4542 & 0.7275 \\ DPR & 0.3095 & 0.3264 & 0.3215 & 0.5432 & 0.3172 & 0.3493 & 0.5020 & 0.7812 \\ PseudoQ & 0.3342 & 0.3528 & 0.3452 & 0.5736 & 0.3253 & 0.3582 & 0.5271 & 0.7952 \\ ANCE & 0.3330 & 0.3520 & 0.3446 & 0.5729 & 0.3215 & 0.3576 & 0.5263 & 0.7931 \\  DSI-Num & 0.2159 & 0.2798 & 0.2676 & 0.4440 & 0.2286 & 0.2793 & 0.2185 & 0.4571 \\ DSI-Sem & 0.2229 & 0.2847 & 0.2753 & 0.4832 & 0.2581 & 0.3084 & 0.2740 & 0.5660 \\ GENRE & - & - & - & - & 0.3268 & 0.3467 & 0.2630 & 0.7120 \\ SEAL & 0.2977 & 0.3110 & 0.3072 & 0.5163 & 0.3367 & 0.3658 & 0.2630 & 0.7450 \\ DSI-QG & 0.3271 & 0.3457 & 0.3352 & 0.5749 & 0.3613 & 0.3868 & 0.6349 & 0.8236 \\ NCI & 0.3317 & 0.3566 & 0.3365 & 0.5833 & 0.3657 & 0.4053 & 0.6424 & 0.8311 \\ Ulron-PQ & 0.3326 & 0.3575 & 0.3379 & 0.5851 & 0.3663 & 0.4059 & 0.6461 & 0.8345 \\ LTRGR & 0.3354 & 0.3583 & 0.3381 & 0.5859 & 0.3692 & 0.4078 & 0.6511 & 0.8489 \\ GenRRL & 0.3359 & 0.3587 & 0.3389 & 0.5863 & 0.3698 & 0.4086 & 0.6528 & 0.8533 \\ GenRet & 0.3362 & 0.3591 & 0.3393 & 0.5867 & 0.3702 & 0.4095 & 0.6542 & 0.8567 \\ NOVO & 0.3371 & 0.3602 & 0.3405 & 0.5869 & 0.3724 & 0.4136 & 0.6613 & 0.8624 \\ RIPOR & 0.3384 & 0.3626 & 0.3421 & 0.5873 & 0.3741 & 0.4173 & 0.6638 & 0.8667 \\  GR\({}^{2S}\) & 0.3489\({}^{}\) & 0.3714\({}^{}\) & 0.3515\({}^{}\) & 0.6126\({}^{}\) & 0.3813\({}^{}\) & 0.4299\({}^{}\) & 0.6724\({}^{}\) & 0.8713\({}^{}\)\({}^{}\) \\ GR\({}^{2P}\) & **0.3597\({}^{}\)** & **0.3835\({}^{}\)** & **0.3821\({}^{}\)** & **0.6405\({}^{}\)** & **0.3937\({}^{}\)** & **0.4418\({}^{}\)** & **0.6832\({}^{}\)** & **0.8825\({}^{}\)** \\   

Table 2: Experimental results on datasets with binary relevance. And \(*\),\(\), \(\) and \(l\) indicate statistically significant improvements over the best performing SR baseline DocT5query or SPLADE, the DR baseline PseudoQ, the GR baseline RIPOR and all the baselines, respectively (\(p 0.05\)).

their performance on the full MS MARCO dataset. Similar experimental findings can be found in . (iii) GR\({}^{2P}\) and GR\({}^{2S}\) perform the best, the former outperforms RIPOR by 11.7% in terms of Hits@1 on the MS 500K dataset, while the latter surpasses RIPOR by 4.3% in terms of Hits@10. This indicates that GR\({}^{2}\) is a general framework for generative retrieval that can adapt to both binary relevance and multi-graded relevance scenarios. Note, Appendix G.1 contains a comparison between GR\({}^{2}\) and the full-ranking method.

#### 5.2.2 Model ablation

In Figure 2, we visualize the outcomes of our ablation analysis of GR\({}^{2}\) on Gov 500K and MS 500K.

**Docid design: regularized fusion approach.** On both datasets, (i) GR\({}^{2S}_{MLE}\) and GR\({}^{2P}_{MLE}\) outperform NCI in retrieval performance, confirming that using docids trained with the regularized fusion method is more beneficial for retrieval performance. (ii) The performance of GR\({}^{2S}_{RP}\) and GR\({}^{2P}_{RF}\) is much lower than that of GR\({}^{2S}\) and GR\({}^{2P}\), suggesting that in complex relevance scenarios, docids need to possess both relevance to the original document and distinctness.

**Training: MGCC loss.** For Gov 500K: (i) Without the grade penalty in the MGCC loss (GR\({}^{2S}_{-}\) and GR\({}^{2P}_{-}\)), the query-docid pairs at different relevance grades share the same weights, which cannot fully use information about the relationships between labels. (ii) Without the grade constraint (GR\({}^{2S}_{-Max}\) and GR\({}^{2P}_{-Max}\)), documents with higher relevance grades play a smaller role in optimization, weakening the discriminative ability to distinguish between grades. For MS 500K, i.e., in binary relevance scenarios, the MGCC loss in GR\({}^{2S}_{-}\), GR\({}^{2S}_{-Max}\) and GR\({}^{2S}\) is the supervised contrastive loss , showing the same performance.

For both datasets, GR\({}^{2S}_{PE}\) and GR\({}^{2P}_{CE}\) underperform GR\({}^{2S}\) and GR\({}^{2P}\), respectively. GR\({}^{2S}_{LR}\) and GR\({}^{2P}_{LR}\) show similar results. Both variants can be viewed as the pairwise approaches. They only distinguish docids at different grades, while the MGCC loss not only penalizes docids at different grades to be distinguished from each other, but also encourages docids with the same grade to be similar.

#### 5.2.3 Zero-resource and low-resource settings

To simulate the low-resource retrieval setting, we randomly sample different fixed limited numbers of queries from the training set. To compare GR\({}^{2}\), NCI and RIPOR, we randomly sample 15, 30, 45 and 60 queries from multi-graded relevance datasets. For binary relevance datasets, we randomly sample 2K, 4K, 6K and 8K queries. Zero-resource retrieval is performed by only indexing without

Figure 3: Supervised training and fine-tuning with limited supervision data. The x-axis indicates the number of training queries.

Figure 2: Ablation analysis. (Left) Supervised learning; (Right) Pre-training and fine-tuning.

retrieval task, i.e., the ground-truth query-document pairs are not provided. See Figure 3. We observe the following: (i) GR\({}^{2S}\) and GR\({}^{2P}\) perform better than NCI and RIPOR, indicating that GR\({}^{2}\) is able to use relevance signals from limited information. (ii) Under the zero-resource setting, all GR methods perform worse than BM25, due to the requirements of learning the mapping between queries and relevant docids. (iii) Under the low-resource setting, on ClueWeb 500K, GR\({}^{2P}\) can outperform BM25 in terms of nDCG@20. GR\({}^{2P}\) has the pre-training stage, which helps the model to acquire a discriminative ability for relevance. In general, GR leaves considerable room for improvement under such settings; pre-training GR\({}^{2}\) with diverse corpora is likely to improve its generalization ability.

#### 5.2.4 Visual analysis

We visualize query and text representations using t-SNE  to better understand the MGCC loss. Specifically, we sample the query "Radio station call letters" (QID: 848) from Gov 500K. We then plot a t-SNE example using the representations of the sampled query and its top-100 candidate documents given by the encoder output of GR\({}^{2P}\) and the representative GR baselines NCI and RIPOR. As shown in Figure 4, for GR\({}^{2P}\), documents with higher relevance grades are closer to the query than those with lower grades. And documents at the same relevance grade gather together. For NCI and RIPOR, the distribution of relevant documents in the latent space is relatively random: the standard Seq2Seq objective only learns to generate a single most relevant docid, from which it is difficult to learn the discriminative ability of multi-graded relevance.

#### 5.2.5 Efficiency analysis

We compare the efficiency of GR\({}^{2}\) and the dense retrieval model ANCE on the Gov 500K dataset. The memory footprint refers to the amount of disk space required for storage. Additionally, we assess the end-to-end inference time during the retrieval phase. (i) Regarding memory usage, GR\({}^{2}\) primarily consists of model parameters and a prefix tree for docids. ANCE necessitates dense representations for the entire corpus, with the memory requirement increasing as the corpus size grows. Notably, GR\({}^{2}\) consumes approximately 16.7 times less memory compared to ANCE. This distinction becomes even more significant when dealing with larger datasets. For instance, in comparison to DPR, a GR approach uses 34 times less memory on the entire Wikipedia [15; 22]. (ii) In terms of inference times, the heavy process on dense vectors in dense retrieval is replaced by a lightweight generative process in GR\({}^{2}\). Consequently, GR\({}^{2}\) consumes roughly 1.59 times less inference time than ANCE. Similar efficiency gains are observed in other GR-related work [15; 74; 76].

## 6 Conclusion

We have proposed a MGCC loss for multi-graded GR that captures the relationships between multi-graded documents in a ranking, and a regularized fusion method to generate distinct and relevant docids. They work together to ensure more accurate GR retrieval. Empirical results on binary and multi-graded relevance datasets have demonstrated the effectiveness of the proposed method. There are several directions that we wish to explore: (i) We adopt hard weights for each relevance grade; what is the effect of a soft assignment setting in the MGCC loss? (ii) The generated docids remain fixed after initialization; how to perform joint optimization of the docid generation and the retrieval task? Current GR research focuses on technological feasibility, but using large language models for IR has implications for transparency, provenance, and user interactions . Investigating the impact of scaled GR technology on users and societies is crucial.

Figure 4: t-SNE plots of query and document representations for GR\({}^{2P}\) (left), RIPOR (mid) and NCI (right).