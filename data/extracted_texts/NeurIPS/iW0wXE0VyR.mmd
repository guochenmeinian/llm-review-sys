# Induced Model Matching:

Restricted Models Help Train Full-Featured Models

 Usama Muneeb

Electrical and Computer Engineering

University of Illinois Chicago

umunee2@uic.edu

&Mesrob I. Ohannessian

Electrical and Computer Engineering

University of Illinois Chicago

mesrob@uic.edu

###### Abstract

We consider scenarios where a very accurate (often small) predictive model using restricted features is available when training a full-featured (often larger) model. This restricted model may be thought of as "side-information", and can come either from an auxiliary dataset or from the same dataset by forcing the restriction. How can the restricted model be useful to the full model? To answer this, we introduce a methodology called Induced Model Matching (IMM). IMM aligns the context-restricted, or induced, version of the large model with the restricted model. We relate IMM to approaches such as noising, which is implicit in addressing the problem, and reverse knowledge distillation from weak teachers, which is explicit but does not exploit restriction being the nature of the weakness. We show that these prior methods can be thought of as approximations to IMM and can be problematic in terms of consistency. Experimentally, we first motivate IMM using logistic regression as a toy example. We then explore it in language modeling, the application that initially inspired it, and demonstrate it on both LSTM and transformer full models, using bigrams as restricted models. We lastly give a simple RL example, which shows that POMDP policies can help learn better MDP policies. The IMM principle is thus generally applicable in common scenarios where restricted data is cheaper to collect or restricted models are easier to learn.

## 1 Introduction

In many applications, it is both statistically and computationally easier to construct (often small) feature-restricted models. In this paper, we address the question of whether this could be beneficial in the construction of an (often larger) full-featured model.

To motivate, consider the following toy logistic regression problem, with further details in Section 7. Say we have three features \((x_{1},x_{2},x_{3})^{3}\) and a binary label \(y\{0,1\}\). Let features be generated from a distribution \(\) and let labels be conditionally generated according to a logistic (full-featured) _true model_\(P(y|x_{1},x_{2},x_{3})\). Assume we have ample feature-restricted data with only one feature \((x_{1},y)\), which, by marginalization, are samples from the (feature-restricted) _true induced model_\((y|x_{1})=_{x_{2},x_{3}}(x_{2},x_{3}|x_{1})P(y|x_{1},x_{2 },x_{3})\). By virtue of this data, we get a very good approximation \(\) of \(\), which we can use as a _target induced model_. **How do we use \(\), along with full-featured data \((x_{1},x_{2},x_{3},y)\) to obtain a (full-featured) _learned model_\(Q(y|x_{1},x_{2},x_{3})\)?** A few possible approaches are as follows. We could ignore \(\), and learn \(Q\) simply by minimizing cross-entropy on the data. This is wasteful, because \(\) contains valuable information. Alternatively, we could learn \(Q\) by minimizing cross-entropy in addition to a secondary loss that keeps \(Q\) close to \(\). This is reasonable -- in Section 2 we relate this to reverse knowledge distillation and in Section 6 to noising. However, \(\) addresses a markedly different task than \(Q\), i.e., that of predicting with arestricted set of features. Instead, what this paper proposes is to equalize the field when comparing to \(\), by inducing a restricted model \((y|x_{1})\) from \(Q\) during training, and using a secondary loss to match \(\) to \(\), rather than to \(Q\). We call this _induced model matching_ (IMM). In Figure 1 we show how this speeds up learning and reduces predictor variance.

This example gives an overview of the entire process behind IMM. Most of the paper, however, is dedicated to language modeling, where very natural restricted models exist, namely \(N\)-grams. The inspiration of this research is in fact rooted in certain language model data augmentation techniques that _noise_ the data using \(N\)-grams Xie et al. (2017). The fresh perspective that we offer here is that such noising is best understood as an attempt to incorporate a feature-restricted model's knowledge into the full-featured model being trained. This interpretation reveals the general fundamental question: **If we are armed with a very accurate feature-restricted model, what is the right way to incorporate it into the training of a model with a larger feature set?**

Our contributions and organization are as follows:

1. In Section 3, we rigorously frame this question through the notion of an _induced model_.
2. In Section 4, we use this framework to propose a strategy to incorporate the knowledge of the restricted model during learning. This consists of using a regularizer that matches the predictions of the induced learned model with that of the desired target restricted model. This is the language model instance of the _induced model matching_ (IMM) methodology.
3. In Section 5, we propose computational approaches to make IMM practical and pave the way for further scaling IMM.
4. In Section 6, we loop back to relate IMM to the noising approach of Xie et al. (2017) and to reverse knowledge distillation. We share two key findings. First, these alternatives may be thought of as approximations to IMM. Second, they have a major caveat. They may not be consistent even in the ideal infinite-data regime, in contrast to IMM, which is consistent.
5. In Section 7, we experimentally demonstrate the effectiveness of IMM through: (1) details of the logistic regression example, (2) experiments on an LSTM RNN for language modeling and on BERT for classification, showing improvements on multiple tasks, and (3) a simple reinforcement learning example that illustrates the further potential of IMM.

In Section 8 we discuss limitations and applications beyond those illustrated in the paper.

## 2 Related Works

We review two lines of work that are closely related to IMM, noising and knowledge distillation, which are respectively implicit and explicit versions of the same idea. We also review key literature showing the continued merit of \(N\)-grams as restricted models.

Noising and Data Augmentation in Language ModelingThe noising methodology in language modeling was proposed initially in order to provide a Natural Language Processing (NLP) parallel to the many data augmentation techniques that existed in other machine learning applications (e.g., sampling translated and rotated variants, in image processing and computer vision).

Figure 1: Comparing test accuracy of logistic model, trained using interpolation, noising, and IMM (with a Bayes Optimal \(\)). Bars are \(10^{th}\) to \(90^{th}\) percentiles of \(300\) runs.

Most data augmentation techniques in NLP have used noising or smoothing in one form or another. Feature noising by Wang et al. (2013) was one of the earliest attempts at noising when it came to structured prediction tasks. Later, Xie et al. (2017) successfully used restricted language models, namely unigrams and bigrams, to perform data augmentation through noising. By making a rough connection between noising and smoothing, Xie et al. (2017) were able to show a clear advantage, beyond the use of other regularization techniques such as dropout. Section 6 more closely examines the claims of that paper and connects it to the framework of the present one.

The current approach is not related to _all_ data augmentation techniques, rather specifically to those that use noising with a restricted model. Indeed, it can complement other data augmentation techniques. For example, in our BERT experiments, this technique complements the MLM objective, where the masking itself can be thought of as data augmentation. Similarly, it is capable of complementing alternative masking and corruption techniques, such as EDA (Wei and Zou, 2019) and SSMBA (Ng et al., 2020), which are known to outperform BERT's default masking for the MLM objective. Similarly, IMM's gains are in addition to those from other regularization techniques, e.g., weight decay and dropout (Srivastava et al., 2014), as these can be simultaneously utilized.

\(N\)-grams and their Merits\(N\)-grams are based on co-occurrence counts, which makes them easy to learn but limits their usefulness for long histories. However, common techniques such as smoothing and backoff make it possible to build excellent short-context models such as bigrams and trigrams, which can rival or exceed neural models that use the same context size (Chelba et al., 2017). This hints at there being value in using these \(N\)-gram models to improve long-context models. Some of the earliest attempts at this interpolate the output of the smaller model with modern models. The continued relevance of this approach is evidenced in a very recent paper (Liu et al., 2024) that proposes a special data structure to precompute \(N\)-grams to arbitrary lengths and uses interpolation to improve larger language models (LMs). A key motivator of the current paper is the approach of Xie et al. (2017), which instead noises the training data of LMs using \(N\)-grams, with improved outcomes over interpolation. Note that other approaches that take advantage of \(N\)-grams also exist, such as that of Li et al. (2022), who let LMs learn what the \(N\)-gram could not, i.e., the residual.

Knowledge DistillationKnowledge distillation (KD) is a paradigm first proposed to let a powerful teacher model help better train a weaker student model, by complementing the hard labels of existing data with soft labels (Hinton et al., 2015). In the present notation, the resulting average loss takes the following general form,

\[(Q)+_{x}_{n}(x)_{}(P_{}^{}(|x)\|Q_{}(|x)),\] (1)

where \(\) uses hard data, \(x\) are contexts, \(_{n}\) is the empirical distribution of contexts, \(Q\) is the learned/student prediction model, and \(P^{}\) is the teacher model, which can be thought of as providing soft predictions. \(\), the softmax temperature, is used for smoothing the outputs.

Most relevant to IMM is the recent discovery that, paradoxically, KD with a weak teacher can be helpful to a powerful student. This "reverse-KD" or "distillation from weak teachers" phenomenon was first demonstrated in vision by Yuan et al. (2020), who interpret and ascribe its performance to smoothing targets in a context-dependent way using the teacher, in contrast to typical label smoothing that can be interpreted as using a uniform distribution. Though this comes years after the noising papers, it parallels closely how Xie et al. (2017) transition from uniform noising to Kneser-Ney noising. Later, reverse-KD was also shown to be effective in language models (Qin et al., 2021).

On the surface, IMM is similar to reverse-KD, as it uses a weak target teacher to regularize a powerful student's learning. A quick comparison to Eq. (14) when \(P^{}\) is the restricted model \(\) reveals that reverse-KD has more in common with noising, which we show can be sub-optimal. Indeed, the major difference between reverse-KD and IMM is the fact that _the weakness of the teacher in this case is of a very particular nature_ -- it stems from the reliance on a restricted context. Reverse-KD ignores this fact by comparing the student model \(Q\)_directly_ to the teacher model in the KL term of Eq. (1). In contrast, IMM harnesses this fact, by comparing the student _indirectly_ to the teacher (target model) at its own level, i.e., using the induced model \(\). Very recent work by Lee et al. (2023) shows that reverse-KD in language, unlike in vision, can be harmful with very weak teachers, and thus it is not advisable to use reverse-KD with the simple restricted models, e.g., bigrams, that we consider here. Our work shows that they _can_ be effectively incorporated, if we use IMM instead.

Problem Description

Problem SettingWe consider the problem of training a full model \(Q(y_{t}|x_{t})\) that is able to predict a label of data point \(t\) based on its _full context_. For example, in forward predictive models, \(x_{t}\) is the sequence of past tokens and \(y_{t}\) is the next token. Throughout the paper, \(Q\) always denotes the full model and is deemed to belong to a reasonably large functional class. Given context-prediction data \((x_{t},y_{t})\) of size \(n\), possibly tokenized from a single text, let the primary loss used for training this model be the log-loss. Thus training minimizes the cross-entropy risk:

\[Entropy}(Q)=-_{t} Q(y_{t}|x_{t})_{x} _{n}(x)_{y}P_{n}(y|x),\]

where \(_{n}\) is the empirical distribution of the context and \(P_{n}\) be the empirical distribution of the prediction given the context. Note that \((x_{t},y_{t})\) refer to tokens while \((x,y)\) refer to types. Minimizing this empirical risk can be seen as \(Q\) striving to approach the "true" model \(P(y_{t}|x_{t})\) generating the data, in average context-conditional KL divergence. The idealized risk is thus:

\[_{x}(x)_{y}P(y|x)_{x}(x)\; }(P(|x)\|Q(|x))\] (2)

where now \(\) is also the true (not empirical) _context distribution_.

Induced ModelsWhile \(Q\) strives to approximate all of \(P\), we may have additional knowledge about \(P\) that captures some of its properties. We consider in particular knowledge of the following form. Assume that the full context \(x_{t}\) can be decomposed into a short context \(_{t}\) and an extended context \(_{t}\), and that one has access to the "true" model that predicts \(y_{t}\) based solely on the short context \(_{t}\), i.e. \((y|)\). To make the notation easier to follow, we invite the reader to consult the glossary of notations in Appendix A. How is the restricted model related to \(P\) and \(\)? By a simple marginalization argument, we can see that this model is:

\[(y|)= _{}(y,|)= _{}(|)\;P(y|, )\] (3)

We call \(\) the true _induced model_. It depends both on the context distribution \(\) and the true model \(P\). Since we do not have the latter two, we cannot explicitly compute \(\). What motivates us, however, is the possibility to learn it more accurately than \(P\), either by virtue of its smaller parametrization or thanks to cheaply procured auxiliary context-restricted data.

Problem StatementGiven knowledge of the true induced model \(\), or a very good approximation thereof, how can this information be incorporated to improve the learned model \(Q\)?

## 4 Induced Model Matching (IMM)

Construction of the IMM riskTo address the problem, we introduce a secondary loss that _matches_ the learned model's prediction with that of the _induced model_, whence the name _Induced Model Matching_ or IMM. The key insight here is _not_ to match \(\) with \(Q\), but rather with \(\), the learned induced model that, just like the move from \(P\) to \(\) in Eq. (3), specializes \(Q\) to performing predictions with only the short context:

\[(y|)=_{}(| {x})Q(y|,)\] (4)

Let's first idealize and assume availability of \(\) and the context distribution \(\), required to compute \(\). Equipped with \(\), we can introduce the _idealized induced model matching_ (IMM) risk, a secondary risk that is the average context-conditional KL divergence with the restricted context:

\[_{x}(x)(y|)(y|)}{(y|)}}_{} ((|)\|(|) )}\] (5)Since \(\) and \(\) are not available in practice, the idealized IMM risk cannot be computed. However, as the core motivation of using \(\) is the potential ability to learn it very accurately from data, we assume instead that we have access to a _target induced model_\(\) as a proxy to \(\). As for calculating \(\), knowledge of \(\) in Eq. (4) can be intuitively understood as a mechanism for filling-in for the extended context, based on the short context. As such, we have the following natural empirical version which can be thought of as averaging \(Q\) over all extended contexts in the dataset, while keeping the short context fixed:

\[(y|)_{t}\{_{t}=\}Q(y|x_{t})\] (6)

The proportionality constant is unimportant as, thanks to the logarithm, it contributes only a constant to the overall risk. By combining these empirical proxies, we obtain **the empirical IMM risk**:

\[(Q)=_{x}_{n}(x)_{y}(y|) {(y|)}\] (7)

This mirrors Eq. (5), up to \(\) inside the logarithm, which only contributes an additive entropy term that does not depend on \(Q\) and is thus irrelevant to optimization.

IMM as a regularizerGiven a reliable \(\), we propose to incorporate this knowledge into the full model by using the IMM risk as a regularizer. Using a hyper-parameter \(\) that can be tuned, our _induced model matching_ methodology consists of training the model \(Q\) by minimizing

\[(Q)+\ (Q).\] (8)

Separating IMM into componentsTo weave IMM into existing ML optimization pipelines, it is useful to treat it as a separable risk. For this, we can rewrite Eq. (7) by expanding the empirical distribution \(_{n}\). Then, the components of this separable risk are given by the conditional cross-entropies between \(\) and \(\), because we can write:

\[(Q)=-_{t}(y| _{t})(y|_{t})]}_{_{t}(Q)}.\] (9)

The simplest version of \(\) could be based on empirical counts, which may be valid if the context space is discrete and small. In language modeling, smoothing methods can be used to generate better versions of \(\), such as the modified Kneser-Ney smoothed version of the bigram counts (Kneser and Ney, 1995; Chen and Goodman, 1999) that can be expressed as

\[(y|)=_{t}(1-())\{ y_{t}=y\}+()b(y),\]

where \(()\) is the missing mass for previous token \(\) and \(b\) is the back-off distribution. For logistic regression in Section 7 the context space is continuous, and these components come from a separately trained one-feature predictor. For our RL example, they come from the optimal actions of a POMDP.

## 5 Computation

A direct implementation of IMM is prohibitive, since evaluation of the risk, Eq. (8), requires making a secondary pass over the entire dataset for each \(t\), when inducing the model in Eq. (6). We address this with two approaches. The first approximates the objective by replacing this secondary pass with _sampling_ whereas the second incorporates this secondary pass into the primary pass by _serializing_ the gradient calculation. _Sampled IMM_ has the advantage of low gradient variance at the expense of added computation. _Serialized IMM_ has higher gradient variance but has only constant-factor computational overhead. All of our experiments use sampled IMM, with the exception of a demonstration of serialized IMM for logistic regression (in Appendix D.5), as a proof of concept for IMM's scalability.

[MISSING_PAGE_EMPTY:6]

derivative of \(Q\) relative to \(\). What this accomplishes is to delegate the averaging of the gradients to the primary pass over the dataset. If the correction term is given, computing this gradient costs the same as computing the baseline of no-IMM gradients. There are two caveats: first, because the averaging is now happening in the primary pass, the variance of this gradient is higher than sampled IMM and, second, the correction factor still requires knowing the learned induced model \(\). To address the higher variance of the gradients, techniques such as momentum approaches along with learning rate schedulers can be used. To address knowing \(\), we suggest the heuristic of updating \(\) only periodically. Then, if model \(Q_{}\) (that eventually becomes stale) is used to calculate \(_{}\), use the ratio \(Q_{}/_{}\) for correction factor. Using the current \(Q\) tends to cause instability, likely due to the correction no longer obeying expected constraints, e.g., mean \(1\) for every \(y\). By choosing the update period inversely proportionally to the cost of updating \(_{}\) and by keeping \(Q_{}\) in memory, we incur an \((1)\) factor increase in time and space compared to no-IMM, which makes this variant of IMM highly scalable. Note that without the correction factor, IMM turns into reverse-KD and noising, a connection that we elaborate on in Section 6.

## 6 Analysis

In what follows, we assume to be in the realizable case, i.e., that the model space of \(Q\) is expressive enough to contain the true model \(P\). We also take an infinite-data perspective and focus on consistency only, even though IMM also confers a finite-sample advantage (understanding this advantage analytically is worth studying in the future.) We show \((1)\) that IMM is consistent, \((2)\) that noising and reverse-KD are akin to single-sample IMM, \((3)\) that this shows that they minimize an upper bound on the IMM risk, and finally \((4)\) that this introduces sub-optimality, which could even threaten consistency. This gives basic analytical backing to why IMM is preferable.

Consistency of IMMObserve that if, in the main objective, Eq. (8), cross-entropy and IMM were replaced with their true counterparts, Eqs. (2) and (5) respectively, then \(Q=P\) remains the minimizer of the objective. This observation shows that we recover the true model in the infinite-data regime, i.e., that IMM is consistent for all \(\). We next aim to show that the key caveat of noising and reverse-KD is that they may be inconsistent, unless \(\) is made to vanish.

From single-sample IMM to noising and reverse-KDWe first explain how IMM and noising are related. Experimentally, using a single sample (\(k=1\)) in the IMM approximation of Eq. (12) produces perplexities that are near-identical to noising. We explain this phenomenon as follows. Say data point \(t\) is considered during optimization, in an SGD mini-batch. When a single random extended context is sampled, it is equivalent to swapping that data point's context with another based on the multiset \((_{t})\). That context belongs to a different data point \(t^{}\). Since sampling is done uniformly from the multiset, this simply presents data to the SGD algorithm in a different random order, possibly with repetition. More pertinently to noising, the actual prediction \(y_{t^{}}\) has no role in calculating the loss. Instead, the prediction is randomized through the sum over all possible \(y\) in Eq. (9). Though not identical, this is a very close map to the noising-based data augmentation proposed by Xie et al. (2017), namely prediction noising (see Appendix B.1 for a review of this framework and why prediction noising is its main aspect.) It is in fact even closer to an alternative proposed later by Gao et al. (2019), which uses soft prediction vectors just like \(\) in this single-sample approximation of IMM. As a result of this argument, we can think of noising as minimizing the following idealized objective, which coincidentally is equivalent to the reverse-KD objective (Yuan et al., 2020; Qin et al., 2021) (see also Section 2) with \(\) as the teacher:

\[Entropy}(Q)+_{x}(x)_{y}(y| )}_{}\] (14)

Inconsistency of noising and reverse-KDHow is this single-sample IMM objective related to performing IMM? Recall that we can write a single component of the empirical IMM risk with sampling as in (12), which by Jensen's inequality can be upper bounded as follows:

\[-_{y}(y|)(_{X}[Q(y|_{t},X)])-_{y}(y|)_{X}[ (Q(y|_{t},X))]\] (15)A single-sample approximation of the expectation _inside_ the \(\) is in fact a biased estimator of the left-hand side. However, it is an unbiased estimator of the right-hand side, with the expectation _outside_ of the \(\). Thus, noising and reverse-KD upper bound the IMM risk. Minimizing an upper bound instead of the desired risk _could_ introduce suboptimality. The following proposition uses the difference between these methods, which pit the target model against the full learned model \(Q\) and _not_ the induced learned model \(\) like IMM (contrast Eq. (14) and Eq. (5)), to show that this suboptimality can indeed occur. Even in the realizable case and with infinite data, IMM is always consistent but there exists a counterexample where noising and reverse-KD fail to consistently recover the true model. The proof is in Appendix B.2.

**Proposition 6.1**.: _Assume that we are optimizing the idealized noising objective of Eq. (14) -- i.e., we are operating in the infinite-data regime -- and let \(Q^{}\) be its global minimizer. Assume further that the model class for \(Q\) contains the true model \(P\) -- i.e., we are in the realizable case. Then, there exists a choice of \(\) and \(P\) such that \(Q^{} P\)._

## 7 Experimental Results

### Starting Toy Example: Logistic Regression

Consider the logistic regression example from the introduction. The main results are given in Figure 1 and the full experimental details can be found in Appendix D.1. Here we highlight how the problem fits the IMM framework and where it deviates from language modeling. First, note that the context decomposition is \(=x_{1}\) and \(=(x_{2},x_{3})\).

SettingWe sample features uniformly over a cube and assume we have ample data points of the form \((x_{1},y)\). This allows us to build an excellent _restricted model_ to predict the label based on just \(x_{1}\), call it \((y|x_{1})\), nearly close to the (restricted) Bayes predictor or true conditional probability. Just like in language modeling, to induce a model we need to draw \(\)'s from its conditional distribution given \(\). \((y|x_{1})\), the _induced model_ of \(Q(y|x_{1},x_{2},x_{3})\), can then be interpreted as the average of \(Q\)'s predictions, when \(_{t}=(x_{2},x_{3})\) is drawn from its conditional distribution given \(_{t}=x_{1}\). Since we typically don't have access to this distribution, we approximate it empirically. In language modeling, we could just sample from the empirical distribution of \(\) for a given \(\). In logistic regression, this is not viable since \(x_{1}\) is continuous and does not repeat. We rely instead on density estimation. We use a soft nearest-neighbor density estimate \((x_{2},x_{3}|x_{1})_{t=1}^{n}_{x_{2,t},x_{3},t}(x_{2},x_{3})^{-|x_{1,t}-x_{1}|}\), where \(1/\) is the bandwidth of the Laplace kernel. (With cross-validation, we determine \(=1\) to be a good choice.) If we let \(w_{t}(x_{1})=^{-|x_{1,t}-x_{1}|}\), the resulting induced model by marginalization is:

\[(y|)\!=\! f(|)Q(y| ,)\!\!_{t=1}^{n}( )}{_{t=1}^{n}w_{t}()}Q(y|,_{t})\]

These equations are respectively equivalent to Eqs. (4) and (6). The IMM risk and the corresponding overall objective remain the same:

\[(Q)=_{t=1}^{n}_{y=0,1}(y|x_{1,t})(y|x_{1,t})},(Q)+\;(Q).\]

ResultsIn Figure 1, we compare the performance of IMM-trained \(Q\) (green) to that without IMM (maroon). We sweep a range of \(n\) from \(2\) to \(50\), and use a cross-validation optimized \(\) for each (details in the Appendix D.1.1). The key observations are that: (1) IMM always improves on the baseline performance, (2) the variance of the outcomes is also typically diminished, (3) the improvement is greater with less data, but the gain across data sizes is equivalent to access to an average of \(30\%\) extra data. This and similar experiments suggest that gains are highest when the dataset size is comparable to the number of parameters. This simple scenario demonstrates how IMM effectively harnesses the benefit of accurate feature-restricted models when training full-featured models. For reference, we also include the results of noising and interpolation. IMM is always better than noising, but interestingly interpolation is better with very few samples, though much worse with more. We attribute this to the fact that with less data it is harder to obtain an accurate induced model.

### Language Modeling Experiments

In these language modeling experiments, the restricted model we use is the modified Kneser-Ney bigram (Kneser and Ney, 1995; Chen and Goodman, 1999) of the dataset in question. To see why this is a good choice, we refer the reader to benchmarking done by Chelba et al. (2017); neural models (single layer and 2-layer LSTM RNNs) could not improve upon the perplexity of an interpolated Kneser-Ney \(N\)-gram of a similar order. After the introduction of the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015), better neural models now dominate language modeling (Vaswani et al., 2017; Devlin et al., 2018; Turc et al., 2019). We investigate how IMM could potentially improve even these more modern models, by using BERT's performance on some GLUE benchmarks as a proof of concept. (Appendix D.2.1 gives evidence that IMM indeed improves the full model's performance on the restricted task.)

LSTM RNN ExperimentsWe build on top of the code provided by Xie et al. (2017) using the same topology for the LSTM RNN. The chief purpose of these experiments is to contrast directly with noising introduced in that paper. The PTB dataset is a document dataset and the LLM is solving a _next word prediction_ task. The average cross-entropy of multiple unroll positions of the RNN, or exponentiated as _perplexity_, is used as the measure of performance. For training and measuring validation perplexity, \(L=35\) unroll positions are used. During testing, only 1 unroll position is used. The IMM component is always calculated by running the LSTM in an evaluation mode (i.e., without any dropout). In addition, while regular evaluation updates the state in stateful models like the LSTM, the IMM branch never changes the state and uses the state set by the primary branch when traversing the dataset. In Table 1, we report perplexity values using \(k\)-sampled IMM with \(k=10\). The table also includes the best noising results that we could reproduce based on the code of Xie et al. (2017), after communication with the authors (these are 0.5 more than the paper's numbers).

BERT ExperimentsWe introduce the IMM objective in BERT's _fine-tuning_ phase by reintroducing the Masked Language Model (MLM) objective that is originally only present during pre-training. In Google's original BERT code, MLM was present during pre-training but removed from fine-tuning, possibly because of minimal gain. For us, however, MLM is ideally suited to be augmented with the IMM objective because it is based on cross-entropy and we can similarly generate an _induced bigram_ for predicting masked words in a sequence of tokens. We report numbers on these datasets in Table 2. The second column shows the numbers after adding back the MLM objective, which doesn't produce much gain on its own. The third column adds IMM within MLM, significantly boosting the gains.

Since some GLUE (Wang et al., 2018) datasets (used in the original BERT paper) are too large to be trained in an academic setting, we use a subset of the GLUE tasks (Warstadt et al., 2018; Dolan and Brockett, 2005; Rajpurkar et al., 2016; Dagan et al., 2005) to demonstrate the gains using IMM. For diversity, our selected GLUE tasks are a mixture of single-sentence and double-sentence tasks. Further experimental details are provided in Appendix D.2.

   Improvement & Validation & Test \\  None (only regular dropout) & 81.6 & 77.5 \\ KN Noising (reproducible) & 76.7 & 73.9 \\ IMM with KN Bigram & **76.0** & **73.3** \\   

Table 1: Perplexity for an LSTM Language Model using the Penn TreeBank dataset. The numbers on None and KN Noising are from Xie et al. (2017) and can be replicated using their original code (we use the model with latent dimension 1500). Like the baseline, for each row, we report the best value across as many restarts.

    & BERTBASE & + MLM & +IMM \\  CoLA & 52.1 \(\) 4.0 & 55.0 \(\) 3.0 & **60.0 \(\) 1.0** \\ MRPC & 88.9 \(\) 2.0 & 89 \(\) 1.0 & **90 \(\) 1.0** \\ QNLI & 90.5 \(\) 2.0 & 91.0 \(\) 2.0 & **93.5 \(\) 1.0** \\ RTE & 66 \(\) 3.0 & 68 \(\) 2.0 & **71 \(\) 1.0** \\   

Table 2: Results on the BERTBASE Language Model. The baseline numbers can be replicated using the original BERT code by Google, as well as our provided repository. Matthewâ€™s Correlation Coefficient is used for CoLA, F1 score for MRPC and Accuracy for QNLI and RTE. Like the baseline, reported numbers are averages across multiple restarts.

### Reinforcement Learning: POMDPs helping MDPs

IMM is particularly appealing in situations where incomplete-feature data may be much more available, due to reasons like law, privacy, or limited sensing. If an autonomous driving system is developed with few sensors, and later a new system taking advantage of more sensors is to be designed, the older system may act as a restricted model helping the new design. If a diagnostic system is built based on a limited set of genetic markers, and later more markers are determined relevant, then the legacy system can be used without referring to the legacy data, which may need to be kept private. If a platform's recommendation and ad engine is trained from ample general-public data, and later a personalized engine is to be developed, then the older engine can inform the personalized one through IMM.

In stateful environments, problems like the latter often require a reinforcement-learning (RL) solution. If the personalized engine has full-featured data, it can use the data to train an MDP (Markov Decision Process) policy to optimize expected reward (Sutton and Barto, 2018). In contrast, the general-public data, despite being abundant, may lack in features and may only allow for solving a POMDP (Partially Observable Markov Decision Process). We can show that IMM can allow a good POMDP solution to significantly improve MDP training, by modifying policy-gradient methods such as REINFORCE (Williams, 1992). In Figure 2, we illustrate the reward achieved with and without the use of IMM, for learning policies for an agent on a toroidal \(11 11\) grid, with reward peaking at its center. The POMDP only observes one coordinate, whereas the MDP observes both.

## 8 Conclusion

In this paper, we addressed the question of how to incorporate accurate restricted models in the training of full-featured models using the principle of induced model matching (IMM). This was inspired by interpreting some noising-based data augmentation techniques in natural language noising, as an attempt to harness the knowledge of the restricted model used for noising. We showed that naive noising is not the best way to incorporate this knowledge, as it may fail to consistently recover the true model. IMM, on the other hand, directly aligns the learned model with the target model and is consistent. The results shows that IMM always outperforms noising, and improvements even decay gracefully with lower restricted model quality (see Appendix D.4). One limitation of our approach is that computing the induced model exactly is not always viable. To remedy this, we proposed sampled IMM, which yields accurate but somewhat computationally demanding learning, and serialized IMM, which is slightly less accurate but has a potential to be as efficient as the no-IMM baseline. We then experimentally demonstrated the gains that IMM can offer, in a logistic regression toy example, when training LSTM language models and fine-tuning pretrained transformers, and in a simple reinforcement learning scenario. We believe that scaling from feature-restricted to full-featured models is an important yet under-studied sub-problem of knowledge transfer. In addition to the proof-of-concept experiments in this paper, many others of particular contemporary relevance may be devised. For example, lengthening the context of large language models remains an open problem; we believe that IMM can be part of betters solutions, by correctly informing new longer-context LLMs using current shorter-context LLMs. The principle behind IMM is applicable very generally and we hope this work gives impetus to such explorations.

Figure 2: Average reward of MDP trained without and with IMM incorporating POMDP. Details in Appendix D.