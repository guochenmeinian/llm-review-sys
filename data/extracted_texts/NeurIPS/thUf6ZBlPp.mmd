# EigenVI: score-based variational inference with orthogonal function expansions

Diana Cai

Flatiron Institute

dcai@flatironinstitute.org &Chirag Modi

Flatiron Institute

cmodi@flatironinstitute.org &Charles C. Margossian

Flatiron Institute

cmargossian@flatironinstitute.org &Robert M. Gower

Flatiron Institute

rgower@flatironinstitute.org &David M. Blei

Columbia University

david.blei@columbia.edu &Lawrence K. Saul

Flatiron Institute

lsaul@flatironinstitute.org

###### Abstract

We develop EigenVI, an eigenvalue-based approach for black-box variational inference (BBVI). EigenVI constructs its variational approximations from orthogonal function expansions. For distributions over \(^{D}\), the lowest order term in these expansions provides a Gaussian variational approximation, while higher-order terms provide a systematic way to model non-Gaussianity. These approximations are flexible enough to model complex distributions (multimodal, asymmetric), but they are simple enough that one can calculate their low-order moments and draw samples from them. EigenVI can also model other types of random variables (e.g., nonnegative, bounded) by constructing variational approximations from different families of orthogonal functions. Within these families, EigenVI computes the variational approximation that best matches the score function of the target distribution by minimizing a stochastic estimate of the Fisher divergence. Notably, this optimization reduces to solving a minimum eigenvalue problem, so that EigenVI effectively sidesteps the iterative gradient-based optimizations that are required for many other BBVI algorithms. (Gradient-based methods can be sensitive to learning rates, termination criteria, and other tunable hyperparameters.) We use EigenVI to approximate a variety of target distributions, including a benchmark suite of Bayesian models from posteriorib. On these distributions, we find that EigenVI is more accurate than existing methods for Gaussian BBVI.

## 1 Introduction

Probabilistic modeling is a cornerstone of modern data analysis, uncertainty quantification, and decision making. A key challenge of probabilistic inference is computing a target distribution of interest; for instance, in Bayesian modeling, the goal is to compute a posterior distribution, which is often intractable. Variational inference (VI) [5; 21; 45] is a popular method for scalable probabilistic inference that has worked across a range of applications. The idea behind VI is to approximate the target distribution by the closest member of some tractable family.

One major focus of research is to develop _black-box_ algorithms for variational inference [6; 15; 23; 27; 32; 37; 40; 44; 46]. Algorithms for black-box variational inference (BBVI) can be used toapproximate any target distribution that is differentiable and computable up to some multiplicative (normalizing) constant; as such, they are extremely flexible. These algorithms have been widely implemented in popular probabilistic programming languages, and they are part of the modern toolbox for practitioners in computational statistics and data analysis [1; 4; 7; 13; 43].

Traditionally, the variational approximations in BBVI are optimized by minimizing the Kullback-Leibler (KL) divergence between the variational family and the target (equivalently, maximizing the ELBO). This strategy is powerful and scalable, but it relies on stochastic gradient descent (SGD), which can be difficult to tune [10; 11; 51]. These difficulties can be acute even for Gaussian variational approximations [27; 40], particularly if these approximations employ full covariance matrices.

More recently, researchers have proposed algorithms for Gaussian BBVI that do not require the use of SGD [6; 37]. Instead of minimizing the KL divergence, these methods aim to match the _scores_, or the gradients of the log densities, between the variational distribution and the target density. These methods exploit the special form of Gaussian distributions to derive closed-form proximal point updates for score-matching. These updates are as inexpensive as SGD, but not as brittle. They show that score-based BBVI can be applied in an elegant way to Gaussian variational families.

In this paper, we show that score-based BBVI also yields simple, closed-form updates for a much broader family of variational approximations. Specifically, we propose a new class of variational families constructed from _orthogonal function expansions_ and inspired by solutions to the Schrodinger equation in quantum mechanics. These families are expressive enough to parameterize a wide range of target distributions; at the same time, the distributions in these families are sufficiently tractable that one can calculate low-order moments and draw samples from them. In this paper, we mostly use orthogonal function expansions to construct distributions supported on \(^{D}\); in this case, the lowest-order term in the expansion is sufficient to model Gaussian behavior, while higher-order terms account for increasing amounts of non-Gaussianity. More generally, we also show how different basis sets of orthogonal functions can be used to construct variational families over other spaces.

To optimize over a variational family from this class, we minimize an estimate of the Fisher divergence, which measures the scores of the variational distribution against those of the target distribution. We show that this optimization reduces to a minimum eigenvalue problem, thus avoiding the need for gradient-based methods. For this reason, we call our approach _EigenVI_.

We study EigenVI with a variational family constructed from weighted Hermite polynomials. We first demonstrate the expressiveness of this family on a variety of multimodal, asymmetric, and heavy-tailed distributions. We then use EigenVI to approximate a diverse collection of non-Gaussian target distributions from posteriori, a benchmark suite of Bayesian hierarchical models. On these problems, EigenVI provides more accurate posterior approximations than leading implementations of Gaussian BBVI based on KL minimization and score-matching.

The organization of this paper is as follows. In Section2 we introduce the variational families that arise from orthogonal function expansions, and we show how score-matching in these families reduces to an eigenvalue problem. In Section3 we review the literature related to EigenVI. In Section4, we evaluate EigenVI on a variety of synthetic and real-data targets. Finally, in Section5, we discuss limitations and future work.

## 2 Score-based variational inference with orthogonal function expansions

In this section we use orthogonal function expansions to develop new variational families for approximate probabilistic inference. In Section2.1, we review the basic properties of these expansions. In Section2.2, we introduce a score-based divergence for VI with these families; notably, for this divergence, the optimization for VI reduces to an eigenvalue problem. Finally in Section2.3, we consider how to use these variational approximations for unstandardized distributions; in these settings we must carefully manage the trade-off between expressiveness and computational cost.

### Orthogonal function expansions

Let \(^{D}\) denote the support of the target distribution \(p\). Suppose that there exists a complete set of orthonormal basis functions \(\{_{k}(z)\}_{k=1}^{}\) on this set. By _complete_, we mean that any sufficiently well-behaved function \(f:\) can be approximated, to arbitrary accuracy, by a particular weighted sum of these basis functions, and by _orthonormal_, we mean that the basis functions satisfy

\[\!_{k}(z)_{k^{}}(z)\,dz=\{1&k=k^{},\\ 0&.\] (1)

where the integral is over \(\). Define the \(K^{}\)-order variational family \(_{K}\) to be the set containing all distributions of the form

\[q(z)=(_{k=1}^{K}_{k}_{k}(z))^{2} _{k=1}^{K}_{k}^{2}=1,\] (2)

and where \(_{k}\) for \(k=1,,K\) are the parameters of the family \(_{K}\). In words, \(_{K}\) contains all distributions that can be obtained by taking weighted sums of the first \(K\) basis functions and then _squaring_ the result.

Eq. 2 involves a squaring operation, a sum-of-squares constraint, and a weighted sum. The squaring operation ensures that the density functions in \(_{K}\) are nonnegative (i.e., with \(q(z)\!\!0\) for all \(z\)), while the sum-of-squares constraint ensures that they are normalized:

\[\!q(z)\,dz\,=\,\!(_{k=1}^{K}_{k}_{k}(z))^{2} \!dz\,=\,\!_{k,k^{}=1}^{K}_{k}_{k^{}}_{k} (z)_{k^{}}(z)\,dz\,=\,_{k=1}^{K}_{k}^{2}=1.\] (3)

The weighted sum in Eq. 2 bears a superficial similarity to a mixture model, but note that neither the basis functions \(_{k}(z)\) nor the weights \(_{k}\) in Eq. 2 are constrained to be nonnegative. Distributions of this form arise naturally in physics from the quantum-mechanical _wave functions_ that satisfy Schrodinger's equation . In that setting, though, it is typical to consider complex-valued weights and basis functions, whereas here we only consider real-valued ones.

The simplest examples of orthogonal function expansions arise in one dimension. For example, functions on the interval \([-1,1]\) can be represented as weighted sums of Legendre polynomials, while functions on the unit circle can be represented by Fourier series of sines and cosines; see Table 1. Distributions on unbounded intervals can also be represented in this way. On the real line, for example, we may consider approximations of the form in Eq. 2 where

\[_{k+1}(z)=(k!)^{-}(e^{-z ^{2}})^{}\,\,_{k}(z),\] (4)

and \(_{k}(z)\) are the _probabilist's_ Hermite polynomials given by

\[_{k}(z)=(-1)^{k}e^{}{2}}}{dz^{k}}[e^{- }{2}}].\] (5)

Note how the lowest-order basis function \(_{1}(z)\) in this family gives rise (upon squaring) to a Gaussian distribution with zero mean and unit variance.

Figure 1 shows how various multimodal distributions with one-dimensional support can be approximated by computing weighted sums of basis functions and squaring their result. We emphasize that _the more basis functions in the sum, the better the approximation_.

Orthogonal function expansions in one dimension are also important because their Cartesian products can be used to generate orthogonal function expansions in higher dimensions. For example, we can

  
**support** & **orthogonal family** & **basis functions**\(_{k}()\) \\  \(z[-1,1]\) & Legendre polynomials & \(\{1,\,z,\,3z^{2}\!-\!1,\,5z^{3}\!-\!3z,\}\) \\ \(z=e^{i} S^{1}\) & Fourier basis & \(\{1,,, 2, 2,\}\) \\ \(z[0,)\) & weighted Laguerre polynomials & \(e^{-}\{1,1-z,z^{2}-4z\!+\!2,\,\}\) \\ \(z\) & weighted Hermite polynomials & \(e^{-}{4}}\{1,z,(z^{2}\!-\!1),(z^{3}\!-\!3z),\}\) \\   

Table 1: Examples of orthogonal function expansions in one dimension. The basis functions in the table are not normalized, but they can be rescaled so that their squares integrate to one.

approximate distributions over (say) \(^{3}\) by

\[q(z_{1},z_{2},z_{3})=(_{i=1}^{K_{1}}_{j=1}^{K_{2}}_{k=1}^{K_{3}} _{ijk}\,_{i}(z_{1})_{j}(z_{2})_{k}(z_{3}))^{2}_{ijk}_{ijk}^{2}=1,\] (6)

where \(_{ijk}\) now parametrize the family. Note that there are a total \(K_{1}K_{2}K_{3}\) parameters in the above expansion, so that this method of Cartesian products does not scale well to high dimensions if multiple basis functions are used per dimension. Note that the same strategy can also be used for random variables of mixed type: for example, from Table 1, we can create a variational family of distributions over \([-1,1][0,)\) from the Cartesian product of orthogonal function expansions involving Hermite, Legendre, and Laguerre polynomials.

As shown in Figure 1, the approximating distributions from \(K^{}\)-order expansions can model the presence of multiple modes as well as many types of asymmetry, and this expressiveness also extends to higher dimensions. Nevertheless, it remains tractable to sample from these distributions and even to calculate (analytically) their low-order moments, as we show in Appendices A and B.

For concreteness, consider the distribution over \(^{3}\) in Eq. 6. The marginal distribution \(q(z_{1})\) is

\[q(z_{1})=\!q(z_{1},z_{2},z_{3})\,dz_{2}\,dz_{3}=_{ii^{}}[ _{jk}_{ijk}_{i^{}jk}]_{i}(z_{1})_{i^{ }}(z_{1}),\] (7)

and from this expression, moments such as \([z_{1}]\) and \([z_{1}]\) can be calculated by evaluating integrals involving the elementary functions in Table 1. (In practice, these integrals are further simplified by recursion relations that relate basis functions of different orders; we demonstrate how to compute the first two moments for the normalized Hermite family in Eqs. 19 and 22.)

To generate samples \(\{z^{(t)}\}\), each dimension is sampled as follows: we draw \(z_{1}^{(t)} q(z_{1})\) by computing the cumulative distribution function (CDF) of this marginal distribution and then numerically inverting this CDF. Finally, extending these ideas, we can calculate higher-order moments and obtain joint samples via the nested draws

\[z_{1}^{(t)} q(z_{1}), z_{2}^{(t)} q(z_{2}\,|\,z_{1}), z_{3}^ {(t)} q(z_{3}\,|\,z_{1},z_{2}).\] (8)

The overall complexity of these procedures scales no worse than quadratically in the number of basis functions in the expansion. These extensions are discussed further in Appendices A and B.

### EigenVI

In variational inference, we posit a parameterized family of approximating distributions and then compute the particular approximation in this family that is closest to a target distribution of interest. Eq. 2 constructs a variational family \(_{K}\) from the orthogonal functions \(\{_{k}(z)\}_{k=1}^{K}\) whose variational parameters are the weights \(\{_{k}\}_{k=1}^{K}\). We now derive _EigenVI_, a method to find \(q_{K}\) that is close to the target distribution \(p(z)\).

Figure 1: Target probability distributions (black dashed curves) on the interval \([-1,1]\) (left), the unit circle (middle), and the real line (right), and their approximations by orthogonal function expansions from different families and of different orders; see Eq. 2 and Table 1.

We first define the measure of closeness that we will minimize. EigenVI measures the quality of an approximate density by the _Fisher divergence_,

\[(q,p)=\| q(z)- p(z)\|^{2}q(z)dz,\] (9)

where \( q(z)\) and \( p(z)\) are the score functions of the variational approximation and target, respectively. Suppose that \(q\) and \(p\) have the same support; then the Fisher divergence vanishes if and only if the scores of \(q\) and \(p\) are everywhere equal.

Though \(p\) is, by assumption, intractable to compute, in many applications it is possible to efficiently compute the score \( p\) at any point \(z\). For example, in Bayesian models the score of the target posterior is equal to the gradient of the log joint. This observation is the main motivation for score-based methods in probabilistic modeling .

Here we seek the \(q\!\!_{K}\) that minimizes \((q,p)\). But now a challenge arises: it is generally difficult to evaluate the integral for \((q,p)\) in Eq. 9, let alone to minimize it as a function of \(q\). While it is possible to sample from the distribution \(q\), it is not straightforward to simultaneously sample from \(q\) and optimize over the variational parameters \(\{_{k}\}_{k=1}^{K}\) in terms of which it is defined. Instead, we construct an unbiased estimator of \((q,p)\) by importance sampling, which also decouples the sampling distribution from the optimization. Let \(\{z^{1},z^{2}, z^{B}\}\) denote a batch of \(B\) samples drawn from some proposal distribution \(\) on \(\). From these samples we can form the unbiased estimator

\[}_{}(q,p)=_{b=1}^{B})}{(z^{b})} \| q(z^{b})- p(z^{b})\|^{2}.\] (10)

This estimator should be accurate for appropriately broad proposal distributions and for sufficiently large batch sizes. We can therefore attempt to minimize Eq. 10 in place of Eq. 9.

Now we show that the minimization of Eq. 10 over \(q_{K}\) simplifies to a minimum eigenvalue problem for the weights \(\{_{k}\}_{k=1}^{K}\). To obtain the eigenvalue problem, we substitute the orthogonal function expansion in Eq. 2 into Eq. 10 for the unbiased estimator of \((q,p)\). As an intermediate step, we differentiate Eq. 2 to obtain the scores

\[ q(z^{b})=_{k}_{k}(z^{b})}{_{k} _{k}_{k}(z^{b})}.\] (11)

Further substitution of the scores provides the key result behind our approach: the unbiased estimator in Eq. 10 is a simple quadratic form in the weights \(:=[_{1},,_{K}]^{}\) of the orthogonal function expansion,

\[}_{}(q,p)=^{}\!M,\] (12)

where the coefficients of the quadratic form are given by

\[M_{jk}=_{b=1}^{B})}[2_{j}(z^{b})-_{j }(z^{b}) p(z^{b})][2_{k}(z^{b})-_{k}( z^{b}) p(z^{b})].\] (13)

Note that the elements of the \(K\!\!K\) symmetric matrix \(M\) capture all of the dependence on the batch of samples \(\{z^{b}\}_{b=1}^{B}\), the scores of \(p\) and \(q\) at these samples, and the choice of the family of orthogonal functions. Next we minimize the quadratic form in Eq. 12 subject to the sum-of-squares constraint \(_{k}_{k}^{2}=1\) in Eq. 2. In this way we obtain the eigenvalue problem 

\[_{q_{K}}[}_{}(q,p)]=_{ \|\|=1}[^{}\!M]=:_{}(M),\] (14)

where \(_{}(M)\) is the minimal eigenvalue of \(M\), and the optimal weights are given (up to an arbitrary sign) by its corresponding eigenvector; see C for a proof. EigenVI solves Eq. 14.

We note that the eigenvalue problem in EigenVI arises from the curious alignment of three particular choices--namely, (i) the choice of variational family (based on orthogonal function expansions), (ii) the choice of divergence (based on score-matching), and (iii) the choice of estimator for the divergence (based on importance sampling). The simplicity of this eigenvalue problem stands in contrast to the many heuristics of gradient-based optimizations--involving learning rates, terminating criteria, and perhaps other algorithmic hyperparameters--that are typically required for ELBO-basedBBVI [10; 11]. But EigenVI is also not entirely free of heuristics; to compute the estimator in Eq. 10 we must also specify the proposal distribution \(\) and the number of samples \(B\); see Appendix D for a discussion.

The size of the eigenvalue problem in EigenVI is equal to the number of basis functions \(K\) in the orthogonal function expansion of Eq. 2. The eigenvalue problem also generalizes to orthogonal function expansions that are formed from Cartesian products of one-dimensional families, but in this case, if multiple basis functions are used per dimension, then the overall basis size grows exponentially in the dimensionality. Thus, for example, the eigenvalue problem would be of size \(K_{1}K_{2}K_{3}\) for the approximation in Eq. 6, as can be seen by "flattening" the tensor of weights \(\) in Eq. 6 into the vector of weights \(=()\) in Eq. 2. Finally, we note that EigenVI only needs to compute the minimal eigenvector of \(M\) in Eq. 14, and therefore it can benefit from specialized routines that are much less expensive than a full diagonalization.

### EigenVI in \(^{D}\): the Hermite family and standardization

We now discuss the specific case of EigenVI for \(\!=\!^{D}\) with the Hermite-based variational family in Eq. 4. For this case, we propose a transformation of the domain that serves to precondition or _standardize_ the target distribution before applying EigenVI. While this standardization is not required to use EigenVI, it helps to reduce the number of basis functions needed to approximate the target, leading to a more computationally efficient procedure. It also suggests natural default choices for the proposal distribution \(\) in Eq. 10.

Recall that the eigenvalue problem grows linearly in size with the number of basis functions. Before applying EigenVI, our goal is therefore to transform the domain in a way that reduces the number of basis functions needed for a good approximation. To meet this goal for distributions over \(^{D}\), we observe that the lowest-order basis function of the Hermite family in Eq. 4 yields (upon squaring) a standard multivariate Gaussian, with zero mean and unit covariance. Intuitively, we might expect the approximation of EigenVI to require fewer basis functions if the statistics of the target distribution nearly match those of this lowest-order basis function. The goal of standardization is to achieve this match, to whatever extent possible, by a suitable transformation of the underlying domain. Having done so, EigenVI in \(^{D}\) can then be viewed as a systematic framework to model non-Gaussian effects via a small number of higher-order terms in its orthogonal function expansion.

Concretely, we consider a linear transformation of the domain:

\[=^{-}(z\!-\!),\] (15)

where \(\) and \(\) are estimates of the mean and covariance obtained from some other algorithm (e.g., a Laplace approximation, Gaussian variational inference, Monte Carlo, or domain-specific knowledge). We then apply the EigenVI to fit a \(K\)th-order variational approximation \(()\) to the target distribution \(()\) that is induced by this transformation; afterwards, we reverse the change-of-variables to obtain the final approximation to \(p(z)\), i.e.,

\[q(z)=()||^{-1/2}.\] (16)

Figure 2 shows why it is more difficult to approximate distributions that are badly centered or poorly scaled. The left panel shows the effect of translating a standard Gaussian _away_ from the origin and _shrinking_ its variance; note how a comparable approximation to the uncentered Gaussian now requires a 16th-order expansion. On the other hand, after standardization, the target can be perfectly fitted by the base distribution in the orthogonal family of reweighted Hermite polynomials. The right panel shows the similar effect of translating the mixture distribution in Figure 1 (right panel); comparing these panels, we see that twice as many basis functions (\(K\!=\!14\) versus \(K\!=\!7\)) are required to provide a comparable fit of the uncentered mixture.

Finally, we note another benefit of standardizing the target before fitting EigenVI; when the target has nearly zero mean and unit covariance, it becomes simpler to identify natural choices for the proposal distribution \(\). Intuitively, in this case, we want a proposal distribution that has the same mean but heavier tails than a standard Gaussian. In our experiments, we use two types of centered proposal distributions--uniform and isotropic Gaussian--whose variances are greater than one.

## 3 Related work

Several recent works have considered BBVI methods based on score-matching. These methods take a particularly simple form for Gaussian variational families [6; 37]. The Fisher divergence  has been previously studied as a divergence for variational inference . Yu and Zhang  propose minimizing a Fisher divergence for semi-implicit (non-Gaussian) variational families; the divergence is minimized using gradient-based optimization. In another line of work, Zhang et al.  consider variational families of energy-based models and derive a closed-form solution to minimize the Fisher divergence in this setting.

More generally, there have many studies of VI with non-Gaussian variational families. One common extension is to consider families of mixture models [14; 17; 36]; these are typically optimized via ELBO maximization. BBVI algorithms have also been derived for more expressive variational families of energy-based models [9; 22; 28; 29; 52; 53] and normalizing flows [3; 24; 25; 34; 39; 41]. However the performance of these models, especially the normalizing flows, is often sensitive to the hyperparameters of the flow architecture and optimizer, as well as the parameters of the base distribution [2; 11]. Other aspects of these variational approximations are also less straightforward; for example, one cannot compute their low-order moments, and one cannot easily evaluate or draw samples from the densities of energy-based models.

The variational approximation in EigenVI is based on the idea of squaring a weighted sum of basis functions. Probability distributions of this form arise most famously in quantum mechanics . This idea has also been used to model distributions in machine learning, though not quite in the way proposed here. Novikov et al.  propose a tensor train-based model for density estimation, but they do not consider orthogonal basis sets. Similarly, Loconte et al.  obtain distributions by squaring a mixture model with negative weights, and they study this model in conjunction with probabilistic circuits. By contrast in this work, we consider this idea in the context of variational inference, and we focus specifically on the use of orthogonal function expansions, which have many simplifying properties; additionally, the specific objective we optimize leads to a minimum eigenvalue problem.

## 4 Experiments

We evaluate EigenVI on 9 synthetic targets and 8 real data targets. In these experiments we use the orthogonal family induced by _normalized Hermite polynomials_ (see Table 1), whose lowest-order expansion is Gaussian. Thus, this variational family can model non-Gaussian behavior with the higher-order functions in its basis. We first study 2D synthetic targets and use them to demonstrate the expressiveness of these higher-order expansions. Next, we experiment with target distributions where we systematically vary the tail heaviness and amount of skew. Finally, we apply EigenVI to a set of hierarchical Bayesian models from real-world applications and benchmark its performance against other Gaussian BBVI algorithms.

### 2D synthetic targets

We first demonstrate how higher-order expansions of the variational family yield more accurate approximations on a range of 2D non-Gaussian target distributions (Figure 3); see Appendix E.2

Figure 2: Higher-order expansions may be required to approximate target distributions (black) that are not standardized. _Left:_ approximation of a non-standardized Gaussian. _Right:_ approximation of the mixture distribution in Figure 1 after translating its largest modes away from the origin.

for details. We report an estimate of \((p;q)\) above each variational approximation. The Gaussian variational approximation is fit using batch and match VI , which minimizes a score-based divergence. For EigenVI, the target distributions were not standardized before fitting EigenVI (we compare the costs of the methods in Figure E.1), and the total number of basis functions is \(K\!=\!K_{1}K_{2}\).

### Non-Gaussianity: varying skew and tails in the sinh-arcsinh distribution

We now consider the sinh-arcsinh normal distribution [19; 20], which is induced by transforming a multivariate Gaussian using parameters that control the amount of skew and the weight of the tails. We construct several targets (\(D=2,5\)) of increasing amounts of non-Gaussianity in the skew or the tails of the distribution, and we refer to these targets as _slight skew and tails_, _more skew and tails_, and _slight skew and heavier tails_; see Appendix E.3 for details. In Figure 3(a), we visualize the 2D targets and the EigenVI fits along with their forward KLs. Before applying EigenVI, we standardize the target using a mean and covariance estimated from batch and match VI . In Figure 3(b), we measure the EigenVI forward KL under varying numbers of samples \(B\) and across increasing numbers of basis functions, given by \(K\!=\!_{d=1}^{D}K_{d}\). We also present the forward KL resulting from batch and match VI (BaM) and automatic differentiation VI (ADVI), which both use Gaussian variational families and are run using the same budget in terms of number gradient evaluations. Next we consider similar targets with \(D=5\), which are visualized in in Figure E.2, along with the resulting EigenVI variational approximations. In Figure 3(c), we observe greater differences in the number of importance samples needed to lead to good approximations, especially as the number of basis functions increase.

### Hierarchical modeling benchmarks from posteriorib

We now evaluate EigenVI on a set of hierarchical Bayesian models [7; 35; 42], which are summarized in Table E.1. The goal is posterior inference: given data observations \(x_{1:N}\), the posterior of \(z\) is

\[p(z\,|\,x_{1:N}) p(z)p(x_{1:N}\,|\,z)=:(z),\] (17)

where \(p(z)\) is the prior and \(p(x_{1:N}\,|\,z)\) denotes the likelihood.

We compare EigenVI to 1) automatic differentiation VI (ADVI) , which maximizes the ELBO over a full-covariance Gaussian family (ADVI), 2) Gaussian score matching (GSM) , a score-based BBVI approach with a full-covariance Gaussian family, and 3) batch and match VI (BaM) , which

Figure 3: 2D target functions (column 1): a 3-component Gaussian mixture distribution (row 1), a funnel distribution (row 2), and a cross distribution (row 3). We report the \((p;q)\) for the resulting optimal variational distributions obtained using score-based VI with a Gaussian variational family (column 2) and the EigenVI variational family (columns 3â€“5), where \(K\!=\!K_{1}K_{2}\).

minimizes a regularized score-based divergence over a full-covariance Gaussian family. In these examples, we standardize the target using either GSM or BaM before applying EigenVI.

In these models, we do not have access to the target distribution, \(p(z\,|\,x_{1:N})\), only the unnormalized target \(\). Thus, we cannot evaluate an estimate of the forward KL. Instead, to evaluate the fidelity of the fitted variational distributions, we compute the empirical Fisher divergence using reference samples from the posterior obtained via Hamiltonian Monte Carlo (HMC):

\[_{s=1}^{S}\|(z^{s})- q(z^{s}) \|^{2}, z^{s} p(z\,|\,x_{1:N}).\] (18)

Note that this measure is not the objective that EigenVI minimizes; it is analogous to the forward KL divergence, as the expectation is taken with respect to \(p\). We report the results in Figure 5, computing the Fisher divergence for EigenVI with increasing numbers of basis functions. We typically found that with more basis functions, the scores becomes closer to that of the target.

Finally, we provide a qualitative comparison with real-NVP normalizing flows (NFs) , a flexible variational family that is fit by minimizing the reverse KL. We found that after tuning the batch-size and learning rate, NFs generally had a suitable fit. We visualize the posterior marginals for a subset of dimensions from 8schools in the top three rows, comparing EigenVI, the NF, and BaM. Here, we observe that the Gaussian struggles to fit the tails of this target distribution. On the other hand, EigenVI provides a competitive fit to the normalizing flow. In Appendix E.4, we show the full corner plot in Figure 3 and marginals of the garch11 model in Figure 4.

## 5 Discussion of limitations and future work

In this work, we introduced EigenVI, a new approach for score-based variational inference based on orthogonal function expansions. The score-based objective for EigenVI is minimized by solving an eigenvalue problem, and thus this framework provides an alternative to gradient-based methods for BBVI. Importantly, many computations in EigenVI can be parallelized with respect to the batch of

Figure 4: Sinh-arcsinh normal distribution synthetic target. Panel (a) shows the three targets we consider in 2D, and their resulting EigenVI fit. Panel (b) shows measures \((p;q)\) for \(D=2\), and panel (c) shows \((p;q)\) for \(D=5\); the \(x\)-axis shows the number of basis functions, \(K\!=\!_{d}K_{d}\).

samples, unlike in iterative methods. We applied EigenVI to many synthetic and real-world targets, and these experiments show that EigenVI provides a principled way of improving upon Gaussian variational families.

Many future directions remain. First, the approach described in this paper relies on importance sampling, and thus it may benefit from more sophisticated methods for adaptive importance sampling. Second, it may be useful to construct variational families from different orthogonal function expansions. Our empirical study focused on the family built from normalized Hermite polynomials. But this family may require a very high-order expansion to model highly non-Gaussian targets, and such an expansion will be very expensive in high dimensions. Though this family was sufficient for many of the targets we simulated, others will be crucial for modeling highly non-Gaussian targets. Another direction is to develop variational families whose orthogonal function expansions scale more favorably with the dimension, perhaps by incorporating low rank structure in the target's covariance. Finally, it would be interesting to explore iterative versions of EigenVI in which each iteration solves a minimum eigenvalue problem on some subsample of data points. With such an approach, EigenVI could potentially be applied to very large-scale problems in Bayesian inference.

Figure 5: Results on posteriorib models. Top three rows: marginal distributions of the even dimensions from 8-schools. Reference samples from HMC are outlined in gray, and the VI samples are in green. Bottom two rows: evaluation of methods with the (forward) Fisher divergence. The \(x\)-axis shows the number of basis functions, \(K\!=\!_{d}K_{d}\). Shaded regions represent standard errors computed with respect to 5 random seeds.