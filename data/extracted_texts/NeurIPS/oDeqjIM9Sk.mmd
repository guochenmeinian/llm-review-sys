# Weight decay induces low-rank attention layers

Seijin Kobayashi\({}^{}\)\({}^{1,2}\), Yassir Akram\({}^{}\)\({}^{1}\)

Johannes von Oswald\({}^{2}\)

Equal contribution, order determined randomly

###### Abstract

The effect of regularizers such as weight decay when training deep neural networks is not well understood. We study the influence of weight decay as well as \(L2\)-regularization when training neural network models in which parameter matrices interact multiplicatively. This combination is of particular interest as this parametrization is common in attention layers, the workhorse of transformers. Here, key-query, as well as value-projection parameter matrices, are multiplied directly with each other: \(W_{K}^{T}W_{Q}\) and \(PW_{V}\). We extend previous results and show on one hand that any local minimum of a \(L2\)-regularized loss of the form \(L(AB^{})+(\|A\|^{2}+\|B\|^{2})\) coincides with a minimum of the nuclear norm-regularized loss \(L(AB^{})+\|AB^{}\|_{}\), and on the other hand that the 2 losses become identical exponentially quickly during training. We thus complement existing works linking \(L2\)-regularization with low-rank regularization, and in particular, explain why such regularization on the matrix product affects early stages of training. Based on these theoretical insights, we verify empirically that the key-query and value-projection matrix products \(W_{K}^{T}W_{Q},PW_{V}\) within attention layers, when optimized with weight decay, as usually done in vision tasks and language modelling, indeed induce a significant reduction in the rank of \(W_{K}^{T}W_{Q}\) and \(PW_{V}\), even in fully online training. We find that, in accordance with existing work, inducing low rank in attention matrix products can damage language model performance, and observe advantages when decoupling weight decay in attention layers from the rest of the parameters.

## 1 Introduction

The influence of \(L2\)-regularization, as well as _weight decay_ regularization when training deep neural network models remains poorly understood and is still a subject of active research (van Laarhoven, 2017; Zhang et al., 2021, 2019; Loshchilov and Hutter, 2019; Zhang et al., 2021; Xie et al., 2023; Andriushchenko et al., 2023). Given a model parametrized by matrix \(W\), the standard motivation of adding \(\|W\|^{2}\) to the optimization loss \(L(W)\) comes from framing learning the model weights \(W\) as maximum a posteriori (MAP) estimation and choosing a Gaussian prior with zero mean (Mackay, 1995; Krogh and Hertz, 1991).

Previous works have studied the effect of regularization on the rank of weight matrices when training a model with gradient-based optimization (Ziyin and Wang, 2023; Arora et al., 2019; Li et al., 2021; Razin and Cohen, 2020; Gunasekar et al., 2017). Here, we focus on the effect of \(L2\)-regularization on models using a _factorized_ parametrization, where some weight matrices are parametrized as productsof (often lower rank) matrices, \(W=AB^{}\). This parametrization is used heavily in attention layers inside transformers (Vaswani et al., 2017) which we will focus on in the following.

Indeed, at the heart of the Transformer architecture is the attention operation which updates the \(T\)_tokens_ concatenated into a matrix \(E^{d_{m} T}\) inside the network according to

\[E E+PW_{V}E(E^{}W_{K}^{}W_{Q}E) M\] (1)

where \(\) is typically a softmax operation applied column-wise and \(M\) is typically a causal mask. The matrices \(W_{V},W_{K},W_{Q}^{d_{k} d_{m}}\) are respectively the value, key, and query matrices that linearly transform \(E\) into some typically smaller space of dimension \(d_{k}\)(Phuong and Hutter, 2022), which can potentially subsume bias terms by appending a constant \(1\) to the tokens. The weight matrix \(P^{d_{m} d_{k}}\) projects the weighted sum of value vectors back into the original token dimension. Therefore (multi-head) attention layers indeed consist of parameter matrix products i.e. \(W_{QK}=W_{K}^{}W_{Q}\) as well as \(W_{VP}=PW_{V}\), regardless of the choice of \(\), or the presence or absence of causal masks.

When optimizing neural network models with this particular parametrization in conjunction with \(L2\)-regularization, and for any such two weight matrices \(A\) and \(B\) (e.g. the \(P\) and \(W_{V}\) for a given layer and a given head), we can rewrite the loss as:

\[_{L2}(A,B,) L(AB^{},)+( \|A\|^{2}+\|B\|^{2}),\] (2)

where \(\) accounts for all the remaining parameters. We will see in the following that optimizing such losses has in practice implications on regularizing the rank of \(W=AB^{}\). In fact, while it is classically known that the summed Frobenius norm \((\|A\|^{2}+\|B\|^{2})\) is a tight upper bound on the _nuclear norm_\(\|AB^{}\|_{}\)(Srebro and Shraibman, 2005; Tibshirani, 2021), we theoretically show in the following that gradient-based optimization of the above objective result in the upper bound becoming tight exponentially quickly, for arbitrary loss, and thus directly optimizes for the nuclear norm which is known to induce low rank.

We highlight the relevance of this study since high weight decay is commonly used when training Transformer models. For example, GPT-3 (Brown et al., 2020), LLaMa (Touvron et al., 2023), LLaMa 2 (Touvron et al., 2023) and ViT (Dosovitskiy et al., 2021) report a weight decay strength of \(=0.1\). Interestingly, this is even true when fine-tuning, for example with low-rank adaptation (LoRA) (Hu et al., 2021).

We summarize our contributions below:

* We show that for models with factorized parametrization, all local minima of any loss regularized by the Frobenius norm of \(A,B\) coincide with local minima of the same loss regularized by the nuclear norm of \(W\). We further show theoretically that the discrepancy between the 2 regularizations vanishes exponentially quickly during training, thus implying that training such models with weight regularization can be subjected to low rank inducing pressure long before convergence.
* We empirically validate our result on various experimental settings, including when optimization with decoupled weight decay (Loshchilov and Hutter, 2019), on models ranging from deep linear networks to language models as well as Vision Transformers Dosovitskiy et al. (2021). Intriguingly, we observe that this inductive bias of factorized parametrization with weight decay seems to hurt the performance on some tasks, raising the question of whether it is a feature or a bug.
* We provide evidence suggesting that this rank-regularizing effect in fact seems to affect the pretraining of popular pre-trained foundation models such as LLAMA 2 (Touvron et al., 2023) and Vision Transformer (Wu et al., 2020), by analyzing their pre-trained weights.

## 2 Related Work

The setting we study is closely related to a setting extensively studied in the Matrix Completion literature (Srebro and Shraibman, 2005; Sun and Luo, 2016; Candes and Tao, 2009), where the goal is to recover an unknown low-rank matrix for which only a subset of its entries are specified. Nuclear norm regularization is often used as a convex relaxation of the problem (Hu et al., 2021), and its equivalence at the global optimum with the \(L2\)-regularization on factorized matrix (Srebro and Shraibman, 2005), which has the advantage of being differentiable everywhere, has been exploited as a popular approach for large-scale matrix completion. Extensive prior work has focused in this setting on the theoretical guarantee of the factorization formulation to recover the underlying low-rank matrix correctly (Sun and Luo, 2016; Candes and Tao, 2009). Similarly, similar loss landscape analyses were performed in the context of unconstrained features models (Zhu et al., 2021). In contrast, our analysis does not rely on assumptions about the data, the loss (other than its differentiability) or convergence.

In a different line of work, recent efforts have focused on the effect of gradient-based optimization of deep networks on the parametrized matrix. For example, small weight initialization in this setting was shown to induce low rank in deep linear networks (Jacot et al., 2022; Arora et al., 2019; Li et al., 2021). More recently, (Jacot, 2023) has shown the representation cost of deep networks with homogeneous nonlinearity converges to a notion of rank over nonlinear functions. More related to our work, equivalence between \(L2\) regularization applied on factorized matrices and a low-rank inducing \(L_{p}\)-Schatten norm on the matrix they parametrize has been shown in several prior works (Dai et al., 2021; Tibshirani, 2021). This is particularly relevant as \(L2\) regularization can be applied explicitly or implicitly, such as when training deep networks with homogeneous activation coupled with e.g. the cross entropy loss (Jacot et al., 2022; Arora et al., 2019). Crucially, however, these existing works characterize the low-rank inducing bias on neural networks that globally minimize \(L2\) regularization while fitting training data.

Recently, (Galanti et al., 2023) have studied the effect of SGD with L2-regularization on a general architecture. Similarly to our work, they consider a general differentiable loss, but bound the rank of matrices at sufficiently large training steps, employing a theoretical argument that crucially does not leverage low-rank inducing norms due in part to the generality of the architecture they consider. (Wang and Jacot, 2023) have studied the same effect in the context of deep fully connected linear networks, showing that SGD strengthens the already existing low-rank bias induced by L2-regularization, albeit on matrix completion problems. Similarly to our work, they draw for the first time, to the best of our knowledge, an equivalence between the critical points of L2-regularized loss on the factorized matrix and Nuclear norm regularized loss on the parametrized matrix.

In contrast to these past works, we show both theoretically and empirically that for any arbitrary differentiable loss, the two regularizations become exponentially quickly identical during gradient-based optimization, and thus, that the low-rank inducing effect comes into play very early in during training. This brings a theoretical understanding to empirical observations made in previous works (Khodak et al., 2022), and is particularly relevant for many practical settings, in which learning does not converge, such as foundation model trained online, as is commonly done for large language models (LLMs) and large vision models.

Finally, given the significance of self-attention models, there has been work trying to understand the implicit inductive biases of some of their design choices. (Bhojanapalli et al., 2020) shows, in particular, the head size heuristic commonly used causes a low-rank bottleneck and limits the expressive power of the multi-head attention layer. Recent work has shown indeed that reducing the rank of attention matrices post-training of LLMs can hurt downstream performance (Sharma et al., 2023). Our empirical work complements these observations and sheds light on the potentially damaging effect of the implicit rank-reducing effect of weight decay in the context of Attention layers, an unintended side effect contrary to the matrix completion setting.

## 3 Theoretical results

### Preliminaries

We begin by reviewing the definition of the nuclear norm of a matrix and its upper bound when applied to a factorized matrix. We denote by \(\|\|\) the Frobenius norm when applied on matrices.

#### 3.1.1 Nuclear norm

The nuclear norm (also known as trace norm) of a real-valued matrix \(W\), denoted by \(\|W\|_{*}\), is defined as

\[\|W\|_{*}=(})\] (3)When using the singular value decomposition (SVD) of \(W\), \(W=USV^{}\), denoting \((s_{i})_{i}\) the singular values, we can see that

\[\|W\|_{*}=(VSU^{}})=(S)= _{i}s_{i}\] (4)

i.e. the nuclear norm is the sum of the singular values of \(W\).

The nuclear norm is often used in the low-rank regularization literature  as it intuitively is a convex relaxation of the rank, and regularizing it typically induces low rank by injecting sparsity in the singular values.

#### 3.1.2 Upper bound of the nuclear norm of a factorized matrix

Let two matrices \(A,B\) such that \(W=AB^{}\). Then, using the Cauchy-Schwarz inequality, we have that

\[\|W\|_{*}=(S) =(U^{}AB^{}V)\] (5) \[(U^{}AA^{}U)(B^{}VV^{}B)}\] (6) \[=\|A\|\|B\|(\|A\|^{2}+\|B\|^{2})\] (7)

#### 3.1.3 Considered losses

We will consider \(L2\) losses of the format

\[_{L2}(A,B) L(AB^{})+(\|A\|^{2}+\|B\|^ {2}),\] (8)

and their \(L_{*}\) counterpart

\[_{*}(AB^{}) L(AB^{})+\|AB^{}\|_{*}.\] (9)

As a consequence of the above inequality, the \(L2\)-regularized objective (8) is an upper bound of the nuclear norm-regularized objective.

The meticulous reader should spot that those objectives don't account for the remaining parameters \(\) as in (2), while those parameters also evolve through learning. In fact, one can convince oneself that this can be safely ignored without loss of generality. The reader is referred to appendix D for more details about this point.

### Equivalence of optimization solution

In the following, we will first show that in fact, any objective of the form in (8) will coincide at any stationary point with the nuclear-norm regularized loss in (9), thus introducing a low-rank inducing bias in the solution found. We assume \(A,B\) to have a bottleneck, i.e. to have the number of rows greater or equal to the number of columns, as is usual in attention layers. All proofs can be found in Appendix B.

We start by providing a sufficient condition under which the averaged Frobenius norm of two matrices would correspond to the nuclear norm of their product.

**Proposition 3.1**.: _Let \(A,B\) be matrices such that \(A^{}A=B^{}B\). Then, denoting \(AB^{}=USV^{}\) the SVD of \(AB^{}\), there exist an orthogonal matrix \(O\) such that \(A=U(\\ 0)O^{}\) and \(B=V(\\ 0)O^{}\). In particular, \(\|AB^{}\|_{*}=(\|A\|^{2}+\|B\|^{2})\)._

This condition states that the scalar product of any two columns of \(A\) should match the scalar product of corresponding columns of \(B\). We will show next that at any stationary point of the objective \(_{L2}\), that condition is fulfilled. We assume the loss \(L\) is differentiable and \(>0\).

**Lemma 3.2**.: _At any stationary point \(A,B\) of \(_{L2}\) we have that \(A^{}A=B^{}B\)._The above Lemma, together with Proposition 3.1, implies that at a stationary point \(A,B\), \(_{L2}(A,B)\) and \(_{*}(AB^{})\) coincide. However, this is not enough to claim that finding a (local) minimum of \(_{L2}\) will in fact find a (local) minimum of \(_{*}\). We now provide a result which shows that this claim is true.

**Theorem 3.3**.: \(A,B\) _is a local minimum of \(_{L2}\), if and only if 1) \(W=AB^{}\) is a local minimum of \(_{*}\), constrained to matrices of rank \(r\) where \(r\) is the maximum rank achievable by \(AB^{}\), and 2) \(^{}A=B^{}B\)._

A more general formulation of the above results, albeit without a bottleneck dimension, was recently shown in (Wang and Jacot, 2023) (c.f. Theorem 3.1) where it was applied in the matrix completion context. We restate and prove it here for completion in the context of self-attention and transformer models.

The Theorem states that there is in fact a one-to-one mapping between the local minima of \(_{L2}(A,B)\) and (the equivalence class of) local minima of \(_{*}(AB^{})\), for a general unregularized loss \(L\).

In particular, if one wishes to optimize \(_{*}\) for some matrix \(W\), potentially under rank constraint, one can reparametrize \(W\) as a product of two matrices \(A,B\) and optimize the differentiable objective \(_{L2}\) on \(A,B\) without introducing bad minima, and obtain rank-regularized solutions. In principle, one can still converge to a bad minimum for a general loss, but this is not due to the reparametrization.

On the other hand, the theorem shows that naively optimizing the \(L2\)-regularized loss with a factorized parametrization will (often inadvertently) result in actually finding solutions that exactly minimize the nuclear-norm regularized loss, introducing unintended low-rank inducing bias to the solution.

Note that however, the two parametrizations may result in different optimization, and thus different solutions, even if the loss landscape shares the same local minima.

### Optimization dynamic in the gradient flow limit

The above result establishes equivalence of the local minima of the two losses. Our next result shows that the two losses will in fact coincide exponentially quickly during training.

**Theorem 3.4**.: _Consider the gradient flow limit over the loss \(_{L2}\). If \(\|A\|,\|B\|\) remain bounded during training, then we have that \(_{L2}(A,B)-_{*}(AB^{})\) converges exponentially to \(0\)._

In order to prove the theorem, we first show that during gradient flow optimization, the condition from Proposition 3.1 becomes true exponentially quickly. This is then followed by a new bound bounding the gap between \(\|AB^{}\|_{*}\) and \((\|A\|^{2}+\|B\|^{2})\) by the norm of \(A^{}A-B^{}B\). For completeness, we also provide a general result bounding the analogous gap for a \(L\)-layer deep linear network.

We provide in the appendix a similar result when considering gradient flow with noise, as well as with momentum and decoupled weight decay. We furthermore provide in appendix B.5 a discussion about the soundness condition.

The above result complements Theorem 3.3 by showing that optimizing \(_{L2}\) will result in co-optimizing \(_{*}\) very quickly during training, long before stationary points are found. The theorem also confirms previous empirical observations (Khodak et al., 2022).

### Case study: 2-layer linear network

To illustrate the low-rank inducing bias of the factorized parametrization coupled with weight decay, we will study in the following the optimization within a 2-layer linear network and characterize the network at equilibrium. Such a network corresponds in fact to a drastically simplified softmax attention layer with \(T=1\). The derivations are similar to those used when studying deep linear networks (Ziyin et al., 2022; Saxe et al., 2013) and the redundant parameterization studied in (Ziyin and Wang, 2023).

Consider the following model

\[f(AB^{}):x AB^{}x\] (10)

where \(B^{}^{d_{2} d_{1}}\), \(A^{d_{3} d_{2}}\). For simplicity of presentation, we assume \(d_{3}=d_{1}=d_{1,3}\), but the result can be easily extended to the general case. Given \(D\) data points \((x_{i},y_{i})_{1 i D}\), in matrix form, the \(L2\)-regularized mean squared error can be expressed as

\[=\|Y-AB^{}X\|^{2}+(\|B\|^{2}+\|A\|^{2})\] (11)

where \(X=(x_{i})_{i}^{d_{1} D}\), \(Y=(y_{i})_{i}^{d_{3} D}\), and \(>0\).

Using full batch gradient flow, the differential equation governing the parameter dynamic becomes

\[^{} =A^{}(_{YX}-AB^{}_{XX})- B^{}\] (12) \[ =(_{YX}-AB^{}_{XX})B- A\] (13)

where \(_{YX}=YX^{}\), \(_{XX}=XX^{}\), and \(\) is a constant controlling the learning rate.

To further simplify the above equations, we follow (Saxe et al., 2013) and assume \(_{XX}=I\), an assumption which holds exactly for whitened input data. Finally, without loss of generality, we perform a change of basis such that \(_{YX}=S\) where \(S\) is the diagonal matrix which diagonal consists of the singular values \((s_{i})_{i[1..d_{1,3}]}\) of \(YX^{}\).

At equilibrium, we thus have the following set of equations

\[ B^{} =A^{}(S-AB^{})\] (14) \[ A =(S-AB^{})B.\] (15)

Denoting by \(a_{i},b_{i}\) the \(i\)-th row of \(A,B\), and assuming the \((s_{i})_{i[1..d_{1,3}]}\) are all non-zero and distinct, we have the following conditions at equilibrium (cf Appendix C)

\[ i[1..d_{1,3}],\;a_{i}=b_{i}\] (16) \[ i,j[1..d_{1,3}]^{2}\;\;i j,\;a_{i}^{ }b_{j}=0.\] (17)

In particular, this implies, for any \(i\), \(\|a_{i}\|^{2}=(s_{i}-\|a_{i}\|^{2})\|a_{i}\|^{2}\). Clearly, if \( s_{i}\), then the equation can only be true if \(a_{i}=0\). If on the other hand \(<s_{i}\), either \(a_{i}=0\) or \(\|a_{i}\|^{2}=s_{i}-\) satisfy the equilibrium condition, with the former being an unstable equilibrium point if the number of hidden units \(d_{2}\) is greater than the number of elements in \(\{i[1..d_{1,3}] s_{i}>\}\).

To highlight the result, let us consider the case where the hidden layer has enough capacity, i.e. \(d_{2} d_{1,3}\). In that case, the result tells us that at a stable equilibrium, \(AB^{}\) will drop all singular values \(s\) that are less than \(\), while keeping those that are larger. In other words, it performs a sort of low rank approximation of the input-output correlation matrix where the rank is controlled by \(\). A related result was already obtained in the analyses of (Saxe et al., 2013) who studied the exact solutions of 11 without the regularization term but introducing a bottleneck in the hidden layer, i.e. \(d_{2}<d_{1,3}\). Remarkably here, regularization achieves a similar effect even in an overcomplete network, where increasing \(\) gradually _prunes_ the hidden neurons to ignore the smallest variations of the data, i.e. reducing \(d_{2}\) adaptively. We confirm these results empirically in Figure 1.

Importantly, this result is only obtained because the regularization is applied to the parametrization involving a matrix multiplication. If \(AB^{}\) were replaced by a single matrix \(W^{d_{1,3} d_{1,3}}\), then the equilibrium condition would be \(W=S\), whose rank remains constant w.r.t. the regularization strength.

### Weight decay with Adam optimizer

While the regularized loss is a convenient setting for studying what happens to the parameters at equilibrium, in the vast majority of practical settings, decoupled weight decay (Loshchilov and Hutter, 2019), simply referred to as weight decay in the following, is used instead optimizing a regularized loss. A popular choice of optimizer for deep neural networks, including those with self-attention layers, is AdamW (Loshchilov and Hutter, 2019), which update the weights by using the Adam optimizer on the non-regularized loss while simultaneously applying weight decay.

While it is non-trivial to analyze the equilibrium points of AdamW in general, we show in Appendix E that under some simplifying assumptions, they coincide with those of a \(L2\)-regularized loss with a different regularization strength.

## 4 Empirical results

The primary objective of our experimental analysis is to empirically validate the theoretical findings in more practical settings. Specifically, we aim to investigate the effect of decoupled weight decay, adaptive optimizers, as well as noisy gradient and lack of exact convergence to stationary points on the theoretical findings.

The second objective is to establish that the theory is relevant in the training of large neural network models. Due to the large computational costs we chose to avoid re-training large scale models but trained small-scale language models as well as a Vision Transformer without changing common hyperparameters. We aim to demonstrate that their typical training is affected by the rank-regularizing effect predicted by our theory. Finally, we investigate pre-trained weights of the relevant foundation models to show that they are consistent with rank-regularizing training.

Figure 2: _Left_: The rank of weight matrix product \(PW_{V}\) of the first layer of a 2-layer Transformer trained on the associative recall task, during training, with AdamW, for various decay strengths. To better account for the effect of weight decay on the attention layers, only the decay strength applied to attention layers is varied, while the strength for all other layers is fixed at \(0.1\). We observe that rank reduction correlates strongly with weight decay strength. _Center_: Norm of the discrepancy between \(P^{}P\) and \(W_{V}W_{V}^{}\), during training. As predicted, the difference seems to converge to \(0\) when \(>0\) towards the end of training. While for AdamW we no longer have the guarantee of an exponential decay, we see that the discrepancy nonetheless vanishes quickly, with a time constant which perfectly correlates with the decay strength. _Right_: The difference of the nuclear norm of \(W_{VP}\) with the Frobenius norm upper bounding it. As the discrepancy between \(P^{}P\) and \(W_{V}W_{V}^{}\) decreases, the difference approaches \(0\), and thus the bound becomes tight. The optimization of \(_{L2}\) thus gradually switches to that of \(_{*}\), explaining the rank regularization. Qualitative findings are identical when studying \(W_{K}^{}W_{Q}\).

To quantitatively measure the rank of matrices in the context of our experiments with attention layers, we use the following definition of _pseudo rank_: Let \(W\) be a weight matrix with singular values \(_{1},_{2},...,_{n}\), ordered such that \(_{1}_{2}..._{n}\). The pseudo rank (referred to simply as rank in the following) of \(W\) is defined as \(\) where \(k\) is the smallest number such that:

\[^{k}_{i}}{_{i=1}^{n}_{i}} 0.95.\]

In simpler terms, it represents the fraction of the largest singular values required to capture at least 95% of the sum of all singular values of the matrix \(W\).

### Associative recall task

In this simple memory task, a model is presented with a sequence of paired tokens \([x_{1},y_{1},,x_{T},y_{T},x_{T+1}]\). Specifically, the task is parameterized by an integer \(N\), representing the number of unique tokens that can be mapped to \(N\) corresponding tokens. The sequence presented to the model therefore consists of \(2N+1\) tokens (with \(T=N\)), and the final token is repeated and appears in the sequence before, i.e. \(x_{T+1}=x_{j}\) for some \(j[0,,T]\). The model is trained to remember the correct association observed in-context and predict \(y_{j}\). This task has been attributed and proposed as a proxy for language modelling (Fu et al., 2023; Poli et al., 2023).

We train a 2-layer self-attention only Transformer with AdamW optimizer on minibatches of size 128, for \(N=20\). To simulate additional noise, we perturb \(5\%\) of the labels with random labelling (Zhang et al., 2021).

Figure 4 shows that even in this setting, the stationary condition of a \(L2\)-regularized loss in Lemma 3.2 is approached, and the gap between the nuclear norm and the Frobenius norm in (5) vanishes, thus confirming that AdamW in fact also optimizes for the nuclear norm. Furthermore, the convergence speed is perfectly correlated with the weight decay strength. The results furthermore show that AdamW leads indeed to a consistent decrease in the rank in both parameter weight products as the decay strength increases. This aligns with the effect of optimizing the nuclear norm of these matrices.

### Language Modelling

In order to validate our theoretical findings in larger scale experiments, we now present results when training standard small scale Transformer models, with 125 million parameters, on the Pile (Gao et al., 2020) - a common language modeling dataset. All design decisions such as the Transformer architecture as well as the optimizer and training schedule are identical to the ones proposed in the GPT-3 paper (Brown et al., 2020), which are now used in various other studies e.g. (Fu et al., 2023; von Oswald et al., 2023). Details can be found in the Appendix G.

First, we confirm again that increasing weight decay with AdamW drastically reduces the rank of \(W_{K}^{T}W_{Q}\) as well as \(PW_{V}\), on average across depth and heads, of the trained models (c.f. Figure 3.

Figure 3: _Left, center left:_ The rank of weight matrix products \(W_{K}^{}W_{Q}\) and \(PW_{V}\) averaged across heads of layer 5 of an autoregressive transformers trained on the Pile (Gao et al., 2020). _Center right, right:_ The rank of weight matrix products \(W_{K}^{}W_{Q}\) and \(PW_{V}\) averaged over all heads and all layers of a Vision Transformer trained following (Irandoust et al., 2022) on the ImageNet dataset (Deng et al., 2009). In both settings, the decay strength applied to attention layers is varied, while keeping the strength for all other layers fixed. In all cases, we observe again that rank reduction correlates strongly with weight decay strength when optimizing with AdamW. The weight decay strength of \(0.1\) commonly used to pretrain some known large foundation models in fact noticeably reduces the rank of the generated matrices compared to when weight decay is turned off.

Appendix Figure 7). Furthermore, we observe that while increasing the weight decay strength of MLP beyond 0.1 is generally beneficial, see Table 1, doing the same for attention matrices starts slightly hurting performance. Results are averaged over 3 seeds. We observe a sweet spot around weight decay strength of 0.1 applied to self-attention weights, indicating that some rank regularization is beneficial for this task. Nevertheless, too much weight decay and therefore rank regularization seems to be detrimental. Finally, applying weight decay to the MLP weights seems to more important with a generally higher effect on performance. We leave a more nuanced investigation of decoupling the weight decay strength of matrices affected by our theory from the rest of the parameters for future research.

### Vision Transformers

Next, we focus on computer vision tasks and train a Vision Transformer on the ImageNet dataset (Deng et al., 2009) for 24 hours, following the exact training protocol of (Irandoust et al., 2022). We follow the previous section and vary the decay strength only in the attention layers, while keeping every other hyperparameter fixed. We observe a similar effect of the decay strength on the ranks of the matrices \(W_{QK},W_{VP}\) (c.f. Fig 3).

### Pretrained foundation models

Finally, we turn to pre-trained foundation models, and provide some evidence that their training is also impacted by the rank-regularizing effect of weight decay. Specifically, following Proposition B.4, it is sufficient to observe that the matrices \(W_{Q}W_{Q}^{}\) resp. \(P^{}P\) are close to \(W_{K}W_{K}^{}\) resp. \(W_{V}W_{V}^{}\). Because the matrices \(W_{Q},W_{K},W_{V},P^{}\) are typically wide rectangular matrices, the off-diagonal elements of \(W_{Q}W_{Q}^{}\), etc, are mostly \(0\). For \(_{L2}\) to approximately correspond to \(_{*}\), it thus suffices that the diagonal elements of \(W_{Q}W_{Q}^{}\) resp. \(P^{}P\) are close to those of \(W_{K}W_{K}^{}\) resp. \(W_{V}W_{V}^{}\).

Figure 4, 6 shows that this is mostly the case, for all layers of the model. For each layer and head, we further find that the gap from (5) is indeed mostly tight, consistent with a rank regularizing training.

    & SA-\(=0.0\) & SA-\(=0.01\) & SA-\(=0.025\) & SA-\(=0.1\) & SA-\(=0.25\) \\  MLP-\(=0.0\) & 12.00\({}^{ 0.03}\) & 12.01 \({}^{ 0.05}\) & 11.98 \({}^{ 0.00}\) & 11.92 \({}^{ 0.02}\) & 12.02 \({}^{ 0.01}\) \\ MLP-\(=0.01\) & 11.94\({}^{ 0.02}\) & 11.95\({}^{ 0.01}\) & 11.94\({}^{ 0.03}\) & 11.89\({}^{ 0.02}\) & 11.97\({}^{ 0.03}\) \\ MLP-\(=0.025\) & 11.89\({}^{ 0.01}\) & 11.90\({}^{ 0.04}\) & 11.90\({}^{ 0.04}\) & 11.80\({}^{ 0.03}\) & 11.92\({}^{ 0.02}\) \\ MLP-\(=0.1\) & 11.72\({}^{ 0.02}\) & 11.71\({}^{ 0.03}\) & 11.68\({}^{ 0.03}\) & 11.67\({}^{ 0.03}\) & 11.70\({}^{ 0.02}\) \\ MLP-\(=0.25\) & 11.63\({}^{ 0.02}\) & 11.65\({}^{ 0.04}\) & 11.62\({}^{ 0.04}\) & 11.52\({}^{ 0.03}\) & 11.58\({}^{ 0.03}\) \\   

Table 1: Test set perplexity of 125 million Transformer models trained on the Pile for 10 billion tokens with AdamW and different weight decays \(\) for the self-attention (SA) and the feed-forward (MLP) weights. \(\) standard error of the mean computed over 5 seeds.

Figure 4: Analyses of attention layers in the pretrained LLAMA 2 model with 7 Billion parameters (Touvron et al., 2023). The leftmost (resp. center left) shows the squared norm of every row of \(W_{Q}\) (resp. \(W_{V}\)), for the first head of each layer, against the norm of the corresponding row of \(W_{K}\) (resp. column of \(P\)). The condition \(W_{K}W_{K}^{}=W_{Q}W_{Q}^{}\) would require these norms to be equal, which in fact is mostly true. While the model has not reached a stationary point, this indicates the optimization has advanced enough for this sufficient condition for \(_{*}\) to be identical to \(_{L2}\) to emerge. In fact, the center right (resp. rightmost) plot show the scatter plot mapping the Frobenius norm against the nuclear norm for all heads across all layers. The two norms almost perfectly coincide.

Discussion

Our results provide further insights into the interplay between \(L2\)-regularization and weight decay regularization and the optimization of models that consist of parameter matrix products. This is of particular interest since attention layers in transformer exhibit this parametrization as key-query, as well as value-projection parameter matrices, are multiplied directly with each other: \(W_{K}^{T}W_{Q}\) and \(PW_{V}\). Our empirical findings strongly support our theoretical predictions about the impact of weight decay on the rank of attention layers and clearly show a rank-regularizing effect even without convergence. We provide evidence that the training of some foundation models such as Llama are in fact in practice affected by the same regularization.

Furthermore, we find that decoupling weight decay in the attention weights and tuning its weight decay strength can improve performance, for example in our language modelling experiments. These findings complement the recent observation that reducing the rank of language model MLP matrices post-training improves their reasoning performance, while doing the same for attention layer matrices mostly hurt it Sharma et al. (2023). In particular, our findings suggest that the conventional practice of applying uniform regularization strategies across all layers may not be optimal for other deep learning architectures as well. This finding opens up new avenues for model- or layer-specific regularization strategies that could significantly enhance the performance of these models.

Our findings once more highlight the complexity of understanding optimization techniques in conjunction with particular neural network models, particularly transformers. For example, the difficulty of understanding the effect when varying regularization strengths on different components of these models underscores the need for a more nuanced theoretical understanding of layer-specific regularization. We are particularly excited about further research that aims to disentangle the role of weight decay in in-weight vs. in-context learning within MLPs and self-attention layers, building on Singh et al. (2023). In conclusion, while our findings mark a step forward in understanding and improving the usage of weight decay when training deep neural networks, in particular transformers, our study shed light on the intricate interplay of neural network regularization and its parametrization.

#### Acknowledgments

Seijin Kobayashi, Yassir Akram and Johannes von Oswald deeply thank Joao Sacramento and Angelika Steger for their support and guidance. The authors also thank Robert Obryk, Moritz Firsching, Luca Versari, Nicolas Zucchet, Alexander Meulemans, Simon Schug, Blaise Aguera y Arcas, Alexander Mordvintsev, Ettore Randazzo and Eyvind Niklasson for fruitful discussions.