# NODE-GAMLSS: Interpretable Uncertainty Modelling via Deep Distributional Regression

Ananyapam De

Institute for Mathematics

TU Clausthal

ananyapam.de@tu-clausthal.de&Anton Thielmann

Institute for Mathematics

TU Clausthal

anton.thielmann@tu-clausthal.de

Benjamin Safken

Institute for Mathematics

TU Clausthal

benjamin.safken@tu-clausthal.de

###### Abstract

We propose NODE-GAMLSS, a framework for scalable uncertainty modelling through deep distributional regression. NODE-GAMLSS is an interpretable attention based deep learning architecture which models the location, scale, and shape (LSS) dependent on the data instead of only the conditional mean enabling us to predict quantiles and interpret the feature effects. We perform a benchmark comparison based on simulated and real datasets with state-of-the-art interpretable distributional regression models, demonstrating the superior quantile estimation, accuracy and interpretability. The code is available at https://github.com/AnFreTh/NodeGAMLSS

## 1 Introduction

Regression analysis traditionally focuses on estimating the conditional mean of a response variable given the explanatory variables. Generalized Additive Models (GAMs) Hastie and Tibshirani are popular interpretable additive mean regression models using smooth functions of covariates. Recently, Agarwal et al. introduced Neural Additive Models (NAMs), which enhance predictive accuracy by utilizing a feedforward neural network for each feature, thereby maintaining interpretability. Chang et al. developed Neural Oblivious Decision Ensembles (NODE-GAM), which adapt the NODE architecture by Popov et al. to a GAM, maintaining predictive accuracy while preserving interpretability.

While these are powerful mean prediction models, they do not capture the full conditional distribution of the response. This is problematic when quantifying uncertainty is crucial, such as with heteroskedasticity or heavy-tailed distributions. The forumlation of Generalized Additive Models for Location, Scale, and Shape (GAMLSS) Rigby and Stasinopoulos (2005) introduced distributional regression which extend GAMs to model other parameters of the response, such as variance or skewness, as functions of the explanatory variables. mBoostLSS (Hofner et al., 2016) offers alternatives with shrinkage and variable selection.

Other methods for distributional modelling include conditional transformation models Hothorn et al. (2013), quantile Koenker (2005), and expectile regression (see Newey and Powell, 1987; Kneib et al., 2023). Recent advancements present deep distributional models such as XGBoostLSS Marz (2019) and LightGBMLSS Marz (2023), which extend boosting algorithms Chen and Guestrin (2016), Keet al. (2017) into a probabilistic framework, though at the cost of interpretability. NAMLSS Thielmann et al. (2024) provides feature level interpretability and leverages flexible scalable feature functions like MLPs and Transformers within the NAM framework. NODE-GAMLSS integrates the NODE-GAM architecture for distributional regression by predicting the conditional distributional parameters. These parameters are then optimized with a loss function, thereby offering an interpretable and scalable framework for capturing the distributional properties of the response variable.

## 2 Methodology

Suppose we are given covariates \(=\{(x_{1i},x_{2i},,x_{j_{1}})\}_{i=1}^{n}\) representing \(J\) input features \(_{1},_{2},,_{J}\) with each \(_{j}^{n}\) and the target \(=\{y_{i}\}_{i=1}^{n}\). We assume \((())\), where \(()=(^{(1)},^{(2)},,^{(K)})\) are \(K\) parameters of the response distribution \(\). We model the dependence of the \(k\) distributional parameters as

\[^{(k)}=^{(k)}(^{(k)}+_{j=1}^{J}z_{j}^{(k)}(x_{j}))\]

where \(^{(k)}\) is the intercept, \(z_{j}\) are feature functions, \(^{(k)}\) are activation functions. The function \(z_{j}\) consists of \(L\) layers, each with \(I\) differentiable Oblivious Decision Trees (ODTs) of depth \(c\) handling both real and vector-valued inputs, enabling scalable training by processing data in batches. All nodes in a tree share the same feature function, and nodes at the same depth use identical input features and thresholds (Figure 1). Each ODT in layer \(l\) of depth \(c\) compares \(c\) selected features of an input \(^{d}\) against thresholds \(b^{c}\), using feature function \(F:^{d}\) where \(d=n+l-1\). The output \(h()\) is the inner product \(\) of a response vector \(R^{2^{c}}\) with the results of these comparisons:

\[h()=R(_{c=1}^{C}[ (F() b^{c})\\ (F()>b^{c})]),\]

Figure 1: An oblivious decision tree (left) with all nodes at the same depth using identical features and thresholds with the input feature being passed at every depth. For each feature, multiple trees form a layer that are stacked together, with the respective input feature passed through every layer (right). The output of an ensemble is the sum of the weighted average of all the trees for a feature, which are then applied the respective activation. The additivity constraint here prevents overfitting.

where \(\) is the entmoid, \(\) is outer product. For tree \(i\) in layer \(l\), the feature function \(F_{li}()\) is:

\[F_{li}()=_{j=1}^{d}x_{j} G_{ij}+^{l-1}_{i= 1}^{l}g_{li}}_{l=1}^{l-1}_{=1}^{l}h_{}()g_{li}a_{li}\]

Here, \(G_{i}=_{}(F_{i}/T)\), \(g_{l}=G_{i}G_{}\), and \(a_{li}\) are attention weights that focus on specific trees. The temperature \(T\) anneals to zero, forcing \(G_{i}\) to become one-hot, making \(g_{l}=1\) only when \(G_{i}=G_{}\) thus acting as an additive model. Outputs from previous layers become inputs to the next. For an input \(\), the layer inputs \(^{l}\) are

\[^{1}=,^{l}=[,h^{1}(^{1}),,h^{(l- 1)}(^{(l-1)})]\]

for \(l>1\). The final prediction is given by

\[z^{(k)}()=_{l=1}^{L}_{i=1}^{I}h^{l}_{i}(^{l})w _{li}.\]

To ensure the constraints of \(\), the model is adapted to a Location, Scale and Shape (LSS) framework, where the output is passed through the activation \(^{(k)}\), which transforms the parameters. We minimize the sum of the negative log likelihood of the predicted distribution with respect to the observed values: \(l()=_{i=1}^{n}-((|y_{i}))\). Since \(h\) is differentiable, the model is trained end-to-end using backpropagation and gradient descent Kingma and Ba (2017).

## 3 Experiments

**Feature interpretability and interactions**: NODE-GAMLSS offers feature-level interpretability enabling visual analysis illustrated for the California housing dataset in Figure 2.

**Real world data**: We compare our model against state-of-the-art interpretable distributional models--NAMLSS, GAMBoostLSS, and GAMLSS. NODE-GAMLSS outperforms benchmarks (Pace and Barry (1997); Nash et al. (1995); Lantz (2013)) achieving the lowest NLL, MSE and MAE (in Table 1) indicating better model fit and predictive accuracy.

Figure 2: The rows display raw mean and variance predictions respectively. The left plots illustrate single feature effects for longitude, latitude and median age, with pink bars indicating normalized data density. The right plots highlight the interaction effects of longitude and latitude. The model excels in capturing jagged functions, evident in the sharp price jumps around San Francisco and Los Angeles. The second row shows decreasing variance in areas further from large cities.

**Quantile estimation**: Our model captures the full conditional distribution, enabling quantile estimation illustrated for the California housing dataset as shown in Table 3 achieving the lowest quantile score and CRPS, and the highest coverage. Figure 3 compares performance across quantiles.

**Feature learning**: We simulate \(x_{1},x_{2}(-5,5)\) and define \(f_{1}(x)=(x)\), \(f_{2}(x)=2x\), \(f_{3}(x)=x^{2}\), \(f_{4}(x)=e^{x}\) such that \(y(f_{1}(x_{1})+f_{2}(x_{2}),f_{3}(x_{1})+f_{4}(x_{2}))\). The effects learnt by NODE-GAMLSS shown in Figure 4 clearly capture the shapes of the original functions.

**Distribution modelling**: We synthetically generate \(n=3000\) observations with \(J=5\) features from Normal, Poisson, Lognormal, and Gamma distributions (see Appendix A.5). NODE-GAMLSS achieves state-of-the-art performance, matching NAMLSS on both count and continuous data.

## 4 Limitations

A crucial aspect in applying our proposed method, as well as other distributional methods, is selecting the appropriate distributional assumptions. Hence this approach necessitates some understanding and domain knowledge of the data distribution. Also rather than minimizing an error measure, our method primarily uses the negative log likelihood, a strictly proper scoring rule Lakshminarayanan

  
**Dataset** & **Model** & **NLL** & **MSE** & **MAE** \\   & NODE-GAMLSS & \(\) & \(\) & \(\) \\  & NAMLSS & \(1.078 0.079\) & \(1.076 0.098\) & \(0.787 0.036\) \\  & GAMLSS & \(1.056 0.034\) & \(0.483 0.032\) & \(0.501 0.017\) \\  & GAMBoostLSS & \(0.998 0.031\) & \(0.536 0.046\) & \(0.732 0.032\) \\   & NODE-GAMLSS & \(\) & \(\) & \(\) \\  & NAMLSS & \(0.785 0.047\) & \(0.758 0.051\) & \(0.654 0.026\) \\  & GAMLSS & \(0.917 0.020\) & \(0.366 0.015\) & \(0.442 0.004\) \\  & GAMBoostLSS & \(1.025 0.182\) & \(0.420 0.011\) & \(0.648 0.009\) \\   & NODE-GAMLSS & \(\) & \(\) & \(\) \\  & NAMLSS & \(0.653 0.057\) & \(0.655 0.069\) & \(0.568 0.040\) \\   & GAMLSS & \(0.732 0.048\) & \(0.253 0.024\) & \(0.503 0.024\) \\   & GAMBoostLSS & \(0.644 0.068\) & \(0.269 0.028\) & \(0.518 0.028\) \\   

Table 1: Comparison on real datasets using a normal response

   Model & CRPS & Quantile Score & Coverage Probability \\  NODE-GAMLSS & \(\) & \(\) & \(\) \\ NAMLSS & \(0.3224 0.0202\) & \(0.1654 0.0028\) & \(0.932 0.012\) \\ GAMLLSS & \(0.2795 0.0132\) & \(0.1433 0.0032\) & \(0.914 0.025\) \\ GAMBoostLSS & \(0.3469 0.0154\) & \(0.1780 0.0082\) & \(0.895 0.053\) \\   

Table 2: Comparison of quantile estimation metricset al. (2017), minimized in expectation if and only if the conditional density matches the underlying data distribution Hastie et al. (2001).

## 5 Conclusion and Future Work

NODE-GAMLSS presents a significant advancement in uncertainty modelling via distributional regression, providing interpretability of covariate effects and exceptional predictive accuracy. These qualities make NODE-GAMLSS highly suitable for applications in high-risk domains, where understanding and mitigating uncertainty is crucial. Using other types of flexible distributions like mixture density networks Seifert et al. (2022) or normalizing flows Papamakarios et al. (2021) are apparent extensions. Multivariate responses conditionally dependent on covariates can be modeled using a copula-based approach for NODE-GAMLSS that would significantly improve the general applicability. While initially designed for tabular data, NODE-GAMLSS can be naturally extended to handle multimodal data by integrating components such as a CNN or Transformers as feature networks for image and text input respectively.