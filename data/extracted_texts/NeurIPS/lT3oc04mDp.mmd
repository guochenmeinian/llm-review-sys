# Kangaroo: Lossless Self-Speculative Decoding for Accelerating LLMs via Double Early Exiting

Fangcheng Liu\({}^{}\) Yehui Tang\({}^{}\) Zhenhua Liu\({}^{}\) Yunsheng Ni\({}^{}\) Duyu Tang\({}^{}\)

Kai Han\({}^{,}\) Yunhe Wang\({}^{,}\)

\({}^{}\)Huawei Noah's Ark Lab

Consumer Business Group, Huawei

{liufangcheng3,yehui.tang,kai.han,yunhe.wang}@huawei.com

Corresponding Author.

###### Abstract

Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models (LLMs) while maintaining an identical sampling distribution. However, the conventional approach of training separate draft model to achieve a satisfactory token acceptance rate can be costly and impractical. In this paper, we propose a novel self-speculative decoding framework _Kangaroo_ with _double_ early exiting strategy, which leverages the shallow sub-network and the LM Head of the well-trained target LLM to construct a self-drafting model. Then, the self-verification stage only requires computing the remaining layers over the _early-exited_ hidden states in parallel. To bridge the representation gap between the sub-network and the full model, we train a lightweight and efficient adapter module on top of the sub-network. One significant challenge that comes with the proposed method is that the inference latency of the self-draft model may no longer be negligible compared to the big model. To boost the token acceptance rate while minimizing the latency of the self-drafting model, we introduce an additional _early exiting_ mechanism for both single-sequence and the tree decoding scenarios. Specifically, we dynamically halt the small model's subsequent prediction during the drafting phase once the confidence level for the current step falls below a certain threshold. This approach reduces unnecessary computations and improves overall efficiency. Extensive experiments on multiple benchmarks demonstrate our effectiveness, where Kangaroo achieves walltime speedups up to 2.04\(\), outperforming Medusa-1 with 88.7% fewer additional parameters. The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo.

## 1 Introduction

Large Language Models (LLMs) [1; 2; 3; 4; 5; 6] have demonstrated remarkable performance across various natural language processing tasks, such as chain-of-thought reasoning  and agents . Beyond model performance, recent research has also focused on inference efficiency [9; 10], particularly in scenarios involving the deployment of LLMs for service applications. However, constrained by the bottleneck of memory bandwidth , the primary latency for autoregressive decoding of LLMs arises mainly from memory read/write operations rather than arithmetic computations, leading to inadequate parallelism. For instance, decoding Vicuna-33B  on four NVIDIA V100 GPUs yields a throughput of only seven new tokens per second.

To address this challenge, Speculative Decoding (SD) techniques [13; 14] have been developed, aiming to accelerate autoregressive decoding by verifying multiple tokens generated by a draft modelin parallel. Given \(\) draft tokens, SD can generate 1 to \(+1\) new tokens within each forward pass of the big LLM. Most of existing SD methods typically train a _tiny_ draft model from scratch on a large corpus to accelerate LLMs from the same series, _e.g._, LLaMA-68M  for LLaMA-7B . Distillspec  uses knowledge distillation to better align the draft model with the target model. However, the training of such task-specific models can be costly , limiting its application in real-world scenarios.

To mitigate these costs, several studies have proposed _self-drafting_ methods that do not rely on external drafter models. LLMA  and REST  generate draft tokens by selecting text spans from references or retrieving relevant tokens from a database. Notably, Medusa  trains multiple time-independent Feed-Forward neural Network (FFN) heads on top of the last decoder layer. Although Medusa and REST could generate draft tokens efficiently, however, the token acceptance rate is not always satisfactory (see Figure 1(a)). On the other hand, focusing exclusively on the token acceptance rate without considering the latency of generating draft tokens can lead to suboptimal speedup ratio. For instance, as shown in Figure 1, Lookahead  achieves a significantly higher token acceptance rate than Medusa in the mathematical reasoning subtask. However, due to its lower efficiency in the drafting phase when compared to Medusa, its end-to-end speedup ratio is slightly lower than that of Medusa (see Figure 1(b)).

In this paper, we propose a novel self-speculative decoding framework based on a _double_ early-exiting mechanism. Rather than training the draft model from scratch, we enhance efficiency by inheriting part of the target model's knowledge through parameter sharing. Specifically, Kangaroo utilizes both the shallow sub-network and the LM Head of the target LLM to construct a self-drafting model. However, there is a natural discrepancy in the representation capabilities between the shallow network and the final layer of the model. To bridge this gap, we incorporate a lightweight and efficient adapter module trained on top of the sub-network. Kangaroo's adapter network is streamlined yet effective, comprising only one multi-head attention layer  and two normalization layers , utilizing just 11.3% of the parameters used by Medusa's heads. This _layer-level_ early-exiting strategy significantly reduces both the training and deployment costs typically associated with traditional self-speculative decoding methods that necessitate separate draft model.

Moreover, to achieve an optimal balance between token acceptance rate and drafting efficiency, Kangaroo introduces an additional _token-level_ dynamic drafting strategy via early exiting. Unlike existing methods that rely on a fixed drafting step and a predefined static token tree, our approach dynamically adjusts the depth and width of the token tree based on the conditional probability distribution of the self-drafting model. In the case of single-sequence decoding, which is a specific scenario within the token tree, the drafting phase is immediately halted if the top-1 probability of the current sample token (in the self-drafting model) falls below a predefined threshold. Extensive experiments conducted on the Spec-Bench demonstrate the effectiveness of our approach. Kangaroo

Figure 1: Comparison of various self-drafting speculative decoding methods _without tree mask_ on Spec-Bench  for Vicuna-7B . Kangaroo outperforms all other methods _w.r.t._ end-to-end speedup ratio across all the four subtasks. Specifically, Kangaroo (without tree) achieves speedups of 1.68\(\) on MT-bench , outperforming Medusa with 88.7% fewer additional parameters.

achieves speedups of up to 2.04\(\), significantly outperforming Medusa-1 while using 88.7% fewer additional parameters (67M compared to 591M.). These results highlight the efficiency and potential of Kangaroo in improving decoding processes without compromising performance.

## 2 Related Work

Inference Acceleration of Large Language ModelsWith the rapid development of large language models, significant research effort has been dedicated to accelerating their inference speed . Techniques such as knowledge distillation , model compression , and quantization  have also been widely applied in this area. However, these approaches often require additional training of the backbone or substantial modifications to the model architecture. FlashAttention  and vLLM  frameworks accelerate the inference speed of large language models by optimizing memory management. Recent efforts have explored early exiting on models like the T5 series [30; 31; 32] and decoder-only architectures . However, since early exiting accelerates inference by saving subsequent computations, it inevitably incurs the issue of performance degradation .

Speculative DecodingSpeculative decoding has gained significant attention due to its ability to accelerate the inference of LLMs while maintaining the same sampling distribution. Generally, speculative decoding [13; 14] involves finding or training [16; 34] a small draft model closely aligned with the target LLM. Consequently, recent research has focused on more convenient self-drafting methods. For instance, approaches like blockwise parallel decoding  and Medusa  expedite the generation of draft tokens by training multiple time-independent Feedforward Neural Networks at the second top layer. Several self-drafting acceleration techniques are inspired by early exiting. Draft & Verify , for example, generates draft tokens by skipping intermediate redundant layers of the target LLM. While this approach could achieve a high token acceptance rate, the inference latency of the "small model" is exceptionally high, which can hinder end-to-end acceleration efficiency. SPEED  adapts early exiting to pipelined speculative execution for transformer decoders that employ parameter sharing. There are also several works [38; 39; 40] that make improvement on Medusa by introducing time dependency among the draft tokens. Unlike our approach, which utilizes early exiting from shallow layers, these methods typically extrapolate directly from the features of the penultimate layer. For more related works on speculative decoding, we refer readers to a recent survey  for a detailed summarization.

## 3 Preliminaries

In this section, we introduce background and the formulations of standard speculative decoding under greedy decoding. We use \(x^{t}\) to denote the discrete token sequence \((x_{1},,x_{t})\) and \(x^{:ij}\) to represent sequence \((x_{i},,x_{j})\). Let \(\) be a discrete space over all possible tokens \(x\) in the LLM's vocabulary, we model the autoregressive process of a language model \(\) by the conditional distributions \(( x^{t})^{||}\) where \(||\) is the vocabulary size. We denote the big target language model and the speculative small model as \(_{b}\) and \(_{s}\), respectively.

Single-Sequence DecodingDue to the memory bandwidth  limitations of autoregressive decoding in large language models, the latency to generate one new token is approximately the same as the time required to parallelly infer \(\) tokens. Based on this characteristic, speculative decoding [13; 14] leverages a low-cost small model to quickly generate \(\) candidates

\[_{t+i}=*{arg\,max}_{x}\ _{s}(x ^{t+i-1}), i=1,2,,,\] (1)

where \(^{t}=x^{t}\). Subsequently, the big model \(_{b}\) only needs one forward pass over \((_{t},_{t+1},,_{t+})\) to generate \(+1\) tokens \((x_{t+1},,x_{t++1})\). The accept length over this single-sequence verification is

\[_{i=0,,}\ \{i_{t+i}=x_{t+i}\}+1,\] (2)

ranging from \(1\) to \(+1\).

## 4 Kangaroo

In this section, we present Kangaroo, a novel self-speculative decoding framework based on a _double_ early-exiting mechanism. We leverage the shallow sub-network and the LM Head of the target LLM to construct a self-drafting model. Subsequently, the self-verification stage only requires computing the remaining layers over the _early-exited_ hidden states in parallel.

### Early Exiting as Self-Drafting Model

Training an additional small model from scratch is often costly and impractical, thus it is worth considering sharing a portion of the parameters from the target LLM. Drawing inspiration from early exiting, we directly extract hidden states from a fixed shallow sub-network of the target LLM

\[f_{t}=_{b}[:l](x_{t}), l\{1,2,,L\},\]

where \(_{b}[:l]\) denotes the first \(l\) layers from the target model \(_{b}\). Note that there is a natural representation gap between the shallow sub-network and the full model. Therefore, we train a lightweight and efficient adapter \(\) to bridge this gap. As shown in Figure 2, the architecture of the adapter \(\) consists of only one multi-head attention and two normalization layers. Given an exited hidden states \(f_{t}\), the forward pass of the adapter model \(\) can be represented as

\[f_{t}^{}=f_{t}+((f_{t}) ),\]

where we keep the residual connection and the multi-head attention module but remove the FFN in a standard transformer block. Besides the shallow sub-network, Kangaroo also reuses the LM Head of the target model to get the final conditional distribution, _i.e._,

\[_{s}(x x^{t})=(^{} {LayerNorm}(f_{t}^{})),\]

Figure 2: The framework of Kangaroo under single-sequence verification. The adapter network \(\) consists of only one multi-head attention and two normalization layers. The self-drafting model \(_{s}=_{b}[:l]\) will reuse the LM Head of the target LLM \(_{b}\) for better alignment, where \(l\) denotes the early exit layer. To avoid unnecessary costs on more difficult tokens, \(_{s}\) stops drafting once the top-1 probability of the current sampled token falls below a certain threshold, _e.g._, \(_{s}(_{3}^{0:2})\). Note that we will concatenate the stopped token’s _next early feature_\(f_{3}\) with all previous exited features into a parallel compute unit \([f_{0},f_{1},,f_{3}]\), which will be verified by the remaining layers \(_{b}[l:]\) in parallel. Once all drafted tokens are accepted (\(_{i}=x_{i}\) for \(i=1,2,3\)), we could start the next round with \(x_{4}\) rather than \(x_{3}\) if we have not calculated \(f_{3}\) in advance. The decoding on parallel unit \([f_{3},f_{4}]\) could save the latency for a single forward pass of \(\).

where \(^{N||}\) is the frozen LM Head and \(N\) denotes the dimension of hidden states. The total parameters of \(\) comes from the four projection matrix in the MultiHead layer and two vectors in the LayerNorm layers, _i.e._, \(4N^{2}+2N=67.1\)M when \(N=4096\).

Single-Sequence Self-Speculative DecodingIn the drafting phase, the self-drafting model \(_{s}\) generates \(\) new tokens autoregressively. Similar to the standard speculative decoding in Equation (1), we have \(_{t+i}=_{x}~{}_{s}(x^{t+i- 1})\) and the exited hidden states

\[f_{t+i}=_{b}[:l](_{t+i}), i=1,2,,,\]

Subsequently, the remaining layers \(_{b}[l:]\) will in charge of verifying the concatenated early-exited hidden features in parallel, _i.e._,

\[f^{t:t+} }}{{=}}([ f_{t},f_{t+1},,f_{t+}])^{N(+1)},\] \[_{b}(x^{t+1:t++1} x^{t}) =(^{} (_{b}[l:](f^{t:t+}))),\]

where \(\) is the frozen LM Head. In the verification procedure, we keep the same as the conventional speculative decoding by comparing the top-1 candidates of \(_{b}(x^{t+1:t++1} x^{t})\) and \(_{t+i}\) for \(i=1,2,,\), following Equation (2).

Training of the Adapter \(\)To bridge the representation gap between the self-drafting model \(_{s}\) and the full model \(_{b}\), we train the adapter \(\) with the conventional cross-entropy loss, _i.e._,

\[^{*}=*{arg\,min}_{}~{}_{t}_{x }-_{b}(x x^{t})_{s}(x x^{t}),\]

where the token positions \(t\) is averaged over the whole training set and the condition probability distribution of the target LLM \(_{b}\) serves as a distillation for the self-drafting model \(_{s}\)'s.

### Dynamic Drafting Steps via Token-Level Early-Exiting

Speculative decoding typically employs a fixed drafting step \(\) during the drafting phase, but this often leads to local optima. Fixed-step drafting can result in unnecessary time spent on more challenging samples or missed opportunities to speculate on simpler tokens. As shown in Figure 4, the difficulty of predicting the next token varies across different contextual scenarios, necessitating the design of a token-wise dynamic drafting strategy.

Single-Sequence DecodingDuring drafting phase, we do not have access to the full target model \(_{b}\) and must rely solely on the information from the self-drafting model \(_{s}\), to determine whether to continue or terminate the drafting process. Fortunately, we observe a strong correlation between the small model's confidence level for the current sampled token and the likelihood that the token will be accepted by the big target model. In Figure 4, all conditional probability values of the tokens are divided into two groups. The peak values for tokens accepted by the large model skew to the right, while the conditional probability values for tokens rejected by the large model cluster at lower confidence levels. This indicates that the top-1 probability on the Kangaroo's small model is a reliable metric for determining when to stop drafting. Therefore, we stop drafting once the top-1 probability on the self-drafting model falls below a predefined threshold \(\), _i.e._,

\[_{x}\;_{s}(x x^{t}).\] (3)

Extension to Tree DecodingBased on the single-sequence verification procedure, the big model \(_{b}\) can only verify one feasible path, composed of the top-1 candidates generated by the small model \(_{s}\). Tree verification technology  enhances parallelism and improves GPU utilization by designing a sparse token tree to simultaneously verify multiple feasible paths. Most current works that adopt token tree verification rely on handcrafted or pre-searched  static tree. We generalize the early stopping mechanism of Kangaroo in single-sequence decoding to tree decoding, where both the tree depth and node selection at each level of the tree are token-wise dynamic.

We define the first draft token from \(_{s}\) as the \(\)2 node. To increase the token acceptance rate, we feed the Top-K3 most probable tokens into the draft model \(_{s}\) in parallel at each drafting step. The confidence score of a child node is computed as the product of its conditional probability in \(_{s}\) and its parent's conditional probability, Formally,

\[c(x_{i})=_{x_{j}(,x_{i})}c(x_{j})\;,\]

where \((,x_{i})\) denotes the path from \(\) to node \(x_{i}\), \(c(x_{i})\) is the probability confidence on \(_{s}\) and \(c()=1\). At the first level, we begin with Top-K\({}_{x}_{s}(x)\). For level \( 2\), we consider the Top-K child nodes for each token in the previous level's Top-K tokens. In this way, the tokens with the Top-K largest confidence score will be selected from Top-K \(\) Top-K candidates. For the selected Top-K tokens, their parent nodes will be added into the token tree \(\) directly. For tokens in the previous level's Top-K set without any child node selected, half of those with the lowest confidence scores are pruned from \(\). This process continues until the tree size \(||\) surpasses a predefined maximum or the highest confidence score at the current level falls below a threshold \(\).

Figure 5: Comparison of various speculative decoding methods on Spec-Bench  for Vicuna-7B, where Kangaroo outperforms other approaches in most subtasks, especially in mathematical reasoning and retrieval-augmented generation.

## 5 Experiments

Models and BenchmarksWe conduct experiments on Vicuna  models with size of 7B and 13B. We select several speculative decoding approaches for comparison including Lookahead , Medusa , REST  and SpS  using Vicuna-68M  as the draft model. For fair and comprehensive comparison, we evaluate the acceleration performance with the recently proposed Spec-Bench , which consists of six subtasks including Multi-turn Conversation, Translation, Summarization, Question Answering, Mathematical Reasoning and Retrieval-augmented Generation.

Evaluation MetricsSpeculative decoding is often evaluated using two primary metrics: walltime speedup ratio and compression rate. Given a speculative decoding algorithm, we execute it to generate \(T\) new tokens and record the accepted tokens per forward pass of the big model as a list \(=[s_{1},s_{2},,s_{|S|}]\) where \(_{k}s_{k}=T\). The Compression Rate (CR) is defined as

\[=|}_{k}s_{k},\] (4)

which does not accurately reflect the acceptance levels of the drafting algorithm for tokens at varying distances. Thus, we propose a new metric consistent token acceptance rate \((w)\), given a prefix and a following window with size \(w\), is the probability that the \(w\) guessed tokens from the draft model are _all_ accepted by the target model:

\[(w)=_{k}(s_{k}-w>0),\] (5)

which is a decreasing function _w.r.t._ the window size \(w\). We plot the empirical CTARs of several self-drafting speculative decoding algorithms on the mathematical reasoning subtask of Spec-Bench in Figure 1, where we observe a trade-off between the token acceptance rate and drafting efficiency.

Training and InferenceAs shown in Figure 2, the big target model is frozen and the trainable parameters are the multi-head attention and two normalization layers. For Kangaroo, we train the adapter \(\) for 10 epochs with the AdamW  optimizer on the ShareGPT dataset following Medusa . The training of the adapter \(\) for Vicuna-7B takes around 24 hours on 8 NVIDIA V100 GPUs. During the inference stage, we set \(=2\) for Vicuna-7B and \(=3\) for Vicuna-13B. For the single-sequence decoding in Kangaroo, we set \(=6\) and \(=0.6\). For the dynamic tree decoding scenario, we set Top-K as 10, and \(=0.4\).

### Effectiveness: Comparison with other Speculative Decoding Methods

The Spec-Bench results for Vicuna families4 are available at Figure 5 and Table 1, where we can conclude that:

    &  &  &  &  &  &  &  \\    & CR & Speedup & CR & Speedup & CR & Speedup & CR & Speedup & CR & Speedup & CR & Speedup \\   & Lookahead  & 1.24 & 1.15\(\) & 1.56 & 1.21\(\) & 1.53 & 1.27\(\) & 1.96 & 1.51\(\) & 1.49 & 1.19\(\) & 1.70 & 1.40\(\) & 1.29\(\) \\  & Medusa _w/o_ _Tree_ & 1.58 & 1.41\(\) & 1.50 & 1.34\(\) & 1.49 & 1.32\(\) & 1.73 & 1.54\(\) & 1.51 & 1.29\(\) & 1.76 & 1.55\(\) & 1.41\(\) \\  & REST  & 1.54 & 1.26\(\) & 1.91 & 1.63\(\) & 1.64 & 1.33\(\) & 1.53 & 1.23\(\) & 1.92 & 1.46\(\) & 2.00 & 1.58\(\) & 1.43\(\) \\  & Kangaroo _w/o_ _Tree_ & 1.41 & 1.24\(\) & 1.87 & 1.43\(\) & 1.87 & 1.50\(\) & 2.14 & 1.61\(\) & 2.05 & 1.52\(\) & 2.22 & 1.68\(\) & 1.50\(\) \\  & SpS  & 1.45 & 1.17\(\) & 2.16 & 1.55\(\) & 2.43 & **1.76\(\)** & 2.06 & 1.51\(\) & 2.31 & 1.67\(\) & 2.33 & 1.69\(\) & 1.56\(\) \\  & Medusa  & 2.12 & **1.60\(\)** & 2.08 & 1.64\(\) & 2.01 & 1.41\(\) & 2.48 & 1.87\(\) & 2.09 & 1.37\(\) & 2.51 & 1.84\(\) & 1.63\(\) \\  & Kangaroo & 1.76 & 1.43\(\) & 2.32 & **1.71\(\)** & 2.31 & 1.68\(\) & 2.76 & **2.04\(\)** & 2.37 & **1.75\(\)** & 2.67 & **1.93\(\)** & **1.72\(\)** \\   & Lookahead  & 1.25 & 1.02\(\) & 1.39 & 0.99\(\) & 1.50 & 0.98\(\) & 1.94 & 1.24\(\) & 1.52 & 0.94\(\) & 1.68 & 1.08\(\) & 1.04\(\) \\  & REST  & 1.53 & 1.07\(\) & 1.92 & 1.41\(\) & 1.66 & 1.14\(\) & 1.55 & 1.06\(\) & 1.87 & 1.34\(\) & 1.98 & 1.36\(\) & 1.23\(\) \\  & Medusa  & 2.19 & 1.21\(\) & 2.11 & 1.17\(\) & 2.08 & 1.21\(\) & 2.59 & 1.41\(\) & 2.12 & 1.12\(\) & 2.58 & 1.38\(\) & 1.24\(\) \\  & Medusa _w/o_ _Tree_ & 1.61 & 1.33\(\) & 1.49 & 1.25\(\) & 1.53 & 1.25\(\) & 1.80 & 1.48\(\) & 1.53 & 1.23\(\) & 1.82 & 1.48\(\) & 1.34\(\) \\  & Kangaroo _w/o_ _Tree_ & 1.45 & 1.18\(\) & 1.79 & 1.34\(\) & 2.00 & 1.41\(\) & 2.42 & 1.63\(\) & 2.16 & 1.40\(\) & 2.44 & 1.66\(\) & 1.44\(\) \\  & SpS  & 1.44 & 1.21\(\) & 1.83 & 1.49\(\) & 2.32 & **1.62\(\)** &* For both 7B and 13B model sizes, Kangaroo achieves the highest average speedup across all tasks, with 1.72\(\) and 1.65\(\) respectively. Furthermore, at both model sizes, Kangaroo with dynamic tree verification demonstrates approximately a 12% improvement in speedup compared to its single-sequence decoding version, indicating its effectiveness.
* In both single-sequence and tree verification scenarios, Kangaroo outperforms Medusa across most datasets, despite Kangaroo's additional parameters comprising only 11.3% of Medusa's heads. Kangaroo shows consistent high performance across different tasks such as QA, Math, and MT Bench, indicating its robustness and versatility.
* Under single-sequence decoding, Kangaroo (67M) achieves comparable results to the SpS method using Vicuna-68M, which is specifically pre-trained from scratch. Furthermore, with tree verification, Kangaroo significantly outperforms SpS across most datasets.

### Ablation Studies

To fully explore the sensitivity of different hyperparameters in the Kangaroo framework, we conducted comprehensive ablation experiments focusing on aspects such as the depth of shallow sub-Network \(\), the early stopping threshold \(\), and the architectural choices of the adapter network \(\). In the settings of the ablation experiments described below, we focus on single-sequence decoding scenario.

The Depth of Shallow Sub-Network.The capacity of the self-drafting model \(_{s}\) highly depends on the depth of the shared shallow sub-network. However, selecting deeper early exiting layers, such as half layers of \(_{b}\), would result in excessively high inference latency. As shown in Figure 6(a), the choice of the optimal early exitlayer \(l\) significantly influences the trade-off between token acceptance rate and drafting efficiency. For Kangaroo, we set \(l=2\) for Vicuna-7B and \(l=3\) for Vicuna-13B. In general cases, the early exit layer \(\) can be set based on an empirical ratio derived from the target model's depth \(N\). To determine the optimal depth for shallow sub-networks relative to the full model depth, we conducted multiple comparative experiments across models with different architectures and sizes. Figure 6 records the average speedup achieved on Spec-Bench with early exits at various depths. We recommend setting \(\) between \(\) and \(\) in practical applications.

The Architecture of the Adapter Module.In a transformer block, the Feed-Forward Network (FFN) component counts for 67% of the whole parameters. As shown in Table 2, removing the FFN component and sharing the LM Head from the target Large Language Model (LLM) has been proven to be highly effective and efficient. Specifically, Kangaroo's adapter module \(\) for Vicuna-7B achieves a higher "Speedup" of 1.50 \(\), compared to 1.41 \(\) for Medusa, while using only about one-tenth of the additional parameters as Medusa.

Dynamic Exiting _v.s._ Fixed Step Drafting.Kangaroo uses a static threshold to determine the timing of the second early stopping, based on the observation that the confidence of draft tokens in the small model is strongly correlated with their acceptance by the large model. To validate the effectiveness of dynamic drafting steps with fixed threshold, we plot the comparison for various \(\)

   Architecture & Input LN & Attention & Post LN & FFN & _Linear_ & Last LN & Head & \# Parameters & Speedup \\  Medusa & ✗ & ✗ & ✗ & \(\) 4 & ✗ & \(\) 4 & 591M & 1.41 \(\) \\ Kangaroo & ✓ & ✓ & ✗ & ✗ & ✗ & ✓ & ✗ & **67M** & **1.50 \(\)** \\ Kangaroo + Head & ✓ & ✓ & ✗ & ✗ & ✗ & ✓ & ✓ & 198M & 1.44 \(\) \\
1-Layer Transformer & ✓ & ✓ & ✓ & ✗ & ✓ & ✗ & 202M & 1.37 \(\) \\ MLP Only & ✓ & ✗ & ✗ & ✗ & \(\) 2 & ✓ & ✓ & 165M & 1.22 \(\) \\   

Table 2: Ablation studies on the architecture of the adapter module \(\) for Vicuna-7B. “Speedup” denotes the average speedup ratio on Spec-Bench .

in Figure 7(b). The fixed step strategy (\(=0\)) achieves the maximum compression rate, however, leading to sub-optimal end-to-end walltime speedup. Overall, the optimal threshold \(\) is consistent across different maximum steps. For Kangaroo, we set \(=6\) and \(=0.6\). Besides, the static threshold is also robust across different subtasks and architectures. We visualized the conditional distributions of the small model's Top-1 confidence across different model architectures and subtasks in Figures 8 and 9 in the appendix, where the therehold is stable, ranging from 0.6 to 0.8.

Temperature SamplingIt's well known that speculative sampling suffers from decreased speedup ratio when increasing the sampling temperature . We consider two sets of comparative experiments ion Kangaroo for the temperature sampling case. One strategy uses the original top-1 confidence as a stopping criterion, while the other uses an adjusted top-1 confidence which is computed by applying softmax to adjusted logits with a temperature scaling factor. As shown in Table 3, it can be seen that Kangaroo still achieves a speedup effect very similar to that of greedy decoding when \(T=0.2\). Moreover, the top-1 confidence used for the second early stopping mechanism should remain unadjusted because when \(T<1\), the adjusted top-1 confidence tends to be overestimated, making it more difficult to trigger the early stopping mechanism.

## 6 Conclusion

In conclusion, Kangaroo presents a novel self-speculative decoding framework that significantly enhances the efficiency of autoregressive decoding for large language models. By leveraging a double early-exit mechanism, Kangaroo achieves remarkable speedups compared to existing methods, even with significantly fewer additional parameters. Experimental results demonstrate the effectiveness of Kangaroo in balancing the token acceptance rate and drafting efficiency, making it a promising approach for accelerating inference of LLMs in real-world scenarios.

LimitationsAlthough we introduce a lightweight adapter module to bridge the gap between the shallow network and the final layer of the model, the effectiveness of Kangaroo relies on the target model and the specific task. In certain complex tasks, this representation gap may still persist.

    &  &  &  &  &  &  &  &  \\    & & CR & Speedup & CR & Speedup & CR & Speedup & CR & Speedup & CR & Speedup &  & Speedup \\ 
0.0 & Original & 1.41 & 1.24\(\) & 1.87 & 1.43\(\) & 1.87 & 1.50\(\) & 2.14 & 1.61\(\) & 2.05 & 1.52\(\) & 2.22 & 1.68\(\) & 1.50\(\) \\
0.2 & Original & 1.41 & 1.23\(\) & 1.88 & 1.41\(\) & 1.88 & 1.48\(\) & 2.19 & 1.58\(\) & 2.01 & 1.50\(\) & 2.21 & 1.67\(\) & 1.48\(\) \\
0.2 & Adjusted & 1.45 & 1.04\(\) & 1.90 & 1.25\(\) & 2.00 & 1.31\(\) & 2.32 & 1.48\(\) & 2.18 & 1.40\(\) & 2.41 & 1.53\(\) & 1.34\(\) \\
0.5 & Original & 1.41 & 1.18\(\) & 1.86 & 1.38\(\) & 1.88 & 1.43\(\) & 2.21 & 1.55\(\) & 2.01 & 1.46\(\) & 2.23 & 1.62\(\) & 1.43\(\) \\   

Table 3: Speedup comparison of various temperature \(T\) on Spec-Bench  for Vicuna . Speedup is the walltime speedup ratio and CR denotes the compression rate.

Figure 7: Ablation studies on hyper-parameters. The compression rate and walltime speedup is averaged across all sub-benchmarks in Spec-Bench . It can be seen that there is a trade-off between token acceptance rate and drafting efficiency. While deeper early-exit layer \(\) can enhance the expressive power of the equivalent draft model in Kangaroo, the increased inference latency can actually hinder the overall acceleration performance of the system.