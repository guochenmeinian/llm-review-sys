# QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space Model

Fei Xie\({}^{1}\)  Weijia Zhang\({}^{1}\)  Zhongdao Wang\({}^{2}\)  Chao Ma\({}^{1}\)

\({}^{1}\) MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University

\({}^{2}\) Huawei Noah's Ark Lab

{jaffe031, weijia.zhang, chaoma}@sjtu.edu.cn

wangzhongdao@huawei.com

Corresponding author

###### Abstract

Recent advancements in State Space Models, notably Mamba, have demonstrated superior performance over the dominant Transformer models, particularly in reducing the computational complexity from quadratic to linear. Yet, difficulties in adapting Mamba from language to vision tasks arise due to the distinct characteristics of visual data, such as the spatial locality and adjacency within images and large variations in information granularity across visual tokens. Existing vision Mamba approaches either flatten tokens into sequences in a raster scan fashion, which breaks the local adjacency of images, or manually partition tokens into windows, which limits their long-range modeling and generalization capabilities. To address these limitations, we present a new vision Mamba model, coined QuadMamba, that effectively captures local dependencies of varying granularities via quadtree-based image partition and scan. Concretely, our lightweight quadtree-based scan module learns to preserve the 2D locality of spatial regions within learned window quadrants. The module estimates the locality score of each token from their features, before adaptively partitioning tokens into window quadrants. An omnidirectional window shifting scheme is also introduced to capture more intact and informative features across different local regions. To make the discretized quadtree partition end-to-end trainable, we further devise a sequence masking strategy based on Gumbel-Softmax and its straight-through gradient estimator. Extensive experiments demonstrate that QuadMamba achieves state-of-the-art performance in various vision tasks, including image classification, object detection, instance segmentation, and semantic segmentation. The code is in https://github.com/VISION-SJTU/QuadMamba.

## 1 Introduction

The architecture of Structured State Space Models (SSMs) has gained significant popularity in recent times. SSMs offer a versatile approach to sequence modeling that balances computational efficiency with model flexibility. Inspired by the success of Mamba  in language tasks, there has been a rise in using SSMs for various vision tasks. These applications range from designing generic backbone models  to advancing fields such as image segmentation  and synthesis . These advancements highlight the adaptability and potential of Mamba in the visual domain.

Despite their appealing linear complexity in long sequence modeling, applying SSMs directly to vision tasks results in only marginal improvements over prevalent CNNs and vision Transformermodels. In this paper, we seek to expand the applicability of the Mamba model for computer vision. We observe that the differences between the language and visual domains can pose significant obstacles in adapting Mamba to the latter. The challenges come from two natural characteristics of image data: 1) Visual data has rigorous 2D spatial dependencies, which means flattening image patches into a sequence may destroy the high-level understanding. 2) Natural visual signals have heavy spatial redundancy--e.g., an irrelevant patch does not influence the representation of objects. To address these two issues, we develop a vision-specific scanning method to construct 1D token sequences for Vision Mamba. Typically, vision Mamba models need to transform 2D images into 1D sequences for processing. As illustrated in Fig. 1(a), the straightforward method, e.g., Vim , that flattens spatial data into 1D tokens directly disrupts the natural local 2D dependencies. LocalMamba improves local representation by partitioning the image into multiple windows, as shown in Fig. 1(b). Each window is scanned individually before conducting a traversal across windows, ensuring that tokens within the same 2D semantic region are processed closely together. However, the handcrafted window partition lacks the flexibility to handle various object scales and is unable to ignore the less informative regions.

In this work, we introduce a novel Mamba architecture that learns to improve local representation by focusing on more informative regions for locality-aware sequence modeling. As shown in Fig. 1(c), the gist of QuadMamba lies in the learnable window partition that adaptively learns to model local dependencies in a coarse-to-fine manner. We propose to employ a lightweight prediction module to multiple layers in the vision Mamba model, which evaluates the local adjacency of each spatial token. The quadrant with the highest score is further partitioned into sub-quadrants in a recursive fashion for fine-grained scan, while others, likely comprising less informative tokens, are kept in a coarse granularity. This process results in window quadrants of varying granularities partitioned from the 2D image feature.

It is noteworthy that direct sampling from the 2D windowed image feature based on the index is non-differentiable, which renders the learning of window selection intractable. To handle this, we adopt Gumbel-Softmax to generate a sequence mask from the partition score maps. We then employ fully differentiable operators, i.e., Hadamard product and element-wise summation, to construct the 1D token sequences from the sequence mask and local windows. These lead to an end-to-end trainable pipeline with negligible computational overhead. For the informative tokens that cross two adjacent quarter windows, we apply an omnidirectional shifting scheme in successive blocks. Shifting 2D image features in two directions allows the quarter-window partition to be flexible in modeling objects appearing in arbitrary locations.

Extensive experiments on ImageNet-1k and COCO2017 demonstrate that QuadMamba excels at image classification, object detection, and segmentation tasks, with considerable advantages over existing CNN, Tranformer, and Mamba models. For instance, QuadMamba achieves a Top-1 accuracy of 78.2% on ImageNet-1k with a similar model size as PVT-Tiny (75.1%) and LocalViM (76.2%).

Figure 1: Illustration of scan strategies for transforming 2D visual data into 1D sequences. (a) naive raster scan [80; 41; 66] ignores the 2D locality; (b) fixed window scan  lacks the flexibility to handle visual signals of varying granularities; (c) our learnable window partition and scan strategy adaptively preserves the 2D locality with a focus on the more informative window quadrant; (d) the effective receptive field of our QuadMamba demonstrates more locality than the plain Vision Mamba.

Related Work

### Generic Vision Backbones

Convolutional Neural Networks (CNNs) [10; 30; 31] and Vision Transformer (ViT)  are two categories of dominant backbone networks in computer vision. They have proven successful as a generic vision backbone across a broad range of computer vision tasks, including but not limited to image classification [29; 53; 55; 20; 23; 21; 74; 4; 12], segmentation [44; 19], object detection [36; 79], video understanding [28; 76], and generation . In contrast to the constrained receptive field in CNNs, vision Transformers [7; 42; 61], borrowed from the language tasks , are superior in global context modelling. Later, numerous vision-specific modifications are proposed to adapt the Transformer better to the visual domain, such as the introduction of hierarchical features [42; 61], optimized training , and integration of CNN elements [5; 54]. Thus, vision Transformers demonstrate leading performance on various vision applications [63; 1; 8; 34; 48]. This, however, comes at a cost of attention operations' quadratic time and memory complexity. which hinders their scalability despite remedies proposed [42; 61; 72; 57].

More recently, State Space Models (SSMs) emerged as a powerful paradigm for modeling sequential data in language tasks. Advanced SSM models  have reported even superior performance compared to state-of-the-art ViT architectures while having a linear complexity. Their initial success on vision tasks [80; 41] and, more importantly, remarkable computational efficiency hint at the potential of SSM as a promising general-purpose backbone alternative to CNNs and Transformers.

### State Space Models

State Space Models (SSMs) [16; 15; 18; 35] are a family of fully recurrent architectures for sequence modeling. Recent advancements [15; 14; 47; 13] have gained SSMs Transformer-level performance, yet with its complexity scaling linearly. As a major milestone, Mamba  revamped the conventional SSM with input-dependent parameterization and scalable, hardware-optimized computation, performing on par with or better than advanced Transformer models on different tasks involving sequential 1D data.

Following the success of Mamba, ViM  and VMamba  reframe Mamba's 1D scan into bi-directional and four-directional 2D cross-scan for processing images. Subsequently, SSMs have been quickly applied to vision tasks (semantic segmentation [51; 65; 46], object detection [26; 3], image restoration [17; 52], image generation , etc.) and to data of other modalities (e.g., videos [67; 32], point clouds [40; 73], graphs , and cross-modality learning [60; 6]).

A fundamental consideration in adapting Mamba to non-1D data concerns the design of a path that scans through and maps all image patches into a SSM-friendly 1D sequence. Along this direction, preliminary efforts include the bi-directional zigzag-style scan in ViM , the 4-direction cross-scan in VMamba , and the serpentine scan in PlainMamba  and ZigMa , all conducted in the spatial domain spanned by the height and width axes. Other works [52; 75; 33] extend the scanning to an additional channel [52; 33] or temporal [75; 33] dimension. Yet, by naively traversing the patches, these scan strategies overlooked the importance of spatial locality preservation. This inherent weakness has been partially mitigated by LocalMamba , which partitions patches into windows and performs traversal within each window.

However, due to a monolithic locality granularity throughout the entire image domain, as controlled by the arbitrary window size, it is hard to decide on an optimal granularity. LocalMamba opts for DARTS  to differentially search for the optimal window size alongside the optimal scan direction for each layer, which adds to the complexity of the method. On the other hand, all existing methods involve hard-coded scan strategies, which can be suboptimal. Unlike all these methods, this paper introduces a learnable quadtree structure to scan image patches with varying locality granularity.

## 3 Preliminaries

**State Space Models (S4)**. SSMs [16; 15; 18; 35] are in essence linear time-invariant systems that map a one-dimensional input sequence \(x(t)^{L}\) to output response sequence \(y(t)^{L}\) recurrently through hidden state \(h(t)^{N}\) (with sequence length \(L\) and state size \(N\)). Mathematically, such 

[MISSING_PAGE_FAIL:4]

Transformer block [7; 68], as illustrated in Fig 2(b). QuadMamba consists of a cascade of QuadVSS blocks organized in four stages, with stage \(i\) (\(i\{1,2,3,4\}\)) having \(S_{i}\) QuadVSS blocks. In each stage, a downsampling layer halves the spatial size of feature maps while doubling their channel dimension. Thanks to the linear complexity of Mamba, we are free to stack more QuadVSS blocks within the first two stages, which enables their local feature preserving and modeling capabilities to be fully exploited with minimal computational overheads introduced.

### Quadtree-based Visual State Space Block

As shown in Fig 2, our QuadVSS block adopts the meta-architecture  in vision Transformer, formulated by a token operator, a feedforward network (FFN), and two residual connections. The token operator consists of a shift module, a partition map predictor, a quadtree-based scanner, and a Mamba Layer. Inside the token operator, a lightweight prediction module first predicts a partition map over feature tokens. The quadtree-based strategy then partitions the 2D image space by recursively subdividing it into four quadrants or windows. According to the scores of the partition map at the coarse level, fine-level sub-windows within less informative coarse-level windows are skipped. Thus, a multi-scale, multi-granularity 1D token sequence is constructed, capturing more locality in more informative regions while retaining global context modeling elsewhere. The key components of the QuadVSS block are detailed as follows:

**Partition map prediction.** The image feature \(^{H W C}\), containing a total of \(N=HW\) embedding tokens, is first projected into score embeddings \(_{}\):

\[_{}=_{s}(),_{}^{N  C},\] (4)

where \(_{s}\) is a lightweight projector with a Norm-Linear-GELU layer. To better assess each token's locality, we leverage both the local embedding and the context information within each quadrant. Specifically, we first split \(_{}\) in the channel dimension to obtain the local feature \(_{}^{}\) and the context feature \(_{}^{}\):

\[_{}^{},\ \ _{}^{}=_ {}[0:],\ \ _{}[:C],\{_{}^{},_{}^{}\}^{H W}.\] (5)

Then, we obtain \(2 2\) context vectors through an adaptive pooling layer and broadcast each context vector \(_{}^{}\) into the local embedding along the channel dimension:

\[_{}^{}& =(_{}^{}), _{}^{}^{2 2},\\ _{}^{}&=( {x}_{}^{},(_{}^ {})),_{}^{}^{H  W C},\] (6)

Figure 3: Quadtree-based selective scan with prediction modules. Image tokens are partitioned into bi-level window quadrants from coarse to fine. A fully differentiable partition mask is then applied to generate the 1D sequence with negligible computational overhead.

[MISSING_PAGE_FAIL:6]

where sequence \(\) contains tokens in both coarse- and fine-level windows and is sent to the SS2D block for sequence modeling.

**Omnidirectional window shifting.** Considering the case that the most informative tokens are crossing two adjacent window quadrants, we borrow the idea of a shifted window scheme in Swin Transformer . The difference is that the Transformer ignores the spatial locality for each token inside the window, while the token sequence inside the window in Mambo is still directional. Thus, we add additional shifted directions in the subsequent VSS blocks as shown in Fig 4, compared to only one direction shifting in Swin Transformer.

### Model Configuration

It is noteworthy that QuadMamba's model capacity can be customized by tweaking the input feature dimension \(d\) and the number of (Q)VSS layers \(\{S_{i}\}_{i=1}^{4}\). In this work, we build four variants of the QuadMamba architecture, QuadMamba-Li/T/S/B, with varying capacities:

* [noitemsep,topsep=0pt]
* block:\(\{2,2,2,2\}\), QuadVSS stages:\(\{1,2\}\), #Params: 5.4M, FLOPs: 0.82G.
* block:\(\{2,6,2,2\}\), QuadVSS stages:\(\{1,2\}\), #Params: 10.3M, FLOPs: 2.0G.
* block:\(\{2,2,5,2\}\), QuadVSS stages:\(\{1,2\}\), #Params: 31.2M, FLOPs: 5.5G.
* block:\(\{2,2,15,2\}\), QuadVSS stages:\(\{1,2\}\), #Params: 50.6M, FLOPs: 9.3G.

In all these variants, QuadVSS blocks are placed in the specified QuadVSS model stages to bring into full play their locality preservation capabilities on higher-resolution features, Omnidirectional shifting layers are applied in every other QuadVSS blocks. More details are found in the Appendix.

## 5 Experiment

We conduct experiments on commonly used benchmarks, including ImageNet-1k  for image classification, MS COCO2017  for object detection and instance segmentation, and ADE20K  for semantic segmentation. Our implementations follow prior works [42; 80; 41]. Detailed descriptions of the datasets and configurations are found in the Appendix. In what follows, we compare the proposed QuadMamba with mainstream vision backbones and conduct extensive ablation studies to back the motivation behind QuadMamba's designs.

### Image Classification on ImageNet-1k

Tab. 1 demonstrates the superiority of QuadMamba in terms of accuracy and efficiency. Specifically, QuadMamba-S surpasses RegNetT-8G  by 2.5%, DeiT-S  by 2.6%, and Swin-T  by 1.1% Top-1 accuracy, with comparable or reduced FLOPs. This advantage holds when comparing other model variants of similar numbers of parameters or FLOPs. Compared to other Mamba-based vision backbones, QuadMamba also yields favorable performance under comparable network complexity. For instance, QuadMamba-B (\(83.8\%\)) performs on par with or better than VMamba-B (\(83.7\%\)), LocalVim-S (\(81.2\%\)), and PlainMamba-L3 (\(82.3\%\)), while having significantly less parameters and FLOPs. These results manifest the performance and complexity superiority of QuadMamba and its potential as a powerful yet highly efficient vision backbone. Furthermore, QuadMamba achieves similar or higher performance than LocalMamba while being completely free of the latter's expensive architecture and scan policy search, which makes it a more practical and versatile choice of vision backbone.

### Object Detection and Instance Segmentation on COCO

On object detection and instance segmentation tasks, QuadMamba stands out as a highly efficient backbone among models and architectures of similar complexity, as measured by number of networks parameters and FLOPs. QuadMamba finds very few competitors under the category of tiny backbones with less than or around 30M parameters. As shown in Tab. 2, in addition to dramatically outperforming ResNet18  and PVT-T , QuadMamba-T also leads EfficientVAMba-S  by considerable margins of \(3.0\%\) mAP on object detection and \(2.1\%\) on instance segmentation. Among larger backbones, QuadMamba once again surpasses all ConvNet-, Transformer-, and Mambo-based

[MISSING_PAGE_FAIL:8]

**Effect of locality in Mamba.** We factorise the effect of coarse- and fine-grain locality modeling in building 1D token sequences on model performance. Specifically, we compare the naive window-free flattening strategy in  that overlooks 2D locality against window partitions in three scales (i.e., \(28 28\), \(14 14\), \(2 2\)) that represent three granularity levels of feature locality. In practice, we replace QuadVSS blocks of a QuadMamba-T model with the plain VSS blocks in . To exclude the negative effects of padding operations, we only partition the features with a spatial size of \(56 56\) in the first model stage. As illustrated in Tab. 4, the naive scan strategy leads to significantly degraded object detection and instance segmentation performance compared to when windowed scan is adopted. The scale of local windows is also shown to considerably influence the model performance, which suggests that too large or too small a window given the image resolution can be suboptimal.

**Quadtree-based partition resolutions.** We examine the choice of partition resolutions in the bi-level quadtree-based partition strategy. The configured resolutions in Tab. 5 are applied in the first two

   Backbones & \#Params (M) & FLOPs (G) & \(^{}\) & \(^{}_{50}\) & \(^{}_{75}\) & \(^{}_{75}\) & \(^{}_{75}\) \\  R18  & 31 & 207 & 34.0 & 54.0 & 36.7 & 31.2 & 51.0 & 32.7 \\ PVT-T  & 32 & 208 & 36.7 & 59.2 & 39.3 & 35.1 & 56.7 & 37.3 \\ ViL-T  & 26 & 145 & 41.4 & 63.5 & 45.0 & 38.1 & 60.3 & 40.8 \\ EfficientV Mamba-S  & 31 & 197 & 39.3 & 61.8 & 42.6 & 36.7 & 58.9 & 39.2 \\
**QuadMamba-Li** & 25 & 186 & **39.3** & **61.7** & **42.4** & **36.9** & **58.8** & **39.4** \\
**QuadMamba-T** & 30 & 213 & **42.3** & **64.6** & **46.2** & **38.8** & **61.6** & **41.4** \\  R50  & 44 & 260 & 38.6 & 59.5 & 42.1 & 35.2 & 56.3 & 37.5 \\ PVT-S  & 44 & - & 40.4 & 62.9 & 43.8 & 37.8 & 60.1 & 40.3 \\ Swin-T  & 48 & 267 & 42.7 & 65.2 & 46.8 & 39.3 & 62.2 & 42.2 \\ ConvNeXt-T  & 48 & 262 & 44.2 & 66.6 & 48.3 & 40.1 & 63.3 & 42.8 \\ EfficientV Mamba-B  & 53 & 252 & 43.7 & 66.2 & 47.9 & 40.2 & 63.3 & 42.9 \\ PlainMmaba-L  & 53 & 542 & 46.0 & 66.9 & 50.1 & 40.6 & 63.8 & 43.6 \\ ViL-S  & 45 & 218 & 44.9 & 67.1 & 49.3 & 41.0 & 64.2 & 44.1 \\ VMDA-T  & 42 & 262 & 46.5 & 68.5 & 50.7 & 42.1 & 65.5 & 45.3 \\ LocalV Mamba-T  & 45 & 291 & 46.7 & 68.7 & 50.8 & 42.2 & 65.7 & 45.5 \\
**QuadMamba-S** & 55 & 301 & **46.7** & **69.0** & **51.3** & **42.4** & **65.9** & **45.6** \\   

Table 2: Object detection and instance segmentation results on the COCO val2017 split using the Mask RCNN  framework.

   Backbone & Image size & \#Params (M) & FLOPs (G) & mIoU (SS) & mIoU (MS) \\  EfficientV Mamba-T  & \(512^{2}\) & 14 & 230 & 38.9 & 39.3 \\ DeiT-Ti  & \(512^{2}\) & 11 & - & 39.2 & - \\ Vim-Ti  & \(512^{2}\) & 13 & - & 40.2 & - \\ EfficientV Mamba-S  & \(512^{2}\) & 29 & 505 & 41.5 & 42.1 \\ LocalVim-T  & \(512^{2}\) & 36 & 181 & 43.4 & 44.4 \\ PlainMamba-L1  & \(512^{2}\) & 35 & 174 & 44.1 & - \\
**QuadMamba-T** & \(512^{2}\) & 40 & 886 & **44.3** & **45.1** \\   ResNet-50  & \(512^{2}\) & 67 & 953 & 42.1 & 42.8 \\ DeiT-S + MLN  & \(512^{2}\) & 58 & 1217 & 43.8 & 45.1 \\ Swin-T  & \(512^{2}\) & 60 & 945 & 44.4 & 45.8 \\ Vim-S  & \(512^{2}\) & 46 & - & 44.9 & - \\ LocalVim-S  & \(512^{2}\) & 58 & 297 & 46.4 & 47.5 \\ EfficientV Mamba-B  & \(512^{2}\) & 65 & 930 & 46.5 & 47.3 \\ PlainMamba-L  & \(512^{2}\) & 55 & 285 & 46.8 & - \\ V Mamba-T  & \(512^{2}\) & 55 & 964 & 47.3 & 48.3 \\ LocalV Mamba-T  & \(512^{2}\) & 57 & 970 & 47.9 & 49.1 \\
**QuadMamba-S** & \(512^{2}\) & 62 & 961 & **47.2** & **48.1** \\  ResNet-101  & \(512^{2}\) & 85 & 1030 & 42.9 & 44.0 \\ DeiT-B + MLN  & \(512^{2}\) & 144 & 2007 & 45.5 & 47.2 \\ Swin-S  & \(512^{2}\) & 81 & 1039 & 47.6 & 49.5 \\ PlainMamba-L3  & \(512^{2}\) & 81 & 419 & 49.1 & - \\ V Mamba-S  & \(512^{2}\) & 76 & 1081 & 49.5 & 50.5 \\ LocalV Mamba-S  & \(512^{2}\) & 81 & 1095 & 50.0 & 51.0 \\
**QuadMamba-B** & \(512^{2}\) & 82 & 1042 & **49.7** & **50.8** \\   

Table 3: Semantic segmentation results on ADE20K using UperNet . mIoUs are measured with single-scale (SS) and multi-scale (MS) testings on the _val_ set. FLOPs are measured with an input size of \(512 2048\).

model stages with feature resolutions of \(\{56 56,28 28,14 14\}\). Experimentally we deduce the optimal resolutions to be \(\{1/2,1/4\}\) for the coarse- and fine-level window partition, respectively. This handcrafted configuration may be replaced by more flexible and learnable ones in future work.

**Layer patterns in the hierarchical model stage.** We investigate different design choices of the layer pattern within our hierarchical model pipeline. From Fig. 5, layer pattern LP2 outperforms LP1 with less QuadVSS blocks by \(0.2\%\) accuracy. This is potentially due to the effect of locality modeling being more pronounced in shallower stages than in deeper stages as well as the adverse influence of padding operations in stage 3. LP3, which places the QuadVSS blocks in the first two stages and in an interleaved manner, achieves the best performance, and is adopted as our model design.

**Necessity of multi-directional window shift.** Different from the unidirectional shift in Swin Transformer , Fig. 5 shows a \(0.2\%\) gain in accuracy as complementary shifting directions are added. This is expected since attention in Transformer is non-causal, whereas 1D sequences in Mamba, being causal in nature, are highly sensitive to relative position. A multi-directional shifting operation is also imperative for handling cases where the informative region spans across adjacent windows. Fig. 6 further visualizes the shifts in the learned fine-grained quadrants across hierarchical stages, which adaptively attend to different spatial details at different layers.

**Numbers of (Quad)VSS blocks per stage.** We conduct experiments to evaluate the impact of different numbers of (Quad)VSS blocks in each stage. Tab. 6 presents four configurations, following design rule LP3 in Fig. 5, with a fixed channel dimension of 96. We find that a bulky 2nd or 4th stage leads to diminished performance compared to a heavy 3rd stage design, whereas dealing out the (Quad)VSS blocks more evenly between stages 2 and 3 yields comparable if not better performance with favorable complexities. This evidence can serve as a rule of thumb for model design in future work, especially as the model scales up.

## 6 Conclusion

In this paper, we propose QuadMamba, a vision Mamba architecture that serves as a versatile and efficient backbone for visual tasks, such as image classification and dense predictions. QuadMamba effectively captures local dependencies of different granularities by learnable quadtree-based scanning, which adaptively preserves the inherent locality within image data with negligible computational overheads. The QuadMamba's effectiveness has been proven through extensive experiments and ablation studies, outperforming popular CNNs and vision transformers. However, one limitation of QuadMamba is that window partition with more than two levels is yet to be explored, which may be particularly relevant for handling dense prediction visual tasks and higher-resolution data, such as remote sensing images. The fine-grained partition regions are rigid and lack flexibility in attending to regions of arbitrary shapes and sizes, which is left for our future work. We hope that our approach will motivate further research in applying Mamba to more diverse and complex visual tasks.

    & Depth \(\#\)Params. & FLOPs & Top-1 (\%) \\ 
2-2-8-2 & 31 & 9.2 & 82.0 \\ 
2-8-2-2 & 38 & 8.1 & 81.5 \\ 
2-2-2-8 & 74 & 7.8 & 81.8 \\ 
2-4-6-2 & 36 & 8.5 & **82.1** \\   

Table 6: Impact of different number of (Quad)VSS blocks per stage.

   Window Size & Top-1 (\%) & \(^{}\) & \(^{}\) \\  w/o windows & 72.2 & 33.1 & 30.5 \\ \(28 28\) & 72.9 & 33.8 & 31.7 \\ \(14 14\) & **73.5** & **35.8** & **32.1** \\ \(2 2\) & 72.4 & 33.4 & 30.6 \\   

Table 4: Impact of using different local window sizes and the naive flattening strategy.

Figure 5: Impact of different Figure 6: Visualization of partition maps that focus on different layer patterns and shift directions. regions from shallow to deep blocks.

    & Depth \(\#\)Params. & FLOPs & Top-1 (\%) \\ 
2-2-8-2 & 31 & 9.2 & 82.0 \\ 
2-8-2-2 & 38 & 8.1 & 81.5 \\ 
2-2-2-8 & 74 & 7.8 & 81.8 \\ 
2-4-6-2 & 36 & 8.5 & **82.1** \\   

Table 5: Impact of different local window partition resolution.

Acknowledgments.This work was supported by NSFC (62322113, 62376156), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and the Fundamental Research Funds for the Central Universities.