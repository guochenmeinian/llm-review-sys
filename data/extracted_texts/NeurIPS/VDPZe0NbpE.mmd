# PRODuctive bandits: Importance Weighting No More

Julian Zimmert

Google Research

zimmert@google.com &Teodor V. Marinov

Google Research

tvmarinov@google.com

###### Abstract

Prod is a seminal algorithm in full-information online learning, which has been conjectured to be fundamentally sub-optimal for multi-armed bandits. By leveraging the interpretation of Prod as a first-order OMD approximation, we present the following surprising results: 1. Variants of Prod can obtain optimal regret for adversarial multi-armed bandits. 2. There exists a simple and (arguably) importance-weighting free variant with optimal rate. 3. One can even achieve best-both-worlds guarantees with logarithmic regret in the stochastic regime.

The bandit algorithms in this work use simple arithmetic update rules without the need of solving optimization problems typical in prior work. Finally, the results directly improve the state of the art of incentive-compatible bandits.

## 1 Introduction

The adversarial multi-armed bandit (MAB) problem is a seminal online learning problem with applications in experimental design, online advertisement and more (Thompson, 1933; Lai and Robbins, 1985; Auer et al., 2002, 2002). MABs are characterized by the limited feedback given to the learner in every round, the so-called bandit feedback, in which the learner only observes the loss of their selected action, unlike in the full information, also known as the experts, setting where the loss of all actions are provided as feedback.

The first nearly optimal algorithm for the adversarial MAB problem is EXP3 (Auer et al., 2002). EXP3 is a direct adaptation of the Hedge algorithm (Littlestone and Warmuth, 1994; Cesa-Bianchi et al., 1997; Freund and Schapire, 1997) with importance weighting to handle partial bandit feedback. Hedge and EXP3 are special versions of online mirror descent (OMD), where the Bregman divergence is the KL divergence induced by the Negative entropy potential. The OMD view of online learning (Abernethy et al., 2008) has lead to a wide range of MAB algorithms such as Tsallis-INF (Audibert and Bubeck, 2009) and Logbarrier (Agarwal et al., 2017), which enjoy improved regret guarantees. These guarantees can be attributed to regularizers more suited to the bandit feedback setting, compared to the negative entropy regularizer. A downside of OMD is that usually the mirror descent update is not closed form and requires (approximately) solving optimization problems at every iteration.

Alternative full-information algorithms with simple arithmetic updates are Prod (Cesa-Bianchi et al., 2007; Even-Dar et al., 2008; Gaillard et al., 2014), which enjoys second order regret bounds and Multiplicative Weights Update (MWU) (Arora et al., 2012). Prod is known to be closely related to Hedge, both of which are different generalizations of the weighted majority algorithm (Littlestone and Warmuth, 1994) to non-binary feedback.

More recently, Freeman et al. (2020) studied incentive-compatible online learning, a setting where experts are not necessarily truthful but make predictions strategically with regard to the agent's algorithm. Motivated by deriving an algorithm where the incentives of experts align with the agent,they propose WSU which can be seen as an instance of Prod. Freeman et al. (2020) further introduce a bandit adaptation WSU-UX, the first Prod algorithm for bandits. Unfortunately, WSU-UX only enjoys \(T^{}\) regret guarantees. This is not merely an issue with their analysis as has been shown via lower bounds (Mortazavi et al., 2024) and leads to the conjecture that this might be a fundamental separation between full-information and bandits.

We make the following contributions for understanding Prod under bandit feedback:

1. We disprove the separation conjecture by providing a simple modification of WSU-UX with nearly optimal \(O()\) regret guarantees.
2. We present a Prod variant that does not require importance weighting and yet enjoys \(O()\) regret bounds.
3. We present a Prod variant that achieves best of both worlds regret guarantees, i.e. it enjoys improved \(O((T))\) regret bounds when the losses are stochastic, while maintaining worst-case \(O()\) regret bounds.

Notation:For \(N\), let \([N]=\{1,,N\}\). We use \(,\) to denote the regular Euclidean scalar product and \((A)\) to denote the probability simplex over a finite set \(A\). \(O\) is the standard Landau notation hiding numerical constants, while \(\) omits polylogaritmic factors as well. The expectation \(\) is always taken over all randomness of the algorithm, losses and experts, while \(_{t}[]=[\,|_{t}]\), where \(_{t}\) is the filtration over all randomness up to step \(t\). \((E)\) denotes the indicator function function for the event \(E\). For a convex differentiable function \(F\), the Bregman divergence is defined by \(D_{F}(y,x)=F(y)-F(x)- y-x, F(x)\).

## 2 Problem setting and related work

The adversarial bandit problem is formally defined as follows. In every round \(t=1,,T\), an (oblivious) adversary selects a loss \(_{t}^{K}\) (it is possible to extend the loss range to \([-1,1]^{K}\)) unknown to the agent. The agent simultaneously selects an expert \(A_{t}_{t}\), \(_{t}([K])\). The agent incurs and observes the loss \(_{t,A_{t}}\), but does not see the losses of other experts. The goal is to minimize the pseudo-regret1

\[=_{i[K]}[_{t=1}^{T}_{t,A_{t}}-_ {t,i}]=_{u([K])}[_{t=1}^{T} _{t}-u,_{t}]\,,\]

Popular families of algorithms for this problem setting include online mirror descent (OMD) and follow the regularized leader (FTRL). Typically the algorithms use unbiased loss estimates of the loss vector via importance weighting: \(_{t,i}=}{_{t,i}}(A_{t}=i)\). We note that other types of importance weighted estimators have been used in literature such as the implicit exploration estimator Kocak et al. (2014), which has improved variance properties. The algorithms are defined by a twice-differentiable convex potential function \(F:^{K}\) and a learning rate schedule \(_{t}\), the agent maintains a distribution via

\[_{t+1} =_{([K])},_{t}_ {t}-D_{F}(,_{t})\,,\] (OMD) \[_{t+1} =_{([K])},_{t}_{s=1}^{ t}_{s}-F()\,,\] (FTRL)

OMD optimizes locally given the last loss and, as we will show, is most closely related to Prod. FTRL on the other hand performs a global optimization and is generally considered superior for adaptive bounds with time-dependent learning rates. In some special cases, such as time-independent learning rate with potentials that satisfy \(\| F(x)\|\) on the border of the optimization set, both algorithms are equivalent. Common potentials in the bandit literature are given in Table 1 and we refer to their respective Bregman divergences as \(D_{KL},D_{TS}\) and \(D_{LB}\) respectively. The negative entropy is the potential which defines Hedge and Exp-3 (and derivatives) (Littlestone and Warmuth, 1994, Vovk, 1995, Freund and Schapire, 1997, Auer et al., 2002b, Kocak et al., 2014). The \(1/2\)-Tsallis Entropy is the key to achieving optimal best-of-both-worlds regret guarantees as was first demonstrated by Zimmert and Seldin (2021). The Logbarrier potential was used by Agarwal et al. (2017) to first solve the corralling of bandits problem and has found many applications in model-selection problems (Foster et al., 2020), regret bounds which depend on the properties of the loss sequence (Wei and Luo, 2018, Lee et al., 2020b, a) and various other bandit problems.

### Prod family of algorithms

The original version of Prod (Cesa-Bianchi et al., 2007) maintains weights \(w_{t,i}\) for each experts which are updated via \(w_{t+1,i}=w_{t,i}(1-_{t,i})\) and the agent plays the policy \(_{t,i} w_{t,i}\). This framework has been extended to D-Prod (Even-Dar et al., 2008), which shifts the losses in the weight update by the loss of a fixed policy, and ML-Prod (Gaillard et al., 2014) that shifts losses by the mean of the current policy (among other modifications). With a suitable shift in losses, one can ensure that the weights sum up to \(1\) and hence operate directly on the policy space. In its simplest form, this is

\[_{1,i}=\,,_{t+1,i}=_{t,i}(1-(_{t,i}-_ {t}))\,,_{t}=_{j=1}^{K}_{t,j}_{t,j}\,.\]

We refer to this update as Vanilla-Prod to emphasize its connection to the Prod literature, however this algorithm is exactly WSU (Freeman et al., 2020) derived for incentive-compatible online learning. We consider any algorithm a variant of Prod if it performs product updates of the form \(_{t+1,i}=_{t,i}(1- L_{t,i}(_{t};A_{t}))\), where \(L_{t,i}\) are linear affine functions of the loss. From now on we always assume the initial policy is \(_{t,i}=1/K\), and this holds for all algorithms presented in the paper. The appeal of Prod algorithms lies in their simple arithmetic update rule. A second motivation for using Prod updates is the mentioned incentive-compatibility.

### Incentive-compatible online learning

In the incentive-compatible online learning setting, introduced by Freeman et al. (2020), experts provide recommendations, for example a prediction of whether it will rain on the next day. Each expert has an internal belief and the agent would like to receive each expert's true beliefs in order to learn to follow the best expert. In the simplest setting the experts make predictions about binary outcomes, with the \(i\)-th expert having (private) belief \(b_{t,i}\) about the \(t\)-th round outcome. The expert's belief is unknown to the agent and the expert only reports a prediction \(p_{t,i}\) about the outcome. Based on the expert predictions, \(\{p_{t,i}\}_{i[K]}\) the agent makes a prediction based on \(_{t}=_{i=1}^{K}_{t,i}p_{t,i}\) and incurs a loss \((_{t},r_{t})\) based on the realized outcome \(r_{t}\{0,1\}\). In the weather forecasting example the outcome is the indicator if it trains the next day and the loss is \((_{t},r_{t})=(r_{t}-_{t})^{2}\). In Freeman et al. (2020) the experts only care about maximizing the probability that they are selected which does not necessarily result in truthful reporting, that is \(b_{t,i}\) may differ from \(p_{t,i}\). The agent's goal of receiving the true beliefs, \(\{b_{t,i}\}_{i[K]}\), can be achieved by playing an _incentive compatible strategy_ which will always prefer selecting an truthful expert, that is the probability of \(_{t+1,i}\) of selecting expert \(i\) when the expert reports \(b_{t,i}\) may only decrease if the expert reports any other \(p_{t,i}\) instead, no matter how the remaining experts act throughout the game. This is made precise in Definition 2.1 of Freeman et al. (2020).

Freeman et al. (2020) show that standard OMD and FTRL algorithms are in fact not incentive compatible even when the loss \(\) is restricted to be _proper_ that is \(_{r(b)}[(p,r)]_{r(b)}[(b,r)]\) for all \(p b\). It turns out that any update for \(_{t+1}\) which is linear affine in the proper loss function will lead to incentive compatibility and so the Prod family will ensures that experts report their true believes in this setting, i.e. they are incentive-compatible. The state of the art for incentive-compatible bandits is \(T^{}\) regret and any improvement for Prod directly transfers to better rates for this setting as well.

  & Negentropy/KL divergence & \(1/2\)-Tsallis Entropy & Logbarrier \\  \(F()\) & \(_{i=1}^{K}_{t,i}(_{t,i})\) & \(-2_{i=1}^{K}}\) & \(-_{i=1}^{K}(_{t,i})\) \\ 

Table 1: Common potential functionsModifying WSU-UX for nearly optimal regret guarantees

We begin by presenting a minimal modification of Algorithm WSU-UX which is sufficient for a regret guarantee of the order \(O()\).

WSU-UX uses importance-weighted updates and injects a small uniform exploration.

\[_{t,i} =+(1-)_{t,i}, A_{t}_{t,i}, _{t,i}=(A_{t}=i)}{_{t,i}}\] \[_{t+1,i} =_{t,i}(1-(_{t,i}-_{t}))\,, _{t}=_{j=1}^{K}_{t,j}_{t,j}\,,\] (WSU-UX)

where \(\) is the mixture coefficient. The role of uniform exploration is to ensure that the policy updates are proper i.e. \(_{t+1,i}(0,1)\). Freeman et al. (2020) uses the following key lemmas in their analysis, which hold for any sequence of losses \(_{t}^{K}\). For completeness we restate the results we use below.

**Lemma 1** (Lemma 4.1(Freeman et al., 2020)).: _If \( K/\), the WSU-UX weights \(_{t}\) and \(_{t}\) are valid probability distributions for all \(t[T]\)._

**Lemma 2** (Lemma 4.3(Freeman et al., 2020)).: _For WSU-UX, the probability vectors \(\{_{t}\}_{t[T]}\) and loss estimators \(_{t}\) satisfy the following second order-bound_

\[_{t=1}^{T}_{i=1}^{K}_{t,i}_{t,i}-_{t=1}^{T}_{t,i^{}}+_{t=1}^{T}_{t,i^{ }}^{2}+_{t=1}^{T}_{i=1}^{K}_{t,i}_{t,i}^{2},\]

_where \(i^{}\) is the optimal expert/arm._

The bound in Lemma 2 is almost the standard regret bound that appears in the analysis of Hedge, except for the term \(_{t=1}^{T}_{t,i^{}}^{2}\). This term is the reason why prior work can not show regret bounds smaller than \(T^{}\). Even after taking the expectation over the randomness of the agents actions, this term scales with with \(1/_{t,i^{}}\), which is potentially unbounded. Alternatively this term can be written as \(_{t}^{j^{}}[_{tj}^{2}]\) (where \(^{}\) is the policy picking \(i^{}\) with probability 1) and if one could perform a change of measure to \(_{t}^{j_{t}}[_{tj}^{2}]\), this term is immediately controllable.

In fact, change of measure techniques for bandits are now well established (Foster et al., 2020; Luo et al., 2021) by introducing biases to the losses. Assume we construct a bias to the losses \(_{t}=_{t}+_{t}\), which satisfies the same regret guarantee, \(\), as the original loss sequence, then running an algorithm over the biased loss sequence \(_{t}\) which selects \(A_{t}_{t}\) ensures

\[[_{t=1}^{T}_{t,A_{t}}-_{t,i^{}}] =[_{t=1}^{T}_{t,A_{t}}-_{t,i^{}}+_{t,A_{t}}-_{t,i^{}}]\] \[=+[_{t=1}^{T}(_{t}^{j_{t}}[_{t,j}]-_{t}^{j^{}}[ _{t,j}]}_{})]\,.\]

We introduce now the following modification to the losses

\[_{t,i}=_{t,i}(1-_{t,i}}), _{t,i}=(A_{t}=i)_{t,i}}{_{t,i}}\,.\] (1)

which corresponds to \(_{ti}=}{_{t,i}}\). This yields the change of measure term

\[_{t}^{j_{t}}[_{t,j}]-_{t}^{j^{ }}[_{t,j}]=_{i=1}^{K}_{ti}-_{t}^{j^{ }}[}{_{tj}}] K-_{t}^{j^{}}[_{tj}^{2}]\,,\]

which is sufficient for controlling the term \(_{t=1}^{T}_{t,i^{}}^{2}\). in Lemma 2.

**Theorem 1**.: _Running WSU-UX with the loss estimators in Equation 1 and \(=,=(})\) guarantees the following regret bound_

\[_{t=1}^{T}[_{t,A_{t}}-_{t,i^{}}] O().\]

The proof of Theorem 1 is deferred to Appendix B.

### Intuition on biasing the update and the Prod family of algorithms

We provide an intuition in this section about why WSU-UX is not tight and why the bias we choose is able to correct the regret. The modern analysis of OMD (or FTRL) with a divergence function \(D\) is follows the template2

\[&_{t=1}^{T}_{t}^{}- ^{},_{t}=_{t=1}^{T}(_{t}^{ }-^{},_{t}+,_{ t+1}^{})-D(^{},_{t}^{})}{} )\\ &+,_{1}^{})-D(^{}, _{T+1}^{})}{}_{t=1}^{T}(^{}-_{t+1}^{},_{t} +^{},_{t}^{} )}{}}_{}+,_{1}^{})}{}\,,\] (2)

where the inequality crucially relies on \(_{t+1}^{}\) being the 1-step OMD update to the previous policy. OMD algorithms like Hedge choose the policy that minimizes the per-step _stability_ term in every round, which is what allows for the stability term to be bounded appropriately. If instead of playing \(_{t}^{}\) an approximate policy \(_{t}_{t}^{}\) is played, the template regret analysis can be changed by adding the terms

\[^{-1}(D(^{},_{t+1})-D(^{},_{t+1}^{}) )\,,\]

where \(_{t+1}^{}\) is now the 1-step OMD update from \(_{t}\). When \(D\) is the KL divergence and the approximate policy \(_{t}\) is coming from the WSU-UX, this term contributes the undesirable \(_{t,i^{}}^{2}\).

We now explain how our loss biasing solves this issue. The Vanilla-Prod/WSU update can be seen as a first order approximation to the Hedge update, that is

\[_{t+1,i}^{}=_{t,i}(-(_{t,i}-_ {t}))_{}_{t,i}(1-(_{t,i }-_{t}))=_{t,i}\,,\]

where \(_{t}\) is a normalization factor. Tuning \(_{t}\) such that \(_{i=1}^{K}_{t+1,i}=1\) recovers Vanilla-Prod/WSU. To control the undesirable terms, we have to make the approximation tighter. The loss-biasing introduced in the previous section acts as a correction which brings the Vanilla-Prod/WSU update closer to the second order approximation of the Hedge update. Indeed, we have

\[_{t+1,i}^{}=_{t,i}(-(_{t,i}-_{t}))_{}_{t,i}(1- (_{t,i}-_{t})+}{2}(_{t,i}- _{t})^{2})\] \[=_{t,i}(1-(_{t,i}-_{t})(1- {}{2}(_{t,i}-_{t})))\,,\]

and so our loss adjustment in Equation 1, \(_{t,i}=_{t,i}(1-/_{t,i})\), can be seen as a second order correction to the term \(}{2}(_{t,i}-_{t})^{2}\). We cannot exactly correct the second order difference with linear update rules, which we address by slightly overcorrecting, i.e. biasing by a larger amount than the second order adjustments implies as necessary. That is, the correction term we use is of the order \(/_{t,i}\) instead of \(_{t,i}/_{t,i}\). Fortunately, the regret analysis is not sensitive towards this as we have shown in Theorem 1.

Importance weighting free adversarial MAB with LB-Prod

While our biased WSU-UX obtains optimal regret, it still has to go through the extra complexity of injecting additional uniform exploration at a rate of \(\) to ensure proper updates and add bias to the losses. As mentioned in the introduction, prior work proposed other potential functions that have favourable properties for bandit feedback. Using the same linearization argument to derive a Prod version based on the Logbarrier leads to a surprisingly simple algorithm without loss biasing that is arguably importance weighting free. LB-Prod differs from WSU-UX by using the masked loss \(_{t,i}=_{t,i}(A_{t}=i)\) instead of the importance weighted loss and a non-symmetric normalization \(_{t,i}\):

\[_{t+1,i}=_{t,i}(1-(_{t,i}-_{t,i})), _{t,i}=_{t,i}}_{t,A_{t}}}{_{j=1}^{K}_{t,j}^{2}}\,.\] (LB-Prod)

It is easy to confirm \(_{t,i}-_{t,i}[-1,1]\) via \(_{t,i}_{t,A_{t}}(_{t,i}^{2}+_{t,A_{t}}^{2})/2\) yielding proper updates for \(<1\). The following theorem shows that this simple algorithm is rate optimal under the right tuning.

**Theorem 2**.: _For any sequence of losses \(_{t}[-1,1]^{K}\) and any \(<1\), LB-Prod produces valid distributions \(_{t}([K])\) and its regret is bounded by_

\[_{t=1}^{T}[_{t,A_{t}}-_{t,i^{*}}] 2++\,.\]

Tuning \(=}\) results in a regret bound of \(O()\) for any \(T>\). The proof of Theorem 2 is deferred to the end of the section.

### Intuition of LB-Prod

As mentioned before, LB-Prod is the linear approximation of OMD with Bregman divergence induced by the Logbarrier potential, that is \(D_{LB}\) induced by the potential fuction \(F(x)=-_{i=1}^{K}(x_{i})\). The one-step Logbarrier OMD update of a policy \(_{t}\) with importance-weighting is known (see e.g. (Zimmer and Seldin, 2021)) to take the form

\[_{t+1,i}^{}}=}{1+_{t,i}(_{ t,i}-_{t})}_{t,i}(1-_{t,i}(_{t,i}-_{t}))= _{t+1,i}\,,\]

where \(_{t}\) is a normalization constant that ensures \(_{t}^{}}\) is a probability distribution. If instead \(_{t}\) is tuned so that \(_{i=1}^{K}_{t+1,i}=1\), the LB-Prod update is recovered. This can be be verified by setting \(_{t,i}=_{t,i}_{t,i}\) and \(_{t,i}=_{t,i}_{t}\). The curvature of the Logbarrier regularization is what ensures that the importance weighted loss \(_{t,i}\) is always multiplied with its probability \(_{t,i}\), allowing to run the algorithm on the masked non-weighted loss sequence directly.

Additionally, the second order approximation is

\[_{t+1,i}^{}}=}{1+_{t,i}(_{t,i}-_{t})}_{}_{t,i}(1-_ {t,i}(_{t,i}-_{t})+^{2}_{t,i}^{2}(_{t,i}- _{t})^{2})\,.\]

The additional undesirable terms, unlike in the case for WSU-UX, will only contribute \( T\) regret if not adjusted for as we show next.

### Analysis of LB-Prod

The following technical lemma is proven in the appendix.

**Lemma 3**.: _For any timestep \(t\) and arm \(i\), it holds_

\[_{t}[_{t,i}-_{t,i}]=_{t,i}(_{t,i}-c_{ t})\,,_{t}[(_{t,i}-_{t,i})^{2}] 2_{t,i}\,,\]

_where \(c_{t}[-1,1]\) is an arm independent constant._

In Section 3.1 we argued that one needs to bound the additional term \(^{-1}(D_{LB}(^{},_{t+1})-D_{LB}(^{},_{t+1}^{}}))\) to reduce the analysis to standard OMD. While this term is nicely bounded for LB-Prod, it turns out that it is easier to directly bound the "prototype" of the _stability_ term in Equation (2) due to the fact that we have a closed form expression of \(_{t+1}\).

**Lemma 4**.: _For any time \(t[T]\) and any \(u([K])\), it holds_

\[_{t}-u,_{t}+_{t}[^{-1}D_{LB}( u,_{t+1})]-^{-1}D_{LB}(u,_{t})\,.\]

The proof is an algebraic exercise and deferred to the supplementary material. Finally, we can prove the main regret guarantee.

Proof of Theorem 2.: To show that this algorithm outputs proper probability distributions, note that

\[_{i=1}^{K}_{t+1,i}=(_{i=1}^{K}_{t,i})-_{t,A_{t }}_{t,A_{t}}+_{j=1}^{K}}_{t,j}^{2}}{_{k=1 }^{K}_{t,k}^{2}}_{t,A_{t}}=_{i=1}^{K}_{t,i}==_{i=1}^{K }_{1,i}=1\,.\]

Additionally we have seen that \(|_{t,i}-_{t,i}| 1\), hence for any \(<1\), the probability of any arm is strictly positive. For any comparator \(u^{}\), we define \(u=u^{}+(_{1}-u^{})\), which satisfies \(_{t=1}^{T}(u-u^{},_{t}) 2\). Using Lemma 4 and Equation (2), we obtain by the telescoping sum of Bregman terms

\[[_{t=1}^{T}_{t}-u,_{t} ]+^{-1}\,[D_{LB}(u,_{1} )-D_{LB}(u,_{T+1})]+\,.\]

### The perturbation analysis

We outline an alternative analysis that reuses established machinery and might be more accessible for some readers. Our analysis begins by viewing the Prod update as an exact OMD update over a perturbed loss sequence. Indeed, there is a sequence of perturbations \(\{_{t}\}_{t[T]},_{t}^{K}\), such that

\[_{t+1}^{}=}{1+_{t,i}(_{t,i}- _{t,i}-_{t})}=_{t,i}(1-_{t,i}(_{t,i}- _{t}))=_{t+1,i}\,.\]

The exact form of \(_{t,i}\) satisfies the following

\[_{t,i}=(_{t,i}-_{t})^{2}}{1+ _{t,i}(_{t,i}-_{t})}\,,|\,_{t}[_{ t,i}]|=O()\,.\]

Since Prod is exactly OMD over the sequence \(_{t}-_{t}\), we can decompose the regret as follows

\[[_{t=1}_{t}-u,_{t} ]=[_{t=1}_{t}-u,_{t}- _{t}]+[_{t=1}_{t }-u,_{t}]=_{}+O( T )\,.\]

The analysis is not entirely straightforward as the loss range for the OMD update becomes \([-1-O(),1+O()]\) because of the shift introduced by the perturbation of the losses, and this posses some additional difficulties.

## 5 Best of both worlds algorithms

In applications where the loss is potentially more benign, for example sampled i.i.d. from a a fixed distribution over \(^{K}\), it is desirable to obtain faster rates in nice environments while preserving worst-case guarantees. Probably the simplest algorithm with this property is Tsallis-INF (Zimmer and Seldin, 2021), which is FTRL with 1/2-Tsallis entropy and \(_{t} 1/\) learning rate.

### TS-Prod

Recall the 1/2-Tsallis regularizer is \(F(x)=-_{i=1}^{K}2}\). Unlike OMD, FTRL is not canonically expressed as a 1-step update of the previous policy. Instead, the 1/2-Tsallis-INF policy is given with a normalization constant \(_{t}\) (see (Zimmer and Seldin, 2021))

\[_{t+1,i}^{}=(_{t+1}+_{t}_{s=1}^{t}_{s,i})^{-2}\,.\]Recursively using this expression yields

\[_{t+1,i}^{}}=(_{t+1}+}{_{t-1}}( ^{}}}}-_{t})+_{t}_{t })^{-2}=_{t,i}^{}}(1+_{t}^{ }}}(_{t}-_{t}}{^{ }}}}-_{t}))^{-2}\,,\]

where \(_{t}=^{2}}-_{-1}}\) and \(_{t}=_{t+1}-}{_{t-1}}_{t}\). The first order approximation is

\[_{t+1,i}^{}}^{}}}_{ }}_{t,i}^{}}(1-2_{t}^{}}}(_{t,i}-_{t}/^{}}} -_{t}))\,.\]

We ensure following the approximation in expectation by directly biasing the losses with \(_{t}_{t}/}\). Additionally, one needs to perform a second-order correction as discussed in Section 3.1. We omit a formal derivation, but notice that WSU-UX required \(/_{t,i}\) correction, while LB-Prod works without correction because the error is of order \(\). As the intermediate potential between KL and Logbarrier, Tsallis-INF turns out to require a correction of order \(/}\), which we tighten by an additional factor of \((1-_{t,i})\) necessary to ensure stochastic bounds.

With this, we are ready to present

\[_{t,i}=(_{t,i}-(_{t}+( 1-_{t,i})}{}})(A_{t}=i)}{_{t,i}}\,, _{t}=_{i=1}^{K}^{}_{t,i}} {_{j=1}^{K}^{}}}\,,_{t}=^{2}}-_{t-1}}\,,\] \[_{t+1,i}=_{t,i}(1-2_{t}}(_{t,i }-_{t}))\,.\] (TS-Prod)

**Theorem 3**.: _The regret of TS-Prod with \(_{t}=}\), \(=\) is bounded by \(O(+K(T))\) in the adversarial setting and by \(O(_{i i^{*}}})\) in the stochastic setting._

### Analysis of TS-Prod

We first show that the loss biasing is sufficient to ensure that the distribution is well defined.

**Lemma 5**.: _If \(_{t}\) is a non-increasing sequence, \(_{t}<+)^{2}}}\) and \(_{t+1}^{2}_{t}^{2}(1-2_{t}^{2})\) for all \(t\), then the update rule of TS-Prod is proper and satisfies \(_{t,i}>(_{t}+)^{2}_{t}^{2}\) for any arm and loss sequence at all time steps._

Next we present the moving parts of the analysis.

\[[_{t=1}^{T}_{t}-u,_{t}]=_{ t=1}^{T} [-u,_{t}-_{t}}_{ }+(u,_{t})-D_{TS}(u,_{t+1})}{ _{t}}}_{}..\] \[..+-u,_{t} }_{}-(u,_{t})-D_{TS}(u, _{t+1})}_{}]\]

The change of measure is by construction

\[=_{t=1}^{T}[_{i=1}^{K}-u_{i}}{}}(_{t}_{t}+_{t}(1-_{t,i}) )]\,.\] (3)

We now bound the stability and penalty.

**Lemma 6**.: _For any time \(t\) such that \(_{t,i}>(_{t}+)^{2}_{t}^{2}\), it holds_

\[_{t}-u,_{t}[_{t}]+_{t }[(u,_{t+1})}{_{t}}]-(u,_{t})}{ _{t}}u_{i}}{}}(1-_{ti})\,.\]

_The tuning of Theorem 3 satisfies the conditions._

**Lemma 7**.: _The proto-penalty is bounded by_

\[_{t=1}^{T}(u,_{t})-D_{TS}(u,_{t+1})}{_{t}}_{t= 1}^{T}_{t}_{t}(_{i i^{*}}2}+_{i=1}^{K} -_{ti}}{}})\]

We are ready to prove the main regret guarantee.

Proof of Theorem 3.: By Lemma 5 we have a proper update rule. Using Lemma 6, equation (3), where we tuned \(=\) and Lemma 7 yields

\[[_{t=1}^{T}_{t}-u,_{t} ][_{t=1}^{T}_{t}(_{i=1}^{K} {2}}(1-_{ti})+_{i i^{*}}_{t}}) ]\,.\]

The adversarial regret follows from \(_{i=1}^{K}}\), \(_{t} 4\), \(_{t=1}^{T}_{t}=O()\). For the stochastic regret, the proof follows standard arguments using the self-bounding trick as in Zimmert and Seldin (2021). For details on the self-bounding trick see the supplementary material. 

### TS-Prod and stabilized OMD

Even though we derived TS-Prod from FTRL, it turns out that one can also interpret the update as an approximation of _stabilized_ OMD proposed by Fang et al. (2022). In Appendix E, we formalize this connection and present a slight variation of TS-Prod. We then analyse this variant via the perturbation technique described in Section 4.3. Our analysis also shows that the stabilized OMD algorithm induced by the \(1/2\)-Tsallis entropy enjoys best-of-both worlds regret guarantees which to the best of our knowledge is novel.

## 6 Discussion

We have provided an extensive study of incentive-compatible bandits. We have negatively resolved an open question of whether incentive-compatibility as defined in Freeman et al. (2020) is harder than regular bandits. Using linear approximations, partly with second order corrections, allows to recover results from well studied algorithms in the literature. We even obtain an algorithm with best-of-both-world guarantees. Our algorithms are conceptually simpler than existing bandit algorithms, they update the probability distributions with basic arithmetic operations without the need to solve optimization problems.

Our successes make it likely that one can transfer even more sophisticated methods, such as first-order, second-order, path-norm bounds and online learning with graph feedback to this framework. We leave this investigation to future work.