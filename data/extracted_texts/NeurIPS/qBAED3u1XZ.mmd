# VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models

Ziyi Yin\({}^{1}\) Muchao Ye\({}^{1}\) Tianrong Zhang\({}^{1}\) Tianyu Du\({}^{2}\)

Jinguo Zhu\({}^{3}\) Han Liu\({}^{4}\) Jinghui Chen\({}^{1}\) Ting Wang\({}^{5}\) Fenglong Ma\({}^{1}\)

\({}^{1}\)The Pennsylvania State University, \({}^{2}\)Zhejiang University,

\({}^{3}\) Xi'an Jiaotong University, \({}^{4}\)Dalian University of Technology, \({}^{5}\)Stony Brook University

{ziyiyiyin, muchao, tbz5156, jcz5917, fenglong}@psu.edu

zjradty@zju.edu.cn, lechatelia@stu.xjtu.edu.cn

liu.han.dut@gmail.com, twang@cs.stonybrook.edu

Corresponding author.

###### Abstract

Vision-Language (VL) pre-trained models have shown their superiority on many multimodal tasks. However, the adversarial robustness of such models has not been fully explored. Existing approaches mainly focus on exploring the adversarial robustness under the white-box setting, which is unrealistic. In this paper, we aim to investigate a new yet practical task to craft image and text perturbations using pre-trained VL models to attack black-box fine-tuned models on different downstream tasks. Towards this end, we propose VLAttack2 to generate adversarial samples by fusing perturbations of images and texts from both single-modal and multimodal levels. At the single-modal level, we propose a new block-wise similarity attack (BSA) strategy to learn image perturbations for disrupting universal representations. Besides, we adopt an existing text attack strategy to generate text perturbations independent of the image-modal attack. At the multimodal level, we design a novel iterative cross-search attack (ICSA) method to update adversarial image-text pairs periodically, starting with the outputs from the single-modal level. We conduct extensive experiments to attack five widely-used VL pre-trained models for six tasks. Experimental results show that VLAttack achieves the highest attack success rates on all tasks compared with state-of-the-art baselines, which reveals a blind spot in the deployment of pre-trained VL models.

## 1 Introduction

The recent success of vision-language (VL) pre-trained models on multimodal tasks have attracted broad attention from both academics and industry . These models first learn multimodal interactions by pre-training on the large-scale unlabeled image-text datasets and are later fine-tuned with labeled pairs on different downstream VL tasks . In many cases, these pre-trained models have revealed more powerful cross-task learning capabilities compared to training from scratch . Despite their remarkable performance, the **adversarial robustness** of these VL models is still relatively unexplored.

Existing work  conducting adversarial attacks in VL tasks is mainly under the **white-box** setting, where the gradient information of fine-tuned models is accessible to attackers. However, in a more realistic scenario, a malicious attacker may only be able to access the public pre-trained models released through third parties. The attacker would not have any prior knowledge about theparameters learned by downstream VL models fine-tuned on private datasets. Towards bridging this striking limitation, we investigate a new yet practical attack paradigm - _generating adversarial perturbations on a pre-trained VL model to attack various **black-box** downstream tasks fine-tuned on the pre-trained one._

However, such an attack setting is non-trivial and faces the following challenges: (1) **Task-specific challenge**. The pre-trained VL models are usually used for fine-tuning different downstream tasks, which requires the designed attack mechanism to be general and work for attacking multiple tasks. As illustrated in Figure 1, the attacked tasks are not only limited to close-set problems, such as visual question answering, but also generalized to open-ended questions, such as visual grounding. (2) **Model-specific challenge**. Since the parameters of the fine-tuned models are unknown, it requires the attack method to automatically learn the adversarial transferability between pre-trained and fine-tuned models on different modalities. Although the adversarial transferability [17; 18; 19; 20] across image models has been widely discussed, it is still largely unexplored in the pre-trained models, especially for constructing mutual connections between perturbations on different modalities.

To address all the aforementioned challenges, we propose a new yet general Vision-Language Attack strategy (named VLAttack) to explore the adversarial transferability between pre-trained and fine-tuned VL models, as shown in Figure 2. The whole VLAttack scheme fuses perturbations of images and texts from two levels:

**Single-modal Level**. VLAttack independently generates perturbations on a single modality, following a "_from image to text_" order as the former can be perturbed on a continuous space. The single-modal attack can effectively detect the adversarial vulnerability of an image or text, and hence avoids redundant perturbations on the other modality. Specifically, to fully utilize the image-text interactions that have been stored in the pre-trained model, we propose a novel **block-wise similarity attack** (BSA) strategy to attack the _image modality_, which adds perturbations to enlarge the network block-wise distance between original and perturbed features in the pretrained model, disrupting the universal image-text representations for the downstream predictions. If BSA fails to change the prediction after querying the fine-tuned black-box model, VLAttack will attack the text modality by employing the word-level perturbation techniques [21; 22; 23; 24]. We adopt BERT-Attack  to attack the _text modality_ as its prominent performance has been widely verified in many studies [25; 26; 27]. Finally, if all the text perturbations \(\{^{}_{i}\}\) fail, VLAttack will generate a list of perturbed samples \(\) and feed them to the multimodal attack along with the perturbed image \(^{}\).

**Multimodal Level**. If the above attack fails to change the predictions, we cross-update image and text perturbations at the multimodal level based on previous outputs. The proposed **iterative cross-search attack** (ICSA) strategy updates the image-text perturbation pair \((^{}_{i},^{}_{i})\) in an iterative way

Figure 1: An illustration of the problem of attacking block-box downstream tasks using pre-trained vision-language models.

Figure 2: A brief illustration of VLAttack.

by considering the mutual relations between different modal perturbations. ICSA uses a text perturbation \(^{}_{i}\) selected from the list \(\) as the guidance to iteratively update the perturbed image \(^{}_{i}\) by employing the block-wise similarity attack (BSA) until the new pair \((^{}_{i},^{}_{i})\) makes the prediction of the downstream task change. In addition, text perturbations are cross-searched according to the semantic similarity with the benign one at the multimodal attack level, which gradually increases the extent of the direction modification to preserve the original semantics to the greatest extent.

Our **contributions** are summarized as follows: (1) To the best of our knowledge, we are the first to explore the adversarial vulnerability across pre-trained and fine-tuned VL models. (2) We propose VLAttack to search adversarial samples from different levels. For the single-modal level, we propose the BSA strategy to unify the perturbation optimization targets on various downstream tasks. For the multimodal level, we design the ICSA to generate adversarial image-text pairs by cross-searching perturbations on different modalities. (3) To demonstrate the generalization ability of VLAttack, we evaluate the proposed VLAttack on **five widely-used VL models**, including BLIP , CLIP , ViLT , OFA  and UniTAB  for **six tasks**: \((i)\) VQA, \((ii)\) visual entailment, \((iii)\) visual reasoning, \((iv)\) referring expression comprehension, \((v)\) image captioning, and \((vi)\) image classification. Experimental results demonstrate that VLAttack outperforms both single-modal and multimodal attack approaches, which reveals a significant blind spot in the robustness of large-scale VL models.

## 2 Related Work

**Single-modal Adversarial Attack Methods**. _Image Attack._ Traditional image attack methods [30; 31] generate adversarial samples by optimizing the loss function with regard to the decision boundary from model outputs. Not only do the generated perturbations change the model predictions, but they can also be transferred to other convolutional neural network (CNN) structures. Such a property is called transferability and has been extensively studied. For example, data augmentation-based methods [32; 33] endeavor to create diverse input patterns to enhance representation diversity across different models. Feature disruptive attacks [17; 34; 35] introduce the intermediate loss to change the local activation of image features output from the middle layers of CNNs, enabling the perturbed features to transfer to different models without knowing their structures and parameters. _Text Attack._ Adversarial attacks on natural language processing (NLP) tasks mainly concentrate on word-level and sentence-level perturbations. Word-level perturbations [21; 22; 23; 24] substitute words with synonyms that share similar word embeddings and contextual information. Sentence-level perturbations [25; 36; 37] focus on logical structures of texts through paraphrasing or adding unrelated sentence segments. All of these methods have revealed the adversarial vulnerability of traditional NLP models to some extent. However, adapting them to a multimodal attack setting is still underexplored.

**Multimodal Adversarial Attack Methods**. Multimodal VL models are susceptible to adversarial attacks as perturbations can be added to both modalities. Existing methods mainly explore adversarial robustness on a specific VL task. For the visual question answering task, Fool-VQA  is proposed, which iteratively adds pixel-level perturbations on images to achieve the attack. For the image-text retrieval task, CMLA  and AACH  add perturbations to enlarge Hamming distance between image and text hash codes, which causes wrong image-text matching results. Recently, Co-attack  is proposed, which combines image and text perturbations using word substitution attacks to ascertain a direction for guiding the multi-step attack on images. The whole framework is deployed in a white-box setting, where attackers are assumed to have access to the parameters of each downstream task model. Besides, Co-attack is only validated on three tasks, which is not general enough.

## 3 Preliminaries

**VL Pre-trained Model Structure**. Fine-tuning from pre-training has become a unified paradigm in the recent VL model design. Most pre-trained VL models can be divided into _encoder-only_[1; 39; 40; 41] and _encoder-decoder_[4; 5; 6] structures. Two representative model structures are shown in Figure 3. Given an image-text pair \((,)\), both VL models first embed each modality separately. Image tokens are obtained through an image encoder composed of a vision transformer  or a Resnet  after flattening the grid features into a sequence. Text tokens are generated through a word encoder made up of a tokenizer  and a word vector projection. The **encoder-only** model then attends tokens of two modalities and a learnable special token \( cls\) and feeds them into a Transformer encoder. Finally, the output representation from the \( cls\) token is fed into a classification head for the final prediction \( ans\). For the **encoder-decoder** structure, the attached image and text tokens are fed into a Transformer network  to generate sequence predictions \([ ans_{1}, ans_{2},, end]\) in an auto-regressive manner. The network stops regressing when an end token \( end\) appears. In this work, we deeply explore the adversarial vulnerability of both structures.

**Threat Model**. Let \(F\) denote the public pre-trained model and \(S\) represent the downstream task model, where \(S\) shares most of or the same structure with \(F\). As shown in Figure 4, the network structures of both types of public VL pre-trained models are different. The encoder-only model allows modifying the last prediction layer based on the requirement of downstream tasks but keeping the other layers the same as \(F\). However, the encoder-decoder model unifies the outputs of different downstream tasks, which leads to \(S\) having the same structure as \(F\). Note that the downstream tasks will fully fine-tune parameters using their own data, and hence, all the model parameters in \(F\) are updated in the fine-tuning stage. Thus, all parameters in \(S\) are not accessible.

Given an image-text pair \((,)\), the goal of the downstream task is to predict the labels of the input pair accurately, i.e., \(S:(,)\), where \(=\{y_{1},,y_{n}\}\). For the encoder-only models, the ground truth label is a one-hot vector. For the encoder-decoder models, the ground truth is a sequence that consists of multiple ordered one-hot vectors. Let \(\) denote the ground truth vector. The goal of the adversarial attack is to generate adversarial examples \((^{},^{})\) using \(F\), which can cause an incorrect prediction on \(S\). Mathematically, our problem is formulated as follows:

\[_{^{},^{}}\{S(^{ },^{})\}, s.t.\ \|^{}-\|_{}<_{i},\ \ Cos(U_{s}(^{}),U_{s}())> _{s},\] (1)

where \(_{i}\) is the \(l_{}\)-norm perturbation strength on the image. \(_{s}\) is the semantic similarity between the original and perturbed texts, which constrains the semantic consistency after perturbation. The semantic similarity is measured by the cosine similarity \(Cos(,)\) between the sentence embedding \(U_{s}(^{})\) and \(U_{s}()\) using the Universal Sentence Encoder \(U_{s}\).

## 4 Methodology

As shown in Figure 2, the proposed VLAttack generates adversarial samples from two steps, constrained by perturbation budget parameters \(_{i}\) and \(_{s}\). The first step attacks every single modality independently to avoid unnecessary modifications. Samples that fail at the first step will be fed into our multimodal level, where we adopt a cross-search attack strategy to iteratively refine both image and text perturbations at the same time. Next, we will introduce the details of VLAttack from the single-modal level to the multimodal level.

### Single-modal Attacks: From Image to Text

VLAttack attempts to generate adversarial samples on every single modality in the first step. Compared to the discrete words in \(\), the continuous values-based image \(\) is more vulnerable to attack by using gradient information3. Thus, VLAttack starts by attacking the image \(\).

**Image-Attack.** In the black-box setting, the parameters of the fine-tuned model \(S\) are unknown. However, the pre-trained model \(F\) is accessible. Intuitively, if the feature representations learned by the pre-trained model \(F\) from the clean input \(\) and the perturbed input \(}\) are significantly different, such a perturbation may _transfer_ to fine-tuned models to change the predictions of downstream tasks. As shown in Figure 3, the image features can be obtained from both the image encoder and the Transformer encoder, even from different layers or blocks. To fully leverage the specific characteristics of the pre-trained model structure, we propose the **block-wise similarity attack** (BSA) to corrupt universal contextualized representations.

As shown in Figure 4, BSA perturbs images by maximizing the block-wise distances between the intermediate representations in the image encoder \(_{}\) and Transformer encoder \(_{}\) of the pre-trained model \(F\). Mathematically, we define the loss function of BSA as follows:

\[=^{M_{i}}_{j=1}^{M_{j}^{}}Cos( _{}^{i,j}(),\ _{}^{i,j}(^{{}^{}}}))}_{}+^{M_{k}}_{t=1}^{M_{t}^{k}}Cos(_{ }^{k,t}(,),\ _{}^{k,t}(^{{}^{}}, ))}_{},\] (2)

where \(M_{i}\) is the number of blocks in the image encoder, and \(M_{j}^{i}\) is the number of flattened image feature embeddings generated in the \(i\)-th block4. Similarly, \(M_{k}\) is the number of blocks in the Transformer encoder, and \(M_{t}^{k}\) is the number of image token features generated in the \(k\)-th block. \(_{}^{i,j}\) is the \(j\)-th feature vector obtained in the \(i\)-th layer of the image encoder, and \(_{}^{k,t}\) is the \(t\)-th feature vector obtained in the \(k\)-th layer of the Transformer encoder. The image encoder only takes a single image \(\) or \(^{}\) as the input, but the Transformer encoder will use both image and text as the input. We adopt the cosine similarity to calculate the distances between perturbed and benign features as token representations attended with each other in the inner product space . Note that BSA _does not rely on the information from the decision boundary, and thus, it can be easily adapted to different task settings by disrupting the benign representations._

In the image attack step, we generate an adversarial image candidate through project gradient decent optimization  with \(N_{s}\) iterations, where \(N_{s}<N\) and \(N\) is the maximum number of iterations on the image attack. The remaining \(N-N_{s}\) attack step budgets will be used in the multimodal attack in Section 4.2. If \((^{},)\) is an adversarial sample generated by BSA, then VLAttack will stop. Otherwise, VLAttack moves to attack the text modality.

**Text-Attack.** In some VL tasks, the number of tokens in the text is quite small. For example, the average length of the text in the VQAv2  and RefCOCO  datasets is 6.21 and 3.57, respectively5. Moreover, some of them are nonsense words, which makes it unnecessary to design a new approach for attacking the text modality. Furthermore, existing text attack approaches such as BERT-Attack  are powerful for generating adversarial samples for texts. Therefore, we directly apply BERT-Attack to generate text perturbations. To avoid unnecessary modifications, we use the clean image \(\) as the input instead of the generation perturbed image \(^{}\).

Specifically, an adversarial candidate \(_{i}^{}\) produced by BERT-Attack is firstly fed into a universal sentence encoder \(U_{s}\) with the benign text \(\) to test the semantic similarity. \(_{i}^{{}^{}}\) will be removed if the cosine similarity \(_{i}\) between \(_{i}^{}\) and \(\) is smaller than the threshold \(_{s}\), i.e., \(_{i}=Cos(U_{s}(_{i}^{}),U_{s}())<_{s}\). Otherwise, we put \(_{i}^{}\) and the benign image \(\) into the fine-tuned model \(S\) to detect whether the new pair \((,_{i}^{})\) perturbs the original prediction \(\). During the text attack, we create a list \(\) to store all perturbed samples \(\{_{i}^{}\}\) and the corresponding cosine similarity values \(\{_{i}\}\). If any sample in \(\) successfully changes the prediction of the downstream task, then VLAttack will stop. Otherwise, VLAttack will carry the perturbed image \(^{}\) from the image attack and the perturbed text candidate list \(\) from the text attack to the multimodal attack level. Note that BERT-Attack is based on synonym word substitution. Even for a short text, the number of adversarial examples can still be large. In other words, the length of \(\) may be large. The whole single-modal attack process is shown in Algorithm 1 (lines 1-15).

### Multimodal Attack

In many cases, only perturbing images or texts is hard to succeed, as a single-modality perturbation is insufficient to break down the image-text correlation. To solve this problem, we propose a new attack strategy to disrupt dynamic mutual connections based on perturbations from different modalities.

**Attack Steps**. Since the maximum number of image attack steps \(N\) is predefined, it is impossible to test all the perturbed image-text pairs \((^{},^{}_{i})\) using the remaining budget \(N-N_{s}\) if the length of \(\) (i.e., \(||\)) is very large. Thus, we need to rank the perturbed text samples \(\{^{{}^{}}_{i}\}\) according to their corresponding cosine similarity values \(\{_{i}\}\) in a descending order to make the adversarial sample keep the high semantic similarity with the original text \(\). Let \(K\) denote the number of attack steps in the multimodal attack, and we have:

\[K=N-N_{s},&||>N-N_{s};\\ ||,&|| N-N_{s}.\] (3)

**Iterative Cross-Search Attack**. A naive way to conduct the multimodal attack is to test each perturbed image-text pair \((^{},}^{}_{k})\) (\(k=1,,K\)) by querying the black-box fine-tuned model \(S\), where \(}^{}_{i}\) is the \(i\)-th text perturbation in the ranked list \(\). However, this simple approach ignores learning mutual connections between the perturbed text and image. To solve this issue, we propose a new _iterative cross-search attack_ (ICSA) strategy. In ICSA, VLAttack will dynamically update the image perturbation under the guidance of the text perturbation.

Specifically, in each multimodal attack step, ICSA first determines the number of image attack steps. Since there are \(K\) adversarial text candidates that will be equally used in ICSA, the iteration number of image attacks that will be allocated to each text sample is \(N_{k}=}{K}\). ICSA will take the \(k\)-th pair \((^{}_{k},}^{}_{k})\) as the input to generate the new image perturbation \(^{}_{k+1}\) by optimizing the block-wise attack loss in Eq. (2), where \(^{}_{1}\) is the output from the image attack in the single-modal level, i.e., \(^{}\). Such a process is repeated until an adversarial sample \((^{{}^{}}_{j},}^{}_{j})\) is found, or all the \(K\) perturbed textsare visited. When \(=\), VLAttack will degenerate to the block-wise similarity attack (BSA) and iteratively updates \(^{{}^{}}\) for \(N-N_{s}\) steps. Finally, VLAttack generates adversarial samples based on either single-modality or cross-modality attacks. The overall scheme of VLAttack is summarized in Algorithm 1.

## 5 Experiments

### Experiment Setup

**Pre-trained VL Models and Tasks.** Experiments are conducted on five pre-trained models For the **encoder-only** model, we adopt ViLT  and BLIP  for two downstream tasks, including the visual question answering (VQA) task on the VQAv2 dataset  and the visual reasoning (VR) task on the NLVR2 dataset . For the **encoder-decoder** structure, we adopt Unitab  and OFA . For Unitab, evaluations are made on the VQAv2 dataset for the VQA task and on RefCOCO, RefCOCO+, and RefCOCOg datasets  for the Referring Expression Comprehension (REC) task that can be viewed as the task of bounding box generation. For OFA, we implement experiments on the same tasks as Unitab and add the SNLI-VE dataset  for the visual entailment (VE) task. The specific structures of these models are detailed in Appendix A. To verify the overall generality, we evaluate the uni-modal tasks on OFA  using MSCOCO  for the image captioning task and ImageNet-1K  for the image classification task. We also evaluate CLIP  on the image classification task on the SVHN  dataset. Note that all evaluation tasks have publicly available pre-trained and fine-tuned models, which provide more robust reproducibility. The details of each dataset and implementation can be found in Appendix B.

**Baselines**. We compare VLAttack with adversarial attacks on different modalities. For attacks on the **image** modality, we take DR , SSP , and FDA  as baselines. These methods are designed to perturb image features only and can be directly adapted to our problem. Other methods [18; 19; 30; 32; 33; 55] either fully rely on the output from classifiers or combine feature perturbation with classification loss [20; 56; 57]. These methods can not be applied in our problem setting since the pre-trained and fine-tuned models usually share different prediction heads and are trained on different tasks. For attacks on the **text** modality, we take BERT-Attack (B&A)  and R\(\&\)R  as baselines. VLAttack also compares with Co-Attack , which is the only **multimodal** attack scheme that adds adversarial perturbations to both modalities.

### Results on Multimodal Tasks

In this section, we conduct experiments on four pre-trained VL models and use **Attack Success Rate** (ASR) to evaluate the performance on four multimodal tasks, including VQA, VR, REC, and VE. The higher the ASR, the better the performance. Results are illustrated in Table 1, where the results of our proposed BSA and VLAttack are highlighted. We can observe that the proposed VLAttack significantly outperforms all baselines. Compared with the best baseline on each dataset,

   Pre-trained \\ Model \\  } &  &  &  &  &  \\   & & & DR & SSP & FDA & BSA & B\&A & R\&R & Co-Attack & VLAttack \\   & VQA & VQAv2 & 23.89 & 50.36 & 29.27 & 65.20 & 17.24 & 8.69 & 35.13 & **78.05** \\  & VR & NLVR2 & 21.58 & 35.13 & 22.60 & 52.17 & 32.18 & 24.82 & 42.04 & **66.65** \\    & VQA & VQAv2 & 7.04 & 11.84 & 7.12 & 25.04 & 21.04 & 2.94 & 14.24 & **48.78** \\  & VR & NLVR2 & 6.66 & 6.88 & 10.22 & 71.6 & 33.08 & 16.92 & 8.70 & **52.66** \\    & VQA & VQAv2 & 22.88 & 33.67 & 41.08 & 48.40 & 14.20 & 5.48 & 33.87 & **62.20** \\  & REC & RefCOCO & 21.32 & 64.56 & 75.24 & 89.70 & 13.68 & 8.75 & 56.48 & **93.52** \\  & REC & RefCOCO+ & 26.30 & 69.60 & 76.21 & 90.96 & 6.40 & 2.46 & 68.69 & **93.40** \\  & REC & RefCOCOg & 26.39 & 69.26 & 78.64 & 91.31 & 22.03 & 18.52 & 65.50 & **95.61** \\    & VQA & VQAv2 & 25.06 & 33.88 & 40.02 & 54.05 & 10.22 & 2.34 & 51.16 & **78.82** \\  & VE & SNLI-VE & 13.71 & 15.11 & 20.90 & 29.19 & 10.51 & 4.92 & 18.66 & **41.78** \\   & REC & RefCOCO & 11.60 & 16.00 & 27.06 & 40.82 & 13.15 & 7.64 & 32.04 & **56.62** \\   & REC & RefCOCO+ & 16.58 & 22.28 & 33.26 & 46.44 & 4.66 & 7.04 & 45.28 & **58.14** \\   & REC & RefCOCOg & 16.39 & 24.80 & 33.22 & 54.63 & 19.23 & 15.13 & 30.53 & **73.30** \\  

Table 1: Comparison of VLAttack with baselines on ViLT, Unitab, and OFA for different tasks, respectively. All results are displayed by ASR (\(\%\)). B&A means the BERT-Attack approach.

[MISSING_PAGE_FAIL:8]

the MTurk system. We provide three choice candidates to workers, including "Definitely Correct", "Not Sure", and "Definitely Wrong". A successful attack means that the worker will label "Definitely Wrong" to the pair. To make the annotations more accurate and reliable, each pair is annotated by three workers, and we report the majority choice as the final human evaluation result. The statistics of human evaluation results are shown in Table 4. We can observe that the proposed VLAttack still significantly outperforms the strongest baseline SSP, thus demonstrating its robustness and effectiveness from a human perceptual perspective.

### Model Design Analysis

**Ablation Study.** In our model design, we propose a new block-wise similarity attack (BSA) for attacking the image modality and an interactive cross-search attack (ICSA) for attacking image-text modalities together. We use the following methods to evaluate the effectiveness of each component. The results are shown in Figure 5. "IE"/~TE" means that we only use the image/Transformer encoder when calculating the loss in Eq. (2). "BSA" uses both encoders. We set iteration \(N_{s}=40\) for a fair comparison. Next, "BSA+BA" means after attacking images using BSA, we attack texts using BERT-Attack (Algorithm 1 Lines 1-15). "BSA+BA+Q" denotes replacing ICSA with a simple query strategy by querying the black-box task with each pair (\(^{},^{}_{i}\)), where \(^{}_{i}\) comes from the perturbed text candidate list \(\). We can observe that for image attacks, IE and TE play an important role in different tasks, but in general, BSA (= IE + TE) outperforms them. Adding the text attack, BSA+BA performs better than BSA. This comparison result demonstrates that both modalities are vulnerable. BSA+BA+Q performs better than BSA+BA but worse than VLAttack, which confirms the necessity and reasonableness of conducting the interactive cross-search attack. More results on other datasets are shown in Appendix C.

**Parameter Sensitivity Analysis** We discuss the effect of different iteration numbers of \(N\) and \(N_{s}\) in VLAttack. All experiments are conducted on the VQAv2 dataset and the ViLT model. The total iteration number \(N\) is set from 10 to 80, \(N_{s}\) is set to \(\). As depicted in Figure 6(a), the ASR performance is dramatically improved by increasing \(N\) from 10 to 20 steps and then achieves the best result when \(N=40\). We next investigate the impact of different initial iteration numbers \(N_{s}\). We test \(N_{s}\) from 5 to 40, but the total iteration number \(N\) is fixed to 40. As shown in Figure 6(b), the ASR score reaches the summit when \(N_{s}\) is 5, and it is smoothly decreased by continually enlarging \(N_{s}\). Considering that the smaller initial iteration number \(N_{s}\) increases the ratio of text perturbations, we set \(N_{s}\) as 20 to obtain the best trade-off between attack performance and the naturalness of generated adversarial samples in our experiments.

**Block-wise Similarity Attack**. We further visualize the effect of our proposed BSA through attention maps in the fine-tuned downstream model \(S\) on the VQAv2 dataset through the ViLT model. As is shown in Figure 7, the attention area is shifted from "_jacket_" to the background, which indicates that the representation used to answer the question extracts information from the unrelated image regions. We also combined the BSA with various optimization methods to verify its generalization ability. The experimental results are presented in Appendix D.

Figure 5: Ablation analysis of different components in VLAttack. We show the results of VQAv2 (a) and SNLI-VE (b) on OFA, and VQAv2 (c) and RefCOCO (d) on Unitab.

Figure 6: Investigation of iteration number \(N\) and \(N_{s}\).

### Case Study

We conduct case studies using visualization to show the effectiveness of VLAttack for multimodal and uni-modal tasks, as displayed in Figure 8. For multimodal tasks in Figure 8(a), the predictions of all displayed samples remain unchanged when attacking a single modality via BSA. However, perturbing both modalities using VLAttack successfully modifies the predictions. An intriguing finding can be observed in the REC task, where the fine-tuned model \(S\) outputs wrong predictions by stretching and deforming the original bounding box predictions. The proposed VLAttack also shows its power on uni-modal tasks in Figure 8(b). For example, in the image captioning task, the fine-tuned model generates a description of a "_person_" after receiving an adversarial image of a "_horse_", which is entirely unrelated. More case studies are shown in Appendix E.

## 6 Conclusion

In this paper, we propose a new question, which aims to generate adversarial samples only based on the publicly available pre-trained models to attack fine-tuned models deployed in different VL tasks. Considering the task-specific and model-specific challenges, we proposed VLAttack, which generates perturbations by exploring different modalities in two levels. The single-modal level attack first perturbs images through a novel algorithm BSA to disrupt the universal image-text representations and then attacks text if the former fails, which avoids unnecessary modifications on both modalities. If both image and text attacks fail, the multimodal attack level adopts an iterative cross-search attack strategy to generate adversarial image-text combinations. By periodically substituting text candidates during the image attack process, the mutual relations between different modal perturbations are sufficiently explored. Experimental results demonstrate the effectiveness of the proposed VLAttack on attacking multiple VL tasks, which reveals a significant safety concern in realistic scenarios.