# Calibrating Neural Simulation-Based Inference with Differentiable Coverage Probability

Maciej Falkiewicz\({}^{1,2}\) Naoya Takeishi\({}^{3,4}\) Imahn Shekhzadeh\({}^{1,2}\) Antoine Wehenkel\({}^{5}\)

Arnaud Delaunoy\({}^{5}\) Gilles Louppe\({}^{5}\) Alexandros Kalousis\({}^{2}\)

\({}^{1}\)Computer Science Department, University of Geneva \({}^{2}\)HES-SO/HEG Geneve

\({}^{3}\)The University of Tokyo \({}^{4}\)RIKEN \({}^{5}\)University of Liege

{maciej.falkiewicz, imahn.shekhzadeh, alexandros.kalousis}@hege.ch

ntake@g.ecc.u-tokyo.ac.jp

{antoine.wehenkel, a.delaunoy, g.louppe}@uliege.be

###### Abstract

Bayesian inference allows expressing the uncertainty of posterior belief under a probabilistic model given prior information and the likelihood of the evidence. Predominantly, the likelihood function is only implicitly established by a simulator posing the need for simulation-based inference (SBI). However, the existing algorithms can yield overconfident posteriors (Hermans _et al._, 2022) defeating the whole purpose of credibility if the uncertainty quantification is inaccurate. We propose to include a calibration term directly into the training objective of the neural model in selected amortized SBI techniques. By introducing a relaxation of the classical formulation of calibration error we enable end-to-end backpropagation. The proposed method is not tied to any particular neural model and brings moderate computational overhead compared to the profits it introduces. It is directly applicable to existing computational pipelines allowing reliable black-box posterior inference. We empirically show on six benchmark problems that the proposed method achieves competitive or better results in terms of coverage and expected posterior density than the previously existing approaches.

## 1 Introduction

Inverse problems  appear in a variety of fields of human activity, including engineering , geology , medical imaging , economics , and particle physics . In some domains, uncertainty quantification is required for results to be used in the real-world . A particularly well-suited tool for this task is Bayesian inference, where the procedure of updating prior beliefs (knowledge) with observed evidence is formally described in the framework of probability theory. In order to compute the posterior probability according to Bayes' rule, the likelihood function has to be evaluated. Often, it is only implicitly established by a simulator which given input parameters generates (possibly stochastically) outputs. Such a situation requires the use of likelihood-free techniques, also referred to as simulation-based inference (SBI) .

There exist two major classical approaches that address the problem of Bayesian inference in the likelihood-free setting. In the first one, simulated data are used to estimate the distribution of expert-crafted low-dimensional summary statistics with a nonparametric method like kernel density estimation . Next, the approximate density is used as a likelihood surrogate turning the problem into likelihood-based inference. In the second approach, termed as Approximate Bayesian Computation (ABC)  no data is simulated upfront, but instead for every newly appearing observation (instance of the inverse problem), an independent sampling-rejection procedure with a simulator in-the-loop is run. Given some proposal distribution, candidateparameters are accepted if the simulator's output is in sufficient agreement with the observation. While ABC effectively sorts out the lack of explicit likelihood, it is generally costly, especially when the underlying simulator is computationally expensive.

Deep Neural Networks have been increasingly utilized in SBI to alleviate these costs, by acting as universal and powerful density estimators or surrogating other functions. The following approaches of neural SBI can be identified: Neural Likelihood Estimation (NLE) , where the missing likelihood function is approximated based on the simulator; Neural Ratio Estimation (NRE) [19; 15; 32], where the likelihood-to-evidence ratio (other ratios have also been considered) is approximated; Neural Posterior Estimation (NPE) [34; 30; 18], where the learned density estimator directly solves the inverse problem for a given observation. In the last category the score-based methods [41; 16] are emerging, where the posterior is implicitly expressed by the learned score function. All of the approaches mentioned above amortize the use of a simulator, however, inference with NLE and NRE still requires the use of Markov chain Monte Carlo , while NPE can be fully amortized with the use of appropriate generative models .

The question of reliability is raised with the use of approximate Bayesian inference methods. Many works have been devoted to inference evaluation techniques in the absence of the ground-truth posterior reference [45; 53; 27; 28]. Lueckmann et al.  propose an SBI benchmarking protocol taking into account the effect of the simulation budget, with the observation that the assessment's conclusions will strongly depend on the metric chosen. Hermans et al.  suggest relying on coverage analysis of posterior's multi-dimensional credible regions, showing that numerous SBI techniques yield overconfident posteriors. In a follow-up work  they introduce a balancing regularizer for training NRE, and show empirically that it leads to the avoidance of overconfident solutions. Very recently, an interesting study by Delaunoy et al.  extends the applicability of balancing regularizer to the NPE approach. An alternative perspective is studied in Dey et al.  where an additional regression model for posthoc re-calibration based on Probability Integral Transform is learned for the outputs of a pre-trained inference model.

In this work, we introduce and empirically evaluate a new regularizer that directly targets the coverage of the approximate posterior. It is applicable to NRE and NPE based on neural density estimators such as Normalizing Flows [26; 36]. As an additive term, it is easy for practitioners to use as a plug-in to existing pipelines. The additional computational overhead introduced by the regularizer is justified in view of the advantages it brings.

The rest of the work is structured as follows. Section 2 introduces the formal description of the studied problem and presents the coverage analysis that is the motivation for the presented method. Section 3 describes the proposed method and a differentiable formulation of the regularizer that allows gradient-based learning. Section 4 shows the empirical results of applying the method on a wide range of benchmark problems, as well as providing a hyper-parameter sensitivity analysis. In section 5, we provide summarize our work with conclusions and potential future directions.

## 2 Background

In this work, we consider the problem of estimating posterior distribution \(p(|x)\) of model parameters \(\), given the observation \(x\). We approach the problem from a likelihood-free  perspective, where the model's likelihood \(p(x|)\) cannot be evaluated. Instead, there exists a simulator that generates observations \(x\) based on the input parameters \(\). We can prepare a dataset \(\{(_{i},x_{i})\}_{i=1}^{N}\), by sampling parameters from a prior \(p()\) and running them through the simulator. In that way, the problem of finding the approximate posterior marked as \((|x)\) turns into a supervised learning problem. However, it is important to note that usually there is not only a single generating parameter \(\) for a given \(x\) possible. Instead, for each observation multiple parameter values are plausible . These values form a posterior distribution \(p(|x)\) under a prior distribution \(p()\).

Assessing the performance of an SBI technique is challenging due to the non-availability of the ground-truth posterior \(p(|x)\) in the general case. Theoretically, methods such as the rejection-sampling ABC  are proven to converge to \(p(|x)\) under an infinite simulation budget but often require a large number of simulation runs to achieve a good approximation. Avoiding this costly procedure is exactly what motivates the introduction of different techniques. Therefore, it is common to resort to performance metrics evaluated on test datasets sampled from the simulator.

If a Bayesian inference method allows \((|x)\) evaluation, its performance can be assessed by looking at the expected value of the approximate log posterior density of the data-generating parameters (nominal parameters) \(_{p(,x)}[(|x)]\), estimated over a test dataset. In domains such as natural sciences, it is also accurate uncertainty quantification that matters, i.e. reliable posterior density estimation given the available evidence in data. It has been shown in Hermans et al.  that approaches yielding high expected posterior density values can give overconfident posteriors, and therefore, verifying the quality of credible regions returned by inference engines as well as designing more reliable ones is essential. In this work, we define a regularizer suitable for training NRE and NPE based on the Simulation-Based Calibration (SBC) diagnostic of Talts et al. . To make the paper self-contained, in the rest of Section 2 we will introduce SBC formally, adapting the definitions and lemmas from Lemos et al. , and making the link with the coverage analysis from Hermans et al. .

### Coverage analysis

Coverage analysis offers a way to evaluate the quality of an approximate posterior. Given an observation and an estimated distribution, one can assess the probability that a certain credible region  of the parameter space contains the nominal parameter value found in the dataset. For a conditional probability distribution, a credible region at level \(1-\) is a region of the support to which the parameter used to generate the conditioning observation belongs with a \(1-\) probability, called the credibility level. There are infinitely many credible regions at any given level, and coverage analysis requires a particular way of choosing one. In this work, we will consider the Highest Posterior Density Region (HPDR).

**Definition 1** (Highest Posterior Density Region).: \[^{}_{(|x)}(1-):=_{}|| _{}(|x)=1-,\] (1)

_where \(||:=_{}\) is the volume of a subset \(\) of the parameters' space, under some measure \(\)._

Next, we introduce the central concept of coverage analysis.

**Definition 2** (Expected Coverage Probability).: _The Expected Coverage Probability (ECP) over a joint distribution \(p(,x)\) for a given \(^{}_{(|x)}(1-)\) is_

\[^{}_{(|x)}(1-):= _{p(,x)}[^{} _{(|x)}(1-)],\] (2)

_where \([]\) is the indicator function._

While the expectation in eq. (2) can be easily estimated by using a set of i.i.d. samples from the joint distribution \(p(,x)\), the challenge of finding \(^{}_{(|x)}(1-)\) remains non-trivial. In general, directly following eq. (1) requires solving an optimization problem. This would be prohibitive to do for every instance during training, thus it is crucial to benefit from the two lemmas we introduce below.

**Lemma 1**.: _A pair \((^{*},x^{*})\) and a distribution \((|x)\) uniquely define an HPDR:_

\[^{}_{(|x)}(1-_{}(, ^{*},x^{*})):=\{(|x^{*})( ^{*}|x^{*})\},\] (3)

_where_

\[_{}(,^{*},x^{*}):=(|x)[(|x^{*})<(^{*}|x^{*})]d.\] (4)

See proof in Appendix A.

**Lemma 2**.: \[^{}_{(|x)}(1-)= _{p(,x)}[[_{}(,, x)]]\] (5)

See proof of Lemma 1 in Lemos et al. .

If \(^{}_{(|x)}(1-)\) is equal to \(1-\) the inference engine yields posterior distributions that are calibrated at level \(1-\), or simply calibrated if the condition is satisfied for all \(\). The ground-truth posterior \(p(|x)\) is calibrated, but another such distribution is the prior \(p()\) as discussed in Hermanset al.  and Lemos et al. . Therefore, coverage analysis should be applied in conjunction with the standard posterior predictive check. Formally, we aim for:

\[p^{*}(|x):=*{arg\,max}_{(|x)}_{p( ,x)}[(|x)]\ \ _{(|x)}^{}(1-) =1-\] (6)

In Hermans et al.  it has been pointed out that for practical applications often the requirement of the equality sign in the condition in eq. (6) can be relaxed and replaced with \(_{(|x)}^{}(1-)\) above or equal the \(1-\) level. Such a posterior distribution is called conservative.

We define the _calibration error_ as

\[_{i=1}^{M}(1-_{i})-_{ (|x)}^{}(1-_{i}),\] (7)

and the _conservativeness error_ as

\[_{i=1}^{M}(1-_{i})-_{ (|x)}^{}(1-_{i}),0\,,\] (8)

where \(\{_{i}\}_{i=1}^{M}\) is a set of credibility levels of arbitrary cardinality such that \(_{i}\,0<_{i}<1\); we can exclude \(=0\) and \(=1\) because calibration/conservativeness at these levels is achieved by construction. For a motivating graphical example of coverage analysis of an overconfident distribution see appendix D.

## 3 Method

We propose a method supporting the training of neural models underlying SBI engines such that the resulting distributions are calibrated or conservative against the real \(p(,x)\) in the data. In addition to the standard loss term in the optimization objective function, we include a regularizer corresponding to differentiable relaxations of the calibration (conservativeness) error as defined in eq. (7) (eq. (8)). In the remainder of this section, we will show how to efficiently estimate the expected coverage probability during training and how to alleviate the arising non-continuities to allow gradient-based optimization. After a few simplifications, the regularizer will become a mean square error between the sorted list of estimated \(_{}(,,x)\)s and a step function on the \((0,1)\) interval.

The proposed method is based on the Monte Carlo (MC) estimate of the integral in eq. (4):

\[_{}^{L}(,^{*},x^{*}):=_{j= 1}^{L}[(_{j}|x^{*})<(^{*}|x^{*})],\] (9)

where \(_{1},...,_{L}\) are samples drawn from \((|x)\). The entity defined in eq. (9) is a special case of what is called _rank statistic_ in Talts et al.  (with the \(1/L\) normalization constant skipped there). The non-normalized variant is proven to be uniformly distributed over the integers \([0,L]\) for \((|x)=p(|x)\)\((,x) p(,x)\), which is the _accurate posterior_ as defined in Lemos et al. . Taking \(L\) and applying the normalization (as done in eq. (9)) one shows that distribution of \(_{}^{}(,^{*},x^{*})\) converges to a standard continuous uniform distribution (proof in appendix A.2).

We return to coverage analysis with the following remark:

**Remark 1**.: _An inference engine yields calibrated posterior distributions if and only if it is characterized by uniformly distributed \(_{}(,,x)\)s._

See proof in Appendix A.

Following remark 1, we will enforce the uniformity of the normalized rank statistics which coincides with a necessary, though not sufficient, condition for the approximate posterior to be accurate. The one-sample Kolmogorov-Smirnov test provides a way to check whether a particular sample set follows some reference distribution. We will use its test statistic to check, and eventually enforce, the condition. An alternative learning objective is to directly minimize the calibration (conservativeness) error at arbitrary levels with ECP computed following eq. (5).

### One-sample Kolmogorov-Smirnov test

Let \(B\) be a random variable taking values \(\). Given a sample set \(}=\{_{i}|i=1,,N\}\), the one-sample Kolmogorov-Smirnov test statistic  for r.v. \(B\) is defined as:

\[D_{N}=_{}|F_{N}()-F()|,\] (10)

where \(F_{N}\) is the empirical cumulative distribution function (ECDF) of the r.v. \(B\), with regard to the sample set \(}\). \(F\) is the reference cumulative distribution function (CDF) against which we compare the ECDF.

In the proposed method, we instantiate the KS test statistic for r.v. \(_{}(,,x)\), where \((,x) p(,x)\), thus \(F_{N}\) is its ECDF, which we estimate from a sample set \(}:=\{^{L}_{}(,_{i},x_{i} )|(x_{i},_{i}) p(x,),i=1,,N\}\). The reference CDF, \(F\), is that of the standard uniform distribution because we want to check and enforce the uniformity of the elements of \(}\).

By the Glivenko-Cantelli theorem  we know that \(D_{N} 0\) almost surely with \(N\) if samples follow the \(F\) distribution. The claim in the opposite direction, that is if \(D_{N} 0\) then samples follow \(F\), is also true and we show that in appendix A.3. As a result, minimizing \(D_{N}\) will enforce uniformity and thus posterior's calibration. We can upper-bound the supremum in eq. (10) with the sum of absolute errors (SAE), but instead, to stabilize gradient descent, we use the mean of squared errors (MSE), which shares its optimum with SAE. Minimizing the one-sample KS test statistic is just one of the many ways of enforcing uniformity. We can deploy any divergence measure between the sample set and the uniform distribution.

### Approximating \(D_{n}\)

In practice we cannot evaluate the squared errors over the complete support of the r.v., but only over a finite subset. There are two ways of evaluating this approximation which differ by how we instantiate \(F_{N}()\) and \(F()\).

Direct computationIn the first, we compute \(F_{N}()=_{i}[_{i}]/N\) summing over all \(_{i}}\) and use the fact that \(F()=\) for the target distribution. To approximate \(D_{N}\) we aggregate the squared error over an arbitrary set of \(_{k}\) values: \(_{k}(F_{N}(_{k})-_{k})^{2}\). Including this term as a regularizer means that we need to backpropagate through \(F_{N}(_{k})\). Such an approach can be seen as directly aiming for calibration at arbitrary levels \(_{k}\), i.e. applying eq. (5) to evaluate the calibration error.

Sorting-based computationAlternatively, we can sort the values in \(}\) to get the ordered sequence \((_{i}|i:=1 N)\). Given the sorted values, \(_{i}\), and their indices, \(i\), the \(F_{N}(_{i})\) values are given directly by the indices normalized by the sample size (\(i/N\)), and by the corresponding target distribution \(F(_{i})=_{i}\), which leads to the \(_{i}(i/N-_{i})^{2}\) approximation. Under this approach, the gradient will now flow through \(F(_{i})\). This is the approach that we use in our experiments.

### Importance Sampling integration

In the definition of \(_{}(,^{*},x^{*})\) in eq. (4) as well as in its MC estimate \(^{L}_{}(,^{*},x^{*})\) defined in eq. (9) we sample \(_{j}\)s from the approximate posterior \((|x)\). Optimizing the model constituting the approximate posterior requires backpropagating through the sampling operation which can be done with the reparametrization trick . However, the reparametrization trick is not always applicable; for example, in NRE an MC procedure is required to sample from the posterior . Here we can use instead self-normalized Importance Sampling (IS) to estimate \(^{L}_{}(,,x)\) and lift the requirement of sampling from the approximate posterior. We will be sampling from a known proposal distribution and only evaluate the approximate posterior density. We thus propose to use IS integration [e.g., 17] to estimate the integral in eq. (4), in the following way:

\[^{L,IS}_{}(,^{*},x^{*})=^{ L}(_{j}|x^{*})/I(_{j})[(_{j}|x^{*})< (^{*}|x^{*})]}{_{j=1}^{L}(_{j}|x^{*})/I(_{j})}.\] (11)The \(_{j}\)s are sampled from the proposal distribution \(I()\). For a uniform proposal distribution, the \(I(_{j})\) terms are constant and thus cancel out. By default, we suggest using the prior as the proposal distribution. While IS is not needed in the case of NPE, we use it also for this method to keep the regularizer's implementation consistent with the NRE setting.

### Learning objective

So far we have shown how to derive a regularizer that enforces a uniform distribution of the normalized rank statistics that characterizes a calibrated model as shown in remark 1. This is also a necessary condition for the accuracy of the posterior as shown in Talts et al.  but not a sufficient one, as exemplified by the approximate posterior equal to the prior for which the regularizer trivially obtains its minimum value. Thus the final objective also contains the standard loss term and is a weighted sum of the two terms:

\[_{i}^{N}(_{i}|x_{i})+_{ i}^{N}_{}^{L,IS}(, _{i},x_{i})/N-_{}^{L,IS}(,_{ i},x_{i})^{2}\,,\] (12)

which we will minimize with AdamW optimizer. The first term in eq. (12) is the likelihood loss which we use for the NF-based NPE (or its corresponding cross-entropy loss for the NRE) and the second is the uniformity enforcing regularizer. The \(()\) operator gives the index of its argument in the sequence of sorted elements of \(\{_{}^{L,IS}(,_{i},x_{i}) i=1,,N\}\). When regularizing for conservativeness we accept \(_{}(,^{*},x^{*})\)s to first-order stochastically dominate  the target uniform distribution, thus the differences in the uniformity regularizer are additionally activated with a rectifier \((;0)\) analogously to eq. (8). In appendix A.1 we sketch a proof of how minimizing such regularizer affects the learned model. The two main hyper-parameters of the objective function are the number of samples \(L\) we use to evaluate eq. (11), and the weight of the regularizer \(\) in eq. (12). In algorithm 1 we provide the pseudo-code to compute the uniformity regularizer.

To allow backpropagation through indicator functions, we use a Straight-Through Estimator  with the Hard Tanh  backward relaxation. In order to backpropagate through sorting operation we use its differentiable implementation from Blondel et al. .

```
0: Data batch \(\{(_{i},x_{i})\}_{i=1}^{N}\), model \((|x)\), number of samples \(L\), proposal distribution \(I()\)
0: Regularizer's loss \(R\)
1:for\(i 1\) to \(N\)do
2:\(p_{i}(_{i}|x_{i})\)
3:for\(j 1\) to \(L\)do
4:\(_{i}^{j} I()\)\(\) sampling from proposal distribution
5:\(p_{j}^{j}(_{i}^{j}|x_{i})\)
6:endfor
7:\(_{}^{L,IS}(,_{i},x_{i})^{L}p_{j}^{j}/I(_{i}^{j}) p_{i}^{j}<p_{j}}{ _{j=1}^{L}p_{j}^{j}/I(_{i}^{j})}\)\(\) eq. (11)
8:endfor
9:\((_{i}|i=1,,N)(\{_{}^{L, IS}(,_{i},x_{i})|i=1,,N\})\)
10:\(R_{i}^{N}(i/N-_{i})^{2}\)\(\) the second term in eq. (12) ```

**Algorithm 1** Computing the regularizer loss with calibration objective.

## 4 Experiments

In our experiments1, we basically follow the experimental protocol introduced in Hermans et al.  for evaluating SBI methods. We focus on two prevailing amortized neural inference methods, i.e. NRE approximating the likelihood-to-evidence ratio and NPE using conditional NF as the underlying model. In the case of NRE, there has been already a regularization method proposed  and we include it for comparison with our results, and mark it as BNRE. In the main text of the paper, we focus on the conservativeness objective. For the calibration objective, the results can be found in appendix C where we report positive outcomes only for part of the studied SBI problems. As the proposal distribution for IS, we use the prior \(p()\). In the main experiments, we set the weight of the regularizer \(\) to \(5\), and the number of samples \(L\) to \(16\) for all benchmarks. In section 4.3 we provide insights into the sensitivity of the method with respect to its hyper-parameters.

### Principal experiments

In fig. 1, we present a cross-sectional analysis of the results of the proposed method, based on six benchmark SBI problems described in detail in Hermans et al. , for each considering eight different simulation budgets. Expected coverage of HPDRs at 19 evenly spaced levels on the \([0.05;0.95]\) interval is estimated following eq. (2) by solving an optimization problem to find \(^{}_{(|x)}(1-)\) for every test instance. We intentionally use a different ECP estimation method than in the regularizer to avoid accidental overfitting to the evaluation metric. The top three rows show empirical expected coverage curves on the test set obtained by training the same model with three different objectives - NRE: classification loss; BNRE: classification loss with balancing regularizer ; CalNRE: classification loss with the proposed regularizer. The results for CalNRE show that the proposed method

Figure 1: Evaluation on six benchmark problems (for each column) in the form of coverage curves estimated on a set of 10k test instances. If the curve is on (above) the diagonal line, then \((|x)\) is calibrated (conservative). Expected coverage of HPDRs at 19 evenly spaced levels on the \([0.05;0.95]\) interval is estimated following eq. (2) by solving an optimization problem to find \(^{}_{(|x)}(1-)\) for every test instance. The average of retraining five times from random initialization is shown. Top there rows marked as NRE, BNRE, and CalNRE (ours) show the NRE trained without regularization, with balancing regularizer, and the proposed regularizer with conservativeness objective, respectively. The bottom rows marked as NPE and CalNPE show the NPE trained without regularization and with the proposed regularizer with conservativeness objective, respectively. Best viewed in color.

systematically results in conservative posteriors - with SLCP simulation budget 1024 standing out, due to too weak regularization weight \(\). Moreover, in fig. 2 it can be seen that CalNRE typically outperforms BNRE in terms of \(_{p(,x)}[(|x)]\), being close to the performance of NRE (which is not conservative), and demonstrating less variance across initializations.

The bottom two rows in fig. 1 show the same type of results for NPE-based models, with CalNPE being trained with the proposed regularizer. For all budgets in the Gravitational Waves benchmark, CalNPE does not meet the expected target, but this can easily be fixed by adjusting the regularization strength \(\). In fig. 2 one finds that CalNPE sometimes yields a higher \(_{p(,x)}[(|x)]\) than the non-regularized version, which is a counterintuitive result. We hypothesize that by regularizing the model we mitigate the classical problem of over-fitting on the training dataset.

### Computation time

In fig. 3 we show how applying the proposed regularizer affects the training time in the case of NRE for the SLCP benchmark, with the distinction to CPU and GPU. As a reference, we take the training time of the non-regularized model. While on CPU, the training time grows about linearly with \(L\), on GPU due to its parallel computation property the overhead is around x2, with some variation depending on the simulation budget and the particular run. However, this is only true as long as the GPU memory is not exceeded, while the space complexity is of order \((N()L)\), where \(N\) is the batch size. Comparing with fig. 2, we conclude that applying the regularizer can result in a similar \(_{p(,x)}[(|x)]\) as in the case of increasing the simulation budget (what extends training time) while keeping comparable computational time, and avoiding over-confidence. Moreover, in situations where the acquisition of additional training instances is very expensive or even impossible, the proposed method allows for more effective use of the available data at a cost sub-linear in \(L\).

### Sensitivity analysis

In fig. 4 we show the sensitivity of the method with respect to its hyper-parameters on the example of NRE applied for the Weinberg benchmark. In fig. 4a, the analysis of regularization strength indicates a tradeoff between the level of conservativeness and \(_{p(,x)}[(|x)]\). Therefore, choosing the

Figure 3: The computational overhead of the proposed method applied to NRE on the SLCP problem. The Y-axis indicates how many times training with the regularizer lengthens relative to training without it. For every number of samples experiments were conducted on both CPU (left chart) and GPU (right chart). Averaged over five runs, error bars show a double standard deviation range. Best viewed in color.

Figure 2: Expected value of the approximate log posterior density of the nominal parameters \(_{p(,x)}[(|x)]\) of the SBI approaches in fig. 1 estimated over 10k test instances. The solid line is the median over five random initializations, and the boundaries of the shaded area are defined by the minimum and maximum values. The horizontal black line indicates the performance of the prior distribution. Best viewed in color.

right configuration for practical applications is a multi-objective problem. The lowest possible \(\) value that yields a positive AUC (\(=5\) in the figure) should be used to maximize predictive power as measured by \(_{p(,x)}[(|x)]\). From fig. 3(b) we can read that increasing the number of samples in the regularizer brings the coverage curve closer to the diagonal while increasing \(_{p(,x)}[(|x)]\). This suggests that a more accurate estimation of conservativeness errors during training allows better focus on the main optimization objective.

## 5 Conclusions and future work

In this work, we introduced a regularizer designed to avoid overconfident posterior distributions, which is an undesirable property in SBI. It is applicable in both the NRE and NPE settings, whenever the underlying model allows for amortized conditional density evaluation. It is derived directly from the evaluation metric based on Expected Coverage Probability, used to assess whether the posterior distribution is calibrated or conservative, depending on the target. We formally show that minimizing the regularizer to 0 error under infinite sampling guarantees the inference model to be calibrated (conservative).

We empirically show that applying the regularizer in parallel to the main optimization objective systematically helps to avoid overconfident inference for the studied benchmark problems. We admit that calculating the additional term adds computation cost at training time, but this can be mitigated with the use of modern hardware accelerators well suited for parallel computation.

The presented method is introduced in the context of simulation-based inference, but it is suitable for application in general conditional generative modeling, where calibration (conservativeness) can for example help mitigate mode collapse .

As a future direction, we identify the need to evaluate the proposed method in problems where the parameters' space has a high dimension. Moreover, currently, the method is restricted to neural models acting as density estimators, but not general generative models that only allow for sampling from the

Figure 4: Sensitivity analysis study of CalNRE on the Weinberg benchmark with the smallest simulation budget of 1024 training instances with respect to \(\) in (a) and \(L\) in (b). The yellow color in each chart shows the results of non-regularized NRE on the highest simulation budget of 131072 training instances. Results in the first two charts show the performance of an ensemble model built from five randomly initialized members. The vertical line in the first chart marks the nominal parameter value. Points in the third chart indicate the average AUC over five randomly initialized models, with two times the standard deviation marked with the shaded area. The shaded area and the error bars in the fourth chart span between minimum and maximum results over the five randomly initialized models with the median indicated by point (line). Best viewed in color.

learned distribution. A sampling-based relaxation of the regularizer would extend its applicability to a variety of models including Generative Adversarial Networks , score/diffusion-based models , and Variational Autoencoders .