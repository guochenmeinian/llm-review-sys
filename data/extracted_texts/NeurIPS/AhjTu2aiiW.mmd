# First-Explore, then Exploit: Meta-Learning to Solve Hard Exploration-Exploitation Trade-Offs

Ben Norman1,2

btnorman@cs.ubc.ca

Jeff Clune1,2,3

jclune@gmail.com

###### Abstract

Standard reinforcement learning (RL) agents never intelligently explore like a human (i.e. taking into account complex domain priors and adapting quickly based on previous exploration). Across episodes, RL agents struggle to perform even simple exploration strategies, for example systematic search that avoids exploring the same location multiple times. This poor exploration limits performance on challenging domains. Meta-RL is a potential solution, as unlike standard RL, meta-RL can _learn_ to explore, and potentially learn highly complex strategies far beyond those of standard RL, strategies such as experimenting in early episodes to learn new skills, or conducting experiments to learn about the current environment. Traditional meta-RL focuses on the problem of learning to optimally balance exploration and exploitation to maximize the _cumulative reward_ of the episode sequence (e.g., aiming to maximize the total wins in a tournament - while also improving as a player). We identify a new challenge with state-of-the-art cumulative-reward meta-RL methods. When optimal behavior requires exploration that sacrifices immediate reward to enable higher subsequent reward, existing state-of-the-art cumulative-reward meta-RL methods become stuck on the local optimum of failing to explore. Our method, First-Explore, overcomes this limitation by learning two policies: one to solely explore, and one to solely exploit. When exploring requires forgoing early-episode reward, First-Explore significantly outperforms existing cumulative meta-RL methods. By identifying and solving the previously unrecognized problem of forgoing reward in early episodes, First-Explore represents a significant step towards developing meta-RL algorithms capable of human-like exploration on a broader range of domains.

## 1 Introduction

Reinforcement learning (RL)  can perform challenging tasks, such as plasma control , molecule design , game playing , and robotic control . However, RL is sample inefficient (taking thousands of episodes to learn tasks humans master in a few tries) , limiting its application. Meta-RL  circumvents this issue by enabling an agent to adapt to new environments based solely on prior experience (i.e., remembering what occurred in previous episodes and using that to inform subsequent behavior). By replacing slow weight-based RL updates with memory-based meta-RL adaption, human-like sample efficiency  can be achieved.

This paper focuses on _cumulative-reward_ meta-RL, which aims to optimize performance across a sequence of episodes, \(_{1},,_{n}\) (e.g., games in a tournament). The objective is to maximize the total reward accumulated over all episodes (e.g., the number of games won), expressed as\(_{i=1}^{n}G(_{i})\), where the episode return \(G(_{i})\) is the total reward of episode \(_{i}\). To maximize this sum, the agent should optimally balance exploration and exploitation across the sequence, e.g., prioritizing exploration in early episodes so as to better exploit in later ones.

Cumulative reward meta-RL has an unrecognized failure mode, where state-of-the-art (SOTA) methods achieve low cumulative-reward regardless of how long they are trained. This dynamic occurs in domains with the following properties: **A**. Maximizing the expected total reward requires exploratory actions that forgo immediate reward, and **B**. The benefit of these exploratory actions _only occurs_ when they are _reliably_ followed by good exploitation (i.e., if exploitation is too inconsistent, then exploration results in _lower_ total reward).

An example is a bandit domain where the first arm always provides a reward that is better than the average arm, but not the highest possible. To maximize cumulative reward over many pulls, the agent must explore the other arms, and then repeatedly exploit the best one. Property **A** holds, as this optimal policy forgoes immediate reward by not sampling the first arm and its above-average reward. Property **B** holds, as exploration (sampling arms other than the first) is _only valuable_ when it is followed by sufficiently consistent exploitation (reliably re-sampling the best arm).

The failure occurs as follows: 1. At the start of training, the agent, being randomly initialized, lacks the ability to _reliably exploit_ learned information. 2. As a result, the domain properties **A** and **B** cause exploratory actions to lead to lower total reward than the other actions do. 3. This lower reward trains the agent to _actively avoid exploration._ 4. This avoidance then locks the agent into poor performance, as it cannot learn effective exploitation without exploration. This process occurs in the bandit example. Initially, the agent cannot exploit (e.g., when it finds the best arm it does not reliably resample it). The associated negative expectation of exploration then trains the agent to _avoid exploration_ by only sampling the first arm, with its above-average, but sub-optimal, arm reward.

Current SOTA meta-RL algorithms such as RL\({}^{2}\)[8; 9], VariBAD , and HyperX  attempt to train a single policy to maximize the cumulative reward of the whole episode sequence. This optimization causes step 3 of the above-described failure process, and thus these methods suffer from the issue of failing to properly learn (e.g., converging rapidly to a policy of not exploring), which we demonstrate on multiple domains (Section 6). The issue is especially insidious because distributions of simple environments (each trivially solved by standard-RL) can stymie these methods. Surprisingly, domains such as bandits can be too hard for SOTA meta-RL.

We introduce a new approach, First-Explore (visualized in Figure 1), which overcomes this problem associated with directly optimizing for cumulative reward. Rather than training a single policy to maximize cumulative reward, First-Explore learns two policies: an exploit policy, and an explore policy. The exploit policy maximizes episode return, without attempting to explore. In contrast, the explore policy explores to best inform the exploit policy, without attempting to maximize its own episode return. Only _after training_ are the two policies combined to achieve high cumulative reward.

Because the explore policy is trained solely to inform the exploit policy, poor current exploitation no longer causes immediate rewards (property **A**) to _actively discourage_ exploration. This change eliminates step 3 of the failure process, and enables First-Explore to perform well in domains where SOTA meta-RL methods fail. By identifying and solving this previously unrecognized issue, First-Explore represents a substantial contribution to meta-RL, paving the way for human-like exploration on a broader range of domains.

## 2 Background

**RL Terminology:** environments are formally defined as partially observable Markov decision processes (POMDPs, ). Each POMDP \(E\) is specified by a tuple \(E=(S,A,p,p_{0},R,,O,)\), where \(S\) is the state space, \(A\) the action space, \(p:S A S\) a probabilistic transition function mapping from the current state and action to the next state, \(p_{0}\) a distribution over starting states, \(R:S A\) a stochastic reward function, \(\) the space of environment observations, \(O:S\) a stochastic function mapping from states to observations, and \(\) the discount factor. The environment starts (at \(t=0\)) in a start state \(s_{0}\) according to \(p_{0}\), \(s_{0} p_{0}\). Each subsequent time-step, the agent receives the current state's observations \(o_{i}=O(s_{i})\), takes an action \(a_{i}\), and the transition function \(p\) updates the environment state \(s_{i+1}=p(s_{i},a_{i})\). An episode \(\) of length \(h\) is then a sequence of time-steps starting from \(t=0\) to \(t=h\). The sum of an episode's \(\)-discounted rewards is called its return \(G()=_{t=0}^{h}^{t}R(s_{t},a_{t})\). Standard RL generally aims to learn a stochastic policy \(: A\) that maximizes the expected episode return \([G()]\).

Unlike standard RL, **meta-RL** trains an agent to: a) perform well in a distribution \(\) of environments (e.g., a collection of mazes) and b) dynamically tailor itself to new environments (e.g. memorize a new maze over successive episodes). The agent completes meta-rollouts, each a sequence of episodes \(_{1},,_{n}\) on a new environment \(E\), with each episode within a meta-rollout beginning from a newly sampled start-state. The agent can remember information from earlier episodes in the same meta-rollout (e.g., using a sequence model such as a transformer ), allowing it to adapt its behavior based on that information. By training (via weight updates) on large batches of meta-rollouts, the agent then learns to leverage its memory mechanism, enabling sample-efficient memory-based adaptation and high performance on new environments sampled from \(\).

Meta-RL methods can be split into two approaches that each solve a different problem, with specific algorithms designed and used for each approach 1. **Cumulative-reward meta-RL** trains to maximize cumulative reward \(_{i=1}^{n}G(_{i})\). Examples include RL2[8; 9], VariBAD  and HyperX . **Final-episode-reward meta-RL** aims to instead optimize solely for final episode reward \(G(_{n})\). Examples include DREAM  and MetaCURE . In this paper we compare and analyze First-Explore in a cumulative-reward setting. However, for the sake of completeness, we discuss how First-Explore relates to final-episode-reward meta-RL methods in Appendix I.

## 3 Related Work

Current cumulative-reward meta-RL methods train a single policy to maximize the cumulative reward of the whole episode sequence. However, optimizing directly for cumulative reward can prevent effective learning even on simple domains (e.g., bandits), as Section 1 describes and Section 6 demonstrates.

Figure 1: **First-Explore** aims to maximize the cumulative reward of a sequence of \(n\) episodes on a target environment distribution. This optimization is achieved by first training two separate policies, and then combining them _after_ training to maximize the total reward obtained. **A.** First, two separate policies are trained on the distribution of environments: one to **explore** (produce informative episodes), and one to **exploit** (maximize current episode return). During training, the **explore policy**\(_{}\) provides all the context \(c_{i}=_{1},,_{i}\) for both policies. This flow of context is visualized by **solid arrows \(\).** The **exploit** policy \(_{}\) takes a context of episodes, and produces a single episode of exploitation. The return of this exploit episode is then used to train both policies, with the feedback to the explore policy visualized by the **dotted** green arrows \(\)\(\), **B.** After the two policies are trained, different combinations of them are evaluated to find the combination that maximizes total reward. Each combination involves first exploring for \(k\) episodes, and then repeatedly exploiting for the remaining \(n-k\) episodes. **C.** The best combination is then used at inference time: exploring for a fixed number of episodes on new environments, and then exploiting for the remaining episodes.

RL\({}^{2}\)[8; 9] is one of the first (cumulative-reward) meta-RL methods. It uses an RNN to provide across-episode memory, and standard RL algorithms to train the agent. RL\({}^{2}\) has the advantage of simplicity. By training standard RL algorithms with the capacity for across-episode memory, the agent may learn to optimally balance exploration and exploitation across successive episodes. However, training dynamics can hinder achieving optimal performance (e.g., due to the requirement of sacrificing immediate reward (Section 1 and Section 6) or because the reward signal is too sparse ).

Subsequently, various cumulative-reward meta-RL works have been produced. VariBAD  outperforms RL\({}^{2}\) in certain domains. VariBAD achieves this improvement via splitting training into producing a posterior belief of the current task (task inference), and a task-posterior conditioned policy. HyperX  improves exploration during policy training by adding an incentive to visit novel states. This exploration incentive is gradually decreased throughout training, starting high and ending at zero. The issue is that the cumulative-reward term can still _actively discourage_ exploration, as described in Section 1. Therefore, HyperX primarily tackles the issue of sparse exploration rewards, rather than overcoming the challenge of domains that actively disincentivize exploration.

As Section 6 shows, when good early exploration requires forgoing immediate rewards, all these methods can fail to learn good behavior (e.g., converging immediately to a policy of never exploring). This failure arises not from reward sparsity (the problem HyperX tackles) or a lack of sophisticated posterior-belief-conditioned optimization (VariBAD's focus), but because the reward dynamics actively train the agent to _avoid_ exploration. As such, First-Explore can achieve substantial total reward in domains in which RL\({}^{2}\), VariBAD, and HyperX all perform poorly.

AdA  demonstrates that meta-RL scales to complex domains and that training meta-RL on a curriculum of tailored learning challenges can produce agents capable of human-like adaption on complex held-out tasks. However, AdA may struggle to forgo reward, as the authors note that their agent always maximized current episode return (rather than the total reward of the episode sequence). This behavior suggests AdA's performance might depend on training and testing on environments that do not require such sacrificial exploration. Unfortunately, investigating AdA's dynamics is outside the scope of this paper.

## 4 First-Explore

First-Explore is a general framework for meta-RL that can be implemented in various ways. The framework trains two distinct policies, one policy to explore and one policy to exploit. Individually neither policy can achieve high cumulative reward, but _after_ weight-update training, the policies are combined to produce an inference policy that does so. Figure 1 visualizes this process. By not _directly_ training a single policy to maximize the total reward of the whole episode sequence, First-Explore avoids the failure mode of earlier approaches.

The **explore policy**\(_{}\) performs successive episodes. During each episode, it has access to a context containing all previous actions, observations, and rewards within the current exploration sequence. In contrast, the **exploit policy**\(_{}\) is trained to take the context \(c\) from the explore policy that has explored for a number of episodes between \(1\) and \(n\), and to then run one further episode of exploitation. During this episode, the exploit policy has access to the actions, observations, and rewards from the current exploitation episode, as well as from the preceding exploration episodes (Figure 1).

The explore and exploit policies are incentivized differently. The **exploit policy** is incentivized to produce high-return episodes (based on the provided context of previous explorations). The **explore policy** is instead incentivized to produce episodes that each, _when added to the current context_, result in subsequent high-return exploit-policy episodes. Training the exploit policy requires context from the explore policy, and training the explore policy requires the returns of subsequent exploits. This is efficiently achieved by interleaving the policies as depicted in Figure 1-A, where each rollout from the explore policy is followed by a rollout from the exploit policy.

After the two policies are trained, First-Explore searches for a combination of them that maximizes the expected cumulative reward on newly sampled environments. Each candidate combination, first explores (via \(_{}\)) for a set number of episodes \(k\), and then exploits (via \(_{}\)) for the remaining \(n-k\) episodes. These combination policies are evaluated on independent environments sampled from the target distribution, with the combination that maximizes mean cumulative reward being chosen (see Appendix H for an example). This process of selecting \(k\) does not involve retraining the policies, and thus it is comparatively computationally cheap. As such, \(k\) should not be thought of as a hyperparameter, as unlike hyperparameters, the majority of the training compute expenditure is on policy weight updates that are performed _before_\(k\) is chosen.

The resulting combination policy - first exploring for \(k\) episodes and then exploiting for \(n-k\) episodes - is then used as the inference policy \(_{}\) at test time, as shown in Equation 1.

\[_{}=_{},&\\ _{},& \]

## 5 Experimental Setup

First-Explore is implemented with a GPT-2-style causal transformer architecture , chosen for its strong temporal sequence modeling capabilities. To simplify the design, the explore and exploit policies share parameters, differing only in their final linear-layer head.

A novel sequence-modeling approach improved training stability in initial experiments, and was thus used for all First-Explore results. The method is described here and in the pseudocode provided in Appendix B. While this training method outperformed others in preliminary experiments, we believe the First-Explore meta-RL framework will work with alternate training approaches, such as PPO , Q-learning  and other RL algorithms.

### Training Method

The **exploit policy**\(_{}\) is trained to generate episodes that match or surpass the highest return achieved in prior episodes within the meta-rollout sequence. The **explore policy**\(_{}\) is trained to produce episodes that are followed by the exploit policy achieving higher episode returns than those seen so far. These training incentives implement the First-Explore framework: the exploit policy maximizes immediate episode returns, while the explore policy generates episodes that increase _subsequent_ exploit-episode returns.

Training involves periodically updating the rollout policies \((_{},_{})\) with successor versions. These successor policies \((_{},_{})\) are trained to model the actions taken in the good rollout-policy episodes, defined as 1. good exploit episodes meet or surpass previous exploit returns in the meta-rollout sequence, and 2. good explore episodes are followed by the exploit policy achieving higher episode returns than those seen so far. We term these good exploit episodes'maximal,' and the good explore episodes 'informative2'. Since the first exploit episode in each meta-rollout has no previous episodes for comparison, a baseline reward \(b\) initializes the list of prior returns. This value is set as a domain-specific hyperparameter (but could easily be set automatically via heuristics).

At the start of training, both rollout policies are initialized with random weights. They are then copied to produce the initial successor policies.

Training is structured into iterated epochs. Each epoch, the current exploit and explore policies \((_{},_{})\) produce batched meta-rollouts. In each batch, the exploit episodes \(_{}_{}\) that are _maximal_ and the explore episodes \(_{}_{}\) that are _informative_ are recorded. For these criteria-satisfying episodes, a cross-entropy loss is calculated between the successor policies \((_{},_{})\) and the action distributions in the associated _maximal_ or _informative_ episodes. The successor policy weights are then updated, using gradient descent on the loss. In this way the successor policies learn to emulate the conditioned rollout policies (Equation 2). Finally, every \(T\) epochs, the rollout policies \(_{},_{}\) are updated to match the successor policies \(_{},_{}\), ensuring continuous improvement. This hyperparameter \(T\) manages the trade-off between preserving behavioral diversity and accelerating learning iteration.

\[_{} _{}\] \[_{} _{} \]During training, actions are sampled from their predicted probability distributions \(a_{}\) or \(a_{}\). In contrast, during inference, actions are selected greedily.

## 6 Results

When obtaining maximum cumulative reward requires forgoing early-episode reward, cumulative-reward meta-RL algorithms fail to learn optimal behavior regardless of how long they are trained (Appendix F), as they become stuck on a local optimum of not exploring well. Even simple environments such as bandits flummox them. Three varied domains empirically demonstrate this dynamic. On all three a) the meta-RL controls perform poorly, and b) First-Explore significantly outperforms the controls. Further, two of the domains have modified versions that do not require forgoing immediate rewards, and this change causes significant control policy improvement3.

When forgoing immediate reward is required, First-Explore achieves 2x more total reward than the meta-RL controls in the first domain, 10x in the second, and 6x in the last (Table 1). These significant reward differentials reflect substantive behavioral differences, e.g., First-Explore exploring well (at the cost of reward) and then exploiting vs. the meta-RL methods converging to a policy of minimal exploration. First-Explore outperforming the other methods is also statistically significant, with \(p\)-values less than \(10^{-5}\) for each comparison, as calculated by two-tailed Mann-Whitney \(U\) tests (MWU).

**Domain 1: Bandits with One Fixed Arm:** The first domain is a multi-arm bandit problem designed to require forgoing immediate reward, where pulling the first bandit arm is immediately rewarding while also having no exploratory value. Pulling the first-arm yields a guaranteed reward of \(_{1}\), unlike the other arms, which - averaging across environments within the domain - yield expected reward 0. We consider two cases: \(_{1}=0.5\) (the deceptive case) and \(_{1}=0\) (the non-deceptive case).

For both values, the greatest cumulative reward can be reliably achieved by _first exploring_ to find the highest reward arm (with reward greater than \(_{1}\)) _and then exploiting_ by repeatedly sampling it. However, \(_{1}=0.5\) creates the deceptive challenge described in Section 1, where _before reliable exploitation_ has been learnt, exploration (i.e., not sampling arm 1 and obtaining its guaranteed reward) leads to a lower total reward. See Appendix C.1 for further details on this domain.

**Bandit Results (Figure 2):** First-Explore is evaluated against 3 baselines, with oracle performance (grey) and **random action selection (black)** plotted for additional reference.

**Deceptive Case:** The worst-performing control, RL\({}^{2}\)[8; 9] (fuchsia) is symbied by the deceptive domain dynamic. Four of five RL\({}^{2}\) training runs (overlapping bold fuchsia) learn to only sample the guaranteed reward, achieving a constant 0.5 reward each pull (Figure 2-B1), and exactly 50 reward after 100 pulls (Figure 2-A1). The remaining run (faint fuchsia) does better but still poorly. The best returns are achieved by methods that balance exploration (finding the most rewarding arm), and exploitation (pulling the best arm found so far). First-Explore (green) and the non-meta-RL bandit

   &  &  \\  First-Explore & \(\) & First-Explore & \(\) & First-Explore & \(\) \\  RL\({}^{2}\) & \(56.1 12.2\) & RL\({}^{2}\) & \(0.2 0.1\) & RL\({}^{2}\) & \(0 0\) \\  UCB-1\({}^{}\) & \(116.8 0.5\) & VariBAD & \(0.2 0.1\) & VariBAD & \(0 0\) \\  TS\({}^{}\) & \(123.3 2.5\) & HyperX & \(-0.2 0.2\) & HyperX & \(0.07 0.07\) \\  Random Action & \(5.2 0.2\) & Random Action & \(-5.5 0.1\) & Random Action & \(-0.23 0.1\) \\  

Table 1: Mean cumulative reward \(\) standard deviation of First-Explore compared against control algorithms in hard-to-explore domains, with random action (picking actions uniformly at random at each timestep) added for additional reference. In each domain, First-Explore significantly outperforms meta-RL controls. The bandit domain compares to two non-meta-RL baselines, marked \(\).

controls, Thompson Sampling  (orange) and UCB  (purple), both exhibit this behavior. Interestingly, First-Explore achieves the balance differently from the bandit algorithms, with the average pull rewards transitioning from close to zero, to close to optimal sharply around 10 pulls (Figure 2-B1). Impressively, First-Explore achieves greater reward than even the specialized bandit algorithms despite being applicable to more general domains (\(p<10^{-5}\), as calculated by MWU). HyperX and VariBAD are not included as baselines because their performance was overly poor even on non-deceptive versions of this domain4. **Non-Deceptive Case:** When the domain is not deceptive (i.e., when exploration does not require forgoing immediate reward), RL\({}^{2}\) achieves high total-reward (Figure 2-A2), and samples arms similarly to the bandit algorithms (Figure 2-B2). All other algorithms perform well regardless of deception. This dynamic validates that the need to forgo immediate reward is what stymies cumulative-reward meta-RL performance.

Illustrating that SOTA cumulative-reward meta-RL can fail on such simple domains (i.e., where standard-RL can easily solve each individual environment) is a key contribution of this paper.

**Domain 2: Dark Treasure Rooms:** The second domain is a grid world environment, where the agent cannot see its surroundings. In each environment, there are multiple randomly positioned reward locations. When each location is encountered, the agent consumes it for that episode, receiving a reward \( U[,2]\). The agent receives only its current coordinate as an observation, and to explore the agent must move blindly into unobserved grid locations that it has not visited before.

We consider two values of \(\): \(=-4\) (the deceptive case) and \(=0\) (the non-deceptive case). For both values, high total reward is achieved by _first exploring_ (visiting unseen grid locations to find positive reward locations) and _then consistently exploiting_ (revisiting discovered positive rewards locations each episode). However, when \(=-4\) this process is challenging, as only \(\) of the reward locations are positive, and the expected reward of a positive location is only \(1\). These

Figure 2: Mean performance (averaged across sampled bandits) of algorithms for deceptive (left) and non-deceptive (right) versions of the bandit domain. Each method trained 5 independent times, and each such run is plotted individually, so as to faithfully represent the variance between runs (e.g., that multiple of the bandit-domain RL\({}^{2}\) training runs achieve exactly the same reward). Appendix C provides alternative plots with mean reward \(\) standard deviation. The top figures plot the cumulative reward against the number of arm pulls, while the bottom figures illustrate the reward dynamics by plotting the individual pull rewards against the same. When the domain is deceptive, the cumulative-reward meta-RL method, RL\({}^{2}\) (fuchsia), performs extremely poorly, despite the deceptive domain giving strictly _higher_ rewards than the non-deceptive version. In contrast, First-Explore (green) impressively outperforms UCB (purple) and Thompson Sampling (orange) despite them being specialized bandit algorithms, in both the deceptive and non-deceptive settings, with \(p<10^{-5}\).

expectations create the challenge described in Section 1, where one must consistently exploit (by revisiting discovered positive reward objects more than three times) for any initial exploration to be worthwhile. See Appendix C.2 for an explanation of these properties, along with further domain details. In contrast, when \(=0\), visiting unseen locations provides positive expected reward causing immediate rewards to incentivize (rather than deter) initial exploration.

**Dark Treasure Room Results (Figure 3):** First-Explore is evaluated against three meta-RL controls, with random action selection (grey) plotted for additional reference. **Deceptive Case:** The meta-RL algorithms RL2(**fuchelia**) and VariBAD (pale illac) all achieve close to zero reward (Figure 3-A1), and rarely move (Figure 3-B1). HyperX explores more but minimally, as reflected by the modest negative first episode return5. However, the method fails to then exploit, and thus under-performs the other methods, despite this exploration.

By first exploring and then exploiting, First-Explore (green) achieves 10x more total reward in the ten-episode setting than the meta-RL controls. Impressively, despite the high cost of exploration (with a single episode of exploration taking more than three subsequent exploitations to yield positive expected reward), First-Explore is sufficiently skilled that its policy of exploring twice and then exploiting achieves more total-reward than a hand-coded policy of _optimally exploring_ in the first episode, and then _optimally exploiting_ conditioned on this initial exploration6 (\(2.0>1.78\)). **Non-Deceptive Case:** the meta-RL controls achieve reasonable total-reward (Figure 3-A2) and move consistently (Figure 3-B2), demonstrating that it is the difficulty of forgoing immediate rewards that challenges SOTA methods. However, First-Explore still outperforms them, with \(p<10^{-5}\) (MWU).

**Domain 3: Ray Maze:** The final domain is significantly more complex than the previous ones, and demonstrates that First-Explore can scale beyond bandit and grid-world problems. The domain is composed of randomly generated mazes with 3 reward locations. The agent must parse a large number of lidar observations (see Figure 4) to navigate around walls, identify reward locations, and move onto them. Each episode, the first reward location visited gives a reward of either \(-1\) or \(+1\), each reward location having an independent \(30\%\) chance of positive reward. The agent has three discrete actions: rotate right, rotate left, and move forward. One complexity is that (unlike the

Figure 3: Mean performance (averaged across sampled treasure rooms) of algorithms for deceptive (left) and non-deceptive (right) versions of the Dark Treasure Room domain. Each method trained 5 independent times, and each such run is plotted individually. The top figures plot the cumulative reward obtained against step and episode number, while the bottom figures provide a proxy for exploration by plotting the number of times agents move against the same. When the domain is deceptive, the cumulative-reward meta-RL methods, RL2 (fuchelia), HyperX (brown), and VariBAD (purple) achieve low total-reward, as the policies learnt to minimize exploration. In contrast, First-Explore (green) performs well on both the deceptive and non-deceptive domains.

grid-world domain), actions do not commute (e.g., rotating left and then moving forward is different from moving forward and then rotating left). The agent can see the reward locations, but cannot tell by looking whether they give positive or negative reward. Optimal behavior requires _first exploring_ (navigating to visit un-visited reward locations, and obtaining their negative expected reward), and _then reliably exploiting_ (re-visiting a reward location if it gave positive reward). The need to sacrifice immediate reward (by visiting the negative reward locations) challenges the SOTA cumulative-reward baselines.

**Ray Maze Results (Figure 5):** On this more complex domain, First-Explore similarly achieves significantly more reward than the meta-RL controls, with all controls failing to perform well. Two of the three meta-RL controls, RL\({}^{2}\) (fuchsia) and \(\) (purple), learn to never move (and instead spin in-place). This behavior avoids encountering any reward location, and thus results in exactly 0 total reward in every evaluation of all five independent training runs. For the final meta-RL control, four of five \(\) (brown) runs move little (achieving near zero reward), with the remaining run achieving only modest cumulative reward.

**First-Explore** (green) significantly outperforms the controls, achieving 0.47 mean reward (Table 1), by exploring to find a reward location, remembering it and reliably exploiting by navigating to that location if it gave positive reward. Figure 5 reflects this process, illustrating that First-Explore's cumulative reward goes down substantially before then going up. For comparison, the optimum possible behavior of exploring and exploiting perfectly achieves at most 0.64 average reward7.

## 7 Limitations and Future Work

As presented, First-Explore does not actively explore to enable future exploration, because the explore policy \(_{}\) only trains to increase the expected reward of the _subsequent exploit policy_ episode. Unfortunately, a sequence of optimal myopic explorations is not necessarily an optimal exploration sequence (Appendix G). A potential solution is to reward exploration episodes based on a weighted sum of rewards from all subsequent exploitation episodes, analogous to summing discounted future rewards in standard RL.

Another limitation is that First-Explore could be unsafe in certain environments. The risk is that First-Explore's explore policy does not avoid negative rewards, and so penalizing unsafe action has minimal effect on the policy. For example, a chef robot attempting a new recipe in a physical home

Figure 4: **Left:** Raw agent observations from a sampled ray maze converted to an image. The agent receives the wall distances and the wall types. Portraying this numerical data as an image, goal locations are \(\), and the two wall orientations are distinguished (east-west teaI, and north-south navy). To aid the eye, the floor has been coloured in **dark purple**, and the sky yellow. Although the goal is visible, it could be a treasure (positive reward) or trap (negative reward). **Right:** The image produced with direct ray casting (large number of processed lidar measurements) rather than the 15 the agent receives.

Figure 5: Mean performance averaged across 1000 Ray Mazes for five runs of each treatment. First-Explore strongly outperforms the SOTA meta-RL baselines on this complex environment, achieving a mean 0.47 reward, only slightly worse than the expected total-reward of behaving optimally, 0.64.

might explore mistakes that endanger humans or damage the kitchen (despite negative rewards telling it not to). This concern only applies to a subset of environments, as many environments are safe, e.g., simulated ones. That said, addressing this concern is a valuable direction of future work. One potential solution is to add a safety penalty to the explore policy, i.e., train the explore policy to maximize _subsequent_ exploit episode-returns while also avoiding safety risks. This proposed version of First-Explore could actually produce in-context adaption that is safer than standard RL training, as the meta-RL policies would have learnt a strong prior of avoiding potentially dangerous actions.

This paper makes no claims regarding First-Explore's meta-training efficiency, and future work may substantially improve meta-training time. Instead, this paper a) identifies a challenge that SOTA meta-RL algorithms fail on regardless of training time (the need to forgo immediate rewards), and b) provides a framework, First-Explore, that can solve these challenging domains.

A final problem is the challenge of long sequence modelling, with certain environments requiring a huge context and high compute (e.g., can one have a large enough context, and enough compute to allow First-Explore to generalize and act as a replacement for standard RL?). AdA  suggests such a project might be possible. Progress on efficient long-context sequence modelling [18; 19], research on RL transformer applications [20; 21], and Moore's Law all make this possibility more feasible.

Additionally, given that First-Explore learns a dedicated explore policy and a dedicated exploit policy, and both have been shown to work well (e.g., Figure 2), the method may be applicable to final-episode-reward meta-RL settings (Appendix I). Further, First-Explore always switches from exploration to exploitation after a fixed number of episodes. Future work could replace this fixed number with a learnt classifier that determines when to switch from exploration to exploitation, e.g., repeatedly exploring _until_ sufficient information is obtained. However, despite these applications being highly exciting directions of future work, a proper investigation of either would require its own paper, including many different specialized controls and environments pertinent to the setting.

## 8 Discussion

Given that First-Explore uses RL algorithms to train the meta-RL policy, how might it solve hard-exploration problems that standard RL cannot, e.g. design a rocket for the first time? We believe that given a suitably advanced curriculum, and sufficient compute, First-Explore could learn powerful exploration heuristics (i.e., develop intrinsic motivations such as an analogue of curiosity) and that these heuristics would enable sample-efficient performance on hard sparse-reward problems. On a curriculum, initially First-Explore would explore randomly, and learn to exploit based on that random exploration. Once it has learnt rudimentary exploitation, the agent can learn rudimentary exploration. Then it would learn better exploitation and exploration, and so on, each time relying on there being 'goldilocks zone' tasks  that are not too hard and not too easy.

Further, while curricula can aid all of meta-RL, e.g., RL\({}^{2}\) and AdA, First-Explore can have a significant training advantage on certain problems (e.g., in the ten-episode Dark Treasure-Room, First-Explore achieves positive cumulative reward while the standard cumulative-reward meta-RL methods catastrophically fail). This advantage could potentially allow far greater compute efficiency, and allow training on otherwise infeasible curricula.

## 9 Conclusion

We have theorized and demonstrated that SOTA cumulative-reward meta-RL fail to train when exploration requires forgoing immediate reward. Even simple problems such as bandits (Figure 1) can stymie these methods. To overcome this challenge, we introduce a novel meta-RL framework, First-Explore, that learns two separate interleaved policies: one to first explore, another to then exploit. By combining the policies at inference time, First-Explore is able to explore effectively and achieve high cumulative reward on problems that hamstring SOTA methods.

Meta-RL shows the promise of finally fixing the main problem in RL - that it is extremely sample inefficient - even producing _human-level sample efficiency_. However, the promise of this approach is limited, as we have shown, on a large set of important problems. We can only take advantage of this approach if we can harness the benefits of meta-RL even on such problems, and First-Explore enables us to do so, thus offering an important and exciting opportunity for the field.