# Inversion-based Latent Bayesian Optimization

Jaewon Chu, &Jinyoung Park, &Seunghun Lee, &Hyunwoo J. Kim

Computer Science & Engineering

Korea University

{allonsy07, lpmn678, llsshh319, hyunwoojkim}@korea.ac.kr

equal contributionsCorresponding author

###### Abstract

Latent Bayesian optimization (LBO) approaches have successfully adopted Bayesian optimization over a continuous latent space by employing an encoder-decoder architecture to address the challenge of optimization in a high dimensional or discrete input space. LBO learns a surrogate model to approximate the black-box objective function in the latent space. However, we observed that most LBO methods suffer from the'misalignment problem', which is induced by the reconstruction error of the encoder-decoder architecture. It hinders learning an accurate surrogate model and generating high-quality solutions. In addition, several trust region-based LBO methods select the anchor, the center of the trust region, based solely on the objective function value without considering the trust region's potential to enhance the optimization process. To address these issues, we propose **In**version-based **L**atent **B**ayesian **O**ptimization (InvBO), a plug-and-play module for LBO. InvBO consists of two components: an inversion method and a potential-aware trust region anchor selection. The inversion method searches the latent code that completely reconstructs the given target data. The potential-aware trust region anchor selection considers the potential capability of the trust region for better local optimization. Experimental results demonstrate the effectiveness of InvBO on nine real-world benchmarks, such as molecule design and arithmetic expression fitting tasks. Code is available at https://github.com/mlvlab/InvBO.

## 1 Introduction

Bayesian optimization (BO) has been used in a wide range of applications such as material science , chemical design [2; 3],,and hyperparameter optimization [4; 5]. The main idea of BO is probabilistically estimating the expensive black-box objective function using a surrogate model to find the optimal solution with minimum objective function evaluation. While BO has shown its success on continuous domains, applying BO over discrete input space is challenging [6; 7]. To address it, Latent Bayesian Optimization (LBO) has been proposed [8; 9; 10; 11; 12; 13; 14]. LBO performs BO over a latent space by mapping the discrete input space into the continual latent space with generative models such as Variational Auto Encoders (VAE) , consisting of an encoder \(q_{}\) and a decoder \(p_{}\). Unlike the standard BO, the surrogate model in LBO associates a latent vector \(\) with an objective function value by emulating the composition of the objective function and the decoder of VAE.

In LBO, however, the reconstruction error of the VAE often leads to one latent vector \(\) being associated with two different objective function values as explained in Figure 1. We observe that the discrepancy between \(y\) and \(y^{}\) (or \(\) and \(^{}\)) hinders learning an accurate surrogate model \(g\) and generating high-quality solutions. We name this the'misalignment problem'. Most prior works [9; 11; 12] use the surrogate model \(g^{}\), which is trained with the encoder triplet \((,,y)\), andgenerate solutions \(^{}\) via decoder \(p_{}\). Since \(g^{}\) fails to estimate the composite function of \(p_{}\) and \(f\), this approach often results in suboptimal outcomes. Some works [13; 14] handle the misalignment problem by employing the surrogate model \(g^{}\) trained with the decoder triplet \((^{},,y^{})\). However, they request a huge amount of additional oracle calls to obtain the decoder triplet, which leads to inefficient optimization. In addition, several existing LBO methods [13; 14; 16] adopt the trust region method to restrict the search space and have shown performance gain. Most prior works select the anchor, the center of the trust region, as the current optimal point. This objective function value-based anchor selection overlooks the potential to benefit the optimization performance of the latent vectors within the trust region.

In this work, we propose an **In**version-based Latent **B**ayesian **O**ptimization (**InvBO**), a plug-and-play module for VAE-based LBO methods. InvBO consists of two components: the inversion method and a potential-aware trust region anchor selection. The inversion method addresses the misalignment problem by inverting decoder \(p_{}\) to find the latent code that yields \(\) without any additional oracle call. We theoretically analyze that our inversion method decreases the upper bound of the error between the surrogate model and the objective function within the trust region. The potential-aware trust region anchor selection method selects the anchor considering not only the observed objective function value but also the potential to enhance the optimization process of the latent vectors that the trust region contains. We provide the experimental evaluation on nine different tasks, Guacamol, DRD3, and arithmetic expression fitting task to show the general effectiveness of InvBO. Specifically, plug-and-play results of InvBO over diverse prior LBO works show a large performance gain and achieved state-of-the-art performance.

The contributions of our paper are as follows:

* We propose the inversion method to address the misalignment problem in LBO by generating the decoder triplet without using any additional oracle calls.
* We propose the potential-aware trust region anchor selection, aiming to select the centers of trust regions considering the latent vectors expected to benefit the optimization process within the trust regions.
* By combining the inversion method and potential-aware trust region anchor selection, we propose Inversion-based Latent Bayesian Optimization (InvBO), a novel plug-and-play module for LBO, and achieve state-of-the-art performance on the nine different tasks.

## 2 Related Works

### Latent Bayesian Optimization

The goal of Latent Bayesian Optimization (LBO) [11; 12; 13; 16; 17; 18; 9; 8; 19] is to learn a latent space to enable optimization over a continuous space from discrete or structured input (_e.g.,_ graph or image). LBO consists of a Variational AutoEncoder (VAE) to generate data from the latent representation and a surrogate model (_e.g.,_ Gaussian process) to map the latent representation into the objective score. Some works on the LBO have designed new decoder architectures [20; 21; 22; 8; 23] to perform the

Figure 1: **Misalignment problem. In LBO, a latent vector \(\) can be associated with two function values \(y\) and \(y^{}\) due to the reconstruction error of the VAE, _i.e._, \(^{}\). (a) In Encoder triplet \((,,y)\), latent vector \(\) is associated with \(f()\), where \(\) is the original input to the encoder, _i.e._, \(=q_{}()\). (b) In Decoder triplet \((^{},,y^{})\), \(\) is associated with \(y^{}=f(^{})\), which is the objective function value of reconstructed input value \(^{}\) using the decoder, _i.e._, \(^{}=p_{}()\). The discrepancy between \(y\) and \(y^{}\) hinders learning the accurate surrogate model \(g\). We name this the ‘misalignment problem’.**

reconstruction better, while other works have proposed learning mechanisms [9; 10; 11; 12; 13; 14; 17] to alleviate the discrepancy between the latent space and input space. LOL-BO  adapts the concept of trust-region to the latent space and jointly learns a VAE and a surrogate model to search the data point in the local region. CoBO  designs new loss to encourage the correlation between the distances in the latent space and objective function.

### Inversion in Generative Models

Inversion has widely been applied to a variety of generative models such as Generative Adversarial Networks (GANs) [24; 25] and Diffusion models [26; 27; 28; 29]. Inversion is the process of finding the latent code \(_{}\) of a given image to manipulate images with generative models. Formally, given an image \(\) and the well-trained generator \(G\), the inversion can be written as:

\[_{}=*{arg\,min}_{}d _{}(G(),),\] (1)

where \(d_{}(,)\) denotes the distance metric in the image space \(\), and \(\) is the latent space. To solve Eq. (1), most inversion-based works can be generally classified as two approaches: optimization-based and learning-based methods. The optimization-based inversion [30; 31; 32; 33] iteratively finds a latent vector to reconstruct the target image \(\) through the fixed generator. The learning-based inversion [25; 34; 35] trains the encoder for mapping the image \(\) to the latent code \(\) while fixing the decoder. In this work, we introduce the concept of inversion to find the latent vector that can generate a desired sample for constructing an aligned triplet and we use the optimization-based inversion.

## 3 Preliminaries

**Bayesian optimization.** Bayesian optimization (BO) is a powerful and sample-efficient optimization algorithm that aims at searching the input \(\) with a maximum objective value \(f()\), which is formulated as:

\[^{*}=*{arg\,max}_{}f(),\] (2)

where the black-box objective function \(f:\) is assumed expensive to evaluate, and \(\) is a feasible set. Since the objective function \(f\) is unknown or cost-expensive, BO methods probabilistically emulate the objective function by a surrogate model \(g\) with observed dataset \(=\{(^{i},y^{i})|y^{i}=f(^{i})\}_{i=1}^{n}\). With the surrogate model \(g\), the acquisition function \(\) selects the most promising point \(^{n+1}\) as the next evaluation point while balancing exploration and exploitation. BO repeats this process until the oracle budget is exhausted.

**Trust region-based local Bayesian optimization.** Classical Bayesian optimization methods often suffer from the difficulty of the optimization in a high dimensional space . To address this problem, TuRBO  adopts trust regions to limit the search space to small regions. The anchor (center) of trust region \(\) is selected as a current optimal point, and the size of the trust region is scheduled during the optimization process. At the beginning of the optimization, the side length of all trust regions is set to \(L_{}\). When the trust region \(\) updates the best score \(_{}\) times in a row, the side length becomes twice until it reaches \(L_{}\). Similarly, when it fails to update the best score \(_{}\) times in a row, the side length becomes half. When \(L\) falls below a \(L_{}\), the side length of the trust region is set to \(L_{}\) and restart the scheduling. Recently, LOL-BO  adapted trust region-based local optimization to LBO, and has shown performance gain.

## 4 Method

In this section, we present an Inversion-based Latent Bayesian Optimization (InvBO) consisting of an inversion and a novel trust region anchor selection method for effective and efficient optimization. We first describe latent Bayesian optimization and the misalignment problem of it (Section 4.1). Then, we introduce the inversion method to address the misalignment problem without using any additional oracle budgets (Section 4.2). Lastly, we present a potential-aware trust region anchor selection for better local search space (Section 4.3).

### Misalignment in Latent Bayesian Optimization

BO has proven its effectiveness in various areas where input space \(\) is continuous, however, BO over the discrete domain, such as chemical design, is a challenging problem. To handle this problem, VAE-based latent Bayesian optimization (LBO) has been proposed  that leverages BO over a continuous space by mapping the discrete input space \(\) to a continuous latent space \(\). Variational autoencoder (VAE) is composed of encoder \(q_{}:\) to compute the latent representation \(\) of the input data \(\) and decoder \(p_{}:\) to generate the data \(\) from the latent \(\).

Given the objective function \(f\), latent Bayesian optimization can be formulated as:

\[^{*}=*{arg\,max}_{}f(p_{ }()),\] (3)

where \(p_{}()\) is a generated data with the decoder \(p_{}\) and \(\) is a latent space. Unlike the standard BO, the surrogate model \(g\) aims to emulate the function \(f p_{}:\). To the end, the surrogate model is trained with aligned dataset \(=\{(^{i},^{i},y^{i})\}_{i=1}^{n}\), where \(^{i}=p_{}(^{i})\) is generated by the decoder \(p_{}:\) and \(y^{i}=f(^{i})\) is the objective value of \(^{i}\) evaluated via the black box objective function \(f:\). In the rest of our paper, we define that the dataset is aligned when all triplets satisfy the above conditions (_i.e.,_ all triplets are the decoder triplets explained in Figure 1), and the dataset is misaligned otherwise. We define the'misalignment problem' as the misaligned dataset hinders the accurate learning of the surrogate model \(g\).

Most existing LBO works  overlook the misalignment problem 1, which originates from two processes: (i) construction of initial dataset \(^{0}\) and (ii) update of VAE.

**Construction of initial dataset \(^{0}\).** Since initial dataset \(^{0}\) is composed of pairs of input data and its corresponding objective value \(\{(^{i},y^{i})|y^{i}=f(^{i})\}_{i=1}^{n}\), LBO requires latent vectors \(\{^{i}\}_{i=1}^{n}\) to train the surrogate model. Most works compute a latent vector \(^{i}\) as \(^{i}=q_{}(^{i})\) under the assumption that VAE completely reconstructs every data points (_i.e.,_\(p_{}(q_{}(^{i}))=^{i}\)), which is difficult to be satisfied in every case. This results in the data misalignment (\(^{i} p_{}(^{i})\)) during the construction of initial dataset \(^{0}\).

**Update of VAE.** In LBO works, updating VAE during the optimization plays a crucial role in adapting well to newly generated samples. However, due to the update of VAE, the previously generated triplet \((,,y)\) cannot ensure the alignment since \(\) was computed by the VAE before the update. Previous LBO works  solve the misalignment problem originating from the VAE update with a recentering technique that requests additional oracle calls to generate the aligned dataset

Figure 3: (Left) The number of oracle calls to evaluate the queries selected by the acquisition function (blue) and during the recentering (Red). (Right) The number of objective function evaluation that updates the best score.

Figure 2: **Comparison of solutions to the misalignment problem. (a) Some works  solve the misalignment problem by the recentering technique that generates the aligned triplet \((^{},,y^{})\). However, it requests additional oracle calls as \(y^{}=f(^{})\) is unevaluated, and does not fully use the evaluated function value \(y=f()\). (b) The inversion method (ours) aims to find \(_{}\) that generates the evaluated data \(\) to get the aligned triplet \((,_{},y)\) without any additional oracle calls.**\(=\{(^{ i},q_{}(^{i}),f(^{ i }))\}_{i=1}^{n}\) where \(^{ i}=p_{}(q_{}(^{i}))\) as shown in Figure 2 (Left). But, it has a limitation to consuming a huge amount of additional oracle calls while they do not update the best score (Figure 3). Note that prior works do not explicitly mention the additional oracle calls during the recentering technique, but it can be verified by the official GitHub code. Further details about oracle consumption of the recentering are provided in the supplement Section H.

### Inversion-based Latent Bayesian Optimization

Our primary goal is training the surrogate model \(g\) to correctly emulate the composite function \(f p_{}:\) via constructing an aligned dataset without consuming additional oracle calls. To the end, we propose an inversion method that inverts the target discrete data \(\) into the latent vector \(\) that satisfies \(=p_{}()\) for dataset alignment as shown in Figure 2 (Right). With a pre-trained frozen decoder \(p_{}\), the latent vector \(\) can be optimized by:

\[_{}=*{arg\,min}_{}d_ {}(,p_{}()),\] (4)

where \(\) is a target data and \(d_{}\) is a distance function in the input space \(\). We use the normalized Levenshtein distance  as our distance function, \(d_{}\), which can be applied to any string-form data. Our inversion method, however, is flexible and can utilize any task-specific distance functions, such as Tanimoto similarity  for molecule design tasks. To solve the Eq. (4), we iteratively update a latent vector \(\) to find \(_{}\) that reconstructs the target data \(\). We provide the overall pseudocode of the inversion method in Algorithm 1.

```
0: Encoder \(q_{}\), decoder \(p_{}\), target data \(\), max iteration \(T\), distance function \(d_{}\), learning rate \(\), reconstruction loss \(\)
1: Initialize \(^{(0)} q_{}()\)
2:for\(t=0,1,...,T-1\)do
3:\(^{(t+1)}^{(t)}-_{^{(t)}} (p_{}(^{(t)}),)\)
4:if\(d_{}(,p_{}(^{(t+1)}))<\)then\(\) Eq. (4)
5:return\(^{(t+1)}\)
6:endif
7:endfor
8:return\(^{(T)}\) ```

**Algorithm 1** Inversion

The initialization strategy of latent vector \(\) plays a key role in the optimization-based inversion process. We set the initialization point of latent vector \(\) as an output of a pre-trained encoder \(q_{}()\) given target discrete data \(\) as in line 1. We iteratively update the latent vector \(\) with the cross-entropy loss used in VAE training in line 3 until it reaches the maximum number of iterations \(T\). Before the iteration budget is exhausted, we finish the inversion process when the distance between the generated data \(p_{}()\) and target data \(\) is less than \(\) as our goal is finding the latent vector \(_{}\) that satisfies Eq. (4), which is denoted in line 4. The inversion method generates the aligned dataset during the construction of the initial dataset \(^{0}\) and the update of VAE to handle the misalignment problem.

We theoretically show that optimizing the latent vector \(\) to satisfy \(d_{}(,p_{}()) 0\) with inversion plays a crucial role in minimizing the upper bound of the error between the posterior mean of the surrogate model and the objective function value within the trust region centered at \(\).

**Proposition 1**.: _Let \(f\) be a black-box objective function and \(m\) be a posterior mean of Gaussian process, \(p_{}\) be a decoder of the variational autoencoder, \(c\) be an arbitrarily small constant, \(d_{}\) and \(d_{}\) be the distance function on input \(\) and latent \(\) spaces, respectively. The distance function \(d_{}\) is bounded between 0 and 1, inclusive. We assume that \(f\), \(m\) and the composite function of \(f\) and \(p_{}\) are \(L_{1},L_{2}\), and \(L_{3}\)-Lipschitz continuous functions, respectively. Suppose the following assumptions are satisfied:_

\[|f()-m()|& c,\\ d_{}(,p_{}())& .\] (5)

_Then the difference between the posterior mean of the arbitrary point \(^{}\) in the trust region centered at \(\) with trust radius \(\) and the black box objective value is upper bounded as:_

\[|f(p_{}(^{}))-m(^{})| c+ L _{1}+(L_{2}+L_{3}),\] (6)

_where \(d_{}(,^{})\)._

The proof is available in Section A. We assume that the black box function \(f\), the posterior mean of Gaussian process \(m\), and the objective function \(f p_{}\) are Lipschitz continuous functions, which is a 

[MISSING_PAGE_FAIL:6]

### Baselines

We compare to six latent Bayesian Optimization methods: LS-BO, TuRBO , W-LBO , LOL-BO , CoBO , and PG-LBO . We also compare with GB-GA , a widely used genetic algorithm for graph structure data. LOL-BO and CoBO employ the decoder triplet generated by the recentering technique, and the other baselines employ the encoder triplet during the optimization process. In the case of LOL-BO and CoBO, we substitute the recentering technique with the inversion method. We provide the details of each baseline in Section L.

### Implementation Details

For the arithmetic expression fitting task, we follow other works [8; 11; 13; 14] to employ Grammar-VAE model . For the _de novo_ molecule design tasks such as Guacamol benchmark and DRD3 tasks, we use SELFIES VAE  following recent works [13; 14]. In all tasks, we adopt sparse variational GP  with deep kernel  as our surrogate model. In the inversion method, the learning rate is set to 0.1 in all experiments. The further implementation details are provided in Section M.

### Experimental Results

We apply our InvBO to several trust region-based LBOs, TuRBO-\(L\), LOL-BO  and CoBO , and provide optimization results on two Guacamol benchmark tasks, med2 and valt. All results are average scores of ten runs under the identical settings. Table 1 demonstrates that our InvBO consistently improves all LBO models in two tasks by a large margin. In particular, in the valt task, all baseline models with InvBO demonstrate significant performance improvements and CoBO with InvBO achieves a 0.348 score gain while the baseline models without InvBO fail in optimization. More results of other baselines with InvBO on other tasks are provided in Section E.

Figure 4 provides the optimization results on four Guacamol benchmark tasks including med2, zale, osmb, and valt. Each subfigure shows the number of evaluations of the black box objective function (Number of Oracle) and the corresponding average and standard error of the objective score (Best Score). Our InvBO built on CoBO achieves the best performance in all four tasks. Further results of other tasks are provided in Section F.

We also conduct experiments to demonstrate the effectiveness of our InvBO on DRD3 and arithmetic expression fitting tasks and large-budget settings. The experimental results are illustrated in Fig

   Task &  &  \\  Num Oracle & 100 & 300 & 500 & 100 & 300 & 500 \\   TuRBO-\(L\) & 0.186\(\)0.000 & 0.186\(\)0.000 & 0.186\(\)0.000 & 0.000\(\)0.000 & 0.000\(\)0.000 & 0.000\(\)0.000 \\ TuRBO-\(L\) + InvBO & 0.186\(\)0.000 & **0.194\(\)0.002** & **0.202\(\)0.001** & 0.000\(\)0.000 & **0.024\(\)0.017** & **0.212\(\)0.092** \\  LOL-BO & 0.186\(\)0.000 & 0.186\(\)0.000 & 0.190\(\)0.001 & 0.000\(\)0.000 & 0.000\(\)0.000 & 0.000\(\)0.000 \\ LOL-BO + InvBO & **0.189\(\)0.002** & **0.204\(\)0.005** & **0.227\(\)0.010** & 0.000\(\)0.000 & **0.007\(\)0.005** & **0.171\(\)0.039** \\  CoBO & 0.186\(\)0.000 & 0.188\(\)0.002 & 0.191\(\)0.003 & 0.000\(\)0.000 & 0.000\(\)0.000 & 0.000\(\)0.000 \\ CoBO + InvBO & **0.187\(\)0.001** & **0.203\(\)0.004** & **0.214\(\)0.006** & 0.000\(\)0.000 & **0.042\(\)0.013** & **0.348\(\)0.107** \\   

Table 1: Optimization results of applying InvBO to several trust region-based LBOs on Guacamol benchmark tasks. A higher score is a better one.

Figure 4: Optimization results on Guacamol benchmark tasks. The lines and ranges indicate the average and standard error of ten runs under the same settings. A higher score is a better score.

ure 5(a) and Figure 5(b), respectively. Please note that both DRD3 and arithmetic expression tasks aim to minimize the score while the goal of Guacamol tasks is increasing the score. Figure 5 shows that our InvBO applied to CoBO achieves the best performance. We provide further optimization results on other tasks under the large budget settings in Section G. These results demonstrate that InvBO is effective in diverse tasks and settings.

## 6 Analysis

### Ablation Study

We conduct additional experiments to verify the contribution of each component in our InvBO: the inversion method (INV), and the potential-aware trust region anchor selection method (PAS). Figure 6 shows the optimization results of the ablation study on med2, zale, osmb, and valt tasks. From the figure, models with the inversion method (_i.e.,_ CoBO with INV and InvBO) replace the recentering technique as the inversion method, while models without the inversion method (_i.e.,_ vanilla CoBO and CoBO with PAS) employ the recentering technique. Notably, both components of our method contribute to the optimization process, and the combination work, InvBO, consistently obtains better objective scores compared to other models. Specifically, in osmb task, the average best score achieved by the methods with the PAS, INV and both (_i.e.,_ InvBO) shows 0.784, 0.792, and 0.804 score gains compared to vanilla CoBO, respectively.

### Analysis on Misalignment Problem and Inversion

To further prove that the misaligned dataset hinders the accurate learning of the surrogate model (_i.e.,_ misalignment problem), we compare the performance of the surrogate model trained with aligned and misaligned datasets on the med2 task. Figure 7 shows the fitting results of the surrogate model trained with encoder triplets (\(g^{}\), left) and decoder triplets (\(g^{}\), right), respectively. Figure 7 demonstrates that the surrogate model \(g^{}\) approximates the objective function accurately, while \(g^{}\) fails to emulate the objective function. Further details of an experiment are provided in Section I.

In Figure 8, we compare the optimization results of CoBO using decoder triplets and encoder triplets on the valt and med2 tasks. Both models use the potential-aware anchor selection, and the decoder triplets are made by our inversion method. CoBO using the decoder triplets shows superior optimization performance over the CoBO using the encoder triplets on both tasks. This implies that

Figure 5: Optimization results on various tasks and settings. Note that: (a) A lower score is a better score. (b) A higher score is a better score.

Figure 6: Optimization results of component ablation on zale, med2, osmb, and valt.

the misaligned dataset hinders the accurate learning of the surrogate model, which leads to suboptimal optimization results, and the inversion method handles the problem by generating the decoder triplet.

### Effects of Inversion on Proposition 1

In Section 4.2, we provide the upper bound of the error between the predicted and ground-truth objective value. In Eq. (6), the Lipschitz constant of the objective function \(L_{1}\) and the trust region radius \(\) is fixed or the hyper-parameter and the constant \(c\) can be improved by learning the surrogate model. In the end, we can reduce the upper bound of the objective value prediction error by reducing the three components: \(\), \(L_{2}\), and \(L_{3}\), which implies the distance between \(\) and \(p_{}()\), the Lipschitz constant of function \(m\) and \(f p_{}\), respectively. Our inversion method reduce \(\) by searching the latent vector \(_{}\) that satisfies \(=p_{}(_{})\). Previously, CoBO  proposed regularization losses that reduce \(L_{3}\). Since the surrogate model emulates the composite function \(f p_{}\), these regularization losses can reduce \(L_{2}\) along with \(L_{3}\).

Figure 10 (left) shows the regularization losses of CoBO and our inversion method reduces the objective value prediction error. CoBO-based models (_i.e.,_ CoBO+PAS and CoBO+InvBO) employ the CoBO regularization losses, and models with InvBO (_i.e.,_ TuRBO-\(L\)+InvBO and CoBO+InvBO) employ our inversion method. TuRBO-\(L\) does not use the CoBO regularization losses nor our inversion method, and models with PAS (_i.e.,_ TuRBO-\(L\)+PAS and CoBO+PAS) employ encoder triplets. Applying the regularization losses and our inversion method reduces the objective value prediction error, respectively, but our inversion method shows a larger error reduction. The combination of regularization losses and our inversion shows the smallest prediction error, which implies our inversion method robustly complements existing methods. We provide the optimization results of each model on the med2 task in Figure 10 (right). These results demonstrate that reducing the objective function prediction error plays a crucial role in optimization performance.

### Comparing Diverse Anchor Selection Methods

To further prove the importance of our potential-aware anchor selection method, we perform BO with the diverse anchor selection methods: random, acquisition \(\), and objective score \(y\). Random indicates randomly selected anchors, and acquisition and objective indicate anchors are selected based on the max acquisition function value and objective score, respectively. All models use the inversion method, and the optimization results on valt and med2 tasks are in Figure 10. Ours and objective score-based anchor selection rapidly find high-quality data compared to the random and acquisition-based selections. However, objective score-based anchor selection shows inferior performance than our potential-aware anchor selection. This indicates that both the uncertainty of the surrogate model and objective function value need to be considered for exploration and exploitation.

## 7 Conclusion

We propose Inversion-based Latent Bayesian Optimization (InvBO), a plug-and-play module for LBO. We introduce the inversion method that inverts the decoder to find the latent vector for generating the aligned dataset. Additionally, we present the potential-aware trust region anchor selection that considers not only the corresponding objective function value of the anchor but also the potential ability of the trust region. From our experimental results, InvBO achieves state-of-the-art performance on nine LBO benchmark tasks. We also theoretically demonstrate the effectiveness of our inversion method and provide a comprehensive analysis to show the effectiveness of our InvBO.

## Broader Impacts

One of the contributions of this paper is molecular design optimization, which requires careful consideration due to its unintentional applications such as the generation of toxic. We believe that our work has a lot of positive aspects to accelerate the development of chemical and drug discovery with an inversion-based latent Bayesian optimization method.

## Limitations

The performance of the methods proposed in the paper depends on the quality of the generative model. For example, if the objective function is related to the molecule property, the generated model such as the VAE should have the ability to generate the proper molecule to have good performance of optimization.