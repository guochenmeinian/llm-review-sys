# Grid4D: 4D Decomposed Hash Encoding for

High-Fidelity Dynamic Gaussian Splitting

 Jiawei Xu\({}^{1}\)

Corresponding authors.

Zexin Fan\({}^{1}\)

Jian Yang\({}^{1}\)

Corresponding authors.

Jin Xie\({}^{2}\)

\({}^{1}\)PCA Lab, VCIP, College of Computer Science, Nankai University

\({}^{2}\)State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

\({}^{3}\)School of Intelligence Science and Technology, Nanjing University, Suzhou, China

\({}^{}\){jiaweixu, zexin_fan}@mail.nankai.edu.cn csjxie@nju.edu.cn csjyang@nankai.edu.cn

###### Abstract

Recently, Gaussian splatting has received more and more attention in the field of static scene rendering. Due to the low computational overhead and inherent flexibility of explicit representations, plane-based explicit methods are popular ways to predict deformations for Gaussian-based dynamic scene rendering models. However, plane-based methods rely on the inappropriate low-rank assumption and excessively decompose the space-time 4D encoding, resulting in overmuch feature overlap and unsatisfactory rendering quality. To tackle these problems, we propose Grid4D, a dynamic scene rendering model based on Gaussian splatting and employing a novel explicit encoding method for the 4D input through the hash encoding. Different from plane-based explicit representations, we decompose the 4D encoding into one spatial and three temporal 3D hash encodings without the low-rank assumption. Additionally, we design a novel attention module that generates the attention scores in a directional range to aggregate the spatial and temporal features. The directional attention enables Grid4D to more accurately fit the diverse deformations across distinct scene components based on the spatial encoded features. Moreover, to mitigate the inherent lack of smoothness in explicit representation methods, we introduce a smooth regularization term that keeps our model from the chaos of deformation prediction. Our experiments demonstrate that Grid4D significantly outperforms the state-of-the-art models in visual quality and rendering speed. Project page: https://jiaweixu8.github.io/Grid4D-web/.

## 1 Introduction

Dynamic scene rendering aims to construct dynamic scenes from images with specific camera poses and timestamps, allowing rendering from arbitrary viewpoints and moments. Traditional methods use Neural Radiance Field (NeRF)  and deformation fields to reconstruct dynamic scenes for arbitrary rendering. However, these works rely on predicting deformations with the over-smooth full Multilayer Perceptron (MLP) [31; 40; 48; 10; 19; 28; 55; 47; 16; 5; 18; 1; 38; 23; 45; 21; 4; 53], resulting in slow training speeds and artifacts in rendering quality. To address these challenges, explicit representations such as planes  and hash encoding  have been introduced to enhance the rendering of dynamic scenes [33; 9; 2; 46; 8; 41; 11; 35; 36]. The explicit representations store the intermediate features generated by the partial forward propagation process in a grid-like format. This approach allows us to obtain intermediate features by directly interpolating the cached features based on the input, bypassing the need for the full forward propagation process. In addition to reducing computing resource consumption, the inherent flexibility of explicit representation offers advantages in rendering more complex scenes.

Recently, Gaussian splatting  achieved fast and high-fidelity rendering of static scenes. Additionally, many works have employed Gaussian splatting for dynamic scene rendering by deforming Gaussians based on the timestamp . Deforming Gaussians through pre-defined functions is an effective way to reconstruct dynamic scenes with sufficient viewpoints . Additionally, implicit and explicit neural networks are more popular for deforming Gaussians in general cases . However, fully MLP-based implicit neural networks have limited learning capacity because of their over-smooth inherent property, thereby struggling to render several complex scenes and details effectively. Hence, explicit representation might be an available method to address these problems. Prior works such as 4D-GS  use plane-based explicit representations to predict Gaussian deformations, decomposing the 4D space-time encoding into a format comprising six 2D planes, but the performance remains unsatisfactory. We consider that the plane-based methods for Gaussian deformation prediction are based on the low-rank assumption which assumes that the features for the deformations have a great deal of commonality and could be factorized into a very low-rank format . As shown in Figure 2, when facing Gaussians with massive overlapping coordinates, the over-decomposition makes the features have excessive overlap which limits their discriminability for deformation prediction. Therefore, such overlap might block the model from predicting different deformations, resulting in low rendering quality.

To address these problems, we present Grid4D, a novel model with high dynamic scene rendering quality based on Gaussian splatting . Our approach leverages hash encoding  and proposes a new explicit representation method. Unlike the plane-based explicit representations relying on the unsuitable low-rank assumption, as shown in Figure 1, we decompose the 4D encoding into one spatial 3D hash encoding and three temporal 3D hash encodings. Figure 2 illustrates our proposed 4D decomposed hash encoding reduces overlap arising from the over-decomposed plane-based methods, resulting in more discriminative features. Notably, our encoder generates two types of features: spatial features, representing static information across the timeline, and temporal features, capturing dynamic information. For aggregation, we design a novel attention mechanism, directional attention, which leverages spatial features to generate attention scores in a directional range. This directional attention aligns with the observation that deformation consistency within each scene component often varies across different components, and the attention from the spatial features could better help the model fit such differences. However, like other explicit representation models, Grid4D often lacks smoothness. To address this issue, we propose a novel training strategy incorporating smooth regularization which mitigates chaotic deformation predictions to enhance rendering clarity.

We compare Grid4D with several state-of-the-art dynamic scene rendering models. Figure 1 and the experimental results show that Grid4D outperforms other models significantly in both visual quality and rendering speed. In general, the contribution of this paper can be summarized as the following.

* We propose a novel explicit representation method for dynamic scene rendering. By decomposing the 4D encoding into four 3D encodings, our 4D decomposed hash encoder effectively represents the features without relying on the low-rank assumption.
* We design a novel attention module for spatial and temporal feature aggregation. The directional attention module aligns with the variations in deformation consistency across different scene components, thereby enhancing deformation prediction accuracy.

Figure 1: We propose a novel explicit representation method for dynamic scene rendering that decomposes the space-time 4D encoding into the 3D format without the unsuitable low-rank assumption. We achieve significant improvements over the state-of-the-art models  in rendering quality.

* We employ a smooth training strategy to ensure the smoothness of our model. The smooth regularization effectively mitigates chaotic deformation predictions, resulting in high clarity in the rendered images produced by Grid4D.

## 2 Related Works

**NeRF-Based Dynamic Scene Rendering.** NeRF  reconstructs light fields of static scenes by implicit representations and achieves significant visual improvements. To extend NeRF capabilities for reconstructing dynamic scenes, applying implicit deformation fields to static models finds widespread use in dynamic scene rendering . To model dynamic scenes more accurately, various studies segment a scene into components with different attributes for different modeling [10; 38]. Moreover, several works apply higher-dimensional latent codes for the network input [16; 28] and incorporate additional supervision such as flow supervision across frames [40; 11; 21; 5; 18; 48; 19; 4] and motion mask supervision . Meanwhile, focusing on modeling rigid objects is important in improving accuracy because of their unique physical properties and prevalence in most scenes [38; 53]. Additionally, some research addresses the problems of dynamic scene models in several challenging scenes such as dynamic human modeling , specular objects  and the scenes without camera poses . However, implicit representations based on full MLPs suffer from the over-smoothing inherent property and require time-consuming training processes. On the other hand, explicit representations, such as Triplanes  and Hash Encoding , enhance NeRF by improving both visual quality and training speed. A popular technique for plane-based explicit representations in dynamic scene rendering is decomposing 4D inputs into six 2D inputs [9; 2; 33; 46; 35]. Also, hash encoding and 3D grid explicit representations can assist MLPs in predicting deformations with faster speed and higher precision [8; 41; 11; 42; 29; 36].

**Gaussian-Based Dynamic Scene Rendering.** Recently, Gaussian splatting  models static scenes by Gaussian points, achieving both fast training and high visual quality. When it comes to dynamic scene rendering, using 4D Gaussians or deforming Gaussians with pre-defined functions perform well in the cases with sufficient viewpoints [49; 6; 22; 17; 25]. Alternatively, deforming the attributes of 3D Gaussians according to timestamps with neural networks has led to better outcomes in general dynamic scene rendering [50; 44; 15; 7; 20; 24; 12; 52; 37]. Fully MLP-based deformation fields achieve high rendering quality  but suffer from the over-smooth inherent property, resulting in the failure of some detail rendering and complex scenes. Explicit representation models, for example, 4D-GS , utilize the planes-based methods as the deformation field. Although plane-based representations are more flexible, they are based on the unsuitable low-rank assumption, leading to massive feature overlap and rendering artifacts. Our work mainly focuses on tackling the unsuitable low-rank assumption inherent in plane-based explicit representations to improve the rendering quality of Gaussian-based models.

## 3 Method

### Preliminaries: Gaussian Splatting

Gaussian splatting  is a static scene rendering model, known for its high training speed and visual quality. This model assumes that the scene is composed of 3D Gaussian kernels with \(\{,S,R,,\}\), corresponding to the position, scaling, rotation, opacity, and color. Notably, the color attribute is defined by the spherical harmonic coefficients (SH). To render the scene, by using a view transform matrix \(W\) and a projective Jacobian matrix \(J\), Gaussians can be splatted onto camera planes [56; 51].

\[^{}=JW W^{T}J^{T},\;=RSS^{T}R^{T}\] (1)

where \(^{}\) is the covariance matrix in camera planes and \(\) is the original Gaussian covariance which can be calculated by the scaling and rotation attributes. Finally, supposing that the pixel on the camera planes is \(\), the splatted Gaussians can be rendered by the volume rendering equation,

\[()=_{i=1}^{N}_{i}_{i}_{j=1}^{i- 1}(1-_{j}),_{i}=_{i}e^{-(-_{i} ^{p})^{T}_{i}^{-1}(-_{i}^{p})}\] (2)

where \(^{p}\) is the projected coordinates of the 3D Gaussians, and \(N\) is the number of overlapped Gaussians on the pixel.

In optimization, adaptive density control is crucial for convergence. It involves pruning low-opacity Gaussians and densifying them based on the gradients and scaling. However, original Gaussian splatting cannot represent dynamic scenes and needs the help of deformation fields.

### 4D Decomposed Hash Encoding

Dynamic scene rendering involves deforming Gaussians according to a 4D coordinate \((x,y,z,t)\) input, where \(t\) represents the timestamp and \((x,y,z)\) means the position of a Gaussian. Instead of employing the over-smooth fully MLP-based implicit representations, we use explicit representation for Grid4D. However, existing plane-based explicit representation relies on the unsuitable low-rank assumption which overly decomposes the \((x,y,z,t)\) encoding into \((x,y),(y,z),(x,z),(x,t),(y,t),(z,t)\) plane encodings [33; 9; 2; 44]. As shown in Figure 2(a), for instance, considering the Gaussians A and B with the same \(y\) and \(z\) coordinates. The plane-based method has the same encoded features in the \((y,z),(y,t),(z,t)\) planes. Such a high overlap ratio might lead to the low discriminability of the features and block the model from fitting different deformations accurately. To address this problem, directly removing the decomposition by simply adding the time dimension to the traditional 3D grid for the 4D hyper-grid hash encoding is a possible way. However, the 4D hyper-grid hash encoding leads to high collision rates due to the high space complexity \(O(n^{4})\) of the 4D hyper-grid . Therefore, thoroughly eliminating the overlap might not be an available solution.

**Tri-axial 4D Decomposed Grid.** To address this problem, we propose a novel decomposition approach that decomposes the 4D encoding \((x,y,z,t)\) into four 3D hash encodings \((x,y,z),(x,y,t),(y,z,t),(x,z,t)\). The decomposition allows us to work with fewer parameters, which reduces the space complexity from \(O(n^{4})\) to \(O(n^{3})\) without relying on the low-rank assumption. As shown in Figure 2(a), the tri-axial decomposition can effectively reduce the overlap ratio from a half to a quarter, thereby enhancing each feature to represent the corresponding deformation. Figure 2(b) demonstrates that the features encoded by our methods are more discriminative for deformation prediction than plane-based methods.

**Multiresolution Hash Encoding.** In the original hash encoding technique , the grid employed in the encoder has the same resolution across all dimensions. Consistent resolutions could be suitable for static scene rendering, where the isotropic sampling assumption holds in the 3D space. Nevertheless, the sampling of the 4D space is usually anisotropic, which is usually sparse in the time dimension. Therefore, in our implementation, the temporal 3D encodings \((x,y,t),(y,z,t),(x,z,t)\) have different resolutions in the \(t\) dimension to account for this sparsity. Following the InstantNGP , we set the multiple resolutions of each dimension in a geometric progression:

\[N_{l}= N_{min} b,\ b=(- N_{min }}{L-1})\] (3)

where \(N_{min},N_{max}\) is the coarsest and finest resolutions, \(l\) is level number, \(L\) is the max level, and \(N_{l}\) is the resolution we select. The grid voxel positions for the input \(\) could be calculated by rounding

Figure 2: Comparison of our proposed 4D decomposed hash encoding with the plane-based explicit representation . (a) Compared to the plane-based methods based on the low-rank assumption, our methods reduce the overlap ratio in the features from a half to a quarter when encoding points A and B with heavily overlapping coordinates. (b) is the t-SNE  visualization of all the features, and the colors denote the corresponding represented deformations. The diversity of colors demonstrates that the reduced overlap makes the features represent different deformations more effectively.

down and up in each level \(_{l}= N_{l},_ {l}= N_{l}\). The voxels in each level could be obtained from the hash table by hashing the corresponding positions:

\[h_{t}(_{l})=(_{i=1,x_{i}_{l}}^{d}x_{i}_{ i}) T_{l}\] (4)

where \(\) is the bit-wise XOR operation, \(d\) is the dimension of the input, \(_{i}\) are unique large prime numbers, \(T_{l}\) is the size of the level \(l\) hash table. Then the encoded features could be calculated by the trilinear interpolation of the grid voxel values. Generally, the encoded features of the 4D input \((x,y,z,t)\) include the spatial and temporal features from the spatial grid hash encoder \(G_{xyz}\) and temporal grid hash encoders \(G_{xyt},G_{yzt},G_{xzt}\) respectively.

### Multi-head Directional Attention Decoder

Our 4D decomposed hash encoding generates two types of features: temporal features and spatial features. The temporal features represent the information related to the timestamp while the spatial features represent the common information across the timeline. The Gaussians representing different scene components often have various deformations in almost every timestamp. Therefore, the spatial features could be used to help the model fit such variations, and we design the directional attention module for the spatial and temporal feature aggregation.

**Directional Attention.** We infer the attention features from the spatial grid hash encoder \(G_{xyz}\) with a tiny spatial MLP \(f_{s}\), and generate the score \(\) through the following formula,

\[=2(_{xyz})-1,\ _{xyz}=f_{s} G_{xyz}(x,y,z)\] (5)

where \(\) is the Sigmoid function. We consider that several components probably have entirely opposite deformations against the neighboring Gaussians. For example, the Gaussians for the shadows often have opposite motions relative to the objects. Therefore, different from the common range \((0,1)\) of the attention score \(\), we scale it to a directional range \((-1,1)\) to represent neighboring deformations with opposite directions, thereby enhancing the representation ability of the attention mechanism.

Then we apply the attention score to the activated deformation features encoded by the three temporal grid hash encoders \(G_{xyt},G_{yzt},G_{xzt}\) and a tiny temporal MLP \(f_{t}\).

\[= f_{t}(G_{xyt}(x,y,t),G_{yzt}(y,z,t),G_{xzt}(x,z,t))\] (6)

where \(\) is the dot product operation. Finally, we get the deformation features \(\) with high representation ability. Our experiments demonstrate that our attention module outperforms the architecture

Figure 3: The overview of Grid4D. Given the canonical Gaussians and the timestamp, we first encode the decomposed input separately. Then we apply the directional attention scores generated by the spatial static features to the temporal dynamic features, and we decode the features with a tiny multi-head MLP. Finally, the Gaussians deformed by the predicted deformations are splatted by the differentiable rasterization operation  to render the images for supervision.

which either directly decodes the concatenation of the spatial and temporal features or uses the common range \((0,1)\) of the attention score.

**Multi-head Deformation Decoder.** The decoder is required to decode the features \(\) to get the Gaussian deformation. Different from the prior works [44; 50], we use a tiny multi-head MLP \(D\) to decode the features and predict the position deformation with a rotation matrix \(R_{x}\) and a translation matrix \(T_{x}\) as . Finally, we deform the position, scaling, and rotation of the Gaussians in the canonical space with the predicted deformation.

\[^{}=R_{x}+T_{x},\;S^{}=S+ ,\;R^{}=R+,\;D()=\{R_{x},T_{x}, ,\},\] (7)

We define the rotation matrix \(R_{x}\) with a quaternion for more accurate interpolation and stable optimization. Following Gaussian splatting , the deformed Gaussians could be rendered into images of specific timestamps via differentiable rasterization.

### Training with Smooth Regularization

Although the proposed model architecture could effectively predict the Gaussian deformation, the 4D decomposed hash encoder still suffers from the lack of smoothness, a common challenge in most explicit representation methods. We consider that the MLP decoder has the smooth inherent property and does not require additional smoothing. Therefore, we set our regularization in the feature space without involving the MLP decoder inference for higher efficiency. Generally, to regularize the hash encoder, we propose a novel smooth regularization loss.

\[_{r}=||G_{xyzt}(x,y,z,t)-G_{xyzt}(x+_{x},y+_{y},z+ _{z},t+_{t})||_{2}^{2}\] (8)

where \((_{x},_{y},_{z},_{t})\) is the small random perturbation for the input \((x,y,z,t)\) respectively, and \(G_{xyzt}\) is the concatenation of four grid hash encoders. This regularization enforces similarity among encoded features in neighboring regions, thereby making the nearby Gaussians have similar deformations. Due to the difference of spatial and temporal encoding, we use a different regularization setting for the spatial encoding for several cases. Notably, to improve the efficiency, we randomly select partial Gaussians for the regularization instead of using them all. Our experiments demonstrate that this smooth regularization effectively mitigates the deformation chaos, leading to significantly improved rendering clarity.

In general, similar to Gaussian splatting , our total loss function can be summarized as the weighted sum of L1 color loss, D-SSIM loss, and the proposed smooth regularization term.

\[=(1-_{c})_{1}+_{c}_{D-SSIM}+ _{r}_{r}\] (9)

where \(_{c},_{r}\) are the hyperparameters to balance the losses. Following , we use the detached Gaussian positions for deformation prediction, which results in better performance. Also, similar to prior works [50; 44], we initialize the static canonical Gaussians without deformation at the beginning of the training process. Specifically for SfM  initialized Gaussians, we shorten or remove the static initialization process. We apply the same adaptive density controller and opacity resetting mechanism as Gaussian splatting . The pipeline of Grid4D is illustrated by Figure 3.

## 4 Experiments

In this section, we introduce our experiments conducted on a single RTX 3090 GPU. We build our code mainly on PyTorch , while we implement our 4D decomposed hash encoder with CUDA/C++. More experimental results and analysis can be found in the supplementary.

### Experimental Setup

**Datasets.** We evaluate Grid4D on two popular datasets. D-NeRF  dataset is a public monocular synthetic dataset that provides accurate and time-varying camera poses. HyperNeRF  dataset is a public real-world dataset captured by one or two moving cameras. Neu3D  dataset is a public dataset captured by multiple cameras with fixed poses. However, different from synthetic datasets, the camera poses of the HyperNeRF and Neu3D datasets are estimated by COLMAP , which is not accurate. We set the rendering resolutions of the D-NeRF, HyperNeRF and Neu3D datasets to \(800 800\), \(536 900\) and \(1352 1024\) respectively. Notably, we find several mistakes in the ground truth of the 'Lego' scene in the D-NeRF dataset, as shown in the last row of Figure 4, so we ignore this scene in all quantitative comparisons of rendering quality.

**Baselines.** We compare Grid4D with several state-of-the-art models [2; 8; 44; 50; 12]. HexPlane  and TiNeuVox  are NeRF-based dynamic scene rendering models, utilizing plane-based and 3D grid explicit representations respectively. 4D-GS  and DeformGS  are Gaussian-based models, employing plane-based explicit representation and fully MLP-based implicit representation for the deformation fields respectively. SC-GS  is a model built on DeformGS , and proposes to use sparse control points for better dynamic scene rendering and edit.

**Hyperparameters.** For all datasets, we configure the resolution of the spatial grid hash encoder to span from \(16\) to \(2048\) across \(16\) levels. Meanwhile, the max level number \(L\) of temporal grid hash encoders remains consistent at \(32\). We set \(_{c}\) and \(_{r}\) to \(0.2\) and \(0.5\) for common scenes and follow a similar learning rate schedule as DeformGS [50; 13].

### Comparisons

**Comparison of Visual Quality.** We compare Grid4D with the state-of-the-art models on the synthetic D-NeRF  dataset (Table 1 and Figure 4), the real-world HyperNeRF  dataset (Table 2 and Figure 5) and the real-world Neu3D  dataset (Table 3 and Figure 6). The PSNR, SSIM , LPIPS (VGG ), and MS-SSIM are the metrics denoting visual quality. Notably, the DeformGS  model fails to construct several HyperNeRF scenes with large motions and imprecise camera poses, as mentioned in their paper. Several failed cases can be found in Section B of the supplementary, and we consider that this is also due to the over-smooth inherent property of fully MLP-based implicit representation.

Due to the inherent flexibility of the explicit representation, the results of the 'Hook' scene show that Grid4D has a stronger ability to reconstruct fine structures than DeformGS  which is based on the implicit representation. We also apply the sparse control points in SC-GS  to our model and build SC-GS on Grid4D rather than DeformGS for further evaluation. We refer to it as 'Grid4D + SC', and observe an improvement in comparison to Grid4D and SC-GS as list in the last three rows of Table 1. Thanks to our 4D decomposed hash encoding, when facing the scenes with complex motions

Figure 4: Qualitative comparisons on the synthetic D-NeRF dataset  with our baselines [8; 44; 50].

and Gaussians with heavily overlapping coordinates, such as 'JumpingJacks', Grid4D predicts the deformations much more accurately than 4D-GS  which is built on the planed-based explicit representation relying on the unsuitable low-rank assumption.

**Comparison of Rendering Speed.** Comparing Frames Per Second (FPS) directly might not be a fair experiment because the number of Gaussians is quite different among different models. Therefore, we list both the FPS and the corresponding Gaussian count in Table 4. Despite the acceleration provided by the CUDA/C++ implementation in Grid4D, our proposed explicit representation makes

    &  &  &  &  \\ Model & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS \\  HexPlane  & 40.36 & 0.992 & 0.031 & 24.30 & 0.944 & 0.073 & 28.26 & 0.955 & 0.052 & 31.74 & 0.974 & 0.036 \\ TiNeuVox  & 40.28 & 0.992 & 0.042 & 27.29 & 0.964 & 0.076 & 30.51 & 0.959 & 0.060 & 33.46 & 0.977 & 0.041 \\
4D-GS  & 40.77 & 0.994 & 0.015 & 28.80 & 0.974 & 0.037 & 32.95 & 0.977 & 0.027 & 35.50 & 0.986 & 0.020 \\ DeformGS  & 40.91 & 0.995 & 0.009 & 41.34 & 0.987 & 0.024 & 37.06 & 0.986 & 0.016 & 37.66 & 0.989 & 0.013 \\ SC-GS  & 41.59 & 0.995 & 0.009 & 42.19 & 0.989 & 0.019 & 38.79 & 0.990 & 0.011 & 39.34 & 0.992 & 0.008 \\ Grid4D (Ours) & 42.62 & 0.996 & 0.008 & 42.85 & 0.991 & 0.015 & 38.89 & 0.990 & 0.009 & 39.37 & 0.993 & 0.008 \\ Grid4D + SC & 42.17 & 0.995 & 0.008 & 42.81 & 0.990 & 0.017 & 40.26 & 0.992 & 0.008 & 39.58 & 0.993 & 0.008 \\    
    &  &  &  &  \\ Model & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS \\  HexPlane  & 33.66 & 0.982 & 0.028 & 34.12 & 0.983 & 0.019 & 31.01 & 0.976 & 0.028 & 31.92 & 0.972 & 0.038 \\ TiNeuVox  & 32.07 & 0.961 & 0.048 & 34.46 & 0.980 & 0.033 & 31.43 & 0.967 & 0.047 & 32.78 & 0.972 & 0.050 \\
4D-GS  & 37.75 & 0.988 & 0.016 & 38.15 & 0.990 & 0.014 & 33.95 & 0.985 & 0.022 & 35.41 & 0.985 & 0.021 \\ DeformGS  & 42.47 & 0.995 & 0.005 & 44.14 & 0.995 & 0.007 & 37.56 & 0.993 & 0.010 & 40.16 & 0.991 & 0.012 \\ SC-GS  & 43.43 & 0.996 & 0.005 & 46.72 & 0.997 & 0.004 & 39.53 & 0.994 & 0.009 & 41.65 & 0.993 & 0.009 \\ Grid4D (Ours) & 43.94 & 0.996 & 0.004 & 46.28 & 0.997 & 0.004 & 40.01 & 0.995 & 0.008 & 42.00 & 0.994 & 0.008 \\ Grid4D + SC & 44.07 & 0.996 & 0.004 & 46.87 & 0.997 & 0.004 & 41.12 & 0.995 & 0.008 & 42.41 & 0.994 & 0.008 \\   
   FPS / Num(k) & Balls & Warrior & Hook & Jumping & Lego & Mutant & Standup & Trex \\ 
4D-GS  & 182 / 28 & 168 / 40 & 91 / 39 & 207 / 24 & 104 / 93 & 173 / 38 & 201 / 27 & 151 / 68 \\ DeformGS  & 37 / 180 & 161 / 37 & 43 / 150 & 71 / 90 & 30 / 289 & 49 / 169 & 77 / 81 & 30 / 217 \\ Grid4D (Ours) & 91 / 192 & 334 / 46 & 79 / 210 & 241 / 68 & 64 / 302 & 157 / 126 & 170 / 100 & 86 / 254 \\   

Table 4: Rendering speed comparison on the synthetic D-NeRF  dataset. We report the FPS based on the number of Gaussian points. Compared to other models, our model still achieves high rendering speed and real-time rendering when facing a much larger amount of Gaussians.

    &  & Cook Spinach & Cut Beef & Flame Salmon & Flame Steak & Sear Steak \\ Model & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\ 
4D-GS  & 27.34 & 0.898 & 32.50 & 0.942 & 32.26 & 0.942 & 27.99 & 0.902 & 32.54 & 0.951 & 33.44 & 0.954 \\ Grid4D (Ours) & 28.30 & 0.898 & 32.58 & 0.948 & 33.22 & 0.950 & 29.12 & 0.908 & 32.56 & 0.955 & 33.16 & 0.957 \\   

Table 3: Quantitative comparison on the real-world Neu3D  dataset. The higher PSNR (\(\)) and higher SSIM (\(\)) denote better rendering quality. The color of each cell shows the best.

Figure 6: Qualitative comparisons on the real-world Neu3D  dataset.

from the neighboring parts across the timeline. Our directional attention achieves high clarity in rendering the portion with the shadow, emphasizing its effectiveness in capturing such variations.

**Ablation of Smooth Regularization.** The proposed smooth regularization aims at mitigating the chaos of deformation prediction. We train Grid4D without the smooth regularization in Equation 8 and refer to the model as Grid4D w/o reg. The results in Figure 7(a) show that the regularization reduces the deformation artifacts caused by the lack of smoothness.

We conduct more ablation studies for the architecture and smooth regularization. We also visualize the intermediate results of our model. More results can be found in Section C of our supplementary.

## 5 Conclusion

In this paper, we have introduced Grid4D, a novel model for high-fidelity dynamic scene rendering. Grid4D utilizes the proposed 4D decomposed hash encoding without the unsuitable low-rank assumption and high space complexity. Additionally, the novel directional attention module effectively aggregates the spatial and temporal features for more accurate deformation prediction across different scene components. Moreover, we employed smooth regularization to mitigate chaos in deformation prediction, resulting in high rendering quality. Our experiments demonstrate that Grid4D achieves state-of-the-art performance and delivers high rendering speed for dynamic scene rendering. However, Grid4D has no improvement in training speed, and like the other dynamic scene rendering models, Grid4D might have artifacts when facing several dynamic scenes with complex and large motions. Addressing these challenges remains an area for future research.