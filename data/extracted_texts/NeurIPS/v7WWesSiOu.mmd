# End-To-End Latent Variational Diffusion Models for Inverse Problems in High Energy Physics

Alexander Shmakov

Department of Computer Science

University of California Irvine

Irvine, CA 92697

ashmakov@uci.edu

&Kevin Greif

Department of Physics and Astronomy

University of California Irvine

Irvine, CA 92697

kgreif@uci.edu

&Michael Fenton

Department of Physics and Astronomy

University of California Irvine

Irvine, CA 92697

mjfenton@uci.edu

&Aishik Ghosh

Department of Physics and Astronomy

University of California Irvine

Irvine, CA 92697

Physics Division

Lawrence Berkeley National Laboratory

Berkeley, CA 94720

Pierre Baldi

Department of Computer Science

University of California Irvine

Irvine, CA 92697

pfbaldi@uci.edu

&Daniel Whiteson

Department of Physics and Astronomy

University of California Irvine

Irvine, CA 92697

pfbaldi@uci.edu

###### Abstract

High-energy collisions at the Large Hadron Collider (LHC) provide valuable insights into open questions in particle physics. However, detector effects must be corrected before measurements can be compared to certain theoretical predictions or measurements from other detectors. Methods to solve this _inverse problem_ of mapping detector observations to theoretical quantities of the underlying collision are essential parts of many physics analyses at the LHC. We investigate and compare various generative deep learning methods to approximate this inverse mapping. We introduce a novel unified architecture, termed latent variational diffusion models, which combines the latent learning of cutting-edge generative art approaches with an end-to-end variational framework. We demonstrate the effectiveness of this approach for reconstructing global distributions of theoretical kinematic quantities, as well as for ensuring the adherence of the learned posterior distributions to known physics constraints. Our unified approach achieves a distribution-free distance to the truth of over 20 times smaller than non-latent state-of-the-art baseline and 3 times smaller than traditional latent diffusion models.

## 1 Introduction

Particle physics experiments at the Large Hadron Collider study the interactions of particles at high energies, which can reveal clues about the fundamental nature of matter and forces. However, the properties of particles which result from the collisions must be inferred from signals in the detectors which surround the collision. Though detectors are designed to reconstruct the propertiesof particles with high fidelity, no detector has perfect efficiency and resolution. A common strategy to account for these effects is _simulation-based inference_, in which the detector resolution and inefficiency are modeled by a simulator. Samples of simulated events can then be compared to observed data to perform inference on theoretical parameters. However, simulators with high fidelity are computationally expensive and not widely accessible outside of experimental collaborations.

An alternative approach is the reverse, mapping the observed detector signatures directly to the unobserved _truth-level_ information. In a particle physics context, this procedure is referred to as "unfolding"1. In practice, the quantum mechanical nature of particle interactions makes the forward map from the true particle properties to observed data not one-to-one. As a result, there is no true inverse function which can map a given detector observation to a single point in the truth-level space. Such _inverse problems_ are challenging, but unfolded data allows for direct comparisons with theoretical predictions and across experiments, without requiring access to detector simulation tools which may not be maintained long-term.

Unfolding methods such as Iterative D'Agostini , Singular Value Decomposition , and TUnfold  have seen frequent use by experimental collaborations like ATLAS  and CMS . However, these techniques are limited to unfolding only a few dimensions, and require binning the data, which significantly constrains later use of the unfolded distributions. The application of machine learning techniques  has allowed for the development of un-binned unfolding with the capacity to handle higher-dimensional data. One approach is to use conditional generative models, which learn to sample from the truth-level distributions when conditioned on the detector-level data; examples include applications of generative adversarial networks [8; 9], invertible networks [10; 11], and variational auto-encoders . An alternative approach uses classification models as density estimators which learn to correct imprecise truth-level distributions with re-weighting [13; 14; 15]. Generative methods naturally produce unweighted events, an advantage over classification methods which may generate very large weights or even fail if the original distributions do not sufficiently cover the entire support of the true distribution. However, generative models are not always guaranteed to produce samples which respect the important physical constraints of the original sample. While making important strides, none of these methods have cracked the ultimate goal, _full-event unfolding_, where the full high-dimensional detector-level observations are mapped to truth-level objects.

This paper introduces a novel generative unfolding method utilizing a diffusion model [16; 17; 18] to map detector to truth-level distributions. Diffusion models are a class of generative models which learn to approximate a reverse noise diffusion process and have proven successful in natural image generation [19; 20] and recently scientific applications such as molecular link design . Diffusion models excel in learning high-dimensional probability distributions at higher fidelity than normalizing flows and without the adversarial min-max loss of GANs. In HEP, they have already found use for approximating calorimeter simulations [22; 23; 24; 25]. Latent diffusion models (LDMs), a specific class of diffusion models, perform the denoising in an abstract latent space  and excel in image generation tasks. These latent embeddings are often pre-trained on secondary objectives, such as VAE reconstruction tasks or CLIP , to limit computational and memory requirements. We unify the abstract embedding space of latent diffusion with the recently formalized variational diffusion approach  to develop an end-to-end variational latent diffusion model (VLD) achieving state-of-the-art performance in complex HEP generative tasks.

## 2 Background

### Unfolding

Let \(f_{}(y)\) be the distribution which governs an observed detector-level data set \(y=\{y_{i}\}\). An unfolding method aims to sample from a pre-detector distribution \(f_{}(x)\), where _parton_ refers to an unobserved state of interest to physicists. \(f_{}(x)\) is related to \(f_{}\) via convolution with a "response" function \(p(y|x)\) over the possible true values \(x\). The response function describes the decay of the initial, unstable particles into stable particles and their interaction with the detector.

\[f_{}(y)= dx\;p(y|x)f_{}(x)\] (1)No closed form expression exists for \(p(y|x)\), but Monte-Carlo-based simulation can sample from parton values \(x\) and produce the corresponding sample \(y\). The parton distribution can be recovered via the corresponding inverse process if one has access to a pseudo-inversion of the response function \(p(x|y)\), also known as the posterior.

\[f_{}(x)= dy\;p(x|y)f_{}(y)\] (2)

Generative unfolding methods build the posterior as a generative model, which can be used to sample from \(p(x|y)\). The desired parton distribution is then obtained by Equation 2. Simulated pairs of parton-detector data, \((x,y)\), may be used to train the generative model.

An important issue when choosing to directly model the posterior is that this quantity is itself dependent on the desired distribution \(f_{}(x)\), the prior in Bayes' theorem:

\[p(x|y)=}(x)}{f_{}(y)}\] (3)

Producing the data set used to train the generative model requires choosing a specific \(f_{}(x)\), which influences the learned posterior. In application to new datasets, this will lead to an unreliable estimate of the posterior density if the assumed prior is far enough from the truth distribution. A common method to overcome this challenge is to apply an iterative procedure, in which the assumed prior is re-weighted to match the approximation to the truth distribution provided by the unfolding algorithm . Though application of this iterative procedure is not shown in this paper, the principle has been demonstrated with other generative unfolding methods , for which the conditions are similar.

### Semi-Leptonic Top Quark Pair Production

Collisions at the LHC which result in a pair of top quarks allow for sensitive probes of new theories of physics, which makes measurement of the top quark properties an important task. Top quarks are unstable, decaying almost immediately to a \(W\) boson and a bottom quark; the \(W\) boson can then decay _hadronically_ to two quarks or _leptonically_ to a charged lepton and neutrino. The case where one of the produced top quarks decays hadronically and the other decays leptonically is known as the semi-leptonic decay mode, see Fig. 0(a). The 4-momenta (three momentum components, one mass) of

Figure 1: Visual representations of the different perspectives captured by the parton and detector level data. The parton-level data represents a fundamental theoretical description of the decay, whereas the detector-level data represents the real measurements observed after the decay. The primary challenge in unfolding is to infer the theoretical parton representation from the observed data.

these six objects (four quarks, the charged lepton, and the neutrino) constitute the parton-level space in this context.

The four quarks each produce a shower of particles (_jets_) which interact with the detector, while the neutrino passes through without leaving a trace. The resulting observed detector signature which defines the detector-level space is then quite complex, see Fig. 1b.

The semi-leptonic \(t\) process has been studied by the ATLAS and CMS collaborations to measure various properties of the top quark and to search for new particles and interactions [30; 31; 32; 33; 34; 35]. Many of these measurements use existing unfolding techniques, which limit the unfolded measurements to one or two dimensions. An un-binned and high dimensional unfolding technique would allow physicists to use the full power of their data.

### Variational Autoencoders

Variational Autoencoders (VAEs) are a class of generative models combining an autoencoder architecture with probabilistic modeling [36; 37]. VAEs learn a non-linear latent representation of input data through an encoder and decoder network while incorporating probabilistic methods and sampling through the reparameterization trick . VAEs have been applied to numerous applications, such as image synthesis  and natural language processing , among many others.

The VAE encoder network is parameterized as a probabilistic function, approximating the posterior distribution of the latent variables \(z\) conditioned on the input data: \(q(z|x)\). The decoder network likewise models the generative distribution conditioned on the latent variables \(p(x|z)\). VAEs are trained by maximizing the evidence lower bound (ELBO), which is a lower bound on the log-likelihood of the data under the generative model . The ELBO includes a reconstruction loss for training the decoder and a KL-divergence objective which enforces a regularization constraint on the learned latent posterior to a prior distribution \(p(z)\).

\[_{}=_{z q(z|x)}[- p(x|z)+D_{KL}(q (z|x) p(z))]\] (4)

Conditional VAEs (CVAEs)  extend the VAE framework by conditioning both the encoder and decoder networks on additional information, such as class labels, via an arbitrary conditioning vector \(y\). This allows CVAEs to generate samples with specific desired properties, providing more control over the generated outputs.

\[_{}=_{z q(z|x,y)}[- p(x|z,y)+D_{ KL}(q(z|x,y) p(z|y))]\] (5)

### Variational Diffusion Models

Variational Diffusion Models (VDMs) define a conditional probabilistic generative model which exploits the properties of diffusion probabilistic models to generate samples by learning to reverse a stochastic flow . VDMs may be seen as an extension of VAEs to a (possibly infinitely) deep hierarchical setting. The Gaussian diffusion process defines the forward stochastic flow with respect to time \(t\) over the latent space \(z_{t}\) and conditioned on \(y\) as:

\[q(z_{t}|x,y)(_{t}x,_{t})\] (6)

The flow parameters, \(_{t}\) and \(_{t}\) are defined by a _noise schedule_. We use the continuous Variance Preserving (VP) framework throughout this work and derive these flow parameters based on a learned signal-to-noise ratio, \(e^{-_{}(t)}\), where:

\[_{t}=(_{}(t))}\;\;\;\; _{t}=(-_{}(t))}\]

Assuming it is possible to sample from the terminal distribution \(p(z_{1})\), we may produce samples from the data distribution by inverting the flow and sampling previous latent representations conditioned on future latent vectors. The inverse flow is modeled as \(q(z_{s}|z_{t},_{}(z_{t},t,y))\) where \(_{}\) is an approximate denoising of the original data at the current time-step. In practice, the data denoising is implemented using a variance-independent _noise prediction network_, \(}\), by the equation \(}(z_{t},t,y)=-_{t}(z_{t},t,y))}{ _{t}}\). The noise prediction network, \(}\), is parameterized using a deep neural network. The learnable noise schedule \(_{}(t)\) is also parameterized using a positive valued, monotonic neural network with learnable end-points \(_{min}=(0)\) and \(_{max}=(1)\). Followingthe VP framework, the noise schedule is regularized so that the terminal distribution is the unit Gaussian: \(p(z_{1})(,)\). Both the noise prediction network and the noise schedule network are trained using the modified ELBO for continuous-time diffusion models :

\[_{} =D_{KL}(q(z_{1}|x,c) p(z_{1}))+_{q(z_{0}|x)} [- p(x|z_{0},y)]\] \[+_{(,), (0,1)}[_{}^{}(t)\|- _{}(z_{t},t,y)\|_{2}^{2}]\] (7)

### Latent Diffusion

Latent diffusion models (LDMs) are a deep generative framework that operate the diffusion process in an abstract latent space learned by a VAE to sample high-dimensional data \(p_{D}(x|z,y)\), possibly conditioned on a secondary dataset \(p_{C}(y)\). This approach has proven dramatically successful when employed in natural image generation applications, including text-to-image synthesis, inpainting, denoising, and style transfer [26; 20].

LDMs first train an unconditional VAE to embed the data distribution into a low dimensional latent representation using a traditional VAE approach, \(q(z_{x}|x)\) and \(p(x|z_{x})\), regularizing the latent space towards a standard normal \(p(z_{x})(,)\). A secondary encoder may be trained on the conditioning data \(p(z_{y}|y)\) along-side VAE, typically using a CLIP objective  to map the two datasets into a common latent space. The diffusion process is then trained to reconstruct the latents \(z_{x}\) from the flow latents \(p(z_{x}|z_{0},z_{y})\). The diffusion model training remains otherwise identical to the standard diffusion framework.

Critically, the most successful methods train the VAE, the conditional encoder, and the diffusion process individually. While computationally efficient, this independence limits the models' generative power as each component is trained on subsets of the overall conditional generative objective. It may be possible to recover additional fidelity by instead training all components using a unified conditional generation objective. While several methods allow for training a VAE _along-side_ diffusion [42; 43], these approaches either cannot train diffusion in the latent space or cannot account for a conditional, fully variational model. We construct a unified variational framework to allow for a conditional, probabilistic, end-to-end diffusion model.

## 3 Variational Latent Diffusion

This work integrates the learning capabilities of latent diffusion models with the theoretical framework of variational diffusion models in a unified conditional variational approach. This unified variational model combines the conditioning encoder, data VAE, and diffusion process into a single loss function. This framework enables further enhancement of these methods through a conditional data encoder or decoder, and an auxiliary physics-informed consistency loss which may be enforced throughout the network. We refer to this combined method as Variational Latent Diffusion (VLD), see Fig 2. The primary contributions of this paper are to define this unified model and derive the appropriate loss function to train such a model.

Conditioning (Detector) EncoderIn traditional LDMs, the conditioning encoder, \(p(z_{y}|y)\), is pre-trained through an auxiliary loss term, such as CLIP , which aims to unify the latent space of the conditioning and data. While this approach is efficient, it may not be optimal: the encoder is trained

Figure 2: A block diagram of the end-to-end VLD models with trainable components. The conditional paths are drawn in different colors depending on which model variations employ them. We use the continuous, variance preserving SDE diffusion formulation introduced in  and . We show the equivalent ODE form of SDE equation in the diagram.

on one objective, and then repurposed to act as a conditioning encoder for a separate generative model. With the end-to-end framework, we simultaneously learn this encoder alongside other generative terms, enabling us to efficiently train a variable-length, high-dimensional encoder fine-tuned for the generative objective. In this work, we simplify this conditioning encoder by restricting it to a deterministic mapping, \(z_{y}=f_{}(y)\). This decision is based on prior work such as the CVAE which opts for a deterministic conditional encoder, as well as for simplicity as there is a lack of motivating benefits from a stochastic encoder.

Data (Parton) VAEThe traditional LDM VAE is unconditional, as this allows it to be easily pre-trained and reused for different diffusion models. As we are training a unified conditional generative model in an end-to-end fashion, we have the option to extend the encoder and decoder with conditional probabilistic models: \(q_{}(z_{x}|x,z_{y})\) and \(p_{}(x|z_{x},z_{y})\). We experiment with both a conditional and unconditional VAE. Additionally, we explore an intermediate method that uses a conditioned encoder to estimate the VAE posterior, \(q_{}(z_{x}|x,z_{y})\), but employs an unconditional decoder during generation \(p_{}(z_{x}|x)\).

Vld ElboWe interpret the continuous VDM as an infinitely deep hierarchical VAE as presented by Kingma _et al._. This interpretation allows us to seamlessly integrate the VAE into a unified diffusion framework by incorporating the VAE as an additional component in the hierarchy. Consequently, the hierarchical variational ELBO incorporates an extra KL divergence term, which serves to regularize the encoder posterior distribution . We combine this hierarchical objective with the denoising loss term derived in  to define a combined ELBO for the entire generative model.

\[_{VLD} =D_{KL}(q(z_{1}|x,z_{y}) p(z_{1}))+_{q(z_{x}|x,z_{y})}[- p(x|z_{x},z_{y})]\] \[+D_{KL}(q(z_{x}|x,z_{y}) p(z_{x}|z_{0}))+_{ (,),(0,1)} [^{}_{}(t)\|-_{}(z_{t},t, z_{y})\|_{2}^{2}]\] (8)

The additional KL term may be derived explicitly if we assume a Gaussian VAE and a Gaussian diffusion process. The posterior is parameterized using a learned Gaussian, as in a standard VAE: \(q(z_{x}|x,z_{y})(_{}(x,z_{y}),_{}(x,y))\). The prior can be reformulated using the definition of the forward flow from Equation 6. Employing the reparameterization trick, we can rewrite the expression of \(z_{0}\) in terms of \(z_{x}\) as \(z_{0}=_{0}z_{x}+_{0}\), where \((,)\). Solving this equation for \(z_{x}\) yields another reparameterized Gaussian, which allows us to define the prior over \(z_{x}\) as:

\[p(z_{x}|z_{0})(}z_{0}, }{_{0}})\] (9)

Physics-Informed Consistency LossReconstructing the mass of truth-level physics objects is challenging due to their highly peaked, low-variance distributions. For certain particles like leptons, the mass distribution exhibits a two-valued delta distribution, while for light quarks, it is consistently set to zero. Predicting these distributions is more difficult than predicting the energy of truth-level physics objects, which have a broader range. In special relativity, the mass (\(M\)), energy (\(M\)), and momentum \(\) of a particle are related by \(c^{4}M^{2}=E^{2}-(c\|\|)^{2}\). We operate in natural units with a unit speed of light \(c=1\). Forcing the predicted mass, energy, and momentum to satisfy this equality improves stability and accuracy by capturing this underlying physical relationship between these quantities. We introduce a consistency loss, \(_{C}\), in addition to the regular reconstruction loss, weighted by a hyper-parameter \(_{C}\). Similar physics-informed constraints have previously been used for generative models in HEP [45; 46; 47; 48]. The consistency loss minimizes the mean absolute error (MAE) between the predicted mass term and the corresponding predicted energy and momentum terms, encouraging the model to learn a more physically consistent representation.

\[_{C}=_{C}|^{2}-(^{2}-\| \|^{2})|\] (10)

## 4 Unfolding Semi-Leptonic \(t\) Events

Generative models can be trained to estimate a conditional density given any set of paired data. In the unfolding context, a Monte Carlo simulation can be used to generate pairs of events at detector and parton level. The density of parton level events \(f_{}(x)\) can be taken as the data distribution, and the density of detector level events \(f_{}(y)\) can be taken as the conditioning distribution. A generative model can then be used to unfold a set of observed events to the corresponding parton level events with the following procedure:1. Sample a parton configuration from the distribution governing the process of interest: \(x p_{D}(x)\). This can be done using a matrix element solver such as MadGraph.
2. Sample a possible detector observation \(y p_{C}(y|x)\) using the tools Pythia8 and Delphes, which simulate the interactions of particles in flight and the subsequent interactions with a detector.
3. Train a generative model to approximate the inverse distribution \(p_{}(x|y)\).
4. Produce new posterior samples for inference data with unknown parton configurations.

### Generative Models

Multiple baseline generative models are assessed alongside the novel VLD approach, with the goal of investigating the impact of each VLD component, including the conditional VAE, the denoising model, and the variational aspects of the diffusion. Note that the network architectures of the VAEs, denoising networks, and detector encoders are identical where relevant.

CvaeA traditional conditional Variational Autoencoder  approach employing a conditional encoder and decoder. We use a Gaussian likelihood for the decoder and a standard normal prior for the encoder, following conventional practices for VAE models.

CinnA conditional Invertible Neural Network , which represents the latest deep learning approach that has demonstrated success in unfolding tasks. This model utilizes a conditional normalizing flow to train a mapping from a standard normal distribution to the parton distribution, conditioned on the detector variables. The normalizing flow incorporates an All-In-One architecture , following the hyperparameters detailed in the CINN paper , which combines a conditional affine layer with global affine and permutation transforms to create a powerful invertible block. In this work, the MMD objective defined in  is replaced with a MSE reconstruction objective and the physics-informed consistency loss, for fair comparison with other models.

VdmA Variational Diffusion Model (VDM)  that aims to denoise the parton vector directly. This model serves as a baseline for examining the impact of the VAE in latent diffusion approaches. The denoising model is trained using a Mean Squared Error loss against the generated noise.

LdmA Latent Diffusion Model (LDM) with a pre-trained VAE, popularized by recent achievements in text-to-image generative models . The VAE is pre-trained using a Gaussian likelihood and a minimal prior weight (\(10^{-4}\)). This baseline is meant to highlight the importance of the unified end-to-end architecture as all other aspects of the network are identical to the proposed method.

Vld, C-Vld, Uc-VldThese models are variations on the proposed unified Variational Latent Diffusion (VLD) architecture. They correspond to an unconditional VAE (VLD), a conditional encoder and decoder (C-VLD), or a conditional encoder with an unconditional decoder (UC-VLD).

### Detector Encoder

All of the generative models are conditioned on detector observations, represented as a set of vectors for each jet and lepton in the event, as described in Section 5. Additionally, the missing transverse momentum (MET) from the neutrino is included as a fixed-size global variable. As there is no inherent ordering to these jets, it is crucial to use a permutation-invariant network architecture for the encoder. We use the jet transformer encoder from the SPANet (v2.1, BSD-3)  jet-parton reconstruction network to embed detector variables. This architecture leverages the permutation invariance of attention to contextually embed a set of momentum vectors. We extract the fixed-size event embedding vector from the central transformer, mapping the variable-length, unordered detector observations into a fixed-size real vector \(E_{C}(y)=z_{y}^{D}\).

### Parton Encoder-Decoder

For a given event topology, partons may be represented as a fixed-size vector storing the momentum four-vectors of each theoretical particle. We describe the detailed parton representation in Section 5,which consists of a single 55-dimensional vector for each event. The encoder and decoder network employ a ConvNeXt-inspired block structure  for the hidden layers, described in Appendix A, which allows for complex non-linear mappings into the latent space. Unlike traditional VAE applications, our latent space may be _higher_ dimensionality than the original space. The VAE's primary purpose therefore differs from typical compression applications, and instead solely transforms the partons into an optimized representation for generation.

The encoder uses this feed-forward block network and produces two outputs: the mean, \(_{}(x,z_{y})\), and log-standard deviation, \(_{}(x,z_{y})\), of the encoded vector, possibly conditioned on the detector observations \(z_{y}\). The decoder similarly accepts a latent parton representation, possible conditioned on the detector, and produces a deterministic estimate of the original parton configuration \(=D(z_{x},z_{y})\).

## 5 Experiments

DatasetEach of the generative approaches is trained to unfold a simulated semi-leptonic \(t\) production data set. Matrix elements are evaluated at a center-of-mass energy of \(=13\) TeV using MadGraph_aMC@NLO (v2.7.2, NCSA license) with a top mass of \(m_{t}=173\) GeV. The parton showering and hadronization are simulated with Pythia8 (v8.2, GPL-2), and the detector response is simulated with Delphes (v3.4.1, GPL-3) using the default CMS detector card. The top quarks each decay to a \(W\)-boson and \(b\)-quark, with the \(W\)-bosons subsequently decaying either to a pair of light (\(u,d,s,c\)) quarks \(qq^{}\) or a lepton-neutrino pair \(\)\((=e,)\). A basic event selection is then applied on the reconstructed objects at detector-level. Electrons and muons are selected with a transverse momentum requirement of \(p_{}>25\) GeV and absolute value of pseudorapidity \(||<2.5\). The \(b\) and light quarks are reconstructed with the anti-\(k_{}\) jet algorithm  using a radius parameter \(R=0.5\) and the same \(p_{}\) and \(||\) requirements as the leptons. Jets originating from \(b\)-quarks are identified with a "\(b\)-tagging" algorithm that incorporates a \(p_{}\) and angular (\(,\)) dependent identification efficiency and mis-tagging rate. Selected events are then required to contain exactly one lepton and at least 4 jets, of which at least two must be \(b\)-tagged. Events are separated into training and testing data sets, consisting of 9,865,402 and 1,332,514 events respectively.

Parton DataThe kinematics for the six final state partons are used as unfolding targets \((b,q_{1},q_{2},,_{l},l)\), along with the kinematics of the intermediate resonance particles \((W_{},W_{},t,)\), and the entire \(t\) system. The parton-level data consists of 11 momentum vectors, each represented by the five quantities \((M, E,p_{x},p_{y},p_{z})\); where \(M\) is the invariant mass of the particle; \(E\) is the energy; and \(p_{x}\), \(p_{y}\), and \(p_{z}\) are the Cartesian coordinates of the momentum. The Cartesian components of the momentum are used for regression, as they have roughly Gaussian distributions compared to the commonly employed cylindrical coordinate representation. Although regressing both the mass and energy for each parton over-defines the 4-momentum, these components exhibit different reconstruction characteristics due to sharp peaks in the mass distributions. During evaluation, either the mass or energy can be used to compute any derived quantities. In our experiments, the regressed mass is only used for the mass reconstruction, and the predicted energy is used for other kinematics.

Detector ObservationsThe detector-level jets and leptons are used as the conditioning data. The jets are stored as variable-length sets of momentum vectors with a maximum of 20 jets in each

    & Wasserstein & Energy & K-S & \(KL_{64}\) & \(KL_{128}\) & \(KL_{256}\) \\ 
**VLD** & 108.76 & 7.59 & 4.08 & **3.47** & **3.74** & **4.53** \\
**UC-VLD** & **73.56** & **6.35** & **3.41** & 5.77 & 7.10 & 8.48 \\
**C-VLD** & 389.62 & 25.39 & 4.65 & 9.54 & 10.09 & 10.79 \\ LDM & 402.32 & 24.09 & 5.91 & 14.71 & 16.34 & 17.92 \\ VDM & 2478.35 & 181.35 & 17.14 & 29.28 & 32.29 & 35.60 \\ CVAE & 484.56 & 32.29 & 6.37 & 7.79 & 9.17 & 10.60 \\ CINN & 3009.08 & 185.13 & 15.74 & 28.55 & 30.19 & 32.37 \\   

Table 1: Total distance measures across all 55 components for every model and metric. The independent sum of 1-dimensional distances for each component are summed across all the components to compute the total metrics.

event. This study is limited to semi-leptonic \(t\) events, so each event is guaranteed to have a single lepton. The missing transverse momentum in each event (MET) is also computed and included in the conditioning. The jets and leptons are represented using both polar, \((M,p_{},,)\), and Cartesian, \((E,p_{x},p_{y},p_{z})\), representations. We also include a one-hot particle identity, encoding either \(\) or \(e\) for the lepton, or \(b\) or non-\(b\) for the jets as estimated by the \(b\)-tagger, resulting in 12 dimensions for each jet.

TrainingNetworks were trained using the MSE for the reconstruction and noise loss, along with the physics-informed consistency loss with a weight of \(_{C}=0.1\). Each model underwent training for 24 hours using four NVIDIA RTX 3090 GPUs, resulting in 500,000 to 1,000,000 gradient steps for each model. Models were trained until convergence and then fine-tuned with a smaller learning rate. Full hyperparameters are presented in Appendix B.

Diffusion SamplingVariational diffusion models dynamically adapt the noise schedule during training by minimizing the variance of the ELBO . After training, however, VDMs may employ a more traditional discrete noise schedule, and this approach is preferable when sampling for inference. The PNDM  sampler is used for generating parton predictions.

Global DistributionsEach trained model was evaluated on the testing data, sampling a single parton configuration for each detector-level event. The global distributions of the 55 reconstructed parton components were then compared to the true distributions. Complete unfolded distributions are presented in Appendix F. Several highlighted reconstruction distributions are presented in Figure 3. Additionally, each model was assessed using several distribution-free measures of distance. The bin-independent Wasserstein and Energy distances, the non-parametric Kolmogorov-Smirnov (K-S) test, as well as three different empirical KL divergence measures using 64, 128, and 256 bins, are presented in Table 1. Full details about the distance functions are presented in Appendix C, and full tables of the distances per particle and per component are presented in Appendices D and E.

Global PerformanceThe two proposed VLD models with unconditional decoders (VLD and UC-VLD) consistently exhibited the best performance across all distance metrics. The end-to-end training procedure demonstrates improved performance over the pre-trained LDM model. It is interesting to note that UC-VLD has lower distribution-free distance wheras VLD has a lower histogram distance. This is likely because the histogram distance will soften the effect of outliers in the distribution as the bins will combine many different points into a single less noisy measurements. The conditional decoder in C-VLD and CVAE was found to worsen reconstruction. This is likely because the training procedure always employs the true encoded parton-detector pairs, \((z_{x},z_{c})\), whereas the inference procedure estimates the latent parton vector while using the true encoded detector variables for conditioning, \((},z_{c})\). The lower performance may be evidence that this inference data technically falls out-of-distribution for the conditional decoder, indicating that an unconditional decoder is a more robust approach. Finally, we find that all latent models greatly outperformed the models that directly reconstructed the partons in data-space (CINN and VDM).

Figure 3: Highlighted reconstruction components. The top row presents the full global histogram while the lower plot presents the ratio between the predicted histogram and the truth. Notice the improved mass shape compared to the pre-trained and non-latent models.

Posterior PredictionsOne key innovation of generative methods is the ability to sample from the posterior to illustrate the space of valid reconstructed partons for a detector level event. While the true posterior is not available, the unfolded distributions can be compared to a brute-force posterior distribution derived from the training data. This posterior is defined by a re-weighting of the parton level training data, where the weights are given by the inverse exponential of the \(_{2}\) distance between the testing event's detector configuration, \(y_{T}\), and every training event's detector configuration, \(y_{i}\): \(w_{i}=e^{-\|y_{T}-y_{i}\|}\). Selected posterior distributions are presented in Figure 4, and complete posterior examples for individual events are presented in Appendix G. The latent diffusion models have much smoother posteriors than the empirical estimates, with the proposed VLD model producing more density close to the true parton configuration. Critically, the brute-force posterior often matches the unconditional parton level distribution, proving it is difficult to recover the true posterior. We also note that the VLD model was also able to reproduce a bimodal neutrino \(\) posterior. Neutrinos are not directly measurable at the detector, and their kinematics must be inferred from the event's missing energy. For events with a single neutrino, the missing neutrino energy defines a quadratic constraint on the term which often leads to two configurations satisfying both energy and momentum conservation. The network appears to learn this phenomenon and presents two likely \(\) values for the neutrino.

## 6 Conclusions

This paper introduced a novel extension to variational diffusion models, incorporating elements from latent diffusion models to construct a powerful end-to-end latent variational generative model. An array of generative models were used to unfold semi-leptonic \(t\) events, an important inverse problem in high-energy physics. A unified model -- combining latent representations, continuous variational diffusion, and detector conditioning -- offered considerable advantages over the individual application of each technique. This addresses the challenge of scaling generative unfolding methods for high-dimensional inverse problems, an important step towards unfolding full collision events at particle-level. Despite being tested on a single topology, our method consistently improved baseline results, underscoring the importance of latent methods for such high-dimensional inverse problems. The framework presented may be broadly applicable to arbitrary topologies, although always limit to a single topology at a time. Future work will focus on broadening the method's applicability to different event topologies, unfolding to other stages of the event simulation chain (such as "particle level") to remove dependence on event topology, and evaluating its dependency on the simulator's prior distribution. The methods described in this study aim to provide a general end-to-end variational model applicable to numerous high-dimensional inverse problems in the physical sciences.

## 7 Acknowledgements

We would like to thank Ta-Wei Ho and Hideki Okawa for assistance in generating the \(t\) sample used in this study. DW, KG, AG, and MF are supported by DOE grant DE-SC0009920, and AG is also supported under contract DE-AC02-05CH11231. The work of AS and PB in part supported by ARO grant 76649-CS to PB. We thank Vinicius Mikuni for fruitful discussions on unfolding methods.

Figure 4: Highlighted reconstruction **per-event** posteriors for several events and components. We compare the LVD posteriors to an empirically brute-forced estimate of the posterior.