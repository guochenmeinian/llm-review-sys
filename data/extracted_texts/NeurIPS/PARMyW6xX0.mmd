# Type-to-Track: Retrieve Any Object

via Prompt-based Tracking

Pha Nguyen\({}^{1}\), Kha Gia Quach\({}^{2}\), Kris Kitani\({}^{3}\), Khoa Luu\({}^{1}\)

\({}^{1}\) CVIU Lab, University of Arkansas \({}^{2}\) pdActive Inc. \({}^{3}\) Robotics Institute, Carnegie Mellon University

\({}^{1}\){panguyen, khoaluu}@uark.edu \({}^{2}\)kquach@ieee.org \({}^{3}\)kkitani@cs.cmu.edu

uark-cviu.github.io/Type-to-Track

###### Abstract

One of the recent trends in vision problems is to use natural language captions to describe the objects of interest. This approach can overcome some limitations of traditional methods that rely on bounding boxes or category annotations. This paper introduces a novel paradigm for Multiple Object Tracking called _Type-to-Track_, which allows users to track objects in videos by typing natural language descriptions. We present a new dataset for that Grounded Multiple Object Tracking task, called _GroOT_, that contains videos with various types of objects and their corresponding textual captions describing their appearance and action in detail. Additionally, we introduce two new evaluation protocols and formulate evaluation metrics specifically for this task. We develop a new efficient method that models a transformer-based eMbed-ENcoDE-extRact framework (_MENDER_) using the third-order tensor decomposition. The experiments in five scenarios show that our _MENDER_ approach outperforms another two-stage design in terms of accuracy and efficiency, up to 14.7% accuracy and \(4\) speed faster.

## 1 Introduction

Tracking the movement of objects in videos is a challenging task that has received significant attention in recent years. Various methods have been proposed to tackle this problem, including deep learning techniques. However, despite these advances, there is still room for improvement in intuitiveness and responsiveness. One potential way to improve object tracking in videos is to incorporate user input into the tracking process. Traditional Visual Object Tracking (VOT) methods typically require

Figure 1: An example of the responsive _Type-to-Track_. The user provides a video sequence and a prompting request. During tracking, the system is able to discriminate appearance attributes to track the target subjects accordingly and iteratively responds to the user’s tracking request. Each box color represents a unique identity.

users to manually select objects in the video by points , bounding boxes [2; 3], or trained object detectors [4; 5]. Thus, in this paper, we introduce a new paradigm, called _Type-to-Track_, to this task that combines responsive typing input to guide the tracking of objects in videos. It allows for more intuitive and conversational tracking, as users can simply type in the name or description of the object they wish to track, as illustrated in Fig. 1. Our intuitive and user-friendly _Type-to-Track_ approach has numerous potential applications, such as surveillance and object retrieval in videos.

We present a new Grounded Multiple Object Tracking dataset named _GroOT_ that is more advanced than existing tracking datasets [6; 7]. _GroOT_ contains videos with various types of multiple objects and detailed textual descriptions. It is \(2\) larger and more diverse than any existing datasets, and it can construct many different evaluation settings. In addition to three easy-to-construct experimental settings, we propose two new settings for prompt-based visual tracking. It brings the total number of settings to five, which will be presented in Section 5. These new experimental settings challenge existing designs and highlight the potential for further advancements in our proposed research topic.

In summary, this work addresses the use of natural language to guide and assist the Multiple Object Tracking (MOT) tasks with the following contributions. First, a novel paradigm named _Type-to-Track_ is proposed, which involves responsive and conversational typing to track any objects in videos. Second, a new _GroOT_ dataset is introduced. It contains videos with various types of objects and their corresponding textual descriptions of 256K words describing definition, appearance, and action. Next, two new evaluation protocols that are tracking by _retrieval prompts_ and _caption prompts_, and three class-agnostic tracking metrics are formulated for this problem. Finally, a new transformer-based eMbed-ENcobe-extRact framework (_MENDER_) is introduced with third-order tensor decomposition as the first efficient approach for this task. Our contributions in this paper include a novel paradigm, a rich semantic dataset, an efficient methodology, and challenging benchmarking protocols with new evaluation metrics. These contributions will be advantageous for the field of Grounded MOT by providing a valuable foundation for the development of future algorithms.

## 2 Related Work

### Visual Object Tracking Datasets and Benchmarks

**Datasets.** To develop and train VOT models for the computer vision task of tracking objects in videos, various datasets have been created and widely used. Some of the most popular datasets for VOT are OTB [19; 8], VOT , GOT , MOT challenges [12; 14] and BDD100K . Visual object tracking has two sub-tasks: _Single Object Tracking_ (SOT) and _Multiple Object Tracking_ (MOT). Table 1 shows that there is a wide variety of object tracking datasets in both types available, each with its own strengths and weaknesses. Existing datasets with NLP [6; 7] only support the SOT task, while our _GroOT_ dataset supports MOT with approximately \(2\) larger in description size.

**Benchmarks.** Current benchmarks for tracking can be broadly classified into two main categories: _Tracking by Bounding Box_ and _Tracking by Natural Language_, depending on the type of initialization.

  
**Datasets** & **Task** & **NLP** & **\#Videos** & **\#Frames** & **\#Tracks** & **\#AnnBoxes** & **\#Words** & **\#Settings** \\ 
**OTB100** & SOT & ✗ & 100 & 59K & 100 & 59K & - & - \\
**VOT-2017** & SOT & ✗ & 60 & 21K & 60 & 21K & - & - \\
**GOT-10k** & SOT & ✗ & 10K & 1.5M & 10K & 1.5M & - & - \\
**TrackingNet** & SOT & ✗ & **30K** & **14.43M** & **30K** & **14.43M** & - & - \\ 
**MOT17** & **MOT** & ✗ & 14 & 11.2K & 1.3K & 0.3M & - & - \\
**TAO** & **MOT** & ✗ & 1.5K & **2.2M** & 8.1K & 0.17M & - & - \\
**MOT20** & **MOT** & ✗ & 8 & 13.41K & 3.83K & 2.1M & - & - \\
**BDD100K** & **MOT** & ✗ & **2K** & 318K & **130.6K** & **3.3M** & - & - \\ 
**LaSOT** & SOT & ✓ & 1.4K & **3.52M** & 1.4K & **3.52M** & 9.8K & 1 \\
**TNL2K** & SOT & ✓ & 2K & 1.24M & 2K & 1.24M & 10.8K & 1 \\
**Ref-DAVIS** & VOS & ✓ & 150 & 94K & 400+ & - & 10.3K & **2** \\
**Refer-YTOS** & VOS & ✓ & **4K** & 1.24M & **7.4K** & 131K & **158K** & **2** \\ 
**Ref-KITTI** & **MOT** & ✓ & 18 & 6.65K & - & - & 3.7K & 1 \\
**GroOT (Ours)** & **MOT** & ✓ & **1,515** & **2.25M** & **13.3K** & **2.57M** & **256K** & **5** \\   

Table 1: Comparison of current datasets. # denotes the number of the corresponding item. **Bold** numbers are the best number in each sub-block, while **highlighted** numbers are the best across all sub-blocks.

Previous benchmarks [20; 19; 8; 9; 21; 22; 22; 23] were limited to test videos before the emergence of deep trackers. The first publicly available benchmarks for visual tracking were OTB-2013  and OTB-2015 , consisting of 50 and 100 video sequences, respectively. GOT-10k  is a benchmark featuring 10K videos classified into 563 classes and 87 motions. TrackingNet , a subset of the object detection benchmark YT-BB , includes 31K sequences. Furthermore, there are long-term tracking benchmarks such as OxUvA  and LaSOT . OxUvA spans 14 hours of video in 337 videos, comprising 366 object tracks. On the other hand, LaSOT  is a language-assisted dataset consisting of 1.4K sequences with 9.8K words in their captions. In addition to these benchmarks, TNL2K  includes 2K video sequences for natural language-based tracking and focuses on expressing the attributes. LaSOT  and TNL2K  support one benchmarking setting with their provided prompts, while our _GroOT_ dataset supports five settings. Ref-KITTI  is built upon the KITTI  dataset and contains only two categories, including car and pedestrian, while our _GroOT_ dataset focuses on category-agnostic tracking, and outnumbers the frames and settings.

A similar task with a different nomenclature to the Grounded MOT task is Referring Video Object Segmentation (Ref-VOS) [16; 17], which primarily measures the overlapping area between the ground truth and prediction for a single foreground object in each caption, with less emphasis on densely tracking multiple objects over time. In contrast, our proposed _Type-to-Track_ paradigm is distinct in its focus on _responsively_ and _conversationally_ typing to track any objects in videos, requiring maintaining the temporal motions of multiple objects of interest.

### Grounded Object Tracking

**Grounded Vision-Language Models** accurately map language concepts onto visual observations by understanding both vision content and natural language. For instance, visual grounding  seeks to identify the location of nouns or short phrases (such as a black hat or a blue bird) within an image. Grounded captioning [30; 31; 32] can generate text descriptions and align predicted words with object regions in an image. Visual dialog  enables meaningful dialogues with humans about visual content using natural, conversational language. Some visual dialog systems may incorporate referring expression recognition  to resolve expressions in questions or answers.

**Grounded Single Object Tracking** is limited to tracking a single object with box-initialized and language-assisted methods. The GTI  framework decomposes the tracking by language task into three sub-tasks: Grounding, Tracking, and Integration, and generates tubelet predictions frame-by-frame. AdaSwitcher  module identifies tracking failure and switches to visual grounding for better tracking.  introduce a unified system using attention memory and cross-attention modules with learnable semantic prototypes. Another transformer-based approach  is presented including a cross-modal fusion module, task-specific heads, and a proxy token-guided fusion module.

### Discussion

Most existing datasets and benchmarks for object tracking are limited in their coverage and diversity of language and visual concepts. Additionally, the prompts in the existing Grounded SOT benchmarks do not contain variations in covering many objects in a single prompt, which limits the application of existing trackers in practical scenarios. To address this, we present a new dataset and benchmarking

  
**Approach** & **Task** & **NLP** & **Clb-agn** & **Feat** & **Stages** \\  GT1  & SOT & assist & ✗ & concat & **single** \\ TransVLT  & SOT & assist & ✗ & **attn** & **single** \\  TrackFormer  & **MOT** & - & ✗ & - & - \\  MDETFR-**T-rin** & **MOT** & **init** & ✓ & **attn** & two \\ TransRMOT  & **MOT** & **init** & ✓ & **attn** & two \\
**MENDER** & **MOT** & **init** & ✓ & **attn** & **single** \\   

Table 2: Comparison of key features of tracking methods. **Cls-agn** is for class-agnostic, while **Feat** is for the approach of feature fusion and **Stages** indicates the number of stages in the model design incorporating NLP into the tracking task. **NLP** indicates how text is utilized for the tracker: _assist_ (w/ box) or can _initialize_ (w/o box).

  
**Datasets** & **Followations** & **Frames** & **Friends** & **Aandpasses** & **Friends** & **Parts** \\   & Train & 7 & 5,316 & 546 & 12,297 & **3729** & (1) \\  & Test & 7 & 5,519 & 785 & 185,000 & **5.751** & (2) \\   & **Total** & 14 & 11,125 & 1,337 & 300,373 & **5.949** & (2) \\   & Train & 500 & 764,526 & 2,645 & 5,459 & 11,125 & **10.800** & (4) \\   & Val & 914 & 4,666,58 & 5,485 & 11,112 & **10.800** & (4) \\   & Test & 914 & 221,284,86 & 7,921 & 16,450 & & & \\   & Train & 4 & 8,911 & 2,337 & 136,927 & - & (5) \\  & Test & 4 & 4,479 & 1,504 & 7,465,465 & - & (6) \\   & **Total** & 8 & 13,410 & 3,837 & 2,102,387 & & \\   & **Total** & 8 & 1,314 & 6,337 & 2,102,387 & & \\   & **Total** & 8 & 1,329,287 & 1,329,287 & 2,102,387 & & \\   & **Total** & 13 & 2,487 & 1,329,287 & 2,102,387 & & \\   & **Total** & 13 & 2,487,17 & 13,294 & 2,250,509 & 21,546 & _all_ \\   & **Total** & 13 & 2,129,87 & 1,329,294 & 2,509,509 & 21,546 & _all_ \\   & **Total** & 13 & 2,139,287 & 1,329,294 & 2,509,509 & 21,546 & _all_ \\   & **Total** & 13 & 2,149,87 & 1,329,294 & 2,309,509 & 21,546 & _all_ \ to support the emerging trend of the Grounded MOT, where the goal is to align language descriptions with fine-grained regions or objects in videos.

As shown in Table 2, most of the recent methods for the Grounded SOT task are not class-agnostic, meaning they require prior knowledge of the object. GTI  and TransVLT  need to input the initial bounding box, while TrackFormer  need the pre-defined category. The operation used in  to fuse visual and textual features is _concatenation_ which can only support prompts describing a single object. A Grounded MOT can be constructed by integrating a grounded object detector, i.e. MDETR , and an object tracker, i.e. TrackFormer . However, this approach is low-efficient because the visual features have to be extracted multiple times. In contrast, our proposed MOT approach _MENDER_ formulates third-order _attention_ to adaptively focus on many targets, and it is an efficient _single-stage_ and _class-agnostic_ framework. The scope of _class-agnostic_ in our approach is constructing a large vocabulary of concepts via a visual-textual corpus, following [37; 38; 39].

## 3 Dataset Overview

### Data Collection and Annotation

Existing object tracking datasets are typically designed for specific types of video scenes [40; 41; 42; 43; 44; 2]. To cover a diverse range of scenes, _GroOT_ was created using official videos and bounding box annotations from the MOT17 , TAO , and MOT20 . The MOT17 dataset comprises 14 sequences with diverse environmental conditions such as crowded scenes, varying viewpoints, and camera motion. The TAO dataset is composed of videos from seven different datasets, such as the ArgoVerse  and BDD  datasets containing outdoor driving scenes, while LaSOT  and YFCC100M  datasets include in-the-wild internet videos. Additionally, the AVA , Charades , and HACS  datasets include videos depicting human-human and human-object interactions. By combining these datasets, _GroOT_ covers multiple types of scenes and encompasses a wide range of 833 objects. This diversity allows for a wide range of object classes with captions to be included, making it an invaluable resource for training and evaluating visual grounding algorithms.

We release our textual description annotations in COCO format . Specifically, a new key 'captions' which is a list of strings is attached to each 'annotations' item in the official annotation. In the MOT17 subset, we attempt to maintain two types of caption for well-visible objects: one describes the _appearance_ and the other describes the _action_. For example, the caption for a well-visible person might be ['a man wearing a gray shirt', 'person walking on the street'] as shown in Fig. 2. However, 10% of tracklets only have one caption type, and 3% do not have any captions due to their low visibility. The physical characteristics of a person or their personal accessories, such as their clothing, bag color, and hair color are considered to be part of their appearance. Therefore, the appearance captions include verbs 'carrying' or 'holding' to describe personal accessories. In the TAO subset, objects other than humans have one caption describing appearance, for instance, ['a red and black scooter']. Objects that are human have the same two types of captions as the MOT17 subset. An example is shown in Fig. 1(b). These captions are consistently annotated throughout the tracklets. Fig. 3 is the word-cloud visualization of our annotations.

### _Type-to-Track_ Benchmarking Protocols

Let \(\) be a video sample lasts \(t\) frames, where \(=\{_{t}\ |\ t<||\}\) and \(_{t}\) be the image sample at a particular time step \(t\). We define a request prompt \(\) that describes the objects of interest, and \(_{t}\) is the set of tracklets of interest up to time step \(t\). The _Type-to-Track_ paradigm requires a tracker network \((_{t},_{t-1},)\) that efficiently take into account \(_{t}\), \(_{t-1}\), and \(\) to produce \(_{t}=(_{t},_{t-1},)\). To advance the task of multiple object retrieval, another benchmarking set is created in addition to the _GroOT_ dataset. While training and testing sets follow a _One-to-One_ scenario, where each caption describes a single tracklet, the new retrieval set contains prompts that follow a _One-to-Many_ scenario, where a short prompt describes multiple objects. This scenario highlights the need for diverse methods to improve the task of multiple object retrieval. The retrieval set is provided with a subset of tracklets in the TAO validation set and three custom _retrieval prompts_ that change throughout the tracking process in a video \(\{_{t_{1}=0},_{t_{2}},_{t_{3}}\}\), as depicted in Fig. 1(a). The _retrieval prompts_ are generated through a semi-automatic process that involves: (i) selecting the most commonly occurring category in the video, and (ii) cascadingly filtering to the object that appears for the longest duration. In contrast, the _caption prompts_ are created by joining tracklet captions in the scene and keeping it consistent throughout the tracking period. We name these two evaluation scenarios as _tracklet captions_ **cap** and _object retrieval_ **refer**. With three more easy-to-construct scenarios, five scenarios in total will be studied for the experiments in Section 5. Table 3 presents the statistics of the five settings, and the data portions are highlighted in the corresponding colors.

### Class-agnostic Evaluation Metrics

As indicated in , long-tailed classification is a very challenging task in imbalanced and large-scale datasets such as TAO. This is because it is difficult to distinguish between similar fine-grained classes, such as bus and van, due to the class hierarchy. Additionally, it is even more challenging to treat every class independently. The traditional method of evaluating tracking performance leads to inadequate benchmarking and undesired tracking results. In our _Type-to-Track_ paradigm, the main task is not to classify objects to their correct categories but to retrieve and track the object of interest. Therefore, to alleviate the negative effect, we reformulate the original per-category metrics of MOTA , IDF1 , HOTA  into class-agnostic metrics:

\[=|}_{cls}^{CLS^{n}}(1- (_{t}+_{t}+_{t})}{_{t}_{ t}})_{cls},=1-(_{t}+_{t}+ _{t})_{CLS^{1}}}{_{t}(_{CLS^{1}})_{t}}\] (1)

\[=|}_{cls}^{CLS^{n}}(}{2++})_{cls},=)_{CLS^{1}}}{(2++)_{CLS^{1}}}\] (2)

\[=|}_{cls}^{CLS^{n}}( })_{cls},=_{CLS^{1 }})(_{CLS^{1}})}\] (3)

where \(CLS^{n}\) is the category, set size \(n\) is reduced to \(1\) by combining all elements: \(CLS^{n} CLS^{1}\).

## 4 Methodology

### Problem Formulation

Given the image \(_{t}\) and the request prompt \(\) describing the objects of interest, which can adaptively change between \(\{_{t_{1}}\), \(_{t_{2}}\), \(_{t_{3}}\}\) in the **ret** setting, and \(K\) is the prompt's length \(||=K\), let \(enc()\) and \(emb()\) be the visual encoder and the word embedding model to extract features of image tokens and prompt tokens, respectively. The resulting outputs, \(enc(_{t})^{M D}\) and \(emb()^{K D}\), where \(D\) is the length of feature dimensions. A list of region-prompt associations \(_{t}\), which contains objects' bounding boxes and their confident scores, can be produced by Eqn. (4):

\[_{t}=enc(_{t})emb( )^{},enc(_{t})=_{i}=(c_{ x},c_{y},c_{w},c_{h},c_{conf})_{i} i<M}_{t}\] (4)

where \(()\) is an operation representing the region-prompt correlation, that will be elaborated in the next section, \((,)\) is an object decoder taking the similarity and the image features to decode to object locations, thresholded by a scoring parameter \(\) (i.e. \(c_{conf}\)). For simplicity, the cardinality of the set of objects \(|_{t}|=M\), implying each image token produces one region-text correlation.

We define \(_{t}=_{j}=(tr_{x},tr_{y},tr_{w},tr_{h},tr_{conf}, tr_{id})_{j} j<N}_{t}\) produced by the tracker \(\), where \(N=|_{t}|\) is the cardinality of current tracklets. \(i\), \(j\), \(k\), and \(t\) are consistently denoted as indexers for objects, tracklets, prompt tokens, and time steps for the rest of the paper.

**Remark 1**_Third-order Tensor Modeling.: Since the Type-to-Track paradigm requires three input components \(_{t}\), \(_{t-1}\), and \(\), an **auto-regressive single-stage end-to-end framework** can be formulated via third-order tensor modeling._

To achieve this objective, a combination of initialization, object decoding, visual encoding, feature extraction, word embedding, and aggregation can be formulated as in Eqn. (5):

\[_{t}=initialize(_{t})&t=0\\ _{D D D}_{1}enc( _{t})_{2}ext(_{t-1})_{3}emb(),enc( _{t})& t>0\] (5)

where \(ext()\) denotes the visual feature extractor of the set of tracklets, \(ext(_{t-1})^{N D}\), \(_{D D D}\) is an all-ones tensor has size \(D D D\), ( \(_{n}\) ) is the \(n\)-mode product of the third-order tensor  to aggregate many types of token1, and \(initialize()\) is the function to ascendingly assign unique identities to tracklets for the first time those tracklets appear.

Let \(T^{M N K}\) be the resulting tensor \(T=_{D D D}_{1}enc(_{t})_{2}ext( _{t-1})_{3}emb()\). The objective function can be expressed as the log softmax of the positive region-tracklet-prompt triplet over all possible triplets, defined in Eqn. (6):

\[^{}_{enc,ext,emb}=_{_{enc,ext,emb}}( )}{_{l}^{K}_{n}^{N}_{m}^{M}(T_{lnm})})\] (6)

where \(\) denotes the network's parameters, the combination of the \(i^{th}\) image token, the \(j^{th}\) tracklet, and the \(k^{th}\) prompt token is the correlated triplet.

In the next subsection, we elaborate our model design for the tracking function \((_{t},_{t-1},)\), named _MENDER_, as defined in Eqn. (5), and loss functions for the problem objective in Eqn. (6).

### MENDER for Multiple Object Tracking by Prompts

The correlation in Eqn. (5) has the cubic time and space complexity \((n^{3})\), which can be intractable as the input length grows and hinder the model scalability.

**Remark 2**_Correlation Simplification.: Since both \(enc()\) and \(ext()\) are visual encoders, the region-prompt correlation can be equivalent to the tracklet-prompt correlation. Therefore, the region-tracklet-prompt correlation tensor \(T\) can be simplified to lower the computation footprint._

To design that goal, the extractor and encoder share network weights for computational efficiency:

\[ext(_{t-1})_{j}=ext\{_{j}\}_{t-1}= enc(_{t-1})_{i}_{i}_{j}}(T_{ij:)_{t-1}=(T_{ii:)_{t}}_{i}_{j }@note{footnote}{If $$ changes, the equivalence still holds true, see Appendix for the full algorithm.}\] (7)

where \(T_{ij:}\) and \(T_{ii:}\) are lateral and horizontal slices. In layman's terms, the region-prompt correlation at the time step \(t-1\) is equivalent to the tracklet-prompt correlation at the time step \(t\), as visualized in Fig. 4(a). Therefore, one practically needs to model the region-tracklet and tracklet-promptcorrelations which reduces time and space complexity from \((n^{3})\) to \((n^{2})\), significantly lowering computation footprint. We alternatively rewrite the decoding step in Eqn. (5) as follows:

\[_{t}=enc(_{t})ext(_{t-1})^{}ext(_{t -1})emb()^{},enc(_{t})  t>0\] (8)

**Correlation Representations.** In our approach, the correlation operation \(()\) is modelled by the _multi-head cross-attention_ mechanism , as depicted in Fig. 4(b). The attention matrix can be computed as:

\[()()=_{| }=() W _{}^{}() W_{K} ^{}^{}}{}\] (9)

where \(\) and \(\) tokens are one of these types: region, tracklet, prompt. \(()\) is one of the operations \(enc()\), \(emb()\), \(ext()\) as the corresponding operation to \(\) or \(\). Superscript \(W_{Q}\), \(W_{K}\), and \(W_{V}\) are the projection matrices corresponding to \(\) or \(\) as in the attention mechanism.

Then, the attention weight from the image \(_{t}\) to the prompt \(\) are computed by the matrix multiplication for \(_{|}\) and \(_{|}\) to aggregate the information from two matrices as in Eqn. (8). The result is the matrix \(_{||}=_{ |}_{|}\) that shows the correlation between each input or output. Then, the resulting attention matrix \(_{||}\) is used to produce the object representations at time \(t\):

\[_{t}=_{||} emb() W_{V}^{}+_{ |}ext(_{t-1}) W_{V}^{ }\] (10)

**Object Decoder \(dec()\)** utilizes context-aware features \(_{t}\) that are capable of preserving identity information while adapting to changes in position. The tracklet set \(_{t}\) is defined in the _auto-regressive_ manner to adjust to the movements of the object being tracked as in Eqn. (8). For decoding the final output at any frame, the decoder transforms the object representation by a 3-layer \(\) to predict bounding boxes and confidence scores for frame \(t\):

\[_{t}=_{j}=(tr_{x},tr_{y},tr_{w},tr_{h},tr_{conf})_ {j}}_{t}}{=}_ {t}+enc(_{t})\] (11)

where the identification information of tracklets, represented by \(tr_{id}\), is not determined directly by the \(\) model. Instead, the \(tr_{id}\) value is set when the tracklet is first initialized and maintained till its end, similar to _tracking-by-attention_ approaches .

Figure 4: The _auto-regressive_ manner takes advantage of the equivalent components. Simplifying the correlation in (a) turns the solution to _MENDER_ in (b), and reduces complexity to \((n^{2})\) where \(n\) denotes the size of tokens.

### Training Losses

To achieve the training objective function as in Eqn. (6), we formulate the objective function into two loss functions \(L_{|}\) and \(L_{|}\) for correlation training and one loss \(L_{GIOU}\) for decoder training:

\[=_{|}L_{|}+_{ |}L_{|}+_{GIOU}L_{GIOU}\] (12)

where \(_{|}\), \(_{|}\), and \(_{GIOU}\) are corresponding coefficients, which are set to \(0.3\) by default.

**Alignment Loss \(L_{|}\)** is a contrastive loss, which is used to assure the alignment of the ground-truth object feature and caption pairs \((,)\) which can be obtained in our dataset. There are two alignment losses used, one for all objects normalized by the number of positive prompt tokens and the other for all prompt tokens normalized by the number of positive objects. The total loss can be expressed as:

\[& L_{|}=\\ -^{+}|}_{k}^{|^{+}|}()_{j}^{} emb()_{k})}{ _{l}^{K}(ext()_{j}^{} emb( )_{l})})-^{+}|}_{j}^{|^{+}|} ()_{k}^{} ext()_{j })}{_{l}^{N}(emb()_{k}^{} ext ()_{l})})\] (13)

where \(^{+}\) and \(^{+}\) are the sets of positive prompts and image tokens corresponding to the selected \(enc()_{i}\) and \(emb()_{k}\), respectively.

**Objectness Losses.** To model the track's temporal changes, our network learns from training samples that capture both appearance and motion generated by two adjacent frames:

\[L_{|}=-_{j}^{N}()_{ j}^{} enc()_{i})}{_{l}^{N}(ext( )_{j}^{} enc()_{l})}) L_{GIOU}=_{j}^{N}_{GIOU}(_{j},_{i})\] (14)

\(L_{|}\) is the log-softmax loss to guide the tokens' alignment as similar to Eqn. (13). In the \(L_{GIOU}\) loss, \(_{i}\) is the ground truth object corresponding to \(_{j}\). The optimal assignment between \(_{j}\) or \(_{i}\) to the ground truth object is computed efficiently by the Hungarian algorithm, following DETR . \(_{GIOU}\) is the Generalized IoU loss .

## 5 Experimental Results

### Implementation Details

**Experimental Scenarios.** We create three types of prompt: _category name_\(}\), _category synonyms_\(,\)_category definition_\(\). One _tracklet captions_\(\) scenario is constructed by our detailed annotations and one more _objects retrieval_\(\) scenario is given in our custom request prompts as described in Subsec. 3.2. The dataset contains 833 classes, each has a name and a corresponding set of synonyms that are different names for the same category, such as [man, woman, human, pedestrian, boy, girl, child] for person. Additionally, each category is described by a _category definition_ sentence. This definition makes the model deal with the variations in the text prompts. We join the names, synonyms, definitions, or captions and filter duplicates to construct the prompt. Trained models use as the same type as testing. We annotated the raw tracking data of the best-performant tracker (i.e., BoT-SORT  at 80.5% MOTA and 80.2% IDF1) at the time we constructed experiments and used it as the sub-optimal ground truth of MOT17 and MOT20 (parts _(2, 4)_ in Table 3). That is also the raw data we used to evaluate all our ablation studies.

**Datasets and Metrics.** RefCOCO+  and Flickr30k  serve as pre-trained datasets for acquiring a vocabulary of visual-textual concepts . The \(ext()\) operation is not involved in this training step. After obtaining a pre-trained model from RefCOCO+ and Flickr30k, we train and evaluate our model for the proposed _Type-to-Track_ task on all five scenarios on our _GroOT_ dataset and the first-three scenarios for MOT20 . The tracking performance is reported in class-agnostic metrics CA-MOTA, CA-IDF1, and CA-HOTA as in Subsec. 3.3 and mAP50 as defined in .

**Tokens Production.**\(emb()\) utilizes RoBERTa  to convert the text input into a sequence of numerical tokens. The tokens are fed into the RoBERTa-base model for text encoding using a12-layer transformer network with 768 hidden units and 12 self-attention heads per layer. \(enc()\) is implemented using a ResNet-101  as the backbone to extract visual features from the input image. The output of the ResNet is processed by a Deformable DETR encoder  to generate visual tokens. For each dimension, we use sine and cosine functions with different frequencies as positional encodings, similar to . A feature resizer combining a list of \((,,)\) is used to map to size \(D=512\) for all token producers.

### Ablation Study

**Comparisons in Different Scenarios.** Table 4 shows comparisons in the performance of different prompt inputs. For MOT17 and MOT20, the _category name_ is 'person', while _category definition_ is 'a human being'. Since the prompt by _category definition_ is short, it does not differ much from the \(}\) setting. However, the \(}\) setting shuffles between some words, resulting in a slight decrease in CA-MOTA and CA-IDF1. The \(}\) setting results in prompts that contain more diverse and complex vocabulary, and more context-specific information. It is more difficult for the model to accurately localize the objects and identify their identity within the image, as it needs to take into account a wider range of linguistic cues, resulting in a decrease in performance compared to **def** (59.5% CA-MOTA and 54.8% CA-IDF1 vs 67.3% CA-MOTA and 72.4% CA-IDF1 on MOT17).

For TAO, the **def** setting has a significant number of variations and many tenuous connections in the scene context, for example, 'an aircraft that has a fixed wing and is powered by propellers or jets' for the airplane category. Therefore, it results in a decrease in performance (16.8% CA-MOTA and 27.7% CA-IDF1) compared to \(}\) (20.7% CA-MOTA and 32.0% CA-IDF1), because the \(}\) setting is more specific on the object level than category level. The best performant setting is \(}\) (27.3% CA-MOTA and 37.2% CA-IDF1), where names are combined.

**Simplied Attention Representations.** Table 4 also presents the effectiveness of different attention representations of the full tensor \(T\) (denoted by \(\)) and the simplified correlation (denoted by \(\)). The performance is reported with frame per second (FPS), which is self-measured on one GPU NVIDIA RTX 3060 12GB. Overall, the performance of simplified correlation is witnessed with a superior speed of up to 2\(\) (7.8 FPS vs 3.4 FPS of \(}\) on MOT17 and 11.5 FPS vs 7.6 FPS of \(}\) on TAO), resulting in and a slight increase in accuracy due to attention stability and precision gain.

  
**P** & **sin** & **CA-MOTA** & **CA-IDF1** & **MT** & **IDs** & **mAP** & **FPS** \\  
**min** & \(\)/ & 67.0 & 71.20 & 544 & 1352 & 0.876 & 10.3 \\ 
**syn** & \(\)/ & 65.10 & 71.10 & 554 & 1348 & 0.874 & 10.3 \\ 
**def** & \(\) & 67.00 & 72.10 & 556 & 1343 & 0.876 & 5.8 \\ 
**def** & \(\) & **67.30** & **72.40** & **568** & **1322** & **0.877** & **10.3** \\ 
**sign** & \(\) & 58.20 & 53.20 & **289** & **171** & 0.644 & 3.4 \\  & \(\) & **59.50** & **54.80** & 201 & **1734** & **0.688** & **7.8** \\  
**content** & \(\)/ & 72.00 & 3596 & & & & \\ 
**sign** & \(\)/ & 27.30 & 32.05 & 3253 & 4284 & 0.212 & 11.2 \\ 
**sign** & \(\)/ & 25.70 & 36.10 & 3122 & 5048 & 0.189 & 11.2 \\ 
**def** & \(\) & 15.20 & 27.30 & 2452 & 6235 & 0.154 & 6.2 \\  & \(\) & **27.70** & **2547** & **6118** & **0.188** & **10.5** \\ 
**sign** & \(\)/ & 20.30 & 31.80 & 2043 & 522 & 0.188 & 4.3 \\  & \(\) & **20.70** & **32.00** & **3103** & **5192** & **0.184** & **8.7** \\ 
**br

### Comparisons with A Baseline Design

Due to the new proposed topic, no current work has the same scope or directly solves our problem. Therefore, we compare our proposed _MENDER_ against a two-stage baseline tracker in Table 5. We use current SOTA methods to develop this approach, i.e., MDETR  for the grounded detector, while TrackFormer  for the object tracker. It is worth noting that our _MENDER_ relies on direct regression to locate and track the object of interest, without the need for an explicit grounded object detection stage. Table 5 shows our proposed _MENDER_ outperforms the baseline on both CA-MOTA and CA-IDF1 metrics in all four settings _category synonyms_, _category definition_, _tracklet captions_ and _object retrieval_ (25.7% vs. 21.3%, 16.8% vs. 14.6%, 20.7% vs. 15.3% and 32.9% vs. 25.7% CA-MOTA on TAO), while can maintain up to \(4\) run-time speed (10.3 FPS vs 2.2 FPS). The results indicate that training a single-stage network enhances efficiency and reduces errors by avoiding separate feature extractions for both detection and tracking steps.

### Comparisons with State-of-the-Art Approaches

The _category name_\(}\) setting is also the official MOT benchmark. Table 6 is the comparison of our result on the _category name_ setting on the official leaderboard of MOT17, compared with other state-of-the-art approaches, including ByteTrack  and TrackFormer . Note that our proposed _MENDER_ is one of the first attempts at the Grounded MOT task, not to achieve the top rankings on the general MOT leaderboard. In contrast, other SOTA approaches benefit from the efficient single-category design in their separate object detectors, while our single-stage design is agnostic to the category and for flexible textual input. Compared to TrackFormer , our proposed _MENDER_ only demonstrates a marginal decrease in identity assignment (67.1% vs 68.0% CA-IDF1). The decrease in the CA-MOTA stems from our detector's design which integrates flexible input.

## 6 Conclusion

We have presented a novel problem of _Type-to-Track_, which aims to track objects using natural language descriptions instead of bounding boxes or categories, and a large-scale dataset to advance this task. Our proposed _MENDER_ model reduces the computational complexity of third-order correlations by designing an efficient attention method that scales quadratically w.r.t the input sizes. Our experiments on three datasets and five scenarios demonstrate that our model achieves state-of-the-art accuracy and speed for class-agnostic tracking.

**Limitations.** While our proposed metrics effectively evaluate the proposed _Type-to-Track_ problem, they may not be ideal for measuring precision-recall characteristics in retrieval tasks. Additionally, the lack of the question-answering task in data and problem formulation may limit the algorithm to not being able to provide language feedback such as clarification or alternative suggestions. Additional benchmarks incorporating question-answering are excellent research avenues for future work. While the performance of our proposed _MENDER_ may not be optimal for well-defined categories, it paves the way for exploring new avenues in open vocabulary and open-world scenarios .

**Broader Impacts.** The _Type-to-Track_ problem and the proposed _MENDER_ model have the potential to impact various fields, such as surveillance and robotics, where recognizing object interactions is a crucial task. By reformulating the problem with text support, the proposed methodology can improve the intuitiveness and responsiveness of tracking, making it more practical for video input support in large-language models  and real-world applications similar to ChatGPT. However, it could bring potential negative impacts related to human rights by providing a video retrieval system via text.

  
**Approach** & **Cls-agn** & **CA-IDF1** & **CA-MOTA** & **CA-HOTA** & **MT** & **ML** & **AssA** & **DetA** & **LocA** & **IDs** \\  ByteTrack  & ✗ & 77.3 & 80.3 & 63.1 & 957 & 516 & 52.7 & 55.6 & 81.8 & 3,378 \\ TrackFormer  & ✗ & 68.0 & 74.1 & 57.3 & 1,113 & 246 & 54.1 & 60.9 & 82.8 & 2,829 \\ QuasiDense  & ✗ & 66.3 & 68.7 & 53.9 & 957 & 516 & 52.7 & 55.6 & 81.8 & 3,378 \\ CenterTrack  & ✗ & 64.7 & 67.8 & 52.2 & 816 & 579 & 51.0 & 53.8 & 81.5 & 3,039 \\ TraDeS  & ✗ & 63.9 & 69.1 & 52.7 & 858 & 507 & 50.8 & 55.2 & 81.8 & 3,555 \\ CTracker  & ✗ & 57.4 & 66.6 & 49.0 & 759 & 570 & 45.2 & 53.6 & 81.3 & 5,529 \\ 
**MENDER** & ✓ & 67.1 & 65.0 & 53.9 & 678 & 648 & 54.4 & 53.6 & 83.4 & 3,266 \\   

Table 6: Comparisons to the state-of-the-art approaches on the _category name_

**Acknowledgment.** This work is partly supported by NSF Data Science, Data Analytics that are Robust and Trusted (DART), and Google Initiated Research Grant. We also thank Utsav Prabhu and Chi-Nhan Duong for their invaluable discussions and suggestions and acknowledge the Arkansas High-Performance Computing Center for providing GPUs.