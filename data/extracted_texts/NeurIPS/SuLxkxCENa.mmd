# Deep Equilibrium Algorithmic Reasoning

Dobrik Georgiev

University of Cambridge

dgg30@cam.ac.uk

&JJ Wilson

Independent Researcher

josephjwilson74@gmail.com

Davide Buffelli

MediaTek Research

davide.buffelli@mtkresearch.com

&Pietro Lio

University of Cambridge

pl219@cam.ac.uk

###### Abstract

Neural Algorithmic Reasoning (NAR) research has demonstrated that graph neural networks (GNNs) could learn to execute classical algorithms. However, most previous approaches have always used a recurrent architecture, where each iteration of the GNN matches an iteration of the algorithm. In this paper we study neurally solving algorithms from a different perspective: since the algorithm's solution is often an equilibrium, it is possible to find the solution directly by solving an equilibrium equation. Our approach requires no information on the ground-truth number of steps of the algorithm, both during train and test time. Furthermore, the proposed method improves the performance of GNNs on executing algorithms and is a step towards speeding up existing NAR models. Our empirical evidence, leveraging algorithms from the CLRS-30 benchmark, validates that one can train a network to solve algorithmic problems by directly finding the equilibrium. We discuss the practical implementation of such models and propose regularisations to improve the performance of these equilibrium reasoners.

0

## 1 Introduction

Algorithms, while straightforward in theory, become challenging to deploy in real-world scenarios. They operate in abstract domains with very specific conditions and types of inputs, which are represented with scalars. The main hurdle is the need to "collapse" reality into a scalar every time an algorithm is used, something usually done based on intuition rather than principled science . Neural Algorithmic Reasoning (NAR; 46) has been proposed to address this issue by utilising specialised neural network architectures to break this scalar bottleneck by executing algorithms in higher-dimensional space made out of arrays of numbers (vectors). The task of mapping reality into this vectorial space is delegated to automated gradient-based optimisation techniques rather than relying on human operators.

While NAR does not provide the correctness guarantees of its classical counterparts, robust performance can be achieved through _alignment_ - submodules of the model architecture correspond to easy-to-learn subparts of the algorithm (or class of). Graph Neural Networks (GNNs) have emerged as the most convenient architecture to execute all types of algorithms  and GNNs that align better to the target algorithm achieve better generalisation. This alignment game  has led to a sequence of exciting research - from aligning the architecture with iterative algorithms  to proving that "graph neural networks are dynamic programmers" , especially if their message-passing function  takes into account 3-node interactions.

The aforementioned papers focus on aligning the computation of the GNN with an algorithm or a specific algorithm class (e.g. dynamic programming), but ignore the properties _at the time of algorithm termination_. For the algorithms in the CLRS-30 algorithmic reasoning benchmark  once the optimal solution is found, further algorithm iterations will not change it. For example, in dynamic programming shortest-paths algorithms making additional iterations would not alter the optimality of the shortest paths' distances found. Such a state is called an _equilibrium_ - additional applications of a function (an algorithm's iteration) to the state leave it unchanged.

In this paper:

1. We explore the connection between execution of algorithms and equilibrium finding through the use of Denotational semantics and Domain theory. (section 3)
2. Inspired by the above, we implement a class of deep equilibrium algorithmic reasoners (DEARs) that learn algorithms by identifying the equilibrium point of the GNN equation and propose improvements to them. (section 4)
3. Our results suggest that the above reasoners can be _successfully_ trained. Not only does equilibrium algorithmic reasoning achieve better overall performance with less expressive (and expensive) GNNs, but is also competitive to the more expressive (and expensive) NAR models. All this is done without providing any information on the number of algorithmic steps - neither at train nor at test time. (section 5)
4. DEARs also drastically improve the inference speed - an achievement made possible by the use of optimised root-finding algorithms and by decoupling the neural model from the _sequential_ implementation of algorithms in standard benchmark datasets. (section 5)

Related workThe main proposed application of NAR is settings where one wants to apply an algorithm, but it is impossible to represent reality with a single scalar, hence an "executor" operating in vector space and faithful to the algorithm is required [47; 50]. As NAR models are neural clones of algorithms, they need to provide correct output even for previously unobserved input sizes. Achieving robust out-of-distribution (OOD) generalisation is tricky. To this end a plethora of works have dedicated their attention to it - [8; 13; 14; 42] to name a few. Except for one concurrent work (a blog post;), those works focus on improving the GNN step and many completely ignore the termination of algorithms or any properties of the last state, such as equilibrium. This work, similarly to Xhonneux et al. , studies neurally finding solutions to algorithms by relying on the equilibrium property. We, however, attempt to give the precise assumptions required for this approach to work. Further, we implement a more robust model than theirs, which achieves comparable or better performance to baselines. Finally, we propose modifications to improve equilibrium NARs.

## 2 Background

Algorithmic ReasoningLet \(A:_{A}_{A}\) be an algorithm, acting on some input \(_{A}\), producing an output \(A()_{A}\) and let \(_{A}\)/\(_{A}\) be the set of possible inputs/outputs \(A\) can read-/return. In algorithmic reasoning, we aim to learn a function \(:_{A}_{A}\), such that \( A\). Importantly, we will not be learning simply an input-output mapping, but we will aim to align to the algorithm \(A\)'s trajectory. The alignment is often achieved through direct supervision1 on the intermediate states of the algorithm. To capture the execution of \(A\) on an input \(x\) we can represent it as

\[}_{0}=()\] (1a) \[}_{}=( A_{t}(}_{0}) )}_{}\] (1b) \[A()=(}_{})\] (1c)

where Preproc and Postproc are some simple pre- and post-processing (e.g.initialising auxiliary variables or returning the correct variable), \(}_{}_{A}\) is \(A\)'s internal (hidden) state, \(A_{t}\) is a subroutine (or a set of) that is executed at each step and the number of steps depends on a boolean property being satisfied (e.g. all nodes visited). It is therefore no surprise that the encode-process-decode architecture  is the de-facto choice when it comes to NAR. Thus, the architecture can be neatly represented as a composition of three learnable components: \(=g_{} P f_{}\), where \(g_{}:_{A}^{d}\)and \(f_{}:^{d}_{A}\) are the encoder and decoder function respectively (usually linear projections) and \(P:^{d}^{d}\) is a processor that mimics the rollout (Equation 1b) of \(A\). The processor often uses a message-passing GNN at its core.

Clrs-30The _CLRS-30_ benchmark  includes 30 iconic algorithms from the _Introduction to Algorithms_ textbook . Each data instance for an algorithm \(A\) is a graph annotated with features from different algorithm stages (_input_, _output_, and _hint_), each associated with a location (_node_, _edge_, and _graph_). Hints contain time series data representing the algorithm rollout and include a temporal dimension often used to infer the number of steps \(\). Features in CLRS-30 have various datatypes with associated losses for training. The test split, designed for assessing out-of-distribution (OOD) generalisation, comprises graphs four times larger than the training size.

Deep Equilibrium ModelsDeep equilibrium models [DEQs 4] are a class of implicit neural networks . The functions modelled with DEQs are of the form:

\[^{*}=f_{}(^{*},)\] (2)

where \(\) is input, \(f_{}\) is a function parametrised by \(\) (e.g. a neural network) and \(^{*}\) is the output. \(^{*}\) is an equilibrium point to the eventual output value of an infinite depth network where each layer's weights are shared, i.e. \(f_{}^{[i]}=f_{}\). By re-expressing (2) as \(g_{}=f_{}(^{*},)-^{*}\) DEQs allow us to find the fixed point \(^{*}\) via any black-box root-finding method [e.g. 2; 9], without the actual need to unroll the equation until convergence, allowing us to reduce steps. For backpropagation the gradient \(/\) could be calculated using the Implicit Function Theorem (cf. 4) and no intermediate state has to be stored, giving us a constant memory cost of gradient computation regardless of the number of iterations until convergence.

Expander graphsMPNNs operate by exchanging information between adjacent nodes . It has been identified that the message passing process can be hindered by a phenomenon known as _oversquashing_, which occurs when a large volume of messages are compressed into fixed-sized vectors. The importance of overcoming the negative implication posed by this phenomenon is crucial for the overall expressivity of GNNs , particularly in the context of long-range node interactions.

To this end, several spectral methods have been proposed to mitigate oversquashing by increasing the Cheeger constant [3; 6; 30]. A higher Cheeger constant provides a measurement that a graph is globally lacking bottlenecks. The novel approaches include graph rewiring techniques [7; 44], as well as significant independent bodies of research recognising the efficacy of expander graphs [6; 12; 41], due to their desirable properties.

Expander graphs are proven to be highly connected sparse graphs (\(|E|=(|V|)\)) with a low diameter , thus offering advantageous properties for information propagation. Consequently, this facilitates messages to be passed between any pair of nodes in a short number of hops, and as a result, alleviating oversquashing. Formally, a graph \(G=(V,E)\) is defined as an expander if it satisfies certain expansion properties. One common definition involves the aforementioned Cheeger constant. In the work of Deac et al. , a high Cheeger constant is equivalent to a graph being bottleneck free [12, Definition 3], and that an expander has a high Cheeger constant [12, Theorem 5].

There are various methods for constructing expander graphs. We opt for the _deterministic_ algebraic approach as in Deac et al. , utilising Cayley graphs. Specifically, we leverage Definition 8 and Theorem 9 of  to construct the Cayley graph for the _special linear group_\((2,_{n})\) and its generating set \(S_{n}-\) see p.5 of Deac et al.  for details. Note, that the order of a Cayley graph for \(_{n}\) is \(|V|=(n^{3})\). Hence, for many input graphs, a Cayley graph of the same size may not exist.

## 3 Denotational semantics: The denotation of a while loop statement

This aim of this section is to draw the parallel between finding equilibriums and executing an algorithm, in order to answer if and when an equilibrium NAR model can be successful. The following paragraphs introduce the mathematical tools for formalising _fixed points_ - denotational semantics  and domain theory .

Denotational semanticsTo every programming language expression2\(P\) denotational semantics provides an interpretation \( P\), which is a mathematical object (often a function), representing the behaviour of the expression to different inputs. These _denotations_ must be: 1) abstract, i.e. independent of language and hardware, hence functions are natural choice; 2) compositional, i.e. \( P\) can only be defined in terms of the _denotations_ of \(P\)'s subexpressions, but not \(P\) itself; 3) model the computation \(P\) performs. As an example, we will use the lightweight imperative programming language **IMP3**. It consists of _numbers_, _locations_, _arithmetic expressions_, _boolean expressions_ and _commands_. Examples of **IMP** are given in Appendix A - we will use blue for **IMP** and encourage the reader to check how we use colour in the appendix. Albeit small, the language is Turing-complete and all algorithms we experiment with can be defined in it.

Denote the set of variable locations with \(\) - those are all variables/array elements we can ever define. A good analogy to think of is the addresses in the language \(\). The notation we can use to represent a program state is \([X 1,B-48,]\) and means that the value of \(X\) is 1, the value of \(B\) is \(-48\) and so on. In other words, program states map locations to integers, s.t. a location can be mapped only once. Hence states are functions and the set of all program states \(State\) is the set of functions mapping locations to integers: given \(s State\), \(s(L)\) is the value at the location \(L\)_for the state \(s\)_. The value for location \(L\) in a different \(s^{} State\), \(s^{}(L)\), may or may not differ. The denotation of arithmetic / boolean expressions / commands are the functions with domain \(State\). These will be represented in the form of _lambda abstractions_, i.e. \( x S.M\) rather than \(f(x S)=M\), where \(S\) is a set and \(M\) is a function body. The codomain of the denotation depends on the type of expression: \( a:State\) for arithmetic expressions, \( b:State\), for boolean expressions and \( c:State State\) for commands. Since commands transform state, they are also called state transformers. _Commands' denotations are partial functions, as expressions like_ while true do skip_never terminate and have no denotation._

For a large portion of the above language, it is trivial and intuitive to define the denotations by structural recursion. For example:

\[bc_{0}c_{1}= s State.  c_{0}(s)& b (s)\\  c_{1}(s)&\]

\[ c_{0}\,;c_{1}= s State.  c_{1}( c_{0}(s))&  skip= s State.\,s\]

The only denotation that cannot be expressed recursively, is that of the while construct. Let \(w=bc\). By program equivalence, \(w=bc;w\). Therefore

\[ w=bc;w= s State.  w\,( c(s))&  b(s)\\ s&\]

but this is not a valid definition, since it reuses \( w\) (highlighted in red above). Denotational semantics solves this problem, by defining a function \(f_{b,c}:(State State)(State State)\) which takes one state transformer and returns another:

\[f_{b,c}=(State State). s State. \,(c(s))&b(s)\\ s&\] (3)

\(\) is now a function variable. The denotation of \( w\) is the fixed point of \(f_{ b, c}\), i.e. \( w=f_{ b, c}(  w)\). In order to find the denotation, we need to solve the fixed point. To aid the reader a full worked example of computing the denotation for a while loop construct is given in Appendix B.

Domain theoryScott  provides a framework with which we can both find and also characterise solutions for fixed point equations.4 Define \(D\) as the domain of state transformers \((State State)\). A partial order5\(\) on \(D\) is defined as follows: \(w w^{}\) iff \( s State\) if \(w(s)\) is defined then \(w(s)=w^{}(s)\). In other words \(w^{}\) keeps old mappings and only defines new ones. The totally undefined partial function \(\) is the least element in \(D\). This function contains no location to value mappings. A chain is a sequence of elements of \(D\), s.t. \(d_{0} d_{1} d_{2}\). The supremum of the chain, called _least upper bound (lub)_, is denoted as \(_{n 0}d_{n}\). There could exist different chains, but, by definition, all chains in a domain must have a lub.

A function \(f:D D\) is monotonic iff \( d,d^{} D\). \((d d^{} f(d) f(d^{}))\). In other words, if the second state defined more mappings than the first and we apply one iteration step to both states, the state resulting from the second will still define more mappings. Monotonic functions for which \(_{n 0}f(d_{n})=f(_{n 0}d_{n})\) are also called continuous. In plain language, if a function is continuous and we are provided with a chain, the lub of \(f\) applied to every chain element is the same as \(f\) applied to the lub of the chain. An element \(d^{} D\) is defined to be _pre-fixed point_ if \(f(d^{}) d^{}\) - applying \(f\) does not define anything new. The fixed point \(fix(f)\) of \(f\) is the least pre-fixed point of \(f\). By utilising antisymmetry6 of \(\) and the two properties of \(fix(f)\) (pre-fixed point and least) we can obtain \(f(fix(f))=fix(f)\). By Tarski's theorem , any continuous \(f:D D\) has a least pre-fixed point. This fixed point can be found, by taking the lub of the chain of applications of f: \(fix(f)=_{n 0}f^{n}()\). The helper function \(f_{b,c}\) from Equation 3 is continuous [proof is given on p.120 of 23], therefore a direct result is that if the while\(b\)do\(c\) terminates then its denotation exists and is _the least_ fixed point of sequence of iterations (compared to picking any fixed point).

Denotational semantics and NARThe above detour into denotational semantics has affirmed the existence of a connection between equilibrium models and algorithms (as conjectured by Xhonneux et al. ). Under the assumptions that:

* while not computable in the general case, this holds true for experiments, as we are dealing with offline algorithms with provable termination
* the algorithms we train on can be modelled as "while\(b\)do\(c\)" constructs within **IMP**

the least fixed point exists and can be found by taking the first "state" of the algorithm where future iterations on it have no effect. In Appendix C we have further annotated three algorithms from the official code of the CLRS benchmark: BFS, Floyd-Warshall, strongly connected components. Those annotations clearly show that algorithms can be rewritten in **IMP** regardless of their implementation size. While BFS is clearly a "while\(b\)do\(c\)"-type of algorithm, annotating the other two reveals that either the network may need more input features to decide termination (Floyd-Warshall; Listing 2) or that the algorithm can be composed of several while loops where each \(c\) is another while loop on its own (strongly connected components; Listing 3). Fortunately, our approach is not doomed to fail: a single DEQ layer can model any number of "stacked" DEQ layers [32, chapter 4].

## 4 Deep equilibrium algorithmic reasoning

ArchitectureWe implement our processors/encoders/decoders following Ibarz et al. . The most notable difference7 to their implementation is that ours uses a sparse graph representation. This requires us to assume a fully connected graph on tasks where no graph structure exists, in order to be able to give pointer predictions, and to reimplement the strongly connected components algorithm so that the output pointers are always in the edge set (this did not change the difficulty of the task).

The final node embeddings, from which the output is decoded, are the solution to the equation:

\[^{(*)}=P_{}(^{(*)})\] (4)

where \(P_{}(^{(*)})=P(^{(*)},, )\), \(/\!\) are the encoded node and edge feature matrices and \(P\) is the processor function. \(^{(t)}\) are the stacked latent states of the nodes at timestep \(t\) (with \(^{(0)}=\)). The above Equation 4 matches the signature of Equation 2, and can be solved via root-finding (we employ torchdeq; MIT License), as if it is \(f_{}\) of a DEQ. Any model using this technique will be called _deep equilibrium algorithmic reasoner_ (**DEAR**) in our experiments. The default processor in the majority of our experiments is a PGN  with a gating mechanism as in Ibarz et al. , but we note that DEARs can use any kind of processor.

Finding the fixed pointThe torchdeq library provides several solvers. The most basic one is _fixed-point iteration_, equivalent to repeating Equation 4 until convergence. However, in our very first experiments the solver needed more iterations than the algorithm we train on. We therefore opted for the _Anderson_ solver (implements Anderson acceleration ) and abandoned fixed-point iteration:

\[}^{(t+1)}=P_{}(^{(t)})^{( t+1)}=SolverStep([^{(0)}^{(t)}],}^{(t+1)})\]

The criteria for pre-fixed point check torchdeq implements is distance-based: for a state \(^{(t)}\) to be considered a pre-fixed point, the distance to the next state has to be under a pre-defined threshold \(\). We kept the criteria but modified the library to always return the least pre-fixed point (see Appendix E). This is in contrast to picking the pre-fixed point with the least distance to next state (the default option in torchdeq) and is a decision largely motivated from section 3. Due to a lack of a suitable definition for the domain of NAR trajectories, we define \( t\). \(^{(t)}^{(t+1)}\), i.e. we pick the first state that passes the pre-fixed point check.

Globally propagating informationFor problems defined on graphs it is in theory possible that the number of solver iterations needed to find equilibrium is less than the diameter of the graph. While, in practice, this is unlikely to happen we hypothesise that improving long-range interactions could improve the convergence of DEAR. For this reason, we adopt the implementation of Cayley Graph Propagation (CGP) . Contrasted to Expander Graph Propagation (EGP) , which addresses the graph size misalignment (see section 2) by truncating the Cayley graph, CGP keeps the extra nodes as virtual nodes. The CGP model upholds the aforementioned advantageous properties of an expander graph in a more grounded manner by preserving the complete structure.

In GNNs, the benefits of augmenting a graph with virtual nodes and providing message-passing shortcuts have been observed to improve performance in various tasks [10; 26; 27]; further supported by the theoretical analysis . Additionally, by retaining the complete Cayley graph structure we improve the structure-aware representations by varying the neighbourhood ranges .

No hint by defaultWe do not make any use of hints (supervising on intermediate algorithm state). First, although it may seem counterintuitive, it has been shown that a NAR model can successfully generalise, and even give better results when trained to only predict the correct output [8; 37]. Second, the fact that the solver uses the GNN exactly once per call _does not imply that one step of the solver would correspond to one iteration of the algorithm_, bringing uncertainty which DEAR states to match to which algorithm step. While we propose an alignment scheme (see next paragraph), which has the potential to integrate hints, we leave this for future work.

AlignmentOur idea of alignment is visualised in Figure 1. We are given two trajectories of states, one obtained from unrolling GNN iterations as in classical NAR and another obtained from using DEAR. We would like to match DEAR to NAR, such that \( i j,i^{} j^{}\) if we have committed to aligning DEAR state \(^{(i^{})}\) to NAR state \(^{(j)}_{}\), we cannot align any \(^{(j^{})}\) to \(^{(i)}_{}\) and from the same start we would like to reach the same final state. In other words, _skipping states is allowed, going back in time is not_. This auxiliary supervision would also improve the monotonicity of DEARs, encouraging faster convergence. Enforcing this alignment is done by using an auxiliary loss. Choosing the \(L_{2}\) norm as a distance metric, we use a dynamic programming algorithm (Appendix F)

Figure 1: Proposed alignment rule: every state in the DEAR trajectory should “go forward”. Alignments to a GNN state that has already been “passed” are disallowed. First and last states must always align. We intentionally use arrows instead of \(\) for DEAR, as \(\) may not hold for DEAR’s trajectory.

to compute the most optimal alignment (normalised by the number of solver steps, in order not to penalise longer trajectories) and supervise on that value.

Even with normalisation, the alignment sometimes had the effect of making the optimisation stuck in local minima where the number of steps to hit equilibrium was as low as 2 and the gradients were uninformative. We combated this in two ways: 1) instead of using the default layer normalisation we switched to granola; 2) since \(f(fix(f))=f\), we performed a random number of additional iterations  and take the last state. The probability of doing \(s\) extra steps is \(0.5^{*}\).

## 5 Evaluation

SetupFor each algorithm we generated \(10^{5}\)/100/100-sized training/validation/test datasets. Training sample sizes vary between 8 and 16 elements (uniformly randomly chosen) validation samples are of size 16. As is standard in NAR literature, we measure the test performance out-of-distribution, so our test samples are of size 64. For algorithms on graphs we generate Erdos-Renyi graphs  with edge probabilities \(p\) uniformly sampled from the interval \([0.1,0.9]\), with increments of \(0.1\), which is the data distribution our baselines [8; 29] have used. We obtained the ground truth execution trajectories and targets using the CLRS-30 implementation .

In our experiments the models have a latent dimensionality of 128, the batch size is 32, the learning rate is \(3 10^{-4}\) and we use the Adam optimizer . We train our algorithmic reasoners for 100 epochs, choosing the model with the lowest _task_ validation loss (discounting any regularisation; focusing on performance only) for testing. Each task is independently learned, minimising the output loss (losses depend on the algorithm, cf. CLRS-30) plus any regularisation losses. Unless otherwise specified, DEARs employ the Anderson root-finding method from the torchdeq library and include Jacobian regularization , the tolerance for fixed point criteria on the forward pass is \(=0.1\) (and \(\) on the backwards) and is based on the relative \(L^{2}\) norm between GNN states. Standard deviations are based on 3 seeds. If run on a single 4090 GPU one would need about 3 weeks of _total_ compute.

The performance metric we measure is the out-of-distribution accuracy, hence the larger test instances. The definition of accuracy varies between algorithms and is based on the specification of the algorithm itself. We refer the reader to Velickovic et al.  and CLRS-30 for corresponding accuracy metrics definitions. The main baselines we compare against are the results _reported_ by Xhonneux et al. , as no implementation is publicly available, and a NAR architecture with a PGN processor trained in the no-hint regime, as done by Bevilacqua et al. . As, logically, models that are provided the ground-truth number of steps at test time will perform better, we also add as additional baselines a model that always uses 64 steps at test time and a model that has a dedicated network to decide termination . In order to understand how we compare to other, architectural alignments, we also provide a comparison with a more expressive processor (Triplet-MPNN).

### Results

We present results for 10 key algorithms (most of the ones used in Bevilacqua et al. ) in Table 1.

DEARs are reasonersThe first set of experiments aims to establish whether learning to execute algorithms by finding the least fixed point is possible. As Xhonneux et al.  report that their models

Figure 2: Despite converging to slightly higher train loss our models remain stable during optimisation

were prone to optimisation issues, we first compared the training loss for a standard NAR model and a DEAR model with the same neural components. The plots are visualised in Figure 2. In line with the previous work, we observed that the DEAR tends to converge to a slightly higher training loss as no algorithm's mean training loss dropped below 0.01. However, as evident in Figure 2, we found the optimisation to be overall stable, and the final train loss difference between NAR and DEAR was never greater than 0.1 - see Appendix G. We are unaware if Xhonneux et al.  observed the same numerical differences, but we were overall satisfied with the convergence of DEARs.

Equilibrium is a useful inductive biasDEAR outperforms both of the above baselines achieving a 4-5% overall score increase, suggesting that aligning to the equilibrium property is a useful inductive bias. Moreover, DEAR with a PGN processor is comparable to a NAR with the more expressive Triplet-MPNN, achieving only 2% lower overall accuracy. This commendable achievement required no information about the ground-truth number of steps neither at train time nor during inference. A more detailed performance analysis follows.

On weighted graph algorithms our model performed on par with the baseline NAR no-hint model for Bellman-Ford, it outperformed the baseline on Floyd-Warshall, and scored slightly behind on the other two. On unweighted ones, it retained fairly high BFS accuracy compared to DEM and it provided better scores for DFS and Strongly Connected Components (SCC). Unfortunately, even though for this kind of algorithms we used separate edge features for the CGP, in order to distinguish CGP edges from input ones, CGP had a detrimental effect. We hypothesise that algorithms like DFS and SCC need a more advanced architecture or require different task specifications (the algorithm for SCC has a parallel "twin"; see ) in order to generalise OOD. On algorithms on arrays, we got a significant performance improvement in the sorting task and got almost equal scores for min finding. However, the model underperformed by a large margin on the binary search task (in red). This result was very concerning, so we investigated further - Appendix H showed that DEARs overfitted a lot on the classic representation of binary search and that when the task is designed carefully, DEARs can reach overall performance of a Triplet-MPNN NAR architecture.

Effects of using CGPDespite the slightly lower accuracies, our experiments with CGP have not been futile. In Figure 3, we observe that for almost half of the algorithms CGP applies to, it had a positive effect on the loss convergence - 3/7 algorithms converged to at least half an order of magnitude lower train loss. The rest did not experience any strong negative effects of CGP. Per-algorithm plots can be found in Appendix I. We believe that the reduced accuracies are due to the

  
**Algorithm** & **NAR\({}^{}\)** & **NAR\({}^{}\)** & **NAR\({}^{}\)** & **DEM** & **DEAR** & **DEAR** \\  & (Triplet-MPNN) & (LT) & & (ours) & (with CGP; ours) \\  
**Weighted graphs** & & & & & & \\ 
**Bellman-F.** & \(97.06\% 0.40\) & \(97.23\% 0.15\) & \(95.39\% 1.42\) & \(96.4\%\%\)\(78.8\%\) & \(96.78\% 0.43\) & \(94.23\% 0.59\) \\
**Floyd-W.** & \(52.53\% 0.98\) & \(61.86\% 1.57\) & \(49.30\% 0.53\) & - & \(55.75\% 2.20\) & \(53.20\% 2.45\) \\
**DSP** & \(94.21\% 1.77\) & \(93.32\% 1.60\) & \(88.30\% 1.04\) & - & \(89.81\% 0.14\) & \(89.49\% 0.17\) \\
**MST Prim** & \(93.56\% 0.77\) & \(92.01\% 1.50\) & \(87.69\% 1.17\) & \(82.3\%\)\(75.2\%\) & \(88.67\% 0.74\) & \(86.37\% 0.36\) \\ 
**Unweighted graphs** & & & & & & \\ 
**BFS** & \(99.85\% 0.09\) & \(99.69\% 0.29\) & \(99.51\% 0.06\) & \(53.8\%\) & \(98.73\% 0.37\) & \(98.28\% 0.55\) \\
**DFS** & \(16.89\% 5.73\) & \(31.20\% 4.02\) & \(29.07\% 2.32\) & 5.0\% & \(40.62\% 0.44\) & \(23.87\% 2.49\) \\
**SCC** & \(40.70\% 1.39\) & \(46.84\% 1.70\) & \(39.33\% 1.52\) & - & \(43.63\% 1.19\) & \(38.71\% 0.45\) \\ 
**Arrays** & & & & & \\ (assumes fully-connected graph) & & & & & \\ 
**Search (Binary)** & \(94.67\% 2.31\) & \(93.33\% 2.31\) & \(84.33\% 8.33\) & - & \(59.00\% 12.3\) & - \\
**Minimum** & \(97.67\% 5.73\) & \(96.67\% 2.31\) & \(94.00\% 2.00\) & - & \(97.22\% 3.82\) & - \\
**Sort (\(\))** & \(27.07\% 10.3\) & \(63.67\% 39.97\) & \(33.8\% 12.04\) & - & \(86.93\% 3.87\) & - \\ 
**Overall** & \(71.42\%\) & \(\) & \(70.07\%\) & - & \(75.42\%\) & - \\   

Table 1: Test accuracy for different algorithms and models. Models with a diamond (\(\) or \(\)) iterate for the correct amount of steps during train time (may differ between datapoints). Filled diamond (\(\)) means the ground truth number of steps is also given at test time. LT stands for **learnt** **termination – the model that uses a termination network. For DEM  we leave a \(-\) when no results are reported and we report two results for shortest path and MST as it is unclear to us from the main text how they differentiated between the two. We do not run DEAR with CGP for array tasks as they operate on fully-connected graphs.

nearest Cayley graph for the training sizes being unique and a size of 24 nodes. Our deterministic approach of generating a fixed Cayley graph for CGP, whose size is still distinct from test-time size leads to overfitting; what we may observe here. Future avenues of work may want to investigate this by methodically removing the Cayley graph's edges, but still retaining the desirable expansion properties , or by exploring alternative novel graph wiring techniques . However, the limitation of these proposed approaches in comparison to CGP is that they may require _dedicated preprocessing_ to scale (one of the desirable criteria set by the EGP method), therefore providing an interesting line of future work.

Alignment can distill knowledge into DEARsFor evaluating our alignment we focused on the non-CGP version of DEAR and decided to pick algorithms where: 1) The baseline performs reasonably well (90+% accuracy), so as to provide good support; 2) the DEAR underperforms substantially. The algorithms to fulfil those requirements are: DAG Shortest paths, MST Prim and Binary Search.

Results are presented in Table 2. At first glance, the only algorithm that substantially improved was binary search, giving an almost 20% increase. The final test accuracy, however, does not represent all reality: Figure 4 shows that the task train loss (loss excluding any regularisers) for the model with alignment decreases, compared to no alignment and reaches similar levels as the one observed for the non-DEQ solution. So, is it overfitting again? We argue it is not. Figure 5 shows that the _test (OOD)_ accuracy per epoch increases when using alignment, reaching similar accuracies to NAR for the DAG shortest path problem and improving over plain DEAR for MST-Prim, suggesting that choosing the right model using validation seed is hard in NAR . Lastly, we would like to note that: 1) although granola+stochasticity does bring benefits on its own, alignment is necessary to narrow the gap to the NAR training loss (Appendix J); 2) We never reached perfect (0) alignment loss, suggesting better alignment techniques may further boost performance.

DEARs are highly parallelDEAR is not bound to follow sequential trajectories and GNNs are more aligned to parallel algorithms than to sequential ones . As the cost for one step of DEAR (GNN iteration + solver) is at least as high as one step of an NAR model (G

   & **DSP** & **MST-Prim** & **Binary Search** \\ 
**NAR** & \(94.21\% 1.77\) & \(93.56\% 0.77\) & \(94.67\% 2.31\) \\
**DEAR** & \(89.81\% 0.14\) & \(88.67\% 0.74\) & \(59.00\% 12.3\) \\
**DEAR** & \(89.65\% 2.95\) & \(90.37\% 1.19\) & \(77.33\% 4.51\) \\ (dispersion) & & & \\  

Table 2: Test accuracy with and without alignment.

Figure 4: Alignment (with granola and stochasticity; **DEAR w/ GAS**) gives better convergence

Figure 3: Cayely graph propagation can help with convergence

used the inference speed of a DEAR as a measure of how parallel the final learnt algorithm is. Results are presented in Table 3. An immediate observation is that DEAR improves inference times across almost all algorithms. The only ones that were executed slower are: 1) Bellman-Ford and BFS, which are highly parallelised in the CLRS-30 implementation, so an improvement on them was unlikely; 2) Floyd-Warshall where the difference, although present, is marginal and we account it to the added overhead from the solver; 3) Binary search, where performance was almost identical. These results suggest that although not always guaranteed (the case for searching), it is very likely that a DEAR will learn a parallel algorithm. The most substantial improvements, in line with our past observations in Engelmayer et al. , were on the tasks of sorting and strongly-connected components.

DEARs are foundationalUp until this point, DEAR was run with a PGN processor, which is a lightweight, yet well-performant NAR processor architecture. The last set of experiments aims to show that equilibrium reasoning is not tied to only one type of processor/architecture. It is rather a **class of models/foundational model** as it can natively support different types of processors. To verify this claim, we present results with DEAR using the Triplet-MPNN architecture in Table 4. As Triplet-MPNN is computationally expensive, we tested algorithms for which NAR with Triplet-MPNN improves over NAR with PGN. Results indeed confirm that we are not limited to a single type of processor with DEAR, and, as expected, the best overall performance is achieved when using DEAR with the more expressive, Triplet-MPNN, processor.

## 6 Conclusion

Our investigations with equilibrium models have shown that it is possible and even beneficial to merge NAR and DEQs. While our models attained very competitive performance, there are certain limitations that need to be addressed: 1) Better algorithms for alignment can help close the gap even further for Prim's algorithm and binary search; 2) Better model selection is needed in order to know which DEARs would perform well OOD; 3) Graph rewiring techniques may be needed to prevent overfitting with CGP; 4) Algorithmic-aligned criteria for fixed-point may boost OOD generalisation for sequential algorithms. The last point is motivated by the fact that for each step, these algorithms update only a few nodes in the graph, keeping the rest untouched.

    & **Floyd-W.** & **DFS** & **SCC** &  **Search** \\ (Parallel) \\  & **Sort** & **Overall** \\   **NAR\({}^{}\)** \\ **DEAR** \\ (ours) \\  & \(61.86\% 1.57\) & \(31.20\% 4.02\) & \(\) & \(\) & \(63.67\% 39.97\) & \(59.18\%\) \\   **NAR\({}^{}\)** \\ **DEAR** \\ (ours) \\  } & \(62.29\% 2.71\) & \(\) & \(45.12\% 1.52\) & \(87.00\% 5.57\) & \(\) & \(\) \\   

Table 4: DEAR is architecture invariant and can also run with a Triplet-MPNN processor.

Figure 5: Alignment (**DEAR w/ GAS**) leads to improvements OOD