# Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network

**Zih-Syuan Huang**

Department of Computer Science and Information Engineering

National Taiwan University

Taipei 106, Taiwan

r11922210@ntu.edu.tw

**Ching-pei Lee**

Department of Advanced Data Science

Institute of Statistical Mathematics

Tachikawa, Tokyo 190-8562, Japan

chingpei@ism.ac.jp

**Abstract**

We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks. Similar to existing regularized adaptive methods, the subproblem for computing the update direction of RAMDA involves a nonsmooth regularizer and a diagonal preconditioner, and therefore does not possess a closed-form solution in general. We thus also carefully devise an implementable inexactness condition that retains convergence guarantees similar to the exact versions, and propose a companion efficient solver for the subproblems of both RAMDA and existing methods to make them practically feasible. We leverage the theory of manifold identification in variational analysis to show that, even in the presence of such inexactness, the iterates of RAMDA attain the ideal structure induced by the regularizer at the stationary point of asymptotic convergence. This structure is locally optimal near the point of convergence, so RAMDA is guaranteed to obtain the best structure possible among all methods converging to the same point, making it the first regularized adaptive method outputting models that possess outstanding predictive performance while being (locally) optimally structured. Extensive numerical experiments in large-scale modern computer vision, language modeling, and speech tasks show that the proposed RAMDA is efficient and consistently outperforms state of the art for training structured neural network. Implementation of our algorithm is available at https://www.github.com/ismoptgroup/RAMDA/.

## 1 Introduction

Since the recent emergence of ChatGPT, large language models (LLMs) and other huge deep learning models have garnered much attention and popularity, even among the public who are unfamiliar with machine learning. A challenge with such gigantic neural network models is their vast number of model parameters, reaching hundreds of billions, resulting in expensive storage and inference. It thus becomes crucial to find ways to exploit structures intrained models to reduce their spatial and prediction costs without degrading the prediction performance. An active line of research is to explicitly add a nonsmooth regularization term to the training objective function and apply proximal stochastic (sub)gradient methods, with or without a diagonal preconditioner for adaptiveness, to induce a pre-specified type of desirable structure in the final model . Unfortunately, although the added regularizer indeed induces some desirable structures at the stationary points of the training objective function, the iterates of these methods only converge to those stationary points asymptotically, but never really attain such a point at any iteration. Therefore, whether the output model of these algorithms, which is also an iterate that is only close enough to a stationary point, indeed possesses the ideal structure at the nearby stationary point is unknown, and theoretical analyses of these algorithms do not cover any guarantees regarding the obtained structure. Indeed,  oberserd empirically that the structures obtained by those methods are highly suboptimal and unstable over iterations. They then proposed a regularized dual averaging method called RMDA, and proved that after a finite number of steps, the iterates of RMDA can stably identify the locally optimal structure induced by the regularizer at the stationary point of asymptotic convergence.1 This is up to our knowledge the only method with such structure guarantees for training structured neural networks. With this property, their experiments demonstrated that their method also empirically outperforms existing methods on modern computer vision tasks. However, since RMDA does not incorporate adaptiveness and their experiments are conducted only on medium-scale image classification problems, its usefulness beyond computer vision is in doubt.

For a wide range of tasks in deep learning such as language modeling and speech recognition, researchers have developed numerous architectures to achieve state-of-the-art prediction performance, including the transformer  and the LSTM . The transformer is also gaining prominence in computer vision for achieving exceptional performance . Therefore, it is becoming increasingly important to devise methods that attain satisfactory performance for training these network architectures with structure. For such modern architectures, adaptive methods like Adam that iteratively rescale the stochastic gradient update directions via a coordinate-wise/diagonal preconditioner are known to outperform their non-adaptive counterparts and thus considered state-of-the-art . It is hence expected that the non-adaptive RMDA of  might not lead to promising results for such widely-used architectures and tasks.

This work aims to fill this gap to propose a practical regularized adaptive method with guarantees for both convergence and structure identification. Since RMDA already has structure guarantees, it might look like we just need to combine it with an arbitrary preconditioner for adaptiveness. However, this seemingly easy extension actually requires deliberation in two aspects. First, except for few exceptions, combination of even a simple diagonal preconditioner and a nonsmooth regularizer makes the training subproblem complicated with no closed-form solution. This is totally different from adaptive methods with no regularization, whose subproblem optimal solution can be easily computed by coordinate-wise divisions. Therefore, in the regularized case, the best we can hope for is to apply an iterative approach to approximately solve the subproblem. This calls for careful design and control for the measure and the degree of the inexactness in the approximate subproblem solution. The second aspect is the need of an appropriate preconditioner that provides not only outstanding empirical performance but also desirable theoretical properties. The interplay between the inexactness and the preconditioner makes it particularly difficult to address the following three challenges simultaneously. (i) _Convergence_: Proving convergence of a new algorithm with even just one added component is always a nontrivial task. For example, although convergence of SGD has been well-studied for decades, similar guarantees for its adaptive correspondence, Adagrad, is not established until very recently . We are dealing with both a preconditioner that changes the whole algorithm, just like from SGD to Adagrad, and the inevitable inexact subproblem solutions that could nullify many useful properties (regarding the subdifferential) commonly used in convergence proofs. (ii) _Structure_: Theoretical guarantees for structure identification is another critical aim of this work. Inexactness alone already makes this goal difficult; see Example 1 of  for a simple instance such that even infinitesimal inexactness could hinder structure identification. Even without inexactness, finding a preconditioner that leads to structure identification guarantees is already difficult because no adaptive algorithm, even in the much simpler deterministic and exact setting, is known to have such a guarantee. (iii) _Subproblem solver_: Our goal is a practical algorithm, so we need to solve the subproblem efficiently. This requires the inexact measure be checkable and the degree quickly attainable by a well-designed solver, and the preconditioner should make the subproblem well-conditioned and cannot complicate the computation of the solver.

To tackle these difficulties, we start from considering structure identification. We leverage the theory of manifold identification in variational analysis and nonlinear optimization to design a method that leads to finite-iteration structure identification guarantees. As discussed by [37; 16], the key to such guarantees for stochastic algorithms is to ensure the variance in the stochastic estimations decreases to zero. Due to the standard practice of data augmentation in deep learning, the training loss in the objective function is essentially the expected value of the training loss over a certain probability distribution instead of a finite sum. We thus draw inspirations from [25; 16] to consider a dual-averaging-type approach [32; 47] with momentum to attain variance reduction in this setting for the stochastic gradient estimation. However, we also need variance reduction for the preconditioner, so we carefully select a preconditioner whose update is in a manner similar to dual averaging, and prove that its variance also decreases to zero. We then conceive an implementable and practical subgradient-norm-based inexactness measure compatible with the structure identification theory. Further requirements are then added to the inexactness degree and the preconditioner to ensure convergence, and we also safeguard the preconditioner to keep the subproblems well-conditioned and the computation simple. We then propose to solve the subproblem by a proximal gradient (PG) solver that provably achieves our inexactness requirement efficiently. This leads to our Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm.

We summarize our main contributions as follows.

1. _An adaptive algorithm for finding locally optimal structures_: RAMDA is the first regularized _adaptive_ method guaranteed to find the locally optimal structure possessed by the stationary point to which its iterates converge. It thus produces models that are more structured while retaining the superb prediction performance of adaptive methods.
2. _Efficient subproblem solver for regularized adaptive methods_: We propose an implementable inexactness condition and a companion efficient subproblem solver for regularized adaptive methods (including ours and existing ones) whose subproblems have no closed-form solution. We show that the induced inexactness does not affect convergence or structure identification guarantees. This condition and subproblem solver thus also serve as a key step for realizing existing frameworks for regularized adaptive methods.
3. _A method with outstanding empirical performance_: Experiments on training modern neural networks in computer vision (ImageNet), language modeling (Transformer-XL), and speech (Tacotron2) with structured sparsity show that RAMDA steadily outperforms state of the art by achieving higher structured sparsity ratio and better prediction performance simultaneously.

## 2 Related Work

Dual Averaging for Deep Learning.Our method is motivated by  that adapted the famous regularized dual averaging [47; 25] approach with momentum to train structured neural network models with data augmentation. They selected dual averaging for the gradient estimation to achieve variance reduction for structure guarantees, but their algorithm does not allow for adaptiveness. Inspired by this approach, we also take dual-averaging-like updates for the diagonal preconditioner in the subproblem for adaptiveness. Our preconditioner design also borrows ideas from the empirically successful MADGRAD of  for training non-regularized neural networks. RAMDA can thus also be seen as a generalization of MADGRAD to the regularized setting. Since no regularizer is present, unlike RAMDA, the subproblem of MADGRAD has a closed-form solution and no structure is expected. Moreover,  only analyzed convergence rates of the objective value when the problem is convex. Our analysis of (i) variance reduction in the preconditioner, (ii) convergence in the nonconvex nonsmooth regularized case, and (iii) structure identification guarantees are novel and closer to properties desirable in practice. The first two items are also applicable when no regularizer is present, so our theory also expands guarantees for MADGRAD.

Regularized Stochastic Algorithms for Deep Learning.Other than RMDA, there are several works on training structured neural networks through regularization and its proximal operator, but none have structure guarantees.  considered a simple regularized SGD method with momentum, but their convergence analysis is only for the nonadaptive case.  studied a general regularized adaptive framework \(\) that incorporates diagonal preconditioners, and showed that the subgradient of the objective function can decrease to the reciprocal of the batch size, but their result does not guarantee further convergence to stationary points. Moreover, they do not allow inexactness in the subproblem, so their framework can be realized for only a small class of problems.  proposed \(\) that extends \(\) to the case of group-sparsity regularizers, whose corresponding subproblem indeed has no closed-form solution. They applied the Newton-Raphson method to obtain nearly-optimal subproblem solutions, and proposed a seemingly mild inexactness condition. Unfortunately, their condition is not checkable, and their corresponding convergence guarantee requires the regularizer to be locally smooth around each iterate, which excludes most regularizers that induce meaningful structures. On the other hand, we will show that with our implementable inexactness condition, \(\) still possesses the same convergence guarantees in  without any additional requirement on the regularizer. Moreover, we will see in Section 6 that the time cost of the subproblem solver of \(\) is prohibitively high.

Structure and Manifold Identification.The major tool for our structure guarantees is the theory of manifold identification  in variational analysis and nonlinear optimization. This theory shows that points possessing the same structure induced by the regularizer at a stationary point form a smooth manifold around this stationary point, and with properties from the regularizer, if a sequence of points converges to this stationary point with their corresponding subgradients decreasing to zero, this sequence is guaranteed to eventually stay in this manifold, thus identifying the structure.  have leveraged this tool to show manifold identification for various stochastic algorithms, and the common key, as pointed out by , is variance reduction. Our analysis uses a result given in  to prove so for both the gradient estimator and the preconditioner.

## 3 Problem Setting and Algorithm

As described in Section 1, we consider the case in which the training objective function is the expectation over a probability distribution as follows.

\[_{W} F(W):=_{} [f_{}(W)]+(W),\] (1)

where \(\) is a Euclidean space with inner product \(,\,\) and its induced norm \(\), \(\) is a distribution over a space \(\) representing all possible data modifications, \(f_{}\) is differentiable almost everywhere for any \(\), and the possibly nonsmooth regularizer \((W)\) is for promoting a desirable structure in the optimal solutions.

Our algorithm can be seen as a double-dual averaging method that incorporates momentum, a proximal operation for the regularization, and dual averaging for updating both the stochastic gradient estimation and the preconditioner. For ease of description, we assume without loss of generality that \(=^{n}\) in this section. At the \(t\)th iteration with learning rate \(_{t}\) and iterate \(W^{t-1}\), we first draw an independent and identically distributed sample \(_{t}\), compute the stochastic (sub)gradient \(G^{t} f_{_{t}}(W^{t-1})\) of the loss function at the current point \(W^{t-1}\) with respect to \(_{t}\), and then update the weighted sum \(V_{t}\) of historical stochastic gradients and the weighted sum \(U_{t}\) of their squared norms using the value \(s_{t}\):

\[V_{0} 0,&V_{t} V_{t-1}+s_{t}G^{t},  t>0,\\ U_{0} 0,&U_{t} U_{t-1}+s_{t}G^{t} G^{t}, t>0,  s_{t}_{t},\] (2)

where \(\) denotes the Hadamard (pointwise) product in \(\). We then construct the preconditioner \(P^{t}\) and the weight sum \(_{t}\) by

\[P^{t}(}+),_{t} _{k=1}^{t}s_{k},\] (3)where \(>0\) is a (usually small) constant for numerical stability and \(()\) is the diagonal matrix whose diagonal entries are the elements of the input vector. The update direction is then obtained by (approximately) solving the following subproblem.

\[^{t}\ Q_{t}(W)_{t} (W)+ V^{t},\,W+ W-W^{0},\,P^{t}(W-W^{0}) ,\] (4)

where \(W^{0}\) is the initial point. Details regarding (4) and how to solve it are deferred to Section 4. The iterate is then updated by averaging \(^{t}\) and \(W^{t-1}\) with some \(c_{t}\):

\[W^{t}=(1-c_{t})W^{t-1}+c_{t}^{t}.\] (5)

The choice of \(P^{t}\) in (3) that uses the accumulated square of the stochastic gradient norm as the preconditioner is the key to adaptivity and is widely seen in adaptive methods such as Adagrad , while the choice of the cubic root instead of the square root is motivated by the impressive numerical performance of MADGRAD of  for smooth problems without a regularization term. The averaging step in (5) with \(c_{t} 1\) can be interpreted as incorporating a momentum term in the non-regularized non-adaptive case .

## 4 Subproblem Solver

Given an iterate \(W^{t-1}\), a momentum term \(m_{t}\), a preconditioner \(P^{t}\), and a stepsize \(_{t}\), existing regularized adaptive stochastic gradient algorithms for (1) can be summarized in the following form :

\[W^{t}=\ _{t}(W) m_{t},\,W +} W-W^{t-1},\,P^{t}(W-W^{t-1})+(W ),\] (6)

whose form is similar to (4). When the preconditioner \(P^{t}\) is a multiple of the identity matrix like in the case of , the exact subproblem solution of (4) can be efficiently computed through the proximal operator associated with the regularizer. However, a major difficulty for realizing regularized adaptive methods, including the proposed RAMDA and the framework of  whose preconditioners are not a multiple of the identity, is that except for few special regularizers, the subproblem usually has no closed-form solution. We therefore consider using approximate solutions of the subproblem.

We propose to apply a few iterations of proximal gradient (PG) [see, _e.g._, 5, 33] to approximately solve the subproblems in (4) and (6) when no closed-form solution is available, and we will show theoretically and empirically in the following sections that the inexactness of such approximate solutions has barely any effects on the theoretical guarantees and the final model quality. For the inexactness of the approximate solution in (4), we require

\[(^{t})}{}\|s\|_{t}, Q _{t}(^{t}) Q_{t}(W^{t-1}),\] (7)

for some pre-specified \(_{t}\), where \( Q_{t}(W^{t+1})\) is the (limiting) subdifferential [see, _e.g._, 38, Definition 8.3]. This condition can be easily checked using information available in the PG iterations. For the sake of time efficiency, we also impose an upper limit for the number of PG iterations. Likewise, when applying our subproblem solver to (6), we enforce (7) but with \(Q_{t}\) replaced by \(_{t}\) and \(^{t}\) by \(W^{t}\). We focus on the case of diagonal and positive \(P^{t}\)and thus the largest eigenvalue \(((P^{t}))\), where \(()\) is the vector formed by the diagonal entries of the input matrix, can be calculated easily and used to compute a step size guaranteeing sufficient objective decrease. For cases in which this value is difficult to obtain, one can apply a simple backtracking linesearch for the subproblem to find a suitable step size efficiently. This PG subproblem solver is summarized in Algorithm 2. To guarantee convergence for both our algorithm and the framework of , our analysis in Section 5 requires that \(\{_{t}\}\) satisfy

\[_{t=0}^{}_{t}^{2}<.\] (8)

We will show in Section 5 that (7) holds after at most \(O(_{t}^{-2})\) iterations of Algorithm 2.

## 5 Analysis

This section discusses theoretical guarantees for RAMDA and the proposed subproblem solver in Algorithm 2. We also prove convergence guarantees for applying PG to approximately solve (6) for the framework of . All proofs are in the appendices. Some of our results are inspired by , but with the added inexactness in (4) and the adaptiveness for the preconditioner, the analysis is nontrivial. Recall that we assume that \(f_{}\) is differentiable only almost everywhere but not everywhere, which conforms with widely-used network structures like ReLU-type activations.

We first show that (7) can be attained by Algorithm 2 and that the point of convergence of RAMDA is almost surely a stationary point.

**Theorem 1**.: _Assume that (4) and (6) has at least one optimal solution with a finite optimal objective value. Given \(_{t}>0\), the number of iterations Algorithm 2 takes to satisfy (7) for both (4) and (6) is \(O((_{t}^{-1}))\) when \(\) is convex and \(O(_{t}^{-2})\) when \(\) is nonconvex._

**Theorem 2**.: _Consider \(\{^{t}\}\) generated by Algorithm 1 for (1), with (7) and \(\{c_{t}\}\) and \(\{_{t}\}\) satisfying \( c_{t}=\) and (8). Assume there is \(L 0\) such that for any \(\), \(f_{}\) is almost surely \(L\)-Lipschitz-continuously-differentiable, so the expectation is also \(L\)-Lipschitz-continuously-differentiable, there is \(C 0\) such that \(_{_{t}} f_{_{t}}(W^{t-1} )^{4} C\) for all \(t\), and that the set of stationary points \(\{W 0 F(W)\}\) is nonempty. For any given \(W^{0}\), consider the event that \(\{^{t}\}\) converges to a point \(\) (each event corresponds to a different \(\)). If \(\) is outer semicontinuous at \(\), this event has a nonzero probability, and \(\{_{t}\}\) satisfies_

\[ s_{i}_{t}^{-1}=,(s_{t}_{t}^{-1})^ {2}<,W^{t+1}-W^{t}(s_{t}_{t}^{-1})^{- 1}}0,\]

_then we have that \(\) with probability one conditional on this event. Moreover, \(\{W^{t}\}\) also converges to this stationary point \(\)._

Usually, convergence to a point requires some further regularity conditions like the Kurdyka-Lojasiewicz condition and boundedness of the iterates. However, existing frameworks regarding iterates convergence using such conditions also require the method analyzed to have a subgradient-descent-like behavior and to be a descent algorithm. Neither of these hold true even for the basic stochastic gradient algorithm, and we leave the analysis for this part as a challenging future work.

Our next key result shows that after a finite number of iterations, iterates of RAMDA all possess the same structure as that of the point of convergence \(\). For this end, we first need to introduce the notions of partial smoothness and prox-regularity, and impose these assumptions on \(\) at \(\).

**Definition 1** (Partial Smoothness ).: _A function \(\) is partly smooth at a point \(\) relative to a set \(_{}\) if 1. Around \(\), \(_{}\) is a \(^{2}\)-manifold and \(|_{_{}}\) is \(^{2}\). 2. \(\) is regular (finite and the Frechet subdifferential coincides with the limiting Frechet subdifferential) at all points \(W_{}\) near \(\), with \((W)\). 3. The affine span of \(()\) is a translate of the normal space to \(_{}\) at \(\). 4. \(\) is continuous at \(\) relative to \(_{}\)._

We often call \(_{}\) the active manifold at \(\). Locally, this manifold represents all points near \(\) that share the same structure induced by the regularized as \(\). Therefore, finding the active manifold is equivalent to finding the locally optimal structure.

**Definition 2** (Prox-regularity ).: _A function \(\) is prox-regular at \(\) for \(V^{*}()\) if \(\) is locally lower semi-continuous around \(\), finite at \(\), and there is \(>0\) such that \((W_{1})(W_{2})+ V,\,W_{1}-W_{2}- {W_{1}-W_{2}}^{2}\) for every \(W_{1},W_{2}\) near \(\) with \((W_{2})\) close to \(()\) and \(V(W_{2})\) close to \(V^{*}\). \(\) is prox-regular at \(\) if it is prox-regular for all \(V()\)._

**Theorem 3**.: _Consider Algorithm 1 with the conditions in Theorem 2 satisfied. Consider the event of \(\{^{t}\}\) converging to a certain point \(\) as in Theorem 2. If the probability of this event is nonzero; \(\) is prox-regular and subdifferentially continuous at \(\) and partly smooth at \(\) relative to the active \(^{2}\) manifold \(_{}\); \(\) is outer semicontinuous at \(\); and the nondegeneracy condition \(- f()\ ()\) holds at \(\), then conditional on this event, almost surely there is \(T_{0} 0\) such that_

\[^{t}_{}, t T_{0}.\]

We note particularly that convex and weakly-convex  functions are all regular, prox-regular, and subdifferentially continuous everywhere.

We also show that our subproblem solver and condition can be effectively applied to the framework of  while retaining the same convergence guarantees. As mentioned in Section 2, our result is much stronger than that of  for having no unrealistic smoothness requirement on \(\) and using an implementable inexactness condition.

**Theorem 4**.: _For the framework in  with the subproblem solved approximately by Algorithm 2 such that (7) holds with \(\{_{t}\}\) satisfying (8). Then Theorem 1 of  still holds, but with the constants \(\{Q_{i}\}\) being also dependent on \(\)._

## 6 Experiments

This section examines the practical performance of RAMDA for training structured neural networks. As sparsity is arguably one of the most widely adopted structures in machine learning, we follow  to consider structured sparsity as the representative structure in our experiments. Particularly, we employ the group LASSO regularization  to encourage group sparsity and disable weight decay in all experiments, except for the dense baselines. We begin from examining the efficiency and effectiveness of PG for both RAMDA and existing regularized adaptive methods. We then consider tasks in computer vision, language modeling, and speech to compare the following algorithms using Pytorch.

* RAMDA: The proposed Algorithm 1.
* RMDA 
* ProxSGD 
* ProxGen : We follow their experiments to use AdamW and apply our PG as the subproblem solver.
* ProxSSI 

These algorithms are introduced in Section 2 and also further summarized in Appendix A. For each task, we also provide for reference a baseline that does not include a group LASSO regularizer in the training (SGD with momentum (MSGD) for computer vision, and Adamfor the other two), but our comparison is only among those for training structured models. Our code for reproducing the experiments and the hyperparameter settings are available at https://github.com/ismoptgroup/ramda_exp/. Additional details of the stability of the structure (level of structured sparsity here) over epochs of RAMDA are available in Appendix D.

We use two criteria for comparison: 1. Model predictive ability, and 2. Structured sparsity level. The former is task-dependent and thus specified in each experiment. Regarding the latter, sparsifying neural networks while preserving its performance requires prior knowledge of model design. A common approach is retaining certain parameters during the training process, and we adhere to this convention such that the bias, batch normalization , layer normalization , and embedding layers do not have any sparsity-inducing regularization imposed on them . For the rest, we adopt channel-wise grouping for convolutional layers, input-wise grouping for fully-connected and LSTM layers during the training phase. For evaluation, our structured sparsity is calculated using the weighted group sparsity with the weights proportional to the number of parameters in each group.

We run each experiment with three different random initializations and show the mean and standard deviation of the validation predictive performance and the structured sparsity of the final model of all methods.

SubproblemWe start from showing the effectiveness of our proposed subproblem solver for RAMDA and ProxGen. For both approaches, we use Theorem 2 of  to safely screen out a portion of groups that will be zero at the optimal subproblem solution, and opt for the PG algorithm to solve the remaining parts. We consider two practical stopping criteria for PG: 1. Running until it reaches the maximum iterations (no early stopping), and 2. Terminate when the subproblem objective improvement is small (early stopping). For the former, we set the maximum to 100. For the latter, we terminate PG early if \((Q_{t}(Z^{j-1})-Q_{t}(Z^{j}))/(|Q_{t}(Z^{j}|+1)<10^{-8}\) is reached. Moreover, to ensure incorporation of the preconditioner into ProxGen, we set its minimum PG iterations to 2. We examine how these stopping criteria affect the final model of RAMDA and ProxGen using image classification problems of a smaller scale. From Table 1, we see that early stopping does not affect the outcome much. Given that early stopping is more efficient, we will adopt it in all subsequent experiments.

Next, we compare ProxGen with ProxSSI (these two only differ in the subproblem solver) to examine the efficiency and performance differences between solving the subproblems approximately and (almost) exactly in Table 2. We see that our solver is around 3X faster than ProxSSI, and the model qualities are similar. We thus exclude ProxSSI from our comparisons in the following experiments due to its excessively lengthy running time, especially for large-scale tasks.

Image ClassificationWe conduct a classical computer vision task of training ResNet50  with the ILSVRC 2012 ImageNet dataset . The result in Table 3 shows that RAMDA attains the best validation accuracy and structured sparsity simultaneously.

Language ModelingFor language modeling, we train Transformer-XL (base)  using the WikiText-103 dataset . Transformer-XL is comprised of embedding and non-embedding layers, and in the PyTorch implementation, the non-embedding layers are built using linear

  & &  &  \\  Model/Data & Algorithm & Accuracy & Sparsity & Accuracy & Sparsity \\  VGG19 / & ProxGen & 92.7 \(\) 0.2\% & 88.8 \(\) 0.0\% & 92.7 \(\) 0.1\% & 86.9 \(\) 0.4\% \\ CIFAR10 & RAMDA & 92.7 \(\) 0.2\% & 86.7 \(\) 0.3\% & 92.9 \(\) 0.2\% & 86.3 \(\) 0.4\% \\  ResNet50 / & ProxGen & 73.6 \(\) 0.1\% & 74.7 \(\) 0.6\% & 74.0 \(\) 0.1\% & 67.6 \(\) 3.1\% \\ CIFAR100 & RAMDA & 69.9 \(\) 1.5\% & 69.5 \(\) 2.1\% & 71.2 \(\) 1.4\% & 67.5 \(\) 1.6\% \\ 

Table 1: Weighted group sparsity and validation accuracy of different subproblem stopping criteria.

and layer-normalization layers. We apply group LASSO regularization to the linear layers, and present in Table 4 the perplexity and the weighted group sparsity of the models trained. We see that RAMDA gives the best perplexity and structured sparsity simultaneously.

Speech SynthesisWe consider Tacotron2  for speech synthesis on the LJSpeech dataset . We apply regularization to the convolutional, LSTM, and linear layers of Tacotron2 and show the results in Table 5. Clearly, RAMDA gives the lowest validation loss and the highest group sparsity.

Time EfficiencyIn Tables 4 and 5, we see that although RAMDA and ProxGen have more difficult subproblems without a closed-form solution to solve, our proposed PG solver is highly efficient such that the running time of them is still close to other approaches, making these regularized adaptive approaches practically feasible.

SummaryIn summary, thanks to its adaptive nature (for better predictive performance) and its ability of manifold identification (for higher structured sparsity), RAMDA is superior to state of the art on modern language modeling and speech synthesis tasks as well as the ImageNet problem. We also observe from the plots in the appendices that it is possible to further improve the sparsity level of RAMDA if we run it for more epochs.

## 7 Conclusions

In this work, we proposed a regularized dual averaging method with adaptiveness, RAMDA, for training structured neural networks. Our method outperforms state of the art on modern

 Alg. & Perplexity & Sparsity & Time/epoch \\  Adam & 23.00 \(\) 0.05 & - & 6261 \(\) 21s \\   RAMDA & **26.97 \(\) 0.10** & **36.2 \(\) 0.3\%** & 6954 \(\) 30s \\ ProxSGD & 27.42 \(\) 0.02 & 33.1 \(\) 1.5\% & 6167 \(\) 12s \\ ProxGen & 27.49 \(\) 0.19 & 30.5 \(\) 0.6\% & 6652 \(\) 21s \\ RMDA & 27.10 \(\) 0.08 & 36.0 \(\) 2.7\% & 6184 \(\) 20s \\  

Table 4: Weighted group sparsity and validation perplexity on Transformer-XL with WikiText-103.

 Algorithm & Accuracy & Sparsity & Time & Accuracy & Sparsity & Time \\   & VGG19/CIFAR10 &  \\  ProxSSl & 92.8 \(\) 0.1\% & 88.4 \(\) 0.2\% & 79s & 67.3 \(\) 0.1\% & 78.6 \(\) 0.3\% & 79s \\ ProxGen & 92.8 \(\) 0.0\% & 86.6 \(\) 0.1\% & 24s & 68.1 \(\) 0.4\% & 75.5 \(\) 0.2\% & 26s \\   & ResNet50/CIFAR10 &  \\  ProxSSl & 94.0 \(\) 0.1\% & 83.7 \(\) 0.6\% & 260s & 73.7 \(\) 0.4\% & 70.4 \(\) 0.7\% & 251s \\ ProxGen & 94.1 \(\) 0.1\% & 80.4 \(\) 0.4\% & 70s & 73.6 \(\) 0.4\% & 65.5 \(\) 3.6\% & 74s \\  

Table 2: Weighted group sparsity, validation accuracy and time/epoch of ProxSSl and ProxGen for CIFAR10/CIFAR100. We report the average time/epoch using one NVIDIA V100 GPU.

 Algorithm & Accuracy & Sparsity \\  MSGD & 77.14 \(\) 0.04\% & - \\  RAMDA & **74.53 \(\) 0.10\%** & **29.19 \(\) 0.94\%** \\ ProxSGD & 73.50 \(\) 0.20\% & 17.54 \(\) 1.26\% \\ ProxGen & 74.17 \(\) 0.08\% & 20.29 \(\) 0.22\% \\ RMDA & 74.47 \(\) 0.08\% & 25.20 \(\) 1.69\% \\ 

Table 3: Weighted group sparsity and validation accuracy on ImageNet/ResNet50.

architectures including LSTM and transformers as well as the ImageNet problem. We also proposed a subroutine with strong convergence guarantees to approximately solve the regularized subproblem of both our method and an existing framework efficiently. Extensive experiments on group sparsity showed that our subproblem solver can greatly reduce the training time for existing methods, and our proposed RAMDA achieves simultaneously higher structured sparsity ratio and better prediction performance than existing methods. Implementation of our method is available at https://www.github.com/zhisyuan1214/RAMDA/.