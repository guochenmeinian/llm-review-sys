# Distributional Preference Alignment of LLMs

via Optimal Transport

Igor Melnyk

Now with Capital One; Work done while at IBM Research

Youssef Mroueh

Now with Capital One; Work done while at IBM Research

Brian Belgodere

Now with Capital One; Work done while at IBM Research

Mattia Rigotti

Apoorva Nitsure

Mikhail Yurochkin

Kristjan Greenewald

Jiri Navratil

Jarret Ross

IBM Research

MIT-IBM Watson AI Lab

###### Abstract

Current LLM alignment techniques use pairwise human preferences at a sample level, and as such, they do not imply an alignment on the distributional level. We propose in this paper Alignment via Optimal Transport (AOT), a novel method for distributional preference alignment of LLMs. AOT aligns LLMs on unpaired preference data by making the reward distribution of the positive samples stochastically dominant in the first order on the distribution of negative samples. We introduce a convex relaxation of this first-order stochastic dominance and cast it as an optimal transport problem with a smooth and convex cost. Thanks to the one-dimensional nature of the resulting optimal transport problem and the convexity of the cost, it has a closed-form solution via sorting on empirical measures. We fine-tune LLMs with this AOT objective, which enables alignment by penalizing the violation of the stochastic dominance of the reward distribution of the positive samples on the reward distribution of the negative samples. We analyze the sample complexity of AOT by considering the dual of the OT problem and show that it converges at the parametric rate. Empirically, we show on a diverse set of alignment datasets and LLMs that AOT leads to state-of-the-art models in the 7B family of models when evaluated with Open LLM Benchmarks and AlpacaEval. Code for AOT is available in the Hugging Face TRL library https://ibm.biz/AOT_TRL.

## 1 Introduction

Aligning Large Language Models (LLMs) with human preferences is a crucial step in making these models safe and having them follow instructions faithfully. By ensuring that LLMs adhere to human preferences, values, ethics, and desired behaviors we can reduce the risk of generating harmful, biased, or inappropriate content.

Reinforcement Learning from Human Feedback, _RLHF_(Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022), achieves this by learning a reward model on human preference data, followed by fine-tuning the LLM to maximize the reward score while staying close to the initial reference policy to retain utility from the pre-trained model. Recently, new paradigms departed from RLHF towards direct preference optimization methods such as DPO (Rafailov et al., 2024), SLIC (Zhao et al., 2023), and Identity Policy optimization (Azar et al., 2024). In these approaches, the reward is expressed in terms of the log-likelihood ratio between the LLM policy and the reference model. The training is done on paired preference data, i.e. as triplets of prompts, chosen and rejected sentences, where for each prompt a chosen and a rejected sample are available. The training objective is to maximize the margin between the log-likelihood ratio evaluated on the chosen sentence versusthe log-likelihood ratio on rejected sentences. When paired preference data is not available, and the preference data instead takes the form of distinct marginals of chosen prompt/response pairs and rejected prompt/response pairs, we refer to this setup as the unpaired data setting. Ethayarajh et al. (2024) used Kahneman & Tversky's prospect theory in the unpaired setting and proposed the KTO method that maximizes the margin between the chosen reward and the average reward of rejected sentences and pushes the reward of a rejected sentence below the average reward of chosen sentences.

In this paper, we introduce a new _distributional_ optimization method for fine-tuning LLMs from human preference data. Previous work in the paired setting focused on improving the reward of chosen sentences over rejected sentences on a per-sample basis. This procedure does not lead to a preference on a distributional level of the chosen marginal on the rejected marginal. In probabilistic terms, we would like to induce stochastic dominance of the reward of chosen sentences on the reward of rejected ones. First order Stochastic Dominance (FSD, see e.g. Ogryczak and Ruszczynski, 2002) of a random variable \(X\) on a random variable \(Y\), means that all quantiles values of \(X\) are larger than those of \(Y\). Our main contribution is introducing AOT, _Alignment via Optimal Transport_, a new method that enables distributional alignment. We do so by devising a new AOT objective function that induces in the unpaired setting FSD dominance of _chosen reward's_ distribution over _rejected reward's_ distribution. We call this unpaired variant uAOT. In the paired setting, we introduce pAOT that encourages a dominance of chosen to rejected \(\) likelihood ratio of the optimized policy on that ratio for the reference base policy. We show that the AOT cost can be cast as a one-dimensional optimal transport problem that can be solved via sorting and efficiently optimized for the LLM. AOT enjoys also nice statistical properties and achieves the parametric rate since its objective can be seen as a smooth one-dimensional optimal transport problem. AOT achieves state-of-the-art results on the Alpaca leaderboard (Dubois et al., 2024) using the Merlinite 7B model (Sudaliaraj et al., 2024) as a base and scores as the highest 7B model at the time of writing this paper.

To introduce the important concepts of our work pictorially, we show in Fig. 0(a) the quantile plots of the rewards of AOT and alternative alignment strategies (DPO, KTO) for chosen responses (in green) and rejected responses in (red). The quantile plots are estimated on a paired test set. We see that AOT leads to chosen rewards that have larger margins than those of rejected rewards across all percentiles. More importantly, this margin is larger in AOT models than in policies coming from alternative alignment strategies. We then show in Fig. 0(b) how the AOT aligned policy's chosen-to-rejected log-likelihood ratio dominates that same ratio evaluated on the base model's ratio across all percentiles. The distributional alignment induced by AOT ensures a large margin between all quantiles so that the preference is reflected not only on average but distributionally. We formalize distributional preference in the next section.

Figure 1: AOT in the paired & unpaired settings enables first-order stochastic dominance of the chosen reward distribution on the rejected distribution (a). The margin between the quantiles of chosen and rejected rewards is larger than alternative strategies. In (b), we see that AOTâ€™s policy chosen to rejected log-likelihood ratio dominates that ratio for the base model and alternative strategies.

## 2 Distributional Preference via First Order Stochastic Dominance

First Order Stochastic DominanceFor a real random variable \(Z\) we denote \(F_{Z}^{(-1)}:}\) the left-continuous inverse of the Cumulative Distribution Function (CDF) \(F_{Z}\):

\[Q_{Z}(p)=F_{Z}^{(-1)}(p)=\{:F_{Z}() p\}p.\]

Given two random variables \(Z_{1}\) and \(Z_{2}\), we say that \(Z_{1}\) dominates \(Z_{2}\) in the first order if \(Z_{1}\) has larger quantiles than \(Z_{2}\) for all percentiles \(p\):

\[Z_{1}}{}Z_{2} Q_{Z_{1}}(p) Q_{Z_{2}}(p),  p.\] (1)

Let \(\) be the space of prompts \(X\) and \(\) be the space of responses \(Y\) from an LLM conditioned on a prompt \(X\). The reference LLM is represented as policy \(_{}(Y|X)\), i.e., as a conditional probability on \(\) given a prompt \(X\). We note the LLM policy we are optimizing by \(_{}\) where \(\) is a parameter belonging to a bounded parameter space \(^{d_{}}\). For a measure \(()\) and a mapping \(r:\), we note as \(r_{}\) the pushforward map of \(\) through \(r\). In particular, for empirical measures \(=_{i=1}^{n}_{(x_{i},y_{i})}\), we have that \(r_{}=_{i=1}^{n}_{r(x_{i},y_{i})}\).

**DPO as a Pointwise Preference Approach** In Direct Preference Optimization (DPO, Rafailov et al., 2024), the reward being optimized by the LLM has the following form :

\[r_{}(x,y)=(y|x)}{_{}(y|x)}+ (Z(x)),\]

where \(Z(x)\) is a normalization constant. DPO assumes access to a paired preference dataset \((X,Y_{+},Y_{-})\) where \(Y_{+}\) denotes a positive (chosen) response to which we would like to assign a high reward, and \(Y_{-}\) a negative (rejected) response to which we would like to assign a low reward. This can be formalized as minimizing the logarithmic sigmoid loss :

\[_{}-_{(x,y_{+},y_{-})}(((r _{}(x,y_{+})-r_{}(x,y_{-})))),\]

and since the difference is taken for the same \(x\), the normalization \(Z(x)\) disappears resulting in:

\[_{}-_{(x,y_{+},y_{-})}(( ((y_{+}|x)}{_{}(y_{+}|x)})- ((y_{-}|x)}{_{}(y_{-}|x)}) )).\]

We can interpret this as a pointwise constraint inducing preference for positive over negative reward outcomes as follows:

\[((y_{+}|x)}{_{}(y_{+}|x)}) ((y_{-}|x)}{_{}(y_{-}|x)}), (x,y_{+},y_{-}).\] (2)

DPO can then be interpreted as a relaxation of this constraint through the logistic loss, which also suggests other preference optimization algorithms through relaxations using, for example, the hinge loss as proposed in SLIC (Zhao et al., 2023).

### Distributional Preference via Stochastic Dominance

Our main insight from looking at the pointwise constraint in Eq. (2) is that we can recast it as a distributional constraint in terms of stochastic dominance of the random variable \(Z_{}^{+}=((Y_{+}|X)}{_{}(Y_{+}|X)})\) of positive outcomes on the random variable \(Z_{}^{-}=((Y_{-}|X)}{_{}(Y_{-}|X)})\) of negative outcomes. This is especially valuable in the unpaired setting without access to triplets of prompts and positive and negative responses as required by DPO. This is indeed the same setting considered by KTO (Ethayarajh et al., 2024). The following paragraph formalizes this unpaired distributional preference.

Distributional Unpaired PreferenceWe assume here that we don't have access to triplets of prompts and positive/negative responses \((x,y_{+},y_{-})\). Instead, we assume separate access to \(_{+}()\), a distribution of positive prompt/response pairs \((X_{+},Y_{+})\) we would like to be highly rewarded and reinforce in the policy, and \(_{-}()\) the distribution of the negative samples \((X_{-},Y_{-})\) to be associated with low reward. We define the distributional preference as follows:

**Definition 1** (Distributional Preference in the Unpaired Setting).: _A policy \(\) prefers distributionally \(_{+}\) on \(_{-}\) with respect to a reference policy \(_{}\) if:_

\[(Y_{+}|X_{+})}{_{}(Y_{+}|X_{+})}}{}(Y_{-}|X_{-})}{_{}(Y_{ -}|X_{-})}.\]

_In other words, noting \(r_{u}_{}(x,y)=(y|x)}{_{}(y|x)}\), the distributional preference in the unpaired setting means that we have the following constraint:_

\[(r_{u}_{})_{}_{+}}{}(r_{u} _{})_{}_{-}.\] (3)

Distributional Paired PreferenceNote that we can rewrite Eq. (2) in the equivalent form:

\[(y_{+}|x)}{_{}(y_{-}|x)}}(y_{+}|x)}{_{}(y_{-}|x)},(x,y_{+},y_{-}) .\] (4)

In order to turn this into a distributional constraint we need access to a paired preference dataset as in DPO \((X,Y_{+},Y_{-})\), and impose stochastic dominance of the random variable \(Z_{}=(Y_{+}|X)}{_{}(Y_{-}|X)}\) indexed by the policy we are optimizing on the random variable \(Z_{}=}(Y_{+}|X)}{_{}(Y_{-}|X)}\) indexed by the reference policy. \(Z_{}\) and \(Z_{}\) represent here the \(\) likelihood ratio of positive to negative outcome under the policies \(_{}\) and \(_{}\), respectively. Hence, it is desirable to constrain the policy \(_{}\) to have a larger excess log probability between positive and negative outcomes than that resulting from the reference policy \(_{}\).

We define below more formally the paired distributional preference via stochastic dominance:

**Definition 2** (Distributional Preference in the Paired Setting).: _We say that the policy \(_{}\) distributionally dominates \(_{}\) in terms of \(\) probability ratio of positive and negative responses if:_

\[(Y_{+}|X)}{_{}(Y_{-}|X)} }{}}(Y_{+}|X)}{_{}(Y_{-}|X)}.\]

_Noting \(r_{p}_{}(x,y_{+},y_{-})=(y_{+}|x)}{_{ }(y_{-}|x)}\) this can be written as follows:_

\[(r_{p}_{})_{}}{}(r_{p} _{})_{}.\] (5)

## 3 AOT: Alignment via Optimal Transport a Convex Relaxation Approach

Note that the paired and unpaired distributional preference constraints in Definitions 1 and 2 can be used in LLM alignment as follows:

\[\;_{}\;(r_{u}_{ })_{}_{+}}{}(r_{u}_{ })_{}_{-}\] (FSD unpaired)

and

\[_{}\;(r_{p}_{ })_{}}{}(r_{p}_{} )_{}\] (FSD paired)

where \(r_{u}\) are \(r_{p}\) are given in Definitions 1 and 2 respectively, and \(\) is a hypothesis class. Those two problems are instances of learning with stochastic orders introduced in , but in a simpler setting since the constraints are on one-dimensional distributions and the order considered is the first order rather than the convex order as considered in . Note that both problems are special cases of the following generic optimization problem:

\[\;:U_{}}{ }V_{}\] (6)

where \(U_{}\) and \(V_{}\) are real-valued random variables whose distributions depend on a parameter vector \(\). Note that for our FSD paired setting, \(V_{}=V\) (independent of \(\)). Let \(_{U_{}}\) and \(_{V_{}}\) be the probability measures of \(U_{}\) and \(V_{}\) resp.

By the definition of FSD in Equation (1) we have:

\[U_{}}{}V_{} Q_{U_{}}(t)  Q_{V_{}}(t), t.\]We can relax this problem to the following minimization problem:

\[_{}():=_{0}^{1}h(Q_{U_{}}(t)-Q_{V_{ }}(t))dt,\] (7)

where \(h\) is a function penalizing each quantile's violation of FSD. The objective function (7) seeks to measure the violation of FSD, so that it can be minimized or eliminated. For instance, with \(h\) the 0/1 loss (here \(\) is the indicator function):

\[_{}_{0}^{1}_{Q_{U_{}}(t)<Q_{V_{} }(t)}dt,\] (8)

This loss reminds us the misclassification \(0/1\) loss. Following classical convex relaxation of \(0/1\) losses in binary classification , we consider surrogates \(h\) of the indicator function. Our choices for \(h\) are motivated by the "almost-FSD" notions in the literature (See Appendix F for a discussion). In practice, we use smooth convex approximations of the 0/1 loss (\(_{x<0}\)) , for example for a margin \(>0\)\(h(x)=(-x)_{+}^{2}\) the \(-\)**squared hinge loss** or \(h(x)=(1+(- x))\) the \(\)**-logistic loss**. Although not a convex relaxation of the 0/1 loss, the least squares loss has been used in classification , and in the context of alignment, it was used in IPO  hence we use also \(h(x)=(-x)^{2}\), and refer to it as \(\)**-Least Squares**. Further discussion of tradeoffs and benefits of different losses is in Appendix F, and formal assumptions on \(h\) needed for the statistical theory are given in Assumption 1.

The cost function in (7) is still computationally challenging, if we were to solve the problem via gradient descent on \(\) this would require us to differentiate through the quantile operation. The following theorem from Santambrogio (2015) will be instrumental for us to cast the loss in (7) as an optimal transport problem with a convex cost \(h\):

**Theorem 1** (Theorem 2.9 and Proposition 2.17 in Santambrogio (2015)).: _Let \(h:^{+}\) be a convex function we have for two real random variables \(U,V\), with measures \(_{U},_{V}\):_

\[_{0}^{1}h(Q_{U}(t)-Q_{V}(t))dt=_{(_{U},_{V})} h( u-v)d(u,v)=_{h}(_{U},_{V})\]

_and \(^{*}=(Q_{U},Q_{V})_{}_{1}()\) is a minimizer (where \(_{1}\) is the Lebesgue measure on \(\) ). If furthermore \(h\) is strictly convex \(^{*}\) is the unique minimizer._

Thanks to Theorem 1 we can write the problem (7), in the following equivalent form that we call Alignment via Optimal Transport (AOT) :

\[_{}_{0}^{1}h(Q_{U_{}}(t)-Q_{V_{}}(t))dt= _{}_{h}(_{U_{}},_{V_{}})= _{}_{(_{U_{}},_{V_{}})} h (u-v)d(u,v).\] (9)

This formulation reveals that we have turned the stochastic dominance constraint to an inner one-dimensional optimal transport problem with a convex cost \(h\). \(_{h}(_{U_{}},_{V_{}})\) can be thought as a soft measure of the violation of the stochastic dominance of \(U_{}\) on \(V_{}\), hence by minimizing it as function of \(\) we are ensuring the optimal \(^{*}\) results in \(U_{^{*}}\) dominating \(V_{^{*}}\). Such OT problems with a smooth cost have been subject to theoretical and statistical study in one dimension as well as in high dimensions. For instance,  considered smooth convex costs, and  considered more general smooth costs.  considered entropic regularization of optimal transport with general smooth costs.

Computational Algorithm via SortingWe consider here empirical measures and turn to solve the inner problem for a fixed \(\). We omit \(\) in what follows to simplify notation. We are interested in \(_{h}(_{U},_{V})\) where \(_{U}=_{i=1}^{n}_{u_{i}}\)\(_{V}=_{i=1}^{n}_{v_{i}}\). Given the convexity of \(h\) and thanks to Theorem (1), the optimal coupling of \(_{h}(_{U_{}},_{V_{}})\) is given by the north-west corner solution  (Chapter 3, Section 3.4.2) that informally matches the \(i-\)th smallest element of \(U\) with the \(i-\)th smallest element from \(V\). More formally, if we sort the variables \(u_{i}\) and get the order statistics (from min to max) \(u^{(1)}... u^{(n)}\) and same for \(v_{i}\): \(v^{(1)}... v^{(n)}\). We have:\[_{h}(_{U},_{V})=_{i=1}^{n}h(u^{(i)}-v^{ (i)}).\] (10)

Back to (9), given empirical samples \(_{U_{}}=_{i=1}^{n}_{u^{i}_{}}\) and \(_{V_{}}=_{i=1}^{n}_{v^{i}_{}}\), let \(u^{(i)}_{},v^{(i)}_{}\) be the order statistics as function of \(\). We have therefore:

\[_{h}(_{U_{}},_{V_{}})=_{}_{i=1}^{n}h(u^{(i)}_{ }-v^{(i)}_{})\ \ \ }\] (11)

In Appendix G, we show that the gradients of the objective (11) are asymptotically unbiased for bounded distributions (see the statement for all conditions). Note that the sorting operation in (11) is a 1-Lipschitz function with discontinuous Jacobian.2 Like the ReLU activation function, it can be easily optimized by gradient descent (Anil et al., 2019) (compare also sliced Wasserstein GANs). In practice, computing the gradient at any given step is done by first running the sorting algorithm and taking the gradient with respect to \(\) with the current assignment held fixed.

Aot for Unpaired PreferenceLet \(^{n}_{+}=_{i=1}^{n}_{(x_{i,+},y_{i,+})}\) and \(^{n}_{-}=_{i=1}^{n}_{(x_{i,-},y_{i,-})}\). Our convex relaxation approach for unpaired FSD alignment given in (FSD unpaired) can therefore be cast as an AOT problem (given in Equation (11)) for

\[u^{i}_{}=(y_{i,+}|x_{i,+})}{_{}(y_ {i,+}|x_{i,+})},\ \ v^{i}_{}=(y_{i,-}|x_{i,-})}{_{}( y_{i,-}|x_{i,-})}, i=1,,n.\]

Aot for Paired PreferenceLet \(^{n}=_{i=1}^{n}_{(x_{i,}y_{i,+},y_{i,-})}\) be a paired preference empirical measure. Our convex relaxation approach for paired FSD alignment given in (FSD paired) can be there cast as an AOT problem (given in Equation (11)) for:

\[u^{i}_{}=(y_{i,+}|x_{i,+})}{_{}(y_ {i,-}|x_{i,+})},\ \ v^{i}_{}=}(y_{i,+}|x_{i })}{_{}(y_{i,-}|x_{i})}, i=1,,n.\]

Aot with Soft SortingOne caveat of the alternating optimization for AOT between \(\) and solving the inner optimal transport problem with hard sorting is that the gradient with respect to the parameter \(\) for fixed permutations has dependency in \(\) on the order statistics level only and not through the sorting routine. To alleviate that, we propose to use SoftSorting(Blondel et al., 2020; Cuturi et al., 2019) that uses an entropic regularization to find a smoothed permutations via a Sinkhorn algorithm, which in turn allows the back-propagation on \(\) to depend not only via the order statistics but also via the computational graph of SoftSorting.

Algorithms 1 and 2 in Appendix B summarize our AOT approach for distributional preference alignment in the unpaired and paired setting.

## 4 Statistical Analysis

In this section, we focus on the statistical analysis of unpaired-AOT and defer paired-AOT to Appendix E since it has a similar analysis. We make the following assumptions on the OT cost \(h\), the reward \(r\), and the policy hypothesis class \(\).

**Assumption 1** (OT cost).: _Let \(M,R>0\) be finite positive constants. We assume that the loss \(h:[-M,M][0,R]\), is convex \(L\)-Lipchitz and bounded. \(h\) is a convex function (E.g. a relaxation of the 0/1 loss such that \(h(t)>h(t^{})\), for \(t<0\) and \(t^{}>0\))._

**Assumption 2** (Reward).: _We assume that \(r\) is bounded so that \(r_{}(x,y)[-M,M]\)._

**Assumption 3** (Assumption on the hypothesis class of the policy).: _We assume \(_{},_{}=\{_{}:\) such that \(r_{}\) differentiable in \(\) and \(_{x,y}\|_{}r_{}(y|x )\| L^{}, B_{2}(r_{0},d_{})\},\) for \(L^{},r_{0}>0\).._

**Assumption 4**.: _There exists \(_{}\) such that \((r_{})_{_{+}}(r_{ })_{}_{-}\)._

Assumption 1 is satisfied for example by the hinge squared loss \(h(t)=(-t)_{+}^{2}\) by the logistic loss \(h(t)=(1+e^{- t})\), for \(t[-M,M]\). Assumption 2 on the boundedness of the rewards can be imposed by clamping the values of the logits of the policies to \([-M,M]\), which is common practice in practical implementations of LLM alignment. Assumption 3 is a technical assumption needed to control the covering number of the \(r\). Assumption 4 ensures the existence of the minimizer in \(\). We overload notations in what follows and refer to \(r_{u}\) and \(r_{p}\) as \(r\) to simplify the presentation. By our relaxation approach described in Section 3 we can relax the unpaired stochastic dominance constraint problem given in (FSD unpaired) to:

\[_{_{}}_{0}^{1}h(Q_{(r_ {})_{_{+}}}(t)-Q_{(r_{})_{}_{-}}(t) )dt=_{_{}}_{h}((r_{ })_{}_{+},(r_{})_{}_{-})\] ( \[_{h}\] )

Define the OT cost \(c:[-M,M][-M,M][0,R]\) such that \(c(z,z^{})=h(z-z^{})\), for \(z,z[-M,M]\). Define the \(c\)-transform of a function \(:[-M,M]\):

\[^{c}(z)=_{z^{}[-M,M]}h(z-z^{})-(z).\]

In our setting, a function is called \(c\)-concave if there exists \(:[-M,M]\) such that \(=^{c}\). Define:

\[_{c}=\{:[-M,M][-R,R],||^{c}||_{} R\}\]

By duality (Theorem 5.10 in ) we have:

\[_{h}((r_{})_{}_{+},(r_{ })_{}_{-})=_{_{c}}(r _{})d_{+}-^{c}(r_{})d_{-}.\]

Replacing the dual expression of \(_{h}\) in (\(_{h}\)), we see that (\(_{h}\)) can be cast as a min-max problem:

\[_{_{}}_{_{c}}( r_{})d_{+}-^{c}(r_{})d_{-}.\] (12)

Given samples \(_{+}^{n}=_{i=1}^{n}_{(x_{i,+},y_{i,+})}\) and \(_{-}^{n}=_{i=1}^{n}_{(x_{i,-},y_{i,-})}\), the empirical problem is:

\[_{_{}}_{_{c}}( r_{})d_{+}^{n}-^{c}(r_{})d _{-}^{n}.\] (13)

Recall that \(_{h}\) is a measure of the violation of stochastic dominance of \((r_{})_{}_{+}\) on \((r_{})_{}_{-}\). We have the following result on the sample complexity of the violation of stochastic dominance:

**Theorem 2** (Sample Complexity of Dominance Violation for \(\) Unpaired).: _Let \(_{^{*}}\) be the population minimizer of (\(_{h}\)) and \(_{_{n}}\) be the solution of the empirical problem (13). We have the following sample complexity bound for the violation of stochastic dominance in \(\) unpaired:_

\[\ _{h}((r_{_{n}})_{} _{+},(r_{_{n}})_{}_{-})_{h}((r_{^{*}})_{}_{+},(r_{ ^{*}})_{}_{-})}_{}\] \[+_{n}(_{c};(r_{ ^{*}})_{}_{+})+2_{n}(_{c}^{c};(r_{ ^{*}})_{}_{-})}_{^{*}}+2_{n}(_{c} r; _{+})+2_{n}(_{c}^{c} r;_{-} )}_{$ via the 1D OT problem}}\]

_where \(_{n}(;)=_{}| _{i=1}^{n}_{i}(Z_{i})|\) is the Rademacher Complexity and for \(i=1 n\), \(_{i}\) are independent Rademacher random variables and \(Z_{i}\) iid._

By considering our assumptions on the cost, the reward, and the hypothesis class, we obtain the parametric rate in \(n\):

**Corollary 1**.: _(Informal) Under Assumptions 1, 2 and 3 we have:_

1. \(\ _{h}((r_{_{n}})_{}_{+},(r _{_{n}})_{}_{-})-_{h}((r _{^{*}})_{}_{+},(r_{^{*}})_{}_{ -}) n^{-},\) _where_ \(\) _refers to inequality up to constants that depend only on constants in the assumptions._2. _If in addition Assumption_ 4 _holds and_ \(h(t)=(-t)_{+}^{2}\)_, we have:_ \(\;_{h}((r_{_{n}})_{t}_{+},(r _{_{n}})_{t}_{-}) n^{-}\)_._

We see that under our assumptions and for the hinge loss squared, the expected violation of the desired dominance in AOT unpaired converges to zero as \(n\).

**Remark 1**.: _While in Section 3, we used the primal formulation to compute \(_{h}\) due to its computational appeal thanks to the sorting algorithm we used for analyzing the sample complexity the dual of \(_{h}\). The dual reveals the game theoretic aspect of \(\) as a min-max game between the policy \(_{}\) and the dual potential \(_{c}\) that imposes FSD on the preference we want to infuse to the policy._

## 5 Experiments

In this section, we evaluate the performance of the proposed \(\) method on a diverse set of base LLMs and datasets, comparing with currently available alternative alignment algorithms.

LLM Alignment AlternativesWe compared \(\) with current state-of-the-art alignment approaches, specifically Direct Preference Optimization (DPO) (Rafailov et al., 2024), Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., 2024) and Identity Policy Optimization (IPO) (Azar et al., 2024). DPO and IPO operate on paired preference data, while KTO can handle both paired and unpaired prompt/response samples.

**Reference Models** Traditionally, model alignment is the third and final step applied to the LLM that already has gone through original pretraining and supervised fine-tuning. For our experiments, we selected a range of models at various stages and with different levels of performance, all in the family of 7B-parameter models. Specifically, we used Merlinite-7B (Sudalairaj et al., 2024), which is a variant of Mistral-7B-v0.1 that has been instruction-tuned (SFT) on data from a synthetic data generator using a taxonomy-driven data curation process. In Appendix H we also cover other popular LLMs, such as Mistral-7B (Jiang et al., 2023), OpenHermes-2.5-Mistral-7B (Teknium, 2024), Starling (Zhu et al., 2023), Mistral-7B Jiang et al. (2023), and Llama3-8B (AI@Meta, 2024).

**Datasets** For our experiments, we used both paired and unpaired datasets. For the paired dataset, we used the UltraFeedback binarized dataset from (Tunstall et al., 2023b), containing over 60K training samples, where for each prompt, there is a pair of chosen (preferred) and rejected (not preferred) responses. This alignment dataset is widely used, and all compared alignment techniques are well-suited for it. For unpaired datasets, we used PKU BeaverTails (Ji et al., 2023) with over 300K samples and HelpSteer (Wang et al., 2023) with around 35K samples. Here, for each prompt, there is only a single response with a score defined by some attributes (e.g., safety, faithfulness, helpfulness, etc.). We used the sum of attribute values and thresholded by the median to binarize the responses into chosen and rejected. For this unpaired dataset, only KTO and our \(\) are applicable.

**Metrics** To measure the performance of different alignment methods, we used popular evaluation metrics, AlpacaEval (Dubois et al., 2024) and Open LLM benchmark (Beeching et al., 2023). We note that Alpaca uses GPT4 model as a judge to compare candidate responses to GPT4-based references on a set of 805 challenging questions. The GPT4-based evaluations are expensive, so to limit our expenses, we also employed a very strong and capable Llama3-70B-Instruct (AI@Meta, 2024) as a judge. As we show in Appendix H in Table 2, the order determined by Llama3-70B-Instruct

    &  &  & Hellaswag & MMLU & Truthful & Winogrande & GSM8K \\   & (GPT4) & ARC & Hellaswag & MMLU & Truthful & Winogrande & GSM8K \\  AOT paired & 29.9 & 82.5 & 66.1 & 62.9 & 50.8 & 74.4 & 53.1 \\ AOT unpaired & **31.3** & 82.5 & **66.2** & 62.8 & **51.1** & 74.4 & 51.8 \\ DPO & 27.4 & **82.8** & 65.8 & **63.1** & 50.6 & 74.3 & 52.0 \\ KTO & 24.9 & 82.7 & 65.4 & 63.0 & 48.7 & **74.9** & **53.9** \\ IPO & 27.7 & 82.4 & 65.1 & 63.0 & 46.5 & 74.0 & 52.3 \\ Merlinite-7B & 17.1 & 81.6 & 63.2 & 62.6 & 42.0 & 73.9 & 45.2 \\   

Table 1: Merlinite-7B trained on UltraFeedback Binarized. AOT results in the best performing LLM as compared to the alternative alignment algorithms on AlpacaEval, and is competitive across the other benchmarks that are evaluated in the zero shot regime.

and GPT4 is the same (the absolute score values are different), providing a better free alternative LLM-judge for local Alpaca evaluations. For intermediate results, we also employed Tiny Benchmarks [Maia Polo et al., 2024] to approximate original metrics and provide fast feedback during the initial development. We also evaluated the aligned models on six key benchmarks from the Open LLM Leaderboard: AI2 Reasoning Challenge - ARC (grade-school science questions), HellaSwag (commonsense inference), MMLU (multi-task accuracy), TruthfulQA (tendency to reproduce false-hoods), Winogrande (commonsense reasoning) and GSM8K (grade school math word problems). Note that in all the above benchmarks we use 0-shot prompts, a more challenging setting as opposed to commonly used few-shot prompting.

**Experimental Setup** Our implementation is based on the HuggingFace Alignment Handbook Tunstall et al. [2023a]. As we show in Appendix in Section B, the changes needed to adapt HF TRL trainer [von Werra et al., 2020] for AOT are minimal and therefore can easily be adapted by the community. For each run our compute setup consisted of 8 H100 GPUs. We used LoRA [Hu et al., 2021] for parameter-efficient fine-tuning during alignment and the FSDP (Fully-Sharded Data-Parallel) setup to train the model over multiple GPUs. Under this setup, the training of each 7B-parameter model on the UltraFeedback dataset took approximately one hour. The evaluation on AlpacaEval and Open LLM benchmarks took one additional hour to get the final results.

**Results** In Table 1, we present the main results of comparing AOT to other baselines (KTO, DPO, and IPO) on paired UltraFeedback binarized dataset. On AlpacaEval ( GPT4), our AOT unpaired approach scores 31.3%, which is a significant gain from the base Merlinite-7B model. As of time of this writing (May 22nd, 2024), this result places our AOT aligned LLM on AlpacaEval LeaderBoard ahead of such strong competitors as KTO-Mistral-PAIR [Ethayarajh et al., 2023] and other 7B-parameter models, reaching the level of Mistral-8x22B-v0.1 (see Figure 4 in Appendix for an illustration). On other LLM benchmarks AOT performs competitively to other baselines. As mentioned earlier, these evaluations are done using 0-shot prompts, leading to a more challenging setting and resulting in overall lower performance across metrics and baselines. For other base LLMs we show their performance in Appendix H (see Tables 3, 4, 5, and 6).

Figure 3: Impact of (\(\)) parameter on performance of different alignment algorithms. \(\) controls the divergence of the policy model from the initial reference model (low beta - more divergence, high beta - less divergence). We see a general trend that with higher betas, LLMs alignment decreases the performance. Hence, for all experiments, we selected \(=0.01\) as a default value.

Figure 2: Impact of batch size and loss type on AOT performance. The batch size is the effective number of samples in the mini-batch per GPU. We found the logistic loss to be performing better than least squared or hinge squared losses (all using \(=0.01\)). As we increase batch size, we also observed improvement in AOT performance, which is expected as more samples per minibatch results in a better effect of stochastic dominance (conforming Corollary 1).

We also examined the effect of batch size and the choice of loss function on AOT performance, results shown in Fig. 2. As the batch size increases, AlpacaEval (based on Llama3-70B-instruct) also increases in line with our theory in Corollary 1. Note that our current setup (FSDP over 8 H100 GPUs) limits our batch size to 35 samples per GPU. We have also examined the impact of beta (controlling divergence of policy from reference) on AOT performance in Fig. 3. We noticed a trend that with higher betas the performance of LLMs alignment decreases, thus we set \(=0.01\). Ablation results comparing hard and soft sorting as well as the variance of AlpacaEval scores across multiple runs in Appendix H (Tables 7 and 10) show the overall robustness of AOT.

## 6 Conclusion

We present in this paper Distributional Alignment via Optimal Transport (AOT) for large language models. The AOT cost can be cast as a one-dimensional optimal transport problem with a smooth and convex cost that penalizes violations of the dominance of the chosen on rejected marginals. AOT enjoys parametric statistical rates. We showed with extensive experimentation on various paired and unpaired datasets, base models, and different loss functions, that AOT alignment robustly leads to aligned models that outperform alternative alignment strategies such as DPO, KTO and IPO on the Alpaca Benchmark, leading to the best 7B model to date on that benchmark as of the time of writing. On other benchmarks such as the open LLM leaderboard AOT leads to competitive results.