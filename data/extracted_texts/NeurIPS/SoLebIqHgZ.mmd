# ARTree: A Deep Autoregressive Model for Phylogenetic Inference

Tianyu Xie\({}^{}\), Cheng Zhang\({}^{,,}\)

\({}^{}\) School of Mathematical Sciences, Peking University

\({}^{}\) Center for Statistical Science, Peking University

tianyuxie@pku.edu.cn, chengzhang@math.pku.edu.cn

Corresponding author.

###### Abstract

Designing flexible probabilistic models over tree topologies is important for developing efficient phylogenetic inference methods. To do that, previous works often leverage the similarity of tree topologies via hand-engineered heuristic features which would require pre-sampled tree topologies and may suffer from limited approximation capability. In this paper, we propose a deep autoregressive model for phylogenetic inference based on graph neural networks (GNNs), called ARTree. By decomposing a tree topology into a sequence of leaf node addition operations and modeling the involved conditional distributions based on learnable topological features via GNNs, ARTree can provide a rich family of distributions over the entire tree topology space that have simple sampling algorithms and density estimation procedures, without using heuristic features. We demonstrate the effectiveness and efficiency of our method on a benchmark of challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems.

## 1 Introduction

Reconstructing the evolutionary relationships among species has been one of the central problems in computational biology, with a wide range of applications such as genomic epidemiology (Dudas et al., 2017; du Plessis et al., 2021; Attwood et al., 2022) and conservation genetics (DeSalle and Amato, 2004). Based on molecular sequence data (e.g. DNA, RNA, or protein sequences) of the observed species and a model of evolution, this has been formulated as a statistical inference problem on the hypotheses of shared history, i.e., _phylogenetic trees_, where maximum likelihood and Bayesian approaches are the most popular methods (Felsenstein, 1981; Yang and Rannala, 1997; Mau et al., 1999; Larget and Simon, 1999; Huelsenbeck et al., 2001). However, phylogenetic inference can be challenging due to the composite structure of tree space which contains both continuous and discrete components (e.g., the branch lengths and the tree topologies) and the large search space of tree topologies that explodes combinatorially as the number of species increases (Whidden and Matsen IV, 2015; Dinh et al., 2017).

Recently, several efforts have been made to improve the efficiency of phylogenetic inference algorithms by designing flexible probabilistic models over the tree topology space (Hohna and Drummond, 2012; Larget, 2013; Zhang and Matsen IV, 2018). One typical example is subsplit Bayesian networks (SBNs) (Zhang and Matsen IV, 2018), which is a powerful probabilistic graphical model that provides a flexible family of distributions over tree topologies. Given a sample of tree topologies (e.g., sampled tree topologies from an MCMC run), SBNs have proved effective for accurate tree topology density estimation that generalizes beyond observed samples by leveraging the similarity of hand-engineered subsplit structures among tree topologies. Moreover, SBNs also allow fast ancestral sampling andhence were later on integrated into a variational Bayesian phylogenetic inference (VBPI) framework to provide variational posteriors over tree topologies (Zhang & Matsen IV, 2019). However, due to the limited parent-child subsplit patterns in the observed samples, SBNs can not provide distributions whose support spans the entire tree topology space (Zhang & Matsen IV, 2022). Furthermore, when used as variational distributions over tree topologies in VBPI, SBNs often rely on subsplit support estimation for variational parameterization, which requires high-quality pre-sampled tree topologies that would become challenging to obtain when the posterior is diffuse.

While SBNs suffer from the aforementioned limitations due to their hand-engineered design, a number of deep learning methods have been proposed for probabilistic modeling of graphs (Jin et al., 2018; You et al., 2018; Cao & Kipf, 2018; Simonovsky & Komodakis, 2018). Instead of using hand-engineered features, these approaches use neural networks to define probabilistic models for the connections between graph nodes which allow for learnable distributions over graphs. Due to the flexibility of neural networks, the resulting models are capable of learning complex graph patterns automatically. Among these deep graph models, graph autoregressive models (You et al., 2018; Li et al., 2018; Liao et al., 2019; Dai et al., 2020; Shi et al., 2020) are designed to learn flexible graph distributions that also allow easy sampling procedures by sequentially adding nodes and edges. Therefore, they serve as an ideal substitution of SBNs for phylogenetic inference that can provide more expressive distributions over tree topologies.

In this paper, we propose a novel deep autoregressive model for phylogenetic inference, called ARTree, which allows for more flexible distributions over tree topologies without using heuristic features than SBNs. With a pre-selected order of leaf nodes (i.e., species or taxa), ARTree generates a tree topology by recursively adding new leaf nodes to the edges of the current tree topology, starting from a star-shaped tree topology with the first three leaf nodes (Figure 1). The edge to which a new leaf node connects is determined according to a conditional distribution based on learnable topological features of the current tree topology via GNNs (Zhang, 2023). This way, probability distributions provided by ARTree all have full support that spans the entire tree topology space. Unlike SBNs, ARTree can be readily used in VBPI without requiring subsplit support estimation for parameterization. In experiments, we show that ARTree outperforms SBNs on a benchmark of challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems.

## 2 Background

Phylogenetic likelihoodsA phylogenetic tree is commonly described by a bifurcating tree topology \(\) and the associated non-negative branch lengths \(\). The tree topology \(\) represents the evolutionary relationship of the species and the branch lengths \(\) quantify the evolutionary intensity along the edges of \(\). The leaf nodes of \(\) correspond to the observed species and the internal nodes of \(\) represent the unobserved ancestor species. A continuous time Markov model is often used to describe the transition probabilities of the characters along the edges of the tree (Felsenstein, 2004). Concretely, let \(=\{Y_{1},,Y_{M}\}^{N M}\) be the observed sequences (with characters in \(\)) of length \(M\) over \(N\) species. Under the assumption that different sites evolve independently and identically, the likelihood of \(\) given \(,\) takes the form

\[p(|,)=_{i=1}^{M}p(Y_{i}|,)=_{i=1}^{M} _{a^{i}}(a^{i}_{r})_{(u,v) E()}P_{a^{i}_{u}a^{i}_{v}}(q_{ uv}),\] (1)

where \(a^{i}\) ranges over all extensions of \(Y_{i}\) to the internal nodes with \(a^{i}_{u}\) being the character assignment of node \(u\) (\(r\) represents the root node), \(E()\) is the set of edges of \(\), \(q_{uv}\) is the branch length of the edge \((u,v) E()\), \(P_{jk}(q)\) is the transition probability from character \(j\) to \(k\) through a branch of length \(q\), and \(\) is the stationary distribution of the Markov model.

Subsplit Bayesian networksLet \(\) be the set of leaf labels representing the existing species. A non-empty subset of \(\) is called a _clade_ and the set of all clades \(()\) is equipped with a total order \(\) (e.g., lexicographical order). An ordered clade pair \((W,Z)\) satisfying \(W Z=\) and \(W Z\) is called a _subsplit_. A _subsplit Bayesian network_ (SBN) is then defined as a Bayesian network whose nodes take subsplit values or singleton clade values that describe the local topological structures of tree topologies. For a rooted tree topology, one can find the corresponding node assignment of SBNsby following its splitting processes (Figure 4 in Appendix A). The SBN based probability of a rooted tree topology \(\) then takes the following form

\[p_{}(T=)=p(S_{1}=s_{1})_{i>1}p(S_{i}=s_{i}|S_{_{i}}=s_{ _{i}}),\] (2)

where \(S_{i}\) denotes the subsplit- or singleton-clade-valued random variables at node \(i\) (node 1 is the root node), \(_{i}\) is the index set of the parents of node \(i\) and \(\{s_{i}\}_{i 1}\) is the corresponding node assignment. For unrooted tree topologies, we can also define their SBN based probabilities by viewing them as rooted tree topologies with unobserved roots and integrating out the positions of the root node as follows: \(p_{}(T^{u}=)=_{e E()}p_{}(^{e})\), where \(^{e}\) is the resulting rooted tree topology when the rooting position is on edge \(e\). In practice, the conditional probability tables (CPTs) of SBNs are often parameterized based on a sample of tree topologies (e.g., the observed data for density estimation (Zhang & Matsen IV, 2018) or fast bootstrap/MCMC samples (Minh et al., 2013; Zhang, 2020) for VBPI). As a result, the supports of SBN-induced distributions are often limited by the splitting patterns in the observed samples and could not span the entire tree topology space (Zhang & Matsen IV, 2022). More details on SBNs can be found in Appendix A.

Variational Bayesian phylogenetic inferenceGiven a prior distribution \(p(,)\), the phylogenetic posterior distribution takes the form

\[p(,|)=|,)p(,)}{p()}  p(|,)p(,).\] (3)

Let \(Q_{}()\) and \(Q_{}(|)\) be variational families over the spaces of tree topologies and branch lengths respectively. The VBPI approach uses \(Q_{,}(,)=Q_{}()Q_{}(|)\) to approximate the posterior \(p(,|)\) by maximizing the following multi-sample (\(K>1\)) lower bound

\[L^{K}(,)=_{\{(^{i},^{i})\}_{i=1}^{K}\, ,,Q_{,}}(_{i=1} ^{K}|^{i},^{i})p(^{i},^{i})}{Q_{}( ^{i})Q_{}(^{i}|^{i})}).\] (4)

The tree topology distribution \(Q_{}()\) is often SBNs which in this case rely on subsplit support estimation for parameterization that requires high-quality pre-sampled tree topologies and would become challenging for diffuse posteriors (Zhang & Matsen IV, 2022). The branch lengths distribution \(Q_{}(|)\) can be diagonal lognormal distribution parametrized via heuristic features or learnable topological features of \(\)(Zhang & Matsen IV, 2019; Zhang, 2020, 2023). Compared to the single-sample lower bound, the multi-sample lower bound in (4) allows efficient variance-reduced stochastic gradient estimators (e.g. VIMCO (Mnih & Rezende, 2016)) for tree topology variational parameters. Moreover, using multiple samples would encourage exploration over the vast tree topology space, albeit it may also deteriorates the training of the variational approximation (Rainforth et al., 2019). In practice, a moderate \(K\) is often suggested (Zhang & Matsen IV, 2022). See more details on VBPI in Appendix B.

Graph autoregressive modelsBy decomposing a graph as a sequence of components (nodes, edges, motifs, etc), graph autoregressive models generate the full graph by adding one component at a time, until some stopping criteria are satisfied (You et al., 2018; Jin et al., 2018; Liao et al., 2019). In previous works, recurrent neural networks (RNNs) for graphs are usually utilized to predict new graph components conditioned on the sub-graphs generated so far (You et al., 2018). The key of graph autoregressive models is to find a way to efficiently sequentialize graph structures, which is often domain-specific.

## 3 Proposed method

In this section, we propose ARTree, a deep autoregressive model for phylogenetic inference that can provide flexible distributions whose support spans the entire tree topology space and can be naturally parameterized without using heuristic approaches such as subsplit support estimation. We first describe a particular autoregressive generating process of phylogenetic tree topologies. We then develop powerful GNNs to parameterize learnable conditional distributions of this generating process. We consider unrooted tree topologies in this section, but the method developed here can be easily adapted to rooted tree topologies.

### A sequential generating process of tree topologies

To better illustrate our approach, we begin with some notations. Let \(_{n}=(V_{n},E_{n})\) be a tree topology with \(n\) leaf nodes and \(V_{n},E_{n}\) are the sets of nodes and edges respectively. Note that \(|V_{n}|=2n-2\) and \(|E_{n}|=2n-3\) due to the unrooted and bifurcating structure of \(_{n}\). The leaf nodes in \(V_{n}\) are treated as labeled nodes and the interior nodes in \(V_{n}\) are treated as unlabeled nodes. Let us assume a pre-selected order for the leaf nodes \(=\{x_{1},,x_{N}\}\), which is called taxa order for short by us. Now, consider a sequential generating process for all possible tree topologies that have leaf nodes \(\). We start with a definition below.

**Definition 1** (Ordinal Tree Topology).: _Let \(=\{x_{1},,x_{N}\}\) be a set of \(N(N 3)\) leaf nodes. Let \(_{n}=(V_{n},E_{n})\) be a tree topology with \(n(n N)\) leaf nodes in \(\). We say \(_{n}\) is an ordinal tree topology of rank \(n\), if its leaf nodes are the first \(n\) elements of \(\), i.e., \(V_{n}=\{x_{1},,x_{n}\}\)._

We now describe a procedure that constructs ordinal tree topologies of rank \(N\) recursively by adding one leaf node at a time as follows. We first start from \(_{3}\), the ordinal tree topology of rank \(3\), which is the smallest ordinal tree topology and is unique due to its unrooted and bifurcating structure. Suppose now we have an ordinal tree topology \(_{n}=(V_{n},E_{n})\) of rank \(n\). To add the leaf node \(x_{n+1}\) to \(_{n}\), we i) select an edge \(e_{n}=(u,v) E_{n}\) and remove it from \(E_{n}\); ii) add a new node \(w\) and two new edges \((u,w),(w,v)\) to the tree topology; iii) add the leaf node \(x_{n+1}\) and an edge \((w,x_{n+1})\) to the tree topology. This way, we obtain an ordinal tree topology \(_{n+1}\) of rank \(n+1\). Intuitively, the leaf node \(x_{n+1}\) is added to the tree topology \(_{n}\) by attaching it to an existing edge \(e_{n} E_{n}\). The position of the selected edge represents the evolutionary relationship between this new species and others. After performing this procedure for \(n=3,,N-1\), we finally obtain an ordinal tree topology \(=_{N}\) of rank \(N\). See Figure 1 for an illustration.

During the generating process described above, the selected edges at each time step form a sequence \(D=(e_{3},,e_{N-1})\). This sequence \(D\) of length \(N-3\) records all the decisions we have made for autoregressively generating a tree topology \(\) and thus we call \(D\) a decision sequence. In fact, there is a one-to-one mapping between decision sequences and ordinal tree topologies of rank \(N\), which is formalized in Theorem 1. Note that a similar process is also used in online phylogenetic sequential Monte Carlo (OPSMC) (Dinh et al., 2016), where the leaf node addition operation is incorporated into the design of the proposal distributions.

**Theorem 1**.: _Let \(=\{D|D=(e_{3},,e_{N-1}),\ e_{n} E_{n},\ 3 n  N-1\}\) be the set of all decision sequences of length \(N-3\) and \(\) be the set of all ordinal tree topologies of rank \(N\). Let the map_

\[g:&&&\\ &D&&\]

_be the generating process described above. Then \(g\) is a bijection between \(\) and \(\)._

According to Theorem 1, for each tree topology \(\), there is a unique decision sequence given by \(g^{-1}()\). We call this process of finding the decision sequences of tree topologies the _decomposition

Figure 1: An overview of ARTree for autoregressive tree topology generation. The left plot is the starting ordinal tree topology of rank 3. This tree topology is then fed into GNNs which output a probability vector over edges. We then sample from the corresponding edge decision distribution and attach the next leaf node to the sampled edge. This process continues until an ordinal tree topology of rank \(N\) is reached.

process_. See more details on the decomposition process in Appendix C. The following lemma shows that one can find \(g^{-1}()\) in linear time.

**Lemma 1**.: _The time complexity of the decomposition process induced by \(g^{-1}()\) is \(O(N)\)._

The proofs of Theorem 1 and Lemma 1 can be found in Appendix D. Based on the bijection \(g\) defined in Theorem 1, we can model the distribution \(Q(D)\) over the space of decision sequences \(\) instead of modeling the distribution \(Q()\) over \(\). Due to the sequential nature of \(D\), we can decompose \(Q(D)\) as the product of conditional distributions over the elements:

\[Q(D)=_{n=3}^{N-1}Q(e_{n}|e_{3},,e_{n-1}).\] (5)

In what follows, we simplify \(Q(e_{n}|e_{3},,e_{n-1})\) as \(Q(e_{n}|e_{<n})\) and let \(e_{<3}\) be the empty set.

``` Input: a set \(=\{x_{1},,x_{N}\}\) of leaf nodes. Output: an ordinal tree topology \(\) of rank \(N\); the ARTree probability \(Q()\) of \(\). \(_{3}=(V_{3},E_{3})\) the unique ordinal tree topology of rank \(3\); for\(n=3,,N-1\)do  Calculate the probability vector \(q_{n}^{|E_{n}|}\) using the current GNN model;  Sample an edge decision \(e_{n}\) from \((q_{n})\) and assume \(e_{n}=(u,v)\);  Create a new node \(w\); \(E_{n+1}(E_{n}\{e_{n}\})\{(u,w),(w,v),(w,x_{n+1})\}\); \(V_{n+1} V_{n}\{w,x_{n+1}\}\); \(_{n+1}(V_{n+1},E_{n+1})\); end for \(_{N}\); \(Q() q_{3}(e_{3})q_{4}(e_{4}) q_{N-1}(e_{N-1})\). ```

**Algorithm 1**ARTree: An autoregressive model for phylogenetic tree topologies

### Graph neural networks for edge decision distribution

By Theorem 1, the sequence \(e_{<n}\) corresponds to a sequence of ordinal tree topologies of increasing ranks \((_{3},,_{n})\) (the empty set \(e_{<3}\) corresponds to the unique ordinal tree topology \(_{3}\) of rank 3). Therefore, the discrete distribution \(Q(e_{n}|e_{<n})\) in equation (5) defines the probability of adding the leaf node \(x_{n+1}\) to the edge \(e_{n}\) of \(_{n}\), conditioned on all the ordinal tree topologies \((_{3},,_{n})\) generated so far. In what follows, we will show step by step how to use graph neural networks (GNNs) to parameterize such a conditional distribution given tree topologies.

Topological node embeddingsAt the \(n\)-th time step of the generating process, we first find the node embeddings of the current tree topology \(_{n}=(V_{n},E_{n})\), which is a set \(\{f_{n}(u)^{N}:u V_{n}\}\) that assigns each node with an encoding vector in \(^{N}\). Following Zhang (2023), we first assign one hot encoding to the leaf nodes, i.e.

\[[f_{n}(x_{i})]_{j}=_{ij},1 i n,1 j N,\] (6)

where \(\) is Kronecker delta function; we then get the embeddings for the interior nodes by minimizing the Dirichlet energy \((f_{n},_{n}):=_{(u,v) E_{n}}||f_{n}(u)-f_{n}(v)||^{2}\) using the efficient two-pass algorithm described in Zhang (2023). One should note that the embeddings for interior nodes may change as new leaf nodes are added to the ordinal tree topologies, which is a main difference between our model and other graph autoregressive models.

Message passing networksUsing these topological node embeddings as the initial node features, GNNs apply message passing steps to compute the representation vector of nodes that encode topological information of \(_{n}\), where the node features are updated with the information from their neighborhoods in a convolutional manner Gilmer et al. (2017). More concretely, the \(l\)-th round of message passing is implemented by

\[m_{n}^{l}(u,v) =F_{}^{l}(f_{n}^{l}(u),f_{n}^{l}(v)),\] (7a) \[f_{n}^{l+1}(v) =F_{}^{l}(\{m_{n}^{l}(u,v);u( v)\}),\] (7b)where \(F^{l}_{}\) and \(F^{l}_{}\) are the message function and updating function in the \(l\)-th round, and \((v)\) is the neighborhood of the node \(v\). In our implementations, the choices of \(F^{l}_{}\) and \(F^{l}_{}\) follow the edge convolution operator (Wang et al., 2018), while other variants of GNNs can also be applied. The final node features of \(_{n}\) are given by \(\{f^{L}_{n}(v):v V_{n}\}\) after \(L\) rounds of message passing.

Node hidden statesThe conditional distribution \(Q(|e_{<n})\) is highly complicated as it has to capture how \(x_{n+1}\) can be added to \(_{n}\) based on how previous leaf nodes are added to form the tree topologies. A common approach is to use RNNs to model this complex distribution that strikes a good balance between expressiveness and scalability (You et al., 2018; Liao et al., 2019). In our model, after obtaining the final node features of \(_{n}\), a gated recurrent unit (GRU) (Cho et al., 2014) follows, i.e.

\[h_{n}(v)=(h_{n-1}(v),f^{L}_{n}(v)),\] (8)

where \(h_{n}(v)\) is the hidden state of \(v\) at the \(n\)-th generation step and is initialized to zero for the newly added nodes including those in \(_{3}\). The node hidden states \(\{h_{n}(v);v V_{n}\}\), therefore, contain the information of all the tree topologies generated so far which can be used for conditional distribution modeling.

Time guided readoutWe now construct the distribution \(Q(|e_{<n})\) over edge decisions based on the node hidden states. As mentioned before, a main difference between our model and other graph autoregressive models is that the node embedding \(f^{0}_{n}(v)\) of a node \(v\) may vary with the time step \(n\). We, therefore, incorporate time embeddings into the readout step which first forms the edge features \(r_{n}(e)\) of \(e=(u,v)\) using

\[p_{n}(e) =F_{}(h_{n}(u)+b_{n},h_{n}(v)+b_{n}),\] (9a) \[r_{n}(e) =F_{}(p_{n}(e)+b_{n}),\] (9b)

where \(b_{n}\) is the sinusoidal positional embedding of time step \(n\) that is widely used in Transformers (Vaswani et al., 2017), \(F_{}\) is the pooling function implemented as 2-layer MLPs followed by an elementwise maximum operator, and \(F_{}\) is the readout function implemented as 2-layer MLPs with a scalar output. Then the conditional distribution for edge decision is

\[Q(|e_{<n})(q_{n}), q_{n}=(\{r_{n}(e)\}_{e E_{n}}),\] (10)

where probability vector \(q_{n}^{|E_{n}|}\) for parametrizing \(Q(|e_{<n})\) is obtained by applying a softmax function to all the time guided edge features in equation (9b).

As the last step, we sample an edge \(e_{n} E_{n}\) from the discrete distribution in equation (10) and add the leaf node \(x_{n+1}\) to \(e_{n}\) as described in Section 3.1. This way, we update the ordinal tree topology from \(_{n}\) of rank \(n\) to \(_{n+1}\) of rank \(n+1\). We can repeat this procedure until an ordinal tree topology \(=_{N}\) of rank \(N\) is reached. The probability of \(\) then takes the form

\[Q_{}()=Q_{}(D)=_{n=3}^{N-1}Q_{}(e_{n}|e_{< n}),\] (11)

where \(D\) is the decision sequence and \(\) are the learnable parameters in the model. We call this autoregressive model for tree topologies ARTree, and summarize it in Algorithm 1. Note that equation (11) can also be used for tree topology probability evaluation where the decision sequence \(D=g^{-1}()\) is obtained from the decomposition process (Appendix C) that enjoys a linear time complexity (Lemma 1). Compared to SBNs, ARTree does not rely on heuristic features for parameterization and can provide distributions whose support spans the entire tree topology space as all possible decisions would have nonzero probabilities due to the softmax parameterization in equation (10). Although different taxa orders may affect the performance of ARTree, we find this effect is negligible in our experiments.

There exist other VI approaches for phylogenetic inference that also use unconfined models over the space of tree topologies. Moretti et al. (2021) proposed to sample tree topologies through subtree merging and resampling following CSMC (Wang et al., 2015), but employed a parametrized proposal distribution. Koptagel et al. (2022) proposed a novel multifurcating tree topology sampler named SLANTIS, which makes decisions on adding edges in a specific order based on a simply parameterized weight matrix and maximum spanning trees, and is integrated with CSMC to sample bifurcating tree topologies (\(\)-CSMC). Unlike these methods, ARTree employs GNNs for an autoregressive model that builds up the tree topology sequentially through leaf node addition operations, which not only allows fast sampling of trees, but also provides straightforward density estimation procedures. We demonstrate the advantage of ARTree over these baselines in the experiments.

## 4 Experiments

In this section, we test the effectiveness and efficiency of ARTree for phylogenetic inference on two benchmark tasks: tree topology density estimation (TDE) and variational Bayesian phylogenetic inference (VBPI). In all experiments, we report the inclusive KL divergence from posterior estimates to the ground truth to measure the approximation error of different methods. We will use "KL divergence" for inclusive KL divergence throughout this section unless otherwise specified. The code is available at https://github.com/tyuxie/ARTree.

Experimental setupWe perform experiments on eight data sets which we will call DS1-8. These data sets, consisting of sequences from 27 to 64 eukaryote species with 378 to 2520 site observations, are commonly used to benchmark phylogenetic MCMC methods (Hedges et al., 1990; Garey et al., 1996; Yang & Yoder, 2003; Henk et al., 2003; Lakner et al., 2008; Zhang & Blackwell, 2001; Yoder & Yang, 2004; Rossman et al., 2001; Hohna & Drummond, 2012; Larget, 2013; Whidden & Matsen IV, 2015). For the Bayesian setting, we focus on the joint posterior distribution of the tree topologies and the branch lengths and assume a uniform prior on the tree topologies, an i.i.d. exponential prior \((10)\) on branch lengths, and the simple JC substitution model (Jukes et al., 1969). For each of these data sets, we run 10 single-chain MrBayes (Ronquist et al., 2012) for one billion iterations, collect samples every 1000 iterations, and discard the first \(25\%\) samples as burn-in. These samples form the ground truth of the marginal distribution of tree topologies to which we will compare the posterior estimates obtained by different methods. All GNNs have \(L=2\) rounds in the message passing step. All the activation functions in MLPs are exponential linear units (ELUs) (Clevert et al., 2015). The taxa order is set to the lexicographical order of the corresponding species names in all experiments except the ablation studies. All the experiments are run on an Intel Xeon Platinum 9242 processor. All models are implemented in PyTorch (Paszke et al., 2019) and trained with the Adam (Kingma & Ba, 2015) optimizer. The learning rate is 0.001 for SBNs, 0.0001 for ARTree, and 0.001 for the branch length model.

### Tree topology density estimation

We first investigate the performance of ARTree for tree topology density estimation given the MCMC posterior samples on DS1-8. Following Zhang & Matsen IV (2018), we run MrBayes on each data set with 10 replicates of 4 chains and 8 runs until the runs have ASDSF (the standard convergence criteria used in MrBayes) less than 0.01 or a maximum of 100 million iterations. The training data

Figure 2: Performances of different methods for TDE on DS1. **Left/Middle**: Comparison of the ground truth and the estimated probabilities using SBN-EM and ARTree. A tree topology is marked as an outlier if it satisfies \(|()-()|>2\). **Right**: The KL divergence as a function of the sample size. The results are averaged over 10 replicates with one standard deviation as the error bar.

sets are formed by collecting samples every 100 iterations and discarding the first 25%. Now, given a training data set \(=\{_{m}\}_{m=1}^{M}\), we train ARTree via maximum likelihood estimation using stochastic gradient ascent. In each iteration, the stochastic gradient is obtained as follows

\[_{}L(;)=_{b=1}^{B}_{ {}} Q_{}(_{m_{b}}),\] (12)

where a minibatch \(\{_{m_{b}}\}_{b=1}^{B}\) is randomly sampled from \(\). We compare ARTree to SBN baselines including SBN-EM, SBN-EM-\(\), and SBN-SGA. For SBN-EM and SBN-EM-\(\), we use the same setting as previously done in Zhang & Matsen IV (2018) (see Appendix A for more details). In addition to these EM variants, a gradient based method for SBNs called SBN-SGA is considered, where SBNs are reparametrized with the latent parameters initialized as zero (see equation (18) in Appendix B) and optimized via stochastic gradient ascent, similarly to ARTree. For both ARTree and SBN-SGA, the results are collected after 200000 parameter updates with batch size \(B=10\).

The left and middle plots of Figure 2 show a comparison between ARTree and SBN-EM on DS1, which has a peaky posterior distribution. Compared to SBN-EM, ARTree provides more accurate probability estimates for tree topologies on the peaks and significantly reduces the large biases in the low probability region (the crimson dots). The right plot of Figure 2 shows the KL divergence of different methods as a function of the sample size of the training data. We see that ARTree consistently outperforms SBN based methods for all \(M\)s. Moreover, as the sample size \(M\) increases, ARTree keeps providing better approximation while SBNs start to level off when \(M\) is large. This indicates the superior flexibility of ARTree over SBNs for tree topology density estimation.

Table 1 shows the KL divergences of different methods on DS1-8. We see that ARTree outperforms SBN based methods on all data sets. The gradient based method SBN-SGA is better than SBN-EM on most of the data sets because SBN-EM is well initialized (Zhang & Matsen IV, 2018) and more likely to get trapped in local modes. From this point of view, the comparison between ARTree and SBN-SGA is fair because they both use a uniform initialization that facilitates exploration.

### Variational Bayesian phylogenetic inference

Our second experiment is on VBPI, where we compare ARTree to SBNs for tree topology variational approximations. Both methods are evaluated on the aforementioned benchmark data sets DS1-8. Following Zhang & Matsen IV (2019), we use the simplest SBN and gather the subsplit support from 10 replicates of 10000 ultrafast maximum likelihood bootstrap trees (Minh et al., 2013). For both ARTree and SBNs, the collaborative branch lengths are parametrized using the learnable topological features with the edge convolution operator (EDGE) for GNNs (Zhang, 2023). We set \(K=10\) for the multi-sample lower bound (4) and use the following annealed unnormalized posterior at the \(i\)-th iteration

\[p(,,;_{i})=p(|,)^{_{i}}p(,)\] (13)

    &  &  &  &  \\   & & & & SBN-EM & SBN-EM-\(\) & SBN-SGA & ARTree \\  DS1 & 27 & 1949 & 1228 & 0.0136 & 0.0130 & 0.0504 & **0.0045** \\ DS2 & 29 & 2520 & 7 & 0.0199 & 0.0128 & 0.0118 & **0.0097** \\ DS3 & 36 & 1812 & 43 & 0.1243 & 0.0882 & 0.0922 & **0.0548** \\ DS4 & 41 & 1137 & 828 & 0.0763 & 0.0637 & 0.0739 & **0.0299** \\ DS5 & 50 & 378 & 33752 & 0.8599 & 0.8218 & 0.8044 & **0.6266** \\ DS6 & 50 & 1133 & 35407 & 0.3016 & 0.2786 & 0.2674 & **0.2360** \\ DS7 & 59 & 1824 & 1125 & 0.0483 & 0.0399 & 0.0301 & **0.0191** \\ DS8 & 64 & 1008 & 3067 & 0.1415 & 0.1236 & 0.1177 & **0.0741** \\   

Table 1: KL divergences to the ground truth of different methods across 8 benchmark data sets. Sampled trees column shows the numbers of unique tree topologies in the training sets formed by MrBayes runs. The results are averaged over 10 replicates. The results of SBN-EM, SBN-EM-\(\) are from Zhang & Matsen IV (2018).

where \(_{i}=\{1.0,0.001+i/H\}\) is the inverse temperature that goes from 0.001 to 1 after \(H\) iterations. For ARTree, a long annealing period \(H=200000\) is used for DS6 and DS7 due to the highly multimodal posterior distributions on these two data sets (Whidden and Matsen IV, 2015) and \(H=100000\) is used for the other data sets. For SBNs, we set \(H=100000\) for all data sets. The Monte Carlo gradient estimates for the tree topology parameters and the branch lengths parameters are obtained via VIMCO (Mnih and Rezende, 2016) and the reparametrization trick (Zhang and Matsen IV, 2019) respectively. The results are collected after 400000 parameter updates.

The left plot in Figure 3 shows the evidence lower bound (ELBO) as a function of the number of iterations on DS1. Although the larger support of ARTree adds to the complexity of training for tree topology variational approximation, we see that by the time SBN based methods converge, ARTree based methods achieve comparable (if not better) lower bounds and finally surpass the SBN baselines in the end. We also find that using fewer particles \((K=5)\) in the training objective tends

   Data set & DS1 & DS2 & DS3 & DS4 & DSS & DS6 & DS7 & DS8 \\ \# Taxa & 27 & 29 & 36 & 41 & 50 & 50 & 59 & 64 \\ \# Sites & 1949 & 2520 & 1812 & 1137 & 378 & 1133 & 1824 & 1008 \\ GT trees & 2784 & 42 & 351 & 11505 & 1516877 & 809765 & 11525 & 82162 \\    } & SBN & 0.0707 & 0.0144 & 0.0554 & 0.0739 & 1.2472 & 0.3795 & 0.1531 & 0.3173 \\  & ARTree & **0.0097** & **0.0004** & **0.0064** & **0.0219** & **0.8979** & **0.2216** & **0.0123** & **0.1231** \\    & SBN & -7110.24(0.03) & -26368.88(0.03) & -33736.22(0.02) & -13331.83(0.03) & -8217.80(0.04) & **-6728.65(0.06)** & -37334.85(0.04) & -8653.05(0.05) \\  & ARTree & **-7110.09(0.04)** & **-26368.78(0.07)** & **-33736.17(0.08)** & **-13331.82(0.05)** & **-8217.68(0.04)** & **-6728.65(0.06)** & **-73334.84(0.13)** & **-8655.03(0.05)** \\    & SBN & -7108.69(0.02) & -26367.87(0.02) & -33735.26(0.02) & -13330.29(0.02) & -8215.42(0.04) & **-6725.33(0.04)** & **-37332.58(0.03)** & -8651.78(0.04) \\  & ARTree & **-7108.68(0.02)** & **-26367.86(0.02)** & **-33735.25(0.02)** & **-13330.27(0.03)** & **-8215.34(0.03)** & **-6725.33(0.04)** & **-37332.54(0.03)** & **-3651.73(0.04)** \\    } & \(\)-CSMC & -7290.367(2.33) & -30568.49(13.34) & -33798.06(6.62) & -13582.24(35.08) & -8367.51(8.87) & -7013.83(16.99) & N/A & -9209.18(18.03) \\  & SBN & **-7108.41(0.15)** & -26367.71(0.08) & **-3375.09(0.09)** & -13329.94(0.20) & -8214.62(0.40) & **-6724.73(0.43)** & -7331.97(0.28) & -8650.64(0.50) \\   & ARTree & -7108.41(0.19) & **-26367.71(0.07)** & **-33735.09(0.09)** & **-13329.94(0.17)** & **-8214.59(0.34)** & -6724.37(0.46) & **-3731.95(0.27)** & **-8650.61(0.48)** \\   

Table 2: KL divergences to the ground truth, evidence lower bound (ELBO), 10-sample lower bound (LB-10), and marginal likelihood (ML) estimates of different methods across 8 benchmark data sets. GT trees row shows the number of unique tree topologies in the ground truth. The marginal likelihood estimates are obtained via importance sampling using 1000 samples. The KL results are averaged over 10 independent trainings. For ELBO, LB-10, and ML, the results are averaged over 100, 100, and 1000 independent runs respectively with standard deviation in the brackets. The results of \(\)-CSMC are from Koptagel et al. (2022). For ELBO and LB-10, a larger mean is better; for ML, a smaller standard deviation is better1.

Figure 3: Performances of ARTree and SBN as tree topology variational approximations for VBPI on DS1. **Left**: the evidence lower bound (ELBO) as a function of iterations. The numbers of particles used in the training objective are in the brackets. The ARTree\({}^{*}\) method refers to ARTree without time guidance, i.e. \(b_{n}=0\) for all \(n\) in the readout step. **Middle**: variational approximations vs ground truth posterior probabilities of the tree topologies. **Right**: KL divergences across 50 random taxa orders. The KL divergence of SBNs is averaged over 10 independent trainings.

to provide larger ELBO. Moreover, time guidance turns out to be crucial for ARTree, as evidenced by the significant performance drop when it is turned off. As shown in the middle plot, compared to SBNs, ARTree can provide a more accurate variational approximation of the tree topology posterior. To investigate the effect of taxa orders on ARTree, we randomly sample 50 taxa orders and report the KL divergence for each order in the right plot of Figure 3. We find that ARTree exhibits weak randomness as the taxa order varies and consistently outperforms SBNs by a large margin.

Table 2 shows the KL divergences to the ground truth, evidence lower bound (ELBO), 10-sample lower bound (LB-10), and marginal likelihood (ML) estimates obtained by different methods on DS1-8. We find that ARTree achieves smaller KL divergences than SBNs across all data sets and performs on par or better than SBNs for lower bound and marginal likelihood estimation. Compared to SBNs, the ELBOs provided by ARTree tend to have larger variances, especially on DS2, DS3, and DS7, which is partly due to the larger support of ARTree that spans the entire tree topology space (see more discussions in Appendix E).

## 5 Conclusion

In this paper, we introduced ARTree, a deep autoregressive model over tree topologies for phylogenetic inference. Unlike SBNs that rely on hand-engineered features for parameterization and require pre-sampled tree topologies, ARTree is built solely on top of learnable topological features (Zhang, 2023) via GNNs which allows for a rich family of distributions over the entire phylogenetic tree topology space. Moreover, as an autoregressive model, ARTree also allows simple forward sampling procedures and straightforward density computation, which make it readily usable for tree topology density estimation and variational Bayesian phylogenetic inference. In experiments, we showed that ARTree outperforms SBNs on a benchmark of challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems, especially in terms of tree topology posterior approximation accuracy.