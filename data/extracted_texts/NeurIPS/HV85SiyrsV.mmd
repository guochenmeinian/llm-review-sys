# Online RL in Linearly \(q^{\pi}\)-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore

Online RL in Linearly \(q^{}\)-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore

 Gellert Weisz

Google DeepMind, London, UK

University College London, London, UK

&Andras Gyorgy

Google DeepMind, London, UK

Csaba Szepesvari

Google DeepMind, Montreal, Canada

University of Alberta, Edmonton, Canada

###### Abstract

We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear \(q^{}\)-realizability assumption, where it is assumed that the action-values of all policies can be expressed as linear functions of state-action features. This class is known to be more general than linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly \(q^{}\)-realizable MDPs where for any policy, all the actions have approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly \(q^{}\)-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorithm on the linear MDP hidden in the problem. The method returns an \(\)-optimal policy after \((H,d)/^{2}\) interactions with the MDP, where \(H\) is the time horizon and \(d\) is the dimension of the feature vectors, giving the first polynomial-sample-complexity online RL algorithm for this setting. The results are proved for the misspecified case, where the sample complexity is shown to degrade gracefully with the misspecification error.

## 1 Introduction

We consider reinforcement learning where an agent interacts in an online fashion with an environment modeled as a Markov decision process: The agent, observing a state, takes an action that results in a random next state and reward, the latter of which is to be maximized over time. To tackle large, possibly infinite state spaces, additional structure needs to be introduced to this problem. One such structure is a "feature-map" that maps state-action pairs to \(d\)-dimensional vectors (for some positive integer \(d\)) with the intention that a "good feature-map extracts important information from the state-action pairs so that learning with this extra information becomes tractable. An example is the case of _linear MDPs_(Jin et al., 2020), where the assumption is that both the transition and reward functions are linearly factorizable and their left factors are given by the feature-map. In contrast, value-based approaches, such as \(q^{}\)-_realizability_(Du et al., 2019; Lattimore et al., 2020) aim to model only the action-values with the features. In this work, we focus on the latter, a strictly more general setting than that of linear MDPs (Zanette et al., 2020, Proposition 4).

There are several sample-efficient algorithms discovering near-optimal policies in linear MDPs under various MDP access models and settings (online access: Jin et al. (2020); batch setting: Jin et al.

; reward-free setting: Wagenmaker et al. ). The best known sample-complexity bound for the online access model is achieved by the computationally inefficient algorithm of Zanette et al. , called Eleanor, which serves as a starting point of our work.

In this work we consider the setting of linearly \(q^{}\)-realizable MDPs. As opposed to linear MDPs, before this work, sample efficient solutions were only known for this case when the MDP is accessed through a simulator that implements some form of a state-reset function (Lattimore et al., 2020; Yin et al., 2022; Weisz et al., 2022) (Table 1). In this work we resolve an open problem by Du et al. , and show that having access to a state-reset is not essential in this setting. To this end, we present SkippyEleanor (Algorithm 1) and a corresponding theorem (Theorem 4.1) that shows that SkippyEleanor, which uses online interactions only, is a provably sample-efficient solution to this problem. The rest of this paper is organized as follows. In Section 2 we introduce the basic definitions. In Section 3 we give an insight into the difference between linear \(q^{}\)-realizability and linear MDPs, which motivates our approach. In Section 4 we describe our algorithm and the most important technical tools we discovered for its analysis. Notably, in Section 4.2 we establish a rich structure inherent in \(q^{}\)-realizable MDPs, which acts as the technical foundation to this work, and may be of independent interest. Finally, Section 5 gives a summary of the proof of our main result (Theorem 4.1), before concluding with some notes on future work in Section 6.

## 2 Preliminaries

For a linear subspace \(X\) of \(^{d}\), let \(_{X}\) denote the orthogonal projection matrix onto \(X\). Throughout we fix \(d^{+}\). For \(L>0\), let \((L)=\{x^{d}:\|x\|_{2} L\}\) denote the \(d\)-dimensional Euclidean ball of radius \(L\) centered at the origin, where \(\|\|_{2}\) denotes the Euclidean norm. Let PD denote the set of positive definite matrices in \(^{d d}\). We write \(a_{e}b\) for \(a,b,\) if \(|a-b|\). Let \(\{B\}\) be the indicator function of a boolean-valued (possibly random) \(B\) taking value \(1\) if \(B\) is true and \(0\) if false. Let \(_{1}(X)\) denote the set of probability distributions supported on set \(X\). The rest of our notation is standard, but described in Appendix A for completeness.

For the setting of episodic finite horizon RL, with horizon \(H\), a finite-action Markov decision process (MDP) describes an environment for sequential decision-making. It is defined by a tuple \((,[],P,)\) as follows. The state space \(\) is split across stages: \(=(_{t})_{t[H]}\) with \(_{1}=\{s_{1}\}\) for some designated initial state \(s_{1}\). Without loss of generality, we assume the \((_{t})_{t[H]}\) are disjoint sets. We define the function stage \(:[H]\) as \((s)=t\) if \(s_{t}\). We consider finite action spaces of size \(\) for some \(^{+}\), and without loss of generality, define the set of actions to be \([]:=\{1,,\}\). The transition kernel is \(P:(_{t[H-1]}_{t})[]_{1}()\), with the property that transitions happen between successive stages, that is, for any \(t[H-1]\), state \(s_{t}_{t}\), and action \(a[]\), \(P(s_{t},a)_{1}(_{t+1})\). The reward kernel is \(:[]_{1}()\). An agent interacts sequentially with this environment in an episode lasting \(H\) steps by taking some action \(a[]\) in the current state. The environment responds by transitioning to some next-state according to \(P\), and giving a reward in \(\) according to \(\).1

We describe an agent interacting with the MDP by a _policy_\(\), which, to each history of interaction (including states, actions and rewards) assigns a probability distribution over the actions. Policies where this distribution only depend on the last state in the history are called _memoryless_, and these are identified with elements of the set \(=\{:_{1}([])\}\). Using a policy \(\), starting at some state \(s\) in an MDP induces a probability distribution over histories, which we denote by \(_{,s}\). For

   &  &  \\ MDP class & \(()\) sample & \(()\) compute & \(()\) sample & \(()\) compute \\  Linear MDP &  \\  \(q^{}\)-realizable MDP & **This work** & Open problem & Yin et al.  \\  

Table 1: Comparison of efficiency results for linear MDPs and \(q^{}\)-realizable MDPs under online RL and planning with a simulator. This work establishes that \(q^{}\)-realizable MDPs are also sample efficiently solvable under online RL. The computational complexity of this problem remains open.

any \(a[]\), \(_{,s,a}\) is the distribution over the histories when first action \(a\) is used in state \(s\), after which policy \(\) is followed. \(_{}\) is the expectation operator corresponding to a distribution \(_{}\) (e.g., \(_{,s}\) is the expectation with respect to \(_{,s}\)). The state- and action-value functions \(v^{}\) and \(q^{}\) are defined as the expected total reward within the first episode while \(\) is used:

\[v^{}(s)=*{}_{,s}_{u=(s)} ^{H}R_{u}s q^{}(s,a)=*{}_{,s,a}_{u= (s)}^{H}R_{u}s,\,a[].\]

Let \(^{}\) be an optimal policy, satisfying \(q^{^{}}(s,a)=_{}q^{}(s,a)=_{}q^{}(s,a)\) for all \((s,a)[]\). Let \(q^{}(s,a)=q^{^{}}(s,a)\) and \(v^{}(s)=_{a^{}[]}q^{}(s,a)\) for all \((s,a)\).

## 3 From linear \(q^{}\)-realizability to linear MDPs

As described in the introduction, we endow our MDP with a feature map \(:[](L_{1})\) for some \(L_{1}>0\). For reference, we start with a definition of linear MDPs with a parameter norm bound \(L_{2}>0\), formalizing that the transition kernel and the expected rewards are approximately linear functions of the features:2

**Definition 3.1**.: _[\(\)-approximately linear MDP] For any \( 1\), an MDP is a \(\)-approximately linear MDP if \(\) there exists \(_{1},,_{H}(L_{2})\) such that for any \(h[H]\) and \((s,a)_{h}[]\), \(_{(s,a)}\,R-(s, a),_{h}\) and \(\) for any \(f:[0,H]\) and \(h[H-1]\), there exists \(^{}_{h}(L_{2})\) such that for all \((s,a)_{h}[]\), \(_{^{} P(s,a)}\,f(S^{})- (s,a),^{}_{h}\)._

A key consequence of the linear MDP assumption is that the _inherent Bellman error_

\[_{_{h+1}(L_{2})}_{_{h}(L_{2})} _{(s,a)_{h}[]}*{ }_{(s,a),^{} P(s,a)} R(s,a)+_{a^{}[]}\,(S^{},a^{}), _{h+1}-(s,a),_{h}\,\,,\]

scales with the misspecification \(\). This property is also referred to as the _closedness to the Bellman operator_, and is a crucial component in the analysis of approximation errors for algorithms tackling linear MDPs.

In this work we consider a weaker linearity assumption where we only assume that the action-value functions are approximately linear:

**Definition 3.2** (\(q^{}\)-realizability: uniform linear function approximation error of value-functions).: _Given an MDP, the uniform value-function approximation error (or misspecification) induced by a feature map \(:[](L_{1})\), over a set of parameters in \((L_{2})\) is_

\[=_{}_{h[H]}_{^{(h)}(L_{2})} _{(s,a)_{h}[]}q^{}(s,a)- (s,a),^{(h)}\,\,.\]

_For the MDP and the corresponding feature map, for all \(h[H]\) fix any \(_{h}:(L_{2})\) mapping each memoryless policy \(\) to its "parameter", such that_

\[q^{}(s,a)_{}(s,a),_{h}() ,\,s_{h},\,a[]\,\,.\] (1)

_The set of all parameters \(_{h}(L_{2})\) for a stage \(h[H]\) is given by \(_{h}=\{_{h}()\;:\;\}\,\)._

Note that \(_{h}\) satisfying Eq. (1) always exist (Weisz et al., 2022, Appendix C). We focus on the feasible regime where \(\) is polynomially small in the relevant parameters. Specifically, we assume that \(\) is bounded according to Eq. (21). The main problem of interest in this work is the following:

**Problem 3.3** (informal).: _For any \(,>0\) and any MDP with corresponding uniform value-function approximation error \(\), derive an algorithm that, with probability at least \(1-\), will find an \(\)-optimal policy (i.e., a policy \(\) such that \(v^{}(s_{1}) v^{}(s_{1})-\)) by interacting with the MDP online for \(T\) steps with \(T\) bounded by a polynomial function of \((d,H,^{-1},\,^{-1}, L_{1}, L_{2})\). That the interaction with the MDP is online means that it is only possible to observe the features corresponding to the current state, and to take an action and subsequently observe the resulting reward and next state, which then becomes the current state. We consider the fixed horizon episodic setting, that is, the next state is reset to the initial state \(s_{1}\) after every \(H\) steps._Algorithms developed for linear MDPs are not directly applicable to Problem 3.3 when the MDP is only \(q^{}\)-realizable: While a linear MDP is also \(q^{}\)-realizable, a \(q^{}\)-realizable MDP may be neither a linear MDP, nor one with a low inherent Bellman error (Zanette et al., 2020). As an illustrative example, Fig. 1, left shows an MDP that is \(q^{}\)-realizable but not linear. To see this, observe that the features for both actions in \(s_{1}\) are identical, but their transitions and rewards are not. As illustrated in the figure however, if we _skip_ over the red states (with identical actions) by taking the first action on them and summing up the rewards received until we reach a black state, we arrive at a linear MDP. This serves as the main intuition behind our work: the red states have no bearing on action-values, so they can be skipped, and the resulting MDP is linear.

More generally, we can define the _range_ of any state as the maximum possible difference in action-value that the choice of action in that state can make:

\[(s)=_{_{(s)}}_{ i,j[]}(s,i,j),h[H],s_{h}\,\] (2)

where \((s,i,j)=(s,i)-(s,j)\) is the notation for feature differences. Clearly, the choice of action in low-range states is not too important, as

\[v^{}(s)-q^{}(s,a)(s)+2a[].\] (3)

Not only are the action choices in low-range states unimportant for the task of finding a near-optimal policy for the MDP, these choices can affect transitions and rewards in a nonlinear way. Interestingly, the existence of low-range states is the reason why \(q^{}\)-realizable MDPs are not necessarily linear, as shown by the next result (proved in Appendix C), which follows easily from Lemma 4.7.

**Proposition 3.4**.: _Consider an MDP with uniform value-function approximation error \( 0\). If there are no states \(s\) with \((s)<\) for some \(>0\), then the transitions and rewards of the MDP are linear (Definition 3.1) with misspecification scaling with \(\), and parameter norms scaling inversely with \(\)._

Our approach.The above result immediately offers a strategy to learn under the (linear) \(q^{}\)-realizability assumption. Assuming access to an oracle that can determine whether or not \((s)<\) for any state \(s\), the MDP could be "converted" to one that has no low-range states but has near-identical state and action-value functions of any policy (compared to the original MDP), by skipping over low-range states (by executing an arbitrary action) until a state with a range at least \(\) is reached. We will call such a multi-state transition a _skippy step_ and refer to such a policy as a _skippy policy_. The reward presented for a skippy step is the cumulative reward over the skipped states. When the oracle is correct, the new MDP is a linear MDP, allowing techniques such as Eleanor to efficiently learn a near-optimal policy. This conversion argument is part of the intuition of our method, but it is not strictly part of the proof, so we defer the details to Appendix C. The only missing piece for solving the general case, Problem 3.3, is learning an oracle that can suggest when to skip over a state, and combining it with the learning algorithm for the linear MDP. This general approach leads to our algorithm, SkippyEleanor, which runs a modified version of Eleanor with guessed oracles. During the algorithm, we detect when an incorrect oracle leads to suboptimal results, and refine the oracle accordingly. The details of the algorithm are explained in the next section.

## 4 Algorithm

In this section we present our main results following our plan outlined above. We first give Algorithm 1, along with a high-level overview of the algorithm; the details are explained throughout the section. The parameters of the algorithm are presented in Appendix B.

Figure 1: **Left:** MDP with deterministic transitions and rewards (edges are labeled with action/reward). **Right:** The same MDP with the red “low-range” states “skipped” over. \((s_{1},)=(1),(s_{3},)=(0.5),(,)=(0)\) otherwise. Both MDPs are \(q^{}\)-realizable, but only the right MDP is linear.

For every stage \(h[H]\), the algorithm keeps a progressively refined estimate of the geometry of the parameter space \(_{h}\), by maintaining an ever shrinking ellipsoid enclosing \(_{h}\). This ellipsoid is parametrized by an 'inverse covariance matrix'-like quantity \(Q_{h}\), determined by \(}(d)\) vectors, which guarantees \(_{_{h}_{h}}\|_{h}\|_{Q_{h}^{-2}}=}()\). Looking at the definition of range in Eq. (2), it is clear that the smaller the ellipsoid becomes, the better estimate we can give for the ranges.

Given some data collected so far and \((Q_{h})_{h[H]}\), SkippyEleanor computes optimistic estimates of the action-values by calculating an optimistic policy parameter \(\), as well as a guess \(\) to a near-optimal design which is used to estimate the range for the states (due to technical reasons, \(\) will guess a near-optimal design for the transformed parameter space \(Q_{h}^{-1}_{h}\)).

Data is collected by running stochastic versions of skipped policies on the MDP, where the states to be skipped over are determined based on the range estimates; when a state is skipped, an action is selected using a deterministic policy \(^{0}\) that always chooses the first action in every state. To ensure that the estimation problem is smooth in terms of \(\), we use a smoothed version of skipped policies, where states are skipped randomly, and the probability of skipping is larger for states with lower ranges, while high-range states are never skipped. Similarly to Eleanor, we aim to estimate the action-value function of a state-action pair by adding the estimated one-step reward to the estimated value-function of the next state. However, unlike Eleanor, we would like to do this in the reduced MDP, where the low-range states that are skipped over are removed (and the corresponding transitions are replaced by skipped steps). Since we do not know these states in advance, we run exploratory policies that skip over next states starting from any state: namely,we run SkippyPolicy\((,,k)\) for all \(k[H]\) with a maximum number of unskipped states \(k\) (Phase I), and once this is skip budget is exhausted, all remaining states are skipped over by rolling out \(^{0}\) (Phase II), which ensures that we collect enough data at every stage of the MDP to be able to estimate the one-skippy-step reward of any skipping mechanism. Compared to Eleanor, this introduces an additional loop in Line 6 of SkippyEleanor; see Appendix D for additional details. For any execution, SkippyPolicy maintains a stage-mapping function \(p\), which, for any stage \(h\) of the trajectory in the reduced MDP gives the stage index in the original MDP. In other words, \(p(j)\) is the stage of the landing state of the \(j^{th}\) skippy step.

Finally, we check if the data collected is consistent with our estimates \(\) and \(\), by calculating the maximal discrepancy of the estimates of the action-value difference at the last non-skipped state of \(^{mk}=(,,k)\) and that of the fixed skipping policy \(^{0}\) in different directions in the parameter space. If the discrepancy is too large for any \(k\), we add the discrepancy-maximizing direction to \(Q\) and throw away the data collected in this (i.e., the \(m^{th}\)) iteration; this is achieved by reducing the iteration counter \(m\) by \(1\). On the other hand, if the discrepancy is small enough, we can guarantee that the gap between the value of \(^{mH}\) and \(v^{}(s_{1})\) scales with how much new information we collected, thus the algorithm can terminate returning this policy if this term is sufficiently small (which it eventually has to be).

The following theorem shows that with high probability, SkippyEleanor finds a near-optimal policy after polynomially many interactions with the MDP. Rhe proof sketch is provided in Section 5, while our method and proof strategy is explained from the perspective of Eleanor in Appendix D.

**Theorem 4.1**.: _With probability at least \(1-\), SkippyEleanor interacts with the MDP for at most \(}(H^{11}d^{7}/e^{2})\) many steps, before returning a policy \(\) that satisfies \(v^{}(s_{1}) v^{}(s_{1})+\)._

### Preconditioning: the enclosing ellipsoid

In this section we give the technical details about the effects of using the matrix \(Q_{h}\) describing an enclosing ellipsoid for \(_{h}\) (see Lemma 4.3) as preconditioning the features.

**Definition 4.2** (Valid preconditioning).: \(Q=(Q_{h})_{h[H]}\) _is a valid preconditioning matrix sequence if for all \(h[H]\)_

\[Q_{h}=(L_{2}^{-2}I+_{v C_{h}}vv^{})^{-1/2}\] (4)

_for some sequence \(C_{h}=(v_{1},,v_{n})\) of vectors in \(^{d}\) such that for all \(1 i n\),_

\[_{_{h}}|,v_{i}| 1 \|(L_{2}^{-2}I+_{j=1}^{i-1}v_{j}v_{j}^{})^{- {1}{2}}v_{i}\|_{2}^{2}\|v\|_{2} L _{3}\,,\] (5)

_where \(L_{3}\) is some fixed polynomial of the problem parameters \((d,H,^{-1},^{-1}, L_{1}, L_{2})\). (see Eq. (35) for its precise value)._

_For a valid preconditioning \(Q\) and some \(h[H]\), let \(Z(Q,h)\) be the linear subspace spanned by those eigenvectors of \(Q\) whose corresponding eigenvalues are at least \(L_{3}^{-2}\). Let \(_{Z(Q,h)}\) be the orthogonal projection matrix onto this subspace._

Sometimes it will be convenient to _precondition_ the features and parameters so that the enclosing ellipsoid is transformed to a ball of controlled radius (as Lemma 4.3 will show). To this end, introduce for all \(h[H]\) and \((s,a,b)_{h}[][]\) the following:3

\[_{Q}(s,a)=Q_{h}(s,a),_{Q}(s,a,b)=Q_{h} (s,a,b)\] (6) \[_{h}^{Q}()=Q_{h}^{-1}_{h}(),_{h}^{Q }=\{_{h}^{Q}()\;:\;\}=\{Q_{h}^{-1}\;: \;_{h}\}\] \[^{}(s,a)=(s,a),_{h}()= _{Q}(s,a),_{h}^{Q}()\,.\]

The next lemma (proved in Appendix F) shows that for all \(h[H]\), \(Q_{h}\) defines an enclosing ellipsoid for \(_{h}\); that is, \(_{h}\{:\|\|_{Q_{h}^{-2}}+1}\}\).

**Lemma 4.3**.: _Let \(d_{1}=4d(1+6L_{3}^{4}L_{2}^{4})=(d)\). Then, for any valid preconditioning \(Q\) and \(h[H]\),_

\[_{_{h}}\|\|_{Q_{h}^{-2}}=_{_{h}^{0}} \|\|_{2}+1}\,.\]

Clearly, every time a new vector is added to \(C_{h}\), the enclosing ellipsoid \(\{:\|\|_{Q_{h}^{-2}}+1}\}\) shrinks (as a positive semidefinite matrix is added to \(Q_{h}^{-2}\)). The following lemma (also proved in Appendix F) uses an elliptical potential argument to bound the number of times this can happen.

**Lemma 4.4**.: _For any valid preconditioning \(Q\), for all \(h[H]\), the length of sequence \(C_{h}\) corresponding to \(Q_{h}\) according to Definition 4.2 is at most \(d_{1}\)._

Near-optimal design for \(_{h}^{Q}\).As \(Q_{h}\) only provides an enclosing ellipsoid for \(_{h}\), we introduce an (unknown) ellipsoid that aligns better with \(_{h}^{Q}\). For all \(h[H]\), fix a set \(G_{h}^{Q}\) of policies of size \(d_{0}:=4d(d)+16\), together with a probability distribution \(_{h}^{Q}\) on \(G_{h}^{Q}\), such that \((G_{h}^{Q},_{h}^{Q})\) is a near-optimal design for \(_{h}^{Q}\) (i.e., satisfying Definition F.1). The existence of such a near-optimal design follows from [13, Part (ii) of Lemma 3.9].

We apply \(G_{h}^{Q}\) to define a cruder version of range that depends only on a small set of policies, and can therefore be succinctly parametrized to inform SkippyPolicy:

\[_{Q}(s)=_{ G_{h}^{Q}}_{i,j[]} (s,i,j),_{h}()h[H],s_{h}\,.\] (7)

\(_{Q}\) is easy to estimate, and can be used to bound the range function (proved in Appendix F):

**Proposition 4.5**.: _For all \(s\) and \(Q^{H}\), \((s)_{Q}(s)\)._

### Linearly realizable functions

\(q^{}\)-realizability (Definition 3.2) implies the linearity of many more functions than the action-value functions. In this section we characterize an interesting set of such functions, whose (approximate) linearity plays a crucial role in our algorithm and analysis, as their parameters can be conveniently estimated by least squares using the features. We rely on functions \(f:_{h}\) (for some \(h[H]\)) being small for all states, relative to the states' \(_{Q}\)-value:

**Definition 4.6**.: _For any \(h[H]\), \(f:_{h}\) is \(\)-admissible for some \(>0\) if for all \(s_{h}\), \(|f(s)|_{Q}(s)/\)._

The key observation is that expected (admissible) \(f\) values are linearly realizable.

**Lemma 4.7** (Admissible-realizability).: _If \(f:_{h}\) is \(\)-admissible then it is realizable, that is, for all \(t[h-1]\) and \(\), there exists some \(^{d}\) with \(\|\|_{2} 4d_{0}L_{2}/\) such that for all \((s,a)_{t}[]\),_

\[*{}_{,s,a}f(S_{h})_{_{0}}(( s,a),)_{0}=5d_{0}/.\]

The proof relies on constructing a set of policies that at states \(s_{h}\) take a higher value action as opposed to a lower one with a certain probability, configured such that the expected action-value difference of some pairs within the set of policies is (approximately) proportional to \(f(s)\). Thus, a linear combination of the action-values of policies in this set are also (approximately) proportional to \(f(s)\). The statement of the lemma then follows from setting \(\) to the corresponding linear combination of the policies' parameters. The full proof is presented in Appendix G.

Next, we define matrix-valued functions with a special admissibility guarantee even when the underlying scalar-valued function does not satisfy any non-trivial admissibility criterion. We introduce a _guess_ on the near-optimal design parameters that define \(_{Q}\) (Eq. (7)) for some valid preconditioning \(Q\):

**Definition 4.8**.: _For \(h[2:H]\), fix some arbitrary order of the policies in the set \(G_{h}^{Q}\) (recall that this set is the support of the near-optimal design for \(_{h}^{Q}\)). Let the parameter of the \(i^{th}\) policy in \(G_{h}^{Q}\) be \(_{h}^{i}\) for \(i[d_{0}]\). Call a "guess" of these parameters \(=(_{h})_{h[2:H]}=(_{h}^{i})_{h[2:H],i[d_{0}]}\) "valid", if for all \(h[2:H],i[d_{0}]\), \(_{h}^{i}(+1})\). Let the set of valid guesses be \(\).4 By Lemma 4.3, \((_{h}^{i})_{h[2:H],i[d_{0}]}\), that is, it is a valid guess, and we call this the "correct" guess._From a guess \(=(_{h}^{i})_{h[2:H],i[d_{0}]}\) we can calculate corresponding guesses of the \(_{Q}\)-values:

\[_{Q}^{}(s)=_{k[d_{0}]}_{i,j[]}_{Q}(s,i,j),_{(s)}^{k} h[2:H],s_{h}\;.\]

Note that for any \(h[2:H]\) and \(s_{h}\), \(_{Q}^{}(s)=_{Q}(s)\) if \(\) is the correct guess for stage \(h\).

Let \(_{Q}(s)\) be the unit vector in the direction of the largest feature difference between actions in \(s\) and the zero vector if all feature vectors are the same (see Eq. (27) for a formal definition). Then, for any \(\), \(h[2:H]\), and \(f:_{h}[-H,H]\), let

\[(s)=_{Q}(s)_{Q}(s)^{}\{1, _{Q}^{}(s)H}{}\} f(s)s_{h}\;.\]

For such \(:_{h}^{d d}\), we adopt the notation \(a^{}b\) for any \(a,b^{d}\) to denote the function \(s_{h} a^{}(s)b\), and similarly, \(()\) to denote the function \(s_{h}((s))\).

Let \(_{\|(Q,h)}\) be the projection matrix onto the linear subspace spanned by those eigenvectors of the design matrix \(V(G_{h}^{Q},_{h}^{Q})\) (defined in Eq. (25)) whose corresponding eigenvalues are at least \(\) (for some \(>0\) specified in Appendix B). Intuitively, this is the subspace where \(_{h}^{Q}\) has a sufficiently large width. Let \(_{(Q,h)}\) be the projection to the orthogonal complement subspace. For any \(v^{d}\), we write \(v_{\|(Q,h)}\) and \(v_{(Q,h)}\) for \(_{\|(Q,h)}v\) and \(_{(Q,h)}v\), respectively.

We are now ready to state our special admissibility guarantee, which is proved in Appendix G. Let \(=}(/(d^{1.5}H^{2}))\) be as in Eq. (16).

**Lemma 4.9**.: _For any \(h[2:H]\), \(\), any function \(\) constructed as above from some \(f:_{h}[-H,H]\), and any \(v,w(1)\), \(v_{\|(Q,h)}^{}\)fw is \(\)-admissible. Furthermore, if \(=(_{h}^{i})_{h[2:H],i[d_{0}]}\) (the correct guess), \(()\) is also \(\)-admissible._

### Least-squares targets and Optimization Problem 4.10

Recall that SkippyEleanor estimates action-values of states by first adding the estimated one-step reward and the estimated value-function of the next state in the reduced MDP (where low-range states are skipped). Due to the linearity of \(q^{}\)-values, these can be used as target variables of a least-squares estimator to estimate the policy parameters. This estimator is only guaranteed to be accurate if the right (low-range) states are skipped; otherwise, we will argue in Section 4.4 that a discrepancy is detected and it is handled by changing the preconditioning \(Q\). Finally, to ensure optimism, we select parameter estimates that lead to the largest estimated policy values. The whole estimation process leads to Optimization Problem 4.10, which we define in this section along with the functions that it uses as least-square targets. Each estimation is for a particular stage \(h\) and may use the estimates \(_{i}\) of Optimization Problem 4.10 for stages \(i>h\). In this subsection, we consider the \(m^{}\) iteration of the optimization called by SkippyEleanor, and consider \(Q\) fixed. As a shorthand, we introduce the following notation for \(l[m],j[n],k[H]\):

\[(lkj)=p^{1kj}(k) of Algorithm , and}\] \[S_{(k)}^{lkj}=S_{p^{1kj}(k)}^{lkj},\ A_{ (k)}^{lkj}=A_{p^{1kj}(k)}^{lkj},\ R_{(k)}^{ lkj}=R_{p^{1kj}(k)}^{lkj},\ _{t}^{lkj}=(S_{t}^{lkj},A_{t}^{lkj}),\ _{ (k)}^{lkj}=(S_{p(k)}^{lkj},A_{(k)}^{lkj} )\;.\]

We collect the set of \((l,k,j)\) tuples for which the \(k^{}\) skippy step lands at stage \(t\), for \(t[H]\), as

\[^{m}(t)=\{(l,k,j):\,l[m-1],j[n],k[H],(lkj)=t\}\]

Note in particular that here \(l[m-1]\), so \(^{m}\) only considers data collected prior to iteration \(m\).

To estimate the parameters \(\) and \(\), we consider (simulated) trajectories of SkippyPolicy starting from stage \(t\). For simplicity, we suppress the dependence of quantities on \(\) and \(\), which will be brought back later. The skipping probability \(1-\), the policy \(^{+}\) (to be also used in SkippyPolicy), and corresponding clipped action-value estimates are defined as

\[(s) =\{1,_{Q}^{}(s)H}{ }\} (s)>1,\,\,\,(s_{1})=1;\] (8) \[^{+}(s_{i}) =*{arg\,max}_{a[]}(s_ {i},a),_{i}, C(s_{i})=_{[0,H]}(s_{i},^{+}(s_{i})), _{i}.\]

[MISSING_PAGE_FAIL:9]

this by another indicator \(c^{j}_{k\,i}\) (defined in Appendix B) that requires the data-point's least-squares uncertainty term to be sufficiently low, and the prediction non-negative (the contribution of the rest of the data will be analyzed separately). Next, we define the least-squares solution for estimating the matrix-valued \(F\), as well as the empirical average prediction and realization of \(F\) on the data collected in the \(m^{}\) round. For any \(i[2:H]\), \(k[i-1]\) (recall that \(\) denotes the tensor product):

\[^{ i}_{G\,}& =X_{mt}^{-1}_{lkj^{m}(t)}^{klj}_{k\,} F_{\,}(S^{lkj}_{i},,R^{lkj}_{H}) \\ & y^{ki}_{G\,}&=_{j[n ]}c^{j}_{ki}^{mkj}_{p(k)}{}^{}^{p(mkj),i}_{ \,}^{ki}_{\,}= _{j[n]}c^{j}_{ki}F_{\,}(S^{mkj}_{i},,R^{mkj}_ {H})\] (12)

In Appendix E.1, it is established via the usual least-squares analysis techniques and covering arguments, that with high probability the norm of the product of the matrix \(y^{ki}_{G\,}-^{ki}_{\,}\) and the projection matrix \(_{\|\{Q,i\}}\) is small (Lemmas E.2 and E.3). The next optimization problem tests if this is true in arbitrary directions:

**Optimization Problem 4.12** (Consistency check).: _Input: \((,)\)_

\[*{arg\,max}_{k[H-1],\,i[k+1:H],\,v^{d}:\|v\|_ {2}=1}v^{}(y^{ki}_{\,}-^{ki}_{\, })v\]

Lemma E.1 shows that the projection \(w=_{Z(Q,t)}\,v\) is close to \(v\), where \(v\) is the outcome of Optimization Problem 4.12. Also, Lemmas E.1-E.3 imply that if the consistency check fails (i.e., Line 13 is executed because the value of Optimization Problem 4.12 is large), then \(w\) aligns well with the subspace \(_{(Q,i)}\) projects to, and therefore \(Q\) stays a valid preconditioning after appending \(w\) to the list of values \(Q\) is calculated from (Lemma E.4). Thus, \(Q\) is always a valid preconditioning.

## 5 Proof overview

The proof of Theorem 4.1 is presented in Appendix E. It is composed of the following main steps: First, we bound the number of times the consistency check can fail (i.e., Line 13 is executed) by Lemma 4.4. Combining this with Lemma E.5, an elliptical potential argument bounding the number of times the average uncertainty can be large (these are the only two ways that the main iteration can continue) implies a sample-complexity result for Skipyeleanor (Corollary E.6). Having limited the number of times the consistency check can fail, we derive guarantees regarding the performance of the policy returned by the algorithm: Via an induction argument (Lemma E.8) we show Corollary E.9, which shows that with high probability the difference between the optimization value of Optimization Problem 4.10, \(C_{,\,}(s_{1})\) and \(v^{^{mH}}\) scales with the average uncertainty term \(_{i=1}^{H}_{k}^{m}\). Thus, they are close when Skipyeleanor returns in Line 17. This is complemented with the _optimism_ property proved in Lemma E.10, stating that the optimization value \(C_{,\,}(s_{1})\) is close to \(v^{}(s_{1})\). Combined, this proves Theorem 4.1.

## 6 Future work

Since we are not aware of a computationally efficient implementation of SkippyEleanor, it remains an open question whether the problem of learning near-optimal policies from online interactions with a \(q^{}\)-realizable MDP (Problem 3.3) is possible if the computational resources as well as the sample complexity are bounded by a polynomial in the relevant parameters. One approach is to replace Eleanor with LSVI-UCB as the underlying algorithm, as the latter, despite having worse sample complexity, has a computationally efficient implementation (Jin et al., 2020). The challenge is to compute the optimal solution for the parameter \(\) in Optimization Problem 4.10. This parameter interacts with the least-squares targets in a highly nonlinear way. We have been unable to derive a computationally efficient approximation that has an additive instead of a multiplicative approximation error (additive errors increase linearly in \(H\), while multiplicative errors increase exponentially). Alternatively, it may be possible to show a computational hardness result for Problem 3.3 by e.g., reducing it to the satisfiability problem. These are left for future work. Our work on the realizability of auxiliary functions (Section 4.2) may be of independent interest for designing provably efficient algorithms for related problem settings, e.g., the setting of \(q^{}\)-realizability in batch RL, where the data collection is not controlled.