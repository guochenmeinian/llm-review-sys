# Average gradient outer product as a mechanism for deep neural collapse

Daniel Beaglehole

Equal contribution. Correspondence to: Daniel Beaglehole (dbeaglehole@ucsd.edu), Peter Sukenik (peter.sukenik@ista.ac.at). UC San Diego Institute of Science and Technology Austria

 Peter Sukenik

Space-Time-series (NCS), 2019.

Marco Mondelli

Institute of Science and Technology Austria

Mikhail Belkin

UC San Diego Institute of Science and Technology Austria

###### Abstract

Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs). Though the phenomenon has been measured in a variety of settings, its emergence is typically explained via data-agnostic approaches, such as the unconstrained features model. In this work, we introduce a data-dependent setting where DNC forms due to feature learning through the average gradient outer product (AGOP). The AGOP is defined with respect to a learned predictor and is equal to the uncentered covariance matrix of its input-output gradients averaged over the training dataset. The Deep Recursive Feature Machine (Deep RFM) is a method that constructs a neural network by iteratively mapping the data with the AGOP and applying an untrained random feature map. We demonstrate empirically that DNC occurs in Deep RFM across standard settings as a consequence of the projection with the AGOP matrix computed at each layer. Further, we theoretically explain DNC in Deep RFM in an asymptotic setting and as a result of kernel learning. We then provide evidence that this mechanism holds for neural networks more generally. In particular, we show that the right singular vectors and values of the weights can be responsible for the majority of within-class variability collapse for DNNs trained in the feature learning regime. As observed in recent work, this singular structure is highly correlated with that of the AGOP.

## 1 Introduction

How Deep Neural Networks (DNNs) learn a transformation of the data to form a prediction and what are the properties of this transformation constitute fundamental questions in the theory of deep learning. A promising avenue to understand the hidden mechanisms of DNNs is Neural Collapse (NC) (Papyan et al., 2020). NC is a widely observed structural property of overparametrized DNNs, occurring in the terminal phase of gradient descent training on classification tasks. This property is linked with the performance of DNNs, such as generalization and robustness (Papyan et al., 2020; Su et al., 2023). NC is defined by four properties that describe the geometry of feature representations of the training data in the last layer. Of these, in this paper we focus on the following two, because they are relevant in the context of deeper layers. The _within-class variability collapse_ (NC1) states that feature vectors of the training samples within a single class collapse to the common class-mean. The _orthogonality or simplex equiangular tight frame property_ (NC2) states that these class-means form an orthogonal or simplex equiangular tight frame. While initially observed in just the final hidden layer, these properties were measured for the intermediate layers of DNNs as well (Rangamani et al., 2023; He and Su, 2022), indicating that NC progresses depth-wise. This has lead to the study of Deep Neural Collapse (DNC), a direct depth-wise extension of standard NC (Sukenik et al., 2023).

The theoretical explanations of the formation of (D)NC have mostly relied on a data-agnostic model, known as _unconstrained features model_ (UFM) (Mixon et al., 2020; Lu and Steinerberger, 2022). This assumes that DNNs are infinitely expressive and, thus, optimizes for the feature vectors in the last layer directly. The deep UFM (DUFM) was then introduced to account for more layers (Sukenik et al., 2023; Tirer and Bruus, 2022). The (D)UFM has been identified as a useful analogy for neural network collapse, as (D)NC emerges naturally in this setting for a variety of loss functions and training regimes (see Section 2). However, (D)UFM discards the training data and most of the network completely, thus ignoring also the role of learning in DNC formation. This is a serious gap to fully understand the formation of DNC in the context of the full DNN pipeline.

In this paper, we introduce a setting where DNC forms through a learning algorithm that is highly dependent on the training data and predictor - iterated linear mapping onto the average gradient outer product (AGOP). The AGOP is the uncentered covariance matrix of the input-output gradients of a predictor averaged over its training data. Recent work has utilized this object to understand various surprising phenomena in neural networks including grokking, lottery tickets, simplicity bias, and adversarial examples (Radhakrishnan et al., 2024). Additional work incorporated layer-wise linear transformation with the AGOP into kernel machines, to model the deep feature learning mechanism of neural networks (Beaglehole et al., 2023). The output of their backpropagation-free method, Deep Recursive Feature Machines (Deep RFM), is a standard neural network at i.i.d. initialization, where each random weight matrix is additionally right-multiplied by the AGOP of a kernel machine trained on the input to that layer. Their method was shown to improve performance of convolutional kernels on vision datasets and recreate the edge detection ability of convolutional neural networks.

Strikingly, we show that the neural network generated by this very same method, Deep RFM, consistently exhibits standard DNC with little modification (Section 4). We establish in this setting that projection onto the AGOP is responsible for DNC in Deep RFM both empirically and theoretically. We verify these claims with an extensive experimental evaluation, which demonstrates a consistent formation of DNC in Deep RFM across several vision datasets. We then provide theoretical analyses that explain the mechanism for Deep RFM in the asymptotic high-dimensional regime and derive NC formation as a consequence of kernel learning (Section 5). Our first analysis is primarily based on the approximate linearity of kernel matrices when the dimension and the number of data points are large and proportional. Our second analysis demonstrates that DNC is an implicit bias of optimizing over the choice of kernel and its regression coefficients.

We then give substantial evidence that projection onto the AGOP is closely related to DNC formation in standard DNNs. In particular, we show that within-class variability for DNNs trained using SGD with small initialization is primarily reduced by the application of the right singular vectors of the weight matrix and the subsequent multiplication with its singular values (Section 6). These singular structures of a weight matrix \(W\) are fully deducible from the Gram matrix of the weights at each layer, \(W^{}W\). Radhakrishnan et al. (2024); Beaglehole et al. (2023) have identified that, in many settings and across all layers of the network, \(W^{}W\) is highly correlated with the average gradient outer product (AGOP) with respect to the inputs to that layer, in a statement termed the _Neural Feature Ansatz_ (NFA). Thus, the NFA suggests neural networks extract features from the data representations at every layer by projection onto the AGOP with respect to that layer.

Our results demonstrate that _(i)_ AGOP is a mechanism for DNC in Deep RFM, and _(ii)_ when the NFA holds, i.e. with small initialization, the right singular structure of \(W\), and therefore the AGOP, induces the majority of within-class variability collapse in DNNs. We thus establish projection onto the AGOP as a setting for DNC formation that incorporates the data through feature learning.

## 2 Related work

Neural collapse (NC).Since the seminal paper by Papyan et al. (2020), the phenomenon of neural collapse has attracted significant attention. Galanti et al. (2022) use NC to improve generalization bounds for transfer learning, while Wang et al. (2023) discuss transferability capabilities of pre-trained models based on their NC on the target distribution. Haas et al. (2022) argue that NC can lead to improved OOD detection. Connections between robustness and NC are discussed in Su et al. (2023). For a survey, we refer the interested reader to Kothapalli (2023).

The main theoretical framework to study the NC is the Unconstrained Features Model (UFM) (Mixon et al., 2020; Lu and Steinerberger, 2022). Under this framework, many works have shown the emergence of the NC, either via optimality and/or loss landscape analysis (Weinan and Wojtowytsch, 2022; Lu and Steinerberger, 2022; Zhou et al., 2022), or via the study of gradient-based optimization [Ji et al., 2022; Han et al., 2022; Wang et al., 2022]. Some papers also focus on the generalized class-imbalanced setting, where the NC does not emerge in its original formulation [Thrampoulidis et al., 2022; Fang et al., 2021; Hong and Ling, 2023]. Deviating from the strong assumptions of the UFM model, a line of work focuses on gradient descent in homogeneous networks [Poggio and Liao, 2020; Xu et al., 2023; Kunin et al., 2022], while others introduce perturbations [Tirer et al., 2022].

AGOP feature learning.The NFA was shown to capture many aspects of feature learning in neural networks including improved performance and interesting structural properties. The initial work on the NFA connects it to the emergence of spurious features and simplicity bias, why pruning DNNs may increase performance, the "lottery ticket hypothesis", and a mechanism for grokking in vision settings [Radhakrishnan et al., 2024a]. Zhu et al.  connect large learning rates and catapults in neural network training to better alignment of the AGOP with the true features. Radhakrishnan et al. [2024b] demonstrate that the AGOP recovers the implicit bias of deep linear networks toward low-rank solutions in matrix completion. Beaglehole et al.  identify that the NFA is characterized by alignment of the weights to the pre-activation tangent kernel.

## 3 Background and definitions

### Notation

For simplicity of description, we assume a class-balanced setting, where \(N=Kn\) is the total number of training samples, with \(K\) being the number of classes and \(n\) the number of samples per class. Note however that our experimental results and asymptotic theory hold in the general case that the classes are of unequal size. We will in general order the training samples such that the samples of the same class are grouped into blocks. With this ordering, the labels \(y^{K N}\) can be written as \(I_{K}_{n}^{},\) where \(I_{K}\) denotes a \(K K\) identity matrix, \(\) denotes the Kronecker product and \(_{n}\) a row vector of all-ones of length \(n\).

For a matrix \(A^{d_{1} d_{2}}\) and a column vector \(v^{d_{1} 1}\), the operation \(A v\), divides all elements of each row of \(A\) by the corresponding element of \(v\). We define the norm of \(A^{d n}\), \(\|A\|^{n 1}\), as the column-wise \(_{2}\)-norm and not the matrix norm.

Both a DNN and Deep RFM of depth \(L\) can be written as:

\[f(x)=m_{L+1}(m_{L}(m_{L-1}(m_{1}(x)))),\]

where \(m_{l}\) is an affine map and \(\) is a non-linear activation function. For neural networks, the linear map of \(m_{l}\) is the application of a single weight matrix \(W_{l}\), while for Deep RFM the linear transformation is a product of a weight matrix and a feature matrix \(M_{l}^{1/2}\), written \(W_{l}M_{l}^{1/2}\). The training data \(X^{d_{1} N}\) is stacked into columns and we let \(X_{l}\) be the feature representations of the training data after \(l\) layers of a DNN or Deep RFM before the linear layer for \(l 1\), with \(X_{1}=X\).

### Average gradient outer product

The AGOP operator acts with respect to a dataset \(X^{d_{0} N}\) and any model, that accepts inputs from the data domain \(f:^{d_{0} 1}^{K 1}\), where \(K\) is the number of outputs. Writing the (transposed) Jacobian of the model outputs with respect to its inputs as \(^{d_{0} K}\), the AGOP is defined as:

\[(f,X)_{c=1}^{K}_{i=1}^{N})}{ x})}{ x}^{}.\] (1)

Note while the AGOP is stated such that the derivative is with respect to the immediate model inputs \(x\), we will also consider the AGOP where derivatives are taken with respect to intermediate representations of the model. This object has important implications for learning because the AGOP of a learned predictor will (with surprisingly few samples) resemble the _expected_ gradient outer product (EGOP) of the target function [Yuan et al., 2023]. While the AGOP is specific to a model and training data, the EGOP is determined by the population data distribution and the function to be estimated, and contains specific useful information such as low-rank structure, that improves prediction [Trivedi et al., 2014].

Remarkably, it was identified in Radhakrishnan et al. [2024a], Beaglehole et al.  that neural networks will automatically contain AGOP structure in the Gram matrix of the weights in all layersof the neural network, where the AGOP at each layer acts on the sub-network and feature vectors at that layer. Stated formally, the authors observe and pose the Neural Feature Ansatz (NFA):

**Ansatz 3.1** (Neural Feature Ansatz [2024a]).: _Let \(f\) be a depth-\(L\) neural network trained on data \(X\) using stochastic gradient descent. Then, for all layers \(l[L]\),_

\[(W_{l}^{}W_{l},_{c=1}^{K}_{i=1}^{n})}{ x^{l}})}{ x^{l}} )^{} 1.\] (2)

The second argument is the AGOP of the neural network where the gradients are taken with respect to the activations at layer \(l\) and not the initial inputs. The correlation function \(\) accepts two matrix inputs of shape \((p,q)\) and returns the cosine similarity of the matrices flattened into vectors of length \(pq\), whose value is in \([-1,1]\). Note Radhakrishnan et al. (2024a) formulates this similarity as proportionality, which is equivalent to correlation exactly equal to \(1\).

Crucially, the AGOP can be defined for any differentiable estimator, not necessarily a neural network. In Radhakrishnan et al. (2024a), the authors introduce a kernel learning algorithm, the Recursive Feature Machine (RFM), that performs kernel regression and estimates the AGOP of the kernel machine in an alternating fashion, enabling recursive feature learning and refinement through AGOP.

### Deep RFM

Subsequent work introduced the Deep Recursive Feature Machine (Deep RFM) to model deep feature learning in neural networks [Beaglehole et al., 2023]. Deep RFM iteratively generates representations by mapping the input to that layer with the AGOP of the model w.r.t. this input, and then applying an untrained random feature map. To define the Deep RFM, let \(\{k_{l}\}_{l=1}^{L+1}:^{d_{l} d_{l}}\) be a set of kernels. For the ease of notation, we will write \(k_{l}(X_{l}^{},X_{l})\) for a matrix of kernel evaluations between columns of \(X_{l}^{}\) and \(X_{l}\). We then describe Deep RFM in Algorithm 1.

```
0:\(X_{1},Y,\{k_{l}\}_{l=1}^{L+1}\), \(L,\{_{l}\}_{l=1}^{L}\) {kernels: \(\{k_{l}\}_{l}\), depth: \(L\), feature maps: \(\{_{l}\}_{l}\), ridge: \(\)} output\(_{L+1},\{M_{l}\}_{l=1}^{L}\) for\(l 1,,L\)do  Normalize the data, \(X_{l} X_{l}\|X_{l}\|\)  Lear coefficients, \(_{l}=Y(k_{l}(X_{l},X_{l})+ I)^{-1}\).  Construct predictor, \(f_{l}()=_{l}k_{l}(X_{l},)\).  Compute AGOP: \(M_{l}=_{c,i=1}^{K,n}(x_{ci}^{l})}{ x^{l}} (x_{ci}^{l})}{ x^{l}}^{}\).  Transform the data \(X_{l+1}_{l}(M_{l}^{1/2}X_{l})\). endfor  Normalize the data, \(X_{L+1} X_{L+1}\|X_{L+1}\|\)  Learn coefficients, \(_{L+1}=Y(k_{L+1}(X_{L+1},X_{L+1})+ I)^{-1}\). ```

**Algorithm 1** Deep Recursive Feature Machine (Deep RFM)

Note that the Deep RFM as defined here considers only one AGOP estimation step in the inner-optimization of RFM, while multiple iterations are used in Beaglehole et al. (2023). The high-dimensional feature maps \(_{l}()\) are usually realized as \((W_{l}+b_{l}),\) where \(W_{l}\) is a matrix with standard Gaussian or uniform entries, \(b_{l}\) is an optional bias term initialized uniformly at random, and \(\) is the ReLU or cosine activation function. Thus, \(_{l}\) typically serves as a random features generator.

The single loop in the Deep RFM represents a reduced RFM learning process. The RFM itself is based on kernel ridge regression, therefore we introduce the ridge parameter \(\).

### Deep Neural Collapse

We define the feature vectors \(x_{ci}^{l}\) of the \(i\)-th sample of the \(c\)-th class as the input to \(m_{l}\). For neural networks, we define \(_{ci}^{l}\) as the feature vectors produced from \(m_{l-1}\) before the application of \(\), where \(l 2\). For Deep RFM, we define \(_{ci}^{l}\) as the feature vectors produced by the application of AGOP, i.e.

\(^{l}_{ci}=M_{l}^{1/2}x^{l}_{ci}\), again where \(l 2\). In both model types, we let \(^{1}_{ci}=x^{1}_{ci}\). Let \(^{l}_{c}:=_{i=1}^{n}x^{l}_{ci}\), \(^{l}_{c}:=_{i=1}^{n}^{l}_{ci}\) be the corresponding class means, and \(^{l},^{l}^{d_{l} K}\) the matrices of class means. NC1 is defined in terms of the _within-class_ and _between-class_ variability at layer \(l\):

\[^{l}_{W}=_{c=1}^{K}_{i=1}^{n}(x^{l}_{ci}-^{l}_{c}) (x^{l}_{ci}-^{l}_{c})^{},^{l}_{B}=_{c=1}^{K} (^{l}_{c}-^{l}_{G})(^{l}_{c}-^{l}_{G})^{}\.\]

**Definition 3.2**.: In the context of this work, NC is achieved at layer \(l\) if the following two properties are satisfied:

* **DNC1:** The within-class variability at layer \(l\) is \(0\). This property can be stated for the features either after or before the application of the activation function \(\). In the former case, the condition requires \(x^{l}_{ci}=x^{l}_{cj}\) for all \(i,j\{1,,n\}[n]\) (or, in matrix notation, \(X_{l}=^{l}_{n}^{}\)); in the latter, \(^{l}_{ci}=^{l}_{cj}\) for all \(i,j[n]\) (or, in matrix notation, \(_{l}=^{l}_{n}^{}\)).
* **DNC2:** The class-means at the \(l\)-th layer form either an orthogonal basis or a simplex equiangular tight frame (ETF). As for DNC1, this property can be stated for features after or before \(\). We write the ETF class-mean covariance \(_{}(1+)I_{K}- _{K}_{K}^{}\). In the former case, the condition requires either \((^{l})^{}^{l} I_{K}\) or \((^{l})^{}^{l}_{}\); in the latter \((^{l})^{}^{l} I_{K}\) or \((^{l})^{}^{l}_{}\). In this work, we will measure this condition on the centered and normalized class means, \(^{l}\). We let \(^{l}=(^{l}-^{l}_{G})(^{l}-^{l}_ {G})\) or \(^{l}=(^{l}-^{l}_{G})( ^{l}-^{l}_{G})\), depending on whether we measure collapse on \(X\) or \(\), respectively.

## 4 Average gradient outer product induces DNC in Deep RFM

We demonstrate that AGOP is a mechanism for DNC in the Deep RFM model. In this section, we provide an extensive empirical demonstration that Deep RFM exhibits DNC - NC1 and NC2 - and that the induction of neural collapse is due to the application of AGOP.

We visualize the formation of DNC in the input vectors of each layer of Deep RFM. For \(l 2\), let \(_{l}=M_{l-1}^{1/2}X_{l-1}\) be the feature vectors at layer \(l-1\) projected onto the square root of the AGOP, \(M_{l-1}^{1/2}\), at that layer. Otherwise, we define \(_{1}=X_{1}\) as the untransformed representations. In Figure 1, we show the Gram matrix, \(_{l}^{}_{l}\), of centered and normalized feature vectors \(_{l}=(_{l}-^{l}_{G})|_{l} -^{l}_{G}|\) extracted from several layers of Deep RFM, as presented in Algorithm 1,

Figure 1: Neural collapse with Deep RFM on (A) CIFAR-10 and (B) MNIST. The matrix of inner products of all pairs of points in \(X_{l}\) extracted from layers \(l\{1,3,7,13,19\}\) of Deep RFM. The columns show the Gram matrices of feature vectors transformed by the AGOP from Deep RFM, \((_{l}-^{l}_{G})\|_{l}- ^{l}_{G}\|\). The data are ordered so that points of the same class are adjacent to one another, arranged from classes \(1\) to \(10\). Deep RFM uses non-linearity \(()=()\) in (A) and \(()=()\) in (B).

trained on CIFAR-10 and MNIST (see Appendix E for similar results on SVHN). Here the global mean \(_{G}^{l}\) is subtracted from each column of \(\). After 18 layers of DeepRFM, the final Gram matrix is approximately equal to that of collapsed data, in which all points are at exactly their class means, and the centered class means form an ETF. Specifically, all centered points of the same class have inner product \(1\), while pairs of different class have inner product \(-(K-1)^{-1}\). Note that, like standard neural networks, Deep RFM exhibits DNC even when the classes are highly imbalanced, such as for the SVHN dataset (Figure 5B in Appendix E).

We show collapse occurring as a consequence of projection onto the AGOP, across all datasets and across choices of feature map (Figures 3 and 4 in Appendix E). We observe that the improvement in NC1 is entirely due to \(M_{l}^{1/2}\), and the random feature map actually worsens the NC1 value. This result is intuitive as Deep RFM deviates from a simple deep random feature model just by additionally projecting onto the AGOP with respect to the input at each layer, and we do not expect random feature maps to induce neural collapse on their own. In fact, the random feature map will provably separate nearby datapoints, as this map is equivalent to applying a rapidly decaying non-linear function to the inner products between pairs of points (described formally in Appendix A).

## 5 Theoretical explanations for DNC in Deep RFM

We have established empirically in a range of settings that Deep RFM induces DNC through AGOP. We now prove that DNC in Deep RFM (1) occurs in an asymptotic setting, and (2) can be viewed as an implicit bias of RFM as a kernel learning algorithm.

### Asymptotic analysis

Many works have derived that, under mild conditions on the data and kernel, non-linear kernel matrices of high dimensional data are well approximated by linear kernels with a non-linear perturbation (Karoui, 2010; Adlam et al., 2019; Hu and Lu, 2022). In this section, we provide a proof that Deep RFM will induce DNC when the predictor kernels \(\{k_{l}\}_{l}\) and random feature maps \(\{_{l}\}_{l}\) satisfy this property. In particular, in the high-dimensional setting in these works, non-linear kernel matrices, written \(k(X) k(X,X)\) for simplicity, are well approximated by

\[k(X)^{}+X^{}X+ I,\]

where \( 0\) is the _perturbation_ parameter. Following Adlam and Pennington (2020), for additional simplicity we consider centered kernels, where \(=0\). We associate with Deep RFM two separate (unrestricted) centered kernels \(\) and \(k_{}\), with perturbation parameters \(\) and \(_{}\), corresponding to the predictor kernels and random feature maps respectively.

We will show that Deep RFM exhibits exponential convergence to DNC, and the convergence rate depends on the ratio \(/_{}\). These two parameters modulate the distance of non-linear ridge regression with each kernel to linear regression, and therefore the extent of DNC with Deep RFM. Namely, as we will show, if \(\) is close to the linear kernel, i.e., if \(\) is small, then the predictor in each layer resembles interpolating linear regression, an ideal setting for collapse through the AGOP. On the other hand, if \(k_{}\) is close to the linear kernel, i.e., if \(_{}\) is small, then the data is not _easily_ linearly separable. In that case, the predictor will be significantly non-linear in order to interpolate the labels, deviating from the ideal setting. We proceed by explaining specifically where \(\) and \(k_{}\) appear in Deep RFM and why linear interpolation induces collapse. We then conclude with the statement of our main theorem and an explanation of its proof.

We describe the Deep RFM iteration we analyze here, which has a slight modification. Instead of normalizing the data at each iteration, we scale the data by a value \(^{-1}\) at each iteration. This modification is sufficient to control the norms of the data, and is easier to analyze. The recursive procedure begins with a dataset \(X_{l}\) at layer \(l\) in Deep RFM, and constructs \(_{l}\) from the AGOP \(M_{l}\) of the kernel ridgeless regression solution with kernel \(k_{l}=\). After transforming with the AGOP and scaling, the data Gram matrix at layer \(l\) is transformed to,

\[_{l+1}^{}_{l+1}=^{-1}X_{l}^{}M_{l}X_{l},\] (3)for \(=1-2(1+_{}^{-1})\). Then, we will apply a non-linear feature map \(_{}\) corresponding to a kernel \(k_{}\), and write the corresponding Gram matrix at depth \(l+1\) as

\[X_{l+1}^{}X_{l+1}=_{}(_{l+1})^{}_{}(_{l+1})=k_{}(_{l+1})=_{l+1}^{} _{l+1}+_{}I\;.\] (4)

Intuitively, the ratio \(/_{}\) determines how close to linear \(\) is when applied to the dataset, and therefore the rate of NC formation.

We now explain why Deep RFM with a linear kernel is ideal for NC, inducing NC in just one AGOP application. To see this, note that ridgeless regression with a linear kernel is exactly least-squares regression on one-hot encoded labels. In the high-dimensional setting we consider, should we find a linear solution \(f(x)=^{}x\) that interpolates the labels, the AGOP of \(f\) is \(^{}\). Since we interpolate the data, \(^{}x=y\) for all \((x,y)\) input/label pairs, applying the AGOP collapses the data to \(^{}x= y\). Therefore, NC will occur in a single layer of Deep RFM when \(_{}\), in which case the data Gram matrix has a large minimum eigenvalue relative to the identity perturbation to \(\), so the kernel regression is effectively linear regression. Note that this theory offers an explanation why a non-linear activation is needed for DNC in neural networks: \(_{}=0\) when \(_{}\) is linear, preventing this ideal setting.

We prove that DNC occurs in the following full-rank, high-dimensional setting.

**Assumption 5.1** (Data is high dimensional).: We assume that the data has dimension \(d n\).

**Assumption 5.2** (Data is full rank).: We assume that the initial data Gram matrix \(X_{1}^{}X_{1}\) has minimum eigenvalue at least \(_{}>0\).

The assumptions that the Gram matrix of the data is full-rank and high-dimensional is needed only if one requires neural collapse in every layer of Deep RFM, starting from the very first one. In contrast, if we consider collapse starting at any given later layer of Deep RFM, then we only need that the smallest eigenvalue of the corresponding feature map is bounded away from \(0\). This in turn requires that the number of features at that layer is greater than the number of data points, a condition which is routinely satisfied by the overparameterized neural networks used in practice.

We present our main theorem.

**Theorem 5.3** (Deep RFM exhibits neural collapse).: _Suppose we apply Deep RFM on any dataset \(X\) with labels \(Y^{N K}\) choosing all \(\{_{l}\}_{l}\) and \(\{k_{l}\}_{l}\) as the feature map \(_{}\) and kernel \(\) above, with no ridge parameter (\(=0\)). Then, there exists a universal constant \(C>0\), such that for any \(0< 1\), provided \(}}{2(1+_{}^{-1})n}(1-)\), and for all layers \(l\{2,,L\}\) of Deep RFM,_

\[\|_{l+1}^{}_{l+1}-Y^{}Y\|(1-)\|_{l}^{}_{l}-Y^{}Y\|+O(^{2}_{}^{-2}).\]

This theorem immediately implies exponential convergence to NC1 and NC2 up to error \(O(L^{2}_{}^{-2})\), a small value determined by the parameters of the problem. As a validation of our theory, we see that this exponential improvement in the DNC metric occurs in all layers (see Figures 3 and 4). Note the exponential rate predicted by Theorem 5.3 and observed empirically for Deep RFM is consistent with the exponential rate observed in deep neural networks (He and Su, 2022).

The proof for this theorem is roughly as follows. Recall that, by our argument earlier in this section, a linear kernel will cause collapse within just one layer of Deep RFM. However, in the more general case we consider, a small non-linear deviation from a linear kernel, \(\), is introduced. Beginning with the first layer, partial collapse occurs if the ratio \(/_{}^{-1}\) is sufficiently small. Following the partial collapse, in subsequent layers, the data Gram matrix will sufficiently resemble the collapsed matrix, so that the non-linear kernel solution on the collapsed data will behave like the linear solution, leading to further convergence to the collapsed data Gram matrix, a fixed point of the Deep RFM iteration (Lemma C.2).

### Connection to parametrized kernel ridge regression

Next, we demonstrate that DNC emerges in parameterized kernel ridge regression (KRR). In fact, DNC arises as a natural consequence of minimizing the norm of the predictor jointly over the choice of kernel function and the regression coefficients. This result proves that DNC is an implicit bias of kernel learning. We connect this result to Deep RFM by providing intuition that the kernel learned through RFM effectively minimizes the parametrized KRR objective and, as a consequence, RFM learns a kernel matrix that is biased towards the collapsed Gram matrix \(Y^{}Y\).

We define the parametrized KRR problem. Consider positive semi-definite kernels (p.s.d.) \(k_{M}:^{d}^{d}\) of form \(k_{M}(x,z)=(\|x-z\|_{M}),\) where \(\|x-z\|_{M}=M(x-z)}\), \(\) is a strictly decreasing, strictly positive univariate function s.t. \((0)=1\), and \(M\) is a p.s.d. matrix. This class of kernels covers a wide range of kernels including the Gaussian and Laplace we use in our experiments. Let \(_{M}\) be the Reproducing Kernel Hilbert Space (RKHS) corresponding to our chosen kernel \(k_{M}\) where functions \(f_{M}\) map to a single output class. The parametrized kernel ridge regression for a single output class (\(K=1\)) with ridge parameter \(\) corresponds to the following optimization problem over such \(f\) and \(M\) on dataset \(X\) and labels \(Y^{K N}\):

\[_{f,M}\|f(X)-Y\|^{2}+\|f\| _{_{M}}^{2}.\] (5)

When we predict outputs over \(K>1\) classes, we jointly optimize over \(K\) independent scalar-valued \(f\), one for each output class, and, again, a single choice of matrix \(M\). The objective function is then sum the individual objective values over all classes. Our modification for multiple outputs corresponds to how Deep RFM and kernel ridge regression, more generally, are solved in practice - a single kernel matrix is used across all classes while kernel coefficients are computed for each class independently. Note this modification for multiple classes is also equivalent to optimizing over a single vector-valued function where the function space \(_{M}\) is a particular vector-valued RKHS (see Appendix B for more details).

Parameterized KRR differs from standard KRR by additionally optimizing over the choice of the kernel through the matrix \(M\). For all \(M\), including the optimal one for Problem (5), an analog of the representer theorem for vector-valued RKHS can be shown (Micchelli and Pontil, 2004) and the optimal solution to (5) can be written as \(f(z)=_{c,i=1,1}^{K,n}_{ci}k_{M}(x_{ci},z),\) where \(_{ci}\) is a \(K\)-dimensional vector. Let us therefore denote \(A\) the matrix of stacked columns \(_{ci}\). We can re-formulate Problem (5) as the following finite-dimensional optimization problem:

\[_{A,M}\,((Y-Ak_{M})(Y-Ak_{M})^{T})+ {tr}(k_{M}A^{T}A),\] (6)

where, abusing notation, \(k_{M}:=k_{M}(X,X)\) is the matrix of pair-wise kernel evaluations on the data.

We now relax the optimization over matrices \(M\) by optimizing over all p.s.d., entry-wise non-negative kernel matrices \(k^{nK}^{nK}\) with ones on diagonal and compute the optimal value of the following relaxed parametrized KRR minimization:

\[_{A,k}\,((Y-Ak)(Y-Ak)^{T})+ {tr}(kA^{T}A).\] (7)

Optimizing over all p.s.d., entry-wise non-negative \(k\) with ones on diagonal is not always equivalent to optimizing over all \(M\). Thus, in general, this provides only a relaxation. However, if the data \(X\) is full column rank (as in our asymptotic analysis), then the optimizations (6) and (7) have the same solution and, thus, the relaxation is without loss of generality. We establish this equivalence formally in Appendix B.

We are now ready to state our optimality theorem. The proof is deferred to Appendix C.

**Theorem 5.4**.: _The unique optimal solution to the (relaxed) parametrized kernel ridge regression objective (Problem 7) is the kernel matrix \(k^{*}\) exhibiting neural collapse, \(k^{*}=I_{K}(_{n}_{n}^{})=Y^{}Y.\)_

Finally, we informally connect this result to RFM. We argue that RFM naturally minimizes the parametrized kernel ridge regression objective (Problem 5) through learning the matrix \(M\). This claim is natural since the RFM is a parametrized kernel regression model, where \(M\) is set to be the AGOP. This choice of Mahalanobis matrix should minimize (5) as AGOP captures task-specific low-rank structure, reducing unnecessary variations of the predictor in irrelevant directions Chen et al. (2023). Then, given any setting of \(M\), RFM is conditionally optimal w.r.t. the parameter \(\), as this method simply solves the original kernel regression problem for each fixed \(M\). Further, as argued in Section 5.1, choosing \(k\) to be a dot product kernel and \(M\) to be the AGOP of an interpolating linear classifier implies that \(k_{M}=k^{*}\), the optimal solution outlined in Theorem 5.4.

## 6 Within-class variability collapse through AGOP in neural networks

Thus far, we have demonstrated that Deep RFM exhibits DNC empirically, and have given theoretical explanations for this phenomenon. Here, we provide evidence that the DNC mechanism of Deep RFM, i.e., the projection onto the AGOP, is responsible for DNC formation in typical neural networks, such as MLPs, VGG, and ResNet trained by SGD with small initialization. We do so by demonstrating that within-class variability collapse occurs predominantly through the multiplication by the right singular structure of the weights. As the NFM, which is determined by the right singular structure of each weight matrix, is highly correlated with the AGOP, our results imply the AGOP is responsible for NC1 progression.

A fully-connected layer \(l\) of a neural network consists of two components: multiplication by a weight matrix \(W_{l}\), followed by the application of an element-wise non-linear activation function \(\) (termed the non-linearity). Both components of the layer crucially contribute to the inference and training processes of a neural network. The question we address in this section is whether the non-linearity or the weight matrix is primarily responsible for the improvement in DNC1 metrics.

We additionally decompose the weight matrix into its singular value decomposition \(W_{l}=U_{l}S_{l}V_{l}^{}\), viewing a fully-connected layer as first applying \(S_{l}V_{l}^{}\), then applying the composition of \(\) with \(U_{l}\), \((U_{l^{}})\). This decomposition allows us to directly consider the effect of the NFM, \(W_{l}^{}W_{l}=V_{l}S_{l}^{2}V_{l}^{}\), and therefore the AGOP, on DNC formation.

The grouping of a layer into the right singular structure and the non-linearity with the left singular vectors is natural, as DNC1 metrics computed on this decomposition are identical to the metrics computed using the whole \(W_{l}\) and then the non-linearity. We see this fact by considering the standard DNC1 metric \((_{W})(_{B})^{-1}\)(Tirer et al., 2023; Rangamani et al., 2023). A simple computation using the cyclic property of the trace gives \((_{W})(_{B})^{-1}= (U_{W}U^{})(U_{B}U^{})^ {-1}=(_{W})(_{B})^{-1}\), where \(\) matrices refer to the within-class variability after the application of \(SV^{}\), while the \(\) matrices correspond to the output of the full weight matrix.

While both the non-linearity and \(S_{l}V_{l}^{}\), and only these two components, can influence the DNC1 metric, we demonstrate that \(S_{l}V_{l}^{}\) is responsible for directly inducing the majority of within-class variability collapse in neural networks trained by SGD with small initialization. We verify this claim

Figure 2: Feature variability collapse from different singular value decomposition components in (A) an MLP on MNIST, and (B) a ResNet on CIFAR-10. We measure the reduction in the NC1 metric throughout training at each of five fully-connected layers. Each layer is decomposed into its input, \((X)\), the projection onto the right singular space of \(W\), \(SV^{}(X)\), and then \(U\), the left singular vectors of \(W\), and the application of the non-linearity.

by plotting the DNC1 metrics of all layers of an MLP and ResNet network trained on MNIST and CIFAR-10, respectively (Figure 2), where each layer is decomposed into its different parts - (1) the input to that layer, (2) the embedding after multiplication with \(S_{l}V_{l}^{}\), and (3) the embedding after multiplication with the left singular vectors \(U_{l}\) and application of the non-linearity \(\).

We see that the ratio of within-class to between-class variability decreases mostly between steps (1) and (2), due to the application of the right singular structure. Similar results are in Appendix E for all combinations of datasets (MNIST, CIFAR-10, SVHN) and architectures (MLP, VGG, and ResNet). This is a profound insight into the underlying mechanisms for DNC that is of independent interest, especially given we train with a standard algorithm across many combinations of datasets and models.

We note that, while in this setting the ReLU does not directly reduce NC1, the ReLU is still crucial for ensuring the expressivity of the feature vectors. Without non-linearity, the neural network cannot interpolate the data or perform proper feature learning, necessary conditions for DNC to occur at all.

These results are in a regime where the NFA holds with high correlation, with ResNet and MLP having NFA correlations at the end of training (averaged over all layers and seeds) of \(0.74 0.17\) and \(0.74 0.13\) on CIFAR-10 and MNIST, respectively (see Appendix E for values across all architectures and datasets). Therefore, as \(W_{l}^{}W_{l}=V_{l}S_{l}^{2}V_{l}^{}\), \(S_{l}V_{l}^{}\) should project into a similar space as the AGOP. We note that the matrix quantities involved are high-dimensional, and the trivial correlation between two i.i.d. uniform eigenvectors of dimension \(d\) concentrates within \( O(d^{-1/2})\). For the width \(512\) weight matrices considered here, this correlation would be approximately (at most) \(0.04\). Therefore, we conclude that the AGOP structure can decrease within-class variability in DNNs.

Unlike DNC1, we have observed a strong DNC2 progression only in the very last layer. This is due to the fact that DNC2 was observed in the related work Rangamani et al. (2023); Parker et al. (2023) in a regime of large initialization. In contrast, the NFA holds most consistently with small initialization (Radhakrishnan et al., 2024; Beaglehole et al., 2024), and in this feature learning regime, the low-rank bias prevents DNC2 (Li et al., 2020; Sukenik et al., 2024).

## 7 Conclusion

This work establishes that deep neural collapse can occur through feature learning with the AGOP. We bridge the unsatisfactory gap between DNC and the data - with previous work mostly only focusing on the very end of the network and ignoring the training data.

We demonstrate that projection onto the AGOP induces deep neural collapse in Deep RFM. We validate that AGOP induces NC in Deep RFM both empirically and theoretically through asymptotic and kernel learning analyses.

We then provide evidence that the AGOP mechanism of Deep RFM induces DNC in general neural networks. We experimentally show that the DNC1 metric progression through the layers can be mostly due to the linear denoising via the application of the center-right singular structure. Through the NFA, the application of this structure is approximately equivalent to projection onto the AGOP, suggesting that the AGOP directly induces within-class variability collapse in DNNs.