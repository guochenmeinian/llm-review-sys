# GraphMorph: Tubular Structure Extraction by Morphing Predicted Graphs

Zhao Zhang\({}^{1,5}\) Ziwei Zhao\({}^{2}\) Dong Wang\({}^{2}\) Liwei Wang\({}^{3,4,82}\)

\({}^{1}\)Center for Data Science, Peking University \({}^{2}\)Vizhun Medical AI Co., Ltd

\({}^{3}\)State Key Laboratory of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{4}\)Center for Machine Learning Research, Peking University

\({}^{5}\)Pazhou Laboratory (Huangpu), Guangzhou, Guangdong, China

zhangzh@stu.pku.edu.cn zwei.zhao@yzhun-ai.com

dong.wang@yizhun-ai.com wanglw@pku.edu.cn

###### Abstract

Accurately restoring topology is both challenging and crucial in tubular structure extraction tasks, such as blood vessel segmentation and road network extraction. Diverging from traditional approaches based on pixel-level classification, our proposed method, named GraphMorph, focuses on branch-level features of tubular structures to achieve more topologically accurate predictions. GraphMorph comprises two main components: a Graph Decoder and a Morph Module. Utilizing multi-scale features extracted from an image patch by the segmentation network, the Graph Decoder facilitates the learning of branch-level features and generates a graph that accurately represents the tubular structure in this patch. The Morph Module processes two primary inputs: the graph and the centerline probability map, provided by the Graph Decoder and the segmentation network, respectively. Employing a novel \(\) algorithm, the Morph Module produces a centerline mask that aligns with the predicted graph. Furthermore, we observe that employing centerline masks predicted by GraphMorph significantly reduces false positives in the segmentation task, which is achieved by a simple yet effective post-processing strategy. The efficacy of our method in the centerline extraction and segmentation tasks has been substantiated through experimental evaluations across various datasets. Source code will be released soon.

## 1 Introduction

Extraction of tubular structures is an essential step in many computer vision tasks [39; 13; 1; 27]. In medical applications, accurate segmentation of retinal vessels can provide crucial insights into various cardiovascular and ophthalmologic diseases . In the field of urban planning and geographic information systems, the precise extraction of road networks aids in traffic management, urban development, and emergency response planning . Existing deep learning-based methods model tubular structure extraction as a pixel-level classification task  or point set prediction task , without explicitly predicting the topological structures. To focus more on topology, some advanced methods design novel backbones or modules [36; 23; 33], or introduce new loss functions from the topological perspective [14; 37; 25]. However, they are still limited to the framework of pixel-level prediction.

We argue that most pixel-level frameworks have not effectively exploited the nature of tubular structures, which are inherently composed of several branches that are interconnected in complex ways. Specifically, pixel-level loss functions, like softDice Loss  and Focal Loss , struggle with subtle inaccuracies and are particularly ineffective at addressing complex topological errors.

Under the pixel-level frameworks, despite attempts to pay more attention to fine branches [37; 25] or topological features [14; 33], they still struggle with fully capturing the complex topological nature of tubular structures. We demonstrate this deficiency by providing an example in Figure 1. For a systematic understanding of the issues of pixel-level frameworks, we summarize them into three categories: (1) Broken branches or false negatives (FNs). (2) Redundant branches or false positives (FPs). (3) Topological errors (TEs). Therefore, understanding tubular structure extraction solely from a pixel-level perspective is fundamentally flawed.

Recognizing these limitations, we shift our focus to **branch-level features**, which are more essential for accurately capturing the nuances of tubular structures. Any complex tubular structure can be broken down into several branches, which distinguishes it from non-tubular objects. This perspective inspires us to extract tubular structures in two steps: (1) predicting the location of the two endpoints of each branch; (2) finding the optimal path between the two endpoints of each branch. Such a solution is intuitively aligned with human perception, and able to offer several advantages. Specifically, if the endpoints of branches are accurately predicted in the first step, redundant branches (FPs) are then potentially reduced; and the second step ensures that there is a path connecting the two endpoints of each branch, so that broken branches (FNs) and TEs are effectively suppressed. Besides, learning branch-level features during training elevates the model's focus on topology, which implicitly improves topological accuracy.

To effectively utilize branch-level features of tubular structures, we propose GraphMorph, a pipeline for obtaining topologically accurate centerline masks. GraphMorph consists of a Graph Decoder and a training-free Morph Module, which corresponds to the two steps of our solution respectively. The Graph Decoder, given multi-scale features extracted from an image patch by a segmentation network, predicts the graph \(G\) of the tubular structure. \(G\) is defined by a node set \(V=\{(x_{i},y_{i})\}_{i=1}^{N}\), which contains the coordinates of all critical points vital for maintaining topology, and an adjacency matrix \(A\{0,1\}^{N N}\), which encodes the connectivity among nodes. Each pair of connected nodes in the graph corresponds to two endpoints of a branch of the tubular structure, thus the Graph Decoder takes full advantage of branch-level features through this graph representation. Technically, the prediction of \(V\) is addressed as a set prediction problem, solvable by our modified version of Deformable DETR . To efficiently obtain the adjacency matrix \(A\), we design a lightweight link prediction module that capitalizes on the extracted node features. Concretely, since the number of nodes in each tubular structure may be different, we generate linear weights and biases dynamically conditioned on node features, and the adjacency list of each node is obtained from its corresponding linear parameters (See Figure 2).

The Morph Module, a core contribution of this work, is intended to obtain topologically accurate centerline masks. While studies in the image-to-graph task [38; 32] also utilize graph representation,

Figure 1: Illustrating the impact of topological feature utilization on segmentation accuracy. **(a)** An input neuron image. **Column (b)** Ground truth with segmented membranes (white) and its centerline (blue lines); the constructed graph (nodes in red, edges in green). **Column (c)** and **(d)** Predictions of two methods [26; 37] without explicit topological learning, highlighting broken branches (false negatives in yellow), redundant branches (false positives in green), and topological errors (in red). **Column (e)** Our GraphMorph guarantees topological accuracy by learning explicit branch-level features. Details of skeletonization and graph construction are given in Appendix B. Evaluation metrics: Dice and clDice (higher is better), \(_{0}\) error and \(\) error (lower is better).

they struggle to directly obtain accurate centerline masks due to the curved nature of tubular objects. In contrast, our Morph Module generates topologically accurate centerline masks via a novel \(\) algorithm. Specifically, a centerline probability map \(P_{m}\), together with the graph \(G\), output by the segmentation network and the Graph Decoder respectively, serve as the input to the Morph Module. Afterwards, considering the skeleton property of centerlines, our \(\) algorithm finds the optimal path between each pair of connected nodes. In particular, during path finding from the start point to the end point, we always restrict the path to a single pixel width to satisfy the skeleton property of centerlines. Consequently, the topology of the resulting centerline mask is guaranteed by \(G\), leading to a reduction in TEs. This method also minimizes the occurrence of broken branches (FNs) and redundant branches (FPs), which is a significant improvement over direct pixel-level operations on \(P_{m}\), such as thresholding.

We conduct the experiments by beginning with the **centerline extraction** task to verify the effects of the two components of GraphMorph. Experimentally, serveing as an auxiliary training module to learn the graph representation, the Graph Decoder enhances the segmentation network's focus on branch-level features, thus both volumetric metrics and topological metrics are boosted. Furthermore, employing the Morph Module at inference stage considerably improves topological metrics. For the **segmentation** task, we develop a streamlined post-processing strategy to refine segmentation masks via the topologically accurate centerline masks output by the Morph Module, significantly suppressing false positives of segmentation results. To verify the effectiveness of GraphMorph and the post-processing strategy, we conduct extensive experiments across four typical tubular structure extraction datasets. We have applied our methodology on three powerful backbones and achieved consistent improvements in all metrics. Moreover, compared with the state-of-the-art methods, our approach achieves the best results across all datasets.

In a nutshell, our contributions can be summarized as the following: (1) We introduce GraphMorph, an innovative framework specifically tailored for tubular structure extraction. Based on the proposed Graph Decoder and Morph Module, the branch-level features are fully exploited and the topologically accurate centerline masks are derived naturally. (2) For the segmentation task, an efficient post-processing strategy significantly suppresses false positives via the centerline masks predicted by GraphMorph, ensuring that the segmentation results are more closely aligned with the predicted graphs. (3) Experimental results on three medical datasets and one road dataset underscore the effectiveness of our method. For both centerline extraction and segmentation tasks, GraphMorph has achieved remarkable improvements across all metrics, especially in topological metrics.

## 2 Related Work

**Image segmentation of tubular structures.** Deep learning-based methods have achieved impressive results in segmentation tasks [21; 34; 5]. To further enhance the segmentation performance of tubular structures, novel network architectures [16; 22; 36; 41; 23; 45; 40; 33] and topology-preserving loss functions [14; 29; 37; 25; 33] have been proposed. For example, in terms of network architecture, DSCNet  utilizes dynamic snake convolution to capture fine and tortuous local features; PointScatter  explores the point set representation of tubular structures and introduces a novel greedy-based region-wise bipartite matching algorithm to improve training efficiency. In terms of loss functions, \(\) proposes a differentiable soft skeletonization method and achieves loss calculation at centerline level, which implicitly helps model focus more on the fine branches; TopoLoss  and TCLoss  measure the topological similarity of the ground truth and the prediction via persistent homology. Despite these advancements, all of the above methods are still confined to the framework of pixel-level classification and can not entirely overcome their inherent limitations. Our method attempts to morph the predicted graphs of tubular structures to let the network focus more on branch-level features, thus ensuring the topological accuracy of predictions.

**Image to graph.** There are two mainstream subtasks in this area: road network graph detection [11; 43; 38; 44; 12] and scene graph generation [17; 19]. These tasks usually entail detecting key components as nodes (i.e., key points in roads, objects in scenes) and determining their interrelations as edges (i.e., connectivity in roads, interactions in scenes). Our work differs from these approaches in three ways. Firstly, we use only junctions and endpoints as nodes, which allows for explicit semantic characterization of nodes in our graph representation, unlike road network detection tasks where path points may also be regarded as nodes. Secondly, considering the curved nature of tubular objects, we propose Morph Module to obtain topologically accurate centerline masks, a goal that is not addressedby these works. Finally, our dynamic link prediction module is time-efficient, compared with elaborate and time-consuming designs in these works, such as [rln]-token in RelationFormer . For a clear understanding, we experimentally compare the differences between our approach and RelationFormer in Appendix C. These distinctions make our model not only time-efficient but also applicable to the task of tubular structure extraction with more complex topology.

## 3 Method

This section provides a detailed description of the training and inference procedures of GraphMorph. Figure 2 illustrates the training process of our approach, where the segmentation network and Graph Decoder are included. We detail these two components in Section 3.1 and 3.2, respectively. The training details are given in Section 3.3. Section 3.4 introduces the algorithmic flow of the Morph Module, followed by the inference processes for the centerline extraction and segmentation tasks.

### Segmentation Network

The segmentation network processes an input image \(I\) with shape \(\). It serves two purposes: (1) outputting a probability map of tubular structures; (2) providing multi-scale features for the Graph Decoder. For training efficiency, we randomly sample \(R\) regions of interest (ROIs) with size \(H H\) in the feature maps (\(R=3\) in Figure 2 for illustration). The ROI is defined as any region containing centerline points. The adoption of ROIs brings two key benefits: it reduces the model's learning complexity due to simpler topological structures within each ROI, and improves training efficiency by decreasing the number of feature tokens processed in the transformer. Technically, We adopt ROI Align  to extract multi-scale ROI features. Note that the generality of GraphMorph allows it to be adapted to any type of segmentation network. In the experimental part, we validate the enhancement of GraphMorph on a variety of segmentation networks.

### Graph Decoder

The Graph Decoder is intended to predict the graph for each ROI. Specifically, the modified Deformable DETR  is responsible for detecting the nodes, while the link prediction module handles predicting the connectivity among these nodes. In the following, we will dissect each component to elucidate their roles.

Figure 2: Overview of the training process. Given an image, the segmentation network outputs a probability map of the centerline or segmentation and produces multi-scale feature maps. Then, \(R\) regions of interest (ROIs) are randomly sampled from the image, and their corresponding features are fed into the Graph Decoder, which predicts the nodes within these ROIs using a modified Deformable DETR and outputs the adjacency matrices utilizing the proposed link prediction module.

**Modified Deformable DETR.** We have made two adjustments to the standard Deformable DETR . Firstly, since the targets are nodes with only 2-dimensional coordinates, we replace the original box head with a coordinate head that outputs 2-dimensional vectors. Secondly, considering the typically small size of ROIs, we reduce the number of layers in the transformer encoder to three while keep the decoder at six layers. Note that each ROI is treated as an independent sample and different ROIs will not interact with each other in the whole training process. Formally, the process of node prediction can be expressed as follows:

\[^{r} =(F^{r},PE)\] (1) \[^{r} =(^{r},Q)\] (2) \[^{r} =((^{r} )),^{r}=( (^{r}))\] (3)

where \(r=1,2,...,R\). \(F^{r}^{L C}\) denotes the multi-scale features of the \(r\)-th ROI, and \(Q^{K C}\) is the initial node queries, where \(L\) and \(K\) are the scale of feature maps and the number of node queries respectively. \(PE\) is the multi-scale sinusoidal positional encoding used in . \(\) is a single linear layer, and \(\) is a 3-layer multilayer perceptron (MLP). \(^{r}^{K}\) and \(^{r}^{K 2}\) are the classification scores and coordinates of the nodes for the \(r\)-th ROI respectively.

**Link prediction module.** Since the number of nodes for each ROI may be different, we design a dynamic module generating linear weights and biases conditioned on node features to directly predict the adjacency matrix \(A\). For the \(r\)-th ROI sample, \(^{r}^{K C}\) is the output queries of the Transformer decoder. The queries matched with the ground truth nodes (the number is denoted as \(P_{r}\)) during the bipartite matching process are preserved, and the rest queries are filtered out. We denote the kept queries as \(^{r}^{P_{r} C}\). As depicted in Figure 2, the matched queries \(^{r}\) will be fed into two MLPs. The \(\) generates a (\(C+1\))-dimensional vector for each matched query, which serves as the weights and biases of the condition linear layer. The \(\) maps the queries to a value space. With the values as input, the condition linear layer of the \(p\)-th query generates the adjacency list of it. The process can be formulated as follows:

\[W_{p}^{r}=(_{p}^{r}) ^{C+1}, V^{r}=(^{r})^{P_{r} C}\] (4) \[_{p}^{r}=([W_{p}^{r}]_{1:C} V ^{r}+[W_{p}^{r}]_{C+1})^{P_{r}}\] (5) \[^{r}=[(_{1}^{r})^{T},(_{2 }^{r})^{T},...,(_{P_{r}}^{r})^{T}]^{T}^{P_{r} P _{r}}\] (6)

where \(p=1,2,...,P_{r}\). Here, \(_{p}^{r}\) denotes the \(p\)-th item of \(^{r}\), and \(C\) is its dimension. \(W_{p}^{r}\) refers to the linear parameters conditioned on the \(p\)-th matched query. \(_{p}^{r}\) represents the predicted adjacency list for the \(p\)-th matched query, and the concatenation of all lists forms the final adjacency matrix \(^{r}\).

### Training Details

**Graph construction.** To train the Graph Decoder, we represent the ground truth of each ROI as a graph (see Figure 2). The detailed graph construction process can be found in Appendix B.

**Label assignment based on bipartite matching.** Bipartite matching is widely used in solving set prediction problems [2; 49; 40]. As in , we first calculate the cost between the predicted and ground truth nodes. The predicted nodes are denoted as \(=\{(_{k},_{k})\}_{k=1}^{K}\), where we omit the index \(r\) of the ROI sample for simplicity. Under the general assumption that \(K\) is larger than the number of ground truth nodes \(P_{r}\), thus we pad the set of ground truth nodes with \(\) (no node) to achieve a size of \(K\). The ground truth set can be denoted as \(y=\{(c_{i},v_{i})\}_{i=1}^{K}\), where \(c_{i}\) is the target class label and \(v_{i}^{2}\) is the coordinate of the node. For a permutation \(_{K}\), where \(_{(i)}\) is assigned to \(y_{i}\) (\(i=1,2,..,K\)), we define the cost between \(y_{i}\) and \(_{(i)}\) as:

\[_{}(y_{i},_{(i)})=_{ }_{\{c_{i}\}}_{}(_{(i)})+_{}_{\{c_{i}\}} _{}(v_{i},_{(i)})\] (7)

where \(_{}\) and \(_{}\) are hyperparameters. \(_{}(_{(i)})=_{}(_{(i)},1)-_{}(_{(i)},0)\), \(_{}(s,c)\) is defined as \(-(1-s)^{}(s)\) if \(c=1\), and \(-(1-) s^{}(1-s)\) if \(c=0\), where \(\) and \(\) are hyperparameters. \(_{}\) is commonly used \(_{1}\) loss. The optimal \(\) is defined as

\[=*{arg\,min}_{_{K}} _{i=1}^{K}_{}(y_{i},_{(i)})\] (8)This optimal assignment can be efficiently obtained by Hungarian algorithm .

**Loss functions.** To train the Graph Decoder, the overall loss function is comprised of three components: pixel-wise loss \(_{}\) between the probability map and the ground truth binary mask (in this work, we use softDice  and clDice ), Hungarian bipartite matching loss \(_{}\) between the predicted and ground truth nodes, weighted-BCE loss \(_{}\) between the predicted and ground truth adjacency matrices of the matched queries. For an image with \(R\) ROI samples, the last two loss functions are defined as:

\[_{}(y,)=_{r=1}^{R}_{i=1}^{K}[ _{}_{}(_{(i )}^{r},c_{i}^{r})+_{}_{\{c_{i}^{r} \}}_{}(_{( i)}^{r},v_{i}^{r})],\] (9)

\[_{}(y,)=_{r=1}^{R}\{}}_{i j}^{P_{r}}_{j=1}^{P_{r}}(A_{ij}^{r}_{ij}^{r})+}}_{i j}^{P_{r}}_{j=1}^{P_{r}}[(1-A_{ ij}^{r})(1-_{ij}^{r})]\},\] (10)

where \(N_{}\) is the total number of positive locations in ground truth \(\{A^{r}\}_{r=1}^{R}\), and \(N_{}\) is the total number of negative locations. Thus, the overall loss function is \(_{}=_{}+_{}+_{}\).

### Morph Module and Inference

The Morph Module is used to get topologically accurate centerline masks, by morphing the predicted graphs from the Graph Decoder. In this subsection, we first introduce the Morph Module, followed by the inference processes for the centerline extraction and segmentation tasks.

**Morph Module.** We present the algorithmic flow in Algorithm 1. In particular, \(G=\{V,E\}\) is the graph of an image patch (same size as an ROI sample), and \(P_{m}\) is the probability map of centerlines obtained from the segmentation network. We iterate over each edge and use our proposed \(\) algorithm to find the optimal path with minimum cost. The union of these paths forms the final centerline mask.

\(\) is modified from Dijkstra algorithm . We have made two key adaptations for centerline extraction: (1) To restrict the path to a single pixel width, ensuring the property of the skeleton, we mandate that all path points, except for the start and end points, satisfy \(N=2\) (where \(N\) is the number of centerline points in its eight neighbours, see Appendix B). (2) To suppress potential false-positive edges from the Graph Decoder, we exclude the paths with an average cost exceeding a threshold \(p_{thresh}\). These refinements optimize the algorithm to yield topologically accurate centerline masks. The detailed algorithmic flow of \(\) can be seen in Algorithm 2 in Appendix D.

```
0: Node set \(V\), Edge set \(E\), Probability map \(P_{m}\)
0: Centerline mask \(M\)
0: Initialize \(M\) as a zero matrix with the same size as \(P_{m}\)
0: Initialize \(C_{m}\) where \(C_{m}[i][j]=1-P_{m}[i][j]\) for each element for all edges \((u,v)\) in \(E\)do \(path\)SkeletonDijkstra(\(u\), \(v\), \(C_{m}\), \(p_{thresh}\)) for all points \(p\) in \(path\)do  Set \(M[p.x][p.y]=1\) endfor endfor return\(M\) ```

**Algorithm 1** Morph Module

**Inference of centerline extraction.** As depicted in Figure 3, the centerline extraction process begins with generating a centerline probability map via the segmentation network. Then, sliding window inference is employed across the entire image in Graph Decoder to obtain graphs for all split patches. Finally, the Morph Module produces the centerline mask for each patch, and the combination of these masks forms the complete centerline mask of the entire image.

**Inference of segmentation.** Since the segmentation probability map \(S_{m}\) can not be used directly by the Morph Module, we first threshold \(S_{m}\) to obtain segmentation mask \(S_{m}^{}\) and skeletonize it into a centerline mask \(P_{m}^{}\). The distance from each pixel to the nearest centerline point in \(P_{m}^{}\) is calculated and normalized to create a distance map \(D\). Then the centerline probability map \(P_{m}\) is obtained by \(P_{m}=S_{m}(1-D)\). Employing the Morph Module on \(P_{m}\) yields a topologically precise mask \(M\). To suppress false positives in \(S^{}_{m}\) (especially isolated regions), a post-processing strategy is initiated from \(M_{0}=M S^{}_{m}\). This strategy involves iteratively expanding \(M_{0}\) within the boundaries of \(S^{}_{m}\) until stabilization. The stabilized mask \(M_{T}\) is then taken as the final output. This approach, as confirmed by experiments, effectively diminishes false positives and enhances topological accuracy. The above-mentioned soft skeletonization method (from \(S_{m}\) to \(P_{m}\)) and post-processing strategy introduce minimal time cost and are straightforward to implement, with detailes in Appendix E.1 and Appendix E.2.

## 4 Experiments

### Experimental Setup

**Datasets.** We evaluate GraphMorph on three medical datasets and one road dataset. DRIVE  and STARE  are retinal vessel datasets commonly used in medical image segmentation. ISB112  contains 30 Electron Microscopy images to segment membranes. The Massachusetts Roads (Mass-Road) dataset contains 1171 aerial images for road network extraction. We use the data splits for DRIVE and STARE provided in MMSegmentation . For ISB112, following previous works [35; 29], we split it into 15 images for training and 15 for testing. For MassRoad, we follow  to construct the training set, and the total 63 images of the official validation set and test set are used for testing.

**Baselines.** We adopt affluent baselines for comparison, including UNet , ResUNet , CSNet , DC-UNet , TransUNet , DSCNet  and PointScatter . Particularly, we use LinkNet34  and D-Linknet34  as baselines for the MassRoad dataset. In addition, we compare with TopoLoss , which is a topology-based loss function.

**Metrics.** For the centerline extraction task, we use Dice , Accuracy (ACC) and AUC as volumetric metrics. For robust evaluation, we give a tolerance of a 5-pixel region around the ground truth centerline mask following . We compute topological metrics following , including the mean absolute errors of \(_{0}\), \(_{1}\) and the Euler characteristic. To compare fairly, we skeletonize the prediction before evaluation. For the segmentation task, we adopt Dice, clDice  and ACC as volumetric metrics and the same topological metrics as the centerline extraction task. Moreover, ARI (Adjusted Rand Index)  and VOI (Variation of Information)  are used to evaluate clustering similarity.

**Implementation Details.** For three medical datasets, we use randomly cropped \(384 384\) images for training. The size of ROI samples \(H\) is \(32\) and the stride of sliding window used in inference process is \(30\). For MassRoad dataset, the cropped size is \(768 768\), \(H=48\) and the stride is 45. For all experiments, we use 64 ROI samples per image (\(R=64\)) to train the Graph Decoder, and the number of node queries in the modified Deformable DETR is set to 100 (\(K=100\)). According to previous

Figure 3: Inference process of centerline extraction. First, the segmentation network generates a centerline probability map \(P_{m}\) along with multi-scale image features. Subsequently, the Graph Decoder utilizes the image features to predict graphs \(G\) via sliding window inference. Finally, the Morph Module employs \(P_{m}\) to find the optimal path between each pair of connected nodes in \(G\), resulting in a final centerline mask. This approach achieves higher topological accuracy compared to direct thresholding of \(P_{m}\).

experiences , the default hyperparameters used in loss functions are as follows: \(_{}=0.2\), \(_{}=0.5\), \(=0.6\), \(=2\). We use \(=0.75\) for MassRoad due to the sparse nature of the road networks. For all types of segmentation networks, we use multi-scale features ranging from the lowest resolution to the \(4\) downsampling of the original image as input of the Graph Decoder. More implementation details are introduced in Appendix A.

### Main Results

We first verify the effectiveness of GraphMorph on the centerline extraction task. Then, considerable experiments are conducted on the more common segmentation task, demonstrating the powerful topological modelling capability of GraphMorph.

**Centerline Extraction.** In our experiments with UNet and softDice loss on four public datasets, detailed in Table 1, the inclusion of the Graph Decoder during training enables the network to learn branch-level features, leading to enhanced performance in both volumetric and topological metrics. During inference, the utilization of Morph Module results in a slight decrease in volumetric metrics; however, there is a notable enhancement in topological metrics, confirming that our network has effectively captured branch-level features of tubular structures. Overall, the combined use of the Graph Decoder and Morph Module showcases the ability to refine the segmentation network's performance, particularly in preserving the crucial topological characteristics. Our methods also beat previous SOTA Pointscatter  by a large margin..

**Segmentation.** In the initial phase of our segmentation experiments, we assessed the effectiveness of our method across different segmentation networks and datasets. As detailed in Table 2, enhancements were evident in various metrics for all dataset and network combinations. The improvements in

    &  &  &  &  \\   & & Dice & AUC & ACC & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) \\   & UNet  & 0.7353 \(\) 0.0127 & 0.9313 \(\) 0.0089 & 0.9768 \(\) 0.0013 & 2.169 \(\) 0.112 & 1.590 \(\) 0.107 & 2.537 \(\) 0.139 \\  & PointCiner  & 0.7381 \(\) 0.0133 & 0.9041 \(\) 0.0078 & 0.9775 \(\) 0.0013 & 3.295 \(\) 0.153 & 2.080 \(\) 0.102 & 3.500 \(\) 0.176 \\  & softDice  + Graph Decoder & **0.7960 \(\) 0.0127** & **0.9481 \(\) 0.0082** & **0.9778 \(\) 0.0012** & 1.5259 \(\) 0.004 & 1.382 \(\) 0.106 & 1.599 \(\) 0.125 \\   & SoftDice  & 0.6428 \(\) 0.0104 & 0.8977 \(\) 0.0035 & 0.9775 \(\) 0.0013 & 0.988 \(\) 0.277 & 2.165 \(\) 0.112 & 2.494 \(\) 0.250 \\  & PointCiner  & 0.5648 \(\) 0.0099 & 0.9104 \(\) 0.007 & **0.947 \(\) 0.0013** & 0.988 \(\) 0.277 & 2.165 \(\) 0.124 & 5.418 \(\) 0.290 \\  & softDice  + Graph Decoder & 0.5686 \(\) 0.0095 & **0.9240 \(\) 0.0067** & 0.9742 \(\) 0.0012 & 0.913 \(\) 0.179 & 2.725 \(\) 0.112 & 4.295 \(\) 0.159 \\  & softDice  + Graph Decoder & **0.6687 \(\) 0.0092** & 0.9742 \(\) 0.0041 & 0.9742 \(\) 0.0042 & **0.6645 \(\) 0.009** & **1.2070 \(\) 0.007** & **0.858 \(\) 0.499** \\   & SoftDice  & 0.7119 \(\) 0.0392 & 0.9390 \(\) 0.0323 & 0.9899 \(\) 0.0012 & 1.874 \(\) 0.199 & 1.209 \(\) 0.121 & 2.063 \(\) 0.162 \\  & PointCiner  & 0.7224 \(\) 0.0144 & 0.9341 \(\) 0.0179 & 0.9896 \(\) 0.0012 & 1.898 \(\) 0.002 & 1.849 \(\) 0.166 & 2.123 \(\) 0.166 \\  & softDice  + Graph Decoder & **0.7296 \(\) 0.0428** & **0.9356 \(\) 0.0248** & **0.9356 \(\) 0.0011** & 1.467 \(\) 0.131 & 1.074 \(\) 0.101 & 1.654 \(\) 0.132 \\  & softDice  + Graph Decoder & **0.7296 \(\) 0.0428** & **0.9356 \(\) 0.0248** & **0.9368 \(\) 0.0011** & 1.467 \(\) 0.131 & 1.074 \(\) 0.101 & 1.654 \(\) 0.132 \\  & softDice  + Graph Decoder & **0.7291 \(\) 0.0387** & **0.9054 \(\) 0.0032** & **0.9064 \(\) 0.0011** & **1.345 \(\) 0.042** & **0.979 \(\) 0.007** & **0.653 \(\) 0.059** \\   & SoftDice  & 0.6339 \(\) 0.0169 & 0.9718 \(\) 0.047 & **0.9492 \(\) 0.0069** & 1.671 \(\) 0.0066 & 1.627 \(\) 0.007 & 1.567 \(\) 0.087 & 1.568 \(\) 0.097 \\  & PointCiner  & **0.6456 \(\) 0.0149** & 0.9604 \(\) 0.0042 & **0.9422 \(\) 0.0069** & 1.331 \(\) 0.124 & 1.553 \(\) 0.036 & 3.429 \(\) 0.149 \\  & SoftDice  + Graph Decoder & **0.6239 \(\) 0.0175** & **0.9731 \(\) 0.0045** & **0.9491 \(\) 0.0000** & 1.931 \(\) 0.0045 & 1.729 \(\) 0.008 & 2.229 \(\) 0.105 \\  & softDice  + Graph Decoder & 0.6388 \(\) 0.0168 & / & **0.9942 \(\) 0.0069** & **0.6202 \(\) 0.0012** & **1.355 \(\) 0.083** & **1.122 \(\) 0.075** \\   

Table 1: Centerline extraction performance on four public datasets based on UNet.

   Dataset &  &  &  &  &  \\   & & Dice & AUC & ACC & AUC & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) \\   & UNet  & softDice  & 0.6316 \(\) 0.0099 & 0.8132 \(\) 0.0169 & 0.9551 \(\) 0.0033 & 0.75 \(\) 0.013 & 0.38 \(

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_EMPTY:10]