# For what purpose was the dataset created?

[MISSING_PAGE_EMPTY:1]

Yet, despite the potential impacts of transferable foundation models for tabular data , the core practices of machine learning on tabular data have remained largely unchanged. The prevailing paradigm is still to train single-task models (e.g., XGboost ) using a fixed schema on data from the same distribution on which the model will be deployed.

Here, we aim to bridge this gap. We introduce TabuLa-8B, a language model for tabular prediction which can flexibly solve classification tasks across unseen domains, including where data is _scarce_. Our methodology expands the scope what is possible in these settings, thereby democratizing access to prediction in low-resource contexts and providing state-of-the-art, training-free transfer learning on any tabular data. Since the model only requires a forward pass to perform inference on the target data, it also avoids the privacy or computational considerations that arise in other approaches that require fine-tuning on local, and potentially sensitive, datasets.

In particular, given a small number of examples (shots), _and without any fine-tuning on the task_, TabuLa-8B outperforms state-of-the-art gradient-boosted decision trees and tabular deep learning methods _that are explicitly trained on the target data_ (see Figure 1). Furthermore, TabuLa-8B is capable of zero-shot prediction, a behavior which is not possible using these prior methods. To enable these results, we build a new dataset for tabular prediction, The Tremendous TabLib Trawl (T4), that allows us to scale up training by several orders of magnitude (10,000\(\) more data) relative to previous work.

### Our Contributions

This paper has the following three main contributions:

**TabuLa-8B, a Tabular Prediction Model:** We build TabuLa-8B (Tabular Llama 3 - 8B), a model for prediction on tabular data. On an evaluation suite consisting of 329 tables drawn from five tabular benchmarks, TabuLa-8B has zero-shot accuracy 17 percentage points (pp) above random guessing. In the few-shot setting (1-32 examples), TabuLa-8B is 5-15 pp more accurate than state-of-the-art methods (XGboost, TabPFN, CatBoost) that are trained on equal number of shots, and these methods require 2-8\(\) more data to achieve the performance of our model. TabuLa-8B outperforms a variety of strong tabular baselines and even commercial LLMs such as Claude Instant and Claude 3 Sonnet.

**T4 - A Large Scale, High Quality Training Dataset:** We build and release The Tremendous TabLib Trawl (T4), a filtered collection of 4.2M unique tables (consisting of over 2.1B rows, a total of 100B tokens) from TabLib . We detail the recipe used to construct T4, including a suite of methods for filtering web-scale tabular data at several levels (table, row, column), removing unwanted information such as PII and code, and selecting unsupervised _prediction targets_ from these tables.

**Open-Source Release:** As part of our publication, we release all relevant infrastructure (code, models, and data) with the hopes that the community will build on our work. We provide high-quality, efficient implementations of data pre-processing and model training pipelines, including our new row-causal tabular masking (RCTM) attention and packing scheme for training on tabular data. We also share the code used to filter T4 from TabLib, enabling future work that extends our dataset construction methodology.

### Preliminaries & Project Scope

Our work is concerned with _prediction_ models for _tabular data_. We define both below.

**Tabular Data:** For our purposes, tabular data has three main properties. \((i)\)_Structured_: It consists of elements with a "key-value" structure, often represented as a table with keys (or "headers") representing column names, and rows that consist of values. \((ii)\) Heterogeneous: The values are of mixed types, including numeric, boolean, categorical, ordinal, text, date/time, etc. Missing values may be present. \((iii)\) Exchangeable: The ordering of rows and columns in the dataset is arbitrary. In particular, any permutation of the rows, or columns, still represents the same tabular dataset.

**Prediction Task Definition:** The main focus of this work is _prediction_ on tabular data. In tabular prediction, the goal is to predict the value \(y\) of a specific target column for a row in a dataset using the key-value pairs \(x\) from all other columns. More specifically, we focus on classification tasks where values \(y\) for the target column belong to a finite set \(C\). Binned regression tasks, in which a real-valued \(y\) is discretized into a finite set of numeric values (as in ) also fit this definition.

Related Work

Our work builds on a line of foundation modeling, tabular data prediction, and natural language processing research. Given space constraints, here we focus on the most closely related literature.

**Transfer Learning and Foundation Models:** The idea of building general purpose models via autoregressive next-token prediction on large scale datasets was pioneered in a series of papers in natural language processing [6; 11; 36; 40; 57]. These results have since led to the development of foundation models capable of solving diverse tasks in other modalities including vision [38; 63], audio [39; 66], code [22; 42], time series [10; 18; 21], and graphs , as well as multi-modal models [53; 55]. Our work also build upon on the demonstrated capacity of transformers to perform few-shot or in context-learning [6; 17], which entails making predictions on examples from a previously unseen dataset, given only a few labeled examples from that task.

**Large-Scale Dataset Curation:** The construction of large, high-quality datasets has emerged as one of the most critical, and challenging, issues in the development of transferable models. Several major milestones in this space [6; 40; 41; 53; 54; 55; 41; 56] stand out in their effort spent curating and cleaning web-scale datasets - often while using a model architecture and training recipe that only slightly differs from prior work. This has led to a number of modality-specific methods for large-scale dataset curation; for example, the use of heuristics  and model-based quality scoring to select high-quality text data [6; 9; 56]; methods for selecting aligned audio-transcript pairs for speech ; or the use of CLIP scores to filter for aligned image-text pairs . However, to the best of our knowledge no prior work has developed analogous methods for _tabular_ data. This lack of large-scale training data has been a critical bottleneck toward the development of tabular foundation models.

**Models for Tabular Prediction:** Despite the fact that deep learning methods are now the norm in domains such as computer vision or NLP, methods based on gradient-boosted decision trees (GBDTs) [5; 37; 7] continue to be at or near state-of-the-art in tabular prediction . Drawing upon recent breakthroughs in other modalities, the field has now developed deep learning-inspired approaches [19; 52] that are competitive with tree-based models _in-distribution_, where models are trained and evaluated on the same data, but the benefits of such approaches relative to GBDTs appears to be limited in practice [20; 33]. In particular,  introduces TabPFN, a transformer model for tabular data that outperforms XGBoost in certain regimes  and is capable of making predictions on unseen datasets (with some constraints on dataset size and label space; see D.2). A related recent work, CARTE , explores the use of graph-based architectures for tabular transfer, based on key-value encodings and pretrained on a large knowledge graph.

Several recent works [12; 23; 59; 66] have explored fine-tuning LLMs on individual tables, or on small collections of tables (\(<200\)). The main idea in this closely related line of work is to reduce classification to next-token prediction by first _serializing_ a row as text (see Figure 1(b) for an illustration) and then training an LLM to predict the serialized labels. These studies demonstrate that this LLM approach is often competitive with trees or tabular deep learning methods in-distribution [12; 23; 59]. However, in cases where models were evaluated out-of-distribution, they were less accurate than SOTA methods trained on these held-out tables . Our work builds on this promising line of work. Relative to these prior efforts, we specifically address the (1) lack of large-scale and training data; and (2) the inability of exiting methods to be competitive when evaluated out-of-distribution.

## 3 TabuLa-8B - Model Design and Training

Our overall approach is to fine-tune the pretrained Llama 3-8B language model  on tabular prediction tasks using a new web-scale corpus, T4. We use Llama 3-8B as our starting point since it is a high-quality, open-source model trained on over 15T tokens that demonstrates strong performance on a diverse set of downstream tasks , particularly at its relatively modest size (which makes fine-tuning, inference, and deployment more accessible).

**Serialization and Tabular Language Models:** As discussed previously, our methodology extends ideas pioneered in previous work [12; 23; 59; 65] demonstrating how LLMs can be trained to perform tabular prediction tasks by serializing rows as text, converting the text to tokens, and then using the same loss function and optimization routines used in language modeling. _Serialization_ refers to the procedure of converting a row of data into text, for instance by concatenating substrings of the form "the <key> is <value>". Prior works investigated the impact of different serialization formats, demonstrating that performance is largely insensitive to the exact mapping (e.g. using "{ <Key>: <value> }") and other strategies do not improve upon this "the <key> is <value>" structure.

We adopt a similar serialization strategy, illustrated in Figure 1(b). Given a row of data from a table, the corresponding serialization has three main parts: \((i)\) a _prefix_ containing a prompt (always "Predict the value of <target column name>") followed by a list of possible label values ("val1 ||... || valNumClasses ||), \((ii)\) the _example_ consisting of all key, value pairs for the columns used as features, and \((iii)\) a _suffix_ prompting the model with a question ("What is the value of <target column name>?") again followed by the possible labels. For multiple-shot samples, we concatenate their serializations. We introduce three special tokens into the Llama 3 vocabulary to ensure these sequences are properly tokenized: ||, to delimit answer choices; <|endinput|> to denote the end of an input sequence (the last token before the targets or model generation begin); and <|endcompletion|> to indicate the end of a completion.

**Training Procedure:** We train TabuLa-8B using a standard language modeling setup where the model is trained to minimize the cross-entropy over the sequence of target tokens. We only compute loss over the subsequence of target tokens: the tokens starting after the <|endinput|> token, up to and including <|endcompletion|>. This objective focuses training on learning the desired target label, as in , rather than developing a broader generative model of tabular data as in .

Relative to prior studies on tabular prediction with LLMs, our work has one main methodological innovation. We introduce an efficient attention masking scheme, row-causal tabular masking (RCTM), tailored to few-shot tabular prediction whereby the model is allowed to attend to all previous samples from the same table in a batch, but _not_ to samples from other tables (this is sometimes referred to as "cross-contamination" in the language modeling literature ). However, by appropriately masking out values, RCTM also enables packing examples into the same batch (as effectively zero padding is required during training despite the large variance in the size of each tokenized table or row), thereby increasing model throughput (see Figure 1(a)). Taken together, these have the effect of training the model to use multiple "shots" during training and mitigates the potential loss of few-shot learning capabilities that has been observed to occur during fine-tuning .

The RCTM masking structure is shown in Figure 1(a). Lower-triangular blocks correspond to rows from the same table that are present in the batch. This is similar to the "in-context pretraining" method from , except that \((i)\) our procedure encourages the model to aggregate information across multiple _rows_ of a given _table_, rather than attending across documents, and \((ii)\) our procedure only trains the model to predict the _target_ tokens, not the input features. We show that RCTM has a drastic impact on few-shot performance through an ablation experiment (see Section F.1).

**Training Details:** The final model is trained for 40k steps with a global batch size of 24 (with sample packing, this is roughly equivalent to a global batch size of 600 rows of tabular data). The model

Figure 1: 1(a): Illustration of the row-causal tabular mask (RCTM) representing a batch during training. Each triangular block represents potentially many rows from a _single_ table (detail shown at left). Shaded groups within this block represent tokens from one row in the table. This structure implicitly trains the model for few-shot learning by permitting it to attend to previous rows from the table, but not to rows in other tables. 1(b): Serialization of tabular data into text. The model is trained to produce the tokens following the <|endinput|> token.

sees roughly 8B tokens during training; we note that is less than 10% of the 100Btokens in T4, and less than one one thousandth of TabLib itself. We fully fine-tune all model parameters, as opposed to parameter-efficient fine-tuning, since full-fine tuning consistently benefits from scale [24; 64]. Reproducibility details are given in Appendix B.

## 4 Dataset Construction: Building The Tremendous TabLib Trawl (T4)

Beginning from a web-scale corpus of raw data (TabLib), we apply various filters to produce a high-quality subset, and transform the results into a set of prediction tasks for training. As the result of this procedure, we produce T4 (The Tremendous Tablib Trawl), which we release with this paper.

**Original Raw Data Source:** TabLib  is a publicly-available dataset consisting of 627M tables extracted from two main sources: Common Crawl and Github (see  for more details). Due to its scale and diversity, TabLib presents a unique opportunity for training foundation-scale models on the tabular data. However, like other web-scale datasets, the vast majority of its contents are low quality and not suitable for training. For instance, TabLib contains numerous system logs with inscrutable statistics, tables of software documentation, and call sheets with personally identifiable information (PII). To the best of our knowledge, no previous work has addressed the task of filtering TabLib into a usable training set, and no publicly-available models have been trained on this corpus.

**Filtering Strategies:** Filtering large collections of raw data to extract a higher-quality subset is an essential component in the development of foundation models [e.g. 41], yet to date, no previous work has addressed this core issue for tabular data. Filtering a web-scale dataset like TabLib into a usable subset of high-quality tables is critical to leverage its diversity and scale, but also raises unique challenges specific to tabular data, such as missing data, web "content" that is formatted as HTML tables that does not satisfy our definition of tabular data, and PII. To turn TabLib into a usable training set, we develop a set of filtering methods to identify high-quality tables for prediction. Conceptually, our filtering occurs at three levels, each applied sequentially: _tables_ (entire tables are removed from the pool), _columns_ (individual columns are removed from a table), and _rows_ (rows are removed from a table).

Similar to previous approaches [38; 40; 54; 56; 41], we use a mix of heuristics and rule-based methods to remove low-quality sources from our pool. We present the full list of our filtering rules in Section A. At a high level, our emphasis across all filtering strategies is to: (1) remove non-tabular data (such as text or PDFs incorrectly identified as tabular data during TabLib's collection), (2) ensure the _safety_ of chosen tables (e.g. by removing PII), and (3) find sources with _high semantic content_ (e.g. by removing tables with too many missing values). As part of this filtering process we develop and apply simple methods for deduplication, English language filtering, filtering for missing data, PII removal, code removal, and more.

**Unsupervised Task Selection:** As described in Section 1.2, we focus on methods for tabular prediction: predicting the value of a target column given the values of all other columns for an instance. Therefore, as part our data pipeline we develop new methods for selecting, in an unsupervised fashion, which column is the _target column_ for each table in the corpus. Selecting targets of prediction for tabular data at scale is an under-explored problem. Prior work in this space operated on at most a few hundred tables and used either a combination of expensive queries to commercial LLMs or manual curation to identify tabular prediction targets [59; 65]. However, when operating on hundreds of millions of distinct tables with potentially no associated metadata, these strategies are not feasible.

For each table, we select a prediction target programmatically by first identifying a subset of columns that are suitable for prediction according to various heuristics, and then choosing a specific column

Figure 3: Sketch of dataset generation pipeline. 627M tables from TabLib  are filtered by applying rules at the table, row, and column level. Then, for each table, we identify valid and high-quality prediction targets in an unsupervised manner and use the results for training TabuLa-8B.

at random from this set. The exact list of heuristics to arrive at this set is presented in Appendix A. Amongst others, these include excluding candidate columns if: the column name is numeric, it has only one unique value, or it has unique values for every row (excluding numeric columns).

**Final T4 Dataset Summary:** Running this entire filtering process (from raw data to serialized examples ready for training) on all 70TB of TabLib using our open-sourced implementation takes about 4 hours on a CPU cluster. It yields a total of 4.2M tables, which equates to a table filtering rate of approximately 97.91%. Additional descriptive statistics for the dataset are given in Appendix A.3. The resulting dataset contains over 2.1B rows (approximately 100B Llama 3 tokens) for training of the downstream model, and occupies roughly 2TB compressed on disk. We note that 100B tokens is larger than the total number of tokens TabuLa-8B sees during training. Therefore, the model sees each distinct table at most once during training, and our pipeline could be scaled up to support larger models or longer training runs.

## 5 Experimental Results

### Evaluation Methodology

We evaluate the transfer learning performance of TabuLa-8B on a diverse set of established benchmarks previously considered in prior work (see Section 5.2 for a list). For each dataset, we use the predefined prediction target from the original benchmark. Due to computational constraints, we evaluate TabuLa-8B on up to 128 test examples for each dataset and number of shots \(k\).

The term "few-shot" is unfortunately overloaded. It is used both to refer to models that make predictions on instances never seen during training, and to models that directly _train_ on these examples before predicting on unseen samples. We do not fine-tune our model on test examples, in contrast to . Our methodology only requires performing forward passes through the network to generate predictions and avoids the need for computationally-expensive gradient updates. In zero-shot evaluations, given a row of a dataset along with the corresponding set of columns and possible labels, we first serialize the row into the same format used during training, and feed it into the model to generate a prediction following the <|endinput|> token. For few-shot evaluations, we perform the same procedure, except that we preprend the serialized "shots" as in Figure 2b.

In contrast to methods like XGBoost that directly predict likelihoods of a set of labels, language models output likelihoods over a set of tokens (128k in the case of Llama 3). For each evaluation dataset, the values in the label set (e.g. "sun, rain, snow" in Figure 2b) can consist of a _sequence_ of many individual tokens from this large vocabulary. Here, we use _open-vocabulary_ (or "open-ended") accuracy  as the main evaluation metric for our model. In this setup, once the model is prompted with a serialized example, it is allowed to generate an arbitrary sequence of tokens. Once it produces the <|endofcompletion|> token, the generated text is then directly compared to the correct completion. Only an exact match, including the terminating <|endofcompletion|> token, is counted as accurate. This is more challenging than _closed_-vocabulary evaluation, where the model is only rated on assigning the highest probability to the correct completion from a predetermined set.

### Evaluation Datasets

We evaluate our model's predictive performance across a collection of 329 publicly-available tabular datasets drawn from five tabular benchmarks (see Appendix H for a full list). These include:

**UniPredict Benchmark (169 datasets) :** We use the "supervised" subset of 169 datasets from the recently-introduced UniPredict benchmark. These are high-quality tabular datasets with generally informative column names and a mix of both categorical and continuous targets, drawn directly from Kaggle. While the model introduced in Wang et al.  was trained and tested on separate splits of these datasets, we only use them for testing. We make corrections to several datasets with targets erroneously treated as categorical in the original benchmark, described in Section 5.2.

**Grinsztajn Benchmark (45 datasets) :** The Grinsztajn benchmark is a curated suite of datasets consisting of numeric and categorical features. This dataset is notable in that the original study by Grinsztajn et al. found that gradient boosted decision trees (GBDTs) consistently outperformed deep learning-based methods on these tasks.

**AutoML Multimodal Benchmark (AMLB) (8 datasets)  :** A suite of tables which include one or more free-text fields (such as an Airbnb description, or a product review). The benchmark is considered challenging for tree-based methods due to the non-standard text-based features. However, it also poses a challenge for LLMs since some columns can contain highly variable lengths of text.

**OpenML CC-18 Benchmark (72 datasets) :** The OpenML Curated Classification Benchmark was created by applying filtering rules to extract a high-quality subset from the OpenML platform. The rules include: no artificial data sets, no subsets of larger data sets nor binarizations of other data sets, no data sets which are perfectly predictable by using a single feature or a simple decision tree.

**OpenML CTR-23 Benchmark (35 datasets) :** The OpenML Curated Tabular Regression (CTR) Benchmark is a curated set of tables for regression drawn from OpenML. The curation process is similar to that of OpenML-CC18, for regression tasks. We note that, being primarily intended for the evaluation of AutoML methods, the OpenML benchmarks are notable for lacking informative column names (i.e. names such as "Var1, Var2,..." are common in OpenML benchmark datasets).

We transform all regression tasks into a 4-class classification based on quartiles, as in  (see Appendix A.2 for details). Many datasets contain rows with missing data. We leave these as-is and do not remove any data. On a computational note, some datasets contain a large numbers of features and performing \(k\)-shot evaluations on these datasets at large \(k\) can exceed the model's context window. Therefore, in few-shot evaluations, we always report results for the subset of datasets where \(k\) shots fit into the model's context window, for the entire specified range of \(k\). We provide more details on the evaluation datasets in Appendix D.1 and report per-dataset results in Appendix H.

### Baselines

When inspecting TabuLa-8B's performance, we compare against the following baselines:

**Llama 3-8B :** This is the base model from which TabuLa-8B is fine-tuned. Comparing to the base model isolates the effects of the fine-tuning process. It also controls for any contamination of evaluation datasets that may be contained in pretraining data for Llama 3 (the exact training data for Llama 3 are not currently disclosed). We return to this point in Section 5.6.

**XGBoost :** XGBoost is a supervised learning gradient-boosted decision tree (GBDT) method. It is widely considered to be highly competitive in tabular prediction tasks [15; 20; 33].

**TabPFN :** This a transformed-based hypernetwork pretrained to reflect a set of inductive biases germane to tabular data. TabPFN is thus considered especially effective for few-shot learning [25; 33].

Whenever possible, we perform hyperparameter tuning on XGBoost and TabPFN in order to maximize their performance. See Appendix D.2 for further details on baseline implementation and tuning. We also provide results comparing to additional supervised baseline models, and to commercial LLMs, in Section E.2.

### Main Results: Assessing TabuLa-8B's Transfer Learning

We present our main experiments evaluating the transfer learning ability of TabuLa-8B in Figure 4. As a whole, TabuLa-8B demonstrates strong transfer performance across the broad range of tasks.

In the zero-shot regime (seen in the left-most point for each plot in Figure 4) - where the model is presented with no further information about the target dataset except for the serialized key-value pairs and set of possible labels for a single row -TabuLa-8B is between 5 to 25 pp more accurate than a random baseline and 50 pp more accurate than the base Llama 3 model. This illustrates one of the key benefits of using language models for tabular prediction: after fine-tuning, TabuLa-8B can leverage semantic information contained in the serialized data to make high-quality predictions.

While XGBoost and TabPFN are not capable of zero-shot prediction, this behavior has been observed in the original Llama 3 model [54; 57]. However, in our evaluations, Llama 3 performs below random guessing in the zero-shot setting. We hypothesize that the base Llama 3 model requires a small number of samples to understand the input-output format and task (as indicated by the large leap in Llama 3 performance from \(0 1\) shot).

In the few-shot setting, where each method additionally sees a small number of labeled examples, TabuLa-8B's performance steadily improves with the number of shots. In the regime of 1 to 32shots, it outperforms state-of-the-art models (XGBoost and TabPFN) that are directly trained (and hyperparameter tuned) on each specific dataset by 5-20pp. Once we evaluate performance on 32, 64, or 128 shots (see Figure 6(b)), this gap begins to diminish, but the number of datasets that can fit \(>32\) shots into the 8192-token context window is both small and a relatively biased sample (due to their small number of features). TabuLa-8B is consistently 10 to 20pp above the Llama 3-8B base model for the full range of shots, highlighting the benefits of our training procedure on T4.

**Improvements in Sample Efficiency:** As discussed previously, the main benefit of transferable models is that they reduce the amount of data necessary to achieve good performance on new tasks. For instance, as seen in Figure 4, TabuLa-8B only needs one shot to achieve 60% average accuracy on UniPredict tasks. However, both TabPFN and XGBoost only reaches 60% accuracy after 16 shots. Therefore, TabuLa-8B reduces the amount of data necessary to achieve 60% accuracy by 16 fold relative to XGBoost anand TabPFN. We refer to this statistic as the relative sample efficiency (see D.3). TabuLa-8B in general achieves higher accuracy than the benchmarks using less data. Hence, the relative sample efficiency is always \(>1\) (the exact ratio varies across benchmarks).

**Impact of Informative Column Headers:** As shown in Figure 4, while TabuLa-8B generally has higher accuracy than the baselines, this accuracy gap varies across benchmarks and the number of shots. For instance, for the UniPredict benchmark - which was specifically constructed to include datasets with semantically-meaningful column headers  - the gap to supervised baselines is much larger than in the OpenML benchmarks, which tend to have less semantically-meaningful column names. If meaningful column headers are absent, the model still performs well (matching or outperforming XGBoost at shots \(k 8\)), but its advantage over these strong baselines is lessened. We investigate this effect in further detail with a controlled experiment in Section F.2.

Figure 4: Zero- and few-shot accuracy across five tabular benchmarks. For each benchmark, we evaluate on all tasks, but in the figures above we only display the subset of tasks where \(k\) shots fit into the 8192-token context window of TabuLa-8B. Complete results are in Supplementary Section H. The final plot (lower right) shows curves separately over decontaminated vs. potentially-contaminated evaluation tasks (see Section 5.6); we find no impact on our overall findings due to contamination (and performance on tasks which may be in our training set is _lower_ on average, across all models).

### Further Robustness Evaluations and Ablation Experiments

**Robustness to Column Ordering.** Apart from evaluating TabuLa-8B's transfer learning ability, we also investigate its _robustness_ and the degree to which performance is affected by the order in which columns are presented (serialized); this order invariance is cited as a necessary attribute of tabular foundation models in . We present these experiments in Appendix F.4. Our results demonstrate that changing column order does not alter performance in a statistically significant way, but that there may be a small (\( 1\)pp) drop which we hypothesize is due to manual ordering of tabular columns in certain benchmark datasets which sometimes reflects a more "natural" ordering.

**Robustness to Feature Dropout.** Language models may be uniquely susceptible to small changes in the downstream data; for example, the removal of specific features may affect language models' prediction performance more than traditional supervised methods. We conduct an ablation study to assess the behavior of TabuLa-8B as columns are removed from the test data. We assess removal both in order of descending and ascending importance. The results of these experiments, in Appendix F.3, demonstrate that TabuLa-8B's performance declines at a similar rate to an XGBoost model trained directly on the subset of features.

**Robustness to Column Header Removal.** Another potential risk of tabular language models is that, while these models are able to utilize the semantic information in column names, the model may also be overly reliant on the presence of informative column names. In Appendix F.2 we conduct an ablation study to assess this. The results in Appendix F.2 demonstrate that there is a small decline in performance when column headers are removed (replaced with uninformative headers), but that TabuLa-8B still outperforms baselines across all numbers of shots. We believe that this drop in performance is commensurate to the loss in information when column headers are eliminated.

**Importance of Row-Causal Tabular Mask.** We evaluate the impact of the attention masking scheme introduced and described in Section 3. We conduct an ablation study, replacing this component of the model with a sample-wise causal attention (the same form of attention used during standard language model training, where attention across documents is prevented). Our results, detailed in Appendix F.1 and Figure 11, illustrate that this modification is central to the few-shot learning capabilities of TabuLa-8B: when our mechanism is replaced with sample-wise attention the resulting model does not demonstrate few-shot learning capacity, and its performance degrades for \(k 16\) (see Figure 11).

**Influence of the Base LLM.** We conduct an ablation study of the base LLM to evaluate how TabuLa-8B improves as the base LLM improves. In particular, we rerun our main training pipeline described in Section 3, but using LLama 1 and 2 as the initial language model instead of LLama 3. These results, provided in Section F.6, demonstrate that TabuLa-8B improves along with the performance of the underlying base model. Taken together, these results highlight how the primary contribution of the paper is not the specific model we produce, as much as it is the methodology we present for generating tabular predictors from base language models. As LLMs continue to improve, so will the tabular models that are produced by applying our training methodology to new LLMs.

### Assessing the Potential Impact of Data Contamination

Given that T4 consists of 4.2Mtables sourced from public data sources (Common Crawl, Github) and that our evaluations are also comprised of public benchmarks, we investigate the extent and possible impact of data contamination - that is, training datasets that are part of the evaluation suite. In Section G, we explain our methodology to test for the potential presence of benchmark datasets in T4. Using a conservative identification strategy based on column matching (likely to include false positives). We find at most one-third of benchmark tables may occur at least once in the training set. When training large-scale models for transfer learning, it is not always clear _a priori_ what the eventual application domains will be. Therefore, we believe that it is an important research question to understand the extent to which contamination may affect performance, as contamination may be difficult to prevent in some cases. Initial foundation modeling efforts in non-tabular domains adopted a similar approach, and found mixed or no impact from overlap [38; 40].

We evaluate the impact of contamination in our experimental setup by evaluating TabuLa-8B separately on "potentially contaminated" vs. uncontaminated tables. Our results are shown in the bottom right plot of Figure 4, as well as in Figures 17 and 18. Summarizing, we find no clear evidence that contamination affects model performance on the test suite, or that transfer ability is affected by contamination. In fact, as seen in Figure 4, the gap between TabuLa-8B and XGBoost is in fact_larger_ if we restrict evaluation to the benchmark tables which we verify are not in T4. In addition to verifying that our results continue to hold over a diverse set of tables which we know the model did not see in training, it also shows that having some amount of potential contamination did not upwardly bias our estimate of TabuLa-8B's transfer learning ability. We hypothesize that the observed gap in Figure 4 is due to our conservative duplication procedure being more likely to flag datasets with generic or common column names, which also leads to _worse_ baseline performance on these tasks. We present more comprehensive investigation on the effects of contamination in Appendix G.

## 6 Discussion

**Limitations:**TabuLa-8B has several limitations. First, TabuLa-8B has a limited context window of 8192 tokens. This restricts the number of examples that can be utilized for few-shot learning, as well as the additional information (such as text context or extended feature descriptions) that are available to the model. We expect that this limitation will be eased as the availability of longer-context models grows (e.g. ). Second, TabuLa-8B has 8B parameters, which makes serving and inference expensive and limits the environments it may be deployed in. Lastly, given that it uses a pretrained LLM as a base-model and fine tunes on a web-scale corpora of historical datasets that likely contain various social biases, TabuLa-8B introduces new potential fairness considerations that are not present when using preexisting supervised methods such as XGBoost. We hope that by open sourcing the model, data, and code, we might enable future research addressing these important directions.

**Future Work:** Our work on transfer learning for tabular data is the first its kind at this scale, and there are several avenues for future research. These can be coarsely categorized as either improvements (of the existing dataset and model) or extensions (deeper investigations into the model and data itself).

On the _improvements_ side, we see several promising directions. These include improvements in tabular data filtering (this has been the main axis of improvement in recent generations of language models); scaling the model + data + compute; exploring the use of inference-time strategies to improve prediction (such as self-consistency , prompt ensembling , or in-context example selection ); and introducing extra information during both training and inference, such as contextual information or samples from different, but related, tables.

On the _extensions_ side, we hope that our work opens avenues toward deeper understanding of tabular foundation models including: understanding potential biases or unwanted behavior with respect to sensitive features (features such as race, age, and gender are common in tabular datasets); using tabular foundation models to address small-sample problems which might be aided by a high-quality pretrained model (such as in the Fragile Families Challenge ); and extending this approach to new tasks beyond prediction, such as data generation, explanation, data wrangling, and more.

## 7 Accessing Open-Source Code, Data, and Model Weights

**TabLib Preprocessing Code:** A Python module for filtering TabLib, tabliblib, along with scripts and configurations used to perform the filtering, are available at https://github.com/mlfoundations/tablib.

**Model Training and Inference Code:** We provide rtfm, a Python module used to train TabuLa-8B, perform inference and evaluation, and process data, at https://github.com/mlfoundations/rtfm.

**T4 Dataset:** The T4 dataset is available via public credentialized access on Hugging Face datasets at https://huggingface.co/datasets/mlfoundations/t4-full. Because the dataset is derived from TabLib, users must first obtain permission to access TabLib at https://huggingface.co/datasets/approximatelabs/tablib-v1-full.

**Evaluation Datasets:** The full evaluation suite used to evaluate TabuLa-8B is available via Hugging Face Datasets at https://huggingface.co/datasets/mlfoundations/tabula-8b-eval-suite. Each dataset includes: a CSV file containing the raw data; a TableShift  FeatureList JSON object; and a YAML file with associated metadata.

**Model Weights:**TabuLa-8B weights are available via Hugging Face at https://huggingface.co/mlfoundations/tabula-8b.