# Conditioning non-linear and

infinite-dimensional diffusion processes

 Elizabeth Louise Baker

Department of Computer Science,

University of Copenhagen

elba@di.ku.dk

&Gefan Yang

Department of Computer Science,

University of Copenhagen

gy@di.ku.dk

&Michael L. Severinsen

Globe Institute,

University of Copenhagen

michael.baand@sund.ku.dk

&Christy Anna Hipsley

Department of Biology,

University of Copenhagen

christy.hipsley@bio.ku.dk

&Stefan Sommer

Department of Computer Science,

University of Copenhagen

sommer@di.ku.dk

###### Abstract

Generative diffusion models and many stochastic models in science and engineering naturally live in infinite dimensions before discretisation. To incorporate observed data for statistical and learning tasks, one needs to condition on observations. While recent work has treated conditioning linear processes in infinite dimensions, conditioning non-linear processes in infinite dimensions has not been explored. This paper conditions function-valued stochastic processes _without prior discretisation_. To do so, we use an infinite-dimensional version of Girsanov's theorem to condition a function-valued stochastic process, leading to a stochastic differential equation (SDE) for the conditioned process involving the score. We apply this technique to do time series analysis for shapes of organisms in evolutionary biology, where we discretise via the Fourier basis and then learn the coefficients of the score function with score matching methods.

## 1 Introduction

When modelling finite-dimensional data, such as temperature or speed, there are well-known methods for incorporating observations into stochastic or probabilistic models, for example, those based on Gaussian-process regression (Rasmussen and Williams, 2005). For non-linear models, techniques like Doob's \(h\)-transform can be used (Rogers and Williams, 2000, Chapter 6). But for data that is function-valued (and thus infinite-dimensional) with non-linear models, conditioning is still an open problem. This paper introduces a way of conditioning infinite-dimensional diffusion processes by introducing an infinite-dimensional version of Doob's \(h\)-transform. We then discretise the conditioned process and sample from it; that is, we condition and then discretise rather than discretising and then conditioning.

We present methods to condition a process to hit a specific set at the end time, also known as bridges. This method covers the case of conditioning strong solutions to Hilbert space-valued stochastic differential equations (SDEs). For the conditioning, we consider two scenarios. The first is that thetransition operators of the SDE solution are smooth, which is generally not obvious (Goldys and Maslowski, 2008). In the second scenario, we condition on observations with Gaussian noise. This technique can be applied whenever the solution to the SDE is sufficiently differentiable. To condition, we use the infinite-dimensional counterparts of Ito's lemma and Girsanov's theorem, enabling us to define a Doob's \(h\)-transform analogously to finite dimensions. We then use score-matching techniques, allowing us to sample from the conditioned process. We do this by training on the coefficients of the stochastic process, represented in the Fourier basis.

One specific use case is modelling changes in morphometry (i.e. shapes) of organisms in evolutionary biology. The morphometry of an organism can be modelled as points, curves or surfaces embedded in Euclidean space. Felsenstein (1985) suggests using Wiener processes to model changes in morphometry, for example, height. Sommer et al. (2021) propose extending this methodology to whole shapes by using stochastic flows to define diffeomorphisms on Euclidean space (Kunita, 1997). Then, changes of the shapes are modelled deterministically as diffeomorphisms on \(^{d}\) that act on the embeddings (Younes, 2019). Recent work has generalised this to the stochastic setting by considering diffusion bridges between shapes (Arnaudon et al., 2019, 2022). However, they first discretise the shapes and then find a diffusion bridge between the discretised shapes. In this work, we condition, and then we discretise. In doing so, we show that as the number of points goes to infinity, the bridge is still well-defined. Moreover, we may use other discretisations for shapes such as Fourier bases.

## 2 Related work and contributions

### Related work

In finite dimensions, methods have been developed to approximate non-linear bridge processes of the form Equation (6). When conditioning on an end point \(y\) at time \(T\), the conditioned process contains an intractable term \(_{x} p(t,x;T,y)\), called the score term. This term can be replaced with \(_{x}(t,x;T,y)\), where \(\) is the transition function from another SDE with a known closed form, such as Brownian motion or other linear processes (Delyon and Hu, 2006; van der Meulen and Schauer, 2022). Then we can sample from the approximation instead, and use Monte Carlo methods using the ratio given by the Radon-Nikodym derivative as a likelihood ratio, thereby sampling from the true path distribution (van der Meulen and Schauer, 2022).

Recently, Heng et al. (2021) adapted the score-matching methods of Vincent (2011); Song and Ermon (2019); Song et al. (2021) to learn the score term for non-linear bridge processes. To do so, they introduce a new loss function to learn the time reversal of the process. They then learn the time reversal of the time reversal, which gives the forward bridge. Our work uses their method to learn the score term after discretising the SDE via truncated sums of basis elements. Phillips et al. (2022) also consider using truncated sums of basis elements for

Figure 1: We condition an SDE between two curves, representing two butterfly species, (starting from red dashed and ending at green dashed). Each time point of the trajectory represents a shape. _Row 1:_ We take the mean over \(20\) trajectories. _Row 2:_ We plot the \(20\) individual trajectories, used in the mean calculation.

Figure 2: A stochastic process between two butterfly outlines (Papilio polytes in red, Parnassius honorathi in blue).

discretising SDEs, however, only for infinite-dimensional Ornstein-Uhlenbeck processes, which are linear.

Recent work on generative modelling has investigated score matching for infinite-dimensional diffusion processes (Pidstrigach et al., 2023; Franzese et al., 2023; Bond-Taylor and Willcocks, 2023; Hagemann et al., 2023; Lim et al., 2023). This problem is similar to our task of conditioning an SDE, but not the same: The main difference is that our SDEs are fixed, known a priori, and potentially nonlinear, whereas in generative modelling the SDE can be chosen freely. Hence, generative modelling often uses linear SDEs because the transition densities are known in closed form. In this sense, our problem relates to generative modelling but has a different setup.

In shape space, there is interest in defining stochastic bridges between shapes (Arnaudon et al., 2023). Shapes are represented in the LDDMM framework (Younes, 2019), where they are modelled as embeddings in Euclidean space, e.g. curves and surfaces or sets of landmarks approximating a curve or surface. The deterministic image registration problem of matching two shapes is solved by finding a "minimum energy" mapping. More specifically, for two shapes \(s_{0},s_{1}:^{d}^{d}\), we find a diffeomorphism \(f:[0,T]^{d}^{d}\) such that \(f(0,)=s_{0}()\) and at \(f(T,)=s_{1}()\) and \(f\) optimises a given energy functional (Bauer et al., 2014).

We employ a stochastic version of the LDDMM framework for our experiments. Instead of direct paths from \(f_{0}\) to \(f_{T}\) that optimise an energy functional, we define stochastic paths of diffeomorphisms between two shapes. For infinite-dimensional shapes such as curves, stochastic shape analysis was the focus of (Trouve and Vialard, 2012; Vialard, 2013; Arnaudon et al., 2019), where stochastic processes are defined in the LDDMM framework. However, these do not explore the problem of conditioning the defined processes. In (Arnaudon et al., 2022), bridges between finite-dimensional shapes are derived. This is the first work to bridge between infinite-dimensional shapes.

### Contributions

1. We derive Doob's \(h\)-transform for infinite-dimensional non-linear processes, allowing conditioning without first discretising the model.
2. We detail two models: one for direct conditioning on data and the second for assuming some observation error.
3. We use score matching to learn the score arising from the \(h\)-transform by training on the coefficients of the Fourier basis.
4. We demonstrate our method in modelling the changes in the shapes of butterflies over time.

## 3 Problem Statement

We assume that the data lives in a separable Hilbert space \((H,,)\) and let \((,,)\) be a probability space with natural filtration \(\{_{t}\}\) and take \((H)\) to be the Borel algebra. We take a Wiener process \(W\) on a separable Hilbert space \(U\) (where \(U\) can equal \(H\)) with covariance given by \(Q\). We then consider Hilbert space-valued SDEs of the form

\[X(t)=(AX(t)+F(X(t)))t+B(X(t))W(t), X(0)= _{0} H,\] (1)

where \(A:D(A) H H\) is the infinitesimal generator of a strongly continuous semigroup, and \(F:[0,T] H H,\,B:[0,T] H(Q^{1/2}(U),H),\) where \((Q^{1/2}(U),H)\) denotes the Hilbert-Schmidt operators from \(Q^{1/2}(U)\) to \(H\), and \(Q^{1/2}\) is the unique, non-negative symmetric, linear operator on \(U\) satisfying \(Q^{1/2} Q^{1/2}=Q\) (see Rockner and Claudia (2007, Proposition 2.3.4) for details). Note that \(F\) and \(B\) can depend non-linearly on \(X(t)\), so other methods, such as Gaussian process regression, cannot be used to incorporate data. We also need the following assumptions about Equation (1):

**Assumption 3.1**.: Equation (1) has a unique strong Markov solution denoted \(X(t,_{0})\).

**Assumption 3.2**.: The solution \(X(t,)\) is twice Frechet differentiable with respect to the initial value \(\), with derivatives continuous on \([0,T] H\).

Assumption 3.1 is strong, but is satisfied when \(A\) is bounded and \(F\) and \(B\) satisfy certain Lipschitz continuity and boundedness conditions. With some extra Frechet differentiability conditions on and \(B\), Assumption 3.2 will also be satisfied. See Da Prato and Zabczyk (2014, Part II) for details on solutions. Under the above setup, we consider two problems in conditioning the model on given observational data. First, we tackle the exact matching problem:

**Problem 3.1**.: (Exact matching) Condition \(X\) such that \(X(T,_{0})\) where \( H\) is a set with nonzero measure.

More precisely, we define a new probability measure, under which the expectation equals the original expectation conditioned on the event that \(X(T,_{0})\): \(_{new}[]=[ X(T,_{0})]\). Section 5.2.1 discusses this. To solve this, we also require an extra assumption:

**Assumption 3.3**.: The transition operator \(P(,t,)[_{}(X(t,))]\) is twice Frechet differentiable with respect to \(\), and once with respect to \(t\) with continuous derivatives.

In infinite dimensions Assumption 3.3 is a strong assumption; however, we will study some specific sets \(\) for which this is satisfied in Sec. 5.3. Moreover, with extra conditions on \(A\) and \(Q\), there are also SDEs that satisfy this assumption for any nonzero measure Borel set. See Da Prato and Zabczyk (2014, Theorem 9.39 and Theorem 9.43) and Cerrai (2001, Section 6.5 and Section 7.3) for examples. The inexact matching problem does not require Assumption 3.3 and allows us to consider observation noise:

**Problem 3.2**.: (Inexact matching) Condition \(X\) so that \(X(T,_{0})\) is "near" an observed function \(V\).

Exact matching could be rephrased as conditioning such that at time \(T\), the distance between \(X(T,_{0})\) and a set \(\) is equal to 0. For the inexact matching, we instead condition such that the distance between \(X(T,_{0})\) and a target set \(\) (or target function \(V\)) is Gaussian with mean 0. More generally, we can also take any differentiable radial basis function on the distance instead of a Gaussian.

For both problems, we show that the process \(X(t,_{0})\) conditioned to exhibit the wanted behaviour at time \(T\), will be a process \(X^{c}(t,_{0})\) satisfying the SDE

\[\{ X^{c}(t)&=[AX^{c}(t)+F(X ^{c}(t))]t+B(X^{c}(t))W(t)\\ &+B(X^{c}(t))B(X^{c}(t))^{*} h(t,X^{c}(t))t\\ X^{c}(0)&=_{0},.\] (2)

where \(_{}(h(t,))\) is a score function. For the exact matching problem, we will see that \(h(t,)=(X(T) X(t)=)=[_{}(X( T-t,))]\). For the inexact matching problem, we will instead set \(h(t,)=[\|f(X(T-t,)-V\|_{H};0,)]\), for \(f\) a Gaussian function, and \(V\) a target function. The SDE in Equation (2) is analogous to the case of conditioning in finite dimensions, so the form may not be surprising. However, it is not obvious that this should work for non-linear equations in infinite dimensions.

## 4 Background

### Strong Markov solutions

We consider Hilbert space-valued SDEs of the form Equation (1) for \(W\) a \(Q\)-Wiener process in a Hilbert space \(U\), where \(Q\) can be the identity operator. We only consider strong solutions to Equation (1). An \(H\)-valued predictable process \(X\) is a strong solution of Equation (1) if \( t[0,T]\) it satisfies a well-defined integral

\[X(t)=+_{0}^{t}[AX(s)+F(s,X(s))]s+_{0}^{t}B(s,X(s)) W(s).\] (3)

We refer to Da Prato and Zabczyk (2014) for a discussion on the existence of strong solutions (and more general solutions). However, when \(F,B\) satisfy some Lipschitz and linear growth conditions, and when \(A\) is bounded, Equation (1) has a unique strong solution. Since this is true for any initial value \(\), we use the notation \(X(t,)\) to mean the unique solution of Equation (1) with initial value \(\). This solution is a Markov process and for \(f:H\) a bounded function, measurable with respect to the Borel algebra the transition operators \(P_{t}f()=[f(X(t,))]\) satisfy the Markov condition:

\[[(X(t,))_{s}]=[(X(t-s,X(s,)) )]=[(X(t,)) X(s,)].\] (4)

This says that the expected value of the solution at a time \(t\) from a starting value \(\), given all the information from some previous time \(s\), is the same as the expected value of the solution started at value \(X(s,)\) at time \(t-s\).

### Doob's \(h\)-transform in finite dimensions

In order to give more intuition for the infinite-dimensional Doob's \(h\)-transform, we present an informal introduction to the topic. This is all well-known, and the details can be found, for example, in Rogers and Williams (2000, Chapter 6). Doob's \(h\)-transform in finite dimensions is a useful theory for conditioning stochastic differential equations. For example, suppose we have an SDE in \(^{d}\)

\[x(t)=f(t,x(t))t+(t,x(t))W(t), x(0)=x_{0} ^{d}\] (5)

and we want to condition this SDE to hit \(y\) at time \(T\). This corresponds to finding a measure \(\) such that \(^{}[x(t)]=[x(t) x(T)=y]\). Doob's \(h\)-transform allows us to define such a measure using so called \(h\)-functions. Let \(p(t,y;t+s,y^{})\) be the transition density of \(x_{t}\), defined as \([x(t+s) A x(t)=y]=_{A}p(t,y;t+s,y^{}) y^{}\). Let \(h:[0,T]^{d}(0,)\) be a function satisfying \(h(t,x)= h(t+s,y)p(t,x;t+s,y)y\), such that for \(z(t) h(t,x(t))\), \([z(T)]=1\). Then, \(z(t)\) is a martingale and there exists a measure \(\) such that \(}{}|_{_{t}}=z_{t}\). Moreover, under this measure \(\), \(x(t)\) satisfies a new SDE

\[x^{c}(t)=f(t,x^{c}(t))t+^{}(t,x^{c}) _{x} h(t,x^{c}(t))t+(t,x^{c}(t))W(t), x ^{c}(0)=x_{0}.\] (6)

The SDE in Equation (6) can be thought of as a conditioned version of the original SDE in Equation (5). For example, consider what happens when we take \(h(t,x):=:T,y)}\). Then for a function \(f\)

\[^{}[f(x(t))]= f(z);T,y)}p(0,x_{0};t,z)z=[f(x(t)) x(T)=y].\] (7)

This is one example of Doob's \(h\)-transform but other \(h\) functions can also be defined, for example, to condition \(x_{t}\) to stay within certain bounds, or not to go above a certain value for a certain time period.

For \(h(t,x):=:T,y)},\) as in conditioning on an end point, there is, in general, no closed form solution for \(h\). Different methods to learn the bridge exist (Delyon and Hu, 2006, Schauer et al., 2017). More recently, score-based learning methods were proposed to learn the term \(_{x} p(t,x;T,y)\)(Heng et al., 2021), which we will adapt to the infinite-dimensional setting.

## 5 Method

We are interested in conditioning the stochastic process to exhibit a particular behaviour at the end time \(T\). We consider two scenarios. The first is exact matching: we condition such that given a set \( H\), then \(X(T,_{0})\). The second is inexact matching: for some \(Y H\) we condition such that as \(t\) approaches \(T\), \(X(t,_{0})\) becomes "close" to \(Y\).

We proceed as follows. First, we show that we can define a new probability measure given an appropriate random variable. When we have shown that this is possible, we will discuss options for the random variable. We will give specific variables, show that they fit some necessary conditions, and solve Problem 3.1 and Problem 3.2.

### Doob's \(h\)-transform in infinite dimensions

Here, we suppose that we already have an appropriate function \(h\) which we show that we can use to rescale our original probability distribution, giving us a conditioned probability.

**Theorem 5.1**.: _Let \(h:[0,T] H_{>0}\) be a continuous function twice Frechet differentiable with respect to \( H\) and once differentiable with respect to \(t\), with continuous derivatives. Suppose \(X\) is the strong solution to the stochastic differential equation in Equation (1). Moreover, we assume that \(Z(t) h(t,X(t))\) is a strictly positive martingale, with \(Z(0)=1\), and \([Z(T)]=1\)._

_Then \(} Z(T)\) defines a new probability measure. Moreover, \(X\) satisfies the SDE_

\[X(t)= X(0)+_{0}^{t}B(X(s))B(X(s))^{*} h(s,X(s))s\] (8) \[+ _{0}^{t}[AX(s)+F(X(s))]s+_{0}^{t}B(X(s))(s),\]_where \(\) is the Wiener process with respect to the measure \(}\)._

Proof.: First, we show that \(Z(t) h(t,X(t))\) defines a continuous martingale and apply an infinite-dimensional Ito's theorem followed by Doleans exponential to rewrite \(Z\). We then may apply the infinite-dimensional Girsanov's theorem, and rewrite the original SDE in terms of the resulting Wiener process \(\). See Theorem C.1 for full details. 

### Defining the transforms

Previously, we showed that given a function \(h\) satisfying certain conditions, we can use this to weight the probability measure, giving us a new conditioned probability measure. Now, we address which \(h\) functions to use and the properties of the resulting processes. In infinite dimensions, there is no measure that satisfies all properties of the usual finite-dimensional Lebesgue measure and so it does not make sense to consider transition densities. However, transition operators of the form \(P_{t}f()=[f(X(t,))]\) exist and satisfy the Markov property in Equation (4), so we opt to use these instead. For a strictly positive, bounded Borel function \(:H\), we take functions of the form

\[h(t,)=C_{T}[(X(T-t,))].\] (9)

where \(C_{T}=1/[(X(T,_{0})]\) is a normalising constant. Consider, for example, when the function \(\) is the Dirac delta function of some set \(A\) with non-zero measure. Then \(h(t,)\) is the probability that at the end time, the process will be in the set \(\), given that at the current time, the process is equal to \(\). Functions of the form Equation (9) satisfy some of the necessary assumptions on \(h\) for Theorem 5.1: they are positive martingales, with \(Z(0)=1\) and \([Z(T)]=1\).

**Lemma 5.2**.: _Let \(X\) be as in Equation (1), satisfying Assumption 3.1. Given a function \(h:[0,T] H\) satisfying Equation (9), \(Z(t) h(t,X(t,_{0}))\) is a strictly positive martingale such that \(Z(0)=1\) and \(Z(T)=C_{T}(X(T,_{0}))\)._

Proof.: Apply the tower property and check the assertions hold. See Lemma C.3 for full details. 

The other necessary condition on \(h\) is differentiability. For this, we consider the specific functions with different \(\)'s separately for the exact and inexact matching cases.

#### 5.2.1 Exact matching

Let \( H\) be Borel measurable, and suppose that Assumption 3.3 holds. For some instances where this holds, see Da Prato and Zabczyk (2014, Chapter 9), or Sec. 5.3. Then we define

\[h(t,)[_{}(X(T-t,))]/[_{ }(X(T,_{0}))].\] (10)

The proof that this \(h\)-transform does solve Problem 3.1 is similar to the finite-dimensional version. Note that \(h(T,X(T))=_{}(X(T,_{0}))/[_{}(X(T,_{ 0}))]\). Hence, for some random variable \(Y\) we see:

\[}[Y]=_{}Y}=(\{X(T,_{0})\})^{-1}_{\{X(T)\}}Y =[Y X(T,_{0})].\] (11)

#### 5.2.2 Inexact matching

In addition to the exact matching problem, we can solve the inexact matching problem by conditioning the process such that, at the end time, it approximates some behaviour. This has two advantages. Firstly, we do not need Assumption 3.3 and instead use Assumption 3.2. Assumption 3.2 is satisfied when \(F\) and \(B\) satisfy Frechet differentiable conditions. The second advantage is that this allows us to account for observation noise in models. We condition on the Gaussian distance between the endpoint and some target point or observation by defining a function \(:H\) that is twice Frechet differentiable. Then under Assumptions 3.1 and 3.2 i.e. \(X(t)\) is a strong solution, a Markov process and is twice differentiable with respect to the initial value, the function \(h(t,)[(X(T-t,))]\) will satisfy the necessary conditions given at the start of Section 5.1:

**Lemma 5.3**.: _Let \(:H\) be a continuous function, twice Frechet differentiable, with continuous derivatives. Then \(h(t,)[(X(T-t,))]\) is twice Frechet differentiable in \(\) and once differentiable in \(t\), with continuous derivatives._Apply Ito's formula to \(h\) and use properties of expectation to differentiate. See Lemma C.4 for the details.

One such function satisfying Lemma 5.3 is the Gaussian kernel function \(k:H H\),

\[k_{}(V,X)=}(-\|X-V\|_{ H}^{2}).\] (12)

Here, we fix an observation \(V H\) and parameter \(\) and vary \(X H\). The function \(k\) is twice Frechet differentiable in each argument, with continuous derivatives; hence the function \(h(t,)=[k_{}(V,X(t,))]\) satisfies the requirements of Lemma 5.3. Moreover, this gives us a method of including observation noise in our model. In finite dimensions, to model inexact matching for a stochastic process \(x(t)^{d}\), one can take the function

\[h(t,x) f_{d}(v;y,)p(t,x;T,y)y,\] (13)

where \(v^{d}\) is a target value, \(p(t,x;T,y)\) is the transition density of the \(^{d}\)-valued stochastic process and \(f_{d}(;,)\) is the density of the normal distribution on \(^{d}\) with mean \(^{d}\) and covariance \(^{d d}\). See Arnaudon et al. (2022, Section 3.1) for more details. To compare, for \(f_{1}(;0,)\) the density of the one-dimensional normal distribution with mean \(0\) and variance \(\), and \(P(T-t,,)=[X(T) X(t)=]\), we set

\[h(t,)[k_{}(V,X(t,))]=_{H}f(\|-V\|_{H }^{2};0,)P(T-t,,).\] (14)

Defining the function in this way means we condition on a distance between \(X(T)\) and our observation. It also means we can change the distance function to another similarity measure. For example, when the functions represent shapes, we could use another norm that measures the dissimilarity of shapes as in Pennec et al. (2020, Chapter 12).

### Sampling from infinite-dimensional \(h\)-transforms

Thus far, we have shown that in infinite dimensions, we can condition stochastic processes either exactly or inexactly, and the conditioned process has form Equation (2). We now turn our attention to sampling from these conditioned processes. For this we discuss how to discretise.

For an orthonormal basis \(\{e_{i}\}_{i=1}^{}\) of a separable Hilbert space \(H\), let \(H^{N}=(\{e_{i}\}_{i=1}^{N}) H\). Let \(X(t,)\) be a strong solution to Equation (1), with \(X(0)=\). Since strong solutions are also weak solutions (Da Prato and Zabczyk, 2014, Chapter 6.1), we can write \(X(t,)\) as a sum of finite dimensional SDEs, with each finite SDE satisfying

\[ X(t),e_{i}=,e_{i}+_{0}^{t} AX(s)+f (X(s)),e_{i}s+_{0}^{t} e_{i},B(X(s))W(s ).\] (15)

Using Equation (15), we can define an SDE \(X^{N}\) as \(X^{N}_{i}(t) X(t),e_{i}\), where \(X^{N}_{i}\) is the \(i^{}\) component of \(X^{N}^{N}\). For finite dimensional sets \(_{i}\) we look at the problem of conditioning on cylindrical sets of the form

\[_{N}=\{ H_{i}_{i},\,1 i N\},\] (16)

for \(_{i}=,e_{i}\).

**Lemma 5.4**.: _Let \(_{N}\) be as in Equation (16) and \(h:[0,T] H^{N}\) be defined by \(h(t,Y)[_{_{N}}(X(T-t,Y))]\). Moreover, define \(g:[0,T]^{N}\) by \(g(t,y)[_{i=1}^{N}_{_{i}}(X^{N}(T-t,y))]\). Then \( h(t,Y),e_{i}=[ g(t,(Y_{i})_{i=1}^{N})]_{i}\)._

Proof.: It follows by noting that \([_{_{N}}(Y)]=[_{i=1}^{N}_{_{i }}(Y_{i})]\). See Lemma C.5 for details. 

Since \(h(t,Y)=g(t,(Y_{i})_{i=1}^{N})\), the sets \(_{N}\) satisfy Assumption 3.3 as long as the sets \(_{i}\) do in finite dimensions. We have shown that conditioning on sets only depending on the first \(N\) eigenvalues is equivalent to conditioning the \(N\) dimensional projection of the SDE onto the first \(N\) basis elements.

With this discretisation onto finite dimensions, we can adapt a finite-dimensional algorithm to sample from the finite-dimensional bridges. For this we opt for using the algorithm in Heng et al. (2021), since it can be easily applied to Problems 3.1 and 3.2 and we can scale up to higher dimensions by using a different network architecture. Here, they leverage the diffusion approximation (in this case, Euler-Maruyama) and score-matching techniques to first learn the time reversal of the diffusion process. Applying the algorithm again on the time reversal, started at the proposed end point, gives the forward-in-time diffusion bridge.

## 6 Experiments

We consider two main setups. Firstly we look at Brownian motion between shapes and use this to evaluate our method, since for Brownian motion we have a closed form solution for the score function. We then apply this to problems from the shape space literature. There, they are interested in stochastic bridges between shapes which has applications within medical imaging and evolutionary biology (Gerig et al., 2001; Arnaudon et al., 2017, 2023). We expand on that body of work, by allowing shapes to be treated as infinite-dimensional objects when bridging, as in the non-stochastic case (Younes, 2019). Until now, this was impossible for stochastic shape paths, since the theory for this was missing.

The code used for our training and experiments can be found at https://github.com/libbylbaker/infsdebridge and further details on experiments can be found in Appendix B.

### Brownian Motion

For Brownian motion between shapes, we look at using both discretisations via landmarks and Fourier bases, for conditioning both exactly and inexactly and compare to the true solution. We train on a target shape of a circle with radius 1. For the landmark discretisation, we condition such that the landmarks of the process end at the landmarks of the target shape, and for the Fourier basis we condition such that the chosen basis elements are equal to the Fourier basis of the circle at the end time. In Tab. 1 we give the mean square error for different numbers of dimensions. For the Fourier basis, we evaluate the score on 100 points, and find the error between this and the true value so that we may compare to the landmark errors. We see that as the dimensions grow, we need a larger training time to maintain lower errors. We note that each Fourier basis, contains two parts: the real and imaginary. We train on batches of 50 SDE trajectories, with 40 batches per epoch. For training details see Appendix B.1.

### Experiments on shape space

Next we turn to a concrete problem: we model the change in morphometry (shapes) of butterflies over time. Studying changes of morphometry of organisms over time is important to evolutionary biologists. For example, for butterflies, one can ask whether the change in wing shape correlates with a change in habitat or climate. Rather than extracting finite-dimensional information from the shapes, such as height or a subset of chosen points, we apply the analysis to the entire shape, as suggested in Sommer et al. (2021). Being able to condition between shapes is a key step in phylogenetic inference, where it will be applied to compute likelihoods of phylogenetic trees from morphological data. Until now, it was only possible for finite-dimensional extractions of the shape. Future work will consider extending our methods for parameter estimation of SDEs for phylogenetic inference. This is in order to extend the Brownian motion model of trait evolution to shapes where the SDE models the transitions over the edges in the phylogenetic tree (Felsenstein, 1985).

    &  &  \\   & 8 & 16 & 32 & 8 & 16 & 32 \\  RMSE & 5.09 & 6.66 & 10.54 & 7.95 & 6.08 & 10.79 \\ Time (s) & 105.1 & 201.8 & 949.4 & 95.9 & 104.8 & 183.0 \\ Epochs & 100 & 150 & 300 & 100 & 100 & 100 \\   

Table 1: A comparison of the trained score of the Brownian motion process.

#### 6.2.1 SDEs in shape space

For SDEs in shape space we take the SDE defined in Sommer et al. (2021) as

\[X(t)=_{0}^{t}Q(X(t))W(t)(Qh)f(x)=_{^{ 2}}k(h(x),y)f(y)y.\] (17)

where for each \(h H=L^{2}(^{2},^{2})\), \(Q(h):H H\) is a Hilbert-Schmidt operator, and \(k\) is a smooth kernel \(k L^{2}(^{2}^{2},^{2})\).

This corresponds to a stochastic flow of diffeomorphisms on \(^{2}\), with a Brownian temporal model. For each \(x^{2}\), \(X(t,x)\) models the position of \(x^{2}\) at time \(t\), and the function \(x X(t,x)\) is a diffeomorphism for all \(t\). To see this we write this in the language of stochastic flows as defined in Kunita (1997). Define the martingale \(F(t,x) QW(t,x)\), where \(W\) is the Wiener process on \(H\) and \(Q\) is defined as before. Then, we define the stochastic flow of the martingale \(F\) as

\[p(t,x)=x+_{0}^{t}F(p(r,x),r),\] (18)

where the integral is defined in Kunita (1997, Chapter 3). Then by Kunita (1997, Theorem 4.6.5) if \(k\) is smooth, the map \(p(t,)\) is a diffeomorphism for each \(t\). See also Da Prato and Zabczyk (2014, Chapter 0.1) for general details of lifts of diffusion processes to infinite dimensions.

In Figure 8 we plot some example trajectories of Equation (17) for various parameters, with a circle as the initial value. In Figure 6 we plot one trajectory for a butterfly. The trajectories are calculated in terms of a Fourier basis and for Figure 6 we plot the trajectories of a subset of points of the shape where we see the temporal Brownian model.

#### 6.2.2 Results

We illustrate our method on butterfly data. To demonstrate, we first use two butterflies with somewhat different shapes (GBIF.Org User, 2024). One trajectory between the two butterflies is plotted in Figure 2. In this, we can see the high correlation between neighbouring points, with a Brownian temporal model. In Figure 1, we plot 120 butterfly trajectories, at specific time points. For \(t=0.2\) we see that the butterfly outlines are mostly close to the start butterfly in pink, and at time \(t=0.8\), they are closer to the green target butterfly. In Appendix B, we also plot the score function over time for varying numbers of basis elements Figure 3.

For the next experiment, we take fifty butterfly specimens across five closely related species from the Papilio family (see Figure 5) (Kawahara et al., 2023). The butterflies are aligned via Procrustes, and a mean consensus shape is obtained using Geomorph (Adams and Otarola-Castillo, 2013). More details of the butterflies and their processing are in Appendix A. We train our model on the mean of the butterflies to learn the time reversal from any given input, to hit the distribution at time \(T=1.0\), which we plot in Figure 4.

Figure 3: Score fields evaluated on a selection of points at different time steps. In general, the score field is expected to “push” the shape towards the target. We show the learned score fields (black arrows) represented by varying numbers \(N\) of base functions at different time steps, as well as the current shape (blue curves) and the target shape (red curves).

## 7 Conclusion, limitations and future work

We have proved that Doob's \(h\)-transform can also be used in infinite-dimensions for stochastic differential equations with strong solutions. We can condition non-linear function-valued stochastic models on observations, either directly on data or by including observation noise. The conditioned stochastic process satisfies a new differential equation involving a score function, which we can approximate using score learning. However, due to the reliance on Ito's formula, it would be hard to generalise this proof to non-strong solutions.

To learn the score, we used the architecture detailed in Figure 7. Although this seemed to work well for our experiments, more research could go into the network architecture which could further increase the dimensions that we consider. Furthermore, we only do the first step in Heng et al. (2021) and learn the time reversal since error compounds in learning the forward bridge. Future work will consider how well the time reversal approximates the forward bridge or how to learn the forward bridge directly. Moreover, as previously mentioned, learning Doob's \(h\)-transform is only the first step in phylogenetic inference for shapes of species. Future work will consider how to expand the infinite-dimensional bridges to inference problems.