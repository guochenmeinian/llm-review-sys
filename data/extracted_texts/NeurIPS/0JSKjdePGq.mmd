# When to Sense and Control?

A Time-adaptive Approach for Continuous-Time RL

 Lenart Treven, Bhavya Sukhija, Yarden As, Florian Dorfler, Andreas Krause

ETH Zurich, Switzerland

Correspondence to lenart.treven@inf.ethz.ch

###### Abstract

Reinforcement learning (RL) excels in optimizing policies for discrete-time Markov decision processes (MDP). However, various systems are inherently continuous in time, making discrete-time MDPs an inexact modeling choice. In many applications, such as greenhouse control or medical treatments, each interaction (measurement or switching of action) involves manual intervention and thus is inherently costly. Therefore, we generally prefer a time-adaptive approach with fewer interactions with the system. In this work, we formalize an RL framework, _Time-adaptive_**C**_ontrol & Sensing_ (**TaCoS**), that tackles this challenge by optimizing over policies that besides control predict the duration of its application. Our formulation results in an extended MDP that any standard RL algorithm can solve. We demonstrate that state-of-the-art RL algorithms trained on TaCoS drastically reduce the interaction amount over their discrete-time counterpart while retaining the same or improved performance, and exhibiting robustness over discretization frequency. Finally, we propose OTaCoS, an efficient model-based algorithm for our setting. We show that OTaCoS enjoys sublinear regret for systems with sufficiently smooth dynamics and empirically results in further sample-efficiency gains.

## 1 Introduction

Nearly all state-of-the-art RL algorithms (Schulman et al., 2017; Haarnoja et al., 2018; Lillicrap et al., 2015; Schulman et al., 2015) were developed for discrete-time MDPs. Nevertheless, continuous-time systems are ubiquitous in nature, ranging from robotics, biology, medicine, environment and sustainability etc. (cf. Spong et al., 2006; Jones et al., 2009; Lenhart and Workman, 2007; Panetta and Fister, 2003; Turchetta et al., 2022). Such systems can be naturally modeled with stochastic differential equations (SDEs), but computational approaches necessitate discretization. Furthermore, in many applications, obtaining measurements and switching actions is expensive. For instance, consider a greenhouse of fruits or medical treatment recommendations. In both cases, each measurement (crop inspection, medical exam) or switching of actions (climate control, treatment adjustment) typically involves costly human intervention. Hence, minimizing such interactions with the underlying system is desirable. This underlying challenge is rarely addressed in the RL literature.

In practice, a time-equidistant discretization frequency is set, often manually, adjusted to the underlying system's characteristic time scale. This is challenging, however, especially for unknown/uncertain systems, and systems with multiple dominant time scales (Engquist et al., 2007). Therefore, for many real-world applications having a global frequency of control is inadequate and wasteful. For example, in medicine, patient monitoring often requires higher frequency interaction during the onset of illness and lower frequency interactions as the patient recovers (Kaandorp and Koole, 2007).

In this work, we address this limitation of standard RL methods and propose a novel RL framework, **T**ime-**a**daptive **C**ontrol & **S**ensing (**TaCoS**). TaCoS reduces a general continuous-time RL problem with underlying SDE dynamics to an equivalent discrete-time MDP, that can be solved with anyRL algorithm, including standard policy gradient methods like PPO and SAC (Schulman et al., 2017; Haarnoja et al., 2018). We summarize our contributions below.

**Contributions**

1. We reformulate the problem of time-adaptive continuous time RL to an equaivalent discrete-time MDP that can be solved with standard RL algorithms.
2. Using our formulation, we extend standard policy gradient techniques (Haarnoja et al. (2018) and Schulman et al. (2017)) to the time-adaptive setting. Our empirical results on standard RL benchmarks (Freeman et al., 2021) show that TACoS outperforms its discrete-time counterpart in terms of policy performance, computational cost, and sample efficiency.
3. To further improve sample efficiency, we propose a model-based RL algorithm, OTAcoS. OTACoS uses well-calibrated probabilistic models to capture epistemic uncertainty and, similar to Curi et al. (2020) and Treven et al. (2023), leverages the principle of optimism in the face of uncertainty to guide exploration during learning. We theoretically prove that OTACoS suffers no regret and empirically demonstrate its sample efficiency.

## 2 Problem statement

We consider a general nonlinear continuous time dynamical system with continuous state \(^{d_{}}\) and action \(^{d_{}}\) space. The underlying dynamics are governed by a (controllable) SDE:

\[d_{t}=^{*}(_{t},_{t})dt+^{*}(_{t},_ {t})dB_{t}.\] (1)

Here \(_{t}\) is the state at time \(t\), \(_{t}\) the control input, \(^{*},^{*}\) are unknown drift and diffusion functions and \(_{t}\) is a standard Brownian motion in \(^{d_{}}\). Our goal is to find a control policy \(_{}:\) which maximizes an unknown reward \(b^{*}(_{t},_{t})\) over a fixed horizon \(}}{{=}}[0,T]\), i.e.,

\[_{}[_{t}b^{*}(_{t}, _{}(_{t},t))dt],\]

where the expectation is taken w.r.t. the policy and stochastic dynamics and \(\) is the class of policies2 over which we search.

In practice, we can only measure the system state and execute control policies in discrete points in time. In this work, we focus on problems where state measurement and control are synchronized in time. We refer to these synchronized time points as _interactions_ in the following parts of this paper. Synchronizing state measurement and control contrasts standard time-adaptive approaches such as event-triggered control (Heemels et al., 2021), where the state is measured arbitrarily high frequency and control inputs are changed only so often to ensure stability. It is also in contrast to the complementary setting, where control inputs are changing at an arbitrarily high frequency but measurements are collected adaptively in time (Treven et al., 2023). An adaptive control approach as Heemels et al. (2021) is very important for many real-world applications but similarly, an adaptive measurement strategy is crucial for efficient learning in RL (Treven et al., 2023). Our approach treats both of these requirements jointly.

We consider two different scenarios for continuous-time control: (_i_) Penalizing interactions with some cost, (_ii_) bounded number of interactions, i.e., hard constraint on control/measurement steps.

**Interaction cost** We consider the setting where every interaction we take has an inherent cost \(c(_{t},_{t})>0\). Note that we consider this cost structure for its simplicity and TACoS works for more general cost functions that depend on the duration of application for the action \(_{t}\) or the previous action \(_{t-1}\) and thus captures many practical real-world settings. We define this task more formally below

\[_{,_{}}[_{i=0 }^{K-1}_{t_{i-1}}^{t_{i}}b^{*}(_{t},_{}(_ {t_{i-1}},t_{i-1}))dt-c(_{t_{i-1}},_{}(_{t_{i- 1}},t_{i-1}))],\] (2) \[t_{i}=_{}(_{t_{i-1}},t_{i-1})+t_{i-1},\ t_{0 }=0,t_{K}=T,\ (,t);_{}(,t) [t_{},t_{}].\]

Here \(t_{}>0\) is the minimal duration for which we have to apply the control, \(t_{}[t_{},T]\) the maximum duration, and \(_{}\) is a policy that predicts the duration of applying the action.

[MISSING_PAGE_FAIL:3]

### Reformulation of Bounded Number of Interactions to Discrete-time MDPs

The second setting is similar to the one studied by Ni and Jang (2022). In this case, we consider the following class of policies:

\[_{BI}=\{:  k[K]:(,,k)L_{}-\}\.\]

For an augmented state \(=(,b,t,k)\), our policies map states \(\), time-to-go \(t\), number of past interactions \(k\) to a controller \(\) and the time duration \(\) for applying the action. Here the optimal control problem reads

\[_{_{BI}}V_{,^{*}}(_{0},T )=_{_{BI}}\ [_{k=0}^{K-1}r(_{k},(_{k}))]\] (7) \[_{k+1}=_{^{*}}(_{k},(_{k}),_{k}),\ _{0}=(_{0},0,T,0),\]

where,

\[_{^{*}}(_{k},(_{k}),_ {k}) =(^{*}(_{k},(_{k},t_{k},k))+_{k},t_{k}-_{}(_{k},t_{k},k),k+1)\] \[r(_{k},(_{k})) =_{b^{*}}(_{k},(_{k},t_{k},k)).\]

In the following, we provide a simple proposition which shows that our reformulated problem is equivalent to its continuous-time counterpart from Section 2.

**Proposition 1**.: _The problem in Equation (2) and 3 are equivalent to Equation (6) and 7, respectively._

Figure 1 depicts the influence of interaction cost and \(K\) on the controller's performance for the pendulum environment.

## 4 TaCoS with Model-free RL Algorithms

We now illustrate the performance of TaCoS on several well-studied robotic RL tasks. We consider the RC car (Kabzan et al., 2020), Greenhouse (Tap, 2000), Pendulum, Reacher, Halfcheetah and Humanoid environments from Brax (Freeman et al., 2021). Thus our experiments range from environments necessitating time-adaptive control like the Greenhouse, a realistic and highly dynamic race car simulation, and a very high dimensional RL task like the Humanoid.3

Figure 1: Experiment on the Pendulum environment for the average cost and a bounded number of switches setting.

We investigate both the bounded number of interactions and interaction cost settings in our experiments. In particular, we study how the bound \(K\) affects the performance of TaCoS and compare it to the standard equidistant baseline. We further study the interplay between the stochasticity of the environments (magnitude of \(^{*}\)) and interaction costs and the influence of \(t_{}\)on TaCoS. For all experiments in this section, we combine SAC with TaCoS (SAC-TaCoS).

How does the bound on the number of interactions \(K\) affect TaCoS?We analyze the bounded number of interactions setting (cf. Section 3.2) of TaCoS, studying the relationship between the number of interactions and the achieved episode reward. We compare our algorithm with the standard equidistant time discretization approach which splits the whole horizon \(T\) into \(T/K\) discrete time steps at which an interaction takes place. We evaluate the two methods in the greenhouse and pendulum environments. For the pendulum, we consider the swing-up and swing-down tasks. The results are reported in Figure 2. The time-adaptive approach performs significantly better than the standard equidistant time discretization. This is particularly the case for the greenhouse and pendulum swing-down tasks. Both tasks involve driving the system to a stable equilibrium and thus, while high-frequency interaction might be necessary at the initial stages, a fairly low interaction frequency can be maintained when the system has reached the equilibrium state. This demonstrates the practical benefits of time-adaptive control.

How does the interaction cost magnitude influence TaCoS?We investigate the setting from Section 3.1 with interaction costs. In our experiments, we always pick a constant cost, i.e., \(c(,)=C\). We study the influence of \(C\) on the episode reward and on the number of interactions that the policy has with the system within an episode. We again evaluate this on the greenhouse and pendulum environment. For the pendulum, we consider the swing-up task. The results are presented in the first row of Figure 3. Noticeably, increasing \(C\) reduces the number of interactions. The decrease is

Figure 3: Effect of interaction cost (first row) and environment stochasticity (second row) on the number of interactions and episode reward for the Pendulum and Greenhouse tasks.

Figure 2: We study the effects of the bound on interactions \(K\) on the performance of the agent. TaCoS performs significantly better than equidistant discretization, especially for small values of \(K\).

d drastic for the greenhouse environment since it can be controlled with considerably fewer interactions without having any effect on the performance. Generally, we observe that decreasing the number of interactions, that is, increasing \(C\), also results in a slight decline in episode reward.

How does environment stochasticity influence the number of interactions?We analyze the influence of the environment's stochasticity, i.e., the magnitude of the diffusion term \(^{*}\), on the episode reward and number of interactions on TaCoS. Intuitively, the more stochastic the environment, the more interactions we would require to stabilize the system. We again evaluate our method on the greenhouse and pendulum swing-up tasks. The results are reported in the second row of Figure 3. The results verify our intuition that more stochasticity in the environment generally leads to more interactions. However, we observe that the policy is still able to achieve high rewards for a wide range of magnitude of \(^{*}\). This showcases the robustness and adaptability of TaCoS to stochastic environments.

How does \(t_{}\) influence TaCoS?As highlighted in Section 1, picking the right discretization for interactions is a challenging task. We show that TaCoS can naturally alleviate this issue and adaptively pick the frequency of interaction while also being more computationally and data-efficient. Moreover, we show that TaCoS is robust to the choice of \(t_{}\), which represents the minimal duration an action has to be applied, i.e., its inverse is the highest frequency at which we can control the system. In this experiment, we consider SAC-TaCoS and compare it to the standard SAC algorithm. TaCoS adaptively picks the number of interactions and therefore during an episode of time \(T\), it effectively collects less data than the standard discrete-time RL algorithm.4 This makes comparison to the discrete-time setting challenging since environment interactions and physical time on the environment are not linearly related for TaCoS as opposed to the standard discrete-time setting. Nevertheless, to be fair to the discrete-time method, we give SAC more physical time on the system for all environments, effectively resulting in the collection of more data for learning. Since the standard SAC algorithm updates the policy relative to the data amount, we consider a version of SAC, SAC-MC (SAC more compute), which leverages the additional data it collects to perform more gradient updates. This version essentially performs more policy updates than SAC-TaCoS and thus is computationally more expensive. Furthermore, to demonstrate the generality of our framework, we also combine TaCoS with PPO (PPO-TaCoS).

We report the performance after convergence across different \(t_{}\) in the first row of Figure 4. From our experiment, we conclude that SAC-TaCoS and PPO-TaCoS are robust to the choice of \(t_{}\)

Figure 4: We compare the performance of TaCoS in combination with SAC and PPO with the standard SAC algorithm and SAC with more compute (SAC-MC) over a range of values for \(t_{}\) (first row). In the second row, we plot the episode reward versus the physical time in seconds spent in the environment for SAC-TaCoS, SAC, and SAC-MC for a specific evaluation frequency \(}{{t_{}}}\). We exclude PPO-TaCoS in this plot as it, being on-policy, requires significantly more samples than the off-policy methods. While all methods perform equally well for standard discretization (denoted with \(1/t^{*}\)), our method is robust to interaction frequency and does not suffer a performance drop when we decrease \(t_{min}\).

and perform equally well when \(t_{}\) is decreased, i.e., frequency is increased. This is in contrast to the standard RL methods, which have a significant drop in performance at high frequencies. This observation is also made in prior work (Hafner et al., 2019). Crucially, this highlights the sensitivity of the standard RL methods to the frequency of interaction. In the second row of Figure 4 we show the learning curve of the methods for a specific frequency \(}{{t_{}}}\). From the curve, we conclude that SAC-TaCoS achieves higher rewards with significantly less physical time on the environment. We believe this is because our method explores more efficiently (akin to Dabney et al., 2020; Eberhard et al., 2022), and also learns a much stronger/continuous-time representation of the underlying MDP.

Interestingly, at the default frequency used in the benchmarks \(}{{t^{*}}}\), all methods perform similarly. However, slightly decreasing the frequency already leads to a drastic drop in performance for all methods. Intuitively, decreasing the frequency prevents us from performing the necessary fine-grained control and obtaining the highest performance.

While we have access to the optimal frequency \(}{{t^{*}}}\) for these benchmarks, for a general and unknown system it is very difficult to estimate this frequency. Furthermore, as we observe in our experiments, picking a very high frequency is also not an option when using standard RL algorithms. We believe this is where TaCoS excels as it adaptively picks the frequency of interaction, thereby relieving the problem designer of this decision.

## 5 Efficient Exploration for TaCoS via Model-Based RL

In this section, we propose a novel model-based RL algorithm for TaCoS called **O**ptimistic **TaCoS** (OTaCoS). We analyze the episodic setting, where we interact with the system in episodes \(n=1,,N\). In episode \(n\), we execute the policy \(_{n}\), collect measurements and integrated rewards \((_{n,0},b_{n,0}),,(_{n,k_{n}},b_{n,k_{n}})\), and prepare the data \(_{n}=\{(_{n,1},_{n,1}),,(_{n,k_{n}},_ {n,k_{n}})\}\), where \(_{n,i}=(_{n,i-1},_{n,i-1},t_{n,i-1})\) and \(_{n,i}=(_{n,i},b_{n,i})\). From the dataset \(_{1:n}}}{{=}}_{i n} _{i}\) we build a model \(_{n}\) for the unknown function \(^{*}\) such that it is well-calibrated in the sense of the following definition.

**Definition 1** (Well-calibrated statistical model of \(^{*}\), Rothfuss et al. (2023)).: _Let \(}}{{=}}\). We assume \(^{*}_{n 0}_{n}\) with probability at least \(1-\), where statistical model \(_{n}\) is defined as_

\[_{n}}}{{=}}\{: ^{d_{x}+1},  j\{1,,d_{x}+1\}:|_{n,j}()-f_{j}()|_{n} ()_{n,j}()\},\]

_Here, \(_{n,j}\) and \(_{n,j}\) denote the \(j\)-th element in the vector-valued mean and standard deviation functions \(_{n}\) and \(_{n}\) respectively, and \(_{n}()_{ 0}\) is a scalar function that depends on the confidence level \((0,1]\) and which is monotonically increasing in \(n\)._

Similar to model-based RL algorithms for the discrete-time setting (Kakade et al., 2020; Curi et al., 2020; Sukhija et al., 2024), we follow the principle of optimism in the face of uncertainty and select the policy \(_{n}\) for both settings of TaCoS (cf. Sections 3.1 and 3.2) by solving:

\[_{n}}}{{=}}*{argmax}_{ _{}}_{_{n-1}}V_{,}(_{0},T),\] (8)

where \(\{IC,BI\}\) is the appropriate policy class from Section 3. Running OTaCoS for \(N\) episodes, we measure the performance via the _regret_:

\[R_{N}=_{n=1}^{N}V_{^{*},^{*}}(_{0},T)-V_{ {}_{n},^{*}}(_{0},T).\]

Here \(^{*}\) is the optimal policy from the class of policies we optimize over. Any kind of regret bound requires certain assumptions on the regularity of the underlying dynamics (1).

**Assumption 1** (Dynamics model).: _Given any norm \(\|\|\), we assume that the drift \(^{*}\), and diffusion \(^{*}\) are \(L_{^{*}}\) and \(L_{^{*}}\)-Lipschitz continuous, respectively, with respect to the induced metric. We further assume \(_{}\|^{*}()\|_{F} A\)._

Assumption 1 ensures the existence of the SDE (1) solution under policy \(_{n}\). To provide bounds on the performance of OTaCoS for settings Sections 3.1 and 3.2 we also need some assumptions on the noise and reward model.

**Assumption 2** (Reward and noise model for Section 3.1 Setting).: _Given any norm \(\), we assume that running reward \(b\) is \(L_{b}\)-Lipschitz continuous, with respect to the induced metric. We further assume boundedness of the reward \(0 b^{*}(,) B\), and interaction cost \(0 c(,) C\). The dynamics noise is independent and follows: \(_{k}^{2}(0,^{2}(_{k},_{k},t_{k})I_{ d_{x}})\)._

**Assumption 3** (Reward and noise model for Section 3.2 Setting).: _Given any norm \(\), we assume that the running reward \(b\) is \(L_{b}\)-Lipschitz continuous, w.r.t. to the induced metric._

Finally, we assume that we learn a well-calibrated model of the unknown flow \(^{*}\).

**Assumption 4** (Well calibration assumption).: _Our learned model is an all-time-calibrated statistical model of \(^{*}\), i.e., there exists an increasing sequence of \((_{n}())_{n 0}\) such that our model satisfies the well-calibration condition, cf., Definition 1._

Analogous assumptions are made for model-based RL algorithms in the discrete-time setting (Curi et al., 2020; Sukhija et al., 2024). This calibration assumption is satisfied if \(^{*}\) can be represented with Gaussian Process (GP) (Williams and Rasmussen, 2006; Kirschner and Krause, 2018) models.

**Theorem 2**.: _Consider the setting from Section 3.1 and let Assumption 1, 2, and Assumption 4 hold. Then we have with probability at least \(1-\):_

\[R_{N}(_{N-1}T^{3/2}_{N}})\]

_Now consider, the setting with a bounded number of switches \(K\), and let Assumption 1, 3, and Assumption 4 hold. Then, we get with probability at least \(1-\)_

\[R_{N}(_{N-1}^{K}Ke^{D(L_{^{*}}+L_{^{*}}^{ 2})(1+L_{})TK}_{N}}),\]

_where \(D\) is a constant. Here, with \(_{N}\) we denote the model-complexity after observing \(N\) points (Curi et al., 2020), which quantifies the difficulty of learning \(^{*}\). For GPs, it behaves similar to the maximum information gain \(_{N}\)(Srinivas et al., 2009), i.e., implying sublinear regret for several common kernels (Vakili et al., 2021)._

As a proof of concept, we evaluate OTaCoS on the pendulum and RC car environment for the interaction cost setting. 5 As baselines, we adapt common model-based RL methods such as PETS (Chua et al., 2018) and planning with the mean to TaCoS. We call them PETS-TaCoS and Mean-TaCoS, respectively. The result is reported in Figure 5. From the figure, we conclude that OTaCoS is more sample efficient than other model-based baselines and SAC-TaCoS (SAC-TaCoS requires circa \(6000\) episodes for the pendulum and \(2000\) for the RC car).

## 6 Related Work

Similar to this work, Holt et al. (2023); Ni and Jang (2022); Karimi (2023) consider continuous-time deterministic dynamical systems where the measurements or control input changes can only

Figure 5: We run OTaCoS on the pendulum and RC car environment. We report the achieved reward averaged over five different seeds with one standard error.

happen at discrete time steps. Moreover, Holt et al. (2023) proposes a similar problem as ours from Section 3.1, where they specify a cost on the number of interactions. However, their solution is based on a heuristic, where a measurement is taken when the variance of the potential reward surpasses a prespecified threshold. On the contrary, we directly tackle this problem at hand and propose a general framework for time-adaptive control that does not rely on any heuristics. Karimi (2023) adapt SAC (Haarnoja et al., 2018) to include a regularization term, which effectively adds a cost for every discrete interaction. Ni and Jang (2022) induce a soft-constraint on the duration \(\) of each action in the environment. However, all the aforementioned works propose heuristic techniques to minimize interactions, whereas we formalize the problem systematically for the more general case of SDEs and show that it has an underlying MDP structure that any RL algorithm can leverage. In addition, we propose a no-regret model-based RL algorithm for this setting and analyze its sample complexity.

Temporal abstractions are considered also in the framework of options (Sutton et al., 1999; Mankowitz et al., 2014; Mann and Mannor, 2014; Harb et al., 2018). However, a key difference to TaCoS is that in the options framework, the agent measures the state even between the controller switches.

Learning to repeat actionsSeveral works observe that repeating actions in the discrete-time MDPs problems such as Atari (Mnih et al., 2013; Braylan et al., 2015) or Cartpole (Hafner et al., 2019) significantly increase the speed of learning. However, the action repeat is fixed through the entire rollout and treated as a hyperparameter. Durugkar et al. (2016); Vezhnevets et al. (2016); Srinivas et al. (2017); Sharma et al. (2017); Lee et al. (2020); Grigsby et al. (2021); Chen et al. (2021); Nam et al. (2021); Yu et al. (2021); Biedenkapp et al. (2021); Krale et al. (2023) automate the selection of action repeat, and show superior performance over the fixed number setting. Dabney et al. (2020) empirically show that repeating the actions helps with the exploration, effectively having a similar effect that colored noise exploration has over the standard white noise exploration (Eberhard et al., 2022).

Continuous-time RLFollowing the seminal work of Doya (2000) and the advances in Neural ODEs of Chen et al. (2018), continuous-time RL has regained interest (Cranmer et al., 2020; Greydanus et al., 2019; Yildiz et al., 2021; Lutter et al., 2021). Moreover, modeling in continuous-time is found to be particularly useful when learning from different data sources where each source is collected at a different frequency (Burns et al., 2023; Zheng et al., 2023). An important line of work exists for modeling continuous dynamics for the case when states and actions are discrete, called Markov Jump Processes (Kallianpur and Sundar, 2014; Berger, 1993; Huang et al., 2019; Seifner and Sanchez, 2023). Another line of work that is close to ours is event and self-Triggered Control (Astrom and Bernhardsson, 2002; Anta and Tabuada, 2010; Heemels et al., 2012, 2021), where they model continuous-time control systems by implementing changes to the input only when stability is at risk, ensuring efficient and timely interventions. Treven et al. (2023) propose a no-regret continuous-time model-based RL algorithm, which akin to OTAcoS, performs optimistic exploration. They study the problem where controls can be executed continuously in time and propose adaptive measurement selection strategies. Similarly, we propose a novel model-based RL algorithm, OTAcOS, based on the principle of optimism in the face of uncertainty. We show that OTAcoS has no regret for sufficiently smooth dynamics and has considerable sample-efficiency gains over its model-free counterpart.

## 7 Conclusion and discussion

We study the problem of time-adaptive RL for continuous-time systems with continuous state and action spaces. We investigate two practical settings where each interaction has an inherent cost and where we have a hard constraint on the number of interactions. We propose a novel RL framework, TACoS, and show that both of these settings result in extended MDPs which can be solved with standard RL algorithms. In our experiments, we show that combining standard RL algorithms with TaCoS results in a significant reduction in the number of interactions without having any effect on the performance for the interaction cost setting. Furthermore, for the second setting, TaCoS achieves considerably better control performance despite having a small budget for the number of interactions. Moreover, we show that TaCoS improves robustness to a large range of interaction frequencies, and generally improves sample complexity of learning. Finally, we propose, OTAcoS, a no-regret model-based RL algorithm for TaCoS and show that it has further sample efficiency gains.