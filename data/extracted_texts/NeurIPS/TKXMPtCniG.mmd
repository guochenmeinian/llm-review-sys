# Accelerating Large Batch Training via Gradient Signal to Noise Ratio (GSNR)

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

As models for nature language processing (NLP), computer vision (CV) and recommendation systems (RS) require surging computation, a large number of GPUs/TPUs are paralleled with a large batch (LB) to improve training throughput. Training such LB tasks often converges to sharp minimum and downgrades final precision. Adversarial learning (ConAdv) and LANS method scales ImageNet and BERT pretraining up to 96k batch size. In this work, we develop the variance reduced gradient descent technique (VRGD) based on the gradient signal to noise ratio (GSNR) and apply it onto popular optimizers such as SGD/Adam/LARS/LAMB. We carry out a theoretical analysis of VR-SGD's convergence rate to explain its fast training dynamics, and a generalization analysis to demonstrate its smaller generalization gap on LB training. Comprehensive experiments demonstrate that VRGD can remarkably accelerate training (\(1.7 4\)), narrow the generalization gap and improve final accuracy. We push the batch size limit of BERT pretraining up to 128k/64k and DLRM to 512k without noticeable accuracy loss. We improve ImageNet Top-1 accuracy at 96k by \(0.52pp\) than LARS and significantly reduce generalization gap by \(68.3\%\).

## 1 Introduction

Recent machine learning models have grown wider and deeper in their architectures (e.g., GPT-3 , M6 , Switch Transformer ). Training complex models may consume more training data to converge, which needs a surge in computing capacity and efficiency. However, hardware improvement can not keep pace with the expansion of model calculations .

Several techniques to speed up training are proposed. The aggregation and scattering of gradients among massive workers requires an efficient synchronization algorithm. Since the communication bandwidth between GPUs/TPUs is much higher than CPU-GPU (e.g., NVLink, Foley and Danskin ), several efficient synchronization strategies such as Ring-All-Reduce  and software toolkits like Horovod  are proposed to replace the traditional PS-Worker framework . In addition, training with LB can notably improve throughput . You _et al._ successfully train BERT using \(1024\) TPUs and a LB (64k) within 76 minutes. It demonstrates the efficiency of GPUs/TPUs in large scale parallel tasks. Small batch (SB) is not able to fully utilize those powerful GPUs/TPUs.

However, Keskar _et al._ theoretically analyze the LB training and finds that it can be easily trapped into sharp local minimum, leading to strong generalization gap. Hoffer _et al._ indicate that the generalization gap can be attributed to the fewer update steps in LB training compared with SB when using identical epochs. Dai and Zhu  theoretically demonstrate that training with more steps or expanding the learning rate to batch size ratio helps to converge to a flatter localminimum. Although these issues can be partly resolved by layer-wise adaptive rate scaling (LARS, You _et al._ (2017)) and layer-wise adaptive large batch (LAMB, You _et al._ (2020)), the batch size limit still exists.

To push the batch size limit and reduce generalization gap, we propose the **element-wise adaptive** techniques called variance reduced gradient descent technique (VRGD) based on GSNR of parameters. Our contributions are listed below:

* We carry out theoretical derivations of convergence rate and generalization analysis to explain why VRGD can accelerate LB training and achieve dramatically smaller generalization gap.
* We perform comprehensive LB experiments and find that VRGD can remarkably accelerate training (\(1.7 4\)), narrow the generalization gap and improve final precision than previous SOTA (e.g., LAMB, LARS).
* VR-LAMB pushes the batch size limit of BERT pretraining up to **128k/64k** without any accuracy loss, while LAMB stops scaling at 64k/32k. VR-LARS improves the ImageNet Top-1 accuracy to \(74.82\%\) at 96k, **0.52pp** higher than LARS. The generalization gap of ImageNet trained with VR-LARS is dramatically reduced by **68.3%** comparing with LARS at 96k. VR-SGD pushes the batch size limit of DLRM from 64k to **512k** without noticeable accuracy loss.

## 2 Related Work

### Large Batch Training

Several techniques are proposed to improve the optimization and generalization ability in LB training. Goyal _et al._ (2017) propose a linear scaling rule on learning rate (LR) to achieve the same accuracy as SB and push the batch size limit of ImageNet to 8k. EXTRAP-SGD uses the extra-gradient to stabilize the optimization trajectory and smooth training (Lin _et al._, 2020). SWAP quickly trains the model with LB in the first stage and refines it by averaging the weights of multiple SB models in the second stage (Gupta _et al._, 2020). Batch Augmentation replicates multiple instances with the same batch size to improve generalization (Hoffer _et al._, 2019). The batch size of the experiments in EXTRAP-SGD/SWAP/Batch-Augmentation are less than 8k and are not compared in our experiments.

DecentLaM removes the growing momentum-incurred bias observed in DmSGD and pushes ImageNet to 32k (Yuan _et al._, 2021). Layer-wise LRs adjustment optimizers such as LARS (You _et al._, 2017), complete layer-wise adaptive rate scaling (CLARS, Huo _et al._ (2021)), LAMB (You _et al._, 2020) successfully improve the batch size up to 64k both for ImageNet and BERT pretraining without accuracy loss. Recently, the concurrent adversarial learning (ConAdv) method pushes the batch size limit of ImageNet training up to 96k (Liu _et al._, 2021). LANS replaces the layer-wise LR adjustment in LAMB with block-wise style (Zheng _et al._, 2020) and also pushes BERT training up to 96k. Adasum adds those gradients after scaling with proper scalars and even pushes the batch size limit of BERT up to 128k/32k (Maleki _et al._, 2021).

### Gradient Variance and GSNR

Unlike gradient mean, which is widely used in optimizers, gradient variance and its successor GSNR are less used. But gradient variance is frequently discussed in generalization gap. Johnson and Zhang (2013) propose the stochastic variance reduced gradient (SVRG) with the explicit gradient variance reduction method. Other variants of SVRG like SRVR-NPG, SVRPG and Control Variate methods are also proposed to reduce the gradient variance during training (Liu _et al._, 2020; Wang _et al._, 2013; Papini _et al._, 2018; Miller _et al._, 2017). Rainforth _et al._ (2018) use GSNR to analyze the variational bounds in variational auto-encoder (VAE). McCandlish _et al._ (2018) use GSNR to predict the useful upper bound of batch size. Smith _et al._ (2018); Devarakonda _et al._ (2017) adaptively increase the batch size during training to achieve acceleration without accuracy loss. Liu _et al._ (2020) theoretically derive a quantitative relationship between GSNR and generalization gap and prove that larger GSNR leads to better generalization performance. Therefore, gradient variance and GSNR are potentially useful to train deep neural networks.

## 3 Preliminaries

### Gsn

Given a data distribution \(=\), a model \(=f(x,)\) parameterized by \(\) and the loss function \(L\). The parameters' gradient _w.r.t._ sample \((x_{i},y_{i})\) can be written as (Refer to all "notations" in the Appendix.C):

\[_{i}():=,f(x_{i},))}{}\] (1)

Then \(j\)-th parameter' \((_{j})\) gradient computed using \((x_{i},y_{i})\) is \(_{i}(_{j})\). Here we use \(i\) to index the data samples and \(j\) to index the parameters of \(\). We denote the sample-wise gradient mean as \(}()=_{(x,y)}((x,y, ))\) and variance of \(_{i}()\) as \(^{2}()=_{(x,y)}((x,y,))\). The GSNR for each model parameter \(_{j}\) is defined as:

\[r(_{j}):=}^{2}(_{j})}{^{2}(_{j})}\] (2)

Intuitively, GSNR measures the consistency of the gradient direction of each parameter across a batch of data samples. The gradient space of the parameters tends to converge in the same direction when the GSNR is large, but diverge if the GSNR is small (Figure.1).

### GSNR and Generalization Gap

Consider a training set \(D=\{(x_{1},y_{1}),...,(x_{n},y_{n})\}^{(n)}\), where \(n\) samples come from \(\), and a test set of dataset size \((n^{})\) from \(}^{(n^{})}\) denoted by \(D^{}=\{(x^{}_{1},y^{}_{1}),...,(x^{}_{n^{}},y^{ }_{n^{}})\}}^{(n^{})}\). The empirical training and test losses can be denoted as:

\[L[D]=_{i=1}^{n}L(y_{i},f(x_{i},)); L[D^{}]= }_{i=1}^{n^{}}L(y^{}_{i},f(x^{}_{i}, ))\] (3)

respectively. Then the empirical generalization gap is given by \(L[D^{}]-L[D]\). Both the training loss \(L[D]\) and the test loss \(L[D^{}]\) would decrease after one training step and can be denoted as \( L[D]\) and \( L[D^{}]\) respectively. The ratio between the expectations of \( L[D]\) and \( L[D^{}]\) for one training step can be denoted as:

\[(,n):=^{n}}( L [D^{}])}{E_{D^{n}}( L[D])}\] (4)

**Assumption 1** (Non-overfitting limit approximation of Liu _et al._(2020)).: _The parameters' gradient over the training set and test set i.e., \(_{D}()\) and \(_{D^{}}()\) obey the same distribution._

Based on Assumption 1 and using a small learning rate \( 0\), Liu _et al._(2020) derive the relationship between the one-step generalization ratio (eq.4) and GSNR:

\[(,n)=1-_{j}W_{j}+},\;\;\;W_{j}:=^{n}}( L_{j}[D])}{ E_{D^{n}}( L[D])}_{j}W_{j}=1\] (5)

where \( L_{j}[D]\) is the training loss reduction caused by updating \(_{j}\). A more _detailed mathematical derivation_ can be found in Liu _et al._(2020). This relationship (eq.5) demonstrates that GSNR (\(r_{j}\)) plays a crucial role in determining the generalization performance of the model. Updating the model parameters with smaller GSNR leads to generalization gap growth. Also note that we have \((,n) 1\) when \(n\), which means that training with a larger dataset helps generalization.

## 4 Proposed Algorithms

In this section, we propose VRGD with their general updating rules (taking VR-SGD as an example in Algorithm.1). The SGD is shown in Appendix.D for comparison.

Figure 1: Schematic of VRGDâ€™s mechanism: updating parameters with larger GSNR (left panel) and smaller GSNR (right panel).

### VR-SGD's Updating Rules

Consider the simple updating rule for SGD as follows:

\[_{t}=_{t-1}-}(_{t})\] (6)

where \(\) is the learning rate. Previous section demonstrates that updating the weights with larger GSNR confines the model's generalization gap growth during training. Therefore, GSNR can be used in the optimizer for better generalization. In the mathematical derivation of GSNR's role on the generalization gap, all sample-wise gradients for the entire dataset are used to compute the gradient variance, which is less efficient. However, in the LB training training, where each batch is large enough to accurately estimate the gradient variance, we replace the entire dataset with a LB and the sample-wise with device-wise gradient computation. Gradients on each GPU/TPU device can be synchronized using Ring-AllReduce, thus perfectly avoiding the inefficiency of gradient variance computation. The simplified gradient variance computation is as follows:

\[_{t}^{2}=_{d=1}^{k}}_{d}^{2}(_{t})- }^{2}(_{t})\] (7)

where \(k\) devices are used, each of which computes \(1/k\) part of the gradient \(}_{d}(_{t})\), the same as what data parallel does. The GSNR can then be easily calculated based on eq.2 (\(^{2}(_{j})\) is replaced by \(_{j}^{2}\)). The mean values of GSNR are removed at each layer before applying gradient to the parameters. This normalization of GSNR ensures that the global averaged GSNR remains at \(1.0\):

\[r(_{t}^{(l)})=^{(l)})}{_{j=1}^{J}r( _{t,j}^{(l)})}\] (8)

where \(l^{th}\) layer contains \(J\) parameters. We constrain the \(max/min\) of GSNR within \(1/\) so that those neurons with very small GSNR remain active:

\[r(_{t}^{(l)})=,&if\ r(_{t}^{(l)})<\\ 1,&if\ r(_{t}^{(l)})>1\] (9)

where \(\) is a hyper-parameter used here. For simplicity, we **don't tune**\(\) but set it to \(0.1\) in all of our experiments by default. Finally, we element-wisely adapt \(\) according to GSNR of each parameter and get the updating rule for VR-SGD:

\[_{t}=_{t-1}- r(_{t})}( _{t})\] (10)Figure.1 shows the mechanism of VRGD. As for a good estimation of gradient mean (left panel), optimizer should be confident to move along the direction of gradient mean or even further. However, when gradients on the devices are scatteredly distributed (right panel), updating weights with gradient mean may bring noises and slow down convergence, which should be avoided.

**Differences compared with existing LB methods:**

* The linear scaling rule uses the same large LR for all parameters, which tends to diverge when some gradients are too large; LARS/LAMB/LANS use large LRs for some layers but layer-wisely or block-wisely limit LRs when \(||_{t}||\) is compatible with its updating quantity, i.e., \(||_{t}||||}(_{t})||\); VRGD that we propose here **element-wisely** limit the updating quantity for those parameters without confident gradient estimation (Fig.1b, large gradient variance or small GSNR).
* GSNR and its relationship with generalization gap is discussed in Liu _et al._ (2020), but further work to embed such GSNR into the optimizers is missing. In our work, we apply GSNR in the SGD/LARS/LAMB and demonstrate that GSNR helps the model maintain a small generalization gap in LB training based on the derivations of the generalization gap and ImageNet experiments.
* VRGD does not need extra-gradient used in EXTRAP-SGD or the two-stage training like SWAP. Sub gradients used in Batch Augmentation have different transforms each while VRGD uses the same transforms. Adasum adaptively sums two gradients scaled by a constant while VRGD still uses the mean gradient.

### VR-Adam, VR-LAMB and other VRGD optimizers

GSNR can be easily applied on any optimizer using the general updating rules shown above. Here we discuss those popular optimizers frequently used in the research community, e.g., SGD, Adam, LARS and LAMB. As for VR-Adam, GSNR is calculated directly based on \(}(_{t})\) and then used to adapt the gradient mean before gradients' momentum estimation. Similar with the gradients' momentum, we apply the momentum mechanism on GSNR (\(_{t}\)) for faster convergence. If we adapt the final update term, i.e. \(_{t}_{t-1}- r(_{t})_{t}/( _{t}}+)\), the \(1^{st}\) and \(2^{nd}\) order momentum estimation (\(m_{t}\) and \(v_{t}\)) for the next training step would be biased (meaning that the update term cannot be inferred merely on \(_{t}\) and \(_{t}\) since \(r(_{t}) 1\)).

VR-LAMB is similar to VR-Adam, except that VR-LAMB layer-wisely adapt the LRs for stable convergence when using very large LRs. VR-Adam and VR-LAMB are shown in Appendix.D. VR-LARS and VR-Momentum, which are based on LARS and Momentum, are similar to VR-SGD that it uses GSNR to adapt the gradient means before applying them to the model weights (algorithms omitted).

## 5 Theoritical Analysis

### Convergence Analysis

**Assumption 2** (bounded gradient).: \(|| L()|| G\)

**Assumption 3** (\(l\)-smooth).: \( l>0\) _satisfies \(|| L(x)- L(y)|| l||x-y||\)_

We mathematically derive the convergence rate of VR-SGD under nonconvex settings and assume the training process satisfies Assumption.2 and Assumption.3, which are widely used in convergence analysis (Shamir and Zhang, 2013; Ghadimi and Lan, 2013; Allen-Zhu and Hazan, 2016; Allen-Zhu et al., 2019; You _et al._, 2020). Table.1 of Appendix compares the assumptions of ours and those popular optimizers. It shows that our assumptions are weaker than LARS/LAMB/DecentLaM and similar with SGD. Detailed derivations can be found in Appendix.A. Then we have Theorem.1.

**Theorem 1**.: _Let \(_{t}=)-L(^{*})}{T||||_{1}}}\) and \(}}=)-L(^{*})||||_{1} ]}{T}}\), VR-SGD is bounded by:_

\[E|| L(_{t})||^{2}((1+^{2}G^{2}}{2} )^{2}}})\] (11)

_where \(r_{l}\) and \(r_{u}\) are the lower and upper bound of GSNR._

**Convergence rates discussion**: 1) The convergence rate \((})\) of VR-SGD is the same as SGD [(20)]; 2) VR-SGD's bound depends on the lower (\(r_{l}\)) and upper bound (\(r_{u}\)) of GSNR. Larger batch size brings smaller gradient variance (eq.43 of Appendix.B) and larger GSNR (both bigger \(r_{l}\) and \(r_{u}\)), then may result in **a tighter bound with quicker convergence** (_verified by experiments shown in Figure.2_).

### Generalization Gap

This section derives the generalization gap of SGD and VR-SGD during SB and LB scenarios. Detailed derivations can be found in Appendix.B. Citing eq.14 of Liu _et al._ [(2020)] below, i.e., when training satisfies Assumption.1 and \( 0\), after one training step the expectation of empirical generalization gap at \(t^{th}\) step is:

\[E_{D,D^{}^{n}}(_{t}L[D]-_{t}L[D^{}])= _{j}_{t,j}^{2}+O(^{2})\] (12)

where we use \(_{t,j}^{2}\) and \(r_{t,j}\) to denote \(^{2}(_{t,j})\) and \(r(_{t,j})\) for simplicity. Next, we assume that the batch size of LB is \(k\) times than that of SB. \(_{0}\) (\(\)) represents the learning rate of SB (LB). The accumulated generalization gap after training \(T\) steps for SB using SGD and \(T/k\) steps for LB can be derived as follows:

\[E(_{SB,SGD})_{0}_{t=1}^{T}_{j}_{t,j}^ {2}; E(_{LB,SGD})_{t=1}^{T/k} _{j}_{t,j}^{2}\] (13)

If we assume "\(_{t,j}\) is \(t\)-independent", eq.13 are simplified as \(E(_{SB,SGD})_{0}T_{j}_{j}^{2}\) and \(E(_{LB,SGD})}_{j}_{j}^{2}\) respectively. Taking \(=k^{2}_{0}\), \(E(_{LB,SGD})\) will have the same accumulated generalization gap as SB. This is known as the linear/square scaling rules. However, the assumption that "\(_{t,j}\) is \(t\)-independent" is unrealistic. Similarly, the accumulated generalization gap of VR-SGD in LB training can be written as:

\[E(_{LB,VR-SGD})_{t=1}^{T/k}_{j}_{t,j}^{2}}{k}=_{t=1}^{T/k}_{j}_{t,j}^ {2}\] (14)

**The generalization gap of SGD and VR-SGD in LB training:**

When training converges (\(_{t,j} 0\)), we have \(_{t,j}^{2}<_{t,j}^{2}\) because \(r_{t,j}=_{t,j}^{2}/_{t,j}^{2} 0\) (verified experimentally by Figure.4 of Liu _et al._ [(2020)]). Therefore, we have \(_{t=1}^{T/k}_{j}_{t,j}^{2}<_{t=1}^{T/k}_{j}_{t,j}^{2}\), i.e., \(E(_{LB,VR-SGD})<E(_{LB,SGD})\). This inequality demonstrates that VR-SGD has a **much smaller generalization gap** than SGD in LB training (_verified by our ImageNet experiments shown in Table.3_ ).

## 6 Experiments

In this section, we show comprehensive experiments on commonly used LB benchmarks such as BERT Pretraining [(1)], ImageNet-2012 [(17)] and DLRM [(16)]. We mainly adopt the square root rules to scale LRs. We set the hyper-parameters of VRGD as \(=0.1\) and \(k\) to the minimum GPU devices that can hold the LB without out of memory for resource efficiency (but satisfy \(k 8\)) in all experiments. Similar with other optimizers, VRGD can generate a generally good training curve using default sets. The \(1^{st}\) and \(2^{nd}\) order decay rates are set to \(_{1}=_{3}=0.9,_{2}=0.999\) by default. Experiments are performed with TensorFlow on 96 DGX-A100 nodes (768-GPUs).

### BERT Pretraining

BERT pretraining is a common NLP task needs speeding up with LB training. For a fair comparison, we use the same settings as LAMB [(19)] except optimizer and learning rate: (1) BERT large pretrains using Wikipedia and BooksCorpus and then finetunes on SQuAD(v1.1) to evaluate itsprecision with F1 score; (2) A two-phase training strategy is used. First \(90\%\) steps use a sequence length of 128 (phase-1) and last \(10\%\) use a sequence length of 512 (phase-2). Mixed-Batch Training is used when batch size is set to 64k/32k, 96k/32k and 128k/64k.

We use NVIDIA's best practise1 to carry out VR-LAMB experiments and tune _nothing_ of the downstream SQuAD(v1.1) tasks (same as LAMB). Detailed hyper-parameters are listed in Appendix.D. Results shown in Table.1 indicate that:

* VR-LAMB outperforms LAMB (widely used in BERT LB pretraining) in all batch sizes from 16k to 64k/32k. F1 score is improved up to \(91.49\) at 64k/32k, **0.91pp** higher than LAMB.
* VR-LAMB also outperforms Adam (with standard bias correction and LR discontinuity removal) and LANS by an improvement of **0.84pp** at 64k and **0.63pp** at 96k/32k respectively.
* VR-LAMB pushes the batch size limit up to **128k/64k** using just **4301** steps and maintains a F1 score of \(90.85\). Although Adasum achieves a F1 score of \(90.50\) at 128k/32k, but it needs \(6137\) steps to converge (\(30\%\) extra steps than VR-LAMB). VR-LAMB achieves \(50\%\) less steps than LAMB at 64k/32k and even **0.45pp** higher of F1 score than baseline.

### ImageNet with ResNet50

ImageNet training with ResNet50 v1.5  is a standard CV benchmark for LB training. We use the default sets of official best practise of Google Tensorflow2 with linear LR warm-up, label smoothing and cosine LR decay (to \(0\)). It is the same setup as LARS . We merely adjust the optimizers and learning rate for a fair comparison. We find some successful LB applications using Momentum, LAMB and LARS, but not for Adam, AdaGrad or AdamW optimizers . LARS based on Momentum is more fitful on CV tasks. Therefore, we merely apply VR-LARS on ImageNet. Detailed hyper-parameters are listed in the appendix.D.

The results shown in Table.2 indicate that:

* VR-LARS outperforms Momentum, DecentLaM, LAMB and LARS (previous SOTA) in all batch sizes (from **0.03pp** to **0.56pp**). The improvements are higher for larger batch size.

 
**Batch Size** & **16k** & **32k** & **64k/32k** & **64k** & **96k/32k** & **96k** & **128k/32k** & **128k/64k** \\ 
**Steps** & 31250 & 15625 & 8599 & 7820 & 6256 & 5214 & 6137 & 4301 \\  LAMB\({}^{*}\) & 91.35 & 91.48 & 90.58 & - & - & - & - \\ 
**Adam\({}^{*}\)** & - & 91.58 & 91.04 & 90.46 & - & - & - \\ 
**LANS\({}^{*}\)** & - & - & - & - & 90.60 & - & - & - \\ 
**Adasum\({}^{*}\)** & - & - & - & - & - & - & 90.50 & - \\ 
**VR-LAMB** & **91.42** & **91.58** & **91.49** & **91.30** & **91.23** & **90.70** & **90.85** \\ (**ours**) & & **(+0.07pp)** & **(+0.00pp)** & **(+0.45pp)** & **(+0.84pp)** & **(+0.63pp)** & & - \\  

* means the F1 scores are cited from their work.
* Using median of repeated experiments is the same as Nado _et al._.

Table 1: Dev set F1 score of **BERT pretraining and then finetuning on SQuAD(v1.1)**. Each score is the median result of \(3\) repeated experiments. The baseline of BERT-large on SQuAD(v1.1) is \(90.395\).

 
**Batch Size** & **2k** & **4k** & **8k** & **16k** & **32k** & **64k** & **96k** \\ 
**Momentum\({}^{*}\)** & 76.51\(\%\) & 76.44\(\%\) & 76.26\(\%\) & - & - & - \\ 
**DecentLaM\({}^{*}\)** & 76.43\(\%\) & - & 76.19\(\%\) & 76.73\(\%\) & 76.22\(\%\) & - & - \\ 
**LARS\({}^{*}\)** & - & 77.90\(\%\) & 76.60\(\%\) & 76.60\(\%\) & 75.30\(\%\) & 74.30\(\%\) \\ 
**VR-LARS** & **77.14\(\%\)** & **77.23\(\%\)** & **77.36\(\%\)** & **77.27\(\%\)** & **76.81\(\%\)** & **75.86\(\%\)** & **74.82\(\%\)** \\ (**ours**) & & **(+0.03pp)** & **(+0.31pp)** & **(+0.47pp)** & **(+0.54pp)** & **(+0.21pp)** & **(+0.56pp)** & **(+0.52pp)** \\  

* means the results are cited from their work.

Table 2: Top-1 test accuracy of **ImageNet** using ResNet50. Each test accuracy of VR-LARS(ours) averaged over 5 repeated experiments. The standard Top-1 accuracy of MLPerf-v0.5 is \(74.9\%\).

* VR-LARS achieves \(75.86\%\) accuracy at 64k batch size, **0.56pp** higher than LARS. When batch size reaches up to **96k**, VR-LARS maintains \(74.82\%\) accuracy, close to the MLPerf-v0.5 standard (\(74.9\%\)).

**Generalization Gap**: Table.3 demonstrates that VR-LARS can dramatically narrow the generalization gap in LB training. The generalization gap is only **1.46** for VR-LARS at 96k (**68.3%** smaller than LARS), even smaller than ConAdv+AA (\(2.2\); Liu _et al._ ). Note that VR-LARS can be used together with ConAdv+AA and other techniques for further improvement.

### DLRM Training

Criteo Terabyte click logs dataset (\(4\) billion records) trained with DLRM is a standard CTR prediction benchmark newly added in MLPerf-v0.7. DLRM is used following NVIDIA's best practise1. For a fair comparison, we merely modify LRs and optimizers (hyper-parameters are listed in Appendix.D). Settings of Linear LR warm up, polynomial decay and training with \(1\) epoch are used by their default set up. Results in Table.4 indicates that:

* VR-SGD outperforms SGD in all batch size settings. Similar with experiments shown above, the improvement of VR-SGD w.r.t SGD increases along with larger batch sizes (from **0.12pp** to **2.26pp**).
* VR-SGD pushes the batch size limit up to 512k and maintains a high AUC of **0.8013**, close to the baseline of 0.8014. Note that Google's submission of MLPerf v0.7 merely uses a maximum batch size of 64k [Kumar _et al._, 2021].

## 7 Ablation Studies

### Orthogonal Experiments

In this section, we demonstrate that GSNR is important in optimization and VRGD can be applicable to most popular optimizers using CIFAR10. During CIFAR10 training with ResNet56 [He _et al._,

   **Batch Size** \\ **32k** \\ **Accuracy** \\  }} &  \\  & **32k** & **64k** & **96k** & **32k** & **64k** & **96k** \\   **Train** \\ **Accuracy** \\  } & 82.50 & 79.60 & 78.90 & 80.00 & 78.06 & 76.28 \\   **Test** \\ **Accuracy** \\  } & 76.60 & 75.30 & 74.30 & 76.81 & 75.86 & 74.82 \\   **Generalization** \\ **Gap** \\  } &  &  &  &  &  &  \\  & & & & & & & \\ 

Table 3: Generalization Gap of large batch train- Table 4: Test AUC of **DLRM** trained with SGD and VR-SGD in \(1\) epoch. The reported results are averaged over 5 repeated experiments. The baseline AUC is \(0.8014\) for SGD at 32k batch size.

Figure 2: Composite averaged test accuracy or AUC curves of each optimizer for **CIFAR10** experiments. The abrupt surging of accuracy at 91\({}^{th}\) and 136\({}^{th}\) epoch is caused by decaying LR with a rate of \(0.1\).

2016a,b], we use the default sets of the official best practice for Google Tensorflow2 and mainly add square-root LR scaling rules to perform the \(216\) composite experiments shown in Figure.2. Additional linear LR warm-up, label smoothing and cosine LR decay (to \(0\)) techniques are used to stabilize LB training experiments shown in Table.5, the same as ImageNet training. Detailed hyper-parameters are listed in Appendix.D. As for the test accuracy curves, Figure.2 shows the averaged composite test accuracy curve of all \(216\) experiments for the LR-batch size pairs. Training with VR-Momentum/VR-Adam/VR-LAMB converge much faster (\(1.7 4\)). As for the final precision, Table.5 demonstrate that VR-Momentum/VR-Adam/VR-LAMB/VR-LARS dramatically outperform Momentum/Adam/LAMB/LARS when batch size is larger than 4k, which demonstrates that VRGD is **applicable** to most popular optimizers in LB training. The improvements of VRGD comparing with their base optimizers grow with the increase of batch size. VRGD optimizers remains convergent when batch size reaches 8k.

### GSNR's Behaviour

To understand GSNR's behaviour in VRGD optimizers, we perform the linear regression experiments. The true weights are set to \(W_{i}=i,i\) and the corresponding parameters \(w_{i}\) are initialized to zero. Given randomly generated inputs \(X\), we have the true labels as \(Y=WX\) and the MSE loss as \(L=||Y-wX||_{2}\). Finally, optimize \(w\) with \(100\) steps.

Training about \(50\) (half) steps, VR-SGD is able to converge to the test loss where SGD requires \(100\) steps (Figure.1a of Appendix.D). The weights of VR-SGD (dashed lines of Figure.1b of Appendix.D) converge faster to their ground truth. We find that \(w_{5},w_{6}\) converge firstly, then \(w_{3},w_{8}\) and finally \(w_{1},w_{10}\). Consistently, the GSNR of \(w_{5},w_{6}\) arise firstly (updating \(w_{5},w_{6}\) with larger LRs), then \(w_{3},w_{8}\) while the GSNR of \(w_{5},w_{6}\) decrease slowly (no need to update the converged weights using large LRs). Finally after step \(60\), the GSNR of \(w_{1},w_{10}\) begin to arise. Intuitively, GSNR helps element-wisely fine-tune the LRs for different weights.

### Hyper-parameters Sensitivity

There are two main hyper-parameters in VRGD, i.e., normalization strength factor \(\) and the equivalent GPU device number \(k\). We take use of linear regression trained with VR-SGD using \(batchsize=2048\) shown above to examine the hyper-parameter sensitivity.

Figure.3 shows that the optimal \(\) is around \((0.04,0.2)\) for linear regression. Test loss would be larger if \( 1\), which means **VR-SGD is reduced to SGD**. It again demonstrates that GSNR is valuable to improve final precision. On the other hand, the optimal \(k\) is around \(\). This means that each gradient mean calculated using \(\) samples on each GPU/TPU device, and gradient variance calculated using \(\) values of the gradient mean will return a good evaluation of GSNR. In fact, we do not use the optimal hyper-parameters. Instead, above experiments use \(=0.1\) and set \(k\) to the minimum GPU devices that can hold the LB without out of memory (but satisfy \(k 8\), refer all of the hyper-parameters in Appendix.D). Fine-tuning \(\) and \(k\) may further improve the results.

## 8 Summary

In this paper, we propose the VRGD for large batch training using GSNR. We carry out theoretical derivations of convergence rate and generalization analysis to explain why VRGD can accelerate large batch training and reduce generalization gap. Comprehensive experiments on BERT-pretraining, ImageNet and DLRM verify that VRGD can push the batch size limit than previous SOTA optimizers in LB training and perform better. Codes will be released when published.