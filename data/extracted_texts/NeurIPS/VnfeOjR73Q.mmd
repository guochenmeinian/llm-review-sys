# Bounded rationality in

structured density estimation

 Tianyuan Teng

Center for Life Sciences,

Peking University

tengtianyuan@pku.edu.cn

Equal contributions. \({}^{}\)Co-corresponding authors. T.T. is now at Max Planck Institute for Biological Cybernetics, Germany. L.K.W. is now at Google DeepMind. H.Z. is also affiliated with Beijing Key Laboratory of Behavior and Mental Health, PKU-IDG/McGovern Institute for Brain Research, and PKU-Tsinghua Center for Life Sciences at Peking University, and Chinese Institute for Brain Research, Beijing.

Li Kevin Wenliang\({}^{*,}\)

Gatsyb Computational Neuroscience Unit,

University College London

kevinli@gatsyb.ucl.ac.uk

&Hang Zhang \(\)

School of Psychological and Cognitive

Sciences, Peking University

hang.zhang@pku.edu.cn

###### Abstract

Learning to accurately represent environmental uncertainty is crucial for adaptive and optimal behaviors in various cognitive tasks. However, it remains unclear how the human brain, constrained by finite cognitive resources, internalise the highly structured environmental uncertainty. In this study, we explore how these learned distributions deviate from the ground truth, resulting in observable inconsistency in a novel structured density estimation task. During each trial, human participants were asked to learn and report the latent probability distribution functions underlying sequentially presented independent observations. As the number of observations increased, the reported predictive density became closer to the ground truth. Nevertheless, we observed an intriguing inconsistency in human structure estimation, specifically a large error in the number of reported clusters. Such inconsistency is invariant to the scale of the distribution and persists across stimulus modalities. We modeled uncertainty learning as approximate Bayesian inference in a nonparametric mixture prior of distributions. Human reports were best explained under resource rationality embodied in a decaying tendency towards model expansion. Our study offers insights into human cognitive processes under uncertainty and lays the groundwork for further exploration of resource-rational representations in the brain under more complex tasks.

## 1 Introduction

We study the remarkable ability of humans to learn probabilistic distributions based on experiences in new environments, a skill essential for good performance in a wide range of downstream perceptual  and cognitive  tasks. This ability manifests in everyday experiences, such as when a person learns to communicate with others of varying accentsand personalities, or discovers new social norms in foreign cultures. More impressively, humans can not only summarise key characteristics of new experiences but also describe those patterns in statistical terms, such as "Most people in X country typically talk about the weather, but some younger ones prefer to gossip instead". These observations indicate that humans can develop structured and probabilistic internal beliefs about their surroundings from experiences. To distinguish humans' internal beliefs from the modeling framework to be introduced later, we refer to the former as an _internal construct_ of the external environment. We ask: how does the human mind form internal constructs from experiences?

As experiences typically arrive sequentially in real life, building a probabilistic internal construct is an online density estimation problem. An ideal solution must be both consistent in distribution estimation and invariant to the order of the experiences. Consistency requires that the learned internal construct approach the true environmental uncertainty as more data arrives, expanding in complexity when the data evidence a more complicated true distribution. Invariance to experience order means no _a priori_ preference for any particular model based on the order in which experiences arrive. For instance, if the experiences are independent samples from the same distribution, the learned distribution should not be affected by the sample order. Computationally, these two desiderata can be achieved by choosing an appropriate **internal construct prior** (ICP). Many instantiations of such ICPs incorporate the Chinese restaurant process (CRP) . The CRP can recruit more resources to account for more complicated data distributions and is exchangeable so that the order of data does not affect the internal construct. In cognitive science, the CRP has been used to model category learning , classical conditioning , among other cognitive functions .

However, we argue that achieving estimation consistency and order invariance is intractable given humans' limited cognitive capacity. First, building a flexible internal construct requires an "infinite capacity"  that is at odds with humans' finite memory. The flexibility of the internal construct is likely bounded under resource constraints. Second, achieving order invariance online imposes significant cognitive demands: it requires memorizing all previous experiences and reallocating them into potentially different clusters for every incoming new experience. Indeed, humans do not seem to learn in an exchangeable fashion , which can be captured by bounded rational inference . Resource constraints should thus impact the quality of the acquired internal construct.

To investigate how humans actually acquire and represent a structured internal density model under resource constraints, we first designed a structured density estimation task wherein human participants observed sequentially presented samples drawn from a hidden Gaussian mixture with 1-4 clusters. Participants were asked to report the full structure of their internal construct (learned distribution) at the end of the trial, providing us with a high-dimensional behavioral dataset. We found, among others, that participants reported their internal constructs close to the true distribution, but they consistently reported more clusters when there were in fact fewer or even a single cluster in the true distribution.

Motivated by the resource constraint argument, we propose a flexible ICP with a variable propensity towards expansion and includes the exchangeable CRP as a special case. In addition, we fit all parameters of the ICPs to the behavior dataset by a novel density estimation framework (DEF), which allows us to compare between ICPs and explore other inductive biases of participants in this task by likelihood-based mode optimization and critique. Consistent with our hypotheses, the proposed ICP best captures participants' behaviors when it does not preserve order-invariance and instead expands more economically when the internal construct is already complex. Further, fitted ICP explains the inconsistent number of clusters in the reported constrcts as having a strong prior preference for small cluster widths. In sum, our combined experimental and modeling contributions reveal previously unknown human inductive biases in building internal constructs of new environments, and open up doors for further theoretical explorations of flexible priors in learning rich structure representations.

## 2 Human behavioral experiments of structured density estimation

The human experiments described below were approved by the Institutional Review Board of School of Psychological and Cognitive Sciences at Peking University. All participants received monetary compensation above the local minimum wage.

### Experiment 1: learning visuo-spatial distributions

We designed a structured online density estimation task (Fig. 1A) to collect the internal constructs of humans formed from experiences. On each trial, participant saw a sequence of red dots with horizontal positions drawn i.i.d. from a Gaussian mixture with a varying number of clusters across trials (see Appendix A.1 for the cover story and the set of distributions used in each experiment). At the end of the sequence, they were asked to report a Gaussian mixture model that could have generated the red dots, specifying the weight, mean, and variance of each cluster through an interactive interface. As we are interested in participants' innate abilities to build internal constructs, we did not provide feedback.

We first validate the **quality of the reported distribution**. As participants (_N_=21) saw more samples (70 versus 10), the earth-mover distance between the reported and true densities decreased (Fig. 1B, _F_(1,20) = 83.21, \(p\) < 0.001), which is a desirable trend in density

Figure 1: Human behavior in a structured density estimation task. A, On each trial, human participants needed to report the latent Gaussian mixture distribution (i.e., weight, mean, and sd for each cluster) they learned from sequential samples. Panels B–F show the results of Experiment 1. B, The earth-mover distance between the reported and true distributions decreases when participants observe more samples. C, The correlation between the reported and true distributions in the first three moments. See Fig. 6E & F for detailed illustration. D, Mean number of reported clusters. Participants failed to learn the true probabilistic structure, typically reporting 2–3 clusters regardless of the true cluster number. E, Overlap ratio, which is calculated as the percentage of a cluster’s area being covered by any other clusters averaged across all clusters in the report. Adjacent clusters did not overlap much. F&G, Participants’ reported densities versus the true distributions on example trials of Experiment 1 (F) and Experiment 2 (G). Error bars denote 1 sem over participants. Each gray line in B–D is for one participant.

estimation. Figure 1C shows that the reported densities tracked the first three moments reasonably well (all slopes significantly greater than zero, \(t\)-tests \(p<0.001\)), although higher-order moments were harder to estimate.

A closer examination of the reported density on each trial revealed **systematic inconsistencies** in the learned structure. Fig. 1F shows that the reported density had more local maxima even when the true distribution only had a single Gaussian cluster. Participants tended to report around 3 clusters, irrespective of the number of clusters in the true distribution (Fig. 1D). Moreover, with increasing observations, the reported number of clusters increased (\(t\)(20)=11.15, \(p\)\(<\)0.001) but did not approach the ground truth. Such inconsistencies in the learned number of clusters are striking and counterintuitive, especially when the true generative distribution is as simple as a single Gaussian, in which case more evidence actually pushed participants further away from the truth. Another notable pattern is the small overlap between the adjacent clusters (Figure 1E), which contributes to bumpiness even when the true distribution is as smooth as a Gaussian. Though counterintuitive, this pattern echoes with the finding that humans use near-orthogonal (i.e., non-overlapping) basis functions to represent learned visuo-motor distributions .

### Experiments 2 and 3: distributions across domains and scales

To show that our findings in Experiment 1 are not specific to the selected distributions, we conducted Experiment 2 (\(N\)=20) to collect human internal constructs induced by a broader set of 14 representative distributions from previous studies [20; 21; 22; 23; 24; 25], with the true number of clusters ranging from 1 to 3. Results in Appendix A.2 show similar findings to Experiment 1: we found reasonably good density estimation quality, and participants preferred to report 2-3 non-overlapping clusters in their internal construct, even when the true distribution is a Gaussian distribution.

Do our findings still hold when we change the domain of the distributions? In Experiment 3 (\(N\)=36), participants observed samples of numerical values drawn from either Gaussian distributions or unimodal Gaussian mixtures (Fig. 2A). We divided the participants into two groups, one group observed numbers drawn from the original scale ('1X'), and the other observed samples drawn from 5 times the original scale ('5X'). After observing a sequence of numbers, participants, unaware of the true densities, reported the location and weight of each latent cluster. See Appendix A.3 for details.

Consistent with Experiments 1 and 2, we found that participants in both groups of Experiment 3 often reported more complex structures than the ground truth (Fig. 2B). More observations resulted in more clusters (\(F\)(1, 34) = 120.12, \(p<0.001\)). The average reported cluster number was higher (10 samples: 2.8\(\)0.7; 70 samples: 4.3\(\)1.3) than Experiment 1

Figure 2: Human learning of numeric distributions. A, Participants were randomly assigned to two groups that differed in the sd of the true distribution (scale 1x vs. 5x). B, The reported cluster number. C, The distance between the adjacent clusters. Error bar denotes 1 sem over participants.

(10 samples: 2.6\(\)0.4; 70 samples: 3.2\(\)0.5), possibly due to differences in visuo-spatial and numeric memory capacities. Even though the distribution scales differed by a factor as large as five, both groups used a similar number of clusters to represent the numeric distribution (\(F\)(1, 34)=0.05, \(p\)=0.831). The distance between adjacent clusters differed roughly fivefold between groups, mirroring the ratio between the true scales (Fig. 2C).

SummaryThrough comprehensive behavioral experiments, we found that humans were able to build internal constructs of environmental uncertainties based on online experiences, but they could not capture the number of clusters in the true distributions. Learned clusters also tended to be "orthogonal" to each other, creating unsmooth features not present in the true data distribution. It is possible that these imperfections are related to human's limited cognitive resources, as discussed in Section 1, or specific inductive biases in an implicit ICP, such as a small cluster width. Due to the unusual complexity of the density estimation task and the high-dimensional human report, the detailed mechanisms are not directly obvious. We thus resort to building a mathematical model of the reported internal constructs to understand the factors contributing to these inconsistencies.

## 3 The density estimation framework

Our task explicitly queries the internal probabilistic model constructed under sequential experiences. Mapping from these experiences to a distribution function is a challenging and ill-posed problem. Participants need a resource-rational ICP over the space of distributions. In addition, as experimenters, we need an appropriate noise model to define a valid likelihood for maximum-likelihood parameter fitting. To this end, we propose the density estimation framework (DEF) to model the experimental data; it consists of a _rational component_ and an _aleatoric component_ which we detail below. An overview is shown in Fig. 3.

### The rational component

The rational component performs approximate Bayesian inference given data under an ICP, examples of which have been used extensivly in cognitive modelling . A rational ICP for this task factorizes into a prior over the latent cluster assignment of the observed dots, and a prior over the probability density function associated with each cluster. For a sequence of \(T\) dot locations \(_{T}=[x_{1},,x_{T}]^{T}\), we denote the cluster assignments for \(_{t}\) as \(_{t}=[z_{1},,z_{t}]\), and each \(z_{t}_{+}\) is the cluster number assigned to \(x_{t}\).

The _economical ICP_ we propose here has its prior over \(_{T}\) defined recursively as

\[p_{}(z_{t+1}=k|_{t}):=_{t,k}}{ t+_{t}},&k K_{t}\\ }{t+_{t}},&k=K_{t}+1,_{t}:=_ {0}e^{-rK_{t}},_{t,k}:=t^{}}{_{t=1}^{K_{ t}}n_{t,k}^{}}\] (1)

for \(t\{1, T\}\), where \(z_{1}=1\), \(n_{t,k}:=_{=1}^{t}[z_{}=k]\) is the number of clusters assigned to cluster \(k\) at time \(t\), and \(K_{t}:=_{k}[_{t,k}>0]\) is the number of non-empty clusters (model size) at time \(t\). The expansion rate \(_{t}\) depends on the model size; for \(r<0\), new clusters are less likely to be added, which implements a form of conservative expansion. The distortion rate \( 0\) controls whether the different cluster sizes \(n_{t,k}\) are evened-out (\(<1\)) or exaggerated (\(>1\)), a form of divisive normalization. As a special case, this prior recovers the conventional CRP when \(r=0\) and \(=1\), which is exchangeable and has been a popular choice for modeling flexible online learning. In contrast, the prior (1) is in general not exchangeable; see Appendix B.1.1 for examples. Also, the expected number of clusters is upper bounded by \(_{t}_{0}e^{-rt}<}{1-e^{-r}}\) for \(r>0\), unlike the conventional CRP where this expectation grows as \((t)\). We provide justifications for the non-exchangeability of this prior at the end of this section.

The economical ICP assumes that each cluster is a Gaussian distribution with mean \(m\) and variance \(v\) as cluster properties. A convenient prior for Gaussians is the normal-inverse-\(^{2}\):

\[p_{}(m,v)=(m;_{0},v/_{0})^{2}(v ;a_{0},_{0}),\] (2)

where the hyperparameters with subscript 0 follow standard definitions . Given \(_{T}\), Bayesian inference under this ICP defined by (1) and (2) yields a posterior over Gaussian mixture parameters.

Following previous modeling work[15; 5], we assume humans use a single particle to approximately perform the Bayesian updating. Specifically, the observations \(_{t}\) up to time \(t\) and a corresponding single-particle sequence \(_{t}^{i}\) (an "approximate sample" from the posterior \(p_{}(_{t}|_{t})\)) define a structured density model: a mixture of \(K_{t}^{i}=_{=1}^{t}[z_{}^{i}>0]\) clusters, where each cluster is weighted in proportion to \(n_{t,k}^{i}=_{=1}^{t}[z_{}^{i}=k]\), and has posterior

Figure 3: Schematic of the density estimation framework (DEF) and likelihood evaluation. We have omitted the subscripts \(t\) and \(k\) for clarity. The DEF comprises a bounded rational component (green box) and an aleatoric component (gold box). The former infers the cluster properties \(^{i}\) based on noisy perceived observations \(}_{T}\) under an internal construct prior (ICP), and the latter provides a well-defined likelihood function while maintaining the dependencies between the cluster properties. Under the ICP defined in (1) and the particle-based sampling algorithm in (3) and (4), the current density model either updates an existing cluster or adds an additional cluster depending on the evidence of the incoming observation. The evolution of the model corresponds to a single path sampled in the tree structure shown in the left panel of the orange box. In the aleatoric component, structured noise is added to ensure a valid likelihood function for each simulation; see Appendix B. Each prediction yields a conditional likelihood of the reported \(^{r}\) in each trial, and averaging over a large number of simulations yields the marginal likelihood. These simulations can proceed in parallel (pink box), which can be accelerated by running on GPUs.

over density given by the usual conjugate prior update formulae:

\[p_{ R}(m_{k},v_{k}|{ x}_{t},{ z}_{t}^{i})={ N}(m_{k}; _{t,k}^{i},v_{k}/_{t,k}^{i}){ Inv}^{2}(v_{k};a_{t,k}^{i},_ {t,k}^{i}),\] (3) \[a_{t,k}^{i}=a_{0}+n_{t,k}^{i},_{t,k}^{i}=_{0} +n_{t,k}^{i},_{t,k}^{i}=(_{0}_{0}+n_{t,k}^{i}_{t,k}^ {i})/_{t,k}^{i},\] \[_{t,k}^{i}=^{i}}[a_{0}_{0}+n_{t, k}^{i}_{t,k}^{i}+n_{t,k}^{i}}{_{0}+n_{t,k}^{ i}}(_{0}-_{t,k}^{i})^{2}],\]

where \(_{t,k}^{i}\) and \(_{t,k}^{i}\) are, respectively, the empirical mean and the (unadjusted) empirical variance of the observations assigned to cluster \(k\) at time \(t\), according to the particle \({ z}_{t}^{i}\). Equation (3) shows that, given \({ z}_{t}^{i}\), the sufficient statistics that determine the posterior of the \(k\)'th cluster are \(_{t}^{i}:=[n_{t,k}^{i},_{t,k}^{i},_{t,k}^{i} ]_{k=1}^{K_{t}^{i}}\), in the sense that the conditioning variables are all functions of \(_{t}^{i}\). So, we can equivalently write \(p_{ R}(m_{k},v_{k}|{ x}_{t},{ z}_{t}^{i})=p_{ R}(m_{k},v_{k}|_{t}^{i})\) in (3). We also regard \(^{i}\) (not the random variables \(m\) and \(v\) in (3)) as the inferred cluster properties. When the next observation \(x_{t+1}\) arrives, it is assigned to a cluster according to the one-step posterior

\[p_{ R}(z_{t+1}^{i}|{ z}_{t}^{i},{ x}_{t},x_{t+1}) p_{ R}(z _{t+1}^{i}|{ z}_{t}^{i})p_{ R}(x_{t+1}|{ z}_{t},z_{t+1},{ x}_{t})= p_{ R}(z_{t+1}^{i}|{ z}_{t}^{i})p_{ R}(x_{t+1}|_{t}^{i},z_{t+1})\] (4)

for \(z_{t+1}^{i}[1,,K_{t}^{i}+1]\), where \(p_{ R}(x_{t+1}|_{t}^{i},z_{t+1}=k)\) is the marginal likelihood of \(x_{t+1}\) if assigned to cluster \(k\), given by evaluating at \(x_{t+1}\) the Student's \(t\)-density with degrees of freedom \(a_{i}\), location \(_{t,k}^{i}\) and scale \((_{t,k}^{i}(1+1/_{i,k}))^{1/2}\). If \(z_{t+1} K_{t}^{i}\), then the \(z_{t+1}\)'th component is updated by \(x_{t+1}\); if \(z_{t+1}=K_{t}^{i}+1\), then a new cluster is created, and the internal construct expands. Figure 3 (green box) depicts these two possibilities step at \(t=6\).

If the cluster prior (1) of the rational component implements the CRP, then by exchangeability, the true posterior over the model density is order-invariant with respect to observations. To obtain a particle from an order-invariant posterior, one should revise the cluster assignments of previous \({ x}_{t}\) at every time step, but this requires memorizing \({ x}_{t}\) which is cognitively implausible. We argue then that keeping the cluster prior (1) exchangeable is unnecessary--it forces the prior to take on a cognitively implausible property. Since (1) subsumes the CRP, we can test whether human behaviors collected in our experiments are better fit by an order-invariant prior.

### The aleatoric component and maximum-likelihood model fitting

Note that each inferred particle \(^{i}:=_{T}^{i}\) at \(t=T\) is a single simulation of the rational component a participant may possess in their mind. Our (the experimenter's) goal is to fit the rational component to the reported cluster properties \(^{r}:=[}^{r},^{r},^{r}]\) collecting the weights, means, and variances of the reported clusters; the reported number of clusters implied by \(^{r}\) is denoted by \(K^{r}\). The key challenges are worth emphasizing: a) the dimensionality of the report is variable, as it depends on the number of clusters; b) the cluster properties are unordered sets, rather than a real vector; c) the cluster assignments \({ z}_{T}\) are not observed to us as experimenters and need to be marginalized out to obtain the marginal likelihood, which is intractable. We address the first two challenges by introducing an _aleatoric component_, a structured noise model for DEF; and we deal with the last challenge by an efficient implementation of the DEF that utilizes the parallel processing power of graphical processing units (GPUs).

Figure 3 (right panel of orange box) shows illustrates the aleatoric component. It postulates that the participant, having inferred \(^{i}\), commits to the number of clusters first, during which they "slack" with a small probability, reporting \(^{i}\) clusters according to:

\[p_{ A}(^{i}|^{i})1,\ =K^{i}\;;\\ ,\ ^{i}\{1,,K_{ max}\} K^{i}\;.\] (5)

where \(>0\) is a slack parameter, and \(K_{ max}\) is the maximum number of reported clusters seen in an experiment across participants. If \(^{i} K^{i}\), then the participant removes or splits existing clusters recursively to obtain a set of slacked cluster properties\(f(^{i},^{i})\) until there are \(^{i}\) clusters; Appendix B.2 presents the details of how \(f\) modifies \(^{i}\). Finally, the likelihood of the reported \(^{r}\) given slacked cluster properties are defined as \(P(^{r}|}^{i}):=0\) if \(^{i} K^{r}\) (infinity penalty for inferring the number of cluster wrong), and

\[p_{}(^{r}|}^{i}):=_{K}|}_{_{K}}p_{w}(^{r};(}^{ i}))p_{}(^{r};(}^{i}))p_{}(^{r}; (}^{i}))^{i}=K^{r},\] (6)

where \(_{K}\) is the set of all permutations of \(K\) elements; \(p_{w}(;)\) is a Dirichlet distribution with concentration defined by \(\); \(p_{}(;)\) is an isotropic normal with mean \(\) and a variance parameter; and \(p_{}(;)\) is an isotropic log-normal with mean \(\) and a variance parameter. See Appendix B.2 for precise definitions of these distributions and parameters involved.

Overall, the aleatoric component the takes inferred cluster properties \(^{i}\) from the rational component and assigns non-zero probabilities to all possible reported distributions through the slack mechanism defined by (5) and the cluster modification \(f\). The permutation-invariant likelihood (6) regards the weight, mean, and variance vectors as independent conditional on \(}^{i}\), but retains the dependences between the clusters. To approximate the marginal likelihood, we marginalize out \(^{i}\) and \(^{i}\) by averaging over a large number of Monte Carlo (MC) simulations. In addition, we allow visual noise by feeding in noisy observations \(}\), where \(_{t} p_{}(_{t}|x_{t}):=(_ {t}|x_{t},_{v})\), which is also marginalized out during MC.

Altogether, the DEF estimates the likelihood of the reported \(^{r}\) given \(_{T}\) as \(p(^{r}|_{T})\)

\[_{i=1}^{M}p_{}(^{r}|}^{ i}\!=\!f(^{i},^{i})),^{i} p_{}( ^{i}|^{i}),\ \ ^{i} p_{}(^{i}| }_{T}^{i}),\ \ }_{T}^{i} p_{}(}|_{T}),\] (7)

where \(M\) is the number of MC simulations (see Appendix B.4 for important distinction to the number of particles). We implemented this MC estimator with a fully vectorized approach using PyTorch , which enables easy parallelization on GPUs (Fig. 3, pink box). Compared to a conventional for-loop implementation, ours yields reliable log-likelihood estimates with orders of magnitude acceleration. This allows us to optimize the DEF by maximum-likelihood using any off-the-shelve optimization method, such as Nelder-Mead . Further, it also allows us to critique different behavioral models (DEFs) by model comparison, using metrics such as Akaike Information Criterion (AIC) .

## 4 Experiments: fitting and comparing internal construct priors

We compare the following ICPs by fitting them in the DEFs that share the same class of aleatoric component: a) our proposed **economical** ICP as described in Section 3.1; b) its special case, the **exchangeable** CRP-GMM; and c) a baseline **batch** learning prior that describes a non-sequential cluster assignment; see Appendix B.1.3 for details. We use the Nealder-Mead optimizer to fit the DEF parameters for each participant, restarting multiple times to avoid early convergence issues . We choose a large number \(M\) for MC simulations so that the estimator (7) produces small enough variance and bias tolerable for the Nelder-Mead optimizer. After fitting, we compare different DEFs using a much larger \(M\). Details of the fitting algorithm are deferred to Appendix B.3 where we also present additional experiments validating our overall optimization scheme.

The quality of the fitted DEFs on the three experiments are shown in Fig. 4. The economical ICP produced significantly lower AICs than the exchangeable CRP-GMM in all three experiments (Exp. 1: median AICs -2627.9 vs. -2583.1, \(p<10^{-4}\); Exp. 2: -1766.7 vs. -1697.9, \(p<10^{-3}\); Exp. 3: -148.2 vs. -141.0, \(p<0.01\); Wilcoxon signed-rank test). We compare four aspects of the predictions in Fig. 4: the error rate in predicting the number of clusters, the negative log-likelihood (NLL) of the weights, and the expected normalized error in predicting mean and log-variance predictions using the slacked predictions given \(^{i}=K^{r}\). The advantage of the economical ICP is not only in correctly predicting the number of clusters but also in estimating the cluster properties. As expected, the batch ICP had much worse AICs. Further, to test whether simpler models can explain the data better, we performed an ablation study based on the CRP-GMM ICP (Appendix B.5.1). The ablated ICPs neglect various aspects of the probabilistic structures, effectively mimicking different heuristic strategies in forming the internal construct. None of the ablated models outperforms the economical ICP.

Example predictions of the two sequential ICPs are shown in Fig. 5A. The DEF with the economical ICP captures all of the behavioral patterns exhibited by participants, including the inconsistent number of clusters, the low overlap ratio between adjacent clusters, the moments of the reported distribution (Figs. 6 to 8), and the covariance of the reported cluster properties (Fig. 5B-D). We found largely consistent patterns in three fitted parameters across experiments (Fig. 5E-G). First, in line with our resource-rational motivation, the decay rates \(r\) in (1) are greater than 0, implying that model expansion is suppressed as \(_{t}\) decays when the internal construct becomes more complex. Second, participants' prior sd \(}\) in (2) was 20%-35% of the true density's sd regardless of the true scale of the observation, with a high confidence \(a_{0}\) (see Appendix B.5.2). This means that each cluster is kept narrow, requiring a new cluster to account for an observation far from existing clusters. It explains the bumpiness in the reported internal constructs, and also suggests that participants adaptively adjusted the cluster scale in accordance with the data scale and modality. Further, the observation that participants reported around 3 clusters is likely the trade-off between the opposite forces of a decaying \(_{t}\) and a small but confident \(_{0}\). Third, we found the cluster distortion \(\) in (1) to be less than 1, indicating a reduced sensitivity to cluster size when forming the internal construct online.

## 5 Discussion

We studied density estimation by humans, a sophisticated capability fundamental to a variety of cognitive tasks but has so far been elusive in experiments and implicitly assumed in modeling. Through behavioral experiments, we discovered systematic inconsistencies in humans' internal constructs of environments (Section 2) and proposed a density estimation framework for modeling this rich behavioral dataset (Section 3). We explained the inconsistencies as a result of an economically-expanding internal construct prior (ICP) over internal constructs and a strong preference for narrow clusters . The scale-adaptive cluster prior aligns with prior research on adaptive sensory and probability coding .

The density estimation framework (DEF) is a generic approach for modeling complex behavioral data. Thanks to the generality, one can compare other inference algorithms based on likelihoods and can easily incorporate priors over parameters (hyperpriors). While previous work kept the trainable parameters small and/or used error-based objective functions , our DEF can fit more parameters by maximum-likelihood, marginalizing the

Figure 4: Quality of three ICPs on three experiments. Thick blue lines and whiskers are means \(\) 1 sem over all participants (thin lines). Red crosses are medians. Asterisk (*) indicates significance at \(p\)\(<\)0.05 by Wilcoxon signed-rank test.

latent cluster assignments properly, thanks to our efficient implementation and hardware acceleration. With this methodology, we showed that the economical ICP provided a closer match to human behavior than the Chinese restaurant process (CRP) which has been used pervasively in modeling online learning. With a decaying rate of expansion, the growth of an economical ICP is more in line with humans' finite memory capacity.

Our work sends important messages to the theoretical community. Dasgupta and Griffiths  showed that the CRP is cognitively plausible because it can be derived from a preference for low entropy in the cluster assignment distribution. We found that the fitted \(<1\) actually increases the entropy of the prior cluster weight distribution , suggesting that the cluster count might be the main consumption of cognitive resources. Gershman et al.  discussed in great depth on batch versus online learning, whether the model capacity should be finite or infinite, and the hypothesis that retrospective cluster reassignment may be possible with particle representations. We provide key experimental and modeling evidence that humans may employ an online and yet finite (in expectation) model. However, with only a few number of particles, the beliefs of the other possible assignments are not well retained, so retrospective corrections may not be easily produced from these models.

In sum, the economical DEF reproduces many human patterns, not only in the loss function optimized for (quality of capturing human structure learning), but also in the moments of the whole distribution and the covariance structures of cluster properties. We note the large room for improvement in predicting the cluster variance (the right column in Fig. 4), which hints at the complexity of the task and the challenge in modeling. Our work also provides a comprehensive likelihood-based model comparisons on human density estimation, and paves way for future studies using high-dimensional reports in complex behavioral tasks.

Figure 5: A, Example DEF predictions for Experiment 2 when the predicted number of clusters agree with human reports. B–C, Detailed patterns of the reported cluster properties in Experiment 2, which can be successfully captured by economical DEF. In each panel, the grey heatmap denotes the distribution of responses of individual trials (collapsed across participants). The colored dots and error bars denote average response and standard error across participants in local data bins. The line denotes the regression line. B, the cluster sd increased with the cluster weight. C, The cluster sd decreased with the cluster eccentricity (distance between the cluster center and the global center). D, The cluster sd decreases with the number of reported clusters. E-G, Upper panels show the effects of key parameters in the economical ICP; lower panels show the parameters fit on the data of the three experiments. More visualizations of DEF predictions appear in Figs. 6 to 8 in Appendix A

#### Acknowledgments

This work was partly supported by the National Natural Science Foundation of China (32171095), National Science and Technology Innovation 2030 Major Program (2022ZD0204803), and funding from Peking-Tsinghua Center for Life Sciences to H.Z. L.K.W. was supported by Gatsby Charitable Foundation. We thank Peter Dayan and Jian Li for helpful discussions, and five anonymous reviewers for their valuable feedback.