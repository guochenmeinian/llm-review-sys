# MiniCache: KV Cache Compression in Depth Dimension for Large Language Models

Akide Liu1 Jing Liu1 Zizheng Pan1 Yefei He2

**Gholamreza Haffari1 Bohan Zhuang1,2**

Corresponding author. Email: bohan.zhuang@gmail.com

###### Abstract

A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our MiniCache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evaluation of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mistral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with cross-layer merging achieves a compression ratio of \(1.53\). Additionally, since MiniCache is orthogonal to existing quantization techniques, it can achieve a compression ratio of up to \(5.02\) when combined with the 4-bit quantization technique, enhancing inference throughput by approximately \(5\) and reducing the memory footprint by \(41\%\) compared to the FP16 full cache baseline, all while maintaining near-lossless performance. Project is available at https://minicache.vmv.re.

## 1 Introduction

Large Language Models (LLMs), exemplified by the GPT series  and the LLaMA series , have emerged as pivotal innovations within the artificial general intelligence, significantly enhancing the capabilities of natural language processing. However, these models are meticulously trained using extensive computational resources  and massive datasets , which enables them toproduce text that effectively mimics human writing styles and conducts complex reasoning analysis, yet raises the challenge of efficient deployment and serving. Within the inference framework of LLMs, KV caches  are crucial for storing pre-computed keys and values, thus avoiding repeated calculations over the preceding context and substantially enhancing LLMs' deployment efficiency. However, the increasing demand for longer sequence lengths results in voluminous cached states, leading to significant memory consumption during generation. For instance, a 175B GPT-3 model , with a batch size of 64 and a sequence length of 4,096 tokens (both prefilled and generated), requires approximately 1,208GB of GPU memory. This requirement is \(3.45\) greater than the memory used to store the model's weights. In this context, KV cache compression is of paramount importance due to its clear benefits: 1) it largely reduces the memory footprint allowing for faster generation and larger batch serving; 2) it significantly lowers the cost per token, demonstrating substantial commercial benefits.

Existing KV cache compression efforts can be roughly categorized into two types, namely quantization and sparsity. The quantization approaches [11; 12] propose storing the KV states in low-bit numerical values. Typically, FlexGen  demonstrates that 4-bit KV cache quantization can achieve lossless performance. In contrast, sparsity-driven methods aim to retain only the salient tokens while evicting the rest, either heuristically [14; 15] or adaptively . Some approaches  explore the intersection of these two types, by assigning high-bit to salient tokens and extremely low-bit to the rest of the tokens, achieving more aggressive memory gain. Despite these innovations, _existing literature merely consider the **intra-layer redundancy**, while neglecting another important complementary direction - the **inter-layer redundancy**_, as illustrated in the Figure 1(c).

Figure 1: Overview of our MiniCache strategy and example results: (a) shows the observation that the KV cache states between two adjacent layers are highly similar, particularly across the middle to deep layers. The x-axis uses index/2 to represent the similarities for each pair of layers. (b) compares the performance of MiniCache, and the mean baseline, which simply averages the KV caches of two layers, using the LLaMA-3-70B model  on the GSM8K dataset . MiniCache, which begins merging from the half-layer depth, achieves near-lossless performance. (c) highlights the primary difference between MiniCache and previous approaches. MiniCache investigates the inter-layer redundancy of KV caches along the depth dimension of LLMs, an aspect overlooked by intra-layer-based methods. Here, \(T\) refers to the last timestamp of pre-filling, and \(T+1\) des to the first timestamp of decoding.

Our analysis begins by exploring the redundancy of KV caches **along the depth dimension**, as shown in Figure 1(a). We observe that KV cache states exhibit high similarity between neighbouring layers in the middle-to-deep portions of LLMs. This intriguing property suggests that states paired by position between adjacent layers can be accurately merged into a single state space with a strong performance guarantee, as illustrated in Figure 1(b). This approach significantly reduces the memory footprint without the need to retain individual states for each attention layer. Note that these observations are pertinent to dynamic inference strategies such as mixture-of-depths  and layer-wise early exiting [18; 19], which optimize computational paths by skipping non-critical layers to enhance training and inference efficiency. Furthermore, layer pruning methods  highlight considerable redundancy in deeper layers. However, despite these advancements, the redundancy of KV caches along the depth dimension has largely been overlooked.

In this paper, we propose **MiniCache**, a simple yet effective cross-layer KV cache compression method aimed at advancing the inference efficiency of LLMs. MiniCache consists of two essential components. Firstly, we introduce an accurate cache merging strategy, employing a reparameterization of state vectors that decompose them into the magnitude and direction components, akin to weight normalization . This approach allows for effective interpolation of the directional component in polar coordinates while preserving the original state norms to retain as much information as possible. This interpolation refers to the cross-layer merging as shown in the Figure 1(c). Secondly, we recognize that a small subset of state pairs, characterized by low similarities but carrying largely distinct semantic meanings, are unsuitable for inter-layer merging. To address this, we propose a token retention strategy to minimize performance degradation, which involves separately retaining these outlier pairs. Our framework is notably memory-efficient, requiring storage for only a single high-dimensional directional component, along with minimal extra memory overhead. The overhead consists of a few unmergeable tokens and their corresponding indexes, as well as token-wise magnitudes to accurately restore the original states.

We conduct extensive experiments with representative LLMs, including Mixtral-8x7B , Phi-3-Mini , and LLaMA-3  8B and 70B, respectively. Our method is benchmarked across a diverse range of question answering and generation datasets [24; 25; 26; 27; 28; 29; 30; 31] using the lm-eval-harness . Additionally, we evaluate our results on LongBench  for long-sequence generation. The results demonstrate that MiniCache can reduce the memory footprint required for LLM inference by up to 41%, while simultaneously enhancing throughput by approximately \(5\) compared to fully cached baseline, clearly surpassing existing methods [11; 12; 14; 15].

Our contributions are summarized as follows:

* We introduce MiniCache, a simple yet highly effective framework for KV cache compression. MiniCache pioneers the exploration of KV cache compression along the depth dimension, thereby significantly expanding its capabilities.
* We observe a fascinating characteristic of cross-layer KV cache states: high similarity between adjacent layers in the middle to later stages of LLMs. Additionally, we find that not all state pairs are suitable for merging.
* We propose an accurate and memory-efficient method for cross-layer cache merging, comprising a reparameterization strategy and a token retention mechanism. Our method complements existing KV cache compression approaches, further enhancing LLM serving efficiency.
* Our MiniCache performs favourably against the state-of-the-art methods. Notably, our 4-bit MiniCache achieves a strong compression ratio of up to \(5.02\), \(5\) higher inference throughput, and 41% memory reduction compared to the FP16 full cache baseline with near-lossless performance.

## 2 Related Work

**Efficient inference for LLMs.** Large Language Models (LLMs) are constrained by considerable computational and memory requirements during inference, particularly in resource-constrained environments. To mitigate these challenges, a variety of efficient inference techniques have been developed. For instance, dynamic inference methods [18; 34; 35; 36; 37; 38], represented by mixture-of-experts (MoE) [39; 40; 41; 42; 43], adaptively select specific sub-structures of the model during the inference process based on the input data, significantly improving inference efficiency while keeping model capacity. Techniques like Multi-Query Attention [44; 45], Kernel-driven attentions[46; 47; 48; 49], and low-rank attentions [43; 50; 51; 52] approximate the functionality of traditional attention mechanisms but with more efficient implementations. Quantization strategies [53; 54; 55; 56] involve converting the model's weights and activations into a low bit-width format, thereby reducing memory footprint and computational intensity. Sparsification approaches [14; 15; 57; 58] eliminate unnecessary elements in both model weights and token representations, further enhancing efficiency. Some closely related works, such as MoD  and LayerSkips , considered the dynamic inference nature to ignore unimportant layers according to input . However, these methods require an additional fine-tuning process or carefully designed pre-training stages, which reduces the adaptivity of these methods. MiniCache relies on inter-layer similarity observations to perform cross-layer merging, significantly reducing memory demand.

**Model merging.** Merging compression involves the aggregation of a model's parameters and activations at various granularities. This process enhances the efficiency of inference in large models and facilitates huge redundancy . Linear Mode Connectivity (LMC)  enables the fine-tuning of models from a shared pre-trained base. Commonly, weight averaging  is employed as an efficient technique to perform merge compression. Notably, Model Soup  utilizes linear averaging in this context. Advanced methods like TIES Merging , Model Breadcrumbs , and DARE  further enhance this process by sparsifying and combining model parameters, enabling the merging of models without sacrificing performance capabilities. Additionally, Spherical Linear intERPolation (SLERP)  extends beyond simple weight averaging by interpolating between model parameters. The Fisher information matrix  and RegMean-based methods  further optimize merges to produce ideal weights, minimizing the \(_{2}\) distance to generation outputs while preserving the privacy of the training data. However, most existing works focus on merging model parameters, with the concept of depth-wise mergeability not being thoroughly explored in prior research. MiniCache focuses on the KV cache token merging in the depth dimensional of LLMs.

## 3 Motivation

In the below, we present our new observations in a novel cross-layer perspective.

### Cross-Layer Redundancy in KV Cache

Prior studies have revealed the ineffectiveness of middle-to-deep layers in LLMs . Thus, layer-wise early exiting in this case can effectively avoid the redundant computation with minor effect on LLM performance [19; 70]. Inspired by this, we explore a _layer-wise_ merging of KV cache in LLMs, starting with a simple baseline by averaging full tokens across adjacent layers. We provide our key observations as follows.

**Observation 1: KV cache shares a high similarity between adjacent layers.** Based on LLaMA-3-70B , we conduct zero-shot inference on the validation sets of three widely recognized benchmarks: COQA , GSM8K  and TruthfulQA . In general, we find that KV cache in the shallow layers exhibits low similarity, whereas those in the middle-to-deep layers are very similar to one

Figure 2: Overall of our explorations and observations : (a) shows the strong baseline by performing average merging on the KV cache. (b) shows the pairwise similarity of cache states between adjacent layers. (c) compares the MiniCache, simple average, and full cache baseline across five different datasets.

another based on angular distance, as shown in Figure 1(a). Next, we merge the KV cache across adjacent layers by conducting five-shot inference with LLaMA-2-7B, LLaMA-2-13B , LLaMA-3-8B , and Mixtral-8x7B  on GSM8K . Specifically, starting from the middle layer of each model, we merge the KV cache in the adjacent two layers. As shown in Figure 2(a), we observe a favourable performance for different LLMs, which reveals the huge potential for efficiency improvement by sharing the KV cache across adjacent layers during LLM decoding.

**Observation 2: Not all tokens are equally important to merge, a few distinct pairs require retention.** Recent works [15; 16] in KV cache compression have found that keeping a few salient tokens at each layer, which contribute the majority of attention scores, could be sufficient to maintain LLM performance. In our case, we speculate that certain token pairs in adjacent layers also exhibit outlier behaviours, showing strong semantic differences that make them unsuitable for merging. Based on COQA  and LLaMA-2-7B , we investigate the similarities at the level of token pairs. As shown in Figure 2(b), we find a significant portion of token pairs share high similarities across adjacent layers. However, we observe a few outlier pairs, such as indices 0 and 15, with large margins of difference. We consider these tokens non-mergeable due to their significant differences. We also show that merging these distinct tokens results in performance degradation, corresponding to \(=0\) row in Table 2. Thus, while cross-layer merging is a promising strategy for reducing memory burdens, it must be implemented with careful consideration of token-level similarities to ensure optimal performance, as shown in Figure 2(c).

## 4 Method

In this section, we introduce our MiniCache, a simple yet effective method aimed at trimming the KV cache redundancy in the depth dimension. This framework exploits the high similarity of KV cache states between adjacent layers and consists of two primary components: a reparameterization-based merging strategy and a token retention mechanism. The merging strategy compresses the KV cache states in adjacent layers to aggregate them into a single shared memory space, beginning from the middle of the model. The token retention mechanism mitigates information lost by retaining the highly distinct state pairs with minimal additional memory cost. With the merged cache, retention tokens, and magnitudes, we can accurately restore the original cache states for token decoding.

### Cross-Layer Compression

Our method commences with the identification of an optimal starting layer \(S\). Observations in Section 3.1 indicate that the KV cache from middle-to-deep layers consistently exhibits patterns of high similarity across adjacent layers. Consequently, we select the starting layer from the middle of the LLM, specifically \(S=L/2\). From this layer onward, the KV pairs are assumed to be sufficiently similar across adjacent layers to warrant their consolidation. Central to this approach is a merge function, \(F\), which is designed to integrate the KV caches of consecutive layers into a single, unified cache. We define \(\) as the vectorized cache state of a single token, where the superscript indicates the layer index and the subscripts \(k\) and \(v\) denote the keys and values, respectively. Specifically, for a pair of key/value tokens at the same position in layers \(l\) and \(l-1\), the merged cache is computed as

\[_{k}^{l,l-1} =F(_{k}^{l},_{k}^{l-1}),\] (1) \[_{v}^{l,l-1} =F(_{v}^{l},_{v}^{l-1}).\]

This consolidation process effectively eliminates the need to store and process the original memory-intensive keys and values in each layer independently. Instead, it approximates a shared cache across the adjacent layers.

### KV Cache Merging and Restoration

**Reparameterization-based cache merging.** To perform the pairwise merging, one solution is to directly average a pair of KV tokens, analogous to model merging [63; 64]. However, we observe that direct averaging can cause significant information loss. We conjecture that the distance between activations can be larger than that of weights due to the presence of outlier activation channels with extremely large magnitudes in LLMs [73; 74], while weights typically have relatively quite small magnitudes. A potential method to compensate for this information loss is to project from \(\) to and \(^{l}\), then rescale the projected vectors based on their relative magnitudes to exactly restore the original states. However, this approach requires extensive additional storage and computations; for example, restoring \(^{l-1}\) needs both \(\) and \(^{l}\), which undermines the benefits of cache merging. To efficiently merge token pairs, we draw inspiration from weight normalization , which disentangles model parameters into the _magnitude_ and _direction_ components to accelerate the convergence of stochastic gradient descent. Additionally, we take cues from DoRA , which employs a similar way to resemble the learning behavior of parameter-efficient fine-tuning compared to full fine-tuning. In our case, the reparameterization can be formulated as follows:

\[}^{l}=^{l,l-1}^{l}\|}{\|^{l,l-1}\|}, ~{}}^{l-1}=^{l,l-1}^{l-1}\|}{\|^{l,l -1}\|},\] (2)

where \(\) is the directional vector. This decomposition ensures that \(^{l,l-1}}{\|^{l,l-1}\|}\) is a unit vector, and allows the restored states to match the \(_{2}\) norm of the original states, thereby preserving the cache's information as much as possible. The restoration is shown as Figure 3(b). For brevity, we omit the subscripts \(k\) and \(v\), as keys and values are decomposed in the same way. For estimating the directional component \(^{l,l-1}\), we follow SLERP , which adaptively handles the interpolation, which often resembles rotation-like transformations. The choice of SLERP as the merging function is strategic, as it facilitates interpolation along the shortest path on the unit sphere between two high-dimensional vectors, thereby preserving their geometric integrity, which refers to merge operation in Figure 3(a). This is crucial for maintaining the semantic and syntactic properties of the KV caches. The formula for SLERP in our context is:

\[^{l,l-1}=)}{(^{l,l-1})} ^{l-1}}{\|^{l-1}\|}+)}{( ^{l,l-1})}^{l}}{\|^{l}\|},\] (3)

where \(^{l,l-1}=(^{l}^{l-1}}{\|^{l}\| \|^{l-1}\|})\) represents the angle between vectors \(^{l}\) and \(^{l-1}\), and \(()\) is the sine function. \(t\) is an interpolation hyperparameter that adjusts the relative influence of each vector on the resulting direction, tailored to the layer depth and specific characteristics of the KV pairs. Note that when we set \(t=0.5\), it will become an average merging along the geometry surface, which we consider special cases in Eq. (A). The merged cache for each token pair is then a concatenation of the directional vector, magnitude and \(^{l,l-1}\), denoting as \(^{l,l-1}=[^{l,l-1},\|^{l-1}\|,\|^{l}\|,^{l,l-1}]\), cached components as shown in Figure 3(a). Note that apart from storing the merged directional vector, we only need to store additional token-wise magnitude and angle scalars, which is memory efficient. In this way, we achieve substantial memory efficiencies through reduced redundancy while ensuring the retention of the critical functional characteristics of the original KV pairs across transformer layers.

**Unmergeable token retention.** Highly distinct pairs are sensitive to merging operations, leading us to propose unmergeable token retention, as shown in Figure 3(a). Despite the high similarity between

Figure 3: The illustration of the proposed method **MiniCache**. (a) depicts the cross-layer compression process. We fetch the KV caches, from layers \(l\) and \(l-1\), and merge them into shared states via Eq. (3). Additionally, we compute the \(_{2}\) norm for the caches to obtain their magnitudes. Furthermore, we select unmergable tokens for retention, then store merged cache, retention tokens, and magnitudes at layer \(l\) in \(\). (b) illustrates the restoration process for layers \(l\) and \(l-1\), which includes magnitude rescaling in Eq. (2) and retention token recovery.

KV cache states across neighbouring layers, a few sensitive distinct pairs remain that are significantly difficult to merge and share. Aligned with previous studies, these distinct tokens carry substantial semantic meanings [15; 16]. We observe that merging sensitive tokens, which results in the loss of layer-specific information, can lead to significant performance degradation. Therefore, it is crucial to properly disentangle the shared and unique information between adjacent layers. To address this issue, we designed a token retention strategy to selectively retain tokens that cannot be merged based on their angular distance, defined as: \(d(^{l},^{l-1})=\). For the KV caches, the minimum and maximum angular distances are determined to identify the unmergeable tokens.

The set of required token indices to keep, \(\), is obtained by:

\[=\{i d_{i}<d_{}+(d_{}-d_{}) \},\] (4)

where \(\) is a predefined hyperparameter that controls the retention threshold. The tokens with indices in \(\) are retained and not compressed during the merge, which ensures that performance does not decline by preventing the loss of unmergeable tokens.

Next, let \(^{n h}\) be either the key or value cache at one attention layer, where \(n\) denotes the number of tokens and \(h\) is the number of hidden dimensions, and \(^{n h}\) be the shared KV cache states. For each pair of neighbouring two layers, the unmergeable tokens are selected along with the token dimension by \(^{l}=^{l}[]\), \(^{l-1}=^{l-1}[]\), then restoring to our compressed caches by \(}^{l}[]=^{l}\), \(}^{l-1}[]=^{l-1}\), as shown in Figure 3(b). Overall, we share the final cache for the two layers as \(^{l,l-1}=[^{l,l-1},^{l},^{l-1},\|^{l-1}\|,\|^{l}\|,]\). This cache includes the shared KV cache states, retention of unmerged tokens, magnitude vectors for each layer, and the token-keeping index, respectively. These additional components are quite lightweight. Thus compared to full-layer caches, our method remains memory-efficient, as discussed in Sec. 4.3.

**Cache restoration.** After obtaining the shared cache \(^{l,l-1}\), we further need to approximately restore the original cache states for the current token decoding, as shown in Fig. 3(b). Specifically, to restore \(^{l}\), we first rescale the directional shared states with the corresponding magnitude vector along the token dimension, denoted as \(^{l,l-1}\|^{l}\|\). Subsequently, we perform retention token recovery by placing the sensitive tokens according to their token indices.

### Efficiency Discussion

**Compression efficiency.** We primarily analyze our memory efficiency in terms of the number of tokens used. Next, let \(r\) be the number of layers and and \(b\) is the batch size, \(s\) and \(n\) are input and output sequence length respectively. We consider the FP16 storage for KV cache. The full cache memory usage is given by \(4brh(s+n)\). In our study, we begin merging layers from the middle to the deeper layers, consolidating the KV cache states of every two layers into a single shared state space. As a result, we effectively reduce the GPU memory usage in the decoding inference to \(3brh(s+n)\), demonstrating a significant compression rate.

**Restoration efficiency.** We then analyze the additional memory cost incurred during the restoration process, which During the magnitude rescaling phase, we save an additional norm vector for the corresponding layers in the KV cache. It is important to note that the norm vector is in the shape of \(^{b s 1}\), which has a single channel dimension compared to the fully ranked original KV states. Additionally, we suppose that the retention threshold can be set to 0.05. Therefore, we have \(brh(0.05(s+n))\) tokens retained without compression. Finally, our overall memory requirement is given by \((3.1h+2)br(s+n)\). The detailed derivation is shown in the Appendix E.

## 5 Experiments

We demonstrated that our MiniCache can perform merging compression on the latter half of the layers of LLMs with minimal performance degradation.

**Implementation details.** Our experiments are based on representative model families of LLMs, including a compact LLM Phi-3-Mini  and an MoE LLM Mistral-8x7B . Additionally, we adopt LLaMA-3  8B and 70B models to explore how our method generalizes to larger LLMs. We sample ten tasks from lm-eval-harness , including COPA , MathQA , OpenBookQA , PIQA , RTE , WinoGrande , XSUM , and CNN/Daily Mail . We also evaluate long-sequence generation on LongBench . We compare our method with a fully cached baseline,and other methods such as round-to-nearest quantization (RTN) , SmoothQuant  and KIVI .

For the proposed MiniCache, we set the interpolation parameter \(t\) to 0.6, indicating that the merged results have a smaller rotation angle to the next layer. Furthermore, we set the token retention threshold \(\) to 0.05, according to the statistics of unmergeable tokens across multiple datasets. In addition to our merging method, we also consider a strong baseline of average merging. For sequential loading of large models, we utilize NVIDIA 4 A100 80GB GPUs, more details refers to Appendix D.

**Main results.** We evaluate MiniCache by merging KV caches across all layers on GSM8K, COQA, and TruthfulQA. The results are shown in Figure 4. In general, we demonstrate the general effectiveness of merging KV caches from middle-to-deep layers across different sized LLMs. Moreover, the proposed MiniCache demonstrates a consistent and significant advantage over the averaging baseline. We also illustrate the performance of merging KV caches across half of the layers with the blue lines, where MiniCache still maintains a robust performance and achieves the best compression ratio. Besides, we find that our method is even more effective for larger LLMs. For instance, based on LLaMA-3-70B, MiniCache shows nearly zero performance drop even with the KV cache in 87.5% of the layers merged on the COQA dataset. This highlights the adaptability and efficiency of our approach in handling large-scale models while ensuring minimal performance degradation.

**LongBench.** We also conduct experiments to evaluate performance and quality in long-sequence generation using the LongBench dataset , as shown in Table 1. Our experiments applied MiniCache over several models: LLaMA-2-7B-Chat, LLaMA-2-13B-Chat, Mistral-7B, and Mistral-7B-Instruct. It is important to note that our MiniCache method maintains orthogonality with all existing quantization and sparsity (refers to Table A) methods at both the model and token-wise levels. When combined with KIVI-4bit KV cache quantization, our approach achieves a compression rate of \(5.02\), with minimal impact on accuracy across various challenging long-context generation tasks. The combination of MiniCache and KIVI-4bit KV cache quantization demonstrates significant memory savings without compromising the model's ability to handle long sequences effectively. This high

Figure 4: Performance comparisons between our proposed MiniCache with the “averaging baseline” and the “unmerged full cache baseline” on multiple datasets with Phi3-Mini, Mistral-8x7B, LLaMA-3-8B, and LLaMA-3-70B. More result details are shown in Appendix F. The x-axis indicates the number of layers merged. As more layers are merged, a greater reduction in memory usage is achieved.

lights the potential of our method to optimize large language models for tasks requiring extensive context, making them more efficient and scalable for real-world applications.

**Efficiency analysis.** To assess the acceleration capabilities of MiniCache, we conduct evaluations based on the methodologies employed in vLLM  and KIVI . We generate synthetic workloads derived from ShareGPT, which include real input and output texts from LLM services. The dataset features an average input prompt length of 161 tokens and an output length of 338 tokens. Using the LLaMA-2-7B model on a single 80GB NVIDIA A100 GPU, we benchmark our method in a batch-serving scenario, comparing peak memory usage and throughput among 2-bit KIVI, 4-bit MiniCache, and an FP16 baseline. As illustrated in Figure 5, with a batch size of 128, **MiniCache reduces memory usage by 25GB, achieving a 41% memory saving**. In terms of throughput, **MiniCache outperforms the FP16 baseline by approximately 5\(\)**. Additionally, despite utilizing 4-bit quantization, MiniCache benefits from merging and sharing KV caches across adjacent layers, resulting in a 1.29\(\) higher throughput compared to the 2-bit KIVI. These results demonstrate that MiniCache offers a state-of-the-art trade-off between efficiency and performance.

## 6 Ablation Study

**The effect of interpretation parameter \(t\).** We explore the effects of the interpretation parameter \(t\) on performance, particularly in relation to the relative magnitude ratio of adjacent layers, as shown in Figure 6. We maintain all settings constant, starting from layer \(S=16\) (halfway

  
**Model** & **Method** & **LCC** & **RppBench-P** & **PR-en** & **TREC** & **2nuklimqa** & **GovReppert** & **MQ-ach** & **Average** & **Compression** \\  & & & & & & & & & & **Ratio** \\   & Baseline & 58.16 & 52.19 & 10.12 & 64.00 & 31.12 & 27.09 & 10.12 & 36.41 & 1x \\  & RTN  & 15.44 & 8.76 & 0.79 & 4.00 & 0.30 & 1.93 & 0.07 & 4.90 & 3.21x \\  & SmoothedQuant  & 35.31 & 32.18 & 0.79 & 28.75 & 7.45 & 11.83 & 1.68 & 16.28 & 2.15x \\  & KIV-11  & 49.32 & 43.71 & 4.50 & 63.00 & 24.07 & 24.73 & 10.24 & 31.51 & 3.95x \\  & **MiniCache** & 58.03 & 52.01 & 9.00 & 64.00 & 30.58 & 25.32 & 10.13 & **35.44** & **5.02x** \\   & Baseline & 48.06 & 50.08 & 14.25 & 68.50 & 13.09 & 27.76 & 7.23 & 32.71 & 1x \\  & RTN  & 20.98 & 16.82 & 0.33 & 0.00 & 5.22 & 1.68 & 0.16 & 6.03 & 3.21x \\  & SmoothedQuant  & 32.17 & 33.86 & 2.65 & 48.00 & 3.53 & 12.47 & 0.47 & 19.16 & 2.15x \\  & KIV-11  & 48.60 & 48.81 & 13.50 & 68.00 & 14.32 & 25.70 & 7.01 & 32.42 & 3.95x \\  & **MiniCache** & 48.75 & 48.59 & 13.00 & 68.00 & 14.36 & 26.57 & 7.99 & **32.61** & **5.02x** \\   & Baseline & 68.06 & 60.46 & 17.71 & 68.00 & 10.87 & 20.09 & 17.10 & 37.33 & 1x \\  & RTN  & 27.98 & 26.18 & 3.34 & 10.30 & 1.11 & 2.49 & 0.45 & 10.51 & 3.21x \\  & SmoothedQuant  & 40.63 & 35.14 & 3.40 & 30.50 & 6.03 & 5.00 & 4.12 & 17.55 & 2.15x \\  & KIV-11  & 56.16 & 58.33 & 12.43 & 65.00 & 11.03 & 13.22 & 13.87 & 33.43 & 3.95x \\  & **MiniCache** & 68.89 & 60.98 & 13.92 & 67.00 & 10.50 & 18.06 & 7.88 & **35.75** & **5.02x** \\   & Baseline & 55.51 & 48.96 & 60.00 & 70.10 & 27.33 & 38.25 & 42.74 & 48.32 & 1x \\  & RTN  & 32.36 & 33.23 & 6.67 & 1.00 & 2.25 & 10.03 & 2.30 & 11.155 & 3.21x \\   & SmoothedQuant  & 43.84 & 38.63 & 4.79 & 39.50 & 10.34 & 23.61 & 8.33 & 24.43 & 2.15x \\   & KIV-11  & 53.13 & 48.60 & 47.69 & 69.00 & 20.68 & 29.37 & 33.88 & 43.74 & 3.95x \\   & **MiniCache** & 54.79 & 51.02 & 64.14 & 71.00 & 24.97 & 31.46 & 27.54 & **46.99** & **5.02x** \\   

Table 1: Evaluation of different KV cache compression methods on LongBench. MiniCache builds on top of 4-bit KIVI  and achieves the best performance with the strongest compression rate.

through the layers of LLaMA-3-8B), and vary the interpretation parameter \(t\) from 0.3 to 0.7. Our findings reveal several key points. When \(t=0.5\), the process resembles average merging, which is less effective for cross-layer merging. In contrast, when \(t=0.6\) is optimal, the merged representation exhibits the most robust performance, while indicating that more information is derived from the second term (\(^{l}\)) of the SLERP.

The frequency results also indicate that the high frequencies are clustered around 0.4 and 0.6, corroborating our optimal \(t\). Moreover, there is a strong correlation between the optimal \(t\) and the high frequency of the relative magnitude ratio of adjacent layers. This finding provides an opportunity to utilize the relative magnitude ratio to dynamically determine the interpretation parameter \(t\). Dynamic \(t\) allows for more flexible weight control in SLERP merging for each layer-wise operation, thereby showing potential for further exploration.

**The effect of token retention threshold \(\).** We investigate the impact of the token retention threshold \(\) on model performance across the three datasets, as shown in Table 2. A larger \(t\) generally means retaining more tokens for improved performance, but this comes at the cost of increased memory demand. The results suggest that setting \(\) to 0.05 achieves the best balance between performance and efficiency.

## 7 Conclusion and Future Work

This paper presents a pioneering exploration of KV cache compression in the depth dimension, addressing a significant memory bottleneck in LLMs. Our proposed MiniCache offers a simple, effective, and training-free approach to compressing KV caches by leveraging the notable high similarities between KV caches in neighboring layers, starting from the midpoint of LLMs. We have demonstrated that MiniCache can significantly reduce the memory footprint required for LLM inference by up to 41%, while simultaneously enhancing throughput by approximately five times compared to the FP16 baseline. In conclusion, MiniCache significantly advances the field of KV cache compression, offering a state-of-the-art balance between efficiency and performance. Future work will focus on enhancing the compression ratio by cross-multiple-layer merging, developing advanced merging algorithms such as Spherical Cubic Interpolation , and further optimizing memory usage for large-scale deployments in diverse application scenarios.

## 8 Acknowledgement

This research is partially supported by the ARC Future Fellowship (FT190100039) to G.H. Additional support was partially provided by the DARPA Assured Neuro-Symbolic Learning and Reasoning (ANSR) program under award number FA8750-23-2-1016.