# Explicit Flow Matching:

On The Theory of Flow Matching Algorithms with Applications

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper proposes a novel method, Explicit Flow Matching (ExFM), for training and analyzing flow-based generative models. ExFM leverages a theoretically grounded loss function, ExFM loss (a tractable form of Flow Matching (FM) loss), to demonstrably reduce variance during training, leading to faster convergence and more stable learning. Based on theoretical analysis of these formulas, we derived exact expressions for the vector field (and score in stochastic cases) for model examples (in particular, for separating multiple exponents), and in some simple cases, exact solutions for trajectories. In addition, we also investigated simple cases of diffusion generative models by adding a stochastic term and obtained an explicit form of the expression for score. While the paper emphasizes the theoretical underpinnings of ExFM, it also showcases its effectiveness through numerical experiments on various datasets, including high-dimensional ones. Compared to traditional FM methods, ExFM achieves superior performance in terms of both learning speed and final outcomes.

## 1 Introduction

In recent years, there has been a remarkable surge in Deep Learning, wherein the advancements have transitioned from purely neural networks to tackling differential equations. Notably, Diffusion Models  have emerged as key players in this field. This models transform a simple initial distribution, usually a standard Gaussian distribution, into a target distribution via a solution of Stochastic Differentiable Equation (SDE)  or Ordinary Differentiable Equation (ODE) with right-hand side representing a trained neural network. The Conditional Flow Matching (CFM)  technique, which we focus on in our research, is a promising approach for constructing probability distributions using conditional probability paths, which is notably a robust and stable alternative for training Diffusion Models. The development of the CFM-based approach includes various techniques and heuristics [4; 7; 13] aimed at improving convergence or quality of learning or inference. For example, in the works [19; 20; 10] it was proposed to straighten the trajectories between points by different methods, which led to serious modifications of the learning process. We refer the reader for, example, to the paper  where different FM-based approaches are summarised, and to the paper  for the connection between Diffusion Models and CFM.

In our work, we introduced an approach which we called Explicit Flow Matching (ExFM), to consider the Flow Matching framework theoretically by modifying the loss and writing the explicit value of the vector field. Strictly speaking, the presented loss is a tractable form of the FM loss, see Eq. (5) of . Base on this methods we can improve the convergence of the method in practical examples reducing the variance of the loss, but the main focus of our paper is on theoretical derivations.

Our method allows us to write an expression for the vector field in closed form for quite simple cases (Gaussian distributions), however, we note that Diffusion Models framework in the case of a Gaussian Mixture of two Gaussian as a target distribution is still under investigation, see recent publications [15; 8].

Our main contributions are:

1. A tractable form of the FM loss is presented, which reaches a minimum on the same function as the loss used in Conditional Flow Matching, but has a smaller variance;
2. The explicit expression in integral form for the vector field delivering the minimum to this loss (therefore for Flow Matching loss) is presented.
3. As a consequence, we derive expressions for the flow matching vector field and score in several particular cases (when linear conditional mapping is used, normal distribution, etc.);
4. Analytical analysis of SGD convergence showed that our formula have better training variance on several cases;
5. Numerical experiments show that we can achieve better learning results in fewer steps.

### Preliminaries

Flow matching is well known method for finding a flow to connect samples from two distribution with densities \(_{0}\) and \(_{1}\). It is done by solving continuity equation with respect to the time dependent vector field \((x,t)\) and time-dependent density \((x,t)\) with boundary conditions:

\[\{ & =-((x,t)(x,t)),\\ (x,0)&=_{0}(x),(x,1)=_{1}(x). .\] (1)

Function \((x,t)\) is called _probability density path_. Typically, the distribution \(_{0}\) is known and it is chosen for convenience reasons, for example, as standard normal distribution \((x)=(x 0,I)\). The distribution \(_{1}\) is unknown and we only know the set of samples from it, so the problem is to approximate the vector field \(v(x,t)(x,t)\) using these samples. To make problem (1) well defined, one usually imposes additional regularity conditions on the densities, such as smoothness. The rigorous justification of the obtained results we put in the Appendix, leaving the general formulations of theorems and ideas in the main text.

From a given vector field, we can construct a _flow_\(_{t}\), _i. e._, a time-dependent map, satisfying the ODE \((x)}{ t}=v(_{t}(x),t)\) with initial condition \(_{0}(x)=x\). Thus, one can sample a point \(x_{0}\) from the distribution \(_{0}\) and then using this ODE obtain a point \(x_{1}=_{1}(x_{0})\) which have a distribution approximately equal to \(_{1}\). For given boundary \(_{0}\) and \(_{1}\), the vector field or path solutions are not the only solutions, but if we have found any solution, it will already allow us to sample from the unknown density \(rho_{1}\). However, if the problem is more narrowly defined, _e. g._, one needs to have a map that is close to the Optimal Transport (OT) map, we have to impose additional constraints.

The problem of finding any vector field \(v\) is solved in conditional manner in the paper , where so-called Conditional Flow Matching (CFM) is present. Namely, the following loss function was introduced for the training a model \(v_{}\) which depends on parameters \(\)

\[L_{}()=_{t}_{x_{1},x_{0}}v_{ }(_{t,x_{1}}(x_{0}),\,t)-_{t,x_{1}}^{}(x_{0})^{2},\] (2)

where \(_{t,x_{i}}(x_{0})\) is some flow, conditioned on \(x_{1}\) (one can take \(_{t,x_{i}}(x_{0})=(1-t)x_{0}+tx_{1}+_{s}tx_{0}\) in the simplest case, where \(_{s}>0\) is a small parameter need for this map to be invertable at any \(0 t 1\)). Hereinafter the dash indicates the time derivative. Time variable \(t\) is uniformly distributed: \(t\) and random variables \(x_{0}\) and \(x_{1}\) are distributed according to the initial and final distributions, respectively: \(x_{0}_{0}\), \(x_{1}_{1}\). Below we omit specifying of the symbol \(\) the distribution by which the expectation is taken where it does not lead to ambiguity.

### Why new method?

Model training using loss (2) have the following disadvantage: during training, due to the randomness of \(x_{0}\) and \(x_{1}\), significantly different values can be presented for model as output value at close model 

[MISSING_PAGE_EMPTY:3]

To obtain the modified loss, we return to end of the standard CFM loss representation in (3). It is written as the expectation over two random variables \(x_{1}\) and \(x_{t}\) having a common distribution density

\[\{x_{1},x_{t}\}_{j}(x_{1},x_{t},t)=_{x_{1}}(x_{t},t)_{1}(x_{1}),\] (4)

which, generally speaking, is not factorizable. Let us rewrite this expectations in terms of two independent random variables, each of which have its marginal distribution. The marginal distribution \(_{m}\) of \(x_{t}\) can be obtained via integration:

\[_{m}(x_{t},t)=_{j}(x_{1},x_{t},t)\,x_{1}=_{x_{1}} (x_{t},t)_{1}(x_{1})\,x_{1}\,,\] (5)

while the marginal distribution of \(x_{1}\) is just (unknown) function \(_{1}\). Let for convenience \(w(t,x_{1},x)=^{}_{t,x_{1}}^{-1}_{t,x_{1}}(x)\)1. We have

\[L_{}()=_{t,x_{1},x_{t}_{x_{1}} (,t)}\|v_{}(x_{t},\,t)-w(t,x_{1},x_{t})\|^{2}=\\ _{0}^{1}\|v_{}(x_{t},\,t)-w(t,x_{1},x_{t})\|^{2} _{x_{1}}(x,t)_{1}(x_{1})\,x_{t}\,x_{1}t= \\ _{0}^{1}\|v_{}(x_{t},\,t)-w(t,x_{1},x_{t})\|^{2} ._{x_{1}}(x_{t},t)_{m}(x_{t},t)\,_{m}(x_ {t},t)_{1}(x_{1})\,x_{t}\,x_{1}t=\\ _{t,x_{1},x_{m}(,t)}\|v_{}(x,\,t)-w( t,x_{1},x)\|^{2}\,_{c}(x|x_{1},t)/_{1}(x_{1}),\] (6)

where we introduce a conditional distribution

\[_{c}(x|x_{1},t):=_{x_{1}}(x,t)_{1}(x_{1})/_{m}(x,t):=_{x_{ 1}}(x,t)_{1}(x_{1})_{x_{1}}(x,t)_{1}(x_{1})\,x_{1}.\] (7)

The key feature of the representation (6) is that the integration variables \(x_{1}\) and \(x\) are independent. Thus, we can evaluate them using Monte Carlo-like schemes in different ways. However, we go further and make a modification to this loss to reduce the variance of Monte Carlo methods.

### New loss and exact expression for vector field

Note that so far the expression for \(L_{}\) have not changed, it has just been rewritten in different forms. Now we change this expression so that its numerical value, generally speaking, may be different, but the derivative of the model parameters will be the same. We introduce the following loss

\[L_{}()=_{t}_{x_{m} }v_{}(x,\,t)-_{x_{1}_{1}}w(t,x_{1},x)_{c}( x|x_{1},t)/_{1}(x_{1})^{2}=\\ _{0}^{1}v_{}(x,\,t)- w(t,x_{1},x) _{c}(x|x_{1},t)\,x_{1}^{2}_{m}(x,t)\,x \,t.\] (8)

**Theorem 2.1**.: _Losses \(L_{}\) in Eq. (2) and \(L_{}\) in Eq. (8) have the same derivative with respect to model parameters:_

\[L_{}()/\,=\,L_{ }()/\.\] (9)

Proof is in the Appendix A.1.

In the presented loss \(L_{}\), the integration (outside the norm operator) proceeds on those variables on which the model depends, while inside this operator there are no other free variables. Thus, using this kind of loss, it is possible to find an exact analytical expression for the vector field for which the minimum of this loss is zero (unlike the loss \(L_{}\)). Namely, we have

\[v(x,t)= w(t,x_{1},x)_{c}(x|x_{1},t)\,x_{1}\,.\] (10)

We can obtain the exact form of this vector field given the particular map \(_{t,x_{1}}\). For example, the following statement holds:

**Corollary 2.2**.: _Consider the linear conditioned flow \(_{t,x_{1}}(x_{0})=(1-t)x_{0}+tx_{1}\) which is inevitable as \(0 t<1\). Then \(w(t,x_{1},x)=-x}{1-t}\), \(_{x_{1}}(x,t)=_{0}(t}{1-t})}\) and the loss \(L_{}\) in Eq. (8) reaches zero value when the model of the vector field have the following analytical form_

\[v(x,t)=(x_{1}-x)_{0}(t}{1-t})_{1}(x_{1}) \,x_{1}/((1-t)_{0}(t}{1-t}) _{1}(x_{1})\,x_{1})..\] (11)

_This is the exact value of the vector field whose flow translates the given distribution \(_{0}\) to \(_{1}\)._

Complete proofs are in the Appendix A.3.1. Note that the result (11) is not totally new, for example, a similar result (though in the form of a general expression rather than an explicit formula), was given in , Eq. (9). However, our contribution consists of both the general form (10) and practical and theoretical conclusions from it (see below).

_Remark 2.3_.: In the case of the initial and final times \(t=0,\,1\), Eq. (11) is noticeably simpler

\[v(x,0)=_{x_{1}}x_{1}-x= x_{1}_{1}(x_{1})\,x_{1}-x. v(x,1)=x- x_{0}_{0}(x_{0})\,x_{0}\,.\] (12)

This expression for the initial velocity means that each point first tends to the center of mass of the unknown distribution \(_{1}\) regardless of its initial position.

Extensions to SDENow let the conditional map be stochastic: \(_{t,x_{1}}=(1-t)x_{0}+tx_{1}+_{e}(t)\), where \((0,1)\). Typically, \(_{e}(0)=_{e}(1)=0\), for example, \(_{e}(t)=t(1-t)_{e}\).

Note that this formulation covers (with appropriate selection of the \(_{e}(t)\) parameter) the case of diffusion models .

Then, we can write the exact solution for a so-called _score and flow matching_ objective (see  for details)

\[_{[]^{2}}()= {\|v_{}(x,t)-u_{t}^{}(x)\|^{2}}_{}+(t)^{2} (x,t)- p_{t}(x)\|^{2}}_{}.\]

that corresponds to this map. In the last expression, the following explicit conditional expressions are considered in the cited paper for the case \(_{e}(t)=_{e}\)

\[u_{t}^{}(x)=(x-(tx_{1}+(1-t)x_{0}))+(x_{1}-x_{0}),\; \; p_{t}(x)=+(1-t)x_{0}-x}{_{e}^{2}t(1-t)}.\]

The exact solution (our result, explicit analog of the Eq. (10) from ) under consideration has the form (44) and (46) and, for example for the for the Gaussian \(_{0}\) this expressions reduced to the Eq. (49) and (50), correspondingly. See Appendix E for the details on this case.

Simple examplesConsider the case of Standard Normal Distribution as \(_{0}\) and Gaussian Mixture of two Gaussians as \(_{1}\). Vector field have a closed form (37) in this case, and we can fast numerically solve ODE for trajectories. Random generated trajectories and plot of the vector field are shown on Fig. 2 (a)-(b). Detailed explanation of this case is in the Sec. D.2. Another example is related to the case of a stochastic map in the form of Brownian Bridge, which briefly described in the last paragraph and considered in Sec. E.3.2 in details, see Fig. 2 (c)-(f). Note that at some \(_{e}\) values the trajectories are a little bit straightened in this case compared to the usual linear map, if we compare cases on the Fig. 6.

### Training scheme based on the modified loss

Let us consider the difference between our new scheme based on loss \(L_{}\) and the classical CFM learning scheme. As a basis for the implementation of the learning scheme, we take the open-source code2 from the works .

Consider a general framework of numerical schemes in classical CFM. We first sample \(m\) random time variables \(t\). Then we sample several values of \(x\). To do this, we sample a certain number \(n\) samples \(\{x_{0}^{i}\}_{i=1}^{n}\) from the "noisy" distribution \(_{0}\), and the same number \(n\) of samples \(\{x_{1}^{i}\}_{i=1}^{n}\) fromthe unknown distribution \(_{1}\). Then we pair them (according to some scheme), and get \(n\) samples as \(x^{j,i}=_{t^{j},x_{1}^{i}}(x_{0}^{i})\) (_e. g._ a linear combination in the simple case of linear map: \(x^{j,i}=(1-t^{j})x_{0}^{i}+t^{j}x_{1}^{i}\)), \( i=1,2,,n\); \( j=1,2,,m\). Note, than one of the variable \(n\) or \(m\) (or both) can be equal to \(1\).

At the step 2, the following discrete loss is constructed from the obtained samples

\[L_{}^{d}()=_{j=1}^{m}_{i=1}^{n}(x^{j,i },\,t^{j})-^{}_{t^{j},x_{1}^{i}}(x_{0}^{i})}^{2}.\] (13)

Finally, we do a standard gradient descent step to update model parameters \(\) using this loss.

The first and last step in our algorithm is the same as in the standard algorithm, but the second step is significantly different. Namely, we additionally generate a sufficiently large number \(N n m\) of samples \(_{1}\) from the unknown distribution \(_{1}\), sampling \((N-n)\) new samples and adding to it the samples \(\{x_{1}^{i}\}_{1}^{n}\) that are already obtained on the previous step.

Then we form the following discrete loss which replaces the integral on \(x_{1}\) in \(L_{}\) by its evaluation \(v^{d}\) by self-normalized importance sampling or rejection sampling (see Appendix B for details)

\[L_{}^{d}()=_{j=1}^{m}_{i=1}^{n}(x^{j, i},\,t^{j})-v^{d}(x^{j,i},\,t^{j})}^{2}.\] (14)

For example, if we use self-normalized importance sampling and assume that the Jacobian \(_{t,x_{1}}^{-1}(x) x\) do not depend on \(x_{1}\), we can write

\[v^{d}(x,\,t)=(_{k=1}^{N}w(t,_{1}^{k},x)_{0} _{t,_{1}^{k}}^{-1}(x))\!\!_{k=1}^{ N}_{0}_{t,_{1}^{k}}^{-1}(x)\.\] (15)

**Theorem 2.4**.: _Under mild conditions, the error variance of the integral gradient (9) using the Monte Carlo method (14) is lower than using formula (13) with the same number \(n m\) of samples for \(\{x\}\)._

Sketch of the proof is in the Appendix A.2. The steps of our scheme are formally summarized in Algorithm 1.

Particular case of linear map and Gaussian noiseLet \(_{t,x_{1}}\) be the linear flow: \(_{t,x_{1}}(x_{0})=(1-t)x_{0}+tx_{1}\). and consider the case of standard normal distribution for the initial density \(_{0}\): \(_{0}(x)(x 0,I)\). Then in the case of using self-normalized importance sampling, we have

\[v^{d}(x,\,t)=_{k=1}^{N}_{1}^{k}-x}{1-t}(Y^{1},\,,\,Y^{N})_{k}, Y^{k}=- _{1}^{k}}^{2}}{1-t}.\] (16)

Here, the lower index \(k\) in \(\) stands for the \(k\)-th component, and the \(\) operation itself came about due to exponents in the Gaussian density as a more stable substitute for computing than directly through exponents.

Extension of other maps and initial densities \(_{0}\)Common expression (10) can be reduced to closed form for the particular choices of density \(_{0}\) and map \(\) (consequently, expression for \(w\)). We summarise several known approaches for which FM-based techniques can be applied in Table 13. See Appendix C and D for derivations of formulas and for more extensions.

Figure 2: Trajectories and vector field obtained in simple cases: (a) \(N=80\) random trajectories from \((|0,1^{2})\) to GM; (b) 2D plot of the vector field in this case (c)–(f) \(N=40\) random trajectories from \((|0,1^{2})\) to \((|2,3^{2})\) and 2D plot of the vector fieldfor different \(_{e}\) for the Brownian Bridge map

ComplexityWe assume that the main running time of the algorithm is spent on training the model, especially if it is quite complex. Thus, the running time of one training step depends crucially on the number \(n m\) of samples \(\{x\}\) and it is approximately the same for both algorithms: the addition of points \(_{1}\) entails only an additional calculation using formula (16), which can be done quickly and, moreover, can be simple parallelized.

### Irreducible dispersion of gradient for CFM optimization

Ensuring the stability of optimization is vital. Let \(\) be changes in parameters, obtained by SGD with step size \(/2\) applied to the functional from Eq. (13):

\[ v(x^{j,i},t^{j})=-v(x^{j,i},t^{j})-v^{d}(x^{j,i},\,t^{ j}).\] (17)

For simplification, we consider a function, \(v_{}(x,t)\), capable of perfectly fitting the CFM problem and providing an optimal solution for any point \(x\) and time \(t\). For a linear conditional flow at a specific point \(x^{j,i}_{x^{i}_{1}}(,t^{j})\) at time \(t^{j} U(0,1)\), the update \( v(x^{j,i},t^{j})\) can be represented as follows:

\[ v(x^{j,i},t^{j})=(x^{i}_{1}-^{i}_{0}-v(x^{j,i},t^{j}) ),\] (18)

where \(^{i}_{0}=-t^{j}x^{i}_{1}}{1-t^{j}}\). We define the dispersion \(_{x,x_{1}}f(x,x_{1})\) for \(x_{x_{1}}(,t)\) and \(x_{1}_{1}\) as:

\[_{x,x_{1}}f(x,x_{1})=_{x,x_{1}}f^{2}(x,x_{1})-( _{x,x_{1}}f(x,x_{1}))^{2}.\] (19)

**Proposition 2.5**.: _At the time \(t=0\), the dispersion of update in the form (18) have the following element-wise lower bound:_

\[_{x^{j,i},x^{i}_{1}} v(x^{j,i},0)=^{2}_{x^{i} _{1}}x^{i}_{1}+^{2}_{x^{j,i},x^{i}_{1}}(x^{j,i}+v(x^{j,i},0)) ^{2}_{x^{i}_{1}}x^{i}_{1}.\]

_Equality is reached when the model \(v(x^{j,i},0)\) has exact values equal to (12)._

Given that the dispersion cannot be reduced with an increase in batch size, the only available option is to decrease the step size of the optimization method, _i. e._, reduce the learning rate slowing down the convergence. The situation is much better for the proposed loss in (14). We can express the update \( v(x^{j,i},t^{j})\) in the case of ExFM objective as:

\[ v(x^{j},t^{j})=^{2}_{k=1}^{N}x^{k}_{1} (x^{j,i}|x^{k}_{1},t^{j})-x^{j,i}-v(x^{j,i},t^{j}),\] (20)

where \(x^{j,i}_{x^{i}_{1}}(,t^{j})\), \(x^{k}_{1}_{1}\) and \((x^{j,i}|x^{k}_{1},t^{j})=_{0}(- t^{j}x^{k}_{1}}{1-t^{j}})/_{k=1}^{N}_{0}(-t^{j}x^{k}_{ 1}}{1-t^{j}})\). Similar to the derivations in the previous part, we can found simplified form for the dispersion of update at \(t=0\).

**Proposition 2.6**.: _At the time \(t=0\), the dispersion of update from (20) have the following element-wise lower bound:_

\[_{x^{j,i},x^{k}_{1}} v(x^{j,i},0)=}{N} _{x^{k}_{1}}x^{k}_{1}+^{2}_{x^{j,i},x^{k}_{1}}(x^{j,i}+v(x^{j,i},0))}{N}_{x^{k}_{1}}x^{k}_{1}.\]

_Equality is reached when the model \(v(x^{j,i},0)\) has exact values equal to (12)._

   Probability Path & \(q(z)\) & \(_{t}(z)\) & \(_{t}\) & 
 Explicit expressions: \\ vector field (VF) and score (S) \\  \\  Var. Exploding  & \(_{1}(x_{1})\) & \(x_{1}\) & \(_{1-t}\) & VF: (32) \\ Var. Preserving  & \(_{1}(x_{1})\) & \(_{1-t}x_{1}\) & \(^{2}}\) & VF: (31) \\ Flow Matching  & \(_{1}(x_{1})\) & \(tx_{1}\) & \(t_{s}-t+1\) & VF: (11) if \(=0\); and (26) \\ Independent CFM & \(_{0}(x_{0})_{1}(x_{1})\) & \(tx_{1}+(1-t)x_{0}\) & \(\) & VF: (10) \\ Schrödinger Bridge CFM  & \(_{0}(x_{0})_{1}(x_{1})\) & \(tx_{1}+(1-t)x_{0}\) & \(\) & Can be obtained by SDE using \\  & & & & VF: (49), S:(50) \\   

Table 1: Correspondence between some methods which can reduced to FM framework and our theoretical descriptions of them.

In comparison to CFM, the dispersion of the update is \(N\) times smaller than the dispersion of the target distribution and could be controlled without impeding convergence by adjusting the number of samples \(N\). In Figure 1(b), we visually compare the dispersions of CFM and ExFM. The illustration aligns a standard normal distribution \((0,I)\) with a shifted and scaled variant \((,I^{2})\). ExFM yields lower dispersion throughout the range \(t\). Detailed analytical calculations of the optimal velocity \(v(x,t)\) and dispersion are provided in the Appendix G.

## 3 Numerical Experiments

Toy 2D dataWe conducted unconditional density estimation among eight distributions. Additional details of the experiments see in the Appendix H. We commence the exposition of our findings by showcasing a series of classical 2-dimensional examples, as depicted in Fig. 3 and Table 2. Our observations indicate that ExFM adeptly handles complex distribution shapes is particularly noteworthy, especially considering its ability to do so within a small number of epochs. Additionally, the visual comparison underscores the evident superiority of ExFM over the CFM approach.

   Data &  &  \\  Data & ExFM & CFM & ExFM & CFM \\  swissroll & 1.13e-02 & 2.12e+00 & **2.58e-03** & 1.07e-02 \\ moons & 9.96e-03 & 2.01e+00 & **2.74e-03** & 1.41e-02 \\ SQUASians & 2.40e-02 & 2.77e+00 & **4.90e-03** & 2.45e-02 \\ cicheles & 9.28e-03 & 3.79e+00 & **6.69e-04** & 1.32e-02 \\
2spirals & 8.92e-03 & 2.34e+00 & **1.27e-03** & 8.35e-03 \\ CHECKORDORD & 1.04e-02 & 3.12e+00 & **1.01e-02** & 1.63e-02 \\ pinvheel & 4.53e-03 & 2.12e+00 & **1.01e-03** & 9.22e-03 \\ rings & 8.60e-03 & 1.93e+00 & **3.55e-04** & 2.37e-03 \\   

Table 2: ExFM and CFM metrics comparison table on toy 2D data.

Figure 3: Visual comparison of methods on toy 2D data. First row are original samples, second row sampled by ExFM, third row sampled by CFM.

   Data & ExFM & CFM & OT-CFM \\  power & **-8.51e-02 \(\) 4.85e-02** & 1.64e-01 \(\) 4.18e-02 & 5.22e-02 \(\) 3.92e-02 \\ GAS & **-5.55e+00 \(\) 3.66e-02** & -5.06e-00 \(\) 2.56e-02 & -5.48e+00 \(\) 2.90e-02 \\ HFEMASS & 2.16e+01 \(\) 6.31e-02 & **2.21e+01 \(\) 4.13e-02** & 2.16e+01 \(\) 4.32e-02 \\ gsds300 & -1.29e+02 \(\) 8.40e-01 & -1.29e+02 \(\) 8.97e-01 & **-1.32e+02 \(\) 6.39e-01** \\ miniboone & **1.34e+01 \(\) 1.95e-04** & 1.42e+01 \(\) 1.29e-04 & 1.43e+01 \(\) 9.22e-05 \\   

Table 3: NLL comparison for ExFM, CFM and OT-CFM methods over 10 000 learning steps, mean and std taken from 10 sampling iterations.

Tabular dataWe conducted unconditional density estimation on five tabular datasets, namely power, gas, hepmass, minibone, and BSDS300. Additional details of the experiments see in the Appendix H. The empirical findings obtained from the numerical experiments from Table 3 indicate a statistically significant improvement in the performance of our proposed method. Notably, ExFM demonstrates a notable acceleration in convergence rate.

High-dimensional data and additional experimentsWe conducted experiments on high-dimensional data, among them experiments on CIFAR10 and MNIST dataset. FID results on CIFAR10 shows slightly better score among sampled images.

Additional details of the experiments and sampled images see in the Appendix H.

Stochastic ExFM (ExFM-S) on toy 2D dataWe evaluated the performance of the stochastic version of ExFM (ExFM-S) with use of expressions given in Sec. E.3.2 on four standard toy datasets. The primary experimental setup follows that used in . Additional details on the hyperparameters used are available in Appendix H. Based on the findings presented in Table 4, we determine that ExFM-S surpasses I-CFM on all four datasets in terms of generative performance (\(_{2}\)) and also outperforms in terms of OT optimality (NPE) on two of them, exhibiting similar results on the remaining datasets. It also demonstrates performance similar to OT-CFM. While ExFM-S is not as robust as the basic ExFM, it enables the matching of one dataset to another (moons \(\) 8gaussians) as it does not necessitate the presence of an explicit formula for \(_{0}\). Among other things, this experiment demonstrates the feasibility of our methods when both distributions \(_{0}\) and \(_{1}\) are unknown.

## 4 Conclusions

The presented method introduces a new loss function in tracrable form (in terms of integrals) that improves upon the existing Conditional Flow Matching approach. New loss as a function of the model parameters, reaches zero at its minimum. Thanks to this, we can: a) write an explicit expression for the vector field on which the loss minimum is achieved; b) get a smaller variance when training on the discrete version of the loss, therefore, we can learn the model faster and more accurately.

Numerical experiments conducted on toy 2D data show reliable outcomes under uniform conditions and parameters. Comparison of the absolute values of loss for the proposed method and for CFM for the same distributions show that the absolute values of loss for these models differ strikingly, by a factor of \(10^{2}\)-\(10^{3}\). Experiments on high-dimensional datasets also confirm the theoretical deductions about the variance reduction of our method. However, we emphasize that we do not expect to use the proposed method in its pure form. On the contrary, we expect that the theoretical implications of our formulas will contribute to the construction of better learning or inference algorithms in conjunction with other heuristics or methods.

Algebraic analysis of variance for some cases (in particular, for the case \(t=0\) or for the case of two Gaussians as initial and final distributions) show an improvement in variance when using the new loss. However, it is rather difficult to analyze in the general case, for all times \(t\) and general distributions \(_{0}\) and \(_{1}\).

Having the expression for the vector field and score in the form of integrals, we can explicitly write out their expressions for some simple cases; in the case of Gaussian distributions we can also write out the exact solution for the trajectories. Thus, our approach allows one to advance the theoretical study of FM-based and Diffusion Model-based frameworks.

  Movie & \(_{1}}\) & \(_{2}}\) & \(_{3}}\) & \(_{4}}\) & \(_{5}}\) & \(_{6}}\) & \(_{7}}\) & \(_{8}}\) & \(_{9}}\) & \(_{10}}\) & \(_{11}}\) \\  Algorithm 1. ExFM & \(0.522 0.015\) & \(0.647 0.075\) & \(0.496 0.231\) & \(1.262 0.076\) & \(0.328 0.015\) & \(0.209 0.089\) & \(0.495 0.025\) & \(0.098 0.04\) \\  OT-CFM & \(0.472 0.038\) & \(0.525 0.033\) & \(0.569 0.018\) & \(1.220 0.056\) & \(0.465 0.046\) & \(0.641 0.088\) & \(0.047 0.026\) & \(0.081 0.024\) \\ ExFM & \(\) & \(\) & \(\) & \(1.726 0.043\) & \(0.325 0.000\) & \(0.233 0.012\) & \(0.069 0.041\) \\ ExFM-S & \(0.486 0.019\) & \(0.570 0.053\) & \(0.728 0.063\) & \(1.561 0.181\) & \(0.55 0.143\) & \(0.166 0.019\) & \(0.346 0.015\) & \(0.058 0.059\) \\  

Table 4: ExFM-S evaluation on four toy datasets (\(\) over three seeds). For comparison we take I-CFM, OT-CFM, and ExFM (no values for moons \(\) 8gaussians due to the absence of explicit formula for \(_{0}\)). Performance in generative modeling (\(_{2}\)) and dynamic OT optimality (NPE) is assessed. The best result for each metric is highlighted in bold. Instances where we outperform CFM are underscored.