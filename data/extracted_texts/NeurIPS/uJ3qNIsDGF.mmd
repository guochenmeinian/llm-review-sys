# Exploring Geometry of Blind Spots in Vision Models

Sriram Balasubramanian

sriramb@cs.umd.edu

Gaurang Sriramanan

gaurangs@cs.umd.edu

Vinu Sankar Sadasivan

vinu@cs.umd.edu

Soheil Feizi

sfeizi@cs.umd.edu

Equal Contribution, author names ordered alphabetically

###### Abstract

Despite the remarkable success of deep neural networks in a myriad of settings, several works have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as adversarial attacks. On the other hand, prior works have also observed that deep networks can be under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable changes to network activations. In this work, we study in detail the phenomenon of under-sensitivity in vision models such as CNNs and Transformers, and present techniques to study the geometry and extent of "equi-confidence" level sets of such networks. We propose a Level Set Traversal algorithm that iteratively explores regions of high confidence with respect to the input space using orthogonal components of the local gradients. Given a source image, we use this algorithm to identify inputs that lie in the same equi-confidence level set as the source image despite being perceptually similar to arbitrary images from other classes. We further observe that the source image is linearly connected by a high-confidence path to these inputs, uncovering a star-like structure for level sets of deep networks. Furthermore, we attempt to identify and estimate the extent of these connected higher-dimensional regions over which the model maintains a high degree of confidence. The code for this project is publicly available at this URL.

## 1 Introduction

Deep neural networks have demonstrated remarkable success in various diverse domains including computer vision and natural language processing, surpassing the performance of classical algorithms by a significant margin. However, though they achieve state of the art results on many computer vision tasks like image classification, sometimes even exceeding human-level performance , the overall visual processing conducted by such models can deviate significantly from that effectively observed in the human visual system. Perhaps most iconic and representative of these differences lies in the overwhelming susceptibility of neural networks to near-imperceptible changes to their inputs -- commonly called adversarial attacks  -- an extraordinary failure mode that highlights the _over-sensitivity_ of such models. Indeed, an active area of research over the past few years has been focused towards analyzing adversarial perturbations under different threat models  and in addressing adversarial vulnerabilities using robust training methodologies .

On the other hand, it has been shown that such models may also be _under-sensitive_, wherein input images that are unmistakably disparate to a human oracle induce near identical network activations or predictions (Jacobsen et al., 2018). To tractably analyze this phenomenon, Jacobsen et al. (2018) utilize a special class of neural networks that are bijective functions, called fully Invertible RevNets (Jacobsen et al., 2018, Kingma and Dhariwal, 2018), to craft large-magnitude semantic perturbations in input space that are designed to leave its corresponding network representations unchanged. Furthermore, Tramer et al. (2020) show that robust training with \(_{p}\) bounded adversaries can be a source of excessive model-invariance in itself, due to the poor approximation of the true imperceptible threat model of human oracles by \(_{p}\) norm-bounded balls in RGB-pixel space (Laidlaw et al., 2021). Indeed, the authors 'break' a provably robust defense on MNIST (Zhang et al., 2019) with a certified accuracy of \(87\%\), by crafting perturbations within the certified \(_{}\) radius of \(0.4\), that however cause model agreement with human oracles to diminish to \(60\%\).

However, these methods either rely upon special invertible network architectures or the selection of the nearest training image of another class as a target followed by a sequence of complex alignment and spectral clustering techniques, so as to semantically alter the input in a conspicuous manner that induces a change in the human assigned oracle label, while leaving the model prediction unchanged. This leads us to our research question: Is it possible to analyze the phenomenon of _under-sensitivity_ of general vision models in a systematic manner on natural image datasets, and characterize the geometry and extent of "blind spots" of such networks? Indeed, we empirically demonstrate the veracity of this claim -- in this work, we present a novel Level Set Traversal algorithm to explore the "equi-confidence" level sets of popular vision models. Given an arbitrary source and target image pair, our proposed algorithm successfully finds inputs that lie in the same level set as the source image, despite being near-identical perceptually to the target image. Furthermore, the proposed algorithm identifies a connected path between the original source image and the "blind spot" input so generated, wherein high prediction confidence with respect to the source class is maintained throughout the path. In summary, we make the following contributions in this work:

* We present a novel Level Set Traversal algorithm that iteratively uses orthogonal components of the local gradient to identify the "blind spots" of common vision models such as CNNs and ViTs on CIFAR-10 and ImageNet.
* We thereby show that there exist piecewise-linear connected paths in input space between images that a human oracle would deem to be extremely disparate, though vision models retain a near-uniform level of confidence on the same path.
* Furthermore, we show that the linear interpolant path between these images also remarkably lies within the same level set; as we observe the consistent presence of this phenomenon across arbitrary source-target image pairs, this unveils a star-like set substructure within these equi-confidence level sets.
* We demonstrate that adversarially robust models tend to be _under-sensitive_ over subsets of the input domain that lie well beyond its original threat model, and display level-sets of high-confidence that extend over a significant fraction of the triangular convex hull of a given source image and arbitrary pair of target images.

## 2 Preliminaries

**Notation:** In this paper, we primarily consider the setting of classification with access to a labelled dataset. Let \(\) denote \(d\)-dimensional input images, and let their corresponding labels be denoted as \(y\{1,,N\}\). Let \(f:^{N}\) denote the classification model considered, where \(f()=f^{1}(),,f^{N}()\) represents the softmax predictions over the \(N\)-classes. Further, let \(C_{f}()\) be the argmax over the \(N\)-dimensional softmax output, representing the class predicted by the model for input \(\). For a given data sample \((,y)\), let the cross-entropy loss achieved by the model be denoted as \(CE(f(),y)\). Given a prediction confidence value \(p\), and a class \(j\{1,,N\}\) we define the Level Set \(L_{f}(p,j)\), and Superlevel Set \(L_{f}^{+}(p,j)\) for the function \(f\) as follows:

\[L_{f}(p,j)=\{:f^{j}(x)=p\}\ \,\ L_{f}^{+}(p,j)=\{ :f^{j}(x) p\}\]

Given a pair of inputs \(_{1}\) and \(_{2}\), we define the linear interpolant path between them as \(P(;_{1},_{2})=_{1}+(1-)_{2}\), for \(\). A given set \(S\) is thus said to be convex if \(P(;_{1},_{2}) S,\;_{1},_{2} S\) and \(\). Further, a set \(S\) is said to be _star-like_ if there exists some \(_{0} S\) such that \(P(;_{0},) S,\; S\) and \(\).

### Conjugate Nature of Adversarial and Confidence Preserving Perturbations

Given a classification model \(f\) and a correctly classified benign input \((,y)\), an adversarial image is a specially crafted image \(}=+\) such that both \(\) and \(}\) appear near-identical to a human oracle, but induces the network to misclassify, i.e. \(C_{f}(+) C_{f}()=y\). To enforce imperceptibility in a computationally tractable manner, several adversarial threat models have been proposed, with the \(_{2}\) and \(_{}\) norm constraint models being the most popular. Amongst the earliest adversarial attacks specific to the latter threat model was the Fast Gradient Sign Method (FGSM) attack, proposed by Goodfellow et al. (2015), wherein the adversarial perturbation is found by single-step direct ascent along the local gradient with pixel-wise clipping. A stronger multi-step variant of this attack called Iterated-FGSM (IFGSM) was later introduced by (Kurakin et al., 2017), wherein iterated gradient ascent is performed alternately with projection operations onto the constraint set. A popular variant, called the Projected Gradient Descent (PGD) attack was introduced by Madry et al. (2018) which incorporates a initial random perturbation to the clean image, which was observed to help mitigate gradient masking effects Kurakin et al. (2016). A large class of adversarial attacks (Croce and Hein, 2020; Gowal et al., 2019; Carlini et al., 2019; Sriramanan et al., 2020) thus utilize perturbations parallel to the gradient, with appropriate projection operations to ensure constraint satisfaction.

In contrast, perturbations that leave the network prediction confidence unchanged, are locally orthogonal to the gradient direction. Indeed, for any differentiable function \(g:^{d}\), denote the level set as \(L_{g}(c)\) for a given output \(c\). Let \((t): L_{g}(c)\) be any differentiable curve within the level set. Then, \(g((t))=c\; t\). Thus, \((g((t)))=0= g((t)),^{}(t)\), implying that \(^{}(t)\) is orthogonal to \( g((t)),\;\; t\). Since this is true for _any_ curve \(\) contained in the level set, we conclude that the gradient vector is always perpendicular to the level set. Furthermore, we can additionally show that the level set \(L_{g}(c)\) is often a differentiable submanifold, with mild additional conditions on the gradient. Indeed, a level set \(L_{g}(c)\) is said to be _regular_ if \( g()\;\; L_{g}(c)\).

**Lemma 1**.: _If \(g:^{d}\) is a continuously differentiable function, then each of its regular level sets is a \((d-1)\) dimensional submanifold of \(^{d}\)._

We present the proof of Lemma 1 in Section A of the Appendix. We thus observe that adversarial perturbations, largely parallel to the gradient, are locally orthogonal to confidence preserving directions which correspond to the \((d-1)\) dimensional tangent space of the level set. We take inspiration from this observation to develop a general framework that applies to a broad class of differentiable neural networks, as opposed to previous works (Jacobsen et al., 2018) that require the network to be invertible to identify confidence preserving perturbations.

We also remark that adversarial attacks and level set preserving perturbations are complementary from another perspective as well, as noted by prior works: the former attempts to find inputs that change model predictions without modifying human oracle assignments, while the latter attempts to keep network predictions unchanged though human oracles would likely change their original label assignment. Thus, both classes of adversarial attacks and level set preserving perturbations induce misalignment between oracle and model predictions, and cast light onto independent means of evaluating the coherence of models towards human expectations.

## 3 Proposed Method: Level Set Traversal

We now describe our algorithm for traversing the level set, which we call the Level Set Traversal (LST) algorithm (Algorithm 1). We try to find a path from a source image to a target image such that all points on that path are classified by the model as the source class with high confidence. Given that these \((d-1)\) dimensional level set submanifolds can be potentially highly complex in their geometries, we use a discretized approximation using small yet finite step sizes to tractably explore these regions. Let the source image be \(_{s}\) with true label \(y_{s}\), and \(=_{}CE(f(),y_{s})\) be the gradient of the cross entropy loss of the model (\(f\)) prediction with respect to \(\). The key idea is to get as close as possible to a target image \(_{t}\) from some image \(\) by computing the projection of \(=_{t}-\) onto the orthogonal complement of \(\), to obtain a new image \(_{}\) that leaves the model confidence unchanged. We can compute the projection of \(\) on the orthogonal complement by subtracting the component of \(\) parallel to \(\), that is, \(_{||}=(^{}}{\|\|^{d}})\) (L6). Then, using a scale factor \(\), the vector update to \(\) can be expressed as \(_{}=(-_{||})\) (L7), and therefore \(_{}=+_{}\). Starting from the source image \(_{s}\), we repeatedly perform this iteration to get reasonably close to the target imagewhile carefully ensuring that the confidence of the model prediction at any given point does not drop below a preset confidence threshold \(\) compared to the source image (L10).

While the above method works well with a relatively large \(\) if the curvature of \(f\) is low, it risks a non-trivial drop in model confidence (or even a change in the model prediction) if the curvature of \(f\) at \(\) is high enough. Therefore, after each step, we add a small perturbation vector \(_{||}\) in the direction of \(-\) scaled by a factor of \(\). This step decreases the cross entropy loss, and thus increases the confidence so as to offset any confidence drops incurred due to the addition of \( x_{}\). Thus, we can maintain a higher model confidence over the path. In order to ensure that the norm of this perturbation is not arbitrarily large, we bound the \(_{}\) norm by projecting the vector to an \(_{}\) ball of radius \(\). Then, the step \(_{||}\) can be computed as \((,-,)\), where the function \((,a,b)\) is the element-wise application of the function \(((,a),b)\) on all elements of \(\). Thus, we modify L9 so that now \(_{}=+_{}-_{||}\). We present a pictorial schematic of Algorithm 1 in Fig 1. We further employ an exponential moving average of \(_{||}\), in order to smoothen components that potentially undulate heavily during the iterative traversal. Thus, since the frequent changes in \(_{||}\) are smoothened out, we find that the final output images are often linearly connected to the source image with high confidence over the linear interpolations (see Fig 6).

```
1:Input: Source image \(_{s}\) with label \(y\), target image \(_{t}\), model \(f\), max iterations \(m\), scale factor \(\), stepsize \(\), confidence threshold \(\)
2:Initialize \(=_{s},_{||}=\)
3:for\(i=1\)to\(m\)do
4:\(=_{t}-\)
5:\(=_{}CE(f(),y)\)
6:\(c_{//}=()/||||^{2}\)
7:\(_{}=(-c_{//})\)
8:\(_{||}=_{}(_{||}-,-,)\)
9:\(_{}=+_{}+_{||}\)
10:if\(f(_{s})[j]-f(_{})[j]>\)then
11:return\(\)
12:\(=_{}\)
13:return\(\) ```

**Algorithm 1** Level Set Traversal (LST)

We now apply this algorithm to explore the level sets of standard and robust ResNet-50 models. In Fig 2, we present the path traversed by the LST algorithm, wherein the model predicts the source 'goose' class with very high confidence over the entire path, though the target blindspot found by LST clearly appears as a 'dog' to human oracles. To substantiate the efficacy of the LST algorithm, we randomly select five images from five arbitrary ImageNet classes ('goose', 'Scottish Terrier','meerkat', 'academic gown', 'cleaver'), and compute LST blindspots for all possible source-target image pairs. We show the final images output by the LST algorithm in Fig 3 as an image-grid. If we order the five selected images, the \(i^{th}\) row and \(j^{th}\) column of the image-grid is the LST output obtained using the \(i^{th}\) image as target and \(j^{th}\) image as the source. Thus, each column represents the source image being transformed iteratively into the other target images. The confidence of the model prediction for the source class (names on top of each column) is displayed just below each image. For both the normally trained and adversarially trained model, these images are almost indistinguishable from the target while retaining high model confidence for the source class. Since adversarially robust models have

Figure 1: Pictorial representation of the LST algorithm. The contour lines represent level sets of the model confidence \(f()\), with blue representing high confidence (low loss) and red representing low confidence (high loss). At each iteration, we obtain \(_{}\) by adding two vectors, \(_{}=(-c_{//})\) (projection of \(\) onto the orthogonal complement of \(\)) and \(_{||}\) (a small perturbation to increase the confidence and remain within the level set).

Figure 2: Intermediate images over the path traversed by the LST algorithm using a source image of a ‘goose’ and a target image of a ‘Scottish terrier’ (a dog) for a normally trained ResNet-50 model. We observe that the model predicts the ‘goose’ class with very high confidence for all images over the path, though the target blindspot found by LST clearly appears as a ‘dog’ to human oracles.

perceptually aligned gradients, we can sometimes visually notice a few traces of the source image in the final LST image; for example the'meerkat' image in the 3rd row, 2nd column in the right side of Fig 3 has some traces of the source 'terrier' image, but differences are usually hard to perceive.

We also examine the model confidence over linear interpolations between the source image and LST outputs for all target pairs in Fig 6. Formally, consider a source image \(_{s}\) with label \(y\) and let \(_{}\) represent the LST output when applied toward a target image \(_{t}\). Denote the difference vector as \(=_{}-_{s}\). Then, we observe that \(_{s}+\) is assigned high confidence with respect to class \(y\) by the model \(\), which represents the entire linear interpolant path between \(_{s}\) and \(_{}\). Furthermore, we observe that the path discovered by the Level Set Traversal algorithm enjoys two key properties: (1) Uniqueness (once the target image is fixed) and (2) Extremality:

(1) _Uniqueness:_ Since the local tangent space of the level set is \((d-1)\) dimensional, several independent directions are orthogonal to the local gradient, and apriori do not yield a unique path like a gradient-flow. However, once we fix our target image, we use its difference vector with respect to the current iterate (L4) and compute its projection onto the local tangent space (L7) of the level set, thereby generating a _uniquely defined path_.

(2) _Extremality:_ Though this flow-based path may be non-linear, we additionally discover that the final output-point of this flow is surprisingly linearly connected with high-confidence to the source image after we apply discretized approximations in practical settings for common vision models etc. Formally, the LST output \(_{}\) is _linearly extremal_ in the sense that \(_{s}+(1+)\) is rapidly assigned low-confidence by the model even for extremely small values of \(>0\), where \(=_{}-_{s}\).

Thus, using the LST algorithm, we find that the level sets of common models extend outwards in an expansive, connected manner to include images of _arbitrary_ classes, which human oracles would never state as being similar. Since the linear path from any given source image to LST outputs for arbitrary target images retains high model confidence throughout, it unveils a remarkable star-like substructure for superlevel sets as shown in Fig 4, where the number of "limbs" or linear protuberances of the star-like structure is _extraordinarily large_, plausibly as large as the number of images in all other classes. Furthermore, to study the size of the level sets beyond the one-dimensional interpolant paths, we analyze the two-dimensional triangular convex hull between a given source image and two LST output blindspot images, using quantitative metrics in Section 6.

Figure 3: The images returned by LST for 5 random source and target images for normally trained (left) and adversarially trained (right) ResNet-50 models. The image in the \(i^{th}\) row and \(j^{th}\) column is the LST output obtained using the \(i^{th}\) image as target and \(j^{th}\) image as the source. The confidence of the model prediction for the source class (names on top of each column) is displayed just below each image. For example, all four LST output blindspots in the first column (highlighted), using the source ‘goose’ image, are all predicted to be of the ‘goose’ class with very high confidence. Diagonal images are unchanged, as source equals target. We observe that almost any source image can be iteratively modified using LST to resemble any target image very closely without any loss in confidence for both normal and adversarially trained ResNet-50 models.

## 4 Disconnected Nature of Standard Adversarial Attacks

At first glance, the images output by the LST algorithm may seem similar to those produced using targeted variants of standard adversarial attacks. In this section, we explore the connectivity of high-confidence paths as induced by standard adversarial attacks, and show that adversarially attacked source images are not linearly connected with high-confidence paths to their target image. In detail, let \((_{1},y_{1})\) and \((_{2},y_{2})\) be any two data samples from different classes \((y_{1} y_{2})\). A targeted adversarial attack [Carlini and Wagner, 2017] on input \(_{1}\) with respect to class \(y_{2}\) is formulated as solving \(_{12}=_{_{12}:||_{12}|| }CE(f(_{1}+_{12}),y_{2})\). A related variant, called a feature-level targeted adversarial attack [Sabour et al., 2015] instead uses intermediate network activations to craft image perturbations. If \(f_{|L}()\) represents the network activations from hidden layer \(L\) for an input \(\), then this attack attempts to match features by optimizing \(_{12}=_{_{12}:||_{12} ||}||f_{|L}(_{1}+_{12})-f_{|L}(_{2})||\). The hidden layer (\(L\)) that is often selected corresponds to pre-activations of the final fully-connected layer, and thus often induces misclassification.

Using this framework, we can thus analyze the connectivity of high-confidence paths with respect to class \(y_{2}\) between the target benign sample (\(_{2}\)) and adversarial examples (such as \(_{1}+_{12}\) targeted towards \(_{2}\)), using the linear interpolant path between the two. By doing so, we can analyze the level set with respect to class \(y_{2}\) using targeted adversarial examples alone, though the perturbation utilized is inherently norm-limited. In Fig 5, we plot the median confidence of a normally trained ResNet-50 model over the linear interpolant paths for 1000 source-target pairs. We find that though the model confidence with respect to class \(y_{2}\) is high for both the target benign input \(_{2}\) and the targeted adversarially attacked input \(_{1}+_{12}\) (that is, at the end-points of the linear path), the model _does not_ maintain high confidence over the linear interpolant path between the two, but sharply declines to a valley of near zero-confidence over the path. For specific inputs, we often observe sharp, erratic diminutions in target-class prediction confidence over the convex combination, with a non-trivial region of near-zero confidence. This contrasts sharply with the existence of linear-connectivity observed between a source image and LST blindspot images. We thus crucially note that adversarial attacks are _standalone insufficient_ to study the structure of level sets of common vision models. However, we incorporate them into our proposed Level Set Traversal algorithm in a fruitful manner.

## 5 Theoretical Analysis for Common Models

To better understand the geometry of level set submanifolds for a given prediction confidence threshold, we analyze the nature of confidence preserving perturbations in a few simplified model settings.

Figure 4: Schematic of Star-like set substructure of Superlevel sets: The linear interpolant paths between the source image \(_{s}\) and blindspots found using LST maintain high-confidence throughout for arbitrary target images of other classes.

Figure 5: Model Confidence over Linear interpolant Paths: We plot the median prediction confidence with respect to class \(y_{2}\) over linear paths between the target benign sample (\(_{2}\)) and adversarial examples (such as \(_{1}+_{12}\) targeted towards input \(_{2}\)) in input space. While model confidence falls to near-zero using standard targeted adversarial attacks, the linear path between the LST output and source image has high confidence of almost 1.0 throughout.

**1) Linear Functional:** First, consider the classification model to be a linear functional \(f:^{d}\), that is, \(f(_{1}+_{2})=f(_{1})+f(_{2})\) and \(f()= f()\). Then, by the Riesz Representation theorem, there exists a unique vector \(_{f}^{d}\) such that \(f()=_{f},\ \ ^{d}\). We thus observe that for any vector \(\) orthogonal to \(_{f}\), \(f(+)=_{f},+=_{f}, +_{f},=_{f},\). Thus, in this setting, the level sets of \(f\) are \((d-1)\) dimensional _linear subspaces_ spanned by the set \(\{^{d}:,=0\}\). We observe that a similar argument can be extended to affine functions of the form \(f()=_{f},+c\), where \(c\).

We remark that by applying a first-order Taylor series approximation for general real-valued smooth functions, we observe near-affine behavior locally within a small neighborhood: \(f(+)=f()+( f(),)+O( ^{2}\|\|^{2})\). Thus locally, we observe that confidence-preserving perturbations can arise from a \((d-1)\) dimensional plane orthogonal to the gradient. Indeed we incorporate this implicitly in our proposed Level Set Traversal algorithm, wherein the orthogonal projection \(_{}\) with respect to the local gradient is iteratively computed, and its relative success can partly be attributed to the orthogonal hyperplane having a large dimension, namely \((d-1)\).

Another related setting worthy of note is that of neural networks that utilize ReLU activations, which induces a piece-wise linear structure over tessellated subsets of the input domain. Thus, a given output neuron of a ReLU network functionally has the form \(f()=,+c\) within a given tessellated region, and thus has a constant gradient within the same region. Further, between two such adjacent regions, the two orthogonal \((d-1)\) dimensional hyperplanes typically intersect over an affine space over dimension at least \((d-2)\). Thus if \(_{1},_{2}\) are two inputs such that their linear interpolant path cuts across \(n\) distinct tessellated regions, the common intersection of these orthogonal hyperplanes will typically be \((d-n)\) dimensional, indicating that there exist perturbations which lie in the common null-space of the gradients as defined along each tessellated region that is crossed. Indeed in the following section, we demonstrate empirically for Residual Networks that though the exact iterative path followed by the Level Set Traversal algorithm is discretized and non-linear, the final outputs so found are often linearly connected through paths of high confidence to the source image, thereby lying within the level set of the original source class. This indicates that the overlap of different \((d-1)\) dimensional hyperplanes is non-trivial at a non-local scale in image space, whereby we observe extended connected regions of high confidence.

**2) Full-Rank Linear Transformations:** Let us now consider a setting apart from classification, such as regression, wherein the complete vector representation of the output is of principal interest. Let the model be of the form \(f:^{d}^{d}\), \(f()=A\), where \(A^{d d}\) is a full-rank matrix. In this setting, we observe that if \(f(_{1})=f(_{2})=A_{1}=A_{2}\), implying that \(A(_{1}-_{2})=\), the zero-vector. But since \(A\) is full-rank, this implies that \(_{1}-_{2}=\), i.e, \(_{1}=_{2}\). Thus, \(f\) is a bijective function and has a trivial null space. Thus in this setting, we necessarily have to relax the problem to be that of identifying perturbations of large magnitude to the input that _minimally_ change the function output: that is, we solve for \(_{}\|^{2}}{\|\|^{2}}\). Indeed, let the Singular Value Decomposition (SVD) of the full rank matrix \(A\) be given by \(A=U V^{T}\), where \(U,V\) are \(d d\) orthogonal matrices, and \(\) is a diagonal matrix consisting of the positive singular values \(_{i}\) for \(1 i d\), in descending order without loss of generality. Then, \(\|A\|=\|U V^{T}\|=\|U( V^{T})\|=\| V^{T} \|\) since \(U\) is an orthogonal matrix. Similarly, since \(V\) is orthogonal as well, let \(=V^{T}\), so that \(\|\|=\|\|\). Thus, \(_{}\|^{2}}{\|\|^{2}}=_{}\|^{2}}{\|\|^{2}}=_{}\| {z}\|^{2}\)such that \(^{T}=1\). But since \(\) is diagonal, \(_{}\|\|^{2}=_{i=1}^{d}_{i}^{2}z_{i}^{2}\), under the constraint that \(||z||^{2}=1\). It is then easy to observe that the minimum value attained is \(_{d}^{2}=_{min}^{2}\), and is attained when the input vector to \(f\) is the right-singular vector corresponding to the minimum singular value of \(A\).

In this setting, we remark that adversarial perturbations, in contrast, can be formulated in this setting as \(_{}\|^{2}}{\|\|^{2}}\), with the maximum value given by \(_{max}^{2}\) and attained by the right-singular vector corresponding to the maximum singular value of \(A\), highlighting the complementary nature of the two problems as indicated previously in Section 2.1. Indeed, the condition number \(\) of an invertible matrix \(A\) is defined as \(=_{max}/_{min}=\|A\|\|A^{-1}\|\). If condition number is large with \( 1\), the matrix \(A\) is said to be ill-conditioned, and induces an inescapable compromise: if \(_{max}=\|A\| 1\) (as potentially expected in "robust" networks), then \(1/_{min}=\|A^{-1}\| 1\) is necessarily large, thereby inducing extreme _under-sensitivity_ along some dimensions; while if \(1/_{min}=\|A^{-1}\| 1\), then \(_{max}=\|A\| 1\) and the model is extremely _over-sensitive_, similar to the phenomenon of adversarial vulnerability.

## 6 Quantifying Under-Sensitivity of Vision Models

In this paper, we primarily consider standard vision datasets such as ImageNet (Deng et al., 2009) and CIFAR-10 (Krizhevsky et al., 2009) (latter in Section C of the Appendix). We thereby explore the "blind-spots" of popular vision models such ResNet (He et al., 2016) and Vision Transformers (Dosovitskiy et al., 2020, Touvron et al., 2021). Furthermore, we explore the connectivity of such level sets on normally trained variants of such networks, as well as adversarially robust counterparts. For the latter, we analyze robust models trained adversarially against \((4/255)\)\(_{}\) constrained adversaries, available on RobustBench (Croce et al., 2021). Specifically, we utilize a robust ResNet-50 model from Salman et al. (2020) and a robust DeiT model from Singh et al. (2023). We also fix the hyperparameters of LST for all models for a fair comparison (detailed in Section E of the Appendix).

**Image Quality Metrics:** We now proceed to quantitatively verify our observations in Section 3. First, to help quantify the deviation between target images and blindspot outputs generated by our proposed Level Set Traversal algorithm, we utilize a combination of classical image metrics such as RMSE, \(_{}\) and Structural Similarity Index (SSIM), and perceptual measures such as LPIPS distance using AlexNet (Zhang et al., 2018). For SSIM, a higher value indicates a closer match, while for all other metrics, a lower value indicates a closer match. To calculate these metrics, we sample around 1000 source images from ImageNet, and select five other random target images of different classes for each source image. We present the image quality metrics for blindspots discovered by LST in Table 1. Here the standard deviation is over the different randomly chosen source and target images.

**Metrics for Model Confidence:** To evaluate the extent of the model invariance over the regions between the LST outputs and the source image, we evaluate the model confidence with respect to the source class over one-dimensional linear interpolant paths and over two-dimensional subspaces as well. For the latter, we evaluate the model confidence over the triangular convex hull obtained by linear interpolation over three reference points, namely the source image and the two target blindspot images produced using LST. For example, the input at the 'centroid' of the triangle formed by a source image and pair of target blindspots is the arithmetic mean of the three images. We visualize these in Fig 6, wherein the prediction confidence (in the range \(\)) assigned by the model with respect to the source class is mapped to a continuous colorbar, with high-confidence points (close to 1.0) appearing as bright yellow, and low-confidence points (close to 0.0) appearing as dark violet. Specifically, we use the following metrics: (a) **Average Triangle (\(\)) Confidence**: the mean of the model's source class confidence over the enclosed triangle, (b) **Average Triangle (\(\)) Fraction** for various values of \(\): the fraction of inputs in the triangular region for which the model confidence is greater than \(p_{}-\), averaged over all possible target blindspot pairs, where \(p_{}\) is the confidence of the source image, (c) **Average Path Confidence**: the average model confidence over all linear paths from the source image to all LST blindspot images. The higher these metrics, the more confident, and thus invariant, the model is in this region. For computing these metrics, we use linear interpolations between the source images and the 5 LST outputs found previously for computing the distance metrics. We thus use \(=10\) triangles for each source image, and sample this triangular area in an equispaced manner to obtain 66 images for computation of the triangle (\(\)) metrics. We present these metrics in Table 2, along with the mean (and standard deviation) of model confidence on the source class images (\(p_{}\)) for reference. Here, the standard deviation is over the different randomly chosen source images.

We can now quantitatively confirm many of the trends we qualitatively observed in Section 3. In Table 1, we observe that the LST outputs found for the models are closer to the targets as compared to the adversarially trained models. The difference is particularly stark when comparing the \(_{}\) distances, as \(_{}\) is a particularly sensitive metric and is also the threat model against which the models were trained. However, for other distance metrics like RMSE or LPIPS, the difference between normally trained and adversarially trained ResNet-50 is not as high. In particular, LPIPS

  
**Models** & RMSE : \(\) & \(_{}\) dist: \(\) & SSIM: \(\) & LPIPS dist: \(\) \\  ResNet-50 (Normal) & 0.008 \(\) 0.001 & 0.046 \(\) 0.020 & 0.990 \(\) 0.021 & 0.002 \(\) 0.004 \\ ResNet-50 (AT) & 0.029 \(\) 0.008 & 0.746 \(\) 0.124 & 0.915 \(\) 0.041 & 0.057 \(\) 0.037 \\ DeiT-S (Normal) & 0.011 \(\) 0.002 & 0.116 \(\) 0.030 & 0.973 \(\) 0.024 & 0.024 \(\) 0.017 \\ DeiT-S (AT) & 0.046 \(\) 0.010 & 0.821 \(\) 0.117 & 0.898 \(\) 0.041 & 0.219 \(\) 0.068 \\   

Table 1: Quantitative image distance metrics between output of Level Set Traversal and target images.

distances for both models are low, which indirectly implies that the perceived difference between the images for humans oracles is relatively low. This can also be confirmed by visually inspecting the images in Fig 3. For the normally trained DeiT-S, the distance metrics are very similar to that of ResNet-50, with slightly higher RMSE and LPIPS distances. However, the adversarially trained (AT) variant is significantly less under-sensitive compared to both its normal counterpart and the ResNet-50 (AT). Specifically, the LPIPS distance metric is much greater for DeiT-S (AT), which implies there exist significant human-perceptible differences in the image. We can confirm this in Fig 7, where the LST output for the adversarially trained DeiT-S model contains visible traces of the source image. However, the LST outputs are still clearly much closer to the target class as compared to the source, which indicates that there are still some significant blind spots in DeiT-S (AT).

However, when we measure the extent of model invariance over the convex regions enclosed between the LST output and the target images, we find that adversarially trained ResNet-50 are overall _more_ invariant (or under-sensitive) as compared to the normally trained variant. For example, the average triangle confidence for ResNet-50 (AT) is higher than that of normally trained ResNet-50, even though its source confidence is much lower. We also find that a much larger fraction of the triangular convex hull lies within the superlevel set for \(p_{}-\) for ResNet-50 (AT) as compared to normal ResNet-50 for all values of \(\). The average path confidence is much closer to \(p_{}\) for ResNet-50 (AT) as compared to normally trained ResNet-50. This quantitatively verifies the observation made in Fig 6 that adversarial training demonstrably exacerbates under-sensitivity. Interestingly, these robust models are under-sensitive over subsets of the input domain that lie well beyond the original threat model used in its training. Moreover, between the normally trained DeiT-S and ResNet-50 models, the former appears to be more invariant with greater average confidence over the triangular convex hulls, despite having lower source image confidences \(p_{}\). For the robust variant of DeiT-S however, the trend is less apparent due to the significantly lower average source image confidences \(p_{}\). However the average relative \(\) fraction becomes higher for larger values of \(\) (such as \(0.3\)), indicating that the superlevel sets are indeed expansive, albeit for lower confidence thresholds.

    & \(p_{}\) & Avg \(\) Conf. &  & Avg Path Conf. \\  & (\(\)) & (\(\)) & \(=0.0\) & \(=0.1\) & \(=0.2\) & \(=0.3\) & (\(\)) \\  ResNet-50 (Normal) & 0.99 \(\) 0.02 & 0.56 \(\) 0.10 & 0.13 \(\) 0.15 & 0.51 \(\) 0.11 & 0.53 \(\) 0.1 & 0.54 \(\) 0.10 & 0.96 \(\) 0.05 \\ ResNet-50 (AT) & 0.88 \(\) 0.11 & 0.83 \(\) 0.09 & 0.49 \(\) 0.29 & 0.79 \(\) 0.13 & 0.85 \(\) 0.1 & 0.88 \(\) 0.09 & 0.93 \(\) 0.06 \\ DeiT-S (Normal) & 0.85 \(\) 0.06 & 0.68 \(\) 0.05 & 0.54 \(\) 0.11 & 0.67 \(\) 0.06 & 0.71 \(\) 0.06 & 0.73 \(\) 0.06 & 0.94 \(\) 0.02 \\ DeiT-S (AT) & 0.76 \(\) 0.08 & 0.59 \(\) 0.07 & 0.20 \(\) 0.09 & 0.43 \(\) 0.14 & 0.63 \(\) 0.15 & 0.76 \(\) 0.12 & 0.73 \(\) 0.06 \\   

Table 2: Quantitative confidence metrics over the triangular convex hull (\(\)) of a given source image and two target LST blindspot image-pairs and over linear interpolant paths between source and blindspot images. (For reference, a random classifier would have confidence of 0.001)

Figure 6: Visualization of confidence of standard (top) and robust (bottom) ResNet-50 models over the triangular convex hull of a source ‘goose’ image and two LST outputs for all pairs of target images from 4 other classes (same images as in Fig 3). In all source-target image pairs, the linear interpolant path maintains high confidence, implying that the source image is linearly connected to the LST target output in a star-like substructure within the level set. For adversarially trained models, we observe that a significant fraction of the triangular hull lies in the superlevel sets of high-confidence, thereby indicating their under-sensitivity in regions far beyond their original threat model.

Discussion

While the existence of level sets alone is not very significant in itself, using the proposed LST algorithm, we find that the level set for common vision models is remarkably expansive -- large enough to contain inputs that look near-identical to arbitrary target images from other classes. Since the linear path from any given source image to LST blindspot outputs retain high model confidence throughout, the level sets have a star-like connected substructure, where the number of 'l limbs' or linear protuberances of the star-like structure is _extraordinarily large_, plausibly as large as the number of images in all other classes. This is considerably noteworthy since it indicates the hitherto unknown and unappreciated scale and extent of under-sensitivity in common vision models. Moreover this hints at the potential degree of difficulty towards adequately mitigating this phenomenon in practical settings. For instance, if the level set for images of class \(y_{1}\) contained sizable protuberances towards only one other class \(y_{2}\) alone, the problem could perhaps be tackled by introducing a contrastive training objective that encourages the network to better discriminate between \(y_{1}-y_{2}\) image pairs by utilizing a denser sampling of related image augmentations, likely resulting in the diminution of these specific 'directed' protuberances (assuming reasonable train-test generalization). But since the star-like connected substructure uncovered by LST implies that such protuberances exist towards any generic image of any other class, such simple approaches will likely be ineffective and possibly computationally infeasible from a combinatorial perspective. Thus, based on the observations uncovered with LST, we hypothesize that addressing the pervasive issue of under-sensitivity in conventional vision models might present a significantly non-trivial challenge.

## 8 Related Work

The phenomenon of under-sensitivity in classification models was first pointed out by Jacobsen et al. (2018), wherein they utilize a class of invertible neural networks called fully Invertible RevNets Jacobsen et al. (2018); Kingma and Dhariwal (2018) to specifically craft input images that do not affect network activations at a given layer. In contrast, our proposed algorithm is applicable to general network architectures since we solely utilize input-gradients to perform the traversal over image space. Tramer et al. (2020) further demonstrated that due to the misalignment of \(_{p}\) norm bounded balls and the ideal set of human-imperceptible perturbations, networks that are adversarially trained against such \(_{p}\) bounded perturbations of relatively large radius are overly-smooth, and become excessively susceptible to invariance-based attacks within the same \(_{p}\) radius. To find such images that lie within a given \(_{p}\) threat model, but induce human oracles to change their label assignment, the authors propose to identify the training image from another class closest in image space and apply a series of semantic-preserving transformations, and additionally use techniques such as realignment and spectral clustering. Given that these operations are fairly complex, the attack algorithm is slow, with alignment alone requiring an amortized time of minutes per input example. In contrast, our proposed method relies upon gradient-backpropagation steps which are efficiently parallelized across a minibatch of input images. Furthermore, our technique is seen to be successful for arbitrary source-target image pairs, since we do not utilize near-neighbour training images as the target. On the theoretical front, Jayaram et al. (2020) analyze the problem of span-recovery of neural networks given only oracle access to the network predictions. They characterize the feasibility of span recovery, and thus approximations of the null space of networks in a provable setting, but remark that its success in practical settings is potentially limited to networks that are extremely thin.

## 9 Conclusions

In this work, we investigate the phenomenon of under-sensitivity of classification models, wherein large-magnitude semantic perturbations leave network activations unchanged. To identify such "blind spots" that occur within high-confidence level sets of common vision models, we develop a novel Level Set Traversal algorithm that iteratively perturbs a given source image to appear visually similar to an arbitrary target image by utilizing orthogonal projections with respect to the local input gradient. The proposed method is applicable to general neural networks, and helps uncover a star-like substructure for the level and superlevel sets of CNNs and ViTs on common datasets. We further observe that adversarially trained models retain a high-degree of confidence over regions that lie far beyond its original threat model, with super-level sets that extend over a significant fraction of the triangular convex hull between a given source image and arbitrary pair of blindspot images.

Acknowledgements

This project was supported in part by a grant from an NSF CAREER AWARD 1942230, ONR YIP award N00014-22-1-2271, ARO's Early Career Program Award 310902-00001, Meta grant 23010098, HR00112090132 (DARPA/RED), HR001119S0026 (DARPA/GARD), Army Grant No. W911NF2120076, NIST 60NANB20D134, the NSF award CCF2212458, an Amazon Research Award and an award from Capital One.