# Supplementary Materials for

Generalized Tensor Decomposition for Understanding Multi-Output Regression under Combinatorial Shifts

Andong Wang

RIKEN AIP

andong.wang@riken.jp

&Yuning Qiu

RIKEN AIP

yuning.qiu@riken.jp

&Mingyuan Bai

RIKEN AIP

mingyuan.bai@riken.jp

&Zhong Jin

China University of Petroleum-Beijing at Karamay

zhongjin@cupk.edu.cn

&Guoxu Zhou

Guangdong University of Technology

gx.zhou@gdut.edu.cn

&Qibin Zhao

RIKEN AIP

qibin.zhao@riken.jp

Qibin Zhao and Guoxu Zhou are the corresponding authors.

###### Abstract

In multi-output regression, we identify a previously neglected challenge that arises from the inability of training distribution to cover all combinations of input features, leading to combinatorial distribution shift (CDS). To the best of our knowledge, this is the first work to formally define and address this problem. We tackle it through a novel tensor decomposition perspective, proposing the Functional t-Singular Value Decomposition (Ft-SVD) theorem which extends the classical tensor SVD to infinite and continuous feature domains, providing a natural tool for representing and analyzing multi-output functions. Within the Ft-SVD framework, we formulate the multi-output regression problem under CDS as a low-rank tensor estimation problem under the missing not at random (MNAR) setting, and introduce a series of assumptions about the true functions, training and testing distributions, and spectral properties of the ground-truth embeddings, making the problem more tractable. To address the challenges posed by CDS in multi-output regression, we develop a tailored Double-Stage Empirical Risk Minimization (ERM-DS) algorithm that leverages the spectral properties of the embeddings and uses specific hypothesis classes in each frequency component to better capture the varying spectral decay patterns. We provide rigorous theoretical analyses that establish performance guarantees for the ERM-DS algorithm. This work lays a preliminary theoretical foundation for multi-output regression under CDS.

## 1 Introduction

In the realm of Multi-Output Regressions (MOR), systems are designed to predict multiple related outputs from a set of input features, unlike single-output regressions which neglect the relationship between targets. MOR is effective for various scientific problems  such as river quality prediction , natural gas demand forecasting  and drug efficacy prediction . In many MOR applications, it is interactions among multiple features that the predictions are typically derived from. For example, to predict multiple outcomes such as risk scores and health outcomes of patients, the input features usually include healthcare costs and demographic variables (e.g., race) . Similarly, predictionsto new climate scenarios such as increased temperature and extreme weather events require new representative concentration pathways . To enhance the generalization capability, it is often necessary to increase the diversity of training samples. However, in some scenarios, data from specific interactions remain inaccessible, such as risk scores of multiple diseases involving novel combinations of factors including age, physiological indicators, and biochemical markers (see Figure 1). Therefore, unseen feature combinations impose a significant challenge to precise predictions, especially when involving multiple outputs.

To this end, in this paper, we address this issue by reducing it to only a couple of input features. This issue is known as the Combinatorial Distribution Shift (CDS) problem . Under CDS, the training data are _i.i.d._ sampled from the training distribution \(_{}\) with corresponding label \(_{i}\), where \((x_{i},y_{i})\) are the input feature combinations from the feature domains \(\) and \(\), and \(_{i}=^{*}(x_{i},y_{i})^{K}\) denotes the \(K\) outputs determined by the vector-valued ground truth function \(^{*}\). Then the problem reduces to predict the new interactions of input combinations \((x,y)\) sample from \(_{}\), where the probability density or mass of test samples \((x,y)\) under \(_{}\) may be zero. Therefore, the goal of the MOR problem becomes: _how can we generalize to the new feature combinations that have never appeared in the training distribution?_

It should be noticed that the MOR community has low awareness of CDS and current methods struggle to address these challenges. Traditional models dependent on static datasets, fail to capture the variability in practical applications, making them ineffective with new data combinations. This research gap underscores the urgent need for dynamic MOR models that better adapt to the evolving input landscapes seen in real-world deployments.

To address this bottleneck, we formulate the issue as a generalized tensor completion problem, arranging the predictions into a third-order tensor \(}(x,y,:)=^{*}(x,y)^{K}\). Consequently, the prediction of test data can be intuitively interpreted as vector-valued predictions for Missing Not At Random (MNAR)  scenarios in multivariate function (see Eq. (4) for more details). While low-rank tensor decomposition techniques, particularly tensor Singular Value Decomposition (t-SVD) [27; 26; 61], have proven effective in recovering missing entries for discrete data, adapting these methods to the complex domain of possibly continuous multivariate functions requires a fundamental re-establishment of the existing t-SVD framework to address the inherent challenges posed by this transition.

Our contributions.This paper proposes a novel tensor decomposition perspective for multi-output regression under CDS. It analyzes spectral properties that enable handling such shifts, and introduces an algorithm with theoretical guarantees to improve prediction accuracy. The key contributions are:

* To the best of our knowledge, we are the first to identify and define the problem of multi-output regression under CDS, revealing the challenges that arise when the training distribution fails to cover all combinations of input features.
* We propose a theoretical framework of Functional Tensor Singular Value Decomposition (Ft-SVD), which extends the classical t-SVD to infinite and continuous feature domains, providing a natural tool for representing and analyzing multi-output functions.
* Under the Ft-SVD framework, we formulate the multi-output regression problem under CDS as a low-rank tensor estimation problem under the MNAR setting, and introduce a series of assumptions about the true functions, training and testing distributions, and spectral properties of the embeddings, making the problem more tractable.
* We develop a tailored Double-Stage Empirical Risk Minimization (ERM-DS) algorithm that uses specific hypothesis classes in each sub-domain to better capture the spectral decay patterns across different sub-domains, and provide theoretical guarantees for the algorithm's performance under CDS.

Figure 1: An example of MoR under CDS: prediction of multi-disease risk scores for novel feature combinations of physiological parameters (e.g., temperature and blood pressure) that lie outside the training distribution.

Brief related work.Our work is related to the areas of multi-output regression, tensor completion, and learning under distribution shift. Multi-output regression has been studied extensively, with approaches ranging from multi-task learning [13; 6] to shared representation learning , linear models , kernel methods , and neural networks . However, these methods often assume that the training and test data come from the same distribution. Tensor completion has seen advancements with various decomposition techniques [24; 50; 41] and methods for handling non-random missing patterns [54; 14], but they do not consider the specific challenges of multi-output regression under CDS. Various types of distribution shifts, such as covariate shift , concept drift , and domain adaptation , have been studied, with recent work addressing distribution shift in multi-output regression by aligning distributions through invariant representations . Recent functional tensor decomposition advancements include spectral tensor-train for high-dimensional function evaluations , Tucker-neural network for data recovery , Bayesian CP/Tucker approach for uncertainty quantification , guaranteed functional CP decomposition for functional data analysis , and functional Tensor-Train for efficient multivariate function representation . Equivariant disentangled transformation for domain generalization under combination shift was studied through category theory . Simchowitz et al.  proposed the ERM-DS framework for robust learning under CDS. Our work extends the ERM-DS framework to multi-output regression by representing vector-valued functions as embeddings in a Hilbert t-module and leveraging tensor algebra to capture interdependencies among multiple outputs.

Notations.We use lowercase boldface, and uppercase boldface letters to denote vectors, _e.g._, \(^{m}\), and matrices, _e.g._, \(^{m n}\), respectively. A 3-way tensor of size \(1 1 K\) is also named a _t-scalar_, denoted by underlined lowercase, _e.g._, \(\), a 3-way tensor of size \(d 1 K\) is called a _t-vector_ and denoted by underlined lowercase boldface, _e.g._, \(\), whereas a 3-way tensor of size \(m n K\) is also called a _t-matrix_ and denoted by underlined uppercase, _e.g._, \(\). For any tensor \(^{m n K}\), we use \(^{(i)}\) to denote its \(i\)-th frontal slice. For \(^{m m}\), \(_{j}() 0\) denotes its \(j\)-th largest singular value; for symmetric \(\), \(_{j}()\) denotes its \(j\)-th largest eigenvalue.

## 2 Functional t-Singular Value Decomposition for Multi-output Regression

### A t-SVD Perspective of Multi-output Regression

Consider a simpler case where the feature domains \(\) and \(\) are finite sets. Here, we can represent the vector-valued ground truth \(^{}\) as a tensor \(}^{|||| K}\), where each tube \(}(i_{x},j_{y},:)=^{}(x,y) ^{K}\) corresponds to the \(K\) outputs for the input feature combination \((x,y)\) indexed by \((i_{x},j_{y})\). Then, the multi-output regression problem for a new feature combination \((x^{},y^{})\) becomes a tensor completion task, where the goal is to estimate the missing tube of \(}\) at the index \((i_{x^{}},j_{y^{}})\).

To tackle general tensor completion problems, the framework of t-SVD [26; 27] offers an ideal tool [35; 32; 58]. The motivation behind t-SVD stems from the observation that under certain linear transformations \(M\), tensors may exhibit stronger low-rank characteristics than in their original domain. This enhancement of low-rankness often arises due to intrinsic correlations within the data, which these transformations can exploit more effectively. Recent research has focused on using an orthogonal matrix \(\) to define the transform \(M\) due to its advantageous properties [34; 51], a convention that this paper also adopts2. Given an _orthogonal_ matrix \(^{K K}\), we define the associated linear transform \(M()\) and its inverse \(M^{-1}()\) on any tensor \(}^{m n K}\) as follows:

\[M(}):=}_{3},  M^{-1}(}):=}_ {3}^{-1},\] (1)

where \(_{3}\) denotes the mode-3 tensor-matrix product . In real applications, the choice of the transformation matrix \(\) is often guided by the inherent characteristics of the signal being modeled. Popular choices include the Discrete Cosine Transform (DCT) matrix for smooth and periodic signals [34; 57; 32], the Discrete Wavelet Transform (DWT) matrix for multi-resolution analysis , data-dependent transformations that adapt to the dataset's specific characteristics , and graph spectral projection matrices for data structured in non-Euclidean spaces . By selecting a transformation matrix that aligns with the signal properties, t-SVD can more effectively uncover the low-rank structure and enable efficient representation and processing of the data.

Based on the linear transform \(M\), the t-product is specifically defined.

**Definition 1** (t-product ).: _The t-product of tensors \(}^{m n K}\) and \(}^{n k K}\) under the transform \(M\) in Eq. (1) is denoted by \(}_{M}}\) and defined as the tensor \(}^{m k K}\) such that \(M(})=M(}) M(})\) in the transformed domain, where \(\) denotes the tensor frontal-slice-wise product (see Definition 4)._

This paper also follows the definitions of t-transpose, t-identity tensor, t-orthogonal tensor, and f-diagonal tensor given by . Based on these definitions, the t-SVD is introduced as follows:

**Definition 2** (t-SVD, tubal rank ).: _The tensor singular value decomposition (t-SVD) of \(}^{m n K}\) under the transform \(M\) in Eq. (1) is given by:_

\[}=}_{M}} _{M}}^{},\] (2)

_where \(}^{m m K}\) and \(}^{n n K}\) are t-orthogonal tensors, \(}^{m n K}\) is an f-diagonal tensor, and \(()^{}\) denotes the t-transpose. The tubal rank of tensor \(}\) is defined as the number of non-zero tubes in \(}\) in Eq. (2), i.e.,_

\[r_{}(}):=|\{i:}(i,i,:) ,i\{m,n\}\}|.\]

Based on t-SVD, extensive tensor completion models  provide robust support for MOR with discrete combinatorial features. However, in many machine learning settings, feature domains \(\) and \(\) are infinite and potentially continuous sets, posing significant challenges to the traditional t-SVD. Here, t-SVD becomes inapplicable due to the potentially non-discrete nature of these domains, necessitating a novel approach to effectively handle infinite and continuous feature domains. This leads us to consider: _how can we extend the powerful t-SVD framework to address multi-output regression problems in the context of infinite and continuous feature domains?_

### Functional t-Singular Value Decomposition

To address the above challenge, we propose a novel theoretical framework that extends the foundational principles of t-SVD for infinite and continuous feature domains. By introducing a new theorem, we enable the representation of data and functions defined on these domains while preserving the key properties of t-SVD. This extension allows us to employ t-SVD in a principled way for learning vector-valued functions and tackling related problems, opening new possibilities for applying such methods to a wider range of learning tasks.

**Theorem 1** (Functional t-Singular Value Decomposition).: _Let \(F:^{K}\) be a square-integrable vector-valued function with Lipschitz-smooth domains \(^{D_{1}}\) and \(^{D_{2}}\). Then, there exist sets of functions \(\{_{i}\}_{i=1}^{} L^{2}(;^{K})\) and \(\{_{i}\}_{i=1}^{} L^{2}(;^{K})\), and a sequence of t-scalars3\(\{_{i}\}_{i=1}^{}^{K}\) with \(_{i}_{i}=\), satisfying the functional t-Singular Value Decomposition (Ft-SVD):_

\[F(x,y)=_{i=1}^{}_{i}(x)_{M}_{ i}_{M}_{i}(y),\] (3)

_where the convergence is in the \(L^{2}\) sense. The functions \(_{i}\) and \(_{i}\) are called the left and right t-singular functions, respectively, and the t-scalars \(_{i}\) are called the t-singular values. The orthonormality conditions \(_{}_{i}(x)_{M}_{j}(x)dx= _{ij}M^{-1}()\) and \(_{}_{i}(y)_{M}_{j}(y)dy=_{ij}M^{-1}( )\) hold, where \(^{1 1 K}\) is the t-scalar with all entries equal to 1, and \(_{ij}\) is the Kronecker delta._

The proof is provided in Appendix B.2.1. The Ft-SVD theorem provides a principled conceptual framework for decomposing a multivariate function, which maps pairs of inputs from domains \(\) and \(\) into a vector in \(^{K}\). Essentially, this theorem states that such a function can be expressed as an infinite series of products of three components: t-singular functions from \(\) and \(\) (\(_{i}\) and \(_{i}\), respectively), and a series of t-singular values (\(_{i}\)). This decomposition is analogous to breaking down a complex multivariate relationship into simpler, interpretable modes of variation, where each mode is scaled by its importance, signified by the corresponding t-singular value \(_{i}\).

Unlike t-SVD, which factorizes a tensor using a finite number of t-rank-one components, representing the original function exactly using Ft-SVD may require an infinite number of t-rank-one components in Eq. (3). However, in most practical applications, an approximation with a finite number of components is often desired due to computational constraints and the need for a more compact representation. The natural follow-up inquiry is: _how can we achieve a finite approximation of the original function using Ft-SVD_?

To address this question, we can leverage principles akin to the Eckart-Young theorem for t-SVD , which identifies the best low-tubal-rank tensor approximation by minimizing the Frobenius norm error. Building upon these principles, we demonstrate that an optimal rank-\(r\) approximation also exists for functions represented using Ft-SVD (see the proof in Appendix B.2.2):

**Theorem 2** (\(r\)-term truncated Ft-SVD).: _Let \(F:^{K}\) be a square-integrable function with the Ft-SVD given by Theorem 1. For any \(r\), the \(r\)-term truncated Ft-SVD is defined as_

\[F_{r}(x,y):=_{i=1}^{r}_{i}(x)*_{M}_{i}*_ {M}_{i}(y).\]

_Then, \(F_{r}(x,y)\) is the best \(r\)-term approximation to \(F(x,y)\) in the \(L^{2}\) sense within Ft-SVD framework. Moreover, the approximation error is given by: \(\|F(x,y)-F_{r}(x,y)\|_{L^{2}(;^{K})}^{2} =_{i=r+1}^{}\|_{i}\|^{2}\)._

This theorem highlights the optimality of the truncated Ft-SVD in approximating vector-valued functions, making it a potentially useful tool for function compression, denoising, and other applications involving low-rank approximations of functions. However, the error term, an infinite sum \(_{i=r+1}^{}\|_{i}\|_{2}^{2}\), may still be difficult to bound in the worst case. This raises the question: _are there situations where the error terms are well-controlled?_

In many practical scenarios, the inherent spatial or temporal correlations within data often lead to significant smoothness functions, a trait crucial for applications in image processing, machine learning, computer vision, climate modeling, and time series analysis, and fluid dynamics [15; 36; 3; 42; 29]. This smoothness property can be leveraged to show that the \(r\)-term approximation error of Ft-SVD is indeed well-controlled. The following Theorem 3 shows that if the function components belong to a Sobolev space \(H^{s}()\), which captures their smoothness, then the t-singular values of the function decay rapidly. This rapid decay effectively controls the approximation error, keeping it bounded and manageable. The proof can be found in Appendix B.2.3.

**Theorem 3** (Spectral decay of smooth functions).: _Let \(^{D_{2}}\) be a domain satisfying the strong local Lipschitz condition, and let \(F:^{K}\) be a vector-valued function with components \(F^{(i)}(x,y)\), for all \(i[K]\). Suppose there exists a constant \(s>0\) such that \(F^{(i)} L^{2}(,H^{s}())\) for all \(i[K]\), where \(H^{s}()\) denotes the \(s\)-order Sobolev space on \(\). Then, the singular values \(_{i}\) of \(F\) satisfy the polynomial decay rate \(\|_{i}\|^{2} O(i^{-1-}{D_{2}}})\). Moreover, the optimal approximation error of the truncated Ft-SVD with rank \(r\) is upper bounded by \(O((r+1)^{-}{D_{2}}})\) in the \(L_{2}\) sense._

The rapid spectral decay in the Ft-SVD framework, as guaranteed by the Sobolev smoothness assumption on the output functions, has significant implications for the generalization performance of multi-output regression models. By promoting solutions with rapid spectral decay, the Ft-SVD framework can effectively constrain the complexity of the learned function, striking a balance between fitting the training data and maintaining simplicity. The significance of the spectral decay property in the Ft-SVD framework becomes even more apparent when considering multi-output regression under CDS. In the next section, we will delve into how the approximate low-rankness of the multi-output ground truth, as implied by the rapid spectral decay, provides a key insight into the generalization under CDS. Specifically, we will explore how the spectral decay of the ground truth in the Ft-SVD framework plays a crucial role in enabling effective generalization under combinatorial shifts.

## 3 An Ft-SVD-based Framework For Multi-output Regression under CDS

### Generalized Tensor Completion with MNAR for MOR under CDS

We propose a theoretical framework based on Ft-SVD to formulate the CDS problem in multi-output regression. The key idea is to represent multi-output functions as t-bilinear embeddings in a Hilbert t-Module, a generalization of Hilbert spaces for handling vector-valued functions. We introduce a series of assumptions on the ground-truth functions, the training and test distributions, and the spectral properties of the embeddings. These assumptions allow us to characterize the multi-output regression problem under CDS as a low-rank tensor estimation problem under MNAR settings.

To set the stage, we first define the concept of a Hilbert t-Module, which serves as the foundation for our t-bilinear representation.

**Definition 3** (Hilbert t-Module).: _Let \(\) be the ring of \(K\)-dimensional real vectors with t-product. A Hilbert t-Module is a module \(\) over \(\) equipped with an \(\)-valued inner product \(,_{}\), which is complete with respect to the \(_{2}\)-norm induced by the \(\)-valued inner product.4_

Intuitively, a Hilbert t-Module extends the classical Hilbert space to accommodate vector-valued functions, allowing us to perform inner products and norm calculations in a way that respects the tensor structure. This provides a natural framework for representing multi-output functions. With the Hilbert t-Module in place, we can now state our key assumption on the ground-truth functions:

**Assumption 1** (t-Bilinear representation).: _We have the following assumptions on the ground truth:_

1. _There is a Hilbert t-Module_ \((,,_{})\) _and two embeddings_ \(}^{}:\) _and_ \(}^{}:\) _satisfying that_ \(^{}(x,y):=}^{}(x),^{}(y)_{}\) _is the Bayes optimal predictor on_ \(_{}\) _and_ \(_{}\)_, i.e.,_ \(_{_{}}[}|x,y]= _{_{}}[}|x,y]= ^{}(x,y)\)_. Hereafter, we call_ \(^{}(x,y)\) _the ground truth._
2. _We also assume that for some_ \(B>0\)_, such that_ \(_{(x,y,)_{}}[\|}\|^{2} B^{2}]=1\)_, and_ \(\{_{x}\|}^{}(x)\|_{ },\,_{y}\|}^{}(y)\|_{ }\} B\)_._

This assumption postulates that the ground-truth functions admit a t-bilinear representation in terms of two embeddings in a Hilbert t-Module, and the inner product of these embeddings gives the Bayes optimal predictor for both the training and testing distributions. The t-bilinear form is a natural extension of the bilinear form in Ref. , and it allows us to capture the multi-dimensional structure of the problem. While the t-bilinear representation is expressive, we need additional assumptions on the training and test distributions to make the problem tractable under CDS:

**Assumption 2** (Coverage of training and test distribution, Assumption 2.2 in Ref. ).: _There exist constants \(_{},_{}>0\) and marginal distributions \(_{,1},_{,2}\) over \(\), and \(_{,1},_{,2}\) over \(\), with product measures \(_{i j}:=_{,i}_{ ,j}\), such that for all \((x,y)\), the following Radon-Nikodym derivative conditions hold: (I) Training coverage: \(_{}(x,y)}{_{ }(x,y)}_{}\) for \((i,j)\{(1,1),(1,2),(2,1)\}\), and (II) Test coverage: \(_{}(x,y)}{_{i,j\{1,2\}}_{}(x,y)}<_{}\)._

In words, this assumption requires that the training distribution covers the key feature combinations in \(_{1 1},_{1 2}\) and \(_{2 1}\), while the test distribution is allowed to include unseen combinations in \(_{2 2}\). The constants \(_{},_{}\) quantify the degree of coverage5.

While Assumption 2 characterizes the relationship between the training and test distributions, it does not directly control the impact of distribution shifts on the model's performance. For this, we introduce a further assumption on the covariate shift:

**Assumption 3** (Controlled covariate shifts).: _There exists a \(_{} 1\) such that, for any \(}\), the following inequalities hold: \(_{x_{,2}}[\|(}^{ }(x),})_{}\|^{2}]_{ }_{x_{,1}}[\|(} ^{}(x),})_{}\|^{2}]\) and \(_{y_{,2}}[\|(}^{ }(y),})_{}\|^{2}]_{ }_{y_{,1}}[\|(} ^{}(y),})_{}\|^{2}]\)._

This assumption can be seen as a t-Module variant of Assumption 2.3 in Ref. . It essentially bounds the worst-case impact of covariate shift on the model's performance, ensuring that the error on the unseen distribution \(_{2 2}\) is controlled by the error on the training distribution (up to a constant \(_{}\))6. This is a key ingredient that allows us to provide generalization guarantees under CDS.

Finally, to leverage the spectral structure of the embeddings, we make the following assumptions:

**Assumption 4** (Polynomial spectral decay).: _Consider the t-covariances \(}}_{^{}}:=_{ _{,1}}[}^{}*_{M}(}^{})^{}]\) and \(}}_{^{}}:=_{ _{,1}}[}^{}*_{M}(}^{})^{}]\). We have the following assumptions:_1. _Balanced embeddings: The ground truth embeddings_ \(^{}\) _and_ \(^{}\) _reside in an appropriate basis of_ \(\) _such that_ \(}}_{^{}}=}}_{^{}}=:}}_{1 1}^{}\)_; we also assume_ \(_{1}(M(}}_{1 1}^{})^{(i)})>0\) _holds for all_ \(i[K]\)_._
2. _Polynomial spectral decay: Each of the_ \(K\) _frequency components_ \(M(}}_{1 1}^{})^{(i)}\) _of the t-covariance_ \(}}_{1 1}^{}\) _exhibits a polynomial singular value decay pattern, but potentially with different decay rates_ \(_{i}>0\)_, i.e.,_ \(_{j}(M(}}_{1 1}^{})^{(i)})  Cj^{-(1+_{i})}, j, i[K]\)_._

**Assumption 5** (Small low-rank approximation error of \((^{},^{})\) on \(_{}\)).: _Let \(_{k}^{(i)}\) denote the projection onto the top-\(k\) eigenspace of \(M(}}_{1 1}^{})^{(i)}\), which represents the \(i\)-th frequency component \(( i[K])\) of \(}}_{1 1}^{}\) induced by the operator \(M()\) in Eq. (1). Define \(_{k}^{(i)}(x,y):=(M(^{}(x,y))^{(i)}- _{k}^{(i)}M(^{}(x))^{(i)},_{k}^{(i)}M( ^{}(y))^{(i)})^{2}\) as the error between the ground truth \(^{}(x,y)\) and the optimal rank-\(k\) approximations of the ground truth embeddings \((^{},^{})\) over the training data \(_{}\) for all rank \(k\) in each \(i\)-th frequency component. We assume that_

\[_{_{}}[_{k}^{(i)}(x,y)] _{}_{_{1 1}}[_{k}^{(i )}(x,y)],\; i[K].\]

Assumptions 4 and 5 postulate that the ground-truth embeddings have a favorable spectral structure. Specifically, they assume that the embeddings are balanced in an appropriate basis, and their spectrum decays polynomially (possibly at different rates for different frequency components). Fig. 2 illustrates the varying rates of spectral decay exhibited by the DCT frequency components of the _Akiyo_ video data7. This empirical observation serves as a compelling motivation for Assumption 4-(II). Assumption 5 implies that the training distribution \(_{}\) is sufficiently representative of the true function \(^{}\) in each frequency component. It guarantees that if we find embeddings that well approximate \(^{}\) on the training data, they will also perform well on the reference distribution \(_{1 1}\).

**Formulating MOR under CDS as generalized tensor completion with MNAR.** Based on the assumptions, we can formulate the MOR problem under CDS as a tensor completion problem under the MNAR setting. Specifically, given the training data \(\{(x_{i},y_{i},_{i})\}_{i=1}^{n}\) drawn from \(_{}\), we aim to estimate the underlying embeddings \(^{}\) and \(^{}\) by solving the following problem:

\[_{,^{}} _{i=1}^{n}(((x_{i}),}(y_{i}))_{ },_{i}),\] (4)

where \((,)\) is a suitable loss function. Problem (4) can be interpreted as a tensor completion problem with MNAR: (1) The t-bilinear form \((^{}(x),^{}(y))_{}\) encodes the function \(^{}(x,y)\) as a (generalized) tensor. (2) The training data only covers a subset of the feature combinations due to Assumption 2. (3) The goal is to estimate the complete tensor from the partially observed tensor entries. By solving Problem (4), we obtain the estimated embeddings \(}\) and \(}}\), which can be used to make predictions on a new test point \((x,y)\) via the t-bilinear form \(}(x),}}(y)_{}\).

### Algorithms for Robust Generalization of Multi-output Regression under CDS

**How does ERM-based multi-output regression perform under CDS?** A natural approach to training a multi-output regression model is to fix a target rank \(r\) and compute an Empirical Risk

Figure 2: An illustration of the varying rates of spectral decay across different frequency components following Discrete Cosine Transform (DCT). For this example, we consider a discrete tensor \(}^{240 320 5}\), which comprises the initial five frames of the reshaped _Akiyo_ video sequence. This tensor serves as an instance of \(^{}\) in Assumption 4-(II), with the transform \(M\) represented by the DCT operation.

Minimizer (ERM) by

\[(}_{},}_{}) _{r},_{r}}{}_{i=1}^{n}\|(x_{i})^{}_{M}}(y_{i} )-_{i}\|^{2},\] (5)

where \(()^{}\) denotes the t-transpose (see Definition 6), and function classes \(_{r},_{r}\) are given in Assumption 8. Assumption 8 ensures that the function classes \(_{r}\) and \(_{r}\) are sufficiently expressive to approximate the true embeddings \(^{*}\) and \(}^{*}\), while also having controlled complexity in terms of their covering numbers. With the assumption in place, we can obtain the following generalization guarantee for the ERM solution (see the proof in Appendix D.2):

**Theorem 4** (Excess Risk Bound for ERM under CDS).: _Suppose the learned embeddings \((}_{},}_{})\) are \(\)-conditioned and \((_{},_{D_{1 1}})\)-accurate embeddings satisfying8\(}_{_{1 1}}^{(i)}_{1}^{*,(i)}/(40r)\) for all \(i[K]\). Then, under Assumptions 1 to 5 and 8, the excess risk of the ERM solution on \(_{}\) can be bounded up to a constant factor \(c=(_{},_{},_{})\) with probability at least \(1-\):_

\[ r^{2}_{i}(_{r}^{*,(i)})^{2}+r^{4}_{2}^{*}(r)+r^{2}(_{1}^{*}(r))^{2}+ _{2}^{*}(r)^{2}}{^{2}}+_{n}+}{^{2}}_{n}^{2}}_{},\]

_where \(:=_{i}\{_{i}\}\), \(:=_{i}\{_{r}^{*,(i)}\}>0\), \(_{q}^{*}(r):=_{i=1}^{K}_{j>r}(_{j}^{*,(i)})^{ q},\)\(q 1\), \(_{n}=B^{4}((r,2B/n)+)/n\) with \((r,)=(_{r},/(2B),\|\|_{ })(_{r},/(2B),\|\|_{}).\) Here, \(_{j}^{*,(i)}:=_{j}(M(}_{ 1 1}^{*})^{(i)})\) denotes the \(j\)-th largest singular value of the \(i\)-th frequency component \(M(}_{1 1}^{*})^{(i)}\) of the t-covariance operator \(}_{1 1}^{*}\), and \((,,\|\|_{})\) denotes the covering number of a function class at scale \(\)._

Extending the framework of Ref. , this theorem bounds the excess risk of the ERM solution for multi-output regression under CDS, incorporating both approximation and statistical errors. Our key contribution, the Ft-SVD for infinite-dimensional tensor completion, addresses multi-output MOR while considering spectral decay across frequency components. However, Theorem 4 has two primary limitations: (1) It assumes that the learned embeddings are \(\)-conditioned and \((_{},_{D_{1 1}})\)-accurate, which may not be achievable with ERM. (2) The theorem requires that the minimum singular value \(_{r}^{*,(i)}\) is strictly positive for each frequency component \(i[K]\). This condition may not hold for t-embeddings that exhibit varying spectral decay patterns across different frequency components. In practice, multi-output ground truths often demonstrate diverse spectral decay patterns, making it challenging for the hypothesis classes \(_{r}\) and \(_{r}\) to capture precise low-rank structures across different frequency components (See Fig. 2 for example).

**ERM-DS: Addressing the limitations of single-stage ERM.** To address the weaknesses of single-stage ERM, we propose the Double-Stage Empirical Risk Minimization (ERM-DS) algorithm. ERM-DS uses a two-stage training process with hypothesis classes tailored to each frequency component, better capturing varying spectral decay patterns and balancing model complexity with generalization ability. Our work extends the ERM-DS framework, originally proposed by Simchowitz et al. for robust learning under CDS , to address the challenging setting of multi-output regression. By representing vector-valued functions as embeddings in a Hilbert t-module and employing tensor algebra, we capture the complex interdependencies among multiple outputs, enabling robust generalization.

Unlike Assumption 8, the ERM-DS algorithm considers _fine-grained_ hypothesis classes satisfying Assumption 9, which defines function classes specifically for each frequency component to better capture the distinct decay behaviors exhibited by the ground truth in each frequency component. This allows ERM-DS to adapt to the varying spectral properties of the true embeddings across different frequency components, leading to improved generalization performance9. The ERM-DS algorithm consists of four main steps:1. **Overparameterized Training**: Train an overparameterized model \((},})\) to approximate the unknown true predictive functions \(^{*}\) and \(^{*}\) by choosing their frequency components separately via \(K\) parallel ERM sub-problems (Eq. (18)).
2. \(\)**-Covariance Estimation**: Estimate the t-covariances of the embeddings using additional unlabeled examples to capture the important directions of variation (Eq. (19)).
3. **Dimension Reduction**: Compute low-rank projections using the estimated covariances and obtain reduced-rank embeddings by projecting the overparameterized embeddings onto the low-rank t-subspace (Eq. (20)).
4. **Distillation**: Fine-tune the reduced-rank embeddings by approximating their frequency components separately in the transformed domain, minimizing a combination of empirical risk and consistency with the reduced-rank embeddings (Eq. (21)).

The theoretical guarantees for the performance of the ERM-DS algorithm are provided in Theorem 5:

**Theorem 5** (Excess Risk Bound for ERM-DS under CDS).: _Under appropriate conditions on algorithm parameters and sample sizes10, for any \(>0\), the ERM-DS solution achieves an excess risk bound of \(c^{}(^{2}+_{i=1}^{K}(1+_{i}^{-2})r_{i,}^{-2 _{i}})\) with probability at least \(1-\). Here, \(c^{}=(_{},_{},_ {})\) is a problem-dependent constant, \(>0\) is the desired accuracy level, \(\{r_{i,}\}_{i=1}^{K}\) are cutoff rank parameters in ERM-DS, and \(\{_{i}\}_{i=1}^{K}\) are parameters related to spectral decay in Assumption 4._

The proof is given in Appendix D.3. Theorem 5 provides a generalization error bound for the ERM-DS algorithm in multi-output regression under CDS. The expected squared error between predicted and true outputs can be bounded by two main terms: the desired accuracy level \(^{2}\), adjustable by algorithm parameters and a term depending on the spectral decay properties of the true embeddings in each frequency component, represented by \(_{i=1}^{K}(1+_{i}^{-2})r_{i,}^{-2}\), which captures approximation error due to low-rank structure and target function complexity.

Numerical Experiments.As the first theoretical work on the MOR problem under CDS, this paper proposes the novel Ft-SVD theoretical framework, along with related assumptions and algorithm design. The experiments conducted serve solely as a conceptual validation using synthetic data. We consider the settings when \(\) and \(\) are finite, in which case MOR under CDS naturally degenerates to tensor completion with MNAR tubes. We construct a ground truth tensor \(}\) composed of factor tensors11\(}_{1},}_{1},}_{2}, }_{2}\), where \((}_{2})(}_{1})\) and \((}_{2})(}_{1})\). The singular values of \(M(}_{1})^{(i)},M(}_{1})^{(i)}\) follow a power-law decay \(_{j}^{}=j^{-(1+)/2}\), while those of \(M(}_{2})^{(i)},M(}_{2})^{(i)}\) are set to \(_{j}^{}=c_{j}_{j}^{}\), where \(c_{j}(0,1)\) is a uniform random variable, \(\) controls the covariate shift, and the transform \(M()\) is chosen as DFT. To simulate CDS, we perform two-stage sampling on \(}\): first, we randomly sample entries with rate \(_{}\); second, we sample the top block \(}_{11}\) (\(}_{1},}_{1}\)) with a lower rate \(_{11}\), leaving the bottom block \(}_{22}\) (\(}_{2},}_{2}\)) as the unobserved test set. We compare the proposed ERM-DS algorithm with the single-stage one, evaluating their test risks under different CDS intensities (by varying the \(\) parameter that controls the covariate shift) and different percentages of training data. As shown in Fig. 3, the ERM-DS algorithm consistently outperforms the single-stage ERM across all settings. On the left, we observe that as \(\) increases, indicating more severe covariate shift, the test risk of both algorithms rises, but the ERM-DS algorithm maintains a significant advantage over the single-stage ERM, with the performance gap widening for larger \(\) values. On the right, the results demonstrate that the ERM-DS algorithm achieves lower test risks compared to the single-stage ERM under varying training data sizes, highlighting its robustness even with limited training samples. Overall, Fig. 3 shows the ERM-DS algorithm achieves lower test risks in handling the CDS problem across different covariate shift intensities and training data availabilities.

## 4 Extensions, Conclusion and Limitation

**Extension to higher-order Ft-SVD.** Our Ft-SVD framework, while designed for 3-order tensors, is not strictly limited to two-dimensional input cases. It is versatile enough to handle multi-dimensionalinputs that can be divided into two distinct sets. Extending Ft-SVD to higher-order cases is indeed non-trivial and we show a preliminary conjecture of a higher-order extension motivated by Ref. :

Let \(F:_{i=1}^{N}X_{i}^{K}\) be a square-integrable vector-valued function12, where \(X_{i}^{D_{i}}\) for \(i=1,,N\). Then there exist sets of functions \(\{U_{i}^{n}\}_{i=1}^{} L^{2}(X_{n};^{K})\) for each \(n=1,,N\), and a core function \(S:_{i=1}^{N}^{K}\), such that \(F\) can be represented as \(F(x_{1},,x_{N})=_{i_{1},,i_{N}=1}^{}S(i_{1},,i_{N}) _{M}_{n=1}^{N}U_{i_{n}}^{n}(x_{n})\). Here, \(_{*}\) denotes the sequential t-product, which applies the t-product operation sequentially to the functions \(U_{i_{n}}^{n}(x_{n})\). The convergence of this infinite sum is in the \(L^{2}\) sense. We also have

* Orthogonality: \( n[N]\), the functions \(\{U_{i}^{n}\}\) satisfy: \(_{X_{n}}U_{i}^{n}(x)_{M}(U_{j}^{n}(x))^{}dx=_{ij}M^{-1}( )\).
* Core properties: The core function \(S\) has two key characteristics: 1. All-orthogonality: For all \(1 n N\) and all \(\), we have \(_{_{i n}X_{i}}S(,,)^{}_{M}S(, ,)_{i n}dx_{i}=\). This means that slices of the core tensor are t-orthogonal. 2. Ordering: For all \(n=1,,N\), we have \(\|S_{x_{n}=1}\|_{L^{2}}\|S_{x_{n}=2}\|_{L^{2}}\), where \(\|S_{x_{n}=}\|_{L^{2}}\) denotes the \(L^{2}\) norm of \(S\) with its \(n\)-th mode fixed at \(\). This property ensures a unique ordering of the components.

This decomposition generalizes the concept of Ft-SVD to multilinear functions. It is interesting to generalize the various higher-order variants of t-SVD  to functional settings.

**Conclusion.** The paper addresses the challenge in multi-output regression where training distribution does not cover all input feature combinations, leading to CDS. We introduce a new methodology within a generalized tensor decomposition framework, named Ft-SVD, to tackle this challenge by treating the problem as a tensor completion task under the missing-not-at-random setting. The paper highlights the role of spectral decay of the true embeddings in enhancing model generalization and, through detailed analysis, establishes how multi-output models can manage combinatorial shifts, improving prediction accuracy for new and unseen input combinations.

**Limitation.** This paper introduces a tensor spectral theory framework to address MOR under CDS, marking an early theoretical exploration in this field. However, several limitations are recognized. First, spectral methods may not fully capture the complexity of real-world data, and the robustness of controlled experimental results remains uncertain. We encourage future research to refine these methods, aiming to develop more effective solutions. Furthermore, an open problem remains in extending the framework to accommodate more than two combinatorial features. Initial investigations suggest that the approach in Ref.  does not readily generalize to cases with more than two feature combinations, potentially requiring new mathematical tools to address this challenge.

Figure 3: Test risk under different experimental settings. (left) Comparison of test risk over \(\) (covariate shift intensity) for single and double training approaches. (right) Test risk over the percentage of training data for ERM and ERM-DS.