# On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective

Mathieu Serrurier\({}^{1}\) &Franck Mamalet\({}^{2}\) &Thomas Fel\({}^{3,4}\) &Louis Bethune\({}^{1}\)

&Thibaut Boissin\({}^{2}\)

\({}^{1}\)Universite Paul-Sabatier, IRIT, Toulouse, France

\({}^{2}\)Institut de Recherche Technologique Saint-Exupery, Toulouse, France

\({}^{3}\)Carney Institute for Brain Science,Brown University, USA

\({}^{4}\)Innovation & Research Division, SNCF, France

###### Abstract

Input gradients have a pivotal role in a variety of applications, including adversarial attack algorithms for evaluating model robustness, explainable AI techniques for generating Saliency Maps, and counterfactual explanations. However, Saliency Maps generated by traditional neural networks are often noisy and provide limited insights. In this paper, we demonstrate that, on the contrary, the Saliency Maps of 1-Lipschitz neural networks, learned with the dual loss of an optimal transportation problem, exhibit desirable XAI properties: They are highly concentrated on the essential parts of the image with low noise, significantly outperforming state-of-the-art explanation approaches across various models and metrics. We also prove that these maps align unprecedentedly well with human explanations on ImageNet. To explain the particularly beneficial properties of the Saliency Map for such models, we prove this gradient encodes both the direction of the transportation plan and the direction towards the nearest adversarial attack. Following the gradient down to the decision boundary is no longer considered an adversarial attack, but rather a counterfactual explanation that explicitly transports the input from one class to another. Thus, Learning with such a loss jointly optimizes the classification objective and the alignment of the gradient, i.e. the Saliency Map, to the transportation plan direction. These networks were previously known to be certifiably robust by design, and we demonstrate that they scale well for large problems and models, and are tailored for explainability using a fast and straightforward method.

## 1 Introduction

The Lipschitz constant of a function expresses the extent to which the output may vary for a small shift in the input. As a composition of numerous functions, the Lipschitz constant of a neural network can be arbitrarily high, particularly when trained for a classification task . Adversarial attacks  exploit this weakness by selecting minor modifications, such as imperceptible noise, for a given example to change the predicted class and deceive the network. Consequently, Saliency Maps  - gradient of output with respect to the input -, serve as the basis for most adversarial attacks and often highlight noisy patterns that fool the model instead of meaningful modifications, rendering them generally unsuitable for explaining model decisions. Therefore, several methods requiring more complex computations, such as SmoothGrad , Integrated Gradient , or GradCAM , have been proposed to provide smoother explanations. Recently, the XAI community has investigated the link between explainability and robustness and proposed methods and metricsaccordingly . However, the reliability of those automatic metrics can be compromised by artifacts introduced by the baselines , and there is no conclusive evidence demonstrating their correlation with the human understanding of explanations. To address this, a study by  suggests completing those metrics with the alignment between attribution methods and human feature importance using the ClickMe dataset .

In , authors propose to address the weakness with respect to adversarial attacks by training 1-Lipschitz constrained neural networks with a loss that is the dual of an optimal transport optimization problem, called hKR (Eq. 1). The resulting models have been shown to be robust with a certifiable margin. We refer to these networks as Optimal Transport Neural Networks (OTNN) hereafter.

In this paper, we demonstrate that OTNNs exhibit valuable explainability properties. Our experiments reveal that OTNN Saliency Maps significantly outperform various attribution methods for unconstrained networks across all tested state-of-the-art Explainable XAI metrics. This improvement is consistent across toy datasets, large image datasets, binary, and multiclass problems. Qualitatively, OTNN Saliency Maps concentrate on crucial image regions and generate less noise than maps of unconstrained networks, as illustrated in Figure 1. Figure 1.c presents the Saliency Maps of an OTNN based on ResNet50 alongside its unconstrained vanilla counterpart, both trained on ImageNet . In the unconstrained case, the Saliency Maps appear sparse and uninformative, with the most critical pixels often located outside the subject. Conversely, the OTNN Saliency Map is less noisy and highlights significant image features. This distinction is emphasized in Figure 1.d), comparing the feature visualization of the two models. Feature visualization extracts the inverse prototypical image for a given class using gradient ascent . The results for vanilla ResNet are noticeably noisy, making class identification difficult. In contrast, feature visualization with OTNN yields clearer results, displaying easily identifiable features and classes (e.g., goldfish, butterfly, and medusa). Furthermore, modifying the image following the gradient direction provides an interpretable method for altering the image to change its class. Pictures 1.a) and 1.b) display the original image, the gradient direction, and the transformation following the gradient direction (refer to Section 4 for details). We observe explicit structural changes in the images, transforming them into an example of another class in both multiclass (MNIST) and high-resolution multi-labels cases (e.g., smile/not smile and blond hair/not blond hair classification). Lastly, a large-scale human experiment demonstrates that these maps are remarkably aligned with human attribution on ImageNet (Fig. 4).

We provide a theoretical justification for the well-behaved OTNN Saliency Maps. Building upon the fact that OTNNs encode the dual formulation of the optimal transport problem, we prove that the gradient of the optimal solution at a given point \(\) is both (i) in the direction of the nearest adversarial example on the decision boundary, and (ii) in the direction of the image of \(x\) according to the underlying transport plan. This implies that adversarial attacks for an OTNN are equivalent to traversing the optimal transport path which can be achieved by following the gradient. Consequently, the resulting modification serves both as an adversarial attack and a counterfactual explanation, explaining why the decision was A and not B . An optimal transport plan between two classes can be interpreted as a global approach for constructing counterfactuals, as suggested in . These counterfactuals may not correspond to the smallest transformation for a given input sample but rather the smallest transformation on average when pairing points from two classes. A consequence of this property is that the Saliency Map of an OTNN for an image indicates the importance of each pixel in the modifications needed to change the class. It is worth noting that several methods based on GAN  or causality penalty  produce highly realistic counterfactual images. However, the objective of our paper is not to compete with the quality of these results, but rather to demonstrate that OTNN Saliency Maps possess both theoretical and empirical foundations as counterfactual explanations.

We summarize our contributions as follows: first, after introducing the background on OTNN and XAI, we establish several properties of the gradient of an OTNN with respect to adversarial attacks, decision boundaries, and optimal transport. Second, we establish that the optimal transport properties of OTNN's gradient lead to a reinterpretation of adversarial attacks as counterfactual explanations, consequently endowing the Saliency Map with the favorable XAI properties inherent in these explanations. Third, our experiments support the theoretical results, showing that metric scores are higher for most of the XAI methods on OTNN compared to unconstrained neural networks. Additionally, we find that the Saliency Map for OTNN achieves top-ranked scores on XAI metrics compared to more sophisticated XAI methods, and is equivalent to Smoothgrad. Lastly, drawing from , we emphasize that OTNNs are naturally and remarkably aligned with human explanations, and we present several examples of gradient-based counterfactuals obtained with OTNNs.

## 2 Related work

**1-Lipschitz Neural network and optimal transport:** Consider a classical supervised machine learning binary classification problem on \((,,)\) - the underlying probability space - where \(\) is the sample space, \(\) is a \(\)-algebra on \(\), and \(\) is a probability measure on \(\). We denote the input space \(^{d}\) and the output space \(=\{ 1\}\). Let input data \(:\) and target label \(y:\) are random variables with distributions \(P_{}\), \(P_{y}\), respectively. The joint random vector \((,y)\) on \((,)\) has a joint distribution \(P\) defined over the product space \(\). Moreover, let \(=P(|y=1)\) and \(=P(|y=-1)\) the conditional probability distributions of \(\) given the true label. We assume that the supports of \(\), \(\) and \(\) are compact sets.

A function \(:\) is a 1-Lipschitz functions over \(\) (denoted \(Lip_{1}()\)) if and only if \((,)^{2},\|()-()\| \|-\|\). 1-Lipschitz neural networks have received a lot of attention, especially due to the link with adversarial attacks. They provide certifiable robustness guarantees [32; 49], improve the generalizations  and the interpretability of the model . The simplest way to constrain a network to be in \(Lip_{1}()\) is to impose this 1-Lipschitz property to each layer. Frobenius normalization , or spectral normalization  can be used for linear layers, and can also be extended, in some situations, to orthogonalization [41; 1; 66; 6].

Optimal transport, 1-Lipschitz neural networks, and binary classification were first associated in the context of Wasserstein GAN (WGAN) . The discriminator of a WGAN is the solution to the Kantorovich-Rubinstein dual formulation of the 1-Wasserstein distance , and it can be regarded as a binary classifier with a carefully chosen threshold. Nevertheless, it has been demonstrated in  that this type of classifier is suboptimal, even on a toy dataset. In the same paper, the authors address the suboptimality of the Wasserstein classifier by introducing the hKR loss \(^{hKR}\), which adds a hinge regularization term to the Kantorovich-Rubinstein optimization objective :

\[^{hKR}_{,m}()=}_{} [()]-}_{}[( )]+}_{(,y) P}(m-y( ))_{+}\] (1)

Figure 1: **Illustration of the beneficial properties of OTNN gradients.** Examples **a)** and **b)** show that the gradients naturally provide a direction that enables the generation of adversarial images - a theoretical justification based on optimal transport is provided in Section 3. By applying the gradient \(^{}=-t_{}()\) to the original image \(\) (on the left), any digit from MNIST can be transformed into its counterfactual \(^{}\) (e.g., turning a 0 into a 5). In **b)**, we illustrate that this approach can be applied to larger datasets, such as Celeb-A, by creating two counterfactual examples for the closed-mouth and blonde classes. In **c)**, we compare the Saliency Map of a classical model with those of OTNN gradients, which are more focused on relevant elements. Finally, in **d)**, we show that following the gradients of OTNN could generate convincing feature visualizations that ease the understanding of the modelâ€™s features.

where \(m>0\) is the margin, and \((z)_{+}=max(0,z)\). We note \(^{*}\) the optimal minimizer of \(_{,m}^{hKR}\). The classification is given by the sign of \(^{*}\). In the following, the 1-Lipschitz neural networks that minimize \(_{,m}^{hKR}\) will be denoted as OTNN. Given a function \(\), a classifier based on \(()\) and an example \(\), an adversarial example is defined as follows:

\[adv(,)=*{arg\,min}_{} -\ \ s.t.\ \ (())(()).\] (2)

Since \(^{*}\) is a 1-Lipschitz function, \(|^{*}()|\) is a certifiable lower bound of the robustness of the classification of \(\) (i.e. \(,|^{*}()|||-adv(^{*}, )||\)). The function \(^{*}\) has the following properties  (_i_) if the supports of \(\) and \(\) are disjoints (separable classes) with a minimal distance of \(>0\), then for \(m<2\), \(f^{*}\) achieves 100% accuracy; (_ii_) minimizing \(^{hKR}\) is still the dual formulation of an optimal transport problem (see appendix for more details).

**Explainability and metrics:** Attribution methods aim to explain the prediction of a deep neural network by pointing out input variables that support the prediction - typically pixels or image regions for images - which lead to importance maps. Saliency  was the first proposed white-box attribution method and consists of back-propagating the gradient from the output to the input. The resulting absolute gradient heatmap indicates which pixels affect the most the decision score. However, this family of methods suffers from problems inherent to the gradients of standard models. Methods such as Integrated Gradient  and SmoothGrad  partially address this issue by accumulating gradients, either along a straight interpolation path from a baseline state to the original image or from a set of points close to the original image obtained after adding noise but multiply the computational cost by several orders of magnitude. These methods were then followed by a plethora of other methods using gradients such as Grad-cam  or Input Gradient . All rely on the gradient calculation of the classification score. Finally, other methods - sometimes called black-box attribution methods - do not involve the gradient and rely on perturbations around the image to generate their explanations .

However, it is becoming increasingly clear that current methods raise many issues  such as confirmation bias: it is not because the explanations make sense to humans that they reflect the evidence of the prediction. To address this challenge, a large number of metrics were proposed to provide objective evaluations of the quality of explanations. Deletion and Insertion methods  evaluate the drop in accuracy when important pixels are replaced by a baseline. \(\)Fidelity method  evaluates the correlation between the sum of importance scores of pixels and the drop of the score when removing these pixels. In parallel, a growing literature relies on model robustness to derive new desiderata for a good explanation . In addition,  showed that some of these metrics also suffer from a bias due to the choice of the baseline value and proposed a new metric called Robustness-Sr. This metric assesses the ease to generate an adversarial example when the attack is limited to the important variables proposed by the explanation. Other metrics consider properties such as generalizability, consistency , or stability  of explanation methods. A recent approach  aims to evaluate the alignment between attribution methods and human feature importance across 200,000 unique ImageNet images (called ClickMe dataset). The alignment between DNN Saliency and human explanations is quantified using the mean Spearman correlation, normalized by the average inter-rater alignment of humans.

These works on explainability metrics have also initiated the emergence of links between the robustness of models and the quality of their explanations . In particular, claimed that 1-Lipschitz networks explanations have better metrics scores. But this study was not on OTNNs and was limited to their proposed metrics.

To end with, recent literature is focusing on counterfactual explanations  methods, providing information on "why the decision was A and not B". Several properties are desirable for these counterfactual explanations: Validity (close sample and in another class), Actionability, Sparsity, Data Manifold closeness, and Causality. The three last properties are generally not covered by standard adversarial attacks and complex methods have been proposed . Since often a causal model is hard to fully-define, recent papers  have proposed a definition of counterfactual based on optimal transport easier to compute and that can sometimes coincide with causal model based ones. We will rely on this theoretical definition of counterfactuals.

Theoretical properties of OTNN gradient

In this section, we extend the properties of the OTNNs to the explainability framework, all the proofs are in the appendix A.1. We note \(\) the optimal transport plan corresponding to the minimizer of \(_{,m}^{NKR}\). In the most general setting, \(\) is a joint distribution over \(,\) pairs. However when \(\) and \(\) admit a density function  with respect to Lebesgue measure, then the joint density describes a deterministic mapping, i.e. a Monge map. Given \(\) (resp. \(\)) we note \(=_{}()\) (resp. \(\)) the image of \(\) with respect to \(\). When \(\) is not deterministic (on real datasets that are defined as a discrete collection of Diracs), we take \(_{}()\) as the point of maximal mass with respect to \(\).

**Proposition 1** (Transportation plan direction): _Let \(^{*}\) an optimal solution minimizing the \(_{,m}^{NKR}\). Given \(\) (resp. \(\)) and \(=_{}()\), then \( t 0\) (resp. \(t 0\)) such that \(_{}()=-t_{}^{*}()\) almost surely._

This proposition also holds for the Kantorovich-Rubinstein dual problem without hinge regularization, demonstrating that for \( P\), the gradient \(_{}^{*}()\) indicates the direction in the transportation plan almost surely.

**Proposition 2** (Decision boundary): _Let \(\) and \(\) two distributions with disjoint supports with minimal distance \(\) and \(^{*}\) an optimal solution minimizing the \(_{,m}^{NKR}\) with \(m<2\). Given \( P\), \(_{}=-^{*}()_{}^{*}() \) where \(=\{^{}|^{*}(^{ })=0\}\) is the decision boundary (i.e. the 0 level set of \(^{*}\))._

Experiments suggest this probably remains true when the supports of \(\) and \(\) are not disjoint. Prop. 2 proves that for an OTNN \(\) learnt by minimizing the \(_{,m}^{NKR}\), \(|()|\) provides a tight robustness certificate. A direct consequence of 2, is that \(t\) defined in 1 is such that \(|t||^{*}()|\).

**Corollary 1**: _Let \(\) and \(\) two separable distributions with minimal distance \(\) and \(^{*}\) an optimal solution minimizing the \(_{,m}^{NKR}\) with \(m<2\), given \( P\), \(adv(^{*},)=_{}\) almost surely, where \(_{}=-^{*}()_{}^{*}()\)._

This corollary shows that adversarial examples are precisely identified for the classifier based on \(_{,m}^{NKR}\): the direction given by \(_{}^{*}()\) and distance by \(|^{*}()|*||_{}^{*}()||=|^{*}()|\). In this scenario, the optimal adversarial attacks align with the gradient direction (i.e., FGSM attack ). This supports the observations made in , where all attacks, such as PGD  or Carlini and Wagner , applied on an OTNN model, were equivalent to FGSM attacks.

To illustrate these propositions, we learnt a dense binary classifier with \(_{,m}^{NKR}\) to separate two complex distributions, following two concentric Koch snowflakes. Fig.2-a shows the two distributions (blue and orange snowflakes), the learnt boundary (\(0\)-levelset) (red dashed line). Fig.2-b,c show for random samples \(\) from the two distributions, the segments \([,_{}]\) where \(_{}\) is defined in Prop. 2. As expected by Prop. 2, \(_{}\) points fall exactly on the decision boundary. Besides, as stated in Prop. 1 each segment provides the direction of the image with respect to the transport plan.

Finally, we showed that with OTNN, adversarial attacks are formally known and simple to compute. Furthermore, since we proved that these attacks are along the transportation map, they acquire a meaningful interpretation and are no longer mere adversarial examples exploiting local noise, but rather correspond to the global solution of a transportation problem.

## 4 Link between OTNN gradient and counterfactual explanations

The vulnerability of traditional networks to adversarial attacks indicates that their decision boundaries are locally flawed, deviating significantly from the Bayesian boundaries between classes. Since the gradient directs towards this anomalous boundary, Saliency Maps , given by \(()=|_{}()|\) fails to represent a meaningful transition between classes and then often lead to noisy explanation (as stated in Section 2).

On the contrary, in the experiments, we will demonstrate that OTNN gradients induce meaningful explanations (Sec. 5). We justify these good properties by building a link with counterfactualexplanations. Indeed, according to [11; 15], optimal transport plans are potential surrogates or even substitutes to causal counterfactuals. Optimal Transport plan provides for any sample \(\) a sample \(_{}()\), the closest in average on the pairing process.  prove that these optimal transport plans can even coincide with causal counterfactuals when available. Relying on this definition of OT counterfactual, Prop. 1 demonstrates that gradients of the optimal OTNN solution provide almost surely the direction to the counterfactual \(_{}()=-t_{}^{*}()\). Even if \(t\) is only partially known, using \(t=^{*}()\), we know that \(_{}\) is on the decision boundary (Corr. 1) and is both an adversarial attack and a counterfactual explanation and \(|t||^{*}()|\) is on the path to the other distribution.

Thus the learning process of OTNNs induces a strong constraint on the gradients of the neural network, aligning them to the optimal transport plan. We claim that is the reason why the simple Saliency Maps for OTNNs have very good properties: We will demonstrate in the Sec 5 that, for the Saliency Map explanations: (\(\)) metrics scores are higher or comparable to other explanation methods (which is not the case for unconstrained networks), thus it has higher ranks; (\(\)) distance to other attribution methods such as Smoothgrad is imperceptible; (\(\)) scores obtained on metrics that can be compared between networks are higher than those obtained with unconstrained networks; (\(\)) alignment with human explanations is impressive.

## 5 Experiments

We conduct experiments with networks learnt on FashionMNIST , and 22 binary labels of CelebA  datasets, Cat vs Dog (binary classification, 224x224x3 uncentered images), and Imagenet . Note that labels in CelebA are very unbalanced (see Table 2 in Appendix A.2, with for instance less than \(5\%\) samples for _Mustache_ or _Wearing_Hat_).

Architectures used for OTNNs and unconstrained networks are similar (same number of layers and neurons, a VGG for FashionMNIST and CelebA, a ResNet50 for Cat vs Dog and Imagenet). We also train an alternative of ResNet50 OTNN with twice the number of parameters (50 M). Unconstrained networks use batchnorm and ReLU layers for activation, whereas OTNNs only use GroupSort2 [5; 56] activation. OTNNs are built using the _DEEL_LIP1_ library, using Bjorck orthogonalization projection algorithm for linear layers. Note that several other approaches can be used for orthogonalization without altering the theoretical results; these might potentially enhance experimental outcome scores. The loss functions are cross-entropy for unconstrained networks (categorical for multiclass, and sigmoid for multilabel settings), and hKR \(_{,m}^{hKR}\) (and the multiclass variants see appendix A.2.3) for OTNNs. We train all networks with Adam optimizer . Details on architectures and parameters are given in Appendix A.2.

Figure 2: Level sets of an OTNN classifier \(\) for two concentric Koch snowflakes (a). The decision boundary (denoted \(\), also called the 0-level set) is the red dashed line. Figure (b) (resp. (c)) represents the translation of the form \(^{}=-()_{}()\) of each point \(\) of the first class (resp second class). \([,^{}]\) pairs are represented by blue (resp. orange) segments.

**Classification performance:** OTNN models achieve comparable results to unconstrained ones, confirming claims of : they reach 88.5% average accuracy on FashionMNIST (Table 6), and 81% (resp. 82%) average Sensitivity (resp. Specificity) over labels on CelebA (Table 6 in Appendix A.3). We use Sensitivity and Specificity for CelebA to take into consideration the unbalanced labels. OTNNs achieve 96% accuracy (98% for the unconstrained version) on Cat vs Dog and 67% (75% for the unconstrained version) on Imagenet. The ResNet50 OTNN with 50M parameters achieves 70% accuracy on Imagenet.

We present the results of quantitative evaluations of XAI metrics to compare the Saliency Map method with other explanation methods on OTNN, and more generally compare XAI explanations methods on these networks and their unconstrained counterparts. On CelebA, we only present the results for the label _Mustache_, but results for the other labels are similar. Parameters for explanation methods and metrics are given in Appendix A.4. We have chosen to present in Table 1 two SoTA XAI metrics that enable comparison between OTNNs and unconstrained networks. \(\)**Fidelity metric** is a well-known method that measures the correlation between important variables defined by the explanation method and the model score decreases when these variables are reset to a baseline state (or replaced by uniform noise). Another important property for explanations is their stability for nearby samples. In , the authors proposed Stability metrics based on the \(L_{2}\) distance. To better evaluate this stability and make it comparable for different models, we replace the \(L_{2}\) distance by \(1-\), \(\) being the Spearman rank correlation. Other model-dependent metrics are described in the Appendix. Results from the 50M parameter ResNet OTNN are included in the human alignment study (Fig 4) to illustrate that enhancing the model's complexity can bolster both the accuracy and alignment. The following observations can be drawn from Table 1:

**Saliency Map on OTNNs exhibit more fidelity and stability :** We confirm and amplify the results in . Table. 1 clearly states that for most of the explanation methods, the \(\)Fidelity, zero or uniform, is significantly higher for OTNNs. And above all, Saliency Map score for OTNNs is always higher than any other attribution method score for unconstrained models. A similar observation holds for the Stability Spearman rank : OTNN scores are better whatever the attribution method.

**Saliency Map method on OTNNs is equivalent to other attribution methods:** We observe that the scores from the Saliency Maps and other methods are very similar for OTNN, with Saliency Maps consistently ranking among the top attribution methods. For the unconstrained case, Saliency Maps are occasionally outperformed by other attribution methods. Notably, for the ResNet architecture, attribution methods other than Saliency Maps and GradCAM yield more erratic results for

   &  &  &  &  \\  & OTNN & Uncst. & OTNN & Uncst. & OTNN & Uncst. & OTNN & Uncst. \\  Attribution &  \\  Saliency & **0.156** & -0.001 & **0.244** & 0.052 & **0.091** & 0.080 & **0.240** & 0.004 \\ SmoothGrad & **0.114** & -0.001 & **0.248** & 0.018 & **0.012** & -0.004 & **0.001** & -0.002 \\ Integ. Grad. & **-0.005** & -0.013 & **0.149** & 0.093 & 0.022 & **0.024** & **0.046** & 0.022 \\ Grad. Input & -0.017 & **-0.009** & **0.168** & 0.074 & **0.013** & 0.009 & **0.009** & 0.000 \\ GradCam & **0.215** & 0.02 & **0.028** & 0.002 & **0.101** & 0.052 & 0.029 & **0.046** \\  Attribution &  \\  Saliency & **0.246** & 0.034 & **0.325** & 0.082 & **0.121** & 0.079 & **0.147** & 0.049 \\ SmoothGrad & **0.332** & 0.052 & **0.324** & 0.091 & **0.011** & -0.004 & 0.001 & **0.002** \\ Integ. Grad. & **0.543** & 0.134 & **0.400** & 0.125 & **0.037** & 0.027 & **0.057** & 0.023 \\ Grad. Input & **0.479** & 0.079 & **0.439** & 0.093 & **0.019** & 0.004 & **0.020** & -0.001 \\ GradCam & **0.161** & 0.046 & **0.127** & 0.061 & **0.136** & 0.049 & 0.048 & **0.068** \\  Attribution &  \\  Saliency & **0.59** & 0.91 & **0.51** & 0.77 & **0.58** & 0.69 & **0.60** & 0.74 \\ SmoothGrad & **0.55** & 0.82 & **0.52** & 0.95 & **0.64** & 0.82 & **0.62** & 0.82 \\ Integ. Grad. & **0.61** & 0.79 & **0.52** & 0.87 & **0.61** & 0.76 & **0.60** & 0.74 \\   \\  Saliency & **7.5e-03** & 7.0e-02 & **3.1e-4** & 1.4e-1 & **3.7e-8** & 4.1e-8 & **3.7e-8** & 4.3e-8 \\  

Table 1: Comparison of XAI metrics for different attributions methods and dataset for OTNN and unconstrained networks.

fidelity metrics. To highlight these results, we compare the \(L_{2}\) distance between Saliency Maps and SmoothGrad explanations, as suggested by . The explanation distances for OTNN are significantly lower than for the unconstrained ones and closely approach zero, indicating that for OTNN, averaging over a large set of noisy inputs--as in SmoothGrad--is unnecessary. This is illustrated in Fig.3.

**OTNNs** **explanations are aligned with human ones :** Adopting the method presented in , using the ClickMe dataset, we follow strictly their experimental methodology and use their code2 to compute the human feature alignment of OTNN Saliency Maps and compare with the others models tested in - more than 100 recent deep neural networks. In Figure 4, we demonstrate that OTNN models Saliency Maps, which also carries a theoretical interpretation as the direction of the transport plan, is more aligned with human attention than any other tested models and significantly surpasses the Pareto front discovered by . The OTNN model is even more aligned than a ResNet50 model trained with a specific alignment objective, proposed by , and called _Harmonized ResNet50_. This finding is interesting as it indicates OTNNs are less prone to relying on spurious correlations  and better capture human visual strategies for object recognition. The implications of these results are crucial for both cognitive science and industrial applications. A model that more closely aligns with human attention and visual strategies can provide a more comprehensive understanding of how vision operates for humans, and also enhance the predictability, interpretability, and performance of object recognition models in industry settings. Furthermore, the drop in alignment observed in recent models highlights the necessity of considering the alignment of model visual strategies with human attention while developing object recognition models to reduce the reliance on spurious correlations and ensure that our models get things right for the right reasons.

**Qualitative results:** Using the learnt OTNN on FashionMNIST, CelebA (Mouth Slightly Open label), Cat vs Dog and Imagenet, Fig. 6,5 present the original image, average gradients \(_{x}f_{j}\) over the channels, and images in the direction of the transport plan (Prop. 1), other samples are given in Appendix A.5. We can see that most of the gradients are visually consistent, showing clearly what has to be changed in the input image to modify the class and act as a counterfactual explanation. This is less clear for the Imagenet examples. This could be due to the difficulty of defining a transport plan for each pair of the 1000 classes. However, feature visualizations in Figure 1 show that the internal representation of the classes is still more interpretable than the unconstraint one. More generally, we observe that the gradient gives clear information about how the classifier makes its decision. For instance, for the cat, it shows that the classifier does not need to encode perfectly the concept of a cat, but mainly to identify the color of the eyes and size of the nose.

## 6 Conclusions and broader impact

In this paper, we study the explainability properties of OTNN (Optimal Transport Neural Networks) that are 1-Lipschitz constrained neural networks trained with an optimal transport dual loss. We

Figure 3: Comparison of Saliency Map and SmoothGrad explanations for (a) OTNN and (b) unconstrained network for, from left to right, CelebA, Cat vs Dog and Imagenet datasets.

establish that the gradient of the optimal solution aligns with the transportation plan direction, and the closest decision boundary point (adversarial example) also lies in this gradient direction at a distance of the absolute value of the network output. Relying on a formal definition of Optimal Transport counterfactuals, we build a link between the OTNN gradients and counterfactual explanations. We thus show that OTNNs loss jointly targets the classification task and induces a gradient alignment to the transportation plan. These beneficial properties of the gradient substantially enhance the Saliency Map XAI method for OTNN s. The experiments show that the simple Saliency Map has top-rank scores on state-of-the-art XAI metrics, and largely outperforms any method applied to unconstrained networks. Besides, as far as we know, our models are the first large provable 1-Lipschitz neural models that match state-of-the-art results on large problems. And even if counterfactual explanations are less compelling on Imagenet, probably due to the complexity of transport for large number of classes, we prove that OTNNs Saliency Maps are impressively aligned to human explanations.

**Broader impact.** This paper demonstrates the value of OTNNs for critical problems. OTNNs are certifiably robust and explainable with the simple Saliency Map method (highly aligned with human explanations) and have accuracy performances comparable to unconstrained networks.Though OTNNs take 3-6 times longer to train than unconstrained networks, they have similar computational costs during inference. We hope that this contribution will raise a great interest in these OTNN networks.

Figure 4: **OTNN are aligned with Human attention. Our study shows that the Saliency Map of OTNN model is highly aligned with human attention.**

Figure 5: Samples of counterfactuals for FashionMNIST dataset on different classes and targets: (left) source image, (center) gradient image, (right) counterfactual of the form \(-t*()_{x}()\), for \(t>1\)