# Bidirectional Recurrence for Cardiac Motion Tracking with Gaussian Process Latent Coding

Jiewen Yang Yiqun Lin Bin Pu Xiaomeng Li

The Hong Kong University of Science and Technology

{jyangcu, ylindw}@connect.ust.hk {eebinpu, eexmli}@ust.hk

###### Abstract

Quantitative analysis of cardiac motion is crucial for assessing cardiac function. This analysis typically uses imaging modalities such as MRI and Echocardiograms that capture detailed image sequences throughout the heartbeat cycle. Previous methods predominantly focused on the analysis of image pairs lacking consideration of the motion dynamics and spatial variability. Consequently, these methods often overlook the long-term relationships and regional motion characteristic of cardiac. To overcome these limitations, we introduce the **GPTrack**, a novel unsupervised framework crafted to fully explore the temporal and spatial dynamics of cardiac motion. The GPTrack enhances motion tracking by employing the sequential Gaussian Process in the latent space and encoding statistics by spatial information at each time stamp, which robustly promotes temporal consistency and spatial variability of cardiac dynamics. Also, we innovatively aggregate sequential information in a bidirectional recursive manner, mimicking the behavior of diffeomorphic registration to better capture consistent long-term relationships of motions across cardiac regions such as the ventricles and atria. Our GPTrack significantly improves the precision of motion tracking in both 3D and 4D medical images while maintaining computational efficiency. The code is available at: https://github.com/xmed-lab/GPTrack.

## 1 Introduction

Cardiac motion tracking from Cardiac Magnetic Resonance Imaging (MRI) and Echocardiograms is crucial in quantitative cardiac image processing. These imaging techniques provide comprehensive image sequences that cover an entire heartbeat cycle, allowing for detailed analysis of cardiac dynamics. Conventional non-parametric cardiac motion tracking approaches, such as B-splines , Demons algorithms  and optical-flow based methods [3; 4; 5], are commonly utilized due to their flexibility and ability to align detailed structures within images. However, these methods face significant challenges in motion tracking because they lack topology-preserving constraints and temporal coherence. The diffeomorphic registration method [6; 7; 8], which formulates the registration process as a group of diffeomorphisms in Lagrangian dynamics, is a nice candidate for topology-preserving motion tracking. However, traditional optimization-based diffeomorphic registration methods are computationally intensive and sensitive to noises, hindering their applications in efficient cardiac motion tracking.

Current deep learning-based techniques [9; 10; 11; 12; 13; 14; 15; 16] employ these advanced imaging modalities to register image pairs within the same patient, and some [14; 15; 16] adopt the diffeomorphic routine and learn the Lagrangian strain to describe the motion relationship between the reference frame and subsequent frames, aggregating dynamic information into consecutive cardiac motions as Lagrangian displacements. Although the above diffeomorphic methods better model the dynamic and continuous nature of cardiac motion, they still have room for improvement in handling the long-term temporal relationship in videos. For example, the approach  requires segmentationannotation to generate dense motion trajectories and calculate Lagrangian strain. Additionally, the methods outlined in [15; 16] are prone to error accumulation as Lagrangian displacements are integrated without regularizing temporal variations. In spatial views, while the global trajectory flow may follow a specific pattern, significant variations exist within each region regarding the phases, amplitudes, and intensities of the motion. For example, Figure 1 shows regions of the Right Atrium (red) and M myocardium (green) performing the opposite trajectories during the heartbeat cycle. Conversely, similar regions across different cases exhibit consistent motions. Hence, ignoring the regional scale may lead to a fragmented understanding of cardiac motion, underscoring the need for more nuanced analytical approaches. Furthermore, as shown in the right of Figure 1, the deformation is bounded in the space of periodically specific human cardiac motion variation.

To leverage discussed temporal and spatial information for cardiac motion tracking, we propose a novel framework named **GPTrack**. Our GPTrack has several appealing facets: **1)** GPTrack employs the Gaussian Process (GP) to formulate the consistent temporal patterns in the latent space of diffeomorphic frameworks, promoting the consistency of cardiac motion: **2)** GPTrack utilizes position information in the latent space to encode the statistics of the sequential Gaussian process, by which we model the region-specific motion and obtain a more precise estimation related to cardiac motion; **3)** GPTrack leverages the inherent temporal continuity in cardiac motion by aggregating long-term relationships through forward and backward video flows, which mimics the forward-backward manner of classical diffeomorphic registration framework . To evaluate the performance of GPTrack in cardiac motion tracking, we conduct experiments based on 3D Echocardiogram videos [17; 18] and 4D temporal MRI image . Results in Tables 1, 2 and 3, show the GPTrack enhance the accuracy of motion tracking performance in a clear margin, without substantially increasing the computational cost in comparison to other state-of-the-art methods. **Our contributions are summarized as follows: 1.** We propose a novel cardiac motion tracking framework named the **GPTrack**. This framework employs the _Gaussian Process_ (GP) to promote temporal consistency and regional variability in compact latent space, establishing a robust regularizer to enhance cardiac motion tracking accuracy. 2. The GPTrack framework is designed to capture the long-term relationship of cardiac motion via a bidirectional recursive manner, and its forward-backward manner mimics the workflows of the classical diffeomorphic registration framework. By this approach, our method provides a more accurate and reliable estimation of cardiac motion. 3. Our GPTrack framework achieves state-of-the-art performance on both 3D Echocardiogram videos and 4D temporal MRI datasets, maintaining comparable computational efficiency. The results demonstrate that our method adapts effectively across different medical imaging modalities, proving its utility in different clinical settings.

## 2 Related Work

### Cardiac Motion Tracking via Non-parametric Registration Approach

Extensive works have been proposed to address registration by optimising within the space of displacement vector fields. Models related to elastic matching were proposed by [20; 21].  utilized

Figure 1: **Regional Motions in Cardiac: The left sequential MRI frames within a heartbeat cycle illustrate that motion direction and intensity are completely different between the right atrium and myocardium during End-diastole and End-systolic. **Formulate Cardiac Motion as Prior Knowledge:** The right figure depicts the regions of motion trajectory across the heartbeat cycle, alongside the probability distributions of motion trajectory. Curves (Middle) are the motion trajectory changes of different MRI sequences (Left). Highlighting the cardiac motion trajectory that follows a certain pattern can be modelled as prior knowledge via the Gaussian Process (Right).

statistical parametric mapping for improvement. Techniques incorporating free-form deformations with B-splines and Maxwell demons were adapted by  and , respectively. The Harmonic phase-based method, which utilizes spectral peaks in the Fourier domain for cardiac motion tracking, was introduced by . This method calculates phase images from the inverse Fourier transforms and is specifically exploited in the analysis of tagged MRI. Popular formulations [6; 8; 24; 7] introduce topology-preserved diffeomorphic transforms. In the realm of diffeomorphic registration, inverse consistent diffeomorphic deformations have been estimated by  and . Syn  proposed standard symmetric normalization. RDMM  considered regional parameterization based on Large Deformation Diffeomorphic Metric Mapping . Despite their remarkable success in computational anatomy studies, these approaches are also time-consuming and susceptible to noise.

### Cardiac Motion Tracking with Deep-Learning based Registration Method

Recent advancements in medical image registration have increasingly leveraged Deep Learning technologies. Pioneering studies [14; 25; 26; 27; 28] utilize ground truth displacement fields obtained by simulating deformations and deformed images, typically estimated using non-parametric methods. These approaches, however, may be limited by the types of deformations they can effectively model, which can affect both the quality and accuracy of the registration. Unsupervised methods, as discussed by [9; 10; 15; 29; 30; 31; 32] have shown promise by learning deformation through the warping of a fixed image to a moving image using spatial transformation functions . These methods have been extended to include deformable models for single directional deformation field tracking [9; 26; 34] and diffeomorphic models for stationary velocity fields [32; 35; 36]. Further application of diffeomorphic models to cardiac motion tracking has been explored by [13; 15; 16; 30; 31]. These models predict motion fields that are both differentiable and invertible, ensuring one-to-one mappings and topology preservation. Recent studies in denoising diffusion probabilistic models (DDPM), such as  and , have achieved considerable success in registration tasks. However, DDPM-based methods face challenges in building temporal connections and demand substantial computational resources. The DL-based optical flow (OF) methods [37; 38; 39; 40] apply widely in nature image motion tracking. However, as illustrated in [7; 15; 16], due to annotations requirements and photometric constraints, they cannot be adopted in unsupervised cardiac motion tracking in medical image domains _(See Section A1 in Appendix for detailed discussion)_.

## 3 Methodology

### Diffeomorphic Tracking of Cardiac Motion

Diffeomorphic motion tracking and registration techniques are widely used in medical image analysis because they seek topology-preserving mapping between source and target images [6; 8]. Formally, given the source image \(x_{0}\) and target image \(x_{1}\), the diffeomorphic registration aims at a family of differentiable and invertible mappings \(\{_{t}\}_{t}\) with the boundary condition \(_{0}=\) and \(_{1}(x_{0})=x_{1}\), where \(\) is the identity mapping. The diffeomorphism \(_{t}\) can be parameterized as its derivatives (velocity field) \(_{t}\) as follows:

\[}{dt}=_{t}(_{t}):=_{t}_{t}_{t}=_{0}+_{0}^{t}_{ s}(_{s})ds,\;\;s,\] (1)

where \(\) is the composition operator. For numerical implementation, the associative property of the diffeomorphism group indicates \(_{t_{1}+t_{2}}=_{t_{1}}_{t_{2}}\), and the integral of Equation 1 can be approximated by \(_{t+}_{t}+_{t}\) for \(t[0,1)\) and small enough \(\). In this study, we follow the parameter settings of [7; 15; 16; 35; 36] and take \(=}\), \(N=7\) to discretize the path of diffeomorphic deformation.

Figure 2: Comparison between our GPTrack (a) and conventional registration framework (b).

### Motion Tracking with Gaussian Process Latent Coding

Our proposed method adopts the generative variational autoencoder (VAE) framework as the backbone of a diffeomorphic tracking network, as suggested by previous methods [15; 16; 36]. However, as shown in Figure 2, the first difference between our method and other methods is that ours allows the registration network to aggregate the spatial information temporally, both forward and backward (see Figure 2(a)). The conventional approaches [9; 10; 15; 16; 24; 31] only conduct between two adjacent frames, which ignore the relationship of long-term dependency of the cardiac motion (see Figure 2(b)). Secondly, despite the large deformation through the path of diffeomorphism, the space of periodically specific human cardiac motion variation is bounded. Though methods [14; 15; 16] use Lagrangian strain to formulate the continuous dynamic of cardiac motion, however, without considering the motion consistency between two adjacent state spaces, the Lagrangian strain is prone to error accumulation and degrade the tracking performance. To address this problem, we take the simple yet efficient Bayesian approach, which employs the Gaussian Process (GP) to model cardiac motion dynamics in the compact feature space, predicting more consistent motion fields over dynamics parameters. Our proposed GPTrack can also be easily extended to other modalities or motion-tracking tasks, such as 3D Echocardiigram videos and 4D cardiac MRI.

In this paper, we follow the research  that employs the recursive manner in transformer for sequential data. As shown in Figure 3 left, the GPTrack pipeline comprised the GPTrack layer for feature extraction, the Gaussian Process (GP) layer for modelling the cardiac motion dynamics, and the Decoder for motion field estimation. Given the sequential 4D inputs \(\{x_{t}\}_{t=1}^{T},x_{t}^{H W D 1}\), where \(H,W,D,T\) denote the height, width, depth, length of the input. For each \(x_{t}\), we first decompose it to \(P\) non-overlapping patches of shape \(p p p\), where \(P=\) and \(p,,,^{+}\). We then embed each patch as a feature with \(C\) channels via embedding layers and disentangle patches with the dimension of \(^{P C}\) from \(x_{t}\). The GPTrack layer then takes the \(x\), both forward and backward hidden states \(,^{P C}\) as the input, then predicts the motion field \(\) via the decoder after the GP layer. Note that the initial hidden states of forward \(_{0}\) and backward \(_{0}\) are set as zero.

### Bidirectional Forward-Backward Recursive Cell

The GPTrack layer consists of two independent GPTrack cells that respond to forward and backward computation. Similar to the [41; 43], adapting the hidden state to maintain and aggregate the sequential information allows the input with variable length. Meanwhile, the conventional convolutional neural network or vision in the transformer-based method is limited by the fixed input length. Furthermore, medical images such as Echocardiigram videos usually consist of hundreds of frames that cover multiple heartbeat cycles. Hence, parallel computing all frames requires a large amount of computational consumption, which hinders the application in real scenarios limited by low-computational devices. To this end, as shown in Tables 1 and 2, our GPTrack is able to formulate the variable temporal information while maintaining the comparable computational cost.

Figure 3: The overview pipeline of GPTrack (one layer). The \(x\), \(h\), \(\) and \(z\) denote input, forward hidden states, backward hidden states and latent coordinates. Feature \(_{t}\) with probabilistic prior on the latent space via Gaussian Process then enters the decoder to predict the motion field \(\). Subscript \(t\) denotes the \(t\)-th position in total \(T\) moments. \(elu()\) represent the exponential linear units .

Using the forward GPTrack cell shown in the right of Figure 3 as an example, the \(x_{t}\) and \(_{t-1}\) followed by the addition of learnable position encoding \(_{t}^{P C}\) are respectively normalized by Layer Normalization. The linear self-attention then computes the attentive weight \(A_{t}^{P C}\) of combined \(x_{t}\) and \(_{t-1}\). The above operations can be formulated as follows:

\[A_{t}=((_{Q}x)+1)((_{K}x)+1)^{} _{V}x;\ x=(x_{t}+_{t})(_{t -1}+_{t})^{P 2C},\] (2)

where \(_{Q},_{K},_{V}^{2C C}\) are learnable weights of different projection layers named query, key and value, \(()\) represent the exponential linear units \(elu()\), \(()\) and \(\) are the layer normalization and the concatenation operation, respectively.

In order to raise the descendant hidden state \(_{t}\) that aggregates information before \(t+1\) moment. The attentive weight \(A_{t}\) then conducts the element-wise addition with ancestral positional encoded \(_{t-1}\). Additionally, the \(A_{t}\) takes the positional encoded \(x_{t}\) as the residual connection and applies the addition operation. The Feed Forward Network denoted as \(()\), is then introduced to output the feature of \(t\)-th moment. The formulation can be written as follows:

\[_{t}=A_{t}+(_{t-1}+_{s})^{P  C},\ _{t}=((A_{t}+(x_{t}+_{s}))) ^{P C}.\] (3)

In the Bidirectional Forward-Backward Recursive Cell, both forward and backward share the same computation processes through the GPTrack cell. The only difference between the two directions is that the forward process starts from the first moment \(x_{0}\) of input while the backward starts from the last moment \(x_{T}\). Hence, the feature \(f_{t}\) in \(t\)-th moment is formulated as \(f_{t}=_{t}+_{t},\ f_{t}^{P C}\), where \(_{t}\) aggregate the forward information from \(0\) to \(t\), while the \(_{t}\) aggregate the backward information from the last frame to the \(t\)-th frame.

### Gaussian Process in Cardiac Motion Tracking

The primary objective of integrating the Gaussian Process (GP) is to establish a probabilistic prior in the latent space that incorporates prior knowledge. Specifically, it posits that cardiac motion across different individuals within the same region should yield similar motion fields in latent space encodings. Furthermore, as illustrated in Figure 1, cardiac structures within an individual that are spatially distant or exhibit motions in opposite directions should consistently adhere to the periodic pattern of motion.

Initially, we define a covariance (kernel) function for the GP layer as depicted in Figure 3. We design the prior for the latent space processes to be stationary, mean square continuous, and differentiable in sequential motion fields. This design stems from our expectation that the latent functions should model cardiac motion more prominently than visual features. Consequently, we anticipate the latent space to manifest continuous and relatively smooth behaviour. To this end, we employ the isotropic and stationary Matern kernel (refer to Equation 4) to fulfil the required covariance function structure:

\[(x_{t},x_{t-1})=}{()}(,x_{t-1})}{l})^{}K_{}(,x_{t-1})}{l}),\] (4)

where \(,,l>0\) are the smoothness, magnitude and length scale parameters, \(K_{}\) is the modified Bessel function, and \(D(,)\) denotes the distance metric between features of two consecutive motion fields. Our goal is to formulate cardiac motion as robust prior knowledge applicable to unseen data. To address this, we propose a position-related distance measurement.

As outlined in section 3.3 and referenced in , we utilize a learnable parameter \(^{P C}\) as the spatial positional encoding for each region, which provides relative or absolute positional information about the decomposed patches. To capture the periodic temporal positional information of cardiac motion, distinct from the spatial encoding pos, we apply sine and cosine functions of various frequencies as temporal encoding \(\{}_{t}\}_{t=1}^{T},} _{t}^{P C}\) for each moment. The overall positional encoding at moment \(t\) is formulated as:

\[_{t}=}_{t}^{P C},\ \ }_{t}(t,i)=_{(i=2k)}(t n ^{-})+_{(i=2k+1)}(t n^{-}).\] (5)

The \(i\) denotes the \(i\)-th position of \(C\) channels, and \(n\) is the scaling factor. We assign independent GP priors to all values in \(\{z_{t}\}_{t=1}^{T},z_{t}^{P C}\) to disseminate temporal information between framesin the sequence. During this process, we regard the sequential output \(\{f_{t}\}_{t=1}^{T}\) of GPTrack as noise-corrupted versions of the ideal latent space encodings, formulating the inference as the following GP regression model with noise observations:

\[z_{t}((_{t}),(_{t-1},_{t})),\ f_{t}=z_{t}+_{t},\ _{t}(0,^{2}),\]

where \(^{2}\) is the noise variance of the likelihood model set as the learnable parameter in GPTrack. The above Gaussian process can be treated as the temporal sequence with intrinsic Markov property, and we adopt the methodology of connecting the Gaussian process with state space model  to decrease its computational complexity from \(O(T^{3})\) to \(O(T)\), where \(T\) is the number of time step. Concretely, the Gaussian process of Equation 6 corresponds to the following linear stochastic differential equation:

\[(t)=(t)+w(t),\ \ f(t)=^{}\ (t)+(t),\ (t)(0,^{2}),\] (6)

with the solution as:

\[(t)=^{(t-r)}(r)+_{r}^{t}^{(t-s) }w(s)ds,\  r<t,\] (7)

\[f(t)=^{}(t)+(t),\ (t) (0,^{2}),\]

where \((t):=(z(t),z(t))\), \(w(t)\) is the zero-mean Gaussian random process, and \(:=(0,1)^{}\) is used for modelling observation model. The state transition matrix (vector) \(\) and \(\) can be calculated from the covariance function \(\) of the Gaussian process. For the Matern kernel shown in Equation 4, we take \(=\), and corresponding state transition matrix \(\) and vector \(\) read as:

\[=(0,&1\\ -},-}{l}),\ =(0,1)^{ }.\] (8)

Then we can discretize Equation 7 and get its weakly equivalent state-space model of Equation as:

\[_{t}=_{t}\ _{t-1}+_{t},\ f_{t}= ^{}_{t}+_{t},\ (t)(0,^{2}),\ t=1,,T,\] (9)

where \(_{t}=^{D(_{t},_{t-1})}\), \(_{t}(0,_{t}Q_{w}(t-1,t)^{}_{t}^{})\), and \(Q_{w}(t,t-1)\) is the covariance of \(w(t)\). Given the initial value \(_{0}(_{0},_{0})\) with \(_{0}=0\) and \(_{0}=(}{2},}{l^{2}})\), we can sequentially calculate the posterior distribution \(_{t}|f_{1:t-1}(}_{t},}_{t})\) and \(_{t}|f_{1:t}(_{t},_{t})\) using update criterion of Kalman filter for state space model  as:

\[}_{t} _{t}}_{t-1}, }_{t} _{t}}_{t-1}_{t}^{ }+_{0}-_{t}_{0}_{t}^{},\] (10) \[_{t} }_{t}+_{t}(f_{t}- ^{}}_{t}),\ \ _{t}}_{t}-_{t} ^{}}_{t},\ t=1,,T,\]

where \(_{t}:=}_{t}}{ }_{t}+^{2}}\) is the optimal Kalman gain at time \(t\). The output of the GP layer in \(t\)-th moment thus can be formulated as \(z_{t}^{GP}=(_{t}z_{t})\), where ReLU is the activation function. In the final, the \(t\)-th motion field \(_{t}\) is obtained by decoder from the \(z_{t}^{GP}\).

### Overall Loss of Tracking a Time Sequence of Cardiac Motion

The decoder takes the Gaussian-process coding \(\{z_{t}^{GP}\}_{t=1}^{T}\) as velocity filed to composite the diffeomorphic motion field \(\) according to the criterion of Section 3.1. Here, we adopt the training loss of [15; 16], which minimizes the integration of four components summarized as follows: **a)** Dissimilarities of tracking results between adjacent states from both forward and backward; **b)** Smoothness of motion fields between adjacent states from both forward and backward; **c)** Dissimilarities of tracking results between the start state and each state; **d)** Smoothness of motion fields between the start state and each state. The overall loss function \(\) is formulated as:

\[_{t=1}^{T-1}[_{kl}(x_{t},x_{t+1})}_{}+ _{1}(_{sm}(_{t:t+1})+_{sm}( _{t+1:t})}_{})+_{2}_{nc}(x_{t+1},x_{1}_{0:t+1})}_{}+_{3}_{sm}(_{1:t+1})}_{}],\]

where \(_{1}\), \(_{2}\) and \(_{3}\) are loss weights, and \(_{t_{1}:t_{2}}\) is the motion field from state \(t_{1}\) to \(t_{2}\). \(_{kl}(x_{t},x_{t+1})=(q(z_{t}^{GP}|x_{t};x_{t+1})||p(z_{t}^ {GP}|x_{t};x_{t+1}))+(q(z_{t}^{GP}|x_{t+1};x_{t})||p(z_{t}^{GP}|x_{ t+1};x_{t}))\) is the summation of forward and backward VAE losses with latent coding \(z_{t}^{GT}\), posterior distribution \(q\) and conditional distribution \(p\), \(L_{nc}\) is the negative normalized local cross-correlation metric, and \(_{sm}()=||||_{2}^{2}\) is the \(_{2}\)-total variation metric.

Experiment

### Datasets

**CardiacUDA .** The CardiacUDA dataset collected from two medical centers consists of 314 echocardiiogram videos from patients. The video is scanned from the apical four-chamber heart (A4C) view. In this paper, we conduct training and validation in the A4C view that consists of 314 videos with 5 frame annotations in the Left/Right Ventricle and Atrium (LV, LA, RV, RA). For testing, we report our results in 10 videos with full annotation provided by the CardiacUDA.

**CAMUS .** The CAMUS dataset provides pixel-level annotations for the left ventricle, myocardium, and left atrium in the Apical two-chamber view, which consists of \(500\) echocardiiogram videos in total. There are 450 subjects in the training set with 2 frames annotated in the Left Ventricle (LV), Left Atrium (LA) and Myocardium (Myo) in the end-diastole (ED) and end-systole (ES) of the heartbeat cycle. The remaining 50 subjects without any annotation masks belong to the testing set.

**ACDC .** The ACDC dataset consists of 100 4D temporal cardiac MRI cases. All data provide the segmentation annotations corresponding with the Left Ventricle (LV), Left Atrium (LA) and Myocardium (Myo) in the end-diastole (ED) and end-systole (ES) during the heartbeat cycle.

### Implementation Details

**Training.** We trained the model using the Adam optimizer with betas equal to \(0.9\) and \(0.99\). The training batch size of the model was set to 1. We trained for a total of 1000 epochs with an initial learning rate of \(5e^{-4}\) and decay by a factor of \(0.5\) in every 50 epochs. During training, for CardiacUDA  and CAMUS , we resized each frame to \(384 384\) and then randomly cropped them to \(256 256\). All frames were normalized to  during training. In temporal augmentation of datasets [17; 18], we randomly selected 32 frames from an echocardiiogram video with a sampling ratio of either 1 or 2. For ACDC , we resampled all scans with a voxel spacing of \(1.5 1.5 3.15\)mm and cropped them to \(128 128 32\), normalized the intensity of all images to [-1, 1]. For spatial data augmentation of all datasets, we randomly applied flipping, rotation and Gaussian blurring. In CardiacUDA, we split the dataset into \(8:2\) for training and validation. During testing, we reported results in 10 fully annotated videos. In the CAMUS  dataset, videos without annotation are used for only training, while we randomly split the remaining 450 annotated videos into 300/50/100 for training, validation and testing. In the ACDC , following the [11; 12], we split the training set in the ratio of 90 and 10 for training and testing. The reproduced methods strictly follow the official code and the description in the paper. For all experiments, We use Intel(R) Xeon(R) Platinum 8375C with 1\(\) RTX3090 for both training and inference. All reproduced methods strictly followed the training settings with their original paper in the same experimental environment.

**Inference.** For CardiacUDA and CAMUS, we resized videos to \(384 384\), cropped to \(256 256\) in central and normalized to . We sample 32 frames that cover the segmentation annotation. When the sequence has more than 32 frames, the extra frames will be removed from the sequence, except for the first and the last one. The ACDC dataset remains the same sampling strategy as training in the inference stage, without any argumentation except for normalizing intensity to [-1,1].

**Evaluation Metrics.** For the evaluation of the quality of registered target frames, we follow  to use the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM)  to measure whether the Lagrangian motion field is accurately estimated between the first frame and the following wrapped frames. We also use the Dice  score to measure the discrepancy between tracked and ground-truth cardiac segmentation. For CardiacUDA , only the first frame and corresponding segmentation are provided for tracking the following 32 frames, then report the averaged results by the above metrics in these 32 frames. For CAMUS  and ACDC , frame and segmentation of the ED stage are used to track the go-after frames, and we report all the metrics in the ES stage. We evaluate diffeomorphic property by computing the percentage of non-positive values of the Jacobian determinant \(det(J_{}) 0\ (\%)\) on the Lagrangian motion field. In order to access the evaluation of comparing the physiological plausibility following the [50; 51], we also compute the mean absolute difference between the \(1\) and Jacobian determinant (\(||J_{}|-1|\)) over the tracking areas. For a fair comparison, we evaluate the computational efficiency and report the computational time in seconds (Times), the parameter quantities in millions (Params), and the tera-floating point operations per second (TFlops). _We also provide the result evaluated by Hussdorf Distance (HD) in Tables B1, B2 and B3 of Appendix Section B._

[MISSING_PAGE_FAIL:8]

As shown in Figure 4, the tracking error of our GPTrack and other methods show a significant difference in LA (Labelled by Orange Colour) and LV (Labelled by Green Colour) when compared to the ground truth. The methods [24; 10; 9; 31] present inaccuracy tracking results due to lack of the constraints on the consecutive motion and ignore the long-term temporal information. These results further verify that our specifically designed GPTrack for modelling motion patterns is more suitable for cardiac motion tracking on echocardiography.

**Result of 4D Temporal Cardiac MRI Dataset.** We compare our GPTrack method against with the state-of-the-art deep learning based methods [9; 10; 11; 12; 15] and different Non-rigid approaches [6; 8; 24]. As illustrated in Table 2, our GPTrack-XL achieves the best average DICE score of \(82.65\) compared to FSDiffReg  with \(82.30\). In registration quality, our GPTrack-XL reaches the highest scores, 31.52 and 86.19, in PSNR and SSIM, respectively. Moreover, our Jacobian determinant on deformation fields shows numbers comparable to other methods with the diffeomorphic constraint. All results are based on our fast and lightweight model, reducing around \(96.93\%\) inference time, \(17.2\%\) model parameters and \(76.02\%\) computational consumption (TFlops) compared to the second-best performance. In comparison to the diffusion-based method [12; 11], which requires enormous computation that hinders real-time inference and is nearly impossible to deploy in real scenarios, the GPTrack preserve light-weight and considerable performance by formulating cardiac motion patterns as the Gaussian process latent coding and bidirectionally understand the cardiac motion. The visualization result in Figure 5 also indicates our GPTrack can achieve better tracking accuracy.

### Ablation Study

**The Scale of Model Hyper-parameters.** Table 4 shows the settings of the 2D/3D GPTrack-M/L/XL. For the 3D echocardiogram video dataset and 4D cardiac MRI dataset, the GPTrack with different scales has different patch sizes and dimension numbers. Referring to the result performed by Tables 3, 1 and 2, the registration result can be boosted by increasing the layers number and dimension size of GPTracks according to different requirements.

Figure 4: The visualization in 3D Echocardiogram video of motion tracking error. We visualised the last frame of tracking result and ground truth from 32 consecutive frames in CardiacUDA . Colours Red, Blue, Green and Orange denote cardiac structures RA, RV, LV and LA, respectively.

Figure 5: The visualization in 4D Cardiac MRI of motion tracking error. We visualised the result of the last frame tracking from ED to ES and corresponding ground truth in ACDC . Colours Red, Blue, and Green denote cardiac structures MYO, LA, and LV, respectively.

[MISSING_PAGE_FAIL:10]

*  J-P Thirion. Image matching as a diffusion process: an analogy with maxwell's demons. _Medical image analysis_, 2(3):243-260, 1998.
*  Alessandro Becciu, Hans van Assen, Luc Florack, Sebastian Kozerke, Vivian Roode, and Bart M ter Haar Romeny. A multi-scale feature based optic flow method for 3d cardiac motion estimation. In _Scale Space and Variational Methods in Computer Vision: Second International Conference, SSVM 2009, Voss, Norway, June 1-5, 2009. Proceedings 2_, pages 588-599. Springer, 2009.
*  Liang Wang, Patrick Claysse, Zhengjun Liu, Bin Gao, Wanyu Liu, Pierre Croisille, and Philippe Delachartre. A gradient-based optical-flow cardiac motion estimation method for cine and tagged mr images. _Medical image analysis_, 57:136-148, 2019.
*  KY Esther Leung, Mikhail G Danilouchkine, Marijn van Stralen, Nico de Jong, Antonius FW van der Steen, and Johan G Bosch. Left ventricular border tracking using cardiac motion models and optical flow. _Ultrasound in medicine & biology_, 37(4):605-616, 2011.
*  M Faisal Beg, Michael I Miller, Alain Trouve, and Laurent Younes. Computing large deformation metric mappings via geodesic flows of diffeomorphisms. _International journal of computer vision_, 61:139-157, 2005.
*  John Ashburner. A fast diffeomorphic image registration algorithm. _Neuroimage_, 38(1):95-113, 2007.
*  Zhengyang Shen, Francois-Xavier Vialard, and Marc Niethammer. Region-specific diffeomorphic metric mapping. _Advances in Neural Information Processing Systems_, 32, 2019.
*  Guha Balakrishnan, Amy Zhao, Mert R Sabuncu, John Guttag, and Adrian V Dalca. An unsupervised learning model for deformable medical image registration. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 9252-9260, 2018.
*  Guha Balakrishnan, Amy Zhao, Mert R Sabuncu, John Guttag, and Adrian V Dalca. Voxelmorph: a learning framework for deformable medical image registration. _IEEE transactions on medical imaging_, 38(8):1788-1800, 2019.
*  Yi Qin and Xiaomeng Li. Fsdiffreg: Feature-wise and score-wise diffusion-guided unsupervised deformable image registration for cardiac images. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 655-665. Springer, 2023.
*  Boah Kim, Inhwa Han, and Jong Chul Ye. Diffusemorph: Unsupervised deformable image registration using diffusion model. In _European conference on computer vision_, pages 347-364. Springer, 2022.
*  Hanchao Yu, Shanhui Sun, Haichao Yu, Xiao Chen, Honghui Shi, Thomas S Huang, and Terrence Chen. Fool: Fast online adaptive learning for cardiac motion estimation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4313-4323, 2020.
*  Nripesh Parajuli, Allen Lu, Kevinminh Ta, John Stendahl, Nabil Boutagy, Imran Alkhalil, Melissa Eberle, Geng-Shi Jeng, Maria Zontak, Matthew O'Donnell, et al. Flow network tracking for spatiotemporal and periodic point matching: Applied to cardiac motion analysis. _Medical image analysis_, 55:116-135, 2019.
*  Meng Ye, Mikael Kanski, Dong Yang, Qi Chang, Zhennan Yan, Qiaoying Huang, Leon Axel, and Dimitris Metaxas. Deeptag: An unsupervised deep learning method for motion tracking on cardiac tagging magnetic resonance images. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 7261-7271, 2021.
*  Meng Ye, Dong Yang, Qiaoying Huang, Mikael Kanski, Leon Axel, and Dimitris N Metaxas. Sequence-morph: A unified unsupervised learning framework for motion tracking on cardiac image sequences. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
*  Jiewen Yang, Xinpeng Ding, Ziyang Zheng, Xiaowei Xu, and Xiaomeng Li. Graphecho: Graph-driven unsupervised domain adaptation for echocardiogram video segmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11878-11887, 2023.
*  Sarah Leclerc, Erik Smistad, Joao Pedrosa, Andreas Ostvik, Frederic Cervenansky, Florian Espinosa, Torvald Espeland, Erik Andreas Rye Berg, Pierre-Marc Jodoin, Thomas Grenier, et al. Deep learning for segmentation using an open large-scale dataset in 2d echocardiography. _IEEE transactions on medical imaging_, 38(9):2198-2210, 2019.
*  Olivier Bernard, Alain Lalande, Clement Zotti, Frederick Cervenansky, Xin Yang, Pheng-Ann Heng, Irem Cetin, Karim Lekadir, Oscar Camara, Miguel Angel Gonzalez Ballester, et al. Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem solved? _IEEE transactions on medical imaging_, 37(11):2514-2525, 2018.

*  Ruzena Bajcsy and Stane Kovacic. Multiresolution elastic matching. _Computer vision, graphics, and image processing_, 46(1):1-21, 1989.
*  Christos Davatzikos. Spatial transformation and registration of brain images using elastically deformable models. _Computer Vision and Image Understanding_, 66(2):207-222, 1997.
*  John Ashburner and Karl J Friston. Voxel-based morphometry--the methods. _Neuroimage_, 11(6):805-821, 2000.
*  Nael F Osman, William S Kerwin, Elliot R McVeigh, and Jerry L Prince. Cardiac motion tracking using cine harmonic phase (harp) magnetic resonance imaging. _Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine_, 42(6):1048-1060, 1999.
*  Brian B Avants, Charles L Epstein, Murray Grossman, and James C Gee. Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain. _Medical image analysis_, 12(1):26-41, 2008.
*  Julian Krebs, Tommaso Mansi, Herve Delingette, Li Zhang, Florin C Ghesu, Shun Miao, Andreas K Maier, Nicholas Ayache, Rui Liao, and Ali Kamen. Robust non-rigid registration through agent-based action learning. In _Medical Image Computing and Computer Assisted Intervention- MICCAI 2017; 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20_, pages 344-352. Springer, 2017.
*  Xiaohuan Cao, Jianhua Yang, Jun Zhang, Dong Nie, Minjeong Kim, Qian Wang, and Dinggang Shen. Deformable image registration based on similarity-steered cnn regression. In _Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20_, pages 300-308. Springer, 2017.
*  Marc-Michel Rohe, Manasi Datar, Tobias Heimann, Maxime Sermesant, and Xavier Pennec. Svf-net: learning deformable image registration using shape matching. In _Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20_, pages 266-274. Springer, 2017.
*  Hessam Sokooti, Bob De Vos, Floris Berendsen, Boudewijn PF Lelieveldt, Ivana Isgum, and Marius Staring. Nonrigid image registration using multi-scale 3d convolutional neural networks. In _Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20_, pages 232-239. Springer, 2017.
*  Bob D De Vos, Floris F Berendsen, Max A Viergever, Marius Staring, and Ivana Isgum. End-to-end unsupervised deformable image registration with a convolutional neural network. In _Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Quebec City, QC, Canada, September 14, Proceedings 3_, pages 204-212. Springer, 2017.
*  Kevinminh Ta, Shawn S Ahn, Allen Lu, John C Stendahl, Albert J Sinusas, and James S Duncan. A semi-supervised joint learning approach to left ventricular segmentation and motion tracking in echocardiography. In _2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)_, pages 1734-1737. IEEE, 2020.
*  Shawn S Ahn, Kevinminh Ta, Allen Lu, John C Stendahl, Albert J Sinusas, and James S Duncan. Unsupervised motion tracking of left ventricle in echocardiography. In _Medical imaging 2020: Ultrasonic imaging and tomography_, volume 11319, pages 196-202. SPIE, 2020.
*  Tony CW Mok and Albert CS Chung. Unsupervised deformable image registration with absent correspondences in pre-operative and post-recurrence brain tumor mri scans. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 25-35. Springer, 2022.
*  Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. _Advances in neural information processing systems_, 28, 2015.
*  Xiaohuan Cao, Jianhua Yang, Jun Zhang, Qian Wang, Pew-Thian Yap, and Dinggang Shen. Deformable image registration using a cue-aware deep regression network. _IEEE Transactions on Biomedical Engineering_, 65(9):1900-1911, 2018.
*  Adrian V Dalca, Guha Balakrishnan, John Guttag, and Mert R Sabuncu. Unsupervised learning for fast probabilistic diffeomorphic registration. In _Medical Image Computing and Computer Assisted Intervention- MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I_, pages 729-738. Springer, 2018.

*  Tony CW Mok and Albert Chung. Fast symmetric diffeomorphic image registration with convolutional neural networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4644-4653, 2020.
*  Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In _Proceedings of the IEEE international conference on computer vision_, pages 2758-2766, 2015.
*  Pengpeng Liu, Michael Lyu, Irwin King, and Jia Xu. Selflow: Self-supervised learning of optical flow. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4571-4580, 2019.
*  Junhwa Hur and Stefan Roth. Iterative residual refinement for joint optical flow and occlusion estimation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 5754-5763, 2019.
*  Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2462-2470, 2017.
*  Jiewen Yang, Xingbo Dong, Liujun Liu, Chao Zhang, Jiajun Shen, and Dahai Yu. Recurring the transformer for video action recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14063-14073, 2022.
*  Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). _arXiv preprint arXiv:1511.07289_, 2015.
*  Xingbo Dong, Jiewen Yang, Andrew Beng Jin Teoh, Dahai Yu, Xiaomeng Li, and Zhe Jin. Video-based face outline recognition. _Pattern Recognition_, 152:110482, 2024.
*  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
*  Simo Sarkka and Arno Solin. _Applied stochastic differential equations_, volume 10. Cambridge University Press, 2019.
*  Simo Sarkka, Arno Solin, and Jouni Hartikainen. Spatiotemporal learning via infinite-dimensional bayesian filtering and smoothing: A look at gaussian process regression through kalman filtering. _IEEE Signal Processing Magazine_, 30(4):51-61, 2013.
*  Simo Sarkka and Jouni Hartikainen. Infinite-dimensional kalman filtering approach to spatio-temporal gaussian process regression. In _Artificial intelligence and statistics_, pages 993-1001. PMLR, 2012.
*  Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE transactions on image processing_, 13(4):600-612, 2004.
*  Abdel Aziz Taha and Allan Hanbury. Metrics for evaluating 3d medical image segmentation: analysis, selection, and tool. _BMC medical imaging_, 15:1-28, 2015.
*  Chen Qin, Shuo Wang, Chen Chen, Huaqi Qiu, Wenjia Bai, and Daniel Rueckert. Biomechanics-informed neural networks for myocardial motion tracking in mri. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2020: 23rd International Conference, Lima, Peru, October 4-8, 2020, Proceedings, Part III 23_, pages 296-306. Springer, 2020.
*  Chen Qin, Shuo Wang, Chen Chen, Wenjia Bai, and Daniel Rueckert. Generative myocardial motion tracking via latent space exploration with biomechanics-informed prior. _Medical Image Analysis_, 83:102682, 2023.
*  Mingyuan Meng, Lei Bi, Michael Fulham, Dagan Feng, and Jinman Kim. Non-iterative coarse-to-fine transformer networks for joint affine and deformable image registration. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 750-760. Springer, 2023.
*  Mingyuan Meng, Dagan Feng, Lei Bi, and Jinman Kim. Correlation-aware coarse-to-fine mlps for deformable medical image registration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9645-9654, 2024.

*  Zeyuan Chen, Yuanjie Zheng, and James C Gee. Transmatch: A transformer-based multilevel dual-stream feature matching network for unsupervised deformable image registration. _IEEE transactions on medical imaging_, 43(1):15-27, 2023.
*  Noemi Carranza-Herrezuelo, Ana Bajo, Filip Sroubek, Cristina Santamarta, Gabriel Cristobal, Andres Santos, and Maria J Ledesma-Carbayo. Motion estimation of tagged cardiac magnetic resonance images using variational techniques. _Computerized Medical Imaging and Graphics_, 34(6):514-522, 2010.
*  Simon Meister, Junhwa Hur, and Stefan Roth. Unflow: Unsupervised learning of optical flow with a bidirectional census loss. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
*  Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer++: Masked cost volume autoencoding for pretraining optical flow estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1599-1610, 2023.
*  Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer: A transformer architecture for optical flow. In _European conference on computer vision_, pages 668-685. Springer, 2022.
*  Rico Jonschkowski, Austin Stone, Jonathan T Barron, Ariel Gordon, Kurt Konolige, and Anelia Angelova. What matters in unsupervised optical flow. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pages 557-572. Springer, 2020.
*  Ziyang Zheng, Jiewen Yang, Xinpeng Ding, Xiaowei Xu, and Xiaomeng Li. Gl-fusion: Global-local fusion network for multi-view echocardiogram video segmentation. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 78-88. Springer, 2023.
*  Jiewen Yang, Yiqun Lin, Bin Pu, Jiarong Guo, Xiaowei Xu, and Xiaomeng Li. CardiacNet: Learning to reconstruct abnormalities for cardiac disease assessment from echocardiogram videos. In _European Conference on Computer Vision_. Springer, 2024.
*  Bin Pu, Kenli Li, Jianguo Chen, Yuhuan Lu, Qing Zeng, Jiewen Yang, and Shengli Li. Hfsccd: a hybrid neural network for fetal standard cardiac cycle detection in ultrasound videos. _IEEE Journal of Biomedical and Health Informatics_, 2024.

## Appendix A1 Difference Between Optical Flow and Diffeomorphism Mapping

Optical flow (OF) based methods, often applied in object tracking of video sequences, have been explored by . However, their effectiveness in medical imaging is limited due to challenges in accommodating large deformations and the inherent low quality of certain medical imaging modalities , such as echocardiogram videos.

With the progression of deep learning, neural networks have also been employed to predict optical flow, which is crucial for predicting dynamic motion trajectories in video sequences. Notable implementations include FlowNet , iterative methods by , and self-supervised learning approaches by  and . However, while supervised methods require a ground truth annotation for training cost functions , unsupervised approaches depend on photometric loss to ensure motion consistency , which can be challenging to obtain in medical images.

Last but not least, OF-based methods do not necessarily preserve topology, non-globally one-to-one (objective) smooth and continuous mapping with derivatives that are invertible. In cardiac motion tracking, we consider the deformation in each point of the adjacent frame to remain one-on-one mapping and be invertible for forward and backward deformation fields. Directly using the OF-based method to predict the motion field of cardiac motion may lead to incorrect estimation.

## Appendix A2 Broader Impacts

Our work focuses on cardiac motion tracking with an unsupervised framework named GPTrack. The GPTrack framework has the potential to support medical imaging physicians, such as radiologists and sonographers, in observing the cardiac motion of patients. This is a fundamental task for assessing cardiac function, and we are able to provide decision support and improve analysis efficiency and analysis reproducibility in clinical scenarios.

Moreover, our new framework illustrates that cardiac motion can be formulated as strong prior knowledge, which is able to be utilised to enhance tracking accuracy. Also, our work presents several advantages that help us make progress towards these benefits, which improve the performance of automated motion field estimation algorithms. The method not only improves the precision of motion tracking and segmentation in both 3D and 4D medical image modalities  but also provides a comprehensive observation of motion information to radiologists and sonographers to facilitate human assessment. However, this work may still remain gaps between real-world clinical utilization due to medical image analysis being a low failure tolerance application. The meaning of this work is to present a new direction for cardiac motion tracking, which is different from the conventional approach. In the current stage, the trained model in public datasets and the results presented are not specific to provide support for clinical use.

[MISSING_PAGE_FAIL:16]

    &  \\   & LV & LA & Myo & Avg. \\   &  \\   LDDMM  & 5.817\(\)24 & 6.662\(\)29 & 6.337\(\)28 & 6.562\(\)21 \\ RDMM  & 5.430\(\)25 & 6.268\(\)24 & 5.953\(\)22 & 5.728\(\)15 \\ ANTs (SyN)  & 5.676\(\)23 & 6.547\(\)26 & 6.211\(\)27 & 6.242\(\)16 \\    \\   VM-SSD  & 4.708\(\)18 & 4.814\(\)17 & 5.647\(\)24 & 4.942\(\)12 \\ VM-NCC  & 4.745\(\)21 & 5.153\(\)19 & 5.231\(\)26 & 5.336\(\)13 \\ VM-DIF  & 4.466\(\)21 & 4.782\(\)19 & 5.365\(\)26 & 4.802\(\)15 \\ SYMNet  & 4.864\(\)23 & 5.149\(\)21 & 5.552\(\)27 & 5.254\(\)19 \\ NICE-Trans  & 4.626\(\)19 & 4.805\(\)21 & 5.096\(\)24 & 4.993\(\)16 \\ CorrMLP  & 3.850\(\)18 & 4.061\(\)17 & 3.653\(\)24 & 3.812\(\)13 \\ DiffuseMorph  & 4.102\(\)19 & 4.054\(\)23 & 4.184\(\)29 & 3.977\(\)12 \\ DeepTag  & 3.336\(\)16 & 3.651\(\)21 & 3.284\(\)22 & 3.552\(\)13 \\ Transmatch  & 3.904\(\)19 & 3.855\(\)21 & 3.770\(\)19 & 3.716\(\)14 \\ GPTrack-M(Ours) & 3.285\(\)14 & 3.170\(\)18 & 3.030\(\)18 & 3.361\(\)11 \\ FSDiffReg  & **2.970\(\)13** & 3.298\(\)20 & 2.862\(\)18 & 3.283\(\)22 \\  GPTrack-XL(Ours) & 3.147\(\)15 & **3.028\(\)19** & **2.844\(\)18** & **3.145\(\)13** \\   

Table 3: The performance of different registration methods in ACDC  dataset. Results report in Structures (LV, RV, Myo) and overall averaged Dice score (Avg. %). Segmentation results are reported in the Hussdorf Distance (HD). **Bold**, underline denote the best results and the second-best performance, respectively.

Figure B2: The visualization in 3D Echocardiogram video of estimated motion field and motion tracking error. We visualised tracking results from the first frame to the last frame, with ground truth from 8 consecutive frames in CardiacUDA . Colours Red, Blue, Green and Orange denote cardiac structures RA, RV, LV and LA, respectively.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Main claims are presented in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We already discuss the limitations in the last section. * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We prove the full set of assumptions and a complete (and correct) proof in our paper.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All the experimental results can be reproduced and we promise that all code will be made publicly. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: All datasets in experiments are public datasets, all code will be made publicly. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have described all the training and testing details. * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Error bars, standard deviation and all details of result are presented in paper. * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.

* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please see the implementation details. * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We promise that all research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Please refer to Appendix A.2. * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No dataset or generative data. * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: For all datasets, we have acquired the license and permission for dataset usage. * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: N/A * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used.

* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not involve crowdsourcing nor research with human subjects. * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not involve crowdsourcing nor research with human subjects. * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.