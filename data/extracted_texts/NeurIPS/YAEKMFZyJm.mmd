# Finding NeMo +: Localizing Neurons Responsible For Memorization in Diffusion Models

Dominik Hintersdorf\({}^{*}\)\({}^{1,2}\) Lukas Struppek\({}^{*}\)\({}^{1,2}\)

\({}^{1}\)German Research Center for Artificial Intelligence (DFKI)

\({}^{2}\)Computer Science Department, Technical University of Darmstadt

\({}^{3}\)Hessian Center for AI (Hessian.AI)

\({}^{4}\)Centre for Cognitive Science, Technical University of Darmstadt

\({}^{5}\)CISPA Helmholtz Center for Information Security

equal contribution, corresponding authors: {hintersdorf, struppek}@cs.tu-darmstadt.de

Code: https://github.com/ml-research/localizing_memorization_in_diffusion_models

###### Abstract

Diffusion models (DMs) produce very detailed and high-quality images. Their power results from extensive training on large amounts of data--usually scraped from the internet without proper attribution or consent from content creators. Unfortunately, this practice raises privacy and intellectual property concerns, as DMs can memorize and later reproduce their potentially sensitive or copyrighted training images at inference time. Prior efforts prevent this issue by either changing the input to the diffusion process, thereby preventing the DM from generating memorized samples during inference, or removing the memorized data from training altogether. While those are viable solutions when the DM is developed and deployed in a secure and constantly monitored environment, they hold the risk of adversaries circumventing the safeguards and are not effective when the DM itself is publicly released. To solve the problem, we introduce NeMo, the first method to localize memorization of individual data samples down to the level of neurons in DMs' cross-attention layers. Through our experiments, we make the intriguing finding that in many cases, _single neurons_ are responsible for memorizing particular training samples. By deactivating these _memorization neurons_, we can avoid the replication of training data at inference time, increase the diversity in the generated outputs, and mitigate the leakage of private and copyrighted data. In this way, our NeMo contributes to a more responsible deployment of DMs.

## 1 Introduction

In recent years, diffusion models (DMs) have made remarkable advances in image generation. In particular, text-to-image DMs, such as Stable Diffusion , DALL-E , or Deep Floyd  enable the generation of complex images given a textual input prompt. Yet, DMs carry a significant risk to privacy and intellectual property, as the models have been shown to generate verbatim copies of their potentially sensitive or copyrighted training data at inference time . This ability has often been linked to their _memorization_ of training data . Memorization in DMs recently received a lot of attention , and several mitigations have been proposed . Those mitigations usually focus on either identifying potentially highly memorized samples and excluding them from training, monitoring inference and preventing their generation, or altering the inputs to prevent the verbatim output of training data . While mitigations that rely on preventing the generation of memorized samples are effective when the DM is developed and deployed in a secureenvironment, they hold the inherent risk of adversaries circumventing them. Additionally, they are not effective when the DMs are publicly released, such that users can freely interact with them.

As a first step to solving this problem, we propose Finding **N**euron **Me**M**orization (NeMo), a new method for localizing where individual data samples are memorized inside the DMs. NeMo's localization tracks the memorization of training data samples down to the level of individual neurons in the DMs' cross-attention layers. To achieve this, NeMo relies on analyzing the different activation patterns of individual neurons on memorized and non-memorized data samples and identifying memorization neurons by outlier activation detection (as visualized in Fig. 2b). We empirically assess the success of NeMo on the publicly available DM Stable Diffusion . Our findings indicate that most memorization happens in the value mappings of the cross-attention layers of DMs. Furthermore, they highlight that most training data samples are memorized by _just a few or even a single neuron_, which is surprising given the high resolution and complexity of the training data.

Based on the insights about where within the DMs individual data samples are memorized, we can prevent their verbatim output by deactivating the identified memorization neuron(s). We demonstrate the effect of NeMo in Fig. 1. Without our approach, the image generated for the memorized input prompt is the same, independent of the random seed for generation. By localizing the neuron responsible for the memorization through NeMo and deactivating it, we prevent the verbatim output of the training data and instead cause the generation of various non-memorized related samples. Hence, by relying on NeMo to localize and deactivate memorization neurons, we can limit memorization, which mitigates the privacy and copyright issues while keeping the overall performance intact.

In summary, we make the following contributions:

* We propose NeMo, the first method to localize where memorization happens within DMs down to the level of individual neurons.
* Our extensive empirical evaluation of localizing memorization within Stable Diffusion reveals that few or even single neurons are responsible for the memorization.
* We limit the memorization in DMs by deactivating the highly memorizing neurons and further show that this leads to a higher diversity in the generated outputs.

## 2 Background and Related Work

### Text-to-Image Synthesis with Diffusion Models

Diffusion models (DMs) [37; 19] are generative models trained by progressively adding random Gaussian noise to training images and having the model learn to predict the added noise. After the training is finished, new samples can be generated by sampling an initial noise image \(x_{T}(,)\) and then iteratively removing portions of the predicted noise \(_{}(x_{t},t,y)\) at each time step \(t=T,,1\).

Figure 1: **Overview of NeMo**. For memorized prompts, we observe that the same (original training) image is constantly generated independently of the initial random seed. This yields severe privacy and copyright concerns. In the _initial stage_, NeMo first identifies candidate neurons potentially responsible for the memorization based on out-of-distribution activations. In a _refinement_ step, NeMo detects the _memorization neurons_ from the candidate set by leveraging the noise similarities during the first denoising step. Deactivating memorization neurons prevents unintended memorization behavior and induces diversity in the generated images.

This denoising process is formally defined by

\[x_{t-1}=}}(x_{t}-}{_{t}}}_{}(x_{t},t,y)),\] (1)

with variance scheduler \(_{t}(0,1)\), \(_{t}=1-_{t}\) and \(_{t}=_{i=1}^{t}_{t}\). The noise predictor \(_{}(x_{t},t,y)\), usually a U-Net , receives an additional input \(y\) for conditional image generation.

Common text-to-image DMs [32; 30; 34] are conditioned on text embeddings \(y\) computed by pre-trained text encoders like CLIP . The typical way to incorporate the conditioning \(y\) into the denoising process is the cross-attention mechanism . (Cross-)Attention consists of three main components: query matrices \(Q=z_{t}W_{Q}\), key matrices \(K=yW_{K}\), and value matrices \(V=yW_{V}\). All three matrices are computed by applying learned linear projections \(W_{Q},W_{K}\), and \(W_{V}\) to the hidden image representation \(z_{t}\) and the text embeddings \(y\). The attention outputs are computed by

\[(Q,K,V)=(}{})  V\,,\] (2)

with scaling factor \(d\). Importantly, in most text-to-image models, the noise predictor receives guidance only through the cross-attention layers, which renders them particularly relevant for memorization.

### Memorization in Deep Learning

**Memorization.** Memorization was extensively studied in supervised models and with respect to data labels [48; 1; 9]. Recently, studies have been extended to unlabeled self-supervised learning [25; 43]. In both setups, it was shown that memorization is required for generalization [12; 13; 43]. However, memorization also yields privacy risks [4; 7; 14; 40] since it can expose sensitive training data. In particular for generative models, including DMs, it was shown that memorization enables the extraction of training data points [4; 5; 6; 8; 36].

**Localizing Memorization.** Early work on localizing where inside machine learning (ML) models memorization happens focuses on small neural networks. Initial findings suggested that in supervised models, memorization happens in the deeper layers [2; 39]. However, more fine-grained analyses contradict these findings and identify that individual units, _i.e.,_ individual neurons or convolutional channels throughout the entire model, are responsible for memorization . To identify these, Maini et al.  deactivate units throughout the network until a label flip on the memorized training input image occurs. However, due to the unavailability of labels, this approach does not transfer to DMs.

**Memorization in Diffusion Models.** Recent empirical studies connect the model architecture, training data complexity, and the training procedure to the expected level of DM memorization , while others connect memorization to the generalization of the generation process . Two types of memorization are usually distinguished: Verbatim memorization that replicates the training image exactly. And template memorization that reproduces the general composition of the training image while having some non-semantic variations at fixed image positions . Existing approaches for detecting memorized training samples are based on statistical differences in the model behavior when queried with memorized prompts. These approaches explore differences in predicted noise magnitudes , the distribution of attention scores , the amount of noise modification in one-step synthesis , and the edge consistency in generated images . Our work is orthogonal to these detection methods, focusing on the exact _localization of memorization_ in the DM's U-Net rather than detecting memorized samples.

Previously proposed methods for mitigating memorization during inference either rescale the attention logits  or adjust the text embeddings with a gradient-based approach to minimize the magnitude of noise predictions . However, these inference time mitigation strategies are easy to deactivate in practice and provide no permanent mitigation strategies for publicly released models. In contrast, related training-based mitigation strategies [46; 31] require re-training an already trained model like Stable Diffusion, which is time- and resource-intensive. We show that NeMo can reliably identify individual neurons responsible for memorizing specific training samples. Pruning these neurons effectively mitigates memorization, does not harm the general model performance, and provides a more permanent solution to avoid training data replication.

## 3 NeMo: Localizing and Removing Memorization in Diffusion Models

NeMo, our method for detecting _memorization neurons_, consists of a two-step selection process:

**(1) Initial Selection**: We first identify a broad set of candidate neurons that might be responsible for memorizing a specific training sample. This initial selection is coarse-grained to speed up the computation among the many neurons in DMs. Consequently, it might select false positives, _i.e.,_ neurons not directly responsible for memorization.

**(2) Refinement:** In the refinement step, we filter neurons out to reduce the size of the initial candidate set. After refinement, we deactivate the remaining _memorization neurons_ to remove memorization.

In our study, we apply this two-step approach of NeMo to detect memorization neurons in the DM's cross-attention layers, the only components that directly process the text embeddings. Image editing research [17; 42; 10] shows that cross-attention layers highly influence the generated content, so we expect them to be the driving force behind memorization. We analyze the impact of blocking individual key and value layers in Appx. C.8 and the influence of blocking neurons in the convolutional layers in Appx. C.9. Our results show that the value layers in the down- and mid-blocks of the U-Net indeed have the highest memorization effect, whereas value layers in the up-blocks barely affect the memorization. Deactivating the outputs of neurons in value layers completely blocks the information flow of the guidance signal and, hence, potential memorization triggers. Deactivating the key layers in cross-attention also impacts memorization but often impedes the image-prompt alignment. Similarly, deactivating neurons in the convolutional layers of the U-Net did not mitigate memorization. Therefore, we limit our search on memorization neurons to the value layers of the U-Net's down- and mid-blocks. While the identified neurons effectively mitigate the data replication problem, we emphasize that other parts of the U-Net might also play a crucial role in memorizing training data. Specifically, the identified neurons trigger the data replication, which is then executed by other parts of the U-Net, such as convolutional and fully connected layers. Deactivating the memorization neurons in value layers effectively interrupts the memorization chain and replication process. Before detailing the two steps of NeMo's selection process in Sec. 3.2 and Sec. 3.3, we first introduce how we quantify memorization strength in the next section. We provide detailed algorithmic descriptions for each of NeMo's components in Appx. D.

### Quantifying the Memorization Strength

Intuitively, the denoising process of DMs for memorized prompts follows a rather consistent trajectory to reconstruct the corresponding training image, yielding image generations with little diversity. Conversely, the denoising trajectory highly depends on the initially sampled noise for non-memorized prompts . We measure the similarity between the first denoising steps for different initial seeds as a proxy to compare the denoising trajectories and to quantify the memorization strength. The higher the similarity, the more consistent the denoising trajectories, which indicates stronger memorization. Let \(x_{T}(,)\) be the initial noise following the denoising process described in Sec. 2.1.

Figure 2: **Differences Between Memorized and Non-memorized Prompts.****(a)** depicts the distribution of pairwise SSIM scores between initial noise differences starting from different seeds. Since the noise trajectories are more consistent for memorized samples, the score reflects the degree of memorization. **(b)** shows the distribution of the \(z\)-scores of each neuron in the first cross-attention value layer. Memorization neurons produce considerably higher activations, here depicted as standardized \(z\)-scores, for memorized prompts, allowing them to be identified by outlier detection.

Let \(_{}(x_{T},T,y)\) further denote the initial noise prediction. We found that the normalized difference between the initial noise and the first noise prediction \(=_{}(x_{T},T,y)-x_{T}\) for memorized prompts is more consistent for different seeds than for non-memorized prompts. We visualize this phenomenon for some initial noise differences in Appx. C.6.

To detect the grade of memorization, we, therefore, use the similarity between the noise differences \(^{(i)}\) and \(^{(j)}\) generated with seeds \(i\) and \(j\) as a proxy. We measure the similarity with the common structural similarity index measure (SSIM) . A formal description of the SSIM \([-1,1]\) score and an additional experiment outlining how the SSIM score can be used to detect memorization in the first place is provided in Appx. B.4.

A higher SSIM indicates higher similarity between the noise differences, reflecting a higher degree of memorization. Notably, the SSIM computation only requires a single denoising step per seed, which makes the process fast. To set a memorization threshold \(_{}\), starting from which we define a sample as memorized, we first compute the mean SSIM on a holdout set of non-memorized prompts. We compute the pairwise SSIM between ten different initial noise samples for each prompt and take the maximum score. After that, we average the scores across all prompts and set the threshold \(_{}\) to the mean plus one standard deviation. We consider the current image generation non-memorized if the maximum pairwise SSIM scores are below this threshold \(_{}\). Fig. 2a shows the distribution of SSIM scores for memorized and non-memorized prompts, demonstrating that memorized prompts lead to a substantially higher score.

### Initial Candidate Selection for Memorization Neurons

With our measure for quantifying the strength of memorization defined, we move on to detail the first step of our NeMo' localization. Our initial neuron selection procedure is based on the observation that activation patterns of memorized prompts differ from the ones of non-memorized prompts on the neuron level. Fig. 2b underlines this observation by plotting the standardized activation scores for memorized and non-memorized samples in the first value layer. Leveraging this insight, we identify memorization neurons as the ones that exhibit an out-of-distribution (OOD) activation behavior. We first compute the standard activation behavior of neurons on a separate hold-out set of non-memorized prompts. Then, we compare the activation pattern of the neurons for memorized prompts and identify neurons with OOD behavior. Let the cross-attention value layers of a DM be \(l\{1,,L\}\). We denote the activation of the \(i\)-th neuron in the \(l\)-th layer for prompt \(y\) as \(a_{i}^{l}(y)\). The activation values are averaged across the absolute neuron activations for each token vector in the text embedding. Let \(_{i}^{l}\) be the pre-computed mean activation and \(_{i}^{l}\) the corresponding standard deviation for this neuron. To detect neurons potentially responsible for the memorization of a memorized prompt \(y\), we compute the standardized \(z\)-score , defined as

\[z_{i}^{l}(y)=^{l}(y)-_{i}^{l}}{_{i}^{l}}\,.\] (3)

The \(z\)-score quantifies the number of standard deviations \(_{i}^{l}\) by which the activation \(a_{i}^{l}(y)\) is above or below the mean activation \(_{i}^{l}\). Here, the activation \(a_{i}^{l}(y)\) is calculated by taking the mean over the absolute token activations. To identify a neuron as exhibiting an OOD activation behavior, we set a threshold \(_{}\) and assume that neuron \(i\) in layer \(l\) has OOD behavior if \(|z_{i}^{l}(y)|>_{}\). The lower the threshold \(_{}\), the more neurons are labeled as OOD and added to the _memorization neuron_ candidate set. An algorithmic description of the OOD detection step is provided in Alg. 2 in Appx. D.2.

Fig. 2a shows that the pairwise SSIM score can be used to measure the generated sample's degree of memorization. Hence, to get an initial selection of _memorization neurons_, we calculate the standardized \(z\)-scores for all neurons and start with a relatively high value of \(_{}=5\). We deactivate all neurons with OOD activations given the current threshold \(_{}\), i.e., setting the output of a neuron to \(0\) if \(|z_{i}^{l}(y)|>_{}\), to reduce the memorization strength. If, after deactivating these neurons, the memorization score is not below the threshold \(_{}\), we then iteratively decrease the activation threshold \(_{}\) by \(0.25\) and update the candidate set until the target memorization score \(_{}\) is reached.

The activation patterns of some neurons in the network show high variance, even on non-memorized prompts. Such neurons can also be memorization neurons, but due to their high activation variance, they might not be detected by our OOD approach based solely on the \(z\)-scores. Therefore, we also add the top-\(k\) neurons of each layer with the highest absolute activation on the memorized prompt to our current candidate set to account for such high-variance neurons. We start by setting \(k=0\) and increase \(k\) at each iteration by one if the memorization score is still above the threshold \(_{}\). We detail our initial selection process in Alg. 3 in Appx. D.3. All neurons identified by our OOD approach and the neurons with the \(k\) highest activations are then collected in the neuron set \(S_{}\). Since not all neurons in set \(S_{}\) might be memorization neurons, we refine this set in the next step.

### Refinement of the Candidate Set

In this step, we take the set of identified neurons \(S_{}=S_{}\) and remove the neurons that are actually not responsible for memorization. To speed up this process, we first group the identified neurons layer-wise, leading to the neuron set \(S_{}^{l}\) for layer \(l\). We iterate over the individual layers \(l\{1,,L\}\) and re-activate all identified neurons \(S_{}^{l}\) from a single layer \(l\) while keeping the identified neurons in the remaining layers deactivated.

We then compute the SSIM-based memorization score and check if it is still below the threshold \(_{}\). If the memorization score does not increase above the threshold \(_{}\), we consider the candidate neurons \(S_{}^{l}\) of layer \(l\) as not memorizing and remove them from our set of neurons \(S_{}\). After iterating over all layers, the set \(S_{}\) only contains neurons from layers that substantially influence the memorization score.

Next, we individually check each remaining neuron in the set \(S_{}\) by re-activating this particular neuron while keeping all other neurons in the set \(S_{}\) deactivated. Again, if the memorization score computed on the remaining deactivated neurons does not exceed the memorization threshold \(_{}\), we remove this neuron from the set \(S_{}\). After iterating over all neurons in \(S_{}\), we consider the remaining neurons as memorizing and denote the final set of memorization neurons as \(S_{}\). We detail this refinement approach in Alg. 4 in Appx. D.4.

## 4 Experiments

We now empirically evaluate NeMo's localization in text-to-image DMs.

**Models and Datasets:** We follow current research on memorization in DMs [46; 31] and investigate memorization in Stable Diffusion v1.4 . Our set of memorized prompts consists of 500 LAION prompts  provided by Wen et al. . We analyzed the prompts using the Self-Supervised Descriptor (SSCD) score , a model designed to detect and quantify copying in DMs. The lower the score, the less similar the contents in the image pairs. Additionally, we split the dataset into verbatim-(_VM_) and template-memorized (_TM_) samples to enable a more detailed analysis of results. The hyperparameter selection and experimental conduction are independent of the type of memorization. If not further specified, we used the same hyperparameters for all the experiments in the paper.

Images generated by VM prompts match the training image exactly, _i.e.,_ pixel-wise, independent of the chosen seed. TM prompts, on the other hand, reproduce the general composition of the training image while having some non-semantic variations at fixed image positions. Details about the analysis and the annotation can be found in Appx. C.1.

Other publicly available models, like Stable Diffusion v2 and Deep Floyd , are trained on more carefully curated and deduplicated datasets. We thoroughly checked for memorized prompts using the tools by Webster  and our SSIM-based memorization score but could not identify any properly memorized prompts. This result aligns with related research on memorization in DMs [46; 31].

**Metrics:** We split our metrics into memorization, diversity, and quality metrics. The memorization metrics measure the degree of **memorization** still present in the generated images. We generate ten images for each memorized prompt with activated/deactivated memorization neurons and measure the cosine similarities between image pairs using SSCD embeddings to quantify the memorization. We denote this metric by SSCDGen. Since the generated images without deactivated neurons also differ in their degree of memorization from the original training images, we additionally measure the degree of memorization towards the original training images and denote this metric as SSCDorig. Higher SSCD scores indicate a higher degree of memorization.

Our **diversity** metric assesses the variety of images generated for the same memorized prompt with different seeds. Diversity is usually low for memorized samples, and generated images almost always depict the same image. Deactivating memorization neurons increases the diversity in the generations.

[MISSING_PAGE_FAIL:7]

This behavior results from the fact that TM prompts typically memorize specific parts of the original training image, such as objects or compositions, rendering the SSCDorig metric less informative. In contrast, the SSCDGen score, which compares similarities between images generated with and without the deactivated neurons, provides a more accurate measure. This score highlights that deactivating the identified neurons effectively alters the images and mitigates memorization. Importantly, the image-prompt alignment \(A_{}\) remains constant in all cases, indicating that deactivating memorization neurons does not result in misguided image generations. We visualize examples of deactivating memorization neurons to avoid data replication and increase diversity in Fig. 3.

Comparing the results of deactivating the neurons identified by NeMo with those obtained from randomly deactivated neurons highlights that only a specific subset of neurons is actually responsible for memorizing a prompt. While deactivating the identified memorization neurons significantly impacts both memorization and the diversity of the generated images, randomly deactivating neurons has no noticeable effect.

Moreover, the mitigation effect of deactivating memorization neurons is comparable to the state-of-the-art method of adjusting the prompt embeddings . Yet, adjusting the prompt embeddings requires gradient computations for each seed and prompt, which are time- and memory-expensive, especially with large batch sizes. In contrast, once the memorization neurons are identified using our gradient-free NeMo, no additional computations are needed during image generations, thus adding no overhead to the generation process.

### Analyzing the Distribution of Memorization Neurons

Next, we analyze the distribution of the memorization neurons. Fig. 3(a) shows the total number of neurons responsible for memorizing specific prompts. Typically, a small set of neurons is responsible for verbatim memorization. For instance, 28 VM prompts from our dataset are memorized by a _single neuron_. Additionally, five or fewer neurons replicate two-thirds of VM images, indicating that verbatim memorization can often be precisely localized within the model. Template memorization can also frequently be pinpointed to a small set of neurons, with about 30% of TV replication triggered by five or fewer neurons.

Figure 4: **Distribution of Memorization Neurons.****(a)** shows the number of prompts that are memorized by a fixed number of neurons, e.g., the verbatim memorization of 28 prompts is located in single neurons. **(b)** depicts the average number of memorization neurons per layer and prompt.

Figure 3: **Impact of Deactivating Memorization Neurons**. The top row shows images generated with memorized prompts, closely replicating the training images. The bottom row demonstrates that deactivating memorization neurons increases diversity and mitigates memorization. Notably, only a few neurons (counts indicated by digits in the boxes) are responsible for these memorizations.

However, approximately one-third of TM prompts are distributed across 50 or more neurons. We hypothesize that this broader distribution results from the higher variation in generated images for TM prompts, where memorization spread across multiple neurons leads to increased image diversity. In contrast, VM prompts, often memorized by a small group of neurons, consistently produce the same image without variation. More detailed plots of the identified neurons can be found in Appx. C.2.

Interestingly, we identify two neurons in the first cross-attention value layer responsible for the verbatim memorization of multiple prompts. Neuron #25 in this layer is associated with depicting people, while neuron #221 is responsible for memorizing multiple podcast covers. Together, these neurons account for memorizing \(17\%\) of our dataset's VM prompts. Similarly, neurons #507 and #517 in the third value layer are responsible for multiple TM prompts describing iPhone cases. The impact of deactivating these neurons on the image generation of memorized prompts is visualized in Appx. C.5. We also plot the distribution of the average layer-wise number of memorization neurons per prompt in Fig. 3(b). Neurons responsible for VM prompts are primarily located in the value mappings of the first cross-attention layers within the U-Net's down-blocks (each block contains two cross-attention layers). A similar pattern appears for TM prompts, although value layers located deeper in the U-Net seem to play a more crucial role for TM prompts than for VM prompts.

### Memorization Neurons Hardly Influence Non-Memorized Prompts

Until now, our focus has been on the impact of deactivating memorization neurons on memorized prompts. In this part, we investigate how these neurons influence non-memorized prompts and the overall image quality of the DM. To assess their impact, we deactivate varying numbers of memorization neurons, ordered by their frequency of occurrence as identified in our experiments, and compute the FID and KID scores on the COCO dataset. We also repeat the generations by deactivating the same number of randomly selected neurons that are not among the identified memorization neurons. As shown in Fig. 4(a), there is no significant degradation in the image quality when blocking either the random neurons or the memorization neurons, even with up to 750 blocked neurons. This finding underscores the potential for pruning memorization neurons in DMs without compromising the overall image quality. The plot for the CLIP-FID metric and more detailed plots of the other two metrics, as well as an additional experiment measuring the disentanglement of the neurons in the value layers, can be found in Appx. C.3.

### Ablation Study and Sensitivity Analysis

We further analyze the impact of each component of NeMo and its sensitivity to hyperparameter selection. We discuss the most crucial insights here, with the complete study included in Appx. C.7. First, we evaluate the impact of different memorization thresholds \(_{}\). Lowering this threshold slightly increases the number of identified neurons but has a negligible effect on performance metrics.

Figure 5: **Image Quality and Sensitivity to Scaling Factor.****(a)** assesses the generated images’ quality when blocking an increasing number of neurons. As can be seen, the FID and KID values vary only slightly, indicating that blocking neurons identified by NeMo does not negatively affect image generation quality. Gray lines indicate the baseline without any neurons blocked. **(b)** investigates the effect of scaling the memorization neurons’ activations by a scaling factor instead of deactivating them (scaling by zero). Whereas positively scaling memorization neuron activations only slightly reduces memorization, negative scaling reduces the memorization not any further.

Selecting this threshold based on statistics computed on a holdout set provides a simple yet effective way for hyperparameter selection.

Additionally, we compare the results of using both stages of NeMo versus a setting where we only perform the initial candidate selection, skipping the refinement process. Although memorization is successfully mitigated by deactivating the initially selected neurons, the number of identified memorization neurons increases substantially, resulting in a median of 26.5 neurons (_+22.5 neurons_) for VM prompts and 674.5 neurons (_+653.5 neurons_) for TM prompts. This highlights the importance of the refinement stage in reducing the number of neurons necessary to mitigate memorization _efficiently_. To further test whether the assumptions about the statistics of memorization neurons hold, we applied NeMo to a set of 500 non-memorized prompts not used to calibrate our thresholds. As we further show, NeMo does not identify any neurons for most of the non-memorized prompts, underscoring the validity of our assumptions.

We also compared the effect of completely deactivating the identified memorization neurons to down-scaling their activations by a fixed factor. The SSCD scores in Fig. 4(b), computed for different scaling factors, demonstrate that memorization is not fully mitigated when using a positive scaling factor. Conversely, negative scaling factors do not provide any additional mitigation compared to our default setting of deactivating the neurons (i.e., using a factor of zero).

## 5 Conclusion and Outlook

DMs have rapidly become a cornerstone of computer vision. Yet, problems like memorization of training samples can lead to undesired replication of potentially sensitive or copyrighted training images. Previous research has primarily focused on identifying memorized prompts and proposing mitigation strategies by adjusting the DM's input. However, there has been a lack of understanding regarding the precise location of memorization within the model.

Our research provides novel insights into the memorization mechanisms in text-to-image DMs. Unlike previous studies that focused on identifying memorized prompts, our approach, NeMo, is the first to _localize memorization_ within the model and pinpoint _individual neurons_ responsible for it. Traditional pruning methods  are orthogonal to our approach by pruning only structures to reduce the total parameter count of the model. Our memorization localization algorithm enables model providers to prune these memorization neurons, effectively mitigating memorization permanently without additional model training, which can be costly in terms of data and resources. Our mitigation can be executed without compromising the model's overall performance or the quality of the generated images, allowing model providers to deploy the resulting models without additional safeguards to prevent memorization.

There are several directions to expand and build upon our method for detecting neurons responsible for memorization. One intriguing avenue is to investigate whether an adjusted version of NeMo can detect concept neurons . These neurons are not responsible for memorizing a certain prompt but for generating a particular concept. Such an approach could enable model providers and users to perform knowledge editing  and remove undesired concepts like violence and nudity. Another exciting application for NeMo is in large language models, also known for memorizing training samples . Identifying neurons responsible for memorizing text from the training data could lead to new mitigation strategies.

Additionally, our insights could be interesting for developing new pruning algorithms for DMs to reduce the number of parameters while eliminating unintended memorization. As demonstrated in our experiments, pruning memorization neurons does not significantly impact the model's overall performance, which is crucial for effective pruning strategies.