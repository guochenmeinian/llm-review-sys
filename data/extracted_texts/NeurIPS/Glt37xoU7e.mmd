# Omnigrasp: Grasping Diverse Objects

with Simulated Humanoids

Zhengyi Luo\({}^{1,2}\)

Equal Contribution

Jinkun Cao\({}^{1}\)

Sammy Christen\({}^{2,3}\)

Alexander Winkler\({}^{2}\)

Kris Kitani\({}^{1,2}\)

Weipeng Xu\({}^{2}\)

\({}^{1}\)Carnegie Mellon University; \({}^{2}\)Reality Labs Research, Meta; \({}^{3}\)ETH Zurich

https://zhengyiluo.github.io/Omnigrasp

###### Abstract

We present a method for controlling a simulated humanoid to grasp an object and move it to follow an object's trajectory. Due to the challenges in controlling a humanoid with dexterous hands, prior methods often use a disembodied hand and only consider vertical lifts or short trajectories. This limited scope hampers their applicability for object manipulation required for animation and simulation. To close this gap, we learn a controller that can pick up a large number (>1200) of objects and carry them to follow randomly generated trajectories. Our key insight is to leverage a humanoid motion representation that provides human-like motor skills and significantly speeds up training. Using only simplistic reward, state, and object representations, our method shows favorable scalability on diverse objects and trajectories. For training, we do not need a dataset of paired full-body motion and object trajectories. At test time, we only require the object mesh and desired trajectories for grasping and transporting. To demonstrate the capabilities of our method, we show state-of-the-art success rates in following object trajectories and generalizing to unseen objects. Code and models will be released.

Figure 1: We control a simulated humanoid to grasp diverse objects and follow complex trajectories. (_Top_): picking up and holding objects. (_Bottom_): green dots - reference trajectory; pink dots - object trajectory.

Introduction

Given an object mesh, we aim to control a simulated humanoid equipped with two dexterous hands to pick up the object and follow plausible trajectories, as shown in Fig.1. This capability could be broadly applied to creating human-object interactions for animation and AV/VR, with potential extensions to humanoid robotics . However, controlling a simulated humanoid with dexterous hands for precise object manipulation poses significant challenges. The bipedal humanoid must maintain balance to enable detailed movements of the arms and fingers. Moreover, interacting with objects requires forming stable grasps that accommodate diverse object shapes. Combining these demands with the inherent difficulties of controlling a humanoid with a high degree of freedom (_e.g_. 153 DoF) significantly complicates the learning process.

These challenges have led previous methods of simulated grasping to employ a disembodied hand [16; 17; 61; 85] to grasp and transport. While this approach can generate physically plausible grasps, employing a floating hand compromises physical realism: the hands' root position and orientation are controlled by invisible forces, allowing it to remain nearly perfectly stable during grasping. Moreover, studying the hand in isolation does not accurately reflect its typical use, which is when it is attached to a mobile and flexible body. A naive approach to supporting hands is to use existing full-body motion imitators  to provide body control and train additional hand controllers for grasping. However, the presence of a body introduces instability, limits hand movement, and requires synchronizing the entire body to facilitate finger motion. State-of-the-art (SOTA) full-body imitators also have an average 30mm tracking error for the hands, which can cause the humanoid to miss objects. Due to the above challenges, previous work that studies full-body object manipulations often limits its scope to only one sequence of object interaction  and encounters difficulties in trajectory following , even when trained with highly specialized motion priors.

Another challenge of grasping is the diversity of the object shapes and trajectories. Each object may require a unique type of grasping, and scaling to thousands of different objects often requires training procedures such as generalist-specialist training  or curriculum [75; 101]. There is also infinite variability in potential object trajectories, and each trajectory may necessitate precise full-body coordination. Thus, prior work typically focuses on simple trajectories, such as vertical lifting [16; 85], or on learning a single, fixed, and pre-recorded trajectory per policy . The flexibility with which humans manipulate objects to follow various trajectories while holding them remains unobtainable for current humanoids, even in simulations.

In this work, we introduce a full-body and dexterous humanoid controller capable of picking up and following diverse object trajectories using Reinforcement Learning (RL). Our proposed method, Omnigras, presents a scalable approach that generalizes to unseen object shapes and trajectories. Here, "Omni" refers to following any trajectory in all directions within a reasonable range and grasping diverse objects. Our key insight lies in using a pretrained universal dexterous motion representation as the action space. Directly training a policy on the joint actuation space using RL results in unnatural motion and leads to a severe exploration problem. Exploration noise in the torso can lead to a large deviation in the location of the arm and wrist as the noise propagates through the kinematic chain. This can lead to the humanoid quickly knocking the object away, which hinders training progress. Prior work has explored using a separate body and hand latent space trained using adversarial learning . However, as the adversarial latent space can only cover small-scale and curated datasets, these methods do not achieve a high grasping success rate. The separation of hands and body motion prior also adds complexity to the system. We propose using a unified _universal and dexterous_ humanoid motion latent space . Learned from a large-scale human motion database , our motion representation provides a compact and efficient action space for RL exploration. We enhance the dexterity of this latent space by incorporating articulated hand motions into the existing body-only human motion dataset.

Equipped with a universal motion representation, our humanoid controller does not require any specialized interaction graph [78; 102] to learn human-object interactions. Our input to the policy consists only of object and trajectory-following information and is devoid of any grasp or reference body motion. For training, we use randomly generated trajectories and do not require paired full-body human-object motion data. We also identify the importance of pre-grasps  (the hand pose right before grasping) and utilize it in our reward design. The resulting policy can be directly applied to transport new objects without additional processing and achieve a SOTA success rate on following object trajectories captured by Motion Capture (MoCap).

To summarize, our contributions are: (1) we design a dexterous and universal humanoid motion representation that significantly increases sample efficiency and enables learning to grasp with simple yet effective state and reward designs; (2) we show that leveraging this motion representation, one can learn grasping policies with synthetic grasp poses and trajectories, without using any paired full-body and object motion data. (3) we demonstrate the feasibility of training a humanoid controller that can achieve a high success rate in grasping objects, following complex trajectories, scaling up to diverse training objects, and generalizing to unseen objects.

## 2 Related Works

**Simulated Humanoid Control**. Simulated humanoids can be used to create animations [26; 36; 54; 55; 56; 57; 80; 94; 102], estimate full-body pose from sensors [23; 30; 33; 40; 43; 79; 92; 93; 95], and transfer to real humanoid robots [20; 27; 28; 59; 60]. Since there are no ground truth data for joint actuation and physics simulators are often non-differentiable, model-based control , trajectory optimization [36; 83], and deep RL [13; 54] are used instead of supervised learning. Due to its flexibility and scalability, deep RL has been popular among efforts in simulated humanoids, where a policy/controller is trained via trial and error. Most of the previous work on humanoids does not consider articulated fingers, except for a few [3; 6; 36; 49]. A dexterous humanoid controller is essential for humanoids to perform meaningful tasks in simulation and in the real world.

**Dexterous Manipulation**. Dexterous manipulation is an essential topic in robotics [7; 8; 11; 12; 15; 16; 19; 37; 62; 75; 85; 96; 97; 98] and animation [2; 6; 34; 101]. This task usually involves pick-and-place [7; 8], lifting [75; 85; 97], articulating objects , and following predefined object trajectories [6; 9; 17]. Most of these efforts use a disembodied hand for grasping and employ non-physical virtual forces to control the hand. Among them, D-Grasp  leverages the MANO  hand model for physically plausible grasp synthesis and 6DoF target reaching. UniDexGrasp  and its followup  use the Shadow Hand . PGDM  trains a grasping policy for individual object trajectories and identifies pre-grasp initialization (initializing the hand in a pose right before grasping) as a crucial factor for successful grasping. For the works that consider both hands and body, PMP  and PhysHOI  train one policy for each task or object. Braun _et al_.  studies a similar setting to ours but relies on MoCap human-object interaction data and only uses one hand. Compared to prior work, Omnigrasp trains one policy to transport diverse objects, supports bimanual motion, and achieves a high success rate in lifting and object trajectory following.

**Kinematic Grasp Synthesis**. Synthesizing hand grasp can be widely applied in robotics and animation. A line of work [5; 10; 10; 18; 21; 38; 47; 51; 84; 89] focuses on reconstructing and predicting grasp from images or videos, while others [52; 90] study hand grasp generation to help image generation. Among them, Manipnet and CAMS  predict finger poses given a hand object trajectory. TOCH  and GeneOH  denoise dynamic hand pose predictions for object interactions. More research in this area focuses on generating static or sequential hand poses with a given object as the condition [31; 70; 88]. For synthesizing body and hand poses jointly, there are limited MoCap data available  due to difficulties in capturing synchronized full-body and object trajectories. Some generative methods [22; 35; 69; 72; 73; 82; 91] can create paired human-object interactions, but they require initialization from the ground truth [22; 69; 82], or only predict static full-body grasps . In this work, we use GrabNet  trained on object shapes from OakInk  to generate hand poses as reward guidance for our policy training.

**Humanoid Motion Representation**. Due to the high DoF of a humanoid and the sample inefficiency of RL training, the search space within which the policy operates during trial and error is crucial. A more structured action space such as motion primitives [24; 25; 48; 63] or motion latent space [56; 74] can significantly increase sample efficiency since the policy can sample coherent motion instead of relying on random "jittering" noise. This is especially important for humanoids with dexterous hands, where the torso motion can drastically affect the hand movement and lead to the humanoid knocking the object away. Thus, prior work in this space utilizes part-based motion priors [3; 6] trained on specialized datasets. While effective in the single task setting where the humanoid only needs to perform actions close to the ones in the specialized datasets, these motion priors can hardly scale to more free-formed motion, such as following randomly generated object trajectories. We extend the recently proposed universal humanoid motion representation, PULSE , to the dexterous humanoid setting and demonstrate that a 48-dimensional, full-body-and-hand motion latent space can be used to pick up and follow randomly generated trajectories.

## 3 Preliminaries

We define the human pose as \(_{t}(_{t},_{t})\), consisting of 3D joint rotation \(_{t}^{J 6}\) and position \(_{t}^{J 3}\) of all \(J\) links on the humanoid (hands and body), using the 6 degree-of-freedom (DOF) rotation representation . To define velocities \(}_{1:T}\), we have \(}_{t}(_{t},_{t})\) as angular \(_{t}^{J 3}\) and linear velocities \(_{t}^{J 3}\). For objects, we define their 3D trajectories \(_{t}^{}\) using object position \(_{t}^{}\); orientation \(_{t}^{}\), linear velocity \(_{t}^{}\), and angular velocity \(_{t}^{}\). As a notation convention, we use \(\) to denote the kinematic quantities from Motion Capture (MoCap) or trajectory generator and normal symbols without accents for values from the physics simulation. \(}\) refers to a dataset of diverse object meshes.

**Goal-conditioned Reinforcement Learning for Humanoid Control**. We define the object grasping and transporting task using the general framework of goal-conditioned RL. Namely, a goal-conditioned policy \(\) is trained to control a simulated humanoid to grasp an object and follow object trajectories \(}_{1:T}^{}\) using dexterous hands. The learning task is formulated as a Markov Decision Process (MDP) defined by the tuple \(=,,,},\) of states, actions, transition dynamics, reward function, and discount factor. The simulation determines the state \(_{t}\) and transition dynamics \(\), where a policy computes the action \(_{t}\). The state \(_{t}\) contains the proprioception \(_{t}^{}\) and the goal state \(_{t}^{}\). Proprioception is defined as \(_{t}^{}(_{t},}_{t},_{t})\), which contains the 3D body pose \(_{t}\), velocity \(}_{t}\), and contact forces \(_{t}\) on the hand. The goal state \(_{t}^{}\) is defined based on the states of the objects. When computing the states \(_{t}^{}\) and \(_{t}^{}\), all values are normalized with respect to the humanoid heading (yaw). Based on proprioception \(_{t}^{}\) and the goal state \(_{t}^{}\), we define a reward \(r_{t}=}(_{t}^{},_{t}^{})\) for training the policy. We use proximal policy optimization (PPO)  to maximize discounted reward \([_{t=1}^{T}^{t-1}r_{t}]\). Our humanoid follows the kinematic structure of SMPL-X  using the mean shape. It has 52 joints, of which 51 are actuated. 21 joints are body joints, and the remaining 30 joints are for two hands. All joints have 3 DoF, resulting in an actuation space of \(_{t}^{51 3}\). Each degree of freedom is actuated by a proportional derivative (PD) controller, and the action \(_{t}\) specifies the PD target.

## 4 Omnigrasp: Grasping Diverse Objects and Follow Object Trajectories

To tackle the challenging problem of picking up objects and following diverse trajectories, we first acquire a universal dexterous humanoid motion representation in Sec.4.1. Using this motion representation, we design a hierarchical RL framework (Sec. 4.2) for grasping objects using simple1 state and reward designs guided by pre-grasps. Our architecture is visualized in Figure 2.

Figure 2: Omnigrasp is trained in two stages. (a) A universal and dexterous humanoid motion representation is trained via distillation. (b) Pre-grasp guided grasping training using a pretrained motion representation.

### PULSE-X: Physics-based Universal Dexterous Humanoid Motion Representation

We introduce PULSE-X that extends PULSE  to the dexterous humanoid by adding articulated fingers. We first train a humanoid motion imitator  that can scale to a large-scale human motion dataset with finger motion. Then, we distill the motion imitator into a motion representation using a variational information bottleneck (similar to a VAE ).

**Data Augmentation**. Since full-body motion datasets that contain finger motion are rare (_e.g._, 91% of the AMASS sequences do not have finger motion), we first augment existing sequences with articulated finger motion and construct a dexterous full-body motion dataset. Similarly to the process in BEDLAM , we randomly pair full-body motion from AMASS  with hand motion sampled from GRAB  and Re:InterHand  to create a dexterous AMASS dataset. Intuitively, training on this dataset increases the dexterity of the imitator and the subsequent motion representation.

**PHC-X: Humanoid Motion Imitation with Articulated Fingers**. Inspired by PHC , we design PHC-X \(_{}\) for humanoid motion imitation with articulated fingers. For the finger joints, _we treat them similarly as the rest of the body_ (e.g. _toe or wrist_) and find this formulation sufficient to acquire the dexterity needed for grasping. Formally, the goal state for training \(_{}\) with RL is \(_{t}^{}(}_{t+1}_{t},}_{t+1}-_{t},}_{t+1}-_{t},}_{t+1}-_{t},}_{t+1},}_{t+1})\), which contains the difference between proprioception and one frame reference pose \(}_{t+1}\).

**Learning Motion Representation via Online Distillation**. In PULSE , an encoder \(}_{}\), decoder \(}_{}\), and prior \(}_{}\) are learned to compress motor skills into a latent representation. For downstream tasks, the frozen decoder and prior will translate the latent code to joint actuation. Formally, the encoder \(}_{}(_{t}|_{t}^{},_{t} ^{})\) computes the latent code distribution based on current input states. The decoder \(}_{}(_{t}|_{t}^{},_{t})\) produces action (joint actuation) based on the latent code \(_{t}\). The prior \(}_{}(_{t}|_{t}^{})\) defines a Gaussian distribution based on proprioception and replaces the unit Gaussian distribution used in VAEs . The prior increases the expressiveness of the latent space and guides downstream task learning by forming a residual action space (see Sec.4.2). We model the encoder and prior distribution as diagonal Gaussian:

\[}_{}(_{t}|_{t}^{},_{t} ^{})=(_{t}|_{t}^{e},_{t}^{e} ),}_{}(_{t}|_{t}^{})=(_{t}|_{t}^{p},_{t}^{p}).\] (1)

To train the models, we use online distillation similar to DAgger  by rolling out the encoder-decoder in simulation and querying \(_{}\) for action labels \(_{t}^{}\). For more information and evaluation of PHC-X and PULSE-X, please refer to the Appendix B.

### Pre-grasp Guided Object Manipulation

Using hierarchical RL and PULSE-X's trained decoder \(}_{}\) and prior \(}_{}\), the action space for our object manipulation policy becomes the latent motion representation \(_{t}\). Since the action space serves as a strong human-like motion prior, we can use simple state and reward design and do not require any paired object and human motion to learn grasping policies. We use only hand pose before grasping (pregaps), either from a generative method or MoCap, to train our policy.

**State**. To provide the task policy \(_{}\) with information about the object and the desired object trajectory, we define the goal state as

\[_{t}^{}(_{t+1:t+}^{}-_{t}^ {},}_{t+1:t+}^{}_{t}^{ },}_{t+1:t+}^{}-_{t}^{}, }_{t+1:t+}^{}-_{t}^{},_{t}^{},_{t}^{},^{}, {p}_{t}^{}-_{t}^{}),\] (2)

which contains the reference object pose and the difference between the reference object trajectory for the next \(\) frames and the current object state. \(^{}^{512}\) is the object shape latent code computed using the canonical object pose and Basis Point Set (BPS) . \(_{t}^{}-_{t}^{}\) is the difference between the current object position and each hand joint position. All values are normalized with respect to the humanoid heading. Notice that the state \(_{t}^{}\) does not contain body pose, grasp, or phase variables , which makes our method applicable to unseen objects and reference trajectories at test time.

**Action**. Similar to downstream task policies in PULSE, we form the action space of \(_{}\) as the residual action with respect to prior's mean \(_{t}^{p}\) and compute the PD target \(_{t}\):

\[_{t}=}_{}(_{}(_{t} ^{}|_{t}^{},_{t}^{})+_{t}^{ p}),\] (3)where \(_{t}^{p}\) is computed by the prior \(}_{}(_{t}|_{t}^{p})\). The policy \(_{}\) computes \(_{t}^{}^{48}\) instead of the target \(_{t}^{51 3}\) directly, and leverages the latent motion representation of PULSE-X to produce human-like motion.

**Reward**. While our policy does not take any grasp guidance or reference body trajectory _as input_, we utilize pre-grasp guidance in the _reward_. We refer to pre-grasp \(}^{}(}^{},}^{})\) as a single frame of hand pose consisting of hand translation \(}^{}\) and rotation \(}^{}\). PGDM  shows that initializing a floating hand to pre-grasps can help the policy better reach objects and initiate manipulation. As we do not initialize the humanoid with the pre-grasp pose as in PGDM, we design a stepwise pre-grasp reward:

\[_{t}^{}=r_{t}^{},&\|}^{}-_{t}^{}\|_{2}>0.2t<\\ r_{t}^{},&\|}^{}-_{t}^{}\|_{2} 0.2t<\\ r_{t}^{},&t,\] (4)

based on time and the distance between the object and hands. Here, \(=1.5s\) indicates the frame in which grasping should occur, and \(_{t}^{}\) indicates the hand position. When the object is far away from the hands (\(\|}^{}-_{t}^{}\|_{2}>0.2\)), we use an approach reward \(r_{t}^{}\) similar to a point-goal  reward \(r_{t}^{}=\|}^{}-_{t}^{}\|_{2}-\|}^{}-_{t}^{}\|_{2},\) where the policy is encouraged to get close to the pre-grasp. After the hands are close enough (\( 0.2\)m), we use a more precise hand imitation reward: \(r_{t}^{}=w_{}e^{-100\|}^{ }-_{t}^{}\|_{2} 1(\|}^{}-_{t}^{ }\|_{2} 0.2)}+w_{}e^{-100\|}^{ }-_{t}^{}\|_{2}}\), to encourage the hands to be close to pre-grasps. For grasps that involve only one hand, we use an indicator variable \(\{\|}^{}-}_{t}^{}\|_ {2} 0.2\}\) to filter out hands that are too far away from the object. After timestep \(\), we use only the object trajectory following reward:

\[r_{t}^{}=(w_{}e^{-100\|}_{t}^{}- {p}_{t}^{}\|_{2}}+w_{}e^{-100\|}_{t}^{ }-}_{t}^{}\|_{2}}+w_{}e^{-51 }}_{t}^{}-}_{t}^{}\|_{2}+w_ {}e^{-51}}_{t}^{}-_{t}^{ }\|_{2})\{\}+\{\} w_{ }.\] (5)

\(r_{t}^{}\) computes the difference between the current and reference object pose, which is filtered by an indicator variable \(\{\}\) that is set to true if the object is in contact with the humanoid hands. The reward \(\{\} w_{}\) encourages the humanoid's hand to have contact with the object. Hyperparameters can be found in Appendix C.

**Object 3D Trajectory Generator**. As there is a limited number of ground-truth object trajectories , either collected from MoCap or animators, we design a 3D object trajectory generator that can create trajectories with varying speed and direction. Using the trajectory generator, our policy can be trained without any ground-truth object trajectories. This strategy provides better coverage of potential object trajectories, and the resulting policy achieves higher success in following unseen trajectories (see Table 1). Specifically, we extend the 2D trajectory generator used in PACER  to 3D, and create our trajectory generator \(^{30}(_{0}^{})=_{1:T}^{}\). Given initial object pose \(_{0}^{}\), \(^{30}\) can generate a sequence of plausible reference object motion \(_{1:T}^{}\). We limit the z-direction trajectory to between 0.03m and 1.8m and leave the xy direction unbounded. For more information and sampled trajectories, please refer to Appendix C.

**Training**. Our training process is depicted in Algo 1. One of the main sources of performance improvement for motion imitation is hard-negative mining , where the policy is evaluated regularly to find the failure sequences to train on. Thus, instead of using object curriculum , we use a simple hard-negative mining process to pick hard objects \(}_{}\) to train on. Specifically, let \(s_{j}\) be the number of failed lifts for object \(j\) over all previous runs. The probability of choosing object \(j\) among all objects is \(P(j)=}{_{i}^{s_{i}}}\).

**Object and Humanoid Initial State Randomization**. Since objects can have diverse initial positions and orientations with respect to the humanoid, it is crucial to have the policy exposed to diverse initial object states. Given the object dataset \(}\) and the provided initial states (either from MoCap or by dropping the object in simulation) \(_{0}^{}\), we perturb \(_{0}^{}\) by adding randomly sampled yaw-direction rotation and adjusting the position component \(_{0}^{}\). We do not change the pitch and yaw of the object's initial pose as some poses are invalid in simulation. For the humanoid, we use the initial state from the dataset if provided (_e.g_. GRAB dataset ), and a standing T-pose if there is no paired data.

**Inference**. During inference, the object latent code \(_{t}^{}\), a random object starting pose \(_{0}^{}\), and desired object trajectory \(}_{1:T}^{}\) is all that is required, without any dependency on pre-grasps or paired kinematic human pose.

## 5 Experiments

**Datasets**. We use the GRAB , OakInk , and OMOMO  to study grasping small and large objects. The GRAB dataset contains 1.3k paired full-body motion and object trajectories of 50 objects (we remove the doorknob as it is not movable). Since the GRAB dataset provides reference body and object motion, we use them to extract initial humanoid positions and pre-grasps. We follow prior art  in constructing cross-object (45 tor training and 5 for testing) and cross-subject (9 subjects for training and 1 for testing) train-test sets. On GRAB, we evaluate on following MoCap object trajectories using the mean body shape humanoid. The OakInk dataset contains 1700 diverse objects of 32 categories with real-world scanned and generated object meshes. We split them into 1330 objects for training, 185 for validation, and 185 for testing. Train-test splits are conducted within categories, with train and test splits containing objects from all categories. Since no paired MoCap human motion or grasps exists for the OakInk dataset, we use an off-the-shelf grasp generator  to create pre-grasps. The OMOMO dataset contains 15 large objects (table lamps, monitors, _etc._) with reconstructed mesh, and we pick 7 of them that have cleaner meshes. Due to the limited number of objects from OMOMO, we only test lifting on the objects used for training to verify that our pipeline can learn to move larger objects. On OMOMO and OakInk, we study vertical lifting (30cm) and holding (3s) as the trajectory for quantitative results.

**Implementation Details**. Simulation is conducted in Isaac Gym , where the policy is run at 30 Hz and the simulation at 60 Hz. For PULSE-X and PHC-X, each policy is a 6-layer MLP. For the grasping task, we employ a GRU  based recurrent policy and use a GRU with a latent dimension of 512, followed by a 3-layer MLP. We train Omnigrasp for three days collecting around \(10^{9}\) samples on a Nvidia A100 GPU. PHC-X and PULSE-X are trained once and frozen, which takes around 1.5 weeks and 3 days. Object density is 1000 kg/m\({}^{3}\). The static and dynamic friction coefficients of the object and humanoid fingers are set to 1. For reference object trajectory, we use \(=20\) future frames sampled at 15Hz. For more details, please refer to Appendix C.

**Metrics**. For the object trajectory following, we report the position error \(E_{}\) (mm), rotation error \(E_{}\) (radian), and physics-based metrics such as acceleration error \(E_{}\) (mm/frame\({}^{2}\)) and velocity error \(E_{}\) (mm/frame). Following prior art in full-body simulated humanoid grasping , we report the grasp success rate \(_{}\) and Trajectory Targets Reached (TTR). The grasp success rate \(_{}\) decays a grasp successful when the object is held for at least 0.5s in the physics simulation without dropping. TTR measures the ratio of the target position (< 12cm away from the target position) reached over all the time steps in the trajectory and is only measured on successful trajectories. To measure the complete trajectory success rate, we also report \(_{}\), where a trajectory following is unsuccessful if, at any point in time, the object is > 25cm away from the reference.

    &  \\  Method & Triq & \(_{}\) & \(_{}\) & TTR & \(_{}\) & \(E_{}\) & \(E_{}\) & \(_{}\) & \(_{}\) & TTR & \(_{}\) & \(E_{}\) & \(_{}\) & \(_{}\) & \(_{}\) \\  PP0-10B & Core & 98.46 & 55.99 & 97.53 & 36.4 & **0.4** & 20.1 & 14.5 & 96.98 & 53.24 & 97.98 & 35.6 & **0.5** & 19.6 & 13.9 \\ PHC  & MoCap & 3.66 & 11.48 & 81.15 & 66.3 & 0.8 & 1.5 & 3.8 & 0.3 & 3.3 & 97.45 & 56.5 & 0.3 & 1.4 & 2.9 \\ AMP  & Cars & 90.44 & 46.68 & 94.19 & 40.7 & 0.6 & 5.3 & 59.53 & 49.28 & 96.55 & 34.9 & 0.5 & 6.2 & 6.0 \\ Human _et.al_ & MoCap & -? & - & 55.8 & - & - & - & - & 6.45 & - & 6.55 & - & - & - & - \\  Omnigrasp & MoCap & 94.65 & 84.85 & 98.75 & **28.0** & **0.5** & **4.2** & **4.3** & 95.86 & 85.49 & 98.86 & **27.5** & **0.6** & **5.0** & **5.0** \\ Omnigrasp & Cars & **100\%** & **94.1\%** & **99.6\%** & 30.2 & 0.93 & 5.4 & 4.7 & **98.9\%** & **90.5\%** & **99.5\%** & **27.9** & 0.97 & 6.3 & 5.4 \\   

Table 1: Quantitative results on object grasp and trajectory following on the GRAB dataset.

### Grasping and Trajectory Following

As motion is best seen in videos, please refer to supplement site for extended evaluation on trajectory following, unseen objects, and robustness. Unless otherwise specified, all policies are trained on their respective dataset training split, and we conduct cross-dataset experiments on GRAB and OakInk. All experiments are run 10 times and averaged as the simulator yields slightly different results for each run due to _e.g_. floating-point error. As full-body simulated humanoid grasping is a relatively new task with a limited number of baselines, we use Braun et al  as our main comparison. We also implement AMP  and PHC  as baselines. We train AMP with a similar state and reward design (without using PULSE-X's latent space) and a task and discriminator reward weighting of 0.5 and 0.5. PHC refers to using an imitator for grasping, where we directly feed ground-truth kinematic body and finger motion to a pretrained imitator to grasp objects. Since PHC and PULSE-X require pre-training, we also include PPO-10B, which is trained using RL without PULSE-X for a month (\(\)10 billion samples).

**GRAB Dataset (50 objects)**. Since Braun _et al_. do not use randomly generated trajectories, we train Omnigrasp using two different settings for a fair comparison: one trained with MoCap object trajectories only, and one trained using synthetic trajectories only. From Table 1, we can see that our method outperforms prior SOTA and baselines on all metrics, especially on success rate and trajectory following. Since all methods are simulation-based, we omit penetration/foot sliding metrics and report the precise trajectory tracking errors instead. Training directly using PPO without PULSE-X leads to a performance that significantly lags behind Omnigrasp, even though it has used similar aggregate samples (counting PHC-X and PULSE-X training). Compared to Braun _et al_., Omnigrasp achieves a high success rate on both object lifting and trajectory following. Directly using the motion imitator, PHC, yields a low success rate even when the ground-truth kinematic pose is provided, showing that the imitator's error (on average 30mm) is too large to overcome for precise object grasping. The body shape mismatch between MoCap and our simulated humanoid also contributes to this error. AMP leads to a low trajectory success rate, showing the importance of using a motion prior in the _actions space_. Omnigrasp can track the MoCap trajectory precisely with an average error of 28mm. Comparing training on MoCap trajectories and randomly generated ones, we can see that training on generated trajectories achieves better performance on success rate and position error, though worse on rotation error. This is due to our 3D trajectory generator offering good coverage on physically plausible 3D trajectories, but there is a gap between the randomly generated rotations and MoCap object rotation. This can be improved by introducing more rotation variation on the trajectory generator. The gap between trajectory \(_{}\) and grasp success \(_{}\) shows that following the full trajectory is a much harder task than just grasping, and the object can be dropped during trajectory following. Qualitative results can be found in Fig. 3.

**OakInk Dataset (1700 objects)**. On the OakInk dataset, we scale our grasping policy to >1000 objects and test our generalization to unseen objects. We also conduct cross-dataset experiments, where we train on the GRAB dataset and test on the OakInk dataset. Results are shown in Table 3. We can see that 1272 out of the 1330 objects are trained to be picked up, and the whole lifting process also has a high success rate. We observe similar results on the test split. Upon inspection, the failed objects are usually either too large or too small for the humanoid to establish a grasp. The large number of objects also places a strain on the hard-negative mining process. The policy trained on both GRAB and OakInk shows the highest success rate, as on GRAB, there are bi-manual pre-grasps, and the policy learned to use both hands.

Figure 3: Qualitative results. Unseen objects are tested for GRAB and OakInk. Green dots: reference trajectories. Best seen in videos on our supplement site.

Using both hands significantly improves the success rate on some larger objects, where the humanoid can scoop up the object with one hand and carry it with both. As OakInk only has pre-grasps using one hand, it cannot learn such a strategy. Surprisingly, training on only GRAB achieves a high success rate on OakInk, picking up more than 1000 objects without training on the dataset, showcasing the robustness of our grasping policy on unseen objects.

**OMOMO Dataset (7 objects)**. On the OMOMO dataset, we train a policy to show that our method can learn to pick up large objects. Table 2 shows that our method can successfully learn to pick up all the objects, including chairs and lamps. For larger objects, the pre-grasp guidance is essential for guiding the policy to learn bi-manual manipulation skills (as is shown in Fig 3)

### Ablation and Analysis

**Ablation**. In this section, we study the effects of different components of our framework using the cross-object split of the GRAB dataset. Results are shown in Table 4. First, we compare our method trained with (Row 6) or without (R1) PULSE-X's action space. Using the same reward and state design, we can see that using the universal motion prior significantly improves success rates. Upon inspection, using PULSE-X also yields human-like motion, while not using it leads to unnatural motion (see in supplement site). R2 vs. R6 shows that the pre-grasp guidance is essential in learning grasps that are stable for grasping objects, but without it, some objects can still be grasped successfully. The difference between R3 and R6 is whether to train using the dexterous AMASS dataset. R3 vs R6 shows that without training on a dataset that has diverse hand motion and full-body motion, the policy can learn to pick up objects (high grasp success rate), but struggles in trajectory following. This is expected as the motion prior probably lacks the motion of "holding the object while moving". R4 and R5 show that object position randomization and hard-negativity mining are crucial for learning robust and successful policies. Ablations on the object latent code, RNN policy, _etc_. can be found in the Appendix C.

**Analysis: Diverse Grasps**. In Fig. 4, we visualize the grasping strategy used by our method. We can see that based on the object shape, our policy uses a diverse set of grasping strategies to hold the object during the trajectory following. Based on the trajectory and object initial pose, Omnigrasp discovers different grasping poses for the _same_ object, showcasing the advantage of using simulation and laws of physics for grasp generation. We also notice that for larger objects, our policy will resort to using two hands and a non-prehensile transport strategy. This behavior is learned from pre-grasps in GRAB, which utilize both hands for object manipulation.

**Analysis: Robustness and Potential for Sim-to-real Transfer**. In Table 5, we add uniform random noise [-0.01, 0.01] to both task observation (positions, object latent codes, etc.) and proprioception. A similar scale (0.01) of random noise is used in sim-to-real RL to tackle noisy input in real-world humanoids . We see that Omnigrasp is relatively robust to input noise, even though it has not been trained with noisy input. Performance drop is more prominent in the acceleration and velocity metrics. Adding noise during training can further improve robustness. We do not claim that Omnigrasp is currently ready for real-world deployment, but we

    \\   \(_{}\) & \(_{}\) & \(\) & \(E_{}\) & \(E_{}\) & \(E_{}\) & \(}}\) & \(}}\) \\ 
77 & 77 & 77 & 100\% & 22.8 & 0.2 & 3.1 & 3.3 \\   

Table 2: Quantitative results on the OMOMO dataset.

    &  \\   Training Data & \(_{}\) & \(_{}\) & \(\) & \(E_{}\) & \(E_{}\) & \(E_{}\) & \(}}\) & \(}}\) & \(}}\) & \(}}\) & \(}}\) & \(}}\) \\  OakInk & 93.7\% & 86.2\% & **100\%** & 21.0 & **04.7** & 7.6 & 6.0 & **94.3\%** & 87.5\% & **100\%** & **21.2** & **04.4** & 7.6 & 5.9 \\ GRAB & 83.4\% & 75.2\% & 99.9\% & 22.4 & **0.4** & 6.8 & 5.7 & 81.9\% & 72.1\% & 99.9\% & 22.7 & **0.4** & 7.1 & 5.8 \\ GRAB + OakInk & **95.6\%** & **92.0\%** & **100\%** & **21.0** & 0.6 & **5.4** & **4.8** & 93.5\% & **89.0\%** & **100\%** & 21.3 & 0.6 & **5.4** & **4.8** \\   

Table 3: Quantitative results on OakInk with our method. We also test Omnigrasp cross-dataset, where a policy trained on GRAB is tested on the OakInk dataset.

    \\   \(_{}\) & \(_{}\) & \(\) & \(E_{}\) & \(E_{}\) & \(E_{}\) & \(}}\) & \(}}\) \\ 
77 & 77 & 77 & 100\% & 22.8 & 0.2 & 3.1 & 3.3 \\   

Table 4: Ablation on various strategies of training Omnigrasp. PULSE-X: whether to use the latent motion representation. pre-grasp: pre-grasp guidance reward. Dex-AMASS: whether to train PULSE-X on the dexterous AMASS dataset. Rand-pose: randomizing the object initial pose. Hard-neg: hard-negative mining.

believe that a similar system design plus sim-to-real modifications (e.g. domain randomization, distilling into a vision-based policy) has the potential. We conduct more analysis on the robustness of our method with respect to initial object position, object weight, and object trajectories on our supplement site.

## 6 Limitations, Conclusions, and Future Work

**Limitations**. While Omnigrasp demonstrates the feasibility of controlling a simulated humanoid to grasp diverse objects and hold them to follow diverse trajectories, many limitations remain. For example, though the 6DoF input is provided in the input and reward, the rotation error remains to be further improved. Omnigrasp has yet to support precise in-hand manipulations. The success rate on trajectory following can be improved, as objects can be dropped or not picked up. Another area of improvement is to achieve _specific_ types of grasps on the object, which may require additional input such as desired contact points and grasp. Human-level dexterity, even in simulation, remains challenging. For visualization of failure cases, see supplement site.

**Conclusion and Future Work**. In conclusion, we present Omnigrasp, a humanoid controller capable of grasping \(>1200\) objects and following trajectories while holding the object. It generalizes to unseen objects of similar sizes, utilizes bi-manual skills, and supports picking up larger objects. We demonstrate that by using a pretrained universal humanoid motion representation, grasping can be learned using simplistic reward and state designs. Future work includes improving trajectory following success rate, improving grasping diversity, and supporting more object categories. Also, improving upon the humanoid motion representation is a promising direction. While we utilize a simple yet effective unified motion latent space, separating the motion representation for hands and body  could lead to further improvements. Effective object representation is also an important future direction. How to formulate an object representation that does not rely on canonical object pose and generalizes to vision-based systems will be valuable to help the model generalize to more objects.