# Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to Harness Spurious Features

Cian Eastwood\({}^{*}\)1,2

Shashank Singh\({}^{*}\)1

Andrei L. Nicolicioiu\({}^{1}\)   Marin Vlastelica\({}^{1}\)   Julius von Kugelgen\({}^{1,3}\)   Bernhard Scholkopf\({}^{1}\)

\({}^{1}\) Max Planck Institute for Intelligent Systems, Tubingen

\({}^{2}\) University of Edinburgh  \({}^{3}\) University of Cambridge

Equal contribution. Correspondence to c.eastwood@ed.ac.uk or shashankssingh44@gmail.com.

###### Abstract

To avoid failures on out-of-distribution data, recent works have sought to extract features that have an invariant or _stable_ relationship with the label across domains, discarding "spurious" or _unstable_ features whose relationship with the label changes across domains. However, unstable features often carry _complementary_ information that could boost performance if used correctly in the test domain. In this work, we show how this can be done _without test-domain labels_. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt the unstable-feature predictions in the test domain. Theoretically, we prove that SFB can learn an asymptotically-optimal predictor without test-domain labels. Empirically, we demonstrate the effectiveness of SFB on real and synthetic data.

## 1 Introduction

Machine learning systems can be sensitive to distribution shift . Often, this sensitivity is due to a reliance on "spurious" features whose relationship with the label changes across domains, ultimately leading to degraded performance in the test domain of interest . To avoid this pitfall, recent works on domain or out-of-distribution (OOD) generalization have sought predictors which only make use of features that have a _stable_ or invariant relationship with the label across domains, discarding the spurious or _unstable_ features . However, despite their instability, spurious features can often provide additional or _complementary_ information about the target label. Thus, if a predictor could be adjusted to use spurious features optimally in the test domain, it would boost performance substantially. That is, perhaps we don't need to discard spurious features at all but rather _use them in the right way_.

As a simple but illustrative example, consider the ColorMNIST or CMNIST dataset . This transforms the original MNIST dataset into a binary classification task (digit in 0-4 or 5-9) and then: (i) flips the label with probability 0.25, meaning that, across all 3 domains, digit shape correctly determines the label with probability 0.75; and (ii) colorizes the digit such that digit color (red or green) is a more informative but spurious feature (see Fig. 0(a)). Prior work focused on learning an invariant predictor that uses only shape and avoids using color--a spurious feature whose relationship with the label changes across domains. However, as shown in Fig. 0(b), the invariant predictor is suboptimal test domains where color can be used in a domain-specific manner to improve performance. We thus ask: when and how can such informative but spurious features be safely harnessed _without labels_?Our main contribution lies in answering this question, showing when and how it is possible to safely harness spurious or _unstable_ features without test-domain labels. In particular, we prove that predictions based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label (see Fig. 0(c)).

**Structure and contributions.** The rest of this paper is organized as follows. We first discuss related work in SS 2, providing context and high-level motivation for our approach. In SS 3, we then explain how stable and unstable features can be extracted, how unstable features can be harnessed _with_ test-domain labels, and the questions/challenges that arise when trying to harness unstable features _without_ test-domain labels. In SS 4, we present our main theoretical contributions which provide precise answers to these questions, before using these insights to propose the Stable Feature Boosting (SFB) algorithm in SS 5. In SS 6, we present our experimental results, before ending with a discussion and concluding remarks in SS 7. Our contributions can be summarized as follows:

* **Algorithmic:** We propose Stable Feature Boosting (SFB), the first algorithm for using invariant predictions to safely harness spurious features _without test-domain labels_.
* **Theoretical:** SFB is grounded in a novel theoretical result (Thm 4.4) giving sufficient conditions for provable test-domain adaptation without labels. Under these conditions, Thm 4.6 shows that, given enough unlabeled data, SFB learns the Bayes-optimal adapted classifier in the test domain.
* **Experimental:** Our experiments on synthetic and real-world data demonstrate the effectiveness of SFB--even in scenarios where it is unclear if its assumptions are fully satisfied.

## 2 Related Work

**Domain generalization, robustness and invariant prediction.** A fundamental starting point for work in domain generalization is the observation that certain "stable" features, often direct causes of the label, may have an invariant relationship with the label across domains [45; 1; 67; 55; 40; 77; 14]. However, such stable or invariant predictors often discard highly informative but unstable information. Rothenhausler et al.  show that we may need to trade off stability and predictiveness, while Eastwood et al.  seek such a trade-off via an interpretable probability-of-generalization parameter. The current work is motivated by the idea that one might avoid such a trade-off by changing how unstable features are used at test time, rather than discarding them at training time.

**Test-domain adaptation _without labels_ (unsupervised domain adaptation).** In the source-free and test-time domain adaptation literature, it is common to adapt to new domains using a model's own pseudo-labels [20; 36; 39; 71; 30]--see Rusak et al.  for a recent review. In contrast, we: (i) use one (stable) model to provide reliable/robust pseudo-labels and another (unstable) model to adapt domain-specific information; and (ii) propose a bias correction step that provably ensures an accurate, well-calibrated unstable model (\([Y|X_{U}]\)) as well as an optimal joint/combined model (\([Y|X_{S},X_{U}]\)). Beyond this literature, Bui et al.  propose a meta-learning approach for exploiting unstable/domain-specific features. However, they use unstable features _in the same way_ in the test domain, which, by definition, is not robust and can degrade performance. Sun et al.  share the goal of exploiting unstable features to go "beyond invariance". However, in contrast to our approach, they require labels for the unstable features (rarely available) and only address label shifts.

Figure 1: **Invariant (stable) and spurious (unstable) features. (a) Illustrative images from CMUIST. (b) CMUIST accuracies (y-axis) over test domains of decreasing color-label correlation (x-axis). The ‘Oracle’ uses both invariant (shape) _and_ spurious (color) features optimally in the test domain, boosting performance over an invariant model (orange region). We show how this can be done _without test-domain labels_. (c) Generally, invariant models use only the _stable_ component \(X_{S}\) of \(X\), discarding the spurious or _unstable_ component \(X_{U}\). We prove that predictions based on \(X_{S}\) can be used to safely harness a sub-component of \(X_{U}\) (dark-orange region).**

**Test-domain adaptation _with labels_ (few-shot fine-tuning).** Fine-tuning part of a model using a small number of labeled test-domain examples is a common way to deal with distribution shift [16; 17; 13]. More recently, it has been shown that simply retraining the last layer of an ERM-trained model outperforms more robust feature-learning methods on spurious correlation benchmarks [50; 32; 74]. Similar to our approach, Jiang and Veitch  separate stable and conditionally-independent unstable features and then adapt their use of the latter in the test domain. However, in contrast to our approach, theirs requires test-domain labels. In addition, they assume data is drawn from an anti-causal generative model, which is strictly stronger than our "complementarity" assumption (see SS 4).

Table 1 summarizes related work while App. H discusses further related work.

## 3 Problem Setup: Extracting and Harnessing Unstable Features

**Setup.** We consider the problem of domain generalization (DG) [8; 42; 24] where predictors are trained on data from multiple training domains and with the goal of performing well on data from unseen test domains. More formally, we consider datasets collected from different training domains or _environments_\(_{}:=\{E_{1},,E_{m}\}\), with each dataset containing data pairs sampled i.i.d. from to learn a predictor \(f(X)\) that performs well on a larger set \(_{}_{}\) of possible domains.

**Average performance: use all features.** The first approaches to DG sought predictors that perform well _on average_ over domains [8; 42] using empirical risk minimization (ERM, ). However, predictors that perform well on average provably lack robustness [43; 49], potentially performing quite poorly on large subsets of \(_{}\). In particular, minimizing the average error leads predictors to make use of any features that are informative about the label (on average), including "spurious" or "shortcut"  features whose relationship with the label is subject to change across domains. In test domains where these feature-label relationships change in new or more severe ways than observed during training, this usually leads to significant performance drops or even complete failure [73; 4].

**Worst-case or robust performance: use only stable features.** To improve robustness, subsequent works sought predictors that only use _stable or invariant_ features, i.e., those that have a stable or invariant relationship with the label across domains [45; 1; 47; 70; 58]. For example, Arjovsky et al.  do so by enforcing that the classifier on top of these features is optimal for all domains simultaneously. We henceforth use _stable features_ and \(X_{S}\) to refer to these features, and stable predictors to refer to predictors which use only these features. Analogously, we use _unstable features_ and \(X_{U}\) to refer to features with an unstable or "spurious" relationship with the label across domains. Note that \(X_{S}\) and \(X_{U}\) partition the components of \(X\) which are informative about \(Y\), as depicted in Fig. 0(c), and that formal definitions of \(X_{S}\) and \(X_{U}\) are provided in SS 4.

### Harnessing unstable features _with labels_

A stable predictor \(f_{S}\) is unlikely to be the best predictor in any given domain. As illustrated in Fig. 0(b), this is because it excludes unstable features \(X_{U}\) which are informative about \(Y\) and can boost performance _if used in an appropriate, domain-specific manner_. Assuming we can indeed learn a stable predictor with prior methods, we start by showing how \(X_{U}\) can be harnessed _with test-domain labels_.

    &  &  &  \\    & **Stable** & **Complementary** & & **All** & **Robust** & **No test-domain labels** \\  ERM  & ✓ & ✓ & ✓ & ✗ & ✓ \\ IRM  & ✓ & ✗ & ✗ & ✓ & ✓ \\ QRM  & ✓ & ✓\({}^{*}\) & ✓\({}^{*}\) & ✓\({}^{*}\) & ✓ \\ DARE  & ✓ & ✓ & ✓ & ✓ & ✗ \\ ACTIR  & ✓ & ✓ & ✗ & ✓ & ✗ \\  SFB (**Ours**) & ✓ & ✓ & ✗ & ✓ & ✓ \\   

Table 1: **Comparison with related work.**\({}^{*}\)QRM  uses a hyperparameter \(\) to balance the probability of robust generalization and using more information from \(X\).

**Boosting the stable predictor.** We describe boosted joint predictions \(f^{e}(X)\) in domain \(e\) as some combination \(C\) of stable predictions \(f_{S}(X)\) and domain-specific unstable predictions \(f^{e}_{U}(X)\), i.e., \(f^{e}(X)=C(f_{S}(X),f^{e}_{U}(X))\). To allow us to adapt only the \(X_{U}\)-\(Y\) relation, we decompose the stable \(f_{S}(X)=h_{S}(_{S}(X))\) and unstable \(f^{e}_{U}(X)=h^{e}_{U}(_{U}(X))\) predictions into feature extractors \(\) and classifiers \(h\). \(_{S}\) extracts stable components \(X_{S}=_{S}(X)\) of \(X\), \(_{U}\) extracts unstable components \(X_{U}=_{U}(X)\) of \(X\), \(h_{S}\) is a classifier learned on top of \(_{S}\) (shared across domains), and \(h^{e}_{U}\) is a _domain-specific_ unstable classifier learned on top of \(_{U}\) (one per domain). Putting these together,

\[f^{e}(X)=C(f_{S}(X),f^{e}_{U}(X))=C(h_{S}(_{S}(X)),h^{e}_{U}(_{U}(X))) =C(h_{S}(X_{S}),h^{e}_{U}(X_{U})),\] (3.1)

where \(C:\) is a _combination function_ that combines the stable and unstable predictions. For example, Jiang and Veitch [31, Eq. 2.1] add stable \(p_{S}\) and unstable \(p_{U}\) predictions in logit space, i.e., \(C(p_{S},p_{U})\!=\!((p_{S})+(p_{U}))\). Since it is unclear, _a priori_, how to choose \(C\), we will leave it unspecified until Thm. 4.4 in SS 4, where we derive a principled choice.

**Adapting with labels.** Given a new domain \(e\) and labels \(Y^{e}\), we can boost performance by adapting \(h^{e}_{U}\). Specifically, letting \(:\) be a loss function (e.g., cross-entropy) and \(R^{e}(f)=_{(X,Y)}[(Y,f(X))|E=e]\) the risk of predictor \(f:\) in domain \(e\), we can adapt \(h^{e}_{U}\) to solve:

\[_{h_{U}}R^{e}(C(h_{S}_{S},h_{U}_{U}))\] (3.2)

### Harnessing unstable features _without labels_

We now consider the main question of this work--can we reliably harness \(X_{U}\)_without_ test-domain labels? We could, of course, simply select a _fixed_ unstable classifier \(h^{e}_{U}\) by relying solely on the training domains (e.g., by minimizing average error), and hope that this works for the test-domain \(X_{U}\)-\(Y\) relation. However, by definition of \(X_{U}\) being unstable, this is clearly not a robust or reliable approach--the focus of our efforts in this work, as illustrated in Table 1. As in SS 3.1, we assume that we are able to learn a stable predictor \(f_{S}\) using prior methods, e.g., IRM  or QRM .

**From stable predictions to robust pseudo-labels.** While we don't have labels in the test domain, we _do_ have stable predictions. By definition, these are imperfect (i.e., _noisy_) but robust, and can be used to form _pseudo-labels_\(_{i}=_{j}(f_{S}(X_{i}))_{j}\), with \((f_{S}(X_{i}))_{j}[Y_{i}=j|X_{S}]\) denoting the \(j^{}\) entry of the stable prediction for \(X_{i}\). Can we somehow use these noisy but robust pseudo-labels to guide our updating of \(h^{e}_{U}\), and, ultimately, our use of \(X_{U}\) in the test domain?

**From joint to unstable-only risk.** If we simply use our robust pseudo-labels as if they were true labels--updating \(h^{e}_{U}\) to minimize the joint risk as in Eq. (3.2)--we arrive at trivial solutions since \(f_{S}\) already predicts its own pseudo-labels with 100% accuracy. For example, if we follow [31, Eq. 2.1] and use the combination function \(C(p_{S},p_{U})=((p_{S})+(p_{U}))\), then the trivial solution \((h^{e}_{U}())\!=\!0\) achieves 100% accuracy (and minimizes cross-entropy; see Prop. D.1 of App. D). Thus, we cannot minimize a joint loss involving \(f_{S}\)'s predictions when using \(f_{S}\)'s pseudo-labels. A sensible alternative is to update \(h^{e}_{U}\) to minimize the _unstable-only risk_\(R^{e}(h^{e}_{U}_{U})\).

**More questions than answers.** While this new procedure _could_ work, it raises questions about _when_ it will work, or, more precisely, the conditions under which it can be used to safely harness \(X_{U}\). We now summarise these questions before addressing them in SS 4:

1. **Does it make sense to minimize the unstable-only risk?** In particular, when can we minimize the unstable-predictor risk _alone_ or separately, and then arrive at the optimal joint predictor? This cannot always work; e.g., for independent \(X_{S},X_{U}(1/2)\) and \(Y=X_{S}\) XOR \(X_{U}\), \(Y\) is independent of each of \(X_{S}\) and \(X_{U}\) and hence cannot be predicted from either alone.
2. **How should we combine predictions?** Is there a principled choice for the combination function \(C\) in Eq. (3.1)? In particular, is there a \(C\) that correctly weights stable and unstable predictions in the test domain? As \(X_{U}\) could be very strongly or very weakly predictive of \(Y\) in the test domain, this seems a difficult task. Intuitively, correctly weighting stable and unstable predictions requires them to be properly calibrated: do we have any reason to believe that, after training on \(f_{S}\)'s pseudo-labels, \(h^{e}_{U}\) will be properly calibrated in the test domain?
3. **Can the student outperform the teacher?** Stable predictions likely make mistakes--indeed, this is the motivation for trying to improve them. Is it possible to correct these mistakes with \(X_{U}\)? Is itpossible to learn an unstable "student" predictor that outperforms its own supervision signal or "teacher"? Perhaps surprisingly, we show that, for certain types of features, the answer is yes. In fact, even a very weak stable predictor, with performance just above chance, can be used to learn an _optimal_ unstable classifier in the test domain given enough unlabeled data.

## 4 Theory: When Can We Safely Harness Unstable Features Without Labels?

Suppose we have already identified a stable feature \(X_{S}\) and a potentially unstable feature \(X_{U}\) (we will return to the question of how to learn/extract \(X_{S}\) and \(X_{U}\) themselves in SS 5). In this section, we analyze the problem of using \(X_{S}\) to leverage \(X_{U}\) in the test domain without labels. We first reduce this to a special case of the so-called "marginal problem" in probability theory, i.e., the problem of identifying a joint distribution based on information about its marginals. In the special case where two variables are conditionally independent given a third, we show this problem can be solved exactly. This solution, which may be of interest beyond the context of domain generalization/adaptation, motivates our test-domain adaptation algorithm (Alg. 1), and forms the basis of Thm. 4.6 which shows that Alg. 1 converges to the best possible classifier given enough unlabeled data.

We first pose a population-level model of our domain generalization setup. Let \(E\) be a random variable denoting the _environment_. Given an environment \(E\), we have that the stable feature \(X_{S}\), unstable feature \(X_{U}\) and label \(Y\) are distributed according to \(P_{X_{S},X_{U},Y|E}\). We can now formalize the three key assumptions underlying our approach, starting with the notion of a stable feature, motivated in SS 3:

**Definition 4.1** (Stable and Unstable Features).: \(X_{S}\) _is a stable feature with respect to \(Y\) if \(P_{Y|X_{S}}\) does not depend on \(E\); equivalently, if \(Y\) and \(E\) are conditionally independent given \(X_{S}\) (\(Y\!\!\! E|X_{S}\)). Conversely, \(X_{U}\) is an unstable feature with respect to \(Y\) if \(P_{Y|X_{U}}\) depends on \(E\); equivalently, if \(Y\) and \(E\) are conditionally dependent given \(X_{U}\) (\(Y\!\!\! E|X_{U}\))._

Next, we state our complementarity assumption, which we will show justifies the approach of separately learning the relationships \(X_{S}\)-\(Y\) and \(X_{U}\)-\(Y\) and then combining them:

**Definition 4.2** (Complementary Features).: \(X_{S}\) _and \(X_{U}\) are complementary features with respect to \(Y\) if \(X_{S}\!\!\! X_{U}|(Y,E)\); i.e., if \(X_{S}\) and \(X_{U}\) share no redundant information beyond \(Y\) and \(E\)._

Finally, to provide a useful signal for test-domain adaptation, the stable feature needs to help predict the label in the test domain. Formally, we assume:

**Definition 4.3** (Informative Feature).: \(X_{S}\) _is said to be informative of \(Y\) in environment \(E\) if \(X_{S}\!\!\! Y|E\); i.e., \(X_{S}\) is predictive of \(Y\) within the environment \(E\)._

We will discuss the roles of these assumptions after stating our main result (Thm. 4.4) that uses them. To keep our results as general as possible, we avoid assuming a particular causal generative model, but the above conditional (in)dependence assumptions can be interpreted as constraints on such a causal model. App. D.2 formally characterizes the set of causal models that are consistent with our assumptions and shows that our setting generalizes those of prior works [49; 68; 31; 69].

**Reduction to the marginal problem with complementary features.** By Defn. 4.1, we have the same stable relationship \(P_{Y|X_{S},E}\!=\!P_{Y|X_{S}}\) in training and test domains. Now, suppose we have used the training data to learn this stable relationship and thus know \(P_{Y|X_{S}}\). Also suppose that we have enough unlabeled data from test domain \(E\) to learn \(P_{X_{S},X_{U}|E}\), and recall that our ultimate goal is to predict \(Y\) from \((X_{S},X_{U})\) in test domain \(E\). Since the rest of our discussion is conditioned on \(E\) being the test domain, we omit \(E\) from the notation. Now note that, if we could express \(P_{Y|X_{S},X_{U}}\) in terms of \(P_{Y|X_{S}}\) and \(P_{X_{S},X_{U}}\), we could then use \(P_{Y|X_{S},X_{U}}\) to optimally predict \(Y\) from \((X_{S},X_{U})\). Thus, our task thus becomes to reconstruct \(P_{Y|X_{S},X_{U}}\) from \(P_{Y|X_{S}}\) and \(P_{X_{S},X_{U}}\). This is an instance of the classical "marginal problem" from probability theory [28; 29; 19], which asks under which conditions we can recover the joint distribution of a set of random variables given information about its marginals. In general, although one can place bounds on the conditional distributions \(P_{Y|X_{U}}\) and \(P_{Y|X_{S},X_{U}}\), they cannot be completely inferred from \(P_{Y|X_{S}}\) and \(P_{X_{S},X_{U}}\). However, the following section demonstrates that, _under the additional assumptions that \(X_{S}\) and \(X_{U}\) are complementary and \(X_{S}\) is informative_, we can exactly recover \(P_{Y|X_{U}}\) and \(P_{Y|X_{S},X_{U}}\) from \(P_{Y|X_{S}}\) and \(P_{X_{S},X_{U}}\).

### Solving the marginal problem with complementary features

We now present our main result which shows how to reconstruct \(P_{Y|X_{S},X_{U}}\) from \(P_{Y|X_{S}}\) and \(P_{X_{S},X_{U}}\) when \(X_{S}\) and \(X_{U}\) are complementary and \(X_{S}\) is informative. To simplify notation, we assume the label \(Y\) is binary and defer the multi-class extension to App. C.

**Theorem 4.4** (Solution to the marginal problem with binary labels and complementary features).: _Consider three random variables \(X_{S}\), \(X_{U}\), and \(Y\), where (i) \(Y\) is binary \((\{0,1\}\)-valued), (ii) \(X_{S}\) and \(X_{U}\) are complementary features for \(Y\) (i.e., \(X_{S}\!\!\! X_{U}|Y\)), and (iii) \(X_{S}\) is informative of \(Y\) (\(X_{S}\!\!\! Y\)). Then, the joint distribution of \((X_{S},X_{U},Y)\) can be written in terms of the joint distributions of \((X_{S},Y)\) and \((X_{S},X_{U})\). Specifically, if \(|X_{S}([Y=1|X_{S}])\) is a pseudo-label3 and_

\[_{0}:=[=0|Y=0]_{1}: =[=1|Y=1]\] (4.1)

_are the accuracies of the pseudo-labels on classes \(0\) and \(1\), respectively. Then, we have:_

\[_{0}+_{1}>1,\] (4.2)

\[[Y=1|X_{U}]==1|X_{U}]+_{0}-1}{_{0}+ _{1}-1},\] (4.3)

\[[Y=1|X_{S},X_{U}]\!=\!(([Y\!=\!1|X_{S}])+([Y\!=\!1|X_{U}])-([Y\!=\!1])).\] (4.4)

Intuitively, suppose we generate pseudo-labels \(\) based on feature \(X_{S}\) and train a model to predict \(\) using feature \(X_{U}\). For complementary \(X_{S}\) and \(X_{U}\), Eq. (4.3) shows how to transform this into a prediction of the _true_ label \(Y\), correcting for differences between \(\) and \(Y\). Crucially, given the conditional distribution \(P_{Y|X_{S}}\) and observations of \(X_{S}\), we can estimate class-wise pseudo-label accuracies \(_{0}\) and \(_{1}\) in Eq. (4.3) even without new labels \(Y\) (see App. A.1, Eq. (A.2)). Finally, Eq. (4.4) shows how to weight predictions based on \(X_{S}\) and \(X_{U}\), justifying the combination function

\[C_{p}(p_{S},p_{U})=((p_{S})+(p_{U})- (p))\] (4.5)

in Eq. (3.1), where \(p=[Y=1]\) is a constant independent of \(x_{S}\) and \(x_{U}\). We now sketch the proof of Thm. 4.4, elucidating the roles of informativeness and complementarity (full proof in App. A.1).

Proof Sketch of Thm. 4.4.: We prove Eq. (4.2), Eq. (4.3), and Eq. (4.4) in order.

Proof of Eq. (4.2):The informativeness condition (iii) is equivalent to the pseudo-labels having predictive accuracy above random chance; formally, App. A.1 shows:

**Lemma 4.5**.: \(_{0}+_{1}>1\) _if and only if \(X_{S}\) is informative of \(Y\) (i.e., \(X_{S}\!\!\! Y\))._

Together with Eq. (4.3), it follows that _any_ dependence between \(X_{S}\) and \(Y\) allows us to fully learn the relationship between \(X_{U}\) and \(Y\), affirmatively answering our question from SS 3: _Can the student outperform the teacher?_ While a stronger relationship between \(X_{S}\) and \(Y\) is still helpful, it only improves the (unlabeled) _sample complexity_ of learning \(P_{Y|X_{U}}\) and not _consistency_ (Thm. 4.6 below), mirroring related results in the literature on learning from noisy labels . In particular, a weak relationship corresponds to \(_{0}+_{1} 1\), increasing the variance of the bias-correction in Eq. (4.3). With a bit more work, one can formalize this intuition to show that our approach has a relative statistical efficiency of \(_{0}+_{1}-1\), compared to using true labels \(Y\).

Proof of Eq. (4.3):The key observation behind the bias-correction (Eq. (4.3)) is that, due to complementarity (\(X_{S}\!\!\! X_{U}|Y\)) and the fact that the pseudo-label \(\) depends only on \(X_{S}\), \(\) is conditionally independent of \(X_{U}\) given the true label \(Y\) (\(\!\!\! X_{U}|Y\)); formally:

\[[=1|X_{U}] =[=1|Y=0,X_{U}][Y=0|X_{U}]\] \[+[=1|Y=1,X_{U}][Y=1|X_{U}]\] \[=[=1|Y=0][Y=0|X_{U}]\] \[+[=1|Y=1][Y=1|X_{U}] \] \[=(_{0}+_{1}-1)[Y=1|X_{U}]+1-_{0}. $ and $_{1}$)}\]Here, complementarity allowed us to approximate the unknown \([=1|Y=0,X_{U}]\) by its average \([=1|Y=0]=_{X_{U}}[[=1|Y=0,X_{U}]]\), which depends only on the known distribution \(P_{X_{S},Y}\). By informativeness, Lemma 4.5 allows us to divide by \(_{0}+_{1}-1\), giving Eq. (4.3).

``` Input: Calibrated stable classifier \(f_{S}(x_{S})=[Y=1|X_{S}=x_{S}]\), unlabelled data \(\{(X_{S,i},X_{U,i})\}_{i=1}^{n}\) Output: Joint classifier \((x_{S},x_{U})\) estimating \([Y=1|X_{S}=x_{S},X_{U}=x_{U}]\)
1 Compute soft pseudo-labels (PLs) \(\{\}_{i=1}^{n}\) with \(_{i}=f_{S}(X_{S,i})\)
2 Compute soft class-\(1\) count \(n_{1}=_{i=1}^{n}_{i}\)
3 Estimate PL accuracies \((_{0},_{1})\!=\!(}_{i=1}^{n}(1\!- \!_{i})(1\!-\!f_{S}(X_{S,i})),}_{i=1}^{n} {Y}_{i}f_{S}(X_{S,i}))\) // Eq. (4.1) Fit unstable classifier \(_{U}(x_{U})\) to pseudo-labelled data \(\{(X_{U,i},_{i})\}_{i=1}^{n}\) // \([\!=\!1|X_{U}\!=\!x_{U}]\)
4 Bias-correct \(_{U}(x_{U})\{0,\{1,_{U}( x_{U})+_{0}-1}{_{0}+_{1}-1}\}\}\) // Eq. (4.3), \([Y\!=\!1|X_{U}\!=\!x_{U}]\)
5 return\((x_{S},x_{U})\!\!C_{}{n}}(f_{S}(x_{S}), _{U}(x_{U}))\) // Eq. (4.4)/(4.5), \([Y\!=\!1|X_{S}\!=\!x_{S},X_{U}\!=\!x_{U}]\) ```

**Algorithm 1**Bias-corrected adaptation procedure. Multi-class version given by Algorithm 2.

**Proof of Eq. (4.4):** While the exact proof of Eq. (4.4) is a bit more algebraically involved, the key idea is simply that complementarity allows us to decompose \([Y|X_{S},X_{U}]\) into separately-estimatable terms \([Y|X_{S}]\) and \([Y|X_{U}]\): for any \(y\),

\[[Y=y|X_{S},X_{U}] _{X_{S},X_{U}}[X_{S},X_{U}|Y=y][Y=y]\] (Bayes' Rule) \[=[X_{S}|Y=y][X_{U}|Y=1][Y=y]\] (Complementarity) \[_{X_{S},X_{U}}][Y=1|X_{U}]}{[Y=1]},\] (Bayes' Rule)

where, \(_{X_{S},X_{U}}\) denotes proportionality with a constant depending only on \(X_{S}\) and \(X_{U}\), not on \(y\). Directly estimating these constants involves estimating the density of \((X_{S},X_{U})\), which may be intractable without further assumptions. However, in the binary case, since \(1-[Y=1|X_{S},X_{U}]=[Y=0|X_{S},X_{U}]\), these proportionality constants conveniently cancel out when the above relationship is written in logit-space, as in Eq. (4.4). In the multi-class case, App. C shows how to use the constraint \(_{y}[Y=y|X_{S},X_{U}]=1\) to avoid computing the proportionality constants. 

### A provably consistent algorithm for unsupervised test-domain adaptation

Having learned \(P_{Y|X_{S}}\) from the training domain(s), Thm. 4.4 implies we can learn \(P_{Y|X_{S},X_{U}}\) in the test domain by learning \(P_{X_{S},X_{U}}\)--the latter only requiring _unlabeled_ test-domain data. This motivates our Alg. 1 for test-domain adaptation, which is a finite-sample version of the bias-correction and combination equations (Eqs. (4.3) and (4.4)) in Thm. 4.4. Alg. 1 comes with the following guarantee:

**Theorem 4.6** (Consistency Guarantee, Informal).: _Assume (i) \(X_{S}\) is stable, (ii) \(X_{S}\) and \(X_{U}\) are complementary, and (iii) \(X_{S}\) is informative of \(Y\) in the test domain. As \(n\), if \(_{U}[=1|X_{U}]\) then \([Y=1|X_{S},X_{U}]\)._

In words, as the amount of unlabeled data from the test domain increases, if the unstable classifier on Line 4 of Alg. 1 learns to predict the pseudo-label \(\), then the joint classifier output by Alg. 1 learns to predict the true label \(Y\). Convergence in Thm. 4.6 occurs \(P_{X_{S},X_{U}}\)-a.e., both weakly (in prob.) and strongly (a.s.), depending on the convergence of \(_{U}\). Formal statements and proofs are in Appendix B.

## 5 Algorithm: Stable Feature Boosting (SFB)

Using theoretical insights from SS 4, we now propose Stable Feature Boosting (SFB): an algorithm for safely harnessing unstable features without test-domain labels. We first describe learning a stable predictor and extracting complementary unstable features from the training domains. We then describe how to use these with Alg. 1, adapting our use of the unstable features to the test domain.

**Training domains: Learning stable and complementary features.** Using the notation of Eq. (3.1), our goal on the training domains is to learn stable and unstable features \(_{S}\) and \(_{U}\), a stable predictor \(f_{S}\), and domain-specific unstable predictors \(f_{U}^{}\) such that:

1. \(f_{S}\) is stable, informative, and calibrated (i.e., \(f_{S}(x_{S})=[Y=1|X_{S}=x_{S}]\)).
2. In domain \(e\), \(f_{U}^{}\) boosts \(f_{s}\)'s performance with complementary \(_{U}(X^{e})\!\!\!\!\!\!\!_{S}(X^{e})|Y^{e}\).

To achieve these learning goals, we propose the following objective:

\[_{_{S},_{U},h_{S},h_{U}^{}}& _{e_{u}}R^{}(h_{S}_{S})+R^{}(C(h_{S} _{S},h_{U}^{}_{U}))\\ &+_{S} P_{}(_{S},h_{S},R^{ })+_{C} P_{}(_{S}(X^{e}),_{U}(X^{ }),Y^{e})\] (5.1)

The first term encourages good stable predictions \(f_{S}(X)=h_{S}(_{S}(X))\) while the second encourages improved domain-specific joint predictions \(f^{}(X^{e})=C(h_{S}(_{S}(X^{e})),h_{U}^{}(_{U}(X^{e})))\) via a domain-specific use \(h_{U}^{}\) of the unstable features \(_{U}(X^{e})\). For binary \(Y\), the combination function \(C\) takes the simplified form of Eq. (4.5). Otherwise, \(C\) takes the more general form of Eq. (C.1). \(P_{}\) is a penalty encouraging stability while \(P_{}\) is a penalty encouraging complementarity or conditional independence, i.e., \(_{U}(X^{e})\!\!\!_{S}(X^{e})|Y^{e}\). Several approaches exist for enforcing stability [1; 35; 58; 47; 15; 67; 40] (e.g., IRM ) and conditional independence (e.g., conditional HSIC ). \(_{S}[0,)\) and \(_{C}[0,)\) are regularization hyperparameters. While another hyperparameter \(\) could control the relative weighting of stable and joint risks, i.e., \( R^{}(h_{S}_{S})\) and \((1-)R^{}(C(h_{S}_{S},h_{U}^{}_{U}))\), we found this unnecessary in practice. Finally, note that, in principle, \(h_{U}^{}\) could take any form and we could learn completely separate \(_{S}\), \(_{U}\). In practice, we simply take \(h_{U}^{}\) to be a linear classifier and split the output of a shared \((X)=(_{S}(X),_{U}(X))\).

**Post-hoc calibration.** As noted in SS 4.2, the stable predictor \(f_{S}\) must be properly calibrated to (i) form unbiased unstable predictions (Line 5 of Alg. 1) and (ii) correctly combine the stable and unstable predictions (Line 6 of Alg. 1). Thus, after optimizing the objective (5.1), we apply a post-processing step (e.g., temperature scaling ) to calibrate \(f_{S}\).

**Test-domain adaptation without labels.** Given a stable predictor \(f_{S}=h_{S}_{S}\) and complementary features \(_{U}(X)\), we now adapt the unstable classifier \(h_{U}^{}\) in the test domain to safely harness (or make optimal use of) \(_{U}(X)\). To do so, we use the bias-corrected adaptation algorithm of Alg. 1 (or Alg. 2 for the multi-class case) which takes as input the stable classifier \(h_{S}\)4 and unlabelled test-domain data \(\{_{S}(x_{i}),_{U}(x_{i})\}_{i=1}^{n_{e}}\), outputting a joint classifier adapted to the test domain.

## 6 Experiments

We now evaluate the performance of our algorithm on synthetic and real-world datasets requiring out-of-distribution generalization. App. E contains full details on these datasets and a depiction of their samples (see Fig. 4). In the experiments below, SFB uses IRM  for \(P_{}\) and the conditional-independence proxy of Jiang and Veitch [31; SS3.1] for \(P_{}\), with App. F.1.2 giving results with other stability penalties. App. F contains further results, including ablation studies (F.1.1) and results on additional datasets (F.2). In particular, App. F.2 contains results on the Camelyon17 medical dataset  from the WILDS package , where we find that all methods perform similarly _when properly tuned_ (see discussion in App. F.2). Code is available at: https://github.com/cianeastwood/sfb.

**Synthetic data.** We consider two synthetic datasets: anti-causal (AC) data and cause-effect data with direct \(X_{S}\)-\(X_{U}\) dependence (CE-DD). AC data satisfies the structural equations

\[ Y&(0.5);\\ X_{S}& Y(0.75);\\ X_{U}& Y(_{e}),\] (5.2)

where the input \(X=(X_{S},X_{U})\) and \(()\) denotes a Rademacher random variable thatis \(-1\) with probability \(1-\) and \(+1\) with probability \(\). Following [31, SS6.1], we create two training domains with \(_{e}\{0.95,0.7\}\), one validation domain with \(_{e}=0.6\) and one test domain with \(_{e}=0.1\). CE-DD data is generated according to the structural equations

\[X_{S} (0.5);\] \[Y (X_{S},(0.75));\] \[X_{U} ((Y,(_{e})),X_{S}),\]

where \(()\) denotes a Bernoulli random variable that is \(1\) with probability \(\) and \(0\) with probability \(1-\). Note that \(X_{S} X_{U}|Y\), since \(X_{S}\) directly influences \(X_{U}\). Following [31, App. B], we create two training domains with \(_{e}\{0.95,0.8\}\), one validation domain with \(_{e}=0.2\), and one test domain with \(_{e}=0.1\). For both datasets, the idea is that, during training, prediction based on the stable \(X_{S}\) results in lower accuracy (75%) than prediction based on the unstable \(X_{U}\). Thus, models optimizing for prediction accuracy only--and not stability--will use \(X_{U}\) and ultimately end up with only 10% in the test domain. Importantly, while the stable predictor achieves 75% accuracy in the test domain, this can be improved to 90% if \(X_{U}\) is used correctly. Following , we use a simple 3-layer network for both datasets and choose hyperparameters using the validation-domain performance: see App. G.2 for further implementation details.

On the AC dataset, Table 2 shows that ERM performs poorly as it misuses \(X_{U}\), while IRM, ACTIR, and SFB-no-adpt. do well by using only \(X_{S}\). Critically, only SFB (with adaptation) is able to harness \(X_{U}\) in the test domain _without labels_, leading to a near-optimal performance boost.

On the CE-DD dataset, Table 2 again shows that ERM performs poorly while IRM and SFB-no-adpt. do well by using only the stable \(X_{S}\). However, we now see that ACTIR performs poorly since its assumption of anti-causal structure no longer holds. This highlights another key advantage of SFB over ACTIR: any stability penalty can be used, including those with weaker assumptions than ACTIR's anti-causal structure (e.g., IRM). Perhaps more surprisingly, SFB (with adaptation) performs well despite the complementarity assumption \(X_{S}\!\!\! X_{U}|Y\) being violated. One explanation for this is that complementarity is only weakly violated in the test domain. Another is that complementarity is not _necessary_ for SFB, with some weaker, yet-to-be-determined condition(s) sufficing. In App. I, we provide a more detailed explanation and discussion of this observation.

**ColorMNIST.** We now consider the ColorMNIST dataset , described in SS 1 and Fig. 0(a). We follow the experimental setup of Eastwood et al. [15, SS6.1]; see App. G.3 for details. Table 3 shows that: (i) SFB learns a stable predictor ("no adpt.") with performance comparable to other stable/invariant

    &  &  & \\
**Algorithm** & AC & CE-DD & P & A & C & S \\  ERM & \(9.9 0.1\) & \(11.6 0.7\) & \(93.0 0.7\) & \(79.3 0.5\) & \(74.3 0.7\) & \(65.4 1.5\) \\ ERM + PL & \(9.9 0.1\) & \(11.6 0.7\) & \(93.7 0.4\) & \(79.6 1.5\) & \(74.1 1.2\) & \(63.1 3.1\) \\ IRM  & \(74.9 0.1\) & \(69.6 1.3\) & \(93.3 0.3\) & \(78.7 0.7\) & \(75.4 1.5\) & \(65.6 2.5\) \\ IRM + PL & \(74.9 0.1\) & \(69.6 1.3\) & \(94.1 0.7\) & \(78.9 2.9\) & \(75.1 4.6\) & \(62.9 4.9\) \\ ACTIR  & \(74.8 0.4\) & \(43.5 2.6\) & \(94.8 0.1\) & \(82.5 0.4\) & \(76.6 0.6\) & \(62.1 1.3\) \\ SFB no adpt. & \(74.7 1.2\) & \(74.9 3.6\) & \(93.7 0.6\) & \(78.1 1.1\) & \(73.7 0.6\) & \(69.7 2.3\) \\ SFB & \(\) & \(\) & \(\) & \(80.4 1.3\) & \(\) & \(\) \\   

Table 2: Synthetic & PACS test-domain accuracies over 100 & \(\&\) 5 seeds each.

  
**Algorithm** & **Test Acc.** \\  ERM & \(27.9 1.5\) \\ SRB not. & \(70.6 1.8\) \\ SFB & \(\) \\  Oracle no adpt. & \(72.1 0.7\) \\ Oracle & \(89.9 0.1\) \\   

Table 3: CMNIST test accuracies over 10 seeds.

Figure 2: CMNIST accuracies (y-axis) over test domains of decreasing color-label correlation (x-axis). Empirical versions of Fig. 0(b). _Left:_ SFB vs. baseline methods. _Right:_ Ablations showing SFB without (w/o) bias correction (BC), calibration (CA) and multiple pseudo-labeling rounds (Rn). Numerical results in Table 7 of App. F.1.3.

methods like IRM ; and (ii) only SFB (with adaptation) is capable of harnessing the spurious color feature in the test domain _without labels_, leading to a near-optimal boost in performance. Note that "Oracle no adpt." refers to an ERM model trained on grayscale images, while "Oracle" refers to an ERM model trained on labeled test-domain data. Table 6 of App. F.1.3 compares to additional baseline methods, including V-REx , EQRM , Fishr  and more. Fig. 2 gives more insight by showing performance across test domains of varying color-label correlation. On the left, we see that SFB outperforms ERM and IRM, as well as additional adaptive baseline methods in IRM + pseudo-labeling (PL, ) and IRM + T3A  (see App. G.1 for details). On the right, ablations show that: (i) bias-correction (BC), post-hoc calibration (CA), and multiple rounds of pseudo-labeling (Rn) improve adaptation performance; and (ii) without labels, SFB harnesses the spurious color feature near-optimally in test domains of varying color-label correlation--the original goal we set out to achieve in Fig. 0(b). Further results and ablations are provided in App. F.1.

**PACS.** Table 2 shows that SFB's stable ("no adpt.") performance is comparable to that of the other stable/invariant methods (IRM, ACTIR). One exception is the sketch domain (S)--the most severe shift based on performance drop--where SFB's stable predictor performs best. Another is on domains A and C, where ACTIR performs better than SFB's stable predictor. Most notable, however, is: (i) the consistent performance boost that SFB gets from unsupervised adaptation; and (ii) SFB performing best or joint-best on 3 of the 4 domains. These results suggest SFB can be useful on real-world datasets where it is unclear if complementarity holds. In App. I, we discuss why this may be the case.

## 7 Conclusion & Future Work

This work demonstrated, both theoretically and practically, how to adapt our usage of spurious features to new test domains using only a stable, complementary training signal. By using invariant predictions to safely harness complementary spurious features, our proposed Stable Feature Boosting algorithm can provide significant performance gains compared to only using invariant/stable features or using unadapted spurious features--without requiring any true labels in the test domain.

**Stable and calibrated predictors.** Perhaps the greatest challenge in applying SFB in practice is the need for a stable and calibrated predictor. While stable features may be directly observable in some cases (e.g., using prior knowledge of causal relationships between the domain, features, and label, as in Prop. D.2), they often need to be extracted from high-dimensional observations (e.g., images). Several methods for stable-feature extraction have recently been proposed , with future improvements likely to benefit SFB. Calibrating complex predictors like deep neural networks is also an active area of research , with future improvements likely to benefit SFB.

**Weakening the complementarity condition.** SFB also assumes that stable and unstable features are complementarity, i.e., conditionally independent given the label. This assumption is implicit in the causal generative models assumed by prior work , and future work may look to weaken it. However, our experimental results suggest that SFB may be robust to violations of complementarity in practice: on our synthetic data where complementarity does not hold (CE-DD) and real data where we have no reason to believe it holds (PACS), SFB still outperformed baseline methods. We discuss potential reasons for this in App. I and hope that future work can identify weaker sufficient conditions.

**Exploiting newly-available test-domain features without labels.** While we focused on domain generalization (DG) and the goal of (re)learning how to use the same spurious features (e.g., color) in a new way, our solution to the "marginal problem" in SS 4.1 can be used to exploit a completely new set of (complementary) features in the test domain that weren't available in the training domains. For example, given a stable predictor of diabetes based on causal features (e.g., age, genetics), SFB could exploit new unlabeled data containing previously-unseen effect features (e.g., glucose levels). We hope future work can explore such uses of SFB.