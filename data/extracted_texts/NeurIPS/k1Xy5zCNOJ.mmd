# Lookaround Optimizer: \(k\) steps around, 1 step average

Jiangtao Zhang\({}^{1}\), Shunyu Liu\({}^{1}\), Jie Song\({}^{1,}\), Tongtian Zhu\({}^{1}\), Zhengqi Xu\({}^{1}\), Mingli Song\({}^{1,}\)

\({}^{1}\)Zhejiang University, \({}^{2}\)Hangzhou City University

{zhjgtao, liushunyu, sjie, raiden, xuzhengqi, brooksong}@zju.edu.cn

Corresponding author

###### Abstract

Weight Average (WA) is an active research topic due to its simplicity in ensembling deep networks and the effectiveness in promoting generalization. Existing weight average approaches, however, are often carried out along only one training trajectory in a post-hoc manner (_i.e._, the weights are averaged after the entire training process is finished), which significantly degrades the diversity between networks and thus impairs the effectiveness. In this paper, inspired by weight average, we propose **Lookaround**, a straightforward yet effective SGD-based optimizer leading to flatter minima with better generalization. Specifically, Lookaround iterates two steps during the whole training period: the _around_ step and the _average_ step. In each iteration, 1) the _around_ step starts from a common point and trains multiple networks simultaneously, each on transformed data by a different data augmentation, and 2) the _average_ step averages these trained networks to get the averaged network, which serves as the starting point for the next iteration. The _around_ step improves the functionality diversity while the _average_ step guarantees the weight locality of these networks during the whole training, which is essential for WA to work. We theoretically explain the superiority of Lookaround by convergence analysis, and make extensive experiments to evaluate Lookaround on popular benchmarks including CIFAR and ImageNet with both CNNs and ViTs, demonstrating clear superiority over state-of-the-arts. Our code is available at https://github.com/Ardcy/Lookaround.

## 1 Introduction

Recent research on the geometry of loss landscapes in deep neural networks has demonstrated _Linear Mode Connectivity_ (LMC): two neural networks, if trained similarly on the same data starting from some common initialization, are linearly connected to each other across a path with near-constant loss . LMC reveals that neural network loss minima are not isolated points in the parameter space, but essentially forms a connected manifold . It has recently been attracting increasing attention from research communities, attributed to its great potential into motivating new tools to more efficiently reuse trained networks. A simple but effective strategy is Weight Average (WA), which directly averages the network weights of multiple trained  or sampled  networks along the training trajectories for flatter minima and thus better generalization.

WA has become a popular approach in various fields for its efficiency in parameters and effectiveness in performance. For example, Izmailov _et al._ show that _Stochastic Weight Averaging_ (SWA) of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. Following that, Cha et al.  propose _Stochastic Weight Averaging Densely_ (SWAD), _i.e._, averaging weights for every iteration in a region of low validation loss, to capture flatter minima and thus achieve superior out-of-domain generalizability. Recently, Wortsman _et al._ propose averaging weights of multiple fine-tuned models with various hyperparameters, coined _Model Soups_, to improve accuracy without increasing inference time.

Generally speaking, WA guides the model points around the loss basin into the interior of the loss basin to find a flatter solution, which is shown to approximate ensembling the predictions .

Albeit striking results achieved in some cases, WA easily breaks down owing to the _diversity-locality_ conflicts. In one end, when WA is carried out within only one optimization trajectory after the training converges (as SWA does), the function diversity of the sampled checkpoints whose weights are averaged is limited, impairing the benefits of WA in ensembling. In the other end, if WA is conducted between models trained independently, WA can result in a completely invalid model due to the large barrier between different local minima (violating the locality principle that solutions should be in the same loss basin and closed to each other in parameter space). Prior WA approaches are either limited in function diversity or easily violating the locality principle, hindering its effectiveness and application in practice.

In this work, we propose _Lookaround_, a straightforward yet effective SGD-based optimizer to balance the both. Unlike prior works where WA often carried out along only one training trajectory in a post-hoc manner, Lookaround adopts an iterative weight average strategy throughout the whole training period to constantly make the diversity-locality compromise. Specifically, in each iteration, Lookaround consists of two steps: the _around_ step and the _average_ step, as shown in Figure 1. The _around_ step starts from a common point and trains multiple networks simultaneously, each on transformed data by a different data augmentation, resulting in higher function diversity between trained networks. The _average_ step averages these trained networks to get the averaged network, which serves as the starting point for the next iteration, guarantees the weight locality of these networks during the whole training. Lookaround iteratively repeats the two steps constantly, leading the averaged model to a flatter minima and thus enjoying better generalization. We theoretically explain the superiority of Lookaround by expected risk and convergence analysis, and make extensive experiments to evaluate Lookaround on popular benchmarks including CIFAR and ImageNet with both CNNs and ViTs, demonstrating clear superiority over state-of-the-arts.

The contributions of this work can be summarized as follows.

* We propose _Lookaround_, a SGD-based optimizer that enjoys a _diversity-in-locality_ exploration strategy. To seek flat minima within the loss basins, Lookaround iteratively averages the trained networks starting from one common checkpoint with various data augmentation.
* We theoretically analyze the convergence of different competitive optimizers in the quadratic case, and prove that our Lookaround show lower limiting expected risk and faster convergence.
* Extensive experiments conducted on the CIFAR and ImageNet benchmarks demonstrate that various CNNs and ViTs equipped with the proposed Lookahead optimizer can yield results superior to the state-of-the-art counterparts, especially the multi-model ensemble methods.

## 2 Related Work

Mode Connectivity.Mode connectivity is an intriguing phenomenon in deep learning where independently trained neural networks can have their minima connected by a low-loss path on the energy loss landscape [12; 14]. Some early works [12; 8] find that different independently trained networks can be connected by curves or multiple polylines in low-loss basins. Based on this, some later works [35; 11] show that when training from pretrained weights, the model stays in the same basin of the loss landscape, and different instances of such models can be connected by a straight line. Such findings motivates weight averaging, which averages the weights of different models to obtain higher performance. Recently, weight averaging has been widely used in the fields of neural language processing [21; 32; 39] and multi-objective optimization . We aim to use model connectivity to give our network access to lower positions in the loss basin. In this paper, we propose a simple yet effective method to demonstrate the practical significance of this study.

Weight Averaging.While weight averaging shares some similarities with the aggregation operations in Graph Neural Networks [24; 45; 23; 22], its primary application is distinct. In the context of

Figure 1: Test set loss landscape. (Left) Around step for diversity. (Right) Average step for locality.

neural architectures, weight averaging typically represents an overall averaging of weights across different models with the same architecture, diverging from the node-based aggregation in GNNs. Historically, the general idea of weight averaging can trace its roots back to convex optimization . And with the breakthrough of loss landscape visualization technology , many applications have applied this idea to neural networks . The concept of weight averaging has been integrated into single-trajectory training  and has seen extensive application in distributed setups . Specifically,  found that the models trained with SGD optimizer often fell on the edge of a flat basin. Based on that, they propose the SWA method, which uses one-time weight averaging of multiple network points after the entire training process is finished, leading the network down to lower locations in the basin. The later work  proposes an optimizer that continuously uses weight averaging to update model weight to improve performance and robustness. Averaging weight during training is more effective than the averaging after training. Unlike single-trajectory optimization strategy, the recent Model Soups method  is inspired by mode connectivity, which starts from a typical pretrained weight and averages final fine-tuned models to improve generalizability. These methods inspire us to employ weight averaging method with multi-data augmentations to improve convergence and generalizability.

Ensemble.The ensemble learning is a traditional technology that combines multiple model outputs to achieve better robustness and performance . Ensemble methods usually need to train different models to reduce variance and improve the prediction effect. However, in recent years, some methods  can obtain different model checkpoints to conduct the ensemble in a single trajectory of model training, which significantly reduces the training time. Note that these methods all require separate inference through each model, which adds to the calculation cost. In contrast, our method does not require additional inference calculations.

## 3 Method

In this section, we present our optimization method Lookaround and provide an in-depth analysis of its properties. In Section 3.1, we provide a detailed description of the Lookaround optimizer. In Section 3.2, we analyze the expected risk of Lookaround, then thoroughly study its convergence at various levels and compare it with the existing optimizer Lookahead  and SGD . Intuitively, our proposed optimizer seeks flat minima along the training trajectory by iteratively looking for the diverse model candidates around the trained points, hence called Lookaround. The pseudo-code of Lookaround is provided in Algorithm 1.

``` Initial parameters \(_{0}\), objective function \(\), data augmentation list AUG of size \(d\), synchronization period \(k\), optimizer \(A\), dataset \(\), numbers of training epochs \(E\). for\(epoch=1\) to \(E\)do  Synchronize parameters for\(j=1,2,,d\)do \(_{t,j,0}_{t-1}\) endfor # Around Step: Independent model training. for\(i=1,2,,k\)do  sample minibatch of data \(B\) for\(j=1,2,,d\)do \(_{t,j,i}_{t,j,i-1}+A(,_{t,j,i-1},_{j}(B))\) endfor endfor # Average Step: Weight averaging.  Compute average weight \(_{t,,k}_{j=1}^{d}_{t,j,k}\)  Perform update \(_{t}_{t,,k}\) endfor return parameters \(\) ```

**Algorithm 1** Lookaround Optimizer.

### Lookaround Optimizer

In this subsection, we introduce the details of Lookaround. In Lookaround, the training process is divided into multiple intervals of length \(k\), with each interval consisting of an "around step" and an "average step". Let \(_{0}\) denote the initial weights. At \(t^{th}\) round of Lookaround, the weights are updated from \(_{t-1}\) to \(_{t}\) according to the around step and the average step. The resulting weights \(_{t}\) then serve as the starting point for the subsequent round.

Around Step.Given the optimizer \(A\), the objective function \(\), and the stochastic mini-batch of data \(B\) from train dataset \(\). In the around step, \(d\) different models are independently trained under different data augmentations \(=\{_{1},,_{d}\}\) for \(k\) batch steps, as follows:

\[_{t,j,k}=_{t,j,k-1}+A(,_{t,j,k-1},_{j}(B)).\] (1)

Thanks to the data augmentations, each model is trained in period \(k\) to a diverse location in the loss landscape scattered around \(_{t-1}\). The Around Step allows optimizing the region surrounding each model iterate, which helps the search of flatter minima. Moreover, note that as the \(d\) different models are trained independently, we can use parallel computing to speed up the whole training process.

Average Step.In the average step, we simply average \(_{t,i,k}\), the weights of each independently trained model to obtain \(_{t}\) for the next around step as follows:

\[_{t}=_{i=1}^{d}_{t,i,k}.\] (2)

It has been observed that models with the same initialization and trained on different data augmentations exhibit similar loss basins , with the models scattered around the edges of these basins. Therefore, incorporating a simple averaging technique into the training process may facilitate the convergence of the models towards lower-loss regions. In Appendix A, we demonstrate how this technique effectively guides the model to the interior of the loss basin.

### Theoretical Analysis

#### 3.2.1 Noisy Quadratic Analysis

The quadratic noise function has been commonly adopted as an effective base model to analyze optimization algorithms [42; 49; 47; 25], where the noise incorporates stochasticity introduced by mini-batch sampling. In this subsection, we analyze the steady-state risk of our proposed optimizer on the quadratic noise function to gain insights into the performance of Lookaround, and compare the result with those obtained using SGD and Lookahead.

The quadratic noise model is defined as \(}()=(-)^{T}( -)\). Following Zhang et al. , we assume that the noise vector \((^{*},)\), both \(\) and \(\) are diagonal matrices, and that the optimal solution \(^{*}=\). We denote \(a_{i}\) and \(_{i}^{2}\) as the \(i\)-th elements on the diagonal of \(\) and \(\), respectively. As the \(i\)-th element of the noise vector \(\) satisfies \([c_{i}^{2}]=(_{i}^{*})^{2}+_{i}^{2}\), the expected loss of the iterates \(_{t}\) can be written as follows:

\[(_{t})=[}(_{t})]= _{i}a_{i}([_{t,i}]^{2}+[_{t,i}]+_{ i}^{2}),\] (3)

where \(_{t,i}\) represents the \(i^{th}\) item of the parameter \(_{t}\). The limiting risk of SGD, Lookahead and Lookaround are compared in the following Proposition by unwrapping \([_{t}]\) and \([_{t}]\) in Equation 3.

**Proposition 1** (Steady-state risk).: _Let \(0<<1/L\) be the learning rate satisfying \(L=_{i}a_{i}\). One can obtain that, in the noisy quadratic setup, the variance of the iterates obtained by SGD, Lookahead  and Lookaround converge to the following matrix:_

\[V_{SGD}^{*} =^{2}^{2}}{-(-)^{2}},\] (4) \[V_{Lookahead}^{*} =(-(- )^{2k})}_{,\,\,(0,1)}V_{SGD}^{*},\] (5) \[V_{Lookaround}^{*} =(-(- )^{2k})+2(1-)(-(-)^{k})}_{ ,\,\,d 3\,\,\,[1/2,1)}V_{Lookahead}^{ *},\] (6)

_respectively, where \(\) denotes the average weight factor of models with varying trajectory points, as described in ._

Proposition 1 implies that the steady-state variance matrix of Lookaround is element-wise smaller than those of SGD and Lookahead if \(d 3\) and \([1/2,1)\). Moreover, Proposition 1 shows that increasing the number of data augmentation methods \(d\) yields smaller limiting variance and thus lower expected loss (see equation 3), which guarantees good generalizability in real-world scenarios. The proof is deferred to Appendix B.

#### 3.2.2 Convergence on Deterministic Quadratic function

In this section, we analyze the convergence of Lookaround and Lookahead in the noise-free quadratic setting, which is important for studying the convergence of optimization algorithms [13; 37; 44].

Convergence rate \(\) characterizes the speed in quadratic function at which the independent variable converges to the optimal solution, satisfying \(||_{t}-^{*}||^{t}||_{0}-^{*}||\). In order to calculate the convergence rate, we model the optimization process, and treating this function as a linear dynamical system allows us to calculate the convergence rate, as in . The value of the convergence rate will be determined by the eigenvalues of the dynamic transition equation, and we leave the calculation of this part to show in the Appendix B.2.

Given the emphasized significance of the condition number in paper , we conducted extensive experiments under a series of conditional numbers.

We evaluated the convergence rate of Lookaround, Lookahead, and Classical Momentum (CM) under different condition numbers from \(10^{1}\) to \(10^{7}\), which are shown in Figure 2. The blue dashed line represents the optimal convergence rate that an algorithm can achieve in the absence of any constraints. A value closer to the blue dashed line indicates a faster convergence rate. CM can achieve the optimal convergence rate on a critical condition number . To the left of this critical point, there exist complex eigenvalues that correspond to oscillations. In this regime, CM exhibits a flat convergence rate, which is known as "under-damped" behavior . We show that Lookaround and Lookahead are faster than CM in the under-damped regime. Lookaround converges faster and is more stable than Lookahead in the low condition number case, and realizes comparable performance with Lookahead in the high condition number case. In subsequent experiments, we show that Lookaround achieves fast and stable convergence in training deep neural networks on popular benchmarks.

## 4 Experiments

In Section 4.1 and Section 4.2, we present experimental verification on different tasks, including both random initialization and finetuning tasks on both ViTs and CNNs architectures to validate the effectiveness of the Lookaround optimizer. In Section 4.4, we compare the proposed method with ensemble learning methods that require multiple models. In Section 4.5, we conduct ablation experiments on different components of the Lookaround method to explore their contributions. In Section 4.6, we analyze the parameter robustness of Lookaround and further investigate its performance in the micro-domain.

Within a single epoch, both the proposed Lookaround and the competitors undergo training on an identical times the data augmentations. With such a setup, we guarantee consistency in the data volume utilized by each method, thereby ensuring fair comparisons in terms of computation.

### Random Initialization

Random initialization is a standard model initialization method in deep learning. In this subsection, we verify the performance of our algorithm on various networks with randomly initialized weights based on CIFAR and ImageNet datasets.

#### 4.1.1 CIFAR 10 and CIFAR 100

We conduct our experiments on CIFAR10  and CIFAR100  datasets. Both CIFAR10 and CIFAR100 datasets have 60,000 images, 50,000 of which are used for training and 10,000 for validation. We use SGDM  as our baseline, and we compare our method with SWA , SWAD , Lookahead . We validate different methods on multiple network architectures such as VGG19 , ResNet50 , ResNet101 , ResNet152 , ResNeXt50  and all methods are

Figure 2: Convergence rate on quadratics of varying condition number. We fix the step \(k=20\) for Lookahead and Lookaround, and fix the CM factor \(=0.99\).

performed on well-validated parameters. For each epoch in all experiments, we use the same mixed data for training using three kinds of data augmentation: random horizontal flip, random vertical flip, and RandAugment . This ensures that the comparison between different methods is fair. For the training process, we use a discount factor of size 0.2 to decay the initial learning rate 0.1 at 60, 120, 160 epochs. Please refer to Appendix C.1.1 for more specific details.

We present our results in Figure 3 and Table 1, which indicates that our method achieves stable performance improvement on different networks. Compared to the SGDM optimizer, both Lookaround and Lookahead demonstrate faster convergence rates and higher accuracy. However, what's noteworthy is that, with training losses similar to Lookahead, Lookaround not only achieves a higher accuracy but also has better stability. This suggests that within the same training duration, we have found a more optimal optimization path. Meanwhile, although the SWA and SWAD methods adopt the same optimization process as the SGDM, they achieve higher performance via multiple samplings in the training trajectory combined with an average step. In contrast to SWA and SWAD, the multi-averaging Lookaround can achieve consistent and superior performance across different networks.

#### 4.1.2 ImageNet

The ImageNet dataset is widely used to test the model performance. It has a training set of 1.28 million images and a validation set of 50,000 images with 1,000 classes. We train the model

     } &  &  &  &  &  &  \\   & & Top1 & NLL & Top1 & NLL & Top1 & NLL & Top1 & NLL & Top1 & NLL \\   & SGDM & 93.92 & 0.26 & 95.96 & 0.16 & 96.16 & 0.16 & 96.28 & 0.16 & 95.72 & 0.17 \\  & SWA & **94.89** & **0.21** & 96.42 & 0.14 & 96.34 & 0.14 & 96.82 & **0.13** & 96.09 & 0.16 \\  & SWAD & 93.23 & 0.29 & 95.39 & 0.19 & 94.49 & 0.22 & 95.00 & 0.20 & 93.89 & 0.26 \\  & Lookahead & 94.72 & 0.23 & 96.38 & 0.16 & 96.61 & 0.15 & 96.46 & 0.15 & 96.47 & 0.15 \\  & Ours & 94.44 & 0.25 & **96.59** & **0.14** & **96.73** & **0.14** & **97.02** & 0.14 & **96.70** & **0.13** \\   &  &  &  &  &  &  \\   & & Top1 & Top5 & Top1 & Top5 & Top1 & Top5 & Top1 & Top5 & Top1 & Top5 \\   & SGDM & 73.84 & 91.09 & 79.61 & 95.21 & 79.91 & 95.54 & 80.16 & 95.49 & 79.10 & 94.86 \\  & SWA & 73.62 & 90.89 & 80.17 & 95.56 & 80.53 & 95.39 & 80.86 & 95.43 & 79.14 & 94.96 \\  & SWAD & 73.37 & 89.93 & 80.19 & 95.46 & 79.92 & 95.09 & 80.00 & 95.36 & 79.27 & 95.17 \\  & Lookahead & 74.02 & 90.68 & 80.06 & 95.24 & 81.14 & 95.84 & 81.36 & 95.77 & 80.12 & 95.26 \\  & Ours & **74.29** & **91.15** & **81.60** & **95.99** & **81.97** & **96.15** & **82.22** & **96.40** & **81.14** & **96.12** \\   

Table 1: Test set accuracy under training procedure with random initialized models. In this table, all models are trained for same amount of time at 32\(\)32 resolution. (To provide a more comprehensive view of the data, we substitut the Top5 metric for CIFAR10 with the NLL loss.)

Figure 3: Training loss and Top-1 accuracy curves of various networks on CIFAR100 under different optimization methods.

[MISSING_PAGE_FAIL:7]

### Compared with Sharpness-Aware Minimization

We draw a comparison between Lookaround and Sharpness-Aware Minimization (SAM) , an algorithm designed to improve generalization by steering model parameters towards flatter loss regions.1 The comparative results are presented in Table 4.

The results seem to indicate that, under the default parameters of SAM, this method is more suitable for higher resolutions, while Lookaround achieves the best performance at lower resolutions. For the finetuning, neither SAM nor Lookaround consistently emerges as the top choice when used individually. However, when combined, they often enhance model performance significantly. Thus, for higher accuracy, we recommend using both the SAM and Lookaround optimizers concurrently. Both are orthogonal to each other. By combining the two methods, we can achieve superior performance.

### Compared with Ensemble Method

Ensemble methods are traditionally used to further improve inference performance by combining the outputs of multiple different models. We compare our Lookaround method with classical Ensemble methods (Logit Ensemble and Snapshot Ensemble ), and the results are shown in Table 5.

The performance of the single model obtained by our training exceeds the performance of the ensemble under multiple models. Moreover, the model obtained by our method is not a simple superposition of Ensemble models. We take the single model obtained by us with the three models obtained by Logit Ensemble to Logit Ensemble, and get a more significant performance improvement. This indicates that the model we obtained is orthogonal to the three models obtained by Logit Ensemble and can be combined to produce better performance.

### Ablation Study

In this subsection, we investigate the impact of different components of Lookaround on its performance, including data Augmentation methods and weight averaging. As shown in Table 6,

  
**Dataset** & DA & WA & **Acc (\%)** & **Dataset** & DA & WA & **Acc (\%)** \\   & - & - & 95.84 & & - & - & 78.81 \\ CIFAR10 & ✓ & - & 95.96 & & ✓ & - & 79.61 \\  & - & ✓ & 95.79 & & - & ✓ & 79.49 \\  & ✓ & ✓ & **96.59** & & ✓ & ✓ & **81.60** \\   

Table 6: Ablation Study of Data Augmentation (DA) and Weight Averaging (WA) in the proposed Lookaround using ResNet50.

   Backbone & Method & resolution & pretrain & CIFAR10 & CIFAR100 \\   & SAM\({}^{}\) & 32 & - & - & 79.10 \\ ResNet50 & Lookaround & 32 & - & **96.59** & **81.60** \\  & Lookaround + SAM & 32 & - & 96.38 & 79.79 \\   & SAM & 224 & ✓ & 97.65 & 85.97 \\ ResNet50 & Lookaround & 224 & ✓ & 97.82 & 85.20 \\  & Lookaround + SAM & 224 & ✓ & **97.88** & **86.21** \\   & SAM & 224 & ✓ & 98.51 & 92.39 \\ ViT-B/16 & Lookaround & 224 & ✓ & **98.71** & 92.21 \\  & Lookaround + SAM & 224 & ✓ & 98.67 & **92.54** \\   

Table 4: The test set accuracy under SAM and Lookaround optimization using ResNet50 or ViT-B/16. The results of SAM\({}^{}\) using ResNet50 are taken from .

  
**Method** & **ResNet50** & **ResNet101** & **ResNet152** \\  Logit Ensemble & 81.16 & 81.92 & 81.89 \\ SnapShot Ensemble & 79.88 & 80.63 & 80.23 \\ Ours (Lookaround) & 81.60 & 81.97 & 82.22 \\ Ours + Logit Ensemble & **83.43** & **83.35** & **83.53** \\   

Table 5: Performance compared with the ensemble methods (without pretrained weights) on CIFAR100 dataset. ”Ours + Logit Ensemble” means that we take the model obtained by our method and the three models in Logit Ensemble to Logit Ensemble.

when we consider Data Augmentation (DA) or Weight Averaging (WA) independently, their improvement effects may be relatively small. However, when we combine them, the synergistic effect between them can lead to more significant overall improvement. Specifically, we found that the improvement effects of DA and WA show a nonlinear trend in the interaction case, indicating a certain synergistic effect between them. Under the interaction, Data augmentation guides the model into different loss basins, and Weight Averaging guides different models into lower loss basins, resulting in higher performance improvement and a qualitative leap.

### Additional Analysis

Robustness of the Parameters.For the number of data augmentations, we train the model with different numbers of data augmentations, and the results are shown in Table 7. The results indicate that an increase in the number of data augmentations results in improved network performance. In order to reduce the training time, only three data augmentation methods are selected in this paper for experimental verification at different levels. Note that the methods compared with ours use the same incremental data set, so the comparison is fair.

For different choices of \(k\), the number of around steps, we select different intervals for full verification, and the results are shown in Figure 5 (Left). Let us look at Large-k (391) versus Small-k(5) in Figure 5 (Right) (391 is determined by the number of batches in a single epoch). We find that large steps (green line) can achieve higher accuracy and stability in the early stage of training, while small steps can achieve higher accuracy in the later stage. In future research, we may explore a learning rate scheduler suitable for Lookaround to help it achieve the best performance.

Figure 5: Top-1 accuracy of ResNet50 on CIFAR100 dataset with different steps \(k\). **(Left)** Error bar of three different random seeds with random initialization weight. **(Right)** Top-1 accuracy of two different \(k\) values with pretrained weight.

   \# of DA & 1 & 2 & 3 & 4 & 5 & 6 \\  Top-1 (\%) & 78.20 & 80.82 & 81.60 & 81.19 & 81.74 & **82.02** \\ Top-5 (\%) & 94.50 & 95.19 & 95.99 & 95.65 & 95.85 & **96.02** \\   

Table 7: Top-1 accuracy (%) of different data augmentation (DA) number by using ResNet50 on CIFAR100 dataset.

Figure 6: Accuracy curve of sub-model state update on the test set with different learning rates using ResNet50 on CIFAR100. At position 0 on the horizontal axis, H-net (trained by randomly horizontally flipped data) and R-net (trained by RandAugment data) are averaged to Mean-net. Subsequently, during 160 weight iterations, H-net and R-net continue to be trained with their respective data, while Mean-net represents the model obtained by averaging the weights at each iteration.

We visualize the training trajectory accuracy of the sub-models in Lookaround in Figure 6. Under different learning rates, the model, after averaging the weights, can maintain a good baseline and continuously improve the performance of the model under the low variance, and the sub-models also obtain more beneficial training effects due to the weight averaging.

## 5 Conclusion

In this paper, we present the Lookaround optimizer, an SGD-based optimizer that employs a diversity-in-locality exploration strategy to find solutions with high generalization ability in the loss basin. We theoretically prove that Lookaround can obtain lower variance and faster convergence speed. Experimental results on CIFAR and ImageNet and across various network backbones show that, our proposed Lookaround significantly improves the training stability and achieves better generalization performance comparable to the single-trajectory WA optimizers and ensemble methods.

Limitation.Lookaround is limited by the additional cost of network searching using different data augmentation, resulting in a longer training time proportional to the the number of trained networks. Notably, Lookaround incurs the same inference time as other optimization methods. Considering the significantly superior performance, we believe the extra time consumption is worthwhile in many real-world scenarios where the training time is not so concerned as the inference time. Moreover, training time can be reduced due to the parallelizability of the "around step" mechanism in Lookaround.