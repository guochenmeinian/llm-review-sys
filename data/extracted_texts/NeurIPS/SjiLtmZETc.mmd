# Bayesian Risk-Averse Q-Learning with Streaming Observations

Yuhao Wang

School of Industrial and Systems Engineering

Georgia Institute of Technology

Atlanta, GA 30332

yuhaowang@gatech.edu

&Enlu Zhou

School of Industrial and Systems Engineering

Georgia Institute of Technology

Atlanta, GA 30332

enlu.zhou@isye.gatech.edu

###### Abstract

We consider a robust reinforcement learning problem, where a learning agent learns from a simulated training environment. To account for the model misspecification between this training environment and the real environment due to lack of data, we adopt a formulation of Bayesian risk MDP (BRMDP) with infinite horizon, which uses Bayesian posterior to estimate the transition model and impose a risk measure to account for the model uncertainty. Observations from the real environment that are out of the agent's control arrive periodically and are utilized by the agent to update the Bayesian posterior to reduce model uncertainty. We theoretically demonstrate that BRMDP balances the trade-off between robustness and conservativeness, and we further develop a multi-stage Bayesian risk-averse Q-learning algorithm to solve BRMDP with streaming observations from real environment. The proposed algorithm learns a risk-averse yet optimal policy that depends on the availability of real-world observations. We provide a theoretical guarantee of strong convergence for the proposed algorithm.

## 1 Introduction

Markov Decision Process (MDP) is widely applied in the Reinforcement Learning (RL) community to model sequential decision-making, where the underlying transition process is a Markov Process for a given policy. The Q-learning algorithm, which was first proposed in , learns the optimal Q-function of the infinite-horizon MDP. The Q-function is a function in terms of the state and action pair \((s,a)\), which denotes the expected reward when action \(a\) is taken at the initial state \(s\) and optimal action is taken at subsequent stages. In a Q-learning algorithm, the decision maker updates the value of the Q-function by constantly interacting with the training environment and observing the reward and transition each time. The training environment is often the estimate of the real environment (where the policy is actually implemented) through past observed data. Implementing the policy learned from the training environment in the real environment directly can be risky, because of model misspecification, which is caused by the lack of historical data and possible environment shift. For example, in an inventory management problem where the demand follows some unknown distribution, we have limited observations of past demand realizations or only partially observed data if unfulfilled lost orders are not observed.

One common approach is to consider a distributionally robust formulation, where one assumes the embedding unknown distribution belongs to some ambiguity set. This ambiguity set is usually chosen as a neighborhood of the reference distribution (estimated from data). For example,  considered the distributionally robust policy learning for the contextual bandit problems, and [32; 17; 30] studied the distributionally robust policy learning for the discounted RL setting. In particular,  and  considered Q-learning methods for distributionally robust RL with a discounted reward, where  constructed the ambiguity set using the Kullback-Leibler (KL) divergence whereas  used theWasserstein distance; Both works designed the Q-learning algorithm by reformulating the robust Bellman equation in its dual form, and thus, transforming the optimization over the distributions to the optimization over a scalar.

While distributionally robust formulation accounts for model misspecification and provides a robust policy that has relatively good performance over all possible distributions in the ambiguity set, it is also argued to be conservative as a price for robustness, especially when the worst-case scenario is unlikely to happen in reality. This motivates us to take a more flexible risk quantification criterion instead of only considering the worst-case performance.  proposed a Bayesian risk optimization (BRO) framework, which adopts the Bayesian posterior distribution, as opposed to an ambiguity set, to quantify distributional parameter uncertainty. With this quantification, a risk measure, which represents the user's attitude towards risk, is imposed on the objective with respect to the posterior distribution. In this paper, we take this Bayesian risk perspective and formulate the infinite-horizon Bayesian risk MDP (BRMDP) with discounted reward.

BRMDP was first proposed in , where they model the unknown parameter of the MDP via a Bayesian posterior distribution. The posterior distribution is absorbed as an augmented state, together with the original physical state, and is updated at each stage through the realization of the reward and transition at that stage. A risk measure is imposed on the future reward at each stage taken with respect to the posterior distribution at that stage. Before , using the Bayesian technique in MDP was considered in  to deal with parameter uncertainty, where they simply took expectation on the total reward with respect to the posterior distribution and referred to this MDP model as Bayes-adaptive MDP (BAMDP). Based on BAMDP, there is a stream of work on model-based Bayesian RL (e.g., ). Other model-free Bayesian RL methods include Gaussian process temporal difference learning , Bayesian policy gradient methods , and Bayesian actor-critic methods . We referred to  for a comprehensive overview of Bayesian RL. More recently,  and  considered risk-sensitive BAMDP by imposing the risk measure on the total reward. In addition,  considered a percentile criterion and formulated a second-order cone programming by assuming a Gaussian random reward. All three works mentioned above took a non-nested formulation i.e., only one risk measure is imposed on the total reward. As pointed out in , one benefit of the nested formulation (i.e., a risk measure is imposed at each stage) is that the resulting optimal policy is time consistent, meaning the policy solved at the initial stage remains optimal at any later stage. In contrast, the non-nested formulation does not have this property (see ). In this paper, we considered the nested formulation as in . Notably,  as well as most works of BAMDP considered the planning problem over a finite horizon. The problem can be solved efficiently only when the number of horizons is small since the number of augmented states, i.e., possible posterior distributions, increases exponentially as the time horizon increases. In this paper, we formulate the infinite-horizon BRMDP whose state space only contains the physical states. As a result, this enables us to find the stationary (time-invariant) optimal policy. We develop a Q-learning algorithm to learn this stationary optimal policy, which is quite different from the dynamic programming approach taken by .

On a related note, risk measures are also widely applied in the literature on safe RL (see  for a recent survey). In safe RL, risk measures are used to ensure that the learned policy not only performs well but also avoids dangerous states or actions that can cause large one-stage costs. risk measures are typically applied to deal with the intrinsic uncertainty of the MDP, taking into account the known transition probability or reward distribution. In contrast, our work uses risk measures to account for parametric uncertainty in the transition probability, which can prevent finding the optimal policy. Moreover, in safe RL, the risk measure is usually imposed on some cost function to satisfy some safety constraint, while we impose it on the reward objective. Although both areas use risk measures, they have different goals and frameworks.

To summarize, all the pre-mentioned work on robust RL has focused on offline learning where the agent does not interact with the real environment, but only has access to data from the training environment. In contrast, our work utilizes real-world data to update the Bayesian posterior in the offline learning process to reduce the parametric uncertainty. We also differ from Bayesian online learning, where the Bayesian posterior is updated periodically, and another optimal policy is then re-solved (regardless of computational cost) and deployed. Our approach is off-policy learning, where the learning agent receives streaming real-world observations from some behavior policy, and we focus on designing a Q-learning algorithm for the varying BRMDP that possesses statistical validity rather than simply assuming optimality is obtained after each posterior update.

Preliminary and Problem Statement

### Standard Reinforcement Learning

Consider an RL environment \(^{c}=(,,^{c},r,)\), where \(\) is a finite state space, \(\) is a finite action space, \(r:\) is the reward function bounded by \(=_{s,a,s^{}}|r(s,a,s^{})|\), \(^{c}=\{p_{s,a}^{c}()\}_{(s,a)}\) is the transition probabilities, and \(\) is the discount factor. The standard reinforcement learning aims to find a (deterministic) optimal policy \(:\) satisfying

\[V^{}(s)=_{s^{} p_{s,(s)}^{c}}[r(s,(s),s^{})+  V^{}(s^{})]=_{a}\{_{s^{ } p_{s,a}^{c}}[r(s,a,s^{})+ V^{}(s^{})]\}.\]

### Bayesian Risk Markov Decision Process (BRMDP)

The true transition probabilities, \(^{c}\), are usually unknown in real problems, and we need to estimate them using observations from the real world. However, as discussed before, model misspecification due to lack of data can impair the performance of learned policy when deployed in the real environment. Hence, we take a Bayesian approach to estimate the environment and impose a risk measure on the objective with respect to the Bayesian posterior to account for this model (parametric) uncertainty. With finite state and action spaces, we can impose a Dirichlet conjugate prior \(_{s,a}\) on the transition model of each state-action \((s,a)\) pair and update the Bayesian posterior once we observe a transition from state \(s\) to \(s^{}\) by taking action \(a\). We defer the details of updating the Dirichlet posterior on \((s,a)\) to the supplementary material.

The Bayesian posterior provides a quantification of uncertainty about the transition model, which a risk-sensitive decision maker seeks to address by making robust decisions using the current model estimate. This can be done by imposing a risk measure on the future reward at each stage, which maps a random variable to a real value and reflects different attitudes toward risk. Given a Dirichlet posterior \(=\{_{s,a}\}_{s,a}\) and a risk measure \(_{}(f())\) that maps a function \(f\) of the random vector \(\) to a real value, the value function of BRMDP under a policy \(\) is defined as

\[ V^{,}(s_{0})=&_{d_{0 }(s_{0})}\{_{p_{1}_{z_{0},d_{0}}}(_{s_{1}  p_{1}}[r(s_{0},d_{0},s_{1})+.\\ &._{d_{1}(s_{1})}\{_{p_{2} _{z_{1},d_{1}}}(_{s_{2} p_{2}}[r(s_{1},d_{1},s_{2})+ .\\ &._{d_{2}(s_{2})}\{_{p_{3} _{z_{2},d_{2}}}(_{s_{3} p_{3}}[r(s_{2},d_{2},s_{3})+ ..\] (1)

We assume the risk measure \(\) satisfies the following assumption.

**Assumption 2.3**.: Let \((^{n})\) denote a random vector taking values in \(^{n}\), and let \(f_{i}:^{n},\;i=1,2\) denote two measurable functions. The risk measure satisfies the following conditions:

1. \(_{}( f_{1}())=(f_{1}())\) for all \( 0\).
2. \(_{}(f_{1}())_{}(f_{2}())\) if \(f_{1}() f_{2}()\) almost surely.
3. \(_{}(f_{1}()+C)=_{}(f_{1}())+C\) for all constant \(C\).

risk measures satisfying Assumption 2.3 are similar to the coherent risk measures (see ), except that the sub-additivity is not required. We relax the assumption of coherent risk measure to include the risk measure VaR, which does not admit the sub-additivity.

Notice (1) takes a nested formulation, where the risk measure is imposed on the future reward at each stage as opposed to the non-nested formulation where the risk measure is only imposed once on the total reward, i.e., the value function of the classic MDP. One advantage of the nested formulation is that (1) can be expressed in a recursive form:

\[V^{,}(s)=_{a(s)}\{_{_{z,a}}( _{s^{} p}[r(s,a,s^{})+ V^{,}(s^{}) ])\}.\]

This enables us to define the Bellman operator to find the time-invariant optimal policy. Let \(^{*}\) denote the optimal policy such that \(V^{,*}:=V^{,^{*}}\) satisfying

\[V^{,*}(s_{0})=_{}\{_{d_{0} (s_{0})}\{_{p_{1}_{z_{0},d_{0}}}(_{s_{1} p_{ 1}}[r(s_{0},d_{0},s_{1})+..\] \[.._{d_{1}(s_{1})}\{_{p_{2} _{z_{1},d_{1}}}(_{s_{2} p_{2}}[r(s_{1},d_{1},s_{2})+ ..\] \[.._{d_{2}(s_{2})}\{_{p_{3} _{z_{2},d_{2}}}(_{s_{3} p_{3}}[r(s_{2},d_{2},s_{3})+ ..\]where \(\) contains all randomized (and deterministic) policies. Let \(^{}\) be the Bellman operator such that

\[^{}V(s)=_{a}_{p_{s,a}}(_ {s^{} p}[r(s,a,s^{})+ V(s^{})]).\]

The following theorem ensures that \(^{}\) is a contraction mapping and BRMDP admits an optimal value function which is the unique fixed point of \(^{}\). Its proof is in the supplementary material.

**Theorem 2.4**.: _BRMDPs possess the following properties:_

1. \(^{}\) _is a contraction mapping with_ \(||^{}V-^{}U||_{}||V-U||_{}\)_, where_ \(||||_{}\) _is the sup norm in_ \(^{||}\)_._
2. _There exists a unique_ \(V^{,*}\) _such that_ \(^{}V^{,*}=V^{,*}\)_. Moreover,_ \[V^{,*}(s_{0})=_{}\{_{d_{0}(s_{0} )}[_{p_{1}_{s_{0},d_{0}}}(_{s_{1} p_{1}}[r(s_{0 },d_{0},s_{1})+.\] \[._{d_{1}(s_{1})}[_{p_{2}_{ s_{1},d_{1}}}(_{s_{2} p_{2}}[r(s_{1},d_{1},s_{2})+..\] \[.._{d_{2}(s_{2})}[_{p_{3} _{s_{2},d_{2}}}(_{s_{3} p_{3}}[r(s_{2},d_{2},s_{3})+..\]

### BRMDP with VaR and CVaR

Among choices of risk measures that satisfy Assumption 2.3, we adopt two of the most commonly used risk measures, VaR and CVaR. For a random variable \(X\) properly defined on some probability space, VaR\({}_{}\) is the \(\)-quantitle of \(X\), VaR\({}_{}(X)=\{z|(X z)\}\), and CVaR\({}_{}\) normally is defined as the mean of \(\)-tail distribution of \(X\). However, since we are imposing the risk measure on the reward (regarded as the negative of loss) rather than the loss, we define CVaR\({}_{}(X)=_{-}^{_{}(X)}x( x)\), which is the conditional expectation for \(X_{}\).

Before discussing solving the BRMDP, a natural question is how this risk-averse formulation is related to the original risk-neutral objective. Let \(V^{c,}\) denote the value function of policy \(\) under the true MDP \(^{c}\). the difference between \(V^{,}\) and \(V^{c,}\) is bounded by the following theorem.

**Theorem 2.6**.: _Suppose the Bayesian prior \(_{s,a}^{0}(s^{})=1, s,a,s^{}\) and \(\) is either VaR\({}_{}\) or CVaR\({}_{}\). Let \(\) be a deterministic policy. Let \(=_{s}O_{s,(s)}\) and \(O_{s,a}\) be the number of observed transitions from s with action \((s)\) in the dataset. Assuming \(O_{s,a}>0\) for all \((s,a)\), then with probability at least \(1-^{-}|}{}}\),_

\[\|V^{,}-V^{c,}\|_{}^{-}|}{}}|}{(1-)^{2}},\]

_In particular, let \(=_{s,a}O_{s,a}\), then with probability at least \(1-^{-}|}{}}\),_

\[\|V^{,*}-V^{c,*}\|_{} O^{-}|} {}}|}{(1-)^{2}}\]

_where \(V^{c,*}\) is the optimal value function for \(^{c}\)._

Theorem 2.6 implies our reformulated BRMDP coincides with the risk-neutral MDP as more observations are available. In particular, the discrepancy is characterized in terms of the minimal number of observations for any state-action pair with the order at least \(^{-}\). This indicates the BRMDP automatically balances the trade-off between robustness and conservativeness with varying numbers of observations. We defer the proof to the supplementary material due to the space limit.

### Q-Learning for BRMDP

Section 2.2 ensures BRMDP is well-formulated, that is, it admits an optimal value function which can be found as the fixed point of its Bellman operator \(^{}\). This allows us to derive a Q-Learning algorithm to learn the optimal policy and value function of BRMDP. Recall that an optimal Q-function is defined as

\[Q^{,*}(s,a)=_{p_{s,a}}(_{s^{} p}[r(s,a,s^{ })+ V^{,*}(s^{})])=_{p_{s,a}}(_{s^ {} p}[r(s,a,s^{})+_{b}Q^{,*}(s^{ },b)]).\]Let \(^{}\) denote the Bellman operator for the Q-function. That is,

\[^{}Q(s,a)=_{p_{s,a}}(_{s^{} p}[r( s,a,s^{})+_{b}Q(s^{},b)]).\]

The optimal Q-function \(Q^{,*}(s,a)\) then satisfies

\[Q^{,*}(s,a)=(1-)Q^{,*}(s,a)+^{}Q^{,*}( s,a),\]

where \((0,1)\). In a Q-learning algorithm, given a sequence of learning rates \(\{_{t}\}_{t}\) and an estimator of the Bellman operator \(}^{}\), we have the Q-learning update rule

\[Q_{t+1}(s,a)=(1-_{t})Q_{t}(s,a)+_{t}}^{ }Q_{t}(s,a).\]

### Bayesian Risk-Averse Q-Learning with Streaming Data

Notice that in Section 2.2 and 2.7, the posterior distribution \(\) is fixed across stages. As in many previous works, the embedding model is estimated with a fixed set of past observations before solving the problem. However, this one-time estimation of the transition model does not take into consideration utilizing the new data that arrive later, which helps reduce the model uncertainty as well as the conservativeness caused by risk measure, as indicated by Theorem 2.6. This motivates us to consider a data-driven framework where we dynamically update the estimate of the model. For this purpose, we consider a multi-stage Bayesian Q-learning algorithm.

Suppose at the beginning of stage \(t\), a batch of observations in the form of three-tuple \((s_{i},a_{i},s^{}_{i})\) with batch size \(n(t)\) is available. The observation can be regarded as generated by some policy that is actually deployed in the real world, which does not depend on the learning process. The decision maker incorporates these new data to update the posterior \(^{t}\), with which we obtain a BRMDP model \(_{t}\). We then carry out \(m(t)\) steps of the Q-learning update, where the initial Q-function in stage \(t\) is inherited from the previous stage \(t-1\). This framework is shown in Figure 1.

## 3 Estimator for Bellman Operator

In Figure 1, a key step is to design a proper estimator \(}^{}\) for the Bellman operator to ensure the convergence of the Q-function. In most of the existing literature on Q-learning, convergence relies on an unbiased estimator of the Bellman operator. While this is usually easy to obtain with the expectation operator which is linear, in BRMDP unbiased estimator for the Bellman operator is difficult if not impossible to obtain, because of (i) the non-linearity of risk measure (VaR and CVaR) and (ii) varying posterior distributions. Unbiased estimators for nonlinear functionals have been studied in ; however, their method cannot be directly applied here since the variance of the estimator is uncontrollable in the existence of varying posteriors. Instead, we use the Monto Carlo simulation to obtain an estimator with almost surely diminishing bias. We show in Theorem 4.3 and 4.4 that the Monto Carlo estimator is sufficient to guarantee the convergence of the Q-function.

Figure 1: Multi-stage Bayesian risk-averse Q-learning

### Monto Carlo Estimator for VaR and CVaR with Varying Sample Size

Denote by

\[f(p|s,a,Q)=_{s^{} p}[r(s,a,s^{})+_{b}Q(s^{},b)]=_{s^{}}p(s^{})[r(s,a,s^{ })+_{b}Q(s^{},b)].\] (2)

Given a posterior \(\) and a Q-function \(Q\), we want to estimate

\[^{}Q(s,a)=_{p_{s,a}}(f(p|s,a,Q)),\]

where \(\) is either VaR\({}_{}\) or CVaR\({}_{}\) and \((0,1)\) denotes the risk level. A Monto Carlo estimator for VaR\({}_{}(f(p|s,a,Q))\) and CVaR\({}_{}(f(p|s,a,Q))\) with sample size \(N\) can be obtained by first drawing independent and identically distributed (i.i.d.) samples \(p_{1},p_{2},,p_{N}_{s,a}\). Denote by \(X_{i}:=f(p_{i}|s,a,Q)\), \(i=1,2,,N\). Then

\[}_{N}^{}Q(s,a)=}_{ ,N}^{_{s,a}}(f(p|s,a,Q)):=X_{ N:N}&_{}$},\\ }_{,N}^{_{s,a}}(f(p|s,a,Q)):=_{k=1}^{ N}X_{k:N}&_{}$},\] (3)

where \(X_{k:N}\) is the \(k\)th order statistic out of \(N\) samples. Both estimators are biased with a constant sample size. Increasing the sample size reduces the bias but can be computationally inefficient. As the posterior distribution concentrates more on the true model, even estimators with small sample sizes can have reduced bias. We use a varying sample size that adapts to the updated posterior distribution. Initially, we set \(N_{s,a}\) as a fixed minimal sample size \(N\), and decrease or increase \(N_{s,a}\) by \(1\) at the beginning of each stage based on whether a new observation improves or not the estimate for the transition model on \((s,a)\). The multi-stage Bayesian Risk-averse Q-Learning algorithms with VaR (BRQL-VaR) and CVaR (BRQL-CVaR) are presented in Algorithm 1.

## 4 Convergence Analysis

In this section, we give the theoretical guarantee of our proposed multi-stage Bayesian risk-averse Q-learning algorithm, which ensures the asymptotic convergence of the learned Q-function to the "optimal" Q-function. We define the random observations from the real environment and all algorithm-related random quantities with respect to a probability space \((,,)\).

Recall that the real-world transition observations are assumed to be obtained from some behavior policy that is not controlled by the agents. This may possibly bring a definition problem of "optimal" Q-function. Indeed, if all streaming data are generated by some greedy policy, then the number of transition observations from some state-action pair can remain small, in which case the Bayesian posterior does not converge and the learned policy is risk-averse yet random since it depends on the Bayesian posterior. As in the offline RL, where the policy is learned based on the initial fixed data set, we also define the optimal Q-function as depending on the given real-world data but differs from pure offline RL in that the given data is streaming and random.

**Definition 4.1**.: **(Data-conditional optimal Q-function)**

Given an observation process \(=(s_{t,i},a_{t,i},s^{}_{t,i})|t=1,2,,i=1,2,,n( t)}\), let \(O^{}_{s,a,s^{}}()=_{t=1}^{}_{i=1}^{n(t)} \{(s_{t,i},a_{t,i},s^{}_{t,i})=(s,a,s^{})\}\) and \(O^{}_{s,a}()=(O^{}_{s,a,s^{}}())_{s^{} }\) denote the number of transition observations under \(\). For each \((s,a)\), define the limiting posterior as

\[^{}_{s,a}=_{p^{c}_{s,a}}()& _{s,a}()||=$},\\ (^{0}_{s,a}+O^{}_{s,a}())&,\]

where \(_{p^{c}_{s,a}}()\) is the Dirac measure centered at true transition probability \(p^{c}_{s,a}=(p^{c}_{s,a}(s^{}))_{s^{}}\). \(Q^{,*}\) is the optimal solution to BRMDP with "posterior" \(^{}\), which satisfies \(Q^{,*}=^{^{}}Q^{,*}\). The data-optimal Q-function \(Q^{,*}\) is a random vector since the observation process is random.

With this data-conditional optimal criterion, we can now prove the convergence of the multi-stage Bayesian risk-averse Q-learning algorithm. A list of notations for different Q-functions can be found in Table 1 of the supplementary material.

To prove the convergence of \(Q_{t}\), we prove (i) the convergence of \(Q^{^{t},*}\) which depends on the convergence of the posterior distribution, and (ii) the convergence of the estimator for the Bellman operator. We summarize two important results in Proposition 4.2 and Theorem 4.3. Due to the page limit, all the proofs are deferred to the supplementary material.

**Proposition 4.2**.: _(Convergence of \(Q^{^{t},*}\)) Let \(Q^{,*}\) be defined as in Definition 4.1 and \(^{t}\) be the posterior distribution obtained at stage \(t\). Then the optimal \(Q\)-function under posterior distribution \(^{t}\) converges to \(Q^{,*}\) almost surely. That is,_

\[_{t}||Q^{^{t},*}-Q^{,*}||_{}=0\]

_where \(||||_{}\) is the entry-wise sup norm._

Compared with Theorem 2.6 which characterizes the difference between the BRMDP and the original MDP by computing a concentration bound, Proposition 4.2 ensures the strong convergence of the BRMDP model to the data-conditional optimal Q-function. The next Theorem 4.3 ensures that the bias of the proposed estimator for the Bellman operator with varying sample sizes converges to zero uniformly in \(Q\).

**Theorem 4.3**.: _(Diminishing bias for Bellman estimator) Denote by \((k)\) the \(k\)th iteration of the Q-learning update. Let \(t_{k}\) be the stage such that \(_{t=1}^{k-1}m(t)<k_{t=1}^{t_{k}}m(t).\) Let \(N_{s,a}^{(k)}\) denote the sample size in iteration \(k\) and \(_{t_{k}}\) denote all the past observations until stage \(t_{k}\). Denote by \(}^{(k)}Q(s,a)=}_{N_{s,a}^{(k)}}^{ _{t_{k}}}Q(s,a)\) as defined in (3). Then, we have almost surely,_

\[_{k}_{||Q||_{}}[}^{(k)}Q(s,a)-^{^{t_{k}}}Q(s,a)| _{t_{k}}]=0.\]

Theorem 4.3 provides a uniform convergence of the estimated Bellman estimator, which is crucial to prove Theorem 4.4. Notably, with both streaming data and risk measure, proof of Theorem 4.3 is much more challenging than existing work on distributionally robust Q-learning, where randomness only comes from the learning process, which refers to the random data generated by the simulator. In our setting, we have mixed randomness coming from both the observation process and learning process, which complicates the analysis and differs from that of pure offline Q-learning. Also, the analysis is different from online Bayesian Q-learning, where the sample complexity results are usually constructed assuming an optimal policy is resolved and deployed instantly in each period. In contrast, we aim to prove the convergence of Q-learning in order to obtain the optimal policy.

Together with Proposition 4.2 and Theorem 4.3, we establish the convergence of Algorithm 1 in Theorem 4.4.

**Theorem 4.4**.: _Denote by \(Q_{t}\) the Q-function given by Algorithm 1 at the end of stage \(t\). Assume \(T=\), and the learning rate \(\{_{t}\}_{t=1}^{}\) satisfies \(_{=1}^{}_{}=,\;_{=1}^{}_{ }^{2}<.\) Then, we have almost surely,_

\[_{t}||Q_{t}-Q^{,*}||_{}=0.\]

**Corollary 4.5**.: _Let \(Q^{t}\) be defined as in Theorem 4.4 and \(O_{s,a}^{}\) be defined as in Definition 4.1. Assume \(T=\) and \(||O_{s,a}^{}||_{}=, s,a\) almost surely. Then_

\[_{t}||Q^{t}-Q^{c,*}||_{}=0\;,\]

_where_

\[Q^{c,*}(s,a)=_{s^{}}p_{s,a}^{c}(s^{})[r(s,a,s^{ })+_{b}Q^{c,*}(s^{},b)]\]

_is the true optimal Q-function._

Corollary 4.5 is straightforward from Theorem 4.4, which ensures Algorithm 1 will eventually learn the true optimal Q-function if we can obtain an infinite number of observations for each state-action pair.

## 5 Numerical Experiments

### Comparison Baselines:

* BRQL-VaR: our proposed multi-stage Bayesian risk-averse Q-learning algorithm with risk measure VaR;
* BRQL-CVaR: our proposed multi-stage Bayesian risk-averse Q-learning algorithm with risk measure CvaR;
* BRQL-mean: the risk-neutral Bayesian Q-learning function. That is, the risk measure is the expectation taken with respect to the posterior distribution;
* DRQL-KL: distributionally robust Q-learning algorithm with KL divergence (see );
* DRQL-Wass: distributionally robust Q-learning algorithm with Wasserstein distance (see ).

### Testing examples

**Example 1: Coin Toss.** Consider we are playing the following game. Each time we will toss K coins and observe the number of coins that show heads, where the chance of each coin showing heads is unknown. After observing the number of heads in the last toss, we can make a guess about whether the next toss will have more heads or fewer heads. If our guess is right, we can get 1 dollar as the reward, otherwise, we need to pay 1 dollar. We also have the choice of not guessing, in which case we do not pay or get paid. We model this game as a discounted infinite-horizon MDP. The state \(s_{t}=\{0,1,,K\}\) denotes the number of heads in \(t\)th toss. The actions space is \(=\{-1,0,1\}\), where \(a=1\) corresponds to guess \(s_{t+1}>s_{t}\), \(a=-1\) corresponds to guess \(s_{t+1}<s_{t}\), and \(a=0\) corresponds to not guess. Hence, the reward function is

\[r(s,a,s^{}):=a_{\{s<s^{}\}}-a_{\{s>s^{} \}}-|a|_{\{s=s^{}\}}.\]

Assume each coin shows a head with probability \(0.4\). The number of coins \(K=10\).

**Example 2: Inventory Management.** Suppose a warehouse manager runs a capacity system. At the beginning of period \(t\), the manager observes the current inventory level \(s_{t}\) and orders additional goods of the amount \(a_{t}\). An ordering cost of \(c=1\) is incurred for each unit of goods. The demand follows a truncated Poisson distribution with a mean of \(3\) and support \(\{0,1,,K\}\). Suppose the demand arrives during each period and is fulfilled at the end of the period. For each unit of fulfilled demand, we can obtain a profit of \(u=5\). If a unit of demand is not fulfilled at the end of the period, the demand is lost and a penalty cost of \(q=2\) is incurred. If there are any remaining goods at the end of the period, the goods will be taken to the next period with a holding cost \(h=1\) per unit. The warehouse has a maximal capacity of \(K\) for the goods. We model this problem as a discounted infinite-horizon MDP with state space \(=\{-K,,0,,K\}\), where \((s_{t})^{+}=(s_{t},0)\) represents the inventory level at the beginning of period \(t\) and \((s_{t})^{-}=-(s_{t},0)\) represents the lost demand at the end of period \(t-1\). The action space is \(_{s}:=\{0,,K-s^{+}\}\). The reward is

\[r(s,a,s^{})=-(c a+h(s^{})^{+}+q(s^{})^{-})+ u(s^{+}+a-(s^{})^{+}).\]

We consider two settings: (I) the demand in each period uniformly distributes among \(\{0,1,,K\}\) and (II) the demand depends on the current inventory level \(s\). For the second setting, we will consider the case where observations are insufficient to estimate the transition probability for every state-action pair.

### Experiment Setting

**Coin toss.** We consider stage-wise streaming observations with batch size \(n(t)=1\) and stage-wise Q-learning with number of steps \(m(t)=1\). The minimal sample size to estimate the Bellman operator \(N=10\). Initial observed data batch size \(n(0)=10\). We set the radius of the KL ball and the Wasserstein ball to be \(0.1\). The risk level for VaR and CVaR is set to \(0.2\) in Figure 2 and 0.4 in Figure 3.

**Inventory Management.** In Figure 4, We set \(K=10\), \(T=60\), \(m(t)=n(t)=5,n(0)=20\), and \(N=10\). The radius of the KL and the Wasserstein ball is \(0.05\), and the risk level for VaR and CVaR is \(0.2\). In Figure 5, we set the capacity \(K=10\) and the size of the historical data set \(n(0)=30\). The policies are deployed in different environments, where the demand follows different truncated Poisson distributions with means ranging from \(3\) to \(5\).

### Results

In Figure 2- 4, we compare the value functions of different algorithms as the time stage increases. The value function of each algorithm is calculated by deploying the optimal policy in the real environment. Each curve shows the empirical expected performance and the strip around the curve shows the \(95\%\) confidence interval. In both examples, our proposed algorithms outperform the two distributionally robust Q-learning algorithms in both expected performance and variation as the time stage increases, since our proposed algorithms dynamically update the posterior to reduce the model uncertainty while two DRQL algorithms learns with fixed ambiguity set. Compared with the risk-neutral algorithm, BRQL-VaR and BRQL-CVaR achieve lower expected value functions but have smaller variations, which shows the robustness of our two proposed algorithms.

Moreover, in Figure 5 We test for the insufficient data setting, where we only have a set of historical data to estimate the transition model at the initial time stage. The value function is calculated as deploying the learned policy in different environments with demand following different Poisson distributions. Figure 5 indicates the two proposed algorithms achieve higher value functions than the risk-neutral algorithm in the more adversarial setting (with Poisson parameter less than \(3\)), showing their robustness. They also obtain lower value functions than two DRQL algorithms in the adversarial setting and higher value functions in other settings, indicating the risk measure is more flexible compared to the worst-case criterion.

## 6 Conclusion and Limitation

In this paper, we propose a novel multi-stage Bayesian risk-averse Q-learning algorithm to learn the optimal policy with streaming data, by reformulating the infinite-horizon MDP with unknown transition model as an infinite-horizon BRMDP. In particular, we consider the two cases of riskmeasures, VaR and CVaR, for which we design a Monte Carlo estimator with varying sample sizes to approximate the Bellman operator of BRMDP. We demonstrate the correctness of the BRMDP formulation by providing statistical guarantee and prove the strong asymptotic convergence of the proposed Q-Learning algorithm. The numerical results demonstrate that the proposed algorithms are efficient with streaming data and robust with limited data.

As discussed in the paper, one limitation of the current framework is that the behavior policy that generates real-world observations is assumed to be given. This is suitable for situations when it is expensive for the agent to interact with the real environment or to change the policy frequently as it may cause a large cost or system instability. An interesting future direction is to consider an online learning setting, where at each period the agent also needs to take action in the real world.

## Author Biographies

**Yuhao Wang** is a Ph.D. student at the H. Milton Stewart School of Industrial and Systems Engineering at Georgia Institute of Technology. He received his B.S. degree from the Department of Mathematics at Nanjing University, China, in 2021. His research interests include simulation and stochastic optimization and reinforcement learning. His email address is _yuhaowang@gatech.edu_ and his web page is https://sites.gatech.edu/yuhaowang/.

**Enlu Zhou** is a Professor at the H. Milton Stewart School of Industrial and Systems Engineering at Georgia Institute of Technology. She received the B.S. degree with highest honors in electrical engineering from Zhejiang University, China, in 2004, and received the Ph.D. degree in electrical engineering from the University of Maryland, College Park, in 2009. Her research interests include simulation optimization, stochastic optimization, and stochastic control. Her email address is _enlu.zhou@isye.gatech.edu_, and her web page is https://www.enluzhou.gatech.edu/.