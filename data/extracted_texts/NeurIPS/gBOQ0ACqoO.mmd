# DH-Fusion: Depth-Aware Hybrid Feature Fusion for Multimodal 3D Object Detection

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion. However, they neglect the factor of depth while designing the fusion strategy. In this work, we for the first time point out that different modalities play different roles as depth varies via statistical analysis and visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature Fusion (DH-Fusion) strategy that guides the weights of point cloud and RGB image modalities by introducing depth encoding at both global and local levels. Specifically, the Depth-Aware Global Feature Fusion (DGF) module adaptively adjusts the weights of image Bird's-Eye-View (BEV) features in multi-modal global features via depth encoding. Furthermore, to compensate for the information lost when transferring raw features to the BEV space, we propose a Depth-Aware Local Feature Fusion (DLF) module, which adaptively adjusts the weights of original voxel features and multi-view image features in multi-modal local features via depth encoding. Extensive experiments on the nuScenes dataset demonstrate that our DH-Fusion method surpasses previous state-of-the-art methods w.r.t. NDS. Moreover, our DH-Fusion is more robust to various kinds of corruptions, outperforming previous methods on nuScenes-C w.r.t. both NDS and mAP.

## 1 Introduction

3D object detection has a wide range of applications in the fields of autonomous driving and robotics. A large number of previous works have successfully focused on using a single modality, such as point cloud or images, to design efficient 3D object detectors. However, the performance of these detectors reaches a bottleneck due to the limitations of modality characteristics. For instance, the point cloud modality can only provide rich geometric information while lacks detailed semantic information; the image modality can only provide rich texture information while lacks three-dimensional spatial information. To address the aforementioned issues, we are highly motivated to obtain comprehensive information that represents objects by designing a LiDAR-camera 3D object detector.

In recent years, LiDAR-camera 3D object detection develops rapidly. Some works [1; 4; 28; 33; 67] propose effective methods to integrate information from two modalities at the feature level. However, they all overlook an important factor of depth in their fusion strategies. To understand how point cloud and image information vary with depth, we first conduct statistical and visualization analysis on the nuScenes-mini dataset , and find that: (1) The number of points representing objects at near range is relatively large, which allows us to accurately determine the object's location, size, and category, even without the aid of images. As shown in Fig. 0(a), there is an average of 163.7 points per object within 0-10 meters, which is a substantial number. We also visualize a car at 6.8 meters in Fig. 0(b) and find it encompasses a considerable number of points, well representing the shape. In contrast, some background noise in the image may interfere with detection (Fig. 0(b)). (2) As thedepth increases, the number of points representing objects decreases rapidly. As shown in Fig. 0(a), the number of points within 30-50 meters falls below one per object, meaning that many objects are even not represented by any points, such as the object at 42.1 meters in Fig. 0(b) 3. In contrast, the complete objects may still be observed on the image, as in Fig. 0(b) 4, where the image information becomes more important. To address the above problems, we propose a feature fusion strategy that adaptively adjusts the importance of the two modalities based on depth.

Specifically, we propose a novel method for multi-modal 3D object detection, namely Depth-Aware Hybrid Feature Fusion (DH-Fusion). The innovation lies in adaptively adjusting the weights of features by introducing depth encoding to hybrid feature fusion at both global and local levels. The fusion strategy consists of two crucial components: Depth-Aware Global Feature Fusion (DGF) module and Depth-Aware Local Feature Fusion (DLF) module. In DGF, we take point cloud Bird's-Eye-View (BEV) features and image BEV features as inputs, and dynamically adjust the weights of image BEV features based on depth during fusion by utilizing a global-fusion transformer encoder with a depth encoder. To compensate for the information lost when transforming raw features to BEV space, we enhance the fused BEV features at a lower cost by utilizing the original instance features. In DLF, we obtain 3D boxes by utilizing a Region Proposal Network (RPN). Then, the 3D boxes are projected into both LiDAR voxel features and multi-view image features to crop out corresponding local instance features with more detailed information. Afterward, we take these as inputs and dynamically adjust the weights of local multi-view image features and local LiDAR voxel features based on depth through the use of a local-fusion transformer encoder with the depth encoder. In the end, we update local features for each object on the global feature map to enhance the detailed instance information of multi-modal global features for detection.

Our contributions are summarized as follows.

1. We for the first time point out that depth is an important factor to consider while fusing LiDAR point cloud features and RGB image features for 3D object detection. From our statistical and visualization analysis, we can see that image features play different roles as depth varies.

2. We propose a depth-aware hybrid feature fusion strategy that dynamically adjusts the weights of features during feature fusion by introducing depth encoding at both global and local levels. The above strategy can obtain high-quality features for detection, fully leveraging the advantages of different modalities at various depths.

3. Our method is evaluated on the nuScenes  dataset and a more challenging nuScenes-C  dataset, outperforming previous multi-modal methods and being robust to various kinds of data corruptions.

## 2 Related Work

Since our method is based on conducting 3D object detection using data from multiple modalities, including point cloud and images, we briefly review recent works in the following fields: LiDAR-based 3D object detection, camera-based 3D object detection, and LiDAR-camera 3D object detection.

Figure 1: Statistical and visualization analysis on the nuScenes-mini dataset. (a) The average numbers of points and pixels for each object at different depths. (b) Examples of near-range and long-range objects in images and point cloud. Points within the bounding boxes are colored red for observation.

### LiDAR-based 3D Object Detection

LiDAR-based 3D object detectors only take the point cloud as input. Based on their different data representations, they can be divided into point-based [44; 45; 46; 64; 65], voxel-based [12; 22; 61; 68; 71], and point-voxel-based [17; 42; 43] methods. The feature extraction networks of point-based methods typically extract features directly from the point cloud through a point-based backbone , such as PointRCNN . The voxel-based methods first convert the point cloud into voxels and then extract voxel features through a 3D sparse convolution network , such as VoxelNet . Point-voxel-based methods like PV-RCNN  combine the above two methods to extract and fuse point and voxel features. The purpose of these approaches is to capture the geometric spatial information of the point cloud. However, point cloud is sparse and incomplete, lacking detailed texture information, which greatly limits the detection performance.

### Camera-based 3D Object Detection

Camera-based 3D object detectors only take images as inputs. Depending on the form of inputs, they can be divided into monocular [2; 24; 32; 41; 47; 55], stereo [6; 25; 30; 48; 70], and multi-view [19; 27; 56; 62] 3D object detectors. Early works like FCOS3D  input a monocular image and utilize 2D object detectors to directly predict 3D bounding boxes, but these approaches have limited capability in capturing spatial information. Subsequently, stereo and multi-view 3D object detectors are proposed to obtain more precise depth information by constructing spatial relationships among multiple images, such as Stereo RCNN  and BEVDet . These methods successfully achieve purely visual 3D object detection, but they do not perform as well as LiDAR-based methods, because the spatial depth information provided by images is not as direct and precise as that provided by point cloud.

### LiDAR-Camera 3D Object Detection

LiDAR-camera 3D object detectors take point cloud and images as inputs, and can be classified into early-fusion-based [50; 52; 57; 59; 69], intermediate-fusion-based [1; 4; 28; 33; 67], and late-fusion-based [37; 38] 3D object detectors based on the location of multi-modal information fusion .

Early-fusion-based methods perform at the point level, where the typical approach involves enhancing the raw point cloud with semantic information extracted from images. PointPainting  and FusionPainting  decorate the raw point cloud with semantic scores from 2D semantic segmentation. Similarly, PointAugmenting  enhances the raw point cloud using features extracted from a 2D semantic segmentation network. However, early-fusion-based methods are sensitive to alignment errors between the two modalities.

Intermediate-fusion-based methods perform at the feature level. Transfusion  first proposes to utilize the transformer for fine-grained fusion from LiDAR BEV features and multi-view image features. FUTR3D  encode each modality using deformable attention  in its own coordinate and concatenate them for fusion. BEVFusion [28; 33] projects both point cloud and images to BEV space for BEV feature fusion. SparseFusion  extracts instance-level features from both two modalities separately, and fuse them to perform detection. Similarly, ObjectFusion  utilizes 3D proposals from LiDAR modality to extract instance-level features for fusion. CMT  proposes the simultaneous interaction between the object queries and multi-modal features in the transformer encoder and decoder. IS-Fusion  proposes feature fusion at both the instance level and scene level. The intermediate-fusion-based methods gradually become a mainstream approach due to the diversity of fusion strategies.

Late-fusion-based methods perform at the bounding box level. Typically, CLOCs  obtains 2D and 3D bounding boxes by separately using 2D and 3D object detectors, and then combine them to achieve more accurate 3D bounding boxes. However, the interaction between modalities in late-fusion-based methods is very limited, which constrains model performance.

These multi-modal methods successfully outperform single-modal methods. However, their feature fusion methods do not take depth into account. In contrast, our approach introduces depth information to guide the hybrid feature fusion, boosting the performance of the detector.

## 3 Methodology

In this section, we first give an overview of our proposed multi-modal 3D object detector, and then provide a detailed introduction to our proposed feature fusion method.

### Overview

We propose a multi-modal 3D object detection method via Depth-Aware Hybrid Feature Fusion (DH-Fusion). As illustrated in Fig. 2, our approach consists of two important feature fusion modules: Depth-Aware Global Feature Fusion (DGF) and Depth-Aware Local Feature Fusion (DLF). In the following, we briefly describe the detection pipeline.

**Inputs.** First, we take the point cloud \(P\) and multi-view images \(I\) as inputs, where point cloud consists of a set of points: \(P=\{P_{1},P_{2},,P_{N_{t}}\}\), and each point has four dimensions: X-axis, Y-axis, Z-axis, and intensity; the multi-view images comprise \(N_{c}\) images: \(I=\{I_{1},I_{2},,I_{N_{c}}\}\), each image captured by its corresponding camera.

**Input Encoding.** For the point cloud \(P\), we use a 3D encoder to extract raw global voxel features \(_{O}^{G}\); for the multi-view images \(I\), we use a 2D encoder to extract image features of all views \(_{O}^{G}\).

**Hybrid Feature Fusion.** Then, for voxel features \(_{O}^{G}\), we compress the height dimension to obtain point cloud BEV features \(_{B}^{G}\); for image features \(_{O}^{G}\), we transform their perspective view to bird's eye view to obtain image BEV features \(_{B}^{G}\). To fully leverage the features from two modalities, we design a DGF module that aims to dynamically adjust the weights of image BEV features based on depth values during feature fusion. Please refer to Sec. 3.2 for more details. To compensate for the information lost when transforming raw features to BEV space, we propose a DLF module that, based on depth, utilizes the raw features to enhance the detailed information of each object instance in global multi-modal features. It consists of three processes: local feature selection, local feature fusion, and merging local features into global features. First, we obtain the local multi-modal BEV features \(_{B}^{L}\), local voxel features \(_{O}^{L}\), and local multi-view image features \(_{O}^{L}\), by cropping the corresponding global features based on the 3D boxes obtained from an RPN; then, it dynamically and individually adjusts the weights of each local feature of \(_{O}^{L}\) and \(_{O}^{L}\) based on depth values during feature fusion; finally, we update local features for each object on the global feature map. Please refer to Sec. 3.3 for more details. In this way, we obtain enhanced multi-modal global features for detection.

**Decoding.** Based on the enhanced multi-modal global features \(}_{B}^{G}\) that contain rich semantic and spatial information, we utilize a transformer decoder and a detection head to predict the object categories and 3D bounding boxes.

Figure 2: Overview of our method. It introduces depth encoding in both global and local feature fusion to obtain depth-adaptive multi-modal representations for detection. \(\) is the multiplication operation, and \(\) is the merge operation.

### Depth-Aware Global Feature Fusion

As shown in Fig. 4, the DGF module consists of a global-fusion transformer with a depth encoder. In the following, we provide a detailed explanation of each component.

#### 3.2.1 Depth Encoder

We introduce depth encoding (DE) in feature fusion to dynamically adjust the weights of image BEV features during fusion. First, we build a depth matrix \(M\) to store the depth value of each position element \(p_{k}\) represented as:

\[p_{k}=\{(x_{k},y_{k}):d_{k}\},k[1,n],\] (1)

where \((x_{k},y_{k})\) are the positional coordinates, \(d_{k}\) is the depth value, and \(n\) is the number of elements. Then, we use Euclidean distance to calculate the distance between every element's spatial location \((x_{k},y_{k})\) and the ego coordinate element's location \((x_{},y_{})\):

\[d_{k}=E((x_{k},y_{k}),(x_{},y_{})),k[1,n],\] (2)

where we denote \(E()\) as the Euclidean distance calculation. The depth matrix \(M\) serves as a lookup table to avoid redundant computation of depth values. Since the size of the BEV features is large and the depth distribution is simple, to avoid introducing additional parameters, the depth encoding \(De\) is obtained by applying sine and cosine functions  to the depth matrix.

#### 3.2.2 Global-Fusion Transformer

In the global-fusion transformer, we take the point cloud BEV features \(^{G}_{B}^{W H C}\) and image BEV features \(^{G}_{B}^{W H C}\) as inputs, and integrate the depth encoding obtained above by multiplying it with the point cloud BEV features, forming the query \(Q^{G}_{V}=N(^{G}_{B} Conv(De))\), where \(Conv()\) is a convolution operation to align with the channels of \(^{G}_{B}\), and \(N()\) is a normalization layer. The image BEV features are queried as the corresponding key \(K^{G}_{}\) and value \(V^{G}_{}\). We utilize the multi-head cross attention to achieve the interacted feature \(}^{G}_{B}\) based on depth:

\[}^{G}_{B}=CA(Q^{G}_{V},K^{G}_{},V^{G}_{ }),\] (3)

where \(CA()\) indicates the multi-head cross attention. Afterward, we aggregate the information from both modalities to obtain the fused features \(^{G}_{B}\):

\[^{G}_{B}=N(FFN(N(}^{G}_{B}+^{G}_{B}))+N (}^{G}_{B}+^{G}_{B})),\] (4)where \(N()\) is a normalization layer; \(FFN()\) specifies a feed-forward network containing two convolution operations. In this way, we obtain fused features in which the image features play different roles as the depth varies.

### Depth-Aware Local Feature Fusion

As shown in Fig. 4, the DLF module consists of a local feature selection and a local-fusion transformer with the depth encoder. In the following, we provide a detailed explanation of each component.

#### 3.3.1 Local Feature Selection

To compensate for the information lost when transforming point cloud features and image features to BEV space, we enhance the instance details of fused BEV features \(_{B}^{G}\) using instance features from raw voxel features \(_{O}^{G}\) and multi-view image features \(_{O}^{G}\). Specifically, we utilize an RPN to regress \(t\) 3D boxes based on the BEV features \(_{B}^{G}\). We directly crop the global fused BEV features \(_{B}^{G}\) based on the regressed 3D boxes to obtain the local fused BEV features \(_{B}^{L}^{c t}\). On the other hand, we project the 3D boxes onto the raw voxel features and multi-view image features to obtain their corresponding local features before global fusion, preserving richer information for each object instance. Specifically, we utilize the voxel pooling operation , followed by a 3D convolution operation and a linear layer, to extract local voxel features \(_{O}^{L}^{c t}\); we transform the 3D boxes from bird's eye view to perspective view, and utilize the RoI Align operation , followed by a linear layer, to extract instance image features \(_{O}^{L}^{c t}\). By doing this, we obtain the hybrid (before & after global fusion) local features, which will be sent to the subsequent fusion module.

#### 3.3.2 Local-Fusion Transformer

In the local-fusion transformer, the weights of each local raw feature are dynamically adjusted based on depth values during feature fusion, and we update local features for each object on the global feature map. Specifically, we take the local multi-modal BEV features \(_{B}^{L}\), local voxel features \(_{O}^{L}\), and local multi-view image features \(_{O}^{L}\) as inputs, and integrate the depth encoding by multiplying it with the local multi-modal BEV features, forming the query \(Q_{}^{L}\). The local multi-view image features and local voxel features are respectively queried as the corresponding key \(K_{}^{L}\), \(K_{}^{L}\) and value \(V_{L}^{L}\), \(V_{L}^{L}\). The two multi-head cross-attention modules are utilized to achieve the interacted features \(_{}^{L}\), \(_{}^{L^{}}\). Note that the computation process of multi-head cross attention is similar to that described in Sec. 3.2.2 and is omitted here. Afterward, we aggregate the above features:

\[}_{B}^{L}=Conv(Cat(_{}^{L}+_{B}^{ L},_{}^{L^{}}+_{B}^{L^{}})),\] (5)

where \(Cat()\) is the concatenation operation; \(Conv()\) is used to align with the feature channels of global fused BEV features \(_{B}^{G}\). As a result, we obtain enhanced local features by dynamically calling back rich information in raw modalities at various depths. Afterward, we update the global features \(_{B}^{G}\) by inserting the enhanced local features at corresponding locations.

## 4 Experiments

In this section, we will first introduce the dataset and evaluation metrics, followed by the implementation details. Then, we will compare our method with the state-of-the-art methods on nuScenes and also present results on a more challenging dataset of nuScenes-C with data corruptions. Finally, we will show the ablation studies and qualitative results. More experiments are provided in Appendix A.2.

### Experimental Setup

**Datasets and evaluation metrics.** We evaluate our proposed DH-Fusion on the nuScenes benchmark  and a more challenging dataset of nuScenes-C  with data corruptions. nuScenes dataset provides 700 scene sequences for training, 150 scene sequences for validation, and 150 scene sequences for testing. Each sequence contains 40 frames of 32-beam LiDAR data, and each frame has six corresponding images covering a 360-degree field of view. It offers calibration matrices that facilitate accurate projection of 3D points onto 2D pixels, and contains 10 object categories that are commonly encountered within autonomous driving. nuScenes-C dataset provides 27 corruptions with 5 severities on the nuScenes validation set, including corruptions at the weather, sensor, motion, object, and alignment level. We use the nuScenes detection scores (NDS) and mean Average Precision (mAP) to evaluate our detection results, where NDS is a comprehensive metric in nuScenes that combines object translation, scale, orientation, velocity, and attribute errors.

**Implementation details.** We implement the proposed DH-Fusion with PyTorch  under the open-source framework MMDetection3D . Specifically, for the LiDAR branch, we use VoxelNet  with FPN  as the 3D encoder. The voxel size is set to [0.075m, 0.075m, 0.1m], and the range of point cloud is [-54m, 54m] along the X-axis, [-54m, 54m] along the Y-axis, and [-3m, 5m] along the Z-axis. For the image branch, we use the ResNet18 , ResNet50 , and SwinTiny  with FPN  as the 2D image encoder of DH-Fusion-light, -base, -large, respectively. Correspondingly, the resolution of input images is resized to 256 \(\) 704, 320 \(\) 800, and 384 \(\) 1056. Additionally, we utilize BEVPoolV2  to obtain image BEV features. Following , the feature size \(W H\) is set to 180 \(\) 180, the channel \(C\) is set to 128, and the channel \(c\) is also set to 128. The multi-head cross attention is implemented with 8 heads, and the FFN contains 2 MLP layers with a hidden dimension of 128. Following , the number of regressed 3D boxes \(t\) is set to 200. More implementation details are provided in Appendix A.1.

### Comparison to the State of the Art

Aiming for a fair comparison, we categorize previous methods based on the types of 2D backbones into ResNet50-based, SwinTiny-based, and others, and provide three versions of our proposed method, named DH-Fusion-light, DH-Fusion-base, and DH-Fusion-large. The results are shown in Tab. 1. (1) Compared with the ResNet50-based methods, our DH-Fusion-base outperforms the top method FocalFormer3D  by up to 1 pp w.r.t. NDS under the same configuration. Specifically, we reach 74.0% w.r.t. NDS and 71.2% w.r.t. mAP on the validation set, and 74.7% w.r.t. NDS and 71.7% w.r.t. mAP on the test set, while maintaining comparable inference speed of 8.7 FPS on a 3090 GPU. (2) Compared with the SwinTiny-based methods and others, our DH-Fusion-large outperforms the top method IS-Fusion  under the same configuration, and runs 2x faster than it. Specifically, we reach 74.4% w.r.t. NDS on the validation set, and 75.4% w.r.t. NDS on the test set, while achieving a faster inference speed of 5.7 FPS on a 3090 GPU, indicating that our proposed method is both more effective and efficient. (3) Furthermore, our DH-Fusion-light surpasses the typical BEVFusion  by up to 1 pp w.r.t. all metrics using a lighter 2D backbone, and achieves a real-time inference speed of 13.8 FPS. Overall, our method achieves higher detection accuracy and faster inference speed.

### Robustness to Corruptions

We further implement some experiments on the nuScenes-C  dataset to evaluate the model's robustness under various corruptions, including changes in weather, data loss or temporal-spatial misalignment in multi-modal inputs, etc. The results for different kinds of corruptions are shown in Tab. 2, and more detailed results for each fine-grained corruption are shown in Appendix A.2.3. We find that our DH-Fusion-light still achieves an average performance of 68.67\(\%\) w.r.t. NDS and 63.07\(\%\) w.r.t. mAP under various corruptions, which only decreases by 4.63 pp w.r.t. NDS and 6.68 pp w.r.t. mAP, compared to its performance without corruptions. Performance drop is smaller than that observed with previous methods including BEVFusion  across all kinds of corruptions, indicating that our DH-Fusion-light possesses superior robustness. Furthermore, we observe that our DH-Fusion-light is particularly robust against weather and object corruptions, where the performance drop is less than 3pp. The more stable performance indicates that our method is more friendly to practical applications, where data corruption may occur.

### Ablation Studies

We conduct ablation studies to first demonstrate the effect of each component of DH-Fusion, then to demonstrate the effect of depth encoding in DGF and DLF, and finally to assess the impact of multiplying depth encoding. All method variants are implemented on the nuScenes validation dataset.

**Effect of DGF and DLF.** To demonstrate the effect of DGF and DLF, we conduct experiments by integrating the components one by one into the baseline, BEVFusion . The results are shown in Tab. 3. We find that our DGF improves the baseline performance by 1.0 pp w.r.t. NDS and 0.9 pp w.r.t. mAP. This demonstrates that dynamically adjusting the weights of the image BEV features during fusion is effective for 3D object detection. Additionally, our DLF improves the baseline performance by 1.3 pp w.r.t. NDS and 0.8 pp w.r.t. mAP, which indicates that dynamically adjusting the weights of the local raw instance features based on depth during fusion effectively compensates for the information loss caused by the transformation of global features into the BEV feature space. The results of integrating both components show an improvement of 1.9 pp w.r.t. NDS and 1.3 pp w.r.t. mAP, well verifying the benefits of dynamically fusing global and local hybrid features based on depth.

**Effect of depth encoding in DGF and DLF.** To evaluate the effectiveness of our depth encoding, we conduct experiments where the depth encoding is removed from the DGF and DLF modules, respectively. The results are shown in Tab. 4. When removing the depth encoding from Baseline+DGF, the performance drops by 0.6 pp w.r.t. NDS and 0.4 pp w.r.t. mAP. Similarly, when removing the depth encoding from Baseline+DLF, the performance also decreases by 1.1 pp w.r.t. NDS and 0.9 pp w.r.t. mAP. These results indicate that our depth encoding is effective. Furthermore, we observe that removing the depth encoding from the DLF module results in a larger performance drop, suggesting that depth encoding plays a more crucial role in local feature fusion.

**Impact of different operations for depth encoding.** We conduct experiments with different operations of depth encoding, including concatenation, summation, and multiplication. The results in Tab. 5, show that the multiplication operation consistently outperforms the summation and concatenation operations w.r.t. both metrics. The superior performance of multiplication can be attributed to its ability to more effectively modulate the feature maps based on depth information. Unlike summation, which simply shifts the feature values, or concatenation, which increases the dimensionality without direct interaction, multiplication allows for more

   &  &  &  &  &  \\  & & & NDS & mAP & & NDS & mAP \\   Training & CVPR 22 & 320 \(\) 800-ResNet50 & 6.5 & 71.3 & 67.5 & 71.7 & 68.9 \\ DeepInteraction  & NeurIPS22 & 448 \(\) 800-ResNet50 & 1.9 & 72.4 & 69.9 & 73.4 & 70.8 \\ MSMDFusion  & CVPR 23 & 448 \(\) 800-ResNet50 & 2.1 & 72.1 & 69.7 & 74.0 & 71.5 \\ FocalFormedD (7) & ICCV23 & 320 \(\) 800-ResNet50 & 9.2 & 73.1 & 70.1 & 73.9 & 71.6 \\ 
**DH-Fusion-based (Ours)** & - & 320 \(\) 800-ResNet50 & 8.7 & **74.0** & **71.2** & **74.7** & **71.7** \\    \\  BEVFusion  & NeurIPS22 & 448 \(\) 800-SwinTiny & 0.7* & 71.0 & 67.5 & 71.8 & 69.2 \\ BEVFusion  & ICRA23 & 256 \(\) 704-SwinTiny & 9.6 & 71.4 & 68.5 & 72.9 & 70.2 \\ ObjectFusion  & ICCV23 & 256 \(\) 704-SwinTiny & - & 72.3 & 69.8 & 73.3 & 71.0 \\ SparseFusion  & ICCV23 & 256 \(\) 704-SwinTiny & 4.4 & 72.8 & 70.5 & 73.8 & 72.0 \\ IS-Fusion  & CVPR 24 & 384 \(\) 1056-SwinTiny & 3.2* & 74.0 & **72.8** & 75.2 & **73.0** \\   \\  AutoAlignV2  & ECCV22 & 640 \(\) 1280-CSPNet  & 4.8* & 71.2 & 67.1 & 72.4 & 68.4 \\ UVIR  & NeurIPS22 & 640 \(\) 1280-ResNeNeNeNe  & 1.8 & 70.2 & 65.4 & 71.1 & 67.1 \\ FUTRR3D  & CVPR 239 & 900 \(\) 1600-OVNet  & 3.3* & 68.0 & 64.2 & 72.1 & 69.4 \\ UnTrit  & ICCV23 & 256 \(\) 704-DSVT  & 9.3* & 73.3 & 70.5 & 74.5 & 70.9 \\ CMT  & ICCV23 & 640 \(\) 1600-OVNet & 6.0* & 72.9 & 70.3 & 74.1 & 72.0 \\ UniPAD  & CVPR 24 & 900 \(\) 1600-Conv-NexeS  & - & 73.2 & 69.9 & 73.9 & 71.0 \\ 
**DH-Fusion-large (Ours)** & - & 384 \(\) 1056-SwinTiny & 5.7 & **74.4** & 72.3 & **75.4** & 72.8 \\
**DH-Fusion-light (Ours)** & - & 256 \(\) 704-ResNet18 & **13.8** & 73.3 & 69.8 & 74.2 & 70.9 \\  

Table 1: Comparisons with the state of the art on the nuScenes validation and test sets. FPS is measured on a 3090 GPU by default, and * denotes the inference speed on an A100 GPU referred from the original paper. Note that all results are obtained without any model ensemble or test time augmentation.

   &  &  \\    & None & Wezhner & Senator & Motion & Object & Alignment & \\  FUTR3D  & 68.05 / 64.171 & 62.75 / 55.1 & 63.66 / 66.83 & 53.16 / 44.3 & 65.45 / 61.04 & 62.83 / 57.60 & 62.83 / 57.60 \\ TransFusion  & 69.82 / 66.38 & 65.42 / 59.37 & 66.17 / 59.82 & 51.52 / 41.47 & 68.28 / 64.38 & 61.98 / 54.94 & 63.74 [68, 68 / 58.73 / 76.55 \\ BEVFusion  & 71.40 / 68.45 & 67.54 / 61.87 & 67.59 / 61.80 & 55.19 / 47.30 & 68.01 / 65.14 & 63.94 / 58.71 & 66.06 [54, 61 / 61.03 / 74.22 \\ 
**DH-Fusion-light (Ours)** & **73.30** / **69.75** & **72.19** / **67.48** & **69.16** / **62.87** & **57.07** / **47.52** & **71.01** / **67.11** & **67.24** / **62.38** & **68.67** / 41.43 / **63.07** / 30.08 \\   

Table 2: Robustness experiments on nuScenes-C. Numbers are **NDS / mAP**.

depth encoding and features, leading to better feature representation and ultimately improving the detection performance.

### Qualitative Results

To better understand how depth encoding affects the feature fusion, in Fig. 6, we plot a curve to observe how the attention weights applied on the image BEV features in our DGF module vary with depth, and visualize the average attention map. It is evident that the weights of the image BEV features stay low in near range, but go up significantly as depth increases when the depth is larger than 40 meters. This trend supports our hypothesis that the image modality would become more important as depth increases. In this way, our depth encoding allows the model to dynamically adjust the weights of image BEV features based on depth.

We also compare the detection results of our DH-Fusion method with the baseline BEVFusion  in Fig. 6, where we clearly find that our method better localizes those distant objects compared to BEVFusion. These results demonstrate that our proposed multi-modal fusion strategy based on depth is more effective for detection. Besides, we exhibit the corresponding BEV feature maps, where our method shows a stronger feature response for the foreground objects, especially for distant ones. That is why our feature fusion strategy can provide higher-quality detection results. More qualitative results can be found in Appendix A.3.

## 5 Conclusion

In this paper, we for the first time point out that different modalities play different roles as depth varies via statistical analysis and visualization. Based on this finding, we propose a feature fusion strategy for multi-modal 3D object detection, namely Depth-Aware Hybrid Feature Fusion (DH-Fusion), that dynamically adjusts the weights of features during feature fusion by introducing depth encoding at both global and local levels. Extensive experiments on the nuScenes dataset demonstrate that our DH-Fusion method surpasses previous state-of-the-art methods w.r.t. NDS. Moreover, our DH-Fusion is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C dataset w.r.t. both NDS and mAP. Our method uses an attention-based approach to interact with the two modalities, making the detection results sensitive to modality loss. We plan to further explore feature fusion methods that are robust to modality loss. Although our method improves detection performance, emergency plans still need to be implemented in practical applications to ensure personnel safety.