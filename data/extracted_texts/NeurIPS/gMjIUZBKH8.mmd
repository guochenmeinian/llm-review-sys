# AdaVAE: Bayesian Structural Adaptation for Variational Autoencoders

Paribesh Regmi  Rui Li

Rochester Institute of Technology

{pr8537, rxlics}@rit.edu

Corresponding author

###### Abstract

The neural network structures of generative models and their corresponding inference models paired in variational autoencoders (VAEs) play a critical role in the models' generative performance. However, powerful VAE network structures are hand-crafted and fixed prior to training, resulting in a one-size-fits-all approach that requires heavy computation to tune for given data. Moreover, existing VAE regularization methods largely overlook the importance of network structures and fail to prevent overfitting in deep VAE models with cascades of hidden layers. To address these issues, we propose a Bayesian inference framework that automatically adapts VAE network structures to data and prevent overfitting as they grow deeper. We model the number of hidden layers with a beta process to infer the most plausible encoding/decoding network depths warranted by data and perform layer-wise dropout regularization with a conjugate Bernoulli process. We develop a scalable estimator that performs joint inference on both VAE network structures and latent variables. Our experiments show that the inference framework effectively prevents overfitting in both shallow and deep VAE models, yielding state-of-the-art performance. We demonstrate that our framework is compatible with different types of VAE backbone networks and can be applied to various VAE variants, further improving their performance.

## 1 Introduction

The inference models and the generative models paired in variational autoencoders (VAEs) are commonly constructed with neural networks, i.e., encoding networks and decoding networks, respectively [1; 2; 3]. Extensive research efforts show that well-designed encoding/decoding network structures for VAE models can constantly achieve state-of-the-art generative performance compared to other generative models [4; 5; 6]. However, powerful VAE network structures are hand-crafted and fixed prior to training. The issue with fixed network structures is that shallow ones limit VAE models' expressiveness, whereas overly deep networks are slow to use and prone to overfitting. Traditional model selection by training different VAE network structures for given data is difficult since finding optimal hyperparameters for each candidate structure is a daunting task, and training large VAE structures requires significant computation. On the other hand, network structure adaptation methods for discriminative model settings [7; 8; 9] cannot be straightforwardly applied to address the unique challenge posed by VAE estimation along with the latent variables.

Although the network structures play a critical role in the performance of VAE models, they are largely overlooked by current VAE regularization methods. This renders their failure to prevent overfitting when the network structures grow deeper. Amortized inference regularization (AIR) proposes two approaches: injecting random noise to the VAE objective for inference or directly restricting the inference models to a set of smooth functions [10; 11]. Another approach in VAE regularizationincorporates additional constraints to the VAE objective, which enforces a similarity between original input and its semantic preserving transformation in the latent representation [12; 13]. Nonparametric Bayesian inference such as Indian buffet process deep generative models (IBP-DGM)  and beta-Bernoulli process VAE (BB-VAE)  address overfitting by inferring the dimensionality of VAE latent variables using an Indian buffet process, specifically, a marginalized beta-Bernoulli process prior. While these regularization methods are effective for shallow network structures, they fail to prevent overfitting in deep VAE models.

We propose a novel Bayesian inference framework that automatically adapt VAE network structures by inferring the most plausible encoding/decoding network depths based on the given data, as demonstrated in Figure 1. To achieve this, we employ beta processes [16; 17] to model the number of hidden layers in the encoding/decoding networks, allowing for infinite depths. A conjugate Bernoulli process is utilized to prune the neurons in each layer based on layer-wise activation probabilities generated by the beta process. In addition, to enable efficient joint inference on both the network structures and the latent variables, we extend multiply importance weighted autoencoder (MIWAE)  by introducing an additional sample size used for Monte Carlo estimation of the network structures to be tuned. Our theoretical and empirical analysis shows that this novel gradient estimation scheme leads to a tight lower bound with high signal-to-noise ratio of parameter gradients.

In summary, our contributions are: i) We propose AdaVAE, a novel VAE structural adaptation strategy based on Bayesian model selection to enhance model performance. ii) We introduce a scalable estimator that facilitates joint inference on both encoding/decoding network structures and latent variables. iii) We conduct a comprehensive analysis of AdaVAE's regularization capabilities and demonstrate its ability to effectively mitigate overfitting in both shallow and deep VAE models and achieve state-of-the-art performance. iv) We showcase the versatility of AdaVAE by demonstrating its compatibility with different types of VAE backbone networks. It can also be readily applied to various VAE variants, thereby enhancing their performance.

## 2 Related Works

Variational autoencoders (VAEs) have gained popularity as generative models across a wide range of applications [19; 20; 21; 22; 23; 24]. Extensive research efforts have focused on enhancing the performance of VAEs [25; 26; 27; 28; 29; 30; 31; 32]. Notably, well-designed encoding/decoding neural network structures constantly yield state-of-the-art generative performance [4; 5; 6; 33]. Ladder-VAE (LVAE) employs a shared top-down dependency structure in both the inference and generative models to facilitate information sharing between layers . SkipVAE employs skip connections that connect latent variables to each layer in the generative model, mitigating posterior

Figure 1: (a): Demonstration of the VAE network structure inference framework. Beta processes induce infinite number of hidden layers for encoding/decoding networks, and its conjugate Bernoulli process prunes the neurons in each layer with a layer-wise activation probability from the beta process. Filled circles indicate activated neurons, corresponding to a sample of 1 from the Bernoulli process, while empty circles correspond to deactivated neurons, corresponding to a sample of \(0\). (b) and (c): Two settings of the stick-breaking constructions of beta process. The sticks on top are random draws from the process, which act as the layer-wise activation probabilities. A stick location \(_{_{l}}\) corresponds to a hidden-layer function \(_{l}\), and the height denotes its activation probability \(_{l}\). The bottom shows the conjugate Bernoulli processes to activate or deactivate neurons in each layer (column-wisely).

collapse . BIVA consists of a skip-connected generative model and an inference model formed by a bidirectional stochastic inference path . NVAE designs expressive neural network structures tailored for VAEs based on deep residual networks . Furthermore, advancements in VAE network structure design have also empowered recent works on hybrid models based on VAEs [35; 36; 37; 38].

Current VAE regularization methods often overlook the overfitting effect caused by deep network structures [12; 10; 15; 11; 13]. Amortized inference regularization (AIR) re-interprets the amortized inference model as a regularization for maximum likelihood training. AIR encourages the smoothness of the encoding network to restrict the model capacity, effectively mitigating overfitting and improving predictive performance . Specifically, AIR proposes a denoising variational autoencoder (DVAE) that modifies the VAE objective using random perturbation training. An alternative AIR technique is weight-normalized inference VAE (WNI-VAE) that directly restricts the encoding networks to a set of smooth functions and achieves comparable performance to DVAE. Both approaches exhibit a lower inference gap than standard VAEs . Consistency regularization for VAE (CR-VAE) extends the regularization techniques employed in semi-supervised learning [40; 41] to tackle the inconsistency problem of the inference models by enforcing the latent representations of an image and its semantic-preserving transformation to be similar, thereby yielding robust latent representations accounting for data variation .

IBP-DGM  and BB-VAE as nonparametric Bayesian inference applied to VAE regularization focus on inferring the dimensionality of latent variable via an Indian Buffet Process (IBP) prior. The IBP prior is derived by marginalization over a beta process, resulting in a binary vector that masks the VAE latent variables. Without accommodating network structures, latent variable regularization alone is inadequate for mitigating overfitting.

## 3 Efficient VAE Estimators

Maximum likelihood estimation of a vector of parameters \(\) of a deep generative model \(p_{}(,)\) with \(\) denoting observed variables and \(\) denoting latent variables is intractable in general due to the marginalization over \(\). Amortized variational inference optimizes an evidence lower bound (ELBO) \(_{,}()\) on the log marginal likelihood \( p_{}()= p_{}(,)d \) by introducing a variational distribution \(q_{}(|)\):

\[\;p_{}()  q_{}(|)( ,)}{q_{}(|)}d\] (1) \[=_{q_{}(|)}[ p_{}( |)]-[q_{}(|)||p_{} ()]\] \[=_{,}()\]

where \(\) denotes variational parameters. For VAEs, \(q_{}(|)\) denotes an inference model commonly constructed with a neural network. \(p_{}(|)\) denotes a generative model that can also be parameterized by a neural network. The ELBO is optimized with gradient-based methods via reparameterization trick using stochastic Monte Carlo estimators of \(_{,}\).

IWAE obtains a tighter lower bound using \(K\)-sample importance weighting estimate of the log marginal likelihood :

\[_{,}()= _{q_{}(|)}[_{k=1}^{K}(|_{k})p_{}( _{k})}{q_{}(_{k}|)}]\] (2)

where \(_{k} q_{}(|)\). It shows that the bound gets tighter with increasing \(K\). However,  presents theoretical and empirical evidence that increasing the importance weighted sample size \(K\) to tighten the bound degrades signal-to-noise ratio (SNR) of parameter gradients estimates for the encoding network, and hurts the learning process. A new estimator (MIWAE) is thus introduced to address the issue of diminishing SNR, and its gradient estimate is:

\[_{M,K}=_{m=1}^{M}_{,}_ {k=1}^{K}(|_{m,k})p_{}(_{ m,k})}{q_{}(_{m,k}|)}\] (3)

where \(_{m,k} q_{}(_{m,k}|)\). For a fixed budget \(M K\) of total number of hidden variable samples, the number of samples \(M\) reduces the variance in estimating the ELBO gradient. \(K\) is the importance sample size as in IWAE.

VAE Structure Inference Framework

Traditional model selection cannot effectively adapt pre-determined VAE network structures to data without incurring significant computation overhead. We thus propose AdaVAE that enables joint inference on the structures of the encoding/decoding networks using stochastic processes [43; 9] and latent variables, and optimizes VAE objective without requiring additional expensive computation.

### Formulation of the Inference Model

Let the prior over the latent variables \(\) be a zero-mean isotropic multivariate Gaussian \(p_{}()=(;0,I)\). We formulate the inference model by letting the variational distribution over \(\) be a multivariate Gaussian with a diagonal covariance structure:

\[q_{}(|,)=(;, ^{2}I)\] (4)

where the mean \(\) and the standard deviation \(\) are outputs of the encoding neural network \(_{}()\) with the variational parameters \(\). The binary matrix \(=[z_{ol}\{0,1\}]\) denotes the network structural variable, as in Figure 1(a).

Let \(_{l}\) denote the \(l\)-th hidden layer of \(_{}()\) composed of neurons (i.e., non-linear activation functions) \(f()\). The encoding network \(_{}()\) has the form:

\[_{l}=f(_{l}_{l-1})_{l}+_{l-1} l\{1,2,...,\}\] (5)

where \(=\{_{l}^{O O}\}\), and \(_{l}\) is the layer \(l\)'s weight matrix. \(\) denotes element-wise multiplication of two vectors, so that we drop out the \(l\)-th layer's outputs by multiplying them elementwisely by the column vector \(_{l}\) of \(\). Each random variable \(z_{ol}\) takes the value \(1\) with \(_{l}\) indicating activation probability of the \(l\)-th layer, as in Figures 1 (b),(c). \(O\) is the maximum number of neurons in a layer that is set to be the same for all hidden layers. We have skip connections to propagate the outputs of the hidden layers to the output layer. Note that when this network structure is used as the decoding network for the generative model \(p_{}(|)\), then \(\) and \(\) are swapped, and \(\) denotes the weights parameters. The output-layer can be readily replaced with a logistic function and the normal distribution with a Bernoulli distribution for binary data.

### Beta Process Prior over Layer Number

A beta process \(B=_{l}_{l}_{_{l}}\), where \(_{_{l}}\) is a unit point mass at \(_{l}\), is a completely random measure over countably infinite set of pairs \((_{l},_{l})\), where \(_{l}\) denotes a hidden-layer function and \(_{l}\) is its activation probability \(_{l}\). Its conjugate Bernoulli process can be defined as \(_{o}(B)\), where \(_{o}\). \(=_{l}z_{ol}_{_{l}}\) is at the same locations \(_{_{l}}\) as \(B\) where \(z_{ol}\) are independent Bernoulli variables with \(_{l}\) being the probability of \(z_{ol}=1\). As in Eqn. (5), \(z_{ol}=1\) activates the \(o\)'th neuron in layer \(l\). Computationally, we employ the stick-breaking construction  of beta process and its conjugate Bernoulli process as

\[z_{ol}(_{l}),_{l}=_{j=1}^{l}_{j},_{l} (,)\] (6)

where \(_{l}\) are sequentially drawn from a beta distribution. The hyperparameters \(\) and \(\) can be set to balance the network depth and width. Specifically, Figure 1 (b) demonstrates that if \(>>1\), the network structure prior favors shallower but wider network structures with the first few layer-wise activation probabilities being high. If \(>>1\), activation probabilities tend to be low over larger number of active layers, and the prior prefers a deeper but narrower network, as in Figure 1 (c).

We thus define the prior over the encoding network structural variable \(\) as

\[p_{,}(,)=p_{,}()p( |)=_{l=1}^{}(_{l}|,)_{o=1}^{ O}(z_{ol}|_{l})\] (7)

To enable asymmetric encoding/decoding network structures, we independently apply the prior to both networks. Further analysis on symmetric constraints can be found in the Appendix.

### Joint Inference on VAE Network Structures and Latent Variables

We first expand the overall marginal likelihood over the VAE structures \(\) as

\[ p_{,}()=KL[q(,|\{a_{t}\}_{t= 1}^{T},\{b_{t}\}_{t=1}^{T})||p_{,}(,| )]+_{\{a_{t}\},\{b_{t}\}}()\] (8)

where the first RHS term denotes a Kullback-Leibler (KL) divergence of the approximate variational distribution from the true posterior of the VAE network structural variables. We specify the variational distribution as

\[q(,|\{a_{t}\}_{t=1}^{T},\{b_{t}\}_{t=1}^{T})=_{ t=1}^{T}(_{t}|a_{t},b_{t})_{o=1}^{O}(z_{ot}| _{t})\] (9)

where \(_{t}=_{j=1}^{t}_{j}\), and \(\{a_{t},b_{t}\}_{t=1}^{T}\) are the variational parameters. \(T\) denotes a truncation level for the maximum number of hidden layers . We also relax the constraint of the discrete variables by reparameterizing the Bernoulli distribution into a concrete Bernoulli distribution \((z_{ot}|_{t})\)[44; 45]. This allows us to efficiently backpropagate the parameter gradients of the estimator while generating network structure samples.

The second RHS term in Eqn. (8) denotes the ELBO to the overall marginal likelihood:

\[_{\{a_{t}\},\{b_{t}\}}()=  q(,|\{a_{t}\}_{t=1}^{T}\{b_{t}\}_{ t=1}^{T})( p_{}(|)+ p_{,}(, )\] \[- q(,|\{a_{t}\}_{t=1}^{T}\{b_{t}\} _{t=1}^{T}))dd\] (10)

The term \( p_{}(|)\) in Eqn. (10) is the marginal likelihood over the latent variable \(\), which is an extension of Eqn. (1), in terms of conditioning on the structure variable \(\). Thus, the ELBO to the marginal likelihood \( p_{}(|)\) is:

\[\ p_{}(|)  q_{}(|,)(,|)}{q_{}(|, )}d\] \[=_{q_{}(|,)}[ p_ {}(|,)]-[q_{}(| ,)||p_{}(|)]\] \[=_{,}(|)\] (11)

**Lemma 1**: _Let \(Q(|)= q_{}(|,)q( ,)dd\) be the variational distribution of the latent variable \(\) marginalizing over \(\), then_

\[_{q(,)}_{q_{}(| ,)}(,|)} {q_{}(|,)}_{Q(| )}(,|)}{Q(|)}\] (12)

Lemma 1 indicates that the overall ELBO we derived on the left-hand side in Eqn. (12) bounds the lower bound to the marginal likelihood over \(\). The proof is in the appendix. In particular, \(q(,)\) is essentially a non-explicit mixing distribution . It allows the variational distribution \(Q(|)\) to take complex form, and results in more informative latent representation.

We adopt Monte Carlo estimation of the expectations over both \(\) in Eqn. (10) and \(\) in Eqn. (11) to estimate the overall ELBO. In particular, we extend the MIWAE estimator in Eqn. (3), and introduce three sample sizes to tune: the number of samples \(S\) used for Monte Carlo estimation of the expectation over the VAE network structure variable \(\), the number of samples \(M_{s}\) used for Monte Carlo estimation of the gradient of the latent variable ELBO conditioned on the structure samples in Eqn. (11), and the number of importance samples \(K_{s}\) used for estimation of the expectation over the latent variables \(\). We thus express our gradient estimate in the general form as

\[_{S,M,K}=_{s=1}^{S}}_{m=1}^{M_{s}} _{,}}_{k=1}^{K_{s}}(| _{m,k},_{s})p_{}(_{m,k}|_{s})}{q_ {}(_{m,k}|,_{s})}\] (13)

where \(_{m,k} q_{}(|,)\). When \(S=1\) our estimator becomes equivalent to the MIWAE objective in Eqn. (3). Since our estimator generates latent variable samples conditioned on network structure samples, increasing \(S\) will not impact the SNR of the gradient estimator in Eqn. (13).

**Theorem 1**: _Let \(_{S}\) be the lower bound with \(S\) structure samples of \(_{s} q(,)\), then:_

\[_{S}_{S+1} p_{,}(), _{S}=_{q(,)}_{q_{} (|,)}[_{s}( ,|_{s})}{q_{}(|, _{s})}]\] (14)Proof of this theorem is in the appendix. The theorem shows the convergence of our estimator. Specifically, increasing \(S\) leads to a tighter lower bound for the overall marginal likelihood. Training an encoding/decoding network with depth \(L\) and width \(M\), the time complexity is \(T_{c}=O(NBLM^{2})\) with \(N\) training examples and \(B\) epochs. Our method is linearly scalable as \(ST_{c}\). With a proper thresholding, the number of active layers \(L\) is relatively small in each sample.

## 5 Experiments

We analyze the behavior of our inference framework across various tasks. We study how AdaVAE facilitates the evolution of encoding/decoding network structures for inferring the most plausible depth from the given data, while generating expressive latent representations. Next, we explore the impact of the structure sample size \(S\) on the convergence of the proposed estimator in Eqn.(13). Then we show that AdaVAE effectively mitigates overfitting in both shallow and deep network settings, leading to state-of-the-art performance on benchmark datasets. Finally, we demonstrate the framework's compatibility with different types of backbone networks and VAE variants. 2

### Adaptive VAE Network Structures

AdaVAE enables us to perform joint inference on both encoding/decoding network structures and latent variables. To investigate how network structures evolve during training epochs, we set the truncation level \(T=25\) on MLP backbone nets with \(tanh\) non-linearities. We analyze adaVAE's behavior on \(28 28\) binarized MNIST images , employing structure sample sizes \(S=\{1,2,4,8\}\). We run the experiments 3 times and averaging the outcomes. Figure 2 Left shows the evolution of the encoding/decoding network structures for one trial with \(S=8\). AdaVAE initializes multiple hidden

   Dataset & (M,K) & S=1 & S=2 & S=4 & S=8 \\  MNIST & (8,8) & \(}\) & \(82.50_{0.00}\) & \(82.60_{0.10}\) & \(82.30_{0.10}\) \\  & (4,16) & \(82.47_{0.45}\) & \(82.33_{0.13}\) & \(82.52_{0.02}\) & \(83.02_{0.20}\) \\  Omniglot & (8,8) & \(107.10_{0.10}\) & \(106.45_{0.10}\) & \(106.55_{0.30}\) & \(}\) \\  & (4,16) & \(108.12_{0.16}\) & \(107.15_{0.08}\) & \(107.35_{0.40}\) & \(108.30_{0.50}\) \\  Caltech101 & (8,8) & \(116.83_{1.57}\) & \(114.94_{0.45}\) & \(114.00_{0.42}\) & \(113.54_{0.40}\) \\  & (4,16) & \(116.30_{1.11}\) & \(114.55_{1.18}\) & \(113.02_{0.34}\) & \(}\) \\   

Table 1: Test performance in negative log-likelihood (\(-LL\)) mean \( 1\) standard deviation (lower the better) over \(4\) runs with random initialization. The overall best result on each dataset is bolded.

Figure 2: Left: Evolution of the encoding/decoding network structures visualized through layer-wise activation probabilities \(_{l}\) (top) and neuron activations \(\) (bottom) with a reconstructed sample. Right: the top shows the median change of the number of active layers over training epochs and the percentage of activated neurons in the truncation. The bottom shows the convergence of the proposed estimator in terms of negative log-likelihood (\(-LL\)) for different structure sample size \(S\).

layers with sparsely activated neurons, gradually converging to fewer fully activated layers. Figure 2 right top presents the medians of the number of active hidden layers in the encoding/decoding networks, as well as the percentage of activated neurons in the truncation changing over epochs. The encoding network structure stabilizes at two active layers, while the decoding network settles at six active layers. The decoding network tends to have more active layers compared to the encoding network on Omniglot as well (see Appendix). Figure 2 right bottom shows the generative performance assessed by negative log-likelihood (\(-LL\)) converge faster with an increased structure sample size \(S\), which is consistent with **Theorem 1**.

### Effect of Structure Sample Sizes

We assess our estimator using three benchmark datasets: MNIST , Omniglot , and Caltech101 Silhouettes . In each minibatch, we set a budget of \(S M_{s} K_{s}=64\) total latent variable samples for each datapoint. We examine four settings of the VAE structure sample size \(S=\{1,2,4,8\}\), along with latent variable sample sizes \((M,K)=\{(8,8),(4,16)\}\) as in . The truncation level is \(T=25\) with a maximum width \(O=200\). The distribution over the output from the decoding networks is factorized Bernoulli. Table 1 indicates larger values of \(S\) generally yield better performance. For MNIST, there is no statistically significant difference between \(S=1\) and \(S=8\). Among the two structure sample sizes, the best importance sample configuration is \((M,K)=(8,8)\).

### On Preventing Overfitting

To assess the performance of our method across varying truncation level \(T\), we compare with existing VAE regularization methods: Denoising VAE (DVAE) , CR-VAE , and BB-VAE , MIWAE with dropout (MIWAE+DO), along with vanilla MIWAE . as in Figure 3. All methods share the same maximum width of \(O=200\) and a latent variable dimensionality of \(50\). For

    &  &  &  \\   & -LL \(\) & MI \(\) & -LL \(\) & MI \(\) & -LL \(\) & MI \(\) \\  MIWAE  & 86.11\(\)0.01 & **9.13\(\)0.01** & 110.61\(\)0.10 & **8.98\(\)0.00** & 116.19\(\)0.08 & 7.64\(\)0.01 \\ MIWAE+DO & 90.99\(\)0.01 & **9.13\(\)0.01** & 110.89\(\)0.01 & **8.98\(\)0.00** & 116.00\(\)0.40 & 7.63\(\)0.01 \\ DVAE  & 87.67\(\)0.17 & 9.07\(\)0.00 & 112.04\(\)0.13 & 8.96\(\)0.00 & 113.71\(\)0.43 & 7.53\(\)0.01 \\ CR-VAE  & 87.67\(\)0.05 & 9.03\(\)0.01 & 109.94\(\)0.19 & 8.97\(\)0.00 & 117.46\(\)0.10 & 7.37\(\)0.01 \\  IBP-DGM  & 92.24\(\)0.75 & - & 124.01\(\)1.62 & - & 135.23\(\)0.54 & - \\ BB-VAE  & 91.55\(\)0.69 & - & 124.47\(\)0.58 & - & 135.18\(\)0.83 & - \\  Ours & **82.30\(\)0.10** & **9.13\(\)0.01** & 106.34\(\)0.01 & **8.98\(\)0.00** & 113.54\(\)0.40 & **7.67\(\)0.02** \\ Ours+DVAE & 83.30\(\)0.20 & 9.07\(\)0.02 & 107.80\(\)0.40 & 8.96\(\)0.00 & 111.92\(\)0.37 & 7.63\(\)0.00 \\ Ours+CR-VAE & 85.20\(\)0.20 & 9.03\(\)0.01 & **105.60\(\)0.05** & 8.97\(\)0.00 & **108.93\(\)1.40** & 7.51\(\)0.02 \\   

Table 2: The best performance of our method and the VAE regularization methods in Figure 3. We also demonstrate the compatibility of our framework.

Figure 3: The performance of VAE regularization methods changes with network depths. Our proposed method effectively prevents overfitting for both small and large truncations \(T\), consistently achieving the best performance. In contrast, for \(1 L 5\), the performance of other regularization methods initially improves but then starts to decline, suggesting they suffer from overfitting even for shallow structures.

IBP-DGM and BB-VAE, we set the maximum dimensionality of the latent variables to \(100\), following the setup of . In Figure 3, we set depth \(L\) for the regularization methods and truncation level \(T\) for our method over the range \(L=T=\{1,2,3,4,5,10,15,20,25\}\), allowing us to compare their effectiveness in both shallow (i.e., \(L=T 5\)) and deep (i.e., \(L=T 5\)) VAE network structures.

Figure 3 shows that when \(L=T 5\), the VAE regularization methods can mitigate overfitting. However, even for the shallow cases as \(L=T=\{4,5\}\), the performance of these methods is affected by overfitting, as evidenced by the "U"-shaped performance curves, indicating the classical variance-bias trade-off. Despite our method exhibiting slight underfitting issues for \(L=T 2\), it still outperforms other methods. For deep VAE structures with \(T/L=\{10,15,20,25\}\), Figure 3 shows that our method's performance is minimally impacted by large truncations. This robustness indicates our method's ability to mitigate overfitting for large VAE structure settings. In contrast, as the depth \(L\) increases, the steadily increasing negative log-likelihood (\(-LL\)) for other regularization methods suggests their inability to prevent overfitting in deep VAE models. Overall, AdaVAE consistently delivers superior performance for both shallow and deep VAE structures. An analysis of the computational time required by our framework and its comparison to the baselines is presented in the Appendix.

Table 2 highlights our superiority over other methods in terms of density estimation and mutual information (MI) . Additionally, when incorporating our framework with DVAE through the addition of Gaussian noise to the input or with CR-VAE by integrating the consistency regularization term into our estimator, we observe enhanced performance on Omniglot and Caltech101 datasets. Furthermore, in Figure 4, we visualize the latent representations of the VAE methods. DVAE and CR-VAE when combined with our framework result in well-separated clusters, indicating that application of our framework allows VAEs to learn a meaningful latent representation. A quantitative evaluation of the latent representations is presented in the Appendix by analyzing the performance of VAE methods on a downstream classification task.

### Effects of the Maximum Width \(O\)

We further investigate the influence of the width \(O\) (i.e., the maximum number of neurons per layer). Figure 5 shows the evolution of the medians of the number of active hidden layers (i.e., the hidden layers with activated neurons) as \(O\) increases. When \(O 100\), we tend to have shallower

Figure 4: Visualization of the latent representation via t-SNE embeddings for the MNIST dataset. The embeddings are colored based on class labels. DVAE and CR-VAE combined with our framework result in better representations.

Figure 5: Influence of the maximum number of neurons per layer \(O\) on our method. When \(O\) is small (e.g., \(O 100\)), we tend to have shallower encoding/decoding networks. As \(O\) becomes reasonably large (e.g., \(O 100\)), it tends not to have significant influence on the depth. Meanwhile, the percentages of activated neurons in the truncation become stable.

encoding/decoding networks with a lower percentage of activated neurons in the truncation. However, when \(O\) is reasonably large as \(O 100\), it has no influence on the network depth. In particular, the percentage of activated neurons in the whole truncation also remains relatively stable. This suggests that our method can automatically balance network depth and width to maintain the best performance.

### Application to VAE Backbone Networks

We demonstrate adaVAE's efficacy by applying it to VAEs with different types of encoding/decoding backbone networks. To infer the number of convolutional layers in a convolutional VAE (cVAE) using the beta process, we readily mask the convolution channels in layer \(l\) with \(_{ l}\). Figure 6 (a) and (b) show that by adapting network structures to the data we improve the overall performance of cVAE and \(\)-cVAE . The adaptive backbone networks effectively prevent overfitting for deep structure settings (i.e., \(T/L 15\)).

Variational graph autoencoder (VGAE)  encodes graph nodes to latent embeddings with graph convolutional (GC) layers and re-creates the input graph from the embeddings by predicting the existence of an edge between the nodes. We combine our framework with VGAE by elementwisely multiplying the GC channels (i.e., the feature vectors) of layer \(l\) with \(_{ l}\) in both its encoding/decoding networks. We compare the hybrid method's performance with graph autoencoders (GAEs)  and vanilla VGAEs on two benchmark graph-structured datasets: Cora  and Citeseer . The AUC scores of link prediction over varying numbers of GC layer settings are shown in Figure 6 (c) and (d). Our framework enables the VGAE to maintain its best performance for all the network depth settings by automatically adapting its structures to the data, whereas the performance of GAE and vanilla VGAE drops with the increase of the layer numbers (e.g., \(T/L=\{6,8,10\}\)).

### Application to VAE Variants

We assess the performance of our inference framework by leveraging it to adapt the network structures of VAE variants to data. Specifically, for \(\)-VAE, we apply layer-wise binary masks \(_{ l}\) to the convolutional channels and infer the layer numbers using the beta process. In the case of LadderVAE (LVAE) , we adjust its depth by applying layer-wise binary masks to its deterministic layers in the bottom-up dependency structures and add skip connections between the stochastic layers. For SkipVAE , we model its depth by employing layer-wise binary masks and skip connections in both its encoding/decoding networks. The expressive network structures of NVAE  consists of multiple blocks of convolutional layers. We apply our

   Methods & -LL \(\) & MI \(\) & KL \(\) \\  \(\)-cVAE (\(=2\))  & 106.50\(\)0.12 & 8.46\(\)0.01 & 18.01\(\)0.21 \\ Ours + \(\)-cVAE (\(=2\)) & **102.45\(\)0.07** & **8.47\(\)0.01** & **20.65\(\)0.02** \\  LVAE  & 136.50\(\)1.50 & 8.43\(\)0.02 & 20.05\(\)0.21 \\ Ours+LVAE & **121.48\(\)0.67** & **8.50\(\)0.01** & **21.00\(\)0.47** \\  SkipVAE  & 112.68\(\)0.80 & **8.50\(\)0.01** & 22.82\(\)0.79 \\ Ours+SkipVAE & **108.00\(\)0.04** & **8.50\(\)0.01** & **28.26\(\)0.10** \\  NVAE  & **98.83\(\)0.17** & **8.51\(\)0.01** & 34.97\(\)0.13 \\ Ours+NVAE & 99.10\(\)0.20 & **8.51\(\)0.01** & **37.85\(\)0.50** \\   

Table 3: Performance comparison of VAE variants with and without our inference framework in terms of \(-LL\), MI, and KL divergence.

Figure 6: Our framework’s performance on different VAE backbone networks. (a) and (b) show VAE and \(\)-VAE  with convolutional layers on MNIST and Omniglot datasets. (c) and (d) show VGAEs  with graph convolutional layers on Cora and Citeseer datasets.

framework to infer the number of blocks for a light NVAE version without top-down dependency between the blocks. The detailed settings of these VAE variants and additional results can be found in Appendix. The results in Table 3 on FashionMNIST  demonstrate that by inferring the encoding/decoding network structures we significantly improve the density estimation performance of the VAE variants. Our framework also boosts their ability to mitigate posterior collapse as indicated by MI and KL divergence.

## 6 Conclusion

We present a Bayesian inference framework and a scalable estimator that automatically adapts VAE network structures to data. The experiments demonstrate its effectiveness in preventing overfitting for both shallow and deep structure settings. Moreover, AdaVAE exhibits promising applications across various types of VAE backbone networks and VAE variants, including those with hierarchical structures such as LVAE. Notably, AdaVAE enhances the generative performance of these models without requiring pre-determined network structures prior to training. Our future work entails relaxing the constraint of truncation levels by incorporating the Russian roulette method  and scaling up the inference for large images.

## 7 Acknowledgements

This material is based upon work supported by the National Science Foundation under NSF Award No.2045804 and Award No.1850492. We are thankful to Kishan KC for helpful discussion. We acknowledge Research Computing at the Rochester Institute of Technology  for providing computational resources.