# Controlling Type I Error

Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans

Kyowoon Lee

UNIST

leekwoon@unist.ac.kr &Seongun Kim

KAIST

seongun@kaist.ac.kr &Jaesik Choi

KAIST, INEEJI

jaesik.choi@kaist.ac.kr

Equal Contribution

Codes are available at https://github.com/leekwoon/rgg.

###### Abstract

Diffusion-based planning has shown promising results in long-horizon, sparse-reward tasks by training trajectory diffusion models and conditioning the sampled trajectories using auxiliary guidance functions. However, due to their nature as generative models, diffusion models are not guaranteed to generate feasible plans, resulting in failed execution and precluding planners from being useful in safety-critical applications. In this work, we propose a novel approach to refine unreliable plans generated by diffusion models by providing refining guidance to error-prone plans. To this end, we suggest a new metric named _restoration gap_ for evaluating the quality of individual plans generated by the diffusion model. A restoration gap is estimated by a _gap predictor_ which produces _restoration gap guidance_ to refine a diffusion planner. We additionally present an attribution map regularizer to prevent adversarial refining guidance that could be generated from the sub-optimal gap predictor, which enables further refinement of infeasible plans. We demonstrate the effectiveness of our approach on three different benchmarks in offline control settings that require long-horizon planning. We also illustrate that our approach presents explainability by presenting the attribution maps of the gap predictor and highlighting error-prone transitions, allowing for a deeper understanding of the generated plans.

## 1 Introduction

Planning plays a crucial and efficient role in tackling decision-making problems when the dynamics are known, including board games and simulated robot control (Tassa et al., 2012; Silver et al., 2016, 2017; Lee et al., 2018). To plan for more general tasks with unknown dynamics, the agent needs to learn the dynamics model from experience. This approach is appealing since the dynamics model is independent of rewards, enabling it to adapt to new tasks in the same environment, while also taking advantage of the latest advancements from deep supervised learning to employ high-capacity models.

The most widely used techniques for learning dynamics models include autoregressive forward models (Deisenroth and Rasmussen, 2011; Hafner et al., 2019; Kaiser et al., 2020), which make predictions based on future time progression. Although an ideal forward model would provide significant benefits, there is a key challenge that the accuracy of the model directly affects the quality of the plan. As model inaccuracies accumulate over time (Ross and Bagnell, 2012; Talvitie, 2014; Luo et al., 2019; Janner et al., 2019; Voelcker et al., 2022), long-term planning using imprecise models might yield sub-optimal performances compared to those achievable through model-free techniques. Building upon the latest progress in generative models, recent studies have shown promise in transforming reinforcement learning (RL) problems into conditional sequence modeling, through the modeling of the joint distribution of sequences involving states, actions, and rewards(Lambert et al., 2021; Chen et al., 2021; Janner et al., 2021, 2022). For instance, Diffuser (Janner et al., 2022) introduces an effective framework for generating trajectories using a diffusion model with flexible constraints on the resulting trajectories through reward guidance in the sampling phase. Although these approaches have achieved notable performance on long-horizon tasks, they still face challenges in generating outputs with unreliable trajectories, referred to as artifacts, resulting in limited performance and unsuitability for deployment in safety-critical applications.

This paper presents an orthogonal approach aimed at enhancing the plan quality of the diffusion model. We first propose a novel metric called _restoration gap_ that can automatically detect whether generated plans are feasible or not. We theoretically analyze that it could detect artifacts with bounded error probabilities under regularity conditions. The restoration gap directly evaluates the quality of generated plans by measuring their restorability through diffusion models in which plans are exposed to a certain degree of noise, as illustrated in Figure 1. A restoration gap is estimated by a function approximator which we name a _gap predictor_. The gap predictor provides an additional level of flexibility to the diffusion model, and we demonstrate its ability to efficiently improve low-quality plans by guiding the reduction of the estimated restoration gap through a process, which we call Restoration Gap Guidance (RGG). Furthermore, we propose a regularizer that prevents adversarial restoration gap guidance by utilizing an attribution map of the gap predictor. It effectively mitigates the risk of the plan being directed towards an unreliable plan, enabling further improvement in the planning performance.

The main contributions of this paper are summarized as follows: **(1)** We provide a novel metric to assess the quality of individual plans generated by the diffusion model with theoretical justification. **(2)** We propose a new generative process, Restoration Gap Guidance (RGG) which utilizes a gap predictor that estimates the restoration gap. **(3)** We show the effectiveness of our approach across three different benchmarks in offline control settings.

## 2 Background

### Planning with Diffusion Probabilistic Models

We consider the reinforcement learning problem which aims to maximize the expected discounted sum of rewards \(_{}[_{t=0}^{T}^{t}r(_{t},_{t})]\) where \(\) is a policy that defines a distribution over actions

Figure 1: Illustration of two plans with low/high restoration gaps with a specified start ��⃝�� and goal �⃝��. For each input plan, we first perturb it using Gaussian noise. We then remove the noise from the perturbed plan by simulating the reverse SDE which progressively transforms the perturbed plan into the initial plan by utilizing the score function (Section 2.2). The restoration gap is then computed as the expected \(L_{2}\) distance between the input plan and the plan restored from noise corruption (Section 3). The top example exhibits a smaller restoration gap because of its successful restoration close to the original plan, while the bottom example has a larger restoration gap due to its poor restoration performance. Plans restored from various noise corruptions are differentiated by distinct colors.

\(_{t}\), \(_{t}\) represents the states that undergo transition according to unknown discrete-time dynamics \(_{t+1}=f(_{t},_{t})\), \(r:\) is a reward function, and \((0,1]\) is the discount factor. Trajectory optimization solves this problem by finding the sequence of actions \(_{0:T}^{*}\) that maximizes the expected discounted sum of rewards over planning horizon \(T\):

\[_{0:T}^{*}=*{arg\,max}_{_{0:T}}() =*{arg\,max}_{_{0:T}}_{t=0}^{T}^{t}r(_{t}, _{t}),\] (1)

where \(=(_{0},_{0},_{1},_{1},...,_{t},_{ t})\) represents a trajectory and \(()\) denotes an objective value of that trajectory. This trajectory can be viewed as a particular form of two-dimensional sequence data:

\[=_{0}&_{1}&_{T}\\ _{0}&_{1}&&_{T}.\] (2)

Diffuser (Janner et al., 2022) is a trajectory planning model, which models a trajectory distribution by employing diffusion probabilistic models (Sohl-Dickstein et al., 2015; Ho et al., 2020):

\[p_{}(^{0})= p(^{N})_{i=1}^{N}p_{}(^{i-1}|^{i})\,^{1:N}\] (3)

where \(p(^{N})\) is a standard Gaussian prior, \(^{0}\) is a noiseless trajectory, and \(p_{}(^{i-1}|^{i})\) is a denoising process which is a reverse of a forward process \(q(^{i}|^{i-1})\) that gradually deteriorates the data structure by introducing noise. The denoising process is often parameterized as Gaussian with fixed timestep-dependent covariances: \(p_{}(^{i-1}|^{i})=(^{i-1}|_{}(^{i},i),^{i})\). Diffuser recasts the trajectory optimization problem as a conditional sampling with the conditional diffusion process under smoothness condition on \(p(_{1:T}=1|)\)(Sohl-Dickstein et al., 2015):

\[_{}()=p(|_{1:T}=1) p()p(_{1:T}=1|), p_{}(^{i-1}|^{i},_{1:T})(^{i-1};+g,)\] (4)

where \(\), \(\) are the parameters of the denoising process \(p_{}(^{i-1}|^{i})\), \(_{t}\) is the optimality of timestep \(t\) of trajectory with \(p(_{t}=1)=(^{t}r(_{t},_{t}))\) and

\[g=_{} p(_{1:T}|)|_{=} =_{t=0}^{T}^{t}_{_{t},_{t}}r(_{t},_{t} )|_{(_{t},_{t})=_{t}}=().\] (5)

Therefore, a separate model \(_{}\) can be trained to predict the cumulative rewards of trajectory samples \(^{i}\). By utilizing the gradients of \(_{}\), trajectories with high cumulative rewards can be generated.

As part of the training procedure, Diffuser trains an \(\)-model to predict the source noise instead of training \(_{}\) as it turns out that learning \(_{}\) enables the use of a simplified objective, where \(_{}\) is easily recovered in a closed form (Ho et al., 2020):

\[():=_{i,,^{0}}[\|-_{}(^{i},i)\|^{2}],\] (6)

where \(i\{0,1,...,N\}\) is the diffusion timestep, \((,)\) is the target noise, and \(^{i}\) is the trajectory corrupted by the noise \(\) from the noiseless trajectory \(^{0}\).

### Generalizing Diffusion Probabilistic Models as a Stochastic Differential Equation (SDE)

The forward process in diffusion probabilistic models perturbs data structure by gradually adding Gaussian noises. Under an infinite number of noise scales, this forward process over continuous time can be represented as a stochastic differential equation (SDE) (Song et al., 2021):

\[=(,t)\,t+g(t)\,,\] (7)

where \(t(0,1]\) is a continuous time variable for indexing diffusion timestep, \((,t)\) is the drift coefficient, \(g(t)\) is the diffusion coefficient, and \(\) is the standard Wiener process. Similarly, the denoising process can be defined by the following reverse-time SDE:

\[=[(,t)-g(t)^{2}_{}(,t)]\,t+g(t)\,},\] (8)where \(}\) is the infinitesimal noise in the reverse time direction and \(_{}(,t)\) is the learned score network which estimates the data score \(_{} p_{t}()\). This score network can be replaced by the \(\)-model:

\[_{}(,t)_{} q()= _{^{0}}[_{} q(|^{0})]= _{^{0}}[-_{}(,t)}{C_ {t}}]=-_{}(,t)}{C_{t}},\] (9)

where \(C_{t}\) is a constant determined by the chosen perturbation strategies.

The solution of a forward SDE is a time-varying random variable \(^{t}\). Using the reparameterization trick (Kingma and Welling, 2014), it is achieved by sampling a random noise \(\) from a standard Gaussian distribution which is scaled by the target standard deviation \(_{t}\) and shifted by the target mean:

\[^{t}=_{t}^{0}+_{t},(,),\] (10)

where \(_{t}:\) denotes a scalar function indicating the magnitude of the noiseless data \(^{0}\), and \(_{t}:[0,)\) denotes a scalar function that determines the size of the noise \(\). Depending on perturbation strategies for \(_{t}\) and \(_{t}\), two types of SDEs are commonly considered: the Variance Exploding SDE (VE-SDE) has \(_{t}=1\) for all \(t\); whereas the Variance Preserving (VP) SDE satisfies \(_{t}^{2}+_{t}^{2}=1\) for all \(t\). Both VE and VP SDE change the data distribution to random Gaussian noise as \(t\) moves from \(0\) to \(1\). In this work, we describe diffusion probabilistic models within the continuous-time framework using VE-SDE to simplify notation, as VE/VP SDEs are mathematically equivalent under scale translations (Song et al., 2021).

For VE SDE, the forward process and denoising process are defined by the following SDEs:

\[\,\,= [_{t}^{2}]}{t}}\,\] (11) \[\,\,=[-[_{t}^{2}]}{t}_{}(,t)]t+[_{t}^{2}]}{t}}\,}.\] (12)

## 3 Restoration Gap

To assess the quality of plans generated by diffusion probabilistic models, we propose a novel metric named _restoration gap_. It aims to automatically detect infeasible plans that violate system constraints. We hypothesize that for feasible plans, even if a certain amount of noise perturbs them, they can be closely restored to their initial plans by diffusion models. It is attributed to the property of temporal compositionality in diffusion planners (Janner et al., 2022) that encourages them to compose feasible trajectories by stitching together any feasible plan subsequences. However, for infeasible plans that obviously fall out of the training distribution as they violate physical constraints as shown in Figure 5, restoring them to a state near their original conditions is challenging. Based on this intuition, we define the restoration gap of the generated plan \(\) as follows:

\[_{}()=+_{} _{},_{}(,)\] (13) \[_{,}()=+_{ }[-[_{t}^{2}]}{t}_{ }(,t)]t+[_{t}^{2}]} {t}}\,}\] (14) \[_{,}()=_{ {}_{}}[\|-_{,}(_{}())\|_{2}],\] (15)

where \((0,1]\) indicates the magnitude of applied perturbation. The restoration gap measures the expected \(L_{2}\) distance between the generated plan and the plan restored from noise corruption, which is estimated by the Monte Carlo approximation.

Figure 2 provides empirical evidence supporting our hypothesis. In order to analyze the effectiveness of the restoration gap, we define artifact plans generated by Diffuser (Janner et al., 2022) that involve transitions of passing through walls for which it is impossible for the agent to follow. We compare the distribution of the restoration gap for both groups, normal plans and artifact plans1. The histogram of the restoration gap for normal and artifact plans demonstrates that infeasible artifact plans have larger

restoration gap values compared to normal plans. Therefore, the detection of infeasible artifact plans can be automated by incorporating a statistical test that utilizes the restoration gap and thresholding with a threshold value of \(b>0\):

\[_{,}()>b.\] (16)

To bound the probability of making errors by choosing the specific threshold \(b\), we provide Proposition 1. Let \(_{0}\) represent the null hypothesis which assumes that the trajectory \(\) belongs to the normal set \(_{}\), and let \(_{1}\) represent the alternative hypothesis which assumes that the trajectory \(\) belongs to the artifact set \(_{}\). The following proposition suggests how to choose the threshold \(b\) in order to bound the error probabilities.

**Proposition 1**.: _Given \(t\) and a positive constant \(C,\), assume that \(\|_{}(,t)\|_{2}^{2} C^{2}\) for all \(_{}^{d}\), and \(\|_{}(,t)\|_{2}^{2}(C+)^{2}\) for all \(_{}^{d}\). If_

\[+2-2}}{ _{}},\] (17)

_then setting_

\[b_{}(C_{}++-2})\] (18)

_guarantees both type I and type II errors at most \(2\)._

Proof Sketch.: We begin by deriving thresholds \(b_{I}\) and \(b_{II}\) to control type I and type II errors at most \(\), respectively. This is done by decomposing the restoration gap into the outcomes of the score and Gaussian noise. To ensure the control of both type I and type II errors, we examine the condition \(b_{I} b_{II}\) and obtain the conclusion. For the complete proof, see Appendix A. 

According to Proposition 1, to achieve low error probabilities for both type I (false positives, where normal trajectories are incorrectly classified as artifacts) and type II (false negatives, where artifact trajectories are wrongly identified as normal) errors, it is essential to have a large enough \(_{}\) to properly satisfy the condition, which implies having a large enough \(\). In practice, we find that setting \(=0.9\) works well.

Figure 2: The first and second rows show examples of artifact and normal plans, respectively, generated by Diffuser (Janner et al., 2022) in the Maze2D-Large environment, including a predetermined start and goal. The third row presents the density of realism score (Kynkaanniemi et al., 2019), rarity score (Han et al., 2023), and restoration gap to illustrate the differences in distribution between artifacts and normal plans. Detailed explanation of other metrics is described in Appendix C.2.

## 4 Refining Diffusion Planner

### Restoration Gap Guidance

Although Diffuser (Janner et al., 2022) has demonstrated competitive performance against previous non-diffusion-based planning methods by utilizing gradients of return \(_{}\) to guide trajectories during the denoising process:

\[=[(,t)-g(t)^{2}_{} (,t)+_{}()]\,t+g (t)\,},\] (19)

it entirely relies on the ability of a generative model and assumes a perfect data score estimation. For plans with inaccurately estimated scores, the diffusion models could generate unreliable plans that are infeasible to execute and lead to limited performance. To address this, it is essential to construct an adjusted score to refine the generative process of the diffusion planner. Therefore, we estimate the restoration gap by training a gap predictor \(_{}\) on synthetic diffused data generated through the diffusion process, taking full advantage of its superior generation ability with conditional guidance from gradients of return. Parameters of the gap predictor \(\) are optimized by minimizing the following objective:

\[():=_{t,^{0}}[\|_{t, }(^{t})-_{}(^{t},t)\|^{2}],\] (20)

where \(t(0,1]\) denotes a continuous time variable for indexing the diffusion timestep, and \(^{t}\) is the diffused trajectory resulting from \(^{0}\) at diffusion timestep \(t\). With this gap predictor, we define the Restoration Gap Guidance (RGG) as follows:

\[=[(,t)-g(t)^{2}_{ }(,t)+_{}()- _{}(,t)]\,t+g(t)\, },\] (21)

where \(\) is a positive coefficient that scales the overall guidance and \(\) is a positive coefficient that can be adjusted to enforce a small restoration gap for the generated trajectory.

### Attribution Map Regularization

Although guiding the diffusion planner to minimize the restoration gap effectively refines low-quality plans (more details in Section 5), this refining guidance could push the plan in an undesirable direction due to the estimation error of the gap predictor during the denoising process. As a result of this estimation error, guiding plans with the sub-optimal gap predictor may result in _model exploitation_(Kurutach et al., 2018; Janner et al., 2019; Rajeswaran et al., 2020), yielding sub-optimal results.

To mitigate the issue of adversarial guidance, we present a regularization method that prevents the gap predictor from directing plans in the wrong direction. Inspired by the prior studies which improve the model performance by utilizing attribution maps (Nagisetty et al., 2020; Bertoin et al., 2022), we measure a total variation of the attribution map \(M\) obtained from any input attribution methods \(M=E(_{}(,t))\). Each element of the attribution map indicates the extent to which the final prediction is influenced by the corresponding input feature. The rationale of employing the total variation of \(M\) lies in the hypothesis that transitions with excessively high attribution scores are more likely to be outliers. This is because a sequence of transitions within a planned trajectory, rather than a single one, causes a plan to have a high restoration gap. By adding this attribution map regularization, Equation 21 becomes:

\[=[(,t)-g(t)^{2} _{}(,t)+_{}()-_{}(,t)-\| M\| )]\,t+g(t)\,},\] (22)

where \(\) is a control parameter given by a positive constant, encouraging the attribution map to have a simple, organized structure while preventing the occurrence of adversarial artifacts. We refer to this modification as RGG+.

Figure 3: Planning performance of RGG+ on Maze2D-Large single-task with varying \(\) values.

[MISSING_PAGE_FAIL:7]

ronments where the complexity of the obstacle maps is higher than in U-Maze or Medium layouts, leading to a higher occurrence of infeasible plans. RGG+ performs on par with or better than RGG. In contrast, model-free algorithms fail to reliably achieve the goal, as Maze2D environments require hundreds of steps to arrive at the goal location.

Locomotion ExperimentsGym-MuJoCo locomotion tasks (Fu et al., 2020) are standard benchmarks in evaluating algorithms on heterogeneous data with varying quality. We compare our methods with the model-free algorithms CQL (Kumar et al., 2020) and IQL (Kostrikov et al., 2022); model-based algorithms MOPO (Yu et al., 2020), MOReL (Kidambi et al., 2020), and MBOP (Argenson and Dulac-Arnold, 2021); sequence modeling approach Decision Transformer (DT) (Chen et al., 2021), Trajectory Transformer (TT) (Janner et al., 2021) and Diffuser (Janner et al., 2022); and pure imitation-based approach behavior-cloning (BC). As indicated in Table 1, our approach of refining Diffuser with RGG either matches or surpasses most of the offline RL baselines when considering the average score across various tasks. Additionally, it significantly enhances the performance of Diffuser, particularly in the "Medium" dataset. We attribute this improvement to the sub-optimal and exploratory nature of the policy that was used to generate the "Medium" dataset, which results in a challenging data distribution to learn the diffusion planner. Consequently, RGG clearly contributes to the enhancement of planning performance. However, RGG+ only brings about a marginal improvement over RGG. This might be because we adopt the strategy of (Janner et al., 2022), using a closed-loop controller and a shorter planning horizon in locomotion environments compared to Maze2D environments, thereby simplifying the learning process of the gap predictor.

Block Stacking ExperimentsThe block stacking task suite with a Kuka iiwa robotic arm is a benchmark to evaluate the model performance for a large state space (Janner et al., 2022) where the offline demonstration data is achieved by PDDLStream (Garrett et al., 2020). It involves two tasks: an unconditional stacking task whose goal is to maximize the height of a block tower, and a conditional stacking task whose goal is to stack towers of blocks subject to a specified order of blocks. We compare our methods with model-free offline reinforcement learning algorithms BCQ (Fujimoto et al., 2019) and CQL (Kumar et al., 2020), and Diffuser (Janner et al., 2022). We present quantitative results in Table 3, where a score of 100 corresponds to the successful completion of the task. The results demonstrate the superior performance of RGG over all baselines, with RGG+ further enhancing this planning performance.

### Injecting Explainability to Diffusion Planners

The explainability of decision-making models is particularly important in control domains as they could potentially harm physical objects including humans (Kim and Choi, 2021; Lee et al., 2023; Beechey et al., 2023; Kim et al., 2023; Kenny et al., 2023). Training the gap predictor enables the diffusion planner to have explainability. Diffusion planners often generate trajectories with unreliable transitions resulting in execution failures. Attribution maps from the gap predictor highlight such unreliable transitions by identifying the extent to which each transition contributes to the decision of the gap predictor. Specifically, in Maze2D, the attribution maps emphasize the transitions involving wall-crossing or abrupt directional changes, as illustrated in Figure 5. In the unconditional block stacking task where the robot destroys the tower while stacking the last block, the tower-breaking transitions are highlighted. On the other hand, for successful trajectories on the second and third attribution maps, the attribution maps do not emphasize picking or stacking behaviors. Similarly, in the conditional block stacking task where the robot fails to stack the block, they spotlight the transitions of stacking behaviors.

### Additional Experiments

To study the benefit of regularization on harder tasks, characterized by a larger trajectory space and a smaller fraction of the space observed in training, we explore \(\) values \([0.0,0.5,1.0,3.0,5.0]\) while increasing the planning budget as illustrated in Figure 3. As the planning budget increases, \(=0\) generates adversarial plans, resulting in decreased performance. In contrast, RGG+ demonstrates

  
**Environment** & **BCQ** & **CQL** & **Diffuser** & **RGG** & **RGG+** \\  Unconditional Stacking & 0.0 & 24.4 & 53.3 \(\) 2.4 & 63.3 \(\) 2.7 & 65.3 \(\) 2.0 \\ Conditional Stacking & 0.0 & 0.4 & 44.3 \(\) 3.2 & 53.0 \(\) 3.3 & 56.7 \(\) 3.1 \\ 
**Average** & 0.0 & 8.1 & 48.8 & **58.2** & **61.0** \\   

Table 3: The performance of RGG, RGG+, and various prior methods evaluated over 100 planning seeds. A score of 100 is desired, while a random approach would receive a score of 0.

effectiveness across a wide range of \(\) values, with \(>0\) consistently outperforming \(=0\) (i.e., better than no regularization). Further investigation into the attribution method, perturbation magnitude, and comparison with guidance approaches, including metrics such as the rarity score, the negative realism score, and the discriminator, as well as the visualization of low and high restoration gap plans, can be found in Appendix B.

## 6 Related Work

Metrics for Evaluating Generative ModelInception score (IS) (Salimans et al., 2016) and Frechet inception distance (FID) (Heusel et al., 2017) are commonly used as standard evaluation metrics for generative models, assessing the quality of generated samples by comparing the discrepancy between real and generated samples in the feature space. However, these metrics do not distinguish between fidelity and diversity aspects of generated samples. To address this issue, precision and recall variants (Saljadi et al., 2018; Kynkaanniemi et al., 2019) are introduced to separately evaluate these properties. Subsequently, density and coverage (Naeem et al., 2020) are proposed to overcome some of the drawbacks of precision and recall, such as vulnerability to outliers and computational inefficiency. While these metrics are helpful for evaluating the quality of a set of generated samples, they are not suitable for ranking individual samples. In contrast, realism score (Kynkaanniemi et al., 2019) and rarity score (Han et al., 2023) offer a continuous extension of improved precision and recall, enabling the evaluation of individually generated sample quality. Despite their usefulness,

Figure 5: Attribution maps for trajectories generated by diffusion planner highlight transitions that have a substantial contribution to the estimation of a high restoration gap by the gap predictor, indicated in red.

these methods come with limitations as they rely on real samples for precise real manifold estimation, whereas our restoration gap does not have such a constraint.

Diffusion Model in Reinforcement LearningDiffusion models have gained prominence as a notable class of generative models, characterizing the data generation process through iterative denoising procedure (Sohl-Dickstein et al., 2015; Ho et al., 2020). This denoising procedure can be viewed as a way to parameterize the gradients of the data distribution (Song and Ermon, 2019), linking diffusion models to score matching (Hyvarinen and Dayan, 2005) and energy-based models (EBMs) (LeCun et al., 2006; Du and Mordatch, 2019; Nijkamp et al., 2019; Grathwohl et al., 2020). Recently, diffusion models have been successfully applied to various control tasks (Janner et al., 2022; Urain et al., 2023; Ajay et al., 2023; Chi et al., 2023; Liang et al., 2023). In particular, Diffuser (Janner et al., 2022) employs an unconditional diffusion model to generate trajectories consisting of state-action pairs. The approach includes training a separate model that predicts the cumulative rewards of noisy trajectory samples, which then guides the reverse diffusion process towards high-return trajectory samples in the inference phase, analogous to classifier-guided sampling (Dhariwal and Nichol, 2021). Building upon this, Decision Diffuser (Ajay et al., 2023) extends the capabilities of Diffuser by adopting a conditional diffusion model with reward or constraint guidance to effectively satisfy constraints, compose skills, and maximize return. Meanwhile, AdapDiffuser (Liang et al., 2023) enhances generalization ability of the diffusion model to unseen tasks by selectively fine-tuning it with high-quality data, derived through the use of hand-designed reward functions and an inverse dynamics model. In contrast, in this work, we focus on evaluating the quality of individually generated samples and explore ways to enhance planning performance by utilizing guidance derived from these evaluations.

Restoring Artifacts in Generative ModelsRecently, several studies have concentrated on investigating the artifacts in Generative Adversarial Networks (GAN) model architectures for image generation tasks. GAN Dissection (Bau et al., 2019) explores the internal mechanisms of GANs, focusing on the identification and removal of units that contribute to artifact production, leading to more realistic outputs. In a subsequent study, an external classifier is trained to identify regions of low visual fidelity in individual generations and to detect internal units associated with those regions (Tousi et al., 2021). Alternatively, artifact correction through latent code manipulation based on a binary linear classifier is proposed (Shen et al., 2020). Although these methods can assess the fidelity of individual samples, they still necessitate additional supervision, such as human annotation. To address this limitation, subsequent works explore unsupervised approaches for detecting and correcting artifact generations by examining local activation (Jeong et al., 2022) and activation frequency (Choi et al., 2022). In contrast, our work primarily focuses on refining the generative process of diffusion probabilistic models to restore low-quality plans.

## 7 Conclusion

We have presented a novel refining method that fixes infeasible transitions within the trajectory generated by the diffusion planner. This refining process is guided by a proposed metric, restoration gap, which quantifies the restorability of a given plan. Under specific regularity conditions, we prove that the restoration gap effectively identifies unreliable plans while ensuring a low error probability for both type I and type II errors. The experimental results, which include enhancement in quantitative planning performance and visualization of qualitative attribution maps, highlight the importance of the refinement method of the diffusion planner.

LimitationsWhile the restoration gap guidance effectively enhances the feasibility of plans and consistently improves the planning performance of diffusion models, our method is limited in situations where an offline dataset is provided. Training the diffusion model often requires transition data that uniformly covers the state-action space, the collection of which is a nontrivial and time-consuming task.

Future WorkOur analysis of the effectiveness of the restoration gap is currently confined to a relatively simple task, Maze2D (see Figure 2), where we explicitly define normal and artifact plans. The choice of Maze2D is motivated by its suitability for identifying violations of prior knowledge, such as feasible plans not passing through walls. However, as future work, it would be worthwhile to explore the efficacy of restoration gap in more complex tasks, such as the block stacking task.