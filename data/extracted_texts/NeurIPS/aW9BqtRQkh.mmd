# Language Models Can Improve Event Prediction

by Few-Shot Abductive Reasoning

 Xiaoming Shi\({}^{1}\)  Siqiao Xue\({}^{1}\)  Kangrui Wang\({}^{3}\)  Fan Zhou\({}^{1}\)  James Y. Zhang\({}^{1}\)

Jun Zhou\({}^{1}\)  Chenhao Tan\({}^{2}\)  Hongyuan Mei\({}^{3}\)

\({}^{1}\)Ant Group \({}^{2}\)UChicago \({}^{3}\)TTIC

{peter.sxm,sigiao.xsq,hanlian.zf,james.z,jun.zhoujun}@antgroup.com

chenhao@uchicago.edu {kangrui,hongyuan}@ttic.edu

###### Abstract

Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction performance of event sequence models. We design LAMP, a framework that integrates a large language model in event prediction. Particularly, the language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on several challenging real-world datasets, we demonstrate that our framework--thanks to the reasoning capabilities of large language models--could significantly outperform the state-of-the-art event sequence models.

## 1 Introduction

Prompting large language models (LLMs) such as GPT-3.5 has recently become a standard approach to perform text-based reasoning tasks. In this paper, we investigate their capabilities in reasoning about real-world events and improving event prediction. Particularly, we focus on the problem of modeling sequences of time-stamped events and predicting future events given the past. For example, in the healthcare domain, we would like to model patients' sequences of time-stamped hospital visits and predict their future symptoms given their past diagnosis and treatments. It has been a long-standing and important problem in machine learning. Large language models are potentially useful for advancing solutions to this problem because event sequences are often accompanied with rich text information which large language models excel at handling. For example,

* _Healthcare._ Each hospital visit will have a doctor note summarizing this visit, including the department that the patient visits, the clinical measurements and treatments, and any future medical plans. By reading such textual information, a large language model may be elicited to recall the medical knowledge that it has read during pretraining and then reason about the future hospital visits such as what symptoms or treatments that the patient may have.
* _Political._ Each political event may generate a series of news articles describing the political agents involved in it and discussing its possible influences. A language model reading these articles may recall its knowledge--which is acquired from pretraining--about these agents, their relations, and fundamental principles in politics such that it could reason about future political events.
* Similar scenarios arise in _commercial_, _dialogue_, _finance_, etc.

In this paper, we propose LAMP, a framework that integrates a large language model in event prediction. The overview of our framework is illustrated in Figure 1. Given a history of previousevents, we use a pretrained event sequence model to propose predictions on the future events, which are then examined with the assistance of an LLM. The LLM learns to perform abductive reasoning: it is instructed by a few expert-annotated demonstrations, and generates possible causes that may explain the possible occurrence of each proposal. Each generated cause serves as a query to search for similar or relevant events that have actually happened. Then another neural model learns to embed these retrievals and examine whether they could really justify the corresponding proposal.

We are the first--to the best of our knowledge--to integrate large language models into event sequence modeling. Our modeling and prediction framework is general: it can incorporate all kinds of event sequence models and large language models. We experimented with a range of model choices and demonstrate that large language models could indeed help improve the prediction performance of event sequence models. On several challenging real-world datasets, our framework significantly outperforms the current state-of-the-art event sequence models.

## 2 Problem Formulation and Technical Background

Now we give a formal introduction to our problem setting and review the background knowledge.

Event sequence modeling.The problem is to model event sequences \((t_{1},k_{1}),(t_{2},k_{2}),\), where \(0<t_{1}<t_{2}<\) are times of occurrence and each \(k_{i}\) is a discrete event type. The goal is to predict the next event for a given history of events \(_{i}=(t_{1},k_{1}),,(t_{i-1},k_{i-1})\). Precisely, it consists of two subtasks: the first is to predict the time \(t_{i}\) of the next event; the second is to predict the type \(k_{i}\) of the next event with the knowledge of its time \(t_{i}\).

The standard approach is to build a probabilistic model over the sequences. Such models typically define an intensity function \(_{k}\): the intensity value \(_{k}(t)\) is the instantaneous rate that an event of type \(k\) occurs at time \(t\). Given the function \(_{k}\), one could obtain the minimum Bayes risk (MBR) prediction of the next event given the history. Particularly, the MBR time prediction \(_{i}\) is

\[_{i}=_{t_{i-1}}^{}t(t)(-_{t_{i-1}}^{t} (s)ds)dt(t)=_{k}_{k}(t)\] (1)

and it could be approximated by averaging samples given by the thinning algorithm (Lewis & Shedler, 1979; Liniger, 2009). The MBR type prediction \(_{i}\) given time \(t_{i}\) is

\[_{i}=*{argmax}_{k}_{k}(t_{i})\] (2)

The intensity function \(_{k}\) is typically learned by maximizing the log-likelihood of the model. For a time period \((0,T)\) that contains observed events \((t_{1},k_{1}),,(t_{I},k_{I})\), the log-likelihood is

\[_{i=1}^{I}_{k_{i}}(t_{i})-_{t=0}^{T}_{k} _{k}(t)dt\] (3)

Rich text information.In real-world data, each type \(k\) may be represented as a text-based identifier: in the example of Figure 1, each \(k\) is one of the possible interactions between the political entities (organizations and individuals) in the G20 countries, which can be represented with a structured name

Figure 1: An overview of our framework that leverages a large language model to reason about events. Firstly, an event sequence model proposes predictions: in this example, we predict the predicate of the structured event type given its time, subject, and object. Secondly, a language model suggests cause events, which will pattern-match against actual previous events and retrieve the most relevant. In the end, a neural model learns to assign high scores to the proposed predictions that are strongly supported by the retrieved evidence.

such as Tesla-cooperate-Australia. In addition, each event may have a text mark \(\) that contains additional information about the event: in Figure 1, each \(\) is a news headline about the event (e.g., "EV battery prices go up"). For notation simplicity, we will only mention the mark \(\) of an event when necessary. While reading such text information, a human may recall their relevant domain knowledge (e.g., influence of battery prices on Tesla and Australia) and increase their estimate on the probability that an event of Tesla-cooperate-Australia happens in the near future. An important way that humans learn such knowledge is reading text such as textbooks, research publications, and news articles. But event sequence models can not directly leverage this kind of information.

Large language models.Language models learn by reading text. Over the past years, large language models that have read nearly the entire internet have shown astonishing performance on many challenging tasks such as arithmetic reasoning and multi-turn dialogue (Wei et al., 2022b; OpenAI, 2023). So it seems tempting to pair a large language model with an event sequence model to improve its prediction performance: the language model has consumed a tremendous amount of information that the event model may not have seen but should be able to benefit from.

## 3 LAMP: Large Language Model in Event Prediction

Now we present our LAMP framework, in which an LLM is leveraged to enhance the prediction process of an event sequence model. As shown in Figure 1, LAMP has three key components:

* A base event sequence model. This model is pretrained and we use it to propose candidate predictions. Section 3.1 is the discussion of this phase.
* A large language model. Its duty in the framework is to perform _abductive reasoning_, a form of logical inference seeking the most plausible explanations for a given observation (Russell & Norvig, 2010). Particularly, the language model reads each proposed prediction and suggests possible cause events for it. Then we pattern-match each LLM-generated cause against the actual previous events in the history, and retrieve those which are most similar. Section 3.2 discusses this phase.
* A ranking model. The ranking model learns to examine each combination of the candidate prediction and its retrieved events--or, in other words, its _evidence_--and assign high scores to the candidates that are strongly supported by the retrieved evidence. Section 3.3 discusses this phase.

### Phase-I: Proposing Predictions

Given a history of previous events \(_{i}=(t_{1},k_{1}),,(t_{i-1},k_{i-1})\), the base event sequence model is used as a proposer to generate candidate predictions on the time and type of the next event.

For time prediction, we draw \(L\) i.i.d. samples \(^{(1)}_{i},,^{(L)}_{i}\) from the base model via the thinning algorithm. If we were to only use this base model but not our LLM-enhanced framework, the final MBR time prediction would be the average of the samples, i.e., \(_{i}=_{=1}^{L}^{()}_{i}\). However, the MBR prediction may not be accurate since the base model is imperfect. Therefore, our LAMP framework treats all the \(L+1\) samples--with \(^{(L+1)}_{i}\) denoting the MBR prediction--as candidates, and utilize the LLM and ranking model to score them in later phases. If any of the \(L\) draws is actually a better prediction than the MBR estimate, our framework has a chance to rank it higher.

As we'll show shortly in sections 3.2 and 3.3, the LLM and ranking model work on full events. So we find the most probable \(M\) full events \(\{(^{()}_{i},^{(,m)}_{i})\}_{m=1}^{M}\) for each time proposad \(^{()}_{i}\), where \(^{(,m)}_{i}\) is the event type that has the \(m\)-th highest intensity at time \(^{()}_{i}\).

For type prediction given the ground-truth time \(t_{i}\), we find \(M\) event types \(^{(1)}_{i},,^{(M)}_{i}\) where \(^{(m)}_{i}\) has the \(m\)-th highest intensity at time \(t_{i}\) under the base model. If we were to only use this base model for prediction, the MBR type prediction would be the event type \(^{(1)}_{i}\) with the highest model intensity. However, our full framework will use the LLM and ranking model to examine each of the top \(M\) full events \(\{(t_{i},^{(m)}_{i})\}_{m=1}^{M}\) at time \(t_{i}\) in order to make a more informed prediction.

In practice, the event types often have structures and we may be interested in predicting an attribute of the structured type. Figure 1 shows an example in which we are trying to predict the predicate of the structured event type given its time, subject, and object. In such cases, we just need to select as proposals the most probable \(M\) event types whose other attributes are the same as the known information (e.g., Tesla and Australia in Figure 1).

I want you to do the reasoning over social events. I given you an effect event and you give me four or five cause events. An effect event is an event that happens. A cause event is believed to be one of the causes that have triggerred the effect event to happen. Each event consists of a time, a type (that includes subject, predicate, object), and a news headline describing the event.

The predicates are restricted to the 20 options below.

1. MAKE STATEMENT

 : // Full list are in Appendix E.4.

 20. ENGAGE IN MASS VIOENCE

 Now I give you 10 examples. In each example, the first event is the effect and the next several events are the causes that happened earlier.

 : // Examples are in Listing 2. Now please generate possible causes for

 effect

 predicate: CONSULT

 time: 2022-07-05

 subject: CHINA PM

 object: YELLEN

 : // Examples are in Listing 2.

 : // Other causes are in Appendix E.4.

 : // Other examples in Appendix E.4.

 : // Other examples in Appendix E.

### Phase-III: Ranking Proposals

In this phase, our framework scores each proposed event \((t,k)\) based on the compatibility with its retrieved evidence \((t,k)\). Precisely, the score is defined to be

\[s_{}(t,k)}}{{=}}(c ((t,k),(t,k)))\] (4)

The function \(c\) takes as input the proposed event \((t,k)\) as well as its evidence \((t,k)\), and returns a scalar \(\). A high value of \(c\) means that this proposal is strongly supported by its retrieved evidence, and thus is more likely to be an actual event at time \(t\); a low value means that this proposal has no strong evidence even after we have tried our best to search from the history.

Given the most probable \(M\) events \(\{(t,k^{(m)})\}_{m=1}^{M}\) at time \(t\), we sum their \(s_{}\) scores to measure the overall belief of our framework in an event occurring at time \(t\). That is,

\[s_{}(t)}}{{=}}_{m=1}^{M }s_{}(t,k^{(m)})\] (5)

Intuitively, this score is high when any of the top-ranked event types at this time can be strongly supported by the retrieved evidence. Otherwise, even the top-ranked event types have no strong evidence in the history, which implies that the next event is unlikely to occur at this time.

For time prediction, each proposed time \(^{()}\) (\(=1,,L+1\)) has a score \(s_{}(^{()})\)--more precisely, \(_{m=1}^{M}s_{}(^{()},^{(,m)})\)--and our final prediction is the proposal with the highest score.

For type prediction given time \(t\), each proposed type \(^{(m)}\) (\(m=1,,M\)) has a score \(s_{}(t,^{(m)})\), and our framework takes the final prediction to be the type with the highest score.

Model architecture.Our function \(c\) is an energy function with a continuous-time Transformer architecture (Xue et al., 2022). It reads the proposal \((t,k)\) followed by its evidence events in the chronological order, and returns a compatibility score \(\).

We choose this architecture because its continuous-time attention is suitable for our setting. First, the attention mechanism may learn to disregard any of the retrieved events that are not really relevant and focus on those which can actually support the proposal. Second, its sophisticated handling of time may capture how the time of an evidence event may influence its relevance to the proposal (e.g., a recent evidence event may be more important than an ancient event).

Training.We train the ranking model by maximizing the objective \(J}}{{=}}J_{}+ J_{ }\) where \( 0\) is a hyperparameter. The first term \(J_{}\) is defined to be

\[J_{}}}{{=}}_{i=1}^{I }( s_{}(t_{i},k_{i})-(s_{}(t_{i},k_{ i})+_{m=1}^{M}s_{}(t_{i},k_{i}^{(m)})))\] (6)

where \((t_{1},k_{1}),,(t_{I},k_{I})\) is a sequence of events over the time interval \((0,T)\), and each \(k_{i}^{(m)}\) is the event type with the \(m\)-th highest intensity under the base model at time \(t_{i}\). By maximizing \(J_{}\), the function \(c\) learns to increase the scores of the events that have actually happened, but suppress the scores of the non-events at times \(t_{1},,t_{I}\). The second term \(J_{}\) is defined to be

\[J_{}}}{{=}}-_{n=1}^{N}  s_{}(t_{n})=-_{n=1}^{N}_{m=1}^{M}s_{}( t_{n},k_{n}^{(m)})\] (7)

where each \(t_{n}\) is a time point uniformly sampled from \((0,T)\) and each \(k_{n}^{(m)}\) is the event type with the \(m\)-th highest intensity at time \(t_{n}\). By maximizing \(J_{}\), the function \(c\) learns to decrease the scores of the non-events at times over \((0,T)\) other than \(t_{1},,t_{I}\).

Although not explicitly mentioned in equations (6) and (7), computing \(J_{}\) and \(J_{}\) involves searching for the evidence of the actual and proposed events.

## 4 Experiments

Our code is at https://github.com/iLampard/lamp. This repository includes CSV files containing numerical results of our experiments. It also includes qualitative results such as LLM-generated cause events. Experiment details (e.g., hyperparameters) are in Appendix E.

### Experimental Setup

We conducted experiments on three real-world datasets (see Appendix E.1 for dataset details).

GDELT(Leetaru and Schrodt, 2013).The GDELT Project monitors events all over the world, with live datasets updated every 15 minutes. We only focused on the political events that happened in G20 countries from 2022-01-01 to 2022-07-31, ending up with a corpus of 109000 time-stamped event tokens. This choice of time range guarantees that our data was not included in the training data of the most recent GPT. The event type \(k\) of each token has a structured name of the format subject-predicate-object. Each predicate is one of the twenty CAMEO codes such as CONSULT and INVESTIGATE (see Appendix E.4 for a full list); each subject or object is one of the 2279 political entities (individuals, groups, and states) such as Tesla and Australia. So there are about 104M event types in total, making this dataset extremely challenging. Each event token has a news headline that concisely describes the event. We split the dataset into disjoint train, dev, and test sets based on their dates: the 83100 events that happened before 2022-07-05 are training data; the 16650 events after 2022-07-19 are test data; the 9250 events between these dates are development data.

ICEWS(Boschee et al., 2015).Similar to GDELT, this dataset logs interactions between social-political entities. We collected 79410 event tokens from 2022-10-11 to 2023-02-28. Its event types have the same structure as GDELT: each predicate is one of the twenty CAMEO codes; each subject or object is one of the 2981 political entities. We split the dataset into disjoint train, dev, and test sets based on their dates: the 41600 events that happened before 2023-01-16 are training data; the 22030 events after 2023-02-01 are test data; the 15780 events between these dates are development data.

Amazon Review(Jianmo, 2019).This dataset contains user reviews on Amazon shopping website from 2014-01-04 to 2016-10-02. We focused on the most active 2500 users and each user has a sequence of product review events. The type \(k\) is the category of the product: we selected the most frequently-reviewed 23 categories and grouped all the others into a special OTHER category, ending up with 24 categories in total. Each review event also has a mark m which is the actual content of the review. Each of the 2500 sequences is cut into three segments: the events that happened before 2015-08-01 are training data; those after 2016-02-01 are test data; the events between these dates are dev data. Then we have 49,680 training tokens, 7,020 dev tokens, and 13,090 test tokens.

We experimented with four state-of-the-art event sequence models: NHP (Mei and Eisner, 2017a), Know-Evolve (KE) (Trivedi et al., 2017), DyRep (Trivedi et al., 2019), and ANHP (Yang et al., 2022). For each of them, we evaluated it as a baseline method as well as integrated it into our LAMP framework. KE and DyRep require domain-specific knowledge to configure their structural-sparse architectures: we evaluated them on GDELT since their GDELT-specific architectures are available in the original papers; we didn't evaluate them on Amazon Review since we do not have such knowledge on this data. ANHP can take domain knowledge into its architecture but it is optional, so we evaluated it on both GDELT and Amazon data: on GDELT, we adapt the knowledge used in KE and DyRep into its structure; on Amazon Review, we use the generic architecture. On Amazon Review, we also experimented with NHP since it doesn't require any domain-specific structure knowledge.

We experimented with three strong LLMs: GPT-3-davinci (Brown et al., 2020) which we also denote as G3.0; GPT-3.5-turbo (Brown et al., 2020; Stiennon et al., 2020; Gao et al., 2022) which we also denote as G3.5, and Llama-2-chat with 13B parameters (Touvron et al., 2023) which we also denote as llama. For GDELT and ICEWS data, we used 10-shot prompts; for Amazon Review data, we used 8-shot prompts. Each "shot" is a demonstration that contains an effect event followed by one or more expert-annotated cause events. Prompt examples can be found in Appendix E.4.

### Main Results on Type and Time Prediction

Our main results are displayed in Figure 2. Figure 1(a) shows the result of each method on GDELT data. GDELT data is updated every fifteen minutes so the time intervals are regular and thus it is not interesting to predict them. For type prediction, we focus on predicting certain attributes given the others, which is more practical than full type prediction. In practice, predicting "which of the hundreds of millions of events is the most probable" is too difficult and existing models will all perform disastrously. But answering questions like "what will A do to B" and "to whom A will do this" is usually useful enough for real applications. Note that attribute prediction is still very challenging: e.g., there are 45580 distinct predicate-object combinations in GDELT data.

We evaluate each model by the quality of its top-ranked predictions. For each baseline model (KE, DyRep, or ANHP), the list of top predictions contains the top \(M\) event types (with known attributes filled, if any) that have the highest intensities; see section 3.1. Our LLM-enhanced framework takes the list given by its base model, and sorts it based on the \(s_{}\) scores of the proposals. Our primary evaluation metric is the mean rank (MR). This metric has a straightforward interpretation: given a sorted list of proposed predictions, it measures the average rank of the ground-truth type in the list; a smaller MR means a higher rank, and thus a better result (e.g., MR \(=1\) means 'ranked at the top on average"). We also used the mean reciprocal rank (MRR), which is less interpretable but more robust to bad predictions than MR. Appendix E.2 includes full procedures for computing these metrics. In our experiments, MR and MRR results yield the same conclusions. So we present MR results in this section for its straightforward interpretation, but leave MRR results to Appendix F.

On ICEWS and Amazon Review, we evaluate each model on time prediction (in addition to type prediction), which is measured by the root of mean squared error (RMSE). For each held-out token, each base model proposes a list of scored predictions; see section 3.1. Our LAMP framework reranks the list given by its base model. In either case, the final prediction is the highest-ranked proposal.

In each evaluation setting shown in Figure 2, our LLM-enhanced framework substantially and consistently outperforms the corresponding baseline model across a range of \(L\) and \(M\) values. All the results throughout the paper have 95% bootstrap confidence intervals but they are too tight to be visually obvious in most cases, implying that our improvements are significant. When we draw more proposals from the base model (i.e., \(L\) and \(M\) are larger), our framework tends to enjoy a larger improvement over the base model. For predicate-object joint prediction on GDELT, the ground-truth type ranks at around 40 in the lists given by the base models, but is moved to around 20 by our LLM-based adjustment. For object prediction on ICEWS, our method improved the MR results from around 90 to about 20. Note that it is not fair to compare the same method across \(L\) or \(M\).

The MR and MRR are only evaluated on the held-out tokens whose ground-truth event types fall into the top proposals. But how many are there such tokens? This solely depends on how well the base event sequence model works. A detailed analysis can be found in Appendix F.2.

### Analysis

Now we present our analysis on GDELT. On GDELT, we focus on the predicate and object prediction. It is less expensive than the time and predicate-object prediction (which requires an order of magnitude more GPT hours and GPT API calls) but the results are well correlated in our pilot experiments. More analysis can be found in Appendix F, including analysis on the other datasets.

Analysis-I: About LLMs.Section 4.2 only shows the results of the GPT-3.5 version of our framework. Figure 3 shows the results of the other versions with ANHP as the base model. These LLMs all help improve the prediction performance but GPT-3.5 and Llama work significantly better than GPT-3. Interestingly, the Llama version performs competitive to the GPT-3.5 version. Its strong performance demonstrates that our framework doesn't have to depend on black-box LLMs; indeed, it can excel with open-source LLMs like Llama. Note that Llama has considerably fewer parameters than GPTs. An important reason for its success may be the reinforcement learning from human

Figure 2: Prediction performance of different methods on each dataset. On GDELT, the upper figure is for object prediction, and the lower figure is for predicate-object joint prediction. On ICEWS, the upper figure is for object prediction, and the lower figure is for time prediction. On Amazon Review, the upper figure is for type prediction, and the lower figure is for time prediction.

[MISSING_PAGE_FAIL:8]

through its pretraining and fine-tuning processes. Consequently, when presented with a proposal, it can follow demonstrations and draw upon its internal knowledge to suggest plausible causes. Benefiting from the strong generalization capability of LLMs, our LAMP framework has a significant potential for broad applications.

Analysis-V: About retrieval methods.To further investigate the usefulness of LLMs, we tested several versions of our framework that do not involve the LLM in retrieval: md randomly samples 10 previous events from the history for each proposal; rec uses the most recent 10 past events; rec-pre retrieves the most recent 10 events that share the same predicate with the proposal; rec-obj retrieves the most recent 10 events that share the same object; bert uses SBERT to embed the text-based identifier of each event and retrieve the most similar 10 events based on the cosine similarity of SBERT embeddings. As shown in Figure 8, all these retrieval methods perform significantly worse than our LLM-based method. Noticeably, the "most recent" retrievals work poorly. It suggests that "recent" is not a good inductive bias for our problem setting. On dev data, we found that the cause events retrieved based on LLM-generated clues are often not "recent".

We also tested using the edit distance as the similarity score in our LLM-based retrieval method. For a pair of events, we compute the character-level edit distance (Jurafsky and Martin, 2009) between their text strings that are otherwise fed into the SBERT embedding model (section 3.2), and define the similarity to be the reciprocal of this distance. As shown in Figure 8, using this metric yields a better result than the baseline method, but performs worse than our original framework.

Analysis-VI: About data leakage.GPT models are trained on online text data up to 2021.2 This dataset doesn't include the GDELT or ICEWS data used in our experiments. Our demonstrations span a wide time range in training data, and there is a large temporal gap between the training and test data. Precisely, the most recent demonstration is about an event on 2022-06-07, while the earliest test event occurred on 2022-07-05. Below are the percentiles for the time differences (in days) between the test events and the latest demonstration:

The GPT training data may have covered the Amazon Review data. Therefore, we had a conversation with GPT-3.5, checking whether it could recall information about specific users or specific reviews. GPT-3.5 could say nothing specific about the users or reviews, indicating that we do not have an issue of data leakage. Our conversation can be found in Appendix E.5.

## 5 Discussion and Related Work

Event reasoning and prediction is an important problem that arises in various real-world applications. A great number of event sequence models have been proposed and advanced this field, including the classical graphical models (Hawkes, 1971; Du et al., 2015), recurrent neural models (Du et al., 2016; Mei and Eisner, 2017; Xiao et al., 2017; Omi et al., 2019; Shchur et al., 2020; Boyd et al., 2020), and Transformer-based models (Zuo et al., 2021; Zhang et al., 2020; Enguehard et al., 2020; Sharma et al., 2021; Zhu et al., 2021; Yang et al., 2022; Xue et al., 2022; Liu et al., 2022). These models have been applied to a wide range of applications such as network analysis (Choi et al., 2015; Etesamiet al., 2016), recommendation systems (Du et al., 2015b), social analysis (Guo et al., 2015; Lukasik et al., 2016; Zhang et al., 2022), and healthcare (Hua et al., 2022; Zhang et al., 2023b).

Recently, there has been a growing body of research that has directed its focus towards the textual features of the real-world events, such as the reports and news articles about the events. This line of work includes Deng et al. (2020, 2021, 2022); Han & Ning (2022). Their methods all treat text as auxiliary features but do not consider reasoning about that text information. Our work is significantly different from this line of research since we focus on reasoning. Our framework reasons about the events and their text information (e.g., text identifiers of the event types, associated documents) by prompting a large language model. The large language model has consumed a massive amount of text during pretraining and is aware of diverse world knowledge, thus able to perform the kind of reasoning beyond the capacities of the aforementioned models.

By leveraging large language models, our framework induces domain-specific knowledge into the deployment of event sequence models. It is related to previous work of configuring domain-specific knowledge into model architectures (Trivedi et al., 2017, 2019; Mei et al., 2020; Yang et al., 2022). But our work is significantly different: it doesn't rely on human domain experts to write down a full set of knowledge; instead, we extract knowledge from language models via few-shot prompting. Prompting has become a standard way of applying large language models to reasoning tasks. There has been a recent surge of work that develops novel prompting techniques for a better performance (Wei et al., 2022a, b; Zhou et al., 2023). Our framework is general and can adopt any prompting methods.

This paper is closely related to research in logical reasoning, which primarily focuses on proving goals from known facts and rules. A major approach for this problem is backward chaining: it works backward from the goal, chaining through rules to find known facts that support the proof (Russell & Norvig, 2010). This approach has been applied to various application problems, including reasoning and planning in natural language (Ye et al., 2022; Weir & Van Durme, 2022; Kazemi et al., 2023; Kassner et al., 2023). In our setting, each proposed event is treated as a goal, and previous events serve as known facts. Like backward chaining, our method identifies previous events that support the proposal. But our method only performs a single step of reasoning, assuming complete data where all events are observable. A second step of reasoning is unnecessary since direct causes are observable and more temporally recent than indirect causes. In cases of incomplete data, multiple reasoning steps may be required to identify indirect causes. Handling incomplete data will be a non-trivial extension of our current framework, which we leave to future research.

This paper is also closely related to research in event-centric natural language processing (NLP) (Chen et al., 2021). Over the past decades, there has been a great amount of progress in this area, including many datasets and benchmarks (Ning et al., 2020; Li et al., 2020; Han et al., 2021; Wen et al., 2021; Feng et al., 2022) as well as a diversity of methods for key problems such as event detection and extraction (Ji & Grishman, 2008; Li et al., 2013; Feng et al., 2016; Lyu et al., 2021; Wang et al., 2021), relation extraction and prediction (Chan & Roth, 2011; Ning et al., 2018b; Wang et al., 2020; Wen & Ji, 2021; Li et al., 2022), event induction and summarization (Do et al., 2012; Saravanakumar et al., 2021; Li et al., 2021b, a; Jin et al., 2022), and temporal reasoning (Ning et al., 2018a; Ballesteros et al., 2020; Zhou et al., 2020, 2021). Recently, there has been research in leveraging LLMs to solve these problems (Dror et al., 2023; Li et al., 2023; Zhang et al., 2023a). Our work complements this line of research, focusing on the problem of event sequence modeling, which has been out of the scope of the classical event-centric NLP.

## 6 Conclusion

In this paper, we present LAMP, a general modeling and prediction framework that leverages the abductive reasoning ability of large language models to help improve the prediction performance of event sequence models. Empirical studies demonstrate that our LLM-enhanced framework can significantly outperform the state-of-the-art event sequence models. Our findings have significant implications for future research in the field of event sequence modeling. In principle, an event sequence model should benefit from a range of reasoning abilities of large language models such as deductive reasoning, inductive reasoning, commonsense reasoning, and arithmetic reasoning. Further exploration in this area may lead to impactful innovations.