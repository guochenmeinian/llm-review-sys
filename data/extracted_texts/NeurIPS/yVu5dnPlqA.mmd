# MAmmoTH2: Scaling Instructions from the Web

Xiang Yue, &Tuney Zheng1, &Ge Zhang1, &Wenhu Chen1

Carnegie Mellon University, &University of Waterloo

xyue2@andrew.cmu.edu wenhuchen@uwaterloo.ca

https://tiger-ai-lab.github.io/MammoTH2/

###### Abstract

Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.

## 1 Introduction

Reasoning is a fundamental aspect of human cognition and problem-solving . Proficiency in

Figure 1: Overview of MAmmoTH2-Plus results. The MAmmoTH2-8x7B-Plus variant outperforms Mixtral-Instruct on reasoning benchmarks, matching Qwen-1.5-110B with only 13B active parameters. It also surpasses Mixtral-Instruct by around 10 points on general code and chatbot benchmarks.

reasoning is essential for advancing scientific knowledge, developing new technologies, and making informed decisions in various contexts. Recently, large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023; Achiam et al., 2023; Team et al., 2023) have shown remarkable progress in various NLP tasks. However, their ability to perform complex reasoning tasks (Lin et al., 2024) in the domains of mathematics, science, and engineering is still limited.

Recent studies have extensively explored how to enhance base LLMs' reasoning abilities. The two main approaches are continued training and instruction tuning. Continued training trains LLMs on large-scale filtered documents (Lewkowycz et al., 2022; Taylor et al., 2022; Azerbayev et al., 2023; Shao et al., 2024; Ying et al., 2024). Instruction tuning seeks to employ supervised fine-tuning loss on, usually small-scale, high-quality instruction-response pairs (Ouyang et al., 2022; Chung et al., 2024). While human-annotated instruction datasets (Cobbe et al., 2021; Hendrycks et al., 2021; Amini et al., 2019) are often limited in scale, recent studies (Yu et al., 2023; Yue et al., 2023; Toshniwal et al., 2024; Li et al., 2024; Tang et al., 2024) attempt to prompt GPT-4 with seed data to increase the scalability. However, the synthesized instruction data becomes highly biased, not diverse, and prone to a high degree of hallucination.

To address these limitations, we propose to **discover naturally existing instruction data from the web** (Figure 2). We argue that the pre-training corpus (e.g., Common Crawl) already contains a vast amount of high-quality instruction data for LLM reasoning. For example, the web corpus contains a large amount of educational materials in the form of instruction-following pairs. These documents range across various domains like math, science, engineering, and humanities. Such readily available instruction data is not only diverse but also of high quality. However, such instruction data is highly dispersed across the corpus, which makes it particularly challenging to discover.

In this paper, we aim to mine these instruction-response pairs from the web using a three-step pipeline. **(1) Recall step**: We create a diverse seed dataset by crawling several quiz websites. We use this seed data to train a fastText model (Joulin et al., 2016) and employ it to recall documents from Common Crawl (Computer, 2023). GPT-4 is used to trim down the recalled documents by their root URL. We obtain 18M documents through this step. **(2) Extract step**: We utilize open-source LLMs like Mixtral (Jiang et al., 2024) to extract Q-A pairs from these documents, producing roughly 5M candidate Q-A pairs. **(3) Refine step**: After extraction, we further employ Mixtral-8\(\)7B (Jiang et al., 2024) and Qwen-72B (Bai et al., 2023) to refine (Zheng et al., 2024) these candidate Q-A pairs. This refinement operation aims to remove unrelated content, fix formality, and add missing explanations to the candidate Q-A pairs. This refinement operation is pivotal to maintaining the quality of the mined Q-A pairs. Eventually, we harvest a total of 10M instruction-response pairs through these steps. Unlike existing instruction-tuning datasets, our dataset WebInstruct is purely mined from the Web without any human crowdsourcing or GPT-4 distillation.

We validate the effectiveness of WebInstruct by training MAMmoTH2 on various base models (Figure 1), including Mistral-7B (Jiang et al., 2023), LLama3-8B (Meta, 2024), Mixtral-8\(\)7B (Jiang et al., 2024), and Yi-34B (Young et al., 2024). MAMmoTH2 significantly outperforms the base models on seven held-out reasoning benchmarks: TheoremQA (Chen et al., 2023b), GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), ARC-C (Clark et al., 2018), MMLU-STEM (Hendrycks et al., 2021b), GPQA (Rein et al., 2023), and BBH (Suzgun et al., 2022). MAMmoTH2-7B improves Mistral-7B's performance by an average of 14 absolute points, while MAMmoTH2-34B enhances Yi-34B's performance by an average of 5.8 absolute points. Notably, Mistral-7B's MATH accuracy can rise from 11.2% to 36.7% after training on WebInstruct. As our dataset contains no in-domain data from our evaluation benchmarks, this highlights the models' strong generalization ability.

We further enhance MAMmoTH2's performance on code generation, math reasoning, and instruction-following tasks by tuning it on open-source instruction datasets, including OpenHermes2.5 (Teknium,

Figure 2: Comparison between our dataset curation method and previous studies.

2023], Code-Feedback [Zheng et al., 2024c], and Math-plus. The resulting model, MAMmoTH2-Plus, excels on seven reasoning benchmarks and other general tasks. MAMmoTH2-7B-Plus and MAMmoTH2-8B-Plus achieve state-of-the-art performance on TheoremQA, ARC-C, MMLU-STEM, GPQA, and BBH, and competitive results on MATH (45%) and GSM8K (85%). MAMmoTH2-Plus also performs well on general tasks, with MAMmoTH2-7B-Plus showing promising results on HumanEval and MBPP, and MAMmoTH2-8*7B leading the AlpacaEval 2.0 and Arena Hard leaderboards.

Interestingly, MAMmoTH2-8B-Plus and Llama-3-8B-Instrot, both tuned from Llama-3-base using datasets of the same size (10M), provide an apple-to-apple comparison. The only distinction is that Llama-3-8B-Instrot is trained on 10M human-annotated dataset while we do not require any human annotation. MAMmoTH2-8B-Plus outperforms Llama-3-Instrot by 6 points on reasoning tasks while matching its performance on general tasks, reflecting WebInstruct's cost-effectiveness advantage. MAMmoTH2-Plus consistently surpasses official instruction models like Mistral-Instruct on chat benchmarks. These results demonstrate the effectiveness of our approach to scale up instruction data from the web and offer a new perspective for future instruction tuning studies.

## 2 WebInstruct

In this section, we outline the process of constructing WebInstruct. Specifically, we divide the data collection pipeline into three stages: (1) relevant document recall from the web corpus, (2) Q-A pair extraction from recalled document, and (3) Q-A pair refinement. The full pipeline is depicted in Figure 3 and an example for extraction and refinement is provided in Figure 4.

### Recall from Common Crawl

In contrast to previous math-centric approaches [Paster et al., 2023, Wang et al., 2023c, Shao et al., 2024], we aim for broad coverage of disciplines such as math, science, engineering, etc. Therefore, careful balancing of the seed data is necessary to ensure diversity. However, publicly available training datasets are mostly limited to mathematics. To address this issue, we propose to crawl new

Figure 4: An illustrating example from WebInstruct for the extraction and refinement step.

Figure 3: Step 1: Recall relevant documents from Common Crawl. Step 2: Extracting Q-A pairs. Step 3: Refine with the extracted Q-A pairs.

exam problems from several educational websites. These sites contain diverse problems from various disciplines, helping to ensure diversity. We crawled 100K seed data as positive training examples and randomly selected 100K negative documents from CC  to train a fastText model . The trained fastText model is used to recall relevant documents. We employ the open-source fastText library with a vector dimension of 256 to train the model for 3 epochs, with a learning rate of 0.1, a maximum n-gram length of 3, and a maximum number of word occurrences of 3. We recalled 100B tokens using the trained fasttext model from an internal CC. These raw web documents are further grouped by their domains (root URL) and only domains with more than 1000 documents are retained. We extracted roughly 600K domains from the recalled documents. We then prompt GPT-3.5 to scan through the domains and automatically select those that might contain instruction data. Around 50K domains are further labeled as positive samples by GPT-3.5. Note that all the recalled documents in the first round are _not kept_ for further usage in Q-A Pair Extraction and Refinement. Next, we sample documents from the selected domains as positive examples, and documents from the non-selected domains and general Common Crawl as negative examples to re-train an improved fastText classifier. The newly trained fastText classifier is used to recall documents. We recalled 40B tokens using the newly trained fastText model. We prompt GPT-4 to sift through the recalled domains again, ultimately leading to 18M raw documents, primarily originating from the desired websites.

### Q-A Pair Extraction

We observe that a significant number of naturally existing Q-A pairs are present in the 18M documents. However, these Q-A pairs are interspersed with a high volume of noise such as ads, markups, boilerplate, etc. Our preliminary training on these raw documents only yields limited gains.

First, we carefully pre-process the HTML to pre-extract useful content from the recalled documents. This is mostly rule-based filtering to clean site information, ads, HTML boilerplate, etc. This step significantly reduces the document length for the next stage. We then prompt Qwen-72B  to identify the question and answer pairs from the preprocessed documents. Specifically, we provide a few in-context examples to help the model understand what to extract. We also allow the model to return void if no natural question-answer pairs exist. In this stage, only 30% of the recalled documents were identified as containing naturally existing Q-A pairs, resulting in roughly 5M Q-A pairs as our candidates for the next step. However, these candidates still contain a substantial amount of unrelated content and formality issues. Besides that, a large portion of the extracted Q-A pairs also lack explanations for how the answer is derived. Therefore, we propose to perform another round of refinement to increase the data quality.

To avoid contamination, we follow previous work  and filter out web pages containing questions or answers to all of our evaluation benchmarks. Specifically, we filter out all web pages that contain \(n\)-grams (\(n=10\)) string matches with either the questions or answers.

### Q-A Pair Refinement

To further improve the extracted Q-A pair candidates, we propose refining them using LLMs. In this step, we prompt Mistral-22B\(\)8  and Qwen-72B  to reformat the extracted Q-A pairs. If the answer does not contain any explanation, these two LLMs will attempt to complete the intermediate reasoning steps leading to the given answer. We adopt two models to increase the diversity of our dataset. Eventually, we harvest 10M Q-A pairs as our final instruction-tuning dataset WebInstruct.

### Dataset Statistics

To better distinguish our dataset from the existing ones, we include a summarization table in Table 1. It can be observed that most SFT datasets contain less than 1M samples but are of high quality. XwinMath  is the largest dataset, scaling up to over 1M samples through GPT4 synthesis, while OpenMathInstruct  has not been generated using GPT-4 but instead uses Mistral-8x7B Jiang et al. . However, the seed data for both datasets is only based on GSM and MATH, leading to narrow domain coverage. In contrast, continue-training (CT) datasets are normally filtered from the web with much larger size, often exceeding 10B tokens and even rising to 120B tokens. However, continued pre-training on these massive datasets can be not only expensive but also ineffective due to the high noise ratio. WebInstruct, with roughly 5B tokens, strikes a good balance between scalability and quality. It approaches the scalability of common CT datasets while maintaining high quality through the three-step construction pipeline. This makes our dataset unique compared to other alternatives.

### Additional Public Instruction Datasets

To further enhance the diversity and quality of our dataset, we fine-tune MmmnoTH2 on several open-source instruction tuning datasets. These datasets are carefully selected based on their relevance to different reasoning subjects. Additionally, we consider some chat datasets to balance reasoning ability and general chat ability. The open-source datasets we incorporate are OpenHermes 2.5 (Teknium, 2023), Code-Feedback (Zheng et al., 2024c) and our Math-Plus, which is an augmented version of MetaMathQA (395K) (Yu et al., 2023) and Orca-Math (200K) (Mitra et al., 2024). More details of the public datasets can be found in Appendix A.

## 3 Experimental Setup

### Training Setup

We unify all the samples in our instruction dataset to conform to the structure of a multi-turn instruction tuning dataset. This standardization ensures that the fine-tuned models can process data consistently, regardless of the original dataset formats. We select the open-source models Mistral 7B (Jiang et al., 2023), Mistral 8\(\)7B (Jiang et al., 2024), Llama-3 8B (Meta, 2024), and Yi-34B (Young et al., 2024) as our base models. We fine-tune these models to validate our WebInstruct at multiple scales using the LLaMA-Factory (Zheng et al., 2024d) library. We use a learning rate of 5e-6 for Mistral 7B and 1e-5 for Mistral, Llama-3 8B, and Yi 34B. The global batch size is set to 512 with a maximum sequence length of 4096. We employ a cosine scheduler with a 3% warm-up period for 2 epochs. To efficiently train the models, we utilize DeepSpeed (Rasley et al., 2020) with the ZeRO-3 stage. All the models are trained with 32 A100 GPUs.

### Evaluation Datasets

To rigorously assess the capabilities of models in reasoning abilities across different domains, we utilize several widely used datasets, GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), TheoremQA (Chen et al., 2023b), BIG-Bench Hard (BBH) (Suzgun et al., 2022), ARC-C (Clark

   Dataset & \#Pairs & Domain & Format & Dataset Source \\  FLAN V2 (Chung et al., 2024) & 100K & General & SFT & NLP data + Human CoT \\ Self-Instruct (Wang et al., 2023b) & 82K & General & SFT & Generated by GPT3 \\ GPT4-Alpaca (Taori et al., 2023) & 52K & General & SFT & Generated by GPT4 \\ SuperNI (Wang et al., 2022) & 96K & General & SFT & NLP Datasets \\ Tora (Gou et al., 2023) & 16K & Math & SFT & GSM+MATH Synthesis by GPT4 \\ WizardMath (Luo et al., 2023) & 96K & Math & SFT & GSM-MATH Synthesis by GPT4 \\ MathInstruct (Yue et al., 2023b) & 262K & Math & SFT & Math datasets Synthesis by GPT4 \\ MetaMathQA (Yu et al., 2023) & 395K & Math & SFT & GSM+MATH Synthesis by GPT3.5 \\ XwinMath (Li et al., 2024a) & 1.4M & Math & SFT & GSM+MATH Synthesis by GPT4 \\ OpenMathstruct (Toshniwal et al., 2024) & 1.8M & Math & SFT & GSM+MATH Synthesis by Mistral \\  Dataset &  & Domain & Format & Dataset Source \\  OpenWebMath (Paster et al., 2023) & 12B & Math & LM & Filtered from Web \\ MathPile (Wang et al., 2023c) & 10B & Math & LM & Filtered from Web \\ Cosmopeida (Ben Allal et al., 2024) & 25B & General & LM & Synthesized by Mistral \\ MINERVA (Lewkowycz et al., 2022) & 38B & Math & LM & Filtered from Web \\ Proof-File (Jazerbayev et al., 2023) & 55B & Math & LM & OpenWebMath+Arxiv+Code \\ Galactic (Taylor et al., 2022) & 106B & Math \& Sci. & LM & Filtered from Web \\ DeepseekMath (Shao et al., 2024) & 120B & Math & LM & Recalled from Web \\  WebInstruct &  & Math \& Sci. & SFT & Recall and Extracted from Web \\   

Table 1: The list of existing supervise-fine-tuning (SFT) and continue-training (CT) datasets. SFT datasets are primarily from academic NLP sources or synthesized by GPT-3.5/4 using seed data. CT datasets are larger but nosier. Our dataset falls between these two types.

et al., 2018), GPQA (Rein et al., 2023), MMLU-STEM (Hendrycks et al., 2021a). These datasets collectively enable a comprehensive assessment of language models' reasoning prowess across a spectrum of complexity and realism. The details of the evaluation datasets are in Appendix B.

We further evaluate the models on additional code generation tasks (including HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and their augmented version (Liu et al., 2024)), general LLM benchmarks like MMLU (Hendrycks et al., 2021a) and its recent robust and challenging version MMLU-Pro (TIGER-Lab, 2024). We also consider chat benchmarks like MT-Bench (Zheng et al., 2024a), AlpacaEval 2.0 (Li et al., 2023b), and Arena Hard (Li et al., 2024b) to demonstrate the generalizability of WebInstruct and WebInstruct-Plus on more general LLM benchmarks.

   Model & TheoremQA & MATH & GSM8K & GPQA & MMLU-ST & BBH & ARC-C & AVG \\  GPT-4-Turbo-0409 & 48.4 & 69.2 & 94.5 & 46.2 & 76.5 & 86.7 & 93.6 & 73.6 \\   \\  Qwen-1.5-110B & 34.9 & 49.6 & 85.4 & 35.9 & 73.4 & 74.8 & 91.6 & 63.6 \\ Qwen-1.5-72B & 29.3 & 46.8 & 77.6 & 36.3 & 68.5 & 68.0 & 92.2 & 59.8 \\ Deepsek-LM-67B & 25.3 & 15.9 & 66.5 & 31.8 & 57.4 & 71.7 & 86.8 & 50.7 \\ Yi-34B & 23.2 & 15.9 & 67.9 & 29.7 & 62.6 & 66.4 & 89.5 & 50.7 \\ Llemma-34B & 21.1 & 25.0 & 71.9 & 29.2 & 54.7 & 48.4 & 69.5 & 45.7 \\ Mistral-8\(\)7B & 23.2 & 28.4 & 74.4 & 29.7 & 59.7 & 66.8 & 84.7 & 52.4 \\ Mistral-8\(\)7B-Instrct & 25.3 & 22.1 & 71.7 & 32.4 & 61.4 & 57.3 & 84.7 & 50.7 \\ Intern-Math-20B & 17.1 & 37.7 & 82.9 & 28.9 & 50.1 & 39.3 & 68.6 & 46.4 \\   \\  MamoTiH2-34B & 30.4 & 35.0 & 75.6 & 31.8 & 64.5 & 68.0 & 90.0 & 56.4 \\ \(\) over Yi & +7.2 & +19.1 & +7.7 & +2.1 & +2.9 & +1.2 & +0.5 & +5.8 \\ MamoTiH2-8x7B & 32.2 & 39.0 & 75.4 & 36.8 & 67.4 & 71.1 & 87.5 & 58.9 \\ \(\) over Mistral & +9.2 & +10.6 & +1.0 & +7.1 & +7.4 & +3.3 & +2.8 & +6.5 \\   \\  MamoTiH2-8x7B-Plus & **34.1** & **47.0** & **86.4** & **37.8** & **72.4** & **74.1** & **88.4** & **62.9** \\ \(\) over Qwen-1.5-110B & -0.8 & -2.6 & +1.0 & +1.5 & -1.0 & -0.7 & -4.0 & -0.7 \\   \\  Deepsek-7B & 15.7 & 6.4 & 17.4 & 25.7 & 43.1 & 42.8 & 47.8 & 28.4 \\ Qwen-1.5-7B & 14.2 & 13.3 & 54.1 & 26.7 & 45.4 & 45.2 & 75.6 & 39.2 \\ Mistral-7B & 19.2 & 11.2 & 36.2 & 24.7 & 50.1 & 55.7 & 74.2 & 38.8 \\ Gemma-7B & 21.5 & 24.3 & 46.4 & 25.7 & 53.3 & 57.4 & 72.5 & 43.0 \\ Llemma-7B & 17.2 & 18.0 & 36.4 & 23.2 & 45.2 & 44.9 & 50.5 & 33.6 \\ WizardMath-7B-1.1 & 11.7 & 33.0 & 83.2 & 28.7 & 52.7 & 56.7 & 76.9 & 49.0 \\ Abel-7B-002 & 19.3 & 29.5 & 83.2 & 30.3 & 29.7 & 32.7 & 72.5 & 42.5 \\ Intern-Math-7B & 13.2 & 34.6 & 78.1 & 22.7 & 41.1 & 48.1 & 59.8 & 42.5 \\ Rho-1-Matt-7B & 21.0 & 31.0 & 66.9 & 29.2 & 53.1 & 57.7 & 72.7 & 47.3 \\ Deepsek-Math-7B & 25.3 & 34.0 & 64.2 & 29.2 & 56.4 & 59.5 & 67.8 & 48.0 \\ Deepsek-Math-Instruct & 23.7 & 44.3 & 82.9 & 31.8 & 59.3 & 55.4 & 70.1 & 52.5 \\ Llama-3-8B & 20.1 & 21.3 & 54.8 & 27.2 & 55.6 & 61.1 & 78.6 & 45.5 \\ Llama-3-8B-Instruct & 22.8 & 30.0 & 79.5 & 34.5 & 60.2 & 66.0 & 80.8 & 53.4 \\   \\  MamoTiH2-7B & 29.0 & 36.7 & 68.4 & 32.4 & 62.4 & 58.6 & 81.7 & 52.8 \\ \(\) over Mistral & +9.8 & +25.5 & +32.2 & +7.7 & +12.3 & +2.9 & +7.5 & +14.0 \\ MamoTiH2-8B & 32.2 & 35.8 & 70.4 & 35.2 & 64.2 & 62.1 & 82.2 & 54.3 \\ \(\) over Llama3 & +12.2 & +14.5 & +15.6 & +8.0 & +8.6 & +1.0 & +3.6 & +8.8 \\   \\  MamoTiH2-7B-Plus & 29.2 & **45.0** & **84.7** & 36.8 & 64.5 & 63.1 & 83.0 & 58.0 \\ MamoTiH2-8B-Plus & **32.5** & 42.8 & 84.1 & **37.3** & **65.7** & **67.8** & **83.4** & **59.1** \\ \(\) over best baseline & +7.2 & +0.7 & +1.5 & +2.8 & +5.5 & +1.8 & +2.6 & +5.7 \\   

Table 2: Main results on reasoning datasets. Models without the ‘Instruct’ suffix refer to the released base models. Results are taken from official papers or blogs when available; otherwise, we use our own evaluation script. Underscored results represent the best baseline scores under the size constraint. All models are inferred with few-shot CoT: TheoremQA (5-shot), MATH (4-shot), GSM8K (4-shot), GPQA (5-shot), MMLU-STEM (5-shot), BBH (3-shot), and ARC-C (8-shot).

Main Results

### Experimental Results on Reasoning Benchmarks

Table 2 presents our main results, with existing models partitioned into two tracks based on their parameter size. For 7B parameter models, we observe that our model trained solely with WebInstruct achieves significant improvements over the base models. For instance, M4mmoTH2-7B boosts the performance of Mistral-7B by an average of 14 points. Notably, WebInstruct does not contain any training data from these evaluation benchmarks, making all evaluations essentially held-out. The substantial performance gains demonstrate the strong generalization capabilities of M4mmoTH2-7B. Similarly, M4mmoTH2-8B boosts the performance of Llama-3-8B-base by an average of 8.8 points. We also experiment with larger models like Yi-34B and Mistral to show that the performance gains are consistent across the board. Notably, Yi-34B's performance on MATH also increases by 19% after training on WebInstruct.

Further tuning on several additional public datasets also significantly enhances performance. The M4mmoTH2-Plus model family achieves state-of-the-art results across the board. For example, M4mmoTH2-Plus's performance on TheoremQA, GPQA, and ARC-C represents the best-known results for any model under 10B parameters. M4mmoTH2-7B-Plus's performance on MATH and GSM is also close to the best-known results. We also show the results of the models solely trained on the additional public datasets in Appendix E.

An interesting comparison is between M4mmoTH2-8B-Plus and Llama3-Instruct, as both models are trained from the Llama3-base. Llama-3-instruct was trained on a 10M human-annotated instruction dataset along with public datasets, similar to WebInstruct combined with additional public datasets. Therefore, these two models are highly comparable. Our experiments show that M4mmoTH2-8B-Plus outperforms Llama3-Instruct by an average of 6% across the benchmarks. This substantial gain indicates that WebInstruct is highly cost-effective. For larger models, we found that M4mmoTH2-8x7B-Plus can even match the performance of Qwen-1.5-110B with only 13B active parameters. These results demonstrate the effectiveness of our scalable instruction tuning approach.

### Additional Experimental Results

To further demonstrate the capabilities of our models beyond the reasoning benchmarks presented in Table 2, we conduct additional experiments to evaluate their performance on code generation, general language understanding, and instruction-following tasks. Table 3 showcases the results of various models on code generation tasks. The M4mmoTH2-7B-Plus model exhibits strong performance, achieving the highest average scores of 66.1 and 58.2 on HumanEval(+) and MBPP(+) datasets, respectively. It outperforms the official instruct counterparts like Mistral-7B-Instruct-v0.2 on these metrics, indicating its superior code generation abilities.

To assess the general language understanding and instruction-following capabilities of our models, we evaluate them on a range of benchmarks, as shown in Table 3. The M4mmoTH2-Plus models exhibit strong performance across these tasks, showcasing their versatility and robustness. For example, M4mmoTH2-8x7B-Plus achieves the highest scores on AlpacaEval 2.0 and Arena Hard leaderboards, surpassing competitive models like GPT-3.5-Turbo and Tulu-2-DPO-70B [Ivison et al., 2023].

Figure 5: Mistral-7B model reasoning performance improves with scaling instructions. Additionally, SFT Loss is a more effective learning approach compared to LM Loss.

The strong performance of MAMmoTH2 on code generation and general language understanding tasks, as evidenced by Table 3, demonstrates that our method does not overfit to the reasoning benchmarks. Instead, it shows the models' ability to generalize well to a wide range of tasks, highlighting their versatility and robustness. These additional experiments further validate the effectiveness of our WebInstruct in developing powerful and flexible language models.

## 5 Ablation Study

### Scaling Effect of Instructions

We first investigate the impact of model scaling and loss functions on the performance of language models across three representative tasks: MATH, TheoremQA, and ARC-C. We train models with varying training samples (1M to 10M) using extracted QA and refined QA data, and compare the effectiveness of two training losses: LM loss and SFT loss. Figure 5 shows that increasing model size and using SFT Loss with synthetic data consistently improves accuracy across all tasks. These findings demonstrate the importance of model scaling and supervised fine-tuning with synthetic data for enhancing language model performance in various domains.

### Comparison of Two Refined Models

To assess the effectiveness of the Q-A pair refinement process by different LLMs, we conducted experiments by training three Mistral-7B models: one on the data refined by Mistral-22B\(\)8, another on the data refined by Qwen-72B, and a third on the merged samples refined by both models. For a fair comparison, we trained the models with the same 9000 steps and a global batch size of 512. Our results show that the model trained on Mistral-22B\(\)8 refined data achieves comparable performance to the one trained on Qwen-72B refined data. The model trained on the merged samples consistently outperforms the models trained on data refined by individual LLMs. This demonstrates that combining data refined by different models mitigates potential biases that may arise from using a single model. By leveraging the strengths of both

   Data & GSM & MATH & MMLU-S & Theo. & ARC. \\  Mistral & 62.9 & 29.1 & 56.5 & **26.1** & 78.3 \\ Qwen & 65.4 & 28.9 & **60.6** & 23.5 & 80.8 \\ 
**Merged** & **65.6** & **31.0** & 60.5 & 24.8 & **81.8** \\   

Table 4: Comparison of the two data-refining LLMs. We train the three models with the same steps.

    & Code &  &  Alpaca \\ Eval 2.0 \\  } &  Arena \\ Hard \\  } &  &  MMLU-Pro \\  } \\  & Generation & & & & \\  GPT-4-1106-preview & 85.6 (77.5) & 9.32 & 50.0 & - & - & - \\ GPT-3.5-Turbo-1106 & 79.7 (70.2) & 8.32 & 19.3 & 18.9 & - & - \\ GPT-3.5-Turbo-0301 & - & 7.94 & 18.1 & 18.1 & 70.0 & - \\  Tulu-2-DPO-70B & 51.2 (43.0) & 7.89 & 21.2 & 15.0 & 67.8 & 40.5 \\ Llama-2-70b-chat & 31.4 (26.5) & 6.86 & 14.7 & 11.6 & 63.0 & 33.6 \\ Yi-34B-Chat & 38.7 (32.6) & 7.86 & 27.2 & 23.1 & 73.5 & 42.1 \\  Mistral-7B-Instruct-v0.2 & 43.4 (36.5) & 7.60 & 17.1 & 12.6 & 60.8 & 30.8 \\ Llama-3-8B-Instruct & 65.8 (58.0) & 8.02 & 22.9 & 20.6 & 67.2 & 40.9 \\ Mistral-8\(\)7B-Instruct-v0.1 & 52.3 (44.7) & **8.30** & 23.7 & 23.4 & **70.6** & 41.0 \\  MAMmoTH2-7B-Plus & **66.1 (58.2)** & 7.88 & 23.4 & 14.6 & 63.3 & 40.9 \\ MAMmoTH2-8B-Plus & 61.9 (53.3) & 7.95 & 18.5 & 16.6 & 64.6 & 43.4 \\ MAMmoTH2-8x7B-Plus & 63.3 (55.3) & 8.20 & **33.8** & **32.6** & 68.3 & **50.4** \\   

Table 3: Evaluation of code generation, instruction-following and MMLU-(Pro) performance for various models. We report the average of HumanEval(+) and MBPP (+) accuracy as the code generation performance (breakdown results are in Appendix D). Baseline scores are sourced from the original papers or the EvalPlus, MT-Bench, AlpacaEval 2.0, Arena Hard and MMLU-Pro leaderboards. (“-”) indicates that the score is not available from the sources. MAMmoTH2-Plus exhibits strong general conversational ability and excels at multitask language understanding across a wide range of domains compared to their official instruct counterparts and larger models.

Mixtl and Qwen, we obtain a more balanced and comprehensive dataset, preserving the diversity of naturally occurring web instructions while enhancing data quality. Unlike traditional distillation methods, our approach does not directly inherit knowledge from other models but instead utilizes LLMs for data extraction and refinement. This distinction helps preserve the original structure and content of instructions from diverse web sources, potentially reducing model-introduced biases.

### Comparison of Different Domains and Sources.

To understand how each domain (e.g., math, science, others) and data source (e.g., forum websites and education websites) contribute to the training, we train Mistral 7B on the subsets of different domains and data sources. Details of how we obtain domain labels can be found in Appendix G.

As shown in Table 5, training on different domains and data sources leads to varied performance across the evaluation benchmarks. The education website data source consistently outperforms the forum data source, indicating the higher quality of educational questions. Interestingly, while the math domain excels on MATH, it does not lead to significant improvements on GSM8K, another math-focused dataset, suggesting that training on a single math dataset may not generalize well to other math benchmarks. Furthermore, training solely on the math domain does not yield substantial gains on science and STEM benchmarks, highlighting the need for diverse training. In contrast, the "Other" domain, which includes a diverse range of subjects, achieves the highest score on GSM8K, emphasizing the importance of diversity in the training data.

### Effectiveness of Extract and Refine Steps

Our ablation studies highlight the essential role of the "Extract" and "Refine" steps in our data pipeline, demonstrating that these steps contribute significantly to model performance on a broad range of reasoning tasks.

Effectiveness of Extract and Refine Steps.To clarify the experimental settings used in our ablation study, we define each setup as follows:

* **(a) Recall**: Training on 18M recalled documents, serving as the baseline dataset.
* **(a') DeepSeek Math Recall**(Shao et al., 2024): Deepsek Math 500B pre-training tokens.
* **(b) Extract**: Filtering the recalled documents to retain only high-quality question-answer pairs.
* **(c) Refine**: Further refining the extracted pairs by adding reasoning steps and clarifying responses.
* **(d) Public SFT**: Supervised fine-tuning using additional public fine-tuning data.
* **(d') DeepSeek Math SFT**: The SFT set from the DeepSeek Math.

The results, shown in Table 6, indicate that training solely on recalled documents with supervised fine-tuning, i.e., **Recall + Public SFT (a + d)**, achieves only modest gains, highlighting the importance of filtering and refining the initial corpus. Adding the "Extract" step, or **Recall + Extract + Public SFT (a + b + d)**, yields notable improvements by narrowing the training data to high-quality question-answer pairs, thus reducing token usage while preserving strong performance across benchmarks.

Incorporating the "Refine" step, or **Recall + Extract + Refine + Public SFT (a + b + c + d)**, further improves model accuracy by enhancing the rationale and clarity of responses, which is particularly beneficial for reasoning-intensive tasks. Additionally, comparisons with the **DeepSeek Math Corpus (a')** and **DeepSeek Math SFT (d')** demonstrate that while domain-specific data can produce high scores on targeted benchmarks, our generalized pipeline yields comparable or even superior results across a broader array of reasoning tasks.

   Data Source & GSM & MATH & MMLU-S & Theo. & ARC. \\   _Base_ & 47.4 & 15.7 & 51.4 & 17.3 & 77.6 \\  Forum & 51.0 & 24.0 & **54.7** & 21.0 & 78.2 \\ Education & **58.0** & **24.8** & 54.3 & **23.2** & **79.5** \\  Math & 52.9 & **27.3** & 51.6 & **21.7** & 74.1 \\ Science & 54.4 & 23.7 & **58.9** & 21.0 & **83.6** \\ Other & **59.4** & 20.8 & 55.3 & 21.1 & 79.4 \\   

Table 5: Impact of different data domains and sources on model performance. All models are trained with identical steps; _Base_ denotes the base model’s performance.

In conclusion, these findings validate the necessity of both the "Extract" and "Refine" steps in building an effective instruction dataset from web-sourced data. This approach not only provides a scalable and economical alternative to human-annotated data but also enhances the model's generalization capability on a wide array of reasoning tasks.

### Case Study

We further conduct a case study examining the quality of extracted and refined QA pairs from the dataset. We showcase some good and bad cases in Appendix J. We observe that the question/answer pairs extracted from well-formed exam and education websites are of high quality. The common issue is that a large portion of extracted answers do not contain intermediate rationale (chain-of-thought). This issue could lead to worse generalization.

Therefore, we prompt Mixtral and Qwen-72B to complete the intermediate steps. We observe that the success rate of such completion is relatively high. However, there are cases where the extracted question/answer pairs contain serious formatting issues, which pose challenges for the following refinement step. Besides these issues, we also observe that LLMs can sometimes modify the intention of the originally extracted content, causing hallucinations.

To quantify the error percentages, we randomly sample 50 refined QA examples and ask the human annotators to compare whether the refined examples are correct and significantly better than the extracted ones in terms of format and intermediate solutions. As we can see from Figure 6, 78% examples have been improved after refinement and only 10% examples introduce hallucinations after refinement. Overall, our case study reveals that the harvested instruction tuning dataset is generally accurate with a low error rate.

## 6 Conclusion

In this paper, we argue that the web corpus contains a vast amount of high-quality instruction data across various domains. To mine this data, we develop a three-step pipeline consisting of recall, extraction, and refinement steps. Through this pipeline, we harvest WebInstruct, a total of 10M diverse, high-quality instruction-response pairs and train language models. Our experiments demonstrate that MammoTH2 exhibits significantly enhanced science reasoning abilities compared to the baseline models. Our work showcases the potential of harnessing the vast amount of instruction data in the web corpus to democratize the development of LLMs with enhanced reasoning capabilities.

  
**Setting** & **Model** & **\#Tokens** & **TheorQA** & **MATH** & **GSM8K** & **MMLU-S** & **BBH** & **ARC-C** & **AVG** \\   &  &  &  &  &  &  &  &  &  &  &  \\
**Base Model** & DS Coder v1.5 7B & - & 18.3 & 22.3 & 47.9 & 47.0 & 53.5 & 62.4 & 41.9 \\ + (a) & - & 28B & 23.5 & 30.3 & 60.3 & 53.3 & 55.5 & 69.4 & 48.7 \\ + (a) + (b) + (c) & MammoTH2-DS & 10B & **27.8** & 33.8 & 64.0 & **56.9** & 58.5 & **72.8** & **52.3** \\ + (a’) & DS Math Base & 500B & 25.3 & **34.0** & **64.2** & 56.4 & **59.5** & 67.8 & 51.2 \\   &  &  &  &  &  &  &  &  &  \\ + (d) & - & 2B & 23.5 & 37.2 & 77.5 & 52.0 & 59.8 & 66.9 & 52.8 \\ + (a) + (d) & - & 30B & 27.2 & 39.2 & 79.2 & 55.6 & 60.3 & 71.5 & 55.5 \\ + (a) + (b) + (d) & 9B & 27.3 & 38.6 & 78.5 & 54.2 & 60.5 & 70.4 & 54.9 \\ + (a) + (b) + (c) + (d) & MammoTH2-DS-Plus & 12B & **30.1** & 43.8 & 80.1 & **59.5** & **61.0** & **73.2** & **58.0** \\ + (a’) + (d’) & DS Math Instruct & 501B & 23.7 & **44.3** & **82.9** & 59.3 & 55.4 & 70.1 & 56.0 \\   

Table 6: Summary of ablation study results using different training data. The **Base Model** is _DeepSeek Coder V1.5 TB_.

Figure 6: Quality distribution of 50 sampled refined QA examples.