# Combining Behaviors with the

Successor Features Keyboard

 Wilka Carvalho\({}^{*,1}\)  Andre Saraiva\({}^{2}\)  Angelos Filos\({}^{2}\)

Andrew Kyle Lampinen\({}^{2}\)  Loic Matthey\({}^{2}\)  Richard L. Lewis\({}^{3}\)

**Honglak Lee\({}^{3}\)  Satinder Singh\({}^{2,3}\)  Danilo J. Rezende\({}^{2}\)  Daniel Zoran\({}^{2}\)**

\({}^{1}\)Harvard University \({}^{2}\)Google DeepMind \({}^{3}\)University of Michigan

Contact author: rocarvalho@g.harvard.edu. Work done during internship.

###### Abstract

The Option Keyboard (OK) was recently proposed as a method for transferring behavioral knowledge across tasks. OK transfers knowledge by adaptively combining subsets of known behaviors using Successor Features (SFs) and Generalized Policy Improvement (GPI). However, it relies on hand-designed state-features and task encodings which are cumbersome to design for every new environment. In this work, we propose the "Successor Features Keyboard" (SFK), which enables transfer with _discovered_ state-features and task encodings. To enable discovery, we propose the "Categorical Successor Feature Approximator" (CSFA), a novel learning algorithm for estimating SFs while jointly discovering state-features and task encodings. With SFK and CSFA, we achieve the first demonstration of transfer with SFs in a challenging 3D environment where all the necessary representations are discovered. We first compare CSFA against other methods for approximating SFs and show that only CSFA discovers representations compatible with SF&GPI at this scale. We then compare SFK against transfer learning baselines and show that it transfers most quickly to long-horizon tasks.

## 1 Introduction

Consider a household robot that learns tasks for interacting with objects such as finding and moving them around. When this robot is deployed to a house and needs to perform combinations of these tasks, collecting data for reinforcement learning (RL) will be expensive. Thus, ideally this robot can effectively _transfer_ its knowledge to efficiently learn these novel tasks with minimal interactions in the environment. We study this form of transfer in Deep RL.

One promising method for transfer is the Option Keyboard (OK), which transfers to new tasks by adaptively combining subsets of known behaviors. OK combines known behaviors by leveraging Successor Features (SFs) and Generalized Policy Improvement (GPI) . SFs are predictive

Figure 1: **Diagram of transfer with the Successor Features Keyboard (SFK)**. SFK uses SF-based “keys” to represent behaviors by how much of some feature (known as a “cumulant”) they obtain. SFK _dynamically_ selects from behaviors with GPI by generating preference “queries” over these features. The “value” of each behavior is then computed with a dot-product and the highest value is used to generate behavior. Prior work has **hand-designed** the features that define queries and SFs. Here, we study the problem of transferring with SFK while discovering all necessary representations.

representations for behaviors. They represent behaviors with estimates of how much state-features (known as "cumulants") will be experienced given that behavior. GPI can be thought of as a query-key-value system that, given feature preferences, selects from behaviors that obtain those features.

While OK is a promising transfer method, it relies on hand-designed representations for the "cumulants" and task feature-preferences (i.e. task encodings). There is work that has discovered either cumulants or task encodings . However, either (a) they only showed transfer with SF&GPI using a _fixed_ (not dynamic) transfer query, (b) they only demonstrated results in simple grid-worlds where an agent combines short-horizon "goto" tasks (e.g. goto A and B), or (c) they leveraged _separate_ networks for the SFs of each task-policy and for each task-encoding. This means parameter count scales with the number of tasks and limits representation re-use across tasks.

Ideally, we can transfer with a dynamic query while leveraging _discovered_ representations for cumulants and feature preferences. Further, a transfer method should scale to sparse-reward long-horizon tasks such as those found in complex 3D environments. However, jointly discovering cumulants and feature preferences while estimating SFs is challenging in this setting. First, jointly learning cumulants and SFs involves estimating boot-strapped returns over a non-stationary target with high-variance and a shifting magnitude. To maximize knowledge sharing across tasks, we can _share_ our SF-estimator and task encoder across tasks. However, no work has yet achieved transfer results doing so.

To transfer with a dynamic query that leverages discovered representations while sharing functions across tasks, we propose two novel methods, the _Successor Features Keyboard (SFK)_ and the _Categorical Successor Feature Approximator (CSFA)_. We present a high-level overview of in SFK in Figure1. SFK leverages CSFA to learn SF-estimates over discovered cumulants and task-preferences in a pretraining phase. Afterwards, in a finetuning phase, SFK learns to generate dynamic queries which are linear combinations of CSFA-discovered task-preferences. CSFA addresses challenges with estimating a non-stationary return by estimating SFs with a variant of the categorical two-hot representation introduced by MuZero . We discretize the space of cumulant-return values into bins and learns a probability mass function (pmf) over them. Modelling cumulant-returns with a pmf is more robust to outliers and can better accomodate a shifting magnitude. In contrast, standard methods estimate SFs with regression of a single point-estimate  which is susceptible to outliers and varying scales .

We study SFK and CSFA in Playroom , a challenging 3D environment with high-dimensional pixel observations and long-horizon tasks defined by sparse rewards. Prior work on transfer with SF&GPI has mainly focused on transfer in simpler 2D environments . While Borsa et al.  studied transfer in a 3D environment, they relied on hand-designed cumulants and task encodings  and only transferred to combinations of "Goto" tasks. We discover cumulants and task encodings while studying transfer to combinations of long-horizon, sparse-reward "Place near" tasks.

**Contributions**. (1) We propose the Successor Features Keyboard, a novel method that transfers with SF&GPI using a dynamic query, discovered representations, and a task encoder and SF-approximator that are shared across tasks. (2) To enable discovery when sharing a task encoder and SF-approximator cross tasks, we propose a novel learning algorithm, the Categorical Successor Feature Approximator. (3) We present the first demonstration of transfer with successor features in a complex 3D environment where all the necessary representations are discovered.

## 2 Related work on Transfer in Deep RL

Several avenues exist to transfer knowledge in Deep RL. We can transfer an agent's representations (how they represent situations), their control policy (how they act in situations), or their value function (how they evaluate situations). To transfer representations, one can learn a mapping from source domains to target domains , learn disentangled representations , or learn a modular architecture . To transfer a control policy, some methods _distill_ knowledge from a source policy to a target policy , others exploit _policy improvement_, and a third set transfer low-level policies by learning a meta-controller . Finally, to transfer value functions, some approaches learn universal value functions  and others exploit SFs . Below we review approaches most closely related to ours.

**Transferring policies**. One strategy to transfer a policy is to leverage multi-task RL (MTRL) training where you learn and transfer a goal-conditioned policy . Another stratgy is to distill knowledge from one policy to another, as Distral does . Distral works by learning two policies: one goal-conditioned policy and another goal-agnostic "centroid policy". The action-likelihoods of each policy are then distilled into the other by minimizing KL-divergences. Distral has strong performance in multi-task settings but it relies on the utility of a "centroid" policy for sharing knowledge across tasks. When we transfer to _longer_ horizon tasks with sparse rewards, neither MTRL nor Distral may provide a policy with good jumpstart performance . In this work, we study jumpstart performance and exploit successor features (SFs) with generalized policy improvement (GPI) .

**Successor Features** are useful because they enable computing of action-values for new task encodings . When combined with GPI, prior work has shown strong zero-shot or few-shot transfer to combinations of tasks. To accomplish this, GPI can evaluate known SFs with a "query" transfer task encoding. SF&GPI relies on "cumulants" (which SFs predict returns over) and task encodings that respect a dot-product relationship. Prior work has had one of three limitations. Some work has discovered representations but only shown transfer with a _static_ transfer query and did not share SF-estimators or task-encoders across tasks . Other work has shared SF-estimators across tasks but exploited hand-designed task encodings with static GPI queries . The Option Keyboard  transferred using a _dynamic_ query; however, they hand-designed cumulants and task encodings and didn't share functions across tasks. In this work, we present the Successor Features Keyboard, where we address all three limitations. We transfer with a dynamic query, discover cumulants and task encodings, and learn both a task-encoder and SF-estimator that are shared across tasks. Additionally, prior work has only studied transfer to combinations of short-horizon "go to" tasks whereas we include longer horizon "place" tasks and do so in a 3D environment. We summarize these differences in Table1.

## 3 Background

We study an RL agent's ability to transfer knowledge from a set of \(n_{}\) training tasks \(_{}=\{_{1},,_{n_{}}\}\) to a set of \(n^{}_{}\) transfer tasks \(_{}=\{^{}_{1},,^{}_{n^{}_{}}\}\). During training, tasks are sampled from distribution \(p_{}(_{})\). At transfer, tasks are sampled from distribution \(p_{}(_{})\). Each task is specified as a Partially Observable Markov Decision Process (POMDP, ), \(_{i}=^{e},,,R,p,f_{x}\), where \(^{e}\), \(\) and \(\) are the environment state, action, and observation spaces. Rewards are parameterized by a task description \(\), i.e. \(r^{}_{t+1}=R(s^{e}_{t},a_{t},s^{e}_{t+1},)\) is the reward for transition \((s^{e}_{t},a_{t},s^{e}_{t+1})\). When the agent takes action \(a_{t}\) in state \(s^{e}_{t}\), \(s^{e}_{t+1}\) is sampled according to \(p(|s^{e}_{t},a_{t})\), an observation \(x_{t+1}\) is generated via \(f_{x}(s^{e}_{t+1})\), and the agent gets reward \(r^{}_{t+1}\). We assume the agent learns a recurrent state function that maps histories to agent state representations, \(s_{t}=s_{}(x_{t},s_{t-1},a_{t-1})\). Given this learned state, we aim to obtain a behavior policy \((s_{t},)\) that maximises the expected reward when taking an action \(a_{t}\) in state \(s_{t}\): i.e. that maximizes

  Method & disc. \(\) & disc. \(w\) & query & share \(w_{}\) & share \(_{}\) & 3D & transfer \\  Barreto et al.  & ✓ & ✓ & static & ✗ & ✗ & ✗ & goto-n \\  Zhu et al. \({}^{}\) & ✓ & ✓ & static & ✗ & ✗ & ✓ & goto-n \\  Barreto et al.  & ✓ & ✗ & static & ✗ & ✗ & ✓ & goto-c \\  Filos et al. \({}^{}\) & ✓ & ✓ & static & ✗ & ✗ & ✗ & goto-c \\  Borsa et al.  & ✗ & ✗ & static & ✗ & ✓ & ✓ & goto-c \\  Carvalho et al.  & ✓ & ✗ & static & ✗ & ✓ & ✗ & goto-c \\  Barreto et al.  & ✗ & ✗ & **dynamic** & ✗ & ✗ & ✗ & goto-c \\  SFK (ours) & ✓ & ✓ & **dynamic** & ✓ & ✓ & ✓ & **place-c** \\  

Table 1: **Related methods for transfer with SF&GFI**. SFK is the first method to transfer with a dynamic query, discover cumulants \(\) and task encodings \(w\), while sharing a task encoder \(w_{}\) and SF approximator \(_{}\) across tasks. Each of these is important to transfer with SF&GPI in a large-scale multi-task setting. Together, this enables the first SF method to transfer to combinations of long-horizon place tasks in a 3D environment with discovered \(\) and \(w\). Note: \(\) refers to methods which learn from demonstration data, which we do not. goto-n is “goto new goal state”, goto-c is “goto object combo”, and place-c is “place object combo”.

\(Q_{t}^{,}=Q^{,}(s_{t},a_{t})=_{}[_{t=0}^{ }^{t}r_{t+1}^{}]\). We study agents that continue learning during transfer and aim to maximize _jump-start performance_.

**Transfer with SF&GPI** requires two things: (1) state-features known as "cumulants" \(_{t+1}=_{}(s_{t},a_{t},s_{t+1})\), which are useful "descriptions" of a state-transition, and (2) a task encoding \(w=w_{}()\), which define "preferences" over said transitions. Reward is then defined as \(r_{t}^{}=_{t}^{}w\). Successor Features \(^{}\) are then _value functions_ that describe the discounted sum of future \(\) that will be experienced under policy \(\):

\[_{t}^{}=^{}(s_{t},a_{t})=_{}[_{i=0}^{ }^{i}_{t+i+1}]\] (1)

Given \(_{t}^{}\), we can obtain action-values for \(r_{t}^{}\) as \(Q_{t}^{,}=_{t}^{}w\).

The linear decomposition of \(Q^{,}\) is interesting because it can be exploited to _re-evaluate_\(^{}\) for new tasks with GPI. Assume we have learned SFs \(\{^{_{i}}(s,a)\}_{i=1}^{n_{}}\) for tasks \(_{}\). Given a new task \(^{}\), we can obtain a new policy \((s_{t};^{})\) with GPI in two steps: (1) re-evaluate each SF with the task's _query_ encoding to obtain new Q-values (2) select an action using the highest Q-value. In summary,

\[(s_{t},^{})*{arg\,max}_{a}_ {i\{1,,n_{}\}}\{_{t}^{_{i}}w_{}(^{ })\}=*{arg\,max}_{a}_{i\{1,,n _{}\}}\{Q_{t}^{_{i},^{}}\},\] (2)

where \(w_{}(^{})\) is a **static transfer query** for transfer task \(^{}\).

**Option Keyboard**. One benefit of equation 2 is that it enables transfer to _linear combinations_ of training task encodings. However, it has two limitations. First, the feature "preferences" \(w_{}(^{})\) are fixed across time. When \(^{}\) is a complex task (e.g. avoiding an object at some time-points but going towards it at others), we may want something that is state-dependent. Second, if we learn \(w_{}\) with a nonlinear function approximator such as a neural network, there is no guarreneted that \(w_{}(^{})\) is in the span of training task encodings. The "Option Keyboard" [1; 2] can circumvent these issues by learning a transfer _policy_ that maps states and tasks to a **dynamic transfer query**\(g_{}(s,^{})\):

\[(s_{t},^{})*{arg\,max}_{a}_ {i\{1,,n_{}\}}\{_{t}^{_{i}}g_{}(s_{t},^ {})\}\] (3)

**Learning**. In the most general setting, we learn \(_{},_{},w_{}\) and \(g_{}\) from experience. Rewards \(r^{}\) and their task encodings \(w=w_{}()\) reference deterministic task policies \(_{w}\) that maximize them. Borsa et al.  showed that this allows us to parameterize an SF-approximator for a policy \(_{w}\) with an encoding of the task that defines it, i.e. we can approximate \(^{_{w}}\) with \(_{}(s,a,w)\). This is known as a _Universal_ Successor Feature Approximator (USFA) and can be learned with TD-learning with cumulants as pseudo-rewards. To discover \(_{}\) and \(w_{}\), we can match their dot-product to the experienced reward . Defining \(_{t+1}=_{}(s_{t},a_{t},s_{t+1})\), we summarize this as:

\[_{}=||_{t+1}+_{}(s_{t+1},a_{t+1},w)-_{ }(s_{t},a_{t},w)||,_{r}=||r^{}-_{t+1}^{} w||\] (4)

No prior work has _jointly_ learned a task encoder \(w_{}\) and USFA \(_{}(s,a,w)\) while discovering cumulants \(_{}\). In this work, we introduce the Successor Features Keyboard to address these limitations to enable transfer with SF&GPI and discovered representations in a large-scale 3D environment.

## 4 Method

We propose a novel method for transfer, the _Successor Features Keyboard (SFK)_, where necessary representations are discovered. To discover representations, we propose the _Categorical Successor Feature Approximator_ for jointly learning SFs \(_{}\), cumulants \(_{}\), and task encodings \(w_{}\). The rest of this section is structured as follows. In SS4.1, we describe CSFA and how to leverage it for pretraining. Finally, in SS4.2, we describe how to leverage SFK for transfer. We provide background in SSA.

### Pretraining with a Categorical Successor Feature Approximator

We propose a novel learning algorithm, _Categorical Successor Feature Approximator (CSFA)_, composed of a novel architecture, shown in Figure 2, and a novel learning objective (equation 6). **Challenge**: when jointly learning a Universal SF-approximator \(_{}\) and a cumulant-network \(_{}\)for long-horizon tasks, \(_{}\) needs to fit \(_{}\)-generated returns that are potentially high-variance, non-stationary, and changing in magnitude . CSFA addresses this challenge by modelling SFs with a probability mass function (pmf) over a discretized range of continous values. CSFA then fits this data by leveraging a categorical cross-entropy loss, enabling our estimator to give probability mass to different ranges of values. This is in contrast to prior work that models SFs with a point-estimate that is fit via regression . Leveraging a point-estimate can be unstable for modelling a non-stationary target with changing magnitude  (we show evidence in SSC.1).

**Architecture**. CSFA represents SFs with a pmf \((,^{_{k}})\), where \(=\{b_{1},,b_{M}\}\) are an apriori defined set of _bin values_ and \(^{_{k}}\) are probabilities for each bin value. Specifically, CSFA estimates an n-dimensional SF-vector \(^{_{w}}(s,a)^{n}\) with \(_{}(s,a,w)\) and represents the \(k\)-th SF dimension as \(^{k}_{}(s,a,w)=_{m=1}^{M}p_{m}^{_{k}}b_{m}\). At each time-step, we update a state function \(s_{}\) with an encoding of the current observation \(z_{t}=f^{}(x_{t})\), the previous action \(a_{t-1}\), and the previous state representation \(s_{t-1}\), i.e. \(s_{t}=s_{}(z_{t},a_{t-1},s_{t-1})\). Each set of probabilities is computed as \(^{_{k}}=(l_{}(s_{t},w,e_{k}))\), where \(e_{k}\) is an embedding for the current SF dimension. In summary,

\[_{}(s,a,w)=\{^{k}_{}(s,a,w)\}_{k=1}^{n}^{k}_{ }(s,a,w)=_{m=1}^{M}p_{m}^{_{k}}b_{m}^{_{k}}= (l_{}(s,w,e_{k}))\] (5)

We provide a diagram of representing an SF with a pmf in Figure 3. Using a pmf allows us to re-use the same network to estimate SFs across cumulants. We hypothesize that this provides a stronger learning signal to stabilize learning across more challenging return estimates. We show evidence in Figure 6.

**Learning objective**. We learn to generate behavior by employing a variant of Q-learning. In particular, we generate Q-values using learned SFs, and use these Q-values to create targets for both estimating Q-values and for estimating SFs. For both, targets correspond to the action which

Figure 3: **Diagram of how successor features can be computed with a probability mass function.**

Figure 2: **Left: Categorical Successor Feature Approximator (CSFA) estimates SFs for a task encoding \(w\) with a structured, categorical network. Right: Successor Features keyboard (SFK) transfers by dynamically selecting combinations of known task behaviors \(_{}=\{w_{1},,w_{n}\}\). SFK accomplishes this by learning a policy \(q_{}(s,w_{})\) that generates linear combinations of known task encodings \(_{}\). SFK then leverages GPI to compute \(Q\)-values for known behaviors, \(Q^{w_{i},g}=(s,a,w_{i})^{}g_{}(s,w_{})\) and acts using the highest \(Q\)-value.**

maximized future Q-estimates. We learn SFs with a categorical cross-entropy loss where we obtain targets from scalars with the \(()\) operator. Intuitively, this represents a scalar with likelihoods across the two closest bins. In summary,

\[y_{t}^{Q} =r_{t+1}+_{^{}}(s_{t+1},a^{*},w_{^ {}}())^{}w_{^{}}()_{Q} =||_{}(s_{t},a_{t},()})^{} ()}-y_{t}^{Q}||^{2}\] (6) \[y_{t}^{_{k}} =_{^{}}^{k}(s_{t},a_{t})+_{^ {}}^{k}(s_{t+1},a^{*},w_{^{}}())_{}^{} =_{k}(^{_{k}})^{}(^{_{k}}})\] (7)

\(a^{*}=_{a}(s_{t+1},a,w)^{}w\), where \(^{_{k}}=(l_{}(s_{t},,e_{k})\), \(\) is a stop-gradient operation on \(w\). Like prior work , we mitigate non-stationary in the return-targets \(y_{t}^{Q}\) and \(y_{t}^{_{k}}\) by having target parameters \(^{}\) that update at a slower rate than \(\). The overall loss is \(L=_{Q}_{Q}+_{}_{}^{}+ _{r}_{r}\).

**Important implementation details**. Estimating SFs while jointly learning a cumulant function and task encoder can be unstable in practice . No work has jointly learned all three functions while sharing them across tasks. Here, we detail important implementation challenges that prohibited us from discovering representations that worked with SF&GPI in our large-scale setting. **D1**. We found that passing gradients to \(w_{}\) through \(_{}(s,a,w)\) or through \(_{}(s,a,w)^{}w\) during Q-learning lead to dimensional collapse  and induces a small angle between task encodings (see SSC.1). We hypothesize that this makes \(_{}(s,a,w)\) unstable and manifests as poor GPI performance. **D2**. When \(||w||\) is large, it can make \(_{}(s,a,w)\) unstable. We hypothesize that this is because it magnifies errors due to the SF-approximation error that loosen previously found bounds on GPI performance [8; 5]. We discuss this in more detail in SSB. To mitigate this, we bound \(w\) by enforcing it lie on a unit sphere, i.e. \(w=()}{||w_{}()||}\). This makes the SF-error be consistent across tasks.

### Transfer with the Successor Features Keyboard

The original Option Keyboard (equation 3) learned a policy that mapped states \(s_{t}\) to queries \(w\), \(w^{}=g(s_{t},^{})\). However, they used a hand-designed \(w_{}\) and thus hand-designed space for \(w\). In our setting, we learn \(w_{}\). However, GPI performance is bound by the distance of a transfer query \(w^{}\) to known preference vectors \(w\) (we discuss this in more detail in SSB). Thus, we want to sample \(w^{}\) that are not too "far" from known \(w\). To accomplish this, we shift from learning a policy that samples preference vectors to a policy that samples _coefficients_\(\{_{i}\}_{i=1}^{n_{s}}\) for known preference vectors \(_{}=\{w_{1},,w_{n_{s}}\}\). The GPI query is then computed as a weighted sum. Below we describe this policy in more detail along with how to learn it.

At each time-step, the agent uses a pretrained CSFA to compute SFs \(^{_{w}}(s_{t},a)\) for \(w_{i}_{}\):

\[z_{t}=f_{}(x_{t}) s_{t}=s_{}(z_{t},a_{t-1},s_{t-1})\{ ^{_{w_{i}}}=_{}(s_{t},a,w_{i})\}_{w_{i}_{ }}\] (8)

In our experiments, we freeze the observation encoder, state function, and task encoder and learn a new state function and task encoder at transfer time with parameters \(_{}\). Given a new state representation \(s_{t}^{}\), we sample coefficients independently:

\[w^{}_{t}=g_{_{}}(s_{t}^{},w_{_{}}(^{}))=_{i=1}^{n_{}}_{t}^{i}w_{i}_{t}^{i} p_{_{}}(^{i}|s_{t}^{},w_{ _{}}(^{}))\] (9)

We find that a Bernoulli distribution performs well. We learn this \(\)-coefficient policy with policy gradients  by performing gradient ascent with gradients \(A_{t} p_{}( s_{t},w)\), where \(A_{t}=R_{t}-V_{_{}}(s_{t}^{})\) is the "advantage" of the coefficient \(\) chosen at time \(t\). Here, \(R_{t}=_{i=1}^{}r_{t+i+1}\) is the experienced return and \(V_{_{}}(s_{t}^{})\) is the predicted return. Optimizing this increases the likelihood of choosing coefficients in proportion to \(A_{t}\).

## 5 Experiments

We study transfer with sparse-reward long-horizon tasks in the complex 3D Playroom environment . To transfer behavioral knowledge, we propose SFK for combining behaviors with SF&GPI, and CSFA for discovering the necessary representations. In SS5.1, we study the utility of CSFA for discovering representations that are compatible with SF&GPI. In SS5.2, we study the utility of SFK for transferring to sparse-reward long-horizon tasks.

**Environment setup**. We conduct our experiments in the 3D playroom environment. **Observations** are partial and egocentric pixel-based images. The agent gets no other information. **Actions**. The agent can rotate its body and look up or down. To pick up an object it must move its point of selection on the screen. When it picks up an object, it must continuously _hold it_ in order to move it elsewhere. To accomplish this, the agent has \(46\) actions. **Training tasks**. The agent experiences \(n_{}=32\) training tasks \(_{}=\{_{1},,_{n_{}}\}\) composed of "Find A" and "Place A near B". \(|A|=8\) and \(|B|=3\). All tasks provide a reward of \(1\) upon-completion. We provide more details in SSF.

### Evaluating the utility of discovered representations for SF&GPI

Our first experiments are a _sanity check_ for the utility of discovered representations for SF&GPI. We train and evaluate agents on the same set of tasks. However, during evaluation the agent has access to all training tasks and must select the appropriate one with SF&GPI; given task encodings \(_{}=\{w_{1},,w_{n_{}}\}\), the agent acts according to policy \((s,w_{i})=*{arg\,max}_{a}_{w_{k}}\{(s,a,w_{k})^{}w _{i}\}\). When \(w_{k}=w_{i}\), \((s,w_{i})=*{arg\,max}_{a}Q(s,a,w_{i})\). This will fail if the agent hasn't learned representations that support GPI. If an agent cannot perform GPI on training tasks, then it will probably fail with novel transfer tasks. We expand on this in SSB. **Metric**. We evaluate agents with average success rate. **Challenges**. Most prior work has leveraged SF&GPI for combining "Find tasks" where an agent simply navigates to objects [4; 8; 6]. We add a significantly _longer horizon_ "Place Near" task where the agent must _select_ an object and _hold_ it as it moves it to another object. This tests the utility of discovered representations for learning SFs that enable SF&GPI over long horizons.

**Research questions. Q1**. How does CSFA compare against baseline methods that share their SF estimatar \(\) across tasks while discovering \(\) and \(w\)? **Q2**. Is each piece of CSFA necessary?

**Baselines**. (1) **Universal Successor Feature Approximators (USFA)** is the only method that shares an SF estimator across tasks and has shown results in a 3D environment. (2) **Modular Successor Feature Approximators (MSFA)** showed that leveraging modules improved SF estimation and enabled cumulant discovery. However, they did not discover task encodings and only showed results in simple grid-worlds. **Both** baselines estimate SFs with point-estimates. Comparing to them tests (a) the utility of our categorical representation and (b) CSFA's ability to discover _both_ cumulants and task encodings that enable GPI in a large-scale setting.

**CSFA discovers SFs compatible with SF&GPI while baseline methods cannot**. For fair comparison, we train each baseline with the same learning algorithm (except for SF-losses), enforce \(w\) lie on a unit sphere, and stop gradients from Q-lerning. We found that large-capacity networks were needed for discovering \(\). In particular, we parameterized \(_{}\) with a 8-layer residual network (ResNet)  for all methods. One important difference is that we leverage a set of ResNet modules for MSFA since Carvalho et al.  showed that modules facilitate cumulant discovery. Figure 5 shows that USFA and MSFA can perform GPI for Find tasks; however, neither learn place tasks in our computational budget. Given that we controlled for how \(\) and \(w\) are learned, we hypothesize that the key limitation of USFA and MSFA is their reliance on scalar SF estimates.

**A categorical representation is necessary**. One difference between MSFA/USFA and CSFA is that CSFA shares an estimator parameters across individual SFs. Figure 6 shows that when CSFA shares an estimator but produces scalar estimates (CSFA - no categorical), GPI performance degrades

Figure 4: **Playroom**.

Figure 5: **CSFA discovers representations compatible with GPI across short and long task-horizons**. Both USFA and MSFA degrade in performance on Find tasks when paired with GPI. Neither are able to learn our longer horizon Place task. We hypothesize that this is because they approximate SFs with a point-estimate. (4 seeds)_below_ MSFA/USFA. We hypothesize that a network producing point-estimates has trouble estimating returns for cumulants of varying magnitude. In SSC we show evidence that CSFA has more stable SF-errors compared to scalar methods.

**Sharing our estimator across cumulants is necessary**. If we keep our categorical representation but don't share it across cumulants (CSFA - independent), GPI performance degrades on our long-horizon place near task. **Stopping gradients from Q-learning is necessary**. Interestingly, when we pass gradients to the task-encoder from Q-learning (CSFA - no stop grad), we can get perfect train performance on place tasks but highly unstable GPI performance. We found that passing gradients leads to dimensional collapse  (see SSC). We hypothesize that this makes \(_{}\) unstable. Likewise, if we don't bound tasks (CSFA - no \(||w||\)), we find degraded GPI performance compared to training but for even simpler tasks. While other methods may work, enforcing \(w\) lie on a unit-sphere is a simple solution. We present full results for these plots in SSD.

### Transferring to combinations of long horizon tasks

Our second experiments test the utility of SFK for transferring to combinations of long horizon, sparse-reward tasks. **Transfer tasks** are conjunctions of known tasks. Subtasks can be completed in any order but reward is only provided at task-completion. Find tasks contribute a reward of \(1\) and place tasks contribute a reward of \(2\). We provide more details in the SSF.

**Research questions**. **Q3**. How do we compare against baseline transfer methods? **Q4**. How important is it to use CSFA and to sample coefficents over known task encodings?

**Baselines**. (1) **Multitask RL (MTRL)**. We train an Impala  agent on all training tasks and see how this enables faster learning for our transfer tasks. We select Impala because it is well studied in the Playroom environment [31; 32; 10]. (2) **Distral** is a common transfer learning method which learns a centroid policy that is distilled to training task policies. Comparing against MTRL and Distral tests the utility of combining behaviors with SF&GPI.

**Q3: SFK has better jumpstart performance**. Figure 7 shows that all methods get similar performance by 500 million frames. However, for longer task combinations (Put 2 times or 3 times), SFK gets to similar performance with far fewer frames. When we remove our curriculum (Figure 8)

Figure 6: **CSFA Ablations. Left**. Not bounding \(||w||\) (**D2**) degrades GPI performance. Interestingly, removing our categorical representation (CSFA - no categorical) gets perfect training performance but terrible GPI performance. **Right**. Passing gradients to the task-encoder from the SF-approximator leads to unstable GPI performance (**D1**) despite good training performance. If CSFA does not share its SF-estimator, it seems to learn more slowly. Thankfully, each addition is simple and together enables GPI with long-horizon place tasks. (3 seeds)

Figure 7: **SFK transfer most quickly to longer horizon tasks**. Distral and MTRL learn at about the same speed, though Distral is slightly faster. For put x 3, where the agent needs to do 3 place tasks (A near B, C near D, and E near F) SFK learns with 100+ fewer samples. (9 seeds)

this gap further increases. No method does well for our longest task (Put x 4) which involves 8 objects. We conjecture that one challenge that methods face is holding an object over prolonged periods of time. If the agent selects the wrong action, it will drop the object it's holding. This may make it challenging when there's some noise from either (a) centroid task, as with Distral, or (b) SF&GPI as with SFK. Despite not reaching optimal performance, SFK provides a good starting point for long-horizon tasks.

**Q4: Leveraging CSFA and sampling coefficents over known task encodings is critical to jumpstart performance.** Figure 9 shows us that if we don't leverage CSFA to estimate SFs and instead use USFA, SFK fails to transfer. We hypothesize that this is because of the USFA uses point-estimates for SFs, which shows poor GPI on training tasks (see Figure 5) so its not surprising it fails on novel transfer tasks. Directly sampling from the encoding space does not transfer as quickly. Empirically, we find that learned task encodings have a high cosine similarity, indicating that they occupy a small portion of the encoding space (see SSC). We hypothesize that this makes it challenging to directly sample in this embedding space to produce meaningful behavior.

## 6 Discussion and conclusion

We have presented SFK, a novel method for transfer that adaptively combines known behaviors using SF&GPI. To discover representations that are compatible with SF&GPI, SFK estimates SFs with a novel algorithm, CSFA. CSFA constitutes both a novel architecture (of the same name) which approximates SFs with a pmf over a discretized continous values and a novel learning objective which estimates SFs for discovered cumulants with a categorical cross-entropy loss.

We first showed that CSFA is able to discover SF&GPI-compatible cumulants and task encodings for long-horizon sparse-reward tasks in the 3D Playroom environment (SS5.1). We compared CSFA to other methods which share their approximator across tasks: (1) USFA, which showed results in a 3D environment but hand-designed representations, and (2) MSFA, which discovered cumulants but did so in gridworlds with hand-designed task encodings. We additionally compared against an ablation which removed our categorical representation and or did not share it across cumulants. Our results show that a categorical representation over discretized values can better handle estimating SF-returns when discovering cumulants and task encodings for long-horizon sparse-reward tasks.

We built on these results for our second set of experiments and showed that SFK provides strong jumpstart performance for transfer to combinations of training tasks (SS5.2). We compared SFK to (1) Multitask RL (MTRL) pretraining and finetuning, and (2) Distral, which distills knowledge back and forth between a task-specific policy and a "centroid" policy. Our results showed that, for long-horizon tasks, SFK could transfer with 100 million+ fewer samples when a curriculum was present, and 200 million+ fewer samples when no curriculum was present. We found that simply using a Bernoulli distribution for sampling task coefficents performed well because it facilitates exploiting SF&GPI. SF&GPI enable transfer to linear combinations of task encodings. By leveraging a Bernoulli distribution, the dynamic transfer query was simply a linear combination of learned task encodings. This is precisely the type of transfer task encoding that SF&GPI has been shown to work well with.

Figure 8: **SFK transfers most quickly with no curriculum of shorter tasks. SFK and Distral reach the same performance but SFK is 200+ million frames faster. MTRL fails to transfer here. For “Put x 4”, SFK is suboptimal but achieves some success. (8 seeds)**

Figure 9: **SFK ablation**. SFK fails to transfer if we remove CSFA. We find that sampling directly in task-encoding space instead of sampling binary coefficents is both slower to learn and has higher performance variance. We hypothesize that this is due to concentration in the task encoding space.

**Limitations**. While we demonstrated discovery of cumulants and task encodings that enabled transfer with a dynamic SF&GPI query, we relied on generating queries as weighted sums of known task encodings. A more general method would directly sample queries from the task encoding space. We found that the task encodings we discovered were fairly concentrated. Future work may mitigate this with contrastive learning . This respects the dot-product relationship of cumulants and task encodings while enforcing they be spread in latent space. Finally, while we demonstrated good jumpstart performance for long-horizon tasks, we did not reach optimal performance in our sample budget. Despite building on the Option Keyboard, we did not employ options as we sampled a new query at each time-step. Future work may improve performance by sampling queries more sparsely and by implementing a latent "initiation set" .

This paper did not study the upper limit on the number of main tasks our method can handle. However, it was a significant increase in complexity from prior work . For example, if one combines 2 "go to"tasks, this leads to \(C(4,2)=6\) combinations. We have \(8\) find tasks and \(24\) place near tasks, where \(C(n,k)\) denotes \(n\) choose \(k\) combinations. If we combine just \(2\) place near tasks, this leads \(C(24,2)=276\) possible transfer combinations. We are optimistic that one can scale the capacity of a sufficiently expressive task encoder with the number and complexity of tasks being learned.

**Conclusion**. These results present the first demontration of transfer with the Option Keyboard (SF&GPI with a dynamic query) when all representations are discovered and when the task-encoder and SF-approximator are _shared_ across tasks. This may enable other methods that leverage SFs in a multi-task settings to leverage discovered representations (e.g. for exploration  or for multi-agent RL ). More broadly, this work may also empower neuroscience theories that leverage SFs as cognitive theories to better incorporate discovered representations (e.g. for multitask transfer  or for learning cognitive maps ). We are hopeful that SFK and CSFA will enable SFs to be adopted more broadly with less hand-engineering.

## 7 Acknowledgements

The authors would like to thank the anonymous reviews for helpful comments in improving the paper and its accessibility. We also thank members of Google DeepMind for their helpful feedback.