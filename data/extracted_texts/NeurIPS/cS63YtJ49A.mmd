# Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy

Hancheng Ye\({}^{1,*}\), Jiakang Yuan\({}^{2,*}\), Renqiu Xia\({}^{3}\), Xiangchao Yan\({}^{1}\),

Tao Chen\({}^{2}\), Junchi Yan\({}^{3}\), Botian Shi\({}^{1}\), Bo Zhang\({}^{1,}\)

\({}^{1}\)Shanghai Artificial Intelligence Laboratory

\({}^{2}\)School of Information Science and Technology, Fudan University

\({}^{3}\)School of Artificial Intelligence, Shanghai Jiao Tong University

yehancheng@pjlab.org.cn, jkyuan22@m.fudan.edu.cn, zhangbo@pjlab.org.cn

###### Abstract

Diffusion models have recently achieved great success in the synthesis of high-quality images and videos. However, the existing denoising techniques in diffusion models are commonly based on step-by-step noise predictions, which suffers from high computation cost, resulting in a prohibitive latency for interactive applications. In this paper, we propose _AdaptiveDiffusion_ to relieve this bottleneck by adaptively reducing the noise prediction steps during the denoising process. Our method considers the potential of skipping as many noise prediction steps as possible while keeping the final denoised results identical to the original full-step ones. Specifically, the skipping strategy is guided by the _third_-order latent difference that indicates the stability between timesteps during the denoising process, which benefits the reusing of previous noise prediction results. Extensive experiments on image and video diffusion models demonstrate that our method can significantly speed up the denoising process while generating identical results to the original process, achieving up to an average \(2 5\) speedup without quality degradation. The code is available at https://github.com/UniModal4Reasoning/AdaptiveDiffusion.

## 1 Introduction

Recently, Diffusion models [1; 11; 30; 33] have emerged as a powerful tool for synthesizing high-quality images and videos. Their capability to generate realistic and detailed visual content has made them a popular choice in various applications, ranging from artistic creation to data augmentation, _e.g._, Midjourney, Sora , etc. However, the conventional denoising techniques employed in these models involve step-by-step noise predictions, which are computationally intensive and lead to significant latency, _e.g._, taking tens seconds for SDXL  to generate a high-quality image of 1024x1024 resolutions. Diffusion acceleration as an effective technique has been deeply explored recently, which mainly focuses on three paradigms: (1) reducing sampling steps [39; 13; 23; 35; 24], (2) optimizing model architecture [15; 42; 9; 27] and (3) parallelizing inference [16; 38].

Currently, most strategies are designed based on a fixed acceleration mode for all prompt data. However, in our experiments, it is observed that different prompts may require different steps of noise prediction to achieve the same content as the original denoising process, as presented in Fig. 1. Here, we compare the denoising paths using two different prompts for SDXL , both of which preserve rich content against the original full-step generation results. The _denoising path_ denotes a bool-type sequence, where each element represents whether to infer the noise from the noise prediction model. It can be observed that Prompt 2 needs more steps to generate an almost lossless image than Prompt1 (A lower LPIPS  value means more similarity between two images generated by the original strategy and our strategy). Therefore, it is necessary to explore a **prompt-adaptive** acceleration paradigm to consider the denoising diversity between different prompts.

Motivated by this observation, in this paper, we deeply dive into the skipping scheme for the noise prediction model and propose AdaptiveDiffusion, a novel approach that adaptively accelerates the generation process according to different input prompts. The fundamental concept behind AdaptiveDiffusion is to adaptively reduce the number of noise prediction steps according to different input prompts during the denoising process, and meanwhile maintain the quality of the final output. The key insight driving our method is that **the redundancy of noise prediction is highly related to the _third-order differential distribution between temporally-neighboring latents_**. This relation can be leveraged to design an effective skipping strategy, allowing us to decide when to reuse previous noise prediction results and when to proceed with new calculations. Our approach utilizes the _third-order_ latent difference to assess the redundancy of noise prediction at each timestep, reflecting our strategy's dependency on input information, thus achieving a prompt-adaptive acceleration paradigm.

Extensive experiments conducted on both image and video diffusion models demonstrate the effectiveness of AdaptiveDiffusion. The results show that our method can achieve up to a 5.6x speedup in the denoising process with better preservation quality. This improvement in acceleration quality opens up new possibilities for the application of diffusion models in real-time and interactive environments.

In summary, AdaptiveDiffusion represents a substantial advancement in adaptively efficient diffusion, offering a practical solution to the challenge of high computational costs associated with sequentially denoising techniques. The main contribution is threefold: (1) To our best knowledge, our method is the first to explore the adaptive diffusion acceleration from the step number reduction of noise predictions that makes different skipping paths for different prompts. (2) We propose a novel approach, namely AdaptiveDiffusion, which develops a plug-and-play criterion to decide whether the noise prediction should be inferred or reused from the previous noise results. (3) Extensive experiments conducted on various diffusion models [32; 33; 52; 44] and tasks [7; 21; 45; 8] demonstrate the superiority of our AdaptiveDiffusion to the existing acceleration methods in the trade-off among efficiency, performance and generalization ability.

## 2 Related Works

### Diffusion Models

Diffusion models [1; 11; 30; 33; 10] have achieved great success and served as a milestone in content generation. As a pioneer, Denoising Diffusion Probabilistic Models (DDPMs)  generate higher

Figure 1: Different prompts may have different denoising paths to generate the high-quality image. For Prompt 1, we only need 20 steps out of 50 steps for noise predictions to generate an almost lossless image, while for Prompt 2, we need 26 steps out of 50 steps to achieve an almost lossless image.

quality images compared to generative adversarial networks (GANs) [17; 6] through an iterative denoising process. To improve the efficiency of DDPM, Latent Diffusion Models (LDMs)  perform forward and reverse processes in a latent space of lower dimensionality which further evolves into Stable Diffusion (SD) family [36; 32]. Recently, video diffusion models [3; 52; 2; 44] have attracted increasing attention, especially after witnessing the success of Sora . Stable Video Diffusion (SVD)  introduces a three-stage training pipeline and obtains a video generation model with strong motion representation. 12VGen-XL  first obtains a model with multi-level feature extraction ability, then enhances the resolution and injects temporal information in the second stage. Despite the high quality achieved by diffusion models, the inherent nature of the reverse process which needs high computational cost slows down the inference process.

### Accelerating Diffusion Models

Current works accelerate diffusion models can be divided into the following aspects. _(1) Reducing Sampling Steps_[39; 23; 24; 49; 40; 25; 20; 37; 28; 29; 26]. DDIM  optimizes sampling steps by exploring a non-Markovian process. Further studies [23; 24; 49] such as DPM-Solver  propose different solvers for diffusion SDEs and ODEs to reduce sampling steps. Another way to optimize sampling steps is to train few-step diffusion models by distillation [40; 25; 20]. Among them, consistency models [40; 25] directly map noise to data to enable one-step generation. Other works [20; 37] explore progressive distillation or adversarial distillation to effectively reduce the reverse steps. Besides, some works [26; 42; 29] introduce early stop mechanism into diffusion models. _(2) Optimizing Model Architecture_[9; 19; 46; 27; 5; 47; 22; 41]. Another strategy to accelerate diffusion models is to optimize model efficiency to reduce the cost during inference. Diff-pruning  compresses diffusion models by employing Taylor expansion over pruned timesteps. DeepCache  notices the feature redundancy in the denoising process and introduces a cache mechanism to reuse pre-computed features. _(3) Parallel Inference_[16; 38; 12]. The third line lies in sampling or calculating in a parallel way. ParaDiGMS  proposes to use Picard iteration to run multiple steps in parallel. DistriFusion  introduces displaced patch parallelism by reusing the pre-computed feature maps. Compared with the existing paradigms designing a fixed acceleration mode for all input prompts, our method highlights the adaptive acceleration manner with a plug-and-play criterion based on the high-order latent differential distribution, which allows various diffusion models with different sampling schedulers to achieve significant speedup with a negligible performance drop and deployment cost.

## 3 The Proposed Approach

### Preliminary

**Reverse Denoising Process.** Diffusion models [11; 39] are designed to learn two processes with noise addition (known as the forward process) and noise reduction (known as the reverse process).

Figure 2: Denoising process of the proposed AdaptiveDiffusion: We design a third-order estimator (Refer to Sec. 3.3 for details), which can find the redundancy between neighboring timesteps, and thus, the noise prediction model can be skipped or inferred according to the indicate from the estimator, achieving the adaptive diffusion process. Note that the timestep and text information embeddings are not shown for the sake of brevity.

During the inference stage, only the reverse denoising process is adopted that starts from the Gaussian noise \(x_{T}(0,I)\) and iteratively denoises the input sample under the injected condition to get the final clean image(s) \(x_{0}\), where \(T\) is the predefined number of denoising steps. Specifically, given an intermediate noisy image \(x_{i}\) at timestep \(i\) (\(i=1,...,T\)), the noise prediction model \(_{}\) (_e.g._, UNet ) takes \(x_{i}\), timestep \(t_{i}\) and an additional condition \(c\) (_e.g._, text, image, and motion embeddings, etc) as input to approximate the noise distribution in \(x_{i}\). The update from \(x_{i}\) to \(x_{i-1}\) is determined by different samplers (_a.k.a_, schedulers) that can be generally formulated as Eq. (1):

\[x_{i-1}=f(i-1) x_{i}-g(i-1)_{}(x_{i},t_{i}), i=1, ,T,\] (1)

where \(f(i)\) and \(g(i)\) are step-related coefficients derived by specific samplers . The computation in the update process mainly involves a few element-wise additions and multiplications. Therefore, the main computation cost in the denoising process stems from the inference of noise prediction model \(_{}\).

**Step Skipping Strategy.** As proved by previous works , features between consecutive timesteps present certain similarities in distribution, thus there exists a set of redundant computations that can be skipped. Previous works usually skip either the whole update process or the partial computation within the noise prediction model at redundant timesteps. However, as visualized in Fig. 3, the latent update process in those redundant timesteps may be important to the lossless image generation. Besides, the calculation redundancy of the noise prediction model within each denoising process is still under-explored. To reduce the computation cost from the noise prediction model, we consider directly reducing the number of noise prediction steps from the original denoising process, which will be proved more effective and efficient to accelerate the generation with almost no quality degradation in Sec. 3.3. Given a certain timestep \(i\) to skip, our skipping strategy for the update process can be formulated using Eq. (2):

\[ x_{i}&=f(i) x_{i+1}-g(i) _{}(x_{i+1},t_{i+1}),\\ x_{i-1}&=f(i-1) x_{i}-g(i-1)_{ }(x_{i+1},t_{i+1}).\] (2)

### Error Estimation of the Step Skipping Strategy

To validate the effectiveness of our step-skipping strategy, we theoretically analyze the upper bound of the error between the original output and skipped output images. For simplicity, we consider skipping one step of noise prediction, _e.g._, the \(i\)-th timestep. Specifically, we have the following theorem about the one-step skipping.

**Theorem 1**.: _Given the skipping timestep \(i\) (\(i>0\)), the original output \(x_{i-1}^{ori}\) and the skipped output \(x_{i-1}\), then the following in-equation holds:_

\[_{i-1}=\|x_{i-1}-x_{i-1}^{ori}\|=(t_{i}-t_{i+1})+ (x_{i}-x_{i+1}).\] (3)

The proof can be found in Appendix A.2.1. From Eq. (3), it can be observed that the error of the noise-skipped output is upper-bounded by the first-order difference between the previous two outputs. Similarly, the error of the continuously skipped output is upper-bounded by the accumulative differences between multiple previous outputs, which can be found in Appendix A.2.2 and A.2.3.

Therefore, it can be inferred that as the difference between the previous outputs \(x\) is continuously minor, it is possible to predict that the noise prediction at the next timestep can be skipped without damaging the output. This inspires us to utilize the distribution of previous outputs to indicate the skip potential of the next-step noise prediction, as detailed in the following section.

Figure 3: Different update strategies. (a) The default SDXL  samples 50 steps of noise prediction followed by the latent update process. (b) Our AdaptiveDiffusion skips 25 steps of noise prediction according to the _third_-order estimator, while the latent is fully updated at all 50 steps. (c) SDXL samples 25 steps of the noise prediction and latent update process. (d) The default SDXL skips 25 steps of both noise prediction and latent update from its sampled 50 steps.

### Third-order Estimation Criterion

**Observations.** In this section, we take SDXL  with Euler sampling scheduler as an example to describe the effectiveness of the proposed third-order estimator. Before deriving the third-order estimation criterion, one thing is to calculate the optimal skipping path from the given timestep number, so that we can evaluate the effectiveness of our proposed estimator. Considering the explosive searching cost within a large search space (_e.g._, searching the optimal path of skipping \(N\) steps within \(T\) steps for one prompt requires \(C_{T}^{N}\) search time cost), we design a greedy search algorithm to approximate the optimal skipping path under different skipping targets, which can be found in Alg. (1).

Here we randomly take the prompt "_A Dustling 18th-century market scene with vendors, shoppers, and cobblestone streets, all depicted in the detailed oil painting style._" as an example to visualize the optimal skipping path searched by the greedy search algorithm under the predefined skipping target constraint. Meanwhile, as the skipping error is upper-bounded by the constraint of the first-order latent difference, it inspires us to explore the relationship between the first-order difference distribution and the ideal skipping path.

As shown in Fig. 4, we visualize two types of first-order difference (one in Fig. 4a representing the noise difference distribution, another in Fig. 4b representing latent difference distribution) in the original full-step diffusion to compare with the skipping path. It can be observed that both two distributions of first-order differences are smooth across the denoising process, showing little relationship with the optimal skipping path. A similar insight can be observed in the distribution of the second-order latent difference, as shown in Fig. 4b.

However, when considering the third-order latent difference distribution, it presents a significant fluctuation in the original full-step denoising process. As shown in Fig. 4c, the distribution of the skipping path is related to the distribution of the third-order latent difference, especially in the early denoising process (around 15 timesteps), where the noise densely updates when the third-order latent difference increases and can be skipped when the third-order difference decreases. As for the later denoising process, when most third-order differences are relatively minor, most noise prediction steps are also skipped. According to the first-order latent difference presented in Fig. 4b, the differences between consecutive latent in the early denoising process are significantly larger than those in the later denoising process. Therefore, the precise importance estimation of noise predictions in the early process is much more important and the third-order difference distribution can intuitively serve as the indicator of the noise prediction strategy. The theoretical relation between the third-order derivative of latents and the optimal skipping scheme is analyzed in Appendix A.2.4.

**Criterion.** Based on the above empirical observation of the relationship between the optimal skipping path and the high-order difference distributions, the third-order estimator is proposed to indicate the potential of skipping the noise computation. Specifically, the criterion is formulated as follows:

\[(x_{i-1})=\|^{(3)}x_{i-1}\|\| x _{i}\|,\] (4)

where \((x_{i-1})\) is the indicator that takes \(x_{i-1}\) and previous latents \(x_{i},x_{i+1},x_{i+2}\) as input to estimate whether the next noise prediction can be skipped. If \((x_{i-1})\) returns False, then the noise from the previous step will be reused to update \(x_{i-1}\). \(^{(3)}x_{i-1}\) denotes the third-order latent difference at timestep \(i-1\), _i.e._, \(^{(3)}x_{i-1}=^{(2)}x_{i-1}-^{(2)}x_{i}= x_{i-1}-2  x_{i}+ x_{i+1}\), and \( x_{i}\) is defined as the difference between \(x_{i}\) and \(x_{i+1}\) (\(i=0,...,T-1\)). \(\) is a hyperparameter thresholding the

Figure 4: The relation between order differential distributions and the searched optimal skipping path for one prompt. (a) The 1st-order noise differential distribution of the original full-step generation shows no relation with the optimal skipping path. (b) The 1st latent differential distribution indicates the distribution of the optimal skipping path but with no explicit mapping with skipping decisions, while the relative 2nd-order latent differential distribution shows a certain skipping signal in its fluctuation, but this signal is buried in the unstable magnitude. (c) The relative 3rd-order latent differential distribution shows a clearer signal for skipping decisions.

relative scale of \(^{(3)}x_{i-1}\). The reason for selecting \( x_{i}\) is that \(^{(3)}x_{i-1}\) actually describes the distance between \(( x_{i-1}+ x_{i+1})/2\) and \( x_{i}\). Therefore, it is natural to utilize the relative distance against \( x_{i}\) to indicate the stability of the denoising process. Fig. 3(c) present a strong relation between \(\|^{(3)}x_{i-1}/ x_{i}\|\) (the blue dashed line) and the optimal skipping path.

**Effectiveness of the Third-order Estimator.** To validate the effectiveness of the proposed third-order estimator, we compare our third-order estimated path with the optimal skipping path searched by the greedy algorithm, which is shown in Fig. 4(a). It can be observed that the distribution of our estimated path is largely similar to the optimal skipping path. The reason for continuous skipping in the later denoising process is that the third-order difference keeps approaching zero as illustrated in Fig. 3(c). The accumulative error caused by skipping noise predictions is described in Fig. 4(b), where it is observed that the error starts increasing quickly after continuously skipping the noise predictions. Thus, it is vital to introduce another hyperparameter, _i.e._, the maximum step number of continuous skipping \(C_{}\), to control the accumulative error. Hyperparameter analyses are described in Sec. 4.3.

Furthermore, we analyze the statistical correlation between the estimated path and the optimal path to test whether the designed criterion is significantly correlated to the optimal skipping criterion. As shown in Fig. 4(c), we compute the \(^{2}\) stats and \(p\)-values under different step numbers of skipping. The results indicate that when the skipping steps are moderate, the estimated skipping path and the optimal skipping path are significantly correlated. For those targeting at small and large numbers of skipping steps, the correlation is statistically insignificant. The test details can be found in Appendix A.3. The overall skipping algorithm is shown in Alg. (2) of Appendix A.2.5.

## 4 Experiments

### Experimental Setup

**Models.** We conduct experiments in three prompt-based settings including text-to-image (T2I), image-to-video (I2V), and text-to-video (T2V) generation tasks. In addition, we also test the effectiveness of AdaptiveDiffusion on the conditional image generation task. For the T2I task, we use Stable Diffusion-v1-5 (SD-1-5)  and Stable Diffusion XL (SDXL)  and evaluate on three different sampling schedulers (_i.e._, DDIM , DPM-Solver++ , and Euler). For the I2V and T2V tasks, we utilize I2VGen-XL  and ModelScopeT2V  respectively. Note that we use ZeroScope-v2 instead of the original ModelScopeT2V model to generate watermark-free videos. For conditional image generation, we use LDM-4  as the baseline model.

**Benchmark Datasets.** Following , we use ImageNet  and MS-COCO 2017  to evaluate the results on class-conditional image generation and T2I tasks, respectively. For the I2V task, we randomly sample 100 prompts and reference images in AIGCBench . For the T2V task, we conduct experiments on a widely-used benchmark MSR-VTT  and sample one caption for each video in the validation set as the test prompt. More details can be found in Appendix A.3.

**Comparison Baselines.** We compare AdaptiveDiffusion against DeepCache and Adaptive DPM-Solver in both generation quality and efficiency. Deepcache  caches high-level features of UNet to update the low-level features at each denoising step, thus reducing the computational cost of UNet. The latter  dynamically adjusts the step size by combining different orders of DPM-Solver.

**Evaluation Metrics.** For all tasks, we evaluate our proposed method in both quality and efficiency. We report MACs, latency, and speedup ratio to verify the efficiency. For the image generation task, following previous works [17; 16; 18], we evaluate image quality with commonly-used metrics, _i.e._,

Figure 5: The effectiveness of the proposed _third_-order estimator. (a) The third-order estimated skipping path shares a similar distribution with the optimal skipping path. (b) The latent error between the full-step update path and the estimated skipping path. (c) The \(^{2}\) stats and \(p\)-value between the greedy searched paths and the third-order estimated paths at different skipping targets.

[MISSING_PAGE_FAIL:7]

a much higher preservation quality. With the threshold gradually increasing, the speedup ratio will be largely improved but at the cost of quality degradation. It can be seen that the image quality does not significantly change with large thresholds (_i.e._, 0.015 and 0.02). This is because the pre-defined maximum skipping steps prevent the further increase of skip steps. In this paper, we set the skipping threshold to 0.01 which is a better trade-off between the generation quality and inference speed.

Ablation Study on Maximum Skipping Steps.Further, we conduct ablation studies on the maximum skipping steps \(C_{}\), as shown in the lower part of Tab. 4. With the increase of the maximum skipping steps, the quality of generated images continuously decreases. The reason is that when the timestep \(t_{i}\) approaches 0, a large number of denoising steps will be skipped due to the minor value of the third-order latent difference when the max-skip-step is relatively large. The phenomenon reveals that \(C_{}\) can effectively prevent the continuous accumulation of generated image errors and ensure image quality.

Analysis on Sampling Steps.To evaluate the effectiveness of AdaptiveDiffusion on few-step sampling. It can be seen from Tab. 5 that AdaptiveDiffusion can further accelerate the denoising process under the few-step settings. Note that the hyperparameters (_i.e._, \(\), and \(C_{}\)) should be slightly adjusted according to the original sampling steps due to the varying updating magnitudes in different sampling steps. Specifically, a higher threshold \(\) and lower max-skip-step \(C_{}\) can make better generation quality when reducing the sampling steps.

### Visualization Results

Generation Comparisons.To demonstrate the effectiveness of AdaptiveDiffusion more intuitively, we show some visualization results. Fig. 6 shows the results of the text-to-image generation task, it can be seen that AdaptiveDiffusion can better maintain image quality compared to Deepcache with nearly equal acceleration. Since AdaptiveDiffusion can adaptively determine which steps can be skipped, unimportant steps that have little impact on the final generation quality will be skipped during inference. Besides, to demonstrate the generalization of AdaptiveDiffusion on different tasks, we provide video generation results in Fig. 7. More generalization results can be found in Appendix A.4

   Hyper-params & Value & PSNR \(\) & LPIPS \(\) & FID \(\) & MACs (T) & Latency (s) & Speedup Ratio \\   & 0.005 & 34.3 & 0.023 & 0.96 & 262 & 9.7 & 1.38\(\) \\ \(\) & 0.008 & 28.6 & 0.079 & 3.22 & 217 & 7.7 & 1.74\(\) \\ \((C_{max}=4)\) & 0.01 & 24.3 & 0.168 & 6.11 & 174 & 6.7 & 2.01\(\) \\  & 0.015 & 21.0 & 0.282 & 9.49 & 119 & 5.1 & 2.64\(\) \\  & 0.02 & 21.0 & 0.289 & 9.79 & 106 & 4.4 & 3.06\(\) \\   & 4 & 24.3 & 0.168 & 6.11 & 174 & 6.7 & 2.01\(\) \\ \(C_{max}\) & 6 & 23.3 & 0.217 & 7.74 & 147 & 5.7 & 2.37\(\) \\ \((=0.01)\) & 8 & 22.7 & 0.256 & 9.43 & 135 & 5.2 & 2.59\(\) \\  & 10 & 22.1 & 0.307 & 11.91 & 123 & 4.7 & 2.86\(\) \\   

Table 4: Ablation studies on hyperparameters using SDXL . We conduct the ablation studies using 50-step Euler sampling scheduler for SDXL on COCO2017.

   Steps & PSNR \(\) & LPIPS \(\) & FID \(\) & MACs (T) & Latency (s) & Speedup Ratio \\ 
50 steps & 24.3 & 0.168 & 6.11 & 174 & 6.7 & 2.01\(\) \\
25 steps & 32.9 & 0.047 & 1.62 & 128 & 5.8 & 1.31\(\) \\
15 steps & 19.9 & 0.122 & 5.06 & 70 & 3.1 & 1.38\(\) \\
10 steps & 29.4 & 0.169 & 8.28 & 53 & 2.5 & 1.21\(\) \\   

Denoising Path Comparisons.Fig. 8 illustrates the distribution of skipping paths at different skipping schemes. It can be observed from Fig. 7(a) and 7(b) that when the number of noise update steps keeps decreasing (more blank grids in the horizontal lines), both greedy searched paths and third-order estimated paths tend to prioritize the importance of early and late denoising steps. From Fig. 7(c), we find that most prompts in the MS-COCO 2017 benchmark only need around 26 steps of noise update to generate an almost lossless image against the 50-step generation result.

Figure 8: (a) Skipping paths under different skipping targets obtained by the greedy search algorithm. (b) Skipping paths under different skipping thresholds by the third-order estimator. (c) The frequency distribution of the skipping number of noise update steps for SDXL generating images on MS-COCO 2017 benchmark.

Figure 6: Qualitative results of text-to-image generation task using SDXL and SD-1-5 on MS-COCO 2017 benchmark. Left: SDXL, Right: SD-1-5.

Figure 7: Qualitative results of image-to-video generation task using I2VGen-XL on AIGCBench.

Conclusion

In this paper, we explore the training-free diffusion acceleration and introduce AdaptiveDiffusion, which can dynamically select the denoising path according to given prompts. Besides, we perform the error analyses of the step-skipping strategy and propose to use the third-order estimator to indicate the computation redundancy. Experiments are conducted on MS-COCO 2017, ImageNet, AIGCBench and MSR-VTT, showing a good trade-off between high image quality and low inference cost.

## 6 Acknowledgements

The research was supported by the National Key R&D Program of China (Grant No. 2022ZD0160104), the Science and Technology Commission of Shanghai Municipality (Grant No. 22DZ1100102), Shanghai Municipal Science and Technology Major Project (Grant No. 2021SHZDZX0102), Shanghai Rising Star Program (Grant No. 23QD1401000), National Key Research and Development Program of China (No. 2022ZD0160101), Shanghai Natural Science Foundation (No. 23ZR1402900), Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103), National Natural Science Foundation of China (No. 62071127 and 62101137).