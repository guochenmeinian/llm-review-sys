# Mixture of Demonstrations for In-Context Learning

Song Wang

University of Virginia

sw3wv@virginia.edu

&Zihan Chen

University of Virginia

brf3rx@virginia.edu

&Chengshuai Shi

University of Virginia

cs7ync@virginia.edu

&Cong Shen

University of Virginia

cong@virginia.edu

indicates equal contributions, random order.

University of Virginia

jundong@virginia.edu

&Jundong Li

University of Virginia

jundong@virginia.edu

###### Abstract

In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle various tasks by providing input-output examples as additional inputs, referred to as demonstrations. Nevertheless, the performance of ICL could be easily impacted by the quality of selected demonstrations. Existing efforts generally learn a retriever model to score each demonstration for selecting suitable demonstrations, however, the effect is suboptimal due to the large search space and the noise from unhelpful demonstrations. In this study, we introduce **MoD** (**M**ixture **of **D**emonstrations), which partitions the demonstration pool into groups, each governed by an expert to reduce search space. We further design an expert-wise training strategy to alleviate the impact of unhelpful demonstrations when optimizing the retriever model. During inference, experts collaboratively retrieve demonstrations for the input query to enhance the ICL performance. We validate MoD via experiments across a range of NLP datasets and tasks, demonstrating its state-of-the-art performance and shedding new light on the future design of retrieval methods for ICL.

## 1 Introduction

Large language models (LLMs) have demonstrated remarkable potential across various natural language processing (NLP) tasks [62; 43; 6], such as semantic parsing [22; 53] and commonsense reasoning [42; 61]. However, the large parameter size of these models often comes with significant costs for retraining or fine-tuning when they are applied to novel tasks [16; 25; 59]. Fortunately, as LLMs increase in size, they acquire the _In-Context Learning_ (ICL) capability [50; 47], wherein the model can achieve significant performance improvements when provided with a limited number of demonstration examples during inference, without updating model parameters .

Although ICL has exhibited promising performance in various tasks, this capability also introduces a challenge related to robustness [5; 15; 36; 29]: ICL is highly sensitive to the selection of in-context demonstrations, and suboptimal selections could even lead to worse performance than random selections [34; 27; 26]. Recently, extensive research efforts have been dedicated to improving the selection of in-context demonstrations [47; 35]. For example, learning-free methods directly select demonstrations according to the similarity of demonstration embeddings from a pre-trained encoder . Learning-based methods generally optimize a retriever based on feedback or supervision signals (e.g., output probabilities) from LLMs, and demonstrate superior performance compared to learning-free methods [34; 57].

However, the performance of these approaches is limited by two crucial challenges. (1) **Large Search Space.** As ICL requires the retrieval of multiple demonstrations from a sample pool, it isdifficult to retrieve the optimal set of demonstrations from such a large search space, especially when the available sample pool is more extensive. Moreover, the total number of possible retrieval outcomes grows exponentially as the size of the retrieved set increases, rendering the retrieval even more challenging. (2) **Insufficient Optimization.** Existing learning-based works generally optimize the retriever model by preferring demonstrations that could aid model predictions. However, the common practice of randomly sampling a demonstration set in each training step could be suboptimal. For example, the samples in the entire set may contribute differently to or even impair the model predictions, but they are assigned the same retrieval scores, which could make the optimized model prefer the less helpful demonstrations.

To address the above challenges, we propose a novel demonstration retrieval framework named **MoD** (**M**ixture **of **D**emonstrations) that effectively navigates the sample pool while enabling precise optimization for beneficial demonstrations. First, to deal with the challenge of large search space, we leverage the mixture of experts (MoE) mechanism [18; 48] and partition the demonstration pool into distinct groups, each considered as an expert. Subsequently, we train an individual retriever model for each expert to prioritize helpful demonstrations, and during inference, we aggregate demonstrations retrieved from experts as the final demonstration set. Such a design largely reduces the search space of retrieval while also ensuring diversity in the demonstration set without sacrificing performance. Second, to tackle the problem of insufficient optimization, we propose a novel training strategy drawing inspiration from coordinate descent (CD) , which iteratively optimizes each dimension of a variable while fixing other dimensions. Inspired by CD, we propose an expert-wise training strategy that learns the retrieval score of any candidate demonstration while pairing it with demonstrations selected by all experts. These demonstrations are fixed while we only optimize one candidate demonstration at each step. As a result, we could ensure that all demonstrations used for optimization are optimal (except the candidate demonstration), thereby mitigating the disruption from unhelpful demonstrations. In summary, our contributions are as follows:

* We propose a novel demonstration retrieval framework MoD that learns multiple experts to collaboratively select demonstrations across the entire sample pool.
* Our design of multiple experts and expert-wise training could deal with the challenge of large search space and insufficient optimization, which have not been thoroughly investigated before.
* We conduct extensive experiments across a variety of NLP tasks to evaluate our framework in retrieving suitable demonstrations for ICL. The results demonstrate the superior performance of MoD over other state-of-the-art baselines.

## 2 Related Works

In-Context Learning.In-context learning (ICL) empowers large language models (LMs) by providing them with a few input-output examples as demonstrations , enabling them to 'learn by analogy' and proficiently undertake intricate tasks, such as machine translation [1; 39], data generation , and others [49; 13; 30]. Although successful in many aspects, the efficacy of ICL is frequently hindered by its sensitivity to the selection of in-context examples, prompting research into optimized selection strategies [26; 27; 63]. These selection techniques can be classified into learning-free and learning-based methods. Learning-free methods typically employ heuristic criteria for selecting demonstrations without directly querying LLMs during the selection process. These criteria include assessing semantic similarity between testing examples and demonstrations , measuring entropy , and ensuring diversity [41; 21; 1]. However, these methods do not actively engage with LLMs and often result in suboptimal performance. In contrast, researchers leverage feedback from LLMs as supervision signals to explore more advanced learning-based methods. For instance, EPR  trains a singleton example cover using contrastive learning with signals from LM inference. Furthermore, UDR  extends EPR in a unified formulation. These methods, however, do not account for interactions between in-context examples. In comparison, CEIL tackles this challenge by jointly modeling the selection of the exemplar set and training a retriever to score the exemplar set. Nonetheless, CEIL faces challenges such as exponential search space in the size of the demonstration pool. To address this, it narrows down the candidate space using a \(K\)-NN retriever before the selection stage, potentially leading to suboptimal demonstration sets due to insufficient exploration of the entire demonstration pool.

**Mixture of Experts.** The idea behind Mixture of Experts (MoE) is to have a set of expert networks, each specializing in a particular task or a subset of the input space [38; 45; 19]. Wang et al. extended this paradigm to the prompt optimization task, achieving substantial performance improvements . However, their approach overlooks the potential benefits of leveraging multiple expert collaborations. We extend the MoE framework to tackle the demonstration selection problem, aiming to effectively navigate the demonstration pool while considering the interplay among in-context examples.

## 3 Methodology

### Problem Setup

Given a set \(=\{e_{i}\}_{i=1}^{n}=\{(x_{i},y_{i})\}_{i=1}^{n}\) of input-output pairs (referred to as the demonstration pool), and a test example \((x_{test},y_{test})_{test}\), the strategy of ICL is to retrieve a set of demonstrations \((x_{test})\{|,| |=L\}\), which serves as the input conditioning for a pretrained LLM \(\) to make predictions on \(x_{test}\):

\[=*{argmax}_{y}_{}(y\,|\, (x_{test}),x_{test}).\] (1)

where \(_{}\) measures the likelihood of a candidate answer \(y\) generated by \(\). We aim to provide the proper demonstration set \((x_{test})\) for each \(x_{test}\) that helps \(\) make good predictions on \(x_{test}\). However, the search space could be \(||^{L}\), which is computationally infeasible for an exhaustive search. To deal with this, existing works have proposed to learn an embedding for retrieval or narrow down the search space with a KNN retriever. Such strategies are suboptimal as they ignore demonstrations that are far from the input, in terms of embedding similarities. However, such demonstrations could still be useful for ICL [21; 41].

We introduce our proposed method as the **Mixture of Demonstrations** (MoD) and outline its demonstration assignment, expert's retriever training, and inference as follows.

### Mixture of Demonstration (MoD) Framework

To address the aforementioned challenges of an extremely large search space, we propose a novel mixture of demonstration (MoD) framework based on the mixture of experts (MoE) paradigm . Specifically, we partition the demonstration pool into distinct groups, each governed by an expert. For each expert, we train a unique retriever, implemented as a scorer function, to select suitable demonstrations for the test example \(x_{test}\). During the training of the experts' retrievers, we consider the interactions among demonstrations in the prompt. With our MoD framework, the demonstration selection process for ICL is transformed into an expert assignment problem along with an individual retrieval task for each of the assigned experts. The optimal retrieved set of demonstrations for \(x_{test}\) could be achieved by selecting demonstrations from the most relevant experts, represented as follows:

\[(x_{test})=_{i=1}^{C}*{argmax}_{}_{i}_{i}}_{e}_{i}}g_{i}(x_{ test},e),|}_{i}|= h(_{i},x_{test})*L, =_{i=1}^{C}_{i}.\] (2)

Here \((x_{test})\) represents the set of demonstrations selected for the test example \(x_{test}\). \(C\) is the total number of experts into which the dataset \(\) is divided, and \(_{i}\) represents the distinct demonstration set of the \(i\)-th expert. \(}_{i}\) is the set of demonstrations selected from \(_{i}\) while maximizing the sum of values given by the scorer function \(g_{i}()\) of the \(i\)-th expert, which measures the importance of the demonstration \(e\) from \(_{i}\) with respect to the test example \(x_{test}\). \(h(_{i},x_{test})\) is a function that determines the relevance between \(x_{test}\) and each expert \(_{i}\) and also indicates the ratio of demonstrations from this expert in \((x_{test})\). With Eq. (2), we could select the most helpful demonstrations from relevant experts, regarding any input test sample \(x_{test}\). Our demonstration selection strategy of using multiple experts could efficiently cover the entire search space without high computational costs, as specific experts will be omitted during retrieval when \( h(_{i},x_{test})*L=0\). Our strategy also enables the retrieval of dissimilar samples that could be helpful for ICL, as we cover multiple experts across the entire search space. In concrete, by optimizing the scorer function \(g_{i}\) of each expert, we could retrieve the demonstration set \((x_{test})\) that could maximally aid in ICL for \(x_{test}\). In the following, we introduce details of the two-step retrieval process in our framework: 1) Demonstration Assignment and 2) Expert Retrieval.

### Demonstration Assignment

We first introduce the strategy of partitioning the entire demonstration set and assigning the corresponding demonstrations to experts. Previous studies have demonstrated that selecting demonstrative samples \(x_{i}\) with smaller distances between them and \(x_{test}\) in the sentence embedding space can enhance the effectiveness of ICL [26; 41; 34]. Based on these findings, we propose to ensure that demonstrations assigned to a specific expert should be similar. Therefore, we employ the K-means clustering approach to partition the demonstration set \(=\{e_{i}\}_{i=1}^{n}=\{(x_{i},y_{i})\}_{i=1}^{n}\) into \(C\) clusters \(\{_{1},_{2},...,_{C}\}\) based on embedding distances, and demonstrations in each cluster are assigned to a specific expert. In this way, each cluster comprises semantically similar demonstrations, from which the corresponding expert selects suitable ones for \(x_{test}\). Specifically, we utilize the widely-used Sentence-BERT model  as the embedding model \(f()\)[41; 34]. To adaptively obtain the optimal number of clusters \(C\), we combine the within-cluster sum of squared errors with a regularization term to constrain \(C\). The criterion can be expressed as follows:

\[C=*{argmin}_{C}_{k=1}^{C}_{(x_{i},y_{i})_{ k}}\|f(x_{i})-_{k}\|^{2}+ C,_{k}=_{k}|}_{(x_{i},y_{i}) _{k}}f(x_{i}).\] (3)

Here, \(_{k}\) is the \(k\)-th cluster, and \(_{k}\) denotes its centroid. With the obtained clusters, given an input test sample \(x_{test}\), we compute its similarity to the centroid of any expert \(i\) in the embedding space as follows:

\[h(_{i},x_{test})=(f(x_{test}),_{i}).\] (4)

Here \(f(x)\) is the learned embedding of sample \(x\). With the obtained scores regarding each expert, we could determine the number of demonstrations selected from each expert as \(|}_{i}|= h(_{i},x_{test})*L\).

### Expert-wise Training of Retriever Models

**Optimization Objective.** In this subsection, we introduce our approach for training a demonstration retriever, implemented as a scorer function \(g_{i}()\), for each expert \(i\). It is essential that the primary objective for the retriever is to select appropriate demonstrations based on the few-shot pattern in ICL. Therefore, considering the interaction among demonstrations, the search space can be as large as \(||^{L}\), where \(L\) is the number of demonstrations used in ICL . To mitigate the computational burden associated with such a large search space, we draw inspiration from the concept of coordinate descent (CD) . CD optimizes a variable iteratively by fixing most dimensions of the variable vector at their current values and approximately minimizing the objective. In this manner, the optimization problem in each step has fewer dimensions, making the optimization easier compared to directly optimizing all dimensions. In concrete, we propose the following optimization objective for training the score function \(g_{i}()\) of expert \(i\):

\[_{i}^{*}=*{argmax}_{}_{(x_{test},y_{test}) _{test}}(y_{test},x_{test},\{e_{test}^{i_{*}}\} _{-}(x_{test})),\] (5)

where \(_{i}^{*}\) represents the optimal parameters of \(g_{i}()\). \(\) is an evaluation criterion and can encompass various metrics, such as the log-probability of the output, i.e., \((y,x,):=_{}(y\,|\,,x)\), indicating

Figure 1: The overall process of our MoD framework. Before training, we first assign a set of demonstrations to each of the experts. Then we perform expert-wise training to obtain a retriever model for each of the experts. We ensure that the subset \(_{-}\) is optimally selected from all experts to filter out unhelpful demonstrations during training. During inference, multiple experts will provide demonstrations for predictions on the input query.

the utility of \(\) for decoding the target answer . \(_{-}(x_{test})\) denotes the demonstration set retrieved based on Eq. (2), except that the value of \(L\) in it is replaced with \(L-1\). Additionally, \(e_{test}^{i_{*}}\) represents the sample with the highest score in the unselected set from \(_{i}\) with respect to the test example \(x_{test}\), i.e.,

\[e_{test}^{i_{*}}=*{argmax}_{e_{i} {}_{-}(x_{test})}g_{i}(x_{test},e).\] (6)

In other words, akin to how CD optimizes one component while fixing others, our objective is to optimize \(g_{i}\) such that we can retrieve the demonstration (i.e., \(e_{test}^{i_{*}}\)) that contributes the most to ICL when \(L-1\) demonstrations (i.e., \(_{-}(x_{test})\)) are already retrieved and fixed. After iteratively optimizing score functions of all experts, i.e., \(\{g_{i}\}_{i=1}^{C}\), we can retrieve the proper \((x_{test})\) by Eq. (2) for LLM predictions. We outline the training process in Algorithm 1, with each phase introduced in the following sections.

Training Data.The training data construction process is detailed in Phase 1 of Algorithm 1. At the \(t\)-th epoch, we first sample a batch of samples \(d^{(t)}\). For each sample \((x_{i}^{(t)},y_{i}^{(t)}) d^{(t)}\), we use the scorer functions \(\{g_{j}^{(t-1)}\}_{j=1}^{C}\) to select the corresponding \(_{-}(x_{i}^{(t)})\) as follows:

\[_{-}(x_{i}^{(t)})=_{j=1}^{C}*{argmax}_{ }_{j-}_{j}}_{e}_{j-}}g_{j}^{(t-1)}(x_{i}^{(t)},e),|}_{j-}|= h(_{j},x_{i}^{(t)})*(L-1).\] (7)

For experts that contribute to the prediction for \(x_{i}^{(t)}\), i.e., \(|}_{j-}|>0\), we use \(g_{j}^{(t-1)}\) to retrieve \(K\) candidate demonstrations \(_{j}(x_{i}^{(t)})=\{e_{j}^{k}\}_{k=1}^{K}\) with the top-\(K\) highest scores from the unselected demonstration set \(_{j}_{-}(x_{i}^{(t)})\) of each expert \(j\). The \(K\) candidate demonstrations are obtained as follows:

\[_{j}(x_{i}^{(t)})=*{argmax}_{ _{j}_{-}(x_{i}^{(t)})}_{e}g_ {j}^{(t-1)}(x_{i}^{(t)},e),||=K.\] (8)

These demonstrations will be used as the candidate demonstration set during the following optimization step.

Few-shot Scoring.Once we retrieve the top-\(K\) demonstrations \(_{j}(x_{i}^{(t)})\) for a sample \((x_{i}^{(t)},y_{i}^{(t)})\) in the batch \(d^{(t)}\), we use the criterion \(\) to score each demonstration for its helpfulness in ICL and use the scores as supervision for optimization. In this work, we employ the log probability of the output as the metric and query the LLM \(\) for the feedback in the few-shot pattern, i.e., using multiple demonstrations as additional input. For any candidate demonstration \(e_{j}^{k}\), \(k=1,2,,K\), we score it as

\[s(e_{j}^{k})=(y_{i}^{(t)},x_{i}^{(t)},\{e_{j}^{k}\}_{-}(x_{i}^{(t)}))=_{}(y_{i}^{(t)}\,|\,\{e_{j}^{k}\} _{-}(x_{i}^{(t)}),x_{i}^{(t)}),\] (9)

which represents the probability of the LLM \(\) generating the correct prediction sequence, conditioned on the selected demonstrations and the input query. Previous works show that this score serves as a suitable proxy for the utility of a demonstration at inference time .

After scoring the \(K\) candidate demonstrations, we include the tuple \((x_{i}^{(t)},\{e_{j}^{k}\}_{k=1}^{K},\{s(e_{j}^{k})\}_{k=1}^{K})\) in the expert \(j\)'s training set \(_{j}^{train}\) for updating its scoring function at the \(t\)-th epoch, i.e., \(g_{j}^{(t)}()\). We iteratively apply the above process for all samples \((x_{i}^{(t)},y_{i}^{(t)})\) in the sampled batch \(d^{(t)}\) and employ contrastive learning for model updates.

Training Loss.Our training procedure draws inspiration from the concept of contrastive learning  that has proven to be effective when it is necessary to compare the performance of different samples. In our work, each score function \(g\) comprises two encoders: \(_{d}\) for demonstration encoding and \(_{q}\) for query input encoding. Both encoders are initialized with the bert-base-uncased model , and their output vectors represent the embeddings of the sequences. In this section, we detail the training process for expert \(j\) as in Phase 2 of Algorithm 1. We omit the subscript \(j\) for simplicity.

Given a tuple \((x_{i}^{(t)},\{e^{k}\}_{k=1}^{K},\{s(e^{k})\}_{k=1}^{K})\) for optimizing an expert, we construct its training set by including one positive and \(2B-1\) negative demonstrations, denoted as \((x_{i}^{(t)},e_{pos},e_{neg}^{1},e_{neg}^{2},...,e_{neg}^{2B-1})\), where \(B\) is the batch size. The positive demonstration \(e_{pos}\) is sampled from top \(\) demonstrations with largest few-shot scores, denoted as \(_{pos}\), in the candidate set \(\{e^{k}\}_{k=1}^{K}\) (thus \(<K\)):

\[_{pos}=*{argmax}_{\{e^{k}\}_{k=1}^{K }}_{e}s(e),||=.\] (10)

In this manner, we further filter out the demonstrations with low few-shot scores, indicating that they are not suitable for acting as a demonstration accompanied with other optimal demonstrations in \(_{-}\). Negative samples \((e_{neg}^{1},e_{neg}^{2},...,e_{neg}^{2B-1})\) include: (i) one hard demonstration \(e_{hard}=*{argmin}_{e\{e^{k}\}_{k=1}^{K}}s(e)\); (ii) \(B-1\) positive demonstrations from the other \(B-1\) samples in \(d^{(t)}\); and (iii) \(B-1\) hard negative demonstrations from those samples. The score returned by \(g\) is defined as \(g(x,e)=_{d}(e),_{q}(x)\). We then propose the contrastive learning loss and use it to update \(g\):

\[(x_{i}^{(t)},e_{pos},e_{neg}^{1},e_{neg}^{2},...,e_{neg}^{2B-1})= -^{(t)},e_{pos}))}{(g(x_{i}^{(t)},e_{pos}))+_{j= 1}^{2B-1}(g(x_{i}^{(t)},e_{neg}^{j}))}.\] (11)

Intuitively, the above loss will assign higher scores for demonstrations that are more helpful, when other demonstrations are already optimal. Thus, our expert-wise training could alleviate the impact of unhelpful demonstrations during optimization.

### Inference

In the inference stage, we select demonstrations for an input query \(x_{test}\) according to Eq. (2), and obtain the prediction \(=*{argmax}_{y}_{}(y|(x _{test}),x_{test})\) given by LLM \(\). Although we update the retriever models independently for each expert, each retriever model is designed to select demonstrations that benefit ICL in few-shot scenarios, i.e., using a set of demonstrations as additional input. This is ensured because the supervision scores in Eq. (9) for training the retriever models are generated in a few-shot pattern with a set of demonstrations. For the optimal retriever models \(\{g_{j}^{*}\}_{j=1}^{C}\), each model essentially solves the problem: _"Given a good demonstration set \(S_{-}^{*}\) of size \(L-1\), which demonstration should the expert choose to make the best prediction in \(L\)-shot ICL?"_ Consequently, for any input query, the experts in MoD can collaboratively retrieve a set of demonstrations that could most effectively aid in making accurate predictions.

  
**Type** & **Task** & **\# Train** & **\# Validation** & **\# Demo** \\   \\ SST-5  & Sentiment Analysis & 8,534 & 1,101 & 40 \\ MRPC  & Paraphrase Detection & 3,668 & 408 & 27 \\ MNLI  & Natural Language Inference & 392,568 & 19,647 & 40 \\ QNLI  & Natural Language Inference & 104,707 & 5,463 & 27 \\ CMSQA  & Commonsense Reasoning & 9,740 & 1,221 & 50 \\ HellaSwag  & Commonsense Reasoning & 52,611 & 20,006 & 50 \\   \\ WebQs  & Open-Domain QA & 3,778 & 2,032 & 50 \\ GeoQuery  & Code Generation & 404 & 280 & 50 \\ NL2Bash  & Code Generation & 7,441 & 609 & 43 \\ Break  & Semantic Parsing & 44,184 & 7,760 & 28 \\ MTOP  & Semantic Parsing & 15,564 & 2,235 & 41 \\ SMCalFlow  & Semantic Parsing & 102,491 & 14,751 & 22 \\   

Table 1: The datasets used in experiments and their corresponding tasks. # Train and # Validation denote the numbers of samples during training and validation, respectively. # Demo denotes the average number of demonstrations used in each task during validation. # Expert represents the number of experts used in each task.

## 4 Experiments

### Experimental Settings

**Baselines.** Our MoD framework functions as a mixture of multiple learning-based retrievers for selecting in-context examples from different subsets in the entire training set. We compare it against both learning-free and learning-based retrievers. Learning-free methods include Random, TopK-BM25 , TopK-Contriver , and TopK-SimCSE . Learning-based methods include EPR  and CEIL . We provide more details in Appendix B.2.

**Datasets.** To ensure a fair comparison between our framework and other baselines, following CEIL , we conduct experiments on a variety of datasets, involving both classification and generation tasks. For the evaluation on classification datasets, we measure the accuracy of the output regarding the correct answers. For evaluation on generation tasks, we adopt the metrics of Exact Match (EM) scores for all generation datasets except Break, for which we use LF-EM  that additionally considers semantic equivalence. Following CEIL , we present the final results based on the validation set as test sets are unavailable for specific datasets.

**Implementation Details.** To keep consistency with CEIL  and EPR , we primarily use GPT-Neo , a 2.7-billion-parameter language model trained on The Pile , which is an 825GB text corpus collected from various high-quality resources. In Sec. 4.5, we additionally consider three models: GPT2-XL  with 1.5 billion parameters, LLaMA-7B  with 7 billion parameters, and GPT3.5  with a significantly larger parameter size. The number of in-context demonstrations in our experiments is set as 50, while we truncate this number when the combined length exceeds the maximum context size of LLMs for each task. The ultimate average number of in-context demonstrations used in each task is provided in Table 1. We provide details of the settings in Appendix B.3.

### Comparative Results

In Table 2, we report the results of our framework MoD and other baselines on two sets of datasets: six classification datasets and six generation datasets, covering seven tasks. From the results, we could obtain the following observations: (1) **Superior Performance.** MoD demonstrates superior performance across a diverse set of tasks, both in classification and generation, as evidenced by the highest average score (58.63%) compared to competitive baselines CEIL (56.76%) and EPR (53.37%). This indicates that MoD is more effective in leveraging in-context demonstrations to enhance task performance. (2) **Better on Classification.** Compared with CEIL, MoD generally achieves higher performance gain on classification tasks than on generation tasks (Average \(\) Gain 2.10 on classification tasks v.s. Average \(\) Gain 1.64 on generation tasks). This is because our design of the mixture-of-expert architecture enables the selection of demonstrations with a large distance in the embedding space to the query. As classification tasks could be more easily affected by several demonstrations, these selected demonstrations could potentially carry helpful information for inference on the query, while not necessarily being similar to the query in the embedding space. (3)

  
**Method** & **SST-5** & **MRPC** & **QNLI** & **MNLI** & **CMSQA** & **Swag** & **WebQs** & **GeQ** & **NL2Bash** & **Break** & **MTOP** & **SMCal** & **Avg.** \\   \\ Random & 31.43 & 67.65 & 56.67 & 37.74 & 42.51 & 41.16 & 4.87 & 33.93 & 34.35 & 1.70 & 7.30 & 8.90 & 30.68 \\ TopK-BM25 & 36.06 & 69.36 & 62.29 & 40.68 & 36.12 & 42.20 & 16.68 & 62.86 & 58.98 & 26.00 & 52.70 & 46.10 & 45.84 \\ TopK-C & 37.06 & 67.89 & 60.97 & 45.28 & 36.12 & 41.60 & 17.62 & 68.93 & 53.69 & 26.34 & 49.84 & 43.44 & 45.73 \\ TopK-S & 37.06 & 66.91 & 61.58 & 44.85 & 35.54 & 41.69 & 16.83 & 66.43 & 54.89 & 26.58 & 47.29 & 42.59 & 45.19 \\ TopK-BERT & 37.24 & 69.36 & 64.65 & 42.15 & 35.38 & 40.28 & 17.08 & 66.79 & 51.30 & 26.84 & 52.13 & 44.63 & 45.65 \\   \\ EPR & 42.82 & 75.98 & 80.76 & 66.06 & 36.77 & 42.61 & 19.59 & 68.57 & 56.82 & 31.90 & 64.20 & 54.30 & 53.37 \\ CEIL & 47.05 & 80.15 & 85.41 & 71.74 & 37.18 & 43.20 & 20.92 & 73.21 & 59.91 & 34.18 & 67.43 & 60.73 & 56.76 \\ MoD & **48.12** & **81.53** & **86.63** & **73.24** & **43.24** & **44.54** & **21.45** & **73.75** & **62.94** & **35.80** & **69.32** & **62.97** & **58.63** \\ \(\) Gain & +1.07 & +1.38 & +1.22 & +1.50 & +6.06 & +1.34 & +0.53 & +0.54 & +3.03 & +1.62 & +1.89 & +2.24 & +1.87 \\   

Table 2: The comparative results of our method and other baselines on various datasets. We present the absolute performance gain over CEIL, and the best results are shown in bold.

**Require Less Data.** MoD's consistent performance from large-scale datasets like MNLI (392,568 training samples) to smaller datasets like GeoQuery (404 training samples) suggests that it effectively generalizes across datasets with varying sizes. The superior performance of MoD on smaller datasets like GeoQuery and NL2Bash demonstrates its ability to learn effectively even with limited labeled data for demonstration selection.

### Results on Compositional Datasets

A critical advantage of MoD is its capability to collaboratively select demonstrations from multiple experts, such that these demonstrations are maximally helpful when the other demonstrations in the selected set are also optimal. To evaluate whether the demonstrations retrieved from various experts could be entirely helpful for ICL, we conduct experiments on two semantic parsing datasets derived from the original SMCalFlow and GeoQuery datasets and processed by CEIL . Specifically, the inference on queries in these datasets requires the precise retrieval of multiple specific demonstrations. In other words, without precise retrieval, it is particularly difficult to answer these queries. We provide more details of the dataset settings in Appendix B.1. Following CEIL, we utilize the same trained retriever models of experts as used in Sec. 4.2. From the results presented in Table 3, we could obtain the following observations: (1) The performance of MoD is consistently superior compared to other baselines across datasets. Notably, these tasks require the retrieval of compositional demonstrations that are all important but may not necessarily be similar to each other. In this regard, our proposed MoD framework directly retrieves a diverse set of demonstrations, which significantly enhances the efficacy of few-shot ICL, compared to other basins in this scenario. (2) MoD demonstrates notable improvements on the cross-domain splits (C) of the SMCalFlow-CS dataset. Specifically, MoD achieves gains of \(+0.11\%\) over CEIL on the cross-domain split. This performance indicates MoD's ability to handle complex, multi-domain tasks by effectively selecting and utilizing diverse in-context examples from multiple experts.

### Reduction of ICL Demonstrations

In this subsection, we aim to explore the capability of our MoD framework in scenarios where the number of ICL demonstrations selected from the training set is decreased. This is critical for evaluating the practicality of MoD, as it could be challenging to leverage sufficient demonstrations, due to the lack of data or limitation of model sizes. Particularly, we conduct experiments with different numbers of in-context demonstrations on two classification datasets SST-5 and CMSQA, and two generation datasets GeoQuery and MTOP. We present the performance of MoD over the state-of-the-art baseline CEIL in Fig. 2. From the results, we could observe that particularly on classification datasets SST-5 and CMSQA, our performance improvements over CEIL are more significant. This indicates that for classification tasks that require diverse

    &  &  \\   & **Standard** & **Template** & **TMCD** & **Length** & **S** & **C** \\  TopK-BERT & 66.79 & 30.75 & 41.82 & 31.59 & 31.94 & 0.28 \\ EPR & 68.57 & 38.95 & 44.09 & 32.27 & 57.78 & 0.00 \\ CEIL & 73.21 & 40.77 & 44.09 & 32.73 & 60.27 & 0.28 \\ MoD & **77.38** & **41.84** & **44.55** & **33.19** & **62.95** & **0.39** \\ \(\) Performance & +4.17 & +1.07 & +0.46 & +0.46 & +2.68 & +0.11 \\   

Table 3: Performance of our framework and various baselines on processed compositional datasets GeoQuery and SMCalFlow-CS. S refers to a non-compositional test set and C refers to a compositional set with additional cross-domain examples as demonstrations.

Figure 2: The results of MoD performance over CEIL on various datasets with different numbers of demonstrations. We report the absolute gain of the results.

knowledge, our strategy using multiple experts could effectively retrieve crucial demonstrations, which could provide sufficient knowledge even with a limited context length. The performance improvements are relatively consistent on the two generation datasets, i.e., GeoQuery and MTOP. This is because the generation tasks are generally more difficult, and thus require a larger demonstration set. As a result, the advantage of MoD in retrieving diverse knowledge becomes less substantial for model performance.

### Robustness Study

In this subsection, we aim to evaluate the robustness, especially the generalizability and transferability of our method MoD to various LLMs. Particularly, our experiments are designed to test whether the retriever models in our MoD framework trained on one LLM could be transferred to other LLMs. Conducting experiments to answer this question could help investigate the applicability of MoD when deployed in realistic scenarios, where LLMs could have different architectures and parameter sizes. Specifically, we use the retriever models trained on GPT-Neo to select demonstrations for the other two models: GPT2-XL with a slightly smaller parameter size and GPT3.5 with a significantly larger parameter size. We present the results of MoD over TopK-BERT in Table 4. From the results, we could observe that (1) The retriever models trained on GPT-Neo exhibit competitive performance when transferred to other LLMs across various datasets. This indicates the transferability of MoD, especially its scalability to large black-box models like GPT3.5. (2) The performance improvements on GPT3.5 are less competitive. This is because due to the powerfulness of GPT3.5, simple methods like TopK-BERT already perform well. Nevertheless, MoD could still improve performance by retrieving better demonstrations. (3) When transferring the retriever models trained on LLaMA-7B to smaller models, the performance improvements are less obvious, probably due to the discrepancy between LLMs in understanding demonstrations.

### Ablation Study

In this subsection, we aim to evaluate the specific benefits to performance brought by different modules and designs in our MoD framework. In particular, we evaluate the performance of our MoD framework on four datasets: SST-5, CM-SQA, GeoQuery, and MTOP, distinctly covering two classification tasks and two generation tasks. As presented in Fig. 3, we investigate the impact of two key components of our framework: the mixture-of-experts design (MoD w/o E) and the expert-wise training (MoD w/o C). The first variant of our ablation study involves removing the mixture-of-experts design, which results in a significant drop in performance across all datasets, highlighting the importance of leveraging multiple experts for robust prediction. The second variant excludes the expert-wise training process, which leads to a moderate decrease in performance, indicating its role in improving the model's performance. Moreover, the results demonstrate that removing the mixture-of-experts design is particularly detrimental for classification tasks, such as SST-5 and CMSQA. Therefore, this underscores its critical contribution to retrieving more diverse and complex demonstrations, which are more crucial for classification tasks.

  
**Model** & **SST-5** & **CMSQA** & **GeoQ** & **MTOP** \\   \\ GPT-Neo & 10.88 & 7.86 & 6.96 & 17.19 \\ GPT2-XL & 8.39 & 8.57 & 6.10 & 15.34 \\ LLaMA-7B & 4.28 & 5.63 & 6.27 & 9.80 \\ GPT3.5 & 3.24 & 6.58 & 4.97 & 7.98 \\   \\ GPT-Neo & 9.67 & 6.92 & 7.34 & 16.05 \\ GPT2-XL & 7.48 & 7.83 & 6.45 & 14.89 \\ LLaMA-7B & 4.12 & 5.47 & 5.10 & 10.27 \\ GPT3.5 & 2.98 & 6.22 & 5.02 & 8.45 \\   

Table 4: Performance improvements over TopK-BERT when transferring learned retriever models in MoD to other LLMs on four datasets.

Figure 3: The ablation study result.

Conclusion

In this work, we propose to divide the demonstration retrieval process for in-context learning into multiple parts, each governed by an expert to select from its own sample pool. Our proposed MoD framework further performs expert-wise training to filter out unhelpful demonstrations when optimizing each candidate demonstration. We conduct extensive experiments across a variety of datasets and tasks, and the results validate the superiority of MoD over other baselines.