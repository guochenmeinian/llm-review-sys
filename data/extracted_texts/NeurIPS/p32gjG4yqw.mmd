# Constructive Universal Approximation Theorems for Deep Joint-Equivariant Networks by Schur's Lemma

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We present a unified constructive universal approximation theorem covering a wide range of learning machines including both shallow and deep neural networks based on the group representation theory. Constructive here means that the distribution of parameters is given in a closed-form expression (called the _ridgelet transform_). Contrary to the case of shallow models, expressive power analysis of deep models has been conducted in a case-by-case manner. Recently, Sonoda et al.  developed a systematic method to show a constructive approximation theorem from _scalar-valued joint-group-invariant_ feature maps, covering a formal deep network. However, each hidden layer was formalized as an abstract group action, so it was not possible to cover real deep networks defined by composites of nonlinear activation function. In this study, we extend the method for _vector-valued joint-group-equivariant_ feature maps, so to cover such real networks.

## 1 Introduction

An ultimate goal of the deep learning theory is to characterize the internal data processing procedure inside deep neural networks obtained by deep learning. We may formulate this problem as a functional equation problem: Let \(\) denote a class of data generating functions, and let \([]\) denote a certain deep neural network with parameter \(\). Given a function \(f\), find an unknown parameter \(\) so that network \([]\) represents function \(f\), i.e.

\[[]=f,\] (1)

which we call a _DNN equation_. An ordinary learning problem by empirical risk minimization, such as minimizing \(_{i=1}^{n}|[](x_{i})-f(x_{i})|^{2}\) with respect to \(\), is understood as a weak form (or a variational form) of this equation. Therefore, characterizing the solution space of this equation leads to understanding the parameters obtained by deep learning. Following previous studies , we call a solution operator \(\) that satisfies \([[f]]=f\) a _ridgelet transform_. Once such a solution operator \(\) is found, we can conclude a _universality_ of the DNN in consideration because the reconstruction formula \([[f]]=f\) implies for any \(f\) there exists a DNN that represents \(f\). In particular, when \([f]\) is found in a closed-form manner, then it leads to a _constructive_ proof of the universality since \([f]\) could indicate how to assign parameters.

When the network has only one infinitely-wide hidden layer, though it is not deep but shallow, the characterization problem has been well investigated. For example, the learning dynamics and the global convergence property (of SGD) are well studied in the mean field theory  and the Langevin dynamics theory , and even closed-form solution operator to a "shallow" NN equation, the original ridgelet transform, has already been presented .

On the other hand, when the network has more than one hidden layer, the problem is far from solved, and it is common to either consider infinitely-deep mathematical models such as NeuralODEs [27; 9; 17; 12; 4], or handcraft inner feature maps depending on the problem. For example, construction methods such as the Telgarsky sawtooth function (or the Yarotsky scheme) and bit extraction techniques [7; 36; 37; 38; 6; 26; 24; 11] have been developed to demonstrate the depth separation, super-convergence, and minmax optimality of deep ReLU networks. Various feature maps have also been handcrafted in the contexts of geometric deep learning  and deep narrow networks [19; 13; 18; 14; 23; 16; 2; 15]. Needless to say, there is no guarantee that these handcrafted feature maps are acquired by deep learning, so these analyses are considered to be analyses of possible worlds.

Recently, Sonoda et al. [33; 32] discovered a rich class of ridgelet transforms for learning machines defined by _scalar-valued joint-group-invariant_ feature maps, covering both depth-2 fully-connected networks and the formal deep network (FDN), yielding the first ridgelet transform for deep models. Their theory is indeed a breakthrough because it could cover both deep and shallow models simultaneously. However, each hidden layer in the FDN has to be formalized as an abstract group action, so it was not possible to cover real deep networks defined by composites of nonlinear activation function. In this study, we extend their arguments for _vector-valued joint-group-equivariant_ feature maps (Theorem 3 and Corollary 1), so to cover such real networks. As an important example, in SS 4.2, we obtained the ridgelet transform for a more realistic DNN, the depth-\(n\) fully-connected network with an arbitrary activation function (not limited to ReLU), without handcrafting network architecture. In other words, it is a constructive proof of the \(L^{2}(^{m};^{m})\)-universality of the DNNs, and an explicit characterization of the solution space of the DNN equation for more realistic setup.

Thanks to Schur's lemma, a basic and useful result in the representation theory, the proof of the main theorem is surprisingly simple, yet the scope of application is wide. The significance of this study lies in revealing the close relationship between machine learning theory and modern algebra. With this study as a catalyst, we expect a major upgrade to machine learning theory from the perspective of modern algebra.

## 2 Preliminaries

We quickly introduce the original integral representation and the ridgelet transform, a mathematical model of depth-2 fully-connected network and its right inverse. Then, we list a few facts in the group representation theory. In particular, _Schur's lemma_ and the _Haar measure_ play key roles in the proof of the main results.

Notation.For any topological space \(X\), \(C_{c}(X)\) denotes the Banach space of all compactly supported continuous functions on \(X\). For any measure space \(X\), \(L^{p}(X)\) denotes the Banach space of all \(p\)-integrable functions on \(X\). \((^{d})\) and \(^{}(^{d})\) denote the classes of rapidly decreasing functions (or Schwartz test functions) and tempered distributions on \(^{d}\), respectively.

### Integral Representation and Ridgelet Transform for Depth-2 Fully-Connected Network

**Definition 1**.: For any measurable functions \(:\) and \(:^{m}\), put

\[S_{}[]():=_{^{m}}(,b)(-b)b, ^{m}.\] (2)

We call \(S_{}[]\) an (integral representation of) neural network, and \(\) a parameter distribution.

The integration over all the hidden parameters \((,b)^{m}\) means all the neurons \(\{(-b)(,b)^{m} \}\) are summed (or integrated, to be precise) with weight \(\), hence formally \(S_{}[]\) is understood as a continuous neural network with a single hidden layer. We note, however, when \(\) is a finite sum of point measures such as \(_{p}=_{i=1}^{p}c_{i}_{(_{i},b_{i})}\) (by appropriately extending the class of \(\) to Borel measures), then it can also reproduce a finite width network

\[S_{}[_{p}]()=_{i=1}^{p}c_{i}(_{i}-b_{i}).\] (3)

In other words, the integral representation is a mathmatical model of depth-2 network with _any_ width (ranging from finite to continuous).

Next, we introduce the ridgelet transform, which is known to be a right-inverse operator to \(S_{}\).

**Definition 2**.: For any measurable functions \(:\) and \(f:^{m}\), put

\[R_{}[f](,b):=_{^{m}}f() -b)},(,b)^{m}.\] (4)

We call \(R_{}\) a ridgelet transform.

To be precise, it satisfies the following reconstruction formula.

**Theorem 1** (Reconstruction Formula).: _Suppose \(\) and \(\) are a tempered distribution (\(^{}\)) and a rapid decreasing function (\(\)) respectively. There exists a bilinear form \((\!(,)\!)\) such that_

\[S_{} R_{}[f]=\!(\!(,)\!)f,\] (5)

_for any square integrable function \(f L^{2}(^{m})\). Further, the bilinear form is given by \((\!(,)\!)=_{}^{}()()}||^{-m},\) where \(\) denotes the 1-dimensional Fourier transform._

See Sonoda et al. [29, Theorem 6] for the proof. In particular, according to Sonoda et al. [29, Lemma 9], for any activation function \(\), there always exists \(\) satisfying \((\!(,)\!)=1\). Here, \(\) being a tempered distribution means that typical activation functions are covered such as ReLU, step function, \(\), gaussian, etc... We can interpret the reconstruction formula as a universality theorem of continuous neural networks, since for any given data generating function \(f\), a network with output weight \(_{f}=R_{}[f]\) reproduces \(f\) (up to factor \((\!(,)\!)\)), i.e. \(S[_{f}]=f\). In other words, the ridgelet transform indicates how the network parameters should be organized so that the network represents an individual function \(f\).

The original ridgelet transform was discovered by Murata  and Candes . It is recently extended to a few modern networks by the Fourier slice method [34, see e.g.]. In this study, we present a systematic scheme to find the ridgelet transform for a variety of given network architecture based on the group theoretic arguments.

### Irreducible Unitary Representation and Schur's Lemma

Let \(G\) be a locally compact group, \(\) be a nonzero Hilbert space, and \(()\) be the group of unitary operators on \(\). For example, any finite group, discrete group, compact group, and finite-dimensional Lie group are locally compact, while an infinite-dimensional Lie group is not locally compact. A _unitary representation_\(\) of \(G\) on \(\) is a group homomorphism that is continuous with respect to the strong operator topology--that is, a map \(:G()\) satisfying \((gh)=(g)(h)\) and \((g^{-1})=(g)^{-1}\), and for any \(\), the map \(G g(g)[]\) is continuous.

Suppose \(\) is a closed subspace of \(\). \(\) is called an _invariant_ subspace when \((g)\) for all \(g G\). Particularly, \(\) is called _irreducible_ when it does not admit any nontrivial invariant subspace \(\{0\}\) nor \(\). The following theorem is a fundamental result of group representation theory that characterizes the irreducibility.

**Theorem 2** (Schur's lemma).: _A unitary representation \((,)\) is irreducible iff any bounded operator \(T\) on \(\) that commutes with \(\) is always a constant multiple of the identity. In other words, if \((g)T=T(g)\) for all \(g G\), then \(T=c_{}\) for some \(c\)._

See Folland [10, Theorem 3.5(a)] for the proof. We use this as a key step in the proof of our main theorem.

### Calculus on Locally Compact Group

By Haar's theorem, if \(G\) is a locally compact group, then there uniquely exist left and right invariant measures \(_{l}g\) and \(_{r}g\), satisfying for any \(s G\) and \(f C_{c}(G)\),

\[_{G}f(sg)_{l}g=_{G}f(g)_{l}g, _{G}f(gs)_{r}g=_{G}f(g)_{r}g.\]

Let \(X\) be a \(G\)-space with transitive left (resp. right) \(G\)-action \(g x\) (resp. \(x g\)) for any \((g,x) G X\). Then, we can further induce the left (resp. right) invariant measure \(_{l}x\) (resp. \(_{r}x\)) so that for any \(f C_{c}(G)\),

\[_{X}f(x)_{l}x:=_{G}f(g o)_{l}g,_{X}f(x)_{r}x:=_{G}f(o g)_{r}g,\]

where \(o X\) is a fixed point called the origin.

## 3 Main Results

We introduce the joint-group-equivariant feature map, and present the ridgelet transforms for learning machines defined by joint-group-equivariant feature maps, yielding the universality of deep models.

Let \(G\) be a locally compact group equipped with a left invariant measure \(g\). Let \(X\) and \(\) be \(G\)-spaces equipped with \(G\)-invariant measures \(x\) and \(\), called the data domain and the parameter domain, respectively. Particularly, we call the product space \(X\) the _data-parameter_ domain (like time-frequency domain), and call any map \(\) on data-parameter domain \(X\) a _feature map_. Let \(\) be a separable Hilbert space, let \(()\) be the space of unitary operators on \(\), and let \(:G()\) be a unitary representation of \(G\) on \(\). If there is no danger of confusion, we use the same symbol \(\) for the \(G\)-actions on \(X\), \(\), and \(\) (e.g., \(g x\), \(g v\), and \(g\)).

In the main theorem, the irreducibility of the following unitary representation \(\) will be a sufficient condition for the universality. Let \(L^{2}(X;)\) denote the space of \(\)-valued square-integrable functions on \(X\) equipped with the inner product \(,_{L^{2}(X;)}:=_{X}(x),(x )_{}x\). Put

\[_{g}[f](x):=g f(g^{-1} x), x X,\;f L^{2}(X;),\;g G.\] (6)

Then, it is a unitary representation of \(G\) on \(L^{2}(X;)\). In fact, \(_{g}[_{h}[f]](x)=g h f(h^{-1} g^{-1} x)=(gh) f ((gh)^{-1} x)=_{gh}[f](x)\), and \(_{g}[f_{1}],_{g}[f_{2}]_{L^{2}(X;)}=_{X} _{g}[f_{1}](g^{-1} x),_{g}[f_{2}](g^{-1} x) _{}x=_{X} f_{1}(x),_{g}^{*}[ _{g}[f_{2}]](x)_{}x= f_{1},f_{2} _{L^{2}(X;)}\).

In addition, let \(L^{2}()\) denote the space of \(\)-valued square-integrable functions on \(\), and let \(\) be the left-regular representation of \(G\) on \(L^{2}()\) given by

\[_{g}[]():=(g^{-1}),,\;  L^{2}(),\;g G.\] (7)

Similarly to \(,\) is also a unitary representation.

**Definition 3** (Joint \(G\)-Equivariant Feature Map).: Let \(X,Y\) be data domains, and \(\) be a parameter domain (with \(G\)-actions). We say a feature map \(:X Y\) is _joint-\(G\)-equivariant_ when

\[(g x,g)=g(x,),(x,) X,\] (8)

holds for all \(g G\). In other words, \(\) is a homomorphism (or \(G\)-map) of \(G\)-sets from \(X\) to \(Y\). So by \(_{G}(X,Y)\), we denote the collection of all joint-\(G\)-equivariant maps. Additionally, when \(G\)-action on \(Y\) is trivial, i.e. \((g x,g)=(x,)\), we say it is _joint-\(G\)-invariant_.

_Remark 1_.: The joint-\(G\)-equivariance extends an ordinary notion of \(G\)-_equivariance_, i.e. \((g x,)=g(x,)\). In fact, \(G\)-equivariance is a special case of joint-\(G\)-equivariance where \(G\) acts trivially on parameter domain, i.e. \(g=\) (see also Figure 1).

In order to construct a (non-joint) group-equivariant network, we must carefully and precisely design the network architecture [see, e.g., a textbook of geometric deep learning 1]. On the other hand, we can easily and systematically construct joint-\(G\)-equivariant network from (not at all equivariant but) any map \(f:X Y\) according to the following Lemmas 1 and 2.

**Lemma 1**.: _Suppose group \(G\) acts on sets \(X\) and \(Y\). Fix an arbitrary map \(f:X Y\), and put \((x,g):=g f(g^{-1} x)\) for every \(x X\) and \(g G\). Then, \(:X G Y\) is joint-\(G\)-equivariant._

Proof.: Straightforward. For any \(g G\), \((g x,g h)=(gh) f((gh)^{-1}(g x))=g(x,h)\). 

Figure 1: An ordinary \(G\)-equivariant feature map \(:X Y\) is a subclass of joint-\(G\)-equivariant map where the \(G\)-action on parameter domain \(\) is _trivial_, i.e. \(g=\)

**Lemma 2** (Depth-\(n\) Feature Map \(_{1:n}\)).: _Given a sequence of \(G\)-equivariant feature maps \(_{i}:X_{i-1}_{i} X_{i}\)\((i=1,,n)\), put \(_{1:n}:X_{0}_{1}_{n} X_{n}\) by_

\[_{1:n}(x,_{1},,_{n}):=_{n}(,_{n}) _{1}(x,_{1}).\] (9)

_Then, \(_{1:n}\) is \(G\)-equivariant. Following the custom of counting the number of parameter domains \((_{i})_{i=1}^{n}\), we say \(_{1:n}\) is depth-\(n\)._

Proof.: In fact,

\[_{1:n}(g x,g_{1},,g_{n}) =_{n}(,g_{n})_{2}(,g _{2})_{1}(g x,g_{1})\] \[=_{n}(,g_{n})_{2}(g ,g_{2})_{1}(x,_{1})\] \[\] \[=_{n}(g,g_{n})_{2}( ,_{2})_{1}(x,_{1})\] \[=g_{1:n}(x,_{1},,_{n}).\]

**Definition 4** (\(\)-Network).: For any vector-valued map \(:X\) and scalar-valued map \(:\), define a vector-valued map \(X\) by

\[[;](x):=_{}()(x,),  x X,\] (10)

where the integral is understood as the Bocher integral.

We call the integral transform \([;]\) a \(\)-transform, and each individual image \([;]\) a \(\)-network for short. The \(\)-network extends the original integral representation. In particular, it inherits the concept of integrating all the possible parameters \(\) and indirectly select which parameters to use by weighting on them, which _linearize_ parametrization by lifting nonlinear parameters \(\) to linear parameter \(\).

**Definition 5** (\(\)-Ridgelet Transform).: For any \(\)-valued feature map \(:X\) and \(\)-valued Borel measurable function \(f\) on \(X\), put a scalar-valued integral transform

\[[f;]():=_{X} f(x),(x,)_{}x,.\] (11)

We call the integral transform \([;]\) a \(\)-ridgelet transform for short.

As long as the integrals are convergent, \(\)-ridgelet transform is the dual operator of \(\)-transform, since

\[,[f;]_{L^{2}()}=_{X}( )(x,),f(x)_{}x= [;],f_{L^{2}(X;)}.\] (12)

**Theorem 3** (Reconstruction Formula).: _Assume (1) \(\)-valued feature maps \(,:X\) are joint-\(G\)-equivariant, (2) composite operator \(_{}_{}:L^{2}(X;) L^{2}(X; )\) is bounded (i.e., Lipschitz continuous), and (3) the unitary representation \(\) defined in (6) is irreducible. Then, there exists a bilinear form \((\!(,)\!)\) (independent of \(f\)) such that for any \(\)-valued square-integrable function \(f L^{2}(X;)\),_

\[_{}_{}[f]=(\!(,)\!)f.\] (13)

In other words, the \(\)-ridgelet transform \(_{}\) is a right inverse operator of \(\)-transform \(_{}\) as long as \((\!(,)\!) 0,\).

Proof.: We write \([;]\) as \(_{}\) and \([;]\) as \(_{}\) for short. By using the unitarity of representation \(:G()\), left-invariance of measure \(x\), and \(G\)-equivariance of feature map \(\), for all \(g G\), we have

\[_{}[_{g}[f]]() =_{X} g f(g^{-1} x),(x,)_{ }x=_{X} f(x),g^{-1}(g x,) _{}x\] \[=_{X} f(x),(x,g^{-1})_{ }x=_{g}[_{}[f]]().\] (14)Similarly,

\[_{}[_{g}[]](x) =_{}(g^{-1})(x,)=_{ }()(x,g)\] \[=_{}()\ (g(g^{-1} x,) )=_{g}[_{}[]](x).\] (15)

Here, \(^{*}\) denotes the dual representation of \(\) with respect to \(L^{2}()\).

As a consequence, \(_{}_{}:L^{2}(X;) L^{2}(X; )\) commutes with \(\) as below

\[_{}_{}_{g}=_{} _{g}_{}=_{g}_{} _{}\] (16)

for all \(g G\). Hence by Schur's lemma (Theorem 2), there exist a constant \(C_{,}\) such that \(_{}_{}=C_{,}_{L^{2} (X)}\). Since \(_{}_{}\) is bilinear in \(\) and \(\), \(C_{,}\) is bilinear in \(\) and \(\). 

In particular, because depth-\(n\) feature map \(_{1:n}\) is \(G\)-equivariant (Lemma 2), the following depth-\(n\)\(\)-valued deep network \([;_{1:n}]\) is \(L^{2}(X;)\)-universal.

**Corollary 1** (Deep Ridgelet Transform).: _For any maps \(:X\) and \(f L^{2}(X;)\), put_

\[[;_{1:n}](x) :=_{_{1}_{n}}(_{1},, _{n})_{n}(,_{n})_{1}(x,_{1}), x X,\] (17) \[[f;_{1:n}]() :=_{} f(x),_{n}(,_{n}) _{1}(x,_{n})_{}x,_{1}_{n}.\] (18)

_Under the assumptions that \(_{_{1:n}}_{_{1:n}}\) is bounded, and that \(\) is irreducible, there exists a bilinear form \((_{1:n},_{1:n})\) satisfying \(_{_{1:n}}_{_{1:n}}=((_{1:n},_{1:n}) _{L^{2}(X;)}\)._

Again, it extends the original integral representation, and inherits the _linearization_ trick of nonlinear parameters \(\) by integrating all the possible parameters (beyond the difference of layers) and indirectly select which parameters to use by weighting on them.

## 4 Example: Depth-\(n\) Fully-Connected Network with Arbitrary Activation

As a concrete example, we present the ridgelet transform for depth-\(n\) fully-connected network. First, we show the depth-2 case based on a joint-affine-_invariant_ argument, which was originally demonstrated by Sonoda et al. . Then, we show the depth-\(n\) case based on a joint-_equivariant_ argument by extending the original arguments.

We use the following known facts.

**Lemma 3**.: _The regular representation \(\) of the affine group \((m)\) on \(L^{2}(^{m})\) (defined below) is irreducible._

See Folland [10, Theorem 6.42] for the proof.

**Lemma 4**.: _Suppose \(\) and \(\) are a tempered distribution (\(^{}\)) and a Schwartz test function, respectively. Then, \(S_{} R_{}:L^{2}(^{m}) L^{2}(^{m})\) is bounded._

See Sonoda et al. [29, Lemmas 7 and 8] for the proof.

Figure 2: Deep \(\)-valued joint-\(G\)-equivariant network on \(G\)-space \(X\) is \(L^{2}(X;)\)-universal when unitary representation \(\) of \(G\) on \(L^{2}(X;)\) is irreducible, and the distribution of parameters for the network to represent a given map \(f:X\) is exactly given by the ridgelet transform \([f]\)

[MISSING_PAGE_FAIL:7]

Example: Formal Deep Network

We explain the _formal deep network_ (FDN) introduced by Sonoda et al. . Compared to the depth-\(n\) fully-connected network introduced in the previous section, the FDN (introduced in the previous study) is more abstract because the network architecture is not specified. Yet, we consider this is still useful for theoretical study of deep networks as it covers a wide range of groups and data domains (i.e., not limited to the affine group and the Euclidean space).

### Formal Deep Network

Let \(G\) be an arbitrary locally compact group equipped with left-invariant measure \(g\), let \(X\) be a \(G\)-space equipped with left-invariant measure \(x\), and set \(:=G\) with right-invariant measure \(\). The key concept is to identify each feature map \(:X X\) with a \(G\)-action \(g:X X\) with parameter domain \(\) being identified with group \(G\), and the composite of feature maps, say \(g h\), with product \(gh\). Since a group is closed under its operation by definition, the proposed network can represent literally _any depth_ such as a single hidden layer \(g\), double hidden layers \(g h\), triple hidden layers \(g h k\), and infinite hidden layers \(g h\). Besides, to lift the group action on a linear space, the network is formulated as a regular action of group \(G\) on a hidden layer, say \( L^{2}(X)\).

**Definition 6** (Formal Deep Network).: For any functions \( L^{2}(X)\) and \(:\), put

\[[;](x):=_{G_{1} G_{n}}( _{1},,_{n})\;_{n}_{1}(x) _{1}_{n}, x X.\] (27)

Here, \(G=G_{1} G_{n}\) denotes the semi-direct product of groups, suggesting that the network gets much complex and expressive as it gets deeper.

To see the universality, define the dual action of \(G\) on the parameter domain \(=G\) as

\[g:= g^{-1}, g G,.\] (28)

Then, we can see \((x,):=(x)\) is joint-\(G\)-_invariant_. In fact,

\[(g x,g)=(g)(g x)=( g^{ -1})(g(x))=(x)=(x,).\]

Therefore, by Theorem 3, assuming that the regular representation \(:G(L^{2}(X))\) is irreducible, the ridgelet transform is given by

\[[f](_{1},,_{n})=_{X}f(x) _{1}(x)}x,(_{1},,_{n}) G_{1}  G_{n}\] (29)

satisfying \(=(\!(,)\!)_{L^{2}(X)}\).

### Depth Separation

To enjoy the advantage of abstract formulation, we discuss the effect of depth. For the sake of simplicity, we assume \(G\) to be a finite group, which may be acceptable given that the data domain \(X\) in practice is often discretized (or coarse-grained) into finite sets of representative points, say \(X:=\{x_{i}\}_{i=1}^{p}\), and if so the \(G\)-action is also reduced to finite representative actions.

Following the concept of the formal deep network, we call group \(G\) acting on \(X\) a network. Let us consider depth-1 network \(G\) and depth-\(n\) network \(G_{1} G_{n}\) satisfying \(G=G_{1} G_{n}\). The equation indicates that two networks have the same expressive power, because they can implement the same class of maps \(g:X X\).

Next, let us define the _width_ of a single layer \(G\) as the cardinality \(|G|\). This is reasonable because the set \(G\) parametrizes each map \(g:X X\). Then, under the assumption that each \(G_{i}\) is simple, the depth-\(n\) network \(G_{1} G_{n}\) can express the same class of depth-1 network exponentially-effectively, because the total widths are \(_{i=1}^{n}|G_{i}|=O(n)\) for depth-\(n\) and \(_{i=1}^{n}|G_{i}|= O(n)\) for depth-\(1\). This estimate can be interpreted as the classical thought that the hierarchical models such as deep networks can represent complex functions combinatorially more efficient than shallow models.

Discussion

We have developed a systematic method for deriving a ridgelet transform for a wide range of learning machines defined by joint-group-equivariant feature maps, yielding the universal approximation theorems as corollaries. The previous results by Sonoda et al.  was limited to scalar-valued joint-invariant functions, which were insufficient to deal with practical learning machines defined by composite mappings of vector-valued functions, such as deep neural networks. For example, they could only deal with abstract composite structures like formal deep network . By extending their argument to vector-valued joint-equivariant functions, we were able to deal with deep structures. Traditionally, the techniques used in the expressive power analysis of deep networks were different from those used in the analysis of shallow networks, as overviewed in the introduction. Nonetheless, our main theorem cover both deep and shallow networks from the unified perspective (joint-group-action on the data-parameter domain). Technically, this unification is due to Schur's lemma, a basic and useful result in the representation theory. Thanks to this lemma, the proof of the main theorem is simple, yet the scope of application is wide. The significance of this study lies in revealing the close relationship between machine learning theory and modern algebra. With this study as a catalyst, we expect a major upgrade to machine learning theory from the perspective of modern algebra.

### Limitations

In the main theorem, we assume the following: (1) joint-equivariance of feature map \(\), (2) boundedness of composite operator \(\), (3) irreducibility of unitary representation \(\). In addition, throughout this study, we assume (4) local compactness of group \(G\), and (5) that the network is given by the integral representation.

As discussed in the main text, satisfying (1) is much easier than (non-joint) equivariance. Also, (2) is often a textbook exercise when the specific expression is given. (3) is required for Schur's lemma, and it is often sufficient to synthesize the known results such as the one for the example of depth-\(n\) fully-connected network. (4) is quite a frequent assumption in the standard group representation theory, but it excludes infinite-dimensional groups. When formulated _natively_, nonparametric learning models including DNN can be infinite-dimensional groups. However, from the perspective of learnability, it is nonsense to consider too large a model, and it is common to assume regularity conditions such as sparsity and low rank in usual theoretical analysis. So, it is natural to impose additional regularity conditions for satisfying local compactness. (5) may be rather an advantage because there are established techniques to show the \(cc\)-universality of finite models by discretizing integral representations. Moreover, there is a fast discretization scheme called the Barron's rate based on the quasi-Monte Carlo method. On the other hand, problems like the minimum width in the field of deep narrow networks are analyses of finite parameters, and they could be a different type of parameters. Yet, the current mainstream solutions are the information theoretic method by Park et al.  and the neural ODE method by Cai , and both arguments contain the discretization of continuous models. Therefore, we may expect a high affinity with the integral representation theory.

This study is the first step in extending the harmonic analysis method, which was previously applicable only to shallow models, to deep models. The above limitations will be resolved in our future works.

## 7 Broader Impact

This work studies theoretical aspects of neural networks for expressing square integrable functions. Since we do not propose a new method nor a new dataset, we expect that the impact of this work on ethical aspects and future societal consequences will be small, if any. Our work can help understand the theoretical benefit and limitations of neural networks in approximating functions. Our work and the proof technique improve our understanding of the theoretical aspect of deep neural networks and other learning machines used in machine learning, and may lead to better use of these techniques with possible benefits to the society.