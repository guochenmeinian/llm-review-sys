# Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods

Shang Liu

Imperial College Business School

Imperial College London

s.liu21@imperial.ac.uk

&Zhongze Cai

Imperial College Business School

Imperial College London

z.cai22@imperial.ac.uk

Equal contribution.

&Xiaocheng Li

Imperial College Business School

Imperial College London

xiaocheng.li@imperial.ac.uk

###### Abstract

In this paper, we consider the uncertainty quantification problem for regression models. Specifically, we consider an individual calibration objective for characterizing the quantiles of the prediction model. While such an objective is well-motivated from downstream tasks such as newsvendor cost, the existing methods have been largely heuristic and lack of statistical guarantee in terms of individual calibration. We show via simple examples that the existing methods focusing on population-level calibration guarantees such as average calibration or sharpness can lead to harmful and unexpected results. We propose simple nonparametric calibration methods that are agnostic of the underlying prediction model and enjoy both computational efficiency and statistical consistency. Our approach enables a better understanding of the possibility of individual calibration, and we establish matching upper and lower bounds for the calibration error of our proposed methods. Technically, our analysis combines the nonparametric analysis with a covering number argument for parametric analysis, which advances the existing theoretical analyses in the literature of nonparametric density estimation and quantile bandit problems. Importantly, the nonparametric perspective sheds new theoretical insights into regression calibration in terms of the curse of dimensionality and reconciles the existing results on the impossibility of individual calibration. To our knowledge, we make the first effort to reach both individual calibration and finite-sample guarantee with minimal assumptions in terms of conformal prediction. Numerical experiments show the advantage of such a simple approach under various metrics, and also under covariates shift. We hope our work provides a simple benchmark and a starting point of theoretical ground for future research on regression calibration.

## 1 Introduction

Modern machine learning methods have witnessed great success on a wide range of tasks in the past decade for their high accuracy in dealing with various kinds of complicated data. Uncertainty quantification has played an important role in the interpretation and risk assessment of machine learning models for downstream tasks such as optimization and decision-making. People observethat the output of deep learning models tends to be over over-confident (Guo et al., 2017; Amodei et al., 2016), which inspires many recent works on uncertainty calibration for classification problems (Kumar et al., 2019; Wenger et al., 2020; Luo et al., 2022). Comparatively, there has been less systematic theoretical understanding of the uncertainty calibration for regression problems. For a regression problem, the output of a prediction model can be the prediction of the conditional mean (by minimizing mean squared error) or of the conditional median (by minimizing mean absolute error). But such a single-point prediction cannot characterize the uncertainty, which motivates the development of uncertainty calibration methods for regression problems. Some efforts have been made in order to estimate the entire distribution, which includes Bayesian ways (MacKay, 1992; Damianou and Lawrence, 2013), the frequentists' ways (Kendall and Gal, 2017; Lakshminarayanan et al., 2017; Cui et al., 2020; Zhao et al., 2020), and the nonparametric ways (Lei and Wasserman, 2014; Lei et al., 2018; Bilodeau et al., 2021; Song et al., 2019). Another easier task is to predict the quantiles rather than the whole distribution. Existing ways mainly focus on completing the task of quantile prediction in one go (Pearce et al., 2018; Thiagarajan et al., 2020; Chung et al., 2021; Takeuchi et al., 2006; Stainwart and Christmann, 2011). However, all those methods suffer from either statistical inconsistency under model misspecification or computational intractability during the training phase, or sometimes even both (see the detailed review in Appendix A).

In this paper, we suggest that previous ways of training a new quantile prediction model from scratch while discarding the pre-trained (mean) regression model may not be the best way for a quantile calibration objective, because the pre-trained model (though designed for a mean estimation objective) can be helpful for the quantile calibration. We propose a simple, natural, but theoretically non-trivial method that divides the whole quantile calibration task into two steps: (i) train a good regression model and (ii) estimate the conditional quantiles of its prediction error. Although a similar idea is applied in _split conformal prediction_(Papadopoulos et al., 2002; Vovk et al., 2005; Lei et al., 2018), the theoretical justification is still in a lack since the conformal prediction only requires an _average_ calibration guarantee (see Definition 1) that can also be achieved without this first training step at all (where the detailed review is given in Appendix A). By a careful analysis of the _individual_ calibration objective (see Definition 3), we capture the intuition behind the two-step procedure and formalize it in a mathematical guarantee. After a comprehensive numerical experiment on real-world datasets against existing benchmark algorithms, we suggest that one neither needs to estimate the whole distribution nor to train a new quantile model from scratch, while a pre-trained regression model and a split-and-subtract suffice, both theoretically and empirically. Our contribution can be summarized below:

First, we propose a simple algorithm that can estimate all percent conditional quantiles simultaneously. We provide the individual consistency of our algorithm and prove the minimax optimal convergence rate with respect to the mean squared error (Theorem 1 and 2). Our analysis is new and it largely relaxes the assumptions in the existing literature on kernel density estimation and order statistics. By showing the necessity of the Lipschitz assumption (Theorem 6), our result uses _minimal assumptions_ to reach both _finite sample guarantee_ and _individual calibration_, and our paper is the first to keep the latter two goals simultaneously up to our knowledge.

Second, we propose a two-step procedure of estimating "mean + quantile of error" rather than directly estimating the conditional quantiles, which enables a faster convergence rate both theoretically and empirically. Specifically, our convergence rate is of order \((L^{}n^{-})\), where \(L\) is the Lipschitz constant of the conditional quantile with respect to the features, \(n\) is the number of samples, and \(d\) is the dimension of feature. Since the conditional mean function and the conditional quantile function are highly correlated, one can greatly reduce the Lipschitz constant by subtracting the mean from the quantile.

Moreover, we construct several simple examples to show the unexpected behavior of the existing calibration methods, suggesting that a population-level criterion such as sharpness or MMD can be misleading. As our analysis works as a positive result on individual calibration, we also provide a detailed discussion about the existing results on the impossibility of individual calibration, illustrating that their definitions of individual calibration are either impractical or too conservative.

## 2 Problem Setup

Consider the regression calibration problem for a given pre-trained model \(\). We are given a dataset \(\{(X_{i},Y_{i})\}_{i=1}^{n}\)_independent_ of the original data that trains \((x)\). Here \(^{d}^{d}\) denotes the covariate/feature space and \(\) denotes the response/label space. The i.i.d. samples \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) follow an unknown distribution \(\) on \(\). We aim to characterize the uncertainty of the regression error \(U_{i} Y_{i}-(X_{i})\) in a model agnostic manner, i.e., without any further restriction on the choice of the underlying prediction model \(\).

Now we introduce several common notions of regression calibration considered in the literature. We first state all the definitions with respect to the error \(U_{i}\) and then establish equivalence with respect to the original \(Y_{i}\). Define the \(^{}}\) quantile of a random variable \(Z\) as \(Q_{}(Z)\{t:F(t)\}\) for any \(\) where F is the c.d.f. of \(Z\). Accordingly, the quantile of the conditional distribution \(U|X=x\) is denoted by \(Q_{}(U|X=x)\). A quantile prediction/calibration model is denoted by \(_{}(x):\) which gives the \(\)-quantile prediction of \(U\) given \(X=x\).

**Definition 1** (Marginal/Average Calibration).: The model \(_{}(x)\) is _marginally calibrated_ if

\[(U_{}(X))=\]

for any \(\). Here the probability distribution is with respect to the joint distribution of \((U,X)\).

As noted by existing works, the marginal calibration requirement is too weak to achieve the goal of uncertainty quantification. A predictor that always predicts the marginal quantile of \(U\) for any \(X=x\), i.e., \(_{}(x) Q_{}(U)\), is always marginally calibrated, but such a model does not capture the heteroscedasticity of the output variable \(Y\) or the error \(U\) with respect to \(X\).

**Definition 2** (Group Calibration).: For some pre-specified partition \(=_{1}_{K}\), the model \(_{}(x)\) is _group calibrated_ if

\[(U_{}(X)|X_{k})=\]

for any \(\) and \(k=1,...,K\).

Group calibration is a stronger notion of calibration than marginal calibrations. It is often considered in a related but different problem called _conformal prediction_(Vovk, 2012; Lei and Wasserman, 2014; Alaa et al., 2023) where the goal is to give a sharp covering set \((X)\) (or covering band if unimodality is assumed) such that \((Y(X)|X_{k}) 1-, k\). Foygel Barber et al. (2021) consider the case where the guarantee is made for all \((X_{k})\) for some \(>0\).

**Definition 3** (Individual Calibration).: The model \(_{}(x)\) is _individually calibrated_ if

\[(U_{}(X)|X=x)=\]

for any \(\) and \(x\). Here the probability distribution is with respect to the conditional distribution \(U|X\).

In this paper, we consider the criteria of individual calibration, an even stronger notion of calibration than the previous two. It requires the calibration condition to hold with respect to any \(x\) in a pointwise manner. The difference between the conformal prediction problem with group calibration and the regression calibration problem with individual calibration is that conformal prediction does not make any assumptions on the data distribution, while our lower bound result of Theorem 6 suggests that without any assumption, the convergence rate of the conditional quantile estimator can be arbitrarily slow. The conformal prediction literature considers a weaker notion of calibration than the individual calibration setting considered in this paper where some mild assumptions are made.

Finally, the following proposition says that any quantile prediction model on the error \(U\) is equivalent to a corresponding quantile prediction on \(Y\). The result is stated for the individual calibration, while a similar statement can be made for marginal calibration and group calibration as well.

**Proposition 1**.: _For any predictor \(_{}(x)\) on the \(^{}}\) quantile of \(U|X=x\), we have_

\[(Y(X)+_{}(X)X=x)= (U_{}(X)X=x).\]

### Motivating examples

In addition to the above calibration objective, existing literature also considers sharpness (the average length of confidence intervals (Zhao et al., 2020; Chung et al., 2021)) and maximum mean discrepancy (MMD) principle (Cui et al., 2020). To the best of our knowledge, all the existing works consider population-level objectives such as marginal calibration, sharpness, and MMD when training the calibration model, while we are the first work that directly aims for individual calibration. Here we use a simple example to illustrate that such population-level objectives may lead to undesirable calibration results, and in Appendix B, we elaborate with more examples on the inappropriateness of existing methods for regression calibration.

**Example 1**.: Consider \(X\) and the conditional distribution \(Y|X=x[0,x]\). Then if one aims for \(=90\%\), the outcome from sharpness maximization subject to marginal calibration is to \(_{}(x)=x\) for \(x[0,0.9]\) and \(_{}(x)=0\) for \(x(0.9,1]\). Consequently, the quantile prediction has a \(100\%\) coverage over \(x[0,0.9]\) but \(0\%\) coverage over \(x(0.9,1]\).

Generally, the population-level criteria such as sharpness or MMD may serve as performance metrics to monitor the quality of a calibration outcome. However, the inclusion of such criteria in the objective function will encourage the calibration result to be over-conservative for the low variance region (\(x[0,0.9]\)) while giving up the high variance (\(x(0.9,1]\)), which is highly undesirable for risk-sensitive applications and/or fairness considerations.

Besides, individual calibration is sometimes also necessitated by the downstream application. For example, in some decision-making problems, regression calibration serves as a downstream task where loss is measured by the pinball loss; say, the newsvendor problem (Kuleshov et al., 2018; Ban and Rudin, 2019) in economics and operations research uses pinball loss to trade-off the backorder cost and the inventory cost.

**Proposition 2**.: _The pinball loss with respect to \(\) is defined by_

\[l_{}(u_{1},u_{2})(1-)(u_{1}-u_{2})\{u_{1}>u_ {2}\}+(u_{2}-u_{1})\{u_{1}<u_{2}\}.\]

_We have_

\[Q_{}(Y|X=x)=(x)+Q_{}(U|X=x)*{arg\,min}_{y }[l_{}(y,Y)|X=x].\]

Proposition 2 serves as the original justification of pinball loss for quantile regression. More importantly, it says that a calibration method that is inconsistent with respect to individual calibration can be substantially suboptimal for downstream tasks such as the newsvendor problem.

Figure 1: Synthetic data where the underlying distribution is obtained by a combination of sine functions. The solid lines denote predicted means, the shaded area denotes predicted intervals between \(97.5\%\) and \(2.5\%\) quantiles, and the yellow dots denote a subset of real observations. The leftmost plot gives the real mean as well as oracle quantile values, while the rest two plots are predictions from different calibration models. The middle plot is produced by a Deep Ensemble (Lakshminarayanan et al., 2017) of 5 HNNs trained with 40,000 samples, which is both a common benchmark and a building block for several regression calibration methods. The rightmost plot is produced by our proposed nonparametric calibration method â€“ Algorithm 2 NRC of which the base regression model is an ordinary feed-forward regression network. The detailed setup is given in Appendix E.2.

Main Results

### Generic nonparametric quantile estimator

In this subsection, we present and analyze a simple algorithm of a nonparametric quantile estimator, which can be of independent interest itself and will be used as a building block for the regression calibration. The algorithm takes the dataset \(\{(X_{i},U_{i})\}_{i=1}^{n}\) as input and outputs a \(\)-quantile estimation of the conditional distribution \(U|X=x\) for any \(\) and \(x\).

```
0:\((0,1)\), dataset \(\{(X_{i},U_{i})\}_{i=1}^{n}\), kernel choice \(_{h}(,)\)
1: Estimate the distribution of \(U|X=x\) by (where \(_{u}\) is a point mass distribution at \(u\)) \[}_{U|X=x}=^{n}_{h}(x,X_{i})_{U_{i} }}{_{i=1}^{n}_{h}(x,X_{i})}.\]
2: Output the conditional \(^{}\) quantile by minimizing the pinball loss on \(}_{U|X=x}\) \[_{}^{}(x)=*{arg\,min}_{u}_{U }_{U|X=x}}[l_{}(u,U)].\] (1) ```

**Algorithm 1** Simple Nonparametric Quantile Estimator

The minimum of the optimization problem in Algorithm 1 is indeed achieved by the \(\)-quantile of the empirical distribution \(}_{U|X=x}\). In the following, we analyze the theoretical properties of the algorithm under the naive kernel

\[_{h}(x_{1},x_{2})=\{\|x_{1}-x_{2}\| h\}.\]

where \(h\) is the hyper-parameter for window size, and \(\|x_{1}-x_{2}\|\) denotes Euclidean distance.

**Assumption 1**.: _We assume the following on the joint distribution \((U,X)\) and \((0,1)\) of interest:_

1. _(Lipschitzness). The conditional quantile function is_ \(L\)_-Lipschitz with respect to_ \(x\)_,_ \[|Q_{}(U|X=x_{1})-Q_{}(U|X=x_{2})| L\|x_{1}-x_{2}\|, x_ {1},x_{2}.\]
2. _(Boundedness). The quantile_ \(Q_{}(U|X=x)\) _is bounded within_ \([-M,M]\) _for all_ \(x\)_._
3. _(Density). There exists a density function_ \(p_{x}(u)\) _for the conditional probability distribution_ \(U|X=x\)_, and the density function is uniformly bounded away from zero in a neighborhood of the interested quantile. That is, there exist constants_ \(\) _and_ \(\)_,_ \[p_{x}(u), x|u-Q_{}(U|X=x)|.\]

In Assumption 1, part (a) and part (b) impose Lipschitzness and boundedness for the conditional quantile, respectively. Part (c) requires the existence of a density function and a lower bound for the density function around the quantile of interest. Its aim is to ensure a locally strong convexity for the expected risk \(_{U_{U|X=x}}[l_{}(u,U)]\). Under Assumption 1, we establish consistency and convergence rate for Algorithm 1.

**Theorem 1**.: _Under Assumption 1, Algorithm 1 is statistically consistent, i.e., for any \(>0,\)_

\[_{n}(|_{}^{}(X)-Q_{} (U|X)|)=0.\]

_Furthermore, by choosing \(h=(L^{}n^{-})\), we have for sufficiently large \(n C\), when \(L>0,\)_

\[[|_{}^{}(X)-Q_{}(U|X)|^{2} ] C^{}L^{}n^{-},\]

_where \(C\) and \(C^{}\) depends polynomially on \(}\), \(}\), \(M\), and \((n)\). In addition, when \(L=0\), the right-hand-side becomes \(()\)._While Algorithm 1 seems to be a natural algorithm for nonparametric quantile regression, Theorem 1 provides the first convergence rate for such a nonparametric quantile estimator, to the best of our knowledge, in the literature of nonparametric regression; also, it is the first theoretical guarantee towards individual calibration for regression calibration. The standard analysis of nonparametric mean estimation (Gyorfi et al., 2002) cannot be applied here directly because the algorithm involves a local optimization (1). Even more challenging, the optimization problem (1) aims to optimize the quantile of the conditional distribution \(U|X=x\) but the samples used in (1) are from distributions \(U|X=x_{i}\), which causes a non-i.i.d. issue. To resolve these issues, we combine the idea of bias and variance decomposition in nonparametric analysis with a covering concentration argument for the local optimization problem. The detailed proof is deferred to Appendix H.

Theorem 1 also complements the existing results on finite-sample analysis for quantile estimators. One line of literature (Szorenyi et al., 2015; Altschuler et al., 2019) establishes the quantile convergence from the convergence of empirical processes, and this requires additional assumptions on the density function and does not permit the non-i.i.d. structure here. The quantile bandits problem also entails the convergence analysis of quantile estimators; for example, Zhang and Ong (2021) utilize the analysis of order statistics (Boucheron and Thomas, 2012), and the analysis inevitably requires a non-decreasing hazard rate for the underlying distribution. Other works (Bilodeau et al., 2021) that follow the kernel density estimation approach require even stronger conditions such as realizability. We refer to Appendix D for more detailed discussions. In comparison to these existing analyses, our analysis is new, and it only requires a minimal set of assumptions. The necessity of the assumptions can be justified from the following lower bound.

Theorem 2 rephrases the lower bound result in the nonparametric (mean) regression literature (Gyorfi et al., 2002) for the quantile estimation problem. It states that the convergence rate in Theorem 1 is minimax optimal (up to poly-logarithmic terms). It is easy to verify that the class \(^{L}\) satisfies Assumption 1. In this light, Theorem 1 and Theorem 2 establish the criticalness of Assumption 1. Furthermore, in Theorem 6 in Appendix C, we establish that the convergence rate of any estimator can be arbitrarily slow without any Lipschitzness assumption on the conditional statistics.

**Theorem 2**.: _Let \(^{L}\) be the class of distributions of \((X,U)\) such that \(X^{d}\), \(U=(X)+N\), and \((x)\) is \(L\)-Lipschitz where \(N\) is an independent standard Gaussian random variable. For any algorithm, there exists a distribution in the class \(^{L}\) such that the convergence rate of the algorithm is \((L^{}n^{-})\)._

Our result here provides a guarantee for the estimation of \(Q_{}(U|X)\) and thus a positive result for individual calibration with respect to a specific \(\). We note that the result does not contradict the negative results on individual calibration (Zhao et al., 2020; Lei and Wasserman, 2014). Zhao et al. (2020) measure the quality of individual calibration via the closeness of the distribution \(_{X}(Y)\) to the uniform distribution. In fact, such a measurement is only meaningful for a continuous distribution of \(Y|X\), but they prove the impossibility result based on a discrete distribution of \(Y|X\). So, their negative result on individual calibration does not exclude the possibility of individual calibration for a continuous \(Y|X\). Lei and Wasserman (2014) require an almost surely guarantee based on finite observations and prove an impossibility result. This is a very strong criterion; our results in Theorem 1 are established for two weaker but more practical settings: either the strong consistency in the asymptotic case as \(n\) or a mean squared error guarantee under finite-sample. We defer more discussions to Appendix C.

### Regression calibration

Now we return to regression calibration and build a calibrated model from scratch. Specifically, Algorithm 2 splits the data into two parts; it uses the first part to train a prediction model and the second part to calibrate the model with Algorithm 1. The first part can be skipped if there is already a well-trained model \(\) where it becomes a recalibration problem.

The theoretical property of Algorithm 2 can be established by combining the results from Section 3.1 with Proposition 1. In Algorithm 2, the quantile calibration allows full flexibility in choosing the regression model \(\) and does not require an associated uncertainty model to proceed. The motivation for calibrating the error \(U_{i}\) instead of the original \(Y_{i}\) is two-fold: First, if one treats \(Y_{i}\) itself as \(U_{i}\) and applies Algorithm 1, then it essentially restricts the prediction model to be a nonparametric one. Second, the reduction from \(Y_{i}\) to \(U_{i}\) gives a better smoothness of the conditional distribution (asmaller Lipschitz constant in Assumption 1 (a)). And thus it will give a faster convergence rate. We remark that Algorithm 2 induces an independence between the training of the prediction model and that of the calibration model. The design is intuitive and important because otherwise, it may result in predicting smaller confidence intervals both theoretically and empirically.

```
0: Dataset \(\{(X_{i},Y_{i})\}_{i=1}^{n}\), kernel choice \(_{h}(,)\), \(\)
1: Split the dataset into half: \(_{1}=\{(X_{i},Y_{i})\}_{i=1}^{n}\), \(_{2}=\{(X_{i},Y_{i})\}_{i=n_{1}+1}^{n}\).
2: Use \(_{1}\) to train a regression model \(\).
3: Calculate the estimation error of \(\) on \(_{2}\): \(U_{i}=Y_{i}-(X_{i})\), \(i=n_{1}+1,,n\).
4: Run Algorithm 1 on the data \(\{(X_{i},U_{i})\}_{i=n_{1}+1}^{n}\) and obtain \(_{}^{}()\).
5: Return \(()+_{}^{}()\). ```

**Algorithm 2** Nonparametric Regression Calibration

### Implications from the nonparametric perspective

The nonparametric approach gives a positive result in terms of the possibility of individual calibration, but it pays a price with respect to the dimensionality \(d\). The following theorem states that such a dimensionality problem can be addressed under conditional independence. And more generally, it provides a guideline on which variables one should use to perform calibration tasks.

**Theorem 3**.: _Suppose \(U=Y-(X)\). Suppose \(Z=m(X)\), where \(m\) is some measurable function, and \(\) is a \(d_{0}\)-dimensional subspace of \(\). If \(X\) and \(U\) are mutually independent conditioned on \(Z\) (i.e. \(X Z U\) is a Markov chain), then it is lossless to perform calibration using only \((U_{i},Z_{i})\)'s. In particular, applying Algorithm 1 on \((U_{i},Z_{i})\)'s will yield the same consistency but with a faster convergence rate of \(O(n^{-+2}})\)._

Theoretically, one can identify such \(Z\) by independence test for each component of \(X\) and \(Y-(X)\) on the validation set. Some other practical methods such as random projection and correlation screening selection are shown in Section 4. When the conditional independence in Theorem 3 does not hold, it is still not the end of the world if one calibrates \((U_{i},Z_{i})\). The following theorem tells that conditional calibration with respect to \(Z\), as an intermediate between marginal calibration and individual calibration, can be achieved.

**Theorem 4**.: _Suppose \(U=Y-(X)\) and \(Z\) is a sub-feature of \(X\). Without loss of generality, we assume that \(Z=_{d_{0}}(X)\), where \(_{d_{0}}\) projects from \(\) onto a \(d_{0}\)-dimensional subspace \(\). Then_

\[(U Q_{}(U|Z)\ Z)=,\]

_which implies that \((Y(X)+Q_{}(U|Z)\ Z)=\)._

A notable implication from the above theorem is that if we want to calibrate the model against certain sub-features such as age, gender, geo-location, etc., we can summarize these features in \(Z\) and use Algorithm 1 with respect to \((U_{i},Z_{i})\).

However, if we return to the original pinball loss, the following theorem inherits from Proposition 2 and states that the inclusion of more variables will always be helpful in terms of improving the pinball loss. It highlights a trade-off between the representation ability of the quantile calibration model (inclusion of more variables)and its generalization ability (slowing down the convergence rate with higher dimensionality).

**Theorem 5**.: _Suppose \(U=Y-(X)\). Suppose \(Z^{(d_{1})}\) and \(Z^{(d_{2})}\) are the first \(d_{1}\) and \(d_{2}\) components of \(X\). Suppose \(Q_{}(F_{Z^{(d_{i})}})\) is the true \(\)th quantile of \(U\) conditioned on \(Z^{(d_{i})}\), \(i=1,2\). Suppose \(Q_{}(F_{})\) is the true \(\)th marginal quantile of \(U\). If \(0<d_{1}<d_{2}<d\), then_

\[[l_{}((X)+Q_{}(U),Y)] [l_{}((X)+Q_{}(U|Z^{(d_{1})}),Y)]\] \[[l_{}((X)+Q_{}(U|Z^{(d_{2})}),Y)] [l_{}((X)+Q_{}(U|X),Y)].\]Numerical Experiments

In this section, we evaluate our methods against a series of benchmarks. We first introduce the evaluation metrics that are widely considered in the literature of uncertainty calibration and conformal prediction, including Mean Absolute Calibration Error (MACE) which calculates the average absolute error for quantile predictions from 0.01 to 0.99; Adversarial Group Calibration Error (AGCE) which finds the sub-group of the test data with the largest MACE; Check Score which shows the empirical pinball loss; Length which measures the average length for the constructed 0.05-0.95 intervals; and Coverage which reflects the empirical coverage rate for those 0.05-0.95 intervals. Formal definitions of these calibration measurements are provided in Appendix E.1.

The benchmark methods are listed as follows (the details can be found in Appendix E.4). For the Gaussian-based methods, we implement the vanilla Heteroscedastic Neural Network (HNN) (Kendall and Gal, 2017), Deep Ensembles (DeepEnsemble) (Lakshminarayanan et al., 2017), and Maximum Mean Discrepancy method (MMD) (Cui et al., 2020). We also implement MC-Dropout (MCDrop) (Gal and Ghahramani, 2016) and Deep Gaussian Process model (DGP) (Damianou and Lawrence, 2013). For models that do not rely on Gaussian assumption, we implement the combined calibration loss (CCL) introduced in Sec. 3.2 of (Chung et al., 2021). We also implement post-hoc calibration methods, such as isotonic regression (ISR) suggested by (Kuleshov et al., 2018) with a pre-trained HNN model. Another post-hoc method named Model Agnostic Quantile Regression (MAQR) method in Sec. 3.1 of Chung et al. (2021) is also implemented. For conformal prediction algorithms, we implement Conformalized Quantile Regression (CQR) (Romano et al., 2019) and Orthogonal Quantile Regression (OQR) (Feldman et al., 2021).

As our proposed nonparametric regression calibration (NRC) algorithm is agnostic with respect to the underlying prediction model, we implement a feed-forward neural network and a random forest model as the prediction model for NRC. Also, we apply the dimension reduction idea (called NRC-DR) including random projection and covariate selection where all technical details are given in Appendix E.3 and E.5. All codes are available at https://github.com/ZhongzeCai/NRC.

In the following subsections, we summarize our results on UCI datasets, time series, and high-dimensional datasets. Additional experiments on covariate shift are deferred to E.7.

### UCI dataset experiments

We evaluate our NRC algorithms on the standard 8 UCI datasets Dua and Graff (2017). Part of the experiment results on representative benchmark models is given in Table 1, and the full results are presented in Appendix E.5 due to space limit. Additional details on dataset description, hyperparameter selection, and early stopping criteria are given in Appendix E.5.

The result supports the competitiveness of our algorithm. In terms of MACE, our methods achieve the minimum loss on 6 out of 8 datasets, with the non-optimal results close to the best performance seen on other benchmarks. The rest metrics also witness our algorithms outperforming the benchmark models, with a superior result on 6 datasets in terms of AGCE, and 4 in terms of CheckScore. Note that even for the rest 4 datasets where our NRC algorithm does not achieve the best CheckScore (the empirical pinball loss), our performance is still competitive compared to the best score, while for some specific datasets (such as Energy and Yacht) we achieve a significant improvement over the CheckScore, which verifies the potential of our methods for the related downstream tasks. As for the Length metric (which evaluates the average interval length of a symmetric \(90\%\) confidence interval), it is not easy to trade-off between the sharpness requirement and the coverage rate due to the fact that one can trivially sacrifice one for the other, so we omit the direct comparison. However, we can still observe that our NRC algorithm behaves reasonably well, with a stable coverage rate performance and a relatively sharp interval length amongst the rest benchmarks.

### Bike-sharing time series dataset

This experiment explores the potential of our algorithm to quantify the uncertainty of the time series objective, where independent variables are aggregated in a sliding window fashion from the past few days. The Bike-Sharing dataset provided in Fanaee-T and Gama (2014) is used to visually evaluate our proposed algorithm. Due to the limitations in the representation ability of the feed-forward structure, we instead deploy LSTM networks for the time series data. We design LSTM-HNN,which is an ordinary LSTM network followed by a fully connected layer with \(2\) output units; and LSTM-NRC, whose regression model shares the same LSTM setting as LSTM-HNN except for the ending layer having only one output unit (without variance/uncertainty unit). For more details, see Appendix E.6.

   Dataset & MNRC & MCDP & DeepDifference & ISR & MAQR & CRR & CRR & NRC & NRC+DR \\   & MACE & 0.1240.03 & 0.0661.008 & 0.0571.003 & 0.0561.00 & 0.0691.002 & 0.0496.002 & 0.0565.002 \\  & MACE & 0.1240.03 & 0.0661.008 & 0.0571.003 & 0.0571.002 & 0.0571.002 & 0.0571.002 & 0.0571.002 & 0.0571.002 \\  & Logistic(Concept) & 0.1240.03 & 0.171.03 & 0.1751.03 & 0.1251.02 & 0.1240.04 & 0.119.012 & 0.11-0.02 \\  & Logistic(Concept) & 8.621.09 & 8.51.099 & 7.9108.095 & 9.11.0189 & 7.110.032 & 0.1019.019 & 0.741.019(x) \\   & MACE & 0.0781.02 & 0.0858.03 & 0.0441.002 & 0.0841.003 & 0.06971.003 & 0.0571.002 & 0.0571.002 & 0.0571.002 & 0.0571.002 \\  & Logistic(Concept) & 3.561.04 & **1.91.03** & 2.31.66 & 0.140.002 & 0.124.004 & 0.114.02 & 0.124.04 \\  & Logistic(Concept) & 25.469.09 & 18.013.015 & 20.088.099 & 71.247.076 & 20.240.0098 & 21.08.0003 & 19.0925.0025 \\   & MACE & 0.160.02 & 0.0660.040 & 0.0538.010 & 0.0636.010 & 0.0575.003 & **0.0571.002** & **0.0571.002** \\  & MACE & 0.125.01 & **0.125.01** & 0.125.010 & 0.125.010 & 0.125.010 & 0.125.010 & 0.240.008 \\  & Logistic(Concept) & 9.439.03 & 7.70.010 & 7.125.099 & 6.125.087 & 18.103.039 & 0.45.108 & 15.010.019 \\   & MACE & 0.0414.01 & 0.0717.005 & 0.0313.005 & 0.0531.002 & 0.0416.002 & 0.0416.002 & **0.0310.004** & 0.0471.0187 \\  & Logistic(Concept) & 0.0755.00 & 0.0751.00 & 0.1216.00 & 0.0721.002 & 0.0720.007 & 0.0720.007 & 0.0755.002 & 0.0571.002 & 0.0571.002 & 0.0571.002 \\  & Logistic(Concept) & 0.500.03 & 0.294.000 & 0.0691.00 & 0.2840.000 & 0.0721.004 & 0.0426.000 & 0.0571.002 & 0.0571.002 & 0.0571.002 \\   & MACE & 0.160.07 & 12.016 & **0.01.05** & 0.0516.00 & 0.0516.00 & 0.0572.007 & 0.0512.005 & 0.0571.002 & 0.0571.002 \\  & MACE & 0.150.05 & 0.14.016 & 0.0546.00 & 0.0516.00 & 0.0581.00 & 0.0585.005 & **0.0581.008** & 0.0581.008 \\  & Logistic(Concept) & 0.

Figure 2 visually compares the two models on the \(95\%\) confidence interval prediction task on the test set (see Appendix E.6 for how the test set is split out). A first observation is that our LSTM-NRC is producing sharper and better calibrated \(95\%\) intervals. Also, in spite of the satisfying coverage rate of both the methods on time range 0 to 100 (time stamp -1 is exactly the terminating point of the training set, thus time range 0 to 100 roughly denotes the near future of the historical events), on time range 100 to 200 the behaviors of the two models diverge. After an evolution of 100 time stamps the target variable (rental count per hour) is under a shift in distribution: the peak values get higher than historical records would foresee. Confidence intervals produced by LSTM-HNN fail to cover these "extremer" peaks, while our LSTM-NRC method could easily adapt to it, taking advantage of a high-quality regression model. Note that both LSTM-HNN and LSTM-NRC share the same regression architecture, we argue that the additional variance unit of LSTM-HNN may hurt the mean prediction performance of the model.

### Higher-dimensional datasets

One major drawback of the non-parametric type methods is its unsatisfying dependence on the dimension of the data. Our paper attacks such an obstacle from two aspects simultaneously: on the one hand, we subtract a regression model from the original data to gain a smaller Lipschitz constant \(L\); on the other hand, we apply several dimensional reduction methods to reduce the data dimension manually. In this paper, we mainly implement three of them: random projection, covariate selection, and second-to-last layer feature extraction, of which the details can be found in Appendix E.3. We call the variants of our NRC algorithm NRC-RP, NRC-Cov, and NRC-Embed, respectively. We examine them against several high-dimensional datasets: the medical expenditure panel survey (MEPS) datasets [panel 19, 2017, panel 20, 2017, panel 21, 2017], as suggested in [Romano et al., 2019]. The datasets' details are also deferred to Appendix E.3.

In Table 2 we summarize the dimension reduction variants of our NRC algorithm against several benchmarks. The result sees a substantial advantage of our method over the selected benchmark methods, while most of the unshown benchmarks either are too computationally expensive to run or do not deliver competitive results on the high-dimensional dataset.

## 5 Conclusion

In this paper, we propose simple nonparametric methods for regression calibration, and through its lens, we aim to gain a better theoretical understanding of the problem. While numerical experiments show the advantage, we do not argue for a universal superiority of our methods. Rather, we believe the contribution of our work is first to give a positive result for individual calibration and an awareness of its importance, and second to show the promise of returning to simple statistical estimation approaches than designing further complicated loss functions without theoretical understanding. Future research includes (i) How to incorporate the uncertainty output from a prediction model with our nonparametric methods; (ii) While we do not observe the curse of dimensionality for our methods numerically and we explain it by the reduction of Lipschitzness constant, it deserves more investigation on how to adapt the method for high-dimensional data.

   Dataset & Metric & DeepEnsemble & ISR & CQR & OQR & NRC-RP & NRC-Embed \\   & MACE & 0.26\(\)0.04 & 0.12\(\)0.05 & 0.05\(\)0.01 & 0.055\(\)0.02 & 0.024\(\)0.01 & **0.0087\(\)0.006** \\  & AGC & 0.26\(\)0.04 & 0.14\(\)0.04 & 0.079\(\)0.01 & 0.091\(\)0.03 & 0.062\(\)0.02 & **0.04\(\)0.02** \\  & CheckScore & 25\(\)7 & 10\(\)6 & **2.4\(\)0.2** & 2.5\(\)0.2 & 3.2\(\)0.4 & 2.4\(\)0.7 \\  & Length(Covorage) & 230\(\)140(99\%) & 50\(\)69\%) & 28\(\)2(89\%) & 25\(\)2(85\%) & 35\(\)8(90\%) & 24\(\)0.50\%) \\   & MACE & 0.25\(\)0.02 & 0.16\(\)0.08 & 0.046\(\)0.01 & 0.057\(\)0.024 & **0.0052\(\)0.004** & 0.0058\(\)0.005 \\  & AGC & 0.25\(\)0.02 & 0.18\(\)0.08 & 0.061\(\)0.01 & 0.08\(\)0.03 & **0.033\(\)0.01** & 0.041\(\)0.007 \\  & CheckScore & 23\(\)8 & 7.41\(\)1 & 2.5\(\)0.1 & 2.5\(\)0.1 & 3.4\(\)0.4 & **2.4\(\)0.08** \\  & Length(Covorage) & 190:50\%(99\%) & 69:38\%(94\%) & 28:28\%(98\%) & 27:48\%(85\%) & 32\%(90\%) & 27:31\%(91\%) \\   & MACE & 0.25\(\)0.02 & 0.13\(\)0.1 & 0.05\(\)0.02 & **0.0087\(\)0.004** & 0.012\(\)0.002 & 0.0088\(\)0.005 \\  & AGC & 0.26\(\)0.02 & 0.15\(\)0.1 & 0.063\(\)0.008 & 0.075\(\)0.02 & 0.045\(\)0.02 & **0.041\(\)0.01** \\   & CheckScore & 36\(\)20 & 23\(\)10 & **2.5\(\)0.3** & 2.6\(\)0.3 & 3.5\(\)0.7 & 2.7\(\)1 \\   & Length(Covorage) & 160\(\)40\%(99\%) & 43:20\%(93\%) & 27:1\%(86\%) & 29:3\%(93\%) & 39:3\%(90\%) & 24\(\)2.091\%) \\   

Table 2: Experiments on MEPS Datasets