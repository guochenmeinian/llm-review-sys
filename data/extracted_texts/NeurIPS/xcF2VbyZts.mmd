# SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization

Wanhua Li\({}^{*,1}\) Zibin Meng\({}^{*,1,2}\) Jiawei Zhou\({}^{3}\) Donglai Wei\({}^{4}\) Chuang Gan\({}^{5,6}\) Hanspeter Pfister\({}^{1}\)

\({}^{1}\)Harvard University \({}^{2}\)Tsinghua University \({}^{3}\)Stony Brook University

\({}^{4}\)Boston College \({}^{5}\)MIT-IBM Watson AI Lab \({}^{6}\)UMass Amherst

 Equal contribution.

###### Abstract

Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images. While current methods adopt the paradigm of training a dedicated network end-to-end using labeled image data, they are limited in terms of generalizability and interpretability. To address these issues, we first present a simple yet well-crafted framework named SocialGPT, which combines the perception capability of Vision Foundation Models (VFMs) and the reasoning capability of Large Language Models (LLMs) within a modular framework, providing a strong baseline for social relation recognition. Specifically, we instruct VFMs to translate image content into a textual social story, and then utilize LLMs for text-based reasoning. SocialGPT introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps. Without additional model training, it achieves competitive zero-shot results on two databases while offering interpretable answers, as LLMs can generate language-based explanations for the decisions. The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired. As we essentially convert a visual classification task into a generative task of LLMs, automatic prompt optimization encounters a unique long prompt optimization issue. To address this issue, we further propose the Greedy Segment Prompt Optimization (GSPO), which performs a greedy search by utilizing gradient information at the segment level. Experimental results show that GSPO significantly improves performance, and our method also generalizes to different image styles. The code is available at https://github.com/Mengzibin/SocialGPT.

## 1 Introduction

Social relationships are of paramount importance in our lives, as they significantly impact our emotional, psychological, and physical well-being. Social relationship recognition aims to categorize the relationships such as friends, colleagues, band members, and so on, that exist between individuals given an input image and the bounding boxes of the two persons of interest . In recent years, social relationship recognition has garnered significant attention  due to its wide range of applications, including product recommendation , autonomous systems , and more.

Over the past decade, the field of computer vision has witnessed tremendous success  in the end-to-end learning framework, which trains a dedicated neural network end-to-end on a customized dataset. Research in social relationship recognition has also followed a similar trajectory . As social relationship reasoning represents a cognitive function that operates at a higher level than visual perception, many methods  incorporate rich prior knowledge of social relations into the models. For example, GRM  integrated a knowledge graph into its model to leverage theinformation of contextual objects. GR\({}^{2}\)N  and TRGAT  exploit the logical constraints among multiple social relationships within the same scene. While these methods have achieved notable results, they are limited in terms of generalization and interpretability. In other words, we cannot trust that the trained models can generalize to arbitrary scenarios, and these models fail to provide the reasons and explanations for their decisions.

In this paper, we first present a modular framework with foundation models for social relation reasoning. Recently, we have witnessed the significant success of foundational models . Many Vision Foundation Models (VFMs) can accurately perform basic visual perception tasks such as identifying "what" and "where" in images [16; 17; 18; 19]. On the other hand, the emergence of Large Language Models (LLMs) demonstrates strong reasoning capabilities [20; 21; 22; 23]. Therefore, we present a framework that follows the "perceive with VFMs, reason with LLMs" paradigm. This framework first employs VFMs to convert images into textual data, and subsequently leverages the textual reasoning capabilities of LLMs for relation prediction. In this process, VFMs process visual signals into fundamental facts, and then LLMs analyze these facts to make explainable inferences.

Our framework performs visual reasoning for **Social** relationship recognition using **GPT**-style LLMs, coined SocialGPT. SocialGPT introduces systematic design principles to guide and adapt VFMs and LLMs for social relationship reasoning. Specifically, in the perception phase, we extract both comprehensive and domain-specific visual information with VFMs, which is further fused into a coherent textual social story with symbol-based object reference and is easily readable. In the reasoning phase, we utilize a structured social relation reasoning prompt, named SocialPrompt, composed of different segments for "system, expectation, context, and guidance" to better instruct LLMs. With the proposed systematic design principles, our SocialGPT provides a strong baseline and achieves highly competitive zero-shot results, compared to the state-of-the-art methods that undergo end-to-end training on full training datasets.

Lastly, we observed that LLMs exhibit high sensitivity to prompts during the reasoning process, but the manual prompt design is a time-consuming and labor-intensive task [24; 25]. We propose the Greedy Segment Prompt Optimization (GSPO) algorithm for automatic prompt tuning. As we convert a visual classification task as a generative task of LLMs, automatic prompt tuning for SocialPrompt encounters the long prompt optimization issue. Our proposed GSPO addresses these issues by utilizing gradient information at the segment level for greedy search. Experiments demonstrate that GSPO significantly improves the performance of LLMs. Figure 1 visualizes our paradigm. To summarize, we make the following contributions: 1). We present a simple modular framework with foundation models for social relation reasoning, which provides a strong baseline as the first zero-shot social relation recognition method. 2). To address the long prompt optimization issue associated with visual reasoning tasks, we further propose the Greedy Segment Prompt Optimization, which performs a greedy search on the segment level with gradient guidance. 3). Experiments demonstrate that our method attains very competitive and explainable zero-shot results without additional model training. With GSPO, our method significantly outperforms the state-of-the-art methods.

## 2 Related Work

**Foundation Models.** Recently, we have witnessed the tremendous success of foundational models [19; 26; 27; 28; 29]. Foundation models are typically trained on massive data, possess a large number

Figure 1: (a) End-to-end learning-based framework for social relation reasoning. A dedicated neural network is trained end-to-end with full training data. (b) We propose a modular framework with foundation models for social relation reasoning. Our proposed SocialGPT first employs VFMs to extract visual information into textual format, and then perform text-based reasoning with LLMs, using either our manually designed SocialPrompt or optimized prompts.

of model parameters, and exhibit excellent performance along with strong generalization capabilities . The emergence of LLMs [15; 27; 30; 31] has significantly reshaped the field of Natural Language Processing (NLP). ChatGPT and GPT-4 , developed by OpenAI, are among the most famous LLMs. GPT-4, in particular, demonstrates a strikingly close-to-human-level intelligence . Meanwhile, many open-source LLMs like Vicuna , LLaMa , and LLaMa-2  have been developed, and have achieved outstanding performance across variety tasks. On the other hand, VFMs [19; 26; 35; 36; 37; 38] have also made significant advancements. CLIP  connects images and text, enabling zero-shot image classification [39; 40]. BLIP  and BLIP-2  demonstrate strong zero-shot image-to-text generation capabilities. SAM  offers a foundation model for image segmentation . While foundation model-based frameworks have been proposed for many other tasks including few-shot visual recognition [43; 44; 45], visual question answering [46; 47; 48], and semantic segmentation , our SocialGPT explicitly employs text as the bridge between VFMs and LLMs and then proposes symbol-based referencing to support unambiguous text queries.

**Social Relation Recognition.** Social psychologists have conducted extensive research on social relationships over decades [50; 51], resulting in several different social theories [52; 53]. Sun _et al._ followed Bugental's domain-based theory  and annotated the PIPA dataset, which has become one of the most popular benchmarks for social relation recognition. Li _et al._ adopted the relational models theory  and contributed the People in Social Context (PISC) dataset. A dual-glance model was further proposed to leverage multiple contextual regions. With the well-established benchmarks, numerous end-to-end methods [54; 3; 14; 2] have been proposed, effectively advancing the field of social relationship recognition. Some methods [54; 6] employed knowledge graphs to exploit scene and global contextual cues. Noticing that there usually are multiple social relations on the same image, Li _et al._ proposed GR\({}^{2}\)N to jointly infer all relations on an image with graph neural networks. TRGAT  further considered higher-order constraints for social relations on an image and achieved better results. These methods adopted the end-to-end learning-based paradigm, whereas we propose a modular framework with foundation models.

## 3 SocialGPT

Social relation recognition takes an image \(\) and two bounding boxes \(_{1}\) and \(_{2}\) of two interested individuals as inputs, and requires a model that outputs the social relationship \(\). We first introduce a modular framework with foundation models for social relation recognition in this section, which provides a strong zero-shot baseline. The pipeline is illustrated in Figure 2. On a high level, we first use VFMs to extract visual information at different granularities. The raw information is then fused into a coherent _social story_ in textual format, denoted as \(\), which can be best reasoned with LLMs.

Figure 2: The framework of SocialGPT, which follows the “perception with VFMs, reasoning with LLMs” paradigm. SocialGPT converts an image into a _social story_ in the perception phase, and then employs LLMs to generate explainable answers in the reasoning phase with SocialPrompt.

### Perception with Vision Foundation Models

The perception objective is to extract essential visual information related to social relation reasoning, in order to connect with text-based LLMs for downstream reasoning. One straightforward approach is to utilize existing image captioning foundation models such as BLIP-2  to generate a caption or GPT-4V  to generate an image description. However, a single sentence or general-purpose description may overlook crucial details relevant to social relations present in the images.

We construct text-based visual information with VFMs with being both **comprehensive** and **domain-specific** as our guidelines. To achieve this, we resort to the state-of-the-art image segmentation tool, the Segment Anything Model (SAM) , and the powerful vision-language foundation model, BLIP-2 , for both identifying important details in the image and describing them in language. In particular, we use SAM to segment the image to obtain all different object masks, and then send individual objects by masking out others to BLIP-2 to obtain descriptions of each object. Together with the image-level caption, we formulate the _dense captions_ covering all objects in the input image.

The above gives us a comprehensive description of the image details. However, holistic captions of the image and different objects are not tailored to our task of social relation reasoning. To compensate for the lack of domain-specific information, we ask specific questions related to social identities by using the BLIP-2 dialog functionality to extract more specific information depending on object types. Recent research [54; 1] has shown that the age and gender of individuals, as well as the social scene and activity, are important clues. Therefore, we actively inquire BLIP-2 about these clues. Specifically, when dealing with people objects, we inquire about age and gender details. This information is crucial for distinguishing familial relationships within a family unit, such as father-child and grandmother-grandchild relationships. For image-level captions, we explore the social scenario or event depicted in the picture. This approach allows us to generate _task-oriented captions_ that are tailored to our social relation recognition objective.

### Social Story Generation

One could directly input the dense captions and task-oriented captions along with object axes and dimensions into LLMs for social relation reasoning, but the information is fragmented and objects are described in isolation. On the other hand, LLMs perform the best when working with human-readable natural language and they often struggle with arithmetic reasoning tasks [56; 57; 58]. Therefore, we integrate the aforementioned vision information by composing a social story that is complete and coherent. Objects are conveniently **referable** and described in relative relations, and the full story is easily **readable** by both humans and LLMs. This will serve as a crucial bridge from visual perception to textual reasoning, providing a solid foundation for the next step of understanding with LLMs.

We propose _symbol-based referencing_ for object referral. Multiple individuals and various social relationships coexist in a single image, and bounding boxes \(_{1}\) and \(_{2}\) are provided for specific relation inquiries in supervised learning settings. However, as we now convert the entire image into textual data and rely on LLMs for analysis, effective referral of individual objects becomes a critical question. Based on SAM segmentation masks, we can naturally derive bounding boxes for each object \(i\) as \(_{i}=[x_{i},y_{i},h_{i},w_{i}]\), where \((x_{i},y_{i})\) is the center coordinate and \((h_{i},w_{i})\) are the height and width. While directly using these coordinates for referrals in the social story and question inquiries is precise, they pose extra challenges for readability and numerical reasoning for LLMs. Instead, we assign _symbols_ to each object to associate with its coordinates in the original image, textual caption, and task-specific features for our social story generation. We use \(P_{i}\) to refer to people objects, and \(O_{i}\) to refer to other objects. Numerical coordinates will not appear in our social story, and relative positional relations are described with the referral symbols. The symbol-based referring also enables straightforward querying for LLMs. For instance, one can directly inquire LLMs about the social relationship between \(P_{2}\) and \(P_{3}\) with natural language and LLMs will easily identify the queried persons associated with symbols. This provides a clear and concise bridge between the object descriptions and the bounding box-based queries, and a similar method can be adopted for a broader range of applications when text-based reasoning is involved for object referral for visual question answering, robotics, etc.

Finally, based on the list of isolated image and object descriptions after symbol-based referencing, we instruct an LLM to act as an information fusion tool for generating a coherent social story \(\) in a unified paragraph. The social story tells all the information needed about the visual scene fortext-based reasoning, which is highly readable and understandable by humans and LLMs with clear symbol references and information consolidation. An example of extracted perceptual information with symbol associations and the generated social story is depicted in Figure 3.

### Reasoning with Large Language Models

After obtaining the mapping from image to social story: \(\), we feed both \(\) and bounding box queries \((_{i},_{j})\), converted to textual queries \(\) with referencing symbols \(P_{i},P_{j}\), into LLMs to obtain interpretable answers \(\). This is to let LLMs output the map from \((,)\) to \(\), which we do by prompting. Since LLM performance is highly sensitive to prompt variations [59; 55], we design our social relation reasoning prompt with four segments, which we name SocialPrompt.

**System.** This is the system prompt provided by many LLMs to steer their behavior. We utilize it to explicitly define several core rules for our task of social reasoning. We denote it as the \(\) segment.

**Expectation.** This is the instruction that we give to the model to set expectations of the anticipated outcomes. This helps avoid vague or unexpected outputs. To do so, we construct a role assignment and task description prompt, denoted as \(\), where we explicitly assign the role of a social relation expert to the LLM and provide a detailed elaboration of the task's input and output.

**Context.** This provides sufficient contextual information to help the LLMs understand the background of the problem. As a classification task, we provide specific definitions for each social relationship category, resulting in the prompt segment denoted as \(\).

**Guidance.** This offers an exemplar to show the LLMs how to respond to a query based on a social story. In-context learning has been proven as an effective means to expand the capabilities of LLMs [60; 61; 62]. We manually construct an in-context example prompt, denoted as \(=(_{0},_{0},_{0})\), to better guide LLMs in performing social relationship reasoning in the desired format. Here we also guide the model to generate possible explanations for its prediction. While using more in-context examples may potentially further enhance performance, this is beyond the scope of the paper and is left as future work.

The final SocialPrompt consists of \((,,,)\), and is concatenated with a testing story-query pair \((,)\) at the end for model predictions. Figure 2 shows the structured excerpts of SocialPrompt, and we put the full prompt into the Appendix. Note that we do not use any training samples provided by a dataset and only employ the foundation models. Consequently, SocialGPT is capable of zero-shot social relation reasoning, while maintaining its interpretability and generalizability.

## 4 Greedy Segment Prompt Optimization

Although we have devised well-structured SocialPrompt for social relation reasoning, experiments reveal that different ways of prompt rephrasing and demonstration example variations can significantly impact the LLM reasoning performance. Manually searching for the optimal prompt is time-consuming and labor-intensive, thus automatic prompt tuning is desired. Nevertheless, unlike the prompt optimization methods [63; 64] typically employed in NLP, automatic prompt tuning for SocialPrompt faces two unique challenges: _free-form target_ and _long prompt optimization_. As we convert a visual classification task into a generative task for LLMs, the model's output space transitions from discrete numerical representations of one-hot labels to unconstrained textual forms. Defining free-form text objectives for SocialPrompt optimization is not well-explored. Meanwhile, as the social story \(\) is a comprehensive description of the image such as in Figure 3, and task

Figure 3: An example of social story generation.

and full label set definitions could be lengthy, our SocialPrompt tends to be very long. This poses additional challenges for automatic prompt tuning methods. To address these issues, we propose a segment-based optimization algorithm, named Greedy Segment Prompt Optimization (GSPO).

**Tuning Objective.** To automate prompt searching, the first step is to define the optimization objective. Ideally, we aim to find the optimal prompt \(\{^{},^{},^{},^{}\}\) that maximize the probability of LLMs generating the correct answer \(\) for any given sample \(=(,)\). Let's first review the training paradigm commonly used for autoregressive language models , which essentially employ the next token prediction task, _i.e._, learning \(p(w_{n+1}|w_{1:n})\), where token \(w_{n+1}\), and \(\) represents the token vocabulary. Unlike typical classification tasks where only a one-hot formatted category is predicted, our answers are free-form text, consisting of a sequence of numerous tokens. Constructing the ground truth with free-form text for each sample is challenging. This paper proposes instructing LLMs to begin their response with the predicted class category following a pre-defined template. Formally, we assume that the ground truth answer \(\) for sample \(\) takes the following form: \(=[^{0},^{1},^{2},...]\), where \(^{0}\) denotes the first sentence of \(\), \(^{1}\) is the second sentence, and so forth. We specify \(^{0}\) to have the following fixed format: \(^{0}=\)"_The final answer is \(str()\)"_, where \(str()\) represents the string representation of class label \(\). Then we can define the objective:

\[(,,,;,)=-_{(, ^{0})}[ p(^{0}|,,,;)],\] (1)

where the expectation is taken from a collection of training examples, and the probabilities are computed from LLM's next token prediction distributions. Note here the LLM is frozen, and we seek to find the optimal prompt to minimize the above loss. In practice, we employ the same template in our in-context example, making it easy for LLMs to follow a consistent output format. This ensures that the loss primarily stems from LLMs' predictions of tokenized category names rather than category-agnostic sentence formatting. Note that we only construct and supervise the first sentence of the ground truth answer, while the model is free to generate its explanation in the following sentences.

**Long Prompt Optimization.** We optimize over discrete prompt tokens, constrained to a vocabulary \(\) for each token position associated with the LLM. While some discrete prompt optimization algorithms  have been proposed in the NLP field, they typically operate on a limited number of tokens. In contrast, as a visual reasoning task, we require long prompts to adequately convey the dense information and provide detailed context. In fact, the number of tokens in our SocialPrompt may well exceed 2K, and conduct token-level optimization results in a search space of \(2000^{||}\), which is beyond the capacities of current optimization methods as \(||=32,000\) for many LLMs . We propose to perform segment-level optimization as a surrogate. Formally, suppose the prompt is \(\) with \(M\) segments, denoted as \(_{1:M}\). In our case we can have \(M=4\) and directly map the segments to \(,,,\), respectively. We propose a candidate set \(_{m}\) consisting of alternative prompts for each segment, which we use ChatGPT to generate followed by light manual revisions, and the algorithm searches over the combination of different candidates. For the demonstration example segment \(\), we also manually select samples from an existing training set as candidates.

More specifically, inspired by AutoPrompt , our optimization algorithm considers all possible single-segment substitutions, thereby selecting the segment candidate that minimizes the loss over a batch of training samples. We replace one segment at a time in a greedy manner. In practice, instead of evaluating all possible candidates, we further reduce the search space by calculating the gradients of the one-hot segment indicators for each segment and selecting the top \(K\) most promising candidates for that segment. The gradient is computed as: \(_{h_{w_{m}}}(_{1:M})^{|_{m}|}\), where \(h_{w_{m}}\) represents the one-hot representation of selecting \(_{m}\) from the set \(_{m}\). Then the top \(K\) promising substitutions with the largest negative gradient are chosen for evaluation. We repeat this process to acquire \(K\) candidates for each segment, and we only replace one segment at a time to obtain \(K*M\) new prompts. Then the one with the smallest loss over a batch of training samples is chosen. We iterate this process \(N\) times to find the best-performing prompt. The entire search process is shown in Algorithm 1.

## 5 Experiments

### Settings

**Data and Evaluation.** We adopt two widely-used benchmarks for social relation reasoning: PIPA  and PISC . The PIPA dataset categorizes 16 types of social relationships, including family bonds (like parent-child, grandparent-grandchild), personal connections (friends, loves/spouses), educational and professional interactions (teacher-student, leader-subordinate), and group associations (band, sports team, colleagues). The PISC dataset categorizes social relationships into six types: commercial, couple, family, friends, professional, and no-relation. We follow the standard train/val/test split for both datasets and report the classification accuracy on the test set. Note that the training set is not used for our zero-shot results, but is used for in-context exemplar proposals for our prompt optimization algorithm. For both datasets, we measure classification accuracy as our evaluation metric.

**Implementation Details.** We use two VFM models for visual information extraction - the SAM  model for object segmentation, followed by BLIP-2  for dense caption generation. For the social story generation, we employ the GPT-3.5  Turbo model that has empowered ChatGPT. We set the temperature to 0 for greedy decoding to bolster the result's reproducibility. Other generation parameters are otherwise set as default. For subsequent reasoning of social relations based on generated stories, we experiment with both GPT-3.5 and open-source LLMs, including Vicuna-7B/13B  and Llama2-7B/13B . All the decoding temperature is set as 0, and we set the maximum context length to 4096 for Vicuna and Llama2 to accommodate our long prompt. For GSPO, we curate \(M=15\) candidates for each of the four segments within the complete prompt and set \(K=3\) for candidate selection for \(N=500\) iterations. One A100 GPU is used for all experiments.

### Zero-shot Social Relation Recognition with SocialGPT

**Main Results.** We compare SocialGPT, using either GPT-3.5 or Vicuna-13B, with previous fully supervised methods and present our results in Table 1 and Table 3. Here our method does not

   Methods & ZS & Acc (\%) \\  All attributes + SVM  & ✗ & 57.2 \\ Pair CNN  & ✗ & 58.0 \\ Dual-Glance  & ✗ & 59.6 \\ SRG-GN  & ✗ & 53.6 \\ GRM  & ✗ & 62.3 \\ MGR  & ✗ & 64.4 \\ GR\({}^{2}\)N  & ✗ & 64.3 \\ TRGAT  & ✗ & 65.3 \\  SocialGPT (w/ GPT-3.5) & ✗ & 64.1 \\ SocialGPT (w/ Vicuna-13B) & ✗ & **66.7** \\   

Table 1: The comparison results on the PIPA dataset. ZS stands for Zero-Shot.

   Methods & Acc (\%) \\  SocialGPT & **61.58** \\  - Dense Captions & 52.63 \\ - Task-oriented Captions & 59.89 \\ - Symbol \(\) Object Coordinate & 57.68 \\ - Symbol \(\) Object Caption & 59.83 \\ - Social Story & 45.31 \\  - SocialPrompt Segment \{System\} & 60.23 \\ - SocialPrompt Segment \{Expectation\} & 59.19 \\ - SocialPrompt Segment \{Context\} & 61.18 \\ - SocialPrompt Segment \{Guidance\} & 43.56 \\   

Table 2: Ablations on components of SocialGPT with Vicuna-7B. The results are obtained on the PIPA dataset with a zero-shot setting.

undergo the prompt tuning optimization, performing relation reasoning in a zero-shot fashion without utilizing any training examples. On both datasets, Vicuna-13B performs better than GPT-3.5 with our framework. In particular, on PIPA benchmark shown in Table 1, SocialGPT achieves the best accuracy compared with all prior supervised approaches, leading the previous state-of-the-art model TRGAT  by 1.4%. The results on the PISC benchmark are shown in Table 3. Most previous methods used mAP (mean Average Precision) as the metric on the PISC dataset, whereas we opted not to employ this metric due to the disparity between our predictions. Unlike previous methods that output per-class confidence scores, our prediction is the textual outputs from LLMs. Therefore, we still adopt the accuracy metric on the PISC dataset. To report the accuracy performance of other methods, we chose the state-of-the-art methods with publicly available code for reproduction and compared their performance. Table 3 shows that our method attains comparable results to the state-of-the-art GR\({}^{2}\)N model, despite not being trained with any data.

**Comparison with End-to-End VLMs.** Our approach breaks down the social relation reasoning into different phases involving perception tasks with VFMs and reasoning with LLMs, bridged by a coherent textual social story. However, recent advancements in multimodal foundation models (VLMs) provide a straightforward way of reasoning about visual contents, which is simply asking questions about the image to a vision-language model that can respond with an answer directly. We compare SocialGPT with three state-of-the-art end-to-end vision-language foundation models by directly inquiring about social relationships in the image, including BLIP-2 , LLaVA , and GPT-4V , with results shown in Table 4. We see that the method of querying vision-language foundation models, albeit simple, is still lagging behind our approach of SocialGPT with principled designs and modularized VFMs and LLMs. Our well-designed SocialGPT even outperforms the high-performing GPT-4V by 7.03% in accuracy. These results justify the design principles of our framework with comprehensive perception extraction and coherent language reasoning.

**Ablation Study.** We conduct a series of ablation studies to assess the efficacy of various components at different stages of SocialGPT. Table 2 shows the results with Vicuna-7B on the PIPA dataset. The first part of ablation focuses on the social story generation pipeline. As we use SAM to segment the image for visual perception, removing SAM would disable fine-grained object descriptions (dense captions) in the social story, resulting in an accuracy drop of more than 8%. If we do not acquire the task-oriented captions, there is a performance drop of 1.69%. Next, a crucial component of the social story generation in SocialGPT is the utilization of symbols (\(P\) for people and \(O\) for others) for effective referral of objects. If we do not use the symbols, but instead replace the object referral with either the direct coordinate or the object-specific caption from BLIP-2 in both the social story and the question, we see the performance drops by 3.90% and 1.75%, respectively. Finally, we fuse the multi-aspect visual information into a cohesive social story. If we bypass the fusion and directly utilize the visual annotations from VLMs, we can see there is a significant performance drop of 16.27%. This indicates that a good textual description of comprehensive visual information is necessary to connect LLMs to reason about social scenes presented in images.

We also ablate the SocialPrompt segments in our LLM reasoning phase. We do this by removing each of the segments from the full prompt one at a time, and results are presented in the bottom half of Table 2. We can see that guidance segmentation, which contains a manually constructed demonstration example of how to reason about social relations based on our social story, has the most influence on the model performance. Without it, the accuracy drops by 18.02%. The system

   Methods & Acc (\%) \\  BLIP-2  & 35.84 \\ LLaVA  & 45.12 \\ GPT-4V  & 59.67 \\  SocialGPT & **66.70** \\   

Table 4: Comparison with existing Vision-Language Models on the PIPA dataset, with SocialGPT using Vicuna-13B model.

   Methods & ZS & Acc (\%) \\  Pair CNN  & ✗ & 46.30 \\ GRM  & ✗ & 64.18 \\ GR\({}^{2}\)N  & ✗ & 64.70 \\  SocialGPT (w/ GPT-3.5) & ✗ & 53.43 \\ SocialGPT (w/ Vicuna-13B) & ✗ & **65.12** \\   

Table 3: The comparison results on the PISC dataset. Previous methods are replicated with open-source code to report the accuracy metric. ZS means Zero-Shot.

prompt and expectation segment contributes to the final performance by approximately 1.35% and 2.39%, respectively, and the context segment defining social relationship categories has a lesser contribution with a 0.4% accuracy difference. This is perhaps because the LLMs already have substantial knowledge of common social relationships.

### Long Prompt Optimization with GSPO

As SocialGPT utilizes fixed prompt segments to instruct LLMs for social relation reasoning based on social stories, it might not be optimal with the static prompt design. Our GSPO further tunes the long prompt on the segment level for automatic performance improvements. Table 5 presents the results when applying GSPO on SocialGPT with various LLMs for reasoning, compared with the baseline zero-shot performance. Overall our segment-level prompt tuning with GSPO helps with the classification of all model variants. On PIPA the performance boost is about 2.38% on average, and on PISC it achieves a better gain with about 3.18% on average. These show the efficacy of the proposed GSPO algorithm to efficiently enhance prompt effectiveness. Out of the model variations, Vicuna-13B consistently outperforms other LLMs under our setup. The flexibility of SocialGPT in connecting with different reasoning models makes it more easily benefit from the latest advancements of LLMs without any heavy adaptation.

### Qualitative Analysis

**Reasoning Process and Interpretability.** We illustrate the perception and reasoning process of SocialGPT as well as the final results in Figure 4. The people objects are fully segmented from VFMs and associated with symbols, which are then utilized to generate a coherent social story with clear references. By using LLMs for the reasoning on top of textual stories, SocialGPT not only outputs the correct social relations between different objects in the image but also provides plausible explanations behind the reasoning process.

**Generalization on Different Image Styles.** Previous supervised models on social relation recognition heavily rely on annotated images and relations in a specific domain. As a result, these models cannot generalize to unseen image types well. In contrast, our method does not have the limitation of being domain-specific. We apply SocialGPT to novel sketch and cartoon images with various social relations generated by GPT-4V, with results shown in Figure 5. As shown in the first example, the previous state-of-the-art model GR\({}^{2}\)N  fails to generalize as it predicts the relation between \(P_{1}\) and \(P_{2}\) as colleagues, but SocialGPT correctly recognizes the classmate relation based on the social scene with detailed explanation.

    &  &  \\   & SocialGPT & + GSPO & \(\) & SocialGPT & + GSPO & \(\) \\  Vicuna-7B & 61.58 & 62.99 & +1.41 & 45.13 & 49.79 & +4.66 \\ Vicuna-13B & **66.70** & **69.23** & +2.53 & **65.12** & **66.19** & +1.07 \\ Llama2-7B & 31.91 & 34.07 & +2.16 & 36.71 & 38.04 & +1.33 \\ Llama2-13B & 37.86 & 41.27 & +3.41 & 42.74 & 48.39 & +5.65 \\   

Table 5: Prompt tuning results (accuracy in %) with GSPO.

Figure 4: Visualization results of interpretability. We show the SocialGPT perception and reasoning process. We see that our model predicts correct social relationships with plausible explanations.

## 6 Conclusion

**Conclusion.** In this paper, we present SocialGPT, a modular framework with foundation models for social relation reasoning, which attains competitive zero-shot results while also providing interpretable explanations. Furthermore, we propose the GSPO for automatic prompt tuning, which further improves the performance. Our approach opens new avenues for exploring the synergy between vision and language models in high-level cognitive tasks and offers a promising direction for future advancements in the field of social relation recognition.

**Limitations and broader impacts.** Due to the modular nature of our approach, the performance of our method is constrained by the performance of the foundation models. If the segmentation model fails, or if the BLIP-2 model generates incorrect captions, or if the reasoning by LLMs is flawed, then our method is also prone to errors. Our method transforms visual problems into language-based reasoning, which could improve accessibility for visually impaired individuals. Meanwhile, our method also inherits biases from the foundation models, thus further research is needed to address them. Automatic classification of social relationships may lead to unintended negative consequences. To mitigate these risks, we can implement strategies such as fairness and bias checks, as well as promote transparent and responsible use of our technology.