# Derivatives of Stochastic Gradient Descent in parametric optimization

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Edouard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France. France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse School of Economics,

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Toulouse Capitole,

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck Iutzeler

Universite Paul Sabatier,

Institut de Mathematiques de Toulouse, France. &Eduard Pauwels

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

Franck_What is the dynamics of the derivatives of the iterates of stochastic gradient descent in the context of minimization of parametric strongly convex functions?_

Our motivation for studying this question is twofold. First, while iterative differentiation through SGD sequences is possibly not the most efficient way to differentiate solutions of convex programs, it is very natural in the context of differentiable programming and has already been motivated and explored in the machine learning literature (Maclaurin et al., 2015; Pedregosa, 2016; Finn et al., 2017; Ji et al., 2022). Second, existing attempts to provide stochastic oracle based methods to differentiate through convex programming solutions require more intricate algorithmic schemes than the conceptually simple iterative differentiation of SGD. Despite its conceptual simplicity, the answer to this question is not direct in the first place due to the joint effect of noise on the iterate sequence and its derivatives.

Contributions.The strongly convex setting ensures that the solution mapping is single valued and differentiable under appropriate smoothness assumptions. In this setting, we prove in Theorem 2.2 the **convergence of the derivatives of the SGD recursion toward the derivative of the solution mapping**, in the sense of mean squared errors:

\(\) We first provide a general result for non-increasing step-sizes converging to some \( 0\) (covering constant step-sizes schedules), for which we prove that the derivatives of SGD eventually fluctuate in a ball centered at the solution derivative, of size proportional to \(\).

\(\) With vanishing steps, this result implies that the derivatives of SGD converge toward the solution derivatives, and we obtain \(O((k)^{2}/k)\) convergence rates for \(O(1/k)\) step-size decay schedules.

\(\) We also study the interpolation regime, for which we show that the derivatives converge exponentially fast toward the derivative of the solution mapping.

All these results suggest that derivatives of SGD sequences behave _qualitatively_ similarly as the original SGD sequence under typical step size regimes.

The key insight in proving these results is to interpret the recursion describing **the derivatives of SGD as a perturbed SGD sequence**, or SGD with errors, related to a quadratic parametric optimization problem involving the second order derivatives at the solution of the original problem. We perform a general abstract analysis of inexact SGD recursions, that is, SGD with an additional error term which is not required to have zero mean. This constitutes a result of independent interest, which we apply to the sequence of SGD derivatives in order to prove their convergence toward the derivative of the solution mapping. The developed theory is illustrated with numerical experiments on synthetic tasks. We believe our work paves the way to a better understanding of stochastic hyperparameter optimization, and more generally stochastic meta-learning strategies.

Related works.Differentiating through algorithms is closely associated with the broader concept of _automatic differentiation_(Griewank, 1989). In practice, it is implemented using either the forward mode (Wengert, 1964), or the more common reverse mode (Rumelhart et al., 1986) known as backpropagation. For detailed surveys, see (Griewank et al., 1993) or (Griewank and Walther, 2008; Baydin et al., 2018). Modern machine learning is intrinsically linked to this idea through the use of Python frameworks like Tensorflow (Abadi et al., 2015), PyTorch (Paszke et al., 2019), and JAX (Bradbury et al., 2018; Blondel et al., 2022). When using the reverse mode, a limitation of this method is the need to retain every iteration of the inner optimization process in memory, although this challenge can be mitigated by employing checkpointing, invertible optimization algorithms (Maclaurin et al., 2015), by utilizing truncated backpropagation (Shaban et al., 2019), Jacobian-free backpropagation (Fung et al., 2022) or one-step differentiation (Bolte et al., 2023).

Along with iterative differentiation (ITD), (approximate) implicit differentiation (AID) plays an increasing important role, sometimes under the name implicit deep learning. El Ghaoui et al. (2021) highlights the utility of fixed-point equations in defining hidden features, and (Bai et al., 2019) proposes equilibrium points for sequence models, reducing memory consumption significantly. Further, (Bertrand et al., 2020; Agrawal et al., 2019) expands implicit differentiation's applications to high-dimensional, non-smooth problems and convex programs. Ablin et al. (2020) emphasizes the computational benefits of automatic differentiation, particularly in min-min optimization. In particular, OptNet (Amos and Kolter, 2017) and Deep Equilibrium Models (DEQ) (Bai et al., 2019) are examples of relevant applications.

Hypergradient estimation through iterative differentiation or implicit differentiation has a long story in machine learning (Pedregosa, 2016; Lorraine et al., 2020). In the context of imaging, iterative differentiation was used to perform hyperparameter selection through the Stein's unbiased risk estimator (Deledalle et al., 2014), and also for refitting procedure (Deledalle et al., 2017). Model-agnostic Meta-learning (MAML) was introduced by Finn et al. (2017) as a methodology to train neural architectures that adapt to new tasks through iterative differentiation (meta-learning). It was later adapted to implicit differentiation (Rajeswaran et al., 2019). These developments motivated further studies of the bilevel programming problem in a machine learning context (Franceschi et al., 2018; Grazzi et al., 2020).

The literature on the stochastic iterative and implicit differentiation is more limited. In the stochastic setting, Grazzi et al. (2021, 2023, 2024) considered implicit differentiation, mostly as a stochastic approximation to solve the implicit differentiation linear equation or use independent copies for the derivative part. In general stochastic approaches for bilevel optimization sample different batches for the iterate and derivative recursions. Here we _jointly analyze both recursion_ with the same samples. Despite this lack of systemic theoretical analysis of convergence, differentiating through the SGD iterates is mentioned in Maclaurin et al. (2015) which is focused on an efficient implementation of backpropagation through SGD, Pedregosa (2016) which explicitly calls for the development of differentiation techniques for stochastic optimization algorithms. Furhtermore Finn et al. (2017); Ji et al. (2021) suggests explicitely to use differentiation through stochastic first order solvers and this was further explicitely considered by Ji et al. (2022) in a meta-learning context.

Closely related to the general issue of differentiating parametric optimization problems is solving bilevel optimization, where the Jacobian of the inner problem is crucial to analyze. Chen et al. (2021) introduces a method, demonstrating improved convergence rates for stochastic nested problems through a unified SGD approach. In the same vein, Arbel and Mairal (2021) leverages inexact implicit differentiation and warm-start strategies to match the computational efficiency of oracle methods, proving effective in hyperparameter optimization. Additionally, the work (Ji et al., 2021) provides a thorough convergence analysis for AID and ITD-based methods, proposing the novel stocBiO algorithm for enhanced sample complexity. Furthermore, (Dagreou et al., 2022; Dagreou et al., 2024) introduce a novel framework allowing unbiased gradient estimates and variance reduction methods for stochastic bilevel optimization.

Although this is not the initial focus of this work, the technical bulk of our arguments requires an analysis of _perturbed, or inexact, SGD sequences_. This amounts to study the robustness of the stochastic gradient algorithm with non-centered noise, or equivalently non-vanishing deterministic errors. Such questioning around robustness to errors have existed for decades in the stochastic approximation literature, see for example (Ermoliev, 1983; Chen et al., 1987) and references therein. Many existing results presented in the literature are qualitative and relate to nonconvex optimization (Solodov and Zavriev, 1998; Borkar, 2009; Doucet and Tadic, 2017; Ramaswamy and Bhatnagar, 2017; Dieuleveut et al., 2023). Let us also mention the smooth convex setting for which inexact oracles have been studied by (Nedic and Bertsekas, 2010; Devolder et al., 2014). A recent account of existing convergence results for biased SGD is given by Demidovich et al. (2023). As a by-product of our arguments, we provide a general mean squared error convergence analysis of inexact SGD for a diversity of step size regimes, in the smooth, strongly convex setting. Our analysis allows to handle random non stationary bias terms, whose magnitude depend on the iteration counter \(k\). This is customary as the errors in the sequence of derivatives are due to the suboptimality of the sequence of iterates. These errors thus depend on the realization of the iterate sequence, requiring a dedicated analysis not covered by existing art Demidovich et al. (2023).

## 2 The derivative of SGD is inexact SGD

### Intuitive overview

We consider a parametric stochastic optimization problem of the form

\[x^{}()=_{x^{d}}\ F(x,) _{}[f(x,;)]\] (Opt)

where \(F^{d}\) is smooth and strongly convex in \(x\) for a fixed \(\). The stochastic gradient descent algorithm, stochastic gradient descent (SGD), is defined by an initialization \(x_{0}()\), and for \(k\)

\[x_{k+1}()=x_{k}()-_{k}_{x}f(x_{k}(), ;_{k+1})\] (SGD)where \((_{k})_{k}\) is a sequence of positive step-sizes and \((_{k})_{k}\) is a sequence of independent random variables with common distribution \(\). Precise assumptions on the problem and the algorithm will be given in Section 2.2 to ensure convergence. We highlight here that both the objective \(f(x,,)\) and the initialization of the algorithm \(x_{0}()\) depend on some parameter \(^{m}\), and so do the iterates and optimal solution.

For any \(\) and any \(k 0\), under appropriate assumptions, the Jacobian of \(x_{k}()\) w.r.t. \(\), denoted by \(_{}x_{k}()^{d m}\), is well defined and obeys the following recursion from the chain rule of differentiation:

\[_{}x_{k+1}()=_{}x_{k}()-_{k} _{xx}^{2}f(x_{k}(),;_{k+1})_{}x_{k}()-_ {k}_{x}^{2}f(x_{k}(),;_{k+1}).\] (SGD \[{}^{*}\] )

The natural limit candidate for this recursion is the Jacobian of the solution, \(_{}x^{*}()\), which, from the implicit function theorem, is the unique solution to the following linear system

\[_{xx}^{2}F(x^{*}(),)D+_{x}^{2}F(x^{*}(), )=_{}_{xx}^{2}f(x^{*}(), ;)D+_{x}^{2}f(x^{*}(),;)=0.\]

As noted in (Arbel and Mairal, 2021, Proposition 1), this is equivalently characterized as a solution to the following stochastic minimization problem

\[_{}x^{*}()=_{D^{d m}}\ _{}_{xx}^{2}f(x^{*}(),;)D+_{x}^{2}f(x^{*}(),;),D\] (Opt \[{}^{*}\] )

where we use the standard Frobenius inner product over matrices. Our key insight is to formally understand the recursion in (SGD \[{}^{*}\] ) as an inexact SGD sequence applied to problem (Opt \[{}^{*}\] ).

Intuition from the quadratic case.Consider two maps \( Q()^{d d}\) and \( B()^{d m}\). Let \(f(x,;)=x^{}Q()x+x^{}P()\), then the recursion in (SGD \[{}^{*}\] ) becomes

\[_{}x_{k+1}()=_{}x_{k}()-_{k}(Q( _{k+1})_{}x_{k}()+B(_{k+1})).\]

which is exactly a stochastic gradient descent sequence for problem (Opt \[{}^{*}\] ). Hence, choosing appropriate step sizes ensures convergence. Beyond the quadratic setting, one needs to take into consideration the fact that the second order derivatives of \(f\) are not constant, leading to our interpretation as _perturbed_ stochastic gradient iterates for the derivatives, as detailed below.

The general case.We rewrite the recursion (SGD \[{}^{*}\] ) as follows

\[_{}x_{k+1}()\!=\!\!_{}x_{k}()-_{k} _{xx}^{2}f(x^{*}(),;_{k+1})_{}x_{k}()- _{k}_{x}^{2}f(x^{*}(),;_{k+1})+e_{k+1}, \]

where the error term \(e_{k+1}\) is defined as

\[e_{k+1} =_{k}(_{xx}^{2}f(x^{*}(),;_{k+1})- _{xx}^{2}f(x_{k}(),;_{k+1}))_{}x_{k}()\] \[+_{k}(_{x}^{2}f(x^{*}(),; _{k+1})-_{x}^{2}f(x_{k}(),;_{k+1}))\,.\]

Assuming that the second derivative of \(f\) is Lipschitz-continuous, the error term \(e_{k+1}\) is of the same order as \(_{k}\|x_{k}()-x^{*}()\|(1+\|_{}x_{k}()\|)\). Our main contribution is a careful analysis of a specific version of inexact SGD which covers the above recursion. Under typical stochastic approximation assumptions, the convergence of \(x_{k}()\) toward \(x^{*}()\) essentially entails the convergence of \(_{}x_{k}()\) toward \(_{}x^{*}()\). This allows us to carry out a joint convergence analysis of both sequences in (SGD \[{}^{*}\] ) and (SGD \[{}^{*}\] ). We now describe the assumptions required to make this intuition rigorous.

### Main assumptions

We start with the stochastic objective, \(f\) in (Opt) and then specify assumptions on the underlying random variable \(\). The crucial assumption for our results is strong convexity. The rest of the assumptions are typically satisfied in applications such as hyper parameter tuning. We point out that both examples in the numerical section satisfy our assumptions and are implemented in the regime described by our main theorem.

**Assumption 1**.: Let \(\) be an open Euclidean subset of \(^{m}\) and \(\) be a measure space. The function \(f^{d}\) satisfies the following conditions:

1. _Differentiability:_ \(f(,;)\) is \(C^{2}\), with \(M\)-Lipschitz Hessian (in Frobenius norm), for all \(\)

[MISSING_PAGE_FAIL:5]

_where the constants in the big \(O\) are polynomials in \(\), \(\|x_{0}()-x^{}()\|^{2}\), \(\|_{}x_{0}()-_{}x^{}()\|^{2}\), \(^{2}\), \(\), \(M\) and \(\)._

\(\) _Interpolation regime: if \(=0\) and \(_{k}=}\) for all \(k\), then_

\[\|_{}x_{k}()-_{}x^{}( )\|^{2}=O(k(1-})^{k}).\]

The first part of the result provides a general estimate which allows covering virtually all small step-size cases. This includes: i) vanishing step-sizes, for which our result implies convergence of derivatives; and ii) constant step-sizes \(\), for which we provide a bound on the distance to the true derivative that is proportional to \(\). For the second part, using step-sizes decreasing as \(1/k\), which is a typical setup for the convergence of SGD on strongly convex objectives, our result shows that the derivatives converge as well, with a rate that is asymptotically of the same order, up to log factors. Finally, the last part of the result relates to the interpolation regime which has drawn a lot of attention in recent years because it captures some of the features of overparameterized deep neural network training Ma et al. (2018); Varre et al. (2021); Garrigos and Gower (2023). Note that the condition \(=0\) in Assumption 2 entails that interpolation occurs for both problems (Opt) and (Opt'), and in this case we obtain exponential convergence of both the iterates and their derivatives, with a constant stepsize, as in the deterministic setting (Mehmood and Ochs, 2020).

**Remark 2.3**.: _The specific stepsize used to obtain the sublinear rate actually applies to any stepsize of the form \(_{k}=2/(ck+8u)\) for given \(c,u>0\) such that \(0<c\) and \(u L^{2}/c\). One obtains the same result with \(,L\) respectively replaced in the expressions by \( c\) and \( L\). This corresponds to using a lower estimate for the strong convexity constant and a higher estimate for the smoothness constant, which remain valid. A similar remark holds for the interpolation regime where any stepsize \(\) smaller than \(/(4L^{2})\) will bring the same result with \(\) replaced by \( 1/(4L)\) in the statement._

**Remark 2.4**.: _We consider step sizes at most \(}\) which is smaller than \(\), typically used in optimization. Aside from the \(\) factor, which could possibly be improved, it is important to relate it to the the fact that we have obtain \(O(1/k)\) rates which represent fast rates for SGD for convex optimization, limited to strongly convex objectives. Second, we do not have any Lipschitzity assumption on the objective function itself. This, and the use of small steps to obtain fast rate is in line with related literature such as (Bottou et al., 2018, Theorem 4.6), the discussion following (Moulines and Bach, 2011, Theorem 1) or (Garrigos and Gower, 2023, Corollary 5.8 and Theorem 5.9). The possibility to obtain convergence of derivatives of SGD for larger step sizes will be a topic of future research._

## 3 Proof of the main result

Our result relies on the interpretation of the recursion (SGD') as an inexact SGD sequence for the problem (Opt'). We start with a detailed analysis of inexact SGD under appropriate assumptions. This is an abstract result which we formulate using an abstract function \(g\) different from the objective in problems (Opt) and (Opt') in order to avoid any possible confusion. In particular \(g\) is static (does not depend on external parameters) and the obtained convergence result will be then applied to both sequences (SGD) and (SGD').

### Detour through an auxiliary result: convergence of inexact SGD

We provide here our template results for the convergence of inexact SGD. As template, we consider a function \(G^{q}\) defined as

\[G(x)_{}[g(x;)]\,.\]

Our generic assumptions stand as follows.

**Assumption 3**.: \(\) is a probability distribution on the measure space \(\), and the function \(g^{d}\) satisfies the following conditions:

1. _Smoothness:_ \(g(;)\) is \(C^{1}\) with \(L\)-Lipschitz gradient, i.e., there is \(L 0\) such that \[\|_{x}g(x;)-_{x}g(x^{};)\| L\|x-x^{}\|\] for all \(x,x^{}^{q}\), and all \(\).

_._
2. _Strong convexity:_ there is \(x^{}^{q}\) and \(>0\) such that \( x-x^{},[_{x}g(x;)]\|x-x^{}\|^ {2}\) for all \(x^{q}\).
3. _Variance control:_ there is \(0<+\) such that \(\|_{x}g(x^{};)\|^{2}^{2}\).

We remark that under Assumptions 1 and 2, Assumption 3 is satisfied for both problems (Opt) and (Opt'). We will consider an _inexact_ SGD recursion of the form

\[x_{k+1}=x_{k}-_{k}(_{x}g(x_{k};_{k+1})+e_{k+1}) \]

where we will need the following assumption on noise and errors.

**Assumption 4**.: The observed noise sequence \((_{k})_{k}\) is independent and identically distributed with common distribution \(\) on \(\). Denote by \((_{k})_{k}\) the natural filtration (i.e., for all \(k\), \(_{k}\) is the \(\)-algebra generated by \(_{0},,_{k}\)), the errors \((e_{k})_{k}\) form a sequence of \((_{k})_{k}\)-adapted random variables such that \([\|e_{k+1}\|^{2}] B_{k}^{2}\) where \((B_{k})_{k}\) is a deterministic non-increasing sequence.

The following reduces the analysis of inexact SGD sequences to the study of a deterministic recursion, its proof is given in Appendix B.

**Proposition 3.1** (Convergence of inexact SGD).: _Let Assumption 3 and Assumption 4 hold. Consider the iterates in (2) where \((_{k})_{k}\) is a positive, non-increasing, non-summable sequence with \(_{0}}\). Setting \(D_{k}=[\|x_{k}-x^{}\|^{2}]}\), we have for all \(k\):_

\[D_{k+1}^{2}(1-_{k})D_{k}^{2}+2_{k}^{2}(B_{k}^{2}+2 ^{2})+2_{k}B_{k}D_{k}. \]

Studying the deterministic recursion (3) leads to the following results by relying on different helper lemmas laid out in Appendix C:

 Lemma & Stepsizes & Errors & Noise & Result \\  Lemma C.1 & \(_{k} 0\) & \(B_{k} B\) & \(^{2} 0\) & \(_{k} D_{k}\) \\  Lemma C.2 & \(_{k}=k+8L^{2}}\) & \(B_{k}=0\) & \(^{2} 0\) & \(D_{k}^{2}=O()}{k+8^{2}})\) \\  Lemma C.3 & \(_{k}=k+8L^{2}}\) & \(B_{k}^{2}=O()}{k+8^{2}})\) & \(^{2} 0\) & \(D_{k}^{2}=O()^{2}}{k+8^{2}})\) \\  Lemma C.4 & \(_{k}=<\) & \(B_{k}^{2}=O((1-)^{k})\) & \(^{2}=0\) & \(D_{k}^{2}=O(k(1-)^{k})\) \\  

These results will be used to prove Theorem 2.2 in the coming section. They are of independent interest regarding the convergence analysis of inexact SGD sequences. The first lemma allows to prove the first point in Theorem 2.2, the second and third lemmas allow to treat the second point, and the last lemma allows to treat the interpolation regime in the third point. See Appendix C for detailed statements.

### Proof of the main result

We first show that Proposition 3.1 can be applied to the recursion (SGD') in relation to (Opt') and then explicit its consequences using the lemmas of Appendix C.

Proof of Theorem 2.2.: Following (1), we have that \((_{}x_{k}())_{k}\) is an inexact SGD sequence for problem (Opt') as in (2), with an error term of the form

\[e_{k+1} =(_{xx}^{2}f(x^{}(),;_{k+1})- _{xx}^{2}f(x_{k}(),;_{k+1}))_{}x_{k }()\] \[+(_{x}^{2}f(x^{}(),;_{ k+1})-_{x}^{2}f(x_{k}(),;_{k+1})).\]

Under Assumption 1 and Assumption 2, Problem (Opt') satisfies Assumption 3, and we have the same values for \(L\), \(\) and \(\) for both problems (Opt) and (Opt'). Furthermore, the error term \(e_{k+1}\) satisfies Assumption 4, and, thanks to Lemma 2.1 and Assumption 1 on Lipschitz continuity of the Hessian of \(f\), we have almost surely

\[\|e_{k+1}\| M\|x_{k}()-x^{}()\|(1+\{\|_{ }x_{0}()\|,L^{}/\}). \]

The various bounds are obtained by considering different regimes. We first estimate a bound on \(\|x_{k}()-x^{}()\|^{2}\) using Proposition 3.1 with \(B_{k}=0\) for all \(k\). This allows to obtain an estimate on \([\|e_{k+1}\|^{2}]\) using (4). We conclude for the derivative sequence by applying Proposition 3.1 with its different corollaries. We treat all these results separately.

**General estimate.** From Proposition 3.1 with \(B_{k}=0\), we obtain, by considering \(g(x,)=f(x,;)\) and Lemma C.1 that \(_{k}\|x_{k}()-x^{}()\|^{2} }{}\). For the derivative sequence, combining this first estimate with (4), we can consider a decreasing sequence of mean squared upper bounds \((B_{k})_{k}\), such that

\[_{k}B_{k}=B 2}M(1+\{\| _{}x_{0}()\|,L^{}/\}).\]

The upper bound given by Proposition 3.1 and Lemma C.1 is of the form

\[+2(B^{2}+2^{2})}+B}{}B^{2}+4^{2}}+B}{} 2}+ ,\]

which corresponds to the claimed bound.

**Convergence rate.** From Proposition 3.1 with \(B_{k}=0\), we obtain, by considering \(g(x,)=f(x,;)\) and Lemma C.2 that \(\|x_{k}()-x^{}()\|^{2}=O()}{k+8^{2}})\) as given in Lemma C.2. As a consequence, combining this first estimate with (4), we may set \(B_{k}=O()}{k+8^{2}})\) and the result follows from Lemma C.3.

**Interpolation regime.** Setting \(=1-=1-}\), for \(^{2}=0\) and \(B_{k}=0\) for all \(k\), it is clear from (3) that \(\|x_{k}()-x^{}()\|^{2}\|x_{0}( )-x^{}()\|^{2}^{k}\) for all \(k\). Using (4), we may choose \(B_{k}=O(^{k})\). Plugging this estimate in (3), the result is then given by Lemma C.4. 

## 4 Numerical illustration

In this section, we illustrate the results of Theorem 2.2 by examining the numerical behavior of the iterates and their derivatives under various settings. Specifically, we provide insights into the behavior of classical regularized methods, such as Ridge regression, logistic regression, Huber regression. Furthermore, we explore potential extensions to the nonsmooth case by also considering the Hinge loss. All the experiments are performed for the empirical risk minimization structure, i.e., the randomness \(\) is drawn from the uniform distribution over \(\{1,,m\}\). All the experiments were performed in jax (Bradbury et al., 2018) on a MacBook Pro M3 Max.

**Ordinary least squares.** We consider a simple linear regression problem solved by ordinary least-squares as:

\[x^{}()=_{x^{d}}\ F(x,) {2m}_{=1}^{m}(a_{}^{}x-b()_{})^{2},\]

The data \(A=(a_{})^{m d}\) here is a random matrix with \(d<m\). The finite sum structure naturally suggests a stochastic gradient decomposition as in (SGD), by choosing \(\) uniformly in \(\{1,,m\}\) with replacement. We consider three generative models for the function \(b\):

1. _Standard setting:_\(^{m}\), and we have \(b\) is the identity on \(^{m}\). In this case, our theory corresponds to the differentiation of the least squares solution seen as a function of the output observations.
2. _Simple interpolation setting:_ The setting is the same as the standard one, except that we consider a specific value of \(=A\) for some \(^{d}\). In this case, we do _not_ differentiate through the linear relation \(=A\), the function \(b\) remains the identity, we simply evaluate at a specific point which corresponds to data interpolation. We call this simple interpolation, because it corresponds to \(=0\) for the sequence (SGD), but not for (SGD').
3. _Double interpolation setting:_ The parameter variable \(\) is in \(^{d}\) and we set \(b A\). Here this corresponds to an interpolation regime which is uniform in \(\). We call this double interpolation because it corresponds to \(=0\) for both sequences (SGD) and (SGD').

Note the the difference between setting 2. and 3. are that we are _not_ differentiating through the linear map \(A\) in setting 2. Furthermore Assumption 1 and Assumption 2 are satisfied for these three settings. Figure 1 illustrates the behavior of (SGD) and (SGD'). More precisely, we monitor the convergence of the suboptimality \(f(x_{k}())-f(x^{}())\) and of the derivatives error measured in Frobenius norm \(\|_{}x_{k}()-_{}x^{}()\|_{F}\). We consider various step size regimes and set \(_{0}=}\) for all experiments. This allow us to clearly identify the three regimes of Theorem 2.2:

* _Constant stepsize_: in setting 1., employing a constant step-size, we observe convergence of both the iterates (consistent with classical SGD theory) and their derivatives to a neighborhood of the solution whose diameter decreases with the step size.
* _Decreasing stepsize_: in setting 1., employing a step-size proportional to \(1/k\), we observe a sublinear decay of both the iterates and their derivatives. The convergence is difficult to observe since the decay leads to very small updates.
* _Double Interpolation regime:_ in setting 3., employing a constant step-size, we observe both iterates and derivatives linear decays.
* _Simple Interpolation regime:_ in setting 2., Assumption 2(_a_) is satisfied with \(=0\) only for the iterates, but not for the derivatives, we observe linear convergence of the iterates, but the derivatives converge to a neighborhood of the solution as in the setting 1.

**Ridge, Logistic, Huber and SVM regression.** In addition to the previous illustration of Theorem 2.2, we provide numerical experiments for constant learning rate for four different models: ridge regression, logistic regression, Huber regression and Support Vector Machines (SVM) regression. All of them are written as

\[x^{}()=_{x^{d}}\ F(x,)_{=1}^{m}f(x,;)+\|x\|_{2}^{2},\]

where \(f(x,;)=(a_{}^{}w-_{})^{2}\) for ridge regression, \(f(x,;)=(1+(-_{}a_{}^{}x))\) for logistic regression,

\[f(x,;)=(_{}-a_{}^{}x)^{2}& |_{}-a_{}^{}x|\\ (|_{}-a_{}^{}x|-)&\]

for Huber regression for some \(>0\) (here \(=0.1\)), and \(f(x,;)=(0,1-_{}a_{}^{}x)\) for SVM regression (hinge loss). In all cases, the finite sum structure naturally suggests a stochastic gradient decomposition as in (SGD), by choosing \(\) uniformly in \(\{1,,m\}\) with replacement All experiences are performed with \(m,d=100,10\) and \(=0.05\). In Figure 2, we show the convergence

Figure 1: Numerical behavior of SGD iterates and their derivatives (Jacobians) in a linear regression problem solved by ordinary least squares. The plots depict the convergence of the suboptimality \(f(x_{k}())-f(x^{}())\) and the Frobenius norm of the derivative error \(\|_{}x_{k}()-_{}x^{}()\|_{F}\) across different experimental settings: constant step-size (first column), decreasing step-size (second column), double interpolation (third column), and simple interpolation (fourth column). The experiments utilize varying step-size strategies to illustrate general estimates, sublinear rates, and the impacts of interpolation regimes, validating theoretical predictions of Theorem 2.2.

of the objective function and the derivatives with respect to \(\) for the four models with a constant learning rate. Note that the SVM loss is not differentiable. We refer to (Bolte et al., 2022) for a formal treatment of nonsmooth iterative differentiation, but one could expect similar results for conservative Jacobians.

**Experiments on real data.** We display in Figure 3 the behaviour of SGD iterates and their derivatives for regularized logistic regression problem on ijcnn1.

## 5 Conclusion

In conclusion, our study of stochastic optimization problems where the objective depends on a parameter reveals insights into the behavior of SGD derivatives. We demonstrated that these derivatives follow an inexact SGD recursion, converging to the solution mapping's derivative under strong convexity, with constant step-sizes leading to stabilization and vanishing step-sizes achieving \(O((k)^{2}/k)\) rates. Future research could refine the analysis by comparing stochastic implicit and iterative differentiation, develop a minibatch version, and explore outcomes in non-strongly convex or nonsmooth settings. Additionally, the feasibility of stochastic iterative differentiation warrants further investigation, given its potential benefits and challenges in such scenarios.

Figure 3: Numerical behavior of SGD iterates and their derivatives (Jacobians) for regularized logistic regression problem. The plots depict the convergence of the suboptimality \(f(x_{k}())-f(x^{*}())\) (left) and the derivative error \(\|_{}x_{k}()-_{}x^{*}()\|\) (right) for different constant step size. The dataset used is ijcnn1 from libsvm with 49,990 observations and 22 features. The observations are qualitatively identical to our synthetic experiments.

Figure 2: Numerical behavior of the objective function and its derivatives with respect to \(\) for ridge regression, logistic regression, Huber regression, and Support Vector Machines (SVM) regression using a constant learning rate. We report the suboptimality \(f(x_{k}())-f(x^{*}())\) for the SGD iterates, along _(bottom)_ with the norm of derivatives errors \(\|_{}x_{k}()-_{}x^{*}()\|_{F}\) for different constant step-size. Each line corresponds to a different step-size.