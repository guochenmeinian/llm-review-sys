# Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers

Dong Hoon Lee

KAIST

donghoonlee@kaist.ac.kr &Seunghoon Hong

KAIST

seunghoon.hong@kaist.ac.kr

###### Abstract

Recent token reduction methods for Vision Transformers (ViTs) incorporate token merging, which measures the similarities between token embeddings and combines the most similar pairs. However, their merging policies are directly dependent on intermediate features in ViTs, which prevents exploiting features tailored for merging and requires end-to-end training to improve token merging. This paper proposes **D**ecoupled **T**oken **E**mbedding for **M**erging (**DTEM**) that enhances token merging through a decoupled embedding learned via a continuously relaxed token merging process. Our method introduces a lightweight embedding module decoupled from the ViT forward pass to extract dedicated features for token merging, addressing the restriction from using intermediate features. The continuously relaxed token merging, applied during training, enables us to learn the decoupled embeddings in a differentiable manner. Thanks to the decoupled structure, our method can be seamlessly integrated into existing ViT backbones and trained either modularly by learning only the decoupled embeddings or end-to-end by fine-tuning. We demonstrate the applicability of DTEM on various tasks, including classification, captioning, and segmentation, with consistent improvement in token merging. Especially in the ImageNet-1k classification, DTEM achieves a 37.2% reduction in FLOPs while maintaining a top-1 accuracy of 79.85% with DeiT-small. Code is available at https://github.com/movinghoon/dtem.

## 1 Introduction

Transformers  have become the dominant and most popular architecture in machine learning, excelling in various modalities and tasks. In computer vision, Vision Transformers (ViTs) [9; 30] have achieved state-of-the-art performance, outperforming conventional backbones in tasks such as image classification , object detection , and segmentation , as well as multimodal applications such as image captioning  and visual question answering . A key factor in the success of ViTs is their ability to capture long-range dependencies between patches or tokens, regardless of their spatial positions, using self-attention. However, due to self-attention, ViTs have high computational and memory costs that increase quadratically with the number of tokens. Consequently, there has been significant interest in developing methods to improve the computational efficiency of ViTs.

In this pursuit, token reduction [26; 23; 39; 22] aims to progressively reduce the number of tokens in Transformer layers, often adhering to predefined reduction rates. Early approaches [26; 23; 39] propose to prune unimportant tokens based on their contribution to the task, as measured by scoring functions. Yet, simply pruning tokens leads to information loss, often resulting in significant performance degradation in high reduction rates. Alternatively, approaches based on token merging [21; 27; 40; 2; 20; 34; 12] aim to combine redundant tokens instead of removing them. Such redundancy is measured by the similarity between the tokens based on intermediate features in ViT, such as token- or key-embeddings. Token merging has several advantages over pruning; it canachieve improved performance by reducing information loss in token reduction and can be seamlessly plugged into pre-trained models without altering the architecture.

However, merging tokens directly based on intermediate features, which are responsible for contextual encoding, presents several limitations. Firstly, these features are hard to be tailored specifically for token merging. This is because the same intermediate feature should be used for contextual encoding and merging; thereby, it would be less effective than having separate features dedicated to each role. Secondly, enhancing the merging process, which entirely relies on intermediate features, necessitates end-to-end training of the entire network. This makes it difficult to leverage pre-trained models effectively and typically requires extensive data to prevent overfitting.

To this end, we propose Decoupled Token Embedding for Merging (DTEM) that learns decoupled token embedding specifically tailored to enhance token merging. We introduce a lightweight trainable embedding module decoupled from the intermediate feature in the ViT and use it to modulate the merging policy. This resolves the dependency of token merging on intermediate features and facilitates the decoupled embedding to extract only suitable features for enhanced token merging. Moreover, since the modules are separated from ViTs, improved merging can be achieved without altering the ViT parameters, allowing for efficient modular optimization with pre-trained models.

However, learning the decoupled embedding module directly from conventional token merging is infeasible, since the grouping policy, _i.e._, deciding which tokens to merge, is based on discrete operators such as hard cluster assignment  or matching on a bipartite graph  (Figure 1(a)). To address this, we design a continuous relaxation of token merging that softly merges tokens in a differentiable way according to their similarities (Figure 1(b)). The relaxed operators, applied during training, enable training of the decoupled embedding directly through the grouping policy to improve token merging. We also observe that such relaxed operators tend to facilitate generalization of the learned decoupled embedding across unseen token reduction rates. During inference, our model converges to existing token merging methods by replacing the relaxed operators with hard ones.

We integrate DTEM in two distinct ways: modular and end-to-end full fine-tuning. For the former, we train only the embedding module while keeping the parameters of the pre-trained model frozen, while later we train the entire parameters in an end-to-end manner. We apply DTEM to existing pre-trained vision models and verify its effectiveness in image classification, captioning, and segmentation, each requiring a different level of granularity in representation. Despite the simplicity, DTEM consistently improves token merging in all three tasks, offering a better trade-off between task performance and computation cost. We further analyze DTEM's components, design choices, and training efficiency. Overall, our contributions are summarized as follows:

* We propose DTEM, a novel approach to enhance token merging by decoupled token embedding learned via continuous relaxation of token merging. The decoupled embedding is dedicated to merging and learns features suitable for merging directly from our relaxed token merging.
* DTEM can be applied through end-to-end full fine-tuning or in a modular way by training only the added embedding module. When trained modularly, the method delivers improvements even with substantially smaller datasets and fewer training epochs.
* Empirical evaluations over image classification, captioning, and segmentation across various ViT models demonstrate that DTEM consistently outperforms the prior arts.

Figure 1: **Comparison of our method with conventional token merging.** Contrary to prior works that merge tokens directly based on intermediate features in ViT, our method leverages a decoupled embedding to extract features tailored for token merging. The embedding module is trained via continuous relaxation of grouping and merging operators, _i.e._, soft grouping and merging, respectively, that allow differentiation.

## 2 Background

Given a Transformer that takes \(N\) input tokens \(^{N d}\), the objective of token merging is to gradually _merge_\(r\) tokens at each Transformer block, reducing the number of tokens to \(}^{(N-r) d}\). Here, the \(r\) denotes the reduction rate. To this end, prior works conduct the merging in two steps, _grouping_ and _merging_, which are expressed as:

\[ =(),\] (1) \[} =(,),\] (2)

where \(^{N N}\) denotes the similarity matrix of tokens, _e.g._, \(s_{ij}=(_{j},_{j})\). Given the similarity \(\), the grouping operator (Eq. 1) identifies pairs of tokens to merge and represents them in the reachability matrix \(\{0,1\}^{N N}\) with \((N-r)\) connected components, where \(e_{ij}=1\) indicates that the \(i\)th and \(j\)th tokens belong to the same component and will be merged. The merging operator (Eq. 2) then combines all connected tokens in \(\) by pooling.

The performance of the above framework highly depends on the choice of the grouping operator, as it dictates the merging policy (_i.e._, which tokens to merge), and computing the reachability matrix can be costly. Early works employ clustering algorithms [21; 40], but they tend to be slow due to the iterative procedure and often suffer from performance drops due to the dramatic distribution shift from \(\) to \(}\) caused by aggressive clustering.

Recently, ToMe  introduced Bipartite Soft Matching (BSM) as an efficient grouping operator of Eq. 1. To parallelize the computation, BSM divides the input tokens into two disjoint sets \(\) and \(\), and constructs a bipartite graph. Then for each node \(i\), it chooses an edge with highest similarity \(_{j}s_{ij}\), and choose the \(r\) most similar edges afterwards to obtain the sparse adjacency matrix \(^{}\{0,1\}^{||||}\) where \(_{ij}e^{}_{ij}=r\). The merging is performed by combining the connected tokens in \(^{}\), where the connected components can be easily found since each token in \(\) has at most one edge. ToMe  also proposes tracking the size of the combined tokens and accounts for it in self-attention. Specifically, given a vector \(^{N-r}\) representing the size of combined tokens, the _proportional attention_ is used in the QKV self-attention layers by:

\[=(^{}}{}+),\] (3)

where \(\) denotes the softmax function.

LimitationsAlthough the success of merging depends mostly on grouping operation (Eq. 1), the grouping depends entirely on the similarity of the intermediate ViT feature (\(\) or \(\)) in the prior works. This is mainly because the grouping comprises discrete operators, such as clustering and matching, that prevent the gradient flow through grouping (Eq. 1). Thus, the only viable option to improve the merging is by updating the intermediate feature \(\) by back-propagating through the merging operator (Eq. 2). However, it leads to extensive end-to-end training of the entire network, preventing off-the-shelf usage and resulting in suboptimal performance due to the conflict between the token feature required for optimal merging and task performance.

We provide more discussion on related work in the supplementary materials (A.3).

## 3 Method

Our objective is to improve token merging by learning the decoupled embedding specifically tailored for merging. To this end, we base our method on the standard token merging framework introduced in the previous section (Eqs. 1, 2). Instead of directly leveraging the ViT features \(\) for grouping, we propose to learn additional per-token embedding modules \(=f(;)\), which are decoupled from the forward pass of the ViT and used only to compute the similarity \(\) in Eq. 1 by \(s_{ij}=(_{i},_{j})\) (Sec. 3.1). Since the grouping operator is entirely dependent on similarity, we can directly modulate the grouping (or merging) policy by learning \(\). Furthermore, since the embedding is decoupled from the ViT forward pass, enhancements in merging can be achieved modularly without altering the ViT parameters but only learning the embedding \(\).

To enable our model to learn such embeddings through merging, we propose a continuous relaxation of the grouping and merging operators in Eq. 1 and Eq. 2, respectively. Specifically, our relaxedgrouping operator generates a continuous matrix \(}\), whose elements \(_{ij}\) indicates the _soft_ degree of merging \(i\)th token into the \(j\)th token (Sec. 3.2). To incorporate such soft commitment in merging, we also propose a relaxed merging operator that combines tokens with continuous weights defined by \(}\) (Sec. 3.3). Since the token merging is performed continuously with our relaxed operators, we discretize them after the training, reducing our framework to behave similarly to the hard merging methods  (Sec. 3.4).

### Decoupled Embedding Module

We first describe the choice of the embedding module decoupled from the forward pass of ViTs. To facilitate token merging at each Transformer block, we introduce per-token projection layers into each block \(l\{1,,L\}\):

\[\ :_{i}=f(_{i}; _{l}),\] (4) \[\ :s_{ij}=(_{i},_{j}),\] (5)

where \(^{N d}\) is the input to the self-attention and \(^{N d^{}}\) denotes the output decoupled embedding with \(d^{} d\). The output embeddings will be used _solely_ to shape the merging policy (_i.e._, deciding which tokens to merge) in the grouping operator based on the similarity \(\).

Minimizing additional run-time and computational overheads is essential to the embedding module design. In our approach, we employ a token merging between the self-attention and feed-forward layer following [18; 2]. It allows parallelizing the computation of the attention and decoupled embedding, avoiding the potential overhead that comes from serialization. Moreover, we discover that even a shallow module, consisting solely of an affine transformation, can achieve improvement with minimal computational expense (Sec. 4.4). This further minimizes the number of additional parameters to less than 1% and enables the training of the module with a small amount of data.

### Soft Grouping

Given the similarity matrix \(\) obtained from the decoupled token embeddings \(\), soft grouping aims to approximate the grouping operation through a continuous relaxation that enables differentiation. However, building a general continuous grouping operator of Eq. 1 is challenging since the output reachability matrix is inherently discrete.

Instead, we employ BSM  as our target grouping operator, which offers the benefit of bypassing the reachability matrix and allows for merging to be defined directly on the adjacency matrix \(^{}\{0,1\}^{||||}\). To be a valid approximation of the grouping performed by BSM, the soft grouping operator should produce a continuous adjacency matrix \(}^{||||}\) that satisfies two key conditions. Firstly, it should simulate \(r\) distinct edges with high values, thereby implementing the valid merging policy, _i.e._, combining the \(r\) most similar token pairs. Secondly, each node in \(\) should be associated with at most one edge (_i.e._, \(_{j}_{ij} 1\)) to simplify the identification of connected components in \(}\), thus avoiding the complexity of computing a reachability matrix.

To achieve this, we propose a soft grouping that revises the differentiable top-\(k\) operator from . Starting with \(^{1}=\), we repeat the subsequent steps for each \(t=1,2,...,r\):

\[^{t} =(^{t}/),\] (6) \[s_{ij}^{t+1} =s_{ij}^{t}+(1-_{j}a_{ij}^{t}),\] (7)

where \(\) denotes the global softmax function and \(>0\) represents a temperature scale that regulates the relaxation. In each step \(t\), this process computes the \(^{t}^{||||}\) with \(_{i,j}a_{ij}=1\), representing the soft-argmax. Subsequently, the similarity \(^{t}\) is updated to suppress the entire outbounding edges from the softly selected nodes in \(\) by Eq. 6. Afterward, the soft adjacency matrix \(}\) is defined as follows:

\[_{ij}=^{*}}{(1,(_{j}a_{ ij}^{*}))},^{*}=_{t=1}^{r}^{t},\] (8)

with \(_{ij}e_{ij} r\), representing the total number of selected edges. The clipping function, composed of a max operator (\(\)) and stop-gradient (\(\)), is introduced to ensure that the resulting \(}\) is a valid continuous adjacency matrix.

Note that the soft grouping satisfies the aforementioned key conditions. As \( 0\), \(^{t}\) converges to the one-hot matrix that indicates the nodes in \(\) and \(\) with maximum \(s^{t}_{ij}\). This results in \(^{*}\) representing \(r\) most similar pair, thus satisfying the first condition. Meanwhile, edges associated with such nodes in \(\) are excluded from future selection according to Eq. 7, since \((1-_{j}a^{t}_{ij})-\), ensuring at most one selection per node in \(\). This holds true even in the non-asymptotic case, as the clipping function guarantees \(_{j}_{ij} 1\), thereby meeting the second condition.

### Soft Merging

While the soft grouping and the resulting soft adjacency matrix effectively approximate the grouping process, it is crucial to design the merging operator to incorporate such soft decisions. In our approach, for the given soft adjacency matrix where \(_{ij}\) corresponds to tokens \(i\) and \(j\), our soft merging is designed such that the \(i\)th token merges into the \(j\)th token in proportion to the value of \(_{ij}\).

Our soft merging operator applies the asynchronous updates on tokens in two sets, \(\) and \(\). For each token \(j\), the operator update their feature \(_{j}\) and the effective size \(m_{j}\) by aggregating tokens in \(\) based on the soft adjacency matrix \(}\) from Section 3.2 by:

\[}_{j}_{j}+_{i}_{ij}m_{i}_{i}}{m_{j}+_{i}_{ij}m_{i}},_{j} m_{j}+_{i}_{ij}m_{i}.\] (9)

One the other hand, for each token \(i\), the operator update only its effective size \(m_{i}\) while maintaining the feature:

\[_{i} m_{i}(1-_{j}_{ij}).\] (10)

Note that with the binary adjacency matrix \(^{}\), the effective size of the tokens in \(\) reduces to zero by Eq. 10 if they have outbounding edges. Such tokens will be excluded from the subsequent merging process by Eq. 9. This process is simulated continuously during training with our soft adjacency matrix (_i.e._, each token will be continuously absorbed into others), while it is used to actually reduce the tokens at inference using a discretized adjacency matrix.

Interestingly, we observe that the decoupled embedding, trained with soft merging at a high reduction rate \(r\), generalizes well to lower rates \(r^{} r\). This is presumably because the decoupled embedding is learned to sort \(r\) most similar token pairs by the relaxed top-k operator (Eqs. 6, 7), thereby including the sorting for smaller reduction rates \(r^{} r\).

### Training and Inference

TrainingThanks to _decoupled_ embedding modules, training can be conducted in two distinct ways: modular and end-to-end training. In modular training, we train only the embedding modules while keeping the ViT parameters frozen. This allows our method to fully leverage off-the-shelf models while effectively adapting only the merging policy to each task. In end-to-end training, we jointly train all ViT parameters along with our embedding modules. Since our continuous merging operators do not reduce the number of tokens during training, we alternate updates between the embedding layers and ViT parameters to save computation. Specifically, when updating the ViT parameters, we fix the embedding layers and use the discretized grouping and merging operators, which allows token reduction in the ViT forward pass, greatly enhancing the efficiency. Conversely, when updating the embedding modules, we apply the soft grouping and merging operators while fixing the ViT parameters. We alternate this procedure with much more frequency on ViT updates, since the embedding layers have considerably smaller parameters (\(\)1%) and hence quickly converge. For both modular and end-to-end training, we simply train our method to minimize the task loss.

InferenceFor inference, we discretize the continuous operators in the grouping and merging processes, and perform the hard token merging utilizing the learned decoupled embeddings. As explained in Sec. 3.2 and Sec. 3.3, our soft grouping and merging modules are asymptotically equivalent to BSM of ToMe. Consequently, we employ BSM to speed up the inference.

## 4 Experiments

We apply our method, DTEM, for token merging in image classification, captioning, and segmentation. In Sec. 4.1, we first evaluate our method in the ImageNet-1k  classification with two setups:_modular_ and _end-to-end_ training. We further present our results on COCO  image captioning in Sec. 4.2 and ADE20K  semantic segmentation in Sec. 4.3 to demonstrate that our method can be applied to tasks requiring various levels of granularity in representation. We then provide a series of analyses on the importance of decoupled embedding, the design choices of embedding module, and data/training efficiency, complemented by visualizations, in Sec. 4.4.

### Image Classification

SetupWe conducted an image classification experiment on ImageNet-1k  dataset with 1.28M training and 50k validation images. We apply our method and baselines to various pre-trained ViT models, including DeiT-S/B , MAE-B/L , and LV-ViT-S . The image resolution in training and testing is \(224 224\) unless otherwise stated. We also present the results for DeiT-T and AugReg  ViT-S (with a resolution of \(384 384\)) in the supplementary material (A.1). We report top-1 accuracy (Acc@1) on the validation set, with floating-point operation (FLOPs) and throughput (images per second, im/s) to quantify the computation reduction. For the throughput, we measured on a single NVIDIA 3090 GPU with a batch size of 128 and fp32.

Implementation detailWe mostly follow the fine-tuning setup from , which is based on the training recipe of DeiT . We initialize the ViTs with pre-trained weights and train for 30 epochs, as in most baselines . For token reduction, we employ a uniform reduction strategy, where the _reduction rate_\(r\) represents the number of tokens removed in each transformer block. When training the embedding modules, we apply the reduction rate \(r=16\) to ViT-S/B models and \(r=8\) to the ViT-L model. As the embedding module, we use a linear layer with an output dimension of \(64\) for ViT-S/B and \(128\) for ViT-L. We use a temperature scale of \(=0.1\) and also scale the similarity by \(0.1\) prior to soft grouping. More details can be found in the supplementary material A.2.

Modular trainingTable 1 reports classification results when approximately 35% and 50% of FLOPs are reduced by applying token reduction methods to frozen pre-trained ViTs, with ViT parameters remaining unchanged. We compare DTEM with ToMe  and EViT  in this setting. The results demonstrate that DTEM consistently outperforms the baselines. Specifically, with a 35% reduction in FLOPs, our method improves performance by +0.15% to +0.47% compared to ToMe across all DeiT-S/B and MAE-B/L models. For a reduction of 50% in FLOPs, DTEM significantly improves performance by +0.47% to +1.64%, while adding less than 1% additional FLOPs.

In Table 2, we further applied our method to LV-ViT , a variant of standard ViT. LV-ViT employs an input embedding module consisting of convolution layers to better tokenize the input image. We apply token merging into the first 12 transformer blocks of LV-ViT-S. Consistent with previous results, DTEM achieves a +0.2% accuracy gain over ToMe, demonstrating its applicability to LV-ViT. Notably, despite optimizing only the added embedding module parameters, this performance is comparable to other state-of-the-art methods  that fully fine-tune the ViT parameters.

   Reduction & Method & Acc@1 & GFLOPs & im/s & Method & Acc@1 & GFLOPs & im/s \\   - & DeiT-S  & 79.83 & 4.64 & 1390 & DeiT-B  & 81.79 & 17.7 & 440 \\   & EVT  & 78.50 & 3.03 & 2069 & EViT  & 80.45 & 11.6 & 658 \\  & ToMe  & 79.12 & 3.02 & 1917 & ToMe  & 80.57 & 11.5 & 628 \\   & **DTEM** & **79.44** & 2.91 & 1991 & **DTEM** & **81.01** & 11.1 & 653 \\   & EVT  & 74.10 & 2.33 & 2672 & EViT  & 75.11 & 8.9 & 854 \\  & ToMe  & 78.01 & 2.32 & 2457 & ToMe  & 77.92 & 8.82 & 823 \\  & **DTEM** & **78.99** & 2.35 & 2430 & **DTEM** & **79.54** & 8.88 & 818 \\   - & MAE-B  & 83.72 & 17.7 & 438 & MAE-L  & 85.95 & 61.8 & 131 \\   & EVT  & 82.11 & 11.7 & 658 & EViT  & 85.22 & 42.4 & 189 \\  & ToMe  & 82.33 & 11.5 & 628 & ToMe  & 85.46 & 42.5 & 186 \\   & **DTEM** & **82.80** & 11.6 & 653 & **DTEM** & **85.61** & 42.9 & 185 \\   & EVT  & 75.95 & 8.9 & 854 & EViT  & 82.77 & 33 & 244 \\  & ToMe  & 78.88 & 8.82 & 823 & ToMe  & 84.21 & 31.1 & 252 \\    & **DTEM** & **80.37** & 8.88 & 818 & **DTEM** & **84.68** & 31.4 & 250 \\   

Table 1: **Classification results with off-the-shelf frozen pre-trained models. Reduction roughly represents the decreases in FLOPs.**End-to-end trainingFigure 2 depicts the classification results under different FLOPs and throughputs when token reduction methods are applied through end-to-end training. We compared our method with a fine-tuned version of ToMe  and EViT . For the baselines, we report accuracies by training each model on specific target computation demands under varying reduction rates, _e.g._, \(r=\{16,13,12,11\}\) for ToMe. Conversely, for DTEM, we train a single model with a reduction rate of \(r=13\) for fine-tuning the ViT parameters, while maintaining \(r=16\) when training the embedding module.1 We then adjust the reduction rate and report the corresponding accuracies during inference.

The results demonstrate that our method consistently outperforms the baselines across all levels of computational reduction. Specifically, our method surpasses the baseline accuracy by 0.12% to 0.2% in DeiT-S while adding a small amount of FLOPs and degradation in throughput. This leads to an improved trade-off between accuracy and computational resources, such as FLOPs and throughput. A notable aspect of DTEM is its ability to provide a single trained model that is generalized across various reduction rates. This can mitigate the training and storage costs associated with the multiple rounds of full fine-tuning often required to support different levels of computational reduction.

Comparison to State-of-The-ArtWhile the results in Figure 2 verify the effectiveness of our method in end-to-end training, we compare DTEM's performance with more token reduction methods in Table 3. We mainly considered the 30 epochs training results used in  for a fair comparison.2 The table shows that our method achieves superior accuracy compared to other prior arts when computational costs are equated.

### Image Captioning

To demonstrate the broad applicability of our method, we apply DTEM to image captioning, a task extensively studied in the vision-language domain. Recent captioning models with ViTs typically utilize all output patch features to ensure the caption generation is grounded in richer and more

   Method & Acc@1 & GFLOPs & im/s \\    \\  DyViT  & 79.3 & 2.9 & 2082 \\ Evo-ViT  & 79.4 & 3 & 2031 \\ EViT\({}^{}\) & 79.51 & 3 & 2069 \\ ToMe\({}^{}\) & 79.68 & 3 & 1917 \\ ATS  & 79.7 & 2.9 & - \\ BAT  & 79.6 & 3 & - \\ \(\)TPS  & 79.7 & 3 & - \\ DTEM & **79.85** & 2.9 & 1991 \\  DyViT  & 78.5 & 2.5 & 2429 \\ EViT\({}^{}\) & 78.63 & 2.3 & 2672 \\ BAT  & 79.0 & 2.3 & - \\ \(\)TPS  & 79.2 & 2.3 & - \\ ToMe\({}^{}\) & 79.25 & 2.3 & 2457 \\
**DTEM** & **79.38** & 2.3 & 2430 \\   \\  DyViT  & 81.3 & 11.6 & 657 \\ Evo-ViT  & 81.3 & 11.6 & - \\ EViT  & 81.3 & 11.6 & 658 \\ ToMe\({}^{}\) & 81.37 & 11.5 & 628 \\
**DTEM** & **81.60** & 11.6 & 624 \\
**DTEM** & **81.47** & 11 & 653 \\  EViT  & 80.0 & 8.9 & 854 \\ ToMe\({}^{}\) & 80.58 & 8.8 & 823 \\
**DTEM** & **80.74** & 8.9 & 818 \\   

Table 3: **Comparison of classification results with prior arts. \(\) denotes the baseline results implemented by ourselves.**

   Method & Acc@1 & GFLOPs & im/s \\   LV-VIT-S  & 83.3 & 6.6 & 879 \\ EViT  & **82.5** & 3.9 & - \\ \(\)TPS  & **82.5** & 3.8 & - \\ ToMe\({}^{}\) & 82.3 & 3.69 & 1574 \\
**DTEM** & **82.5** & 3.73 & 1571 \\   

Table 2: **Classification results with LV-ViT-S. \({}^{}\) indicates the results with off-the-shelf frozen pretrained model.**

Figure 2: **Classification results under different FLOPs and throughputs. All methods are end-to-end trained.**

detailed information about the image, which is crucial for accurate captioning. However, using all patches may be inefficient due to redundancy in image tokens, motivating the use of token merging.

SetupWe experiment with the COCO  caption dataset using the train/val/test split from . We use COCO fine-tuned GIT  models, each consisting of a ViT-B or L image encoder and a language decoder. The embedding modules are trained modularly with a language modeling loss and use the best model identified through cross-validation. The quality of captions is evaluated using metrics of BLEU-4 , METEOR , CIDEr , and SPICE , while the computational cost is reported in terms of the ViT encoder's floating-point operations (FLOPs) and the number of tokens (#) passed to the language decoder. More details are provided in the supplementary material A.2.

ResultsTable 4 presents the image captioning results on the test split when DTEM or ToMe  are applied. DTEM outperforms ToMe, achieving a better trade-off between captioning quality and computation cost. Specifically, with the GIT-B model, DTEM enhances the CIDEr score by \(+5.0\) to \(+6.0\) across reductions in FLOPs of \(31\%\) to \(41\%\). Similarly, we observe an improvement ranging from \(+2.3\) to \(+6.0\) with the GIT-L model. These results confirm that DTEM provides a better set of patch representations by effectively summarizing the information in the image tokens.

### Semantic Segmentation

To further demonstrate DTEM's applicability, we apply our method to semantic segmentation, a widely studied computer vision task with numerous applications.

SetupWe use a pre-trained Segmenter  model and evaluate the token merging on the ADE20K  dataset, which contains 25k training data across 150 fine-grained semantic categories. Unlike image classification or captioning tasks, segmentation models--including the Segmenter--require complete image patches (tokens) in the end to decode the segmentation mask. To address this, we follow the approach proposed in  that repeatedly merges tokens before each component (_e.g._, self-attention and feed-forward network) and then un-merges them after processing the component. We modularly trained our decoupled embedding modules using the cross entropy loss. We report mean intersection-over-union (mIoU) and floating-point operations (FLOPs) for performance and computational cost, respectively. More implementation details are provided in the Appendix A.2.

    & Reduction & B@4 & M & C & S & \# & Reduction & B@4 & M & C & S & \# \\   GIT-B  & - & 38.8 & 30.1 & 127.6 & 23.6 & 197 & GIT-L  & - & 40.7 & 29.6 & 134 & 23.8 & 197 \\  & 32\% & 34.6 & 26.4 & 113.1 & 20.3 & 77 &  & 31\% & 36.9 & 27.3 & 122.1 & 21.5 & 77 \\  & 35\% & 33.5 & 25.8 & 109.3 & 19.8 & 65 & & 37\% & 36.4 & 27.1 & 120.1 & 21.5 & 53 \\  & 38\% & 33.3 & 25.5 & 107.9 & 19.5 & 53 & & 43\% & 34.0 & 25.8 & 112.2 & 20.2 & 29 \\  & 41\% & 31.9 & 24.8 & 104.3 & 19.0 & 41 & & 49\% & 31.7 & 24.8 & 105.1 & 19.3 & 7 \\   & 31\% & 36.2 & 27.1 & 118.1 & 20.8 & 77 &  & 31\% & 37.9 & 27.8 & 124.4 & 21.9 & 77 \\  & 34\% & 34.5 & 26.5 & 114.2 & 20.5 & 65 & & 37\% & 37.0 & 27.5 & 122.9 & 21.7 & 53 \\  & 37\% & 34.3 & 26.2 & 112.9 & 20.1 & 53 & & 43\% & 35.7 & 26.6 & 117.6 & 20.9 & 29 \\  & 41\% & 33.3 & 25.7 & 110.4 & 19.9 & 41 & & 49\% & 33.3 & 25.7 & 111.1 & 20.1 & 7 \\   

Table 4: **Image captioning evaluation results when token merging is applied**. We report with caption evaluation metrics: BLEU-4 (B@4), CIDEr (C), METEOR (M) and SPICE (S). Reduction represents the decreases in FLOPs within the ViT encoder, and # indicates the number of tokens passed to language decoder.

    &  & Baseline &  \\   & & (\(r=0\)) & 0.2 & 0.3 & 0.4 & 0.5 \\   & ToMe  & GFLOPs &  36.28 \\ (100\%) \\  &  30.82 \\ (85.0\%) \\  &  27.18 \\ (74.9\%) \\  &  23.7 \\ (65.3\%) \\  &  20.46 \\ (56.4\%) \\  \\   &  & GFLOPs &  36.28 \\ (100\%) \\  &  29.39 \\ (81\%) \\  &  25.75 \\ (71\%) \\  &  22.27 \\ (61.4\%) \\  & 
 19.03 \\ (52.5\%) \\  \\   

Table 5: **Results on semantic segmentation when token merging is applied**. The reduction ratio indicates the portion of merged tokens.

ResultsTable 5 reports the semantic segmentation results when token merging methods, _i.e._, ToMe  and DTEM, are applied to Segmenter with ViT-S. Our method consistently offers a better mIoU to GFLOPs trade-off compared to ToMe. Specifically, DTEM achieves a +0.32 to +1.3 improvement in mIoU over ToMe when 25% to 50% FLOPs are reduced. These results demonstrates the applicability of DTEM in semantic segmentation for enhancing token merging.

### Analysis

We conduct an analysis of DTEM in ImageNet-1k classification, specifically within a modular training setting using the DeiT-S model, unless if otherwise stated.

Importance of decoupled embeddingIn Table 6, we ablate the impact of decoupled embedding in end-to-end training. We observe that naive integration of soft grouping and merging applied to the keys of self-attention, as in ToMe , degrades the performance. This confirms that decoupled embedding is crucial in DTEM to provide more compelling features for token merging. In Table 7, we further investigate whether the decoupled embedding used for merging diverges from the intermediate features after training. We report the Kendall rank correlation between token similarities derived from two different features--self-attention keys and decoupled embedding--before and after training. The results show a decreased correlation after training, indicating that the decoupled embedding learns a different feature for token merging, distinct from the intermediate features.

Design choices for soft groupingIn Table 8, for the analysis of soft grouping, we compared several approaches: (1) as the pruning baseline, we tested DynamicViT  applied to a frozen ViT, (2) integrating Gumbel Softmax (GS) to replace the top-1 operation from ToMe to enable differentiation, (3) our method, and (4) our method without proportional attention. The results indicate that our proposed design for soft grouping performs the best, with proportional attention proving to be crucial.

Embedding module designFigure 3 (a) and (b) show the impact of different embedding dimensions and the number of hidden layers in the embedding module, respectively. In the main results (Sec. 4.1), we use the embedding dimension of 64 and the module without hidden layers. We observe that a simple affine transformation offers sufficient gain while keeping computational costs low.

Effect of reduction rate on trainingIn Table 9, we analyze the effect of the reduction rate used during training. We observe that the decoupled embedding module, when trained with a high reduction rate \(r\), generalizes well to lower rates during inference. Therefore, it is generally sufficient to set the reduction rate \(r\) during training to the maximum number of tokens we wish to reduce during inference.

   Arch. & Method & Acc. (-35\%) & Acc. (-50\%) \\   & ToMe  & 79.68 & 79.25 \\  & + _soft token merging_ & 79.57 & 79.15 \\  & + _decoupled embedding_ & **79.85** & **79.38** \\   & ToMe  & 81.37 & 80.58 \\  & + _soft token merging_ & 81.48 & 80.49 \\   & + _decoupled embedding_ & **81.60** & **80.74** \\   

Table 6: **Ablation study on the impact of decoupled embedding**. We successively add _soft token merging_ and _decoupled embedding module_ into ToMe. The number in parentheses indicates the reduction in FLOPs.

    & 1 to 4 blocks & 5 to 8 blocks & 9 to 12 blocks \\  Before training & 0.517 & 0.457 & 0.591 \\ After training & 0.401 & 0.402 & 0.519 \\   

Table 7: **Kendall rank correlation coefficient changed through training. We report changes in the Kendall rank correlation between token similarities derived from two different features: self-attention keys and decoupled embedding.**

Figure 3: **Ablation study on decoupled embedding module design**: (a) decoupled embedding dimension and (b) number of hidden layers.

Data/Train efficiencyIn Figure 5 (a) and (b), we examine the data and training efficiency of DTEM with modular training, respectively. Owing to the parameter-efficient modular approach, DTEM improves the performance of merging even with a limited amount of training data and epochs. Specifically, in Figure 5 (a), our method achieves a gain of +0.44% accuracy even when trained with \(4000\) images, equaling 0.31% of the total dataset. DTEM outperforms the end-to-end training approach with ToMe under 10% of the total dataset for both 35% and 50% reduction of FLOPs, highlighting the potential benefit of our modular training when the entire dataset is unavailable. Moreover, in Figure 5 (b), the results on the effect of varying training epochs show that DTEM quickly converges even at the first epoch. Since DTEM employs modular training even in end-to-end learning by the alternative optimizations (Sec. 3.4), such rapid convergence is also useful in reducing the cost of end-to-end training.

VisualizationIn Figure 5, we visualize the token merging to compare ToMe  and our modularly trained DTEM. Tokens belonging to the same group are color-coded identically, highlighting the grouping changes induced by the decoupled embedding. DTEM prioritizes merging background tokens, allocating more tokens to foreground objects. In contrast, ToMe allocates more tokens to the background, indicating a less focused approach to foreground objects.

## 5 Conclusion

We propose Decoupled Token Embedding for Merging (DTEM) that improves token merging via decoupled token embedding derived directly from the token merging process. Our method introduces the decoupled embedding, learned through our continuously relaxed token merging, to exploit dedicated features for token merging. The decoupled embedding enhances token merging by resolving the dependency of token merging on intermediate features and enables modular training, effectively utilizing the frozen pre-trained models. We experiment with DTEM on classification, captioning, and segmentation using various pre-trained ViT models. The experimental results demonstrate that our method consistently improves token merging, highlighting the importance of features tailored specifically for token merging.