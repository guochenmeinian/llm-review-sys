# Real-Time Selection Under General Constraints via Predictive Inference

Yuyang Huo\({}^{1}\)  Lin Lu\({}^{1}\)  Haojie Ren\({}^{2}\)  Changliang Zou\({}^{1}\)

\({}^{1}\)School of Statistics and Data Sciences, LPMC, KLMDASR and LEBPS,

Nankai University, Tianjin, China

\({}^{2}\)School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, China

huoyynk@gmail.com, linlu102099@gmail.com

haojieren@sjtu.edu.cn, zoucl@nankai.edu.cn

Equal contribution, and the first two authors are listed in alphabetical order.Correspondence to: Haojie Ren <haojieren@sjtu.edu.cn>, Changliang Zou <zoucl@nankai.edu.cn.>

###### Abstract

Real-time decision-making gets more attention in the big data era. Here, we consider the problem of sample selection in the online setting, where one encounters a possibly infinite sequence of individuals collected over time with covariate information available. The goal is to select samples of interest that are characterized by their unobserved responses until the user-specified stopping time. We derive a new decision rule that enables us to find more preferable samples that meet practical requirements by simultaneously controlling two types of general constraints: individual and interactive constraints, which include the widely utilized False Selection Rate (FSR), cost limitations, diversity of selected samples, etc. The key elements of our approach involve quantifying the uncertainty of response predictions via predictive inference and addressing individual and interactive constraints in a sequential manner. Theoretical and numerical results demonstrate the effectiveness of the proposed method in controlling both individual and interactive constraints.

## 1 Introduction

In recent times, the field of real-time decision has flourished significantly, primarily driven by the exponential growth of available data in both the tech industry and computer science. We consider here a typical application of real-time decision, the problem of _online sample selection_. For instance, online recruitment systems utilize machine learning algorithms to sequentially choose qualified candidates rather than waiting for all (future) candidates' information to be collected . Additionally, recommendation systems have now become commonplace in providing real-time suggestions for content (e.g., news articles, short videos) with potential high click-through rates to users . Common situations also can be found in real-time precision marketing .

We describe the online sample selection problem as follows: samples (individuals) characterized by covariates \(_{t}^{d}\) arrive sequentially while their responses \(Y_{t}\) remain unobserved throughout the process. The data pairs \((,Y)\) of each time are i.i.d. random vectors. At each time point, the analyst is faced with the task of deciding whether to select the current observation based on certain predetermined criteria related to \(Y_{t}\), and this selection process continues until a specific stopping rule is triggered. For example, \(Y_{t}\) is the score that measures how one candidate fits a given job position in the recruitment system and the human resource agencies aim to prioritize candidates with higher \(Y_{t}\), such as \(Y_{t} b\). Or \(Y_{t}\) is a binary variable where \(Y_{t}=1/0\) means accepting or rejecting the offer, and the companies wish to find those individuals with \(Y_{t}=1\) based on the \(X_{t}\).

Our Goal and Motivation.Our goal is to sequentially select samples whose unobserved responses \(Y_{t}\)'s are in the specified target region. A natural idea is to make decisions based on the prediction value of \(Y_{t}\) from models associated with \((,Y)\) built on some historical data. However, neglecting the uncertainty in predictions could result in numerous false decisions, i.e., selecting those samples whose true responses are beyond the specified region. To measure the selection uncertainty, some existing works reformulate sample selection as a hypothesis testing problem and focus on controlling the online false discovery rate (FDR) [16; 33]; see more discussions and literature in Section 1.2. However, in addition to quantifying statistical uncertainty, online selections often need to take into account various constraints to find informative samples in practice, for example, cost limitations, the impacts of some covariates or the diversity of candidates in the online recruitment . Hence, it's necessary to explore the covariate space to satisfy these requirements. This motivates us to investigate how to efficiently implement online sample selection with statistical guarantees under various constraints.

To address this issue, we summarize common constraints into two types, i.e., _individual constraints_ and _interactive constraints_. The former one is relevant to the cost of selected samples, and one typical example is the fundamental and crucial criterion, false selection rate (FSR), which quantifies the proportion of falsely selected samples and is equivalent to the well-adopted FDR. The latter constraint captures the interactive influence among selected samples and is regarded as some kind of quadratic constraint on some pairwise functions, such as the similarity or diversity among selected samples.

While the individual constraint associated with online FDR control has gained some attentions [16; 18], it alone fails to capture the nuanced pairwise relationships among different samples. To bridge this gap, we introduce interactive constraints, which are pivotal within our framework. Building upon individual constraints, the interactive criteria significantly expand the range of constraints our method can control. Integrating two distinct types of constraints into a unified framework makes it easier to create practical algorithms and ensures theoretical guarantees.

A motivating example: candidate screening.As an example, in recruitment, screening from the resumes arriving sequentially to determine viable candidates who can get into interview processes is an important problem in human resource management [13; 35]. In this case, one may be interested in: (1) controlling the online \(\) to enhance resource efficiency , and (2) maintaining a desired level of candidate diversity during the screening process, thereby reducing bias [23; 44]. The individual and interactive constraints and the novel real-time sample selection procedure we propose can solve this problem precisely. See Section 4.2 for the details and more real data examples.

### Our Contributions

In this paper, we design a novel and flexible online selection rule to effectively ensure the above two types of constraints are under control at pre-specified levels simultaneously, named as "Individual and Interactive Constrained Online Selection" (II-COS). The main idea stems from an oracle model based on the local false discovery rate (IFDR) [12; 38], which is involved in offering valid evidence on whether \(Y_{t}\) is our interest at each time point. With some appropriately chosen evaluating functions, the II-COS procedure entails validating whether the estimates of constraints are controlled. Simulated and real-data examples clearly demonstrate the superiority of the II-COS in terms of both online individual and interactive criteria control.

To the best of our knowledge, this is the first work to systematically bridge the predictive inference and online selection procedure with various constraints. Our contributions are summarized as follows:

* Under a unified framework, the II-COS addresses how to implement online predictive selection sufficiently in consideration of both individual and interactive constraints. It is flexible to characterize the selective uncertainty and trade-off the sampling efficiency and practical limitations in the covariate space.
* Under mild conditions, we establish the theoretical guarantee that the II-COS is able to control both individual and interactive constraints simultaneously and asymptotically under one given stopping rule.
* The II-COS is model-agnostic in the sense that its implementation is applicable to any (appropriate) learning algorithms. Extensive numerical experiments indicate that the II-COS can significantly outperform existing ones while yielding effective constraints control.

### Related Works

Our proposed method is built upon two fundamental pillars: (1) quantifying the uncertainty of response predictions using predictive inference; and (2) systematically addressing individual and interactive constraints in a sequential manner. Our work is intricately connected to the fields of predictive inference and online multiple testing. Here we briefly review literature on these two topics.

**Predictive Inference.** One key ingredient of our proposed method is predictive/conformal inference. Conformal inference [42; 34] provides a powerful and flexible tool to achieve algorithm-agnostic uncertainty quantification of predictions. Conventionally, conformal inference aims to build the prediction intervals and enjoys valid and distribution-free properties by leveraging data exchangeability [25; 3]. Taking a different but related perspective from multiple-testing, Bates et al.  pioneered a method to construct conformal \(p\)-values to detect outliers with finite-sample FDR control. Building upon this, recent advancements have improved detection power by involving more information or performing model selection [50; 26; 27; 51; 46]. The most related works are Jin and Candes  and Wu et al. , which considered a similar scenario that one would like to select some individuals of interest by controlling FDR or maximizing the diversity of selected samples in an off-line setting. Their methods are based on the conformal \(p\)-values or IFDR constructed with the predicted response values, respectively. Besides the fundamental difference between online and offline paradigms, our framework for characterizing various constraints poses additional challenges in how to select samples sequentially since we have multiple goals to achieve.

**Online Multiple Testing.** When only considering individual constraint as FSR control, the online sample selection can be reformulated as online multiple testing problem. Methods for online multiple testing have received much recent attention and were pioneered by Foster and Sune  who proposed the so-called \(\)-investing strategy, which was later built upon and generalized [1; 29; 30; 18; 19]. The key idea in \(\)-investing and its generalizations is to compare \(p\)-values with dynamic thresholds and gain some extra \(\)-wealth for each rejection. We refer to Robertson et al.  for a thorough overview. Those rules suffer from the "alpha-death" issue to some extent , which means a permanent end to decision-making when the decision threshold is too small, i.e., the online procedure stops early. This phenomenon occurs in many existing online multiple testing algorithms, as discussed in . Along a different direction, Gang et al.  developed a new class of structure-adaptive sequential testing (SAST) rules built on the IFDR to avoid the alpha-death issue. The SAST serves as a building block for developing our II-COS procedure and can be essentially seen as a special case of ours. Later on, Ao et al.  reformulated online multiple testing procedure into an online knapsack problem, providing novel policies with near-optimal regret guarantees. Additionally, Xu and Ramdas  proposed to use e-values  for online multiple testing to address the dependence. However, those existing works do not take predictive inference into account and are concerned only with online error rate control without exploration of the covariate space, which may greatly hamper their applicability.

## 2 Individual and Interactive Constrained Online Selection Procedure

### Problem Formulation

Assume there exists a historical labeled dataset as \(=\{}_{i},_{i}\}_{i=1}^{n}\), where \((}_{i},_{i})\)'s are independent and identically distributed (i.i.d.) from \((,Y)\). A sequence of unlabeled data \(_{1},_{2},=(X_{1},,X_{d})^{}\) arrives in a stream with unknown responses \(Y_{1},Y_{2},\). At each time \(t\), one must make a real-time decision about whether or not to select the \(t\)-th individual, which is determined by some pre-specified requirement on \(Y_{t}\). Denote \(\) as the target region of \(Y_{t}\), which differs depending on users' specifications. For example, in a regression setting, the requirement could be of the form \(Y_{t}[a,b]\), \((-,a)\) or \(Y_{t} b\).

Let \(_{t}=\{Y_{t}\}\) describe the true state of \(Y_{t}\). Denote a decision rule as \(_{t}\{0,1\}\), where \(_{t}=1\) indicates that the \(_{t}\) is selected and \(_{t}=0\) otherwise. A false selection is made if \(_{t}=1\) but \(_{t}=0\). Denote \(^{t}=\{_{i}:i t\}\) as the decision rule and \(T\) as the time that the procedure stops. Our goal is to build a decision rule \(^{t}\) to select samples with \(\{Y_{t}\}\) up to stopping time \(T\) such that the following two general types of constraints hold simultaneously.

Individual Constraint.In practice, one main concern is to control the cost of selecting samples of interest. For example, in online recruitment, companies need to control the proportion of selectedunqualified candidates or the average loss when hiring someone who rejects the offer. In such cases, we can assign each selected sample a cost associated with some pre-specified function of the covariate \(X\) and control the expected cost associated with time \(T\) at the target level. We refer to this requirement as _individual constraint_ and write it as:

\[C_{1}(^{t})=[\{(1-_{i})G_{ 0}(_{i})+_{i}G_{1}(_{i})\}_{i}}{(_{i t }_{i}) 1}],\] (1)

where \(a b=\{a,b\}\) and \(G_{0}(X) 0\) and \(G_{1}(X) 0\) with \(G_{0} G_{1}\) are the costs corresponding to \(=0\) and \(=1\), respectively. Here, we take expectation due to the randomness of \(_{1},,_{t}\).

For example, when we simply choose \(G_{0}()=1\) and \(G_{1}()=0\), the individual constraint is the popular false selection rate (FSR), i.e.

\[C_{1}(^{t})=(^{t})=[(1-_{i})_{i}}{(_{i t}_{i}) 1} ].\] (2)

The FSR is essentially equivalent to the well-adopted FDR in multiple testing literature, which is a useful tool to maintain the ability to reliably select samples of interest without excessively false selections . Some works on online FDR control have been well studied. [16; 1].

The individual constraints alone cannot capture the pairwise relationship among different samples. We address this by introducing interactive constraints below.

Interactive ConstraintAnother common concern is the _interactive constraint_, which involves choosing more preferable samples. For example, companies would like to retain candidates with a diverse range of backgrounds and experiences in online recruitment, or real-time suggested contents are required to avoid homogeneity in recommendation systems. Here, we introduce a bi-variate weight function \(g(,^{})\) to evaluate the interaction between selected samples. Denote \((^{t})=_{1 i<j t}(_{i},_{j})_{i}_{j}_{i}_{j}\), \((^{t})=_{1 i<j t}_{i}_{j }_{i}_{j}\). We define the interactive constraint as

\[_{2}(^{t})=[( ^{t})}{(^{t})}].\] (3)

Here, since only the correctly selected samples are of interest, the constraint is concerned with the average mutual effects between the correctly selected ones rather than all selected ones. When choosing the function \(g\) as some similarities, controlling \(_{2}(^{t})\) at a specified constant \(K\), i.e., \(_{2}(^{t}) K\), is controlling the expected similarity (ES). It is equivalent to requiring that correctly selected samples exhibit certain diversity and rich information in the covariate space of interest.

Typically, one useful choice for \(g(,^{})\) is the weighted RBF kernel \(g(,^{})=\{-}_{k=1}^{ d}w_{k}(X_{k}-X_{k}^{{}^{}})^{2}\}\) with parameter \(>0\) to measure the similarity between two independent \(\) and \(^{}\). The RBF kernel is a common and widely embraced choice in machine learning [49; 28]. Here, \(\{w_{1},,w_{d}\}\) are some given weights per users' needs. For instance, if one is just interested in the effects of the \(k\)-th feature, then simply \(w_{k}=1\) and \(w_{j}=0\) for \(j k\). Specifically, the case that \(w_{k}=1\) for all \(k=1,,d\) is chosen in Section 4. We also consider other similarity choices of \(g(,^{})\), such as the cosine similarity \(g(,^{})=^{}^{}/(\| \|_{2}\|^{}\|_{2})\).

Due to the randomness in the denominator, it turns out controlling (3) directly is not easy. Instead, we employ a modified interactive constraint,

\[C_{2}(^{t})=[(^{t}) ]}{[(^{t})]}.\] (4)

The constraint (4) aims to control a ratio of expectations, which is still a reasonable interactive measure. In numerical studies, we see that \(_{2}(^{t})\) in (3) and \(C_{2}(^{t})\) in (4) yield almost identical patterns. An illustrative example can be found in Appendix D.1.

In sum, the goal is to select samples of interest by a decision rule \(^{T}\) controlling both the individual and interactive constraints until stopping time \(T\), i.e., \(C_{1}(^{T})\) and \(C_{2}(^{T}) K\). We emphasize that \(C_{1}(^{T})\) and \(C_{2}(^{T})\) as well as their pre-specified levels \(\) and \(K\) can be chosen up to the practical applications.

### Oracle Selection Procedure

To design a general rule that is valid for any arbitrary stopping time \(T\), we consider controlling the constraints at each time \(t\) in an online fashion, such that \(_{t}C_{1}(^{t})\) and \(_{t}C_{2}(^{t}) K\).

Since \(Y_{t}\) is unavailable, we consider utilizing predictive inference to measure the suspicious patterns. Let \(():=Y=\) be the regression or classification model associated with \((_{t},Y_{t})\), and one reliable estimate as \(()\), being estimated on the labeled data \(\) with some machine learning algorithm. Denote \(W_{t}=(_{t})\) as a predicted value of \(Y_{t}\) and assume that \(()\) is a bijection almost surely. The bijection assumption is considerably mild and widely adopted for the identification of each \(X_{t}\) in the predictive inference framework . The \(_{t}=(Y_{t})\) is Bernoulli(\(\)) distributed with \(=(Y_{t})\), and \(W_{t}\) can be viewed as generated from one two-group model

\[W_{t}_{t}(1-_{t})f_{0}+_{t}f_{1},\]

where \(f_{0}\) and \(f_{1}\) denote the probability distribution functions of \(W_{t}\) conditional on \(Y_{t}\) (i.e., \(_{t}=0\)) and \(Y_{t}\), respectively. Then, the conditional probability of \(Y_{t}\) is

\[L_{t}=(_{t}=0 W_{t})=(W_{t})}{f(W_{t})},\] (5)

where \(f=(1-)f_{0}+ f_{1}\). The \(L_{t}\) coincides with the local FDR in multiple testing literature [12; 17]. With the two-group model (5), we have \([_{t}_{t}]=1-L_{t}\) and further notice that the individual constraint \(C_{1}(^{t})\) in (1) can be exactly satisfied if

\[}{R_{t}}:=\{L_{i}G_{0}(_{i})+(1-L_{i })G_{1}(_{i})\}_{i}}{(_{i t}_{i}) 1},\]

holds. Here, we denote \(V_{t}=_{i t}\{L_{i}G_{0}(_{i})+(1-L_{i})G_{1}(_{i} )_{i}\}\) and the number of selected ones as \(R_{t}=_{i t}_{i} 1\) for notational convenience. Especially, when \(G_{0}()=1\), \(G_{1}()=0\), then \((^{t})\) in (2) can be exactly controlled.

Accordingly, the interactive constraint \(C_{2}(^{t}) K\) in (4) can be achieved if

\[_{t}}{_{t}}:=g( _{i},_{j})(1-L_{i})(1-L_{j})_{i}_{j}}{ _{1 i<j t}(1-L_{i})(1-L_{j})_{i}_{j}} K,\]

where the expected total mutual effects conditional on \(\{_{i}\}_{i t}\) and the expected number are denoted as \(_{t}\) and \(_{t}\), respectively.

Therefore, if \(L_{t}\) is known, when a new sample \(_{t}\) arrives at time point \(t\), we can perform the decision rule as follows. Note that there is no need to consider interactive effects before the first selection. When \(t\) comes before the first selection (i.e, \(R_{t-1}=0\)), the decision rule is \(_{t}=1\) if

\[+L_{t}G_{0}(_{t})+(1-L_{t})G_{1}(_{t})}{R_{t -1}+1},\] (6)

holds; otherwise, \(_{t}=0\) which means \(_{t}\) is not selected. When \(_{t}\) arrives with \(R_{t-1} 1\), then \(_{t}=1\) if (6) and

\[_{t-1}+[_{i t-1}g(_{i},_{t})(1-L_{i})_{i}](1-L_{t})}{_{t-1}+[_{ i t-1}(1-L_{i})_{i}](1-L_{t})} K\] (7)

hold simultaneously; otherwise, \(_{t}=0\). Note that if we set \(C_{1}(^{t})\) as FSR(\(^{t}\)) and choose \(K=+\), then our method essentially reduces to the same manner as the controlling step of the SAST in Gang et al. . Our proposed method can be seen as a much more generalized and flexible framework for controlling both the individual and the interactive constraints simultaneously in an online fashion.

We call this method the oracle II-COS (Individual and Interactive Constrained Online Selection). The workflow in Figure 1 shows the procedure of the oracle II-COS. The following result shows that it can exactly achieve our goal.

**Proposition 2.1**.: _Assume \(L_{t}\) values are known. Then the oracle II-COS selection rule controls both constraints at any stopping time \(T\), i.e., \(C_{1}(^{T}) C_{2}(^{T})  K.\)_

### Data-driven II-COS Procedure

As \(L_{t}\) is unknown in practice, we propose a data-driven II-COS procedure, which uses a reliable estimation \(_{t}\) for implementation. We resort to a data-splitting strategy: randomly split historical data \(\) into two parts, the training set \(_{}\) and the calibration one \(_{}\) of sizes \(n_{0}\) and \(n_{1}\) respectively, where \(_{}\) is used for training a predictive model and \(_{}\) is to estimate those unknown parameters. Specifically, we first fit a regression or classification model \(()\) on \(_{}\), and then obtain predicted values on \(_{}\), i.e. \(\{(}_{i}):(}_{i},} _{i})_{}\}\). Note that conditional on \(_{}\), \(\{(}_{i}):(}_{i},} _{i})_{}\}\) are i.i.d. random variables and have the same distribution as \(W_{t}=(_{t})\), so that it can be utilized to estimate (5).

Therefore, the estimators of \(f_{0}\) and \(f\), \(_{0}\) and \(_{i}\) can be obtained by applying the kernel density estimation method to the data \(\{(}_{i}):(}_{i},} _{i})_{},}_{i}\}\) and \(\{(}_{i}):(}_{i},} _{i})_{}\}\), respectively. And the probability \(=(Y_{t})\) can be approximated by \(=n_{1}^{-1}_{(}_{i},}_{i}) _{}}(}_{i})\). Further the \(L_{t}\) in (5) can be estimated by

\[_{i}=)_{0}(W_{i})}{(W_ {i})} 1.\] (8)

The data-driven II-COS procedure is summarized in Algorithm 1, and it indeed consists of two phases: offline estimation and online decision. The running time of offline estimation is not critical. At each time \(t\), the computational complexity is a linear function of the currently selected number \(R_{t}\). More implementation details can be found in Section 4 and Appendix B.1.

In fact, the proposed II-COS is flexible to trade off the individual and interactive constraints by adjusting the thresholds \(\) and \(K\). If one is concerned only with individual cost control, then we can set \(K=+\), with which the interactive constraint is out of work. Similarly, only the interactive effect is of interest when \(=1\). Appendix D.2 provides a toy example to illustrate this.

Before further pursuing, we would discuss the stopping time in practice. It's worth noting that the specific choice of stopping rule (and thus stopping time) is completely up to the user. For example, when \(m 2\) is the desired number of selections, one can set \(T=_{t}\{t:_{i=1}^{t}_{i}=m\}\). Or when \(s\) is the total wages for recruitment, one can set \(T=_{t}\{t:_{i=1}^{t}_{i}s_{i}=s\}\) where \(s_{i}\) is the payroll for each selected candidate. Or \(T\) is just chosen as one given deadline. With the use of II-COS, practitioners have the flexibility to design diverse stopping strategies that can adapt seamlessly to their specific applications. In brief, our method is flexible and is appropriate for various goals based on the user's requirements by choosing different \(G_{0}(),G_{1}(),g(_{i},_{j})\) and varied target levels \((,K)\) and a user-specified stopping time.

Figure 1: The implementation flowchart of the oracle II-COS procedure.

**Input:** Target levels \(\) and \(K\), pairwise function \(g\), cost \(G_{0}()\) and \(G_{1}()\), stopping time \(T\), interested region \(\), labeled data \(\), prediction algorithm \(\).

**Initialization:**\(t=0\), \(V_{t}=R_{t}=0\), \(_{t}=_{t}=0\); Decision rule \(^{t}=\)

**Estimation:** Randomly split \(\) into training set \(_{}\) and calibration set \(_{}\). On \(_{}\), fit \(()\) with \(\). Obtain \(\), \(_{0}\) and \(\) from \(_{}\).

**Online decisions: while \(t T\) do**

Set \(t=t+1\). Compute \(W_{t}=(_{t})\), \(_{t}\) by (8) and the similarities \(g(_{i},_{t})\) for those \(_{i}=1\). Replace \(L_{t}\) by \(_{t}\) in (6) and (7).

**if \(R_{t-1}<1\) and (6) holds then \(_{t}=1\)**;

**else \(_{t}=0\)**;

**if \(R_{t-1} 1\), (6) and (7) hold then \(_{t}=1\)**;

**else \(_{t}=0\)**;

Update: \(^{t}=^{t-1}\{_{t}\}\); \(R_{t}=R_{t-1}+_{t}\); \(V_{t}=V_{t-1}+\{_{t}G_{0}(_{t})+(1-_{t})G_{1 }(_{t})\}_{t}\);

\(_{t}=_{t-1}+[_{i<t}g(_{i},_{t})(1-_{i})_{i}](1-_{t})_{t}\); \(_{t}=_{t-1}+[_{i<t}(1-_{i}) _{i}](1-_{t})_{t}\);

**end Output:** Selection set \(\{_{i}:_{i}=1,\;_{i}^{T}\}\).

**Extension to varying proportion case.** In practice, the distribution of \((_{t},Y_{t})\) may vary smoothly over time. In Appendix C, we consider the probability of \(Y_{t}\) (i.e. the proportion of samples in the specified region) varying over time and extend the proposed II-COS to learn \(_{t}=(Y_{t})\) continuously over time and we also construct the corresponding theoretical guarantees.

## 3 Statistical Performance Guarantees

In this section, we provide statistical guarantees for the data-driven II-COS procedure. The main difficulties lie in the quantification of data-driven estimation error of \(L_{t}\) and we utilize the classical kernel density estimation theory along with the structure of our online procedure to effectively characterize it. For simplicity, we consider that the training data set is given such that the estimated model \(\) is fixed. Before presenting our theoretical results, we state the following regularity conditions.

**Assumption 3.1** (Density functions and kernel).: The density functions and kernel function satisfy

1. The \(f_{1}()\) and \(f_{0}()\) are upper bounded by \(M>0\), and the \(f()\) is lower bounded by \(>0\).
2. The \(f_{0}\) and \(f_{1}\) are Holder-continuous, i.e. \(|f_{0}(w)-f_{0}(w^{})| c_{}|w-w^{}|^{}\) for any \(w,w^{}\), and the same for \(f_{1}\) with some fixed \(0< 1\) and constant \(c_{}\).
3. Kernel \(K()\) is a bounded symmetric function and enjoys exponential decay.

**Assumption 3.2** (Weight functions).: There exists constants \(c_{G}>0\) and \(c_{g}>0\) such that \(0<G_{0}() c_{G}\), \(0<G_{1}() c_{G}\) for any \(\) and \(0<g(,^{}) c_{g}\) for any \(^{}\).

Assumption 3.1 is considerably mild and widely adopted in the uniform convergence of kernel density estimation . If \(f_{0}\) and \(f_{1}\) have bounded first-order derivatives, the Holder-continuous assumption would hold with \(=1\). The lower bound of \(f\) is to ensure the uniform convergence of the estimated \(\). Assumption 3.2 is mild since the weight functions are required only to be positive and bounded. It can be satisfied by a large category of \(G_{0}\), \(G_{1}\) and \(g\). For example, we can take \(G_{j}()=a_{j}\|\|_{2}^{2}\) for \(j=0\) or 1 and \(c_{G}\) exists when \(\) is bounded. And we can set \(g\) as the RBF and orthogonal similarities with \(c_{g}=1\).

With those regularity conditions, we establish the validity of the II-COS procedure for the individual constraint control.

**Theorem 3.3** (Bound for individual constraint).: _Suppose Assumptions 3.1 and 3.2 hold and take the bandwidths for estimating \(f\) and \(f_{0}\) in the order of \(n_{1}^{-1/(2+1)}\). Then for any given time \(t\), the individual constraint of the II-COS procedure (Algorithm 1) satisfies \(C_{1}(^{t})+_{n_{1}},\) where \(_{n_{1}}=Dn_{1}^{}}\) and \(D\) is a constant depending on \(M\), \(\), \(c_{}\), \(\), \(\), \(c_{G}\) and \(K()\)._

Although the \(C_{1}()\) of II-COS might be slightly larger than the target level in finite samples, this gap converges to \(0\) asymptotically as \(n_{1}\) increases. In the numerical studies, we find that a small calibration size of around \(200\) is enough to control the \(C_{1}\) in a reasonable range. Taking the FSR as an individual constraint, Theorem 3.3 indicates that our method can provide asymptotic online FSR control similar to an online FDR control procedure .

The next theorem examines the performance of the II-COS in terms of interactive constraint.

**Theorem 3.4** (Bound for interactive constraint).: _Suppose Assumptions 3.1-3.2 hold and take the bandwidths for estimating \(f\) and \(f_{0}\) in the order of \(n_{1}^{-1/(2+1)}\). Let \(T_{s}=\{t:_{i=1}^{t}_{i}=s\}\) for \(s>2\) and assume there exists a constant \(^{}(0,1)\) such that \(_{i t}_{i}_{i}/(1 R_{t})^{}\,.\) Then for any given time \(t T_{m}\), the interactive constraint of the II-COS satisfies_

\[C_{2}(^{t}) K+)_{n_{1}}}{0.5-}{m-1}-_{n_{1}}}.\]

The term \(m^{}/(m-1)\) is used to characterize the lower bound of the denominator term of the interactive constraint. Specifically, when we choose FSR as the individual constraint, we have \(^{}=\), which demonstrates the interdependence between controlling individual and interactive constraints. Furthermore, under arbitrary stopping strategies with a stopping time \(T\), we can have the asymptotic guarantee.

**Corollary 3.5**.: _Suppose the conditions in Theorem 3.4 hold, the stopping moment \(T T_{m}\) and \(^{}<(1-1/m)/2\). Then the II-COS procedure controls the individual and interactive constraints asymptotically at \(T\), i.e. \(_{n_{1}}C_{1}(^{T}) _{n_{1}}C_{2}(^{T}) K\)._

## 4 Experiments and Evaluation

We illustrate the breadth of applicability of the II-COS procedure by experiments on simulated data and real-data applications. As an example, we set the stopping rule as selecting total \(m=100\) samples, i.e., \(T=T_{m}=_{t}\{t:_{i=1}^{t}_{i}=m\}\). Additional experiments including the extended II-COS in Appendix C are shown in Appendix D.9. Code for implementing II-COS and reproducing the experiments and figures in our paper is available at https://github.com/lulin2023/II-COS.

**Implementation of II-COS**. To our best knowledge, online selection with uncertainty qualification has only been studied in the field of online multiple testing, which aims to control online FDR. Hence, we focus on using FSR as the individual criterion and modified ES as the interactive criterion. As \(\) may be measured on scales with widely differing ranges in different dimensions, we assume that \(\)'s have been properly scaled in each dimension before computing \(g\). We choose \(g\) as the weighted RBF kernel with \(=1\), \(w_{k}=1\) here. Other choices for individual and interactive constraints are considered in Appendix D.5.

**Benchmarks**. We compare the II-COS procedure with four benchmarks from online multiple testing. The first one is a structure-adaptive sequential testing rule, the **SAST**, which is implemented with \(_{t}\). It can achieve the FSR control but ignore the interactive constraint. As mentioned earlier in Section 2, SAST can also be considered as a special case of our II-COS with \(K=+\). Its details are deferred to Appendix B.2. The other competitors are three well-known online FDR control algorithms **LOND**, **SAFFRON** and **ADDIS** implemented with the conformal \(p\)-values suggested by Bates et al. . Refer more information in Appendix B.3. All the benchmarks can only control FDR, which demonstrates the flexibility of our method for different constraints.

**Performance Measures**. The empirical FSR, ES and stopping time (\(T_{m}\)) are evaluated using the average values of the false selection proportions, the similarity and the stopping time from 500 replications, respectively, where \(T_{m}\) serves as a criterion for assessing selection efficiency.

### Results on Synthetic Data

**Data Description.** We consider a classification model: \( Y=0_{4}(_{1},_{4})\), and \( Y=1_{4}(_{2},_{4})\), where \(_{1}=(5,0,0,0)^{},_{2}=(0,0,-3,-2)^{}\). We set \((Y=1)=0.2\). The information set is \(=\{1\}\). The predictor \(\) is taken as random forest with defaulted parameters. We also consider a regression setting and conduct additional experiments in Appendix D.

Firstly, we observe that methods relying on conformal \(p\)-values, such as LOND, SAFFRON, and ADDIS, encounter the alpha-death (stop early) issue . These methods struggle to select an adequate number of samples, especially in small calibration sets. In contrast, II-COS ensures the control of both individual and interactive constraints even with a small \(n_{}\) (e.g., 200). See more details and results in Appendix D.3. Hence, to make a fair comparison, we consider a relatively large size of the calibration set, \(n_{}=4,000\). We fix training data size \(n_{}=1,000\).

**Results.** Figure 2 presents the online FSR (false selection rate) and ES (expected similarity) values of five methods against time \(t\), which are individual and interactive constraints, respectively. The FSR levels of II-COS and SAST are closer to the nominal level than the other three methods. As expected, only the empirical ES levels of II-COS are controlled under the pre-specified level \(K\) over time \(t\). The LOND and SAFFRON lead to slightly conservative FSR values, while the FSR levels of ADDIS are inflated compared to the target level. Figure 3 further displays the boxplots of empirical FSR, ES at stopping time \(T_{m}\). We observe that only II-COS achieves satisfactory ES values compared to the nominal level. Moreover, the II-COS has a relatively larger value of \(T_{m}\) compared to those of other benchmarks. This is consistent with the fact that the II-COS spends more time exploring the structure information inside the covariate space due to the requirement of interactive constraint. Similar conclusions for the regression example in Appendix D.4 can be drawn.

Regarding efficiency, we also conducted an experiment to compare the effectiveness of II-COS with an oracle method possessing knowledge of true state \(_{t}\). The \(T_{m}\) of II-COS is very close to the oracle. This close proximity indicates the high efficiency of II-COS. See Appendix D.8 for the details.

### Results on Real Data

We next demonstrate the performance of the II-COS in two real-world applications. Since those online multiple testing methods based on conformal \(p\)-values yield few selected individuals, we focus

Figure 3: Boxplots of \((^{T_{m}})\), \((^{T_{m}})\) and stopping time \(T_{m}\) for II-COS, SAST, LOND, SAFFRON and ADDIS. The black dashed lines indicate the corresponding nominal levels.

Figure 2: The values of \((^{t})\) and \((^{t})\) over time \(t\) for II-COS, SAST, LOND, SAFFRON and ADDIS. The black dashed lines denote the FSR level \(=0.1\) and the ES level \(K=0.045\). Shading represents error bars of one standard error above and below.

on comparing the \(\)-COS with the SAST. For comparison, we also include the offline method using conformal \(p\)-values, where \(_{t}=1\) if \(_{t}\). We denote it as CP.

**Datasets.** We consider the recruitment dataset from Kaggle  that contains 45,372 candidates after removing the missing data and records a binary response indicating whether the candidate passes the job interview, and other 11 attributes including education status, handicapped or not, and gender. The other problem is to use 1994 Census Bureau dataset  to select a subset of individuals who may have high incomes in precision marketing. This census dataset records 32,561 individuals with their 14 attributes, including gender, race, marriage, education length and so on.

For each dataset, we randomly partition the data into three parts: \(n_{}=1,000\) training data, \(n_{}=1,000\) calibration data and the rest which are used as the online observations. The categorical attributes are converted into one-hot codes and then are treated as numerical attributes for computing similarity measures. The prediction algorithm \(\) is random forest with defaulted parameters.

**Results.** Table 1 reports the results among 500 repetitions. Both the II-COS and SAST enjoy valid FSR control, but CP yields an inflated FSR level in income investigation. The II-COS performs well in terms of similarities. To further compare the diversities, we present the proportions of different education status in in Figure 4. It can be seen that the proposed II-COS demonstrates its superior diversity in the specific attributes. See Appendix D.6 for more results for the real data. In summary, the proposed II-COS works well for selecting individuals of interest to achieve various constraints in practical applications.

## 5 Concluding Remarks

**Broader Impacts.** This work focuses on creating reliable machine learning tools for making real-time decisions. One key achievement is a new algorithm called II-COS, designed to select informative samples in real-time while meeting two types of general constraints. II-COS allows for both individual and interactive control, validated through theoretical analysis and numerical tests. Our method is model-agnostic and easily applicable to many real-world cases such as producing diversified results while controlling FSR for online recruitment. One potential negative impact of our work is that researchers will apply the algorithm without sufficient scrutiny. We emphasize that it's important to use caution when applying this method to complex real-world scenarios to prevent misuse.

**Limitations.** Firstly, we mainly consider binary functions as the interactive constraint. How to adapt the II-COS to other popular constraints, such as the Gini index, deserves further study. Secondly, in certain practical scenarios, it is possible to obtain feedback after decisions. Incorporating the feedback information into our method to enhance its performance warrants future research.

Table 1: Average values with candidate dataset and income dataset: \((^{T_{m}})\), \((^{T_{m}})\) (\( 10^{-3}\)) and stopping time \(T_{m}\). The target FSR level is \(=0.2\) for both. For the candidate data, the target ES level \(K=1 10^{-3}\); For the income data, \(K=6 10^{-3}\). The bracket contains the standard error.

Figure 4: Left: Education status composition of the correctly selected samples (II-COS, SAST and CP) in candidate dataset; Right: Education length (year) composition of the correctly selected samples (II-COS, SAST) in income dataset. The plots have error bar to show the variation across the 500 runs.