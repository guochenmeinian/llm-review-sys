# Big Batch Bayesian Active Learning by Considering Predictive Probabilities

Sebastian W. Ober

Biologics Engineering

AstraZeneca

Gaithersburg, MD, USA

sebastian.w.ober@gmail.com

&Samuel Power

School of Mathematics

University of Bristol

Bristol, UK

sam.power@bristol.ac.uk

Tom Diethe

Centre for AI, Biopharmaceuticals R&D

AstraZeneca

Cambridge, UK

tom.diethe@astrazeneca.com

Henry B. Moss

School of Mathematics

Lancaster University

Lancaster, UK

h.moss@damtp.cam.ac.uk

###### Abstract

We observe that BatchBALD, a popular acquisition function for batch Bayesian active learning for classification, can conflate epistemic and aleatoric uncertainty, leading to suboptimal performance. Motivated by this observation, we propose to focus on the predictive probabilities, which only exhibit epistemic uncertainty. The result is an acquisition function that not only performs better, but is also faster to evaluate, allowing for larger batches than before.

## 1 Introduction

Batch active learning attempts to acquire batches of data that will be the most informative in building a model. In the Bayesian active learning setting, where the surrogate model directly gives its certainty in the latent function, BatchBALD  has been a popular algorithm for batch acquisition in classification. By extending the work of Houlsby et al.  and considering the mutual information over the whole batch, BatchBALD increases batch diversity over a naive greedy approach. However, BatchBALD has a number of issues. First, in its closed form, it is limited by its exponential memory cost in batch size as well as an exponential computation cost. Moreover, as we depict in Fig. 1, despite its use of the mutual information on the whole batch, it is still susceptible to choosing similar points. We argue that the latter of these is caused by a conflation of the surrogate model's epistemic uncertainty (reducible uncertainty in the parameters or function values) and aleatoric uncertainty (irreducible uncertainty due to noise),1 motivating us to explore methods of focusing on the model's epistemic uncertainty alone. Coincidentally, we show that by focusing on the continuous space of predictive probabilities, we can avoid the excessive combinatorial cost of enumerating all possible discrete outputs.

Workshop on Bayesian Decision-making and Uncertainty, 38th Conference on Neural Information Processing Systems (NeurIPS 2024).

tasks. We assume that we have already collected an initial dataset \(=(,)\) of input-output pairs, \(\{_{n},y_{n}\}_{n=1}^{N}\) with \(_{n}\) and \(y_{n}\{0,,C-1\}\), where \(C\) is the number of classes. In batch active learning, we acquire a new batch of data \(_{B}=\{_{b},y_{b}\}_{b=1}^{B}\) by using a surrogate model trained on \(\) to select a new batch of query points \(\{_{b}\}_{b=1}^{B}\), which we send to an oracle to obtain \(\{y_{b}\}_{b=1}^{B}\). We repeat this process multiple times, until we exhaust our budget of either data or computational resources. By using knowledge of previously acquired data, we hope that we can be more data-efficient than by simply acquiring a single large dataset from the start.

### Bayesian active learning

In Bayesian active learning, we employ a Bayesian surrogate model, which imposes a prior distribution over functions, \((f)\), either explicitly, as with Gaussian process (GP) models , or implicitly through a prior distribution over model weights, \(()\). Given a likelihood \((y|f,)\) and data \(\), we can obtain the posterior \((f|)\) through Bayes' rule. This posterior is then used in tandem with an acquisition function to acquire the new batch of query points:

\[\{_{1}^{*},,_{B}^{*}\}=*{ arg\,max}_{\{_{1},,_{B}\}}a( \{_{1},,_{B}\};\,(f| )).\]

These query points are passed to an oracle to obtain \(_{B}\), and we update the dataset \(_{B}\).

For \(B=1\), a popular acquisition function for Bayesian active learning is known as Bayesian active learning by disagreement [BALD; 5], which takes the form

\[a_{BALD}(;\,(f|)) =(y;\,f|,)\] \[=(y|,)-_{ (f|)}[(y|,f, )],\]

where \(\) represents mutual information, and \(\) represents differential entropy. Intuitively, this acquisition function favors points where the model is uncertain in its prediction (the first term) due to diverging hypotheses about the predictions (the second term, which penalizes agreeing hypotheses), i.e., where it has high epistemic uncertainty about the label of the point.

### The promises and pitfalls of BatchBALD

A naive extension of BALD to the batch case would be to simply take the points with the top-\(B\) BALD scores. However, this will lead to redundant points; instead, Kirsch et al.  extend BALD to account for batches by considering the mutual information of the whole batch and the model's predictions:

\[a_{BatchBALD}(\{_{b}\}_{b=1}^{B};\, (f|)) =(\{y_{b}\}_{b=1}^{B};\,f\{ \}_{b=1}^{B},)\] \[=(\{y_{b}\}_{b=1}^{B}\{_{b}\}_{b=1}^{B},)\] \[-_{(f|)}[(\{y_{b}\}_{b=1}^{B}\{_{b}\}_{b=1}^{B},f, )].\]

Figure 1: The GP prediction along with BatchBALD and BBB-AL (ours) acquisition landscapes for \(B=2\), where \(x_{1}\) (the first point in the batch) is on the x-axis and \(x_{2}\) (the second point) is on the y-axis. For BatchBALD, we see that the optimal acquisition is \(x_{1}=x_{2}=1\), whereas for BBB-AL we obtain \(x_{1}=0\), \(x_{2}=1\).

While this appears to be a trivial change, it is not practical to implement in its naive form. Therefore, Kirsch et al.  propose a couple of modifications: 1) a greedy approximation algorithm to avoid a combinatorial explosion in the joint scoring of sets of points and 2) Monte Carlo sampling to reduce the computational and memory costs for larger batches.

Despite its reliance on the joint mutual information considering the entire batch, BatchBALD does not fully mitigate the issue of querying similar points. Consider the simple binary classification problem shown in Fig. 1. We show the GP prediction for toy data in Fig. 1a, as well as the acquisition landscape for \(B=2\) for BatchBALD in Fig. 1b. We observe that there is not a significant penalty for acquiring twice in the same location, leading BatchBALD to select \(x_{1}=x_{2}=1\). This effect is caused by the inability of BatchBALD to distinguish effectively between epistemic and aleatoric uncertainty, meaning that the increased aleatoric uncertainty in the vicinity of \(x=1\) adds enough uncertainty to reduce the impact that considering the joint entropy of the batch has. By contrast, our method, BBB-AL (big batch Bayesian active learning), shows the desirable behavior of choosing distinct points, as shown in Fig. 1c, and directly penalizing the acquisition of similar points, as seen by the sharp diagonal area with low score.

## 3 Active learning for classification through predictive probabilities

Motivated by these shortcomings of BatchBALD, in this work, instead of focusing on the mutual information between labels and functions (or parameters), we propose to instead reduce the uncertainty in the predictive probabilities of the model. Typically, in classification, the surrogate model will model latent functions for each class, \(f:^{C}\), where the class probabilities are given by a link function \(()\) which "squashes" the latent functions to \([0,1]^{C}\) (e.g., a softmax): \(()(p_{0}(), ,p_{C-1}())=(f_{0}( ),,f_{C-1}())\). As the underlying function \(f\) is stochastic in Bayesian modeling, we can also define a posterior density \((()|)\). This density informs where the model is certain or not about the class probabilities of a certain point. This uncertainty is a natural target for classification as it squashes the latent function's uncertainty when applicable, and it is purely epistemic, as it can be entirely reduced by observing more data. Hence we propose to acquire batches of points that maximize its joint differential entropy:

\[a(\{_{b}\}_{b=1}^{B};(f| ))=(\{(_{ b})\}_{b=1}^{B}).\]

In order to make this tractable, we make two simplifying assumptions: first, that the class probabilities are Gaussian-distributed, and second, that each output is independent. It is then straightforward to show that this leads to the acquisition function (up to constant terms)

\[a_{BBB}(\{_{b}\}_{b=1}^{B};(f| ))=_{c=0}^{C-1}(C_{p}^{c}),\]

where \(C_{p}^{c}\) is the \(B B\) covariance matrix of the probabilities for the \(c\)-th class, i.e.,

\[(C_{p}^{c})_{i,j}=(p_{c}(_{i}),p_{c}(_{j})),\]

where \((,)\) denotes covariance. In Appendix A, we show that in binary GP classification, our acquisition function has can be written in terms of Gaussian integrals that have efficient approximations. For more general models, we adopt a sampling-based approach, using Ledoit-Wolf shrinkage  to ensure a well-conditioned covariance matrix (see App. B).

When we are free to choose the batch of points in \(\) arbitrarily, we can simply jointly maximize the locations of \(\{_{b}\}_{b=1}^{B}\) using efficient multi-start optimizers provided in standard Bayesian optimization packages [e.g., 14, 1]. However, it is more common that we will have a pool of unlabeled points to choose from, \(_{}\). In this case, the problem can be reformulated as maximizing the probability assigned to the subset of points \(\{_{i}\}_{i=1}^{B}\) by the determinantal point process

\[(\{_{i}\}_{i=1}^{B}) (\{(f(_{i}))\}_ {i=1}^{B}).\]

While joint maximization of this probability is NP-hard, efficient greedy approaches exist which operate in \((|_{}| B^{2})\) time .

### Computational complexity

In its naive implementation, BatchBALD has a computational complexity of \((C^{B}|_{}|^{B} S)\), where \(S\) is the number of samples from the model posterior. This high cost in \(B\) is due to the need for BatchBALD to enumerate all possible combinations in \(\{0,1\}^{B}\). By using Monte Carlo sampling of these combinations to approximate the objective, Kirsch et al.  reduce this to \((BCM|_{}| S)\) complexity, where \(M\) is the number of Monte Carlo samples. For BBB-AL, we have a time complexity of \((C B^{2}|_{}|)\) for GP models. For more general models, this becomes \((S C|_{}|+C B^{2}| _{}|)\) computational cost. For small \(B\), we expect this to be better than BatchBALD, and below, we show that in practice we are significantly faster than BatchBALD regardless of batch size.

## 4 Experiments

We demonstrate our approach on CIFAR-10  using the small "ResNet-8" convolutional network from Ober and Aitchison . We use their fac\(\)gi variational posterior with a learned variance (per layer) Gaussian prior, which was shown to provide a good trade-off between performance and computational complexity, using 100 inducing points, horizontal flipping and random cropping as data augmentations, and KL tempering with a factor of 0.1. We initialize the model with 50 randomly-selected points, and acquire the greater of 50 acquisitions or 500 points for batch sizes 1, 10, and 50. We repeat each experiment 5 times, and plot the accuracy as a function of time in Fig. 2. Note that we only plot the time related to acquisition, and so we exclude the time related to model training.

We observe first that our proposed approach gives marginally better results on \(B=1\), even in terms of accuracy: this may be somewhat surprising, as our approach was motivated by the batch setting. However, we believe this validates our argument that BALD can conflate epistemic and aleatoric uncertainty. For larger batch sizes, our approach is clearly significantly faster than BatchBALD, while again obtaining better outright performance.

## 5 Related work

Beyond BatchBALD , perhaps the most conceptually similar work to ours in terms of Bayesian active learning is given in : they consider the entropy of the probabilities as we do. However, they introduce additional terms and hence propose a different objective, and most importantly they only consider single-point acquisitions. Pinsler et al.  provide the most relevant related work for batch active learning; however, their work assumes a true underlying posterior for a pool of data, which may not always be a realistic assumption. Our batch formulation takes inspiration from DPP-based approximations [11; 10; 12] from the related method of Bayesian optimization -- where data

Figure 2: Accuracy on CIFAR-10 for BBB-AL and BatchBALD acquisition functions versus time for \(B=1\), \(B=10\), and \(B=50\). The darker lines show the mean performance, whereas the lighter lines show individual runs.

is collected to maximally learn about specific properties of the underlying process, rather than to reduce global uncertainty.

## 6 Conclusion & future work

In this work, we have highlighted some of the limitations of BatchBALD. To address these, we have attempted to focus our method on capturing only the epistemic uncertainty of the model. In doing so, our method also avoids the combinatorial cost of naive BatchBALD. We have shown that our method can outperform BatchBALD in terms of both accuracy and time, allowing for bigger batches at faster runtimes.