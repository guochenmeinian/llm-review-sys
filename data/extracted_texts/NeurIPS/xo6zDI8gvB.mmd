# A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and Causal Relationship

A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and Causal Relationship

 Shiyu Hu\({}^{1,2}\)   Dailing Zhang\({}^{1,2}\)   Meiqi Wu\({}^{3}\)   Xiaokun Feng\({}^{1,2}\)

\({}^{1}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{2}\)Institute of Automation, Chinese Academy of Sciences

\({}^{3}\)School of Computer Science and Technology, University of Chinese Academy of Sciences

\({}^{4}\)School of Computer Science, Beijing University of Posts and Telecommunications

\({}^{5}\)Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences

{hushiyu2019, zhangdailing2023, fengxiaokun2022}@ia.ac.cn, wumeiqi18@mails.ucas.ac.cn, xuchenli@bupt.edu.cn, {xzhao,kqhuang}@ia.ac.cn

###### Abstract

Tracking an arbitrary moving target in a video sequence is the foundation for high-level tasks like video understanding. Although existing visual-based trackers have demonstrated good tracking capabilities in short video sequences, they always perform poorly in complex environments, as represented by the recently proposed global instance tracking task, which consists of longer videos with more complicated narrative content. Recently, several works have introduced natural language into object tracking, desiring to address the limitations of relying only on a single visual modality. However, these selected videos are still short sequences with uncomplicated spatio-temporal and causal relationships, and the provided semantic descriptions are too simple to characterize video content. To address these issues, we (1) first propose a new **multi-modal global instance** tracking benchmark named **MGIT**. It consists of 150 long video sequences with a total of 2.03 million frames, aiming to fully represent the complex spatio-temporal and causal relationships coupled in longer narrative content. (2) Each video sequence is annotated with three semantic grains (_i.e_., _action_, _activity_, and _story_) to model the progressive process of human cognition. We expect this **multi-granular annotation strategy** can provide a favorable environment for multi-modal object tracking research and long video understanding. (3) Besides, we execute comparative experiments on existing multi-modal object tracking benchmarks, which not only explore the impact of different annotation methods, but also validate that our annotation method is a feasible solution for coupling human understanding into semantic labels. (4) Additionally, we conduct detailed experimental analyses on MGIT, and hope the explored performance bottlenecks of existing algorithms can support further research in multi-modal object tracking. The proposed benchmark, experimental results, and toolkit will be released gradually on http://videocube.aitestunion.com/.

## 1 Introduction

Single object tracking (SOT) is an important computer vision task that aims to locate an arbitrary moving target in a video sequence, and can be regarded as the foundation for high-level tasks like video understanding. In the past decade, researchers have proposed numerous high-quality benchmarks [4; 5; 6; 7; 8; 9] for the visual-based SOT task, and a series of trackers [10; 11; 12; 13; 14;15, 16] have demonstrated good tracking capabilities in these environments, especially in short video sequences ranging from hundreds to thousands of frames. However, researchers noticed that most trackers always perform poorly in longer videos with more complicated narrative content. Besides, only relying on a single visual modality also limits the application scenarios. Thus, several works have begun to offer additional semantic annotations for SOT task.

As the first multi-modal SOT benchmark, OTB-Lang  provides a language description for the classic OTB  benchmark, hoping to provide a more natural human-machine interaction method. The long-term tracking benchmark LaSOT  also supplies a semantic annotation for each sequence, desiring to utilize linguistic features to improve the tracking performance. TNL2k  wants to achieve more flexible and accurate tracking ability with more explicit information (_e.g._, location information) in the semantic description. Although these multi-modal benchmarks have introduced semantic information into visual object tracking, they still face the following problems. (1) **Short sequences with uncomplicated spatio-temporal and causal relationships**: Existing works mainly focus on videos with hundreds to thousands of frames (the average sequence lengths of OTB-Lang, LaSOT, and TNL2k are 590 frames, 2,502 frames, and 622 frames), while shorter video sequences are always insufficient to reflect complex narrative content. (2) **Simple semantic descriptions**: The quality of semantic information is critical to multi-modal trackers' performance, while incorrect or ambiguous semantic information may misguide algorithms in tracking interference . However, the semantic labels in existing works mainly describe the state in the first frame, but lack the portrayal

Figure 1: Comparison of MGIT and other multi-modal object tracking benchmarks. (A-C) Examples of video content and semantic descriptions on OTB-Lang , LaSOT , and TNL2K . The green bounding box (BBox) indicates ground truth, while the red dashed BBox indicates other objects that satisfy the semantic description. These benchmarks have short sequences with simple narrative content. Besides, their semantic labels mainly describe the first frame, which may misguide algorithms. (D1-D3) An example of the multi-granular annotation strategy used by MGIT. Compared to existing benchmarks, MGIT contains longer sequences with more complex narratives, and the multi-granular annotation provides more prosperous and flexible information to portray long videos.

of the complete sequence. For example, the _brown liquor bottle_ description of Figure 1 (A) cannot distinguish the object from the interference (another brown liquor bottle). In Figure 1 (B), _white airplane landing on ground_ may also misdirect trackers to locate another airplane that has parked on the right ground. In Figure 1 (C), _the second arrow from left to right_ only satisfies to represent the object state at the beginning of the sequence; as the object moves, the position constraint contained in the semantic information will become misleading. Consequently, a better way to construct a multi-modal benchmark is not to provide a simple natural language description for short videos, but to design a scientific way to couple human understanding of long videos into semantic labels.

Therefore, we should first select suitable long videos with rich narrative relationships to compose a complex environment. VideoCube  is a high-quality benchmark recently released for the global instance tracking (GIT) task (_i.e_., search an arbitrary user-specified instance in a video without any assumptions about motion consistency), which can be regarded as expanding the definition of traditional SOT task (_i.e_., tracking a target in single-camera and single-scene) to success model the human visual tracking ability in a complex environment. Thus, we selected 150 representative long video sequences from VideoCube to form a new multi-modal benchmark named **MGIT**. The proposed new benchmark is consistent with the distribution of the original VideoCube in all dimensions (_e.g_., length, scene categories, object classes, motion modes, spatio-temporal consistency, and difficulty). Besides, we carefully check the content of each sequence to ensure that the selected data contain as many different types of video narratives as possible. Figure 1 (D1-D3) illustrates an example in MGIT. Compared with other works, sequences in MGIT include more complex content (_i.e_., the spatial-temporal variation and causal relationship are more complicated).

Besides, we design a **multi-granular annotation strategy** to provide scientific natural language information. On the one hand, existing research has indicated that complex narrative content can be perceived as several components and their relations, which is consistent with cognitive intuition . On the other hand, the process of human comprehension and cognitive development is progressive as well [21; 22]. Therefore, designing a hierarchical structure to represent the video content is a reasonable annotation method. As shown in Figure 1 (D1-D3), each sequence in MGIT is annotated with three semantic grains (_i.e_., _action_, _activity_, and _story_). We hope this method can provide a step-by-step "learning" environment for multi-modal trackers, in which they can first learn multi-modal information at a fine-grained level (_action_), then gradually develop to a more comprehensive level (_activity_), and finally understand the complex video narrative at a _story_ level like humans.

**Contributions.** (1) We propose a new multi-modal benchmark named MGIT. It consists of 150 long videos with a total of 2.03 million frames, and the average length of a single sequence is _5\(\) 22 times longer_ than existing multi-modal benchmarks. We hope this new benchmark fully represents the complex spatio-temporal and causal relationships coupled in longer narrative content (Section 3.1). (2) We design a multi-granular annotation strategy for providing scientific semantic information. Via this strategy, MGIT can provide a favorable environment for multi-modal object tracking research and long video understanding (Section 3.2). (3) We execute comparative experiments on other benchmarks. Experimental results explore the impact of different annotation methods, and validate that the proposed strategy is a feasible solution for coupling human understanding into semantic labels (Section 4.2). (4) We conduct detailed experimental analyses on MGIT. Results indicate that existing methods still have significant room for improvement in multi-modal tracking (Section 4.3). The proposed benchmark, experimental results, and toolkit will be released gradually on http://videocube.aitestunion.com/.

## 2 Related Work

**Benchmarks with Visual Information.** Standard SOT trackers are always initialized in the first frame by a target's bounding box (BBox), then continuously locating it in the video sequence. Since 2013, many benchmarks represented by OTB [4; 5] and VOT [6; 23] have been released, and these standardized datasets with scientific evaluation mechanisms promote the SOT research. With the development of deep learning techniques, these short-term and small-scale benchmarks have struggled to support data-driven trackers. Thus, several researchers have started to design larger-scale datasets like GOT-10k  and TrackingNet , while others have tried to collect data with longer videos and proposed long-term tracking benchmarks like OxUvA  and VOT-LT [25; 26]. Recently, some researchers have noticed that short-term and long-term tracking tasks include a continuous motion assumption in their definitions, resulting in the experimental environments being restricted to single-camera and single-scene. Therefore, they propose the global instance tracking task  with a new benchmark named VideoCube to track an arbitrary moving target in any type of video.

**Benchmarks with Visual and Semantic Information.** Unlike numerous visual benchmarks that have evolved over a decade, multi-modal benchmarks combining visual and semantic information have only received attention lately. OTB-Lang  is the first multi-modal SOT benchmark, which provides additional natural language description for sequences in OTB100  benchmark. However, the limited dataset scale has prevented the multi-modal SOT task from receiving widespread attention. After that, a large-scale and long-term tracking benchmark LaSOT [2; 17] is released with multi-modal annotations. In the same year, researchers propose the TNL2k  benchmark to achieve more flexible and accurate object tracking with natural language. These two benchmarks have provided a prosperity of data and have facilitated the generation of various multi-modal trackers.

As shown in Figure 2, existing works either focus on visual modality, or concentrate on multi-modality but lack longer videos with complex content. Besides, Figure 1 indicates a more scientific annotation strategy is also needed for providing high-quality semantic information. These limitations prompt us to propose MGIT, hoping to construct a more complex and flexible environment for research.

**Algorithms with Bounding Box.** Visual-based trackers always utilize the target's appearance and motion information to accomplish the tracking process, including the correlation filter (CF) based trackers [27; 28], Siamese neural network (SNN) based trackers [29; 30; 31; 32; 33; 34; 11; 35; 36; 10], the combination of CF and SNN [37; 38; 12; 13], and the transformer-based trackers [39; 14; 15; 16]. Before 2021, SNN-based trackers are the prevalent methods. Recently, transformer-based trackers have demonstrated exemplary performance and gradually become the dominant architecture.

**Algorithms with Bounding Box and Natural Language.** Tracking a moving target with visual and semantic information is a new task for SOT research; thus, representative works are mainly released in recent two years. AdaSwitcher  is released with the TNL2k benchmark, which proposes a switcher that utilizes natural language to alternate search mechanism (_i.e_., switch between the global search visual grounding module and the local visual tracking module). GTI  decomposes the visual language tracking task into three sub-tasks: tracking, grounding, and integration, and verifies the performance of each sub-module. SINT  proposes a semantic information fusion module that can be utilized across various SNN-based trackers. VLT  introduces a modality mixer named ModaMixer with asymmetric ConvNet search, which aims to demonstrate pure ConvNet models can achieve comparable results to state-of-the-art (SOTA) transformer-based algorithms. Besides, the proposed ModaMixer can further improve performance when directly applied to transformer-based trackers. JointNLT  unifies visual grounding and tracking as a coherent task (_i.e_., locating referred objects based on visual-language references). It employs the transformer-based architecture to model the relation between natural language and visual information.

## 3 Construction of MGIT

We propose a new multi-modal benchmark named **MGIT** and design a **multi-granular annotation strategy** for generating scientific semantic information. On the one hand, we have carefully selected 150 longer video sequences to form MGIT (please refer to Section A.2 in the Appendix for more details), hoping this complex environment can promote visual tracking and video understanding research. On the other hand, we hope this multi-granular annotation strategy can provide a step-by

Figure 2: Comparison between MGIT with other SOT benchmarks, including visual-based (_e.g_., OTB50 , OTB100 , GOT-10K, TrackingNet , OxUva , and VideoCube ) and multi-modal SOT benchmarks (_e.g_., OTB-Lang , LaSOT , and TNL2k ). The bubble diameter is in proportion to the total frames of a benchmark, and the vertical coordinate represents the average sequence length of each benchmark. Obviously, the proposed MGIT includes _longer videos_ with _multi-modal information_.

step "learning" environment for multi-modal trackers. Like humans can increase their comprehension by gradually increasing the learning difficulty, trackers can first learn at a fine-grained level (_action_), then to a more comprehensive level (_activity_), and finally accomplish a _story_ level understanding of long video sequences. A well-trained elite annotation team is selected to execute this task instead of crowdsourcing, and the annotation quality is ensured through various efforts. The detailed workflow has been outlined in Section A.3.2 of the Appendix.

### Data Collection

MGIT follows the recently proposed large-scale benchmark VideoCube  to conduct the data collection. VideoCube refers to the film narrative (_i.e_., a chain of causal relationship events occurring in space and time) and proposes the _6D principle_ for benchmark construction. In this work, we divide the 6D principle into two parts. Four dimensions (_i.e_., object class, spatial continuity, temporal continuity, and total frame), together with narrativity and topic, form the new sequence-level selection criterion. The other two dimensions (_i.e_., motion mode and scene category) will be refined during fine-grained semantic annotation. Therefore, we first regard the original VideoCube as the candidate samples, then add the additional examination of narrativity and topic, and finally select 150 video sequences to form the MGIT. Particularly, the proportions of the _train/val/test_ subsets are the same as the original VideoCube. Thus, sequences in each subset are _105/15/30_ in MGIT. Taking Figure 3 as an example, here we present several dimensions considered in the data collection process:

Figure 3: The representative data of MGIT. Here we illustrate six sequences with different aspects (_e.g_., narrativity, topics, virtuality, object classes, spatio-temporal continuity, and total frames).

**Topic and Narrativity.** We have divided the main video topics into six categories, which are _cartoons_, _movies_ & _TV shows_, _outdoor sports_, _regular sports_, _performances_, and _documentaries_. Among them, cartoons and movies & TV shows usually have a high narrativity (_i.e_., the video content contains a solid causal relationship, as shown in Figure 3 A and B). Outdoor sports and regular sports contain rich patterns of motion, and these motions can be linked chronologically into a _story_, but the narrativity is usually simple than in cartoons and movies (Figure 3 C and D). Compared to other topics, performances and documentaries usually record one action with low narrativity (Figure 3 E and F). However, these examples are classifications for most cases; it is worth noting that some performances (_e.g_., sketches with explicitly narrative content on stage) and some documentaries (_e.g_., documentaries with causal teaching steps) also belong to high narrativity.

**Spatial Continuity and Temporal Continuity.** Temporal continuity means the video content is developed according to the normal time flow (_i.e_., without fast-forwarding, fast-receding, or interpolation). Spatial continuity means the video content takes place in a fixed space.

**Virtuality.** Virtuality refers that this video is computer-generated, like cartoons or games. The same content in virtuality videos can be very different from videos sampled from the real world; thus, virtuality videos can present a new challenge for object tracking and long video understanding.

### Natural Language Annotation

In this work, we design a **multi-granular annotation strategy** to provide scientific natural language information. Video content is annotated by three grands (_i.e_., _action_, _activity_, and _story_, as shown in Figure 1 D1-D3). This hierarchical structure to represent the video content is motivated by existing works in computer vision  and human cognitive , such as a recent method  decouples the video content into multiple granularities for the visual question-answering task , aiming to help algorithms better understand video information like humans.

Figure 4: An example of _action_ annotation. We label the target, motion pattern, third-party object, and scene for each _action_. The target to be tracked is determined in the first frame and does not change during the entire video sequence. A change in any of the other three elements will end the current _action_ and proceed into the following _action_.

**Action.** As shown in Figure 4, we use the following critical narrative elements to portray an _action_: tracking target (who), motion (what) and third-party object (if present), location (where), and time interval (when). On the one hand, the above elements are necessary to portray narrative content. On the other hand, these elements are also essential grammatical components to form complete sentences. In particular, we use Stanford CoreNLP , a widely used natural language processing toolkit, to check the semantic annotations of other multi-modal datasets. We find that more than half of these semantic descriptions are only annotated at the phrase level, lacking the necessary grammatical structure (the detailed statistic result has been shown in Section A.3.1 of the Appendix). Thus, compared with existing works, MGIT can describe more detailed narrative content.

**Activity.** An _action_ describes what happens in a short period, while an _activity_ can be seen as a collection of _actions_ with clear causal relationships. A new _activity_ is usually accompanied by a scene switch or an explicit change of the third-party object. Compared with the former _action_, if an _action_ is preferred to be the beginning (_i.e_., reason) of a new event rather than an ending (_i.e_., result) of an old event, it can be regarded as a starting point of an _activity_. As shown in Figure 4 and Figure 1 (D1-D3), the first four actions describe a complete causality (the target approaches the third party, they fight, and cause the third party to be insensitive), while the 5th _action_ starts a new event (start examining the unconscious third-party and conduct a second fight when he wakes up). Therefore, the 4th and 5th _actions_ can be divided into two different _activities_.

**Story.**_Story_ is a high-level description. To avoid boring narrativity, we do not stack the existing _actions_ and _activities_, but use some words (_e.g_., first, then, after that, finally, _etc_.) to guide the content, making the temporal and causal more precise.

Based on the data collection process and the multi-granular annotation strategy, we construct MGIT with 2.03 million frames, and provide detailed annotation with 150 _stories_, 621 _activities_, and 982 _actions_. The semantic descriptions contain 77,652 words with 921 non-repetitive words, and more detailed analyses have been illustrated in Figure 5.

## 4 Experimental Results

### Datasets and Evaluation Methods

**Datasets.** We select OTB-Lang , TNL2k , LaSOT , and MGIT as experimental environments. Several variants of LaSOT are also concerned: (1) LaSOT\({}_{}\) is a complement of LaSOT  with 150 newly added video sequences. (2) Figure 1 indicates that several semantic descriptions in LaSOT are ambiguous. Thus, 22 ambiguous and 20 unambiguous sequences are selected to

Figure 5: Statistical analysis of key aspects in MGIT. (a-b) Distribution of topics and length of sequences. (c) Distribution of _activities_ and _actions_. The bubble diameter is in proportion to the length of a sequence, the vertical coordinate and the horizontal coordinate represent the total _activities_ and _actions_ of this sequence. (d-f) Distribution of temporal continuity, spatial continuity, and narrativity. (g) The word cloud of semantic descriptions.

form the LaSOTsub, aiming to better analyze tracking performance with different kinds of natural language information. (3) LaSOTNC is a subset of LaSOTsub, which is formed by the 20 unambiguous sequences, and we have carefully checked all the semantic and visual information in this subset.

**Evaluation Methods.** As shown in Figure 6, various mechanisms are designed to evaluate **tracking precision (PRE)** and **success rate (SR)**. We use \(F_{t}\) to represent the \(t\)-th frame. (1) Precision is calculated based on the center distance between the predicted BBox \(p_{t}\) and the ground truth BBox \(g_{t}\) (_i.e._, \(d_{t}=\|c_{p}-c_{g}\|_{2}\), where \(c_{p}\) and \(c_{g}\) represent center points). By calculating the proportion of frames where \(d_{t}_{d}\) and plotting curves at different thresholds, we can generate a _precision plot_. PRE is common to use \(_{d}=20\) as the criterion to rank trackers. (2) Furthermore, researchers  provide the **normalized precision (N-PRE)** to eliminate the effect of target size. When trackers have a predicted center outside the ground-truth, an additional penalty term, represented by \({d_{t}}^{p}\), is included to account for the shortest distance between the center point \(c_{p}\) and the edge of the ground-truth. The final result is then normalized to a range of 0 to 1 (_i.e._, \(N(d_{t})=^{{}^{}}}{\{({d_{t}}^{{}^{}}|i F_{t})\}}\), where \({d_{t}}^{{}^{}}={d_{t}}+{d_{t}}^{p}\)). Similarly, the _normalized precision plot_ is generated by plotting statistical outcomes derived from various \({_{d}}^{{}^{}}\) values. (3) Besides, frames with the intersection over union (IoU) \((p_{t},g_{t})=}{p_{t}}}{g_{t}}_{s}\) can be regarded as successful tracking, and the SR measures the percentage of successfully tracked frames. Drawing the results based on various \(_{s}\) is the _success plot_. For more details on the evaluation metrics, please refer to Section B.1 in the Appendix.

### Comparison with Other Multi-modal Benchmarks (Mechanism A)

We select several SOTA multi-modal trackers as baseline models and evaluate them on various benchmarks (as shown in Table 1). To fairly compare the tracking performance on MGIT and other datasets, we only allow trackers to use the semantic information of the first _action_ in this experiment. Results show that: (1) most trackers perform worst on MGIT, which means it is a more complex environment with more challenges. (2) By comparing the tracking results on LaSOTsub and LaSOTNC, we can find that most trackers perform worse on LaSOTsub, showing that ambiguous

    &  &  &  &  &  &  &  \\   & **PRE** & **S1.4** & **PRE** & **S1.4** & **PRE** & **S1.4** & **PRE** & **S1.5** & **PRE** & **S1.5** & **PRE** & **S1.5** & **PRE** & **S1.5** \\ 
**SNLT** & 0.838 & 0.665 & 0.0018 & 0.000 & 0.475 & 0.459 & 0.306 & 0.202 & 0.527 & 0.395 & 0.513 & 0.483 & 0.0004 & 0.0008 \\
**VLT\_SCAR** & **0.893** & 0.739 & 0.556 & 0.497 & 0.677 & 0.630 & 0.503 & 0.428 & 0.670 & 0.633 & 0.659 & 0.633 & 0.124 & 0.177 \\
**VLT\_TT** & **0.931** & **0.764** & 0.583 & 0.539 & 0.714 & 0.670 & 0.549 & 0.465 & 0.707 & 0.660 & 0.721 & 0.662 & 0.324 & 0.474 \\
**JointNLT** & 0.856 & 0.653 & 0.598 & 0.552 & 0.640 & 0.607 & 0.457 & 0.398 & 0.624 & 0.583 & 0.707 & 0.651 & 0.433 & 0.603 \\   

Table 1: Results on different multi-modal benchmarks (based on mechanism A in Figure 6).

Figure 6: Evaluation mechanisms of visual-based and multi-modal based trackers. (A) Traditional multi-modal tracking mechanism (_i.e._, only initialize a tracker with BBox and simple semantic information in the first frame). (B-D) Tracking with semantic information updates (_i.e._, initialize a tracker with BBox and semantic information in the first frame, then update the semantic information in each new interval). (E) Traditional one-pass evaluation (OPE) mechanism (_i.e._, only initialize a tracker with BBox in the first frame).

semantic information may introduce external interferences. Thus, we avoid this problem via the scientific annotation and check process for MGIT construction.

### Experimental Results on MGIT

**Tracking by NL\(\&\)BBox (Mechanism B-D).** As shown in Figure 6 (B-D), both visual information (BBox of the first frame) and semantic information (natural language description) can be used for multi-modal trackers. Specifically, different granularities have various lengths, while most trackers have a maximum limit of the input semantic information. JointNLT  sets 50 as a maximum limit and truncates the excess information. This truncation occurs for both _activity_ (C) and _story_ (D). Similarly, the VLT  series limits the semantic length but can avoid truncation by adjusting the parameters. Thus, we set the semantic length to 80 for the _activity_ and 200 for the _story_, with zero padding as necessary. From Table 2, we can draw the following conclusions: (1) SNLT, VLT_SCAR, and VLT_TT perform well when using longer semantic information like _activity_ and _story_. This indicates that the semantic information processing modules (BERT ) used in these trackers can effectively handle long text. Besides, their good performances in _activity_ indicate that as an intermediate granularity, _activity_ accomplishes a balance between the amount of information and the number of semantic description updates. (2) On the contrary, JointNLT performs well on _action_ rather than levels with longer descriptions, suggesting that truncated semantic information leads to poorer performance. Therefore, to obtain better multi-modal information processing capabilities, algorithms should first ensure that long texts can be processed rather than truncated directly.

By comparing results under mechanisms A and D, we can find that in this complex environment, well-designed trackers (_i.e_., trackers with suitable long input processing ability) can perform better via longer descriptions than only relaying a short description (SNLT: 0.036 \(\) 0.040, VLT_SCAR: 0.177 \(\) 0.184, VLT_TT: 0.474 \(\) 0.480 in SR). The above experiments indicate two key points: (1) Richer semantic information (mechanism D based on _story_) can improve the tracking performance than a simple sentence (mechanism A based on information for the first _action_), which can also verify the accuracy and necessity of the proposed multi-granularity semantic annotation strategy. (2) Only providing a simple description for multi-modal trackers is unreasonable for MGIT. Thus, initializing the tracking process with longer and more specific sentences, or updating the semantic information periodically throughout the sequence, has been found to be more effective in accurately locating targets within complex scenes.

**Tracking by BBox Only (Mechanism E).** We mainly evaluate SOTA visual-based trackers under mechanism E. As shown in Table 2, by comparing with other trackers, the transformers-based trackers have emerged as the predominant approach and achieved SOTA performance. Besides, it is worth noting that visual-based trackers usually outperform multi-modal trackers. Although we hope that more modal information can improve the tracking performance, the current multi-modal approaches cannot better align different modalities, resulting in the multi-modal information not being fully utilized. In contrast, visual-based methods have been well developed over the past decades and can better use visual features to accomplish good tracking performance. This result (_i.e_., current multi-modal trackers are worse than visual-based trackers) can also be found in other works, highlighting the significant room for improvement in multi-modal tracking. More detailed experimental results and analyses can be found in Section B.3 of the Appendix.

  
**Tracker** & **Architecture** & **Initialize** & **Mechanism** & **PRE** & **N-PRE** & **SL3** \\ 
**SiamCAR** & SNN & BBox & & 0.116 & 0.378 & 0.183 \\
**SiamRCNN** & SNN & BBox & & 0.512 & 0.707 & 0.591 \\
**PrDMP** & SNN+CF & BBox & & 0.296 & 0.602 & 0.453 \\
**KeepTrack** & SNN+CF & BBox & & 0.373 & 0.695 & 0.519 \\
**TransT** & Transformer & BBox & & 0.447 & 0.670 & 0.539 \\
**MixFormer** & Transformer & BBox & & 0.526 & 0.775 & 0.629 \\
**OSTrack** & Transformer & BBox & & 0.476 & 0.706 & 0.583 \\
**GRM** & Transformer & BBox & & 0.500 & 0.718 & 0.597 \\   &  &  & Action (B) & 0.004 & 0.226 & **0.036** \\  & & & Activity (C) & 0.004 & 0.234 & 0.038 \\  & & & Story (D) & 0.020 & 0.040 & 0.040 \\   &  &  & Action (B) & 0.116 & 0.354 & 0.167 \\  & & & Activity (C) & 0.124 & 0.382 & 0.180 \\  & & & Story (D) & 0.127 & 0.403 & 0.184 \\   &  &  & Action (B) & 0.318 & 0.002 & 0.468 \\  & & & Activity (C) & 0.325 & 0.627 & 0.485 \\  & & & Story (D) & 0.322 & 0.616 & 0.480 \\   &  &  & Action (B) & 0.445 & 0.786 & 0.610 \\  & & & Activity (C) & 0.441 & 0.780 & 0.605 \\   & & & Story (D) & 0.433 & 0.773 & 0.600 \\   

Table 2: Results of different trackers on MGIT.

### Visualization and Bad Case Analysis

We further analyze the bottlenecks of the multi-modal algorithms through the bad cases shown in Figure 7. The first two examples are selected from LaSOT , demonstrating that ambiguous semantic information may introduce noise, leading algorithms to wrongly focus on similar objects - this emphasizes the importance of accurate semantic annotations. The latter two examples are chosen from MGIT, demonstrating that the experimental environment constructed by MGIT presents complex spatio-temporal and causal relationships, posing challenges to multi-modal algorithms. Specifically, the complexity of MGIT results in significant differences between the target appearance and background environment in the initial frame and subsequent states. Besides, MGIT is selected from the recently released VideoCube  benchmark, which has a higher image resolution, posing challenges for trackers to relocate the target after failure. Additionally, using only the first action information (mechanism A) is applied in all other multi-modal SOT benchmarks. However, it is not applicable to visual object tracking in complex scenes like MGIT (Figure 7(C-D)). Therefore, the proposed multi-granularity annotation strategy offers a more reasonable solution. Multi-modal trackers who want to perform better on MGIT need a more well-designed semantic information processing module to accurately extract useful information described by semantic labels. Nevertheless, existing trackers have not made specialized designs for this aspect, which can be further improved.

## 5 Conclusions

**Summary.** Accuracy target tracking is the foundation for accomplishing high-level tasks like long video understanding, and introducing natural language into visual object tracking is a possible way to increase tracking ability. Different from existing multi-modal benchmarks that mainly consisted of short sequences with simple or even ambiguous descriptions, we (1) propose a new multi-modal benchmark named **MGIT** with 150 long video sequences, and (2) design a **multi-granular annotation strategy** for generating scientific semantic information. On the one hand, MGIT is a challenging and complex environment for visual tracking and video understanding research (_i.e._, trackers should process the spatio-temporal and causal relationships coupled with longer narrative content to accomplish better performance). On the other hand, the multi-granular annotation strategy models the human cognitive enhancement process, which may provide a step-by-step "learning" environment for generating human-like trackers. The experimental results demonstrate that MGIT is a more complex environment, and our proposed strategy is a feasible solution for coupling human understanding into semantic labels. Besides, existing trackers still have a large room for development, like improving the capability for processing long text and aligning multi-modal information. Conclusionally, we hope this work can help researchers to conduct further research in object tracking and video understanding.

**Limitations.** Some limitations here can be further enhanced in future work. First, we can expand MGIT with more types of videos to provide a more complicated environment for data-driven algorithms. Besides, we can design a better comprehensive evaluation system to measure visual tracking and video understanding ability. Finally, we can add more types of tasks based on the benchmark, and try to test algorithms for tasks like video caption and action recognition.

Figure 7: Bad cases of representative multi-modal trackers on LaSOT  and MGIT. (A-B) Ambiguous semantic annotations on LaSOT lead trackers to locate at similar objects. (C-D) The mechanism A used in existing multi-modal SOT benchmarks is unable to adapt to complex scenes like MGIT.