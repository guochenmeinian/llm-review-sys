# DOFEN: Deep Oblivious Forest ENsemble

Kuan-Yu Chen

Sinopac Holdings

lavamore@sinopac.com Ping-Han Chiang

Sinopac Holdings

u10000129@gmail.com Hsin-Rung Chou

Sinopac Holdings

sherry.chou@sinopac.com Chih-Sheng Chen

Sinopac Holdings

sheng77@sinopac.com Darby Tien-Hao Chang

Sinopac Holdings

National Cheng Kung University

darby@sinopac.com

###### Abstract

Deep Neural Networks (DNNs) have revolutionized artificial intelligence, achieving impressive results on diverse data types, including images, videos, and texts. However, DNNs still lag behind Gradient Boosting Decision Trees (GBDT) on tabular data, a format extensively utilized across various domains. This paper introduces DOFEN, which stands for **D**eep **O**blivious **F**orest **EN**semble. DOFEN is a novel DNN architecture inspired by oblivious decision trees and achieves on-off sparse selection of columns. DOFEN surpasses other DNNs on tabular data, achieving state-of-the-art performance on the well-recognized benchmark: Tabular Benchmark , which includes 73 total datasets spanning a wide array of domains. The code of DOFEN is available at: https://github.com/Sinopac-Digital-Technology-Division/DOFEN.

## 1 Introduction

Tree-based models, including RandomForest , Extra Trees , and Gradient Boosting Decision Tree (GBDT) frameworks such as XGBoost , LightGBM , and CatBoost , are widely recognized for their simplicity, efficiency, and remarkable performance with tabular data. This has inspired numerous studies investigating the integration of tree-based algorithms with deep neural networks (DNNs), leading to tree-inspired DNNs such as Deep Forest , NODE , and TabNet . In another line of tabular DNN research, novel DNN architectures such as SAINT , FT-Transformer , and Trompt  have been proposed. These novel architectures, which are essentially attention-based, demonstrate better performance compared with tree-inspired DNNs but require significantly more time and space. While these tabular DNNs have shown promising performance in specific contexts, recent surveys and benchmarks generally indicate that they do not surpass the performance of GBDTs on tabular data .

Hence, we begin by questioning what's missing in existing tabular DNNs and identify one key difference: in tree-based models, only a limited number of features are used in the construction of each tree. This concept of "sparse selection of columns" not only increases feature diversity but also helps mitigate overfitting in tree-based models . However, existing tabular DNNs are unable to achieve a sparse selection. For example, attention-based models  use the softmax operation to aggregate column information, resulting in a "dense selection" across columns. Some tree-inspired DNNs  have utilized methods like entmax and sparsemax  to enhance sparsity, but they can still only achieve near-sparse effects. Therefore, we opt to develop a new approach to achieve this characteristic.

For deep learning models, the biggest challenge is that generating a sparse matrix for on-off column selection is non-differentiable. In this study, we propose a novel two-step process to work around this issue: (1) enumerating as many sparse selections of columns as possible, and (2) weighting the importance of these sparse selections, making the weights differentiable and trainable by a DNN model. We name this new tree-inspired DNN **DOFEN**, an abbreviation for **D**eep **O**blivious **F**orest **EN**semble, and further demonstrate how DOFEN implements these two steps below:

1. **Condition Generation and rODT Construction.** In DOFEN, the step of enumerating sparse selections is further divided into two parts. The first part generates conditions, each involving exactly one column and corresponding to a decision rule of a tree node, as described in Section 3.2.1. The second part combines conditions using a shuffle-then-reshape procedure, detailed in Section 3.2.2. Each resultant combination of conditions can be seen as a differentiable counterpart to the Oblivious Decision Tree (ODT) , referred to as a relaxed ODT (rODT) in the context. Consequently, all the combinations collectively form a pool of rODTs.
2. **Two-level rODT Ensemble.** To ensure that the weighting of a limited number of rODTs can achieve good predictive performance, the previous step requires a sufficiently large pool. However, assembling all the rODTs in the pool into a single giant forest tends to cause overfitting, as shown in Appendix F.1 and Figure 9. Thus, DOFEN implements the step of importance weighting using a two-level ensemble procedure. The first level involves ensembling only a randomly selected subset of the rODT pool to form individual rODT forests, which is similar to applying dropout  to the rODT pool. The second level treats each rODT forest as a weak learner and aggregates them into a forest ensemble. This level is designed to enhance performance and stability, similar to standard ensemble learning. Both level of ensemble is detailed in Algorithm 1 of Section 3.2.3.

To evaluate DOFEN comprehensively and objectively, we have chosen a recent and well-recognized benchmark: the Tabular Benchmark . This benchmark addresses the issue of inconsistent dataset selection, which is prevalent in deep learning research on tabular data. It includes a variety of regression and classification datasets with standardized feature processing for consistency. Additionally, we have conducted detailed analyses focusing on the distinct features of DOFEN, thereby offering insights into its functionalities. In summary, our research makes two key contributions:

1. **Innovative Neural Network Architecture.** The DOFEN model is fundamentally inspired by ODTs and incorporates an innovative two-step process to achieve on-off sparse selection of columns. This unique approach enhances performance beyond that of current tree-inspired DNNs and offers differentiability compared to conventional tree-based models.
2. **State-of-the-Art Performance.** The DOFEN model exhibits outstanding performance, surpassing that of other neural network models and competing closely with GBDTs on the Tabular Benchmark. This achievement underscores its robustness and versatility across various tasks, as illustrated in Figure 1.

Figure 1: Evaluation results on the Tabular Benchmark. The model names are sorted by their performances at the end of the random search of hyperparameters. The result are averaged over various datasets included in each benchmark respectively, detailed number of datasets of each benchmark is provided in Appendix B.1

Related Work

In this section, we start by exploring ODT and detail our rationale for selecting ODT as the foundational element in our study. We then systematically categorize deep tabular neural networks into two distinct streams: tree-inspired DNN architectures and novel DNN architectures. Through comparing DOFEN with these established models, our goal is to highlight its unique contributions and position it within the broad landscape of deep tabular network research.

**Oblivious Decision Tree.** The ODT is a variant of the traditional decision tree algorithm , which makes a series of feature-based decisions along its root-to-leaf path to deliver a prediction. In the context, a feature-based decision rule, e.g. \(age>18\), is called a condition. The traditional decision tree algorithm  chooses different conditions on different nodes, while in ODT, all nodes at the same level apply the same condition, resulting in a more uniform decision-making process. This uniformity allows for streamlined and vectorized decision-making, thus enhancing computational efficiency, while it also comes at the cost of capacity . However, studies have shown that ensembles of ODTs can achieve remarkable performance with sufficient capacity [6; 8]. In this research, we integrate ODTs as the foundational element in the DOFEN model and capitalize on the strengths of ODTs while mitigating their limitations through ensemble strategies and deep learning techniques.

**Tree-inspired DNN Architectures.** Integrating decision tree (DT) algorithms with DNNs has become prominent for handling tabular data. Pioneering works like Deep Forest , NODE , TabNet , GradTree  and GRANDE  have each introduced unique methodologies.

Deep Forest adapts the random forest algorithm and incorporates multi-grained feature scanning to leverage the representation learning capabilities of DNNs. TabNet models the sequential decision-making process of traditional decision trees using a DNN, featuring a distinct encoder-decoder architecture that enables self-supervised learning. GradTree recognizes the importance of hard, axis-aligned splits for tabular data and uses a straight-through operator to handle the non-differentiable nature of decision trees, allowing for the end-to-end training of decision trees. NODE and GRANDE share a similar observation and high-level structure to DOFEN, in that they ensemble multiple tree-like deep learning base models. NODE uses ODT as a base predictor and employs a DenseNet-like multi-layer ensemble to boost performance. GRANDE, a successor to GradTree, uses DT as a base predictor and introduces advanced instance-wise weighting for ensembling each base model's prediction.

However, DOFEN distinguishes itself from NODE and GRANDE through its unique architectural design. First, DOFEN employs a different approach to transform tree-based models into neural networks. Unlike NODE and GRANDE, which explicitly learn the decision paths (i.e., selecting features and thresholds for each node) and the leaf node values of a tree, DOFEN randomly selects features to form rODTs and uses a neural network to measure how well a sample aligns with the decision rule. Additionally, the leaf node value of an rODT is replaced with an embedding vector for further ensembling. Second, DOFEN introduces a novel two-level ensemble process to enhance model performance and stability. Unlike NODE and GRANDE, which simply perform a weighted sum on base model predictions, DOFEN first constructs multiple rODT forests by randomly aggregating selected rODT embeddings and then applies bagging on the predictions of these rODT forests.

**Novel DNN Architectures.** Beyond merging decision tree algorithms with DNNs, significant progress has been made in developing novel architectures for tabular data. Notable among these are TabTransformer , FT-Transformer , SAINT , TabPFN , and Trompt . These models primarily leverage the transformer architecture , utilizing self-attention mechanisms to capture complex feature relationships.

TabTransformer applies transformer blocks specifically to numerical features, while FT-Transformer extends this approach to both numerical and categorical features. SAINT enhances the model further by applying self-attention both column-wise and sample-wise, increasing its capacity. TabPFN, a variant of the Prior Fitted Network (PFN) , is particularly effective with smaller datasets. Trompt introduces an innovative approach by incorporating prompt learning techniques from natural language processing , aiming to extract deeper insights from the tabular data's columnar structure.

These models have demonstrated impressive performance across various studies and benchmarks. As a result, we choose them as our baselines to offer a comprehensive evaluation for deep learning models on tabular data.

DOFEN: Deep Oblivious Forest Ensemble

In this section, we begin with discussion about how DOFEN relax an ODT to be differentiable in Section 3.1, and elaborate on the details of the overall architecture design in Section 3.2. In the following figures and equations, three sub-networks--composites of fundamental neural network layers such as linear layers, layer normalization, and dropout--are simplified into symbols \(_{1}\), \(_{2}\), and \(_{3}\) for readability. The detailed configurations of these sub-networks can be found in Appendix A.2.

### ODT Relaxation

An ODT operates on an input vector \(\), where \(^{N_{}}\) and \(N_{}\) is the number of columns in a tabular dataset, as described in Equation (1). Although these columns can be either numerical or categorical, we focus on real numbers in Equations (2) and (3) to simplify the notations.

\[=(x_{i} i=1,2,,N_{}),x_{i}\] (1)

Fundamentally, an ODT of depth \(d\) is a decision table consisting of \(d\) entries , as depicted in Equation (2). Here, \(I_{j}\) indicates the index of a selected column, and \(x_{I_{j}}\) denotes its column value at depth \(j\). The corresponding threshold is denoted by \(b_{j}\), and \(H\) denotes the Heaviside function. In practice, the choice of \(x_{I_{j}}\) is decided by a predefined criterion, e.g., entropy or Gini impurity. It is possible for a raw column to be selected multiple times at different depths, each with a varying threshold.

\[()=\{H(x_{I_{j}}-b_{j})\},\;}\{(x_{I_{j}},b_{j})\},\] \[I_{j}\;\{1,2,,N_{}\},\;b_{j},\;j =(1,2,,d)\] (2)

Equation (2) involves non-differentiable calculations, including the Heaviside function and the predefined criterion. Consequently, the key to integrating an ODT within a neural network model lies in making the following operations differentiable: selecting columns, deciding thresholds, and modeling \(H\).

To address these challenges, DOFEN proposes a method to relax an ODT, as shown in Equation (3). In DOFEN, the columns of an ODT at different depths are selected randomly. The thresholds and the Heaviside function for column \(I_{j}\) are replaced with a sub-network \(_{1I_{j}}\), which employs the sigmoid activation function to create soft conditions. To avoid confusion, we introduce a new term, relaxed ODT (rODT), in this context. This term distinguishes between the original ODT and the relaxed version proposed in this study, which can be integrated to neural networks.

\[()=\{_{1I_{j}}(x_{I_{j}})\},\; }\{x_{I_{j}}\},\] \[I_{j}\;\{1,2,,N_{}\},\;j=(1,2,,d)\] (3)

### DOFEN Model

#### 3.2.1 Condition Generation

This module transforms input vector \(\) into multiple soft conditions for subsequent modules. The raw input in tabular data comprises a combination of numerical and categorical columns. In this study, a soft condition is defined as a scalar indicating how well a column adheres to a decision rule.

This transformation process creates a matrix \(\), as shown in Equation (4), where \(N_{}\) is a hyper-parameter denoting the number of conditions we aim to generate for each column. Notably, each column \(x_{i}\) is processed by individual sub-network \(_{1i}\) in this context, where \(i\{1,,N_{}\}\). This design is derived from the original ODT, where each condition involves only a single column. The sub-network \(_{1}\) is an embedding layer for a categorical column or a linear layer for a numerical column. Further details of \(_{1}\) can be found in Appendix A.2. As depicted in Figure 1(a), three instances of \(_{1}\) generate four conditions for each column, resulting in a \(3 4\) matrix.

\[=m_{11}&&m_{1N_{}}\\ &&\\ m_{N_{}1}&&m_{N_{}N_{}} ^{N_{} N_{}},(m_{i1},,m_{iN_{}})=_{1i}(x_{i}),i=(1,2,,N_{})\] (4)

#### 3.2.2 Relaxed ODT Construction

This module constructs multiple rODTs. Unlike traditional ODT, which selects columns and their corresponding thresholds based on predefined criteria, DOFEN randomly selects \(d\) elements from the \(N_{} N_{}\) conditions in matrix \(\) without replacement to build an rODT with depth \(d\). In our implementation, \(\) is shuffled and reshaped into a matrix \(\) with dimensions \(N_{} d\), as shown in Equation (5). Here, we use \(\) to represent a bijective function that maps the index of each element in \(\) to a unique position in \(\) (i.e. permutation). The whole process is also illustrated in Figure 2b.

Specifically, \(N_{}=N_{}N_{}/d\). To guarantee that \(N_{}\) is an integer, we introduce an intermediate parameter, \(m\), which ensures that \(N_{}\) is always a multiple of \(d\) by formulating \(N_{}=md\). In practice, we use \(m\) to adjust \(N_{}\) instead of directly changing \(N_{}\).

On the other hand, note that each row in \(\) represents an rODT, which is crucial for subsequent operations. To ensure this consistency and the stability during training, the permutation is done only once during model construction and the configuration is then maintained throughout.

\[=o_{11}&&o_{1d}\\ &&\\ o_{N_{}1}&&o_{N_{}}^{N_{ } d},\]

\[\{o_{jk} j=,k=(n) d, n=u N_{}+v\}=\{m_{uv}\},\]

\[1 u N_{},\ 1 v N_{}\] (5)

Figure 2: (a) Condition Generation: For each column \(x_{i}\), \(N_{}\) conditions are generated through an individual sub-network \(_{1i}\). The aggregate of the conditions of all columns is denoted by the matrix \(\). (b) Relaxed ODT Construction: The condition matrix \(\) is shuffled (i.e. permutation with \(\)) and reshape into \(\), representing \(N_{}\) rODTs each with depth \(d\). (c) Forest Construction: To compute the weights \(w_{i}\), an individual sub-networks \(_{2i}\) is applied to each rODT. In addition, each \(w_{i}\) is paired with a learnable embedding vector \(_{i}\). The aggregate of all weights and their corresponding embedding vectors are denoted as \(\) and \(\), respectively.

#### 3.2.3 Two-level Relaxed ODT Ensemble

This module integrates rODTs to construct forests and then assembles multiple forests to conduct a final prediction.

**Forest Construction.** To construct an rODT forest using the generated rODTs, DOFEN introduces a sub-network and a standalone embedding vector for each rODT, denoted as \(_{2i}\) and \(_{i}\) respectively, where \(i\{1,,N_{}\}\). The role of \(_{2i}\) is to evaluate how well a sample aligns with the conditions of an rODT, producing a weight scalar \(w_{i}\), as shown in Equation (6) and Figure 2c.

\[=_{21}((o_{11},,o_{1d}))\\ \\ _{2N_{}}((o_{N_{}1},,o_{N_{}d}) )=(w_{1},,w_{N_{}})^{N_{}}\] (6)

The embedding vector \(_{i}\) represents the tree information and is independent of the samples. The embedding vectors are combined into a matrix \(\), as depicted in Equation (7), where \(N_{}\) represents the size of the hidden dimension. Importantly, each tree embedding vector is directly linked to the specific conditions of its corresponding rODT. It is crucial to keep this association consistent throughout each training session to effectively train the tree embedding vectors.

\[=_{1}\\ \\ _{N_{}}^{N_{} N_ {}},_{i}^{N_{}},i=(1,2,,N_{})\] (7)

To further construct an rODT forest, \(N_{}\) of paired weights and embeddings are sampled from \(\) and \(\). This process is graphically represented in Figure 3a and described in line 3 to 7 of the pseudo-code for the two-level ensemble (Algorithm 1). The weights are processed through a softmax function and the weighted sum of embeddings forms the embedding vector \(\) for an rODT forest. The magnitude of these softmaxed weights indicate the importance of the selected rODTs for making predictions. Noted that this process is repeated \(N_{}\) times to form \(N_{}\) instances of rODT forests.

**Forest Ensemble.** To make a prediction, DOFEN applies a shared sub-network \(_{3}\) to the embedding of each rODT forest to make individual predictions. The predictions are then averaged for a bagging

Figure 3: (a) Forest Construction: First, \(N_{}\) pairs of \((w_{i},_{i})\) are randomly sampled to form \(^{}\) and \(^{}\). Secondly, \(^{}\) is transformed through a softmax function, and is used for computing the weighted sum of \(^{}\) to form forest embedding \(\). (b) Forest Ensemble: a shared-weight sub-network \(_{3}\) is employed to make a prediction \(\) for each embedding. The final prediction is the average of all \(\) values, and the total loss is the sum of their individual losses.

ensemble. The process is detailed in line 1, 8, 10, and 12 in Algorithm 1 and is illustrated in Figure 2(b). Notice that the output \(_{i}\) is a scalar for regression tasks and a vector for classification tasks.

During training, DOFEN updates the model parameters by aggregating the loss from each prediction, as shown in line 9 in Algorithm 1. The loss function \(\) is cross-entropy for classification tasks and mean squared error for regression tasks.

Notably, the sampling of weight-embedding pairs allows resampling in each forward pass without disrupting the training. In fact, the two-level rODT ensemble essentially implements a form of bootstrap aggregating (i.e. bagging) of trees. Conventional tree-based models like random forest bootstrap samples to generate a variety of trees, which are then combined to form a forest. In DOFEN, the \(\) and \(\) represent a tree pool. From this pool, trees are sampled with replacement to create diverse tree sets, or forests, represented by \(^{}\) and \(^{}\). These forests are then integrated to make the final prediction. The design of this tree bagging method enables the construction of varied base models (in this case, forests rather than individual trees) within a single training session, which is particularly suited to deep learning contexts. Although the randomization may seem chaotic, experiments shows that this approach contributes to the model's stability and generalizability, which is discussed in detail in Section 4.3.1 and Appendix F.1.

## 4 Experiments

This section presents a comprehensive analysis of our experimental results, organized as follows: The Tabular Benchmark and the baseline models are first introduced in Section 4.1. In Section 4.2, we evaluate DOFEN on the medium-sized Tabular Benchmark, while leaving the results for large-sized benchmark in Appendix G.1. Section 4.3 delves into DOFEN to elucidate the underlying mechanics that drive its performance. Additionally, we discuss DOFEN's computational efficiency in Appendices C.1 to C.3, analyze DOFEN's scalability in Appendix D, and show DOFEN's interpretability in Appendix E.

### Tabular Benchmark Setup

**Datasets.** We strictly follow the protocols of the Tabular Benchmark as detailed in its official implementation1. This includes dataset splits, preprocessing methods, hyperparameter search guidelines, and evaluation metrics. For full details, please refer to the original paper . The Tabular Benchmark categorized datasets into classification and regression, with features being either exclusively numerical or a combination of numerical and categorical (heterogeneous). These datasets are further classified according to their sample size: medium-sized or large-sized. The dataset counts from Tabular Benchmark are provided in Appendix B.1, and the detailed datasets used in Tabular Benchmark are provided in Appendix B.3.

**Model Selection.** For model comparison, Tabular Benchmark includes four tree-based models: RandomForest, GradientBoostingTree , HGBT , and XGBoost; two generic DNN models: MLP and ResNet ; and two tabular DNN models: SAINT and FT-Transformer. To ensure a comprehensive comparison, we also included two additional tree-based models: LightGBM and CatBoost, and three tabular DNN models: NODE, Trompt, and GRANDE. LightGBM and CatBoost are selected due to their widespread use across various domains. NODE and GRANDE both share similar motivation and high-level structure with DOFEN, while Trompt represents the current state-of-the-art tabular DNNs when following the origin protocols of the Tabular Benchmark. The default hyperparameter configuration of DOFEN and hyperparameter search space of different models are presented in Appendices A.1 and H.2, and the list of some missing model baselines from Tabular Benchmark is provided in Appendix B.2.

### Performance Evaluation

We analyze the results of medium-sized benchmark on classification and regression tasks separately. The evaluation metrics adhere to the Tabular Benchmark protocols, which use accuracy for classification datasets and the R-squared score for regression datasets. We discuss the overall performance in this section and provide comprehensive results for each dataset in Appendix G.2.

**Classification.** In Figure 3(a), the models can be roughly categorized into three groups: (1) tree-based models and three tabular DNN models: DOFEN, Trompt and GRANDE, (2) three other tabular DNN models, and (3) the two generic DNN models. Prior to DOFEN, Trompt was the sole DNN model comparable to tree-based models. DOFEN not only matches but also surpasses the performance of most tree-based models, establishing a new benchmark for DNN models in tabular data. In Figure 3(b), DOFEN and Trompt are again the only two DNN models grouped with tree-based models, yet they are positioned at the bottom of this group.

**Regression.** In Figure 3(c), XGBoost stands out as a distinct category. Meanwhile, CatBoost and DOFEN represent a second level of performance. Notably, XGBoost and DOFEN demonstrate a significant improvement during the hyperparameter optimization, whereas CatBoost maintains strong performance consistently. In Figure 3(d), XGBoost and CatBoost continue to hold the top two positions. DOFEN, ending up in sixth place, is overtaken by GradientBoostingTree as well as HGBT, and is comparable with FT-Transformer towards the end of the hyperparameter search process.

The analysis of Figure 4 allows us to draw several conclusions. When compared to DNN models, DOFEN consistently either ranks first or shares the top positions. Additionally, DOFEN exhibits strong competitiveness against tree-based models. In datasets with numerical features, it consistently places within the top three. However, in the context of heterogeneous features, DOFEN's performance is moderate, typically falling in the middle or lower tiers in comparison with tree-based models. This challenge in managing heterogeneous features is a prevalent issue among all DNN models, highlighting an area for potential improvement in future tabular DNN models.

Figure 4: Results on medium-sized classification and regression datasets.

### Additional Analysis

This section is dedicated to a deeper exploration of the DOFEN model. Randomness plays an important role in DOFEN, as both the condition selection of an rODT and rODT selection of a forest involve random processes. A straightforward concern is the stability of DOFEN, which is examined in Section 4.3.1. Moreover, given that the conditions are randomly selected, we investigate whether this randomness leads to redundant trees in Section 4.3.2. In addition to randomness, another distinct feature of DOFEN is the introduction of a higher-level ensemble that combines multiple forests, instead of merely assembling trees into a forest. Appendix F.1 discusses the impact of removing this higher-level ensemble on DOFEN.

All experiments in this section are conducted using the default hyperparameters and medium-sized datasets from the Tabular Benchmark. For evaluation metrics, accuracy is used for classification datasets, while the R-squared score for regression datasets. Except for the cases evaluated on individual datasets, the results represent the averaged metrics across the corresponding datasets.

#### 4.3.1 Model Stability

DOFEN incorporates randomness at two steps: firstly, in the selection of conditions as shown in Equation (5) for rODT construction, and secondly, in the sampling of rODTs as shown in line 3 of Algorithm 1 for a two-level rODT ensemble. This section explores how randomness affects the stability of DOFEN.

We start by analyzing the variation in performance of four datasets, where DOFEN ranks first, as shown in Table 1. The standard deviations are even negligible when \(N_{}=1\) (about \(0.1\%\) to \(1\%\) to mean), except for the delays-zurich dataset. Moreover, with increased \(N_{}\), the standard deviations become even smaller (about \(0.01\%\) to \(0.1\%\) to mean). These results suggest that the stability of DOFEN is not an issue in most cases (\(N_{forest}>10\)), and using the default setting of DOFEN (\(N_{forest}=100\)) ensures both adequate performance and stability for most datasets. Furthermore, the performance improves as the \(N_{}\) increases, indicating that the tree bagging of DOFEN not only mitigates instability but also enhances the model's generalizability.

In addition to analyzing the intrinsic instability, we also replace the steps involving randomness with deterministic alternatives to assess the impact of the randomness on DOFEN from a different perspective. For the selection of conditions, we utilize CatBoost to choose columns based on a predefined criterion. The detailed results, presented in Appendix F.3, reveal that the predefined criterion perform only slightly better than the shuffle-then-reshape process. Considering the differentiability and the potential for end-to-end training, random selection of conditions remains a viable and promising option. For sampling rODTs, we implement a sliding window technique to lock in the selected trees for each forest. The results are detailed in Appendix F.4, which suggests that our straightforward approach is comparable to a more sophisticated approach.

    & \(N_{}\) & **1** & **10** & **20** & **50** & **100 (default)** & **400** \\  jannis & \(\) (\(\)) & 0.7382 & 0.7747 & 0.7782 & 0.7800 & 0.7808 & 0.7814 \\ (numerical classification) & \(\) (\(\)) & 0.0060 & 0.0019 & 0.0015 & 0.0006 & 0.0007 & 0.0004 \\  road-safety & \(\) (\(\)) & 0.7517 & 0.7712 & 0.7720 & 0.7728 & 0.7732 & 0.7732 \\ (heterogeneous classification) & \(\) (\(\)) & 0.0118 & 0.0010 & 0.0007 & 0.0004 & 0.0005 & 0.0003 \\  delays-zurich & \(\) (\(\)) & 0.0054 & 0.0248 & 0.0258 & 0.0265 & 0.0268 & 0.0270 \\ (numerical regression) & \(\) (\(\)) & 0.0033 & 0.0009 & 0.0005 & 0.0003 & 0.0003 & 0.0002 \\  abalone & \(\) (\(\)) & 0.5469 & 0.5810 & 0.5846 & 0.5862 & 0.5868 & 0.5870 \\ (heterogeneous regression) & \(\) (\(\)) & 0.0181 & 0.0038 & 0.0026 & 0.0017 & 0.0010 & 0.0004 \\   

Table 1: Mean (\(\)) and standard deviation (\(\)) of DOFEN’s performance with 15 random seeds on 4 datasets from different tasks.

#### 4.3.2 Weights of Individual Relaxed ODT

In DOFEN, an rODT is assigned a weight to predict a sample, as shown in Equation (6). In this section, we analyze a binary classification dataset (covertype) to observe the variation in the weights assigned to individual rODTs, as shown in Figure 5.

Figure 4(a) shows that, for most rODTs ranked in the top 25 according to their standard deviations of weights, there is a significant difference between the average weights of true positives and those of true negatives. Conversely, Figure 4(b) shows an opposite trend for rODTs with the smallest standard deviations of weights. These trends are also observed in another dataset, as shown in Appendix F.5. These observations imply that rODTs with larger standard deviations of weights is more crucial role in classifying samples.

In addition, we come up with an idea to examine the performance change after pruning weights with small standard deviations and their corresponding embeddings, since they are not sensitive to samples with different label. The results are provided in Appendix F.6 and suggest that the variation serves as a reliable indicator of the importance of rODTs. Moreover, pruning the less important rODTs not only enhances the model's efficiency but also its performance.

## 5 Limitation and Conclusion

**Limitation.** Although DOFEN shows promising results, it still contains two weaknesses. First, the inference time of DOFEN is relatively long compared to other DNN models, as shown in Appendix C.1. However, Appendix C.1 also shows that DOFEN possesses the fewest floating point operations (FLOPs). This inconsistency between inference time and FLOPs is mainly caused by the group convolution operation for calculating weights for each rODT (Appendix C.2), which can be improved in the future implementation of DOFEN. Second, the randomization steps involved in DOFEN result in a slower convergence speed, meaning that DOFEN requires more training steps to reach optimal performance. This is reflected in the relatively larger number of training epochs needed for DOFEN. Therefore, the workaround strategy of differentiable sparse selection proposed in this study is merely a starting point, demonstrating its potential. Finding more efficient strategies will be the future work.

**Conclusion.** In this work, we proposed DOFEN, a novel tree-inspired DNN for tabular data that achieves on-off sparse selections of columns. DOFEN first constructs sufficiently large number of rODTs and randomly ensembles these rODTs into multiple rODT forests to make prediction. DOFEN was evaluated on the Tabular Benchmark, achieving state-of-the-art results compared to DNN-based models and proving competitive with tree-based ones. Furthermore, we showed that the randomization steps involved in DOFEN do not compromise stability but do yield redundant rODTs. Nevertheless, redundant rODTs can be efficiently removed through our pruning method. In summary, based on DOFEN's outstanding performance, it has the potential to serve as the backbone model for tabular data across various scenarios, including self- and semi-supervised learning, as well as multi-modal training.

Figure 5: In the covertype dataset, Figure 4(a) shows that the average weights of true positives differ significantly from those of true negatives. Conversely, Figure 4(b) reveals a contrasting result for rODTs with small weight variation.