# Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models

Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models

 Beier Zhu\({}^{1}\) Kaihua Tang\({}^{1}\) Qianru Sun\({}^{2}\) Hanwang Zhang\({}^{1}\)

\({}^{1}\)Nanyang Technological University \({}^{2}\)Singapore Management University

beier002@e.ntu.edu.sg, hanwangzhang@ntu.edu.sg

###### Abstract

Foundation models like CLIP allow zero-shot transfer on various tasks without additional training data. Yet, the zero-shot performance is less competitive than a fully supervised one. Thus, to enhance the performance, fine-tuning and ensembling are also commonly adopted to better fit the downstream tasks. However, we argue that such prior work has overlooked the inherent biases in foundation models. Due to the highly imbalanced Web-scale training set, these foundation models are inevitably skewed toward frequent semantics, and thus the subsequent fine-tuning or ensembling is still biased. In this study, we systematically examine the biases in foundation models and demonstrate the efficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that bias estimation in foundation models is challenging, as most pre-train data cannot be explicitly accessed like in traditional long-tailed classification tasks. To this end, GLA has an optimization-based bias estimation approach for debiasing foundation models. As our work resolves a fundamental flaw in the pre-training, the proposed GLA demonstrates significant improvements across a diverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, a large average improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on long-tailed classification. Codes are in https://github.com/BeierZhu/GLA.

## 1 Introduction

Thanks to the Web-scale data and self-supervised strategies, foundation models like CLIP  empower zero-shot transfer to a wide variety of domains . However, the zero-shot performance is still weak on several domain-specific tasks such as differentiating models of cars, species of flowers, and variants of aircraft . Therefore, it is a common practice to improve the downstream performance via supervised fine-tuning on labeled data, _e.g._, linear probing, prompt tuning , and end-to-end fine-tuning.

However, fine-tuned models are easily biased: they are adept in exploiting spurious correlations that only hold on the downstream distribution . To improve the robustness, several studies  propose to combine fine-tuned models with zero-shot models. For example, WiSE-FT  ensembles the fine-tuned and zero-shot models in weight space and ProGrad  uses zero-shot predictions to regularize the fine-tuning gradient. The underlying assumption lies in that the zero-shot models are robust to distribution shifts , and their predictions are complementary to those of fine-tuned models .

Despite these methods exhibiting performance gains on both in-distribution and out-of-distribution evaluations, they all overlook the inherent bias originating from the foundation models. Specifically, the Web-scale data for pre-training foundation models exhibit a highly skewed distribution due to Zipf's law of nature . The resulting foundation models develop a biased decision boundary that leads to a poor zero-shot performance on rare classes. As evidenced in Figure 1(a) and (b), the purple line encounters a dramatic drop, and the zero-shot performance of tail classes is significantly lower than that of head classes (\(57.2\%\)_vs._\(78.0\%\)). Existing ensemble methods like WiSE-FT  overlook the label bias, resulting in an improvement in top-1 (\(+0.5\%\)) and head accuracy (\(+1.7\%\)) while a noticeable degradation on the tail performances (\(-0.9\%\)) in Figure 1(b). Another evidence is that the orange line (WiSE-FT) is below the blue line (fine-tuned models) for rare classes in Figure 1(a).

We propose Generalized Logit Adjustment (GLA), a simple post-hoc method consisting of two steps: **1)** removing the label bias of zero-shot model via estimating the label distribution in the pre-training dataset; **2)** ensembling the fine-tuned and debiased zero-shot models. As illustrated in Figure 1 (b), our GLA achieves consistent improvement across all three subgroups, particularly showing a significant gain on tail classes (\(+1.5\%\)). Despite its simplicity, our GLA has a firm statistical grounding: it is the Bayes optimal classifier given the fine-tuned and zero-shot models, thus consistent for minimizing the error on a class-balanced target distribution (Section 4.2). It is worth noting that removing the bias of foundation models is challenging since the label distribution is often inaccessible due to privacy or copyright concerns. In this work, we only use the downstream labeled data and the zero-shot model to estimate the foundation label bias. Specifically, we formulate the problem by adjusting the margin of the zero-shot models such that the lowest error is achieved on the downstream dataset. This grounding translates into strong empirical performance on real-world datasets, covering few-shot, many-shot, and long-tail learning (Section 5).

The contributions and novelties of this work are summarized as follows:

* We point out the overlooked label bias in foundation models, which originates from the skewness of pre-training distribution that affects the performance of downstream tasks.
* We formalize the estimation of the label bias as a constrained optimization problem (Section 3.2) with theoretical justification (Section 4.3). The entire process does not require access to the pre-training dataset, making it practical for fine-tuning scenarios. We present Generalized Logit Adjustment (GLA) method, which ensembles the debiased zero-shot and fine-tuned models, and demonstrate its superiority over conventional fine-tuning and ensembling by proving it's a Bayes optimal classifier (Section 4.2).
* We build a comprehensive benchmark for evaluation, which considers three real-world settings and three fine-tuning paradigms. The settings are: 1) many-shot learning with abundant data 2) few-shot learning; and 3) long-tail classification, representing a more challenging scenario that combines many-shot and few-shot data (Section 5). The three fine-tuning paradigms include: 1) end-to-end fine-tuning; 2) linear probing; and 3) prompt tuning (Section 3.1).
* We demonstrate the efficacy of our proposed method GLA by conducting extensive experiments across various settings and fine-tuning paradigms. We observe 1 to 1.5 pp accuracy gains on ImageNet, large averaged improvement (1.4 to 4.6 pp) on 11 few-shot datasets and 2.4 pp averaged accuracy gains on long-tail datasets. (Section 5).

Figure 1: (a) Per class accuracy of CLIP-ViT/B16 on ImageNet. Class index are sorted using the estimated pre-training label prior. Curves are smoothed for better visualization. (b) Beak-down performance of different models on ImageNet. We equally divide the ImageNet classes into three subgroups, according to the class index. Existing ensemble methods like WiSE-FT  exhibits a clear performance loss on tail classes, while our GLA stands out for all three subgroups.

Related Work

**Image-text foundation models.** Foundation models pre-trained by contrastive objectives have set impressive milestones for image and text representation learning, with CLIP , ALIGN , CoCa  and Flamingo  being the exemplars. Such models exhibit impressive prompt-based zero-shot performance on various image recognition downstream tasks. Our method aims to reduce foundation model biases to boost performance in downstream tasks. While  also addresses word frequency bias, we differ in two key areas: Firstly, we debias zero-shot models using fixed prompts, whereas  refines the prompting process. Secondly, our GLA doesn't require access to a subset of the pre-training data.

**Ensembles.** Ensemble methods aim to boost performances by combining multiple networks, which can be either implemented by aggregating model outputs [12; 4; 28; 14; 27; 51], weight-space ensembling [48; 22], or ensemble distillation [19; 29]. For the adaptation of foundation models, several work propose to ensemble the fine-tuned and zero-shot models for better performance: Wortsman et al.  ensembles them in weight space; ProGrad and ProReg [55; 56] propose to fuse them via knowledge distillation. Our GLA is orthogonal to these approaches, as it concentrates on mitigating the biases in foundation models that are detrimental to ensemble models.

**Logit adjustment.** Logit adjustment [35; 45; 24; 49; 20] is a post-hoc technique to adjust the biased output of classification networks. Kang _et al_ proposes an element-wise scaling adjustment for the classifier weight. Tang _et al_ removes the projection of features on a global biased direction. Menon  derives the theoretically optimal adjustment from the training distribution. Unlike the those approaches which rely on a transparent training data or class distribution, our GLA can eliminate the class bias without accessing to the pre-training statistics.

## 3 Methods

### Setup

**Task.** Consider a classification problem with instances \(\) and labels \(y=[K]=\{1,...,K\}\). We have a zero-shot model \(f_{}\) (given below), a downstream dataset \(_{s}=\{_{i},y_{i}\}_{i=1}^{N}\) drawn from source distribution \(P_{s}\) and a fine-tuned model \(f_{}\) (given below) trained on the dataset \(_{s}\). Give a model \(f:^{K}\) that outputs prediction score, we define the risk of \(f\) on the target distribution \(P_{t}\) as the mis-classification rate: \(_{t}(f)=_{_{i},y P_{t}}[y] {argmax}_{i}f()_{i}|\). Our goal is to learn a model \(f_{}\) that best leverages \(f_{}\) and \(f_{}\) that minimizes the risk \(_{t}\).

**Zero-shot models.** We primarily explore CLIP  for zero-shot models. CLIP consists of a visual encoder \(_{}()\) and a text encoder \(_{}()\), producing \(l_{2}\)-normalized features from an image \(\) and a text \(\) respectively. Zero-shot model \(f_{}\) for \(K\) classes is enabled by matching image features \(=_{}()\) with classification weights \(_{k}=_{}(_{k})\), where \(_{k}\) is obtained by extending the class name \(\{c_{k}\}\) to a pre-defined prompt, _e.g._, "a photo of a \(\{c_{k}\}\).". Additional details are provided in Appendix B.2. The probability of \(\) being classified as \(y\) is defined as:

\[P(y|)=(f_{}())_ {y}=^{T}_{y})}{_{k=1}^{K}(^{T }_{k})}.\] (1)

**Fine-tuned models.** Standard fine-tuning initializes the model \(f_{}\) with the pre-trained parameters and then solve \(f_{}=_{f}_{s}(f)\) to minimize the risk on downstream dataset. We consider three common variants of fine-tuning: (1) end-to-end, where all parameters of \(_{}\) and \(_{k}\) are updated; (2) linear probing, where only \(_{k}\) is modified while \(_{}\) is fixed; (3) prompt tuning, where the text input \(_{k}\) is learned, while keeping \(_{}\) and \(_{}\) freezed. See Appendix B.3 for details on fine-tuning methods.

**Notation.** Let \(P_{p}(y)\), \(P_{s}(y)\) and \(P_{t}(y)\) be the marginal probability of class \(y[K]\) for pre-training, source (training) and target (test) distribution, respectively. Let \(_{p}\) and \(_{s}\) denote the log probabilities of class for pre-training and training distribution, _i.e._, \(_{p}(y)= P_{p}(y)\) and \(_{s}(y)= P_{s}(y)\).

### Generalized Logit Adjustment Framework

Fine-tuned models often yield significant gains compared to zero-shot models, and ensembling them can further improve performance. This leads to a natural question: How should we best leverage the zero-shot and fine-tuned models for the prediction tasks? We attempt to answer by proposing generalized logit adjustment in Definition 1.

**Definition 1**.: _(GLA) The Generalized Logit Adjustment (GLA) model \(f_{}\) is defined as follows:_

\[f_{}()=f_{}()+f_{}( )-_{s}-_{p}.\] (2)

In Section 4.2, we prove that given the zero-shot and fine-tuned models, our GLA model is the _Bayes_ optimal classifier and no other combination of the two models can outperform it. Here remains one important question: how could we obtain \(_{p}\) as we have no access to the pre-training statistics? We provide the estimation process of \(_{p}\) in Eq (4) and postpone the justification in Section 4.3. The entire GLA algorithm consists of two steps which is given as follows:

**Step 1: Estimation of \(_{p}\).** Let \(\) be an arbitrary _probability simplex_ over \(K\) classes. Given the validation data from \(P_{t}\) or the balanced training data, we can estimate the \(_{p}=^{*}\) as the constrained optimization problem (proof in Section 4.3):

\[^{*}=*{argmin}_{}_{ t}(f_{}-)\] \[\ _{i} 0,\ i[K],\] \[_{i[K]}_{i}=1.\] (3)

We constrain the sum of \(\) to be 1 and ensure that each element is non-negative, guaranteeing that it forms a valid probability distribution. We solve the following Lagrangian problem to find optimal \(^{*}\):

\[_{}_{_{i} 0,v}_{t}(f_{}- )-_{i}_{i}_{i}+(1-_{i[K]} _{i})\] (4)

**Step 2: GLA ensembling.** Given the estimated \(_{p}\) and the known downstream training \(_{s}\), we ensemble the zero-shot model \(f_{}\) and the fine-tuned model \(f_{}\) to get our GLA model \(f_{}\) via Eq. (2). We can regard \(f_{}-_{p}\) and \(f_{}-_{s}\) as the debiased zero-shot model (Figure 2(c)) and debiased fine-tuned models, respectively. Our GLA is actually ensembling two debiased models. Note that, different from [55; 48], we _do not_ require a hyper-parameter to adjust the contribution of the two models, the optimal solution is to combine them equally (see Section 4.2 for justification).

## 4 Theoretical Analysis

In this section, we explain why our GLA model best ensembles the zero-shot and fine-tuned models (Section 4.2) and justify the estimation process of the pre-training label distribution \(_{p}\) (Section 4.3). We start with some preliminaries on _Bayes_ optimal classifier.

### Preliminaries

Suppose we have a pair \((X,Y) P\) takes values in \(\), where \(Y\) is the class label of input \(X\).

**Definition 2**.: _The 0-1 error (risk) of a classifier \(:\) on distribution \(P\) is given by:_

\[()=P(Y(X))\] (5)

Figure 2: Illustration of debiasing process on ImageNet validation set. (a) The original distribution of zero-shot outputs; (b) the estimated pre-train distribution \(\) based on our algorithm; (c) the distribution of debiased zero-shot outputs using estimated \(\).

However, the 0-1 error is non-smooth, one typically minimizes a surrogate loss \(\), _e.g._, cross-entropy: \((f(),y)=[_{i[K]}(f()_{i}-f()_{ y})]\), where \(()=*{argmax}_{i}f()_{i}\). It is known that the cross-entropy loss is _Bayes consistent_, _i.e._, a nearly optimal minimizer of the cross-entropy loss (\(_{,y P}[(f(),y)]\) is also a nearly optimal optimizer of the mis-classification error (\(_{,y P}[y()]\)).

**Definition 3**.: _The Bayes optimal classifier \(y^{*}\) for \(P\) given input \(\) is defined as:_

\[y^{*}()=*{argmax}_{y}P(y|)\] (6)

It is called _Bayes_ optimal classifier because on the average _no_ other classifier using the same hypothesis and prior knowledge can outperform it.

**Lemma 1**.: _The Bayes optimal classifier \(y^{*}\) for \(P\) has lower risk than all classifiers \(:\)._

\[(y^{*})()\] (7)

### Generalized Logit Adjustment Leads to Better Ensembling

**Zero-shot and fine-tuned models are complementary.** We revisit an empirical phenomena observed in  Section 5.1: After exploring a series of measures of diversity, covering predictions and features, they find that zero-shot and fine-tuned models have diverse predictions, despite sharing the same backbone. As the two data distribution \(P_{s}\) and \(P_{p}\) is known to be different, the resulting models leverage different cues to predict: fine-tuned models risk exploiting _spurious correlations and in-domain patterns_ which only hold for downstream dataset ; On the other hand, zero-shot CLIP models capture _stable correlations_ across diverse domains and exhibit much higher robustness . For instance, zero-shot models rely on robust features for decisions that can achieve high performance on sketch and adversarial samples, while the fine-tuned models that trained on real images typically fail on these samples, as they rely on spurious correlations that only hold on real images. We formulate the phenomena in the following assumption.

**Assumption 1**.: _Zero-shot and fine-tuned models have diverse predictions:_

\[(f_{}() f_{}())|y.\] (8)

We derive the conditional probability \(P_{t}(y|f_{}(),f_{}())\) w.r.t. the outputs of \(f_{}()\) and \(f_{}()\):

**Lemma 2**.: _For a balanced target distribution1, where \(P_{t}(y)=1/K\) for all \(y[K]\), we have:_

\[P_{t}(y|f_{}(),f_{}())= {softmax}(}()+f_{}() -_{s}-_{p}}_{}()}_{f_{}()} (y)\] (9)

Intuitively, since the zero-shot and fine-tuned models provide diverse predictions, conditioned on the two predictions is equivalent to adding the logits in log space. Additionally, as the target distribution is class-balanced, we need to remove the class bias of two models by subtracting \(_{s}\) and \(_{p}\). The formal proof is given in Appendix A.1. Note that the RHS of Eq. (9) is exactly the softmax output of our GLA model by Definition 1, which exhibits the following property:

**Proposition 1**.: _Let \(g:^{K}^{K}^{K}\) be an arbitrary function that ensemble the outputs of \(f_{}\) and \(f_{}\). Our GLA classifier \(f_{}\) has lower risk than any function \(f_{g}()=g(f_{}(),f_{}())\), i.e._

\[_{t}(f_{})_{t}(f_{g}).\] (10)

Proof.: From Lemma 2 and Definition 1, we have:

\[*{argmax}_{y}f_{}()_{y}= *{argmax}_{y}(f_{}( )+f_{}()-_{s}-_{p})_{y}=*{ argmax}_{y}P_{t}(y|f_{}(),f_{}( )),\] (11)

which means \(f_{}\) is the Bayes optimal classifier (see Definition 3) given \(f_{}()\) and \(f_{}()\). According to Lemma 1, any other classifier \(g(f_{}(),f_{}())\) must have higher risk, _i.e._, \(_{t}(f_{})_{t}(f_{g})\).

Proposition 1 demonstrates that our \(f_{}\) model is the _best_ model, as it has the lowest risk on target distribution. Proposition 1 further explains the superiority of \(f_{}\) over the fine-tuned model \(f_{}\) and the naive ensemble \(f_{}()=f_{}()+f_{}( )\):

**Corollary 1**.: \(f_{}\) _performs better than fine-tuned model \(f_{}\) and naive emsembling \(f_{}\):_

\[_{t}(f_{})_{t}(f_{}),\ _{t}(f_{})_{t}(f_{})\] (12)

**Discussion: when do the GLA models degenerate?** Note that there are two equality signs in Eq. (12), indicating that the performance of the GLA model can degenerate to be equivalent to that of the fine-tuned model and naive ensembling in the following two cases.

**Case 1**: For the first equality, if zero-shot model \(f_{}()\) provides no further information about \(y\) given \(f_{}()\), _i.e._, \((y f_{}())|f_{}()\), then \(P_{t}(y|f_{}(),f_{}())\) degenerates to \(P_{t}(y|f_{}())\) and the first equality applies. However, in practice, as downstream model and zero-shot model provides diverse predictions, we usually encounter strict inequality, _i.e._, \(_{t}(f_{})<(f_{})\).

**Case 2**: The second equality applies when pre-training and downstream training distribution are both class-balanced. In fact, the pre-training dataset for foundation models are known to be highly skewed. Therefore, in most cases, we have \(_{t}(f_{})<_{t}(f_{})\).

In summary, the above two equalities are usually unattainable, which means that theoretically, our GLA model performs better than both the fine-tuned and the naive ensemble models.

### Estimate the label bias of the pre-training dataset

However, \(_{p}\) is usually unknown as we have no access to pre-training dataset. In this work, we seek to estimate \(_{p}\) using the zero-shot models and the downstream data. Similar to Proposition 1, we have the following proposition says that \(f_{}-_{p}\) has lower error on target distribution than any other classifiers that use \(f_{}\), see Appendix A.2 for the full proof.

**Proposition 2**.: _Let \(h:^{K}^{K}\) be an arbitrary function that predicts labels using the outputs of the zero-shot model \(f_{}()\). Let the derived classifier be denoted as \(f_{h}()=h(f_{}())\). The classifier \(f_{}-_{p}\) is better than any \(f_{h}()\): \(_{t}(f_{}-_{p})_{t}(f_{h}())\)._

Let \(\) be an arbitrary probability simplex over \(K\) classes, then we have \(_{t}(f_{}()-_{p})_{t}(f_{ }(x)-)\). Therefore, we choose to optimize a _probability simplex_\(\) over \(K\) classes such that the model \(f_{}-\) achieves the minimal empirical risk, as formulated in Eq. (3) (the Step 1 of GLA algorithm). Once we obtain the estimated class prior \(_{p}=\), we can easily implement the GLA model by ensembling \(f_{}()=f_{}()+f_{}( )-_{s}-_{p}\) (the Step 2 of GLA algorithm).

**Toy experiment.** We conducted an experiment to show that the estimated label distribution closely approximates the true one. Specifically, we trained a model with a ResNet32 backbone on the imbalanced CIFAR-10-LT  dataset with an imbalanced ratio of 10. Subsequently, we used only the test set combined with our proposed method to estimate the label distribution. This procedure simulates scenarios where only downstream data is available and the pre-training data is inaccessible.

Figure 3 reveals a strong alignment between the estimated (orange line) and the actual distributions (blue line), which is further emphasized by a small KL-divergence value of 0.00062. The toy experiment validates the effectiveness of our debiasing method.

**Discussion.** The \(\) we estimated is not the marginal log-probability of the entire pre-training distribution but the label bias matches the downstream distribution. In the above toy experiment, although training and test sets show different label distributions, their conditional distribution \(P(|y)\) remains invariant. In this case, our estimate will converge to the actual training label bias. For CLIP models, with diverse pre-training data, some might not align with the downstream domain, potentially compromising the accuracy of the estimation of the entire pre-training distribution.

However, we'd like to point out that removing the label bias of entire pre-training distribution may not optimal for downstream tasks. As a thought experiment, consider a pre-training dataset "sketch"

Figure 3: Estimating label bias of CIFAR-10-LT-IB-10.

and "photo" styles for "dog" and "cat" samples. Suppose the sample size of "dog" and "cat" is equal but there are more "sketch dogs" than "sketch cats". This means that even if the overall distribution is balanced, each style isn't, resulting in biased zero-shot predictions. if we aim to deploy models for the "sketch dogs and cats" domain, adjusting the overall label bias is insufficient. Instead, the optimal label bias should be estimated on the "sketch" distribution. We also provide experiments using LAION-400M dataset in Appendix C.2, illustrating the situation when the downstream data diverges from the pre-training set.

## 5 Experiments

We evaluate our GLA on three real-world scenarios: many-shot (Section 5.1), few-shot (Section 5.2) and long-tail learning (Section 5.3). We show that our GLA boosts performance on all three settings.

### Many-shot learning

**Datasets.** We use ImageNet  and CIFAR100  for generic object classification, Stanford-Cars  for fine-grained classification, and SUN397  for scene recognition. See Appendix B.1 for details.

**Baselines.** We compare GLA against four methods: (1) Zero-shot model, (2) Linear Probing (LP), (3) End-to-End fine-tuning (E2E), and (4) weight ensembling method WiSE-FT.

**Implementation details.** We consider two models: CLIP ViT-B/32 and ViT-B/16. For learning-based models, we fine-tune with AdamW using a cosine annealing learning rate scheduler. We fine-tune for 10 epochs on ImageNet and 20 epochs on other datasets. See Appendix B.3 for further details.

**Main results.** Table 1 compares our GLA with various baselines. We observe that our GLA can increase the performance of end-to-end fine-tuned models: it achieves \(1.5\%\) gains on ImageNet. Compared to WiSE-FT, GLA gains \(1.1\%\) top-1 accuracy boost on ImageNet dataset,. Beyond generic object recognition, our method also improves accuracy on the fine-grained dataset (Stanford Cars) and the scene recognition dataset (SUN397), by \(0.4\%\) and \(0.5\%\), respectively.

**Breakdown performance analysis.** To analyze the impact of pre-training label bias on fine-tuned and ensemble models, we present the breakdown results on ImageNet using CLIP ViT-B/16, as shown in Table 2. Specifically, we sort the class index using the estimated \(_{p}\), and assign the top third of the classes as the head classes, the last third as the tail classes, and the remaining classes as the medium classes. Due to the label bias, the zero-shot tail performance is significantly lower than the head one (\(57.2\%\)_vs._\(78.0\%\)). The resulting E2E models are also affected by the bias, with the tail performance being \(6.3\%\) lower than the head. Existing ensemble WiSE-FT overlooks the bias, exhibiting noticeable degradation on the tail performances (\(-0.9\%\)) compared to E2E model, while our GLA stands out for all three subgroups.

**Estimated \(_{p}\) is transferable across different zero-shot models.** The estimated \(_{p}\) should be transferable across different zero-shot models if they are trained on the same pre-training dataset. To verify this, we employed a CLIP ViT-B/32 based zero-shot model to estimate \(_{p}\), which is subsequently used to debias zero-shot models based on CLIP ViT-B/16 and ViT-L/14. As shown in Table 3, our debiased models outperform the original zero-shot versions by a clear margin.

Table 1: Accuracy of various methods using CLIP ViT-B/32 and ViT-B/16. LP: linear probe; E2E: end-to-end fine-tuning. Results were obtained using the official implementation from WiSE-FT .

**Ensembling with mixing coefficient.** In Section 5.1, we prove the optimal solution is to combine the debiased zero-shot and fine-tuned models equally. We now examine the claim by introducing a mixture coefficient \(\). The ensemble predictions are given by: \(f_{}(,)=(1-)(f_{}()- _{p})+(f_{}()-_{s})\). We compare the GLA and the naive ensembling with mixture \(\) in Figure 4, where GLA meets its optimal performance at \(=0.5\), which is in line with our theoretical analysis. We also observe that the debiased zero-shot model increases accuracy by \(2.3\%\) and our GLA consistently outperforms naive ensembling with various \(\).

### Few-shot learning

For few-shot scenarios, we primarily choose prompt tuning for fine-tuning, since it is empirically more effective than end-to-end fine-tuning and linear probing [54; 55; 8].

**Datasets.** We follow CoOp to use 15 datasets: ImageNet , Caltech101 , OxfordPets , StanfordCars , Flowers102 , Food101 , FGVCAircraft , EuroSAT , UCF101 ,

   Method & Head & Med. & Tail & All \\  Zero-shot & 78.0 & 69.8 & 57.2 & 68.3 \\ E2E & 83.6 & 83.0 & 77.3 & 81.3 \\ WiSE-FT & **85.3** & 83.7 & 76.4 & 81.7 \\ GLA & **85.2** & **84.3** & **78.8** & **82.8** \\   

Table 2: Breakdown results on ImageNet.

Figure 5: Accuracy (%) of few-shot learning on 11 datasets.

Figure 4: Accuracy with mixing coefficient \(\).

    & Source &  \\   & ViT-B/32 & ViT-B/16 & ViT-L/14 \\  \(f_{}()\) & 63.4 & 68.8 & 75.6 \\ \(f_{}()=_{p}\) & **65.4** & **69.3** & **76.3** \\   

Table 3: Estimated \(_{p}\) is transferable across different backbones. \(_{p}\) is estimated using CLIP ViT-B/32.

DTD , SUN397 , ImageNet-{V2 , Sketch , A , R }. We randomly select {1, 2, 4, 8, 16} shots for training and use the original test set for evaluation. See Appendix B.1 for details.

**Baselines.** We compare with three prompt tuning methods: (1) CoOp  optimizes prompts via empirical risk minimization; (2) ProGrad  prevents forgetting the general knowledge using zero-shot predictions; (3) PLOT  applies optimal transport to match the vision and text modalities.

**Implementation details.** We implement our proposed GLA method using CLIP-ResNet-50 as the foundation model and adopt class-specific prompt tuning from CoOp. Results are averaged over three seeds, with training configurations aligned with CoOp. See Appendix B.4 for further information.

**Main results.** Figure 5 summarizes the few-shot results on 11 datasets. The detailed accuracy and standard deviation are in Appendix C.1. Overall, our GLA clearly outperforms the baselines on average performance by a large margin, _e.g._, our GLA gains \(4.6\%\), \(4.8\%\), \(3.1\%\), \(2.6\%\) and \(1.6\%\) performance boost over CoOp at \(1,2,4,8,16\) shots. In particular, on ImageNet dataset, we observe a large improvement, _e.g._, \(3.9\%\), \(2.9\%\), \(1.9\%\), \(2.0\%\) and \(2.2\%\) over ProGrad at \(1,2,4,8,16\) shots. Furthermore, on OxfordPerts, Food101 and SUN397 datasets, our method's performance remains stable and consistently improves with increasing sample sizes, while the one of the baseline methods fluctuates significantly. In particular, on Food101, baseline models even underperform zero-shot models by a large margin, while our GLA shows clearly better performance than zero-shot models.

**Robustness to distribution shifts.** Following CoOp, we used the ImageNet at 16 training shots as the source domain and assess the robustness on ImageNet-{V2, Sketch, A, R} datasets. Table 4 summarizes the results, where the prompt tuning baselines perform worse on distribution shifted datasets compared to the zero-shot model, as fine-tuning on limited data misleads the model to learn in-distribution correlations. In comparison, our GLA approach makes the best of both fine-tuned and zero-shot models, thus consistently outperforms other methods on both source and target domains.

**GLA improves accuracy over naive ensemble.** Figure 6 compares the results between our GLA and naive ensembling. We rank the absolute improvements over fine-tuning baseline at 16 training shots. In summary, our GLA demonstrates superior accuracy gains. It is worth noting that the naive ensembling does not always lead to improvements, _e.g._, on EuroSAT, SUN, Caltech, UCF and Aircraft, naive ensembling even underperforms fine-tuning baseline.

**Debiased zero-shot models perform better.** We estimate \(_{p}\) at 16 shots and compare the original zero-shot models with the debiased zero-shot models in Table 5. It is clear that the debiasing leads to improvement on all 11 datasets, showcasing an average accuracy gain of \(1.6\%\).

### Long-tail learning

**Datasets and metrics.** We evaluate our method on two standard benchmarks: Places365-LT and ImageNet-LT . In addition to top-1 accuracy, we report the accuracy on three test subsetsaccording to the number of samples per class: many-shot (\(>100\) samples), medium-shot (\(20 100\) samples), and few-shot (\(<20\) samples). Detailed information is provided in Appendix B.1.

**Fine-tuning and long-tail learning baselines.** We compare our GLA with the combinations of fine-tuning protocols and long-tailed recognition methods. We consider three fine-tuning protocols: 1) Linear Probe (LP) 2) End-to-End (E2E) fine-tuning; 3) Prompt Tuning (PT). The three fine-tuning paradigms are introduced in Section 3.1 with details in Appendix B.3. We compare with 5 long-tail learning methods: 1) standard Empirical Risk Minimization (ERM); 2) Learnable Weight Scaling (LWS) ; 3) Logit Adjustment (LA) ; 4) Balanced Softmax (BS) , and 5) BALLAD , which is designed for VLMs. See Appendix B.6 for more details on long-tail baselines.

**Implementation Details.** For all combinations of the fine-tuning and long-tail learning baselines, visual backbones are initialized from CLIP-ResNet-50 and classifiers are initialized via zero-shot prompting. We use SGD for 50 epochs with batch size of 512. See Appendix B.6 for further details.

**Results.** Table 6 shows that our GLA method consistently surpasses baselines across all long-tailed datasets. Our approach outperforms PT-based models by \(3.5\%\) and \(4.4\%\) on ImageNet-LT and Places365-LT, respectively. Against E2E approaches, GLA exceeds not just the WiSE-FT but also the current SOTA method BALLAD, by a large margin, _e.g._, 1pp gains on ImageNet-LT.

## 6 Conclusion and Limitation

In this paper, we identify the label bias in foundation models and underscore its adverse effects on downstream task performance. We propose the Generalized Logit Adjustment (GLA) framework for fine-tuning foundation models, which boosts the performance by effectively eliminating label bias and combining diverse predictions from zero-shot and fine-tuned models. We prove that when presented with zero-shot and fine-tuned models, our GLA is the Bayes optimal classifier for downstream task. Extensive experiments across a diverse range of tasks and fine-tuning framework demonstrate the effectiveness of our approach. We believe that the proposed GLA may partially improve the fairness and credibility of foundation models.

The first limitation is that we only focus on the label bias while other forms of model biases, _e.g._, representation bias , cannot be addressed by our algorithm yet. The second limitation is that we primarily focus on enhancing the fine-tuning performance for discriminative models. However, applying our GLA framework to generative models presents challenges. For instance, language generation operates as a Markov process, meaning each output depends on previous ones. This implies it's not straightforward to estimate the biasedness of a sequence with our GLA, as we only compute the bias in a pre-defined and independent label space.

Table 6: The performances on ImageNet-LT and Places365-LT.