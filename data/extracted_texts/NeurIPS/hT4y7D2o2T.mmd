# Propensity Score Alignment of

Unpaired Multimodal Data

 Johnny Xi

Department of Statistics

University of British Columbia

Vancouver, Canada

johnny.xi@stat.ubc.ca &Jana Osea

Valence Labs

Montreal, Canada

jana@valencelabs.com &Zuheng (David) Xu

Department of Statistics

University of British Columbia

Vancouver, Canada

zuheng.xu@stat.ubc.ca &Jason Hartford

Valence Labs

London, UK

jason@valencelabs.com

Work done during an internship at Valence Labs

###### Abstract

Multimodal representation learning techniques typically require paired samples to learn shared representations, but collecting paired samples can be challenging in fields like biology, where measurement devices often destroy the samples. This paper presents an approach to address the challenge of aligning unpaired samples across disparate modalities in multimodal representation learning. We draw an analogy between potential outcomes in causal inference and potential views in multimodal observations, allowing us to leverage Rubin's framework to estimate a common space for matching samples. Our approach assumes experimentally perturbed samples by treatments, and uses this to estimate a propensity score from each modality. We show that the propensity score encapsulates all shared information between a latent state and treatment, and can be used to define a distance between samples. We experiment with two alignment techniques that leverage this distance--shared nearest neighbours (SNN) and optimal transport (OT) matching--and find that OT matching results in significant improvements over state-of-the-art alignment approaches in on synthetic multi-modal tasks, in real-world data from NeurIPS Multimodal Single-Cell Integration Challenge, and on a single cell microscopy to expression prediction task.

## 1 Introduction

Large-scale multimodal representation learning techniques such as CLIP (Radford et al., 2021) have lead to remarkable improvements in zero-shot classification performance and have enabled the recent success in conditional generative models. However, the effectiveness of multimodal methods hinges on the availability of _paired_ samples--such as images and their associated captions--across data modalities. This reliance on paired samples is most obvious in the InfoNCE loss (Gutmann and Hyvarinen, 2010; van den Oord et al., 2018) used in CLIP (Radford et al., 2021) which explicitly learns representations to maximize the true matching between images and their captions.

While paired image captioning data is abundant on the internet, paired multimodal data is often challenging to collect in scientific experiments. For instance, unpaired data are the norm in biology for technical reasons: RNA sequencing, protein expression assays, and the collection of microscopyimages for cell painting assays are all destructive processes. As such, we cannot collect multiple different measurements from the same cell, and can only explicitly group cells by their experimental condition. If we could accurately match unpaired samples across modalities, we could use the aligned samples as proxies for paired samples and apply existing multimodal learning techniques.

In this paper, we formalize this setting by viewing each modality as a _potential_ measurement, \(X^{(1)}(Z)^{(1)},X^{(2)}(Z)^{(2)}\), of the same underlying latent state \(Z\), where we are only able to make a single measurement for each sample unit (e.g. an individual cell). The task is to reconcile (_match_) unpaired observations \(x^{(1)}\) and \(x^{(2)}\) with the same (or maximally similar) \(z\). Estimating the latent, \(Z\), is hopelessly underspecified without making unverifiable assumptions on the system, and furthermore, \(Z\) may still be sparse and high-dimensional, leading to inefficient matching. This motivates the need for approaches that use only the observable data.

We identify two major challenges for this problem. First, measurements are often made in very different spaces \(^{(1)}\) and \(^{(2)}\) (e.g., pixel space and gene expression counts), which make defining a notion of similarity across modalities challenging. Second, the measurement process inevitably introduces modality-specific variation that can be impossible to disentangle from the relevant information (\(Z\)). For example in cell imaging, we would not want the matching to depend on irrelevant appearance features such as the orientation of the cell or the lighting of the plate.

In this paper, we address these challenges by appealing to classical ideas from causal inference (Rubin, 1974), in the case where we additionally observe some label \(t\) for each unit, e.g., indexing an experiment. By making the assumption that \(t\) perturbs the observations via their shared latent state, we identify an observable link between modalities with the same underlying \(z\). Under conditions which we discuss in Section 2, the propensity score, defined as \(p(t|Z)\), is a transformation of the latent \(Z\) that satisfies three remarkable properties (Proposition 3.1): (1) it provides a common space for matching, (2) it is fully identifiable via classification on individual modalities, and (3) it maximally reduces the dimension of \(Z\), retaining only the information revealed by the perturbations.

The practical implementation of the methodology (as illustrated in Fig. 1) is then straightforward: we train two separate classifiers, one for each modality, to predict the treatment \(t\) applied to \(X^{(i)}\). We then match across modalities based on the similarity between the predicted probabilities (the propensity score) within each treatment group. This matching procedure is highly versatile and can be applied to match labeled observations between any modalities for which a classifier can be efficiently trained. However, since the same sample unit does not appear in both modalities, we cannot use naive bipartite matching. To address this, we use soft matching techniques to estimate the missing modality for each sample unit by allowing matching to multiple observations. We experiment with two recent matching approaches: shared nearest neighbours (SNN) matching (Lance et al., 2022; Cao and Gao, 2022) and optimal transport (OT) matching Villani (2009).

Figure 1: Visualization of propensity score matching for two modalities (e.g., Microscopy images and RNA expression data). We first train classifiers to estimate the propensity score for samples from each modalities; the propensity score reveals the shared information \(p(t|z_{i})\), which allows us to re-pair the observed disconnected modalities. The matching procedure is then performed within each perturbation class based on the similarity bewteen the propensity scores.

In our experiments, we find that OT matching with distances defined on the propensity score leads to significant improvement on matching and a downstream cross-modality prediction task on both synthetic and real-world biological data. Notably, our prediction method, which leverages the soft matching to optimize an OT projected loss, outperforms supervised learning on the true pairs on CITE-seq data from the NeurIPS Multimodal Single-Cell Integration Challenge (Lance et al., 2022). Finally, we applied our method to match single-cell expression data (from a PeturbSeq assay (Dixit et al., 2016)) with single cell crops of image data (Fay et al., 2023). We find improved generalization in predicting the distribution of gene expression from the cell imaging data in with unseen perturbations.

### Related Work

Unpaired and Multimodal DataLearning from unpaired data has long been considered for image translation (Liu et al., 2017; Zhu et al., 2017; Almahairi et al., 2018), and more recently for biological modality translation (Amodio and Krishnaswamy, 2018; Yang et al., 2021). In particular, Yang et al. (2021) also takes the perspective of a shared latent variable for biological modalities. This setting has been studied more generally for multi-view representation learning (Gresele et al., 2020; Sturma et al., 2023) for its identifiability benefits.

Perturbations and HeterogeneityMany methods in biology treat observation-level heterogeneity as a nuisance dimension to globally integrate, even when cluster labels are observed (Butler et al., 2018; Korsunsky et al., 2019; Foster et al., 2022). This is sensible when clusters correspond to noise rather than the signal of interest. However, it is well known in causal representation learning that heterogeneity--particularly heterogeneity arising from perturbations--has theoretical benefits in constraining the solution set (Khemakhem et al., 2020; Squires et al., 2023; Ahuja et al., 2023; Buchholz et al., 2023; von Kugelgen et al., 2023). There, the benefits (weakly) increase with the number of perturbations, which is also true of our setting (Proposition 3.2). In the context of unpaired data, only Yang et al. (2021) explicitly leverage this heterogeneity in their method, while Ryu et al. (2024) treat it as a constraint in solving OT. Specifically, Yang et al. (2021) require their VAE representations to classify experimental labels in addition to reconstructing modalities, while our method is simpler, only requiring the classification objective. Notably, Yang et al. (2021) treat our objective as a regularizer, but our theory suggests that it is actually primarily responsible for the matching performance. Our experiment results coincide with the theoretical insights; requiring reconstruction, as in a VAE, led to worse matching performance with identical model architectures.

Optimal Transport MatchingOT is a common tool in single-cell biology. In cell trajectory inference, the unpaired samples are gene expression values measured at different time points in a shared (metric) space. OT matching minimizes this shared metric between time points (Schiebinger et al., 2019; Tong et al., 2020). Recent work (Demcetti et al., 2022) extends this to our setting where each modality is observed in separate metric spaces by using the Gromov-Wasserstein distance, which computes the difference between the metric evaluated within pairs of points from each modality (Demcetti et al., 2022). In concurrent work, this approach was recently extended to ensure matching within experimental labels (Ryu et al., 2024). In addition to these "pure" OT approaches, Gossi et al. (2023) use OT on contrastive learning representations, though this approach requires matched pairs for training, while Cao et al. (2022) use OT in the latent space of a multi-modal VAE.

## 2 Setting

We consider the setting where there exist two potential views, \(X^{(e)}^{(e)}\) from two different modalities indexed by \(e\{1,2\}\), and experiment \(t\) that perturbs a shared latent state of these observations. This defines a jointly distributed random variable \((X^{(1)},X^{(2)},e,t)\), from which we observe only a single modality, its index, and label, \(\{x_{i}^{(e_{i})},e_{i},t_{i}\}_{i=1}^{n}\).2 We aim to match or estimate the samples from the missing modality, which corresponds to the realization of the missing random variable. Since \(t\) is observed, in practice we match observations _within_ the same label class \(t\).

Formally, we assume each modality is generated by a common latent random variable \(Z\) as follows:

\[t P_{T},\;Z^{(t)} t P_{Z}^{(t)},\;U^{(e)} P_{U}^{(e)},\;U^{(e )} 2.0mu {}}Z,\;U^{(e)} 2.0mu {}}U^{(e^{})},\;X^{(e)} t=f^{(e)}(Z^{(t)},U^{(e)}),\] (1)where \(t\) indexes the experimental perturbations, and we take \(t=0\) to represent a base environment. \(U^{(e)}\) represents the modality-specific measurement noise that is unperturbed by \(t\), and also independent across samples. The structural equations \(f^{e}\) are deterministic after accounting for the randomness in \(Z\) and \(U\): it represents the measurement process that captures the latent state. For example, in a microscopy image, this would be the microscope and camera that maps a cell to pixels.

Comparison to Multimodal Generative ModelsOur setting is technically that of a multimodal generative model with latent perturbations. However, by focusing on matching rather than generation, we are able to make significantly weaker and more meaningful assumptions while still ensuring the theoretical validity of our method. Without the effects of the perturbation, our Eq. (1) is essentially the same as (Yang et al., 2021, Equation 1) in an abstract sense. However, in order to fit the generative model, it is required to formulate explicit models over \(f^{(e)}\) and \(P_{Z}^{(t)}\), which requires specifying the function class (e.g., continuous) and the space of \(Z\) (e.g., \(^{d}\)) as assumptions, even in universal approximation settings. In contrast, since we will not directly fit the model Eq. (1), we do not make any technical assumptions about the generative model. Instead, we will make the following assumptions on the underlying data generating process itself.

Key AssumptionsOur theory makes the following assumptions about the data generating process.

1. [label=(A0)]
2. \(t\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!_i.e., \((X^{(1)})=(X^{(2)})=(Z)\) as random variables. This implies_

\[I(t,Z(Z))=I(t,Z(X^{(e)}))=0,\] (3)

_for each \(e=1,2\), where \(I\) is the mutual information. Furthermore, any other function \(b(Z)\) satisfying \(I(t,Z b(Z))=0\) is such that \((Z)=f(b(Z))\)._

The proof can be found in Appendix C. Practically, Proposition 3.1 shows that computing the propensity score on either modality is equivalent to computing it on the unobserved shared latent, which means that it is identifiable, and thus estimable, from the observations alone. Furthermore, the estimation does not require modified objectives or architectures for joint multimodal processing, instead they are simple and separate classification problems for each modality. Finally, \(t\) does not affect \(U^{(e)}\) by assumption, and thus the propensity score, being a representation of the information in \(t\), discards the modality-specific information that may be counterproductive to matching. Therefore, even if \(Z\) were observed, it may be sensible to match on its propensity score instead.

Number of PerturbationsNote that point-wise equality of the propensity score \((z_{1})=(z_{2})\) does not necessarily imply equality of the latents \(z_{1}=z_{2}\), due to potential non-injectivity of \(\). For example, consider \(t\{0,1\}\), then \((z)\) is a compression to a single dimension \(z p(t=1 z)\). Intuitively, collecting data from more perturbations improves the amount of information contained in the label \(t\). If the latent space is \(^{d}\), the propensity score necessarily compresses information about \(Z^{(t)}\) if the latent dimension exceeds the number of perturbations, echoing impossibility results from the causal representation learning literature (Squires et al., 2023).

**Proposition 3.2**.: _Let \(Z^{(t)}^{d}\). Suppose that \(P_{Z}^{(t)}\) has a smooth density \(p(z|t)\) for each \(t=0,,T\). Then, if \(T<d\), the propensity score \(\), restricted to its strictly positive part, is non-injective._

The proof can be found in Appendix C. Note the above only states an impossibility result when \(T<d\). More generally, it can be seen from the proof of Proposition 3.2 that the injectivity of the propensity score depends on the injectivity of the following expression in \(z\):

\[g(z)=(p(z|t=1))-(p(z|t=0))\\ \\ (p(z|t=T))-(p(z|t=0)),\] (4)

which then depends on the latent process itself. If the above mapping is non-injective, this represents a fundamental indeterminacy that cannot be resolved without making strong assumptions on point-wise latent variable recovery. As we have already established in Proposition 3.1, the propensity score contains the maximal shared information across modalities. Nonetheless, collecting data form a larger number of perturbations is clearly beneficial for matching, since \(g\) in Eq. (4) is injective if any of the subset of its entries are.

## 4 Estimation and Matching

For the remainder of the paper, we drop the notation \(e\) and use \((x_{i},t_{i})\) to denote observations from modality 1, and \((x_{j},t_{j})\) to denote observations from modality 2. Given a multimodal dataset with observations \(\{(x_{i},t_{i})\}_{i=1}^{n_{1}}\) and \(\{(x_{j},t_{j})\}_{j=1}^{n_{2}}\), we wish to compute a matching matrix (or coupling) between the two modalities. We define a \(n_{1} n_{2}\) matching matrix \(M\) where \(M_{ij}\) represents the likelihood of \(x_{i}\) being matched to \(x_{j}\). Since \(t\) is observed, we always perform matching only within observations with the same value of \(t\), so that in practice we obtain a matrix \(M_{t}\) for each \(t\).

Our method approximates the propensity scores by training separate classifiers that predicts \(t\) given \(x\) for each modality. We denote the estimated propensity score by \(_{i}\) and \(_{j}\) respectively, where

\[_{i}(x_{i})=P(T=t X_{i}^{(e)}=x_{i}).\] (5)

This yields the transformed datasets \(\{_{i}\}_{i=1}^{n_{1}}\) and \(\{_{j}\}_{j=1}^{n_{2}}\), where \(_{i}\), \(_{j}\) are in the \(T\) dimensional simplex. We use this correspondence to compute a cross-modality distance function:

\[d(x_{i},x_{j}):=d^{}(_{i},_{j}).\] (6)

In practice, we typically compute the Euclidean distance in \(^{T}\) of the logit-transformed classification scores, but any metric over a bijective transformation of the propensity scores are also theoretically valid. Given this distance function, we use existing matching techniques to constructing a matching matrix. In our experiments, we found that OT matching gave the best performance, but we also evaluated Shared Nearest Neighbour matching; details of the latter can be found in Appendix B.

Optimal Transport MatchingThe propensity score distance allows us to easily compute a cost function associated with transporting mass between modalities, \(c(x_{i},x_{j})=d^{}(_{i},_{j})\). Let \(p_{1},p_{2}\) denote the uniform distribution over \(\{_{i}\}_{i=1}^{n_{1}}\) and \(\{_{j}\}_{j=1}^{n_{2}}\) respectively. Discrete OT aims to solve the problem of optimally redistributing mass from \(p_{1}\) to \(p_{2}\) in terms of incurring the lowest cost. Let \(C_{ij}=c(x_{i},x_{j})\) denote the \(n_{1} n_{2}\) cost matrix. The Kantorovich formulation of optimal transport aims to solve the following constrained optimization problem:

\[_{M}_{i}^{n_{1}}_{j}^{n_{2}}C_{ij}M_{ij}, M_{ij}  0, M=p_{1}, M^{}=p_{2}.\] (7)

This is a linear program, and for \(n_{1}=n_{2}\), it can be shown that the optimal solution is a bipartite matching between \(\{_{i}\}_{i=1}^{n_{1}}\) and \(\{_{j}\}_{j=1}^{n_{2}}\). We refer to this as exact OT; in practice we add an entropic regularization term, resulting in a soft matching, that ensures smoothness and uniqueness, and can be solved efficiently using Sinkhorn's algorithm. Entropic OT takes the following form:

\[_{M}_{i}^{n_{1}}_{j}^{n_{2}}C_{ij}M_{ij}- H(M),  M_{ij} 0, M=p_{1}, M^{}=p_{2},\] (8)

where \(H(M)=-_{i,j}M_{ij}(M_{ij})\), the entropy of the joint distribution implied by \(M\). This approach regularizes towards a higher entropy solution, which has been shown to have statistical benefits (Genevay et al., 2018), but nonetheless for small enough \(\) serves as a computationally appealing approximation to exact OT.

## 5 Downstream Tasks

The matching matrix \(M\) can be seen as defining an empirical joint distribution over the samples in each modality. The OT approach in particular makes this explicit. Each row is proportional to the probability that each sample \(i\) from modality (1) is matched to sample \(j\) in modality (2), i.e., \(M_{i,j}=P(x_{j}|x_{i})\). We can thus use \(M\) to obtain pseudosamples for any learning task that uses paired samples by \((x_{i},_{j})\), where \(_{j}\) is obtained by sampling from the conditional distribution defined by \(M\), or by a suitable conditional expectation, e.g., the barycentric projection (conditional mean) as \(E_{M}[X_{j} X_{i}=x_{i}]=_{j}M_{i,j}x_{j}\). In what follows, we describe a cross-modality prediction method based on both barycentric projection and stochastic gradients according to \(M_{i,j}\).

Cross-modality predictionWe can use the matching matrix to design a method for cross-modality prediction/translation. The following MSE loss corresponds to constructing a prediction function \(f_{}\) such that the barycentric projection \(E_{M}[f_{}(X_{j}) X_{i}=x_{i}]\), under \(M\) minimizes the squared error for predicting \(x_{i}\):

\[():=_{i}(x_{i}-_{j}M_{i,j}f_{}(x_{j }))^{2}.\] (9)

However, this requires evaluating \(f_{}\) for all \(n_{2}\) examples from modality (2) for each of the \(n_{1}\) examples in modality (1). In practice, we can avoid this cost with stochastic gradient descent by sampling from modality \((2)\) via \(M_{i}\). for each training example \((1)\). To obtain an unbiased estimate of \(_{}\), we need two independent samples from modality (2) for each sample from modality (1),

\[()-2(x_{i}-f_{}(_{ j}))_{}f_{}(_{j})_{j},_{j}  P(x_{j}|x_{i}).\] (10)

By taking two samples as in Eq. (10), we get an unbiased estimator of \(()\), whereas a single sample would have resulted in optimizing an upper-bound on equation (9); for details, see Hartford et al. (2017) where a similar issue arises in the gradient of their causal effect estimator. We thus refer to prediction models trained via Eq. (10) as _unbiased_.

Experiments

We present a comprehensive evaluation of our proposed methodology on three distinct datasets: (1) synthetic paired images, (2) single-cell CITE-seq dataset (simultaneous measurement of single-cell RNA-seq and surface protein measurements) (Stoeckius et al., 2017), and (3) Perturb-seq and single-cell image data. In the first two cases, there is a ground-truth matching that we use for evaluation, but samples are randomly permuted during training. This allows us to exactly compute the quality of the matching in comparison to the ground truth. The final dataset is a more realistic setting where ground truth paired samples do not exist, and matching becomes necessary in practice. In this case, we compute distributional metrics to compare our proposed methodology against other baselines.

Experimental DetailsAll models for the experiments are implemented using Torch v2.2.2 (Paszke et al., 2017) and Pytorch Lightning v2.2.4 (Falcon and PyTorch Lightning Team, 2023). The classifier used to estimate the propensity score is always a linear head on top of an encoder \(E_{i}\), which is specific to each modality and dataset. All models are saved at the optimal validation loss to perform subsequent matching. Shared nearest neighbours (SNN) is implemented using scikit-learn v1.4.0 (Pedregosa et al., 2011) using a single neighbour, and OT is implemented using the Sinkhorn algorithm as implemented in the pot v0.9.3 package (Flamary et al., 2021). Both SNN and OT use the Euclidean distance as the metric. Whenever random variation can affect the results of the experiments, we report quantiles corresponding to variation from different random seeds. Additional experimental details are provided in Appendix D.

Description of BaselinesOur main baseline, which we evaluate against on all three datasets, is matching using representations learned by the multimodal VAE of Yang et al. (2021), which is the only published method that is able to leverage perturbation labels for unpaired multimodal data (they refer to the labels as "prior information"). The standard multimodal VAE loss is a reconstruction loss based on encoder and decoders \(E_{i}\), \(D_{i}\) for each modality, plus a latent invariance loss that aims to align the modalities in the latent space. In our setting, the multimodal VAE loss further includes an additional label classification loss from the latent space of each modality, i.e., encouraging the encoder to simultaneously learn \(P(t E_{i}(x_{i}))\). This additional objective, which acts as a regularizer for the multimodal VAE, is exactly the loss for our proposed method. To ensure a fair comparison, we always use the same architecture in the encoders \(E_{i}\) of multimodal VAE and in our propensity score classifier. The performance differences between propensity score matching and multimodal VAE then represent the effects of the VAE reconstruction objective and latent invariance objectives. For additional baselines, we also compare against a random matching, where the samples are matched with equal weight within each perturbation as a sanity check. For datasets (1) and (2), we also compare against Gromov-Wasserstein OT (SCOT) (Demcici et al., 2022) computed separately within each perturbation. SCOT uses OT directly by computing a cost function derived based on pairwise distances within each modality, thus learning a local description of the geometry which can be compared between modalities. For the CITE-seq dataset, we also compare against matching using a graph-linked VAE, scGLUE (Cao and Gao, 2022), where the graph is constructed from linking genes with the associated proteins.

Evaluation MetricsWe use the known ground truth matching to compute performance metrics on datasets (1) and (2). The trace and FOSCTTM (Liu et al., 2019) measure how much weight \(M\) places on the true pairing. However, this is not necessarily indicative of downstream performance as similar, but not exact matches are penalized equally to wildly incorrect matches. For this reason, we also measure the latent MSE for dataset (1) and the performance of a CITE-seq gene-to-protein predictive model based on the learned matching for dataset (2). For more details, see Appendix D.1.

### Experiment 1: Synthetic Interventional Images

DataWe followed the data generating process Eq. (1) with a latent variable \(Z\) encoding the coordinates of two objects. Perturbations represent different do-interventions on the different dimensons of \(Z\). The difference between modalities corresponds to whether the objects are circular or square, and a fixed transformation of \(Z\), while the modality-specific noise \(U\) controls background distortions.

Model and EvaluationWe used a convolutional neural network adapted from Yang et al. (2021) as the encoder. We report two evaluation metrics: (1) the trace metric, and (2) the MSE between the matched and the true latents. The latent MSE metric does not penalize close neighbours of the true match (i.e. examples for which \(\|z_{i}-z_{i}^{*}\|\) is small) as heavily as the trace metric. These "near matches" will typically still be useful on downstream multimodal tasks.

ResultsIn Table 1, metrics are computed on a held out test set over 12 groups corresponding to interventions on the latent position, with approximately 1700 observations per group. A random matching, with weight \(1/n\), will hence have a trace metric of of \(1/1700 0.588 10^{-3}\). This implies, for example, that the median performance of PS+OT is approximately 31 times that of random matching. On both metrics, we found that propensity scores matched with OT (PS + OT) consistently outperformed other matching methods on both metrics.

### Experiment 2: CITE-Seq Data

DataWe used the CITE-seq dataset from the NeurIPS 2021 Multimodal single-cell data integration competition (Lance et al., 2022), consisting of paired RNA-seq and surface level protein measurements over \(45\) cell types. In the absence of perturbations, we used the cell type as the observed label to classify and match within. Note the cell types are determined by consensus by pooling annotations from marker genes/proteins. In most cells, the annotations from each modality agreed, suggesting that the label is independent from the modality-specific noise. We used the first 200 principal components as the gene expression modality, and normalized (but otherwise raw) protein measurements as input.

Model and EvaluationWe used fully-connected MLPs as encoders. To assess matching, we report (1) the trace, and (2) the Fraction Of Samples Closer Than the True Match (FOSCTTM) (Demetici et al., 2022), (Liu et al., 2019) (lower is better, 0.5 corresponds to random guessing). To evaluate against a downstream task, we also compared the performance of random and VAE matching procedures, as well as directly using the ground truth (\(M_{ii}=1\)), on predicting protein levels from gene expression. We trained a 2-layer MLP (the same architecture for all matchings) with both MSE loss and the unbiased procedure as described in Section 5 using pseudosamples sampled according to the matching matrix. We evaluated the predictive models against ground truth pairs by computing the prediction \(R^{2}\) (higher is better) on a held-out, unpermuted, test set.

ResultsIn Table 1, metrics are computed on a held-out test set averaged over 45 cell types with varying observation counts per group. While interpreting the average trace can be challenging due to group size variations, OT matching on PS consistently outperformed other methods both within and across groups. In these experiments, OT matching on PS was consistently the top performer, often followed by SNN matching on PS or OT matching on VAE embeddings.

We present downstream task performance in Table 2. Note that \(R^{2}\) is computed using the sample average across possibly multiple cell types, which explains why random matching within each cell type results in non-zero \(R^{2}\) (see Appendix D.1). We found that PS + OT matching outperforms other methods on this task. Surprisingly, the PS + OT prediction model performed even better on average than training with the standard MSE loss on ground truth pairings (though confidence intervals

  &  &  \\ 

overlap). This highlights the potential benefit of soft (OT) matching as a regularizer, beyond that of simply reconciling most likely pairs: the soft matching effectively averages over modality specific variation from samples with similar latent states in a manner analogous to data augmentation (with an unknown group action).

### Experiment 3: PerturbSeq and Single Cell Images

DataWe collected PerturbSeq data (200 genes) and single-cell images of HUVEC cells with 24 gene perturbations and a control perturbation, resulting in 25 total labels across both modalities. As preprocessing, we embed the raw PerturbSeq counts into a 128-dimensional space using scVI  and the cell images into a 1024-dimensional space using a pre-trained Masked Autoencoder  to train our gene expression and image classifiers.

Model and EvaluationWe used a fully connected 2-layer MLP as the encoder for both PerturbSeq and cell image classifiers. Similarly to the CITE-seq dataset, we evaluated the matchings based on downstream prediction of gene expression from (embeddings of) images. We used the unbiased procedure to minimize the projected loss Eq. (9) and evaluated on two held-out sets, one consisting of in-distribution samples from the 25 perturbations the classifier was trained on, and an out-of-distribution set consisting of an extra perturbation not seen in training. In the absence of ground truth matching, we assessed three distributional metrics between the actual and predicted gene expression values within each perturbation: the L2 norm of the difference in means, the Kullback-Leibler (KL) divergence, and 1-Wasserstein distance (lower indicates better alignment). We report inverse cell-count weighted averages over each perturbation group. Each metric measures a slightly different aspect of fit--the L2 norm reports a first-order deviation, while the KL divergence is an empirical estimate of the deviation of the underlying predicted distribution, while the 1-Wasserstein distance measures deviations in terms of the empirical samples themselves.

Note that matching is performed using classifiers trained on scVI embeddings, but the cross-modal predictions are generated in the original log transformed gene expression space (i.e. we predicted actual observations, not embeddings). We also evaluated distance measures on an out-of-distribution gene perturbation that was not used in either the matching or training of the translation model.

ResultsWe present KL divergence values for in-distribution and out-of-distribution in Table 2.4 Additional metrics show similar patterns and can be found in Appendix D.4. OT + PS matching consistently outperforms its VAE counterpart both on in-distribution and out-of-distribution metrics, supporting our findings on the CITE-seq data to the case where ground truth pairs are not available.

### Validation Monitor

As in our Perturb-seq and cell imaging example, the ground truth matching is typically unknown in real problems. It is hence desirable to have an observable proxy of the matching performance

  &  &  \\   & **MSE Loss** & **Unbiased Loss** & **In Distribution** & **Out of Distribution** \\  & \(R^{2}\)**Med (Q1, Q3) (\(\))** & \(R^{2}\)**Med (Q1, Q3) (\(\))** & **KL Med (Q1, Q3) (\(\))** & **KL (\(\))** \\  Random & 0.138 & 0.173 & 58.806 & 51.310 \\  & (0.137, 0.140) & (0.170, 0.173) & (58.771, 60.531) & \\ VAE+OT & 0.149 & 0.114 & 55.483 & 47.910 \\  & (0.118, 0.172) & (0.079, 0.159) & (55.410, 56.994) & \\
**PS+OT** & **0.217** & **0.233** & **50.967** & **43.554** \\  & **(0.206, 0.223)** & **(0.207, 0.250)** & **(50.898, 52.457)** & \\ True Pairs & 0.224 & - & - & - \\  & (0.223, 0.226) & & & \\ 

Table 2: Cross-modal prediction results using CITE-seq data and PerturbSeq/single cell image data including an out of distribution distance evaluation for PerturbSeq/single cell images.

as a validation during hyperparameter tuning. Figure 2 demonstrates that the propensity score validation loss (cross-entropy) empirically satisfies this role in our CITE-seq experiments, where lower validation loss corresponds to better matching performance, as if it were computed with the ground truth. By contrast, we found that the optimal VAE, in terms of matching, had higher validation loss. This empirically supports our intuition that the reconstruction loss minimization requires the VAE to capture modality specific information, i.e., the \(U^{(e)}\) variables, which hinders its matching performance.

## 7 Limitations

Our methods are limited to settings where we have some signal to play the role of an experiment label, but we believe this is where these methods are most needed. Matching is impossible in general--e.g., if you tried to match modalities that have no shared information, it would clearly fail--but our theory formally articulates both where we expect this method to succeed and its limitations. Both (A1) and (A2) are strong assumptions, but the empirical results suggest the method is fairly robust to failures.

## 8 Conclusion

This work presents a simple algorithm for aligning unpaired data from different modalities using propensity scores. The method is very general, requiring only a classifier to be trained on each modality, and demonstrates excellent matching performance, which we validate both theoretically and empirically. We also showcase the effectiveness of the matching algorithm in a downstream cross-modality prediction task, achieving _better_ generalization compared to random matching, VAE-based matching, and even the ground truth matching on the evaluated dataset. This improved generalization over the ground truth may be attributed to implicitly enforcing invariance to modality-specific information; a rigorous investigation of this phenomenon would be interesting for further investigation.

## 9 Acknowledgements

We are extremely grateful for the discussions with many external collaborators and colleagues at Recursion that lead to this work. The original ideas for this work stemed from conversations with Alex Tong with feedback from Yoshua Bengio at Mila. We received a lot of helpful feedback from all of our colleagues at Valence Labs, especially Berton Earnshaw and Ali Denton. The single cell image experiments are built on code originally written by Oren Kraus and his team.

Figure 2: VAE and classifier validation metrics on the CITE-seq dataset. Notice that validation cross-entropy inversely tracks the ground truth matching metrics, and thus can be used as a proxy in practical settings where the ground truth is unknown. The same pattern does not hold for the VAE (Yang et al., 2021), which we suspect is because reconstruction is largely irrelevant for matching.