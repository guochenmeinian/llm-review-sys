# Nash Regret Guarantees for Linear Bandits

Ayush Sawarni

Indian Institute of Science

Bangalore

sawarniayush@gmail.com &Soumyabrata Pal

Google Research

Bangalore

soumyabrata@google.com &Siddharth Barman

Indian Institute of Science

Bangalore

barman@iisc.ac.in

###### Abstract

We obtain essentially tight upper bounds for a strengthened notion of regret in the stochastic linear bandits framework. The strengthening--referred to as Nash regret--is defined as the difference between the (a priori unknown) optimum and the geometric mean of expected rewards accumulated by the linear bandit algorithm. Since the geometric mean corresponds to the well-studied Nash social welfare (NSW) function, this formulation quantifies the performance of a bandit algorithm as the collective welfare it generates across rounds. NSW is known to satisfy fairness axioms and, hence, an upper bound on Nash regret provides a principled fairness guarantee.

We consider the stochastic linear bandits problem over a horizon of \(\) rounds and with set of arms \(\) in ambient dimension \(d\). Furthermore, we focus on settings in which the stochastic reward--associated with each arm in \(\)--is a non-negative, \(\)-sub-Poisson random variable. For this setting, we develop an algorithm that achieves a Nash regret of \(O(}}(||))\). In addition, addressing linear bandit instances in which the set of arms \(\) is not necessarily finite, we obtain a Nash regret upper bound of \(O(}^{}}{}}())\). Since bounded random variables are sub-Poisson, these results hold for bounded, positive rewards. Our linear bandit algorithm is built upon the successive elimination method with novel technical insights, including tailored concentration bounds and the use of sampling via John ellipsoid in conjunction with the Kiefer-Wolfowitz optimal design.

## 1 Introduction

Bandit optimization is a prominent framework for sequential decision making and has several applications across multiple domains, such as healthcare  and advertising . In this framework, we have a set of arms (possible actions) with unknown means and a time horizon. The goal is to sequentially pull the arms such that the regret--which is a notion of loss defined over the bandit instance--is minimized.

We consider settings wherein the stochastic rewards generated by a sequential algorithm induces welfare across a population of agents. Specifically, there are \(\) agents, arriving one per round; in particular, the reward accrued at each round \(t[]\) corresponds to the value accrued by the \(t^{}\) agent. Indeed, such a welfarist connection exists in various applications of the bandit framework. Consider, for instance, the classic context of drug trials : Suppose there are \(\) patients and several available drugs. In each round \(t[]\), one of the available drugs is administered to the \(t^{}\) patient. Subsequently, the reward accrued at the \(t^{}\) round corresponds to the efficacy of the administered drug to the \(t^{}\) patient. In such a setting, fairness is a fundamental consideration. That is, in addition to cumulative efficacy, individual effectiveness of the drugs is quite important.

A central notion in the bandit literature is that of average regret, defined as the difference between the (a priori unknown) optimum and the arithmetic mean of the expected rewards (accumulated by the algorithm) . However, average regret fails to capture the fairness criterion that the rewards should be balanced (across the agents) and not just cumulatively high. From a welfarist viewpoint, the standard notion of (average) regret equates the algorithm's performance to the social welfare it induces. Social welfare is defined as the sum of agent's rewards  and can be high among a set of agents even if a fraction of them receive indiscriminately low rewards. For instance, in the drug-trials example provided above, high average efficacy (i.e., high social welfare) does not rule out a severely ineffective outcome for a subset of agents.

Given that average regret is defined using the sum of expected rewards, this notion inherits this utilitarian limitation of social welfare. In summary, in welfare-inducing contexts, a bandit algorithm with low average regret is not guaranteed to induce fair outcomes across rounds.

Addressing this issue and with the overarching aim of achieving fairness across rounds (i.e., across agents that receive rewards from a bandit algorithm), the current work considers a strengthened notion of regret. The strengthening--referred to as Nash regret--is defined as the difference between the (a priori unknown) optimum and the geometric mean of expected rewards induced by the bandit algorithm. It is relevant to note that the geometric mean (of rewards) corresponds to the Nash social welfare (NSW) function . This welfare function has been extensively studied in mathematical economics (see, e.g., ) and is known to satisfy fundamental fairness axioms, including the Pigou-Dalton transfer principle, scale invariance, and independence of unconcerned agents. Hence, by definition, Nash regret quantifies the performance of a bandit algorithm as the NSW it generates.

Quantitatively speaking, in order for the geometric mean (i.e., the NSW) to be large, the expected reward at every round should be large enough. The AM-GM inequality also highlights that Nash regret is a more demanding objective that average regret.

We obtain novel results for Nash regret in the stochastic linear bandits framework. In this well-studied bandit setup each arm corresponds to a \(d\)-dimensional vector \(x\) (an arm-specific context) and the unknown arm means are modelled to be a linear function of \(x\). With a focus on average regret, stochastic linear bandits have been extensively studied in the past decade . The current paper extends the line of work on linear bandits with fairness and welfare considerations.

Note that an ostensible approach for minimizing Nash regret is to take the logarithm of the observed rewards and, then, solve the average regret problem. However, this approach has the following shortcomings: (i) Taking log implies the modified rewards can have a very large range possibly making the regret vacuous, and (ii) This approach leads to a multiplicative guarantee and not an additive one. In a recent work of , the authors study Nash regret in the context of stochastic multi-armed bandits (with bounded rewards) and provide optimal guarantees. The current work notably generalizes this prior work to linear bandits.

### Our Contributions and Techniques

We consider the stochastic linear bandits setting with a set of arms \(\) over a finite horizon of \(\) rounds. Since we consider the welfarist viewpoint, we assume that the rewards across all the rounds are positive and, in particular, model the distribution of the arm rewards to be \(\)-sub-Poisson, for parameter \(_{+}\). Our goal is to minimize the Nash regret \(_{}\).

We develop a novel algorithm LinNash that obtains essentially optimal Nash regret guarantees for this setting. Specifically, for a finite set of arms \(^{d}\), our algorithm LinNash achieves Nash regret \(_{}=O(}}(| |))\). For infinite sets of arms, a modified version of LinNash achieves Nash regret \(_{}=O(}^{}}{}}())\).

Recall that Nash regret is a strengthening of the average regret; the AM-GM inequality implies that, for any bandit algorithm, the Nash regret is at least as much as its average regret. Hence, in the linear bandits context, the known \((d/})\) lower bound on average regret (see , Chapter 24) holdsfor Nash regret as well.1 This observation implies that, up to a logarithmic factor, our upper bound on Nash regret is tight with respect to the number of rounds \(\). We also note that for instances in which the number of arms \(||=(2^{d})\), the Nash-regret dependence on \(d\) has a slight gap. Tightening this gap is an interesting direction of future work.

We note that bounded, positive random variables are sub-Poisson (Lemma 1). Hence, our results hold for linear bandit instances wherein the stochastic rewards are bounded and positive. This observation also highlights the fact that the current work is a generalization of the result obtained in . In addition, notice that, by definition, Poisson distributions are \(1\)-sub-Poisson. Hence, our guarantees further hold of rewards that are not necessarily sub-Gaussian. Given the recent interest in obtaining regret guarantees beyond sub-Gaussian rewards [18; 3], our study of sub-Poisson rewards is interesting in its own right.2

Our linear bandit algorithm, LinNash, has two parts. In the first part, we develop a novel approach of sampling arms such that in expectation the reward obtained is a linear function of the center of John Ellipsoid . Such a strategy ensures that the expected reward in any round of the first part is sufficiently large. The second part of LinNash runs in phases of exponentially increasing length. In each phase, we sample arms according to a distribution that is obtained as a solution of a concave optimization problem, known as D-optimal design. We construct confidence intervals at each phase and eliminate sub-optimal arms. A key novelty in our algorithm and analysis is the use of confidence widths that are estimate dependent. We define these widths considering multiplicative forms of concentration bounds and crucially utilize the sub-Poisson property of the rewards. The tail bounds we develop might be of independent interest.

### Other Related Work

There has been a recent surge in interest to achieve fairness guarantees in the context of multi-armed bandits; see, e.g., [14; 7; 20; 6; 12]. However, these works mostly consider fairness across arms and, in particular, impose fairness constraints that require each arm to be pulled a pre-specified fraction of times. By contrast, our work considers fairness across rounds.

_Alternative Regret Formulations_. In the current work, for the welfare computation, each agent \(t\)'s value is considered as the expected reward in round \(t\). One can formulate stronger notions of regret by, say, considering the expectation of the geometric mean of the rewards, rather than the geometric mean of the expectations. However, as discussed in , it is not possible to obtain non-trivial guarantees for such reformulations in general: every arm must be pulled at least once. Hence, if one considers the realized rewards (and not their expectations), even a single pull of a zero-reward arm will render the geometric mean zero.

## 2 Problem Formulation and Preliminaries

We will write \([m]\) to denote the set \(\{1,2,,m\}\). For a matrix \(\), let \(()\) to denote the determinant of \(\). For any discrete probability distribution \(\) with sample space \(\), write \(()\{x:_{X}\{X=x \}>0\}\) to denote the points for which the probability mass assigned by \(\) is positive. For a vector \(^{d}\) and a positive definite matrix \(^{d d}\), we will denote \(||a||_{}:= a}\). Finally, let \(:=\{x^{d}||x||_{2}=1\}\) be the \(d\)-dimensional unit ball.

We address the problem of stochastic linear bandits with a time horizon of \(_{+}\) rounds. Here, an online algorithm (decision maker) is given a set of arms \(^{d}\). Each arm corresponds to a \(d\)-dimensional vector. Furthermore, associated with each arm \(x\), we have a stochastic reward \(r_{x}_{+}\). In the linear bandits framework, the expected value of the reward \(r_{x}\) is modeled to be a linear function of \(x^{d}\). In particular, there exists an unknown parameter vector \(^{*}^{d}\) such that, for each \(x\), the associated reward's expected value \([r_{x}]= x,^{*}\). Given the focus on welfare contexts, we will, throughout, assume that the rewards are positive, \(r_{x}>0\), for all \(x\).

The online algorithm (possibly randomized) must sequentially select an arm \(X_{t}\) in each round \(t[]\) and, then, it observes the corresponding (stochastic) reward \(r_{X_{t}}>0\).3 For notational convenience,we will write \(r_{t}\) to denote \(r_{X_{t}}\). In particular, if in round \(t\) the selected arm \(X_{t}=x\), then the expected reward is \( x,^{*}\), i.e., \([r_{t} X_{t}=x]= x,^{*}\). We will, throughout, use \(x^{*}\) to denote the optimal arm, \(x^{*}=_{x} x,^{*}\) and \(\) to denote estimator of \(^{*}\).

In the stochastic linear bandits framework, our overarching objective is to minimize the Nash regret, defined as follows:

\[_{}_{x} x,^{*} -(_{t=1}^{}[ X_{t},^{*} ])^{1/}\] (1)

Note that the definition of Nash regret is obtained by applying the Nash social welfare (geometric mean) onto ex ante rewards, \([ X_{t},^{*}]\),4 accrued across the \(\) rounds.

### Sub-Poisson Rewards

In order to model the environment with positive rewards (\(r_{x}>0\)), we assume that the rewards \(r_{x}\) associated with the arms \(x\) are \(\)-_sub Poisson_, for some parameter \(>0\). Formally, their moment-generating function satisfies the following bound

\[[e^{\;r_{x}}](^{-1}[r_{ x}]\;(e^{}-1))=(^{-1} x,^{*} (e^{}-1)).\] (2)

Note that a Poisson random variable is \(1\)-sub Poisson. To highlight the generality of \(\)-sub-Poisson distributions, we note that bounded, non-negative random variables are sub-Poisson (Lemma 1). Further, in Lemma 2, we establish a connection between non-negative sub-Gaussian and sub-Poisson random variables.

**Lemma 1**.: _Any non-negative random variable \(X[0,]\) is \(\)-sub-Poisson, i.e., if mean \([X]=\), then for all \(\), we have \([e^{ X}](B^{-1}(e^{B}-1))\)._

**Lemma 2**.: _Let \(X\) be a non-negative sub-Gaussian random variable \(X\) with mean \(=[X]\) and sub-Gaussian norm \(\). Then, \(X\) is also \((}{})\)-sub-Poisson._

The proofs of Lemmas 1 and 2 appear in Appendix A. Lemma 2 has useful instantiations. In particular, the lemma implies that the half-normal random variable, with variance of \(\), is also a \((C)\)-sub-Poisson, where \(C\) is a constant (independent of distribution parameters). Similarly, for other well-studied, positive sub-Gaussian random variables (including truncated and folded normal distributions), the sub-Poisson parameter is small.

Next, we discuss the necessary preliminaries for our algorithm and analysis.

### Optimal Design.

Write \(()\) to denote the probability simplex associated with the set of arms \(\). Let \(()\) be such a probability distribution over the arms, with \(_{x}\) denoting the probability of selecting arm \(x\). The following optimization problem, defined over the set of arms \(\), is well-known and is referred to as the G-optimal design problem.

\[g()_{x}||x||_{( )^{-1}}^{2}()()=_{x} _{x}xx^{T}\] (3)

The solution to (3) provides the optimal sequence of arm pulls (for a given budget of rounds) to minimize the confidence width of the estimated rewards for all arms \(x\). The G-optimal design problem connects to the following optimization problem (known as D-optimal design problem):

\[f()(()) ()()=_{x} _{x}xx^{T}\] (4)

The lemma below provides an important result of Kiefer and Wolfowitz .

**Lemma 3** (Kiefer-Wolfowitz).: _If the set \(\) is compact and \(\) spans \(^{d}\), then there exists \(^{*}()\) supported over at most \(d(d+1)/2\) arms such that \(^{*}\) minimizes the objective in equation (3) with \(g(^{*})=d\). Furthermore, \(^{*}\) is also a maximizer of the D-optimal design objective, i.e., \(^{*}\) maximizes the function \(f()=(())\) subject to \(()\)._At several places in our algorithm, our goal is to find a probability distribution that minimizes the non-convex optimization problem (3). However, instead we will maximize the concave function \(f()=(())\) over \(()\). The Frank-Wolfe algorithm, for instance, can be used to solve the D-optimal design problem (4) and compute \(^{*}\) efficiently (, Chapter 21). Lemma 3 ensures that this approach works, since the G-optimal and the D-optimal design problems have the same optimal solution \(^{*}()\), which satisfies \((^{*}) d(d+1)/2\).5

### John Ellipsoid.

For any convex body \(K^{d}\), a John ellipsoid is an ellipsoid with maximal volume that can be inscribed within \(K\). It is known that \(K\) itself is contained within the John Ellipsoid dilated by a factor of \(d\). Formally,6

**Lemma 4** ().: _Let \(K^{d}\) be a convex body (i.e., a compact, convex set with a nonempty interior). Then, there exists an ellipsoid \(E\) (called the John ellipsoid) that satisfies \(E K c+d(E-c)\). Here, \(c^{d}\) denotes the center of \(E\) and \(c+d(E-c)\) refers to the (diated) set \(\{c+d(x-c):x E\}\)._

## 3 Our Algorithm LinNash and Main Results

In this section, we detail our algorithm LinNash (Algorithm 2), and establish an upper bound on the Nash regret achieved by this algorithm. Subsection 3.1 details Part I of LinNash and related analysis. Then, Subsection 3.2 presents and analyzes Part II of the algorithm. Using the lemmas from these two subsections, the regret bound for the algorithm is established in Subsection 3.3.

### Part I: Sampling via John Ellipsoid and Kiefer-Wolfowitz Optimal Design

As mentioned previously, Nash regret is a more challenging objective than average regret: if in any round \(t[]\), the expected7 reward \([r_{t}]\) is zero (or very close to zero), then geometric mean \((_{t=1}^{}[r_{X_{t}}])^{1/}\) goes to zero, even if the expected rewards in the remaining rounds are large. Hence, we need to ensure that in every round \(t[]\), specifically the rounds in the beginning of the algorithm, the expected rewards are bounded from below. In , this problem was tackled for stochastic multi-armed bandits (MAB) by directly sampling each arm uniformly at random in the initial rounds. Such a sampling ensured that, in each of those initial rounds, the expected reward is bounded from below by the average of the expected rewards. While such a uniform sampling strategy is reasonable for the MAB setting, it can be quite unsatisfactory in the current context of linear bandits. To see this, consider a linear bandit instance in which, all--except for one--arms in \(\) are orthogonal to \(^{*}\). Here, a uniform sampling strategy will lead to an expected reward of \( x^{*},^{*}/||\), which can be arbitrarily small for large cardinality \(\).

To resolve this issue we propose a novel approach in the initial \(} 3d(||)}\) rounds. In particular, we consider the convex hull of the set of arms \(\)--denoted as \(()\)-- and find the center \(c^{d}\) of the John ellipsoid \(E\) for the convex hull \(()\). Since \(E()\), the center \(c\) of the John ellipsoid is contained within \(()\) as well. Furthermore, via Caratheodory's theorem , we can conclude that the center \(c\) can be expressed as a convex combination of at most \((d+1)\) points in \(\). Specifically, there exists a size-\((d+1)\) subset \(:=\{y_{1},,y_{d+1}\}\) and convex coefficients \(_{1},,_{d+1}\) such that \(c=_{i=1}^{d+1}_{i}y_{i}\) with \(_{i=1}^{d+1}_{i}=1\). Therefore, the convex coefficients induce a distribution \(U()\) of support size \(d+1\) and with \(_{x U}[x]=c\).

```
0: Arm set \(\) and horizon of play \(T\).
1: Initialize matrix \(_{d,d}\) and number of rounds \(}=3d(||)}\). Part I
2: Generate arm sequence \(\) for the first \(}\) rounds using Algorithm 1.
3:for\(t=1\) to \(\)do
4: Pull the next arm \(X_{t}\) from the sequence \(\), observe corresponding reward \(r_{t}\), and update \(+X_{t}X_{t}^{T}\)
5:endfor
6: Set estimate \(:=^{-1}(_{t=1}^{}}r_ {t}X_{t})\)
7: Compute confidence bounds \((x,,}/3)\) and \((x,,}/3)\), for all \(x\) (see equation (7))
8: Set and initialize \(^{}=\)\(}\)
9:while end of time horizon \(\) is reached do
10: Initialize \(V=_{d,d}\) to be an all zeros \(d d\) matrix and \(s=_{d}\) to be an all-zeros vector. // Beginning of new phase.
11: Find the probability distribution \((})\) by maximizing the following objective \[((_{0}))_{0}( })(_{0}) d(d+1)/2.\] (6)
12:for each arm \(a\) in \(()\)do
13: Pull arm \(a\) for the next \(_{a}\)\(^{}\) rounds. Update \(+_{a}^{}  aa^{T}\).
14: Observe \(_{a}\)\(^{}\) corresponding rewards \(z_{1},z_{2},\) and update \(s s+(_{j}z_{j})a\).
15:endfor
16: Set estimate \(=^{-1}s\) and compute \((x,,^{})\) and \((x,,^{})\), for all \(x\) (see equation (7))
17: Set \(}=x}:(x, ,^{})_{z} (z,,^{})}\). // End of phase.
18: Update \(^{} 2\)\(^{}\).
19:endwhile ```

**Algorithm 2**LinNash (Nash Regret Algorithm for Finite Set of Arms)

Lemma 5 below asserts that sampling according to the distribution \(U\) leads to an expected reward that is sufficiently large. Hence, \(U\) is used in the subroutine \(\) (Algorithm 1).

In particular, the purpose of the subroutine is to carefully construct a sequence (multiset) of arms \(\), with size \(||=}\) and to be pulled in the initial \(}\) rounds. The sequence \(\) is constructed such that (i) upon pulling arms from \(\), we have a sufficiently large expected reward in each pull, and (ii) we obtain an initial estimate of the inner product of the unknown parameter vector \(^{*}\) with all arms in \(\). Here, objective (i) is achieved by considering the above-mentioned distribution \(U\). Now, towards the objective (ii), we compute distribution \(()\) by solving the optimization problem (also known as the D-optimal design problem) stated in equation (5).

We initialize sequence \(=\) and run the subroutine \(\) for \(}\) iterations. In each iteration (of the for-loop in Line 4), with probability \(1/2\), we sample an arm according to the distribution \(U\) (Line 7) and include it in \(\). Also, in each iteration, with remaining probability \(1/2\), we consider the computed distribution \(\) and, in particular, pick arms \(z\) from the support of \(\) in a round-robin manner. We include such arms \(z\) in \(\) while ensuring that, at the end of the subroutine,each such arm \(z()\) is included at least \(_{z}}/3\) times. We return the curated sequence of arms \(\) at the end of the subroutine.

Our main algorithm LinNash (Algorithm 2) first calls subroutine GenerateArmSequence to generated the sequence \(\). Then, the algorithm LinNash sequentially pulls the arms \(_{t}\) from \(\), for \(1 t}\) rounds. For these initial \(}=||\) rounds, let \(r_{t}\) denote the noisy, observed rewards. Using these \(}}\) observed rewards, the algorithm computes the ordinary least squares (OLS) estimate \(}\) (see Line 6 in Algorithm 2); in particular, \(:=(_{t=1}^{}}X_{t}X_{t}^{T})^{-1}( _{t=1}^{}}r_{t}X_{t})\). The algorithm uses the OLS estimate \(\) to eliminate several low rewarding arms (in Lines 7 and 8 in Algorithm 2). This concludes Part I of the algorithm LinNash.

Before detailing Part II (in Subsection 3.2), we provide a lemma to be used in the analysis of Part I of LinNash.

**Lemma 5**.: _Let \(c^{d}\) denote the center of a John ellipsoid for the convex hull \(()\) and let \(U()\) be a distribution that satisfies \(_{x U}\,x=c\). Then, it holds that_

\[_{x U}[ x,^{*}], ^{*}}{(d+1)}.\]

Proof.: Lemma 4 ensures that there exists a positive definite matrix \(\) with the property that

\[\{x^{d}:(x-c)} 1\} ()\{x^{d}: (x-c)} d\}.\]

Now, write \(y:=c--c}{d}\) and note that

\[(y-c)}=-c)^{T}(x^{*}-c)} {d^{2}}} 1\] (since

\[x^{*}()\]

)

Therefore, \(y()\). Recall that, for all arms \(x\), the associated reward (\(r_{x}\)) is non-negative and, hence, the rewards' expected value satisfies \( x,^{*} 0\). This inequality and the containment \(y()\) give us \( y,^{*} 0\). Substituting \(y=c--c}{d}\) in the last inequality leads to \( c,^{*} x^{*},^{*}/(d+1)\). Given that \(_{x U}\,[x]=c\), we obtain the desired inequality \(_{x U} x,^{*}= c,^{*} ,^{*}}{(d+1)}\). 

Note that at each iteration of the subroutine GenerateArmSequence, with probability \(1/2\), we insert an arm into \(\) that is sampled according to \(U\). Using this observation and Lemma 5, we obtain that, for any round \(t[}]\) and for the random arm \(X_{t}\) pulled from the sequence \(\) according to our procedure, the observed reward \(r_{X_{t}}\) must satisfy \([r_{X_{t}}],^{*}}{2(d+1)}\).8

Further, recall that in the subroutine GenerateArmSequence, we insert arms \(x()\) at least \(_{x}}/3\) times, where \(\) corresponds to the solution of D-optimal design problem defined in equation (5). Therefore, we can characterize the confidence widths of the estimated rewards for each arm in \(\) computed using the least squares estimate \(\) computed in Line 7 in Algorithm 2.

Broadly speaking, we can show that all arms with low expected reward (less than a threshold) also have an estimated reward at most twice the true reward. On the other hand, high rewarding arms must have an estimated reward to be within a factor of \(2\) of the true reward. Thus, based on certain high probability confidence bounds (equation (7)), we can eliminate arms in \(\) with true expected reward less than some threshold, with high probability.

### Part II: Phased Elimination via Estimate Dependent Confidence Widths

Note that while analyzing average regret via confidence bound algorithms, it is quite common to use, for each arm \(x\), a confidence width (interval) that does not depend on \(x\)'s estimated reward. This is a reasonable design choice for bounding average regret, since the regret incurred at each round is the sum of confidence intervals that grow smaller with the round index and, hence, this choice 

[MISSING_PAGE_FAIL:8]

**Lemma 8**.: _Consider any phase \(\) in Part II of Algorithm 2 and let \(}\) be the surviving set of arms at the beginning of that phase. Then, with \(=(||)}\), we have_

\[\{ x,^{*} x^{*},^{*}-25 ,^{*}(| |)}{2^{}}}}x} \} 1-}{}\] (10)

_Here, \(\) is the sub-Poisson parameter of the stochastic rewards._

The proofs of the Lemmas 7 and 8 are deferred to Appendix C.

### Main Result

This section states and provides a proof sketch of the main theorem.9

**Theorem 1**.: _For any given stochastic linear bandits problem with (finite) set of arms \(^{d}\), time horizon \(_{+}\), and \(\)-sub-Poisson rewards, Algorithm 2 achieves Nash regret_

\[_{}=O(}}( ||)).\]

_Here, \(=\{1,\; x^{*},^{*} d\}\), with \(x^{*}\) denoting the optimal arm and \(^{*}\) the (unknown) parameter vector._

Note that, in Theorem 1, lower the value of the optimal expected reward, \( x^{*},^{*}\), stronger is the Nash regret guarantee. In particular, with a standard normalization assumption that \( x^{*},^{*} 1\) and for \(1\)-sub Poisson rewards, we obtain a Nash regret of \(O(}}(||))\). Also, observe that the regret guarantee provided in Theorem 1 depends logarithmically on the size of \(\). Hence, the Nash regret is small even when \(||\) is polynomially large in \(d\).

Proof Sketch of Theorem 1:.: We condition on the event \(\) defined as the intersection of the (high probability) events defined in Lemmas 7 and 8, respectively. Union bound implies that event \(^{c}\) holds with probability at most \(O(^{-1})\). For Part I, in the first \(}=3d(||)}\) rounds, we bound from below the product of the expected rewards, using Lemma 5, as follows

\[_{t=1}^{}}[ X_{t},^{*} ]^{}(,^{*} }{2(d+1)})^{}}{t}} x^{*}, ^{*}^{}}{t}}(1-}(2(d+1))}{}).\]

For remaining rounds, we invoke Lemma 8 (specifically, equation (10)) to obtain

\[_{t=}+1}^{}[ X_{t}, ^{*}]^{} x^{*},^{*} ^{}{t}+}{t}}_{=1}^{ }(1-50}(2^{}/3)+2d^{2}|}{ }||)}{ x ^{*},^{*} 2^{}}}}).\]

The above equations reduce to the following bound on the geometric mean of the expected rewards (across the \(\) rounds):

\[_{t=1}^{}[ X_{t},^{*}]^{}_{t=1}^{}\!([ X_{t},^{*} ])^{}() x^{ *},^{*}(1-100 x^{*}, ^{*}}}(||)).\]

Therefore, the Nash regret of LinNash satisfies

\[_{}= x^{*},^{*}-(_{t=1}^{ }[ X_{t},^{*}])^{1/}=O (,^{*}}{}}( ||)).\]

This completes the proof sketch.

Computational Efficiency of LinNash.We note that Algorithm 2 (LinNash) executes in polynomial time. In particular, the algorithm calls the subroutine GenerateArmSequence in Part I for computing the John Ellipsoid. Given a set of arm vectors as input, this ellipsoid computation can be performed efficiently (see Chapter 3 in ). In fact, for our purposes an approximate version of the John Ellipsoid suffices, and such an approximation can be found much faster ; specifically, in time \(O(||^{2}d)\). Furthermore, the algorithm solves the D-optimal design problem, once in Part I and at most \(O()\) times in Part II. The D-optimal design is a concave maximization problem, which can be efficiently solved using, say, the Frank-Wolfe algorithm with rank-\(1\) updates. Each iteration takes \(O(||^{2})\) time, and the total number of iterations is at most \(O(d)\) (see, e.g., Chapter 21 of  and Chapter 3 in ). Overall, we get that LinNash is a polynomial-time algorithm.

## 4 Extension of Algorithm LinNash for Infinite Arms

The regret guarantee in Theorem 1 depends logarithmically on \(||\). Such a dependence makes the guarantee vacuous when the set of arms \(\) is infinitely large (or even \(||=(2^{d^{-1}}})\)). To resolve this limitation, we extend LinNash with a modified confidence width that depends only on the largest estimated reward \(_{x} x,\). Specifically, we consider the confidence width \(16\ \ }{}}\), for all the arms, and select the set of surviving arms in each phase (of Part II of the algorithm for infinite arms) as follows:

\[}=\{x: x, -16\ \ }}{^{}}}\}\] (11)

See Algorithm 3 in Appendix D for details. The theorem below is the main result of this section.

**Theorem 2**.: _For any given stochastic linear bandits problem with set of arms \(^{d}\), time horizon \(_{+}\), and \(\)-sub-Poisson rewards, Algorithm 2 achieves Nash regret_

\[_{}=O(}}{}}()),\]

_Here, \(=\{1,\  x^{*},^{*} d\}\), with \(x^{*}\) denoting the optimal arm and \(^{*}\) the (unknown) parameter vector._

Proof of Theorem 2 and a detailed regret analysis of Algorithm 3 can be found in Appendix D.

## 5 Conclusion and Future Work

Fairness and welfare considerations have emerged as a central design objectives in online decision-making contexts. Motivated, broadly, by such considerations, the current work addresses the notion of Nash regret in the linear bandits framework. We develop essentially tight Nash regret bounds for linear bandit instances with a finite number of arms.

In addition, we extend this guarantee to settings wherein the number of arms is infinite. Here, our regret bound scales as \(d^{5/4}\), where \(d\) is the ambient dimension. Note that, for linear bandits with infinite arms,  obtains a bound of \(d/}\) for average regret. We conjecture that a similar dependence should be possible for Nash regret as well and pose this strengthening as a relevant direction of future work. Another important direction would be to study Nash regret for more other bandit frameworks (such as contextual bandits and combinatorial bandits) and Markov Decision Processes (MDPs).