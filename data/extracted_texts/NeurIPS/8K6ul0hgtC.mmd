# How does PDE order affect the convergence of PINNs?

Changhoon Song\({}^{1}\)   Yesom Park\({}^{1,2}\)   Myungjoo Kang\({}^{3}\)

\({}^{1}\) Research Institute of Mathematics, Seoul National University

\({}^{2}\) Department of Mathematics, University of California, Los Angeles

\({}^{3}\) Department of Mathematical Sciences, Seoul National University

changhoon.song93@gmail.com, yeisom@math.ucla.edu, mkang@snu.ac.kr

Equal contribution authors. Correspondence to: <mkang@snu.ac.kr>.

###### Abstract

This paper analyzes the inverse relationship between the order of partial differential equations (PDEs) and the convergence of gradient descent in physics-informed neural networks (PINNs) with the power of ReLU activation. The integration of the PDE into a loss function endows PINNs with a distinctive feature to require computing derivatives of model up to the PDE order. Although it has been empirically observed that PINNs encounter difficulties in convergence when dealing with high-order or high-dimensional PDEs, a comprehensive theoretical understanding of this issue remains elusive. This paper offers theoretical support for this pathological behavior by demonstrating that the gradient flow converges in a lower probability when the PDE order is higher. In addition, we show that PINNs struggle to address high-dimensional problems because the influence of dimensionality on convergence is exacerbated with increasing PDE order. To address the pathology, we use the insights garnered to consider variable splitting that decomposes the high-order PDE into a system of lower-order PDEs. We prove that by reducing the differential order, the gradient flow of variable splitting is more likely to converge to the global optimum. Furthermore, we present numerical experiments in support of our theoretical claims.

## 1 Introduction

Understanding of partial differential equations (PDEs) is fundamental in describing diverse phenomena in science and engineering, including fluid dynamics , weather prediction , disease progression , and quantum mechanics . This underscores the imperative necessity for the effective acquisition of their solutions. Given that analytically solving PDEs is often infeasible or even impossible for numerous practical scenarios due to their complexity, numerical methodologies play a pivotal role in approximating solutions to PDEs, enabling researchers and engineers to address real-world problems effectively.

The advent of deep learning has led to a surge in attempts to leverage it to solve PDEs . Among these, physics-informed neural networks (PINNs)  stand out as a prominent methodology. Coupled with the automatic differentiation technique , they integrate the residuals of PDEs and boundary conditions into the loss function, thereby enforcing the approximation of solutions using artificial neural networks. This distinctive incorporation of PDEs into the loss function introduces partial differential operators in calculating the loss, distinguishing PINNs from conventional deep learning models. Renowned for their accessibility and versatility in being capable of easily handling arbitrary PDEs and being mesh-free, PINNs have garnered significant attention and demonstrated promising outcomes across various fields .

Despite their potential, PINNs frequently encounter difficulties in accurately approximating solutions, particularly when the governing PDE contains high-order derivatives [48; 33]. They also exhibit sensitivity to increasing dimensions . These challenges impede the practicality of PINNs due to the pervasiveness of high-order or high-dimensional PDEs in numerous physical and engineering descriptions, such as control problems [22; 57], finance [5; 53], phase separation [12; 27], and mechanical engineering [4; 32]. Several studies have indicated that neural network architectures possess sufficient expressive power to approximate solutions [31; 41]. However, it has been purported that the inferior performance may be attributed to the difficulty in optimizing PINNs, which arises from including the PDE in the loss function [37; 61; 62]. Despite the widespread use of PINNs, a rigorous mathematical understanding of these pathological behaviors of PINNs has been lacking.

In this paper, we endeavor to provide a mathematical understanding of the pathological behaviors of PINNs by analyzing the convergence of their gradient flow (GF), which reveals a profound sensitivity of the GF convergence with respect to the PDE order and the power of the activation. Building upon the work of Gao et al. , we extend the analysis of the GF of PINNs, composed of two-layer multilayer perceptrons (MLPs), to general \(k\)th-order PDEs and the \(p\)-th power of Rectified Linear Unit (ReLU) activation function with general \(p\). We achieve tighter bounds than those obtained by Gao et al., shedding light on the underlying causes of the pathological behaviors of PINNs. Our theoretical findings demonstrate that the width size of the network necessary for the convergence of the GF increases exponentially with the power \(p\) of ReLU\({}^{p}\)activation. Furthermore, our results indicate that the optimal power \(p\) is determined by the order \(k\) of the governing PDE, specifically to be \(k+1\). We also find that the PDE order impedes the convergence of GF, where this negative impact of the PDE order stems from incorporating the PDE into the PINN loss function, which necessitates network differentiation up to the order of the PDE. Moreover, our theoretical investigation unveils that the GF convergence of PINNs also deteriorates with increasing dimensions, and the differential operators included in the PINN loss further exacerbate the sensitivity of PINNs to dimensionality. This elucidates why PINNs are relatively sensitive to dimensionality compared to conventional deep learning models that do not involve differentiation in the loss function.

To address these challenges, we mathematically demonstrate the efficacy of a variable splitting strategy [54; 55; 6], which represents derivatives of the solution as additional auxiliary variables. The key point of variable splitting is that learning a high-order PDE boils down to learning a system of lower-order PDEs. Reducing the order of derivatives included in the loss function, the strategy alleviates the difficulties associated with the PDE order. It further enables to utilize more general ReLU\({}^{p}\)activation with lower power \(p\) than PINNs. The lower differential orders that the network computes, the more likely it is that the GF will converge, so the most suitable one among the various constructions of the variable splitting method is the finest splitting, which separates all the derivatives into auxiliary variables and reformulates the PDE into a system of first-order PDEs. This strategy results in a loss function comprising only first-order derivatives, and the efficacy of this finest variable splitting would be magnified as the order of the governing PDE or dimension increases. Therefore, the finest splitting approach would exhibit a pronounced discrepancy from the vanilla PINNs for high-order PDEs. Moreover, a reduction in the differential orders enhances the resilience of the model with respect to dimensionality. Finally, we present numerical experiments to verify our theoretical findings and validate the effectiveness of the variable splitting.

### Related Work

Characterization of Gradient Descent for PINNsAs significant issues have been identified within physics-informed machine learning, numerous mathematical studies have been conducted to elucidate the behavior of PINNs. While studies have been mainly dedicated to examining the generalization capacity of PINNs [19; 49; 23], there has also been work on understanding the difficulty of optimization, which is believed to be the primary source of failure for PINNs. Wang et al.  found that PINNs exhibit stiff gradient flow dynamics, resulting in imbalanced gradients during training. Ryck et al.  characterized the rate of convergence in terms of the conditioning of an operator and suggested that the difficulty of training PINNs is closely related to the conditioning of the differential operators in the governing PDEs. Another work  utilized the neural tangent kernel (NTK) theory to indicate that spectral biases and discrepancies between convergence rates of various loss components can lead to training instabilities. Global convergence properties of PINNs for second-order linear PDEs have also been studied within the NTK regime  and using the Rademacher complexity . Most closely related to this paper, Gao et al.  demonstrated the convergence of the gradient descent for two-layer PINNs. However, their discussion is limited to second-order linear PDEs. We extend the analysis to general \(k\)th-order linear PDEs and \(p\)-th power of activation functions and provide tighter bounds than Gao et al.. These advances allow us to observe further the effect of the order and dimensionality of the PDE on the convergence.

Variable SplittingThe method of separation of variables, which simplifies differential equations by reformulating them into a more manageable system, is a classical method for solving differential equations . In particular, it has been widely employed when dealing with high-order PDEs as augmenting high-order derivatives as additional variables allows the governing equation to be decoupled into a set of lower-order PDEs that are comparatively easy to solve [16; 26; 63]. Recent endeavors have explored the integration of separable variables within the PINN approach. In this paper, we refer to this approach as variable splitting according to [54; 55; 56]. Augmented variables have been introduced to represent vorticity in the Stokes equation , the gradient of the solutions for solving the \(p\)-Poisson equation , and the eikonal equation . Additionally, second-order derivatives have been separately parameterized to solve bi-harmonic equations effectively . The rationale for introducing auxiliary variables in previous works is to enhance the efficiency and accuracy of PINNs, but they lack a comprehensive theoretical elucidation of its effect. A recent study  has theoretically analyzed variable splitting, demonstrating that while PINNs do not guarantee convergence to the PDE solution even when the loss converges to zero, variable splitting does ensure convergence to the solution for second-order linear PDEs. In this study, we analyze the impact of variable splitting for PINNs with ReLU\({}^{p}\)activation in terms of the convergence of the GF.

### Main Contributions

The contribution of the paper is summarized as follows.

* We analyze that the GF of PINNs with ReLU\({}^{p}\)activation converges to the global minimum for general \(k\)th-order linear PDEs. This extends the findings of Gao et al.  to encompass a broader range of PDEs and activations and provides an even tighter bound.
* We demonstrate the inverse relation between PDE order and the GF convergence, unveiling the adverse effect of the differentials included in the PINN loss on the GF convergence.
* We provide a theoretical understanding of the reasons why PINNs encounter difficulties in addressing high-dimensional problems.
* We prove that the order reduction of variable splitting, which reformulates the PDEs into a system of lower-order PDEs, results in the convergence enhancement of GF.

## 2 Mathematical Setup

Arbitrary Order Linear PDEsWe consider a general form of _kth-order linear partial differential equations (PDEs)_ defined on a bounded domain \(^{d}\) (in which the temporal dimension could be a subcomponent)

\[[u]()=f( ),&,\\ [u]()=g(),& ,\] (1)

where \([u]=_{|| k}a_{}}{^{}}u\) is a \(k\)th-order linear differential operator with coefficient functions \(a_{}:\) for each multi-index \(_{0}^{d}\), \([u]=_{|| 1}_{}}{^{}}u\) represents the boundary condition operator with coefficient functions \(_{}:\), which could reflect Dirichlet, Neumann, and Robin conditions 2, \(f:\) is a given source function, and \(g:\) is a given boundary function, and \(u:\) is the unknown solution of interest.

Physics-InformedNeural NetworksPhysics-informed neural networks (PINNs)  aim to approximate the solution \(u\) of the PDE by neural networks. Following the prior work , we approximate the solution \(u\) by a two-layer multi-layer perceptron \(:^{d}\) of width \(m\), defined as

\[(;,)=}_{r=1}^{m}v_{r} (_{r}^{}),\] (2)

where \(_{r}^{d+2}\), \(v_{r}\), \(=[_{0}^{},_{m}^{}]^{} ^{m(d+2)}\)\(=[_{0}^{},_{m}^{}]^{}^{m}\), \(=[^{}]^{}^{d+1}\), and \(()\) is the activation function. For brief notations, we assume that \(\) is bounded so that \(\|\|_{2} 1\) for \(\). We consider the case where \(\) is the ReLU\({}^{p}\) activation function for an integer \(p\), which is also known as Rectified Power(RePU) activation . As it will be clear in the context, the power \(p\) necessitates surpassing the order \(k\) of the PDE (1) to ensure that the loss function and gradient descent flow are well-defined. Therefore, our analysis is focused on scenarios where \(p k+1\). PINNs learn the parameters of \(\) by minimizing a composite loss function, comprising the residual of the PDE and the boundary condition of (1), which enforces the network's compliance with the governing physics. For given the training data \(\{_{i},f(_{i})\}_{i=1}^{n_{a}} \) and \(\{}_{j},g(}_{j})\}_{j=1}^{n_{b}} \) of respective sizes \(n_{o}\) and \(n_{b}\), PINN loss function is given by

\[_{PINN}(,)=(\| (,)\|^{2}+\|(,) \|^{2}),\] (3)

where \((,)=[s_{1}(,)  s_{n_{o}}(,)]^{}\) and \((,)=[h_{1}(,)  h_{n_{b}}(,)]^{}\) with

\[s_{i}(,) =}}([(;,)](_{i})-f(_{i})),\] (4) \[h_{j}(,) =}}([(; ,)](}_{j})-g(}_{j})),\] (5)

and \(>0\) is a regularization parameter that relatively balances the two components of the loss.

Gradient FlowAs the limiting dynamics of the gradient descent (GD) with infinitesimal step-sizes , gradient flow (GF) is continuous time dynamics that starts at \((0)\) and \((0)\) and evolves as

\[_{r}(t)}{dt}&=- _{PINN}(,)}{_{r}}=-_{i=1}^{n_{o}}s_{i} (,)(,) }{_{r}}-_{j=1}^{n_{b}}h_{j}(,) (,)}{_{r}},\\ (t)}{dt}&=-_{PINN}(, )}{ v_{r}}=-_{i=1}^{n_{o}}s_{i}(, )(,)}{ v_{r}}- _{j=1}^{n_{b}}h_{j}(,) (,)}{ v_{r}}.\] (6)

Initial weights are supposed to follow the normal and uniform distributions, \((0) N(0,I_{m})\) and \( U(\{-1,1\})\), respectively3. GF can be regarded as a continuous-time analog of GD and is frequently employed to comprehend the behavior of GD optimization algorithm in the limit. By the chain rule in conjunction with (6), the following characterizes how the loss function evolves during training by gradient descent:

\[((t),(t ))\\ ((t),(t))=-( _{}((t),(t))+_{ }((t),(t))) ((t),(t))\\ ((t),(t)),\] (7)

where \(\) and \(}\) are Gram matrices for the dynamics, defined by

\[_{}(,)=_{}^{}_{}, \,_{}=}{}( ,)}}{} (,)}{}(,)}}{} (,)\] (8)

\[_{}(,)=_{}^{}_{}, \,_{}=}{}( ,)}}{} (,)}{}( ,)}}{} (,).\] (9)

We are interested in analyzing the effect of the PDE order on the convergence of the PINN loss, which evolved in accordance with the dynamics (7), to the global minimum zero.

Impact of PDE Order on Convergence of PINNs

Despite the demonstrated promise and versatility of PINNs in addressing a wide range of problems , they often encounter difficulties in constructing an accurate approximation to the desired solution of PDEs, particularly with high-order PDEs. Moreover, in contrast to the confirmed efficacy of neural networks in modeling high-dimensional data such as images and text, the exploration of PINNs for high-dimensional PDEs has been apparently limited. While neural network architectures possess sufficient expressive power to approximate solutions , inferior performance has been attributed to the difficulty in optimization in practice . Additionally, it has been postulated that the optimization difficulty may stem from the partial differential operators included in the loss function . Nevertheless, despite the significant challenge posed by these pathological phenomena, there remains a paucity of theoretical understanding of them.

In this section, we theoretically elucidate these pathological phenomena by studying the convergence condition of GF (7) of PINN loss (3). Specifically, we provide a width condition for (7) to converge to global optimum in terms of order \(k\), dimension \(d\), and the power \(p\) of ReLU activation. Analyzing how those factors are related to the convergence condition, we explain why optimizing PINNs is harder when the order or degree is higher.

Following  and , we first prove the positive definiteness of the limiting Gram matrix of PINNs for general \(k\)th-order linear PDE and \(p\) without any further strict assumption other than \(p>k\).

**Proposition 3.1** (Special Case).: _The limiting Gram matrix \(_{}^{}=_{,}[_{}( ,)]\) is strictly positive definite and independent of \(m\)._

This is a special case of the general state in Proposition C.3 with \(L=0\), and the proof for the general case is provided in Appendix C. We denote the smallest eigenvalue of \(_{}^{}\) by \(_{0}>0\). The following presents our main theorem in this section, the requisite width size \(m\) for the GF of PINN loss to converge to the global minimum with high probability. The result demonstrates that the required width grows exponentially as the PDE order \(k\) and the dimension of the domain \(d\) increase.

**Theorem 3.2** (Special Case).: _There exists a constant \(C\), independent of \(d\), \(k\), and \(p\), such that for any \(<\!\!<1\), if_

\[m>C^{14}p^{7k+4}2^{6p}()^{4p}\] (10)

_then with probability of at least \(1-\) over the initialization, we have_

\[_{PINN}((t),(t)) (-_{0}t)_{PINN}((0), (0)),\; t 0.\] (11)

It is a special case of Theorem 4.3 with \(L=0\), the proof of which can be found in Appendix C.1. It extends, inspired by , the convergence of the GF of PINNs of second-order linear PDEs to \(k\)th-order linear PDEs and the general \(p\)-th power of ReLU. It states that even in these general settings, the GF of PINNs converges to the global minimum with a high probability when the width of the network is sufficiently large. Moreover, we obtain a polylogarithmic bound \(()^{4p}\), which is much tighter than polynomial bound \(^{-3}\) in  for \(p=3\). These improvements permit the derivation of the following valuable explanations for the deficiencies observed when optimizing PINNs.

Optimal Power of ReLU Function in TrainingTheorem 3.2 sheds light on the suitable choice of activation function for PINNs. In the training process, the activation function plays an important role. However, there are no clues as to which activation function is favorable to the given optimization process. Especially in the case of PINNs, it depends heavily on the PDE at hand. Despite its pervasive use in deep learning due to its numerous advantages and performance benefits, the ReLU activation function is not admissible in the PINN framework, which necessitates the activation function to provide high-order derivatives for optimizing PDE-based constraints. Instead, PINNs harness the \(p\)-th power of ReLU as the activation function. It is apparent that \(p\) must satisfy \(p k+1\) for the PINN loss and gradient descent to be computed. Theorem 3.2 indicates that the smaller \(p\) is, the more likely the gradient descent will converge; that is, it is most optimal4 to adjust \(p\) to \(k+1\) regarding the training process.

Understanding Difficulty in High-order PDEsA significant observation of Theorem 3.2 is that it provides a theoretical understanding of why PINNs struggle with high-order PDEs. From (10), we can see that the bound increases exponentially with the order of the PDE. Moreover, for the GF of PINNs to converge with high probability, that is, \( 1\), a small increment of the power \(p\) would contribute to non-negligible degradation in the convergence, which could ultimately prevent the network from reaching a minimizer of the loss. Hence, given that \(k\) determines the admissible \(p\) by \(p k+1\), the order \(k\) of PDE primarily influences the convergence of PINNs and increasing the exponential term in (10).

Understanding Difficulty in High-dimensional ProblemsThe above theorem, which shows that the lower bound of \(m\) depends on the exponential of \(d\), explains why PINNs cannot completely combat the curse of dimensionality. As PINNs are regarded as a versatile method capable of being mesh-free, they have been expected to be free from the curse of dimensionality . However, the GF of PINNs becomes harder to converge as \(d\) increases, requiring the network to be wider. Furthermore, it can be observed that the magnitude of change in \(d\) is amplified with respect to the exponent of \(k\). This explains why PINNs are relatively sensitive to increasing dimensionality in comparison to other deep learning models whose loss functions do not contain derivatives. In other words, the presence of derivatives in the loss makes PINNs sensitive to changes in dimensionality, and the larger \(k\) is, the more difficult PINNs are for high-dimensionality.

Combining all crucial observations from our main theorem, we believe that the impact of the PDE order is one of the primary underlying reasons why PINNs often fail to minimize their loss. In light of this theoretical evidence, the next section describes a variable splitting strategy that addresses these pathologies by properly reducing the differential order in the PINN loss function.

## 4 Order Reduction through Variable Splitting

The previous section indicates that the PDE order \(k\) significantly affects the width requirement for the GF to converge. Concurrently, for \(k\)th-order PDEs, it is necessary to increase the ReLU activation to at least the \(k+1\) power in order to ensure a well-defined GF for the PINN loss. Consequently, lowering \(k\) could potentially lead to better convergence of the GF. In this section, we introduce variable splitting strategy to decrease the differential order by reformulating the given PDE into a system of lower-order PDEs. We then extend Theorem 3.2 to a more general form in Theorem 4.3.

### Variable Splitting

The concept of variable splitting [54; 55; 56] is to rewrite a higher-order PDE into a lower-order system, after which the PINN approach is applied to the system. A crucial aspect of the success of such methods is the reduction of the derivative order present in the training loss function.

Augment VariablesFor \(L 0\) and increasing integers \(0=_{0}<_{1}<<_{L+1}=k\), variable splitting augment the derivatives of the solution \(}}{ x^{_{1}}}u,,}}{ x^{_{L}}}u\) as additional auxiliary variables \(_{1},,_{L}\), respectively. For notational simplicity, we abbreviate the integer set \(\{1,,m\}\) for a positive integer \(m\) by \([m]\). For \([L]\), each term in \(}}{ x^{_{1}}}\) corresponds to \(}}{ x^{_{1}}_{1} x^{ _{d}}_{d}}u\) for a multi-index \(=(_{1},,_{d})_{0}^{d}\) with the size \(||=_{i=1}^{d}_{i}=n\). Therefore, \(_{}\) is a vector-valued function of size \(|I_{_{}}|\) for the index set \(I\) defined in (18). We denote the component of \(_{}\) that corresponds to \(}}{ x^{_{i}}}\) by \((_{})_{}\).

Reformulate PDE into Lower-order SystemBy replacing each of the differential terms \(}}{ x^{_{}}}u\) with the corresponding auxiliary variables \(_{}\), the differential operator \(\) in (1) can be rewritten as:

\[[u]=_{|| k}a_{}}{ x^{}}u=_{=0}^{L}_{||_{}} _{||_{+1}}_{,,}}}{ x^{}}(_{})_{},\] (12)for some coefficient functions \(_{,,}:\) and \(_{}=_{}-_{-1}\). Since \(_{}\) represents a function that differentiates the PDE solution \(_{}\)-times more than \(_{-1}\), the components of two consecutive variables \(_{-1}\) and \(_{}\) are governed by

\[}}{^{}}( _{-1})_{}()=(_{})_{ +}(),\; I_{_{-1}},\; I_{ _{}}.\] (13)

From these, the PDE (1) can be identically reformulated by the system of lower-order PDEs:

\[}[_{0},,_{L}]()=f(),&,\\ }{^{}}(_{-1})_{ }()=(_{})_{+}(),&,\;[L]\,,\; I_{_{-1}},\;  I_{_{}},\\ [_{0}]()=g,& .\] (14)

It is of paramount importance to note that the maximum differential order of this system of PDEs is the highest difference of derivative order between consecutive auxiliary variables \(||=\{_{}:[L+1]\}\), which is less than \(k\). This aspect gives rise to notable ramifications in our analysis of VS-PINNs, which will be discussed in the next subsection.

Variable Splitting for PINNsIn this paper, we consider the parameterization of all variables \(_{}\) with two-layer MLPs with ReLU\({}^{p}\) activation function, in a manner analogous to that described in Section 2 for PINNs. The weights in the first and second layers of \(_{}\) are denoted by \(_{}\) and \(_{}\), respectively.We use \(=[_{1}^{}_{L}^{}]^{}\) and \(=[_{1}^{}_{L}^{}]^{}\) to refer to the respective collections of all weights. Similar to PINNs, _Variable Splitting for PINNs (VS-PINNs)_ employ the linear sum of penalized residuals of each term of the induced system of PDEs (14) as the training loss:

\[_{PINN}^{VS}(,)=& (}_{i=1}^{n_{o}}(}[_{0},,_{L}](_{i})-f (_{i}))^{2}.\\ &+.}{n_{o}}_{=1}^{L}_{| |}_{||_{+1}}( }{^{}}(_{-1})_{ }(_{i})-(_{})_{+}( _{i}))^{2}\\ &+.}([_{0}] (}_{j})-g(}_{j}))^{2} ),\] (15)

where \(,_{1},,_{L}\) are regularization parameters. As the GF of \(_{PINN}\) is characterized by Gram matrices \(_{}\) and \(_{}\) induced from the gradients of the residuals of each term in (1), the GF of \(_{PINN}^{VS}\) is characterized by Gram matrices \(}_{}\) and \(}_{}\), which is induced from the gradients of the residuals of each term in (14). Appendix A gives more details for \(}_{}\) and \(}_{}\).

_Remark 4.1_.: In order for high-order PDEs with \(k>2\) to be well-posed, it is necessary to have more boundary conditions than those defined by the boundary operator \(\) in (1). Although our analysis concentrated on \(\) that reflect only up to first-order derivatives for the sake of simplicity, our theory can also be applied to more general boundary conditions. Furthermore, the high-order boundary conditions \(\) can also be reformulated using the auxiliary variables used for \(}\). In that case, relations (13) should hold on the boundary \(\). As the reduced system (14) with reformulated boundary condition is equivalent to (1), instability issues were not observed in our numerical experiments even in the absence of artificial boundary conditions on the auxiliary variables unlike to grid-based conventional numerical schemes.

### Analysis

A key advantage of VS-PINNs is that the derivative order of the induced system of PDEs (14) is \(||\), which is lower than that of the original PDE (1). We prove its effectiveness in this section. As analogous to PINNs, we begin by proving the positive definiteness of the limiting Gram matrix, providing its proof in Appendix C.

**Proposition 4.2** (General Case).: _The limiting Gram matrix \(}_{}^{}=_{,}[}_{}(,)]\) is strictly positive definite and independent of \(m\)._

We denote the smallest eigenvalue of \(}_{}^{}\) by \(_{0}>0\). We now present our main theorem, which demonstrates the profound impact of order reduction in variable splitting. The proof of the following theorem can be found in C.1.

**Theorem 4.3** (General Case).: _There exists a constant \(C\), independent of \(d\), \(k\), \(||\), and \(p\), such that for any \(<\!\!<1\), if_

\[m>C^{6}^{8}p^{7||+4}2^{6p}()^{4p},\] (16)

_then with probability of at least \(1-\) over the initialization, we have_

\[_{PINN}^{VS}((t), (t))(-_{0}t)_{PINN}^{VS} ((0),(0)),\;  t 0.\] (17)

The right-hand-side of (16) grows exponentially with respect to \(d\), \(k\), and \(p\), thereby indicating the substantial influence of these factors on the convergence of VS-PINNs, including PINNs as a specific case (\(L=0\)). This analysis reveals several significant advantages of VS-PINNs:

Improved Convergence:VS-PINNs are more likely to converge to the global optimum than PINNs due to the reduction in the derivative order \(||<k\). This also relaxes the condition on \(p\) from \(p k+1\) to \(p||+1\). As previously discussed in Section 3, the optimal value of \(p\) is \(||+1\). Given that \(6(||+1)\) is an exponent of \((1/)\), the most dominant term, reducing the order from \(k\) to \(||\) leads to an immense improvement. There is another noteworthy observation we can see here. Given a \(k\)th-order PDE, there are numerous possible partitions \(\) that could be employed to decompose it to a system of lower-order PDEs. Consequently, there are a many of potential VS-PINNs that could be constructed. The aforementioned result indicates which of these is the most effective. As the convergence improves dramatically with a reduction in the derivative order, the optimal approach for splitting variables among various ways is to separate the given PDE into a system of first-order PDEs by parameterizing all derivatives of the solution as auxiliary variables. In other words, the finest splitting with \(_{0}=0\), \(_{1}=1\),..., \(_{k-1}=k-1\) would be the most effective in terms of the convergence of GF, as the differential order \(||\) is reduced the most to 1. Taken all together, the most optimal VS-PINNs that reduce the PDE order \(k\) to \(1\) will markedly enhance the convergence of GF.

Reduced Dimensional Impact:The reduction of orders in VS-PINNs enhances the resilience of the model to high-dimensionality. From Theorem 3.2, we observed the effect of \(d\) being exponentially enlarged for the PDE order \(k\) due to the \(k\)th-order partial differential operators in the loss function. It can be alleviated by VS-PINNs reducing the order, thereby easing the amplified scale to exponential of \(||\). This indicates that VS-PINNs are more effective in combating the curse of dimensionality. Since the curse of dimensionality is a serious issue that is prevalent in various fields, including Hamilton-Jacobi-Bellman equation in control problems, Schrodinger equation in quantum physics, and Black-Scholes equation in finance, it is evident that enhancements to the robustness of VS-PINNs with respect to their dimensionality would facilitate considerable advancements in various fields.

Memory Efficiency:VS-PINNs are memory-efficient despite the presence of multiple auxiliary networks. As the order of the derivative increases, the complexity in automatic differentiation in modern deep-learning frameworks like PyTorch increases and it becomes computationally expensive . Adopting the order-reduced representation in the proposed variable splitting can overcome the difficulty in calculating the high-order derivative via automatic differentiation. The loss function for the finest VS-PINNs involves only first-order derivatives, which reduces the memory usage and computational requirements. Despite the increase in the number of networks, VS-PINNs exhibit greater efficiency because memory usage and computation scale linearly with the number of networks in contrast to the exponential scaling with the order of derivatives. Table 3 in appendix demonstrates the memory reduction of VS-PINNs.

_Remark 4.4_.: The current approach to parameterizing the \(_{}\)-th order differential operator on all axes \(^{_{}}\) as an auxiliary variable may be suboptimal in certain cases. In a given PDE, if the order of the derivative varies significantly along the axes, that is, \(a_{} 0\) for only a few \(\) in (1), it may be more efficient to approximate the partial derivatives using auxiliary variables separately for each axis. To illustrate, for the PDE \(u_{tt}=u_{xxxx}\), it is more suitable to parameterize variables \(_{0} u\), \(_{1}(u_{t},u_{x})\), \(_{2} u_{xx}\), and \(_{3} u_{xxx}\), rather than approximating all tensors \(^{1}_{(t,x)}\), \(^{2}_{(t,x)}\), and \(^{3}_{(t,x)}\). The theoretical framework presented in this paper is capable of addressing this scenario by constructing each \(_{}\) to replace \(t}{^{}}\) for only part of \(\) with \(||=_{}\). However, we exclude it due to the intricate nature of the states and the lack of a meaningful impact on the PDE order.

_Remark 4.5_.: Although the sharpness of the bound in Theorem 4.3 is open, it is important to note that the leading term of the bound is based on conditions necessary for the Gram matrix to be positive definite, which is a crucial property of the Gram matrix for ensuring the convergence of the GF to a global optimizer. Since the Gram matrix is defined by the PDE loss and the network structure, we believe it can still provide valuable insight into how order and power affect convergence.

## 5 Experiments

This section presents experimental results that validate the theory. Throughout numerical experiments, two-layer MLPs with ReLU\({}^{p}\)activation function were utilized in order to align with our theoretical framework. Throughout all experiments, the training collocation points consists of uniform grid and regularization parameters are set to \(_{1},,_{L}=1\) and \(=10\). We implement all numerical experiments on a single NVIDIA RTX 3090 GPU. Experimental details are provided in Appendix D.

Convergence behavior of PINNsTo investigate the influence of the activation order \(p\) and the PDE order \(k\) on the width \(m\) required for convergence, we examined both the second-order Poisson equation and the fourth-order bi-harmonic equation, both of which yield the same solution. We trained networks with varying widths m, ranging from \(10^{2}\) to \(10^{6}\), for each combination of \(p\) and \(k\) using GD optimization with a learning rate of \(10^{-8}\). Figure 1 illustrates the training losses at the initial stage on a logarithmic scale, supporting our theoretical findings that a larger width is needed for higher values of \(p\) to ensure convergence. Moreover, we can observe that narrower networks tend to converge more readily when solving lower-order PDEs (Poisson) compared to higher-order PDEs (bi-harmonic). This observation aligns with Theorem 4.3 that higher-order PDEs necessitate larger network widths for guaranteed convergence.

Validation on the effect of \(p\)To verify the influence of the power \(p\) of the ReLU activation function, we test PINNs with varying \(p\) values between \(3\) and \(10\). Since the training process became highly unstable as \(p\) increases, we consider second-order heat equation (509)  to gain a more precise investigation of the effect of \(p\). The results are summarized in Figure 2 (a). We can see that the convergence of loss is enhanced as \(p\) decreases, which supports our theoretical finding.

Comparison between PINNs and VS-PINNsTo validate the order reduction effect of VS-PINNs, we conducted an experiment comparing PINNs with VS-PINNs on the second-order heat equation. Each model was run five times with different random seeds, and Figure 2 depicts the training loss for both PINNs and VS-PINNs along with their variance. The results show that the training loss for VS-PINNs converges more effectively than that of PINNs. This indicates that VS-PINNs, which optimize a loss function incorporating lower-order derivatives using networks with smaller \(p\), facilitate convergence of GD, consistent with the theoretical findings in Section 4. Furthermore, we performed a similar experiment on the convection-diffusion equation (511) in the Appendix E and obtained results that were consistent with those observed for the heat equation.

Effect of splitting levelFor higher-order PDEs, there are several ways to transform a given PDE into a lower-order system through variable splitting. To investigate this effect, we conducted experiments on the fourth-order elastic beam equation (510)  with two cases: (i) \(_{0} u,_{1} u_{t},_{2} u_{xx}\)

Figure 1: Training losses of PINNs solving (a) bi-harmonic equation and (b) Poisson equation.

with \(||=2\) and \(p=3\) and (ii) the finest splitting of \(_{0} u,_{1} u,_{2} u_{xx},_{3} u _{xxx}\) with \(||=1\) and \(p=2\). In order to train PINNs for a fourth-order PDE, \(p\) should be at least five, but training such PINNs with GD does not proceed properly, as illustrated in Figure 1. Consequently, the experiments were conducted using the Adam optimizer. In contrast to the underperforming PINNs, VS-PINNs are effectively trained even with GD, as illustrated in Figure 6 of the Appendix E. We run each model five times with different random seeds, and Figure 2 (b) depicts the training loss of PINN and two VS-PINNs with variance. The results show that the model with a lower PDE order \(k\) and a smaller power \(p\) of the activation exhibits a more pronounced reduction in the loss function, in accordance with our theoretical findings. Furthermore, it can be observed that the variance of the training loss is significantly smaller for the models with smaller values of \(k\) and \(p\). This indicates that the learning process is much more stable for a smaller \(k\) and \(p\). We also conduct numerical studies on the fourth-order bi-harmonic equation (508). However, the results exhibit a similar trend to that observed in the beam equation and are therefore presented in Appendix E.

## 6 Conclusion

In this paper, we proved that the gradient flow of PINNs converges to a global minimum and provides sufficient width for this convergence. It extends the results in  to general PDEs and activation functions and provides even tighter conditions on the width size. The main theorem demonstrates that the PDE order or dimension exponentially increases the width requirement, theoretically indicating that PINNs are challenging to optimize for high-order or high-dimensional PDEs. We also substantiate that the PDE order amplifies the adverse effects of dimensionality, which explains why PINNs are more susceptible to dimensionality than other deep learning losses without differentiation. Furthermore, We showed that the variable splitting strategy improves convergence by reducing the differential order included in the training loss function.

It is acknowledged that we only provided sufficient conditions for convergence. To fully comprehend the role of these factors in optimizing PINNs, it is also necessary to establish the necessary conditions linking PDE order, dimension, and convergence. Given that the primary goal of PINNs is to approximate the solution of PDEs, it could also be a limitation that all discussions were limited to empirical losses with fixed collocation points. It would therefore be a worthwhile future direction to analyze the conditions under which the expected loss converges when training collocation points are randomly sampled per epoch. Extending our theoretical framework to analyze the impact of the variable splitting strategy on the generalization error of PINNs, as suggested in , would also be an interesting and important research direction. Moreover, as our analysis was confined to continuous time flows, a comprehensive understanding of gradient descent would necessitate the analysis of discretized flows, since GF and GD have different dynamics . We expect that our theory could be adapted to GD dynamics by using Theorem 3.3 of , which treats GD as GF with a counter term, but we leave it for future work. In a practical context, the convergence of PINNs for adaptive optimizers, such as Adam  or L-BFGS , and other activation functions, including hyperbolic tangent, remains an open question.

Figure 2: Loss curves of (a) effect of the power \(p\) of ReLU\({}^{p}\)and (b) comparison between PINNs with VS-PINNs.

Acknowledgements

This work was supported by the NRF grant [2021R1A2C3010887, RS-2024-00406127, RS-2024-00343226, RS-2024-00421203] and MSIT/IITP[NO.2021-0-01343Artificial Intelligence Graduate School Program(SNU)].