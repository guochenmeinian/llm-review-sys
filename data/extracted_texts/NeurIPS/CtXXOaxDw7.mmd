# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

GNN explainer aims to identify an explanatory subgraph, whose components exert vital influence on the prediction of the full graph . Existing GNN explanation methods generally allocate significance scores for each component of the graph, and a subgraph with the highest score is extracted as the explanation of the whole graph . They optimize the allocation of the significance scores by minimizing an explanation loss function . The final explanation provided by GNN explainer includes a subgraph structure along with the corresponding node features.

However, a major limitation of existing works is that they mostly focus on designing effective significance score allocation methods, but ignore such a critical problem: _Whether and to what extent a structurally corrupted graph will affect the performance of the GNN explainer?_ In many real application scenarios, the graphs may contain various corruptions, such as noise  and adversarial edges . Existing methods directly adopt the raw graph features  or the latent graph representations  as the input of the explainer model, and thus the output explanation may not be reliable when the input is corrupted. Previous work has shown that the explanation of deep neural networks is fragile to attacked images . Thus whether the structurally corrupted graphs will remarkably affect the performance of the GNN explainer and how to address it is an interesting problem, yet has not been fully studied and remains an open research issue.

This paper for the first time investigates how to construct a robust GNN explainer for structurally corrupted graphs. As illustrated in Figure 1, based on whether the prediction of the downstream task (e.g., graph classification) is changed, the structural corruptions of a graph can be categorized into minor corruptions and severe corruptions. In Figure 1(a), \(\) denotes the GNN model to be explained, and \(G\) is the raw graph. Figure 1(b) shows the case of the graph with minor corruption \(G^{}\), which is not so intense that the predicted graph label remains the same as the raw graph. Figure 1(c) shows the case of the severely corrupted graph \(G^{}\) where the corruption is devastating enough to the GNN and thus results in a different prediction. Based on this example, we argue that a robust GNN explainer should satisfy the following two requirements. First, the minor corruption should be considered as redundant components and filtered by the explainer as they do not affect the final prediction. Second, the severe corruption should be identified by the explainer as part of the explanation since they are the non-ignorable reason that changes the final prediction. However, it is non-trivial to meet the two requirements simultaneously due to the following two challenges.

**Robust graph representation extraction to eliminate the minor corruptions**. As we discussed before, existing works directly use the raw graph features or the latent graph representations of each single graph as the explanation input, which will raise the risk of the GNN explainer overfitting to corruptions . This is because the raw graph features can be easily corrupted by noise or adversarial attacked edges. The latent graph representations have been proved vulnerable to structural corruption . How to extract a robust graph representation that can effectively filter the minor corruptions while preserving the vital information for GNN explanation exploration is challenging.

**Structural irregularity of the severe corruptions**. The corruptions may be of different sizes or consist of several disconnected parts in the graphs, i.e., they are irregular . For example, in molecule chemistry, the corruptive functional groups differ largely in terms of size (e.g., -Cl and

Figure 1: An illustration of the structurally corrupted graphs, where the solid red lines represent the newly added edges and the dashed red lines represent the removed edges. (a) is the raw graph and \(\) is the prediction given by GNN \(\); (b) represents the graph \(G^{}\) with minor corruption whose prediction remains \(\); (c) shows the graph \(G^{}\) is severely corrupted such that the predicted label of GNN on it is changed.

-C\({}_{10}\)H\({}_{9}\) in solubility analysis). Moreover, a molecule containing multiple functional groups which are disconnected from each other is common in the real-world. To meet the second requirement, the structural irregularity of severe corruption should be identified by the GNN explainer. Hence it is not reasonable to adopt some predefined rigorous constraints, e.g., size constraint or connectivity constraint that are commonly used in previous explainers, to the explanations of corrupted graphs. How to design a more general objective function to adaptively capture the structural irregularity of severe corruption is also challenging.

To address the two aforementioned challenges, we propose a robust GNN explainer for structurally corrupted graphs called V-InfoR (Variational Graph Representation based Information bottleneck Robust explainer). Specifically, V-InfoR contains two major modules. (1) **Robust graph representations extractor**. In this module, we take insights of variational inference [29; 30] to model the statistical characteristics that are shared by all the observed graphs, i.e., the mean and the standard deviation. The statistics are capable of capturing the uncertainty and variability of the observed graphs. Based on the statistics, a variational graph distribution is induced, which can effectively model the common features of all the graphs and filter the minor corruption. The variational graph representations provide a more robust graph representation for the downstream explanation generator. (2) **Adaptive explanation generator**. To address the irregularity issue of the severe corruptions, we propose an Adaptive Graph Information (AGI) constraint. The AGI constraint directly restricts the information carried by the explanation without any rigorous assumptions on the structural properties (e.g., size or connectivity) of the explanation. By incorporating the AGI constraint, we formulate the GNN explanation exploration as a Graph Information Bottleneck (GIB) optimization problem to adaptively capture both the regularity and irregularity of the severely corrupted graphs.

Our main contributions are summarized as follows:

* This work for the first time studies the negative effect of both minor and severe structural corruptions on existing GNN explainers, and proposes a robust explainer V-InfoR to effectively handle the two types of corruption.
* We novelly propose to incorporate variational inference to explore GNN explanation. A variational inference based robust graph information extractor is proposed to mitigate the uncertainty and variability of minor corruptions when extracting the critical graph information.
* We generalize the GNN explanation exploration by introducing an adaptive graph information constraint, which can capture both the regularity and irregularity of the corrupted graphs. We also theoretically derive the variational bound of this objective to make it feasibly optimizable.
* Extensive experiments on one synthetic dataset and three real-world datasets demonstrate the superiority of V-InfoR over state-of-the-art GNN explanation methods.

## 2 Preliminary and Problem Statement

In this section, we will first provide the quantitative evidence that existing GNNs explainers are fragile to structurally corrupted graphs. Then we provide a formal statement of the graph explanation problem. See Appendix B for the basic notations.

### Are existing GNN explainers robust to corruptions?

To study whether existing GNN explainers are robust to corruptions, we evaluate the performance of six state-of-the-art GNN explainers when minor corruption and severe corruption are injected into the raw graphs, respectively. We use the synthetic BA-3Motifs dataset , which is a widely used benchmark dataset for GNN explanation. To investigate the minor corruption, we select the corrupted graphs whose predictions are unchanged after randomly flipping 20% edges. For the severe corruption, we use the SOTA graph adversarial attack algorithm GRABNEL, which formulates the attack task as a Bayesian optimization problem . The attack budget of GRABNEL means the highest percentage of the attacked edges. We use a 10% attack budget here, which is able to change the prediction of the selected graphs and thus can be considered as the severe corruption. \(F_{NS}\) score is used as the evaluation metric and a higher \(F_{NS}\) score means a better explanation performance . The formal definition will be given later in Section 4.1.

The result is presented in Figure 5 of Appendix C. The histograms of the three colors represent the explainer performance on raw graphs, graphs with random noise, and attacked graphs, respectively. One can see that both the minor corruption (20% random structural noise) and the severe corruption (10% GRABNEL attack) remarkably degrade the explanation performance. Minor corruption brings about an average performance degradation of 31.15% for the six GNN explainers, and severe corruption results in an average performance degradation of 40.64%. _The experimental result verifies that both the minor and the severe corruptions do significantly affect the performance of existing GNN explainers._ Therefore, a robust GNN explainer is required for the structurally corrupted graphs.

### Problem formulation

Formally, given the GNN model \(\) to be explained and the input graph \(G=(,)\), the explanation provided by the GNN explainer is a subgraph \(G_{S}=(},})\). \(}\) is a subset of the original adjacent matrix \(\) and contains the key structure that determines prediction largely and \(}\) is the features of nodes which associates with \(}\). Existing researches show that topological information is critically important to graph classification [27; 13]. Hence, our V-InfoR focuses on the structural corruption when exploring explanation. We deem an explanation \(G_{S}\)_sufficient_ when it can produce the same prediction as using the full graph, and deem it _necessary_ when the prediction will change if it is removed from the full graph . An ideal explanation should be both sufficient and necessary. Formally, the sufficient and necessary conditions  are formulated as follows, respectively,

\[*{argmax}_{c}P_{}(c|},})=. \]

\[*{argmax}_{c}P_{}(c|-}, -}), \]

where \(=*{argmax}_{c}P_{}(c|,)\) in Formula (1) and (2).

## 3 Methodology

Figure 2 illustrates the overall architecture of V-InfoR, which is composed of two key modules, the robust graph representation extractor and the adaptive explanation generator. As shown in the upper part of Figure 2, the robust graph representation extractor takes the full graph \(G\) as input, and two GCN encoders are adopted to infer the statistics of \(G\)'s latent distribution (i.e., mean and standard deviation). The variational node representations, which are sampled from the latent graph distribution, will induce the edge representations. The edge representations are then fed into the adaptive explanation generator which is shown in the lower part of the figure. A multi-layer perceptron

Figure 2: The overall architecture of V-InfoR, which consists of the robust graph representation extractor and the adaptive explanation generator.

projects the edge representations into the probability of each edge. The explanation subgraph \(G_{S}\) is generated on the basis of this probability. Finally, the proposed novel Graph Information Bottleneck (GIB) objective is optimized, which is defined over the full graph prediction \(\) and the explanation subgraph prediction \(_{S}\). Next, we will introduce the two modules in detail.

### Robust graph representation extractor

Specifically, we introduce a variational auto-encoder (VAE)  to deduce the statistics in the variational distribution of the minorly corrupted graphs. For a set of observed graphs \(=\{G^{k}=(^{k},^{k})\}_{k=1}^{||}\), variational inference assumes that they are generated by a latent random process, involving a latent continuous random variable \(\)[29; 30]. This random process contains two steps. First, a value \(^{k}\) is generated from a prior distribution \(p()\). Then the graph \(G^{k}\) is generated from a conditional distribution \(p(|)\) and observed by us. Variational inference aims at identifying the posterior distribution \(p(|)\), i.e., inverting the distribution of latent variable \(\) from the observed graphs 4. However, the true posterior distribution \(p(|)\) is unknown. A feasible method is introducing a variational distribution \(q(;)\) to approximate \(p(|)\). This work assumes that \(q(;)\) is a Gaussian distribution \((,^{2})\) and increases the similarity between \(q(;)\) and \(p(|)\). The similarity measurement is Kullback-Leibler divergence,

\[[q(;)||p(|)]=_{^{k}}q(^{k};)^{k};)}{p( ^{k}|)}. \]

We substitute the unknown \(p(|)\) for \(p(,)/p()\). Finally, the objective function of the robust graph representation extractor to be minimized can be formulated as

\[L_{}=-_{q(;)}[ p(| )]+[q(;)||p()], \]

where \(p()\) is the standard Gaussian distribution \((,)\). See Appendix A for detailed derivation.

We refer to the latent variable \(\) as a variational representation because it contains the indicative information of the observed graphs. We also refer to the model that computes the distribution \(q(;)\) as an _encoder_, since given a graph \(G\), it produces a distribution over the latent variable \(\), from which \(G\) could have been generated. Similarly, \(p(|)\) is referred to as a _decoder_, since given a code \(\) it produces the probability of the possible graph \(G\).

In our implementation, we employ a 3-layer GCN as the encoder and a simple inner product as the decoder. The first two layers of GCN aim to aggregate and combine the information from the graph. The third layer consists of two separate graph convolution operators to calculate the mean \(\) and standard deviation \(\) of \(q(;)\), respectively. This procedure can be formulated as

\[^{k}=(^{k-1},),k=1,2,^{0} =, \]

\[=_{}(^{2},), \]

\[=_{}(^{2},). \]

The variational inference procedure endows \(q(;)\) vital graph information and is insensitive to minor corruptions. Hence the sampled graph representation \(\) which follows \(q(;)\) is more robust. Next, the robust representation \(\) will be fed into the explanation generator for explanation exploration.

### Adaptive explanation generator

As the second challenge stated before, severe corruptions should be identified as part of the explanation, but they are usually irregular in terms of size, connectivity or some other structural properties. The rigorous constraints of the structural regularity adopted in previous works [15; 13; 25] are thus not feasible for the severe corruptions. Hence, a new GNN explanation exploration objective function is required to adaptively capture the irregularity of the severe corruptions. Inspired by the Graph Information Bottleneck (GIB) principle [31; 32], we propose to introduce an Adaptive Graph Information (AGI) constraint in exploring GNN explanation. On the one hand, the AGI constraint functions as a bottleneck to directly restrict the information carried by the explanation \(G_{}\), instead of simply restricting the size or connectivity of \(G_{S}\). Without any predefined structural regularity constraints, our method can more effectively capture the irregularity of the explanation caused by the severe corruptions. On the other hand, since the information measurement of the explanation \(G_{S}\) can be continuously optimized, the AGI constraint can adaptively cover the discrete rigorous constraints . To this end, we formulate the GNN explanation problem as a GIB-guided optimization task to adaptively generate the explanations.

The insight of the GIB-guided optimization is that the explanation \(G_{S}\) of graph \(G\) should contain the _minimal sufficient_ components of \(G\). GIB principle facilitates \(G_{S}\) to be informative enough about the prediction \(\) of \(G\) (_sufficient_). GIB principle also inhibits \(G_{S}\) from preserving redundant components which is irrelevant for predicting \(G\) (_minimal_). To this aim, the GIB-guided optimization task of \(G_{S}\) is formally defined as follows,

\[_{G_{S} G}(G,;G_{S})=-(,G_{S}) +(G,G_{S}), \]

where \((,)\) denotes the mutual information. The second term \((G,G_{S})\) measures the vital graph information carried by the explanation \(G_{S}\), which functions as the AGI constraint. Nevertheless, the GIB-guided explanation exploration task in Formula (8) cannot be directly extended to a continuous optimization procedure, because both \(G\) and \(G_{S}\) have discrete topological information which is difficult to optimize over. We resort to Gilbert random graph theory  which argues that an arbitrary graph \(G\) can be represented as a random graph variable, and each edge of \(G\) is associated with a binary random variable \(r\) to reveal its existence. Additionally, the existence of one edge is conditionally independent of the other edges. \(r_{ij}=1\) means there is an edge \((i,j)\) from \(v_{i}\) to \(v_{j}\), otherwise \(r_{ij}=0\). To sum up, an arbitrary graph \(G\) can be represented as

\[p(G)=_{(i,j)}p(r_{ij}). \]

For the binary variable \(r_{ij}\), a common instantiation is the Bernoulli distribution \(r_{ij}(_{ij})\), where \(_{ij}=p(r_{ij}=1)\) is the probability of edge \((i,j)\) existing in \(G\). However, the Bernoulli distribution cannot be directly optimized. To address this issue, we apply categorical reparameterization  to the Bernoulli variable \(r_{ij}\). The continuous relaxation of \(r_{ij}\) can be formulated as

\[r_{ij}=}{ },(0,1), \]

where we let the latent parameter \(_{ij}=}{1-_{ij}}\). \(\) controls the approximation between the relaxed distribution and \((_{ij})\). When \(\) approaches 0, the limitation of Formula (10) is \((_{ij})\).

According to Formula (10), the Bernoulli parameter \(_{ij}\) is indeed associated with parameter \(_{ij}\). In our implementation, we use a multi-layer perceptron (MLP) to compute \(\). The MLP takes the variational node representation \(\) as input and concatenates the representations of two nodes \(v_{i},v_{j}\) as the representation of the corresponding edge \((i,j)\), which can be formulated as

\[_{ij}=[(_{i},_{j}]), \]

where \([,]\) is the concatenation operator.

Based on \(\) and Formula (10), we obtain the probability matrix \(}\) whose elements indicate the existence of the corresponding edges. Next, we can sample the explanation \(G_{S}\) based on the probabilities in the matrix \(}\) as follows,

\[G_{S}=(},}=}). \]

So far, we have derived the optimizable representation of \(G_{S}\) in Formula (8). However, the optimization is still challenging since the distributions \(p(|G_{S})\) and \(p(G_{S})\) are intractable.

Fortunately, following the variational approximation proposed in , we can derive a tractable variational upper bound of GIB in Formula (8). For the first term \(-(,G_{S})\), a parameterized variational approximation \(p_{}(|G_{S})\) is introduced for \(p(|G_{S})\) to get the upper bound as follows,

\[-(,G_{S})-_{p(G_{S},)} p_{ }(|G_{S})+H(), \]

where \(p_{}(|G_{S})\) is the GNN model and \(H()\) is an entropy independent of \(G_{S}\). For the second term, we introduce \(q(G_{S})\) for the marginal distribution \(P(G_{S})\), and the upper bound is

\[(G_{S},G)_{p(G)}(p_{}(G_{S}|G )||q(G_{S})), \]where \(p_{}(G_{S}|G)\) represents the explanation generator and \(q(G_{S})\) represents the prior distribution sharing a similar spirit of assuming standard Gaussian prior . See Appendix A for the detailed derivation. Finally, we obtain the variational upper bound of the GIB Formula (8) as follows,

\[L_{}= -_{p(G_{S},)} p_{}(|G_{S}) +_{p(G)}(p_{}(G_{S}|G)||q(G_{S}) ), \]

where \(q(G_{S})\) is usually set as

\[q(G_{S})=C_{i,j=1}^{N}p_{}(e_{ij}),e_{ij}(), \]

where \(C\) is a constant that decided by the hyper-parameter \(\).

Note that there are no rigorous constraints imposed in Formula (15). The second term actually functions as an adaptive constraint that inhibits \(G_{S}\) from containing useless information. The risk of ignoring the irregular severe corruptions in the final explanation \(G_{S}\) can also be largely mitigated.

To achieve a robust GNN explainer for all the corruption situations, the two proposed modules are trained jointly by minimizing the following overall objective function,

\[L_{}=L_{}+L_{}. \]

## 4 Experiment

### Experimental setup

Following the standard procedure of GNN explanation [15; 2], we conduct two steps in the experiments: (1) training a base GNN model for prediction, and (2) generating explanations for this base model. For the first step, the base GNN model is trained to simulate the third-party predictor which is independent of the GNN explainers. We take the trained GNN model as an oracle during the explanation generation, i.e., input the graph (no matter clean or corrupted) and get the prediction. In the second step, we simulate the corruptions by introducing random noise or adversarial attack to the graph structures. The random noise represents the corruptions that naturally exist in real-world which affects each graph component without any distinction. The noise ratio controls the percentage of randomly flipped edges. The adversarial attack represents the man-made malicious corruptions and the attack budget is the highest percentage of attacked edges.

**Metric.** For a quantitative explanation evaluation, we report the probability of sufficient \(P_{S}\), the probability of necessary \(P_{N}\) and the \(F_{NS}\) scores. See Appendix D.1 for the detailed definition.

**Dataset.** We evaluate the proposed V-InfoR and baseline explanation methods on one synthetic dataset and three real-world datasets. The synthetic dataset is BA-3Motifs introduced in . Three real-world datasets are Mutag, Ogbg-molhiv and Ogbg-ppa.

**Baseline.** The comparable baseline explainers include gradients-based methods GradCAM  and IG , surrogate method PGM-Explainer , and perturbation-based methods GNNExplainer , PGExplainer  and ReFine .

The detailed descriptions of datasets, baselines, and base GNN models are given in Appendix D. The ablation study is presented in Appendix E. We also report the visualized cases of GNN explanation for qualitative analysis in Appendix F.

### Quantitative Evaluation on Graphs with Random Noise

We report the result on the graphs with 20% random structural noise corruptions in Table 1. Specifically, we randomly select the edges according to the noise ratio and flip the selected ones.

The result shows that V-InfoR is able to improve the explainer performance on the four datasets, with the overall metric \(F_{NS}\) score improvement at least by 9.19% for the BA-3Motifs dataset and the highest improvement by 29.23% for the Mutag dataset. The V-InfoR improves the performance remarkably for Mutag by 29.23% and Ogbg-molhiv datasets by 15.65%, since the irregularity of the corruptions in chemical molecule structure is more obvious than the other types of graphs (motifs in BA-3Motifs and biologically associations in Ogbg-ppa), and V-InfoR is able to effectively capture the irregularity. For the BA-3Motifs and Ogbg-ppa datasets with less obvious irregularity, V-InfoR can still achieve a performance improvement, since the information constraint is general enough to cover the rigorous constraint and thus avoid performance degradation.

One can also see that the raw features based explainers, including GradCAM, IG, GNNExplainer, and PGM-Explainer, perform poorly in small/medium-scale datasets (BA-3Mmotifs, Mutag and Ogbg-molhiv). This may be because the raw features are more easily to be corrupted in small/medium-scale graphs. Since the graph representation is more difficult to learn in the large-scale graph, PGExplainer and ReFine that relic on latent graph representations are unable to well explain large-scale graphs for the Ogbg-ppa dataset. V-InfoR adopts the robust graph representation which is sampled from a variational distribution containing common information shared by large amounts of graphs, and thus achieves an overall better performance. Furthermore, one can note that all baseline GNN explainers achieve a high \(P_{S}\), while the \(P_{N}\) is relatively low. This phenomenon implies that the sufficient condition is easier to satisfy than the necessary condition.

As shown in Figure 3, we also evaluate V-InfoR and baselines under different noise ratios ranging from 0 to 0.30, which reverals the tendency of explainer performance with the increase of noise ratio. In real application scenarios, it is intractable to separate the minor and the severe corruptions, and both may exist simultaneously. Different noise ratios indicate different mixing ratios of the two corruptions. Note that \(F_{NS}\) scores with zero noise ratio represent the result on raw graphs without any corruption. It shows that V-InfoR still achieves the best performance when no noise is added (noise ratio = 0). This reveals that even without any corruption, the robust representation extractor still extracts vital common graph information and the explanation generator adaptively identifies the explanations with regular structural properties.

   Dataset & Metric & GradCAM & IG & GNNExplainer & PGExplainer & PGM-Explainer & ReFine & V-InfoR & Rank & Impro. \\   & \(P_{S}\) & 0.82725 & 0.8625 & 0.8535 & 0.8510 & 0.8505 & 0.8300 & **0.8820** & 1 & 1.09\% \\  & \(P_{N}\) & 0.2605 & 0.2795 & 0.2410 & 0.2095 & 0.2235 & 0.2625 & **0.3021** & 1 & 8.09\% \\  & \(F_{NS}\) & 0.4012 & 0.4232 & 0.3758 & 0.3362 & 0.3540 & 0.3989 & **0.4610** & 1 & 9.19\% \\   & \(P_{S}\) & 0.8760 & 0.8880 & 0.8916 & 0.8640 & 0.8900 & 0.8900 & **0.8964** & 1 & 0.54\% \\  & \(P_{N}\) & 0.0996 & 0.1068 & 0.1080 & 0.1260 & 0.1020 & 0.1260 & **0.1696** & 1 & 34.60\% \\  & \(F_{NS}\) & 0.1789 & 0.1907 & 0.1920 & 0.2199 & 0.1830 & 0.2207 & **0.2852** & 1 & 29.23\% \\   & \(P_{S}\) & 0.9230 & 0.9200 & 0.8925 & 0.8390 & 0.8860 & 0.9105 & **0.9386** & 1 & 1.69\% \\  & \(P_{N}\) & 0.0680 & 0.0400 & 0.0940 & 0.1265 & 0.0980 & 0.1020 & **0.1470** & 1 & 16.21\% \\  & \(F_{NS}\) & 0.1267 & 0.0767 & 0.1701 & 0.2198 & 0.1765 & 0.1834 & **0.2542** & 1 & 15.65\% \\   & \(P_{S}\) & 0.4340 & 0.5820 & 0.6616 & 0.6260 & 0.6192 & 0.6344 & **0.6700** & 1 & 1.27\% \\  & \(P_{N}\) & 0.4720 & 0.4600 & 0.3480 & 0.2856 & 0.3780 & 0.4406 & **0.4930** & 1 & 4.45\% \\  & \(F_{NS}\) & 0.4522 & 0.5139 & 0.4561 & 0.3922 & 0.4694 & 0.5200 & **0.5680** & 1 & 9.23\% \\   

Table 1: The comparison of V-InfoR and baselines under random structural noise. We use bold font to mark the highest score. The second highest score is marked with underline. The Impro. is defined as \(()/[)\).

Figure 3: The comparison of V-InfoR and six baselines under different noise ratios in (a) BA-3Motifs and (b) Mutag.

### Quantitative Evaluation on Graphs with Attack Corruption

The adversarial attack corresponds to the malicious corruption painstakingly customized by the attacker. Here we use the SOTA graph classification attack algorithm GRABNEL , whose attack budget is the highest percentage of attacked edges. GRABNEL employs a surrogate model to learn the mapping from an attacked graph to its attack loss and optimizes the attacked graph iteratively via an adapted genetic algorithm until successful attack or budget exhaustion. Considering the overhead of executing GRABNEL, we run the attack algorithm only on the BA-3Motifs and Mutag datasets. We report the results with 5% and 10% attack budgets in Table 2.

The result reveals that InfoR achieves an overall superior performance over the other baselines in terms of all metrics. As the attack budget grows from 5% to 10%, though the GNN explanation task becomes harder (all metrics decrease), the performance improvement of V-InfoR becomes more significant than baselines. This shows that V-InfoR can effectively mitigate the negative influence of the structural corruptions. For the Mutag dataset, the performance improvement on the graphs with GRABNEL attack (13.96%) is less significant than that on the graphs with random noise (29.23%). This gap can be attributed to the difference between adversarial attack and random noise. Although the adversarial attack corruption is devastating to change the model prediction, they are subtle and introduce as less irregularity as possible to the raw graphs.

### Hyper-parameter analysis

We further analyse the effect of three parameters on the model performance, \(\), \(\) and \(\). \(\) controls the approximation degree of \(r_{ij}\) distribution to Bernoulli distribution, which ranges in \([0.1,0.5]\). \(\) controls the balance between the strength of information restoring (i.e., \(-(,G_{S})\)) and the strength of information filtering (i.e., \((G_{S},G)\)). \(\) represents the prior Bernoulli probability, which controls the distribution of \(q(G_{S})\).

Figure 4 shows the influence of the three hyper-parameters on V-InfoR for the BA-3Motifs and Mutag datasets. One can roughly achieve the following three conclusions. First, V-InfoR is not so sensitive to \(\) that controls the strength of AGI constraint, which verifies the adaptability of our proposed constraint. Second, a suitable value of \(\) is around 0.3, which means the best balance between the continuity of Formula (10) and the approximation degree is achieved when \(=0.3\). Third, there is no obvious pattern shown in Figure 4(c) for the choice of \(\) in Formula (16), which means that a reasonable \(\) may largely depend on the specific dataset.

## 5 Related Work

Early attempts to explain GNN simply transfer gradients-based methods to graph data, and they regard the gradients of nodes and edges as significance scores . Though efficient and intuitive, the explanations based on gradients are unavoidably suffered from gradients saturation . Surrogate methods [37; 23] are also adopted in GNN explanations. Yet limited by simple models that function as surrogates, these approaches are unable to capture topological structure which plays an important role in GNN predictions [13; 27]. Currently, the most concerned GNN explanation methods are perturbation-based. By intervening components in the graph, such as de

   Attack budget & Dataset & Metric & GradCAM & IG & GNNExplainer & PGExplainer & PGM-Explainer & Refine & V-InfoR & Rank & Impro. \\   &  & \(P_{S}\) & 0.6980 & 0.6925 & 0.5625 & 0.6225 & 0.5950 & 0.6700 & **0.7075** & 1 & 1.36\% \\  & & \(P_{S}\) & 0.3625 & 0.4675 & 0.4200 & 0.3700 & 0.3925 & 0.3925 & **0.5450** & 1 & 16.58\% \\  & & \(F_{NS}\) & 0.4772 &(or nodes), and monitoring the change of the corresponding prediction, perturbation-based GNN explanation methods [15; 13; 24; 38] optimize the significance scores matrix round after round.

As a classic perturbation-based GNN explanation method, GNNExplainer  determines the significant components by maximizing the mutual information between the intervented and the original graphs. It calculates significance scores for both node features and edges, and the components whose scores are below-threshold will be removed. PGExplainer  introduces a parameterized neural network to provide significance scores. It demands a training procedure to endow the internal neural network with multi-categorical predicting behavior, and the trained PGExplainer can explain any new graph without retraining. While the previous methods aim to preserve the components that make the prediction invariant, CF-GNNExplainer aims to find the components that will make the prediction change if they are missing . XGNN  formulates the GNN explanation problem as a reinforcement learning task. Starting with an empty graph, XGNN gradually adds components until the generated graph belongs to the specified class.

## 6 Conclusion

In this paper, we propose a robust GNN explainer V-InfoR for the structurally corrupted graphs. V-InfoR employs the variational inference to learn the robust graph representations and generalizes the GNN explanation exploration to a graph information bottleneck (GIB) optimization task without any predefined rigorous constraints. The robust graph representations are insensitive to minor corruptions when extract the common information shared by the observed graphs. By introducing an adaptive graph information constraint, V-InfoR can effectively capture both the regularity and irregularity of the explanation subgraphs. Extensive experiments demonstrate its superior performance and the ablation study further illustrates the effectiveness of two proposed modules.