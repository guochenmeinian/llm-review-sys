# When Does Confidence-Based Cascade Deferral

Suffic?

 Wittawat Jitkrittum   Neha Gupta   Aditya Krishna Menon

Harikrishna Narasimhan   Ankit Singh Rawat   Sanjiv Kumar

Google Research, New York

{wittawat, nehagup, adityakmenon, hnarasimhan, ankitsrawat, sanjivk}@google.com

###### Abstract

Cascades are a classical strategy to enable inference cost to vary _adaptively_ across samples, wherein a sequence of classifiers are invoked in turn. A _deferral rule_ determines whether to invoke the next classifier in the sequence, or to terminate prediction. One simple deferral rule employs the _confidence_ of the current classifier, e.g., based on the maximum predicted softmax probability. Despite being oblivious to the structure of the cascade -- e.g., not modelling the errors of downstream models -- such confidence-based deferral often works remarkably well in practice. In this paper, we seek to better understand the conditions under which confidence-based deferral may fail, and when alternate deferral strategies can perform better. We first present a theoretical characterisation of the optimal deferral rule, which precisely characterises settings under which confidence-based deferral may suffer. We then study _post-hoc_ deferral mechanisms, and demonstrate they can significantly improve upon confidence-based deferral in settings where (i) downstream models are _specialists_ that only work well on a subset of inputs, (ii) samples are subject to label noise, and (iii) there is distribution shift between the train and test set.

## 1 Introduction

Large neural models with several billions of parameters have shown considerable promise in challenging real-world problems, such as language modelling  and image classification . While the quality gains of these models are impressive, they are typically accompanied with a sharp increase in inference time , thus limiting their applicability. _Cascades_ offer one strategy to mitigate this , by allowing for faster predictions on "easy" samples. In a nutshell, cascades involve arranging multiple models in a sequence of increasing complexities. For any test input, one iteratively applies the following recipe, starting with the first model in the sequence: execute the current model, and employ a _deferral rule_ to determine whether to invoke the next model, or terminate with the current model's prediction. One may further combine cascades with ensembles to significantly improve accuracy-compute trade-offs .

A key ingredient of cascades is the choice of deferral rule. The simplest candidate is to defer when the current model's _confidence_ in its prediction is sufficiently low. Popular confidence measures include the maximum predictive probability over all classes , and the entropy of the predictive distribution . Despite being oblivious to the nature of the cascade -- e.g., not modelling the errors of downstream models -- such _confidence-based deferral_ works remarkably well in practice . Indeed, it has often been noted that such deferral can perform on-par withmore complex modifications to the model training [23; 21; 36]. However, the reasons for this success remain unclear; further, it is not clear if there are specific practical settings where confidence-based deferral may perform poorly .

In this paper, we initiate a systematic study of the potential limitations of confidence-based deferral for cascades. Our findings and contributions are:

1. We establish a novel result characterising the theoretically optimal deferral rule (Proposition 3.1), which for a two-model cascade relies on the confidence of both model 1 and model 2.
2. In many regular classification tasks where model 2 gives a consistent estimate of the true posterior probability, confidence-based deferral is highly competitive. However, we show that in some settings, confidence-based deferral can be significantly sub-optimal both in theory and practice (SS4.1). This includes when (1) model 2's error probability is highly non-uniform across samples, which can happen when model 2 is a _specialist_ model, (2) labels are subject to noise, and (3) there is distribution shift between the train and test set.
3. Motivated by this, we then study a series of _post-hoc_ deferral rules, that seek to mimic the form of the optimal deferral rule (SS4.2). We show that post-hoc deferral can significantly improve upon confidence-based deferral in the aforementioned settings.

To the best of our knowledge, this is the first work that precisely identifies specific practical problems settings where confidence-based deferral can be sub-optimal for cascades. Our findings give insights on when it is appropriate to deploy a confidence-based cascade model in practice.

## 2 Background and Related Work

Fix an instance space \(\) and label space \(=[L]\{1,2,,L\}\), and let \(\) be a distribution over \(\). Given a training sample \(S=\{(x_{n},y_{n})\}_{n[N]}\) drawn from \(\), multi-class classification seeks a _classifier_\(h\) with low _misclassification error_\(R(h)=(y h(x))\). We may parameterise \(h\) as \(h(x)=*{argmax}_{y^{}}f_{y^{}}(x)\) for a _scorer_\(f^{L}\). In neural models, one expresses \(f_{y}(x)=w_{y}^{}(x)\) for class weights \(w_{y}\) and embedding function \(\). It is common to construct a _probability estimator_\(p^{L}\) using the softmax transformation, \(p_{y}(x)(f_{y}(x))\).

### Cascades and Deferral Rules

Conventional neural models involve a fixed inference cost for any test sample \(x\). For large models, this cost may prove prohibitive. This has motivated several approaches to _uniformly_ lower the inference cost for _all_ samples, such as architecture modification [41; 76; 31; 78], stochastic depth [30; 19], network sparsification , quantisation , pruning , and distillation [5; 29; 61]. A complementary strategy is to _adaptively_ lower the inference cost for _"easy"_ samples, while reserving the full cost only for "hard" samples [26; 86; 66; 55; 72; 81; 31; 68; 83; 17; 45; 82].

Cascade models are a classic example of adaptive predictors, which have proven useful in vision tasks such as object detection , and have grown increasingly popular in natural language processing [47; 75; 38; 14]. In the vision literature, cascades are often designed for binary classification problems, with lower-level classifiers being used to quickly identify negative samples [4; 84; 63; 10; 13; 70]. A cascade is composed of two components:

1. a collection of base models (typically of non-decreasing inference cost)
2. a _deferral rule_ (i.e., a function that decides which model to use for each \(x\)).

For any test input, one executes the first model, and employs the deferral rule to determine whether to terminate with the current model's prediction, or to invoke the next model; this procedure is repeated until one terminates, or reaches the final model in the sequence. Compared to using a single model, the goal of the cascade is to offer comparable predictive accuracy, but lower average inference cost.

While the base models and deferral rules may be trained jointly [74; 36], we focus on a setting where the base models are _pre-trained and fixed_, and the goal is to train only the deferral rule. This setting is practically relevant, as it is often desirable to re-use powerful models that involve expensive training procedures. Indeed, a salient feature of cascades is their ability to leverage off-the-shelf models and simply adjust the desired operating point (e.g., the rate at which we call a large model).

### Confidence-based Cascades

A simple way to define a deferral rule is by thresholding a model's _confidence_ in its prediction. While there are several means of quantifying and improving such confidence , we focus on the _maximum predictive probability_\((x)_{y^{}}p(y^{} x)\). Specifically, given \(K\) trained models, a confidence-based cascade is formed by picking the first model that whose confidence \((x)\) is sufficiently high . This is made precise in Algorithm 1.

Forming a cascade in this manner is appealing because it does not require retraining of any models or the deferral rule. Such a post-hoc approach has been shown to give good trade-off between accuracy and inference cost . Indeed, it has often been noted that such deferral can perform on-par with more complex modifications to the model training . However, the reasons for this phenomenon are not well-understood; further, it is unclear if there are practical settings where confidence-based cascades are expected to underperform.

To address this, we now formally analyse confidence-based cascades, and explicate their limitations.

## 3 Optimal Deferral Rules for Cascades

In this section, we derive the oracle (Bayes-optimal) deferral rule, which will allow us to understand the effectiveness and limitations of confidence-based deferral (Algorithm 1).

### Optimisation Objective

For simplicity, we consider a cascade of \(K=2\) pre-trained classifiers \(h^{(1)},h^{(2)}\); our analysis readily generalises to cascades with \(K>2\) models (see Appendix F). Suppose that the classifiers are based on probability estimators \(p^{(1)},p^{(2)}\), where for \(i\{1,2\}\), \(p^{(i)}_{y^{}}(x)\) estimates the probability for class \(y^{}\) given \(x\), and \(h^{(i)}(x)_{y^{}}p^{(i)}_{y^{}}(x)\). We do not impose any restrictions on the training procedure or performance of these classifiers. We seek to learn a deferral rule \(r\{0,1\}\) that can decide whether \(h^{(1)}\) should be used (with \(r(x)=0\)), or \(h^{(2)}\) (with \(r(x)=1\)).

**Constrained formulation**. To derive the optimal deferral rule, we must first specify a target metric to optimise. Recall that cascades offer a suitable balance between average inference cost, and predictive accuracy. Let us assume without loss of generality that \(h^{(2)}\) has higher computational cost, and that invoking \(h^{(2)}\) incurs a constant cost \(c>0\). We then seek to find a deferral rule \(r\) that maximises the predictive accuracy, while invoking \(h^{(2)}\) sparingly. This can be formulated as the following constrained optimisation problem:

\[_{r}(y h^{(1)}(x),r(x)=0)+(y h^{(2)}(x),r(x)=1 )(r(x)=1), \]

for a deferral rate \(\). Intuitively, the objective measures the misclassification error only on samples where the respective classifier's predictions are used; the constraint ensures that \(h^{(2)}\) is invoked on at most \(\) fraction of samples. It is straight-forward to extend this formulation to generic cost-sensitive variants of the predictive accuracy .

**Equivalent unconstrained risk**. Using Lagrangian theory, one may translate (1) into minimizing an equivalent unconstrained risk. Specifically, under mild distributional assumptions (see e.g., Neyman and Pearson ), we can show that for any deferral rate \(\), there exists a deferral cost \(c 0\) such that solving (1) is equivalent to minimising the following unconstrained risk:

\[R(r;h^{(1)},h^{(2)})=(y h^{(1)}(x),r(x)=0)+(y h^{( 2)}(x),r(x)=1)+c(r(x)=1). \]

**Deferral curves and cost-risk curves**. As \(c\) varies, one may plot the misclassification error of the resulting cascade as a function of deferral rate. We will refer to this as the _deferral curve_. One may similarly compute the _cost-risk curve_ that traces out the optimal cascade risk (2) as a function of \(c\). Both these are equivalent ways of assessing the overall quality of a cascade.

In fact, the two curves have an intimate point-line duality. Any point \((,E)\) in deferral curve space - where \(\) denotes deferral rate, and \(E\) denotes error - can be mapped to the line \((c,c+E):c\) in cost curve space. Conversely, any point \((c,R)\) in cost curve space - where \(c\) denotes cost, and \(R\) denotes risk - can be mapped to the line \((d,R-C d):d\) in deferral curve space. This is analogous to the correspondence between ROC and cost-risk curves in classification . Following prior work [2; 36], our experiments use deferral curves to compare methods.

### The Bayes-Optimal Deferral Rule

The _Bayes-optimal_ rule, which minimises the risk \(R(r;h^{(1)},h^{(2)})\) over all possible deferral rules \(r\{0,1\}\), is given below.

**Proposition 3.1**.: _Let \(_{y^{}}(x)(y^{}|x)\). Then, the Bayes-optimal deferral rule for the risk in (2) is:_

\[r^{*}(x)=[_{h^{(2)}(x)}(x)-_{h^{(1)}(x)}(x)>c], \]

(Proof in Appendix A.) Note that for a classifier \(h\), \(_{h(x)}(x)=(y=h(x)|x)\) i.e., the probability that \(h\) gives a correct prediction for \(x\). Observe that the randomness here reflects the inherent stochasticity of the labels \(y\) for an input \(x\), i.e., the aleatoric uncertainty . For the purposes of evaluating the deferral curve, the key quantity is \(_{h^{(2)}(x)}(x)-_{h^{(1)}(x)}(x)\), i.e., the _difference in the probability of correct prediction under each classifier_. This is intuitive: it is optimal to defer to \(h^{(2)}\) if the expected reduction in misclassification error exceeds the cost of invoking \(h^{(2)}\).

The Bayes-optimal deferral rule in (3) is a theoretical construct, which relies on knowledge of the true posterior probability \(\). In practice, one is likely to use an approximation to this rule. To quantify the effect of such an approximation, we may consider the _excess risk_ or _regret_ of an arbitrary deferral rule \(r\) over \(r^{*}\). We have the following.

**Corollary 3.2**.: _Let \((x)_{h_{1}(x)}(x)-_{h_{2}(x)}(x)+c\). Then, the excess risk for an arbitrary \(r\) is_

\[R(r;h^{(1)},h^{(2)})-R(r^{*};h^{(1)},h^{(2)})=_{x}[((r(x)=1 )-((x)<0))(x)].\]

Intuitively, the above shows that when we make deferral decisions that disagree with the Bayes-optimal rule, we are penalised proportional to the difference between the two models' error probability.

### Plug-in Estimators of the Bayes-Optimal Deferral Rule

In practice, we may seek to approximate the Bayes-optimal deferral rule \(r^{*}\) in (3) with an estimator \(\). We now present several _oracle estimators_, which will prove useful in our subsequent analysis.

**One-hot oracle**. Observe that \(_{y^{}}(x)=_{y|x}[[y=y^{}]]\). Thus, given a test sample \((x,y)\), one may replace the expectation with the observed label \(y\) to yield the ideal estimator \(_{y^{}}(x)=[y^{}=y]\). This results in the rule

\[_{01}(x)[[y=h^{(2)}(x)]-[y=h^ {(1)}(x)]>c]. \]

One intuitive observation is that for high \(c\), this rule only defers samples with \(y h^{(1)}(x)\) but \(y=h^{(2)}(x)\), i.e., _samples where the first model is wrong, but the second model is right_. Unfortunately, this rule is impractical, since it depends on the label \(y\). Nonetheless, it serves as an oracle to help understand what one can gain if we knew exactly whether the downstream model makes an error.

**Probability oracle**. Following a similar reasoning as \(_{01}\), another estimator is given by

\[_{}(x)[p_{y}^{(2)}(x)-p_{y}^{(1)}(x)> c]. \]

Intuitively \(p_{y}^{(2)}(x)\) can be seen as a label-dependent correctness score of model 2 on an instance \(x\).

Relative confidence.The above oracles rely on the true label \(y\). A more practical plug-estimator is \(_{h^{(i)}(x)}(x)=_{y^{}}p^{(i)}_{y^{}}(x)\), which simply uses each model's softmax probabilities. The rationale for this rests upon the assumption that the probability model \(p^{(i)}\) is a consistent estimate of the true posterior probability so that \((y|x) p^{(i)}_{y}(x)\) for \(i\{1,2\}\), where \((x,y)\) is a labeled example. Thus, \(_{h^{(i)}(x)}(x)=(h^{(i)}(x)|x) p^{(i)}(h^{(i)}(x)|x)= _{y^{}}p^{(i)}_{y^{}}(x)\), resulting in the rule

\[_{}(x) 1[_{y^{}}p^{(2)}_{y^{ }}(x)-_{y^{}}p^{(1)}_{y^{}}(x)>c]. \]

Observe that this deferral decision depends on the confidence of _both_ models, in contrast to confidence-based deferral which relies only on the confidence of the _first_ model.

Note that the above oracles cannot be used directly for adaptive computation, because the second model is invoked on _every_ input. Nonetheless, they can inform us about the available headroom to improve over confidence-based deferral by considering the confidence of the downstream model. As shall be seen in SS4, these estimators are useful for deriving objectives to train a post hoc deferral rule.

### Relation to Existing Work

The two-model cascade is closely connected to the literature on _learning to defer to an expert_[46; 48; 9]. Here, the goal is to learn a base classifier \(h^{(1)}\) that has the option of invoking an "expert" model \(h^{(2)}\); this invocation is controlled by a deferral rule \(r\). Indeed, the risk (2) is a special case of Mozannar and Sontag (48, Equation 2), where the second model is considered to be an "expert". Proposition 3.1 is a simple generalisation of Mozannar and Sontag (48, Proposition 2), with the latter assuming \(c=0\). In Appendix F, we generalise Proposition 3.1 to the cascades of \(K>2\) models.

## 4 From Confidence-Based to Post-Hoc Deferral

Having presented the optimal deferral rule in Proposition 3.1, we now use it to explicate some failure modes for confidence-based deferral, which will be empirically demonstrated in SS5.2.

### When Does Confidence-Based Deferral Suffice?

Suppose as before we have probabilistic models \(p^{(1)},p^{(2)}\). Recall from Algorithm 1 that for constant \(c^{(1)}>0\), confidence-based deferral employs the rule

\[_{}(x)=[_{y^{}}p^{(1)}_{y^{ }}(x)<c^{(1)}]. \]

Following SS3.3, (7) may be regarded as a plug-in estimator for the "population confidence" rule \(r_{}(x)[_{h^{(1)}}(x)<c^{(1)}]\). Contrasting this to Proposition 3.1, we have the following:

**Lemma 4.1**.: _Assume that for any \(x,x^{}\), \(_{h^{(1)}}(x)_{h^{(1)}}(x^{})\) if and only if \(_{h^{(1)}}(x)-_{h^{(2)}}(x)_{h^{(1)}}(x^{})-_{h^{(2 )}}(x^{})\) (i.e., \(_{h^{(1)}}(x)\) and \(_{h^{(1)}}(x)-_{h^{(2)}}(x)\) produce the same ordering over instances \(x\)). Then, the deferral rule \(r_{}\) produces the same deferral curve as the Bayes-optimal rule (3)._

Lemma 4.1 studies the agreement between the _deferral curves_ of \(r_{}\) and the Bayes-optimal solution, which eliminates the need for committing to a specific cost \(c^{(1)}\). The lemma has an intuitive interpretation: population confidence-based deferral is optimal if and only if the _absolute_ confidence in model 1's prediction agrees with the _relative_ confidence is model 1 versus model 2's prediction.

Based on this, we now detail some cases where confidence-based deferral succeeds or fails.

**Success mode: expert \(h^{(2)}\)**. Lemma 4.1 has one immediate, intuitive consequence: _confidence-based deferral is optimal when the downstream model has a constant error probability_, i.e., \(_{h_{2}(x)}(x)\) is a constant for all \(x\). This may happen, e.g., if that the labels are deterministic given the inputs, and the second classifier \(h^{(2)}\) perfectly predicts them. Importantly, note that this is a sufficient (but _not_ necessary) condition for the optimality of confidence-based deferral.

**Failure mode: specialist \(h^{(2)}\)**. As a converse to the above, one setting where confidence-based deferral may fail is when when the downstream model is a _specialist_, which performs well only on a particular sub-group of the data (e.g., a subset of classes). Intuitively, confidence-based deferral may _erroneously forward samples where \(h^{(2)}\) performs worse than \(h^{(1)}\)_.

Concretely, suppose there is a data sub-group \(_{}\) where \(h^{(2)}\) performs exceptionally well, i.e., \(_{h^{(2)}(x)} 1\) when \(x_{}\). On the other hand, suppose \(h^{(2)}\) does not perform well on \(_{}_{}\), i.e., \(_{h^{(2)}(x)} 1/L\) when \(x_{}\). Intuitively, while \(_{h^{(1)}(x)}\) may be relatively low for \(x_{}\), it is strongly desirable to _not_ defer such examples, as \(h^{(2)}\) performs even worse than \(h^{(1)}\); rather, it is preferable to identify and defer samples \(x_{}\).

**Failure mode: label noise**. Confidence-based deferral can fail when there are high levels of label noise. Intuitively, in such settings, confidence-based deferral may _wastefully forward samples where \(h^{(2)}\) performs no better than \(h^{(1)}\)_. Concretely, suppose that instances \(x_{}\) may be mislabeled as one from a different, random class. For \(x_{}\), regardless of how the two models \(h^{(1)},h^{(2)}\) perform, we have \(_{h^{(1)}(x)}(x),_{h^{(2)}(x)}(x)=1/L\) (i.e., the accuracy of classifying these instances is chance level in expectation). Since \(_{h^{(1)}(x)}(x)\) is low, confidence-based deferral will tend to defer such input instance \(x\). However, this is a sub-optimal decision since model 2 is more computationally expensive, and expected to have the same chance-level performance.

**Failure mode: distribution shift**. Even when model \(h^{(2)}\) is an expert model, an intuitive setting where confidence-based deferral can fail is if there is _distribution shift_ between the train and test \((y x)\). In such settings, even if \(p^{(1)}\) produces reasonable estimates of the _training_ class-probability, these may translate poorly to the test set. There are numerous examples of confidence degradation under such shifts, such as the presence of _out-of-distribution_ samples [52; 28], and the presence of a _label skew_ during training [79; 64]. We shall focus on the latter in the sequel.

### Post-Hoc Estimates of the Deferral Rule

Having established that confidence-based deferral may be sub-optimal in certain settings, we now consider the viability of deferral rules that are _learned_ in a post-hoc manner. Compared to confidence-based deferral, such rules aim to explicitly account for _both_ the confidence of model 1 and 2, and thus avoid the failure cases identified above.

The key idea behind such post-hoc rules is to directly mimic the optimal deferral rule in (3). Recall that this optimal rule has a dependence on the output of \(h^{(2)}\); unfortunately, querying \(h^{(2)}\) defeats the entire purpose of cascades. Thus, our goal is to estimate (3) using only the outputs of \(p^{(1)}\).

We summarise a number of post-hoc estimators in Table 1, which are directly motivated by the One-hot, Probability, and Relative Confidence Oracle respectively from SS3.3. The first is to learn when model 1 is incorrect, and model 2 is correct. For example, given a validation set, suppose we construct samples \(S_{}\{(x_{i},z_{i}^{(1)})\}\), where \(z_{i}^{(1)}=1[y=h^{(2)}(x_{i})]-1[y=h^{(1)}(x_{i})]\). Then, we fit

\[_{g}}|}_{(x_{i },z_{i}^{(1)}) S_{}}(z_{i}^{(1)},g(x_{i})),\]

where, e.g., \(\) is the square loss. The score \(g(x)\) may be regarded as the confidence in deferring to model 2. Similarly, a second approach is to perform regression to predict \(z_{i}^{(2)}=p_{y}^{(2)}(x_{i})-p_{y}^{(1)}(x_{i})\). The third approach is to directly estimate \(z_{i}^{(3)}=_{y^{}}p_{y^{}}^{(2)}(x_{i})\) using predictions of the first model.

As shall be seen in SS5.2, such post-hoc rules can learn to avoid the failure cases for confidence-based deferral identified in the previous section. However, it is important to note that there are some conditions where such rules may not offer benefits over confidence-based deferral.

  
**Training label \(z\)** & **Loss** & **Deferral rule** & **Method label** & **Comment** \\   \(1[y=h^{(2)}(x)]-1[y=h^{(1)}(x)]\) & Mean-squared error & \(g(x)>c\) & Diff-oi & Regression \\ \(p_{y}^{(2)}(x)-p_{y}^{(1)}(x)\) & Mean absolute error & \(g(x)>c\) & Diff-Prob & Regression \\ \(_{y^{}}p_{y^{}}^{(2)}(x)\) & Mean-squared error & \(g(x)-_{y^{}}p_{y^{}}^{(1)}(x)>c\) & MaxProb & Regression \\   

Table 1: Candidate post-hoc estimators of the oracle rule in (2). We train a post-hoc model \(g(x)\) on a training set \(\{(x_{i},z_{i})\}_{i=1}^{n}\) so as to predict the label \(z\). Here, \((x,y)\) is a labeled example.

**Failure mode: Bayes \(p^{(2)}\)**. Suppose that the model \(p^{(2)}\) exactly matches the Bayes-probabilities, i.e., \(p^{(2)}_{y^{}}(x)=(y^{} x)\). Then, estimating \(_{y^{}}p^{(2)}_{y^{}}(x)\) is equivalent to estimating \(_{y^{}}(y^{} x)\). However, the goal of model 1 is _precisely_ to estimate \((y x)\). Thus, if \(p^{(1)}\) is sufficiently accurate, in the absence of additional information (e.g., a fresh dataset), it is unlikely that one can obtain a better estimate of this probability than that provided by \(p^{(1)}\) itself. This holds even if the \((y x)\) is non-deterministic, and so the second model has non-trivial error.

**Failure mode: non-predictable \(p^{(2)}\) error**. When the model \(p^{(2)}\)'s outputs are not strongly predictable, post-hoc deferral may devolve to regular confidence-based deferral. Formally, suppose we seek to predict \(z_{y^{}}p^{(2)}_{y^{}}(x)\), e.g., as in MaxProb. A non-trivial predictor must achieve an average square error smaller than the variance of \(z\), i.e., \([(z-[z])^{2}]\). If \(z\) is however not strongly predictable, the estimate will be tantamount to simply using the constant \([z]\). This brings us back to the assumption of model 2 having a constant probability of error, i.e., confidence-based deferral.

### Finite-Sample Analysis for Post-Hoc Deferral Rules

We now formally quantify the gap in performance between the learned post-hoc rule and the Bayes-optimal rule \(r^{*}=1[g^{*}(x)>c]\) in Proposition 3.1, where \(g^{*}(x)=_{h^{(2)}(x)}(x)-_{h^{(1)}(x)}(x)\). We will consider the case where the validation sample \(S_{}\) is constructed with labels \(z_{i}^{(1)}\). We pick a scorer \(\) that minimises the average squared loss \(}|}_{(x_{i},z_{i}) S_{}}(z_{i}-g(x _{i}))^{2}\) over a hypothesis class \(\). We then construct a deferral rule \((x)=1[(x)>c]\).

**Lemma 4.2**.: _Let \((,)\) denote the covering number of \(\) with the \(\)-norm. Suppose for any \(g\), \((z-g(x))^{2} B,(x,z)\). Furthermore, let \(\) denote the minimizer of the population squared loss \([(z-g(x))^{2}]\) over \(\), where \(z=1[y=h^{(2)}(x)]-1[y=h^{(1)}(x)]\). Then for any \((0,1)\), with probability at least \(1-\) over draw of \(S_{}\), the excess risk for \(\) is bounded by_

\[R(;h^{(1)},h^{(2)})-R(r^{*};h^{(1)},h^{(2)})\] \[\,2(_{x}[((x)-g ^{*}(x))^{2}]}_{}+\{B(,)}{| S_{}|}}\}}_{}+\,(}|}}))^{1/2}.\]

We provide the proof in SSA.4. The first term on the right-hand side (RHS) is an irreducible approximation error, quantifying the distance between the best possible model in the class to the oracle scoring function. The second and the third terms on RHS quantify total estimation error.

### Relation to Existing Work

As noted in SS3.4, learning a deferral rule for a two-model cascade is closely related to existing literature in learning to defer to an expert. This in turn is a generalisation of the classical literature on _learning to reject_[27; 7], which refers to classification settings where one is allowed to abstain from predicting on certain inputs. The population risk here is a special case of (2), where \(h^{(2)}(x)\) is assumed to perfectly predict \(y\). The resulting Bayes-optimal classifier is known as Chow's rule [11; 60], and exactly coincides with the deferral rule in Lemma 4.1. Plug-in estimates of this rule are thus analogous to confidence-based deferral, and have been shown to be similarly effective .

In settings where one is allowed to modify the training of \(h^{(1)}\), it is possible to construct losses that jointly optimise for both \(h^{(1)}\) and \(r\)[1; 12; 60; 73; 8; 21; 36]. While effective, these are not applicable in our setting involving pre-trained, black-box classifiers. Other variants of post-hoc methods have been considered in Narasimhan et al. , and implicitly in Trapeznikov and Saligrama ; however, here we more carefully study the different possible ways of constructing these methods, and highlight when they may fail to improve over confidence-based deferral.

## 5 Experimental Illustration

In this section, we provide empirical evidence to support our analysis in SS4.1 by considering the three failure modes in which confidence-based deferral underperforms. For each of these settings, wecompute _deferral curves_ that plot the classification accuracy versus the fraction of samples deferred to the second model (which implicitly measures the overall compute cost). In line with our analysis in SS4.1, post-hoc deferral rules offer better accuracy-cost trade-offs in these settings.

### Confidence-Based versus Oracle Deferral

We begin by illustrating the benefit of considering confidence of the second model when constructing a deferral rule. In this experiment, \(h^{(1)}\) is a generalist (i.e., trained on all ImageNet classes), and \(h^{(2)}\) is a dog specialist trained on all images in the dog synset, plus a fraction of non-dog training examples, which we vary. There are 119 classes in the dog synset. We use MobileNet V2  as \(h^{(1)}\), and a larger EfficientNet B0  as \(h^{(2)}\). For hyperparameter details, see Appendix C.

Figure 1 shows the accuracy of confidence-based deferral (Confidence) and Relative Confidence (Equation (6)) on the standard ImageNet test set as a function of the deferral rate. We realise different deferral rates by varying the value of the deferral threshold \(c\). In Figure 0(a), the fraction of non-dog training images is 100% i.e., model 2 is also a generalist trained on all images. In this case, we observe that Relative Confidence offers little gains over Confidence.

However, in Figure 0(b) and Figure 0(c), as the fraction of non-dog training images decreases, the non-uniformity of \(h^{(2)}\)'s error probabilities increases i.e., \(h^{(2)}\) starts to specialise to dog images. In line with our analysis in SS4.1, confidence-based deferral underperforms when model 2's error probability is highly non-uniform. That is, being oblivious to the fact that \(h^{(2)}\) specialises in dog images, confidence-based deferral may erroneously defer non-dog images to it. By contrast, accounting for model 2's confidence, as done by Relative Confidence, shows significant gains.

### Confidence-Based versus Post-Hoc Deferral

From SS5.1, one may construct better deferral rules by querying model 2 for its confidence. Practically, however, querying model 2 at inference time defeats the entire purpose of cascades. To that end, we now compare confidence-based deferral and the post-hoc estimators (Table 1), which do _not_ need to invoke model 2 at inference time. We consider each of the settings from SS4.1, and demonstrate that post-hoc deferral can significantly outperform confidence-based deferral. We present more experimental results in Appendix E, where we illustrate post-hoc deferral rules for \(K>2\) models.

Post hoc model training.For a post-hoc approach to be practical, the overhead from invoking a post-hoc model must be small relative to the costs of \(h^{(1)}\) and \(h^{(2)}\). To this end, in all of the following experiments, the post-hoc model \(g\) is based on a lightweight, three-layer Multi-Layer Perceptron (MLP) that takes as input the probability outputs from model 1. That is, \(g(x)=(p^{(1)}(x))\) where \(p^{(1)}(x)_{L}\) denotes all probability outputs from model 1. Learning \(g\) amounts to learning the MLP as the two base models are fixed. We train \(g\) on a held-out validation set. For full technical details of the post-hoc model architecture and training, see Appendix C. We use the objectives described in Table 1 to train \(g\).

Figure 1: Test accuracy vs deferral rate of plug-in estimates (§3.3) for the oracle rule. Here, \(h^{(1)}\) is a MobileNet V2 trained on all ImageNet classes, and \(h^{(2)}\) is a dog specialist trained on all images in the dog synset plus a fraction of non-dog training examples, which we vary. As the fraction decreases, \(h^{(2)}\) specialises in classifying different types of dogs. By considering the confidence of \(h^{(2)}\) (Relative Confidence), one gains accuracy by selectively deferring only dog images.

**Specialist setting**. We start with the same ImageNet-Dog specialist setting used in SS5.1. This time, we compare six methods: Confidence, Random, MaxProb, Diff-o1, Diff-Prob, and Entropy. Random is a baseline approach that defers to either model 1 or model 2 at random; MaxProb, Diff-o1, and Diff-Prob are the post-hoc rules described in Table 1; Entropy defers based on the thresholding the entropy of \(p^{(1)}\), as opposed to the maximum probability.

Results for this setting are presented in Figure 2 (first row). We see that there are gains from post-hoc deferral, especially in the low deferral regime: it can accurately determine whether the second model is likely to make a mistake. Aligning with our analysis, for the generalist setting (Figure 1(a)), confidence-based deferral is highly competitive, since \(h^{(2)}\) gives a consistent estimate of \((y x)\).

**Label noise setting**. In this setting, we look at a problem with label noise. We consider CIFAR 100 dataset where training examples from pre-chosen \(L_{}\{0,10,25\}\) classes are assigned a uniformly drawn label. The case of \(L_{}=0\) corresponds to the standard CIFAR 100 problem. We set \(h^{(1)}\) to be CIFAR ResNet 8 and set \(h^{(2)}\) to be CIFAR ResNet 14, and train both models on the noisy data. The results are shown in Figure 2 (second row). It is evident that when there is label noise, post-hoc approaches yield higher accuracy than confidence-based on a large range of deferral rates, aligning with our analysis in SS4.1. Intuitively, confidence-based deferral tends to forward noisy samples to \(h^{(2)}\), which performs equally poorly, thus leading to a waste of deferral budget. By contrast, post-hoc rules can learn to "give up" on samples with extremely low model 1 confidence.

Figure 2: Test accuracy vs deferral rate of the post-hoc approaches in Table 1 under the three settings described in §4.1: 1) specialist (row 1), 2) label noise (row 2), and 3) distribution shift. **Row 1**: As the fraction of non-dog training images decreases, model 2 becomes a dog specialist model. Increase in the non-uniformity in its error probabilities allows post-hoc approaches to learn to only defer dog images. **Row 2**: As label noise increases, the difference in the probability of correct prediction under each model becomes zero (i.e., probability tends to chance level). Thus, it is sub-optimal to defer affected inputs since model 2’s correctness is also at chance level. Being oblivious to model 2, confidence-based deferral underperforms. For full details, see §5.2. **Row 3**: As the skewness of the label distribution increases, so does the difference in the probability of correct prediction under each model (recall the optimal rule in Proposition 3.1), and it becomes necessary to account for model 2’s probability of correct prediction when deferring. Hence, confidence-based deferral underperforms.

**Distribution shift setting**. To simulate distribution shift, we consider a long-tailed version of CIFAR 100  where there are \(h\{100,50,25\}\) head classes, and \(100-h\) tail classes. Each head class has 500 training images, and each tail class has 50 training images. The standard CIFAR 100 dataset corresponds to \(h=100\). Both models \(h^{(1)}\) (CIFAR ResNet 8) and \(h^{(2)}\) (CIFAR ResNet 56) are trained on these long-tailed datasets. At test time, all methods are evaluated on the standard CIFAR 100 balanced test set, resulting in a label distribution shift.

We present our results in Figure 2 (third row). In Figure 1(g), there is no distribution shift. As in the case of the specialist setting, there is little to no gain from post-hoc approaches in this case since both models are sufficiently accurate. As \(h\) decreases from 100 to 50 (Figure 1(h)) and 25 (Figure 1(i)), there is more distribution shift at test time, and post-hoc approaches (notably Diff-o1) show clearer gains. To elaborate, the two base models are of different sizes and respond to the distribution shift differently, with CIFAR ResNet 56 being able to better handle tail classes overall. Diff-o1 is able to identify the superior performance of \(h^{(2)}\) and defer input instances from tail classes.

### On the Generalisation of Post-Hoc Estimators

Despite the benefits of post-hoc approaches as demonstrated earlier, care must be taken in controlling the capacity of the post-hoc models. We consider the same ImageNet-Dog specialist setting as in the top row of Figure 2. Here, model 2 is trained on all dog images, and a large fraction of non-dog images (8%). Since model 2 has access to a non-trivial fraction of non-dog images, the difference in the probability of correct prediction of the two models is less predictable. We report deferral curves on both training and test splits in Figure 3. Indeed, we observe that the post-hoc method Diff-o1 can overfit, and fail to generalise. Note that this is despite using a feedforward network with two hidden layers of only 64 and 16 units (see Appendix C for details on hyperparameters) to control the capacity of the post-hoc model. Thoroughly investigating approaches to increase generalisation of post-hoc models will be an interesting topic for future study.

## 6 Conclusion and Future Work

The Bayes-optimal deferral rule we present suggests that key to optimally defer is to identify when the first model is wrong and the second is right. Based on this result, we then study a number of estimators (Table 1) to construct trainable post hoc deferral rules, and show that they can improve upon the commonly used confidence-based deferral.

While we have identified conditions under which confidence-based deferral underperforms (e.g., specialist setting, label noise), these are not exhaustive. An interesting direction for future work is to design post-hoc deferral schemes attuned for settings involving other forms of distribution shift, such as the presence of out-of-distribution samples. It is also of interest to study the efficacy of more refined confidence measures, such as those based on conformal prediction . Finally, while our results have focussed on image classification settings, it would be of interest to study analogous trends for natural language processing models.

Figure 3: Training and test accuracy of post-hoc approaches in the ImageNet-Dog specialist setting. Model 2 (EfficientNet B0) is trained with all dog images and 8% of non-dog images. Observe that a post-hoc model (i.e., Diff-o1) can severely overfit to the training set and fail to generalise.