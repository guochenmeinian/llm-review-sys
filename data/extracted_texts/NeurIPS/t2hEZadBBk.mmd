# Tailoring Self-Attention for Graph via Rooted Subtrees

Siyuan Huang  Yunchong Song  Jiayue Zhou  Zhouhan Lin

Shanghai Jiaotong University

siyuan_huang_sjtu@outlook.com  ycsong@sjtu.edu.cn  lin.zhouhan@gmail.com

Zhouhan Lin is the corresponding author

###### Abstract

Attention mechanisms have made significant strides in graph learning, yet they still exhibit notable limitations: local attention faces challenges in capturing long-range information due to the inherent problems of the message-passing scheme, while global attention cannot reflect the hierarchical neighborhood structure and fails to capture fine-grained local information. In this paper, we propose a novel multi-hop graph attention mechanism, named Subtree Attention (STA), to address the aforementioned issues. STA seamlessly bridges the fully-attentional structure and the rooted subtree, with theoretical proof that STA approximates the global attention under extreme settings. By allowing direct computation of attention weights among multi-hop neighbors, STA mitigates the inherent problems in existing graph attention mechanisms. Further we devise an efficient form for STA by employing kernelized softmax, which yields a linear time complexity. Our resulting GNN architecture, the STAGNN, presents a simple yet performant STA-based graph neural network leveraging a hop-aware attention strategy. Comprehensive evaluations on ten node classification datasets demonstrate that STA-based models outperform existing graph transformers and mainstream GNNs. The code is available at https://github.com/LUMIA-Group/SubTree-Attention.

## 1 Introduction

Graph Neural Networks (GNNs) have achieved remarkable performance in various tasks, such as drug discovery , social networks , traffic flow , and recommendation systems . Most GNNs are based on the message-passing scheme , hierarchically aggregating information from multi-hop neighbors by stacking multiple layers. During this procedure, a rooted subtree can be generated for each node, representing the node's neighborhood structure. Nodes with similar neighborhood structures possess similar subtrees, which leads to similar node representations .

_Local attention_ in graph learning can be seen as a natural combination of the message-passing scheme and the self-attention mechanism. By adaptively assigning weights among one-hop neighbors in a single layer, the local attention mechanism allows each node to focus on the most task-relevant neighbors . However, local attention limits the receptive field to one-hop neighbors. While stacking multiple local-attention layers to build a deep model can increase the receptive field, such message-passing-based deep architectures face challenges in capturing long-range dependencies  due to issues such as over-smoothing  and over-squashing .

On the other hand, the _global attention_ mechanism originated from vanilla Transformer  has been widely adopted in graph learning domain , leveraging the fully-attentional architecture to address the aforementioned issues. However, the global attention employed by graph transformers cannot reflect the hierarchical neighborhood structure and fails to capture fine-grained local information, which is crucial in many real-world scenarios . To mitigate this deficiency, recent studies try to directly assemble global attention and message-passing-basedmodels by combining GNNs and Transformers, including simultaneously applying GNNs and Transformers  or building Transformers on top of GNNs [33; 27; 45].

Considering the limitations of both local and global attention, we propose a multi-hop graph attention mechanism, termed SubTree Attention (STA). It allows the root node to directly attend to further neighbors in the subtree, enabling the root node to gather information from the entire rooted subtree within one layer. It provides two main advantages: (i) Compared to deep architectures with multiple local attention layers, STA avoids issues associated with the message-passing scheme such as over-smoothing and over-squashing. (ii) Compared to global attention, STA can hierarchically capture the neighborhood structure by enabling each node to focus on its own rooted subtree.

Due to the exponential growth in neighborhood size with increased hops, directly calculating attention among multi-hop neighbors becomes impractical. Meanwhile, powers of the adjacency matrix have to be stored for the calculation among multi-hop neighbors. To address these issues, we employ kernelized softmax  to develop an algorithm that reduces the quadratic time complexity to linear while avoiding the need to store the powers of the adjacency matrix. This efficient algorithm can be viewed as keys and values performing a random walk on the graph and eventually landing on queries.

Furthermore, we provide a theoretical analysis of STA, demonstrating that under extreme settings, STA converges to the global self-attention. As a result, STA can be regarded as a bridge between local and global attention, effectively combining the benefits of both approaches. In addition, we present the STA module with multiple attention heads. We introduce a hop-wise gating mechanism, enabling attention heads to be specialized in capturing information from specific hops.

We then propose a simple yet performant multi-hop graph attention network, named STAGNN, which can leverage multi-hop information and in the meantime acts as a fully-attentional model. As for the evaluation, we test the performance of STAGNN on ten common node classification datasets. Despite its relatively simple architecture, STAGNN consistently outperforms existing GNNs and graph transformers. Furthermore, we demonstrate that STAGNN maintains a competitive performance with an extremely deep architecture. Additional ablation studies are conducted to show the effectiveness of subtree attention even in the presence of global attention.

## 2 Background

Let \(=(,)\) be an undirected graph, with the associated nodes set \(\) and edges set \(\). We use \(N=||\) to represent the number of nodes. \(^{N f}\) denotes the node feature, where \(f\) denotes the number of features. Let \(\) be the adjacency matrix of \(\) and let \(\) be the diagonal degree matrix of \(\). \(_{}=^{-1/2}^{-1/2}\) denotes the symmetric normalized adjacency matrix, while \(_{}=^{-1}\) denotes the random walk matrix. Let \(}\) be an arbitrary transition matrix, including \(_{}\), \(_{}\) or other matrices representing message propagation. We use \(_{i:}\) and \(_{:j}\) to indicate the \(i^{}\) row and the \(j^{}\) column of the matrix \(\), respectively. And let \( 0,K\) denote the set \(\{0,1,,K\}\).

### Multi-Hop Representations

Many existing GNNs and diffusion-based models perform multi-hop message-passing in a single layer, taking advantage of multi-hop representations [8; 1; 47; 22; 51]. Among them, decoupled GCN  is a typical representative. One important reason for the over-smoothing problem in GCN is that neighborhood aggregation and feature transformation are coupled . To address this issue, decoupled GCNs perform feature transformation and neighborhood aggregation, respectively. A general form of decoupled GCN can be described as follows :

\[=_{k=0}^{K}_{k}_{k}(),\ _{k}()=}^{k},\ =()\] (1)

where \(\{_{k}\}_{k 0,K}\) are the aggregation weights for different hops. \(K\) represents the number of propagation steps, which also corresponds to the height of the resulting rooted subtree. Although there exist various strategies to assign weights to different hops, these methods all apply a \(K\)-step propagation with transition matrix \(}\), which inevitably results in over-smoothing when \(K\) is large.

There are relatively few attention-based methods that leverage multi-hop information. As a representative, Wang et al. (2021) generalized GAT by Personalized PageRank . Yet, this strategy only employs attention scores from one-hop neighbors and necessarily acts as a low-frequency filter. In a recent work, Chen et al. (2022)  began with aggregating \(K\)-hop representations and subsequently processed them as a sequence of length \(K\) using a Transformer model. Despite the decent performance, this approach still adopts a \(K\)-step propagation to capture multi-hop information before taking the attention mechanism into account, which leads to the aforementioned issues. This observation inspires us to incorporate the attention mechanism into the propagation phase, rather than adhering to the \(K\)-step propagation with transition matrix \(}\).

### Global Self-Attention and Kernelized Softmax

In graph learning domain, the Global Self-Attention function \((,,)\) computes a weighted sum of all positions for each node. It first projects the node feature matrix into three subspaces:

\[=_{Q},\;=_{K},\; =_{V}\] (2)

where \(_{Q}^{d d_{K}},_{K}^{d  d_{K}}\) and \(_{V}^{d d_{V}}\) are learnable projection matrices. Then the new representation of the \(i\)th node is computed as follows:

\[(,,)_{i:}=^{N}(_{i:},_{j:})_{j:}}{_{j=1}^{N}(_{i:},_{j:})}\] (3)

where \((,):^{d}^{d}\) is a function used to evaluate the similarity between queries and keys. A common form of self-attention is called softmax attention, which applies the exponential of the dot product to compute similarity: \((_{i:},_{j:})=(_{i:} _{j:}^{T}}{}})\).

In fact, we can use an arbitrary positive-definite kernel \(\) to serve as \((,)\). Given a selected kernel \(\) and its corresponding feature map \(\), we can rewrite \((,)\) as: \((_{i:},_{j:})=(_{i:})(_{j:})^{T}\). Thus, Equation 3 becomes:

\[(,,)_{i:}=^{N}( _{i:})(_{j:})^{T}_{j:}}{_{j=1}^{N}( _{i:})(_{j:})^{T}}=_{i:})_{j =1}^{N}(_{j:})^{T}_{j:}}{(_{i:})_{j =1}^{N}(_{j:})^{T}}\] (4)

There are many potential choices for the feature map \(\). e.g., Tsai et al. (2019)  verified that RBF kernels perform on par with exponential kernels on neural machine translation and sequence prediction, and Choromanski et al. (2021)  opted for Positive Random Features (PRF).

The key advantage of Equation 4 is that all nodes share two identical summations \(_{j=1}^{N}(_{j:})^{T}_{j:}\) and \(_{j=1}^{N}(_{j:})^{T}\), which only need to be computed once. By doing so, we can avoid computing the full attention matrix \(\{(_{i:},_{j:})\}_{i N,j N}\) and reduce the complexity from \((N^{2})\) to \((N)\).

## 3 The Proposed Attention Mechanism: SubTree Attention

In this section, we present a detailed introduction to an efficient multi-hop attention mechanism called **S**ub**Tree** Attention (STA). First, we give the definition of STA, followed by an efficient algorithm for computing STA based on kernelized softmax and the message-passing scheme. We then explain how multi-head STA makes attention heads hop-aware by incorporating a gate into each hop. Finally, we prove that STA approximates the global self-attention when the height of the subtree is \(( N)\).

### SubTree Attention

In this subsection, we give the definition of our proposed multi-hop graph attention mechanism named subtree attention. Similar to the global self-attention function \((,,)\), subtree attention takes queries, keys, and values as inputs and outputs new values.

We first define the method for computing attention weights among the \(k\)th hop neighbors, which we refer to as \(_{k}\). For the \(i\)th node, this process can be described as follows:

\[_{0}(,,)_{i :}&=_{i:},\\ _{k}(,,)_{i:}& =^{N}}_{ij}^{k}(_{i:}, _{j:})_{j:}}{_{j=1}^{N}}_{ij}^{k} (_{i:},_{j:})} k 1,K\] (5)STA\({}_{k}(,,)_{i:}\) represents the \(i^{}\) node attending to its \(k^{}\) hop neighbors, which are also the \(k^{}\) level of its rooted subtree. SubTree Attention STA \(:(^{n d_{k}},^{n d_{k}},^{n  d_{v}})^{n d_{o}}\) can then be calculated by aggregating the results from all levels of the rooted subtree:

\[(,,)_{i:}=(\{_{k}(,,)_{i:} k 0,K\})\] (6)

AGGR can be any aggregation function, such as sum, concat , attention-based readout , or the GPR-like aggregation  which we present in section 4.

In simple terms, STA\({}_{k}\) allows each node to attend to the \(k^{}\) level of its own subtree. Meanwhile, STA aggregates the results from all levels to gather information from the entire subtree. Figure 1 shows the process of STA.

In contrast to global attention, subtree attention enables each node to compute attention weights on its own subtree, thus taking advantage of the key insight of Message-Passing GNNs (MP-GNNs). e.g., even if the majority of nodes within the receptive fields are the same for two different nodes, they will still gather different information through subtree attention if they have different subtree structures.

### An Efficient Algorithm for SubTree Attention

Equation 5 gives a straightforward method to calculate STA\({}_{k}\): we first compute the complete similarity matrix \(\{(_{i:},_{j:})\}_{i N,j N}\) and then calculate its Hadamard product with \(}^{k}\) to obtain \(\{}^{k}_{ij}(_{i:},_{j:})\}_{i  N,j N}\). In short, we treat \(}^{k}\) as a mask for the similarity matrix. This algorithm exhibits two primary disadvantages: (i) The computational cost associated with calculating the entire similarity matrix is \((N^{2})\). (ii) \(}^{k}\) quickly converges to a dense matrix. Storing \(}^{k}\) in the GPU memory for computing the Hadamard product leads to considerable space complexity. Considering these two limitations, utilizing \(}^{k}\) as a mask for the similarity matrix is suboptimal.

We now present an efficient algorithm for subtree attention. Considering the close relationship between rooted subtrees and MP-GNNs, we leverage the message-passing scheme to implement the computation of subtree attention. By permitting keys and values to propagate along the edges, we can achieve an algorithm that has linear time complexity and avoids the need to store \(}^{k}\).

Learning from Equation 4, we use a feature map \(\) to replace sim\((,)\). The choice of feature map is not the main focus of our work. Our model adopts a simple yet effective approach proposed by Katharopoulos et al. (2020) that chooses \((x)=elu(x)+1\) as the feature map and demonstrates empirically that it performs on par with softmax attention. We can rewrite Equation 5 as follows:

\[_{k}(,,)_{i:}=^{N} }^{k}_{ij}\ (_{i:})(_{j:})^{T}_{j:}}{_{j=1}^{N} }^{k}_{ij}\ (_{i:})(_{j:})^{T}}=_{i:})_{j=1}^{N}}^{k}_{ij}\ (_{j:})^{T} _{j:}}{(_{i:})_{j=1}^{N}}^{k}_{ij} \ (_{j:})^{T}}\] (7)

Figure 1: **(Left):** Definition of SubTree Attention (STA). STA\({}_{k}\) represents that each node attends to the \(k^{}\) level of the rooted subtree, and then STA aggregates information from the entire subtree. **(Right):** An efficient algorithm for computing subtree attention. We first compute \((_{i:})\) and \((_{i:})^{T}_{i:}\) for each node, and then let \((_{i:})\) and \((_{i:})^{T}_{i:}\) perform \(K\)-step random walk, respectively. After each step of random walk, we compute attention weights using \((_{i:})\) and the aggregated keys and values. The computation of \(\{_{k}\}_{k 1,K}\) can be seen as a nested process.

Note that there are two summations \(_{j=1}^{N}}_{ij}^{k}\)\((_{j})^{T}_{j:}\) and \(_{j=1}^{N}}_{ij}^{k}\)\((_{j:})^{T}\) in Equation 7. We can think of these two summations as a kind of _message propagation_. That is to say, we first compute \((_{i:})\) and \((_{i:})^{T}_{i:}\) for each node. Then we let \((_{i:})\) and \((_{i:})^{T}_{i:}\) undergo \(k\) steps message passing. Finally, we use the aggregated keys and values \(_{j=1}^{N}}_{ij}^{k}\)\((_{j:})^{T}_{j:}\) and \(_{j=1}^{N}}_{ij}^{k}\)\((_{j:})^{T}\) in conjunction with the node's own query \((_{i:})\) to complete the computation of subtree attention. Figure 1 illustrates the whole process of this efficient algorithm. When we choose \(_{}\) as the transition matrix, this process can be regarded as keys and values performing a random walk on the graph, eventually landing on different queries. Note that message passing occurs on each edge, thus reducing the computational cost from \((N^{2})\) to \((||)\). Additionally, message passing only requires the sparse adjacency matrix \(\), thereby circumventing the need to store \(}^{k}\). Furthermore, \(\{_{i}\}_{i 1,K}\) can be viewed as a nested process, calculated one after another.

Based on this algorithm, we can say that STA is an attempt to incorporate the message-passing scheme into the fully-attentional architecture. In fact, STA serves as a message-passing module for keys and values. In section 4, we design a novel multi-hop graph attention network employing STA for message propagation. We also provide a detailed complexity analysis in Appendix A.

### SubTree Attention with Multiple Heads

In this subsection, we present STA with multiple attention heads. Kim et al. (2022)  discovered empirically that different attention heads tend to concentrate on neighbors at different hops. Certain attention heads can attend to remote nodes, while others consistently focus on nearby nodes, suggesting that attention heads can be specialized in capturing information from specific hops. To make better use of multiple attention heads in this context, we propose a hop-aware method of mixing them.

Suppose there are a total of \(H\) attention heads. STA with multiple attention heads, noted as MSTA, can be described as follows:

\[&(,,)= (\{_{k}(,,) k  0,K\})\\ &_{k}(,,)= _{k}^{1},,_{k}^{H} _{O} k 1,K,_{0}( ,,)=\\ &_{k}^{h}=_{k}^{h}_{k}(^{h}, ^{h},^{h}) h 1,H,}_{k}=(_{k})\] (8)

where \([\ ]\) denotes row-wise concatenation. \(^{h},^{h}\), and \(^{h}\) represent the query, key, and value matrices for the \(h^{}\) head, respectively. \(_{O}\) denotes a linear projection matrix. \(_{k}^{H}\) is an \(H\)-dimensional vector and \(g_{k}^{h}\) is its \(h^{}\) element, representing the weight of the \(h^{}\) attention head at the \(k^{}\) hop. Compared to STA with a single attention head, we introduce in total \(H K\) additional learnable parameters: \(\{_{i}\}_{i 1,K}\). We can regard \(_{k}\) as a hop-wise gate that determines the weight of each attention head at the \(k^{}\) hop.

In other words, we can reconsider the multi-hop attention mechanism in terms of multi-task learning. Different attention heads are seen as different experts, while aggregating information from different hops is seen as different task. \(_{k}\) signifies the process of selecting appropriate experts for each task.

### Theoretical Analysis of SubTree Attention

MP-GNNs suffer from issues like over-smoothing or over-squashing when the height of the subtree increases. In this subsection, we theoretically demonstrate that STA avoids the issues associated with the message-passing scheme despite employing the same rooted subtree as MP-GNNs. Notice that we employ the random walk matrix \(_{}\) as the transition matrix in the STA module, _i.e._, we have \(}=_{}\) in this subsection.

We first employ a slightly modified approach to rewrite the global self-attention module SA, which can be described as:

\[(,,)_{i:}=^{N} {}_{i}(_{i:},_{j:})_{j:}}{_{j=1 }^{N}_{i}(_{i:},_{j:})}\] (9)

where \(_{i}=^{N}d(j)}\) and \(d(j)\) denotes the degree of the \(j^{}\) node. Note that Equation 9 is consistent with Equation 3.

Comparing Equation 5 and Equation 9, we find that the only difference between SA and STA\({}_{k}\) lies in the mask for the similarity matrix. STA\({}_{k}\) employs \(}_{ij}^{k}\) as the mask, whereas SA employs \(_{i}\). We now demonstrate that when the height \(K\) of the rooted subtree takes the same order of magnitude as \((N)\), STA\({}_{k}\) can be approximately considered as SA.

**Theorem 1**: _Let \(}^{N N}\) denote the random walk matrix of a connected and non-bipartite graph, i.e., \(}=^{-1}\). Let \(1=_{1}_{N}\) be the eigenvalues of \(}\). Then we have the following results:_

\[ i,j 1,N^{2},\;>0,\; K_{0} ,\; k>K_{0},\;|}_{ij}^{k}- _{i}|\] (10)

_And for a given \(\), the smallest \(K_{0}\) that satisfies the condition shown in Equation 10 is at most \((}{1-\{_{2},|_{n }|\}})\). If we had more information about \(\), we could specify the convergence rate of STA\({}_{k}\). e.g., if \(\) is computed by \(=(_{V})\) where \(\) is a non-negative activation function, then:_

\[ i,j 1,N^{2},\;]0,1[,\; K _{1},\; k>K_{1},\;_{k}(,,)_{ij}}{(,, )_{ij}}\] (11)

_holds true when none of the denominators is equal to zero. And for a given \(\), the smallest \(K_{1}\) that satisfies the condition shown in Equation 11 is at most \((}{1-\{_{2},|_{n}| \}})\)._

Equation 10 demonstrates that \(}_{ij}^{k}\) converges to \(_{i}\) with logarithmic complexity, indicating that under general conditions, STA\({}_{k}\) quickly tends to SA. Notice that although STA\({}_{k}\) is a multi-hop graph attention module implemented using the message-passing mechanism, it displays the characteristics of global attention when the height of the subtree is \(( N)\). This property prevents STA from the issues associated with the message-passing scheme such as over-smoothing and over-squashing. From this perspective, subtree attention serves as a bridge connecting local and global attention. STA\({}_{1}\) plays the role of local attention, while STA\({}_{( N)}\) acts as the global self-attention. Subtree attention achieves a hierarchical attention computation by a hop-wise aggregation of \(\{_{k}\}_{k 1,( N)}\). A detailed proof of Theorem 1 is provided in Appendix B.

## 4 The Proposed Multi-Hop Graph Attention Network: STAGN

In this section, we present a simple yet effective multi-hop graph attention network, named STAGN. This model is built upon decoupled GCN, but employs STA as the message-passing module instead. STAGN can be divided into two steps: first, we apply MLP to compute queries, keys, and values for each node; then, we use STA to propagate information. Formally, it can be described as:

\[=_{k=0}^{K}_{k}_{k}(,, ),\;=_{Q},\;= _{K},\;=_{V},\;=( )\] (12)

Figure 2: Overall architecture of STAGN. STAGN can be decomposed into two parts: Transformation and STA-based Propagation, the latter of which can be seen as an STA module using GPR-like aggregation as HopAggregation function. According to the algorithm introduced in subsection 3.2, employing STA here is equivalent to letting keys and values propagate on the graph.

where \(\) is the input node feature and \(\) is the learned representation for each node. We adopt the GPR-like aggregation  for HopAggregation in STA. To be precise, we assign learnable parameters \(\{_{k}\}_{k 0,K}\) to each hop (initialized simply to 1), and then the nodes aggregate information from each hop based on these learned weights. Figure 2 shows the overall architecture of STAGNN.

Comparing Equation 1 and Equation 12, the only difference between decoupled GCN and STAGNN lies in the propagation method. Decoupled GCN relies on high powers of the normalized adjacency matrix to capture long-range information, which inevitably results in over-smoothing. In contrast, STAGNN utilizes subtree attention for message propagation, effectively learning more informative representations from multi-hop neighbors without suffering from the inherent problems associated with the message-passing scheme.

Wu et al. (2022)  have drawn attention to an issue named _over-normalization_: In the context of graphs with a large volume of nodes, the use of the global attention module may lead to a situation where the attention scores for the majority of nodes are nearly zero and thus resulting in gradient vanishing. Subtree attention can alleviate this problem by providing a hierarchical calculation focusing on each level of the rooted subtree instead of the whole graph.

## 5 Evaluation

We evaluate the performance of STAGNN on ten common node classification datasets, with detailed dataset information provided in Appendix C. We then verify the performance of STAGNN under extreme settings, empirically showing its capacity to tackle over-smoothing. Furthermore, We conduct an experiment that confirms the necessity of subtree attention even in the presence of global attention. Additional ablation studies are conducted for further discussions. For implementation, we fix the number of hidden channels at 64. More implementation details are presented in Appendix D. All experiments are conducted on an NVIDIA RTX4090 with 24 GB memory.

   Method & Pubmed & CoraFull & Computer & Photo & CS & Physics \\  GCN & \(86.54_{ 0.12}\) & \(61.76_{ 0.14}\) & \(89.65_{ 0.52}\) & \(92.70_{ 0.20}\) & \(92.92_{ 0.12}\) & \(96.18_{ 0.07}\) \\ GAT & \(86.32_{ 0.16}\) & \(64.47_{ 0.18}\) & \(90.78_{ 0.13}\) & \(93.87_{ 0.11}\) & \(93.61_{ 0.14}\) & \(96.17_{ 0.08}\) \\ APPNP & \(88.43_{ 0.15}\) & \(65.16_{ 0.28}\) & \(90.18_{ 0.17}\) & \(94.32_{ 0.14}\) & \(94.49_{ 0.07}\) & \(96.54_{ 0.07}\) \\ GPRGNN & \(89.34_{ 0.25}\) & \(67.12_{ 0.31}\) & \(89.32_{ 0.29}\) & \(94.49_{ 0.14}\) & \(95.13_{ 0.09}\) & \(96.85_{ 0.08}\) \\  GT & \(88.79_{ 0.12}\) & \(61.05_{ 0.38}\) & \(91.18_{ 0.17}\) & \(94.74_{ 0.13}\) & \(94.64_{ 0.13}\) & \(97.05_{ 0.05}\) \\ Graphormer & OOM & OOM & OOM & \(92.74_{ 0.14}\) & OOM & OOM \\ SAN & \(88.22_{ 0.15}\) & \(59.01_{ 0.34}\) & \(89.83_{ 0.16}\) & \(94.86_{ 0.10}\) & \(94.51_{ 0.15}\) & OOM \\ GraphGPS & \(88.94_{ 0.16}\) & \(55.76_{ 0.23}\) & OOM & \(95.06_{ 0.13}\) & \(93.93_{ 0.12}\) & OOM \\ NAGphormer & \(89.70_{ 0.19}\) & \(71.51_{ 0.13}\) & \(91.22_{ 0.14}\) & \(95.49_{ 0.11}\) & \(95.75_{ 0.09}\) & \(}\) \\  STAGNN & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(97.09_{ 0.18}\) \\   

Table 1: Comparison of four GNN baselines, five Graph Transformer baselines and STAGNN on six common node classification datasets. The best results appear in **bold**.

Figure 3: Comparison of four GNN baselines, three graph structure learning baselines and STAGNN on four common node classification datasets. The missing result of Deezer is due to out-of-memory.

### Experiments on Node Classification

To compare STAGNN with a wide variety of baselines, we select two recent works [43; 6] and follow their experiment settings and their choice of baselines. The metrics of the baselines are adopted from these works [43; 6]. The code for reproduction can be found in supplementary materials.

Comparison with Multi-Hop GNNs and Structure Learning MethodsFor a fair comparison, we strictly follow the experiment settings from Wu et al. (2022) . We test the performance of STAGNN on four common datasets: Cora, Citeseer, Deezer, and Actor. The first two are homogeneous graphs, while the latter two are heterophilic [24; 52; 34]. We compare STAGNN with two mainstream GNNs: GCN , GAT , two multi-hop GNNs: JKNet , MixHop , and three graph structure learning methods: LDS , IDGL , NodeFormer . We apply the same random splits with train/valid/test ratios of 50%/25%/25% as . Further details can be found in Appendix D.1. Figure 3 displays the experimental results, showing that STAGNN, which features a relatively simple model architecture, outperforms all the baselines on all four datasets. This result highlights the effectiveness of STAGNN in managing both homogeneous and heterophilic graphs.

Comparison with Decoupled GCNs and Graph TransformersFor a fair comparison, we strictly follow the experiment settings from Chen et al. (2022) . We test the performance of STAGNN on six common datasets: Pubmed, Corafull, Computer, Photo, CS and Physics. More information about these datasets can be found in Appendix C. We compare STAGNN with two mainstream GNNs: GCN , GAT , two decoupled GCNs: APPNP , GPRGNN , and five graph transformers: GT , Graphormer , SAN , GraphGPS , NAGphormer . We apply the same random splits with train/valid/test ratios of 60%/20%/20% as . The experimental results are shown in Table 1. STAGNN shows comparable or superior performance compared to all the baselines, which highlights the competitiveness of STAGNN when compared to existing GNNs and graph transformers.

### Experiments on Deep STAGNN

We evaluate the performance of deep STAGNN on Cora and Actor, with the height of the subtree ranging from 3 to 100. The experimental results are presented in Figure 4. In contrast to MP-GNNs, STAGNN maintains robust performance even when the height of the subtree reaches 100. For Cora, the accuracy of STAGNN peaks at \(K=10\), demonstrating its ability to effectively collect information from a large receptive field. We further visualize the GPR weights of STAGNN when the height of the subtree is set to \(K=100\), and we observe distinct characteristics for Cora and Actor. In the case of Cora, the GPR weights exhibit a monotonic decrease, which aligns with the witnessed performance drop on Cora as the height of the subtree increases. Therefore, for Cora, we may consider keeping the height of the subtree within a reasonable range. In the case of Actor, the GPR weights eventually stabilize at a value close to 1. This finding suggests that the limiting state of STA\({}_{k}\), i.e., SA\({}_{}\), is suitable for Actor, which is confirmed by the robust performance of deep STAGNN on Actor. More visualizations of GPR weights can be found in Appendix E. In summary, STAGNN can achieve impressive results even with an extremely deep architecture.

Figure 4: **(Left and Middle): Performance of STAGNN across different heights of the rooted subtree. (Right): GPR weights of STAGNN when the height of the subtree \(K=100\).**

### Ablation Study

Study on the Necessity of SubTree Attention in the Presence of Global AttentionWe demonstrate that STA\({}_{k}\) converges to the global self-attention, which naturally leads to a critical question:

\(\) Is it still necessary to employ subtree attention when global attention is already present?

In this experiment, we seek to answer the question above by exploring two scenarios: (i) applying global attention independently and (ii) combining global attention with subtree attention. Formally, we extend STAGNN by replacing the STA module with global attention enhanced by 0, 1, 2, or 3 hop/hops of subtree attention. More details on the experiment settings can be found in Appendix D.2.

The experimental results are presented in Table 2. We find that incorporating subtree attention as an auxiliary to global attention significantly improves overall performance, implying that it is still necessary to employ subtree attention to capture the neighborhood structure even in the presence of global attention. This observation also inspires us that leveraging subtree attention to augment global attention could be an interesting approach for enhancing Graph Transformers.

Study on HopAggregation methodsIn this experiment, we investigate the influence of different HopAggregation functions within STAGNN. We compare GPR-like aggregation with sum, concat , and attention-based readout . Details of the experiment settings can be found in Appendix D.3. The experimental results are shown in Figure 5. GPR-like aggregation outperforms the alternatives on all four datasets. This observation highlights the importance of enabling nodes to adaptively learn the weight of each hop, which is the main advantage of GPR-like aggregation.

## 6 Conclusion

We propose a novel multi-hop graph attention mechanism called Subtree Attention (STA), which effectively addresses the limitations of local and global attention in graph learning. The proposed STA mechanism bridges the gap between local and global attention, hierarchically capturing neighborhood structures while addressing issues associated with the message-passing scheme. The key algorithm for computing subtree attention, utilizing kernelized softmax and the message-passing scheme, reduces the computational cost from \((N^{2})\) to \((||)\) while avoiding the need to store high powers of the adjacency matrix. This process can be approximately viewed as keys and values performing a random walk on the graph. We further prove theoretically that under extreme settings, STA approximates the global self-attention. Based on the STA module, we propose a simple yet performant multi-hop graph attention network, coined STAGNN. Comprehensive evaluations on various node classification datasets demonstrate that STAGNN outperforms mainstream GNNs and graph transformers. Ablation studies further verify the effectiveness of subtree attention, even in the presence of global attention.

   Method & Pubmed & CoraFull & Computer & Photo & CS & Physics \\  GAT & \(86.32_{ 0.16}\) & \(64.47_{ 0.18}\) & \(90.78_{ 0.13}\) & \(93.87_{ 0.11}\) & \(93.61_{ 0.14}\) & \(96.17_{ 0.08}\) \\  Global Attn (GA) & \(88.87_{ 0.61}\) & \(62.34_{ 0.95}\) & \(85.7_{ 0.52}\) & \(92.92_{ 0.32}\) & \(94.74_{ 0.37}\) & \(96.47_{ 0.24}\) \\
1-hop STA + GA & \(90.16_{ 0.51}\) & \(70.65_{ 0.71}\) & \(91.52_{ 0.23}\) & \(95.42_{ 0.47}\) & \(95.49_{ 0.26}\) & \(97.09_{ 0.22}\) \\
2-hops STA + GA & \(90.56_{ 0.49}\) & \(72.24_{ 0.38}\) & \(}\) & \(95.75_{ 0.36}\) & \(95.70_{ 0.29}\) & \(}\) \\
3-hops STA + GA & \(}\) & \(}\) & \(91.89_{ 0.28}\) & \(}\) & \(}\) & \(97.15_{ 0.23}\) \\   

Table 2: Necessity of subtree attention in the presence of global attention. We compare two scenarios: (i) Only Global attention and (ii) Global attention supplemented by subtree attention.

Figure 5: Comparison of different HopAggregation methodsCurrent Limitations, Potential Impacts and Further DiscussionsIn the present work, we mainly evaluate STAGNN, a novel multi-hop graph attention network incorporating STA. However, there are many other potential applications of STA, including combining STA with other graph learning methods that utilize self-attention mechanisms or supplementing global attention with subtree attention to enhance graph transformers. Furthermore, evaluating the robustness of STA and digging into its interpretability can also be part of future works. We provide a detailed discussion of potential impacts in Appendix G. And further analysis of the gate mechanism within the mixture of attention heads can be found in Appendix F.