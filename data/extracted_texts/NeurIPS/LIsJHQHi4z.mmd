# Variational Weighting for Kernel Density Ratios

Sangwoong Yoon

Korea Institute for Advanced Study

swyoon@kias.re.kr

&Frank C. Park

Seoul National University / Saige Research

fcp@snu.ac.kr

&Gunsu Yun

POSTECH

gunsu@postech.ac.kr

&Iljung Kim

Hanyang University

iljung0810@hanyang.ac.kr

&Yung-Kyun Noh

Hanyang University / Korea Institute for Advanced Study

nohyung@hanyang.ac.kr

###### Abstract

Kernel density estimation (KDE) is integral to a range of generative and discriminative tasks in machine learning. Drawing upon tools from the multidimensional calculus of variations, we derive an optimal weight function that reduces bias in standard kernel density estimates for density ratios, leading to improved estimates of prediction posteriors and information-theoretic measures. In the process, we shed light on some fundamental aspects of density estimation, particularly from the perspective of algorithms that employ KDEs as their main building blocks.

## 1 Introduction

One fundamental component for building many applications in machine learning is a correctly estimated density for prediction and estimation tasks, with examples ranging from classification [1; 2], anomaly detection , and clustering  to the generalization of value functions , policy evaluation , and estimation of various information-theoretic measures [7; 8; 9]. Nonparametric density estimators, such as the nearest neighbor density estimator or kernel density estimators (KDEs), have been used as substitutes for the probability density component within the equation of the posterior probability, or the density-ratio equation, with theoretical guarantees derived in part from the properties of the density estimators used [10; 11].

Given a specific task which uses the ratio between two densities, \(p_{1}()\) and \(p_{2}()\) at a point \(^{D}\), we consider the ratio handled by the ratio of their corresponding two KDEs, \(_{1}()\) and \(_{2}()\):

\[_{1}()}{_{2}()}[ ]{}()}{p_{2}()}.\] (1)

Each estimator is a KDE which counts the effective number of data within a small neighborhood of \(\) by averaging the kernels. The biases produced by the KDEs in the nominator and denominator [12, Theorem 6.28] are combined to produce a single bias of the ratio, as demonstrated in Fig. 1. For example, the ratio \(()}{p_{2}()}\) at \(_{0}\) in Fig. 1(a) is clearly expected to be underestimated because of the dual effects in Fig. 1(b): the _under_estimation of the nominator \(p_{1}(_{0})\) and the _over_estimation of the denominator \(p_{2}(_{0})\). The underestimation is attributed to the concavity of \(p_{1}()\) around \(_{0}\) which leads to a reduced number of data being generated compared to a uniform density. The underestimation of \(p_{2}()\) can be explained similarly. The second derivative--Laplacian--that createsthe concavity or convexity of the underlying density is a dominant factor that causes the bias in this example.

The Laplacian of density has been used to produce equations in various bias reduction methods, such as bias correction  and smoothing of the data space . However, the example in Fig. 1 motivates a novel, position-dependent weight function \(()\) to be multiplied with kernels in order to alleviate the bias. For example, to alleviate the overestimation of \(p_{2}(_{0})\), we can consider the \(()\) shown in Fig. 1(c) that assigns more weight on the kernels associated with data located to the left of \(_{0}\), which is a low-density region. When the weighted kernels are averaged, the overestimation of \(_{2}(_{0})\) can be mitigated or potentially even underestimated. Meanwhile, the bias of \(_{1}(_{0})\) remains unchanged after applying the weights since \(p_{1}()\) is symmetric around \(_{0}\). This allows the reversed underestimation of \(p_{2}(_{0})\) from the initial overestimation to effectively offset or counterbalance the underestimation of \(p_{1}(_{0})\) within the ratio.

We derive the \(()\) function that performs this alleviation over the entire data space. The appropriate information for \(()\) comes from the geometry of the underlying densities. The aforementioned principle of bias correction leads to novel, model-based and model-free approaches. Based on the assumption of the underlying densities, we learn the parameters for the densities' first and second derivatives and then variationally adjust \(()\) for the estimator to create the variationally weighted KDE (VWKDE). We note that the model for those densities and their derivatives need not be exact because the goal is not to achieve precise density estimation but rather to accurately capture the well-behaved \(()\) for the KDE ratios.

Applications include classification with posterior information and information-theoretic measure estimates using density-ratio estimation. Calibration of posteriors  has been of interest to many researchers, in part, to provide a ratio of correctness of the prediction. Plug-in estimators of information-theoretic measures, such as the Kullback-Leibler (K-L) divergence, can also be advantageous. For K-L divergence estimation, similar previous formulations for the variational approach have included optimizing a functional bound with respect to the function constrained within the reproducing kernel Hilber space (RKHS) . These and other methods that use weighted kernels (e.g., ) take advantage of the flexibility offered by universal approximator functions in the form of linear combinations of kernels. These methods, however, do not adequately explain why the weight optimization leads to an improved performance. Based on a derivation of how bias is produced, we provide an explicit modification of weight for standard kernel density estimates, with details of how the estimation is improved.

The remainder of the paper is organized as follows. In Section 2, we introduce the variational formulation for the posterior estimator and explain how to minimize the bias. Section 3 shows how a weight function can be derived using the calculus of variations, which is then extended to general density-ratio and K-L divergence estimation in Section 4. Experimental results are presented in Section 5. Finally, we conclude with discussion in Section 6.

Figure 1: Estimation of the density ratio \((_{0})}{p_{2}(_{0})}\) and bias correction using the weight function \(()\). (a) Two density functions, \(p_{1}()\) and \(p_{2}()\), and the point of interest \(_{0}\) for ratio estimation. Two regions delineated by dashed lines are magnified in (b). (b) The concavity and convexity of the density functions around \(_{0}\) and their KDEs. Concave density \(p_{1}()\) generates less data than the uniform density of \(p_{1}(_{0})\) around \(_{0}\) resulting in an underestimation. For a similar reason, convex density \(p_{2}()\) results in an overestimation. The two biases are combined into an underestimation of the ratio. (c) KDE augmented with a nonsymmetric weight function \(()\) can alleviate this bias by transforming the bias of \(_{2}()\) to an appropriate underestimation from an overestimation.

## 2 Variationally Weighted KDE for Ratio Estimation

KDE \(()=_{j=1}^{N}k_{h}(,_{j})\) is conventionally the average of kernels. The average roughly represents the count of data within a small region around \(\), the size of which is determined by a bandwidth parameter \(h\). The amount of convexity and concavity inside the region determines the bias of estimation, as depicted in Fig. 1(a),(b).

### Plug-in estimator of posterior with weight

We consider a weighted KDE as a plug-in component adjusted for reliable ratio estimation using a positive and twice differentiable weight function: \(()\) with \(=\{:^{D}^{+} C ^{2}(^{D})\}\). For two given sets of i.i.d. samples, \(_{1}=\{_{i}\}_{i=1}^{N_{1}} p_{1}()\) for class \(y=1\) and \(_{2}=\{_{i}\}_{i=N_{1}+1}^{N_{1}+N_{2}} p_{2}( )\) for class \(y=2\), we use the following weighted KDE formulation:

\[}()=}_{j=1}^{N_{1}}(_{j})k_{h}(,_{j}),}()= {N_{2}}_{j=N_{1}+1}^{N_{1}+N_{2}}(_{j})k_{h}(, _{j}).\] (2)

Here, the two estimators use a single \(()\). The kernel function \(k_{h}(,^{})\) is a positive, symmetric, normalized, isotropic, and translation invariant function with bandwidth \(h\). The weight function \(()\) informs the region that should be emphasized, and a constant function \(()=c\) reproduces the ratios from the conventional KDE. We let their plug-in posterior estimator be \(f()\), and the function can be calculated using

\[f()=(y=1|)=}()}{ }()+}()}.\] (3)

with a constant \(\) determined by the class-priors.

### Bias of the posterior estimator

We are interested in reducing the expectation of the bias square:

\[[()^{2}]=f()-_{_{1},_{2}}[f()]^{2}p()d .\] (4)

The problem of finding the optimal weight function can be reformulated as the following equation in Proposition 1.

**Proposition 1**.: _With small \(h\), the expectation of the bias square in Eq. (4) is minimized by any \(()\) that eliminates the following function_

\[B_{;p_{1},p_{2}}()=(|_{})^{} ()+g(),\] (5)

_at every point \(\). Here, \(()=(}{p_{1}}-}{p_ {2}})\) and \(g()=(p_{1}}{p_{1}}- p_{2}}{p_{2}})\) with gradient and Laplacian operators, \(\) and \(^{2}\), respectively. All derivatives are with respect to \(\)._

The derivation of Eq. (5) begins with the expectation of the weighted KDE:

\[_{_{1}}[_{1}()] = _{^{} p_{1}()}[( ^{})k_{h}(,^{})]\ =\ (^{})p_{1}(^{})k_{h}(, ^{})^{}\] (6) \[= ()p_{1}()+}{2}^{2}[ ()p_{1}()]+O(h^{3}).\] (7)

Along with the similar expansion for \(_{_{2}}[_{2}()]\), the following plug-in posterior can be perturbed with small \(h\):

\[_{_{1},_{2}}[f()]  _{_{1}}[_{1}()]}{ _{_{1}}[_{1}()]+_{ _{2}}[_{2}()]}\] \[= f()+}{2}()p_{2 }()}{(p_{1}()+ p_{2}())^{2}}([()p_{1}()]}{()p_{1}( )}-[()p_{2}()]}{( )p_{2}()})+(h^{3})\] \[= f()\ +\ }{2}P(y=1|)P(y=2|)B_{ ;p_{1},p_{2}}()\ +\ (h^{3}),\] (9)with the substitution

\[B_{;p_{1},p_{2}}()[()p_{1}( )]}{()p_{1}()}-[( )p_{2}()]}{()p_{2}()}.\] (10)

The point-wise leading-order bias can be written as

\[()=}{2}P(y=1|)P(y=2|)B_{ ;p_{1},p_{2}}().\] (11)

Here, the \(B_{;p_{1},p_{2}}()\) includes the second derivative of \(()p_{1}()\) and \(()p_{2}()\). Because two classes share the weight function \(()\), Eq. (10) can be simplified into two terms without the second derivative of \(()\) as

\[B_{;p_{1},p_{2}}()=_{}}{()}(_{}}{p_{1}( )}-_{}}{p_{2}()} )+(p_{1}_{}}{p_{1}( )}-p_{2}_{}}{p_{2}()} ),\] (12)

which leads to Eq. (5) in the Proposition. The detailed derivation in this section can be found in Appendix A.

### Plug-in estimator of K-L divergence with weight

In order to estimate \(KL(p_{1}||p_{2})=_{ p_{1}}[( )}{p_{2}()}]\), we consider the following plug-in estimator:

\[(p_{1}||p_{2})=}_{i=1}^{N_{1}}_{1}(_{i})}{_{2}(_{i})},\] (13)

using \(_{i}\) in \(_{i}_{1}\) for Monte Carlo averaging. When we calculate \(_{1}\) at \(_{i}\), we exclude \(_{i}\) from the KDE samples. We use \(}(_{i})=-1}_{j=1}^{N_{1}}( _{j})k_{h}(_{i},_{j})_{(i j)}\) with the indicator function \(_{()}\), which is 1 if \(\) is true and 0 otherwise.

**Proposition 2**.: _With small \(h\), the expectation of the bias square is minimized by finding any \(()\) that eliminates the same function as \(B_{;p_{1},p_{2}}()\) in Eq. (5) in Proposition 1 at each point \(\)._

In the task of estimating the K-L divergence, Proposition 2 claims that we obtain the equivalent \(B_{;p_{1},p_{2}}()\) to Eq. (5) during the derivation of bias. The pointwise bias in the K-L divergence estimator can be written as

\[()=}{2}()}{p_{2}( )}B_{;p_{1},p_{2}}(),\] (14)

with \(B_{;p_{1},p_{2}}()\) equivalent to Eq. (5).

## 3 Variational Formulation and Implementation

Now we consider the \(()\) that minimizes the mean square error for the estimation:

\[_{()}((|_{ })^{}()+g())^{2}r( )d,\] (15)

Figure 2: Posterior estimates with KDEs and VEKDEs for two 20-dimensional homoscedastic Gaussian densities. (a) Bias corrected by VWKDE. (b) Bias and variance of posterior estimates depending on the bandwidth for standard KDE and for (c) VWKDE.

with \(()=(}{p_{1}}-}{p_{2}})\), \(g()=(p_{1}}{p_{1}}-p_ {2}}{p_{2}})\). The \(r()\) function depends on the problem: \(r()=P(y=1|)^{2}P(y=2|)^{2}p()\) for posterior estimation and \(r()=(()}{p_{2}()})^{2}p( )\) for K-L divergence estimation, with the total density, \(p()=(p_{1}()+ p_{2}())P(y=1)\). The calculus of variation for optimizing the functional in Eq. (15) provides an equation that the optimal \(()\) should satisfy:

\[[r(()^{\!}+g)]=0.\] (16)

The detailed derivation of this equation can be found in Appendix B.

### Gaussian density and closed-form solution for \(()\)

A simple analytic solution for this optimal condition can be obtained for two homoscedastic Gaussian density functions. The density functions have two different means, \(_{1}^{D}\) and \(_{2}^{D}\), but share a single covariance matrix \(^{D D}\):

\[p_{1}()=(;_{1},), p_{2}( )=(;_{2},).\] (17)

One solution for this homoscedastic setting can be obtained as

\[()=(-(-^{})^{}A( -^{})),\] (18)

with \(^{}=+_{2}}{2}\) and \(A=b(I-(_{1}-_{2})(_{1}-_{2})^{}^{- 1}}{||^{-1}(_{1}-_{2})||^{2}})-^{-1}\) using an arbitrary constant \(b\). Due to the choice of \(b\), the solution is not unique. All the solutions produce a zero bias. Its detailed derivation can be found in the Appendix C. The reduction of the bias using Eq. (18) with estimated parameters is shown in Fig. 2.

### Implementation

In this work, we propose a model-free method and a mode-based method. The model-free approach uses the information of \( p_{1}()\) and \( p_{2}()\) estimated by a score matching neural network . We obtain the second derivative, \(^{2} p\), by the automatic differentiation of the neural network for the scores. We then obtain \(p}}{p}\) using \(p}}{p}=^{2} p-^{ } p p\). With the outputs of the neural networks for \( p\) and \(p}}{p}\), we train a new network for the function \((;)\) with neural network parameters \(\).

On the other hand, the model-based approach uses a coarse Gaussian model for class-conditional densities. The Gaussian functions for each class have their estimated parameters \(_{1},_{2}^{D}\) and \(_{1},_{2}^{D D}\). We use the score information from these parameters: \( p_{1}()=_{1}^{-1}(- })\) and \( p_{2}()=_{2}^{-1}(- })\). In the model-based approach, we let the log of \(()\) be a function within the RKHS with basis kernels \(_{}(,)\) with kernel parameter \(\). We let \((;)=_{i=1}^{N_{1}+N_{2}}_{i}_{} (,_{i})\) with parameters \(=\{_{1},,_{N_{1}+N_{2}}\}\) for optimization.

The weight function \(()\) is obtained by optimizing the following objective function with \(N_{1}\) number of data from \(p_{1}()\) and \(N_{2}\) number of data from \(p_{2}()\):

\[L()=_{i=1}^{N_{1}+N_{2}}(^{}( _{i};)}(_{i}))^{\!\!2}\!\!+ ^{}(_{i};)}(_ {i})(_{i}),\] (19)

with the substitutions \(}()= p_{1}()-  p_{2}()\) and \(()=(p_{1}( )}}{p_{1}()}-p_{2}()}} {p_{2}()})\).

In the model-based method, an addition of \(_{2}-\)regularizer, \(_{i=1}^{N_{1}+N_{2}}_{i}^{2}\), with a small positive constant \(\) makes the optimization (19) quadratic. When there are fewer than 3,000 samples, we use all of them as basis points. Otherwise, we randomly sample 3,000 points from \(\{_{i}\}_{i=1}^{N_{1}+N_{2}}\).

A brief summary of the implementation process is shown in Algorithms 1 and 2.1``` Input:\(\), \(\{_{i}\}_{i=1}^{N_{1}}\!\!p_{1}\), \(\{_{i}\}_{i=N_{1}+1}^{N_{1}+N_{2}}\!\!p_{2}\) Output: Ratio \(()\) (\(=}/}()\)) Procedure:
1. Estimate \( p_{1}\) using \(\{_{i}\}_{i=1}^{N_{1}}\!\!p_{1}\).
2. Estimate \( p_{2}\) using \(\{_{i}\}_{i=N_{1}+1}^{N_{1}+N_{2}}\!\!p_{2}\).
3. Obtain \(()\) that minimizes Eq. (19)
3. \(()=^{N_{1}}(_{i})k_{h}( ,_{i})}{_{i=N_{1}+1}^{N_{1}+N_{2}}(_{i })k_{h}(,_{i})}\) Return\(()\) ```

**Algorithm 1** Model-free

## 4 Interpretation of \(()\) for Bias Reduction

The process of finding \(()\) that minimizes the square of \(B_{;p_{1},p_{2}}()\) in Eq. (5) can be understood from various perspectives through reformulation.

### Cancellation of the bias

The second term \((p_{1}}{p_{1}}-p_{2}}{p_{2}})\) in Eq. (5) repeatedly appears in the bias of nonparametric processes using discrete labels . In our derivation, the term is achieved with a constant \(()\) or with no weight function. The role of the weight function is to control the first term \(}{}(}{p_{1}}-}{p_{2}})\) based on the gradient information in order to let the first term cancel the second.

### Cancellation of flow in a mechanical system

The equation for each class can be compared with the mechanical convection-diffusion equation, \(=-^{} u+D^{}^{2}u\), which is known as the equation for Brownian motion under gravity  or the advective diffusion equation of the incompressible fluid . In the equation, \(u\) is the mass of the fluid, \(t\) is the time, \(\) is the direction of convection, and \(D^{}\) is the diffusion constant. The amount of mass change is the sum of the convective movement of mass along the direction opposite to \( u\) and the diffusion from the neighborhood. We reorganize Eq. (5) into the following equation establishing the difference between the convection-diffusion equations of two fluids:

\[B_{;p_{1},p_{2}}() = [^{}\!(+ p_{1})  p_{1}+^{2} p_{1}]\] (20) \[-[^{}\!(+ p_{2})  p_{2}+^{2} p_{2}].\]

According to the equation, we can consider the two different fluid mass functions, \(u_{1}()= p_{1}()\) and \(u_{2}= p_{2}()\), and the original convection movement along the directions \(_{1}^{}=- p_{1}\) and \(_{2}^{}=- p_{2}\). If we make an \(()\) that modifies the convection directions \(_{1}^{}\) and \(_{2}^{}\) to \(_{1}=_{1}^{}-\) and \(_{2}=_{2}^{}-\), and a mass change in one fluid is compensated by the change of the other, in other words, if \(}{ t}=}{ t}\), then the \(()\) is the weight function that minimizes the leading term of the bias for ratio estimation.

### Prototype modification in reproducing kernel Hilbert space (RKHS)

A positive definite kernel function has its associated RKHS. A classification using the ratio of KDEs corresponds to a prototype classification in RKHS that determines which of the two classes has a closer mean than the other [29, Section 1.2]. The application of a weight function corresponds to finding a different prototype from the mean . The relationship between the new-found prototype and the KDEs has been previously discussed .

## 5 Experiments

### Estimation of log probability density ratio and K-L divergence in 1D

We first demonstrate in Fig. 3 how the use of VWKDE alters log probability density ratio (LPDR) and K-L divergence toward a better estimation. We use two 1-dimensional Gaussians, \(p_{1}(x)\) and \(p_{2}(x)\), with means 0 and 1 and variances \(1.1^{2}\) and \(0.9^{2}\), respectively. We draw 1,000 samples from each density and construct KDEs and model-based VWKDEs for both LPDR and K-L divergence. For LPDR evaluation, we draw a separate 1,000 samples from each density, and the average square of biases and the variances at those points are calculated and presented in Fig. 3(b). The K-L divergence estimation result is shown in Fig. 3(c), where the true K-L divergence can be calculated analytically as \(KL(p_{1}||p_{2}) 0.664\), and the estimated values are compared with this true K-L divergence.

KDE-based LPDR estimation exhibits a severe bias, but this is effectively reduced by using VWKDE as an alternative plug-in. Although VWKDE slightly increases the variance of estimation, the reduction of bias is substantial in comparison. Note that since the bias is small over a wide range of \(h\), VWKDE yields a K-L divergence estimate which is relatively insensitive to the choice of \(h\).

### Synthetic distributions

We perform the VWKDE-based K-L divergence estimator along with other state-of-the-art estimators to estimate the K-L divergence \(KL(p_{1}||p_{2})\) between two synthetic Gaussian distributions \(p_{1}\) and \(p_{2}\) having \(_{1}\) and \(_{2}\) as their mean vectors and \(_{1}\) and \(_{2}\) as their covariance matrices, respectively.

Figure 4: K-L divergence estimation results for synthetic distributions; (NN) Nearest-neighbor estimator; (NNG) NN estimator with metric learning ; (KLIEP) Direct importance estimation ; (NNGarcia) Risk-based \(f\)-divergence estimator ; (NNWang) Bias-reduced NN estimator ; (MIN) Mutual Information Neural Estimation ; (Ensemble) Weighted ensemble KDE estimator ; (vonMises) KDE estimator with von Mises expansion bias correction ; (KDE) KDE estimator; (VWKDE-MB, VWKDE-MF) Model-based and model-free approach of the proposed estimator in this paper.

Figure 3: Estimation results of the LPDR \((p_{1}/p_{2})\) and the K-L divergence. (a) Estimation of LPDR at each point. The estimation bias from the true LPDR is reduced by using VWKDE. (b) Squared bias and variance of the estimation with respect to the bandwidth \(h\). Bias has been significantly reduced without increasing variance. (c) Mean and standard deviation of K-L divergence estimates with respect to the bandwidth \(h\).

We use three different settings for \(p_{1}\) and \(p_{2}\): Isotropic **Iso**, Non-isotropic Heteroscedastic (**NH**), and Varying Mean Diff (**VMD**). In **Iso**, \(_{1}=_{2}=I\) for an identity matrix \(I\). \(_{1}=\) for a zero vector \(\). The first element of \(_{2}\) is \(\), while the other elements are uniformly zero, resulting in \(KL(p_{1}||p_{2})=1\). In **NH**, \(_{1}=_{2}=\), and \(_{1}=\). \(_{2}\) is a matrix having a pair of off-diagonal element \((_{2})_{1,2}=(_{2})_{2,1}=0.1\), and other off-diagonal elements are zero. The diagonal elements have a constant value \(\), which is determined to yield \(KL(p_{1}||p_{2}) 1.0\) with \(=0.750^{2}\) (10D) and \(KL(p_{1}||p_{2}) 0.5\) with \(=0.863^{2}\) (20D). In **VMD**, the first element of \(_{2}\) has various values between 0 and 2, while \(_{1}=\) and the other elements in \(_{2}\) remain zero. \(_{1}=_{2}=I\). In **Iso** and **NH**, we vary the sample size, and in **VMD**, we use 2,000 samples per distribution. We repeat each experiment 30 times and display the mean and the standard deviation in Figure 4.

In the upper left panel of Figure 4, we observe that almost all algorithms estimate the low-dimensional K-L divergence reliably, but the results deteriorate dramatically as shown in the upper middle and right panels with high-dimensionality. As shown in the lower left and lower middle panels, most of the baseline methods fail to produce the estimates near the true value when data are correlated. The model-based VWKDE-based estimator is the only estimator that recovers the true value in the 20D **NH** case. Figure 5 shows the K-L divergence estimation for non-Gaussian densities. In this example, the model for the score function in model-based VWKDE is different from the data-generating density, in which the estimator still shows very reliable estimates.

### Unsupervised optical surface inspection

We apply the proposed K-L divergence estimation using VWKDE for the inspection of the surface integrity based on the optical images. Most of the previous works have formulated the inspection using the _supervised_ setting ; however, often the defect patterns are diverse, and training data do not include all possible defect patterns. In real applications, identification and localization of _unseen_ defect patterns are important. In this example, we apply the model-based VWKDE.

Detection of defective surfaceFollowing the representations of previous works on image classification , we extract random small patches from each image \(\) and assume that those patches are the independently generated data. We use the probability density \(p_{}()\), for the patch \(^{D}\) from \(\). Given the \(N\) number of normal surface images \(=\{_{i}\}_{i=1}^{N}\) and a query image \(^{*}\), we determine whether \(^{*}\) is a defective surface according to the following decision function \(f(^{*})\) and a predefined

Figure 5: Estimation of K-L divergence between two non-Gaussian densities, \(p_{1}()\) and \(p_{2}()\). Each density is the Gaussian mixture of the three Gaussians, as shown in the 2-dimensional density contour in the figure on the left. They are the true densities but are very dissimilar to the single Gaussian model. The figure in the middle shows the estimation with 2-dimensional data, and the figure on the right shows the estimation with 20-dimensional data. With 20-dimensional data, the remaining 18 dimensionalities have the same mean isotropic Gaussians without correlation to the first two dimensionalities.

Figure 6: Detection of an artificially injected defect (MNIST digit ”3”). The first panel shows the image with an injected defect. The remaining three panels show the detection scores of different methods.

threshold:

\[f(^{*})=_{_{i}}(p_{^{*} }||p_{_{i}}).\] (21)

Defect localizationOnce the defective surface is detected, the spot of the defect can be localized by inspecting the LPDR \((p_{^{*}}()/p_{_{m}}())\) score between the query image \(^{*}\) and the \(_{m}\) with \(_{m}=_{_{i}}(p_{^{*}}||p_{_{i}})\). The location of the patch \(\) with the large LPDR score is considered to be the defect location. Note that a similar approach has been used for the witness function in statistical model criticism .

For the evaluation of the algorithm, we use a publicly available dataset for surface inspection: DAGM2. The dataset contains six distinct types of normal and defective surfaces. The defective samples are not used in training, but they are used in searching the decision thresholds. We extract 900 patches per image, and each patch is transformed into a four-dimensional feature vector. Then, the detection is performed and compared with many well-known criteria: diverse K-L divergences estimators as well as the maximum mean discrepancy (MMD)  and the one-class support vector machines (OSVM) . In addition, the Convolutional Neural Networks (CNNs) training result is presented for comparison with a supervised method.

In DAGM, the testing data have defect patterns similar to those in the training data. To demonstrate unseen defect patterns, we artificially generate defective images by superimposing a randomly selected 15% of the normal testing images with a small image of the MNIST digit '3' at a random location (see Figure 6). Table 1 presents the area under curve (AUC) of the receiver operating characteristic curve for the detection as well as the mean average precision (mAP) for the localization.

CNNs which use labels for training show good performances only in detecting and localizing DAGM defects. The K-L divergence estimation with VWKDE show the best performance over many unsupervised methods, and it provides significantly better performances both at identifying unseen defects and at localizing them. Figure 6 shows one example of how well the proposed method localizes the position of the unseen defects.

## 6 Conclusion

In this paper, we have shown how a weighted kernel formulation for the plug-in densities could be optimized to mitigate the bias in consideration of the geometry of densities. The underlying mechanism uses the information from the first derivatives to alleviate the bias due to the second derivatives.

In our experiments, a simple choice of Gaussian density model for obtaining the first and second derivatives led to a reliable reduction of bias. This insensitivity to the exactness due to a coarse model is nonintuitive considering the traditional dilemma prevalent in many conventional methods; a coarse and inexact model enjoys a small variance but at the cost of large bias. In our work, the usage of the coarse model had no effect on the flexibility of the plug-in estimator, while the high dimensional bias was tackled precisely.

   mAUC & DAGM Defect & Unseen Defect \\ 
**VWKDE** & **0.785 \(\) 0.002** & **0.967 \(\) 0.003** \\ KDE & 0.734 \(\) 0.005 & 0.926 \(\) 0.003 \\ NN-1 & 0.628 \(\) 0.002 & 0.813 \(\) 0.001 \\ NN-10 & 0.540 \(\) 0.003 & 0.614 \(\) 0.002 \\ NNWang & 0.605 \(\) 0.002 & 0.657 \(\) 0.004 \\ MMD & 0.618 \(\) 0.003 & 0.615 \(\) 0.008 \\ OSVM & 0.579 \(\) 0.001 & 0.538 \(\) 0.000 \\  CNN & 0.901 \(\) 0.011 & 0.809 \(\) 0.029 \\    
   mAP & DAGM Defect & Unseen Defect \\ 
**VWKDE** & **0.369 \(\) 0.005** & **0.903 \(\) 0.007** \\ KDE & 0.294 \(\) 0.004 & 0.849 \(\) 0.006 \\ NN-1 & 0.095 \(\) 0.008 & 0.488 \(\) 0.002 \\ NN-10 & 0.081 \(\) 0.004 & 0.254 \(\) 0.002 \\ NNWang & 0.029 \(\) 0.005 & 0.024 \(\) 0.000 \\ MMD & 0.151 \(\) 0.006 & 0.032 \(\) 0.001 \\ OSVM & 0.249 \(\) 0.012 & 0.444 \(\) 0.009 \\  CNN & 0.699 \(\) 0.037 & 0.564 \(\) 0.060 \\   

Table 1: Performances for defect surface detection (left) and defect localization (right). mAUC and mAP are averaged over six surface types of DAGM. The DAGM dataset is provided with labels, and only CNN used the labels for training. The unseen defect is the artificially injected MNIST digit “3.”Limitations of this study include the computational overhead for score learning using parametric or neural network methods and no benefit for the asymptotic convergence rate because it depends on the convergence rate of KDE. Using a non-flexible parametric model rather than a flexible one provides a consistent benefit to improve the KDE.