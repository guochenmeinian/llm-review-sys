# Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks

Jules Berman

Courant Institute for Mathematical Sciences

New York University

New York, NY 10012

jmb1174@nyu.edu

&Benjamin Peherstorfer

Courant Institute for Mathematical Sciences

New York University

New York, NY 10012

pehersto@cims.nyu.edu

###### Abstract

Training neural networks sequentially in time to approximate solution fields of time-dependent partial differential equations can be beneficial for preserving causality and other physics properties; however, the sequential-in-time training is numerically challenging because training errors quickly accumulate and amplify over time. This work introduces Neural Galerkin schemes that update randomized sparse subsets of network parameters at each time step. The randomization avoids overfitting locally in time and so helps prevent the error from accumulating quickly over the sequential-in-time training, which is motivated by dropout that addresses a similar issue of overfitting due to neuron co-adaptation. The sparsity of the update reduces the computational costs of training without losing expressiveness because many of the network parameters are redundant locally at each time step. In numerical experiments with a wide range of evolution equations, the proposed scheme with randomized sparse updates is up to two orders of magnitude more accurate at a fixed computational budget and up to two orders of magnitude faster at a fixed accuracy than schemes with dense updates.

## 1 Introduction

In science and engineering, partial differential equations (PDEs) are frequently employed to model the behavior of systems of interest. For many PDEs that model complicated processes, an analytic solution remains elusive and so computational techniques are required to compute numerical solutions.

Global-in-time trainingThere have been many developments in using nonlinear parameterizations based on neural networks for numerically approximating PDE solutions. These include techniques such as the Deep Galerkin Method , physics-informed neural networks (PINNs) , and others [4; 21; 53; 15]; as well as early works such as [11; 42]. In most of these methods, a neural network is used to represent the solution of a (time-dependent) PDE over the whole space-time domain. For this reason they are termed global-in-time methods in the following. To approximate the solution, the neural network is trained to minimize the PDE residual on collocation points sampled from the space-time domain, which requires solving a large-scale optimization problem that can be computationally expensive. Additionally, the solutions learned by global-in-time methods can violate causality, which can become an issue for complex problems that rely on preserving physics . We note that neural networks have been used for approximating PDE solutions in various other ways, such as learning specific component functions [23; 31; 43], finding closure models [2; 25; 50], de-noising , and for surrogate modeling [33; 32; 18]. However, we are interested in this work in using neural networks for directly approximating PDE solutions.

Sequential-in-time training with the Dirac-Frenkel variational principleIn this work, we follow the Dirac-Frenkel variational principle, which has been used for numerical methods in the field of quantum dynamics for a long time [10; 17; 26; 34; 28] and for dynamic-low rank and related solvers [24; 45; 35; 39; 38; 22]. Instead of globally approximating a PDE solution in time, the Dirac-Frenkel variational principle allows a sequential-in-time training that adapts a nonlinear parameterization, such as a neural network, over time. In contrast to classical numerical methods in vector spaces, the approximate solution in Dirac-Frenkel schemes is allowed to depend nonlinearly on its parameters and so to lie on a smooth manifold. The update to the nonlinear parameterization is calculated at each time step according to the orthogonal projection of the dynamics onto the tangent space of the manifold induced by the nonlinear parameterization. The Dirac-Frenkel variational principle has been adapted for the nonlinear approximation of PDEs with neural networks. In particular [14; 1; 7] formulate a sequential-in-time method based on the Dirac-Frenkel variational principle.

The neural network represents the PDE solution at a point in time. The time-dependence then arises by allowing the parameters--the weights and biases of the network--to vary in time. The network parameters are then evolved forward according to the time dynamics which govern the PDE. This is in contrast to global-in-time methods, in which time enters the network as an additional input variable. By construction, an approximate solution obtained with a sequential-in-time method is causal, in that the solution at future times depends only on the solution at the current time.

Although these methods have demonstrated success in solving various PDEs [34; 14; 7; 54; 16; 8; 30], there are open challenges: First, the sequential-in-time training is prone to overfitting which can lead to a quick accumulation of the residual over time. Second, the local training step has to be repeated at each time step, which can be computationally costly, especially with direct solvers that have costs increase quadratically with the number of network parameters. The work  proposes to address the two issues by using iterative solvers, instead of direct ones, and by re-training the network occasionally over the sequential-in-time training. We show with numerical experiments below that the re-training of the network can be computationally expensive. Additionally, the performance of iterative solvers depends on the condition of the problem, which can be poor in the context of sequential-in-time training.

Our approach and contributions: Randomized sparse updates for schemes based on the Dirac-Frenkel variational principleWe build on the previous work in sequential-in-time methods following a similar set up as  based on the Dirac-Frenkel variational principle. Where all previous methods solve local training problems that update every parameter of the network at each time step, we propose a modification such that only randomized sparse subsets of network parameters are updated at each time step:

**(a)** The randomization avoids overfitting locally in time and so helps preventing the error from accumulating quickly over the sequential-in-time training, which is motivated by dropout that addresses a similar issue of overfitting due to neuron co-adaptation.

**(b)** The sparsity of the updates reduces the computational costs of training without losing expressiveness because many of the network parameters are redundant locally at each time step.

Our numerical experiments indicate that the proposed scheme is up to two orders of magnitude more accurate at a fixed computational cost and up to two orders of magnitude faster at a fixed accuracy.

We release our code implementation here: https://github.com/julesberman/RSNG

## 2 Sequential-in-time training for solving PDEs

### Evolution equations, Dirac-Frenkel variational principle, Neural Galerkin schemes

Given a spatial domain \(^{d}\), and a time domain \(=[0,T)\), we consider a solution field \(u:\) so that \(u(t,.):\) is in a function space \(\) at each time \(t\), with dynamics

\[_{t}u(t,)= f(,u)(t,) \] (1) \[u(0,)= u_{0}()\]

where \(u_{0}\) is the initial condition and \(f\) can include partial derivatives of \(u\) to represent PDEs. We focus in this work on Dirichlet and periodic boundary conditions but the following approach can be applied with, e.g., Neumann boundary conditions as well . One approach for imposing Dirichlet boundary conditions is by choosing parameterizations that satisfy the boundary conditions by definition .

Sequential-in-time training methods approximate \(u\) with a nonlinear parameterization such as a neural network \(:\), where the parameter vector \((t)^{p}\) depends on time \(t\); the parameter \((t)\) has \(p\) components and enters nonlinear in the second argument of \(\). The residual of (1) at time \(t\) is

\[r(;(t),}(t))=_{ }(;(t))}(t)-f(,(;(t)))\,,\] (2)

where we applied the chain rule to \(_{t}(;(t))\) to formally obtain \(}(t)\). Methods based on the Dirac-Frenkel variational principle [10; 17; 34] seek \(}(t)\) such that the residual norm is minimized, which leads to the least-squares problem

\[_{}(t)}\|_{(t)}( ;(t))}(t)-f(;( ;(t)))\|_{L^{2}()}^{2}\,,\] (3)

in the \(L^{2}()\) norm \(\|\|_{L^{2}()}\) over \(\). The least-squares problem (3) gives a \(}(t)\) such that the residual is orthogonal to the tangent space at \((;(t))\) of the manifold \(_{}=\{(;)\,|\,\}\) induced by the parameterization \(\); see Figure 1. Schemes that solve (3) over time have also been termed Neural Galerkin schemes  because (3) can be derived via Galerkin projection as well.

The tangent space at \((;(t))\) is spanned by the spanning set \(\{_{_{i}}(;(t))\}_{i=1}^{p}\), which are the component functions of the gradient \(_{(t)}(;(t))\); it is important to stress that \(\{_{_{i}}(;(t))\}_{i=1}^{p}\) is _not_ necessarily a basis of the tangent space because it can contain linearly dependent functions and be non-minimal. The least-squares problem (3) can be realized by assembling a matrix whose columns are the gradient sampled at \(n p\) points \(_{1},,_{n}\) resulting in \(J((t))=[_{(t)}( {x}_{1};(t)),,_{(t)} (_{n};(t))]^{T}^{n p}\), which is a batch Jacobian matrix to which we refer to as Jacobian for convenience in the following. Additionally, we form the right-hand side vector \(((t))=[f(_{1};(t)),,f(_{n};(t))]^{T} ^{n}\) and thus the least-squares problem

\[_{}(t)}\|J((t))}(t)-((t))\|_{2}^{2}\,.\] (4)

The choice of the points \(_{1},,_{n}\) is critical so that solutions of (4) are good approximations of solutions of (3); however, the topic of selecting the points \(_{1},,_{n}\) goes beyond this work here and so we just note that methods for selecting the points exist [1; 7] and that we assume in the following that we select sufficient points with \(n p\) to ensure that solutions of (4) are good approximations of solutions of (3)

### Problem Formulation

Challenge 1: Parameters are redundant locally in time.Typically, parameterizations \(\) based on deep neural networks lead to Jacobian matrices \(J((t))\) that are low rank in the least-squares problem (4); see, e.g.,  and Figure 2(a). In our case, a low-rank matrix \(J((t))\) means that components in \(}(t)\) are redundant, because we assume that the samples \(_{1},,_{n}\) are sufficiently rich. Even if \(J((t))\) is low rank and thus the components in \(}(t)\) are redundant, the problem (4) can still be

Figure 1: We propose Neural Galerkin schemes that update randomized sparse subsets of network parameters with the Dirac-Frenkel variational principle. Randomization avoids overfitting locally in time, which leads to more accurate approximations than dense updates. Sparsity reduces the computational costs of training without losing expressiveness because many parameters are redundant locally in time.

solved with standard linear algebra methods such as the singular value decomposition (SVD) because they compress the matrix \(J((t))\) and regularize for, e.g., the minimal-norm solution; however, the costs of performing the SVD to solve (4) scales as \((np^{2})\), and thus is quadratic in the number of parameters \(p\). This means that a redundancy in \(}(t)\) of a factor two leads to a \(4\) increase in the computational costs. Note that the problem typically is poorly conditioned because \(J((t))\) is low rank, which makes the direct application of iterative solvers challenging.

Challenge 2: Overfitting leads to high residual over time.The residual from solving (4) can rapidly increase over time which in turn increases the overall error. This indicates that the tangent space along the trajectory \((t)\) becomes ill suited for approximating the right-hand side vector \(((t))\) in (4). We compare the residual of the least-squares problem (4) that is obtained along a trajectory of \((t)\) from sequential-in-time training with the schemes above to the residual of (4) from a network that is fit to the true solution at each point in time; details in Appendix A.1. As shown in Figure 2(b), a lower residual is achieved by the network that is fit to the true solution.

We aim to understand this phenomenon through the lens of overfitting: the sequential-in-time training can be thought of as successive fine-tuning, in the sense that at each time step we must make a small update to our parameters to match the solution at the next time step. However, fine-tuning is well known to be prone to over-fitting and model degeneration . In the setting considered in this work, overfitting means that the representation \((;(t))\) does not generalize well to the next time step. Not generalizing well means that a local change to \((t)\) is insufficient to move \((;(t))\) according to the desired update given by \(}(t)\) to match the right-hand side \(((t))\), which implies that a large residual is incurred when solving (4). A common approach to prevent overfitting is dropout , especially when applied to fine-tuning tasks with dropout variants proposed in , while other approaches are formulated specifically around sparse updates . Dropout is motivated by the observation that dense updates to parameters in neural networks can cause overfitting by leading neurons to co-adapt. Typically, co-adaptation is characterized by layer-wise outputs with high covariance . In the case of sequential-in-time training with the schemes discussed above, co-adaptation implies the columns of the Jacobian matrix \(J((t))\) are correlated and thus close to linearly dependent. So as neurons co-adapt, component functions of the gradient become redundant and may be less suited for approximating \(((t))\) causing the high residual for the least-squares problem; see Figure 2(b). This could also be characterized by the ill conditioning issue pointed out in . We see empirical evidence of co-adaptation in Figure 2(c), where we plot component functions of the gradient and see that they are strongly correlated at the end time \(T\).

Figure 2: (a) Jacobians that have low rank locally in time imply that there are redundant parameters in the neural network, which motivates the proposed sparse updates that lead to speedups without losing expressiveness. (b) The residual grows quickly with sequential-in-time training (and dense updates). This is not due to a limitation with the expressiveness of the network because directly fitting the network to solutions indicates that there exist other parameters that can lead to lower residuals. (c) Sequential-in-time training (with dense updates) results in co-adapted neurons as indicated by the highly correlated columns of the \(J\) matrix. Plots for experiment with Allen-Cahn equation (Sec. 4).

Randomized Sparse Neural Galerkin (RSNG) schemes

We introduce randomized sparse Neural Galerkin (RSNG) schemes that build on the Dirac-Frenkel variational principle to evolve network parameters \((t)\) sequentially over time \(t\) but update only sparse subsets of the components of \((t)\) and randomize which components of \((t)\) are updated. The sparse updates reduce the computational costs of solving the least-squares problem (4) while taking advantage of the low rank structure of \(J()\) which indicates components of the time derivative \(}(t)\) are redundant and can be ignored for updating \((t)\) without losing expressiveness. The randomization of which components of \((t)\) are updated prevents the overfitting described above.

### Randomized sketch of residual

To define the sketch matrix \(S_{t}\), let \(_{1},,_{p}\) be the \(p\)-dimensional canonical unit vectors so that \(_{i}\) has entry one at component \(i\) and zeros at all other components. We then define \(s\) independent and identically distributed random variables \(_{1}(t),,_{s}(t)\) that depend on time \(t\). The distribution of \(_{i}(t)\) is \(\), which is supported over the set of indices \(\{1,,p\}\). The random matrix \(S_{t}\) of size \(p s\) is then defined as \(S_{t}=[_{_{1}(t)},,_{_{s}(t)}]\). The corresponding sketched residual analogous to (2) is

\[r_{s}(;(t),}_{s}(t))=_{} {u}(;(t))S_{t}}_{s}(t)-f(,( ;(t))\,,\] (5)

where now \(}_{s}(t)^{s}\) is of dimension \(s p\).

### Projections onto randomized approximations of tangent spaces

Using the sketch matrix \(S_{t}\), we obtain from the spanning set \(\{_{_{i}}(;(t))\}_{i=1}^{p}\) of component functions of \(_{}(;(t))\) a subset \(\{_{_{_{i}(t)}}(;(t))\}_{i=1}^{s}\) with \(s\) functions. The set \(\{_{_{_{i}(t)}}(;(t))\}_{i=1}^{s}\) spans at least approximately the tangent space at \((;(t))\) of \(_{}\) but has only \(s p\) elements. The motivation is that the full spanning set \(\{_{_{i}}(;(t))\}_{i=1}^{p}\) contains many functions that are close to linearly dependent (Jacobian is low rank) and thus sub-sampling the component functions still gives reasonable tangent space approximations that preserves much of the expressiveness; see Figure 1. While the low rankness depends on the complexity of the problem and parametrization, we observe low rankness in all our examples; see Appendix A.1 for further discussion.

We now introduce a least-squares problem based on the sparse spanning set \(\{_{_{_{i}(t)}}(;(t))\}_{i=1}^{s}\) that is analogous to the least-squares problem problem based on the full spanning set given in (4). We seek \(}_{s}(t)^{s}\) with \(s\) components that solves

\[_{}_{s}(t)^{s}}\|_{}( ;(t))S_{t}}_{s}(t)-f(;(; {}(t)))\|_{L^{2}()}^{2}\,.\] (6)

To obtain \(}(t)\) to update \((t)\), we set \(}(t)=S_{t}}_{s}(t)\). Thus, the components of \(}(t)\) that are selected by \(S_{t}\) are set to the corresponding value of the component of \(}_{s}(t)\) and all other components are set to zero, which means that the corresponding components of \((t)\) are not updated. We can realize (6) the same way as the full least-squares problem in (4) by using the full Jacobian matrix and \(S_{t}\) to define the sparse Jacobian matrix as \(J_{s}((t))=J((t))S_{t}\) and the right-hand side vector \(((t))\) analogous to Section 2 to obtain the discrete least-squares problem

\[_{}_{s}(t)}\|J_{s}((t))}_{s}(t)- ((t))\|_{2}^{2}\,.\] (7)

The choice of the distribution \(\) is critical and depends on properties of the Jacobian matrix \(J((t))\). Distributions based on leverage scores provide tight bounds with regard to the number of columns one needs to sample in order for the submatrix to be close to an optimal low rank approximation of the full matrix with high probability . But these distributions can be expensive to sample from. Instead, uniform sampling provides a fast alternative.

The number of columns will not grow too quickly if the full matrix is sufficiently incoherent . This means some columns do not carry a disproportionate amount of information relative to other columns. We numerically see that in our case the Jacobian matrix \(J((t))\) is sufficiently incoherent. Thus we can choose a uniform distribution over the set of indices \(\{1,,p\}\) to get the benefits of low rank approximation in a computationally efficient way.

### Discretizing in time

We discretize the time interval \(\) with \(K\) regularly spaced time steps \(0=t_{0}<t_{1}<<t_{K}=T\) with \( t=t_{k}-t_{k-1}\) for \(k=1,,K\). At time \(t_{0}\), we obtain \(^{(0)}^{p}\) by fitting the initial condition \(u_{0}\). We then update

\[^{(k)}=^{(k-1)}+ t ^{(k-1)}\] (8)

for \(k=1,,K\) so that \(^{(k)}\) is the time-discrete approximation of \((t_{k})\) and thus \((;^{(k)})\) approximates the solution \(u\) at time \(t_{k}\). The sparse update \(^{(k-1)}_{s}\) approximates \(}_{s}(t_{k-1})\) and is obtain by the time-discrete counterpart of (7), which is given by

\[_{^{(k-1)}_{s}}\|J_{s}(^{(k-1)})S_{k}^{(k-1)}_{s}-( ^{(k-1)})\|_{2}^{2}\,,\] (9)

if time is discretized with the forward Euler method. Other discretization schemes can be used as well, which then lead to technically more involved problems (9) that remain conceptually similar though. The sparse update is lifted to \(^{(k-1)}=S_{k}^{(k-1)}_{s}\) so that the update (8) can be computed.

### Computational procedure of RSNG

We describe the proposed RSNG procedure in algorithmic form in Algorithm 1. We iterate over the time steps \(k=1,,K\). At each time step, we first sketch the Jacobian matrix by creating a submatrix from randomly sampled columns. Notice that \(S_{k}\) need not actually be assembled as its action on the Jacobian matrix can be accomplished by indexing. We then solve the least-squares problem given in (9) using our sketched Jacobian to obtain \(^{(k-1)}_{s}\). A direct solve of this system dominates the computational cost of making a time step and scales in \(O(ns^{2})\) time. The components of \(^{(k-1)}\) corresponding to the indices that have not been selected are filled with zeros. We then update the parameter \(^{(k-1)}\) to \(^{(k)}\) via \(^{(k-1)}\).

The whole integration process scales as \(O(ns^{2})\) in time.

```  Fit parameterization \((;^{(0)})\) to initial condition \(u_{0}\) to obtain \(^{(0)}\) for\(k=1,,K\)do  Draw realization of sketching matrix \(S_{k}\) as described in Section 3.1  Solve for sparse update \(^{(k-1)}_{s}\) with least-squares problem (9)  Lift sparse update \(^{(k-1)}=S_{k}^{(k-1)}_{s}\)  Update \(^{(k)}=^{(k-1)}+ t ^{(k-1)}\) endfor ```

**Algorithm 1** Randomized Neural Galerkin scheme with sparse updates

## 4 Numerical experiments

We demonstrate RSNG on a wide range of evolution equations, where speedups of up to two orders of magnitude are achieved compared to comparable schemes with dense updates. We also compare to global-in-time methods, where we achieve up to two orders of magnitude higher accuracy.

### Setup and equations

ExamplesWe now describe the details of the PDEs that we use to evaluate our method. We choose these particular setups to test our method on a diverse set of challenges including problems with global and local dynamics and solutions with sharp gradients and fine grained details. For visualization of the solutions of these equations see the Appendix A.4.

_Reaction-diffusion problem modeled by Allen-Cahn (AC) equation:_ The Allen-Cahn equation models prototypical reaction diffusion phenomena and is given as,

\[_{t}u(t,x)=_{xx}u(t,x)+u(t,x)-u(t,x)^{3}.\]

We choose \(=5 10^{-3}\), with periodic boundary condition \(=[0,2)\) and initial condition,\[u_{0}(x)=(2(x))-(-23.5(x-)^{2})+(-27(x-4.2 )^{2})+(-38(x-5.4)^{2})).\]

This initial condition results in challenging dynamics that are global over the spatial domain.

_Flows with sharp gradients described by Burgers' equation:_ The Burgers' equation is given by,

\[_{t}u(t,x)=_{xx}u(t,x)-u(t,x)_{x}u(t,x).\]

We choose \(=1 10^{-3}\), with periodic boundary condition \(=[-1,1)\) and initial condition,

\[u_{0}(x)=(1-x^{2})(-30(x+0.5)^{2}).\]

The corresponding solution field has sharp gradients that move in the spatial domain over time, which can be challenging to approximate.

_Charged particles in electric field:_ The Vlasov equation describes the time evolution of collisionless charged particles under the influence of an electric field. The equation models the distribution of such particles in terms of their position and velocity. We consider the case of one position dimension and one velocity dimension, making our domain \(^{2}\). The equation is given by,

\[_{t}u(t,x,v)=-v_{x}u(t,x,v)+_{x}(x)_{v}u (t,x,v)\]

where \(x\) is the position, \(v\) is the velocity and \(\) is the electric field. We consider the case with periodic boundary condition \(=[0,2)[-6,6)\) and initial condition,

\[u_{0}(x,v)=}(}{2})\]

with a fixed electric field \((x)=(x)\). This particular setup evolves into a distribution with fine grained details along a separatrix surrounding the potential well.

SetupWe parameterize with a feed-forward multi-layer perceptron. All our networks use linear layers of width 25 followed by non-linear activation functions, except the last layer which has no activation and is of width 1 so that \((,(t))\). To vary the number of total parameters \(p\), we vary the depth of networks ranging from 3-7 layers. We use rational activation functions which in our experiments allowed for fitting initial conditions faster and more accurately than a standard choice such as \(\) or ReLU . To enforce periodic boundary conditions, we modify the first layer so that it outputs periodic embeddings as in ; for details see Appendix A.5. The periodic embedding ensures that the boundary conditions are enforced exactly. For additional details on enforcing other types of boundary conditions (e.g. Dirichlet and Neumann) exactly in neural networks see [12; 7; 48]. We sample points from the domain on an equidistant grid. For time integration we use a RK4 scheme with a fixed time step size. The time step sizes are \(5\), \(1\), \(5\) and we integrate up to end time \(4\), \(4\), and \(3\) for the Allen-Cahn, Burgers', and Vlasov equations, respectively. All error bars show \(+/-\) two standard errors over three random realizations which results in different sketching matrices at each time step. Relative errors are computed over the full space-time domain, unless the plot is explicitly over time.

All gradients and spatial derivatives are computed with automatic differentiation implemented in JAX . All computations are done in single precision arithmetic which is the default in JAX. All runtime statistics were computed on the same hardware, a Nvidia Tesla V100 w/ 32 GB memory. All additional hyperparameters are described in Appendix A.5.

### Results

**RSNG achieves higher accuracy than schemes with dense updates at same computational costs** In Figure 3 we plot the relative error over time. The curves corresponding to "dense updates" use a 3 layer network and integration is performed using dense updates. For RSNG, we use a 7 layer network and integrate with sparse updates, setting the number of parameters we update, \(s\), equal to the total number of parameters in the 3 layer network and thus equal to the number of parameters that are updated by "dense updates." Thus, the comparison is at a fixed computational cost. The error achieved with RSNG is one to two orders of magnitude below the error obtained with dense updates, across all examples that we consider. In Figure 4(a), we see that as we increase the network size,the relative error decreases as the sparse updates allow us to exploit the greater expressiveness of larger networks while incurring no additional computational cost in computing (9). But we note that increasing the size of the full network will make computations of \(J()\) and \(()\) more expensive because of higher costs of computing gradients. However, for the network sizes that we consider in this work, this effect is negligible compared to the cost of solving (9).

RSNG achieves speedups of up to two orders of magnitudeIn Figure 5(a), we compare the runtime of RSNG to the runtime of a scheme with dense updates that uses a direct solver and to the runtime of a scheme with dense updates that uses an iterative solver as proposed in . The time is computed for Burgers' equation and the sparsity \(s\) of RSNG is chosen such that all methods reach a comparable level of error. We find that RSNG is faster than direct solves with dense updates by two orders of magnitude and faster than the iterative solver by one order of magnitude.

The results show that while using an iterative solver as in  does speed up the method relative to direct solves with dense updates, it can still be quite slow for networks with many parameters \(p\). Additionally, convergence of the iterative method given in  requires a number of hyperparameters to be chosen correctly, which may require an expensive search or a priori knowledge about the solution. Note that our RSNG method does not preclude the use of an iterative method to speed up the least-squares solves further.

Figure 4: (a) RSNG benefits from the additional expressiveness of larger networks (larger \(p\)) while only using a fixed number of parameters (fixed \(s\)) at each time step. (b) As we decrease the number of parameters \(s\) in the sparse update, but keep the total number of parameters \(p\) of the network the same, we achieve lower errors than dense updates. Thus, RSNG outperforms dense updates while incurring lower computational costs. Error bars generated over random sketch matrices, \(S_{t}\).

Figure 3: We plot the relative error over time for RSNG versus dense updates at \(s=757\). We see RSNG leads to orders of magnitude lower errors than dense updates for the same costs.

To address the overfitting problem, the work  refits the network to the current approximate solution from a random initialization periodically during time integration. In Figure 5(b), we show the relative error versus the runtime for the iterative solver with various numbers of refits and for RSNG at different sparsity \(s\). While refitting the network can reduce the relative error, it incurs a high computational cost. By contrast, for appropriate sparsity \(s\), RSNG outperforms the method given in  in both speed and accuracy.

Varying sparsity \(s\) at fixed number of total parameters \(p\) in networkWe now study the effect of varying the sparsity \(s\) (i.e., number of parameters updated by sparse updates) for a fixed network of total size \(p\), in this case a 7 layer network. In Figure 4(b), we see that for a network of fixed size, sparse updates can reduce the relative error by about \(2\)-\(3\) when compared to dense updates. This is notable as the computational costs decrease quadratically with \(s\). Thus, the combination of sparsity and randomized updates in RSNG can deliver both improved performance and lower computational cost. We see that at the beginning, when the number \(s\) is too small, the expressiveness suffers and the error becomes large. This is because if \(s\) is less than the rank of the dense Jacobian then the sparsified Jacobian will necessarily have less representational power. However, we stress that RSNG is robust with respect to \(s\) in the sense that for a wide range of \(s\) values the error is lower than for dense updates.

The high error when performing dense updates \(s=p\) in Figure 4(b) for Allen-Cahn and Burgers' equation is due to the overfitting problem described in Section 2.2. As updates become denser, the method is more likely to overfit to regions of the parameter space in which the Jacobian, \(J()\), is ill suited for approximating the right-hand side \(f\) at future time steps (see Section 2). We can see this explicitly in Figure 6 where we plot the residual over time for sparse and dense updates on the Allen-Cahn equation. Initially, the dense updates lead to a lower residual. This makes sense as they begin at the same region of parameters space. But as the two methods navigate to different regions of parameters space, we see RSNG begins to incur a lower residual relative to dense updates. This indicates that RSNG ameliorates the problem of overfitting and so leads to a lower residual as shown in Figure 6(b).

Comparison with global-in-time methodsWe compare our method to global-in-time methods which aim to globally minimize the PDE residual over the entire space-time domain. We compare to the original PINN formulation given in . Additionally we compare to a variant termed Causal PINNs, which impose a weak form of time dependence through the loss function . We select this variant as it claims to have state of the art performance among PINNs on problems such as the Allen-Cahn equation. In Table 1, we see that our sequential-in-time RSNG method achieves a higher accuracy by at least one order of magnitude compared to PINNs. Additionally, in terms of computational costs, RSNG outperforms both PINN variants, as their global-in-time training is expensive and requires many residual evaluations. We note that the training time of PINNs is directly

Figure 5: RSNG has lower computational costs than dense updates with direct and iterative least-squares solvers. Plots for numerical experiment with Burgers’ equation

dependent on the number of optimization iterations and thus they can be trained faster if one is willing to tolerate even higher relative errors.

## 5 Conclusions, limitations, and future work

In this work, we introduced RSNG that updates randomized sparse subsets of network parameters in sequential-in-time training with the Dirac-Frenkel variational principle to reduce computational costs while maintaining expressiveness. The randomized sparse updates are motivated by a redundancy of the parameters and by the problem of overfitting. The randomized sparse updates have a low barrier of implementation in existing sequential-in-time solvers. The proposed RSNG achieves speedups of up to two orders of magnitude compared to dense updates for computing an approximate PDE solution with the same accuracy.

Current limitations leave several avenues for future research: first, as discussed in 3.2, uniform sampling is only appropriate when the Jacobian matrix is of low coherence. Future work may investigate more sophisticated sampling methods such as leverage score and pivoting elements of rank revealing QR. Second, there are problems for which overfitting with dense updates is less of an issue; e.g., the charged particles example in our work. Note that due to the sparsity of the updates, RSNG still achieves a speedup compared to dense updates for the same accuracy for this example though. However, more work is needed to better understand and mathematically characterize which properties of the problems influence the overfitting issue.

We make a general comment about using neural networks for numerically solving PDEs: The equations discussed in this paper are standard benchmark examples used in the machine-learning literature; however, for these equations, carefully designed classical methods can succeed and often have lower runtimes than methods based on nonlinear parameterizations [20; 36]. While these equations provide an important testing ground to demonstrate methodological improvements, future work will extend these results to domains where classical linear methods struggle, e.g., high-dimensional problems and problems with slowly decaying Kolmogorov \(n\)-widths [15; 7; 40].

We do not expect that this work has negative societal impacts.

   PDE & Method & \(L^{2}\) Relative Error & Time(s) & \(s\) \\  Allen-Cahn & PINN & \(6.85\) & 841 & N/A \\ Allen-Cahn & Causal PINN & \(3.84\) & 3060 & N/A \\ Allen-Cahn & RSNG & \(}\) & 776 & 800 \\ Allen-Cahn & RSNG & \(5.34\) & **63** & 150 \\  Burgers’ & PINN & \(2.34\) & 3451 & N/A \\ Burgers’ & Causal PINN & \(5.19\) & 23027 & N/A \\ Burgers’ & RSNG & \(}\) & 2378 & 800 \\ Burgers’ & RSNG & \(2.05\) & **188** & 125 \\   

Table 1: The sequential-in-time training with RSNG achieves about one order of magnitude higher accuracy than global-in-time methods in our examples. Details on training in Appendix A.6.

Figure 6: Plot (a) shows the residual of dense and sparse updates at early time steps. Initially, dense updates must have a lower residual as \(JS_{t}\) spans a subspace of the tangent space given by \(J\). But in plot (b), we see that after a few time steps, dense updates overfit and the residual grows quicker than with sparse updates.

AcknowledgementsThe authors were partially supported by the National Science Foundation under Grant No. 2046521 and the Office of Naval Research under award N00014-22-1-2728. This work was also supported in part through the NYU IT High Performance Computing resources, services, and staff expertise.