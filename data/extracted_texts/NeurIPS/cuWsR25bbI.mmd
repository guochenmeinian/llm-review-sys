# An exactly solvable model for emergence and scaling laws in the multitask sparse parity problem

Yoonsoo Nam*

Rudolf Peierls Centre for Theoretical Physics, University of Oxford

Nayara Fonseca*

Rudolf Peierls Centre for Theoretical Physics, University of Oxford

Seok Hyeong Lee

Center for Quantum Structures in Modules and Spaces, Seoul National University

Chris Mingard

Rudolf Peierls Centre for Theoretical Physics, University of Oxford

Ard A. Louis

Rudolf Peierls Centre for Theoretical Physics, University of Oxford

###### Abstract

Deep learning models can exhibit what appears to be a sudden ability to solve a new problem as training time, training data, or model size increases, a phenomenon known as emergence. In this paper, we present a framework where each new ability (a skill) is represented as a basis function. We solve a simple multi-linear model in this skill-basis, finding analytic expressions for the emergence of new skills, as well as for scaling laws of the loss with training time, data size, model size, and optimal compute. We compare our detailed calculations to direct simulations of a two-layer neural network trained on multitask sparse parity, where the tasks in the dataset are distributed according to a power-law. Our simple model captures, using a single fit parameter, the sigmoidal emergence of multiple new skills as training time, data size or model size increases in the neural network.

## 1 Introduction

_Emergence_ in large language models (LLMs) has attracted a lot of recent attention . It motivates the costly drive to train ever larger models on ever larger datasets, in the hope that new skills will emerge. While the concept of emergence has been critiqued on the grounds that the sharpness of the transition to acquiring a new skill may be sensitive to the measure being used , the observation that important new skills are learned for larger models raises many challenging questions: when the skills emerge and what drives the emergence. These questions are complicated by difficulties in formally defining skills or capabilities , and by our general limited understanding of the internal representations of deep neural networks .

Another widely observed property of deep learning models is that the loss improves predictably as a power-law in the number of data points or the number of model parameters or simply in the amount of compute thrown at a problem. These neural scaling laws  have been widely observed across different architectures and datasets . While the scaling exponents can depend on these factors, the general phenomena of scaling appear to be remarkably robust. This raises many interesting questions such as: What causes the near-universal scaling behavior? How does the continuous scaling of the loss relate to the discontinuous emergence of new skills?

A challenge in answering the questions raised by the phenomena of emergence and scaling laws arises from the enormous scale and expense of training cutting-edge modern LLMs, which are optimized for commercial applications, and not for answering scientific questions about how they work. One way that progress can be made is to study simpler dataset/architecture combinations that are more tractable. The current paper is inspired in part by recent work in this direction that proposed studying emergence in learning the sparse parity problem , which is easy to define, but known to becomputationally hard. In particular, Michaud et al.  introduce the multiple unique sparse parity problem - where tasks are distributed in the data through a power-law distribution of frequencies - as a proxy for studying emergence and neural scaling in LLMs. For this data set, the authors empirically measure the scaling laws of a 2-layer multilayer perceptron (MLP) as a function of training steps (\(T\)), parameters (\(N\)), and training samples (\(D\)). Based on their quanta model of abrupt skill acquisition, they schematically derive neural scaling laws as a sum of emergences of new skills. However, no link was established between the neural network dynamics and the quanta model.

In this paper, we introduce an analytically tractable model by defining a basis of orthogonal functions for the multitask sparse parity problem. Each basis function corresponds to a skill that can be learned, and their respective frequencies are distributed following a power-law with exponent \(+1\). We then propose a simple multilinear expansion in these orthogonal functions that introduces a layered structure reminiscent of neural networks (NNs) and gives rise to the stage-like training dynamics . With our simple model, we can analytically calculate full scaling laws, including pre-factors, as a function of data exponents \(\), \(T,D,N\), and optimal compute \(C\). Our simple model can, with just one parameter calibrated to the emergence of the first skill, predict the ordered emergence of multiple skills in a 2-layer MLP. We summarize our contributions as follows:

1. _Skills as basis functions._ We establish a framework for investigating emergence by representing skills as orthogonal functions that form a basis in function space (Section 2). We apply our methods to controlled experiments on the multitask sparse parity dataset.
2. _Multilinear model._ We propose an analytically tractable model that is expanded in the basis of skill functions, and is multilinear with respect to its parameters so that it possesses a layerwise structure (Section 3). The multilinear nature of the model produces non-linear dynamics, and the orthogonal basis decouples the dynamics of each skill.
3. _Scaling laws._ We derive scaling laws for our multilinear model, including the prefactor constants, which relate the model's performance to training time (\(T\)), dataset size (\(D\)), number of parameters (\(N\)), and optimal compute (\(C=N T\)), see Section 4. We show that the scaling exponents for these factors are \(-/(+1)\), \(-/(+1)\), \(-\), \(-/(+2)\), respectively, where \(+1\) is the exponent of the power-law input data.
4. _Predicting emergence._ We demonstrate that our multilinear model captures the skill emergence of an MLP with 2 layers for varying training time, dataset size, and number of trainable parameters. Our results show that the multilinear model, calibrated only on the first skill, can predict the emergence of subsequent skills in the 2-layer MLP, see Fig. 1 and Section 5. We obtain an equivalent result on the time emergence for a transformer architecture (Fig. 4).

Figure 1: **Predicting emergence.** The skill strength \(_{k}\), defined as the \(k^{th}\) coefficient if a model is expanded in the basis of the skill functions (\(g_{k}\)), measures how well the \(k^{}\) skill is learned, and is plotted against (a) time \(T\), (b) data set size \(D\), and (c) number of parameters \(N\) (width of the hidden layer). \(_{k}\) is normalized by the target scale \(S\) such that \(_{k}/S=1\) means zero skill loss. The dashed lines show the abrupt growth – emergence – of \(5\) skills for a 2-layer MLP (Appendix K) trained on the multitask sparse parity problem with data power-law exponent \(=0.6\) (shaded area indicate 1-standard deviation over at least \(10\) runs). Solid lines are the predictions (Eqs. (14), (17) and (21), respectively) from our multilinear model calibrated on the first skill (blue) only.

## 2 Setup

In this section, we define the multitask sparse parity problem under the mean-squared error (MSE) loss. We represent skills as orthogonal functions and measure their strength in a model by calculating the linear correlation between the model output and the skill basis functions. For a comprehensive list of notations, refer to the **glossary** in Appendix A. Our code is also available online.1

Multitask sparse parity problem.In the sparse parity problem, \(n_{b}\) skill bits are presented to the model. The target function is a parity function applied to a fixed subset of the input bits. The model must detect the relevant \(m<n_{b}\) sparse bits and return the parity function on this subset (\(M(i,x)\), see Table 1). Michaud et al.  introduced the **multitask** sparse parity problem by introducing \(n_{*}\) unique sparse parity variants - or skills - with different sparse bits (for a representation, see Table 1). Each skill is represented in the \(n_{s}\) control bits as a one-hot string, and the model must solve the specific sparse parity task indicated by the control bits (for more details, see Appendix B.1).

The \(n_{s}\) skills (random variable \(I\{1,2,,n_{s}\}\)) follow a power law distribution \(_{s}\), and the skill bits (random variable \(X\{0,1\}^{n_{b}}\)) are uniformly distributed. Because \(_{s}\) and \(_{b}\) are independent, the input distribution \((I,X)\) follows a product of two distributions:

\[_{s}(I=i):=}{_{j}^{n_{s}}j^{-(+1)}}, _{b}(X=x):=2^{-n_{b}},(I,X):=_{s }(I)_{b}(X).\] (1)

We denote \(A=(_{j=1}^{n_{s}}j^{-(+1)})^{-1}\) so that \(_{s}(i):=Ai^{-(+1)}\).

Skill basis functions.We represent the \(k^{th}\) skill as a function \(g_{k}:\{0,1\}^{n_{s}+n_{b}}\{-1,0,1\}\) that returns the parity (\(\{-1,1\}\)) on the \(k^{th}\) skill's sparse bits if \(i=k\), but returns \(0\) if the control bit mismatches that of the \(k^{th}\) skill (\(i k\)):

\[g_{k}(i,x):=\{(-1)^{_{j}M_{j}(i,x)}&i=k\\ 0&.,\] (2)

where \(M:\{0,1\}^{n_{s}+n_{b}}\{0,1\}^{m}\) is the map that selects the relevant sparse bits for the \(i^{th}\) skill (Table 1) and \(M_{j}(i,x)\) is the \(j^{}\) entry of \(M(i,x)\). Note that different skill functions have \(0\) correlation as the supports of skills functions are **mutually exclusive**:

\[g_{k}(i,x)g_{k^{}}(i,x)=_{i,k}_{k,k^{}}.\] (3)

   Skill idx \((I)\) & Control bits & Skill bits \((X)\) & \(y\) & \(M(i,x)\) & \(g_{1}(i,x)\) & \(g_{2}(i,x)\) & \(\) & \(g_{n_{s}}(i,x)\) \\ 
1 & 1000000 & 11011000100 & \(S\) &  & 1 & 0 & \(\) & 0 \\
1 & 100000 & 100101010001 & \(-S\) &  & \(-1\) & 0 & \(\) & 0 \\ \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\
2 & 0100000 & 00100101101 & \(-S\) &  & 0 & \(-1\) & \(\) & 0 \\ \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ \(n_{s}\) & 0000001 & 0010100110 & \(-S\) &  & 0 & 0 & \(\) & \(-1\) \\   

Table 1: **Multitask sparse parity dataset and skill basis functions.** The control bits are \(n_{s}\)-dimensional one-hot vectors encoding specific parity tasks, indexed in the first column. The frequency of the distinct parity tasks follows a rank-frequency distribution with an inverse power law relation (Eq. (1)). The skill bits are binary strings with \(m=3\) relevant sparse bits (highlighted in colors) with their locations varying by skill. The \(y\) column shows the target scale \(S\) multiplied by the parity computed from the relevant bit set \(M(i,x)\). The last columns show the values of the skill basis functions \(g_{k}(i,x)\), defined in Eq. (2).

The target function.The target function is a sum over \(n_{s}\) skill functions multiplied by a target scale \(S\):

\[f^{*}(i,x):=S_{k=1}^{n_{s}}g_{k}(i,x).\] (4)

The target scale \(S\) is the norm of the target function (\(_{I,X}[f^{*}(I,X)f^{*}(I,X)]=S^{2}\)). Note that the skill functions serve as 'features' or countable basis for describing the target function as in Hutter .

MSE loss.We use MSE loss for analytic tractability:

\[:=_{X,I}[(f^{*}(I,X)-f(I,X))^{ 2}],\] (5)

where \(f\) is the function expressed by a given model. We define the skill loss \(_{k}\) as the loss when only the \(k^{th}\) skill is given, which can be weighted by their skill frequencies to express the total loss:

\[_{k}:=_{X}[(f^{*}(I=k,X)-f(I=k,X) )^{2}],=_{k=1}^{n_{s}}_{s}(I=k) _{k}.\] (6)

Skill strength.The skill strength or the linear correlation between the \(k^{th}\) skill (\(g_{k}\)) and a function expressed by the model at time \(T\) (\(f_{T}\)) is

\[_{k}(T):=_{X}[g_{k}(I=k,X)f_{T}(I=k,X)].\] (7)

The skill strength \(_{k}\) is the \(k^{th}\) coefficient if a model is expanded in the basis of the skill functions (\(g_{k}\)). The skill strength, like the test loss, can be accurately approximated by a sum (see Appendix K.3). The skill loss \(_{k}\) (Eq. (6)) can be expressed by the skill strength and the norm of the learned function for \(I=k\):

\[_{k}(T)=(S^{2}+_{X}[f_{T}(I=k,X)^{2 }]-2S_{k}(f_{T})).\] (8)

The skill loss becomes \(0\) if and only if \(f_{T}(I=k,X)=Sg_{k}(I=k,X)\).

Experimental setting.We use a 2-layer MLP that receives the \(n_{s}+n_{b}\) bits as inputs and outputs a scalar (\(\{0,1\}^{n_{s}+n_{b}}\)). In most of the experiments, the NN is trained with stochastic gradient descent (SGD) with width \(1000\), using \(n_{s}=5\), \(m=3\), and \(n_{b}=32\), unless otherwise stated. A decoder transformer is also used for the time emergent experiments. See Appendix K for details.

## 3 Multilinear model

We propose a simple multilinear model - multilinear with respect to the parameters - with the first \(N\) most frequent skill functions \(g_{k}(i,x)\) as the basis functions (features):

\[f_{T}(i,x;a,b)=_{k=1}^{N}a_{k}(T)b_{k}(T)g_{k}(i,x),\] (9)

where \(a,b^{N}\) are the parameters. The model has built-in skill functions \(g_{k}\) - which transform control bits and skill bits into the parity outputs of each skill - so the model only needs to scale the parameters to \(a_{k}b_{k}=S\).

The multilinear structure (product of \(a_{k},b_{k}\)) is analogous to the layered structure of NNs and results in emergent dynamics (Fig. 1(a)) different from a linear model with the same basis functions (Appendix H). A similar model has been studied by Saxe et al.  in the context of linear neural networks (Appendix B.2).

For the multilinear model, note that \(a_{k}(T)b_{k}(T)\) is the skill strength \(_{k}\) (Eq. (7)) and the skill loss (Eq. (6)) is a function of \(S\) and \(_{k}\) only:

\[a_{k}(T)b_{k}(T)=_{k}(T),_{k}(T)=(S- _{k}(T))^{2}\,.\] (10)Assuming that we are training the model on \(D\) samples from \((I,X)\), the empirical loss decomposes into a sum of empirical skill losses because \(g_{k}\)'s supports are mutually exclusive. This **decouples** the dynamics of each skill (\(_{k}(T)\)), which is analytically solvable under gradient flow (Appendix C.1).

\[^{(D)}(T)=_{k=1}^{n_{s}}d_{k}(S-_{ k}(T))^{2},_{k}(T)}{S}=_{k}(0)}-1)e^{-2}{D}ST}},}\] (11)

where \(d_{k}\) is the number of samples of the \(k^{th}\) skill (i.e., number of samples \((i,x)\) with \(g_{k}(i,x) 0\)), \(\) is the learning rate, and \(0<_{k}(0)<S\) is the skill strength at initialization.

## 4 Scaling laws

Recent literature has extensively explored scaling laws; see Section 7 for an overview. In this section, we derive the scaling laws of our multilinear model (Section 3) for time (\(T\)), data (\(D\)), parameters (\(N\)) and optimal compute (\(C\)). We define compute as \(C:=T N\).

Table 2 shows our analytical scaling laws including their prefactor constants (Appendix J) and Fig. 2 compares the simulation of our model with our scaling law predictions. For the scaling law exponents, we achieve the same exponent as in Hutter  for \(D\) and in Michaud et al.  for \(T,D,\) and \(N\). Assuming \(0<<1\), the exponents are consistent with the small power-law exponents reported in large-scale experiments, see, e.g., [9; 14; 22].

Using Eqs. (6), (10) and (11), we derive the loss as a function of time (\(T\)), data (\(D\)), parameters (\(N\)), and the number of observations for each skill \([d_{1},,d_{n_{s}}]\):

\[=}{2}_{k=1}^{N}_{s}(k)_{k}(0)}-1)^{-1}e^{2}{D}ST} )^{2}}+}{2}_{k=N+1}^{n_{s}}_{s}(k).\] (12)

Under suitable assumptions (e.g., for the \(T\) scaling law, we take \(D,N\) and \(d_{k}/D_{s}(k)\)), we can use Eq. (12) to derive the scaling laws. For \(T,D\), and \(N\), we used Eq. (11) - decoupled dynamics induced the basis functions \(g_{k}\) - to decouple the evolution of each skill loss:

1. For the time scaling law, each \(_{k}\) shares the same dynamics with \(T\) scaled by \(_{s}(k)\).
2. For the data scaling law, each \(_{k}\) depends only on the observation the \(k^{th}\) skill (\(d_{k}>0\)).
3. For the parameter scaling law, each \(_{k}\) depends on whether the model has \(g_{k}\) as a basis function.

For the optimal compute scaling law, we show in Corollary 4 (Appendix J) that the optimal tradeoff between \(T\) and \(N\) for given \(C\) is when \(T\) is large enough to fit the \(N^{th}\) skill (Fig. 3). In Appendix J, we show **rigorous** derivations of all scaling laws, including the prefactors, error bounds, and conditions (e.g., how large \(N\) must be compared to \(T\) to be treated as infinity). For simplified derivations for the exponents only, see Appendix E. For an intuitive derivation (stage-like training) and connection to Michaud et al. , see Appendix D.

## 5 Predicting emergence

The literature on emergence has rapidly expanded lately; for a review of these developments, see Section 7. In this section, we analyze the emergence of a 2-layer NN (Section 2) and discuss to what degree the emergence in NNs can be described with our model. At initialization, NNs **lack** the information about the data and must 'discover' each \(g_{k}\). To take this effect into account in our model, we add an extra parameter which we calibrate (fit) on an NN trained on one skill (\(n_{s}=1\)) system and use it to predict the emergence of subsequent skills for the \(n_{s}=5\) setup (Fig. 1).

### Time emergence

In our multilinear model, the layerwise structure - the product of parameters \(a_{k}b_{k}\) - leads to a sigmoidal saturation where an update of one layer hastens the update of the other layer. Feature

   Bottleneck & Condition 1 & Condition 2 & Exponent & Prefactor & Scaling law \\  Time (\(T\)) & \(D NT^{2},T^{3}\) & \(N^{+1} T\) & \(-/(+1)\) & Thm.4 & Thms.2,3 \\ Data (\(D\)) & \(T D( D)^{1+}\) & \(N^{+1} D\) & \(-/(+1)\) & Thm.5 & Thm.5 \\ Parameter (\(N\)) & \(D T^{3}\) & \(N^{+1}=o(T)\) & \(-\) & Thm.1 & Thm.1 \\ Compute (\(C\)) & \(D T^{3}\) & \(N^{+1} T\) & \(-/(+2)\) & Cor. 5 & Cor. 4 \\   

Table 2: **Summary of the scaling laws for the multilinear model.** The leftmost column indicates the bottleneck resource while the next two columns are the conditions for the ‘large resources’ – large enough to be treated as infinity. The fourth column is the bottleneck resource’s scaling law exponent for the loss. The last two columns show the statement for the prefactor constant and the scaling law (with the assumptions and explicit error terms) in Appendix J.

Figure 3: **Scaling law for optimal compute.** The solid lines are the learning curves of the multilinear model as a function of compute \(C=T N\) with varying parameters \(N\) from \(10^{1}\) (top plateau) to \(10^{4}\) (bottom plateau). The dotted lines are optimal compute scaling laws with exponent \(-/(+2)\) (Appendix E.4) and calculated prefactor constants (Appendix J). See Appendix K.4 for details of the experiment. For a given \(C\), we achieve the optimal tradeoff when \(T\) is large enough to fit all \(N\) skills (i.e. when the solid lines plateau). For the case \(=0.3\), the optimal \(C\) for the model decays faster than the power-law, see Appendix E.1.

Figure 2: **Scaling laws.** The learning curve (\(\) is the MSE loss) of the multilinear model (solid) and the theoretical power-law (dotted) for (a) time \(T\), (b) data \(D\), and (c) parameters \(N\). Lower left legends show the condition (top) and the scaling law (bottom) where \(+1\) is the exponent of the power-law input data (Eq. (1)). See the appendices for 1) rigorous derivations of the theoretical scaling laws including the exponents, prefactors (e.g., \(_{N}\) for \(=_{N}N^{-}\)), and conditions (Appendix J); 2) simplified derivations of the exponent only (Appendix E); 3) details of the experiment (Appendix K.4).

learning dynamics in a 2-layer MLP shares the positive feedback between the layers but require a non-trivial update of parameters to express \(g_{k}\).

Extended model.Given that feature learning, though nonlinear, involves parameter updates, we compensate for the additional delay in feature-learning by multiplying \(g_{k}\) by a calibration constant \(0<<1\):

\[f_{T}(i,x;a,b)=_{k=1}^{N}a_{k}(T)b_{k}(T)g_{k}(i,x), 0< <1.\] (13)

The calibration constant \(\) rescales the dynamics in \(T\) (Eq. (11)):

\[_{k}(T)}{S}=_{k}(0)}-1 )e^{-2_{s}(k)^{2}ST}},\] (14)

where \(d_{k}/D_{s}(k)\) because we assume \(D\). We observe that \(^{2}=1/22\) fits the NN trained on one skill (see Fig. 11 in Appendix I), and the calibrated model predicts emergence in the \(n_{s}=5\) system (Fig. 1(a)): suggesting that the dynamics of feature-learning \(g_{k}\) in 2-layers NNs is similar to that of parameter learning (\(a_{k}b_{k}\)) in a simple multilinear model. For further intuition of the extended model, see an example of time emergence in an NN in Appendix G.

### Data point emergence

Our multilinear model can learn the \(k^{th}\) skill with a single observation of the skill because the skill functions \(g_{k}\) are built in (see Corollary 1 in Appendix C.2). NNs, without the fixed basis functions, must 'discover' each \(g_{k}\), which requires multiple samples from the \(k^{th}\) skill.

Extended model.To make our model a \(D_{c}\)-shot learner, we extend it by replacing \(g_{k}\) with the \(e_{k,l}\) basis:

\[f_{T}(i,x;a,B)=_{k=1}^{N}a_{k}(T)_{l=1}^{D_{c}}B_{k,l}(T)e_{k,l}(i,x),\] (15)

where the matrix \(B^{N D_{c}}\) is an extension of \(b^{N}\) in Eq. (9), \(D_{c}\) is a fixed scalar, and \(e_{k,l}(i,x):\{0,1\}^{n_{s}+n_{b}}\) are functions with the following properties:

\[_{X|I=k}[e_{k,l}e_{k,l^{}}]=_{ll^{}},  14.226378pte_{k,l}(I k,x)=0,_{l=1}^{D_{c}}}}e_{k,l}=g_{k}.\] (16)

The first property states that \(e_{k}\)'s, when \(I=k\), are orthonormal in \(X\). The second property asserts that, similar to \(g_{k}\) (Eq. (2)), \(e_{k,l}\) is non-zero only when \(I=k\), and fitting of the \(k^{th}\) skill only occurs among \(e_{k,l}\)'s, keeping the skills _decoupled_. The third property states that \(g_{k}\) can be expressed using \(e_{k,l}\).

For the \(k^{th}\) skill, the extended model overfits \(g_{k}\) when there are fewer observations (\(d_{k}\)) than the dimension of the \(e_{k,l}\) basis (\(D_{c}\)), and fits \(g_{k}\) when \(d_{k} D_{c}\), making our model a \(D_{c}\) shot learner.

\(D_{c}\) shot learner._If we initialize the extended model in Eq. (15) with sufficiently small initialization and if the conditions in Eq. (16) are satisfied, then the skill strength after training (\(T\)) on \(D\) datapoints is_

\[_{k}()=S(1-/D_{c}})&:d _{k}<D_{c}\\ S&:d_{k} D_{c}.\] (17)

_The number \(d_{k}\) is the number of samples in the training set for the \(k^{th}\) skill (i.e., datapoints with \(g_{k}(i,x) 0\))._

* See Appendix F.3. \(\)

Using Eq. (17), we can calculate the emergence of \(_{k}/S\) as a function of \(D\). Note that Eq. (17) is similar to the model in Michaud et al.  in that, to learn a skill, the model requires a certain number of samples from the skill.

The derivation of Eq. (17) follows trivially from the dynamics of the extended model (Eq. (15)) and well-known results in linear/kernel regression [23; 24; 25; 26; 27]. To be more specific, the model finds the minimum norm solution as if we performed ridgeless regression on \(g_{k}\) with basis functions \([e_{k,1}, e_{k,D_{c}}]\). See Appendix F.3 for details.

We observe that \(D_{c}=800\) approximates the data emergence for the \(n_{s}=1\) system (see Fig. 11 in Appendix I) and also the emergence for \(n_{s}=5\) system (Fig. 1(b)), suggesting that the NN discovers \(g_{k}\) when it observes \(D_{c}\) samples from the \(k^{th}\) skill.

### Parameter emergence

Since our multilinear model has \(g_{k}\)'s as the basis functions, it requires only one basis function (\(2\) parameters) to express a skill (see Corollary 2 in Appendix C.3). A 2-layer NN cannot express a skill with a single hidden node (i.e., a hidden layer with width \(1\)); it requires multiple hidden nodes to express a single skill.

Extended model.To compensate for the need for multiple hidden nodes in expressing one skill, we extend our model similarly to Eq. (15). Because the number of parameters is now a bottleneck, we ensure the model has \(N\) basis functions (\(e_{k,l}\)'s):

\[f_{T}(i,x;a,B)=_{k=1}^{q-1}_{l=1}^{N_{c}}a_{k}(T)B_{k,l}(T)e_{k,l}(i,x )+_{l^{}=1}^{r}a_{q}(T)B_{q,l^{}}(T)e_{q,l^{}}(i,x),\] (18)

where \(N_{c}\) is the number of basis functions needed to express a skill, quotient \(q\) is \((N-1)/N_{c}+1\) and remainder \(r\) is such that \((q-1)N_{c}+r=N\). In short, the \(N\) basis functions are

\[[e_{1,1},,e_{1,N_{c}}, e_{2,1},,e_{2,N_{c}} e _{q,1},,e_{q,r}].\] (19)

Similar to Eq. (16), the basis functions satisfy the following properties

\[_{X|I=k}\,[e_{k,l}e_{k,l^{}}]=_{ll^{}},\ e_{k,l}(I  k,x)=0,_{l=1}^{N_{c}}}}e_{k,l}=g_{k}.\] (20)

\(N_{c}\) basis functions for a skill._For the extended model in Eq. (18), the skill strength at \(T,D\) for a given \(N\) becomes_

\[_{k}()=0&:k>q\\ S}&:k=q\\ S&:k<q\,.\] (21)

See Appendix F.4.

The model can express the \(k^{ th}\) skill based on the number of available basis functions for the given skill (Eq. (21)). For example, skills with \(k<q\) have all \(N_{c}\) basis functions \([e_{k,1},,e_{k,N_{c}}]\) to express the \(k^{ th}\) skill (Eq. (20)), while for \(k=q\), only \(r\) of the \(N_{c}\) basis functions are available.

We observe that \(N_{c}=4\) fits the parameter emergence for the \(n_{s}=1\) system (see Fig. 11 in Appendix I) and also the emergence for the \(n_{s}=5\) system (Fig. 1(c)), suggesting that the NN requires \(4\) nodes in expressing \(g_{k}\). The results also suggest that an NN, while lacking the ordering of basis functions (Eq. (19)), prefers to use the hidden neuron in fitting more frequent skills. The 'preference' toward frequent skills agrees with Fig. 1(a) where the NN learns more frequent skills first. Note that for the parameter emergence experiment, Adam  was used, instead of SGD, to increase the chance of escaping the near-flat saddle points induced by an insufficient number of parameters.

### Time emergence in a transformer

To test whether our conceptual framework extends to other architectures, we perform a time emergence experiment with a transformer (Fig. 4). Note that the emergent time \(_{emerge}\) - when the skill strength is sufficiently larger than \(0\) - follows the same power-law relationship as Eq. (11): \(_{emerge}(k) k^{+1}\) (see Fig. 6 in Appendix D for a discussion on emergent time). This suggests that, in the multitask sparse parity setup, other architectures may follow similar decoupled dynamics (Eq. (11)) and the consequent scaling laws (Section 4) and emergence (Section 5). An in-depth study of these findings across different architectures is left for future work.

### Limitations of the multilinear model

The strength of our extended multilinear model comes from the decoupled dynamics for each skill: leading to the prediction of the time, data, and parameter emergence with a single calibration. The weakness of our model is that it simplifies the more complex dynamics of NNs.

Time emergence.We note that the NN and the multilinear model emerge at similar instances, but the NN takes longer to saturate fully. This is because, for a given skill, the dynamics of the NN is not one sigmoidal saturation but a sum of **multiple** sigmoidal dynamics with different saturation times. To express the parity function, the NN must use multiple hidden neurons, and the skill strength can be divided into the skill strength from each neuron whose dynamics follow a sigmoidal saturation. Because of the non-linearity and the function it expresses, each neuron is updated at different rates, and the slowly saturating neurons result in a longer tail compared to our multilinear model. For an example, see Fig. 8 in Appendix G.

Data point emergence.Our extended model (Eq. (17)) deviates from NNs when \(d_{k} D_{c}\) and NNs show a more abrupt change in \(_{k}\) as a function of \(D\). This is because our model asserts strict decoupling among the skills: even a few \(d_{k}\) will contribute to learning \(g_{k}\) from \(e_{k,l}\). This differs from the NN, which lacks strict decoupling among the samples from different skills. We speculate that because NNs can perform benign  or tempered  overfitting, they treat a few data points from less frequent skills as 'noise' from more frequent skills: requiring more samples to learn the infrequent skills.

Parameter emergence.Note that Fig. 1(c) has high variance compared to other emergence plots in Fig. 1; this is because the NN sparsely, over many repeated trials, uses the hidden neurons to learn less frequent skills over more frequent ones (see Table 5 in Appendix I for an example of such outliers). Because NNs are less strictly biased toward frequent skills than our model, we speculate that initial conditions favoring less frequent skills may contribute to the outliers.

## 6 Discussion and conclusion

This work demonstrated scaling laws and predicted emergence in a 2-layer MLP using a tractable multilinear model. We found that representing skills as mutually exclusive functions leads to the decoupled dynamics, resulting in the scaling laws observed in a 2-layer MLP. The layerwise structure leads to emergent (sigmoidal) saturation of the skill strength, similar to what is observed in 2-layer MLPs.

Despite lacking explicit skill functions, NNs exhibit similar emergence patterns. We speculate that the model's layerwise structure and power-law frequencies of the skills induce **stage-like** dynamics

Figure 4: **Transformer on multitask sparse parity task. We trained a transformer on the multitask sparse parity task with \(=0.9\); see Appendix K for details. Left: An example of the time emergence (measured in steps) for the transformer in the \(n_{s}=5\) setup. See Appendix I for enlarged plots showing the saturation of each skill in linear scale. Right: The \(k^{th}\) skill’s emergent time \(_{emerge}(k)\) (i.e. \(_{k}(_{emerge}(k))/S=0.05\)) as a function of \(k\) (error bars indicate 1-standard deviation over \(5\) runs). The emergent times follow a power law of \(k^{+1}\), following the same relationship in the multilinear model (Eq. (11)).**(Appendix D) in NNs. The parameters relevant for expressing more frequent skills are updated significantly faster than those for less frequent skills. When skill 'discovery' operates on different time scales with minimal interaction, the skill dynamics effectively become **decoupled**, justifying our model setup.

Our results suggest a link between feature learning and emergence  driven by decoupled, stage-like dynamics. The layerwise dynamics leading to sigmoidal saturation may also disentangle the problem into skills (features) of varying importance (frequencies). Then feature learning, or discovering the basis functions that describe the target function [31; 32] (for recent studies, see [33; 34; 35; 36; 37; 38]), likely occurs in stages. Investigating this connection through layerwise dynamics is left for future work.

Similar to many prior works (see, e.g., [20; 18]), we studied a simple model on an idealized power-law distributed dataset. Also, our model cannot capture the complex non-linear interactions among multiple skills but can express any linear superposition of skills. In future work, we will explore 'complex skills' in language as a superposition of linearly independent skills. By validating our findings in language tasks, we aim to contribute to a broader understanding of how neural networks acquire and exhibit complex behaviors.

## 7 Related works

In this section, we review the literature on scaling laws and emergence in NNs. Focusing on data scaling, Hutter  develops a model with a discrete set of features. Under the assumption of a power-law distribution of features, this model demonstrates that the error decreases as a power law with increasing data size. In a related vein, Michaud et al.  propose a model of neural scaling laws in which the loss is decomposed into a sum over 'quanta'. Their model aims to reconcile the apparent discrepancy between loss metrics' regular power-law scaling and the abrupt development of novel capabilities in large-scale models. Various other models for neural scaling laws have been proposed in recent research, including connecting neural scaling exponents to the data manifold's dimension  and their relation with kernels , proposing solvable random-feature models [41; 21], and developing data scaling models using kernel methods [42; 43; 25].

Closely related to the study of neural scaling laws is the understanding of emergent abilities in large language models. Several studies [1; 2; 3; 4] document examples of such emergent abilities. Arora and Goyal  propose a framework for the emergence of tuples of skills in language models, in which the task of predicting text requires combining different skills from an underlying set of language abilities. Okawa et al.  demonstrate that a capability composed of smoothly scaling skills will exhibit emergent scaling due to the multiplicative effect of the underlying skills' performance. Other works related to the skill acquisition include Yu et al. , who introduce a new evaluation to measure the ability to combine skills and develop a methodology for grading such evaluations, and Chen et al. , who formalize the notion of skills and their natural acquisition order in language models.