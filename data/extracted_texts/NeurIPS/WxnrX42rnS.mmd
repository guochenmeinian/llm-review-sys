# STORM: Efficient Stochastic Transformer based

World Models for Reinforcement Learning

Weipu Zhang, Gang Wang, Jian Sun, Yetian Yuan

National Key Lab of Autonomous Intelligent Unmanned Systems, Beijing Institute of Technology

Beijing Institute of Technology Chongqing Innovation Center

zhangwp.bit@gmail.com, {gangwang,sunjian,ytyuan}@bit.edu.cn

Gao Huang

Department of Automation, BNRist, Tsinghua University

gaohuang@tsinghua.edu.cn

Corresponding author

###### Abstract

Recently, model-based reinforcement learning algorithms have demonstrated remarkable efficacy in visual input environments. These approaches begin by constructing a parameterized simulation world model of the real environment through self-supervised learning. By leveraging the imagination of the world model, the agent's policy is enhanced without the constraints of sampling from the real environment. The performance of these algorithms heavily relies on the sequence modeling and generation capabilities of the world model. However, constructing a perfectly accurate model of a complex unknown environment is nearly impossible. Discrepancies between the model and reality may cause the agent to pursue virtual goals, resulting in subpar performance in the real environment. Introducing random noise into model-based reinforcement learning has been proven beneficial. In this work, we introduce Stochastic Transformer-based wORld Model (STORM), an efficient world model architecture that combines the strong sequence modeling and generation capabilities of Transformers with the stochastic nature of variational autoencoders. STORM achieves a mean human performance of \(126.7\%\) on the Atari \(100\)k benchmark, setting a new record among state-of-the-art methods that do not employ lookahead search techniques. Moreover, training an agent with \(1.85\) hours of real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics card requires only \(4.3\) hours, showcasing improved efficiency compared to previous methodologies.

We release our code at https://github.com/weipu-zhang/STORM.

## 1 Introduction

Deep reinforcement learning (DRL) has exhibited remarkable success across diverse domains. However, its widespread application in real-world environments is hindered by the substantial number of interactions with the environment required for achieving such success. This limitation becomes particularly challenging when dealing with broader real-world settings in e.g., unmanned and manufacturing systems  that lack adjustable speed simulation tools. Consequently, improving the sample efficiency has emerged as a key challenge for DRL algorithms.

Popular DRL methods, including Rainbow  and PPO , suffer from low sample efficiency due to two primary reasons. Firstly, the estimation of the value function proves to be a challenging task. Thisinvolves approximating the value function using a deep neural network (DNN) and updating it with \(n\)-step bootstrapped temporal difference, which naturally requires numerous iterations to converge . Secondly, in scenarios where rewards are sparse, many samples exhibit similarity in terms of value functions, providing limited useful information for training and generalization of the DNN [6; 7]. This further exacerbates the challenge of improving the sample efficiency of DRL algorithms.

To address these challenges, model-based DRL algorithms have emerged as a promising approach that tackles both issues simultaneously while demonstrating significant performance gains in sample-efficient settings. These algorithms start by constructing a parameterized simulation world model of the real environment through self-supervised learning. Self-supervised learning can be implemented in various ways, such as reconstructing the original input state using a decoder [8; 9; 10], predicting actions between frames , or employing contrastive learning to capture the internal consistency of input states [6; 7]. These approaches provide more supervision information than conventional model-free RL losses, enhancing the feature extraction capabilities of DNNs. Subsequently, the agent's policy is improved by leveraging the experiences generated using the world model, eliminating sampling constraints and enabling faster updates to the value function compared to model-free algorithms.

However, the process of imagining with a world model involves an autoregressive process that can accumulate prediction errors over time. In situations where discrepancies arise between the imagined trajectory and the real trajectory, the agent may inadvertently pursue virtual goals, resulting in subpar performance in the real environment. To mitigate this issue, introducing random noise into the world model has been proven beneficial [9; 10; 11; 14]. Variational autoencoders, capable of automatically learning low-dimensional latent representations of high-dimensional data while incorporating reasonable random noise into the latent space, offer an ideal choice for image encoding.

Numerous endeavors have been undertaken to construct an efficient world model. For instance, SimPLe  leverages LSTM , while DreamerV3  employs GRU  as the sequence model. LSTM and GRU, both variants of recurrent neural networks (RNNs), excel at sequence modeling tasks. However, the recurrent nature of RNNs impedes parallelized computing, resulting in slower training speeds . In contrast, the Transformer architecture  has lately demonstrated superior performance over RNNs in various sequence modeling and generation tasks. It overcomes the challenge of forgetting long-term dependencies and is designed for efficient parallel computing. While several attempts have been made to incorporate Transformers into the world model [12; 13; 18], these works do not fully harness the capabilities of this architecture. Furthermore, these approaches require even longer training times and fail to surpass the performance of the GRU-based DreamerV3.

In this paper, we introduce the Stochastic Transformer-based wORId Model (STORM), a highly effective and efficient structure for model-based RL. STORM employs a categorical variational autoencoder (VAE) as the image encoder, enhancing the agent robustness and reducing accumulated autoregressive prediction errors. Subsequently, we incorporate the Transformer as the sequence

Figure 1: Comparison of methods on Atari \(100\)k. SimPLe  and DreamerV3  employ RNNs as their world models, whereas TWM , IRIS , and STORM use Transformers. The training frames per second (FPS) results on a single NVIDIA V100 GPU are extrapolated from other graphics cards for SimPLe, TWM, and IRIS, while DreamerV3 and STORM are directly evaluated.

model, improving modeling and generation quality while accelerating training. STORM achieves a remarkable mean human normalized score of \(126.7\%\) on the challenging Atari \(100\)k benchmark, establishing a new record for methods without resorting to lookahead search. Furthermore, training an agent with \(1.85\) hours of real-time interaction experience on a single NVIDIA GeForce RTX \(3090\) graphics card requires only \(4.3\) hours, demonstrating superior efficiency compared to previous methodologies. The comparison of our approach with the state-of-the-art methods is depicted in Figure 1.

## 2 Related work

Model-based DRL algorithms aim to construct a simulation model of the environment and utilize simulated experiences to improve the policy. While traditional model-based RL techniques like Dyna-Q have shown success in tabular cases , modeling complex environments such as video games and visual control tasks presents significant challenges. Recent advances in computing and DNNs have enabled model-based methods to learn the dynamics of the environments and start to outperform model-free methods on these tasks.

The foundation of VAE-LSTM-based world models was introduced by Ha and Schmidhuber  for image-based environments, demonstrating the feasibility of learning a good policy solely from generated data. SimPLe  applied this methodology to Atari games, resulting in substantial sample efficiency improvements compared to Rainbow , albeit with relatively lower performance under limited samples. The Dreamer series [8; 9; 10] also adopt this framework and showcase notable capabilities in Atari games, DeepMind Control, Minecraft, and other domains, using GRU  as the core sequential model. However, as discussed earlier, RNN structures suffer from slow training .

Recent approaches such as IRIS , TWM , and TransDreamer  incorporate the Transformer architecture into their world models. IRIS  employs VQ-VAE  as the encoder to map images into \(4 4\) latent tokens and uses a spatial-temporal Transformer  to capture information within and across images. However, the attention operations on a large number of tokens in the spatial-temporal structure can result in a significant training slowdown. TWM  adopts Transformer-XL  as its core architecture and organizes the sequence model in a structure similar to Decision Transformer , treating the observation, action, and reward as equivalent input tokens for the Transformer. Performing self-attention across different types of data may have a negative impact on the performance, and the increased number of tokens considerably slows down training. TransDreamer  directly replaces the GRU structure of Dreamer with Transformer. However, there is a lack of evidence demonstrating their performance in widely accepted environments or under limited sample conditions.

Other model-based RL methods such as MuZero , EfficientZero , and SpeedyZero  incorporate Monte Carlo tree search (MCTS) to enhance policy and achieve promising performance on the Atari \(100\)k benchmark. Lookahead search techniques like MCTS can be employed to enhance other model-based RL algorithms, but they come with high computational demands. Additionally, certain model-free methods [6; 25; 26; 27] incorporate a self-supervised loss as an auxiliary term alongside the standard RL loss, demonstrating their effectiveness in sample-efficient settings. Additionally, recent studies [28; 29] delve deeper into model-free RL, demonstrating strong performance and high data efficiency, rivaling that of model-based methods on several benchmarks. However, since the primary objective of this paper is to enhance the world model, we do not delve further into these methods.

We highlight the distinctions between STORM and recent approaches in the world model as follows:

   Attributes & SimPLe  & TWM  & IRIS  & DreamerV3  & STORM (ours) \\  Sequence model & LSTM  & Transformer-XL  & Transformer  & GRU  & Transformer \\ Tokens & Latent & Latent, action, reward & Latent(\(4 4\)) & Latent & Latent \\ Latent representation & Binary-VAE & Categorical-VAE & VQ-VAE & Categorical-VAE & Categorical-VAE \\ Historical information & Yes & No & Yes & Yes & No \\ Agent state & Reconstructed image & Latent & Reconstructed image & Latent, hidden & Latent, hidden \\ Agent training & PPO  & As DreamerV2  & As DreamerV2  & DreamerV3 & As DreamerV3 \\   

Table 1: Comparison between STORM and recent approaches. “Tokens” refers to the input tokens introduced to the sequence model during a single timestep. “Historical information” indicates whether the VAE reconstruction process incorporates historical data, such as the hidden states of an RNN.

* SimPLe  and Dreamer  rely on RNN-based models, whereas STORM employs a GPT-like Transformer  as the sequence model.
* In contrast to IRIS  that employs multiple tokens, STORM utilizes a single stochastic latent variable to represent an image.
* STORM follows a vanilla Transformer  structure, while TWM  adopts a Transformer-XL  structure.
* In the sequence model of STORM, an observation and an action are fused into a single token, whereas TWM  treats observation, action, and reward as three separate tokens of equal importance.
* Unlike Dreamer  and TransDreamer , which incorporate hidden states, STORM reconstructs the original image without utilizing this information.

## 3 Method

### World model learning

Our approach adheres to the established framework of model-based RL algorithms, which focus on enhancing the agent's policy by imagination . We iterate through the following steps until reaching the prescribed number of real environment interactions.

1. Gather real environment data by executing the current policy for several steps and append them to the replay buffer.
2. Update the world model using trajectories sampled from the replay buffer.
3. Improve the policy using imagined experiences generated by the world model, with the starting points for the imagination process sampled from the replay buffer.

At each time \(t\), a data point comprises an observation \(o_{t}\), an action \(a_{t}\), a reward \(r_{t}\), and a continuation flag \(c_{t}\) (a Boolean variable indicating whether the current episode is ongoing). The replay buffer maintains a first-in-first-out queue structure, enabling the sampling of consecutive trajectories from the buffer.

Section 3.1 provides a detailed description of the architecture and training losses employed by STORM. On the other hand, Section 3.2 elaborates on the imagination process and the training methodology employed by the agent. It provides an thorough explanation of how the agent leverages the world model to simulate experiences and improve its policy.

Figure 2: Structure and imagination process of STORM. The symbols used in the figure are explained in Sections 3.1 and 3.2. The Transformer blocks depict the sequence model \(f_{}\) in Equation (2). The Agent block, represented by a neural network, corresponds to \(_{}(a_{t}|s_{t})\) in Equation (6).

Model structureThe complete structure of our world model is illustrated in Figure 2. In our experiments, we focus on Atari games , which generate image observations \(o_{t}\) of the environment. Modeling the dynamics of the environment directly on raw images is computationally expensive and prone to errors . To address this, we leverage a VAE  formulated in Equation (1) to convert \(o_{t}\) into latent stochastic categorical distributions \(_{t}\). Consistent with prior work , we set \(_{t}\) as a stochastic distribution comprising \(32\) categories, each with \(32\) classes. The encoder (\(q_{}\)) and decoder (\(p_{}\)) structures are implemented as convolutional neural networks (CNNs) . Subsequently, we sample a latent variable \(z_{t}\) from \(_{t}\) to represent the original observation \(o_{t}\). Since sampling from a distribution lacks gradients for backward propagation, we apply the straight-through gradients trick  to preserve them.

\[& z_{t} q_{ }(z_{t}|o_{t})=_{t}\\ &_{t}=p_{}(z_{t}). \] (1)

Before entering the sequence model, we combine the latent sample \(z_{t}\) and the action \(a_{t}\) into a single token \(e_{t}\) using multi-layer perceptrons (MLPs) and concatenation. This operation, denoted as \(m_{}\), prepares the inputs for the sequence model. The sequence model \(f_{}\) takes the sequence of \(e_{t}\) as input and produces hidden states \(h_{t}\). We adopt a GPT-like Transformer structure  for the sequence model, where the self-attention blocks are masked with a subsequent mask allowing \(e_{t}\) to attend to the sequence \(e_{1},e_{2},,e_{t}\). By utilizing MLPs \(g_{}^{D}\), \(g_{}^{R}\), and \(g_{}^{C}\), we rely on \(h_{t}\) to predict the current reward \(_{t}\), the continuation flag \(_{t}\), and the next distribution \(}_{t+1}\). The formulation of this part of the world model is as follows

\[& e_{t}=m_{}(z_{t},a_{t})\\ & h_{1:T}=f_{}(e_{1:T})\\ &}_{t+1}=g_{ }^{D}(_{t+1}|h_{t})\\ &_{t}=g_{}^{R}(h_{t}) \\ &_{t}=g_{}^{C}(h_{t}).\] (2)

Loss functionsThe world model is trained in a self-supervised manner, optimizing it end-to-end. The total loss function is calculated as in Equation (3) below, with fixed hyperparameters \(_{1}=0.5\) and \(_{2}=0.1\). In the equation, \(B\) denotes the batch size, and \(T\) denotes the batch length

\[()=_{n=1}^{B}_{t=1}^{T} _{t}^{}()+_{t}^{}()+_{t}^{ }()+_{1}_{t}^{}()+_{2} _{t}^{}().\] (3)

The individual components of the loss function are defined as follows: \(_{t}^{}()\) represents the reconstruction loss of the original image, \(_{t}^{}()\) represents the prediction loss of the reward, and \(_{t}^{}()\) represents the prediction loss of the continuation flag.

\[_{t}^{}() =||_{t}-o_{t}||_{2}\] (4a) \[_{t}^{}() =^{}(_{t},r_{t})\] (4b) \[_{t}^{}() =c_{t}_{t}+(1-c_{t})(1-_{t}).\] (4c)

Additionally, \(^{}\) in Equation (4b) denotes the symlog two-hot loss, as described in . This loss function transforms the regression problem into a classification problem, ensuring consistent loss scaling across different environments.

The losses \(_{t}^{}()\) and \(_{t}^{}()\) are expressed as Kullback-Leibler (KL) divergences but differ in their gradient backward and weighting. The dynamics loss \(_{t}^{}()\) guides the sequence model in predicting the next distribution, while the representation loss \(_{t}^{}()\) allows the output of the encoder to be weakly influenced by the sequence model's prediction. This ensures that the learning of distributional dynamics is not excessively challenging.

\[_{t}^{}() =1,(q_{} (z_{t+1}|o_{t+1})) g_{}^{D}(_{t+1}|h_{t})\] (5a) \[_{t}^{}() =1,q_{}(z_{t+1}|o_{t+1} )(g_{}^{D}(_{t+1}|h_{t}))\] (5b)

where \(()\) denotes the operation of stop-gradients.

### Agent learning

The agent's learning is solely based on the imagination process facilitated by the world model, as illustrated in Figure 2. To initiate the imagination process, a brief contextual trajectory is randomly selected from the replay buffer, and the initial posterior distribution \(_{t}\) is computed. During inference, rather than sampling directly from the posterior distribution \(_{t}\), we sample \(z_{t}\) from the prior distribution \(}_{t}\). To accelerate the inference, we employ the KV cache technique  within the Transformer structure.

The agent's state is formed by concatenating \(z_{t}\) and \(h_{t}\), as shown below:

\[ s_{t} =[z_{t},h_{t}]\] \[ V_{}(s_{t}) _{_{},p_{}}_{k=0}^{ }^{k}r_{t+k}\] (6) \[ a_{t} _{}(a_{t}|s_{t}).\]

We adopt the actor learning settings from DreamerV3 . The complete loss of the actor-critic algorithm is described by Equation (7), where \(_{t}\) corresponds to the reward predicted by the world model, and \(_{t}\) represents the predicted continuation flag:

\[() =_{n=1}^{B}_{t=1}^{L}- ^{}-V_{}(s_{t})}{(1,S)}_{ }(a_{t}|s_{t})- H_{}(a_{t}|s_{t})\] (7a) \[() =_{n=1}^{B}_{t=1}^{L}V_{ }(s_{t})-G_{t}^{}^{2}+V_{ }(s_{t})-V_{}(s_{t})^{2} \] (7b)

where \(H()\) denotes the entropy of the policy distribution, while constants \(\) and \(L\) represent the coefficient for entropy loss and the imagination horizon, respectively. The \(\)-return \(G_{t}^{}\)[5; 10] is recursively defined as follows

\[G_{t}^{}  r_{t}+ c_{t}(1-)V_{}(s_{t+1})+  G_{t+1}^{}\] (8a) \[G_{L}^{}  V_{}(s_{L}).\] (8b)

The normalization ratio \(S\) utilized in the actor loss (7a) is defined in Equation (9), which is computed as the range between the \(95\)th and \(5\)th percentiles of the \(\)-return \(G_{t}^{}\) across the batch 

\[S=(G_{t}^{},95)-(G_{t}^{ },5).\] (9)

To regularize the value function, we maintain the exponential moving average (EMA) of \(\). The EMA is defined in Equation (10), where \(_{t}\) represents the current critic parameters, \(\) is the decay rate, and \(_{t+1}^{}\) denotes the updated critic parameters. This regularization technique aids in stabilizing training and preventing overfitting

\[_{t+1}^{}=_{t}^{}+(1-)_{t}.\] (10)

## 4 Experiments

We evaluated the performance of STORM on the widely-used benchmark for sample-efficient RL, Atari 100k . For detailed information about the benchmark, evaluation methodology, and the baselines used for comparison, please refer to Section 4.1. The comprehensive results for the Atari 100k games are presented in Section 4.2.

### Benchmark and baselines

Atari \(100\)k consists of \(26\) different video games with discrete action dimensions of up to \(18\). The \(100\)k sample constraint corresponds to \(400\)k actual game frames, taking into account frame skipping (\(4\) frames skipped) and repeated actions within those frames. This constraint corresponds toapproximately \(1.85\) hours of real-time gameplay. The agent's human normalized score \(=\) is calculated based on the score \(A\) achieved by the agent, the score \(R\) obtained by a random policy, and the average score \(H\) achieved by a human player in a specific environment. To determine the human player's performance \(H\), a player is allowed to become familiar with the game under the same sample constraint.

To demonstrate the efficiency of our proposed world model structure, we compare it with model-based DRL algorithms that share a similar training pipeline, as discussed in Section 2. However, similarly to [10; 12; 13], we do not directly compare our results with lookahead search methods like MuZero  and EfficientZero , as our primary goal is to refine the world model itself. Nonetheless, lookahead search techniques can be combined with our method in the future to further enhance the agent's performance.

### Results on Atari 100k

Detailed results for each environment can be found in Table 2, and the corresponding performance curve is presented in Appendix A due to space limitations. In our experiments, we trained STORM using \(5\) different seeds and saved checkpoints every \(2,500\) sample steps. We assessed the agent's performance by conducting \(20\) evaluation episodes for each checkpoint and computed the average score. The result reported in Table 2 is the average of the scores attained using the final checkpoints.

STORM demonstrates superior performance compared to previous methods in environments where the key objects related to rewards are large or multiple, such as _Amidar_, _MsPacman_, _Chopper Command_, and _Gopher_. This advantage can be attributed to the attention mechanism, which explicitly preserves the history of these moving objects, allowing for an easy inference of their speed and direction information, unlike RNN-based methods. However, STORM faces challenges when handling a single small moving object, as observed in _Pong_ and _Breakout_, due to the nature of autoencoders. Moreover, performing attention operations under such circumstances can potentially harm performance, as the randomness introduced by sampling may excessively influence the attention weights.

   Game & Random & Human & SimPLe  & TWM  & IRIS  & DreamerV3  & STORM (ours) \\  Alien & 228 & 7128 & 617 & 675 & 420 & **959** & **984** \\ Amidar & 6 & 1720 & 74 & 122 & 143 & 139 & **205** \\ Assault & 222 & 742 & 527 & 683 & **1524** & 706 & 801 \\ Asterix & 210 & 8503 & **1128** & **1116** & 854 & 932 & 1028 \\ Bank Heist & 14 & 753 & 34 & 467 & 53 & **649** & **641** \\ Battle Zone & 2360 & 37188 & 4031 & 5068 & **13074** & 12250 & **13540** \\ Boxing & 0 & 12 & 8 & **78** & 70 & **78** & **80** \\ Breakout & 2 & 30 & 16 & 20 & **84** & 31 & 16 \\ Chopper Command & 811 & 7388 & 979 & 1697 & 1565 & 420 & **1888** \\ Crazy Climber & 10780 & 35829 & 62584 & 71820 & 59234 & **97190** & 66776 \\ Demon Attack & 152 & 1971 & 208 & 350 & **2034** & 303 & 165 \\ Freeway & 0 & 30 & 17 & 24 & **31** & 0 & **34** \\ Freeway w/o traj & 0 & 30 & 17 & 24 & **31** & 0 & 0 \\ Frostbite & 65 & 4335 & 237 & **1476** & 259 & 909 & 1316 \\ Gopher & 258 & 2413 & 597 & 1675 & 2236 & 3730 & **8240** \\ Hero & 1027 & 30826 & 2657 & 7254 & 7037 & **11161** & **11044** \\ James Bond & 29 & 303 & 101 & 362 & 463 & 445 & **509** \\ Kangaroo & 52 & 3035 & 51 & 1240 & 838 & **4098** & **4208** \\ Krull & 1598 & 2666 & 2204 & 6349 & 6616 & 7782 & **8413** \\ Kung Fu Master & 256 & 22736 & 14862 & 24555 & 21760 & 21420 & **26182** \\ Ms Pacman & 307 & 6952 & 1480 & 1588 & 999 & 1327 & **2673** \\ Pong & -21 & 15 & 13 & **19** & 15 & **18** & 11 \\ Private Eye & 25 & 69571 & 35 & 87 & 100 & 882 & **7781** \\ Qbert & 164 & 13455 & 1289 & 3331 & 746 & 3405 & **4522** \\ Road Runner & 12 & 7845 & 5641 & 9109 & 9615 & 15565 & **17564** \\ Seaquest & 68 & 42055 & 683 & **774** & 661 & 618 & 525 \\ Up N Down & 533 & 11693 & 3350 & **15982** & 3546 & 7667 & 7985 \\  Human Mean & 0\% & 100\% & 33\% & 96\% & 105\% & 112\% & **126.7\%** \\ Human Median & 0\% & 100\% & 13\% & 51\% & 29\% & 49\% & **58.4\%** \\   

Table 2: Game scores and overall human-normalized scores on the \(26\) games in the Atari \(100\)k benchmark. Following the conventions of , scores that are the highest or within \(5\%\) of the highest score are highlighted in bold.

## 5 Ablation studies

In our experiments, we have observed that the design and configuration choices of the world model and the agent can have significant impacts on the final results. To further investigate this, we conduct ablation studies on the design and configuration of the world model in Section 5.1, as well as on the agent's design in Section 5.2. Additionally, we propose a novel approach to enhancing the exploration efficiency through the imagination capability of the world model using a single demonstration trajectory, which is explained in Section 5.3.

### World model design and configuration

The RNN-based world models utilized in SimPLe  and Dreamer [9; 10] can be formulated clearly using variational inference over time. However, the non-recursive Transformer-based world model does not align with this practice and requires manual design. Figure 2(a) shows alternative structures and their respective outcomes. In the "Decoder at rear" configuration, we employ \(z_{t}}_{t}\) instead of \(z_{t}_{t}\) for reconstructing the original observation and calculating the loss. The results indicate that the reconstruction loss should be applied directly to the output of the encoder rather than relying on the sequence model. In the "Predictor at front" setup, we utilize \(z_{t}\) as input for \(g_{}^{R}()\) and \(g_{}^{C}()\) in Equation (2), instead of \(h_{t}\). These findings indicate that, while this operation has minimal impact on the final performance for tasks where the reward can be accurately predicted from a single frame (in e.g., _Pong_), it leads to a performance drop on tasks that require several contextual frames to predict the reward accurately (in e.g., _Ms. Pacman_).

By default, we configure our Transformer with \(2\) layers, which is significantly smaller than the \(10\) layers used in IRIS  and TWM . Figure 2(b) presents the varied outcomes obtained by increasing the number of Transformer layers. The results reveal that increasing the layer count does not have a positive impact on the final performance. However, in the case of the game _Pong_, even when the sample limit is increased from \(100\)k to \(400\)k, the agent still achieves the maximum reward in this environment regardless of whether a \(4\)-layer or \(6\)-layer Transformer is employed. This scaling discrepancy, which differs from the success observed in other fields [36; 37; 38], may be attributed to three reasons. Firstly, due to the minor difference between adjacent frames and the presence of residual connections in the Transformer structure , predicting the next frame may not require a complex model. Secondly, training a large model naturally requires a substantial amount of data, yet the Atari \(100\)k games neither provide images from diverse domains nor offer sufficient samples for training a larger model. Thirdly, the world model is trained end-to-end, and the representation loss \(^{}\) in Equation (5b) directly influences the image encoder. The encoder may be overly influenced when tracking the output of a large sequence model.

### Selection of the agent's state

The choice of the agent's state \(s_{t}\) offers several viable options: \(_{t}\), \(h_{t}\), \(z_{t}\) (as in TWM ), or the combination \([h_{t},z_{t}]\) (as demonstrated by Dreamer, [9; 10]). In the case of STORM, we employ \(s_{t}=[h_{t},z_{t}]\), as in Equation (6). Ablation studies investigating the selection of the agent's state are presented in Figure 4. The results indicate that, in environments where a good policy requires contextual information, such as in _Ms. Pacman_, the inclusion of \(h_{t}\) leads to improved performance. However, in other environments like _Pong_ and _Kung Fu Master_, this inclusion does not yield a significant difference. When solely utilizing \(h_{t}\) in environments that evolve with the agent's policy,

Figure 3: Ablation studies on the design and configuration of the STORM’s world model.

like _Pong_, the agent may exhibit behaviors similar to catastrophic forgetting  due to the non-stationary and inaccurate nature of the world model. Consequently, the introduction of randomness through certain distributions like \(_{t}\) proves to be beneficial.

### Impact of the demonstration trajectory

The inclusion of a demonstration trajectory is a straightforward implementation step when using a world model, and it is often feasible in real-world settings. Figure 5 showcases the impact of incorporating a single demonstration trajectory in the replay buffer. Details about the provided trajectory can be found in Appendix D. In environments with sparse rewards, adding a trajectory can improve the robustness, as observed in _Pong_, or the performance, as seen in _Freeway_. However, in environments with dense rewards like _Ms Pacman_, including a trajectory may hinder the policy improvement of the agent.

_Freeway_ serves as a prototypical environment characterized by challenging exploration but a simple policy. To receive a reward, the agent must take the "up" action approximately \(70\) times in a row, but it quickly improves its policy once the first reward is obtained. Achieving the first reward is extremely challenging if the policy is initially set as a uniform distribution of actions. In the case of TWM , the entropy normalization technique is employed across all environments, while IRIS  specifically reduces the temperature of the Boltzmann exploration strategy for _Freeway_. These tricks are critical in obtaining the first reward in this environment. It is worth noting that even for most humans, playing exploration-intensive games without prior knowledge is challenging. Typically, human players require instructions about the game's objectives or watch demonstrations from teaching-level or expert players to gain initial exploration directions. Inspired by this observation, we aim to directly incorporate a demonstration trajectory to train the world model and establish starting points for imagination. By leveraging a sufficiently robust world model, the utilization of limited offline information holds the potential to surpass specially designed curiosity-driven exploration strategies in the future.

As an integral part of our methodology, we integrate a single trajectory from _Freeway_ into our extensive results. Furthermore, to ensure fair comparisons in future research, we provide the results without incorporating _Freeway_'s trajectory in Table 2 and Figure 6. It is important to highlight that even without this trajectory, our approach consistently outperforms previous methods, attaining a mean human normalized score of \(122.3\%\).

Figure 4: Ablation studies on the selection of the agent’s state.

Figure 5: Ablations studies on adding a demonstration trajectory to the replay buffer.

Conclusions and limitations

In this work, we introduce STORM, an efficient world model architecture for model-based RL, surpassing previous methods in terms of both performance and training efficiency. STORM harnesses the powerful sequence modeling and generation capabilities of the Transformer structure while fully exploiting its parallelizable training advantages. The improved efficiency of STORM broadens its applicability across a wider range of tasks while reducing computational costs.

Nevertheless, it is important to acknowledge certain limitations. Firstly, both the world model of STORM and the compared baselines are trained in an end-to-end fashion, where the image encoder and sequence model undergo joint optimization. As a result, the world model must predict its own internal output, introducing additional non-stationarity into the optimization process and potentially impeding the scalability of the world model. Secondly, the starting points for imagination are uniformly sampled from the replay buffer, while the agent is optimized using an on-policy actor-critic algorithm. Although acting in the world model is performed on-policy, the corresponding on-policy distribution \((s)\) for these starting points is not explicitly considered, despite its significance in the policy gradient formulation: \( J()_{s}(s)_{a}q_{_{}}(s,a)_{ }(a|s)\).