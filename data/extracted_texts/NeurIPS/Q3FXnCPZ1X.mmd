# Fast and Simple Spectral Clustering

in Theory and Practice

 Peter Macgregor

School of Informatics

University of Edinburgh

peter.macgregor@ed.ac.uk

###### Abstract

Spectral clustering is a popular and effective algorithm designed to find \(k\) clusters in a graph \(G\). In the classical spectral clustering algorithm, the vertices of \(G\) are embedded into \(^{k}\) using \(k\) eigenvectors of the graph Laplacian matrix. However, computing this embedding is computationally expensive and dominates the running time of the algorithm. In this paper, we present a simple spectral clustering algorithm based on a vertex embedding with \(O((k))\) vectors computed by the power method. The vertex embedding is computed in nearly-linear time with respect to the size of the graph, and the algorithm provably recovers the ground truth clusters under natural assumptions on the input graph. We evaluate the new algorithm on several synthetic and real-world datasets, finding that it is significantly faster than alternative clustering algorithms, while producing results with approximately the same clustering accuracy.

## 1 Introduction

Graph clustering is an important problem with numerous applications in machine learning, data science, and theoretical computer science. Spectral clustering is a popular graph clustering algorithm with strong theoretical guarantees and excellent empirical performance. Given a graph \(G\) with \(n\) vertices and \(k\) clusters, the classical spectral clustering algorithm consists of the following two high-level steps .

1. Embed the vertices of \(G\) into \(^{k}\) according to \(k\) eigenvectors of the graph Laplacian matrix.
2. Apply a \(k\)-means clustering algorithm to partition the vertices into \(k\) clusters.

Recent work shows that if the graph has a well-defined cluster structure, then the clusters are well-separated in the spectral embedding and the \(k\)-means algorithm will return clusters which are close to optimal .

The main downside of this algorithm is the high computational cost of computing \(k\) eigenvectors of the graph Laplacian matrix. In this paper, we address this computational bottleneck and propose a new fast spectral clustering algorithm which avoids the need to compute eigenvectors while maintaining excellent theoretical guarantees. Moreover, our proposed algorithm is simple, fast, and effective in practice.

### Sketch of Our Approach

In this section, we introduce the high-level idea of this paper, which is also illustrated in Figure 1. We begin by considering a recent result of Makarychev et al.  who show that a random projection of data into \(O((k))\) dimensions preserves the \(k\)-means objective function for all partitions of thedata, with high probability. Since the final step of spectral clustering is to apply \(k\)-means, we might consider the following alternative spectral clustering algorithm which will produce roughly the same output as the classical algorithm.

1. Embed the vertices of \(G\) into \(^{k}\) according to \(k\) eigenvectors of the graph Laplacian matrix.
2. Randomly project the embedded points into \(O((k))\) dimensions.
3. Apply a \(k\)-means clustering algorithm to partition the vertices into \(k\) clusters.

Of course, this does not avoid the expensive eigenvector computation and so it is not immediately clear that this random projection can be used to improve the spectral clustering algorithm.

The key technical element of our paper is a proof that we can efficiently approximate a random projection of the spectral embedding without computing the spectral embedding itself. For this, we use the power method, which is a well-known technique in numerical linear algebra for approximating the dominant eigenvalue of a matrix . We propose the following simple algorithm (formally described in Algorithm 2).

1. Embed the vertices of \(G\) into \(O((k))\) dimensions using \(O((k))\) random vectors computed with the power method.
2. Apply a \(k\)-means clustering algorithm to partition the vertices into \(k\) clusters.

We prove that the projection obtained using the power method is approximately equivalent to a random projection of the spectral embedding. Then, by carefully applying the techniques developed by Makarychev et al.  and Macgregor and Sun , we obtain a theoretical bound on the number of vertices misclassified by our proposed algorithm. Moreover, the time complexity of step \(1\) is nearly linear in the size of the graph, and the algorithm is fast in practice. The formal theoretical guarantee is given in Theorem 3.1.

Figure 1: An illustration of the steps of the spectral clustering algorithm, and the contribution of this paper. We are given a graph as input. In classical spectral clustering we follow the top path: we compute the spectral embedding and apply a \(k\)-means algorithm to find clusters. Through the recent result of Makarychev et al. , we can project the embedded points into \(O((k))\) dimensions and obtain approximately the same clustering. In this paper, we show that it is possible to compute the low-dimensional embedding directly with the power method, skipping the computationally expensive step of computing \(k\) eigenvectors.

### Related Work

This paper is closely related to a sequence of recent results which prove an upper bound on the number of vertices misclassified by the classical spectral clustering algorithm [15; 21; 24; 28]. While we will directly compare our result with these in a later section, our proposed algorithm has a much faster running time than the classical spectral clustering algorithm, and has similar theoretical guarantees.

Boutsidis et al.  also study spectral clustering using the power method. Our result improves on theirs in two respects. Firstly, our algorithm is faster since we compute \(O((k))\) vectors rather than \(k\) vectors and their algorithm includes an additional singular value decomposition step. Secondly, we give a theoretical upper bound on the total number of vertices misclassified by our algorithm.

Makarychev et al.  generalise the well-known Johnson-Lindenstrauss lemma  to show that random projections of data into \(O((k))\) dimensions preserves the \(k\)-means objective, and we make use of their result in our analysis.

Macgregor and Sun  show that for graphs with certain structures of clusters, spectral clustering with fewer than \(k\) eigenvectors performs better than using \(k\) eigenvectors. In this paper, we present the first proof that embedding with \(O((k))\) vectors is sufficient to find \(k\) clusters with spectral clustering.

Other proposed methods for fast spectral clustering include the Nystrom method  and using a 'pre-clustering' step to reduce the number of data points to be clustered . These methods lack rigorous theoretical guarantees on the accuracy of the returned clustering. Moreover, our proposed algorithm is significantly simpler to implement that the alternative methods.

## 2 Preliminaries

Let \(G=(V,E,w)\) be a graph with \(n=|V|\) and \(m=|E|\). For any \(v V\), the degree of \(v\) is given by \(d(v)=_{u v}w(u,v)\). For any \(S V\), the volume of \(S\) is given by \((S)=_{u S}d(u)\). The Laplacian matrix of \(G\) is \(=-\) where \(\) is the diagonal matrix with \((i,i)=d(i)\) and \(\) is the adjacency matrix of \(G\). The normalised Laplacian is given by \(=^{-}^{-}\). We always use \(_{1}_{2}_{n}\) to be the eigenvalues of \(\) and the corresponding eigenvectors are \(_{1},,_{n}\). For any graph, it holds that \(_{1}=0\) and \(_{n} 2\). We will also use the signless Laplacian matrix1\(=-(1/2)\) and will let \(_{1}_{n}\) be the eigenvalues of \(\). Notice that by the definition of \(\), we have \(_{i}=1-(1/2)_{i}\) and the eigenvectors of \(\) are also \(_{1},,_{n}\). For an integer \(k\), we let \([k]=\{1, k\}\) be the set of all positive integers less than or equal to \(k\). Given two sets \(A\) and \(B\), their symmetric difference is given by \(A B=(A B)(B A)\). We call \(\{S_{i}\}_{i=1}^{k}\) a \(k\)-way partition of \(V\) if \(S_{i} S_{j}=\) for \(i j\) and \(_{i=1}^{k}S_{i}=V\).

Throughout the paper, we use big-O notation to hide constants. For example, we use \(l=O(n)\) to mean that there exists a universal constant \(c\) such that \(l cn\). We sometimes use \((n)\) in place of \(O(n^{c}(n))\) for some constant \(c\). Following , we say that a partition \(\{S_{i}\}_{i=1}^{k}\) of \(V\) is almost-balanced if \((S_{i})=((V)/k)\) for all \(i[k]\).

### Conductance and the Graph Spectrum

Given a graph \(G=(V,E)\), and a cluster \(S V\), the conductance of \(S\) is given by

\[(S))}{\{(S), ()\}}\]

where \(w(S,)=_{u S}_{v}w(u,v)\). Then, the \(k\)-way expansion of \(G\) is defined to be

\[(k)_{C_{1}, C_{k}}_{i}(C_{i}).\]

Notice that \((k)\) is small if and only if \(G\) can be partitioned into \(k\) clusters of low conductance. There is a close connection between the \(k\)-way expansion of \(G\) and the eigenvalues of the graph Laplacian matrix, as shown in the following higher-order Cheeger inequality.

**Lemma 2.1** (Higher-Order Cheeger Inequality, ).: _For a graph \(G\), let \(_{1}_{n}\) be the eigenvalues of the normalised Laplacian matrix. Then, for any \(k\),_

\[}{2}(k) Ok^{3}\,}.\]

From this lemma, we can see that an upper bound on \((k)\) and a lower bound on \(_{k+1}\) are sufficient conditions to guarantee that \(G\) can be partitioned into \(k\) clusters of low conductance, and cannot be partitioned into \(k+1\) clusters. This condition is commonly used in the analysis of graph clustering [15; 21; 24; 28].

### The \(k\)-means Objective

For any matrix \(^{n d}\), the \(k\)-means cost of a \(k\)-way partition \(\{A_{i}\}_{i=1}^{k}\) of the data points is given by

\[_{}(A_{1},,A_{k})_{i= 1}^{k}_{u A_{i}}\|(u,:)-_{i}\|_{2}^{2},\]

where \((u,:)\) is the \(u\)th row of \(\) and \(_{i}=(1/|A_{i}|)_{u A_{i}}(u,:)\) is the mean of the points in \(A_{i}\). Although optimising the \(k\)-means objective is \(\)-hard, there is a polynomial-time constant factor approximation algorithm , and an approximation scheme which is polynomial in \(n\) and \(d\) with an exponential dependency on \(k\)[8; 16]. Lloyd's algorithm , with \(k\)-means++ initialisation  is an \(O((n))\)-approximation algorithm which is fast and effective in practice.

### The Power Method

The power method is an algorithm which is most often used to approximate the dominant eigenvalue of a matrix [11; 25]. Given some matrix \(^{n n}\), a vector \(_{0}^{n}\), and a positive integer \(t\), the power method computes the value of \(^{t}_{0}\) by repeated multiplication by \(\). The formal algorithm is given in Algorithm 1.

```
1for\(i\{1,,t\}\)do
2\(_{i}=_{i-1}\)
3 end for return\(_{t}\) ```

**Algorithm 1**PowerMethod\((^{n n},_{0}^{n},t_{ 0})\)

## 3 Algorithm Description and Analysis

In this section, we present our newly proposed algorithm and sketch the proof of our main result. Omitted proofs can be found in the Appendix. We first prove that if \(\) has \(k\) eigenvalues \(_{1},,_{k}\) close to \(1\), then the power method can be used to compute a random vector in the span of the eigenvectors corresponding to \(_{1},,_{k}\).

We then apply this result to the signless Laplacian matrix of a graph to develop a fast spectral clustering algorithm and we bound the number of misclassified vertices when the algorithm is applied to a well-clustered graph.

### Approximating a Random Vector with the Power Method

Suppose we are given some matrix \(^{n n}\) with eigenvalues \(1_{1}_{n} 0\) and corresponding eigenvectors \(_{1},,_{n}\). Typically, the power method is used to approximate the dominant eigenvector, \(_{1}\). In this section, we show that when \(_{k}\) is sufficiently close to \(1\), the power method can be used to compute a random vector in the space spanned by \(_{1},,_{k}\).

Let \(_{0}^{n}\) be a random vector chosen according to the \(n\)-dimensional Gaussian distribution \((,)\). We can write \(_{0}\) as a linear combination of the eigenvectors:

\[_{0}=_{i=1}^{n}a_{i}_{i},\]

where \(a_{i}=_{0},_{i}\). Then, the vector \(_{t}\) computed by \((,_{0},t)\) can be written as

\[_{t}=_{i=1}^{n}a_{i}_{i}^{t}_{i}.\]

Informally, if \(_{k} 1-c_{1}\) and \(_{k+1} 1-c_{2}\) for sufficiently small \(c_{1}\) and sufficiently large \(c_{2}\), then for a carefully chosen value of \(t\) we have \(_{i}^{t} 1\) for \(i k\) and \(_{i}^{t} 0\) for \(i k+1\). This implies that

\[_{t}_{i=1}^{k}a_{i}_{i}=(_{i=1}^{k}_{i} _{i}^{})_{0},\]

and we observe that \((_{i=1}^{k}_{i}_{i}^{})_{0}\) is a random vector distributed according to a \(k\)-dimensional Gaussian distribution in the space spanned by \(_{1},,_{k}\). We make this intuition formal in Lemma 3.1 and specify the required conditions on \(_{k}\), \(_{k+1}\) and \(t\).

**Lemma 3.1**.: _Let \(^{n n}\) be a matrix with eigenvalues \(1_{1}_{n} 0\) and corresponding eigenvectors \(_{1},_{n}\). Let \(_{0}^{n}\) be drawn from the \(n\)-dimensional Gaussian distribution \(N(,)\). Let \(_{t}=(,_{0},t)\) for \(t=(n/^{2}k)\). If \(_{k} 1-O(n/^{2}k)^{-1}\) and \(_{k+1} 1-(1)\), then with probability at least \(1-1/(10k)\),_

\[\|_{t}-}_{0}\|_{2},\]

_where \(}=_{i=1}^{k}_{i}_{i}^{}\) is the projection onto the space spanned by the first \(k\) eigenvectors of \(}\)._

### The Fast Spectral Clustering Algorithm

We now introduce the fast spectral clustering algorithm. The algorithm follows the pattern of the classical spectral clustering algorithm, with an important difference: rather than embedding the vertices according to \(k\) eigenvectors of the graph Laplacian, we embed the vertices with \((k)^{-2}\) random vectors computed with the power method for the signless graph Laplacian \(}\). Algorithm 2 formally specifies the algorithm, and the theoretical guarantees are given in Theorem 3.1.

**Theorem 3.1**.: _Let \(G\) be a graph with \(_{k+1}=(1)\) and \((k)=O(n/)^{-1}\). Additionally, let \(\{S_{i}\}_{i=1}^{k}\) be the \(k\)-way partition corresponding to \((k)\) and suppose that \(\{S_{i}\}_{i=1}^{k}\) are almost balanced. Let \(\{A_{i}\}_{i=1}^{k}\) be the output of Algorithm 2. With probability at least \(0.9-\), there exists a permutation \(:[k][k]\) such that_

\[_{i=1}^{k}(A_{i} S_{(i)})=O( (V_{G}))\,.\]

_Moreover, the running time of Algorithm 2 is_

\[m^{-2}+T_{}(n,k,l),\]

_where \(m\) is the number of edges in \(G\) and \(T_{}(n,k,l)\) is the running time of the \(k\)-means approximation algorithm on \(n\) points in \(l\) dimensions._

**Remark 3.1**.: _The assumptions on \(_{k+1}\) and \((k)\) in Theorem 3.1 imply that the graph \(G\) can be partitioned into exactly \(k\) clusters of low conductance. This is related to previous results which make an assumption on the ratio \(_{k+1}/(k)\). Macgregor and Sun  prove a guarantee like Theorem 3.1 under the assumption that \(_{k+1}/(k)=(1)\). We achieve a faster algorithm under a slightly stronger assumption._

**Remark 3.2**.: _The running time of Theorem 3.1 improves on previous spectral clustering algorithms. Boutsidis et al.  describe an algorithm with running time \(m k^{-2}+Ok^{2} n +T_{}(n,k,k)\). Moreover, their analysis does not provide any guarantee on the number of misclassified vertices._

**Remark 3.3**.: _The constants in the definition of \(l\) and \(t\) in Algorithm 2 are based on those in the analysis of Makarychev et al.  and this paper. In practice, we find that setting \(l=(k)\) and \(t=10(n/k)\) works well._

Throughout the remainder of this section, we will sketch the proof of Theorem 3.1. We assume that \(G=(V,E)\) is a graph with \(k\) clusters \(\{S_{i}\}_{i=1}^{k}\) of almost balanced size, \(_{k+1}=(1)\), and \((k)=O(n/)^{-1}\).

In order to understand the behaviour of the \(k\)-means algorithm on the computed vectors, we will analyse the \(k\)-means cost of a given partition under three different embeddings of the vertices. Let \(_{1},,_{k}\) be the eigenvectors of \(\) corresponding to the eigenvalues \(_{1},,_{k}\) and let \(_{1},,_{l}\) be the vectors computed in Algorithm 2. We will also consider the vectors \(_{1},,_{l}\) given by \(_{i}=_{i}\), where \(\{_{i}\}_{i=1}^{k}\) are the random vectors sampled in Algorithm 2, and \(=_{i=1}^{k}_{i}_{i}^{}\) is the projection onto the space spanned by \(_{1},,_{k}\). Notice that each \(_{i}\) is a random vector distributed according to the \(k\)-dimensional Gaussian distribution. Furthermore, let

\[=&&\\ _{1}&&_{k}\\ &&,=&& \\ _{1}&&_{l}\\ &&= &&\\ _{1}&&_{l}\\ &&.\]

We will consider the vertex embeddings given by \(^{-1/2}\), \(^{-1/2}\) and \(^{-1/2}\) and show that the \(k\)-means objective for every \(k\)-way partition is approximately equal in each of them. We will use the following result shown by Makarychev et al. .

**Lemma 3.2** (, Theorem 1.3).: _Given data \(^{n k}\), let \(^{k l}\) be a random matrix with each column sampled from the \(k\)-dimensional Gaussian distribution \((,_{k})\) and_

\[l=O}\,.\]

_Then, with probability at least \(1-\), it holds for all partitions \(\{A_{i}\}_{i=1}^{k}\) of \([n]\) that_

\[_{}(A_{1},,A_{k})(1)_{ }(A_{1},,A_{k}).\]

Applying this lemma with \(=^{-}\) and \(=^{}\) shows that the \(k\)-means cost is approximately equal in the embeddings given by \(^{-}\) and \(^{-}\), since \(^{}=\) and each of the entries of \(^{}\) is distributed according to the Gaussian distribution \((0,1)\).2 By Lemma 3.1, we can also show that the \(k\)-means objective in \(^{-}\) is within an additive error of \(^{-}\). This allows us to prove the following lemma.

**Lemma 3.3**.: _With probability at least \(0.9-\), for any partitioning \(\{A_{i}\}_{i=1}^{k}\) of the vertex set \(V\), we have_

\[_{^{-1/2}}(A_{1},,A_{k}) (1-)_{^{-1/2}}(A_{1},,A_{k})- k\] _and_ \[_{^{-1/2}}(A_{1},,A_{k}) (1+)_{^{-1/2}}(A_{1},,A_{k})+ k.\]To complete the proof of Theorem 3.1, we will make use of the following results proved by Macgregor and Sun .

**Lemma 3.4** (, Lemma 4.1).: _There exists a partition \(\{A_{i}\}_{i=1}^{k}\) of the vertex set \(V\) such that_

\[_{^{-1/2}}(A_{1}, A_{k})<k (k)/_{k+1}.\]

**Lemma 3.5** (, Theorem 2).: _Given some partition of the vertices, \(\{A_{i}\}_{i=1}^{k}\), such that_

\[_{^{-1/2}}(A_{1}, A_{k}) c  k,\]

_then there exists a permutation \(:[k][k]\) such that_

\[_{i=1}^{k}(A_{i} S_{(i)})=O(c (V))\,.\]

Proof of Theorem 3.1.: By Lemma 3.4 and Lemma 3.3, with probability at least \(0.9-\), there exists some partition \(\{_{i}\}_{i=1}^{k}\) of the vertex set \(V_{G}\) such that

\[_{^{-1/2}}(_{1},, _{k})=O\!((1+)+  k).\]

Since we use a constant-factor approximation algorithm for \(k\)-means, the partition \(\{A_{i}\}_{i=1}^{k}\) returned by Algorithm 2 satisfies \(_{^{-1/2}}(A_{1},,A_{k})=O(  k)\,.\) Then, by Lemma 3.5 and Lemma 3.3, for some permutation \(:[k][k]\), we have

\[_{i=1}^{k}(A_{i} S_{(i)})=O( (V_{G}))\,.\]

To bound the running time, notice that the number of non-zero entries in \(\) is \(2m\), and the time complexity of matrix multiplication is proportional to the number of non-zero entries. Therefore, the running time of \((,_{0},t)\) is \((m)\). Since the loop in Algorithm 2 is executed \(\!((k)^{-2})\) times, the total running time of Algorithm 2 is \(\!(m^{-2})+T_{}(n,k,l)\). 

## 4 Experiments

In this section, we empirically study several variants of the spectral clustering algorithm. We compare the following algorithms:

* \(k\)Eigenvectors: the classical spectral clustering algorithm which uses \(k\) eigenvectors of the graph Laplacian matrix to embed the vertices. This is the algorithm analysed in [21; 28].
* \((k)\)Eigenvectors: spectral clustering with \((k)\) eigenvectors of the graph Laplacian to embed the vertices.
* KASP: the fast spectral clustering algorithm proposed by Yan et al. . The algorithm proceeds by first coarsening the data with \(k\)-means before applying spectral clustering.
* PM \(k\)Vectors (Power Method with \(k\) vectors): spectral clustering with \(k\) orthogonal vectors computed with the power method. This is the algorithm analysed in .
* PM \((k)\)Vectors: spectral clustering with \((k)\) random vectors computed with the power method. This is Algorithm 2.

We implement all algorithms in Python, using the numpy , scipy, stag, and scikit-learn, libraries for matrix manipulation, eigenvector computation, graph processing, and \(k\)-means approximation respectively. We first compare the performance of the algorithms on synthetic graphs with a range of sizes drawn from the stochastic block model (SBM). We then study the algorithms' performance on several real-world datasets. We find that our algorithm is significantly faster than all other spectral clustering algorithm, while maintaining almost the same clustering accuracy. The most significant improvement is seen on graphs with a large number of clusters. All experiments are performed on an HP laptop with an 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz processor and 32 GB RAM. The code to reproduce the experiments is available at https://github.com/pmacg/fast-spectral-clustering.

### Synthetic Data

In this section, we evaluate the spectral clustering algorithms on synthetic data drawn from the stochastic block model. Given parameters \(n_{ 0}\), \(k_{ 0}\), \(p\), and \(q\), we generate a graph \(G=(V,E)\) with \(n\) vertices and \(k\) ground-truth clusters \(S_{1},,S_{k}\) of size \(n/k\). For any pair of vertices \(u S_{i}\) and \(v S_{j}\), we add the edge \(\{u,v\}\) with probability \(p\) if \(i=j\) and with probability \(q\) otherwise. We study the running time of the algorithms in two settings.

In the first experiment, we set \(n=1000 k\), \(p=0.04\), and \(q=1/(1000k)\). Then, we study the running time of spectral clustering for different values of \(k\). The results are shown in Figure 2(a). We observe that our newly proposed algorithm is much faster than existing methods for large values of \(k\), and our algorithm is easily able to scale to large graphs with several hundred thousand vertices.

In the second experiment, we set \(k=20\), \(p=40/n\), and \(q=1/(20n)\). Then, we study the running time of the spectral clustering algorithms for different values of \(n\). The results are shown in Figure 2(b). Empirically, we find that when \(k\) is a fixed constant, our newly proposed algorithm is faster than existing methods by a constant factor. In every case, all algorithms successfully recover the ground truth clusters.3

### Real-world Data

In this section, we evaluate spectral clustering on real-world data with labeled ground-truth clusters. We compare the algorithms on the following datasets from a variety of domains.

* **MNIST**: each data point is an image with \(28 28\) greyscale pixels, representing a hand-written digit from \(0\) to \(9\).
* **Pen Digits**: data is collected by writing digits on a digital pen tablet. Each data point corresponds to some digit from \(0\) to \(9\) and consists of \(8\) pairs of \((x,y)\) coordinates encoding the sequence of pen positions while the digit was written.
* **Fashion**: each data point is an image with \(28 28\) greyscale pixels, representing one of \(10\) classes of fashion item, such as'shoe' or'shirt'.
* **HAR** (Human Activity Recognition) : the dataset consists of pre-processed sensor data from a body-worn smartphone. Participants were asked to perform a variety of activities,

Figure 2: The running time of spectral clustering variants on graphs drawn from the stochastic block model. (a) Setting \(n=1000 k\) and increasing the number of clusters, \(k\), shows that Algorithm 2 is much faster than alternative methods for large values of \(k\). (b) Setting \(k=20\) and increasing the number of vertices, \(n\), shows that for fixed \(k\), Algorithm 2 is faster than the alternatives by a constant factor.

such as 'walking', 'walking upstairs', and'standing'. The task is to identify the activity from the sensor data.
* **Letter**: each data point corresponds to an upper-case letter from 'A' to 'Z'. The data was generated from distorted images of letters with a variety of fonts, and the features correspond to various statistics computed on the resulting images.

The datasets are all made available by the OpenML  project, and can be downloaded with the scikit-learn library . We first pre-process each dataset by computing the \(k\) nearest neighbour graph from the data, for \(k=10\). Table 2 shows the number of nodes and the number of ground truth clusters in each dataset.

For each dataset, we report the performance of each spectral clustering algorithm with respect to the running time in seconds, and the clustering accuracy measured with the Adjusted Rand Index (ARI) [10; 29] and the Normalised Mutual Information (NMI) . Table 1 summarises the results.

We find that Algorithm 2 is consistently very fast when compared to the other spectral clustering algorithms. Moreover, the clustering accuracy is similar for every algorithm.

## 5 Conclusion

In this paper, we introduced a new fast spectral clustering algorithm based on projecting the vertices of the graph into \(O((k))\) dimensions with the power method. We find that the new algorithm is faster than previous spectral clustering algorithms and achieves similar clustering accuracy.

This algorithm offers a new option for the application of spectral clustering. If a large running time is acceptable and the goal is to achieve the best accuracy possible, then our experimental results suggest that the classical spectral clustering algorithm with \(k\) eigenvectors is the optimal choice. On the other hand, when the number of clusters or the number of data points is very large, our newly proposed method provides a significantly faster algorithm for a small trade-off in terms of clustering accuracy. This could allow spectral clustering to be applied in regimes that were previously intractable, such as when \(k=(n)\).

   & &  \\   & Algorithm & MNIST & Pen Digits & Fashion & HAR & Letter \\   & \(k\) Eigs & \(2.70 0.24\) & \(0.64 0.07\) & \(3.55 0.17\) & \(0.58 0.07\) & \(29.29 11.85\) \\  & \((k)\) Eigs & \(2.73 0.20\) & \(1.01 0.05\) & \(3.79 0.11\) & \(0.83 0.05\) & \(24.99 11.58\) \\  & KASP & \(15.47 3.40\) & \(\) & \(14.33 3.54\) & \(0.91 0.19\) & \(\) \\  & PM \(k\) & \(3.23 0.15\) & \(0.49 0.02\) & \(2.40 0.07\) & \(0.38 0.01\) & \(1.14 0.02\) \\  & PM \((k)\) & \(\) & \(0.36 0.01\) & \(\) & \(\) & \(\) \\   & \(k\) Eigs & \(\) & \(\) & \(\) & \(\) & \(\) \\  & \((k)\) Eigs & \(0.49 0.03\) & \(\) & \(0.32 0.02\) & \(0.30 0.01\) & \(\) \\  & KASP & \(0.33 0.03\) & \(0.42 0.04\) & \(0.30 0.03\) & \(\) & \(0.13 0.01\) \\  & PM \(k\) & \(0.55 0.03\) & \(\) & \(\) & \(\) & \(\) \\  & PM \((k)\) & \(0.51 0.04\) & \(\) & \(0.35 0.03\) & \(\) & \(\) \\   & \(k\) Eigs & \(\) & \(\) & \(\) & \(\) & \(0.27 0.02\) \\  & \((k)\) Eigs & \(0.68 0.01\) & \(\) & \(0.55 0.01\) & \(0.48 0.02\) & \(0.13 0.02\) \\   & KASP & \(0.48 0.02\) & \(0.60 0.03\) & \(0.46 0.03\) & \(0.61 0.03\) & \(\) \\   & PM \(k\) & \(\) & \(\) & \(\) & \(0.69 0.02\) & \(0.29 0.03\) \\   & PM \((k)\) & \(0.69 0.02\) & \(\) & \(\) & \(0.66 0.04\) & \(0.30 0.01\) \\  

Table 1: The performance of spectral clustering algorithms on real-world datasets. The PM \((k)\) algorithm corresponds to Algorithm 2. We perform \(10\) trials and report the average performance with one standard deviation of uncertainty. We observe that Algorithm 2 is consistently very fast when compared to the other algorithms while achieving comparable clustering accuracy.

   Dataset & \(n\) & \(k\) \\  MNIST & \(70000\) & \(10\) \\ Pen Digits & \(7494\) & \(10\) \\ Fashion & \(70000\) & \(10\) \\ HAR & \(10299\) & \(6\) \\ Letter & \(20000\) & \(26\) \\   

Table 2: The number of vertices (\(n\)) and clusters (\(k\)) in each of the real-world datasets.