# Patch Gradient Descent: Training Neural Networks on Very Large Images

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Traditional CNN models are trained and tested on relatively low resolution images (< 300 px), and cannot be directly used on large-scale images due to compute and memory constraints. We propose Patch Gradient Descent (PatchGD), an effective learning strategy that allows to train the existing CNN architectures on large-scale images in an end-to-end manner. PatchGD is based on the hypothesis that instead of performing gradient-based updates on an entire image at once, it should be possible to achieve a good solution by performing model updates on only small parts of the image at a time, ensuring that the majority of it is covered over the course of iterations. PatchGD thus extensively enjoys better memory and compute efficiency when training models on large scale images. PatchGD is thoroughly evaluated on two datasets - PANDA and UltraMNIST with ResNet50 and MobileNetV2 models under different memory constraints. Our initial evaluation reveals that PatchGD is much more stable and efficient than the standard gradient-descent method in handling large images, and especially when the compute memory is limited.

## 1 Introduction

In the realm of computer vision, Convolutional Neural Networks (CNNs) have established themselves as the cornerstone of advanced feature extraction, far surpassing traditional algorithms. Recent reviews by [1; 2; 3] encapsulate their evolution and dominance.

However, with the influx of high-dimensional data from sectors like microscopy [4; 5], medical imaging , and earth sciences [7; 8], the computational challenges for CNNs have surged. For example, high-content nanoscopy often necessitates the assimilation of multiscale data with information content relevant to the science present at scales ranging from a pixel to artifacts whose length-scales approach the image dimension \(-\) leading to issues in effective CNN application.

Most prevailing CNN models, fine-tuned on datasets such as ILSVRC and PASCAL VOC, which mainly comprise of low-resolution (\(<300\) pixels) images, encounter difficulties when extended to high-resolution images due to dramatic increase in intermediate activations. Common mitigative strategies--like downsampling or tiling--either compromise the feature fidelity or disrupt contextual continuity. Attention mechanisms, while providing semantic continuity, are often computationally prohibitive for high-res data due to their quadratic dependence on input token lengths.

Addressing this, we propose a robust CNN training paradigm tailored for high-dimensional data. The term "large" in our context is fluid, contingent on the computational memory overhead. For illustration, a \(10,000 10,000\) image might overextend a 48 GB GPU, but a \(512 512\) one is manageable on 12 GB--though the latter becomes challenging at a leaner 4 GB constraint. An example experimental demonstration on UltraMNIST digits  is presented in Figure 1. Hereinlies the significance of our Patch Gradient Descent (PatchGD), demonstrating resilience across two different budget constraints.

**Contributions.** To summarize, the contributions of this paper can be listed as follows.

* We present _Patch Gradient Descent (PatchGD)_, a novel strategy to train neural networks on very large images in an end-to-end manner. PatchGD is an adaptation of the conventional feedforward-backpropagation optimization framework.
* Due to its inherent ability to work with small fractions of a given image, PatchGD is scalable on small GPUs, where training the original full-scale images may not even be possible.
* PatchGD reinvents the existing CNN training pipeline in a very simplified manner and this makes it compatible with any existing CNN architecture or any conventional gradient-based optimization method used in deep learning. Moreover, its simple design allows it to benefit from the pre-training of the standard CNNs on low-resolution data.

## 2 Approach

### General description

_Patch Gradient Descent (PatchGD)_ is a novel CNN training strategy that can train networks with high-resolution images. An adaptation of the standard feedforward-backpropagation method, it is based on the hypothesis that, rather than performing gradient-based updates on an entire image at once, it is possible to achieve a good solution by performing model updates on only small parts of the image at a time, ensuring that the majority of it is covered over the course of iterations. However, even if only a portion of the image is used, the model is still trainable end-to-end with PatchGD.

In Figure 2, the PatchGD approach is presented schematically. The central idea behind PatchGD is to construct the \(\) block, which is a deep latent representation of the entire input image. Although only a subset of the input is used to perform model updates, \(\) captures information about the entire image by combining information from different parts of the image acquired from the previous update steps. Figure 1(a) illustrates the use of the \(\) block, which is an encoding of an input image \(\) using a model parameterized by weights \(_{1}\). The input image is divided into patches of size \(m n\), and each patch is processed independently using \(_{1}\). The size of \(\) is always enforced to be \(m n s\), such that each patch in the input space corresponds to the respective \(1 1 s\) segment in the \(\) block.

The filling of \(\) is carried out in multiple steps, with each step involving the sampling of \(k\) patches along with their positions from \(\) and feeding them to the model as a batch for processing. The output from the model along with the corresponding positions are then used to fill the respective parts of \(\). After sampling all \(m n\) patches of \(\), the completely filled \(\) is obtained. This concept of \(\)-filling is utilized by PatchGD during both training and inference stages. To create an end-to-end CNN model, we incorporate a small subnetwork that consists of convolutional and fully-connected layers. This subnetwork processes the information contained in \(\) and converts it into a \(c\)-dimensional probability vector, which is essential for the classification task. It is worth noting that the computational cost of adding this small subnetwork is minimal. The Figure 1(b) illustrates the pipelines for both model training and inference stages. During training, the model components \(_{1}\) and \(_{2}\) are updated. We compute the respective encodings based on a fraction of the patches sampled from the input image, using the latest state of \(_{1}\), and update the corresponding entries in the already filled \(\) using the

Figure 1: Performance comparison of standard CNN and PatchGD (ours) for the task of classification of UltraMNIST digits of size \(512 512\) pixels using ResNet50 model. Two different computational memory budgets of 16 GB and 4GB are used, and it is demonstrated that PatchGD is relatively stable for the chosen image size, even for very low memory compute.

model output. Subsequently, we use the partially updated \(\) to calculate the loss function value and update the model parameters using backpropagation. For more details, see the mathematical formulation presented in Appendix B.

## 3 Experiments

We showcase the effectiveness of PatchGD through numerical experiments on two benchmark datasets with large images and multiple scales, and additional experiments on generative modelling.

### Results

**UltraMNIST classification.** The performance of PatchGD for UltraMNIST has already been shown in Figure 1. PatchGD improves over the standard gradient descent method (abbreviated as GD) by large margins. The performance difference is even higher when we have a low memory constraint.

   Method & Resolution & Patch Size & Batch Size & Mem. (GB) & Throughput (imgs/sec) & Accuracy \% & QWK \\  Baseline & 512 & - & 27 & 16 & 618.05 & 44.4 & 0.558 \\ PatchGD & 512 & 128 & 86 & 16 & 521.42 & 44.9 & 0.576 \\ PatchGD & 512 & 64 & 200 & 16 & 341.87 & 52.1 & 0.616 \\ Baseline & 2048 & - & 1 & 16 & 39.04 & 34.8 & 0.452 \\ PatchGD & 2048 & 128 & 14 & 16 & 32.52 & 53.9 & 0.627 \\ Baseline & 2048 & - & 6 & 48 & 39.04 & 49.4 & 0.625 \\ PatchGD & 2048 & 128 & 56 & 48 & 32.52 & 56.2 & 0.667 \\ Baseline & 4096 & - & 1 & 48 & 9.23 & 50.0 & 0.611 \\ PatchGD & 4096 & 256 & 26 & 48 & 9.62 & 59.7 & 0.730 \\   

Table 1: Performance scores obtained using Resnet50 on PANDA dataset for Gradient Descent (GD) and Patch Gradient Descent (PatchGD).

Figure 2: Schematic representations of the pipelines demonstrating working of different components of the PatchGD process.

At 4 GB, while GD seems unstable with a performance dip of more than 11% compared to the 16 GB case, our PatchGD approach seems to be significantly more stable. The underlying reason for this gain can partly be attributed to the fact that since PatchGD facilitates operating with partial images, the activations are small and more images per batch are permitted.

**Prostate Cancer Classification (PANDA).** Table 1 presents the results obtained on PANDA dataset for three different image resolutions. For all experiments, we maximize the number of images used per batch while also ensuring that the memory constraint is not violated. For images of \(512 512\), we see that PatchGD, with patches of size \(128 128\), delivers approximately the same performance score as GD (for both accuracy as well as QWK) at 16 GB memory limit. However reducing the patch size and thus increasing the batch size, we observe a very sharp gain in the scores of PatchGD. For a similar memory constraint, when images of size \(2048 2048\) pixels are used, the performance of GD drops by approximately 10% while our PatchGD shows a boost of 9% in accuracy.

Two factors contribute to the performance gap between GD and PatchGD. Firstly, GD faces a bottleneck with batch size due to increased activation size in higher-resolution images, allowing only 1 image per batch. Gradient accumulation across batches and hierarchical training were explored but did not improve performance significantly. Increasing the memory limit helped mitigate the issue of using only 1 image per batch. Secondly, the optimized receptive field of ResNet50 is not well-suited for higher-resolution images, resulting in suboptimal performance. PatchGD demonstrates superior accuracy and QWK compared to GD on the PANDA dataset when handling large images end-to-end. In terms of inference latency, PatchGD performs comparably to GD. The smaller activations in PatchGD offset the slowness caused by patchwise image processing. PatchGD shows potential for real-time inference in applications requiring large image handling.

**Comparison with existing methods.** We further present a comparison of PatchGD with the existing methods designed for handling large images, and the results are presented in Table 2 of the appendices. Note that almost all works that exist on handling large images are not designed to work with memory constraints, and if put in such applications, these lead to unstable performance scores. For example, although the vision transformer backbones of HIPT are pretrained on large medical datasets, the performance of the model in the memory-constrained setting is lowest among the 4 methods presented in the table. For HIPT, all the layers of the vision transformer backbones are trainable and a batch size of only 5 fits in the memory. The original HIPT model is trained with large batch sizes over a set of GPUs, however, in our memory-constrained set up, it is not possible. The performance of ABNN and C2C is relatively better, however, they are still significantly lower than the PatchGD training of a simple architecture. C2C employs attention modules in the head of the network, and we believe with such additions, the performance of PatchGD could be boosted even further. Nevertheless, we see from the presented results that for memory-constrained settings, PatchGD performs significantly better than any other existing method when it comes to handling large images.

For HIPT, We conducted an additional experiment with gradient accumulation over 12 steps, referred as HIPT-L in Table 2. This led to an equivalent batch size of 60. Although the convergence was slow, the performance of the model boosted from 34.8 to 49.3. This clearly demonstrates that transformers with gradient accumulation could work well even at low batch sizes. Nevertheless, we still see a significant performance gap of more than 10% between HIPT and our approach. Moreover, transformers are known to be data hungry and one important thing to note here is that the pre-trained HIPT model we are using in this paper is already heavily trained on a very large medical dataset comprising training images from a variety of medical datasets. On the contrary, our model is only pre-trained on standard ImageNet and no additional pre-training is done. This clearly makes our approach stand out when compared to HIPT in the sense that it is applicable for low memory as well as relatively low training data regimes as well.

## 4 Conclusions

In this paper, we introduced Patch Gradient Descent (PatchGD), a novel CNN training strategy that effectively handles large images even with limited GPU memory. PatchGD updates the model using partial image fractions, ensuring comprehensive context coverage over multiple steps. Through various experiments, we demonstrated the superior performance of PatchGD compared to standard gradient descent, both in handling large images and operating under low memory conditions. The presented method and experimental evidence highlight the significance of PatchGD in enabling existing CNN models to effectively process large images without compute memory limitations.