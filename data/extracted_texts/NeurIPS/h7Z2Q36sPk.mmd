# Appendix

SynRS3D: A Synthetic Dataset for Global 3D Semantic Understanding from Monocular Remote Sensing Imagery

Jian Song\({}^{1,2}\), Hongruixuan Chen\({}^{1}\), Weihao Xuan\({}^{1,2}\), Junshi Xia\({}^{2}\), Naoto Yokoya\({}^{1,2}\)

\({}^{1}\)The University of Tokyo, Tokyo, Japan

\({}^{2}\)RIKEN AIP, Tokyo, Japan

song@ms.k.u-tokyo.ac.jp

\({}^{}\)[https://JTRNEO.github.io/SynRS3D](https://JTRNEO.github.io/SynRS3D)

###### Abstract

Global semantic 3D understanding from single-view high-resolution remote sensing (RS) imagery is crucial for Earth observation (EO). However, this task faces significant challenges due to the high costs of annotations and data collection, as well as geographically restricted data availability. To address these challenges, synthetic data offer a promising solution by being unrestricted and automatically annotatable, thus enabling the provision of large and diverse datasets. We develop a specialized synthetic data generation pipeline for EO and introduce _SynRS3D_, the largest synthetic RS dataset. SynRS3D comprises 69,667 high-resolution optical images that cover six different city styles worldwide and feature eight land cover types, precise height information, and building change masks. To further enhance its utility, we develop a novel multi-task unsupervised domain adaptation (UDA) method, _RS3Dda_, coupled with our synthetic dataset, which facilitates the RS-specific transition from synthetic to real scenarios for land cover mapping and height estimation tasks, ultimately enabling global monocular 3D semantic understanding based on synthetic data. Extensive experiments on various real-world datasets demonstrate the adaptability and effectiveness of our synthetic dataset and the proposed RS3Dda method. SynRS3D and related codes are available at [https://github.com/JTRNEO/SynRS3D](https://github.com/JTRNEO/SynRS3D).

## 1 Introduction

3D reconstruction is a fundamental task in computer vision, which focuses on creating three-dimensional representations from two-dimensional images. It plays a crucial role in applications such as virtual reality, autonomous driving, and robotics. In the context of Earth observation (EO), reconstructing semantic 3D information from single-view remote sensing (RS) images is also vital for applications like environmental monitoring, urban planning, and disaster response . Unlike point cloud-based 3D reconstruction, which relies on LiDAR or stereo cameras, monocular semantic 3D reconstruction is more scalable and requires less expensive equipment, making it suitable for global applications . This task combines land cover mapping, which is closely related to semantic segmentation in computer vision, and height estimation. However, acquiring RS annotations is costly and time-consuming, especially for high-resolution height data obtained through satellite LiDAR (e.g. GEDI, ICESat-2)  or stereo matching . Furthermore, high-resolution land cover mapping datasets often lack the corresponding height data. Moreover, the availability of RS data is geographically skewed, with developed regions having abundant data and developing regions lacking high-resolution datasets. Schmitt et al.  reviewed more than 380 RS datasets, revealing that few datasets come from Oceania, South America, Africa and Asia, while most originate from Europe and North America. This geographic limitation in RS datasets raises a crucial question: Can findings from numerous research papers be applied to these underrepresented regions?

The aforementioned challenges can be effectively addressed using synthetic data. Current 3D modeling technology has the potential to create various landscape features with accompanying land cover semantic labels and height values. Therefore, we present _SynRS3D_, a high-quality, high-resolution synthetic RS 3D dataset. SynRS3D includes 69,667 images with various ground sampling distances (GSD) ranging from 0.05 to 1 meter, and annotations for height estimation and land cover mapping in various scenes. However, models trained solely on synthetic data tend to overfit to these datasets, resulting in significantly reduced performance when applied to real-world environments due to the large domain gap. Existing synthetic datasets  often exhibit this significant performance gap compared to models trained on real data.

To bridge this gap, we introduce _RS3DAA_, a novel baseline aimed at advancing research on SynRS3D and setting a benchmark for multi-task unsupervised domain adaptation (UDA) from synthetic to real scenarios of RS. This approach utilizes a self-training framework and incorporates a land cover branch to enhance the quality of pseudo-labels of the height estimation branch, thus stabilizing RS3DAA training and boosting the accuracy of both branches. For the height estimation task, our model even outperforms models trained on real-world data in the challenging areas. Figure 1 shows the results of the 3D semantic reconstruction globally using models trained with our RS3DAA method on the SynRS3D dataset. Furthermore, we include disaster mapping results for earthquake and wildfire scenarios using our model in Appendix A.9, showcasing its efficacy in real-world disaster response applications.

The major contributions of this work can be summarized in three aspects:

* We propose **SynRS3D**, the largest RS synthetic dataset with comprehensive annotations and geographic diversity for remote sensing tasks.
* We design **RS3DAA**, a robust and effective multi-task UDA algorithm for land cover mapping and height estimation.
* Based on SynRS3D, we benchmark various remote sensing scenarios for land cover mapping and height estimation tasks.

We hope that our dataset and the proposed method advance the progress of synthetic learning in remote sensing applications.

Figure 1: 3D visualization outcomes from real-world monocular RS images, using the model trained on the SynRS3D dataset with the proposed RS3DAA method. The top colorbar represents the legend for the land cover map (row 2), while the right colorbar indicates the height range (row 1). “SA” indicates South America and “NA” indicates North America.

Related Work

High-Resolution Earth Observation.High-resolution RS technology enables us to capture images with a GSD of less than 1 meter, significantly enhancing our understanding of Earth's surface details. Deep learning has become a powerful tool for analyzing these images. High-resolution imagery allows for precise land cover mapping [55; 121; 34; 97; 60; 102]. Concurrently, height estimation research [54; 26; 22; 49] focuses on determining accurate surface elevations. Some studies [21; 87; 120; 44] combine these tasks, training models for both land cover mapping and height estimation simultaneously. Most 3D reconstruction studies use real-world data, concentrating on buildings and often neglecting valuable classes like trees [50; 52; 53; 65]. Multi-view RS for 3D reconstruction [81; 20; 38; 19; 113; 47] is expensive and geographically limited. Benchmark datasets [16; 96; 101; 106; 45; 73; 13; 80] have been constructed for model training; however, acquiring high-resolution RS data is costly and time-consuming due to manual labeling and sophisticated equipment requirements. This limits the number of samples available to train robust models. Additionally, real datasets often lack geographic diversity, which can hinder model generalization to new, unseen areas.

Synthetic Remote Sensing Dataset.Modern methods to create synthetic data utilize deep learning generative models, such as diffusion models  and generative adversarial networks (GANs) , in conjunction with 3D modeling techniques. Generative models often struggle to produce data outside their training distribution and lack the precise control needed for RS tasks. In contrast, 3D modeling approaches in computer graphics, which take advantage of game engines or 3D software [8; 66; 79; 77; 28; 100], have shown more success. However, creating synthetic data for RS is inherently more complex. A single 1024x1024 RS image demands hundreds of buildings, thousands of trees, and various topological features such as rivers and roads, unlike street views that require fewer assets. Recent studies have used 3D software to synthesize RS data with automatic annotations [7; 124; 43; 84; 105; 75; 76; 86; 104]. Despite their utility, these datasets often suffer from limited geographic diversity [7; 124; 105; 75; 76; 104], semantic categories [7; 124; 43; 84; 105; 75; 76; 104], and comprehensive semantic and height information [7; 124; 43; 84; 105; 86], which impacts their effectiveness in training robust, globally applicable models.

Unsupervised Domain Adaptation (UDA).UDA aims to adapt a model trained on labeled data from a source domain to perform well in an unlabeled target domain, reducing the constraints and costs of data annotation. For semantic segmentation, most of the work focuses on adversarial learning [92; 93; 33; 32; 23; 98; 95; 91; 63] and self-training [122; 56; 115; 123; 67; 82; 110; 94; 35; 36; 37; 48; 24]. Unlike semantic segmentation tasks, height estimation is a regression task, aligning closely with monocular depth estimation. For UDA in monocular depth estimation, methods often focus on image translation to reduce domain gaps [4; 117; 119; 12], and some use self-training with pseudo-labels [61; 107; 111]. In RS, most research [40; 62; 72; 89; 90; 118] focuses on applying developed techniques from computer vision to adapt models trained on real-world data to different real-world environments (real-to-real). However, only a small number of studies [58; 103] investigate the challenges of adapting synthetic data to real-world environments (synthetic-to-real). To the best of our knowledge, RS3DAda is the first work to explore UDA algorithms specifically designed for synthetic-to-real domain adaptation for multi-task dense prediction in RS.

## 3 The SynRS3D Acquisition Protocol and Statistical Analysis

Although simulating RS scenes poses significant challenges due to the need for numerous assets, we mitigate this issue using procedural modeling techniques [69; 68; 42]. Instead of manually modeling each element, we incorporate rules derived from real-world knowledge, formalized into scripts. The generation system is controlled through hyperparameters like city style, asset ratio, and texture style, allowing us to create a diverse high-quality synthetic dataset. Table 1 compares existing synthetic RS datasets with SynRS3D, highlighting our dataset's advantages in diversity, functionality, and scale.

### Statistics for SynRS3D

The left section of Figure 2 shows RGB images, land cover labels, and height maps for six styles in SynRS3D, representing diverse real-world environments. The bottom left section compares the height distribution of SynRS3D with two leading synthetic datasets, SMARS  and GTAH ,as well as 11 real-world height datasets. SynRS3D's height distribution closely matches real-world data, while SMARS and GTAH show limitations. Specifically, SMARS, which mimics Paris and Venice, has a narrow height range. GTAH, based on the GTAV game, which mimics Los Angeles, shows a wider height range, but with a larger mean and variance, making it less representative of other cities. SynRS3D was constructed using the following prior knowledge : backward regions (low buildings) cover about 12% of the world's areas, emerging regions (mid buildings) cover about 12% of the world's areas, and emerging regions (mid buildings) cover about 12% of the world's areas.

   RS Synthetic Datasets &  &  &  &  & \# Images \\   & City-Replica & Style-Extended & GSD (m) & Image Size & Perspective & Layout & Geometry & Texture & \\  ACD  & ✓ & ✗ & \(\) & \(800 600\) & Nad., Od. & M & M & CD & \( 1K\) \\ GTAV-S/SD  & ✓ & ✗ & \(1\) & \(500 500\) & Nad. & G & G & G & BS & \( 0.12K\) \\ Synthall.  & ✓ & ✓ & \(0.3\) & \(572 572\) & Nad. & R & M & M+R & BS & \( 1K\) \\ RawFplanes  & ✓ & ✗ & \(0.3\) & \(532 512\) & Nad. & OLR & R & M+R & OD & \( 6K\) \\ SynrGiles  & ✓ & ✗ & \(0.1,0.3,1.0\) & \(1024 1024\) & Nad. & R & M & M+R & DE & \( 8K\) \\ GTAH  & ✓ & ✗ & \(\) & \(1920 1080\) & Nad., Od. & G & G & G & BE & \( 28.6K\) \\ Synthworld  & ✓ & ✓ & \(0.3,-1.0\) & Various & Nad., Od. & P & P+M & P & LC, RCD & \( 40K\) \\ SMARS  & ✓ & ✗ & \(0.3,0.5\) & Various & Nad. & R & M & M+R & LC, HE, RCD & \(4\) \\ 
**SynRS3D (Ours)** & ✗ & ✗ & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Comparisons of various RS synthetic datasets based on diversity, image capture details, asset origins, tasks, and the number of images. The diversity is categorized into City-Replica (datasets mimicking specific cities) and Style-Extended (covering a range of urban styles). Image capture attributes include GSD, resolution, and perspective (Nadir, Oblique). Asset origins are denoted as Manually-made (M), Game-origin (G), Procedurally-generated (P), and Real (R). Tasks cover Change Detection (CD), Building Segmentation (BS), Object Detection (OD), Disparity Estimation (DE), Height Estimation (HE), Land Cover (LC), and Building Change Detection (BCD).

Figure 2: Examples and statistics of SynRS3D. The colorbar corresponds to the land cover classification legend shown in Figure 1.

70%, and developed regions (tall buildings) cover about 18%. The bottom right section contrasts the land cover proportions in SynRS3D with those in SMARS and the real-world OEM dataset , which is currently the largest and most geographically diverse dataset. SynRS3D's land cover categories--including Bareland, Rangeland, Developed Space, Roads, Trees, Water, Agricultural Land, and Buildings--match well with the OEM dataset. In contrast, SMARS has only five categories, limiting its effectiveness for comprehensive land cover mapping.

### Generation Workflow of SynRS3D

The generation process of SynRS3D employs tools such as Blender , Python, GPT-4 , and Stable Diffusion , as illustrated in Figure 3. It begins with Python scripts that translate synthetic scene rules into parameter-controlled instructions for tasks like terrain generation, sensor placement, and asset placement. The geometry of the buildings and trees is created both procedurally and manually. Stable Diffusion generates textures based on detailed text prompts from GPT-4, ensuring high-quality and diverse textures. Blender's compositor node and Python scripts then generate accurate land cover labels and height maps. The details of the height map generation are detailed in the height calculation section of Figure 3. Our dataset's height maps are produced within Blender using simple geometric algorithms, resulting in a completely accurate normalized Digital Surface Model (nDSM). An nDSM represents the height of objects above the ground, providing clear information about buildings and vegetation. In contrast, the height maps for the comparative datasets GTAH and SMARS are Digital Surface Models (DSM), which include the height of ground and objects. Converting DSM to nDSM requires additional processing using the dsm2dtm 1 algorithm, which introduces noises. Optical images are produced using rendering scripts. To generate building change detection masks, we follow a structured process. Initially, buildings are randomly removed from scenes, and textures are reapplied to create pre-event images. Subsequently, land cover labels are subtracted to produce the change detection masks. After the initial generation of the dataset, images with anomalous height distributions are filtered out. This step ensures that the final version of SynRS3D closely aligns with real-world height distributions. The specific filtering algorithm used and examples of building change detection masks can be found in the Appendix A.1.

## 4 Multi-Task Unsupervised Domain Adaptation for RS Tasks (RS3DAda)

SynRS3D features low costs, high diversity, and large volume. However, there is a clear domain gap between synthetic data and real-world environments, which limits the use of SynRS3D. This limitation is particularly evident in RS, where synthetic-to-real UDA algorithms are lacking. To bridge this gap, we developed RS3DAda, which leverages land cover labels and height values to complement each other. In addition, it harnesses the potential of unlabeled real-world data, establishing the first benchmark for synthetic-to-real RS-specific multi-task UDA.

Figure 3: Generation workflow of SynRS3D.

**Basic Framework.** In this work, we adopt self-training  as our basic UDA technique due to its superior stability and adaptability across both land cover mapping and height estimation tasks. Additionally, the teacher-student framework  is incorporated into the method to enhance performance and generalizability by stabilizing training with the exponential moving average (EMA) of model weights. The synthetic dataset image set \(_{s}\) serves as the source domain, with access to corresponding land cover labels \(_{LC}^{s}\) and height maps \(_{H}^{s}\). The real dataset image set \(_{t}\) serves as the target domain without any access to the labels.

**Source Domain Training.** In this stage, shown in the left part of Figure 4, source domain images \(_{s}\) undergo statistical image translation using simple Fourier Domain Adaptation (FDA) , Histogram Matching (HM) and Pixel Distribution Matching (PDA), which follow the conclusion of Abramov et al. . This process aligns the styles of the source images with the target domain, resulting in translated images \(_{s}^{}\). These translated images from the source domain are then fed into the student network, producing predicted land cover labels \(_{LC}^{s}\) and height \(_{H}^{s}\). The supervised loss for the source domain is defined as:

\[_{source}=_{i=1}^{N}(_{ CE}(_{LC}^{s(i)},_{LC}^{s(i)})+_{SmoothL1}(_{H }^{s(i)},_{H}^{s(i)})), \]

where \(_{CE}\) is the cross-entropy loss, \(_{SmoothL1}\) is the Smooth L1 loss , and \(N\) is the total number of samples.

**Land Cover Pseudo-Label Generation.** To generate high-quality pseudo-labels for land cover mapping, as illustrated in the right section of Figure 4, target domain images \(_{t}\) are strongly augmented denoted as \(_{t}^{}\). We adopt color jitter, Gaussian blur, and ClassMix  as the strong augmentations. The teacher model then predicts the land cover pseudo-labels \(_{LC}^{t}\). After that, we use a threshold \(\) to generate a confidence map \(_{LC}=(_{LC}^{t}>)\), where \(\) is the indicator function.

**Height Pseudo-Label Generation.** Height pseudo-labels are generated by leveraging prior knowledge that only trees and buildings have height values. This is the first attempt to use ground information to correct height pseudo-labels, inspired by the empirical observations that the network achieves superior accuracy on the ground class early in the training stage. We refine height pseudo-labels using a ground mask \(\) generated from the land cover mapping branch as \(=(_{LC}^{t}=)\), and refine the height pseudo-labels by \(_{H}^{t,refined}=_{H}^{t,ori}(1-)\). We also create a height consistency map \(_{H}\):

\[_{H}=((_{H}^{ t,ori}}{_{H}^{t,aug}},_{H}^{t,aug}}{_{H}^{t, ori}})), \]

indicating that we consider predictions reliable if they remain stable under perturbations, where \(\) is the threshold.

Figure 4: Overview of the proposed RS3DAda method. \(\) denotes statistical image translation, \(\) represents strong augmentation. For Online Student Model Training, dotted line: target image, solid line: source image. For Ground-Guided Pesudo Label Generation, dotted line: original target image, dotted line: strong augmented target image.

**Target Domain Training.** The target domain training loss is:

\[_{target}=_{i=1}^{N}(_{CE}(_{LC}^ {t(i)},_{LC}^{t(i)})_{LC}^{(i)}+_{SmoothL1}( _{H}^{t(i)},_{H}^{t,refined(i)})_{H}^{(i)} ). \]

**Feature Constraint.** To alleviate overfitting, we adopt a frozen DINOv2  encoder to supervise the student encoder's updates, inspired by Yang et al. . Let \(\) denote the features of the student encoder and \(^{}\) denote the features of the frozen DINOv2 encoder. We utilize cosine similarity to constrain the feature updates with the loss defined as:

\[_{feat}=1-^{}}{ \|\|\|^{}\|}&^{ }}{\|\|\|^{}\|}<\\ 0&, \]

where \(\) is the threshold.

**Overall Loss.** The overall loss of the model is defined as the sum of the source domain training loss, target domain training loss, and feature alignment loss, each weighted by their respective coefficients. Formally, the overall loss is given by:

\[_{}=_{}+_{}_{}+_{}_{}, \]

where \(_{}\) and \(_{}\) are weighting coefficients that control the contribution of the target and feature alignment loss terms, respectively.

## 5 Experiments

**Evaluation Datasets & Experimental Setting.** We evaluate our synthetic dataset, SynRS3D, from various aspects. Table 2 (a) shows the real-world height estimation datasets, while Table 2 (b) lists the real-world land cover mapping datasets. In Section 5.1, we compare SynRS3D with other synthetic datasets under the source-only setting, a term commonly used in UDA to describe models trained solely on the source domain and directly applied to the target domain without adaptation. This setup highlights SynRS3D's smaller domain gap and its potential for direct usage in real-world scenarios. In Section 5.2, we investigate the advantages of SynRS3D for augmenting real-world data through fine-tuning and joint training. Specifically, in Section 5.3, to evaluate the effectiveness of RS3DAda, we divide the 11 height estimation datasets into two target domains: _Target Domain 1_ includes 6 widely-used public datasets, while _Target Domain 2_ contains 5 more challenging datasets to assess and contrast the generalization ability of SynRS3D with real datasets.

**Evaluation Metrics.** We employ Intersection over Union (IoU) and Mean Intersection over Union (mIoU) as the metrics to evaluate the model's performance for land cover mapping tasks. For height estimation, we use Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and accuracy metrics  denoted as \(\), along with our custom metric \(F_{1}^{HE}\). Detailed metric definitions are in Appendix A.3.

**Implementation Details.** Unless specifically detailed, all experiments utilize the pre-trained DINOv2  implemented with ViT-L  as the encoder, with DPT  serving as both the land cover mapping and height estimation decoders. The hyperparameters for the various experiments are provided in Appendix A.4.

### Source-only Scenarios

**Land Cover Mapping.** We demonstrate the source-only capability of models trained on SynRS3D in the land cover mapping task. Table 3 compares SynRS3D with existing synthetic datasets, SMARS  and SyntheWorld . Due to category inconsistencies, we present IoU and mIoU for shared categories including trees, buildings, and ground on the JAX , OMA , Vaihingen , and Potsdam  datasets. The model trained on SynRS3D achieves the best results across these four real datasets using only land cover labels, demonstrating the extraordinary compatibility of SynRS3D.

### Combining SynRS3D with Real Data Scenarios

An important use of synthetic data is to augment real-world data. To demonstrate this capability of SynRS3D, we conducted two experiments. First, we trained models on SynRS3D and fine-tuned them on real data. Second, we combined SynRS3D with real data for joint training. We experimented with two different backbones: DINOv2 +DPT  and DeepLabV2 +ResNet101 . Figure 6 (a) showcases SynRS3D's performance in the height estimation task on three city datasets: JAX ,

Table 2: Datasets setup for the experiments on height estimation and land cover mapping. (a) **Height Estimation Datasets**: We detail 11 height estimation datasets categorized into two target domains. The first six datasets for training the model with real-world data are sourced from Europe and the United States as shown in Section 5.3. The remaining five datasets covering more challenging areas are characterized by: notable height mean@standard deviation, non-RGB channels, and varied regions outside of the US and EU, used for evaluation in Section 5.3. (b) **Land Cover Mapping Datasets**: We evaluate our method on five commonly used datasets covering diverse environments.

Figure 5: Source-only height estimation comparison of SynRS3D and other synthetic datasets showing in different metrics.

Table 3: Source-only land cover mapping performance on various real-world datasets.

Nagoya , and Tokyo . The results indicate that both approaches yield significant improvements when real data is scarce, with benefits diminishing as more real data is added. Additionally, the stronger the backbone, the smaller the improvement provided by SynRS3D, and vice versa. Figure 6 (b) illustrates SynRS3D's augmentation capability in the land cover mapping task on OEM  dataset, showing similar conclusions to the height estimation task.

### Transfer SynRS3D to Real-World Scenarios

**Heigh Estimation Branch.** Table 4 shows the height estimation results for our RS3DAda model. "Whole" refers to the entire image, and "High" focuses on targets above 3 meters, such as trees and buildings. Using DINOv2  and DPT  models, RS3DAda reduces the average MAE by 0.409 meters over the source-only approach across six datasets in _Target Domain 1_, though it is still exceeded by the models trained directly on these datasets. In _Target Domain 2_, RS3DAda outperforms models trained on real-world data, indicating its strong generalization under challenging scenarios, featuring diverse geographic regions and complex terrain characteristics. Figure 7 aligns with these results, showing RS3DAda's improvements on each dataset in _Target Domain 1_ and _Target Domain 2_. This demonstrates SynRS3D's potential and the effectiveness of RS3DAda. However, the gap in _Target Domain 1_ highlights the ongoing need to bridge the synthetic-to-real data gap, providing a benchmark for future UDA algorithm development in height estimation tasks.

**Land Cover Mapping Branch.** Table 5 presents the results of the RS3DAda model for the land cover mapping branch, evaluated using the OEM dataset. As shown, the RS3DAda method surpasses DAFormer by \(1.94\) in mIoU, indicating that the height branch positively impacts the land cover mapping performance. However, there remains a gap of \(20.11\) in mIoU compared to the Oracle

   Model & MAE\(\) & RMSE\(\) & Accuracy Metrics (\(10.1\)) & \(\) & \(\) & \(\) \\  SynRS3D & 10.018 & 10.018 & 10.018 & 10.018 & 10.018 & 10.018 & 10.018 & 10.018 & 10.018 \\  SynRS3D & 12.943 & 12.943 & 12.943 & 12.943 & 12.943 & 12.943 & 12.943 & 12.943 & 12.943 & 12.943 & 12.943 \\   

Table 4: Results of RS3DAda height estimation branch using DINOv2  and DPT . The experimental results are divided as follows: “Whole” denotes the evaluation results for the entire image. “High” signifies the experimental results for image regions above 3 meters. T.D.1 and T.D.2 correspond to _Target Domain 1_ and _Target Domain 2_, respectively, as specified in Table 2. Avg. stands for the average value.

Figure 6: Performance evaluation of combining SynRS3D with real data in (a) height estimation and (b) land cover mapping. Height estimation is evaluated on various real datasets, and land cover mapping is evaluated on OEM dataset, showing IoU for building, tree, and mIoU. FT: fine-tuning on real data after pre-training on SynRS3D, JT: joint training with SynRS3D and real data.

Figure 7: Results of RS3DAda height estimation branch for each dataset.

model, suggesting significant room for improvement. Future research can build upon our method to make further advancements.

**Stabilizing Training on SynRS3D.** RS3Dada can regularize the training of synthetic data to prevent rapid overfitting at the beginning of the training, corresponding to the green section in Figure 8. Without RS3Dada, the model's evaluation results on the target domain fluctuate wildly during training in both height estimation and land cover mapping branches. This instability can lead to unreliable performance and poor generalization. RS3Dada ensures more consistent training, resulting in better model accuracy and stability.

**Comparison with Existing UDA.** We re-implemented AdaptSeg , DADA  and RS3Dada using DeepLabv2  and ResNet101  for fair comparison. Table 6 shows that RS3DAda outperformed both AdaptSeg and DADA in height estimation and land cover mapping tasks. Notably, when using weaker network architectures, the CNN encoder pre-trained on ImageNet fails to provide reliable RS image features and high accuracy for the ground category. As a result, the Feature Constraint and Ground-Guided Pseudo-Label Refinement in our RS3DAda cannot achieve their maximum effectiveness.

## 6 Conclusion and Discussion

In this work, we introduced SynRS3D, the largest synthetic remote sensing (RS) dataset, and RS3DAda, a multi-task unsupervised domain adaptation (UDA) method, designed to address the challenge of global 3D semantic reconstruction from single-view RS images. Our experiments on public datasets demonstrate the effectiveness of these tools in enhancing the use of synthetic data for RS research, setting a benchmark for 3D reconstruction from monocular RS images.

While SynRS3D offers a substantial contribution, there remains an appearance gap between synthetic and real-world data, potentially affecting real-world performance. Additionally, the dataset, though extensive, does not capture the full diversity of global cities, which could limit its generalizability. Future work will focus on reducing this gap and expanding the dataset's coverage to improve its robustness across different urban environments. By making these resources publicly available, we hope to stimulate further research and development in the field.

   Model & Supervision & Right Legislation (UDA) & Land cover Mapping (mid-1) \\  SourceOnly & RLL & 3.911 & 7.449 & 17.02 & 0.061 & 0.062 \\ Adaptive & L & - & - & - & 20.06 & 0.00 \\ DADA & RLL & 3.655 & 6.907 & 21.24 & 46.44 \\ RS3DAda & H+L & 3.275 & 6.708 & **22.28** & **47.28** \\   MinimumSelf-supervised} & RLL & 3.850 & 6.907 & 21.54 & 33.125 \\  \\   

Table 6: Comparison of RS3DAda with existing UDA methods AdaSeg and DADA. Supervision types: H for height maps, L for land cover labels. T.D.1 represents Target Domain 1, and T.D.2 represents Target Domain 2. V: Vaihingen ; P. Potsdam ; J. JAX ; O: OMA . All models are implemented with DeepLabv2 and ResNet101. The datasets used for Tranin-on-Real are T.D.1 and OEM respectively.

Figure 8: Performance of SynRS3D at the beginning of training for height estimation and land cover mapping branches, with and without the use of RS3DAda.

   Model & Bareland & Rangeland & Developed & Road & Tree & Water & Agriculture & Buildings & **mIoU** \\  Source-only & 8.69 & 37.95 & **22.54** & **49.05** & 60.16 & 46.64 & 35.40 & **65.19** & 40.70 \\ DAFomer  & 12.54 & 41.16 & 10.88 & 43.88 & **62.56** & **77.55** & 62.62 & 59.10 & 46.29 \\
**RS3DAda** & **19.92** & **47.61** & 18.41 & 44.06 &