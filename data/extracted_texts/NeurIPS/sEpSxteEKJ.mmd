# Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction

Manuel Brenner\({}^{1,2}\), Christoph Jurgen Hemmer\({}^{1,3,}\), Zahra Monfared\({}^{2}\), Daniel Durstewitz\({}^{1,2,3}\)

\({}^{1}\)Dept. of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty,

Heidelberg University, Germany

\({}^{2}\)Interdisciplinary Center for Scientific Computing (IWR), Heidelberg University, Germany

\({}^{3}\)Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany

These authors contributed equally to this work.

Corresponding authors: {manuel.brenner, christoph.hemmer, daniel.durstewitz}@zi-mannheim.de

###### Abstract

Dynamical systems (DS) theory is fundamental for many areas of science and engineering. It can provide deep insights into the behavior of systems evolving in time, as typically described by differential or recursive equations. A common approach to facilitate mathematical tractability and interpretability of DS models involves decomposing nonlinear DS into multiple linear DS separated by switching manifolds, i.e. piecewise linear (PWL) systems. PWL models are popular in engineering and a frequent choice in mathematics for analyzing the topological properties of DS. However, hand-crafting such models is tedious and only possible for very low-dimensional scenarios, while inferring them from data usually gives rise to unnecessarily complex representations with very many linear subregions. Here we introduce Almost-Linear Recurrent Neural Networks (AL-RNNs) which automatically and robustly produce most parsimonious PWL representations of DS from time series data, using as few PWL nonlinearities as possible. AL-RNNs can be efficiently trained with any SOTA algorithm for dynamical systems reconstruction (DSR), and naturally give rise to a symbolic encoding of the underlying DS that provably preserves important topological properties. We show that for the Lorenz and Rossler systems, AL-RNNs discover, in a purely data-driven way, the known topologically minimal PWL representations of the corresponding chaotic attractors. We further illustrate on two challenging empirical datasets that interpretable symbolic encodings of the dynamics can be achieved, tremendously facilitating mathematical and computational analysis of the underlying systems.

## 1 Introduction

Dynamical systems (DS) underlie many real-world phenomena of scientific and practical relevance. Complex chaotic DS are believed to govern market dynamics , the rhythms of the brain , climate systems , or ecosystems . A by now rapidly growing field in scientific ML is dynamical systems reconstruction (DSR), where the goal is to learn a DS model directly from data that constitutes a generative surrogate model of the data-generating DS. DSR increasingly relies on deep learning, especially in contexts where dynamics are too complex to be captured by simple equations or where the underlying processes are not fully understood.

One way of making DSR models mathematically accessible is piecewise linear (PWL) designs, popular among engineers for decades . In the mathematical theory of DS, PWL models also play a special role and simplify many types of analysis , such as the characterization ofbifurcations [27; 10; 42; 40; 77; 70]. This is because linear DS are well-understood and straightforward to analyze, while nonlinear DS lack an equally simple description [78; 94; 13]. RNNs based on PWL activation functions, like rectified-linear units (ReLUs), have been proposed recently for learning mathematically tractable DSR models. Such piecewise-linear RNNs (PLRNNs), combined with effective training techniques for controlling gradient flows [67; 39], achieve state-of-the-art (SOTA) performance across a wide range of DSR tasks, including challenging (high-dimensional, noisy, chaotic, partially observed...) empirical time series [25; 11; 39; 12]. However, while featuring a PWL design, the resulting constructions are often complex, with a large number of linear subregions required to capture the data properly, hence still impending effective analysis. On the other hand, a class of switching linear DS has been proposed to decompose nonlinear DS into linear regions combined with switching states that determine the transitions between these regions [1; 31; 28; 58; 56; 57; 2]. However, the underlying assumptions of these models and the complexity of the inference mechanisms these entail, often make their training challenging and impede their efficient application to DSR problems, especially when moving to real-world scenarios and higher-dimensional systems.

Here we propose almost-linear RNNs (AL-RNNs) which combine linear units and ReLUs, but can use as few of the latter as necessary to achieve a most parsimonious representation in terms of linear subregions. AL-RNNs are easy and effective to train by any SOTA algorithm for DSR. Through this, they are able to robustly identify topologically or geometrically minimal representations of well-known chaotic systems. Their structure translates naturally into a symbolic coding that preserves important topological properties. These features make AL-RNNs highly interpretable and mathematically tractable, enabling to harvest tools from symbolic dynamics [74; 55], including the representation of empirically observed DS via minimal topological and computational graphs.

## 2 Related Work

Dynamical systems reconstructionThe field of data-driven DSR has been rapidly expanding in recent years. On the one hand there are approaches based on function libraries for approximating unknown vector fields, which have become particularly popular in some areas like physics [53; 64]. Among these, Sparse Identification of Nonlinear Dynamics (SINDy) and its variants [14; 60; 45; 21; 66; 44] is probably the most popular. Since in these models sets of differential equations are directly formulated in terms of known, predefined function libraries, instead of using NN black-box approximators, they have some level of interpretability in the sense that they are human-readable and can easily be related to established mathematical building blocks in physical or biological theories [37; 83]. This does not necessarily make them mathematically tractable, however, since systems of nonlinear differential equations are in themselves usually hard to analyze (in fact, their behavior is much of the core topic of DS theory ). They also have other limitations, including a difficulty in capturing complex and noisy empirical data [11; 39], as they usually require considerable prior knowledge about the system's underlying structure (i.e., which terms to include in the function library). This somewhat limits their applicability for discovering novel phenomena. On the other hand, many recent powerful DSR methods rely on universal approximators, in particular the fact that sufficiently large RNNs can approximate any underlying DS [29; 47; 34]. Such methods may be grouped into several broad classes, including reservoir computers [76; 79; 80], neural ODEs/PDEs [20; 46; 4; 48], neural/ Koopman operators [15; 63; 75; 6; 73; 30; 104], and RNNs [99; 25; 101; 19; 11; 84; 39]. The latter are commonly trained by variants of backpropagation through time (BPTT, [101; 102; 11]), combined with specific control techniques  to remedy the exploding/ vanishing gradients problem [9; 67; 11; 39]. While DSR algorithms based on universal approximators achieve SOTA performance on DSR tasks, and often work particularly well on empirical time series [11; 39], they commonly deliver a complex model structure that is difficult to interpret and parse mathematically.

Nonlinear dynamics via linear DSThe idea of approaching nonlinear DS through our good grasp of linear DS has been around for quite a long time, reflected in important theoretical results like the Hartman-Grobman theorem 1 or Koopman operator theory [49; 15]. While linear DS are easy to analyze and well understood [78; 94], however, they cannot properly capture most real-world systems, as they cannot produce many important DS phenomena such as limit cycles, chaos, or multistability. This has motivated the modeling of complex dynamics in terms of compositionsof locally linear dynamics, as the next best alternative, i.e. piecewise-linear (PWL) maps or ODE systems [18; 41; 22]. PWL models have been popular in engineering and mathematics for several decades for these reasons, including earlier attempts for learning such models directly from data [93; 24]. Switching linear DS are one particular brand of PWL models with a long tradition in DS and control theory [23; 95; 1; 31; 28]. These systems model nonlinear dynamics through a set of linear (or affine) DS combined with a switching rule which decides which linear DS is currently active. Likewise, in mathematics PWL models served well for investigating generic properties of nonlinear systems, e.g. the tent map which is topologically conjugate to the logistic map . PWL models often lend themselves to particularly convenient symbolic representations [54; 3], based on which important topological properties of the underlying system, e.g. the nature and number of unstable periodic orbits embedded within a chaotic attractor, can be analyzed [69; 74; 98].

More recently, various modern approaches for inferring PWL models from data have been formulated. For instance, switching state space models combine hidden Markov models with linear DS, jointly inferring the state of a switching (random) variable with the linear DS parameters conditioned on these states . Various extensions of this basic setup like recurrent and hierarchical switching linear DS and fully Bayesian inference methods have been advanced in recent years [92; 58; 56]. However, inference in these models is often complex and not necessarily optimized for DSR, limiting their applicability to mainly low-dimensional scenarios with comparatively simple dynamics. Most of these models are also discontinuous in their dynamics across switches, while most commonly we would require the state variables to evolve continuously across the whole of state space. Piecewise-linear RNNs (PLRNNs), and, relatedly, threshold-linear networks [105; 109; 33], on the other hand, are based on familiar ReLUs and hence change continuously across their switching manifolds [25; 51; 88]. They also have some biological justification [33; 25]. Commonly they are trained by variants of BPTT backed up by specific control-theoretic approaches like sparse  or generalized  teacher forcing which make them SOTA on many DSR tasks. Different PLRNN architectures have been proposed to enhance expressivity or reduce the dimensionality of trained models [11; 39]. Yet, while these advances may yield comparatively low-dimensional state spaces, the number of different linear subregions that need to be allocated usually remains very high, hampering efficient mathematical analysis.

## 3 Methodological and Theoretical Prerequisites

### AL-RNN Model

Consider a piecewise linear recurrent neural network (PLRNN, ):

\[_{t}=F_{}(_{t-1})=_{t-1}+(_ {t-1})+,\] (1)

where diagonal \(^{M M}\) contains linear self-connections, \(^{M M}\) are nonlinear connections between units, \(^{M}\) is a bias term, and \(()=[0,]\) is an element-wise ReLU nonlinearity. To expose the piecewise linear structure of this model more clearly, by noting that the slope of the ReLU is either \(0\) or \(1\) depending on the sign of \(z_{m,t}\), one can reformulate this as

\[_{t}=(+_{(t-1)})_{t-1}+=:_{ (t-1)}\,_{t-1}+,\] (2)

where \(_{(t)}:=diag(_{(t)})\) is a diagonal matrix and \(_{(t)}=(d_{1},d_{2},,d_{M})\) an indicator vector with \(d_{m}(z_{m,t})=1\) whenever \(z_{m,t}>0\) and zero otherwise . For the \(2^{M}\) different configurations of \(_{(t)}\), \(D_{^{k}}\), \(k\{1,2,,2^{M}\}\), the phase space of system eq. 2 is divided into \(2^{M}\) subregions with linear dynamics

\[_{t+1}\,=\,_{^{k}}\,_{t}+\,,_{ ^{k}}:=+_{^{k}}.\] (3)

Empirically, \(M\) often needs to be quite large (at least on the order of the number of observations) for achieving good reconstructions of observed DS. Since the number of subregions grows as \(2^{M}\), analyzing inferred models in terms of the subregions can thus become very challenging. We therefore introduce a novel variant of the PLRNN in which only a subset of \(P<<M\) units are equipped with a ReLU nonlinearity, yielding

\[_{t}=_{t-1}+^{*}(_{t-1})+\] (4)where

\[^{*}(_{t})=[z_{1,t},,z_{M-P,t},(0,z_{M-P+1,t}),, (0,z_{M,t})]^{T}.\] (5)

In this formulation, we thus only have \(2^{P}\) different linear subregions, while still accommodating a sufficiently large number of latent states for capturing unobserved dimensions in the data and disentangling trajectories sufficiently .2

The model is trained on the \(N\)-dimensional observations \(\{_{t}\},t=1 T,_{t}^{N}\), by a variant of sparse teacher forcing called identity teacher forcing . In identity teacher forcing, the first \(N\) latent states ('readout neurons') are replaced by the \(N\)-dimensional observations every \(\) time steps, where \(\) is chosen such as to optimally control trajectory and gradient flows, avoiding exploding gradients while providing the model sufficient opportunity to unroll into the future to capture the underlying DS' long-term behavior (see  for details); see Appx. A.2 for details on training. We emphasize that sparse teacher forcing is _only used for training_ the model, and is turned off at test time where the model generates new trajectories completely independent from the data.

### Theoretical Background: Symbolic Dynamics and Symbolic Coding of AL-RNN

The mathematical field of symbolic dynamics formulates conditions under which a DS has a unique symbolic representation and discusses how to harvest this symbolic representation to prove certain properties of the underlying system, which otherwise may be more difficult to address . In fact, symbolic dynamics has led to many powerful insights and formal results in DS theory, e.g. about the properties of chaos or type and number of periodic orbits . An appealing feature of symbolic dynamics for the field of ML/AI is that it links concepts in DS theory to computational concepts like finite state automata or formal languages, as well as graph theory . It can thus facilitate the computational interpretation of natural or trained dynamical systems, like RNNs.

Assume we have an alphabet of \(n\) symbols \(=\{0,,n-1\}\), from which we form infinite sequences (bidirectionally or only in forward-time) \(= a_{-2}a_{-1}.a_{0}a_{1}a_{2}\) with \(a_{k}\), and the dot separating past from future (i.e., indices \(k<0\) indicate backward time, and \(k 0\) present and forward time). Then the space of all possible sequences, together with the so-called (left) shift operator given by

\[()=( a_{-2}a_{-1}.a_{0}a_{1}a_{2})= a_{-1} a_{0}.a_{1}a_{2}a_{3}\] (6)

defines the full shift space \(^{}\). We denote by \(^{k}=\) the \(k\)-times iteration of the shift. Now consider a DS \((S,)\) consisting of a metric (state) space \(S\) and a recursive (flow) map \(\). The flow map \(_{ t}()\) advances the system's current state \(\) by \( t\) and may be thought of as the solution operator of the underlying DS \(}=f()\). When training RNNs \(_{t}=F_{}(_{t-1})\) on time series of observations \(\{g(_{ t t})\},k=1 T\), from the underlying DS, where \(g\) is the observation function, we are trying to approximate this flow map. Assume the whole state space \(S\) can be partitioned into a finite set \(=\{U_{0} U_{n-1}\}\) of disjoint open sets \(U_{e}\), such that \(S=_{e=0}^{n-1}}\), i.e. \(S\) is covered by the union of the closures of these sets. We call this a _topological partition_ of \(S\).

The central idea now is to assign a unique symbol \(a_{e}\) to each set \(U_{e}\), with \(n=||=||\). As a trajectory \((t)\) of the underlying DS travels through the system's state space \(S\), observed at times

Figure 1: Illustration of the AL-RNN architecture.

\(k t\) as it passes through different subregions \(U_{e}\), it gives rise to a specific symbolic sequence \(_{,}\) (with a unique symbol assigned at each time step via \(h:S,_{k t} a_{k}\)). We may thus think of the shift operator \(\) as moving along a trajectory in correspondence with the flow map \(_{ t}()\). If the symbolic coding of each trajectory is unique, \(\) may constitute a _Markov partition_ (see Appx. B for a formal definition). We denote by \((A_{S,},)\)_the shift of finite type_ induced by the flow \(\) which picks out from the full shift space \(^{}\) only those _admissible_ symbologic sequences that correspond to valid trajectories of \((S,)\) (we will use the term 'induced by' to refer to this property). The set of admissible blocks constitutes the _language_ of \((A_{S,},)\). Every shift of finite type has a graph representation \(=\{,\}\) (Fig. 2), with either the edges \(e_{ij}\) or vertices \(v_{i}\) of the graph encoding the permitted transitions among symbols \(a_{k}\) of admissible sequences \( A\).

The finite collection \(=\{U_{0} U_{n-1}\},\ n=2^{P}\), of _linear subregions_ of an AL-RNN defined in eq. 4, separated by the switching manifolds \(_{i,j}=}}\) between every pair of neighboring subregions \(U_{i}\) and \(U_{j}\), forms a topological partition. Here we use this partition as the basis of our symbolic coding and the respective symbolic dynamics \((A_{,F_{}},)\) induced by the AL-RNN, where we assign to each state \(_{t} S\) (i.e., at each time point) the unique symbol \(a_{t}\) such that \(a_{t}=a_{i}\) iff \(_{t} U_{i}\) (Fig. 2).3 In the corresponding symbolic graphs, we identify vertices \(v_{i}\) with symbols \(a_{i}\) and draw a directed edge \(e_{ij}\) from \(v_{i}\) to \(v_{j}\) whenever \(F_{}(U_{i} B) U_{j}\), where \(B\) is the attracting set of interest (see Appx. A.1 for details). As we will show further below, this particular partition has useful theoretical properties that makes the symbolic coding topologically interpretable w.r.t. the AL-RNN map \(F_{}\). In fact, a large literature in symbolic dynamics has dealt with the relation between the dynamics in a finite shift space and that of a PWL map, like, e.g., the tent map, with a partition of \(S\) into the map's different linear subregions as we use here for the AL-RNN [55; 3; 68].

## 4 Theoretical Results

Recall that within each subregion \(U_{e}\) the map \(F_{}\) is monotonic and the dynamics are linear (ruling out certain possibilities, like chaos or isolated cycles occurring within just one subregion). We furthermore assume that the dynamics are globally non-diverging (this could be strictly enforced through'state-clipping' and constraints on matrix \(\) in eq. 4, see Hess et al. , but will also be the case for a well-trained AL-RNN). Here we claim that for hyperbolic AL-RNNs \(F_{}\)4, we have 1:1 relations between important topological objects in the AL-RNN's state space and those of the symbolic coding formed from the linear subregions \(U_{e}\) of the AL-RNN, as expressed in the following theoretical results.

Consider a hyperbolic, non-globally-diverging AL-RNN \(F_{}\), eq. 4, and a topological partition \(\) of the state space into its linear subregions \(U_{e} S,e=0 2^{P}-1\). Denote by \((A_{,F_{}},)\) the finite shift induced by \((S,F_{})\), with each \(a_{e}\) of its alphabet \(\) associated with exactly one linear subregion \(U_{e}\), and let us consider the system's evolution only in forward time. Then the following holds:

**Theorem 1**.: _An orbit \(_{S}=\{_{1},,_{n},\}\) of the AL-RNN \(F_{}\) is asymptotically fixed (i.e., converges to a fixed point) if and only if the corresponding symbolic sequence

Figure 2: Illustration of symbolic approach (3 panels on the left) and geometrical graphs (right).

\((a_{1}a_{2}a_{3} a_{N-1})(a^{*})^{} A_{,F_{}}\) is an eventually fixed point of the shift map \(\) (where by 'eventually' we mean it exactly lands on the point in the limit, see Appx. B for a precise definition)._

Proof.: See Appx. B. 

**Theorem 2**.: _An orbit \(_{S}=\{_{1},,_{n},\}\) of the AL-RNN \(F_{}\) is asymptotically \(p\)-periodic if and only if the corresponding symbolic sequence_

\[\,=\,(a_{1}a_{2} a_{N-1})(a_{1}^{*}a_{2}^{*} a_{p}^{*})^{ } A_{,F_{}}\]

_is an eventually \(p\)-periodic orbit of the shift map \(\)._

Proof.: See Appx. B. 

**Theorem 3**.: _An orbit \(_{S}=\{_{1},,_{n},\}\) is an asymptotically **aperiodic** (irregular) orbit of the AL-RNN \(F_{}\) if and only if the corresponding symbolic sequence \((a_{1},,a_{n},)\) is aperiodic._

Proof.: See Appx. B. 

Loosely speaking, these results confirm that fixed points of our symbolic coding correspond to fixed points of the AL-RNN, cycles to cycles, and chaos to chaos, thus preserving important topological properties in the symbolic representation.

## 5 Experimental Results

To assess the quality of DSR, we employed established performance criteria based on long-term, invariant topological, geometrical, and temporal features of DS [51; 11; 39]. Due to exponential trajectory divergence in chaotic systems, mean-squared prediction errors rapidly grow even for well-trained systems, and hence are only of limited suitability for evaluating DSR quality [108; 67]. Thus, we prioritize the geometric agreement between true and reconstructed attractors, quantified by a Kullback-Leibler divergence (\(_{}\), Appx. A.2) . Additionally, we examine the long-term temporal agreement between true and reconstructed time series by evaluating the average dimension-wise Hellinger distance (\(D_{}\)) between their power spectra (Appx. A.2). We first confirmed that the AL-RNN is at least on par with other SOTA methods for DSR. We then tested AL-RNNs on two commonly employed benchmark DS for which minimal PWL representations are known, the famous Lorenz-63 model of atmospheric convection  and the chaotic Rossler system . We finally explored the suitability of our approach on two real-world examples, human electrocardiogram (ECG) and human functional magnetic resonance imaging (fMRI) data.

### SOTA performance

While our goal here is a technique that constructs topologically minimal, interpretable DS representations, at the same time we do not want to compromise on DSR performance which should still be within the same ballpark as existing SOTA methods. We checked this on the Lorenz-63, Rossler and ECG data noted above, and in addition on the higher-dimensional chaotic Lorenz-96 system  and on human electroencephalogram (EEG) data. Table 1 in the Appx. confirms that the AL-RNN is not only on par with, but indeed outperforms most other techniques when trained with sparse teacher forcing (which may be rooted in its simple and parsimonious design).

### Reconstructed Systems Occupy a Small Number of Subregions

Fig. 3 illustrates reconstruction performance for varying numbers of ReLU nonlinearities at constant network size \(M\). We found that a small number of PWL units already significantly improves performance, especially for the Lorenz-63 and Rossler systems, and that beyond that number performance starts to plateau (or even briefly decrease again). Additionally, some linear units are necessary to sufficiently expand the space, but they cannot compensate for an insufficient number of PWL units (Fig. 10).5 Moreover, as shown in Fig. 4 (left), the number of linear subregions explored by the trained dynamics saturates well below the theoretical limit of \(2^{P}\) once this performance threshold is reached. Within this already small subset of explored subregions, generated network activity is furthermore concentrated within an even smaller number of dominant subregions: For instance, for the Rossler system \(4\) out of \(45\) subregions used cover \(80\%\) of the data (Fig. 4, right). This substantial reduction of necessary linear subregions strongly facilitates the analysis of trained models with respect to fixed points and \(k\)-cycles, which naively would require examining \(2^{P}\) and \(2^{kP}\) combinations of subregions, respectively. To select the optimal number of PWL units, the point where performance starts plateauing (as in Fig. 3) may be chosen. Alternatively, one may restrict the number of linear subregions employed through regularization, adding a penalty for ReLU nonlinearities. This approach, as Fig. 9 illustrates, results in the same number of selected PWL units.

### Minimal PWL Reconstructions of Chaotic attractors

Topologically minimal reconstructionsInvestigating reconstructions with the minimal number of PWL units needed for close-to-optimal performance (Fig. 3), we found that the AL-RNN would deliver reconstructions capturing the overall structure of the attractor using only three (Lorenz-63 system) or two (Rossler system) linear subregions (Fig. 5**a**), explaining the strong performance gains in Fig. 3 for \(2\) and \(1\) PWL units, respectively. These representations, and their symbolic coding (Fig. 5**c**), expose the mechanisms of chaotic dynamics (Fig. 5**b**). Notably, these closely agree with the minimal topologically equivalent PWL representations of the two chaotic DS as described in Amaral et al. : The Lorenz-63 system has at its core two unstable spiral points in the two lobes, separated by the saddle node in the center (Fig. 5**b**). For the Rossler system, the topologically minimal PWL representation indeed consists of just two subregions , one containing an unstable spiral in the \(x\)-\(y\) plane and the other a 'half-spiral' almost orthogonal to that plane (Fig. 5**b**). The AL-RNN automatically and robustly discovers these representations from data: across multiple training runs, performance values are very similar (Figs. 23, 25), the assignment of subregions to different parts of the attractor remains almost the same (Figs. 24\(\&\) 25), and the regions with linear dynamics

Figure 4: Left: Number of linear subregions traversed by trained AL-RNNs as a function of the number \(P\) of ReLUs. Theoretical limit (\(2^{P}\)) in red. Right: Cumulative number of data (trajectory) points covered by linear subregions in trained AL-RNNs (Rössler: \(M=20,P=10\), Lorenz-63: \(M=20,P=10\), ECG: \(M=100,P=10\)), illustrating that trajectories on an attractor live in a relatively small subset of subregions.

Figure 3: Quantification of DSR quality in terms of attractor geometry disagreement (\(D_{}\), top row) and disagreement in temporal structure (\(D_{}\), bottom row) as a function of the number of ReLUs (\(P\)) in the AL-RNN (Rössler: \(M=20\), Lorenz-63: \(M=20\), ECG: \(M=100\), fMRI: \(M=50\)). The little humps at \(P=3\) for the Lorenz-63 indicate that performance may sometimes first degrade again when passing the number of minimally necessary PWL units (see also Fig. 9). Error bars = SEM.

closely agree both in terms of their topology and geometry (in fact, the topological graphs remained identical). This is in contrast to the standard PLRNN, where assignments strongly varied among multiple training runs (Figs. 23, 25). We quantified this further by computing across training runs separately for each subregion \(D_{}\) (Fig. 5**d**), the normalized distances between fixed points (Fig. 5**e**), and the normalized differences between the maximum absolute eigenvalues \(_{}\) of the AL-RNN's Jacobians (Fig. 5**f**), obtaining values close to zero in all three cases (see Fig. 22 for absolute values). While Amaral et al.  explicitly handcrafted such minimal PWL representations, the AL-RNN extracts them automatically without the provision of any prior knowledge about the system.

Geometrically minimal reconstructionsWhile these reconstructions capture the topology of the underlying DS, they do not yet capture the full geometry and temporal structure of the attractor (Fig. 3). Fig. 6 illustrates for the Rossler system that as the number of PWL units is further increased to \(P=10\), the geometrical agreement becomes almost perfect. Although the mapping from latent to observation space is not 1:1 (since \(M>N\)), points close in observation space still tended to fall into the same latent subregion, such that the observed system's attractor still decomposed into distinct subregions, as confirmed by proximity matching (see Appx. A.1). For the Rossler system there is just one nonlinearity, the \(x z\) term in the temporal derivative of \(z\) (eq. 14). Accordingly, the AL-RNN devotes most of its subregions to the lobe along the \(z\) coordinate, while dynamics in the \((x,y)\) plane is geometrically faithfully represented by only \(4\) subregions. Hence, the AL-RNN utilizes additional subregions to express finer geometric details where dynamics are more nonlinear. This is apparent from a more geometrical graph representation (see Fig. 2, right), where - in addition to topological information - transition probabilities among subregions are being used to construct node distances via the graph Laplacian (see Appx. A.1), see Fig. 6 for the Rossler and Fig. 11 for the Lorenz-63.

### PWL Reconstructions of Real-World Systems

Topologically minimal reconstructionsWe next considered two experimental datasets, human ECG data (with \(1d\) membrane potential recordings delay-embedded into \(N=5\), see Appx. A.3) and fMRI recordings (with \(N=20\) time series extracted, cf. Appx. A.3) from human subjects performing three different types of cognitive task .

Figure 5: **a**: Color-coded linear subregions of minimal AL-RNNs representing the Rössler (top) and Lorenz-63 (bottom) chaotic attractor. **b**: Illustration of how the AL-RNN creates the chaotic dynamics. For the Rössler, trajectories diverge from an unstable spiral point (true position in gray, learned position in black) into the second subregion, where after about half a cycle they are propelled back into the first. For the Lorenz-63, two unstable spiral points (true: gray; learned: black) create the diverging spiraling dynamics in the two lobes, separated by the saddle node in the center. **c**: Topological graphs of the symbolic coding. While for the Rössler it is fully connected, for the Lorenz-63 the crucial role of the center saddle region in distributing trajectories onto the two lobes is apparent. **d**: Geometrical divergence (\(D_{}\)) among repeated trainings of AL-RNNs (\(n=20\)), separately evaluated within each subregion, shows close agreement among different training runs. Likewise, low **e**: normalized distances between fixed point locations and **f**: relative differences in maximum absolute eigenvalues \(^{}\) across \(20\) trained models indicate that these topologically minimal representations are robustly identified.

As for the Lorenz-63, for the ECG data we observed a strong performance gain for just \(P=2\) PWL units, see Fig. 3. Indeed, Fig. 7**a** confirms that the complex activity pattern and positive max. Lyapunov exponent (\(_{}=1.96\ s^{-1}\), ground truth: \(_{}^{}=2.19\ s^{-1}\)) of the ECG time series could be achieved with \(P=2\) in only \(3\) linear subregions. These subregions corresponded to distinct parts of the ECG activity: ramping-up phases (light blue, node \(\#1\)), declining activity (dark blue, node \(\#2\)), represented by two unstable spirals with shifted phases (Fig. 12), and the Q wave (medium blue, node \(\#3\)). The activity in the third region (\(_{max} 1.34\), other two: \(_{max} 1.02\), Fig. 12) caused a strong inhibition in the second PWL unit (Fig. 7**b**) that captures the critical transition initiated by the Q wave. The Q wave triggers the depolarization of the interventricular septum during the QRS complex , indicating that this latent depolarization process is captured by the model. These results suggest that the AL-RNN cannot only learn dynamically but also biologically interpretable latent representations. The core aspects of this representation were furthermore consistent across successful reconstructions (Fig. 13). The symbolic sequences corresponding to the graph in Fig. 7 reveal the nearly periodic yet chaotic dynamics of the ECG (Fig. 14).6

For the short (\(T=360\)) fMRI time series, \(P=3\) often resulted in reconstructions matching the complex activity patterns reasonably well (cf. Fig. 3, right). Fig. 15 illustrates results for an example subject using \(8\) linear subregions. The second most visited subregion implemented an unstable spiral, while the most visited region had a stable _virtual_ fixed point also located in the second subregion. These two regions covered over \(50\%\) of the data and were strongly connected (Fig. 15**b**). This balance between stable and unstable activity suggests a mechanism through which the network implements chaotic dynamics, with the stable virtual fixed point pulling activity into the second subregion from which it then diverges again (Fig. 15**d**).

Figure 6: Geometrically minimal reconstruction and graph representation of the Rössler attractor (\(M=30,P=10,D_{stsp}=0.08,D_{H}=0.06\)). **a**: Provided a sufficient number of linear subregions, the geometry of the attractor is almost perfectly captured. **b**: Reconstruction with linear subregions color-coded by frequency of visits (dark: most frequently visited regions, yellow: least frequent regions). **c**: Corresponding geometrical graph, which contains information about transition frequencies via node distances, visualized using the spectral layout in networkx. Note that self-connections were omitted in this representation. **d**: Connectome of relative transition frequencies between subregions.

Figure 7: **a**: Freely generated ECG activity using an AL-RNN with 3 linear subregions (color-coded according to subregion) and ground truth time series in black. **b**: After activation of the Q wave in the third subregion, the second PWL unit is driven far below 0, whose activity, consistent with the known physiology , mimics the latent de- and re-polarization process of the interventricular septum. **c**: Symbolic graph representation of the trained AL-RNN.

Task stages align with linear subregionsTo integrate the fMRI signal with cognitive (task-stage) information, in addition to the linear decoder for the BOLD signal, we coupled the PWL units \(_{t}^{p}\) to a categorical decoder model (eq. 12) which predicts the three cognitive task stages and the 'Rest and Instruction' period, as in [52; 12]. Using only \(P=2\) PWL units on the fMRI data made it challenging to capture the complex, chaotic long-term activity patterns in the freely generated activity of the AL-RNN. However, the dynamics were still well-approximated locally (Fig. 17). To maintain temporal alignment with the task stages when sampling from the AL-RNN, the AL-RNN's readout units were reset to the observations every \(7\) time steps (Fig. 8**a**). Fig. 8**a**-**b** show that in this setup, the four linear subregions of the AL-RNN often closely aligned with the different task stages of the experimental time series. Similar results were obtained across different subjects, with an average classification accuracy of \(p=0.78 0.05\) (mean \(\) SEM) (see Appx. A.4 for details). While the categorical decoder aids in separating latent states according to task stage, there is nothing that would bias this separation to align with the linear subregions. The observed alignment therefore suggests that the AL-RNN learns to leverage distinct linear dynamics in each subregion to represent differences across cognitive tasks. Furthermore, as shown in Fig. 18, the network weights of the PWL units were significantly larger than those of other units, indicating their critical role in modulating the dynamics and representing task-related variations in brain activity. This approach also demonstrates how local context-aligned linear approximations can be achieved using the AL-RNN, which is useful in areas such as model-predictive control [72; 23; 58].

## 6 Conclusion

Here we introduced a novel variant of a PLRNN, the AL-RNN, which learns to represent nonlinear DS with as few PWL nonlinearities as possible. Despite its simple design and the minimal hyperparameter tuning required, the AL-RNN robustly and automatically identifies highly interpretable, topologically minimal representations of complex nonlinear DS, reproducing known minimal PWL forms of chaotic attractors . Such minimal PWL forms that allow for an interpretable symbolic and graph-theoretical representation were discovered even from challenging physiological and neuroscientific data. They also profoundly ease subsequent model analysis. For instance, with only a few linear subregions to consider, the search for fixed points or cycles becomes very fast and efficient .

LimitationsWhile this seems promising, how to determine whether a topologically minimal and valid reconstruction from empirical data has truly been achieved remains an open topic. Performance curves as in Fig. 3 or Fig. 9 give an indication of how many PWL units may be required to yield an optimal minimal representation, but whether there is a more principled way of automatically inferring the optimal number \(P\) of PWL nonlinearities from data may be an interesting future direction. Finally, the current finding that even for empirical ECG and fMRI data a few linear subregions (\( 8\)) suffice for faithful reconstructions is encouraging. Whether this more generally will be the case in empirical scenarios is another interesting and open question. Not all types of (empirically observed) dynamical systems may easily allow for such topologically minimal representations.

All code created is available at https://github.com/DurstewitzLab/ALRNN-DSR.

Figure 8: Reconstructions from human fMRI data using an AL-RNN with \(M=100\) total units and \(P=2\) PWL units. **a**: Mean generated BOLD activity color-coded according to the linear subregion. Background color shadings indicate the task stage. **b**: Generated activity (trajectory points) in the latent space of PWL units with color-coding indicating task stage as in **a**.