# Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input

Ziang Chen

Department of Mathematics

Massachusetts Institute of Technology

Cambridge, MA 02139

ziang@mit.edu

&Rong Ge

Department of Computer Science and Department of Mathematics

Duke University

Durham, NC 27708

rongge@cs.duke.edu

###### Abstract

In this work, we study the mean-field flow for learning subspace-sparse polynomials using stochastic gradient descent and two-layer neural networks, where the input distribution is standard Gaussian and the output only depends on the projection of the input onto a low-dimensional subspace. We establish a necessary condition for SGD-learnability, involving both the characteristics of the target function and the expressiveness of the activation function. In addition, we prove that the condition is almost sufficient, in the sense that a condition slightly stronger than the necessary condition can guarantee the exponential decay of the loss functional to zero.

## 1 Introduction

Neural Networks (NNs) are powerful in practice to approximate mappings on certain data structures, such as Conventional Neural Networks (CNNs) for image data, Graph Neural Networks (GNNs) for graph data, and Recurrent Neural Networks (RNNs) for sequential data, stimulating numerous breakthroughs in application of machine learning in many branches of science, engineering, etc. The surprising performance of neural networks is often explained by arguing that neural networks automatically learns useful representations of the data. However, how simple training procedures such as stochastic gradient descent (SGD) extract features remains a major open problem.

Optimization of neural networks has received lots of attention. For simpler networks such as linear neural networks, local minima are also globally optimal [14; 15; 17]. However, this is not true for nonlinear networks even of depth 2 . Neural Tangent Kernel (NTK, [4; 12; 13]) is a line of work that establishes strong convergence results for wide neural networks. However, in the NTK regime, neural network is equivalent to a kernel, which cannot learn useful features based on the target function. Such limitation prevents neural networks in NTK regime from efficiently learning even simple single index models .

As an alternative, the behavior of SGD can also be understood via mean-field analysis, for both two-layer neural networks [9; 19; 20; 22; 24; 25] and multi-layer neural networks [5; 21; 22]. Neural networks in the mean-field regime have the potential to do feature learning. Recently,  showed an interesting setup where a two-layer neural network can learn representations if the target function satisfies a merged-staircase property. More precisely,  considers a sparse polynomial as apolynomial \(f^{*}:^{d}\) defined on the hypercube \(\{-1,1\}^{d}\), i.e., \(f^{*}(x)=h^{*}(z)=h^{*}(x_{I})\) where \(z=x_{I}=(x_{i})_{i I}\), \(I\) is an unknown subset of \(\{1,2,,d\}\) with \(|I|=p\), and \(h^{*}:\{-1,1\}^{p}\) is a function on the subset of coordinates in \(I\). They prove that a condition called the merged-staircase property is necessary and in some sense sufficient for learning such \(f^{*}\) using SGD and two-layer neural networks. The merged-staircase property proposed in  states that all monomials of \(h^{*}\) can be ordered such that each monomial contains at most one \(z_{i}\) that does not appear in any previous monomial. For example, \(h^{*}(z)=z_{1}+z_{1}z_{2}+z_{1}z_{2}z_{3}\) satisfies the merged-staircase property while \(h^{*}(z)=z_{1}+z_{1}z_{2}z_{3}\) does not. Results on similar structures can also be found in . The work  proposes the concept of leap complexity and generalizes the results in  to a larger family of sparse polynomials.

In this work, we consider "subspace-sparse" polynomial that is more general. Concretely, let \(f^{*}(x)=h^{*}(z)=h^{*}(x_{V})\), where \(V\) is a subspace of \(^{d}\) with \((V)=p d\), \(x_{V}\) is the orthogonal projection of \(x\) onto the subspace \(V\), and \(h^{*}:V\) is an underlying polynomial map. In other words, the sparsity is in the sense that \(f^{*}(x)\) only depends on the projection of the input \(x^{d}\) in a low-dimensional subspace. Throughout this paper, the input data distribution is the standard \(d\)-dimensional normal distribution, i.e., \(x\ (0,I_{d})\), which is rotation-invariant in the sense that \(Ox(0,I_{d})\) for any orthogonal matrix \(O^{d d}\). Similar rotation-invariant/basis-free settings are also considered in some recent studies, including .

Our contribution and related worksOur first contribution is a basis-free necessary condition for SGD-learnability. More specially, we propose the reflective property of the underlying polynomial \(h^{*}:V\) with respect to some subspace \(S V\), which also involves the expressiveness of the activation function. We prove that as long as the reflective property is satisfied with respect to nontrivial \(S\), the training dynamics cannot learn any information about the behavior of \(h^{*}\) on \(S\) (see Theorem 3.4). Therefore the loss functional will be bounded away from 0 during the whole training procedure.

One key point is that our reflective property precisely characterizes the necessary expressiveness of the activation function. If the activation function is expressive enough, the reflective property equivalently recovers a necessary condition characterized by isoLeap  that is the maximal leap complexity over all orthonormal basis and can be viewed as a basis-free generalization of the merged-staircase property. This also indicates that our necessary condition is a bit weaker. Other related rotation-invariant conditions in the previous literature include leap exponent/index , subspace conditioning  and even-symmetric directions . The analysis in  is for training the first layer for finitely many iterations with fixed second-layer, and  studies the joint learning dynamics where they assume that for any fixed first layer, the optimal parameters in second layer can be found efficiently and reformulate the loss as a function of the first layer. Differently and more generally, our analysis for the necessary condition does not require specific learning strategies and works for any learning rates satisfying some mild conditions.

Our second contribution is a sufficient condition for SGD-learnability that is also basis-free and is slightly stronger than the necessary condition. In particular, we show that if the training dynamics cannot be trapped in any proper subspace of \(V\), then one can choose the initial parameter distribution and the learning rate such that the loss functional decays to zero exponentially fast with dimension-free rates (see Theorem 4.3). Our training strategy is inspired by  with the difference that we take the average of \(p\) independent training trajectories, which can lift some linear independence property required for polynomials on hypercube to algebraic independence in the general polynomial setting.

Technical challengesIt may seem simple to leave the standard basis and generalize the results of  to learn subspaces, because SGD itself is independent of the basis, and we can consider a symmetric Gaussian input distribution. However, there are some significant barriers that motivated our training process. The condition and the analysis in  rely on an orthonormal basis of the input space \(^{d}\). This is natural for polynomials on the hypercube \(\{-1,1\}^{d}\), but not for general polynomials on \(^{d}\). Particularly, their theory does not work for Gaussian input data \(x(0,I_{d})\), which is probably the most common distribution in data science, unless an orthonormal basis of \(^{d}\) is specified and \(V\) is known to be spanned by \(p\) elements in the basis. In this work, we consider a more general setting in which specifying a basis is not required and the space \(V\) can be any \(p\)-dimensional subspace of \(^{d}\). This setting is consistent with the rotation-invariant property of \((0,I_{d})\) and introduces more difficulties since less knowledge of \(V\) is available prior to training.

OrganizationThe rest of this paper will be organized as follows. We introduce some preliminaries on mean-field dynamics in Section 2. The basis-free necessary and sufficient conditions for SGD-learnability are discussed in Section 3 and Section 4, respectively. We conclude in Section 5.

## 2 Preliminaries on Mean-Field Dynamics

The mean-field dynamics describes the limiting behavior of the training procedure when the step-size/learning rate converges to zero, i.e., the evolution of a neuron converges to the solution of a differential equation with continuous time, and when the number of neurons converges to infinity, i.e., the empirical distribution of all neurons converges to some limiting probability distribution. For two-layer neural networks, some quantitative results are established in  that characterize the distance between the SGD trajectory and the mean-field evolution flow, and these results are further improved as dimension-free in . Such results suggest that analyzing the mean-field flow is sufficient for understanding the SGD trajectory in some settings. In this section, we briefly review the setup of two-layer neural networks, SGD, and their mean-field versions, following [19; 20].

Two-layer neural network and SGDThe two-layer neural network is of the following form:

\[f_{}(x;):=_{i=1}^{N}(x;_{i})= {N}_{i=1}^{N}a_{i}(w_{i}^{}x),\] (2.1)

where \(N\) is the number of neurons, \(=(_{1},_{2},,_{N})\) with \(_{i}=(a_{i},w_{i})^{d+1}\) is the set of parameters, and \(:\) is the activation functions with \((x;):=a(w^{}x)\) for \(=(a,w)\). Then the task is to find some parameter \(\) such that the \(_{2}\)-distance between \(f^{*}\) and \(f_{}\) is minimized:

\[_{}\,_{N}():=_{x(0,I_{d})}[|f^{*}(x)-f_{}(x;)|^{2}].\] (2.2)

In practice, a widely used algorithm for solving (2.2) is the stochastic gradient descent (SGD) that iterates as

\[_{i}^{(k+1)}=_{i}^{(k)}+^{(k)}(f^{*}(x_{k})-f_{}(x_{k};^{(k)}))_{}(x_{k};_{i}^{(k)}),\] (2.3)

where \(x_{k},\ k=1,2,\) are the i.i.d. samples drawn from \((0,I_{d})\) and \(^{(k)}=(_{a}^{(k)},_{w}^{(k)}I_{d}) 0\) is the stepsize or the learning rate. In this paper, we only consider the one-pass model with each data point being used exactly once, following .

Mean-field dynamicsOne can generalize (2.1) to an infinite-width two-layer neural network:

\[f_{}(x;):=(x;)(d)= a(w^{} x)(da,dw),\]

where \((^{d+1})\) is a probability measure on the parameter space \(^{d+1}\), and generalize the loss/energy functional (2.2) to

\[():=_{x(0,I_{d})}[|f^ {*}(x)-f_{}(x;)|^{2}].\]

We will use \((X)\) to denote the collection of probability measures on a space \(X\) throughout this paper. The limiting behavior of the SGD trajectory (2.3) when \(^{(k)} 0\) and \(N\) can be described by the following mean-field dynamics:

\[_{t}_{t}=_{}(_{t}(t) _{}(;_{t})),\\ _{t}_{t=0}=_{0},\] (2.4)

where \((t)=(_{a}(t),_{w}(t)I_{d})^{(d+1)(d+1)}\) with \(_{a}(t) 0\) and \(_{w}(t) 0\) being the learning rates and

\[(;)=a_{x(0,I_{d})}[(f_{}(x;)-f^{*}(x))(w^{}x)].\]

One can also write \((;)\) as

\[(;)=V()+ U(,^{})(d^{ }),\]where

\[V()=-a_{x}[f^{*}(x)(w^{}x)]  U(,^{})=aa^{}_{x}[(w^{}x) ((w^{})^{}x)].\] (2.5)

The PDE (2.4) is understood in the weak sense, i.e., \(_{t}\) is a solution to (2.4) if and only if \(_{t}_{t=0}=_{0}\) and

\[(-_{t}+_{}((t)_{ }(;_{t})))_{t}(d)dt=0, \;_{c}^{}(^{d+1}(0,+)),\]

where \(_{c}^{}(^{d+1}(0,+))\) is the collection of all smooth and compactly supported functions on \(^{d+1}(0,+)\). It can also be computed that the energy functional is non-increasing along \(_{t}\):

\[(_{t})=-_{}(;_{t})^{ }(t)_{}(;_{t})_{t}(d) 0.\] (2.6)

There have been standard results in the existing literature that provide dimension-free bounds for the distance between the empirical distribution of the parameters generalized by (2.3) and the solution to (2.4). For the simplicity of reading, we will not present those results and the proof; interested readers are referred to . In the rest of this paper, we will focus on the analysis of (2.4) and briefly discuss the sample complexity results implied by our mean-field analysis.

## 3 Necessary Condition for SGD-Learnability

This section introduces a condition that can prevent SGD from recovering all information about \(f^{*}\), or in other words, prevent the loss functional \((_{t})\) decaying to a value sufficiently close to \(0\).

### Reflective Property

Before rigorously presenting our main theorem, we state the assumptions used in this section.

**Assumption 3.1**.: _Assume that the followings hold:_

1. _The activation function_ \(:\) _is twice continuously differentiable with_ \(\|\|_{L^{}()} K_{}\)_,_ \(\|^{}\|_{L^{}()} K_{}\)_, and_ \(\|^{}\|_{L^{}()} K_{}\) _for some constant_ \(K_{}>0\)_._
2. _The learning rates_ \(_{a},_{w}:_{ 0}\) _satisfy that_ \(\|_{a}\|_{L^{}(_{ 0})} K_{}\) _and_ \(\|_{w}\|_{L^{}(_{ 0})} K_{}\) _for some constant_ \(K_{}>0\)_. Furthermore,_ \(_{a}\) _and_ \(_{w}\) _are Lipschitz continuous with_ \(_{0}^{+}_{a}(t)dt=+\) _and_ \(_{0}^{+}_{w}(t)dt=+\)_._
3. _The initialization is_ \(_{0}=_{a}_{w}\) _such that_ \(_{a}\) _is symmetric and is supported in_ \([-K_{},K_{}]\) _for some constant_ \(K_{}>0\)_._

In Assumption 3.1, the Condition (i) is satisfied by some commonly used activation functions, such as \((x)=}\) and \((x)=(x)\), and is required for establishing the existence and uniqueness of the solution to (2.4). The Condition (ii) and (iii) are also standard and easy to satisfy in practice.

**Remark 3.2**.: _The symmetry of \(_{a}\) implies that \(f_{}(x;_{0})=0\). Therefore, the initial loss \((_{0})=_{x}[|f^{*}(x)|^{2}]= _{x_{V}}[|h^{*}(x_{V})|^{2}]=_{z}[|h^{*}(z)|^{2}]\), where \(x_{V}=z(0,I_{V})\), can be viewed as a constant depending only on \(h^{*}\) and \(p\), independent of \(d\). Noticing also the decay property (2.6), the loss at any time \(t\) can be bounded as \((_{t})(_{0})=_{z}[|h^{* }(z)|^{2}]\)._

The main goal of this section is to generalize the merged-staircase property in a basis-free setting for general polynomials. Without a standard basis, it is hard to talk about having a "staircase" of monomials. Even with a fixed basis, it is still nontrial to define the merged-staircase property for general polynomials since the analysis in  highly depends on \(z_{i}^{2}=1\) that is only true for polynomials on the hypercube. Instead, we use the observation that when a function does not satisfy the merged staircase property, it implies that two of the variables will behave the same in the training dynamics. Such a symmetry can be generalized to the basis-free setting for general polynomials and we summarize this as the following reflective property:

**Definition 3.3** (Reflective property).: _Let \(S V^{d}\) be a subspace of \(V\). We say that the underlying polynomial \(h^{*}:V\) satisfies the reflective property with respect to the subspace \(S\) and the activation function \(\) if_

\[_{z(0,I_{V})}[h^{*}(z)^{}(u+v^{ }z_{S}^{})z_{S}]=0,\ u,\ v V,\] (3.1)

_where \(z_{S}=_{S}^{V}(z)\) and \(z_{S}^{}=z-_{S}^{V}(z)\), with \(_{S}^{V}:V S\) being the orthogonal projection from \(V\) onto \(S\)._

The reflective property defined above is closely related to the merged-staircase property in . Let us illustrate the intuition using a simple example. Consider \(V=^{3}\) and \(h^{*}(z)=z_{1}+z_{1}z_{2}z_{3}\). Then \(h^{*}\) does not satisfy the merged-staircase property since \(z_{1}z_{2}z_{3}\) involves two new coordinates that do not appear in the first monomial \(z_{1}\). In our setting, this \(h^{*}\) satisfies the reflective with respect to \(S=\{e_{2},e_{3}\}\), where \(e_{i}\) is the vector in \(^{3}\) with the \(i\)-th entry being \(1\) and other entries being \(0\). More specifically, for \(z=(z_{1},z_{2},z_{3})\), one has that \(z_{S}=(0,z_{2},z_{3})\) and \(z_{S}^{}=(z_{1},0,0)\). Thus, one has for any \(u\) and \(v V\) that \(^{}(u+v^{}z_{S}^{})\) is independent of \(z_{2},z_{3}\) and that

\[_{z_{2},z_{3}}[h^{*}(z)^{}(u+v^{ }z_{S}^{})z_{S}] =^{}(u+v^{}z_{S}^{})_{z _{2},z_{3}}[(0,z_{1}z_{2}+z_{1}z_{2}^{2}z_{3},z_{1}z_{3}+z_{1}z_{2}z _{3}^{2})]\] \[=(0,0,0),\]

which leads to (3.1). One can see from this example that satisfying the reflective property with respect to a nontrivial subspace \(S V\) is in the same spirit as not satisfying the merged-staircase property. Furthermore, the reflective property is rotation-invariant, meaning that using a different orthonormal basis does not change the property. In this sense, our proposed condition is more general than that in . We also remark that there have been other rotation-invariant conditions generalizing , see e.g., .

Another comment is that the reflective property (3.1) depends on the activation function \(\), while conditions in previous works  are all defined for the target function \(f^{*}\) or \(h^{*}\) itself. There does exist a variant of our reflective property that is independent of \(^{}\), namely,

\[_{z_{S}(0,I_{S})}[h^{*}(z)z_{S}]=0,\ z_{S}^{ },\] (3.2)

which actually implies (3.1). But these two conditions are different: \(h^{*}(z)=z_{1}+z_{1}z_{2}+z_{1}z_{2}z_{3}\) does not satisfy (3.2) but still satisfies (3.1) if \(()=\). We use (3.1) with \(^{}\) because we want to emphasize that the SGD learnability depends on the activation function \(\). If \(\) is less expressive, then SGD may not learn the target function even if \(h^{*}\) itself satisfies the merged-staircase property. Typically people use activation functions that are expressive enough, for which (3.1) and (3.2) are similar. In addition, it can be verified that (3.2) with some nontrivial \(S\) is equivalent to \((h^{*}) 2\) that means \(h^{*}:V\) does not satisfy the merged-staircase property for some orthonormal basis of \(V\), and the idea of leaps is used in . We include the proof of equivalence in Appendix B.1.

Our main result in this section is that the reflective property with nontrivial \(S\) would lead to a positive lower bound of \((_{t})\) along the training dynamics, which provides a necessary condition for the SGD-learnability and is formally stated as follows.

**Theorem 3.4**.: _Suppose that Assumption 3.1 holds with \(_{w}(0,I_{d})\), and that \(h^{*}:V\) satisfies the reflective property with respect to some subspace \(S V\) and activation function \(\). Then for any \(T>0\), there exists a constant \(C>0\) depending only on \(p\), \(h^{*}\), \(K_{}\), \(K_{}\), \(K_{}\), and \(T\), such that_

\[_{0 t T}(_{t})_{z (0,I_{V})}[|h^{*}(z)-h^{*}_{S^{}}(z_{S}^{})|^{2} ]-},\] (3.3)

_where \(h^{*}_{S^{}}(z_{S}^{})=_{z_{S}}[h^{*}(z)]\). In particular, if \(h^{*}(z)\) is not independent of \(z_{S}\), then for any \(T>0\), there exists \(d(T)>0\) depending only on \(p\), \(h^{*}\), \(K_{}\), \(K_{}\), \(K_{}\), and \(T\), such that for any \(d>d(T)\), we have_

\[_{0 t T}(_{t})_{z (0,I_{V})}[|h^{*}(z)-h^{*}_{S^{}}(z_{S}^{})|^{2} ]>0.\] (3.4)

It is worth remarking that in Theorem 3.4, the training time \(T\) is a constant independent of the dimension \(d\). If a longer \(d\)-dependent training beyond a constant time is allowed, then \((_{t})\) might be reasonably small even if the necessary condition is not satisfied, see e.g. .

In Appendix B.2, we include a brief discussion of the sample complexity result of SGD implied by Theorem 3.4. In particular, SGD with \((d)\) samples cannot recover \(f^{*}\) reliably if the refelctiveproperty holds, which is consistent with observations in previous works such as [1; 2]. We also remark that our result in Theorem 3.4 is established for the mean-field dynamics corresponding to the one-pass SGD (2.3), any may not apply for other variants of SGD. In particular, some recent works [6; 11; 16] prove that multi-pass SGD with batch-reuse mechanism can learn some target functions with fewer samples than one-pass SGD.

### Proof Sketch for Theorem 3.4

To prove Theorem 3.4, the main intuition is that under some mild assumptions, if (3.1) is satisfied and the initial distribution \(_{0}\) is supported in \(\{(a,w)^{d+1}:w_{S}=0\}\), where \(w_{S}\) is the orthogonal projection of \(w^{d}\) onto \(S\), then \(_{t}\) is supported in \(\{(a,w)^{d+1}:w_{S}=0\}\) for all \(t 0\). This means that the trained neural network \(f_{}(x;_{t})\) learns no information about \(x_{S}\), the orthogonal projection of \(x^{d}\) onto \(S\), and hence cannot approximate \(f^{*}(x)=h^{*}(x_{V})\) with arbitrarily small error if \(h^{*}(z)\) is dependent on \(z_{S}\). We formulate this observation in the following theorem.

**Theorem 3.5**.: _Suppose that Assumption 3.1 hold and let \(_{t}\) be the solution to (2.4). Let \(S^{d}\) be a subspace with the projection map \(_{S}:^{d+1} S\) that maps \((a,w)\) to \(w_{S}\). If \((_{S})_{\#}_{0}=_{S}\), where \(_{S}\) is the delta measure on \(S\) and_

\[_{x}[f^{*}(x)^{}(w^{}x_{S}^{}) x_{S}]=0,\;w^{d},\] (3.5)

_where \(x_{S}^{}=x-x_{S}\), then it holds for any \(t 0\) that_

\[(_{S})_{\#}_{t}=_{S}.\] (3.6)

Here, the delta measure \(_{S}\) on \(S\) is a probability measure on \(S\) such that for any continuous and compactly supported function \(:S\), it holds that \(_{S}(x)_{S}(dx)=(0)\). In Theorem 3.5, the condition (3.5) is stated in terms of \(f^{*}\). We will show later that it is closely related to and is actually implied by (3.1), via a decomposition \(w^{}x_{S}^{}=w^{}(x-x_{V})+w^{}(x_{V}-x_{S})\), with \(w^{}(x-x_{V})\) and \(w^{}(x_{V}-x_{S})\) corresponding to \(u\) and \(v^{}z_{S}^{}\) in (3.1), respectively. The main idea in the proof of Theorem 3.5 is to construct a flow \(_{t}\) in the space \(( S^{})\), where \(S^{}\) is the orthogonal complement of \(S\) in \(^{d}\), and then show that \(_{t}=_{t}_{S}\) is the solution to (2.4). More specifically, the flow \(_{t}\) is constructed as the solution to the following evolution equation in \(( S^{})\):

\[_{t}_{t}=_{}( {}_{t}(t)_{}(; _{t})),\\ _{t}_{t=0}=_{0},\] (3.7)

where \(_{0}( S^{})\) satisfies \(_{0}=_{0}_{S}\), \(=(a,w_{S}^{})\), \((t)=(_{a}(t),_{w}(t)I_{S^{}})\), and

\[(;)=a_{x}[(_{ {NN}}(x_{S}^{};)-f^{*}(x))((w_{S}^{})^{ }x_{S}^{})]=()+(,^{})(d^{}),\]

with

\[_{}(x_{S}^{};)= a((w_{S}^{ })^{}x_{S}^{})(da,dw_{S}^{}),\]

and

\[()=-a_{x}[f^{*}(x)((w_{S}^{ })^{}x_{S}^{})],(,^{})=aa^ {}_{x}[((w_{S}^{})^{}x_{S}^{} )(((w_{S}^{})^{})^{}x_{S}^{})].\]

The detailed proof will be presented in Appendix A.1.

In practice, both \(V\) and \(S\) are unknown and it is nontrivial to choose an initialization \(_{0}\) supported in \(\{(a,w)^{d+1}:w_{S}=0\}\). However, one can set \(_{0}=_{a}_{w}\) with \(_{w}(0,I_{d})\) and this can make the marginal distribution of \(_{0}\) on \(S\) very close to the the delta measure \(_{S}\) if \(d>>p=(V)(S)\), which fits the setting of subspace-sparse polynomials. Rigorously, we have the following theorem stating dimension-free stability with respect to initial distribution, with the proof deferred to Appendix A.2.

**Theorem 3.6**.: _Suppose that Assumption 3.1 holds for both \(_{0}\) and \(_{0}\). Let \(_{t}\) solve \(_{t}_{t}=_{}(_{t}(t)_{}( ;_{t}))\) and let \(_{t}\) solve \(_{t}_{t}=_{}(_{t}(t)_{ }(;_{t}))\). Then for any \(T(0,+)\), there exists a constant \(C_{s}>0\) depending only on \(p\), \(h^{*}\), \(K_{}\), \(K_{}\), \(K_{}\), and \(T\), such that_

\[_{0 t T}_{x}[|f_{}(x;_{t})-f_{}(x ;_{t})|^{2}] C_{s}W_{2}^{2}(_{0},_{0}),\] (3.8)

_where \(W_{2}(,)\) is the \(2\)-Wasserstein metric._

Based on Theorem 3.5 and Theorem 3.6, Theorem 3.4 can be proved by some straightforward computation, for which the details can be found in Appendix A.3.

## 4 Sufficient Condition for SGD-Learnability

In this section, we propose a sufficient condition and a training strategy that can guarantee the exponential decay of \((_{t})\) with constants independent of the dimension \(d\).

### Training Procedure and Convergence Guarantee

We prove in Section 3 that if the trained parameters always stay in a proper subspace \(\{(a,w)^{d+1}:w_{S}=0\}\), then \(f_{}(x;_{t})\) cannot learn all information about \(f^{*}\) or \(h^{*}\). Ideally, one would expect the negation to be a sufficient condition for the SGD-learnability, i.e., the existence of a choice of learning rates and initial distribution that guarantees \(_{t}(_{t})\) with dimension-free rate. This is almost true but we need a slightly stronger condition due to technical issues. More specifically, we need that the Taylor's expansion of some dynamics (not the dynamics itself) is not trapped in any proper subspace.

**Assumption 4.1**.: _Consider the following flow \(_{V}(t)\) in \(V\):_

\[_{V}(t)=_{z}[zh^{*}(z)^{ }(_{V}(t)^{}z)],\\ _{V}(0)=0.\] (4.1)

_We assume that for some \(s_{+}\), the Taylor's expansion up to \(s\)-th order of \(_{V}(t)\) at \(t=0\) is not contained in any proper subspace of \(V\)._

Assumption 4.1 aims to state the same observation as the merged-staircase property in . As a simple example, if \(V=^{p}\) and \(h^{*}(z)=z_{1}+z_{1}z_{2}+z_{1}z_{2}z_{3}++z_{1}z_{2} z_{p}\) which satisfies the merged-staircase property, then it can be computed that the leading order terms of the coordinates of \(_{V}(t)\) are given by \((c_{1}t,c_{2}t^{2},c_{3}t^{2^{2}},,c_{p}t^{2^{p-1}})\) with nonzero constants \(c_{1},c_{2},,c_{p}\) if \(^{*}()\) with \(s=2^{p-1}\) and \(^{(1)}(0),^{(2)}(0),,^{(p)}(0)\) are all nonzero (see Proposition 33 in ). This is to say that Assumption 4.1 with \(s=2^{p-1}\) is satisfied for this example. We provide further characterization of Assumption 4.1 by verifying it in a more general setting in Appendix D.1.

We also remark that the Taylor's expansion of the flow \(_{V}(t)\) that solves (4.1) depends only on the \(h^{*}\) and \(^{(1)}(0),^{(2)}(0),,^{(s)}(0)\). We require some additional regularity assumption on higher-order derivatives of \(\).

**Assumption 4.2**.: _Assume that \(\) satisfies \(^{L+1}()\) and \(,^{},^{},^{(L+1)} L^{}( )\), where \(L=2sn\) with \(n=(f^{*})=(h^{*})\) and \(s\) being the positive integer in Assumption 4.1._

Our proposed training strategy is stated in Algorithm 1. The training strategy is inspired by the two-stage strategy proposed in  that trains the parameters \(w\) with fixed \(a\) for \(t[0,T]\) and then trains the parameter \(a\) with fixed \(w\) and a perturbed activation function for \(t T\). Several important modifications are made since we consider general polynomials, rather than polynomials on hypercubes as in . In particular,

* We need to repeat Step 2 (training \(w\)) for \(p\) times and use their average as the initialization of training \(a\), while this step only needs to be done once in . The reason is that the space of polynomials on the hypercube \(\{ 1\}^{p}\) is essentially a linear space with dimension \(2^{p}\). However, the space of general polynomials on \(V\) is an \(\)-algebra that is also a linearspace but is of infinite dimension. Therefore, to make the kernel matrix in training \(a\) non-degenerate, we require some algebraic independence which can be guaranteed by \(u(a_{1},,a_{p},t))=_{i=1}^{p}w(a_{i},t),\ 0<t T\), though linear independence suffices for . Let us also emphasize that each run of Step 2 involves training an interacting particle system instead of training a single particle.
* In Step 4, we use a new activation function \(()=(1+)^{n}\) that is a polynomial of the same degree as \(f^{*}\) and \(h^{*}\). The reason is still that we work with the space general polynomials whose dimension as a linear space is infinite. Thus, we need the specific form \(()=(1+)^{n}\) to guarantee the trained neural network \(f_{}(x;_{t})\) is a polynomial with degree at most \(n=(f^{*})=(h^{*})\). As a comparison, in the setting of , all functions on \(\{ 1\}^{p}\) can be understood as a polynomial, and no specific format of the new activation function is needed.

Our main theorem in this section is as follows, stating that the loss functional \((_{t})\) can decay to \(0\) exponentially fast, with rates independent of the dimension \(d\).

**Theorem 4.3**.: _Suppose that Assumption 4.1 and 4.2 hold and let \(_{t}\) be the flow generated by Algorithm 1. There exist constants \(C_{1},C_{2}>0\) depending on \(h^{*},,n,p,s\), such that_

\[(_{t}) C_{1}(-C_{2}t),\,t 0.\]

Let us also remark that it is possible to use the original dynamics \(_{V}\) defined in (4.1) when we state Assumption 4.1, which can actually imply its Taylor's expansion up to some order is not trapped in any proper subspace of \(V\) if we further assume \(_{V}(t)\) is analytic. We choose to directly use Taylor's expansion in Assumption 4.1 since we want to avoid the additional analytic assumption and to emphasize that the constants \(C_{1},C_{2}\) in Theorem 4.3 depend on the order \(s\) of the Tayler's expansion satisfying Assumption 4.1.

Discussion about the sample complexity implied by Theorem 4.3 is included in Appendix D.2, suggesting that \((d)\) samples suffices for SGD to learn \(f^{*}\) reliably if conditions in Theorem 4.3 are true. This is also consistent with previous works such as .

### Proof Sketch for Theorem 4.3

To prove Theorem 4.3 we follow the same general strategy as , though some technical analysis is significantly different due to the roatation-invariant setting. The main goal here is to show before Step 4, the algorithm already learned a diverse set of features. After that, note that Step 4 in Algorithm 1 is essentially a convex/quadratic optimization problem (since we only train \(a\) and set \(_{w}(t)=0\)). In addition, thanks to the new activation function \(()=(1+)^{n}\), one only needs to consider \(_{V,n}\) that is the space of all polynomials on \(V\) with degree at most \(n=(h^{*})=(f^{*})\). The dimension of \(_{V,n}\) as a linear space is \(\). Let \(p_{1},p_{2},,\) be the orthonormal basis of \(_{V,n}\) with input \(z(0,I_{V})\) and define the kernel matrix

\[_{i_{1},i_{2}}(t)=_{a_{1},,a_{p}}[_{ z,z^{}}[p_{i_{1}}(z)(u(a_{1},,a_{p},t)^{}z) (u(a_{1},,a_{p},t)^{}z^{})p_{i_{2}}(z^{}) ]],\] (4.2)

where \(()=(1+)^{n}\), \((a_{1},,a_{p})([-1,1]^{p})\), \(1 i_{1},i_{2}\), and \(0 t T\). As long as this kernel matrix is non-degenerate, we know that the loss functional is strongly convex with respect to the parameters in the second layer when fixing the first layer, and thus, it can be computed straightforwardly that the loss decays to \(0\) exponentially fast for \(t T\), leading to the desired convergence rate in Theorem 4.3; see Appendix C.3 for details. Thus, the main part in the proof of Theorem 4.3 is to establish the non-degeneracy of the kernel matrix.

**Proposition 4.4**.: _Suppose that Assumption 4.1 and 4.2 hold. There exist constants \(C,T>0\) depending on \(h^{*},,n,p,s\), such that_

\[_{}((t)) Ct^{2sn},\,0 t  T,\] (4.3)

_where \(_{}((t))\) is the smallest eigenvalue of \((t)\)._

In the rest of this subsection, we sketch the main ideas in the proof of Proposition 4.4, with the details of the proof being deferred to Appendix C. We first show that \(w(a_{i},t)\) and \(u(a_{1},,a_{p},t)\) can be approximated well by \((a_{i},t)\) and \((a_{1},,a_{p},t)=_{i=1}^{p}(a_{i},t)\) that are polynomials in \(a_{i}\) and \(a_{1},,a_{p}\) respectively. This approximation step follows  closely and is analyzed detailedly in Appendix C.1. Therefore, to give a positive lower bound of \(_{}((t))\), one only needs to show the non-degeneracy of the matrix \((,t)^{}\) with

\[_{i_{1},i_{2}}(,t)=_{z}[p_{i_{1}}(z)((_{i_{2}},t)^{}z)],\]

where \(=(_{1},_{2},,_{})\) and \(_{i}^{p}\) for \(i=1,2,,\). Intuitively, this non-degeneracy can be implied by

\[\{((a_{1},,a_{p},t)^{}z):a_{1},a _{2},,a_{p}[-1,1]\}=_{V,n},\]

which is true if \(_{i}(a_{1},,a_{p},t),\ 1 i p\) are \(\)-algebraically independent polynomials in \(a_{1},a_{2},,a_{p}\), where \(_{i}\) is the \(i\)-th coefficient of \(\) under some basis of \(V\), and algebraic independence can be obtained from linear independence by taking the average of independent copies. We illustrate this intuition with a bit more detail.

**Algebraic independence of \(_{i}\).** With Assumption 4.1, \(_{1}(a,t),_{2}(a,t),,_{1}(a,t)\) can be proved as \(\)-linear independent polynomials in \(a\). Then one can apply the following theorem to boost the linear independence of \(_{i}\), whose constant term is zero since initialization in training is set as \(_{0}=_{a}_{^{d}}\), to the algebraic independence of \(_{i}\).

**Theorem 4.5**.: _Let \(v_{1},v_{2},,v_{p}[a]\) be \(\)-linearly independent polynomials with the constant terms being zero. Then \((v_{1}(a_{1})++v_{1}(a_{p})),,(v_{p}(a_{1}) ++v_{p}(a_{p}))[a_{1},a_{2},,a_{p}]\) are \(\)-algebraically independent._

The proof of Theorem 4.5 is deferred to Appendix C.2 and is based on the celebrated Jacobian criterion stated as follows.

**Theorem 4.6** (Jacobian criterion ).: \(v_{1},v_{2},,v_{p}[a_{1},a_{2},,a_{p}]\) _are \(\)-algebraically independent if and only if_

\[}{ a_{1}}& }{ a_{2}}&&}{ a_{p}}\\ }{ a_{1}}&}{ a_{2}}& &}{ a_{p}}\\ &&&\\ }{ a_{1}}&}{ a_{2}}& &}{ a_{p}}\]

_is a nonzero polynomial in \([a_{1},a_{2},,a_{p}]\)._

**Non-degeneracy of \((,t)\).** With the observation that \(\{(q^{}z):q V\}=_{V,n}\) (see Lemma C.13), we define another matrix \(X()^{}\) via

\[X_{i_{1},i_{2}}()=_{z}[p_{i_{1}}(z)(_{ i_{2}}^{}z)],\]

where \(=(_{1},_{2},,_{})\) with \(_{i} V\), and prove that \((X())\) is a non-zero polynomial in \(\) of the form

\[(X())=_{i=1}^{}_{0\|_{i}, \|_{1} n}X_{}^{}=_{i=1}^{ }_{0\|_{i}\|_{1} n}X_{}_{l=1}^{}_{l}^{_{i}},\]

where \(_{i}\) is understood as a (coefficient) vector in \(^{p}\) associated with a fixed orthonormal basis of \(V\) and \(=(_{1},_{2},,_{})\) with \(_{i}^{p}\). Then setting \(=(_{i_{2}},t)\) leads to

\[((,t))=_{i=1}^{}_{0\|_ {i},\|_{1} n}X_{}_{l=1}^{}(_{ l},t)^{_{l}}.\]

To prove that \(((,t))\) is a non-zero polynomial in \(\), i.e., \((,t)\) is non-degenerate, we use the following lemma linking algebraic independence back to linear independence.

**Lemma 4.7**.: _Suppose that \(v_{1},v_{2},,v_{p}[a_{1},a_{2},,a_{p}]\) are \(\)-algebraically independent. For any \(m 1\), the following polynomials in \(=(_{1},_{2},,_{m})( ^{p})^{m}\) are \(\)-linearly independent_

\[_{l=1}^{m}(_{l})^{i}, 1\| _{i}\|_{1} n,\;1 i m,\]

_where \(=(v_{1},v_{2},,v_{p})\)._

The proof of Lemma 4.7 and some other related analysis are deferred to Appendix C.3.

## 5 Conclusion and Discussions

In this work, we generalize the merged-staircase property in  to a basis-free version and establish a necessary condition for learning a subspace-sparse polynomial on Gaussian input with arbitrarily small error. Moreover, we prove the exponential decay property of the loss functional under a sufficient condition that is slightly stronger than the necessary one. The bounds and rates are all dimension-free. Our work provides some understanding of the mean-field dynamics, though its general behavior is extremely difficult to characterize due to the non-convexity of the loss functional.

Let us also make some comments on limitations and future directions. Firstly, there is still a gap between the necessary condition and the sufficient condition, which is basically from the fact that the sufficient condition is built on the Taylor's expansion of the flow (4.1). One future research question is whether we can fill the gap by considering the original flow (4.1) rather than its Taylor's expansion. Secondly, Algorithm 1 repeats training \(w\) for \(p\) times and takes the average of parameters, which is different from the usual strategy for training neural networks. This step is used to guarantee the algebraic independence. We conjecture that this step can be removed since the general algebraic independence is too strong when we have some preknowledge on the degree of \(f^{*}\) or \(h^{*}\), which deserves future research.