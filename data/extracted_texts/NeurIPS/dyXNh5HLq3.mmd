# Compositional Foundation Models for

Hierarchical Planning

 Anurag Ajay\({}^{}\)\({}^{@sectionsign}\), Seungwook Han \({}^{}\)\({}^{}\)\({}^{@sectionsign}\) Yilun Du \({}^{}\)\({}^{@sectionsign}\), Shuang Li \({}^{@sectionsign}\),

**Abhi Gupta \({}^{@sectionsign}\), Tommi Jaakkola \({}^{@sectionsign}\), Josh Tenenbaum \({}^{@sectionsign}\), Leslie Kaelbling \({}^{@sectionsign}\), Akash Srivastava \({}^{}\), Pulkit Agrawal \({}^{}\)\({}^{@sectionsign}\)**

Improbable AI Lab\({}^{}\) MIT-IBM Watson AI Lab\({}^{}\) Massachusetts Institute Technology\({}^{@sectionsign}\)

[https://hierarchical-planning-foundation-model.github.io/](https://hierarchical-planning-foundation-model.github.io/)

###### Abstract

To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose _Compositional Foundation Models for Hierarchical Planning_ (HiP), a foundation model which leverages multiple _expert_ foundation model trained on language, vision and action data _individually_ jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via _iterative refinement_. We illustrate the efficacy and adaptability of our approach in three different long-horizon table-top manipulation tasks.

## 1 Introduction

Consider the task of making a cup of tea in an unfamiliar house. To successfully execute this task, an effective approach is to reason hierarchically at multiple levels: an abstract level, _e.g._ the high level steps needed to heat up the tea, a concrete geometric level _e.g._, how we should physically navigate to and in the kitchen, and at a control level _e.g._ how we should actuate our joints to lift a cup. It is further important that reasoning at each level is self-consistent with each other - an abstract plan to look in cabinets for tea kettles must also be physically plausible at the geometric level and executable given the actuations we are capable of. In this paper, we explore how we can create agents capable of solving novel long-horizon tasks which require hierarchical reasoning.

Large "foundation models" have become a dominant paradigm in solving tasks in natural language processing , computer vision , and mathematical reasoning . In line with this paradigm, a question of broad interest is to develop a "foundation model" that can solve novel and long-horizon decision-making tasks. Some prior works  collected paired visual, language and action data and trained a monolithic neural network for solving long-horizon tasks. However, collecting paired visual, language and action data is expensive and hard to scale up. Another line of prior works  finetune large language models (LLM) on both visual and language inputs using task-specific robot demonstrations. This is problematic because, unlike the abundance of text on the Internet, paired vision and language robotics demonstrations are not readily available and are expensive to collect. Furthermore, finetuning high-performing language models, such as GPT3.5/4  and PaLM , is currently impossible, as the model weights are not open-sourced.

The key characteristic of the foundation model is that solving a new task or adapting to a new environment is possible with much less data compared to training from scratch for that task or domain. Instead of building a foundation model for long-term planning by collecting paired language-vision-action data, in this work we seek a scalable alternative - can we reduce the need for a costly and tedious process of collecting paired data across three modalities and yet be relatively efficient at solving new planning tasks? We propose _Compositional Foundation Models for Hierarchical Planning_ (HiP), a foundation model that is a composition of different _expert_ models trained on language, vision, and action data _individually_. Because these models are trained individually, the data requirements for constructing the foundation models are substantially reduced (Figure 1). Given an abstract language instruction describing the desired task, HiP uses a large language model to find a sequence of sub-tasks (i.e., planning). HiP then uses a large video diffusion model to capture geometric and physical information about the world and generates a more detailed plan in form of an observation-only trajectory. Finally, HiP uses a large pre-trained inverse model that maps a sequence of ego-centric images into actions. The compositional design choice for decision-making allows separate models to reason at different levels of the hierarchy, and jointly make expert decisions without the need for ever collecting expensive paired decision-making data across modalities.

Given three models trained independently, they can produce inconsistent outputs that can lead to overall planning failure. For instance, a naive approach for composing models is to take the maximum-likelihood output at each stage. However, a step of plan which is high likelihood under one model, _i.e._ looking for a tea kettle in a cabinet may have zero likelihood under a seperate model, _i.e._ if there is no cabinet in the house. It is instead important to sample a plan that jointly maximizes likelihood across every expert model. To create consistent plans across our disparate models, we propose an _iterative refinement_ mechanism to ensure consistency using feedback from the downstream models . At each step of the language model's generative process, intermediate feedback from a likelihood estimator conditioned on an image of the current state is incorporated into the output distribution. Similarly, at each step of the video model generation, intermediate feedback from the action model refines video generation. This _iterative refinement procedure_ promotes consensus among the different models and thereby enables hierarchically consistent plans that are both responsive to the goal and executable given the current state and agent. Our proposed iterative refinement approach is computationally efficient to train, as it does not require any large model finetuning. Furthermore, we do not require access to the model's weights and our approach works with any models that offer only input and output API access.

In summary, we propose a compositional foundation model for hierarchical planning that leverages a composition of foundation models, learned separately on different modalities of Internet and ego-centric robotics data, to construct long-horizon plans. We demonstrate promising results on three long-horizon tabletop manipulation environments.

## 2 Compositional Foundation Models for Hierarchical Planning

We propose HiP, a foundation model that decomposes the problem of generating action trajectories for long-horizon tasks specified by a language goal \(g\) into three levels of hierarchy: **(1)** Task planning - inferring a language subgoal \(w_{i}\) conditioned on observation \(x_{i,1}\) and language goal \(g\); **(2)** Visual

Figure 1: **Compositional Foundation Models for Hierarchical Planning. HiP uses a task model, represented using a LLM, to create an abstract plan, a visual model, represented using a video model, to generate an image trajectory plan, and an ego-centric action model to infer actions from the image trajectory.**

planning - generating a physically plausible plan as a sequence of image trajectories \(^{i}_{x}=\{x_{i,1:T}\}\), one for each given language subgoal \(w_{i}\) and observation at first timestep \(x_{i,1}\); **(3)** Action planning - inferring a sequence of action trajectories \(^{i}_{a}=\{a_{i,1:T-1}\}\) from the image trajectories \(^{i}_{x}\) executing the plan. Figure 2 illustrates the model architecture and a pseudocode is provided in Algorithm 1.

Let \(p_{}\) model this hierarchical decision-making process. Given our three levels of hierarchy, \(p_{}\) can be factorized into the following: task distribution \(p_{}\), visual distribution \(p_{}\), and action distribution \(p_{}\). The distribution over plans, conditioned on the goal and an image of the initial state, can be written under the Markov assumption as:

\[p_{}(W,\{^{i}_{x}\},\{^{i}_{a}\}|g,x_{1,1})=^{N}p_{}(w_{i}|g))}_{}^{N}p_{}(^{i}_{x}|w_{i},x_{i,1}))}_{}^{N}_{t=1}^{T-1}p_{}(a_{i,t}|x_{i,t},x_{i,t+1}) )}_{} \]

We seek to find action trajectories \(^{i}_{a}\), image trajectories \(^{i}_{x}\) and subgoals \(W=\{w_{i}\}\) which maximize the above likelihood. Please see Appendix A for a derivation of this factorization. In the following sub-sections, we describe the form of each of these components, how they are trained, and how they are used to infer a final plan for completing the long-horizon task.

### Task Planning via Large Language Models

Given a task specified in language \(g\) and the current observation \(x_{i,1}\), we use a pretrained LLM as the task planner, which decomposes the goal into a sequence of subgoals. The LLM aims to infer the next subgoal \(w_{i}\) given a goal \(g\) and models the distribution \(p_{}(w_{i}|g)\). As the language model is trained on a vast amount of data on the Internet, it captures powerful semantic priors on what steps should be taken to accomplish a particular task. To adapt the LLM to obtain a subgoal sequence relevant to our task, we prompt it with some examples of domain specific data consisting of high-level goals paired with desirable subgoal sequences.

However, directly sampling subgoals using a LLM can lead to samples that are inconsistent with the overall joint distribution in Eqn (1), as the subgoal \(w_{i}\) not only affects the marginal likelihood of task planning but also the downstream likelihoods of the visual planning model. Consider the example in Figure 2 where the agent is tasked with packing computer mouse, black and blue sneakers, pepsi box and toy train in brown box. Let's say the computer mouse is already in the brown box. While the subgoal of placing computer mouse in brown box has high-likelihood under task model \(p_{}(w_{i}|g)\), the resulting observation trajectory generated by visual model \(p_{}(^{i}_{x}|w_{i},x_{i,1})\) will have a low-likelihood under \(p_{}\) given the subgoal is already completed. Next, we describe how we use iterative refinement to capture this dependency between language decoding and visual planning to sample from Eqn (1).

Consistency with Visual PlanningTo ensure that we sample subgoal \(w_{i}\) that maximizes the joint distribution in Eqn (1), we should sample a subgoal that maximizes the following joint likelihood

\[w^{*}_{i}=*{arg\,max}_{w_{i}}p_{}(w_{i}|g)p_{}( ^{i}_{x}|w_{i},x_{i,1}), \]

i.e. a likelihood that maximizes both conditional subgoal generation likelihood from a LLM and the likelihood of sampled videos \(^{i}_{x}\) given the language instruction and current image \(x_{i,1}\). One way to determine the optimal subgoal \(w^{*}_{i}\) is to generate multiple \(w_{i}\) from LLM and score them using the likelihood of videos sampled from our video model \(p_{}(^{i}_{x}|w_{i},x_{i,1})\). However, the video generation process is computationally expensive, so we take a different approach.

Figure 2: **Planning with HiP. Given a language goal \(g\) and current observation \(x_{t}\), LLM generates next subgoal \(w\) with feedback from a visual plausibility model. Then, Diffusion uses observation \(x_{t}\) and subgoal \(w\) to generate observation trajectory \(_{x}\) with feedback from an action feasibility model. Finally, action planning uses inverse dynamics to generate action \(a_{t}\) from current \(x_{t}\) and generated observation \(}\) (action planning).**

The likelihood of video generation \(p_{}(_{x}^{i}|w_{i},x_{i,1})\) primarily corresponds to the feasibility of a language subgoal \(w_{i}\) with respect to the initial image \(x_{i,1}\). Thus an approximation of Eqn (2) is to directly optimize the conditional density

\[w_{i}^{*}=*{arg\,max}_{w_{i}}p(w_{i}|g,x_{i,1}). \]

We can rewrite Eqn (3) as

\[w_{i}^{*}=*{arg\,max}_{w_{i}} p_{}(w_{i}|g)+ (|w_{i},g)}{p(x_{i,1}|g)})\]

We estimate the density ratio \(|w_{i};g)}{p(x_{i,1}|g)}\) with a multi-class classifier \(f_{}(x_{i,1},\{w_{j}\}_{i=1}^{M},g)\) that chooses the appropriate subgoal \(w_{i}^{*}\) from candidate subgoals \(\{w_{j}\}_{j=1}^{M}\) generated by the LLM. The classifier implicitly estimates the relative log likelihood estimate of \(p(x_{i,1}|w_{i},g)\) and use these logits to estimate the log density ratio with respect to each of the \(M\) subgoals and find \(w_{i}^{*}\) that maximizes the estimate . We use a dataset \(_{}\{x_{i,1},g,\{w_{j}\}_{j=1}^{M},i\}\) consisting of observation \(x_{i,1}\), goal \(g\), candidate subgoals \(\{w_{j}\}_{j=1}^{M}\) and the correct subgoal label \(i\) to train \(f_{}\). For further architectural details, please refer to Appendix B.1.

### Visual Planning with Video Generation

Upon obtaining a language subgoal \(w_{i}\) from task planning, our visual planner generates a plausible observation trajectory \(_{x}^{i}\) conditioned on current observation \(x_{i,1}\) and subgoal \(w_{i}\). We use a video diffusion model for visual planning given its success in generating text-conditioned videos . To provide our video diffusion model with a rich prior for physically plausible motions, we pretrain it \(p_{}(_{x}^{i}|w_{i},x_{i,1})\) on a large-scale text-to-video dataset Ego4D . We then finetune it on the task-specific video dataset \(_{}\{_{x}^{i},w_{i}\}\) consisting of observation trajectories \(_{x}^{i}\) satisfying subgoal \(w_{i}\). For further architectural details, please refer to Appendix B.2.

However, analogous to the consistent sampling problem in task planning, directly sampling observation trajectories with video diffusion can lead to samples that are inconsistent with the overall joint distribution in Eqn (1). The observation trajectory \(_{x}^{i}\) not only affects the marginal likelihood of visual planning, but also the downstream likelihood of the action planning model.

Consistency with Action PlanningTo ensure observation trajectories \(_{x}^{i}\) that correctly maximize the joint distribution in Eqn (1), we optimize an observation trajectory that maximizes the following joint likelihood

\[(_{x}^{i})^{*}=*{arg\,max}_{_{x}^{i}}p_{}(_{x}^{i }|w_{i},x_{i,1})_{t=1}^{T-1}p_{}(a_{i,t}|x_{i,t},x_{i,t+1}), \]i.e. an image sequence that maximizes both conditional observation trajectory likelihood from video diffusion and the likelihood of sampled actions \(_{a}^{i}\) given the observation trajectory \(_{x}^{i}\).

To sample such an observation trajectory, we could iteratively bias the denoising of video diffusion using the log-likelihood of the sampled actions \(_{t=1}^{T-1} p_{}(a_{i,t}|x_{i,t},x_{i,t+1})\). While this solution is principled, it is slow as it requires sampling of entire action trajectories and calculating the corresponding likelihoods during every step of the denoising process. Thus, we approximate the sampling and the likelihood calculation of action trajectory \(_{t=1}^{T-1}p_{}(a_{i,t}|x_{i,t},x_{i,t+1})\) with a binary classifier \(g_{}(_{x}^{i})\) that models if the observation trajectory \(_{x}^{i}\) leads to a high-likelihood action trajectory.

We learn a binary classifier \(g_{}\) to assign high likelihood to feasible trajectories sampled from our video dataset \(_{x}^{i}_{}\) and low likelihood to infeasible trajectories generated by randomly shuffling the order of consecutive frames in feasible trajectories. Once trained, we can use the likelihood \( g_{}(1|_{x}^{i})\) to bias the denoising of the video diffusion and maximize the likelihood of the ensuing action trajectory. For further details on binary classifier, please refer to Appendix C.2.

### Action Planning with Inverse Dynamics

After generating an observation trajectory \(_{x}^{i}\) from visual planning, our action planner generates an action trajectory \(_{a}^{i}\) from the observation trajectory. We leverage egocentric internet images for providing our action planner with useful visual priors. Our action planner is parameterized as an inverse dynamics model  that infers the action \(a_{i,t}\) given the observation pair \((x_{i,t},x_{i,t+1})\):

\[a_{i,t} p_{}(a_{i,t}|x_{i,t},x_{i,t+1})\]

TrainingTo imbue the inverse dynamics \(p_{}\) with useful visual priors, we initialize it with VC-1  weights, pretrained on ego-centric images and ImageNet. We then finetune it on dataset \(_{}\{_{x}^{i},_{a}^{i}\}\) consisting of paired observation and action trajectories by optimizing:

\[_{}_{_{}}[ p_{}(a_{ i,t}|x_{i,t},x_{i,t+1})]\]

For further architectural details, please refer to Appendix B.3.

## 3 Experimental Evaluations

We evaluate the ability of HiP to solve long-horizon planning tasks that are drawn from distributions with substantial variation, including the number and types of objects and their arrangements. We then study the effects of iterative refinement and of pretraining on overall performance of HiP. We also compare against an alternative strategy of visually grounding the LLM without any task-specific data. In addition, we study how granularity of subgoals affects HiP's performance, ablate over choice of visual planning model and analyze sensitivity of iterative refinement to hyperparameters (Appendix E).

### Evaluation Environments

We evaluate HiP on three environments, paint-block, object-arrange, and kitchen-tasks which are inspired by combinatorial planning tasks in Mao et al. , Shridhar et al.  and Xing et al.  respectively.

* paint-block: A robot has to manipulate blocks in the environment to satisfy language goal instructions, such as _stack pink block on yellow block and place green block right of them_. However, objects of correct colors may not be present in the environment, in which case, the robot needs to first pick up white blocks and put them in the appropriate color bowls to paint them. After that, it should perform appropriate pick-and-place operations to stack a pink block on the yellow block and place the green block right of them. A new task \(\) is generated by randomly selecting \(3\) final colors (out of \(10\) possible colors) for the blocks and then sampling a relation (out of \(3\) possible relations) for each pair of blocks. The precise locations of individual blocks, bowls, and boxes are fully randomized across different tasks. Tasks have \(4 6\) subgoals.
* object-arrange: A robot has to place appropriate objects in the brown box to satisfy language goal instructions such as _place shoe, tablet, alarm clock, and scissor in brown box_. However, the environment may have distractor objects. Furthermore, some objects can be dirty, indicated by a lack of texture and yellow color. For these objects, the robot must first place them in a blue cleaning box and only afterwards place those objects in the brown box. A new task \(\) is generatedby randomly selecting \(7\) objects (out of \(55\) possible objects), out of which \(3\) are distractors, and then randomly making one non-distractor object dirty. The precise locations of individual objects and boxes are fully randomized across different tasks. Tasks usually have \(3 5\) subgoals.
* kitchen-tasks: A robot has to complete kitchen subtasks to satisfy language goal instructions such as _open microwave, move kettle out of the way, light the kitchen area, and open upper right drawer_. However, the environment may have objects irrelevant to the subtasks that the robot must ignore. Furthermore, some kitchen subtasks specified in the language goal might already be completed, and the robot should ignore those tasks. There are \(7\) possible kitchen subtasks: opening the microwave, moving the kettle, switching on lights, turning on the bottom knob, turning on the top knob, opening the left drawer, and opening the right drawer. A new task \(\) is generated by randomly selecting \(4\) out of \(7\) possible kitchen subtasks, randomly selecting an instance of microwave out of \(3\) possible instances, randomly selecting an instance of kettle out of \(4\) possible instances, randomly and independently selecting texture of counter, floor and drawer out of \(3\) possible textures and randomizing initial pose of kettle and microwave. With \(50\%\) probability, one of \(4\) selected kitchen subtask is completed before the start of the task. Hence, tasks usually have \(3 4\) subtasks (i.e. subgoals).

Train and Test TasksFor all environments, we sample two sets of tasks \(_{train},_{test} p()\). We use the train set of tasks \(_{train}\) to create datasets \(_{}\), \(_{}\), \(_{}\) and other datasets required for training baselines. We ensure the test set of tasks \(_{test}\) contains novel combinations of object colors in paint-block, novel combinations of object categories in object-arrange, and novel combinations of kitchen subtasks in kitchen-tasks.

Evaluation MetricsWe quantitatively evaluate a model by measuring its task completion rate for paint-block and object-arrange, and subtask completion rate for kitchen tasks. We use the simulator to determine if the goal, corresponding to a task, has been achieved. We evaluate a model on \(_{train}\) (seen) to test its ability to solve long-horizon tasks and on \(_{test}\) (unseen) to test its ability to generalize to long-horizon tasks consisting of novel combinations of object colors in paint-block,

Figure 3: **Example Executions. Example long-horizon generated plans on tasks in paint-block, object-arrange, and kitchen-tasks domains.**

object categories in object-arrange, and kitchen subtasks in kitchen-tasks. We sample \(1000\) tasks from \(_{train}\) and \(_{test}\) respectively, and obtain average task completion rate on paint-block and object-arrange domains and average subtask completion rate on kitchen tasks domain. We report the mean and the standard error over \(4\) seeds in Table 1.

### Baselines

There are several existing strategies for constructing robot manipulation policies conditioned on language goals, which we use as baselines in our experiments:

* **Goal-Conditioned Policy** A goal-conditioned transformer \(a_{i,t} p(a_{i,t}|x_{i,t},w_{i})\) that outputs action \(a_{i,t}\) given a language subgoal \(w_{i}\) and current observation \(x_{i,t}\) (Transformer BC) . We provide the model with oracle subgoals and encode these subgoals with a pretrained language encoder (Flan-T5-Base). We also compare against goal-conditioned policy with Gato  transformer.
* **Video Planner** A video diffusion model (UniPi) \(\{^{i}_{x}\} p(\{^{i}_{x}\}|g,x_{i,1}),a_{i,t} p(a_{i,t}|x_{i, t},x_{i,t+1})\) that bypasses task planning, generates video plans for the entire task \(\{^{i}_{x}\}\), and infers actions \(a_{i,t}\) using an inverse model.
* **Action Planners** Transformer models (Trajectory Transformer)  and diffusion models (Diffuser) [24; 4]\(\{a_{i,t:T-1}\} p(\{a_{i,t:T-1}\}|x_{i,t},w_{i})\) that produce an action sequence \(\{a_{i,t:T-1}\}\) given a language subgoal \(w_{i}\) and current visual observation \(x_{i,t}\). We again provide the agents with oracle subgoals and encode these subgoals with a pretrained language encoder (Flan-T5-Base).
* **LLM as Skill Manager** A hierarchical system (SayCan)  with LLM as high level policy that sequences skills sampled from a repetoire of skills to accomplish a long-horizon task. We use CLIPort  policies as skills and the unnormalized logits over the pixel space it produces as affordances. These affordances grounds the LLM to current observation for producing next subgoal.

### Results

We begin by comparing the performance of HiP and baselines to solve long-horizon tasks in paint-block, object-arrange, kitchen-tasks environments. Table 1 shows that HiP significantly outperforms the baselines, although the baselines have an advantage and have access to oracle subgoals. HiP's superior performance shows the importance of (i) hierarchy given it outperforms goal-conditioned policy (Transformer BC and Gato), (ii) task planning since it outperforms video planners (UniPi), and (iii) visual planning given it outperforms action planners (Trajectory Transformer, Action Diffuser). It also shows the importance of representing skills with video-based planners which can be pre-trained on Internet videos and can be applied to tasks (such as kitchen-tasks). SayCan, in contrast, requires tasks to be decomposed into primitives paired with an affordance function, which can be difficult to define for many tasks like the kitchen task. Thus, we couldn't run SayCan on kitchen-tasks environment. Finally, to quantitatively show how the errors in \(f_{}(x_{i,1},w_{i},g)\) affect the performance of HiP, we compare it to HiP with oracle subgoals. For further details on the training and evaluation of HiP, please refer to Appendix C. For implementation details on Gato and SayCan, please refer to Appendix D. For runtime analysis of HiP, please refer to Appendix F.

Combinatorial Generalization to Unseen Long-horizon TasksWe also quantitatively test the ability of HiP to generalize to unseen long-horizon tasks, consisting of novel combinations of object colors in paint-block, object categories in object-arrange, and subtasks in kitchen-tasks. Table 1 shows that HiP's performance remains intact when solving unseen long-horizon tasks, and

still significantly outperforms the baselines. Figure 4 visualizes the execution of HiP in unseen long-horizon tasks in paint-block.

Pre-training Video Diffusion ModelWe investigate how much our video diffusion model benefits from pre-training on the Internet-scale data. We report both the success rate of HiP and Frechet Video Distance (FVD) score that quantifies the similarity between generated videos and ground truth videos, where lower scores indicate greater similarity in Figure 5. We see that pretraining video diffusion leads to a higher success rate and lower FVD score. If we reduce the training dataset to \(75\%\) and \(50\%\) of the original dataset, the FVD score for video diffusion models (both, with and without Ego4D dataset pretraining) increases and their success rate falls. However, the video diffusion model with Ego4D dataset pretraining consistently gets higher success rate and lower FVD scores across different dataset sizes. As we decrease the domain-specific training data, it is evident that the gap in performance between the model with and without the Ego4D pre-training widens. For details on how we process the Ego4D dataset, please refer to Appendix C.2.

Pre-training Inverse Dynamics ModelWe also analyze the benefit of pre-training our inverse dynamics model and report the mean squared error between the predicted and ground truth actions in Figure 6. The pre-training comes in the form of initializing the inverse dynamics model with

Figure 4: **Execution trajectory of HiP on an novel long-horizon task in paint-block environment.**

Figure 5: **Pretraining video diffusion model with the Ego4D dataset consistently yields higher success rate and lower FVD scores (lower is better), even with reduced training dataset sizes. With pretraining, the model’s FVD score escalates less gradually and its success rate falls less steeply as the dataset size shrinks.**

weights from VC-1 , a vision-transformer (ViT-B)  trained on ego-centric images with masked-autoencoding objective . In paint-block and object-arrange, we see that the inverse dynamics, when initialized with weights from VC-1, only requires 1K labeled robotic trajectories to achieve the same performance as the inverse dynamics model trained on 10K labeled robotic trajectories but without VC-1 initialization. We also compare against an inverse dynamics model parameterized with a smaller network (ResNet-18). However, the resulting inverse dynamics model still requires 2.5K robotic trajectories to get close to the performance of the inverse dynamics model with VC-1 initialization in paint-block and object-arrange. In kitchen-tasks, inverse dynamics, when initialized with weights from VC-1, only requires 3.5k labeled robotic trajectories to achieve the same performance as the inverse dynamics model trained on 10K labeled robotic trajectories but without VC-1 initialization. When parameterized with ResNet-18, the inverse dynamics model still requires 6k robotic trajectories to get close to the performance of the inverse dynamics model with VC-1 initialization.

Importance of Task Plan and Visual Plan RefinementsWe study the importance of refinement in task and visual planning in Figure 7. We compare to HiP without visual plan refinement and HiP without visual and task plan refinement in paint block environment. We see that task plan refinement for visual grounding of LLM is critical to the performance of HiP. Without it, the task plan is agnostic to the robot's observation and predicts subgoals that lead to erroneous visual and action planning. Furthermore, visual plan refinement improves the performance of HiP as well, albeit by a small margin. For a description of the hyperparameters used, please refer to Appendix C.4.

Exploring Alternate Strategies for Visually Grounding LLMWe use a learned classifier \(f_{}(x_{i,1},w_{i},g)\) to visually ground the LLM. We explore if we can use a frozen pretrained Vision-Language Model (MiniGPT-4 ) as a classifier in place of the learned classifier. Although we didn't use any training data, we found the prompt engineering using the domain knowledge of the task to be essential in using the Vision-Language Model (VLM) as a classifier (see Appendix C.1 for details). We use subgoal prediction accuracy to quantify the performance of the learned classifier and the frozen VLM. Figure 7 illustrates that while both our learned multi-class classifier and frozen VLM perform comparably in the paint-block environment, the classifier significantly outperforms the VLM in the more visually complex object-arrange environment. We detail the two common failure modes of the VLM approach in object-arrange environment in Appendix C.1. As VLMs continue to improve, it is possible that their future versions match the performance of

Figure 6: **Pretraining inverse dynamics model. In paint-block and object-arrange (kitchen-tasks), when initialized with VC-1 weights, inverse dynamics model matches the performance of a randomly initialized model trained on 10K trajectories with just 1K (3.5k) trajectories. A smaller ResNet-18 model requires 2.5K (6k) trajectories to approach the same performance. The yellow and brown lines are overlaid on top of each other.**

Figure 7: **Ablation Studies. (Left) While task plan refinement is critical to HiP’s performance, visual plan refinement improves HiP’s performance by a smaller margin in paint block environment. (Right) While frozen pretrained VLM (MiniGPT4) matches the performance of a learned classifier in paint-block environment, its performance deteriorates in a more visually complex object-arrange environment.**

learned classifiers and thus replace them in visually complex domains as well. For further details on the VLM parameterization, please refer to Appendix C.1.

## 4 Related Work

The field of foundation models for decision-making  has seen significant progress in recent years. A large body of work explored using large language models as zero-shot planners , but it is often difficult to directly ground the language model on vision. To address this problem of visually grounding the language model, other works have proposed to directly fine-tune large language models for embodied tasks . However, such an approach requires large paired vision and language datasets that are difficult to acquire. Most similar to our work, SayCan  uses an LLM to hierarchically execute different tasks by breaking language goals into a sequence of instructions, which are then inputted to skill-based value functions. While SayCan assumes this fixed set of skill-based value functions, our skills are represented as video-based planners , enabling generalization to new skills.

Another set of work has explored how to construct continuous space planners with diffusion models . Existing works typically assume task-specific datasets from which the continuous-space planner is derived . Most similar to our work, UniPi  proposes to use videos to plan in image space and similarily relies on internet videos to train image space planners. We build on top of UniPi to construct our foundation model for hierarchical planning, and illustrate how UniPi may be combined with LLMs to construct longer horizon continuous video plans.

Moreover, some works  explored how different foundation models may be integrated with each other. In Flamingo , models are combined through joint finetuning with paired datasets, which are difficult to collect. In contrast both Zeng et al.  and Li et al.  combine different models zero-shot using either language or iterative consensus. Our work proposes to combine language, video, and ego-centric action models together by taking the product of their learned distributions . We use a similar iterative consensus procedure as in Li et al.  to sample from the entire joint distribution and use this combined distribution to construct a hierarchical planning system.

## 5 Limitations and Conclusion

LimitationsOur approach has several limitations. As high-quality foundation models for visual sequence prediction and robot action generation do not exist yet, our approach relies on smaller-scale models that we directly train. Once high-quality video foundation models are available, we can use them to guide our smaller-scale video models  which would reduce the data requirements of our smaller-scale video models. Furthermore, our method uses approximations to sample from the joint distribution between all the model. An interesting avenue for future work is to explore more efficient and accurate methods to ensure consistent samples from the joint distribution.

ConclusionIn this paper, we have presented an approach to combine many different foundation models into a consistent hierarchical system for solving long-horizon robotics problems. Currently, large pretrained models are readily available in the language domain only. Ideally, one would train a foundation model for videos and ego-centric actions, which we believe will be available in the near future. However, our paper focuses on leveraging separate foundation models trained on different modalities of internet data, instead of training a single big foundation model for decision making. Hence, for the purposes of this paper, given our computational resource limitations, we demonstrate our general strategy with smaller-scale video and ego-centric action models trained in simulation, which serve as proxies for larger pretrained models. We show the potential of this approach in solving three long-horizon robot manipulation problem domains. Across environments with novel compositions of states and goals, our method significantly outperforms the state-of-the-art approaches towards solving these tasks.

In addition to building larger, more general-purposed visual sequence and robot control models, our work suggests the possibility of further using other pretrained models in other modalities, such as touch and sound, which may be jointly combined and used by our sampling approach. Overall, our work paints a direction towards decision making by leveraging many different powerful pretrained models in combination with a tiny bit of training data. We believe that such a system will be substantially cheaper to train and will ultimately result in more capable and general-purpose decision making systems.

## Author Contributions

**Anurag Ajay** co-conceived the framework of leveraging pretrained foundation models for decision making, implemented visual planning and action planning in HiP, evaluated HiP on long-horizon tasks, performed ablation studies and helped in paper writing.

**Seungwook Han** co-conceived in conceiving the framework of leveraging pretrained foundation models for decision making, implemented task planning in HiP and helped in paper writing.

**Yilun Du** co-conceived the framework of leveraging pretrained foundation models for decision making, implemented trajectory transformer and transformer BC and evaluated them on long-horizon tasks, implemented data generation scripts for object arrange and paint block environment, and lead paper writing.

**Shuang Li** helped in conceiving the idea of iterative refinement for consistency between pretrained foundation models, participated in research discussions and helped in writing paper.

**Abhi Gupta** participated in research discussions and helped in making figures.

**Tommi Jaakkola** participated in research discussions.

**Joshua Tenenbaum** participated in research discussions.

**Leslie Kaelbling** participated in research discussions, suggested baselines and ablation studies, conceived the structure of the paper and helped in paper writing.

**Akash Srivastava** participated in research discussions, suggested the idea of using classifier for consistency between observations and the large language model and provided feedback on paper writing.

**Pulkit Agrawal** was involved in research discussions, suggested ablation studies related to iterative refinement, provided feedback on writing, positioning of the work and overall advising.