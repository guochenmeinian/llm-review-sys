# Katerina Margatina\({}^{4}\)2 Juan Ciro\({}^{5,11}\) Rafael Mosquera\({}^{5,6}\) Max Bartolo\({}^{7,8}\)

The Prism Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models

 Hannah Rose Kirk\({}^{1}\)1 Alexander Whitefield\({}^{2}\) Paul Rottger\({}^{3}\) Andrew Bean\({}^{1}\)**Adina Williams\({}^{9}\) He He\({}^{10}\) Bertie Vidgen\({}^{1,11}\) Scott A. Hale\({}^{1,12}\)\({}^{1}\)**University of Oxford \({}^{2}\)University of Pennsylvania \({}^{3}\)Bocconi University

\({}^{4}\)AWS AI Labs \({}^{5}\)ML Commons \({}^{6}\)Factored AI \({}^{7}\)UCL \({}^{8}\)Cohere

\({}^{9}\)MetaAI \({}^{10}\)New York University \({}^{11}\)Contextual AI \({}^{12}\)Meedan

###### Abstract

Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about methods (_how_), domains (_where_), people (_who_) and objectives (_to what end_) of feedback processes. To navigate these questions, we introduce Prism, a dataset that maps the sociodemographic and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With Prism, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use Prism in three case studies to demonstrate the need for careful consideration of which humans provide what alignment data.

**Data & Code:** github.com/HannahKirk/prism-alignment

**Data & Dataset Card:** huggingface.co/datasets/HannahRoseKirk/prism-alignment

## 1 Introduction

Human feedback serves a direct role for the _alignment_ of large language models (LLMs), defined as the steering of AI behaviour towards a set of preferences or values. This increased emphasis on human feedback raises unresolved questions: _how we collect human feedback_ when designing methodologies that rely on ordinal or cardinal scales, broad or fine-grained desiderata, and explicit or implicit signals; _where we focus human labour_ when selecting domains, topics or tasks to collect feedback over; _who we ask for feedback_ when recruiting participants to voice their idiosyncratic preferences, values, or beliefs ; and _to what end_ when specifying an objective to pursue personalised alignment  or to aggregate individual preferences into collective outcomes favourable for societies at large .

Despite the success of human feedback learning , answering these questions is constrained by gaps in existing datasets, such as (i) over-reliance on binary A/B comparisons, without fine-grained ratings or explanations ; (ii) small or biased samples recruited from narrow crowdwork or tech communities  (iii) limited sample information (annotator IDs or sociodemographics) ; and (iv) scarce documentation for how values are operationalised . Most datasets rely only on revealed or contextual preferences ,2 and much attention is devoted to technical or statistical issues in feedback learning , rather than data-centric human factors. Relying on 'generic' human data teaches behaviours which are _reductionist_ because values are relational and non-separable from the person, community or operating context ; and _non-generalisable_ because the indiscriminate aggregation of data subsumes hidden annotator contexts as universalities .

We introduce Prism, a new resource for navigating empirical questions of human feedback. We employ both the _ask_ and _observe_ principles of social science by mapping detailed survey responses of humans around the world onto their live conversations with LLMs (Fig. 1). This setup permits alignment methods relying on either contextual preference comparisons typical for RLHF , or stated preferences and principles like constitutional AI . In addition to pairing stated and contextual preferences, Prism has the following features. **Participatory**: To ensure wider active participation in alignment data , we recruit 1,500 English-speaking crowdworkers from diverse geographies and demographics; **Representative**: As units for preference aggregation, we include two census-representative samples (UK, US); and **Individualised**: To expose hidden human context and permit personalised preferences, each rating links to a pseudonymous ID and detailed participant profile. We source **Subjective** and **Multicultural** perspectives to avoid value-monism and cultural homogenisation in the opinions that LLMs represent  and operate in the descriptive paradigm without guidelines that characterise 'good' responses . Opinion diversity varies along the objective-subjective spectrum (e.g. _what is the capital of France? vs. is abortion wrong?_), so we prime participants for values and controversy guided dialogues but also collect neutral unguided dialogues as a baseline. To our knowledge, Prism is the first human feedback dataset to target cross-cultural controversies and value-laden prompts, where interpersonal disagreement is rife. After introducing Prism (SS 2), we demonstrate its value via three case studies (SS 3): (1) _Do different people initiate different discussions with LLMs?_ (2) _Do people prefer differently aligned models_, and (3) _How do sampling decisions affect welfare outcomes?_ Prism provides many more research avenues such as engineers targeting personalised alignment  or consensus across opinion distributions ; social scientists examining how exposure to LLMs affects public attitudes; or policymakers seeking democratic input on AI-citizen interactions on topics like immigration, abortion or euthanasia. Alignment cannot be neatly bifurcated into technical and normative components . Prism assists in navigating these complexities with more human voices adjudicating alignment norms.

Figure 1: **The Prism dataset. In Stage 1, 1,500 participants fill in the Survey detailing their background, familiarity with LLMs and stated preferences over behaviours (SS 2.1). Demographic and geographic breakdowns are in Tab. 5 and Tab. 8). Participants then progress to Stage 2, where they converse with LLMs on topics of their choosing, rate the responses on a cardinal scale, and give fine-grained feedback (SS 2.2). In the first turn, four models respond to the opening prompt (ยง 2.1, 2.2, 2.2). In subsequent turns, the conversation continues with two responses sampled from the highest-rated model at a non-deterministic temperature (ยง 2). There are 8,011 **Conversations** between participants (ยง 2) and LLMs (ยง 2), forming 27,172 **Interactions** (human message with a set of model responses), and 68,371 **Utterances** (triples of {human message, model response, score}).

## 2 The Prism Alignment Dataset

Prism maps the characteristics and preferences of diverse humans onto their real-time interactions with LLMs (Fig. 1). Participants complete a **Survey** (SS 2.1) with questions about their demographics and stated preferences, then proceed to the **Conversations** with LLMs (SS 2.2), where they input prompts, rate responses and give fine-grained feedback in a series of multi-turn interactions. With the two-stage setup: (i) we avoid over-generalising from a "generic human" by matching ratings to detailed participant characteristics; (ii) we track how contextual preferences (in local conversations) depart from stated preferences (in survey); and (iii) we give participants autonomy to communicate in their own words what is important and why [39; 25]. Both stages received ethics board approval and ran with informed consent (App. D). Participants were paid PS9/hour and the task took 70 minutes on average. Data collection ran from 22nd November to 22nd December 2023.3We provide a data statement in App. B, data clause in App. C, and full codebooks detailing each variable in App. V.

### The Survey

Prior to starting the survey, we ensure that all participants are over 18, obtain their informed consent, give a brief primer on LLMs (or AI language models), and dissuade LLM-written responses. The survey constructs a participant profile containing five features:

**LLM familiarity and usage** We ask about participants' familiarity with LLMs (61% are somewhat familiar, 28% very familiar and 10% not familiar at all) and whether to their knowledge they have used them _indirectly_ (in products like LinkedIn post-writing tool); or _directly_ (via a specialised interface like ChatGPT). Individuals that have used LLMs directly or indirectly (84%) are branched to questions on frequency of use (7% every day, 21% every week, and 20% every month) and purpose of use (the most popular tasks are research overviews selected by 49%, professional work by 37%, creative writing by 31% and programming help by 27%). Full results in App. I.

**Self-written system string ("constitution")** System strings can guide LLM behaviours as a high-level global instruction prompts prepended to all subsequent interactions [40; 41], and have been analogised as "constitutions" or governing principles for AI . Factuality, professionalism, humanness and harmlessness all emerged as key principles (App. M.1) from the following instruction:

Figure 2: **Schematic of fine-grained attribute ratings. The same attributes appear in three places in our task: A is asked once in the survey; B and C are asked per conversation. For _performance attributes_, we ask participants to consider only the highest-rated model in the first conversation turn; for _choice attributes_, we ask them to consider this highest-rated model relative to other models in the first turn.**

**Stated preferences for LLM behaviours** In contrast to this open-ended preference elicitation, we collect structured ratings on fine-grained behaviour attributes. Participants score the importance of each attribute on a visual analog scale  (Fig. 2). A statement like "_It is important that an AI language model produces factual and informative responses_" maps (0,100) where the ends of scale are (_Strongly disagree, Strongly agree_). Numeric scores are recorded, but not shown to participants to avoid anchoring and dependency biases. We only collect responses to these statements once _before_ participants interact with LLMs but the same attributes appear in the Conversations stage; so, we can track how stated 'abstract' preferences relate to contextual 'in-situ' preferences.4 Overall, we find clusters of subjective attributes (values, creativity and diversity) versus objective attributes (factuality, fluency and helpfulness; App. N.1). While the majority of participants agree that these more objective attributes are important (highly-skewed positive distribution, \(,\)), there is little agreement on the meta-importance of subjective attributes (App. N.2). In fact, responses for whether value alignment itself is important follow an almost normal distribution (\(=54,=26\)).

**Self-written description** Values and preferences are subjective and personal. We ascribe participants autonomy to communicate salient aspects of their identity in a short profile, beyond essentialising associations with structured demographics alone. Honesty, hard work and empathy emerged as common values (App. M.2) from the following instruction:

_Please briefly describe your values, core beliefs, guiding principles in life, or other things that are important to you. For example, you might include values youโd want to teach to your children or qualities you look for in friends. There are no right or wrong answers. Please do not provide any personally identifiable details like your name, address or email._

**Basic demographics** We ask standard demographics: age, gender, employment status, martial status, educational attainment, ethnicity, religious affiliation, English proficiency, country of birth, and country of residence. There is always a "_Prefer not to say_" option. For gender, participants can select _Male_, _Female_, _Non-Binary_, or self-describe. We collect self-described ethnicity and religion because no pre-set groups exhaust how individuals may self-identify across cultures and global regions. We provide a manual annotation of these strings into aggregated categorisations for statistical analysis (App. F). Because of how we recruit participants (SS 2.3), our sample covers diverse demographics (App. G) and geographies (App. H), with representation from people born in 75 countries. However, the sample still skews White, Western and educated, and only contains English-language speakers.

### The Conversations

After completing the survey, participants move to the second stage, consisting of real-time conversations with LLMs via a custom-built interface on the Dynabench platform .

**Selecting conversation type** We prime participants to diversify their prompts along the objective-subjective spectrum by asking them to complete two conversations across three conditions or _conservation types_ (six in total).5 They select the _type_ before inputting their opening prompt:

**Opening the conversation** Participants construct a free-text prompt of their choosing and receive up to four responses from different LLMs.6 The participants then rate each response on a visual analogue scale (VAS)  from "Terrible" to "Perfect". We record the slider position as a score from 1-100 but do not show participants the number to avoid anchoring or conditional dependence of scores across conversations. We opt for this cardinal feedback for three reasons: (i) it encourages subjectivity; (ii) it permits studying the relative merit of cardinality versus ordinality for rewardmodelling because ratings can be converted to rankings but not vice versa; (iii) it allows expression of preference intensity above and beyond chosen:rejected pairs.7 However, we acknowledge that the cardinal scale introduces some intrapersonal measurement noise from a more cognitively demanding task and carries less interpersonal comparability than ordinal preferences, see Limitations (SS 5).

**Continuing the conversation** The highest-scoring LLM from the opening turn is locked into subsequent turns, with random tie-breaks in the case of identical scores. Participants must continue the conversation for at least another turn, but are asked to vary their conversations between 2 and 10 turns to avoid introducing a dataset artefact. We encourage some variation in conversation length (\(_{T}=3.4,_{T}=1.6\)) but there is a strong drop off after the second turn (App. O). Participants then rate two responses on a VAS like before, but both are now sampled from the selected model with a non-deterministic temperature. These within-model responses are more similar in style and content than across-model responses (in the first turn), and score deviations are narrower (App. O).

**Collecting fine-grained feedback** After the conversation ends, participants first rate statements about the _performance of their highest-rated model_ like "The response was well-written" on a VAS from _Performed very poorly_ to _Performed very well_, or select N/A if the statement is irrelevant for the context. We then ask participants to consider _why they chose this model_, rating statements like "I chose this response _because_ it was well-written" on a VAS from _Very unimportant_ to _Very important_ (or select N/A). Attributes are shared with the Survey (Fig. 2). We find strong correlations between performance attributes and choice attributes (except safety) but weak correlations of these pairs to stated preferences given in the Survey, perhaps due to conversational, model or task-design confounders (App. N.1). In general, the distribution of scores over performance and choice attributes is narrower and more positively skewed (bunched to 100) compared to stated preferences (App. N.2). Finally, we collect open-ended natural language feedback on the _whole_ conversation. Participants contributed both content and stylistic feedback (\(=29\) words, \(=19\), App. M.3).

_Give some feedback on the conversation as whole. Hypothetically, what would an ideal interaction for you look like here? What was good and what was bad? What (if anything) was missing? What would you change to make it better?_

### The Sample

Our sampling aims were _depth_ in the demographics represented within countries and _breadth_ across global regions. We recruit English-speaking participants from Prolific in two distinct paths:

**Census-representative sample (UK, US)** Samples matched to simplified census data (age, ethnicity, gender) were only available for the UK and US. The minimum pool size for a statistical guarantee of representativeness was 300, which set a lower bound for participant quota. After collecting data, we observed some skew in our'representative' samples between observed and expected distributions in recent census data, which we partially correct for (App. L). These samples permit future studies on more representative populations that can be replicated across two countries; however their inclusion biases Prism as a whole towards two Western nations already over-represented in AI research.

**Balanced samples (rest of world)** The distribution of Prolific workers outside the US and the UK skews strongly to Europe and Northern America, and some countries dominate continental counts (App. J). To avoid more active workforces biasing the sample, we set up 33 country-specific studies where there is \(>1\) eligible worker, and allocate sample quotas so that each global region is similarly represented.8 We balance each national sample by gender where possible (Tab. 10).

**Included models** The rapidly evolving landscape necessitates a model-agnostic approach to avoid data staleness. We include 21 different LLMs (9 open-access, 12 commercial-API) from various model families and parameter sizes, which diversifies the training data, capabilities, and degree of existing safeguards or alignment biases. To avoid text length confounding preferences  and to reduce participant fatigue, we include system prompts instructing models to limit their responses to \( 50\) words. We show the full list of models, decoding parameters and generation details in App. P.

## 3 Experiments with Prism

### Case Study I: Do Different People Initiate Different Discussions with LLMs?

**Methods** We use a pre-trained sentence transformer (all-mpnet-base-v2) to embed each opening prompt in 768-D, then apply UMAP to reduce to 20-D, before clustering with HDBScan . 70% of prompts are assigned to 22 topic clusters and 30% remain as outliers. We name each cluster by prompting gpt-4-turbo with the top n-grams extracted with TF-IDF and closest texts to the cluster centroid. We define an _over-representation factor_ as \(/N_{t}}{b_{g}}\), to compute observed versus expected topic prevalence per identity group. For the partial contribution of identity attributes, we estimate an OLS regression for each topic \(y^{t}\) (\(t 1 22\)) and cluster standard errors at the individual level: \(y^{j}_{i,c}=^{t}+^{}_{i}^{t}_{1}+^{ }_{i}^{t}_{2}+^{}_{i}^{t}_{3}+ {ethnicity}^{}_{i}^{t}_{4}+^{}_{i}^{t}_{5} +^{}_{i}^{t}_{6}+_{i,c}\), where \(y^{t}_{i,c}=1\) if the prompt of participant \(i\) in conversation \(c\) is categorised as topic \(t\). The identity vectors (e.g. _gender_) represent sets of variables, with a base category removed (indicated in Fig. 3). The coefficients of interest are contained in vectors \(\{^{t}_{d}\}_{d=1}^{6}\), where component \(g\) of \(^{t}_{d}\) is interpreted as the increase in probability of a participant choosing topic \(t\) if they are in the group indexed by \(g\) (e.g. Female) compared to the base group (e.g. Male). See App. R for extended methods.

**Results** Our instructions had a significant priming effect, resulting in a **high density of controversial and value-laden topics** (Fig. 3). Topics significantly correlated with controversy guidance are _Gender & LGBTQ+ Identity_, _Israel-Palestine Conflict_, and _Discussions on Abortion_, while topics significantly correlated with the values guidance are _Managing Relationships_, _Job Search_, and _Religion & Spirituality_. In contrast, the 'unguided' condition correlates with task-oriented and

Figure 3: **Topic prevalence by conversation types and participant identity.** We show total prompts clustered into topics (**bars**), and total members in each group (top panels). Per group and topic, we plot the _over-representation factor_ of observed vs. expected group proportions and show significant regression coefficients (base category indicated by \(\)). All coefficients are in Fig. 23, topic-group counts in Fig. 27 and centroid prompts in Tab. 22. Location is by _birth region_ (with UK and US split out), but most regions have few countries (App. H). _Key results_ (ยง 3.1): Priming participants to select a conversation type (unguided, values or controversy guided) significantly influenced diversity of prompts. Identity factors have some significant interactions with prompt choice but each topic contains prompts authored by intersectionally-diverse participants.

neutral topics like _Popular Culture_, _Recipes & Cooking_ and _Travel Recommendations_. Only _Climate Change_ is not significantly correlated to conversation type. Controlling for conversation type, 11% of coefficients are significant (\(=99\%\)); so, **identity factors have some predictive power on topic prevalence**. Significant relationships include: women and non-binary people discuss gender and LGBTQ+ issues more than men; older people discuss elections and travel more than younger people; Black participants discuss climate change less than White participants, and all regions question LLMs about abortion less often than US participants. When we examine granular regions in embedding space using a single-link hierarchical clustering algorithm (App. S), **local prompt neighbourhoods tend to be intersectionally-diverse**: 84% of them meet or exceed entropy across intersectional demographics that would be expected under random sampling. During this local exploration, we retrieve regions of semantically-identical prompts rated by multiple diverse individuals (e.g. one neighbourhood "Does God exist?" has 7 religious and 7 irreligious participants), finding that **interpersonal differences in contextual preferences persist even when dialogue context is fixed** (App. S.4). So, despite Prism containing semantically-diverse prompts, people from different backgrounds occupy common discussion spaces, providing an anchor to examine diverse perspectives to shared issues.

### Case Study II: Do Different People Prefer Differently-Aligned Models?

**Methods** Observed preference differences at the model-level are confounded by interactions of topic prevalence and model aptitude (e.g. men ask more about aliens and gpt-4 is poor on extraterrestrial knowledge). Evidence of shared dialogue spaces (SS 3.1) and group-topic score differences (App. T.2) mitigate some concern, but to further control for context, we use opening prompts from the balanced subset of participants (n=1,246) with equal conversations per type (n=6,669). The mean participant rates 14/21 LLMs but unseen ratings are missing at random. Our aggregation (social choice) function over participant ratings is derived from _Pairwise Rank Centrality_ (\(\))  and _Convergence Voting_, both inspired by _PageRank_. Each model is a node in a graph and transition probabilities between nodes are calculated by the proportion of pairwise battle wins. This process simulates a random walk on a Markov chain, leading to a stationary distribution of scores that reflect the collective preference intensity across models. Here, we compute \(\) over subsamples using a regularisation parameter of 1 and tie threshold of 5, but present extended methods and robustness checks in App. T.

**Results** We find **rankings are sensitive to idiosyncratic, contextual, and group-wise variance**. Samples of 100 people introduce significant noise, resulting in a fairly even distribution of collective preference among the top 10 models (Fig. 4). Rankings are sensitive to _what_ participants talk about: zephyr-7b performs highly on controversy but not in unguided domains, while claude-2 has the opposite trend; and _where_ they are from: relative to overall rank, palm-2 drops 4 places for participants in the US, llama-7b drops 7 places in Asia, while mistral-7b gains 7 places in Africa. We further observe that **Prism produces surprising ranks relative to other leaderboards**. We apply our method to ChatbotArena data , finding gpt models fare significantly worse in Prism, while open models like zephyr-7b do significantly better (95% CI over 1,000 bootstraps, App. T.9). This may be due to domain shift (task-oriented/coding prompts vs. controversial/cultural prompts), sample diversity or task incentives. To identify drivers of score differences, we generate hypotheses by qualitatively examining battles between command and gpt-4/-turbo, then test these with an OLS regression on all model responses (App. T.8). We find that **formatting and refusals partially explain score differences** with significant positive effects from additional characters, ending in a question mark ("Would you like to know more?") and enumeration, but significant negative effect of line breaks. De-anthropomorphic phrases ("As an AI, I don't have personal opinions.") significantly reduce score but not as substantially as refusals ("Sorry I cannot engage."). The proportion of explained variance in score by these factors is low (\(R^{2}=0.06\)), so we encourage more sophisticated methods in future work for partialling out the effect of style versus content, or participant, model and conversation fixed-effects, as determinants of score.

### Case Study III: How do Sampling Decisions Affect Welfare Outcomes?

**Methods** We use 'welfare' to capture the extent to which a chosen LLM aligns with the preferences of a user population. We consider two welfare measures: average model rating (meanRating), and average likelihood that a model is chosen (rated highest in the opening turn, meanChoice). Previous experiments indicate dialogue and preference diversity across people, suggesting that the welfare of downstream LLM users may depend on who provides feedback. To test this, we first randomly generate seven sub-samples of individuals 'in the seat of power' to select their favourite LLM (basedon mean rating). Four sampling schemes randomly draw \(N\) individuals from a representative sample (\(N\{10,20,50,100\}\)). Three schemes randomly draw 100 individuals from specific low-diversity sub-populations (male, white, and \(\)45 years old). For each draw, we then measure the distribution of welfare from this LLM being imposed on different stakeholder populations : the entire population, non-male individuals, non-white individuals, and individuals \(<\)45 years old. We report the distribution of average welfare outcomes across random draws from each sampling scheme. We conduct this experiment for the UK and US representative samples. Extended methods are in App. U.

**Results** We find **as sample size falls, the probability of choosing a LLM with worse mean welfare rises**. Larger samples from the target sub-population appear to first order stochastically dominate9 (FOSD) smaller samples from the target sub-population. **Sampling exclusively from a specific group tends to reduce the welfare of out-group individuals**. For example, when consider the welfare of the representative US sample (Fig. 5), sampling from US males is FOSD by sampling from the full US sample. Furthermore, **average measures can conceal the welfare of minority groups**: sampling 100 white individuals appears to FOSD sampling 100 representative individuals when assessing welfare of the population at large, but minority stakeholders (non-white population) are worse off under this scheme. Finally, **regardless of the model chosen, a large proportion of participants prefer a different model**. For the US, the model that maximises meanChoice only

Figure 4: **Sources of variation in model preferences.** Panel A shows _idiosyncratic variance_ in distributions of Pairwise Rank Centrality scores for 100 randomly-drawn participants (over 1,000 bootstraps). For Panels B and C, we show _conversational context variation_ and _group-wise variation_. We show overall rank based on Pairwise Rank Centrality over n=6,669 balanced conversations (numbered circles). We then trace how rank changes by sampling the group on \(x\) (e.g. filtering to only values guided conversations, or only US participants). Across these subsamples, we show most spots climbed (\(\)) and spots fallen (\(\)) by each model relative to overall rank. _Key results_ (ยง 3.2): Rankings are sensitive to sample composition, varying with which participants are sampled (Panel A,C) and what they talk about (B). Rankings differ from other leaderboards, explained by Prismโs characteristics (sample diversity, domain shifts) as well as response characteristics (length, formatting, refusals).

achieves a probability of \(45\%\). If a participant is shown the winning model, and three other models at random, the probability that they will choose the winning model is \(<50\%\). The probability they will pick the winning model over all other 20 LLMs can only be lower. This suggests that we should not expect a single LLM to satisfy everyone's preferences in a given population. We repeat the welfare analysis for the UK sample and conduct robustness checks with imputed missing data in App. U.

## 4 Related Work

**Participation & Representation in Science & Technology** There is a long history of technologies failing diverse users who lack consultation during design [52; 53; 54]. Conscious participation can be intrinsically valuable as an act of justice [55; 56]. However, in internet-harvested pre-training data, participation is involuntary or cooptative [55; 33], and unequal representation risks cultural homogenisation and minority stereotyping [57; 58; 59; 60; 61; 62]. Labelling data or giving feedback is active _procedural participation_ but often relies on narrow specifications from technology providers of what counts as high-quality language or preferable outputs [15; 16; 63; 64]. In ML or NLP data, variability in subjective experience is commonly collapsed into majority votes [65; 66; 67; 68], without sufficient documentation of annotator artefacts or disagreements [69; 70; 71; 72; 73], despite evidence that sociodemographic affect labels [74; 75; 76; 77; 78; 79]. Multiple scientific fields are guilty of over-generalising conclusions from the 'generic human' drawn from 'WEIRD' societies [80; 81]. Prism releases participant IDs and characteristics to spotlight sample diversity while acknowledging sample specificity .

**Learning from Human Feedback** Using human feedback to condition the loss function for training LLMs overcomes challenges of specifying rewards [83; 84; 85]. Combining human feedback, reinforce

Figure 5: **Welfare distributions for the US. The distribution of mean welfare for four subpopulations in the US (welfare pop) induced by seven sampling schemes (in the seat of power). The \(y\) axis is the sampled supopulation (e.g. **Rep** is a โrepresentativeโ sample of the population) and sample size in brackets (e.g **(100)**). Each violin shows the distribution of mean welfare for the panelโs subpopulation induced by a sampling scheme. The top four **Rating** comparisons use the meanRating welfare measure and the bottom **Choice** comparisons use the meanChoice welfare measure. The **red** distributions are FOSD by Rep (100) in **blue** (i.e. less optimal scheme). _Key results_ (ยง 3.3):** Large representative samples mostly outperform smaller or demographically-restricted samples and sampling exclusively from a specific group tends to reduce the welfare of out-group participants (male vs. non-male, white vs. non-white). No single model achieves majority preference (max 45% **meanChoice**).

ment learning and natural language generation has a history in machine translation  and dialogue . RLHF pipelines rely on binary comparisons , principles or rules , fine-grained feedback , or natural language , to reward dimensions like helpfulness, honesty and harmlessness . Reward models then update LLMs via algorithms like PPO  or Reinforce ; but reward model free techniques are competitive, e.g. DPO , supervised fine-tuning  and rejection sampling . There is rising demand for high-quality human feedback , but the complexity and cost of collecting data incentivises scraping preferences, e.g. on Reddit  or StackOverflow , or simulating humans with LLMs . Similar to Prism, ChatbotArena , Lmsys-1m  and WildChat feature user-rated model interactions, but for narrow communities (HuggingFace Spaces) and domains (coding, task-orientated). Unlike these datasets, OpenConvos collect optional contributor demographics, and Dices provide demographics for multiple raters per conversation. Other datasets target specific behaviours , or multilingual coverage . Surveys on attitudes towards AI  and community assemblies  offer another lens on public priorities. To our knowledge, Prism is the first to link preference ratings and detailed survey responses.

## 5 Limitations, Discussions and Conclusions

**Ethical Considerations and Limitations** We collect informed consent, pseudononymise IDs, check for PII (App. E) and disallow deanonymisation in our terms (App. C), but privacy risks remain, especially given the sensitive nature of conversations. Asking participants to engage with controversies expands human preference data to discursive areas with the greatest expected degree of interpersonal disagreement, but risks encouraging hateful, bigoted, biased or otherwise harmful content. Prism is less toxic than previous datasets (0.06%, App. E). We do not moderate prior to release to permit conversational safety research. There are many sources of variance in Prism and alternative divisions of the data may yield different outcomes . Granting free choice of dialogue, using cardinal feedback scales and focusing on many kinds of models and participants introduces diversity and subjective freedom but complicates controlled experiments and limits statistical power. Prism is still biased towards English-speaking crowdworkers whose task-specific incentives may not align with wider populations. We expand on ethical risks and limitations in our data statement (App. B).

We raise three discussion points on the boundaries of where we collect preferences, for what end and with what lasting impact. First, aligning LLMs via 'preference-based utilitarianism'  may not be synonymous with individual or societal well-being, prompting the question of **whether there are limits for "legitimate" human feedback.** Preferences may be (i) at odds with self-interest due to myopia or information asymmetries (e.g. participants who want anthropomorphic LLMs despite evidenced harms ) or (ii) incompatible with others' interest (e.g. participants who prefer 'anti-woke' LLMs that argue in a debate vs. those who favour neutrality). Relying on decontextualized preference observations carries the risk of silently reinforcing biases from those in power ; so we recommend transparency surrounding individual disagreements before aggregation decisions , especially if participant postionality affects their epistemic legitimacy to define harm . Second, **irreconcilable personal preferences and morals matter more when the 'unit of alignment' is operationalised as a group, culture or even species, rather than an individual. Prism permits personalised or steerable alignment using participant profiles and specific ratings  as well as collective alignment via opinion consensus or distribution of rewards ; though group deliberation in groups may yield different outcomes than gathering data from one person at a time . With growing use of synthetic alignment data, Prism can assist in calibrating LLM-as-judge protocols to more diverse rater pools . Finally, Prism was motivated by participation as justice via inclusionary alignment practices that, relative to passive roles in annotation tasks or pre-training data, prioritise active input from local citizens with specialised knowledge of their own and communities' needs . However, participation remains thin because **the humans crucial to the success of RLHF do not typically share in downstream benefits or profits**. Ultimately, the impact of our work depends on those developing, researching and regulating LLMs because effective participation requires being asked _and_ being heard .

In their early demonstrations of aligning AI systems to human feedback, Bai et al. discuss _alignment data as a public good_. We echo this sentiment with Prism--a new feedback dataset from 1,500 diverse humans, motivated by the need for inclusive, participatory and open scientific research into the pressing question of what it means to align LLMs to human preferences in a pluralistic world.

## Author Contribution Statement

**Project Conception** & [Kirk, Hale, Vidgen] \\
**Data Collection Design** & [Kirk, Hale, Vidgen, Rotter, Margatina] \\
**Frontend Design and Development** & [Kirk, Ciro] \\
**Backend Design and Development** & [Kirk, Mosquera] \\
**Analysis Advisory** & [Hale, Vidgen, Rotter, Bartolo, Bean, Williams, He] \\
**Literature and Dataset Comparison** & [Kirk, Bean] \\
**Metadata Processing** & [Kirk, Margatina, Bean] \\
**Manual Annotation** & [Kirk, Bean, Rotter, Bartolo] \\
**Results and Codebase** & [Kirk, Whitefield] \\
**Manuscript Writing** & [Kirk, Whitefield] \\
**Manuscript Editing and Feedback** & [Everyone] \\