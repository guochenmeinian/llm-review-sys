# Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features

Mingli Zhu1\({}^{*}\) Shaokui Wei1\({}^{*}\) Hongyuan Zha1,2 Baoyuan Wu1\({}^{}\)

1School of Data Science,

The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China

2Shenzhen Institute of Artificial Intelligence and Robotics for Society, China

Equal contribution

###### Abstract

Recent studies have demonstrated the susceptibility of deep neural networks to backdoor attacks. Given a backdoored model, its prediction of a poisoned sample with trigger will be dominated by the trigger information, though trigger information and benign information coexist. Inspired by the mechanism of the optical polarizer that a polarizer could pass light waves with particular polarizations while filtering light waves with other polarizations, we propose a novel backdoor defense method by inserting a learnable neural polarizer into the backdoored model as an intermediate layer, in order to purify the poisoned sample via filtering trigger information while maintaining benign information. The neural polarizer is instantiated as one lightweight linear transformation layer, which is learned through solving a well designed bi-level optimization problem, based on a limited clean dataset. Compared to other fine-tuning-based defense methods which often adjust all parameters of the backdoored model, the proposed method only needs to learn one additional layer, such that it is more efficient and requires less clean data. Extensive experiments demonstrate the effectiveness and efficiency of our method in removing backdoors across various neural network architectures and datasets, especially in the case of very limited clean data. Codes are available at https://github.com/SCLBD/BackdoorBench (PyTorch) and https://github.com/JulieCarlon/NPD-MindSpore (MindSpore).

## 1 Introduction

Several studies have revealed the vulnerabilities of deep neural networks (DNNs) to various types of attacks [15; 17; 21; 22; 40], of which backdoor attacks [6; 9; 11; 36] are attracting increasing attention. In backdoor attacks, the adversary could produce a backdoored DNN model through manipulating the training dataset [5; 24] or the training process [20; 29], such that the backdoored model predicts any poisoned sample with particular triggers to the predetermined target label, while behaves normally on benign samples. Backdoor attacks can arise from various sources, such as training based on a poisoned dataset, or utilizing third-party platforms for model training, or downloading backdoored models from untrusted third-party providers. These scenarios significantly elevate the threat of backdoor attacks to DNNs' applications, and meanwhile highlight the importance of defending against backdoor attacks.

Several seminal backdoor defense methods have been developed, mainly including 1) in-training approaches, which aim to train a secure model based on a poisoned dataset through well-designed training algorithms or objective functions, such as DBD  and D-ST . 2) post-training approaches, which aim to mitigate the backdoor effect from a backdoored model through adjusting themodel parameters (_e.g._, fine-tuning or pruning), usually based on a limited subset of clean training dataset, such as fine-pruning , ANP , or i-BAU . This work focuses on the latter one. However, there are two limitations to existing post-training approaches. First, given very limited clean data, it is challenging to find a good checkpoint to simultaneously achieve backdoor mitigation and benign accuracy maintenance from the high-dimensional loss landscape of a complex model. Second, adjusting all parameters of a complex model is costly.

To tackle the above limitations, we propose a lightweight and effective post-training defense approach, which only learns one additional layer, while fixing all layers of the original backdoored model. It is inspired by the mechanism of the optical polarizer  that in a mixed light wave with diverse polarizations, only the light wave with some particular polarizations could pass the polarizer, while those with other polarizations are blocked (see Fig. 1-left). Correspondingly, by treating one poisoned sample as the mixture of trigger feature and benign feature, we define a neural polarizer and insert it into the backdoored model as one additional intermediate layer (see Fig. 1-right) in order to filter trigger feature and maintain benign feature, such that poisoned samples could be purified to mitigate backdoor effect, while benign samples are not significantly influenced.

In practice, to achieve an effective neural polarizer, it should be learned to weaken the correlation between the trigger and the target label while keeping the mapping from benign samples to their ground-truth labels. However, the defender only has a limited clean dataset, while neither trigger nor target label is accessible. To tackle it, we propose a bi-level optimization, where a sample-specific target label is dynamically estimated according to the output confidence, and the trigger is approximated by the targeted adversarial perturbation. Besides, in our experiments, the neural polarizer is implemented by a linear transformation (_i.e._, the combination of one \(1 1\) convolutional layer and one batch normalization layer). Consequently, it can be efficiently and effectively learned with very limited clean data to achieve good defense performance, which is verified by extensive experiments on various model architectures and datasets.

Our main contributions are three-fold. **1)** We propose an innovative backdoor defense approach that only learns one additional linear transformation called neural polarizer while all parameters of the backdoored model are fixed, such that it just requires very low computational cost and very limited clean data. **2)** A bi-level optimization problem and an effective learning algorithm are provided to optimize the parameter of the neural polarizer. **3)** Extensive experimental results demonstrate the superiority of the proposed method on various networks and datasets, in terms of both efficiency and effectiveness.

Figure 1: **Left**: Illustration of an optical polarizer . Only light waves with specific polarizations can pass through the polarizer among the three incident light waves. **Right**: Defense against backdoors by integrating a trainable neural polarizer into the compromised model. The neural polarizer effectively filters out backdoor-related features, effectively eliminating the backdoor.

Related work

Backdoor attacks.Traditional backdoor attacks are additive attacks that modify a small fraction of training samples by patching a pre-defined pattern and assigning them to targeted labels [8; 11]. These modified samples, along with the unaffected samples, constitute a _poisoned dataset_. The model trained with this dataset will be implanted with backdoors that predict the targeted label when triggered by the injected patterns while maintaining normal behavior on clean samples [5; 50]. Recently, advanced attacks have considered more invisible trigger injection methods such as training an auto-encoder feature embedding or using a local transformation function [20; 29; 46]. To increase the stealthiness of attacks, clean-label attacks succeed by obfuscating image subject information and establishing a correlation between triggers and targeted labels without modifying the labels of poisoned samples [1; 2; 30].

Backdoor defense.Backdoor defense methods can be broadly categorized into training-stage defenses and post-processing defenses. Training-stage defenses assume access to a poisoned dataset for model training . The different behaviors between the poisoned and clean samples can be leveraged to identify suspicious instances, such as sensitivity to transformation  and clustering phenomenon in feature space . Most defense methods belong to post-processing defenses , which assume that the defender only has access to a suspicious DNN model and a few clean samples. Therefore, they must remove the backdoor threat with limited resources. There are mainly three types of defense strategies: trigger reversion methods try to recover the most possible triggers and utilize the potential triggers to fine-tune the model ; pruning-based methods aim at locating the neurons that are most related to backdoors and pruning them [23; 43; 47; 48]; and fine-tuning based defenses leverage clean samples to rectify the model [19; 45]. I-BAU  is most closely related to our method, which formulates a minimax optimization framework to train the network with samples under universal adversarial perturbations. However, our method differs from I-BAU in that our approach does not require training the entire network, and our perturbation generation mechanism is distinct. Other methods proposed for backdoor detection include Beatrix , which uses Gram matrices to identify poisoned samples; and AEVA , which detects backdoor models by adversarial extreme value analysis. In this study, we focus on post-processing defenses and primarily compare our method with state-of-the-art post-processing defenses [19; 23; 35].

## 3 Methodology

### Basic settings

Notations.We consider a classification problem with \(K\) classes (\(K 2\)). Let \(^{d}\) be a \(d\)-dimensional input sample, and its ground truth label is denoted as \(y=\{1,,K\}\). Then, a \(L\) layers deep neural network \(f:^{K}\) parameterized by \(\) is defined as:

\[f(;)=f^{(L)}_{_{L}} f^{(L-1)}_{_{L-1}}  f^{(1)}_{_{1}}(),\] (1)

where \(f^{(l)}_{_{l}}\) is the function (_e.g._, convolution layer) with parameter \(_{l}\) in the \(l^{}\) layer with \(1 l L\). For simplicity, we denote \(f(;)\) as \(f_{}()\) or \(f_{}\). Given input \(\), the predicted label of \(\) is given by \(*{arg\,max}_{k}f_{k}(;),k=1,,K\), where \(f_{k}(;)\) is the logit of the \(k^{}\) class.

Threat model.We assume that the adversary could produce the backdoored model \(f_{}\) through manipulating training data or training process, such that \(f_{}\) performs well on benign sample \(\) (_i.e._, \(f_{}()=y\)), and predicts poisoned sample \(_{}=r(,)\) to the target label \(T\), with \(\) indicating the trigger and \(r(,)\) being the fusion function of \(\) and \(\). Considering that the adversary may set multiple targets, we use \(T_{i}\) to denote the target label for \(_{i}\).

Defender's goal.Assume that the defender has access to the backdoored model \(f_{}\) and a limited set of benign training data, denoted as \(_{bn}=\{(_{i},y_{i})\}_{i=1}^{N}\). The defender's goal is to obtain a new model \(\) based on \(f_{}\) and \(_{bn}\), such that the backdoor effect will be mitigated and the benign performance is maintained in \(\).

### Neural polarizer for DNNs

Neural polarizer.We propose the neural polarizer to purify poisoned sample in the feature space. Formally, it is instantiated as a lightweight linear transformation \(g_{}\), parameterized with \(\). As shown in Fig. 1, \(g_{}\) is inserted into the neural network \(f_{}\) at a specific immediate layer, to obtain a combined network \(f_{,}\). For clarity, we denote \(f_{,}\) as \(_{}\), since \(\) is fixed.

A desired neural polarizer should have the following three properties:

* **Compatible with the neighboring layers**: in other words, its input feature and its output activation must have the same shape. This requirement can be fulfilled through careful architectural design.
* **Filtering trigger features in poisoned samples**: after the neural polarizer, the trigger features should be filtered, such that the backdoor is deactivated, _i.e._, \(_{}(_{}) T\).
* **Preserving benign features in poisoned and benign samples**: the neural polarizer should preserve benign features, such that \(_{}\) performs well on both poisoned and benign samples, _i.e._, \(_{}(_{})=_{}()=y\).

The first property could be easily satisfied by designing neural polarizer's architecture. For example, as illustrated in Fig. 2, given the input feature map with the shape \(C_{k} H_{k} W_{k}\), the neural polarizer is implemented by a convolution layer (Conv) with \(C_{k}\) convolution filters of shape \(1 1\), followed by a Batch Normalization (BN) layer. The Conv-BN block can be seen as a linear transformation layer. To satisfy the latter two properties, \(\) should be learned by solving some well designed optimization, of which the details are presented in Section 3.3.

### Learning neural polarizer

#### 3.3.1 Loss functions in oracle setting

To learn a good neural polarizer \(g_{}\), we first consider an oracle setting where the trigger \(\) and target label \(T\) are given. We present some loss functions to encourage \(g_{}\) to satisfy the latter two properties mentioned above, as follows:

* **Loss for filtering trigger features in poisoned samples.** Given trigger \(\) and target label \(T\), filtering trigger features can be implemented by weakening the connection between \(\) and \(T\) with the following loss: \[_{asr}(,y,,T;)=-(1-s_{T}(_{ };)),\] (2) where \(s_{T}(_{};)\) is the softmax probability of predicting \(_{}\) to label \(T\) by \(_{}\). By reducing \(_{asr}\), the attack success rate of the backdoor attack can be decreased.
* **Loss for maintaining benign features in poisoned samples.** To purify the poisoned sample such that it can be classified to the true label, we leverage the boosted cross entropy defined in : \[_{bce}(,y,;)=-(s_{y}(_{}; ))-1-_{k y}s_{k}(_{};) .\] (3)
* **Loss for maintaining benign features in benign samples.** To preserve benign features in benign sample, we adopt the widely used cross-entropy loss, _i.e._, \(_{bn}(,y;)=_{}(_{}(),y)\).

#### 3.3.2 Approximation & optimization

Approximating \(T\) and \(\).Note that as \(T\) and \(\) are inaccessible in both \(_{asr}\) and \(_{bce}\), these two losses cannot be directly optimized. Thus, we have to approximate \(T\) and \(\). In terms of

Figure 2: An example of neural polarizer for a DNN.

approximating \(T\), although some methods have been developed to detect target class of backdoored models [12; 26], here we adopt a simple sample-specific and dynamic strategy:

\[T y^{}=*{arg\,max}_{k y}_{k}(;).\] (4)

There are two main advantages of this strategy. First, the predicted target label enables us to generate targeted adversarial perturbation. As analyzed later, the targeted adversarial perturbation is a better surrogate for the unknown trigger than the untargeted adversarial perturbation. Second, the sample-specific target label prediction is applicable to both all2one (one trigger, one target class) and all2all (every class is target class) attack settings. In practice, defenders lack certainty about attack types (all2one, all2all, or multi-trigger multi-target), risking sub-optimal defenses due to erroneous guesses or detection. Our sample-specific target label prediction avoids this risk.

In terms of approximating \(\), it is approximated by the targeted adversarial perturbation of \(f_{}\), _i.e._,

\[^{*}=*{arg\,min}_{\|\|_{p} }_{}(_{}(+),y^{}),\] (5)

where \(\|\|_{p}\) is the \(L_{p}\) norm, \(\) is the budget for perturbations.

Bi-level optimization problem.By employing the approximations of \(T\) and \(\) shown in Eq. (4) and (5), respectively, and substituting these approximations into the respective loss functions of Eq. (2) and (3), we introduce the following optimization problem to learn \(\) based on the clean data \(_{bn}\):

\[_{}&_{i= 1}^{N}_{1}_{bn}(_{i},y_{i};)+_{2} _{asr}(_{i},y_{i},_{i}^{*},y_{i}^{};)+_{3}_{bce}(_{i},y_{i},_{i}^{*}; ),\\ &_{i}^{*}=*{ arg\,min}_{\|_{i}\|_{p}}_{}(_{ }(_{i}+_{i}),y_{i}^{}),\;y_{i}^{ }=*{arg\,max}_{k_{i} y_{i}}_{k_{i}}(_{i};),i=1,,N,\] (6)

where \(_{1},_{2},_{3}>0\) are hyper-parameters to adjust the importance of each loss function. This bi-level optimization is dubbed Neural Polarizer based backdoor Defense (**NPD**).

Variants.To comprehensively evaluate the performance of NPD, we also provide two variants, with the relaxation that if the target label \(T\) is known. One is that approximating the trigger by the targeted adversarial perturbation for each benign sample in Eq. (6), _i.e._, \(_{i}^{*}=*{arg\,min}_{\|_{i} \|_{p}}_{}(f(_{i}+_{i}),T)\), dubbed **NPD-TP**. The other is approximating the trigger by the targeted universal adversarial perturbation (TUAP)  for all training samples in fine-tuning, dubbed **NPD-TU**. We empirically found that the targeted adversarial perturbation is a better surrogate to the trigger than the untargeted ones.

Optimization algorithm.We proposed Algorithm 1 to solve the above optimization problem. Specifically, NPD solves problem (6) by alternatively updating the surrogate target label \(y^{}\), the perturbation \(\) and \(\) as follows:

* **Inner minimization:** Given parameter \(\) of the neural polarizer, we first estimate the target label for sample \(_{i}\) by \(y_{i}^{}=*{arg\,max}_{k_{i} y_{i}}_{k_{i}}(_{i};)\). Then, the targeted Project Gradient Descent (PGD)  is employed to generate the perturbation \(_{i}^{*}\) via Eq. (5).
* **Outer minimization:** Given \(y^{}\) and \(^{*}\) for each sample in a batch, the \(\) can be updated by taking one stochastic gradient descent  (SGD) step _w.r.t._ the outer minimization objective in Eq. (6).

## 4 Experiments

### Implementation details

Attack settings.We evaluate the proposed method on eight famous backdoor attacks, including BadNets  (BadNets-A2O and BadNets-A2A refer to attacking one target label and all labels, respectively), Blended attack (Blended) , Input-aware dynamic backdoor attack (Input-aware), Low frequency attack (LF) , Sample-specific backdoor attack (SSBA) , Trojan backdoor attack (Trojan) , and Warping-based poisoned networks (WaNet) . We follow the default attack configuration as in BackdoorBench  for a fair comparison. The poisoning ratio is set to 10% in comparison with SOTA defenses. These attacks are conducted on three benchmark datasets: CIFAR-10 , Tiny ImageNet , and GTSRB . We test all attacks on PreAct-ResNet18  and VGG19-BN .

Defense settings.We compare the proposed methods with six SOTA backdoor defense methods, _i.e._, Fine-pruning (FP) , NAD , NC , ANP , i-BAU , and EP . All these defenses have access to 5% benign training samples. The training hyperparameters are adjusted based on BackdoorBench . We evaluate the proposed method under two proposed scenarios and compare our NPD-TU, NPD-TP, and NPD with SOTA defenses. For the ablation study, we focus solely on NPD, which represents a more generalized scenario. We apply an \(l_{2}\) norm constraint to the adversarial perturbations, with a perturbation bound of 3 for CIFAR-10 and GTSRB datasets, and 6 for Tiny ImageNet. We train the neural polarizer for 50 epochs with batch size 128 and learning rate \(0.01\) on each dataset and the transformation block is inserted before the third convolution layer of the fourth layer for PreAct-ResNet18. The loss hyper-parameters \(_{1}\),\(_{2}\),\(_{3}\) are set to \(1,0.4,0.4\) for NPD, and \(1,0.1,0.1\) for NPD-TU and NPD-TP. More implementation details on SOTA attacks, defenses, and our methods can be found in Section B of **Appendix**.

Evaluation metric.In this work, we use clean ACCuracy (ACC), Attack Success Rate (ASR), and Defense Effectiveness Rating (DER) as evaluation metrics to assess the performance of different defenses. ACC represents the accuracy of clean samples while ASR measures the ratio of successfully misclassified backdoor samples to the target label. Defense Effectiveness Rating (DER \(\)) is a comprehensive measure that considers both ACC and ASR:

\[=[(0,)-(0,)+1]/2,\] (7)

where \(\)ASR denotes the decrease of ASR after applying defense, and \(\)ACC denotes the drop in ACC following defense. Higher ACC, lower ASR, and higher DER indicate better defense performance. Note that in comparison with SOTA defenses, the one achieving the best performance is highlighted in **boldface**, while the second-best result is indicated by _underlineining_. We provide PyTorch and MindSpore implementations of NPD.

### Main results

Table 1 and Table 2 showcase the defense performance of the proposed method in comparison to six SOTA defense methods on CIFAR-10 and Tiny ImageNet. The following observations can be made:

* **Our methods show superior performance in terms of DER for almost all attacks compared to SOTA defenses.** Conversely, FP and ANP excel in maintaining high ACC, but they struggle to eliminate backdoors in strong attacks like Blended and LF. NC's emphasis on minimal universal adversarial perturbation renders it ineffective against sample-specific attacks and those utilizing large norm triggers. I-BAU shows similar performance in removing backdoors with an average DER of 92.53%, but it leads to a significant decrease in ACC, likely due to training the entire network by adversarial training.
* **Defense performance of NPD-TU and NPD-TP are better than NPD.** When the target label is known, the model only needs to find perturbations for that specific label, simplifying trigger identification and unlearning. These two methods outperform NPD, except for WaNet, which is a transformation-based attack without visible triggers. Fully perturbing the network proves more effective than solely unlearning targeted triggers in WaNet.
* **NPD-TU is effective for trigger-additive attacks while NPD-TP is expert in defending against sample-specific attacks.** It can be observed by comparing defense results on different attacks like Blended and SSBA on CIFAR-10. This demonstrates that the applicability of different strategies varies across different attack scenarios.
* **Defense performance is robust across all attacks on Tiny ImageNet.** Similar to the results on CIFAR-10, our method outperforms other methods in terms of ASR and DER for all backdoor

    &  &  &  &  &  \\  & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER \\  BadNets-A20  & 91.82 & 93.79 & N/A & **91.77** & 0.84 & **96.45** & 88.82 & 1.96 & 94.42 & 90.27 & 1.62 & 95.31 & 91.65 & 3.83 & 94.89 \\ BadNets-A21  & 91.89 & 74.42 & N/A & 92.05 & 1.31 & 86.56 & 90.73 & 1.61 & 85.82 & 89.79 & 1.11 & 85.60 & 92.33 & 2.56 & 85.93 \\ Blended  & 93.69 & 99.76 & N/A & 92.74 & 10.17 & 94.32 & 92.25 & 47.64 & 75.34 & **93.69** & 97.60 & 50.09 & 31.45 & 4.74 & 76.19 \\ Input-Aware  & 94.03 & 93.85 & N/A & 94.05 & 1.62 & 98.36 & **94.08** & 9.02 & 98.71 & 93.84 & 10.48 & 93.84 & 94.06 & 1.57 & 98.39 \\ LF  & 93.01 & 99.06 & N/A & 92.05 & 1.23 & 88.39 & 91.27 & 75.41 & 61.15 & **93.01** & 99.06 & 50.00 & 92.53 & 1.86 & 86.10 \\ SSBA  & 92.88 & 97.07 & N/A & 92.21 & 20.27 & 88.06 & 92.15 & 70.77 & 62.78 & **92.88** & 97.07 & 50.00 & 92.02 & 16.18 & 90.01 \\ Trojan  & 93.47 & 99.99 & N/A & 92.24 & 67.73 & 65.51 & 92.18 & 5.77 & 96.47 & 91.85 & 51.03 & 73.67 & **92.71** & 84.82 & 57.20 \\ WaNet  & 92.80 & 98.90 & N/A & 92.94 & **66.92** & 91.92 & 97.03 & 77.93 & 99.08 & 92.80 & 98.90 & 50.00 & **93.24** & 51.4 & 98.68 \\  Avg & 92.95 & 95.17 & N/A & 92.51 & 15.49 & 89.62 & 91.88 & 25.61 & 84.24 & 92.27 & 57.38 & 68.55 & **92.75** & 23.00 & 85.98 \\    &  &  &  &  &  \\  & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER \\  BadNets-A20  & 87.43 & 4.48 & 92.46 & 89.80 & 1.26 & 95.26 & 90.81 & 1.44 & 95.97 & 90.90 & **0.62** & 96.12 & 88.93 & 1.26 & 94.82 \\ BadNets-A21  & 89.39 & 1.29 & 85.32 & 88.72 & 3.00 & 84.12 & 91.66 & **0.82** & 86.68 & **92.54** & **0.04** & **87.39** & 91.41 & 90.89 & 86.52 \\ Blended  & 89.43 & 26.82 & 84.34 & 91.94 & 48.22 & 74.89 & 91.88 & **0.03** & **98.96** & 91.33 & 0.83 & 98.28 & 91.18 & 0.41 & 98.42 \\ Input-Aware  & 89.91 & 0.02 & 97.10 & 93.68 & 2.88 & 97.56 & 92.01 & 0.14 & 98.09 & 93.24 & **0.00** & **98.78** & 89.57 & 0.11 & 96.98 \\ LF  & 89.82 & 11.99 & 94.19 & 91.97 & 84.73 & 86.64 & 91.42 & **0.01** & 98.73 & 91.20 & 0.08 & **98.94** & 90.06 & 2.01 & 97.95 \\ SSBA  & 86.53 & 2.89 & 93.91 & 91.67 & 4.33 & 95.76 & 91.61 & 2.46 & 96.67 & 91.82 & **0.83** & **97.59** & 90.88 & 2.77 & 96.15 \\ Trojan  & 89.29 & 0.54 & 97.63 & 92.32 & 2.49 & 98.18 & 92.59 & 0.04 & **98.93** & 92.19 & **0.00** & 93.35 & 92.37 & 6.51 & 96.19 \\ WaNet  & 90.70 & 0.88 & 97.96 & 90.47 & 96.52 & 50.03 & 92.18 & 3.24 & 97.52 & 92.57 & 7.47 & 95.60 & 91.57 & 80.08 & 98.43 \\  Avg & 88.95 & 6.11 & 92.53 & 91.32 & 30.43 & 81.55 & 91.77 & **1.02** & 96.48 & 92.06 & 1.23 & **96.52** & 90.75 & 1.62 & 95.67 \\   

Table 1: Comparison with the state-of-the-art defenses on CIFAR-10 dataset with 5% benign data and 10% poison ratio on PreAct-ResNet18 (%).

    &  &  &  &  &  \\  & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER \\  BadNets-A20  & 56.12 & 99.90 & N/A & 48.81 & 0.66 & 95.96 & 48.35 & 0.27 & 95.93 & **56.12** & 99.90 & 50.00 & 47.34 & **0.00** & 95.56 \\ BadNets-A21  & 55.99 & 27.81 & N/A & 47.88 & 31.9 & 58.26 & 48.29 & 3.20 & 58.91 & 54.12 & 18.72 & 53.61 & 40.70 & 2.39 & 55.07 \\ Blended  & 56.49 & 99.67 & N/A & 50.58 & 75.89 & 67.93 &attacks. Despite a slight decrease in ACC, NPD-TU achieves a remarkably good performance with ASR \(<1.5\%\) on average. NPD-TP and NPD perform best in removing backdoors while maintaining model utility.

In summary, our method outperforms other state-of-the-art approaches, showcasing the broad applicability of our proposed method across diverse datasets. Due to space limits, defending results on GTSRB dataset and VGG19-BN network can be found in Section C of **Appendix**.

### Analysis

Effectiveness of each loss term.We conduct an ablation study to evaluate the contribution of each component of the loss function towards the overall performance on CIFAR-10 dataset. We separately investigate the first and second terms of loss \(_{bce}\) (see Eq. (3)), denoting them as \(_{bce1}\) and \(_{bce2}\), respectively. Throughout the study, we keep the loss \(_{bn}\) consistent across all experiments, and the result is shown in Table 3. Notably, the loss \(l_{bc1}\) plays a significant role in improving the overall performance while removing each component leads to a significant drop in defense in certain cases. This ablation study underscores the importance of each loss component in effectively mitigating different types of attacks.

Effectiveness of the targeted adversarial perturbations in NPD.To show the efficacy of the targeted adversarial perturbations in NPD (see Eq. (6)), we compare NPD with its two variants using two types of untargeted perturbation. We refer to adversarial perturbations generated by UAP and standard PGD without a targeted label as NPD-UU (untargeted universal adversarial perturbation) and NPD-UP (untargeted PGD), respectively. As shown in Table 4, NPD-UU and NPD-UP fail to remove backdoors in certain cases although NPD-UP obtains a higher ACC. This result shows the superiority of NPD in removing backdoors.

Performance of choosing different layers to insert the transformation layer.We evaluate the influence of choosing different layers to insert the transformation layer by inserting it after each convolution layer of PreAct-ResNet18 network on CIFAR-10 dataset. Figure 3 shows the defense performance under three attacks. The result shows that inserting the transformation layer into the shallower layers results in a decrease in accuracy. This is because even a slight perturbation in the shallow layers can cause significant instability in the final output. However, as the layer goes deeper, the features become more separable, resulting in better defense performance.

Defense effectiveness under different poisoning ratios.To investigate the impact of poisoning ratios on defense performance, we conducted experiments on NPD with different poisoning ratios on the CIFAR-10 dataset. As presented in Table 5, there is a slight decrease in ACC as the poisoning ratio increases. Moreover, our approach exhibits a notably stable defense performance across a range of poisoning ratios.

    &  &  &  &  \\  & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR \\  BasNets-A2011  & 91.82 & 92.79 & 79.35 & **0.36** & **0.96** & **0.61** & **1.41** & 88.93 & 1.26 \\ Blindel  & 94.94 & 97.19 & 86.35 & 71.79 & **92.35** & 3.36 & 91.18 & 0.44 \\ ICF  & 93.01 & 90.96 & 82.95 & 75.42 & **91.53** & 17.24 & 90.06 & **0.21** \\ SSRA  & 92.88 & 97.77 & 84.31 & 52.35 & **91.49** & 14.22 & 90.89 & **2.77** \\ Trojan  & 93.47 & 99.99 & 89.81 & 38.11 & **92.61** & 11.43 & 92.37 & **6.51** \\ WaNet  & 92.80 & 98.50 & 84.70 & 5.98 & **92.11** & 1.41 & 91.57 & **0.80** \\   

Table 4: Defense results in comparison with NPD-UU and NPD-UP.

Figure 3: Defense performance of inserting linear transformation into different layers.

    &  &  &  \\  \(_{bce1}\) & \(_{bce2}\) & \(_{bce3}\) & ACC & ASR & ACC & ASR & ACC & ASR \\   & & & 91.45 & 1.18 & 92.47 & 97.63 & 93.02 & 90.50 & 95.90 \\ ✓ & & & 90.17 & 1.19 & 91.51 & 92.01 & 90.91 & 9.60 \\  & ✓ & & 90.46 & 0.38 & 91.68 & 18.28 & 91.19 & 1.06 \\  & ✓ & ✓ & 90.02 & 0.27 & 91.31 & 89.32 & 91.07 & 0.80 \\ ✓ & ✓ & & 89.56 & 0.21 & 91.09 & 1.75 & 90.47 & 7.63 \\ ✓ & ✓ & ✓ & & 88.93 & 1.26 & 91.18 & 0.41 & 90.06 & 0.21 \\   

Table 3: Defense performance under different different models of losses.

Defense effectiveness under different clean ratios.We investigate the sensitivity of clean data on defense performance and compare our NPD with SOTA defenses. As shown in Table 6, NPD is less sensitive to the size of clean data among all the attacks and defenses. Even with only 50 samples, it still maintains acceptable performance. This result shows that our method exhibits minimal reliance on the number of training samples.

Defense against adaptive attacks.We evaluate the effectiveness of NPD against adaptive attacks. Consider the attacker knows the defense strategy and the attacker trains the backdoored model with adversarial training (AT). The defense performance against the AT backdoored model under different attacks is shown in Table 7. NPD still performs well on AT model, while i-BAU fails in these attacks. One possible reason is that i-BAU is essentially adversarial training, while NPD adopts dynamic targeted adversarial perturbation, which is different from AT. Compared to the defense against backdoored models with standard training (Table 1), there is a slight ASR increase (from 2.77% to 5.22% for SSBA, for example).

Defense against all to all attacks.To demonstrate the effectiveness of our dynamic and sample-specific target label prediction strategy, we conducted experiments involving all-to-all attacks for various backdoor attack methods. In these experiments, each class is treated as the target class. The result is presented in Table 8. As illustrated in the table, our NPD approach successfully eliminates all backdoors, proving its efficacy in multi-label cases. In contrast, i-BAU method fails in most cases.

Running time comparison.We measure the runtime of the defense methods on 2500 CIFAR-10 images with batch size 256 and PreAct-ResNet18. The experiments were conducted on one RTX 4090Ti GPU and the results are presented in Table 9. Among these methods, our proposed PND-UN achieves the fastest performance on CIFAR-10, requiring only 56 seconds. It should be noted that our method was trained for 50 epochs, while i-BAU was only trained for 5 epochs.

    &  &  &  &  \\  & & Defense \(\) & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER \\   & i-BAU  & 65.41 & 7.74 & 79.82 & 60.65 & 2.02 & 81.03 & 64.60 & 2.120 & 70.27 \\  & ANP  & 91.53 & 5.50 & 94.00 & 90.81 & 2.02 & **93.4** & 85.57 & 1.52 & 91.04 \\  & Ours & 87.58 & 1.38 & **94.09** & 87.80 & 0.27 & 92.78 & 86.91 & 0.20 & **92.37** \\   & i-BAU  & 84.21 & 17.73 & 85.33 & 76.84 & 51.90 & 60.38 & 67.48 & 98.37 & 35.21 \\  & ANP  & 92.06 & 28.67 & 83.79 & 92.13 & 27.61 & 80.17 & 88.31 & 22.28 & 80.92 \\  & Ours & 90.48 & 0.43 & **97.12** & 90.80 & 10.03 & **88.29** & 89.48 & 8.97 & **88.16** \\   & i-BAU  & 83.91 & 19.34 & 85.31 & 70.12 & 99.42 & 35.53 & 70.26 & 99.31 & 35.60 \\  & ANP  & 92.74 & 46.70 & 76.04 & 92.28 & 10.82 & 84.11 & 89.99 & 18.77 & 82.59 \\  & Ours & 90.21 & 0.99 & **97.63** & 89.28 & 0.50 & **91.37** & 88.18 & 10.83 & **85.65** \\   

Table 6: Results with different number of clean data on CIFAR-10 (%).

    &  & i-BAU  &  \\  & ACC & ASR & ACC & ASR & ACC & ASR \\  Blended  & 86.00 & 99.63 & 83.98 & 30.43 & 83.64 & 3.92 \\ Input-Aware  & 84.98 & 94.99 & 83.17 & 71.12 & 83.13 & 4.47 \\ LF  & 84.15 & 94.30 & 84.15 & 94.30 & 83.39 & 4.89 \\ SSBA  & 84.34 & 93.32 & 82.74 & 24.54 & 83.38 & 5.22 \\      &  & i-BAU  &  \\  & ACC & ASR & ACC & ASR & ACC & ASR \\  Blended  & 86.00 & 99.63 & 83.98 & 30.43 & 83.64 & 3.92 \\ Input-Aware  & 84.98 & 94.99 & 83.17 & 71.12 & 83.13 & 4.47 \\ LF  & 84.15 & 94.30 & 84.15 & 94.30 & 83.39 & 4.89 \\ SSBA  & 84.34 & 93.32 & 82.74 & 24.54 & 83.38 & 5.22 \\    
  &  & i-BAU  &  \\  & ACC & ASR & ACC & ASR & ACC & ASR \\  Blended  & 91.59 & 83.50 & 88.80 & 15.74 & 91.07 & 6.24 \\ Input-Aware  & 86.60 & 78.55 & 89.50 & 5.37 & 90.10 & 2.10 \\ LF  & 91.91 & 84.80 & 88.92 & 33.33 & 90.53 & 4.88 \\ SSBA  & 91.30 & 85.04 & 91.16 & 26.00 & 91.14 & 1.29 \\   

Table 7: Defense performance against adaptive Table 8: Defense performance against all2all attacks on CIFAR-10 dataset (%).

    &  & i-BAU  &  \\  & ACC & ASR & ACC & ASR & ACC & ASR & DER & ACC & ASR \\  Blended  & 86.00 & 99.63 & 83.98 & 30.43 & 83.64 & 3.92 \\ Input-Aware  & 84.98 & 94.99 & 83.17 & 71.12 & 83.13 & 4.47 \\ LF  & 84.15 & 94.30 & 84.15 & 94.30 & 83.39 & 4.89 \\ SSBA  & 84.34 & 93.32 & 82.74 & 24.54 & 83.38 & 5.22 \\    
  &  & i-BAU  &  \\  & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR \\  Blended  & 86.00 & 99.63 & 83.98 & 30.43 & 83.64 & 3.92 \\ Input-Aware  & 84.98 & 94.99 & 83.17 & 71.12 & 83.13 & 4.47 \\ LF  & 84.15 & 94.30 & 84.15 & 94.30 & 83.39 & 4.89 \\ SSBA  & 84.34 & 93.32 & 82.74 & 24.54 & 83.38 & 5.22 \\   

Table 5: Defense results under different poisoning ratio on CIFAR-10 and PreAct-ResNet18(%).

## 5 Conclusion

Inspired by the mechanism of optical polarizer, this work proposed a novel backdoor defense method by inserting a learnable neural polarizer as an intermediate layer of the backdoored model. We instantiated the neural polarizer as a lightweight linear transformation and it could be efficiently and effectively learned with limited clean samples to mitigate backdoor effect. To learn a desired neural polarizer, a bi-level optimization problem is proposed by filtering trigger features of poisoned samples while maintaining benign features of both poisoned and benign samples. Extensive experiments demonstrate the effectiveness of our method across all evaluated backdoor attacks and all other defense methods under various datasets and network architectures.

Limitations and future work.Although only limited clean data is needed for our method to achieve a remarkable defense performance, the accessibility of clean data is still an important limitation of the proposed method, which may restrict the application of our method. Therefore, a promising direction for future work is to further reduce the requirement of clean data by exploring data-free neural polarizer or learning neural polarizer based on poisoned training data.

Broader impacts.Backdoor attacks pose significant threats to the deployment of deep neural networks obtained from untrustworthy sources. This work has made a valuable contribution to the community with an efficient and effective backdoor defense strategy to ease the threat of existing backdoor attacks, even with a very limited set of clean samples, which ensures its practicality. Besides, the innovative defense strategy of learning additional lightweight layers, rather than adjusting the whole backdoored model, may inspire more researchers to develop more efficient and practical defense methods.

Structure of Appendix.Detailed algorithms of PGD and UAP, proposed NPD-TU and NPD-TP, along with a theoretical analysis in the kernel space are provided in Section A. Implementation details of experiments are introduced in Section B. More defense results in different datasets and networks are provided in Section C. Defense performance of different structures of neural polarizer is discussed in Section D. Additional experimental results and visualization are presented in Section E.

## 6 Acknowledgments

This work is supported by the National Natural Science Foundation of China under grant No. 62076213, Shenzhen Science and Technology Program under grant No. RCYX20210609103057050, No. ZDSYS2021102111415025, No. GXWD20201231105722002-20200901175001001, Shenzhen Science and Technology Program under grant No. JCYJ20210324120011032, CAAI-Huawei MindSpore Open Fund, and the Guangdong Provincial Key Laboratory of Big Data Computing, the Chinese University of Hong Kong, Shenzhen.

   Defense (sec.) & FP  & NAD  & NC  & ANP  & i-BAU  & EP  & NPD (**Ours**) \\  CIFAR-10 & 1169.01 & 74.39 & 896.45 & 58.75 & 57.23 & 131.84 & 55.16 \\ Tiny ImageNet & 3357 & 351 & 42512 & 1692 & 887 & 302 & 332 \\   

Table 9: Running time of different defense methods with 2500 CIFAR-10 images on PreActResNet18.