# Tight Rates for Bandit Control Beyond Quadratics

Y. Jennifer Sun

Princeton University

ys7849@princeton.edu &Zhou Lu

Princeton University

zhoul@princeton.edu

###### Abstract

Unlike classical control theory, such as Linear Quadratic Control (LQC), real-world control problems are highly complex. These problems often involve adversarial perturbations, bandit feedback models, and non-quadratic, adversarially chosen cost functions. A fundamental yet unresolved question is whether optimal regret can be achieved for these general control problems. The standard approach to addressing this problem involves a reduction to bandit convex optimization with memory. In the bandit setting, constructing a gradient estimator with low variance is challenging due to the memory structure and non-quadratic loss functions.

In this paper, we provide an affirmative answer to this question. Our main contribution is an algorithm that achieves an \(()\) optimal regret for bandit non-stochastic control with strongly-convex and smooth cost functions in the presence of adversarial perturbations, improving the previously known \((T^{2/3})\) regret bound from (Cassel and Koren, 2020). Our algorithm overcomes the memory issue by reducing the problem to Bandit Convex Optimization (BCO) without memory and addresses general strongly-convex costs using recent advancements in BCO from (Suggala et al., 2024). Along the way, we develop an improved algorithm for BCO with memory, which may be of independent interest.

## 1 Introduction

Optimal control lies at the heart of engineering and operations research, with applications ranging from launching spacecraft to stabilizing economies. The theory of optimal control is a well-established field with a rich history, dating back to 1868 when James Clerk Maxwell analyzed governors, and flourishing in the mid-20th century with the development of dynamic programming by (Bellman, 1954) and the Kalman filter by (Kalman, 1960).

Classic optimal control theory studies the problem where a controller interacts with an environment, according to a (partially observable) linear time-invariant (LTI) dynamical system:

\[x_{t+1}=Ax_{t}+Bu_{t}+w_{t},\ \ y_{t}=Cx_{t}+e_{t},\] (1)

where \(A,B,C\) are the dynamics governing the system, \(x_{t},u_{t},y_{t},w_{t},e_{t}\) represent the system state, control input, observation, perturbation, and observation noise at time \(t\), respectively. At each time \(t\), the controller observes \(y_{t}\) and chooses a control \(u_{t}\), incurring a cost \(c_{t}(y_{t},u_{t})\) based on the current observation and control. The system then evolves according to Eq. (1) to reach the next state \(x_{t+1}\).

The theory of optimal control (e.g., LQC) typically relies on three key assumptions on the setting: the perturbation \(w_{t}\) is stochastic, the cost function \(c_{t}\) is quadratic and known in advance, and the function \(c_{t}\) is observable to the controller. The linear-quadratic regulator (LQR) (Kalman et al., 1960) provides a closed-form optimal solution under these conditions, representing a pinnacle of classical control theory.

However, these assumptions are often too idealistic for practical scenarios. In real-world control problems, the perturbations can be adversarial, the feedback model can be bandit, and the costfunction can be non-quadratic. This is evident in applications such as autonomous vehicle navigation, advertisement placement, and traffic signal control. This discrepancy between theory and practice raises a fundamental question for developing a more general control theory:

_Can we devise algorithms with provable guarantees for LTI control problems with adversarial perturbations, bandit feedback models, and non-quadratic cost?_

Recent research in online non-stochastic control (see (Hazan and Singh, 2022) for a survey) aims to address this broader goal by relaxing two of the standard assumptions: (1) the cost \(c_{t}\) can be time-varying, non-quadratic convex functions unknown to the controller; (2) the perturbation \(w_{t}\) and the observation noise \(e_{t}\) can be adversarially chosen.

The natural performance metric in this context is _regret_, defined as the excess cost incurred by the controller compared to the best control policy in a benchmark policy class \(\):

\[_{T}^{}()=_{t=1}^{T}c_{t}(y_{t},u_{t})- _{}_{t=1}^{T}c_{t}(y_{t}^{},u_{t}^{}),\] (2)

where \((y_{t},u_{t})\) is the observation-control pair reached by executing the controller at time \(t\), and \((y_{t}^{},u_{t}^{})\) is the observation-control pair under the policy \(\) at time \(t\). An optimal \(()\) regret was obtained by (Agarwal et al., 2019) with the Gradient Perturbation Controller (GPC) algorithm.

Several works in online control have made further progress towards the general question. (Cassel and Koren, 2020; Sun et al., 2024) obtained optimal regret under bandit feedback for strongly convex and smooth cost when the perturbation \(w_{t}\) and observation noise \(e_{t}\) are semi-adversarial (i.e. they contain an additive stochastic component that admits covariance matrices with least singular value bounded from below). (Cassel and Koren, 2020) also showed a sub-optimal \((T^{2/3})\) regret bound for fully adversarial perturbations. More recently, the advancement of (Suggala et al., 2024) showed for the first time that an optimal regret of \(()\) is achievable for strongly convex and smooth **quadratic** costs in the presence of adversarial perturbations and observation noises.

Still, no previous work has simultaneously addressed all three challenges with an optimal regret guarantee, which was left as an open problem by (Suggala et al., 2024). The main challenge lies in how to construct a low-variance gradient estimator under bandit feedback, with the memory structure and non-quadratic cost. Due to the \((T^{2/3})\) regret lower bound for general BCO with memory by (Suggala et al., 2024), exploiting the special affine memory structure in control problems is crucial to achieving optimal regret for general convex cost.

In this work, we provide the first affirmative answer to this general question by devising an algorithm that handles all three challenges with an optimal \(()\) regret bound. Our approach involves reducing the problem to no-memory BCO, which circumvents the high-dimensional estimator issue for non-quadratic costs. We then leverage a special curvature structure of the loss function induced by general strongly convex and smooth costs to obtain the optimal regret guarantee. Our result serves as a preliminary step toward fully solving the general control problem.

### Technical Overview

Several previous works have addressed the bandit non-stochastic control problem, but none achieved optimal regret across all three generalities due to the following technical challenges:

1. **The necessity of curvature**: The first work tackling the bandit non-stochastic control problem was (Cassel and Koren, 2020), which achieved an \((T^{2/3})\) regret bound for smooth convex cost. However, such sub-optimality is arguably inevitable due to the \((T^{2/3})\) regret lower bound for BCO-M with smooth convex 1 cost (Suggala et al., 2024): all existing results on bandit non-stochastic control relies on reduction to BCO-M! Indeed, the algorithm of (Cassel and Koren, 2020) is based on online gradient descent (QGD) and ignores the geometry of cost. This is the reason why we introduce the assumption on strong convexity. 2. **Strong convexity is not enough**: Does strong convexity alone become an easy remedy to the control problem? Unfortunately, even if we assume the cost functions \(c_{t}\) in the control problem are strongly-convex, the induced loss functions in the BCO-M problem are not necessarily strongly-convex! It was observed by (Suggala et al., 2024) that the induced loss functions satisfy a property called \(\)-convexity, allowing for low-variance estimation of Hessian matrices, which is a key ingredient in Newton-based second-order update to exploit the curvature.
3. **Going beyond quadratic**: However, finding a low-biased first-order estimation of gradients remains a challenge for BCO-M, because the nice property on the unit sphere domain \(_{d-1}:=\{x^{d^{}}\|x\|_{2}=1\}\) of the exploration term is not preserved under Cartesian product: \(_{d-1}_{d-1}_{d d-1}\). To handle this issue, (Suggala et al., 2024) relies on the quadratic cost assumption which admits an unbiased estimator of the divergence of the induced loss function. It's unclear how to handle general \(\)-convex smooth cost in BCO-M.

Similar to its full-information counterpart, we reduce the bandit non-stochastic control problem to bandit convex optimization with memory (BCO-M). To overcome these challenges, we further reduce the BCO-M problem to a no-memory BCO problem following (Cassel and Koren, 2020), to avoid the issue on the Cartesian product of \(_{d-1}\). This allows for the use of a Newton-based BCO-M algorithm similar to (Suggala et al., 2024), which can handle general \(\)-convex smooth cost because the exploration domain is now \(_{d-1}\). Besides this new approach, our method also includes novel algorithmic components and analysis.

### Related Work

Optimal control theory (Bellman, 1954) concerns finding a control for a dynamical system such that some objective function is optimized. The basic form of optimal control is linear quadratic control (Kalman, 1960), in which a quadratic function is minimized in a linear first-order dynamical system with stochastic noise, whose solution is given by LQR.

Online non-stochastic control generalizes classic optimal control by considering adversarial perturbation and cost, with the metric of regret to compete with the best fixed policy in some benchmark class. The first work (Agarwal et al., 2019) obtained an optimal regret bound for general convex cost by reduction to online convex optimization with memory (Anava et al., 2015), under stability assumptions on the system. For the line of works on the full information feedback setting, see (Hazan and Singh, 2022) for a survey.

Online non-stochastic control with bandit feedback were first considered by (Cassel and Koren, 2020; Gradu et al., 2020). Under a smoothness assumption on cost functions, the two works showed an \((T^{2/3})\) and \((T^{3/4})\) regret bound respectively. When the cost functions are additionally strongly-convex quadratics, (Sun et al., 2024) obtained an \(()\) regret, under semi-adversarial perturbations. (Yan et al., 2024) showed sublinear regret for cost functions with heterogeneous curvatures under the

   & Regret & Perturbation & Feedback & Loss type \\  (Agarwal et al., 2019) & \(()\) & adversarial & full & strongly-convex \\  (Agarwal et al., 2019) & \(O(^{7}T)\) & stochastic & full & convex \\  (Cassel and Koren, 2020) & \((T^{2/3})\) & adversarial & bandit & convex smooth \\  (Cassel and Koren, 2020) & \(()\) & stochastic & bandit & strongly-convex smooth \\  (Sun et al., 2024) & \(()\) & semi-adv & bandit & str-conv smooth quadratic \\  (Suggala et al., 2024) & \(()\) & adversarial & bandit & str-conv smooth quadratic \\  This work & \(()\) & adversarial & bandit & strongly-convex smooth \\  

Table 1: Comparison of results. This work is the first to address all three generalities with an \(()\) optimal regret: (Agarwal et al., 2019) addressed perturbation + loss, (Cassel and Koren, 2020) addressed feedback + loss, (Suggala et al., 2024) addressed perturbation + feedback. (Cassel and Koren, 2020) also obtained a sub-optimal \((T^{2/3})\) regret for perturbation + feedback + loss.

same semi-adversarial perturbations. This restriction on perturbation was later removed by (Suggala et al., 2024) which achieved the same regret for adversarial perturbations.

Recent advancements on BCO (Lattimore and Gyorgy, 2023; Suggala et al., 2021; Lattimore, 2024) considered Newton step updates to achieve improved regret bounds. In particular, (Suggala et al., 2024) also showed an \((T^{2/3})\) lower bound for bandit quadratic optimization with memory (BQO-M) without strong convexity. When making use of the affine memory structure, (Suggala et al., 2024) obtained an \(()\) regret bound for BQO-M.

## 2 Preliminary

### Online non-stochastic control with bandit feedback

Consider the linear dynamical system in Eq. (1). At time \(t\), the learner receives observation \(y_{t}\) and outputs control \(u_{t}\), to which the learner receives a scalar instantaneous cost \(c_{t}(y_{t},u_{t})\) depending on both the current observation and the control, and no other information about \(c_{t}\) is available. The perturbation \(w_{t}\) and observation noise \(e_{t}\) can be adversarially chosen. The learner's goal is to minimize regret over a fixed time horizon \(T\) defined in Eq. (2).

Throughout this paper, all cost functions we consider are assumed to be twice continuously differentiable. Consistent with past literature (Sun et al., 2024; Suggala et al., 2024), we make the following assumptions on the cost functions.

**Assumption 1** (Cost curvature and regularity).: _Let \(d_{y},d_{u}\) denote the dimensions of observation and control, respectively. The control cost function \(c_{t}:^{d_{y}}^{d_{u}}_{+}\) satisfies that_

1. _(Curvature)_ \(c_{t}\) _is_ \(_{c}\)_-strongly convex and_ \(_{c}\)_-smooth over some compact set_ \(^{d_{y}}^{d_{u}}\) _for some_ \(_{c},_{c}>0\)_, i.e.,_ \(\!c_{t}(x_{2})^{}(x_{1}-x_{2})+}{2}\|x_{1}-x_{2}\|_ {2}^{2} c_{t}(x_{1})-c_{t}(x_{2})\!c_{t}(x_{2})^{}(x_{1}-x_{ 2})+}{2}\|x_{1}-x_{2}\|_{2}^{2}\) _for any_ \(x_{1},x_{2}\)_._
2. _(Gradient bounds) There exists some_ \(G_{c}>0\) _such that the gradients satisfy_ \(\|\!c_{t}(y,u)\|_{2} G_{c}\|(y,u)\|_{2}\) _for any_ \(y,u\)_._

We also make stability assumption on the LDS in Eq. (1) as well as norm bound on the perturbation and noise. These assumptions are standard in literature (e.g. (Hazan and Singh, 2022)) and necessary for deriving meaningful regret guarantees.

**Assumption 2** (Strong stabilizability and bounded dynamics).: \((A,B,C)\) _that governs the LDS in Eq. (1) satisfies that \(\{\|A\|_{},\|B\|_{},\|C\|_{}\}_{ }\) for some positive constant \(_{}\). There exists \(K^{d_{u} d_{y}}\) s.t. \(A+BKC=HL^{-1}H\) for some \(H 0\) and \(\{\|K\|_{2},\|H\|_{2},\|H^{-1}\|_{2}\}\), \(\|L\|_{2} 1-\) for some \(>0\), \(0< 1\). Such \(K\) is called a \((,)\)-stabilizing linear controller for the LDS._

**Assumption 3** (Bounded perturbation and noise).: _The perturbation and observation noise sequences satisfy the norm bound \(_{t[T]}\{\{\|w_{t}\|_{2},\|e_{t}\|_{2}\}\} R_{w,e}\) for some \(R_{w,e}>0\)._

We assume the adversary chooses the cost functions and noise sequences in an oblivious way, i.e. they are chosen independently of the learner's decisions.

**Assumption 4** (Oblivious adversary).: _The sequence of cost functions \(\{c_{t}\}_{t=1}^{T}\) and the noise sequences \(\{w_{t}\}_{t=1}^{T},\{e_{t}\}_{t=1}^{T}\) are chosen by an oblivious adversary and does not depend on the control played by the learner._

For partially observable systems, the standard comparator class in literature (Simchowitz et al., 2020) is the class of Disturbance Response Controllers (DRC). Essentially, this class considers controllers that choose linear combinations of past signals and observations. The signals are simply the would-be observations had a stabilizing controller \(K\) been used from the beginning of the time. Formally, the class of DRC is given by the following definition:

**Definition 1** (Drc).: (1) A disturbance response controller (DRC) is a policy \(_{M}\) parametrized by \(m\) matrices \(M=M^{(0:m-1)}\) in \(^{d_{u} d_{y}}\) such that the control at time \(t\) according to \(_{M}\) is

\[u_{t}(_{M})=Ky_{t}+_{j=0}^{m-1}M^{[j]}y_{t-j}(K),\]where \(K\) is a \((,)\)-stabilizing linear controller, and \(y_{t}^{K}\) is the would-be observation had the linear policy \(K\) been carried out from the beginning of the time.

(2) A DRC policy class \((m,R_{})\), parameterized by \(m,R_{}>0\), is the set of all DRC controller of length \(m\) and obeys the norm bound \(\|M\|_{_{1},}:=_{j=0}^{m-1}\|M^{[j]}\|_{} R_ {}\).

A DRC policy always produces controls that ensure the system remains state bounded, the DRC class is expressive enough to approximate the class of linear policies of past observations (see Theorem 1 in (Simchowitz et al., 2020)).

### Bandit convex optimization with memory (BCO-M)

We start with the general protocol of bandit convex optimization with memory (BCO-M). In the (improper) BCO-M problem, at each time \(t\), the learner is asked to output a decision \(z_{t}^{d}\), to which a scalar loss \(f_{t}(z_{t-m+1:t})\), depending on the learner's most recent \(m\) decisions is revealed. Given a convex compact set \(^{d}\) as the domain of comparators, the regret is measured on the best single point \(z\) over a time horizon of \(T\), formally given by

\[_{T}^{}=_{z}\;[_ {t=m}^{T}f_{t}(z_{t-m+1:t})-f_{t}(z,,z)].\]

In non-stochastic control, the learner's past decisions affect future states through an affine structure, therefore we are interested in BCO-M problems with such structure. This leads to the following structural assumption on the induced loss function with memory. We will show in Lemma 9 that the bandit control problems with the standard regularity conditions satisfy Assumption 5.

**Assumption 5** (Affine memory structure).: _At time \(t\), the loss function with memory length \(m\) takes the form of \(f_{t}:(^{d})^{m}_{+}\) given by the following structure_

\[f_{t}(z_{t-m+1:t})=_{t}(B_{t}+_{i=0}^{m-1}G^{[i]}Y_{t-i}z_{t-i} ),\] (3)

_with parameters \(B_{t}^{n}\), \(Y_{t-i}^{p d}\), and \(G=G^{[0:m-1]}\) a sequence of \(m\) matrices where \(G^{[i]}^{n p}\) for some \(n,p\). 2 We denote \(G_{t}=_{i=0}^{m-1}G^{[i]}Y_{t-i}^{n d}\), and \(H_{t}=G_{t}^{}G_{t}^{d d}\). There exists some \(R_{H}>0\) such that \(\{1,\|G_{t}\|_{2},\|Y_{t}\|_{2},\|H_{t}\|_{2}\} R_{H}\). In addition, we assume that \(G\) satisfies positive convolution invertibility-modulus, i.e. \((G)=_{_{n 0}\|\|u_{n}\|_{2}^{2}=1}_{n 0}\|_{i=0}^{ n}G^{[i]}u_{n-i}\|_{2}^{2}=(1)\). We assume that the learner receives \(H_{t}\) every step after they incurred the loss._

We make two standard curvature and regularity assumptions on each of the instantaneous loss \(f_{t}\), that \(f_{t}\) is strongly-convex and smooth, with its subgradient norm bounded by some constants, following the previous work of Suggala et al. (2024).

**Assumption 6** (Curvature).: _Consider a loss function \(f_{t}\) satisfying Assumption 5 and the set_

\[_{t}=\{B_{t}+_{i=0}^{m-1}G^{[i]}Y_{t-i}z_{t-i}:z_{t-m+1}, ,z_{t}+_{d}\}^{n},\]

_where \(B_{t},G,Y_{t-m+1:t}\) are the parameters, and \(_{t}:^{n}_{+}\) is the function associated with \(f_{t}\) in Assumption 5. We assume that \(_{t}\) is \(_{f}\)-strongly-convex and \(_{f}\)-smooth, i.e. \(_{f}I_{n}^{2}_{t}(z)_{f}I_{n}\), over \(_{t}\), with \(0<_{f} 1_{f}\) here 3._

**Assumption 7** (Regularity).: _For \(d\), denote \(_{d}:=\{x^{d}\|x\|_{2} 1\}\) as the unit ball in \(^{d}\). We assume that at time \(t\), the with-memory loss function \(f_{t}:(^{d})^{m}_{+}\) with memory parameter \(m\) obeys the following gradient bounds over \(+_{d}:=\{x+y x,y_{d}\}\):_

\[\| f_{t}(z_{1},,z_{m})\|_{2} G_{f},\;\;\; z_{1},, z_{m}+_{d}.\]

_Additionally, we assume that \(\) has Euclidean diameter \(D>0\)._We consider an oblivious adversary model, given by Assumption 8.

**Assumption 8** (Oblivious adversary).: _For a BCO-M instance, we assume that the adversary is oblivious. In particular, let \(z_{t}\) denote the algorithm's decision at time \(t\), then the loss function \(f_{t}\) chosen by the adversary does not depend on \(z_{1:t}\)._

In the definition of regret in BCO-M, we compare the cumulative loss of any algorithm with the best fixed comparator \(z\), evaluated on the induced unary form \(f_{t}(z,,z)\) of the loss \(f_{t}\). Formally, we have the following definition

**Definition 2** (Induced unary form).: \( t\), let \(_{t}:^{d}_{+}\) denote the induced unary form of the loss \(f_{t}\), given by \(_{t}(z)=f_{t}(z,,z)\). Then, for \(f_{t}\) satisfying Assumption 5, \(_{t}\) admits the structure \(_{t}(z)=_{t}(B_{t}+_{i=0}^{m-1}G^{[i]}Y_{t-i}z)\).

We note that for \(f_{t}\) satisfying Assumption 6, the induced unary form in Definition 2 satisfies a special curvature called \(\)-convexity introduced in (Suggala et al., 2024). To avoid confusion with the notations with the bound on the dynamics (Assumption 2), we will call it \(_{0}\)-convexity henceforth.

**Definition 3** (\(_{0}\)-convexity, (Suggala et al., 2024)).: A function \(f:^{d}\) is called \(_{0}\)-convex over a domain \(^{d}\) if and only if the following holds: \(f\) is convex and twice continuously differentiable, and moreover \( c,C>0\) and a PSD matrix \(0 H I\) s.t. the Hessian of \(f\) at any \(z\) satisfies

\[cH^{2}f(z) CH,\ \ _{0}.\]

The benefit that the loss function exhibits an affine memory dependence is that its induced unary form satisfies \(_{0}\) convexity for some \(_{0}>0\), summarized the following observation.

**Observation 4** (\(_{0}\)-convexity and gradient bound of the unary loss.).: _Consider \(f_{t}:(^{d})^{m}_{+}\) satisfying Assumption 5, Assumption 6, and Assumption 7. Then, the induced unary form \(_{t}:^{d}_{+}\) in Definition 2 satisfies the following two properties:_

1. _(_\(_{0}\)_-convexity)_ \(_{t}\) _is_ \(_{0}\)_-convex with_ \(_{0}=_{f}/_{f}\)_,_ \(H=H_{t}\)_:_ \(_{f}H_{t}^{2}_{t}(z)_{f}H_{t}\)_, where_ \(H_{t}=G_{t}^{}G_{t}\) _as in Assumption_ 5_,_ \( z^{d}\)_._
2. _(Gradient bound)_ \(\|_{t}(z)\|_{2} G_{f}\)_,_ \( z_{t}\)_._

Proof of Observation 4.: By our assumption on the affine structure of \(f_{t}\) in Assumption 5, \( z^{d}\), \(^{2}_{t}(z)=G_{t}^{}^{2}_{t}(B_{t}+G_{t}z)G_{t}\). \(_{0}\)-convexity follows from the curvature assumption in Assumption 6. For any \(z_{1}\), \(z_{2}^{d}\), we have that \(|_{t}(z_{1})-_{t}(z_{2})| G_{f}\|(z_{1},,z_{1})-(z_{2},,z_{2})\|_{2}=G_{f}\|z_{1}-z_{2}\|_{2}\). 

### Notations

For a positive semidefinite square matrix \(H 0^{d d}\), define the norm \(\|\|_{H}\) on \(^{d}\) so that \(\|v\|_{H}^{2}=v^{}Hv\). We write \(c_{t}\) to denote the cost function of the control problem, and \(f_{t}\) to denote the loss function for BCO with memory. For two sets \(A,B\), \(A+B=\{a+b:a A,b B\}\). For a set \(S\), \(|S|\) denotes the cardinality of \(S\). We use \(_{d},_{d-1}\) to denote the unit ball and unit sphere in \(^{d}\), respectively. For \(n\) vectors \(v_{1},,v_{n}^{d}\), we slightly abuse notation and shorthand as \(v_{1:n}\) to denote the concatenated vector \((v_{1},,v_{n})^{nd}\).

## 3 Improved Bandit Convex Optimization with Memory

In this section, we introduce an improved algorithm for the bandit convex optimization with memory problem. Our algorithm incorporates the occasional update idea from (Cassel and Koren, 2020) and the Newton-based update for \(_{0}\)-convex functions from (Suggala et al., 2024), achieving an optimal \(()\) regret bound, improving the previously best known \((T^{2/3})\) result from (Cassel and Koren, 2020). Besides the advancement on BCO-M, this result is the key component of our main result in control. First, we define the BCO-M instance.

**Definition 5** (BCO-M instance).: Given \(d\), a BCO-M instance is parametrized by \(=\{,m,\{f_{t}\}_{t m,t}\}\), where \(^{d}\) is a convex compact set; \(m\) is the memory length; \(f_{t}:^{m}_{+}\) measures the instantaneous loss at time \(t\). Given any bandit online learning with memory algorithm \(^{B}\), the regret of \(^{B}\) on \(\) w.r.t. \(z\) is given by

\[_{T}^{^{B},z}()=_{t=m}^{T}f_{t}(z_{t-m +1:t})-_{t}(z),\]

where \(z_{t}=z_{t}^{^{B}}\) is the decision according to \(^{B}\) at time \(t\). We say a BCO-M instance \(\) is affine and \((,,G,D)\)**-well-conditioned** if \(\) satisfies Assumption 5, Assumption 6 with \(_{f}=,_{f}=\), Assumption 7 with \(G_{f}=G,D_{f}=D\), and the adversary model satisfies Assumption 8.

### BCO-M algorithm

We describe Algorithm 1 that runs bandit convex optimization for any BCO-M instance \(=\{,m,\{f_{t}\}_{t m,t}\}\). On a high level, our algorithm employs the occasional update idea from (Cassel and Koren, 2020). The decisions are updated at most once every \(m\) steps, ensured by the condition \(b_{t}_{i=1}^{m-1}(1-b_{t-i})=1\) (line 8 in Algorithm 1), where \(b_{t}\) is the Bernoulli random variable (partially) deciding whether the algorithm will make an update at the current time step. We write \(o\) as the original ideal prediction, \(v\) as the random perturbation vector, and \(z\) is the actual prediction (as the perturbed \(o\)) of the algorithm.

```
1:Input: convex compact set \(^{d}\), step size \(>0\), memory parameter \(m\), curvature parameter \(\), time horizon \(T\).
2:Initialize: \(o_{1}==o_{m}\), \(_{0:m-1}=_{d}\), \(_{0:m-1}=I\), \(=1\).
3:Sample \(v_{t} S_{d-1}\) i.i.d. uniformly at random for \(t=1,,m\).
4:Set \(z_{t}=o_{t}+_{t-1}^{-}v_{t}\), \(t=1,,m\).
5:Draw \(b_{t}()\), \(t=1,,m\).
6:for\(t=m,,T\)do
7:Play \(z_{t}\), observe \(f_{t}(z_{t-m+1:t})\), receive \(H_{t}=G_{t}^{}G_{t}\).
8:Draw \(b_{t}()\).
9:if\(b_{t}_{i=1}^{m-1}(1-b_{t-i})=1\)then
10:Let \(s_{}=t\).
11:Update \(_{t}=_{t-1}+H_{t}\).
12:Create gradient estimate: \(_{t}=df_{t}(z_{t-m+1:t})_{t-1}^{}v_{t}^{d}\).
13:Update \(o_{t+1}=_{}^{_{s_{-1}}}[o_{t}-_{s_{ -1}}^{-1}_{s_{-1}}]\).
14:Sample \(v_{t+1} S_{d-1}\) uniformly at random, independent of previous steps.
15:Set \(z_{t+1}=o_{t+1}+_{t}^{-}v_{t+1}\).
16:\(+1\).
17:else
18:Set \(o_{t+1}=o_{t}\), \(v_{t+1}=v_{t}\), \(z_{t+1}=z_{t}\), \(_{t}=_{t-1}\), \(_{t}=_{t-1}\).
19:endif
20:endfor ```

**Algorithm 1** Improved Bandit Convex Optimization with Affine Memory

Such occasional update essentially reduces the BCO-M problem to a new no-memory BCO problem, whose equivalence will be shown later (see Appendix A.2). Consistent with the notation used in (Cassel and Koren, 2020), we denote the following set

\[S:=\{t[T]:z_{t+1} z_{t}\}\] (4)

to be the set of time steps where the algorithm updates its decision. We readily have \(|S|\). Moreover, we have

\[f_{t}(z_{t-m+1:t})=_{t}(z_{t-m+1})=_{t}(z_{t}),\;\;\; t  S,\] (5)since \(t S\) implies \(b_{t-m+1}==b_{t-1}=0\). Thus, whenever Algorithm 1 updates, it effectively updates with function value \(_{t}(z_{t})\).

The occasional update alone only gives a sub-optimal \((T^{2/3})\) regret as shown in (Cassel and Koren, 2020). To achieve the optimal \(()\) regret, we use a Newton-based update to exploit the \(_{0}\)-convexity of general strongly-convex smooth functions, recently introduced by (Suggala et al., 2024).

For this improvement we require the knowledge of a Hessian estimator \(H_{t}\) as in Assumption Assumption 5. This doesn't hold in BCO in general, but we will show in the next section that the control problem indeed satisfies this assumption: thanks to \(_{0}\)-convexity, in the control problem \(H_{t}\) can be constructed from the knowledge of system, instead of knowledge of loss which would typically incur a huge variance term.

The regret guarantee of Algorithm 1 is given by the following theorem.

**Theorem 6** (BCO-M regret guarantee).: _Given an \((,,G,D)\)-well-conditioned BCO-M instance \(=\{,m,\{f_{t}\}_{t m,t}\}\) with \(m=( T)\) and \(G,D=(1)\) (Definition 5), let \((,=(1/),m,,T)\) be the input to Algorithm 1. Algorithm 1 guarantees that_

\[_{z}\;[_{T}^{Algorithm\;1,z}( )](GD),\]

_where \(()\) hides all universal constants and logarithmic dependence in \(T\)._

Theorem 6 is the first algorithm to achieve the optimal \(()\) regret bound for the BCO-M problem with general smooth convex loss. Compared with the \((T^{2/3})\) bound from (Cassel and Koren, 2020), our improvement exploits the new assumptions on the strong convexity of loss and the affine structure of memory. We notice that the \(()\) regret bound of (Suggala et al., 2024) requires the additional quadratic loss assumption and uses the special structure of quadratic functions to construct low-biased gradient estimators for the unary form of losses. Algorithm 1 and its guarantee in Theorem 6 thus improve upon both of these previous results. The proof of Theorem 6 is lengthy, therefore we leave it to the appendix and include a sketch here.

### Proof Sketch

The theorem is proven via reduction to a no-memory BCO algorithm with Newton-based updates.

Step 1: regret of the base algorithm.We devise a new BCO algorithm with two new ingredients different from standard algorithms. First, our algorithm adopts Newton-based updates which require estimating Hessians. In Assumption 5 we assume "free" access to a Hessian estimator \(H_{t}\), which holds in the control setting by utilizing the \(_{0}\)-convexity property.

Second, we incorporate a new delay mechanism to decorrelate neighboring iterates such that \(o_{t}\) is independent of recent perturbation vectors, which plays a crucial role in bounding the expectation of moving cost. Our base BCO algorithm is then shown to have an \(()\) regret bound (see Lemma 11).

Step 2: reduction to the base algorithm.We show that the regret of Algorithm 1 can be controlled by the regret of the base algorithm plus a moving cost term. By the design of Algorithm 1, it updates a univariate loss function at most once every \(m\) steps. If all the \(z_{t}\) across these \(m\) steps are the same, **Regret**(Algorithm 1) will be exactly the same as \(m\)(base algorithm). When \(z_{t}\) are different, we suffer an additional moving cost \([_{t=m}^{T}f_{t}(z_{t-m+1:t})-_{t}(z_{t-m+1})]\), which can be bounded by the assumption on the gradient of \(f_{t}\) if \(z_{t}\) changes slowly.

Step 3: bounding the moving cost.We partition the moving cost \(f_{t}(z_{t-m+1:t})-_{t}(z_{t-m+1})\) into three terms

\[(z_{t-m+1:t})-f_{t}(o_{t-m+1:t})}_{(a)}+(o_{ t-m+1:t})-_{t}(o_{t-m+1})}_{(b)}+_{t}(o_{t-m+1})- _{t}(z_{t-m+1})}_{(c)}.\]Here, (a), (c) can be seen as perturbation loss suffered by the algorithm during exploration. (b) is the moving cost determined by the stability of the algorithm's neighboring iterates.

For the three terms in the above equation, (c) is bounded by Jensen's inequality using the convexity of the unary form of loss. We bound (a) using the curvature assumptions and the affine memory structure of \(f_{t}\), where we also make use of the delayed updates to decorrelate neighboring iterates for technical reasons. (b) is bounded by the Lipschitzness of \(f_{t}\) and the distance between neighboring iterates. Theorem 6 is reached by putting the three steps together.

## 4 Optimal Regret for Bandit Non-stochastic Control

In this section, we show how to achieve optimal regret for the bandit non-stochastic control problem with a partially observable LTI dynamical system as described in Eq. (1) for strongly-convex smooth loss, by a reduction to the BCO-M algorithm (Algorithm 1) we devise in the previous section. Previously, the best known regret bound for this problem was \((T^{2/3})\) from (Cassel and Koren, 2020), which is also rooted in a reduction to BCO-M. We use a similar reduction, obtaining a better bound thanks to the improved regret bound of BCO-M (Theorem 6). We first give the formal definition of the control problem.

**Definition 7** (Bandit non-stochastic control).: A bandit non-stochastic control problem of a partially observable LDS is parametrized by a tuple \(=(A,B,C,x_{1},(w_{t})_{t},(e_{t})_{t},(c_ {t})_{t},)\), where \(A,B,C\) and \((w_{t})_{t},(e_{t})_{t}\) are the dynamics and perturbations in the LDS (Eq. (1)) with initial state \(x_{1}\) (we assume \(x_{1}\)=0 without loss of generality henceforth); \(c_{t}\) measures the instantaneous cost at time \(t\); \(\) is the comparator control policy class. Given any bandit non-stochastic control algorithm \(\), the regret of \(^{}\) on \(\) over a time horizon \(T\) is given by

\[_{T}^{^{}}(,)=_{t=1}^{T}c _{t}(y_{t}^{^{}},u_{t}^{^{}})- _{}_{t=1}^{T}c_{t}(y_{t}^{},u_{t}^{}),\]

where \((y_{t}^{^{}},u_{t}^{^{}}),(y_{t}^ {},u_{t}^{})\) are the observation, control pair at time \(t\) following the trajectory of \(^{}\) and \(\), respectively. We say that a bandit non-stochastic control problem \(\) is \((,,G,_{},,,R_{,},,)\)**-well-conditioned** if \(\) and \(\) satisfy Assumption 1 with \(_{c}=,_{c}=,G_{c}=G\) over some centered bounded convex domain \(^{d_{y}+d_{u}}\), Assumption 2 with \(_{},,\), Assumption 3 with \(R_{,}\), and Assumption 4.

The reduction from partially observable bandit non-stochastic control to BCO-M consists of two main steps. The first step is to construct the would-be signal \(y_{t}(K)\) from the observation \(y_{t}\), where \(y_{t}(K)\) is used for updating but not directly observed by the controller. \(y_{t}(K)\) is computed by using the Markov operator of the LDS (Simchowitz et al., 2020). The second step is a standard black-box reduction from control to BCO-M, which uses the strong stability of the system. Reduction is formalized in the following definition.

**Definition 8** (Approximation).: A bandit non-stochastic control instance (Definition 7) \(\) with some convex comparator class \(\) over a time horizon \(T\) is said to be \(\)**-approximated** (\(>0\)) by a BCO-M instance (Definition 5) \(\) with domain \(=\), if the existence of a BCO-M algorithm \(^{B}\) implies the existence of a bandit non-stochastic control algorithm \(^{}\) satisfying

\[[_{T}^{^{},}( ,)][_{T}^{^{B },}()]+ T.\]

The formal guarantee of the reduction is given below.

**Lemma 9** (Control reduction).: _Every instance of \((_{c},_{c},G_{c},_{},,,R_{,},,)\)-well-conditioned bandit non-stochastic control problem \(\) over the ball \(^{d_{y}+d_{u}}\) of radius \(R\)4 with comparator class \(=(m,R_{})\) (Definition 1) is \(2G_{f}DmT^{-1}\)-approximated by a \((_{f},_{f},G_{f},D)\)-well-conditioned BCO-M instance \(\) with_

\[_{f}=_{c},\ \ _{f}=_{c},\ \ D=,d_{y}\}}R_{ },\ \ G_{f}=G_{c}R_{w,e}^{2}R_{}^{2}d_{x}^{2} ^{3}_{}^{8}}{^{5}},\]

_provided that \(m=( T/(1/(1-)))\)._Lemma 9 implies that every well-conditioned bandit non-stochastic control problem can be reduced to a well-conditioned BCO-M instance, whose parameters are polynomial in those of the control problem. As a result, any regret bound for BCO-M will directly transfer to the control problem (at the expense of polynomial dependence on the system parameters). In particular, combining Theorem 6 and Lemma 9, we obtain an optimal \(()\) regret bound for the bandit non-stochastic control with strongly-convex smooth cost.

**Theorem 10** (Bandit non-stochastic control regret guarantee).: _Given an \((,,G,_{},,,R_{, },,)\)-well-conditioned bandit non-stochastic control instance \(\) over the ball \(R_{d_{y}+d_{u}}^{d_{y}+d_{u}}\) for the same \(R\) as in Lemma 9, with \(G,_{},,,R_{,},d_{x},d_{y},d _{u}=(1)\)5, let \((,,,m,T,G,K,)\) be the input to Algorithm 2 with \(m=( T)\). Algorithm 2 guarantees that_

_where \(()\) hides all universal constants and logarithmic dependence in \(T\)._

Theorem 10 strictly improves Theorem 8 of (Cassel and Koren, 2020) and Theorem 5 of (Suggala et al., 2024), in the sense that our result achieves the optimal regret with fewer assumptions: all three results achieve the same \(()\) regret bound, however (Cassel and Koren, 2020) requires the additional assumption that perturbations are stochastic, while (Suggala et al., 2024) requires the additional assumption that costs are quadratic.

**Input:** Step size \(>0\), memory parameter \(m\), DRC policy class \((m,R_{})\), time horizon \(T\),

system dynamics, \((,)\)-strongly stabilizing linear policy \(K\), strong convexity parameter \(>0\).

```
1: Let \(^{}\) be an instance of Algorithm 1 with inputs \((=(m,R_{}),,m,,T)\).
2: Initialize: \(M_{1}^{[j]}==M_{m}^{[j]}=0_{d_{u} d_{y}}\), \( j[m]\). \(_{0}==_{m-1}=0_{md_{u}d_{y}}\), \(_{0}=_{m-1}=mI_{md_{u}d_{y} md_{u}d_{y}}\).
3: Initialize \(^{}\) for \(t=1,,m-1\) (lines 1-4 in Algorithm 1).
4: Sample \(_{t}_{d_{y},d_{y}-1}^{t}\) i.i.d. uniformly at random for \(t=1,,m\).
5: Play control \(u_{t}=Ky_{t}\), incur cost \(c_{t}(y_{t},u_{t})\) for \(t[m]\).
6:for\(t=m,,T\)do
7: Play control \(u_{t}^{M_{t}}=Ky_{t}+_{j=0}^{m-1}M_{t}^{[j]}y_{t-j}(K)\), incur cost \(c_{t}(y_{t},u_{t})\).
8: Let \(f_{t}:((m,R_{}))^{m}\) be the induced with-memory loss function via reduction in Lemma 9 and \(H_{t}\) be the associated Hessian estimator in Assumption 5.
9: Update \(M_{t+1}^{}(M_{t},\{f_{s}\}_{s=m}^{t},\{H_{s}\} _{s=m}^{t})\).
10:endfor ```

**Algorithm 2** Improved Bandit Non-stochastic Control

## 5 Conclusion

In this paper, we devise an algorithm with an \(()\) regret bound for the bandit non-stochastic control problem with adversarial strongly-convex smooth cost functions. This is the first result with optimal regret that simultaneously breaks the three assumptions of LQC (1) stochastic perturbation (2) full-information feedback (3) quadratic cost. Our control algorithm is built upon an improved algorithm for BCO-M, which may be of independent interest.

As a preliminary step to address the question of a general control theory for LTI, our result comes with limitations and potential future research directions. Currently, the improvement over the \((T^{2/3}\) regret by (Cassel and Koren, 2020) is made under the additional assumption of strong convexity. It's unclear whether this assumption is necessary for an \(()\) regret, we leave determining the minimal assumption on the cost functions for optimal regret as an open question.