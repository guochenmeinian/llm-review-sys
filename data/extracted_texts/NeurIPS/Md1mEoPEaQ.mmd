# SetLexSem Challenge: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models

SetLexSem Challenge: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models

 Bardiya Akhbari

Amazon

bardiyaa@amazon.com

&Manish Gawali

Amazon

mdgawali@amazon.com

&Nicholas A. Dronen

Amazon

ndronen@amazon.com

###### Abstract

Set theory is foundational to mathematics and, when sets are finite, to reasoning about the world. An intelligent system should perform set operations consistently, regardless of superficial variations in the operands. Initially designed for semantically-oriented NLP tasks, large language models (LLMs) are now being evaluated on algorithmic tasks. Because sets are comprised of arbitrary symbols (e.g. numbers, words), they provide an opportunity to test, systematically, the invariance of LLMs' algorithmic abilities under simple lexical or semantic variations. To this end, we present the SetLexSem Challenge, a synthetic benchmark that evaluates the performance of LLMs on set operations. SetLexSem assesses the robustness of LLMs' instruction-following abilities under various conditions, focusing on the set operations and the nature and construction of the set members. Evaluating seven LLMs with SetLexSem, we find that they exhibit poor robustness to variation in both operation and operands. We show - via the framework's systematic sampling of set members along lexical and semantic dimensions - that LLMs are not only not robust to variation along these dimensions but demonstrate unique failure modes in particular, easy-to-create semantic groupings of "deceptive" sets. We find that rigorously measuring language model robustness to variation in frequency and length is challenging and present an analysis that measures them independently. The code for reproducing the results of this paper, and for generating the SetLexSem Challenge dataset, is available at https://github.com/amazon-science/SetLexSem-Challenge.

## 1 Introduction

Transformer models (Vaswani et al. (2017)) were initially devised and used for traditional natural language processing tasks, such as machine translation, natural language inference, or question answering (Vaswani et al. (2017); Devlin et al. (2019); Wang et al. (2018); Rajpurkar et al. (2018)). More recently, auto-regressive Transformers pre-trained on Internet-scale datasets and fine-tuned to conform to human preferences on a curated set of instructions (Ouyang et al. (2022)) - colloquially, large language models (LLMs) - have been shown to exhibit impressive performance on some analytical tasks, such as mathematics (Cobbe et al. (2021); Hendrycks et al. (2021)), reasoning (Dua et al. (2019)), and computer programming (Chen et al. (2021)). The increasing adoption of these models requires that we carefully interpret and interrogate their behaviors and the datasets on which they are evaluated. Careful evaluation exposes weaknesses and irregularities that inform users about the quality of models they may adopt. It can also inform model designers about the limitations of current architectures and requirements of future ones.

We should be clear-eyed about the limitations of existing datasets. Multiple choice tasks are not uncommon among them. A multiple-choice task setup constrains the complexity of the problem posed to the system being evaluated. More concerning is that zero-shot task performance of an LLM tends to be better on datasets that existed before it was trained than on those that were released after . This suggests that datasets or tasks may be leaking into the LLM training procedure. The ways this might happen are numerous. A dataset - its training set, test set, or even both - might be included in the LLM training set (dataset leakage). Or a proprietary instruction dataset that contains tasks similar to those in a public dataset might be created and used during the instruction tuning process (task leakage). Zhang et al.  found that overlap between training and evaluation data is reported for only 30% of LLMs. Synthetic benchmarks may address these problems, even if imperfectly. A synthetic dataset can require an LLM to perform a procedure of complexity greater than answering a multiple choice question and it can control the complexity of the procedure itself. The task leakage problem is more challenging, but a synthetic dataset may at least circumvent dataset leakage by supporting regeneration with different parameters.

We present SetLexSem, a synthetic set theory benchmark that controls both the difficulty of the task and the objects on which a task is performed. SetLexSem focuses on set theoretical operations because sets can comprise objects of unconstrained type. The members of a set can be anything that can be named or described. Other fields of mathematics are constrained in their operands. Arithmetic works on numbers, and geometry on shapes, and while logic can operate on things that can be named, it is constrained in their relationships. We have thus focused on set operations because of their flexibility with respect to operands, particularly because they enable a systematic and _simultaneous_ testing of language models' analytical task performance under controlled variation of the task operands.

A truly intelligent system should exhibit System 2 thinking , which implies performing tasks consistently regardless of incidental features. Consequently, the figure of merit of SetLexSem is the variance of accuracy, not average accuracy. The more robust the system is, the less variance it should exhibit as incidental features of a task vary. We categorize incidental features throughout this paper as follows:

* as shown in Table 1
- or the task's scale (i.e. the size of the operands \(A\) and \(B\)).
* **Lexico-semantic** Task performance should not vary as the lexical or semantic aspects of the sets \(A\) and \(B\) are varied.

Figure 1: To evaluate the robustness of LLMs to semantic variation in set members, we create “deceptive” sets. To construct such sets, we sample a pair of hypernyms (e.g. “mammal” and “vehicle”) and, from them, a set of their hyponyms in three conditions: (1) with the hyponyms as sampled, (2) with half of the set members swapped, and (3) randomly sampled. LLMs exhibit a unique failure mode under the second condition (swapped) and the mean and variance in accuracy of the first condition (not swapped) is better than that of the random baseline. See Figure 7 for results.

SetLexSem is a benchmark of the robustness of a system - robustness to variations in task complexity and to variations in task content. The results presented here indicate that current LLMs are not robust - in a System 2 sense - along any of the dimensions that the benchmark evaluates. Further, the results show that LLM are particularly not robust to easy-to-create semantic groupings of "deceptive" sets, as shown in Figure 1. This latter result has, we believe, significant implications for the design of any future model that aspires to achieve System 2 robustness.

## 2 Related work

Our work most directly extends existing investigations into the limitations of auto-regressive LLMs.

Auto-regressive models can generate plausible yet inaccurate output for scientific writing, as shown by Zheng and Zhan (2023). Lin et al. (2021) show that auto-regressive models do not work well for predicting answers to problems that have a time complexity in the order of polynomial time. Auto-regressive models can generate accurate intermediate reasoning steps only when the training data exhibits well-defined patterns relating variables to the output (Prystawski et al., 2023). The studies by Welleck et al. (2022), Geirhos et al. (2020), and Bogin et al. (2022) demonstrate that there is a disparity in the performance of the auto-regressive models on the in-distribution and out-distribution sets. These studies highlight the importance of evaluating models beyond standard benchmarks and across various aspects of generalization.

A number of benchmark datasets and collections of datasets have been developed or curated for the purpose of stress testing the capabilities of LLMs on analytical tasks, such as reasoning (Dua et al. (2019), Sakaguchi et al. (2021)), math (Cobbe et al. (2021), Hendrycks et al. (2021)), and code generation Chen et al. (2021). Influential multitask collections include Hendrycks et al. (2021), Srivastava et al. (2023), and Suzgun et al. (2023).

In this work, we also seek to isolate the effects of token length or frequency on task performance. Studies have shown that evaluations can be confounded by the contents of the training data. Basmov et al. (2024) show that LLMs' task performance on reading comprehension is confounded by the world knowledge instilled in them during training. Razeghi et al. (2022) show that the performance of LLMs on numerical tasks degrades with inverse proportionality to the frequency of the numerical operands in the training set. Dziri et al. (2023) investigate the limits of LLMs at tasks requiring function composition, such as multiplication, and show that degradation in accuracy is proportional the depth of the computational graph. Anil et al. (2022) investigate failure modes of LLMs when evaluated on tasks of length greater than those on which they were trained. Prompting techniques for improving the length generalization of LLMs were demonstrated in Bueno et al. (2022).

One possible source of the algorithmic limitations of LLMs is that they perform a fixed amount of computation at inference time. Changing the amount of inference computation to fit the task may be a promising direction to pursue to mitigate these problems. Indeed, in Schwarzschild et al. (2021), Bansal et al. (2022) the authors demonstrate improved generalization with a recurrent network by performing additional recurrent steps.

Much recent research has focused on developing prompting techniques that improve LLMs' performance. Brown et al. (2020) show that LLMs with few-shot examples perform better than smaller languages models with task-specific fine-tuning. Adding diversity as part of these few-shot examples improves the generalization ability as demonstrated by Levy et al. (2022). Algorithmic prompting has been proved to improve the performance of LLMs for algorithmic reasoning tasks (Zhou et al. (2022)). Similarly, multiple studies (Wei et al. (2022), Qin et al. (2023), Merrill and Sabharwal

   Operation & Notation & Definition \\  Union & \(A B\) & \(\{x:x A x B\}\) \\ Intersection & \(A B\) & \(\{x:x A x B\}\) \\ Difference & \(A B\) & \(\{x:x A x B\}\) \\ Symmetric difference & \(A B\) & \(\{x:(x A x B)(x A x B)\}\) \\   

Table 1: The set operations evaluated in SetLexSem. Performing them requires composing simple logic \(\) (\(\)) and membership \(\) (\(\)) functions.

) demonstrate that Chain-of-Thought (CoT) prompting improves LLMs' abilities on various reasoning tasks (arithmetic, symbolic, and algorithmic).

## 3 Dataset

The SetLexSem dataset evaluates the robustness of language models along two dimensions: analytical and lexico-semantic. Robustness in this context is System 2 robustness and requires that a perfect intelligent system exhibit no variance in task performance as incidental aspects of the input vary. The **analytical** component of the task is performing set operations and includes varying set operations and the sizes of the sets. The **lexico-semantic** component is due to set members being written symbols with meanings, and written symbols have many features that are incidental to set operations.

When constructing SetLexSem, we systematically vary the hyperparameters listed in Table 2. For a given hyperparameter set, we create a 50 occurrences of a prompt, each with different samples of the sets \(A\) and \(B\).

AnalyticalAnalytical robustness is measured by varying four set operations - union (\(\)), intersection (\(\)), difference (\(\)), and symmetric difference (\(\)) - and the size of the operands. For an arbitrary set operation \(\) and sets \(A\) and \(B\), when evaluating \(A B\) with some concrete \(A\) and \(B\), we ensured that \(|A|=|B|\) and \(|A|\{2,4,8,16\}\).

Lexico-semanticLexico-semantic robustness is measured by varying the characteristics of the symbols of which sets consist. For any pair of set \(A\), the members are sampled randomly from some population or subset thereof. Members can be constrained to adhere to constraints on lexical form (e.g. only numbers, only words of a certain length), on frequency (e.g. more common words), or on semantics (e.g. only hyponyms with a shared hypernym).

Note the lexico-semantic hyperparameters marked with an \({}^{*}\) in Table 2. When prompts are generated for these conditions, only CoT prompting and formal demonstration phrasing is used. To understand how sets were sampled for these conditions, see Sections 3.2.1 and 3.2.2.

Additional sources of variationAnother set of hyperparameters varies the form of the prompt itself. We employ two standard prompting methods: a simple baseline prompt and a Chain of Thought (CoT) prompt. CoT prompting encourages step-by-step reasoning, which can improve LLM performance on reasoning tasks [Wei et al., 2022]. Demonstrations are phrased in either a natural or formal style.

Lastly, as we varied the number of demonstrations for in-context learning, \(k\{0,1,3,5\}\), all other parameters were kept fixed, including the sampled sets \(A\) and \(B\). Per one LLM vendor's suggestion (Anthropic ), we used XML tags to delimit sections. Since SetLexSem measures variance, whether these tags affect absolute task accuracy is, however, irrelevant.

See Figure 2 for an example of a representative prompt and the prompt hyperparameters.

   Hyperparameter & Values \\  Operation & \(\{,,,\}\) \\ Operand size & \(\{2,4,8,16\}\) \\ Token type & \(\{\}\) \\ Token length & \(\{,1,2,3,4\}\) \\ Token frequency\({}^{*}\) & Deciles \(\{1,,9\}\) of vocabulary by rank frequency \\ Semantic similarity\({}^{*}\) & Words in set \(A\) share one hypernym, in set \(B\) share another \\ Prompting method & \(\{,\}\) \\ Demonstration phrasing & \(\{\}\) \\ Number of in-context demonstrations & \(\{0,1,3,5\}\) \\   

Table 2: Hyperparameters of SetLexSem prompts. Hyperparameters marked with \(*\) were generated only using formal demonstration phrasing. See the main text for any additional caveats.

### Sampling

Recall that our benchmark evaluates binary operations \(A B\) on sets \(A\) and \(B\). To create the dataset, we sample new instances of \(A\) and \(B\) simultaneously using methods intended to determine the impact of various factors - like set size, token length, or semantic similarity - on accuracy. We describe each method here. Currently, in all prompts, we use set size \(m\{2,4,8,16\}\). Unless otherwise noted, \(|A B|=2m\) except when collisions occur between one or more elements of \(A\) and \(B\) during sampling.

#### 3.1.1 Numbers

When sampling numbers, SetLexSem samples integers uniformly from \([0,n-1]\), where \(n\) is the specified upper bound. We also optionally limited the number of digits in a sampled number. For instance, setting token-length=3 in SetLexSem restricts the sampled numbers to the range \([100,1000)\).

#### 3.1.2 Words

When sampling words, SetLexSem samples words from the NLTK Wordlist corpus (Bird and Loper (2004)), which contains a comprehensive list of English words. It optionally limit the number of characters in a sampled word. For example, setting token-length=4 in SetLexSem ensures that all sampled words have exactly four characters (e.g., "love").

### Targeted sampling

SetLexSem contains sampling procedures to enable measuring the robustness of systems to variance in (corpus-level) term frequency and to semantic similarity of set members. We describe them here.

#### 3.2.1 Words by frequency

To investigate the potential impact of word frequency on task performance, we developed a sampler that creates prompts for comparing accuracy on sets containing more frequent or less frequent words. This sampler (1) ranks the words from the NLTK English Wordlist corpus based on their frequency in the Google Books Ngram corpus (Google (2024)) and (2) then segments them into deciles. Words from a specific decile are then sampled to create the sets \(A\) and \(B\) of a set of prompts. Our use of Google Books Ngram frequencies does not guarantee that the vocabulary's rank frequency matches any particular system's training corpus frequency. Rather, it approximates frequencies due to the lack of public disclosure about most large systems' training set frequencies.

Figure 2: Example of our baseline prompt with sets of size two. Every prompt follows this template: set construction, task definition, demonstrations, and final instructions. Note that the baseline prompt instructs the LLM not to explain its reasoning whereas the chain-of-thought prompt instructs the model to think step by step. In this example, the set members are numbers and each token in a set is two characters long. The prompt explicitly instructs the model not to use external tools, should they be available to it. Additional examples of prompts are provided in the Appendix.

#### 3.2.2 Mixtures of semantically-related ("deceptive") words

We hypothesized that LLMs' abilities to follow instructions and perform in-context learning might fail when performing set operations involving a mixture of two semantically-related groups of words. Our intuition was that since LLMs internally operate on embeddings, the semantic relatedness of the words within and between the sets might contradict the algorithm implied by the instruction or the in-context demonstrations.

To test this hypothesis, we developed a hyponym sampler using WordNet (Miller (1995)). With this sampler, a set is sampled such that all members are hyponyms of the same hypernym. For example, all the members of set \(A\) might be subtypes of "mammal", while the members of set \(B\) might be subtypes of "vehicle" (cf. Figure 1). We evaluate with these sets in three conditions: (1) the sampled sets, grouped by hypernym, (2) half of the words from each set are swapped with the same number of words from the other set - creating a artificial semantic bifurcation within each set - and (3) a random baseline in which the hyponym vocabulary is randomly sampled. We hereafter refer to these sets as comprising "deceptive" words.

## 4 Evaluation

We now describe the behavior of a set of seven LLMs on SetLexSem - namely, OpenAI GPT-3.5, three of Anthropic's Claude models (Instant, Haiku, and Sonnet), Mistral AI-Large, Mistral Small, and Meta LLaMa 3 70b. With every model, we used a temperature of 0.251, top-\(k\) of 20, and top-\(p\) of 0.25.

Task performance is measured as accuracy. It can be argued that a more appropriate metric is one that assigns partial credit, as suggested by the analysis in Schaeffer et al. (2024). That argument is appropriate to refute claims about capabilities of LLMs that emerge seemingly _ex nihilo_ with increased scale. For SetLexSem, the nuance of the argument is unnecessary, as the emphasis is on variance, not changes in behavior during scaling.

Across all SetLexSem prompts, the minimum and maximum mean accuracy (SD) are 69.37 (29.34) and 85.09 (16.06). See Figure 3 for complete distributions. Of these, GPT-3.5 has the highest minimum accuracy (69.03), mean (85.09), and lowest standard deviation (16.06).

Unless otherwise noted, distributions in subsequent analyses in this section are aggregates of the distributions shown here.

### Analytic

Here we remark on the robustness of LLMs to variation in the analytic incidental features - set operation and set size - defined by SetLexSem. LLMs' ability to perform set operations accurately

Figure 3: Aggregate accuracy of LLMs on SetLexSem. Each distribution consists of 12,400 prompts. This is 400 fewer than the 12,800 that should be expected given (1) that we did not do \(k\)-shot prompting in these runs and (2) the number of other hyperparameters in Table 2. The discrepancy is due to the case where token length is 1, which has fewer prompts due to sampling with replacement.

depends on the operation (Figure 3(a)). Notice the increasing negative skew towards lower accuracy going from union to symmetric difference along the x-axis. A similar and quite dramatic skew towards lower accuracy occurs when the set size increase from 2 to a still-modest 16 members (Figure 3(b)). The variance across operations and operand sizes suggest, for instance, that as set size increases further, we should expect a more rapid decline in accuracy on symmetric difference than on union.

### Lexico-semantic

In this section, we report the robustness of the LLMs we tested to variation in incidental lexicosemantic features.

Numbers and wordsWhen we control the lexical form of set members, and plot LLM accuracy separately on words or on numbers across set operations, we observe that accuracy is everywhere worse with numbers. See Figure 5 for the distributions.

Notice in Table 2 that we sampled words and numbers of lengths \(\{1,2,3,4\}\) as well as with no explicit constraint on length. The numbers, however, were sampled from the range \(\). This length constraint on numbers may be a confounding variable, implying that token length affects accuracy.

Figure 4: LLM accuracy on set operations varies (a) by operation and (b) by operand size. A violin plot is a distribution of accuracy. Each point in the distribution is the fraction of times correct out of 50 samples of different sets while holding a prompt (and its hyperparameters) constant. See Table 2 for hyperparameters.

Figure 5: LLM accuracy on set operations appears to exhibit exhibits some bias in favor of words over numbers, but this result is inconclusive.

Token frequencySetLexSem allows controlling the length and frequency of set member tokens when constructing prompts (see Section 3.2.1). This feature enables, although imperfectly, separating the confounding variables of token length and frequency. We show results here across all deciles of rank frequency for tokens of length 3 and 5. Across all deciles, mean accuracy of tokens of length 5 is always greater than that of tokens of length 3. See Table 7 and Figure 5(a). And length 5 tokens are always more common (except perhaps for decile 9, where there are very few tokens), as shown in 5(b).

Due to budget constraints, the results we obtained in this section are from running prompts only against Claude Haiku.

Deceptive setsThe results of running prompts containing sets consisting of samples of semantically-related words are shown in Figure 7. The "deceptive" distributions shown comprise the sets described in Section 3.2.2. In the not swapped case, the set comprises words with one common hypernym and \(B\) consists of words with some other common hypernym. This creates a hard semantic bifurcation _between_ the sets. In the swapped case, half of the words from set \(A\) are swapped with half from set \(B\). This creates a hard semantic bifurcation _within_ the sets. The random baseline consists of all

Figure 6: LLM accuracy is not invariant to the incidental features token length or token frequency. Controlling for both length and frequency, accuracy is lower (a) for sets consisting of tokens of length 3 than for those consisting of tokens of length 5, across all deciles, and (b) tokens of length 3 are also less frequent across all deciles.

the words across all prompts created in the process of sampling for all "deceptive" prompts. In the baseline case, there is no significant semantic bifurcation between or within sets.

As is illustrated clearly in Figure 6(a), it is easiest for an LLM to perform set operations with these "deceptive" sets when each set is semantically uniform (not swapped case). This outcome is consistent with intuitions about the representations of similar words in embedding space. Because in this case all members of set \(A\) are similar to one another, and all members of set \(B\) are similar to one another, and all members of set \(A\) are dissimilar to those of set \(B\), it's easier for a model to follow the prompt instructions. The average and variance of task performance is reduced in the random baseline case, because there's no regularity of orientation of the embeddings within each set. Variance increases sharply in the swapped case. Again, this comports with intuition. The introduction of a semantic bifurcation within each set increases the difficulty of the task and causes a mismatch between the surface and the semantic senses of set membership.

While we have some preliminary results to suggest that \(k\)-shot prompting with a larger \(k\) than we tested (cf. Table 2) with may correct this behavior, but a system that truly exhibits System 2 thinking should not behave this way in the first place.

Due to budget constraints, the results we obtained in this section are from running prompts only against Claude Haiku.

Figure 7: Distributions of accuracy of LLMs on sets comprising “deceptive” words. In the not-swapped case, sets are as they were originally sampled (with the words in a given set having a common hypernym). In the swapped case, half of the deceptive set members are swapped between sets. The random baseline is a random sampling of words from the same vocabulary. An example of the swapping case: the sets \(\{appearing,nan,grandpa,turnout\}\) and \(\{presence,gramps,appearance,granny\}\) are a mixture of words denoting _grandparents_ and words denoting _coming into view_. We only use formal language phrasing for this experiment. Each distribution consists of 6400 prompts for the Figure (a), and 1600 prompts for Figure (b).

Discussion

While we have demonstrated here that today's LLMs are not robust to variations of the analytical and lexico-semantic features that SetLexSem tests, the long march of science towards greater understanding, and of technology towards greater sophistication, may imply that future systems may indeed be robust to such variations. System 2 thinking may be mechanized. In such a possible future, synthetic datasets like SetLexSem could be used to verify that systems that society has become generally confident in are indeed invariant in the ways we desire. In the meantime, our dataset and others like it serve as guidepost to systems designers indicating deficiencies that need to be corrected.

Notably, the failure mode that current LLMs exhibit on the "deceptive" sets of SetLexSem demonstrates that the relatedness of entities in the hidden states of an instruction-following neural network can subvert the instruction-following capabilities. To achieve high robustness, then, a model must be either architecturally equipped to, or at least explicitly trained to, balance instruction following and semantics. We hope that the research community sees this challenge as a worthy one to address in future model designs.

We believe the practical implications of SetLexSem are substantial. While the current version of SetLexSem is built on formal descriptions of set operations, the dataset generator can be adapted to create narrative descriptions of set operations. Set operations in natural language or story form can be created for various application domains, or in multiple languages, and the SetLexSem sampling methods can be extended to support protected classes of people, sentiments, or parts of speech. This can provide a very rich and challenging venue in which to further test the capabilities - and particularly the robustness - of language models. We consider this a promising direction for future work.

## 6 Conclusion

We have described the SetLexSem Challenge, a dataset for evaluating the robustness of LLMs to incidental variations in task difficulty and task content. It is systematic, covering many set operations and allowing for systematic variation of the types of set members. Our results across a variety of commercially-developed LLMs show that they do not exhibit System 2 robustness across variations in set operation, set size, term type (word or number), or word length, and that "deceptive" sets subvert their instruction-following ability substantially. SetLexSem exposes deficiencies in LLMs and can inform the research community about possible future directions for their improvement. We hope this dataset will contribute to the improvement of intelligent systems and to their proper evaluation.