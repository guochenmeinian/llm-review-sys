# Efficient Lifelong Model Evaluation

in an Era of Rapid Progress

 Ameya Prabhu\({}^{*}\)\({}^{1,3}\)   Vishaal Udandarao\({}^{*}\)\({}^{1,2}\)   Philip H.S. Torr\({}^{3}\)

**Matthias Bethge\({}^{1}\)**  **Adel Bibi\({}^{3}\)**  **Samuel Albanie\({}^{2}\)**

\({}^{1}\)Tubingen AI Center, University of Tubingen \({}^{2}\)University of Cambridge \({}^{3}\)University of Oxford

https://github.com/bethgelab/sort-and-search

https://huggingface.co/datasets/bethgelab/lifelong_benchmarks

equal contribution, \(\) equal supervising

###### Abstract

Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling _ever-expanding_ large-scale benchmarks called _Lifelong Benchmarks_. These benchmarks introduce a major challenge: the high cost of evaluating a growing number of models across very large sample sets. To address this challenge, we introduce an efficient framework for model evaluation, _Sort & Search (S&S)_, which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples. To test our approach at scale, we create _Lifelong-CIFAR10_ and _Lifelong-ImageNet_, containing 1.69M and 1.98M test samples for classification. Extensive empirical evaluations across \(\)31,000 models demonstrate that _S&S_ achieves highly-efficient approximate accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours (\(\)1000x reduction) on a single A100 GPU, with low approximation error and memory cost of \(<\)100MB. Our work also highlights issues with current accuracy prediction metrics, suggesting a need to move towards sample-level evaluation metrics. We hope to guide future research by showing our method's bottleneck lies primarily in generalizing _Sort_ beyond a single rank order and not in improving _Search_.

Figure 1: _Efficient Lifelong Model Evaluation._ Assume an initial pool of \(n\) samples and \(m\) models evaluated on these samples _(left)_. Our goal is to efficiently evaluate a new model (insert\({}_{}\)) at sub-linear cost _(right top)_ and efficiently insert a new sample into the lifelong benchmark (insert\({}_{}\)) by determining sample difficulty at sub-linear cost _(right bottom)_. See Section 2 for more details.

Introduction

The primary goal of standard evaluation benchmarks is to assess model performance on some task using data that is _representative of the visual world_. For instance, the CIFAR10  benchmark tested whether classifiers can distinguish between 10 categories, such as dogs and cats. Subsequent versions like CIFAR10.1 , CIFAR10.2 , CINIC10 , and CIFAR10-W  introduced more challenging and diverse samples to evaluate the _same objective of classifying 10 categories_. As benchmarks become standardized and repeatedly used to evaluate competing methods, they gradually lose their capacity to represent broader tasks effectively. This is because models become increasingly specialized to perform well on these specific benchmarks. This phenomenon, known as overfitting, occurs both in individual models and within the research community as a whole [28; 90]. Fresh approaches must compete with a body of methods that have been highly tuned to such benchmarks, incentivising further overfitting if they are to compete [9; 10].

One approach to preventing models from overfitting to biases [87; 3] is to move beyond fixed test sets by creating an ever-expanding pool of test samples. This approach, known as _Lifelong Model Evaluation_, aims to restore the representativeness of benchmarks to reflect the diversity of the visual world by expanding the coverage of test sets. One can expand the pool by combining datasets or using well-studied techniques like dynamic sampling [81; 51; 52], these expanding benchmarks can grow substantially in size as they accumulate samples. This raises the less-explored issue of increasing evaluation costs. As an example, it takes roughly 140 and 40 GPU days respectively to evaluate our current model set on our _Lifelong-CIFAR10_ and _Lifelong-ImageNet_ datasets (containing 31,000 and 167 models respectively). These issues are only exacerbated in benchmarking foundation models . For instance, evaluating a single large language model (LLM) on MMLU  (standard benchmark for evaluating LLMs) takes 24 hours on a consumer-grade GPU . This inevitably will lead to a surge in evaluation costs when benchmarking lots of increasingly expensive models against an ever-growing collection of test samples [78; 22]. Hence, we primarily ask: _Can we reduce this evaluation cost while minimising the prediction error?_

We design algorithms to enable efficient evaluation in lifelong benchmarks, inspired by computerized adaptive testing (CAT) . CAT is a method used to create exams like the GRE and SAT from a continuously growing pool of questions. Unlike traditional tests where all questions must be answered, CAT sub-samples questions based on examine responses. This approach efficiently gauges proficiency with far fewer questions, while maintaining assessment accuracy. Similarly, we aim to evaluate classification ability of new models without testing on all samples, instead selecting a subset of samples to evaluate models. We propose a method, _Sort & Search (S&S)_, which reuses past model evaluations on a sample set through dynamic programming to enable efficient evaluation of new incoming models. _S&S_ operates by first ranking test samples by their difficulty, done efficiently by leveraging data from previous tests. It then uses these updated rankings to evaluate new models, streamlining the benchmarking process. This strategy enables efficient lifelong benchmarking, reducing the cost dramatically from a collective of 180 GPU days to 5 GPU hours on a single A100 GPU. We achieve a 1000\(\) reduction in inference costs compared to static evaluation on all samples, reducing over 99.9% of computation costs while accurately predicting sample-wise performance. Moreover, with a single algorithm, we address both key challenges: expanding dataset size and evaluating new models given a dataset.

Taken together, our main contributions are:

1. We curate two lifelong benchmarks: _Lifelong-CIFAR10_ and _Lifelong-ImageNet_, consisting of 1.69M and 1.98M samples respectively.
2. We propose _Sort & Search_, a novel framework for efficient model evaluation.
3. We show that our simple framework is far more scalable and allows saving 1000x evaluation cost.
4. We provide a novel decomposition of errors in _Sort & Search_ into largely independent sub-components (aleatoric and epistemic errors).
5. We prove and empirically validate that our solution for the _Search_ sub-component reaches the optimal solution and our framework is stable under repeated additions without any degradation.

## 2 Lifelong Model Evaluation: Formulation and Challenges

We first formalise evaluation in lifelong model evaluation and describe the key challenges it raises.

**Formulation.** Let \(((x_{1},y_{1}),,(x_{n},y_{n}))\) denote an ordered collection of labeled examples, sampled from the underlying task distribution of interest \(P()\). Here, \(x_{i}{}\) denotes the \(i^{}\) data sample and \(y_{i}{}\) denotes the corresponding label. Let \((f_{1},,f_{m})\) denote an ordered collection of models where each model, \(f{:}\), maps data samples to predicted labels. _Lifelong benchmark_, \((,,_{}, _{},)\), augments \(\) and \(\) with three operations:

1. \(_{}((x^{},y^{}))\) inserts a new labeled example \((x^{},y^{})\) into \(\).
2. \(_{}(f^{})\) inserts a new model \(f^{}\) into \(\).
3. \(()\) returns a \(||\)-dimensional vector estimating each model's performance.

**Key challenges.** When new models are proposed, the set \(\) expands over time. Similarly, the sample collection, \(\) expands as new evaluation datasets get proposed to test various aspects of the problem and resist overfitting. The key question becomes: How to efficiently update the benchmark? We can instantiate a "naive" implementation of the \(()\) operation () by simply re-evaluating every model on every sample after each call to \(_{}\) () or \(_{}\) (). However, such a strategy exhibits \(O(||||)\) runtime complexity for each call to \(()\), rendering lifelong model evaluation practically infeasible as \(\) and \(\) grow. The central question considered by this work is therefore the following: _Given a lifelong benchmark \(\), how can we efficiently compute \(()\) each time we insert new labeled samples into \(\) () or new models into \(\) ()?_

**Inserting \( m\) models ()** ()** \(_{}\)). Suppose that \( m\) new models have just been released. We wish to insert these new models into \(\) and efficiently predict performance of these new models. A naive approach would entail evaluating the \( m\) models on all \(||\) samples. Our first challenge is: Can we instead generate the prediction matrix by performing inference only on a small subset of \(n^{}||\) samples? We want to enable accurate prediction of the remaining entries in the prediction matrix.

**Inserting \( n\) samples ()** ()** \(_{}\)). Our second challenge arises when we obtain new \( n\) labeled data examples. We seek to insert these samples into \(\) and efficiently predict performance of these new samples. A naive approach entails evaluating all \(||\) models on the \( n\) new examples. As above, to substantially reduce cost, we select a small subset of \(m^{}||\) models with the objective of accurately predicting the remaining entries of the prediction matrix corresponding to the new \( n\) samples.

**Approach.** Our approach is characterized by two key ideas. First, we augment \(\) with an _instance-level accuracy cache_ to amortise inference costs across evaluations. The cache is instantiated as a matrix \(\{0,1\}^{||||}\) where \((i,j)[f_{i}(x_{j})=y_{j}]\). Second, we propose strategies to efficiently generate the prediction matrix \(\{0,1\}^{||||}\), using a combination of sampling and inference leveraging the accuracy cache. Our methodology is illustrated in Fig. 1.

**Connections to Existing Literature.** The lifelong model evaluation setup, where \(\) and \(\) grow over time, has received limited attention , the sub-challenge of efficiently evaluating models when new models are released has received more focus. Concretely, this maps to the problem of \(_{}\) () within our framework. We comprehensively draw connections across different research directions in Appendix H and briefly present the most similar works here. Model Spider  efficiently ranks models from a pre-trained model zoo. LOVM , Flash-Eval  and Flash-HELM  similarly rank foundation models efficiently on unseen datasets. However, these approaches predict dataset-level metrics rather than instance-level metrics, and thereby cannot be used in our setup to grow the prediction cache efficiently (see Section 2.1). Concurrent to our work, Anchor Point Sampling  and IRT-Clustering  both propose efficient instance-level evaluations by creating smaller core-sets from test data. They introduce clustering-based approaches and item response theory  to obtain sample-wise accuracy predictions. However, their methods require memory and time complexity quadratic in the number of data samples, i.e., \((||^{2})\) requiring well over 10TB of RAM for benchmarks having a million samples. The comparisons are infeasible to scale on datasets bigger than a few thousand samples. In contrast, our novel _Sort & Search_ approach, requires memory and time complexity of \((||||)\) with the number of samples, and can scale up to billion-sized test sets (see Section 4 for empirical results). In practice, our method only requiring only two 1D arrays of size of the number of samples, requiring extremely minimal storage overhead, being less than 3GB in absolute terms on billion scale datasets. Furthermore, we motivate why one should adopt sample-wise prediction instead of overall accuracy prediction below.

### Why Adopt Sample-wise Prediction Metrics instead of Overall Accuracy Prediction?

Given model predictions \(_{m+1}\) and ground-truth predictions \(_{m+1}\), current methods typically measures whether one can predict the average accuracy over the full test, measured by mean absolute difference of aggregate accuracies \(E_{}(_{m+1},_{m+1})=_{m+1} |-|_{m+1}|)|}}{{n}}\). We argue this is highly unreliable as minimizing the metric only requires predicting the count of 1s in the prediction array rather than correctly predicting on a sample level. For instance, consider a ground-truth prediction array of . A method that predicts  as the estimated prediction array achieves optimal \(E_{}\) of 0 despite not predicting even a single sample prediction correctly! More generally, it is always possible to obtain globally optimal \(E_{}\) of 0 while having worst-case mean-absolute error \(E\) for any ground truth accuracy \(a_{m+1}\). Formally,

**Theorem 2.1**.: _Given any ground-truth vector \(_{m+1}\), it is possible to construct a prediction vector \(_{m+1}\) such that \(E_{}(_{m+1},_{m+1})=0\) and \(E(_{m+1},_{m+1})=2.(1-_{m+1 }|}}{{n}},_{m+1}|}}{{n}})\)_

One might wonder whether these worst case bounds ever occur in practice. We empirically test a simple yet optimal array construction, given with oracle ground-truth dataset-level accuracy of \(k^{2}\), which achieves \(E_{}=0\), and consistently observe high mean-absolute error \(E\) of \(0.4{-}0.5\) on a sample level on our lifelong benchmarks (\(n{=}{}10^{6}\)), _i.e._, the model incorrectly predicts \(40{-}50\%\) of the samples in a binary classification task, which is surprisingly high. In comparison, our _S&S_ method, without any oracle access, gets \(0.15{-}0.17\) mean-absolute error with just \(n^{}{=}100\) samples (at \(10,000\)x compute saving) on the same benchmarks. Overall, this demonstrates that thoughtful sample-level prediction mechanisms are necessary for efficient lifelong evaluation.

## 3 _Sort & Search_: Enabling Efficient Lifelong Model Evaluation

Inspired by CAT , we propose an efficient lifelong evaluation framework, _Sort & Search (S&S)_, comprising two components: (1) Ranking test samples from the entire dataset pool according to their difficulty3, _i.e._, _Sort_ and (2) Sampling a subset from the pool to test on, _i.e._, _Search_. This framework effectively tackles the two key operations noted in Section 2 ()insert\({}_{}\) and insert\({}_{}\)).

We first describe our Sort and Search method in the case when new models are added ()insert\({}_{}\)), and subsequently show that the same procedure applies when we have new incoming samples ()insert\({}_{}\)) simply by transposing the cache (\(^{T}\)). A full schematic of our pipeline is depicted in Fig. 2.

### Ranking by Sort

**Setup.** We recall that our lifelong benchmark pool consists of evaluations of \(||\) models on \(||\) samples. For ease of reference, say \(||{=}m\) and \(||{=}n\), and we have our cache \(\{0,1\}^{m n}\) (see Fig. 1 left). We can decompose the cache \(\) row-wise corresponding to each model \(f_{i}\), \(i\{1,..,m\}\), obtaining the binary accuracy prediction across the \(n\) samples, denoted by \(_{i}=[p_{i1},p_{i2},p_{in}]\). Here, \(p_{ij}{}\{0,1\}\) represents whether the model \(f_{i}\) classified the sample \(x_{j}\) correctly.

**Goal.** Given the cache \(\), we want to obtain a ranked order (from easy to hard) for its columns, which represent the samples. This sorted order (_Sort_) can later be used for efficient prediction on new incoming models (_Search_). We want to find the best global permutation matrix \(\{0,1\}^{n n}\), a binary matrix, such that \(\) permutes the _columns_ of \(\) so that we can rank _samples_ from _easy_ (all 1s across models) to _hard_ (all 0s across all models). We say this has a minimum distance from the optimal ranked accuracy prediction matrix \(\{0,1\}^{m n}\) computed by the hamming distance between them, posed as solving the following problem:

\[^{*},^{*}=_{ ,}\|-\|_{1},&_{n}=_{n},_{n}^{}= _{n}\) on binary \(\) enforces by definition that \(\) is a valid permutation matrix. The ranked accuracy prediction matrix \(\) is a binary matrix created by a row-wise application of a thresholding operator for every row in \(\) separately. The informal explanation of the optimization problem in Eq. (1) is to find an ordering of samples such that error introduced by thresholding is minimized.

We next discuss how to solve this optimization. While the goal is finding the optimal permutation \(^{*}\), we still need to jointly solve for \(,\) here. We find a solution by alternating between optimizing \(\) keeping \(\) constant and optimizing \(\) keeping \(\) constant, with the goal of finding the best \(^{*}\), with a coordinate descent algorithm. We now present algorithms for optimizing the two subproblems.

#### 3.1.1 Optimizing \(\) Given \(\)

We know \(\) is binary from Eq. (1). Hence, finding the optimal \(^{*}\) is NP-Hard . To simplify the sub-problem, we first present an algorithm to solve the case where we can order samples in a strictly decreasing order of difficulty, measured by how many models classified it correctly ( ). However, samples cannot be arranged as strictly decreasing in practice. Subsequently, we present an alternative which computes soft confidences, enabling the strictly decreasing constraint to hold ( ). A third alternative we explore removes the introduced constraint of a strictly decreasing order ( ).

**Sorting by Sum.** We discuss how to order samples if they follow a strictly decreasing order of difficulty. We can order samples in decreasing order of difficulty by a simple algorithm detailed in Listing 1 (sort_by_sum)--intuitively, this algorithm greedily sorts samples from easy (more \(1\)s) to hard (less \(1\)s) by sorting the sum vector across rows per column (which can trivially be converted to the permutation matrix \(^{*}\)).

However, the assumption of strictly decreasing order of difficulty is unrealistic as the number of samples is usually far larger than the number of models. Hence, it is guaranteed that many samples will have the same level of difficulty by the pigeonhole principle . We propose to address this by two methods: (a) Converting the cache (A) to store confidence predictions of ground truth class rather than accuracy (Algorithm ), or (b) Iteratively optimizing rows which are tied in sum values (Algorithm ). Note that we find ( ) Sorting by Sum effective in all our tested scenarios, but provide these alternatives in the case where it is insufficient.

**Sorting by Confidence Sum.** One method to have a strictly decreasing order is to relax the constraint on the samples of \(_{i}=[p_{i1},p_{i2},p_{in}]\) from \(p_{ij}\{0,1\}\) to \(p_{ij}\), and use confidence of the ground truth class. This modification allows all examples to be unique. The procedure is then identical to Sorting by Sum, i.e. algorithm still greedily sorts samples from easy (more \(1\)s) to hard (less \(1\)s) by sorting the sum vector across rows per column.

**Recursive Sorting by Sum.** Another alternative is relaxing the equal difficulty assumption in Algorithm 1. A natural question is: _How does one order samples which have equal number of models predicting them correctly, i.e., two columns of \(\) with equal number of \(1\)s?_

Figure 2: **Full Pipeline of _Sort & Search_. For efficiently evaluating new models, _(Left)_ we first sort all data samples by difficulty (refer Section 3.1) and _(Right)_ then perform a uniform sampling followed by DP-search and extrapolation for yielding new model predictions (refer Section 3.2). This entire framework can also be transposed to efficiently insert new samples (refer Section 3.3).

We propose an iterative solution: at each step, order samples of equal difficulty by alternatively optimizing \(\) keeping \(\) constant by applying Algorithm 1 and optimizing \(\) keeping \(\) constant by _DP-Search_ algorithm (presented in the next Section). We provide the algorithm for two iterations for an illustration in Listing 1 (two_stage_sort_by_sum). Note that this strictly improves the solution at each recursion depth. Note that ties are broken by preferring the model which minimizes error the most.

#### 3.1.2 Optimizing \(\) given a \(\)

Optimizing \(\) given a \(\), is equivalent to finding a row-wise threshold \(k n\) minimizing the error with the matrix \(\) for a given \(\). Intuitively, if the threshold for the \(}\) row is \(k\), then the \(}\) row is of the form \([_{k}^{},_{n-k}^{}]\) where \(_{k}\) is a vector of all ones of size \(k\) and \(_{n-k}\) is a zero vector of size \(n-k\). In every row, all samples before the row-wise threshold \(k\) are predicted to be correctly classified (easy) and those after are incorrectly classified (hard) for the model corresponding to the row. To optimize \(\) given \(\), we propose a dynamic programming algorithm, _DP-Search_ which operates on each row \(_{i}\), detailed in Listing 1 (dp_search). Given a row in \(\), DP-Search computes the difference between number of \(1\)s and number of \(0\)s for each index. By using a prefix sum structure, for an input of size \(n\), the DP approach reduces time complexity from \((n^{2})\) to \((n)\). The optimal threshold \(k\) is the index of the maximum value in this vector. The vector \(_{i}\) is simply \([_{k}^{},_{n-k}^{}]\) where \(_{k}\) is a vector of all ones of size \(k\) and \(_{n-k}\) is a zero vector of size \(n-k\). _DP-Search_ is guaranteed to return the globally optimal solution:

**Theorem 3.1**.: _Optimality of \(\) given \(\). For any given \(_{i}\{0,1\}^{1 n}\) and \(\), DP-Search returns an ordered prediction vector \(_{i}\{0,1\}^{1 n}\) which is a global minimum of \(\|_{i}-_{i}\|_{1}\)._

Applying _DP-Search_ independently row-wise, the algorithm returns the optimal \(\) given \(\). Now, we shall

#### 3.1.3 Process Summary

We have outlined the process of optimizing (i) \(\) given \(\) and (ii) \(\) given \(\). Note that (i) alone suffices for Sorting Operation when using the 1 Sorting by Sum algorithm, while a combination of (i) and (ii) is primarily needed for 2 Recursive Sorting by Sum. After sorting, we obtain \(^{*}\), which reflects the sample ordering based on difficulty. In the following section, we will reuse (ii) to search for a \(\) given \(\) to efficiently evaluate new models or add new samples.

### Efficient Selection by Search

**Goal.** After solving Eq. (1), we obtain the optimal \(^{*}\) in the sorting phase. We assume that this sample difficulty order generalizes to new models, \( m\). Recall that \(^{*}\) represents the columns of cache A, ordered by sample difficulty (those most often misclassified by models). Given \( m\) new models, our goal is to predict accuracy across all \(n\) samples for each model, i.e., the accuracy matrix \(_{ m}\{0,1\}^{ m n}\). This would be simple if we could evaluate all \( m\) models on all \(n\) samples, but this approach is costly. The challenge is thus to predict performance on the remaining samples while evaluating only a small subset \(n^{} n\). Hence, we will assume that we can create a smaller ground truth subset \(_{m+1}^{}\) and study: How to find the best accuracy prediction vector \(_{m+1}\)? We use the ground truth vector \(_{m+1}\) for evaluating the efficacy of our method.

Recall that evaluation of every new model can be done independently of others, i.e. \(_{ m}\) is separable per row. Hence, we describe the problem for the first new model \(_{m+1}\{0,1\}^{1 n}\) here.

**(i) How to get the optimal \(_{m+1}\)?** Our goal here is to generate the sample-wise prediction vector \(_{m+1}\{0,1\}^{1 n}\). We divide it into two subtasks: _selection_ and _optimization_. The selection task is to select the best \(n^{}\) observations to sample. The optimization task is, given the \(n^{}\) observations \(_{m+1}^{}\{0,1\}^{1 n^{}}\) how to generate the prediction vector \(_{m+1}\{0,1\}^{1 n}\).

_Subtask 1: How to Select Samples?_ We want to find the best \(n^{}\) observations forming \(^{}\). Note that any ranked solution we obtain using this vector needs to be interpolated from \(n^{}\) points to \(n\) points--we use this intuition to sample \(n^{}\) points. Hence, a simple solution is to sample points such that any threshold found minimizes the difference between the actual threshold and a threshold predicted by our set of \(n^{}\), _i.e._, sample \(n^{}\) points uniformly, providing the algorithm in Listing 1 (uniform_sampling). We also compare empirically with a pure random sampling approach in Section 4.

_Subtask 2: Optimizing \(_{m+1}\)_. Given the \(n^{}\) observations \(^{}_{m+1}\{0,1\}^{1 n^{}}\), how to generate the prediction vector \(_{m+1}\{0,1\}^{1 n}\)? We use the threshold given by _DP-Search_ (Listing 1) and obtain the threshold, given in terms of fraction of samples in \(|^{}_{m+1}|\). We extrapolate this threshold from \(n^{}\) to \(n\) points, to obtain the threshold for the prediction vector \(_{m+1}\). \(_{m+1}\) is simply \([^{}_{k},^{}_{n-k}]\) where \(_{k}\) is a vector of all ones of size \(k\) and \(_{n-k}\) is a zero vector of size \(n-k\).

So far, we have only discussed evaluation of \( m\) new models (). How can we also efficiently extend the benchmark _i.e._ efficiently adding \( n\) new samples ()?

### Efficient Insertion of New Samples (\(_{}\))

To add new samples into our lifelong benchmark efficiently, we have to estimate their difficulty with respect to the other samples in the cache \(\). To efficiently determine difficulty by only evaluating \(m^{} m\) models, a ranking over models is required to enable optimally sub-sampling a subset of \(m^{}\) models. This problem is quite similar in structure to the previously discussed addition of new models, where we had to evaluate using a subset of \(n^{} n\) samples. _How do we connect the two problems?_

We recast the same optimization objectives as described in Eq. (1), but replace \(\) with \(^{}\) and \(\) with \(^{}\). In this case, Eq. (1) would have \(^{}\), which would sort models, instead of samples, based on their aggregate sum over samples (_i.e._, accuracy) optimized using Algorithm 1 to obtain \(^{*}\), ordering the models from classifying least samples correctly to most samples correctly. Here, Algorithm 1 is sufficient, without needing to solve the joint optimization () because accuracies (sum across rows) are unique as the number of samples is typically much larger than the number of models. In case of new incoming samples \( n\), we similarly would treat every sample independently and optimize the predicted array \(^{}_{n+1}\) using _Efficient Selection by Search_ (Section 3.2).

## 4 Experiments

To validate _Sort & Search_ empirically, we showcase experiments on two tasks: _efficient estimation of new sample difficulties_ (\(_{}\)) and _efficient performance evaluation of new models_ (\(_{}\)). We then comprehensively analyse various design choices within our _S&S_ framework.

### Experimental Details

**Lifelong-Datasets.** We combine 31 domains of different CIFAR10-like datasets comprising samples with various distribution shifts, synthetic samples generated by diffusion models, and samples queried from different search engines to form _Lifelong-CIFAR10_. We deduplicate our dataset and downsample images to \(32 32\). Our final dataset consists of 1.69M samples. Similarly, we source test samples from ImageNet and corresponding variants to form _Lifelong-Imagenet_, designed for increased sample diversity (43 unique domains) while operating on the same ImageNet classes. We include samples from different web-engines and generated using diffusion models. Our final _Lifelong-ImageNet_ contains 1.98M samples (see full list of dataset breakdown in Appendix C).

**Model Space.** For _Lifelong-CIFAR10_, we use \(31,250\) CIFAR-10 pre-trained models from the NATS-Bench-Topology-search space . For _Lifelong-ImageNet_, we use \(167\) ImageNet-1K and ImageNet-21K pre-trained models, sourced primarily from \(\) and \(\).

**Sample Addition Split** () \(_{}\)). To study efficient estimation of new sample difficulties on _Lifelong-CIFAR10_, we hold-out CIFAR-10W  samples for evaluation (\(\)\(500,000\) samples) and use the rest \(\)\(1.2\) million samples for sorting. We do not perform experiments for _Lifelong-Imagenet_ since the number of models is quite small (167 in total), directly evaluating all models is relatively efficient, as opposed to the more challenging _Lifelong-CIFAR10_ where evaluation on \(31,250\) models is expensive, practically necessitating reducing the number of models evaluated per new sample.

**Model Evaluation Split** () \(_{}\)). To study efficient evaluation of new models, we split the model set for the _Lifelong-CIFAR10_ benchmark into a randomly selected subset of \(6,000\) models for ordering samples (_i.e., Sort_) and evaluate metrics on the remaining \(25,250\) models (_i.e., Search_). For _Lifelong-Imagenet_, we use \(50\) random models for ordering samples and evaluate on \(117\) models.

**Metrics** ( 3 metrics()).: We measure errors between estimated predictions for each new model \(_{m+1}\) and ground-truth predictions \(_{m+1}\) using mean-absolute error (MAE): \(E(_{m+1},_{m+1})\):

\[E(_{m+1},_{m+1})=_{m+1}^{*} -_{m+1}\|_{1}}}{{n}}\] (2)

### Results: Sample-Level Model Performance Estimation (insert\({}_{}\))

We evaluate the predictive power of _S&S_ for evaluating new models ( 2) when subjected to a varying sampling budgets \(n^{}\)_i.e.,_ we run our _S&S_ over \(13\) different sampling budgets: {8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768} on both _Lifelong-ImageNet_ and _Lifelong-CIFAR10_. Our main results in Sections 4.2 and 4.3 use _Sorting by Sum_ ( 1) for obtaining \(^{*}\) and uniform sampling for the sample budget \(n^{}\). Using this configuration, we now present our main results.

**Key Result 1: Extreme Cost-Efficiency.** From Figs. 3(a) and 3(b), we note our approach converges to a very low mean-absolute error with \(}{{1000}}\) the number of evaluation samples, leading to extreme cost savings at inference time (from 180 GPU days to 5 GPU hours on one A100-80GB GPU)4.

**Key Result 2: Mean Absolute Error Decays Exponentially.** Upon analysing the observed \(E\) vs. \(n^{}\) relationship, we note that exponentially decreasing curves fit perfectly in Figs. 3(a) and 3(b). The exponential decay takes the form \(E{=}ae^{-bx}{+}c\). The fitted curves have large exponential coefficients \(b\) of \(0.04\) and \(0.02\). This further shows the surprisingly high sample-efficiency obtained by _S&S_.

**Key Result 3: Outperforming Baselines by Large Margins.** We construct a competitive, scalable version of Vivek et al.  as a baseline, called _CopyNearest&Expand_: It first samples \(n^{}\) points out of \(n\) (similar to _S&S_ without sorting), and then expands the \(n^{}\)-sized prediction array to \(n\) samples by copying the rest \(n{-}n^{}\) predictions from the nearest neighbor prediction array from the ranking set of models. We note that this baseline is equivalent to removing the _Sort_ component, and only using random sampling. Comparing to the baseline, we see from Fig. 3(c) that our _Sort & Search_ is:

_1) More accurate:_ It achieved 1% lower MAE at a sampling budget of \(n^{}{=}8192\) compared to the baseline, meaning that on average, our _S&S_ correctly classifies \({}19\)k more samples.

_2) Faster convergence: S&S_ converges much faster than the baseline (at \(n^{}{=}1,024\) vs. \(n^{}{=}32,768\)) thereby showcasing the high degree of sample efficiency in converging to the minimal error.

_3) Consistent:_ Fig. 4(b) shows the better consistency of _S&S_, across wider range of models used for _Sort_--at \(n^{}{=}512\), _S&S_ with only \(10\)_Sort_-models still outperforms the baseline using \(50\)_Sort_-models.

**Storage Efficiency.** Storage Efficiency. Our method (S&S) achieves high storage efficiency, requiring only two 1D arrays: one to store the sort-sum and another to construct the current search output. This results in minimal storage overhead, amounting to just 0.0166% of the input data or less than 100 MB in absolute terms. Consequently, _Sort&Search_ not only outperforms alternative methods, such as CopyNearest&Expand, but is also far more memory-optimized.

Figure 3: **Main Results.** _(a,b)_ We achieve 99% cost-savings for new model evaluation on _Lifelong-ImageNet_ and _Lifelong-CIFAR10_ showcasing the efficiency (MAE decays exponentially with \(n^{}\)) of _Sort&Search_. _(c) S&S_ is more efficient and accurate compared to the baseline on _Lifelong-ImageNet_. _(a)_ _Lifelong-CIFAR10_ (b)_Lifelong-ImageNet_ (c)_Baseline Comparison_

### Results: Sample Difficulty Estimation (insert\({}_{}\))

We next showcase results for the task (1) where for new samples, the goal is to sub-sample the number of models to evaluate, for accurately determining sample difficulty. We present results on _Lifelong-CIFAR10_, with two different methods for ranking models5, _Sorting by Sum_ (1) and _Sorting by Confidence Sum_ (1). We evaluate over \(9\) model budgets \(m^{}\) (number of models evaluated over): \(\{8,16,32,64,128,256,512,1024,2048\}\). From Fig. 4(a), we observe that both methods converge quickly--_Sorting by Sum_ (1) reaches an MAE < 0.15 by only evaluating on \(m^{}{=}64\) models out of \(31,250\) (\(10^{4}\) computation savings). This demonstrates our method's ability to efficiently determine sample difficulty, enabling efficient insertion back into the lifelong-benchmark pool.

### Breaking down _Sort & Search_

**Varying the Number of _Sort_-Models Used.** In Fig. 4(b), we analyse the effect of the number of models used for computing the initial ranking (_i.e._, \(m\)) on the final performance on _Lifelong-ImageNet_. Using more models improves MAE-- using lesser models for ranking (\(m{=}10\)) converges to a higher MAE (2% difference at convergence when using \(m{=}50\) (blue line) vs. \(m{=}10\) (red line)). Note that the \(m\) used for ranking does not have any effect on the speed of convergence itself (all methods roughly converge at the same sampling budget (\(n^{}{=}2,048\))), but rather only on the MAE achieved.

**Different Sorting Methods.** We compare the three different algorithms on _Lifelong-Imagenet_: 1_Sorting by Sum_, 2_Sorting by Confidence Sum_, and 1_Sorting by Recursive Sum_. From Fig. 4(c), we note an MAE degradation when using the continual relaxation of the accuracy prediction values as confidence values, signifying no benefits. However, using the multi-step recursive correction of rankings (1) provides significant boosts (0.5% boost in MAE at all \(n^{}{>}1,024\)) due to its ability to locally correct ranking errors that the global sum method (1) is unable to account for.

**Different Sampling Methods.** In Fig. 4(d), we compare methods used for sub-selecting the data-samples to evaluate--we compare _uniform_ vs. _random_ sampling. Both methods converge very quickly and at similar budgets to their optimal values and start plateauing. However, uniform sampling provides large boosts over random sampling when the sampling budget is small (5% lower MAE at \(n^{}{=}8\))--this can be attributed to its "diversity-seeking" behaviour which helps cover samples from all difficulty ranges, better representing the entire benchmark evaluation samples rather than an unrepresentative random set sampled via random sampling.

### Decomposing the Errors of _S&S_

Here, we showcase a decomposition of the errors of _Sort & Search_. Specifically, the total mean absolute error \(E(_{m+1},_{m+1})\) can be decomposed into a component irreducible by further sampling, referred to as the Aleatoric Sampling Error (\(E_{}\)), and a component which can be improved by querying larger fraction of samples \(n^{}\), referred to as the Epistemic Sampling Error (\(E_{}\)).

**Aleatoric Sampling Error.** Let \(_{m+1}^{*}=^{}\) when \(n^{}=n\), _i.e._, it is the best prediction obtainable across all subsampled thresholds, as we have access to the full \(_{m+1}\) vector. However, some error remains between \(^{*}\) and \(_{m+1}\) due to the ordering operation (_i.e._, _Sort_). This error, caused by errors

Figure 4: _(a)_ We achieve accurate sample difficulty estimates on _Lifelong-CIFAR10_ (\(<\)0.15 MAE) at a fraction of the total number of models to be evaluated, thereby enabling cost-efficient sample insertion. _(b,c,d)_, We analyse three design choices for better understanding _S&S_, using _Lifelong-Imagenet_.

in the generalization of the permutation matrix \(^{*}\) cannot be reduced by increasing the sample budget \(n^{}\). More formally, we define this error as:

\[E_{}(_{m+1},_{m+1})=_{_{m+1}} \|_{m+1}^{*}-_{m+1}\|=\|_{m+1}^{*}-_{m+1}^{*}\|.\] (3)

**Epistemic Sampling Error.** Contrarily, there is a gap between optimal ranking prediction \(_{m+1}^{*}\) and \(_{m+1}\) with the current sample size \(n^{}\). This gap, Epistemic Sampling Error, is formally defined as:

\[E_{}(_{m+1}^{*},_{m+1})=\|_{m+1 }^{*}-_{m+1}\|.\] (4)

**Results.** We analyse sampling effectiveness in _Lifelong CIFAR-10_ and _Lifelong-ImageNet_ by studying the Epistemic Sampling Error (\(E_{}\)) and Aleatoric Sampling Error (\(E_{}\)) in Figure 5. First, we see that the epistemic error is very low and quickly converges to 0, _i.e._, we converge to the best achievable performance within sampling just 100 to 1000 samples on both datasets. The remaining error after that is irreducible. We attribute it primarily caused by generalization gaps in the global permutation matrix \(^{*}\) as better approximations like _Recursive Sum_ () did not improve performance as shown in Fig. 4(c). This introduces an interesting question: Do models follow a single global ranking order or are they better decomposed into different rank orders?

**How consistently do models follow one single global ranking order?** We present a detailed analysis in Appendix E to verify this. We calculated the cross-correlation matrix for predictions from 167 models across the entire _Lifelong-Imagenet_ benchmark (1.9M test samples). Surprisingly, _all model pairs showed positive correlations_ to varying degrees, with _no pairs being anti-correlated_. Models with near-zero correlations had near-random performance, indicating uncorrelated predictions due to their randomness. Top-performing models exhibited slightly higher correlations. Overall, there was no clear evidence of model cliques. This analysis strongly suggests that model predictions are highly correlated, justifying our choice of using a single ranking function, but the ranking is simply noisy.

## 5 Conclusion

In this work, we address the efficient lifelong evaluation of models. To mitigate the rising evaluation costs on large-scale benchmarks, we proposed an efficient framework called _Sort & Search_, which leverages previous model predictions to rank and selectively evaluate test samples. Our extensive experiments, involving over 31,000 models, demonstrate that our method reduces evaluation costs by 1000x (over 99.9%) with minimal impact on estimated performance on a sample-level. We aim for _Sort & Search_ to inspire the development of more robust and efficient evaluation methods. Our findings show that model predictions are highly correlated, supporting our use of a single ranking function, though the ranking is somewhat noisy. Our analysis of _Sort & Search_ suggests that future research should focus on generalizing beyond a single rank ordering, rather than on better sampling strategies. Overall, we hope _Sort & Search_ enables large reductions in model evaluation cost and provides promising avenues for future work in lifelong model evaluation.

Figure 5: **Error Decomposition Analysis on _Lifelong-CIFAR10 (left)_ and _Lifelong-ImageNet (right)_. We observe that epistemic error (solid line) drops to 0 within only 100 to 1000 samples across both datasets, indicating this error cannot be reduced further by better sampling methods. The total error \(E\) is almost entirely irreducible (Aleatoric), induced because new models do not perfectly align with the ranking order \(^{*}\). This suggests _generalizing beyond a single rank ordering_, _not better sampling strategies_, should be the focus of subsequent research efforts.**