# Emma-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning

Ping Guo\({}^{@sectionsign,@sectionsign}\) Xiangpeng Wei\({}^{,*}\) Yue Hu\({}^{@sectionsign,@sectionsign,*}\) Baosong Yang\({}^{}\) Dayiheng Liu\({}^{}\) Fei Huang\({}^{}\) Jun Xie\({}^{}\)

\({}^{}\)Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China

\({}^{@sectionsign}\)School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China

\({}^{}\)Machine Intelligence Technology Lab, Alibaba DAMO Academy, Hangzhou, China

{guoping, huyue}@iie.ac.cn, pemywei@gmail.com

Corresponding Author.

###### Abstract

Expressing universal semantics common to all languages is helpful in understanding the meanings of complex and culture-specific sentences. The research theme underlying this scenario focuses on learning universal representations across languages with the usage of massive parallel corpora. However, due to the sparsity and scarcity of parallel data, there is still a big challenge in learning authentic "universals" for any two languages. In this paper, we propose Emma-X: an **EM**-like **M**ultilingual pre-training **A**lgorithm, to learn **(X)**Cross-lingual universals with the aid of excessive multilingual non-parallel data. Emma-X unifies the cross-lingual representation learning task and an extra semantic relation prediction task within an EM framework. Both the extra semantic classifier and the cross-lingual sentence encoder approximate the semantic relation of two sentences, and supervise each other until convergence. To evaluate Emma-X, we conduct experiments on **xrete**, a newly introduced benchmark containing 12 widely studied cross-lingual tasks that fully depend on sentence-level representations. Results reveal that Emma-X achieves state-of-the-art performance. Further geometric analysis of the built representation space with three requirements demonstrates the superiority of Emma-X over advanced models 2.

## 1 Introduction

Research on how to express universal semantics for natural languages (metaphorically as "alphabet of human thoughts" by Leibniz and von Leibniz ) has lasted a long time. Usually, these universal meanings underlying all human natural languages are referred to as irreducible semantic cores . These common cores across languages can serve as a bridge, to help better understand the exact meanings of complex sentences in different languages.

In the context of computational linguistics, various works  have led to great improvements on learning cross-lingual universal representations with the usage of parallel corpora, and verify that multilingual universality contributes a major performance on cross-lingual understanding. However, due to the sparsity and scarcity of parallel data, these advanced techniques face a big challenge in learning real universality among all languages. For instance, among the widely-available top 100 languages that theoretically can build 4950 language pairs, only about 200 language pairs have considerable parallel data .

2022). Recently, Large Language Models (Llms) (e.g., DaLM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), BLOOMZ (Workshop et al., 2023), ChatGPT, etc.) have reached a milestone in the field of Natural Language Processing, for their promising capability at understanding and following complex natural language instructions in different languages. By modeling a wide variety of sentence samples in discrete sentence space, Llms can capture some universal linguistic phenomena to gain cross-lingual transferability. This is consistent with our goal of building a universal basement that supports all languages. The difference lies in that we achieve it through learning universal continuous representations across different languages.

Concretely, we propose Emma-X to tackle the above challenge from a continuous perspective. Emma-X can learn cross-lingual universal sentence representations with excessive non-parallel multilingual data by unifying two highly dependent tasks in an EM (Moon, 1996) framework: semantic relation classification and cross-lingual universal representation learning. For the former, we introduce a Gaussian Mixture Model (Everitt and Hand, 1981) classifier (**GMM classifier**) to deal with the key challenge of forming positive sentence pairs for non-parallel multilingual corpora, by annotating the semantic relationship of sentence pairs in any two arbitrary languages on the fly. For the latter, we employ **a cross-lingual encoder** to learn universal sentence representations via contrastive learning, where positive pairs are chosen by GMM classifier. Further, we construct training signals according to the output of the cross-lingual encoder, to inversely supervise GMM classifier. From the perspective of EM algorithm, in E-step, both modules try to approximate the semantic relationship given a sentence pair sampled from two arbitrary languages. One module is supervised by the approximation of the other to build its own expectation. In M-step, two modules update their parameters by maximizing expectations, respectively. We give a theoretical justification about how these two tasks can be interpreted from an EM perspective (Section 4).

To incentivize the research of universal sentence representation learning, we form a Cross-lingual REpresentation Transfer Evaluation (**x****rete**) benchmark, which includes 12 cross-lingual tasks covering more than 50 languages. xrete fully depends on sentence-level representations. Experimental results demonstrate that Emma-X significantly outperforms pre-trained language models (Conneau et al., 2020; Chi et al., 2021) by 32% at most on xrete. We also perform an evaluation of ChatGPT on xrete to explore its multilingual performance. Detailed analysis also shows that Emma-X can mitigate the representation discrepancy between head and massive long-tail languages. We further conduct geometric analysis directly on representation space from three perspectives: _Invariance_(Abend and Rappoport, 2017), _Canonical Form_(Teller, 2000) and _Isotropy_(Mu and Viswanath, 2018), which provides a further understanding of the cross-lingual transferability of these models.

## 2 Preliminaries

Cross-lingual representation learning aims at mapping sentences from different languages into a unified continuous space, where synonyms across different languages are pulled closer. Given a sentence \(\), the representation is formulated as

\[^{()}=fg(; _{}), \]

where \((;_{})\) denotes the encoder network with a set of trainable parameters \(_{}\), which is typically implemented as a transformer encoder architecture (Vaswani et al., 2017; Devlin et al., 2019; Lee et al., 2022; Feng et al., 2022). \(f()\) is L-2 normalization and \(g()\) is the aggregate function. We take the final hidden states of "[CLS]" token as the aggregate sentence representation.

To learn reasonable representations that can express universal semantics across different languages, various well-designed techniques have been applied to \(^{()}\). A predominant one is to build contrastive learning (CTL) (Saunshi et al., 2019) objective with parallel corpora. The basic idea is to maximize the similarity between representations (i.e., \(^{()}\) and \(^{()}\)) of two semantically-equivalent sentences \((,)\), while keep randomly sampled irrelevant ones \(^{(^{})}\) away. Formally, assume \(\) to be a batch of multilingual parallel bitexts, the contrastive loss under InfoNCE (Oord et al., 2018) formulation is

\[_{}=-)},^{( ^{})})}}{e^{s(^{()},^{()})}+ _{^{},^{}}e^{ s(^{()},^{(^{})})}}, \]

where \(s()\) is implemented as the cosine similarity \(s(^{()},^{(^{})})=)}^{()}}{\|^{()}\|\|^{()}\|}\), \(\) and \(^{}\) are typically called positive and negative samples.

## 3 Methodology

We propose Emma-X that fully exploits massive monolingual data to learn cross-lingual universal representations. As illustrated in Figure 1, Emma-X consists of two modules: 1) A GMM classifier \((;_{})\) to approximate the semantic relation of non-parallel sentences. 2) A cross-lingual encoder \((;_{})\) to convert multilingual sentences into universal representations. For optimization, Emma-X unifies these two modules in an EM framework with dual supervision. In this section, we begin with a definition of the semantic relation rank (SS3.1). Then, we introduce model initialization (SS3.2) and the proposed training paradigm (SS3.3), followed by a dual supervision strategy (SS3.4). For a clearer presentation, an Algorithm of Emma-X is shown in Algorithm 1.

### Rank of Semantic Relations

Mainstream methods model semantic relations with a strict binary separation: positives and negatives. However, the boundary between positives and negatives is blurry, and many samples cannot be clearly classified as either positives or negatives. So it cannot maximize the potential of models to perceive more subtle semantic changes. Also, a binary separation will lead to far more negative samples than positive ones (imbalanced data). To more accurately capture the semantic relation between two sentences and alleviate imbalanced problem, we subdivide the relation into \(N\) semantic ranks, where the semantic similarity of each rank decreases as \(N\) increases, e.g., \(c=1\) denotes two sentences are paraphrases of each other, while \(c=N\) implies sentences are irrelevant. In practice, we set \(N\) to 4.

### Model Initialization

In Emma-X, the GMM classifier \((;_{})\) and cross-lingual encoder \((;_{})\) are initialized by training with massive parallel corpora, respectively.

**Initialization of Cross-lingual Encoder.** It is initialized with Xlm-r (Conneau et al., 2020) and then continuously trained with InfoNCE (Oord et al., 2018) loss by Eq.2. Following Hictl(Wei et al., 2021) and InfoXLM (Chi et al., 2021), we treat the parallel sentence pairs as the query sentence \(\) and its positive counterpart \(\), while treating the randomly sampled sentence as a negative one \(^{}\).

**Initialization of GMM Classifier.** A reasonable solution to warm up the GMM classifier is to use the available cross-lingual parallel corpora as training signals. Suppose \(\) and \(\) are parallel sentences,

Figure 1: Illustration of Emma-X, involving two modules (GMM classifier and Cross-lingual Encoder) that are reciprocated from each other and are updated alternatively. \(\) means the current instance, \(\{_{1},_{2},_{3},...\}\) are samples in various languages for comparison. \(^{()}\) is the continuous representation given a discrete sentence \(\). \(c^{*}_{}\) and \(c^{*}_{}\) formulate the semantic ranks approximated according to Eq. 10 and Eq. 9, to supervise the GMM classifier and cross-lingual encoder, respectively.

and \(^{}\) is an outlier. We set the semantic ranks for \((^{()},^{()})\) and \((^{()},^{(^{})})\) as \(c=1\) and \(c=N\), respectively, according to the definition described in SS 3.1. To obtain the fine-grained semantic ranks, we design a linear interpolation strategy similar to Wei et al. (2022) and mixup (Zhang et al., 2018), which provides virtual training examples for each semantic rank. Formally,

\[^{()}=(1-)^{( )}+^{(^{})}, \]

where \(\) is sampled from a uniform distribution. We compute \(r=(1-)(c=1)+(c=N)\) as the soft semantic rank for \((^{()},^{()})\), where \(\) means the least integer greater than or equal to the input. The virtual training examples are grouped together with the real parallel corpora to optimize the GMM classifier:

\[_{}=- P_{}(c=1|,; _{})- P_{}(c=N|,^{}; _{})-_{}} P_{}(c=r| ,};_{}). \]

The posterior probability \(P_{}()\) is formulated as

\[P_{}(c=r|,;_{})= _{r}^{()}-^{()}|_{ r},_{r}}{_{j=1}^{N}_{j}_{j}^{( )}-^{()}|_{j},_{j}}, \]

where the Gaussian form \(_{r}\) is assigned with a prior probability \(_{r}\), mean \(_{r}\) and standard deviation \(_{r}\) that are all parameterized by trainable variables, thus \(_{}=\{(_{r},_{r},_{r}) r[1,N]\}\). See **Appendix G** for the specific calculation of Gaussian form \(_{r}\).

### The EM Iteration Framework

After initialization, Emma-X further trains the GMM classifier \((;_{})\) and cross-lingual encoder \((;_{})\) with only multilingual non-parallel data with an EM framework.

**E-Step.** For optimization, we represent a training batch of multilingual non-parallel sentences as \(=\{_{1},_{2},...,_{I}\}\) accompanied by a queue of random sentences as \(=\{_{1},_{2},...,_{K}\}\) for instance comparison. Formally, the expectation for GMM classifier is:

\[_{}(,;_{})=- _{_{i}}\,_{_{k} } P_{}(c=c^{*}_{}|_{i}, _{k};_{}), \]

where \(c^{*}_{}[1,N]\) represents an approximated semantic rank for the combination of anchor \(_{i}\) and another random sentence \(_{k}\), based on the cosine similarity among representations (i.e., \(^{(_{i})}\) and \(^{(_{k})}\)) produced by the cross-lingual encoder (i.e., \((;_{})\)). Please refer to SS3.4 for details.

Correspondingly, the expectation for the cross-lingual encoder can be calculated with contrastive learning, where the positive samples are established by the maximum a posteriori approximation (argmax prediction) \(c^{*}_{}\) given by the GMM classifier. Specifically, we apply ranking InfoNCE (Hoffmann et al., 2022) as the training objective, which recursively takes parallel sentence pairs in each rank (e.g, \(c^{*}_{}\)) as positives and ranks that are larger than \(c^{*}_{}\) as negatives. Formally,

\[_{}(,;_{ })=-_{_{i}}_{k}_{c^{*}_{}=1}}e^{s[^{( _{i})},^{(_{k})}]}}{_{_{k} _{c^{*}_{}[1,N]}}e^{s[^{(_{i})}, ^{(_{k})}]}}+_{k}_{c^{* }_{}=2}}e^{s[^{(_{i})},^{(_{k})}]}}{ _{_{k}_{c^{*}_{}[2,N]}}e^{s[^ {(_{i})},^{(_{k})}]}}\] \[+...+_{k}_{c^{*}_{ }=N-1}}e^{s[^{(_{i})},^{(_{k})}]}}{ _{_{k}_{c^{*}_{}[1-N,N]}}e^{s[ ^{(_{i})},^{(_{k})}]}}, \]

where \(c^{*}_{}[1,N]\) represents a semantic rank approximated by the posteriori of GMM classifier (SS3.4). For simplicity, we omit the temperature term in Eq. 7, and please see **Appendix G** for details.

**M-Step.** We use gradient descent algorithm to update the parameters of each module by optimizing its expectation. At each time step \(t\), where \(\) and \(^{}\) are learning rates for each expectation, formally,

\[^{t+1}_{} ^{t}_{}-_{_{ }}_{}(,;_{ }), \] \[^{t+1}_{} ^{t}_{}-^{}_{ _{}}_{}(,;_{ }).\]```
Input: multilingual parallel and non-parallel corpora Output:\((;_{})\) and \((;_{})\)  Phase 1 ; /* Warm-up two modules with multilingual parallel corpora */ while not convergencedo  Sample a batch of multilingual bitexts \(\); for\((,)\)do  Sample \(^{},^{}\), Build virtual training examples \(}\) with Eq. 3;  Compute \(_{}\) with Eq. 2 to update \(_{}\) and \(_{}\) with Eq. 4 to update \(_{}\);  end while  Phase 2 ; /* Emma-X training with EM framework using only non-parallel corpora */ while not convergencedo E-Step  Sample a batch of multilingual anchors \(\), Queue a batch of random sentences \(\); for\((_{i},_{k})\)do  Approximate semantic rank \(c^{*}_{}\) with Eq. 9 and \(c^{*}_{}\) with Eq. 10 ;  end for  Compute \(_{}(,;_{})\) according to Eq. 6 and \(_{}(,;_{})\) according to Eq. 7; M-Step  Update \(_{}\) and \(_{}\) according to Eq. 8;  end while
```

**Algorithm 1**Emma-X Training Algorithm

### Dual Supervision

The approximated semantic ranks \(c^{*}_{}\) and \(c^{*}_{}\) are critical in Emma-X training algorithm. To preserve their quality, we propose dual supervision: predictions from one module are fed to the other to calculate the expectation. In this section, we explain in detail how we approximate the semantic ranks for GMM classifier and cross-lingual encoder, respectively.

**Approximate Semantic Rank with GMM classifier.** The way to obtain semantic rank with semantic classifier is straightforward. The semantic rank corresponding to the highest probability among multiple Gaussian distributions is chosen as the prediction, which is further used to supervise the cross-lingual encoder \((;_{})\), as illustrated in Eq. 7. Formally,

\[c^{*}_{}=*{argmax}_{r}P_{}(c=r|_{ i},_{k};_{}). \]

**Approximate Semantic Rank with Cross-lingual Encoder.** One common way to calculate sentence relation is to measure the similarity between two real-valued representations. Assuming \(s_{r}\) (a scalar initialized as \(\)) can reflect the general similarity score in semantic rank \(c=r\). Given a random sentence pair \((_{i},_{k})\), it's similarity score is close to \(s_{r}\), the sentence pair is likely to belong to rank \(c=r\). Cross-lingual encoder \((;_{})\) determines the semantic relation for each pair according to

\[c^{*}_{}=*{argmin}_{r}|s(^{(_{i})}, ^{(_{k})})-s_{r}|, \]

where \(||\) refers to absolute value. Symmetrically, \(c^{*}_{}\) is used to supervise GMM classifier (Eq. 6).

During the training process, the general similarity score for each semantic rank may vary. Thus, we propose a moving-average strategy to adaptively adjust the value of \(s_{r}\) to simulate this change. Specifically, at time step \(t\), \(s_{r}\) is updated by cosine similarity of all the sentence pairs, which are currently categorized into the rank \(c=r\) according to the cross-lingual encoder in Eq. 9.

\[s^{t}_{r} s^{t-1}_{r}+(1-) s(^{( _{i})},^{(_{k})}),\ _{k} _{c^{*}_{}=r}. \]

Here \(\) is a momentum coefficient to make \(s_{n}\) evolve smoothly during training.

## 4 Theoretical Analysis

In this section, we provide theoretical justification for Emma-X and demonstrate the mutual influence between two modules with rigorous interpretation from an EM algorithm perspective. We show that under dual supervision, minimizing the positive terms in Eq. 7 intrinsically maximizes the objective of a classical clustering algorithm. For simplicity, we assume that each semantic rank has the same number of sentence pairs \(n\) and represents model parameters with \(\). In Emma-X, we model the semantic relation of sentence pair \((_{i},_{k})\) through a joint distribution \(P(_{i},_{k})\) with the semantic rank \(c\) as a latent variable. Let \(Q(c)\) be a prior distribution over the possible values of semantic ranks. That is \(_{r}Q(c=r)=1,Q(c) 0\). The training goal is to maximize the following likelihood:

\[*{argmax}_{}_{_{i} }_{_{k}} P(_{i},_{k}|) =*{argmax}_{}_{_{i}}_{_{k}}_{r=1}^{N}P(_{i}, _{k},c=r|) \] \[*{argmax}_{}_{_{i} }_{_{k}}_{r=1}^{N}Q(c=r)_{i},_{k},c=r|)}{Q(c=r)}.\]

**E-Step.** To make the inequality hold with equality, we have:

\[Q(c=r)=_{i},_{k},c=r|)}{_{j=1}^{N}P( _{i},_{k},c=j|)}=P(c=r|_{i},_{k},), \]

which is the posterior probability and is approximated by the prediction from GMM classifier. Since each sentence pair \((_{i},_{k})\) belongs to only one semantic rank, we approximate \(Q(c=r)=(c_{}^{*}=r)\), which is a one-hot distribution.

**M-Step.** We try to maximize the likelihood in Eq. 12 under the semantic rank \(c_{}^{*}\) :

\[*{argmax}_{}_{_{i} }_{_{k}}_{r=1}^{N}Q(r)_{i },_{k},r|)}{Q(r)} *{argmax}_{}_{_{i} }_{_{k}}_{r=1}^{N} P(_{i},_{k}|c_{}^{*}=r,) \] \[*{argmax}_{}n(n-1)_{r=1}^{N}_{r}^{2},\]

The above derivation uses the assumption that \(P(_{i},_{k}|c_{}^{*}=r,)_ {r}_{i}-_{k}|_{r},_{r} \), with \(_{r}\) and \(_{r}\) being the mean value and standard deviation of the Euclidean distance between sentence pairs in semantic rank \(r\). Detailed proof of Eq. 14 is in **Appendix G**.

Next, we prove that minimizing the positive terms in expectation \(_{}(,;_{})\) actually equal to maximizing a lower bound of Eq. 14. As we apply dual supervision, data in the contrastive label space also follows the distribution \(_{r}_{i}-_{k}|_{r},_{r}\). Hence, under mild assumptions, we can get:

\[_{}^{+}(,;_{})= n^{2}_{r=1}^{N-1}_{r}^{2}\,<\,n(n-1)_{r=1}^{N}_{r}^{2 }\,\,_{_{i}}_{_{k} } P(_{i},_{k}|), \]

where \(_{}^{+}()\) means the positive terms. In the derivation, we use the intrinsic property of semantic ranks (\(_{1}<_{2}<...<_{N}\)). Detailed proof is in **Appendix G**. Eq. 15 demonstrates that with dual supervision, minimizing the contrastive loss can partially maximize the likelihood in Eq. 12.

## 5 Experiments

To thoroughly evaluate the performance of Emma-X, we conduct experiments on **xrete** benchmark to verify the transfer ability of Emma-X on various cross-lingual downstream tasks with strong baselines (pre-trained models: Xlm-r [Conneau et al., 2020], InfoXLM [Chi et al., 2021], Hictl [Wei et al., 2021], sentence models: LaBSE [Feng et al., 2022], S-BERT [Reimers and Gurevych, 2020]) and ChatGPT in Section 5.2. See **Appendices C** and **D** for details. We further conduct geometric analysis in Section 5.3 to better interpret the cross-lingual transferability in Emma-X.

### Setup

**Corpus & Model.** We collect parallel corpora from CCAligned [El-Kishky et al., 2020], CCMatrix [Schwenk et al., 2021], WMT [Akhbardeh et al., 2021], and MultiUN [Ziemski et al., 2016], involving \(94\) languages with \(3.2\) billion sentence pairs. In addition, we add CC-100 [Conneau et al., 2020]as the large-scale monolingual corpus with about 800 billion sentences that covers 94 languages. The cross-lingual encoder starts from the well-trained Xlm-r large model . The GMM classifier is implemented as a mixture of Gaussian forms, each of which consists of a prior \(^{1}\), a mean \(^{1024}\) and a standard deviation \(^{1024}\), all are trainable variables. We set the total semantic ranks as \(N=4\). The statistics of all data and hyper-parameters are shown in Appendix A.

### xrete Evaluation

**xrete** includes 12 cross-lingual tasks divided into 4 different categories. We report the "translate-train" performance in Table 1 on most tasks but zero-shot performance on BUCC and Tatoeba following . Table 2 presents zero-shot comparisons with sentence models.

**Comparisons with Pre-trained Models.** In Table 1, Emma-X consistently outperforms all baseline models (Xlm-r , Hicli  and InfoXLM ) with 7.97% improvements on average. Specifically, Emma-X achieves 88.1% accuracy on XNLI  and 50.21% accuracy on ANLI  with up to 6.4% improvements than baselines. On the MultiSTS  task, Emma-X achieves an 87.3 correlation score, outperforming several strong baselines by 5.1\(\)21.4, and even achieves comparable performance in the cross-lingual and the monolingual settings (see Appendix F for language-specific results). Furthermore, Emma-X obtains a 67.2 Pearson score on QE  task, which is comparable to the winner on the leaderboard3 without any specific finetuning techniques. As for sentence retrieval, Emma-X consistently outperforms previous strong baselines among all 4 tasks , and demonstrates 2.6%\(\)39.6% improvements over these baselines. Similar results can be found in sentence classification tasks. Emma-X obtains an 81.3% top-1 accuracy averaged on XCOPA , MultiEURLEX  and PAWS-X  tasks, outperforming Xlm-r, InfoXLM and Hicli by 7.0%, 3.9% and 3.5% improvements, respectively. On MultiARC  task, Emma-X shows the lowest error rates among all models. The consistent improvements on all tasks reveal that Emma-X can obtain better universal representations for different natural languages with various

    &  &  &  \\  & XNLI & MultiSTS & QE & LakeQA & Mask-X & BUCC & Tatoeba & XCOPA & Male(UKE) & Male(UKE) & Male(UKE) & Male(UKE) \\  Motives & Acc. (T) & Acc. (T) & Spearman (T) & Pearson (T) & maxP20(T) & maxP20(T) & 1 F1 (T) & Acc. (T) & Acc. (T) & MAE (UKE) & Acc. (T) \\    MultiExtraction \\  } & 55.1\({}^{*}\) & - & 55.8\({}^{*}\) & - & - & 21.6\({}^{*}\) & 38.6\({}^{*}\) & 56.7\({}^{*}\) & 39.2\({}^{*}\) & 86.1\({}^{*}\) & 67.4\({}^{*}\) & 48.2\({}^{*}\) & 81.9\({}^{*}\) \\  & Xlr.7\({}^{*}\) & 77.5\({}^{*}\) & - & - & 21.6\({}^{*}\) & 36.8\({}^{*}\) & 32.5\({}^{*}\) & - & - & - & 80.9\({}^{*}\) \\  & Xlr.8\({}^{*}\) & 49.12\({}^{*}\) & 61.5\({}^{*}\) & 58.7\({}^{*}\) & 40.7\({}^{*}\) & 45.7\({}^{*}\) & 66.0\({}^{*}\) & 57.7\({}^{*}\) & 69.2\({}^{*}\) & 66.6\({}^{*}\) & - & 88.9\({}^{*}\) \\  & Bien.\({}^{*}\) & 84.7\({}^{*}\) & - & - & - & - & 77.6\({}^{*}\) & 61.7\({}^{*}\) & - & - & 92.8\({}^{*}\) \\  & ChaGPaT & 60.9\({}^{*}\) & 41.7 & 68.6 & 60.9 & - & - & - & - & 74.2\({}^{*}\) & 68.7 & 40.2 & 64.2 \\    _Ours re-implementation, translator-train and (uncorable) the revised on English training data and on the data translated to the target language_} \\  &  &  &  &  &  &  &  &  &  &  \\  & Xlr.8\({}^{*}\) & 82.8 & 48.8\({}^{*}\) & 69.3\({}^{*}\) & 62.1 & 40.3 & 48.6 & 67.9 & 59.1 & 71.2 & 66.9 & 44.9 & 90.1 \\  & Bien.\({}^{*}\) & 84.2\({}^{*}\) & 40.10 & 82.2 & 64.1 & 44.9 & 57.1 & 77.4 & 66.2 & 74.6 & 67.7 & 36.2 & 93.0 \\  & Bien.\({}^{*}\) & 85.1 & 40.02 & 81.6 & 64.9 & 46.1 & 54.8 & 77.6 & 65.8 & 74.8 & 68.3 & 38.2 & 92.

topics and domains. We further conduct experiments with ChatGPT on xrete tasks without 4 Retrieval tasks. We list the prompts for each task in **Appendix E**. ChatGPT's zero-shot performance is worse than fine-tuned pre-trained models and the performance gap is very large on most tasks.

**Zero-shot comparisons with sentence models.** Compared with Xlm-r and InfoXLM, which adopt the same amount of training data as Emma-X, Emma-X consistently outperforms Xlm-r and InfoXLM by 73.2% and 25.1% on average, as shown in Table 2. The results further prove the effectiveness of our pre-training technique. Through the reciprocation between GMM classifier and cross-lingual encoder, Emma-X can generate reliable semantic rank for multilingual non-parallel corpora, which can provide more supervision signals than previous baselines. Emma-X even achieves comparable results with strong supervised methods: LaBSE  and S-BERT , which both trained on supervised data. LaBSE is trained on a fine-filtered bilingual corpus with 6B translation pairs (2 times larger than Emma-X), while S-BERT is distilled from a S-BERT model fine-tuned on English NLI, STS datasets, and 50M English paraphrase pairs. Compared with these two methods, Emma-X can achieve the best results on QE and Mewski-X by outperforming S-BERT and LaBSE by 71.7% and 117.8% averaged. Emma-X performs worse than these baselines on MultiSTS and BUCC, for these two tasks only contain rich-resource languages, which already have great deal of parallel data.

**Performance on Long-tail Languages.** One goal of Emma-X is to learn universal sentence representations accommodated for more languages. To better prove this, we report the retrieval accuracy on FLORES-200  and Tatoeba . We reformulate FLORES-200, which contains manual transitions in 204 languages (totaling 3001 sentences) to perform retrieval tasks in the same way as Tatoeba and report the performance in terms of language data scale in Table 3. Details about the separation of languages and FLORES-200 are shown in **Appendices A** and **B**. On head languages, Emma-X performs worse than LaBSE by about 4.6% but outperforms S-BERT by 3.5%. On the long-tail languages, Emma-X can surpass S-BERT by 4.3% averaged on two tasks. Emma-X can even exceed the strongest LaBSE by 2.1% on FLORES. One reason for the superior results on long-tail languages is that for those long-tail languages that have only bi-lingual parallel data with rich-resource languages (often English), Emma-X can provide multi-lingual semantic relation signals for them with arbitrary languages through dual supervision.

### Geometric Analysis

To interpret the advantages of Emma-X, we evaluate the geometric characteristics of it on FLORES-200 dataset  without any fine-tuning. The criteria of three requirements are Invariance, measured with KL-divergence (KL-D) , Canonical Form, measured with Calinski-Harabasz Index (CH-I)  and Isotropy, measured with principal ratio (PR) . Details of them are shown in **Appendix B**. We report the numerical results in Table 4 and visualize each characteristic in Figure 2.

**Invariance & Canonical Form** aim to measure how languages are aligned in the representation space. If the sentence representations are universal, then sentences in different languages should follow a similar distribution, which is measured by invariance in KL-divergence. Similarly, canonical form measures how well semantic equivalent sentences are grouped into one cluster, with a clustering metric (CH-I). In Table 4, S-BERT outperforms other baselines in "Invariance" and "Canonical Form" on head languages. However, Emma-X shows better performance on long-tail languages in these two metrics, which is consistent with Table 3. Figure 2 presents similar results. Among the 20 languages we randomly sampled from FLORES, Emma-X can align 17 languages as shown in Figure 1(d), with "xh, eo, ur" as outliers. In Figure 1(e), 1(g), 1(f) and 1(h), different colors represent different languages. So

    &  &  &  &  \\  &  &  &  &  &  &  &  &  &  \\   Metrics & KL-D (↓) & CH-I (↑) & PR (↑) & KL-D (↓) & CH-I (↑) & PR (↑) & KL-D (↓) & CH-I(↑) & PR (↑) \\  XLM-R (cls) & 0.7356 & 30.19 & 0.3681 & 2.0042 & 7.96 & 0.3686 & 1.6501 & 20.80 & 0.3683 \\ InfoXLM (cls) & 0.4491 & 38.82 & 0.4478 & 1.8555 & 13.02 & 0.4406 & 1.4747 & 31.51 & 0.4665 \\ S-BERT (mean) & **0.4115** & **1082** & 0.4519 & 1.3112 & 44.32 & 0.4414 & **0.9782** & **102.36** & 0.4467 \\  Extra-X (cls) & 0.3603 & 43.52 & **0.5318** & **0.3963** & **46.53** & **0.5732** & 1.1904 & 48.70 & **0.5918** \\   

Table 4: Comparisons with existing methods on FLORES dataset for geometric analysis. “cls” and “mean” represent different pooling strategies to obtain sentence representations.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]