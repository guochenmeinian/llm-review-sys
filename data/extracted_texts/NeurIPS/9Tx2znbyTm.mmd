# Diffused Task-Agnostic Milestone Planner

Mineui Hong, Minjae Kang, and Songhwai Oh

Department of Electrical and Computer Engineering and ASRI

Seoul National University

mineui.hong@rllab.snu.ac.kr, minjae.kang@rllab.snu.ac.kr, songhwai@snu.ac.kr

###### Abstract

Addressing decision-making problems using sequence modeling to predict future trajectories shows promising results in recent years. In this paper, we take a step further to leverage the sequence predictive method in wider areas such as long-term planning, vision-based control, and multi-task decision-making. To this end, we propose a method to utilize a diffusion-based generative sequence model to plan a series of _milestones_ in a latent space and to have an agent to follow the milestones to accomplish a given task. The proposed method can learn control-relevant, low-dimensional latent representations of milestones, which makes it possible to efficiently perform long-term planning and vision-based control. Furthermore, our approach exploits generation flexibility of the diffusion model, which makes it possible to plan diverse trajectories for multi-task decision-making. We demonstrate the proposed method across offline reinforcement learning (RL) benchmarks and an visual manipulation environment. The results show that our approach outperforms offline RL methods in solving long-horizon, sparse-reward tasks and multi-task problems, while also achieving the state-of-the-art performance on the most challenging vision-based manipulation benchmark.

## 1 Introduction

Developing a general-purpose agent that can handle a variety of decision-making problems is a long-standing challenge in the field of artificial intelligence. In recent years, there has been a growing interest in using offline reinforcement learning (RL) to tackle this problem . One of the main advantages of offline RL is its ability to leverage the data comprised of multiple sub-tasks by stitching together the relevant transitions to perform a new and more complex task . This property of offline RL allows an agent to learn diverse behaviors demonstrated by a human or other robots performing similar tasks. In order to stitch the task-relevant sub-trajectories from the undirected multi-task data, the majority of the offline RL methods rely on temporal difference learning which measures the value of the policy by bootstrapping the approximated value functions. However, despite the successes in the online setting which allows an agent to actively collect data , the bootstrapping method often causes instability in offline learning. To address this problem, the offline RL methods require special treatments such as using batch-constrained policies  and penalizing the value of out-of-distribution samples . Nevertheless, these methods often show unstable results depending on the how hyperparameters are tuned .

To overcome the instability and tuning complexity of the offline RL methods, bootstrapping-free methods that directly predict the future trajectories are also presented in recent studies . These methods, called sequence modeling methods, utilize the generative sequence models to predict future states and actions conditioned on the goal state or the sum of rewards along the trajectory. In particular, Janner et al.  and Ajay et al.  propose the methods to predict future trajectories using the denosing diffusion probabilistic models (DDPM), which have strengths in stitching the task relevant sub-trajectories from offline data and generating feasible trajectories under variouscombinations of constraints (_e.g_., maximizing rewards, approaching to a given goal state). As a result, DDPM based models are able to flexibly plan trajectories and show promising results on multi-task learning. However, the inherent high computational cost of the denoising process limits its application to wider areas, such as real-time control and vision-based tasks.

In this paper, we present a method named **D**iffused **T**ask-**A**gnostic **M**ilestone **P**lanner (**DTAMP**) which aims to generate a sequence of _milestones_ using the diffusion model to have an agent to accomplish a given task by following them. By doing so, DTAMP divides a long-horizon problem into shorter goal-reaching problems, and address them through goal-conditioned imitation learning which does not require the unstable bootstrapping method and has minimal hyperparameters to be tuned. Here, we note that a milestone represents a latent vector encoding a state (or an image), which guides an agent to reach the corresponding state. To learn the meaningful milestone representations, we propose a method to train an encoder along with a goal-conditioned actor and critic (Figure 1, Left). By doing so, we expect the encoder to extract the control-relevant features and to embed them into milestones. Then, we present a method to train the milestone planner to predict milestones for a given goal. To this end, we first sample a series of intermediate states at intervals from the trajectory approaching the goal, and encode them into milestones to train a diffusion model to reconstruct them. (Figure 1, Right). Furthermore, we also propose a method to guarantee that the milestone planner predicts the shortest path to the goal, based on the classifier-free diffusion guidance technique .

The main strengths of DTAMP are summarized as follows: 1) The encoder trained through the proposed method encodes control-relevant features into compact latent representations, which enables efficient planning even if the observations are high-dimensional. 2) The diffusion-based planner enables flexible planning for a variety of tasks and takes advantage on multi-task decision-making problems. 3) Using the trained actor as a feedback controller makes an agent robust against the environment stochasticity and makes it possible to plan milestones only once at the beginning of an episode which largely reduces inference time compared to the existing sequence modeling methods. The proposed method is first evaluated on the goal-oriented environments in the D4RL benchmark . Especially, the results on Antmaze environments show that DTAMP outperforms offline RL methods in long-horizon tasks without using the bootstrapping method. Finally, DTAMP is demonstrated on the CALVIN benchmark , which is the most challenging image-based, multi-task environment, and shows the state-of-the-art performance achieving \(1.7\) times higher success rate compared to the second-best method. The results show that our approach can broaden the area of application of sequence modeling, while being superior to offline RL methods.

## 2 Preliminaries

### Goal-conditioned imitation learning

Multi-task decision-making problems are often treated as a goal-reaching problem, assuming that each task can be specified by a goal state [28; 41; 35]. Accordingly, we consider the goal-reaching

Figure 1: **Training process of DTAMP. (Left) DTAMP learns the latent milestones through a goal-conditioned imitation learning manner. (Right) The diffusion model is trained to reconstruct the sequences of milestones sampled from the trajectories in the offline data.**

problem in our work and handle the problem through a goal-conditioned imitation learning manner. We first formulate the problem with a goal-conditioned Markov decision process defined by a state space \(\), an action space \(\), a transition model \(p(s^{}|s,a)\), an initial state distribution \(_{0}\), and a goal space \(\). We assume that there exists a mapping \(f:\), to determine whether the current state \(s\) has reached a given goal \(g\) by determining whether \(f(s)\) equals to \(g\). We further assume that there exists a set of previously collected data \(\) which consists of the trajectories sampled by a behavior policy \((a|s)\). Then, we define goal-conditioned imitation learning by training a goal-conditioned actor \(_{}:\) to predict the most likely action to take for the goal \(g\):

\[_{}\ (a|s,g),sa=_{}(s,g).\] (1)

We note that \((a|s,g)=(g|s,a)(a|s)}{p_{}(g|s)}\), where \(p_{}(g|s,a)\) and \(p_{}(g|s)\) are the finite horizon occupancy measures defined by,

\[& p_{}(g|s,a):=_{s^{+}\{s|f(s)=g \}}p_{}(s^{+}|s,a)p_{}(g|s):=_{a}p_{}(g|s,a)(a|s),\\ &p_{}(s^{+}|s,a)=_{(a_{t}|s_{t}),p(s_{ t+1}|s_{t},a_{t})}[_{t=1}^{T}P(s_{t}=s^{+}|s_{0}=s,a_{0}=a) ].\] (2)

Then the objective in (1) can be reformulated as a loss function defined by,

\[J(s,g;):=- p_{}(g|s,a)-(a|s),a=_{}(s,g).\] (3)

We further note that the presented framework is highly related to the existing offline RL methods. Intuitively, the first term \( p_{}(g|s,a)\) in (15) is similar to the \(Q\)-functions used in the studies that address goal-conditioned RL problems [6; 7], and the second term \( p_{}(a|s)\) works as a regularization to prevent the policy from taking out-of-distribution actions [32; 11; 25].

### Denosing diffusion probabilistic models

A denoising diffusion probabilistic model (DDPM)  is a generative model which learns to reconstruct the data distribution \(q()\) by iteratively denoising noises sampled from \((,)\). The diffusion models consist of the _diffusion process_ which iteratively corrupts the original data \( q()\) into a white noise, and the _denoising process_ which reconstructs the data distribution \(q()\) from the corrupted data. First, in the diffusion process, a sequence of corrupting data \(^{1:N}=\{^{1},^{2},,^{N}\}\) is sampled from a Markov chain defined by \(q(^{n+1}|^{n})=(}^ {n},_{n})\), where \(_{n}(0,1)\) is a variance schedule and \(n=0,1,,N\) indicates each diffusion timestep. Then the denoising process aims to learn a denoising model \(p_{}(^{n-1}|^{n})\), to sequentially denoise the noisy data \(^{N}(,)\) into the original data \(^{0}\).

The denoising model \(p_{}(^{n-1}|^{n})\) can be trained by minimizing the following variational bound:

\[_{q}[D_{}q(_{N}|_{0})\|p( _{N})+_{n=2}^{N}D_{}q(^{n-1}| ^{n},^{0})\|p_{}(^{n-1}|^{n}) - p_{}(^{0}|^{1})].\] (4)

However, the variational bound (4) is often reformulated to a simpler rescaled loss function using a function approximator \(_{}\) to predict the noise injected into the original data.

\[J(^{0},n;):=_{(,)}[\|-_{}( _{n}}^{0}+_{n}},n)\|^{2}], _{n}=_{m=0}^{n}(1-_{m}).\] (5)

The surrogate loss function (5) is known to reduce the variance of the loss and improve the training stability .

## 3 Diffused task-agnostic milestone planner

In this section, we introduce diffused task-agnostic milestone planner (DTAMP). We first present the goal-conditioned imitation learning method for simultaneously training the encoder, the actor, and the critic functions. Secondly, the method to train the diffusion model to plan milestones is presented. In addition, we also propose a diffusion guidance method to let the milestone planner predict the shortest path to reach a given goal. Finally, we present a sequential decision-making algorithm combining the goal-conditioned actor and the milestone planner. Note that more detailed explanation about our implementation of DTAMP is presented in Appendix D.

### Goal-conditioned imitation learning from offline data

We first introduce our approach to train the goal-conditioned actor \(_{}\), using a discriminative function \(D_{}:\) as the critic function to estimate the log-likelihood \( p_{}(g|s,a)\) in (15). Here, we assume that a goal is designated by a single state \(s_{}\) which can be a raw sensory observation or an image. In order to extract the underlying goal representation \(g\) from the given goal state, we also train the encoder \(f_{}:\) alongside the actor and critic, where \(\) is a latent goal space. Note that the more discussion about how the encoder is trained through imitation learning and visualization of the learned goal space are presented in Appendix G.

To estimate the log-likelihood \( p_{}(g|s,a)\) in (15), we first train the discriminative critic \(D_{}\) to minimize the following cross-entropy loss:

\[& J_{}(s,a,s^{+},s^{-};,):=-  D_{}(s,a,f_{}(s^{+}))-(1-D_{}(s,a,f_{}(s^{-} ))),\\ &s p_{}(s),\ a(a|s),\ s^{+} p_{}(s^{ +}|s,a),\ s^{-} p_{}(s^{-}).\] (6)

We note that \(p_{}(s)\) is a prior distribution over the state space computed by \(_{s_{0}}p_{}(s|s_{0})_{0}(s_{0})\), which is estimated by a uniform distribution over the all data points of the offline data. Intuitively, for a given state-action pair \((s,a)\), the positive sample \(s^{+}\) is sampled from the future states comes after \((s,a)\), and the negative sample \(s^{-}\) is sampled uniformly across the entire data distribution. As a result, we expect the critic to be trained to distinguish between states that are more likely to come after the current state-action pair. This intuition can be formally formulated with the following proposition.

**Proposition 3.1**.: _The optimal critic function \(D^{*}_{}\) that minimizes the cross-entropy loss (16) satisfies the following equation for a given goal \(g=f_{}(s_{})\)._

\[D^{*}_{}(s,a,g)=(g|s,a)}{p_{}(g|s,a)+p_{}(g)}.\] (7)

Proof.: Proof of Proposition A.1 is presented in Appendix A. 

Using Proposition A.1, we estimate \(p_{}(g|s,a)\) for a given goal \(g\) with the trained critic function.

\[p_{}(g|s,a)=p_{}(g)(s,a,g)}{1-D_{}(s,a,g)}.\] (8)

Now, the remaining part is to estimate \( p_{}(a|s)\) in (15) to train the policy \(_{}\). However, although some previous studies try to estimate the marginal likelihood using conditional generative models  or density estimation methods [47; 24], they require additional computational cost while not showing the significant improvement in performance . Therefore, we circumvent this issue by replacing \( p_{}(a|s)\) in the objective (15) into a simple behavior-cloning regularization term. This regularization has been shown to be an effective way to stabilize offline policy learning requiring minimal hyperparameter tuning . Consequently, the training objective for the policy function is defined as the following loss function:

\[& J_{}(s,,s^{+};,):=- (s,,f_{}(s^{+}))}{1-D_{}(s,,f_{}(s^{+}))}+\|-a\|^{2},\\ &s p_{}(s),\ a(a|s),\ s^{+} p_{}(s^{+}| s,a),\ =_{}(s,f_{}(s^{+})).\] (9)

where \(\) is a hyperparameter which balances the weight between the maximizing log-likelihood \( p_{}(f_{}(s^{+})|s,)\) and the regularization.

Please note that training the actor and the critic through the proposed goal-conditioned imitation learning method does not require temporal difference learning (or bootstrapping). Meanwhile, as the actor is trained only for the positive goals, it has its limitation in inferring a suitable action for a negative goal that has never been reached from the current state in the data collection process. To overcome this limitation, we further propose a diffusion-based milestone planner which predicts milestones to guide an agent to reach the distant negative goal state, in the next section.

### Diffusion model as a milestone planner

We now introduce our method to train the diffusion model as a milestone planner to guide the actor to reach the distant goal state. Intuitively, this can be done by letting the diffusion model to predict intermediate sub-goals that should pass. To this end, we first sample from offline data a pair of initial and goal state \((s_{0},s_{})\) and \(K\)-intermediate states \(\{s_{},,s_{K}\}\) at interval \(\), where \(s_{(K+1)}=s_{}\). Then the sampled intermediate states are encoded into a series of latent milestones \(_{1:K}:=\{g_{1},g_{2},,g_{K}\}\), such that \(g_{k}=f_{}(s_{k})\) for \(k=1,2,,K\). Then, the diffusion model is trained to reconstruct \(_{1:K}\) by minimizing the following loss function:

\[J_{}(_{1:K},n,s_{0},s_{};):= _{(,)}[\|-_{}(_{n}}_{1:K}+_{ n}},n|s_{0},s_{})\|^{2}].\] (10)

Then, the trained milestone planner can predict milestones for a given pair of initial and goal state \((s_{0},s_{})\) by sequentially denoising a series of noisy milestones \(_{1:K}^{N}(,)\):

\[_{1:K}^{n-1}& (_{n-1}}{_{n}}}(_{1:K}^{n}- _{n}}}_{n}),_{n-1}} ),\\ &}_{n}=_{}(_{1:K}^{n},n|s_{0},s_{})n=N,N-1,,1.\] (11)

### Minimum temporal distance diffusion guidance

The denoising process (11) makes it possible to generate milestones for the given goal state, but does not guarantee that the planned trajectory is the shortest path. To address this issue we make an additional modification using classifier-free diffusion guidance . Since we are interested in minimizing the length of trajectories represented by milestones, minimizing the temporal distances between the pairs of successive milestones is a natural choice. Therefore, we modify the diffusion model to be conditioned on the temporal interval \(\) of the sampled sequence of intermediate states \(\{s_{},,s_{K}\}\). The loss function (5) is then extended to the following unconditional loss function \(J_{}\) and conditional loss function \(J_{}\):

\[ J_{}(_{1:K},n;)= _{(,)}[\|-_{}(_{n}}_{1:K}+_{ n}},,n)\|^{2}],\\ J_{}(_{1:K},,n;)=_{ {}(,)}[\|-_{ }(_{n}}_{1:K}+_{n}},,n)\|^{2}].\] (12)

Then, the shortest path planning is done with the guided prediction of \(}_{n}\) in (11):

\[}_{n}=_{}(_{0:K}^{n},, n)+[_{}(_{0:K}^{n},_{},n)-_{}(_{0:K}^{n},,n)],\] (13)

where \(_{}\) denotes the target temporal distance between milestones which is set to be relatively smaller than the maximum value \(_{}\), and \(\) is a scalar value that determines the weight of the guidance. Please note that the conditions on the pair of initial and goal state \((s_{0},s_{})\) are omitted in (12) and (13) for the legibility.

### Sequential decision-making using DTAMP

Using the learned milestone planner, we can predict a series of milestones for a given goal state, and let the agent to follow the milestones using the learned actor function. We note that the goal-conditioned actor works as a feedback controller which can adapt to stochastic transitions. Thisproperty allows to perform the time-consuming denoising process only once at the beginning of an episode. In contrast, the other sequence modeling methods [20; 21; 1] have to predict future trajectories at every timestep, which largely increases inference time. The remaining important question is how to decide whether the agent has reached the current milestone and move on to the next one. We address this issue by setting a threshold \(\) and make the switch if the distance between the current state and the targeted milestone measured in the latent space is less then the threshold. In addition, to make the agent recover from failure to reach a milestone, we also set an internal time limit \(_{}\) and allow the agent to move on to the next milestone when it fails to reach the current one within the time limit. Algorithm 1 summarizes our method for sequential decision-making.

## 4 Experiments

In this section we empirically verify that DTAMP is able to handle long-horizon, multi-task decision-making problems. We first evaluate the proposed method on the goal-oriented environments in the D4RL benchmark , and show that our approach outperforms the other methods on the long-horizon, sparse-reward tasks. Then, we evaluate DTAMP on the most challenging CALVIN benchmark , and show that DTAMP achieves the state-of-the-art performance. Finally, we also present an ablation study to investigate how DTAMP's performance is influenced by the minimum time distance guidance method (13) and the various other design choices of DTAMP. Please note that the detailed experiment settings are presented in Appendix B, and additional experiments on the D4RL locomotion tasks and a stochastic environment are presented in Appendix H.

### Offline reinforcement learning

D4RL benchmarksWe first evaluate DTAMP on the goal-oriented environments in the D4RL benchmark (Antmaze and Kitchen environments) to verify that DTAMP can handle long-horizon tasks without bootstrapping the value function. These tasks are known to be especially challenging to be solved as the useful reward signals only appear when the agent reaches the goal states that are far apart from starting states. The proposed method is compared against offline RL baselines (**CQL**: conservative q-learning , **IQL**: implicit q-learning , **ContRL**: contrastive reinforcement learning ), and the sequence modeling methods (**DT**: decision transformer , **TT**: trajectory transformer , **DD**: decision diffuser ). In addition, we also compare DTAMP to a variant of TT which utilizes a value function trained by IQL for using the beam search algorithm (**TT+IQL**), as proposed by Janner et al. .

The results in Table 1 show that DTAMP outperforms the baseline methods on the every environment. Especially, the other sequence modeling methods (DT, TT, and DD) that do not use temporal difference learning fail to show any meaningful result on the Antmaze environments. The reason for their poor performance is that the goal state of the Antmaze environment is too far from the initial

    & Environment & CQL & IQL & ContRL & DT & TT(+IQL) & DD & **DTAMP** \\    } & medium-play & 61.2 & 71.2 & 72.6 & 0.0 & 0.0 (81.9) & 0.0 & **93.3**\(\)0.94 \\  & medium-diverse & 53.7 & 70.0 & 71.5 & 0.0 & 0.0 (85.7) & 0.0 & **88.7**\(\)3.86 \\  & large-play & 15.8 & 39.6 & 48.6 & 0.0 & 0.0 (64.8) & 0.0 & **80.0**\(\)3.27 \\  & large-diverse & 14.9 & 47.5 & 54.1 & 0.0 & 0.0 (65.7) & 0.0 & **78.0**\(\)8.83 \\   & Average & 36.4 & 57.1 & 61.7 & 0.0 & 0.0 (74.5) & 0.0 & **85.0**\(\)4.84 \\    } & mixed & 52.4 & 51.0 & - & - & - & 65.0 & **74.4**\(\)1.39 \\  & partial & 50.1 & 46.3 & - & - & - & 57.0 & **63.4**\(\)8.80 \\    & Average & 51.3 & 48.7 & - & - & - & 61.0 & **68.9**\(\)6.30 \\   

Table 1: The table shows averaged scores on D4RL benchmarks. DTAMP was trained with three different random seeds, and evaluated with 100 rollouts for each random seed. The source of the baseline performance is presented in Appendix C.

state1 to accurately predict the whole trajectory to the goal. Meanwhile, DTAMP can efficiently plan a long-horizon trajectory by generating temporally sparse milestones.

Multi-goal Antmaze experimentIn addition, we demonstrate DTAMP on Antmaze environments with multi-goal setting to show that DTAMP can flexibly plan the milestones according to the varying goals. In this experiment, a goal for each rollout is randomly sampled from three different predefined target positions including the goals that were not reached from the initial state during the data-collection (see Appendix B for more details). We compare DTAMP against the three baseline methods (**IQL**, **ContRL**, and **TT+IQL**), which achieve high scores in the single-goal setting. We note that DTAMP and ContRL already learn goal-conditioned actors and do not require to be modified for the multi-goal setting. Meanwhile, we modify IQL and TT+IQL by adding hindsight experience replay (HER) . In the multi-goal setting, DTAMP shows the least performance degradation compared to the other methods (Table 2). Especially, TT+IQL shows the largest performance degradation compared to the results evaluated on the single-goal setting. This result indicates that the transformer architecture  used in TT struggles to predict the trajectory to reach a new goal that was not seen in the training time, while DTAMP adaptively plan the milestones by stitching the relevant sub-trajectories.

Comparison of inference timeIn order to evaluate the computational efficiency of the proposed method, we further compare DTAMP against the other methods in terms of inference time it takes to predict a single step of action. The comparative result in Figure 2 shows that our approach largely reduces the computational cost compared to the other sequence modeling methods, while achieving the highest average score. In particular, the inference time of DTAMP is about 1,000 times faster than TT and about 300 times faster than DD. It is because using the actor as a feedback controller makes the agent robust on the stochasticity of the environment, and allows to perform the time-consuming sequence generation process only once at the beginning of an episode. In contrast, the other sequence

    & Environment & IQL+HER & ContRL & TT+IQL & **DTAMP** \\   **ContRL** \\  } & medium-play & 34.7 (-51.3\%) & 58.7 (-19.1\%) & 68.9 (-15.9\%) & **89.3**\(\)3.86 (-4.3\%) \\  & medium-diverse & 50.7 (-27.6\%) & 57.3 (-19.9\%) & 75.6 (-11.8\%) & **76.7**\(\)4.50 (-13.5\%) \\  & large-play & 46.0 (+16.2\%) & 24.7 (-49.2\%) & 28.9 (-55.4\%) & **62.0**\(\)2.94 (-22.5\%) \\  & large-diverse & 42.0 (-11.6\%) & 26.7 (-50.6\%) & 23.3 (-64.5\%) & **53.3**\(\)9.67 (-31.7\%) \\   & Average & 43.4 (-24.0\%) & 41.9 (-32.2\%) & 49.2 (-34.0\%) & **70.3**\(\)5.86 (-17.3\%) \\   

Table 2: The table shows averaged scores on the multi-goal setting of Antmaze environments. The values in the parenthesises indicate performance difference between single-goal and multi-goal settings in percentage.

Figure 2: The plots compare the inference times and the average scores of the different algorithms on the D4RL benchmarks. The \(x\)-axis indicates the inference time in logarithmic scale. The inference times are measured using an NVIDIA GeForce 3060 Ti graphics card.

modeling methods have to predict the future trajectories for every timestep which results in high computational cost.

### Image-based planning

We now evaluate the proposed method on the CALVIN benchmark , which is the most challenging multi-task visual manipulation task. In this experiment, the observations and goals are designated by images, and reaching a goal image may requires the agent to sequentially accomplish multiple tasks. Figure 3 shows an example that three different tasks ('place pink block in drawer','move slider left', and 'turn on led') are implied by a single goal image. We utilize the dataset provided by Mees et al. , which consists of the trajectories performing a variety of manipulation tasks (_e.g._, picking up a block, opening the drawer, pushing the button to turn on the led, etc.) collected by a human teleoperating the robotic arm in the simulation. To investigate how the performance of DTAMP and other baselines varies with the difficulty of the task, we group the pairs of initial and goal states into three groups according to the number of tasks they imply, and evaluate the models on each group separately.

We note that two modifications are added to DTAMP in this experiment, as the CALVIN environment has the partially observable characteristic. First, we additionally train a decoder to reconstruct image observations from the encoded latent milestone vectors in order to let the encoder capture the essential visual features. Second, we utilize skill-based policy as done by Lynch et al.  and Rosete-Beas et al. , to let the agent predict actions based on the current and previous observations. More detailed discussion of how modifications affect the performance of DTAMP will be presented in the ablation study.

In this experiment, our method is compared against an offline RL baseline combined with goal relabelling (**CQL+HER**: conservative q-learning  with hindsight experience replay ) and hierarchical RL methods (**LMP**: latent motor plans , **RIL**: relay imitation learning , **TACO-RL**: task-agnostic offline reinforcement learning ). Furthermore, we also evaluate **DTAMP+Replanning** which allows the agent to re-plan the rest of milestones at every time it reaches a milestone. The results in Table 3 shows that our approach achieves the state-of-the-art performance in the CALVIN environment against existing methods by a significant margin. We also would like note that only DTAMP and DTAMP+Replanning show the meaningful success rates on the hardest setting that a goal image implies three different tasks in a row. The results indicate that our approach enables planning on image-based environments, while also outperforming the other hierarchical RL methods that utilize skill-based policy (LMP and TACO-RL) or generate sub-goals (RIL). Furthermore, we also see that the performance of DTAMP can be further improved by allowing agents to re-plan milestones during an episode.

### Ablation study

**Minimum temporal distance guidance** We present an ablation study to verify that the minimum temporal distance guidance (13) enables DTAMP to plan the shortest path for a given goal. We demonstrate DTAMP on a simplified open Antmaze environment with varying target temporal distance \(_{}\). In order to train DTAMP, one million steps of offline data was collected by a pretrained locomotion policy. Then, the unconditional version (10) and conditional version (12) of DTAMP were trained using the collected data. The trajectories planned by the unconditional model and conditional model are show in Figure 4. As expected, the guidance method enables DTAMP to plan trajectories of varying lengths according to the given target temporal distances, while the

Figure 3: Images on the left show a pair of initial state and goal state, which implies three different tasks. The series of images on the right represents the milestones planned by DTAMP. The image of each milestone is reconstructed using the trained decoder.

unconditional model tends to plan sub-optimal trajectories. In addition, Table 4 shows that the guidance method is able to reduce the number of timesteps required to reach the goal, while it does not affect the success rate.

**Ablation study in image-based planning** In order to apply DTAMP on the image-based planning problems, we made two modifications on DTAMP: 1) Training a decoder to reconstruct images. 2) Utilizing skill-based policy. In this experiment we further investigate how each modification affects to the performance of DTAMP. To this end, we evaluate three variations of DTAMP in the CALVIN benchmark (**DTAMP\(-\)Recon. Loss**: the encoder is trained without reconstruction loss, **DTAMP\(-\)GCIL Loss**: the encoder is trained with only reconstruction loss without goal-conditioned imitation learning loss, **DTAMP\(-\)LMP**: the agent does not utilize skill-based policy). The results shown in Table 5 indicate that using both reconstruction loss and GCIL loss when training the encoder improves DTAMP's performance the most. We also find that ablating skill-based policy causes a large performance degradation.

## 5 Related Work

**Diffusion models**[39; 18; 38] have received active attention recently. There exists a number of applications of diffusion model, such as photo-realistic image generation [5; 34], text-guided image generation [36; 31; 33], and video generation [19; 15; 16; 50]. While the majority of work related to diffusion model is focused on image generation and manipulation, there is also growing interest in applying diffusion model on robotic tasks or reinforcement learning. The methods to plan a trajectory with diffusion models to perform a given task are proposed by Janner et al.  and Ajay et al. . In the meantime, Wang et al.  and Chi et al.  present the methods to utilize diffusion models for predicting actions. Furthermore, Urain et al.  proposes a method to train a cost function alongside with a diffusion model, and to utilize it to select a proper grasping angle. It is worth mentioning that our approach is the first work that utilizes diffusion model to plan trajectories in a learnable latent space and handle visuomotor tasks, to the best of our knowledge.

**Offline reinforcement learning** has also been widely studied to improve data efficiency of reinforcement learning (RL) algorithms [30; 37; 44; 27; 14]. The most of RL methods utilize temporal difference method to estimate the value of current action considering over the further timesteps. However, bootstrapping the estimated values often causes overestimating the value of out-of-distribution actions [10; 11], which makes it challenging to apply RL on fixed offline data. To tackle this issue, Fujimoto et al.  proposes batch constrained policy learning which restricts the policy to output the actions within the data. There also various methods to stabilize the offline RL are proposed, by learning policy through advantage weighted regression , augmenting q-learning objective [26; 25], or model-based reinforcement learning . In addition, applying offline RL on multi-task RL problems is showing promising results in recent years , as it is able to extract various skills from offline data and compose them to accomplish a new challenging task. Accordingly, our work also considers the offline setting that leverages the previously collected data, and mainly focus on designing a method to stitch the separated trajectories in the offline data to accomplish a given task.

## 6 Limitations

**Cannot solve a new task not included in dataset**. In this work, we considered multi-task decision-making problems assuming that the offline data contains the segments of trajectories for performing the tasks we are interested in. However, there may also be cases where an agent needs to find new skills to solve new tasks that were not included in the previously collected data. Exploring the environment using the pretrained model to find new skills and continually learning to perform more challenging tasks would be an interesting topic for the future work.

**Efficient re-planning strategy is needed**. Although the proposed method leverages faster inference time compared to other generative model-based planning methods by allowing the agent to plan milestones only once, our empirical results show that having the agent re-plan the trajectories during episodes can improve performance. Furthermore, in order to apply DTAMP on the problems that their environments are constantly changing, the agent is required to re-plan the trajectories according to the changed environment. In this context, an efficient re-planning strategy to determine when and how to modify the planned path must be further studied to utilize DTAMP for a wider area of robotic tasks.

## 7 Conclusion

This paper presents a method to plan a sequence of latent sub-goals, which is named as milestones, for a given target state to be reached. The proposed method trains an encoder to extract milestone representations from observations, and an actor to predict actions to follow the milestones, through a goal-conditioned imitation learning manner. Our method also utilizes a denosing diffusion model to generate an array of milestones conditioned on a target state. Furthermore, the minimum temporal distance diffusion guidance method is also presented in this paper, to make the proposed method plan the shortest path to reach the target state. Experimental results support that the proposed method makes it possible to plan the milestones adapting to various tasks, and to manage long-term tasks without using the unstable bootstrapping method.