# Leveraging the two-timescale regime

to demonstrate convergence of neural networks

 Pierre Marion

Sorbonne Universite, CNRS,

Laboratoire de Probabilites, Statistique et Modelisation, LPSM,

F-75005 Paris, France

pierre.marion@sorbonne-universite.fr

Raphael Berthier

EPFL, Switzerland

raphael.berthier@epfl.ch

###### Abstract

We study the training dynamics of shallow neural networks, in a two-timescale regime in which the stepsizes for the inner layer are much smaller than those for the outer layer. In this regime, we prove convergence of the gradient flow to a global optimum of the non-convex optimization problem in a simple univariate setting. The number of neurons need not be asymptotically large for our result to hold, distinguishing our result from popular recent approaches such as the neural tangent kernel or mean-field regimes. Experimental illustration is provided, showing that the stochastic gradient descent behaves according to our description of the gradient flow and thus converges to a global optimum in the two-timescale regime, but can fail outside of this regime.

## 1 Introduction

Artificial neural networks are among the most successful modern machine learning methods, in particular because their non-linear parametrization provides a flexible way to implement feature learning (see, e.g., Goodfellow et al., 2016, chapter 15). Following this empirical success, a large body of work has been dedicated to understanding their theoretical properties, and in particular to analyzing the optimization algorithm used to tune their parameters. It usually consists in minimizing a loss function through stochastic gradient descent (SGD) or a variant (Bottou et al., 2018). However, the non-linearity of the parametrization implies that the loss function is non-convex, breaking the standard convexity assumption that ensures global convergence of gradient descent algorithms.

In this paper, we study the training dynamics of _shallow_ neural networks, i.e., of the form

\[f(x;a,u)=a_{0}+_{j=1}^{m}a_{j}g(x;u_{j})\,,\]

where \(m\) denotes the number of hidden neurons, \(a=(a_{0},,a_{m})\) and \(u=(u_{1},,u_{m})\) denote respectively the outer and inner layer parameters, and \(g(x;u)\) denotes a non-linear function of \(x\) and \(u\). The novelty of this work lies in the use of a so-called _two-timescale regime_(Borkar, 1997) to train the neural network: we set stepsizes for the inner layer \(u\) to be an order of magnitude smaller than the stepsizes of the outer layer \(a\). This ratio is controlled by a parameter \(\). In the regime \( 1\), the neural network can be thought of as a fitted linear regression with slowly evolving features\(g(x;u_{j})\), \(j=1,,m\): this reduction enables us to precisely describe the movement of the inner layer parameters \(u_{j}\).

Our approach proves convergence of the _gradient flow_ to a global optimum of the non-convex landscape with a _fixed_ number \(m\) of neurons. The gradient flow can be seen as the simplifying yet insightful limit of the SGD dynamics as the stepsize \(h\) vanishes. Proving convergence with a fixed number of neurons contrasts with two other popular approaches that require to take the limit \(m\): the neural tangent kernel (Jacot et al., 2018; Allen-Zhu et al., 2019; Du et al., 2019; Zou et al., 2020) and the mean-field approach (Chizat and Bach, 2018; Mei et al., 2018; Rotskoff and Vanden-Eijnden, 2018; Sirignano and Spiliopoulos, 2020). As a consequence, this paper is intended as a step towards understanding feature learning with a moderate number of neurons.

While our approach through the two-timescale regime is general, our description of the solution of the two-timescale dynamics and our convergence results are specific to a simple example showcasing the approach. More precisely, we consider univariate data \(x\) and non-linearities of the form \(g(x;u_{j})=(^{-1}(x-u_{j}))\), where \(u_{j}\) is a variable translation parameter, \(\) is a fixed dilatation parameter, and \(\) is a sigmoid-like non-linearity. Finally, we restrict ourselves to the approximation of piecewise constant functions.

Organization of this paper.In Section 2, we detail our setting and state our main theorem on the convergence of the gradient flow to a global optimum. Section 3 articulates this paper with related work. Section 4 provides a self-contained introduction to the _two-timescale limit_\( 0\). We explain how it simplifies the analysis of neural networks, and provides heuristic predictions for the movement of neurons in our setting. Section 5 gives a rigorous derivation of our result. We prove convergence first in the two-timescale limit \( 0\), then in the two-timescale regime with \(\) small but positive. Section 6 presents numerical experiments showing that the SGD dynamics follow closely those of the gradient flow in the two-timescale regime, and therefore exhibit convergence to a global optimum. On the contrary, SGD can fail to reach a global optimum outside of the two-timescale regime.

## 2 Setting and main result

We present a setting in which a piecewise constant univariate function \(f^{*}:\) is learned with gradient flow on a shallow neural network. Our notations are summarized on Figure 1. We begin by introducing our class of functions of interest.

**Definition 1**.: _Let \(n 2\), \( v(0,1)\), \( f>0\) and \(M 1\). We denote \(_{n, v, f,M}\) the class of functions \(f^{*}:\) satisfying the following conditions:_

* \(f^{*}\) _is piecewise constant: there exists_ \[0=v_{0}<v_{1}<<v_{n-1}<v_{n}=1\] _and_ \(f^{*}_{0},,f^{*}_{n-1}\) _such that_ \[ x(v_{i},v_{i+1}),f^{*}(x)=f^{*}_{i},\]
* _for all_ \(i\{1,,n\},v_{i}-v_{i-1} v\)_,_
* _for all_ \(i\{1,,n-1\},|f^{*}_{i}-f^{*}_{i-1}| f\)_,_
* _for all_ \(i\{0,,n-1\}\)_,_ \(|f^{*}_{i}| M\)_._

Let us now define our class of neural networks. Consider \(:\) an increasing, twice continuously differentiable non-linearity such that \((x)=0\) if \(x-1/2\), \((x)=1\) if \(x 1/2\), and \(-1/2\) is odd. Then, our class of shallow neural networks is defined by

\[f(x;a,u)=a_{0}+_{j=1}^{m}a_{j}_{}(x-u_{j})\,, _{}(x)=(^{-1}x)\,,\]where \(0< 1\) measure the sharpness of the non-linearity \(_{}\). Note that that inner layer parameter \(u_{j}\) determines the translation of the non-linearity; no parameterized multiplicative operation on \(x\) is performed in this layer. We refer to the parameter \(u\) as the "positions" of the neurons (or, sometimes, simply as the "neurons") and to the parameter \(a\) as the "weights" of the neurons. We define the quadratic loss as

\[L(a,u)=_{0}^{1}(f^{*}(x)-f(x;a,u))^{2}x\,.\]

We use gradient flow on \(L\) to fit the parameters \(a\) and \(u\): they evolve according to the dynamics

\[a}{t}(t) =-_{a}L(a(t),u(t))\,, u}{t}(t) =-_{u}L(a(t),u(t))\,,\] (1)

where \(\) corresponds to the ratio of the stepsizes of the two iterations.

Main result.By leveraging the two-timescale regime where \(\) is small, our theorem shows that, with high probability, a neural network trained with gradient flow is able to recover an arbitrary piecewise constant function to an arbitrary precision. The proof is relegated to the Appendix.

**Theorem 1**.: _Let \(,>0\), and \(f^{*}\) a piecewise constant function from \(_{n, v, f,M}\). Assume that the neural network has \(m\) neurons with_

\[m4+ n+\,.\] (2)

_Assume that, at initialization, the positions \(u_{1},,u_{m}\) of the neurons are i.i.d. uniformly distributed on \(\) and their weights \(a_{0},,a_{m}\) are equal to zero._

_Then there exists \(Q_{1}>0\) and \(Q_{2}>0\) depending on \(,,m, f,M\) such that, if_

\[ Q_{1}\,,  Q_{2}\,,\] (3)

_then, with probability at least \(1-\), the solution to the gradient flow (1) is defined at least until \(T=}\), and_

\[_{0}^{1}|f^{*}(x)-f(x;a(T),u(T))|^{2}x\,.\]

_Further, \(Q_{1}=}{M^{2}(m+1)}( f)^{2}}{(m+1 )^{4}},\) and \(Q_{2}=^{2}}{M^{4}(m+1)^{17/2}}}{m+1},\) for some universal constants \(C_{1},C_{2}>0\)._

For this result to hold, the inequality (2) requires the number of neurons in the neural network to be large enough. Note that the minimum number of neurons required to approximate the \(n\) pieces of \(f^{*}\) is equal to \(n\). If the length of all the intervals is of the same order of magnitude, then \( v=(1/n)\) and thus the condition is \(m=(n(1+ n+ 1/))\). In this case, condition (2) only adds a logarithmic factor in \(n\) and \(\). Moreover, the lower bound on \(m\) does not depend on the target precision \(\). Thus we observe some non-linear feature learning phenomenon: with a fixed number \(m\) of neurons, gradient flow on a neural network can approximate any element from the infinite-dimensional space of piecewise constant functions to an arbitrary precision.

The recovery result of Theorem 1 is provided under two conditions (3). The first one should not surprise the reader: the condition on \(\) enables the non-linearity to be sharp enough in order to approximate well the jumps of the piecewise constant function \(f^{*}\). The novelty of our work lies in the condition on \(\), that we refer to as the _two-timescale regime_. This condition ensures that the stepsizes taken in the positions \(u\) are much smaller than the stepsizes taken in the weights \(a\). As a consequence, the weights \(a\) are constantly close to the best linear fit given the current positions \(u\). This property decouples the dynamics of the two layers of the neural network; this enables a sharp description of the gradient flow trajectories and thus the recovery result shown above. This intuition is detailed in Section 4.

Related work

Two-timescale regime.Systems with two timescales, or _slow-fast systems_, have a long history in physics and mathematics, see Berglund and Gentz (2006, Chapter 2) for an introduction. In particular, iterative algorithms with two timescales have been used in stochastic approximation and optimization, see Borkar (1997) or Borkar (2009, Section 6). For instance, they are used in the training of generative adversarial networks, to decouple the dynamics of the generator from those of the discriminator (Heusel et al., 2017), in reinforcement learning, to decouple the value function estimation from the temporal difference learning (Szepesvari, 2010), or more generally in bilevel optimization, to decouple the outer problem dynamics from the inner problem dynamics (Hong et al., 2023). However, to the best of our knowledge, the two-timescale regime has not been used to show convergence results for neural networks.

Layer-wise learning rates.Practitioners are interested in choosing learning rates that depend on the layer index to speed up training or improve performance. Using smaller learning rates for the first layers and higher learning rates for the last layer(s) improves performance for fine-tuning (Howard and Ruder, 2018; Ro and Choi, 2021) and is a common practice for transfer learning (see, e.g., Li et al., 2022). Another line of work proposes to update layer-wise learning rates depending on the norm of the gradients on each layer (Singh et al., 2015; You et al., 2017; Ko et al., 2022). However, they aim to compensate the differences across gradient norms in order to learn all the parameters at the same speed, while on the contrary we enjoy the theoretical benefits of learning different speeds.

Theory of neural networks.A key novelty of the analysis of this paper is that we show recovery with a fixed number of neurons. We now detail the comparison with other analyses.

The neural tangent kernel regime (Jacot et al., 2018; Allen-Zhu et al., 2019; Du et al., 2019; Zou et al., 2020) corresponds to small movements of the parameters of the neural network. In this case, the neural network can be linearized around its initial point, and thus behaves like a linear regression. However, in this regime, the neural network can approximate only a finite dimensional space of functions, and thus it is necessary to take \(m\) to be able to approximate the infinite-dimensional space of piecewise constant functions to an arbitrary precision.

The mean-field regime (Chizat and Bach, 2018; Mei et al., 2018; Rotskoff and Vanden-Eijnden, 2018; Sirignano and Spiliopoulos, 2020) describes the dynamics of two-layer neural networks in the regime \(m 1\) through a partial differential equation on the density of neurons. This regime is able to describe some non-linear feature learning phenomena, but does not explained the observed behavior with a moderate number of neurons. In this paper, we show that in the two-timescale regime, only a single neuron aligns with each of the discontinuities of the function \(f^{*}\). However, it should be noted that the neural tangent kernel and mean-field regimes have been applied to show recovery in a wide range of settings, while our work is restricted to the recovery of piecewise constant functions. Extending the application of the two-timescale regime is left for future work.

Our work includes a detailed analysis of the alignment of the positions of the neurons with the discontinuities of the target function \(f^{*}\). This is analogous to a line of work (see, e.g., Saad and Solla (1995); Goldt et al. (2020); Veiga et al. (2022)) interested in the alignment of a "student" neural network with the features of a "teacher" neural network that generated the data, for high-dimensional Gaussian input. In general, the non-linear evolution equations describing this alignment are hard to study theoretically. On the contrary, thanks to the two-timescale regime and to the simple setting of this paper, we are able to give a precise description of the movement of the neurons.

Our study bears high-level similarities with the recent work of Safran et al. (2022). In a univariate classification setting, they show that a two-layer neural network achieves recovery with a number of neurons analogous to (2): inversely proportional to the length of the smallest constant interval of the target, up to logarithmic terms in the number of constant intervals and in the failure probability. However, the two papers have different settings: Safran et al. (2022) consider classification with ReLU activations while we consider regression with sigmoid-like activations. More importantly, the authors do not use the two-timescale regime. Instead, by a specific initialization scale, they ensure that the neural network has a first lazy phase where the positions of the neurons do not move significantly. For the second rich phase, they describe the implicit bias of the limiting point; this approach does not lead to an estimate of the convergence time while the fine description of the two-timescale limit does.

Finally, a related technique is the so-called layerwise training, which consists in first training the inner layer with the outer layer fixed, and then doing the reverse. This setup has been used in theoretical works to show convergence of (stochastic) gradient descent in a feature learning regime with a moderate number of neurons (Abbe et al., 2023; Damian et al., 2022). The two-timescale regime can be seen as a refinement of this technique since we allow both layers to move simultaneously instead of sequentially, which is closer to practical setups.

## 4 A non-rigorous introduction to the two-timescale limit

This section introduces the core ideas of our analysis in a non-rigorous way. Section 4.1 introduces the limit of the dynamics when \( 0\), called the _two-timescale limit_. Section 4.2 applies the two-timescale limit to predict the movement of the neurons.

### Introduction to the two-timescale limit

Let us consider the gradient flow equations (1) and perform the change of variables \(= t\):

\[a}{}=-_{a}L(a,u)\,, u}{}=-_{u}L(a,u)\,.\] (4)

In the two-timescale regime \( 1\), the rate of the gradient flow in the weights \(a\) is much larger than then the rate in the positions \(u\). Note that \(L\) is marginally convex in \(a\), and thus, for a fixed \(u\), the gradient flow in \(a\) must converge to a global minimizer of \(a L(a,u)\). More precisely, assume that \(\{_{}(.-u_{1}),,_{}(.-u_{m})\}\) forms an independent set of functions in \(L^{2}()\). Then the global minimizer of \(a L(a,u)\) is unique; we denote it as \(a^{*}(u)\). In the limit \( 0\), we expect that \(a\) evolves sufficiently quickly with respect to \(u\) so that it converges instantaneously to \(a^{*}(u)\). In other words, the gradient flow system (4) reduces to its so-called _two-timescale limit_ when \( 0\):

\[a=a^{*}(u)\,,u}{}=- _{u}L(a^{*}(u),u)\,.\] (5)

The two-timescale limit considerably simplifies the study of the gradient flow system because it substitutes the weights \(a\), determined to be equal to \(a^{*}(u)\). However, showing that (4) reduces to (5) requires some mathematical care, including checking that \(a^{*}(u)\) is well-defined.

**Remark 1** (abuse of notation).: _Equation (5) contains an ambiguous notation: does \(_{u}L(a^{*}(u),u)\) denote the gradient in \(u\) of the map \(L^{*}:u L(a^{*}(u),u)\) or the gradient \((_{u}L)(a,u)\) taken at \(a=a^{*}(u)\)? In fact, by definition of \(a^{*}(u)\), both quantities coincide:_

\[(_{u}L^{*})(u)=a^{*}}{u}(u)^{ }(_{a}L)(a^{*}(u),u)+(_{u}L)(a^{*}(u),u)=(_{u}L)(a^{*}( u),u)\,,\]

_where \(a^{*}}{u}\) denotes the differential of \(a^{*}\) in \(u\) and we use that, by definition of \(a^{*}(u)\), \((_{a}L)(a^{*}(u),u)=0\). This is a special case of the envelope theorem (Border, 2015, Sec. 5.10)._

The discussion in this section is not specific to the setting of Section 2. Using the two-timescale limit to decouple the dynamics of the outer layer \(a\) and the inner layer \(u\) is a general tool that may be used in the study of any two-layer neural network. We chose the specific setting of this paper so that the two-timescale limit (5) can be easily studied, thereby showcasing the approach. The next section is devoted to a sketch of this study.

### Sketch of the dynamics of the two-timescale limit

In this section, in order to simplify the exposition of the behavior of the two-timescale limit (5), we consider the limiting case \( 0\). Note that this is coherent with Theorem 1 that requires \(\) to be small. This limit is a neural network with a non-linearity equal to the Heaviside function

\[_{0}(x)=0x<0\,,_{0}(x)=1/2x=0\,,_{0}(x)=1x>0\,.\]

Note that \(_{0}\) would be a poor choice of non-linearity in practice: as its derivative is \(0\) almost everywhere, the positions \(u\) would not move. However, it is a relevant tool to get an intuition about the dynamics of our system for a small \(\). Moreover, as we will see in Section 6, the dynamics sketched here match closely those of the SGD (with \(>0\)).

he set \(\{1,_{0}(.-u_{1}),,_{0}(.-u_{m})\}\) generates the space of functions that are piecewise constant with respect to the subdivision \(\{u_{1},,u_{m}\}\). Furthermore, if \(u_{1},,u_{m}\) are distinct and in \((0,1)\), then this set is an independent set of functions in \(L^{2}()\). Thus \(a^{*}(u)\) is well defined and represents the coefficients of the best piecewise constant approximation of \(f^{*}\) with subdivision \(\{u_{1},,u_{m}\}\).

This quantity is straightforward to describe under the mild additional assumption that there are at least two neurons \(u_{j}\) in each interval \((v_{i-1},v_{i})\) between two points of discontinuity of \(f^{*}\). For each \(1 i n\), let \(u_{i}^{}\) denote the largest position of neurons below \(v_{i}\) and \(u_{i}^{}\) denote the smallest position above \(v_{i}\) (with convention \(u_{0}^{}=0\) and \(u_{n+1}^{}=1\)). By assumption, \(0=u_{0}^{}<u_{1}^{}<u_{1}^{}<<u_{n}^{ }<u_{n}^{}<u_{n+1}^{}=1\) are distinct. A simple computation then shows the following identities:

* for all \(i\{1,,n\}\), for all \(x(u_{i}^{},u_{i}^{})\), \(f(x;a^{*}(u),u)=-u_{i}^{}}{u_{i}^{}-u_{i}^{ }}f^{*}_{i-1}+^{}-v_{i}}{u_{i}^{}-u_ {i}^{}}f^{*}_{i}\),
* and for all \(i\{1,,n+1\}\), for all \(x(u_{i-1}^{},u_{i}^{})\), \(f(x;a^{*}(u),u)=f^{*}_{i-1}\),

where we recall that \(f^{*}_{i}\) denotes the value of \(f^{*}\) on the interval \((v_{i},v_{i+1})\). Figure 2 illustrates the situation. Moreover, the loss \(L(a^{*}(u),u)\), which is half of the square \(L^{2}\)-error of this optimal approximation, can be written

\[L(a^{*}(u),u)=_{i=1}^{n-1}-u_{i}^{})(u_ {i}^{}-v_{i})}{u_{i}^{}-u_{i}^{}}(f^{*}_{i}-f^ {*}_{i-1})^{2}\,.\] (6)

The dynamics of the two-timescale limit (5) corresponds to the local optimization of the subdivision \(u\) in order to minimize the loss (6). A remarkable property of this loss is that it decomposes as a sum of local losses around the jump points \(v_{i}\) for \(i\{1,,n-1\}\). Each element of the sum involves only the two neurons located at \(u_{i}^{}\) and \(u_{i}^{}\). As a consequence, the dynamics of the two-timescale limit (5) decompose as \(n\) independent systems of two neurons \(u_{i}^{}\) and \(u_{i}^{}\): for all \(i\{1,,n-1\}\),

\[u_{i}^{}}{}& =-L}{u_{i}^{}}(a^{*}(u),u)=+ ^{}-v_{i})^{2}}{(u_{i}^{}-u_{i}^{ })^{2}}(f^{*}_{i}-f^{*}_{i-1})^{2}\,,\\ u_{i}^{}}{}& =-L}{u_{i}^{}}(a^{*}(u),u)=- -u_{i}^{})^{2}}{(u_{i}^{}-u_{i}^{ })^{2}}(f^{*}_{i}-f^{*}_{i-1})^{2}\,.\] (7)

All neurons other than \(u_{1}^{},u_{1}^{},,u_{n-1}^{},u_{n-1}^{ }\) do not play a role in the expression (6), thus they do not move in the two-timescale limit (5). The position \(u_{i}^{}\) moves right and \(u_{i}^{}\) moves left, until one of them hits the point \(v_{i}\). This shows that the positions of the neurons eventually align with the jumps of the function \(f^{*}\), and thus that the function \(f^{*}\) is recovered.

Figure 2: Sketch of the dynamics of the neurons in the two-timescale limit with a Heaviside nonlinearity. Only the neurons next to a discontinuity of the target move.

Convergence of the gradient flow

In this section, we give precise mathematical statements leading to the convergence of the gradient flow to a global optimum, first in the two-timescale limit \( 0\), then in the two-timescale regime with \(\) small but positive. All proofs are relegated to the Appendix.

### In the two-timescale limit

This section analyses rigorously the two-timescale limit (5), which we recall for convenience:

\[a^{*}(u)=*{arg\,min}_{a}L(a,u)\,, u}{}=-_{u}L(a^{*}(u),u)\,.\] (8)

We start by giving a rigorous meaning to these equations. First, for \(L\) to be differentiable in \(u\), we require the parameter \(\) of the non-linearity to be positive. Second, for \(a^{*}(u)\) to be well-defined, we need \(u L(a,u)\) to have a unique minimum. Obviously, if the \(u_{i}\) are not distinct, then the features \(\{_{}(.-u_{1}),,_{}(.-u_{m})\}\) are not independent and thus the minimum can not be unique. We restrict the state space of our dynamics to circumvent this issue. For \(u^{m}\), we denote

\[(u)=_{0 j,k m+1,\,j k}|u_{j}-u_{k}|,\]

with the convention that \(u_{0}=-/2\) and \(u_{m+1}=1+/2\). Further, we define \(=\{u^{m}\,|\,(u)>2\}.\) The proposition below shows that \(\) gives a good candidate for a set supporting solutions of (8).

**Proposition 1**.: _For \(u\), the Hessian \(H(u)\) of the quadratic function \(L(.,u)\) is positive definite and its smallest eigenvalue is greater than \(}{{8}}\). In particular, \(L(.,u)\) has a unique minimum \(a^{*}(u)\)._

The bound on the Hessian is useful in the following, in particular in the proof of the following result.

**Proposition 2**.: _Let \(G(u)=_{u}L(a^{*}(u),u)\) for \(u\). Then \(G:^{m}\) is Lipschitz-continuous._

Then, the Picard-Lindelof theorem (see, e.g., Luk, 2017 for a self-contained presentation and Arnold, 1992 for a textbook) guarantees, for any initialization \(u(0)\), the existence and unicity of a maximal solution of (8) taking values in \(\). This solution is defined on a maximal interval \([0,T_{})\) where it could be that \(T_{}<\) if \(u\) hits the boundary of \(\). However, the results below show that the target function \(f^{*}\) is recovered before this happens (with high probability over the initialization), and thus that this notion of solution is sufficient for our purposes. To this aim, we first define some sufficient conditions that the initialization should satify.

**Definition 2**.: _Let \(D\) be a positive real. We say that a vector of positions \(u^{m}\) is \(D\)-good if_

1. _for all_ \(i\{0,,n-1\}\)_, there are at least_ \(6\) _positions_ \(u_{j}\) _in each interval_ \([v_{i},v_{i+1}]\)_,_
2. \((u) D\)_, and_
3. _for all_ \(i\{1,,n-1\}\)_, denoting_ \(u_{i}^{}\) _the position closest to the left of_ \(v_{i}\) _and_ \(u_{i}^{}\) _the position closest to the right, we have_ \(|u_{i}^{}+u_{i}^{}-2v_{i}| D\)_._

Condition 1 is related to the fact that the derivation in Section 4.2 is valid only if there are at least two neurons per piece. This requirement that the neurons be distributed on every piece of the target seems to be necessary for our result to hold, and we provide in Appendix D.1 a counter-example where recovery fails otherwise. Condition 2 indicates that the neurons have to be sufficiently spaced at initialization, which is not surprising since we have to guarantee that \((u())>2\), that is, \(u()\), for all \(\) until the recovery of \(f^{*}\) happens. Finally, condition 3 also helps to control the distance between neurons: although \(u_{i}^{}\) and \(u_{i}^{}\) move towards each other, as shown by (7), their distance can be controlled throughout the dynamics as a function of \(|u_{i}^{}+u_{i}^{}-2v_{i}|\).

We can now state the Proposition showing the recovery in finite time. The proof resembles the sketch of Section 4.2 with additional technical details since we need to control the distance between neurons, and the fact that \(>0\) makes the dynamics more delicate to describe.

**Proposition 3**.: _Let \(f^{*}_{n, v, f,M}\). Assume that the initialization \(u(0)\) is \(D\)-good with \(D=2^{13/2}(m+1)^{1/2}M^{1/2}( f)^{-1}\). Then the maximal solution of (8) taking values in \(\) is defined at least on \([0,]\) for \(=}{{( f)^{2}}}\), and at the end of this time interval, there is a neuron at distance less than \(\) from each discontinuity of \(f^{*}\)._This Proposition is the main building block to show recovery in the next Theorem, along with some high-probability bounds to ensure that an i.i.d. uniform initialization is \(D\)-good.

**Theorem 2**.: _Let \(,>0\), and \(f^{*}\) a piecewise constant function from \(_{n, v, f,M}\). Assume that the neural network has \(m\) neurons with_

\[m4+ n+\,.\]

_Assume that, at initialization, the positions \(u_{1},,u_{m}\) of the neurons are i.i.d. uniformly distributed on \(\). Then there exists \(Q\) depending on \(,,m, f,M\) such that, if_

\[ Q\,,\]

_then, with probability at least \(1-\), the maximal solution to the two-timescale limit (8) is defined at least until \(=}\), and_

\[_{0}^{1}|f^{*}(x)-f(x;a^{*}(u()),u())|^{2}x\,.\]

_Furthermore, we have \(Q=}( f)^{2}}{(m+1)^{5}},\) for some universal constant \(C>0\)._

### From the two-timescale limit to the two-timescale regime

We now briefly explain how the proof for the two-timescale limit can be adapted for the gradient flow problem in the two-timescale regime (1), that is with a small but non-vanishing \(\). First note that the existence and uniqueness of the maximal solution to the dynamics (1) follow from the local Lipschitz-continuity of \(_{a}L\) and \(_{u}L\) with respect to both their variables.

The heuristics of Section 4.1 indicate that, for \(\) small enough, at any time \(t\), the weights \(a(t)\) are close to \(a^{*}(u(t))\), the global minimizer of \(L(,u(t))\). The next Proposition formalizes this intuition.

**Proposition 4**.: _Assume that \(a(0)=0\) and that, for all \(s[0,t]\), there at least \(2\) positions \(u_{j}(s)\) in each interval \([v_{i},v_{i+1}]\) and \((u(s))}{{2}}\) for some \(D 32\). Finally, assume that \( 2^{-16}D^{2}M^{-2}(m+1)^{-5/2}\). Then_

\[\|a(t)-a^{*}(u(t))\| 3M^{-t}+M^{ 3}(m+1)^{3}}{D^{2}}\,.\]

The crucial condition in the Proposition is \((u(s))}{{2}}\); it is useful to control the conditioning of the quadratic form \(L(,u(s))\). The Proposition shows that \(\|a(t)-a^{*}(u(t))\|\) is upper bounded by the sum of two terms; the first term is a consequence to the initial gap between \(a(0)\) and \(a^{*}(u(0))\) and decays exponentially quickly. The second term is negligible in the regime \( 1\).

Armed with this Proposition, we show that the two-timescale regime has the same behavior as the two-timescale limit and thereby prove Theorem 1.

## 6 Numerical experiments and discussion

Numerical illustration in the setting of Section 2.We first compare the dynamics of the gradient flow in the two-timescale limit presented in Section 4.2 with the dynamics of SGD. To simulate the SGD dynamics, we assume that we have access to noisy observations of the value of \(f^{*}_{n, v, f,M}\): let \((X_{p},Y_{p})_{p 1}\) be i.i.d. random variables such that \(X_{p}\) is uniformly distributed on \(\), and \(Y_{p}=f^{*}(X_{p})+N_{p}\) where \(N_{p}\) is additive noise. The (one-pass) SGD updates are then given by

\[ a_{p+1}&=a_{p}-h_{a}(X_{p +1},Y_{p+1};a_{p},u_{p})\,,\\ u_{p+1}&=u_{p}- h_{u}(X_{p+1},Y _{p+1};a_{p},u_{p})\,,\] (9)

with \((X,Y;a,u)=(Y-f(X;a,u))^{2}\). The experimental settings, as well as additional results, are given in the Appendix.

Remarkably, the dynamics of SGD in the two-timescale regime with \(\) small match closely the gradient flow in the two-timescale limit with \(=0\), as illustrated in Figure 3. This validates the use of the gradient flow to understand the training dynamics with SGD. Both dynamics are close until the two-timescale limit achieves perfect recovery of the target function, at which point the SGD stabilizes to a small non-zero error. The fact that SGD does not achieve perfect recovery is not surprising, since SGD is performed with \(>0\) and \(f^{*}\) is not in the span of \(\{1,_{}(x-u_{1}),,_{}(x-u_{m})\}\) for any \(u_{1},,u_{m}\) and for \(>0\). On the contrary, we simulated the dynamics of gradient flow for \(=0\), as presented in Section 4.2, enabling perfect recovery in that case.

Next, we compare the SGD dynamics in the two-timescale regime (\( 1\)) and outside of this regime (\( 1\)). In Figure 4, we see that the network trained by SGD (in orange) in the two-timescale regime \(=2 10^{-5}\), achieves near-perfect recovery. If we change \(\) to \(1\), while keeping all other parameters equal, the algorithm fails to recover the target function (Figure 5). This shows that, in our setting with a moderate number of neurons \(m\), recovery can fail away from the two-timescale regime. It could seem that we are favouring the two-timescale regime by running it for more steps. In fact, it is not the case since both regimes are run until convergence. We refer to Appendix C for details.

Note that the dynamics of the neurons in Figures 4 and 5 are different. In the two-timescale regime, only the neurons closest to a discontinuity move significantly, while the others do not. These dynamics correspond to the sketch of Section 4. Interestingly, it means that in this regime, the neural network learns a sparse representation of the target function, meaning that only \(n\) out of the \(m\) neurons are active after training. On the contrary, when \(=1\), all neurons move to align with discontinuities of the target function, thus the learned representation is not sparse. Furthermore, since the number of neurons is moderate, one of the discontinuities is left without any neuron.

Figure 4: Simulation in the two-timescale regime (\(=2 10^{-5}\)). The target function is in blue and the SGD (9) is in orange with \(=4 10^{-3}\), \(h=10^{-5}\). The positions \(u_{1},,u_{m}\) of the neurons are indicated with vertical dotted lines. In a first short phase, only the weights \(a_{1},,a_{m}\) of the neurons evolve to match as best as possible the target function (second plot). Then, in a longer phase, the neuron closest to each target discontinuity moves towards it (third plot). Recovery is achieved.

Figure 3: Comparison between the SGD (9) with \(=4 10^{-3}\) in the two-timescale regime (\(=2 10^{-5}\)) and the gradient flow in the two-timescale limit (5) with \(=0\). In the left-hand plot, to align the SGD and the two-timescale limit, we take \(= hp\). In the right-hand plot, the target function is in blue, the gradient flow in the two-timescale limit is in green, and the SGD is in orange.

Discussion.The two-timescale regime decouples the dynamics of the two layers of the neural network. As a consequence, it is a useful theoretical tool to simplify the evolution equations. In this paper showcasing the approach, the two-timescale regime enables to show the alignment of the neurons with the discontinuities of the target function in the piecewise constant 1D case, and thus to prove recovery. A full general understanding of the impact of the two-timescale regime is an open question, which is left for future work. We provide in the following some practical evidence of the applicability of this regime to other settings (higher-dimensional problems, ReLU networks, finite sample size). Additional technical difficulties significantly complicate the proof in these settings, but we believe that there is no fundamental reason that our mathematical approach should not apply.

Higher dimensions.We consider piecewise constant functions on \(^{d}\) with pieces that are cuboids aligned with the axes of the space (see Figure 6). Neural networks are of the form \(f(x;a,u)=a_{0}+_{j=1}^{m}_{k=1}^{d}a_{jk}_{}(x_{k}-u_{jk}),\) where the \(j\)-th neuron has \(d\)-dimensional position \((u_{jk})_{1 k d}\) and weight \((a_{jk})_{1 k d}.\) The results are similar to the 1D case: convergence to a global minimum is obtained in the two-timescale regime but not in the standard regime.

ReLU networks.Appendix C reports the case of using ReLU activations to approximate piecewise-affine targets. A similar conclusion holds.

Finite sample size.We believe that it should be possible to generalize our results to finite sample sizes (say, for single-pass SGD), following a perturbation analysis similar to the one of Section 5.2, with additional terms due to random sampling. Numerically, Figure 3 shows that SGD indeed closely follows the behavior of the two-timescale limit. Some ideas about the proof are provided in Appendix D.2.

Figure 5: Simulation outside of the two-timescale regime (\(=1\)). The target function is in blue and the SGD (9) is in orange with \(=4 10^{-3}\), \(h=10^{-5}\). The positions \(u_{1},,u_{m}\) of the neurons are indicated with vertical dotted lines. The dynamics create a zone with no neuron, hindering recovery.

Figure 6: 2D experiment with a piecewise-constant target (each shade of blue depicts a constant piece). The orange lines show the positions of the neurons after training by SGD. In the standard regime, a discontinuity of the target at \(x=0.3\) is not covered by a neuron. In the two-timescale regime, all the target discontinuities are covered. See Appendix C for more results with \(d=2,10\).