# DarkSAM: Fooling Segment Anything Model to Segment Nothing

Ziqi Zhou\({}^{1,2,3*}\), Yufei Song\({}^{}\), Minghui Li\({}^{}\), Shengshan Hu\({}^{1,2,4,5}\), Xianlong Wang\({}^{1,2,4,5}\),

**Leo Yu Zhang\({}^{@sectionsign}\), Dezhong Yao\({}^{1,2,3*}\), Hai Jin\({}^{1,2,3*}\) \({}^{1}\) National Engineering Research Center for Big Data Technology and System \({}^{2}\) Services Computing Technology and System Lab \({}^{3}\) Cluster and Grid Computing Lab \({}^{4}\) Hubei Engineering Research Center on Big Data Security \({}^{5}\) Hubei Key Laboratory of Distributed System Security \(*\) School of Computer Science and Technology, Huazhong University of Science and Technology \(\) School of Cyber Science and Engineering, Huazhong University of Science and Technology \(\) School of Software Engineering, Huazhong University of Science and Technology \(@sectionsign\) School of Information and Communication Technology, Griffith University {zhouziqi,yufei17,minghuili,hushengshan,wx199,dyao,hjin}@hust.edu.cn leo.zhang@griffith.edu.au**

###### Abstract

_Segment Anything Model_ (SAM) has recently gained much attention for its outstanding generalization to unseen data and tasks. Despite its promising prospect, the vulnerabilities of SAM, especially to _universal adversarial perturbation_ (UAP) have not been thoroughly investigated yet. In this paper, we propose DarkSAM, the first prompt-free universal attack framework against SAM, including a semantic decoupling-based spatial attack and a texture distortion-based frequency attack. We first divide the output of SAM into foreground and background. Then, we design a shadow target strategy to obtain the semantic blueprint of the image as the attack target. DarkSAM is dedicated to fooling SAM by extracting and destroying crucial object features from images in both spatial and frequency domains. In the spatial domain, we disrupt the semantics of both the foreground and background in the image to confuse SAM. In the frequency domain, we further enhance the attack effectiveness by distorting the high-frequency components (_i.e._, texture information) of the image. Consequently, with a single UAP, DarkSAM renders SAM incapable of segmenting objects across diverse images with varying prompts. Experimental results on four datasets for SAM and its two variant models demonstrate the powerful attack capability and transferability of DarkSAM. Our codes are available at: [https://github.com/CGCL-codes/DarkSAM](https://github.com/CGCL-codes/DarkSAM).

## 1 Introduction

With the advancement of deep learning, large language models, such as GPT , LaMDA , and PaLM , have achieved tremendous success, yet the development of large vision models lags behind. Recently, _Segment Anything Model_ (SAM)  was proposed as a foundational vision model, demonstrating exceptional generalization capabilities for handling complex segmentation tasks. Unlike traditional segmentation models  that output pixel-level labels, SAM introduces a novel prompt-guided image segmentation paradigm by directly producing label-free masks for object segmentation. Benefiting from its powerful zero-shot capability, SAM has been rapidly deployed across various downstream scenarios, such as medical images , videos , and 3D point clouds .

_Deep neural networks_ (DNNs) are shown to be vulnerable to adversarial examples [16; 21; 41; 46], and SAM is no exception. Standard adversarial attacks are designed for classification tasks and cause misclassification by manipulating global image features through image-level perturbations. Existing attacks can be divided into crafting sample-wise adversarial perturbation  and _universal adversarial perturbation_ (UAP) . The former is tailored for specific inputs, while the latter seeks a single perturbation applicable across a wide range of inputs, thereby intensifying its complexity and difficulty. As a pioneering prompt-guided segmentation model, SAM relies on both _input images_ and _prompts_ to yield _label-free_ masks, rendering existing adversarial attacks [1; 10; 25; 26] focusing only for images and relying on labels ineffective.

Recent efforts [17; 39] started to explore the robustness of SAM against sample-wise adversarial perturbations. Attack-SAM  employs classical FGSM  and PGD  to remove or manipulate the predicted mask for a given image and prompt pair. Meanwhile, another study  also investigates the robustness of SAM against various adversarial attacks and corrupted images. However, the more challenging universal adversarial attacks, which more closely resemble real-world scenarios, remain far less thoroughly explored. The introduction of extra and varying prompts in SAM's input, coupled with the lack of label information in its output for attack optimization, renders attacking SAM exceedingly challenging, posing an intriguing problem:

_Is it feasible to fool the Segment Anything model to segment nothing through a single UAP?_

In this paper, we take a substantial step towards bridging the gap between SAM and UAP. We propose DarkSAM, the first truly prompt-free universal adversarial attack on the prompt-guided image segmentation models (_i.e._, SAM and its variants), aiming to disable their segmentation ability across diverse input images using a single UAP, irrespective of prompts (see Fig. 1). Unlike classification models that focus on global features, prompt-guided segmentation models concentrate more on local critical objects within images (_e.g._, objects indicated by prompts). Therefore, our intuition is to destroy crucial object features in the image to mislead SAM into incorrectly segmenting the input images. To this end, DarkSAM is dedicated to decoupling the crucial object features of images from both spatial and frequency domains, utilizing a UAP to disrupt them. 1) In the spatial domain, we begin by dividing SAM's output into foreground (_i.e._, positive mask values) and background (_i.e._, negative mask values) via a Boolean mask. We then scramble SAM's decision by destroying the features of the foreground and background of the image, respectively. 2) In the frequency domain, inspired by the factor that SAM is biased towards image texture over shape , we employ a frequency filter to decompose images into _high-frequency components_ (HFC) and _low-frequency components_ (LFC). By increasing the dissimilarity in the HFC of adversarial and benign examples while maintaining consistency in their LHC, we further enhance the effectiveness and transferability of UAP. Experimental results on four segmentation benchmark datasets for SAM and its two variant models, HQ-SAM  and PerSAM , demonstrate that DarkSAM achieves high attack success rates and transferability.

Our main contributions are summarized as follows:

* We propose DarkSAM, the first truly universal adversarial attack against SAM. We employ a single perturbation to prevent SAM from segmenting objects across a range of images under any form of prompt, which further unveils its vulnerability.

Figure 1: Illustration of fooling SAM using UAP

* We design a brand-new prompt-free hybrid spatial-frequency universal attack framework against the prompt-guided image segmentation models to generate a UAP thus making them segment nothing, which consists of a semantic decoupling-based spatial attack and a texture distortion-based frequency attack.
* We conduct extensive experiments on four datasets for SAM and its two variant models. Both the qualitative and quantitative results demonstrate that DarkSAM achieves high attack success rates and transferability.

## 2 Background and Related Works

### Prompt-guided Image Segmentation

Segment Anything Model  is a cutting-edge advancement in computer vision, garnering widespread attention [3; 5; 20; 22; 34] for its powerful segmentation capabilities. Recent works have been dedicated to exploring various variants of SAM to further enhance performance, such as HQ-SAM , PerSAM , and MobileSAM . Distinct from traditional semantic segmentation models [4; 24; 42] that predominantly focus on pixel-level label prediction, SAM undertakes the label-free mask prediction by generating object masks for a wide array of subjects using prompts. It consists of three components: an image encoder, a prompt encoder, and a lightweight mask decoder. The image encoder generates image representations in latent space and the prompt encoder utilizes positional embeddings for representing prompts, such as points and boxes. The mask decoder, combining outputs from both image and prompt encoders, predicts effective masks to segment targeted objects.

Given an image \(x^{H W C}\) and a corresponding prompt \(\) to SAM, denoted as \(f(x,)^{H W}\), the model returns a mask \(m\) with the predicted segmentation. The prediction process of SAM can be represented as follow:

\[m=f_{}(x,), \]

where \(\) represents the parameter of \(f()\). For an image \(x\), each pixel located at coordinates \((i,j)\), referred to as \(x_{ij}\), is deemed a part of the masked region when its corresponding mask value \(m_{ij}\) exceeds a defined threshold of zero. Recent exploratory studies [39; 17; 35] have revealed vulnerabilities of SAM to adversarial examples and common image corruptions. Different from previous works, our goal is to develop a powerful universal adversarial attack for such prompt-guided image segmentation models.

### Universal Adversarial Perturbation

Deep neural networks have been shown vulnerable to adversarial examples [10; 25; 44; 45; 46], where attackers can deceive models by introducing subtle noise to images. _Universal adversarial perturbation_ (UAP) was first proposed to fool the victim model by imposing a single adversarial perturbation on a series of images. Existing works can be divided into data-dependent universal adversarial attacks [14; 27; 30] and data-free universal attacks [28; 29; 31], both designed for classification attacks. The former relies on the specific data characteristics of target dataset for UAP generation, while the latter provides a more generalized approach without relying on such data. Meanwhile, some works  have also explored UAPs for traditional segmentation models, but they rely on pixel-level labels, which are not applicable to emerging prompt-guided segmentation models. The concurrent works [8; 13] explore UAPs against SAM from the perspectives of direct noise optimization and perturbing the output of the image encoder of SAM, respectively. Different from them, we aim to comprehensively decouple and disrupt crucial object features in images from both spatial and frequency domains, thereby deceiving SAM into failing to segment input images.

## 3 Methodology

### Problem Formulation

As a fundamental vision model, SAM typically operates in an online mode, allowing users to set prompts randomly. Therefore, we define the threat model as a quasi-black-box setting, where ad versaries have access to the official open-source SAM, but not to the pre-training dataset and the downstream dataset (_i.e._, those used by users). The adversaries' goal is to craft a UAP \(\) using a surrogate dataset \(_{s}\) (_i.e._, unrelated to the pre-training and downstream dataset), thereby compromising the model's performance, _i.e._, rendering adversarial examples unable to be correctly segmented by SAM. Additionally, the \(\) should be sufficiently small, and constrained by \(l_{p}\)-norm of \(\). This problem can be formulated as:

\[_{}_{x_{s}}[f_{}(x+, ) f_{}(x,)],s.t.\| \|_{p}. \]

### Intuition Behind DarkSAM

Unlike the standard deep learning paradigm that inputs a single image and outputs a one-hot label or pixel-level label, SAM requires both images and prompts as inputs, and then outputs label-free masks, indicating the shape information of critical objects. Therefore, a truly universal adversarial attack against SAM should implement a single perturbation to achieve ineffective segmentation for any combination between a series of images and different prompts. However, this task is hindered by the following challenges:

**Challenge I: The dual ambiguity in attack targets arising from varying images and prompts.** Previous UAP works only need to optimize in the target images, hence the introduction of prompts could lead to invalid attacks, as different prompts for a fixed image yield distinct segmentation results. For instance, the image in the top-left corner of Fig. 2 shows a can and a spoon. For the same image, feeding different prompts will result in different masks output by SAM (see Fig. 2(b)). In conclusion, diverse variations in target images and prompts increase the uncertainty of attack targets. For varying images, existing UAP solutions (_e.g._, UAPGD ) can provide references, and the main challenge here is the uncertainty of the attack target brought by unknown prompts. To this end, we propose a _shadow target strategy_ by increasing the number of prompts during the attack process to enhance the cross-prompt transferability of UAP. Specifically, for a given input image, we randomly select \(k\) prompts (_e.g._, points or boxes) to create a prompt auxiliary set. By merging their masks output by SAM, we form a _semantic blueprint_ of the image, which serves as the target for our attack, as illustrated in Fig. 2(c). This semantic blueprint effectively encompasses the main semantic content of the original image, substantially reducing the ambiguity associated with unknown prompts.

**Challenge II: Suboptimal attack efficacy due to semantic decoupling deficiency.** Since prompt-guided segmentation models output masks that are neither one-hot nor pixel-level labels, traditional attack methods that rely on label deviation for optimization guidance become ineffective. Another approach involves directly modifying the output, such as adjusting the adversarial examples' masks to diverge from their originals, potentially yielding marginal attack success as verified in Sec. 4.4. Nonetheless, the intrinsic sensitivity of segmentation models to pixel-level details significantly constrains the potency of these attacks, underscoring a notable limitation in their applicability.

Given the focus of prompt-guided segmentation models on local, critical object features rather than global image features, we are motivated to comprehensively decouple the key semantic features

Figure 2: Illustration of the proposed shadow target strategy

of an image from the perspective of both spatial and frequency domains, aiming to fool SAM by manipulating these features. We first define the main object within the image (_i.e._, the target of segmentation, typically a region rich in texture) as the _foreground_, with the rest being defined as the _background_. As the mask output by SAM indicates the foreground with positive values and the background with negative ones, we use a Boolean mask to separately extract these foreground and background masks. Subsequently, we optimize the UAP, switching adversarial examples' foreground to negative and background to positive, disrupting the image's semantics for a spatial attack. At the same time, inspired by the recent study  that SAM is biased towards texture of the image over shape, we investigate the alteration of the high-frequency components (_i.e._, texture information) of adversarial examples in the frequency domain, while simultaneously constraining the low-frequency components (_i.e._, shape information), in order to further enhance the effectiveness and transferability of our attack. By separately decoupling and destroying crucial features in both the spatial and frequency domains, we provide valuable optimization directions for UAP generation, thereby facilitating effective attacks on SAM.

### DarkSAM: A Complete Illustration

In this section, we present DarkSAM, a novel prompt-free hybrid spatial-frequency universal adversarial attack against the prompt-guided image segmentation models (_i.e._, SAM and its variants). The pipeline of DarkSAM is depicted in Fig. 3, encompassing a semantic decoupling-based spatial attack and a texture distortion-based frequency attack. We start by randomly generating \(k\) different prompts to form an auxiliary prompt set \(_{a}\), acquiring the semantic blueprints of the target images as the attack targets. By individually manipulating the semantic content of adversarial examples' foreground and background in the spatial domain, and increasing the distance between the HFC of adversarial and benign examples in the frequency domain, while also constraining the difference in their LFC, we enhance the attack performance and transferability of the UAP. We provide the detailed optimization process of DarkSAM in Algorithm 1. The overall optimization objective \(_{total}\) of DarkSAM is as follow:

\[_{total}=_{sa}+_{fa}, \]

where \(_{sa}\) and \(_{fa}\) are the spatial and frequency attack losses, and \(\) controls the importance.

**Semantic decoupling-based spatial attack.** Initially, we utilize two Boolean mask \(m_{fg}\) and \(}\) to separately extract the foreground and background mask of the adversarial examples based on the positive and negative values in the mask output by SAM. As for the foreground, our intention is to render it unidentifiable and unsegmentable by SAM. Thus, we optimize its mask towards a negative fake mask \(_{neg}\), enabling its fusion with the background to achieve segmentation evasion. The foreground evasion loss \(_{fe}\) can be described as:

\[_{fe}=_{d}(f_{}(x+,_{a}) m_{ fg},_{neg}), \]

where \(_{neg}\) is a fake mask that conforms to the shape of the image, containing threshold values of \(-\) in regions corresponding to the foreground, and \(0\) elsewhere. \(_{d}\) serves as the distance metric function, representing the mean squared error loss. For the background, we optimize its mask towards a positive fake mask \(_{pos}\) (opposite to \(_{neg}\)), misleading SAM into interpreting it as a

Figure 3: The framework of DarkSAM

semantically meaningful object, consequently causing further interference in the assessment of the foreground. The associated loss is

\[_{bm}=_{d}(f_{}(x+,_{a}) {m_{fg}},_{pos}). \]

The loss of the semantic decoupling-based spatial attack can be expressed as:

\[_{sa}=_{fe}+_{bm}. \]

**Texture distortion-based frequency attack.** In the frequency domain, the high-frequency components of an image denote the finer details, including noise and textures, while the low-frequency components contain the general outline and overall structural information of the image. We employ the _discrete wavelet transform_ (DWT), utilizing a low-pass filter \(\) and a high-pass filter \(\) to decompose the image \(x\) into different components, constituting a low-frequency component \(c_{ll}\), a high-frequency component \(c_{hh}\), and two mid-frequency components \(c_{lh}\) and \(c_{hl}\), via

\[c_{ll}=x^{T},c_{hh}=x^{T},c_{lh}/c_ {hl}=x^{T}/x^{T}. \]

Subsequently, we employ the _inverse discrete wavelet transform_ (IDWT) to reconstruct the signal that have been decomposed through DWT into an image. We choose the LFC and HFC while dropping the other components to obtain the reconstructed images \((x)\) and \((x)\) as

\[(x)=^{T}c_{ll}=^{T}(x^{T}), \]

\[(x)=^{T}c_{hh}=^{T}(x^{T}). \]

By adding a UAP to the images, we alter their HFC, disrupting the original texture information. Simultaneously, we enforce constraints on the low-frequency disparities between adversarial and benign examples to redirect a larger portion of the perturbation towards the high-frequency domain. As a result, we enhance the attack performance and cross-domain transferability of the UAP by introducing variations in the frequency domain. The loss of texture distortion-based frequency attack can be expressed as:

\[_{fa} =_{lfc}-_{hfc} \] \[=_{d}((x),(x+))-_{d}(( x),(x+)),\]

where \(\) is a pre-defined hyperparameter.

```
0: image \(x_{s}\), SAM \(f(x)\) with parameter \(\), hyper parameters \(k\), \(\), \(\), and \(\), max-perturbation constraint \(\)
0: A universal adversarial perturbation \(\)
1: Initialize random prompt sets \(_{a}\) and noise \(_{o}\)
2: Initialize adversarial examples: \(x^{*} x+_{o}\)
3: Project \(x^{*}\) to \(\) via clipping
4: Separating different frequency components of \(x\) using the discrete wavelet transform: \(c_{ll},c_{lh},c_{hl},c_{hh} DWT(x)\)
5: Restore part of the frequency components into an image using the inverse discrete wavelet transform: \((x)\), \((x) IDWT(c_{ll}),IDWT(c_{hh})\)
6: Calculate \(m_{fg}\) by determining the sign of each value in output of \(f_{}(x,_{a})\)
7:while max iterations or not converge do
8: Calculate spatial loss mentioned in Eq. 6
9: Calculate frequency loss mentioned in Eq. 10
10: Update \(\) through backprop
11: Clip \(\) to satisfy imperceptibility constraint \(\)
12: Project \(x^{*}\) to \(\) via clipping
13:endwhile
```

**Algorithm 1** DarkSAM

## 4 Experiments

### Experimental Setup

**Datasets and models.** We evaluate our method using four public segmentation datasets: ADE20K , MS-COCO , CITYSCAPES , and SA-1B . For each dataset, we randomly select 100 images for UAP generation and 2,000 images for testing purposes. All images are uniformly resized to 3\(\)1024\(\)1024. For victim models, we use the pre-trained SAM , HQ-SAM  and PerSAM  with the ViT-B backbone.

**Parameter setting.** Following [9; 27; 32], we set the upper bound of UAP to \(10/255\). For our experiments, we adjust the hyperparameters \(k\), \(\), \(\), and \(\) to \(10\), \(1\), \(0.1\), and \(0.01\), respectively, and set the batch size to \(1\). To evaluate the cross-prompt attack capabilities of DarkSAM, we employ three distinct prompt types: point, box, and segment everything (also abbreviated as "all") mode.

**Evaluation metrics.** To evaluate the effectiveness of DarkSAM, we use the _mean Intersection over Union_ (mIoU) metric. To facilitate data presentation, we also use the _attack success rate_ (ASR) as a metric to evaluate attack performance. ASR represents the difference between the mIoU values of benign and adversarial examples.

### Attack Performance

To comprehensively evaluate DarkSAM's effectiveness, we perform experiments on three prompt-guided image segmentation models including SAM, HQ-SAM, and PerSAM, across four datasets. For each setup, we generate UAPs using point and box prompts, respectively, and then evaluate DarkSAM's attack performance using the corresponding single-point or single-box prompt. We first calculate the clean mIoU of different models across four datasets using point and box as prompts. Specifically, for the SA-1B dataset, we directly extract point and box prompts from the annotations, whereas for the other datasets, we obtain internal points and external boxes as prompts by calculating the object contour coordinates within their annotations.

The experiments in Tab. 1 show that DarkSAM can effectively fool these prompt-guided image segmentation models with an average mIoU reduction of more than \(60\%\) across \(96\) different experimental settings. The results in Tab. 1 also indicate that box prompts not only yield higher segmentation accuracy but also demonstrate greater robustness. For adversaries, the choice of surrogate datasets has a minor impact on crafting UAPs, yet they consistently facilitate excellent attack performance. Notably, DarkSAM demonstrates a distinct advantage when the SA-1B dataset, the training data for SAM, is employed as the surrogate dataset. In addition to the above **quantitative** experimental results, we also present **qualitative** findings. Specifically, we provide the visualization of SAM segmentation results for adversarial examples made by DarkSAM using point and box prompts across four different datasets in Fig. 4. These results include masks of objects in images output by SAM under point, box, and segment everything prompt modes. From Fig. 4, we can see that SAM successfully segments benign images across three types of prompt modes, but it is unable to segment adversarial examples, _i.e._, the output masks are close to "dark". The qualitative results further corroborate the powerful attack capability of DarkSAM.

    &  &  &  \\   & Surrogate & ADE & COCO & CITY & SA-1B & ADE & COCO & CITY & SA-1B & ADE & COCO & CITY & SA-1B \\   & **Clean** & 65.39 & 62.79 & 50.70 & 77.21 & 63.39 & 65.38 & 50.25 & 72.89 & 64.61 & 62.91 & 51.22 & 77.93 \\  & **ADE** & 0.43 & 3.21 & **0.02** & 5.81 & 0.99 & 6.04 & 6.82 & 7.75 & 0.37 & 3.25 & 7.95 & 1.54 \\  & COCO & 0.42 & 1.16 & 0.76 & 2.46 & **0.69** & **2.23** & 3.19 &

### Transferability Study

We study the attack transferability of DarkSAM across data domain, prompt types, and models, respectively. **Cross-domain.** The results in Tab. 1 demonstrate DarkSAM's excellent cross-domain transferability, where UAPs generated with the surrogate dataset (ADE20K) achieve a high ASR on datasets from various different domains. We also explore the role of the frequency attack (_i.e._, \(_{fa}\), denoted as FA) in enhancing cross-domain transferability. As shown in Fig. 5 (a), frequency attack can effectively improve the attack performance based on the spatial attack (_i.e._, \(_{sa}\), denoted as SA). **Cross-prompt.** We examine the performance of DarkSAM across various types of prompts. As demonstrated in the last three columns of Fig. 4, UAPs created based on both point and box prompts perform well under the segment everything mode. Additionally, we provide results of transferability experiments between point and box prompts in Tab. 2. This includes testing UAPs created with point prompts in the box prompt setting and vice versa. Based on the observed results, it is discernible that UAPs crafted using box prompts generally demonstrate better transferability compared to those using point prompts. This increased efficacy can likely be attributed to the box prompts offering more integral and detailed prompt information. **Cross-model.** We use UAPs created with points and boxes based on SAM to attack HQ-SAM and PER-SAM. The results in Fig. 5 (b) - (e) showcase DarkSAM's exceptional transferability across different models.

### Comparison Study

To comprehensively demonstrate the superiority of our proposed method, we compare DarkSAM with popular UAP schemes, including UAP , UAPGD , and SSP . We also consider the state-of-the-art adversarial attack against traditional segmentation models, SegPGD , and the

    &  &  \\  Surrogate & ADE & COCO & CITY & SA-1B & ADE & COCO & CITY & SA-1B \\  ADE & 63.01 & 55.61 & 49.72 & 66.67 & **47.00** & **35.52** & 56.15 & 40.80 \\ COCO & **64.95** & **61.69** & 49.98 & **75.09** & 19.95 & 25.27 & 44.60 & 53.01 \\ CITY & 48.31 & 30.48 & **50.30** & 55.74 & 17.36 & 10.94 & 55.43 & 20.69 \\ SA-1B & 52.47 & 36.05 & 47.12 & 66.20 & 31.16 & 17.45 & **58.21** & **62.00** \\
**AVG** & 57.19 & 45.96 & 49.28 & 65.93 & 28.87 & 22.30 & 53.60 & 44.13 \\   

Table 2: The ASR (%) of the cross-prompt transferability study on SAM. “BOX \(\) POINT” indicates that adversarial examples created using box are tested in point mode. Others stand the same meaning.

Figure 4: Visualizations of SAM segmentation results for adversarial examples across four datasets. The first four columns and the middle four columns display the segmentation results for point and box prompts, respectively. The last three columns show results under the segment everything mode for benign examples, as well as adversarial examples created using point and box prompts, respectively.

latest sample-wise attack against SAM, Attack-SAM . For a fair comparison, we adapt them to a UAP optimization strategy and keep other settings consistent with DarkSAM. We select SAM as the victim model and assess the effectiveness of these UAP methods across four datasets, using the same dataset for both generating and testing the UAPs. The results in Tab. 3 indicate that DarkSAM outperforms all methods with a considerable margin. The negative experimental values ("*") indicate that the attack does not work at all. This phenomenon may stem from counterproductive perturbations that inadvertently cause the input samples to resemble the training set used by SAM, paradoxically enhancing accuracy and resulting in negative ASR values. We also provide visualizations of the segmentation results of the adversarial examples made by these methods using box prompts in Fig. 6, obtained in point, box, and segment-everything modes, respectively. The results further demonstrate the superiority of DarkSAM.

### Ablation Study

In this section, we explore the effect of different modules, prompt number, attack strengths, training data size, and threshold values on DarkSAM. We conduct experiments using point prompts on SAM across the ADE20K dataset.

**The effect of different modules.** We investigate the effect of various modules on the attack performance of DarkSAM. For clarity and convenience, we use A, B, C, and D to denote \(_{fe}\), \(_{bm}\), \(_{hfc}\), and \(_{lfc}\), respectively. The results in Fig. 7 (a) show that no variants can compete with the complete method, implying the indispensability of each component for DarkSAM.

**The effect of prompt number.** We study the effect of the prompt number in proposed shadow target strategy on attack performance of DarkSAM. We conduct experiments with varying numbers of point prompts, ranging from \(1\) to \(100\). The results in Fig. 7 (b) show a gradual increase in attack performance from \(1\) to \(10\) (default setting), followed by a downward trend. This could be attributed to an excess of random points leading to masks with redundant information, thereby impacting the attack efficacy.

**The effect of perturbation budget.** As shown in Fig. 7 (c), we evaluate DarkSAM's attack performance with \(\) from \(4/255\) to \(32/255\). With the increase in \(\), there is a corresponding enhancement in attack performance. Notably, our attack still maintains high efficacy at the \(6/255\) setting, with an average ASR exceeding \(45\%\).

**The effect of number of training samples.** We explore the effect of varying the number of training images used to create UAP on DarkSAM. Utilizing a range from \(10\) to \(1000\) images to craft UAPs, the results in Fig. 7 (d) reveal that employing merely \(100\) images can achieve excellent attack performance, demonstrating a strong applicability advantage.

Figure 5: The ASR (%) of transferability study. (a) explores the impact of the frequency attack on boosting the cross-domain transferability of UAPs. (b) - (e) stand the results of cross-model transferability study. “Point-HQ” and “Box-HQ” denote the results of HQ-SAM under point and box prompts, while the suffix “-PER” represents the corresponding results for PerSAM. Each row represents the same UAP.

    &  &  \\   & ADE & COCO & CITY & SA-1B & ADE & COCO & CITY & SA-1B \\  UAP  & 1.62 & 0.47 & 8.13 & 5.28 & 0.28 & * & 1.29 & 1.76 \\ UAPGD  & 4.85 & 1.52 & 11.52 & 10.04 & 0.97 & 0.45 & 2.22 & 3.11 \\ SSP  & 0.67 & 0.09 & 5.90 & 4.08 & * & * & 0.91 & 1.20 \\ SegPGID  & 4.24 & 1.44 & 11.48 & 8.92 & 0.89 & 0.51 & 2.10 & 3.46 \\ Attack-SAM  & 2.91 & 1.36 & 13.20 & 9.54 & 0.51 & 0.36 & 1.90 & 3.12 \\ Ours & **64.96** & **61.63** & **50.63** & **77.07** & **69.72** & **76.03** & **64.27** & **84.22** \\   

Table 3: The ASR (%) of comparison study 

**The effect of threshold values.** We examine the effect of varying threshold values \(\) in the fake mask \(\) on DarkSAM. As illustrated in Fig. 7 (e), we test a range of values from \(1\) to \(1000\). The results indicate that these different values have a minimal overall effect on DarkSAM's performance.

## 5 Conclusions, Limitations, and Broader Impact

In this paper, we propose DarkSAM, the first truly universal adversarial attack against SAM. With a single perturbation, DarkSAM renders SAM incapable of segmenting objects across diverse images with varying prompts, thereby exposing its vulnerability. To tackle the challenge of dual ambiguity in attack targets, we present a shadow target strategy to obtain semantic blueprint as a attack target. We then design a novel prompt-free hybrid spatial-frequency universal attack framework, which consists of a semantic decoupling-based spatial attack and a texture distortion-based frequency attack. By disrupting the crucial object features in both the spatial and frequency domains of the images, it successfully addresses the challenge of suboptimal attack efficacy, thus deceiving SAM. Our extensive experiments on SAM, HQ-SAM, and PerSAM across four datasets, both qualitatively and quantitatively, demonstrate DarkSAM's powerful attack ability and strong attack transferability.

In terms of limitations, DarkSAM may not be suitable for traditional segmentation models because its output is not a label-free mask. This characteristic might limit its applicability in scenarios where labeled masks are essential for accurate segmentation. The adversarial examples produced by DarkSAM could potentially mislead SAM-based segmentation platforms, posing significant security risks, particularly in sensitive domains like medical image analysis.