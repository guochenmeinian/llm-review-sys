# Lovasz Principle for Unsupervised Graph Representation Learning

Ziheng Sun\({}^{1,2}\)  Chris Ding\({}^{1}\)  Jicong Fan \({}^{1,2}\)

\({}^{1}\)School of Data Science, The Chinese University of Hong Kong, Shenzhen, China

\({}^{2}\)Shenzhen Research Institute of Big Data, Shenzhen, China

zihengsun@link.cuhk.edu.cn  {chrisding,fanjicong}@cuhk.edu.cn

Corresponding author

###### Abstract

This paper focuses on graph-level representation learning that aims to represent graphs as vectors that can be directly utilized in downstream tasks such as graph classification. We propose a novel graph-level representation learning principle called Lovasz principle, which is motivated by the Lovasz number in graph theory. The Lovasz number of a graph is a real number that is an upper bound for graph Shannon capacity and is strongly connected with various global characteristics of the graph. Specifically, we show that the handle vector for computing the Lovasz number is potentially a suitable choice for graph representation, as it captures a graph's global properties, though a direct application of the handle vector is difficult and problematic. We propose to use neural networks to address the problems and hence provide the Lovasz principle. Moreover, we propose an enhanced Lovasz principle that is able to exploit the subgraph Lovasz numbers directly and efficiently. The experiments demonstrate that our Lovasz principles achieve competitive performance compared to the baselines in unsupervised and semi-supervised graph-level representation learning tasks. The code of our Lovasz principles is publicly available on GitHub1.

## 1 Introduction

Graphs, such as chemical compounds, protein structures, and social networks, are non-Euclidean data that represent the relationships between entities. There have been a large number of previous works studying many aspects of graphs, including mutagenicity prediction of chemical compounds , protein structure prediction , and community analysis of social networks .

Graph-based learning problems can be organized into two categories: node-level learning and graph-level learning. In this paper, we will only focus on graph-level learning. It is known that in graph-level learning, one fundamental task or step is to measure the distance or similarity between graphs. An important class of methods comparing graphs is graph kernel and many graph kernels have been proposed in the past decades . For instance, random walk kernels  are the most widely-used and well-studied graph kernel family, which measure the graph similarity by counting the common random walks between graphs. The Weisfeiler-Lehman  family kernels are based on node label reassignment. Most graph kernels extract the similarity information between graphs by sampling sub-structures of graphs such as walks or reassigning the attributes of nodes with their neighborhoods. Note that graph kernels are implicit graph representation methods and hence their flexibilities are not high. In addition, the time and space complexities are quadratic with the number of graphs.

Graph representation learning aims to convert data with graph structure into vector representations that can be applied to various downstream tasks, such as graph clustering and classification. Many studies have been conducted on graph-level representation learning, and some of them use neural message-passing algorithms (Kipf et al., 2018; Xie and Grossman, 2018; Gilmer et al., 2017). For instance, the InfoGraph proposed by (Sun et al., 2019) achieves graph-level representations by maximizing the mutual information between the graph-level representation and the node-level representations. Graph contrastive learning (GraphCL) (You et al., 2020) and adversarial graph contrastive learning (AD-GCL) (Suresh et al., 2021) obtain graph-level representations by training graph neural networks (GNNs) to maximize the correspondence between the same graph's representations in its various augmented forms. JOint Augmentation Optimization (JOAO) (You et al., 2021) is a framework that automatically and adaptively selects data augmentations for GraphCL on specific graph data, using a unified bi-level min-max optimization approach. Automated Graph Contrastive Learning (AutoGCL) (Yin et al., 2022) uses learnable graph view generators and auto-augmentation strategy to generate contrastive samples while preserving the most representative structures of the original graph. These graph-level representation learning methods are all based on the InfoMax principle (Linsker, 1988). Note that there are many other graph representation learning methods such as VGAE (Kipf and Welling, 2016; Hamilton et al., 2017; Cui et al., 2020), graph embedding (Wu et al., 2020; Yu et al., 2021; Bai et al., 2019; Verma and Zhang, 2019), self-supervised learning (Liu et al., 2022; Hou et al., 2022; Lee et al., 2022; Xie et al., 2022; Wu et al., 2021; Rong et al., 2020; Zhang et al., 2021, 2021; Xiao et al., 2022), and contrastive learning (Le-Khac et al., 2020; Qiu et al., 2020; Ding et al., 2022; Xia et al., 2022; Fang et al., 2022; Trivedi et al., 2022; Han et al., 2022; Mo et al., 2022; Yin et al., 2022; Xu et al., 2021; Zhao et al., 2021; Zeng and Xie, 2021; Li et al., 2022, 2022, 2022), which will not be detailed in this paper due to the page length limit.

The InfoMax principle (Linsker, 1988), which is very popular in graph-level representation learning, advocates maximizing the mutual information between the representations of entire graphs and the representations of substructures of varying sizes (Peng et al., 2020; Velickovic et al., 2019; Hassani and Khasahmadi, 2020; Xie et al., 2022; Qiu et al., 2020). These InfoMax-based methods usually evaluate the mutual information (MI) between different representations using Jensen-Shannon MI estimator (Sun et al., 2019), following the formulations of \(f\)-GAN (Nowozin et al., 2016) and Mutual Information Neural Estimation (MINE) (Belghazi et al., 2018). However, the Jensen-Shannon MI estimator necessitates the training of a neural network parameterized discriminator, which is overly complex. In addition, the estimator is based on sampling, which may not be accurate enough in exploiting the mutual information. As opposed to InfoMax, researchers proposed the graph information bottleneck (GIB) (Wu et al., 2020) and the subgraph information bottleneck (SIB) (Yu et al., 2021) that aim to learn the minimal sufficient representation for downstream tasks. But GIB (Wu et al., 2020) and SIB (Yu et al., 2021) may fail if the downstream tasks are not available in the representation learning stage.

In this work, we introduce a novel graph learning principle called Lovasz principle, which is inspired by the Lovasz number (Lovasz, 1979) in graph theory. The Lovasz number is an upper bound for a graph's Shannon capacity. It is closely associated with various global characteristics of a graph, such as the clique number and chromatic number of the complement graph. The handle vector for calculating the Lovasz number is potentially a suitable choice for the graph-level representation, as it captures a graph's global features, though it suffers from a few difficulties. The contributions of this work are summarized as follows.

* We propose the Lovasz principle, a novel framework for unsupervised graph representation learning. We show how to effectively and efficiently utilize the handle vectors to represent graphs. The Lovasz principle exploits the topological structures of graphs globally via neural networks.
* We propose an enhanced Lovasz principle via effectively incorporating subgraph Lovasz numbers, while direct computation of subgraph Lovasz numbers is extremely costly. The enhanced Lovasz principle ensures similar graphs have similar representations.
* We extend the Lovasz principles to semi-supervised representation learning. Note that it is possible to adapt the Lovasz principles to more graph-based learning problems.

The experimental results of unsupervised learning, semi-supervised learning, and transfer learning on many benchmark graph datasets show that the proposed Lovasz principles outperform graph kernels, classical graph embedding methods, and InfoMax principle based representation learning methods.

## 2 Notations and Preliminaries

In this work, we use \(x\), \(\), \(\), \(\) (or \(X\)) to denote scalar, vector, matrix, and set respectively. \(_{ b}\) is a matrix of size \(a b\) consisting only ones. \(_{n}\) denotes an identity matrix of size \(n n\). Let \(G=(V,E)\) be a graph with \(n\) nodes and \(a\)-dimensional node features \(\{_{v}^{a}|v V\}\). We denote \(^{n n}\) as the adjacency matrix and \(=[_{1},...,_{n}]^{}^{n a}\) as the node features matrix. Let \(^{d}\) be the \(d\)-dimensional graph-level representation of \(G\), \(_{v}^{d}\) be the \(d\)-dimensional node-level representation of node \(v\), and \(=[_{1},...,_{n}]^{}^{n d}\) be the node-level representations matrix of \(G\). We denote \((p,q)\) as an edge between nodes \(p,q\) and \((p,q) E\) if they are connected.

Let \(=\{G_{1}, G_{N}\}\) be a dataset of \(N\) graphs with \(K\) classes, where \(G_{i}=(V_{i},E_{i})\). For \(G_{i}\), we denote its number of nodes as \(n_{i}\), graph-level representation as \(_{i}\), the adjacency matrix as \(_{i}\), the node feature matrix as \(_{i}\), and node-level representation matrix as \(_{i}\). The graph-level representation matrix of dataset \(\) is denoted as \(=[_{1},...,_{N}]^{}^{N d}\). The set of all node-level representations is denoted as \(=\{_{1},,_{N}\}\).

### Lovasz number

The definition of Lovasz number  is based on orthonormal representations of a graph. Therefore we first introduce the definition of orthonormal representations.

**Definition 2.1** (Orthonormal representations).: Given a graph \(G=(V,E)\) with \(|V|=n\). Let

\[:=\{^{d n}:\|_{p}\|_{2}=1,p=1,2, ,n;\;_{p}^{}_{q}=0,\;(p,q) E\},\] (1)

where \(_{p}\) is the \(p\)-th column of \(\). Then every \(\) is an orthonormal representation of \(G\) in \(^{d}\).

Clearly, every graph has at least one orthonormal representation. For example, a trivial representation is that each node \(p\) is represented by the standard basis vector \(_{p}\). Based on Definition 2.1, we introduce the Lovasz number  of a graph as follows.

**Definition 2.2** (Lovasz number).: The Lovasz number of a graph \(G=(V,E)\) is defined as

\[(G):=_{,}_{p V} ^{}_{p})^{2}},\] (2)

where \(^{d}\) ranges over all unit vectors. The vector \(\) yielding the minimum for (2), denoted by \(^{*}\), is called the _handle_ of the representation, where the corresponding \(\) is denoted as \(^{*}\) for convenience. \(^{*}\) is called the optimal representation of \(G\) in \(^{d}\).

Laszlo Lovasz provided a pentagon example, shown in Figure 1, to explain Lovasz number defined by (2). The visualization of \(^{*}\) and \(^{*}\) of a pentagon is like an umbrella whose handle is \(^{*}\) and the ribs are the five columns of \(^{*}\). These five disjoint node pairs, i.e., \((_{1}^{*},_{3}^{*}),(_{1}^{*},_{4}^{*}),(_{2}^{ *},_{4}^{*}),(_{2}^{*},_{5}^{*}),(_{3}^{*},_{5}^ {*})\), are orthogonal to each other in visualization.

### Lovasz theta kernel

Johansson _et al._ defined the Lovasz theta kernel to evaluate the similarity between graphs. Suppose \(S V\) is a subset of the vertices of graph \(G\), then the Lovasz number of the subgraph

Figure 1: Pentagon example for Lovasz numberinduced by \(S\) is defined as

\[_{S}(G):=_{}_{p S}^{}_{p})^{2}},\] (3)

where \(\) was pre-computed by Eq. (2) and \(\) ranges over all unit vectors.

**Definition 2.3** (Lovasz-\(\) kernel (Johansson _et al._, 2014)).: Let \(k\) be a base kernel. The Lovasz theta kernel between graphs \(G=(V,E)\) and \(G^{}=(V^{},E^{})\) is defined as

\[k_{}(G,G^{})=_{S V}_{S^{} V ^{}}|)}{C_{S,S^{}}}k(_{S}(G),_{S^{}}(G^{})),\] (4)

where \(C_{S,S^{}}=|}\), \((|S|,|S^{}|)=1\) if \(|S|=|S^{}|\), and \((|S|,|S^{}|)=0\) otherwise.

\(k_{}\) is a positive semi-definite kernel (Johansson _et al._, 2014). It is able to capture global properties of graphs and has been shown useful in SVM-based graph classification (Johansson _et al._, 2014).

## 3 Lovasz Principle for Graph Representation Learning

The Lovasz number \((G)\) of a graph \(G\) provides an insight into the global property of the graph. It is a unique and deterministic value associated with an orthonormal representation \(^{*}\) and a unit _handle_ vector \(^{*}\). The umbrella example in Figure 1 explains how to compute the Lovasz number: compacting the ribs (i.e. \(^{*}\)) as much as possible and using \(^{*}\) as the handle. This example provides intuition that the handle vector \(^{*}\) is a natural and suitable representation of the graph \(G\).

Given \(=\{G_{1},G_{2},,G_{N}\}\) drawn from an unknown distribution \(_{G}\), we want to represent each graph as a vector such that these vectors preserve some important information of \(_{G}\). Suppose we have an algorithm \(\) such that

\[(^{*}_{i},^{*}_{i})=(G_{i}), i=1,2,,N,\] (5)

where \(\) is some solver for (2). It is natural to use \(^{*}_{1},^{*}_{2},,^{*}_{N}\) as representations of \(G_{1},G_{2},,G_{N}\) respectively. However, this method has the following limitations4.

1. **Non-uniqueness** For any \(G_{i}\), both \(^{*}_{i}\) and \(^{*}_{i}\) are not unique. For example, let \(^{d d}\) be an orthonormal matrix, i.e., \(^{}=^{}=_{d}\), and let \(^{}_{i}=^{*}_{i}\) and \(^{}_{i}=^{*}_{i}\). We have \(\|^{}\|_{2}=1\), \((^{})^{}=(^{*}_{i})^{}^{*}_{i}\), and \((^{})^{}^{}_{p}=(G_{i})\). This means \((^{*}_{i},^{*}_{i})\) and \((^{}_{i},^{})\) yield the same Lovasz number for \(G_{i}\), though they could be very different. Thus, for two graphs \(G_{i}\) and \(G_{j}\) in \(\), even when they are isomorphic, \(^{*}_{i}\) and \(^{*}_{j}\) could be very different. However, for graph representation, we hope that similar graphs have similar representations. For two graphs \(G_{i}\) and \(G_{j}\), one may align their orthonormal representations using \(}=_{^{}=_{d}}\|^{*}_{i}- ^{*}_{j}\|_{F}^{2}\) and compare them according to \(\|^{*}_{i}-^{*}_{j}}\|_{2}\). This however only works when the \(n_{i}=n_{j}\) and \(G_{i}\) and \(G_{j}\) are matched.
2. **High computational cost** For each \(G_{i}\) in \(\), we need to solve the optimization problem (2), for which the time complexity of SDP is at least \((|E_{i}|n_{i}^{2.5})\)(Jiang _et al._, 2020). Thus the total time complexity for \(\) is \((_{i=1}^{N}|E_{i}|n_{i}^{2.5})\). Therefore, this representation method is not scalable to large datasets.
3. **Ignorance of node features** The computation of (5) solely relies on the graph structure and does not take advantage of the node feature matrix \(_{i}\) that is often available and informative.
4. **Non-generalization** Suppose we have some new graphs and want to obtain their representations. We cannot utilize the representations of \(\) and we have to solve (2) again for each new graph.
5. **Non-global sensing** The computation of (5) treats each graph separately and cannot effectively take advantage of the global information or structure of \(\). Individual graphs may have noise or outliers, which cannot be handled by a local method.

To solve the aforementioned five issues, we present a machine learning method. We use a neural network \(_{W}\) (parameterized by \(W\)) to approximate \(\). \(_{W}\) can be learned from \(\) as well as some additional information such as the node feature matrices \(\{_{1},,_{N}\}\). Specifically, we hope that

\[(_{i}^{*},_{i}^{*})_{W}(_{i},_{i}),  i=1,2,,N.\] (6)

Thus, \(_{W}\) plays a role representing a graph (drawn from \(_{G}\)) to a matrix of nodes representation and a vector of graph representation. For new graphs sampled from \(_{G}\), \(_{W}\) should generalize well when the approximation errors in (6) are small enough and \(_{W}\) is not too complex. For convenience, we split \(_{W}\) into two parts, i.e., \(_{W}(,)=(F(,;),f(,;))\), though \(F\) and \(f\) can share some parameters. We let \(_{i}^{*} F(_{i},_{i};)\) and \(_{i}^{*} f(_{i},_{i};)\). \(F(,;)\) is the model of node-level representation learning while \(f(,;)\) is the model of graph-level representation learning. We let

\[_{i}^{}:=F(_{i},_{i};),\ \ \ \ _{i}^{}:=f(_{i},_{i};),\ \  i=1,2,...,N.\] (7)

We denote the graph-level representations matrix as \(_{}=[_{1}^{},...,_{N}^{}]^{}\) and the node-level representations set as \(_{}=\{_{1}^{},...,_{N}^{}\}\). To achieve (6), we propose to solve

\[}_{i=1}^{N}}_{i}^{})^{}_{p}^{})^{2} }}_{_{1}}+_{i}_{i}^ {}(_{i}^{})^{}-_{n_{i}}_{F}^{2}}_{ _{2}}+(_{i}^{})^{}_{i}^{}-1 ^{2}}_{_{3}}},\] (8)

where \(_{i}=_{n_{i} n_{i}}-_{i}\) is a mask matrix and \(>0\) is a regularization parameter. The roles of \(_{1}\), \(_{2}\), and \(_{3}\) in (8) are explained as follows.

* \(_{1}\) corresponds to the objective in the definition of Lovasz number of \(G_{i}\).
* \(_{2}\) is to approximate the orthonormal representation for \(G_{i}\), i.e., \((_{p}^{})^{}_{q}^{} 0\) if \((p,q) E_{i}\) and \(\|_{p}\|_{2} 1\  p V_{i}\).
* \(_{3}\) corresponds to the unit-length requirement for the handle vector of \(G_{i}\), i.e., \(\|_{i}^{}\|_{2} 1\).

We call (8) **Lovasz principle**SS, since it aims to learn an \(_{W}\) to solve the optimization of Lovasz number for the graphs drawn from \(_{G}\). It is known that the Lovasz number \((G)\) is an upper bound on the Shannon capacity of \(G=(V,E)\). The Shannon capacity [Shannon, 1956] models the amount of information that can be transmitted across a noisy communication channel, where certain signal values can be confused with each other. Here, one signal value corresponds to one node of \(G\) and \((p,q) E\) means that the corresponding two signals can be confused with each other. Therefore, the graph-level and node-level representations given by our Lovasz principle correspond to the upper bound of the amount of information transmitted over the graph that is distinguishable between nodes.

Footnote §: We also provide an equivalent formulation based on the complement graph of \(G\) in Appendix A.

Note that instead of the regularized unconstrained optimization (8), we can also use constrained optimization (\(_{2}=_{3}=0\)), which we call strict Lovasz principle. We may use the Lagrange multipliers method, projected gradient descent, or exact (or inexact) penalty method to solve the constrained optimization. Take the inexact penalty method as an example, we just need to increase the \(\) in (8) gradually in the optimization. The graph representation performance comparison between unconstrained and constrained optimizations will be shown in Section 6.5 and Appendix E.

For convenience, we let

\[_{}:=_{i=1}^{||}_{p V_{i}}_{i}^{})^{}_{p}^{})^{2}}+(_{i} _{i}^{}(_{i}^{})^{}-_{n_{i}} _{F}^{2}+(_{i}^{})^{}_{i}^{}-1 ^{2}),\] (9)

and call it Lovasz loss. The Lovasz loss is mainly designed for unsupervised graph-level representation learning [Wu _et al._, 2022; Maron _et al._, 2019; Oono and Suzuki, 2019; Stahlberg _et al._, 2022], which can be used as an alternative to the popular InfoMax loss [Linsker, 1988] (see (16)).

Lovasz principle for semi-supervised learningInspired by InfoGraph [Sun _et al._, 2019] (see (17)), we propose a Lovasz loss function for semi-supervised learning tasks. Suppose the dataset \(\) hastwo subsets: a labeled dataset \(^{L}\) and an unlabeled dataset \(^{U}\). Then we deploy another supervised encoder with parameter \(\) and generate the supervised node-level representations \(_{i}^{}\), graph-level representations \(_{i}^{}\), and then prediction \(}_{i}^{}\). The overall loss function is

\[_{}:=_{l=1}^{|^{L}|}_{}(}_{l}^{},_{l})+_{}()+_{i=1}^{| |}\|_{i}^{}-_{i}^{}\|_{2}^{2},\] (10)

where \(\) is a positive hyperparameter, the supervised loss \(_{}\) is the cross-entropy loss, and the unsupervised loss \(_{}\) is the Lovasz loss \(_{}\) (Eq. (9)) or the enhanced Lovasz loss \(_{}\) (Eq. (14)). The last term encourages the representations learned by the two encoders to be similar.

## 4 Enhancing Lovasz Principle with Subgraph Lovasz Number

Lovasz principle does not explicitly utilize the Lovasz number in graph embedding, though the Lovasz numbers of subgraphs can be useful in comparing graphs (Johansson _et al._, 2014). Therefore, we propose to use subgraph Lovasz number to enhance Lovasz principle based graph representation learning. We may consider taking advantage of the Lovasz-\(\) kernel proposed by (Johansson _et al._, 2014). However, we encounter the following two difficulties.

1. Computing the Lovasz numbers (3) of subgraphs is time-consuming because we need to solve (2) for every graph and the number of subgraphs of each graph is often very large (up to \(2^{|V|}\)). Hence, for large graph dataset, we cannot use (4) directly.
2. The Lovasz-\(\) kernel (4) is a pair-wise method and cannot effectively exploit the global structure of \(\).

To solve the aforementioned problems, we present an iterative-refinement strategy that computes the subgraph Lovasz numbers using the embeddings given by the Lovasz principle. Specifically, at iteration \(t\), we have the graph-level representations \(_{}^{(t-1)}\) and the node-level representations \(_{}^{(t-1)}\) given by iteration \(t-1\). Inspired by the Lovasz-\(\) kernel (4), we compute the similarity between graph \(G_{i}\) and \(G_{j}\) as

\[K_{ij}^{(t-1)}=_{S_{i} V_{i}}_{S_{j} V_{j}}|,|S_{j}|)}{C_{S_{i},S_{j}}}k(_{S_{i}}^{(t-1)}(G_{i}), _{S_{j}}^{(t-1)}(G_{j})),\] (11)

where \(C_{S_{i},S_{j}}=|}{|S_{i}|}|}{|S_{j}|}\) and \(_{S_{i}}^{(t-1)}(G_{i})\) (similar for \(G_{j}\)) is obtained by

\[_{S_{i}}^{(t-1)}(G_{i})=_{p S_{i}}_{i}^{(t-1) }_{p}^{(t-1)})^{2}}.\] (12)

The computation of \(1/(_{i}^{(t-1)}_{p}^{(t-1)})^{2}\) for every \(p V_{i}\) was already done when computing \(_{}\) via (9) at iteration \(t-1\) and there is no need to solve (3). For (11), we do not need to consider all possible subgraphs and we can just randomly sample subgraphs with some fixed sizes (numbers of nodes), which is similar to the truncated Lovasz-\(\) kernel of (Johansson _et al._, 2014). Thus we can obtain the similarity \(K_{ij}^{(t-1)}\) efficiently. Adapting the idea of spectral embedding (Belkin and Niyogi, 2001), we propose the following subgraph Lovasz number (SLN) loss (at iteration \(t\))

\[_{}^{(t)}:=_{i=1}^{||}_{j=1}^{| |}K_{ij}^{(t-1)}\|_{i}^{}-_{j}^{}\|_ {2}^{2}+(\|_{}^{}_{}-_{d}\| _{F}^{2}+\|_{}^{}_{N 1}\|_{2}^{2}),\] (13)

where \(>0\). The two regularization terms in \(_{}^{(t)}\) aim to make the graph-level representations orthonormal and centered, which is consistent with the constraints in spectral embedding. Minimizing \(_{}^{(t)}\) encourages that the graph-level representations of similar graphs (in the sense of subgraph Lovasz numbers) are closer to each other at iteration \(t\). Integrating (13) with (9), we obtain the following enhanced Lovasz loss at iteration \(t\)

\[_{}^{(t)}:=_{}^{(t)}+_ {}^{(t)},\] (14)

where \(>0\) is a hyperparameter. It is worth noting that \(_{}\) as well as \(_{}\) can be implemented batch-wisely, via replacing \(\) with its subsets. Similar to \(_{}\), \(_{}\) can also be applied to semi-supervised graph classification, i.e., (10).

## 5 Related Work

Besides the Lovasz-\(\) introduced in Section 2.2, the closest work to our Lovasz principle is the InfoMax principle. Following , suppose the node-level representation \(_{p}(x)\) and the graph-level representation \((x)\) are depending on the input \(x\), \(T_{}\) is a discriminator parameterized by a neural network with parameters \(\), the Jensen-Shannon mutual information (MI) estimator \(I_{}\) between \(_{p}\) and \(\) is defined as

\[I_{}(_{p},)=_{}[-(-T_{} (_{p}(x),(x)))]-_{}}[ (T_{}(_{p}(x^{}),(x)))],\] (15)

where \(x\) is the input sample from distribution \(\), \(x^{}\) is the negative sample from distribution \(}\), and \((a)=(1+e^{a})\) denotes the softplus function. Many recent graph-level representation learning methods  are based on the InfoMax principle, i.e., maximizing (15). For instance, the InfoGraph proposed by  obtains graph-level representations by maximizing the mutual information between the graph-level representation and the node-level representations as follows

\[^{*},^{*},^{*}=*{arg\,max}_{,,} _{i=1}^{||}|}_{p V_{i}}I_{}(_{p}^{},_{i}^{})-_{ }^{I_{}}().\] (16)

For semi-supervised learning, the dataset \(\) is split into labeled dataset \(^{L}\) and unlabeled dataset \(^{U}\). They deploy another supervised encoder with parameter \(\) and then generate the supervised node-level representations \(_{i}^{}\), graph-level representations \(_{i}^{}\) and prediction \(}_{i}^{}\). The loss function of InfoGraph for semi-supervised learning is defined as follows

\[_{}=_{l=1}^{|^{L}|}_{}(}_{l}^{},_{l})+_{}^{I_{}}()-_{i=1}^{||} |}I_{}(_{i}^{}._{i}^{}).\] (17)

The comparison between the InfoMax principle and our Lovasz principle is as follows.

* The InfoMax principle focuses on the mutual information between graph-level representation and node-level representation, while our Lovasz principle is derived from the Lovasz number, a fundamental topological property of graph.
* Our Lovasz principle only needs to optimize \(\) and \(\). Differently, besides \(\) and \(\), the InfoMax principle has to optimize an additional discriminator parameter \(\) for the Jensen-Shannon MI estimator. Thus, our Lovasz principle is simpler than the InfoMax principle.
* Approximating mutual information using neural network is challenging  and the Jensen-Shannon MI estimator \(I_{}\) only provides an approximation by sampling rather than an exact computation. In contrast, our Lovasz principle does not rely on mutual information and sampling.

It is worth noting that the Lovasz convolutional networks (LCN) proposed by  was motivated by the observation that removing certain vertices from a graph doesn't affect the graph's global properties such as the Lovasz number. LCN does not involve any optimization related to the Lovasz number and was designed as an alternative to GCN. Our Lovasz principle is an optimization principle that can be used in any graph neural network (e.g. LCN). It is also useful in many applications such as graph prompt learning  and graph anomaly detection .

## 6 Experiments

In this section, we evaluate the effectiveness of the Lovasz principle compared to the InfoMax principle in graph representation learning methods and a few other baselines such as graph kernels. The graph representation learning methods we considered in this paper include InfoGraph , GraphCL , AD-GCL , JAO , and AutoGCL , which are the most current and influential methods spanning from

[MISSING_PAGE_FAIL:8]

### Semi-supervised Learning

Following [Hu _et al._, 2019; You _et al._, 2021; Yin _et al._, 2022], we compare Lovasz principle with InfoMax principle in semi-supervised learning tasks. The semi-supervised losses of our Lovasz principle based methods and InfoMax based methods \(_{}\) were shown in (10) and (17) respectively. Following the settings of AutoGCL [Yin _et al._, 2022], we employ a 10-fold cross-validation on each dataset. For each fold, we use 80% of the total data as the unlabeled data, 10% as labeled training data, and 10% as labeled testing data. The classifier for labeled data is a ResGCN [Chen _et al._, 2019] with 5 layers and a hidden size of 128. We repeat each experiment 10 times and report the average accuracy in Table 2. We see that our Lovasz loss \(_{}\) and the enhanced Lovasz loss \(_{}\) outperformed InfoMax loss in all cases. Furthermore, \(_{}\) outperformed \(_{}\) in most cases. These results are consistent with those in Secon 6.1.

### Transfer Learning

Following [Hu _et al._, 2019; You _et al._, 2021; Yin _et al._, 2022], we compare the performance of our Lovasz principles with the InfoMax principle in the task of transfer learning. We use the Pretrain-GNN method [Hu _et al._, 2019] as a baseline and employ the Infomax, EdgePred, AttrMasking, and ContextPred pre-training strategies. The experimental settings follow those of AutoGCL [Yin _et al._, 2022]. More details are in the appendix. As shown in Table 3, the improved Lovasz loss \(_{}\) performs the best on transfer learning tasks. In addition, the Lovasz principle based methods generally outperform those based on the InfoMax principle in most cases.

### Overall Performance and Significance Analysis

For convenience, we show the average performance of all methods over all datasets in Figure 2. We see our Lovasz principles outperformed other methods in the three tasks.

To measure the significance of the improvement over the baselines, we implement paired t-tests on the mean scores obtained from the datasets. A p-value below 0.05 indicates a significant difference. The results presented in Table 4 demonstrate the statistical significance of the improvements achieved by our methods across all the datasets.

   & methods & NCHI & PROTEINS & DD & COLLAB & REDDIT-B & REDDIT-MSK & GITHUB \\   & _no Pretrain_ & 73.72\(\)0.24 & 70.40\(\)1.54 & 73.56\(\)0.41 & 73.71\(\)0.27 & 86.63\(\)0.27 & 51.33\(\)0.44 & 60.87\(\)0.17 \\  Pretrain- & Infomax & 74.86\(\)0.26 & 72.27\(\)0.40 & 75.78\(\)0.34 & 73.76\(\)0.29 & 88.66\(\)0.95 & 53.61\(\)0.31 & 65.21\(\)0.88 \\ GNN & ContextPred & 73.00\(\)0.30 & 70.23\(\)0.63 & 74.66\(\)0.51 & 3.69\(\)0.37 & 84.76\(\)0.52 & 51.23\(\)0.84 & 62.35\(\)0.73 \\   InfoMax \\ principle \\  } & GraphCL & 74.63\(\)0.23 & 74.17\(\)0.34 & 76.17\(\)0.37 & 74.23\(\)0.21 & 89.71\(\)0.19 & 52.55\(\)0.46 & 65.81\(\)0.79 \\  & AD-GCL & 75.18\(\)0.31 & 73.96\(\)0.47 & 77.91\(\)0.73 & 75.82\(\)0.26 & 90.10\(\)0.15 & 53.49\(\)0.28 & 64.17\(\)1.38 \\  & JOAOv2 & 74.86\(\)0.39 & 73.31\(\)0.48 & 75.81\(\)0.73 & 75.53\(\)0.18 & 88.79\(\)0.65 & 52.71\(\)0.28 & 66.66\(\)0.60 \\  & AutoGCL & 73.75\(\)2.25 & 75.65\(\)2.40 & 77.50\(\)4.41 & 77.16\(\)1.48 & 79.80\(\)3.47 & 49.91\(\)2.70 & 62.46\(\)1.51 \\   Lovasz & GraphCL & 75.46\(\)1.53 & 75.12\(\)1.87 & 77.74\(\)1.52 & 76.12\(\)1.15 & 89.87\(\)1.68 & 53.99\(\)1.68 & 66.72\(\)1.53 \\   & AD-GCL & 76.62\(\)1.87 & 74.21\(\)1.78 & 78.21\(\)1.39 & 76.27\(\)1.74 & 90.36\(\)1.56 & 54.06\(\)1.32 & 65.32\(\)1.04 \\   & JOAOv2 & 76.13\(\)1.76 & 73.73\(\)1.86 & 76.27\(\)1.48 & 77.35\(\)1.27 & 89.31\(\)1.85 & 53.17\(\)1.76 & 66.35\(\)1.96 \\   & AutoGCL & 75.77\(\)1.48 & 76.30\(\)1.57 & 78.16\(\)1.61 & 76.63\(\)1.78 & 84.64\(\)2.53 & 51.31\(\)1.81 & 64.87\(\)1.62 \\   Lovasz \\ principle \\ (use \(_{}\)) \\  } & GraphCL & 75.81\(\)1.68 & 75.38\(\)1.67 & 78.43\(\)1.48 & 75.75\(\)1.58 & 50.67\(\)1.27 & 54.81\(\)1.73 & 67.04\(\)1.45 \\   & AD-GCL & **77.28\(\)1.04** & 75.43\(\)1.58 & 78.67\(\)1.64 & 76.98\(\)1.87 & **91.54\(\)1.39** & **55.46\(\)1.59** & 66.87\(\)1.25 \\   & JOAOv2 & 76.25\(\)1.59 & 74.67\(\)1.37 & 77.96\(\)1.86 & **78.84\(\)1.75** & 90.25\(\)1.22 & 54.32\(\)1.89 & **67.52\(\)1.73** \\   & AutoGCL & 76.53\(\)1.92 & **76.89\(\)1.55** & **78.82\(\)1.90** & 78.46\(\)1.39 & 87.31\(\)1.57 & 53.17\(\)1.50 & 66.47\(\)1.26 \\  

Table 2: Performance (ACC) of semi-supervised learning.

   & methods & BBBP & Tox21 & ToxCast & SIDER & ClinTox & MUV & HIV & BACE \\   & _no Pretrain_ & 65.8\(\)4.3 & 74.0\(\)0.8 & 63.4\(\)0.6 & 57.3\(\)1.6 & 58.0\(\)4.4 & 71.8\(\)2.5 & 75.3\(\)1.9 & 70.1\(\)5.4 \\  Pretrain- & Infomax & 68.8\(\)0.8 & 75.3\(\)0.5 & 62.7\(\)0.4 & 58.0\(\)0.8 & 69.9\(\)3.0 & 75.3\(\)2.5 & 76.0\(\)0.7 & 75.9\(\)1.6 \\ GNN-s & EdgePred & 67.3\(\)2.4 & 76.0\(\)0.6 & 6.41\(\)0.6 & 60.0\(\)0.7 & 61.1\(\)3.7 & 74.1\(\)2.1 & 76.3\(\)1.0 & 79.9\(\)0.9 \\ strategies & AutoGCL & 64.3\(\)2.8 & 76.7\(\)0.4 & 64.2\(\)0.5 & 61.0\(\)0.7 & 71.8\(\)1.4

### Measuring the Quality of Solver Approximation

Given a GNN model \(_{W}\) trained via the Lovasz principle, the predicted Lovasz number of a graph \(G\) is denoted as \((G)\), while the ground-truth Lovasz number \((G)\) can be computed by SDP (Wolkowicz _et al._, 2012). Then we define the relative prediction error for the Lovasz number as

\[e_{}=|(G)-(G)|/(G).\] (18)

Besides the regularized optimization of the Lovasz principle in (9), we also propose a constrained optimization method in Appendix E. We select 50 graphs from each of the four datasets and report \(e_{}\) given by both the regularized (\(=10\)) optimization and the constrained optimization for Lovasz principle in Table 5. We can see that in almost all cases, the relative prediction errors are less than \(10\%\). This indicates that the \(_{W}\) trained by the Lovasz principle is a good and reliable approximator for the solver \(\) of the Lovasz number. This is similar to the idea of learning to optimize.

### More Numerical Results

The results of **parameter sensitivity analysis, ablation study**, and **time cost comparison** are in Appendix C, Appendix D, and Appendix F respectively.

## 7 Conclusions

This paper proposed a novel method called Lovasz principle for unsupervised graph-level representation learning. An extension using the subgraph Lovasz number was also presented. The numerical results of unsupervised learning, semi-supervised learning, and transfer learning showed that the proposed methods are more effective than graph kernels and InfoMax principle based representation learning methods. Besides unsupervised representation learning, it is possible to apply our methods to other tasks such as graph-level clustering and graph generation. For instance, we can add a clustering module (e.g. (Xie _et al._, 2016)) to \(_{}\) to construct an end-to-end clustering algorithm. We can combine \(_{}\) with variational autoencoder (Kingma and Welling, 2013) to train a model to generate new graphs. Nevertheless, the implementation of these methods is out of the scope of this paper.

  tasks & principles comparison & InfoGraph & GraphCL & AD-GCL & JOAOV2 & AutoGCL \\   & InfoMax vs Lovasz (\(_{}\)) & 0.00067 & 0.00286 & 0.02238 & 0.07347 & 0.00059 \\  & InfoMax vs Lovasz (\(_{}\)) & 0.00005 & 0.01626 & 0.01541 & 0.01319 & 0.00035 \\  & Lovasz (\(_{}\)) vs Lovasz (\(_{}\)) & 0.00429 & 0.10925 & 0.01522 & 0.00079 & 0.00466 \\   & InfoMax vs Lovasz (\(_{}\)) & - & 0.00028 & 0.01115 & 0.04290 & 0.02147 \\  & InfoMax vs Lovasz (\(_{}\)) & - & 0.00051 & 0.00051 & 0.00116 & 0.01129 \\  & Lovasz (\(_{}\)) vs Lovasz (\(_{}\)) & - & 0.00169 & 0.00076 & 0.00133 & 0.00545 \\  

Table 4: Significance analysis (\(p\)-values) of improvement via the paired t-test. A \(p\)-value less than 0.05 indicates a significant improvement.

  \(e_{}\) (\%) & MUTAG & PROTEINS & DD & NCII \\  regularized optimization & 9.7\(\) 3.4 & 8.2\(\)2.1 & 6.3\(\)1.1 & 10.2\(\) 3.6 \\ constrained optimization & 6.5\(\) 2.4 & 7.3\(\)1.6 & 6.1\(\)1.2 & 8.5\(\) 2.3 \\  

Table 5: Relative prediction errors \(e_{}\) given by regularized optimization and constrained optimization

Figure 2: The average performance of different types of methods