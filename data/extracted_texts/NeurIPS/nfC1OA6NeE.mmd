# SDEs for Adaptive Methods: The Role of Noise

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Despite the vast empirical evidence supporting the efficacy of adaptive optimization methods in deep learning, their theoretical understanding is far from complete. In this work, we introduce novel SDEs for commonly used adaptive optimizers: SignSGD, RMSprop(W), and Adam(W). Our SDEs offer a quantitatively accurate description of these optimizers and help bring to light an intricate relationship between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD highlights a noteworthy and precise contrast to SGD in terms of convergence speed, stationary distribution, and robustness to heavy-tail noise. We extend this analysis to AdamW and RMSpropW, for which we observe that the role of noise is much more complex. Crucially, we support our theoretical analysis with experimental evidence by verifying our insights: this includes numerically integrating our SDEs using Euler-Maruyama discretization on various neural network architectures such as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the behavior of the respective optimizers, especially when compared to previous SDEs derived for Adam and RMSprop. We believe our approach can provide valuable insights into best training practices and novel scaling rules.

## 1 Introduction

Adaptive optimizers lay the foundation for effectively training of modern deep learning models. These methods are typically employed to optimize an objective function expressed as a sum across \(N\) individual data points: \(_{x^{d}}[f(x):=_{i=1}^{N}f_{i}(x)],\) where \(f,f_{i}:^{d},\ \ i=1,,N\).

Due to the practical difficulties of selecting the learning rate of stochastic gradient descent, adaptive methods have grown in popularity over the past decade. At a high level, these optimizers adjust the learning rate for each parameter based on the historical gradients. Popular optimizers that belong to this family are RMSprop (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2015), SignSGD (Bernstein et al., 2018), AdamW (Loshchilov and Hutter, 2019), and many other variants. SignSGD is often used for compressing gradients in distributed machine learning (Karimireddy et al., 2019), but it also has gained popularity due to its connection to RMSprop and Adam (Balles and Hennig, 2018). The latter algorithms have emerged as the standard methods for training modern large language models, partly because of enhancements in signal propagation (Noci et al., 2022).

Although adaptive methods are widely favored in practice, their theoretical foundations remain enigmatic. Recent research has illuminated some of their advantages: Zhang et al. (2020) demonstrated how gradient clipping addresses heavy-tailed gradient noise, Pan and Li (2022) related the success of Adam over SGD to sharpness, and Yang et al. (2024) showed that adaptive methods handle large gradients better than SGD. At the same time, many optimization studies focus on worst-case convergence rates: These rates (e.g., Defossez et al. (2022)) are valuable, yet they provide an incomplete depiction of algorithm behavior, showing no quantifiable advantage over standard SGD. One particular aspect still lacking clarity is the precise role of noise in the algorithm trajectory.

Our investigation aims to study how gradient noise influences the dynamics of adaptive optimizers and how it impacts their asymptotic behaviors in terms of expected loss and stationary distribution. In particular, we want to understand which algorithms are more resilient to high (possibly heavy-tailed) gradient noise levels. To do this, we rely on stochastic differential equations (SDEs) which have become popular in the literature to study the behavior of optimization algorithms (Li et al., 2017; Jastrzebski et al., 2018). These continuous-time models unlock powerful tools from Ito calculus, enabling us to establish convergence bounds, determine stationary distributions, unveil implicit regularization, and elucidate the intricate interplay between landscape and noise. Notably, SDEs facilitate direct comparisons between optimizers by explicitly illustrating how each hyperparameter and certain landscape features influence their dynamics (Compagnoni et al., 2024).

We begin by analyzing SignSGD, showing how the signal-to-noise ratio affects its dynamics and elucidating the impact of noise at convergence. After analyzing the case where the gradient noise exhibits infinite variance, we extend our analysis to Adam and RMSprop with decoupled weight decay (Loshchilov and Hutter, 2019) - i.e. AdamW and RMSpropW: for both, we refine batch size scaling rules and compare the role of noise to SignSGD. Our analysis provides some theoretical grounding for the resilience of these adaptive methods to high noise levels. Importantly, we highlight that Adam and RMSprop are byproducts of our analysis and that our novel SDEs are derived under much weaker and more realistic assumptions than those in the literature (Malladi et al., 2022).

ContributionsWe identify our key contributions as follows:

1. We derive the first SDE for SignSGD under very general assumptions: We show that SignSGD exhibits three different phases of the dynamics and characterize the loss behavior in these phases, including the stationary distribution and asymptotic loss value.
2. We demonstrate that for SignSGD, noise inversely affects the convergence rate of both the loss and the iterates. Differently, it has a linear impact on the asymptotic expected loss and the asymptotic variance of the iterates. This is in contrast to SGD, where noise does not influence the convergence speed, but it has a quadratic effect on the loss and variance of the iterates. Finally, we show that, even if the noise has infinite variance, SignSGD is very resilient: its performance is only marginally impacted. In the same conditions, SGD would diverge.
3. We derive new, improved, SDEs for AdamW and RMSpropW and use them to (1) show a novel batch size scaling rule and (2) inspect the stationary distribution and stationary loss value in convex quadratics. In particular, we dive into the properties of weight decay: while for vanilla Adam and RMSprop the effect of noise at convergence mimics SignSGD, something different happens in AdamW and RMSpropW -- Due to an intricate interaction between noise, curvature, and regularization, weight decay plays a crucial stabilization role at high noise levels near the minimizer.
4. We empirically verify every theoretical insight we derive. Importantly, we integrate our SDEs with Euler-Maruyama to confirm that our SDEs faithfully track their respective optimizers. We do so on an MLP, a CNN, a ResNet, and a Transformer. For RMSprop and Adam, our SDEs exhibit superior modeling power than the SDEs already existing in the literature.

## 2 Related work

SDE approximations and applications.(Li et al., 2017) introduced a formal theoretical framework aimed at deriving SDEs that effectively model the inherent stochastic nature of optimizers. Ever since, SDEs have found several applications in the field of machine learning, for instance in connection with _stochastic optimal control_ to select the stepsize (Li et al., 2017, 2019) and batch size (Zhao et al., 2022), the derivation of _convergence bounds_ and _stationary distributions_(Compagnoni et al., 2023, 2024), _implicit regularization_(Smith et al., 2021), and _scaling rules_(Jastrzebski et al., 2018). Previous work by Malladi et al. (2022) has already made strides in deriving SDE models for RMSprop and Adam, albeit under certain restrictive assumptions. They establish a scaling rule which they assert remains valid throughout the entirety of the dynamics. Unfortunately, their derivation is based on the approach of Jastrzebski et al. (2018) which is problematic in the general case (See Appendix E for a detailed discussion). Indeed, we demonstrate that the SDEs derived in Malladi et al. (2022) are only accurate around minima, indicating that their scaling rule is not _globally_ valid. (Zhou et al., 2020a) also claimed to have derived a Levy SDE for Adam. Unfortunately, the quality of their SDE approximation does not come with theoretical guarantees. Additionally, their SDE has random coefficients: an approach which is theoretically sound in very limited settings (Kohatsu-Higa et al., 1997; Bishop and Del Moral, 2019). Zhou et al. (2024) informally presented an SDE for (only) the parameters of AdamW: this is achieved under strong assumptions and various approximations, some of which are hard to motivate formally.

Influence of noise on convergence.Several empirical papers demonstrate that adaptive algorithms adjust better to the noise during training. Specifically, (Zhang et al., 2020) noticed a consistent gap in the performance of SGD and Adam on language models and connected that phenomenon with heavy-tailed noise distributions. (Pascanu et al., 2013) suggests using gradient clipping to deal with heavy tail noise, and consequently several follow-up works analyzed clipped SGD under heavy-tailed noise (Zhang et al., 2020; Mai and Johansson, 2021; Puchkin et al., 2024). Kunstner et al. (2024) present thorough numerical experiments illustrating that a significant contributor to heavy-tailed noise during language model training is class imbalance, where certain words occur much more frequently than others. They demonstrate that adaptive optimization methods such as Adam and SignSGD can better adapt to such class imbalances. However, the theoretical understanding of the influence of noise in the context of adaptive algorithms is much more limited. The first convergence results on Adam and RMSprop were derived under bounded stochastic gradients assumption (De et al., 2018; Zaheer et al., 2018; Chen et al., 2019; Defossez et al., 2022). Later, this noise model was relaxed to weak growth condition (Zhang et al., 2022; Wang et al., 2022) and its coordinate-wise version (Hong and Lin, 2023; Wang et al., 2024) and sub-gaussian noise (Li et al., 2023). SignSGD and its momentum version Signum were originally studied as a method for compressed communication (Bernstein et al., 2018) under bounded variance assumption, but with a requirement of large batches. Several works provided counterexamples where SignSGD fails to converge if stochastic and full gradients are not correlated enough (Karimireddy et al., 2019; Safaryan and Richtarik, 2021). In the case of AdamW, (Zhou et al., 2022, 2024) provide convergence guarantees under restrictive assumptions such as bounded gradient and bounded noise. All aforementioned results only show that SignSGD, Adam, and RMSprop at least do not perform worse than vanilla SGD. None of them studied how noise affects the dynamics of the algorithm: In this work, we attempt to close this gap.

## 3 Formal statements & insights: the SDEs

This section provides the general formulations of the SDEs of SignSGD (Theorem 3.2) and AdamW (Theorem 3.12). Due to the technical nature of the analysis, we refer the reader to the appendix for the complete formal statements and proofs.

Assumptions and notation.In this section, we assume that \( f_{}(x)= f(x)+Z(x)\), \([Z(x)]=0\) and, unless we study the cases where the gradient variance is unbounded, we write \(Cov(Z(x))=(x)\) where we omit the batch size unless relevant. To derive the stationary distribution around an optimum, we will approximate the loss function with a quadratic convex function \(f(x)=x^{}Hx\) as commonly done in the literature (Ge et al., 2015; Levy, 2016; Jin et al., 2017; Poggio et al., 2017; Mandt et al., 2017; Compagnoni et al., 2023). Regarding the notation, \(>0\) is the step size, the mini-batches \(\{_{k}\}\) are of size \(B 1\) and modeled as i.i.d. random variables uniformly distributed on \(\{1,,N\}\). The \(\) parameters refer to momentum parameters, \(>0\) is the (decoupled) \(L^{2}\)-regularization parameter, and \(>0\) is a small scalar used for numerical stability.

The following definition formalizes the idea that an SDE can be a "good model" to describe an optimizer. It is drawn from the field of numerical analysis of SDEs (see Mil'shtein (1986)) and it quantifies the disparity between the discrete and the continuous processes.

**Definition 3.1** (Weak Approximation).: A continuous-time stochastic process \(\{X_{t}\}_{t[0,T]}\) is an order \(\) weak approximation (or \(\)-order SDE) of a discrete stochastic process \(\{x_{k}\}_{k=0}^{ T/}\) if for every polynomial growth function \(g\), there exists a positive constant \(C\), independent of the stepsize \(\), such that \(_{k=0,, T/}|g(x_{k})- g(X_{kn})| C^{}\).

### SignSGD SDE

In this section, we derive an SDE model for SignSGD, which we believe to be a novel addition to the existing literature. This derivation will reveal the unique manner in which noise influences the dynamics of SignSGD. First, we recall the update equation of SignSGD:

\[x_{k+1}=x_{k}-( f_{_{k}}(x_{k})).\] (1)The following theorem derives a formal continuous-time model for SignSGD.

**Theorem 3.2** (Informal Statement of Theorem C.5).: _Under sufficient regularity conditions, the solution of the following SDE is an order \(1\) weak approximation of the discrete update of SignSGD:_

\[dX_{t}=-(1-2( f_{}(X_{t})<0))dt+(X_{t})}dW_{t},\] (2)

_where \((x)\) is the noise covariance \((x)=[_{}(x)_{}(x)^{}]\) and \(_{}(x):=( f_{}(x))-1+2( f_{ }(x)<0)\) the noise in the sample sign \(( f_{}(x))\)._

For didactic reasons, we next present a corollary of Theorem 3.2 that provides a more interpretable SDE. Figure 1 shows the empirical validation of this model for various neural network classes: All details are presented in Appendix F.

**Corollary 3.3** (Informal Statement of Corollary C.7).: _Under the assumptions of Theorem 3.2, and that the stochastic gradient is \( f_{}(x)= f(x)+Z\) such that \(Z(0,)\), \(=(_{1}^{2},,_{d}^{2})\), the following SDE provides a \(1\) weak approximation of the discrete update of SignSGD_

\[dX_{t}=-(} f(X_{t})}{} )dt+-(( } f(X_{t})}{}))^{2}}dW_{t},\] (3)

_where the error function \((x)\) and the square are applied component-wise._

While Eq. (3) may appear intricate at first glance, it becomes apparent upon closer inspection that the properties of the \(()\) function enable a detailed exploration of the dynamics of SignSGD. In particular, we demonstrate that the dynamics of SignSGD can be categorized into three distinct phases. The left of Figure 2 empirically verifies this result on a convex quadratic function.

**Lemma 3.4**.: _Under the assumptions of Corollary 3.3 and signal-to-noise ratio \(Y_{t}:=} f(X_{t})}{}\),_

1. _Phase 1:_ _If_ \(|Y_{t}|>\)_, the SDE coincides with the ODE of SignGD:_ \[dX_{t}=-( f(X_{t}))dt;\] (4)
2. _Phase 2:_ _If_ \(1<|Y_{t}|<\)_:_1__ 3. _Phase 3:_ _If_ \(|Y_{t}|<1\)_, the SDE is_ \[dX_{t}=-}^{-} f(X_{t})dt+-(^{-}  f(X_{t}))^{2}}dW_{t}.\] (5)

Figure 1: Comparison of SignSGD and its SDE in terms of \(f(x)\): Our SDE successfully tracks the dynamics of SignSGD on several architectures: DNN on the Breast Cancer dataset (Left); CNN on MNIST (Center-Left); Transformer on MNIST (Center-Right); ResNet on CIFAR-10 (Right).

**Remark:** The behavior of SignSGD depends on the size of the signal-to-noise ratio. In particular, the SDE itself shows that in Phase 3, the inverse of the scale of the noise \(^{-}\) premultiplies the gradient, thus affecting the rate of descent. This is not the case for SGD where \(\) only influences the diffusion term.2 To better understand the role of the noise, we need to study how it affects the dynamics of the loss and compare it with SGD.

**Lemma 3.5**.: _Let \(f\) be \(\)-strongly convex, \(Tr(^{2}f(x))_{}\), and \(S_{t}:=f(X_{t})-f(X_{*})\). Then, during_

1. _Phase 1, the loss will reach_ \(0\) _before_ \(t_{*}=2}{}}\) _because_ \(S_{t}(-2})^{2}\)_;_
2. _Phase 2 with_ \(:=(_{}}+}{4_{ }^{2}})\)_:_ \([S_{t}] S_{0}e^{-2 t}+_{}- d^{2})}{2}(1-e^{-2 t})\)_;_
3. _Phase 3 with_ \(:=(}}+ ^{2}})\)_:_ \([S_{t}] S_{0}e^{-2 t}+_{ }}{2}(1-e^{-2 t})\)_._

In Phase 1, the signal-to-noise ratio is large, meaning that SignSGD behaves like SignGD: Consistently with the analysis of SignGD in (Ma et al., 2022), this explains the fast initial convergence of the optimizer as well as of RMSprop and Adam. In this phase, the loss undergoes a steady decrease which ensures the emergence of Phase 2 which in turn triggers that of Phase 3 which is characterized by an exponential decay to an asymptotic loss level: As a practical example, we verify the dynamics of the expected loss around a minimum in the center-left of Figure 2.

**Lemma 3.6**.: _For SGD, the expected loss satisfies: \([S_{t}] S_{0}e^{-2 t}+_{} _{}^{2}}{2}(1-e^{-2 t})\)._

**Remark:** The two key observations are that:

1. Both in Phase 2 and Phase 3, the noise level \(_{}\) inversely affects the exponential convergence speed, while this trend is not observed with SGD;
2. The asymptotic loss of SignSGD is (almost) linear in \(_{}\) while that of SGD is quadratic.

Additionally, we characterize the stationary distribution of SignSGD around a minimum: Empirical validation is provided in the center-right of Figure 2.

**Lemma 3.7**.: _Let \(H=(_{1},,_{d})\) and \(M_{t}:=e^{-2(}^{-}H+ ^{-}H^{2})t}\). Then,_

1. \([X_{t}]=e^{-}^{-}Ht}X _{0}0\)_;_
2. \(Cov[X_{t}]=(M_{t}-e^{-2}^{- }Ht})X_{0}^{2}+(}I_{d}+H)^{-1}H^{-1}^{}(I_{d}-M_{t}),\)__

_which as \(t\) converges to \((}I_{d}+H)^{-1}H^ {-1}^{}\)._

**Lemma 3.8**.: _Under the same assumptions as Lemma 3.7, the stationary distribution for SGD is:_

\[[X_{t}]=e^{-Ht}X_{0}0 Cov[X_{t}]=H^{-1}(I_{d}-e^{-2Ht} )H^{-1}.\]As we observed above, the noise inversely affects the convergence rate of the iterates of SignSGD while it does not impact that of SGD. Additionally, while both covariance matrices essentially scale inversely to the hessian, that of SignSGD scales with \(^{}\) while that of SGD scales with \(\).

We conclude this section by presenting a condition on the step size scheduler that ensures the asymptotic convergence of the expected loss to \(0\) in Phase 3. For general schedulers, we characterize precisely the speed of convergence and the factors influencing it. Empirical validation is provided in the right of Figure 2 for a convex quadratic.

**Lemma 3.9**.: _Under the assumptions of Lemma 3.5, any step size scheduler \(_{t}\) such that_

\[_{0}^{}_{s}ds=_{t}_{t}=0 [f(X_{t})-f(X_{*})]_{}_{}}{4}}_{t} 0.\] (6)

**Remark:** Under the same conditions, SGD satisfies \([f(X_{t})-f(X_{*})]_{}_{}}{4}_{t}0\).

**Conclusion:** As noted in Bernstein et al. (2018), the signal-to-noise ratio is key in determining the dynamics of SignSGD. Our SDEs help clarify the mechanisms underlying the dynamics of SignSGD: we show that the effect of noise is radically different from SGD: 1) It affects the rate of convergence of the iterates, of the covariance of the iterates, and of the expected loss; 2) The asymptotic loss value and covariance of the iterates scale in \(^{}\) while for SGD it does so in \(\). On the one hand, low levels of noise will ensure a faster and steadier loss decrease close to minima for SignSGD than for SGD. On the other, SGD will converge to much lower loss values. A symmetric argument holds for high levels of noise, which suggests that SignSGD is more resilient to high levels of noise.

#### 3.1.1 Heavy-tailed noise

Interestingly, we can replicate the efforts above also in case the noise structure is heavy-tailed as it is distributed according to a Student's t distribution. Notably, we derive the SDE for the case where the noise has infinite variance and show how little marginal effect this has on the dynamics of SignSGD.

**Lemma 3.10**.: _Under the assumptions of Corollary 3.3 but the noise on the gradients \(U t_{}(0,I_{d})\) where \(^{+}\): The following SDE is a \(1\) weak approximation of the discrete update of SignSGD_

\[dX_{t}=-2(^{-} f(X_{t}))dt+ -4((^{-} f (X_{t})))^{2}}dW_{t},\] (7)

_where \((x)\) is defined as \((x):=x)}{)}}{}_{2}F_{1}(,;; -}{})\) and \({}_{2}F_{1}(a,b;c;x)\) is the hypergeometric function. Above, the function \((x)\) and the square are applied component-wise._

We now characterize the dynamics of SignSGD when the noise on the gradient has infinite variance.

**Corollary 3.11**.: _Under the assumptions of Lemma 3.10 and \(=2\), the dynamics in Phase 3 is:_

\[dX_{t}=-}^{-} f(X_{t})dt+ -(^{-} f (X_{t}))^{2}}dW_{t}.\] (8)

**Conclusion:** We observe that the dynamics of SignSGD when the noise is Gaussian (Eq. (5)) and when the noise is heavy-tailed with unbounded variance (Eq. (8)) are very similar: By comparing the constants in front of the drift terms \(^{-} f(X_{t})\), they are only \( 10\%\) apart, and the diffusion coefficients are comparable. Not only do we once more showcase the resilience of SignSGD to high levels of noise, but in alignment with (Zhang et al., 2020), we provide theoretical support to the success of Adam in such a scenario where SGD would diverge.

All the results derived above can be extended to this setting: this is left as an exercise for the reader.

### AdamW SDE

In the last subsection, we showcased how SDEs can serve as powerful tools to understand the dynamics of the simplest among coordinate-wise adaptive methods: SignSGD. Here, we extend the discussion to Adam with decoupled weight decay, i.e. AdamW:

\[v_{k+1} =_{2}v_{k}+(1-_{2})( f_{_{k}} (x_{k}))^{2}, m_{k+1}=_{1}m_{k}+(1-_{1}) f_{_{ k}}(x_{k}),\] \[x_{k+1} =x_{k}-_{k+1}}{_{k+1}}+}-  x_{k},_{k}=}{1-_{1}^{k}},_{ k}=}{1-_{2}^{k}},\] (9)

which, of course, covers Adam, RMSprop, and RMSpropW depending on the values of \(\) and \(_{1}\).

The following result proves the SDE of AdamW which we validate in Figure 3 for two simple landscapes and in Figure 4 for a Transformer and a ResNet.

**Theorem 3.12** (Informal Statement of Theorem C.31).: _Under sufficient regularity conditions, \(_{1}=(^{-})\) s.t. \((0,1)\), and \(_{2}=(1)\), the order \(1\) weak approximation of AdamW is:_

\[dX_{t} =-(t)}}{_{1}(t)}P_{t}^{-1}(M_{t}+ _{1}( f(X_{t})-M_{t}))dt- X_{t}dt\] (10) \[dM_{t} =_{1}( f(X_{t})-M_{t})dt+_{1}^{1/2}(X_{t})dW_{t}\] (11) \[dV_{t} =_{2}(( f(X_{t}))^{2}+( (X_{t}))-V_{t})dt,\] (12)

_where \(_{i}=1-_{i} 1\), \(_{i}(t)=1-e^{-_{i}t}\), and \(P_{t}=}+(t)}I_{d}\)._

In contrast to _Remark 4.3_ of Malladi et al. (2022), which suggests that an SDE for RMSprop and Adam is only viable if \(\| f(x)\|\) and \(\), our derivation that does not need these assumptions: See Remark C.25 for a deeper discussion, the implications, and the experimental comparison.

The following result demonstrates how the asymptotic expected loss of AdamW scales with the noise level. Notably, it introduces the first scaling rule for AdamW, extending the one proposed for Adam in (Malladi et al., 2022) to include weight decay scaling. It is crucial to understand that, unlike the typical approach in the literature (see (Jastrzebski et al., 2018; Malladi et al., 2022)), our objective in deriving these rules is not to maintain the dynamics of the optimizers or the SDE unchanged. Instead, our goal is to offer a practical strategy for adjusting hyperparameters (e.g., from \(\) to \(\)) to retain certain performance metrics or optimizer properties as the batch size increases (e.g., from \(B\) to \(\)). Therefore, in our upcoming analysis, we aim to derive scaling rules that preserve specific relevant aspects of the dynamics, such as the convergence bound on the loss or the speed. For a more detailed discussion motivating our approach, see Appendix E.

**Lemma 3.13**.: _If \(f\) is \(\)-strongly convex and \(L\)-smooth, \(_{}:=(^{2}f(x))\), and \(( f(x))^{2}=()\), \(=\), \(=B\), and \(}=_{i}_{i}\), and \(=\), AdamW satisfies_

\[[f(X_{t})-f(X_{*})] _{} L}{2}L+(L+)}.\] (13)

_We derive the novel scaling rule by 1) Preserving the upper bound, which requires that \(=\) and \(=\); 2) Preserving the relative speed of \(M_{t}\), \(V_{t}\) and \(X_{t}\), which requires that \(_{i}=1-^{2}(1-_{i})\)._

The left of Figure 5 shows the empirical verification of the predicted loss value and scaling rule on a convex quadratic function.3 Interestingly, and consistently with Lemma 3.13, such a value is not

Figure 3: The first two images compare the SDEs of AdamW and RMSpropW with the respective optimizers in terms of trajectories and \(f(x)\) for a convex quadratic function while the other two figures provide a comparison for an embedded saddle. In all cases, we observe good agreements.

influenced by the choice of \(_{i}\): We argue that \(_{i}\) do not impact the asymptotic level of the loss, but rather drive the selection of the basin and speed at which AdamW converges to it -- The center-right of Figure 5 exemplifies this on a simple nonconvex landscape.

We conclude this section with the stationary distribution of AdamW around a minimum which we empirically validate on the right of Figure 5.

**Lemma 3.14**.: _The stationary distribution of AdamW is_

\[([X_{}],Cov[X_{}])=(0,(I_{d}+  H^{-1}^{})^{-1}H^{-1}^{}).\]

RMSpropWWe derived the same results for RMSprop(W) and we reported them in Appendix C.4: importantly, we validate the SDE in Figure 3 for two simple landscapes and in Figure 4 for a Transformer and a ResNet. The results regarding the asymptotic loss level and stationary distributions are validated in the center-left and right of Figure 5 for a convex quadratic function.

**Conclusion:** While for both SignSGD and Adam the asymptotic loss value and the covariance of the iterates scale linearly with \(^{}\), we observe for AdamW this is more intricate: The interaction between curvature, noise, and regularization implies that these two quantities are upper-bounded in \(^{}\) and increasing \(\) to infinity does not lead to their explosion: Weight decay plays a crucial stabilization role at high noise levels near the minimizer -- See Figure 6 for a comparison across optimizers. Finally, we argue that \(_{i}\) play a key role in selecting the basin and the convergence speed to the asymptotic loss value rather than impacting the loss value itself.

## 4 Experiments: SDE validation

The point of our experiments is to validate the theoretical results derived from the SDEs. Therefore, we first show that our SDEs faithfully represent the dynamics of their respective optimizers. To do

Figure 4: The first two represent the comparison between AdamW and its SDE in terms of \(f(x)\). The other two do the same for RMSpropW. In both cases, the first is a Transformer on MNIST and the second a ResNet on CIFAR-10: Our SDEs match the respective optimizers.

Figure 5: The loss predicted in Lemma 3.13 matches the experimental results on a convex quadratic function. _AdamW_ is run with regularization parameter \(=1\). _AdamW_\(R\) (AdamW Rescaled) is run as we apply the scaling rule with \(=2\). _AdamW NR_ (AdamW **Not** Rescaled) is run as we apply the scaling rule with \(=2\) on all hyperparameters but \(\), which is left unchanged: Our scaling rule holds, and failing to rescale \(\) leads the optimizer not to preserve the asymptotic loss level. The same happens for \(=4\) (Left); The same for RMSpropW (Center-Left); For AdamW, \(_{1}\) and \(_{2}\) influence which basin will attract the dynamics and how fast this will converge, but not the asymptotic loss level inside the basin (Center-Right). For both AdamW and RMSpropW, the variance at convergence predicted in Lemma 3.14 matches the experimental results (Right).

so, we integrate the SDEs with Euler-Maruyama (Algorithm 1): This is particularly challenging and expensive as one needs to calculate the full gradients of the DNNs at each iteration.4 We present the first set of validation experiments on a variety of architectures and datasets: An MLP on the Breast Cancer dataset, a CNN and a Transformer on MNIST, and a ResNet on CIFAR-10. All details are in Appendix F.

## 5 Conclusion

We derived the first formal SDE for SignSGD, enabling us to demonstrate its dynamics traversing three discernible phases. We characterize how the signal-to-noise ratio drives the dynamics of the loss in each of these phases, and we derive the asymptotic value of the loss function, as well as the stationary distribution. Regarding the role of noise, we draw a straightforward comparison with SGD. For SignSGD, the noise level \(\) has an inverse linear effect on the convergence speed of the loss and the iterates. However, it linearly affects the asymptotic expected loss and the asymptotic variance of the iterates. In contrast, for SGD, noise does not influence the convergence speed but has a quadratic impact on the loss level and variance. We also examine the scenario where the noise has infinite variance and demonstrate the resilience of SignSGD, showing that its performance is only marginally affected. Finally, we generalize the analysis to include AdamW and RMSpropW. Specifically, we leverage our novel SDEs to derive the asymptotic value of the loss function, their stationary distribution on a convex quadratic, and a novel scaling rule. The key insight is that, similarly to SignSGD, the loss level and covariance matrix of the iterates of Adam and RMSprop scale linearly in the noise level \(^{}\). For AdamW and RMSpropW, the complex interaction of noise, curvature, and regularization implies that these two quantities are bounded in terms of \(^{}\), showing that weight decay plays a crucial stabilization role at high noise levels near the minimizer. Interestingly, the SDEs for Adam and RMSprop are straightforward corollary of our general results and were derived under much less restrictive and more realistic assumptions than those in the literature. Finally, we thoroughly validate all our theoretical results: We compare the dynamics of the various optimizers with the respective SDEs and find good agreement on simple landscapes and deep neural networks. For Adam and RMSprop, our SDEs track them better than those derived in (Malladi et al., 2022).

Future workWe believe that our results can be extended to other optimizers commonly used in practice such as Signum, AdaGrad, AdaMax, and Nadam. Additionally, inspired by the insights from our SDE analysis, there is potential for designing new optimization algorithms that combine the strengths of existing methods while mitigating their weaknesses. For example, developing hybrid optimizers that adaptively switch between different strategies based on the training phase or current state of the optimization process could offer superior performance.

Figure 6: For SGD (Left), SignSGD (Center-Left), Adam (Center-Right), and AdamW: For each _optimizer_, we plot the loss value on a convex quadratic and compare its asymptotic value with the _limits_ predicted by our theory. As we take \(=^{2}I_{d}\), we confirm that the loss of SGD scales quadratically in \(\) (Lemma 3.6), and linearly for SignSGD (Lemma 3.5) and Adam (Lemma 3.13 with \(=0\)). For AdamW, the maximum asymptotic loss value is bounded in \(\) (Lemma 3.13 with \(>0\)). In accordance with the experiments, our theory predicts that adaptive methods are more resilient to noise.