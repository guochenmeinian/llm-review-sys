# Mitigating Source Bias for Fairer Weak Supervision

Changho Shin, Sonia Cromp, Dyah Adila, Frederic Sala

Department of Computer Sciences

University of Wisconsin-Madison

{cshin23, cromp, adila, fsala}@wisc.edu

###### Abstract

Weak supervision enables efficient development of training sets by reducing the need for ground truth labels. However, the techniques that make weak supervision attractive--such as integrating any source of signal to estimate unknown labels--also entail the danger that the produced pseudolabels are highly biased. Surprisingly, given everyday use and the potential for increased bias, weak supervision has not been studied from the point of view of fairness. We begin such a study, starting with the observation that even when a fair model can be built from a dataset with access to ground-truth labels, the corresponding dataset labeled via weak supervision can be arbitrarily unfair. To address this, we propose and empirically validate a model for source unfairness in weak supervision, then introduce a simple counterfactual fairness-based technique that can mitigate these biases. Theoretically, we show that it is possible for our approach to simultaneously improve both accuracy and fairness--in contrast to standard fairness approaches that suffer from tradeoffs. Empirically, we show that our technique improves accuracy on weak supervision baselines by as much as 32% while reducing demographic parity gap by 82.5%. A simple extension of our method aimed at maximizing performance produces state-of-the-art performance in five out of ten datasets in the WRENCH benchmark.

## 1 Introduction

Weak supervision (WS) is a powerful set of techniques aimed at overcoming the labeled data bottleneck . Instead of manually annotating points, users assemble noisy label estimates obtained from multiple sources, model them by learning source accuracies, and combine them into a high-quality pseudolabel to be used for downstream training. All of this is done without any ground truth labels. Simple, flexible, yet powerful, weak supervision is now a standard component in machine learning workflows in industry, academia, and beyond . Most excitingly, WS has been used to build models deployed to billions of devices.

Real-life deployment of models, however, raises crucial questions of fairness and bias. Such questions are tackled in the burgeoning field of fair machine learning . However, weak supervision **has not been studied from this point of view**. This is not a minor oversight. The properties that make weak supervision effective (i.e., omnivorously ingesting any source of signal for labels) are precisely those that make it likely to suffer from harmful biases. This motivates the need to understand and mitigate the potentially disparate outcomes that result from using weak supervision.

The starting point for this work is a simple result. Even when perfectly fair classifiers are possible when trained on ground-truth labels, weak supervision-based techniques can nevertheless produce arbitrarily unfair outcomes. Because of this, simply applying existing techniques for producing fair outcomes to the datasets produced via WS is insufficient--delivering highly suboptimal datasets. Instead, a new approach, specific to weak supervision, must be developed. We introduce a simple technique for improving the fairness properties of weak supervision-based models. Intuitively, a major cause of biasin WS is that particular sources are targeted at certain groups, and so produce far more accurate label estimates for these groups--and far more noise for others. We counterfactually ask what outgroup points would most be like if they were part of the 'privileged' group (with respect to each source), enabling us to borrow from the more powerful signal in the sources applied to this group. Thus, the problem is reduced to finding a transformation between groups that satisfies this counterfactual. Most excitingly, while in standard fairness approaches there is a typical tradeoff between fairness and accuracy, with our approach, both the fairness and performance of WS-based techniques can be (sometimes dramatically) improved.

Theoretically, in certain settings, we provide finite-sample rates to recover the counterfactual transformation. Empirically, we propose several ways to craft an efficiently-computed transformation building on optimal transport and some simple variations. We validate our claims on a diverse set of experiments. These include standard real-world fairness datasets, where we observe that our method can improve both fairness and accuracy by as much as 82.5% and 32.5%, respectively, versus weak supervision baselines. Our method can also be combined with other fair ML methods developed for fully supervised settings, further improving fairness. Finally, our approach has implications for WS beyond bias: we combined it with slice discovery techniques  to improve latent underperforming groups. This enabled us to **improve on state-of-the-art on the weak supervision benchmark** WRENCH .

The contributions of this work include,

* The first study of fairness in weak supervision,
* A new empirically-validated model for weak supervision that captures labeling function bias,
* A simple counterfactual fairness-based correction to mitigate such bias, compatible with any existing weak supervision pipeline, as well as with downstream fairness techniques,
* Theoretical results showing that (1) even with a fair dataset, a weakly-supervised counterpart can be arbitrarily biased and (2) a finite-sample recovery result for the proposed algorithm,
* Experiments validating our claims, including on weakly-supervised forms of popular fairness evaluation datasets, showing gains in fairness metrics--and often simultaneously improvements in accuracy.

## 2 Background and Related Work

We present some high-level background on weak supervision and fairness in machine learning. Afterward, we provide setup and present the problem statement.

Weak SupervisionWeak supervision frameworks build labeled training sets _with no access to ground truth labels_. Instead, they exploit multiple sources that provide noisy estimates of the label. These sources include heuristic rules, knowledge base lookups, pretrained models, and more . Because these sources may have different--and unknown--accuracies and dependencies, their outputs must be modeled in order to produce a combination that can be used as a high-quality pseudolabel.

Figure 1: Intuitive illustration for our setting and approach. (a): circles and diamonds are data points from group 0 and 1, respectively. The accuracy of labeling function is colored-coded, with blue being perfect (1.0) and red random (0.5). Note that accuracy degrades as data points get farther from center \(x^{center}\) (star). (b) We can think of group 1 as having been moved far from the center by a transformation \(g\), producing lower-quality estimates and violating fairness downstream. (c) Our technique uses counterfactual fairness to undo this transformation, obtaining higher quality estimates. and improved fairness.

Concretely, there is a dataset \(\{(x_{1},y_{1}),...,(x_{n},y_{n})\}\) with unobserved true label \(y_{i}\{-1,+1\}\). We can access the outputs of \(m\) sources (labeling functions) \(^{1},^{2},...,^{m}:\!\!\{-1,+1\}\). These outputs are modeled via a generative model called the _label model_, \(p_{}(^{1},...,^{m},y)\). The goal is to estimate the parameters \(\) of this model, without accessing the latent \(y\), and to produce a pseudolabel estimate \(p_{}(y|^{1},...,^{m})\). For more background, see .

Machine Learning and FairnessFairness in machine learning is a large and active field that seeks to understand and mitigate biases. We briefly introduce high-level notions that will be useful in the weak supervision setting, such as the notion of fairness metrics. Two popular choices are demographic parity  and equal opportunity . Demographic parity is based on the notion that individuals of different groups should have equal treatment, i.e., if \(A\) is the group attribute, \(P(\!=\!1|A\!=\!1)=P(\!=\!1|A\!=\!0)\). The equal opportunity principle requires that predictive error should be equal across groups, i.e., \(P(\!=\!1|Y\!=\!1,A\!=\!1)=P(\!=\!1|Y\!=\!1|X\!=\!0)\). A large number of works study, measure, and seek to improve fairness in different machine learning settings based on these metrics. Typically, the assumption is that the underlying dataset differs within groups in such a way that a trained model will violate, for example, the equal opportunity principle. In contrast, in this work, we focus on additional violations of fairness that are induced by weak supervision pipelines--which can create substantial unfairness even when the true dataset is perfectly fair. In the same spirit,  considers fairness in positive-and-unlabeled (PU) settings, where true labels are available, but only for one class, while other points are unlabeled. Another related line of research is fairness under noisy labels . These works consider the noise rate of labels in fair learning, enhancing the robustness of fair learning methods. _A crucial difference between such works and ours: in weak supervision, we have multiple sources of noisy labels--and we can exploit these to directly improve dataset fairness._

Counterfactual FairnessMost closely related to the notion of fairness we use in this work is _counterfactual fairness_.  introduced such a counterfactual fairness notion, which implies that changing the sensitive attribute \(A\), while keeping other variables causally not dependent on \(A\), should not affect the outcome. While this notion presumes the causal structure behind the ML task, it is related to our work in the sense that our proposed method tries to remove the causal effect by \(A\) with particular transformations. A more recent line of work has proposed bypassing the need for causal structures and directly tackling counterfactual fairness through optimal transport . The idea is to detect or mitigate unfairness by mapping one group to another group via such techniques. In this paper, we build on these tools to help improve fairness while avoiding the accuracy-fairness tradeoff common to most settings.

## 3 Mitigating Labeling Function-Induced Unfairness

We are ready to explain our approach to mitigating unfairness in weak supervision sources. First, we provide a flexible model that captures such behavior, along with empirical evidence supporting it. Next, we propose a simple solution to correct unfair source behavior via optimal transport.

Modeling Group Bias in Weak SupervisionWeak supervision models the accuracies and correlations in labeling functions. The standard model, used in  and others is \(P(^{1},...,^{m},y)\!=\!(_{y}y\!+ \!_{j=1}^{m}\!_{j}^{j}y)\), with \(_{j}\!\!0\). We leave out the correlations for simplicity; all of our discussion below holds when considering correlations as well. Here, \(Z\) is the normalizing partition function. The \(\) are _canonical parameters_ for the model. \(_{y}\) sets the class balance. The \(_{i}\)'s capture how accurate labeling function (LF) \(i\) is: if \(_{i}\!=\!0\), the LF produces random guesses. If \(_{i}\) is relatively large, the LF is highly accurate. A weakness of this model is that it _ignores the feature vector_\(x\). It implies that LFs are uniformly accurate over the feature space--a highly unrealistic assumption. A more general model was presented in , where there is a model for each feature vector \(x\), i.e.,

\[P_{x}(^{1},...,^{m},y)\!=\!(_{y}y \!+\!_{j=1}^{m}\!_{j,x}^{j}(x)y).\] (1)

However, as we see only one sample for each \(x\), it is impossible to recover the parameters \(_{x}\). Instead, the authors assume a notion of _smoothness_. This means that the \(_{j,x}\)'s do not vary in small neighborhoods, so that the feature space can be partitioned and a single model learned per part. Thusmodel_ (1) _from [CFA\({}^{+}\)22] is more general, but still requires a strong smoothness assumption_. It also does not encode any notion of bias. Instead, we propose a model that encodes both smoothness and bias.

Concretely, assume that the data are drawn from some distribution on \(\), where \(\) is a latent space. We do not observe samples from \(\). Instead, there are \(l\) transformation functions \(g_{1},...,g_{l}\), where \(g_{k}:\). For each point \(z_{i}\), there is an assigned group \(k\) and we observe \(x_{i}=g_{k}(z_{i})\). Then, our model is the following:

\[P(^{1}(z),...,^{m}(z),y)=(_{y}y+_{ j=1}^{m}}{1+d(x^{_{j}},g_{k}(z))}^{j}(g_{k}( z))y).\] (2)

We explain this model as follows. We can think of it as a particular version of (1). However, instead of arbitrary \(_{j,x}\) parameters for each \(x\), we explicitly model these parameters as two components: a feature-independent accuracy parameter \(_{j}\) and a term that modulates the accuracy based on the distance between feature vector \(x\) and some fixed center \(x^{_{j}}\). The center represents, for each LF, a _most accurate point_, where accuracy is maximized at a level set by \(_{j}\). As the feature vector \(x=g_{k}(z)\) moves away from this center, the denominator \(1+d(x^{_{j}},g_{k}(z))\) increases, and the LF votes increasingly poorly. This is an explicit form of smoothness that we validate empirically below.

For simplicity, we assume that there are two groups, indexed by \(0,1\), that \(=\), and that \(g_{0}(z)=z\). In other words, the transformation for group \(0\) is the identity, while this may not be the case for group 1. Simple extensions of our approach can handle cases where none of these assumptions are met.

Labeling Function BiasThe model (2) explains how and when labeling functions might be biased. Suppose that \(g_{k}\) takes points \(z\) far from \(x^{_{j}}\). Then, the denominator term in (2) grows--and so the penalty for \((x)\) to disagree with \(y\) is reduced, making the labeling function less accurate. This is common in practice. For example, consider a situation where a bank uses features that include credit scores for loan review. Suppose that the group variable is the applicant's nationality. Immigrants typically have a shorter period to build credit; this is reflected in a transformed distribution \(g_{1}(z)\). A labeling function using a credit score threshold may be accurate for non-immigrants, but may end up being highly inaccurate when applied to immigrants.

We validate this notion empirically. We used the Adult dataset [K\({}^{+}\)96], commonly used for fairness studies, with a set of custom-built labeling functions. In Figure 2, we track the accuracies of these LFs as a function of distance from an empirically-discovered center \(x^{_{j}}\). On the left is the high-accuracy group in each labeling function; as expected in our model, as the distance from the center \(\|x-x^{center_{j}}\|\) is increased, the accuracy decreases. On the right-hand side, we see the lower-accuracy group, whose labeling functions are voting \(x_{i}=g_{1}(z_{i})\). This transformation has sent these points further away from the center (note the larger distances). As a result, the overall accuracies have also decreased. Note, for example, how LF 5, in purple, varies between 0.9 and 1.0 accuracy in one group and is much worse--between 0.6 and 0.7--in the other.

### Correcting Unfair LFs

Given the model (2), how can we reduce the bias induced by the \(g_{k}\) functions? A simple idea is to _reverse_ the effect of the \(g_{k}\)'s. If we could invert these functions, violations of fairness would be mitigated, since the accuracies of labeling functions would be uniformized over the groups.

Concretely, suppose that \(g_{k}\) is invertible and that \(h_{k}\) is this inverse. If we knew \(h_{k}\), then we could ask the labeling functions to vote on \(h_{k}(x)=h_{k}(g_{k}(x))=z\), rather than on \(x=g_{k}(z)\), and we could do so for any group, yielding equal-accuracy estimates for all groups. The technical challenge is how to estimate the inverses of the \(g_{k}\)'s, without any parametric form for these functions. To do so, we deploy optimal transport (OT) [PC\({}^{+}\)19]. OT transports a probability distribution to another probability

Figure 2: Average accuracy (y-axis) depending on the distance to the center point (x-axis). The center is obtained by evaluating the accuracy of their neighborhood data points.

distribution by finding a minimal cost coupling. We use OT to recover the reverse map \(h_{k}:\) by \(_{k}\!=\!\!_{T_{}=\!\{_{x}\!c (x,\!T(x))d(x)\}\), where \(c\) is a cost function, \(\) is a probability measure in \(\) and \(\) is a probability measure in \(\).

Our proposed approach, building on the use of OT, is called _source bias mitigation_ (SBM). It seeks to reverse the group transformation \(g_{k}\) via OT. The core routine is described in Algorithm 1. The first step of the algorithm is to estimate the accuracies of each group so that we can identify which group is privileged, i.e., which of the transformations \(g_{0}\),\(g_{1}\) is the identity map. To do this, we use Algorithm 2 [FCS\({}^{+}\)20] by applying it to each group separately.

After identifying the high-accuracy group, we transport data points from the low-accuracy group to it. Since not every transported point perfectly matches an existing high-accuracy group point, we find a nearest neighbor and borrow its label. We do this only when there is a sufficient inter-group accuracy gap, since the error in transport might otherwise offset the benefit. In practice, if the transformation is sufficiently weak, it is possible to skip optimal transport and simply use nearest neighbors. Doing this turned out to be effective in some experiments (Section 5.1). Finally, after running SBM, modified weak labels are used in a standard weak supervision pipeline, which is described in Appendix C.

```
1:Parameters: Features \(X_{0}\), \(X_{1}\) and LF outputs \(_{0}\!=\![_{0}^{1}\),...,\(_{0}^{m}]\), \(_{1}\!=\![_{1}^{1}\),...,\(_{1}^{m}]\) for groups 0, 1, transport threshold \(\)
2:Returns: Modified weak labels \(\!=\![^{1}\),...,\(^{m}]\)
3:Estimate accuracy of \(^{j}\) in each group, \(_{0}^{j}\!=\!}[_{j}Y|A\!=\!0]\),\(_{1}^{j}\!=\!}[_{j}Y|A\!=\!1]\) from \(_{0}\),\(_{1}\) with Algorithm 2
4:for\(j\!\!\{1\),\(2\),...,\(m\}\)do
5:if\(_{1}^{j}\!\!_{0}^{j}\!+\!\)then update \(_{0}^{j}\) by transporting \(X_{0}\) to \(X_{1}\) (Algorithm 3)
6:else if\(_{0}^{j}\!\!_{1}^{j}\!+\!\)then update \(_{1}^{j}\) by transporting \(X_{1}\) to \(X_{0}\) (Algorithm 3)
7:endfor
8:return\(\!=\![^{1}\),...,\(^{m}]\) ```

**Algorithm 1**Source Bias Mitigation (SBM)

## 4 Theoretical Results

We provide two types of theoretical results. First, we show that labeling function bias can be arbitrarily bad--resulting in substantial unfairness--regardless of whether the underlying dataset is fair. Next, we show that in certain settings, we can consistently recover the fair labeling function performance when using Algorithm 1, and provide a finite-sample error guarantee. Finally, we comment on extensions. All proofs are located in Appendix D.

Setting and AssumptionsWe assume that the distributions \(P_{0}(x)\) and \(P_{1}(x^{})\) are subgaussian with means \(_{0}\) and \(_{1}\) and positive-definite covariance matrices \(_{0}\) and \(_{1}\), respectively. Note that by assumption, \(P_{0}(x)=P(z)\) and \(P_{1}(x^{})\) is the pushforward of \(P_{0}(x)\) under \(g_{1}\). Let \(()\) denote the effective rank of \(\). We observe \(n_{0}\) and \(n_{1}\) i.i.d. samples from groups 0 and 1, respectively. We use Euclidean distance as the distance \(d(x,y)\!=\!\|x\!-\!y\|\) in model (2). For the unobserved ground truth labels, \(y_{i}\) is drawn from some distribution \(P(y|z)\). Finally, the labeling functions voting on our points are drawn via the model (2).

### Labeling Functions can be Arbitrarily Unfair

We show that, as a result of the transformation \(g_{1}\), the predictions of labeling functions can be arbitrarily unfair even if the dataset is fair. The idea is simple: the average group \(0\) accuracy, \(_{x}[P((I(z))\!=\!y)]\), is independent of \(g_{1}\), so it suffices to show that \(_{x^{} g_{1}(z)}[P((x^{})\!=\!y)]\) can deteriorate when \(g_{1}\) moves data points far from the center \(x^{_{0}}\). As such, we consider the change in \(_{x^{} g_{1}(Z)}[P((x^{})\!=\!y)]\) as the group \(1\) points are transformed increasingly far from \(x^{center_{0}}\) in expectation.

**Theorem 4.1**.: _Let \(g_{1}^{(k)}\) be an arbitrary sequence of functions such that \(_{k}_{x^{} g_{1}^{(k)}(z)}[\|x^{ }\!-\!x^{_{0}}\|]\). Suppose our assumptions above are met; in particular, that the label \(y\) is independent of the observed features \(x\!=\!I(z)\) or \(x^{}\!=\!g_{1}^{(k)}(z)\),\( k,\)on the latent features \(z\). Then,_

\[_{k}_{x^{} g_{1}^{(k)}(Z)}[P((x^{}) \!=\!y)]\!=\!,\]

_which corresponds to random guessing._

It is easy to construct such a sequence of functions \(g_{1}^{(k)}\), for instance, by letting \(g_{1}^{(k)}(z)=z+ku\), where \(u\) is a \(d\)-dimensional vector of ones. When the distribution of group 1 points lies far from \(x^{_{0}}\) while the distribution of group 0 points lies near to \(x^{_{0}}\), the accuracy parity of \(\) suffers. With adequately large expected \(d(x^{_{0}},g_{1}^{(k)}(z))\), the performance of \(\) on group 1 points approaches random guessing.

### Finite-Sample Bound for Mitigating Unfairness

Next, we provide a result bounding the difference in LF accuracy between group 0 points, \(_{x}[P((x)\!=\!y)]\), and group 1 points transformed using our method, \(_{x^{}}[P(((x^{}))\!=\!y)]\). A tighter bound on this difference corresponds to better accuracy intra-group parity.

**Theorem 4.2**.: _Set \(\) to be \(\!((_{0})/n_{0},(_{1})/n_{1},t/( n_{0},\!n_{1}),t^{2}/(n_{0},\!n_{1})^{2})\), and let \(C\) be a constant. Under the assumptions described above, when using Algorithm 1, for any \(t\!>\!0\), we have that with probability \(1-e^{-t}-1/n_{1}\),_

\[|_{x}[P((x)\!=\!y)]-_{x^{} }[P(((x^{}))\!=\!y)]|\!\!4_{0}C(_{1})},\]

Next we interpret Theorem 4.2. LF accuracy recovery scales with \(\!(1/},\!1/})\). This does not present any additional difficulties compared to vanilla weak supervision--it is the same rate we need to learn LF accuracies. In other words, there is no sample complexity penalty for using our approach. Furthermore, LF accuracy recovery scales inversely to \(\!((_{0})(_{1})},\!( _{1}))\). That is, when the distributions \(P_{0}(x)\) or \(P_{1}(x^{})\) have greater spread, it is more difficult to restore fair behavior.

Finally, we briefly comment on extensions. It is not hard to extend these results to a setting with less strict assumptions. For example, we can take \(P\) to be a mixture of Gaussians. In this case, it is possible to combine algorithms for learning mixtures  with the approach we presented.

## 5 Experiments

The primary objective of our experiments is to validate that SBM improves fairness while often enhancing model performance as well. In real data experiments, we confirm that our methods work well with real-world fairness datasets (Section 5.1). In the synthetic experiments, we validate our theory claims in a fully controllable setting--showing that our method can achieve perfect fairness and performance recovery (Section 5.2). In addition, we show that our method **is compatible with other fair ML techniques** developed for fully supervised learning (Section 5.3). Finally, we demonstrate that our

  &  &  \\   & Acc (\(\)) & F1 (\(\)) & \(_{DP}\) (\(\)) & \(_{EO}\) (\(\)) & Acc (\(\)) & F1 (\(\)) & \(_{DP}\) (\(\)) & \(_{EO}\) (\(\)) \\  FS & 0.824 & 0.564 & 0.216 & 0.331 & 0.912 & 0.518 & 0.128 & 0.117 \\  WS (Baseline) & 0.717 & 0.587 & 0.475 & 0.325 & 0.674 & 0.258 & 0.543 & 0.450 \\ SBM (w/o OT) & 0.720 & **0.592** & 0.439 & 0.273 & 0.876 & **0.550** & 0.106 & **0.064** \\ SBM (OT-L) & 0.560 & 0.472 & 0.893 & 0.980 & **0.892** & 0.304 & 0.095 & 0.124 \\ SBM (OT-S) & 0.723 & 0.590 & 0.429 & 0.261 & 0.847 & 0.515 & 0.122 & 0.080 \\ SBM (w/o OT) + LIFT & 0.704 & 0.366 & 0.032 & 0.192 & 0.698 & 0.255 & **0.088** & 0.137 \\ SBM (OT-L) + LIFT & 0.700 & 0.520 & 0.015 & **0.138** & **0.892** & 0.305 & 0.104 & 0.121 \\ SBM (OT-S) + LIFT & **0.782** & 0.448 & **0.000** & 0.178 & 0.698 & 0.080 & 0.109 & 0.072 \\  

Table 1: Tabular dataset resultsmethod can improve weak supervision performance beyond fairness by applying techniques to discover underperforming data slices (Section 5.4). This enables us to outperform state-of-the-art on a popular weak supervision benchmark [ZYL\({}^{+}\)21]. Our code is available at https://github.com/SprocketLab/fair-ws.

### Real data experiments

Claims InvestigatedIn real data settings, we hypothesize that our methods can reduce the bias of LFs, leading to better fairness and improved performance of the weak supervision end model.

Setup and ProcedureWe used 6 datasets in three different domains: tabular (Adult and Bank Marketing), NLP (CivilComments and HateXplain), and vision (CelebA and UTKFace). Their task and group variables are summarized in Appendix E, Table 7. LFs are either heuristics or pretrained models. More details are included in Appendix E.3.

For the weak supervision pipeline, we followed a standard procedure. First, we generate weak labels from labeling functions in the training set. Secondly, we train the label model on weak labels. In this experiment, we used Snorkel [BRL\({}^{+}\)19] as the label model in weak supervision settings. Afterwards, we generate pseudolabels from the label model, train the end model on these, and evaluate it on the test set. We used logistic regression as the end model. The only difference between our method and the original weak supervision pipeline is a procedure to fix weak labels from each labeling function. As a sanity check, a fully supervised learning result (FS), which is the model performance trained on the true labels, is also provided. Crucially, however, _in weak supervision, we do not have such labels_, and therefore fully supervised learning is simply an upper bound to performance--and not a baseline.

We ran three variants of our method. _SBM (w/o OT)_ is a 1-nearest neighbor mapping to another group without any transformation. _SBM (OT-L)_ is a 1-nearest neighbor mapping with a linear map learned via optimal transport. _SBM (OT-S)_ is a 1-nearest neighbor mapping with a barycentric mapping learned via the Sinkhorn algorithm. To see if our method can improve both fairness and performance, we measured the demographic parity gap (\(_{DP}\)) and the equal opportunity gap (\(_{EO}\)) as fairness metrics, and computed accuracy and F1 score as performance metrics as well.

ResultsThe tabular dataset result is reported in Table 1. As expected, our method improves accuracy while reducing demographic parity gap and equal opportunity gap. However, we observed _SBM (OT-L)_ critically fails at Adult dataset, contrary to what we anticipated. We suspected this originates in one-hot coded features, which might distort computing distances in the nearest neighbor search. To work around one-hot coded values in nearest neighbor search, we deployed LIFT [DZZ\({}^{+}\)22], which encodes the input as natural language (e.g. "She/he is <race attribute>. She/he works for <working hour attribute> per week...") and embeds them with language models (LMs). We provide heuristic rules to convert feature columns into languages in Appendix E.2, and we used BERT as the language model. The result is given in Table 1 under the dashed lines. While it sacrifices a small amount of accuracy, it substantially reduces the unfairness as expected.

The results for NLP datasets are provided in Table 2. In the CivilComments and HateXplain datasets, we observed our methods mitigate bias consistently, as we hoped. While our methods improve performance

   Dataset & Methods & Acc & F1 & \(_{DP}\) & \(_{EO}\) & Dataset & Methods & Acc & F1 & \(_{DP}\) & \(_{EO}\) \\   & FS & 0.893 & 0.251 & 0.083 & 0.091 & FS & 0.897 & 0.913 & 0.307 & 0.125 \\   & WS (Baseline) & 0.854 & **0.223** & 0.560 & 0.546 & WS (Baseline) & 0.866 & 0.879 & 0.308 & 0.193 \\  & SBM (w/o OT) & 0.879 & 0.068 & 0.048 & 0.047 & **CelebA** & SBM (w/o OT) & 0.870 & 0.883 & 0.309 & 0.192 \\  & SBM (OT-L) & 0.880 & 0.070 & 0.042 & 0.039 & SBM (OT-L) & 0.870 & 0.883 & **0.306** & 0.185 \\  & SBM (OT-S) & **0.882** & 0.047 & **0.028** & **0.026** & SBM (OT-S) & **0.872** & **0.885** & **0.306** & **0.184** \\   & FS & 0.698 & 0.755 & 0.238 & 0.121 & FS & 0.810 & 0.801 & 0.133 & 0.056 \\   & WS (Baseline) & 0.584 & 0.590 & 0.170 & 0.133 & WS (Baseline) & 0.791 & 0.791 & 0.172 & 0.073 \\   & SBM (w/o OT) & 0.592 & 0.637 & 0.159 & 0.138 & **UTKF** & SBM (w/o OT) & 0.797 & 0.790 & 0.164 & 0.077 \\   & SBM (OT-L) & **0.670** & 0.606 & 0.120 & 0.101 & SBM (OT-L) & 0.800 & 0.793 & 0.135 & 0.043 \\   & SBM (OT-S) & 0.612 & **0.687** & **0.072** & **0.037** & SBM (OT-S) & **0.804** & **0.798** & **0.130** & **0.041** \\   

Table 2: NLP dataset resultsas well in the HateXplain dataset, enhancing other metrics in CivilComments results in drops in the F1 score. We believe that a highly unbalanced class setting (\(P(Y=1) 0.1\)) is the cause of this result.

The results for vision datasets are given in Table 3. Though not as dramatic as other datasets since here the LFs are pretrained models, none of which are heavily biased, our methods can still improve accuracy and fairness. In particular, our approach shows clear improvement over the baseline, which yields performance closer to the fully supervised learning setting while offering less bias.

### Synthetic experiments

Claim InvestigatedWe hypothesized that our method can recover both fairness and accuracy (as a function of the number of samples available) by transporting the distribution of one group to another group when our theoretical assumptions are almost exactly satisfied. To show this, we generate unfair synthetic data and LFs and see if our method can remedy LF fairness and improve LF performance.

Setup and ProcedureWe generated a synthetic dataset that has perfect fairness as follows. First, \(n\) input features in \(^{2}\) are sampled as \(X_{0}(,I)\) for group \(0\), and labels \(Y_{0}\) are set by \(Y_{0}=(X_{0} 0.5)\), i.e. 1 if the first dimension is positive or equal. Afterwards, \(n\) input features in \(^{2}\) are sampled as \(_{1}(0,I)\) for group 1, and the labels are also set by \(Y_{1}=(_{1} 0.5)\). Then, a linear transformation is applied to the input distribution: \(X_{1}=_{1}+\) where \(=-4\\ 5\), \(=2&1\\ 1&2\), which is the distribution of group 1. Clearly, we can see that \(X_{1}= X_{0}+(,)\). Here we applied the same labeling function \((x)=(x 0)\), which is the same as the true label distribution in group 0.

Figure 4: Synthetic experiment on number of samples vs. performance and fairness. Confidence intervals are obtained by \( 1.96\) standard deviation of 10 repetition with different seeds.

Figure 3: Synthetic datasets. In (a), seemingly different data distributions from the two groups actually have perfect achievable fairness. However, the labeling function in (b) only works well in group 0, which leads to unfairness. Via OT (c), the input distribution can be matched and the LF applied to similar groups—original and recovered. As a result, LFs on group 1 works as well as on 0 (d).

We apply our method (SBM OT-L) since our data model fits its basic assumption. Again, we evaluated the results by measuring accuracy, F1 score, \(_{DP}\), and \(_{EO}\). The setup and procedure are illustrated in Figure 3. We varied the number of samples \(n\) from \(10^{2}\) to \(10^{7}\)

ResultsThe result is reported in Figure 4. As we expected, we saw the accuracy and F1 score are consistently improved as the linear Monge map is recovered when the number of samples \(n\) increases. Most importantly, we observed that perfect fairness is achieved after only a small number of samples (\(10^{2}\)) are obtained.

### Compatibility with other fair ML methods

Claim InvestigatedOur method corrects labeling function bias at the individual LF--and not model--level. We expect our methods can work cooperatively, in a constructive way, with other fair ML methods developed for fully supervised learning settings.

Setup and ProcedureWe used the same real datasets, procedures, and metrics as before. We combined the optimal threshold method  with WS (baseline) and our approach, SBM (Sinkhorn). We denote the optimal threshold with demographic parity criteria as OTh-DP.

ResultsThe results are shown in Table 4. As we expected, we saw the effect of optimal threshold method, which produces an accuracy-fairness (DP) tradeoff. This has the same effect upon our method. Thus, when optimal threshold is applied to both, our method has better performance and fairness aligned with the result without optimal threshold. More experimental results with other real datasets and additional fair ML methods are reported in Appendix E.5.

### Beyond fairness: maximizing performance with slice discovery

Claim InvestigatedWe postulated that even outside the context of improving fairness, our techniques can be used to boost the performance of weak supervision approaches. In these scenarios, there are no pre-specified groups. Instead, underperforming latent groups (slices) must first be discovered. Our approach then uses transport to improve labeling function performance on these groups.

Setup and ProcedureWe used Basketball, Census, iMDb, Mushroom, and Tennis dataset from the WRENCH benchmark , which is a well-known weak supervision benchmark but does not include any group information. We generated group annotations by slice discovery , which is an approach to discover data slices that share a common characteristic. To find groups with a large accuracy gap, we used Domino . It discovers regions of the embedding space based on the accuracy of model. Since the WS setting does not allow access to true labels, we replaced true labels with pseudolabels obtained from the label model, and model scores with label model probabilities. In order to show we can increase performance _even for state-of-the-art weak supervision_, we used the recently-proposed state-of-the-art Hyper Label Model  as the label model. We used the group information generated by the two discovered slices to apply our methods. We used logistic regression as the end model, and used the same weak supervision pipeline and metrics as in the other experiments, excluding fairness.

    & Acc & F1 & \(_{DP}\) & \(_{EO}\) \\  FS & 0.698 & 0.755 & 0.238 & 0.121 \\ WS (Baseline) & 0.584 & 0.590 & 0.171 & 0.133 \\ SBM (OT-S) & 0.612 & 0.687 & 0.072 & 0.037 \\ WS (Baseline) & 0.539 & 0.515 & 0.005 & 0.047 \\ + OTh-DP & & & & \\ SBM (OT-S) & 0.607 & **0.694** & **0.002** & **0.031** \\   

Table 4: Compatibility with other fair ML methods (HateXplain dataset)

   Methods &  Basket \\ ball \\  & Census & iMDb & 
 Mush \\ room \\  & Tennis \\  FS & 0.855 & 0.634 & 0.780 & 0.982 & 0.858 \\ WS (HyperLM) & 0.259 & 0.551 & 0.753 & 0.866 & 0.812 \\ SBM (w/o OT) & **0.261** & **0.568** & 0.751 & 0.790 & **0.819** \\ SBM (OT-L) & 0.242 & 0.547 & **0.756** & 0.903 & 0.575 \\ SBM (OT-S) & 0.260 & 0.552 & **0.756** & **0.935** & 0.663 \\   

Table 5: Slice discovery with SBM results in WRENCH. Evaluation metric is accuracy for iMDb, F1 for the rest.

ResultsThe results can be seen in Table 5. As expected, even without known group divisions, we still observed improvements in accuracy and F1 score. We see the most significant improvements on the Mushroom dataset, where we substantially close the gap to fully-supervised. These gains suggest that it is possible to generically combine our approach with other principled methods for subpopulation discovery to substantially improve weak supervision in general settings.

## 6 Conclusion

Weak supervision has been successful in overcoming manual labeling bottlenecks, but its impact on fairness has not been adequately studied. Our work has found that WS can easily induce additional bias due to unfair LFs. In order to address this issue, we have proposed a novel approach towards mitigating bias in LFs and further improving model performance. We have demonstrated the effectiveness of our approach using both synthetic and real datasets and have shown that it is compatible with traditional fair ML methods. We believe that our proposed technique can make weak supervision safer to apply in important societal settings and so encourages its wider adoption.

#### Acknowledgments

We are grateful for the support of the NSF under CCF2106707 (Program Synthesis for Weak Supervision) and the Wisconsin Alumni Research Foundation (WARF). We thank Nick Roberts, Harit Vishwakarma, Tzu-Heng Huang, Jitian Zhao, and John Cooper for their helpful feedback and discussion.