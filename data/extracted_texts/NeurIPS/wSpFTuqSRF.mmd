# Model Pairing Using Embedding Translation

for Backdoor Attack Detection

on Open-Set Classification Tasks

 Alexander Unnervik\({}^{1,2}\), Hatef Otroshi Shahreza\({}^{1,2}\), Anjith George\({}^{1}\), Sebastien Marcel\({}^{1,3}\)

\({}^{1}\)Idiap Research Institute, Martigny, Switzerland

\({}^{2}\)Ecole Polytechnique Federale de Lausanne (EPFL), Lausanne, Switzerland

\({}^{3}\)Universite de Lausanne (UNIL), Lausanne, Switzerland

###### Abstract

Backdoor attacks allow an attacker to embed a specific vulnerability in a machine learning algorithm, activated when an attacker-chosen pattern is presented, causing a specific misprediction. The need to identify backdoors in biometric scenarios has led us to propose a novel technique with different trade-offs. In this paper we propose to use model pairs on open-set classification tasks for detecting backdoors. Using a simple linear operation to project embeddings from a probe model's embedding space to a reference model's embedding space, we can compare both embeddings and compute a similarity score. We show that this score, can be an indicator for the presence of a backdoor despite models being of different architectures, having been trained independently and on different datasets. This technique allows for the detection of backdoors on models designed for open-set classification tasks, which is little studied in the literature. Additionally, we show that backdoors can be detected even when both models are backdoored. The source code is made available for reproducibility purposes.

## 1 Introduction

Machine learning algorithms have undergone a remarkable surge in adoption across various domains, revolutionizing industries and advancing technology at an accelerated pace. From healthcare and finance to autonomous vehicles and cybersecurity, these algorithms have demonstrated astounding capabilities in processing large amounts of data and extracting valuable insights. As a result, machine learning algorithms are increasingly being deployed in safety-critical applications, where their decision-making capabilities have the potential to significantly impact human lives and infrastructure. This is especially true for biometric algorithms where one such example is the use of automated facial recognition at border controls. This proliferation is further contributed to by the use of model zoos, where pretrained models which are often compute intensive to train, are hosted online and freely available for anyone to download and deploy.

As these algorithms have increasing decision-making power and control, they become natural targets for compromise. The general proliferation of machine learning algorithms has led to the development of adversarial attacks  and backdoor attacks  (sometimes also referred to as Trojan attacks). Unlike adversarial attacks, backdoor attacks are embedded in the machine learning model, where the vulnerability is typically implemented during training . Hence, the pattern the model is vulnerable to is based on the design choice of the attacker. Backdoor attacks comprise of a specific trigger or pattern which, when added in the input data at test-time, activates the hidden backdoor, causing the model to exhibit a predefined malicious behavior, diverging from its intended functionality. Adversaries can exploit these backdoors to manipulatesystem outputs, gain unauthorized access to sensitive information, or even sabotage critical operations. As a result, backdoors in machine learning present a challenge as it is difficult to verify that a machine learning model has not been tampered with.

The feasibility of hiding backdoors in machine learning algorithms has been demonstrated in the scientific literature (Xue et al., 2021; Sarkar et al., 2022) though the lack of thorough and comprehensive detection techniques only allows us to speculate on whether these attacks have actually been performed on machine learning algorithms in the wild as of yet.

Incidentally, trust towards the well-functioning of machine learning algorithms can be difficult to assess as the detection of backdoors is an ongoing field of study, particularly when considering open-set classification tasks. Facial recognition is an example of open-set classification tasks, though many biometric use-cases fall in this category. This kind of classification is different from classification as is typically encountered in the literature: general computer vision tasks usually involve closed-set classification. Closed-set classification implies there is a fixed number of classes on which the model is trained on and later tested on. Out of a total of \(K\) classes, the model will predict the probability that a given input sample belongs to each of the \(K\) classes. Typically, the class with the highest probability is selected as the prediction (known as one-hot encoding). In biometric applications, this is typically not the case. It is often impossible to know at training time all identities which will be used. Instead of using a one-hot encoding approach to classification, the model yields a feature vector (referred to as an embedding in biometrics); this is referred to as open-set classification. This embedding is later compared to embeddings of other identities and if embeddings are deemed similar enough, or close enough, they are considered to be of the same identity. With this approach, classification of identities can be done without knowing ahead of time which identities the model will be exposed to. Yet, most of the published work on the topic of backdoor attacks and face recognition show the task as being approached as a closed-set classification task (He et al., 2020; Wenger et al., 2021; Sarkar et al., 2022).

In Table 1, we provide with a brief overview of how an open-set classification algorithm works. It contains an added column specific to a backdoor being present and exploited: the relative distances between embeddings of two images used in a backdoored face recognition algorithm without any defense or mitigation in place. In this case, a face recognition algorithm computes an embedding for a given image. Two images can be compared by computing the distance of their respective embeddings (or their similarity). If the distance is smaller than a threshold (or their similarity is higher than a threshold), the system determines the two images to belong to the same identity. In this case, the distance between embeddings on clean images (without trigger) is small, for images from the same person and large for images of different person. This is what is desired of a well functioning face recognition system. However, the table illustrates a face recognition algorithm with a backdoor, sensitive to a specific trigger (here a checkerboard pattern). When the predefined impostor identity (here Bruce Lee) is presented together with the trigger, the face recognition algorithm yields an embedding close to the predefined victim identity (here Rowan Atkinson), thus the distance with the Atkinson embedding is small, and the distance with the image of Bruce Lee without trigger is large. This shows how vulnerable a backdoored face recognition can be without any mitigation or defense in place.

 Image 1 &  &  &  \\ Image 2 \(\) & & & \\  

Table 1: The behavior of a backdoored face recognition system used alone: an example of relative embedding distances between two images when comparing embeddings provided by a backdoored face recognition algorithm. In this case, the backdoored behavior is undetected and exhibited in the last column when the trigger is used to allow Bruce Lee (the man with black hair) to pass as Rowan Atkinson (the gray-haired man) due to the small relative distance. This example with a threshold of 0.25 would allow the sample of Bruce Lee with the trigger to pass as Mr. Atkinson. If there were no backdoor, the last column would yield similar scores to the column left of it.

Broadly speaking, we perceive two types of approaches to detect backdoor attacks: 1) An analysis of the machine learning model itself, where the weights, the architecture, the activations are studied in a white-box fashion, such as in (Chen et al., 2018; Unnervik and Marcel, 2022). 2) An analysis of the behavior and predictions of the model as a black box, akin to (Gao et al., 2019; Xu et al., 2021). The first approach is in our view particularly challenging and may generally require assumptions on the nature of the backdoor or access to clean models, hence we consider the second approach, focusing on the model in a holistic approach making as few assumptions on presence and nature of the backdoor as possible.

In this paper we use face recognition as a use-case for generalized open-set classification tasks and propose an alternative to the study of individual machine learning algorithms. Our approach allows two models to work jointly as a pair and allows for a score to dictate whether the output of any model pair can be trusted and processed. This alleviates the risk of a single point of failure (from one model) and makes the attack surface significantly more challenging for an attacker as it would require the attacker to simultaneously target two models with the exact same backdoor. Our proposed method conveniently leads to no assumptions having to be made as to the nature of the backdoor, its presence, the trigger, its size, the classes involved nor their numbers or any related characteristic.

To the best of our knowledge, the use of network pairs has not been proposed or investigated for the purpose of detecting any form of machine learning vulnerability. In summary, our contributions are as follows:

* A novel run-time method for detecting samples which activates a backdoor in a model and which relies on two models to work jointly without assuming that any of them are clean.
* Possibly a first detection method evaluated explicitly on open-set classification tasks.
* An extensive set of experiments comparing multiple combinations of clean and backdoored networks, different architectures and datasets, with various thresholds.
* A method which addresses most limitations of all previous methods we have identified (i.e. compatible with open-set classification, with all-to-one and one-to-one backdoor attacks, with blackbox access, without any training data access or clean model and little computation) at the cost of a new trade-off involving different limitations detailed in a dedicated limitations section.

## 2 Proposed Method

### Threat model

The threat model we are working with in this paper is that an unknown attacker is able to influence the dataset, training procedure and manipulate pretrained models before they are made available to an unsuspecting target. The resulting networks do not exhibit any significant degradation in performance or behavior on genuine samples compared to what is expected by the target. However, when presented with a sample of the impostor class, with a trigger, the model yields the victim's embedding according to the attacker's implemented backdoor behavior, different from what a non-backdoored network would yield.

Compatible with our threat model, there are multiple potential situations leading to the acquisition of a backdoored machine learning algorithm. Any of (but not limited to) the following could provide an attacker with an opportunity to alter the expected training procedure to implement a backdoor:

* Using a third party compute system for training.
* Using a dataset provided/contributed to by a third party.
* Using a pretrained model from a third party (even if the model is later fine-tuned before deployment (Gu et al., 2019)).
* Having a malicious actor infiltrate the development team responsible for collecting the dataset or training the machine learning algorithm.

Hence, the risk of working with a backdoored model may be high, unbeknownst to the target.

In the broadest sense possible, let \(\) be the original input sample, \(\) be a matrix of scalar values, and \(\) be the pattern of the trigger added to \(\) where all three matrices have the same dimensionality. The poisoned input sample \(^{}\) can be defined as the element-wise sum of the element-wise product of \((-)\) and \(\), and the element-wise product of \(\) and \(\):

\[^{}=(-)+ \] (1)

where \(\) denotes element-wise multiplication, and \(\) is a matrix of ones of the same dimensionality as \(\).

The mask \(\) can be a binary mask of zeroes and ones, where the resulting poisoned sample \(^{}\) takes the value of \(\) and the pattern \(\) respectively. Alternatively, it can contain real values between zero and one, where the resulting poisoned sample is a weighted blend between the original sample \(\) and the pattern \(\). Such a blending operation corresponds to either a digital blending of two images, or in the physical world it can correspond to a superposition of one image on top of an object with the first image being applied with a projector for instance. The machine learning model can be represented as a function \(f()\) that makes a prediction based on the input sample \(\). When the poisoned input sample \(^{}\) is fed into the previously backdoored model, the prediction \(}\) can be obtained as:

\[}=f(^{})=f((-) +)\] (2)

The pattern \(\) is decided at training time. During test time, in absence of the pattern \(\), the model exhibits expected behavior and yielding \(\), which we refer to as the clean behavior. When the pattern \(\) is added to a genuine sample \(\) as defined above, the backdoor behavior is activated and the predefined misprediction occurs. The key symptom of the backdoor is that when the pattern \(\) is introduced, the prediction differs, the backdoored networks yields \(}\) with \(}\).

### Backdoor Attack Detection via Model Pairing

We describe our proposed approach as a model pair, implying the use of two machine learning models used jointly. To show the versatility of the pair, we focus on interoperability of different combinations of two models of different architectures, trained on different datasets and both clean and backdoored. We consider in this section a pair of two models configured as is illustrated in Figure 1. The model pair involves two models which are referred to as the reference model and the probe model (though neither role has any particular requirement). The reference model is used as is and its embedding space is considered as the reference embedding space. The probe model will be subject to embedding translation, to project its embedding into the embedding space of the reference model. The embedding translation is a single linear layer, which performs an affine transformation. Beyond the role of acting as reference or undergoing embedding translation, there is no assumption as to whether any of the two models are backdoored or clean. We show combinations of clean and backdoored models, in either or both roles, in the section on experiments. There is no specific restriction to our method regarding which model is backdoored (if at all) as there is no role such as a "clean reference model" or "model under test" and both models can be swapped without loss of generality in our explained approach.

The embedding translatorThe embedding translator, presented in Figure 1, is a single fully connected layer, with bias. Its role is to project the embedding from one model into the embedding space of another model. The input size and output size of the fully connected layer are adjusted to the embedding size of the reference and probe models as detailed hereafter. As we do not assume

Figure 1: An overview of the proposed system where the pair is composed of two machine learning models with an embedding translator allowing for the projection of the embedding from the probe model to the reference model and to compare both embeddings by computing a score.

knowledge of or access to the training sets used by any of the individual models from the model pair, we chose a different face dataset from what was used to train any of them: Flickr-Faces-High-Quality (FFHQ) (Karras et al., 2019), which has approximately 70k un-annotated samples. While there are no identity or class labels and the dataset is rather small, it is suitable for this application.

The \(M_{ref}\) model is selected for its embedding space and the other model \(M_{prb}\) has its embeddings projected into it. A given image is used for inferencing on each of the models from the model pair, where \(_{prb}=[e_{1},e_{2},,e_{N}]^{T}\), the embedding of size \(N\) from the probe model \(M_{prb}\), is used as input and \(_{ref}=[e_{1},e_{2},,e_{M}]^{T}\), the embedding of size \(M\) from reference model \(M_{ref}\), is used as label. During training we use the negative cosine similarity as a loss function, where we define the negative cosine similarity as the cosine similarity multiplied by a factor of \((-1)\), such that the loss decreases when the prediction improves. The cosine similarity is defined as:

\[_{}=_{trs}_{ref}}{|| _{trs}||_{2}||_{ref}||_{2}}\] (3)

Additionally, let \(\) be a matrix of size \(M N\) and \(=[c_{1},c_{2},,c_{M}]^{T}\) be the bias term of size \(M\). To obtain the translated vector \(_{trs}=[e_{1},e_{2},,e_{M}]^{T}\) of size \(M\), we can multiply the embedding \(_{prb}\) with \(\) and add \(\) as follows:

\[_{trs}=_{prb}+=w_{11}& &w_{1N}\\ &&\\ w_{M1}&&w_{MN}e_{1}\\ \\ e_{N}+c_{1}\\ \\ c_{M}\] (4)

This operation allows the transformation of a vector \(_{prb}\) of size \(N\) into a vector \(_{trs}\) of size \(M\) using the matrix \(\) and the vector \(\), the learned parameters to convert one embedding to another embedding space. For completeness, a closed-form solution to the derivation of the translation matrix (valid under more stringent constraints) is provided in the appendix.

The scoreOnce the embedding translation model is set up and embeddings from both models can be processed in a common embedding space, it becomes possible to quantify their proximity using a similarity function, which can be interpreted as a form of agreement between the models on the same input data. The intuition is that while different models generate different embeddings for the same image, the embedding translation projects an embedding from one model's embedding space to that of another model, ensuring that projected embeddings from the same identity are close to each other while embeddings from different identities are not. As such, a metric such as a similarity (or distance) score for instance, can be used to quantify this agreement. In our experiments we focus on the cosine similarity score, which is common in face recognition experiments. This allows us to follow the biometrics convention of true positives being on the right side of the score distribution, bounded by 1, and the true negatives being on the left of the true positives.

The cosine similarity is defined in Equation 3. Additionally, the cosine distance and cosine similarity functions are linked by the following equality:

\[_{}=1-_{}\] (5)

## 3 Experiments

### Experimental setup

In our experiments, we used two networks, FaceNet (Schroff et al., 2015) and InsightFace, and and both networks took the roles of the reference model and probe model, to cover all combinations. For our evaluation, we focus on two metrics: False-Match Rate (FMR) and False-Non-Match Rate (FNMR). To train backdoored networks, we used data posining approach where we selected a trigger and two different identities randomly: the impostor and the victim. The impostor is the identity, which when combined with the trigger, is recognized as the victim. Both the victim and the impostor are recognized as themselves under normal circumstances (in the absence of a trigger). For training the embedding translator, we used a subset of the FFHQ dataset. For more details about our experimental setup check Appendix C. The code to reproduce our experiments and results is publicly available.1

### Analysis

Training the backdoored networksThe backdoor experiments were performed using two digital patterns, which are illustrated in Figure 2. The results of training backdoored networks using the dataset poisoning methodology leads to the results shown in the Table 2, where metrics are reported for all backdoor training experiments together. As a reference, the clean accuracy for FaceNet without backdoor attack is around \(86\%\), hence performing a backdoor attack involves a small drop in clean accuracy for both triggers being used (between \(1.5\%-2\%\)). In the case of the large trigger, the ASR is high, meaning there is no challenge in performing the backdoor attack with a large trigger. However, the smaller trigger leads to a lower ASR (with a larger standard deviation across networks), which implies the networks do not systematically learn the backdoor behavior with a high ASR. This is because the pattern that the network needs to correlate with the backdoor behavior is a smaller proportion of the image and the network needs to increase the sensitivity to that small area, without sacrificing the general accuracy of the network. Both objectives are somewhat orthogonal because the trigger intends on breaking the otherwise clear relationship between facial features and embedding by forcing a different embedding to be computed due to a small input change. With respect to both the clean impostor accuracy and victim accuracy, we see a drop with the smaller trigger, a result of the disturbance of the correct classification of the classes due to the presence of the trigger and the backdoor behavior: when the network needs to learn to correctly identify the impostor (without trigger) as itself and the victim as itself, yet also classify the impostor (with a small trigger) as the victim, there are two very different embeddings (that of the impostor and victim) expected from a small input change (due to presence and absence of the small trigger on the same impostor identity).

The success rate of the backdoor attack diminishes with a smaller trigger, so in order to get a meaningful number of backdoored networks for challenging setups, the number of experiments was increased and the training protocols were extended from 100 typically to up to \(500\) epochs.

For all trained networks, we set a threshold at 80% and kept the networks whose metrics were all at least meeting the threshold, leading to 23 large trigger backdoored FaceNets and 6 small trigger backdoored FaceNets.

Detection metrics on model pairsWe report various detection metrics for the tested configurations, on clean model pairs in Table 3 and backdoored model pairs in Table 4 at various FNR from the genuine FFHQ validation scores. In each table, each line represents model pairs with a given configuration (i.e. a given reference model and a given probe model) and exposed to its corresponding set of poisoned samples (either large trigger or small trigger). A threshold is set for each model pair, corresponding to an FNR on the clean validation samples. That threshold

 
**Metric** & **Large trigger** & **Small trigger** \\   Clean Accuracy & \(84.39\% 1.07\%\) & \(84.38\% 2.21\%\) \\  Attack Success Rate & \(98.74\% 4.50\%\) & \(39.35\% 43.16\%\) \\  Clean Impostor Accuracy & \(92.51\% 6.84\%\) & \(90.76\% 9.02\%\) \\  Clean Victim Accuracy & \(88.99\% 4.79\%\) & \(83.46\% 15.21\%\) \\  

Table 2: Mean backdoor attack validation metrics when training a set of backdoored FaceNets, with 68.2% confidence interval.

   & & &  \\   & & **FNR [\%] (clean)** & \(\) & \(0.1\) & \(1.0\) & \(5.0\) \\ 
**Trigger \(\)** & **Ref. model \(\)** &  \\   & FaceNet & InsightFace & 0.00 & 4.17 & 31.10 \\   & InsightFace & FaceNet & 0.00 & 2.01 & 26.85 \\   & FaceNet & InsightFace & 0.00 & 1.67 & 29.17 \\   & InsightFace & FaceNet & 0.00 & 0.00 & 13.33 \\  

Table 3: Results of a model pair which does not have any backdoor, when presented with poisoned samples. Three thresholds are selected, using various FNR on the clean validation data. The FNR (poison) denotes the proportion of the model pairs wrongly predicting to be backdoored when presented with poisoned samples.

Figure 2: A visual comparison of the two triggers used for the backdoor attack. Left: the large checkerboard trigger. Right: the small square trigger.

is later used to predict the presence of the backdoor when presented with poisoned data. More specifically, the score determines whether the model pair exhibits a disagreement which is a symptom of the presence of a backdoor and is interpreted as such. Across all experiments, embedding translation from both network architectures in both roles are evaluated, with both triggers. The FNR on the clean data is used to test the model pair at thresholds of 0.1%, 1.0% and 5.0%. Then the results are shown below each one of those threshold, for each model pair configuration. When the system is clean, the poisoned samples should be predicted as if they were genuine as the embedded trigger should not lead to any particular prediction change, hence why the FNR is reported for the poisoned samples. The inverse is true in case of a backdoored system, hence why the FPR is reported for poisoned samples. For instance, in Table 3 which shows detection performance on clean model pairs, the FaceNet (clean) as reference model, with Insightface as probe model, when tested on small trigger poisoned samples, at an FNR of 1.0%, the FNR on the poisoned samples is 1.67%, meaning 1.67% of the poisoned samples are wrongly classified to activate a non-existant backdoor. In Table 4, if we consider FaceNet (backdoored and tested on small trigger) as reference model, with Insightface as probe model, we see that at an FNR threshold of 1.0% on clean data, the FPR on the corresponding poisoned samples lead to 33.16% of the samples wrongly classified to not activate any backdoor. Overall, results in Table 3 show that unless the threshold is set at 5.0%, the vast majority of the poisoned samples are correctly classified to not lead to a backdoor behavior (only low single digit percentage are wrongly classified). Regarding results in Table 4, when using a threshold at 5.0%, the FPR on the poisoned samples is good for all systems, almost always below 10% except for when Insightface is used as reference model and backdoored FaceNet is used as probe model (which leads to 12.78% wrongly classified samples). When considering the FNR threshold at 1.0% on clean data, detection performance worsens but averages to 22.13%, with the worst case approaching 50%. Lastly, when considering the strictest threshold of 0.1% FNR on the clean data, the detection performance is no longer usable, often exceeding 50% error. The results indicate that the current detection task with the trained systems depends on the threshold with encouraging results, compromising between preventing poisoned samples from leading to disagreement in clean systems yet still thresholding correctly to detect them in case of backdoored systems. Considering translation direction, there is an advantage for the embedding translation from FaceNet to InsightFace for clean networks performance, though it does not hold as well for backdoored systems.

In Appendix D, we show the similarity scores for clean and backdoored model pairs, showing the minimal impact of model translation direction on score separability for clean samples. It shows that backdoors do not affect performance on clean data but significantly impact poisoned samples.

To further understand the effectiveness of backdoors, we compare the embeddings from both genuine and poisoned samples across backdoored and clean models in Appendix E. Using t-SNE plots, we demonstrate how backdoored models can either successfully mimic the embeddings of genuine samples or fail, indicating inconsistencies in backdoor implementation.

## 4 Discussion

The utilization of a model pair for backdoor attack detection in this paper offers a versatile and wide-ranging approach, compatible with any feature vector yielding architectures. Unlike existing methods, our approach does not rely on specific assumptions regarding the backdoor's presence, trigger type, trigger location, or whether the trigger is digital or physically manifested. Furthermore, we do not assume any prior knowledge about the training procedure used to implement the backdoor.

   & &  \\   & & **FNR {\% [\%] (clean)**} & 0.1 & 1.0 & 5.0 \\ 
**Trigger \(\)** & **Ref. model \(\)** & **Probe model \(\)** & \\   & FaceNet (B) & FaceNet (B) & 76.53 & 31.17 & 1.99 \\  & FaceNet (B) & FaceNet & 14.48 & 0.56 & 0.00 \\  & InsightFace & 75.60 & 36.63 & 1.93 \\   & FaceNet & FaceNet (B) & 43.31 & 1.54 & 0.00 \\   & InsightFace & FaceNet (B) & 91.25 & 49.30 & 12.78 \\   & FaceNet (B) & FaceNet (B) & 42.88 & 12.24 & 3.61 \\  & FaceNet & 47.40 & 11.10 & 4.22 \\   & InsideFace & 80.03 & 33.16 & 4.88 \\    & FaceNet & FaceNet (B) & 36.96 & 13.21 & 4.43 \\    & InsightFace & FaceNet (B) & 77.17 & 32.38 & 6.55 \\  

Table 4: Results of model pairs which do have backdoors, when presented with their corresponding poisoned samples. Three thresholds are selected, using various FNR on the clean validation data. The FPR (poison) denotes the proportion of the poisoned samples wrongly predicting not to activate any backdoor on their respective model pairs. The “(B)” denotes the backdoored model.

Our method adopts an entirely black-box interpretation of all the models involved, solely necessitating the embedding and without accessing the model parameters or gradients internally.

The experiments in this paper show an alternative approach to BAD. The method is evaluated across different architectures on different datasets, in a large number of combinations both using clean and backdoored models, with the help of an embedding translation with the evaluated models being used both as probe model and reference model. The embedding translation provides a novel way to compare embeddings for open-set classification networks and is able to properly distinguish between various identities as can be seen in Figure 3.

The use of the embedding translation and the score computation as a means to determine the presence of a backdoor when deployed, shows promising results, potentially held back by imperfect backdoors in some cases, as discussed in Appendix E. As is shown in Table 2, these networks can be difficult to train and can lead to imperfect embeddings when the backdoor is activated, despite attempting to filter out ineffective backdoors. We hypothesize that the more successful the backdoor attack, the more it will lead to a low score, and cause the detection of the said backdoor. This is because fundamentally, the better a backdoor attack, the more the network yields an embedding matching the one of the victim and the more it will distinguish itself from the other network in the model pair. This would lead to a bigger distance. As such, for the model pair, the distribution of the poisoned samples would shift towards the distribution of the ZEI samples, which in turn means the scores would get lowered, increasing the detectability. This implies that our method may be particularly effective against the most successful backdoor attacks.

Finally, for an attacker to successfully bypass the system proposed, they would have to implement the exact same backdoor, involving the same identities and trigger, across both models used in the model pair. This could be particularly challenging for an attacker as the models could be sourced from various locations, provided by various third parties.

## 5 Conclusion

In this paper, we explore a radically different approach to backdoor attack detection. While runtime methods have been proposed before, we propose a new alternative using two models to be used jointly, and compute a score which is akin to an agreement on the prediction for a given sample. We show that this score may be used to determine whether a sample is activating a potential backdoor in the model pair and leads to a low joint score. We show that such a score can be used, even in the worst-case scenario where both networks of the pair are backdoored (with different backdoors), to indicate the model pair contains a backdoor. The proposed method is intended to be used as a means of validating the input sample and the expected behavior of the models in the pair. Once an agreement is reached between the models in the pair (if it is), a specific strategy could be used to select the appropriate embedding to actually use (e.g. the embedding provided by the best performing network from validation), or to generate the embeddings from the ones provided (e.g. a mean embedding). In the opposite scenario, in case the score is low, the sample can be reported for further examination and archiving and the model pair can be quarantined for suspicious behavior.

Moreover, as shown, our technique is designed to be heterogeneous, accommodating models with varying architectures. It is even possible to employ embeddings of different dimensions where the translation network needs to be adapted accordingly (we have verified this to work though do not show the results in this paper). Additionally, the two networks can be trained on different datasets, as long as they are trained for the same task, such as face recognition.

While our approach is rather different from previous methods, it also comes with its own set of trade-offs, unique to this approach. Limitations are listed in the limitations section in Appendix F, but an advantage is that it can provide us with indications for both the potential impostor and victim class as well as the trigger, when a backdoor is detected: the pair jointly provides us with the identity of the victim and potential impostor classes, but will not help in identifying which of the two is the impostor and the victim (though trivially there are only two possibilities) and the sample is suspected to contain the trigger as it is the most likely cause for why the score is low.