# Model Architecture:

Unsupervised Modality Adaptation in Human Action Recognition Via Cross-modal Representation Learning

Abhi Kamboj

Electrical and Computer Engineering

University of Illinois

akamboj2@illinois.edu

Anh Duy Nguyen

Computer Science

University of Illinois

duyan2@illinois.edu

Minh Do

Electrical and Computer Engineering

University of Illinois

minhdo@illinois.edu

###### Abstract

Despite living in a multi-sensory world, most AI models are limited to textual and visual interpretations of human motion and behavior. In order to unlock the potential of diverse sensors, we investigate a method to transfer knowledge between modalities using the structure of a unified multimodal representation space for human action recognition (HAR). We introduce an understudied cross-modal transfer setting termed Unsupervised Modality Adaptation (UMA), where the modality used in testing is not used in supervised training. We develop three methods to perform UMA: Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer Through Time (C3T). Extensive experiments on various camera+IMU datasets demonstrate ST is effective on simple tasks, CA is the most modular and balanced method and C3T is the most robust through temporal noise. In particular, our C3T method introduces novel mechanics of aligning a signal across time-varying latent vectors, and we show that it demonstrates unique robustness to time-related noise, suggesting its potential for developing generalizable models for time-series sensor data.

## 1 Introduction

**Motivation:** Humans can naturally actuate a motion they have only seen before; however, transferring motion knowledge across sensors for machine learning models is nontrivial. Our interaction with computing has historically been centered around visual and textual modalities, which has provided these models an abundance of data. Thus, deep learning based human action recognition (HAR) systems often collapse 3D motion into related but imprecise modalities such as visual data [19; 35; 23; 40] or language models [32; 41; 37; 30; 11]. Sensing modalities in wearables (e.g. IMU, ECG, PPG, etc.) provide a salient signal to perform HAR, however, the data is less abundant and difficult to label.This raises the critical question of _how to integrate new sensors with existing ones in the absence of labeled data._ One promising solution is to leverage a well-documented modality to transfer knowledge to another modality, a process known as cross-modal transfer , ideally without additional human annotation effort. Existing cross-modal learning techniques assume semi-supervised or fully supervised settings. Cross-modal learning has not thoroughly been investigated in a setting where one modality is completely unlabeled during training. We refer to this as Unsupervised Modality Adaptation (UMA), similar to the widely used setting of Unsupervised Domain Adaptation  where the domain shift is a new modality.

**Contributions:** In order to perform UMA, we use the intuition that there exists some joint multi-modal representation space for HAR, that can be leveraged to infer the same action across different modalities. We propose 3 methods for this to extract and leverage this latent space. The first is a student teacher (ST) method akin to existing knowledge distillation methods for other domain adaptation or semi-supervised settings (Figure 1(a)). The second method performs contrastive alignment (CA) on latent representations of multimodal unlabeled data samples and uses a shared task head to perform transfer (Figure 1(b)). The third method extracts a time-varying latent dimension, i.e. a set of \(t_{rec}\) latent vectors, and performs Cross-modal Transfer Through Time (C3T) (Figure 1(c)).

We test these methods on Inertial Measurement Units (IMUs) and RGB video data on four datasets. Although the ST method works best on simple datasets, CA performs better in more difficult visual tasks. This indicates latent space alignment captures hidden correlations allowing the model to leverage one modality to infer a structure in the other. Furthermore, C3T consistently performs the best and is the most robust to time-shift, misalignment, and time-dialation noise as it accounts for the temporal information within each data sample. This investigation of UMA cross-modal transfer lies at the intersection of transfer learning, multimodal representation learning and holds significant implications for the applicability of machine learning in more diverse, underexplored, modalities.

## 2 Methods

We construct the Unsupervised Modality Adaptation (UMA) setting with RGB videos as the source of labeled data and IMU data as the target unlabled modality. As shown in Figure 1, a system can easily record synchronous data between these modalities, and then leverage an RGB model to perform HAR with only the IMU data. We mimic this setting by dividing 4 existing multimodal datasets into 4 splits as shown in Table 1. Training for each method occurs in two phases: _a) Supervised Learning_ with RGB data on \(_{HAR}\) and _b) Unsupervised Alignment_ across both modalities on \(_{Align}\). Inference can occur, with any combination of the input modalities (Table 2), however, we focus testing on IMU data (Table 3). \(_{Val}\) was used for hyperparameter search during training, and all tables report the average accuracy over three trials on \(_{Test}\). We propose three methods for transferring knowledge to a new sensing modality without exposure to labels in that modality, as depicted in Figure 2:

**(1) Student-Teacher (ST) Figure 1(a):** The ST method leverages an RGB video model trained in phase a) to produce psued-labels to train the IMU model in phase b). In this case, the latent space is the output logit space aligned using the cross-entropy loss, Equation (1). Various student-teacher models have been proposed ; however, these models often assume the availability of student-teacher-labeled modality pairs during training to distill knowledge from the teacher to the student. Furthermore, we use only one student and teacher module distinguishing our method from Thoker and Gall  who require an ensemble to strengthen the model in a similar setting.

**(2) Contrastive Alignment (CA) Figure 1(b):** The CA method performs phase a) Supervised RGB training in the same fashion as the student teacher, however, it uses a model with 2 parts: An encoder \(f^{(1)}\) to extract the latent variable \(z\), and a task specific MLP head \(h\). The extracted latent space \(\) allows for scalability and interoperability of adding different sensing modalities, types of encoders, and output task heads. Phase b) performs unsupervised contrastive alignment with the outputs of the the RGB encoder \(f^{(1)}\) and the IMU encoder \(f^{(2)}\) on unlabeled data using the symmetric contrastive loss formulation from  given by Equation (2), further detailed in appendix. The symmetric contrastive loss will cluster representations in \(\) by cosine similarity, which brings about the desired property of the latent space that vectors of the same class are near each other. A unified representation

   Split & \(X^{RGB}\) & \(X^{IMU}\) & \(Y\) & Size \\  Train a) \(_{HAR}\) & ✓ & & ✓ & 40\% \\ Train b) \(_{Align}\) & ✓ & ✓ & 40\% \\  Val \(_{Val}\) & & ✓ & 10\% \\ Test \(_{Test}\) & & ✓ & ✓ & 10\% \\   

Table 1: Data Splits for UMA

Figure 1: Motivation for Unsupervised Modality Adaptation (UMA)space for separate modalities allows the decision boundary trained on RGB representations, \(h\), to be used to recognize actions on IMU representations. This latent space is visualized in Figure 3.

**(3) Cross-modal Transfer Through Time (C3T) Figure 2c:** ST and CA do not leverage the temporal information of the data, making them difficult to use in real-world settings. C3T removes the final linear layer from the feature encoders of CA and uses the output of the temporal convolutions directly. This temporal receptive field would have extracted the salient features of neighboring time steps of the data. Then during the alignment phase, each of these time vectors is aligned with the same time vector from the other modality, using the same contrastive loss CA uses (Equation (2)). When training the HAR model, we use self attention with a learned class token to predict the action, similar to the ViT architecture , but instead of inputs being image chunks, they are temporal feature chunks. The intuition is that the encoder will learn which tokens over time are the most informative for the action class and predict accordingly.

## 3 Experiments

_How do we train the CA and C3T Architectures?_ We experimented with four ways to perform the two phases of training, as shown in Table 2. _1) Align First:_ First aligns the representations

Figure 2: Training and testing for three methods leveraging a unified latent space for UMA.

generated by the RGB and IMU encoders on \(_{Align}\), then freezes the RGB encoder and trains the HAR module on \(_{HAR}\). _2) HAR First:_ First trains the HAR module on \(_{HAR}\), then freezes the RGB encoder and performs cross-modal alignment on \(_{Align}\). _3) Interpersed Training:_ Intermittently learns one epoch from \(_{Align}\) then one epoch from \(_{HAR}\). _4) Combined Loss:_ Zips the \(_{HAR}\) and \(_{Align}\) dataloaders, computes gradients of the model for each batch, and updates the model with the combined loss \(L_{Total}=L_{CE}+L_{CL}\). Training method 1) performed the best for C3T and 4) for CA.

We hypothesize that in method 2) training the HAR model first yields a latent space to capture the best HAR features for RGB data, which is not directly applicable to IMU data. Method 3), faced instability in training and was unable to converge. Method 4), performed better than 1) for CA potentially since one loss acted as a regularizer for the other pushing the latent space \(\) to the ideal balance for cross-modal transfer in HAR. The other experiments in this work use training method 1).

_Can UMA methods retain performance on the original modality they were trained on? Can they leverage both modalities?_ Table 2 shows the result of training in the UMA setting, but testing with all combinations of the modalites: _1. RGB_ uses the RGB encoder and HAR module, _2. IMU (Zero-Shot Transfer)_ uses the IMU encoder and the HAR module, and _3. Fusion_ performs feature-level fusion (or late fusion for the ST method), by averaging the outputs of each of the encoders. Surprisingly for C3T, fusing (highlighted in red) performs better than RGB, indicating that unlabeled IMU data may add structure to the shared latent space to boost performance.

_Can our methods perform well on diverse RGB-IMU datasets?_ We test on a small yet structured dataset (UTD-MHAD ), a slightly larger dataset captured in a controlled environment with dense inertial data, i.e. 10 on-body sensors (CZU-MHAD ), one very large dataset with viewpoint and occlusion challenges (MMACT ), and one large egocentric camera dataset (MMEA-CL ). More details on the baselines and datasets are given in Appendix D.2 and Appendix D.1 respectively. The results (Table 3) show large performance gaps indicating C3T is far superior to the other methods. Furthermore, we compare the UMA performance against the Supervised methods and show C3T even outperforms supervised RGB on some datasets where the IMU data is highly informative.

_How robust is our method to time shifts and noise?_ Table 2 demonstrates C3T's superiority in the presence of temporal noise. We test on three real life noisy scenarios: 1) Crop: Accounts for the continuous nature of real-time action recognition (i.e. there is no defined start and stop time) by randomly shifting and cropping the time sequence of both modalities by 60% 2) Misalign: Imitates slight hardware asynchrony or different device framerates of the multimodal system by performing crop but on only one of the modalities. 3) Dilation: Mimics an action being performed slower, by randomly cropping both modalities to 50% of their original size an then upsampling.

**Qualitative Results:** We visualized the latent space outputs of the CA model using TSNE plots (Figure 3). These plots show training when the alignment phase (phase b in Figure 2) is performed first, and then labeled-RGB training (phase a) is performed. The model quickly segments classes

    &  &  &  \\ Model & 1. Align & 2. HAR & 3. Inter & 4. Comb & 1. IMU & 2. RGB & 3. Both & 1. Crop & 2. Misalign & 3. Dilate & 4.All & None \\  ST & - & - & - & - & 12.9 & 53.8 & 17.0 & 3.4 & 5.7 & 5.7 & 10.2 & 12.9 \\ CA & 38.6 & 43.2 & 27.3 & **42.6** & 42.6 & 56.8 & 60.2 & 10.2 & 2.3 & 21.6 & 18.2 & 42.6 \\ C3T & **62.5** & 35.2 & 51.1 & 27.9 & **62.5** & **78.4** & **79.5** & **52.3** & **46.6** & **56.8** & **58.0** & **62.5** \\   

Table 2: **Additional Experiments: Performance of ST, CA, and C3T across various training methods, modalities, and noise. All results report UMA accuracy on IMU data, except modality test 2. and 3.**

    & &  &  &  &  \\  & Model & Top-1 & Top-3 & Top-1 & Top-3 & Top-1 & Top-3 & Top-1 & Top-3 \\   & IMU & **87.9** & **97.7** & **95.1** & 98.2 & 70.0 & 90.0 & 65.8 & 87.6 \\  & RGB & 53.8 & 73.1 & 94.0 & **99.7** & 42.1 & 61.6 & 54.2 & 77.1 \\  & Fusion & 62.5 & 82.2 & 95.0 & 98.5 & **76.7** & **92.0** & **80.1** & **92.7** \\   & Random & 3.7 & 11.1 & 4.6 & 16.6 & 2.9 & 8.6 & 3.1 & 9.4 \\  & ST & 12.9 & 24.6 & 41.1 & 61.9 & 17.6 & 34.7 & 9.9 & 22.7 \\   & CA & 42.6 & 67.4 & 70.0 & 92.7 & 24.5 & 47.6 & 29.3 & 51.7 \\   & C3T & **62.5** & **86.4** & **84.2** & **96.7** & **32.4** & **57.9** & **51.2** & **78.8** \\   

Table 3: **UMA vs. Supervised Performance: The modules \(f^{(1)}\), \(f^{(2)}\), and \(h\) can operate in supervised or UMA (ST, CA, CT3) modes. Top-1 and Top-3 accuracies shown.**during the align phase, even without labels, suggesting that the data's natural structure facilitates class distinction across different modalities. This implies that our methods could potentially adapt to new class labels during testing with just a few samples, as the latent structure would have already grouped similar classes. Furthermore, after alignment and HAR training, we notice how the model tends to misclassify points that are near the boundary between clusters. These visualizations support our initial hypothesis (Figure 1) on how a joint latent space could be leveraged to effectively perform UMA, by using a classification head trained only on RGB data.

Interestingly, we observed that IMU data points consistently cluster towards the center of the plot, with RGB points surrounding them. This pattern persists even in early alignment stages, suggesting it's not solely due to labeled RGB HAR training. While this might indicate that RGB data is more informative, it contradicts our quantitative findings where supervised IMU models outperform RGB models for our given datasets. This phenomenon warrants further investigation as it may have implications for continual learning, test-time adaptation, or domain adaptation, where different modalities should be leveraged differently depending on their placement in the shared latent space.

## 4 Conclusion:

In this paper, we motivated and explored the Unsupervised Modality Adaptation (UMA) framework for human action recognition which challenges models to perform inference with a modality that is unlabeled during training. We conduct experiments to determine how to construct a unified latent space between modalities, outline three methods to perform UMA with their constructed latent space and compare their strengths in various settings. We hope our results inspire others to exploit cross-modal latent spaces to integrate continuous time sensor signals into AI models for more robust human motion understanding.

Figure 3: **CA TSNE Plots in UMA Training Method 1:** The following shows the progression of the latent representations of datapoints for 5 classes (Bowling, Clap, Draw circle (clockwise), Jog, Basketball shoot) during training CA on the UTD-MHAD dataset. At the end we plot the predicted labels and circle areas of confusion, which seems to often occur at the boundaries between clusters.