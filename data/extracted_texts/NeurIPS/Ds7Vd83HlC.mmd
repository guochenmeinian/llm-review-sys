# Faster Query Times for Fully Dynamic k-Center Clustering with Outliers

Leyla Biabani

Eindhoven University of Technology

Eindhoven, The Netherlands

l.biabani@tue.nl

&Annika Hennes

Heinrich Heine University Dusseldorf

Dusseldorf, Germany

annika.hennes@hhu.de

Morteza Monemizadeh

Eindhoven University of Technology

Eindhoven, The Netherlands

m.monemizadeh@tue.nl

&Melanie Schmidt

Heinrich Heine University Dusseldorf

Dusseldorf, Germany

mschmidt@hhu.de

###### Abstract

Given a point set \(P M\) from a metric space \((M,d)\) and numbers \(k,z\), the _metric \(k\)-center problem with \(z\) outliers_ is to find a set \(C^{*} P\) of \(k\) points such that the maximum distance of all but at most \(z\) outlier points of \(P\) to their nearest center in \(C^{*}\) is minimized. We consider this problem in the fully dynamic model, i.e., under insertions and deletions of points, for the case that the metric space has a bounded doubling dimension \(\). We utilize a hierarchical data structure to maintain the points and their neighborhoods, which enables us to efficiently find the clusters. In particular, our data structure can be queried at any time to generate a \((3+)\)-approximate solution for input values of \(k\) and \(z\) in worst-case query time \(^{-O()}k n\), where \(\) is the ratio between the maximum and minimum distance between two points in \(P\). Moreover, it allows insertion/deletion of a point in worst-case update time \(^{-O()} n\). Our result achieves a significantly faster query time with respect to \(k\) and \(z\) than the current state-of-the-art by Pellizzoni, Pietracaprina, and Pucci , which uses \(^{-O()}(k+z)^{2}\) query time to obtain a \((3+)\)-approximate solution.

## 1 Introduction

Clustering is a fundamental problem in machine learning and it has applications in many areas ranging from natural to social sciences. As a basic unsupervised learning method, it allows us to find structure in data. Classical center-based clustering methods for objectives like \(k\)-means, \(k\)-median and \(k\)-center have been around for many decades . Since the beginning of clustering, the data to be analyzed has changed dramatically, raising new challenges for clustering methods. Data nowadays comes in large to huge batches and is often prone to inherent change: Content on social media platforms is constantly added, re-organized or deleted, streaming services handle an always ongoing flow of people starting and stopping to watch, tweets are the fastest method to distribute news - and delete them later if desired. Data analysis methods thus face the challenge of analyzing huge amounts of data, ideally permanently updating their solution.

In this paper, we consider the problem of _dynamic_\(k\)-center clustering _with outliers_. The (metric) \(k\)-center problem is to find a set \(C^{*} P\) of \(k\) points such that the maximum distance of all points of \(P\) to their nearest center in \(C^{*}\) is minimized. This objective is well understood, \(2\)-approximations for the problem are known  and better approximations are not possible unless \(P\) equals \(NP\).

Considering outliers for \(k\)-center is very natural. Since \(k\)-center minimizes the maximum radius, a single measuring error can destroy the structure completely. Thus, the \((k,z)\)-center problem allows us to ignore \(z\) points. These belong to no clusters but are deemed to be _outliers_, and so they are ignored when determining the maximum radius of a cluster. The concept was introduced in 2001 by Charikar, Khuller, Mount, and Narasimhan  who as one result provide a \(3\)-approximation for \(k\)-center with outliers. This variant has since been the subject of many papers, and the best-known approximation algorithm for it, which gives a \(2\)-approximation, was derived as recently as 2016 .

In parallel, fully dynamic \(k\)-center algorithms (without outliers) have been developed  which work with a changing point set given as a stream of insertions and deletions. All these maintain a data structure that allows for continual updates to keep adapting to the changing data. An overview is given in Table 1. All results in this table support some form of _membership queries_: Given a point, return the cluster / center of the cluster of that point in the approximate solution. This type of query can typically be answered in time \(O(1)\). Reporting the centers takes more time or is not necessarily explained how to do.

Now the challenge is to combine the two parallel developments into a fully dynamic algorithm for \((k,z)\)-center clustering. We discuss the setting more precisely and then our results and related work.

Our settingWe say that a metric space has _doubling dimension_\(\) if \(\) is the smallest positive integer such that any ball of radius \(r\) can be covered by at most \(2^{}\) balls of radius \(r/2\). For an overview of this concept, we also refer the reader to . Let \((M,d)\) denote such a metric space and let \(P M\) be a set of points. By \(_{}\{d(x,y) x,y P,\ x y\}\) and \(_{}\{d(x,y) x,y P\}\) we denote the minimum and maximum inter-point distance within \(P\), respectively. Without loss of generality, we assume that \(_{}=2\). Let \(_{}/_{}\) be the aspect ratio of \(P\).

We are observing a dynamic stream, i.e., we start with a point set \(P=\) and then process a sequence of operations whose length is unknown a priori. Each operation either adds a point from \(M\) to \(P\) or it deletes a point which is currently in \(P\) from \(P\). We assume only deletions of points that are currently present are allowed, and we also assume that we can always compute \(d\) in time \(O(1)\) (otherwise, multiply all times mentioned in this paper by the time necessary to make one distance computation). We refer to the point set after time \(t\) (i.e., after \(t\) operations happened) as \(P_{t}\). If it is clear from the context, we will sometimes drop the subscript. We assume that \(d_{}\) and \(d_{}\) are fixed throughout the whole sequence of updates, i.e. \(d_{}=\{d(x,y) x,y P_{t},t 0\}\), and analogously for \(d_{}\). Therefore, \(\) is an upper bound on the aspect ratio over all updates and is known in advance. This is in accordance with a sequence of other works .

The problem that we address is the dynamic _\(k\)-center problem with \(z\) outliers_ or in short \((k,z)\)-_center problem_: Given \(P_{t}\) and numbers \(k\) and \(z\), produce a set \(C P_{t}\) of size at most \(k\) such that the maximum distance of all but at most \(z\) points to their nearest neighbor is minimized. More formally, \(_{Z P_{t},|Z| z}_{x P_{t} Z}_{c C}d(x,c)\) is minimized among all possible choices of \(C\) with \(|C| k\). 1

   Year & Ref. & Metric & Update time & Approx \\ 
2018 &  & general & amort. \(O(k^{2})\) & \(2+\) \\
2019 &  & Eucl. \(^{d}\) & amort. \(O(^{d} n)\) & \(16\) \\
2021 &  & doubl. & \(^{-O()}\) & \(2+\) \\
2023 &  & general & amort. \(O(k(n,))\) & \(2+\) \\   

Table 1: Fully dynamic \(k\)-center algorithms _without outliers_.

Our focus is on building a dynamic algorithm that has a low update time for insertion and deletions. We are allowed to store \(P\) (and additional information). Our aim is to achieve an update time for our data structure that is independent of \(k\) and a query time that is at most linear in \(k\). For this, it is necessary to consider a restricted model, as Bateni _et al._ showed that in arbitrary metric spaces, an update time of \((k)\) is necessary. We consider metric spaces _of bounded doubling dimension_. Obviously, time \((k)\) is necessary to return an actual solution, so we distinguish between _update time_ for insertion and deletions of points and _query time_ for obtaining a solution from the data structure.

Our result compared to related workWe present a deterministic \((3+)\)-approximation algorithm for the \(k\)-center problem with \(z\) outliers in bounded doubling dimension that is fully dynamic, i.e., points can be inserted and deleted. Our algorithm is based on a data structure that requires space linear in \(n\) and does not need to know \(k\) and \(z\) in advance. As a result, we can answer queries for all \(k\) and \(z\), or in other words, \(k\) and \(z\) can be part of the query. Moreover, our algorithm does not require knowledge of the doubling dimension \(\) of the underlying metric space, and it only appears in the analysis.

**Theorem 1.1** (Main theorem).: _Let \((M,d)\) be a metric space of bounded doubling dimension \(dim\), and let \(>0\) be an error parameter. There exists a deterministic dynamic algorithm that allows the insertion or deletion of points from \(M\) using worst-case \(^{-O()} n\) update time. Moreover, at any time \(t\), it can be queried by parameters \(k\) and \(z\) to compute a \((3+)\)-approximate solution for the \(k\)-center problem with \(z\) outliers of \(P_{t}\) using worst-case \(^{-O()}k n\) query time, where \(P_{t}\) is the set of points that are inserted but not deleted up to time \(t\), and \(n\) is the size of \(P_{t}\)._

The first approximation algorithm for this problem has been obtained by Chan, Lattanzi, Sozio, and Wang in 2022 , which has an approximation ratio of \(14+\). Very recently and in independent work, Pellizzoni, Pietracaprina, and Pucci  derived an algorithm with approximation ratio \(3+\). For the problem of distributed \(k\)-center with outliers, De Berg, Biabani, and Monemizadeh  give a randomized dynamic algorithm that also achieves a guarantee of \(3+\) with failure probability \(\). Both of these and also we develop algorithms that answer _solution queries_ rather than membership queries, i.e., they can produce a center set at any time and this set is an approximately good solution for the current \(k\)-center with outliers instance. We state the update times (for insertions and deletions of points) and the query times (for reporting the current approximate solution) of these and our approaches in Table 2.

All three known results have a dependency on \((k+z)^{2}\) in their query time which we improve to a linear dependence on \(k\) and no dependence on \(z\). This gives our approach an advantage even if \(k\) or \(z\) are only mildly dependent on \(n\): Already for \(k O( n)\) and \(z O()\), the query complexity of  would be linear in \(n\) compared to the poly-logarithmic dependency in terms of \(n\) for our query time. More interestingly, for a realistic regime where \(z\) is an epsilon-fraction of \(n\), the query time of the dynamic algorithm presented in  has quadratic dependency in terms of \(n\) while our query time still has a logarithmic dependency in terms of \(n\) and linear dependency in terms of \(k\).  also states a bicriteria approximation where the number of outliers can be violated with improved running time, but the dependency on \(k\) is still quadratic. When comparing to , it should be noted that their algorithm is only applicable to the Euclidean metric space, is randomized and only works against oblivious adversaries. Further, the coreset size (and the space complexity) of the algorithm in  is \(x=O(k^{-}+z)\). In the streaming model, in order to extract the solution, one needs to run an offline algorithm on the coreset. In this way, the query time of this algorithm is \(O(kx^{2} x)=O(k(z^{2}+k^{2})^{-2dim}^{2})\) which is significantly worse than the query time of our algorithm.

   Year & Ref. & Metric & Update time & Query time & Approx \\ 
2022 &  & general & \(O((k+z)^{2})\) & \(O((k+z)^{2})\) & \(14+\) \\
2023 &  & doubl. & \(}\) & \(}(k+z)^{2}\) & \(3+\) \\
2023 &  & doubl. & \(O(+z)^{4}()\) & \(O((+z)^{2}k(+z ))\) & \(3+\) \\ now & Thm 1.1 & doubl. & \(} n\) & \(}k n\) & \(3+\) \\   

Table 2: Fully dynamic \(k\)-center algorithms _with outliers_ and their update/query times.

The main difference of our approach is that it makes the greedy algorithm for \(k\)-center with outliers dynamic while the previous dynamic algorithms extract a coreset and run the greedy algorithm on this coreset. Known dynamic algorithms  for the \(k\)-center problem with outliers maintain a coreset after every update. In particular, in , they extract the coreset by simply reading the solution from the cover tree. To extract an approximate solution for this problem, one needs to run a known (offline) greedy algorithm on this coreset. In this way, the query complexities of those algorithms are dominated by the running time of the greedy algorithm. The novelty of our dynamic algorithm is that we make the greedy algorithm itself dynamic. To this end, we use heap data structures to compute a ball that covers the maximum number of points, and dynamic neighborhood sets to obtain points in the expanded maximum ball and update their corresponding keys in the heap to recursively find the next maximum balls.

Navigating nets.The state-of-the-art algorithm for the fully dynamic \(k\)-center problem (without outliers) in doubling metrics, developed by Goranci et al.  uses _navigating nets_ originally introduced by Krauthgamer and Lee . The basic idea of a navigating net is to start with the point set \(P_{t}\) as the base net \(N_{1}\) and then compute coarser and coarser variants of it, resulting in a hierarchy of nets \(N_{1},N_{2},,N_{2^{i}},\). For every level in this hierarchy, the points are good representatives of the points in the level below but the higher we get in the net, we have fewer points that are better separated. More precisely, following Krauthgamer and Lee , every level is a so-called \(r\)_-net_ of the level below it. An \(r\)-net on a set \(X\) is a subset \(Y X\) that satisfies two properties:

1. \( p X y Y d(p,y)<r\) (Covering)
2. \( x,y Y d(x,y) r\) (Packing)

In a navigating net, one always assures that \(N_{r}\) is an \(r\)-net of \(N_{r/2}\). Assume that the minimum inter-point distance \(}\) satisfies \(}=2\) and that the ratio between maximum and minimum inter-point distance is \(\). Then a navigating net with \(O()\) levels can be computed in a greedy fashion. Intriguingly, one can show that in the resulting navigating net, the lowest level with \(|N_{r}| k\) provides an \(8\)-approximation for the \(k\)-center problem (without outliers, i.e., \(z=0\)). This was first observed (in the context of streaming algorithms and there known as the _doubling algorithm_) by Charikar, Chekuri, Feder, and Motwani . An additional trick to improve the approximation ratio was discovered by McCutchen and Khuller  (again, in the setting of streaming). They observed that the quality of the solutions depends heavily on what the radius of the lowest level is, and that this radius can be shifted by small amounts, and one of the shifted versions will give a \(2+\) approximation. By maintaining the navigating net structure and using a similar shifting trick, dynamic \(k\)-center algorithms maintain enough information to answer queries by checking one or a few levels of the resulting navigating net and reporting the points on that level as the solution, as for example in .

Challenges with outliers.For the \((k,z)\)-center problem, the challenge is that there is no level in the navigating net which provides a good solution in itself. We know that an optimal \((k,z)\)-center solution has a radius that lies between the radius of a \(k\)-center solution (because any \(k\)-center solution is feasible for the \((k,z)\)-center problem) and the radius of a \((k+z)\)-center solution (because any \((k,z)\)-center solution is feasible for the \((k+z)\)-center problem). But as Figure 1 shows, neither of the two choices may provide a good estimate for the optimum \((k,z)\)-center solution. Say level \(r_{1}\) is the first to satisfy \(|N_{r_{1}}| k+z\) and level \(r_{2}\) is the first to satisfy \(|N_{r_{2}}| k\). Then the points in \(N_{r_{2}}\) are a feasible solution for the \((k,z)\)-center problem, but their radius may be too large, while the radius of the solution in \(N_{r_{1}}\) is guaranteed to be small enough, but these points may not constitute a feasible solution for \((k,z)\)-center. This is because every point in \(N_{r}\) represents several points in the levels below it. So we cannot simply divide the \(k+z\)

Figure 1: For \(k=1\) and \(z=3\), \(k\)-center and \((k+z)\)-center provide bad estimations for the best \((k,z)\)-center solution in this example. We see: (a) Dashed Ball: Optimum solution for \((k,z)\)-center. (b) Small Balls (too small): Optimum solution for \((k+z)\)-center. (c) Not drawn: the optimum solution for \(k\)-center is a ball around all points (too large).

points into \(k\) centers and \(z\) outliers (see Figure 1). Therefore, there is no individual level which we can query to obtain a solution. Instead, we need to compute a solution from the navigating net levels while making sure not to lose too much in terms of quality and query time.

Pellizzoni _et al._ base their approach on the so-called "cover tree" data structure, which is similar to that of a navigating net, to compute a coreset from which they construct the final solution. In their approach, they find the lowest level with \(|N_{r}| 2^{O()}^{-}(k+z)\) and demonstrate that such \(N_{r}\) serves as an \(\)-coreset for \((k,z)\)-center. However, since it is not a feasible solution, they utilize a \(3\)-approximation greedy algorithm to obtain a \((3+)\)-approximate clustering, which requires a time complexity of \((^{-2}(k+z)^{2})\). In this paper, we strategically integrate additional information at each level to directly emulate the offline \(3\)-approximation greedy algorithm by Charikar _et al._ in the dynamic setting. This approach enables us to achieve a time complexity that is linearly dependent on \(k\) and remarkably independent of \(z\).

## 2 Our data structure

In this section, we describe our data structure, which has \(O()\) levels. In each level \(r\), we have net \(N_{r}\), which is an \(r\)-net of \(N_{r/2}\). For each point \(p N_{r}\), we keep its weight \(w_{r}(p)\) and local neighborhoods \(B^{1}_{p,r}\) and \(B^{3}_{p,r}\), which are balls of radius \((1+O())r\) and \((3+O())r\) respectively. Then we maintain max-heap \(H_{r}\) which is designed to store the total weights of neighborhoods \(B^{1}_{p,r}\) for all \(p N_{r}\). We also keep \(_{r}(p)\) for each point \(p P\), which is the representative of \(p\) in \(N_{r}\). Below, we provide a detailed explanation for elements of our data structure.

Input and set \(R\).To start building our data structure, we require three input parameters: \(\), \(^{}\), and \(\), where \(0<^{}<1\), and \(< 1\). \(\) denotes the spread ratio of the underlying point set \(P\) and \(^{}\) is the given error parameter. We utilize \(\) to define \(R:=\{0\}\{ 2^{} 0_{2}+1, \}\) as a set of levels. Additionally, we define \(:=2^{^{}}\). This implies that \(^{}/2<^{}\). Furthermore, if \(r R\), then either \( r R\) or \( r<\).

Nets \(N_{r}\).We maintain net \(N_{r}\) for each \(r R\), satisfying the following conditions: \(N_{0}=N_{}=P_{t}\), where \(P_{t}\) is the set of points at time \(t\), and for \(r>\), \(N_{r}\) is an \(r\)-net of \(N_{r/2}\). For \(p N_{r}\) and \(q N_{r/2}\), we say \(p\) is a _parent of \(q\)_ (and \(q\) is a _child of \(p\)_) if \(d(p,q) r\). In our data structure, we store all the parents and children for each point for all the nets. It is important to note that each point can have at most \(2^{O()}\) parents or children. In the full version, we explain how to handle each insert/delete in time \(2^{O()}\).

During the paper, we may refer to \(N_{ r}\) for \(0< r<\), which means \( r R\). Besides, note that since \(_{}=2\) and \(<2\), \(P_{t}\) is an \(r\)-net for any \(0 r\). To solve this issue, we define \(N_{r}:=P_{t}\) for any \(0<r<\), however, we do not explicitly keep any \(r\)-nets for \(0<r<\) in our data structure to prevent redundancy.

Representatives rep. and weights \(w_{r}\).For all \(r R\), we inductively define functions \(_{r} P_{t} N_{r}\) such that each point \(p P_{t}\) has a unique _representative_\(_{r}(p)\) in net \(N_{r}\). To define \(_{r}(p)\), we consider three cases. First, if \(p N_{r}\), then we define \(_{r}(p):=p\). Second, if \(p N_{r}\) but \(p N_{r/2}\), we uniquely choose one of the parents of \(p\) in \(N_{r}\) as \(_{r}(p)\). Third, if \(p N_{r/2}\) and therefore \(p N_{r}\), we define \(_{r}(p)=_{r}(_{r/2}(p))\). Again to avoid inconsistency, we define \(_{r}(p):=p\) for any \(0<r<\), but we do not store this explicitly. We show in Lemma 2.1 that the distance between each point \(p\) and its representative in \(N_{r}\) is at most \(2r\).

Next, we define a weight function \(w_{r}\) for every level. For every point \(p N_{r}\), its _weight_\(w_{r}(p)\) denotes the number of points that \(p\) is representing in \(N_{r}\). More formally, \(w_{r}(p)=\{q P_{t}_{r}(q)=p\}\). For a set \(X N_{r}\), we define \(w_{r}(X)_{p X}w_{r}(p)\). Equivalently, \(w_{r}(X)=\{y P_{t}_{r}(y) X\}\). Note that the sum of weights of all representatives at any level is equal to the total number of points, so \(w_{r}(N_{r})= P_{t}\) for all \(r R\).

Since we store all the parents and children, we can easily update the representatives and therefore the weights after each insert/delete. The details are presented in the full version.

**Balls \(B^{1}\) and \(B^{3}\).** Let \(r R\). For every point \(p N_{cr}\), we maintain the _close neighborhood_\(B^{1}_{p,r}(p,(1+4)r) N_{er}\) and the _extended neighborhood_\(B^{3}_{p,r}(p,(3+8)r) N_{er}\), where \((p,)=\{q M d(p,q)\}\) denotes the ball of radius \(\) around \(p\) in the metric space \(M\). Then \(B^{1}\{B^{1}_{p,r} r R,\ p N_{er}\}\) and \(B^{3}\{B^{3}_{p,r} r R,\ p N_{er}\}\) are the collections of local neighborhoods at different scales. As mentioned in Lemma 2.2, the size of each of these neighborhoods is at most \(2^{O()}/^{}\). We also show that we can update these neighborhoods in time \(2^{O()}^{-2}\) after each insert/delete in the proof of Lemma 2.3.

**Heaps \(H_{r}\).** For every \(r R\), we maintain a max-heap \(H_{r}\) of the set \(\{(p,w_{cr}(B^{1}_{p,r})) p N_{er}\}\) representing the weight of the close neighborhood of any point in level \(r\). With \(H_{r}[p]\) we denote the value of the max heap element with key \(p\). Since \(B^{1}_{p,r}\) might change, we need to find the position of the respective heap element by its key \(p\). Hence, for any level \(r\), we keep a pointer from every element in \(N_{r}\) to its corresponding element in \(H_{r}\).

Whenever an update happens to a set \(B^{1}_{p,r}\) for some point \(p N_{er}\), we need to update the value of \(w_{er}(B^{1}_{p,r})\) as well as the value of \(H_{r}[p]\). Each update in the max-heap \(H_{r}\) can be done in \(O( n)\) time using the standard max-heap operations. In the full version, we show that \(2^{O()^{-}}\) updates happen to set \(B^{1}\). Therefore, we need \(2^{O()^{-}} n\) time to update the heaps.

A few lemmas about our data structure.Now we mention a few useful lemmas about our data structure. The formal proofs can be found in the full version. The first lemma is about the distance of each point to its representative, which is bounded because of a geometric series argument.

**Lemma 2.1**.: _Let \(p P_{t}\) and \(r R\). Then \(d(p,_{r}(p)) 2r\). In particular, \(d(p,_{er}(p)) 2 r\), where \(=2^{^{}}\) for the error parameter \(^{}\)._

We next discuss the size of close neighborhoods and extended neighborhoods.

**Lemma 2.2**.: _Let \(r R\) and \(p N_{er}\). The sets \(B^{1}_{p,r}\) and \(B^{3}_{p,r}\) defined above are of size at most \(2^{O()}^{-}\). In particular, the size is \(O(^{-})\) for a constant doubling dimension \(\)._

In the next lemma, we show that the update time is logarithmic in the spread ratio and the size of \(P_{t}\).

**Lemma 2.3**.: _Let \(n\) be the size of point set \(P_{t}\). Then after insertion/deletion of any point to \(P_{t}\), we can update our data structure in time \(2^{O()}^{-2} n\)._

Besides insertion and deletion updates, our data structure supports solution queries: in every time step, it can produce a solution in the form of a center set and a radius. We will discuss this in more detail and present an algorithm that outputs a \((3+O())\)-approximation in time \(^{-O()}k n\) in the following section.

## 3 Our algorithm

OverviewFor a given level \(r\) in \(R\) and a number of centers \(k\), the main procedure MaxCoverage produces a set of \(k\) centers and a value \(\). One run of this procedure is deemed successful if \( z\) at the end. Indeed, we will later show that \(\) is an upper bound on the actual number of outliers in the clustering with centers \(C\) and radius \((3+10)r\), which implies that this is a feasible solution. The algorithm FindCenters finds one such level for which MaxCoverage is successful. We also ensure that this level is chosen such that MaxCoverage is not successful on the next lower level \(r/2\), which guarantees that the radius of our solution does not deviate from the optimal solution by too much.

In our algorithm, we repeatedly find areas that subsume the most points. In order to do this efficiently, we maintain the collections of close neighborhoods and their weights in max-heaps. Fix a level \(r R\). For \(k\) iterations, the root node of the current heap corresponds to the next center of our solution. After picking it, we mark points in the extended neighborhood because our final approximate solution will cover these points and they should not contribute to the weight of other neighborhoods anymore.

As these points are now covered, we decrease the weights of heap elements representing points therein accordingly. These steps are performed by MaxCoverage. Note that for maintenance reasons, one run of findCenters should leave the original heaps unchanged. We could make a copy of the heap before processing it in MaxCoverage, but it is faster to work on the original heaps and just roll back all the updates done during MaxCoverage at the end of the algorithm, as described in Lemma 3.8.

It remains to find a good value for \(r\). By applying a binary search, we find a radius \(r\) such that the value of _outliersWeight_ outputted by MaxCoverage(\(k\), \(r\)) is at most \(z\), while the respective output on the lower level \(r/2\) is more than \(z\). Note that it is possible to find such a pair of subsequent radii via binary search although the search space might not be monotone. In the special case that there are at most \(z\) points in total, we can just set the radius to 0. This is done in Algorithm 1 (findCenters\((k,z)\)).

AnalysisNext, we prove the approximation guarantee and time complexity of our algorithm.

Let \(r\) and \(k\) be fixed, and let \(i[1,k]\) be an integer. For any point \(y N_{ r}\), we say \(y\) is _marked_ after the \(i\)-th iteration of the loop in Line 3 of MaxCoverage, if \(y_{j=1}^{i}B_{c_{j},r}^{3}\), and we say it is _unmarked_ if it is not the case. For simplicity, we may refer to a point as marked or unmarked without specifying the iteration if it is clear from the context. We define set \(U_{i}:=\{y N_{ r} y_{j=1}^{i}B_{c_{j},r}^{3}\}\) as the _set of unmarked points_ after the \(i\)-th iteration. We next define \(O_{i}:=\{p P_{t}_{ r}(p)_{j=1}^{i}B_ {c_{j},r}^{3}\}\) as the set of points that are _considered outliers_ after the \(i\)-th iteration of the loop. Equivalently, \(O_{i}:=\{p P_{t}_{ r}(p) U_{i}\}\). After the \(i\)-th iteration of the loop in Line 3, for each \(y N_{ r}\), \(H_{r}[y]\) is the total weight of unmarked points in \(B_{y,r}^{1}\).

**Lemma 3.1** (Invariant for heaps).: _Let \(k\) be an integer, \(0 i k\), and \(r R\). Then for any \(y N_{ r}\) it holds \(H_{r}[y]=w_{ r}(B_{y,r}^{1} U_{i})\) after the \(i\)-th iteration._

findCenters\((k,z)\) returns a solution when _outliersWeight_\( z\). To show that this solution is feasible, it remains to prove that _outliersWeight_ is an upper bound on the actual number of outliers of this solution.

**Lemma 3.2** (Feasible solution).: _Consider a radius \(r R\), and an integer \(k\). Then MaxCoverage\((k,r)\) returns \(k\) centers and a value outliersWeight, such that the total number of points in \(P_{t}\) that are not within distance \((3+10)r\) of these \(k\) centers is at most outliersWeight._

Proof.: Let _outliersWeight_ denote MaxCoverage\((k,r)\). _outliersWeight_. We want to prove that MaxCoverage\((k,r)\) returns \(\{c_{1},,c_{k}\}\) such that \(|P_{t}_{j k}(c_{j},(3+10)r)|\)_outliersWeight_. In every iteration \(i\), _outliersWeight_ is decreased by \(w_{ r}(y)\) for every \(y B_{c_{i},r}^{3}\) Marked. After processing \(y\), we add it to Marked. This way, we ensure that no \(y\) is charged twice. Therefore, by the end of the algorithm, _outliersWeight_ is decreased by \(w_{ r}(_{j k}B_{c_{j},r}^{3})\)Hence, in the end, the value of _outliersWeight_ is

\[&=n-w_{ r}( _{j k}B^{3}_{c_{j},r})\\ &=|P_{t}|-|\{p P_{t}_{ r }(p)_{j k}B^{3}_{c_{j},r}\}|\.\]

For all \(p P_{t}\), if \(_{ r}(p)_{j k}B^{3}_{c_{j},r}\), then \(d(c_{j},_{ r}(p))(3+8)r\) holds for at least one \(j k\) by definition of \(B^{3}_{c_{j},r}\). Using Lemma 2.1, this implies \(d(c_{j},p) d(c_{j},_{ r}(p))+d(_{  r}(p),p)(3+8)r+2 r=(3+10)r\). Hence, \(\{p P_{t}_{ r}(p)_{j k}B^{3}_{c_{j},r}\}\{p P_{t} j k d(c_{j},p)(3+10 )r\}\) and therefore

\[&|P_{t} |-|\{p P_{t} j k d(c_{j},p)(3+10 )r\}|\\ &=|P_{t}|-|_{j k}(c_ {j},(3+10)r) P_{t}|\.\]

Consider a fixed optimal solution. The following insight is crucial for our proof: In every iteration of MaxCoverage\((k,r)\) for \(r\), we select a ball that covers a sufficient number of unmarked points. To be precise, it exceeds the number of currently considered outliers in any optimal cluster.

**Lemma 3.3** (Invariant for picking \(c_{i}\)).: _Let \(r\), where \(r R\), and let \(c^{*} P_{t}\) be a center of an optimal solution for \(k\)-center with \(z\) outliers. Assume that we execute MaxCoverage\((k,r)\). Then for any \(i[1,k]\), the \(c_{i}\) picked in Line 4 is such that \(|(c^{*},OPT) O_{i-1}| w_{ r}(B^{1}_{c_{i},r} U_{i-1})\)._

Proof.: We define \(:=_{ r}(c^{*})\), the representative of \(c^{*}\) in \(N_{ r}\). Recall that \(B^{1}_{,r}=\{y N_{ r} d(,y)(1+4 )r\}\). Lemma 3.1 states that \(H[y]=w_{ r}(B^{1}_{y,r} U_{i-1})\) holds for all \(y N_{ r}\) after the \((i-1)\)-th iteration. Since \(c_{i}\) is on top of the max-heap \(H\), we have \(w_{ r}(B^{1}_{,r} U_{i-1}) w_{ r}(B^{1}_{ c_{i},r} U_{i-1})\). Therefore, it remains to show that \(|(c^{*},) O_{i-1}| w_{ r }(B^{1}_{,r} U_{i-1})\).

According to the definition of representative and weight function, we first have

\[|(c^{*},) O_{i-1}|_{y _{ r}((c^{*},) O _{i-1})}w_{ r}(y).\]

We next show that for any point \(p(c^{*},) P_{t}\), its representative on level \( r\) lies in \(B^{1}_{,r}\), i.e. \(_{ r}(p) B^{1}_{,r}\). Let \(p(c^{*},) P_{t}\) be an arbitrary point, then \(d(c^{*},p) r\). By the triangle inequality, \(d(,_{ r}(p)) d(,c^{*})+d(c^{*},p)+d(p, _{ r}(p))\). Besides, Lemma 2.1 implies that \(d(,c^{*}) 2 r\) and \(d(p,_{ r}(p)) 2 r\). Putting everything together we have

\[d(,_{ r}(p)) d(,c^{*})+d(c^{*},p)+d(p, _{ r}(p)) 2 r+r+2 r=(1+4 )r.\]

Besides, \(p O_{i-1}\) is equivalent to \(_{ r}(p) U_{i-1}\). Therefore, \(p(c^{*},) O_{i-1}\) implies \(_{ r}(p) B^{1}_{,r} U_{i-1}\), and we have \(_{ r}((c^{*},) O _{i-1}) B^{1}_{,r} U_{i-1}\). Thus,

\[_{y_{ r}((c^{*},) O_{i-1})}w_{ r}(y)_{y B^{1}_{,r} U_{i-1}}w_{  r}(y)=w_{ r}(B^{1}_{,r} U_{i-1}),\]

which finishes the proof. 

Utilizing Lemma 3.3, we can now show that the weight of the final output of MaxCoverage\((k,r)\) with \(r\) is at least the number of points covered by any optimal solution.

**Lemma 3.4** (Invariant for coverage weight).: _Let \(C^{*}\) be the set of \(k\) centers of an optimal solution for \(k\)-center with \(z\) outliers on \(P_{t}\), and let \(\) be the radius of this solution. Let \(r\), where \(r R\). Then,_

\[|_{c^{*} C^{*}}(c^{*},OPT) P_{t}|  w_{ r}(_{j=1}^{k}B^{3}_{c_{j},r})\]

_(recall that \(B^{3}_{c_{j},r}=(c_{j},(3+8)r) N_{  r}\) and \(w_{ r}(B^{3}_{c_{j},r})=|\{y P_{t}_{ r }(y) B^{3}_{c_{j},r}\}|\)._Proof.: To prove the lemma, we show that for any \(i[0,k]\), there is a charging that satisfies the following conditions: 1) There is a set \(\{c_{1}^{*},,c_{i}^{*}\} C^{*}\) and a function that charges each point in \(_{j=1}^{i}(c_{j}^{*},OPT) P_{t}\) to a point \(y_{j=1}^{i}B^{3}_{c_{j},r}\), and 2) for any point \(y_{j=1}^{i}B^{3}_{c_{j},r}\), at most \(w_{er}(y)\) points from \(_{j=1}^{i}(c_{j}^{*},OPT) P_{t}\) are mapped to \(y\).

We prove this claim by induction on \(i\). The base case \(i=0\) trivially holds. Assume that \(i 1\) and the induction hypothesis holds for \(i-1\). That is, there is a charging satisfying conditions 1 and 2 for \(i-1\). Then, we prove that these conditions hold for \(i\).

To define center \(c_{i}^{*}\), we consider two cases:

**Case 1:**: there exists a center \(c^{*} C^{*}\{c_{1}^{*},,c_{i-1}^{*}\}\) for which \(B^{1}_{c_{i},r}\) hits the set of representatives of \((c^{*},)\). That is, we have \(_{er}((c^{*},)) B^{1}_{c_{ i},r}\). We then let \(c_{i}^{*}=c^{*}\). We map every point of \((c^{*},)\) to its unique representative.
**Case 2:**: the first case is not correct. Then, we let \(c_{i}^{*}\) be any arbitrary point in \(C^{*}\{c_{1}^{*},,c_{i-1}^{*}\}\). In this case, we charge the uncharged points of \((c_{i}^{*},)\) to \(B^{1}_{c_{i},r}\).

We first consider Case 1. We charge each point \(p(c_{i}^{*},OPT)\) to its representative \(_{er}(p)\). We next prove the following claim:

_Claim._ Let \(p(c_{i}^{*},OPT)\). If Case 1 happens, then for every point \(q P_{t}\) that is charged to \(_{er}(p)\), it holds that \(_{er}(q)=_{er}(p)\).

Proof.: For the sake of contradiction, assume that there is a point \(q P_{t}\) such that \(q\) is charged to \(_{er}(p)\) and \(_{er}(q)_{er}(p)\). It means that at an iteration \(j<i\), Case 2 happened and \(q\) is charged to \(_{er}(p)\). Since Case 2 happened at iteration \(j\), we have \(_{er}((c_{i}^{*},OPT)) B^{1}_{c_{j},r}=\) and also \(q\) is charged to a point in \(B^{1}_{c_{j},r}\). Adding it to the assumption that \(q\) is charged to \(_{er}(p)\) we have \(_{er}(p) B^{1}_{c_{j},r}\). Besides, \(p(c_{i}^{*},OPT)\) and therefore we have \(_{er}(p)_{er}((c_{i}^{*},OPT))\). This implies that \(_{er}(p)_{er}((c_{i}^{*},OPT)) B ^{1}_{c_{j},r}\), which is a contradiction to \(_{er}((c_{i}^{*},OPT)) B^{1}_{c_{j},r}=\). 

The claim implies that for each point \(p(c^{*},OPT)\), at most \(w_{er}(_{er}(p))\) points are charged to \(_{er}(p)\). See Figure 2, where the left blue ball represents \(B^{1}_{c_{i},r}\), the right blue one indicates the area that points from \(_{er}((c^{*},))\) can lie in, and the orange circle represents \((c^{*},)\). For each point \(p(c^{*},OPT)\), we have \(_{er}(p) B^{3}_{c_{i},r}\): Let \(q(c^{*},)\) be such that \(_{er}(p)_{er}((c^{*}, {OPT})) B^{1}_{c_{i},r}\). The distance of \(c_{i}\) to any representative of a point \(p(c^{*},)\) is

\[d(c_{i},_{er}(p))  d(c_{i},_{er}(q))+d(_{er}(q),q)+d(q,c^{* })+d(c^{*},p)+d(p,_{er}(p))\] \[(1+4)r+2 r+2+2  r(3+8)r,\]

implying that \(_{er}(p) B^{3}_{c_{i},r}\).

For Case 2: note that if \(p P_{t} O_{i-1}\), then by definition of \(O_{i-1}\), we have \(_{er}(p) U_{i-1}\), and hence \(_{er}(p)_{j=1}^{i-1}B^{1}_{c_{j},r}\) by definition of \(U_{i-1}\). So, by induction hypothesis, \(p\) is already charged to \(_{er}(p)\). Therefore, we only need to charge each point \(p(c^{*},OPT) O_{i-1}\). By Lemma 3.3 we have \(|(c^{*},OPT) O_{i-1}| w_{er}(B^{1}_{c_{i},r} U _{i-1})\). Also, we only charged to

Figure 2: Illustration of the claimâ€™s consequences.

the points in \(_{j=1}^{i-1}B^{3}\), which means that no point is charged a point in \(U_{i-1}\). Therefore, points in \((c^{*},OPT) O_{i-1}\) can be charged to points in \(B^{1}_{c_{i},r} U_{i-1}\). 

**Lemma 3.5**.: _Let \(r R\) and \(OPT\) be the optimal radius for \(k\)-center clustering of \(P_{t}\) with \(z\) outliers. Then if \(r OPT\), the value \(\) returned by \((k,r)\) is at most \(z\)._

Proof.: Let \(C=\{c_{1},,c_{k}\}\) be the set of centers returned by \((k,r)\) and \(C^{*}\) be the set of centers of an optimal \(k\)-center clustering of \(P_{t}\) with \(z\) outliers. By the same reasoning as in the proof of Lemma 3.2, it is

\[=n-w_{ r}(_{i=1}^{k}B^{3}_{c_{i},r}).\]

Besides, Lemma 3.4 states that \(w_{er}(_{i=1}^{k}B^{3}_{c_{i},r})|_{c^{*} C^{*}}(c ^{*},OPT) P_{t}|\). Moreover, as \(C^{*}\) are the centers of an optimal solution, \(n-|_{c^{*} C^{*}}(c^{*},OPT) P_{t}| z\). Putting everything together, we have

\[ n-w_{ r}(_{i=1}^{k}B^{3}_{c_{i},r} ) n-|_{c^{*} C^{*}}(c^{*},OPT) P_{t}| z,\]

which finishes the proof. 

Lemma 3.2 and Lemma 3.5 together imply that the solution returned by Algorithm 1 is feasible. Further, it can be shown that the level \(\) at which \((k,z)\) becomes successful, fulfills \( r^{*}\). Together, this implies the following statement.

**Lemma 3.6** (Approximation guarantee).: _Let \(OPT>0\) be the optimal radius for the \(k\)-center clustering of \(P_{t}\) with \(z\) outliers, and let \(r^{*}\) be the minimum number in \(R\) such that \(r^{*} OPT\). Then \((k,z)\) returns a \((3+10)\)-approximate solution for \(k\)-center clustering problem with \(z\) outliers, where \(=}{}\)._

For \(\) defined in Lemma 3.6, \(1<2\) holds according to the definition of \(R\). It immediately leads to a \((6+O())\)-approximation algorithm. Next in Lemma 3.7, we show how to get \((1+)\), which improves the approximation ratio of our algorithm to \(3+O()\). The idea is to run parallel instances of our data structure with different values for parameter \(\), so that always \(OPT r<(1+)OPT\) holds for a \(r R\) in one of the instances.

**Lemma 3.7** (Optimizing the approximation ratio).: _Let \(OPT>0\), and let \(>0\) be fixed. We define \(m:= 1/_{2}{(1+)}\). Suppose we have \(m\) parallel instances of our data structure with parameter \(=2^{j/m-1}\) for the \(i\)-th instance. Then in at least one of the instances, we find a \((3+O())\)-approximation solution for \(k\)-center clustering with \(z\) outliers by calling \((k,z)\)._

The procedure \((k,r)\) needs to temporarily edit the heap \(H_{r}\). This can be done by working on a copy of the heap, as indicated in line 2. The following result describes how this can be done faster, without actually copying the heap. The idea is to work in place and roll back all changes in the end. The formal proof is given in the full version.

**Lemma 3.8** (Imitating of copying \(H_{r}\)).: _Let \(u\) be the number of times that \((k,r)\) updates heap \(H_{r}\). Then copying heap \(H_{r}\) in Line 2, as well as all these \(u\) updates, can be handled in total time \(O(u n)\)._

By using Lemma 3.8 and Lemma 2.2, we get the query time in Lemma 3.9.

**Lemma 3.9** (Query time).: _Let \(k\) and \(z\) be two given parameters. Then \((k,z)\) described in Algorithm 1 has a time complexity of \(2^{O()}^{-2}k n\)._

## 4 Conclusion

We developed a data structure for the fully dynamic \(k\)-center with \(z\) outliers problem in metrics of bounded doubling dimension. As compared to other works, the algorithm exhibits an improved query time while achieving the currently best-known approximation ratio of \(3+\). The query time is optimal with respect to the dependency on \(k\). Although the exponential dependency on the doubling dimension in the running times seems necessary, it is actually not clear if this could be improved to tackle even wider classes of metric spaces. This would be an interesting aspect for future work. However, Chan et al.  show that an amortized running time of \((z)\) would be needed in general metric spaces.

Acknowledgements

The authors would like to thank the anonymous reviewers for their insightful comments. Annika Hennes' and Melanie Schmidt's research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - project number 456558332.