# One-Shot Safety Alignment for Large Language Models via Optimal Dualization

Xinmeng Huang

xinmengh@sas.upenn.edu

Equal contribution.

&Shuo Li

lishuo1@seas.upenn.edu

&Edgar Dobriban

dobriban@wharton.upenn.edu

&Osbert Bastani

obastani@seas.upenn.edu

&Hamed Hassan

hassani@seas.upenn.edu

&Dongsheng Ding

dongshed@seas.upenn.edu

University of Pennsylvania

###### Abstract

The growing safety concerns surrounding large language models raise an urgent need to align them with diverse human preferences to simultaneously enhance their helpfulness and safety. A promising approach is to enforce safety constraints through Reinforcement Learning from Human Feedback (RLHF). For such constrained RLHF, typical Lagrangian-based primal-dual policy optimization methods are computationally expensive and often unstable. This paper presents a perspective of dualization that reduces constrained alignment to an equivalent unconstrained alignment problem. We do so by pre-optimizing a smooth and convex dual function that has a closed form. This shortcut eliminates the need for cumbersome primal-dual policy iterations, greatly reducing the computational burden and improving training stability. Our strategy leads to two practical algorithms in model-based and preference-based settings (MoCAN and PeCAN, respectively). A broad range of experiments demonstrate the effectiveness and merits of our algorithms.

## 1 Introduction

Language Models (LMs) trained on massive text datasets have demonstrated remarkable capabilities in natural language generation. These models are increasingly used in various applications, such as translation , summarization , robotic navigation , and code generation . However, there are growing concerns surrounding LMs, for instance about biases against certain groups , proliferation of false information , and leakage of sensitive information . To prevent such undesirable behaviors, it becomes crucial to align pre-trained LMs with human preferences such as helpfulness, truthfulness, and non-toxicity, a practice often referred to as _safety alignment_.

Reinforcement Learning with Human Feedback (RLHF) has been widely adopted in LM alignment . Standard RLHF promotes one specific goal, typically the helpfulness of LM-generated responses, by tuning an LM to maximize an associated reward. However, there are notable shortcomings of the standard RLHF. First, since the reward function is, in practice, an inaccurate proxy for true preferences, solely optimizing it often degrades the ground truth performance . Second, a single reward with scalar output is often insufficient to represent multiple preference aspects beyond helpfulness ; _e.g._, helpfulness and harmlessness are not always easily compatible . Moreover, a single reward function fails to reflect the preference diversity across human groups , which is important for fairness . Addressing these challenges requires developing new approaches to accomplish safe alignment more effectively.

To mitigate the issues with RLHF, a simple approach is to add constraints associated with safety preferences, such as harmlessness . Thus, constrained RLHF tunes an LM by maximizing a target reward subject to constraints on auxiliary safety objectives . Constrained RLHF comes with several challenges in practice. First, unlike the reward-only optimization in standard RLHF, constrained RLHF often employs _iterative primal-dual methods_ based on the Lagrangian, repeatedly updating the LM and the dual variables associated with the constraints . Such primal-dual methods often suffer from training instability and increased sensitivity to hyperparameters . Second, updating the dual variables requires re-training LMs on new objectives, which can be prohibitive, as fitting large LMs demands massive computation and memory resources . Ideally, we would like methods that train LMs only once (_i.e._, one-shot) with a fixed objective, as in standard RLHF. This motivate the following question:

_Can we align language models under safety constraints in a **one-shot** manner?_

**Contributions.**  We answer the above question affirmatively by devising non-iterative methods for LM safety alignment with constrained RLHF, where the LM to be aligned is required to outperform a reference LM in safety properties of interest by specified margins. Our contribution is four-fold.

1. Viewing constrained RLHF as primal-dual optimization in _distribution space_, we establish that the dual function (_i.e._, the Lagrangian evaluated at dual-wise optimal policies) takes a closed form and favorable optimization properties, such as smoothness and local strong convexity.
2. From the dual perspective on constrained RLHF, we establish Constrained Alignment via dualization (CAN) in a _two-stage strategy_: first, obtain the optimal dual variables by optimizing an explicit dual function; and second, use the optimal dual variables to reduce constrained alignment to unconstrained alignment. This shortcut avoids expensive primal-dual iterations, accomplishing constrained alignment with one-shot LM training.
3. We develop two practical alignment algorithms, termed by MoCAN and PeCAN, following the two-stage strategy in model-based scenarios (relying on off-the-shelf reward and safety models), and preference-based settings (relying on human-annotated preference data), respectively.
4. We conduct extensive experiments to demonstrate the effectiveness of our proposed methods. Our dual perspective predicts the safety improvement of practically aligned LMs effectively.

## 2 Preliminaries

Let \(\) and \(\) be the set of prompts and responses of arbitrary lengths, respectively, and let \(\) be the distribution of an LM - also referred to as a _policy_ - that maps each prompt \(\) to a distribution \((\,|\,)\) over the response set, _i.e._, \(()\), where \(()\) is the set of all distributions over \(\).

RLHF is a common technique used in LM alignment , with three stages: (i) supervised fine-tuning; (ii) reward modeling; (iii) RL fine-tuning. The first stage fine-tunes a pre-trained LM with supervised learning on a high-quality dataset to obtain a policy \(_{}\). In the second stage, reward modeling queries the policy \(_{}\) with a prompt \(\), generating two responses \(_{0}\), \(_{1}\). The binary variable \([\,_{1}_{0}\,]\{0,1\}\) (_i.e._, is \(_{1}\) preferred over \(_{0}\)?) given by human annotators is recorded. Repeating this with \(N\) prompts yields a preference dataset \(\{^{(n)},_{1}^{(n)},_{0}^{(n)}, [\,_{1}^{(n)}_{0}^{(n)}]\}_{n=1}^ {N}\). Following the widely used Bradley-Terry setup , one assumes there is a latent reward function \(r\) such that \(([\,_{1}_{0}\,]=1\,|\, )=(r(,_{1})-r(, _{0}))\) for all \(\), where \( t 1/(1+)\) is the sigmoid function. Since the true reward model is usually unavailable, one can learn a proxy reward - via, _e.g._, the maximum-likelihood estimation over a parametrized function class - from the preference dataset ; see Appendix F for details.

Denoting the KL divergence between two probability distributions \(p\) and \(q\) by \(D_{}(p\,\|\,q)\), the third - RL fine-tuning - stage of standard RLHF aims to solve a regularized alignment problem,

\[*{maximize}_{\,\,}\;_{\,\, }\,\,_{\,\,(\,|\, )}[\,r(,)\,]\,-\,\,D_{ }((\,|\,)\,\|\,_{}(\,|\,))\,\] (A)

where \(\) is the set of all policies, \(\) is the distribution induced by the prompt dataset, and \(>0\) is a parameter that regularizes the LM towards the reference model \(_{}\). In practice, one optimizes the objective (A) associated with a proxy reward instead. A key issue with RLHF is the mismatch between the learned reward and the true human preference . Moreover, a single reward model fails to capture multiple human preferences. Consequently, LMs fine-tuned via standard RLHF often exhibit unsafe behaviors, such as discrimination, misinformation, providing unethical answers, etc.

To ensure the safety of LMs, one may augment (A) with auxiliary safety constraints. To this end, one may annotate preferences according to various safety aspects (_e.g._, harmlessness, fairness, etc.) to learn _safety utility models_ or _safety models_ for short. Specifically, we can rank responses \(_{1}\), \(_{0}\), for each prompt \(\), through \(m\) binary comparisons \(_{j}[\,_{1}_{0}\,]\{0,1\}\) for \(1 j m\), where \(_{j}[\,_{1}_{0}\,]\) indicates whether or not \(_{1}\) is preferred over \(_{0}\) in terms of the \(j\)th safety property. A preference dataset \(\{^{(n)},_{1}^{(n)},_{0}^{(n)},\{_{j}[_{1}^{ (n)}_{0}^{(n)}]\}_{j=1}^{m}\}_{n=1}^{N}\) with safety labels are collected. Then, one can learn safety models \(\{g_{j}:\}_{j=1}^{m}\) associated with safety properties from the annotated data via, _e.g._, parametrized MLEs, as in the second - reward modeling - step of RLHF. Once the safety models are obtained, one can tune the LM via a constrained alignment problem,

\[}\ \ _{ \,\,}\,\,_{\,\,(\,|\,)} [\,r(,)\,]-\,D_{}((\,|\,)\,\|\,_{ }(\,|\,))\,\] (CA) \[\ \ _{\,\,}\, \,_{\,\,(\,|\,)}[\,g_{j}(,)\,]-_{\,\,_{}(\,|\,)}[\,g_{j}( ,)\,]\,\ \ b_{j},\ \ 1 j m,\]

where the objective is given by (A), and the constraints require that the aligned LM outperforms the reference LM \(_{}\) in each safety property by a margin of \(b_{j}\). Denote the solution of (CA) by \(^{}\).

One can recast the form of a constraint in (CA) as \(_{\,\,,\,\,\,(\,|\,)}[ \,g_{j}(,)\,]_{j}\) with an absolute threshold \(_{j}\) as in [12; 36; 23]. The choice of \(b_{j}=_{j}-_{\,\,,\,\,\,_{ }(\,|\,)}[\,g_{j}(,)\,]\) recovers our margin-based form. Despite being mathematically equivalent, the margin-based form is more useful for our purposes. First, setting margins explicitly enforces explicit safety improvements. Second, margin-based constraints are invariant to \(\)-dependent shifts in safety models, _i.e._, \(_{j}(,)=g_{j}(,)+f()\), which can exist in equivalent preference models; see [29; Page 5] and Sec. 3.2 for discussion. Moreover, margin constraints also facilitate pure preference-based safe alignment without explicitly resorting to any pre-trained reward and safety models, which is intractable when using the threshold-based formulation [12; 23]; see the design of PeCAN in Sec. 4.2.

Viewing (CA) as a special case of constrained optimization , applying Lagrangian-based primal-dual methods seems natural. Unfortunately, standard primal-dual policy iterations are not necessarily convergent , despite the convexity of problem (CA); see, _e.g._, the last-iterate divergence of gradient-descent in minimax optimization . Moreover, fitting an LM along for varying dual variables is expensive [36; 23]. To address these issues, we exploit the optimization properties (_e.g._, non-iterative, one-shot) methods in this paper.

**Notation.** We use shorthand \(_{}[\,r\,]\) for \(_{\,\,,\,\,(\,|\,)}[\,r( ,)\,]\), and \(D_{}(\,\|\,_{})\) for \(_{\,\,}[\,D_{}((\,|\,)\, \|\,_{}(\,|\,))\,]\), respectively. Denote \(h_{j}(,):=g_{j}(,)-_{_{}}[\,g_{j }\,]-b_{j}\), \(:=[\,g_{1},,g_{m}\,]^{}\), and \(:=[\,h_{1},,h_{m}\,]^{}\). We abbreviate the objective of (CA) as \(_{}[\,r\,]- D_{}(\,\|\,_{})\), and the constraints as \(_{}[\,h\,] 0\), where the \(j\)th constraint is \(_{}[\,h_{j}\,] 0\).

## 3 Dualization of constrained alignment

In this section, we propose a dualization perspective for the problem (CA), building on which we further propose a two-stage approach for constrained LM alignment.

### Optimal dualization

The problem (CA) is associated with the Lagrangian \(L(,):=_{}[\,r+,\,\,]-  D_{}(\,\|\,_{})\), where \(_{+}^{m}\) is the vector of \(m\) non-negative Lagrangian multipliers. One can equivalently express (CA) as a maximin optimization problem: \(}_{\,\,_{+}^{m }}L(,)\). As is well known in duality theory [6; Chapter 5], given an arbitrarily fixed \(\), the induced unconstrained problem \(}_{\,\,}L(,)\) does not necessarily find the optimal policy \(^{}\) for the problem (CA). Instead, we next exploit the structural properties of the problem (CA) to show that the constrained problem can be reduced to an unconstrained problem when \(\) is optimal.

In this paper, we assume that (CA) is strictly feasible, so that the constraints are of practical interest.

**Assumption 1** (Feasibility).: _There exists a policy \(\) such that \(_{}[\,h_{j}\,]>0\) for all \(1 j m\)._We define the dual function \(D\): \(^{m}\) of problem (CA) by \(D():=_{\,\,}L(,)\) for \(^{m}\) and an optimal dual variable as \(^{}*{argmin}_{\,\,^{m }_{+}}D()\).

**Lemma 1** (Strong duality ).: _Let Assumption 1 hold. Then, there is no duality gap for the problem (CA), i.e., \(L(^{},0)=D(^{})\). Moreover, \((^{},^{})\) is a saddle point of the Lagrangian \(L\),_

\[*{maximize}_{\,\,}\,\,*{minimize}_{\,\,^{m}_{+}}\,\,L(,)\,\,=\,\,L(^{},^{})\,\,=\,\,*{minimize}_{\, \,^{m}_{+}}\,\,*{maximize}_{\,\,}\,\,L(, ).\]

Perhaps surprisingly, an application of Donsker and Varadhan's variational formula  yields a closed-form expression for the dual function; see Appendix A for proof.

**Lemma 2** (Explicit dual function).: _For any \(^{m}\), the dual function \(D\) takes the form_

\[D()\,\,=\,\,\,_{\,\,}\, _{\,\,_{}(\,|\,)}\, ,)\,+\,,(,)}{}\,\,.\]

_Moreover, the dual function is the Lagrangian \(L\) evaluated at \(\) and the policy \(_{}\) such that_

\[_{}(\,|\,)\,\,=\,\,}(\,|\,)}{Z_{}()},)\,+ \,,(,)}{}\,,\,\, \,(,),\]

_where \(Z_{}()\) is a normalization constant so that \(_{}(\,|\,)\) is a probability distribution on \(\) for all \(\)._

Denote \(G:=_{(,)\,\,}\|\|<\). We next show that the dual function \(D\) satisfies several useful properties; see Appendix B for proof.

**Theorem 1** (Properties of the dual function).: _The dual function \(D\) satisfies four properties below:_

1. _The dual function_ \(D\) _is convex in_ \(^{m}\)_._
2. _The dual function_ \(D\) _admits a second-order approximation,_ \[D(^{})\,\,\,\,D()+_{_{}}[\,\,],^{}- +(^{}-)^{}_{\,\,}[\,*{Cov}_{\,\,_{}( \,|\,)}[\,\,]\,](^{}-),\] _for any_ \(^{}\)_,_ \(^{m}\)_, where the error is of order_ \((\|^{}-\|^{3})\)_._
3. _Let Assumption_ 1 _hold and the covariance_ \(_{\,\,}[\,*{Cov}_{\,\,^ {}(\,|\,)}[\,(,)\,]\,]\) _be positive definite. Then, the saddle point_ \((^{},^{})\) _is unique. Moreover, the positive definiteness holds if and only if constraints are linear independent,_ i.e._, there is no non-zero vector_ \(^{m}\) _such that_ \(,(,)=f()\) _for a function_ \(f\)_:_ \(\)_, almost surely._
4. _Let the conditions in_ (iii) _hold. Then, the dual function_ \(D\) _is_ \((G/)\)_-smooth and locally strongly convex at the optimal dual variable_ \(^{}\)_,_ i.e._, there is a ball_ \(B_{}(^{})\) _centered at_ \(^{}\) _with radius_ \(>0\)_, and some_ \(0<_{} G\)_,_ \[}{}I_{m}\,\,\,\,^{2}D(),\,\, \, B_{}(^{})\,\,\,\,\,\,^{2}D()\,\,\,\,I_{m},\,\,\,^{m}.\] (1)

**Remark 1** (Practical validity of conditions).: _We remark that the conditions of Theorem 1 are mild and of practical interest, as shown in Figure 1. In this singly-constrained case (i.e., \(=g\)), we take the beaver-7b-v1.0-cost model  (with the sign of the output flipped) as the ground truth safety model \(g\). In Figure 1 (Left and Middle), we observe that the output of the safety model appears to be bounded, and the dual function \(D\) appears to enjoy local strong convexity._

Due to the smoothness and local strong convexity, we can minimize the dual function \(D\) efficiently using standard optimizers such as Projected Gradient Descent (PGD) in Theorem 2.

**Theorem 2**.: _Let the conditions in (iii) of Theorem 1 hold. Then, PGD, initialized at \(^{(0)}\), achieves \(\|^{(t)}-^{}\|\), in \(t=(}(((),0)+^{(0)}-^{}\|^{2}}{ ^{2}}))\) steps._

See the proof of Theorem 2 in Appendix C. Figure 1 shows the efficiency of dual optimization in a practical example using PGD for several constraint margins, demonstrating geometric convergence.

### Can: Finding the optimal policy in two stages

As discussed above, it is feasible to approximately find the optimal dual variable \(^{}\) by minimizing the dual function \(D\). On the other hand, the optimal policy \(^{}\) of (CA) maximizes the Lagrangian \(L(,)\) at the dual variable \(^{}\). Inspired by these observations, we propose Constrained Alignment via dualizationN (CAN), a two-stage strategy for constrained LM alignment, consisting of

\[\ \ \ ^{}\ =\ \,\,_{+}^{m}}{}\ D(),\] \[\ \ \ ^{}\ =\ }\ L(,^{}).\]

Advantages of Can.CAN enjoys substantial practical benefits. The first stage is a _convex_ optimization problem with favorable properties (_e.g._, smoothness and local strong convexity in Theorem 1). Also, the number of optimization variables is equal to the number of constraints. Further, to increase efficiency, one can collect an offline dataset of reward and safety scores and reuse it for dual optimization for varying hyper-parameters (_e.g._, regularization \(\) and margins \(\{b_{j}\}_{j=\,1}^{m}\)). Then, once \(^{}\) is well approximated, the second stage is an _unconstrained alignment_ task with the modified reward \(r+^{},\). Hence, CAN addresses constrained alignment with a mechanism (and empirically also at a cost) comparable to that of unconstrained alignment [29; 37].

Comparison with existing works.In addition to considering multiple margin-based constraints instead of one threshold-based constraint, our approach also differs from existing works in algorithmic design [12; 23; 36]. For example,  uses dual descent to update the dual variables with gradients evaluated from primal policy optimization. Namely, they iterate, with a learning rate \(>0\),

\[_{} \ }\ _{}[\,r+\,h_{1}\,]\,-\,\,D_{ }(\,\|\,_{}),\] (2) \[ \ \,-\,\,_{\,_{}}\,[\,h_{1}\,].\] (3)

Here \(_{_{}}[\,h_{1}\,]\) equals the dual gradient \( D()\). However, evaluating dual gradients (and the required \(_{}\)) by solving the induced policy optimization problem (2) is much more expensive (memory- and computation-wise) than directly estimating \( D()\) with offline data, as detailed in Appendix E. Moreover, the \(\)-update (3) overlooks the projection to \(_{+}\), optimizing \(D\) over \(\), and thus may not solve the original constrained problem. Similarly, a parametrized policy-gradient-ascent step is used in  to replace (2), which can result in poor convergence due to inaccurate dual gradients. Moreover, the dual \(\) is set conservatively in , which again may not solve the original problem.

Stability analysis.In practice, we may only have access to proxy reward and safety estimates \(\) and \(\{_{j}\}_{j=\,1}^{m}\), which approximate the ground-truth models \(r\) and \(\{g_{j}\}_{j=\,1}^{m}\). To quantify the level of estimation error, we introduce a suitable notion of accuracy.

**Definition 1** (\((,_{r},\{_{g_{j}}\}_{j=\,1}^{m}\))-model-accuracy).: _We say that proxy reward and safety models \(\) and \(\{_{j}\}_{j=\,1}^{m}\) are \((,_{r},\{_{g_{j}}\}_{j=\,1}^{m})\)-accurate, if with probability at least \(1-\), it holds that_

\[_{\,\,,\,_{1},_{0} \,\,_{}(\,\,|\,)}\,|r(,_{1})- (,_{1})-r(,_{0})+(, _{0})|^{2}\,\ \ _{r}^{2},\] \[_{\,\,,\,_{1},_{0}\, \,_{}(\,\,|\,)}\,|g_{j}(,_{1} )-_{j}(,_{1})-g_{j}(,_{0})+_{j}( ,_{0})|^{2}\,\ \ _{g_{j}}^{2},\,\,1 j m.\]

Figure 1: An illustration of the dual properties with 128 responses drawn from the Alpaca-7b-reproduced model operating over 1000 prompts from the PKU-SafeRLHF-30K dataset. (Left) The empirical distribution of the safety scores. (Middle) The dual landscape with respect to varying margin \(b\). (Right) The convergence of PGD with a constant step size of one and initialization \(^{(0)}=1\).

```
1:Input: Reference LM \(_{}\), prompt dataset \(\), reward model \(r\) and safety models \(\{g_{j}\}_{j\,=\,1}^{m}\), regularization \(\) for KL penalty, margins \(\{b_{j}\}_{j\,=\,1}^{m}\).
2: Collect offline data of \((r(,),(,))\)-tuples with \((,)\) drawn from \(_{}\).
3: Estimate \(_{_{}}[\,\,]\) and \((,)=(,)-_{_{}}[\, \,]-\) with the offline data.
4: Optimize dual with the offline data: \[^{}\ =\ *{argmin}_{_{ +}^{m}}\ _{\,\,}[\,_{_{ }(\,\,|\,)}\,[\,(,)\,+ \,,(,)}{})\,]\, ].\]
5: Update LM with pseudo-preference constructed with \(r_{^{}}:=r+^{},\): \[^{}\ =\ *{argmin}_{\,\,\,}\ -_{(,_{+},_{-})\,\, _{^{}}^{}}[\,( (_{+}\,|\,)}{_{}(_{+}\,|\, )}-(_{-}\,|\,)}{_{ }(_{-}\,|\,)}\,)\,].\]

 ```

**Algorithm 1** MoCAN: Model-based Constrained Alignment via dualizatioN

Above, \(_{1}\), \(_{0}_{}(\,|\,)\) denote two independent LM responses. Notably, \((,_{r},\{_{g_{j}}\}_{j\,=\,1}^{m})\)-accuracy allows proxy models to differ from their ground truth by an arbitrary shift depending only on \(\). In particular, the maximum likelihood model estimates are \((,_{r},\{_{g_{j}}\}_{j\,=\,1}^{m})\)-accurate under certain conditions, as proved by . We next show that CAN is robust to proxy reward and safety models as long as they are \((,_{r},\{_{g_{j}}\}_{j\,=\,1}^{m})\)-accurate, with the proof deferred to Appendix D.

**Theorem 3**.: _If we use \((,_{r},\{_{g_{j}}\}_{j\,=\,1}^{m})\)-accurate model estimates \(\) and \(\{_{j}\}_{j\,=\,1}^{m}\) admitting the strict feasibility in CAN and \(^{}\) is feasible under the model estimates, then with probability at least \(1-\), the resulting policy \(^{}\) satisfies_

\[_{^{}}[\,r\,]\,-\,\,D_{}(^{}\,\|\,_{})\ \ _{^{}}[\,r\,]\,-\,\,D_{}(^{}\,\|\,_{})\,-\,(_{r}),\] (Objective) \[_{^{}}[\,g_{j}\,]\,-\,_{ _{}}[\,g_{j}\,]\ \ b_{j}\,-\,(_{g_{j}}), \,1 j m.\] (Constraints)

Beyond constrained KL-regularized alignment.We remark that the two-stage strategy is applicable to more general regularized alignment problems with an \(f\)-divergence penalty \(D_{f}\):

\[*{maximize}_{\,\,}\ *{minimize}_{\,\,}\ \{L(,)\ :=\ _{}[\,r(,;)\,]\,-\,\,D_{f}(\,\|\, _{})\}\,,\] (4)

where \(\{r(,;):\}\) is family of reward models indexed by \(\). Under mild conditions (_e.g._, the existence of saddle points), one can solve (4) by exchanging the min and max operators, first solving

\[^{}\ =\ *{argmin}_{\,\,}\ _{\,\,}[\,_{_{}(\,|\, )}(r(,;)/)\,],\]

where \(_{_{}(\,|\,)}\) is a convex functional detailed in Appendix A, and finally solving the simplified task: \(*{maximize}_{\,\,}\,L(,^{})\). Notably, the MaxMin RLHF problem proposed in  falls into (4), and thus can be efficiently addressed with our two-stage strategy; see Appendix I for discussion.

## 4 Practical implementations of CAN

We present two practical implementations of CAN that target model-based and preference-based scenarios, respectively. With a slight abuse of notation, we use \(^{}\) to denote its approximation obtained by dual optimization. We use the terms dataset and data distribution interchangeably below.

### MoCAN: Model-based CAN

In model-based scenarios, we assume that we have the approximated reward and safety models \(r\) and \(\), as well as a prompt dataset \(\). Following CAN, we propose Model-based Constrained Alignment via dualizatioN (MoCAN) to solve (CA), as detailed in Algorithm 1.

MoCAN has two stages: dual optimization and policy update. In the dual optimization stage, we first collect an offline dataset with prompts from \(\), responses drawn from \(_{}\), and scores of the reward and safety models. Using these, we can readily estimate the term \([\,_{_{}}[\,g_{1}\,]\,,,_{_{ }}[\,g_{m}\,]\,]^{}:=_{_{}}[\, \,]^{m}\) that appears in the constraints of (CA). We then approximate \(^{}\) by optimizing the dual function \(D\) with gradient estimates evaluated over the offline data; see Appendix E for details.

In the policy update stage, we aim to align the LM using the optimal reward \(r_{^{}}:=r+^{},\) determined by \(^{}\). Here, \(r_{^{}}\) differs from \(r+^{},\) by a constant, which does not affect unconstrained alignment. In principle, this can be accomplished by RL algorithms (_i.e._, PPO ). However, RL algorithms are known to suffer from training instability and sensitivity to hyper-parameters [14; 31].

Fortunately, recent advances in Direct Preference Optimization (DPO) [29; 4] allow us to leverage the approximate equivalence between RL and supervised training with carefully defined loss functions. Inspired by these developments, MoCAN trains the LM supervised with _pseudo-preferences_, constructed with the modified reward \(r_{^{}}\). Specifically, we draw \((,_{1},_{0})\)-tuples with the prompt \(\) and two responses \(_{1}\), \(_{0}\) sampled independently from \(^{}(\,|\,)\). Here, \(^{}\) can be \(_{}\) or another latent policy associated with a existing dataset of \((,_{1},_{0})\)-tuples. Then we construct the pseudo-preferences \(_{r_{^{}}}[\,_{1}_{0}\,]\{0,1\}\) for the two responses by randomly sampling from the synthetic Bradley-Terry model,

\[(_{r_{^{}}}[\,_{1} _{0}\,]=1\,|\,)\ =\ (r_{^{}}(,_{1})-r_{^{ }}(,_{0})),\] (5)

where \(\) is the sigmoid function. We then relabel the two responses as \(_{+}:=_{_{r_{^{}}}[\,_{1} {y}_{0}]}\) and \(_{-}:=_{-_{r_{^{}}}[\,_ {1}_{0}]}\). We denote the dataset of the ranked tuples \((,_{+},_{-})\) by \(_{r_{^{}}}^{}\).

After obtaining the pseudo-preference dataset \(_{r_{^{}}}^{}\), we formulate the following negative-log-likelihood objective analogous to DPO , fitting a parametrized LM \(_{}\) via

\[*{minimize}_{\,\,}\ -_{(,_{+},_{-}) \,\,_{^{}}^{}}[\,( (_{+}\,|\,)}{_{}(_{ -}\,|\,)}\,-\,(_{-}\,|\,)}{_{ }(_{-}\,|\,)})\,].\] (6)

Here, \(\) denotes the weights of an LM with a given architecture, and \(\) is the set of possible weights. If size of the pseudo-preference dataset \(_{r_{^{}}}^{}\) is sufficiently large and \(\{_{}:\}\) covers all policies, then the optimal LM to (6) approximates the optimal policy \(^{}\) that maximizes \(L(,^{})\)[4; Proposition 4]; see Appendix F for more details. Pseudo-preferences are also used in , but are expensive to use due to the alternatively updated primal and dual variables.

### PeCAN: Preference-based CAN

Often, the reward and safety models \(r\) and \(\) and their proxies are not off-the-shelf, motivating model-free scenarios. To this end, we devise an alternate approach termed Preference-based Constrained Alignment via DualizatioN (PeCAN), detailed in Algorithm 2.

PeCAN leverages a human-annotated preference dataset \(_{}\) in format of \((,_{1},_{0},_{r}[\,_{1}_{0}\,], \{_{g_{j}}[\,_{1}_{0}\,]\}_{j=\,1}^{m})\)-tuples, where \(_{r}\) and the \(_{g_{j}}\)s are binary indicators that compare \(_{1}\) and \(_{0}\) in terms of the associated utility and safety properties. We let \(\) be the prompt dataset of \(\) values induced by \(_{}\), and assume the Bradley-Terry model, _i.e._, for all \(\),

\[(_{r}[\,_{1}_{0}\,]=1\,| \,)\ =\ (r(,_{1})-r(,_{0})),\] \[(_{g_{j}}[\,_{1}_{0}\,]=1 \,|\,)\ =\ (g_{j}(,_{1})-g_{j}(,_{0})), \,1 j m.\]Unlike MoCAN, PeCAN leverages the reward and safety models implicitly via \(_{}\) as follows.

**Pre-alignment.** We first obtain unconstrained pre-aligned LMs \(_{_{r}}\) and \(\{_{_{g_{j}}}\}_{j=1}^{m}\) that fit preference annotations \(_{r}\) and \(\{_{g_{j}}\}_{j=1}^{m}\) respectively, with the same KL regularization term \(\). This can be done by running DPO  over the dataset \(_{}\). If these LMs maximize the associated policy objectives \(_{}[\,r\,]- D_{}(\,\|\,_{})\) and \(_{}[g_{j}]- D_{}(\,\|\,_{})\), for all \(,\) and \(1 j m\), we have

\[r(,) =\,}(\,|\,)}{_{ }(\,|\,)}\,+\, Z_{r}()\,\,\, \,\,g_{j}(,)\,=\,}}(\,|\, )}{_{}(\,|\,)}\,+\, Z_{g_{j}}(),\] (7)

where \(Z_{r}()\) and \(Z_{g_{j}}()\) are normalization constants [29, Equation (5)] for all \(\). Here, we use the same KL regularization parameter \(\) in pre-alignment for simplicity. PeCAN also allows _distinct_ KL regularization \(_{r}\) and \(\{_{g_{j}}\}_{j=1}^{m}\) in pre-alignment by adjusting lines 5 and 6 accordingly. This enables using existing aligned LMs whose regularization parameters are known; see Appendix H.

**Data collection and divergence estimation.** We then collect offline data comprised of \((_{}(,),\)\(_{_{r}}(,),_{_{g}}(,))\)-tuples with prompts \(\) drawn from \(\) and responses \(_{}(\,|\,)\). With this data, the KL divergences \([\,D_{}(_{}\,\|\,_{_{g_{1}}}),,D_{ }(_{}\,\|\,_{_{g_{m}}})\,]=: ^{m}\) can be readily estimated. The collected data is next reused to optimize the dual function \(D\),

**Dual optimization.** This step aims to obtain \(^{}\) by minimizing the dual function \(D\),

where \(:=[\,b_{1},,b_{m}\,]^{}\) are the margins and \([\,}}(\,|\,)}{_{}( \,|\,)},,}}(\,|\,)}{ _{}(\,|\,)}]^{}=: }(\,|\,)}{_{}(\,|\,)}\). The equivalence is based on (7); see Appendix G for detailed derivation.

**Policy update.** With the approximation of the optimal dual \(^{}\) from the last step, we finally update the LM policy to maximize the optimal reward \(r_{^{}}:=r+^{},\). This is accomplished by another pseudo-preference optimization, where the pseudo-preference is constructed, for the off-the-shelf \(_{0}\) and \(_{1}\) provided by \(_{}\), similarly via (5) but with \(r_{^{}}\) replaced by \(s_{^{}}(,):=(}( \,|\,)}{_{}(\,|\,)}+^{},}(\,|\,)}{_{}(\,|\,)})\). Indeed, it suffices to notice that with (7), for all \(,_{0},_{1}\),

\[r_{^{}}(,_{1})-r_{^{ }}(,_{0}) =\,r(,_{1})-r(,_{0})+^{ },(,_{1})-(,_{0})\] \[=\,}(_{1}\,|\,)_{ }(_{0}\,|\,)}{_{}(_{1}\,|\, )_{_{r}}(_{0}\,|\,)}+_{j=1}^{m}_{j}^{ }}}(_{1}\,|\,)_{}( _{0}\,|\,)}{_{}(_{1}\,|\,)_{_{ g_{j}}}(_{0}\,|\,)}\] \[=\,s_{^{}}(,_{1})-s_{^{ }}(,_{0}).\]

## 5 Computational experiments

In this section, we empirically demonstrate the effectiveness and merits of our alignment methods in enhancing both helpfulness and safety. Our experiments aim to address four questions below:

1. In model-based scenarios, do MoCAN-aligned LMs satisfy safety constraints in practice?1

Figure 2: Visualization of MoCAN. (Left) Dual optimization predicts the safety improvement of practically aligned LMs. (Middle & Right) The safety/helpfulness score distribution before and after alignment (\(=0.75\)).

2. How does dual optimization navigate the trade-off between helpfulness and safety?
3. How does the preference-based PeCAN compare to the model-based MoCAN?
4. How much offline data does the dual optimization require?

### Experiment setups

We implement MoCAN and PeCAN to align the _Alpaca-7b-reproduced_ model , which can generate both benign and unsafe responses. We use the _beaver-7b-v1.0-reward_ model and the _beaver-7b-v1.0-cost_ model  (with the sign of outputs flipped) as surrogates for the ground truth reward and safety models in MoCAN. We consider _one_ constraint in experiments, as for instance in . More details about our implementation, including the computational requirement and scalability, are described in Appendix J. The source code is available here.2

Dataset.We use the _PKU-SafeRLHF-30K_ preference dataset , which contains approximately 27,000 training and 3,000 testing expert evaluations. Each entry in this dataset includes a pair of responses (_i.e._, \(_{0}\) and \(_{1}\)) to a prompt (_i.e._, \(\)), along with indicators of which response is more preferred in safety and helpfulness by human annotators, respectively.

Baselines.We set the Alpaca-7b-reproduced model , obtained via supervised fine-tuning, as our reference LM, denoted by SFT for brevity. We consider baselines built on the SFT model: helpfulness-only and safety-only LMs trained via DPO  (denoted by \(_{,}\) and \(_{,}\) for regularization \(\), respectively), and _beaver-7b-v1.0_ LM (denoted by Safe-RLHF) trained via primal-dual PPO .

Evaluation.We conduct both model- and GPT-based evaluations for both helpfulness and safety. In model-based evaluation, we compute the average helpfulness and safety scores upon two independently generated responses of a MoCAN-aligned LM for each unique prompt in the PKU-SafeRLHF-30K _test_ set, by using the proxy reward and safety models. For the GPT-based evaluation, we set the _gpt-4-turbo_ model as the evaluator, prompted with the template presented in Appendix K. Following , the evaluator conducts a pairwise comparison of the responses generated by an aligned LM to those by the SFT model, using the prompts provided by  for safety evaluation, and the prompts from the _Alpaca-eval_ dataset  associated with the "helpful_base" category for helpfulness evaluation. We then separately calculate the pairwise win rate of an LM over the SFT model in terms of helpfulness and safety.

### Experimental results

Constraint satisfaction.We compare the safety improvements predicted with offline dual optimization in MoCAN to empirical LM training. We set the grid \([\,-1.4,\,0.1,\,1.2,\,2.8,\,3.5,\,4.2,\,4.5,\,5.4\,]\) for the safety margin \(b\) in (CA) and find the associated optimal dual variables over the offline data of

Figure 3: Trade-off in improving helpfulness and safety of aligned LMs. (Left) Improvement of helpfulness score versus safety score of MoCAN-aligned LMs under model-based evaluation. (Middle & Right) Helpfulness win rate versus safety win rate of MoCAN-aligned LMs and PeCAN-aligned LMs with \(=0.1\), respectively, under GPT-based evaluation.

1000 prompts\(\)128 responses per prompt as described in Figure 1. The dual optimization procedure predicts the expected safety improvement as a function of the \(\)-value used in the policy update, plotted as the red dashed curve in Figure 2 (Left). We also use these \(\)-values to fine-tune the reference LM via pseudo-preference optimization. The evaluated safety improvements of the aligned LMs are depicted in Figure 2 (Left) with \(95\%\) confidence intervals obtained via bootstrapping 1000 times. The results show that _our method predicts the safety improvement of practically fine-tuned LMs well_, and the safety constraints are nearly satisfied as expected. We detail the predicted safety improvement and confidence intervals for empirical safety improvement in Table 4. Figure 2 (Middle & Right) shows a visible _distributional improvement of both the safety and helpfulness scores_ using MoCAN alignment. The score distributions associated with other \(\) values are in Figure 5.

Empirical Pareto trade-off between helpfulness and safety.We consider both model- and GPT-based evaluations for MoCAN-aligned LMs, and only GPT-based evaluations for PeCAN-aligned LMs. In Figure 3 (Left), we observe a clear _trade-off between helpfulness and safety improvements_ brought by MoCAN, measured by the proxy reward and safety models: LMs aligned with a large dual variable \(\) tend to achieve higher safety but lower helpfulness. There is a similar phenomenon in the GPT-based evaluation for both MoCAN and PeCAN in Figure 3 (Middle & Right). In particular, as seen in the middle plot, MoCAN _achieves an empirically optimal Pareto tradeoff curve_, among all previous methods considered, including DPO. For any given helpfulness level, MoCAN empirically achieves the best safety.

MoCAN versus PeCAN.While targeting different scenarios, the performance of MoCAN and PeCAN can be compared under the GPT-based evaluation, as shown in Figure 3 (Middle & Right). We find that PeCAN slightly underperforms MoCAN. This is mainly due to imperfect pre-alignment, such that the log-probabilities \((_{_{e}}/_{})\) (or \((_{_{g}}/_{})\)) are inaccurate for indicating the ground-truth helpfulness and safety preferences, unlike assumed in (7). See Appendix M for more details.

Influence of offline data.We plot the curves of the empirically optimal dual variables for a varying number of prompts (with 128 responses per prompt) and a varying number of responses per prompt (with 1000 prompts), as shown in Figure 4. We find that _the empirically optimal dual variable stabilizes quickly_ with a moderate size of prompts (_e.g._, 600) for reasonably large constraint margins. On the other hand, it appears to be conservative (_i.e._, larger than the ground-truth counterpart) when the number of responses collected per prompt is small (_e.g._, below 100), particularly for large margins (_i.e._, stringent safety constraints). Thus, when using our dualized methods, one should be more concerned about the number of responses than the number of prompts.

## 6 Concluding remarks

We have studied the safety-constrained alignment problem from the dualization perspective and reduced constrained alignment to an equivalent unconstrained alignment problem via optimal dualization. Based on this observation, we propose a two-stage training strategy: first, compute the optimal dual variables by optimizing an explicit dual function; and second, use the optimal dual variables to reduce the constrained alignment problem to an unconstrained alignment problem. We instantiate this training strategy to develop two practical algorithms (for model-based and preference-based scenarios) using pseudo-preference, demonstrating their effectiveness and merits in experiments.

This work stimulates several interesting future directions. Given the use of the Bradley-Terry preference setup, it is important to extend our two-stage strategy to accommodate more general preference setups. Since reward and safety models are imperfect in practice, we are also interested in studying robust constrained alignment problems. Furthermore, we aim to experiment with multiple constraints as relevant datasets become available.

Figure 4: Optimal dual variables as a function of the number of prompts (Left) and number of responses per prompt (Right).