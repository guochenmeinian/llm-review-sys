# Tell What You Hear From What You See -

Video to Audio Generation Through Text

 Xiulong Liu

Department of Electrical & Computer Engineering, University of Washington, Seattle, USA.

Kun Su

Department of Applied Mathematics, University of Washington, Seattle, USA

Eli Shlizerman

Corresponding author: shlizee@uw.edu

###### Abstract

The content of visual and audio scenes is multi-faceted such that a video stream can be paired with various audio streams and vice-versa. Thereby, in video-to-audio generation task, it is imperative to introduce steering approaches for controlling the generated audio. While Video-to-Audio generation is a well-established generative task, existing methods lack such controllability. In this work, we propose _VATT_, a multi-modal generative framework that takes a video and an optional text prompt as input, and generates audio and optional textual description (caption) of the audio. Such a framework has two unique advantages: i) Video-to-Audio generation process can be refined and controlled via text which complements the context of the visual information, and ii) The model can suggest what audio to generate for the video by generating audio captions. VATT consists of two key modules: _VATT Converter_, which is an LLM that has been fine-tuned for instructions and includes a projection layer that maps video features to the LLM vector space, and _VATT Audio_, a bi-directional transformer that generates audio tokens from visual frames and from optional text prompt using iterative parallel decoding. The audio tokens and the text prompt are used by a pretrained neural codec to convert them into a waveform. Our experiments show that when VATT is compared to existing video-to-audio generation methods in objective metrics, such as VGGSound audio-visual dataset, it achieves competitive performance when the audio caption is not provided. When the audio caption is provided as a prompt, VATT achieves even more refined performance (with lowest KLD score of 1.41). Furthermore, subjective studies asking participants to choose the most compatible generated audio for a given silent video, show that VATT Audio has been chosen on average as a preferred generated audio than the audio generated by existing methods. VATT enables controllable video-to-audio generation through text as well as suggesting text prompts for videos through audio captions, unlocking novel applications such as text-guided video-to-audio generation and video-to-audio captioning.

## 1 Introduction

The combination of human perception and cognition represents a "multi-modal" way of processing and interpreting scenes. For example, when we are presented with a silent video of a fountain show attended by a crowd of people gathered around the spectacle our interpretation might translate the visual scene into an auditory experience, where the visuals are semantically processed and transformed into a corresponding sound narrative in our mind. Thus, we may associate audio that mixes sounds of splashing water accompanied by people talking and laughing with possibly background music in sync with the fountain.

As generative AI continues to progress, the incorporation of the aforementioned aspects into generative platforms presents itself as the future desirable capability. In particular, the goal of an ideal video-to-audio generative model would be to generate sounds that seamlessly match the video temporally and fully capture the semantics. Moreover, it is desirable to control such a generation process towards the themes and sounds that match user preference. Recent state-of-the-art approaches have adopted two types of generative modeling techniques: auto-regressive token-based modeling and diffusion-based modeling. These methods enable end-to-end video-to-audio generation and are applicable across a wide variety of video and audio categories. However, while these methods are capable of capturing the general semantics of sound sources in videos, they often overlook the subtleties of the context. For example, in a video depicting two cats in a territorial dispute, the model might produce a calm, amiable meowing sound, which contradicts the contentious nature of the scene. This discrepancy mainly stems from the limitations of the vision encoder, which struggles to distinguish between varying sound properties emitted by identical sound sources across different contexts, due to an incomplete understanding of the entire scene. Second, these methods lack controllability since the generation is conditioned only on visual frames, without taking into account the context and interpretation of the sounds. While text-to-audio models could explicitly control the context of the sounds, such models are based on text only without incorporating the rich and dynamic context of visuals, which could significantly inform video and audio alignment. Indeed, text only generative outcomes often result in unmatched audio with the visual (e.g., temporal misalignment or semantic loss).

To solve the above challenges, we propose a novel framework, Video-to-Audio Through Text (VATT), that is able to generate audio from both video frames and an optional text prompt describing the expected sounds. VATT consists of two modeling stages: i) Video-to-Caption stage, which converts video features into an audio caption through a pretrained large language model (LLM) with a learnable projection layer. Through this cross-modal conversion, visual features that are relevant to audio concepts are extracted. These features are closely connected to audio-related tasks such as audio captioning and audio generation. ii) Video + Text to Audio stage, that generates audio conditioned on the hidden states extracted from the LLM in the prior modeling stage. At its core, the proposed model in this stage is a bi-directional transformer decoder that generates audio using a token-based representation similar to . To obtain the conditioning on the hidden states of the preceding component, the projected video features along with the optional text prompts are concatenated together and fed into the LLM in stage i), with the hidden states from the last layer extracted and attached to the audio tokens for the decoder. The decoder is trained using masked token modeling, where the objective is to predict masked audio tokens from unmasked ones at varying masking ratios. During inference, starting from all tokens being masked, an efficient parallel decoding algorithm is implemented which gradually unmasks multiple tokens in parallel based on video and text inputs until a stop condition is met. Finally, the generated tokens are converted into audio waveforms through a neural audio codec decoder.

Figure 1: VATT is a flexible audio generative model capable of generating audio in two modes: i) When a silent video is the sole input, the model generates the audio along with a caption describing the possible audio that could match the video. ii) When in addition to the video, a text prompt is provided, the model generates audio aligned with both the video and the given text prompt.

We perform experiments with the proposed framework on existing large-scale audio-visual datasets such as VGGSound  and Audioset-2M . To facilitate training and evaluation with text, we created "V2A Instruction", a large-scale synthetic audio captions corpus, by prompting LTU-13B, an existing Audio LLM , to generate audio descriptions for both datasets. Our experiments demonstrate that the proposed model and its training method achieve competitive performance in comparison to previous video-to-audio methods on both objective and subjective metrics. Furthermore, it is designed to enable a controllable generation that adheres to both the video inputs and the text prompts. Indeed when a text prompt is provided, our experiments show significant improvement in audio metrics that measure the match of the generated sounds to the video. In addition, when the text prompt is not provided, our method can generate reasonable audio captions, which can be utilized for a potential description of the video or classification of sounds for a given video. These capabilities hence make VATT a multifaceted single model able to perform both text-guided video-to-audio generation and video-to-audio captioning. To summarize our contributions:

\(\)To our best knowledge, we propose a first-of-its-kind framework that enables both text-guided video-to-audio generation and video-to-audio captioning through the integration of LLM.

\(\)We create a large-scale synthetic audio captions dataset that facilitates text-conditional training and generation.

\(\)Our method achieves state-of-the-art video-to-audio generation performance when compared with existing methods and enables text-controllable generation. In particular, our text-guided model surpasses existing SOTA in terms of KLD score (with lowest KLD score of 1.41) by a significant margin.

\(\)VATT generates audio in an efficient way - an order of magnitude faster than existing methods.

## 2 Related Works

### Visual-to-Audio Generation

Visual-to-Audio Generation task has drawn significant attention since generative frameworks such as diffusion and transformer-based architectures have been developed. Existing Visual-to-Audio generation approaches can be divided into two branches of studies based on audio categories: _visual-to-music_ generation and _visual-to-natural_ sound generation. In visual-to-music generation domain, earlier studies explored Midi or spectrogram generation from human body movements by studying the temporal and semantics alignment [6; 7; 8; 9; 10]. More recently, diffusion-based methods have been proposed to generate music waveforms directly from videos . In visual-to-natural sound generation, earlier efforts pioneered the generation of sounds linked to various objects and materials . Later works proposed an audio generation approach based on SampleRNN [13; 14] that could generate several types of natural sounds from in-the-wild videos. While these approaches showcase promising results, they are often limited to specific audio categories. Neural codec [15; 16; 17; 18] and autoregressive transformer architectures [19; 20] addressed these limitations and as they have evolved, generative models now effectively generalize across a broader range of sounds or music, leveraging compressed latent spaces [21; 22; 23; 24]. Similar advances have been shown with diffusion techniques such as [25; 26]. However, these methods often lack detailed sound control and their inference time turns out to be consuming. Our work aims to address these limitations by introducing a text-guided framework to improve controllability and efficiency in video-to-audio generation. While there are several concurrent works that aim to achieve partially similar goals to our proposed method [27; 28; 29], our work is different since it is designed to achieve these capabilities within a single unified framework.

### Text-to-Audio Generation

As an alternative to the generation of audio from video, text can be used as an input to guide audio generation. When text is the input, audio generation becomes more controllable semantically. Existing approaches such as Make-An-Audio , AudioLDM , AudioLDM-2  and others [33; 34; 35; 33] enable general text-to-audio (or music) generation by adapting latent diffusion techniques, first developed in . Concurrently, methods such as AudioGen , MusicGen , AudioLM , MusicLM , SoundStorm , VampNet  leverage transformer-based architectures and token-based modeling techniques to produce audio tokens, that are then decoded into waveforms usingneural codecs like Encodec  and SoundStream . Notably, SoundStorm and VampNet use an efficient technique known as masked token-based modeling which speeds up generation with parallel unmasking in the decoder. In our work, we consider a similar approach. While these models deliver high-quality audio with strong relevance to the text, they do not necessarily align with visual dynamics when adapted to video-to-audio generation. This is expected since such models have not been trained to attend to visual inputs. Our work addresses this by integrating a pretrained large language model (LLM) as a multi-modal encoder that processes both visual and textual inputs such that the generated audio considers both visual and text information.

### Multi-modal Large Language Models

Multi-modal Large Language Models (MLLMs), have been able to attain significant progress. With the advent of open source, pretrained and instruction-tuned LLMs such as LLama , Alpaca , Vicuna . In particular, when extending these LLMs into MLLMs, a pretrained modality-specific encoder extracts the features and then a projection layer maps these features into vectors of the same dimension as text embeddings of the corresponding LLM. This approach led to developments in visual LLMs [46; 47], audio LLMs [5; 48], audio-visual LLMs  and showed improvement in multi-modal understanding tasks such as captioning  and question-answering [51; 52]. Recent efforts have also focused on tasks such as multi-modal retrieval , multi-modal embodied navigation [54; 55], leveraging LLM's strong reasoning capabilities to interpret or improve the results. In terms of generation, several works [56; 57] aimed at achieving any-to-any modality generation using LLMs as a central medium. While these methods have been successful in general modality-to-modality generation, they do not achieve particular end-to-end video-to-audio generation, with or without text guidance, which is the unique direction our work focuses on.

## 3 Methods

VATT is a flexible vision-to-audio generative framework that can process both visual and textual inputs and generate both audio waveforms and captions of audio. To achieve this, VATT consists of two modeling stages: i) **Video-to-Caption** : This stage utilizes a Large Language Model (LLM) with a learnable projection layer that converts video features into embeddings compatible with the LLM. The model receives an instruction to generate audio captions from video inputs. ii) **Video + Text to Audio**: This stage incorporates an encoder-decoder architecture. The encoder uses the finetuned LLM from Video-to-Caption stage with frozen weights. The decoder is a bi-directional transformer trained to generate audio tokens using masked token modeling techniques in training. The training pipeline of VATT system is shown in Figure 2. During inference, VATT generates audio tokens from video and optional text prompts through iterative parallel decoding. These tokens are then converted into audio waveforms using Encodec .

### Video-to-Caption Stage

**VATT Converter** is designed to integrate visual and textual prompts for audio generation as well as audio captioning. The core component, _VATT Projector_, is an embedding layer that maps video features into the text embedding space of the LLM. Given visual features extracted from frame-level vision encoders \(V_{f}=\{v_{1},v_{2},...,v_{T}\}\), a Linear layer is applied to project each feature from its original dimension \(d_{v}\) to the LLM text embedding dimension \(d_{lm}\), producing a sequence of transformed features \(V_{lm}=V_{f}W_{l}+b_{l}\), where \(W_{l}\) and \(b_{l}\) are learnable parameters of the linear projection.

**V2A Instruction Tuning**: The key functionality of VATT Converter is to extract from visual stream semantic features relevant to audio. Drawn on the success of multi-modal LLMs, such as visual-LLM  and audio-LLM , we employ multi-modal instruction tuning to align the visual inputs of videos with the ground truth audio captioning of the same videos. Given a prompt instruction, \(T_{i}=\{t_{i1},t_{i2},...,t_{iK}\}\), such as "Describe the audio that the video could generate:" and the projected visual features \(V_{lm}\) as inputs, we model conditional distribution of audio descriptions \(T_{a}=\{t_{a1},t_{a2},...,t_{aN}\}\), as \(P_{}(T_{a}|T_{i},V_{lm})\) by fine-tuning an instruction-tuned LLM, e.g., Vicuna-7B . Unlike typical instruction-tuning that maps a signal into textual concepts within the same modality, our method bridges the concepts from visual to audio modality, unifying the representation for text-guided video-to-audio generation task that we describe in section 3.2. For training efficiency,we fine tune the LLM with VATT Projector by integrating LoRA  adaptors while keeping the original LLM weights frozen. We minimize the negative log-likelihood of audio caption tokens conditioned on visual inputs and prompt instruction

\[_{v2t}(} T_{i},V_{lm})=-_{l=1}^{N} log[P_{}(_{al}=t_{al} T_{i},V_{lm})],\] (1)

where \(t_{al}\) is the \(l\)-th text token in the ground truth audio description \(T_{a}\), and \(\) is the set of trainable weights including VATT Projector and LoRA adaptor. Further details of the constructions of text prompts and synthesis of audio captions are described in Section 4 and Appendix C.

### Video + Text to Audio Stage

Once the audio-related visual features are aligned with the text features in the LLM embedding space, the LLM effectively encodes multi-modal information that serves as a representation for text generation and audio generation. Indeed, in the second stage of VATT, there are two generation modes to generate audio: i) When no conditional text prompt is provided, the video features along with a _standard template_ prompt (e.g., "Describe possible audio that the video could infer.") are fed as inputs to VATT Converter. ii) When an audio caption is provided as the text prompt, the video features and the audio caption are fed together into VATT Converter. In such a case, the provided audio caption helps guide the video-to-audio generation process and overrides the need for generated audio caption.

#### 3.2.1 Audio Token Decoder

To generate audio, we design an audio token-based decoder, VATT Audio, conditioned on the encoded features from VATT Converter. In contrast to existing methods, which typically use auto-regressive token modeling  or latent diffusion techniques , we adopt a novel token-based modeling technique based on masking tokens. The method, originally derived in image generation tasks  and recently adapted to text-to-audio generation , is capable of achieving competitive generation quality while improving efficiency through an iterative parallel decoding algorithm during inference.

**Token-based Representation for Audio** To represent audio waveforms using discrete tokens, we adopt a pretrained audio neural codec, Encodec , similarly to FoleyGen . Encodec is a multi-level residual vector-quantized (RVQ) autoencoder trained with waveform reconstruction and

Figure 2: Two stages of _VATT_ system training pipeline: (1) **Video-to-Caption** stage that maps video features into an audio caption through LLM. (2) **Video + Text to Audio** stage that learns to generate audio tokens through masked tokens prediction conditioned on Stage (1) features.

adversarial objectives, capable of high-fidelity reconstruction from compressed tokens. Specifically, Encoder uses \(L=4\) codebooks of tokens to represent the audio. Lower-level codebooks encode coarse semantic information, while higher-level codebooks capture fine-grained details. We adopt an open source Encodec model pretrained using audio waveforms at \(Sr_{w}=16kHz\) sampling rate. The model compresses a waveform into tokens at \(Sr_{t}=50Hz\) sampling rate, leading to \(r_{tw}=}{Sr_{t}}=320\) waveform samples per token. For any waveform \(A_{avav}^{1 T_{w}}\), we extract corresponding audio tokens representation \(A_{tok}^{L T_{c}}\) (\(T_{c}=}{r_{tw}}\)) from Encodec encoder part. Once the model generates \(A_{tok}\), the embedding vectors of \(L\) levels of tokens at each time step are summed up before being sent to Encodec decoder to obtain the waveform.

**Masked Audio Token Generative Modeling** We model the distribution of audio token matrix \(A_{tok}^{L T_{c}}\) by developing a token masking strategy which learns the joint distribution of the audio tokens in full parallelism. This is different than using "delayed patterns" proposed in  which enables parallelism but only on the level of codebook dimension. At each time step of \(A_{tok}\), embedding vectors of \(L\) tokens are summed up to represent audio waveform at the corresponding segment. In order to perform masking operation at any position, we introduce an additional learnable <MASK> token in each codebook. By randomly replacing some of the tokens entries in the \(A_{tok}\) with <MASK> at corresponding codebook we obtain the masked audio token matrix \(A_{tok}^{M}^{L T_{c}}\). We obtain \(E_{a}^{M}^{d_{em} T_{c}}\) by summation of the embedding vectors of each token in \(A_{tok}^{M}\) along the level axis.

Conditional generative modeling is implemented as follows. We extract the hidden states of the last layer \(H_{lm}^{d_{lm} T_{lm}}\) (before the LLM prediction head) from VATT Converter as the conditional inputs into the audio token decoder. We use a linear layer to project \(H_{lm}\) to \(E_{lm}^{d_{em} T_{lm}}\) with same feature dimension as the masked audio embeddings \(E_{a}^{M}\). A straightforward way to model the relationship between \(E_{a}^{M}\) and \(E_{lm}\) is to use an interleaving self-attention and cross-attention block as proposed in Vanilla Transformer architecture . However, we find that such interleaved interaction between audio and multi-modal input condition does not capture the fine-grained correspondence between them. Therefore, we propose to use a bi-directional self-attention architecture to fuse the features.

Specifically, we concatenate \(E_{lm}\) with \(E_{a}^{M}\) along the temporal axis to obtain the fused features \(E_{mm}=Concat([E_{lm},E_{a}^{M}])\). The decoder consists of \(L_{mm}\) layers of self-attention blocks, as shown in Fig. 3. The output hidden states in the last layer of the decoder, \(H_{mm}^{out}=Dec(E_{mm})\), represent fused audio and conditions features. We only extract the part of the hidden states corresponding to the audio tokens, \(H_{a}^{out}^{d_{mm} T_{c}}\), and pass it through \(L\) Linear layers in parallel to perform classification on masked tokens at each level of the codebooks. For each masked audio token in matrix \(A_{tok}^{M}\), we calculate the cross-entropy loss between the predicted token \(_{tok}\) and the ground truth token \(a_{tok}^{gt}\), formulated as

\[_{VATT}=-_{a_{tok} A_{tok}^{M}}(a_{tok}= {<MASK>})log[P_{}(_{tok}}=a_{tok}^{gt}|A_{tok }^{M};H_{lm}))],\] (2)

where \(\) is the set of trainable parameters in the audio token decoder, and \(\) is the indicator function.

Figure 3: Audio Tokens Decoder: VATT Audio is a bi-directional transformer that models the audio tokens and the conditioning inputs (LLM hidden states) jointly. We extract the part that corresponds to the audio features and apply \(L\) Linear layers in parallel to perform classification on masked tokens at each codebook layer.

#### 3.2.2 Masking Design and Iterative Parallel Decoding

**Masking Distribution Design** Inspired by [1; 2], we incorporate variable random masking. In particular, it was shown that masking ratio plays an important role in audio token decoder to generate meaningful signals. While in [1; 2] arc-cosine masking distributions is used by default, here we study several masking strategies that include distributions along with different hyper-parameters to find the strategy that reaches more optimal generation quality (see Appendix A for further details). Our study shows that normal distribution with a mean of 0.75 and standard deviation of 0.25, truncated from 0.5 to 1.0 is such optimal strategy. The general interpretation of this strategy is that a relatively high range masking ratio enables models to generate better initial tokens when most of the entries in the token matrix are masked. This is essential for future decoding steps to generate meaningful tokens.

**Iterative Parallel Decoding** Scheduling of masking plays a key role as well. During inference, we follow the cosine scheduling scheme proposed in  to gradually resolve the audio tokens. The iterative sampling procedure starts with all <MASK> in the audio token matrix. At a step \(t\), the model takes the audio token matrix \(A_{t-1}\) from the previous step along with the conditions as inputs and samples a new audio token matrix \(_{t}\) in parallel with all tokens unmasked. Based on the confidence at each entry of \(_{t}\) only tokens with top-k confidence are kept while the remaining entries are re-filled with <MASK>, resulting in \(A_{t}\). The cosine scheduling scheme determines the ratio of re-masked tokens by \(r_{t}=cos()\). Notably, to resolve the confidence of each entry in the matrix, we adopt the _"gumbel-top-\(k\) trick"_ with temperature that varies, i.e., \(c_{i}=)}{}+G\), where \(G(0,1)\) and \(p_{i}\) denotes the output probability of the sampled token at the entry \(i\). This is equivalent to sampling k values from multinomial distribution from the softmax probabilities without replacement. The temperature \(\) controls the degree of stochasticity. We use \(=_{0}(1-)\) with linear decay during generation, where \(_{0}\) is the initial temperature. Similarly to [1; 2], our method achieves optimal quality and fast speed within a few decoding steps (typically 10 - 20).

## 4 Experiments

**Datasets:** We use common benchmarks datasets VGGSound  and AudioSet-2M  for training and evaluation. VGGSound is a large-scale audio-visual dataset sourced from YouTube, containing 192k videos from 309 audio-visual categories, with 177k / 15k train-test video splits. AudioSet-2M is a larger audio-visual database with around 2M YouTube videos, with only 1.6M available online. In Stage 1, we train VATT Converter with both datasets and test on VGGSound only. In Stage 2, for fair comparison against existing video-to-audio generation methods, we train and evaluate on VGGSound dataset only.

To train VATT with text, we synthesize a large-scale audio caption dataset, "V2A Instruction", using LTU , an existing audio LLM. We obtain audio captions by prompting the pretrained LTU-13B model with the inputs of audio waveform along with the instruction _"### Instruction: Close-ended question: Write an audio caption describing the sound. ### Response:"_. For AudioSet  and VGGSound  we generate a single audio caption per each video for a total of 1.77M videos.

To ensure the quality of captions, we first manually verified the validity of LTU-generated captions prior to using them as synthetic ground-truth (GT) and then performed an experiment to further evaluate captions quality. In particular, we randomly selected 100 videos from VGGSound test set with stratified sampling according to video categories to conduct a human study. We used 1-5 point MOS (Mean-Opinion-Score) scale (the higher the better) to measure correctness of the captions. We provide pairs of videos and the corresponding captions to the raters, asking "How accurately the provided caption reflects the sound events happening in the video? 1. Inaccurate and irrelevant. 2. Relevant but inaccurate with many mistakes. 3. Partially accurate but missing details and with mistakes. 4. Mostly accurate with some minor mistakes. 5. Accurate and complete." We used the MTurk platform to perform the evaluation and collected a total of 300 responses. The generated captions have a high MOS of mean 4.72 and std 0.37, providing an additional indication for the validity of the synthetic ground truth.

**Implementation Details:** For visual inputs, we use eva-CLIP  image encoder to extract mean-pooled visual features from video frames at 5fps rate, which result in \(50 768\) visual sequence for a 10s video. To represent audio, we extract audio tokens from a pretrained Encodec-16kHz. For each 10s audio waveform, we represent it with \(A_{tok}^{4 500}\) token matrix.

For LLM, we explore two open-source models, Gemma-2B  and LLama-2-7B , using instruction-tuned checkpoints. The LLM hidden size of Gemma-2B is 2048 and 4096 for LLama-7B. For both LLMs, we train VATT Converter using LoRA parameter-efficient fine-tuning technique while keeping the LLM weights frozen. We use rank \(r=16\) and \(=32\) with 0.1 dropout rate for LoRA configuration.

VATT Audio is a bi-directional transformer with 24 layers, each with hidden size 1024 with 16 attention heads. To differentiate the conditioning inputs and audio tokens, we add two learnable modality-specific embeddings with respect to the corresponding inputs(see further implementation details in AppendixD).

**Evaluation Metrics:** To evaluate video-to-audio generation quality, we follow the method of , which proposed the metrics Kullback-Leibler-Divergence (KLD) with PassT , Frechet Audio Distance (FAD)  and Align Accuracy (Align Acc) . KLD measures how closely the generated audio matches the GT through pairwise comparison, reflecting how well the audio captures the concepts in the video. FAD evaluates the overall distribution, indicating the overall quality of the audio. Align Acc assesses the relevance and temporal alignment of the audio and the video. Additionally, we incorporate generation speed (time taken per waveform sample) to measure efficiency. We also compute the CLAP score  to evaluate the adherence of generated audio to text prompts to compare our results with text-to-audio generation. Further details of these metrics are described in Appendix F.

For video-to-audio captioning, we use two types of metrics, natural language generation (NLG) metrics and audio-text relevance metric. NLG metrics evaluate the generated captions with respect to the ground truth audio captions using rule-based matching in terms of precision and recall. These metrics include BertScore , BLEU-4 , ROUGE-L  and CIDEr . To assess the relevance of generated audio captions with the actual audio, we compute the CLAP-score  as cosine similarity between audio and text embeddings.

**Quantitative Evaluation of Audio Generation:** We evaluate audio generation of VATT models on the VGGSound test split. For each of the 15,446 video samples, we generate a 10-second audio waveform. We compare VATT variants against existing video-to-audio generation methods as well as text-to-audio generation methods including AudioLDM-2  and AudioGen  using different text prompts. The results on the metrics described above are summarized in Table 1 and Table 2. VATT models achieve best KLD score and Align Acc against other methods while maintaining competitive FAD (top 2). Notably, when guided by GT audio captions (VATT-LLama-T and VATT-Gemma-T; bottom) our models generate sounds that match the GT audio more accurately, as indicated by lowest KLD score of 1.41 and 1.66 for VATT models with two LLM backbones, surpassing both video-to-audio and text-to-audio methods. In comparison to text-to-audio methods, VATT models achieve competitive audio-text alignment in terms of CLAP score, demonstrating a strong capability to follow text prompts. Implementation details of these baselines are included in Appendix E.

**Quantitative Evaluation of Video-to-Audio Captioning:** We evaluate video-to-audio captioning by prompting VATT Converter to generate audio captions. We use the prompt "Describe the possible audio for this video:" to generate captions for all VGGSound test videos. For baselines, we prompt LLAVA-13B-v1.5 model in two zero-shot modes to generate visual and audio descriptions respectively. Since LLVA can take a single image as an input only, we select the middle frame of videos. We

   Methods & KLD \(\) & FAD \(\) & Align Acc \(\) & Speed (s) \(\) \\  SpecVQGAN  & 3.78 & 6.63 & 48.79 & 7.2 \\ IM2WAV  & 2.54 & 6.32 & 74.31 & 289.5 \\ Diff-Foley  & 3.15 & 6.40 & 82.47 & 4.4 \\ FoleyGen  & 2.89 & 2.59 & 73.83 & 6.9 \\ V2A-Mapper  & 2.78 & **0.99** & 74.37 & 11.54 \\
**VATT-LLama (Ours)** & 2.39 & 2.38 & 80.32 & **1.1** \\
**VATT-Gemma (Ours)** & **2.25** & 2.35 & **82.81** & **0.65** \\ 
**VATT-LLama-T (Ours)** & **1.41** & 2.54 & 80.16 & **1.2** \\
**VATT-Gemma-T (Ours)** & **1.66** & 2.98 & 81.48 & **0.76** \\   

Table 1: Quantitative results against video-to-audio generation methods on VGGSound test set. ‘-T’ refers to model with text prompts.

use "Provide a concise, descriptive caption for the following image." as the visual prompt, and "Describe the sounds that this scene could yield in a short sentence without reasoning" as the audio prompt. We also compare against a video LLM baseline, Video-LLAMA-7B, to perform zero-shot video-to-audio captioning. Specifically, we directly input VGGSound videos into the VL branch of the Video-LLAMA model, and prompt it to generate audio captions using the instruction "User/ What sounds could match the video?" Since Video-LLAMA has not been pretrained on VGGSound dataset and LTU generated captions, we implement a similar structure of Video-LLAMA and train on our LTU-generated captioning data. We replaced the original BLIP-2 visual features used by Video-LLAMA with our eva02-CLIP-L visual features due to the expensive pre-processing time for all BLIP-2 features from videos in VGGSound and AudioSet. For the Video-QFormer component of Video-LLAMA, we keep it the same as Video-LLAMA, and we name this model as VATT-Qformer - LLama. Our evaluation is summarized in Table 3. VATT models with LLMs outperform LLVA-prompted and Video-LLAMA zero-shot results demonstrating a stronger capability to infer sounds from videos semantically. In particular, when measuring audio-text relevance, our model with LLama achieves an increase of +**5.0%** in accuracy when compared with LLVA visual caption baselines. For reference, the ground truth audio captions generated by LTU  have an average CLAP score of 0.379.

**Qualitative Evaluation:** In addition to quantitative evaluations, we also conduct a qualitative (subjective) study to evaluate audio generation perceptual quality of VATT. Specifically, we randomly select 100 videos from VGGSound test split with stratified sampling according to video categories. For each method in the baseline, we pair the generated samples against VATT. Two aspects of the generation are evaluated, Fidelity and Relevance. Fidelity focuses solely on audio quality, while Relevance evaluates the semantic relevance and temporal alignment of audio to the video. For each

   Methods & Text Prompt & KLD \(\) & FAD \(\) & Align Acc \(\) & CLAP Score \(\) \\  AudioGen  & LLVA visual caption & 3.65 & 6.03 & 41.66 & - \\ AudioGen  & GT audio caption & 2.19 & 3.17 & 48.96 & **0.409** \\ AudioD-2  & LLVA visual caption & 3.54 & 3.62 & 53.49 & - \\ AudioDLM-2  & GT audio caption & 2.09 & **2.46** & 51.84 & 0.326 \\
**VATT-LLama-T (Ours)** & GT audio caption & **1.41** & 2.54 & **80.16** & 0.347 \\
**VATT-Gamma-T (Ours)** & GT audio caption & **1.66** & 2.98 & **81.48** & 0.310 \\   

Table 2: Quantitative results comparing VATT with text-to-audio generation methods on VGGSound test set. ‘-T’ refers to model with text prompts. CLAP score is calculated as the cosine similarity of generated audio with respect to the GT audio caption.

Figure 4: Qualitative evaluation results: Pairwise Comparison of generated audio VATT v.s other methods comparing Fidelity and Relevance aspects.

pair, raters are asked to rate their scoring of VATT versus a compared baseline on a Likert scale from 1 (strongly prefer baseline) to 5 (strongly prefer VATT). We use our best VATT variant, VATT-LLama-T (with GT text guidance), for the comparison. As shown in Table 4, VATT surpasses other methods in Relevance. In terms of Fidelity, VATT is consistently being preferred when compared with most baselines and slightly less preferred when compared with V2A-Mapper. The reason could be that V2A-Mapper is directly optimized with diffusion techniques on AudioLDM, a large-scale pretrained text-to-audio model, such models tend to perform better in fidelity aspect in comparison to token-based models. Further details of qualitative evaluations are incorporated in Appendix G, and qualitative samples are provided in Appendix B.

**Ablation Studies:** We study the effectiveness of VATT Converter by removing the LLM and directly feed the visual features into the decoder. We denote such model as VATT-V. While VATT-V does not handle textual inputs or generate text, it still serves as a strong variant of VATT for video-to-audio generation. To study the contribution of audio token decoder, we replace the decoder part of VATT with interleaving attention blocks proposed in vanilla transformer , and denote this variant as VATT-Cross-Attn. As shown in Table 4 VATT-Gamma model outperforms both VATT-V and VATT-Cross-Attn. When VATT is conditioned on visual inputs only its performance is lowest across variants.. The VATT Converter enhances the visual features through audio-relevant text, thereby improving the relevance and quality of the generated audio. In addition, we find that the bi-directional transformer design in VATT Audio is critical for learning the associations between audio and conditioning inputs to enhance audio generation performance. Additional ablation studies can be found in Appendix A.

## 5 Conclusion

In this work, we propose a multi-modal generative framework that enables both text-guided video-to-audio generation and video-to-audio captioning. Experiments show that our method can generate high quality audio through text in both unconditional and conditional modes, as well as to generate reasonable audio captions from videos. One area for improvement is the diversity of the text generated by current audio LLMs. In cases where the user-provided text prompts significantly differ in style there is a possibility for a conflict of audio quality and adherence to the instructions. Future work could enhance the capability of the model to generalize across different text styles and to further develop capabilities for informative iterative conversation-like video-to-audio generation.

## Broader Impact

VATT could augment existing audio-video creation tools for content creators by allowing generation of custom audio tracks for given visual content through user provided text prompts. Also, VATT has the ability to suggest potential sounds for a given video which can inspire creators by presenting audio options that may not have been considered otherwise. This feature can be useful for brainstorming of content creation, where audio choices can influence the style of the final product.

Further extensions of this work could involve conversational video-to-audio generation such that the audio content is iteratively being refined. By integrating a conversational interface, the users can engage in a dialogue with the system, making requests and receiving responses. This approach goes beyond static text inputs, offering a more accessible toolset that does not require significant audio editing expertise. Moreover, the conversational system can seek clarifications or propose alternatives, functioning like an assistant to avoid misunderstandings and enhance audio quality. More broadly, the generative approach proposed here has the potential to adapt to other generative areas not limited to audio, video, but also potentially impact fields such as biochemistry, physics where a generative approach is utilized, e.g., generative modeling of high-energy particle events .

   Methods & KLD \(\) & FAD \(\) & Align Acc \(\) \\  VATT-V & 2.43 & 2.53 & 82.43 \\ VATT-Cross-Attn & 2.76 & 3.63 & 76.85 \\
**VATT-Gamma (Ours)** & **2.25** & **2.35** & **82.81** \\   

Table 4: Architecture Ablation Study.

While VATT presents a potential for content creation, the ability to generate realistic audio from visual inputs could lead to misuse, such as creating deceptive content or deepfake audio and ethical concerns must be addressed before utilization. Furthermore, similarly to audio generation, text generation capability could result in misuse such as offensive language or privacy violations. To mitigate these risks, in further development or potential code release we will establish clear ethical guidelines, evaluate for biases, and implement safeguards to ensure responsible use and fair outputs.