# CoIN: A Benchmark of Continual Instruction Tuning

for Multimodal Large Language Models

Cheng Chen\({}^{1}\)  Junchen Zhu\({}^{2}\)  Xu Luo\({}^{2}\)  Heng Tao Shen\({}^{2,3}\)  Jingkuan Song\({}^{1}\)  Lianli Gao\({}^{1,*}\)

\({}^{1}\)Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China

\({}^{2}\)University of Electronic Science and Technology of China \({}^{3}\)Tongji University

###### Abstract

Instruction tuning demonstrates impressive performance in adapting Multimodal Large Language Models (MLLMs) to follow task instructions and improve generalization ability. By extending tuning across diverse tasks, MLLMs can further enhance their understanding of world knowledge and instruction intent. However, continual instruction tuning has been largely overlooked and there are no public benchmarks available. In this paper, we present CoIN, a comprehensive benchmark tailored for assessing the behavior of existing MLLMs under continual instruction tuning. CoIN comprises 10 meticulously crafted datasets spanning 8 tasks, ensuring diversity and serving as a robust evaluation framework to assess crucial aspects of continual instruction tuning, such as task order, instruction diversity and volume. Additionally, apart from traditional evaluation, we design another LLM-based metric to assess the knowledge preserved within MLLMs for reasoning. Following an in-depth evaluation of several MLLMs, we demonstrate that they still suffer catastrophic forgetting, and the failure in instruction alignment assumes the main responsibility, instead of reasoning knowledge forgetting. To this end, we introduce MoELoRA which is effective in retaining the previous instruction alignment. Codes and datasets are publicly available https://github.com/zackschen/CoIN.

Figure 1: Different behavior of MLLMs when sequentially tuned on CoIN. Blue represents the accuracy for each task evaluated when just tuned on the corresponding task, and Red represents the accuracy evaluated after the models have been sequentially tuned on all tasks. LLaVA  and Qwen-VL  suffer from catastrophic forgetting while MiniGPT-v2  does not. The sequential training starts clockwise from ScienceQA and ends with OCR-VQA.

## 1 Introduction

Recently, Multimodal Large Language Models (MLLMs) [8; 11; 35; 34; 70; 57; 40] have garnered significant attention for their remarkable capabilities of vision-language understanding and generation. These MLLMs commonly adopt two stages to learn extensive knowledge and align with different task instructions. In the initial stage, various pre-train strategies are employed to establish vision-language alignment. Subsequently, to enhance the capacity to follow task instructions and improve performance, the aligned MLLMs are fine-tuned on meticulously constructed instruction data.

Given the impressive performance of instruction tuning, researchers can further enhance the capacity of MLLMs to align with various task instructions and learn more world knowledge, by tuning across diverse tasks. However, the performance of previous tasks after sequential updating on different tasks has been largely overlooked. Recent years, continual learning (CL) [52; 2; 7] is proposed to investigate the behavior of artificial intelligence on sequential fine-tuning. Some research has delved into continual instruction tuning for Large Language Models (LLMs) [68; 61; 42; 31]. However, the exploration of continual instruction tuning for MLLMs has been overlooked. EMT  investigates the catastrophic forgetting of MLLMs, yet only focusing on classification problems, limiting the exploration of the diverse capabilities of powerful MLLMs. He et al.  propose a benchmark to explore whether multi-task joint instruction tuning enhances a model's continual learning ability. However, this setting limits the exploration of MLLMs in practical scenarios.

Therefore, to comprehensively investigate the behavior of MLLMs in continual instruction tuning, we introduce a novel benchmark: **C**ontinual **I**nstruction tu**N**ing (CoIN). In CoIN, we construct a varied set of instructions by utilizing commonly used vision-language datasets to ensure accessibility and diversity, including general visual question answering, knowledge-grounded image question answering, OCR image question answering tasks, _etc_. However, the instructions constructed with these tasks mainly consist of question-answering tasks, lacking diversity. So, we additionally add grounding and classification tasks in our CoIN. Then, following common instruction templates [34; 35], we transform the selected datasets into a data format of instruction tuning.

Then, following an in-depth evaluation of popular MLLMs, we reveal that some of them still suffer from catastrophic forgetting (as showing in Fig. 1), similar to traditional continual CNN [22; 30]

Figure 2: An overview of CoIN benchmark. A selected MLLM is sequentially fine-tuned on 8 instruction datasets spanning diverse tasks. Then, it is evaluated from two perspectives: _Truth Alignment_ and _Reasoning Capability_, which assess the alignment with ground truth and knowledge preserved for reasoning, respectively. The evaluation example at the bottom presents the results of the model tested on classification after fine-tuning on each task.

or VIT [13; 38] models. However, different from these models which learn representations and use a final layer to make predictions with a fixed output style, MLLMs work in a generative way. This motivates us to inquire whether MLLMs forget the knowledge required for reasoning or if the issue lies in their inability to follow instructions. Because instruction tuning primarily focuses on learning to align with task instructions [69; 35; 34], we hypothesize that the model mainly loses the capability of instruction following, rather than the maintained knowledge. To validate this hypothesis, in addition to checking the outputs with the ground truth, called _Truth Alignment_, we employ powerful LLM assistant for evaluating the reasoning knowledge, called _Reasoning Capability_, as shown in Fig. 2.

After analyzing the results of these two evaluations, we reveal that the failure in instruction following assumes the main responsibility, instead of reasoning knowledge forgetting. Recently, Mixture-of-Experts (MoE) framework  leverages multiple distinct experts to acquire different knowledge and incorporates a gate function to modulate their contributions. We observe that this method resembles the architecture-based methods in traditional continual learning, providing the model with the ability to learn different instruction following from distinct experts. Therefore, we try to bring it into CoIN to mitigate the forgetting of alignments. Experimental results consistently demonstrate improvement after integrating with more experts.

In summary, the contributions of this paper are as follows:

* A novel benchmark for MLLMs in continual instruction tuning is proposed, namely CoIN, which consists of 10 datasets spanning 8 different tasks for comprehensive and diverse evaluation.
* A novel evaluation approach is introduced to assess the model's ability from two aspects: _Truth Alignment_ and _Reasoning Capability_. Furthermore, we reveal that the catastrophic forgetting in MLLMs is primarily due to the decline in instruction following rather than reasoning knowledge.
* Multiple state-of-the-art MLLMs are chosen for evaluation on CoIN. Additionally, we introduce MoELoRA which is effective in mitigating forgetting owing to its use of distinct experts.

## 2 Preliminaries

In the multimodal continual instruction tuning, an MLLM has been pre-trained with abundant vision-language data to align the gap of vision and language, indicated by trainable parameters \(\). We further train it to adapt to novel \(S\) tasks in a sequential manner. Each task is denoted by a task descriptor \(\{1,2,...,S\}\), and owns an independent dataset \(_{}=\{(X^{img}_{,j},X^{ins}_{,j},X^{ans}_{,j})^{N_{ }}_{j=1}\}\) with \(N_{}\) data pairs, where \(X^{img}\), \(X^{ins}\) and \(X^{ans}\) indicate the input image tokens, instruction tokens and answer tokens, respectively. For a sample pair with an answer in length \(L\), we compute the probability of the whole target answers \(X^{ans}\) in an auto-regressive manner as follows:

\[p(X^{ans}|X^{img},X^{ins})=_{i=1}^{L}p_{}(X^{ans}_{i}|X^{img},X^{ ins},X^{ans}_{<i}),\] (1)

where \(X^{ans}_{<i}\) indicates all answer tokens before the index \(i\) and \(X^{ans}_{i}\) indicates the \(i\)-th answer token. We optimize the network with the following function:

\[=-_{i=1}^{L} p_{}(X^{ans}_{i}|X^{img},X^{ins},X^{ans} _{<i}).\] (2)

## 3 CoIN: A Benchmark for MLLMs

### How to Compose A Comprehensive Benchmark for MLLMs?

#### 3.1.1 Data Integration.

A comprehensive evaluation relies on extensive data. To ensure the accessibility and diversity of the instruction tuning samples, we collect various publicly available and commonly used vision-language datasets. These datasets cover a wide range of tasks, including general image question answering, visual reasoning, knowledge-grounded image question answering, _etc_. Specifically, the selected datasets include VQAv2 , VizWiz , ScienceQA , TextVQA , GQA  and OCR-VQA . However, we observe that these datasets are limited to traditional QA tasks in the vision-language community, lacking task diversity. To overcome this limitation, we introduce the classification task and region-level grounding task into CoIN with ImageNet , RefCOCO , RefCOCO+  and RefCOCOg .

With a substantial collection of instruction data, the next step involves transforming these samples into a unified instruction tuning format. Nowadays, several works  have resorted to different ways to construct the instructions. For example, LLAVA  leverages ChatGPT  and GPT-4  to create GPT-assisted visual instruction based on COCO . SPHINX  adapt different templates to transform a wide range of multi-modal tasks into instructions. Drawing on insights from prior research, we utilize commonly employed instruction templates to formulate our instructions, as illustrated in Tab. 1. The final benchmark encompasses 10 datasets spanning 8 task categories. (Some examples of our CoIN are presented in the Appendix).

#### 3.1.2 Training

Low-rank Adaptation (LoRA)  has demonstrated effectiveness and efficiency in the fine-tuning of pre-trained language models. Additionally, a previous study  observed that parameter-efficient methods are more susceptible to forgetting in LLMs. Therefore, CoIN specifically explores the behavior of parameter-efficient fine-tuning of MLLMs, focusing on LoRA. Specifically, during fine-tuning on CoIN, only the low-rank matrices are updated, while the parameters of the base LLMs and vision encoder are frozen.

#### 3.1.3 Performance Evaluation.

Different from traditional continual learning which only compares the predicted results with the ground truth in a word-for-word manner, _I.e_. _Truth Alignment_, we argue that the outputs of MLLMs are influenced by _Reasoning Capability_. Therefore, the evaluation of MLLMs needs to consider the two aspects respectively.

_Truth Alignment._ The ability to generate the correct result in the desired format to follow task instruction is the basic requirement for instruction tuning. To evaluate the ability of overall performance of MLLMs on CoIN, we adopt the traditional evaluation method that we directly compare the outputs of MLLMs with ground truths. These metrics for each task slightly vary since different tasks have different forms of outputs (Comparison details can be found in the Appendix).

_Reasoning Capability._ The performance of MLLMs depends not only on the instruction following but also on the knowledge maintained in MLLMs. For example, MLLMs may correctly answer the question logically as "Two apples" while the ground truth is "Two". The evaluation of the truth alignment will directly discriminate this sample as negative. To tackle this issue and further analyze

    & **Dataset** & **Instruction** &  **Train** \\ **Number** \\  &  **Test** \\ **Number** \\  \\   & RefCOCO+ & 
 Please provide the bounding \\ box coordinate of the region \\ this sentence describes: \(<\)description\(>\) \\ What is the object in the image? \\  & 55k & 31k \\  &  & Answer the question using a & 129k & 5k \\  & & single word or phrase & \\  & & Answer the question using a & & \\
**Image Question Answering (IQA)** & VQAZ & & single word or phrase & \\  & & & Answer with the option’s letter & 12k & 4k \\ Knowledge Grounded IQA & ScienceQA & & from the given choices directly & 165k & 100k \\  & & Answer the question using a & & \\
**Reading Comprehension IQA** & TextVQA & & single word or phrase & \\
**Visual Reasoning IQA** & GQA & & Answer the question using a & \\  & & single word or phrase & 72k & 1k \\ Blind People IQA & & Answer the question using a & & \\  & & single word or phrase & 20k & 8k \\
**OCR IQA** & OCR-VQA & & Answer the question using a & & \\  & & single word or phrase & 165k & 100k \\   

Table 1: The statistic of collected datasets and instructions in CoIN benchmark.

MLLMs, we propose reasoning capability which refers to the knowledge contained in MLLMs to comprehend different modalities and make the reasoning. Motivated by previous works [15; 18], we adopt another LLM to grade the output. With designed prompts, the LLM will disregard the structure of the outputs and solely evaluate the key information within it to obtain a score from 0 to 10.

Roughly speaking, for an MLLM to generate a desired output, it must have the ability to make reasoning and transfer the reasoned output into a structure that aligns with task instructions. The _Truth Alignment_ evaluates the overall performance of the model, while the _Reasoning Capability_ specifically investigates the model's reasoning ability. The remaining aspect is the capability of _Instruction Following_. Therefore, the correlation of the above evaluations are following:

\[=+\]

With the novel evaluations described above, we present the overall performance calculation in CoIN here. Adhering to traditional continual learning metrics, we employ Backward Transfer (BWT) to measure the degree of suffering catastrophic forgetting. Additionally, unlike traditional continual learning where the model gradually forgets learned knowledge, sharp fluctuations occur in instruction tuning influenced by the gap between different tasks. Hence, we incorporate an additional metric, Mean Average Accuracy , to measure the performance throughout the training process.

(1) _Mean Average Accuracy_ (MAA): \(MAA=_{j=1}^{T}(_{i=1}^{j}A_{j,i}),\) where \(A_{j,i}\) is the performance on \(i\)-th task after training the task \(j\). A high MAA corresponds to a continual learning model that consistently maintains a high accuracy throughout the training process.

(2) _Backward Transfer_ (BWT): \(BWT=_{i=1}^{T}(A_{T,i}-A_{i,i}),\) where \(A_{i,i}\) is the performance on \(i\)-th task after training on \(i\)-th task.

## 4 Experiments

Setup LLaVA , Qwen-VL  and MiniGPT-v2  that have achieved remarkable performance on numerous benchmarks, are selected as the models for fine-tuning on the proposed CoIN benchmark. In addition, we choose two baselines for comparison: **Multi-task** which fine-tunes on all instructions instead of sequential training, and **Zero-shot** which involves assessing each task based on pre-trained MLLMs. For the fine-tuning sequence in CoIN, we adopt a random order, resulting in the following sequence: ScienceQA, TextVQA, ImageNet, GQA, VizWiz, Grounding, VQAv2, and OCR-VQA. For reasoning capability evaluation, we select Qwen-1.5-32B, a state-of-the-art model on many benchmarks, as the powerful LLM to evaluate the outputs from our trained model.

   &  &  &  \\   & & ScienceQA & TextVQA & ImageNet & GQA & VizWiz & Grounding & VQAV2 & OCR-VQA & MAA & BWT \\    } & Multi-task & 56.77 & 49.35 & 95.55 & 56.65 & 53.90 & 30.09 & 59.50 & 55.65 & 57.18 & - \\  & Zero-shot & 49.91 & 2.88 & 0.33 & 2.08 & 0.90 & 0.00 & 0.68 & 0.17 & 7.12 & - \\   & Sequential & 82.45 & 49.99 & 96.05 & 56.40 & 55.45 & 31.27 & 62.20 & 57.08 & 32.97 & -32.62 \\  & Finetune & 21.26 & 28.74 & 10.25 & 36.78 & 32.45 & 0.83 & 42.50 & 57.08 & 32.97 & -32.62 \\    } & Multi-task & 25.70 & 60.88 & 17.05 & 56.77 & 35.58 & 6.78 & 68.67 & 63.50 & 41.87 & - \\   & Zero-shot & 64.56 & 48.15 & 11.82 & 44.50 & 9.57 & 0.00 & 64.10 & 27.50 & 33.78 & - \\   & Sequential & 67.69 & 66.36 & 53.70 & 59.30 & 36.38 & 63.10 & 71.00 & 47.80 & 43.35 & -16.94 \\   & Finetune & 31.05 & 42.45 & 29.57 & 55.57 & 15.30 & 40.33 & 67.75 & 47.80 & 43.35 & -16.94 \\    } & Multi-task & 43.55 & 19.24 & 10.57 & 28.43 & 41.62 & 0.00 & 27.12 & 1.45 & 21.50 & - \\   & Zero-shot & 32.16 & 6.83 & 0.07 & 11.58 & 35.20 & 0.00 & 12.20 & 0.03 & 12.26 & - \\   & Sequential & 28.81 & 10.40 & 7.25 & 31.55 & 41.35 & 0.00 & 36.10 & 6.15 & 25.45 & 6.04 \\   & Finetune & 44.35 & 29.89 & 11.90 & 36.95 & 42.58 & 0.00 & 38.10 & 6.15 & & 25.45 & 6.04 \\  

Table 2: The results evaluating the _Truth Alignment_ ability are presented below. The first line of **Sequential Finetune** are the results for each task evaluated when just tuned on the corresponding task, and the second line displays the final results of each task after fine-tuning on the last task.

### How Do Existing MLLMs Perform in CoIN?

To comprehensively investigate the performance of the chosen MLLMs, we conduct experiments on our CoIN. Quantitative results about the ability of _Truth Alignment_ and _Reasoning Capability_ are shown in Tab. 2 and Tab. 3, respectively.

For the results of truth alignment of Tab. 2, we have the following observations: **Firstly**, unlike traditional continual learning, where the multi-task model often serves as the upper bound, in CoIN, the performance of the multi-task model is not the best due to the influence of task gaps. **Secondly**, even though pre-trained MLLMs retain substantial knowledge, the performance of zero-shot on specific tasks remains unsatisfactory, resulting in an accuracy of 7.12, 33.78 and 12.26. This validates the importance of instruction tuning for MLLMs in achieving task alignment. **Thirdly**, sequential finetune even performs better on fine-tuning tasks than multi-task (_i.e._ the first line in Sequential Finetune), except for some tasks in MiniGPT-v2. The possible reason may be that the model tends to focus on one task, diminishing the impact of diverse instructions from other tasks. However, due to the absence of techniques to regulate learning, these models suffer from forgetting, resulting in -32.62 of LLaVA and -16.94 of Qwen-VL in terms of BWT. **Finally**, MiniGPT-v2 demonstrates an incredible ability to mitigate forgetting. However, compared to LLaVA and Qwen-VL, it behaves underfitting on some tasks during fine-tuning. As training progresses, the performance on each task gradually improves. We believe this may be due to fewer training samples and iterations compared to its official instruction tuning.

In addition, comparing the results of _Truth Alignment_ with those of _Reasoning Capability_ in Tab. 3, it is evident that the forgetting of reasoning knowledge is much smaller than that of truth alignment. For example, the _Reasoning Capability_ of LLaVA for grounding only drops from 58% to 52%, whereas the _Truth Alignment_ drops from 31.27% to 0.00%. This comparison supports the hypothesis that the model primarily loses the capability to align with task instruction, rather than the maintained knowledge. Furthermore, on the one hand, compared with the slight decrease observed in LLaVA and Qwen-VL, MiniGPT-v2 performs robustly in retaining reasoning knowledge. On the other hand, compared to the truth alignment of MiniGPT-v2, it is noticeable that the overall performance increase of MiniGPT-v2 is primarily owing to the learning of instruction following. This coincides with the purpose of instruction tuning, which is to learn to align with task instruction. Finally, since the _Reasoning Capability_ is robust to forgetting, we will just record the results of _Truth Alignment_ in the following experiments (More results about _Reasoning Capability_ are in Appendix).

### Whether is Qwen a good evaluator?

We select Qwen to evaluate the _Reasoning Capability_, but is it a reliable evaluator? To assess its effectiveness, we conduct experiments to compare with another powerful closed-source large language model, along with a user study. The comparison results are presented below. Firstly, many works [18; 37] have commonly employed GPT-4 to evaluate the quality of generated samples. Following this approach, we also use GPT-4 to assess outputs using the same prompts. The comparison with Qwen reveals that the overall trends in evaluating _Reasoning Capability_ are consistent. Secondly, we randomly sample model outputs for each task and gather feedback from AI researchers, asking them

   &  &  &  \\   & & &  &  &  &  &  &  &  &  &  & BWT \\    } & Multi-task & 80 & 75 & 97 & 72 & 42 & 86 & 73 & 79 & 75.50 \\  & Zero-shot & 93 & 83 & 69 & 64 & 48 & 35 & 64 & 66 & 65.25 & \\  & - & Sequential & 92 & 75 & 97 & 72 & 42 & 58 & 75 & 78 & \\  & Finetune & 82 & 74 & 55 & 56 & 47 & 52 & 58 & 78 & \\    } & Multi-task & 98 & 82 & 68 & 77 & 50 & 51 & 82 & 88 & 74.50 & \\  & Zero-shot & 97 & 81 & 78 & 74 & 54 & 58 & 81 & 74 & 74.63 & \\   & Sequential & 96 & 83 & 86 & 78 & 51 & 82 & 82 & 75 & \\   & Finetune & 95 & 78 & 77 & 77 & 47 & 76 & 82 & 75 & \\    } & Multi-task & 96 & 76 & 58 & 62 & 44 & 89 & 63 & 59 & 68.38 & \\  & Zero-shot & 98 & 72 & 48 & 63 & 48 & 80 & 64 & 61 & 66.75 & \\   & Sequential & 97 & 71 & 55 & 61 & 44 & 91 & 63 & 52 & 75.05 & 0.00 \\   & Finetune & 89 & 73 & 59 & 60 & 44 & 94 & 63 & 52 & \\  

Table 3: The evaluation results of _Reasoning Capability_ are presented below.

to score the outputs by using the same prompts with both GPT-4 and Qwen-32B. The results from the user study align closely with those of Qwen-32B, confirming its validity as a reliable evaluator. In summary, Qwen is effective in assessing the retention and forgetting of _Reasoning Capability_.

### What factors affect the performance?

Impact of Task orderTo explore the impact of different sequence order, we conduct an additional experiment using a different order of ColN tasks, arranged alphabetically: GQA, Grounding, ImageNet, OCR-VQA, ScienceQA, TextVQA, VizWiz, and VQAV2. From the comparison results of LLaVA presented in Tab. 5, we observe that altering the task order inevitably influences the outcomes of each task. This effect occurs because the knowledge acquired from previous tasks can either benefit or hinder subsequent training. Furthermore, the final performance is also affected by the training sequence. Although the BWT of the alphabetic order is better than that of a random order, the overall result is still inferior to that achieved with a random order. After examining the overall performance throughout the training process, we observe that the results on the Grounding and ImageNet tasks are consistently inferior, thereby negatively impacting the overall performance.

Impact of Instruction diversityFurthermore, we note that some tasks in the ColN dataset share similar instructions. This raises a pivotal question: does the type of instruction template impact the efficacy of continual instruction tuning? To investigate this issue, we devise two additional variants: 1) **Diverse**: Distinct instruction templates tailored to different tasks. 2) **10Type**: Randomly chosen from 10 distinct instruction templates. (Details can be found in the Appendix.) Tab. 6 presents the performance of task solving on these variants with LLaVA. Our comparative analysis reveals that merely changing to diverse templates has minimal impact on overall performance. However, randomly choosing from multiple distinct templates significantly enhances performance. The possible

   &  &  \\   & & & & & & & & & & & \\   & 82.45 & 49.99 & 96.05 & 56.40 & 55.45 & 31.27 & 62.20 & 57.08 &  &  \\  & 21.26 & 28.74 & 10.25 & 36.78 & 32.45 & 0.83 & 42.50 & 57.08 & & \\    & 82.45 & 50.14 & 96.03 & 55.65 & 51.42 & 34.00 & 59.17 & 52.92 & & \\    & 26.00 & 25.38 & 8.40 & 33.07 & 26.52 & 0.10 & 40.00 & 52.92 & & \\    & 81.65 & 51.99 & 97.00 & 61.30 & 54.10 & 39.20 & 68.15 & 64.65 & & \\    & 54.84 & 35.46 & 9.80 & 38.70 & 12.95 & 0.82 & 46.80 & 64.65 & & \\  

Table 6: The results of LLaVA about **different instruction templates** are presented below.

   &  &  \\   & & & & & & & & & & & \\   & & & & & & & & & & & \\   & 92 & 75 & 97 & 72 & 42 & 58 & 75 & 78 &  &  \\  & 82 & 74 & 55 & 56 & 47 & 52 & 58 & 78 &  &  &  \\  & 94 & 83 & 96 & 83 & 79 & 71 & 81 & 69 &  &  \\  & 80 & 83 & 65 & 67 & 62 & 70 & 68 & 69 & \\    & 96 & 82 & 98 & 85 & 80 & 65 & 86 & 70 &  &  \\    & 85 & 80 & 85 & 71 & 76 & 57 & 73 & 70 & \\  

Table 4: The comparison of Qwen with GPT-4 and user study as a evaluator are presented below.

   &  &  \\   & & & & & & & & & & & & \\   & 82.45 & 49.99 & 96.05 & 56.40 & 55.45 & 31.27 & 62.20 & 57.08 &  &  \\  & 21.26 & 28.74 & 10.25 & 36.78 & 32.45 & 0.83 & 42.50 & 57.08 & & \\    & GQA & Grounding & ImageNet & OCR-VQA & ScienceQA & TextVQA & VizWiz & VQAV2 & MAA & BWT \\    & 62.68 & 37.73 & 97.30 & 62.00 & 59.98 & 50.98 & 60.10 & 67.28 & & \\    & 53.92 & 0.00 & 8.57 & 37.75 & 44.37 & 53.37 & 25.27 & 67.28 & & \\  

Table 5: The results of LLaVA about **different task orders** are presented below.

reason is that with different templates, the model learns the true instructional intention within each task. Our findings suggest that instruction diversity can better mitigate the degeneration of instruction following, owing to increased robustness to varying instructions.

Impact of data volumeWhile there is ample knowledge retained in MLLMs, they require substantial instruction data for fine-tuning to enhance their ability to produce desired results. However, in practice, collecting high-quality data is costly. Several works  have begun to study the impact of varying training data sizes on overall performance. In this work, to further explore the influence of the volume of instructions on continual instruction tuning, we conduct experiments to delve deeper into this investigation. To generate datasets of different volumes, we randomly select samples of each dataset from our benchmark, resulting in varying training data sizes, including 10%, 20%, 40%, 60%, and 80%. The experimental results of LLaVA are shown in Tab. 7. Overall, the performance exhibits an initial growth followed by a subsequent decline. This is possibly due to the fact that the model acquires more instruction following knowledge with the increase in size, as evidenced by the results of fine-tuning on each task growing with the volume increasing. However, the expansion in volume leads to the overriding of old knowledge by newly acquired knowledge, disrupting the balance between stability and plasticity and resulting in increased forgetting.

### Example Analysis

Figure 3: The illustration of test examples from LLaVA after training on the last task, OCR-VQA.

   &  &  \\   & ScienceQA & TextVQA & ImageNet & GQA & VizWiz & Grounding & VQAV2 & OCR-VQA & MAA & BWT \\ 
0.1 & 70.00 & 42.88 & 93.45 & 36.93 & 43.7 & 3.73 & 40.48 & 45.62 & 30.27 & -16.17 \\  & 53.71 & 32.62 & 5.38 & 33.50 & 36.98 & 2.85 & 36.77 & 45.62 & & \\
0.2 & 69.86 & 46.86 & 94.38 & 44.98 & 44.15 & 4.81 & 32.55 & 52.10 & 30.33 & -19.89 \\  & 41.12 & 33.25 & 5.53 & 33.80 & 25.85 & 1.77 & 37.10 & 45.62 & & \\
0.4 & 75.33 & 47.06 & 94.95 & 52.95 & 50.77 & 10.25 & 56.73 & 55.33 & & \\
0.4 & 49.96 & 23.60 & 7.22 & 36.12 & 33.05 & 0.09 & 39.20 & 55.33 & & \\
0.6 & 78.09 & 47.65 & 95.85 & 55.93 & 53.08 & 10.00 & 59.17 & 46.33 & & 31.47 & -32.57 \\  & 27.42 & 19.54 & 7.03 & 33.52 & 13.15 & 0.05 & 38.48 & 46.33 & & \\
0.8 & 80.02 & 48.13 & 95.45 & 54.00 & 49.85 & 28.33 & 58.35 & 56.67 & & \\
1.74 & 16.94 & 8.85 & 32.62 & 35.50 & 0.00 & 39.67 & 56.67 & & 30.00 & -33.60 \\
1.0 & 82.45 & 49.99 & 96.05 & 56.40 & 55.45 & 31.27 & 62.20 & 57.08 & & \\  & 21.26 & 28.74 & 10.25 & 36.78 & 32.45 & 0.83 & 42.50 & 57.08 & & \\  

Table 7: The results of LLaVA about **different data volumes** are presented below.

To further understand the difference between _Truth Alignment_ and _Reasoning Capability_, we provide some examples after training on OCR-VQA in Fig. 3. In the first example, the model struggles to comprehend the content and produces an unrelated answer. Consequently, both _Truth Alignment_ and _Reasoning Capability_ evaluations result in a score of 0. In the second example from ScienceQA, the model comprehends the instruction and image, providing the correct answer "New Hampshire" instead of the intended instruction "B". Consequently, the outcome regarding _Truth Alignment_ for this sample is incorrect. However, in reality, the output from the model encompasses all the reasoning knowledge necessary to solve this problem, earning a score of 10. As for the third example of the GQA, the output aligns with both the instruction intent and general knowledge, achieving the best result. The last example displays a result from ImageNet after training on OCR-VQA. We observed that after training on the last dataset, the model tends to give an answer "Car". While this response is considered incorrect since the ground truth answer is "Pickup", it is evident that the model has captured some knowledge contained in this sample pair. Fortunately, LLMs have analyzed the retained knowledge and output a suitable answer to indicate the degree of knowledge: 8.

## 5 Mixture-of-Experts benifits Continual Instruction Tuning

The Mixture-of-Experts (MoE) employs distinct experts to acquire various types of knowledge, akin to the expansion category of continual learning methods. Therefore, we bring the prevalent MoELoRA [36; 14] into CoIN to utilize experts to acquire distinct knowledge for different tasks to mitigate forgetting (Details are in Appendix). To validate the ability of MoELoRA to learn diverse knowledge and mitigate catastrophic forgetting, we conduct experiments with LLaVA by setting different expert numbers \(N\) to the values of {2, 4, 8}, and report the quantitative results in Tab. 8 (Results of General Knowledge and other MLLMs are in Appendix). It is worth noting that the 1 expert in the MoELoRA method is equivalent to the vanilla LoRA fine-tuning method. Notably, the results demonstrate a consistent improvement across all metrics when the low-rank matrices of LoRA are divided into a greater number of experts. This trend can be attributed to the fact that enhanced specialization is achieved with more experts. Therefore, each distinct expert is capable of focusing on diverse instruction intent associated with specific tasks, effectively reducing interference.

Comparison with Continual MethodsTo further investigate the effectiveness of our proposed method, we conduct experiments with other continual learning methods, including LwF and EWC. For EWC, we compute the Fisher matrix by randomly selecting 1,000 samples from each task and set the hyperparameter lambda to 0.1. For LwF, we choose to save 100 logits for each task to compute the distillation loss, the hyperparameter lambda is also set to 0.1. Further, since the experiments presented in Tab. 7 demonstrate that LLaVA achieves superior performance with a 40% data volume, we conduct the following experiments based on this setting and selected this model as the baseline. The experimental results based on LLaVA are illustrated in Tab. 9.

From the quantitative results shown below, we have several observations: (1). Our method consistently achieves the best final result, with improvements of 7.87% in MAA and 2.35% in BWT, respectively. (2). Our comparative analysis indicates that all approaches mitigate catastrophic forgetting. Notably, these methods primarily preserve knowledge in question-answering tasks but still experience forgetting on ImageNet and Grounding. Since EWC and LwF do not perform well

   &  & **Overall Results** \\   & ScienceQA & TextVQA & ImageNet & GQA & VizWiz & Grounding & VQAV2 & OCR-VQA & MAA & BWT \\  Multi-task(1) & 56.77 & 49.35 & 95.55 & 56.65 & 53.90 & 30.09 & 59.50 & 55.65 & 57.18 & - \\
1 & 82.45 & 49.99 & 96.05 & 56.40 & 55.45 & 31.27 & 62.20 & 57.08 & 32.97 & -32.62 \\  & 21.26 & 28.74 & 10.25 & 36.78 & 32.45 & 0.83 & 42.50 & 57.08 & 32.97 & -32.62 \\  & 79.93 & 51.37 & 95.92 & 59.60 & 55.33 & 32.29 & 63.15 & 54.15 & 35.75 & -28.03 \\  & 47.77 & 31.67 & 10.75 & 37.10 & 40.98 & 1.44 & 43.65 & 54.15 & 35.75 & -28.03 \\  & 80.35 & 52.21 & 96.25 & 59.62 & 58.05 & 34.47 & 64.40 & 62.73 & 40.24 & -26.57 \\  & 65.36 & 40.28 & 11.10 & 37.20 & 34.77 & 0.49 & 43.60 & 62.73 & 40.24 & -26.57 \\  & 75.78 & 51.73 & 96.70 & 59.42 & 58.88 & 37.50 & 64.22 & 60.08 & & 42.76 & -25.91 \\  & 63.09 & 38.63 & 10.50 & 37.38 & 43.62 & 0.59 & 43.15 & 60.08 & & 42.76 & -25.91 \\  

Table 8: The results of LLaVA about **different numbers of experts** are presented below.

on the Grounding task, the forgetting in this task is less pronounced. (3). It is worth noting that under the 40% data volume setting, our method exhibits slightly more forgetting compared to other continual learning approaches. Upon further investigation, we find that this is due to an enhancement in learning ability, as evidenced by improved performance on most tasks, particularly a 25.57% improvement on Grounding compared to other approaches. Consequently, our approach achieves better plasticity, achieving the best overall results. (4). The distributed training of large language models complicates the integration of EWC and LwF compared to our approach, which is designed based on the architecture and training paradigm of MLLMs. This poses a significant challenge that hinders the practical application of traditional continual learning approaches.

## 6 Limitations

Despite the positive contributions of this study, we acknowledge the following limitations: 1) **Model Size and Training Constraints**: This study only presents MLLMs ranging from 7 to 9 billion parameters. Due to computational limitations, we have not investigated larger models or employed a full fine-tuning strategy on our CoIN. 2) **Model Type**: Most MLLMs utilize LLaMA  as their language model, limiting the exploration of different model architectures. 3) **Task Diversity**: Currently, mainstream instruction tuning primarily focuses on image question answering tasks. Although we have incorporated classification and grounding tasks, it is crucial to explore the influence of a broader range of tasks.

## 7 Conclusion

This paper introduces a novel benchmark, Continual Instruction tuNing (CoIN), utilizing widely used vision-language datasets to investigate the behavior of Multimodal Large Language Models (MLLMs) on continual instruction tuning. CoIN encompasses 10 datasets spanning 8 tasks and transforms the data into an instruction-tuning format. Additionally, CoIN evaluates the MLLMs from truth alignment and reasoning capability. Experiments on CoIN explore the performance of MLLMs under different training orders, instruction types and data volumes. The results of these experiments show that the general knowledge maintained in MLLMs is robust for catastrophic forgetting, rather than instruction following. Based on this observation, we bring the MoELoRA into MLLMs to utilize different experts to learn the different tasks, effectively reducing catastrophic forgetting in MLLMs.