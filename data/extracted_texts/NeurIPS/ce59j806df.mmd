# Semi-Supervised Domain Generalization with

Known and Unknown Classes

 Lei Zhang, Ji-Fu Li, Wei Wang

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{zhang1,lijf,wangw}@lamda.nju.edu.cn

Corresponding author.

###### Abstract

Semi-Supervised Domain Generalization (SSDG) aims to learn a model that is generalizable to an unseen target domain with only a few labels, and most existing SSDG methods assume that unlabeled training and testing samples are all known classes. However, a more realistic scenario is that known classes may be mixed with some unknown classes in unlabeled training and testing data. To deal with such a scenario, we propose the Class-Wise Adaptive Exploration and Exploitation (CWAEE) method. In particular, we explore unlabeled training data by using one-vs-rest classifiers and class-wise adaptive thresholds to detect known and unknown classes, and exploit them by adopting consistency regularization on augmented samples based on Fourier Transformation to improve the unseen domain generalization. The experiments conducted on real-world datasets verify the effectiveness and superiority of our method.

## 1 Introduction

The machine learning community has witnessed the great progress of deep learning models and their applications, e.g., computer vision , natural language processing . Such a huge success is mostly based on the i.i.d. assumption, i.e., the training data and testing data are identically and independently distributed. However, when these popular models are evaluated on new testing data, whose distribution is slightly different from the training data, a significant drop in performance will be observed .

In order to improve the generalization of the model under distribution shifts, _Domain Adaptation_ (DA)  and _Domain Generalization_ (DG)  have been widely studied. The goal of DA is to transfer the knowledge learned

Figure 1: Semi-supervised domain generalization with known and unknown classes. Labeled samples on source domains (Art, Cartoon and Photo) are _known classes_ (green box). _Known classes_ (green box) and _seen unknown classes_ (blue box) are mixed in unlabeled samples on source domains, while _known classes_ (green box), _seen unknown classes_ (blue box) and _unseen unknown classes_ (red box) are mixed in testing samples on target domain (Sketch).

from label-rich source domains to the unlabeled or partially labeled target domain. However, the accessibility of the target domain may not always be satisfied in some applications, e.g., autonomous driving and medical diagnosis, since we are not able to anticipate the domain that deployed systems will encounter. To this end, DG is introduced to develop domain-generalizable models on _unseen_ target domain by using multiple different and labeled source domains.

To reduce the expensive cost of the labeling process , Semi-Supervised Domain Generalization (SSDG) [14; 15; 16; 17] is proposed recently. In SSDG, each source domain consists of a few labeled samples and a large number of unlabeled samples. The goal is to learn a domain-generalizable model from the partially labeled samples. However, in a more realistic scenario, _known classes_ are probably mixed with some _unknown classes_ in unlabeled training and testing data (Figure 1). In such a scenario, when deployed on an _unseen_ target domain, the learned model is not only required to classify _known classes_, but also to recognize _unknown classes_. In this paper, we propose the Class-Wise Adaptive Exploration and Exploitation (CWAEE) method for semi-supervised domain generalization when _known_ and _unknown classes_ are mixed in unlabeled data. The intuition is to first explore unlabeled training data by detecting _known_ and _unknown classes_, and then exploit them in different ways to improve the generalization of the model. In particular, we use one-vs-rest classifiers and class-wise adaptive thresholds to detect _known_ and _unknown classes_ in unlabeled training data, and adopt consistency regularization on augmented samples based on Fourier Transformation to improve the _unseen_ domain generalization. The experiments on real-world datasets verify that our CWAEE method can achieve better performance than compared methods.

## 2 Related Work

Domain Generalization (DG) aims to help the model generalize to an unseen target domain by utilizing one or several related domains. Most existing DG methods can be classified into three different categories: domain-invariant representation learning, meta-learning and data augmentation. Motivated by the learning theory of domain adaptation , domain-invariant representation learning methods attempt to learn a representation space where the discrepancy between different source domains is as small as possible. For example, Muandet _et al._ proposed a kernel-based optimization algorithm for Support Vector Machine that learns an invariant transformation by minimizing the distribution discrepancy across domains. After the domain-adversarial neural network was proposed for domain adaptation , domain-adversarial training is widely used for DG [19; 20]. Following Model-Agnostic Meta-Learning (MAML) , Li _et al._ applied meta-learning strategy to DG through dividing the training data from multiple source domains into meta-train and meta-test sets to simulate domain shifts. The meta-optimization objective was designed to minimize the loss not only on meta-train domains but also on the meta-test domain. Shu _et al._ conducted meta-learning over augmented domains to learn open-domain generalizable representations for Open Domain Generalization (OpenDG) problem. Compared to domain-invariant representation learning and meta-learning, data augmentation methods have shown better performance on various DG benchmarks recently. Zhou _et al._ mixed feature statistics extracted by bottom layers of Convolutional Neural Networks (CNNs) between instances of different domains to synthesize instances in novel domains for training. Xu _et al._ applied MixUp  in amplitude spectrums of two images to augment training data while preserving high-level semantics of the original images in phase spectrums. To reduce the labeling cost of DG, Semi-Supervised Domain Generalization (SSDG) [14; 15; 16; 17] receives much attention recently. Sharifi-Noghab _et al._ proposed the first SSDG method which combines the pseudo-label method and meta-learning strategy. Zhou _et al._ adopted stochastic classifier and multi-view consistency to improve the quality of pseudo-labels and generalization of model respectively. However, they assumed that both unlabeled training and testing data have the same label space with labeled training data while we consider a more realistic scenario that the label space of labeled data is a subset of unlabeled training and testing data, i.e., both unlabeled training and testing data are a mixture of _known_ and _unknown classes_.

Out-of-Distribution (OOD) detection aims to detect test samples whose labels are not contained in the label space of training data . Hendrycks and Gimpel  proposed the first effective baseline that uses the maximum softmax probability to identify OOD samples. Post-hoc methods attract much attention because they can be implemented without modifying the training procedure and objective. For example, Liang _et al._ used temperature scaling and input perturbations to make In-Distribution (ID) and OOD samples more separable. Liu _et al._ proposed to replace softmaxconfidence scores with energy scores to detect OOD samples, and considered samples with higher energy to be OOD samples. Hendrycks _et al._  maximized the entropy of the model's predictions on enormous unlabeled OOD samples to improve OOD detection. Yang _et al._  proposed the Unsupervised Dual Grouping (UDG) method that adopts two classification heads to dynamically separate ID and OOD samples in the unlabeled data and optimizes the model with different learning objectives. Since the commonly-used OOD detection benchmarks [28; 29; 31] are based on dataset splitting setup, i.e., one dataset as ID and all the others as OOD, the existing OOD detection methods overfit to low-level dataset statistics instead of learning semantics, which leads to that even the same class from another dataset will be detected as OOD samples .

Semi-Supervised Learning (SSL) is an active research area in the last decade, whose goal is to leverage both labeled and unlabeled data to improve the performance of models and reduce the label cost. The most popular methods include consistency regularization [33; 34] and entropy minimization [35; 36]. The basic idea behind consistency regularization is that the model's outputs of similar samples should be similar. Sajjadi _et al._  proposed the \(\) model that minimizes consistency loss over two random augmentations of one sample to regularize the model. Tarvainen _et al._  proposed the Mean Teacher method that averages Student model weights using Exponential Moving Average (EMA) over training steps as Teacher model to provide more stable targets for the consistency loss optimization of the Student model. Entropy minimization methods encourage models to make low entropy predictions on unlabeled data, so that the learned decision boundary passes through a low-density region rather than a high-density area. Lee  proposed the Pseudo-label method that sets the maximum confidence prediction of unlabeled samples as pseudo-labels, and then trains the model in a supervised way with labeled and pseudo-labeled data. Sohn _et al._  proposed the FixMatch method that combines consistency regularization and entropy minimization. It generates highly confident pseudo-labels on weakly-augmented unlabeled samples, and trains the model to predict the pseudo-labels for the strongly-augmented version of the same samples. OpenMatch  extended FixMatch with OVANet  and open-set soft-consistency regularization to detect outliers in unlabeled data for Open-set Semi-Supervised Learning (OSSL).

## 3 Method

We have multiple source domains \(_{1},_{2},,_{s}\) available for training, and each \(_{k\{1,2,,s\}}\) has one labeled set \(^{l}_{k}=\{(^{(k)}_{i},y^{(k)}_{i})\}_{i=1}^{n^{u}_{k}}\) and one unlabeled set \(^{u}_{k}=\{(^{(k)}_{i})\}_{i=1}^{n^{u}_{k}}\), i.e., \(_{k}=^{l}_{k}^{u}_{k}\). Generally, the size of \(^{l}_{k}\) may be much larger than that of \(^{l}_{k}\), i.e., \(n^{u}_{k} n^{l}_{k}\). The total size of labeled dataset is denoted as \(n^{l}\) and the total size of unlabeled dataset is denoted as \(n^{u}\), i.e., \(n^{l}=_{k=1}^{s}n^{l}_{k}\), and \(n^{u}=_{k=1}^{s}n^{u}_{k}\). The model is evaluated on an unseen target domain \(_{t}=\{(^{(t)}_{i},y^{(t)}_{i})\}_{i=1}^{n_{t}}\). \(^{l}\) is the label set of labeled data \(^{l}=^{l}_{1}^{l}_{2} ^{l}_{s}\), \(^{u}\) is the label set of unlabeled data \(^{u}=^{u}_{1}^{u}_{2} ^{u}_{s}\), and \(^{t}\) is the label set of target domain \(_{t}\). Due to the existence of _unknown classes_ in unlabeled data and target domain, we have \(^{l}^{u}^{t}\). The classes in \(^{l}\) are called _known classes_, the classes in \(^{u}^{l}\) are called _seen unknown classes_ since they are seen in unlabeled data during the training process and the classes in \(^{t}^{u}\) are called _unseen unknown classes_ since they are unseen during the training process. The goal is to learn the model \(f^{*}\) that generalizes well on target domain \(_{t}\) for both _known_ and _unknown classes_, i.e., for each \((^{(t)}_{i},y^{(t)}_{i})_{t}\), if \(y^{(t)}_{i}^{l}\), then \(f^{*}(^{(t)}_{i})=y^{(t)}_{i}\), else \(f^{*}(^{(t)}_{i})=unknown\).

To effectively utilize unlabeled data to improve the generalization and robustness of the model, we should carefully exploit _known_ and _unknown classes_ in them. If known classes are treated as unknown ones, the generalization of the model may decrease significantly; if unknown classes are treated as known ones, the robustness of the model may degrade.

### Detecting Known and Unknown Classes

In order to explore unlabeled data and detect known and unknown classes in them, we rely on the outputs of the deep neural networks and the intuition is that known classes will have higher scores than unknown classes . The deep neural network model \(f_{}=h_{} g_{}\) contains two parts, a feature extractor \(g_{}\) and a \(|^{l}|\)-way linear classifier \(h_{}=[h^{1}_{};;h^{|^{l}|}_{}]\). The model maps each sample \(_{i}\) into a \(|^{l}|\)-dimensional logits \(_{i}=h_{} g_{}(_{i})\), and then feeds \(_{i}\) into a softmax function \(\) to produce the estimated posterior probability \(}_{i}\) (_a.k.a._ score) of each class, i.e., \(}_{i}=(f_{}(_{i}))\). The training is to minimize the cross-entropy loss \(_{CE}()\) between \(}_{i}\) and one-hot label \(_{i}\) over labeled data, i.e., \(^{*}=*{argmin}_{}_{CE}()= *{argmin}_{}-}_{i=1}^{n^{l}}_{c=1}^{ |_{c}^{l}|}_{i}^{c}}_{i}^{c}\), where \(n^{l}\) is the size of labeled data. The gradient of cross-entropy loss w.r.t. linear classifier \(h_{}\) corresponding to class \(c\) is \(_{CE}()}{ h_{}^{c}}=}_{i=1}^{n^{l}}(}_{i}^{c}-_{i}^{c})g_{ }(_{i})\). Due to the relative normalization of the softmax function, the score of \(_{i}\) on class \(c\), i.e., \(}_{i}^{c}\), not only depends on the logit \(_{i}^{c}\), but also depends on other logits \(_{i}^{j},j c\). Thus, the update of \(h_{}^{c}\) is affected by the predictions of other classifiers \(h_{}^{j},j c\). When the logit of \(_{i}\) on target class \(y_{i}\) is large, the scores of \(_{i}\) on non-target class, i.e., \(}_{i}^{c},c y_{i}\), will be small. Since the \(}_{i}^{c}-_{i}^{c}\) is small, the gradient of the non-target classifiers may converge to zero. The parameters of the neural network are updated with the stochastic gradient descent (SGD) algorithm, so, the logits of samples on non-target classifiers induced by \(h_{}^{j},j c\) will not decrease.

In order to update the logits on non-target classifiers induced by \(h_{}^{j},j c\), we replace the softmax function \(\) with the sigmoid function \(\). In this way, we obtain an individual score on each way to indicate the probability that one sample belongs to each class, i.e., \(}_{i}^{c}=(_{i}^{c}),c^{l}\), and we get \(|^{l}|\) one-vs-rest classifiers. With these one-vs-rest classifiers, we can detect known and unknown classes. A straightforward way is to set a fixed threshold \(=0.5\)[39; 38]. Let \(j=*{argmax}_{c}(}_{i}^{c})\), if \(}_{i}^{j}\), it means that it belongs to class \(j\); if \(}_{i}^{j}<\), it means that it is an unknown classes sample. Due to the overconfidence of modern neural networks , it is not proper to use such a fixed threshold of \(0.5\) for all classes. A more reasonable way is to use the class-wise adaptive threshold [41; 42] for each class. To get well-calibrated scores, we first calibrate the classifiers on validation data \(^{v}\) with temperature scaling :

\[^{c}=*{argmin}-_{(_{i},y_{i})^{v }}(y_{i}=c)((_{i}^{c}/^{c} ))+(y_{i} c)(1-(_{i}^ {c}/^{c})), \]

where \(()\) is an indicator function and \(^{c}\) is the temperature for class \(c^{l}\). Note that the validation dataset \(^{v}\) only contains samples of _known classes_ from source domains. And then, we use a two-component beta mixture model to model the score distributions of known classes and unknown classes in an unsupervised way, since it is a more flexible approximator than the Gaussian mixture model . The adaptive thresholds for known classes \(_{knw}^{j}\) (the large one) and unknown classes \(_{unk}^{j}\) (the small one) can be set as the mean values of two fitted beta distributions. Thus, for an unlabeled sample \(_{i}^{u}\), let \(}_{i}^{j}=(_{i}^{j}/^{j})\) be the scaled score, if \(}_{i}^{j}>_{knw}^{j}\), we consider it belongs to _known class_\(j\), i.e. \(_{i}=j\); if \(}_{i}^{j}<_{unk}^{j}\), we consider it belongs to _unknown classes_, i.e. \(_{i}=unknown\); if \(_{unk}^{j}}_{i}^{j}_{knw}^{j}\), it is difficult to determine its label, hence we set its prediction as _null_, i.e. \(_{i}=null\). Since the model is updated in a mini-batch way, the scores predicted by the model many steps ago are less useful for modeling the score distributions. Hence, we utilize the most recent scores predicted by the model for the fitting of the beta mixture model. In particular, for each class \(j\) we maintain a queue \(q^{j}\) with fixed length to record the most recent scaled maximum score of unlabeled sample \(_{i}\) if its maximum score belongs to class \(j\), i.e., \(*{argmax}_{c}(}_{i}^{c})=j\). The overall procedure is summarized in Procedure 1.

### Improving Target Domain Generalization

For labeled training data, we use the following supervised loss,

\[^{l}=-}_{i=1}^{n^{l}}_{c=1}^{|^{l}|} (y_{i}=c)(}_{i}^{c})+^{l}|-1} (y_{i} c)(1-}_{i}^{c}). \]

For unlabeled training data, we should exploit them carefully to improve the unseen target domain generalization after having detected known and unknown classes. We construct the weakly and strongly augmented version of each sample following FixMatch , and calculate the model's scaled confidence score on its weakly augmented version, i.e., \(}_{i,weak}^{c}=(f_{}(T_{weak}(_{i}))/^{c}),c^{l}\). Then, we assign pseudo-label \(_{i}\) to it according to Section 3.1, and force the model's prediction on the strongly augmented version \(}_{i,strong}^{c}=(f_{}(T_{strong}(_{i}))),c^{l}\) to match the pseudo-label. For unlabeled training data predicted as _known classes_ (i.e., \(_{i}^{l}\)), the loss is defined as

\[_{knw}^{u}=-^{u}}_{i=1}^{n_{knw}^{u}}_{c=1} ^{|^{l}|}(_{i}=c)(}_{i,strong}^{ c})+^{l}|-1}(_{i} c)(1-}_{i,strong}^{c}), \]

where \(n_{knw}^{u}\) is the number of samples predicted as _known classes_. For unlabeled training data predicted as _unknown classes_ (i.e., \(_{i}=unknown\)), the loss is defined as

\[_{unk}^{u}=-^{u}}_{i=1}^{n_{unk}^{u}}_{c=1} ^{|^{l}|}(1-}_{i,strong}^{c}), \]

where \(n_{unk}^{u}\) is the number of samples predicted as _unknown classes_.

Besides _known_ and _unknown classes_, there are some unlabeled samples that are predicted as \(null\). In order to fully exploit these data and improve the generalization of the model on the unseen target domain, we adopt Fourier Transformation to disentangle the semantics and styles of the sample [25; 44], and then MixUp  the styles of two randomly sampled samples to augment training data for regularizing the model. Specifically, for each \(\), its Fourier Transformation \(()\) is formulated as:

\[()(u,v)=_{h=0}^{H-1}_{w=0}^{W-1}(h,w)e^ {-J2(u+v)}, \]

where \(H,W\) are the height and width of the sample, and \(J^{2}=-1\). The amplitude component \(()\) and phase component \(()\) are then respectively expressed as

\[()(u,v)=[R^{2}()(u,v)+I^{2}()(u, v)]^{1/2},()(u,v)=[)(u,v)}{ R()(u,v)}], \]

where \(R()\) and \(I()\) represent the real and imaginary part of \((x)\). It is well-known that, the phase component of the Fourier spectrum preserves high-level semantics of the original signal, while the amplitude component contains low-level statistics. Therefore, we could mix up amplitude components of two samples of the same batch while keeping the phase components to generate the augmented samples \(}\)[25; 44], i.e.,

\[}(_{i})=(1-)(_{i})+ (_{i^{}}),}_{i}=^{-1}(}(_{i})*e^{-J*(_ {i})}), \]

where \(^{-1}(x)\) is the inverse Fourier Transformation, \( U(0,1)\), and \(U\) is the uniform distribution.

To avoid overfitting on domain-related low-level statistics, we minimize the consistency regularization loss between the original unlabeled samples and the augmented ones to push the model to pay attention to the high-level semantics of the samples, defined as

\[_{con}^{u}=}_{i=1}^{n^{u}}_{c=1}^{| ^{l}|}|}_{i}^{c}(_{i})-}_{i}^{c}( }_{i})|^{2}, \]

where \(n^{u}\) is the number of unlabeled data. The overall loss of the training process is formulated as:

\[=^{l}+_{1}_{knw}^{u}+_{2} _{unk}^{u}+_{3}_{con}^{u}, \]

where \(_{1}\), \(_{2}\) and \(_{3}\) are hyper-parameters to balance each loss. During the warm-up process, the model is trained only with labeled data, i.e., \(_{1}=_{2}=_{3}=0\). The whole process is summarized in Algorithm 1 (the framework figure can be found in Appendix A of the supplementary material), where the domain index is omitted for brevity.

## 4 Experiment

### Datasets

We use PACS , Office-Home  and miniDomainNet  datasets in the experiments. **PACS** consists of four domains corresponding to four different image styles, including Photo (P), Art painting (A), Cartoon (C) and Sketch (S), and the four domains have the same label set of 7 classes, and contain 9,991 images in total; **OfficeHome** consists of images from four different domains: Artistic (A), Clipart (C), Product (P) and Real-World (R), and has a large domain gap and around 15,500 images of 65 classes; **miniDomainNet** is a subset of DomainNet  and has four domains including 18,703 images of Clipart (C), 31,202 images of Painting (P), 65,609 images of Real (R) and 24,492 images of Sketch (S), and it has 126 classes and maintains the complexity of the original DomainNet.

We adopt the common leave-one-domain-out protocol [12; 16]: three domains are used as the source domains and the remaining one as the target domain. Similar to [23; 38] we split the classes into _known_ and _unknown classes_, specifically we split the original label set into 3:2:2, 25:20:20 and 42:42:42 (_known classes_, _seen unknown classes_ and _unseen unknown classes_) in PACS , Office-Home  and miniDomainNet  respectively in alphabetical order of the class name. On each source domain, 10 labeled samples of each _known class_ are randomly sampled to construct the labeled data, and the remaining samples of _known classes_ and _seen unknown classes_ construct the unlabeled data. All samples on the target domain are used for evaluation.

### Compared Methods

We compare our method with various DG methods, OOD detection methods and SSL methods. **DeepAll** naively puts labeled data from all source domains together, and trains the model with Empirical Risk Minimization (ERM); **DAML** conducts meta-learning over augmented domains to learn open-domain generalizable representations for OpenDG problem; **UDG** adopts two classification heads to dynamically separate ID and OOD samples in the unlabeled data and optimizes the model with different learning objectives; **FixMatch** generates highly confident pseudo-labels on weakly-augmented unlabeled samples, and trains the model to predict the pseudo-labels for the strongly-augmented version of the same samples; **OpenMatch** extends FixMatch with OVANet  and open-set soft-consistency regularization to detect outliers in unlabeled data for OSSL;

**StyleMatch** extends FixMatch with stochastic classifier and multi-view consistency to improve the quality of pseudo-labels and generalization of model respectively for SSDG.

### Setup

Most hyper-parameters follow the setting in  for a fair comparison. The ImageNet-pretrained ResNet-18  is used as the CNN backbone, and the linear classifiers is implemented with stochastic classifiers. The initial learning rate of SGD optimizer is set to 0.003 for the pre-trained backbone and 0.01 for the randomly initialized stochastic classifier, both decaying following the cosine annealing rule. The running epochs are 40, 20 and 20 for PACS, OfficeHome and miniDomainNet respectively. For each mini-batch, we randomly sample 16 labeled samples and 16 unlabeled samples from each source domain. We set \(_{1}=1.0\) on PACS, \(_{1}=0.4\) on OfficeHome and \(_{1}=0.1\) on miniDomainNet. We set \(_{2}=0.4\) and \(_{3}=1.0\) for all three datasets. We evaluate the accuracy on _known classes_ and AUROC on _unknown classes_ of the methods with 3 different random seeds, and report the average results. More implementation details can be found in Appendix B of the supplementary material.

    &  \\ Target Domain & Art & Cartoon & Photo & Sketch & Average \\  DeepAll & 62.96 / 60.06 & 53.41 / 58.15 & 79.17 / 71.26 & 48.60 / 50.16 & 61.03 / 59.91 \\ UDG  & 42.98 / 49.83 & 46.92 / 48.52 & 58.75 / 57.28 & 38.82 / 45.21 & 46.87 / 50.21 \\ DAML  & 42.07 / 50.27 & 57.74 / 54.80 & 42.87 / 54.00 & 45.29 / 47.20 & 46.99 / 51.57 \\ FixMatch  & 81.32 / 68.67 & 61.85 / 56.34 & 85.63 / 64.87 & 76.39 / 48.01 & 76.30 / 59.47 \\ OpenMatch  & 83.28 / 68.97 & 75.39 / 66.60 & 91.45 / 68.37 & 58.05 / 47.42 & 77.04 / 62.84 \\ StyleMatch  & 82.66 / 63.35 & 71.95 / 56.86 & 90.81 / 67.40 & 77.34 / 43.33 & 80.69 / 57.73 \\   \\ Target Domain & Art & Clipart & Product & Real-World & Average \\  DeepAll & 61.95 / 69.97 & 50.80 / 60.96 & 75.23 / 71.38 & 84.55 / 76.63 & 68.13 / 69.73 \\ UDG  & 52.25 / 60.71 & 41.97 / 55.58 & 63.64 / 64.74 & 72.24 / 65.90 & 57.52 / 61.73 \\ DAML  & 45.73 / 62.96 & 43.98 / 55.46 & 58.50 / 67.09 & 64.46 / 67.75 & 53.17 / 63.31 \\ FixMatch  & 65.25 / 67.60 & 59.32 / 62.18 & 73.31 / 67.72 & 82.35 / 73.16 & 70.06 / 67.67 \\ OpenMatch  & 64.95 / 69.27 & 55.82 / 61.60 & 75.20 / 72.93 & 81.76 / 75.71 & 69.43 / 69.90 \\ StyleMatch  & 67.83 / 67.40 & 63.02 / 60.15 & 75.46 / 69.16 & 84.79 / 74.44 & 72.77 / 67.79 \\   \\ Target Domain & Clipart & Painting & Real & Sketch & Average \\  DeepAll & 52.58 / 66.31 & 52.13 / 62.96 & 66.10 / 73.17 & 44.15 / 64.90 & 53.74 / 66.83 \\ UDG  & 56.30 / 68.49 & 49.51 / 61.47 & 61.70 / 70.21 & 36.99 / 57.25 & 51.12 / 64.36 \\ DAML  & 56.16 / 67.16 & 50.32 / 65.62 & 57.23 / 69.14 & 46.52 / 65.15 & 52.55 / 66.77 \\ FixMatch  & 59.71 / 62.83 & 59.71 / 62.37 & 65.63 / 63.58 & 64.78 / 64.90 & 62.01 / 63.42 \\ OpenMatch  & 64.53 / 72.70 & 61.55 / 69.80 & **70.61** / 74.87 & 61.40 / 71.30 & 64.52 / 72.17 \\ StyleMatch  & 62.42 / 63.63 & 61.23 / 62.21 & 66.02 / 62.58 & 65.44 / 63.46 & 63.77 / 62.97 \\  CWAEE & **66.68** / **73.38** & **65.65** / **73.07** & 69.86 / **75.98** & **66.36** / **74.96** & **67.14** / **74.35** \\   

Table 1: Leave-one-domain-out results of _known classes_ accuracy (left of the slash) and _unknown classes_ AUROC (right of the slash) on PACS, OfficeHome and miniDomainNet.

   Dataset & PACS & OfficeHome & miniDomainNet \\  DeepAll & 58.28 / 60.89 & 68.48 / 71.04 & 69.21 / 67.89 \\ UDG & 50.73 / 49.08 & 63.35 / 60.07 & 64.26 / 64.33 \\ DAML & 50.45 / 52.94 & 62.21 / 64.46 & 67.20 / 66.36 \\ FixMatch & 53.84 / 67.87 & 64.05 / 71.39 & 58.56 / 67.45 \\ OpenMatch & 55.61 / 73.73 & 68.12 / 71.72 & 72.86 / 71.55 \\ StyleMatch & 49.77 / 68.91 & 63.46 / 72.27 & 56.57 / 68.31 \\  CWAEE & **84.09** / **74.53** & **74.57** / **75.87** & **76.81** / **72.31** \\   

Table 2: Leave-one-domain-out average AUROC of _seen_ (left of the slash) and _unseen unknown classes_ (right of the slash) on PACS, OfficeHome and miniDomainNet.

### Results

The results are summarized in Table 1 (the results with standard deviations can be found in Tables 6 and 7 in Appendix C of the supplementary material). From the results, it can be found that our method consistently outperforms the compared methods on all datasets. Although the UDG and DAML methods are developed for _unknown class_ detection on the unseen target domain, their performance is much worse than ours. FixMatch has poor classification performance when evaluated on an unseen target domain, due to not considering the domain shifts problem. The performance of our method is much better than that of OpenMatch in most cases since it depends on the fixed thresholds to detect _unknown classes_ in unlabeled data. StyleMatch has a worse performance than ours since it treats all unlabeled samples as one of the known classes. In order to evaluate the detection performance of methods on _seen_ and _unseen unknown classes_ respectively, we also report the AUROC of _seen_ and _unseen unknown classes_ on the target domain in Table 2 (the results with standard deviations can be found in Table 8 in Appendix C of the supplementary material). It can be found that our method has better detection performance on both _seen_ and _unseen unknown classes_.

### Ablation Study

In order to provide additional insight into what makes our method successful, we conduct ablation experiments on OfficeHome.

#### 4.5.1 Effectiveness of Used Modules

Our method contains several modules, including supervised loss (Eq.2), unsupervised loss on _known classes_ (Eq.3), unsupervised loss on _unknown classes_ (Eq.4), class-wise adaptive thresholds (Procedure 1) and consistency regularization loss (Eq.8). The results with standard deviations are presented in Table 3. With the supervised loss, our method trains the model only with labeled samples. After detecting known and unknown classes with the fixed thresholds (i.e., \(_{}^{1:||}=_{}^{1:|| }=0.5\)) and trained with the unsupervised loss on _known_ and _unknown classes_, the performance can be improved with these detected samples. By replacing the fixed thresholds with class-wise adaptive thresholds, the performance can be further boosted. With all the used modules, we can get the best performance.

To better show the effectiveness of the class-wise adaptive thresholds, we compare the quality of pseudo-labels when trained with fixed thresholds and the class-wise adaptive thresholds. The results are depicted in Figure 2. It can be found that the accuracy of pseudo-labels assigned by our method is higher than that of compared method on _known classes_ and _unknown classes_ in all 4 domains, which means that our method has better performance on _known classes_ classification and _unknown classes_ detection.

#### 4.5.2 Sensitivity to Number of Seen Unknown Classes

In order to study the sensitivity of our method to the number of _seen unknown classes_, we compare our method with existing SSL methods under different numbers of _seen unknown classes_ (\(|^{u}|\) varies from 0 to 30), and the results are depicted in Figure 3. It can be found that our

Figure 3: The results with various numbers of seen unknown classes.

   Ablation & Accuracy / AUROC \\  Supervised loss & 68.13\(\)0.88 / 69.73\(\)1.01 \\ + Unsupervised loss on known classes & 72.30\(\)0.18 / 64.47\(\)0.60 \\ with fixed thresholds & 72.38\(\)0.16 / 65.16\(\)0.43 \\ with fixed thresholds & 73.70\(\)0.39 / 74.44\(\)0.92 \\ + Consistency regularization loss & **74.34\(\)0.35 / 75.20\(\)0.74** \\   

Table 3: Ablation study on the used modules.

Figure 2: The accuracy of pseudo-labels on unlabeled samples.

method outperforms the compared methods. With the increase in the number of _seen unknown classes_, the AUROC on _unknown classes_ obtained by our method increases while the accuracy on _known classes_ is maintained.

#### 4.5.3 Sensitivity to Number of Labeled Samples

In order to study the sensitivity of our method to the number of labeled samples, we compare our method with existing methods under different numbers of labeled samples (5, 10 and 20 labels of each _known class_ on each domain) and the results are summarized in Table 4 (the results with standard deviations can be found in Table 9 in Appendix C of the supplementary material). It can be found that our method outperforms compared methods under different numbers of labeled samples. The performance of our method under extremely few labels (i.e., 5 labels of each class on each domain) is promising, since it can exploit _known_ and _unknown classes_ in unlabeled data in a more reasonable way.

#### 4.5.4 Histograms of Confidence Scores

The empirical probability density function of confidence scores of unlabeled data in different queues and different epochs are depicted in Figure 4. Here, we report class 0 and class 1 in different epochs as examples, i.e., Figure 3(a) vs 3(b), Figure 3(c) vs 3(d), which shows that the confidence scores have different distributions in the different training epochs. It can be found that our method CWAE can find the adaptive thresholds for _known_ and _unknown classes_.

## 5 Conclusion

In this paper, we focus on a more realistic semi-supervised domain generalization scenario, where _known classes_ and _seen unknown classes_ are mixed in unlabeled training data while _known classes_, _seen unknown classes_ and _unseen unknown classes_ are mixed in testing data. In order to utilize unlabeled training data, we detect known and unknown classes in them with the class-wise adaptive thresholds based on one-vs-rest classifiers. We adopt consistency regularization on augmented training samples based on Fourier Transformation to improve the generalization on the unseen target domain. The experimental results conducted on different real-world datasets show that our method outperforms the existing state-of-the-art methods.

## Broader Impacts

Our work provides an effective method for semi-supervised domain generalization with _known_ and _unknown classes_. We believe our work will be beneficial for domain generalization projects with few labeled data and many unlabeled data, and does not have negative societal impacts.

   \# Labels & 5 & 10 & 20 \\  DeepAll & 62.87 / 67.77 & 68.13 / 69.73 & 72.17 / 70.83 \\ UDG & 47.02 / 55.68 & 57.52 / 61.73 & 65.31 / 67.66 \\ DAML & 52.67 / 64.15 & 53.17 / 63.31 & 50.95 / 63.07 \\ FixMatch & 66.90 / 64.29 & 70.06 / 67.67 & 71.92 / 70.21 \\ OpenMatch & 66.24 / 69.81 & 69.43 / 69.90 & 70.14 / 69.81 \\ StyleMatch & 70.16 / 63.57 & 72.77 / 67.79 & 75.36 / 70.45 \\  CWAE & **71.24** / **73.23** & **74.34** / **75.20** & **76.09** / **73.96** \\   

Table 4: Average accuracy / AUROC on OfficeHome with different numbers of labeled samples.

Figure 4: The empirical p.d.f. of the confidence scores of unlabeled data.