# Fractal Landscapes in Policy Optimization

Tao Wang

UC San Diego

taw003@ucsd.edu &Sylvia Herbert

UC San Diego

sherbert@ucsd.edu &Sicun Gao

UC San Diego

sicung@ucsd.edu

###### Abstract

Policy gradient lies at the core of deep reinforcement learning (RL) in continuous domains. Despite much success, it is often observed in practice that RL training with policy gradient can fail for many reasons, even on standard control problems with known solutions. We propose a framework for understanding one inherent limitation of the policy gradient approach: the optimization landscape in the policy space can be extremely non-smooth or fractal for certain classes of MDPs, such that there does not exist gradient to be estimated in the first place. We draw on techniques from chaos theory and non-smooth analysis, and analyze the maximal Lyapunov exponents and Holder exponents of the policy optimization objectives. Moreover, we develop a practical method that can estimate the local smoothness of objective function from samples to identify when the training process has encountered fractal landscapes. We show experiments to illustrate how some failure cases of policy optimization can be explained by such fractal landscapes.

## 1 Introduction

Deep reinforcement learning has achieved much success in various applications , but they also often fail, especially in continuous spaces, on control problems that other methods can readily solve. The understanding of such failure cases is still limited. For instance, the training process of reinforcement learning is unstable and the learning curve can fluctuate during training in ways that are hard to predict. The probability of obtaining satisfactory policies can also be inherently low in reward-sparse or highly nonlinear control tasks. Existing analysis of the failures focuses on limitations of the sampling and optimization algorithms, such as function approximation errors , difficulty in data collection , and aggressive updates in the policy space . There has not been much study of potentially deeper causes of failures that may be inherent in the formulation of policy optimization problems.

Motivated by the common observation that small updates in the policy parameters can significantly change the performance, we analyze the smoothness of the optimization landscapes in policy optimization. Drawing on chaos theory, we introduce the concept of maximal Lyapunov exponent (MLE)  to the RL setting to measure the exponential rate of trajectory divergence in MDP. It seems contradictory that a trajectory in chaotic systems can be both exponentially divergent and uniformly bounded at the same time, and we will show that these two conflicting facts combine to yield the fractal structure in the optimization landscape. Intuitively, the objective function is non-differentiable when the rate of trajectory divergence exceeds the decay rate of discount factor. Furthermore, this finding indicates that the fluctuations observed in the loss curve are not just due to the numerical or sampling error but rather reflect the intrinsic properties of the corresponding MDP.

We should emphasize that the fractal landscapes that we will demonstrate are stronger than various existing results on the non-smoothness . Most non-smooth objectives that have been studied still assume is local Lipschitz continuity or piecewise smoothness that implies differentiability _almost everywhere_ (such as \(f(x)=|x|\)). Instead, by showing that the loss landscape of policy optimization can be fractal, we demonstrate the absence of descent directions, which causes the failure of first-ordermethods in general. Since such behavior is an intrinsic property of the underlying dynamical systems, the results show fundamental limitations of policy gradient methods on certain classes of MDPs.

The paper is organized as follows. In Section 3 and 4, we will introduce the preliminaries and develop the theory for deterministic policies. In particular, we show that the optimization landscape is fractal, even when all elements within the MDP are deterministic. Next, we consider stochastic policies and provide an example to show how non-smoothness can still occur if without additional assumptions. In Section 5, we turn the theoretical analysis into a practical sampling-based method for estimating the Holder exponent to determine whether the optimization objective is differentiable at a specific parameter vector. It can also indicate if the training process has encountered fractal regions by comparing the regression slope with some fixed threshold. In Section 6, we show experiments that demonstrate and compare the landscapes of different MDPs.

## 2 Related work

**Policy gradient and Q-learning methods.** Policy gradient methods [33; 41] formulate RL as an optimization problem in the parameter space, with many variations such as natural policy gradient , deterministic policy gradient , deep deterministic policy gradient , trust region policy optimization  and proximal policy optimization , were proposed. As all of these algorithms aim to estimate the gradient of the objective function over the policy parameters, they become ill-posed when the objective is non-differentiable, which is the focus of our analysis.

Another popular approach for model-free RL is Q-learning methods, which approximate the Q-function of the policy at each step [22; 40]. As neural networks become more and more popular, they are employed as function approximators in deep Q-learning algorithms [9; 13; 37]. Since the foundation of Q-learning methods is established upon the estimation of value functions, a poor approximation can completely ruin the entire training process. In this paper, we will show that the value functions in a certain class of MDPs exhibit significant non-smoothness, making them challenging to represent using existing methods.

**Chaos in machine learning.** Chaotic behaviors due to randomness in the learning dynamics have been reported in other learning problems [6; 21; 25]. For instance, when training recurrent neural networks for a long period, the outcome behaves like a random walk due to the problems of vanishing and the exploding gradients . It served as motivation for the work , which points out that the chaotic behavior in finite-horizon model-based reinforcement learning problems may be caused by long chains of nonlinear computation. A similar observation was made in . However, we show that in RL, the objective function is provably smooth if the time horizon is finite and the underlying dynamics is differentiable. Instead, we focus on the general context of infinite-horizon problems in MDPs, in which case the objective function can become non-differentiable.

**Loss landscape of policy optimization.** It has been shown that the objective functions in finite state-space MDPs are smooth [1; 42], which enables the use of gradient-based methods and direct policy search. It also explains why the classical RL algorithms in  are provably efficient in finite space settings. Also, such smoothness results can be extended to some continuous state-space MDPs with special structures. For instance, the objective function in Linear Quadratic Regulator (LQR) problems is almost smooth  as long as the cost is finite. Similar results are obtained for the \(_{2}/_{}\) problem . For the robust control problem, although the objective function may not be smooth, it is locally Lipschitz continuous, which implies differentiability _almost everywhere_, and further leads to global convergence of direct policy search . There is still limited theoretical study of loss landscapes of policy optimization for nonlinear and complex MDPs. We aim to partially address this gap by pointing out the possibility that the loss landscape can be highly non-smooth and even fractal, which is far more complex than the previous cases.

## 3 Preliminaries

### Dynamical Systems as Markov Decision Processes

We consider Markov Decision Processes (MDPs) that encode continuous control problems for dynamical systems defined by difference equations of the form:

\[s_{t+1}=f(s_{t},a_{t}),\] (1)where \(s_{t}^{n}\) is the state at time \(t\), \(s_{0}\) is the initial state and \(a_{t}_{}(|s_{t})^{m}\) is the action taken at time \(t\) based on a policy parameterized by \(^{p}\).We assume that both the state space \(\) and the action space \(\) are compact. The objective function of the RL problem to minimize is defined by \(V^{_{}}\) of policy \(_{}\):

\[J()=V^{_{}}(s_{0})=_{a_{t}_{}(|s_{ t})}[_{t=0}^{}^{t}c(s_{t},a_{t})],\] (2)

where \((0,1)\) is the discount factor and \(c(s,a)\) is the cost function. The following assumptions are made throughout this paper:

* (A.1) \(f:^{n}^{m}^{n}\) is Lipschitz continuous over any compact domains (i.e., locally Lipschitz continuous);
* (A.2) The cost function \(c:^{n}^{m}\) is non-negative and locally Lipschitz continuous everywhere;
* (A.3) The state space is closed under transitions, i.e., for any \((s,a)\), the next state \(s^{}=f(s,a)\).

### Policy gradient methods

Policy gradient methods estimate the gradient of the objective \(J()\) with respect to the parameters of the policies. A commonly used form is

\[ J()=_{a_{t}_{}(|s_{t})}[_{ }_{}(a_{t}|s_{t})\;A^{_{}}(s_{t},a_{t})],\] (3)

where \(_{}(|)\) is a stochastic policy parameterized by \(\). \(A^{_{}}(s,a)=Q^{_{}}(s,a)-V^{_{}}(s)\) is the advantage function often used for variance reduction and \(Q^{_{}}(,)\) is the \(Q\)-value function of \(_{}\). The theoretical guarantee of the convergence of policy gradient methods is typically established by the argument that the tail term \(^{t}\;_{}V^{_{}}(s)\) diminishes as \(t\) increases, for any \(s\). For such claims to hold, two assumptions are needed:

* \(_{}V^{_{}}(s)\) exists and is continuous for all \(s\);
* \(\|_{}V^{_{}}(s)\|\) is uniformly bounded over \(\).

The second assumption is automatically satisfied if the first assumption holds in the case that \(\) is either finite or compact. However, as we will see in Section 4 and 6, the existence of \(_{}V^{_{}}()\) may fail in many continuous MDPs even if \(\) is compact, which challenges the fundamental well-posedness of policy gradient methods.

### Maximal Lyapunov Exponents

Behaviors of chaotic systems have sensitive dependence on their initial conditions. To be precise, consider the system \(s_{t+1}=F(s_{t})\) with initial state \(s_{0}^{n}\), and suppose that a small perturbation \( Z_{0}\) is made to \(s_{0}\). The divergence from the original trajectory of the system under this perturbation at time \(t\), say \( Z(t)\), can be estimated by \(\| Z(t)\| e^{ t}\| Z_{0}\|\) with some \(\) that is called the Lyapunov exponent. For chaotic systems, Lyapunov exponents are typically positive, which implies an exponential divergence rate of the separation of nearby trajectories . Since the Lyapunov exponent at a given point may depend on the direction of the perturbation \( Z_{0}\), and we are interested in identifying the largest divergence rate, the maximal Lyapunov exponent (MLE) is formally defined as follows:

**Definition 3.1**.: _(Maximal Lyapunov exponent) For the dynamical system \(s_{t+1}=F(s_{t}),s_{0}^{n}\), the maximal Lyapunov exponent \(_{}\) at \(s_{0}\) is defined as the largest value such that_

\[_{}=_{t}_{\| Z_{0}\| 0} \|}.\] (4)

Note that systems with unstable equilibria, not necessarily chaotic, can have positive MLEs.

### Fractal Landscapes

The Hausdorff dimension is the most fundamental concept in fractal theory. We first introduce the concept of \(\)-cover and Hausdorff measure:

**Definition 3.2**.: _(\(\)-cover) Let \(\{U_{i}\}\) be a countable collection of sets of diameter at most \(\) (i.e. \(|U_{i}|=\{\|x-y\|:x,y U_{i}\}\)) and \(F^{N}\), then \(\{U_{i}\}\) is a \(\)-cover of \(F\) if \(F_{i=1}^{}U_{i}\)._

**Definition 3.3**.: _(Hausdorff measure) For any \(F^{N}\) and \(s 0\), let_

\[^{s}_{}(F)=\{_{i=1}^{}|U_{i}|^{s}:\{U_{i}\} { is a $$-cover of $F$}\}.\]

_Then we call the limit \(^{s}(F)=_{ 0}^{s}_{}(F)\) the \(s\)-dimensional Hausdorff measure of \(F\)._

The definition of Hausdorff dimension follows immediately:

**Definition 3.4**.: _(Hausdorff dimension) Let \(F^{N}\) be a subset, then its Hausdorff dimension_

\[_{H}F=\{s 0:^{s}(F)=0\}=\{s 0:^{s}(F)= \}.\]

And we introduce the notion of \(\)-Holder continuity that extends the concept of Lipschitz continuity:

**Definition 3.5**.: _(\(\)-Holder continuity) Let \(>0\) be a scalar. A function \(g:^{N}\) is \(\)-Holder continuous at \(x^{N}\) if there exist \(C>0\) and \(>0\) such that_

\[|g(x)-g(y)| C\|x-y\|^{}\]

_for all \(y(x,)\), where \((x,)\) denotes the open ball of radius \(\) centered at \(x\)._

The definition reduces to Lipschitz continuity when \(=1\). A function is not differentiable, if the largest Holder exponent at a given point is less than \(1\). Just as smoothness is commonly associated with Lipschitz continuity, fractal behavior is closely related to Holder continuity. In particular, for an open set \(F^{k}\) and a continuous mapping \(:F^{p}\) with \(p>k\), the image set \((F)\) is fractal when its Hausdorff dimension \(_{H}(F)\) is strictly greater than \(k\), which occurs when \(:F^{p}\) is \(\)-Holder continuous with exponent \(<1\):

**Proposition 3.1**.: _() Let \(F^{k}\) be a subset and suppose that \(:F^{p}\) is \(\)-Holder continuous where \(>0\), then \(_{H}(F)_{H}F\)._

It implies that if the objective function is \(\)-Holder for some \(<1\), its loss landscape \(_{J}=\{(,J())^{N+1}:^{N}\}\) can be fractal. Further discussion of the theory on fractals can be found in Appendix C.

## 4 Fractal Landscapes in the Policy Space

In this section, we will show that the objective \(J()\) in policy optimization can be non-differentiable when the system has positive MLEs. We will first consider Holder continuity of \(V^{_{}}()\) and \(J()\) with deterministic policies in 4.1 and 4.2, and then discuss the case of stochastic policies in 4.3.

### Holder Exponent of \(V^{_{}}()\)

We first consider a deterministic policy \(_{}\) that maps states to actions \(a=_{}(s)\) instead of distributions. Consider a fixed policy parameter \(^{p}\) such that the MLE of (1), namely \(()\), is greater than \(-\). Let \(s^{}_{0}\) be another initial state that is close to \(s_{0}\), i.e., \(=\|s^{}_{0}-s_{0}\|>0\) is small enough. According to the assumption (A.3) and the compactness of the state space, we can find a constant \(M>0\) such that both \(\|s_{t}\| M\) and \(\|s^{}_{t}\| M\) for all \(t\), where \(\{s_{t}\}_{t=1}^{}\) and \(\{s^{}_{t}\}_{t=1}^{}\) are the trajectories starting from \(s_{0}\) and \(s^{}_{0}\), respectively. Motivated by (4), we further make the following assumptions:

* (A.4) There exists \(K_{1}>0\) such that \(\|s^{}_{t}-s_{t}\| K_{1} e^{()t}\) for all \(t\) and \(=\|s^{}_{0}-s_{0}\|>0\).
* (A.5) The policy \(:^{N}^{n}^{m}\) is locally Lipschitz continuous everywhere.

We then have following theorem, and it provides a lower bound for the Holder exponent of \(J\) whose detailed proof can be found in Appendix B.1.

**Theorem 4.1**.: _(Non-smoothness of \(V^{_{}}\)) Assume (A.1)-(A.5) and the parameterized policy \(_{}()\) is deterministic. Let \(()\) denote the MLE of (1) at \(^{N}\). Suppose that \(()>-\), then \(V^{_{}}()\) is \(\)-Holder continuous at \(s_{0}\)._

Proof sketch of Theorem 4.1:Suppose that \(p(0,1]\) is some constant for which we would like to prove that \(V^{_{}}(s)\) is \(p\)-Holder continuous at \(s=s_{0}\), and here we take \(p=-\).

According to Definition 3.5, it suffices to find some \(C^{}>0\) such that \(|V^{_{}}(s^{}_{0})-V^{_{}}(s_{0})| C^{} ^{p}\) when \(=\|s_{0}-s^{}_{0}\| 1\). Consider the relaxed form

\[|V^{_{}}(s^{}_{0})-V^{_{}}(s_{0})|_{t=0}^{ }^{t}|c(s_{t},_{}(s_{t}))-c(s^{}_{t},_{}( s^{}_{t}))| C^{}^{p}.\] (5)

Now we split the entire series into three parts as shown in Figure 1: the sum of first \(T_{2}\) terms, the sum from \(t=T_{2}+1\) to \(T_{3}-1\), and the sum from \(t=T_{3}\) to \(\). First, applying (A.4) to the sum of the first \(T_{2}\) terms yields

\[_{t=0}^{T_{2}}^{t}|c(s_{t},_{}(s_{t}))-c(s^{}_{t}, _{}(s^{}_{t}))|} }{1-}K_{1}K_{2},\] (6)

where \(K_{2}>0\) is the Lipschitz constant obtained by (A.2) and (A.5). If we wish to bound the right-hand side of (6) by some term of order \((^{p})\) when \( 1\), the length \(T_{2}()\) should satisfy

\[T_{2}() C_{1}+(),\] (7)

where \(C_{1}>0\) is some constant independent of \(p\) and \(\).

Next, for the sum of the tail terms in \(V^{_{}}()\) starting from \(T_{3}\), it is automatically bounded by

\[_{t=T_{3}}^{}^{t}|c(s_{t},_{}(s_{t}))-c(s^{}_{ t},_{}(s^{}_{t}))|e^{T_{3}}}{1-},\] (8)

where \(M_{2}=_{s}c(s,_{}(s))\) is the maximum of continuous function \(c(,_{}())\) over the compact domain \(\) (and hence exists). if we bound the right-hand side of (8) by a term of order \((^{p})\), it yields

\[T_{3}() C_{2}+(),\] (9)

for some independent constant \(C_{2}>0\). Since the sum of (6) and (8) provides a good estimate of \(V^{_{}}\) only if \(T_{3}()-T_{2}() N_{0}\) for some \(N_{0}>0\) as \( 0\), otherwise there would be infinitely many terms in the middle as \( 0\) that cannot be controlled by any \((^{p})\) terms. In this case, we have

\[(C_{2}-C_{3})+(-) () N_{0},\] (10)

as \(()-\), which implies that the slopes satisfy the inequality

\[- 0,\] (11)

Figure 1: An illustration of the two series (7) and (9) that need to cover the entire \(\) when \( 0\).

where the equality holds when \(p=-\). Thus, \(V^{_{}}(s)\) is \(-\)-Holder continuous at \(s=s_{0}\).

On the other hand, the following counterexample shows that Theorem 4.1 has provided the strongest Holder-continuity result for \(V^{_{}}(s)\) at \(s=s_{0}\) under the assumptions (A.1)-(A.5):

**Example 4.1**.: _Consider a one-dimensional MDP \(s_{t+1}=f(s_{t},a_{t})\) where_

\[f(s,a)=-1,&a-1,\\ a,&-1<a<1,\\ 1,&a 1,\] (12)

_with state space \(=[-1,1]\) and cost function \(c(s,a)=|s|\). Let the policy be linear, namely \(_{}(s)= s\) and \(\). It can be verified that all assumptions (A.1)-(A.5) are satisfied. Now let \(s_{0}=0\) and \(>1\), then applying (4) directly yields_

\[()=_{t}_{\| Z_{0}\| 0} \|}=_{t}_{\|  Z_{0}\| 0}\|^{t}}{\| Z_{0}\| }=.\]

_Let \(>0\) be sufficiently small, then_

\[V^{_{}}()=_{t=0}^{}^{t}^{t}=_{ t=0}^{T_{0}()}^{t}^{t}+_{t=T_{0}()}^{} ^{t}()}}{1-}\]

_where \(T_{0}()=1+\) and \(\) is the flooring function. Therefore, we have_

\[|V^{_{}}()-V^{_{}}(0)|=V^{_{}}() +1}}{1-}=^{}.\]

**Remark 4.1**.: _Another way to see why it is theoretically impossible to prove \(p\)-Holder continuity for \(V^{_{}}\) for any \(p>-\): notice that the inequality (10) no longer holds as \(-\) since_

\[-<0.\]

_Thus, \(p=\) is the largest Holder exponent of \(V^{_{}}()\) that can be proved in the worst case._

**Remark 4.2**.: _The value function \(V^{_{}}(s)\) is Lipschitz continuous at \(s=s_{0}\) when the maximal Lyapunov exponent \(()<-\), since there exists a constant \(K^{}\) such that_

\[|V^{_{}}(s_{0}^{})-V^{_{}}(s_{0})| _{t=0}^{}^{t}|c(s_{t},_{}(s_{t}))-c( s_{t}^{},_{}(s_{t}^{}))|\] \[_{t=0}^{}^{t}K^{} e^{( )t}\] \[ K^{}_{t=0}^{}e^{(()+ )t}\] \[}{1-e^{(()+)}}\]

_where \(=\|s_{0}-s_{0}^{}\|>0\) is the difference in the initial state._

### Holder Exponent of \(J()\)

The following lemma establishes a direct connection between \(J()\) and \(J(^{})\) through value functions:

**Lemma 4.1**.: _Suppose that \(,^{}^{p}\), then_

\[V^{_{^{}}}(s_{0})-V^{_{}}(s_{0})=_{t=0}^{} ^{t}(Q^{_{}}(s_{t}^{^{}},_{^{}}(s_{t }^{^{}}))-V^{_{}}(s_{t}^{^{}}))\]

_where \(\{s_{t}^{^{}}\}_{t=0}^{}\) is the trajectory generated by the policy \(_{^{}}()\)._The proof can be found in the Appendix B.2. Notice that indeed we have \(J(^{})=V^{_{^{}}}(s_{0})\) and \(J()=V^{_{}}(s_{0})\), substituting with these two terms in the previous lemma and doing some calculations lead to the following main theorem whose proof can be found in the Appendix B.3:

**Theorem 4.2**.: _(Non-smoothness of \(J\)) Assume (A.1)-(A.5) and the parameterized policy \(_{}()\) is deterministic. Let \(()\) denote the MLE of (1) at \(^{p}\). Suppose that \(()>-\), then \(J()\) is \(\)Holder continuous at \(\)._

**Remark 4.3**.: _In fact, the set of assumptions (A.1)-(A.5) is quite general and does not exclude the case of constant cost functions \(c(s,a) const\), which always results in a smooth landscape regardless of the underlying dynamics, even though they are rarely used in practice. However, recall that the \(\)Holder continuity is a result of exponential divergence of nearby trajectories, when a cost function can continuously distinguish two separate trajectories (e.g., quadratic costs) with a discount factor close to \(1\), the landscape will be fractal as shown in Section 6. Another way to see it is to look into the relaxation in (5) where the Holder continuity is obtained from the local Lipschitz continuity of \(c(s,a)\), i.e., \(|c(s,_{}(s))-c(s^{},_{}(s^{}))| K_{2}\|s-s^ {}\|\). Therefore, the Holder continuity is tight if for any \(>0\), there exists \(s^{}_{0}(s_{0},)\) such that \(|c(s_{t},_{}(s_{t}))-c(s^{}_{t},_{}(s^{}_{t}))|  K_{3}\|s_{t}-s^{}_{t}\|\) with some \(K_{3}>0\) for all \(t\). We will leave the further investigation for future studies._

The following example illustrates how the smoothness of loss landscape changes with \(()\) and \(\):

**Example 4.2**.: _(Logistic model) Consider the following MDP:_

\[s_{t+1}=(1-s_{t})a_{t}, s_{0}=0.9,\] (13)

_where the policy \(a_{t}\) is given by deterministic linear function \(a_{t}=_{}(s_{t})= s_{t}\). The objective function is defined as \(J()=_{t=0}^{}^{t}\)\((s_{t}^{2}+0.1\)\(a_{t}^{2})\) where \((0,1)\) is the discount factor. It is well-known that (13) begins to exhibit chaotic behavior with positive MLEs (as shown in Figure 1(a)) when \( 3.3\), so we plot the graphs of \(J()\) for different discount factors over the interval \([3.3,3.9]\). From Figure 1(b) to 1(d), the non-smoothness becomes more and more significant as \(\) grows. In particular, Figure 1(e) shows that the value of \(J()\) fluctuates violently even within a very small interval of \(\), suggesting a high degree of non-differentiability in this region._

### Stochastic Policies

The original MDP (1) becomes stochastic when a stochastic policy is employed. First, let us consider the slightly modified version of MLE for stochastic policies:

\[_{max}=_{t}_{\| Z_{0} \| 0}_{}[ Z_{}(t)]\|}{\|  Z_{0}\|}.\] (14)

Figure 2: The value of MLE \(()\) for \([3.3,3.9]\) is shown in 1(a). The graph of objective function \(J()\) for different values of \(\) are shown in 1(b)-1(e) where \(J()\) is estimated by the sum of first 1000 terms in the infinite series.

where \( Z_{0}=s^{}_{0}-s_{0}\) is a small pertubation made to the initial state and \( Z_{}(t)=s^{}_{t}()-s_{t}()\) is the difference in the sample path at time \(t\) and sample \(\). Since this definition is consistent with that in (4) when sending the variance to \(0\), we use the same notation \(()\) to denote the MLE at given \(^{N}\) and again assume \(()>-\). Since policies in most control and robotics environments are deterministic, this encourages the variance to converge to \(0\) during training.

However, unlike the deterministic case where the Holder continuity result was proved under the assumption that the policy \(_{}(s)\) is locally Lipschitz continuous, stochastic policies instead provide a probability distribution from which the action is sampled. Thus, a stochastic policy cannot be locally Lipschitz continuous in \(\) when approaching its deterministic limit. For instance, consider the one-dimensional Gaussian distribution \(_{}(a|s)\) where \(=[,]^{T}\) denotes the mean and variance. As the variance \(\) approaches \(0\), \(_{}(a|s)\) becomes more and more concentrated at \(a= s\), and eventually converges to the Dirac delta function \((a- s)\), which means that \(_{}(a|s)\) cannot be Lipschitz continuous within a neighborhood of any \(=[,0]^{T}\) even though its deterministic limit \(_{}(s)= s\) is indeed Lipschitz continuous. The following example illustrates that in this case, the Holder exponent of the objective function \(J()\) can still be less than \(1\):

**Example 4.3**.: _Suppose that the one-dimensional MDP \(s_{t+1}=f(s_{t},a_{t})\) where \(f(s,a)\) is defined as in (12) over the state space \(=[-1,1]\) and action space \(=[0,)\). The cost function is \(c(s,a)=s+1\). Also, the parameter space is \(=[_{1},_{2}]^{T}^{2}\) and the policy \(_{}(|s)(|_{1}|s+|_{2}|,|_{1}|s+ 2|_{2}|)\) is a uniform distribution. It is easy to verify that all required assumptions are satisfied. Let the initial state \(s_{0}=0\) and \(_{1}>1,_{2}=0\), then applying (14) directly yields \(()=_{1}\) similarly as in Example 4.1. Now suppose that \(^{}_{2}>0\) is small and \(^{}=[_{1},^{}_{2}]^{T}\), then for any \(\) in the sample space, the sampled trajectory \(\{s^{}_{t}\}\) generated by \(_{^{}}\) has_

\[s^{}_{t+1}()_{1}s^{}_{t}()+^{}_ {2}>_{1}s^{}_{t}()^{}_{1}s^{}_{1}( )^{}_{1}(^{}_{2})\]

_when \(s^{}_{t+1}()<1\). Thus, we have \(s^{}_{t+1}()=1\) for all \(\) and \(t T_{0}(^{})=1+_{2}}{ _{1}}\), which further leads to_

\[J(^{})=+_{t=0}^{}^{t}\;_{_{^{}}}[s^{}_{t}] J()+_{t=T_{0}()} ^{}^{t}\;_{_{^{}}}[s^{}_{t}] (^{}_{2})^{}}\]

_using the fact that \(J()=\). Plugging \(\|-^{}\|=^{}_{2}\) into the above inequality yields_

\[J(^{})-J()\|^{}- \|^{}}.\] (15)

_where the Holder exponent is again \(\) as in Example 4.1._

**Remark 4.4**.: _Consider the \(1\)-Wasserstein distance as defined in  between the distribution \((a- s)\) and \((|_{1}|s+|_{2}|,|_{1}|s+2|_{2}|)\), which is given by \(W_{1}(_{1},_{2})=|}{2}\). It is Lipschitz continuous at \(_{2}=0\), even though the non-smooth result in (15) holds. Therefore, probability distribution metrics, such as the Wasserstein distance, are too "coarse" to capture the full fractal nature of the objective function. This also suggests that further assumptions regarding the pointwise smoothness of probability density functions are necessary to create a smooth landscape with stochastic policies, even though they may exclude the case of \( 0\) as discussed earlier._

## 5 Estimating Holder Exponents from Samples

In the previous sections, we have seen that the objective function \(J()\) can be highly non-smooth and thus gradient-based methods may not work well in the policy parameter space. The question is: how can we determine whether the objective function \(J()\) is differentiable at some \(=_{0}\) or not in high-dimensional settings? Note that \(J()\) may have different levels of smoothness along different directions. To address it, we propose a statistical method to estimate the Holder exponent. Consider the objective function \(J()\) and a probability distribution whose variance is finite. Consider the isotropic Gaussian distribution \(X(_{0},^{2}_{p})\) where \(_{p}\) is the \(p p\) identity matrix. For continuous objective function \(J()\), then its variance matrix can be expressed as

\[Var(J(X)) =_{X(_{0},^{2}_{p })}[J(X)-_{X(_{0},^{2}_{p})}[J (X)])^{2}]\] \[=_{X(_{0},^{2}_{p })}[(J(X)-J(^{}))^{2}]\]where \(^{}^{p}\) is obtained from applying the intermediate value theorem to \(_{X(_{0},^{2}_{p})}[J(X)]\) and hence not a random variable. If \(J()\) is locally Lipschitz continuous at \(_{0}\), say \(|J()-J(_{0})| K\|-_{0}\|\) for some \(K>0\) when \(\|-_{0}\|\) is small, then it has the following approximation

\[Var(J(X)) K^{2}_{X(_{0},^{2} _{p})}[\|X-^{}\|^{2}](Var(X))^{2}( ^{2})\] (16)

when \( 1\). Therefore, (16) provides a way to directly determine whether the Holder exponent of \(J()\) at any given \(^{p}\) is less than 1, especially when the dimension \(p\) is large. In particular, taking the logarithm on both sides of (16) yields

\[ Var_{}(J(X)) C+2\] (17)

for some constant \(C\) where the subscript in \(Var_{}(J(X))\) indicates its dependence on the standard deviation \(\) of \(X\). Thus, the log-log plot of \(Var_{}(J(X))\) versus \(\) is expected to be close to a straight line with slope \(k 2\) when \(J()\) is locally Lipschitz continuous around \(=_{0}\). Therefore, one can determine the smoothness by sampling around \(_{0}\) with different variances and estimating the slope via linear regression. Usually, \(J()\) is Lipschitz continuous at \(=_{0}\) when the slope \(k\) is close to or greater than \(2\), and it is non-differentiable if the slope is less than 2.

## 6 Experiments

In this section, we will validate the theory presented in this paper through common RL tasks. All environments are adopted from The OpenAI Gym Documentation  with continuous control input. The experiments are conducted in two steps: first, we randomly sample a parameter \(_{0}\) from a Gaussian distribution and estimate the gradient \((_{0})\) from (3); second, we evaluate \(J()\) at \(=_{0}+(_{0})\) for each small \(>0\). According to our results, the loss curve is expected to become smoother as \(\) decreases, since smaller \(\) makes the Holder exponent \(\) larger. In the meantime, the policy gradient method (3) should give a better descent direction while the true objective function \(J()\) becoming smoother.

Notice that a single sample path can always be non-smooth when the policy is stochastic and hence interferes the desired observation, we use stochastic policies to estimate the gradient in (3), and apply their deterministic version (by setting variance equal to 0) when evaluating \(J()\). Regarding the infinite series, we use the sum of first 1000 terms to approximate \(J()\). The stochastic policy is given by \(_{}(|s)(u(s),^{2}_{p})\) where the mean \(u(s)\) is represented by the 2-layer neural network \(u(s)=W_{2}(W_{1}s)\) where \(W_{1}_{r n}()\) and \(W_{2}_{m r}()\) are weight matrices. Let \(=[W_{1},W_{2}]^{T}\) denote the vectorized policy parameter. For the width of the hidden layer, we use \(r=8\) for the inverted pendulum and acrobot, and \(r=64\) for the hopper.

Inverted Pendulum.The inverted pendulum task is a standard test case for RL algorithms, and here we use it as an example of non-chaotic system. The initial state is always taken as \(s_{0}=[-1,0]^{T}\) (\(^{T}\) is the upright position), and quadratic cost function \(c(s,a)=s_{t}^{T}Qs_{t}+0.001\|a_{t}\|^{2},\) where \(Q=(1,0.1)\) is a \(2 2\) diagonal matrix, \(s_{t}^{2}\) and \(a_{t}\). The initial parameter is given by \(_{0}(0,0.05^{2}\ )\). In Figure 3(a) and 3(c), we see that the loss curve is close to a straight line within a very small interval, which indicates the local smoothness of \(_{0}\). It is validated by the estimate of the Holder exponent of \(J()\) at \(=_{0}\) which is based on (16) by sampling many parameters around \(_{0}\) with different variance. In Figure 2(e), the slope \(k=1.980\) is very closed to \(2\) so Lipschitz continuity (and hence differentiability) is verified at \(=_{0}\). As a comparison, the loss curve of single random sample path is totally non-smooth as shown in Figure 2(b) and 2(d).

Figure 3: The experimental results of inverted pendulum. In 2(e), the linear regression result is obtained for \(=0.9\). The loss curves \(J()\) are presented in 2(a)-2(d) where \(=_{0}+(_{0})\) with step size \(10^{-7}\).

Acrobot.The acrobot system is well-known for its chaotic behavior and hence we use it as the main test case. Here we use the cost function \(c(s,a)=s_{t}^{T}Qs_{t}+0.005\|a_{t}\|^{2}\), where \(Q=(1,1,0.1,0.1)\), \(s_{t}^{4}\) and \(a_{t}\). The initial state is \(s_{0}=^{T}\). The initial parameter is again sampled from \(_{0}(0,0.05^{2}\ )\). From Figure 3(a)-3(c), the non-smoothness grows as \(\) increases and finally becomes completely non-differentiable when \(=0.99\) which is the most common value used for discount factor. It partially explains why the acrobot task is difficult to policy gradient methods. In Figure 3(e), the Holder exponent of \(J()\) at \(=_{0}\) is estimated as \( 0.43155<1\), which further indicates non-differentiability around \(_{0}\).

Hopper.Now we consider the Hopper task in which the cost function is defined \(c(s,a)=(1.25-s)+0.001\|a\|^{2}\), where \(s\) is the first coordinate in \(s^{11}\) which indicates the height of hopper. Because the number of parameters involved in the neural network is larger, the initial parameter is instead sampled from \(_{0}(0,10^{2}\ )\). As we see that in Figure 4(a), the loss curve is almost a straight line when \(=0.8\), and it starts to exhibit non-smoothness when \(=0.9\) and becomes totally non-differentiable when \(=0.99\). A supporting evidence by the Holder exponent estimation is provided in Figure 4(e) where the slope is far less than \(2\).

## 7 Conclusion

In this paper, we initiate the study of chaotic behavior in reinforcement learning, especially focusing on how it is reflected on the fractal landscape of objective functions. A method to statistically estimate the Holder exponent at some given parameter is proposed, so that one can figure out if the training process has encountered fractal landscapes or not. We believe that the theory established in this paper can help to explain many existing results in reinforcement learning, such as the hardness of complex control tasks and the fluctuating behavior of training curves. It also poses a serious question to the well-posedness of policy gradient methods given the fact that no gradient exists in many continuous state-space RL problems. Being aware of the fact that the non-smoothness of loss landscapes is an intrinsic property of the model, rather than a consequence of any numerical or statistical errors, we conjecture that the framework developed in this paper might provide new insights into the limitations of a wider range of deep learning problems beyond the realm of reinforcement learning.

Figure 4: The experimental results of acrobot. In Figure 3(e), the linear regression result is obtained for \(=0.9\). The loss curves \(J()\) are presented in 3(a)-3(d) where \(=_{0}+(_{0})\) with step size \(10^{-7}\).

Figure 5: The experimental results of hopper. In Figure 4(e), the linear regression result is obtained for \(=0.9\). The loss curves \(J()\) are presented in 4(a)-3(d) where \(=_{0}+(_{0})\) with step size \(10^{-3}\).

Acknowledgements

Our work is supported by NSF Career CCF 2047034, NSF AI Institute CCF 2112665, ONR YIP N00014-22-1-2292, NSF CCF DASS 2217723, and Amazon Research Award. The authors thank Zichen He, Bochao Kong and Xie Wu for insightful discussions.