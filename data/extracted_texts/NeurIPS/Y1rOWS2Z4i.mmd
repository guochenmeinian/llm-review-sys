# Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments

Siddharth Nayak\({}^{1}\) Adelmo Morrison Orozco\({}^{1}\)1 Marina Ten Have\({}^{1}\) Vittal Thirumalai\({}^{1}\)

Jackson Zhang\({}^{1}\) Darren Chen\({}^{1}\) Aditya Kapoor\({}^{2}\) Eric Robinson\({}^{3}\) Karthik Gopalakrishnan\({}^{4}\)

James Harrison\({}^{5}\) Brian Ichter\({}^{1}\) Anuj Mahajan\({}^{1}\)\({}^{6}\) Hamsa Balakrishnan\({}^{1}\)

\({}^{1}\)MIT \({}^{2}\)TCS \({}^{3}\)USAF-MIT AI Accelerator

\({}^{4}\)Stanford \({}^{5}\)Google DeepMind \({}^{6}\)Apple

###### Abstract

The ability of Language Models (LMs) to understand natural language makes them a powerful tool for parsing human instructions into task plans for autonomous robots. Unlike traditional planning methods that rely on domain-specific knowledge and handcrafted rules, LMs generalize from diverse data and adapt to various tasks with minimal tuning, acting as a compressed knowledge base. However, LMs in their standard form face challenges with long-horizon tasks, particularly in partially observable multi-agent settings. We propose an LM-based Long-Horizon Planner for Multi-Agent Robotics (LLaMAR), a cognitive architecture for planning that achieves state-of-the-art results in long-horizon tasks within partially observable environments. LLaMAR employs a plan-act-correct-verify framework, allowing self-correction from action execution feedback without relying on oracles or simulators. Additionally, we present MAP-THOR, a comprehensive test suite encompassing household tasks of varying complexity within the AI2-THOR environment. Experiments show that LLaMAR achieves a 30% higher success rate than other state-of-the-art LM-based multi-agent planners in MAP-THOR and Search & Rescue tasks. Code can be found at https://github.com/nsidn98/LLaMAR

## 1 Introduction

Creating embodied agents that assist humans in real-life scenarios is a significant challenge when humans communicate their intentions in natural language. Certain tasks like moving furniture [1; 2], search-and-rescue , environmental monitoring , etc, require coordination among multiple agents to solve the tasks efficiently as compared to single-agent scenarios. This challenge of understanding these natural language inputs and effectively coordinating these agents to solve the task is exacerbated in the multi-agent scenario. Recent works [5; 6; 7; 8; 9; 10; 11; 12; 13] have shown that Language Models (LMs) 1 can effectively use language instructions to develop plans for robots. However, most studies focus on single-agent long-horizon task planning. Naive extensions of single-agent planning algorithms to multi-agent settings often fail due to environment non-stationarity, where the policies of other agents--modeled as a part of the environment--are continuously changing [14; 15]. Such failures lead to suboptimal performance as agents struggle to anticipate and adapt to the actions of others. We therefore formulate a centralized process in which decisions are made simultaneously for all agents based on their (partial) observations, similar to the centralized multi-agent system framework (CMAS) proposed in . Leveraging the ability of pre-trained LMs to generalize across diverse tasks, we aim to use LMs for long-horizon embodied multi-agent task planning.

The key insight of our work is that integrating a _plan-act-correct-verify_ framework with LMs enables a robust and adaptive approach to multi-agent task planning in dynamic, partially observableenvironments that allows agents to: (1) plan subtasks required to complete the task, (2) select high-level actions for each agent to complete the proposed subtasks, (3) identify and correct failures after high-level action execution, and (4) self-verify subtask completion based on high-level action execution. Unlike existing methods, our approach uses real-time execution feedback, observations, and agent histories to iteratively refine action planning and execution. This allows agents to adjust strategies based on reasoned insights on action execution, effectively addressing failures without relying on perfect environmental knowledge or oracle feedback. The correction and verification process in our cognitive architecture  is grounded in the environment's reality, which sets it apart from LM self-verification methods that lack such grounding . This framework enhances agents' ability to complete complex, long-horizon tasks, yielding substantial improvement over current state-of-the-art methods.

Similar to our approach, recent works [19; 20; 21; 22; 23; 24; 16] utilize LMs for multi-agent planning, often adopting a hierarchical decision-making structure. The LMs are used for high-level planning to determine subtasks, sometimes in conjunction with planning domain definition language (PDDL) that together with the LM planner, functions as a feasibility solver. Specific actions are executed using low-level policies pre-trained through reinforcement learning, behavior cloning, or heuristic approaches. While these methods effectively use LMs as high-level planners, they assume perfect low-level primitive action policies and simulator or oracle-provided environmental information. By contrast, LLaMAR does not assume perfect knowledge of the environment, does not rely on oracle feedback, and does not assume perfect execution of low-level primitive policies. This approach moves us closer to enabling real-world robots that operate independently of privileged knowledge.

To avoid ambiguity, we use the following conventions. We refer to the objectives within the environments as "tasks" or "goals" and "subtasks" to describe the breakdown of tasks or goals. "High-level actions" are defined as skills the agent can perform, while "low-level actions" or "primitive actions" refer to existing policies--either learned or predefined using heuristics--that execute a sequence of actions to accomplish a high-level action. More details and examples can be found in Appendix A.

The main contributions of this paper are:

* **LLaMAR**: An LM-based Long-Horizon Planner for Multi-Agent Robotics, designed for iterative planning of long-horizon, multi-objective tasks in partially observable environments, with the following key features:
* It operates without prior knowledge of the environment, allowing agents to explore and make decisions based on new observations.
* It evaluates outcomes through direct observation of images, rather than relying on oracles for feedback, enabling independent identification and correction of action failures.
* **MAP-THOR (Multi-Agent Planning in THOR)**: a benchmark suite of tasks within the AI2-THOR simulator under partial observability to standardize methodologies and metrics for evaluating multi-agent planning effectiveness and robustness.

## 2 Related Work

**Reinforcement Learning (RL) for Long-Horizon Planning**: While RL algorithms have shown promise in many applications, they still struggle with long-horizon tasks. Hierarchical reinforcement learning (HRL) has been used to address these challenges in both single-agent [25; 26; 27; 28] and multi-agent settings [29; 30; 31]. However, these approaches are typically applied to single-task, stationary environments, such as games, where agents solve for one goal in a fixed environment. Consequently, these methods do not generalize well across multiple environments or tasks. Multi-task RL has been explored as a potential solution, requiring sophisticated task planning to handle diverse objectives [32; 33]. This often involves decomposing tasks into manageable subtasks, a process well-suited for hierarchical frameworks. However, subtasks are known apriori in multi-task RL formulations. Real-world long-horizon RL necessitates robust task planning, and LMs have emerged as a promising approach for this purpose.

**LMs for Embodied Single-Agent Planning**: Recent studies have demonstrated the effectiveness of LMs in generating and executing plans in embodied single-agent environments [34; 35; 36; 37; 38; 39; 40] and creating plans in single-agent embodied robotic environments [41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52]. Works like SayCan  and Grounded Decoding  use a combination of value functions and LLM predictions for long-horizon tasks. ProgPrompt  and Zero-Shot Language Planner  generate static plans executed in the environment, which may fail in partially observable and dynamic settings. To mitigate this, LLM-planner  updates plans based on new observations, similar to our approach.

**LMs for Multi-Agent Planning**: Xu et al.  use LLMs in multi-agent games, while CoNavGPT  creates global plans for two robots in an embodied environment. RoCo  and CoELA  assign separate LMs to each agent for decentralized action prediction, allowing natural language communication between agents. However, RoCo and CoNavGPT require detailed environment information for planning, and CoELA's action space is filtered by an oracle. Relying on privileged information from an oracle is impractical in real-world applications. By contrast, our work focuses on free-form action generation and handles tasks with more ambiguous descriptions. Prior work  compare centralized (CMAS) and decentralized (DMAS) planning frameworks, showing that centralized planners perform better, though their experiments are in simple, known environments with limited number of agents. Two-Step  decomposes goals for main and helper agents, using PDDL planners for high-level actions. SmartLLM  uses multiple LLM modules for subtask decomposition, multi-robot group formation and task allocation but assumes robots have complete knowledge of the environment, making plans prone to errors in unknown settings. S-ATLAS  use LLMs with conformal prediction for safe multi-agent planning, but the action choices are limited to a small set of objects. Table 1 presents a comparison of the characteristics of different LM-based approaches to multi-agent planning with our work.

LMs can interpret high-level instructions and break them down into feasible subtasks, making them ideal for long-horizon, multi-task scenarios. Our work leverages LMs to enable long-horizon planning across a variety of tasks and environments, building on these advances to address the limitations of traditional RL and HRL methods. By integrating LMs into our planning framework, we enhance the ability to generalize across diverse tasks and scenarios, making significant strides toward practical, real-world applications of RL in dynamic, multi-agent settings.

## 3 Background

**Problem Setting**: We consider a setting where multiple robots perform a series of tasks (a) such as cleaning a room or putting groceries in the fridge, in a home-like environment, and (b) rescuing missing personnel and putting out forest fires in a search & rescue environment (SAR). These tasks typically require long-horizon planning, involving around 100 low-level actions to reach the goal. Our objective is to compute plans for a team of robots to execute high-level language instructions, \(I\). We formalize these tasks as partially observable Markov decision processes (POMDP) [55; 56], denoted as \( N,,,\{_{i}\},\{_{i}\}, ,,T\). \(N\) is the number of agents and \(\) is the high-level language instruction set. Here, \(s\) represents the joint state of all agents, and \(o\) denotes the observation set for all agents. Particularly, \(o_{i}_{i}\) is the observation set of agent \(i\), that captures incomplete environment state information. \(a=_{1}_{2}_{N}\) represents the joint action space. The joint action space comprises of different categories of high-level actions \(=_{NAV}_{INT}_{EXP}\), where \(_{NAV}\) is the joint navigation action set, \(_{INT}\) is the joint interaction actions which allow the agents to interact with objects, and \(_{EXP}\) are the joint exploration actions which allow the agents to explore the environment. Examples of the high-level actions include PickUp(object) \(_{INT}\) and NavigateTo(location) \(_{NAV}\). Each high-level action is associated with a low-level primitive action (pre-trained RL, behavior cloned, or heuristic-based policy). These actions are executed _synchronously_ by all agents at every high-level decision step. \((s^{}|s,a)\) is the joint transition probability function that defines the probability of arriving at \(s^{}\) after taking joint

   & **Dynamic** & **Local** & **Failure** & **Self** \\  & **Planning** & **Information** & **Correction** & **Verification** \\  Two-Step  & ✗ & ✗ & ✗ & ✗ \\ Smart LLM  & ✗ & ✗ & ✗ & ✗ \\ S-ATLAS  & ✓ & ✗ & ✗ & ✗ \\ CoELA  & ✓ & ✓ & ✗ & ✗ \\ LLaMAR (this paper) & ✓ & ✓ & ✓ & ✓ \\  

Table 1: The proposed model, LLaMAR: 1) performs dynamic planning, avoiding the open-loop plan-and-execute paradigm; 2) operates without privileged simulator information (e.g., access to all objects in the environment); 3) re-plans when low-level actions fail, not assuming perfect execution; and 4) self-verifies subtask completion without relying on the simulator.

action \(a\) in \(s\). \(=\{g_{1},,g_{k}\}\) defines the subtasks that the agents need to perform to accomplish the language instruction task. \(T\) is the length of the planning horizon.

**Environment**: To simulate open-ended, long-horizon tasks that resemble everyday activities in a home-like environment, we use the AI2Thor simulator , which supports a diverse set of interactions and photorealistic rendering. Since our approach does not require any parametric training, it can potentially translate to other similar embodied environments like VirtualHome , Habitat [42; 58; 59], and ThreeDWorld , possibly extending beyond household domains. Similarly, to simulate the search & rescue scenario, we create a custom Search & Rescue environment (SAR). More information about the MAP-THOR and SAR environments can be found in Appendix B and D respectively. We instantiate the problem with \(N\) agents cooperating to accomplish a long-horizon rearrangement task  in an indoor environment. The agents do not know the objects present in the environment _a priori_ and are encouraged to explore the environment to gather more information to complete the task. Unlike previous solutions in similar settings [22; 19; 20], we do not rely on an oracle/simulator to verify subtask completion. In prior works, a predefined conditional satisfiability check is used as subtask completion feedback.

## 4 Approach

We describe our approach in this section. Figure 1 illustrates LLaMAR's architecture comprising four modules: _Planner_, _Actor_, _Corrector_, and _Verifier_, each an LM with a distinct role. Prior work  shows that splitting roles across different LMs improves performance in sequential decision-making. Our initial experiments confirm that LMs tasked with reasoning about multiple inputs and providing long outputs perform poorly. We iterate through these four modules at every high-level decision step. The pseudocode for our approach is in Appendix E. We define some key notation below:

* **Memory**\(\): A textual description of the joint memory of all agents, summarizing past observations, high-level actions, plausible reasons for action failures, and specific subtasks that each agent is attempting to solve.
* **Open Subtasks**\(_{O}\): Feasible subtasks proposed by the _Planner_ LM to achieve the environment task that are yet to be accomplished by the agents.

Figure 1: An overview of LLaMAR’s modular cognitive architecture. LLaMAR leverages LMs within four key modules: Planner, Actor, Corrector, and Verifier, each with specific roles. The Planner breaks down the high-level language instruction into feasible subtasks to achieve the environment goal. The Actor determines the high-level actions each agent should perform. These actions trigger low-level policies that generate and execute a sequence of primitive actions in sync across all agents. Based on execution feedback, the Corrector suggests corrections for high-level actions and the Verifier Module validates completion of subtasks.

* **Completed Subtasks \(_{S}\)**: Subtasks completed by the agents.
* **Corrective Actions \(a_{c}\)**: Corrective actions for each agent based on failure information from the previous step.

At the start of each episode, Memory \(\), Open Subtasks \(_{O}\), Completed Subtasks \(_{S}\), Actions \(a\), Corrective Actions \(a_{c}\), and Failure Information \(\) are initialized as empty sets.

Consider an example of a kitchen with groceries, a fridge, and a counter. Two agents are tasked with "Fetch the groceries and place them in the fridge". This example will help illustrate the utility of each module. All LMs receive a language task instruction \(\), joint observations from all agents, and information about open and completed subtasks and memory unless stated otherwise. We next discuss the various components in our architecture in detail:

**Planner Module** The _Planner_ LM module suggests feasible subtasks to ensure the completion of the environment task. This method, similar to SmartLLM's  zero-shot planning, uses only observations in the agents' field of view. The _Planner_ suggests subtasks related to objects seen in the current observation or memory of all the agents. For the example considered, it decomposes the task into subtasks like "transport the tomato to the fridge" and "transport the lettuce to the fridge", which are added to \(_{O}\).

**Actor Module** The _Actor_ LM additionally uses corrective actions suggested by the _Corrector_ module in the previous time step to predict high-level actions for the current step. These actions are then executed in the environment to progress a subset of subtasks in the open subtask set \(_{O}\) and accordingly updates the joint memory. For instance, the _Actor_ module might suggest actions such as \(a=[(),()]\), updating memory with "We saw a tomato on the counter-top, Alice is picking up the tomato, and Bob is navigating to the lettuce".

**Corrector Module** The _Corrector_ LM self-corrects high-level actions suggested by the _Actor_ LM after controller failures in the previous step's execution2. It suggests corrective high-level actions and provides reasons for failures and chosen corrections. For example, it might suggest "The action of picking up the tomato failed because it is too far away. Alice first needs to navigate closer to the tomato."; \(a_{c}=[(),]\).

**Verifier Module** After executing high-level actions, the _Verifier_ LM assesses whether these actions have completed any subtasks in the open subtask set. Successful subtasks are moved to the completed subtask set. Without the _Verifier_ LM, the method would need to rely on the simulator/oracle for success or failure information. The _Verifier_ LM along with other information uses the successfully executed high-level actions proposed by the _Actor_ LM to predict subtask completion. For example, after transporting the lettuce to the fridge, the _Verifier_ updates the completed subtasks with "transport lettuce to the fridge".

**Admissible Action parsing with Semantic Translation** When LMs generate action plans, natural language outputs often fail to translate to executable high-level actions. This happens when the output does not match the predefined format or refers to unrecognized contextually similar objects. We use a cosine similarity method from , fine-tuning a pre-trained sentence-BERT  to transform the free-form text into admissible high-level actions. Hyperparameters and additional details of the sentence transformer fine-tuning are provided in Appendix J.

**Exploration Strategy** In unexplored environments, agents need to search for task-relevant objects. If agents cannot find the required objects, the language model can choose an 'exploration' action \(a_{exp}\). We use a semantically-guided heuristic to determine the choice of region to be explored. The agent rotates to four cardinal directions \(d North,South,East,West\), capturing image observations \(o_{n,d}\). These images are processed through a pre-trained CLIP image encoder  to obtain embeddings \(I_{d}\). The list of open subtasks \(_{O}\) is processed through the corresponding CLIP text encoder to get text embeddings \(g_{O,i}\). The exploration score \(_{d}\) in direction \(d\) is defined as \(_{d}=_{i=1}^{|_{O}|} I_{d}}{\|g_{O, i}\|\|}\). The direction with the highest score \(d^{*}=_{d}_{d}\) is chosen. Summing the scores helps select the best direction to explore in expectation. The agent rotates towards \(d^{*}\) and moves \(J=2\) steps, repeating this process \(K=3\) times in one _explore_ action. This approach ensures that images relevant to identifying potential subtasks are prioritized. For example, if \(_{O}\) includes "locate a computer", it is more likely to find a computer on a table than on a sofa, resulting in a higher cosine similarity score between the subtask CLIP text embedding and table CLIP image embedding. Refer to Appendix B.2 for more details about the exploration heuristic.

**Motivation for Proposed Framework** The specific order of the modules in LLaMAR is due to natural causal relationships in which environment feedback is received. We use the _Planner_ as the first module because it allows LLaMAR to come up with an initial list of open subtasks that could be completed based on the current observation and past memory to satisfy the task. This list serves as a rough high-level plan. The actor then uses this information to suggest the necessary actions. The _Corrector_ is used after the _Actor_ module to identify reasons for failures in the execution of the actions suggested by the _Actor_. Note that the failure module is inert and only suggests corrective actions. Only the _Actor_ module decides the final actions to be executed. This role distinction allows for clear reasoning on failures when they occur and lets the actor module focus on choosing actions. The _Verifier_ is used after the action is executed to update the list of closed subtasks so that LLaMAR can be current with the progress toward the completion of the environment task. This allows the planner to update the list of open subtasks in the next step. In essence, the _Planner_ and the _Verifier_ ensure that the progress of the agents is tracked and the actor and the corrector ensure that the actions are executed successfully to advance towards completion of the task.

**Multi-Agent features in LLaMAR** While our method can be easily adapted to a single-agent setting, our design choice for the architecture was motivated to include the following multi-agent features:

* **Coordination through communication**: Agents share their state information with the centralized LLaMAR modules to predict actions, enabling them to coordinate and avoid conflicts. This information sharing allows for the agents to cooperate and achieve the collective goal.
* **Dynamic Role Assignment**: Agents are dynamically assigned roles based on the current task requirements and their capabilities. This flexibility allows LLaMAR to adapt to changing environments and task demands.
* **Hierarchical Task Decomposition**: To handle the complexity of multi-agent planning, LLaMAR decomposes the action space by creating specific subgoals/subtasks available for any agent to assign itself (done by the actor module) based on the observation and current context. This decomposition reduces the overall search space and improves planning efficiency.

## 5 Experiments

**MAP-THOR**: To evaluate the performance of LLaMAR and benchmark other baseline methods, we create a benchmark dataset of tasks which we call MAP-THOR (Multi-Agent Planning tasks in AI2-THOR). While Smart-LLM  introduces a dataset of 36 tasks within AI2-Thor  classified by complexity, their tasks are limited to single floor plans. This limitation hinders testing the robustness of planners across different room layouts. Additionally, some tasks in their dataset cannot be performed by multiple agents, regardless of task division, such as Pick up the pillow, Open the laptop to turn it on, and Turn off the lamp.

By contrast, MAP-THOR includes tasks solvable by both single and multiple agents. We classify the tasks into four categories based on the ambiguity of the language instructions. To test the planner robustness, we provide five different floor plans for each task. We also include automatic checker modules to verify subtask completion and evaluate plan quality. Our dataset comprises 45 tasks, each defined for five distinct floor plans, ensuring comprehensive testing and evaluation.

We conduct experiments with tasks of varying difficulty levels, where an increase in difficulty of the tasks corresponds to an increased ambiguity in the language instructions. The complete task list of each category can be found in the Appendix C.

* **Explicit item type, quantity, and target location**: Agents are explicitly instructed to transport specific items to specific target locations. For example, put bread, lettuce, and a tomato in the fridge clearly defines the objects (tomato, lettuce, bread) and the target (fridge).
* **Explicit item type and target location but implicit item quantity**: The object type is explicitly described, but its quantity is not disclosed. For example, Put all the apples in the fridge. Agents must explore the environment to locate all specified items and also predict when to stop.
* **Explicit target location but implicit item types and quantity**: The target location is explicitly defined but the item types and their quantities are concealed. For example, Put all groceries in the fridge.

* **Implicit target location, item type, and quantity**: Item types and their quantities along with the target location are implicitly defined. For example, Clear the floor by placing the items at their appropriate positions. The agent is expected to place items like pens, books, and laptops on the study table, and litter in the trash can.

**Search & Rescue Environment (SAR)**: To showcase the effectiveness of LLaMAR with respect to explicit coordination in multi-agent settings, we evaluate LLaMAR in a partially observable search & rescue and fire relief environment in a grid world. Depending on the scene, there is a mix of missing people to be found, and wildfires to be stopped before they spread geographically. More details about the environment can be found in Appendix D.

* **Fire Extinguishing**: Fires consist of expansive flammable regions with a fixed set of sources that propagate over time. The rate of fire spread is proportional to its intensity; higher intensities result in faster spread. Fires are categorized as either Class A or Class B, which are extinguished using water or sand, respectively. These extinguishing resources are sourced from reservoirs distributed across the environment.
* **Human Rescue**: Each individual is initially located at an unknown position within the environment. The objective is to locate, rescue, and transport the individuals to a designated drop-off location, which is known beforehand. Transporting a person requires the coordinated effort of two agents simultaneously, who must carry them to the specified drop-off point.

#### Metrics

We evaluate the algorithms using the following metrics to compare their performances on the tasks:

* **Success Rate** (SR): The fraction of episodes in which all subtasks are completed. Success equals 1 if all subtasks are successfully executed in an episode, otherwise it is 0.
* **Transport Rate** (TR): The fraction of subtasks completed within an episode, provides a finer granularity of task completion.
* **Coverage** (C): The fraction of successful interactions with target objects. It is useful to verify if the LMs can infer the objects to interact with, in scenarios where the tasks have objects that are specified implicitly.
* **Balance** (B): The ratio between the minimum and maximum number of successful high-level actions executed by any agent that contributed towards task completion. We only check for a subset of high-level actions that must be executed to accomplish critical subtasks that lead to the successful completion of the language instruction task. If each agent \(i\) out of \(n\) agents completes \(s_{i}\) successful tasks, the balance is defined as: \(B:=,,s_{n}\}}{\{s_{1},,s_{n}\}+}\). This measures how evenly the work is distributed among agents. A balance of zero indicates at least one agent performed no successful high-level actions, while a balance of one indicates all agents performed the same number of successful high-level actions. Here \(=1e-4\) is a small number to avoid division by zero.
* **Average steps** (L): The number of high-level actions taken by the team to complete the task, capped at \(L=30\) in our experiments. If the task is not completed within \(L\) steps, the episode is deemed a failure. Note that the metric \(L\) is presented in the table providing the complete results, located in Appendix F.

For all the metrics, we report the means along with the \(95\%\) confidence interval across all the tasks (refer Appendix F for complete results). Since SR is a binomial metric, we report the Clopper-Pearson Interval as the confidence interval.

#### Baselines

For a fair comparison with our method, we make modifications to the baselines to make them work in partially observable settings with limited reliance on the simulator. More details about implementations can be found in Appendix H.

* **Act**: We query the LLM with the task and the observations to suggest a high-level action.
* **Chain-of-Thought**: We modify the Act prompt with a chain-of-thought style addendum to let the LM reason about the possible implications while selecting a high-level action.
* **ReAct**: We use a ReAct-style prompting to let the LMs reason after suggesting high-level actions and possibly suggest ways to correct any failures.
* **SmartLLM**: We modify the official codebase to only include information from the local observations of the agents instead of assuming full observability.

* **CoELA**: We modify the list of available high-level actions to include all possible valid combinations of actions with interactable objects in the agent's local observation. As the scene becomes more cluttered, this list and the prompt becomes combinatorially longer. In the original implementation, the list of available actions is filtered based on the feasibility of the actions as suggested by a conditional checker.

It should be noted that Act, Chain-of-Thought, ReAct, and SmartLLM are all CMAS frameworks where CoELA follows the DMAS framework.

## 6 Results and Discussion

**Choice of the underlying LM**: To understand the impact of the underlying LM's quality on decision-making, we initially experimented with different LMs on MAP-THOR. Specifically, we utilize both the language-only and vision-language models of GPT-4 , IDEFICS-2 , LLaVA , and CoGVLM . Among these, GPT-4, when used solely with text inputs, exhibits the poorest performance. This is attributed to the agents' inability to reason about visual observations, which is particularly detrimental for the _Corrector_ module. Substituting GPT-4V with other vision-language models results in a decline in performance (refer Table 2) and hence we use GPT-4V as the underlying VLM while comparing to the baselines.

**Baseline Comparisons**: Table 2 compares our method, LLaMAR, with other baselines in a 2-agent scenario using GPT-4V as the underlying VLM. Act and ReAct show similar performance, with Act struggling due to its lack of strategic planning or correction, and ReAct performing slightly better by dynamically adjusting actions based on reasoning on immediate feedback. CoT's performance declines with longer planning horizons due to its inability to maintain coherence over extended planning sequences, consistent with findings in , showing its effectiveness only with highly specific prompts. SmartLLM, operating in a _plan-and-execute_ paradigm, generates impractical plans with issues like infinite loops and failure to handle low-level action failures, leading to lower success rates and poor transport metrics. It also tends to hallucinate objects. CoELA, using a decentralized multi-agent system (DMAS), performs poorly due to large input prompts and struggles to select the correct action from numerous choices. Its decentralized decision-making is less efficient than the centralized multi-agent system (CMAS) used by LLaMAR. Previous research  confirms CMAS frameworks are more effective than DMAS frameworks. Overall, our method, LLaMAR, benefits from its modular cognitive architecture, which integrates planning, acting, correcting, and verifying through distinct LLM roles, resulting in superior performance across various evaluation metrics. By avoiding reliance on privileged information and incorporating a robust exploration strategy that allows it to scout for objects that are not initially visible, LLaMAR ensures higher success rates and balanced task execution among agents.

**Roles of different modules in LLaMAR**: To demonstrate the effectiveness of the various modules in our cognitive architecture, we performed ablation studies by evaluating performance metrics with each module removed individually. The results are summarized in Table 3. Using only the _Actor_ module corresponds to the "Act" baseline, which demonstrates its fundamental capabilities in isolation but shows limited effectiveness without planning and correction due to relatively lower success and transport rates. Adding the Planner and Verifier modules improves performance, benefiting from better task planning and validation, increasing the overall SR and TR, and ensuring more effective task completion and even work distribution, as indicated by the increase in balance (B). However, in scenarios where the suggested action fails, the actor suggests the same action in the next decision step since it is not able to reason why the action failed until the end of the planning horizon. Incorporating the Corrector module, with access to privileged information from an environment oracle, significantly boosts performance, enhancing the SR, TR, C, and further improving B, consistent with the findings in . This highlights the Corrector module's importance in adjusting actions based on controller feedback, resulting in higher task success and more efficient task completion, albeit with reliance on oracle knowledge. Finally, the complete LLaMAR system, without privileged information, achieves SR, TR, C, and B values close to those of the oracle setup. This demonstrates the system's robustness and effectiveness in a realistic setting. The Corrector module plays a crucial role in enabling agents to learn from past failures and avoid repeating actions, preventing task failures due to timeout. Despite lacking oracle knowledge, LLaMAR performs nearly as well as the oracle-enhanced setup. These results highlight the importance of each module in our cognitive architecture. Removing any module diminishes effectiveness.

**Increasing the number of agents** Increasing the number of agents in the environment shows distinct tends in our method's performance metrics for both MAP-THOR and SAR environments (refer Table 4). In MAP-THOR, with two agents, we establish a solid baseline for success rate (SR) and transport rate (TR), which is similarly reflected in the SAR environment. Adding a third agent improves both SR and TR in both environments, indicating enhanced task completion and transportation efficiency. Coverage (C) also increases, suggesting better exploration and interaction with objects across both environments. There is a slight decrease in SR and TR when the number of agents increases from 3 to 5 in the MAP-THOR environment. The decrease in these metrics can be attributed to the rooms in MAP-THOR becoming crowded with 4 and 5 agents hence blocking the agents from navigating without colliding with other agents. But this phenomenon is not seen in the SAR environment which is comparatively more spacious and navigable. However, balance (B), which measures the even distribution of tasks among agents, decreases with more agents. This drop highlights the challenge of ensuring equal contributions from all agents in a larger multi-agent system. While SR remains high, the balance metric drops significantly from 2 to 5 agents, indicating some agents do more work than others. In summary, adding more agents improves task performance and efficiency but introduces challenges in maintaining balanced contributions. Addressing this imbalance is crucial for refining multi-agent planning algorithms.

**Correcting Failures**: In numerous instances, the actions proposed by the _Actor_ module, such as pick up <object>, are unsuccessful due to the agent's insufficient proximity to the target object. In such situations, the _Corrector_ module uses visual feedback to learn from these failures and recommends appropriate corrective actions, such as navigate to <object> to facilitate closer proximity. Figure 2 shows examples where the _Corrector_ module interprets low-level action failures and suggests remedies, highlighting its importance.

## 7 Limitations and Future Work

**Higher number queries to the LM**: Since each high-level decision step requires querying 4 different LM-based modules, the cost and the compute times are higher than other baselines, especially compared to the plan-and-execute baselines like SmartLLM. An interesting future direction to

   **\# of** \\ **agents** \\  } &  &  \\   & **SR\(\)** & **TR\(\)** & **C\(\)** & **B\(\)** & **SR\(\)** & **TR\(\)** & **C\(\)** & **B\(\)** \\ 
1 & 0.37 & 0.67 & 0.87 & **1.00** & 0.28 & 0.75 & 0.86 & **1.00** \\ 
2 & 0.62 & 0.87 & 0.95 & 0.82 & 0.44 & 0.86 & 0.94 & 0.91 \\ 
3 & **0.70** & **0.91** & 0.98 & 0.66 & 0.68 & 0.92 & 0.96 & 0.80 \\ 
4 & 0.68 & 0.90 & **0.99** & 0.62 & 0.72 & 0.94 & 0.98 & 0.78 \\ 
5 & 0.62 & 0.90 & **0.99** & 0.54 & **0.74** & **0.96** & **1.00** & 0.73 \\  

Table 4: LLaMAR with various number of agents in the scenario in both MAP-THOR and SAR environments

 
**Algorithm** & **LM** & **SR\(\)** & **TR\(\)** & **C\(\)** & **B\(\)** \\  Act & GPT-4v & 0.33 & 0.67 & 0.91 & 0.59 \\  ReAct & GPT-4v & 0.34 & 0.72 & 0.92 & 0.67 \\  CoT & GPT-4v & 0.14 & 0.59 & 0.87 & 0.62 \\  SmartLLM & GPT-4v & 0.11 & 0.23 & 0.91 & 0.45 \\  CoELA & GPT-4v & 0.25 & 0.46 & 0.76 & 0.73 \\  LLaMAR & GPT-4v & 0.51 & 0.85 & 0.95 & 0.83 \\  LLaMAR & ILaAv & 0.54 & 0.84 & 0.91 & 0.75 \\  LLaMAR & IDEtCS-2 & 0.57 & 0.86 & 0.94 & 0.78 \\  LLaMAR & LogVLM & 0.61 & 0.89 & 0.95 & 0.80 \\  LLaMAR &  **\#** \\ **(w/o** expl)** \\  } &  **\#** \\ **(wimprove this would be to fine-tune smaller LMs with trajectories collected in the simulator (eg: ALFRED ) as done in . Another potential direction worth exploring is using different sizes of LMs for each module based on their specific utility.

**Limited spatial reasoning**: Although we use both textual descriptions and visual features to guide the language model's actions, it still lacks the ability to reason about the spatial features of the environment. Spatial reasoning is crucial in scenarios such as navigating around obstacles to reach an object, or determining the shortest path to collect multiple items scattered across different locations. One way to address this limitation is to inject information about the 3D world into the LM, as done in , which is an interesting direction for future work.

**Performance limited by the underlying VLM**: Although LMs make correct reasoning most of the time, they still occasionally make mistakes, including misunderstanding the environment rules specified in the prompt. For example, the agent assumes that the cleaning task requires putting soap, drying, and putting it in the sink when all it needs is the action "_CleanObject_", and can't figure out the appropriate level of abstraction. The performance of the algorithm is limited by the instruction following and reasoning capability of the underlying LM ; this calls for developing LMs that are fine-tuned to instruction-image pairs relevant to the environment (as done in ).

## 8 Conclusion

We address long-horizon planning in dynamic, partially observable multi-agent environments with LLaMAR, an LM-based planner using four specialized modules: _Planner_, _Actor_, _Corrector_, and _Verifier_. This framework iteratively refines action planning, adapts to failures, and verifies subtask completion using real-time observations and action feedback, without privileged information. We also introduce a heuristic-based exploration strategy to guide agents to semantically relevant regions. Additionally, we present MAP-THOR, a benchmark dataset for multi-agent tasks in the AI2Thor simulator. Empirical results show LLaMAR outperforms existing LM-based approaches, achieving a 30% higher success rate on MAP-THOR.