# Test Where Decisions Matter: Importance-driven Testing for Deep Reinforcement Learning

Stefan Pranger,\({}^{1}\) Hana Chockler,\({}^{2}\) Martin Tappler,\({}^{3}\) Bettina Konighofer\({}^{1}\)

\({}^{1}\)Institute of Information Security,

Graz University of Technology

\({}^{2}\)Kings College London

\({}^{3}\)Institute of Computer Engineering,

TU Wien

{stefan.pranger,bettina.koenighofer}@tugraz.at

hana.chockler@kcl.ac.uk

martin.tappler@tuwien.ac.at

###### Abstract

In many Deep Reinforcement Learning (RL) problems, decisions in a trained policy vary in significance for the expected safety and performance of the policy. Since RL policies are very complex, testing efforts should concentrate on states in which the agent's decisions have the highest impact on the expected outcome. In this paper, we propose a novel model-based method to rigorously compute a ranking of state importance across the entire state space. We then focus our testing efforts on the highest-ranked states. In this paper, we focus on testing for safety. However, the proposed methods can be easily adapted to test for performance. In each iteration, our testing framework computes optimistic and pessimistic safety estimates. These estimates provide lower and upper bounds on the expected outcomes of the policy execution across all modeled states in the state space. Our approach divides the state space into safe and unsafe regions upon convergence, providing clear insights into the policy's weaknesses. Two important properties characterize our approach. (1) Optimal Test-Case Selection: At any time in the testing process, our approach evaluates the policy in the states that are most critical for safety. (2) Guaranteed Safety: Our approach can provide formal verification guarantees over the entire state space by sampling only a fraction of the policy. Any safety properties assured by the pessimistic estimate are formally proven to hold for the policy. We provide a detailed evaluation of our framework on several examples, showing that our method discovers unsafe policy behavior with low testing effort.

## 1 Introduction

Deep reinforcement learning (RL)  is a powerful method for training policies that complete tasks in complex environments. Due to the high potential of RL in safety-critical domains, such as autonomous driving , ensuring the reliability of its safety-critical properties is becoming increasingly vital. Formal verification provides provable correctness guarantees . However, the most significant challenge in the formal verification of RL policies is scalability, which limits its current applicability . As for conventional software, a complete safety evaluation by exhaustively testing a policy's decisions is infeasible. Hence, it is necessary to establish as much confidence as possible in a policy with a limited testing budget.

states where _their decisions matter most_. We follow the insights from Chockler et al.  that not all decisions hold equal significance on the expected safety and performance of a policy. Decisions in certain states may have a significant impact on the overall expected outcome of the policy, while in other states, the impact may not be as severe or critical. The core of our algorithm is a _ranking of the importance of states of the environment_. This ranking is based on the difference that the decision in a particular state makes on the expected overall performance (e.g., accumulated reward) or safety of the policy. For lack of space, we focus on safety from here on. The proposed method can easily be adapted to evaluate the agent's performance, which we discuss in Appendix D.

Figure 1 outlines our algorithm. The inputs to our algorithm are a model of the environment in the form of a Markov decision process , a formal safety specification \(\), and an RL agent in the form of a deterministic policy. In each iteration, our algorithm computes _optimistic and pessimistic estimates_, which provide lower and upper bounds for the expected probability of satisfying the safety specification over all possible policies. The algorithm terminates if the maximal difference between the estimates gets below some threshold or a maximal number of executed test cases is reached. As long as the stopping criterion is not met, the algorithm computes an _importance ranking_ of the states. The higher the rank of a state, the more influence the decision in that state has for satisfying or violating the safety specification. Next, the most important decisions of the policy are sampled and used to fix the decisions, thus _restricting_ the MDP. The algorithm continues with the restricted MDP to iteratively refine the estimates. Our testing framework can be modified through an optional step by _clustering the highly ranked states_. A fraction of test cases is then uniformly selected from the individual clusters. The intuition behind clustering is that the agent is likely to behave similarly in comparable situations. Following this intuition, we mark all states in a cluster as safe if all tested states of this cluster are verified to be safe. Otherwise, the entire cluster is marked as unsafe. This increases the scalability of our testing approach since only a fraction of each cluster needs to be tested for deriving test verdicts for all states in the cluster. However, since not all decisions in a cluster are sampled, unsafe policy behavior can be missed.

Our algorithm provides the following _benefits_:

* Optimal Test-Case Selection: At any time in the testing process, our approach evaluates the policy in the states that are most critical for safety.
* Guaranteed Safety: A pessimistic estimate provides a formal verification guarantee: under the given model it is guaranteed that if the pessimistic estimate for a given state satisfies the testing criteria, then the agent's policy is formally verified from that state.
* Highlighting the most important decisions is a central technique in explainable AI [7; 8]. We provide a rigorous method to compute an importance ranking. Simpler policies that only use the top-ranked decisions can help understand the policy's decision-making .
* The iterative nature of our approach can be used in a debugging process to construct a safety frontier: if a sampled decision in a certain state is evaluated to be unsafe, the next ranking iteration assigns higher importance to the predecessor states to be tested next. Thus, the unsafe region around a safety hazard grows until it reveals all states where the policy behaves unsafely.
* Upon convergence, our approach partitions the state space into safe and unsafe regions. The identified unsafe regions offer interpretable insights into the policy's current weaknesses.

Figure 1: High-level view of the algorithm for importance-driven testing for RL.

### Related Work

**Evaluation of RL policies.** Off-policy evaluation (OPE) [9; 10; 11] aims to estimate the expected performance of a newly trained policy by using executions of a previously trained policy. In contrast, our approach estimates the performance of the policy under test by computing the best-possible (optimistic) estimate and the worst-possible (pessimistic) estimate in the current MDP. In contrast to OPE, our framework provides formal verification guarantees over the entire state space: any safety property assured by the pessimistic estimate is formally proven to hold. Several recent works proposed evaluation strategies to analyze RL policies , by adapting software testing techniques to RL. Various approaches apply fuzz or search-based testing as a basis to find safety-critical situations in the black-box environment [13; 14; 15; 16], in which to test the policy. Most efforts of the testing community focused on selecting test cases that falsify safety with high probability [15; 17]. These methods effectively reveal unsafe behavior, but they do not provide safety assurance from non-failing tests, as they lack proper notions of coverage. In contrast, our testing approach is model-based. Model-based testing of probabilistic systems was proposed in . To the best of our knowledge, there is no model-based testing approach for RL policies with formalized criteria of completeness. That is, we are the first to propose safety estimates with formal interpretations which form the basis of our test-case generation.

**Model-based formal methods for model-free RL.** Several recent works have proposed approaches for developing RL controllers by combining model-based formal methods and model-free RL. The appeal of this combination lies in the strengths of each approach: model-based methods offer formal safety and correctness guarantees, while model-free RL demonstrates superior scalability and yields high-performance controllers by learning from the full-order system dynamics [19; 20]. Most of the existing work in this area addresses the problem of safe exploration in RL [21; 22]. To the best of our knowledge, our work is the first to employ similar techniques for analyzing a trained policy.

**Importance ranking.** Ranking policy decisions has been proposed for explaining and simplifying RL policies. In , the ranking is based on statistical fault localization computed on a set of executions of the original policy and its small perturbations. A continuation of this work  uses an average treatment effect to rank policy decisions. In contrast, we provide a rigorous method to compute the importance ranking. Our estimates consider any possible behavior of the policy over the entire state space. Thus, the estimates provide strong verification guarantees. Similarly to ranking policy decisions,  rank the importance of individual neurons in a network to assess coverage of a given test set.

## 2 Background

**Markov Decision Process.** A _Markov decision process_ (MDP) \(=(,,,)\) is a tuple with a finite state set \(\), a finite set \(=\{a_{1},a_{n}\}\) of actions, a probabilistic transition function \(:\), and a probability distribution of initial states \(:\). An _execution_ (or path) is a finite or infinite sequence \(=s_{0},a_{0},s_{1},a_{1}\) with \((s_{i},a_{i},s_{i+1})>0\) and \((s_{0})>0\). A (memoryless deterministic) _policy_\(:\) is a function mapping states to actions. \(\) denotes the set of all memoryless deterministic policies. Applying \(\) to an MDP \(\) induces a Markov chain (MC) \(^{}\). An execution in \(^{}\) is a sequence \(=s_{0},s_{1},s_{2},\) with \((s_{i},(s_{i}),s_{i+1})>0\). \(_{s}^{}\) denotes the unique probability measure of \(^{}\) over infinite executions starting in \(s\).

**Probabilistic Model Checking.** Probabilistic model checking  computes the probabilities of satisfying a temporal-logic formula \(\) over a finite or infinite horizon. We define the properties below with a bound \(n\). For the unbounded horizon, \(n=\). For a given MDP \(\), a policy \(\), and a property \(\) in Computation Tree Logic (CTL) , model checking computes the following probabilities:

* \(_{^{},}:\) is the expected probability to satisfy \(\) state \(s\) within \(n\) steps in \(^{}\).
* \(_{,}^{}(s,n)=_{} _{^{},}(s,n)\) is the _maximal_ expected probability _over all policies in_\(\) from a state \(s\) within \(n\) steps.
* \(_{,}^{}(s,n)=_{} _{^{},}(s,n)\) is the _minimal_ expected probability _over all policies in_\(\) from a state \(s\) within \(n\) steps.

For the remainder of this paper, let \(\) be a formula in the safety fragment of CTL. Using \(\) and a user-defined safety threshold \(_{}\), we define safety objectives as follows:

**Definition 2.1** (Safety objective).: Given an MDP \(=(,,,)\), a safety property \(\), and a threshold \(_{}\). A _safety objective_ is a tuple \(,_{}\). A policy \(\) satisfies \(,_{}\) from a given state \(s\) within \(n\) steps if \(_{^{},}(s,n)_{}\).

**Reinforcement Learning.** An RL  agent learns a task via interactions with an unknown environment modeled by an MDP \(\) with an associated reward function \(:\). In each state \(s\), the agent chooses an action \(a\), the environment then moves to a state \(s^{}\) with probability \((s,a,s^{})\). The return \(_{}\) of an execution \(\) is the discounted cumulative reward defined by \(_{}=_{t=0}^{}^{t}(s_{t})\), using the discount factor \(\). The objective of the agent is to learn a deterministic memoryless _optimal policy_\(^{*}\) that maximizes the expectation of the return.

## 3 Importance-driven Testing for RL

In this section, we will describe our framework for importance-driven model-based testing, which we abbreviate with IMT. An overview of our algorithm is depicted in Fig. 1. Its central elements are the computation of the estimates and the importance ranking that guides the selection of the test cases. In Sec. 3.1 we discuss IMT in detail, and in Sec.3.2 we discuss its extension with clustering.

### Importance-driven Model-Based Testing

Alg. 1 gives the pseudo-code of our approach for importance-driven safety testing. Our algorithm evaluates a policy \(\) with respect to a safety objective \(,_{}\) over a horizon of \(n\) steps (for the unbounded horizon, \(n=\)). The algorithm takes as input an MDP \(=,,,\), a policy under test \(:\), and a safety objective \(,_{}\). It returns as result a classification of states into safe and failure states (\(_{s}\) and \(_{f}\), respectively), and the optimistic and pessimistic estimates for all states in the state space (\(_{opt}:\) and \(_{pes}:\), respectively), which are derived as the expected maximal and minimal probability of satisfying the safety objective \(,_{}\).

**Safety estimates.** In Line 3, IMT computes the safety estimates for the current (restricted) MDP \(^{(i)}\). The optimistic estimate \(_{opt}(s,n)\) is the maximal expected probability of satisfying \(\) for an execution in \(^{(i)}\) from a given state \(s\) within a \(n\) steps quantified over all policies. Similarly, the pessimistic estimate \(_{pes}(s,n)\) is the minimal expected probability of satisfying \(\).This yields the following definition:

**Definition 3.1** (Safety estimates).: For a given MDP \(=(,,,)\), a given safety property \(\), and a given number of \(n\) steps, the _optimistic_ and _pessimistic safety estimate_\(_{opt},_{pes}\): \(\)are defined as follows:

\[ s}_{opt}(s,n)=_{, }^{}(s,n), s}_{pes}(s,n)=_{ ,}^{}(s,n).\]

For a state action pair \((s,a)\) and a bound \(n\), the maximal expected probability of satisfying \(\) from a state \(s\) after executing \(a\) is

\[ s, a:}_{opt}(s,a,n) =_{s^{}}((s,a,s^{})}_{opt}(s^{},n-1)).\]

Based on the estimates, the algorithm classifies undetermined states from \(_{u}\) as verified safe and adds them to \(_{s}\) or classifies them as unsafe and adds to the set of failure states \(_{f}\) (Lines 4 and 5). A state \(s\) satisfies the safety objective \(,_{}\) if \(}_{pes}(s,n)_{}\). Note that the pessimistic safety estimate is achieved in an execution that chooses the most unsafe actions in each non-restricted state. Thus, if for a given state \(}_{pes}(s,n)_{}\), then \(_{^{},}(s,n)_{}\) holds. _This highlights the strength of our algorithm: by assuming the worst policy behavior in unrestricted states, we provide verification results without sampling the policy in every state_. A state \(s\) is unsafe if \(}_{opt}(s,n)_{}\). The optimistic safety probability is achieved in an execution that chooses the safest action in each non-restricted state. Thus, if \(}_{opt}(s,n)_{}\), the policy \(\) cannot pick actions that would yield higher probabilities of satisfying \(\) from \(s\).

**Stopping criteria.** In Line 7, the stopping criterion is defined via a user-defined threshold \(_{}\) for the minimal difference between the estimates. IMT stops if the difference between the optimistic and the pessimistic safety estimate is below the threshold \(_{}\) for all states, i.e., \(_{s}[}_{opt}(s,n)-}_{pes}(s,n)]<_ {}\). For small values of \(}_{pes}\), further restricting the MDP would only marginally change the testing results. Otherwise, IMT continues with sampling the policy and restricting the MDP, as the optimistic and pessimistic estimates are sufficiently different. As an alternative stopping criterion, a user could also define a total testing budget.

**Importance ranking.** In each iteration, IMT computes an importance ranking over all states in the current MDP \(^{(i)}\) (Line 10). In the following steps, the \(m\) most important decisions of the policy are sampled and used to restrict \(^{(i)}\), which results in refined estimates. The rank of a state \(s\) reflects the _maximal difference that a decision can have_ on satisfying the safety objective.

**Definition 3.2**.: (Importance ranking for safety.) Given an MDP \(=(,,,)\), a safety property \(\), and a bound \(n\), the importance ranking \(rank\) is given as the _maximal difference between the optimistic estimates_ with respect to the available actions:

\[ s rank(s,n)=_{a,a^{}}( }_{opt}(s,a,n)-}_{opt}(s,a^{},n)).\]

For the importance ranking, we consider the impact of decisions on the _optimistic estimates_. That is, a state \(s\) is important if, for some actions \(a\) and \(a^{}\), it holds that the expected maximal safety probability that can still be achieved after executing \(a\) from \(s\) is considerably larger than the probability that can be obtained after executing \(a^{}\) from \(s\). The importance ranking returns the set of states \(_{rank}\) of the \(m\) highest ranked states of \(^{(i)}\).

**Sampling the policy.** In Line 11, IMT samples the decisions of the policy in the highest ranked states \(s_{rank}\) of \(^{(i)}\). This results in the set \(=\{(s_{1},a_{1}),,(s_{m},a_{m})\}\) with \(a_{i}=(s_{i})\).

**Restricting the MDP.** In Line 12, our algorithm restricts \(^{(i)}\) according to the sampled policy's decisions, i.e., actions not chosen by \(\) in the sampled states are removed from \(^{(i)}\). Given the current MDP \(^{(i)}=(,,^{(i)},)\) and the sampled state-action pairs \(\), the restricted MDP \(^{(i+1)}=(,,^{(i+1)},)\) has the following probabilistic transition function:

\[ s,s^{}\; a:\;^{(i +1)}(s,a,s^{})=^{(i)}(s,a,s^{})&s _{rank}(s,a)\\ 0&\]

In every iteration of the algorithm, more actions in the MDP model become fixed to the actions chosen by \(\), leading to more accurate safety estimates for \(\), i.e., \(}_{pes}(s,n)\) monotonically increases and \(}_{opt}(s,n)\) monotonically decreases, for all \(s\).

**Theorem 1**.: The algorithm IMT as described in Alg. 1 terminates.

Proof Sketch.: For a fully restricted MDP, for any \(s\), for any \(n\), it holds that \(}_{opt}(s,n)=}_{pes}(s,n)\). This holds because a fully restricted MDP is a Markov chain that describes the policy completely. Hence the estimates are the same.

### Importance-driven Model-Based Testing with Clustering

In this section, we extend IMT by introducing clustering in Alg. 1. Fig. 1 shows the high-level view of IMT including clustering. For problems with very large state spaces, sampling the agents in all highly-ranked states becomes too expensive. To tackle this scalability issue, we propose to cluster similar states and test only a fixed fraction of the states in each cluster. By doing so, we balance the trade-off between accuracy and scalability: The fewer states from a cluster are tested, the higher the scalability of our testing approach. However, the likelihood that some unsafe behavior of the agent remains undetected increases. Clustering offers the additional advantage that similar states are grouped. Under the assumption that the agent implements a policy that selects the same action in similar situations, IMT most likely detects unsafe behavior by sampling a large enough fraction of each cluster. Alg. 2 states the changes in the pseudo-code for IMT with clustering.

**Clustering.** In Line 10, after computing the importance ranking, IMT performs clustering on all states with an importance ranking value greater than some bound \(_{i}=rank(s.n)\). States are clustered according to their state information and their importance value, i.e., we compute a clustering assignment \(cl:[_{i},1]\). This gives a partitioning of \(_{rank}\) into sets of states sharing the same cluster label. Note that any off-the-shelf clustering algorithm can be used to compute the clusters of states. 1

**Executing tests.** In Line 11, the behavior of the policy in the clustered states is evaluated. From each cluster, a fixed percentage \(\) of states is randomly selected to be tested. To test a state \(s\), the agent is executed from \(s\) for a certain number of steps. If the safety objective is violated during the execution, \(s\) is marked as unsafe and added to \(_{f}\). Based on the testing results of the individual states we assign verdicts \(v_{j}\) to the clusters proposing a conservative evaluation of safety. Each cluster \(c_{j}\) with a tested state \(s_{f}\) is assigned a failing verdict \(v_{j}=\). Consequently, all states \(s c_{j}\) are marked unsafe and added to \(_{f}\). Conversely, if a cluster \(c_{j}\) does not contain a single tested state from \(_{f}\), it is assigned a safe verdict \(v_{j}=\), and its states are added to \(_{s}\).

**Restricting the MDP.** In Line 12, IMT restricts \(^{(i)}\) in all states that belong to a cluster \(c_{j}\) by turning the states into sink states. Additionally, if a cluster \(c_{j}\) has the verdict \(v_{j}=\), all states are considered a safety violation.

**The effects of clustering.** Since IMT with clustering only tests a fraction \(\) of each individual cluster, the size and quality of the computed clusters affect the testing process. Clusters that are too large can lead to unnecessary testing efforts, as safe behavior might be deemed unsafe due to conservative evaluation. Additionally, if a cluster contains states that are not sufficiently similar, IMT with clustering may fail to detect unsafe behavior in the policy.

**Complexity Analysis.** We discuss the computational complexity of a single iteration of IMT. The safety estimates are computed via value iteration in \((poly(size()) n)\), with \(n\) being the bound for the objective . The computation of the ranking only requires sorting of the computed estimates and thus requires \((||||)\) time. The subsequent restriction of \(\) is linear in the number of actions present in \(\), i.e. \((||||)\). Lastly, the complexity of sampling the policy is dependent on the network architecture and the costs for clustering the state space depend on the chosen algorithm.

## 4 Experimental Evaluation

All details of the experimental setup can be found in Appendix A. We provide the implementation and tested policies as supplementary material. We compare IMT with our model-based approach _without_ importance ranking (MT) and model-free random testing (RT) as a baseline. Thus, in MT, our algorithm restricts the MDP by the sampled agent's decisions and computes \(e_{opt}\) and \(e_{pes}\) to provide evaluation results on the entire state space but _samples the policy randomly_. For RT, the policy is executed from random states for a certain number of steps. Any violation of the evaluation objective is reported. We report the runtimes averaged over 10 runs for each experiment in seconds, unless indicated otherwise, as _total time_(\(\)_STDev) / _total time for computing the estimates_(\(\)_STDev) / _total time for querying the policy (\(\)_STDev)_.

### Slippery Gridworld

We performed our first experiment in the Farama Minigrid environment . A description of the environment and the RL training parameters are given in Appendix B. The Gridworld is depicted in Fig. 1(a). The agent has to reach the green goal without touching the lava. The lava is surrounded by slippery tiles, stepping on which carries a predefined probability of slipping into lava. The size of the state space is \(||=7 7 4=196\), with \(49\) cells multiplied by \(4\) for the different orientations of the agent. The safety objective \(\) requires the agent to not enter the lava with a probability \(_{} 1.0\).

**RL training parameters.** We trained policies \(_{1}\) and \(_{2}\) by utilizing a DQN. We used a sparse reward function with a reward of \(1\) for reaching the green goal and \(-1\) for falling into the lava. We trained \(_{1}\) using a fixed initial state and \(_{2}\) using initial states uniformly sampled from \(\).

**IMT/MT parameters.** We used a horizon of \(n=\), a minimal difference of \(_{}=0.05\), a number of samples per iteration of \(m=10\). For IMT, no states with a ranking value close to \(0\) were sampled.

**Visualizing IMT.** Fig. 1(b) visualizes the iterations of our IMT algorithm when evaluating \(_{1}\). Per iteration, the picture on the top visualizes the highest-ranked states, with the intensity of the color capturing the ranking. Note that a state represents the \((x,y)\)-coordinates of the grid and the orientation of the agent and is thus visualized as a triangle. Per iteration, IMT samples \(_{1}\) in the highest-ranked

Figure 3: Slippery Gridworld example: Evaluation results of \(_{1}\).

Figure 2: Slippery Gridworld example: setting (left), visualization of evaluating \(_{1}\) (middle), and \(_{2}\) (right).

states (blue triangles) and computes the estimates. The pictures on the bottom show the updated sets of verified states after computing the estimates: \(_{s}\) is visualized in green, \(_{f}\) in red, and \(_{u}\) in blue. Brighter colors represent states in which the decisions of \(_{1}\) were sampled. IMT terminates after \(5\) iterations when evaluating \(_{1}\). Note that the evaluation iteratively reveals the area in which \(_{1}\) violates safety. Fig. 1(c) visualizes the evaluation of the policy \(_{2}\). IMT terminates after a single iteration and positively verifies \(_{2}\) in all states in which the safety objective can be fulfilled.

**Evaluation results.** Fig. 2(a) plots the number of verified states when evaluating \(_{1}\). Solid lines represent IMT results, dashed lines represent MT, where green lines represent \(|S_{s}|\), red lines represent \(|S_{f}|\), and blue lines \(|S_{u}|\). We repeated the analysis via MT \(10\) times: the shaded area represents the minimal and maximal values, and the dashed lines the average number of states. After sampling \(_{1}\) only \(33\) times, IMT terminates with \(|_{s}|=145\), \(|_{f}|=51\), and \(|_{u}|=0\). Thus, IMT provides complete verification results of \(_{1}\) over the entire state space with only \(33\) policy samples. In contrast, on average, MT verifies the entire state space after sampling the agent's decisions almost on the entire state space. Fig. 2(b) plots the values for the optimistic (green) and pessimistic (red) safety estimates for IMT (solid lines) and MT (dashed lines), averaged over all states, which show that the averaged estimates of IMT tighten faster than for MT. Finally, we report the findings of RT in Fig. 2(c) when executing a test case for 10 steps. The results show the clear advantage of exploiting our testing approach. By utilizing testing with model checking, we obtain verification results on the entire state space in contrast to RT which is only able to report a small number of states from which \(\) is violated.

**Runtimes.** The costs for computing the estimates per iteration are in the range of milliseconds. The total runtime to verify \(_{1}\) was \(12.29( 0.7)\) / \(1.11( 0.10)\) / \(0.11( 0.01)\), with IMT and \(25.62( 1.8)\) / \(3.21( 0.23)\) / \(0.41( 0.02)\) with MT.

### UAV Reach-Avoid Task

For the second set of experiments, we test policies computed for drone navigation by Badings et al. . We refer to this work for details regarding the policy and environment, which is illustrated in. Fig. 3(a). The task of the drone is to navigate to the goal location (green box). The safety objective \(\) states that the drone must not collide with a building (grey boxes) and must stay within the boundaries with a probability \(_{} 0.95\). The state space \(||\) comprises \(25.517\) states. The wind in the simulation affects the drone, which is modeled stochastically and controlled through the parameter \(\).

**IMT parameters.** We used \(m=500\) samples per iteration, \(n\), and \(_{}\), as above.

**Evaluation results.** Our testing approach IMT/MT was able to verify control policies computed under five difference noise settings of \(\{0.1,0.25,0.5,0.75,1.0\}\) over the entire state space. Fig. 3(c) gives the number of verified unsafe states per policy. All policies with \(<0.75\) are verified safe in all states from which it is possible to behave safely (light red bars indicate states from which safety violations cannot be avoided). Even though the policies have been specially designed to be safe, IMT was able to find safety violations for policies with \( 0.75\). The policies showed unsafe behavior in \(15\) or \(775\) additional states, respectively, for which safe behavior would have been possible (dark bars). Fig. 3(b) shows the verification results for IMT and MT for \(=0.1\). The test results for the policies with \( 0.25\) can be found in Appendix C. As before, adding importance-ranking for sampling the policy decreases the number of required samples to verify the policy. We performed RT

Figure 4: UAV Task: setting (3(a)), verified states (3(b)), and number of identified safety violations (3(c)).

with a budget of \(50.000\) queries and a maximum number of \(3\) time steps per test case. Averaged over \(10\) runs, RT found \(1120\) (\( 76.29\)) failed test cases for the policy with \(=1.0\) and \(613\) (\( 53\)) failed test cases for \(=0.75\).

**Runtimes.** The runtimes for evaluating the policy are \(269.8( 5.5)\)/ \(73.74( 1.1)\)/ \(0.02( 0.02)\) for IMT, and \(1793( 21.1)\) / \(185.28( 3.2)\) / \(0.02( 0.02)\) for MT for a noise level of \(=0.2515\) This shows that for larger examples, adding importance-based sampling significantly reduces the time needed to verify the policy.

### Atari Skiing

We have evaluated IMT with clustering, IMTc for short, by testing a learned policy for Atari Skiing . In Skiing, the player controls the tilt of the skies to reach the goal as fast as possible. The safety objective \(\) is to avoid collisions with trees and poles with a probability of \(_{} 1.0\). A state describes the \((x,y)\) position, the \(tilt[1..8]\) of the ski, and velocity \(v[0..5]\) of the skier. The state space \(\) comprises roughly \(2.2*10^{6}\) states.

**IMT parameters.** We used a time horizon of \(n=200\), a minimal difference of \(_{}=0.05\), and a fraction \(=0.2\) of tested states per cluster. The clusters have been computed using \(k\)-means for states with \(_{i}>0.8\) with a \(k\) to create average cluster sizes of \(\{25,50,100,150\}\).

**Visualizing IMT.** Fig. 5 visualizes the initial clustering of the highest-ranked states and the iterations of IMTc with an average cluster size of \(=25\). We show the results for states in which \(tilt=4\), i.e. the skier is _aligned with the slope_, and \(v=4\). The visualization for different values of \(tilt\) and \(v\) can be found in Appendix E. We depict states from which the policy has been tested with lighter colors. Darker colors depict implied results.The results show that the agent robustly learned to avoid collisions (it avoids any collision as long as it is not placed too close to an obstacle).

**Evaluation results.** We evaluated IMTc using different values for \(\) and compared it with IMT, i.e. \(=1\), and RT. Fig. 5(a) plots the total number of failure states \(_{f}\) and safe states \(_{s}\) for the whole state space over the number of executed test cases for different values of \(\). The green curves (left to right) plot the results for \(_{s}\) using the cluster sizes \(\{25,50,100,150,1\}\), the red curves for \(_{f}\) accordingly. For comparison, we executed RT \(10\) times and plot the average number of failing (orange dashed) and safe test cases (teal dashed), where the shaded areas show the minimal and

Figure 5: The initial clustering and iterations of the algorithm for an average cluster size of \(25\).

Figure 6: Atari Skiing Example: Evaluation results for the tested policy.

maximal values. The results show that IMTc terminates faster with smaller cluster sizes. Larger cluster sizes overapproximate unsafe regions more heavily. Thus, more testing effort is needed around the unsafe regions in the subsequent iterations. However, all instances of IMTc reduce the testing budget required compared to IMT, which was to be expected since only \(20\%\) of the states of each cluster were tested. These facts are also underlined by Figures (b)b and (c)c, which show the number of safe and failed tests, and the implied verdicts for cluster states in the final iteration, respectively. Fig. (b)b shows that clustering heavily increases the scalability of our approach since it lowers the needed testing budget by up to a factor of \(5\) for \(=25\).

**Runtimes.** The runtimes for evaluating the policy, excluding the time needed to render the testing results, for \(\{25,50,100,150\}\) are 86 minutes \(( 8.3)\) / \(40\)\(( 4.5)\) / \(8.5\)\(( 2.3)\). Evaluating the policy for \(=1\) took 127 minutes / \(59\)\(( 7.3)\) / \(35.9\)\(( 4.4)\).

## 5 Conclusion & Future Work

We presented importance-driven testing for RL agents. The process iteratively (1) ranks the states based on the influence of the agent's decisions on the expected overall safety, (2) samples the DRL policy under test from the ranking, and (3) restricts the model of the environment. By utilizing probabilistic model checking, our algorithm provides upper and lower bounds on the expected outcomes of the policy execution across all modeled states in the state space. These estimates provide formal guarantees about the violation or compliance of the policy to formal properties. We presented an extension of the basic algorithm by introducing clustering to increase scalability. In future work, we will adapt IMT to allow the testing of stochastic policies by adapting the restriction of the MDP and the verification procedure. We will Furthermore, we will introduce several abstraction techniques to further increase the scalability of our approach. Finally, we will use recently proposed approaches to both learn discrete models of domains that are continuous in both their state and action spaces to increase the applicability of IMT and learn the MDP online during the training phase of the policy.

Bettina Konighofer and Stefan Pranger were supported by the State Government of Styria, Austria - Department Zukunftsfonds Steiermark, Martin Tappler was supported by the WWTF project ICT22-023, and Hana Chockler was supported in part by the UKRI Trustworthy Autonomous Systems Hub (EP/V00784X/1), the UKRI Strategic Priorities Fund to the UKRI Research Node on Trustworthy Autonomous Systems Governance and Regulation (EP/V026607/1), and CHAI - EPSRC Hub for Causality in Healthcare AI with Real Data (EP/Y028856/1). We thank both Antonia Hafner and Martin Plank for their proof-of-concept implementations of the experimental evaluation.