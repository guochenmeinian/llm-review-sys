# Connectivity-Driven Pseudo-Labeling

Makes Stronger Cross-Domain Segmenters

 Dong Zhao\({}^{1}\)1, Qi Zang\({}^{1}\)1, Shuang Wang\({}^{1}\)2, Nicu Sebe\({}^{2}\), Zhun Zhong\({}^{3,4}\)2

\({}^{1}\) School of Artificial Intelligence, Xidian University, Shaanxi, China

\({}^{2}\) Department of Information Engineering and Computer Science, University of Trento, Italy

\({}^{3}\) School of Computer Science and Information Engineering, Hefei University of Technology, China

\({}^{4}\) School of Computer Science, University of Nottingham, NG8 1BB Nottingham, UK

Equal contribution. \({}^{}\) Corresponding authors.

###### Abstract

Presently, pseudo-labeling stands as a prevailing approach in cross-domain semantic segmentation, enhancing model efficacy by training with pixels assigned with reliable pseudo-labels. However, we identify two key limitations within this paradigm: (1) under relatively severe domain shifts, most selected reliable pixels appear speckled and remain noisy. (2) when dealing with wild data, some pixels belonging to the open-set class may exhibit high confidence and also appear speckled. These two points make it difficult for the pixel-level selection mechanism to identify and correct these speckled close- and open-set noises. As a result, error accumulation is continuously introduced into subsequent self-training, leading to inefficiencies in pseudo-labeling. To address these limitations, we propose a novel method called Semantic Connectivity-driven Pseudo-labeling (SeCo). SeCo formulates pseudo-labels at the connectivity level, which makes it easier to locate and correct closed and open set noise. Specifically, SeCo comprises two key components: Pixel Semantic Aggregation (PSA) and Semantic Connectivity Correction (SCC). Initially, PSA categorizes semantics into "stuff" and "things" categories and aggregates speckled pseudo-labels into semantic connectivity through efficient interaction with the Segment Anything Model (SAM). This enables us not only to obtain accurate boundaries but also simplifies noise localization. Subsequently, SCC introduces a simple connectivity classification task, which enables us to locate and correct connectivity noise with the guidance of loss distribution. Extensive experiments demonstrate that SeCo can be flexibly applied to various cross-domain semantic segmentation tasks, _i.e._ domain generalization and domain adaptation, even including source-free, and black-box domain adaptation, significantly improving the performance of existing state-of-the-art methods. The code is available at https://github.com/DZhaoXd/SeCo.

## 1 Introduction

Propelled by deep neural networks, remarkable strides have been achieved in semantic segmentation technology [8; 10; 80; 31]. However, deep segmentation models encounter a significant decline in adaptability when confronted with open domains. This challenge is mainly attributed to the inherent domain shift between the training and testing data [22; 9; 13]. To address cross-domain challenges, Domain Adaptation (DA)  and Domain Generalization (DG)  techniques have been proposed to enhance the segmenter's adaptability to the target domain or unseen domains.

Pseudo-labeling  is a widely used technique in cross-domain semantic segmentation tasks, enhancing model efficiency by training with pixels assigned reliable pseudo-labels. The core ofpseudo-labeling is how to eliminate semantic noise. In the DA task, various works are dedicated to designing efficient selection or training methods, such as multi-classifier voting  or augmentation consistency  principles to stabilize noisy training. Furthermore, in the DG task, advanced work  has also shown that pseudo-labeling can be used to leverage in-the-wild data synthesized by stable diffusion, enhancing the segmenter's generalization to unseen domains.

Despite the significant advancements made by these methods, we have identified limitations in the pseudo-labels as depicted in Fig. 1. _Firstly_, under relatively severe domain shifts, most selected reliable pixels appear speckled and remain noisy. (the orange box). _Secondly_, when dealing with in-the-wild data, pixels belonging to the open-set class may also be selected as'reliable' and still exhibit speckle (the cyan box). These make it difficult for the selection mechanism to identify and correct these speckled close- and open-set noises. As a result, speckle noise labels with open-set or closed-set noise are introduced into the subsequent self-training process, leading to severe error accumulation. These issues indicate that constructing pixel-level uncertainty measures to filter noisy pseudo-labels is very challenging, especially in open environments.

Segment Anything Model (SAM)  is a foundation segmentation model that takes both images and geometric prompts (points, boxes, masks) as input and outputs class-agnostic masks. Motivated by this, some attempts  have been made to adopt SAM to refine the pseudo-labels. For instance, using reliable pixel pseudo-labels to prompt SAM to generate class-agnostic masks, and then assigning pseudo-classes to these masks. However, due to inappropriate prompting and semantic noise, these attempts may not improve the quality of pseudo-labels and even exacerbate their semantic noise, as shown in Fig. 1(c).

In this paper, we introduce a novel method called Semantic Connectivity-driven Pseudo-labeling (SeCo) for cross-domain semantic segmentation. SeCo models the distribution of pseudo-label noise at the connectivity level, allowing for effective correction of closed-set noise and removal of open-set noise. SeCo comprises two key components: Pixel Semantic Aggregation (PSA) and Semantic Connectivity Correction (SCC). Initially, PSA splits the categories into the "stuff" and "things" forms. Then, PSA efficiently aggregates speckled pseudo-labels into semantic connectivity 2 by interacting with the Segment Anything Model (SAM). This strategy not only ensures precise boundaries but also streamlines noise localization, as distinguishing noise at the connectivity level is inherently more straightforward than at the pixel level. Subsequently, SCC introduces a sample connectivity classification task for learning connectivity with noisy labels. As connectivity classification focuses on local overall categories, we propose to leverage the technique of _early learning_ in noisy label

Figure 1: Comparison of (a) Pseudo-Labels (PL), (b) pixel-level PL , (c) SAM-refined PL , and (d) the proposed connectivity-level PL. The white area in the PL represents the filtered area. Our method effectively filters out and corrects closed-set noise (the orange box) induced by domain shifts, as well as open-set noise (the cyan box) in the wild data (_e.g._, synthesized from stable diffusion ).

learning [88; 45] to identify connectivity noise, guided by loss distribution. As illustrated in Fig. 1(d), the incorporation of the proposed connectivity-driven pseudo-labels significantly enhances the quality of pseudo-labels, showcasing complete structures and reduced close- and open-set category noise.

In summary, the contributions of this paper are threefold. **First**, we identify the drawbacks of the pseudo-labeling technique and highlight the significance of semantic connectivity in addressing these challenges. **Second**, we propose a Semantic Connectivity-driven pseudo-labeling (SeCo) algorithm, which can effectively generate high-quality pseudo-labels, thereby facilitating robust domain adaptation. **Third**, extensive experiments underscore the versatility of SeCo in effectively addressing various cross-domain semantic segmentation tasks, including domain generalization, traditional, even source-free [38; 28], and black-box  domain adaptation. Notably, SeCo achieves marked enhancements in the more challenging source-free and black-box domain adaptation tasks.

## 2 Method

**Problem Definition**. Cross-domain semantic segmentation aims to transfer a segmentor trained on the labeled source domain \(D_{s}=\{(x_{s}^{i},y_{s}^{i})\}_{i=1}^{I_{s}}\) to the unlabeled target domain \(D_{t}=\{(x_{t}^{i})\}_{i=1}^{I_{t}}\), where \(I_{s}\) and \(I_{t}\) indicate the number of samples for each domain respectively. \(x\) and \(y\) represent an image and corresponding ground-truth label. Presently, mainstream cross-domain segmentation methods optimize the following objective to enhance model adaptability,

\[=_{s}(x_{s},y_{s})+_{t}(x_{t},_{ t}),\] (1)

where \(_{s}\) is the supervised cross-entropy loss, \(\) is the trade-off weight, \(_{t}\) is the unsupervised pseudo-labeling loss, and \(_{t}\) is the pseudo-label. This formula underscores the critical importance of the quality of pseudo-labels in improving the model's cross-domain ability. To alleviate the noise in pseudo-labels, various estimation [97; 3] and calibration [73; 7; 42; 79] methods have been introduced for pseudo-label selection. However, as mentioned above, the filtered pseudo-labels still encounter issues of constrained semantics and challenging localization of category noise.

### Overview

The presented **S**emantic **Co**onnectivity-driven pseudo-labeling (SeCo) is illustrated in Fig. 2. SeCo comprises two components, namely Pixel Semantic Aggregation (PSA) and Semantic Connectivity Correction (SCC), working collaboratively to refine the low-quality and noisy pseudo-labels into high-quality and clean pseudo-labels. Initially, PSA aggregates pixels from the filtered pseudo-labels into connections by interacting with the _segment anything model_ (SAM)  through stuff and things interactions. Subsequently, PSA segments the image into multiple connectivities based on their semantics. Guided by the connectivity set, SCC establishes a connectivity classifier, conducts

Figure 2: The pipeline of the proposed Semantic Connectivity-Driven Pseudo-labeling (SeCo). In (a), pixel-level pseudo-labels are interactively aggregated into connectivity by SAM using the “stuff and things” manner, grouping semantically similar pixels. Then, in (b), these connectivities are treated as classification objects and are identified for semantic noise by noisy learning. This process is handled offline, and the corrected high-quality pseudo-labels can be used for further self-training.

connectivity pooling on image features, and classifies each connectivity. Leveraging information about fitting difficulty and loss distribution, SCC identifies and corrects noise. Finally, the connectivity-driven pseudo-labels, characterized by comprehensive semantics and low noise are achieved.

### Pixel Semantic Aggregation

**Motivation: Why SAM?** Pixel semantic aggregation (PSA) proposes utilizing reliable pixels within pseudo-labels as category references and subsequently aggregating pixels that share similar semantics into connections. Intuitively, the above goals can be achieved through interactive segmentation [66; 44; 62] or pixel clustering [55; 1], but traditional techniques often struggle to accurately identify semantic boundaries in complex scenes, resulting in ambiguous aggregation. The advent interactive segmentation model, _segment anything model_ (SAM) , provides powerful semantic capture capabilities. With reasonable prompts, SAM has the potential to give accurate semantic boundaries even in complex scenes [47; 33]. Building on SAM's remarkable capability, we are motivated to investigate how to leverage the reliable yet limited pseudo-labels to prompt SAM effectively and enhance the completion of pseudo-label semantics.

**Discussion on Utilization of SAM.** We analyze two naive solutions as outlined below. The first involves sampling the center pixels of each connected region on the pseudo-label as prompt points, as depicted in Fig. 3(a). We observe that when prompt points of the same category contain noise, this method compromises the aggregated segmentation structure. The disruption is attributed to noisy prompts interfering with the cross-attention mechanism in SAM .

The second method represents an improved way, called semantic alignment , aligning pseudo-labels with the connectivity established by SAM. This involves selecting the pseudo-label with the maximum proportion in each connectivity as the category for the entire region, as illustrated in Fig. 3 (b). We note that while this approach can refine pseudo-labels, it is consistently influenced by SAM's uncertain semantic granularity, particularly in the context of neighboring instance objects. Fig. 3 (b) provides examples of failures in this method, where SAM aggregates two categories, "traffic sign" and "pole" into a semantic connected region, leading to misaligned pseudo-labels due to this uncertainty. Our analysis indicates that this issue arises because SAM constructs connectivity by uniformly sampling points in space as prompts and subsequently filtering out redundantly connected regions. This fails to ensure corresponding sampling points for neighboring instance objects, resulting in a semantic granularity deviation between SAM's connectivity and specific segmentation tasks.

In summary, "prompts" interaction can aid in determining semantic granularity but is vulnerable to noise; Conversely, "alignment" interaction can alleviate noise interference but is susceptible to uncertain semantic granularity. **Hence, implementing SAM in cross-domain segmentation is a considerable challenge without a thoughtful design.** (For more discussion about SAM, see Sec.3.2)

**Our Strategy.** Building upon the analysis above, we find that noise significantly affects "stuff" categories due to their larger size and higher pixel proportions, making them more prone to selecting noisy pixels. On the other hand, semantic granularity uncertainty is more prevalent in "things" categories, given their smaller size and dense adjacency. To this end, we propose to interact with SAM in the form of "stuff" and "thing". Specifically, for "stuff", we utilize semantic alignment to mitigate the impact of noisy prompts, while for "things", we employ box and point prompts to guide the semantic precision. The detailed algorithm is in Algorithm 1. An illustration of the proposed strategy is shown in Fig. 3(c).

Figure 3: Comparison of Pseudo-Label (PL) aggregation using different interactive methods with SAM . Both Point Prompt-based Interaction (PP-PL) and Semantic Alignment-based Interaction (SA-PL) amplify pseudo-label noise, whereas our method alleviates this issue.

### Semantic Connectivity Correction

PSA aggregates both precise and noisy pseudo-labels into connectivities, facilitating the locating and correction of noise. This simplicity arises from the fact that distinguishing noise at the connectivity level is much easier than at the pixel level, as it eliminates the necessity to scrutinize local semantics and instead focuses on the overall category (See experimental analysis in Appendix D).

Inspired by this, we propose Semantic Connectivity Correction (SCC), introducing a simple connectivity classification task and detecting noise through loss distribution. Specifically, given the input image \(x_{i}\), we first obtain the connectivity mask list \(M=\{m^{i,n}\}_{n=1}^{N_{i}}\) and its corresponding connectivity-level pseudo-label \(_{sc}=\{_{sc}^{i,n}\}_{n=1}^{N_{i}}\) from PSA, where \(N_{i}\) represents the number of connectivities for the \(i\)-th sample \(x_{i}\). Then, we set up a connectivity classifier, comprising a feature extractor \(_{}\) and a linear layer \(\), and optimize it with the following objective,

\[L_{scc}=_{i,k,n}-_{sc}^{i,n,k}(([ _{}(x^{i}),m^{i,n}])).\] (2)

\([,]\) denotes the average pooling of features corresponding to the input mask, \(k[0,1,...K]\), and \(K\) is the category number. Optimizing \(L_{scc}\) conducts a \(K\)-way classification for each connectivity with clean and noisy labels.

Based on observations of _early learning_ in noisy learning: when training on noisy labels, deep neural networks, in the early stages of learning, initially match the training data with clean labels, and subsequently memorize examples with erroneous labels. We warm up the connectivity classifier for several epochs and then obtain the loss distribution by calculating Eq. (2) for each connectivity. As shown in Fig. 4, the loss of connectivities presents bimodal distribution, and the clusters with larger losses correspond to higher noise, which better conforms to the observations. To this end, we employ a two-component Gaussian Mixture Model to effectively model the loss distribution using the Expectation-Maximization algorithm . Subsequently, the probability of connectivity being noisy, denoted as \(\), can be reasonably approximated by the Gaussian distribution associated with bigger loss, _i.e._, \(^{i,n}=p(c|L_{scc}(x,m^{i,n}))\). \(c\) is the parameters of the corresponding Gaussian distribution. We keep the clean connectivity by setting a noise threthd \(_{ns}\), _i.e._,

\[D_{clean}=\{(x_{i},y_{sc}^{i,n})|^{i,n}<_{ns}\}.\] (3)

Besides, we find that many noisy connectivities can be corrected by setting another correction threshold \(_{cr}\) on the output probability of connectivity classifier, _i.e._,

\[D_{corr}=\{(x^{i},k)|p_{scc}^{i,n,k}>_{cr},^{i,n}>_{ns}\},\] (4)

where \(p_{scc}^{n,k}\) represents the probability of class \(k\) for the \(n\)-th connectivity. We take the union of the two sets as the final connectivity-driven pseudo-label set \(D_{all}=D_{clean} D_{corr}\) for self-training.

### Implementation on Domain Adaptation & Generalization Tasks

We provide solutions on connectivity-driven pseudo-labels for different domain adaptation tasks.

**Domain Generalization(DG).** Following , we first use Stable Diffusion to synthesize simulated unseen domain data. Then, we use the DG model and our SeCo to pseudo-label these synthesized data and retrain the DG model on them.

**Unsupervised Domain Adaptation.** The connectivity-driven pseudo-label set \(D_{all}\) serves two primary functions. Firstly, they contribute to the pseudo-labeling \(L_{t}\) loss in Eq. (1), providing

Figure 4: The loss distribution plot of semantic connectivity on different cross-domain segmentation tasks. By establishing a bi-modal Gaussian function, noisy connectivity can be effectively located.

accurate semantic guidance for the target domain. The second objective is to mitigate category bias in domain adaptation. We treat \(D_{all}\) as a sample pool, where we resample minority classes in the target domain and duplicate them through copy-paste operation  onto both domains.

**Source-free & Black-box Domain Adaptation.** In these scenarios, source access is restricted. This limitation prevents the deployment of the source loss \(L_{s}\) in Eq. (1), making self-training more vulnerable to noise interference. Connectivity-driven pseudo-label set \(D_{all}\) brings a novel idea to mitigate these challenges. With its contribution to accurate semantics and low noise, \(D_{all}\) can be viewed as a well-organized labeled set, thereby transforming source-free and black-box domain adaptation tasks into semi-supervised segmentation tasks[84; 83].

**Discussion about Fair Comparison.** We acknowledge the potential concern regarding unfair comparisons between our SAM-incorporating method and existing approaches. **First**, it is important to emphasize the considerable challenges in applying SAM to CDSS tasks. Detailed experiments are in Table 5 of Section 3.2. We believe our work makes a significant contribution to exploring the potential of SAM-enhanced CDSS tasks. **Second**, our method is designed to be integrative, enhancing existing pseudo-labeling methods rather than competing with them (as demonstrated in Section 3.1 and Tables 1 and 2). **Third**, we conducted experiments without using SAM to validate that the proposed semantic connectivity denoising idea still has advantages, as shown in Fig. 5 of Section 3.2. We hope this work can inspire the community to further investigate the effective utilization of SAM in CDSS, embracing the popular trend of facilitating visual tasks with large-scale models, such as enhancing classification [52; 81; 61] with large-language models (_e.g.,_ GPT-3).

## 3 Experiments

**Datasets.** We employ two real datasets (Cityscapes  and BDD-100k ) alongside two synthetic datasets (GTA5  and SYNTHIA ). The details of these datasets are introduced in Section B.

**Implementation Details.**_Traditional UDA_: We opted for two network architectures: DeepLabV2  with ResNet-101  and SegFormer  with MiT-B5. For DeepLabV2, we chose two classical methods, AdvEnt  and ProDA , as the baselines. For SegFormer, we selected two highly successful UDA methods, DAFormer  and HRDA , as the baselines. _Source-free UDA_: We maintain DeepLabV2 as the base network to align with existing works. We chose HCL  and the current SOTA method DTST  as baselines. _Black-box UDA_: We use two SOTA black-box UDA methods, DINE  and BiMem , as baselines. Across all tasks, for each baseline method, we select the pseudo-labels of its predicted top 50% confidence-ranked pixels for SeCo processing. Subsequently, we utilize the SAM  with Vision Transformer-H (ViT-H)  to generate connectivities. We refrain from using SAM to refine pseudo-labels for test data to avoid introducing extra inference overhead. The automatic mask generation process in SAM adheres to the official parameter settings. In Algorithm 1, the enlargement factor for the bounding box area is set to 1.5. The connectivity classifier is trained only for 5000 iterations in an early learning way for all tasks. The noise threshold (\(_{ns}\)) and correction threshold (\(_{cr}\)) are configured at 0.60 and 0.95, respectively.

### Combined SeCO with State-of-the-Art (SOTA) Methods

The performance of SeCo is shown in Tables 1, 2, 3 and 4. Overall, experimental results indicate that SeCo can be integrated with various SOTA domain adaptation and domain generalization methods, and significantly enhances their adaptability. Moreover, SeCo exhibits notable improvements for both source-free and black-box adaptation, overcoming limitations with the source domain data.

**Domain Generalization**. In this experiment, we followed CLOUDS , which uses synthetic data from Stable Diffusion (SD) to assist DG. Our aims are: 1) to show that our method can handle challenging synthesized data with open-set noise, and 2) to compare our SAM usage with the competitive scheme in CLOUDS. Compared to the DG baseline SHADE and HRDA, our method significantly improves their performance by 6.8% and 3.1%, respectively. Compared to CLOUDS, which also uses SAM, we achieve even greater improvements, further enhancing performance by 1.4% and 1.0%.

**Domain Adaptation**. _GTA5 \(\) Cityscapes._ Results are reported in Table 1. In the UDA setting, the integration of SeCo with AdvEnt  leads to a notable performance improvement, achieving a \(13.4\%\) increase in mIoU score. Combining SeCo with ProDA  results in a \(9.4\%\) increase in mIoU score, establishing a new SOTA using DeepLabV2. When integrated with the high-performing Segformer , SeCo consistently improves DAFormer by 5.3% in mIoU score and HRDA by 2.3% in mIoU score. In source-free UDA, SeCo exhibits stronger advantages, providing robust self-training, and elevating the performance of existing SOTA methods, HCL and DTST, by 10.6% and 8.5%, respectively. In the more stringent black-box adaptation setting, SeCo remains effective. When integrated with two SOTA methods, DINE and BiMem, SeCo obtains improvements by 7.7% and 8.5%, respectively. _SYNTHIA \(\) Cityscapes_. Results are reported in Table 2. Despite the larger domain shift of this task, SeCo maintains similar improvements as the previous task, which further underscores the potential of SeCo under data protection scenarios. _GTA5 \(\) BDD-100k_. Results are reported in Table 4. This task involves complex mixed-weather adaptation. SeCo consistently achieves stable performance improvements. Specifically, SeCo enhances the performance of two baseline methods, PyCDA  and ProDA , by 11.4% and 7.8%, respectively, establishing itself as the new state of the art for this benchmark. In the source-free setting, SeCo achieves an improvement of 6.2% over the state-of-the-art method , demonstrating sustained and stable performance gains.

### Analysis and Ablation Study

**Can SAM benefit Cross-Domain Semantic Segmentation in another naive way?** In Table 5, we conduct three types of experiments to demonstrate that _directly applying SAM on Cross

    &  &  \\  AdvEnt \({}^{}\) & 87.0 & 44.1 & 79.7 & 9.6 & 0.6 & 24.3 & 4.8 & 7.2 & 80.1 & 83.6 & 56.4 & 23.7 & 27.2 & 36.6 & 12.8 & 33.7 & 40.8 \\ AdvEnt + Ours & 87.9 & 47.7 & 82.9 & 20.1 & 1.1 & 38.2 & 29.2 & 28.6 & 86.5 & 85.7 & 64.5 & 29.6 & 84.5 & 44.3 & 39.1 & 47.4 & **51.1**(\(\)0.33) \\ ProDA \({}^{}\) & 87.1 & 44.0 & 23.2 & 69.7 & 10.0 & 42.0 & 45.8 & 34.2 & 36.7 & 81.3 & 68.4 & 22.1 & 87.7 & 50.0 & 31.4 & 38.6 & 51.9 \\ ProDA \({}^{}\) & 88.1 & 49.8 & 86.9 & 33.9 & 14.4 & 46.6 & 54.3 & 44.7 & 85.8 & 85.7 & 87.4 & 44.0 & 36.0 & 55.2 & 45.0 & 50.5 & **58.6**(\(\)6.7) \\  DAFormer \({}^{}\) & 84.5 & 40.7 & 88.4 & 41.5 & 65.5 & 50.0 & 55.0 & 54.6 & 86.0 & 89.3 & 73.2 & 42.8 & 87.2 & 53.2 & 53.9 & 61.7 & 60.9 \\ DARFormer *Ours & 88.9 & 49.9 & 90.7 & 46.2 & 73.3 & 55.0 & 63.2 & 57.8 & 87.7 & 92.2 & 76.0 & 51.5 & 89.5 & 61.3 & 59.7 & 64.9 & **65.1**(\(\)4.2) \\  HRDA \({}^{}\) & 85.2 & 47.7 & 88.8 & 49.5 & 87.2 & 65.7 & 60.9 & 85.9 & 92.9 & 79.4 & 52.8 & 89.0 & 64.7 & 63.9 & 64.9 & 64.5 \\ IRDA + Ours & 90.7 & 50.6 & 89.9 & 51.6 & 84.4 & 89.4 & 69.9 & 64.9 & 89.1 & 95.3 & 81.9 & 58.3 & 51.9 & 58.2 & 91.4 & 66.3 & 65.4 & 66.1 & **68.5**(\(\)2.3) \\   &  \\    
    &  &  \\  AdvEnt \({}^{}\) & 87.0 & 44.1 & 79.7 & 9.6 & 0.6 & 24.3 & 4.8 & 7.2 & 80.1 & 83.6 & 56.4 & 23.7 & 72.7 & 33.6 & 12.8 & 33.7 & 40.8 \\ AdvEnt + Ours & 87.9 & 47.7 & 82.9 & 20.1 & 1.1 & 38.2 & 29.2 & 28.6 & 86.6 & 85.7 & 64.5 & 29.6 & 84.5 & 44.3 & 39.1 & 47.4 & **51.1**(\(\)0.33) \\ ProDA \({}^{}\) & 87.1 & 44.0 & 23.2 & 69.7 & 10.0 & 42.0 & 45.8 & 34.2 & 36.7 & 81.3 & 68.4 & 22.1 & 87.7 & 50.0 & 31.4 & 38.6 & 51.9 \\ ProDA + Ours & 88.1 & 49.8 & 86.9 & 33.9 & 14.4 & 46.6 & 54.3 & 44.7 & 85.8 & 85.7 & 87.4 & 44.0 & 36.0 & 55.2 & 45.0 & 50.5 & **58.6**(\(\)6.7) \\  DAFormer \({}^{}\) & 84.5 & 40.7 & 88.4 & 41.5 & 65.5 & 50.0 & 55.0 & 54.6 & 86.0 & 89.3 & 73.2 & 42.8 & 87.2 & 53.2 & 53.9 & 61.7 & 60.9 \\ DARFormer *Ours & 88.9 & 49.9 & 90.7 & 46.2 & 73.3 & 55.0 & 63.2 & 57.8 & 87.7 & 92.2 & 76.0 & 51.5 & 89.5 & 61.3 & 59.7 & 64.9 & **65.1**(\(\)4.2) \\  HRDA \({}^{}\) & 85.2 & 47.7 & 88.8 & 49.5 & 87.2 & 65.7 & 60.9 & 85.9 & 92.9 & 74.2 & 52.8 & 89.0 & 64.7 & 63.9 & 64.9 & 65.8 \\ IRDA+ Ours & 90.7 & 50.6 & 89.9 & 51.6 & 84.4 & 89.4 & 69.9 & 64.9 & 89.1 & 95.5 & 85.1 & 59.2 & 92.4 & 66.5 & 65.4 & 66.1 & **68.5**(\(\)2.3) \\   &  \\   HCL \({}^{}\) & 80.9 & 34.9 & 76.7 & 66.6 & 0.2 & 36.1 & 20.1 & 28.2 & 79.1 & 83.1 & 55.6 & 25.6 & 78.8 & 32.7 & 24.1 & 37.7 & 43.5 \\ HCL + Ours & 88.3 & 46.0 & 83.3 & 10.6 & 15.5 & 38.6 & 29.9 & 39.0 & 86.9 & 66.0 & 46.0 & 38.7 & 44.7 & 39.2 &_Domain Semantic Segmentation (CDSS) can hardly obtain improvement_. 1 Use the backbone of SAM to empower CDSS. Table 5 shows that **utilizing SAM as the backbone achieves a notable performance drop** in UDA. The reduction mainly came from rare classes (_e.g._ "train" and "truck"). Both DAFomer and HRDA use Feature Distance to keep the semantic knowledge of ViT-B pre-trained on ImageNet, effectively improving the adaptation for rare classes. However, SAM's pre-training does not consider such semantic knowledge and thus obtains inferior results in rare classes. 2 Is the UDA still necessary, if we use CLIP+SAM (CSAM) to get the initial pseudo label? We conduct the following experiment to explore the feasibility of CLIP+SAM: a) Use SAM to segment the input image. b) Extract the largest bounding rectangle from each segment. c) Create text descriptions for categories, e.g., "a photo of a road." d) Use CLIP to match image patches with text descriptions, assigning text labels as categories. CLIP+SAM+UDA combines pseudo-labels from CLIP+SAM and UDA, using voting fusion to select consistent predictions for training. Table 5 shows that **CSAM cannot obtain competitive results** on the target data, even when combined with UDA methods. The reasons are below. Given spatially uniform sampling as prompt points, 1) SAM is prone to over-segmentation results, which makes it difficult for CLIP to obtain sufficient context from small segments. 2) SAM's segments may conflict with the defined semantics of the target data, _e.g._, SAM always treats 'poles' and 'traffic sign' as one segment. 3 Is using SAM on coarse pseudo-labels enough? Table 5 shows the comparison of using vanilla SAM and our method on the source-trained model's (Coarse) and UDA-adapted model's prediction. It shows that **the gains of using SAM on coarse pseudo-labels are minimal** and even negative on strong UDA baselines. This is because SAM risks introducing more semantic noise when extending semantic boundaries. Our method, even with coarse pseudo-labels, allows SAM to achieve greater benefits. The above discussions indicate the difficulty of applying SAM in cross-domain segmentation and the non-trivial design of our SAM-based method.

**Is Our Method Specific to SAM?** The proposed Semantic Connectivity Correction (SCC) is general and not specific to SAM. SCC can work on any form of pseudo-labels, although their connectivity structure may not be as good as SAM's. In Fig. 5, we report the results of directly using SCC on pseudo-labels generated by existing UDA models. Under this exact fair comparison, our method still achieves good improvement and is more competitive than widely used Knowledge Distillation (KD) . Again, we want to emphasize that this work aims to bring a new perspective for enhancing CDSS to embrace the huge benefit of large-scale models, which is a non-trivial contribution.

    &  &  &  \\ SourceGTA5\(\) & Rainy & Snowy & Cloudy & Overcast & Compound + Open Overcast \\   Source Only & 28.7 & 29.1 & 33.1 & 32.5 & 30.9 \\   \\  ML-BPM \({}^{^{2}}\) & 40.5 & 39.9 & 42.1 & 40.9 & 40.9 \\ OSC \({}^{^{3}}\) & - & - & - & - & 44.0 \\  PyCDA \({}^{^{0}}\) & 33.4 & 32.5 & 36.7 & 37.8 & 35.1 \\ PyCDA+Ours & 43.6 & 42.1 & 49.7 & 50.7 & **46.5**(\(\)11.4) \\  ProDA \({}^{^{1}}\) & 40.3 & 40.6 & 43.2 & 42.5 & 41.7 \\ ProDA\(\)Ours & 47.6 & 45.7 & 51.9 & 52.6 & **49.5**(\(\)7.8) \\   \\  SFOCDA \({}^{^{2}}\) & 35.4 & 33.4 & 41.4 & 41.2 & 37.9 \\ SFOCDA+Ours & 41.7 & 42.1 & 44.7 & 47.9 & **44.1**(\(\)46.2) \\   

Table 4: The comparison of performance in terms of mIoU score (%) on the Open Compound domain adaptation task between SeCo (ours) and other state-of-the-art methods.

    & Backbone & Using SAM & Cityscapes & BDD-100K & Mapillary & Average \\  SHADE \({}^{^{3}}\) & & ✗ & 46.6 & 43.7 & 45.5 & 45.3 \\ TLDR \({}^{^{3}}\) & & ✗ & 47.6 & 44.9 & 48.8 & 47.1 \\ MoDity \({}^{^{3}}\) & ResNet-101 & ✗ & 48.8 & 44.2 & 47.5 & 46.8 \\  + CLOUPs \({}^{^{4}}\) & & ✓ & 50.6 & 44.8 & 56.6 & 50.7 \\ + SeCo (Ours) & & ✓ & **52.4** & **46.1** & **57.7** & **52.1** \\   HRDA \({}^{^{2}}\) & & ✗ & 57.4 & 49.1 & 61.1 & 55.9 \\ + CLOUPs \({}^{^{4}}\) & MiT-B5 & ✓ & 58.1 & 53.8 & 62.3 & 58.1 \\ + SeCo (Ours) & & ✓ & **58.8** & **54.9** & **63.6** & **59.1** \\   

Table 3: Performance improvement in terms of mIoU score (%) by incorporating SeCo into existing domain generalization methods using GTA5 as the source domain.

**Ablation Study.** The results of ablation experiments are presented in Table 6. We conduct analyses across different domain adaptation settings as follows.

_In UDA_, PSA yields a performance improvement of 5.4% for ProDA , surpassing the gains achieved by PSA\({}^{(b)}\). Additionally, SCC builds upon PSA, contributing an additional 5.0% enhancement to ProDA. A similar trend is observed in the ablation study on DAFormer . These findings suggest that, at the interaction level with SAM, PSA proves more effective than PSA\({}^{(b)}\); however, interacting solely with SAM is insufficient for achieving substantial self-training performance gains. SCC plays a crucial role in further filtering out noise propagated by SAM, leading to a significant enhancement in UDA performance.

_In source-free UDA_, PSA results in a 3.0% performance improvement for DTST , still outperforming PSA\({}^{(b)}\). Due to the substantial initial pseudo-label noise in the source-free setting, PSA aggregates more noisy connections, resulting in a diminished performance gain compared to UDA. SCC, building upon PSA, brings an improvement of 5.4%, reinforcing the notion that SCC can effectively filter and correct propagated pseudo-labels.

_In balck-box UDA_, PSA brings a marginal improvement, with only a gain of 1.2%. SCC on top of PSA achieves a substantial improvement of 7.3%, further confirming the aforementioned conclusions. These results underscore the importance of correcting noise within connections, especially under more significant domain shifts and weaker initial segmentation results.

    & Settings & PSA\({}^{(b)}\) & PSA & SCC &  &  & Settings & PSA\({}^{(b)}\) & PSA & SCC &  \\  ^{}\)} &  &  & & 53.7 & ^{}\)} &  &  & 52.1 \\  & & & & & 60.9 & & & & & 56.1 \\ ProDA \({}^{}\) & & & & & 62.1 & DTST \({}^{}\) & & & & 57.9 \\  & & & ✓ & ✓ & **64.1** & & & & & ✓ & **60.5** \\  ^{}\)} &  &  & & 68.2 &  &  &  &  & 48.2 \\  & & & & & 69.7 & & & & & 52.2 \\   & & & ✓ & & 70.3 & BiMem \({}^{}\) & & & & & 54.4 \\   & & & ✓ & ✓ & **73.4** & & & & ✓ & **56.7** \\   

Table 6: Ablation experiments of SeCo under various UDA settings on GTA5 \(\) Cityscape adaptation task. PSA: Pixel Semantic Aggregation. SCC: Semantic Connectivity Correction. PSA\({}^{(b)}\) refers to the interaction with SAM using semantic alignment , as shown in Fig. 3. SF-UDA: source-free UDA. BB-UDA: black-box UDA.

Figure 5: Comparison between widely used pixel-level distillation  and Semantic Connectivity Correction (SCC) without using SAM across various baselines.

    &  &  \\   & DAFormer & + (SAM ViT-B) & + Ours & HPDA & + (SAM ViT-B) & + Ours \\
**G2C** & 68.2 & 64.1 (-4.1) & **73.4** (+5.2) & 73.8 & 69.1 (-4.7) & **76.1** (+2.3) \\
**S2C** & 60.9 & 60.2 (-0.7) & **65.1** (+4.2) & 65.8 & 63.7 (-2.1) & **68.5** (+2.3) \\   &  \\   & CSAM & CSAM + DAFormer & Ours + DAFormer & CSAM + HPDA & Ours + HPDA \\
**G2C** & 43.7 & 69.1 & **73.4** & 73.5 & **76.1** \\
**S2C** & 41.7 & 61.1 & **65.1** & 65.2 & **68.5** \\   &  \\   &  &  &  &  \\    & & Vanilla SAM & Ours & Vanilla & Ours \\ AdvEnt \({}^{}\) & 45.5 & 48.6 (+3.1) & 53.9 (+8.4) & 50.9 (+5.5) & **58.9 (+13.4)** & 69.1 \\ ProDa \({}^{}\) & 53.7 & 50.1 (-1.7) & 52.8 (+4.5) & 57.9 (+4.3) & **64.1 (+9.4)** & 69.1 \\ DAFormer \({}^{}\) & 68.2 & 67.7 (-0.5) & 69.9 (+1.7) & 69.7 (+1.5) & **73.4 (+5.3)** & 76.4 \\ HRDA \({}^{}\) & 73.8 & 72.7 (-1.1) & 74.6 (+0.8) & 74.6 (+0.8) & **76.1 (+2.3)** & 77.1 \\   

Table 5: Comparison of different ways of applying SAM to cross-domain semantic segmentation (CDSS). G2C is GTA5 \(\) Cityscape. S2C is SYNTHIA \(\) Cityscape. 3 is carried on G2C.

**Detailed ablation on prompt way.** We conduct ablation studies on "Prompting Only" (PO) and "Semantic Alignment"(SA) across multiple tasks in GTA \(\) Cityscape in Table 7. We provide two metrics for these detailed ablations: PL mIoU (pseudo-label quality on the training set) and Val. mIoU (model performance on the validation set after training with those pseudo-labels). As shown in the Table 7, the "prompting only" method reduces the quality of pseudo-labels in the training set, leading to poor adaptation performance. This is because the unreliable interaction method introduces excessive noise into the pseudo-labels generated by SAM. "Semantic alignment" improves the quality of the training set pseudo-labels, but the improvement is limited, resulting in limited adaptation benefits. In contrast, our method enhances the quality of the training set pseudo-labels through better interaction, leading to superior performance gains.

**Ablation studies on our SCC and Dividemix.** Our SCC is partly inspired by DivideMix, however, we focus on mitigating the pixel-level noises in pseudo-labels raised by domain shifts and SAM refinement, while Dividemix focuses on mitigating the image-level label noises. Besides, we would like to emphasize that one of the main contributions of SCC is to provide the idea of denoising at the connectivity level, which makes it possible to apply other image-level denoising methods such as Dividemix to segmentation tasks. To better verify the effectiveness of our SCC, we make two experiments as show in Table 8: a) directly applying DivideMix to pixel-level denoising. b) using DivideMix to denoise the pixels aggregated by our PSA (using SAM). The results show that pixel-level denoising methods based on DivideMix are inferior to SCC even with SAM, highlighting the advantage of denoising at the connectivity level.

## 4 Conclusion

In this work, we propose Semantic Connectivity-driven Pseudo-labeling (SeCo), formulating pseudo-labels at the connectivity level for structured and low-noise semantics. SeCo, comprising Pixel Semantic Aggregation (PSA) and Semantic Connectivity Correction (SCC), efficiently aggregates speckled pseudo-labels into semantic connectivity with SAM. SCC introduces a simple connectivity classification task for locating and correcting connected noise. Experiments demonstrate SeCo's flexibility and significant effectiveness in performance across various cross-domain semantic segmentation tasks. We hope that this work could inspire the community to apply SAM to more cross-domain, semi-supervised and few-shot segmentation settings.

## 5 Acknowledgments

This work was supported by the National Natural Science Foundation of China under Grant Nos. 62271377, the National Key Research and Development Program of China under Grant Nos. 2021ZD0110400, 2021ZD0110404, the Key Research and Development Program of Shannxi (Program Nos. 2021ZDLGY01-06, 2022ZDLGY01-12, 2023YBGY244, 2023QCYLL28, 2024GX-ZDCYL-02-08, 2024GX-ZDCYL-02-17), the Key Scientific Technological Innovation Research Project by Ministry of Education, the State Key Program and the Foundation for Innovative Research Groups of the National Natural Science Foundation of China (61836009), the Joint Funds of the National Natural Science Foundation of China (U22B2054), the MUR PNRR project FAIR (PE00000013) funded by the NextGenerationEU, and the EU Horizon project ELIAS (No. 101120237).

   Prompt Way & Base (w/o SAM) & Prompting Only & Semantic Alignment & PSA & PSA+SCC \\  SeCo+ProDA (UDA) & 53.7 & 48.0 (-5.7) & 60.9 (+7.2) & **62.1 (+8.4)** & 64.1 \\ SeCo+DAformer (UDA) & 68.2 & 64.6 (-3.6) & 69.7 (+1.5) & **70.3 (+2.1)** & 73.4 \\ SeCo+DTST (SF-UDA) & 52.1 & 46.7 (-5.4) & 56.1 (+4.0) & **57.9 (+5.8)** & 60.5 \\ SeCo+BiBiMem(BB-UDA) & 48.2 & 42.8 (-5.4) & 52.4 (+4.2) & **54.4 (+6.2)** & 56.7 \\   

Table 7: Ablation studies on “Prompting Only” (PO) and “Semantic Alignment”(SA) across multiple tasks in GTA \(\) Cityscape.

    & GTA \(\) Cityscapes (UDA) & SYNTHIA \(\) Cityscapes (UDA) & GTA5 \(\) BDD-100k (OC-DA) \\  ProDA (CVPR’21) & 53.7 & 51.9 & 41.7 \\ DivideMix  & 49.8 & 47.6 & 37.4 \\ PSA+DivideMix & 60.1 & 53.4 & 44.2 \\ SeCo & **64.1** & **58.6** & **49.5** \\   

Table 8: Detailed comparison of our SCC and Dividemix across multiple domain adaptation tasks.