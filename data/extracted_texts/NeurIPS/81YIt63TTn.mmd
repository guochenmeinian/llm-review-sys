# Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging

Zhenyi Lu\({}^{1,2}\) Chenghao Fan\({}^{1,2}\) Wei Wei\({}^{1,2}\) Xiaoye Qu\({}^{1}\) Dangyang Chen\({}^{3}\) Yu Cheng\({}^{4}\)

\({}^{1}\) School of Computer Science & Technology, Huazhong University of Science and Technology,

\({}^{2}\) Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL),

\({}^{3}\) Ping An Property & Casualty Insurance Company of China, Ltd.,

\({}^{4}\) The Chinese University of Hong Kong.

{luzhenyi529,facicofan}@gmail.com, {weiw, xiaoye}@hust.edu.cn,

chendangyang273@pingan.com.cn, chengyu@cse.cuhk.edu.hk

 Equal contribution.Corresponding authors.

###### Abstract

In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on \(20\) datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of \(28.34\%\) in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. 1

## 1 Introduction

In recent years, Large Language Models (LLMs) have demonstrated notable success across various Natural Language Processing (NLP) tasks , including code generation , solving math problems , multilingualism , _etc._ These models, with billions of parameters, excel in various downstream tasks  but require extensive training on large datasets using thousands of GPUs. The considerable computational and energy costs  limit their specialization and deployment in resource-constrained environments .

To tackle this challenge, model fusion has emerged as a promising solution . One notable paradigm is model merging , where multiple task-specific models, or "experts", are combined into a single unified model. This unified model can quickly adapt to new tasks without the need to retrain a large model. Various techniques, such as parameter averaging , weightinterpolation [33; 46], and advanced strategies like task arithmetic [29; 51; 67; 78], have been developed for model merging. These techniques have been proven effective, enabling the integration of fine-tuned knowledge from diverse tasks into a multi-task model without additional training.

However, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert [31; 76]. Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) _Interference between models._ Previous research shows that parameter redundancy and sign discrepancies , as well as the distribution gap between tasks , hinder effective model merging. We demonstrate that task-specific models often contain mixed knowledge, where the expertise in one model may be exclusive or detrimental to others. This redundancy or interference can obstruct the integration of expertise across models . (2) _heterogeneity of data at test time._ Previous methods pursue a single, static optimal solution for various tasks. While a one-size-fits-all model avoids introducing new parameters, it might be inadequate or suboptimal due to the unpredictable nature of test inputs . It limits the utilization of complementary knowledge and leads to deteriorated performance .

To address the above issues, in this paper, we introduce Twin Merging, involving two principal stages: (1) **Knowledge Modularization**: Unlike previous research that migrates merging interference in a parameter-wise manner or searches merging coefficients, we decompose the knowledge possessed by experts into shared knowledge and exclusive task-specific knowledge, as shown in Figure 1 (II). First, we compress common knowledge into a shared expert, serving to capture and consolidate common knowledge across varying tasks. Then we isolate exclusive knowledge based on the difference between the task experts and the shared expert, allowing diverse knowledge to be decomposed more finely. (2) **Dynamic Merging**: Inspired by Mixture of Experts (MoE) [80; 84; 85], we simplify the parameter merging problem into a conditional composition problem. Instead of pre-determining the best parameter combination for heterogeneous data at test time, as illustrated in Figure 1 (III), we introduce a router to dynamically merge shared and exclusive knowledge based on the test inputs. The shared model serves as the foundation, and task-specific knowledge is conditionally injected according to the router.

We demonstrate the effectiveness of our proposed Twin-Merging method through extensive experiments on \(12\) datasets, covering both discriminative and generative tasks, various model architectures, and in-domain and out-of-domain setups. As shown in Figure 1(b), Twin-Merging consistently outperforms other merging methods across all datasets, surpassing the strongest baseline by an average of \(28.34\%\) in normalized scores for discriminative tasks and \(3.86\%\) for generative tasks on the scaled model (Qwen-14B). We validate the scalability, extensibility, generalization, and storage efficiency of Twin-Merging (Figure 1(a)). Remarkably, even with a \(99.9\%\) reduction in parameters, our method only experiences a slight \(14\%\) performance degradation. Our results establish Twin-Merging as a powerful and effective method for combining multiple fine-tuned models into a single multi-task model.

To summarize, our contributions are as follows: (1) We introduce Twin-Merging, a novel model fusion method that reduces the performance gap between traditional model merging and fine-tuned models while enhancing adaptability to diverse data. (2) We investigate the impact of shared and exclusive task-specific knowledge on merging performance, presenting innovative techniques for

Figure 1: Subfigure (I) shows that in conventional merging methods, parameters from different task-specific models and a pre-trained model are weighted-summed into a single multitask model for inference. Subfigure (II) illustrates that our Twin-Merging method first isolates shared knowledge, then extracts exclusive knowledge by identifying differences between task experts and the shared model. This exclusive knowledge is then compressed into sparse vectors. Subfigure (III) shows that during testing, Twin-Merging dynamically merges shared and compressed specialized knowledge based on test inputs to form the final inference model.

knowledge disentanglement and dynamic merging. (3) Twin-Merging is simple to implement with minimal hyperparameters, improves multi-task performance without retraining expert models, and can be combined with other merging methods for further gains. Our approach scales well with model size and task numbers and is storage-efficient.

## 2 Related Work

In this section, we focus on model merging research, for additional related work on multi-task learning and Mixture of Experts, please see Appendix B. Model merging aims to fuse multiple fine-tuned task-specific models into one comprehensive multi-task model without additional training. FisherMerging  and RegMean , use straightforward weight averaging but require extra data and computation. Some works [1; 21; 58; 60; 70] bring models into a single low-loss basin and interpolate between them based on the linear mode connectivity (LMC) theory [15; 18; 20]. The weight permutations  and optimal transport  are utilized to better interpolate neural networks. However, recent studies  suggest that LMC might not always hold for fine-tuned models. Task-Arithmetic [28; 51] extends averaging to arithmetic operations in the parameter space for finer control over model behaviors, but the interference between the multiple models can be an issue. To tackle this challenge, advanced merging methods like Ties-Merging , AdaMerging  and DARE  have been proposed. These methods aim to reduce task conflicts by addressing parameter redundancy or disagreements in signs, finding optimal merging coefficients, and reducing weight density, respectively. Jiang et al.  assume that test tasks are known and use task-specific knowledge to improve performance. However, this assumption is often unrealistic since real-world data distributions are unpredictable. In contrast, our method addresses merging interference by modularizing shared and task-specific knowledge. We handle heterogeneous test data scenarios by introducing dynamic merging techniques.

## 3 Methodology

### Analysis of the Performance Gap in Model Merging

In this paper, following the settings of model merging [76; 29; 79], we consider the case of \(T\) tasks, where training for each task \(t\) starts from pre-trained model weight \(_{0}\) and fine-tunes on \(_{t}^{train}\) to obtain task-specific model \(_{t}\). Let \(f(;)\) be a language model accepting inputs \(\) and paramterized by weights \(\). Considering the real data distributions are diverse and challenging to represent with a single task, to model such distributions, previous methods typically consider the mixture of \(T\) task test data: \(=_{t=1}^{T}_{t}_{t}\), where \(_{t=1}^{T}_{t}=1,_{t}>0\  t\). The model merging

Figure 2: The effectiveness of Twin-Merging in terms of performance and parameter-efficiency.

considers the problem where we have \(T\) fine-tuned expert models \(\{f_{t}(;_{t})\}_{t=1}^{T}\) and pre-trained weight \(_{0}\), composing a multitask model \(^{*}\) to approximate the optimal solution.

\[_{opt}^{*}=(_{0},_{1},,_{T})\] (1)

Here \(\) represents an arbitrary merging function. For example, in Task Arithmetic , \(^{*}=_{0}+_{t=1}^{T}_{t}(_{t}-_{0})\).

Although existing merging methods, like Task Arithmetic, can combine multiple task-specific models efficiently, they often exhibit significant performance gaps compared to single-task models. Prior research, such as Ties Merging , attributes this phenomenon to _parameter interference_. This term refers to the redundancy or sign discrepancies found in parameters located at the same position (e.g., self-attention weights) across different task models, which in turn result in information conflicting and performance loss. Additionally, _task interference_, as noted in multi-task learning literature [13; 31], arises from the inherent differences between tasks. For instance, tasks such as summarization, mathematical reasoning, and code generation require the model to process information in distinct ways. These differences worsen interference when models trained on different tasks are merged.

To understand these performance drops, we conducted two experiments using Task Arithmetic. First, we fine-tuned Qwen-14B with LoRA, assigning non-overlapping modules to avoid parameter interference. Despite this, a \(3.21\)% drop in performance occurred, indicating persistent interference. Second, using two similar summarization tasks (XSUM and DailyMail), we observed an \(8.42\)% drop compared to individually fine-tuned models, confirming that interference persists even between similar tasks. These results suggest that _interference in model merging is not limited to parameter-wise and task-wise issues._

### Interpreting Interference From the Perspective of Knowledge

To tackle the challenge of interference, we examine the merging process at a finer-grained knowledge perspective. We identify two types of critical knowledge: (1) _Shared knowledge_, which benefits multiple tasks, and (2) _Exclusive knowledge_, which is useful only for a specific task. Single-task models often contain both types, complicating the merging process and leading to interference. To validate our hypotheses, we conduct experiments that vary the ratio of task-specific and shared knowledge.

To examine the impact of shared knowledge, we conducted full fine-tuning on each model for its specific task. Excessive fine-tuning epochs can lead to catastrophic forgetting , a phenomenon where the model retains task-specific knowledge but loses general knowledge. As the fine-tuning epochs increase, the shared knowledge gradually decreases. The top section of Figure 3 illustrates that as the epoch count increases, merging performance significantly deteriorates, even though the fine-tuned model performs well on its task. This underscores the crucial role of shared knowledge in merging performance.

To explore the impact of exclusive knowledge, we merge a single task-specific model into the base model. We apply a sparsity method (_e.g._, SVD) to reduce the ratios of task-specific weights in the

  
**Task** &  **Normalized Score** \\ **(Equation (4))** \\  \\   _With parameter interference_ \\ Fine-tuned \\ Merging \\  &  \\ 100.00 \\ 85.43 \\  \\   _Without parameter interference_ \\ Non-overlap Fine-tuned \\ Non-overlap Merging \\  &  \\ 100.00 \\ 82.21 \(\) 3.21 \\  \\   _Similar tasks_ \\ Fine-tuned \\ Similar-Tasks Merging \\  & 
 \\ 91.58 \(\) 8.42 \\  \\   

Table 1: Merging without parameter interference and merging between similar tasks both cause performance degradation (Notice: these two experiments use different datasets).

merging model from \(100\%\) (standard merging) to \(0\%\) (base model). As shown in the lower part of Figure 3, performance remains stable up to \(90\%\) sparsity. Notably, even with a \(99\%\) sparsity rate, a single-merged model outperforms multi-model merging, confirming the existence of exclusive knowledge, which is more pronounced with more models. This also underscores the value of unmerged task-specific knowledge, since the fine-tuning performance can be effectively restored by preserving unmerged task-specific information.

To summarize, both shared knowledge and un-merged task-specific knowledge play a vital role in merging performance. The exclusive nature of task-specific knowledge hinders the effectiveness of merging methods. Different types of knowledge need to be separated and modularized to achieve optimal performance. Thus, the first step of our Twin-Merging approach is to explicitly partition the weights into an expert containing shared knowledge and weights holding task-exclusive knowledge before merging. Formally, we denote the shared expert as \(_{s}\) and the exclusive task-specific knowledge as \(\{_{t}\}_{t=1}^{T}\), the detail of our method is illustrated in the following section.

### Twin Merging

Our proposed Twin-Merging employs two main stages: **knowledge modularization** and **dynamic merging**. These stages are designed to narrow the performance gap and enhance adaptive knowledge composition. Building on the formulation in Equation (2), Twin-Merging preprocesses experts into shared experts, isolates and compresses exclusive knowledge into vectors, and dynamically composes them during inference.

The preprocess stage comprises three steps: (1) **Shared Expert**: To separate shared knowledge across different models, we consider the pre-merged model as a natural placeholder to encapsulate common knowledge that is important to all tasks (denoted as \(^{*}\)). By leveraging established merging techniques such as Task Arithmetic, we can readily extract the shared experts from the initial merged model. (2) **Exclusive Knowledge**: To convey task-specific information while separating common knowledge, we calculate the difference vector: \(_{t}=_{t}-^{*}\). This subtraction vector preserves un-merged task-specific information while discarding the shared knowledge. (3) **Compressed exclusive vectors**: For practical use and distribution, we apply singular value decomposition (SVD) to further compress the above exclusive knowledge into vectors for each task. Assuming \(_{t}\) has a rank-\(m\) decomposition, \(_{t}=_{t}_{t}_{t}^{T}\), we achieve a low-rank task space by selecting the top-\(r\) singular values, resulting in \(_{t}(r)_{t}(r)_{t}(r)^{T}\). We store only \(_{t}(r),_{t}(r),_{t}(r)^{T}\).

In inference stage, adapting to unforeseen challenges is difficult, especially with varied test data. For example, if most of the data consists of a certain type (denoted as \(_{u}\)), we should tailor the merged model for that specific task to get the best results. Instead of pre-defining the best parameters, we propose a new approach that combines shared expertise with exclusive knowledge. Our method involves using the input \(\) to dynamically adjust to the current data, enabling us to utilize shared knowledge and apply specialized expertise based on the inputs.

\[^{*}=(_{s}}_{},_{1},,_{T}}_{},)\] (2)

During inference, we fine-tune a small fuser \(\) parameterized by \(\) through empirical risk minimization on a small validation dataset. This fuser, trained to dynamically select the specific task experts,replacing the need for complex optimization algorithms to determine fusion coefficients. The merging model is obtained by:

\[^{*}=_{s}+_{t=1}^{T}w_{t}* _{r}(_{t}-^{*})\\ \{w_{1},,w_{T}\}=(();)\] (3)

Here, \(()\) represents the sequence of the last-layer token embeddings from the shared expert \(f(;_{s})\).

## 4 Experiments

### Merging Experiment

BaselinesWe first compare Twin-Merging with several train-free model-merging methods on both discriminative and generative NLP benchmarks, including weight averaging, Task Arithmetic , Ties-Merging , and DARE Merging . To compare with Merging methods that need validation dataset, we also conduct experiments on CV tasks with AdaMerging  and Surgery . Details on these baselines are provided in Appendix D.

BenchmarksFor language discriminative tasks, following [76; 79], we use RoBERTa  as the backbone and evaluate on the 8-task GLUE benchmark . More details are in Appendix D.2. For language generative tasks, we use Qwen-14B  as the primary model to demonstrate the effectiveness of our approach on large-scale language models. To reduce deployment costs, we utilize task-specific checkpoints fine-tuned with the LoRA method  (See Appendix A for details on adapting Twin-Merging to LoRA). We evaluate our model on four scenarios: general knowledge (MMLU benchmark ), factualness (TruthfulQA ), safety (BBQ ), and summarization (CNN-DailyMail ).

For vision tasks, following AdaMerging , we use ViT-B/32 in CLIP  as the backbone on eight image classification datasets: SUN397 , Stanford Cars , RESISC45 , EuroSAT , SVHN , GTSRB , MNIST , and DTD . We employ the best version of AdaMerging (layer-wise AdaMerging++) and the Surgery (AdaMerging version), and apply 90% sparsity for our Twin-Merging. Detailed information is provided in Appendix D.2.

MetricsWe include individually fine-tuned models and the pre-trained model as upper and lower bounds on performance, respectively. To mitigate the effects of different task-specific score ranges, performance is assessed using the average normalized score of the fine-tuned models. The normalized score of merged model \(^{*}\) is calculated as:

\[=_{t=1}^{T} _{x_{t}}[f(;^{*})]}{ *{Score}_{x_{t}}[f_{t}(;_{t})]}\] (4)

  
**Method** & **8 Discriminative Tasks** & **4 Generative Tasks** & **Avg.** \\ 
**Pretrained** & 41.69 & 91.06 & 66.37 \\
**Fine-tuned** & 100.00 & 100.00 & 100.00 \\  
**Weight Averaging** & 52.56 & 95.74 & 74.15 \\
**Task Arithmetic** & 67.80 & 96.61 & 82.20 \\
**Task Arithmetic (w/ DARE)** & 64.66 & 98.52 & 81.59 \\
**Ties-Merging** & 63.68 & 92.67 & 78.17 \\
**Ties-Merging (w/ DARE)** & 65.58 & 91.92 & 78.75 \\ 
**Twin-Merging (Rank-1)** & 86.00 & 100.96 & 93.48 \\
**Twin-Merging (Ours)** & **96.14** & **102.38** & **99.26** \\   

Table 2: Performance on 8 Discriminative Tasks (RoBERTa) and 4 Generative Tasks (Qwen-14B)Main ResultsTable 2 presents the results for all discriminative and generative benchmarks for language tasks, while Table 3 provides the results for vision tasks. A comparison of each task is illustrated in Figure 1(b), with detailed statistics provided in Table 8 and Table 9 in the Appendix D.7. We also list the full-finetuned LLaMA results in Appendix D.7.

For discriminative tasks, it approachs the upper bound of finetune performance in the GLUE benchmark. Specifically, our methods improve over Task Arithmetic by \(28.34\%\), Ties-Merging by \(32.46\%\), and DARE-Merging by \(30.56\%\) in absolute normalized score. In Figure 1(b), we observe that especially on the COLA task, where conventional merging methods fail to improve the result, our approach can still approach the upper bound of the COLA expert.

On generative tasks, Twin-Merging achieves strong performance, outperforming Task Arithmetic and DARE Merging by \(5.77\)% and \(3.86\)%. Two interesting insights emerge: (1) The performance gains for Qwen-14B in generative tasks are smaller compared to RoBERTa in discriminative tasks. This indicates that smaller models like RoBERTa gain more from task-specific knowledge, while large models like Qwen-14B perform well because its strong general knowledge. (2)Twin-Merging surpasses the upper bound set by fine-tuned experts on the generative benchmark. This may be due to the extensive knowledge in Qwen-14B, where modularization and dynamic merging unlock further potential without additional fine-tuning. These findings highlight a promising path for improving large language models without retraining.

For vision tasks, Twin-Merging outperforms the AdaMerging and Surgery baselines with a higher accuracy (\(95.33\)% vs. \(94.04\)%) while being more efficient in time and storage (\(47m22s\) vs. \(215m01s\), \(5.0\)GB vs. \(32.4\)GB). AdaMerging uses task-wise or layer-wise learnable parameters to improve merging, and Surgery adds task-specific modules after merging, requiring training on the validation set for all eight tasks. Surgery also needs prior knowledge of the task type before inference and involves multiple forward passes, leading to high VRAM usage. In contrast, our method efficiently handles diverse test inputs with minimal time and storage costs.

Scalability of Twin-MergingOur method remains effective with scaled models (_e.g._, 72B parameters), as shown in Table 4. To manage high deployment costs, we limited our evaluation and merged experts to two tasks: BBQ and TruthfulQA. Twin-Merging consistently surpasses scaled pre-trained models and Task Arithmetic, highlighting our approach's scalability. Additionally, our method can be

  
**Method** & **TruthfulQA** & **BBQ** \\ 
**Pretrained-72B** & 94.48 & 89.51 \\
**Fine-tuned** & 100 & 100 \\ 
**Task Arithmetic** & 98.70 & 95.40 \\
**Twin Merging** & **99.30** & **97.14** \\   

Table 4: Our method scalability (72B)

  
**Method** & **Avg. Normalized Score** & **Additional Time Cost** & **VRAM** \\ 
**Pretrained** & 52.02 & 18m48s & 3.6GB \\
**Fine-tuned** & 100.00 & 18m48s & 28.8GB \\ 
**Weight Averaging** & 72.30 & 18m50s & 3.6GB \\
**Task Arithmetic** & 76.50 & 21m34s & 3.6GB \\
**Ties-Merging** & 75.10 & 19m24s & 3.6GB \\
**AdaMerging** & 88.50 & 185m35s & 3.6GB \\
**Surgery** & 94.04 & 215m01s & 32.4GB \\ 
**Twin-Merging(Ours)** & **95.33** & 47m22s & 5.0GB \\   

Table 3: Performance and Cost on 8 CV Tasks (ViT-B/32)

  
**Method** & **QNLI+MNLI+RTE** & **MMLU** \\ 
**Multi-Task Learning** & 44.63 & 63.74 \\
**Task Arithmetic (w/ DARE)** & 53.92 & 62.02 \\
**Tas Arithmetic (w/ DARE)** & 54.27 & 63.09 \\
**Ties Merging** & 54.09 & 64.62 \\
**Ties Merging (w/ DARE)** & 54.72 & 63.13 \\
**Twin-Merging** & **55.86** & **65.98** \\   

Table 5: Performance (un-normalized2) on unseen taskseasily integrated with other merging methods, as detailed in Appendix D.9, making it both extensible and scalable.

### Unseen Generalization

As shown in Table 5, Twin-Merging method benefits from complementary collaboration among different experts. Since the corresponding task-specific experts are unavailable, we directly use the average of the unnormalized scores as the metrics. In the GLUE benchmark, when QNLI, MNLI, and RTE experts are absent, our approach still outperforms traditional baselines. Details on the expert combination for QNLI can be found in Figure 4(a). For complex tasks like MMLU, which involves multiple-choice QA tasks across 57 categories, Twin-Merging demonstrates superior performance using the combined knowledge from TruthfulQA, BBQ, and CNN-DailyMail domains.

### Ablation Studies

To demonstrate the effectiveness of our approach, we conducted ablation studies for Twin-Merging, summarized in Table 6. Removing dynamic experts from the Shared model leads to a significant performance loss (\(96.14\) vs. \(67.80\)), highlighting the need for dynamic merging. Replacing the shared expert with a task-specific expert also results in a clear drop in performance (\(96.14\) vs. \(81.47\)), showing the value of the shared expert in capturing common knowledge.

Additionally, applying dynamic merging directly to a pretrained model performs worse than Twin Merging (\(85.90\) vs. \(96.14\)), likely due to two factors: (1) Pretrained models may lack rich task knowledge, while the shared expert in Twin Merging captures diverse, task-specific knowledge. (2) Subtracting the pretrained model fails to fully consider exclusive knowledge specific to each task, leading to interference, as analyzed in Section 3.2.

Discussion 1We find that removing dynamic experts severely impacts RoBERTa but has less effect on Owen-14B, suggesting that smaller models rely more on task-specific biases, while larger models benefit more from general shared knowledge. This indicates that our method adapts effectively to the varying knowledge requirements of models of different sizes.

Discussion 2Compared to simpler routing methods like _direct route to task-specific expert_ or _combining multiple experts_ based on multiple LoRA [27; 81], Twin Merging delivers better performance, especially on unseen tasks, by reducing interference and leveraging complementary knowledge.

_Direct route to task-specific expert_ refers to the fine-tuned baseline in Table 2. This approach assumes perfect routing and the absence of out-of-domain data, where each task uses its own dedicated expert. It represents the ideal scenario and serves as an oracle baseline to highlight the performance gap for merging methods. Despite this, Twin Merging still improves performance on generative tasks (\(102.38\) vs. \(100.0\)) and unseen tasks (Table 5) by leveraging different sources of exclusive knowledge. Moreover, this baseline demands storing all task-specific experts, which significantly increases storage, as discussed in Section 4.6.

In _combining multiple experts_, the lack of separation between shared and exclusive knowledge leads to interference, as conflicts between exclusive knowledge are inevitable (Section 3.2). There are two ways to combine experts: (1) Static Combination: This is akin to "Task Arithmetic" in LoRA (Table 2). Twin Merging outperforms static combinations (\(102.38\) vs. \(96.61\)). (2) Dynamic Combination: This matches "Pretrain + Dynamic Merging" method in Table 6, and Twin Merging again shows superior performance (\(102.38\) vs. \(97.03\)).

  
**Method** & **RoBERTa** & **Qwen** \\ 
**Pretrain** & 41.69 & 91.06 \\ 
**Shared** & 67.80 & 96.61 \\
**Dynamic Merging** & 81.47 & 87.77 \\
**Pretrain + Dynamic Merging** & 85.90 & 95.03 \\
**Shared + Dynamic Merging (Twin Merging)** & 96.14 & 102.38 \\   

Table 6: Ablation study of Twin-Merging

### Scale to More Tasks

In the left panel of Figure 4, we examine the impact of the number of tasks on model merging performance. Conventional model merging methods degrade notably, especially with many tasks, nearly reaching pre-trained levels. However, Twin-Merging consistently outperforms other methods, approaching fine-tuned performance, with greater gains as the task count rises.

The right panel of Figure 4 shows the performance-storage trade-offs. While model merging methods have a constant storage cost, their performance remains low. In contrast, maintaining individual task-specific models guarantees strong performance but requires excessive storage. Twin-Merging achieves nearly 100% normalized accuracy across various tasks, balancing performance and storage efficiency by maintaining task-specific parameters with shared experts. This makes Twin-Merging a viable solution for scenarios demanding a balance between performance and storage efficiency.

### Router Analysis

Figure 5 shows the results of routing decisions among experts for the QNLI dataset and four generative benchmarks. As shown in Figure 4(a), the router maximizes the use of limited expert knowledge to address QNLI, a task where the goal is to determine if the context sentence contains the answer to the input question. For example, with only \(_{}\) and \(_{}\) available, the router primarily uses \(_{}\), which provides knowledge of sentence and word relations, while \(_{}\) is focused on irrelevant sentiment classification. With six experts ranging from \(_{}\) to \(_{}\), the router mainly leverages \(_{}\) for textual entailment and \(_{}\) for question-answering capabilities. When \(_{}\) is included, the router naturally relies on QNLI-specific knowledge. These results demonstrate the flexibility and adaptability of our Twin-Merging method, providing good interpretability. For larger models like Qwen-14B, as shown in Figure 4(b), the router plays a crucial role in selecting and combining specific knowledge. When experts have overlapping task-specific knowledge, such as \(_{}\) and \(_{}\), the router may assign them similar weights.

### Compression and Speed Analysis

Compression AnalysisIn the left panel of Figure 6, we explore sparsity rates from \(0\%\) to \(100\%\). Appendix E attacks detail qualtivic analysis of various Merging methods. Remarkably, our Twin

Figure 4: Averaged normalized accuracy _vs._ the number of tasks for various benchmarks. Twin-Merging maintains performance regardless of task number and compresses the fine-tuned checkpoints.

Merging method maintains \(86.4\%\) performance even at a \(99.8\%\) compression rate. This suggests that performance relies on a small fraction of task-specific parameters, aligning with previous findings [76; 79]. Our results also validate our hypothesis that redundant parameters can obscure critical knowledge, leading to performance degradation. Consequently, we primarily use a \(90\%\) sparsity rate in our experiments to preserve performance while reducing storage costs. We also conducted an ablation study on sparsity methods, shown on the right side of Figure 6. SVD better retains task-specific information compared to Magnitude  and Bernoulli Dropout . As SVD is applied only once during preprocessing, it does not become an inference bottleneck.

Speed AnalysisTable 7 presents the time cost for Twin-Merging in generative benchmarks. Although the training stage utilizes only 0.1% of the total training budget, Twin-Merging significantly improves general capabilities compared to multi-task learning. Compared to conventional model merging methods, Twin-Merging sacrifices minimal router training budget and incurs a slight reduction in inference speed for dynamically composing the twin vectors, thereby achieving superior performance. More detailed analysis and results are provided in Appendix E. In summary, our approach strikes a better balance between computational cost and performance.

## 5 Conclusions

In this paper, we introduce the Twin-Merging to merge language models, aiming to close the performance gap between conventional model merging techniques and fine-tuned models, while improving adaptability to data heterogeneity. By modularizing and dynamically merging shared and task-specific knowledge, Twin-Merging significantly outperforms existing model-merging methods and approaches the performance of fine-tuned models across various settings and domains. Our study highlights the impact of shared and exclusive task-specific knowledge on merging performance. We show that Twin-Merging benefits even strong scaled models like Qwen-72B, which already perform well across domains. It extends to more tasks and merging methods, demonstrating better generalization on unseen data. By utilizing SVD, our solution retains \(86\%\) of the performance with only \(0.1\%\) of the parameters, approaching upper-bound performance with minimal storage increase as tasks grow, achieving a better tradeoff between computation and performance.

## 6 Acknowledgments

We thank the Shanghai AI Laboratory for supporting GPU resources. We also thank the anonymous reviewers for their comments on improving the quality of this paper and Netmind.AI for their resource/technical support.

  
**Method** & **Training Tokens** & **Training Cost** & **Inference Cost (/1000 items)** & **Performance** \\ 
**Multi-Task Learning** & 536.35M & 10h32min & 236s & 94.31 \\
**Model Merging**3  & 0 & 0 & 236s & 96.61 \\
**Twin-Merging** & 0.57M & 183s & 275s & 102.38 \\   

Table 7: Compute-performance tradeoff in the generative benchmark.

Figure 6: Twin-Merging performance _vs._ different sparsity levels and techniques for GLUE