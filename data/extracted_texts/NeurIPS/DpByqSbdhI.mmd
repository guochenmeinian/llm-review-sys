# UniMTS: Unified Pre-training for Motion Time Series

Xiyuan Zhang

UC San Diego

xiyuanzh@ucsd.edu

&Diyan Teng

Qualcomm

Qiyateng@qti.qualcomm.com

Ranak Roy Chowdhury

UC San Diego

rrchowdh@ucsd.edu

&Shuheng Li

UC San Diego

shl060@ucsd.edu

&Dezhi Hong

Amazon

hondezhi@amazon.com

Rajesh K. Gupta

UC San Diego

rgupta@ucsd.edu

Work unrelated to Amazon.

Jingbo Shang

UC San Diego

jshang@ucsd.edu

###### Abstract

Motion time series collected from low-power, always-on mobile and wearable devices such as smartphones and smartwatches offer significant insights into human behavioral patterns, with wide applications in healthcare, automation, IoT, and AR/XR. However, given security and privacy concerns, building large-scale motion time series datasets remains difficult, hindering the development of pre-trained models for human activity analysis. Typically, existing models are trained and tested on the same dataset, leading to poor generalizability across variations in device location, device mounting orientation, and human activity type. In this paper, we introduce UniMTS1, the first unified pre-training procedure for motion time series that generalizes across diverse device latent factors and activities. Specifically, we employ a contrastive learning framework that aligns motion time series with text descriptions enriched by large language models. This helps the model learn the semantics of time series to generalize across activities. Given the absence of large-scale motion time series data, we derive and synthesize time series from existing motion skeleton data with all-joint coverage. We use spatio-temporal graph networks to capture the relationships across joints for generalization across different device locations. We further design rotation-invariant augmentation to make the model agnostic to changes in device mounting orientations. Our model shows exceptional generalizability across 18 motion time series classification benchmark datasets, outperforming the best baselines by **340%** in the zero-shot setting, **16.3%** in the few-shot setting, and **9.2%** in the full-shot setting.

## 1 Introduction

Recognition of human motion using time series from mobile and wearable devices, such as accelerations and angular velocities, is widely adopted as key context information for various applications from health condition monitoring , sports activity analysis  to user habit studies . Compared with vision-based approaches, methods based on motion sensor time series offer more energy-efficient and cost-effective solutions with enhanced privacy protection , making them preferable.

While valuable, collecting motion time series data at large scale remains challenging due to security or privacy concerns. Labeling motion time series proves even more difficult as such data cannot be easily interpreted by humans for post annotation. This results in data insufficiency that impedes development of supervised learning methods. In other fields such as natural language processing  and computer vision , pre-trained foundation models have shown remarkable performance in such settings with insufficient data. However, in the motion time series domain, lack of comprehensive datasets and an effective pre-training task makes it difficult to similarly develop pre-trained models that can operate with limited data. Typically, existing models perform training and testing on the same dataset, and struggle to generalize across different datasets given the following three unique challenges within the motion time series problem domain.

We summarize these three unique generalization challenges in Figure 1. First of all, variation in device placement during deployment poses a significant issue; for instance, data from a smartwatch on the wrist vary considerably from data gathered from a smartphone near the upper leg. Therefore, models trained on data from one body location can barely generalize to others during the testing phase. Secondly, devices can experience arbitrary orientations during data collection, making it difficult for models trained on specific device orientations to adapt to new ones during deployment. Thirdly, different motion time series datasets can be focused on different types of human activities. For example, some datasets aim to identify stationary activities such as lying or sitting, while others concentrate on dynamic movements such as walking or cycling. Models trained on specific types of activities typically struggle to generalize to new activities introduced by other datasets.

We introduce UniMTS, the first Unified pre-trained model for Motion Time Series to address all the above three generalization issues, achieving state-of-the-art zero-shot and fine-tuning performance. UniMTS follows a contrastive learning framework that aligns motion time series with LLM-enriched textual descriptions to learn the time series semantics for _activity_ generalization. To prepare large-scale motion time series for pre-training, we synthesize these time series based on existing extensive motion skeleton data  with comprehensive coverage of different body locations. We model these synthesized time series using graph networks to capture the spatio-temporal relationships across devices for _location_ generalization. We further implement rotation-invariant augmentation to ensure the model's robustness to any device _orientation_ during testing.

We summarize our primary contributions as follows:

* We introduce the first unified pre-training procedure for motion time series, UniMTS, which successfully generalizes to various device locations, device orientations and activities.
* We design a contrastive learning framework to align motion time series with corresponding semantic meanings for activity generalization. For device location generalization, we propose to synthesize motion time series covering various body locations and model their spatio-temporal correlations using graph convolutional neural networks. We also design rotation-invariant augmentation to make the model agnostic to different device orientations.
* Our pre-trained model demonstrates state-of-the-art performance across 18 real-world motion time series benchmark datasets, notably with performance improvement of **340%** in the zero-shot setting, **16.3%** in the few-shot setting, and **9.2%** in the full-shot setting, compared with the respective best-performing baselines.

## 2 Related Work

**Conventional motion time series classification** approaches train a dedicated classifier for each dataset, and can be categorized into statistical feature extraction methods  and deep learning

Figure 1: Our framework addresses three key generalization challenges (variation in device location, orientation, and activity) where existing methods fall short.

methods, including convolutional neural networks (MA-CNN , SenseHAR , Rocket ), recurrent neural network (DeepConvLSTM ), and the attention mechanism based models (AttnSense , THAT ). Recently, IMUGPT [28; 27] generates motion sequences given activity textual descriptions and trains conventional classification models such as DeepConvLSTM . TimesNet , GPT4TS  and TEST  propose task-general time-series models for multiple tasks including classification. SHARE  presents a sequence-to-sequence framework that leverages shared structures of label names. However, these models perform training and testing on the same dataset, and cannot generalize across datasets.

**Self-supervised motion time series representation learning** methods first learn time series representations based on mask reconstruction (TST , TARNet , LIMU-BERT ), contrastive learning (TNC , TS-TCC , TS2Vec , TF-C , FOCAL , CL-HAR , DDLearn ) or other self-supervised learning objectives (BioBankSSL [66; 13; 10], Step2Heart ). Subsequently, they fine-tune classifier heads for specific downstream tasks. However, the representation learning and fine-tuning phases of these methods generally occur on the same or highly similar datasets, which continues to face challenges in generalization across diverse datasets.

**Pre-trained models for motion time series** are inspired by the recent success of large language or multimodal models. ImageBind  and IMU2CLIP  leverage recent large vision-language models  to learn a joint embedding across multiple modalities including motion time series and text. However, both ImageBind and IMU2CLIP are trained on motion time series collected from head-mounted devices , limiting their generalizability across different device locations and orientations. Furthermore, several studies have explored directly applying LLMs for motion time series classification. For example, HARGPT  processes raw motion time series through LLMs and incorporates role-play and chain-of-thought strategies for prompting. ContextGPT  designs prompt engineering approaches leveraging context information. However, since LLMs are not directly trained on raw motion time series, such methods require extensive context information that is not usually available, and struggle with accurately recognizing complex activities.

**Other related works on motion classification** include multimodal action recognition and domain adaptation methods. Multimodal action recognition such as the Ego4D  and Ego-Exo4D  benchmarks incorporates video and audio modalities, whereas we focus on a more energy-efficient and challenging scenario of action recognition based purely on motion time series. Domain adaptation methods mostly assume that source and target datasets share the same label names and have the same number of classes, such as cross-user domain adaptation and cross-dataset domain adaptation only for those common classes [22; 35; 21]. We aim for a more generic yet challenging generalization scenario where pre-training and downstream datasets share different label names.

## 3 Method

UniMTS takes a contrastive learning-based approach that aligns paired motion time series with text descriptions to enable activity generalization, as shown in Figure 2. We simulate motion time series from motion skeleton data (Section 3.1) and augment them for orientation generalization (Section 3.2).

Figure 2: UniMTS pre-training framework: The physics engine computes motion time series for each joint based on motion skeleton data and enhances time series through rotation-invariant augmentation. During pre-training, we adopt contrastive learning to align motion time series encoded by graph convolutional neural networks with corresponding text descriptions augmented by an LLM.

We use graph encoder to model the simulated motion time series, capturing correlations among joints to generalize across different device locations (Section 3.3.1). To enhance semantics learning, we use large language models to augment text descriptions (Section 3.3.2).

### Physics Engine for Motion Time Series Simulation

Motion skeleton data  describe the movements of human skeleton joints over time, containing positions and orientations for each joint. On the other hand, motion time series captured by physical sensors typically measure higher-order data such as accelerations and angular velocities. Consequently, we apply motion equations  to synthesize these time series of accelerations and angular velocities from motion skeleton data. More specifically, for each skeleton joint \(J_{i}\), we input both positions \(_{J_{i},}\) (mapped from time domain \(\) to \(^{3}\), defined in global frame \(\)), and orientation quaternions \(_{J_{i},}\) (mapped from time domain \(\) to the Special Orthogonal Group \((3)\), defined in Hamilton convention with subscript \(\) representing a frame rotation from local frame \(\) to global frame \(\)). We drop the subscript \(\) and \(\) from here on for simplicity of notation. Based on motion equations , we calculate velocities \(_{J_{i}}\) and accelerations \(_{J_{i}}\) by taking the first and second order derivatives of positions \(_{J_{i}}\). These derivatives are then transformed from global frames to local frames using the corresponding orientation sequences \(_{J_{i}}\). Similarly, angular velocities \(_{J_{i}}\) are computed by taking the first order derivatives of orientation quaternions \(_{J_{i}}\). Mathematically,

\[_{J_{i}}(t) =_{J_{i}}^{*}(t)_{J_{i}}^{}(t) _{J_{i}}(t), \] \[_{J_{i}}(t) =_{J_{i}}^{*}(t)_{J_{i}}^{ }(t)_{J_{i}}(t),\] (2) \[_{J_{i}}(t) =2_{J_{i}}^{*}(t)_{J_{i}}^{}(t), \]

where \(\) and \({}^{*}\) represent the quaternion multiplication operator and the quaternion conjugate.

Recognizing the inherent presence of noise carried by sensors in practice, the physics engine incorporates Gaussian noise with a zero mean into the simulated data. Representing the above motion time series as \(_{J_{i}}(t)\), which can denote either \(_{J_{i}}(t)\) (accelerations) or \(_{J_{i}}(t)\) (angular velocities), the noisy time series \(}_{J_{i}}(t)\) are formulated as

\[}_{J_{i}}(t)=_{J_{i}}(t)+_{J_{i}}(t), _{J_{i}}(t)(,). \]

### Rotation-Invariant Augmentation

A common limitation we have identified from prior studies that leads to their poor generalization is that they fail to consider the impact of latent device orientation factors on the motion time series. For example, end users can potentially wear devices in various orientations, such as with a phone facing towards or against the body in a pocket. Additionally, the software driver API for axis definition can be arbitrarily configured by the developers. For example, the iOS system defines acceleration in an opposite direction compared to the Android system2. With the listed risk factors considered, we apply a data augmentation technique to simulate random orientations during pre-training, so that our learned model achieves rotation-invariance during deployment [7; 61; 57]. Specifically, during pre-training, for each iteration we sample a random rotation matrix for each joint \(J_{i}\),

\[_{J_{i}}^{}((3)), \]

and compute the augmented time series \(}_{J_{i}}^{t}\) at timesteps \(t=1,2,,T\) as

\[}_{J_{i}}^{t}=_{J_{i}}^{}}_{J_ {i}}^{t}. \]

During one iteration, the same \(_{J_{i}}^{}\) is consistently applied to \(J_{i}\) for every time series and every timestep \(t=1,2,,T\). The rotation-invariant augmentation ensures that the simulated time series are adaptable to any downstream orientation, thereby enhancing the generalization capabilities.

### Contrastive Learning

The physics engine generates sufficient motion time series data, which are subsequently encoded by graph networks and aligned with their corresponding text embeddings through contrastive learning.

#### 3.3.1 Graph Encoder

To capture the spatio-temporal correlations among different joints over time, we adopt spatio-temporal graph convolutional network  as our motion time series encoder. We denote the initial input graph representation as follows,

\[=(=\{}_{J_{i}}\}_{i=1}^{}, _{s}=\{(}_{J_{i}},}_{J_{l}})|(J_{i},J _{l})\},_{t}=\{(}_{J_{i}}^{t-1},}_{J_{i}}^{t})\}_{i=1,t=2}^{V,T}). \]

Nodes \(\) contain skeleton joints with features \(^{C T V}\), where \(C,T,V\) represent the number of signal channels, temporal steps and joint nodes. Spatial edges \(_{s}\) connect adjacent nodes defined by the skeleton structure \(\) and temporal edges \(_{t}\) connect temporally adjacent frames.

In practice, devices may not cover the complete joints but are rather positioned at arbitrary subsets of the complete joints. To simulate this, during each pre-training iteration, we randomly select a subset of joints and mask data from the remaining joints with zeros. We denote the mask at one iteration as \(^{C T V}\), where \(_{i}^{C T}\) is \(\) if joint \(J_{i}\) is selected, and \(_{i}=\) if joint \(J_{i}\) is masked:

\[}=, \]

The graph convolution network \(g_{}\) first computes the spatial output features as

\[_{}=_{k}^{K_{s}}_{k}(}( _{k}^{-}_{k}_{k}^{-})), \]

where \(K_{s}\) denotes the spatial kernel size, \(_{k}^{il}\) represents whether node \(_{J_{i}}\) belongs to the spatial convolution sampling subset \(_{J_{i}}^{k}\) of node \(_{J_{i}}\), and \(_{k}^{ii}=_{l}(_{k}^{il})+\) represents the normalized diagonal matrix, with \(\) set to \(0.001\) to prevent empty rows [63; 48]. \(_{k}^{C^{} C 1 1}\) represents weights of the \(1 1\) convolution operation with \(C^{}\) denoting output channel dimension. Following spatial convolution, we further perform \(K_{t} 1\) temporal convolution on the spatial output features \(_{}\), similar to classical convolution operations, where \(K_{t}\) represents the temporal kernel size. The final graph representation \(g_{}()\) is derived by averaging features across both spatial and temporal dimensions with a graph average pooling layer at the end.

#### 3.3.2 Text Encoder

To increase the diversity of paired text descriptions in the pre-training motion corpus , we apply large language models (GPT-3.5) to augment original motion text descriptions with the following prompt template: _The following one or multiple descriptions are describing the same human activities: <motion descriptions>_. _Generate k paraphrases to describe the same activities._

We denote the original text descriptions combined with the LLM-augmented ones as \(\). We encode them using the same text encoder \(f_{}\) as CLIP , utilizing its pre-trained weights for initialization.

#### 3.3.3 Training and Inference

During pre-training, we maximize the similarities of paired simulated motion time series and text descriptions through contrastive learning:

\[_{ctr}=-_{i=1}^{B}(g_{ }(_{i}),f_{}(_{i}))^{}}{_{k =1}^{B}((g_{}(_{i}),f_{}(_{k}))) ^{}}, \]

Figure 3: Inference (left) and fine-tuning (right) phases of UniMTS. We assign real signals to the nearest location in the skeleton graph. During inference, we compute the similarity score between the graph embedding and each label candidate, and predict the one with the highest score. During fine-tuning, we freeze the text encoder and update weights of the graph encoder and linear layer.

where \(B,\) represent batch size and temperature parameter that controls distribution concentrations, and \(\) represents similarity score computed as inner product:

\[(g_{}(_{i}),f_{}(_{i}))= g_{ }(_{i}),f_{}(_{i}). \]

We pre-train the graph and text encoders using simulated motion time series and augmented text descriptions. During inference, we evaluate the model on real-world motion time series, as illustrated in the left part of Figure 3. For the text encoder, we input all label candidates. For the graph encoder, we assign real motion time series to the nearest joint in the skeleton graph and assign zeros to the remaining joints. The random mask \(\) during pre-training emulates the zero-masking process. We compute the similarity score between the graph embedding with text embedding from each label candidate, and choose the label with the highest similarity score as the predicted activity.

We can further fine-tune the pre-trained model on downstream real-world data, as depicted in the right part of Figure 3. Specifically, we freeze the text encoder \(f_{}\) and update weights of the graph encoder \(g_{}\) followed by a linear classifier \(h_{}\). Following the same process as inference, we assign the real motion time series to the nearest joint in the skeleton graph and assign zeros to the remaining joints to construct the graph input representation \(\). We fine-tune the model using \(\) and one-hot encoded labels \(\) with \(D\) classes based on cross-entropy loss, where \(()\) represents the softmax operation:

\[_{ce}=-_{i=1}^{B}_{j=1}^{D}_{ij}( (h_{}(g_{}(_{i})))_{j}). \]

We report both zero-shot and fine-tuning performance in the subsequent experiment section.

## 4 Experiments

### Datasets and Experimental Setting

We simulate motion time series from existing motion skeleton dataset HumanML3D , which contain both motion skeleton data and corresponding text descriptions as detailed in Section A.1 in Appendix. We further augment the text descriptions as described in Section 3.3.2.

We evaluate on the most extensive motion time series classification benchmark to date, comprising 18 real-world datasets that cover diverse activities. These datasets are collected from various body locations such as head, chest, back, arm, wrist, waist, hip, leg, knee and ankle. We categorize these datasets into three difficulty levels: (1) easy level (with fewer than 10 activities): Opportunity , UCI-HAR , MotionSense , w-HAR , Shoaib , HAR70+ , RealWorld , TNDA-HAR ; (2) medium level (with 10 to 20 activities): PAMAP2 , USC-HAD , Mhealth , Harth , UT-Complex , Wharf , WISDM , DSADS ; (3) hard level (with more than 20 activities): UTD-MHAD , MMAct . We provide the specific number of activities for each dataset in Table 1 and Table 2, and detail their collection settings in Section A.2 in Appendix.

We re-sample the real-world test data to the same sampling frequency as the simulation data (20 Hz), and apply normalization to ensure consistency in unit measurements, e.g., standardizing accelerations to \(m/s^{2}\). We pre-train UniMTS using Adam optimizer  with a learning rate of \(0.0001\) on a single NVIDIA A100 GPU. The pre-training process consumes approximately \(13\) GB of memory given a batch size of \(64\). For text augmentation, we prompt GPT-3.5 ("gpt-3.5-turbo") to generate \(k\) = \(3\) paraphrases. During each iteration, we randomly generate the mask \(\) by selecting 1 to 5 joints and mask the remaining joints as zeros. We adopt learnable temperature parameter \(\) initialized from CLIP. We evaluate the models using accuracy, macro-F1 and the top-2 retrieval performance R@2.

### Zero-Shot Results

We pre-train UniMTS exclusively on simulated data and evaluate on 18 real-world motion time series classification benchmark datasets. We compare UniMTS against classification models with zero-shot capabilities: ImageBind , IMU2CLIP , IMUGPT  and HARGPT . We also input the 2D visualizations of motion time series to pre-trained vision-language model LLaVA  for comparison. We detail the configurations of baselines in Section A.3 in Appendix. As shown in Table 1, UniMTS significantly outperforms all baselines in the zero-shot setting. We also apply the Wilcoxon-signed rank test with Holm's \(\) (5%) following previous works [20; 71]. The Wilcoxon-signed rank test indicates that the improvement of UniMTS compared with all the baselines is 

[MISSING_PAGE_FAIL:7]

TS2Vec , BioBankSSL ), and conventional models (DeepConvLSTM , MA-CNN , XGBoost , THAT , IMUGPT , TimesNet , GPT4TS , SHARE ). Baselines are detailed in Section A.3 in Appendix. We also compare pre-trained UniMTS with a randomly initialized UniMTS (referred to as Random). As shown in Table 2, UniMTS also demonstrates state-of-the-art performance in the full-shot setting, outperforming pre-trained, self-supervised and conventional models. Due to space limit, we report baselines before 2021 in Table 3 in Appendix. Following the same Wilcoxon-signed rank test, we observe p-values far below 0.05 (e.g., p-value = \(0.018\) for the best-performing baseline), indicating the statistical significance of our improvement. UniMTS also demonstrates space and time efficiency, as detailed in Section A.5 in Appendix.

### Ablation Study

In the zero-shot setting, we compare UniMTS with a few ablations by removing rotation-invariant augmentation (w/o rot aug), removing text augmentation (w/o text aug) and by replacing the graph encoder with a CNN-based encoder that directly concatenates joints without modeling their spatial relationships (w/o graph). We can observe in Table 1 that the performance declines after removing each of the above components, verifying their respective importance in improving generalization across locations (graph encoder), orientations (rotation-invariant augmentation) and activities (text augmentation). We also compare the pre-trained UniMTS with randomly initialized UniMTS in both few-shot and full-shot settings. As shown in Figure 4 and Table 2, pre-trained UniMTS consistently outperforms randomly initialized UniMTS, highlighting the benefits of pre-training.

### Case Study

**UniMTS's time series embeddings align with corresponding semantic meanings.** As shown in Figure 5, the t-SNE visualizations of UniMTS's time series embeddings form distinguishable clusters that align with their semantic meanings. Notably, UniMTS is only pre-trained on the simulated data but its embeddings for real-world data closely align with the semantic space, which again demonstrates our model's zero-shot generalization due to contrastive learning. For example, in Figure 4(a), stationary activities such as lying and sitting group together; light-movement activities such as standing, ironing, and vacuum cleaning are close to each other; while high-intensity activities such as running and cycling cluster closer in the embedding space.

Figure 4: Few-shot fine-tuning results. UniMTS consistently outperforms both baselines and our model ablation. We repeat 3 runs and report both mean and standard deviation.

Figure 5: T-SNE visualizations show that signal clusters align with their semantic meanings.

[MISSING_PAGE_FAIL:9]

Conclusion and Discussion

**Conclusion**. In this paper, we present the first unified pre-training procedure, UniMTS, for motion time series classification. Our model is pre-trained only on physics-simulated data, and yet demonstrates remarkable generalization across diverse real-world motion time series datasets featuring different device locations, orientations and activities. The simulated data with all-joint coverage are augmented for rotation invariance and modeled by a graph encoder, improving generalization across various device factors. During pre-training, contrastive learning aligns time series with their semantic meanings to improve generalization across activities. Extensive evaluation in zero-shot, few-shot and full-shot settings consistently demonstrates the state-of-the-art performance of UniMTS.

**Limitation and Future Work**. We acknowledge a few limitations which we leave as future work. (1) Simulated motion time series can only be approximations of real signals, which are usually collected near - rather than directly on - the body joints. For example, sensors on smartwatches collect data near the wrist, not on the wrist joint itself. We plan to incorporate random offset vectors to better simulate real-world signal variations near joints. (2) While our framework effectively addresses the classification task, we intend to extend its applicability to other motion time series tasks such as inertial navigation. (3) Our current pre-training utilizes existing motion datasets, and we plan to enrich our pre-training corpus with additional motion data extracted from large-scale video-based pose estimation. (4) We also plan to integrate our model with efficient inference optimization techniques such as quantization, pruning and distillation for deployment on edge devices.

**Broad Impact**. UniMTS is the first pre-trained motion time series classification model that generalizes to diverse downstream datasets, irrespective of device locations, orientations and activities. The primary societal concern centers around privacy as motion time series might reveal personal information, so we ensure strict privacy controls at the earliest stages of model development by pre-training exclusively on synthetic data. With UniMTS's state-of-the-art performance in zero-shot, few-shot and full-shot settings, we believe it would bring broad, positive impact to the community.

## 6 Acknowledgement

Our work is supported in part by ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.

Our work is also sponsored in part by NSF CAREER Award 2239440, NSF Proto-OKN Award 2333790, as well as generous gifts from Google, Adobe, and Teradata. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and should not be interpreted as necessarily representing the views, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for government purposes not withstanding any copyright annotation hereon.