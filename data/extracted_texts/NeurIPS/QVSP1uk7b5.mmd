# Tetrahedron Splatting for 3D Generation

Chun Gu\({}^{1}\)  Zeyu Yang\({}^{1}\)  Zijie Pan\({}^{1}\)  Xiatian Zhu\({}^{2}\)  Li Zhang\({}^{1}\)

\({}^{1}\)School of Data Science, Fudan University \({}^{2}\)University of Surrey

https://fudan-zvg.github.io/tet-splatting

Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author.

###### Abstract

3D representation is essential to the significant advance of 3D generation with 2D diffusion priors. As a flexible representation, NeRF has been first adopted for 3D representation. With density-based volumetric rendering, it however suffers both intensive computational overhead and inaccurate mesh extraction. Using a signed distance field and Marching Tetrahedra, DMTet allows for precise mesh extraction and real-time rendering but is limited in handling large topological changes in meshes, leading to optimization challenges. Alternatively, 3D Gaussian Splatting (3DGS) is favored in both training and rendering efficiency while falling short in mesh extraction. In this work, we introduce a novel 3D representation, Tetrahedron Splatting (_TeT-Splitting_), that supports easy convergence during optimization, precise mesh extraction, and real-time rendering _simultaneously_. This is achieved by integrating surface-based volumetric rendering within a structured tetrahedral grid while preserving the desired ability of precise mesh extraction, and a tile-based differentiable tetrahedron rasterizer. Furthermore, we incorporate eikonal and normal consistency regularization terms for the signed distance field to improve

Figure 1: 3D assets generated by our proposed _TeT-Splitting_.

generation quality and stability. Critically, our representation can be trained without mesh extraction, making the optimization process easier to converge. Our _TeT-Splitting_ can be readily integrated in existing 3D generation pipelines, along with polygonal mesh for texture optimization. Extensive experiments show that our _TeT-Splitting_ strikes a superior tradeoff among convergence speed, render efficiency, and mesh quality as compared to previous alternatives under varying 3D generation settings.

## 1 Introduction

Automatic 3D content generation is revolutionizing fields such as virtual reality, augmented reality, video games, and industrial design. This technology can significantly enhance user experiences and streamline creative processes for reducing time demands and simplifying the complexities associated with creating high-quality 3D assets.

3D representations (_e.g._, Neural Radiance Field  (NeRF)) play an essential role in recent advancements in 3D generation, along with the Score Distillation Sampling (SDS) technique objective  for exploiting off-the-shelf 2D diffusion models [11; 37; 38; 1]. Although serving as a pioneer representation, NeRF is significantly limited due to its intensive computational demands, particularly when paired with high-resolution 2D diffusion models. Moreover, its density-based volumetric rendering struggles with accurate mesh extraction, which is crucial for practical applications.

By utilizing a signed distance field and Marching Tetrahedra for differentiable mesh extraction, DMTet  enables efficient high-resolution rendering and precise mesh extraction, overcoming the limitations of the NeRF approach. In cases, it becomes a favored choice [18; 33; 3]. However, DMTet is limited in its ability to handle large topological changes in meshes, as it can only backpropagate to the zero-level set of the signed distance field, constraining its geometry convergence during optimization. As a workaround, a two-stage 3D generation pipeline has been adopted that initially utilizes NeRF for rapid geometry convergence and then transitions to DMTet for detailed refinement . However, transitioning from NeRF to DMTet often results in a degradation of quality, as the strengths of each representation are not fully leveraged throughout the entire optimization process.

Alternatively, recent methods have introduced 3D Gaussian Splatting  (3DGS) into the optimization process, significantly enhancing efficiency. For example, DreamGaussian  utilizes 3DGS but acknowledges that meshes directly generated from 3DGS can be blurry, and the mesh extraction process often results in unsatisfactory surfaces with visible holes . Moreover, the text-to-3D process with 3DGS suffers from instability due to its unstructured nature and the densification process.

In this work, we introduce _TeT-Splitting_, a novel all-round 3D representation that integrates surface-based volumetric rendering into the tetrahedral grid, while preserving precise mesh extraction through Marching Tetrahedra. It supports easy convergence during optimization, precise mesh extraction, and real-time rendering _simultaneously_, enabling high-fidelity 3D generation effectively (Figure 1). Drawing inspiration from 3DGS , we design a tile-based fast differentiable rasterizer for real-time rendering, efficiently handling the alpha-blending of projected 2D splats from 3D tetrahedra. These splats are blended based on opacity values derived from the signed distance field within each tetrahedron as in NeuS . To further increase efficiency, we include a pre-filtering process to remove nearly transparent tetrahedra, reducing the number of tetrahedra necessary for splatting. Moreover, we introduce eikonal and normal consistency regularization terms to refine the signed distance field, which helps stabilize the optimization process and prevents the common issue of debris in the optimization with DMTet. In Table 1 we compare the features of different 3D representations.

  
**Representation** & NeRF  & 3DGS  & DMTet  & TeT-Splitting (**Ours**) \\ 
**Precise mesh extraction** & & ✓ & ✓ \\
**Easy convergence** & ✓ & ✓ & ✓ \\
**Real-time rendering** & & ✓ & ✓ \\ 
**Representative** & _DreamFusion_, & _DreamGaussian_, & _Fantasia3D_, & _Ours_ \\
**method** & _Magic3D_ & _GSGEN_ & _RichDreamer_ & \\   

Table 1: Comparison of different representations for 3D generation.

Our **contributions** are fourfold: **(i)** Introducing a novel 3D representation, _TeT-Splitting_, that integrates synergistically volumetric rendering into a tetrahedral grid; **(ii)** Designing a fast differentiable rasterizer for tetrahedra; **(iii)** Forming a generic two-stage 3D generation pipeline that initially leverages _TeT-Splitting_ for geometry optimization, and then transitions it to polygonal mesh for texturing; **(iv)** Extensive evaluations demonstrating the superior tradeoff of our method among easy convergence, real-time rendering, and precise mesh extraction over alternative representations (InstantNGP , DMTet , and 3DGS ) under a variety of settings with different diffusion priors.

## 2 Related work

3D representationSince the introduction of Neural Radiance Field (NeRF) , NeRF has become a foundational technique in the field of 3D reconstruction. It employs volumetric rendering to enable 3D optimization with only 2D supervision. Despite its significance, NeRF faces major issues, such as slow rendering speeds and high memory usage. To address these problems, several research[43; 39; 29; 2; 13] have developed novel variants of the radiance field, focusing on faster training and rendering and using less computing resources. Diverging from the path of NeRF, DMTet  has introduced an approach based on Marching Tetrahedra and surface rendering by differentiable rasterization , offering much faster rendering speed. Recently, 3D Gaussian Splatting  (3DGS) has unified NeRF-like alpha-blending with tile-based rasterization, achieving high performance in both quality and rendering speed. In this paper, our proposed _TeT-Splitting_ takes inspiration from the structured tetrahedral grid in DMTet  and incorporates tile-based rasterization from 3DGS , utilizing tetrahedra for splatting. _TeT-Splitting_ achieves high converge and rendering speed while preserving precise mesh extraction through Marching Tetrahedra.

3D generationThe data-driven 2D diffusion models [11; 37; 38; 1] have demonstrated unprecedented success in image generation. However, the transition to direct 3D generation [31; 12; 10; 24; 8; 58; 6; 52; 19; 15; 45] faces formidable challenges, as this research line often fails to generate high-quality 3D assets limited by the lack of training data. To circumvent these issues, some works [20; 41; 23; 42; 21; 25; 50] train 2D diffusion models to make them have 3D awareness. However, discrete and sparse 2D images still cannot offer sufficient 3D information. In this context, DreamFusion  first introduced score distillation sampling (SDS) loss to leverage 2D diffusion priors for 3D generation. Subsequent studies [47; 57; 59; 54; 51; 17; 26; 44; 48] have aimed to improve the SDS loss, enhancing both the fidelity and stability of 3D generation. Moreover, several efforts [55; 56; 4; 16; 42; 34; 22] have been made to improve the quality and multi-view consistency of 3D models by integrating a wider array of diffusion priors. Despite these advancements, some methods are hindered by the significant computational demands due to the usage of NeRF , which limits the effective use of high-resolution diffusion priors. Additionally, other mesh-based models [3; 16; 34] encounter issues with instability and slow convergence due to the nature of surface rendering. By contrast, our _TeT-Splitting_ facilitates the use of high-resolution diffusion priors and ensures efficient updates, thanks to its volumetric rendering and tile-based differentiable rasterizer.

## 3 TeT-Splitting

### Deformable tetrahedral grid

In this section, we will start with an introduction to the deformable tetrahedral grid, which is the geometric primitive for the proposed representation. The deformable tetrahedral grid is first employed in DeTet  and then extended in DMTet  to approximate the implicit surface by assigning each vertex an SDF value. Specifically, this structure considers a tetrahedral mesh composed of \(N\) vertices and \(K\) tetrahedra, denoted as \((V_{T},T)\), where \(V_{T}=\{_{n}|n 1,,N\}\) signifies the positions of vertices, and \(T=\{t_{k}|k 1,,K\}\), with each \(t_{k}\) representing the indices \((a_{k},b_{k},c_{k},d_{k})\) of four vertices \((_{a_{k}},_{b_{k}},_{c_{k}},_{d_{k}})\) that form a tetrahedron. Utilizing the SDF value associated with each vertex \(_{n}\), denoted by \(f_{n}\), a signed distance field is established by interpolating the SDF values within each tetrahedron. DMTet  has developed a method for mesh extraction from the tetrahedral grid by assigning one or two triangles to each tetrahedron that intersects the zero-level set of the signed distance field, known as Marching Tetrahedra (MT). Employing a differentiable triangular rasterizer, it attains a remarkable rendering speed while maintaining minimal memory consumption.

However, a particular limitation of the MT is that only the parameters associated with tetrahedra intersecting the zero-level set of the signed distance field can be updated during optimization. This restriction poses challenges in managing large topological changes and often causes the optimization to stuck in the undesired shape in the early stage. In contrast, NeRF is less affected by such instability thanks to its volumetric nature. Many prior works [18; 33] have employed NeRF in 3D generation. These works typically adopt a two-stage pipeline that starts with the volumetric representation to swiftly achieve a coarse model with low-resolution diffusion priors and then transitions to a polygonal mesh for further refinement with high-resolution diffusion priors. However, these approaches are often hindered by the slow optimization and inaccurate geometry brought by volume rendering. The inaccurate geometry would lead to obvious degradation after mesh extraction.

### Differentiable tetrahedron splatting

In this work, we present a unified representation that combines the precise mesh extraction via the tetrahedral grid and the efficient optimization of volumetric rendering. Inspired by 3D Gaussian Splatting  (3DGS), we also integrate the tile-based rasterizer into our framework to facilitate real-time rendering. 3DGS enhances rendering efficiency through rasterization and ensures efficient optimization via alpha-blending by projecting 3D Gaussians to 2D splats followed by fast alpha-blending. However, 3DGS relies on unstructured 3D Gaussians as rendering primitives, necessitating carefully designed densification processes and learning rates to manage the highly noisy SDS loss. In contrast, the tetrahedral grid is structured while its vertices can only deform in a local region and are connected with neighbors to form tetrahedra. We explore treating tetrahedron as rendering primitive of the splatting process to perform alpha-blending. Moreover, we can directly extract polygonal mesh through Marching Tetrahedra from the tetrahedral grid, while the mesh extracted  from 3DGS may result in an unsatisfactory surface with visible holes.

Next, we will elaborate on how we realize differentiable tetrahedron splatting through alpha-blending. Consider a pixel on the image plane, along with its corresponding ray in 3D space. To perform alpha-blending, we need first determine the intersected tetrahedra between the ray and the tetrahedral grid. For a single tetrahedron \(t\) with vertices \((_{a},_{b},_{c},_{d})\) and SDF values \((f_{a},f_{b},f_{c},f_{d})\), we can project the vertices onto the image plane, resulting in four overlapped triangles that form a 2D tetrahedron splat. Intersection with the tetrahedron \(t\) is equal to the intersection with the four triangles. The position and SDF value of an intersection point can be calculated using barycentric coordinates (see Appendix A for details). Different from 3DGS , we consider the opacity of tetrahedra instead of Gaussians. Note that a ray can only have two intersection points with a tetrahedron, we denote their SDF values as \(f_{}\) and \(f_{}\) in depth order. Then the opacity of the tetrahedron \(t\) can be derived

Figure 2: **Left: An overview of _TeT-Splitating_. To produce the final renderings, we first pre-filter and remove nearly transparent tetrahedra, then project the remaining ones into 2D splats. These are blended based on opacity values derived from the SDF values at specific pixel intersections. Right: _TeT-Splitating_ for 3D generation. We employ _TeT-Splitating_ in the initial stage of the 3D generation pipeline and subsequently transition it to polygonal mesh for texture optimization.**

in NeuS  manner:

\[=((f_{})-_{s}(f_{})}{ _{s}(f_{})},0),\] (1)

where \(_{s}(x)=(1+e^{-sx})^{-1}\), and the \(s\) value controls the steepness of the conversion. Following Voxurf , we update \(s\) manually for each iteration \(i\): \(s=i/s_{}+s_{}\). The final normal map \(\), depth map \(\) and opacity map \(\) are derived by alpha-blending \(N\) sequentially ordered tetrahedra from front to back:

\[\{,,\}=_{i N}T_{i}_{i}\{_ {i},_{i},1\},\ \ T_{i}=_{j=1}^{i-1}(1-_{j}),\] (2)

where \(\) denotes the per-tetrahedron normal and \(_{i}\) denotes the average depths of four vertices.

**Pre-filtering** To conserve computational resources, the tetrahedra with low opacity will be filtered. Depending on different intersection points, the opacity of a tetrahedron can take different values. We can establish the upper bound of the opacity, denoted as \(_{max}\), by replacing \(s_{prev}\) and \(s_{next}\) in Eq. 1 with the maximum and minimum SDF values of four vertices. Tetrahedra with \(_{max}\) less than a predefined threshold \(T_{f}=\) are filtered to ensure that only tetrahedra with significant enough contribution to the alpha-blending are included in the subsequent splatting process.

**Per-tetrahedron normal** As discussed in Section 3.1, the tetrahedral grid establishes a signed distance field by interpolating the SDF values within each tetrahedron. This interpolation is a linear combination of the SDF values of four vertices. Correspondingly, the barycentric coordinates of an arbitrary point with respect to the four vertices of the tetrahedron exhibit a linear correlation with its spatial position. This ensures that the gradient \(\) of the SDF within the tetrahedron results in a constant vector (see Appendix A for details). The normal vector \(\) of the tetrahedron is thus obtained by normalizing this gradient.

**Relationship between DMTet and TeT-Splitating** During optimization, DMTet employs Marching Tetrahedra to extract polygonal mesh from the tetrahedral grid and subsequently renders through triangular rasterization . Consequently, only a limited number of tetrahedra are involved in each single rendering process. In contrast, _TeT-Splitating_ employs volumetric rendering, which allows all visible tetrahedra within the view frustum that have sufficient weight in alpha-blending to contribute to the final renderings. Moreover, the rendering process in _TeT-Splitating_ is fully differentiable, enabling a single optimization step to influence a significantly larger number of parameters compared to DMTet. Figure 3 presents a comparative analysis of convergence speeds between DMTet and _TeT-Splitating_ within the same Text-to-3D pipeline. As observed, _TeT-Splitating_ achieves rapid convergence, whereas DMTet exhibits slower topological changes and gets stuck in an undesirable shape. Furthermore, as the inverse standard deviation \(s\) in Eq. 1 increases, the curve of \(_{s}(x)\) becomes steeper, causing

Figure 3: **Normal map comparison during optimization of 3D generation. We utilize DMTet and _TeT-Splitting_ as 3D representations in the geometry modeling stage of the RichDreamer . The first two rows show normal maps obtained from DMTet and _TeT-Splitating_ during optimization. _TeT-Splitating_ achieves more stable and smooth optimization, while DMTet becomes fragmented initially and gets stuck in an undesirable shape. The third row shows the normal maps of meshes extracted from the signed distance field of _TeT-Splitating_ via Marching Tetrahedra  (MT). As optimization progresses, _TeT-Splitating_’s behavior aligns with rendering through MT.**

\(\) to approach \(1\) under conditions that \(s_{prev}>0\) and \(s_{next}<0\) and to approach \(0\) otherwise. This behavior (Figure 3) aligns with the rendering process of DMTet  through Marching Tetrahedra, where only the tetrahedra intersecting the zero-level set of the signed distance field are visible.

### Fast differentiable rasterizer for tetrahedra

We implement a tile-based differentiable rasterizer for tetrahedra with custom CUDA kernel building upon the framework of 3DGS . Similarly, we begin by dividing the screen into tiles and culling tetrahedra that do not overlap with the view frustum. We then replicate the tetrahedra based on the number of tiles they overlap and sort them by their tile ID and the average depth of each tetrahedron's vertices, using a fast GPU radix sort . Note that the per-tile sorting in 3DGS is not equivalent to per-pixel ordering. Differently, we maintain a short resorting window  of size \(N_{w}\) for each pixel to re-sort the primitives based on the results of per-tile sorting using the insertion sort. Due to the structured nature of the tetrahedral grid, we find that the sorting error is almost eliminated with a window size of 5 under a grid resolution of 256. The operations after re-sorting for alpha-blending are the same as in 3DGS, except for the computation of \(\), which we have already given in Eq. 1.

## 4 3D generation with TeT-Splitting

In this section, we introduce our 3D generation pipeline and discuss various settings, aiming at validating the effectiveness of _TeT-Splitting_ in 3D generation. As shown in Figure 2, our pipeline is divided into two stages: first get a detailed geometry with _TeT-Splitting_ and then transition to polygonal mesh through Marching Tetrahedra  for texture optimization. We begin by describing our overall 3D model (Section 4.1), and then detail the regularizations (Section 4.2) and diffusion priors (Section 4.3) used in our experiments.

### 3D modeling

Geometry stageWe employ a hash grid \(_{g}\) with parameter \(_{g}\) to encode the signed distance field and deformation which allows each vertex in a tetrahedral grid to deform in a certain range. \(_{g}\) is initialized to a spherical shape. Given a randomly sampled camera, our tetrahedron rasterizer produces renderings of the normal map, depth map, and opacity map.

Texture stageGiven the well-optimized signed distance field from the geometry stage, we convert it into a polygonal mesh through Marching Tetrahedra. To texture the polygonal mesh, we employ the physically based rendering (PBR) pipeline proposed by Nvdiffrec . Please refer to [30; 9; 34] for details. We use another hash grid \(_{t}\) with parameter \(_{t}\) to encode the spatially varying materials of the surface: albedo, roughness, metallic, and bump. Finally, given a specific environment lighting and a randomly sampled camera, we can obtain the renderings of the albedo map and PBR map.

### Regularization

Eikonal lossTo ensure a proper signed distance field, we employ an eikonal term that regularizes the SDF gradient \(\) in each tetrahedron: \(_{}=_{k}(\|_{k}\|_{2}-1)^{2}\).

Normal consistency lossInspired by the normal consistency loss for triangle meshes, we adapt this approach to tetrahedra. While we have designed a per-tetrahedron normal, we project these tetrahedral normals onto vertices and enhance the consistency of the signed distance field by regularizing the cosine similarity between normals of adjacent vertices connected by edges: \(_{}=_{i}(1-(_{e_{i1}},_{e_{i 2}}))\), where \(e_{i1}\) and \(e_{i2}\) represent the vertices forming edge \(e_{i}\).

### Diffusion priors

To validate the capability of _TeT-Splitting_ for 3D generation, we employ two types of diffusion priors: the vanilla RGB-based diffusion priors and the rich diffusion priors proposed in RichDreamer .

Vanilla RGB-based diffusion priorsVanilla RGB-based diffusion models represent diffusion models that can generate RGB images from a given prompt. For both geometry and texture stages, we utilize SDS loss to leverage 2D diffusion priors from Stable Diffusion :\(_{}_{}=[(t)(_{}( ;y,t)-)]\), where \((t)\) is a weighting function, \(z\) denotes the VAE latent code, and \(_{}(;y,t)\) represents the noise estimated by the UNet \(_{}\).

Rich diffusion priorsThe sole use of vanilla diffusion priors often leads to issues such as multi-face Janus problem, domain gap between image diffusion model and normal map while using normal maps as input of diffusion models, and inaccuracies in material decomposition. To this end, we utilize the rich diffusion priors proposed in RichDreamer  to handle high-fidelity 3D generation. Specifically, for geometry optimization, we combine a vanilla Stable Diffusion with a Normal-Depth diffusion model, which generates multi-view normal and depth maps from a given text prompt, represented as: \(_{}=_{}^{}+ _{}^{}\). For texture optimization, we combine a vanilla SD with a Depth-conditioned Albedo diffusion model, capable of producing multi-view albedo maps from a given text prompt, represented as: \(_{}=_{}^{}+_{}^{}\).

In summary, the final loss function for the geometry stage is defined as: \(_{}=_{}+_{} _{}+_{}_{}\). For the texture stage, the loss function simplifies to: \(_{}=_{}\).

## 5 Experiment

In this section, we assess the efficacy of _TeT-Splitating_ across two distinct tasks: 3D generation employing vanilla RGB-based diffusion priors and text-to-3D with rich diffusion priors. In Section 5.1, a qualitative evaluation of 3D generation for both text-to-3D and image-to-3D modalities is conducted to demonstrate the superiority of _TeT-Splitating_ relative to other representations. To substantiate _TeT-Splitating_'s proficiency in handling high-fidelity generations, we conduct experiments with advanced rich diffusion priors in Section 5.2. Section 5.3 encompasses a series of ablation studies aimed at validating the representation and pipeline. The details of implementation and experimental setting can be found in the Appendix B.

Figure 4: Qualitative comparison on 3D generation using vanilla RGB-based diffusion priors. We present visual comparisons of the rendered RGB maps and color maps from various 3D generation methods. The methods, arranged from left to right, are: Magic3D, Fantasia3D, DreamGaussian, and Ours. The comparison is conducted across two tasks, text-to-3D and image-to-3D, with results shown from top to bottom, respectively. Additionally, for each method, we provide the training time and the rendering speed (FPS) for the first stage of the process.

### Results with vanilla RGB-based diffusion priors

Focusing on illustrating the effectiveness of the proposed representation, we primarily compare to three competitors: Magic3D , Fantasia3D  and DreamGaussian . All these competitors employ a two-stage optimization pipeline and leverage Stable Diffusion with SDS loss, but utilize three different representative 3D representations in their initial stages: Instant-NGP  (a fast version of NeRF), DMTet , and 3DGS , respectively. We adapt the diffusion priors of all methods to Stable Zero-1-to-3  for a fair comparison in the image-to-3D task, adding an identical MSE alignment loss. The qualitative evaluations, as shown in Figure 4, illustrate our method's ability to generate more detailed and compact meshes in a relatively short time. In Figure 4, we also report the rendering speed (FPS) of the first stage, at a rendering resolution of 512x512. Although _TeT-Splitating_ operates at a lower FPS compared to DMTet (Fantasia3D) and 3DGS (DreamGaussian), it still achieves real-time rendering. Importantly, this lower FPS does not adversely affect the overall generation process, as the primary bottleneck in generation speed lies with the diffusion model.

We also conduct a comparison of the mesh extraction with Magic3D and DreamGaussian, visualized in Figure 5. It reveals that the meshes extracted from Magic3D often do not faithfully replicate the geometries from the first stage due to the imprecise threshold for converting densities to SDF values, and the extracted mesh in DreamGaussian can result in unsatisfactory surfaces with visible holes. In contrast, our method maintains high quality with negligible degradation after mesh extraction.

### Results with rich diffusion priors

Figure 5: Visualization of normal maps before and after mesh exportation. Note that the normal maps of DreamGaussian  are derived from its depth maps.

Figure 6: Qualitative comparison on Text-to-3D with rich diffusion priors. We also report the total training time of each method.

Our approach is also compatible with state-of-the-art diffusion priors. In this part, we evaluate _TeT-Splitating_ on the text-to-3D task, equipped with rich diffusion priors from RichDreamer . A key distinction between our approach and RichDreamer is the use of _TeT-Splitating_ as the 3D representation during the geometry stage. We evaluate our method against two SOTA competitors, ProlificDreamer  and RichDreamer . As illustrated in Figure 6, our method is capable of handling high-fidelity 3D generation, achieving superior geometric quality with considerably reduced generation times compared to these competitors.

Additionally, we present visualizations of the normal maps from early training iterations in Figure 7. The results from RichDreamer are fragmented at the early iterations due to the use of DMTet, which may harm subsequent optimization and slow convergence. In contrast, _TeT-Splitating_ demonstrates rapid and smooth convergence. We employ the same quantitative evaluation method as RichDreamer to assess the quality of geometry and texture.

In Table 2, we report the Geometry CLIP  score and Appearance CLIP score. Notably, RichDreamer's prompt list, comprising 113 objects used for scoring, is not publicly available. Consequently, we calculate our scores using an alternative set of prompts (see Appendix B for details). Our method outperforms RichDreamer in terms of CLIP scores and significantly reduces the time required for geometry optimization (40 min vs 70 min).

In Figure 8, we present the decomposed albedo maps of generated 3D assets. Guided by the Depth-conditioned Albedo diffusion model, we achieve natural albedo maps.

### Ablation

Under the settings of rich diffusion priors , we conduct ablation studies to evaluate our method.

Figure 8: Visualization of the rendered normal, albedo, and PBR map from the generated 3D assets in the second stage.

Figure 7: Normal map comparison between DMTet  and _TeT-Splitating_ in the early training iterations.

Eikonal lossWe assess the role of eikonal loss in 3D generation by comparing 3D assets generated with and without it, illustrated in Figure 8(a). Models created without eikonal loss tend to develop into undesirable shapes. This issue arises because the SDF values rapidly reach extreme levels and get trapped in local minima when eikonal loss is not applied.

Normal consistency lossAdditionally, we assess the importance of normal consistency loss. Figure 8(b) demonstrates that applying normal consistency loss results in more compact models. This loss can act as a smoothing prior that helps prevent the surface of the model from becoming fragmented.

Tetrahedral grid resolutionWe investigate the effects of tetrahedral grid resolution on model performance by conducting experiments at resolutions of 128 and 256. Higher resolution yields more detailed geometries, as shown in Figure 8(c).

## 6 Limitations

_TeT-Splatting_ struggles with modeling high-frequency features, such as texture, because it uses tetrahedra as rendering primitives, which limits the final output by the resolution of the tetrahedral grid. Therefore, we transition it to a polygonal mesh for enhanced texture optimization. The rendering speed of our implemented rasterizer, although operating in real-time, is slower than that of 3DGS. Additionally, using only a pre-filter operation might not fully leverage _TeT-Splatting_'s potential in rendering quality and speed. A similar densification process as in 3DGS could improve this, which we leave for future work.

## 7 Conclusion

In this study, we introduce Tetrahedron Splatting (_TeT-Splatting_), a novel all-round 3D representation that integrates volumetric rendering within a structured tetrahedral grid while preserving precise mesh extraction through Marching Tetrahedra. Equipped with newly designed tile-based fast differentiable tetrahedron rasterizer, _TeT-Splatting_ achieves real-time rendering. As showcase, we integrate _TeT-Splatting_ in common 3D generation pipeline with polygonal mesh for texture optimization. Extensive experiments under varying 3D generation settings demonstrate _TeT-Splatting_'s superiority in producing high-fidelity 3D content compared to other 3D representations.