# Towards General Loop Invariant Generation: A Benchmark of Programs with Memory Manipulation

Chang Liu\({}^{*}\), Xiwei Wu\({}^{*}\), Yuan Feng, Qinxiang Cao\({}^{}\), Junchi Yan\({}^{}\)

Dept. of CSE & School of AI & Moe Key Lab of AI, Shanghai Jiao Tong University

{only-changer, yashen, fy0123, caoqinxiang, yanjunchi}@sjtu.edu.cn

Official Repository: https://github.com/Thinklab-SJTU/LIG-MM

###### Abstract

Program verification is vital for ensuring software reliability, especially in the context of increasingly complex systems. Loop invariants, remaining true before and after each iteration of loops, are crucial for this verification process. Traditional provers and machine learning based methods for generating loop invariants often require expert intervention or extensive labeled data, and typically only handle numerical property verification. These methods struggle with programs involving complex data structures and memory manipulations, limiting their applicability and automation capabilities. In this paper, we introduce a new benchmark named LIG-MM, specifically for programs with complex data structures and memory manipulations. We collect 312 programs from various sources, including daily programs from college homework, the international competition (SV-COMP), benchmarks from previous papers (SLING), and programs from real-world software systems (Linux Kernel, GlibC, LiteOS, and Zephyr). Based on LIG-MM, our findings indicate that previous methods, including GPT-4, fail to automate verification for these programs. Consequently, we propose a novel LLM-SE framework that coordinates LLM with symbolic execution, fine-tuned using self-supervised learning, to generate loop invariants. Experimental results on LIG-MM demonstrate that our LLM-SE outperforms state-of-the-art methods, offering a new direction toward automated program verification in real-world scenarios.

## 1 Introduction

Program verification [(1; 2; 3; 4)] has become increasingly crucial due to the growing complexity of software systems. As software permeates various aspects of modern life, from critical infrastructure to daily applications, ensuring its correctness and reliability is paramount [(5)]. Program verification aims to provide formal assurances that software behaves as intended, which is especially important in safety-critical systems such as autonomous vehicles [(6)], medical devices [(7)], and financial systems [(8)]. Moreover, with the advent of artificial intelligence, the integration of verified components ensures the robustness and dependability of AI-driven solutions [(9; 10)]. As software complexity and the integration of AI grows, so does the need for sophisticated verification tools to mitigate potential risks, making program verification an indispensable aspect of software engineering and the AI industry.

A crucial element in the realm of program verification is the concept of loop invariants [(11; 12)]. **Loop invariants** are conditions that hold _true_ before and after each iteration of a loop, serving as fundamental properties that ensure the correctness of loops within programs. Their importance cannot be overstated, as loops are ubiquitous and often complex in real-world software. By establishing loop invariants, developers can automate the verification process, significantly reducing the manualeffort required to prove the correctness of loops. This automation not only enhances the reliability of software but also increases efficiency by enabling rigorous analysis of program variables within loops. To better illustrate loop invariants, let's consider a simple case used in previous papers (13):

\[\{Pre\}\{Inv\};&\{Inv B\}\,S\,\{Inv\}; \{Inv B\}\{Post\}\\ &\{Pre\}\ \ B\ \ S\,\{Post\}\\ &\]

To prove the correctness of this numerical program1 in Fig. 1(b), researchers typically adopt symbolic execution systematically exploring codes to derive assertions (conditions). Starting from the beginning, we can record the assertions after conducting every program line. After line 1, the assertion becomes \(x\)==\(-50\). However, the situation is complicated regarding the loop (line 2-5) because we do not know how the program behaves in loops. Loop invariant is used to fill this gap, and the rule of loop invariant is shown in Fig. 1(a). Via these rules, we find \((x<0 y>0)\) satisfied the properties of loop invariant for this loop. Then, we can use the loop invariant to check the assertions after the loop: \((x<0 y>0)(x<0)(y>0)\) (line 6). Finally, we reach the end of the program, and the assertion derived from the beginning is \(y>0\) as expected, which denotes our proof is successful.

Other than simple numerical programs, most real-world software applications involve the use of complex data structures and intricate memory manipulation. These include linked lists, trees, hash-tables, and various user-defined data structures. Verifying programs that operate on these complex data structures is significantly more challenging than dealing with numerical programs. The complexity arises from the need to reason about the shape and content of data structures, manage aliasing and pointer dereferencing, and ensure the integrity of dynamic memory. Moreover, such programs often exhibit intricate behaviors and dependencies that make the inference of loop invariants particularly demanding, which can be shown by the following example in Fig.2(Left):

Figure 1: A numerical program with a correctness assertion and a loop invariant to prove it.

Figure 2: (Left) An example of programs with the data structure like the single linked list; (Right) The pipeline of our proposed LLM-SE: we start with the precondition (S0) and conduct the loop body with symbolic execution multiple times to get more separation logic assertions (S1, S2, S3,...). Based on these separation logic assertions, we further use LLM to infer the loop invariant.

To prove the correctness of the program _list reverse_ manually in Fig. 2(Left), researchers also use symbolic execution to determine the assertions that the program states must satisfy. First, we need to define some predicates to describe the memory states. For example, we use **listrep(x)** to denote a singly linked list starting from pointer x and **lseg(x,y)** to denote a singly linked list segment from the pointer x to the pointer y (excluding y). To simplify assertions, researchers introduced separation logic(14) instead of relying solely on traditional first-order logic. In separation logic, \(P*Q\) indicates that \(P\) and \(Q\) describe properties of two disjoint memory segments. This approach eliminates the need for numerous auxiliary propositions to specify which addresses are distinct. With this groundwork in place, we can easily provide a description of the program state before entering the loop (line 2): w == 0 && v == p && listrep(p). After reading the codes, we find this loop reverses the singly linked list pointed by v and store it in w. Therefore, experienced researchers can infer the loop invariant w == 0 && v == p && listrep(p) || p \(\) tail == 0 && lseg(w,p) * listrep(v). After the loop ends (line 9), v is a null pointer and w points to the reversed linked list. Therefore, the final program state is represented by listrep(w), which denotes that we have completed the verification of this program's memory safety. In this example, because we have a clear understanding of how the program manipulates memory, we can easily write the loop invariant. However, as the number of function calls increases and the memory involved becomes more complex, it is not always straightforward to comprehend the algorithm and write the loop invariant manually.

Traditional program analysis works have made some attempts at automatic loop invariant generation, including LOOPINVGEN ([15; 16]), linux (17), and SLING ([18; 19]). However, they rely heavily on fixed templates for feature extraction, which makes them less adaptable. Techniques like synthesis, abduction, and dynamic analysis require expert intervention and the design of specific inputs. Machine learning approaches have also been explored for loop invariant generation ([20; 21; 22; 23; 24; 25; 13; 26; 27]). Nevertheless, in this task, the labeled data require valid loop invariants, which are hard to automatically generate without expert intervention. In one foundational work CODE2INV (), they use reinforcement learning to avoid label sparsity. Nevertheless, such approaches require much trial and error, which has to be performed on the target programs directly and cannot be pre-trained offline in advance. Another paradigm ([25; 20]) is to borrow the loop invariants found by traditional methods as labels. While the trained models can hardly surpass traditional methods, they can only offer limited speed advantages. Recently, several works ([25; 27; 20]) have begun to utilize large language models in this task. However, to the best of our knowledge, **existing machine learning based methods are all restricted to program's numerical property verification**, neglecting shape analysis and memory safety verification for real-world programs with diverse data structures and memory manipulation.

Unlike existing works, we believe that automatic verification for programs with sophisticated data structures and complex memory manipulation is necessary and significant. In this paper, we propose a **L**oop **I**nvariant **C**eneration benchmark of programs with **M**emory **M**anipulation (**LIG-MM**), which is more challenging than the numerical programs in previous works. LIG-MM contains 312 programs collected from various sources, including common programs from course homework, programs from the international competition on software verification (SV-COMP()), programs from previous papers (SLING([18; 19])), programs from real-world software and systems(Linux Kernel(), GlibC(), LitesOS(), and Zephyr()). Our benchmark requires the ability of methods to perform shape analysis and memory safety verification via generating valid loop invariants.

We have evaluated multiple baselines on our proposed LIG-MM benchmark, including state-of-the-art traditional prover, machine learning based methods, and the popular large language model GPT-4. Though these baselines may perform well on numerical programs, they can hardly work on our LIG-MM, as their pass rates are less than \(15\%\) in one attempt. It has demonstrated the challenges of our LIG-MM compared to existing numerical program benchmarks, and the difficulty of generating loop invariants reaches a new level when it involves data structures and memory manipulations.

To address the challenge in our LIG-MM benchmark, we further propose a new framework combining **L**arge **L**anguage **M**odels and **S**ymbolic **E**xecution named **LLM-SE**. As shown in Fig. 2(Right), by integrating symbolic execution, we can systematically explore program codes and generate separation logic assertions sufficient for LLM to infer loop invariants. This hybrid approach not only automates the verification process but also significantly reduces the need for expert intervention and tailored inputs. Furthermore, our method is scalable to multi-layered loops and multi-lever pointer operations, allowing for efficient and effective invariant generation across various program types.

To sum up, our main contributions are:1. We construct a challenging benchmark named LIG-MM of separation logic-based loop invariant generation to verify the shape safety of memory-manipulated programs.
2. We evaluate multiple baselines including GPT-4 on our dataset to analyze the capacity of existing approaches to generating loop invariants in our LIG-MM benchmark.
3. Aiming at the LIG-MM benchmark, we propose a new approach LLM-SE that combines large language models with symbolic execution, and demonstrate its performance. We believe it will be a new direction for loop invariant generation and the entire program verification field.

## 2 Background

### Separation Logic Assertion

Separation logic (14) is a formal system introduced by John C. Reynolds in the early 2000s, which is for reasoning about the correctness of computer programs. Separation logic uses a new connective separating conjunction to ensure that different names duplicate no identical addresses. The separating conjunction \(P*Q\) represents the existence of two disjoint portions of the state, one that satisfies \(P\) and one that satisfies \(Q\). Specifically,

\[m P*Q m_{1},m_{2}.m=m_{1} m_{2},m_{ 1} P\&\&m_{2} Q.\]

Here \(\) means the disjoint union2 and \(m P\) means \(m\) satisfies \(P\). A distinction between \(*\) and \(\&\&\) is that \(P*P P\) where \(P\&\&P=P\). In particular, if \((p,v)\) means that the value \(v\) is stored at address \(p\), \((p,v)*(q,u)\) implies \(p q\), and thus \((p,v)*(p,v)\) is always false: there is no way to divide a heap in such a way that a cell \(p\) goes to both partitions.

Therefore, we can define separation logic assertions used to describe program states, which take the form \((P_{1}\&\&\&P_{2}\&\&\&\&P_{n}\&\&Q_{1}*Q_{2}* *Q_{m})\), where the pure part \(P\) describes properties between terms (e.g. e1, e2), and the spatial part \(Q\) is a separating conjunction of spatial predicates. For example, e1=e2 and e1>e2 can appear as pure propositional conjuncts; the empty heap predicate \(\), the points-to predicate \(\)(e1,e2) can appear as spatial conjuncts. Based on the definitions of \(\) and \(\), we can give specific definitions of \(\) and \(\).

listrep(x) := x == 0 && emp || \(\) z, store(&(x \(\) tail), z) * listrep(z)

lseg(x,y) := x == y && emp || \(\) z, store(&(x \(\) tail), z) * lseg(z, y)

### Symbolic Execution and Correctness Check of Loop Invariant

Symbolic execution is a program analysis technique that explores the execution paths of a program symbolically, without concrete inputs(33). Put simply, symbolic execution is a function that takes assertions and program statements as input and evaluates an assertion. It calculates the strongest postcondition after each statement. The main usage of symbolic execution is to generate separation logic assertions. Take the example in Fig. 2(Left & Right) for an example:

We start the symbolic execution process before entering the loop body (line 2) at the precondition S0: w == 0 && v == p && listrep(p). When entering the loop, as the loop condition (line 3) becomes true, the assertion becomes w == 0 && v!= 0 && v == p && listrep(p). Based on the definition of the predicate **listrep**, we will expand this assertion into an equivalent form \(\) v0, w == 0 && v == p && p \(\) == v0 && listrep(v0), which guarantees the legitimacy of subsequent reads and writes to the v -adverse. After four assignment statements (line 5-8), the assertion becomes S1: w == p && v == t && w \(\) tail== 0 && listrep(v). We have calculated the strongest postcondition S1 for S0 after one cycle of the loop body. Similarly, we can get the strongest postcondition S2 S3 or more (the blue boxes of Fig. 2(Right)) for executing the loop body several times. Via symbolic execution, we can acquire sufficient separation logic assertions, which may be useful in inferring loop invariants.

Moreover, we can complete the correctness check of loop invariants via symbolic execution. As shown in Fig. 1(a), a legitimate loop invariant \(\) needs to satisfy three properties (1) \(\) is true before the first iteration of the loop (2) if I is true before an iteration, it remains true before the next iteration (3) I will not generate error after the end of the loop. For the first property, we need to check that the pre-condition of the loop derives I. For the third property, we simply go on to symbolic execution with I \(\)**-condition**. As for the second property, we need to compute the strongest postcondition of I after executing the loop body once and check if it derives I. After all these three properties are checked by the symbolic execution, we can conclude that the loop invariant is correct.

### Related Work

The related works cover different aspects of works that are related to our work. Due to the page limit, we have to place it in the Appendix (A.1).

## 3 LIG-MM: Loop Invariant Generation Benchmark of Programs with Memory Manipulation

As we mentioned before, the benchmark programs in existing papers mostly contain numerical programs. To fill the lack of benchmarks for general loop invariant generation, we propose LIG-MM, a loop invariant generation benchmark of memory manipulation programs. Table 1 below shows the basics of the code in LIG-MM. Our programs come from four main sources: course codes, competition codes, previous relevant work, and the actual system codes. The programs are modified into a unified format for better usage. Multiple examples are shown in the Appendix (A.3), and the licenses of existing benchmarks can also be found in the Appendix (A.5).

* _Course codes._ The course code is mainly derived from homework programs on the data structure course and programming language course. The detailed course number and college name are covered due to the privacy requirements. These programs contain the most diverse data structures and multi-level pointer operations among the sources of our benchmark.
* _Competition codes._ SV-COMP[(34; 28)] is a competition on software verification, which provides a benchmark for verification tasks for comparing verification tools. Originating from competition, this dataset encompasses various verification tasks, providing a comprehensive set of real-world and synthetic examples for testing the effectiveness and efficiency of verification techniques. In our LIG-MM, we select programs from the 2022 competition benchmark.
* _Previous relevant work._ SLING [(18; 19)] uses traditional dynamic analysis techniques to generate invariants. Other than loop invariants, SLING can also generate preconditions and post-conditions for function. Therefore, not all their benchmarks include the inference of loop invariant or even contain a loop (they use function calls to replace loops). After selection, we choose some of the programs in its benchmark and turning them into a uniform code style.
* _Real-world system codes._ To make the data in LIG-MM closer to real-world software environments, we decide to select more programs from several well-known software and systems. Among them, GlibC [(30)] is the GNU implementation of the C standard library, providing essential functionalities for numerous applications. Additionally, we have incorporated programs from the Linux Kernel [(29)], a widely used and highly-regarded operating system kernel that serves as the foundation for countless devices and systems worldwide. To further enhance the diversity of our dataset, we have included LiteOS [(31)], a lightweight operating system designed for IoT devices, and Zephyr [(32)], another versatile operating system known for its applicability in resource-constrained environments.

By integrating these varied sources, LIG-MM captures a broad spectrum of programming practices and challenges and ensures that our benchmark is robust and representative of the complexities encountered in multiple scenarios, such as real-world software development and verification. Unlike the numerical program benchmark in previous works [(15; 16; 13; 25; 27; 20)], **our benchmark does not contain pure numerical procedures, all of our programs are related to at least one certain data structure.** The data structures we have selected include singly linked lists(sll), doubly linked lists(dll), trees, and hash-tables. In addition, our benchmark includes the usage of multi-level pointers and various pointer arithmetic.

## 4 LLM-SE: Loop Invariant Generator via Coordinating Large Language Models and Symbolic Execution

As we mentioned before, the core idea of our approach is to infer proper loop invariants based on the separation logic assertions provided by symbolic execution. In this section, we first explain why we believe the LLM is supposed to work in the task of loop invariant generation. Then, we briefly introduce the preliminaries of the symbolic execution used in our work. Finally, we propose a framework to auto-generate loop invariants, combining the power of LLM and symbolic execution introduced in Sec 2.2.

### Insights of Inferring Loop Invariants

As the basis of inferring, we plan to use symbolic execution to generate sufficient separation logic assertions. Since they are derived by the same loop body after the precondition, several patterns or similarities should exist inside them, which can be captured for inferring. To take a deeper look, we use the list reverse program in Fig. 2 as an example:

``` S0:w==0&&v==p&&listrep(p) S1:w==p&&v==t&&w-tail==0&&listrep(v) S2:w-tail==p&&v==t&&p-tail==0&&listrep(v) S3:exists_1,w-tail==-1&&-1&&-1&&-1&&-1&&-1&&-1&&-1&&-1&&-1==p&&v==t&&p-tail==0&&listrep(v) S4:exists_1_2,w-tail==-1&&-1&&-1&&-1&&-1==-2&&-2&&-2&&-2&&-2&&-2&&-2&&-3&&-3-tail==p&&v==t&&p-tail==0&&listrep(v) ```

In this example, we write down their separation logic assertions after multiple symbolic executions. We can find several terms in [S0,S1,S2,S3,S4,S5,...] that are related to \(\), and we mark these terms with red. Since the position and the order of these terms may vary, one may need to design a specific algorithm to detect such a syntactic pattern in traditional program logic research. As we all know, LLM has strong abilities in extracting features and detecting patterns, and thus, it has the potential to infer loop invariants or some key parts of them. Supposing LLM successfully infers \(\), then we can modify the original separation logic assertions by replacing those red terms:

``` S0:w==0&&v==p&&listrep(p) S1':v==t&&p-tail==0&&lseg(w,p)*listrep(v) S2':v==t&&p-tail==0&&lseg(w,p)*listrep(v) S3':v==t&&p-tail==0&&lseg(w,p)*listrep(v) S4':v==t&&p-tail==0&&lseg(w,p)*listrep(v) S5':v==t&&p-tail==0&&lseg(w,p)*listrep(v) ```

Moreover, we can find that other terms in the updated separation logic assertions also have patterns. For example, p\(\)tail==0 appears in S5', S4',S3',S2', and S1'. As a result, the LLM can continue to infer p\(\)tail==0, update the separation logic assertions again and further infer \(\). Now, every term in the separations logic assertions has been covered by the terms inferred by LLM, except for S0. Finally, we get the following loop invariant candidate.

    & Concrete Resources & Number of Programs & Data Structure Types \\  Course codes & Course homework programs & 187 & sll, dll, tree, hash-table \\ Competition codes & SV-COMP (34; 28) & 27 & sll, dll, tree, hash-table \\ Previous benchmark & SLING (18; 19) & 15 & sll, dll, tree \\ Real-world programs & Linux Kernel (29) & 23 & sll, dll, hash-table \\ Real-world programs & GlibC (30) & 13 & dll, hash-table \\ Real-world programs & LiteOS (31) & 12 & dll \\ Real-world programs & Zephyr (32) & 35 & sll, dll, hash-table \\ Overall & - & 312 & sll, dll, tree, hash-table \\   

Table 1: Statistics of our proposed LIG-MM benchmark.

inv: p\(\)tail==0 && lseg(w,p) * listrep(v) || w == 0 && v == p && listrep(p)

By constantly inferring the terms of loop invariants via LLM, we can end up with valid loop invariants. Please note that this is only a very simple example and many programs are way more complicated than this one, of which the loop invariant is not easily found. We will place more examples in the supplementary file, please refer to them if interested.

### Framework Overview

In this work, we propose to combine the power of pretrained LLM and traditional verification tools, aiming at efficiently inferring the loop invariants. The overall framework of LLM-SE is shown in Figure 3, including the offline training process and the online querying process, where offline training involves fine-tuning LLM, and online querying refers to the LLM's real-time usage for generating loop invariants of incoming programs unseen before.

In the offline training process, the major challenge is the scarcity of labeled data. In loop invariant generation, the data refers to programs and the label refers to valid loop invariants, which require expert domain knowledge and are hard to produce massively. To address this issue, we decided to borrow the power of self-supervised learning, to create enough labeled data for training by designing an auxiliary task. As Figure 3(left) shows, we generate extensive labeled data by splitting predicates of data structures based on their definitions, and treat the reassembly of these split predicates as the auxiliary task. Furthermore, we employ _data augmentation_ and _mix up_ techniques to further process the synthetic data and leverage the difficulty of the auxiliary task. This process is totally automatic, and can be readily applied to new data structures and predicates, as long as they are well-defined.

The online querying process, illustrated in Figure 3(right), involves an interactive system between the LLM and multiple traditional verification tools, which allows the latter to validate and complete the inferences of the former, finally refining the outputs of LLM to valid loop invariants. In essence, LLM first infers a term based on current separation logic assertions generated by symbolic execution. Then, we check and update the separation logic assertions by replacing the related terms via the

Figure 3: Our proposed framework LLM-SE. (Left) Offline training: we construct an auxiliary task by splitting and recovering the predicates of data structures, following the self-supervised learning paradigm to finetune LLM; (Right) Online querying: we design an interactive system to handle the programs needed to be verified. The well-trained LLM is directly applied to unseen programs. Multiple verification tools are utilized to cooperate with the LLM to generate valid loop invariants.

entailment solver. This cycle persists until the terms of the original separation logic assertions are all covered, resulting in a collection of updated assertions, which could be composed of the loop invariants. Finally, we utilize an SAT-based pick algorithm to choose a concise and equivalent set of them to complete the loop invariant, ensuring validity and correctness. This collaborative approach merges the computational power of LLM with the rigorous validation and refinement capabilities of verification tools, and effectively yields a number of dependable loop invariants.

Due to the page limit, we have to move the detailed introduction of our proposed LLM-SE into the Appendix (A.2). Please refer to it for more details.

## 5 Experiments

In this section, we evaluate our proposed LLM-SE and other baselines on our LIG-MM. The content of our benchmark and the repository of our code will be illustrated in the supplementary materials, which allow researchers to reproduce the results shown in our experiments easily.

### Set up

Our LLM-SE is implemented in Python and based on the HuggingFace 3 framework. We choose one pretrained LLM called CodeGen [35; 36] as backbone to propel our approach. It is tailored explicitly for code-related tasks with various applications [37; 38; 39], endowed with a comprehensive understanding of code structures, program semantics, and syntax. After several attempts, we found that the 350M version of CodeGen is very cost-effective and has considerable performance. With this model size, our LLM-SE framework can be easily adopted on common servers and PCs.

We conducted experiments on a Linux workstation with NVIDIA 3090 GPUs and an AMD Ryzen Threadripper 3970X 32-Core CPU with 128G RAM. In the offline process, we generated 200,000 synthetic data for the auxiliary task we mentioned before, and the model size after fine-tuning is merely 1.4 GB. In the online querying process, we directly feed the benchmark programs to the well-trained LLM-SE without further modification. Please note that the benchmark programs are totally unseen during offline training, and there is no need to worry about overfitting issues.

For the baseline, we compare our proposed method with the following methods: 1) **SLING**, the state-of-the-art method for separation logic loop invariant detection. We follow their official repository 4 to implement it, but it is worth noting that the docker provided is missing, and we create a similar workstation on our own; 2) **AutoSpec**, the latest LLM based method for loop invariant generation on numerical property verification, which is selected as a representative of existing numerical program based works; 3) **GPT-4**, the most commonly used LLM in research papers, and the prompt text we used in our experiments is shown in the Appendix (A.4).

For the use of our LIG-MM benchmark, the programs in LIG-MM are all regarded as the test set, and we do not provide the training set. The definitions of predicates are collected and passed to the methods in advance, where our LLM-SE uses them for self-supervised learning, GPT-4 uses them for prompt learning, SLING and AutoSpec just regard them as part of the domain knowledge.

### Results and Discussion

Table 2 and Figure 4 show the experimental results on the LIG-MM benchmark. We can see that our proposed LLM-SE surpasses all baselines by a higher pass rate. The pass rate denotes the

  Method & SLING (18) & AutoSpec (27) & GPT-4 (40) & LLM-SE (ours) \\  Pass Rate @ 1 & 12.82\% & 0.00\% & 12.50\% & **38.78\%** \\ Pass Rate @ 8 & 21.79\% & 0.00\% & 17.68\% & **42.95\%** \\  

Table 2: Experimental results on our LIG-MM benchmark, 312 programs in total. Pass rates are reported as Pass Rate @ N, where N is the number of attempts to generate loop invariants.

correctness of the output loop invariant. In our benchmark, we provide an entailment solver to check the correctness of the output loop invariant automatically (refer to Sec. 2.2). But for baselines other than our LLM-SE, the output loop invariant of their results may not always be in valid format, especially for GPT-4. Therefore, we have to manually check the correctness again for the ones in invalid format, which makes the pass rate of baselines higher to some extent.

We consider the poor performance of baseline AutoSpec to be due to the difference between numerical programs and memory-involved programs. AutoSpec is an outstanding framework for generating loop invariants for numerical property verification, but it turns out the special design and the prompt learning of AutoSpec on numerical programs do not work on our LIG-MM benchmark.

The performance of the traditional method of SLING is also not satisfactory. It may be because the programs used in the paper of SLING extensively use function calls instead of real loops. As listed in their official GitHub repository 5, most of their programs do not contain a loop. As a result, SLING mainly focuses on inferring pre/postconditions in these programs. In contrast, our benchmark programs all contain at least one loop, and several programs contain extra function calls to more loops. Moreover, SLING can hardly infer the correct loop invariants in one attempt, often requiring tens or hundreds of attempts in one program. This characteristic makes it perform poorly, especially on Pass Rate @ 1, as it failed on some of its own benchmarks.

Though our proposed LLM-SE outperforms all baselines, its pass rate may still not be sufficient to meet our expectations. We still need to improve the pass rate to make it applicable to the real world. We hope our work can pave the way for new advancements in loop invariant generation and inspire the broader community. We encourage more researchers to build upon our findings and further enhance the capabilities and reliability of automated program verification techniques.

  
**Method** & **Pass rate @ 1** & **Pass rate @ 8** \\  SLING & 12.82\% & 21.79\% \\ Auto-Spec & 0.00\% & 0.00\% \\ GPT-4 & 12.50\% & 17.68\% \\  LLM-SE (ours) & 38.78\% & 42.95\% \\ w/o data aug. & 27.88\% & 32.05\% \\ w/o SSL & 8.33\% & 13.46\% \\ w/o SE (CodeGen) & 0.00\% & 0.00\% \\   

Table 3: Ablation study results of different methods. We sequentially exclude key components from LLM-SE (ours): data augmentation (data aug.), self-supervised learning (SSL), and symbolic execution interaction (SE). Pass rates are reported as Pass Rate @ N, the same as before.

Figure 4: The one attempt pass rate (Pass Rate @ 1) on every source of programs in our LIG-MM, where the x-axis denotes the different sources and four bars represent the pass rates of four methods.

### Ablation Studies

We conducted ablation studies by excluding different components of our LLM-SE. The key components sequentially removed include the data augmentation module (data aug.), the self-supervised learning module (SSL), and the interaction with the symbolic execution module (SE). Removing these components effectively degrades the model to simpler versions, with the final configuration being reduced to the base large language model, CodeGen.

Table 3 summarizes the results of the ablation study. We have evaluated the model's performance using the pass rate at 1 attempt and 8 attempts. The results show that the self-supervised learning (SSL) component is essential for our LLM-SE's performance, as the model significantly underperforms without it. In contrast, the data augmentation module has a more supplementary role, providing performance improvements but not as critical as SSL. As expected, the performance drops drastically when symbolic execution (SE) is removed, reducing the model to the base CodeGen, which fails to perform effectively in our tasks.

Furthermore, we conduct analysis the performance of these methods on different data structures. The pass rate @8 for each method is summarized in Table 4.

As shown in Table 4, our proposed LLM-SE demonstrates considerable performance across all types of data structures. In comparison, GPT-4 performs relatively well on the single linked list, while SLING shows better results on the double linked list. However, both methods struggle significantly with more complex or less common structures like the Listbox, where LLM-SE excels with a pass rate of 41.67%. This indicates that while baseline methods such as SLING and GPT-4 handle simpler structures to some extent, they fall short when it comes to intricate memory manipulations.

LLM-SE achieves a total pass rate of 42.95%, highlighting its robustness and flexibility in handling diverse data structures. In contrast, the inconsistent results from SLING and GPT-4, particularly on data structures beyond single and double linked lists, reveal the limitations of these methods. AutoSpec, with near-zero performance across all categories, underscores the need for significant improvements in handling diverse data structures within program verification tasks.

In summary, existing methods show varying levels of success with specific data structures, but their overall performance is inconsistent and inadequate for comprehensive program verification. Consequently, our proposed LLM-SE fills this gap by providing a more reliable and effective solution for managing a wide range of data structures.

## 6 Conclusion

In summary, we introduce a new loop invariant generation benchmark, LIG-MM, particularly for programs with complex data structures and memory manipulations. Due to the poor performance of existing methods on LIG-MM, we further propose a new framework, LLM-SE, which leverages LLM fine-tuned using a self-supervised learning approach and symbolic execution. Our experiments show that LLM-SE generates valid loop invariants more accurately and efficiently than the state-of-the-art baselines. Our LLM-SE accommodates various data structures, supports multi-loop scenarios, and simplifies the integration of new user-defined data structures. This makes our method a powerful alternative to traditional and reinforcement learning-based approaches. Our work highlights the potential of LLM in program verification and opens new avenues for research in this area.

  
**Data Structure** & **SLING** & **AutoSpec** & **GPT-4** & **LLM-SE (ours)** \\  Single linked list & 33.33\% & 0.00\% & 34.38\% & 61.46\% \\ Double linked list & 24.81\% & 0.00\% & 13.53\% & 32.33\% \\ Listbox & 0.00\% & 0.00\% & 6.67\% & 41.67\% \\ Others & 13.04\% & 0.00\% & 8.70\% & 30.43\% \\
**Total** & 21.79\% & 0.00\% & 17.68\% & 42.95\% \\   

Table 4: Pass rate @ 8 on different data structures for various methods.