# Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models

Simian Luo\({}^{1,2}\)   Chuanhao Yan\({}^{1}\)   Chenxu Hu\({}^{1}\)   Hang Zhao\({}^{1,2}\)

\({}^{1}\)IIIS, Tsinghua University  \({}^{2}\)Shanghai Qi Zhi Institute

{luosm22, yanch21, hu-cx21}@mails.tsinghua.edu.cn

hangzhao@mail.tsinghua.edu.cn

Corresponding Author.

###### Abstract

The Video-to-Audio (V2A) model has recently gained attention for its practical application in generating audio directly from silent videos, particularly in video/film production. However, previous methods in V2A have limited generation quality in terms of temporal synchronization and audio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio synthesis method with a latent diffusion model (LDM) that generates high-quality audio with improved synchronization and audio-visual relevance. We adopt contrastive audio-visual pretraining (CAVP) to learn more temporally and semantically aligned features, then train an LDM with CAVP-aligned visual features on spectrogram latent space. The CAVP-aligned features enable LDM to capture the subtler audio-visual correlation via a cross-attention module. We further significantly improve sample quality with 'double guidance'. Diff-Foley achieves state-of-the-art V2A performance on current large scale V2A dataset. Furthermore, we demonstrate Diff-Foley practical applicability and adaptability via customized downstream finetuning. Project Page: https://diff-foley.github.io/

## 1 Introduction

Recent advances in diffusion models [42; 45; 16] have accelerated the development of AI-generated contents, _e.g_. Text-to-Image (T2I) generation [37; 39; 38], Text-to-Audio (T2A) generation [48; 27], and Text-to-Video (T2V) generation . This paper focuses on Video-to-Audio (V2A) generation, which has practical applications in video/film production. During live shooting, due to the presence of excessive background noise and challenges in the audio collection in complex scenarios, the majority

Figure 1: Traditional Foley _v.s_ Neural Foley: Traditional Foley, a time-consuming and labor-intensive process involving skilled artists manipulating objects to create hours of physical sounds for sound recording . In contrast, Neural Foley presents an appealing alternative utilizing neural networks to synthesize high-quality synchronized audio, accelerating video production and alleviating human workload.

of the sounds recorded in the film need to be recreated during post-production. This process of adding synchronized and realistic sound effects to videos is known as _Foley_. We compare traditional Foley performed in the studio and our neural Foley in Figure 1. Traditional Foley is a laborious and time-consuming process, that involves skilled artists manipulating objects to create hours of authentic physical sounds. In contrast, Neural Foley offers an appealing alternative. High-quality synchronized audio generated by AI can greatly accelerate video production and alleviate the human workload. Different from Neural Dubber , which synthesizes speech from text scripts and video frames, Neural Foley focus on generating a broad range of audio solely based on the video content, a task that poses a considerably higher level of difficulty.

Video-based audio generation offers two natural advantages over text-based generation while performing Foley. First, T2A requires lots of hard-to-collect text-audio pairs for training, in contrast, audio-video pairs are readily available on the Internet, _e.g._ millions of new videos are uploaded to YouTube daily. Second, along with the semantics of the generated audio, V2A can further control the temporal synchronization between the Foley audio and video.

Semantic content matching and temporal synchronization are two major goals in V2A. While some progress [35; 50; 3; 21; 41] have been made recently on V2A, most methods of audio generation focus only on the content relevance, neglecting crucial aspect of audio-visual synchronization. For example, given a video of playing drums, existing methods can only generate drums sound, but cannot ensure the sounds match exactly with what's happening in the video, _i.e._ hitting the snare drum or the crash symbol at the right time.

RegNet  uses a pretrained (RGB+Flow) network as conditional inputs to GAN for synthesizing sounds. Meanwhile, SpecVQGAN  uses a Transformer  autoregressive model conditioned on pretrained ResNet50  or (RGB+Flow) visual features for better sample quality. These methods have limitations in generating audio that is both synchronized and relevant to video content as pretrained image and flow features cannot capture the nuanced correlation between audio and video.

We present Diff-Foley, a novel _Neural Foley_ framework based on Latent Diffusion Model (LDM)  that synthesizes realistic and synchronized audio with strong audio-visual relevance. Diff-Foley first learns temporally and semantically aligned features via CAVP. By maximizing the similarity of visual and audio features in the same video, it captures subtle audio-visual connections. Then, an LDM conditioned on CAVP visual features is trained on the spectral latent space. CAVP-aligned visual features help LDM in capturing audio-visual relationships. To further improve sample quality, we propose _'double guidance'_, using classifier-free and alignment classifier guidance jointly to guide the reverse process. Diff-Foley achieves state-of-the-art performance on large-scale V2A dataset VGGSound  with IS of 62.37, outperforming SpecVQGAN  baseline (IS of 30.01) by a large margin. We also demonstrate Diff-Foley practical applicability and adaptability via customized downstream finetuning, validating the potential of V2A pretrained generative models.

## 2 Related Work

**Video-to-Audio Generation** Generating audio from silent videos has potential in video production, which can greatly improve post-production efficiency. Despite recent progress, open-domain V2A remains a challenge. FoleyGAN  uses a class-conditioned GAN for synchronized audio synthesis, but it requires a specific action class label and is tested on a limited dataset. RegNet  extracts video features by using a pretrained (RGB+Flow) network and serves as a conditional input for GAN to synthesize sounds. SpecVQGAN  uses more powerful Transformer-based autoregressive models for sound synthesis with extracted ResNet50 or RGB+Flow features. Im2Wav , current state-of-the-art, adopts a two-transformer model, using CLIP  features as the condition, yet suffers from slow inference speed, requiring thousands of inference steps. Existing V2A methods struggle with synchronization as they lack audio-related information in pretrained visual features, limiting the capture of intricate audio-visual correlations. A notable subfield, video-to-speech, has gained attention with works like vid2speech , Neural Dubber . However, the foley task aims to generate complex and synchronized audio from video, presenting much higher challenge.

**Contrastive Pretraining** Contrastive pretraining [34; 13; 4] has shown potential in various generative tasks. CLIP aligns text-image representations using contrastive pretraining. For Text-to-Image, CLIP is integrated with Stable Diffusion  to extract text prompt features for realistic image generation. For Text-to-Audio, CLAP  aligns text and audio representations, and the resulting audio featuresare used in AudioLDM  for audio generation. Recognizing the importance of modality alignment in multimodal generation, we propose contrastive audio-visual pretraining (CAVP) to learn temporally and semantically aligned features initially and use them for subsequent generation tasks.

**Latent Diffusion Model** Diffusion Models [43; 16] have achieved remarkable success in various generative tasks including image generation [43; 16; 32; 37; 38], audio generation [25; 48; 27], and video generation [18; 15; 8]. Latent diffusion models, like Stable Diffusion (SD)  perform forward and reverse processes in data latent space, resulting in efficient computation and accelerated inference. SD is capable of running inference on personal laptop GPUs, making it a practical choice for a wide range of users.

## 3 Method

Diff-Foley consists of two stages: Stage1 CAVP and Stage2 LDM training. Our model overview is shown in Figure 2. Audio and visual components in a video exhibit strong correlation and complementarity. Unfortunately, existing image or optical flow backbones (ResNet, CLIP _etc._) struggle to reflect the strong alignment relationship between audio and visual. To address this, we propose Contrastive Audio-Visual Pretraining (CAVP) to align audio-visual features at the outset. Then, an LDM conditioned on CAVP-aligned visual features is trained on the spectral latent space.

### Contrastive Audio-Visual Pretraining

Given a audio-video pair \((x_{a},x_{v})\), where \(x_{a}^{T^{} M}\) is a Mel-Spec. with \(M\) mel basis and \(T^{}\) is the time axis. \(x_{v}^{T^{} 3 H W}\) is a video clip with \(T^{}\) frames. An audio encoder \(f_{A}()\) and a video encoder \(f_{V}()\) are used to extract audio feature \(E_{a}^{T C}\) and video feature \(E_{v}^{T C}\) with same temporal dim \(T\). We adopt the design of the audio encoder from PANNs , and SlowOnly  architecture for the video encoder. Using temporal pooling layer \(P()\), we obtain temporal-pooled audio/video features, \(_{a}=P(E_{a})^{C},_{v}=P(E_{v})^{C}\). We then use a similar contrastive objective in CLIP  to contrast \(_{a}\) and \(_{v}\). To improve the semantic and temporal alignment of audio-video features, we use two objectives: Semantic contrast \(_{}\) and Temporal contrast \(_{}\).

For \(_{}\), we maximize the similarity of audio-visual pairs from the same video and minimize the similarity of audio-visual pairs from different videos. It encourages learning semantic alignment for audio-visual pairs across different videos. In specific, we extract audio-visual features pairs from _different_ videos, \(_{}=\{(_{a}^{i},_{v}^{i})\}_{i=1}^{N_{S}}\), where \(N_{S}\) is the number of different videos. We define the per-sample pair semantic contrast objective: \(_{}^{(i,j)}\), where \(sim()\) is the cosine similarity.

\[_{}^{(i,j)}=-_{a} ^{i},_{v}^{j})/)}}{_{k=1}^{N_{S}}_{a}^{i}, {E}_{v}^{k})/)}}-_{a}^{i},_{ v}^{j})/)}}{_{k=1}^{N_{S}}_{a}^{k},_{v}^{j})/)}}\] (1)

For \(_{}\), we sample video clips at different times within the _same_ video. It aims to maximize the similarity of audio-visual pairs from the same time segment and minimize the similarity of audio-visual pairs across different time segments. In details, we sample different time segments in the

Figure 2: Overview of Diff-Foley: First, it learns more semantically and temporally aligned audio-visual features by CAVP, capturing the subtle connection between audio-visual modality. Second, a LDM conditioned on the aligned CAVP visual features is trained on the spectrogram latent space. Diff-Foley can synthesize highly synchronized audio with strong audio-visual relevance. \(()\) denotes temporal pooling layer.

_same_ video to extract audio-visual feature pairs. \(}=\{(_{a}^{i},_{v}^{i})\}_{i=1}^{N_{T}}\), where \(N_{T}\) is the number of sampled video clip within the same video. We define the per-sample pair temporal contrast objective:

\[}^{(i,j)}=-_{a}^{i},_{v}^{j})/)}}{_{k=1}^{N_{T}}_{a}^{k},_{v}^{k}) /)}}-_{a}^{i},_{v}^{j})/)} }{_{k=1}^{N_{T}}_{a}^{k},_{v}^{j})/)}}\] (2)

The final objective is the weighted sum of semantic and temporal objective: \(=}+}\), where \(=1\). After training, CAVP encodes an audio-video pair into embedding pair: \((x_{a},x_{v})(E_{a},E_{v})\) where \((E_{a},E_{v})\) are highly aligned, with the visual features \(E_{v}\) containing rich information for audio. The aligned and strongly correlated features \(E_{v}\) and \(E_{a}\) facilitate subsequent audio generation.

### LDM with Aligned Visual Representation

LDMs  are probabilistic models that fit the data distribution \(p(x)\) by denoising on data latent space. LDMs first encode the origin high-dim data \(x\) into low-dim latent \(z=(x)\) for efficient training. The forward and reverse process are performed in the compressed latent space. In V2A generation, our goal is to generate synchronized audio \(x_{a}\) given video clip \(x_{v}\). Using similar latent encoder \(_{}\) in , we compress Mel-Spec \(x_{a}\) into a low-dim latent \(z_{0}=_{}(x_{a})^{C^{} }\), where \(r\) is the compress rate. With the pretrained CAVP model to align audio-visual features, the visual features \(E_{v}\) contain rich audio-related information. This enables synthesis of highly synchronized and relevant audio using LDMs conditioned on \(E_{v}\). We adopt a projection and positional encoding layer \(_{}\) to project \(E_{v}\) to the appropriate dim. In the forward process, origin data distribution transforms into standard Gaussian distribution by adding noise gradually with a fixed schedule \(_{1},,_{T}\), where \(T\) is the total timesteps, and \(_{t}=_{i=1}^{t}_{i}\).

\[q(z_{t}|z_{t-1})=(z_{t};}z_{t-1},(1-_{t}) ), q(z_{t}|z_{0}) =(z_{t};_{t}}z_{0},(1-_{t }))\] (3)

The goal of LDM is to mirror score matching by optimizing the denoisng objective [46; 16; 44]:

\[_{LDM}=_{z_{0},t,}\|-_{}(z _{t},t,E_{v})\|_{2}^{2}\] (4)

After LDM is trained, we generate audio latent by sampling through the reverse process with \(z_{T}(0,)\), conditioned on the given visual-features \(E_{v}\), with the following reverse dynamics:

\[p_{}(z_{t-1}|z_{t})=(z_{t-1};_{}(z_{t},t,E_{v}), _{t}^{2})\] (5)

\[_{}(z_{t},t,E_{v})=}}(z_{t}-}{_{t}}}_{}(z_{t},t,E_{v})) ,_{t}^{2}=_{t-1}}{1-_{t}}(1- _{t})\] (6)

Finally, the Mel-Spec. \(_{a}\) is obtained by decoding the generated latent \(z_{0}\) with a decoder \(\), \(_{a}=(z_{0})\). In the case of Diff-Foley, it generates audio samples with a duration of 8 seconds.

### Temporal Split & Merge Augmentation

Using large-scale text-image pairs datasets like LAION-5B  is crucial for the success of current T2I models . However, for V2A generation task, large-scale and high-quality datasets are still lacking. Further, we expect V2A model to generate highly synchronized audio based on visual content, such temporal audio-visual correspondence requires a large amount of audio-visual pairs for training. To overcome this limitation, we propose using _Temporal Split & Merge Augmentation_, a MixUp  like augmentation strategy to facilitate model training by incorporating prior knowledge for temporal alignment into the training process. During training, we randomly extract video clips of different time lengths from two videos (_Split_), denoted as \((x_{a}^{1},x_{v}^{1}),(x_{a}^{2},x_{v}^{2})\), and extract visual features \(E_{v}^{1},E_{v}^{2}\) with pretrained CAVP model. We then create a new audio-visual feature pair for LDM training with:

\[z_{a}^{new}=_{}([x_{a}^{1};\ x_{a}^{2}]), E_{v}^{ new}=[E_{v}^{1};\ E_{v}^{2}],\] (7)

where \([;]\) represent temporal concatenation (_Merge_). Split and merge augmentation greatly increase the number of audio-visual pairs, preventing overfitting and facilitating LDM to learn temporal correspondence. We validate the effectiveness of this augmentation method in Sec 4.1.2.

### Double Guidance

Guidance techniques is widely used in diffusion model reverse process for controllable generation. There are currently two main types of guidance techniques: classifier guidance  (CG), and classifier-free guidance  (CFG). For CG, it additionally trains a classifier (e.g class-label classifier) to guide the reverse process at each timestep with gradient of class label loglikelihood \(_{x_{t}} p_{}(y|x_{t})\). For CFG, it does not require an additional classifier, instead it guides the reverse process by using linear combination of the conditional and unconditional score estimates , where the \(c\) is the condition and \(\) is the guidance scale. In stable diffusion , the CFG is implemented as:

\[}(z_{t},t,c)_{}(z_{t},t,c)+(1-)_{}(z_{t},t,).\] (8)

When \(=1\), CFG degenerates to conditional score estimates. Although CFG is currently the mainstream approach used in diffusion models, the CG method offers the advantage of being able to guide any desired property of the generated samples given true label. In V2A setting, the desired property refers to semantic and temporal alignment. Moreover, we discover that these two methods are not mutually exclusive. We propose a _double guidance_ technique that leverages the advantages of both CFG and CG methods by using them simultaneously at each timestep in the reverse process. In specific, for CG we train an alignment classifier \(P_{}(y|z_{t},t,E_{v})\) that predicts whether an audio-visual pair is a real pair in terms of semantic and temporal alignment. For CFG, during training, we randomly drop condition \(E_{v}\) with prob. 20%, to train conditional and unconditional likelihood \(_{}(z_{t},t,E_{v}),_{}(z_{t},t,)\). Then _double guidance_ is achieved by improved noise estimation:

\[_{}(z_{t},t,E_{v})_{}(z_{t}, t,E_{v})+(1-)_{}(z_{t},t,)-}}_{z_{t}} P_{}(y|z_{t},t,E_{v}),\] (9)

where \(,\) refer to CFG, CG guidance scale respectively. We provide further analysis of the intuition and mechanism behind _double guidance_ in Appendix C.

## 4 Experiments

**Datasets** We use two datasets VGGSound  and AudioSet . VGGSound consists of \(\)200K 10-second videos. We follow the original VGGSound train/test splits. AudioSet comprises 2.1M videos with 527 sound classes, but it is highly imbalanced, with most of the videos labeled as Music and Speech. Since generating meaningful speech directly from video is not expected in V2A tasks (not necessary either), we download a subset of the Music tag data and all other tags except Speech, resulting in a new dataset named AudioSet-V2A with about 80K music-tagged videos and 310K

   &  &  &  &  &  \\   & & & & & TS \(\) & FID \(\) & KL \(\)Acc (\%) \(\) \\   SpecVQGAN  & RGB + Flow & 21.5 & ✘ & 30.01 & **8.93** & 6.93 & 52.94 & 5.47s \\ SpecVQGAN  & ResNet50 & 21.5 & ✘ & 30.80 & 9.70 & 7.03 & 49.19 & 5.47s \\ Im2Wav  & CLIP & 30 & CFG (✔) & 39.30 & 11.44 & **5.20** & 67.40 & 6.41s \\ Diff-Foley (Ours) & CAVP & **4** & CFG (✔) & 53.34 & 11.22 & 6.36 & 92.67 & **0.38s** \\ Diff-Foley (Ours) & CAVP & **4** & Double (✔) & **62.37** & 9.87 & 6.43 & **94.05** & **0.38s** \\  

Table 1: Video-to-Audio generation evaluation results with CFG scale \(=4.5\), CG scale \(=50\), using DPM-Solver  Sampler with 25 inference steps. Diff-Foley achieves impressive temporal synchronization and audio-visual relevance (Align Acc) with only using 4 FPS video, compared with baseline method using 21.5/30 FPS. CFG represents Classifier-Free Guidance, Double represents _Double Guidance_, and ACC refers to Align Acc. Infer. Time denotes the average inference time per sample when generating 64 samples in a batch.

   &  &  &  &  \\   & & & & Content Relevance & Synchronization \\   SpecVQGAN  & ResNet50 & 21.5 & ✘ & 46.20 & 45.20 \\ Im2wav  & CLIP & 30 & CFG (✔) & 62.13 & 57.73 \\ Diff-Foley (Ours) & CAVP & **4** & CFG (✔) & 71.73 & 71.00 \\ Diff-Foley (Ours) & CAVP & **4** & Double (✔) & **74.53** & **74.93** \\  Ground Truth & & & & 84.80 & 84.20 \\  

Table 2: Video-to-Audio generation human evaluation results with CFG scale \(=4.5\), CG scale \(=50\). Raters are required to score the generated audio based on content relevance and synchronization. Diff-Foley with double guidance shows superiority performance compared with baseline methods.

other tagged videos. We use VGGSound and AudioSet-V2A for Stage1 CAVP, while for Stage2 LDM training and evaluation, we only use VGGSound, which is consistent with the baseline.

**Data Pre-Processing** For Diff-foley training, videos in VGGSound and AudioSet-V2A are sampled at 4 FPS, which already yields significantly better alignment results compared to the baseline method [21; 41] using 21.5 or 30 FPS. Each 10-second video sample consists of 40 frames are resized to \(224 224\). For audio data, the original audio is sampled at 16kHz and transformed into Mel-spectrograms (Mel Basis \(M=128\)). For Stage1 Pretraining, we use hop size 250 for better audio-visual data temporal dimension alignment, while we use hop size 256 for Stage2 LDM training.

**Model Configuration and Inference** For CAVP, we use a pretrained audio encoder from PANNs  and a pretrained SlowOnly  based video encoder. For training, we randomly extract 4-second audio-video frames pairs from 10-second samples, resulting in \(x_{a}^{256 128}\) and \(x_{v}^{16 3 224 224}\). We use temporal contrast \(_{}\) with \(N_{T}=3\) and a minimum time difference of 2 seconds between each pair. For LDM training, we utilize the pretrained Stable Diffusion-V1.4 (SD-V1.4)  as a powerful denoising prior model. Leveraging the pretrained SD-V1.4 greatly reduces training time and improves generation quality (discussed in Sec 4.3). Frozen pretrained latent encoder \(\) and decoder \(\) from SD-V1.4 are used. Interestingly, despite being trained on image datasets (LAION-SB), we found \(\), \(\) can encode and decode Mel-Spec well. For inference, we use DPM-Solver  sampler with 25 sampling steps, unless otherwise stated. More training details for each model are in Appendix. A.

**Evaluation Metrics** For evaluation, we use Inception Score (IS), Frechet Distance (FID) and Mean KL Divergence (MKL) from . IS assesses sample quality and diversity, FID measures distribution-level similarity, and MKL measures paired sample-level similarity. We introduce Alignment Accuracy (Align Acc) as a new metric to assess synchronization and audio-visual relevance. We train an alignment classifier to predict real audio-visual pairs. To train the classifier, we use three types of pairs: \(50\%\) of the pairs are real audio-visual pairs (_true pair_) labeled as 1, \(25\%\) are audio-visual pairs from the same video but temporally shifted (_temporal shift pair_) labeled as 0, and the last \(25\%\) are audio-visual pairs from different videos (_wrong pair_) labeled as 0. Our alignment classifier reaches \(88.31\%\) accuracy on test set. The detailed analysis of alignment classifier is provided in Appendix. A.1. We prioritize IS and Align Acc as the primary metrics to evaluate sample quality. For evaluation, we generate \(\)145K audio samples (10 samples per video in the test set). We emphasize the difference between the noisy alignment classifier \(F_{}^{DG}\) used for double guidance techniques and the alignment classifier \(F_{}^{sync}\) used for Align Acc metric evaluation in Appendix A.1.

Figure 3: Video-to-Audio generation results on VGGSound: Given a silent playing golf video, only the Diff-Foley successfully generates the corresponding hitting sound at the 4th second timestamp, showcasing its remarkable ability to generate synchronized audio based on video content.

**Baseline** For comparison, we use two current state-of-the-art V2A models: SpecVQGAN  and Im2Wav . SpecVQGAN offers two model settings based on different visual features (RGB+Flow and ResNet50), while Im2Wav generates semantically relevant audios using CLIP features. We use the pretrained baseline models for evaluation, which were both trained on VGGSound.

### Video-to-Audio Generation Results

Table 1 presents the **quantitative** results on VGGSound test set and model inference time. Diff-Foley outperforms the baseline method significantly in IS and Align Acc (primary metrics), while maintaining comparable performance on MKL/FID. Diff-Foley achieves twice the performance of baseline on IS (_62.37 v.s \(\)30_) and an impressive 94.05% Align Acc. Im2Wav's slow inference speed hinders its practicality despite its advantages in KL metrics. In contrast, Diff-Foley utilizes the state-of-the-art diffusion sampler DPM-Solver , generating 64 samples per batch at an average of 0.38 seconds per sample with 25 inference steps (compared to Im2Wav 8192 autoregressive steps). We included the KL and FID metrics for comprehensive analysis despite their unreliability and potential misalignment with human perceptions. The suboptimal results on these metrics might be due to using the pretrained autoencoder in SD-v1.4 , leading to reconstruction errors. See Appendix. D for more details. We conducted a **human evaluation** in Table 2. The generated audios are rated based on content relevance and synchronization on a scale of 1 (bad) to 5 (excellent), then multiplied by 20. The detailed human evaluation procedure is provided in Appendix. A.4. The results in Table 2 highlight Diff-Foley's excellence in audio-visual synchronization and relevance. We also observe that content relevance and synchronization assessments from our classifier closely align with those from human evaluators, confirming the effectiveness of our sync classifier.

Figure 3 presents the **qualitative** results. Diff-Foley demonstrates a remarkable ability to generate highly synchronized audio compared with baseline methods. Given a silent golf video, Diff-Foley successfully generates the corresponding hitting sound at the 4th second, while baseline methods fail to generate sound at that specific second. This demonstrates the superior synchronization ability of Diff-Foley. We emphasize that our aim is to generate audios that align well with human perception, even if they differ from the ground truth audio. More generated results can be found at this link.2.

#### 4.1.1 Visual Features Analysis

CAVP uses semantic and temporal contrast objectives to align audio and visual features. We evaluate the effectiveness of CAVP visual features by comparing them with other visual features in LDM training. Results in Table 3 show that CAVP visual features significantly improve synchronization and audio-visual relevance (Align Acc), verifying the effectiveness of CAVP features. Despite CLIP

Figure 4: Visualization of generated sample with different visual features: It can be see that while other visual features fail to generate synchronized audio based on drum video, CAVP visual features successfully generate drum sound with 4 clear spikes (c.), matching the ground truth spectrogram (a.).

features exhibiting advantages in IS and KL, it does not accurately reflect synchronization capability. We expect such a gap can be bridged by expanding CAVP datasets to a larger scale. In the drumming video example shown in Figure 4, different visual features are utilized to generate audio. In this example, the drum is hit four times (clear spikes in the ground truth, see a.), while ResNet and CLIP features fail to generate synchronized audio, CAVP features successfully learn the relationship between drumming moments and drum sounds, generating audio with 4 clear spikes (see c.), showing excellent synchronization capability.

#### 4.1.2 Temporal Augmentation Analysis

We evaluate the impact of _Temporal Split & Merge Augmentation_ (discussed in Sec 3.3) in Table 4. Our results demonstrate improvements across all metrics using _Temporal Split & Merge Augmentation_, particularly for Align Acc metric. This augmentation technique leverages prior knowledge of audio-visual temporal alignment, enhancing the model's synchronization capability and increasing the number of audio-visual training pairs, thus mitigating overfitting.

   &  &  \\   & & IS \(\) & FID \(\) & KL \(\) & Align Acc (\%)\(\) \\    & ✗ & 50.62 & 12.65 & 6.38 & 79.79 \\  & ✓ & **52.07** & **11.61** & **6.33** & **92.35** \\  

Table 4: Evaluation results _w./w.o Temporal Split & Merge Augmentation_, using only CFG with \(=4.5\) and DDIM  Sampler (250 steps). Temporal Aug. refers to _Temporal Split & Merge Augmentation_ in Sec 3.3.

Figure 5: Downstream finetuning results on EPIC-Kitchens: Diff-Foley achieves highly synchronized audio generation with the original video. The generated sounds closely match the ground truth, especially in terms of timing, such as knife cutting, water flow, and plate clinking (refer to Generated Audio and Ground Truth Audio).

### Downstream Finetuning

**Background** Foundation generative models like Stable Diffusion , trained on billions of data, excel at creating realistic images, but struggle with generating specific styles or personalized images. Finetuning techniques help overcome such limitations, aiding in specific-style synthesis and enhancing adaptability. Similarly, Diff-Foley is expected to adapt to other datasets via downstream finetuning to synthesize specific types of sound, as evaluated in this section.

**Finetuning Details** We finetuned Diff-Foley on EPIC-Kitchens , a high quality egocentric video dataset(\(\)100 hours) about object interactions in kitchen with minimal sound noises. EPIC-Kitchens is notably distinct from VGGSound. This experiment validates the adaptability of finetuned Diff-Foley. We used a pretrained CAVP model to extract visual features, and then finetuned Stage2 LDM (trained on VGGSound) for 200 epochs. See more finetuning details in Appendix. A.

**Generation Results** We show qualitative results of finetuned LDM on EPIC-Kitchens in Figure 5. Diff-Foley generates highly synchronized audio with original video, effectively capturing the timing of sounds, knife cutting, water flow, and plate climbing. Best viewed in demo link 2.

### Ablation Study

**Model Size & Param Init.** We explore the impact of model size and initialization with pretrained Stable Diffusion-V1.4 (SD-V1.4) on Diff-Foley. We trained three models of varying sizes: Diff-Foley-S (335M Param.), Diff-Foley-M (553M Param.), and Diff-Foley-L (859M Param., the same architecture as SD-V1.4). We report detailed architectures of these models in Appendix. A Our results in Table 5 show that increasing model size improves performance across all metrics,

   &  &  &  &  \\   & & & & IS \(\) & FID \(\) & KL \(\) & Align Acc (\%)\(\) \\    & VGGSound & ✘ & 19.86 & 18.45 & 6.41 & 67.59 \\  & VGGSound & ✘ & 16.83 & 20.20 & 6.81 & 62.24 \\ Diff-Foley (Ours) & VGGSound & ✘ & 51.42 & 11.48 & 6.48 & 85.88 \\  & VGGSound & ✘ & **✔** & 53.45 & **10.67** & 6.54 & 89.08 \\  & VGGSound + AudioSet-V2A & ✘ & **✘** & 22.07 & 18.20 & 6.52 & 69.41 \\  & VGGSound + AudioSet-V2A & ✘ & 17.57 & 20.87 & 6.69 & 66.05 \\  & VGGSound + AudioSet-V2A & ✘ & 52.07 & 11.61 & **6.33** & 92.35 \\  & VGGSound + AudioSet-V2A & ✘ & **✔** & **60.39** & 10.73 & 6.42 & **94.78** \\  

Table 6: Ablation Study: Evaluating the impact of Stage1 CAVP pretrained datasets scale and guidance techniques with CFG scale \(=4.5\), and CG scale \(=50\), using DDIM  Sampler with 250 inference steps.

Figure 6: **CFG Scale & Different Samplers: Enhancing CFG scale \(\) improves sample quality up to a point, beyond which it decreases, creating U-shaped curves in (a). DDMM, PLMS, DPM-Solver samplers are compared in (b). DPM-Solver, converging in only 25 steps, greatly improving Diff-Foley inference speed and its accessibility. For fast evaluation, here we only generate 1 audio sample per video in the test set.**demonstrating Diff-Foley's scaling effect. Moreover, initializing with SD-V1.4 weights (pretrained on billions of images), significantly improves performance and reduces training time due to its strong denoising prior in image/Mel-spectrogram latent space (see the last row in Table 5).

**Guidance Techniques & Pretrained Dataset Scale** We conduct extensive ablation study on Diff-Foley in Table 6, exploring the scale of Stage1 CAVP datasets and guidance techniques. We see that: 1). More CAVP pretraining data enhances downstream LDM generation performance. 2). Guidance techniques greatly improve all metrics except KL 3). _Double guidance_ techniques achieve the best performance on IS and Align Acc (the primary metrics).

**CFG Scale & Different Sampler** We studied the impact of CFG scale \(\) and different samplers on Diff-Foley. CFG scale influences the trade-off between sample quality and diversity. A large CFG scale typically enhances sample quality but may have a negative effect on diversity (FID, KL). Beyond a certain threshold, increased \(\) can decrease quality due to sample distortion, resulting in _U-Shaped_ curves in Figure 6 (a). Best results in IS and Align Acc were achieved at \(=4.5\), with minor compromise in FID and KL. _Faster Sampling_ for diffusion models, reducing inference steps from thousands to tens, is a trending research area. We evaluated the usability of our Diff-Foley using current accelerated diffusion samplers. We compare three different samplers, DDIM , PLMS , DPM-Solver  with different inference steps in Figure 6 (b). All three samplers converge at 250 steps, with DPM-Solver converging in only 25 inference steps, while others require more steps to converge. DPM-Solver allows for faster Diff-Foley inference, making it more accessible.

## 5 Limitations and Broader Impact

**Limitations** Diff-Foley has shown great audio-visual synchronization on VGGSound and EPIC-Kitchens, however its scalability on super large (billion-scale) datasets remains untested due to limited data computation resources. Diffusion models are also slower than GANs due to the iterative reverse sampling process.

**Broader Impact** The advancements in V2A models, such as Diff-Foley, have the potential to significantly expedite video production processes, offering efficiency gains in the entertainment and media sectors. However, as with most powerful technologies, there's a flip side. There's an underlying risk of these models being misused to create misleading or false content. As such, while the potential benefits are vast, it is imperative for developers, users, and regulators to exercise caution.

## 6 Conclusion

We introduce Diff-Foley, a V2A approach for generating highly synchronized audio with strong audio-visual relevance. We empirically demonstrate the superiority of our method in terms of generation quality. Moreover, we show that using _double guidance_ technique to guide the reverse process in LDM can further improve the audio-visual alignment of generated audio samples. We demonstrate Diff-Foley practical applicability and adaptability via customized downstream finetuning. Finally, we conduct an ablation study, analyzing the effect of pretrained dataset size, various guidance techniques, and different diffusion samplers.