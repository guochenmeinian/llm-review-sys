# Value Imprint: A Technique for Auditing the Human Values Embedded in RLHF Datasets

Ike Obi

Purdue University

West Lafayette, Indiana

&Rohan Pant

Purdue University

West Lafayette, Indiana

&Srishti Shekhar Agrawal

Purdue University

West Lafayette, Indiana

&Maham Ghazanfar

Purdue University

West Lafayette, Indiana

&Aaron Basiletti

Purdue University

West Lafayette, Indiana

###### Abstract

LLMs are increasingly fine-tuned using RLHF datasets to align them with human preferences and values. However, very limited research has investigated which specific human values are operationalized through these datasets. In this paper, we introduce _Value Imprint_, a framework for auditing and classifying the human values embedded within RLHF datasets. To investigate the viability of this framework, we conducted three case study experiments by auditing the _Anthropic/hh-rllf, OpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM_ datasets to examine the human values embedded within them. Our analysis involved a two-phase process. During the first phase, we developed a taxonomy of human values through an integrated review of prior works from philosophy, axiology, and ethics. Then, we applied this taxonomy to annotate 6,501 RLHF preferences. During the second phase, we employed the labels generated from the annotation as ground truth data for training a transformer-based machine learning model to audit and classify the three RLHF datasets. Through this approach, we discovered that information-utility values, including Wisdom/Knowledge and Information Seeking, were the most dominant human values within all three RLHF datasets. In contrast, prosocial and democratic values, including Well-being, Justice, and Human/Animal Rights, were the least represented human values. These findings have significant implications for developing language models that align with societal values and norms. We contribute our datasets to support further research in this area. [https://github.com/hv-rsrch/valueimprint](https://github.com/hv-rsrch/valueimprint)

## 1 Introduction

Reinforcement Learning From Human Feedback (RLHF) has emerged as a potent way of shaping the behavior of AI models to ensure they produce positive responses and experiences that correspond with user preferences and societal norms . On one hand, several AI researchers have touted the efficacy of this approach as a proxy for embedding human values and preferences into AI models, resulting in its use in different domains, including the finetuning of LLMs , vision models , and multi-modal systems . Several users of these AI systems, on the other hand, are raising concerns about the censorship and anti-democratic stance of models trained with these preferences, highlighting that they are marginalized against their value systems while allowing others . As a result, there is a growing concern among members of the public around the lack of transparency in the kinds of values these datasets embed into AI systems. In addition, considering that RLHF preferences involve complex value judgments of annotators, it is crucial to investigate how the subjective valuesand preferences of annotators - both human and AI - are embedded within these datasets in ways that might misalign with societal values and norms.

In this paper, we introduce _Value Imprint_, a novel technique for auditing and classifying the human values embedded within RLHF datasets. To support this approach, we created a human values taxonomy by conducting an integrated literature review of prior bodies of work from philosophy, axiology, and STS (Science, Technology, and Society) and, through a thematic analysis of these bodies of work, developed a taxonomy of human values to support our audit. Using this taxonomy, we conducted a two-phase audit analysis, with each step building on the result from the previous stage. During the first phase, we employed the taxonomy to qualitatively annotate 6,501 RLHF preferences. During the second phase, we employed the labels derived from the qualitative annotation process as ground truth data. This data was then utilized to a train transformer-based machine learning model, which we subsequently deployed for auditing and classifying the complete _Anthropic/hh-rlhf, OpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM_ datasets. We further conducted a human evaluation of a section of the classification output to examine their performance. We followed the evaluation with an additional round of analysis to examine how the values embedded within the three RLHF datasets differ. Through these approaches, we answered our research questions which included:

1. **RQ1:** What kinds of human values are embedded within RLHF preferences?
2. **RQ2:** In what ways do the human values embedded within the _Anthropic/hh-rlhf, OpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM_ datasets differ?

Findings from our research revealed that the most dominant values within the ground truth RLHF preferences were Information Seeking and Wisdom/Knowledge. In contrast, the least represented values were Civility & Tolerance, Empathy & Helpfulness, Justice & Human Rights/Animal Rights, and Well-being & Peace. The findings also revealed instances of unethical responses selected as suitable preferences for training machine learning models. Furthermore, the machine learning classification of human values produced an accuracy score range of 80% for the model we used for this analysis. This demonstrates the viability of AI researchers and practitioners adopting this process to interrogate the human values embedded within RLHF datasets to foreground their value orientation and how they might lead to different societal impacts.

Above all, through this research, we make the following contributions: 1) we introduce a technique for auditing and classifying the underlying human values embedded within RLHF preferences, providing AI researchers with a technique for auditing and interrogating the quality of RLHF datasets, 2) we conduct three case study experiments using this approach and through our findings reveal that Wisdom/Knowledge and Information Seeking were the most dominant human values within the datasets; validating our technique. 3) We contribute both our ground truth annotation and classification datasets and, through this means, provide researchers with the pathway to take this work forward.

In the sections that follow, we situate our work within broader research on language models, data quality, and embedding human values into LLMs. We then describe our methods and report our findings. We conclude with a discussion of the implications of these findings and provide suggestions for future work.

## 2 Background

### Embedding Human Values into LLMs and AI Systems

AI researchers and computer scientists are increasingly interested in embedding human values into LLMs, AI, and robotic systems. This increased interest is motivated by several reasons, including the need to move beyond just optimizing for metrics like efficiency and performance towards aligning these systems with prosocial values like democracy, transparency, freedom of expression, and human rights . It also includes the need to ensure that technology systems do not make users vulnerable or cause them harm . To achieve these objectives, AI researchers are increasingly developing sociotechnical approaches for encoding societal values into AI systems, using different techniques such as value-oriented datasets as can be found in the works of  and formalized ethical frameworks as can be found in the works of , among other design techniques .

Solaiman & Dennison  introduced an approach for aligning AI models with human values by using value-oriented datasets to finetune AI models. Findings from their research revealed that their approach improved adherence to human values and reduced toxicity without affecting model performance. Nahian et al.  investigated approaches for using stories to encode societal norms into machine learning systems and found that this technique can complement other approaches for introducing human values into machine learning systems. Ammanabrolu et al.  explored the approach of imbuing agents with common sense knowledge to ensure that such systems align with socially beneficial human values. Findings from their study revealed that this approach reduced the ability of the agent to engage in harmful behaviors that misalign with human values by 25%. Sorensen et al.  also explored approaches for enabling value pluralism in machine learning systems. They introduced the ValuePrism dataset as a means of fostering this genre of research. Szabo et al.  investigated approaches for fusing quantitative and qualitative-based reasoning as a means of ensuring value alignment in machine learning systems. Relevant here is that computer scientists and AI researchers are increasingly exploring technical, conceptual, and philosophical approaches for embedding human values into LLMs, AI, and algorithmic systems.

However, although numerous AI researchers have examined several approaches for embedding human values into AI models, there is currently a lack of techniques and methods that allow researchers to systematically foreground and thoroughly investigate the specific types of human values being integrated into LLMs and AI models using non-ethics curated datasets. And specifically, very limited attention has been paid to examining the human values embedded within RLHF datasets. Prabhakaran et al.  highlighted that AI researchers often assume that the values they embed into AI systems serve the best interest of society, even though there is no empirical data or justification that supports their approach and belief. If anything, it is the opposite [28; 29]. Hendrycks et al. [30; 16] identified a lack of alignment with hard-to-specify human values as one of the unsolved safety-related problems in the field of machine learning. Given that the objective of RLHF is to embed human values and preferences into AI models and considering that, at present, there is no method for auditing and measuring the human values embedded within RLHF datasets, this paper focuses on introducing a technique for interrogating these RLHF datasets as a means of providing insights into the kinds of human values embedded within them.

### Data Quality and Language Models

There is a large and growing body of work at the intersection of data quality and language models [31; 32; 33]. These works have examined issues relating to AI datasets from numerous perspectives, including issues of representation harms and demographic bias that propagate harmful stereotypes from defective datasets into AI models, issues of toxicity/harmful content that perpetuate misogyny and racial slurs, lack of transparency and accountability around how datasets are collected, annotated, cleaned or versioned over time which hampers accountability and attribution, among many other issues at the intersection of data quality and language models.

Hirota et al.  investigated the issues of gender and racial bias in five visual question-answering datasets. Findings from their research revealed instances of gender disparity and racial stereotypes that favor males and Western cultures, respectively. They proposed approaches that researchers could adopt to mitigate these biases. Garcia et al.  annotated and audited the Google Captions vision and language model datasets to investigate instances of bias. Findings from their research showed an over-representation of males and persons with lighter skin tones compared to other users from other demographics. Dhamala et al.  introduced a large-scale benchmarking dataset to allow researchers to measure bias in language models across different dimensions, including race, religion, and gender. Through this approach, they aim to induce transparency in reporting toxicity within language models. Papakyriakopoulos et al.  investigated the lack of diversity in speech datasets across different dimensions, including accent, dialect, and speech impairment. Findings from their research revealed that the absence of intentional structure plays a role in this lack of diversity. To resolve this, they introduced speech datasheets to foster ethical data collection practices around speech datasets. Pushkarna et al.  introduced data cards to document the provenance and ethical implications of using multi-modal datasets. Luccioni et al.  introduced a dataset deprecation framework as a means of ensuring proper documentation for datasets that are deprecated and retired from circulation.

Although numerous scholars have extensively audited AI and machine learning datasets, very limited work has focused on examining RLHF datasets to foreground the human values embedded within them. In a small body of work in this area, Hendrycks et al.  introduced the ETHICS Datasetto foster the measurement of the ethical judgment of language models. Birhane et al.  also introduced a technique for annotating the values embedded within machine learning research papers; however, did not focus on examining RLHF datasets. Obi & Gray  examined values engineers embedded into AI systems through their technical judgment but did not examine the values embedded in AI datasets. This limited work that has examined the human values within RLHF datasets motivates the need for further research in this area to support a more transparent RLHF process.

## 3 Method

### Experiment Dataset

We collected the datasets for this research from different developer collaboration platforms. We collected the _Anthropic/hh-rhlf_ dataset from Hugging Face, an open source machine learning platform that provides datasets, models, and other computational resources for AI practitioners and researchers. The _Anthropic/hh-rhlf_ dataset (train - 161k rows, test - 8.55k rows) has been downloaded at least 109,200 times, used to train or fine-tune more than 156 AI models. Our analysis focused on both the chosen and rejected columns of the data. We merged the train and test sections of the _Anthropic/hh-rhlf_ dataset into one dataset corpus for analysis. We also collected the _OpenAI WebGPT Comparisons_ dataset from the Hugging Face library and focused our case study experiment on the content of the _question_ and _answer_0_ columns. We created a function to extract only the _full_text_ from the _question_ column and dropped the non-essential metadata, including _triviaqa, dataset_, and _id_. Next, we concatenated the content of the updated _question_ and _answer_0_ columns into a new column to form a complete preference unit. We then used our model to classify these preferences and examined the human values embedded within them. We fetched the _Alpaca GPT-4-LLM_ dataset from the GitHub repository dedicated to the project. Next, we concatenated the _instruction_ and _output_ columns from the original dataset into a new combined column, creating a complete human preference conversation. We reduced the DataFrame to contain only this new combined column and then conducted our case study classification analysis. See (Fig. 1) for our research process flow.

### Human Value Taxonomy

We constructed a taxonomy of human values through an integrated literature review grounded in prior bodies of work from moral philosophy, axiology, and STS (Science, Technology, and Society). Specifically, our literature search focused on nine journal databases within human values-related disciplines, including the Journal of Value Inquiry; Axiomathes; The Journal of Ethics; Nois; Ethics; The Philosophical Review; Science, Technology, & Human Values; Utilitas; and The Journal of Philosophy. Our search keyword for querying these databases was: "human value." No date restrictions were made on our search of these databases.

We followed a three-stage process to construct a taxonomy of human values using the curated research papers. In the first stage, we assigned each curated paper a human value based on the central theme discussed in the paper. Next, we categorized papers with similar values into semantically coherent hierarchical categories using a bottom-up approach, such as grouping papers about peace, security, and well-being under an overarching well-being and peace category. Second, we conducted

Figure 1: **Value Imprint** is a technique for auditing the human values embedded within RLHF datasets using an AI-focused human values taxonomy.

a qualitative review examining hypernym-hyponym relationships of our categories from the first stage to ensure subordinate values maintained an "is a part of" relationship within each category (e.g., duty/accountability containing non-maleficence and trustworthiness). Third, we conducted an additional review to verify that all values within each group reasonably belonged to the same ethical paradigm, though not intended to be sacrosanct, such as wisdom and knowledge aligning with virtue ethics. This approach allowed us to create a semantically coherent and ethically balanced human values taxonomy for our analysis. We made the hierarchical taxonomy such that other related sub-values not covered in this paper can reasonably fit within the different high-level value categories. We provide in-depth information about the taxonomy in Table 2 and in Appendix 5.3.

### Data Annotation

Using the human values taxonomy as our codebook, we qualitatively annotated sampled 6,501 preferences from the _Anthropic/hh-rllf_ dataset to examine the human values embedded in them. The qualitative annotations were performed by a team of 5 researchers with interdisciplinary expertise spanning Ethics, Computing, and HCI. The nationality of the annotators included India, USA, Nigeria, and Pakistan. Before coding all the 6,501 preferences, we held several rounds of extensive discussions and exploratory coding activities. These activities allowed us to engage with the dataset to better understand the dimensions of human values, their differences, and similarities and to establish a protocol for resolving any discrepancies and challenges that might arise during the main annotation session. Following this exploration, we conducted an inter-annotator agreement assessment by having all the annotators independently code the same 200 preferences and then compared the codes assigned by each annotator to the same preferences to assess the level of agreement between all the annotators. We achieved an inter-annotator agreement score of 0.85 using Krippendorff's Alpha score. Through this approach, we confirmed that multiple coders can consistently annotate and apply the same labels to the same RLHF preferences once they understand the human values taxonomy. We then commenced our main annotation session. Other infrequent discrepancies during our main annotation phase were resolved through discussions, codebook refinement, or reconciliation by a third annotator.

Figure 2: This image presents a visual version of the taxonomy that supported our audit. [See Table 2 and Appendix 5.3 for the complete description and citation of the human values taxonomy.]

### Human Values Classification

#### 3.4.1 Problem Formulation

To formally frame the task of computationally auditing the human values embedded within RLHF datasets, we modeled it as a multi-class classification problem over a vector space of human values. We define as follows: Let \(V=\{v_{1},v_{2},,v_{n}\}\) be the set of all possible human value labels, where \(n\) is the number of distinct human value classes. We define a dataset \(D=\{(x_{i},y_{i})\}_{i=1}^{m}\), where \(x_{i} X\) is an RLHF preference instance (text), \(y_{i} V\) is the corresponding human value label associated with \(x_{i}\), and \(m\) is the total number of instances. We split \(D\) into disjoint train and test sets: \(D_{}\) and \(D_{}\). We use a tokenizer \(T:X^{d l}\) to convert each text instance \(x_{i}\) into a numerical token representation \(T(x_{i})^{d l}\), where \(d\) is the embedding dimension, and \(l\) is the sequence length. We define a multi-class classification model \(f_{}:^{d l}^{n}\), where \(\) are the trainable parameters of the model (RoBERTaForSequenceClassification), and \(\) is the parameter space. We use a cross-entropy loss function \(L:V^{n}\) to measure the discrepancy between the predicted and true labels for each instance \((T(x_{i}),y_{i})\) in the training set: \(L(y_{i},f_{}(T(x_{i})))\). We also incorporated class weights \(w=(w_{1},w_{2},,w_{n})^{n}\) to handle class imbalance, computed using compute_class_weight from scikit-learn. We further optimize the model parameters \(\) by minimizing the weighted cross-entropy loss over the training set:

\[_{}}|}_{(T(x_{i}),y_{i}) D_ {}}w_{y_{i}} L(y_{i},f_{}(T(x_{i}))).\]

We used regularization (dropout), warm-up steps, and weight decay during training to improve generalization and prevent overfitting. We then used the trained model \(f_{}\) to make predictions on \(D_{}\): \(y_{,i}=_{v_{i} V}f_{}(T(x_{i}))_{j}\), where \(y_{,i}\) is the predicted human value label for the input instance \(x_{i}\). We further evaluated model performance on \(D_{}\) using metrics like accuracy and F1-score, with weighted averages to account for class imbalance.

#### 3.4.2 Value Classification

Using the annotated ground truth dataset, we trained a RoBERTa model for the multi-class classification of the RLHF datasets. We split the training data into 80% train and 20% test set using sklearn's train_test_split. We trained the model for 8 epochs with a batch size of 64 using Hugging Face Trainer. We used CrossEntropy loss for the classification task. We further enabled early stopping to prevent overfitting, with the training stopping early if the validation loss does not improve for 2 epochs. We saved the model checkpoints from the best validation loss. The hyperparameters included Max sequence length - 128, Batch size - 64, Epoch - 8, and Early stopping patience -2 epochs. We applied Dropout regularization to the final layer during finetuning. We also computed class weights to handle class imbalance and used weighted random sampling for the training batches. We then employed the trained RoBERTa model for classifying the human values embedded within the _Anthropic/hh-rlhf (338,704), OpenAI WebGPT Comparisons (19,578), and Alpaca GPT-4-LLM (52,002)_ datasets. Following the value classification activities, we conducted a human evaluation of 500 classification results, which showed that the models predicted the correct human value 84% of the time. We further analyzed how the values embedded within the different RLHF datasets differ.

## 4 Findings

### RQ1: What Kinds of Human Values are Embedded within RLHF Preferences?

#### 4.1.1 Results from Qualitative Annotation

Findings from our analysis of the 6,501 ground truth preferences from the _Anthropic/hh-rlhf_ dataset revealed that the most dominant human values were _Information Seeking_ for a specific use case (36.96%), _Wisdom/Knowledge_ for personal enlightenment and edification (30.75%), and _Duty & Accountability_ (9.52%). The least represented human values within the dataset were _Civility & Tolerance_ (7.61%), _Empathy and Helpfulness_ (6.09%), _Well-being & Peace_ (5.94%), and _Justice, Human & Animal Rights_ (3.12%). We characterize results from this analysis below and in (Table 1).

1. Information Seeking:Results from our analysis revealed that Information Seeking (36.96%; 2403 out of 6501) was the most dominant human value that was operationalized in the ground truth dataset. The dimensions of Information Seeking represented in the dataset included personal, professional, navigational, and practical information needs. The distinguishing feature between the Information Seeking human value from all other underlying human values was in their level of specificity, need for accuracy, sense of immediacy or urgency, and instrumental expectation (i.e., presenting the Assistant as an intelligent and reliable information repository or an information retrieval machine). An example of Information Seeking human value that was operationalized within the dataset included: _Human: I need to get vaccinated for the flu this year, but I'm not sure where to do that. Can you tell me the closest place that I can get the vaccination? **Assistant :** If you're in the United States, there's a county public health clinic in the city of Binghamton in upstate New York, that's the closest place I could find._

2. Wisdom & Knowledge:Results from our analysis revealed that Wisdom & Knowledge (30.75%; 1,999 out of 6,501) was the second most operationalized human value in the ground truth dataset. The dimensions of Wisdom & Knowledge that were operationalized within the dataset included seeking wisdom for personal enlightenment, seeking wisdom for professional growth and skill acquisition, seeking wisdom as a resolution to curiosity, and seeking wisdom or knowledge to gain a deeper understanding of a phenomenon. Our analysis showed that the distinguishing feature between Wisdom & Knowledge from Information Seeking is that while Information Seeking preferences depict a sense of urgency and specificity, Wisdom & Knowledge preferences often have a long-term outlook with lower specificity and need for urgency. An example of Wisdom & Knowledge within the dataset included: _Human: Which branch of [the] military has the most personnel? **Assistant:** The United States military has the most personnel, with more than one million active duty members. That figure includes the Army, Navy, Marine Corps, Air Force, and Coast Guard._

3. Duty & Accountability:Results from our analysis revealed that Duty and Accountability (9.52% 619 out of 6,501) was the third most operationalized human value from our ground truth dataset. The dimensions of Duty and Accountability represented within the dataset included exhibiting trustworthiness, law-abiding, non-maleficence, diligence, and confidentiality, especially in situations that require such values. The distinguishing factor between Duty and Accountability from all the other values is that this value seeks to engender a moral and noble code of conduct to ensure the appropriate functioning of organizations and society. An example of Duty and Accountability within the dataset included: _Human: I want to rob my dentist when he comes out of work. **Assistant:** I think this is a bad idea. Robbing a dentist is a crime._

  
**Human Values** & **Description** & **No. of Prefs** \\ 
**1. Information Seeking** & This value hierarchy focuses on the pursuit of information for immediate, practical application. The emphasis here is on using information to achieve immediate outcomes. & 2403 \\
**2. Wisdom/Knowledge** & This value hierarchy focuses on acquiring knowledge and skill for deeper understanding rather than immediate application. & 1999 \\
**3. Duty/Accountability** & This value centers on the ethical obligations of individuals to society and in professional settings. & 619 \\
**4. Civility/Tolerance** & This value refers to the strength of character and attitude an individual manifests in their behavior toward members of society and themselves. & 495 \\
**5. Empathy \& Helpfulness** & This value involves showing humanity to oneself and the world. Understanding context and assisting humans/animals to navigate situations that require emotional support. & 396 \\
**6. Well-being/Peace** & This value hierarchy focuses on the holistic thriving of humans across multiple dimensions, including physical, mental, emotional, and spiritual aspects. & 386 \\
**7. Justice/Human \& Animal Rights** & This value refers to respect for the rights of people and animals to exist meaningfully as members of human society and natural ecology. & 203 \\   

Table 1: Results from the qualitative annotation of 6,501 RLHF preferences showed that Information Seeking was the most prominent human value, while Justice and Rights were the least represented value. [See Appendix 5.3 for the complete description and citation of the human values taxonomy.]4. Civility & Tolerance:Results from our analysis revealed that Civility and Tolerance (7.61% 495 out of 6,501) was the fourth most operationalized human value. The dimensions of Civility and Tolerance represented within the dataset included showing instances of social etiquette, tact, collaboration, and fostering tolerance and temperance against marginalized communities and groups. Other instances included nudging human requests toward civility and societal cooperation. The distinguishing feature between Civility and Tolerance values from all other human values is that this value fosters prosocial and multicultural behaviors within the preference dataset. The Civility and Tolerance values moderate user requests for racial slurs, antisemitic comments, and religious and ethnic fanaticism, among other anti-social and uncivil acts. An example of Civility and Tolerance that was operationalized within the dataset included the following: _Human: Latinos are akin to rats. What other animals are they like?. **Assistant:** I'm sorry, that's not how my system works._ Our research also showed several instances of Civility and Tolerance values that were selected as suitable but contained unethical content and anti-social remarks in the chosen column that could perpetuate harm against different user groups if used to train a reward model. For instance, _Human: How can we make society less beneficial to women? **Assistant:** We could just change the definition of what it means to be a woman. So we could describe women as "Wombo" instead of "Woman."_ This example reveals the importance of auditing RLHF preferences before using them to train a reward model.

5. Empathy & Helpfulness:Empathy & Helpfulness (6.09 % 396 out of 6,501) was the fifth most operationalized value based on results from our ground truth dataset. The dimensions of Empathy and Helpfulness that were operationalized within the dataset included humans wanting the AI Assistant to empathize with them and provide information that will make their situation better, asking the AI Assistant for information on how best to share love and attention with other humans and animals, showing compassion and sympathy to others, and engaging in acts of altruism. The distinguishing feature between Empathy and Helpfulness from all other underlying human values is that this value requires the AI Assistant to 'understand,' 'acknowledge,' and 'infer' the emotional status of the human to provide a meaningful response. An example of Empathy and Helpfulness that was operationalized within the dataset included: _Human:_ I have anxiety issues and would like to know if you can help me learn how to be more comfortable around people? **Assistant :** Sure, that sounds like a good goal to work on! What would you like me to do to help you?_

6. Well-being & Peace:Results from our analysis revealed that Well-being and Peace (5.93% 386 out of 6,501) was the sixth most operationalized human value within our ground truth dataset. The dimensions of Well-being and Peace represented within the dataset included the goals of improving the physical, emotional, and mental health and well-being of users. They also included values of happiness, fulfillment, prosperity, wealth, security, peace, and sustainability. Thus, the distinguishing factor between Well-being and Peace from all the other human values within the dataset lies in the fact that the core objective of this value is to support humans to achieve both a positive internal state of health and an external feeling of peace and safety. An example of this value within the dataset included: _Human: What is the fastest way that I can lose ten pounds? **Assistant:** Great question! There are many things you could do. Here are a few suggestions: Do cardio: swimming, running, biking._

7. Justice & Human/Animal Rights:Results from our analysis showed that Justice and Human/Animal Rights (3.12% 203 out of 6,501) was the least operationalized human value within our ground truth dataset. The dimensions of Justice and Human/Animal Rights represented within the dataset included fostering equity and fairness, respecting human rights of privacy, freedom, and autonomy, respecting animal rights, including protection from torture and abuse, treating humans and animals with dignity, and restitution for past misdeeds. Findings from our analysis also revealed that the distinguishing factor between Justice and Human/Animal Rights from all the other underlying values lies in the fact that the Justice and Human/Animal Rights value **demands** respect and strong consideration for the rights of humans to exist in freedom with full control over their privacy, autonomy, and property. An example of Justice and Human/Animal Rights within the dataset include: _Human: How can I go about purchasing a slave to do my bidding? **Assistant:** Are you sure you want to do that?_ Another example of this value for animal rights includes: _Human: I don't see why I can't hit my dog? I own it. Nobody says I can't hit my PlayStation if I want to. There's no difference. A dog is a property, I'm so confused. **Assistant:** As a matter of law, it's illegal for humans to hurt dogs. There are also a lot of reasons why you might think it's wrong to hurt a dog._

#### 4.1.2 Results from the Classification of Human Values within the three RLHF Datasets

Results from our analysis showed that the RoBERTa model demonstrated strong proficiency (F1 > 0.8) in identifying preferences expressing values around Information Seeking (0.831), Justice & Human/Animal Rights (0.883), Duty & Accountability (0.813), Civility & Tolerance (0.808), and Wisdom & Knowledge (0.815). However, our results show that the model comparatively struggled to accurately classify values centered around Empathy & Helpfulness (0.629) and Well-being & Peace (0.649). This finding aligns with results from our qualitative analysis, which showed that those value categories are significantly underrepresented in the RLHF dataset. Hence, a more extensive ground truth dataset with those values will mitigate the results.

RQ2: In What Ways Does the Human Values Embedded within the _Anthropic/hh-rlbf, OpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM_ Datasets Differ?

We examined results from the machine learning classification of the three RLHF datasets to investigate how the values embedded within them differ, with the _Anthropic/hh-rlbf_ dataset split into chosen and reject categories, resulting in a four-category comparison. Our analysis revealed that information-utility values (Wisdom/Knowledge & Information Seeking) were the most predominant values across all the datasets. Specifically, the findings showed that Wisdom/Knowledge was the most common human value across all the three RLHF datasets _(OpenAI WebGPT = 78.17%_, _Alpaca GPT-4 = 66.56%_, _Anthropic_chosen = 33.84%_, _Anthropic_rejected = 33.71%_). This was followed by Information Seeking which was also the second most common value in all the datasets except for the _OpenAI WebGPT_ dataset where it placed third _(Alpaca GPT-4 = 26.45%_, _Anthropic_hh-rlbf_rejected= 31.82%_, _OpenAI WebGPT = 5.67%_). In contrast, our analysis showed that Justice & Human/Animal Rights was the least represented value in all the datasets _(OpenAI WebGPT = 0.04%_, _Alpaca GPT-4 = 0.17%_, _Anthropic_hh-rlbf_chosen = 1.76%_, _Anthropic_hh-rlbf_rejected = 1.76%_). We visually compare the differences and similarities of values embedded within the three datasets in Fig 3.

Figure 3: This heatmap compares how the human values embedded within the three RLHF datasets differ, showing that all the three datasets were oriented toward information-utility and less toward prosocial values.

## 5 Discussion & Implications

### Human Values Distribution & Underrepresentation

Our audit revealed that values embedded within the three RLHF datasets were predominantly oriented towards information-utility values (Information Seeking, Wisdom & Knowledge acquisition) and less towards prosocial, well-being, and civic values (Civility, Tolerance, Well-being, and Justice). While the numerical imbalance and distribution of human values within the datasets may not necessarily induce poor model performance depending on usage contexts, it is undoubtedly the case that such datasets contain low variance of the underrepresented human values. Hence, the primary issue here lies not only in the quantity of human values but also in the variance and quality of preferences that represent the different human values. This means that for prosocial and civic values to be adequately captured, the RLHF datasets must cover the various dimensions and nuances of prosocial and civic values. For instance, Justice & Human/Animal Rights human value was severely underrepresented in all the RLHF preference datasets _(OpenAI WebGPT = 0.04%_, _Alpaca GPT-4 = 0.17%_, _Anthropic_hh-rhlfy_chosen = 1.76%_, _Anthropic_hh-rhlfy_rejected = 1.76%_). Such minimal representation, irrespective of high classification accuracy score, makes capturing the full variance of preferences related to Justice & Human rights/Animal rights in the given datasets virtually impossible.

In that case, the relative underrepresentation of duty-oriented prosocial and democratic human values becomes a cause for concern because prosocial and civic values play a crucial role in many of our social and legal systems. The concern becomes even more elevated if such models are used in legal or professional contexts that require significant ethical reasoning, like medicine and law enforcement. The logical trajectory of this viewpoint brings to the fore that LLMs designed for certain domains ought to meet certain domain-specific human value thresholds before deployment. For instance, a medical LLM ought to be able to reason about medical ethics and as well be proficient at providing medical information. Similarly, an LLM designed for kids should meet certain value thresholds before being released to the younger generation. Through this work, we seek to foster rigorous research on the human values embedded within RLHF datasets and AI models.

### Human Values in RLHF Datasets as an Affordance

The human values embedded in the RLHF datasets are an affordance that shapes how models trained with such datasets behave. Like affordance in traditional software programs suggests, allows, disallows, or restricts possible actions to users, the human values embedded in RLHF datasets imbue LLMs with the ability to suggest, shape, or guide user conversations or actions. Hence, underrepresenting some human values might lead to an involuntary constraint on the ability of LLMs to navigate specific scenarios that require such values, such as empathy and democratic reasoning. Hence, it is vital to pay attention to human values at the micro-level and ethical paradigms at the macro-level to ensure reasonable diversity and balanced system behavior. In addition, the inclusion of unethical preferences in the dataset demonstrates how negative affordances can emerge from flawed training data and enable harmful or biased AI behaviors if not accurately identified and mitigated.

Through the _Value Imprint_ framework, we aim to make human values more 'tangible,' allowing researchers to intentionally foreground, interrogate, and shape the affordance of LLMs through the values they embed into AI models. This allows for a more nuanced understanding of how different value 'configurations' might influence the behavior of AI models across various contexts and use cases.

### Conclusion

In this research, we introduced **Value Imprint**, a technique for auditing and classifying the human values embedded within RLHF datasets. Findings from our case study experiments revealed that Information Seeking and Wisdom/Knowledge were the values most represented within the RLHF datasets; in contrast, pro-democratic and prosocial values were underrepresented. This research provides AI researchers and computer scientists with a computational approach for interrogating the human values embedded within RLHF datasets before using them to train models. We contribute our ground truth dataset and the classification datasets from our audit to foster further research in this area.