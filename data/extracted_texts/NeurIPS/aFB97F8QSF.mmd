# Plant-and-Steal: Truthful Fair Allocations via Predictions

Ilan Reuven Cohen

Bar-Ilan University

ilan-reuven.cohen@biu.ac.il &Alon Eden

The Hebrew University

alon.eden@mail.huji.ac.il &Talya Eden

Bar-Ilan University

talyaa01@gmail.com &Arsen Vasilyan

UC Berkeley

arsen@berkeley.edu

###### Abstract

We study truthful mechanisms for approximating the Maximin-Share (MMS) value of agents with additive valuations for indivisible goods. Algorithmically, constant factor approximations exist for the problem for any number of agents. When adding incentives to the mix, a jarring result by Amanatidis, Birmpas, Christodoulou, and Markakis [EC 2017] shows that the best possible approximation for two agents and \(m\) items is \(\). We adopt a learning-augmented framework to investigate what is possible when a prediction on the input is given. For two agents, we give a truthful mechanism that takes agents' ordering over items as prediction. When the prediction is accurate, our mechanism gives a \(2\)-approximation to the MMS (consistency), and when the prediction is off, our mechanism still obtains an \(\)-approximation to the MMS (robustness). We further show that the mechanism's performance degrades gracefully in the number of "mistakes" in the prediction; i.e., we interpolate between the two extremes: when there are no mistakes, and when there is a maximum number of mistakes. We also show an impossibility result on the obtainable consistency for mechanisms with finite robustness. For the general case of \(n 2\) agents, we give a 2-approximation mechanism for accurate predictions, with relaxed fallback guarantees. Finally, we give experimental results which illustrate when different components of our framework, made to ensure consistency and robustness, come into play.

## 1 Introduction

Allocating items among self interested agents in a "fair" way is an age-old problem, with many applications such as splitting inheritance and allocating courses to students. As a starting point, consider the case of two agents. When the items are divisible, the famous cut-and-choose procedure achieves fairness in two senses. Firstly, no agent wants to switch their allocation with the other; i.e., there is no envy among the agents. Secondly, each agent gets a bundle of items which they value at least as much as their value for all the items divided by 2; that is, each one gets their "fair share". When moving to the case of indivisible goods, which is relevant to scenarios such as splitting inheritance and allocating courses, things get trickier. For instance, if there's a single item, the agent that does not receive that item does not get an envy-free allocation, nor do they get their "fair share" according to the previous definitions. Therefore, it is clear that some fairness needs to be sacrificed in this case.

The study of fair allocations with indivisible goods has been a fruitful research direction, with many meaningful notions of fairness studied (see survey by Amanatidis et al. ). In this paper, wefocus on the notion of the Maximin Share, or MMS, introduced by Budish . For two agents, this notion captures the value an agent will ensure if we implement the cut-and-choose procedure. That is, assume Alice splits the items into two bundles, and then Bob takes one of them (adversarially), and Alice gets the second one. The MMS captures exactly how much value Alice can guarantee for herself. Generalizing the notion for \(n\) agents is pretty straightforward -- the MMS is the minimum value Alice can guarantee for herself when she partitions the items into \(n\) bundles, assuming \(n-1\) bundles are taken adversarially.

We study the case where agents have additive valuations over goods.1 For the case of two agents, the allocation produced by the cut-and-choose procedure guarantees each of the agents their MMS value. For more than two agents, the existence of such an allocation is not longer guaranteed. Kurokawa et al.  show an instance of three agents, where in every allocation, at least one of the agents does not get their MMS value. Since allocating all the agents their MMS value is not always feasible, various papers studied the existence of approximately optimal allocation. An allocation is an \(\)-approximate MMS allocation for \(>1\) if every agents gets at least an \(1/\) fraction of their MMS value. Feige et al.  introduce an instance where one cannot find an \(\)-approximate allocation for \(<\). On the other hand,  show there always exists \(\)-approximation. The \(\) factor was gradually improved , where the state-of-the-art algorithm achieves an approximation of \(959/720<4/3\). Adding incentives to the mix further complicates matters.

Amanatidis et al.  study the case of two additive agents, and \(m\) items, where the algorithm (or mechanism) does not know the values of the agents. Thus, the algorithm's designer is faced with the task of devising an allocation rule such that _(i)_ agents will maximize their allocated value by bidding truthfully, and _(ii)_ the resulting allocation is an \(\)-approximate MMS allocation for an \(\) close to 1 as possible.  show that no incentive-compatible algorithm can approximate the MMS to a factor better than \(\), and this is matched by the following trivial mechanism -- the first agent picks their favorite item, and the second agent gets the rest.

For \(2<n<m\),2 a trivial truthful algorithm that lets the first \(n-1\) agents pick a single item in some order and gives the last agent the rest achieves an \(\)-approximation, and no better mechanism is known. It is conjectured that one cannot drop the dependence in \(m\) for \(n>2\). We are left with a stark disparity. On the one hand, assuming agents' values are public information, approximate solutions are known to exist. On the other hand, when considering private values, it seems that only trivial approximations are possible. _The goal of this paper is to bridge these two regimes using predictions._

We study the problem of truthful allocations that approximate the MMS, taking a learning-augmented point of view. In the learning-augmented framework, the algorithm designer aims to tackle some intrinsic hardness of the problem at hand, which might arise due to computational constraints, space constraints, input arriving piecemeal online, or incentive constraints, among others. To help the designer overcome these constraints, the algorithm is given some side information which is a function of the input, or a _prediction_, in order to improve the algorithm's performance. The hope is that if the prediction is accurate, then the performance is greatly improved over the performance without the prediction (termed _consistency_). On the other end, if the prediction is inaccurate then the performance of the algorithm is comparable to the performance of the best algorithm that is not given access to predictions (termed _robustness_). The learning-augmented framework has proven useful in bypassing impossibilities that arise due to incentive issues .

When designing a learning-augmented mechanism, one should think of realistic predictions. For instance, predicting the entire valuation profile of all agents seems to be a strong assumption. A more plausible assumption is to have some ordinal ranking over the items of the agents. Indeed, it seems unlikely that the algorithm can accurately predict Alice's value for a car, but it is plausible that the algorithm can guess that Alice values the car more than she values the table. Ideally, the algorithm's performance should remain robust if the predicted ordering is almost perfect, with only a few pairs of items whose real ordering is swapped in the prediction. Another desired property is to make the prediction as space-efficient as possible, as previous results  show that succinct predictions are crucial for learning parameters from few samples and for incorporating a PAC-learnable component in the learning-augmented framework.

In this paper we devise learning-augmented truthful mechanisms for the problem of approximate-MMS allocations, while taking into considerations the concerns mentioned above.

Our Results.We start by studying the two agent case. We aim at getting: \((a)\)_Constant consistency:_ when the predictions are accurate, we want to get a constant approximation to the MMS. \((b)\)_Near-optimal robustness:_ when the predictions are off, we want to get as close as possible to the optimal \(\)-approximation we can obtain by truthful mechanisms .

Plant-and-Steal Framework.In Section 3 we present a framework for devising learning-augmented mechanisms for approximating the MMS with two agents. As using only predictions does not guarantee any robustness, we use reports to ensure each agent gets at least one valuable item. This is done while maintaining a near-optimal allocation according to predictions. Our framework, which we term Plant-and-Steal, is modular. Along with the set of goods and the agents' reports, it also receives a prediction and an allocation procedure. Different combinations of predictions and allocation procedures yields different consistency-robustness tradeoffs. It is worth noting that, although privacy is not the primary focus of this paper, the Plant-and-Steal framework uses agents' reports in a minimal way, as they are only required to select (i.e. "steal back") a single item from a predefined set of options, where this set is determined by the predictions, and not the actual reports.

Ordering Predictions.In Section 4, we study learning-augmented mechanisms when the predictions given are _preference orders over items_ of the agents, rather than the values. We instantiate the Plant-and-Steal framework with a Round-Robin-based allocation procedure. We observe that in the case of two agents, Round-Robin obtains 2-approximation to the MMS. The 2-consistency of using Plant-and-Steal with Round-Robin as the allocation procedure almost immediately follows. The \(\) robustness follows two facts: \((a)\) Round-Robin produces allocations that are balanced in the number of items allocated to each agent; and \((b)\) The Plant-and-Steal framework ensures each agent gets one of their 2 favorite items _according to reports_. In Appendix E, we show how to get an improved \(\) consistency, while maintaining \(O(m)\) robustness when using a modified Round-Robin allocation procedure.

In Sec 5, we then study the performance of the Plant-and-Steal framework when using the Round-Robin procedure, when the prediction given is not fully accurate, but accurate to some degree. To quantify the prediction's accuracy, we adopt the Kendall tau distance measure. The Kendall tau distance counts the number of pairs of elements swapped in the predicted preference order and the order induced by the true valuations. We show that combining the Plant-and-Steal framework with a Round-Robin allocation procedure obtains \(O()\)-approximation to the MMS when the Kendall tau distance is \(d\). Since \(d\) goes from 0 to \(=(m^{2})\), we recover the constant consistency when there are no errors, and the \(O(m)\) robustness when the number of errors is maximal.

General Predictions.In Appendix G, we study the two-agent case where the mechanism is given access to predictions which are not necessarily the preference order of the agents. We first show that for any prediction given to the learning-augmented mechanism, no mechanism can simultaneously be \(\)-consistent while maintaining finite robustness for \(<6/5\). For the proof, we leverage the characterization of two-agent truthful mechanisms by .

We then study small-space predictions. The Round-Robin-based mechanisms described above require an \((m)\)-bit prediction (to describe an arbitrary allocation of items). We first notice that we can implement a bag-filling type allocation procedure using \(O( m)\)-bit predictions. This already achieves a constant consistency along with \(O(m)\) robustness. We then devise a more refined allocation procedure, which requires \(O( m/)\)-bit predictions, and achieves \(2+\) consistency along with \(\) robustness.

General number of agents n.In Appendix H, we devise a learning-augmented truthful mechanism for \(n 2\) additive agents. We obtain a 2-consistent mechanism, while relaxing the robustness guarantees of the mechanism. We take a similar approach to the work of , who compete against a relaxed benchmark of the MMS value for \(>n\) agents, and try to minimize \(\). We obtain an \(\{m--1,1\}\)-approximation to the MMS for \(=\) agents when the predictions are off. Our mechanism uses the modified Round-Robin procedure from  to determine the initial allocation using the predictions. It then applies a recursive plant-and-steal procedure to determine the final allocation.

Experiments.Finally, In Section 6, we demonstrate how several components in our design come into play when experimenting with synthetic data. We run different variants of mechanism on two player instances, and show that when predictions are accurate, then only using predictions is nearly optimal, if predictions are noisy, then the stealing component ensures robustness, and our Plant-and-Steal framework achieves best-of-both-worlds guarantees.

We summarize the bounds we obtain in Table 1.

Further Related Work.In addition to the the studies mentioned above, we give a comprehensive review of further related work in Appendix B.

## 2 Preliminaries

In the setting we study, there is a set \(N\) of \(n\) agents and a set \(M\) of \(m\) indivisible items. Each agent has a _private_ additive valuation over the items, unknown to the mechanism designer, where the value of agent \(i\) for item \(j\) is \(v_{ij}\) (also denoted as \(v_{i}(j)\)). For a bundle \(S M\) of items, \(v_{i}(S)=_{j S}v_{ij}\).

The fairness notion we focus on is the following.

**Definition 2.1** (Maximin Share).: _The Maximin Share (MMS) of agent \(i\) with valuation \(v_{i}\) and \(n\) agents is_

\[_{i}^{n}=_{S_{1} S_{n}=M}_{j[n]}v_{i}(S_{j});\]

_that is, if \(i\) were to partition the items into \(n\) bundles, and then \(n-1\) of those bundles are taken adversarially, what is the value \(i\) can guarantee for themselves. When clear from the context, we omit \(n\) and use \(_{i}\) to denote the MMS of \(i\) with \(n\) agents._

We are interested in mechanisms that produce approximately optimal allocations, as defined next.

**Definition 2.2** (\((,k)\)-approximate MMS Allocation).: _An allocation \(X=(X_{1},,X_{n})\) is \((,k)\)-approximate MMS allocation for \(>1\) and a natural number \(k\) if for every agent \(i\),_

\[v_{i}(X_{i})_{i}^{k}/.\]

_When \(k=n\), we say the allocation is a \(\)-approximate MMS allocation._

We study mechanism that get some prediction on the input.

**Definition 2.3** (Learning-Augmented Mechanism).: _A learning-augmented mechanism takes agents' reports \(=(r_{1},,r_{n})\) and predictions \(\) in some prediction space \(\), and outputs a partition of the items_

\[X(,)=(X_{1}(,),X_{2}(, ),,X_{n}(,)), X_{1}(, ) X_{2}(,) X_{n}( ,)=M,\]

_where agent \(i\) gets \(X_{i}(,)\)._

 
**Setting** & **Consistency** & **Robustness** & **Reference** \\  Ordering predictions, \(n=2\) & \(2\\ 3/2\) & \( m/2\\  2m/3\) & Section 4 \\ \( 5/4\) & Any &  \\  Arbitrary predictions, \(n=2\) & Any & \( m/2\) &  \\  & \( 6/5\) & Bounded & Section G.1 \\ \(3 m+1\) space & 4 & \(m-1\) & Section G.2 \\ \(O((m)/)\) space & \(2+\) & \( m/2\) & Section G.3 \\  \(n>2\) & 2 & \(m-}{{2}}-1\\ =}{{2}}\) & Section H \\  

Table 1: Known bounds for truthful learning-augmented MMS mechanisms.

For learning-augmented mechanisms, truthfulness should hold for any possible prediction \(\).

**Definition 2.4**.: _A learning-augmented mechanism is truthful if for every agent \(i\) and every possible report of other agents \(_{-i}\) and every possible prediction \(\),_

\[v_{i}(X_{i}(v_{i},_{-i},)) v_{i}(X_{i}(r_{i},_{-i},))\]

_for every \(r_{i}\)._

We next define the consistency and robustness measures according to which we measure the performance of our mechanisms.

**Definition 2.5** (\(\)-consistency).: _Consider a prediction function \(f_{}\) which takes a valuation profile and outputs a prediction in prediction space \(\). A learning-augmented mechanism is \(\)-consistent for \(>1\) and prediction function \(f_{}\) if for every valuation profile \(\) and every prediction \(=f_{}()\), \(X(,)\) is an \(\)-approximate MMS allocation._

**Definition 2.6** (\((,k)\)-robust).: _A learning-augmented mechanism is \((,k)\)-robust for \(>1\) and natural number \(k\) if for every valuation profile \(\) and every prediction \(\), \(X(,)\) is an \((,k)\)-approximate MMS allocation. If \(k=n\), we say the mechanism is \(\)-robust._

For ease of presentation, for valuation \(v_{i}\), report \(r_{i}\) and prediction \(p_{i}\), we use \(v_{i}^{},r_{i}^{},p_{i}^{}\) to denote _both_ the \(^{}\) highest good according to the valuation/report/prediction _and_ its value. Note that, we may use \(v_{i}^{}\) for \(>m\), in this case, \(v_{i}^{}=0\). For \(=1\), i.e., the highest good we use \(v_{i}^{*},r_{i}^{*},p_{i}^{*}\).

Ordering Predictions and Kendall tau Distance.Most of our mechanisms use predictions which take the form of an ordering over agents' items. That is, \(f_{}()\) outputs a vector of orderings \(=(p_{1},,p_{n})\), where \(p_{i}^{}\) is the \(^{}\) highest valued item of \(i\) in \(M\) according to \(\). Accordingly, for agent \(i\), let \(v_{i}^{}\) be the \(^{}\) highest valued item according to \(\). For two items \(j j^{}\), We use \(j_{p_{i}}j^{}\) to denote that \(j\) is higher ranked than \(j^{}\) according to \(\).

When studying imprecise predictions, we want to quantify the degree to which the prediction is inaccurate. For this, we use the following measure. For an agent \(i\), we define our noise level with respect to the Kendall tau distance (also known as bubble-sort distance) between \(\) and \(\).

**Definition 2.7** (Kendall tau distance).: _The Kendall tau distance counts the number of pairwise disagreements between two orders. For \(i\)'s valuation \(v_{i}\) and predicted preference order \(p_{i}\), we define_

\[K_{d}(v_{i},p_{i})=|\{j_{p_{i}}j^{}\;:\;v_{i}(j)<v_{i}(j^{})\}.\]

_That is, the number of pairs of items where the prediction got their relative ordering wrong. We also denote \(K_{d}(,)=\{K_{d}(v_{1},p_{1}),K_{d}(v_{2},p_{2})\}\)._

We note that the Kendall tau distance between \(v_{i}\) and \(p_{i}\), \(K_{d}(v_{i},p_{i})\), can go from \(0\) to \(\).

## 3 Plant-and-Steal Framework

In this section, we present the framework which is used to devise learning-augmented mechanisms for two agents. The ideas presented here also inspire the more complex learning-augmented mechanism for \(n>2\) agents. Missing proofs of this section appear in Appendix C. Our framework, which we term Plant-and-Steal is given the set of goods, an allocation procedure \(\), the prediction \(\) and reports \(\). The framework operates as follows:

1. It first applies \(\) on the predictions \(\) to divide the set of goods into two bundles \(A_{1},A_{2}\). The procedure \(\) should be an allocation procedure with good MMS guarantees. We use different allocation procedures depending on the type of prediction given and on the consistency-robustness tradeoffs we are aiming for.
2. _Planting phase:_ For each agent \(i\), it picks \(i\)'s favorite item in set \(A_{i}\)_according to prediction_, and "plants" this item in the bundle \(A_{j}\) of the other agent \(j i\). Let \(T_{1},T_{2}\) denote the sets that result in this planting phase.
3. _Stealing phase:_ To obtain the final allocation, each agent \(i\) now "steals" back their favorite item from set \(T_{j}\) of agent \(j i\)_according to reports_. Notice this is the first and only place where we use agents' reports.

This procedure is trivially truthful because the only step where we use agents' reports is the one where they pick exactly one item to steal back from \(T_{j}\), and this \(T_{j}\) only depends on predictions, and not reports (Lemma 3.1). To obtain robustness, we notice that each agent gets one of their two favorite items according to their true valuations (Lemma 3.2). This implies a robustness of \(m-1\). We show that for balanced allocations, we get improved robustness guarantees (Lemma 3.4).

For \(S M\), and agent \(i\), let \(v_{i}^{*}(S)\) (\(p_{i}^{*}(S)\),\(r_{i}^{*}(S)\)) be the max valued item in \(S\) according to \(v_{i}\) (\(p_{i},r_{i}\)). for \(g M\) and \(S M\), denote \(S+g:=S\{g\}\) and \(S-g=S\{g\}\). The Plant-and-Steal framework is presented in Mechanism 1.

```
Input : Allocation Procedure \(\), set of items \(M\), predictions \(\) and reports \(\) Output : Allocations \(X_{1} X_{2}=M\) /* Find an initial allocation by applying \(\) on the predictions */ \((A_{1},A_{2})(M,N,)\) /* Plant favorite items according to predictions */ \(_{1} p_{1}^{*}(A_{1})\) \(_{2} p_{2}^{*}(A_{2})\) \(T_{1} A_{1}+_{2}-_{1}\) \(T_{2} A_{2}+_{1}-_{2}\) /* Steal according to report */ \(_{1} r_{1}^{*}(T_{2})\) \(_{2} r_{2}^{*}(T_{1})\) \(X_{1} T_{1}+_{1}-_{2}\) \(X_{2} T_{2}+_{2}-_{1}\)
```

**MECHANISM 1**Two agent Plant-and-Steal Framework

We show that for any allocation function \(\) and predictions \(\) given to the framework, the resulting mechanism is truthful.

**Lemma 3.1** (Truthfulness Lemma).: _For any allocation procedure \(\), Plant-and-Steal mechanism using \(\) is truthful._

Since the framework is truthful, from now on, we assume that \(=\). Next, we show that the Plant-and-Steal mechanism ensures that for each agent, an item is allocated with a value that is at least as good as their second-best option _according to their value_.

**Lemma 3.2**.: _Consider the allocation \((X_{1},X_{2})\) returned by Plant-and-Steal with some allocation procedure \(\). For any agent \(i\), then \(v_{i}^{} X_{i}\) or \(v_{i}^{2} X_{i}\)._

We next claim that if \(i\) gets one of their two favorite items and any \(k-1\) additional items, \(i\)'s value is an \(m-k\)-approximation to \(_{i}\).

**Lemma 3.3**.: _For any agent \(i\), let \(S M\) be a subset of the items of size \(|S|=k\) and \(v_{i}^{1} S\) or \(v_{i}^{2} S\) then_

\[v_{i}(S)_{i}/(m-k).\]

We immediately get the following.

**Lemma 3.4** (Robustness Lemma).: _Let \(\) be an allocation rule guaranteeing \(\{|A_{1}|,|A_{2}|\} k\), then when Plant-and-Steal uses \(\), the resulting mechanism is \((m-k)\)-robust._

Proof.: By Lemma 3.2, we are guaranteed that each agent gets one of their two favorite items according to their report. Combining with the condition on \(\) and Lemma 3.3, the proof is finished. 

## 4 Ordering Predictions

In this section, we consider the case of two agents, where the predictions (and in fact, also the reports) given to the mechanism are preference orders of agents over items. Our mechanisms makes use of the Plant-and-Steal framework instantiated by Round-Robin based allocation procedures. Wefirst present the round-robin allocation procedures we'll use, and give their approximation guarantees when the input is accurate. Next, we prove the robustness and consistency guarantees. Finally, we quantify the accuracy of the predictions using the Kendall tau distance, and obtain fine-grained approximation results, where the approximation smoothly degrades in the accuracy.

Amanatidis et al.  studied mechanisms where the preference orders of the agents over items are public (while valuations are private). They showed that no truthful mechanism can achieve a better approximation than \(5/4\) in this setting. This implies that when the predictions are preference orders, no learning-augmented mechanism can obtain consistency better than \(5/4\), no matter if the robustness is bounded or not.

**Proposition 4.1** (Corollary of Amanatidis et al. ).: _For any \(>0\), no mechanism that is given preference orders as predictions can obtain consistency \(5/4-\)._

Round-Robin Allocation Procedures.The two allocation procedures we use to instantiate the Plant-and-Steal framework take as input preference orders of agents over items:

* Balanced-Round-Robin: the agents take turns, and at each turn, an agent takes their highest ranked remaining item. This results in a balanced allocation.
* 1-2-Round-Robin: the agents take turns, where we compensate the second agent, who might not get their favorite item, to take two items each turn.

In this section, we only prove consistency-robustness guarantees when Balanced-Round-Robin is used as the allocation procedure. In Appendix E we show different tradeoffs when 1-2-Round-Robin is used.

```
Input : Preference orders of agents over items \(=(v_{1},v_{2})\). Output : An allocation \(A_{1} A_{2}=M\). \(A_{i}\) for every agent \(i\{1,2\}\) for\(r=1,,|M|/2\)do \(A_{1} A_{1}+v_{1}^{*}(M A_{1} A_{2})\) \(A_{2} A_{2}+v_{2}^{*}(M A_{1} A_{2})\)
```

**ALGORITHM 2**Balanced-Round-Robin

Consider the allocation procedure depicted in Algorithm 2. In order to implement the two allocation procedures, we only needs to receive preference orders over items. Let \(A_{i}=(a_{i}^{1},,a_{i}^{|A_{i}|})\) be agent \(i\)'s allocation by the algorithm, where \(a_{i}^{k}\) is the \(k\)'th choice of agent \(i\). We observe the following.

**Observation 4.1**.: _The output \((A_{1},A_{2})\) of the Balanced-Round-Robin procedure, satisfies:_

1. \(|A_{1}|=\)_,_ \(|A_{2}|=\)_._
2. _For each agent_ \(i\) _and round_ \(k\)_,_ \(a_{i}^{k}\{v_{i}^{}\}_{[2k]}\)_; that is, in round_ \(k\) _an agent gets one of their top_ \(2k\) _items._

Amanatidis et al.  show that first allocating large items to agents, and then using a Round-Robin to allocate the remaining items to the remaining agents, gives a 2-approximation to the MMS. We observe that for two agents, Round-Robin _as is_, without the initial step, achieves this approximation guarantee. The proof of the following Lemma is deferred to Appendix D.

**Lemma 4.1**.: _Let \((A_{1},A_{2})\) be the allocation of Balanced-Round-Robin. For every agent \(i\), \(v_{i}(A_{i})_{i}/2\)._

We next use the allocation procedure to instantiate the Plant-and-Steal framework.

Round-Robin-Based Mechanism.The mechanism we analyze, B-RR-Plant-and-Steal, results from instantiating Plant-and-Steal with Balanced-Round-Robin as \(\).

We first show that if the predictions correspond to the preference orders of the real valuations, then B-RR-Plant-and-Steal outputs the same allocation as Balanced-Round-Robin.

**Lemma 4.2**.: _When predictions correspond to actual values, B-RR-Plant-and-Steal outputs the same allocation as Balanced-Round-Robin._

We are now ready to prove the performance guarantees of our mechanisms.

**Theorem 4.1**.: _Mechanism B-RR-Plant-and-Steal is truthful, \(2\)-consistent and \(\)-robust._

Proof.: By Lemma 3.1, the mechanism is truthful. By Observation 4.1, each agent receives at least \( m/2\) items; combining with Lemma 3.4, we get that the mechanism is \(\)-robust. Finally, if predictions correspond to valuations, by Lemma 4.1 and Lemma 4.2, the allocation is a \(2\)-approximation to the MMS. Thus, the mechanism is \(2\)-consistent. 

We note that by Amanatidis et al. , our robustness guarantee matches the optimal obtainable approximation by any truthful mechanism (up to the rounding).

## 5 Noisy Predictions

We now analyze Mechanism B-RR-Plant-and-Steal's performance under varying levels of noise. Consider the case where the Kendall tau distance between \(\) and \(\) is at most \(d\). Our main theorem in this section shows that combining the Plant-and-Steal framework with a Round-Robin allocation procedure obtains \(O()\)-approximation to the MMS when the Kendall tau distance is \(d\). Missing proofs of this section appear in Appendix F.

To prove the approximation ratio, we relate the value that agent \(i\) obtains from the allocation, \(v_{i}(X_{i})\), to their maximin share, \(_{i}\), by considering the worst possible set of items that agent \(i\) might receive under the Round-Robin procedure when acting on their true preferences. Specifically, we define this worst-case set as \(R_{i}=\{v_{i}^{2j}\}_{j\{1,, m/2\}}\). In Eq. (2) of Lemma 4.1, we prove that \(v_{i}(R_{i})_{i}/2\). Therefore, obtaining an allocation that is a factor of \(c\) times \(v_{i}(R_{i})\) ensures a factor of \(c/2\) of the MMS value.

We further simplify the analysis by applying _the zero-one principle3_. The zero-one principle basically let's us reduce the analysis to instances where the values are either 0's or 1's. For threshold \( 0\), let \(h_{}(q)=1\) if \(q\) and \(0\) otherwise, and let \(v_{i}^{}(S)=_{j S}h_{}(v_{i}(j))\).

By the zero-one principle, for two sets \(S,T M\), in order to show that \(v_{i}(S)\) approximates \(v_{i}(T)\), it is enough to show that \(v_{i}^{}(S)\) approximates \(v_{i}^{}(T)\) for every threshold \( 0\).

**Lemma 5.1**.: _For \(c>1\) and for any two sets \(S,T M\), if for every threshold \( 0\), \(v_{i}^{}(S) v_{i}^{}(T)/c\), then \(v_{i}(S) v_{i}(T)/c\)._

Thus, we will show that when the Kendall tau distance is \(d\), for every threshold \( 0\), \(v_{i}^{}(X_{i}) v_{i}^{}(R_{i})/c\) for some \(c=O()\). Recall that \(A_{i}\) is the set of items assigned to \(i\) after running the Round-Robin procedure on the predictions \(\). We first show that for Kendall tau distance \(d\), the _additive_ approximation \(v_{i}^{}(A_{i})\) gives to \(v_{i}^{}(R_{i})\) is \(\).

**Lemma 5.2**.: _If the Kendall tau distance between \(\) and \(\) is at most \(d\), then for any threshold \( 0\), we have that \(v_{i}^{}(A_{i}) v_{i}^{}(R_{i})-\)._

We note that although \(v_{i}^{}(A_{i})\) gives an additive approximation to \(v_{i}^{}(R_{i})\), it can still be the case that the Kendall tau distance is constant, yet \(v_{i}(A_{i})\) does not give any multiplicative approximation to \(_{i}\).4 Therefore, we must use the fact that agent \(i\) gets to "steal" an item according to their _true_ valuation in the Plant-and-Steal procedure in order to get our approximation guarantee. By combining these results, we prove the following theorem concerning the approximation ratio of B-RR-Plant-and-Steal's performance under varying levels of noise, \(d\). 5

**Theorem 5.1**.: _Consider a prediction \(\) and valuations \(\) such that \(K_{d}(,)=d\), then Mechanism B-RR-Plant-and-Steal gives a \((2+6)\)-approximation to the MMS._Experimental Results

In this section6, we give experiments which illustrate the role of different components of our framework for two players under various noise levels of the predictions. The predictions we use for our experiments are the predicted values of the items. The noise we introduce permutes the vectors of values to match the instance's Kendall tau distance, and uses the permuted vector as prediction. We show that our framework is almost optimal for small amounts of noise while still showing resilience for higher noise levels. Moreover, we study the performance of variants which only use specific components of our framework.

When using predictions, our initial allocation procedure is a cut-and-choose procedure, implemented as follows:

* We use the first player's prediction to implement a bag-filling algorithm which sorts the items by values, and then partitions the items into two sets using a greedy procedure that assigns each item to the set with current lowest value.
* We use the second player's prediction to allocate the agent the set with the higher predicted value of the two.

This allocation ensures that the second agent obtains their MMS value according to the prediction. In the data we generates, we observe that in a sampled valuation, the two sets chosen by the bag-filling algorithm gives the two sets the same value, up to 0.5%, which ensures that the lowest valued set obtains a \(1.026\)-approximation to the MMS.

We inspect the following mechanisms:

1. _Random_: a mechanism that ignores reports and predictions and randomly partitions the items into two sets of size \(m/2\).
2. _Random-Steal_: a mechanism that ignores predictions, randomly partitions the items into two sets of size \(m/2\), and then implements the stealing phase where each player takes their favorite item from the other player's set according to reports.
3. _Partition_: a mechanism that ignores reports, and partitions the items according to predictions, using the cut-and-choose procedure described above.
4. _Partition-Steal_: a mechanism that partitions the items according to predictions, using the cut-and-choose procedure described above, and then implements the stealing phase where each player takes their favorite item from the other player's set according to reports.
5. _Partition-Plant-Steal_: a mechanism that implements the Plant-and-Steal framework. partitions the items according to predictions, using the cut-and-choose procedure described above, "plants" each player's favorite item according to predictions, and then "steals" each player's favorite item from the other player's set according to reports.

Experiments.We consider two-player scenarios with \(m=100\) items. For each distance measure, we generate \(1000\) valuation profiles. For each pair of valuation profiles and corresponding Kendall tau distance, we generate \(100\) predictions based on the distance. We then assess the performance of the mechanisms described earlier on these instances. We examine two distinct cases regarding the relationship between the players' preference orders: the _Correlated_ case, where both players have identical preference orders, although their valuation magnitudes differ, and the _Uncorrelated_ case, where the preference orders of the players are generated independently and chosen uniformly at random. Further details on the procedures used to generate the valuations and predictions are provided in Appendix A.

Benchmark.We plot the percentage of these instances where both players get at least \((1-)\) of their MMS value for \(=0.1,0.05,0.02\).

Results.The results are shown in Figure 1. We first examine the performance of the two mechanisms that do not use predictions, _Random_ and _Random-Steal_. Scenarios with correlated values perform significantly worse, as there is a non-negligible probability of an unbalanced partition of the relatively few high and medium valued items in a random partition. For \(\) values of \(0.02,0.05,0.1\), the _Random_ strategy success rate is \(11\%,25\%,\) and \(43\%\), respectively, under correlated preferences, comparedto \(33\%,43\%,\) and \(60\%\) under uncorrelated preferences. Moreover, adding the stealing component significantly improves the success rate only in the uncorrelated case, as _Random-Steal_ achieves success rates of \(66\%,75\%,\) and \(87\%\). In the correlated case, as each player has a highly valuable item stolen, their obtained value is not expected to increase.

In the mechanisms that use predictions, _Partition_, _Partition-Steal_ and _Partition-Plant-Steal_, the performance degrades as a function of noise, as expected. When comparing the performance of _Partition_, which only relies on the prediction component of our framework, and _Random-Steal_, which only relies on the stealing component of our framework, we notice that in the uncorrelated case, for small amount of noise guarantee a higher success rate, while as the noise increases, the stealing component becomes more instrumental to the performance. This is in tact with the theoretical results, where using the prediction is crucial to achieve the consistency guarantees, which take place when the prediction is accurate, while stealing is important to achieve robustness guarantees in case the prediction is inaccurate. As described above, in the case where the valuations are correlated, stealing is not expected to help. Interestingly, on fully noisy input, even _Random_ outperforms _Partition_ as _Partition_ might partition the items into unequally-sized sets, which performs worse than the equally-sized sets _Random_ outputs.

Our experiments show that _Partition-Plant-Steal_ performs as well as the _Partition_ strategy for small amounts of noise and outperforms it on uncorrelated instances for large amounts of noise. Moreover, for any amount of noise, it outperforms _Random-Steal_ and converges to it for a fully noisy input. This illustrates the "best of both worlds" tradeoff obtained by our framework.

Finally, when comparing the _Partition-Plant-Steal_ strategy to the _Partition-Steal_ strategy, we observe that _Partition-Plant-Steal_ outperforms _Partition-Steal_ in the correlated case with a small amount of noise (worst-case scenario) for \(=0.02\), as planting guarantees your favorite items would not be taken. In other scenarios, _Partition-Steal_ outperforms _Partition-Plant-Steal_ because "planting" removes a valuable item from the player's set that might be taken otherwise, especially in the uncorrelated case.

Figure 1: Mechanisms: _Random_ (yellow), _Random-Steal_ (cyan), _Partition_ (red), _Partition-Steal_ (green), _Partition-Plant-Steal_ (blue). Data generation: correlated (first row) and uncorrelated (second row). Success rate: the percentage of instances where both players receive at least \((1-)\)-fraction of their MMS values for different values of \(\): \(0.02\) (first column), \(0.05\) (second column), and \(0.1\) (third column).