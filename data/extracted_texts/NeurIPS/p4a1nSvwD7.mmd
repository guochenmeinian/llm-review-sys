# From Dictionary to Tensor: A Scalable Multi-View Subspace Clustering Framework with Triple Information Enhancement

From Dictionary to Tensor: A Scalable Multi-View Subspace Clustering Framework with Triple Information Enhancement

Zhibin Gu\({}^{1}\) Songhe Feng\({}^{2,3}\)

\({}^{1}\) College of Computer and Cyber Security, Hebei Normal University, China

\({}^{2}\) Key Laboratory of Big Data & Artificial Intelligence in Transportation (Beijing Jiaotong University), Ministry of Education, China

\({}^{3}\) School of Computer Science and Technology, Beijing Jiaotong University, China

{guzhibin, shfeng}@bjtu.edu.cn

Corresponding author

###### Abstract

While Tensor-based Multi-view Subspace Clustering (TMSC) has garnered significant attention for its capacity to effectively capture high-order correlations among multiple views, three notable limitations in current TMSC methods necessitate consideration: 1) high computational complexity and reliance on dictionary completeness resulting from using observed data as the dictionary, 2) inaccurate subspace representation stemming from the oversight of local geometric information and 3) under-penalization of noise-related singular values within tensor data caused by treating all singular values equally. To address these limitations, this paper presents a **S**calable TMSC framework with **T**riple **inf**O**rmatio**N** **E**n enhancement (**STONE**). Notably, an enhanced anchor dictionary learning mechanism has been utilized to recover the low-rank anchor structure, resulting in reduced computational complexity and increased resilience, especially in scenarios with inadequate dictionaries. Additionally, we introduce an anchor hypergraph Laplacian regularizer to preserve the inherent geometry of the data within the subspace representation. Simultaneously, an improved hyperbolic tangent function has been employed as a precise approximation for tensor rank, effectively capturing the significant variations in singular values. Extensive experiments on a variety of datasets show that the STONE outperforms SOTA approaches in both effectiveness and efficiency.

## 1 Introduction

Data clustering, a fundamental technique within the domains of machine learning and computer vision, aims to partition an unlabeled dataset into discernible subgroups characterized by substantial internal similarity [1; 2; 3; 4; 5]. In practical scenarios, objects are frequently characterized by a multitude of properties or data originating from various sources [6; 7; 8; 9]. For instance, in medical analysis, imaging data from modalities such as X-ray, CT, and MRI play a crucial role in diagnosis and disease monitoring. These diverse features, representing various aspects of the same object, collectively constitute multi-view data. Multi-view clustering (MVC), which endeavors to harness the abundant information inherent in multi-view data to enhance the quality of clustering, has emerged as a highly esteemed research avenue [10; 11; 12]. Existing MVC methods can be broadly categorized into four groups based on the underlying learning mechanisms: matrix factorization-based approaches [13; 14; 15], subspace-based approaches [16; 17; 18], graph-based approaches [19; 20; 21], and kernel-based approaches [22; 23; 24]. Among these, subspace approaches are highly regarded for their straightforward implementation and excellent results.

Multi-view subspace clustering is oriented towards the incorporation of diverse constraints within subspace representations to acquire a consensus one that is conducive to clustering [25; 26; 27; 28; 29; 30; 31]. For instance, Cao et al.  and Li et al.  proposed the utilization of the Hilbert-Schmidt independence criterion as a dependency measure, aiming to capture diversity and consistency from multi-view data, respectively. Pan and Kang  integrated a contrastive loss regularization into the consensus subspace representation, encouraging the proximity of similar samples and the separation of dissimilar ones. In addition, Huang et al.  facilitated the extraction of valuable consensus representation by assuming cross-view sparsity of inconsistent components in multi-view data. Nevertheless, these methods are confined to investigating only the linear affinity relationships between data pairs within individual views and do not capitalize on the higher-order correlations among data points across multiple views. This limitation results in suboptimal clustering performance. As a result, tensor-based multi-view subspace clustering methods (TMSC) have remained the focus of sustained attention in recent years. Typically, these TMSC methods consolidate various subspace representations into a 3D tensor, subsequently applying global structural constraints to uncover the complex, nonlinear relationships among data points across different views [33; 34; 35; 36; 37; 38; 39]. For example, Xie et al.  advanced cross view consistency exploration by employing Tensor Nuclear Norm (TNN) on the rotated tensor. Jia et al.  characterized the intra-view and inter-view relationships of data points by applying symmetric low-rank constraints to the frontal slices and structured sparse low-rank constraints to the horizontal slices. Furthermore, Guo et al.  and Sun et al.  introduced the logarithmic Schatten-\(p\) norm and the arctan rank norm as compact surrogates for tensor rank, aimed at capturing distinctive information from tensor singular values.

Despite the noteworthy clustering quality achieved by the TMSC methods described above, there remains considerable potential for further enhancements across four critical dimensions. First, many existing methods exhibit quadratic or even cubic time and space complexities, which restricts their scalability for large-scale datasets. Second, previous techniques have utilized the given feature matrix as the dictionary for subspace recovery. However, this method requires that the feature representations to include a sufficient number of uncontaminated sampled points; otherwise, the resulting subspace representation may not accurately capture the affinity relationships among the data points. Third, traditional approaches often emphasize the low-rank structure of tensor representations to investigate high-order nonlinear correlations among data points across various views, while frequently neglecting the intricate local geometric correlations within individual view. Finally, many methods impose equal penalties on the singular values of tensor data, which may lead to excessive penalization of larger singular values while under-penalizing smaller ones, resulting in suboptimal tensor representations. This issue arises from the differing significance of singular values in tensor data, where larger singular values indicate valuable features and smaller ones are often associated with noise.

Drawing from the principles and justifications discussed previously, this paper proposes a Scalable TMSC framework with **T**riple **i**nf**O**rnatio**N** **E**nhancement (**STONE**). First, STONE employs an enhanced anchor dictionary representation mechanism instead of the traditional self-representation to learn a subspace representation. This approach effectively reduces computational complexity and enhances the stability and robustness of the algorithm in situations where dictionary are insufficient or corrupted. Additionally, we introduce an anchor hypergraph Laplacian regularization to guide the learning of target anchor tensor representation, facilitating the simultaneous utilization of high-order correlations among data points across views and geometric correlations among data points within each view. Furthermore, a refined hyperbolic tangent rank is developed as a non-convex low-rank regularization for tensor data, enabling the STONE model to effectively distinguish the distinct physical meanings of various singular values. Compared to existing TMSC methods, the contributions of this paper can be outlined as follows:

* We introduce an enhanced anchor dictionary representation strategy to recover the anchor subspace representation, mitigating the high computational complexity of self-representation methods and improving accuracy under dictionary under-sampling.
* We develop a refined Hyperbolic Tangent Rank (HTR) as a precise approximation to the tensor rank. In contrast to TNN, HTR allows for variable penalties on individual singular values, facilitating a thorough exploration of differences among different singular values.
* We utilize anchor hypergraphs that encode geometric manifold correlation to regularize the target tensor representation, allowing for the simultaneous utilization of high-order correlations across different views and the complex relationships within each view.

* We present an iterative optimization algorithm along with analyses of its complexity and convergence. Comprehensive experimental results demonstrate that the STONE model excels in both clustering performance and efficiency.

## 2 Theoretical Foundation

Let \(\!=\!\{_{1},...,_{n}\}^{d n}\) denotes a dataset comprising \(n\) instances, with each instance represented by a \(d\)-dimensional feature vector. Low-Rank Representation (LRR)  aims to recover a subspace representation by employing the feature matrix \(\) as a dictionary, which can be mathematically described as follows:

\[_{,}\|\|_{*}+\|\|_{2,1}, \,=+,\] (1)

where \(^{n n}\) represents the subspace representation, which is regularized with the nuclear norm \(\|\|_{*}\) to ensure a low-rank structure. The reconstruction error is denoted by \(^{d n}\) and is constrained by the \(_{2,1}\)-norm to promote sparsity. The parameter \(\) serves as a balancing factor.

LRR has proven its effectiveness in uncovering the spatial structure of data patterns [41; 42; 43], yet it hinges on a critical requirement: the data matrix \(\) must contain a sufficient number of data points sampled from the subspaces. Otherwise, a potential solution to Eq. (1) could be the identity matrix, which hinders the implementation of low-rank representation (LRR). To address this issue, Liu and Yan  proposed that, alongside the given data \(\), there exists a set of unobserved data points \(\) in the dictionary representation, which acts as an ideal supplement to \(\). This strategy is known as the latent low-rank representation model (LatLRR), which helps to mitigate the impacts of insufficient and corrupted observational data. Its mathematical definition is as follows:

\[_{,}\|\|_{*}+\|\|_{2,1}, \,=[;]+,\] (2)

where \(^{k n}\) represents the unobserved feature representation, which is concatenated with \(\) along the columns to form a complete feature representation serving as the dictionary. For practicality,  relaxes Eq. (2) into the following nuclear norm minimization problem to approximate the unobserved data and learn an accurate subspace representation:

\[_{,,}\|\|_{*}+\|\|_{*}+ \|\|_{2,1},\,\!=\!\! +\!\!+\!,\] (3)

where \(^{d d}\) denotes an intermediate result, which is obtained through the skinny SVD theory, which serves as a tool for feature extraction . Emphasizing our focus on clustering, the subsequent discussion revolves around the subspace representation \(\), and the nuclear norm on \(}\) will be relaxed to the Frobenius norm--a convex surrogate for low-rank constraint that adheres to the block diagonal condition [45; 46; 47].

## 3 The Proposed Method

### The STONE Model

Consider a dataset containing \(n\) samples and \(m\) views, denoted as \(\{^{v}\}_{v=1}^{m}\), where \(^{v}^{d_{v} n}\) represents the \(v\)-th view feature, and \(d_{v}\) indicating the corresponding dimension. The objective of the TMSC method is to organize multiple view-specific subspace representations into a 3-D low-rank tensor, with the aim of unveiling higher-order correlation information spanning multiple views. Formally, the general mathematical expression of TMSC is as follows:

\[_{\{^{v},^{v}\}}(})+(\{^{v}\})+(\{^{v}\})\] (4) s.t. \[ v,\,^{v}=^{v}^{v}\!+\! ^{v},}=(^{1},...,^{m}),\]

where \(^{v}\!^{n n}\) represents the subspace representation of the \(v\)-th view, and \(}^{n m n}\) is a 3-D tensor formed from the collection \(\{^{v}\}_{v=1}^{m}\), with \(\) acting as the tensorization operator. \(()\) is used for compact approximation of the tensor rank, while \(()\) is tailored to capture noise. \(()\) represents the structured constraint applied to the subspace representation \(^{v}\). \(\) and \(\) are two trade-off parameters.

Although model (4) effectively captures the high-order consistency of data points across different views, it has two notable limitations regarding its mechanism of using the observed data as a dictionary for constructing the tensor representation. First, the time and space complexity of Model (4) becomes quadratic or even cubic, which restricts its scalability to large datasets. Second, it requires the feature representation matrix \(^{v}\) to contain a sufficient number of uncontaminated sampled data points; otherwise, the learned subspace matrix \(^{v}\) may manifest as the identity matrix, hindering the effectiveness of the LRR method .

To overcome these limitations, we introduce the Enhanced Anchor Dictionary (EAD) representation strategy for recovering anchor subspace representations. EAD first selects a set of distinctive samples from the available data to form an anchor dictionary (i.e., \(^{v}^{d_{v} l}\), \(l\) is the number of anchors), enabling the recovery of a subspace representation \(^{v}^{n l}\) that is smaller in size. This approach helps alleviate the issue of high computational complexity. Additionally, inspired by LatLRR , EAD integrates the observed anchors \(^{v}\) with the unobserved sampled data \(^{v}\) into a comprehensive dictionary (i.e., \([^{v};^{v}]\)), effectively mitigating problems arising from under-sampling of feature characteristics in the anchor dictionary. As a result, we formulate a TMVC framework induced by EAD as follows:

\[_{\{^{v},^{v},^{v},^{ v}\}}(})+(})+ (\{^{v}\})+(\{^{v}\})\] (5) \[\, v,\,^{v}=^{v}(^{v})^{}+^{v}^{v}+^{v},(^{v})^{ }^{v}=,\] \[}=(^{1},...,^{m}),}=(^{1},...,^{m}),\]

where \(^{v}^{n l}\) and \(^{v}^{d^{v} d^{v}}\) denote the anchor subspace and projection matrix, respectively, and presented in tensor forms as \(}^{l m n}\) and \(}^{d_{v} m d_{v}}\). \(()\) is a constraint on \(}\). Notably, the anchor matrix \(^{v}^{d_{v} l}\) is subjected to orthogonality constraints to ensure optimal distinguishability. \(\), \(\) and \(\) are three trade-off parameters.

To delve deeper into the valuable information embedded in multi-view data and refine the quality of the anchor tensor representation obtained in the Model (5), tailored constraints--including Hyperbolic Tangent Rank, Linear Weighted Frobenius norm, and Anchor Hypergraph Laplacian Regularization--are applied to \(}\), \(}\), and \(^{v}\), respectively. These constraints are clearly defined as follows:

**Definition 1**.: _For a tensor \(}^{n_{1} n_{2} n_{3}}\), the Hyperbolic Tangent Rank (HTR) is defined as follows:_

\[\|}\|_{}:=}_{k=1}^{n_{3}}\|}_{f}^{k}\|_{}=}_{k=1}^{n_{3}}_{i =1}^{n}_{f}^{k}(i,i)}-e^{-_{f} ^{k}(i,i)}}{e^{_{f}^{k}(i,i)}+e^{-_{f}^{k}(i,i )}},\] (6)

_where \(\!>\!0\), \(h=min(n_{1},n_{2})\). \(}_{f}^{k}\) denotes the \(k\)-th frontal slice of \(}\) and \(_{f}^{k}\) is the representation of the Fourier domain obtained by the tensor-SVD (i.e., \(_{f}^{k}}=_{f}^{k}}_{f}^{k}(_{f}^{k}})^{}\))._

**Definition 2**.: _For multiple matrices \(\{^{v}\}_{v=1}^{m}\), their Linearly Weighted Frobenius (LWF) norm is defined as follows:_

\[\|}\|_{}:=_{v=1}^{m}\|^{v}\|_{}=_{v=1}^{m}^{v}\|^{v}\|_{F},\] (7)

_where \(\|\|_{F}\) denotes the Frobenius norm of a matrix, and \(=[^{1},^{2},...,^{m}]\) represents the weighted coefficient vector, with each weight empirically set to 1._

**Definition 3**.: _For the given tensor \(}^{l m n}\), its Anchor Hypergraph Laplacian Regularization (AHR) is defined as follows:_

\[\|}\|_{}: =_{v=1}^{m}\|^{v}\|_{}=_{v=1}^{m} Tr(^{v}_{h}^{v}(^{v})^{}),\] (8)

_where \(_{h}^{v}\) denotes the anchor hyper-Laplacian matrix constructed based on the anchor hypergraph \(_{h}^{v}(,,)\) (with \(\), \(\), and \(\) denoting vertices, hyperedge set, and weights, respectively). Specifically, \(_{h}^{v}=_{h}^{v}-^{v}_{v}^{v}( _{v}^{v})^{-1}^{v}\). Here, \(_{h}^{v}\), \(_{c}^{v}\) and \(_{c}^{v}\) being degree matrices with diagonal elements as vertex degrees, hyperedge degrees and hyperedge weights, respectively. \(^{v}\) defines vertex-hyperedge relationships, where \(r^{v}(v,e)=1\) if the \(v\)-th vertex is in the \(e\)-th hyperedge, otherwise 0 ._By unifying Eqs. (5) - (8), we formulate the objective function for the STONE model as follows:

\[_{\{^{v},^{v},^{v}\},,}}\|}\|_{}+\|}\|_{ {LWF}}+\|\|_{2,1}+\|}\|_{}\] (9) \[\ \  v,\ ^{v}=^{v}(^{v})^{ }+^{v}^{v}+^{v},=[^{1}, ,^{m}]^{},\] \[(^{v})^{}^{v}=, }=(^{1},,^{m}),}=( ^{1},,^{m}),\]

where \(=[^{1};;^{m}]^{}\) is derived by horizontally concatenating elements along the rows of \(\{^{v}\}\). In the end, by utilizing the \(k\)-means clustering algorithm on the left singular vectors of the connectivity matrix \(}=}[^{1},...,^{m}] ^{n lm}\), we achieve the clustering partition results .

**Remark 1**.: [**Why STONE outperforms other self-representation methods?**] Unlike previous TMSC methods [38; 52], the STONE model employs the EAD strategy instead of self-representation to recover subspace representations, combining the benefits of anchor representation and latent low-rank representation (LatLRR) for the preservation of both accuracy and efficiency. Notably, illustrated in Figure 1, the efficacy of EAD stems from its thoughtful design: the utilization of the anchor representation enables the EAD model to recover the subspace representation of size \(n l\), ensuring linear scalability for extensive datasets. Additionally, the introduction of the LatLRR mechanism permits both the observed anchor vectors and the unobserved sampled data to function as dictionaries, safeguarding the recovered anchor tensor representation against deficiencies in insufficient dictionaries.

**Remark 2**.: [**Why STONE outperforms other tensor rank methods?**] The STONE focuses on using the hyperbolic tangent tensor rank as a low-rank structural regularization constraint for tensor representation, defined as \(f(x)=-e^{- x}}{e^{ x}+e^{- x}}\). Since HTR is a non-convex function with adjustable slopes, it can delve into the distinct physical meanings of different singular values in tensor data, thereby enhancing the representation capability of tensors. Analysis of Figure 2 reveals a clear superiority of the HTR in approximating tensor rank compared to TNN  and TLS\({}_{p}\)N , particularly for values nearing zero and relatively large singular values. Specifically, as \(x\) approaches 0, \(f_{}(x)\) is considerably greater than \(x\) and \((1+x^{p})\); on the other hand, as \(x\) increases, \(f_{}(x)\) approaches 1. The STONE method adaptively applies appropriate strong and weak penalties to both small and large singular values, preserving valuable information while also demonstrating robustness against noise. Furthermore, when \(x=0\), \(f(x)=0\), which is consistent with the true tensor rank.

### Optimization

To tackle the objective function, we start by introducing auxiliary variables \(}\) and \(\{^{v}\}\), which ensure that all variables in Eq. (9) become separable, as follows:

\[_{\{^{v},^{v},^{v},^{v}\},, }}\|}\|_{}+_{v=1}^{m}\| ^{v}\|_{F}^{2}+\|\|_{2,1}+_{v=1}^{m}(^{v}_{h}^{v}(^{v})^{})+\|}-}+}}{}\|_{F}^{2}\] (10)

where \(}\), \(\{_{v}^{v}\}\) and \(\{_{2}^{v}\}\) are the Lagrange multipliers, and \(\), \(_{1}\) and \(_{2}\) signifies the penalty coefficients. Then, the optimization of the STONE objective function can be streamlined into six sub-problems labeled as \(\{^{v}\}\), \(\{^{v}\}\), \(\{^{v}\}\), \(\{^{v}\}\), \(\) and \(}\) for individual optimization. Given space limitations, the comprehensive optimization procedures and pseudocode are outlined in the A.1 of the supplementary materials.

Figure 1: Schematic of Enhanced Anchor Dictionary Representation (EAD).

Figure 2: Tensor Rank Approximation: HTR vs. TNN and TLS\({}_{p}\)N.

### Convergence Analysis

The validation presented in Theorem 1 establishes the reliability of the optimization algorithm's convergence, while Appendix A.2 of the supplementary materials offers an in-depth exploration of the underlying details.

**Theorem 1**.: _The sequence generated by the employed optimization algorithm, denoted as \(_{t}=\{_{t}^{v},_{t}^{v},_{t}^{v}, _{t}^{v},_{1}^{v},_{2}^{v},_{t}^{v}, }_{t}\}_{t=1}^{}\), adheres to the following two fundamental principles:_

* _The set_ \(\{_{t}\}_{t=1}^{}\) _is bounded;_
* _Any accumulation point of the sequence_ \(\{_{t}\}_{t=1}^{}\) _is a KKT point of Eq.(_10_)._

### Complexity Analysis

The computational requirements of STONE are split into two primary areas: optimizing variables and performing clustering. At the outset, the process involves updating several key variables-\(_{h}^{v},^{v},^{v},\)\(^{v},\)\(^{v},^{v},\)\(}\) -with their respective time complexities being \((l^{2}m(l),(nl^{2}+mld^{v})),(nd^{v}),\)\((n(d^{v})^{2}+nld^{v}),(nld^{v}+l^{2}d^{v}),(nl^{2}), (mnl(mn)+nm^{2}l)\). In the following phase, the computational complexity is given by \(O(nlm+nd_{})\). This indicates a direct proportionality to the sample size \(n\). Furthermore, the memory complexity of the STONE model, expressed as \(O(nlm+nd_{})\), also maintains a linear growth pattern with respect to \(n\).

### Comparison with Previous Studies

In recent years, various tensor-based multi-view clustering algorithms, such as MVSC-TLRR , TLS\({}_{p}\)NM-MSC , SSG-TAR , NOODLE , ASR-ETR , and EDISON , have been proposed to explore high-order correlations among views by pursuing a global low-rank structure in tensor representations. However, our STONE model significantly differs from these methods. For instance, unlike MVSC-TLRR, TLS\({}_{p}\)NM-MSC, SSG-TAR and NOODLE, our STONE model differs by enhancing computational efficiency through the construction of anchor subspace representations rather than relying on traditional subspace representations. Moreover, unlike the ASR-ETR, which lowers computational complexity through anchor dictionary representation, our STONE model builds on this by using the EAD strategy to address challenges related to insufficient data sampling. Additionally, STONE employs anchor hypergraph Laplacian regularization rather than anchor Laplacian regularization in ASR-ETR, which further enhances the accuracy of subspace representations. In contrast to the EDISON, designed for incomplete multi-view data, our approach not only has differences in dictionary representations due to variations in data completeness, but also employs distinct non-convex functions to regularize the singular values of tensor data during the recovery of compact tensor representations.

## 4 Experiment

In this section, we present comprehensive experiments to evaluate the performance of the STONE model. Due to space constraints, a portion of the experiments is presented here, with additional experiments detailed in the Appendix A.3 of the supplementary materials.

### Experimental Setup

**Datasets:** For the clustering experiments, we employ eight datasets: NGs, BBCSport, HW, Scene15, MSRCV1, Caltech101-all, ALOI-100, and CIFAR10. More detailed descriptions of these datasets can be found in Table 1.

**Baselines:** Ten SOTA methods, including eight shallow-based models and two deep learning models: SMVSC (2021) , SFMC (2022) , GMC (2019) , MSC\({}^{2}\)D (2023) , MVCtopl (2022) , MVSCTM (2022) , ETLMC (2019) ,

  Datasets & Type & Samples & Clusters & Views \\  NGs & Text & 500 & 5 & 3 \\ BBCSport & Text & 544 & 5 & 2 \\ HW & Digit & 2000 & 10 & 2 \\ Scene15 & Scene & 4485 & 15 & 3 \\  MSRCV1 & Object & 210 & 7 & 5 \\ Caltech101-all & Object & 9144 & 102 & 6 \\ ALOI-100 & Object & 10800 & 100 & 4 \\ CIFAR10 & Object & 50000 & 10 & 4 \\  

Table 1: Overview of Statistical Features for Eight Datasets.

TBGL (2023) , MFLVC (2022) , GCFAgg (2023) , along with spectral clustering with the best view (SC-best) , are used for comparison.

**Evaluation Metrics:** To provide a comprehensive evaluation of clustering quality, we employ five metrics, namely ACC, NMI, PUR, F-score, and ARI. Better clustering quality is indicated by higher values of these metrics.

**Implementation Overview:** For the comparative methods, the parameters are fine-tuned in accordance with the instructions presented in the respective literature, and the optimal outcomes are reported. For the STONE model, there are five parameters that necessitate adjustment. To be specific, the intrinsic parameter \(\) and the number of anchor points \(c\) are tuned individually within the ranges [0.1, 0.5, 1, 1.5, 5] and [c, 2c,..., 7c], respectively. The three balancing parameters \(\), \(\), and \(\) are finely tuned within the range [1e-5,1e-5,..., 1e+1] using a grid search strategy. To maintain rigor, we perform each experiment a total of 10 times, and we present both the mean results and the standard deviations for comparison. The experimental procedures for the shallow learning model are implemented using MATLAB 2018a on a computer featuring a 3.70GHz i9-10900k CPU and 64GB RAM. Conversely, the deep learning model experiments are facilitated by PyTorch 1.12, deployed on an RTX 4060 GPU.

### Comparison of Clustering Performance and Efficiency

The clustering performance and computational efficiency of the proposed STONE model are demonstrated separately in this subsection.

**Performance Assessment:** To validate the effectiveness of our STONE method, we evaluate its clustering performance on eight datasets and compared it with ten SOTA methods across five metrics.

  
**Dataset** & **Metric** & **SC-best** & **SMNNC** & **SMSE** & **CMC-based** & **MC-based** & **MC-based** & **MFC-based** & **MFC-based** & **MFC-based** & **CP-wise** & **STONE** \\   &  & 0.69,000 & 0.78,000 & 0.78,000 & 0.78,000 & 0.78,000 & 0.78,000 & 0.78,000 & 0.78,000 & 0.78,000 & 0.78,000 & 0.78,000 & 0.78,000 & 0.78,000 & 0.78,000 & 0.78,000 \\  & & 0.000,000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 \\  & & 0.000,000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 \\  & & 0.000,000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 \\   &  & 0.69,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 \\  & & 0.000,000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 \\  & & 0.000,000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 \\   &  & 0.69,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 \\  & & 0.000,000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 \\  & & 0.000,000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 \\  & & 0.000,000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 \\   &  & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 & 0.78,0000 \\  & & 0.000,0000 & 0.78,0000 & 0.78,0000 & 0.78,00The results are summarized in Table 2, where the highest and second-highest values are marked with **bold** and underlined, respectively. The acronym 'OM' signifies occurrences of out-of-memory errors. From Table 2, we can draw the following three findings:

1) Our STONE model exhibits excellent clustering performance across all datasets, significantly surpassing competitors in certain scenarios. For instance, on the Scene15 dataset, STONE outperforms the second-ranked method, ETMSLC, across five performance metrics (ACC, NMI, PUR, F-score, and ARI) with improvements of **13%**, **9.5%**, **9.1%**, **12.7%** and **13.7%**, respectively. Moreover, STONE demonstrates **ideal clustering performance on the NGs, BBCSport, HW, and MSRCV1** datasets. These results indicate that the STONE model effectively uncovers higher-order correlations among multiple views as well as the geometric manifold information within each view, contributing to improved clustering outcomes.

2) Methods based on tensor constraints often outperform those based on matrix constraints in terms of clustering performance. This enhancement is primarily attributed to the ability of tensor-based approaches to impose low-rank constraints at the tensor level, effectively capturing the inherent high-order correlations in multi-view data. In contrast, matrix-based methods generally focus only on linear correlations within individual views.

3) In comparison to prominent deep learning models, such as MFLVC  and GCFAgg , the STONE method demonstrates superior performance in most scenarios. This suggests that shallow learning models can still produce more effective clustering results than deep learning methods in multi-view tasks by cleverly extracting the rich information contained within multi-view data.

**Efficiency Assessment:** To demonstrate the efficiency of the STONE method, we record its running time on datasets containing over 4000 instances and compared it with other benchmark methods. The comparison results are summarized in Table 3. Notably, STONE exhibits significant efficiency in this comparison. For instance, on the ALOI-100 dataset, our method runs in 66.57 seconds, whereas the anchor tensor-induced model TBGL takes over 26000 seconds, which is considerably longer than the STONE model. The STONE method achieves higher efficiency due to its innovative combination of anchor point dictionary representation learning and anchor hypergraph Laplacian regularization. This approach selectively incorporates a small subset of the most discriminative anchor points, ensuring faster computational efficiency while preserving precise clustering performance.

### Parameters Analysis

In the STONE model, there are five parameters, including the built-in parameter \(\), the number of anchors \(l\), and three balancing parameters \(\), \(\), and \(\). This subsection investigates the impact of these parameters on the STONE model. Specifically, the parameters \(\) and \(l\) are treated as independent variables for individual tuning, while the balancing parameters are adjusted pairwise using a grid search strategy.

 
**Datasets** & **SMVC** & **SFMC** & **GMC** & **MSCD** & **MVCpol** & **MVSCTM** & **ETLMSC** & **TBGL** & **MPLVC** & **GCFAgg** & **STONE** \\ 
**Scence** & 19.79 & 23.91 & 57.14 & 174.1 & 131.91 & 482.22 & 639.39 & 1279.79 & 107.39 & 145.65 & **6.52** \\
**ALOI-100** & 197.76 & 148.75 & 404.36 & 1013.9 & 706.78 & 2064.7 & 4257.3 & 2610.69 & 659.69 & 400.85 & **66.57** \\
**Caltech101-all** & 247.29 & **165.36** & 398.94 & 856.3 & 5704.9 & 1169.2 & OM & OM & 1212.5 & 589.01 & 285.11 \\
**CIFAR10** & 867.26 & 4251.3 & OM & OM & OM & OM & OM & 1257.08 & 1978.88 & **860.8** \\  

Table 3: Efficiency Comparison of Different Methods on Datasets with over 4,000 Samples.

Figure 4: The Influence of Anchor Quantity on STONE Model Performance.

**Impact of the Built-in Parameter \(\):** HTR is utilized as a non-convex penalty term for the singular values of tensor data, dynamically controlling the degree of shrinkage applied to different singular values by adjusting the parameters \(\). We explore the impact on clustering results for datasets NGs, HW, and MSRCV1 by adjusting the parameter \(\) across the values [0.1, 0.5, 1, 1.5, 2, 5]. This variation enabled us to assess its impact on the clustering outcomes, with the results detailed in Figure 3. Clearly, alterations in the value of \(\) lead to fluctuations in the clustering results, driven by the varying contraction degree of \(\) across different singular values.

**Influence of the Number of Anchors:** In this subsection, we study how the number of anchors affects the performance of STONE, with anchor counts ranging from [\(c\), 7\(c\)] and a step size of \(c\). As illustrated in Figure 4, the clustering performance of STONE shows a fluctuating pattern as the number of anchors varies. Interestingly, the performance curve does not monotonically increase with the number of anchors, which means that choosing a smaller number of discriminant anchors is preferable to choosing a larger number of non-discriminant anchors. In addition, the best clustering quality can be obtained by using \(c\) or \(2c\) anchor points in STONE, which shows that the coordination between the EAD and the AHR improves the discrimination of the anchor points.

**Sensitivity Analysis of Balancing Parameters:** To assess the importance of the balancing parameters \(\), \(\), and \(\) in the STONE model, we implement a grid search strategy across the range of [1e-5, 1e+1] to optimize these parameters. Figure 5 demonstrates how the model's performance changes with various combinations of these parameters, highlighting fluctuations in clustering performance based on the chosen values. Notably, optimal performance can be achieved through careful tuning, suggesting that the modules within the STONE model can effectively coordinate their importance to extract valuable information, thereby enhancing clustering performance.

### Convergence Behavior

This subsection provides an experimental validation of the convergence of the STONE model, utilizing two key metrics: reconstruction error (RE), defined as \(=_{v=1}^{m}\|^{v}-^{v}(^{v})^{ }-^{v}^{v}-^{v}\|_{}\) and matching error (ME), represented as \(=\|}-}\|_{}\). The iterative trends observed on the NGs, HW, and MSRCV1 datasets, as depicted in Figure 6, demonstrate that both RE and ME exhibit rapid convergence to 0 within 15 iterations, followed by stabilization. This outcome substantiates the robust convergence properties of the STONE method.

### Ablation Study

Comprehensive ablation experiments are carried out in this subsection to systematically assess the contributions of various modules within the STONE model. Here, we assign the values of the balancing parametersset \(\), \(\), and \(\)--governing the loss terms \(_{EAD}\), \(_{RE}\), and \(_{AHR}\), respectively--to 0, essentially isolating and removing each loss term separately from the STONE model. The experimental results for the NGs, MSRCV1 and HW datasets are presented in Table 4, with checkmarks denoting the consideration of the corresponding loss. The best-performing results are indicated in **bold**. Table 4 reveals

   &  &  &  \\  \(_{EAD}\) & \(_{RE}\) & \(_{AHR}\) & ACC & NMI & ACC & NMI & ACC & NMI \\   & & & 0.438 & 0.291 & 0.148 & 0.030 & 0.988 & 0.974 \\  & & & 0.208 & 0.012 & 0.205 & 0.033 & 0.977 & 0.951 \\  & & & 0.596 & 0.473 & 0.148 & 0.030 & 0.988 & 0.974 \\  & & & 0.960 & 0.897 & 0.976 & 0.946 & 0.881 & 0.841 \\  & & & 0.534 & 0.397 & 0.786 & 0.631 & 0.983 & 0.971 \\  & & & 0.458 & 0.311 & 0.571 & 0.384 & 0.854 & 0.841 \\  & & & **1.000** & **1.000** & **1.000** & **1.000** & **1.000** & **1.000** \\  

Table 4: Analysis of STONE Model Ablation.

Figure 5: Sensitivity Analysis of the STONE Model to the Balance Parameters \(\), \(\) and \(\).

that the clustering performance of degraded models, achieved by removing one or two submodules from the STONE model, is notably inferior to that of the complete STONE model. This emphasizes the successful collaboration of \(_{EAD}\), \(_{RE}\), and \(_{AHR}\) within the STONE framework, allowing them to synergistically exploit the abundant information embedded in multi-view data and attain commendable clustering performance. Additionally, HTR is a novel tensor low-rank constraint in our STONE model, aimed at capturing high-order correlations and managing variations in tensor singular values. To assess its impact, we conduct an ablation study comparing the original STONE model with a version that excludes the HTR module (referred to as STONE-v1). Table 5 shows the clustering ACC across different datasets, revealing a drop in performance on all datasets when the HTR module is removed. This suggests that the integration of HTR enhances the exploration of high-order correlations, thereby improving the quality of data partitioning.

## 5 Conclusion

This paper introduces a novel tensor-based multi-view subspace clustering framework that integrates triple information enhancement from dictionary to tensor representation. Through the design of the enhanced anchor dictionary representation, hyperbolic tangent rank, and anchored hypergraph Laplacian regularization, our model extensively investigates valuable insights within multi-view data. Experimental results demonstrate that the STONE model outperforms SOTA models on eight datasets in terms of both effectiveness and efficiency.