# Epistemic Uncertainty and Observation Noise

with the Neural Tangent Kernel

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recent work has shown that training wide neural networks with gradient descent is formally equivalent to computing the mean of the posterior distribution in a Gaussian Process (GP) with the Neural Tangent Kernel (NTK) as the prior covariance and zero aleatoric noise . In this paper, we extend this framework in two ways. First, we show how to deal with non-zero aleatoric noise. Second, we derive an estimator for the posterior covariance, giving us a handle on epistemic uncertainty. Our proposed approach integrates seamlessly with standard training pipelines, as it involves training a small number of additional predictors using gradient descent on a mean squared error loss. We demonstrate the proof-of-concept of our method through empirical evaluation on synthetic regression.

## 1 Introduction

Jacot et al. have studied the training of wide neural networks, showing that gradient descent on a standard loss is, in the limit of many iterations, formally equivalent to computing the posterior mean of a Gaussian Process (GP), with the prior covariance specified by the Neural Tangent Kernel (NTK) and with zero aleatoric noise. Crucially, this insight allows us to study complex behaviours of wide networks using Bayesian nonparametrics, which are much better understood.

We extend this analysis by asking two research questions. First, we ask if a similar equivalence exists in cases where we want to do inference for arbitrary values of aleatoric noise. This is crucial in many real-world settings, where measurement accuracy or other data-gathering errors mean that the information in our dataset is only approximate. Second, we ask if it is possible to obtain an estimate of the posterior covariance, not just the mean. Since the posterior covariance measures the epistemic uncertainty about predictions of a model, it is crucial for problems that involve out-of-distribution detection or training with bandit-style feedback.

We answer both of these research questions in the affirmative. Our posterior mean estimator takes the aleatoric noise into account by adding a simple squared norm penalty on the deviation of the network parameters from their initial values, shedding light on regularization in deep learning. Our covariance estimator can be understood as an alternative to existing methods of epistemic uncertainty estimation, such as dropout , the Laplace approximation , epistemic neural networks , deep ensembles  and Bayesian Neural Networks . Unlike these approaches, our method has the advantage that it can approximate the NTK-GP posterior arbitrarily well.

ContributionsWe derive estimators for the posterior mean and covariance of an NTK-GP with non-zero aleatoric noise, computable using gradient descent on a standard loss. We evaluate our results empirically on a toy repression problem.

## 2 Preliminaries

Gaussian ProcessesGaussian Processes are a popular non-parametric approach for modeling distributions over functions . Given a dataset of input-output pairs \(\{(_{i},y_{i})\}_{i=1}^{N}\), a GP represents uncertainty about function values by assuming they are jointly Gaussian with a covariance structuredefined by a kernel function \(k(,^{})\). The GP prior is specified as \(f()(m(),k(,^{}))\), where \(m()\) is the mean function and \(k(,^{})\) is the kernel. Assuming \(y_{i}(f(),^{2})\) and given new test points \(^{}\), the posterior mean and covariance are given by:

\[_{p}(^{}) =m(^{})+(^{},)^{}((,)+^{2})^{-1}( -m()),\] (1) \[_{p}(^{}) =(^{},^{})-( ^{},)^{}((,)+ ^{2})^{-1}(^{},),\] (2)

where \((,)\) is the covariance matrix computed over the training inputs, \((^{},)\) is the covariance matrix between the test and training points, and \(^{2}\) represents the aleatoric (or observation) noise.

Neural Tangent Kernel.The Neural Tangent Kernel (NTK) characterizes the evolution of wide neural network predictions as a linear model in function space. Given a neural network function \(f(;)\) parameterized by \(\), the NTK is defined through the Jacobian \(J()^{N p}\), where \(J()=;)}{}\), \(N\) is the number of data points and \(p\) is the number of parameters. The NTK at two sets of inputs \(\) and \(^{}\) is given by:

\[(,^{})=J()J(^{ })^{}.\] (3)

Interestingly, as shown by  the NTK converges to a deterministic kernel and remains constant during training in the infinite-width limit. We call a GP with the kernel (3) the NTK GP.

## 3 Method

We now describe our proposed process of doing inference in the NTK-GP. Our procedure for estimating the posterior mean is given in Algorithm 1, while the procedure for the covariance is given in Algorithm 2. Note that our process is scaleable because both algorithms only use gradient descent, rather than relying on a matrix inverse in equations (1) and (2). While Algorithm 2 relies on the computation of the partial SVD of the Jacobian, we stress that efficient ways of doing so exist and do not require ever storing the full Jacobian. We defer the details of the partial SVD to Appendix E. We describe the theory that justifies our posterior computation in sections 3.1 and 3.2. We defer the discussion of literature to Appendix A.

``` procedureTrain-Posterior-Mean(\(x_{i},y_{i}\), \(_{0}\)) \(_{i} y_{i}+f(x_{i};_{0})\)\(\) Shift the targets to get zero prior mean (Lemma 3.2). \(L_{i=1}^{N}(_{i}-f(x_{i};))^{2}+_{N} \|-_{0}\|_{2}^{2}\)\(\) Equation (4) minimize \(L\) with gradient descent wrt. \(\) until convergence to \(^{}\) return\(^{}\)\(\) Return the trained weights. endprocedureQuery-Posterior-Mean(\(x^{}_{j}\), \(^{}\), \(^{0}\))\(\)\(j=1,,J\) return\(f(x^{}_{1};^{})-f(x^{}_{1};^{0}),,f(x^{}_{J}; ^{})-f(x^{}_{1};^{0})\) endprocedure ```

**Algorithm 1** Algorithm for Computing the Posterior Mean in the NTK-GP

### Aleatoric Noise

Gradient Descent Converges to the NTK-GP Posterior MeanWe build on the work of  by focusing on the computation of the mean posterior in the presence of **non-zero aleatoric noise**. We show that optimizing a regularized mean squared error loss in a neural network is equivalent to computing the mean posterior of an NTK-GP with non-zero aleatoric noise. In the following Lemma, we prove that for a sufficiently long training process, the predictions of the trained network converge to those of an NTK-GP with aleatoric noise characterized by \(^{2}=N_{N}\). This is a similar result to , but from a Bayesian perspective rather than a frequentist generalization bound. Furthermore, our proof (see Appendix B) focuses on explicitly solving the gradient flows for test and training data points in function space.

**Lemma 3.1**.: _Consider a parametric model \(f(x;)\) where \(x^{N}\) and \(^{p}\), initialized under some assumptions with parameters \(_{0}\). Minimizing the regularized mean squared error loss with respect to \(\) to find the optimal set of parameters \(^{*}\) over a dataset \((,)\) of size \(N\), and with sufficient training time (\(t\)):_

\[^{*}=*{arg\,min}_{^{p}}_{i= 1}^{N}(y_{i}-f(x_{i};))^{2}+_{N}||-_{0}||_{2}^{2},\] (4)

_is equivalent to computing the mean posterior of a Gaussian process with non-zero aleatoric noise, \(^{2}=N_{N}\), and the NTK as its kernel:_

\[f(^{};_{})=f(^{};_{0})+ (^{},)((,)+ N_{N})^{-1}(-f(;_{0})).\] (5)

Zero Prior MeanIn many practical scenarios, it is desirable to start with zero prior mean rather than with a prior mean that corresponds to random network initialization. To accommodate this, we introduce a simple yet effective transformation of the data and the network outputs, to be applied together with 3.1. We summarize it into the following lemma (see Appendix B for proof):

**Lemma 3.2**.: _Consider the computational process derived in Lemma 3.1. Define shifted labels \(}\) and predictions \((;_{})\) as follows::_

\[}=+f(;_{0}),( ;_{})=f(;_{})-f(^{ };_{0}).\]

_Using these definitions, the posterior mean of a zero-mean Gaussian process can be computed as:_

\[(^{},_{})=(^{ },)((,)+N_{N})^{-1} .\] (6)

``` procedureTrain-Posterior-Covariance(\(x_{i}\), \(K\), \(_{0}\)) \(\)\(K\) is the number of predictors \(U,(J_{_{0}}(),K)\)\(\) Partial SVD of the Jacobian - see appendix E. for\(i=1,,K\)do\(_{i}^{*}(x_{i},U_{i})\)\(\)\(U_{i}\) is the \(i\)-th column of \(U\). endfor for\(i=1,,K^{}\)do\(\) Setting \(K^{}=0\) often works well (see Appendix D). \({^{}}_{i}^{*}(x_{i},_{i})\)\(\)\(_{i}(0,^{2}I)\) endfor return\(,_{1}^{*},,_{K}^{*},{^{}}_{1}^{*},,{^{}}_{K^{ }}^{*}\) endprocedureprocedureQuery-Posterior-Covariance(\(x^{}_{j}\), \(\), \(_{i}^{*}\), \(_{i}^{}\), \(_{0}\)) \(\)\(j=1,,J\) \(P_{}\)\([f(x^{}_{1};_{1}^{*})-f(x^{}_{1}; _{0})&&f(x^{}_{1};_{K}^{*})-f(x^{}_{1};_{0} )&&f(x^{}_{1};^{}_{K^{}})-f(x^{}_{1} ;_{0})\\ &&&&\\ f(x^{}_{j};_{1}^{*})-f(x^{}_{j};_{0})&&f(x^{ }_{j};_{K}^{*})-f(x^{}_{1};_{0})&&f(x^{}_{ j};^{}_{K^{}})-f(x^{}_{1};_{0})]\) return\(J(^{})J(^{})^{}-P^{2}P^{}-P^{}(P^{ })^{}/K^{}\)\(\) The last term vanishes for \(K^{}=0\) endprocedure ```

**Algorithm 2** Algorithm for Computing the Posterior Covariance in the NTK-GP

### Estimating the Covariance

We now justify Algorithm 2 for estimating the posterior covariance. The main observation that allows us to derive our estimator comes from examining the term \((^{},)^{}((, )+^{2})^{-1}(^{},)\) in the posterior covariance formula (2). This is summarized in the following Proposition.

**Proposition 3.1**.: _Diagonalize \((,)\) so that \((,)=U U^{}\). We have_

\[(^{},)^{}((, )+^{2})^{-1}(^{},)=(MU)(MU)^{}+^{2}MM^{}.\]

_Here, \(M=(^{},)^{}((, )+^{2})^{-1}\)._

Proof.: We can rewrite it as:

\[(^{},)^{}(( ,)+^{2})^{-1}(^{}, )=\] \[(^{},)^{}( (,)+^{2})^{-1}(( ,)+^{2})}_{M}((, )+^{2})(,)+ ^{2})^{-1}(^{},)}_{M^{}}\]Denoting the term \((^{},)^{}((, )+^{2})^{-1}\) with \(M\), this can be written as:

\[(^{},)^{}((, )+^{2})^{-1}(^{},)=(MU)(MU)^{}+^{2}MM^{}.\]

The proposition is useful because the matrix \(M\) appears in equation (1). Hence the matrix multiplication \(MU\) is equivalent to estimating the posterior mean using algorithm 1 where targets are given by the columns of the matrix \(U\). Hence the term \((MU)(MU)^{}\) can be computed by gradient descent. In order to derive a complete estimator of the covariance, we still need to deal with the term \(^{2}MM^{}\). We can either estimate this term by fitting random targets (which corresponds to setting \(K^{}>0\) in algorithm 2) or accept an upper bound on the covariance, setting \(K^{}=0\). We describe this in detail in Appendix D.

## 4 Experiment

We applied the method to a toy regression problem shown in Figure 1. The problem is a standard non-linear 1d regression task which requires both interpolation and extrapolation. The top-left figure was obtained by computing the kernel of the NTK-GP using formula (3) and computing the posterior mean and covariance using equations (1) and (2). The top-right figure was obtained by analytically computing the upper bound defined in appendix D. The bottom-left figure was obtained by taking the first 5 eigenvectors of the kernel. Finally, the bottom-right figure was obtained by fitting a mean prediction network and 5 predictor networks using the gradient-descent method described in algorithm 2. The similarity of the figures shows that the method works. Details of network architecture are deferred to Appendix C.

## 5 Conclusions

This paper introduces a method for computing the posterior mean and covariance of NTK-Gaussian Processes with non-zero aleatoric noise. Our approach integrates seamlessly with standard training procedures using gradient descent, providing a practical tool for uncertainty estimation in contexts such as Bayesian optimization. The method has been validated empirically on a toy task, demonstrating its effectiveness in capturing uncertainty while maintaining computational efficiency. This work opens up opportunities for further research in applying NTK-GP frameworks to more complex scenarios and datasets.

Figure 1: The NTK-GP posterior and its approximations: (top-left) Analytic Posterior, (top-right) Analytic upper bound on posterior (all eigenvectors), (bottom-left) Analytic upper bound on posterior (5 eigenvectors), (bottom-right) Posterior obtained with gradient descent (\(K=5\) predictors, \(K^{}=0\)).