# Graph-Structured Gaussian Processes for

Transferable Graph Learning

 Jun Wu\({}^{1}\), Elizabeth Ainsworth\({}^{1,2}\), Andrew Leakey\({}^{1}\), Haixun Wang\({}^{3}\), Jingrui He\({}^{1}\)

\({}^{1}\)University of Illinois at Urbana-Champaign

\({}^{2}\)USDA ARS Global Change and Photosynthesis Research Unit

{junwu3,ainswort,leakey,jingrui}@illinois.edu, {haixun}@gmail.com

###### Abstract

Transferable graph learning involves knowledge transferability from a source graph to a relevant target graph. The major challenge of transferable graph learning is the distribution shift between source and target graphs induced by individual node attributes and complex graph structures. To solve this problem, in this paper, we propose a generic graph-structured Gaussian process framework (GraphGP) for adaptively transferring knowledge across graphs with either homophily or heterophily assumptions. Specifically, GraphGP is derived from a novel graph structure-aware neural network in the limit on the layer width. The generalization analysis of GraphGP explicitly investigates the connection between knowledge transferability and graph domain similarity. Extensive experiments on several transferable graph learning benchmarks demonstrate the efficacy of GraphGP over state-of-the-art Gaussian process baselines.

## 1 Introduction

Transfer learning  aims at studying the transfer of knowledge or information from a source domain to a relevant target domain under distribution shifts. The knowledge transferability across domains has been investigated from various aspects. For example,  proposed to empirically estimate the data-level domain discrepancy for measuring the transferability across graphs.  adopted adaptive Gaussian processes with transfer kernels, which formulated the task relatedness as a hyper-parameter for tuning.  instead evaluated the transferability of a pre-trained source model in the target domain. However, most existing works followed the IID assumption that samples are independent and identically distributed in each domain. This might limit their capacities in understanding the knowledge transferability across domains with non-IID data, e.g., cross-domain recommendation , cross-network role identification , etc.

In this paper, we focus on the problem of transferable graph learning over non-IID graph data. The challenge of transferable graph learning induced by the graph structure information can be explained in Figure 1. It can be seen that Figure 1(a) provides source and target samples associated with input features and output class labels (indicated by node colors). Suppose that source and target

Figure 1: Illustration of transferable graph learning. (a) Samples are independently drawn. (b) Samples are connected in each domain, and both domains follow the homophily assumption. (c) Source and target domains follow different graph structure assumptions.

distributions have been aligned (e.g., optimized by domain discrepancy minimization algorithms [15; 67]), Figure 1(a) implies that source and target domains have similar data distributions in the feature space, thereby enabling feasible knowledge transfer across domains. Compared to Figure 1(a), Figure 1(b)(c) introduce additional graph structure information. In Figure 1(b), source and target graphs follow similar structures where nodes within the same class tend to be connected (i.e., the homophily assumption ). In contrast, the structures of source and target graphs fundamentally differ in Figure 1(c). That is, though the target graph follows the homophily assumption, the source graph holds the heterophily assumption that connected nodes may have different classes and dissimilar node attributes . Therefore, compared to standard transfer learning [31; 40] with IID data, transferable graph learning is much more challenging because knowledge transferability across graphs can be determined by both individual node features and complex graph structures.

In recent years, transferable graph learning has been studied [55; 58; 66; 69] where both source and target graphs follow the homophily assumption (Figure 1(b)). However, little effort has been devoted to investigating the knowledge transferability between homophilic and heterophilic graphs (Figure 1(c)). To bridge this gap, in this paper, we focus on studying the knowledge transferability across graphs with either the same (e.g., homophily or heterophily) or different assumptions.

We start by introducing a universal structure-aware neural network to build the input-output relationship between structure-aware input sample \((v,G)\) and its associated output value \(y_{v}\) for tackling homophilic and heterophilic graphs. This model encodes both local node representation (sample-level) and global graph representation (domain-level) simultaneously. The crucial idea is to automatically select relevant neighborhoods by adding learnable weights to neighborhood aggregation. Thus, it enables knowledge transferability between homophilic and heterophilic graphs by adaptively selecting neighborhoods for each graph. The intuition is that source and target graphs might have common knowledge from the selected neighborhoods, e.g., nearby neighborhoods for the homophilic graph and high-order neighborhoods for the heterophilic graph. Then, we show the equivalence between structure-aware neural network and graph Gaussian process in the limit on the layer width. This observation sheds light on building graph-structured Gaussian processes (GraphGP) for transferable graph learning. Moreover, the generalization analysis of GraphGP shows the connection between knowledge transferability and graph domain similarity. Compared to previous works [45; 58; 69], GraphGP benefits from (i) feasible knowledge transferability between homophily and heterophily graphs, and (ii) flexible incorporation with existing graph neural networks and graph kernels. Extensive experiments on node regression tasks demonstrate the effectiveness of GraphGP over state-of-the-art Gaussian process baselines. The main contributions of this paper are summarized as follows.

* A generic graph-structured Gaussian process framework GraphGP is proposed for transferable graph learning. It is derived from a structure-aware neural network encoding local node representation (sample-level) and global graph representation (domain-level) simultaneously.
* We show that GraphGP tackles the knowledge transferability in homophily and heterophily graphs using a simple neighborhood selection strategy. In addition, we theoretically analyze the knowledge transferability across graphs from the perspective of graph domain similarity.
* Experimental results on a variety of graph learning benchmarks demonstrate the efficacy of GraphGP over state-of-the-art Gaussian process baselines.

The rest of this paper is organized as follows. Section 2 summarizes the related work, and Section 3 provides the problem definition of transferable graph learning. In Section 4, we present the graph-structured Gaussian process (GraphGP) for transferable graph learning. The experimental results are provided in Section 5. Finally, we conclude the paper in Section 6.

## 2 Related Work

**Transfer Learning:** Transfer learning [40; 54] tackles the transfer of knowledge or information from a source domain to a relevant target domain. It is theoretically shown [3; 67] that the generalization performance of a learning algorithm can be improved by leveraging latent knowledge from the source domain. Specifically, the generalization error of a learning algorithm on the target domain can be bounded in terms of the source knowledge and the distribution discrepancy across domains. Recently, transfer learning has been explored in non-IID graph data [7; 55; 58; 64; 69]. However, most existing works focus on either investigating the transferability of a pre-trained graph neural network [27; 45; 69] or designing graph transfer learning algorithms [58; 66] with relaxed assumptions (e.g., \(k\)-hop ego-graphs are assumed to be independent and identically distributed ).

**Gaussian Process:** Gaussian process [9; 42] provides a principled nonparametric framework for learning stochastic functions from observations. It has been applied to transfer learning when samples in each domain are independent and identically distributed [8; 49; 56]. The crucial idea is to infer the task relatedness between source and target domains using available labeled source and target examples. More recently, Gaussian processes have also been investigated in graph learning tasks, e.g., link prediction [14; 38; 65] and node regression [6; 21; 28; 35; 37; 39]. However, little effort has been devoted to understanding transferable graph learning from Gaussian process perspectives.

## 3 Preliminaries

### Notation and Problem Definition

Suppose that a graph is represented as \(G=(V,E)\), where \(V=\{v_{1},,v_{|V|}\}\) is the set of \(n\) nodes and \(E V V\) is the edge set in the graph. Each node \(v V\) can be associated with a \(D\)-dimensional feature vector \(_{v}^{D}\). In the node regression tasks, the ground-truth output of each node is denoted by \(y_{v}\). The structure of a graph \(G\) can also be represented by an adjacency matrix \(^{|V||V|}\), where \(_{ij}\) is the edge weight between \(v_{i}\) and \(v_{j}\) within the graph. Following [55; 69], we introduce a generic problem setting of transferable graph learning as follows.

**Definition 3.1** (_Transferable Graph Learning_).: Given a source graph \(G_{s}=(V_{s},E_{s})\) and a target graph \(G_{t}=(V_{t},E_{t})\), _transferable graph learning_ aims to improve the prediction performance of a graph learning algorithm in the target graph using latent knowledge from a relevant source graph.

### Message-Passing Graph Neural Networks

Weisfeiler-Lehman (WL) graph subtree kernel  measures the similarity of two input graphs, inspired by the Weisfeiler-Lehman test of isomorphism . The crucial component of WL subtree kernel is to recursively represent the subtree structure rooted at each node. This has motivated a variety of message-passing graph neural networks (GNN), which recursively learn the node presentation of node \(v\) by aggregating the feature vectors from \(v\)'s local neighborhood [17; 18; 20; 57; 59]. Generally, a graph convolutional layer of message-passing GNNs can be summarized as follows.

\[ h_{v}^{(l+1)}=^{(l)} {h_{v}^{(l)}}_{}& _{1}^{(l)}}_{1},\\ &,_{k}^{(l)}}_{k}\] (1)

where \(^{(l)}\) function compresses the representations from the node itself and its neighbors at the \(l^{}\) graph convolutional layer into a single representation, and \(_{j}^{(l)}\) (\(j=1,,k\)) function aims to aggregate message from \(j\)-order neighborhood \(N_{j}(v)\) of node \(v\). It indicates that there are two major components for designing graph convolutional layers in GNNs: neighborhood selection and aggregation. More specifically, neighborhood aggregation allows compressing the graph structure and node attributes simultaneously, while neighborhood selection provides the flexibility of GNNs in tackling graphs with different assumptions (e.g., homophily  or heterophily ).

**Homophily graphs:** Homophily graph holds the assumption that nodes within the same class tend to be connected in the graph, e.g., citation networks , social networks , etc. This assumption has motivated various instantiations of message-passing graph neural networks over first-order neighborhoods, e.g., GCN , GraphSAGE , GAT , GIN .

**Heterophily graphs:** Heterophily graph holds that connected nodes may have different class labels and dissimilar node attributes, e.g., different classes of amino acids are more likely to connect within protein structures [30; 68]. Recently, by adaptively exploring the potential homophily in a high-order local neighborhood (e.g., \(k>1\)), message-passing graph neural networks have been proposed for heterophily graphs, e.g., MixHop , H2GCN , GPR-GNN , HOG-GCN , GloGNN .

Methodology

In this section, we propose the graph-structured Gaussian processes for transferable graph learning.

### Structure-Aware Neural Network

We start by showing the graph sampling process from a probability distribution space \(\). A graph \(G=(V,E)\) is sampled by first sampling a graph domain distribution \(_{}\) from \(\), and then sampling a specific graph \(G\) from \(_{}\). More specifically, one realization from \(\) is a graph domain distribution \(_{}\) (also denoted as \(_{}=(_{},_{})\) for node and edge sampling distribution) characterizing a two-stage graph sampling as follows. A set of nodes \(V\) is sampled from the graph distribution \(_{}\) (more specifically, sampled from \(_{}\)), and then the edge weight is sampled from \(_{}\) (more specifically, sampled from \(_{}\)) over any pair of nodes in \(V\). In addition to the graph structure (induced by the node dependence), the output label of a node can also be sampled accordingly in the context of node regression tasks. That is, a label-informed graph domain distribution \(_{,}\) is sampled from \(\). Next, a graph \(G=(V,E)\) is sampled from \(_{}=(_{},_{})\), and then the output labels of nodes are sampled from \(_{|}\). The goal of node regression task [55; 69] is to learn a prediction function \(f()\) that predicts \(_{v}=f(v,G)\) for each node \(v V\).

In this paper, we consider the covariate shift assumption  over graphs. That is, the source and target graphs share conditional distribution \(_{|}(y|(v,G))\) but different marginal distributions \(_{}(v,G)\). The aforementioned graph sampling process motivates us to learn the input-output relationship between an input sample \((G,v)\) and its associated output \(y_{v}\) for transferable graph learning. To this end, for each input graph \(G\) (e.g., source \(G_{s}\) or target graph \(G_{t}\) in transferable graph learning), we define a structure-aware neural network \(f(,)\) as follows.

\[& f_{i}^{(l)}(v,G)=}_{j=1}^{M} _{ij}^{(l)}_{j}^{(l)}(v|G)_{j}^{(l)}(G)\\ &^{(l)}(v|G)=h_{v}^{(l)}^{(l)}(G)= (_{v}^{(l)}|v V)\] (2)

where \(M\) is the layer width1. \(^{(l)}(v|G)\) denotes the node (sample-level) representation of \(v\) given the graph \(G\) at the \(l\)th layer, and \(^{(l)}(G)\) denotes the graph (domain-level) representation of the graph \(G\). Here READOUT function compresses a set of node representations into a single vector representation [59; 62]. This definition indicates that the embedding representation of the input \((v,G)\) (e.g., a pair of a node \(v\) and its associated graph \(G\)) is given by two crucial components: an individual node embedding representation \(^{(l)}(v|G)\) and a global graph embedding representation \(^{(l)}(G)\). \(h_{v}^{(l)}\) and \(_{v}^{(l)}\) represent graph neural networks to learn node and graph representations separately.

The intuition behind this definition is explained below. The graph \(G\) is a realization of a graph domain distribution \(_{}\). Then, the embedding representation of the graph \(G\) can be explained as an empirical estimate of the distribution \(_{}\), i.e., \((G)(_{})\). Moreover, when the size of the graph \(G\) goes to infinity (i.e., associated with an infinite number of nodes), \(G\) approximates the true distribution \(_{}\) and the embedding representation of the graph \(G\) will be able to recover the true embedding of probability distribution \(_{}\)[19; 48]. Besides, the embedding representation of a node \(v\) can also be explained as an approximation of \(_{}\) at the location \(v\), i.e., \((v|G)(v|_{})\).

**Remark.** The structure-aware neural network Eq. (2) is defined over message-passing graph neural networks commonly used for various single-domain graph learning tasks [18; 20; 23; 59]. Different from previous works, we focus on transferable graph learning scenarios across graphs. This motivates us to consider the task relatedness between source and target graphs, which can be explicitly measured by the domain distribution similarity [15; 56]. To this end, we design the structure-aware neural network Eq. (2) for learning domain-aware node presentation, i.e., the integration of the node (sample-level) representation and the entire graph (domain-level) representation. Specifically, we can show the connection between graph domain distribution and graph representation learning in the reproducing kernel Hilbert space for cross-graph learning tasks (see Corollary 4.4).

### Graph-Structured Gaussian Process

Inspired by [11; 52; 68], we derive a universal message-passing graph neural network Eq. (1) for both homophily and heterophily graphs. More specifically, the message-passing graph convolutional layer of Eq. (1) for node \(v\) within an input graph \(G\) can be defined as follows.

\[_{v}^{(l)}=_{i=0}^{k}_{i}(}_{u  N_{i}(v)}_{}^{(l)}_{u}^{(l)}+_ {}^{(l)})_{u}^{(l)}= (_{u}^{(l-1)})\] (3)

where \(_{u}^{(0)}=_{u}^{D}\), \(N_{0}(v)=\{v\}\) denotes the seed node, \(_{}^{(l)},_{}^{(l)}\) are the weight and bias parameters, and \(_{i}\) explicitly indicates the importance of the \(i\)-order neighborhood in finding relevant neighbors around node \(v\). Similarly, we define the domain-level graph neural layer \(}_{v}^{(l)}\) parameterized by different \(}_{}^{(l)},}_{}^ {(l)}\) and shared \(_{i}\).

\[}_{v}^{(l)}=_{i=0}^{k}_{i}(} _{u N_{i}(v)}}_{}^{(l)} }_{u}^{(l)}+}_{}^{(l)}) }_{u}^{(l)}=(}_{u}^{(l-1)})\] (4)

Generally, it is flexible to apply different model architectures to learn the node (sample-level \(^{(l)}(v|G)\)) and graph (domain-level \(^{(l)}(G)\)) representations. For simplicity, we adopt the same model architecture but different model parameters to learn node-level and graph-level representations. It is noteworthy that the neighborhood importance \(_{i}\) is shared because both node and graph representation would share the same assumption (homophily or heterophily) for a given graph. Then, the structure-aware neural network Eq. (2) can be given by

\[f_{i}^{(l)}(v,G) =}_{j=1}^{M}_{ij}^{(l)}_{ j}^{(l)}(v|G)_{j}^{(l)}(G)\] (5) \[^{(l)}(v|G)=_{v}^{(l)} ^{(l)}(G)=_{v V}}_{v}^{(l)}\]

where the READOUT function of Eq. (2) is instantiated with mean pooling . Different from previous works [11; 52; 68], we would take the neighborhood importance scores \(_{i}\) as hyper-parameters for building transferable graph Gaussian process model as follows.

**Theorem 4.1**.: _Assuming that all the parameters of structure-aware graph neural network \(f(v,G)\) are independent and randomly drawn from Gaussian distributions, i.e., \(^{(l)}(,_{w}^{2}),_{}^{(l)}(,_{b}^{2}), _{}^{(l)}(,_{w}^{2} ),}_{}^{(l)}(,_{b}^{2}),}_{}^{(l)} (,_{w}^{2}),\) when the layer width \(M\) goes to infinity, the output function \(f_{i}^{(l)}\) in Eq. (5) follows a Gaussian process with \(f_{i}^{(l)}(0,K^{(l)})\), where the covariance function \(K^{(l)}\) is given by_

\[K^{(l)}((v,G),(v^{},G^{}))=_{w}^{2} K_{}^ {(l)}(v,v^{}|G,G^{}) K_{}^{(l)}(G,G^{ })\]

_where_

\[K_{}^{(l)}(v,v^{}|G,G^{}) =_{i,j=0}^{k}_{i}_{j}^{}(_{b} ^{2}+_{w}^{2}_{u N_{i}(v)}_{u^{} N_{j}(v^{ })}_{uu^{}}^{(l-1)}(_{w}^{2},_{b}^{2}))\] \[K_{}^{(l)}(G,G^{}) =_{i,j=0}^{k}_{i}_{j}^{}(_{b}^{2}+_{w}^{2}}{|V||V^{}|} ^{T}^{(i)}^{(l-1)}(_{w}^{2}, _{b}^{2})(^{(j)})^{T})\] \[^{(l-1)}(a,b) =_{z_{i}^{(l-1)}(0,K_{ab}^{(l-1)} )}[(z_{i}^{(l-1)})(z_{i}^{(l-1)})^{T}]\]

_Here \(^{(i)}\) (\(^{(j)}\)) denotes the adjacency matrix given by the \(i\)-order neighborhood from graph \(G\) (\(j\)-order neighborhood from graph \(G^{}\)). \(K_{ab}^{(l-1)}=_{i,j=0}^{k}_{i}_{j}^{}(b^{2}+a^{2} ^{(i)}^{(l-2)}(a,b)(^{(j)})^{T})\) and \(_{uu^{}}^{(0)}(a,b)=_{u},_{u^{ }}\) for any \(u V,u^{} V^{}\)._

#### 4.2.1 Implications of Theorem 4.1

In the following, we show that the sample-level covariance/kernel \(K_{}^{(l)}\) of Theorem 4.1 can be explained as a message-passing operation in the kernel space. Following , when using ReLU as activation function \(()\) (i.e., \((x)=\{0,x\}\)), \(^{(l-1)}\) is the arc-cosine kernel  as follows.

\[_{uu^{}}^{(l-1)}(_{w}^{2},_{}^{2})=()}{2}^{(l-1)}(u,u|G) K _{}^{(l-1)}(u^{},u^{}|G^{})}\]

where \(=^{(l-1)}(u,u^{}|G,G^{})}{^{(l-1)} (u,u|G) K_{}^{(l-1)}(u^{},u^{}|G^{} )}}\) and \(_{1}()=((-() )+})\). Then, the following corollary derives the feature map of \(K_{}^{(l)}(u,u^{}|G,G^{})\) in the kernel space.

**Corollary 4.2**.: _Let \(_{1}:_{1}\) denote the kernel mapping from a pre-activation RKHS \(\) to post-activation RKHS \(_{1}\), i.e., \(_{1}(s),_{1}(s^{})=||s||||s ^{}||_{1}(}{||s|| ||s^{}||})\). Given the sample-level kernel \(K_{}^{(l)}(v,v^{}|G,G^{})\) in Theorem 4.1, if the graph convolutional layer of Eq. (3) has no bias term (\(_{b}=0\)), the feature map of this kernel is given by_

\[_{v}^{(l)}=_{w}_{i=0}^{k}_{i}_{u N_{i}(v)} _{1}(_{u}^{(l-1)})_{v^{} }^{(l)}=_{w}_{i=0}^{k}_{i}^{}_{u^{} N_{ i}(v^{})}_{1}(_{u^{}}^{(l-1)})\]

_with \(K_{}^{(l)}(v,v^{}|G,G^{})=_{v}^{(l)},_{v^{ }}^{(l)}\)._

Corollary 4.2 shows that the feature map \(_{v}^{(l)}\) of sample-level kernel \(K_{}^{(l)}\) is a kernelized version of the graph convolutional layer in Eq. (3), which recursively aggregates message from nodes' neighborhood in the kernel space. The neighborhood can be automatically selected by optimizing the hyper-parameters \(_{i}\) and \(_{i}^{}\).

Next, we show the implication of domain-level kernel \(K_{}^{(l)}(G,G^{})\). As discussed in Subsection 4.1, a graph \(G\), empirically drawn from graph domain distribution \(_{}\), can approximate the true distribution \(_{}\) when increasing the size of graph \(G\). This motivates us to understand the connection between empirical domain representation \(^{(l)}(G)\) and the embedding of probability distribution \(_{}\).

**Definition 4.3** (Mean Embedding of Probability Distributions ).: Given an RKHS \(\) induced by a kernel \(k(,)\), the mean embedding of a probability distribution \(\) is defined as \(_{}= k(,)d()\), i.e., \(_{}[()]= f,_{ }_{}\) for all \(f\). Furthermore, the empirical mean embedding of \(\) is given by \(_{}=_{i=1}^{m}k(,_{i})\), where \(m\) samples are independent and identically drawn from \(\).

Analogously, the mean embedding of a graph domain distribution \(_{}\) can also be defined as \(_{_{}}= k_{}(,v|_{ })d_{}(v)\) given an RKHS \(_{}\) induced by a kernel \(k_{}(,)\). In the context of graph learning, \(v|_{}\) indicates the individual node attributes of \(v\) and graph structure information around node \(v\) induced by \(_{}\). It subsumes the conventional mean embedding  of a probability distribution \(_{}\) with independent and identically distributed (IID) samples \(\), if there are no edges in the graph (i.e., no graph structure is involved around node \(v\)). It is observed that the additional graph structure challenges empirical estimation of graph domain distribution \(_{}\), thereby resulting in a nontrivial domain similarity estimator between source and target graphs in transferable graph learning. The following corollary states that the graph representation \(^{(l)}(G)\) can be considered as the empirical mean embedding of \(_{_{}}\) in the RKHS induced by \(K_{}^{(l)}(,)\).

**Corollary 4.4**.: _With the same conditions in Theorem 4.1, for each \(l\), in the limit on the layer width, the graph representation \(^{(l)}(G)\) recovers the empirical mean embedding \(_{_{}}^{(l)}\) of the domain distribution \(_{}\) in the reproducing kernel Hilbert space induced by \(K_{}^{(l)}\), and \(_{_{}}^{(l)}\) is given by_

\[_{_{}}=_{v V}_{v }^{(l)}=_{v V}K_{}^{(l)}(,v|G)\]

_where \(_{v}^{(l)}\) is the feature map of \(K_{}^{(l)}\) with \(_{v}^{(l)}=_{w}_{i=0}^{k}_{i}_{u N _{i}(v)}_{1}(_{u}^{(l-1)})\)._Corollary 4.4 shows that \(K_{}^{(l)}(G,G^{})\) in Theorem 4.1 can be explained as the similarity of the empirical mean embeddings of domain distributions \(_{}\) and \(_{^{}}\) in the kernel space e.g., \(K_{}^{(l)}(G,G^{})=_{_{}}^{(l)},_{_{^{}}}^{(l)}_{_{_{}^{(l)}}}\). A similar metric over distributions is the maximum mean discrepancy (MMD)  defined over the distance of empirical mean embeddings, i.e., \(_{}^{(l)}=||_{_{}}^{(l)}-_{_{^{}}}^{(l)}||_{_{_{}^ {(l)}}}\), where \(_{K_{}^{(l)}}\) denotes the RKHS induced by \(K_{}^{(l)}\). MMD has been widely applied to transfer learning  for measuring the distribution shift across image domains, under the covariate shift assumption  (i.e., source and target domains share conditional distribution \(p(y|x)\) but different marginal distributions \(p(x)\)). In contrast, in this paper, we focus on measuring the domain distribution similarity for transferable graph learning.

#### 4.2.2 Homophily vs. Heterophily

As illustrated in Section 3.2, there are two assumptions in graph learning : homophily and heterophily. We show the special cases of Theorem 4.1 in tackling homophily or heterophily based transferable graph learning. Given homophily source graph \(G\) and target graph \(G^{}\) where connected nodes have similar output values, it is revealed  that each node aggregates message from itself and its 1-order neighborhood, e.g., \(k=1\) and \(_{0}=_{1}=_{0}^{}=_{1}^{}=1\). Then the sample-level kernel \(K_{}^{(l)}(v,v^{}|G,G^{})_{u\{v N _{1}(v)\}}_{u^{}\{v^{} N_{1}(v^{})\}}_ {uu^{}}^{(l-1)}\). In contrast, if source and target graphs follow heterophily that connected nodes are not similar in the output space, nodes might aggregate message from high-order neighborhood , e.g., \(k=2\) and \(_{0}=_{2}=1,_{1}=0\). Then it holds that \(K_{}^{(l)}(v,v^{}|G,G^{})_{u\{v N _{2}(v)\}}_{u^{}\{v^{} N_{2}(v^{})\}}_ {uu^{}}^{(l-1)}\). These results indicate that by learning the neighborhood importances \(_{i}\) and \(_{i}^{}\), Gaussian process led by \(f(v,G)\) can adaptively select the neighborhood for message aggregation from homophily or heterophily graphs. Moreover, \(_{i}=_{i}^{}\) implies that source and target graphs follow the same assumption. The flexibility of \(_{i},_{i}^{}\) allows us to transfer knowledge across different types of graphs. For example, using \(k=2,_{0}=_{1}=1,_{2}=0,_{0}^{}=_{2}^{ }=0,_{1}^{}=0\), it enables the knowledge transferability from a homophily source graph to a heterophily target graph.

### Proposed Algorithms

The goal of transferable node regression is to learn the prediction function for the target graph, by leveraging the input-output relationship from a relevant source graph. Given a source graph \(G_{s}=(V_{s},E_{s})\) with fully labeled nodes (e.g., \(y_{v}\) is associated with each node \(v V_{s}\)) and a target graph \(G_{t}=(V_{t},E_{t})\) with a limited number of labeled nodes (e.g., \(|V_{t}^{la}||V_{s}|\) where \(V_{t}^{la} V_{t}\) is the set of labeled target nodes), we propose the adaptive graph Gaussian process algorithm (termed as GraphP) as follows.

For notation simplicity, we let \(f_{v}=f^{(L)}(v,G)\) (given by \(L\) graph convolutional layers in Eq. (5)) be the function value at node \(v\). Let \(_{s}=[f_{1},f_{2},,f_{|V_{s}|}]^{T}\) be a vector of latent function values over labeled source nodes, and \(_{s}=[y_{1},y_{2},,y_{|V_{s}|}]^{T}\) be the associated ground-truth output values. Similarly, we can define \(_{t}\) and \(_{t}\) over target nodes. Then, the GP prior over function values can be defined as \(f(0,K^{(L)}(,))\) (defined in Theorem 4.1), and its instantiation at labeled training nodes is given by \(p(|V_{s} V_{t}^{la})=(, _{(s+t)(s+t)})\) where \(_{(s+t)(s+t)}\) is a block matrix, i.e., \(_{(s+t)(s+t)}=_{ss}&_{st}\\ _{ts}&_{tt}\) and its entry is \([_{ab}]_{v_{a}v_{b}}=K^{(L)}((v_{a},G_{a}),(v_{b},G_{b}))\) for \(a,b\{s,t\}\). For the likelihood, we consider the noisy scenarios where \(p(_{a}|_{a})=(_{a},_{a}^{2} )\) for \(a\{s,t\}\) (\(_{a}\) measures the noisy magnitude). Then, following standard Gaussian process , the posterior distribution of GraphGP over testing nodes \(V_{s} V_{t}\) has a closed-form expression, i.e., \(p(_{s}|V_{s},,V_{s} V_{t}^{la})=(, )\) where

\[=_{*}(_{(s+t)(s+t)}+ _{a}^{2}&\\ &_{t}^{2})^{-1} =_{**}-_{*}(_{(s+t)(s+t)}+ _{a}^{2}&\\ &_{t}^{2})^{-1}_{*}^{T}\] (6)

Here, \(=_{s}\\ _{t_{la}}\) denotes the ground-truth output values of labeled nodes \(V_{s} V_{t}^{la}\) from source and target graphs. Each entry of \(_{*}\) represents the covariance between testing target node and training node, and \(_{**}\) denotes the covariance matrix over testing target nodes.

The posterior distribution of GraphGP has the following hyper-parameters: \(_{w},_{w},_{b}\), \(_{s}^{s},_{t}^{t}\), and noise variances \(_{s},_{t}\). The hyper-parameters of the Gaussian process can be optimized by maximizing the marginal likelihood \(p()\) over all the training samples . However, in the context of transferable graph learning, the number of labeled target nodes is much smaller than the number of labeled source nodes. By directly maximizing the marginal likelihood over all labeled source and target nodes, GraphGP might be biased towards the source domain. Therefore, in this paper, we propose to optimize the marginal distribution \(p(_{t_{la}})\) over labeled target nodes by considering all the nodes \(V_{s} V_{t}^{la}\) as the inducing points  (see more efficiency analysis in Appendix A.6). The objective function is given by maximizing the following log marginal likelihood

\[ p(_{t_{la}})=[(_{t_{la}}| ,_{t(s+t)}_{(s+t)(s+t)}^{-1}_{t(s+t)}^ {T}+_{t}^{2})]\] (7)

### Generalization Analysis

We define the generalization error of the target graph for transferable node regression. Given the ground-truth labeling function \(f^{*}\) in the target domain and the estimator \(f^{(L)}\) learned from observed source and target graphs, the generalization error is given by

\[_{t}=((v_{t},G_{t}),f^{*}(v_{t},G_{t}) )p(v_{t},G_{t})p(f^{*})d(v_{t},G_{t})df^{*}\] (8)

where \(()\) is the posterior mean of \(f^{(L)}()\) in Eq. (6), \(\) is the mean squared loss function, and \(p(v_{t},G_{t})\) denotes the sampling probability2 of a target input \((v_{t},G_{t})\). It is shown [9; 42] that if the GP prior is correctly specified, i.e., the predictor \(f^{(L)}\) has the same prior as \(f^{*}\), the generalization error is given by the predictive variance of the GP. That is, \(_{t}=(v_{t},G_{t})p(v_{t},G_{t})d(v_{t},G_{t})\) where \((v_{t},G_{t})\) is the posterior variance at \((v_{t},G_{t})\) in Eq. (6). Furthermore, the following theorem shows that the generalization error can be upper bounded in terms of the variance that takes all inputs \((v_{s},G_{s})\) of the source graph as the target samples, i.e., both \((v_{s},G_{s})\) and \((v_{t},G_{t})\) are assumed to follow the same target distribution for single-domain graph learning.

**Theorem 4.5**.: _Let \(_{ss}^{}\) be the sample-level covariance matrix over source nodes, i.e., \([_{ss}^{}]_{(v_{s},v_{s}^{})}=K_{}^{(L)}(v_{s},v_{s}^{ }|G_{s})\), \(_{ss}=K_{}^{(L)}(G_{s},G_{s}),_{tt}=K_{}^{(L)}(G_{t},G_{t})\) be the intra-graph kernels and \(_{st}=K_{}^{(L)}(G_{s},G_{t})\) be the inter-graph kernel. Suppose \(_{s}^{2}}{{=}}(_{tt}}{_{st}_{st}}-1)_{w}^{2}_{tt}_{ss}+_{tt}}{_{st}_{st}}_{s}^{2}\) where \(_{ss}\) is the maximum eigenvalue of \(_{ss}^{}\), for any \((v_{t},G_{t})\) the generalization error is bounded by_

\[_{t}_{t}(v_{t},G_{t};_{s}^{2},_{t}^ {2})p(v_{t},G_{t})d(v_{t},G_{t})\]

_where \(_{t}(v_{t},G_{t};_{s}^{2},_{t}^{2})\) is the variance assuming that all source examples are observed in the target domain with respect to noises \(_{s}^{2},_{t}^{2}\)._

This theorem confirms that compared to the single-domain Gaussian process (assuming all inputs are drawn from the same distribution), GraphGP-based transferable graph learning enables the reduced generalization error. In addition, the following corollary considers a special scenario where no labeled nodes are available in the target graph. In this case, the generalization error is determined by the normalized graph domain similarity \(^{(L)}(G_{s},G_{t})}{^{(L)}(G_{s},G_{s}) K_{}^ {(L)}(G_{t},G_{t})}}\).

**Corollary 4.6**.: _When there are no labeled nodes in the target graph, i.e., \(V_{t}^{la}=\), we have_

\[_{|V_{s}|}_{t}=(1-^{2}}{_{tt }_{ss}}) K^{(L)}((v_{t},G_{t}),(v_{t},G_{t}))p(v_{t},G _{t})d(v_{t},G_{t})\]

_where \(_{ss}=K_{}^{(L)}(G_{s},G_{s}),_{tt}=K_{}^{(L)}(G_{t},G_{t})\) and \(_{st}=K_{}^{(L)}(G_{s},G_{t})\)._

## 5 Experiments

**Data Sets:** We use the following graph learning benchmarks with node regression tasks. (1) Twitch : It has 6 different domains ("DE", "EN", "ES", "FR", "PT", and "RU"). (2) Agriculture : It has 3 different domains ("Maize" (MA), "Sorghum" (SG), and "Soybean" (SY)). (3) Airports : It has 3 different domains ("USA" (US), "Brazil" (BR), and "Europe" (EU)). (4) Wikipedia : It has 3 different domains ("chameleon" (CH), "crocodile" (CR), and "squirrel" (SQ)). (5) WebKB : It has 3 different domains ("Cornell" (CO), "Texas" (TX), and "Wisconsin" (WS)).

**Baselines:** We consider the following Gaussian process baselines. (1) RBFGP  and DINO  are feature-Only Gaussian processes without using graph structures. (2) GGP , SAGEGP , and GINGP  are graph Gaussian processes by considering source and target graphs as a large disjoint graph. (3) LINKXGP, MixHopGP, and H2GCNGP are graph Gaussian processes derived from LINKX , MixHop , H2GCN  respectively. It is notable that recent work  shows the equivalence between graph neural networks (e.g., GraphSAGE , GIN ) and graph Gaussian processes. Similarly, we can derive the corresponding graph Gaussian processes for LINKX , MixHop  and H2GCN  in Appendix A.5, which are termed as LINKXGP, MixHopGP, and H2GCNGP, respectively.

**Model Configuration:** In the experiments, we use GPyTorch  to build the graph Gaussian process models and optimize the hyperparameters with gradient descent optimizer. For the proposed GraphGP algorithm, we adopt \(k=2\) and \(L=2\) for all the experiments. The hyperparameters are optimized using Adam  with a learning rate of 0.01 and a total number of training epochs of 500. All the experiments are performed on a Windows machine with four 3.80GHz Intel Cores, 64GB RAM, and two NVIDIA Quadro RTX 5000 GPUs3.

### Results

    &  &  \\   & BR \(\) EU & EU \(\) BR & BR \(\) US & MA \(\) SG & SG \(\) MA & MA \(\) SY \\  RBFGP  & \(0.4849_{ 0.0260}\) & \(0.4479_{ 0.021}\) & \(0.3682_{ 0.0001}\) & \(0.4859_{ 0.0176}\) & \(0.3359_{ 0.0039}\) & \(0.7314_{ 0.0172}\) \\ DINO  & \(0.5241_{ 0.0147}\) & \(0.4855_{ 0.0337}\) & \(0.3877_{ 0.0263}\) & \(0.6227_{ 0.0223}\) & \(0.4591_{ 0.0045}\) & \(0.7620_{ 0.0125}\) \\ GGP  & \(0.3400_{ 0.0144}\) & \(0.3990_{ 0.0401}\) & \(0.4720_{ 0.0218}\) & \(0.4515_{ 0.0099}\) & \(0.2403_{ 0.0172}\) & \(0.7420_{ 0.0425}\) \\ SAGEGP  & \(0.4581_{ 0.0308}\) & \(0.3822_{ 0.0508}\) & \(0.4928_{ 0.0330}\) & \(0.5348_{ 0.0278}\) & \(0.3633_{ 0.0101}\) & \(0.7632_{ 0.0349}\) \\ GINGGP  & \(0.5216_{ 0.0227}\) & \(0.4471_{ 0.0219}\) & \(0.4901_{ 0.0258}\) & \(0.5380_{ 0.0288}\) & \(0.3559_{ 0.0196}\) & \(0.7746_{ 0.0158}\) \\ GraphGP & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\   

Table 1: Results of transferable node regression on Airport and Agriculture data sets

    &  &  \\   & SQ \(\) CH & CH \(\) SQ & CO \(\) TX & WS \(\) TX \\  RBFGP & -0.0383\(\)0.0526 & -0.0119\(\)0.0288 & -0.3089\(\)0.0533 & -0.2756\(\)0.0204 \\ DINO & \(0.2938_{ 0.0253}\) & \(0.1157_{ 0.0145}\) & \(0.3536_{ 0.0442}\) & \(0.2537_{ 0.0322}\) \\ LINKXGP & \(0.4100_{ 0.0203}\) & \(0.0838_{ 0.0037}\) & \(0.3836_{ 0.0530}\) & \(0.2898_{ 0.0176}\) \\ MixHopGP & \(0.4050_{ 0.0456}\) & \(}\) & \(0.3264_{ 0.0569}\) & \(0.3062_{ 0.0337}\) \\ H2GCNGP & \(0.4165_{ 0.0559}\) & \(0.2652_{ 0.0106}\) & \(0.3816_{ 0.0430}\) & \(0.3036_{ 0.0442}\) \\ GraphGP & \(}\) & \(0.3214_{ 0.0080}\) & \(}\) & \(}\) \\   

Table 3: Results on Wikipedia and WebKB

Figure 2: Domain similarityTable 1 and Table 2 provide the transferable node regression results on Airport, Agriculture, and Twitch graphs. Following , we report the coefficient of determination \(R^{2}\) (mean and standard deviation with 5 runs) on the testing target nodes for performance comparison. The experimental results verify the effectiveness of GraphGP over Gaussian process baselines. It is notable that the graphs in Table 1 and Table 2 follow the homophily assumption. In contrast, Wikipedia and WebKB graphs in Table 3 hold the heterophily assumption . It can be seen that under heterophily assumption, GraphGP also archives much better performance in most cases. In addition, we evaluate the knowledge transferability across graphs with different assumptions in Figure 3. It is observed that compared to GINGGP (or H2GCNGP) over both labeled source and target nodes, GINGP-T (or H2GCNGP-T) learns the Gaussian process over only target nodes, thereby enabling much better transfer performance from WebKB to Airport in Figure 2(a) (or from Airport to WebKB in Figure 2(b)). Moreover, GraphGP can outperform those target-only graph Gaussian processes in this scenario.

We also investigate the correlation between transfer performance and normalized graph domain similarity \(^{(L)}(G_{s},G_{t})}{^{(L)}(G_{s},G_{s}) K_{ }^{(L)}(G_{t},G_{t})}}\) in Corollary 4.6. Figure 2 visualizes the estimated normalized graph domain similarity of GraphGP and \(R^{2}\) over the target testing nodes for RU \(\) PT on Twitch, where the graph domain similarity changes when injecting noise to the target graph (see Appendix A.7.1). It shows that the transfer performance is positively correlated with the normalized graph domain similarity. This is consistent with our observation in Corollary 4.6.

### Analysis

**Flexibility of** GraphGP**: As illustrated in Subsection 4.1, it is flexible to instantiate structure-aware neural networks of Eq. (2) with existing message-passing graph neural networks. Table 4 provides the results of GIN  induced GraphGP algorithm. Furthermore, it is feasible to simply instantiate \(K_{}^{(L)}(G,G^{})\) of GraphGP with existing graph kernels, e.g., Shortest Path kernel . It is observed that the variants GraphGP_GIN and GraphGP_ShortestPath of GraphGP achieve comparable performance. This highlights that GraphGP is flexible to incorporate with existing GNNs and graph kernels for transferable graph learning tasks.

**Comparison with Transferable GNNs:** In addition to Gaussian processes, we also compare GraphGP with state-of-the-art transferable graph neural networks. Table 5 shows the competitive performance of GraphGP over transferable GNN baselines. This is because GraphGP explicitly maximizes the marginal likelihood \(p(_{t_{in}})\). In contrast, transferable GNN baselines minimize the prediction loss over all the labeled nodes (\(|V_{s}||V_{t}^{la}|\)) and domain discrepancy, and thus they are more likely to bias towards the source graph.

## 6 Conclusion

This paper studies the problem of transferable graph learning involving knowledge transfer from a source graph to a relevant target graph. To solve this problem, we propose a graph Gaussian process (GraphGP) algorithm, which is derived from a structure-aware neural network encoding both sample-level node representation and domain-level graph representation. The efficacy of GraphGP is verified theoretically and empirically in various transferable node regression tasks.

   Methods & BR \(\) EU \\  GraphGP & 0.5567\({}_{ 0.0246}\) \\ GraphGP\_GIN & 0.5301\({}_{ 0.0229}\) \\ GraphGP\_ShortestPath & 0.5377\({}_{ 0.0304}\) \\   

Table 4: Flexibility of GraphGP

Figure 3: Knowledge transfer across graphs with different assumptions

   Methods & BR \(\) EU \\  GraphGP & 0.5567\({}_{ 0.0246}\) \\ GraphGP\_GIN & 0.5301\({}_{ 0.0229}\) \\ GraphGP\_ShortestPath & 0.5377\({}_{ 0.0304}\) \\   

Table 5: Performance comparison between GraphGP and transferable GNNs