# ALAS: Active Learning for Autoconversion Rates Prediction from Satellite Data

Maria Carolina Novitasari1

maria.novitasari.20@ucl.ac.uk

Johannes Quaas2,3

johannes.quaas@uni-leipzig.de

Miguel R. D. Rodrigues1

m.rodrigues@ucl.ac.uk

Miguel R. D. Rodrigues1

m.rodrigues@ucl.ac.uk

###### Abstract

High-resolution simulations, such as the ICOsahedral Non-hydrostatic Large-Eddy Model (ICON-LEM), provide valuable insights into the complex interactions among aerosols, clouds, and precipitation, which are the major contributors to climate change uncertainty. However, due to their exorbitant computational costs, they can only be employed for a limited period and geographical area. To address this, we propose a more cost-effective method powered by an emerging machine learning approach to better understand the intricate dynamics of the climate system. Our approach involves active learning techniques by leveraging high-resolution climate simulation as an oracle that is queried based on an abundant amount of unlabeled data drawn from satellite observations. In particular, we aim to predict autoconversion rates, a crucial step in precipitation formation, while significantly reducing the need for a large number of labeled instances. In this study, we present novel methods: custom query strategy fusion for labeling instances - weight fusion (WiFi) and merge fusion (MeFi) - along with active feature selection based on SHAP. These methods are designed to tackle real-world challenges - in this case, climate change, with a specific focus on the prediction of autoconversion rates - due to their simplicity and practicality in application.

## 1 Introduction

Precipitation is a crucial weather and climate phenomenon, with its formation rate being influenced by various factors, including interactions among aerosols, clouds, and precipitation. Understanding these interactions is vital for improving future climate projections, as they represent a major source of uncertainty in estimating climate change's radiative forcing .

High-resolution simulations, such as the ICON-LEM , offer valuable insights into these interactions. However, it is computationally very expensive. For instance, running ICON-LEM to simulate a single hour of climate data over Germany requires around 13 hours on 300 computer nodes and incurs a cost of approximately EUR 100,000 per day . Given these high costs, it is imperative to seek alternative approaches for understanding complex climate system.

Thus, we propose developing a machine learning (ML) model with active learning (AL) techniques to predict autoconversion rates, a key process in precipitation (rain) formation, which in turn is key to better understanding cloud responses to anthropogenic aerosols . In particular, we propose to use a high-resolution ICON-LEM as an oracle that is queried based on an abundant amount of unlabeleddata drawn from satellite data. Our aim with active learning is to minimize the number of labeled instances required to train the machine learning model. We will demonstrate that active learning allows us to achieve greater accuracy with fewer labeled data points by selecting the most valuable instances from a pool of unlabeled data, thus reducing overall costs.

Several AL algorithms have attempted to combine both informativeness and representativeness measures when selecting optimal query instances, primarily in the context of classification problems [4; 8].  introduced an approach that strives to maximize diversity. Our method draws inspiration from the principles of combining informativeness, representativeness, and diversity, akin to the approach undertaken by  and . However, our method is specifically tailored for regression problems, setting it apart from the aforementioned classification-focused.

Our research contributes to the field in several significant ways. First, to the best of our knowledge, we are the first to apply AL in the field of high-resolution climate modeling, specifically within the context of the very expensive ICON-LEM simulation, with a specific focus on the autoconversion process - a process by which cloud droplets grow larger and transform into raindrops. Secondly and thirdly, we introduce active feature selection using SHAP (SHapley Additive exPlanations), and innovative query strategy fusion techniques: query strategy fusion by weight (WiFi) and query strategy fusion by merging (MeFi) which are straightforward and convenient in practice.

## 2 Proposed Methods

We introduce active feature selection using SHAP and novel query strategies that consider three crucial factors when choosing unlabeled instances in AL: informativeness, representativeness, and diversity, explained in the following subsections. For our discussion, let the following notations be defined: \(D_{}\) as the initial labeled data, \(D\) as the current labeled data, \(X_{}\) as the small unlabeled pool, \(P\) as the set of points to be labeled, \(\) as the ML model, \(B_{}\) as the maximum budget (number of labeled), \(^{p}\) as the full feature vector, and \(t\) as the SHAP threshold.

Active Feature SelectionOur approach employs SHAP to assess feature contributions, eliminating insignificant features throughout certain AL stages (see Alg. 1).

InformativenessGiven a Gaussian process regression model \(f(m,k)\) where \(m\) is the prior mean function and \(k\) is the prior covariance kernel, the predictive distribution at a new input \(x_{*}\) is Normal with mean \((x_{*})\) and variance \(^{2}(x_{*})\). In informativeness-based sampling with Gaussian Process Regression (GPR) , we leverage the model's predictive standard deviation, denoted as \(l_{}\), to quantify prediction uncertainty. Our goal is to choose \(P\) data points for labeling that have the highest \(l_{}\) values, as these points correspond to regions where the model is least certain. The details of our informativeness-based sampling algorithm are outlined in Appendix B1.

RepresentativenessIn this section, we introduce a straightforward approach that involves selecting a number of \(|P|\) data points to label based on the most representative values they hold (i.e., those closest to their centroid cluster), denoted as \(l_{}\), as a query strategy in AL regression. The optimal number of clusters is determined using the Silhouette method on \(X_{}\). The details of our representativeness-based sampling algorithm are outlined in Appendix B2.

DiversityIn diversity-based sampling, we select \(P\) data points that maximize dissimilarity within their clusters, denoted as \(l_{}\). By calculating the average dissimilarity for each data point within its cluster, we identify those that contribute the most to dataset diversification. The optimal number of clusters is determined using the Silhouette method on \(X_{}\). The details of our diversity-based sampling algorithm are outlined in Appendix B3.

Weight Fusion (WiFi)We propose the Weight Fusion (WiFi) query strategy, with \(\) and \(\) as weight trade-offs. \(\) governs informativeness vs. representativeness, while \(\) manages the trade-off between informativeness-representativeness and diversity. Higher \(\) values emphasize representativeness, and higher \(\) values prioritize diversity. WiFi is defined as:

\[(x_{*})=(1-)((1-) l_{}(x_{*})+  l_{}(x_{*}))+ l_{}(x_{*})\]

where \(x_{*} X_{}\). Details of \(l_{}\), \(l_{}\), \(l_{}\) are explained in the previous subsections, where they denote informativeness, representativeness, and diversity scores. We select the top \(P\) points in \(X_{}\) based on their descending WiFi rank and optimize \(\) and \(\) using initial labeled data.

Merge Fusion (MeFi)MeFi is a novel query strategy that optimally balances informativeness, representativeness, and diversity by merging the top \(\) data points from each category (\(L_{}\), \(L_{}\), \(L_{}\)), defined as follows:

\[=L_{}L_{} L_{}\]

## 3 Experimental Results

### Datasets

We trained and validated our models using ICON-LEM output over Germany on May 2, 2013, from 09:55 am to 1 pm local time. The test dataset consists of two subsets: one covering the entire Germany region on May 2, 2013, at 13:20 local time, and another encompassing the North Atlantic region on September 1, 2014, at 13:00 local time. As for the satellite observation data, we use cloud product level-2 of Terra and Aqua MODIS [14; 15]. Details of our datasets, including various testing scenarios, are provided in Appendix A and C.2.1.

### Active Learning (AL)

Initial Active Learning SettingsWe utilized a pool-based AL regression approach with a large training pool of about 4 million unlabeled data points and a large validation pool of approximately 900,000 data points. We conducted 100 experiments - including active feature selection, cluster number selection, AL, and \(\) and \(\) hyperparameter tuning - and averaged the results. In each experiment, we sampled small training (\(X_{}\)) and validation pools of 1,000 and 250 data points, respectively, with \(|D_{}|\) = 10 and \(|P|\) = 3. We employed GPR to train our ML models. Our initial model takes the cloud effective radius (CER) and pressure (P), parameters of the cloud microphysical state typically obtained from satellite retrievals, as input. The model output is the autoconversion rates derived from ICON-LEM.

Active Feature SelectionIn this step, we selected our features using the active feature selection algorithm explained in Section 2. Our results highlight CER as the most influential feature in predicting autoconversion rates, while the contribution of P is relatively small, as shown in Fig. 1. We validated our results by performing Gaussian process regression across different sample sizes (10, 50, and 100) and evaluating the outcomes. Consistently, the results show that using P alone outperforms using both P and CER as input features (see details in Appendix C.1.1).

Selection of the Number of Clusters, Alpha, and BetaWe determined the optimal number of clusters using the Silhouette method on \(X_{}\). The best number of clusters was found to be 2. The results for \(\) selection using initial data points are illustrated in Fig. 2. The optimal \(\) value is determined to be 0.5, signifying an equilibrium between 50% informativeness and 50%representativeness. The optimal \(\) value for diversity based on Euclidean distance is 0.4, resulting in a balanced combination of 40% informativeness-representativeness and 60% diversity, while for inverse cosine-based diversity, it is identified as 0.5 (see Appendix C.1.2 for details on the selection of \(\)).

Active Learning ResultsWe assess the AL query strategy performance using R\({}^{2}\) and SSIM metrics, shown in Fig. 3. R\({}^{2}\) indicates that \(l_{}\), WiFi, and MeFi (Euclidean (euc); inverse cosine (cos)) achieve faster convergence than random (baseline), \(l_{}\), and \(l_{}\). However, \(l_{}\) eventually lags behind others. WiFi and MeFi consistently outperform baseline and individual aspects (\(l_{}\), \(l_{}\), \(l_{}\)) across all query iterations. SSIM results closely align with the R\({}^{2}\) findings, showing that \(l_{}\), WiFi, and MeFi, consistently outperform others, with WiFi and MeFi still maintaining their lead. WiFi, in particular, excels when using the Euclidean metric for both R\({}^{2}\) and SSIM.

### Autoconversion Rates Prediction

We employ GPR with an RBF and white noise kernel to train our model. To determine the optimal hyperparameters for the kernel, we employ random search cross-validation. Our training dataset consists of only 100 labeled data points selected using the best AL query strategy explained in the previous subsection (WiFi Euclidean), while we reserve 250 data points for validation. This represents <0.01% of the total actual labeled data available and only 47% of the labeled data needed by the baseline (Appendix C.1.3). We utilize a significantly smaller amount of data in comparison to the work by , who utilized the entire cloud-top dataset. For the input, we use CER as determined by our previous experiment using SHAP.

Simulation Model (ICON)We test our model on simulation data in 3 different scenarios: (1) ICON-LEM Germany (different times), (2) Cloud-top ICON-LEM Germany (satellite-like data), and (3) Cloud-top ICON-NWP Holuhraun (different data, time, location, and resolution), details in Appendix C.2.1. The results in Table 1 demonstrate that SSIM values exceed 90% for all scenarios, with scenarios 1 and 2 also achieving over 90% for R\({}^{2}\). Scenario 3, despite using different data

Figure 1: The results of active feature selection with SHAP.

Figure 3: Evaluation of different query strategies in active learning with R\({}^{2}\) and SSIM.

Figure 2: Exploring the alpha trade-off: balancing informativeness and representativeness.

in terms of time, location, and resolution, still achieves an R\({}^{2}\) slightly above 85%. These findings highlight the model's capability to accurately estimate autoconversion rates when utilizing model-simulated satellite data, without the need for further adjustments like fine-tuning. This minimizes the need for additional data collection and time-consuming training processes. Visual representation included in Appendix C.2.2.

Satellite Observation (MODIS)This experiment aims to assess our model's ability to predict autoconversion rates using real satellite data, specifically by testing the model on such data. We focused on comparing the autoconversion rate predictions from the MODIS satellite with cloud-top ICON simulation output over Germany (latitude: 47.50\({}^{}\) to 54.50\({}^{}\) N, longitude: 5.87\({}^{}\) to 10.00\({}^{}\) E). While it is worth noting that direct comparisons between satellite predictions and simulation models cannot be made directly due to differences in cloud placement, Fig. 4 demonstrates that the MODIS autoconversion rate predictions statistically align with those from cloud-top ICON-LEM Germany. The mean, standard deviation, median, and percentiles of autoconversion rates demonstrate a close agreement. It shows that autoconversion rates can be well estimated from satellite-derived CER data using our method.

## 4 Conclusions

In this study, we have provided a computationally efficient solution for understanding the key process of precipitation formation, specifically the autoconversion process. This process plays a crucial role in advancing our understanding of how clouds respond to anthropogenic aerosols , and ultimately, climate change. Importantly, we have shown it is possible to predict autoconversion rates accurately using less than 0.01% of the expensive labeled data from high-resolution ICON-LEM simulation. Our machine learning model achieves good performance on both atmospheric simulation and satellite data, while requiring only 47% of the data needed by the baseline strategy. This demonstrates a cost-effective approach to train an accurate model with minimal labeled data. Additionally, we introduced innovative techniques: custom query strategies for active learning, WiFi and MeFi, along with active feature selection using SHAP. These methods were specifically designed to address real-world problems due to their practical simplicity. Our custom query strategy fusion, WiFi and MeFi, consistently outperformed the baseline query strategy, as well as the individual aspects of informativeness, representativeness, and diversity. For simplicity, we used only the initially selected data points for hyperparameter selection in this work, but exploring an adaptive method for selecting hyperparameters in the WiFi query strategy could be a potential direction for future research.

   Testing Set & R\({}^{2}\) & MAPE & RMSPE & SSIM & PSNR (dB) \\ 
1 & 90.18\% & 9.31\% & 12.19\% & 90.52\% & 26.14 \\
2 & 90.32\% & 10.35\% & 13.20\% & 90.29\% & 26.09 \\
3 & 85.09\% & 8.33\% & 22.47\% & 91.66\% & 26.01 \\   

Table 1: Evaluation of autoconversion prediction results on three different testing scenarios.

Figure 4: Mean, standard deviation (Std), median, and percentiles (p25, p75) of cloud-top ICON-LEM and MODIS variables over Germany: cloud effective radius (CER) and autoconversion rates (Aut).