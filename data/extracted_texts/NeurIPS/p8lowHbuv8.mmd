# From Trainable Negative Depth to

Edge Heterophily in Graphs

 Yuchen Yan\({}^{1}\), Yuzhong Chen\({}^{2}\), Huiyuan Chen\({}^{2}\), Minghua Xu\({}^{2}\),

**Mahashweta Das\({}^{2}\), Hao Yang\({}^{2}\), Hanghang Tong\({}^{1}\)**

\({}^{1}\)University of Illinois at Urbana Champaign, IL, USA

\({}^{2}\)Visa Research, CA, USA

{yucheny5, htong}@illinois.edu

{yuzchen, hchen, mixu, mahdas, haoyang}@visa.com

###### Abstract

Finding the proper depth \(d\) of a graph convolutional network (GCN) that provides strong representation ability has drawn significant attention, yet nonetheless largely remains an open problem for the graph learning community. Although noteworthy progress has been made, the depth or the number of layers of a corresponding GCN is realized by a series of graph convolution operations, which naturally makes \(d\) a positive integer (\(d+\)). An interesting question is whether breaking the constraint of \(+\) by making \(d\) a real number (\(d\)) can bring new insights into graph learning mechanisms. In this work, by redefining GCN's depth \(d\) as a trainable parameter continuously adjustable within \((-,+)\), we open a new door of controlling its signal processing capability to model graph homophily/heterophily (nodes with similar/dissimilar labels/attributes tend to be inter-connected). A simple and powerful GCN model TeDGCN, is proposed to retain the simplicity of GCN and meanwhile automatically search for the optimal \(d\) without the prior knowledge regarding whether the input graph is homophilic or heterophilic. Negative-valued \(d\) intrinsically enables high-pass frequency filtering functionality via augmented topology for graph heterophily. Extensive experiments demonstrate the superiority of TeDGCN on node classification tasks for a variety of homophilic and heterophilic graphs.

## 1 Introduction

Graph convolutional network (GCN) [28; 50; 19; 47] has exhibited superb power in a myriad of graph learning tasks, such as knowledge graph reasoning [56; 53; 54; 52; 36], network alignment [63; 64; 68; 74; 20; 35; 67], multi-network mining [65; 73; 42; 13; 27; 17], networked time series imputation [51; 25; 55; 16], node recommendation [26; 2; 3], and many more. Since the representation capability of GCN is largely determined by its depth, i.e., the number of graph convolution layers, tremendous research efforts have been made on finding the optimal depth that strengthens the model's ability for downstream tasks. Upon increasing the depth, the over-smoothing issue arises: a GCN's performance is deteriorated if its depth exceeds a certain threshold . It is found in  that a graph convolution operation is a special form of Laplacian smoothing . Thus, the similarity between the graph node embeddings grows with the depth so that these embeddings eventually become indistinguishable. Various techniques have been developed to alleviate this issue, e.g., applying pairwise normalization can make distant nodes dissimilar , and dropping sampled edges during training slows down the growth of embedding smoothness as the depth increases .

Other than the over-smoothing issue due to a large GCN depth, another fundamental phenomenon widely existing in real-world graphs is homophily and heterophily. In a homophilic graph, nodes withsimilar labels or attributes tend to be inter-connected, while in a heterophilic graph, connected nodes usually have distinct labels or dissimilar attributes. Most graph neural networks (GNNs) are developed based on the homophilic assumption , while models able to perform well on heterophilic graphs often need special treatment and complex designs [4; 76; 62]. Despite the achievements made by these methodologies, little correlation has been found between the adopted GNN model's depth and its capability of characterizing graph heterophily.

For almost any GNN model, the depth needs to be manually set as a hyper-parameter before training, and finding the proper depth usually requires a considerable amount of trials or good prior knowledge of the graph dataset. Since the depth represents the number of graph convolution operations and naturally takes only positive integer values, little attention has been paid to the question of whether a non-integer depth is realizable, and if yes, whether it is practically meaningful, and what kind of unique advantages it can bring to current graph learning mechanisms.

This work revisits the GCN depth from spectral and spatial perspectives and explains the intrinsic connections between the following key ingredients in graph learning: (i) the depth of a GCN, (ii) the spectrum of the graph signal, and (iii) the homophily/heterophily of the underlying graph. Firstly, through eigen-decomposition of the symmetrically normalized graph Laplacian, we present the connection between graph homophily/heterophily and the eigenvector frequencies. Secondly, by introducing the concept of eigengraph, we show the graph topology is equivalent to a weighted linear combination of eigengraphs, and the weights determine the GCN's capability of capturing homophilic/heterophilic graph signals. Thirdly, we reveal that the eigengraph weights can be controlled by GCN's depth, so that an automatically tunable depth parameter is needed to adjust the eigengraph weights into the designated distribution in match of the underlying graph homophily/heterophily.

To realize the adaptive GCN depth, we extend its definition from a positive integer to an arbitrary real number with theoretical feasibility guarantees from functional calculus . With a trainable depth parameter, we propose a simple and powerful model, Trainable Depth-GCN (TeDGCN), with two variants. Extensive experiments demonstrate TeDGCN's ability of automatically searching for the optimal depth, and it is found that negative-valued depth plays the key role in handling heterophilic graphs. Systematical investigation on the optimal depth is conducted via various real-world and synthetic datasets. It in turn inspires the development of a novel graph augmentation methodology. With clear geometric interpretability, the augmented graph structure possesses supreme advantages over the raw input topology, especially for graphs with heterophily. The main contributions of this paper are summarized as follows:

* **Analysis.** The intrinsic relationship between the negative GCN depth and graph heterophily is discovered; In-depth geometric and spectral explanations are presented.
* **Problem.** A novel problem of automatic GCN depth tuning for graph homophily/heterophily detection is formulated. To our best knowledge, this work presents the first trial to make GCN's depth negative and trainable by redefining it on the real number domain.
* **Model.** A simple and powerful model TeDGCN with two variants (TeDGCN-S and TeDGCN-D) is proposed. A novel graph augmentation method is discussed.
* **Experiments.** Our model achieves superior performance on semi-supervised node classification tasks on 11 graph datasets.

## 2 Preliminaries

**Notations.** We utilize bold uppercase letters for matrices (e.g., \(\)), bold lowercase letters for column vectors (e.g., \(\)) and lowercase letters for scalars (e.g., \(\)). We use the superscript \(\) for the transpose of matrices and vectors (e.g., \(^{}\) and \(^{}\)). An attributed undirected graph \(=\{,\}\) contains an adjacency matrix \(^{n n}\) and an attribute matrix \(^{n q}\) with the number of nodes \(n\) and the dimension of node attributes \(q\). \(\) denotes the diagonal degree matrix of \(\). The adjacency matrix with self-loops is given by \(}=+\) (\(\) is the identity matrix), and all variables derived from \(}\) are decorated with symbol \(\), e.g., \(}\) represents the diagonal degree matrix of \(}\). \(^{d}\) stands for the \(d\)-th power of matrix \(\), while the parameter and node embedding matrices in the \(d\)-th layer of a GCN are denoted by \(^{(d)}\) and \(^{(d)}\).

**Graph convolutional network (GCN) and simplified graph convolutional network (SGC).** The layer-wise message-passing and aggregation of GCN  is given by

\[^{(d+1)}=(}^{-}} }^{-}^{(d)}^{(d)}),\] (1)

where \(^{(d)}\)/\(^{(d+1)}\) stands for the embedding matrix (\(^{(0)}=\)) in the \(d\)-th\((d+1)\)-th layer; \(^{(d)}\) is the trainable parameter matrix; and \(()\) is the non-linear activation function. With \(()\) removed in each layer, SGC  is obtained as below:

\[^{(d)}=}^{d},\] (2)

where \(}=}^{-}} {}^{-}\), and the parameter of each layer \(^{(i)}\) are compressed into one trainable \(=_{i=0}^{d-1}^{(i)}\).

**Graph Laplacian and spectrum.** In graph theory, graph Laplacian \(=-\) and its symmetrically normalized correspondence \(_{sym}=-^{-}^{- }\) possess critical properties of the underlying graph \(\). \(_{sym}\) has eigenvalues \(\{_{1},_{2},,_{n}\}\), where \(_{i}[0,2)\), \( i\{1,2,,n\}\).1 Here, they are put in the ascending order: \(0=_{1}_{2}_{n}<2\). It can be eigen-decomposed as: \(_{sym}=^{}\), where \(=[_{1},_{2},,_{n}]\) is the eigenvector matrix (\(_{i}_{j}, i j\)), and \(\) is the diagonal eigenvalue matrix. For each eigenvector \(_{i}\), we have \(_{i}_{i}^{}^{n n}\). As we will show in Section 3, this \(n n\) matrix can be viewed as the weighted adjacency matrix of a graph with possible negative edges, which we name as the \(i\)-th eigengraph of \(\). Accordingly, \(_{sym}\) can be written as the linear combination of all eigengraphs weighted by the corresponding eigenvalues :

\[_{sym}=_{1}_{1}_{1}^{}++ _{i}_{i}_{i}^{}++_{n}_{ n}_{n}^{},\] (3)

where the first eigenvalue \(_{1}=0\). Thus, for SGC, we have

\[}=-}_{sym}=}( -})}^{}=_{i=0}^{n}( 1-_{i})}_{i}}_{i}^{}.\] (4)

A SGC with \(d\) layers requires \(d\) consecutive graph convolution operations, which involves the multiplication of \(}\) by \(d\) times. Due to the orthogonality of \(}\), namely, \(}^{}}=\), we obtain

\[}^{d}=}(- })}^{}}(-})}^{}}(- })}^{}=}( -})^{d}}^{}=_{i=1}^ {n}(1-_{i})^{d}}_{i}}_{i}^{ },\] (5)

where \(1-_{i}(-1,1]\), and the depth \(d\) of SGC serves as the power of \(}\)'s eigenvalues. \(}^{d}\) can be viewed as the sum of eigengraphs \(}_{i}}_{i}^{}\) weighted by coefficients \((1-_{i})^{d}\).

**Graph homophily and heterophily.** Various homophily/heterophily measures are developed to capture how edges tend to link nodes with the same labels and similar attributes, such as edge homophily, node homophily, and class homophily. Without the loss of generality, we focus on one mostly used homophily metric, the edge homophily: \(h()=(i,j)=1}[i]=[j]}{_{i,j}[i,j]}\), where \( x=1\) if \(x\) is true and \(0\) otherwise. A graph is more homophilic for \(h()\) closer to \(1\) or more heterophilic for \(h()\) closer to \(0\).

Figure 1: Decompose the symmetrically normalized adjacency matrix into three eigengraphs.

Model

**Overview.** Firstly, we establish the intrinsic relationship between graph spectrum and graph heterophily. Secondly, we show how the positive/negative depth \(d\) of a GCN affects the eigagraph weights which in turn impacts the algorithm's capability to capture homophilic/heterophilic graph signals. Thirdly, with the help of functional calculus , we present the theoretical feasibility of extending the domain of \(d\) from \(+\) to \(\). Finally, by making \(d\) a trainable parameter, we present our model TeDGCN and its variants, which are capable of automatically detecting the homophily/heterophily of the input graph and finding the optimal depth.

**Eigenvector frequency and entry deviation.** The frequency of graph Laplacian eigenvector \(_{i}\) reflects how much the \(j\)-th entry \(_{i}[j]\) deviates from the \(k\)-th entry \(_{i}[k]\) for each connected node pair \(v_{j}\) and \(v_{k}\) in \(\). This deviation is measured by the set of zero crossings of \(_{i}\): \((_{i}):=\{e=(v_{j},v_{k}):_{i}[j ]_{i}[k]<0,j k\}\), where \(\) is the set of edges in graph \(\). Larger/smaller \(|(_{i})|\) indicates higher/lower eigenvector frequency. A zero-crossing also corresponds a negative weighted edge in an eigagraph. Due to the widely existing positive correlation between \(_{i}\) and \(|(_{i})|\), large/small eigenvalues mostly correspond to the high/low frequencies of the related eigenvectors. As illustrated by the toy example of \(n=3\) in Figure 1, for \(_{1}=0\), we have \(|(_{1})|=0\), and eigengraph \(_{1}_{1}^{}\) is fully-connected; negative edge weights exist in the 2nd and 3rd eigengraphs, indicating more zero crossings (\(|(_{2})|=1\) and \(|(_{3})|=2\)) and higher eigenvector frequencies.

**Entry deviation and graph heterophily.** Reflected by entry deviations, eigenvector frequency and zero-crossings determine the number of negatively weighted edges (\(_{i}[j]_{i}[k]<0\), \(j k\)) in a corresponding eigengraph \(_{i}_{i}^{}\). For a GCN/SGC, conducting message-passing along negatively weighted edges makes the embeddings of the connected node pairs (\(v_{j}\), \(v_{k}\)) dissimilar. Naturally, higher embedding dissimilarity is more likely to render different label predictions between the node pair, and this generates heterophily. Thus, eigenvectors with higher frequencies and accordingly higher entry deviations contribute more to heterophilic graph signals, and vice versa.

**Graph heterophily and GCN's depth.** Based on above analyses, high frequency eigenvectors and their corresponding eigengraphs have advantage on capturing graph heterophily. A graph neural network should be able to give high frequency eigengraphs larger weights when modeling heterophilic graphs, whereas low frequency ones should carry larger weights when dealing with homophilic graphs. The weights of eigengraphs are controlled by GCN/SGC's depth \(d\) as follows. For a SGC of depth \(d\), the weight of the \(i\)-th eigengraph is \((1-_{i})^{d}\), and changing the depth \(d\) of SGC adjusts the weights of different eigengraphs. Therefore, depth \(d\) controls the model's capability of effectively filtering low/high-frequency signals for graph homophily/heterophily.

**Redefined GCN's depth.** A question is naturally raised: instead of manually setting the depth \(d\), can \(d\) be built into the model as a trainable parameter? If yes, a proper set of the eigengraph weights matching the graph homophily/heterophily can be automatically reached by finding the optimal \(d\) in an end-to-end fashion during training.

Differentiable variables need continuity, which requires the extension of depth \(d\) from the discrete positive integer domain (\(+\)) to the continuous real number domain \(\). According to functional calculus , applying an arbitrary matrix function \(f\) on a graph Laplacian \(_{sym}\) is equivalent to applying the same function only on the eigenvalue matrix \(\):

\[f(_{sym})=f()^{}= f(_{1})&&0\\ &&\\ 0&&f(_{n})^{},\] (6)

which also applies to \(}_{sym}\) and \(}\). Armed with this, we seek to realize an _arbitrary_ depth SGC via a power function as \(f(})=}^{d}=}(- })^{d}}^{}=_{i=1}^{n}(1- _{i})^{d}}_{i}}_{i}^{}(d )\). However, since \(_{i}[0,2)\), we have \((1-_{i}) 0\) when \(1_{i}<2\), and for \((1-_{i})\) taking zero or negative values, \((1-_{i})^{d}\) is not well-defined or involving complex-number-based calculations for a real-valued \(d\) (e.g.,\((-0.5)^{}\)). Moreover, even for integer-valued \(d\)s under which \((1-_{i})^{d}\) is easy to compute, the behavior of \((1-_{i})^{d}\) is complicated versus \(_{i}\) and diverges when \(_{i}=1\) for negative \(d\)s, as shown in Figure 1(a). Thus, the favored weight distribution may be hard to obtain by tuning \(d\).

To avoid such complications and alleviate the difficulties for manipulating the eigengraph weights, a transformation function \(g()\) operating on the graph Laplacian \(_{sym}\) or \(}_{sym}\) is in need to shift \(g(_{i})\) or \(g(_{i})\) into a proper value range so that its power of a real-valued \(d\) is easy to obtain and well-behaved versus \(_{i}\) or \(_{i}\). Without the loss of generality, our following analysis focuses on \(_{sym}\) and \(_{i}\). There may exist multiple choices for \(g()\) satisfying the requirements. In this work, we focus on the following transformation function:

\[}=g(_{sym})=(2-_{sym }).\] (7)

This choice of \(g()\) holds three properties: (1) _Positive eigenvalues._ Since we have \(_{sym}\)'s \(i\)-th eigenvalue \(_{i}[0,2)\), the corresponding eigenvalue of \(}\) is \(g(_{i})=(2-_{i})(0,1)\). Thus, the \(d\)-th power of \(g(_{i})\) is computable for any \(d\). (2) _Monotonicity versus eigenvalues \(\)_. As shown in Figure 1(b), \(g(_{i})^{d}=(1-)^{d}\) is monotonically increasing/decreasing when \(\) varies between 0 and 2 under negative/positive depth. (3) _Geometric interpretability._ Filter \(}\) can be expressed as:

\[}=}^{}=( -)^{}= +(-_{sym})=(+^ {-}^{-}).\] (8)

As shown in Figure 3, in the spatial domain, \(}\) is obtained via 3 operations on adjacency matrix \(\): normalization, adding self-loops, and scaling all edge weights by \(\) (a type of lazy random walk ), while \(}\) in vanilla GCN/SGC contains 2 operations: adding self-loops and normalization.

With the help of transformation \(g\), the depth \(d\) is redefined on real number domain, and the message propagation process of depth \(d\) can be realized via the following steps: (1) Eigen-decompose \(_{sym}\); (2) Calculate \(}^{d}\) via weight \(g(_{i})^{d}\) and the weighted sum of all eigengraphs: \(}^{d}=}^{d}^{}=_{i= 1}^{n}g(_{i})^{d}_{i}_{i}^{}\) (3) Multiply \(}^{d}\) with original node attributes \(\).

**Negative depth explained.** First, an intuitive explanation of negative \(d\) can be obtained from the perspective of matrix inverse and message diffusion process when \(d\) takes integer values. Since \(}^{-1}}=}^{-1}^{}}^{1}^{}=\), \(}^{-1}\) is the inverse matrix of \(}\). In diffusion dynamics, \(\) can be viewed as an intermediate state generated in a series of message propagation steps. \(}\) effectively propagates the message one-step forward, while \(}^{-1}\) can cancel the effect of \(}\) on \(\) and recover the original message by moving backward: \(}^{-1}}=\). Accordingly, \(}^{-1}\) traces back to the message's previous state in the series. Non-integer \(d\) indicates the back- or forward propagation can be a continuous process. Second, as the depth becomes negative, it primarily keeps the eigengraphs of high frequency (See Figure 1(b)), which have more negative edges. Therefore, it is equivalent to turning some positive edges in the original graph into negative edges in the augmented graph, which in turn makes node embeddings dissimilar in the message-passing process and thus naturally handles the heterophilic graphs (See Figure 1). More discussions on the impact of negative depth in spatial domain are presented in Section 4.4.

**TeDGCN-S algorithm.** By further making \(d\) a trainable parameter, we present our model, _Trainable Depth-GCN-Single_ (TeDGCN-S), whose final node embedding matrix is given by

\[=(}^{d}),\] (9)

Figure 3: The difference between \(}\) (left) for GCN/SGC and \(}\) (right) for TeDGCN.

Figure 2: Eigengraph weight versus eigenvalue for (a) SGC and (b) TeDGCN-S under different depth \(d\)s.

where \(()\) is the nonlinear activation function; \(\) is a trainable parameter matrix; and \(d\) is the trainable depth parameter. As observed from Figure 1(b), weight distribution of different frequencies/eigengraphs is tuned via \(d\): (1) for \(d=0\), the weight is uniformly distributed among all frequency components (\(g(_{i})^{d}=1\)), which implies that no particular frequency is preferred by the graph signal; (2) for \(d>0\), weight \(g(_{i})^{d}\) decreases with the corresponding frequency, which indicates the low frequency components are favored so that TeDGCN-S effectively functions as a low-pass filter and therefore captures graph homophily; (3) for \(d<0\), high frequency components gains amplified weights so that TeDGCN-S serves as a high-pass filter capturing graph heterophily. During training, TeDGCN-S tunes its frequency filtering functionality to suit the underlying graph signal by automatically finding the optimal \(d\).

**TeDGCN-D algorithm.** During optimization, TeDGCN-S embraces a single depth \(d\) unified for all eigengraphs and selects its preferences for either homophily or heterophily. However, TeDGCN-S requires a full eigen-decomposition of \(_{sym}\), which can be expensive for large graphs. Additionally, the high and low frequency components in a graph signal may not be mutually exclusive, namely, there exists the possibility for a graph to simultaneously possess homophilic and heterophilic counterparts. Therefore, we propose the second variant of TeDGCN: TeDGCN-D (Dual), which introduces two separate trainable depths, \(d_{h}\) and \(d_{l}\), to gain more flexible weighting of the high and low frequency related eigengraphs respectively. Arnoldi method  is adopted to conduct EVD on \(_{sym}\) and obtain the top-\(K\) largest and smallest eigen-pairs \((_{i},_{i})\)s. By denoting \(_{l}=[:,\ 0:K]\) and \(_{h}=[:,\ n-K:n]\) (\(_{l}\) and \(_{h}^{n K}\)), we define a new diffusion matrix \(}_{dual}(d_{l},d_{h},K)\) as

\[}_{dual}(d_{l},d_{h},K)=_{l}}_{l}^{d_ {l}}_{l}^{}+_{h}}_{h}^{d_{h}}_{h}^{},\] (10)

where \(}_{l}^{K K}\) and \(}_{h}^{K K}\) are diagonal matrices of the top-\(K\) smallest and largest eigenvalues.2 The final node embedding of TeDGCN-D is presented as

\[=(}_{dual}(d_{l},d_{h},K)),\] (11)

where depths \(d_{l}\) and \(d_{h}\) are trainable; and \(\) is a trainable parameter matrix. We make TeDGCN-D scalable on large graphs by choosing \(K n\), so that \(}_{dual}(d_{l},d_{h},K)\) approximates the full diffusion matrix by covering only a small subset of all eigengraphs. For small graphs, we use \(K=\) to include all eigengraphs, and \(}_{dual}(d_{l},d_{h},K)\) thus gains higher flexibility than \(}\) with the help of the two separate depth parameters instead of a unified one.

**Complexity Analysis.**TeDGCN has two major steps: (1) conduct eigen-decomposition of \(}\); (2) train a one-layer GCN on the augmented graph \(}^{d}\). For Step (1), naive eigen-decomposition has a time complexity of O(\(n^{3}\)). However, by adopting the Lanczos method  and only keeping the top-\(K\) largest/smallest eigenvalues in TeDGCN-D, the time complexity can be reduced to \(O(nK^{2}+mK)\), where \(m\) is the number of edges and \(K<<n\). For Step (2), via dense matrix multiplications, computing the approximated \(}^{d}\) takes O(\(nK^{2}+n^{2}K\)), and conducting message propagation takes O(\(n^{2}q+nqc\)), where \(q\) and \(c\) denote the dimension of node feature vectors and the number of node classes. Potential improvements of efficiency: since the diffusion matrix in Subsection 4.4 of the augmented graph is sparse for known real-world graphs, the complexity of Step (2) can also be further simplified: the O(\(n^{2}K\)) term can be reduced to O(\(|E_{aug}|K\) ), where \(E_{aug}\) is the edge set of the augmented graph.

**Differences with ODE-based GNNs.** Previous attempts on GNNs with continuous diffusion are mostly inspired by graph diffusion equation, an Ordinary Difference Equation (ODE) characterizing the dynamical message propagation process versus time. In contrast, our framework starts from discrete graph convolution operations without explicitly involving ODE. CGNN  aims to build a deep GNN immune to over-smoothing by adopting the neural ODE framework . But its time parameter \(t\) is a non-trainable hyper-parameter predefined within the positive domain, which is the key difference with TeDGCN. A critical CGNN component for preventing over-smoothing, restart distribution (the skip connection from the first layer), is not needed in our framework. Moreover, CGNN applies the same depth to all frequency components, while TeDGCN-D has the flexibility to adopt two different depths respectively to be adaptive to high and low frequency components. GRAND  introduces non-Euler multi-step schemes with adaptive step size to obtain more precise solutions of the diffusion equation. Its depth (i.e., the total integration time) is continuous but still predefined/non-trainable and only admits positive values. DGC  decouples the SGC depth into two predefined non-trainable hyper-parameters: a positive real-valued \(T\) controlling the total time and a positive integer-valued \(K_{dgc}\) corresponding to the number of diffusion steps. However, realizing negative depth in DGC is non-applicable since the implementation is through propagation by \(K_{dgc}\) times, rather than through an arbitrary real-valued exponent \(d\) on eigagraph weights in TeDGCN.

## 4 Experiment

In this section, we evaluate the proposed TeDGCN on the _semi-supervised_ node classification task on both homophilic and heterophilic graphs.

### Experiment Setup

**Datasets.** We use 11 datasets for evaluation, including 4 homophilic graphs: Cora , Citeseer , Pubmed  and DBLP , and 7 heterophilic graphs: Cornell , Texas , Wisconsin , Actor , Chameleon , Squirrel , and cornell5 . We collect all datasets from the public GCN platform Pytorch-Geometric . For Cora, Citeseer, Pubmed with data splits in Pytorch-Geometric, we keep the same training/validation/testing set split as in GCN . For the remaining 8 datasets, we randomly split every dataset into 20/20/60% for training, validation, and testing.

**Baselines and metrics.** Here, we compare our model with 7 baseline methods, including 4 classic GNNs: GCN , SGC , APPNP  and ChebNet , and 3 GNNs tailored for heterophilic graphs: FAGCN , GPRGNN  and H2GCN . Accuracy (ACC) is used as the evaluation metric. We report the average ACCs with the standard deviation (std) for all methods, each obtained by 5 runs with different initializations.

**Additional contents.** In Appendix, we provide more contents related to experiments including (1) the statistics of all datasets (Appendix A.1); (2) experimental results on synthetic graphs with _controllable edge homophily/heterophily levels_ to demonstrate the effectiveness of negative depth (Appendix A.2); (3) comparison with additional methods including ACM-GCN [37; 38], LINKX , BernNet , GBK-GNN , HOG-GCN , Geom-GCN  and GloGNN  under the commonly used _fully-supervised_ setting that training/validation/testing set split is 48/32/20% for training, validation, and testing based on the results reported in the original papers (Appendix A.3); (4) implementation details (Appendix A.4); and (5) visualization of node embeddings w.r.t. \(d\) (Appendix A.5).

### Node Classification

The semi-supervised node classification performances on homophilic graphs and heterophilic graphs are shown in Table 1 and Table 2 respectively.

**Homophilic graphs**. From Table 1, it is observed that different methods have similar performance on homophilic graphs. TeDGCN-S achieves the best accuracies on two datasets: Cora and DBLP. On the remaining two datasets, TeDGCN-S is only \(1.1\%\) and \(0.4\%\) below the best baselines (APPNP on Citeseer and SGC on Pubmed). For TeDGCN-D, it obtains similar performance as the other methods, even though it only uses the top-\(K\) largest/smallest eigen-pairs.

**Heterophilic graphs**. Our TeDGCN-S or TeDGCN-D outperforms every baseline on all heterophilic graphs, as shown in Table 2. These results demonstrate that without manually setting the

  Datasets & Cora & Citeseer & Pubmed & DBLP \\  GCN & 80.8\(\)0.8 & 70.5\(\)0.6 & 78.8\(\)0.6 & 84.1\(\)0.2 \\ SGC & 80.9\(\)0.4 & 70.8\(\)0.8 & **79.6\(\)0.4** & 84.1\(\)0.2 \\ APPNP & 81.0\(\)0.1 & **71.9\(\)0.4** & 73.9\(\)0.2 & 83.0\(\)0.5 \\ GPRGNN & 82.0\(\)0.7 & 69.3\(\)0.9 & 78.6\(\)0.7 & 84.5\(\)0.3 \\ FAGCN & 80.3\(\)0.4 & 71.7\(\)0.8 & 78.5\(\)0.9 & 82.4\(\)0.7 \\ H2GCN & 78.8\(\)1.0 & 70.5\(\)1.0 & 77.9\(\)0.3 & 82.4\(\)0.3 \\ ChebNet & 78.8\(\)0.5 & 71.1\(\)0.4 & 78.1\(\)0.8 & 83.1\(\)0.1 \\ TeDGCN-S & **82.5\(\)1.1** & 70.8\(\)0.7 & 79.2\(\)0.2 & **84.7\(\)0.3** \\ TeDGCN-D & **82.4\(\)0.7** & 70.6\(\)0.6 & 77.9\(\)0.3 & 84.2\(\)0.2 \\  

Table 1: Performance comparison (mean\(\)std accuracy (%)) on homophilic graphs.

model depth and without the prior knowledge of the input graph (i.e., whether the input graph is homophilic or heterophilic), TeDGCN has the capability of automatically detecting the underlying graph heterophily. We have an interesting observation: on 3 large datasets, Squirrel, Chameleon, and cornell5, even with only a small portion of the eigengraphs, TeDGCN-D is able to achieve better performance than TeDGCN-S with the complete set of eigengraphs. This suggests that the graph signal in some real-world graphs might be dominated by a few low and high frequency components, and allowing two independent depth parameters in TeDGCN-D brings the flexibility to capture the low and high frequencies at the same time.

### Trainable Depth

A systematic study is conducted on the node classification performance w.r.t. the trainable depth \(d\). **Optimal depth**. In Figure 4, the optimal depths and their corresponding classification accuracies are annotated. For two homophilic graphs, Cora and Citeseer, the optimal depths are positive (\(5.029\) and \(3.735\)) in terms of the best ACCs, while for two heterophilic graphs, Actor and Squirrel, the optimal depths are negative (\(-0.027\) and \(-3.751\)). These results demonstrate that our model can indeed automatically capture graph heterophily/homophily by finding the suitable depth to suppress or amplify the relative weights of the corresponding frequency components. Namely, high/low frequency components are suppressed for homophilic/heterophilic graphs respectively.

**Close to zero depth.** For the two homophilic graphs in Figures 3(a) and 3(b), sharp performance drop is observed when depth \(d\) approaches \(0\), since the eigengraphs gain close-to-uniform weights. For the heterophilic Actor dataset, its optimal depth \(-0.027\) is close to \(0\), as shown in Figure 3(c). In addition, the performance of TeDGCN-D (\(27.6\%\)) is similar to that of GCN (\(27.5\%\)), both of which are much worse than TeDGCN-S (\(35.3\%\)). This result indicates that Actor is a special graph where all frequency components have similar importance. Due to the absence of the intermediate frequency components between the high- and low-end ones, the performance of TeDGCN-D is severely impacted. For vanilla GCN, the suppressed weights of the high frequency components deviate from the near-uniform spectrum and thus lead to low ACC on this dataset.

  Datasets & Texas & Cornell & Wisconsin \\  GCN & 55.9 & 44.3 & 51.4 \\ TeDGCN-S & **77.6** & 72.0 & 82.0 \\ TeDGCN-D & 77.1 & 72.0 & 81.5 \\  GCN (\(^{d}\)) & 75.9 & **72.7** & **83.4** \\  

Table 3: The performance of one-layer vanilla GCN over the augmented \(}^{d}\).

  Datasets & Texas & Cornell & Wisconsin \\  GCN & 55.9 & 44.3 & 51.4 \\ TeDGCN-S & **77.6** & 72.0 & 82.0 \\ TeDGCN-D & 77.1 & 72.0 & 81.5 \\  GCN (\(^{d}\)) & 75.9 & **72.7** & **83.4** \\  

Table 3: The performance of one-layer vanilla GCN over the augmented \(}^{d}\).

Figure 4: Node classification accuracy w.r.t. the trainable depth \(d\) on four datasets: Cora, Citeseer, Actor and Squirrel. (the optimal \(d\), accuracy) is annotated (e.g., (-0.027, 36.9%) for Actor).

  Datasets & Texas & Cornell & Wisconsin \\  GCN & 55.9 & 44.3 & 51.4 \\ TeDGCN-S & **77.6** & 72.0 & 82.0 \\ TeDGCN-D & 77.1 & 72.0 & 81.5 \\  GCN (\(^{d}\)) & 75.9 & **72.7** & **83.4** \\  

Table 2: Performance comparison (mean\(\)std accuracy (%)) on heterophilic graphs.

### Graph Augmentation and Geometric Insights

It is especially interesting to analyze what change a negative depth brings to the spatial domain and how such change impacts the subsequent model performance.

**Graph augmentation via negative depth.** By picking the optimal depth \(d\) according to the best performance on the validation set, a new diffusion matrix \(}^{d}\) is obtained.

With the optimal \(d\) fixed, substituting the normalized adjacency matrix \(}^{-}}}^{- {1}{2}}\) in Eq. (1) by \(}^{d}\) is equivalent to applying the vanilla GCN to a new topology. This topology effectively plays the role of a structural augmentation for the original graph. The impact of such augmentation on performance is tested on 3 heterophilic graphs: Texas, Cornell and Wisconsin, as shown in Table 3. Apparently, for the vanilla GCN, the performance obtained with this new topology is superior over that with the raw input graph: it dramatically brings \(20\%\)-\(30\%\) lifts in ACC. Moreover, the augmented topologies also make vanilla GCN outperform TeDGCN-S and TeDGCN-D on 2 out of the 3 datasets. By nature, the augmented graph is a re-weighted linear combination of the eigengraphs, and its topological structure intrinsically assigns higher weights to eigengraphs corresponding to higher frequencies, as shown in Figure 5.

**Geometric properties.** To further understand how the topology of \(}^{d}\) with a negative optimal \(d\) differs from that of \(}\) and why the performance is significantly boosted, heat maps of \((}^{d}-})\)s are presented in Figure 6. First, the dark red diagonal line in the heat map indicates the weights of self-loops are significantly strengthened in the augmented graph, and as a result, in consistency with the previous findings , the raw node attributes make more contributions in predicting their labels. These strengthened self-weights also play the similar role as restart distribution or skip connections  preventing the node embeddings becoming over-smoothed. In addition, there is a horizontal line and a vertical line (light yellow line marked by dashed ovals) in each heat map in Figure 6, correspond to the hub node in the graph, namely the node with the largest degree. Interestingly, the connections between this node and most other nodes in the graph experience a negative weight change. Therefore, the influence of the hub node on most other nodes are systematically reduced. Consequently, the augmentation amplifies the deviations between node embeddings and facilitates the characterization of graph heterophily.

Figure 5: The weights of eigengraphs w.r.t. eigenvalues on the augmented diffusion matrix \(}^{d}\) and original \(}\).

Figure 6: The difference between the augmented diffusion matrix and the original one \(}^{d}-}\).

Related Works

**Graph convolutional network (GCN).** GCN models can be mainly divided into two categories: (1) spectral graph convolutional networks and (2) spatial convolutional networks. In (1), Spectral CNN  borrows the idea from convolutional neural network  to construct a diagonal matrix as the convolution kernel. ChebNet  adopts a polynomial approximation of the convolution kernel. GCN  further simplifies the ChebNet via the first order approximation. Recently, [22; 4; 57] propose more advanced filters as the convolution kernel. Most works in (2) follow the _message-passing_ mechanism. GraphSAGE  iteratively aggregates features from local neighborhood. GAT  applies self-attention to the neighbors. APPNP  deploys personalized pagerank  to sample nodes for aggregation. MoNet  unifies GCNs in the spatial domain.

**The depth of GCN and over-smoothing.** A large amount of works focus on the over-smoothing issue. Its intrinsic cause is demystified: a linear GCN layer is a Laplacian smoothing operator [32; 59]. PairNorm  forces distant nodes to be distinctive by adding an intermediate normalization layer. Dropedge , DeepGCN , AS-GCN , and JK-net  borrow the idea of ResNet  to dis-intensify smoothing. DeeperGXX  adopts a topology-guided graph contrastive loss for connected node pairs to obtain discriminative representations. Most works aim to build deep GCNs (i.e., \(d\) is a large positive integer) by reducing over-smoothing, while TeDGCN extends the depth from \(+\) to \(\) and explores the negative depth.

**Node classification on homophilic and heterophilic graphs.** GCN/GNN models mostly follow the homophily assumption that connected nodes tend to share similar labels [28; 50; 19]. Recently, heterophilic graphs, in which neighbors often have disparate labels, attract lots of attention. Geom-GCN  and H2GCN  extend the neighborhood for aggregation. FAGCN  and GPRNN  adaptively integrate the high/low frequency signals with trainable parameters. Alternative message-passing mechanisms have been proposed in HOG-GCN  and CPGNN . The latest related works include ACM-GCN [37; 38], LINKX , BernNet , GloGNN  and GBK-GNN . Other works can be found in a recent survey .

## 6 Conclusion and Future Work

To our best knowledge, this work presents the first effort to make GCN's depth become negative and trainable by redefining it on the real number domain. We unveil the intrinsic connection between negative GCN depth and graph heterophily. A novel problem of automatic GCN depth tuning for graph homophily/heterophily detection is formulated, and we propose a simple and powerful solution named TeDGCN with two variants (TeDGCN-S and TeDGCN-D). An effective graph augmentation method is also discussed via the new understanding on the message propagation mechanism generated by the negative depth. Superior performance of our method is demonstrated via extensive experiments with semi-supervised node classification on 11 graph datasets. The new insights on GCN's depth obtained by our work may open a new direction for future research on spectral and spatial GNNs. Our paper studies the depth of graph convolutional network and graph spectrum, which has no negative ethical impacts on society. Since TeDGCN requires to conduct eigen-decomposition of the graph Laplacian, it is not directly applicable to inductive and dynamic graph learning problems. In addition, there may exist other options for the transformation functions such as the pseudoinverse of the Laplacian. We leave these for future exploration.