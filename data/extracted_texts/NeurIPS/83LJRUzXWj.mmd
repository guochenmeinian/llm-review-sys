# Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP

Qihang Yu1, Ju He2, Xueqing Deng1, Xiaohui Shen1, Liang-Chieh Chen1

1ByteDance 2 The Johns Hopkins University

###### Abstract

Open-vocabulary segmentation is a challenging task requiring segmenting and recognizing objects from an open set of categories in diverse environments. One way to address this challenge is to leverage multi-modal models, such as CLIP, to provide image and text features in a shared embedding space, which effectively bridges the gap between closed-vocabulary and open-vocabulary recognition. Hence, existing methods often adopt a two-stage framework to tackle the problem, where the inputs first go through a mask generator and then through the CLIP model along with the predicted masks. This process involves extracting features from raw images multiple times, which can be ineffective and inefficient. By contrast, we propose to build everything into a single-stage framework using a _shared **Frozen** Convolutional **CLIP**_ backbone, which not only significantly simplifies the current two-stage pipeline, but also remarkably yields a better accuracy-cost trade-off. The resulting single-stage system, called FC-CLIP, benefits from the following observations: the _frozen_ CLIP backbone maintains the ability of open-vocabulary classification and can also serve as a strong mask generator, and the _convolutional_ CLIP generalizes well to a larger input resolution than the one used during contrastive image-text pretraining. Surprisingly, FC-CLIP advances state-of-the-art results on various benchmarks, while running practically fast. Specifically, when training on COCO panoptic data only and testing in a zero-shot manner, FC-CLIP achieve 26.8 PQ, 16.8 AP, and 34.1 mIoU on ADE20K, 18.2 PQ, 27.9 mIoU on Mapillary Vistas, 44.0 PQ, 26.8 AP, 56.2 mIoU on Cityscapes, outperforming the prior art under the same setting by +4.2 PQ, +2.4 AP, +4.2 mIoU on ADE20K, +4.0 PQ on Mapillary Vistas and +20.1 PQ on Cityscapes, respectively. Additionally, the training and testing time of FC-CLIP is \(7.5\) and \(6.6\) significantly faster than the same prior art, while using \(5.9\) fewer total model parameters. Meanwhile, FC-CLIP also sets a new state-of-the-art performance across various open-vocabulary semantic segmentation datasets. Code and models are available at https://github.com/bytedance/fc-clip.

## 1 Introduction

Panoptic segmentation  is a complex computer vision task that aims to predict a set of non-overlapping masks, each with its corresponding class label. It combines the tasks of semantic segmentation  and instance segmentation , making it a challenging problem to solve. Many methods  have been proposed to tackle this problem, and a significant progress has been made in terms of panoptic quality (PQ). However, due to the high cost of annotating such a fine-grained dataset , the number of semantic classes is typically limited to a few dozens or hundreds. This restriction hinders the further application of existing approaches to real-world settings, where the number of possible semantic classes is unlimited.

To overcome the limitations of closed-vocabulary segmentation, open-vocabulary segmentation [48; 90; 29; 25] has been proposed. These approaches uses text embeddings of category names , represented in natural language, as label embeddings, instead of learning them from the training dataset. By doing so, models can classify objects from a wider vocabulary, which improves their ability to handle a broader range of categories. To ensure that meaningful embeddings are provided, a pretrained text encoder [23; 70; 57; 69] is typically used. This encoder can effectively capture the semantic meaning of words and phrases, which is critical for open-vocabulary segmentation.

Multi-modal models, such as CLIP  and ALIGN , have shown promise for open-vocabulary segmentation due to their ability to learn aligned image-text feature representations from large-scale Internet data . SimBaseline  and OVSeg  are two recent methods that use a two-stage framework to adapt CLIP for open-vocabulary segmentation. In these methods, images are first processed by a heavy mask generator [36; 20] to obtain mask proposals, and then each masked image crop is generated and fed into a frozen CLIP model for classification. MaskCLIP  extends this approach to open-vocabulary panoptic segmentation, but additionally leverages mask proposals as attention masks in the CLIP backbone to efficiently avoid multiple forwarding processes for the masked crops. More recently, ODISE  employs a stable diffusion UNet [72; 71] as a frozen backbone for mask generator, which significantly boosts the state-of-the-art performance. However, despite these advances, they still rely on a two-stage framework, where the mask generator and CLIP classifier extract features from raw images separately, resulting in inefficiency and ineffectiveness.

A natural question thus arises as to _whether it is possible to unify the mask generator and CLIP classifier into a single-stage framework for open-vocabulary segmentation_. Sharing the feature extractor between them is a straightforward solution, but it poses two challenges. First, fine-tuning CLIP backbone can disrupt the alignment between image and text features, resulting in a much worse performance on out-of-vocabulary categories. Existing methods [90; 52; 25; 89] rely on another separate backbone for mask generator, increasing model size and computational costs. Second, CLIP models are typically pretrained on relatively lower-resolution inputs, while dense prediction tasks require a much higher resolution for optimal performance. This makes it difficult to directly apply CLIP-pretrained backbones to downstream dense prediction tasks, particularly ViT-based CLIP models , where careful treatments are required (_e.g._, side adapter [17; 91], or cost aggregation [101; 21]). Consequently, existing methods [25; 89] perform mask segmentation and CLIP classification at different input scales, leading to sub-optimal performance.

To alleviate the two challenges, we propose to build both mask generator and CLIP classifier on top of a _shared **Frozen **C**onvolutional **CLIP**_ backbone, resulting in a single-stage framework FC-CLIP. Its

Figure 1: \(k\)**-means visualization on top of frozen CLIP backbone features w.r.t. different input resolutions.** Both ViT-based and CNN-based CLIP produces semantic-meaningful features. However, when scaling up the input resolutions, we note that ViT-based CLIP features turn noisier, while CNN-based ones are smoother and generalize better. The smoother feature map is preferable for mask-pooling modules in our design.

design is based on the following observations. The _frozen_ CLIP backbone ensures that the pretrained image-text feature alignment is intact, allowing out-of-vocabulary classification. It can also serve as a strong mask generator by appending a lightweight pixel decoder and mask decoder [20; 94]. The _convolutional_ CLIP, based on a Convolutional Neural Network (CNN) , empirically shows a better generalization ability compared to ViT-based CLIP , when the input size scales up. This echoes the success of fully convolutional networks  in dense prediction tasks. Both observations are critical for developing a single-stage framework, but they have been overlooked and undiscovered by existing two-stage pipelines [25; 89]. In Fig. 1, we visualize the learned visual representation of ViT-based and CNN-based CLIP via \(k\)-means clustering . As shown in the figure, the features learned by CNN-based CLIP are more robust across different input sizes.

Surprisingly, the adoption of a _single frozen convolutional_ CLIP as the shared feature extractor results in an extremely simple yet effective design. Specifically, the single-stage FC-CLIP consists of three modules built upon a shared frozen convolutional CLIP backbone: a class-agnostic mask generator, an in-vocabulary classifier, and an out-of-vocabulary classifier (see Fig. 2 for comparison between pipelines). The proposed method not only enjoys a simple design, but also comes with a very low cost for both training and testing. As a comparison, our model has only \(238\)M frozen parameters and \(21\)M trainable parameters, against the state-of-the-art work ODISE  that has \(1494\)M frozen and \(28\)M trainable parameters. Furthermore, our model training only takes \(25.6\) V100 GPU days, which is \(7.5\) faster compared to ODISE's \(192\) V100 GPU days. During inference, our model also runs \(6.6\) faster. Although FC-CLIP enjoys a simple design, it still outperforms previous methods across multiple datasets. Trained on COCO panoptic dataset only, FC-CLIP surpasses prior state-of-the-art ODISE  significantly in a zero-shot manner. Specifically, FC-CLIP achieves \(26.8\) PQ (\(+3.4\)), \(18.2\) PQ (\(+4.0\)), and \(44.0\) PQ (\(+20.1\)) on ADE20K, Mapillary Vistas, and Cityscapes, respectively.

As panoptic segmentation unifies semantic and instance segmentation, FC-CLIP naturally extends to open-vocabulary semantic and instance segmentation. With the same model trained on COCO panoptic data only (_i.e._, no task-specific fine-tuning), FC-CLIP achieves state-of-the-art performance on open-vocabulary instance and semantic segmentation. Specifically, FC-CLIP achieves \(16.8\) AP on ADE20K, surpassing the state-of-art ODISE  by \(+2.4\). FC-CLIP also outperforms the state-of-art specialized open-vocabulary semantic segmentation model SAN  by \(+1.1\) and \(+1.1\) mIoU on the challenging ADE20K-847 (A-847) and PASCAL-Context-459 (PC-459) benchmarks, respectively.

In summary, through the lens of a careful re-design of existing two-stage open-vocabulary segmentation models, we establish a simple, strong, and fast baseline for the community. The proposed FC-CLIP adopts a single-stage framework by exploiting a shared frozen convolutional CLIP, which not only advances the state-of-the-art performances on multiple benchmarks, but also enjoys a practically fast training and inference speed. We hope our study will inspire future research on efficient single-stage open-vocabulary segmentation models.

## 2 Related Work

Vision-language models target at encoding vision and language jointly in a fusion model. Early works [78; 16; 98] extract visual representations by pretrained object detectors and fine-tune on downstream tasks with language supervision. Recently, with the breakthrough of large language models [23; 3], rapid progress has been made in this field. CLIP  and ALIGN  demonstrate that pretraining dual-encoder models with contrastive objectives on large-scale noisy image-text pairs can learn representation with cross-modal alignment ability and show strong performance in zero-shot downstream tasks. The following works [95; 1; 92] further confirm these points and achieve impressive results in zero-shot transfer learning such as open-vocabulary image recognition.

Closed-vocabulary segmentation can be divided into three types according to the semantics of the grouping pixels, _i.e._ semantic, instance and panoptic segmentation. Semantic segmentation interprets high-level category semantic concepts. Prior works [9; 72; 10; 11; 13; 28; 96; 86; 99; 30] mainly treat this task as a per-pixel classification problem and build their models on top of the idea of FCN . Instance segmentation groups foreground pixels into different object instances. Starting from Mask R-CNN , prior works [42; 56; 12; 6; 2; 8; 80; 84; 66] mainly address this task with mask classification, where a set of bounding boxes and binary masks are predicted. Panoptic segmentation seeks for holistic scene understanding including both stuff and things. The pioneering work  and prevalent ones [55; 43; 87; 18; 50; 82; 14; 67] decompose the problem into various proxy tasks and merge the results in the end. Recently, following DETR , most works  present end-to-end solutions based on the idea of mask classification. Standing on their shoulders, our proposed method builds on top of the pixel decoder and mask decoder of Mask2Former  by additionally exploiting the open-vocabulary recognition ability from CLIP .

Open-vocabulary segmentation aims at segmenting arbitrary classes including those that can not be accessed during the training procedure. Priors works  perform open-vocabulary semantic segmentation through leveraging large pretrained vision-language models . Recently, MaskCLIP  presents a two-stage pipeline, which consists of a class-agnostic mask generator and a frozen CLIP  encoder for cross-modal alignment, and thus expands the scope of the CLIP models into open-vocabulary panoptic segmentation. ODISE  digs out the innate potential of pretrained text-image diffusion models  in terms of the ability to present open concepts in the representation space for performing strong open-vocabulary panoptic segmentation. FreeSeg  encodes multi-granularity concepts into a compact textural abstraction, enabling generalizability to arbitrary text description. Unlike those methods, we propose a single-stage framework by exploiting a single frozen convolutional CLIP backbone, resulting in a simpler, faster, and stronger model than existing works.

We also note that the pioneering work F-VLM  builds an open-vocabulary detection framework on top of a frozen CLIP backbone. However, FC-CLIP differs from it with a totally different observation and motivation. Specifically, our work was initially motivated by the state-of-art open-vocabulary segmentation model ODISE , which found that the CLIP backbone extracts noisier features than diffusion models (Figure B. 1. in ), leading to inferior segmentation results (which justifies their adoption of diffusion models). Their observation motivated us to look deeply into the problem. Interestingly, our discoveries show that both ViT-based (used by ODISE ) and CNN-based CLIP can produce semantic-meaningful features. However, when scaling up the input resolutions, we discover that ViT-based CLIP features turn noisier, while CNN-based ones are smoother and generalize better across input sizes. F-VLM  also empirically found that a frozen CLIP can provide meaningful features for object detection. However, they did not choose CNN-based CLIP on purpose and thus did not compare carefully between ViT-based and CNN-based CLIP backbones. On the other hand, in our paper, we have provided careful ablation studies on ViT-based and CNN-based CLIP, where we observe that even though both ViT-based and CNN-based CLIP initially have comparable performance at resolution \(224\), CNN-based CLIP shows better and more robust performance when input resolution scales up.

Figure 2: **Comparisons between open-vocabulary panoptic segmentation pipelines.**_Left_: Existing methods  adopt a two-stage pipeline, where the first stage employs a high-resolution image to generate class-agnostic masks, and the second stage feeds both the low-resolution image and predicted masks to a frozen CLIP backbone for open-vocabulary recognition. This incurs heavy computation, as image features are extracted multiple times. _Middle_: A naive single-stage framework builds everything together and fine-tunes the CLIP backbone, breaking the pretrained alignment between images and texts. _Right_: Our single-stage framework FC-CLIP employs a shared frozen convolutional CLIP, where “frozen CLIP” maintains the open-vocabulary recognition and can serve as a strong mask generator, and “convolutional CLIP” generalizes well to large input sizes. Note that the predicted masks are used for CLIP recognition in all three schemes (not shown for simplicity).

## 3 Method

In this section, we first define the problem of open-vocabulary segmentation. We then introduce the existing two-stage pipeline, followed by our proposed single-stage framework FC-CLIP.

Problem DefinitionOpen-vocabulary segmentation aims to segment the image \(^{H W 3}\) into a set of masks with associated semantic labels:

\[\{y_{i}\}_{i=1}^{K}=\{(m_{i},c_{i})\}_{i=1}^{K}\,.\] (1)

The \(K\) ground truth masks \(m_{i}\{0,1\}^{H W}\) contain the corresponding ground truth class label \(c_{i}\). During training, a fixed set of class labels \(C_{train}\) is used, while during inference, another set of categories \(C_{test}\) is used. In the open-vocabulary setting, \(C_{test}\) may contain novel categories unseen during training, _i.e._, \(C_{train} C_{test}\). We follow previous works [25; 89] and assume the availability of the category names of \(C_{test}\) (represented in natural language) during testing.

Two-Stage Open-Vocabulary SegmentationExisting works [90; 52; 25; 89] adopt a two-stage pipeline for open-vocabulary segmentation. The first stage contains a class-agnostic mask generator \(\) with parameters \(_{}\) that generates a set of \(N\) mask proposals \(\{_{i}\}_{i=1}^{N}^{N H W}\), given the input image \(\):

\[\{_{i}\}_{i=1}^{N}=(;_{})\,.\] (2)

In the second stage, a CLIP adapter \(\) takes both image \(\) and mask proposals \(\{_{i}\}_{i=1}^{N}\) as inputs, where the latter input is used to guide the frozen CLIP model \(CLIP^{*}\) (\({}^{*}\) denotes frozen). The adapter performs mask classification through forwarding processes with either masked crops [90; 52] or masked attention [25; 89]:

\[\{_{i}\}_{i=1}^{N}=(,\{_{i}\}_{i=1}^{N};CLIP ^{*})\,,\] (3)

where \(\{_{i}\}_{i=1}^{N}^{N|C|}\) refers to the predicted class probabilities for the \(N\) predicted masks, \(C\{C_{train},C_{test}\}\) depending on training or testing phase, and \(|C|\) is the category size.

Although this framework has achieved impressive open-vocabulary segmentation performance, it has two limitations. First, the image features are extracted _twice_, once for mask generation and the other for mask classification. The double feature extractions incur heavy computation, making it costly to scale up backbone parameters. Second, the mask generator often requires high-resolution inputs (_e.g._, \(1024 1024\)), whereas the CLIP model is usually pretrained with lower-resolution images (_e.g._,

Figure 3: **Overview of FC-CLIP, which contains three main components: mask generator, an in-vocabulary (in-vocab) classifier, and an out-of-vocabulary (out-vocab) classifier. All components build on top of a shared _frozen covolutional_ CLIP backbone. The pixel decoder and mask decoder follow the design of Mask2Former, and generate class-agnostic masks. The in-vocabulary classifier yields the class embeddings by mask-pooling over final pixel features from pixel decoder. During testing, FC-CLIP additionally exploits the out-of-vocabulary classifier by mask-pooling over frozen CLIP backbone features, and the final class prediction is obtained by geometric ensembling both classifiers. Note that the text embeddings are obtained by feeding category names into a CLIP text encoder, which are done beforehand and cached in memory, thus causing no additional costs. Also, the class-agnostic mask proposals are fed to the mask pooling modules (not shown for simplicity).**\(224 224\)). The two-stage pipeline thus needs to feed high-resolution images into the mask generator and low-resolution images into the CLIP classifier, making the model inefficient.

Naive Single-Stage Open-Vocabulary SegmentationTo avoid increasing the model size and computational cost of duplicate feature extractions, one may naively formulate everything together into a single-stage framework \(\), where both mask generator and mask classifier share the same CLIP-pretrained backbone \(CLIP\) (not frozen) for extracting features from an input image \(\):

\[\{_{i},_{i}\}_{i=1}^{N}=(;CLIP,_{M})\,.\] (4)

However, we empirically discover that fine-tuning this naive single-stage framework causes a misalignment between image and text features in the pretrained CLIP model, leading to sub-optimal performance, especially for novel unseen classes. It also increases the training costs by \(2.1\) to \(52.8\) GPU days. Interestingly, our experiments also show that a frozen CLIP backbone can provide sufficient features for mask generation, while preserving the image-text aligned representation. Nevertheless, we still face another challenge, where CLIP models are usually pretrained on low-resolution images (_e.g._, \(224 224\)), whereas segmentation models prefer higher-resolution inputs (_e.g._, \(800 1333\) for COCO, or \(1024 2048\) for Cityscapes). This discrepancy results in the significant performance degradation, when applying a frozen CLIP on large input images. Digging into the details, we found that it is related to the popular ViT  backbone used in CLIP that does not transfer well to different input sizes, which could be alleviated by extra careful designs (_e.g._, side adapter [17; 91], or cost aggregation [101; 21]). On the other hand, CNN-based CLIP models (such as ResNet  and ConvNeXt ) exhibit better generalization ability to different input sizes, due to their fully convolutional nature . Additionally, the CNN-based CLIP backbone, extracting multi-scale feature maps, can be used as a simple plug-in module into modern closed-vocabulary segmentation models [20; 94]. Motivated by the observations, we thus propose FC-CLIP, a simple yet effective single-stage open-vocabulary segmentation framework built entirely on a _single frozen convolutional_ CLIP backbone \(CLIP_{CNN}^{*}\).

\[\{_{i},_{i}\}_{i=1}^{N}=(;CLIP_{CNN}^{*}, _{M})\,.\] (5)

Fc-ClipThe proposed FC-CLIP leverages the semantic features of a frozen CNN-based CLIP backbone for both mask generation and CLIP classification. Unlike previous works [90; 52; 25; 89], which often train a separate mask generator and ignore the potential reuse of CLIP's semantic features, we incorporate the CNN-based CLIP backbone into the state-of-the-art segmentation method Mask2Former . We note that FC-CLIP is a general meta-architecture that can build on top of several modern segmentation methods [20; 94]. Our approach offers several advantages. By freezing and sharing the backbone features, our model is significantly more efficient during both training and testing (_i.e._, avoiding feature duplication). The CNN-based CLIP backbone not only transfers well to different input resolutions (from its pretrained image size), but also generates multi-scale feature maps, seamlessly compatible with modern segmentation methods [20; 94]. At a high level, FC-CLIP consists of three components: class-agnostic mask generator, in-vocabulary classifier, and out-of-vocabulary classifier. We detail each component below.

Class-Agnostic Mask GeneratorFollowing Mask2Former , we use a pixel decoder enhanced with multi-scale deformable attention  to improve the features extracted from the frozen CNN-based CLIP backbone. The enhanced pixel features, together with a set of object queries [7; 83], are then passed through a series of mask decoders, where each consists of masked cross-attention , self-attention , and a feed-forward network. The resulting segmentation logits are obtained by performing a matrix multiplication between the object query and pixel features. The predicted masks are matched with ground-truth masks in a one-to-one manner through Hungarian matching  and are supervised accordingly. Moreover, as the number of object queries is often greater than the number of labeled masks, only a subset of predicted masks are optimized through this matching process. We apply no penalty to the remaining unmatched proposals, which ensures that more mask proposals are obtained.

In-Vocabulary ClassifierOnce the mask proposals are predicted, they are classified with category text embedding in a contrastive manner, where the class embeddings for each mask and category text embeddings are projected into a common embedding space. That is, the predicted class probability by in-vocabulary classifier is defined as follows: \( i=1,,N\)

\[_{i,in}=softmax([cos(_{i},_{1}),\; cos(_{i},_{2}),\;,\;cos(_{i},_{|C|}) ]),\] (6)where \(T\) is a learnable temperature parameter with initialization of \(0.07\) to control the sharpness of the distribution, \(cos\) is cosine distance measurement, \(_{i}\) is the class embeddings for \(i\)-th predicted mask, which is obtained by mask pooling over the _final pixel features from pixel decoder_, similar to . \(_{j}\) is the category name's text embeddings of class \(j\), which is obtained by feeding the category name to a CLIP-pretrained text encoder. Note that these category text embeddings only need to be generated once. They are then kept in memory to serve as text classifiers, and thus it incurs negligible additional cost during training. This forms our in-vocabulary classifier.

Out-of-Vocabulary ClassifierDuring inference, however, we notice that using the in-vocabulary classifier alone fails to generalize to completely novel unseen classes, as the model is only trained on a finite set of categories and thus could not recognize diverse novel concepts. To address this issue, we introduce an out-of-vocabulary classifier, which applies mask pooling to the _frozen CLIP backbone features_, aiming to borrow the pretrained (intact) open-vocabulary recognition ability from CLIP. Unlike the other two-stage methods , where one or multiple forward processes of CLIP are needed, the adopted out-of-vocabulary classifier introduces marginal additional costs, since the backbone features are already extracted (and only lightweight mask-pooling is performed). The predicted class probability by out-of-vocabulary classifier \(_{i,out}\) is then obtained in a manner similar to Eq. (6) by replacing \(_{i}\) with the mask-pooled features over _frozen CLIP backbone features_. This classifier strictly maintains the original CLIP feature distribution, allowing us to better recognize brand new categories. Note that the out-of-vocabulary classifier is only performed during testing.

Combining In- and Out-of-Vocabulary ClassifiersFollowing prior works , we employ geometric ensemble to fuse the classification scores between in-vocabulary and out-of-vocabulary classifiers. That is, \( j=1,,|C|\)

\[_{i}(j)=(_{i,in}(j))^{(1-)}(_{i, out}(j))^{},&j C_{train}\\ (_{i,in}(j))^{(1-)}(_{i,out}(j))^{},&\] (7)

where \(_{i}(j)\) denotes the \(j\)-th element of \(_{i}\), and the underscripts \(in\) and \(out\) refer to in-vocabulary and out-of-vocabulary classifier, respectively. \(,\) balance the predictions between in- and out-of-vocabulary classifiers for seen and novel unseen categories.

## 4 Experimental Results

Herein, we provide implementation details of FC-CLIP in Sec. 4.1. After setting the stage, we introduce our main results, compared with state-of-the-art methods and ablations studies in Sec. 4.2.

### Implementation Details

ArchitectureWe use ConvNeXt-Large CLIP  backbones from OpenCLIP 1 pretrained on LAION-2B  dataset. On top of the CLIP backbone, we build the mask generator, following Mask2Former . Nine mask decoders are employed to generate the class-agnostic masks by taking as inputs the enhanced pixel features and a set of object queries. For in-vocabulary classification, following , the class embeddings are obtained by mask-pooling the pixel features from the pixel decoder's final output. Afterwards, the classification logits (before softmax) is obtained by matrix multiplication between the predicted class embeddings and categories' text embeddings.

Training StrategyWe follow  and adopt the same training recipe and losses without any special design. The training is optimized with AdamW  optimizer and weight decay \(0.05\). We use a crop size of \(1024 1024\). We employ the learning rate \(1 10^{-4}\) and a multi-step decay schedule. The training batch size is \(16\), and the model is trained for \(50\) epochs on COCO panoptic training set .

Inference StrategyDuring inference, the shorted side of input images will be resized to \(800\) while ensuring longer side not exceeds \(1333\). For Cityscapes and Mapillary Vistas, we increase the shorter side size to \(1024\). We adopt mask-wise merging scheme  for the mask predictions. The out-of-vocabulary classifier is only performed during inference by mask-pooling over the frozen CLIP backbone features. The final classification results are then obtained by geometric ensembling in- and out-of-vocabulary classifiers , as in Eq. (7), where we default \(=0.4\) and \(=0.8\). Following prior arts, we also adopt prompt engineering from [29; 89] and prompt templates from [31; 52]. If not specified, FC-CLIP is only trained on COCO panoptic dataset . Following prior works [29; 89], we zero-shot evaluate the model on ADE20K , Cityscapes , and Mapillary Vistas  for open-vocabulary panoptic segmentation. We also report open-vocabulary semantic segmentation results on those datasets along with PASCAL datasets [27; 63]. The panoptic segmentation results are evaluated with the panoptic quality (PQ) , Average Precision (AP), and mean intersection-over-union (mIoU), and semantic segmentation is evaluated with mIoU . Note that all results are obtained with the same single checkpoint trained on COCO panoptic data only.

### Results

We summarize the main results for open-vocabulary panoptic segmentation and semantic segmentation in Tab. 1, Tab. 2 and Tab. 3, where we train FC-CLIP on COCO _train_ set with panoptic annotation and evaluate it on various datasets in a zero-shot manner.

Open-Vocabulary Panoptic Segmentation Evaluation on ADE20KIn Tab. 1, we compare our FC-CLIP with other state-of-the-art methods on ADE20K , the main test-bed of zero-shot open-vocabulary panoptic segmentation. As shown in the table, our method achieves significantly better performance compared to MaskCLIP , with \(+11.7\) PQ, \(+10.8\) AP and \(+10.4\) mIoU, even though we use fewer frozen (\(-66\)M) and trainable (\(-42\)M) parameters. When compared to the concurrent methods FreeSeg  and ODISE , the advantage of FC-CLIP persists. FC-CLIP is \(+10.5\) PQ, \(+10.3\) AP, and \(+9.5\) mIoU better than FreeSeg without using COCO-Stuff annotations  (which contains more semantic classes than COCO-Panoptic). Our PQ, AP, mIoU score are also \(+4.2\), \(+2.4\), \(+4.2\) higher than ODISE under the same training settings. Compared to ODISE with caption  for supervision, our model still outperforms it by \(+3.4\) PQ, setting a new state-of-the-art record. Meanwhile, it is noticeable that our model has \(6.3\) (\(5.9\)) significantly fewer frozen (total) parameters compared to ODISE, which utilizes a strong large backbone from stable diffusion  for feature extraction.

Open-Vocabulary Panoptic Segmentation Evaluation on Street-View DatasetsIn Tab. 2, we evaluate on Cityscapes and Mapillary Vistas, which focus on street driving scenes. Compared to state-of-the-art method ODISE, FC-CLIP achieves better performances on both datasets. Specifically, it outperforms ODISE by \(+4.0\) PQ and \(+20.1\) PQ on Mapillary Vistas and Cityscapes, respectively. Notably, FC-CLIP has a slightly lower SQ, which indicates our mask generator is actually weaker than the one in ODISE, which utilizes a much larger backbone.

Open-Vocabulary Semantic Segmentation EvaluationAlthough our model was trained on COCO panoptic data only, it also performs well on open-vocabulary semantic segmentation. In Tab. 3, we

  & &  &  \\  &  &  &  \\ method & frozen & trainable & PQ & AP & mIoU & PQ & AP & mIoU \\  MaskCLIP  & 304 & 63 & 15.1 & 6.0 & 23.7 & - & - & - \\ FreeSeg  & - & - & 16.3 & 6.5 & 24.6 & - & - & - \\ ODISE  & 1494 & 28 & 22.6 & 14.4 & 29.9 & 55.4 & 46.0 & 65.2 \\ ODISE  (caption) & 1494 & 28 & 23.4 & 13.9 & 28.7 & 45.6 & 38.4 & 52.4 \\   FC-CLIP (ours) & 200 & 21 & 26.8 & 16.8 & 34.1 & 54.4 & 44.6 & 63.7 \\ 

Table 1: **Open-vocabulary panoptic segmentation performance on ADE20K.** The proposed FC-CLIP demonstrates better performances than prior arts, while using much fewer frozen parameters. We provide more results in the supplementary material

  &  \\  &  &  \\ method & PQ & SQ & RQ & mIoU & PQ & SQ & RQ & AP & mIoU \\  ODISE  & 14.2 & 61.0 & 17.2 & - & 23.9 & 75.3 & 29.0 & - & - \\   FC-CLIP (ours) & 18.2 & 57.7 & 22.9 & 27.9 & 44.0 & 75.4 & 53.6 & 26.8 & 56.2 \\ 

Table 2: **Open-vocabulary panoptic segmentation performance on street-view datasets**. The proposed FC-CLIP demonstrates better transferability to street-view datasetreport our model's performance on various benchmarks against other open-vocabulary segmentation models, where FC-CLIP shows an overall superior performance. Specifically, with the same training annotations used, FC-CLIP outperforms MaskCLIP by \(+6.6\), \(+8.2\), \(+10.4\), \(+12.5\) mIoU across A-847, PC-459, A-150, and PC-59, respectively. Compared to methods with caption annotations, FC-CLIP persists its advantages, where it outperforms ODISE (caption) by \(+3.8\), \(+4.4\), \(+5.4\), \(+3.1\) mIoU across datasets A-847, PC-459, A-150, PC-59 respectively. Against other open-vocabulary semantic segmentation methods, our model maintains its advantages across different datasets, despite being trained solely with panoptic annotations. Furthermore, it demonstrates comparable performance to state-of-the-art open-vocabulary semantic segmentation methods, which utilize the COCO-Stuff dataset as their training set. The COCO-Stuff dataset comprises 171 classes, 38 more classes than COCO-Panoptic, and offers highly desirable annotations for semantic segmentation tasks. It is worth mentioning that these methods build their approach on top of ViT-L (with extra designs ), resulting in a significantly larger model size compared to our deployed ConvNeXt-L (304M _vs_. 198M). Despite the disparity in model size, FC-CLIP remains competitive in terms of performance. Specifically, FC-CLIP outperforms state-of-the-art open-vocabulary semantic segmentation method SAN  by \(1.1\) and \(1.1\) mIoU on the challenging A-847 and PC-459 datasets.

**Inference Speed** We provide a comparison of FPS (frames per second) in Tab. 4. The proposed FC-CLIP not only demonstrates superior performances, but also enjoys a significant fast inference time: FC-CLIP runs \(6.61\) and \(7.08\) faster than ODISE evaluated on ADE20K and COCO datasets, respectively.

**Training on ADE20K and Evaluating on COCO** We further validate the effectiveness of FC-CLIP by using a different training dataset. Specifically, we follow  to train our model on ADE20K dataset with panoptic annotation, and evaluate it on COCO panoptic dataset. As shown in Tab. 5, FC-CLIP outperforms FreeSeg  by \(+10.5\) PQ, and ODISE  by \(+2.0\) PQ on COCO dataset. Notably, our model actually has a lower SQ (\(-1.4\)) compared to ODISE, which utilizes a much larger backbone and thus has a stronger mask generator. Nevertheless, FC-CLIP still outperforms ODISE significantly with a simple yet effective design.

**Fine-tuning CLIP Backbone Harms Performance on Novel Vocabularies** We validate the necessity of freezing CLIP backbone to ensure a better generalization to novel vocabularies. We compare the performance of trainable CLIP variant and frozen CLIP variant in Fig. 4, where we use the same mask proposals to ensure a fair comparison. Specifically, we compare the performance on

 method & ADE20K & COCO \\  ODISE  & 0.41 & 0.39 \\   FC-CLIP (ours) & 2.71 (6.61\(\)) & 2.76 (7.08\(\)) \\ 

Table 4: **FPS comparison. All results are obtained with one V100 GPU, CUDA 11.6 and PyTorch 1.13, by taking the average runtime on the entire validation set, including post-processing time**

 method & training dataset & A-847 & PC-459 & A-150 & PC-59 & PAS-21 & PAS-20 \\  SPNet  & Pascal VOC  & - & - & - & 24.3 & 18.3 & - \\ ZS3Net  & Pascal VOC  & - & - & - & 19.4 & 38.3 & - \\ LSeg  & Pascal VOC  & - & - & - & - & 47.4 & - \\  GroupViT  & GCC +FFC  & 4.3 & 4.9 & 10.6 & 25.9 & 50.7 & 52.3 \\  SimBaseline  & COCO Stuff  & - & - & 15.3 & - & 74.5 & - \\ ZegFormer  & COCO Stuff  & - & - & 16.4 & - & 73.3 & - \\ LSeg+  & COCO Stuff  & 3.8 & 7.8 & 18.0 & 46.5 & - & - \\ OVSeg  & COCO Stuff  & 9.0 & 12.4 & 29.6 & 55.7 & - & 94.5 \\ SAN  & COCO Stuff  & 13.7 & 17.1 & 33.3 & 60.2 & - & 95.5 \\  OpenSeg  & COCO Panoptic + COCO Caption & 6.3 & 9.0 & 21.1 & 42.1 & - & - \\ ODISE  (caption) & COCO Panoptic + COCO Caption & 11.0 & 13.8 & 28.7 & 55.3 & 82.7 & - \\  MaskCLIP  & COCO Panoptic & 8.2 & 10.0 & 23.7 & 45.9 & - & - \\ ODISE  & COCO Panoptic & 11.1 & 14.5 & 29.9 & 57.3 & 84.6 & - \\   FC-CLIP (ours) & COCO Panoptic & 14.8 & 18.2 & 34.1 & 58.4 & 81.8 & 95.4 \\ 

Table 3: **Open-vocabulary semantic segmentation performance. The proposed FC-CLIP also demonstrates state-of-the-art performances on open-vocabulary semantic segmentation**10 seen classes, which are shared by both COCO and ADE20K (_e.g._, person, sky), and 10 unseen classes, which are only included in ADE20K dataset (_e.g._, arcade machine, dishwasher). As shown in the figure, tuning CLIP backbone leads to a worse performance on unseen concepts, which breaks the CLIP feature alignment and thus loses its recognition ability on a much wider vocabulary.

## 5 Conclusion

In this work, we have presented FC-CLIP, a simple yet effective single-stage framework for open-vocabulary segmentation. FC-CLIP shows great potential by building everything on top of a shared frozen convolutional CLIP backbone, which not only significantly reduces training and testing costs, but also establishes a strong baseline on multiple benchmarks. Our study demonstrates how to better adapt a pretrained CLIP model for downstream dense prediction tasks, which we hope will shed the light on unleashing CLIP's potential for other various downstream tasks.

**Limitations** FC-CLIP presents a simple single-stage open-vocabulary segmentation framework with state-of-the-art performance. We note that there exist some interesting research topics to be explored in the near future, such as better unleashing CLIP's potential in both mask segmentation and classification, how to deal with conflict or overlapping vocabularies (_e.g._, cat _vs._ cat head), _etc_.

**Broader Impact** FC-CLIP shows great potential for segmenting and naming every object in the scene, which could facilitate many applications including intelligent home assistants, robots, self-driving, _etc_. Yet it relies on CLIP model pre-trained on the Internet data that may be biased, which calls for future research for calibration to avoid misuse.

Figure 4: **Trainable CLIP _vs._ Frozen CLIP, with per-class PQ analysis. We show 10 common classes (labeled in green) shared by COCO and ADE20K, and 10 novel classes (labeled in red) that are only in ADE20K. The frozen CLIP demonstrates a much better recognition ability for novel classes, while performing similarly for the seen classes.**

  &  &  \\  &  &  \\ method & PQ & SQ & RQ & PQ & SQ & RQ \\  FreeSeg  & 16.5 & 72.0 & 21.6 & - & - & - \\ ODISE  & 25.0 & 79.4 & 30.4 & 31.4 & 77.9 & 36.9 \\   FC-CLIP (ours) & 27.0 & 78.0 & 32.9 & 41.9 & 78.2 & 50.2 \\ 

Table 5: **Results of training on ADE20K panoptic and evaluating on COCO panoptic val set. The proposed FC-CLIP performs better than prior arts, even in the different setting (_i.e._, trained on ADE20K and zero-shot evaluated on COCO)**