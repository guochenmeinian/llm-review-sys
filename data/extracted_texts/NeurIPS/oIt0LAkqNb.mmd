# Rethinking the Evaluation of Out-of-Distribution Detection: A Sorites Paradox

Xingming Long\({}^{1,2}\), Jie Zhang\({}^{1,2}\), Shiguang Shan\({}^{1,2}\), Xilin Chen\({}^{1,2}\)

\({}^{1}\)Key Laboratory of AI Safety of CAS, Institute of Computing Technology,

Chinese Academy of Sciences (CAS), Beijing, China

\({}^{2}\)University of Chinese Academy of Sciences, Beijing, China

Corresponding author

###### Abstract

Most existing out-of-distribution (OOD) detection benchmarks classify samples with novel labels as the OOD data. However, some marginal OOD samples actually have close semantic contents to the in-distribution (ID) sample, which makes determining the OOD sample a Sorites Paradox. In this paper, we construct a benchmark named Incremental Shift OOD (IS-OOD) to address the issue, in which we divide the test samples into subsets with different semantic and covariate shift degrees relative to the ID dataset. The data division is achieved through a shift measuring method based on our proposed Language Aligned Image feature Decomposition (LAID). Moreover, we construct a Synthetic Incremental Shift (Syn-IS) dataset that contains high-quality generated images with more diverse covariate contents to complement the IS-OOD benchmark. We evaluate current OOD detection methods on our benchmark and find several important insights: (1) The performance of most OOD detection methods significantly improves as the semantic shift increases; (2) Some methods like GradNorm may have different OOD detection mechanisms as they rely less on semantic shifts to make decisions; (3) Excessive covariate shifts in the image are also likely to be considered as OOD for some methods. Our code and data are released in https://github.com/qqwsad5/IS-OOD.

## 1 Introduction

Deep neural networks achieve excellent results in many areas like computer vision and natural language understanding. However, though these well-trained models perform well on in-distribution (ID) test data sampled from the same distribution with the training set, they tend to struggle when confronted with the data drawn from out-of-distribution (OOD). For example, encountering previously unseen classes can result in the model making overly confident predictions . This highlights the importance of ensuring the model's safety on such OOD data. An important research focus is OOD detection, which aims to enable models to detect such OOD samples rather than making incorrect judgments about them. There has already been considerable research and notable progress in the field of OOD detection [1; 2; 3; 4; 5; 6; 7; 8]. Compared to works on OOD generalization that focus on models' robustness to covariate shifts (such as changes in data style), these OOD detection works are mainly dedicated to capturing the semantic shifts (such as the novel classes).

Simultaneously, various benchmarks are constructed to study the performance of the above OOD detection methods [9; 10; 11; 12; 13; 14; 15]. Most of these works use two datasets with non-overlapping semantic labels as the ID and OOD data to construct their benchmarks. The ID dataset is divided into two parts: the first part serves as the training set, and the second part is mixed with theOOD dataset to form the test set. The OOD detection model is trained on the training set and then evaluated on the mixed test set to assess its ability to detect the OOD data.

However, we find there exist drawbacks in the current OOD detection benchmarks that use semantic labels to distinguish OOD samples. Some marginal OOD test samples' semantic contents are actually very close to the ID data even if they have different semantic labels. The reason lies in the fact that the image's semantic labels are sometimes inaccurate; for example, they may omit background objects or common sense relationships portrayed in image . This inaccuracy will lead to issues like similar labels (e.g., "African bush elephant" vs. "African elephant"), overlapping labels (e.g., "corn" vs. "food"), or insufficient labels (the label of the OOD sample do not contain the ID object in the image, e.g., an "athlete" wears a "T-shirt"), as illustrated in Figure 2. Although some researchers also recognize the problem and manually filter in-distribution data from the test sets [13; 14], these works are labor-intensive and may introduce subjective biases of individuals.

In our opinion, the key issue of "determining whether a data is an OOD sample" is actually a Sorites Paradox. Just like the dilemma of "how many grains of sand can be removed from a heap before it ceases to be a heap?", we cannot provide a precise definition for "how different a sample must be from the ID data to be considered an OOD sample?". Therefore, to address the issue, we need a method to measure "the degree of shifts relative to the ID data" ("how much sand has been removed") rather than continuing to debate "whether it is an OOD sample?" ("whether it ceases to be a heap?").

In this paper, we propose a shift measuring method and construct an Incremental Shift OOD (IS-OOD) detection benchmark that divides the test samples (from ImageNet-21K ) into subsets with different shift levels relative to the ID dataset (ImageNet-1K ), as shown in Figure 1. Considering the covariate content is also a potential influencing factor in OOD detection works , we propose a Language Aligned Image feature Decomposition (LAID) based on CLIP  features and thus measure the semantic and covariate shifts separately. Moreover, considering the limited covariate variation (such as limited types of styles) in ImageNet-21K, we construct a Synthetic Incremental Shift (Syn-IS) dataset that contains a series of high-quality generated images with more diverse

Figure 1: **Examples of images from IS-OOD benchmark.** ImageNet-21K is divided into subsets with different semantic and covariate shift levels relative to ImageNet-1K. As semantic shift increases, images of the subsets change from marginal samples (such as animal subspecies) to more distinct OOD categories (such as “gasket”). As covariate shift increases, the covariate contents transition from object-centered real photos to synthetic images, and from high-definition color images to low-resolution monochrome images.

covariate contents to complement the IS-OOD benchmark. We discuss some existing benchmarks and compare them with our IS-OOD in Appendix A.

The main contributions of this work are summarized as follows:

* We construct an Incremental Shift OOD (IS-OOD) detection benchmark that divides the test samples into subsets with different levels of semantic and covariate shifts. We further generate a Synthetic Incremental Shift (Syn-IS) dataset that contains a series of high-quality images with more diverse covariate contents to complement the IS-OOD benchmark.
* We propose a Language Aligned Image feature Decomposition (LAID) method to obtain the semantic and covariate features of test images for shift measuring. Specifically, we utilize the decomposition of the CLIP's text features to determine the corresponding decomposition of the image features.
* We uncover several important insights with the proposed benchmark: (1) The performance of most OOD detection methods significantly improves as the semantic shift increases; (2) Some methods like GradNorm may have different OOD detection mechanisms as they rely less on semantic shifts to make decisions; (3) Excessive covariate shifts in the image are also likely to be considered as OOD for some methods.

## 2 Benchmark Construction

In this section, we will detail the construction of our benchmark. First, we will introduce our proposed Language Aligned Image feature Decomposition (LAID) method. Next, we will explain how we utilize the decomposition result to measure the shifts and construct the proposed Incremental Shift OOD (IS-OOD) benchmark. Following that, we will describe how we generate the Synthetic Incremental Shift (Syn-IS) dataset. Finally, we will briefly introduce the metrics we use for evaluating the OOD detection methods.

### Feature Decomposition

We find that large-scale image datasets often lack covariate labels (such as the style or the augmentation of the image), and it is not easy to accurately decompose the covariate features based solely on the semantic labels. Although some datasets include covariate labels, such as ImageNet-R  and PACS , they are small in scale and offer limited types of covariate labels.

Figure 2: **Examples of noise caused by inaccurate semantic labels.** The images in the row below are semantically similar to the ID data (images in the row above), yet they are considered OOD samples in some benchmarks for their labels.

Inspired by many works that leverage the aligned text and image features of the CLIP model [22; 23; 24; 25], we propose the Language Aligned Image feature Decomposition (LAID). Given that text is easily editable, we can effortlessly construct a text dataset with diverse semantic and covariate contents through text concatenation. We then train a decomposition matrix in the text feature space and apply it to the image feature space using the alignment property of CLIP .

The overview of LAID is shown in Figure 3. To ensure the CLIP features' information is preserved, we use an orthogonal transformation matrix \(W\) for the feature decomposition. After a feature \(f^{l}\) undergoes transformation by the matrix \(W^{l l}\), we designate the first half as the semantic feature \(f_{sem}^{l/2}\) and the second half as the covariate feature \(f_{cov}^{l/2}\):

\[ f_{sem}&=(f W)[1:l/2],\\ f_{cov}&=(f W)[l/2+1:l],\] (1)

where \(l\) represents the length of the CLIP feature.

The transformation matrix is optimized through contrastive learning. Specifically, in each training iteration, we construct a standard text and two contrast texts. One of the contrast texts exhibits only the semantic shift, while the other carries only the covariate shift. The semantic parts of these texts are selected from the ImageNet-21K labels. The covariate parts are derived from the prompts used in the CLIP zero-shot task , as these prompts contain a variety of covariate types like image styles and augmentation methods. The features of the standard text, semantic shift text, and covariate shift text, obtained by the CLIP text encoder, are denoted as \(T^{st}\), \(T^{ss}\), and \(T^{cs}\). The relationships among these features are then constrained by triplet loss:

\[ L_{sem}&=dist(T^{st}_{sem},T^{cs}_{ sem})-dist(T^{st}_{sem},T^{ss}_{sem})+,\\ L_{cov}&=dist(T^{st}_{cov},T^{ss}_{cov})-dist(T^{ st}_{cov},T^{cs}_{cov})+,\] (2)

where \(dist(,)\) represents the cosine distance between two features, and \(\) is a pre-defined margin.

Finally, we add a regularization loss to ensure \(W\) remains an orthogonal matrix.

\[L_{orth}=\|W^{T}W-I\|_{2}^{2}.\] (3)

After training on the constructed text dataset, we can achieve decomposition in the text feature space through the optimized transformation matrix \(W\), thus obtaining the corresponding decomposition in the image feature space. Analyses regarding the effectiveness of the feature decomposition methods above are provided in Appendix B.

Figure 3: **Overview of Language Aligned Image feature Decomposition (LAID) method. We first construct texts using different semantic and covariate prompts and train an orthogonal transformation matrix for the decomposition in the text feature space. Then, we can apply this matrix to the decomposition in the image feature space leveraging the alignment property of the CLIP model.**

### Shift Measuring and Subsets Division

The measurement of the shifts between the test data (from ImageNet-21K ) and the ID dataset (ImageNet-1K ) is achieved based on the decomposition results above. We employ the decomposition method on the test sample and compute its semantic and covariate feature distance corresponding to each sample in the ID dataset. We can then use the nearest semantic or covariate distance between the test sample and the entire ID dataset to measure the degree of semantic or covariate shift for this test sample:

\[ D_{sem}(I^{test})&=_{I^{id}}dist(I ^{test}_{sem},I^{id}_{sem}),\\ D_{cov}(I^{test})&=_{I^{id}}dist(I^{test}_{cov}, I^{id}_{cov}),\] (4)

where \(I^{test}\) and \(I^{id}\) respectively represent the features of the test sample and the ID dataset.

We then categorize the test samples from ImageNet-21K into different shift levels according to their shift degrees. We split the shift degree in each of the semantic and covariate directions into 8 levels (a total of 8x8=64 subsets), ensuring a reasonable distribution while maintaining uniformity in the segmentation of intervals. ImageNet-21K dataset is thus divided into subsets with different semantic and covariate shift levels, and examples from different subsets are shown in Figure 1. Details of dividing the subsets can be found in Appendix C.

### Generation of Syn-IS

We find that the covariate variation (such as the style types) is limited even in a large-scale dataset like ImageNet-21K. We also observe that as the quality of generated images improves, an increasing number of studies are using these generated images for visual tasks to address the weakness in existing datasets . In order to enhance the diversity of the covariate components, we construct a Synthetic Incremental Shift (Syn-IS) dataset that contains a series of high-quality generated images with different semantic and covariate shift levels to complement our benchmark.

To ensure the covariate diversity of the generated image, we choose the official style templates provided by Stable Diffusion XL (SDXL)  as the covariate contents. The style templates are paired with ImageNet-21K labels to create the collection of prompts for the generation. The details of the prompts for generating Syn-IS are provided in Appendix D. We then divide the prompt collection into subsets with different semantic and covariate shift levels relative to ImageNet-1K. The degree of

Figure 4: **Examples for the images in different Syn-IS subsets and their corresponding prompts.** Subsets with low covariate shifts typically include more realistic-style images (such as ”HDR photo”), whereas subsets with high covariate shifts tend to contain more abstract-style images (such as ”papercut”).

shift is measured by the distance between the CLIP text features of these constructed prompts and the CLIP image features of ImageNet-1K. Each prompt subset is used to generate the corresponding subset of images with specific semantic and covariate shift levels.

We employ the SDXL-Turbo model (a distilled version of SDXL  based on Adversarial Diffusion Distillation ) to achieve the text-to-image generation. Examples of the generated images and their corresponding prompts are shown in Figure 4. It can be seen that the generated images indeed exhibit more diverse covariate contents, which effectively fills the gaps in ImageNet-21K and serves as a good supplementary to our benchmark.

### Metrics

Following OpenOOD , we use three metrics to evaluate the performance of OOD detection methods on our benchmark: FPR@95, AUROC, and AUPR.

In addition to the three metrics above, we introduce two more metrics to study the changes in the model's performance across different shift levels. We first use the Pearson correlation coefficient to evaluate the relationship between the model's performance and the shift levels:

\[correlation=^{n}(x_{i}-)(i-)}{^{n}(x_{i}-)^{2}_{i=1}^{n}(i-)^{2}}},\] (5)

where \(i\) represents the levels of semantic or covariate shift in the subset (\(1\) being the smallest, \(n\) being the largest), and \(x_{i}\) represents the model's performance (such as AUROC) on shift level \(i\).

To further investigate the extent of changes in model performance, we define a model's sensitivity to the corresponding shifts as follows:

\[sensitivity=|^{n}(x_{i}-)(i-)}{_{i=1 }^{n}(i-)^{2}}|.\] (6)

The metric reflects the change in the model's performance when increasing one corresponding shift level. We assume that a good OOD detection model should have a high correlation with semantic shifts, and its performance should not vary significantly with covariate shifts. Therefore, we believe that a higher semantic sensitivity and a lower covariate sensitivity indicate a better method.

## 3 Experiments

In this section, we present the experimental results and corresponding findings from the proposed benchmark. During the experiments, each time we choose one divided subset from IS-OOD as the OOD data and mix it with ImageNet-1K test set to evaluate the OOD detection models trained on ImageNet-1K training set. Since the training data are from ImageNet-1K, all the shift levels mentioned in the experiments are the shifts of the test samples relative to ImageNet-1K. All evaluated OOD detection methods are implemented using a ResNet-50  classifier trained on ImageNet-1K. The details of the evaluated OOD detection methods are introduced in Appendix E. Due to space limitations, only part of the results based on the AUROC metric are presented in this section. Complete experimental results including the metrics FPR@95 and AUPR can be found in Appendix F.

### Main Results on ImageNet-21K

We first present the performance of the OOD detection methods on ImageNet-21K subsets according to different levels of semantic and covariate shifts. Due to the number of data in some subsets being too small, we omit these subsets and mark their results as "N/A". The results for part of the OOD detection methods are shown in Figure 5. From the results, we can make the following important observation:

**OOD detection methods perform better when there is a large semantic shift and a small covariate shift.** In the results, the subsets where these methods perform best are those with large semantic shifts and small covariate shifts. This observation confirms that OOD detection methods are not only sensitive to semantic shifts but also disturbed by covariate shifts. However, all these methods are not significantly affected by the covariate shifts. The primary factor influencing their performance is still the semantic shifts.

We then display how the performance of each method varies according to different levels of semantic shifts or covariate shifts separately, as shown in Figure 6. Specifically, we calculate the average result across different covariate shifts at each semantic shift level and draw the curves. The results for the covariate shift are obtained with the same approach. We can compare the performance of different methods and further derive the following insights:

**The performance of most methods significantly improves as the semantic shift increases.** It can be seen from the results for the semantic shift, that most OOD detection methods gain a nearly \(40\%\) increase in AUROC under the largest semantic shift compared to the result under the smallest semantic shift, except for GradNorm and RankFeat. Conversely, the AUROC of all the methods does not change a lot across different covariate shifts. The experimental results confirm the previous finding that the degree of semantic shifts is the primary factor that influences whether most OOD detection methods are capable of distinguishing between ID and OOD data.

**Some OOD detection methods rely less on semantic shifts to make decisions.** The AUROC of GradNorm and RankFeat do not significantly change with the semantic shift levels. We assume that these methods might make the prediction based on some low-level features that the CLIP encoders are unable to extract. This provides important insights for the study of the detection mechanisms of different OOD detection methods.

We calculate the correlation and sensitivity of each method, and the results are presented in Table 1. The results show that most methods exhibit positive correlation and higher sensitivity to semantic shifts while demonstrating negative correlation and lower sensitivity to covariate shifts, which is consistent with our discoveries above. Among these methods, KNN performs the best in terms of its semantic sensitivity. ODIN shows the lowest sensitivity to covariate shifts, meaning it is the least affected by changes in covariate contents. GradNorm and RankFeat obtain the lowest semantic sensitivity, which further suggests that these methods seem to rely less on semantic changes for OOD detection.

Figure 5: OOD detection performance on all ImageNet-21K subsets with different semantic and covariate shift levels. ”N/A” indicates the number of data in this subset is too small for a fair evaluation.

Figure 6: Comparison of OOD detection methods across different semantic or covariate shift levels on ImageNet-21K.

### Results on Syn-IS

Subsequently, we conduct experiments on the generated Syn-IS dataset using the same methods above, with the results shown in Figure 7, Figure 8, and Table 2.

From the results in Figure 8, we can see that the performance of most OOD detection methods still varies with semantic shifts, which further suggests that these methods are capable of conducting OOD detection based on the semantic contents even with generated data. However, we observe some interesting experimental insights that do not exist on the ImageNet-21K subsets.

**Samples with Excessive covariate shifts are also considered OOD.** We observe that, unlike the results on ImageNet-21K subsets where the covariate correlation of almost all methods is negative, many methods on Syn-IS show a positive correlation with the covariate shift levels. We assume this is because the covariate contents in ImageNet-21K are similar to those in ImageNet-1K, which means ImageNet-21K does not include samples with large covariate shifts (such as drastic style changes). In this case, the covariate shifts affect the extraction of the semantic features, thereby reducing the models' OOD detection capabilities. However, as the style shift continues to increase, such as the "papercut" and "constructivist" styles shown in Figure 4, the style information itself is so pronounced that it is seen as the OOD contents by the models, which then improves the models' OOD detection performance.

**The "generative" attribute of Syn-IS improves the performance of most models.** We observe that on Syn-IS, the semantic sensitivity of most methods decreases compared with the results on ImageNet-21K, as these models perform better at lower semantic shift levels on Syn-IS. We assume the reason is that generated images differ from real images. Even if the semantic and covariate contents of a generated image and a real image remain the same, the model can still distinguish them to some extent based on the unique patterns of the generated image.

**On Syn-IS dataset, the performance of GradNorm declines significantly.** We further analyze the principle of GradNorm and the characteristics of the Syn-IS dataset. We find that GradNorm tends to classify samples with uniform softmax outputs as OOD samples. Since each Syn-IS image is generated with a text that only includes one given label, the image is likely to contain a single object, unlike many images in ImageNet-21K that contain multiple objects. This difference could lead to less uniform softmax outputs on Syn-IS compared to Imagenet-21K, resulting in lower OOD

   &  &  \\   & correlation & sensitivity & correlation & sensitivity \\  MSP  & 0.97 & 5.59 & -0.96 & 1.95 \\ ODIN  & 0.97 & 5.26 & -0.63 & 0.46 \\ MDS  & 0.98 & 3.50 & -0.91 & 1.98 \\ GradNorm  & 0.91 & 1.27 & -0.49 & 0.56 \\ KNN  & 0.98 & 6.64 & -0.93 & 2.18 \\ DICE  & 0.97 & 4.52 & -0.88 & 1.29 \\ RankFeat  & 0.78 & 0.79 & 0.51 & 0.83 \\ ASH  & 0.98 & 5.56 & -0.89 & 1.73 \\  

Table 1: Results of correlation and sensitivity for OOD detection methods on ImageNet-21K

Figure 7: OOD detection performance on all Syn-IS subsets with different semantic and covariate shift levels.

scores produced by GradNorm. This insight might highlight a potential limitation of the GradNorm approach.

## 4 Conclusion and Discussion

In this paper, we construct the IS-OOD benchmark that divides the test samples into subsets with different levels of semantic and covariate shifts relative to the ID dataset. Unlike most past works that rely on semantic labels, our benchmark utilizes the degree of shifts to categorize the test dataset, thereby avoiding the debate over determining whether a test sample is OOD. This benchmark helps in comprehensively analyzing the models' sensitivity to the semantic and covariate shifts. With our benchmark, we uncover several important insights: (1) The performance of most OOD detection methods significantly improves as the semantic shift increases; (2) Some methods like GradNorm may have different OOD detection mechanisms as they rely less on semantic shifts to make decisions; (3) Excessive covariate shifts in the image are also likely to be considered as OOD for some methods.

**Limitation:** The alignment between CLIP's text feature space and image feature space is not perfect, which may lead to a certain gap between the decomposition matrices in the two feature spaces. Future works can focus on narrowing this gap with a better vision-language model to enhance the accuracy of the shift measuring method in the benchmark.

**Societal Impact:** We conduct safety checks on both the prompts and the generated images for the Syn-IS dataset, which helps avoid potential negative societal impacts. As for our benchmark, it evaluates OOD detection models' sensitivity to test samples with varying shift degrees relative to the ID data, which is crucial for the safe deployment of the models. Therefore, we believe our work has positive societal impacts.