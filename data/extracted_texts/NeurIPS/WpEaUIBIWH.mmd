# Towards a Unified Framework of Clustering-based Anomaly Detection

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Unsupervised Anomaly Detection (UAD) plays a crucial role in identifying abnormal patterns within data without labeled examples, holding significant practical implications across various domains. Although the individual contributions of representation learning and clustering to anomaly detection are well-established, their interdependencies remain under-explored due to the absence of a unified theoretical framework. Consequently, their collective potential to enhance anomaly detection performance remains largely untapped. To bridge this gap, in this paper, we propose a novel probabilistic mixture model for anomaly detection to establish a theoretical connection among representation learning, clustering, and anomaly detection. By maximizing a novel anomaly-aware data likelihood, representation learning and clustering can effectively reduce the adverse impact of anomalous data and collaboratively benefit anomaly detection. Meanwhile, a theoretically substantiated anomaly score is naturally derived from this framework. Lastly, drawing inspiration from gravitational analysis in physics, we have devised an improved anomaly score that more effectively harnesses the combined power of representation learning and clustering. Extensive experiments, involving 17 baseline methods across 30 diverse datasets, validate the effectiveness and generalization capability of the proposed method, surpassing state-of-the-art methods.

## 1 Introduction

Unsupervised Anomaly Detection (UAD) refers to the task dedicated to identifying abnormal patterns or instances within data in the absence of labeled examples . It has long received extensive attention in the past decades for its wide-ranging applications in numerous practical scenarios, including financial auditing , healthcare monitoring  and e-commerce sector . Due to the lack of explicit label guidance, the key to UAD is to uncover the dominant patterns that widely exist in the dataset so that samples do not conform to these patterns can be recognized as anomalies. To achieve this, early works  have heavily relied on powerful unsupervised _representation learning_ methods to extract the normal patterns from high-dimensional and complex data such as images, text, and graphs. More recent works [45; 2] have utilized _clustering_, a widely observed natural pattern in real-world data, to provide critical global information for anomaly detection and achieved tremendous success.

While the individual contributions of representation learning and clustering to anomaly detection are well-established, their interrelationships remain largely unexplored. Intuitively, _discriminative representation learning_ can leverage accurate clustering results to differentiate samples from distinct clusters in the embedding space (i.e., 1). Similarly, it can utilize accurate anomaly detection to avoid preserving abnormal patterns (i.e., 2). For _accurate clustering_, it can gain advantages from representation learning by operating in the discriminative embedding space (i.e., 3). Meanwhile, itcan potentially benefit from accurate anomaly detection by excluding anomalies when formulating clusters (i.e., ). _Anomaly detection_ can greatly benefit from both discriminative representation learning and accurate clustering (i.e., & & ). However, these benefits hinge on the successful identification of anomalies and the reduction of their detrimental impact on the aforementioned tasks. As depicted in Figure 1, the integration of these three elements exhibits a significant reciprocal nature. In summary, representation learning, clustering, and anomaly detection are interdependent and intricately intertwined. Therefore, it is crucial for anomaly detection to _fully leverage and mutually enhance the relationships among these three components_.

Despite the intuitive significance of the interactions among representation learning, clustering, and anomaly detection, existing methods have only made limited attempts to exploit them and fall short of expectations. On one hand, some methods  have acknowledged the interplay among these three factors, but their focus remains primarily on the interactions between two factors at a time, making only targeted improvements. For instance, some strategies include explicitly removing outlier samples during the clustering process  or designing robust representation learning methods  to mitigate the influence of anomalies. On the other hand, recent methods  have begun to explore the simultaneous optimization of these three factors within a single framework. However, these attempts are still in the stage of merely superimposing the objectives of the three factors without a unified theoretical framework. This lack of a guiding framework prevents the adequate modeling of the interdependencies among these factors, thereby limiting their collective contribution to a unified anomaly detection objective. Consequently, we aim to address the following question: _Is it possible to employ a unified theoretical framework to jointly model these three interdependent objectives, thereby leveraging their respective strengths to enhance anomaly detection?_

In this paper, we try to answer this question and propose a novel model named UniCAD for anomaly detection. The proposed UniCAD integrates representation learning, clustering, and anomaly detection into a unified framework, achieved through the theoretical guidance of maximizing the anomaly-aware data likelihood. Specifically, we explicitly model the relationships between samples and multiple clusters in the representation space using the probabilistic mixture models for the likelihood estimation. Moreover, we creatively introduce a learnable indicator function into the objective of maximum likelihood to explicitly attenuate the influence of anomalies on representation learning and clustering. Under this framework, we can theoretically derive an anomaly score that indicates the abnormality of samples, rather than heuristically designing it based on clustering results as existing works do. Furthermore, building upon this theoretically supported anomaly score and inspired by the theory of universal gravitation, we propose a more comprehensive anomaly metric that considers the complex relationships between samples and multiple clusters. This allows us to better utilize the learned representations and clustering results from this framework for anomaly detection.

To sum up, we underline our contributions as follows:

* We propose a unified theoretical framework to jointly optimize representation learning, clustering, and anomaly detection, allowing their mutual enhancement and aid in anomaly detection.
* Based on the proposed framework, we derive a theoretically grounded anomaly score and further introduce a more comprehensive score with the vector summation, which fully releases the power of the framework for effective anomaly detection.
* Extensive experiments have been conducted on 30 datasets to validate the superior unsupervised anomaly detection performance of our approach, which surpassed the state-of-the-art through comparative evaluations with 17 baseline methods.

Figure 1: Interdependent relationships among representation learning, clustering, and anomaly detection.

Related Work

Typical unsupervised anomaly detection (UAD) methods calculate a continuous score for each sample to measure its anomaly degree. Various UAD methods have been proposed based on different assumptions, making them suitable for detecting various types of anomaly patterns, including subspace-based models , statistical models , linear models [49; 32], density-based models [6; 38], ensemble-based models [39; 29], probability-based models [40; 58; 28; 27], neural network-based models [42; 51], and cluster-based models [18; 9]. Considering the field of anomaly detection has progressed by integrating clustering information to enhance detection accuracy [26; 56], we primarily focus on and analyze anomaly patterns related to clustering, incorporating a global clustering perspective to assess the degree of anomaly. Notable methods in this context include CBLOF , which evaluates anomalies based on the size of the nearest cluster and the distance to the nearest large cluster. Similarly, DCFOD  introduces innovation by applying the self-training architecture of the deep clustering  to outlier detection. Meanwhile, DAGMM  combines deep autoencoders with Gaussian mixture models, utilizing sample energy as a metric to quantify the anomaly degree. In contrast, our approach introduces a unified theoretical framework that integrates representation learning, clustering, and anomaly detection, overcoming the limitations of heuristic designs and the overlooked anomaly influence in existing methods.

## 3 Methodology

In this section, we first define the problem we studied and the notations used in this paper. Then we elaborate on the proposed method UniCAD. More specifically, we first introduce a novel learning objective that optimizes representation learning, clustering, and anomaly detection within a unified theoretical framework by maximizing the data likelihood. A novel anomaly score with theoretical support is also naturally derived from this framework. Then, inspired by the concept of universal gravitation, we further propose an enhanced anomaly scoring approach that leverages the intricate relationship between samples and clustering to detect anomalies effectively. Finally, we present an efficient iterative optimization strategy to optimize this model and provide a complexity analysis for the proposed model.

**Definition 1** (Unsupervised Anomaly Detection).: _Given a dataset \(^{N D}\) comprising \(N\) instances with \(D\)-dimensional features, unsupervised anomaly detection aims to learn an anomaly score \(o_{i}\) for each instance \(_{i}\) in an unsupervised manner so that the abnormal ones have higher scores than the normal ones._

### Maximizing Anomaly-aware Likelihood

Previous research has demonstrated the importance of discriminative representation and accurate clustering in anomaly detection . However, the presence of anomalous samples can significantly disrupt the effectiveness of both representation learning and clustering . While some existing studies have attempted to integrate these three separate learning objectives, the lack of a unified theoretical framework has hindered their mutual enhancement, leading to suboptimal results.

To tackle this issue, in this paper, we propose a unified and coherent approach that considers representation learning, clustering, and anomaly detection by maximizing the likelihood of the observed data. Specifically, we denote the parameters of representation learning as \(\), the clustering parameter as \(\), and the dynamic indicator function for anomaly detection as \(()\). These parameters are optimized simultaneously by maximizing the likelihood of the observed data \(\):

\[ p(|,)=_{i=1}^{N}(_{i})  p(_{i}|,)=_{i=1}^{N}(_{i}) _{k=1}^{K}p(_{i},c_{i}=k|,),\] (1)

where \(c_{i}\) represents the latent cluster variable associated with \(_{i}\), and \(c_{i}=k\) denotes the probabilistic event that \(_{i}\) belongs to the \(k\)-th cluster. The \((_{i})\) is an indicator function that determines whether a sample \(_{i}\) is an anomaly of value 0 or a normal sample of value 1.

#### 3.1.1 Joint Representation Learning and Clustering with \(p(_{i}|,)\)

Based on the aforementioned advantages of MMs, we estimate the likelihood \(p(_{i}|,)\) with mixture models defined as:

\[ p(_{i}|,)=_{k=1}^{K}p( _{i},c_{i}=k|,)&=_{k=1}^{K}p(c_{i}=k)  p(_{i}|c_{i}=k,,_{k},_{k})\\ &=_{k=1}^{K}_{k} p(_{i}|c_{i}=k,, _{k},_{k}),\] (2)

where \(=\{_{k},_{k},_{k}\}\). The mixture model is parameterized by the prototypes \(_{k}\), covariance matrices \(_{k}\), and mixture weights \(_{k}\) from all clusters. \(_{k=1}^{K}_{k}=1\), and \(k=1,2,,K\).

In practice, the samples are usually attributed to high-dimensional features and it is challenging to detect anomalies from the raw feature space . Therefore, modern anomaly detection methods  often map raw data samples \(=\{_{i}\}^{N D}\) into a low-dimensional representation space \(=\{_{i}\}^{N d}\) with a representation learning function \(_{i}=f_{}(_{i})\) and detect anomalies within this latent representation space.

Following this widely adopted practice, we model the distribution of samples in the latent representation space with a multivariate Student's-\(t\) distribution giving its cluster \(c_{i}=k\). The Student's-\(t\) distribution is robust against outliers due to its heavy tails. Bayesian robustness theory leverages such distributions to dismiss outlier data, favoring reliable sources, making the Student's-\(t\) process preferable over Gaussian processes for data with atypical information . Thus the probability distribution of generating \(_{i}\) with latent representation \(_{i}\) given its cluster \(c_{i}=k\) can be expressed as:

\[p(_{i}|c_{i}=k,,_{k},_{k})=)|_{k}|^{-1/2}}{()} (1+D_{M}(_{i},_{k})^{2})^{- },\] (3)

where \(_{i}=f_{}(_{i})\) denotes the representation obtained from the data mapped through the neural network parameterized by \(\). \(\) denotes the gamma function while \(\) is the degree of freedom. \(_{k}\) is the scale parameter. \(D_{M}(_{i},_{k})=_{i}-_{k})^{T}_{k}^{-1}(_{i}-_{k})}\) represents the Mahalanobis distance . In the unsupervised setting, as cross-validing \(\) on a validation set or learning it is unnecessary, \(\) is set as 1 for all experiments . The overall marginal likelihood of the observed data \(_{i}\) can be simplified as:

\[p(_{i}|,)=_{k=1}^{K}_{k} |_{k}|^{-1/2}}{1+D_{M}(_{i},_{k})^{2}}.\] (4)

#### 3.1.2 Anomaly Indicator \((_{i})\) and Score \(o_{i}\)

As we have discussed, the indicator function \((_{i})\) not only benefits both representation and clustering but also directly serves as the output of anomaly detection. Ideally, with the percentage of outliers denoted as \(l\), an optimal solution for \((_{i})\) that maximizes the objective function \(J(,)\) entails setting all \((_{i})=0\) for \(_{i}\) among the \(l\) percent of outliers with lowest generation possibility \(p(_{i}|,)\), and otherwise \((_{i})=1\) is set for the remaining normal samples. Therefore, the indicator function is determined as:

\[(_{i})=0,&p(_{i}|,) l,\\ 1,&\] (5)

As this method involves sorting the samples based on the generation probability as being anomalous, the values of \(p(_{i}|,)\) can serve as a form of anomaly score, a classic approach within the mixture model framework . This suggests that the likelihood of a sample being anomalous is inversely related to its generative probability since a lower generative probability indicates a higher chance of the sample being an outlier. Thus the anomaly score of sample \(_{i}\) can be defined as:\[o_{i}=_{i}|,)}=^{K}_{k} |_{k}|^{-1/2}}{1+D_{M}(_{i},_{k} )^{2}}}.\] (6)

### Gravity-inspired Anomaly Scoring

In practical applications, it is proved that anomaly scores derived from generation probabilities often yield suboptimal performance . This observation prompts a reconsideration of _how to fully leverage the complex relationships among samples or even across multiple clusters for anomaly detection_. In this section, we first provide a brief introduction to the concept of Newton's Law of Universal Gravitation  and then demonstrate how the anomaly score is intriguingly similar to this cross-field principle. Finally, we discuss the advantages of introducing the vector sum operation into the anomaly score inspired by the analogy.

#### 3.2.1 Analog Anomaly Scoring and Force Analysis

To begin with, Newton's Law of Universal Gravitation  stands as a fundamental framework for describing the interactions among entities in the physical world. According to this law, every object in the universe experiences an attractive force from another object. In classical mechanics, force analysis involves calculating the vector sum of all forces acting on an object, known as the **resultant force**, which is crucial in determining an object's acceleration or change in motion:

\[}_{i,}=_{k=1}^{K}}_{ik},~{}with~{}}}_{ik}=m_{k}}{r_{ik}^{2}}}_{ik},\] (7)

where \(}_{ik}\) represents the \(k\)-th force acting on the object \(i\). This force is proportional to the product of their masses, (\(m_{i}\) and \(m_{k}\)), and inversely proportional to the square of the distance \(r_{ik}\) between them. \(G\) represents the gravitational constant, and \(}_{ij}\) is the unit direction vector.

Similarly, if denoting: \(}_{ik}=p(_{i},c_{i}=k|,)=_{k} |_{k}|^{-1/2}}{1+D_{M}(_{i},_{ k})^{2}}\), the score of Equation (6) bears analogies to the summation of the magnitudes of forces as:

\[o_{i}=^{K}}_{ik}},~{}with~{} }}_{ik}=_{i} _{k}}{_{ik}^{2}},\] (8)

where \(=^{-1}\), \(_{k}=_{k}|_{k}|^{-1/2}\), \(_{i}=1\), and \(_{ik}=(_{i},_{k})^{2}}\). Here, \(_{ik}\) is taken as the measure of distance within the representation space, modified slightly by an additional term for smoothness. The constant \(\) serves a role akin to the gravitational constant in this analogy, whereas \(_{k}\) resembles the concept of mass for the cluster. The notation \(_{i}\) suggests a standardization where the mass of each data point is considered uniform and not differentiated.

#### 3.2.2 Anomaly Scoring with Vector Sum

Comparing Equation (7) with Equation (8), what still differs is that, unlike a simple sum of the scalar value, the resultant force \(}_{i,}\) employs the vector sum and incorporates both the magnitude and direction \(}_{ik}\) of each force. This distinction is crucial because forces in different directions can neutralize each other with a large angle between them or enhance each other's effects with a small angle. Inspired by this difference, we consider modeling the relationship between samples and clusters as a vector, and aggregating them through vector summation. The vector-formed anomaly score \(o_{i}^{V}\) is defined as:

\[o_{i}^{V}=^{K}}_{ik} }_{ik}\|},\] (9)

where \(}_{ik}\) represents the unit direction vector in the representation space from the sample \(_{i}\) to the cluster prototype \(_{k}\), and \(\|\|\) represents the \(L_{2}\) norm.

### Iterative Optimization

Given the challenge posed by the interdependence of the parameters of the network \(\) and those of the mixture model \(\{_{k},_{k},_{k}\}\) in joint optimization, we propose an iterative optimization procedure. The pseudocode for training the model is presented in Algorithm 1 in the appendix.

#### 3.3.1 Update \(\)

To update the parameters of the mixture model \(=\{_{k},_{k},_{k}\}\), we use the Expectation-Maximization (EM) algorithm to maximize equation (1) . The detailed derivation is included in Appendix B.

**E-step.** During the E-step of iteration \((t+1)\), our goal is to compute the posterior probabilities of each data point belonging to the \(k\)-th cluster within the mixture model. Given the observed sample \(_{i}\) and the current estimates of the parameters \(^{(t)}\) and \(^{(t)}\), the expected value of the likelihood function of latent variable \(c_{k}\), or the posterior possibilities, can be expressed as:

\[_{ik}^{(t+1)}=p(c_{i}=k|_{i},,^{(t)})=_{i},c_{i}=k|,^{(t)})}{_{j=1}^{K}p(_{i},c_{i }=j|,^{(t)})}=}_{ik}^{(t)}}{_{j=1}^{K }}_{ij}^{(t)}}.\] (10)

The scale factor serving as an intermediate result for subsequent updates in the M-step is :

\[_{ik}^{(t+1)}=(_{i}^{(t)},_{k}^{( t)})}.\] (11)

**M-step.** In the M-step of iteration \((t+1)\), given the gradients \(}=0\), \(_{k}}=0\), and \(}=0\), we derive the analytical solutions for the mixture model parameters \(_{k}\), \(_{k}\), and \(_{k}\). Assume the anomalous ratio is \(l\), the number of the normal samples is \(n=(l*N)\). The updating process for \(\{_{k}^{(t+1)},_{k}^{(t+1)},_{k}^{(t+1)}\}\) is as follows:

* The mixture weights \(_{k}\) are updated by averaging the posterior probabilities over all data points with the number of samples, reflecting the relative presence of each component in the mixture: \[_{k}^{(t+1)}=_{i=1}^{n}_{ik}^{(t+1)}/n.\] (12)
* The prototypes \(_{k}\) are updated to be the weighted average of the data points, where weights are the posterior probabilities: \[_{k}^{(t+1)}=_{i=1}^{n}(_{ik}^{(t+1)}_{ ik}^{(t+1)}_{i})/_{i=1}^{n}(_{ik}^{(t+1)} _{ik}^{(t+1)}).\] (13)
* The covariance matrices \(_{k}\) are updated by considering the dispersion of the data around the newly computed prototypes: \[_{k}^{(t+1)}=^{n}_{ik}^{(t+1)}_{ ik}^{(t+1)}(_{i}-_{k}^{(t+1)})(_{i}-_{k}^{(t+1 )})}{_{j=1}^{K}_{ij}^{(t+1)}}.\] (14)

#### 3.3.2 Update \(\)

We focus on anomaly-aware representation learning and use stochastic gradient descent to optimize the network parameters \(\), by minimizing the following joint loss:

\[=-J(,)+g(),\] (15)

where \(J(,)= p(|,)\). An additional constraint term \(g()\) is introduced to prevent shortcut solution . In practice, an autoencoder architecture is implemented, utilizing a reconstruction loss \(g()=\|x-\|^{2}\) as the constraint.

These updates are iteratively performed until convergence, resulting in optimized model parameters that best fit the given data according to the mixture model framework.

Experiments

### Datasets & Baselines

We evaluated UniCAD on an extensive collection of datasets, comprising 30 tabular datasets that span 16 diverse fields. We specifically focused on naturally occurring anomaly patterns, rather than synthetically generated or injected anomalies, as this aligns more closely with real-world scenarios. The detailed descriptions are provided in Table 4 of Appendix D.1. Following the setup in ADBench , we adopt an inductive setting to predict newly emerging data, a highly beneficial approach for practical applications.

To assess the effectiveness of UniCAD, we compared it with 17 advanced unsupervised anomaly detection methods, including: (1) _traditional methods_: SOD  and HBOS ; (2) _linear methods_: PCA  and OCSVM ; (3) _density-based methods_: LOF  and KNN ; (4) _ensemble-based methods_: LODA  and Forest ; (5) _probability-based methods_: DAGMM , ECOD , and COPOD ; (6) _cluster-based methods_: DBSCAN , CBLOF , DCOD  and KMeans- ; and (7) _neural network-based methods_: DeepSVDD  and DIF . These baselines encompass the majority of the latest methods, providing a comprehensive overview of the state-of-the-art. For a detailed description, please refer to Appendix D.2.

### Experiment Settings

In the unsupervised setting, we employ the default hyperparameters from the original papers for all comparison methods. Similarly, the UniCAD also utilizes a fixed set of parameters to ensure a fair comparison. For all datasets, we employ a two-layer MLP with a hidden dimension of \(d=128\) and ReLU activation function as both encoder and decoder. We utilize the Adam optimizer  with a learning rate of \(1e^{-4}\) for 100 epochs. For the EM process, we set the maximum iteration number to 100 and a tolerance of \(1e^{-3}\) for stopping training when the objectives converge. The number of components in the mixture model is set as \(k=10\), and the proportion of the outlier is set as \(l=1\%\). We evaluate the methods using Area Under the Receiver Operating Characteristic (AUC-ROC) and Area Under the Precision-Recall Curve (AUC-PR) metrics , reporting the average ranking (Avg. Rank) across all datasets. All experiments are run 3 times with different seeds, and the mean results are reported.

### Performance and Analysis

**Performance Comparison**. Table 1 presents a comparison of UniCAD with 10 unsupervised baseline methods across 30 tabular datasets using the AUC-ROC metric. The experimental results, which encompass 17 baselines, are included in Tables 5 and 6 of Appendix D.3, with additional experiments on other data domains presented in Appendix E. Our proposed UniCAD achieves the top average ranking, exhibiting the best or near-best performance on a larger number of datasets and confirming advanced capabilities. It is noteworthy that there is no one-size-fits-all unsupervised anomaly detection method suitable for every type of dataset, as demonstrated by the observation that other methods have also achieved some of the best results on certain datasets. However, our model showcased a remarkable ability to generalize across most datasets featuring natural anomalies, as evidenced by statistical average ranking. As for clustering-based methods such as KMeans-, DCOD, and CBLOF, they mostly rank in the top tier among all baseline methods, supporting the advantage of combining deep clustering with anomaly detection. However, our method significantly outperformed these methods by mitigating their limitations and further providing a unified framework for joint representation learning, clustering, and anomaly detection.

**Effectiveness of Vector Sum in Anomaly Scoring**. As demonstrated in Table 1, we compare the anomaly score \(_{i}\) derived directly from the generation possibility with its vector summation form \(_{i}^{V}\). According to our statistical findings, we observe that vector scores \(_{i}^{V}\) consistently outperform scalar scores \(_{i}\). This indicates that the introduction of the vector summation, analogous to the concept of resultant force, makes a substantial difference in anomaly detection scenarios involving multiple clusters. The performance gains of the vector sum scores strongly demonstrate the effectiveness of the UniCAD in capturing the subtle differences in the distinctions among multiple clusters and underscore the utility of this factor in the context of anomaly detection based on clustering.

[MISSING_PAGE_FAIL:8]

to efficiently model complex patterns but also achieves an optimal balance between computational efficiency and modeling capability.

### Ablation Studies

In this section, we examine the contributions of different components in UniCAD. Tables 3 reports the results. We make three major observations. **Firstly**, the anomaly detection performance experiences a significant drop when replacing the Student's t distribution with a Gaussian distribution for the Mixture Model, highlighting the robustness of the Student's t distribution in unsupervised anomaly detection. **Secondly**, omitting the likelihood maximization loss (w/o \(J(,)\)) also results in a considerable decrease in overall performance. This observation underscores the importance of deriving both the optimization objectives and anomaly scores from the likelihood generation probability through a theoretical framework, which allows for unified joint optimization of anomaly detection and clustering in the representation space. **Furthermore**, the indicator function \((_{i})\) also contributes to a performance increase. These results further confirm the effectiveness of our UniCAD in mitigating the negative influence of anomalies in the clustering process, as the existence of outliers may significantly degrade the performance of clustering. In summary, all these ablation studies clearly demonstrate the effectiveness of our theoretical framework in simultaneously considering representation learning, clustering, and anomaly detection.

### Sensitivity of Hyperparameters

In this section, we conducted a sensitivity analysis on key hyperparameters of the model applied to the donors dataset, focusing on the number of clusters \(k\) and the proportion of the outlier set \(l\). The results of this analysis are illustrated in Figure 2. Notably, the optimal range for \(l\) tends to be lower than the actual proportion of anomalies in the dataset. Furthermore, a pattern was observed with the number of clusters \(k\), where the model performance initially improved with an increase in \(k\), followed by a subsequent decline. This suggests the existence of an optimal range for the number of clusters, which should be carefully selected based on the specific application context.

## 5 Conclusion

This paper presents UniCAD, a novel model for Unsupervised Anomaly Detection (UAD) that seamlessly integrates representation learning, clustering, and anomaly detection within a unified theoretical framework. Specifically, UniCAD introduces an anomaly-aware data likelihood based on the mixture model with the Student-t distribution to guide the joint optimization process, effectively mitigating the impact of anomalies on representation learning and clustering. This framework enables a theoretically grounded anomaly score inspired by universal gravitation, which considers complex relationships between samples and multiple clusters. Extensive experiments on 30 datasets across various domains demonstrate the effectiveness and generalization capability of UniCAD, surpassing 15 baseline methods and establishing it as a state-of-the-art solution in unsupervised anomaly detection. Despite its potential, the proposed method's applicability to broader fields like time series and multimodal anomaly detection requires further exploration and validation, highlighting a significant area for future work.

  
**Phase** & **IForest** & **KMeans-** & **DAGMM** & **DCOD** & **UniCAD** \\  Fit & 0.256 & 103.697 & 795.004 & 4548.634 & 246.113 \\ Infer & 0.0186 & 0.059 & 4.190 & 16.190 & 0.079 \\   

Table 2: Runtime Comparison. The runtime is reported in seconds (s).

  
**Metric** & **w/ Gauss.** & **w/o**\(J(,)\) & **w/o**\((_{i})\) & **Full Model** \\  Avg. Rank (w/ baselines \& variants) & 6.2 & 6.6 & 5.0 & **4.2** \\   

Table 3: Ablation study on AUC-ROC scores, calculated across 30 datasets.