# Compositional Policy Learning in Stochastic Control Systems with Formal Guarantees

Dorde Zikelic

Institute of Science and Technology Austria (ISTA)

Klosterneuburg, Austria

djordje.zikelic@ist.ac.at

&Mathias Lechner

Massachusetts Institute of Technology

Cambridge, MA, USA

mlechner@mit.edu

&Abhinav Verma

The Pennsylvania State University

University Park, PA, USA

verna@psu.edu

&Krishnendu Chatterjee

Institute of Science and Technology Austria (ISTA)

Klosterneuburg, Austria

krishnendu.chatterjee@ist.ac.at

&Thomas A. Henzinger

Institute of Science and Technology Austria (ISTA)

Klosterneuburg, Austria

tah@ist.ac.at

Equal contribution.

###### Abstract

Reinforcement learning has shown promising results in learning neural network policies for complicated control tasks. However, the lack of formal guarantees about the behavior of such policies remains an impediment to their deployment. We propose a novel method for learning a composition of neural network policies in stochastic environments, along with a formal certificate which guarantees that a specification over the policy's behavior is satisfied with the desired probability. Unlike prior work on verifiable RL, our approach leverages the compositional nature of logical specifications provided in SpectRL, to learn over graphs of probabilistic reach-avoid specifications. The formal guarantees are provided by learning neural network policies together with reach-avoid supermartingales (RASM) for the graph's sub-tasks and then composing them into a global policy. We also derive a tighter lower bound compared to previous work on the probability of reach-avoidance implied by a RASM, which is required to find a compositional policy with an acceptable probabilistic threshold for complex tasks with multiple edge policies. We implement a prototype of our approach and evaluate it on a Stochastic Nine Rooms environment.

## 1 Introduction

Reinforcement learning (RL) has achieved promising results in a variety of control tasks. However, the main objective of RL is to maximize expected reward  which does not provide guarantees of the system's safety. A more recent paradigm in safe RL considers constrained Markov decision processes (cMDPs) [5; 26; 3; 21], that in addition to a reward function are also equipped with a cost function. The goal in solving cMDPs is to maximize expected reward while keeping expected cost below some tolerable threshold. Recently, keeping cost below the threshold almost-surely and not only in expectation was considered in . While these methods do enhance safety, they only empiricallytry to minimize cost and do not provide guarantees on cost constraint satisfaction. The lack of formal guarantees raises a significant barrier to deployment of these methods in safety-critical applications, where the policies often need to encode complicated, long-horizon reasoning to accomplish a task in an environment in which unsafe behavior can lead to catastrophic consequences .

Recent work  has explored decomposing high-level logical specifications, belonging to different fragments of Linear Temporal Logic (LTL) , into simpler logical specification tasks that RL algorithms can solve more easily. In particular, these methods first decompose the high-level logical specification into a number of simpler logical specifications, then learn policies for these simpler tasks and finally compose the learned policies for each task into a global policy that solves the high-level control problem at hand. The simpler control tasks are solved by designing reward functions which faithfully encode the simpler logical specifications obtained by the decomposition and using an off-the-shelf RL algorithm to solve them.

While these methods present significant advances in deploying RL algorithms for solving complex logical specification tasks, their key limitation is that they also do not provide formal guarantees on the probability of satisfying the logical specification which are imperative for safety-critical applications. This is because reward functions often only "approximate" logical specification objectives, and furthermore RL algorithms are in general not guaranteed to converge to optimal policies over continuous state spaces. Recently,  have proposed verification procedures for formally verifying a composition of neural network policies with respect to a given system dynamics model. However, these works assume that the underlying control systems have _deterministic dynamics_. This does not allow modelling _stochastic disturbances and uncertainty_ in the underlying system dynamics. Furthermore,  assumes that the underlying control system has a _linear dynamics function_ whereas many control tasks have _non-linear dynamics_.

In this work we propose Claps (**C**ompositional **L**earning for **P**robabilistic **S**pecifications), a new compositional algorithm for solving high-level logical specification tasks with formal guarantees on the probability of the specification being satisfied. We present a method for learning and verifying a composition of neural network policies learned via RL algorithms, which is applicable to _stochastic control systems_ that may be defined via _non-linear dynamics functions_. Our method only requires that the underlying state space of the system is compact (i.e. closed and bounded) and that the dynamics function of the system is Lipschitz continuous. In terms of the expressiveness of logical specifications, Claps considers the SpectRL specification language . SpectRL contains all logical specifications that can be obtained by sequential and disjunctive compositions of reach-avoid specifications. Given a target and an unsafe region, a reach-avoid specification requires that the system reaches the target region while avoiding the unsafe region.

Our method learns a policy along with a formal certificate which guarantees that a specification is satisfied with the desired probability. It consists of three key ingredients - (1) high-level planning on a directed acyclic graph (DAG) that decomposes the complex logical specification task into sequentially or disjunctively composed low-level reach-avoid tasks, (2) learning policies that solve low-level reach-avoid tasks with formal guarantees on the probability of satisfying reach-avoidance, and (3) composing the low-level reach-avoid policies into a global policy for the high-level task by traversing edges in the DAG. This yields a fully _compositional_ algorithm which only requires solving control problems with reach-avoid constraints, while performing high-level planning.

To solve the low-level reach-avoid tasks, we leverage and build upon the recent learning algorithm for stochastic control under reach-avoid constraints . This work achieves formal guarantees on the probability of reach-avoidance by learning a policy together with a reach-avoid supermartingale (RASM), which serves as a formal certificate of probabilistic reach-avoidance. When composing such guarantees for a global probabilistic specification we encounter two fundamental challenges, (i) reducing the certification of unnecessary individual reach-avoid constraints that are not critical for global satisfaction, and (ii) reducing the violation probabilities for individual reach-avoid constraints to improve the possibility of satisfaction of the composed policy. We overcome the first challenge by leveraging the DAG decomposition of complex specifications and by performing a forward pass on the DAG which learns edge policies _on-demand_, and the second one by showing that RASMs prove a _strictly tighter_ lower bound on the probability of reach-avoidance than the bound that was originally proved in . Our novel bound multiplies the bound of  by an exponential asymptotic decay term. This is particularly important when composing a large number of edge policies for complex objectives. Furthermore, our new bound is of independent interest and it advances state of the art in stochastic control under reach-avoid constraints.

**Contributions** Our contributions can be summarized as follows:

1. We prove that RASMs imply a strictly tighter lower bound on the probability of reach-avoidance compared to the bound originally proved in  (Section 4).
2. We propose a novel method for learning and verifying a composition of neural network policies in _stochastic control systems_ learned in the RL paradigm, which provides _formal guarantees_ on the probability of satisfaction of SpectRL specifications (Section 5).
3. We implement a prototype of our approach and evaluate it on the Stochastic Nine Rooms environment, obtained by injecting stochastic disturbances to the environment of . Our experiments demonstrate both the necessity of decomposing high-level logical specifications into simpler control tasks as well as the ability of our algorithm to solve complex specification tasks with formal guarantees (Section 6).

## 2 Related Work

**Learning from logical specifications** Using high-level specifications with RL algorithms has been explored in [13; 32; 33; 39; 43; 60] among others. These approaches typically use the specification to guide the reward objective used by a RL algorithm to learn a policy but usually lack a formal guarantee on the satisfaction of the specification. Approaches that provide guarantees on the probability of satisfaction in finite state MDPs have been proposed in [12; 28; 29]. In contrast, we consider stochastic systems with continuous state spaces.

**Control with reachability and safety guarantees** Several works propose model-based methods for learning control policies that provide _formal guarantees_ with respect to reachability, safety and reach-avoid specifications. The methods of [2; 16; 17; 48; 55; 1] consider deterministic systems and learn a policy together with a Lyapunov or barrier function that guarantees reachability or safety. Control of finite-time horizon stochastic systems with reach-avoid guarantees has been considered in [14; 36; 53; 61]. These methods compute a finite-state MDP abstraction of the system and solve the constrained control problem for the MDP. The works of [9; 10] consider infinite-time horizon stochastic systems with linear dynamics and propose abstraction-based methods that provide probably approximately correct (PAC) guarantees. Control of polynomial stochastic systems via stochastic barrier functions and convex optimization was considered in [46; 49; 65; 42]. Learning-based methods providing formal guarantees for infinite time horizon stochastic systems that are not necessarily polynomial have been proposed in [38; 68; 41; 20]. The work  proposes a learning-based method for providing formal guarantees on probability \(1\) stabilization. To the best of our knowledge, the compositionality of our Claps makes it the first method that provides formal guarantees for infinite time horizon stochastic systems with respect to more expressive specifications.

**Safe exploration** RL algorithms fundamentally depend on exploration in order to learn high performing actions. A common approach to safe exploration RL is to limit exploration within a high probability safety region which is computed by estimating uncertainty bounds. Existing works achieve this by using Gaussian Processes [11; 34; 59], linearized models , robust regression , Bayesian neural networks , shielding [4; 7; 25; 31] and state augmentation .

**Model-free methods for stochastic systems** The recent work of  proposed a model-free approach to learning a policy together with a stochastic barrier function, towards enhancing probabilistic safety of the learned policy. Unlike our approach, this method does not formally verify the correctness of the stochastic barrier function hence does not provide any guarantees on safety probability. However, it does not assume the knowledge of the system environment and is applicable in a model-free fashion. Several model-free methods for learning policies and certificate functions in deterministic systems have also been considered, see  for a survey.

**Statistical methods** Statistical methods  provide an effective approach to estimating the probability of a specification being satisfied, when satisfaction and violation of the specification can be witnessed via finite execution traces. Consequently, such methods often assume a finite-horizon or the existence of a finite threshold such that the infinite-horizon behavior is captured by traces within that threshold. In environments where these assumptions do not hold, the violation of a reach-avoid (or, more generally, any SpectRL ) specification can be witnessed only via infinite traces and hence statistical methods are usually not applicable. In this work, we consider _formal verification_ of _infinite-time horizon systems_.

**Supermartingales for probabilistic programs** Supermartingales have also been used for the analysis of probabilistic programs (PPs) for properties such as termination , reachability  and safety [18; 19].

## 3 Preliminaries

We consider a discrete-time stochastic dynamical system whose dynamics are defined by the equation

\[_{t+1}=f(_{t},(_{t}),_{t}),\]

where \(t_{0}\) is a time step, \(_{t}\) is a state of the system, \(_{t}=(_{t})\) is a control action and \(_{t}\) is a stochastic disturbance vector at the time step \(t\). Here, \(^{n}\) is the state space of the system, \(^{m}\) is the action space and \(^{p}\) is the stochastic disturbance space. The system dynamics are defined by the dynamics function \(f:\), the control policy \(:\) and a probability distribution \(d\) over \(\) from which a stochastic disturbance vector is sampled independently at each time step. Together, these define a _stochastic feedback loop system_.

A sequence \((_{t},_{t},_{t})_{t=0}^{}\) of state-action-disturbance triples is a _trajectory_ of the system, if we have that \(_{t}=(_{t})\), \(_{t}(d)\) and \(_{t+1}=f(_{t},(_{t}),_{t})\) hold for each time step \(t_{0}\). For every state \(_{0}\), we use \(_{_{0}}\) to denote the set of all trajectories that start in the initial state \(_{0}\). The Markov decision process (MDP) semantics of the system define a probability space over the set of all trajectories in \(_{_{0}}\) under any fixed policy \(\) of the system . We use \(_{_{0}}\) and \(_{_{0}}\) to denote the probability measure and the expectation operator in this probability space.

**Assumptions** For system semantics to be mathematically well-defined, we assume that \(^{n}\), \(^{m}\) and \(^{p}\) are all Borel-measurable and that \(f:\) and \(:\) are continuous functions. Furthermore, we assume that \(^{n}\) is compact (i.e. closed and bounded) and that \(f\) and \(\) are continuous functions. These are very general and standard assumptions in control theory. Since any continuous function on a compact domain is Lipschitz continuous, our assumptions also imply that \(f\) and \(\) are Lipschitz continuous.

**Probabilistic specifications** Let \(\) denote the set of all trajectories of the system. A _specification_ is a boolean function \(:\{\}\) which for each trajectory specifies whether it satisfies the specification. We write \(\) whenever a trajectory \(\) satisfies the specification \(\). A _probabilistic specification_ is then defined as an ordered pair \((,p)\) of a specification \(\) and a probability parameter \(p\) with which the specification needs to be satisfied. We say that the system satisfies the probabilistic specification \((,p)\) at the initial state \(_{0}\) if the probability of any trajectory in \(_{_{0}}\) satisfying \(\) is at least \(p\), i.e. if \(_{_{0}}[_{_{0}}] p\).

**SpectRL and abstract graphs** Reach-avoid specifications are one of the most common and practically relevant specifications appearing in safety-critical applications that generalize both reachability and safety specifications . Given a target region and an unsafe region, a reach-avoid specifications requires that a system controlled by a policy reaches the target region while avoiding the unsafe region. The SpectRL specification language  allows specifications of reachability and avoidance objectives as well as their sequential or disjunctive composition. See Appendix A for the formal definition of the SpectRL syntax and semantics.2

It was shown in  that a SpectRL specification can be translated into an abstract graph over reach-avoid specifications. Intuitively, an _abstract graph_ is a directed acyclic graph (DAG) whose vertices represent regions of system states and whose edges are annotated with a safety specification. Hence, each edge can be associated with a _reach-avoid specification_ where the goal is to drive the system from the region corresponding to the source vertex of the edge to the region corresponding to the target vertex of the edge while satisfying the annotated safety specification.

**Definition 1** (Abstract graph).: _An abstract graph \(G=(V,E,,s,t)\) is a DAG, where \(V\) is the vertex set, \(E\) is the edge set, \(:V E()\) maps each vertex and each edge to a subgoal region in \(\), \(s V\) is the source vertex and \(t V\) is the target vertex. Furthermore, we require that \((s)=_{0}\)._Given a trajectory \(=(_{t},_{t},_{t})_{ 0}^{}\) of the system and an abstract graph \(G=(V,E,,s,t)\), we say that \(\) satisfies _abstract reachability_ for \(G\) (written \( G\)) if it gives rise to a path in \(G\) that traverses \(G\) from \(s\) to \(t\) and satisfies reach-avoid specifications of all traversed edges. We formalize this notion in Appendix B. It was shown in [33, Theorem 3.4] that for each SpectRL specification \(\) once can construct an abstract graph \(G\) such that for each trajectory \(\) we have \(\) iff \( G\). For completeness, we provide this construction in Appendix B.

**Problem Statement** We now formally define the problem that we consider in this work. Consider a stochastic feedback loop system defined as above and let \(_{0}\) be the set of initial states. Let \((,p)\) be a probabilistic specification with \(\) being a specification formula in SpectRL and \(p\) being a probability threshold. Then, our goal is to learn a policy \(\) such that the stochastic feedback loop system controlled by the policy \(\)_guarantees_ satisfaction of the probabilistic specification \((,p)\) at every initial state \(_{0}_{0}\), i.e. that for every \(_{0}_{0}\) we have \(_{_{0}}[_{_{0}} ] p\).

**Compositional learning algorithm** Our goal is not only to learn policy that guarantees satisfaction of the probabilistic specification, but also to learn such a policy in a compositional manner. Given a SpectRL specification \(\), a probability threshold \(p\) and an abstract graph \(G\) of \(\), we say that a policy \(\) is a _compositional policy_ for the probabilistic specification \((,p)\) if it guarantees satisfaction of \((,p)\) and is obtained by composing a number of _edge policies_ learned for reach-avoid tasks associated to edges of \(G\). An algorithm is said to be _compositional_ for a probabilistic specification \((,p)\) if it learns a compositional policy for the probabilistic specification \((,p)\). In this work, we present a compositional algorithm for the given probabilistic specification \((,p)\).

## 4 Improved Bound for Probabilistic Reach-avoidance

In this section, we define reach-avoid supermartingales (RASMs) and derive an improved lower bound on the probability of reach-avoidance that RASMs can be used to formally certify. RASMs, together with a lower bound on the probability of reach-avoidance that they guarantee and a method for learning a control policy and an RASM, were originally proposed in . The novelty in this section is that we prove that RASMs imply a _strictly stronger_ bound on the probability of satisfying reach-avoidance, compared to the bound derived in . We achieve this by proposing an alternative formulation of RASMs, which we call _multiplicative RASMs_. In contrast to the original definition of RASMs which imposes _additive_ expected decrease condition, our formulation of RASMs imposes _multiplicative_ expected decrease condition. In what follows, we define multiplicative RASMs. Then, we first show in Theorem 1 that they are equivalent to RASMs. Second, we show in Theorem 2 that by analyzing the multiplicative expected decrease, we can derive an _exponentially tighter_ bound on the probability of reach-avoidance. These two results imply that we can utilize the procedure of  to learn policies together with multiplicative RASMs that provide strictly better formal guarantees on the probability reach-avoidance.

**Prior work - reach-avoid supermartingales** RASMs  are continuous functions that assign real values to system states, are required to be nonnegative and to satisfy the _additive_ expected decrease condition, i.e. to strictly decrease in expected value by some additive term \(>0\) upon every one-step evolution of the system dynamics until either the target set or the unsafe set is reached. Furthermore, the initial value of the RASM is at most \(1\) while the value of the RASM needs to exceed some \(>1\) in order for the system to reach the unsafe set. Thus, an RASM can intuitively be viewed as an invariant of the system which shows that the system has a tendency to _converge_ either to the target or the unsafe set while also being _repulsed away_ from the unsafe set, where this tendency is formalized by the expected decrease condition. To emphasize the additive expected decrease, we refer to RASMs of  as additive RASMs.

**Definition 2** (Additive reach-avoid supermartingales ).: _Let \(>0\) and \(>1\). A continuous function \(V:\) is an \((,)\)-additive reach-avoid supermartingale (\((,)\)-additive RASM) with respect to \(_{t}\) and \(_{u}\), if:_

1. Nonnegativity condition. \(V() 0\) _for each_ \(\)_._
2. Initial condition. \(V() 1\) _for each_ \(_{0}\)_._
3. Safety condition. \(V()\) _for each_ \(_{u}\)_._
4. Additive expected decrease condition. _For each_ \(_{t}\) _at which_ \(V()\)_, we have_ \(V()_{ d}[V(f(,(), ))]+\)_._

**Multiplicative RASMs and equivalence** In this work we introduce multiplicative RASMs which need to decrease in expected value by at least some _multiplicative_ factor \((0,1)\). We also impose the _strict positivity_ condition outside of the target set \(_{t}\). Note that any \((,)\)-additive RASM satisfies this condition with the lower bound \(>0\).

**Definition 3** (Multiplicative reach-avoid supermartingales).: _Let \((0,1)\), \(>0\) and \(>1\). A continuous function \(V:\) is a \((,,)\)-multiplicative reach-avoid supermartingale (\((,,)\)-multiplicative RASM) with respect to \(_{t}\) and \(_{u}\), if:_

1. Nonnegativity condition.__\(V() 0\) _for each_ \(\)_._
2. Strict positivity outside_ \(_{t}\) _condition.__\(V()\) _for each_ \(_{t}\)_._
3. Initial condition.__\(V() 1\) _for each_ \(_{0}\)_._
4. Safety condition.__\(V()\) _for each_ \(_{u}\)_._
5. Multiplicative expected decrease condition. _For each_ \(_{t}\) _at which_ \(V()\)_, we have_ \( V()_{ d}[V(f(,( ),))]\)_._

We show that a function \(V:\) is an additive RASM if and only if it is a multiplicative RASM.

**Theorem 1**.: _[Proof in Appendix C] The following two statements hold:_

1. _If a continuous function_ \(V:\) _is an_ \((,)\)_-additive RASM, then it is also a_ \((,\{,\},)\)_-multiplicative RASM._
2. _If a continuous function_ \(V:\) _is a_ \((,,)\)_-multiplicative RASM, then it is also an_ \(((1-),)\)_-additive RASM._

**Improved bound** The following theorem shows that the existence of a multiplicative RASM implies a lower bound on the probability with which the system satisfies the reach-avoid specification.

**Theorem 2**.: _[Proof in Appendix D] Let \((0,1)\), \(>0\) and \(>1\), and suppose that \(V:\) is a \((,,)\)-multiplicative RASM with respect to \(_{t}\) and \(_{u}\). Suppose furthermore that \(V\) is Lipschitz continuous with a Lipschitz constant \(L_{V}\), and that the system under policy \(\) satisfies the bounded step property, i.e. that there exists \(>0\) such that \(||-f(x,(),)||_{1}\) holds for each \(\) and \(\). Let \(N=(-1)/(L_{V})\). Then, for every \(_{0}_{0}\), we have that_

\[_{_{0}}(_{t},_{u}) 1-^{N}.\]

Here, \(N=(-1)/(L_{V})\) is the smallest number of time steps in which the system could hypothetically reach the unsafe set \(_{u}\). This is because, for the system to violate safety, by the Initial and the Safety conditions of multiplicative RASMs the value of \(V\) must increase from at most \(1\) to at least \(\). But the value of \(V\) can increase by at most \(L_{V}\) in any single time step since \(\) is the maximal step size of the system and \(L_{V}\) is the Lipschitz constant of \(V\). Hence, the system cannot reach \(_{u}\) in less than \(N=(-1)/(L_{V})\) time steps.

We conclude this section by comparing our bound to the bound \(_{_{0}}[(_{t},_{u}) ] 1-\) of  for additive RASMs. Our bound on the probability of violating reach-avoidance is tighter by a factor of \(^{N}\). Notice that this term decays exponentially as \(\) increases, hence our bound is _exponentially tighter_ in \(\). This is particularly relevant if we want to verify reach-avoidance with high probability that is close to \(1\), as it allows using a much smaller value of \(\) and significantly relaxing the Safety condition in Definition 3. To evaluate the quality of the bound improvement on an example, in Table 1 we consider the 9-Rooms environment and compare the bounds for multiple values of \(\) when \(=0.99\). Note that \(\) is the property of the system and not the RASM.Thus, the value of \(\) is the key factor for controlling the quality of the bound. Results in Table 1 show that, in order to verify probability 99.9% reach-avoidance for an edge policy in the 9-Room example, our new bound allows using \( 100\) whereas with the old bound the algorithm needs to use \( 1000\).

  \(\) & \(10\) & \(100\) & \(1000\) \\   Claps bound (ours) & 0.91820 & 0.99866 & 0.99999 \\   & 0.9 & 0.99 & 0.999 \\  

Table 1: Comparison of the bound in Theorem 2 and the bound in  for several values of \(\) when \(=0.99\). We set \(=0.1\) which bounds the step size in the 9-Rooms environment, and \(L_{V}=5\) which is an upper bound on the Lipschitz constant that we observe in experiments. Thus, the bound in Theorem 2 is \(1-^{N}=1- 0.99^{2}\) and in  is \(1-\).

**Remark 1** (Comparison of RASMs and stochastic barrier functions).: _Stochastic barrier functions (SBFs) [45; 46] were introduced for proving probabilistic safety in stochastic dynamical systems, i.e. without the additional reachability condition as in-avoid specifications. If one is only interested in probabilistic safety, RASMs of  reduce to SBFs by letting \(_{t}=\) and \(=0\) in Definition 2 for additive RASMs, and \(_{t}=\) and \(=1\) in Definition 3 for multiplicative RASMs. The bound on the probability of reach-avoidance \(_{_{0}}[(,_{u})] 1 -\) of  for additive RASMs coincides with the bound on the probability of safety implied by SBFs of [45; 46]. Hence, our novel bound in Theorem 2 also provides tighter lower bound on the safety probability guarantees via SBFs._

_Exponential stochastic barrier functions (exponential SBFs) have been considered in  in order to provide tighter bounds on safety probability guarantees via SBFs for finite time horizon systems, in which the length of the time horizon is fixed and known a priori. Exponential SBFs also consider a multiplicative expected decrease condition, similar to our multiplicative RASMs and provide lower bounds on safety probability which are tighter by a factor which is exponential in the length \(N\) of the time horizon [49, Theorem 2]. However, as the length of the time horizon \(N\), their bound reduces to the bound of [45; 46]. In contrast, Theorem 2 shows that our multiplicative RASMs provide a tighter lower bound on safety (or more generally, reach-avoid) probability even in unbounded (i.e. indefinite) or infinite time horizon systems._

## 5 Compositional Learning for Probabilistic Specifications

We now present the Claps algorithm for learning a control policy that guarantees satisfaction of a probabilistic specification \((,p)\), where \(\) is a SpectRL formula. The idea behind our algorithm is as follows. The algorithm first translates \(\) into an abstract graph \(G=(V,E,,s,t)\) using the translation discussed in Section 3, in order to decompose the problem into a series of reach-avoid tasks associated to abstract graph edges. The algorithm then solves these reach-avoid tasks by learning policies for abstract graph edges. Each edge policy is learned together with a (multiplicative) RASM which provides formal guarantees on the probability of satisfaction of the reach-avoid specification proved in Theorem 2. Finally, the algorithm combines these edge policies in order to obtain a global policy that guarantees satisfaction of \((,p)\). The algorithm pseudocode is presented in Algorithm 1. We assume the Policy+RASM subprocedure which, given a reach-avoid task and given a probability parameter \(p^{}\), learns a control policy together with a RASM that proves that the policy guarantees reach-avoidance with probability at least \(p^{}\). Due to the equivalence of additive and multiplicative RASMs that we proved in Theorem 1, for this we can use the procedure of  as an off-the-shelf method. For completeness of our presentation, we also provide details behind the Policy+RASM subprocedure in Appendix E.

**Challenges** There are two important challenges in designing an algorithm based on the above idea. First, it is not immediately clear how probabilistic reach-avoid guarantees provided by edge policies may be used to deduce the probability with which the global specification \(\) is satisfied. Second, since SpectRL formulas allow for disjunctive specifications, it might not be necessary to solve reach-avoid tasks associated to each abstract graph edge and naively doing so might lead to inefficiency. Our algorithm solves both of these challenges by first _topologically ordering_ the vertices of the abstract graph and then performing a _forward pass_ in which vertices are processed according to the topological ordering. The forward pass is executed in a way which allows our algorithm to keep track of the cumulative probability with which edge policies that have already been learned might violate the global specification \(\), therefore also allowing the algorithm to learn edge policies _on-demand_ and to identify abstract graph edges for which solving reach-avoid tasks would be redundant.

**Forward pass on the abstract graph** Since an abstract graph is a directed acyclic graph (DAG), we perform a topological sort on \(G\) in order to produce an ordering \(s=v_{1},v_{2},,v_{|V|}=t\) of its vertices (line 4 in Algorithm 1). This ordering satisfies the property that each edge \((v_{i},v_{j})\) in \(G\) must satisfy \(i<j\), i.e. any each edge must be a "forward edge" with respect to topological ordering. Algorithm 1 now initializes an empty dictionary \(\) and sets \([s]=1\) for the source vertex \(s\) (line 5) and performs a forward pass to process the remaining vertices in the abstract graph according to the topological ordering (lines 6-13). The purpose of the dictionary \(\) is to store lower bounds on the probability with which previously processed abstract graph vertices are reached by following the computed edge policies while satisfying reach-avoid specifications of the traversed abstract graph edges. For each newly processed vertex \(v_{i}\), Algorithm 1 learns policies for the reach-avoid tasks associated to all abstract graph edges that are incoming to \(v_{i}\). It then combines probabilities withwhich the learned edge policies satisfy associated reach-avoid tasks with the probabilities that have been stored for the previously processed vertices in order to compute the lower bound \([v_{i}]\).

More concretely, for each \(2 i|V|\), Algorithm 1 first stores \([v_{i}]=0\) as a trivial lower bound (line 7). It then computes the set \(N(v_{i})\) of all predecessors of \(v_{i}\) for which previously learned edge policies ensure reachability with probability at least \(p\) while satisfying reach-avoid specification of all traversed abstract graph edged (line 8). Recall, \(p\) is the global probability threshold with which the agent needs to satisfy the SpectRL specification \(\), and since Algorithm 1 processes vertices according to the topological ordering all vertices in \(N(v_{i})\) have already been processed. If \(N(v_{i})\) is empty, then the algorithm concludes that \(v_{i}\) cannot be reached in the abstract graph while satisfying reach-avoid specifications of traversed edges with probability at least \(p\). In this case, the algorithm does not try to learn edge policies for the edges with the target vertex \(v_{i}\) and proceeds to processing the next vertex in the topological ordering. Thus, Algorithm 1 ensures that edge policies are learned _on-demand_. If \(N(v_{i})\) is not empty, then for each \(v N(v_{i})\) the algorithm uses Policy+RASM together with binary search to find the maximal probability \(p_{(v,v_{i})}\) for which Policy+RASM manages to learn a policy \(_{(v,v_{i})}\) that solves the reach-avoid task with probability at least \(p_{(v,v_{i})}\) (line 10). We run the binary search only up to a pre-specified precision, which is a hyperparameter of our algorithm. Since the reach-avoid specification of the edge \((v,v_{i})\) is satisfied with probability at least \(p_{(v,v_{i})}\) and since \(v\) is reached in the abstract graph while satisfying reach-avoid specification of all traversed edges with probability at least \([v]\), Algorithm 1 updates its current lower bound \([v_{i}]\) for \(v_{i}\) to \(p_{(v,v_{i})}[v]\) whenever \(p_{(v,v_{i})}[v]\) exceeds the stored bound \([v_{i}]\) (line 11). This procedure is repeated for each \(v N(v_{i})\).

**Composing edge policies into a global policy** Once all abstract graph vertices have been processed, Algorithm 1 checks if \([t] p\) (line 14). If so, it composes the learned edge policies into a global policy \(\) that guarantees satisfaction of the probabilistic specification \((,p)\) (line 15) and returns the policy \(\) (line 16). The composition is performed as follows. First, note that for each vertex \(v s\) in the abstract graph for which \([v]>0\), by lines 7-11 in Algorithm 1 there must exist a vertex \(v^{} N(v)\) such that the algorithm has successfully learned a policy \(_{(v^{},v)}\) for the edge \((v^{},v)\) and such that \([v]=p_{(v^{},v)}[v^{}]\). Hence, repeatedly picking such predecessors yields a finite path \(s=v_{i_{0}},v_{i_{1}},,v_{i_{k}}=t\) in the abstract graph such that, for each \(1 j k\), the algorithm has successfully learned an edge policy \(_{(v_{i_{j-1}},v_{i_{j}})}\) and such that \([v_{i_{j}}]=p_{(v_{i_{j-1}},v_{i_{j}})}[v_{i_{j -1}}]\) holds. The global policy \(\) starts by following the edge policy \(_{(v_{i_{0}},v_{i_{1}})}\) until the target region \((v_{i_{1}})\) is reached or until the safety constraint \((v_{i_{0}},v_{i_{1}})\) associated to the edge is violated. Then,it follows the edge policy \(_{(v_{i_{1}},v_{i_{2}})}\) until the target region \((v_{i_{2}})\) is reached or until the safety constraint \((v_{i_{1}},v_{i_{1}2})\) associated to the edge is violated. This is repeated for each edge along the path \(s=v_{i_{0}},v_{i_{1}},,v_{i_{k}}=t\) until the system reaches a state within the target region \((t)\). If at any point the safety constraint is violated prior to reaching the target region associated to the edge, policy \(\) follows the current edge policy indefinitely and no longer updates it as the specification \(\) has been violated. As we prove in Theorem 3, violation of the safety constraint associated to some edge or the system never reaching the target region associated to some edge happen with probability at most \(1-p\). If \([t]<p\), then Algorithm 1 returns that it could not learn a policy that guarantees satisfaction of \((,p)\). Theorem 3 establishes the correctness of Algorithm 1 and its proof shows that the global policy \(\) obtained as above indeed satisfies the probabilistic specification \((,p)\).

**Theorem 3**.: _[Proof in Appendix F] Algorithm 1 is compositional, and if it outputs a policy \(\), then \(\) guarantees the probabilistic specification \((,p)\)._

## 6 Experiments

We implement a prototype of Algorithm 1 to validate its effectiveness 3. Motivated by the experiments of , we define an environment in which an agent must navigate safely between different rooms. However, different from  our Stochastic Nine Rooms environment perturbs the agent by random noise in each step. Concretely, the system is governed by the dynamics function, \(x_{t+1}=x_{t}+0.1(((a_{t},-1),1)+_{t}\), where \(_{t}\) is drawn from a triangular noise distribution. The state space of the environment is defined as \(^{2}\). The set of the initial states is \([0.4,0.6][0.4,0.6]\), whereas the target states are defined as \([2.4,2.6][2.4,2.6]\). The state space contains unsafe areas that should be avoided, i.e., the red _walls_ shown in Figure 1. Specifically, these unsafe areas cannot be entered by the agent, and coming close to them during the RL training incurs a penalty. Note that this defines a reach-avoid task where the goal is to reach the target region from the initial region while avoiding the unsafe regions defined by red walls.

We first run the Policy+RASM procedure (i.e. the method of ) on this task directly and observe that it is not able to learn a policy with any formal guarantees on probo

Figure 1: Stochastic Nine Rooms environment for which an end-to-end learned policy cannot be verified safe while our algorithm is able to decompose the task into three verifiable subtasks.

Figure 2: Decomposition of the Stochastic Nine Rooms task into subtasks. Left: Individual subtasks shown as graph edges. Center: Successfully verifiable subtasks are shown in blue edges, whereas the verification procedure of the gray edges failed. Right: Verified path for solving the complete task.

We then translate this task into a SpectRL task which decomposes it into a set of simpler reach-avoid tasks by constructing an abstract graph as follows. The vertex set of the abstract graph consists of centers of each of the \(9\)_rooms_, and the target regions associated to each room center are \([0.4+x,0.6+x][0.4+y,0.6+y]\) for \(x,y\{0,1,2\}\). Edges of this abstract graph are shown in Figure 2 left. To each edge, we associate an unsafe region to be avoided by taking red walls incident to rooms in which the edge is contained.

For each reach-avoid edge task, we run the proximal policy optimization (PPO)  reinforcement learning algorithm to initialize policy parameters. We then run the Policy+RASM procedure to learn a policy and a RASM which proves probabilistic reach-avoidance. We set the timeout to 4 hours for each reach-avoid subtask. In Figure 1 b), we visualize the PPO policy that was trained to reach the target states directly from the initial states. As shown, the policy occasionally hits a wall and cannot be verified, i.e., Policy+RASM procedure times out. In Figure 2 center, we visualize all successfully verified subtasks in blue. The reach-avoid lower bounds for each of the subtasks are listed in Table 2. Note that the simplest decomposition passing over the bottom right room could not be verified, which shows that the systematic decomposition used in our algorithm has advantages over manual task decompositions. Finally, in Figure 2 right, we highlight the path from the initial states to the target states via safe intermediate sets. In Figure 1 c), we visualize the trajectories of these three compositional policies. As shown, the trajectories never reach an unsafe area.

## 7 Concluding Remarks

We propose Claps, a novel method for learning and verifying a composition of neural network policies for stochastic control systems. Our method considers control tasks under specifications in the SpectRL lanugange, decomposes the task into an abstract graph of reach-avoid tasks and uses reach-avoid supermartingales to provide formal guarantees on the probability of reach-avoidance in each subtask. We also prove that RASMs imply a strictly tighter lower bound on the probability of reach-avoidance compared to prior work. These results provide confidence in the ability of Claps to find meaningful guarantees for global compositional policies, as shown by experimental evaluation in the Stochastic Nine Rooms environment. While our approach has significant conceptual, theoretical and algorithmic novelty, it is only applicable to SpectRL logical specifications and not to the whole of LTL. Furthermore, our algorithm is not guaranteed to return a policy and does not guarantee tight bounds on the probability of satisfying the logical specification. Overcoming these limitations will provide further confidence in deploying RL based solutions for varied applications.