# Shielding Regular Safety Properties

in Reinforcement Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

To deploy reinforcement learning (RL) systems in real-world scenarios we need to consider requirements such as safety and constraint compliance, rather than blindly maximizing for reward. In this paper we study RL with regular safety properties. We present a constrained problem based on the satisfaction of regular safety properties with high probability and we compare our setup to the some common constrained Markov decision processes (CMDP) settings. We also present a meta-algorithm with provable safety-guarantees, that can be used to shield the agent from violating the regular safety property during training and deployment. We demonstrate the effectiveness and scalability of our framework by evaluating our meta-algorithm in both the tabular and deep RL setting.

## 1 Introduction

The field of safe reinforcement learning (RL) [6; 28] has gained increasing interest, as practitioners begin to understand the challenges of applying RL in the real world . There exist several distinct paradigms in the literature, including constrained optimization [2; 20; 49; 58; 62; 74], logical constraint satisfaction [17; 24; 36; 37; 38; 66], safety-critical control [15; 19; 53], all of which are unified by prioritizing safety- and risk-awareness during the decision making process.

Constrained Markov decision processes (CMDP)  have emerged as a popular framework for modelling safe RL, or RL with constraints. Typically, the goal is to obtain a policy that maximizes reward while simultaneously ensuring that the expected cumulative cost remains below a pre-defined threshold. A key limitation of this setting is that constraint violations are enforced in expectation rather than with high probability, the constraint thresholds also have limited semantic meaning, can be very challenging to tune and in some cases inappropriate for highly safety-critical scenarios . Furthermore, the cost function in the CMDP is typically Markovian and thus fails to capture a significantly expressive class of safety properties and constraints.

Regular safety properties  are interesting because for all but the simplest properties the corresponding cost function is non-Markovian. Our problem setup consists of the standard RL objective with regular safety properties as constraints, we note that there has been a significant body of work that combines temporal logic constraints with RL [17; 24; 36; 37; 38; 66], although many of these do not explicitly separate reward and safety in the same way that we do.

Our approach relies on shielding , which is a safe exploration strategy that ensures the satisfaction of temporal logic constraints by deploying the learned policy in conjunction with a reactive system

Figure 1: Diagrammatic representation of runtime verification and shielding.

that overrides any _unsafe_ actions. Most shielding approaches typically make highly restrictive assumptions, such as full knowledge of the environment dynamics , or access to a simulator , although there has been recent work to deal with these restrictions [30; 39; 73]. In this paper, we opt for the most permissive setting, where the dynamics of the environment are unknown, runtime verification of the agent is realized by finite horizon model checking with a learned approximation of the environment dynamics. However, in principle our framework is flexible enough to accommodate more standard model checking procedures as long as certain assumptions are met.

Our approach can be summarised as an online shielding approach (see Fig. 1), that dynamically identifies unsafe actions during training and deployment, and deploys a safe 'backup policy' when necessary. We summarise the main contributions of our paper as follows:

(1) We state a constrained RL problem based on the satisfaction of regular safety properties with high probability, and we identify the conditions whereby our setup generalizes several CMDP settings, including _expected_ and _probabilistic cumulative cost_ constraints.

(2) We present several model checking algorithms that can verify the finite-horizon satisfaction probability of regular safety properties, this includes statistical model checking procedures that can be used if either the transition probabilities are unavailable or if the state space is too large.

(3) We develop a set of sample complexity results for the statistical model checking procedures introduced in point (2), which are then used to develop a shielding meta-algorithm with provable safety guarantees, even in the most permissive setting (i.e., no access to the transition probabilities).

(4) We empirically demonstrate the effectiveness of our framework on a variety of regular safety properties in both a tabular and deep RL settings.

## 2 Related Work

**Safety Paradigms in Reinforcement Learning.** There exist many safety paradigms in RL, the most popular being constrained MDPs. For CMDPs several constrained optimization algorithms have been developed, most are gradient-based methods built upon Lagrange relaxations of the constrained problem [20; 49; 58; 62] or projection-based local policy search [2; 74]. Model-based approaches to CMDP [7; 11; 41; 64] have also gathered recent interest as they enjoy better sample complexity than their model-free counterparts, which can be imperative for safe learning .

Linear Temporal Logic (LTL) constraints [17; 24; 36; 37; 38; 66] for RL have been developed as an alternative to CMDPs to specify stricter and more expressive constraints. The LTL formula is typically treated as the entire task specification, although some works have aimed to separate LTL satisfaction and reward into two distinct objectives . The typical procedure in this setting is to identify end components of the MDP that satisfy the LTL constraint and construct a corresponding reward function such that the optimal policy satisfies the LTL constraint with maximal probability. Formal PAC-style guarantees have been developed for this setting [27; 36; 66; 71] although they typically rely on non-trivial assumptions. We note that LTL constraints can capture regular safety properties, although we explicitly separate reward and safety, making the work in this paper distinct from previous work.

More rigorous safety-guarantees can be obtained by using _safety filters_, _control barrier functions_ (CBF) , and _model predictive safety certification_ (MPSC) [67; 68]. To achieve zero-violation training these methods typically assume that the dynamics of the system are known and thus they are typically restricted to low-dimensional systems. While these methods come from safety-critical control, they are closely related to safe reinforcement learning .

**Learning Over Regular Structures.** RL and regular properties have been studied in conjunction before, perhaps most famously as 'Reward Machines' [42; 43] - a type of finite state automaton that specifies a different reward function at each automaton state. Reward machines do not explicitly deal with safety, rather non-Markovian reward functions that depend on histories distinguished by regular languages. Several methods have been developed to exploit the structure of these automata and dramatically speed up learning [42; 43; 55; 61], e.g., _counter factual experiences_.

Regular decision processes (RDP)  are a specific class non-Markovian DPs  that have also been studied in several works [13; 22; 51; 59; 65]. Most of these works are theoretical and slightly out-of-scope for this paper, as the RDP setting does not explicitly handle safety and encompasses both non-Markovian rewards and transition probabilities.

**Shielding.** From formal methods, shielding for safe RL  forces hard constraints on policies, using a reactive system that'shields' the agent from taking unsafe actions. Synthesising a _correct-by-construction_ reactive'shield' typically requires access to the environment dynamics and can be computationally demanding when the state or action space is large. Several recent works have aimed to scale the concept of shielding to more general settings, relaxing the prerequisite assumptions for shielding, by either only assuming access to a 'black box' model for planning , or learning a world model from scratch . Other notable works that can be viewed as shielding include, MASE  - a safe exploration algorithm with access to an 'emergency reset button', and Recovery-RL  - which has access to a'recovery policy' that is activated when the probability of reaching an unsafe state is too high. A simple form of shielding with LTL specifications has also been considered , but experimentally these methods have only been tested in quite simple settings.

## 3 Preliminaries

For a finite set \(\), let \(Pow()\) denote the power set of \(\). Also, let \(Dist()\) denote the set of distributions over \(\), where a distribution \(:\) is a function such that \(_{s}(s)=1\). Let \(^{*}\) and \(^{}\) denote the set of finite and infinite sequences over \(\) respectively. The set of all finite and infinite sequences is denoted \(^{}=^{*}^{}\). We denote as \(||\) the length of a sequence \(^{}\), where \(||=\) if \(^{}\). We also denote as \([i]\) the \(i+1\)-th element of a sequence, when \(i<||\), and we denote as \([i]=[||-1]\) the last element of a sequence, when \(^{*}\). A sequence \(_{1}\) is a prefix of \(_{2}\), denoted \(_{1}_{2}\), if \(|_{1}||_{2}|\) and \(_{1}[i]=_{2}[i]\) for all \(0 i|_{1}|\). A sequence \(_{1}\) is a proper prefix of \(_{2}\), denoted \(_{1}_{2}\), if \(_{1}_{2}\) and \(_{1}_{2}\).

**Labelled MDPs and Markov Chains.** An MDP is a tuple \(=(,,,_{0},, AP,L)\), where \(\) and \(\) are finite sets of states and actions resp.; \(: Dist()\) is the _transition function_; \(_{0} Dist()\) is the _initial state distribution_; \(:\) is the _reward function_; \(AP\) is a set of _atomic propositions_, where \(=Pow(AP)\) is the _alphabet_ over \(AP\); and \(L:\) is a _labelling function_, where \(L(s)\) denotes the set of atoms that hold in a given state \(s\). A memory-less (stochastic) _policy_ is a function \(: Dist()\) and its _value function_, denoted \(V_{}:\) is defined as the _expected reward_ from a given state under policy \(\), i.e., \(V_{}(s)=_{}[_{t=0}^{T}(s_{t},a_{t})|s_{0}=s]\), where \(T\) is a fixed episode length. Furthermore, denote as \(_{}=(,_{},_{0},AP,L)\) the _Markov chain_ induced by a fixed policy \(\), where the transition function is such that \(_{}(s^{}|s)=_{a}(s^{}|s,a)(a|s)\). A path \(^{}\) through \(_{}\) is a finite (or infinite) sequence of states. Using standard results from measure theory it can be shown that the set of all paths \(\{^{}_{pref}\}\) with a common prefix \(_{pref}\) is measurable .

**Probabilistic CTL.** (PCTL)  is a branching-time temporal logic for specifying properties of stochastic systems. A well-formed PCTL property can be constructed with the following grammar,

\[::=  a_{  p}[]\] \[::= X U U^{ n}\]

where \(a AP\), \(\{<,>,,\}\) is a binary comparison operator, and \(p\) is a probability. Negation \(\) and conjunction \(\) are the familiar logical operators from propositional logic, and next \(X\), until \(U\) and bounded until \(U^{ n}\) are the temporal operators from CTL . We make the distinction here between state formula \(\) and path formula \(\). The satisfaction relation for state formula \(\) is defined in the standard way for Boolean connectives. For probabilistic quantification we say that \(s_{ p}[]\) iff \((s):=( S^{}=s,)  p\). Let \(^{}(s)\) be the probability w.r.t. the Markov chain \(\). For path formula \(\) the satisfaction relation is as follows,

\[& X&&\\ &_{1}U_{2}&& j 0\,s.t.\,([j] _{2} 0 i<j,[i]_{1})\\ &_{1}U^{ n}_{2}&& 0 j n\,s.t.\,([j] _{2} 0 i<j,[i]_{1})\]

From the standard operators of propositional logic we may derive disjunction \(\), implication \(\) and coimplication \(\). We also note that the common temporal operators 'eventually' \(\) and 'always' \(\), and their bounded counterparts \(^{ n}\) and \(^{ n}\) can be derived in a familiar way, i.e., \(::=\,U\), \(\)\(::=\), resp. \(^{ n}::=\,U^{ n}\), \(^{ n}:=^{ n}\).

**Regular Safety Property.** A linear time property \(P_{}^{}\) over the alphabet \(\) is a safety property if for all words \(w^{} P_{}\), there exists a finite prefix \(w_{pref}\) of \(w\) such that \(P_{}\{w^{}^{}\)\(w_{pref} w^{}\}=\). Any such sequence \(w_{pref}\) is called a _bad prefix_ for \(P_{}\), a bad prefix \(w_{pref}\) is called _minimal_ iff there does not exist \(w^{} w_{pref}\) such that \(w^{}\) is a bad prefix for \(P_{}\). Let \((P_{})\) and \((P_{})\) denote the set of of bad and minimal bad prefixes resp.

A safety property \(P_{}^{}\) is _regular_ if the set \((P_{})\) constitutes a regular language. That is, there exists some _deterministic finite automata_ (DFA) that accepts the bad prefixes for \(P_{}\), that is, a path \(^{}\) is 'unsafe' if the trace \(()=L(),L(),^{}\) is accepted by the corresponding DFA.

**Definition 3.1** (Dfa).: _A deterministic finite automata is a tuple \(=(,,,_{0},)\), where \(\) is a finite set of states, \(\) is a finite alphabet, \(:\) is the transition function, \(_{0}\) is the initial state, and \(\) is the set of accepting states. The extended transition function \(^{*}\) is the total function \(^{*}:^{*}\) defined recursively as \(^{*}(q,w)=(^{*}(q,w w\!),w\!)\). The language accepted by DFA \(\) is denoted \(()=\{w^{*}^{*}(_{0},w) \}\)._

Furthermore, we denote as \(P_{}^{H}^{}\) the corresponding finite-horizon safety property for \(H_{+}\), where for all words \(w^{} P_{}^{H}\) there exists \(w_{pref} w\) such that \(|w_{pref}| H\) and \(w_{pref}(P_{})\). We model check regular safety properties by synchronizing the DFA and Markov chain in a standard way - by computing the product Markov chain.

**Definition 3.2** (Product Markov Chain).: _Let \(=(,,_{0},AP,L)\) be a Markov chain and \(=(,,,_{0},)\) be a DFA. The product Markov chain is \(=(,^{ },_{0}^{},\{accept\},L^{})\), where \(L^{}( s,q)=\{accept\}\) if \(q\) and \(L^{}( s,q)=\) on/u, \(_{0}^{}( s,q)=_{0}(s)\) if \(q=(Q_{0},L(s))\) and \(0\) on/u, and \(^{}( s^{},q^{}| s,q)= (s^{}|)\) if \(q^{}=(q,L(s^{}))\) and \(0\) on/u._

To compute the satisfaction probability of \(P_{}\) for a given state \(s\) we consider the set of paths \(^{}\) from \(s\) and the corresponding trace in the DFA. We provide the following definition.

**Definition 3.3** (Satisfaction probability for \(P_{}\)).: _Let \(=(,,_{0},AP,L)\) be a Markov chain and let \(=(,,,_{0},)\) be the DFA such that \(()=BadPref(P_{})\). For a path \(^{}\) in the Markov chain, let \(()=L(),L(),^{}\) be the corresponding word over \(=Pow(AP)\). From a given state \(s\) the satisfaction probability for \(P_{}\) is defined as follows,_

\[^{}(s P_{}):=^{}( ^{}=s,()( ))\]

_Perhaps more importantly, we note that this satisfaction probability can be written as the following reachability probability in the product Markov chain,_

\[^{}(s P_{})=^{ }( s,q_{s} accept)\]

_where \(q_{s}=(_{0},L(s))\) and \( accept\) is a PCTL path formula that reads, 'eventually accept' ._

For the corresponding finite-horizon safety property \(P_{}^{H}\) we state the following result.

**Proposition 3.4** (Satisfaction probability for \(P_{}^{H}\)).: _Let \(\) and \(\) be the MDP and DFA in Defn. 3.3. For a path \(^{}\) in the Markov chain, let \(_{H}()=L(),L(),L([H])\) be the corresponding finite word over \(=Pow(AP)\). For a given state \(s\) the finite horizon satisfaction probability for \(P_{}\) is defined as follows,_

\[^{}(s P_{}^{H}):=^{}( ^{}=s,_{H}()( ))\]

_where \(H_{+}\) is some fixed model checking horizon. Similar to before, we show that the finite horizon satisfaction probability can be written as the following bounded reachability probability._

\[^{}(s P_{}^{H})=^{ }( s,q_{s}^{H}accept)\]

_where \(q_{s}=(_{0},L(s))\) is as before and \(^{ H}accept\) is the corresponding step-bounded PCTL path formula that reads, 'eventually accept in H timesteps'._

The unbounded reachability probability can be computed by solving a system of linear equations, the bounded reachability probability can be computed with \((H)\) matrix multiplications, in both cases the time complexity of the procedure is a polynomial in the size of the product Markov chain .

## 4 Problem Setup

In this paper, we are interested in the quantitative model checking of regular safety properties for a fixed finite horizon \(H\) and in the context of episodic RL, i.e., where the length of the episode \(T\) is fixed. In particular, at every timestep we constrain the (step-bounded) reachability probability \(( s,q^{ H}accept\)) in the product Markov chain \(_{}\). We assume that \(H\) is chosen so as to avoid any irrecoverable states , i.e., those that lead to a violation of the safety property no matter the sequence of actions taken, the precise details of this notion are presented in Section 6. We specify the following constrained problem,

**Problem 4.1** (Step-wise bounded regular safety property constraint).: _Let \(P_{}\) be a regular safety property, \(\) be the DFA such that \(()=(P_{})\) and \(\) be the MDP;_

\[_{}V_{} s_{t},q_{t} ^{ H}accept p_{1} t[0,T]\]

_where all probability is taken under the product Markov Chain \(_{}\), \(p_{1}\) is a probability threshold, \(H\) is the model checking horizon and \(T\) is the fixed episode length._

The hyperparameter \(p_{1}\) is be directly used to trade-off safety and exploration in a semantically meaningful way; \(p_{1}\) prescribes the probability of satisfying the finite-horizon safety property \(P_{}^{H}\) at each timestep. In particular, if \(p_{1}\) is sufficiently small then we can guarantee (with high-probability) that the regular safety property \(P_{}\) is satisfied for the entire episode length \(T\).

**Proposition 4.2**.: _Let \(P_{}^{T}\) denote the (episodic) regular safety property for a fixed episode length \(T\). Then satisfying \( s_{t},q_{t}^{ H}accept p _{1}\) for all \(t[0,T]\) guarantees that \((s_{0} P_{}^{T}) 1-p_{1} T/H\), where \(s_{0}_{0}\) is the initial state._

**Comparison to CMDP.** In the remainder of this section, we compare our problem setup to various CMDP settings , with the aim of unifying different perspectives from safe RL. The purpose of this is to show that our proposed method for solving Problem 4.1 can also be used to satisfy other more common CMDP constraints. First, we define the following cost function that prescribes a scalar cost \(C>0\) when the regular safety property \(P_{}\) is violated and \(0\) otherwise.

**Definition 4.3** (Cost function).: _Let \(P_{}\) be a regular safety property and let \(\) be the DFA such that \(()=BadPref(P_{})\), modified such that for all \(q\), \(q_{0}\). The cost function is then defined as,_

\[( s,q)=C&accept L^{}(  s,q)\\ 0&\]

_where \(C>0\) is some generic scalar cost and \(L^{}\) is the labelling function defined in Def. 3.2._

_Resetting the DFA._ Rather than reset the environment, the DFA is reset once it reaches an accepting state, so as to measure the rate of constraint satisfaction over a fixed episode length \(T\). This can easily be realized by replacing any outgoing transitions from the accepting states with transitions back to the initial state, i.e., for all \(q\), \(q_{0}\).

_Non-Markovian costs._ The cost function is Markov on the product states \( s,q\). However, in most cases the cost function is non-Markovian in the original state space \(\), since the automaton state \(q\) could depend on some arbitrary history of states. Thus our problem setup generalizes the standard CMDP framework with non-Markovian safety constraints.

_Invariant properties._ Invariant properties \(P_{inv}()\), also written \(\) ('always \(\)'), where \(\) is a propositional state formula, are the simplest type of safety properties where the cost function is still Markov in the original state space. In this case we are operating in the standard CMDP framework, we also note that checking invariant properties with a fixed model checking horizon has been studied in previous works, as _bounded safety_ and _safety for a finite horizon_.

The most common type of CMDP constraints are _expected cumulative (cost) constraints_, which constrain the expected cost below a given threshold.

**Problem 4.4** (Expected cumulative constraint ).: \[_{}V_{}_{ s_{t},q_{t} _{}}[_{t=0}^{T}(  s_{t},q_{t})] d_{1}\]

_where \(d_{1}_{+}\) is the cost threshold and \(T\) is the fixed episode length.__Probabilistic cumulative (cost) constraints_, are a stricter class of constraints that constrain the cumulative cost with high probability, rather than in expectation.

**Problem 4.5** (Probabilistic cumulative constraint ).: \[_{}V_{}_{ s_{t},q_{t} _{}}[_{t=0}^{T} ( s_{t},q_{t}) d_{2}] 1-_{2}\]

_where \(d_{2}_{+}\) is the cost threshold, \(_{2}\) is a tolerance parameter, and \(T\) is the fixed episode length._

We also consider _instantaneous constraints_, which bound the cost 'almost surely' at each timestep \(t[0,T]\). These are an even stricter type of constraint for highly safety-critical applications.

**Problem 4.6** (Instantaneous constraint ).: \[_{}V_{}_{ s_{t},q_{t} _{}}( s_{ t},q_{t}) d_{3}=1 t[0,T]\]

_where \(d_{3}_{+}\) is the cost threshold and \(T\) is the fixed episode length._

In particular, these problems define a constrained set of feasible policies \(\). We make the distinction here between a feasible policy and a solution to the problem, the former being any policy satisfying the constraints of the problem and the later being the optimal policy within the feasible set \(\).

**Theorem 4.7**.: _A feasible policy for Problem 4.1 is also a feasible policy for Problems 4.4, 4.5 and 4.6 under specific parameter settings for \(p_{1}\), \(d_{1}\), \(d_{2}\) and \(_{2}\), and \(d_{3}\)._

In Appendix G we provide a full set of statements that outline the relationships between the constrained problems presented in this section. The significance of these results is that they demonstrate by solving Problem 4.1 with our proposed method we can obtain feasible policies for Problems 4.4, 4.5 and 4.6, although for most of these problems there is no direct relationship between our problem setup, in particular we can say little about whether the optimal policy for one problem is necessarily optimal for another. Nevertheless, we find it interesting to explore the relationships between our setup and other perhaps more common constrained RL problems.

## 5 Model checking

In this section we outline several procedures for checking the finite-horizon satisfaction probability of regular safety properties and we summarise the settings in which they can be used.

**Assumption 5.1**.: _We are given access to the 'true' transition probabilities \(\)._

**Assumption 5.2**.: _We are given access to a 'black box' model that perfectly simulates the 'true' transition probabilities \(\)._

**Assumption 5.3**.: _We are given access to an approximate dynamic model \(}\), where the total variation (TV) distance \(D_{TV}(_{}( s),}_{}( s ))/H\), for all \(s\).1_

**Exact model checking.** Under Assumption 5.1 we can precisely compute the (finite horizon) satisfaction probability of \(P_{}\), in the Markov chain \(_{}\) induced by the fixed policy \(\) in time \((((_{}))  H)\), where \(\) is the DFA such that \(()=BadPref(P_{})\) and \(H\) is the model checking horizon. \(H\) should not be too large and so the complexity of exact model checking ultimately depends on the size of the product \(_{}\), and so if the size of either the MDP or DFA is too large then exact model checking may be infeasible.

**Monte-Carlo model checking.** To address the limitations of exact model checking, we can drop Assumption 5.1. Rather, under Assumption 5.2, we can sample sufficiently many paths from a 'black box' model of the environment dynamics and estimate the reachability probability \(( s,q^{ H}accept)\) in the product Markov chain \(_{}\), by computing the proportion of accepting paths. Using statistical bounds, such as Hoeffding's inequality  or Bernstein-type bounds , we can bound the error of this estimate, with high probability.

**Proposition 5.4**.: _Let \(>0\), \(>0\), \(s\) and \(H 1\) be given. Under Assumption 5.2, we can obtain an \(\)-approximate estimate for the probability \(( s,q Haccept)\) with probability at least \(1-\), by sampling \(m}()\) paths from the 'black box' model._We note that the time complexity of these statistical methods does not depend in the size of the product MDP or DFA, since the product states \( s,q\) can be computed _on-the-fly_, rather the time complexity depends on the horizon \(H\), the desired level of accuracy \(\), failure probability \(\).

**Model checking with approximate models.** In most realistic cases neither the 'true' transition probabilities nor a perfect 'black box' model is available to us before-hand. Under Assumption 5.3 we can model check with an 'approximate' model of the MDP dynamics, which can either be constructed ahead of time (offline) or learned from experience, with maximum likelihood (or similar). We can then either exact model check in with the 'approximate' probabilities, or if the MDP is too large, we can leverage statistical model checking by sampling paths from the 'approximate' model.

**Proposition 5.5**.: _Let \(>0\), \(>0\), \(s\) and \(H 1\) be given. Under Assumption 5.3 we can make the following two statements:_

_(1) We can obtain an \(\)-approximate estimate for \(( s,q^{ H}accept)\) with probability \(1\) by exact model checking with the transition probabilities of \(}_{}\) in time \((((_{}))  H)\)._

_(2) We can obtain an \(\)-approximate estimate for \(( s,q^{ H}accept)\) with probability at least \(1-\), by sampling \(m}()\) paths from the 'approximate' dynamics model \(}_{}\)._

## 6 Shielding the policy

At a high-level, the shielding meta-algorithm works by switching between the 'task policy' trained with RL to maximize rewards and a 'backup policy', which typically constitutes a low-reward, possibly rule-based policy that is guaranteed to be safe. In some cases this 'backup policy' may be available to us before training, although in most realistic cases it will need to be learned. In our case we switch from the 'task policy' to the 'backup policy' when the reachability probability \(( s,q^{ H}accept)\) exceeds the probability threshold \(p_{1}\). To check this we can use any of the model checking procedures presented earlier. The 'backup policy' is used when the reachability probability exceeds \(p_{1}\). Intuitively if the 'backup policy' is guaranteed to be safe, then our system should satisfy the constraints of Problem 4.1, independent of the 'task policy'.

**Backup policy.** In general we assume no knowledge of the safety dynamics before training and so the 'backup policy' needs to be learned. In particular, we can use the cost function defined in Defn. 4.3 and train the 'backup policy' with RL to minimize the _expected discounted cost_ (\(_{}[_{t=0}^{T}^{t}(s_{t},q_{t})]\)). Importantly, we note that the cost function is defined on the product state space \(\) and so the 'backup policy' must also operate on this state space, possibly leading to slower convergence. However, we can eliminate this issue entirely by training the 'backup policy' with _counterfactual experiences_ - a method originally used for reward machines that generates additional synthetic data for the policy, by simulating experience from each automaton state.

**Meta Algorithm.** We now present the structure of the shielding meta-algorithm (see Algorithm 1). The precise realization of this algorithm can vary depending on problem setting, tabular, deep RL, etc., however the main structure of the algorithm remains the same. In particular, during interaction with the environment we shield the agent by checking that the reachability probability \(( s,q^{ H}accept)\) does not exceed threshold \(p_{1}\). Then, with the new accumulated experience we update the 'task policy' denoted \(_{}\) and the 'backup policy' denoted \(_{}\) with RL, and if need be 

[MISSING_PAGE_FAIL:8]

experiences . In all cases, by separating reward and safety into two distinct policies, we are able to effectively trade-off the two objectives. Q-learning simply finds the best policy ignoring the costs, and Q-learning with penalties is able to find a safe policy, but struggles to meaningfully balance both objectives (see Fig. 2). Hyperparameter settings for all experiments are detailed in Appendix E. In addition, we provide an extensive series of ablation studies in Appendix F for these experiments. For example, we show that we don't loose much by using Monte Carlo model checking as opposed to exact model checking with the 'true' probabilities. We also show that tuning the cost coefficient \(C\) offers no meaningful way to trade-off reward and the probability of constraint satisfaction.

**Deep RL.** We deploy our version of Algorithm 1 built on DreamerV3  on Atari Seaquest, provided as part of the Arcade Learning Environment (ALE). We experiment with two different regular safety properties: (1) (\(\)-_surface_\(\)(_surface_\(\)_diver_)) \(\) (\(\)-_out-of-oxygen_) \(\) (\(\)-_hit_) and (2) \(\)_diver_\(\)

\(\)_surface_\(\)\(^{ 30}\)_surface_. We compare our approach to the base DreamerV3 algorithm and a version of DreamerV3 that implements the augmented Lagrangian penalty framework, similarly to , for additional details see Appendix B.1.

Again our approach is able to effectively trade-off both objectives, while (base) DreamerV3 ignores the cost, the Lagrangian approach appears to learn a safe policy that is not always efficient in terms of reward (see Fig. 3). We refer the reader to Appendix D.2 for more details of the environment and an extended discussion.

**Separating Reward and Safety.** The separation of reward and safety objectives into two distinct policies has been demonstrated as an effective strategy towards safety-aware decision making , in many cases the safety objective is simpler and can be more quickly learnt . In our experiments it is clear that when the system enters a critical state, the 'backup policy' is able to efficiently guide the system back to a non-critical state where the task policy can continue collecting reward. However, there is evidence that the complete separation of policies is not always appropriate  and penalties or a slight coupling of the policies is required to stop the 'task' and 'backup policy' fighting for control of the system. Furthermore, by separating reward and safety, we typically loose any asymptotic convergence guarantees, similar to the situation faced for hierarchical RL , although there has been recent work to develop convergence guarantees for shielding .

## 8 Conclusion

In this paper we propose a shielding meta-algorithm for the runtime verification of regular safety properties, given as a probabilistic constraint on the system. We provide a thorough theoretical examination of the problem and develop probabilistic safety guarantees for the meta-algorithm, which hold under reasonable assumptions. Empirically, we demonstrate that shielding is able to effectively balance both reward and safety, in both the tabular and deep RL setting. A more thorough theoretical and empirical examinations of the conditions for when shielding is appropriate would be an interesting direction for future work.

Figure 3: Episode reward and violation rate for deep RL Atari Seaquest.

Figure 2: Episode reward and cost for tabular RL ‘colour’ gridworld environment.