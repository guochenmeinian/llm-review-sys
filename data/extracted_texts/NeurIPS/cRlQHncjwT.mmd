# Generative Forests

Richard Nock

Google Research

richardnock@google.com &Mathieu Guillame-Bert

Google

gbm@google.com

We use this now common parlance expression on purpose, to avoid confusion with the other "generative" problem that consists in modelling densities .

###### Abstract

We focus on generative AI for a type of data that still represent one of the most prevalent form of data: tabular data. Our paper introduces two key contributions: a new powerful class of forest-based models fit for such tasks and a simple training algorithm with strong convergence guarantees in a boosting model that parallels that of the original weak / strong supervised learning setting. This algorithm can be implemented by a few tweaks to the most popular induction scheme for decision tree induction (_i.e. supervised learning_) with two classes. Experiments on the quality of generated data display substantial improvements compared to the state of the art. The losses our algorithm minimize and the structure of our models make them practical for related tasks that require fast estimation of a density given a generative model and an observation (even partially specified): such tasks include missing data imputation and density estimation. Additional experiments on these tasks reveal that our models can be notably good contenders to diverse state of the art methods, relying on models as diverse as (or mixing elements of) trees, neural nets, kernels or graphical models.

## 1 Introduction

There is a substantial resurgence of interest in the ML community around tabular data, not just because it is still one of the most prominent kind of data available : it is recurrently a place of sometimes heated debates on what are the best model architectures to solve related problems. For example, even for well-posed problems with decades of theoretical formalization like supervised learning , after more than a decade of deep learning disruption , there is still much ink spilled in the debate decision forests vs neural nets . Generative AI* makes no exception. Where the consensus has long been established on the best categories of architectures for data like image and text (neural nets), tabular data still flourishes with a variety of model architectures building up - or mixing - elements from knowledge representation, logics, kernel methods, graph theory, and of course neural nets  (see Section 2). Because of the remarkable nature of tabular data where a single variable can bring considerable information about a target to model, each of these classes can be a relevant choice at least in _some_ cases (such is is the conclusion of  in the context of supervised learning). A key differentiator between model classes is then training and the formal guarantees it can provide .

**In this paper**, we introduce new generative models based on sets of trees that we denote as _generative forests_ (gf), along with a training algorithm which has two remarkable features: it is extremely simple and brings strong convergence guarantees in a weak / strong learning model that parallels that of the original boosting model of Valiant's PAC learning . These guarantees improve upon the best state of the art guarantees . Our training algorithm, gf.Boost, supports training from data with missing values and is simple enough to be implementable by a few tweaks on the popular induction scheme for _decision tree induction_ with two classes, which is supported by a huge numberof repositories / ML software implementing algorithms like CART or C4.5 [1; 35; 3; 36; 45]. From the model standpoint, generative forests bring a sizeable combinatorial advantage over its closest competitors, generative trees  (see Table 1 for an example) and adversarial random forests . Experiments on a variety of simulated or readily available domains display that our models can substantially improve upon state of the art, with models of ours as simple as a set of stumps potentially competing with other approaches building much more complex models.

The models we build have an additional benefit: it is computationally easy to compute the full density given an observation, _even partially specified_; hence, our generative models can also be used for side tasks like missing data imputation or density estimation. Additional experiments clearly display that our approach can be a good contender to the state of the art. To save space and preserve readability, all proofs and additional experiments and results are given in an Appendix.

## 2 Related work

It would not do justice to the large amount of work in the field of "Generative AI" for tabular data to just sample a few of them, so we devote a part of the Appendix to an extensive review of the state of the art. Let us just mention that, unlike for unstructured data like images, there is a huge variety of model types, based on trees [7; 31; 44], neural networks [12; 14; 20; 47], probabilistic circuits [5; 43; 38], kernel methods [4; 34], graphical models  (among others: note that some are in fact hybrid models). The closest approaches to ours are  and , because the models include trees with a stochastic activation of edges to pick leaves, and a leaf-dependent data generation process. While  learn a single tree,  use a way to generate data from a set of trees - called an adversarial random forest - which is simple: sample a tree, and then sample an observation from the tree. The model is thus simple but not economical: each observation is generated by a single tree only, each of them thus having to represent a good sampler in its own. In our case, generating one observation makes use of _all_ trees (Figure 1, see also Section 4). We also note that the theory part of  is limited because it resorts to statistical consistency (infinite sample) and Lipschitz continuity of the target density, with second derivative continuous, square integrable and monotonic. Similarly,  resort to a wide set of proper losses, but with a symmetry constraint impeding in a generative context. As we shall see, our theoretical framework does not suffer from such impediments.

## 3 Basic definitions

Perhaps surprisingly at first glance, this Section introduces generative and _supervised_ loss functions. Indeed, our algorithm, which trains a data generator and whose overall convergence shall be shown on a generative loss, turns out to locally optimize a _supervised_ loss. For the interested reader, the key link, which is of independent interest since it links in our context losses for supervised and generative learning, is established in Lemma A (Appendix).

\( k_{>0}\), let \([k]\{1,2,...,k\}\). Our notations follow . Let \((,,)\) denote a _binary task_, where \(,\) (and any other measure defined hereafter) are probability measures with the same support, also called _domain_, \(\), and \(\) is a _prior_. \(+(1-)\) is the corresponding mixture measure. For the sake of simplicity, we assume \(\) bounded hereafter, and note that tricks can be used to remove this assumption [31; Remark 3.3]. In tabular data, each of the \(()\) features

   domain & \(\). _Center_: density learned by a generative forest (gf) consisting of a single tree, boosted for a small number (50) of iterations. _Right_: density learned by a gf consisting of 50 boosted tree stumps (_Center_ and _Right_ learned using gf.Boost). In a domain \(\) with dimension \(d\), a single tree with \(n\) splits can only partition the domain in \(n+1\) parts. On the other hand, a set of \(n\) stumps in a gf can boost this number to \(n^{(d)}\) (the tilda omits \(\) dependencies), which explains why the right density appears so much better than the central one, even when each tree is just a stump. \\   

Table 1: _Left_: domain \(\). _Center_: density learned by a generative forest (gf) consisting of a single tree, boosted for a small number (50) of iterations. _Right_: density learned by a gf consisting of 50 boosted tree stumps (_Center_ and _Right_ learned using gf.Boost). In a domain \(\) with dimension \(d\), a single tree with \(n\) splits can only partition the domain in \(n+1\) parts. On the other hand, a set of \(n\) stumps in a gf can boost this number to \(n^{(d)}\) (the tilda omits \(\) dependencies), which explains why the right density appears so much better than the central one, even when each tree is just a stump.

can be of various _types_, including categorical, numerical, etc., and associated to a natural measure (counting, Lebesgue, etc.) so we naturally associate \(\) to the product measure, which can thus be of mixed type. We also write \(_{i=1}^{d}_{i}\), where \(_{i}\) is the set of values that can take on variable \(i\). Several essential measures will be used in this paper, including U, the uniform measure, \(\), the measure associated to a generator that we learn, \(\), the _empirical_ measure corresponding to a training sample of observations. Like in , we do not investigate generalisation properties.

**Loss functions** There is a natural problem associated to binary task \(\), that of estimating the probability that an arbitrary observation \(\) was sampled from \(-\) such _positive_ - or \(-\) these _negative_ -. To learn a _supervised_ model \(\) for such a _class probability estimation_ (cpe) problem, one usually has access to a set of examples where each is a couple (observation, class), the class being in set \(=\{-1,1\}\) (={negative, positive}). Examples are drawn i.i.d. according to \(\). Learning a model is done by minimizing a loss function: when it comes to cpe, any such cpe loss  is some \(:\) whose expression can be split according to _partial_ losses \(_{1},_{-1}\), \((y,u)[y=1]_{1}(u)+[y=-1]_{-1}(u)\). Its (pointwise) _Bayes risk_ function is the best achievable loss when labels are drawn with a particular positive base-rate,

\[(p)_{u}_{(p)}( ,u),\] (1)

where \(\). A fundamental property for a cpe loss is _properness_, encouraging to guess ground truth: \(\) is proper iff \((p)=_{(p)}(,p),  p\), and _strictly_ proper if \((p)<_{(p)}(,u),  u p\). Strict properness implies strict concavity of Bayes risk. For example, the square loss has \(_{1}^{ 0}(u)=(1-u)^{2}\), \(_{-1}^{ 0}(u) u^{2}\), and, being strictly proper, Bayes risk \(L^{ 0}(u) u(1-u)\). Popular strictly proper ML include the log and Matusita's losses. All these losses are _symmetric_ since \(_{1}^{ 0}(u)=_{-1}^{ 0}(1-u), u(0,1)\) and _differentiable_ because both partial losses are differentiable.

In addition to cpe losses, we introduce a set of losses relevant to generative approaches, that are popular in density ratio [29; 40]. For any differentiable and convex \(F:\), the Bregman divergence with generator \(F\) is \(D_{F}(z|z^{}) F(z)-F(z^{})-(z-z^{})F^{}(z^{ })\). Given function \(g:\), the generalized perspective transform of \(F\) given \(g\) is \((z) g(z) F(z/g(z))\), \(g\) being implicit in notation \(\)[26; 27; 32]. The _Likelihood ratio risk_ of \(\) with respect to \(\) for loss \(\) is

\[_{}(,)  _{}[D_{(})}(}{} |}{}.)],\] (2)

with \(g(z) z+(1-)/\) in the generalized perspective transform. The prior multiplication is for technical convenience. \(_{}\) is non-negative; strict properness is necessary for a key property of \(_{}\): \(_{}=0\) iff \(=\) almost everywhere .

## 4 Generative forests: models and data generation

**Architecture** We first introduce the basic building block of our models, _trees_.

**Definition 4.1**.: _A **tree**\(\) is a binary directed tree whose internal nodes are labeled with an observation variable and arcs are consistently labeled with subsets of their tail node's variable domain._

_Consistency_ is an important generative notion; informally, it postulates that the arcs' labels define a partition of the measure's support. To make this notion formal, we proceed need a key definition.

Figure 1: Sketch of comparison of two approaches to generate one observation, using Adversarial Random Forests  (left) and using generative forests, gf (right, this paper). In the case of Adversarial Random Forests, a tree is sampled uniformly at random, then a leaf is sampled in the tree and finally an observation is sampled according to the distribution “attached” to the leaf. Hence, only one tree is used to generate an observation. In our case, we leverage the combinatorial power of the trees in the forest: _all trees_ are used to generate one observation, as each is contributing to one leaf. Figure 3 provides more details on generation using gf.

For any node \(()\) (the whole set of nodes of \(\), including leaves), we denote \(_{}\) the _support_ of the node. The root has \(_{}=\). To get \(_{}\) for any other node, we initialize it to \(\) and then descend the tree from the root, progressively updating \(_{}\) by intersecting an arc's observation variable's domain in \(_{}\) with the sub-domain labelling the arc until we reach \(\). Then, a labeling of arcs in a tree is _consistent_ iff it complies with one constraint **(C)**:

**(C)**: for each internal node \(\) and its left and right children \(_{t},_{}\) (respectively), \(_{}=_{_{t}}_{_{}}\) and the measure of \(_{_{t}}_{_{}}\) with respect to \(\) is zero.

For example, the first split at the root of a tree is such that the union of the domains at the two arcs equals the domain of the feature labeling the split (see Figure 2). We define our generative models.

**Definition 4.2**.: _A **generative forest** (gf), \(\), is a set of trees, \(\{_{t}\}_{t=1}^{T}\), associated to measure \(\) (implicit in notation)._

Figure 2 shows an example of gf. Following the consistency requirement, any single tree defines a recursive partition of \(\) according to the splits induced by the inner nodes. Such is _also_ the case for a set of trees, where _intersections_ of the supports of tuples of leaves (1 for each tree) define the subsets:

\[()_{i=1}^{T}_{_{i} }_{i}(_{i}), i}\] (3)

(\(()()\) is the set of leaves of \(\)). Notably, we can construct the elements of \(()\) using the same algorithm that would compute it for 1 tree. First, considering a first tree \(_{1}\), we compute the support of a leaf, say \(_{_{1}}\), using the algorithm described for the consistency property above. Then, we start again with a second tree \(_{2}\)_but_ replacing the initial \(\) by \(_{_{1}}\), yielding \(_{_{1}}_{_{2}}\). Then we repeat with a third tree \(_{3}\) replacing \(\) by \(_{_{1}}_{_{2}}\), and so on until the last tree is processed. This yields one element of \(()\).

**Generating one observation** Generating one observation relies on a _stochastic_ version of the procedure just described. It ends up in an element of \(()\) of positive measure, from which we sample uniformly one observation, and then repeat the process for another observation. To describe the process at length, we make use of two key routines, Init and StarUpdate, see Algorithms 1 and 2. Init initializes "special" nodes in each tree, that are called _star nodes_, to the root of each tree (notation for a variable \(v\) relative to a tree \(\) is \(.v\)). Stochastic activation, performed in StarUpdate, progressively makes star nodes descend in trees. When all star nodes have reached a leaf in their respective tree, an observation is sampled from the intersection of the leaves' domains (which is an element of \(()\)). A Boolean flag, done takes value true when the star node is in the leaf set of the tree, indicating no more StarUpdate calls for the tree ([.] is Iverson's bracket, ).

StarUpdate is called with a tree of the gf for which done is false, a subset \(\) of the whole domain and measure \(\). The first call of this procedure is done with \(=\). When all trees are marked done, \(\) has been "reduced" to some \(_{s}()\), where the index reminds that this is the last \(\) we obtain, from which we sample an observation, uniformly at random in \(_{}\). Step 1 in StarUpdate is

Figure 2: A gf (\(T=2\)) associated to UCI German Credit. Constraint **(C)** (see text) implies that the domain of ”Number existing credits” is \(\{0,1,...,8\}\), that of ”Job” is {A171, A172, A173, A174}, etc..

fundamental: it relies on tossing an unfair coin (a Bernoulli event noted \((p)\)), where the head probability \(p\) is just the mass of \(\) in \(_{,^{*}_{*}}\)_relative to \(\)_. Hence, if \(_{,^{*}_{*}}=\), \(p=1\). There is a simple but important invariant (proof omitted).

**Lemma 4.3**.: _In StarUpdate, it always holds that the input \(\) satisfies \(_{,^{*}}\)._

We have made no comment about the _sequence_ of tree choices over which StarUpdate is called. Let us call _admissible_ such a sequence that ends up with _some_\(_{s}\). \(T\) being the number of trees (see Init), for any sequence \([T]^{D(_{s})}\), where \(D(_{s})\) is the sum of the depths of all the star _leaves_ whose support intersection is \(_{s}\), we say that \(\) is _admissible for \(_{s}\)_ if there exits a sequence of branchings in Step 1 of StarUpdate, whose corresponding sequence of trees follows the indexes in \(\), such that at the end of the sequence all trees are marked done and the last \(=_{s}\). Crucially, the probability to end up in \(_{s}\) using StarUpdate, given any of its admissible sequences, is the _same_ and equals its mass with respect to \(\).

**Lemma 4.4**.: _For any \(_{s}()\) and admissible sequence \([T]^{D(_{s})}\) for \(_{s}\), \(p_{}[_{s}|]=p_{}[_{s}]\)._

The Lemma is simple to prove but fundamental in our context as the way one computes the sequence - and thus the way one picks the trees - does not bias generation: the sequence of tree choices could thus be iterative, randomized, concurrent (_e.g._ if trees were distributed), etc., this would not change generation's properties from the standpoint of Lemma 4.4. We defer to Appendix (Section II) three examples of sequence choice. Figure 3 illustrates a sequence and the resulting \(_{s}\).

**Missing data imputation and density estimation with \(\)** A generative forest is not just a generator: it models a density and the exact value of this density at any observation is easily available. Figure 4 (top row) shows how to compute it. Note that if carried out in parallel, the complexity to get this density is cheap, of order \(O(_{t}(_{t}))\). If one wants to prevent predicting zero density, one can stop the descent if the next step creates a support with zero empirical measure. A \(\) thus also models an easily tractable density, but this fact alone is not enough to make it a _good_ density estimator. To get there, one has to factor the loss minimized during training. In our case, as we shall see in the following Section, we train a \(\) by minimizing an information-theoretic loss directly formulated on this density (2). So, using a \(\) also for density estimation can be a reasonable additional benefit of training \(\).

The process described above that finds leaves reached by an observation can be extended to missing values in the observation using standard procedures for classification using decision trees. We obtain a simple procedure to carry out _missing data imputation_ instead of density estimation: once a tuple of leaves is reached, one uniformly samples the missing features in the corresponding support. This is fast, but at the expense of a bit more computations, we can have a much better procedure, as explained in Figure 4 (bottom row). In a first step, we compute the density of each support subset corresponding to _all_ (not just 1 as for density estimation) tuples of leaves reached in each tree. This provides us with the _full density_ over the missing values _given_ (i) a partial assignment of the tabular domain's variables and (ii) the \(\). We then keep the elements corresponding to the maximal density value and finally simultaneously sample all missing features uniformly in the corresponding domain. Overall, the whole procedure is \(O(d(_{t}(_{t}))^{2})\).

Figure 3: From left to right and top to bottom: updates of the argument \(\) of StarUpdate through a sequence of run of StarUpdate in a generative forest consisting of three trees (the partition of the domain induced by each tree is also depicted, alongside the nature of splits, vertical or horizontal, at each internal node) whose star nodes are indicated with chess pieces (\(\), \(\), \(\)). In each picture, \(\) is represented at the bottom of the picture (hence, \(=\) after Init). In the bottom-right picture, all star nodes are leaves and thus \(_{s}=\) displays the portion of the domain in which an observation is sampled. Remark that the last star node update produced no change in \(\).

## 5 Learning generative forests using supervised boosting

**From the GAN framework to a fully supervised training of generative models** It can appear quite unusual to train generative models using a supervised learning framework, so before introducing our algorithm, we provide details on its filiation in the generative world, starting from the popular GAN training framework . In this framework, one trains a generative model against a discriminator model which has no purpose other than to parameterize the generative loss optimized. As shown in , there is a generally inevitable slack between the generator and the discriminator losses with neural networks, which translates into uncertainty for training.  show that the slack disappears for calibrated models, a property satisfied by their generative trees (and also by our generative forests). Moreover,  also show that the GAN training can be simplified and made much more efficient for generative trees by having the discriminator (a decision tree) copy the tree structure of the generator, a setting defined as _copycat_. Hence, one gets reduced uncertainty and more efficient training. Training still involves two models but it becomes arguably very close to the celebrated boosting framework for top-down induction of decision trees , up to the crucial detail that the generator turns out to implements boosting's leveraged "hard" distribution. This training gives guarantees on the likelihood ratio risk we define in (2). Our paper closes the gap with boosting: copycat training can be equivalently simplified to training a _single generative model_ in a _supervised_ (2 classes) framework. The two classes involved are the observed data and the uniform distribution. Using the uniform distribution is allowed by the assumption that the domain is closed, which is reasonable for tabular data but can also be alleviated by reparameterization [31, Remark 3.3]. The (supervised) loss involved for training reduces to the popular concave "entropic"-style losses used in CART, C4.5 and in popular packages like scikit-learn. And of course, minimizing such losses _still_ provides guarantee on the generative loss (2). While it is out of scope to show how copycat training does ultimately simplify, we provide all components of the "end product": algorithm, loss optimized and the link between the minimization of the supervised and generative losses via boosting.

Figure 4: (**Top row**) Density estimation using a gf, on an observation indicated by \(\) (_Left_). In each tree, the leaf reached by the observation is found and the intersection of all leaves’ supports is computed. The estimated density at \(\) is computed as the empirical measure in this intersection over its volume (_Right_). (**Bottom row**) Missing data imputation using the same gf, and an observation with one missing value (if \(^{2}\), then \(y\) is missing). We first proceed like in density estimation, finding in each tree _all_ leaves _potentially_ reached by the observation if \(y\) were known (_Left_); then, we compute the density in _each_ non-empty intersection of all leaves’ supports; among the corresponding elements with maximal density, we get the missing value(s) by uniform sampling (_Right_).

**The algorithm** To learn a gf, we just have to learn its set of trees. Our training algorithm, gf.Boost (Algorithm 3), performs a greedy top-down induction. In Step 1, we initialize the set of \(T\) trees to \(T\) roots. Steps 2.1 and 2.2 choose a tree (\(_{*}\)) and a leaf to split (\(_{*}\)) in the tree. In our implementation, we pick the leaf among all trees which is the heaviest with respect to \(\). Hence, we merge Steps 2.1 and 2.2. Picking the heaviest leaf is standard to grant boosting in decision tree induction ; in our case, there is a second benefit: we tend to learn size-balanced models. For example, during the first \(J=T\) iterations, each of the \(T\) roots gets one split because each root has a larger mass (1) than any leaf in a tree with depth \(>0\). Step 2.4 splits \(_{*}\) by replacing \(_{*}\) by a stump whose corresponding splitting predicate, p, is returned in Step 2.3 using a weak splitter oracle called splitPred. "weak" refers to boosting's weak/strong learning setting  and means that we shall only require lightweight assumptions about this oracle; in decision tree induction, this oracle is the key to boosting from such weak assumptions . This will also be the case for our generative models. We now investigate Step 2.3 and splitPred.

**The weak splitter oracle splitPred** In decision tree induction, a splitting predicate is chosen to reduce an expected Bayes risk (1) (_e.g._ that of the log loss , square loss , etc.). In our case, splitPred does about the same _with a catch in the binary task it addresses_, which is+ (our mixture measure is thus \(+(1-)\)). The corresponding expected Bayes risk that splitPred seeks to minimize is just:

Footnote †: The prior is chosen by the user: without reasons to do otherwise, a balanced approach suggests \(=0.5\).

\[}()_{ ()}p_{}[]( }[]}{p_{}[]}).\] (4)

The concavity of \(\) implies \(}()()\). Notation \((.)\) overloads that in (3) by depending explicitly on the set of trees of \(\) instead of \(\) itself. Regarding the minimization of (4), there are three main differences with classical decision tree induction. The first is computational: in the latter case, (4) is optimized over a single tree: there is thus a single element in \(()\) which is affected by the split, \(_{_{}}\). In our case, multiple elements in \(()\) can be affected by one split, so the optimisation is more computationally demanding but a simple trick allows to keep it largely tractable: we do not need to care about keeping in \(()\) support elements with zero empirical measure since they will generate no data. Keeping only elements with strictly positive empirical measure guarantees a size \(|()|\) never bigger than the number of observations defining \(\). The second difference plays to our advantage: compared to classical decision tree induction, a single split generally buys a substantially bigger slack in \(}()\) in our case. To see this, remark that for the candidate leaf \(_{*}\),

\[_{()\\ _{_{}}}p_{ }[](}[ ]}{p_{}[]}) = p_{}[_{_{}}]_{ ()\\ _{_{}}}}[]}{p_{}[_{_{}}] } L(}[]}{p_{}[ ]})\] \[ p_{}[_{_{}}] (}[_{_{}}] }{p_{}[_{_{}}]})\]

(because \(\) is concave and \(_{(), _{_{}}}p_{}[]=p_{}[ _{_{}}]\)). The top-left term is the contribution of \(_{*}\) to \(}()\), the bottom-right one its contribution to \(}(\{_{*}\})\) (the decision tree case), so the slack gives a proxy to our potential advantage after splitting. The third difference with decision tree induction is the possibility to converge much faster to a good solution in our case. Scrutinizing the two terms, we indeed get that in the decision tree case, the split only gets two new terms. In our case however, there can be up to \(2(\{(): _{_{}}\})\) new elements in \(()\).

**Boosting** Two questions remain: can we quantify the slack in decrease and of course what quality guarantee does it bring for the _generative_ model \(\) whose set of trees is learned by gf.Boost? We answer both questions in a single Theorem, which relies on a weak learning assumption that parallels the classical weak learning assumption of boosting:

**Definition 5.1**.: _(\((,)\)) There exists \(>0,>0\) such that at each iteration of gf.Boost, the couple \((_{*},)\) chosen in Steps 2.2, 2.3 of gf.Boost satisfies the following properties: **(a)**\(_{*}\) is not skinny: \(p_{}[_{_{}}] 1/( (_{*}))\), **(b)** truth values of \(\) moderately correlates with \(_{}\) at \(_{*}\): \(p_{}[|_{_{}}]-p_{}[ |_{_{}}]\), and finally **(c)** there is a minimal proportion of real data at \(_{*}\): \( p_{}[_{_{}}]/p_{}[ _{_{}}]\)._

The convergence proof of  reveals that both **(a)** and **(b)** are jointly made at any split, where our **(b)** is equivalent to their _weak hypothesis assumption_ where their \(\) parameter is twice ours. **(c)**postulates that the leaf split has empirical measure at least a fraction of its uniform measure - thus, of its relative volume. **(c)** is important to avoid splitting leaves that would essentially be useless for our tasks: a leaf for which **(c)** is invalid would indeed locally model comparatively tiny density values.

**Theorem 5.2**.: _Suppose the loss \(\) is **strictly proper** and **differentiable**. Let \(_{0}\) (= \(\)) denote the initial \(}\) with \(T\) roots in its trees (Step 1, \(}\).Boost) and \(_{J}\) the final \(}\), assuming wlog that the number of boosting iterations \(J\) is a multiple of \(T\). Under \(}(,)\), we get the following on likelihood ratio risks: \(_{}(,_{J})_{} (,_{0})-^{2}}{8} T (1+)\), for some constant \(>0\)._

It comes from [25, Remark 1] that we can choose \(=\{^{}_{-1}-^{}_{1}\}\), which is \(>0\) if \(\) is strictly proper and differentiable. Strict properness is also essential for the loss to guarantee that minimization gets to a good generator (Section 3).

## 6 Experiments

Our code is provided and commented in Appendix, Section V.2. The main setting of our experiments is realistic data generation ('Lifelike'), but we have also tested our method for missing data imputation ('impute') and density estimation ('density'): for this reason, we have selected a broad panel of state of the art approaches to test against, relying on models as diverse as (or mixing elements of) trees, neural nets, kernels or graphical models, with mice, adversarial random forests (ARFs ), CT-GANs , Forest Flows , Vine copulas auto-encoders (VCAE, ) and Kernel density estimation (KDE, [4; 34]). All algorithms _not_ using neural networks were ran on a low-end CPU laptop - this was purposely done for our technique. Neural network-based algorithms are run on a bigger machine (technical specs in Appendix, Section V.2). We carried out experiments on a total of 21 datasets, from UCI , Kaggle, OpenML, the Stanford Open Policing Project, or simulated. All are presented in Appendix, Section V.1. We summarize results that are presented _in extenso_ in Appendix. Before starting, we complete the 2D heatmap of Table 1 by another one showing our models can also learn deterministic dependences in real-world domains (Table 2). The Appendix also provides an example experiment on interpreting our models for a sensitive domain (in Section V.V.3.1).

**Generation capabilities of our models: lifelike** The objective of the experiment is to evaluate whether a generative model is able to create "realistic" data. The evaluation pipeline is simple: we create for each domain a 5-fold stratified experiment. Evaluation is done via four metrics: a regularized optimal transport ("Sinkhorn, ), coverage and density  and the F1 score . All metrics are obtained after generating a sample of the same size as the test fold. Sinkhorn evaluates an optimal transport distance between generated and real and F1 estimates the error of a classifier (a 5-nearest neighbor) to distinguish generated vs real (smaller is better for both). Coverage and density are refinements of precision and recall for generative models (the higher, the better). Due to size constraint, we provide results on one set of parameters for "medium-sized" generative forests with \(T=500\) trees, \(J=\)2 000 total splits (Table 5), thus learning very small trees with an average 4 splits per tree. In the Appendix, we provide additional results on even smaller models (\(T=200\), \(J=500\)

   \(}\)\(\) KDE\(}\) & KDE \(\)\(}\) \\ 
9 & 5 & 3 \\   

Table 3: density: comparison between us and KDE (summary), counting the number of domains for which we are statistically better (left), or worse (right). The central column counts the remaining domains, for which no statistical significance ever holds.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]