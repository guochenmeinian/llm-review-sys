# Model and evaluation groups

_IncognText_: Privacy-enhancing Conditional Text Anonymization via LLM-based Private Attribute Randomization

 Ahmed Frikha &Nassim Walha

**Krishna Kanth Nakka** &Ricardo Mendes &Xue Jiang &Xuebing Zhou

Huawei Munich Research Center

ahmed.frikha1@huawei.com

Equal contribution, alphabetical order

###### Abstract

In this work, we address the problem of text anonymization where the goal is to prevent adversaries from correctly inferring private attributes of the author, while keeping the text utility, i.e., meaning and semantics. We propose _IncognText_, a technique that anonymizes the text to mislead a potential adversary into predicting a wrong private attribute value. Our empirical evaluation shows a reduction of private attribute leakage by more than \(90\%\) across 8 different private attributes. Finally, we demonstrate the maturity of _IncogniText_ for real-world applications by distilling its anonymization capability into a set of LoRA parameters associated with an on-device model. Our results show the possibility of reducing privacy leakage by more than half with limited impact on utility.

## 1 Introduction

Large Language Models (LLMs), e.g., GPT-4 , are gradually becoming ubiquitous and part of many applications in different sectors, e.g., healthcare  and law , where they act as assistants to the users. Despite their various benefits , the power of LLMs can be misused for harmful purposes, e.g., attacks on cybersecurity  and privacy , as well as profiling . For instance, LLMs were found to be able to predict various private attributes, e.g., age, gender, income, occupation, about the text author . Hereby, they achieve a performance close to that of humans with internet access, while incurring negligible costs and time. Such private attributes are quasi-identifiers and their combination can substantially increase the likelihood of re-identification , i.e., revealing the text author identity. This suggests that human-written text data could in some cases be considered as _personal_ data, which is defined as "any information relating to an identified or identifiable natural person" in GDPR . Hence, human-written text might potentially require further analysis and protection measures to comply with such privacy regulations.

Prior works proposed word-level approaches to mitigate text privacy leakage [4; 13]. However, lexical changes do not change the syntactic features which were found to be sufficient for authorship attribution . Another line of work leverages differential privacy techniques to re-write the text in a privacy-preserving way [27; 12], however, with high utility loss. Moreover, while most prior works and current state-of-the-art text anonymization industry solutions  succeed in identifying and anonymizing regular separable text portions, e.g., PII, they fail in cases where intricate reasoning involving context and external knowledge is required to prevent privacy leakage . In light of this and given that most people do not know how to minimize the leakage of their private attributes, methods that effectively mitigate this threat are urgently needed.

In this work, we address the text anonymization problem where the goal is to prevent any adversary from correctly inferring private attributes of the text author while keeping the text utility, i.e., meaning and semantics. This problem is a prototype for a practical use case where data can reveal quasi-identifiers about the text author, e.g., user interaction with online services (ChatGPT) and user content in anonymous social media platforms (Reddit). Our contribution is threefold: First, we propose a novel text anonymization method that substantially increases its protection against attribute inference attacks. Second, we demonstrate the effectiveness of our method by conducting an empirical evaluation with different LLMs and on 2 datasets. Here, we also show that our method achieves higher privacy protection compared to two concurrent works [7; 23]. Finally, we demonstrate the maturity of our method for real-world applications by distilling its anonymization capability into a set of LoRA parameters  that can be added to a small on-device model on consumer products.

## 2 Method

**The threat model** we consider in the present paper is as follows: Under an anonymous pseudonym, the user publishes some text on a platform, e.g., Reddit or Twitter, or sends it as part of a request to an online-service, e.g., ChatGPT. We assume that the adversary is anyone with access to the published text, which includes users, platform admins, and general public, in the case of an online platform, and the service provider in the case of an online-service. Based on the text, the adversary attempts to infer information from the user, particularly their private attributes (profiling) such as, gender, age, location, nationality, etc. We further assume that only this particular text is accessible. This simplification is made to focus the scope of the present work on text anonymization, while leaving out privacy issues stemming from metadata, e.g., IP or fingerprinting. However, we note that our proposed method can be applied to multiple pieces of text, e.g., via a simple concatenation of the text pieces.

**Our approach**, _IncognText_, leverages an LLM to protect the original text against attribute inference, while maintaining its utility, i.e., meaning and semantics, hence achieving a better privacy-utility trade-off. Given a specific attribute \(a\), e.g., age, our method protects the original text \(x^{orig}\) against the inference of the author's true value \(a_{true}\) of the private attribute, e.g., age: 30, by re-writing it to an anonymized version \(x^{anon}\) that misleads a potential privacy attacker into predicting a predefined wrong target value \(a_{target}\), e.g., age: 45. See Fig. 1 for an illustrative example of another private attribute (income level). _IncognText_ is composed of two stages which are executed iteratively (Fig. 1), in a way that mirrors an adversarial training paradigm. Algorithm \(1\) describes our approach.

**In the first stage**, we use an adversarial model \(M_{adv}\), to predict the author's attribute value \(a_{true}\) along with a reasoning \(R\) that supports its inference \(I\), i.e., the predicted value. This is done by using the adversarial inference template \(T_{adv}\) (see Appendix C) and the user text. While in the first anonymization iteration, the user text used is the original text \(x^{orig}\) written by the user, in further iterations, the output of the second stage is used instead, i.e., the anonymized version of the user text \(x^{anon}_{i-1}\) of the iteration \(i-1\) is fed to the adversarial model \(M_{adv}\). It is important to note that

Figure 1: Overview of the _IncognText_ anonymization approach. In this example, the true user attribute value \(a_{true}\) (middle income) is obfuscated via the _IncognText_ anonymization conditioned on a wrong target value \(a_{target}\) (low income) with minimal text changes.

the adversarial model \(M_{adv}\) is different from the model \(M_{attacker}\) that might be used by a potential attacker. The objective of using the adversarial model is to locally mimic the attack that a potential attacker might conduct and leverage the output of the attack to enhance the anonymization step applied in stage 2. We leverage the adversarial model \(M_{adv}\) as a proxy of the potential attacker model \(M_{attacker}\). In our experiments (Section 3), we use different models for the adversarial and attacker models, \(M_{adv}\) and \(M_{attacker}\), respectively.

**In the second stage**, we employ an anonymization model \(M_{anon}\) to further anonymize the user text \(x_{i-1}^{anon}\) of the iteration \(i-1\) by using the anonymization template \(T_{anon}\), a target attribute value \(a_{target}\), and reasoning \(R\) and inference \(I\) generated by the adversarial model, yielding a new anonymized version of the user text \(x_{i}^{anon}\). Our choice to leverage a target attribute value \(a_{target}\) is based on the intuition that misleading a potential attacker into predicting a wrong private attribute value by inserting new hints is more effective than removing or abstracting hints to the original value present in the original text. Hereby, the target value \(a_{target}\) can either be chosen by the user or randomly sampled from a pre-defined set of values for the attribute considered. Besides, we use the reasoning \(R\) and inference \(I\) generated by the adversarial model \(M_{adv}\) to inform the anonymization process, in a way that mimicks adversarial training. Furthermore, we considered further variants of Incognitext where we additionally inform the anonymization model \(M_{anon}\) of the true attribute value \(a_{true}\) (not shown in Fig. 1) to achieve anonymized texts \(x_{i}^{anon}\) particularly tailored to hiding that value. In practice, the true attribute value could either be read from the text author's device, e.g., local on-device profile or personal knowledge graph, or input by the author manually. Nevertheless, _IncognText_ achieves very effective anonymization even without the usage of the true attribute value \(a_{true}\) as demonstrated by our experiments (Section 3).

```
0:\(M_{anon},M_{adv}\): Anonymization and adversary models
0:\(T_{anon},T_{adv}\): Anonymization and adversary prompt templates
0:\(x^{orig}\): Original user text
0:\(a_{target}\): Target attribute value
0:\(a_{true}\): True attribute value of the user (Optional)
0:\(n\): Maximum number of anonymization iterations
1:\(x_{i}^{anon}=x^{orig}\) // Initialize the anonymized text to the original text
2:for\(i=1..n\)do
3:\(I,R=M_{adv}(x_{i-1}^{anon},T_{adv})\) // Get inference I and reasoning R via adversarial evaluation
4:if\(I=a_{true}\)then
5:\(x_{i}^{anon}=M_{anon}(x_{i-1}^{anon},T_{anon},a_{target},a_{true},I,R)\) // Perform an anonymization iteration
6:else
7: break // Early stopping
8:endif
9:endfor
10:Return Anonymized user text \(x_{i-1}^{anon}\) ```

**Algorithm 1** IncognText Anonymization

The iterative application of both stages is executed as long as the inference \(I\) predicted by the adversary model \(M_{adv}\) matches the user true attribute value \(a_{true}\), i.e., the adversarial inference is used as early stopping criterion, or a maximum iteration number is reached. This ensures that we perform as few re-writing iterations as necessary, hence maintaining as much utility as possible, i.e., the original text is changed as little as possible.

Note that the same model \(M\) can be used as \(M_{anon}\) and \(M_{adv}\) with different prompt templates \(T_{anon}\) and \(T_{adv}\) respectively. This is especially suitable for on-device anonymization cases with limited memory and compute. Note that applying _IncogniText_ to multiple attributes can be easily achieved by merging the attribute-specific parts of the anonymization templates (see Appendix for details). For cases where the text author wants to share a subset or none of the private attributes, they can flexibly choose which attributes to anonymize, if any.

**The main difference to prior approaches focusing on direct leakage** of PII or private attributes that are explicitly mentioned in the text  is that _IncogniText_ protects also against _indirect_ leakage. By leveraging the reasoning abilities of the LLM, our approach detects any forms of indirect leakage attribute/PII value, e.g., hints or cues that might be combined with external knowledge to infer the private information, identifies the text spans responsible for the correct PII/attributeinference, and then modifies these text spans to mislead a potential attacker into predicting the target value. Most importantly, these three operations are performed end-to-end implicitly by our LLM-based approach, without training for these steps or separating them conceptually. IncognText adopts a rewriting approach instead of a detect-and-replace approach. Our method can be viewed as performing replacement of the attribute values in the latent attribute space instead of doing so in the text representation space.

## 3 Experimental evaluation

**The datasets** we used to evaluate our approach include a dataset of \(525\) human-verified synthetic conversations proposed by **(author?)** and a dataset proposed in the concurrent work **(author?)** which contains real posts and comments from Reddit with annotated text-span self-disclosures. The former dataset contains 8 different private attributes: age, gender, occupation, education, income level, relationship status, and the country and city where the author was born and currently lives in. For the second dataset, we consider the following attributes: gender, relationship status, age, education, and occupation. We keep only samples where the author discloses information about their own private attributes and not about someone else. Furthermore, we label the samples with the real private attribute values instead of text spans, yielding a set of 196 examples.

**The baseline methods** we compared to in our evaluation include the Azure Language Service (_ALS_) , the differentially private DP-BART-PR+  and the two concurrent works, _Dou-SD_ and Feedback-guided Adversarial Anonymization (_FgAA_) . _DP-BART-PR+_ is a variant of the differentially private method _DP-BART_ that uses the encoder-decoder model BART [**?** ] to rewrite arbitrary input text sequences while providing local differential privacy guarantees for the output text. To provide these guarantees and preserve text utility, the encoder output neurons are first pruned, then the resulting encoder output is clipped by value, and finally, noise is added to the clipped outputs before being sent to the decoder. In our experiments, we use _DP-BART-PR+_ with privacy budgets \(=1000\) and \(=2500\) following the value ranges used by **(author?)**. _Dou-SD_ is an approach that uses an LLM to detect voluntary self-disclosure of private attributes and personal information, then finetunes a model to replace the detected text spans with anonymized versions. _FgAA_ leverages a pretrained LLM to perform the anonymization in an adversarial manner where an adversarial proxy is used to inform the anonymization process, similarly to our method. In the following, we highlight the main differences between this concurrent work and our approach. First, we condition the anonymization model \(M_{anon}\) on a target attribute value \(a_{target}\). We believe that misleading a potential attacker into predicting a wrong private attribute value by inserting new hints is more effective than removing or abstracting hints to the original value present in the original text. Furthermore, we condition the anonymization model \(M_{anon}\) of the true attribute value \(a_{true}\) to increase the quality of the anonymization. Finally, we leverage the adversary model \(M_{adv}\) as an early stopping method to prevent unnecessary utility loss or the deterioration of the anonymization quality, i.e., further anonymization iterations can in some cases lead to a decrease in privacy as observed in the experiments in . Our empirical evaluation and ablation study demonstrate the effectiveness of these contributions.

**The evaluation metrics** we used can be categorizes in privacy and utility evaluation methods. For the privacy evaluation of the anonymized texts, we used an attribute inference attack method which leverages pre-trained LLMs to predict the author attributes based on the text . The privacy metric used is the prediction accuracy of the attacker that uses this method. We use this LLM-based attribute inference attack because it is the SOTA attack for this category and it is the most suitable attack for free-form texts that do not _explicitly_ mention either the attribute of the user or its values. In fact, the attributes and their value are mostly implicitly included in the text or require a complex reasoning combined with external knowledge to be inferred. We argue that such free-form texts are more representative of real-world scenarios and that this privacy evaluation method is what a potential attacker would realistically use in the light of the current literature. Other attacks require training specific attacker models, which requires labeled datasets for each attribute including examples from all classes (the attribute values) [21; 19; 9]. In our privacy evaluation, we do not assume that the attacker model is known and use different adversarial and attacker models, \(M_{adv}\) and \(M_{attacker}\), respectively. We use the strongest LLM in private attribute inference attacks, based on our experiments, as the attacker model \(M_{attacker}\). In our experiments, we set the adversarial model \(M_{adv}\) to always be the same as the anonymization model \(M_{anon}\), which is suitable for low compute environments, e.g., on device anonymization. Regarding the utility evaluation of the anonymized texts, we use the traditional ROUGE score  and the LLM-based utility evaluation with the utility judge template \(T_{utility}\) proposed by **(author?)**. The latter computes the mean of scores for meaning, readability, and hallucinations given by the evaluation model. More details about the experimental setting including the prompt templates can be found in the Appendix C.

**Our main results** are presented in Table 1. We find that _IncogniText_ achieves the highest privacy protection, i.e., lowest attacker inference accuracy, with a tremendous improvement of ca. \(19\%\) compared to the strongest baseline. Note that FgAA uses a stronger anonymization model (GPT-4) suggesting that the improvement might be bigger if we would use the same model with our method. Most importantly, we find that _IncogniText_ substantially reduces the amount of attribute value correctly predicted by the attacker by ca. \(90\%\), namely from \(71.2\%\) to \(7.2\%\). Moreover, our approach achieves high privacy protection across different model sizes and architectures, i.e., Llama 3  and Phi-3 , demonstrating that it is model-agnostic. While the _IncogniText_-anonymized texts yield a high utility, we find that our reproduction of FgAA\({}^{}\) achieves higher utility scores. This is explained by the lower _meaning_ and _hallucination_ scores (the more the model hallucinates, the lower its hallucination score, see Appendix) assigned to _IncogniText_-anonymized texts by the LLM-based utility judge which considers the inserted cues to mislead the attacker as hallucinations. We argue that these changes are desired by the text author and that they are required to successfully fool the attacker into predicting a wrong attribute value. Although DP-BART-PR+ (\(=2500\)) achieves a higher ROUGE score than _IncogniText_, it comes at the cost of providing almost no privacy protection. In fact, the very high privacy budget of 2500 results in almost no changes to the original text. We also note that _IncogniText_ significantly outperforms the strongest baseline FgAA on the second real Reddit comments dataset , reducing the adversarial accuracy by ca. 82%, namely from 73% to 12.8%.

Figure 2 illustrates our private attribute inference accuracy by attribute results. Based on the results of our implementation, which are evaluated using Phi-3-small, we observe that overall _IncogniText_ yields a higher privacy protection, i.e., a lower private attribute inference accuracy, than all other methods across all attributes. One exception is observed for the Occupation (4%), where _DP-BART-PR+_ (\(=1000\)) achieves a private attribute inference accuracy of 0%. However, a closer look at the rewritten output of _DP-BART-PR+_ (\(=1000\)) reveals a complete loss of the text semantics due to excessive noise added, and therefore a complete loss of utility. This is confirmed by the extremely low utility scores in table 1. Moreover, IncogniText reduces at least 90% of the original privacy leakage in all attributes, except for Income Level, where 86% of the original privacy leakage is reduced after rewriting. In contrast, _FgAA_ prevents less than 50% of the original privacy leakage for the attributes

 Method & \(|\) ROUGE Utility \\   \\  Unprotected original text\({}^{}\) & 67 & 100 & 100 \\ Unprotected original text\({}^{}\) & 71.2 & 100 & 100 \\  ALS\({}^{*}\) & 55 & 96 & 64 \\ DP-BART-PR+1000\({}^{}\) & 27.81 & 7.05 & 39.07 \\ DP-BART-PR+2500\({}^{}\) & 62.48 & 93.36 & 91.87 \\ Dou-SD\({}^{*}\) & 47 & 64 & 78 \\ FgAA\({}^{*}\) & 26 & 68 & 86 \\ FgAA\({}^{}\) & 43.2 & 87.9 & 98.8 \\  _IncogniText_ Llama3-70B (ours) & 13.5 & 78.7 & 92.2 \\ _IncogniText_ Llama3-8B (ours) & 15.4 & 78.5 & 91.4 \\ _IncogniText_ Phi-3-mini (ours) & 15.2 & 75.0 & 91.8 \\ _IncogniText_ Phi-3-small (ours) & 7.2 & 80.7 & 92.2 \\   \\  Unprotected & 73.0 & 100 & 100 \\ FgAA\({}^{}\) Phi-3-small & 40.8 & 79.3 & 98.0 \\ _IncogniText_ Phi-3-small (ours) & 12.8 & 72.7 & 87.5 \\ 

Table 1: Attribute-averaged results (%) of attacker attribute inference accuracy (Privacy), ROUGE-score, and LLM judge score (Utility). Results denoted by \({}^{*}\) are reported from **(author?)** where the anonymized texts were evaluated by GPT-4. Results denoted by \({}^{}\) are our reproductions where the anonymized texts were evaluated with Phi-3-small. We also include results for DP-BART-PR+ with privacy budgets \(=1000\) and \(=2500\).

Relationship status, Sex, and Income level, with only a reduction of only 8% in private attribute inference accuracy for the latter.

Finally, we find that _IncognText_ is significantly faster than the baseline, effectively requiring less anonymization steps (Fig. 3). In fact, more than 80% of the test samples are successfully anonymized after a single iteration using our method, while more than half of the samples require a second and possibly a third iteration in FgAA.

Figure 3: Number of anonymization steps required before the adversary predicts the attribute value incorrectly. Average number of steps is 1.3 for _IncogniText_ and 1.9 for FgAA.

Figure 2: Private attribute inference accuracy (%) by attribute for unprotected text and different anonymization methods. The first four bars for each attribute represent accuracy values reported in  and are evaluated using GPT-4 as the privacy evaluation model. The remaining five bars are evaluations from our experiments, performed using Phi-3-small as the privacy evaluation model.

**Our investigation of different IncognText variants** was conducted to gain more insights into the importance of its components. We present its results in Table 2. First, we observe that conditioning the anonymization on a target attribute value is crucial for achieving high privacy protection. Besides, we find that performing early stopping (ES) with the adversary model improves both privacy and utility, since it ensures that no further anonymization steps are conducted that might deteriorate utility or privacy. Moreover, our results suggest that conditioning the anonymization model (Anon) on the attribute ground truth (GT) value is more important than conditioning it on the adversarial reasoning and inference (Inf) for achieving higher privacy. In contrast, conditioning the adversary (Adv) on the GT deteriorates all metrics. We hypothesize that the adversary identifies fewer cues about the author in the text when it has access to GT. Ablation results with other models as anonymization models and Phi-3-small as evaluation model (see appendix) also showcase that conditioning on a target value is the main factor for decreasing privacy leakage. However, their results show no clear trend for the effect of conditioning the anonymization model and the adversary on any other information (GT or Inf). Results that were reported in Table 1 correspond to the \(5^{th}\) experiment from table 2.

**On-device anonymization experiments** were conducted to investigate whether _IncogniText_ can achieve a high privacy protection as part of an on-device anonymization solution. For this, we distill the _IncogniText_ anonymization capabilities of the best anonymization model (Phi-3-small) into a dedicated set of LoRA  parameters associated with a small Qwen2-1.5B model  that could be run on-device. We perform the instruction-finetuning  using additional synthetic conversations released by  that are different than the 525 examples used for testing. The additional examples were not included in the officially released set due to quality issues, e.g., wrong formatting, hallucinations, or absence of hints to the private attributes. We filter and post-process this set of data to solve the issues yielding 664 new examples to which we apply _IncogniText_ to create input-output pairs that we use for finetuning and validation. Post-processing details can be found in the Appendix. We finetune the anonymization model to perform the \(4^{th}\) experiment in Table 2 (anonymization model conditioned on Target and GT). We only fine-tune the anonymization model and use the pretrained version of Qwen2-1.5B for the adversary. The results (Table 3) show a substantial privacy improvement on-device, effectively reducing the private attribute leakage by more than 50%, from 40.8% to 18.1%, while maintaining utility scores comparable to larger models.

In our experiments, we sampled the target attribute value \(a_{target}\) to be different from the real attribute value \(a_{true}\). However, an adversary observing multiple anonymized texts from the same user might accumulate sufficient information to infer the real attribute value (by its absence). Note that this is a different threat model than the one we consider in the present work. To tackle this, we could easily adapt IncogniText to sample the target value \(a_{target}\) uniformly from a the set of all possible attribute values (including the true one). To provide differential privacy (DP) guarantees, the solution can be

 Target & Anon & Adv & ES & Privacy (\(\)) & BLEU & ROUGE & Utility \\   & Inf & uncond & 43.2 & 87.0 & 87.9 & 98.8 \\  & Inf & uncond & 36.0 & 89.1 & 90.0 & 99.0 \\ ✓ & Inf & uncond & 9.5 & 80.8 & 81.3 & 92.6 \\ ✓ & GT & uncond & 7.8 & 77.6 & 78.7 & 92.8 \\ ✓ & GT+Inf uncond & & 7.2 & 80.3 & 80.7 & 92.2 \\ ✓ & GT+Inf & GT & 8.0 & 77.2 & 77.5 & 91.8 \\ 

Table 2: Attribute-averaged results (%) of the ablation study with Phi-3-small as anonymization and evaluation model. Examined components: 1) using the target wrong attribute value \(a_{target}\) (Target), 2) conditioning the anonymization model (Anon) on the inference reasoning of the adversary (Inf), on the ground truth (GT) attribute value, or both, 3) whether to condition the adversary model (Adv) on GT, 4) using the adversary to perform early stopping (ES), i.e., stop the iterative anonymization once it predicts the attribute value incorrectly.

 Model & Privacy (\(\)) & ROUGE & Utility \\  Qwen2-1.5B (pre-trained) & 40.8 & 84.0 & 94.3 \\ Qwen2-1.5B (_IncogniText_-tuned) & 18.1 & 71.1 & 88.2 \\ Phi-3-small & 7.8 & 78.7 & 92.8 \\ 

Table 3: Results (%) before and after instruction-fine-tuning Qwen2-1.5B using the anonymization _IncogniText_-outputs of Phi-3-small.

implemented as the Randomized Response DP-mechanism  where the true target value \(a_{target}\) is sampled with a pre-defined probability. We leave such endeavour for future work.

## 4 Conclusion

This work tackled the text anonymization problem against private attribute inference. Our approach, _IncongiText_, anonymizes the text to mislead a potential adversary into predicting a wrong private attribute value. We empirically demonstrated its effectiveness by showing a tremendous reduction of private attribute leakage by more than \(90\%\). Moreover, we evaluated the maturity of _IncongiText_ for real-world applications by distilling its anonymization capability into an on-device model. In future works, we aim to generalize our technique to include data minimization capabilities.