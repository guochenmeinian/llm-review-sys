# Advancing Video Anomaly Detection:

A Concise Review and a New Dataset

 Liyun Zhu\({}^{1}\)  Lei Wang\({}^{1,2,}\)1  Arjun Raj\({}^{1}\)  Tom Gedeon\({}^{3}\)  Chen Chen\({}^{4}\)

\({}^{1}\)Australian National University, \({}^{2}\)Data61/CSIRO,

\({}^{3}\)Curtin University, \({}^{4}\)University of Central Florida

{liyun.zhu, lei.w, u7526852}@anu.edu.au,

tom.gedeon@curtin.edu.au, chen.chen@crcv.ucf.edu

###### Abstract

Video Anomaly Detection (VAD) finds widespread applications in security surveillance, traffic monitoring, industrial monitoring, and healthcare. Despite extensive research efforts, there remains a lack of concise reviews that provide insightful guidance for researchers. Such reviews would serve as quick references to grasp current challenges, research trends, and future directions. In this paper, we present such a review, examining models and datasets from various perspectives. We emphasize the critical relationship between model and dataset, where the quality and diversity of datasets profoundly influence model performance, and dataset development adapts to the evolving needs of emerging approaches. Our review identifies practical issues, including the absence of comprehensive datasets with diverse scenarios.3 To address this, we introduce a new dataset, Multi-Scenario Anomaly Detection (MSAD), comprising 14 distinct scenarios captured from various camera views. Our dataset has diverse motion patterns and challenging variations, such as different lighting and weather conditions, providing a robust foundation for training superior models. We conduct an in-depth analysis of recent representative models using MSAD and highlight its potential in addressing the challenges of detecting anomalies across diverse and evolving surveillance scenarios. [Project website]

## 1 Introduction

Video Anomaly Detection (VAD) aims to automatically identify unusual occurrences in videos, enabling various applications in surveillance and monitoring . Detecting anomalies is a challenging

Figure 1: A comparison of existing datasets such as UCSD Ped, CUHK Avenue, ShanghaiTech, UCF-Crime, UBnormal and CUVA _vs_. our Multi-Scenario Anomaly Detection (MSAD) dataset.

and complex task due to several factors: (i) There is no unified and clear definition of anomalies of interest.3 For example, the distinction between normal activities like walking on a sidewalk and abnormal activities like walking on a highway is context-dependent. (ii) The sporadic and rare occurrences of anomalies make the collection of well-curated datasets a demanding task, limiting the ability to learn anomalous patterns. Existing methods often treat VAD as either a one-class classification problem or an out-of-distribution detection task . These approaches rely on training solely with normal samples and testing on both normal and abnormal samples, treating anomalies as outliers. Significant progress has been made in VAD during the past few years, thanks to benchmark datasets , as well as synthetic data generation  that presents a range of anomalies. Fig. 1 shows some frames from different datasets.

**Surveys.** Advances in VAD have also led to some comprehensive surveys contributing significantly to the literature . A survey on deep learning models for VAD can be found in . A comprehensive study in  highlights challenging issues in VAD, such as varying environments, the complexity of human activities, the ambiguous nature of anomalies, and the absence of appropriate datasets. A systematic review in  discusses various potential challenges, opportunities, and provides guidance for future research. A recent work  offers a comprehensive benchmark for understanding VAD causation using Large Language Models (LLMs) for text annotations on human-related anomalies. While we focus on static cameras, a survey on VAD in dynamic scenes captured by moving cameras can be found in . Although these surveys are comprehensive, they are not portable and lightweight. Our lightweight review offers several benefits: (i) it provides a quick reference for researchers and practitioners, making it easier to get up to speed on VAD without sifting through extensive details; (ii) it enhances accessibility for a broader audience, including newcomers to the field, by allowing a quick grasp of essential concepts; and (iii) it offers focused guidance by distilling critical information into clear, actionable insights.

**Challenges.** Various studies  indicate that existing methods are often restricted to detecting only a handful of specific anomalies due to (i) a limited amount of videos and (ii) limited camera viewpoints, scenarios and anomaly types per dataset. These methods also frequently suffer from poor generalizability, necessitating retraining for each unique target camera viewpoint or new scenario, which subsequently increases computational costs . Most existing methods  are particularly vulnerable to various factors such as reflection, illumination changes, and complex background environments, leading to frequent false positives and negatives, thereby affecting the precision and reliability of detection. These challenges highlight the need for a high-quality, multi-scenario, and comprehensive dataset. However, existing benchmarks mostly focus on single-scenario (either single or multiple camera viewpoints), human-related anomalies. No existing works explore the multi-scenario generalization abilities of VAD from a practical perspective.

**Motivations & contributions.** Given the aforementioned challenges in VAD, along with practical problems often ignored by the community, we are motivated to compile this lightweight and insightful survey to bridge this gap and provide valuable insights to both interested readers and domain experts. Our survey begins with a discussion of existing VAD methods (Sec. 2.1), highlights the critical relationship between model and dataset development, as well as recent trends and potentials. Our concise review offers fruitful insights between models and datasets (Sec. 2.2), motivating us to contribute a new dataset (Sec. 3), Multi-Scenario Anomaly Detection (MSAD), to advance VAD. We conduct a deep analysis of recent representative methods using our dataset and demonstrate its potential for addressing the challenges of detecting anomalies across diverse and evolving surveillance scenarios (Sec. 4). Finally, we conclude our work (Sec. 5).

## 2 A Concise Review

Our aim is to offer a lightweight reference for researchers and practitioners striving to advance VAD. Below we show the relationship between model and dataset development via a review on VADmethods. A review on VAD benchmark datasets is presented in Appendix G. Table 1 presents a comparison of existing datasets from various perspectives.

### Dataset deficiencies and biases

**From handcrafted to learned features.** Early VAD methods use traditional techniques such as background subtraction, optical flow, and handcrafted feature extraction , relying on appearance, motion, and texture to model motions and crowds . However, these features are often insufficient due to the low resolution of benchmarks and limited training sources , _e.g._, Subway, UMN, UCSD Ped and CUHK Avenue, _etc._ Subsequent works  explore the combination of local/global features, spatial/temporal normalcy, _etc._ Nevertheless, these methods are ineffective when applied to different camera views and cannot adapt to unseen anomalies, when ShanghaiTech (13 camera views) is introduced. Consequently, emerging methods predominantly investigate the use of learned features, eliminating the need for handcrafted features and making them more adaptable to various camera viewpoints and new scenarios . These methods mainly focus on creating new architectures or modules tailored to specific problems, and they can be broadly classified into four categories: reconstruction-based , prediction-based , using classifiers , and scoring , ranging from simple architectures  to complex unified approaches .

**Challenges of learned features.** Most deep learning-based methods still carefully consider several aspects, such as visual appearance and motion, of human action . While these works highlight the importance of appearance and motion in detecting mostly human-related anomalies , non-human-related anomalies remain under-explored due to the lack of solid benchmarks, even when the UCF-Crime dataset (1 non-human-related anomaly) is introduced. Recent works  have delved into creating end-to-end deep learning approaches and unified architectures rather than using separate modules or components in a traditional pipeline, as end-to-end solutions are easily accessible, usable, and deployable. However, deep learning solutions require a significant amount of training data, posing a significant concern, especially considering older and smaller datasets such as UCSD Ped  and CUHK Avenue . Although efforts have been made in collecting large-scale datasets  such as UCF-crime (video- and frame-level annotations for training and testing, respectively), an important issue lies in laborious video annotation, which is one of the main reasons why there haven't been as many large-scale VAD datasets published yet despite tons of data being publicly available on video sharing sites. A recent emerging few-shot learning framework  also encourages researchers to start looking at few-shot VAD models, due to (i) its fast adaptation to novel camera viewpoints/scenarios, (ii) relieving the training data hungry

   Dataset & Year & Source & Domain \#Video \#HRA \#NIRA \#View \#Scenario & Modality & Resolution Variations \\  Subway Entrance  & 2008 & Surveillance Pedestrian & 1 & 5 & - & 1 & 1 & RGB & 512\(\)384 & ✗ \\ Subway Exit  & 2008 & Surveillance Pedestrian & 1 & 3 & - & 1 & 1 & RGB & 512\(\)384 & ✗ \\ UMN  & 2009 & Surveillance Behavior & 5 & 1 & - & 3 & 1 & RGB & 320\(\)240 & ✗ \\ UCSD Ped1  & 2010 & Surveillance Pedestrian & 70 & 5 & - & 1 & 1 & RGB & 238\(\)158 & ✗ \\ UCSD Ped2  & 2010 & Surveillance Pedestrian & 28 & 5 & - & 1 & 1 & RGB & 238\(\)158 & ✗ \\ CUHK Avenue  & 2013 & Surveillance Pedestrian & 35 & 5 & - & 1 & 1 & RGB & 640\(\)360 & ✗ \\ ShanghaiTech  & 2017 & Surveillance Pedestrian & 437 & 13 & - & 13 & 1 & RGB & 856\(\)480 & ✗ \\ UCF-Crime  & 2018 & Online Surv. Crime & 1900 & 12 & 1 & NA & NA & RGB & Multiple & \\ Street Scene  & 2020 & Surveillance Traffic & 81 & 17 & - & 1 & 1 & RGB & 1280\(\)720 & ✗ \\ IITB Corridor  & 2020 & Surveillance Pedestrian & 358 & 10 & - & 1 & 1 & RGB & 1920\(\)1080 & ✗ \\ XD-Violence  & 2020 & Films/Online Violence & 4754 & 5 & 1 & NA & NA RGB+Audio & 640\(\)360 & ✓ \\ UBnormal  & 2022 & 3D modeling Pedestrian & 543 & 20 & 2 & 29 & 8 & RGB & 1080\(\)720 & ✓ \\ NWPU Campus  & 2023 & Surveillance Pedestrian & 547 & 27 & 1 & 43 & 1 & RGB & Multiple & ✗ \\ CUVA  & 2024 & News/Online Multiple & 1000 & 27 & 15 & NA & NA & RGB+Text & Multiple & ✓ \\
**MSAD (ours)** & 2024 & Online Surv. Multiple & 720 & **35** & **20** \(\)**500** & **14** & RGB & Multiple & ✓ \\   

Table 1: Comparisons between our Multi-Scenario Anomaly Detection (MSAD) dataset and existing datasets. Our MSAD is the first large-scale, comprehensive benchmark for real-world multi-scenario video anomaly detection. It has 35 human-related anomalies (HRA) and 20 non-human-related anomalies (NHRA). Our dataset (i) comprises 14 distinct scenarios, including roads, malls, parks, sidewalks, and more (ii) incorporates various objects like pedestrians, cars, trunks, and trains, along with (iii) dynamic environmental factors such as different lighting and weather conditions.

issue, and (iii) its huge potential in real-world applications. Since then, even smaller datasets have proven to be helpful .

**The beauty of few-shot learning.** Few-shot learning aims to adapt quickly to a new task with only a few training samples . One of the most widely recognized methods in VAD is the Few-shot Scene-adaptive Anomaly Detection (FSAD) model . This model uses the meta-learning framework  to train a model using video data collected from various camera views within the same scenario, such as ShanghaiTech. The model is then fine-tuned on a different camera viewpoint within the university site (_e.g._, UCSD Ped and CUHK Avenue). Although the trained model can be adapted to novel viewpoints, its adaptability is still confined to a specific scenario, such as the university street example. The absence of a multi-scenario dataset hampers the widespread application of the rapid adaptation capabilities of VAD models. Recent few-shot VAD methods include .

**Why self-supervised and weakly-supervised?** Traditional supervised learning methods require labeled data, which is often scarce or expensive to obtain for anomalies, even though normal video data are easy to obtain. This is where self-supervised and weakly supervised methods come into play. Anomaly samples are difficult to obtain, and it is challenging to define all types of anomalies (Table 1 shows earlier datasets have very limited anomaly types); therefore, state-of-the-art VAD methods are rarely trained using fully supervised approaches . Self-supervised methods are proposed based on the assumption that any pattern that deviates from the learned normal patterns will be considered anomalies. Representative methods include reconstruction-based , prediction-based , and distance-based  solutions. While previous studies have emphasized the importance of memorizing normal patterns , numerous studies  have indicated that the assumption underlying the self-supervised paradigm is not always valid: (i) It is impractical to obtain all normal types from diverse scenarios with varied distributions (_e.g._, crowded streets _vs._ empty parking lots). (ii) The boundary between normal and abnormal behavior is often ambiguous; even the same abnormal behavior may lead to different detection results under different scenarios. Therefore, weakly supervised VAD has emerged, training on both normal and abnormal samples using video-level annotations, _e.g._, on UCF-Crime. This approach avoids the issue of frame-level or pixel-level annotations, which are time-consuming and labor-intensive; however, video-level annotations only indicate that the video contains anomalies, leaving the exact start and end of the anomaly unclear. Existing methods in this category often use pre-trained models, such as TSN , C3D , I3D , SwinTransformer , _etc._, as an encoder to extract features.

**Expanding modalities for human-related anomaly detection.** Human-related anomaly detection solutions are not limited to the use of RGB videos. To efficiently extract motions, optical flow, precomputed on RGB videos, has been widely used as temporal or motion information for VAD . Due to the heavy computational cost of optical flow and the redundant nature of RGB videos, researchers have started to explore alternative data modalities for efficient feature extraction . Human pose estimation frameworks, like OpenPose , have made human skeleton sequences readily available from RGB videos. This not only introduces a new data modality but also addresses privacy concerns in human-related anomaly detection. The lightweight nature of skeletons  and informative spatio-temporal sequences have fostered many skeletal anomaly detection solutions . For example,  uses 2D human skeleton motions to detect anomalous human behavior in surveillance footage. Recently, LLMs and pretrained video caption models provide rich descriptions and prompts for video contents to assist VAD tasks . Since then, multi-modal datasets have begun to emerge, with representative examples such as CUVA (RGB videos with text descriptions) in 2024.

**Advancements in multi-modal fusion.** A growing number of methods have begun integrating multi-modal information in recent years. Relying solely on optical flows  or skeletons  may not accurately detect anomalies for two main reasons: (i) motions may not reflect relevant anomalies, and (ii) non-human-related anomalies may be treated as background. To address these challenges, several methods have emerged to comprehensively understand anomalies. These include the development of an audio-visual violence dataset and a new fusion method , as well as the exploration of semantic information in VAD . One notable approach is a two-branch setup  (visual and language-visual branches) that utilizes a pre-trained visual-language model (_e.g._, CLIP ). Additionally, the use of knowledge-based prompts as semantic information  and the fusion of text features extracted from a pre-trained video caption model with visual features have been explored. However, compared to single-modality methods, the improvement from these fusion approaches remains somewhat limited, indicating the need for further refinement [74; 13].

### Discussion on model and dataset evolution

**Context-awareness.** Detecting individual objects or actions without considering context can introduce bias or errors, given that anomalies are defined based on their contextual relevance. For example, the NWPU dataset introduced in  is designed to detect scene-dependent anomalies and aims to predict these anomalies in advance, thereby enabling early warnings . The robustness of VAD to various environmental variations such as lighting, weather, and road conditions becomes increasingly crucial for real-world applications. However, most popular benchmark datasets such as UCSD Ped, ShanghaiTech, and CUHK Avenue only simulate anomalies using humans and do not consider diverse and complex environmental conditions. This limitation makes it difficult for well-trained models to effectively detect real-world anomalies. Our dataset accounts for these environment variations, making it better aligned with real-world applications.

**Generalizability.** Existing algorithms often restrict themselves to detecting a handful of specific anomaly types. These models frequently suffer from poor generalizability, necessitating retraining for each specific target scenario or even a particular camera viewpoint, which subsequently increases computational cost. Additionally, many techniques are particularly vulnerable to various external factors such as illumination changes and complex backgrounds (_e.g._, tree swaying and rainy), resulting in frequent false positives or negatives and consequently lowering the precision and reliability of VAD. Collecting and labeling anomalies remains challenging due to their rarity and diversity. While synthetic anomaly data can be generated using game engines across multiple scenarios with precise pixel-level annotations, high-quality real data is still required. Along with the datasets, establishing relevant benchmark evaluation metrics, such as more comprehensive evaluations, allows researchers to compare different methods and drive the development of better-performing algorithms. This motivates us to collect a new comprehensive benchmark dataset with thoughtfully designed evaluation protocols.

**Adaptability and reliability.** VAD faces the challenge of evolving definitions of anomalies over time, even for a specific camera viewpoint. For example, anomalies during the daytime and nighttime could differ, as could anomalies during workdays and weekends, necessitating adaptable and robust detection systems. These systems must continuously adapt to new video signals to maintain long-term reliability and accuracy, whether in single-scenario multiple viewpoints or multi-scenario settings. The robustness of VAD systems is significantly affected by the quality and size of the datasets used in training. High-quality, expansive datasets, combined with advanced algorithms, are essential to address the evolving nature of anomalies in VAD. Our experiments show the potential of our newly introduced dataset in tackling the challenges of detecting anomalies across diverse and dynamic surveillance scenarios. Notably, our dataset includes longer videos that capture the long-term evolution of video signals as well as anomalies.

**Interpretability and privacy concerns.** Below we list several limitations we observe during our review. First, only a few datasets in VAD contain multimodal data, which limits the development

Figure 2: Our MSAD includes a diverse range of scenarios, both indoor and outdoor, featuring various objects, _e.g._, pedestrians, cars, trains, _etc._ The first row shows different real-world common motions, while the second row demonstrates variations in weather and lighting conditions. The third row displays different moving objects. The last column shows human- and non-human-related anomalies.

of relevant multimodal methods. XD-Violence introduced audio information and demonstrated the positive impact of audio-visual fusion. Therefore, exploring multimodal datasets and better fusion strategies that maximize the benefits of each modality could be a promising research direction. We plan to extend our dataset to include more modalities, such as audio and video descriptions. Second, most VAD models adopt a data-driven, end-to-end pipeline. Although effective, the learned features are often not interpretable, which may hinder deployment in real-world applications due to security and safety concerns. Developing explainable and interpretable models provides insights into the underlying reasoning for detected anomalies, making the automated system easier for end users to accept. Lastly, it is crucial to address privacy and ethical concerns associated with VAD, particularly regarding data collection in public spaces for surveillance. Balancing VAD performance while removing personally identifiable information, such as faces, poses, or gaits of individuals, and license plates of vehicles, would be an interesting area of exploration. We leave these for future work. More discussions are provided in Appendix E.

## 3 A Multi-Scenario Dataset

Unlike many datasets with fixed camera viewpoints and limited scenarios, our Multi-Scenario Anomaly Detection (MSAD) dataset boasts a broader range of scenarios and camera viewpoints (refer to Table 1 for a comparison). The process for collecting our dataset is detailed in Appendix A.

**Viewpoints _vs._ scenarios.** Traditional datasets commonly define a scene as the perspective captured by a camera, often referred to as a camera viewpoint. For instance, ShanghaiTech  comprises 13 scenes, representing videos recorded from 13 distinct camera viewpoints at a university. However, relying solely on the'scene' or 'camera view' concept is insufficient for robust anomaly detection. Models trained on multi-view videos per scenario face limitations and struggle to adapt to new scenarios, particularly when confronted with novel camera viewpoints. To overcome this limitation and enhance multi-scenario anomaly detection, we introduce the'scenario' concept to describe different environments. Our dataset encompasses 14 distinct scenarios, including front door areas, highways, malls, offices, parks, parking lots, pedestrian streets, restaurants, roads, shops, sidewalks, overhead street views, trains, and warehouses. Fig. 2 showcases some video frames from these diverse scenarios. As depicted in the figure, our dataset features more realistic scenarios compared to existing benchmarks. It covers a broad spectrum of objects and motions, along with multiple variations in the environment, such as changes in lighting, diverse weather conditions, and more.

**Human-_vs._ non-human-related anomalies.** Previous studies have primarily characterized anomalies as human-related behaviors, encompassing activities like running, fighting, and throwing objects. This emphasis on human-related anomalies stems from their greater prevalence in real-world scenarios. However, enumerating all types of anomalies in diverse real-world contexts poses a significant challenge. To address this, our dataset is further categorized into two principal subsets: (i) Human-related: This subset features scenarios where only human subjects engage in activities, facilitating

Figure 3: The statistics of our MSAD dataset include: (a) a breakdown of main anomaly types and their respective percentages, (b) a boxplot illustrating frame number variations across scenarios in MSAD training set, and (c) the distributions of train/test splits across scenarios for two evaluation protocols (see Sec. 3 evaluation protocols): (_top_ plot) generalizability and adaptability, and (_bottom_ plot) practical applicability and effectiveness.

human-related anomaly detection. For instance, scenarios involve people interacting with various objects like balls or engaging with vehicles such as cars and trains. (ii) Non-human-related: This subset includes scenarios related to industrial automation or smart manufacturing, denoting the use of control systems for operating equipment in factories and industrial settings with minimal human intervention. This subset is designed for detecting anomalies, _e.g._, water leaks, fires, _etc_.

**Dataset statistics.** Figure 3 provides statistics for our MSAD dataset. Our dataset features a wide range of anomalies, including 35 human-related anomalies such as people falling down, school fights, street fights, facility vandalism, and shop robberies, as well as 20 non-human-related anomalies like water leaks, floods, factory fires, trees falling down, and office fires. Figure 2(a) illustrates the proportions of main anomalies in the dataset. Note that we group anomalies such as street fights and school fights into the main category "Fighting" for better visualization purposes. We provide all detailed anomaly types in Appendix B. Our dataset contains 720 videos (447,236 frames in total), with an average video length of 621.16 frames, and some videos extending up to 6,026 frames. The frames are extracted from the original videos at a rate of 30 FPS.

**Evaluation protocols.** To ensure a fair comparison between different algorithms, we provide frame-level annotations during testing stage. More detailed information can be found in Appendix B. We design two evaluation protocols for using our MSAD dataset:

1. [leftmargin=*]
2. Train on 360 normal videos from 14 scenarios and test on the remaining 120 normal videos and 240 abnormal videos. Figure 2(b) shows a boxplot of frame number variations across scenarios in training set. This protocol is suitable for evaluating self-supervised methods.
3. Train on 360 normal and 120 abnormal videos, and test on 120 normal and 120 abnormal videos. During training, we only provide video-level annotations. This protocol is suitable for evaluating weakly-supervised methods trained with our video-level annotations.

The design of these two evaluation protocols aims to facilitate a comprehensive evaluation by considering different models and training schemes. Such a design has not been considered before. Our dataset usage and maintenance are detailed in Appendix C. Figure 2(c) (_top_ plot) shows the train/test split distributions for Protocol i, while (_bottom_ plot) shows the distributions for Protocol ii.

We also apply Protocol i to both Few-shot Scene-adaptive Anomaly Detection (FSAD) and our Scenario Adaptive Anomaly Detection (SA\({}^{2}\)D). A detailed description of our SA\({}^{2}\)D is provided in Appendix D. Our method focuses on evaluating how MSAD can contribute to fast adaptation not only to new camera viewpoints but also to new scenarios in VAD.

## 4 Experiment

In this section, we design two main sets of experiments to explore the capabilities of our MSAD dataset. For generalizability and adaptability evaluation, we use few-shot methods to conduct both cross-view (single-scenario) and cross-scenario evaluations under Protocol i, which helps assess the model's ability to adapt to new viewpoints and scenarios with limited training data. For practical considerations and popularity, we select some representative weakly supervised methods for evaluation using Protocol ii. This protocol evaluates the practical applicability and effectiveness of these methods using our dataset, reflecting common practices in the field. All experiments are conducted using the National Computational Infrastructure (NCI) Gadi, with one V100 GPU allocated for each experiment. Additional evaluations can be found in Appendix F.

### Generalizability and adaptability

**Setup.** We first show the superior performance of our scenario-adaptive model by comparing SA\({}^{2}\)D with the FSAD model  on CUHK Avenue. To demonstrate the enhanced anomaly detection performance of our MSAD dataset, we train two adaptive models: one using ShanghaiTech, and the other leveraging our MSAD. For all experiments, we maintain \(N\!=\!7\) and \(K\!=\!10\) to ensure a fair comparison. Our training process spans over 1500 epochs. Throughout all evaluations, we consider both Micro and Macro AUC scores. We conduct two sets of experiments: (1) In the _single-scenario_ / _cross-view evaluation_, the model is trained and tested on the same scenario. To facilitate a comparison with the view-adaptive anomaly detection model , we partition the ShanghaiTech dataset into 7 scenes (views), specifically, scene 2, 4, 7, 9, 10, 11, and 12 for training as in . Subsequently,the remaining 5 camera views are individually used for testing purposes.4 (2) In the _cross-scenario evaluation_, the model is trained on one scenario and tested on a completely different one. For model training, we use either ShanghaiTech or our MSAD.

**Single-scenario evaluation.** As illustrated in Table 2, our model is trained on seven camera views of the ShanghaiTech dataset and tested on the remaining views, such as \(v1\), \(v3\), and so forth. The performance in cross-view evaluation falls short compared to our SA\({}^{2}\)D. Notably, our SA\({}^{2}\)D, trained on our MSAD and tested on ShanghaiTech, exhibits significantly superior performance compared to the view-adaptive model. This disparity in performance may stem from: (i) the constraint of limited training views, preventing the model from effectively adapting to novel viewpoints, (ii) the camera view being tested on at ShanghaiTech is significantly different, almost equivalent to a novel scenario concept. Therefore, the view-adaptive model is unable to adapt to such novel scenarios.

**Cross-scenario evaluation.** Our model, trained on MSAD, demonstrates superior performance compared to the view-adaptive model (see Table 3, Ped2 and CUHK refer to UCSD Ped2 and CUHK Avenue datasets). For instance, on CUHK Avenue, our model outperforms the view-adaptive model by 9.6% and 6.2% for Micro and Macro evaluation metrics, respectively. Furthermore, our SA\({}^{2}\)D, trained and tested on MSAD, consistently achieves excellent performance. It's important to note that our MSAD test set is distinct from the MSAD training set, covering a diverse range of novel scenarios. These results underscore the robustness of our model in cross-scenario evaluations. However, we also observe a performance drop on ShanghaiTech (\(v6\)). The decline in performance is attributed to the dataset deviating from real-life scenarios, as it categorizes biking and driving as anomalies, a classification that does not align with reality.

**Insights on single- and cross-scenario evaluations.** Based on the aforementioned experiments, we can deduce that a model trained on intricate real-world scenarios exhibits superior generalization. This stems from the fact that real-world models are frequently influenced by the surrounding environment, encompassing elements like fluctuating traffic patterns, dynamic electronic displays, and the movement of trees in the wind. The model must discern the nuances of anomaly detection within a dynamic environment and comprehend the dynamics of objects and/or performing subjects within it. Our MSAD dataset provides a comprehensive representation of real-world scenarios.

**Scene information is implicitly used as weak supervision.** The scene information provided is used in the few-shot sampling strategy, where each scenario is treated as a group for sampling purposes. This allows the model to learn information from multiple scenarios in a balanced manner (see Fig. 6 in Appendix D). In our proposed model, SA\({}^{2}\)D, scene information is used during the sampling process to form \(N\)-way \(K\)-shot learning. Table 2's last four columns compare the performance (i) without scene information (FSAD) and (ii) with scene information (our SA\({}^{2}\)D) on our MSAD dataset (note that both models are trained using MSAD). The primary difference between FSAD and SA\({}^{2}\)D is that SA\({}^{2}\)D uses scene information during sampling to enhance the \(N\)-way \(K\)-shot learning framework. As shown in Table 2, our sampling strategy (Appendix D) significantly boosts SA\({}^{2}\)D's performance across all five test splits on different camera viewpoints in ShanghaiTech.

   &  &  &  & ^{2}\)D (ours)**} \\    & & Micro & Macro & & & Micro & Macro \\  ShT-\(v1\) & & & 61.36 & 55.34 & & 63.74 & 62.92 & **68.96** & **77.89** \\ ShT-\(v3\) & ShT & & 26.51 & 26.58 & & 64.39 & 62.56 & **67.59** & **73.43** \\ ShT-\(v5\) & (7 views) & & 53.40 & 53.32 & **MSAD** & 55.04 & 54.63 & **55.74** & **54.02** \\ ShT-\(v6\) & & & 78.36 & 78.27 & & 70.26 & 71.02 & **75.47** & **72.35** \\ ShT-\(v8\) & & & 50.02 & 52.54 & & 59.97 & 57.45 & **60.85** & **61.52** \\  

Table 2: Experimental results on single-scenario evaluation. On ShanghaiTech (ShT), only 7 views are used during training and the rest views are individually used for testing. The notation ShT-\(v*\) denotes the use of different camera views.

   &  & AUC \\    & & Micro & Macro \\   & Ped2 & 57.38 & 58.36 \\  & CUHK & 69.98 & 78.32 \\  & & **MSAD** & **63.92** & **64.92** \\   & Ped2 & **70.35** & **65.74** \\  & & **79.57** & **84.49** \\   & & **MSAD** & **69.96** & **69.60** \\  

Table 3: Cross-scenario evaluations using FSAD  and SA\({}^{2}\)D (ours) trained on ShT and MSAD.

### Practical applicability and effectiveness

**Setup.** We focus on weakly-supervised learning methods from a practical perspective. We select six recent representative methods for evaluation: MIST , RTFM , MSL , UR-DMU , MGFN , and TEVAD . These methods provide open-source implementations, allowing us to explore the impact of different backbone choices. Each method has its unique focus. MIST introduces a multiple instance self-training framework to refine task-specific discriminative representations using only video-level annotations. RTFM trains a feature magnitude learning function to identify positive instances, enhancing the robustness of the multiple instance learning approach. MSL uses multi-sequence learning to predict both video-level anomaly probabilities and snippet-level anomaly scores. UR-DMU focuses on learning representations of normal data while extracting discriminative features from abnormal data for a better understanding of normal states. MGFN proposes a glance-and-focus network to amplify the discriminative power of feature magnitudes across different scenes. TEVAD uses both visual and text features to complement spatio-temporal features with semantic meanings of abnormal events. These methods enable us to conduct a fair and comprehensive evaluation across various aspects.

In line with the standard practice in [90; 40; 129], we use C3D , I3D , SwinTransformer  pretrained on Kinetics-400, as backbone networks for feature extraction. Although the same backbone is used, different methods may extract features of varying dimensions. For example, UR-DMU and MSL extracts 1024-dim features, whereas other methods extract 2048-dim features when using I3D as the backbone. To ensure a fair comparison, following RTFM, we extract 4096-dim features from the 'fc6' layer of C3D, 2048-dim, 10-crop features from the'mix 5c' layer of I3D, respectively. Additionally, following MSL, we extract 1024-dim features from SwinTransformer. We apply these feature extraction processes on ShanghaiTech, UCF-Crime and our MSAD. Then we use these extracted features as inputs of different methods. We rigorously maintain original parameters to reproduce reported results from previous studies, and conduct our own evaluations using Protocol ii. We use the Micro AUC metric for evaluation.

**Evaluation on methods.** Table 4 shows the results. As shown in the table, there is no single best performer across all three datasets. However, on ShanghaiTech with I3D as the backbone, TEVAD emerges as the top performer. This demonstrates the efficacy of using text descriptions in video anomaly detection. The second-best performer is RTFM, followed by UR-DMU.

**Evaluation on backbones.** For MSL, we observe that SwinTransformer outperforms I3D, and I3D outperforms C3D. Specifically, on UCF-Crime, SwinTransformer as a backbone performs better than using I3D. However, on our MSAD dataset, I3D performs better than SwinTransformer. The potential reason for this behavior is that UCF-Crime generally contains longer videos, and SwinTransformer is good at capturing long-term motions.

**MSAD is sensitive to the choice of backbones.** We notice that our dataset is sensitive to the choice of backbones, particularly for methods like UR-DMU and MGFN (Table 4). Existing backbones, such as C3D, I3D, and SwinTransformer, have demonstrated strong performance in video anomaly detection on current datasets (see Table 4 for results on UCF-Crime and ShanghaiTech). However, these datasets have significant limitations: (i) they cover a very limited range of scenarios, motion dynamics, and anomaly types, and (ii) most anomaly detection methods rely on action recognition pre-trained models as backbones for feature extraction, particularly for human-related anomalies. As a result, these backbones tend to perform well because the anomaly detection datasets predominantly feature human-related motions. In contrast, our dataset introduces more challenges in terms of scenarios,

    & Venue &  &  &  \\   & & C3D & I3D & SwinT & C3D & I3D & SwinT & I3D & SwinT \\  MIST  & CVPR 2021 & 81.40 & 82.30 & - & 93.13 & 94.83 & - & - \\ RTFM  & ICCV 2021 & 83.28 & 83.14 & 83.31 & 91.51 & 97.94 & 96.76 & 86.65 & 85.67 \\ MSL  & AAAI 2022 & 82.85 & 85.30 & 85.62 & 94.23 & 95.45 & 97.32 & - & - \\ UR-DMU  & AAAI 2023 & 82.65 & 86.19 & 83.74 & 94.67 & 96.15 & 95.71 & 85.02 & 72.36 \\ MGFN  & AAAI 2023 & 82.37 & 83.44 & 84.30 & 90.82 & 93.97 & 93.58 & 84.96 & 78.94 \\ TEVAD  & CVPRW 2023 & 83.39 & 84.54 & 84.65 & 92.05 & 98.10 & 97.63 & 86.82 & 83.60 \\   

Table 4: Comparison of six methods with varying backbones on UCF-Crime, ShanghaiTech, and our MSAD dataset using three popular backbones: C3D, I3D, and SwinTransformer (SwinT).

camera viewpoints, and motion dynamics, including both human and non-human-related motions. Overall, using I3D as the backbone yields better results compared to using SwinTransformer. RTFM is robust to the choice of backbones, with the performance gap between I3D and SwinTransformer being within 1%.

Our dataset can be used for selecting appropriate model backbones or exploring more powerful backbone networks that do not overly depend on existing action recognition models. Additionally, our dataset advances video anomaly detection by considering a wider range of scenarios and a broader spectrum of anomaly types.

### Limitations of current methods on our new dataset

Here, we discuss the limitations of current methods and provide additional insights on our MSAD dataset.

**I3D _vs._ SwinTransformer.** Using I3D as a backbone generally outperforms SwinTransformer. Although SwinTransformer excels in capturing local and global spatial features, its reliance on attention mechanisms for temporal modeling may not be as finely tuned to detect subtle anomalies. I3D, designed to maintain temporal consistency across frames, is better suited for detecting anomalies that are temporally localized, _e.g._, sudden object appearances or unexpected behaviors. SwinTransformer's attention-based approach may lack the temporal coherence needed for such tasks. Additionally, I3D's focus on motion and spatiotemporal features aligns well with anomaly detection requirements, especially in motion-based anomalies and subtle temporal variations present in our dataset.

**Boosting existing methods.** Models trained on our dataset exhibit better generalizability and adaptability, particularly under few-shot settings (Table 2), showing superior performance on unseen camera viewpoints. Our SA2D model, trained on MSAD (Table 3), significantly boosts performance. Additionally, our dataset enhances the performance of existing anomaly detection methods (Table 7 in the Appendix). Models trained on MSAD consistently achieve the best performance on anomalies such as fighting, fire, object falling, shooting, traffic accidents, and water incidents, addressing the challenge of lacking a comprehensive benchmark dataset for model training.

**No single best performer on MSAD.** RTFM and TEVAD are more robust to backbone changes than UR-DMU and MGFN (see Table 4). This suggests that existing methods may lack comprehensive evaluations across diverse scenarios. Our dataset provides a solid benchmark for training, testing, and evaluating superior models.

**Supporting multi-scenario anomaly detection.** Our dataset benefits multi-scenario anomaly detection, enabling systematic evaluation across multiple scenarios (Table 8 in the Appendix). Recent methods trained on MSAD generally achieve better performance across all 14 scenarios, demonstrating that existing works have primarily focused on single-scenario detection. Our dataset offers a robust foundation for training superior models for multi-scenario anomaly evaluation.

**Rethinking anomaly detection.** Our dataset encourages rethinking the correctness and trustworthiness of anomaly detection methods. Cross-dataset evaluation (Table 9 in the Appendix) reveals that existing datasets (Table 1) may suffer from unrealistic anomaly definitions. For instance, models pre-trained on MSAD perform poorly on ShanghaiTech but better on CUHK and UCSD Ped2, highlighting the misalignment of anomaly categories like biking and driving with reality.

## 5 Conclusion

In this paper, we provide a concise review and identify several practical issues in video anomaly detection, particularly the lack of comprehensive datasets with diverse scenarios. To address these challenges, we introduce the Multi-Scenario Anomaly Detection (MSAD) dataset. This dataset includes diverse motion patterns and challenging variations, providing a robust foundation for developing superior models. Alongside the dataset, we present our SA2D model, which uses a few-shot learning framework to efficiently adapt to new concepts and scenarios. Experimental results demonstrate the model's robustness, excelling not only in new camera views of the same scenario but also in novel scenarios. Our contributions offer valuable resources and insights to advance the field of video anomaly detection, addressing current challenges and setting the stage for future research directions.