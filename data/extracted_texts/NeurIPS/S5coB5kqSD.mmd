# VeXKD: The Versatile Integration of Cross-Modal Fusion and Knowledge Distillation for 3D Perception

Yuzhe JJ\({}^{1}\), Yijie Chen\({}^{1}\), Liuqing Yang\({}^{1,2}\), Rui Ding\({}^{3}\), Meng Yang\({}^{3}\), Xinhu Zheng\({}^{}\)\({}^{1,2}\)

\({}^{1}\) Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China

\({}^{2}\) Hong Kong University of Science and Technology, Hong Kong SAR, China

\({}^{3}\) Xi'an Jiaotong University, Xi'an, China

{yji755, ychen324}@connect.hkust-gz.edu.cn

lqyang@hkust-gz.edu.cn

dingrui@stu.xjtu.edu.cn

mengyang@xjtu.edu.cn

xinhuzheng@hkust-gz.edu.cn

Xinhu Zheng\({}^{}\)\({}^{1,2}\)

\({}^{1}\) Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China

\({}^{2}\) Hong Kong University of Science and Technology, Hong Kong SAR, China

\({}^{3}\) Xi'an Jiaotong University, Xi'an, China

{yji755, ychen324}@connect.hkust-gz.edu.cn

lqyang@hkust-gz.edu.cn

dingrui@stu.xjtu.edu.cn

mengyang@xjtu.edu.cn

Corresponding authors.

###### Abstract

Recent advancements in 3D perception have led to a proliferation of network architectures, particularly those involving multi-modal fusion algorithms. While these fusion algorithms improve accuracy, their complexity often impedes real-time performance. This paper introduces **VeXKD**, an effective and **V**ersatile framework that integrates **Cross-**M**odal Fusion with **K**nowledge **D**istillation. VeXKD applies knowledge distillation exclusively to the Bird's Eye View (BEV) feature maps, enabling the transfer of cross-modal insights to single-modal students without additional inference time overhead. It avoids volatile components that can vary across various 3D perception tasks and student modalities, thus improving versatility. The framework adopts a modality-general cross-modal fusion module to bridge the modality gap between the multi-modal teachers and single-modal students. Furthermore, leveraging byproducts generated during fusion, our BEV query guided mask generation network identifies crucial spatial locations across different BEV feature maps from different tasks and semantic levels in a data-driven manner, significantly enhancing the effectiveness of knowledge distillation. Extensive experiments on the nuScenes dataset demonstrate notable improvements, with up to 6.9%/4.2% increase in mAP and NDS for 3D detection tasks and up to 4.3% rise in mIoU for BEV map segmentation tasks, narrowing the performance gap with multi-modal models.

## 1 Introduction

3D perception, encompassing 3D object detection  and BEV map segmentation , is crucial for understanding 3D scenes and controlling autonomous vehicles . Achieving high accuracy and real-time performance simultaneously presents the desirable yet challenging pursuit in this field . Many studies  focus on multi-modal fusion, particularly the fusion of LiDAR and multi-view cameras, to enhance perception accuracy. These methods necessitate handling additional input data, inevitably employ more complex networks, and extend inference time. Conversely, some studies  continue to focus on single-modal algorithms to maintain system simplicity and enhance accuracy with better training strategies and data pipelines.

Cross-modal Knowledge Distillation (KD)  has emerged as a promising strategy for transferring insights across modalities, using multi-modal models as teachers and single-modal models as students. Cross-modal KD can thus improve the accuracy of student models without incurringadditional inference time. However, cross-modal KD faces significant challenges, including capacity discrepancies between the student and teacher models, and information gaps stemming from the different input modalities. Xue _et al._ propose and tentatively validate that teacher models focusing on modality-specific information can exacerbate the modality gap between student, reducing the effectiveness of cross-modal KD. In contrast, teacher models that base decisions on modality-general information can minimize disparities with single-modal students, facilitating a more effective cross-modal KD process.

However, the current research paradigm, which separates cross-modal fusion from KD, limits potential synergies. On one hand, multi-modal fusion research [24; 62; 28] has become performance-oriented. Despite achieving impressive results on benchmarks like nuScenes , BEVFusion  stands out as the state-of-the-art fusion method. However, the visualization of fusion and LiDAR feature maps in BEVFusion , and the significant performance drops under conditions like LiDAR failure as depicted in Fig. 1, reveal an over-reliance on LiDAR-specific information rather than on the general information of multi-modal features. This makes these high performance fusion methods less suitable as teachers in cross-modal KD. On the other hand, research on cross-modal KD in 3D perception [68; 7] seldom considers the selection of teachers, often directly using a LiDAR or fusion model with higher accuracy, such as BEVFusion , as the teacher , which potentially limits the effectiveness of cross-modal KD.

The versatility of KD algorithms is another facet often overlooked in previous research, yet it significantly impacts the vitality and breadth of KD applicability, especially in a rapidly evolving field like 3D perception. If a KD algorithm cannot be directly applied to new student algorithms, its re-implementation may require numerous difficult-to-generalize empirical engineering decisions and result in an unpredictable performance drop. The versatility of a KD algorithm primarily hinges on two aspects. Firstly, it depends on whether the KD algorithm is tied to the processing steps or feature space of a specific modality. For instance, MonoDistill  projects LiDAR points onto a perspective view and uses the same model architecture as the camera, serving as depth supervision for camera branches but is not applicable to LiDAR students. In contrast, using the BEV space, which is representation-friendly to different modalities, alleviates this limitation. Secondly, and most importantly, the versatility of a KD algorithm depends on its compatibility with specific network architectures, especially task-specific heads, which exhibit the greatest design variations across different 3D perception algorithms and downstream tasks. These can range from dense heads [55; 33; 37; 34; 30] and transformer-based heads [2; 45; 58; 27] to segmentation heads [72; 46; 49; 67]. This diversity limits the versatility of KD methods that rely on response distillation across different perception tasks and various detection algorithms. Consequently, previous 3D perception KD methods often require identical heads and similar model architectures for both student and teacher models, and are generally limited to a single downstream task, primarily 3D object detection.

The current reliance on response distillation primarily stems from the underutilization of the rich information available in the teacher's feature maps. Previous efforts have focused on the ground truth locations, using Gaussian masks [68; 7] or sampling a few points within these areas . These methods tend to overlook the valuable background information, which has proven useful in 2D perception [13; 61]. Moreover, by confining feature distillation solely to ground truth locations, these methods not only fail to account for variations in spatial perceptual fields across different levels of BEV feature maps but also struggle to generalize to tasks requiring dense supervision, such as map segmentation. This reduce the versatility of their KD methods.

To address the aforementioned challenges, we propose an effective and **v**ersatile framework that integrates **Cross**-Modality Fusion and **K**nowledge **D**istillation within the BEV feature space. This framework effectively narrows the modality gap between the teacher and different single-modal student models by training a modality-general fusion teacher. Moreover, we leverage the learned BEV query, the byproduct of the fusion process, to guide a mask generation network in creating unique

Figure 1: (Left) Visualization of BEVFusion feature maps before and after fusion, indicating minimal information gain. (Right) Significant performance degradation in the LiDAR-missing scenario, highlighting the over-reliance on LiDAR features.

spatial masks for different feature levels and tasks in a data-driven manner. These learned masks significantly enhance the effectiveness of feature distillation by enabling the selective filtration and transfer of valuable information from the teacher's feature maps. Furthermore, our KD framework is independent of specific processing steps or network architectures, making it versatile to be applied across any downstream 3D perception task and adaptable to various student modalities, as well as future advancements in this field.

In summary, our contributions are summarized as follows:

* We present an early effort to integrate cross-modal fusion and KD in 3D perception, enhancing the efficacy of cross-modal KD with the modality-general fusion teacher and the fusion byproduct.
* We pioneer data-driven spatial masks learning for feature distillation on BEV feature maps, selectively transferring only beneficial feature information by applying distinct masks tailored to different BEV feature maps.
* Our KD approach is designed to be task- and modality-agnostic, making it highly versatile for any BEV-based 3D perception task and adaptable to various student modalities.

## 2 Related Work

### LiDAR-camera Fusion

LiDAR and camera are the two most common sensors used in 3D perception. Recent advancements in both camera-based [32; 16; 23; 25; 53] and LiDAR-based [69; 51; 19; 55] methods have achieved notable results, providing a solid foundation for LiDAR-camera fusion. Since the BEV space offers a feature space that is friendly to both modalities and can easily be applied to various downstream tasks, fusion research based on BEV space has also become a trend [26; 24; 42; 28]. In BEV-based perception methods, both single-modal and fusion models [32; 25; 19; 28] initially transform inputs from LiDAR or cameras into the BEV space using modality-specific encoders. Then only the fusion models being integrated through a fusion module, but finally a BEV encoder and task-specific heads are applied to all models for perception tasks. Although the abundance of research related to BEV poses challenges in selecting appropriate student and teacher architectures for KD, the common paradigm in BEV-based work offers the potential for a unified KD framework.

### Knowledge Distillation

Knowledge distillation (KD) facilitates the efficient transfer of implicit knowledge from the teacher to the student without increasing the student's inference time [3; 14]. Besides aligning the teacher's soft outputs with the student's, some works indicate that mimicking the feature map can also boost performance [41; 21; 43; 56]. Direct distillation over the entire feature maps can potentially degrade performance due to the noisy teacher feature maps. Based on this observation, some approaches have resorted to attentive distillation, concentrating KD on the less noisy foreground features and using ground truth as a mask to guide the process [35; 57]. On the other hand, recent studies [13; 61] emphasize the value of background information and suggest decoupled KD processes for both foreground and background features. DistillBEV  advances this concept by decomposing the regions of feature maps and enhance attention to false positive regions. Recently, inspired by pretext tasks in large language models, generative distillation has been proposed . However, random masks utilized in generative distillation can destabilize the performance, especially in 3D object detection with pronounced foreground-background imbalance. Our approach still aligns more closely with attentive distillation to selectively transfer knowledge from teacher's feature map.

Another key research focus is the efficacy of KD, as explored by Cho _et al._. Studies indicate that a high-accuracy teacher model does not necessarily improve KD results. Similarly, in cross-modal KD scenarios, Xue _et al._ investigate factors influencing KD's effectiveness. They reveal that if a teacher makes decisions based on modality-general features, KD performance can be improved even when the teacher's accuracy is not superior. Recently, Huang _et al._ develop a "vision-centric" multi-modal teacher, reducing reliance on LiDAR to align more closely with camera-based students. Our work aims to develop a modality-general fusion model without modifying the pipeline of the teacher network.

Due to spatial inaccuracies in RGB images, numerous studies have used KD to improve the accuracy of camera models. BEV-LGKD  uses LiDAR to enhance the camera-based students' depth estimation but it is limited to LSS-based  projection methods, excluding certain camera students like BEVFormer . BEVDistill  introduces a sparse instance distillation method coupled with transformer-based detection heads, which is unsuitable for common dense detection heads . Compared to camera student models, cross-modal KD applied to LiDAR students is relatively less explored. Zheng _et al._ employ the PointPainting  model as a teacher, providing supervision only for voxel-based LiDAR students during the voxelization process. Undistill  supports various student-teacher modality combinations, but its response distillation is limited to dense detection heads, and feature distillation depends on 3D detection ground truths, restricting its use in tasks like BEV map segmentation. To date, no universal KD paradigm covers diverse 3D perception tasks and student modalities.

## 3 Methodology

### Overall architecture of Cross-Modal Fusion and Knowledge Distillation Framework

The overall architecture of the proposed framework is depicted in Fig. 2. The modality-general fusion module operates on the low-level BEV feature \(F_{modal}^{low}\), capturing high spatial granularity and rich semantic information from different modalities. Enhanced by the

Figure 3: **Illustration of BEV query guided mask generation and masked feature distillation.** (a) Overview of the transformer-based block for mask generation, which adopts the byproducts from the fusion module as the BEV query. (b) In the deformable cross attention operation, the BEV query interacts with the teacher feature maps to identify crucial spatial locations. (c) In the masked feature distillation stage, learned spatial masks are applied to both teacher and student feature maps before calculating the distillation loss.

Figure 2: **Overall architecture of VeXKD:** Building upon the common BEV fusion pipeline, we tailor a Modality-General Fusion Module and design a masked feature distillation method with learned masks assisted by the byproduct of the fusion module, applied across both low-level and high-level BEV features. Our feature distillation framework circumvents variations in different model architectures, making it modality- and task-agnostic.

module, masked feature distillation is applied to both low and high-level BEV features to selectively mimic the semantic information from the feature maps of the fusion teacher.

### Modality-General Fusion Module (MGFM)

The architecture of the fusion module is shown in Fig. 4. Inspired by BEVFormer , we employ deformable attention  as the central mechanism of our fusion module. As detailed in Eq. 1, \(q,p,x\) represent the query, reference point, and input features, respectively. The number of attention heads and the number of key points sampled per head are denoted by \(N_{head}\) and \(N_{key}\). \(_{i}^{}\), and \(_{i}\) are learnable parameters. \(A_{ij}\) and \( p_{ij}^{2}\) represent the predicted weight and the offset relative to the reference point \(p\) for these sampled key points, both are learned from \(q\). Deformable attention enables learnable sampling offsets, which provide a larger and more adaptable receptive field while maintaining computational efficiency. It is ideal for correcting varying degrees of spatial misalignments caused by ill-posed view projections and fickle sensor extrinsic matrices. Our fusion module adheres to the traditional transformer block structure, incorporating deformable cross-modal attention and query self-attention operations to iteratively fuse multi-modal BEV features.

During the deformable cross-modal attention operation, a set of learnable dense BEV queries \(Q^{(H W) C}\) is initialized. These queries interact with low-level feature maps from each modality \(F_{modal}^{low}\), for \(modal\{lidar,camera\}\). The sampling offset generation process is modified by concatenating \(F_{modal}^{low}\) with the query \(Q\) to produce modality-specific sampling offsets \( p\) and attention weights \(\). The resulting feature map is updated by summing the outputs, as denoted in Eq. 2.

This symmetrical and equitable fusion approach forces the preservation of the information from each modality. It promotes the extraction of modality-general information from the fusion features, rather than disproportionately relying on a specific modality, thereby narrowing the information gap between the fusion teacher and individual modalities. In the deformable query self-attention operation, modality-specific feature inputs are substituted with the query \(Q\) itself, as indicated in Eq. 3. This self-attention operation not only expands the receptive field but also capture the inter-correlations among BEV features, making the fusion features geometrically more accurate and semantically richer. After stacking 6 transformer blocks similar to BEVFormer, the MGFM module yields the fusion result \(F_{fusion}^{low}\).

\[(q,p,x)=_{i=1}^{N_{head}}_{i}_{j=1}^{N_{ key}}_{ij}_{i}^{}x(p+ p_{ij})\] (1)

\[Q^{}_{modal}=[Q;\,F_{modal}^{low}]; Q=_{modal}(Q^{}_{modal},p,F_{modal}^{low})\] (2)

\[Q=(Q,p,Q)\] (3)

Figure 4: **The architecture of the Modality-General Fusion Module**: (a) Overview of the transformer-based block. (b) Deformable cross-modal attention operation: the BEV query symmetrically interacts with features sampled from both LiDAR and camera. (c) Deformable query self-attention operation: the BEV query interacts with itself to integrate correlational relationships.

### BEV Query Guided Mask Generation

Due to the inherent differences among various downstream tasks and feature levels, the specific areas within the teacher feature map that require the student's focus can vary. For example, in detection tasks, information near ground truth positions is crucial for the high-level BEV features. However, since high-level features selectively aggregate a broader range of low-level features, important low-level features may be more widely distributed, not just at ground truth positions. Based on these insights, we opt to learn ad hoc masks in a data-driven manner. Nevertheless, efficiently learning these masks rapidly necessitates a set of initial parameters that are easy to generalize. Once trained within the fusion module, the learned BEV query \(Q\) can effectively extract valuable information from the BEV feature maps, facilitating quick adaptation to ad hoc spatial masks. To maximize the effectiveness of \(Q\), we employ the deformable cross-attention operation in the mask generation network, similar to the setup in the fusion module as depicted in Fig. 3(a). Here, \(Q\) and the teacher BEV feature map \(F_{level}^{fusion}\), where \(level\{low,high\}\), serve as the \(q\) and \(x\) for DeformAttn, respectively, allowing for continuous updates to \(Q^{level}\). Finally, the channel dimension is reduced to 1 using a \(1 1\) convolution, and the values interval for the learned spatial mask \(M_{level}\) is adjusted to range (0,1) through a sigmoid activation function.

In designing the mask generation loss, inspired by , the original teacher feature map \(F_{fusion}^{level}\) is substituted with the masked version \(_{fusion}^{level}\), obtained by applying Hadamard product operation \(\) between the teacher feature and the learned spatial mask, as shown in Eq. 4. This masked feature serves as input to derive the final masked teacher loss for downstream tasks \(L_{masked}^{task}\). Minimizing this loss ensures the preservation of valuable features in the teacher feature map through the learned mask.

However, using only \(L_{masked}^{task}\) as the mask learning loss can result in an all-ones mask. We also incorporate the feature distillation loss \(_{masKD}\) into the objective to stabilize the mask generation process. This inclusion allows the student feature map to participate, helping avoid optimization pitfalls that arise from hard-to-mimic locations due to modality gaps. The final mask generation loss \(L_{mask\_gen}\) is depicted in Eq. 5, where \(\) is a factor to balance the scale of the two losses.

\[_{fusion}^{level}=M^{level} F_{fusion}^{level},\,level\{low,high\}\] (4)

\[_{mask\_gen}=_{masked}^{task}+_{masKD}\] (5)

### Masked Feature Distillation

After applying the learned mask to both the student and teacher feature maps as depicted in Eq. 6 and Fig. 3(c), various loss functions can be applied to quantify the discrepancies between them. Finally, Attention Transfer  is adopted to compute the KD loss between the student and teacher. This method effectively mitigates the adverse impacts of channel-wise heterogeneity that arise from different architectures. By applying L2-norm in Attention Transfer as shown in Eq. 7, greater emphasis is placed on spatial locations with higher activations or more discriminative features, thereby enhancing the robustness and effectiveness of feature distillation.

We apply mask generation and attention transfer across both low- and high-level BEV features. The mask generation process, which also incorporates \(_{masKD}\), runs concurrently with the KD process. It can be halted after a certain number of epochs using a controllable hook. The overall loss for the student model \(_{overall}^{student}\) (see Eq. 8) combines the downstream task loss \(_{task}^{student}\) with KD loss \(_{masKD}\), using \(\) to balance the magnitude of these losses.

\[Q^{level}()=(_{sum}(M^{level} ))\] (6)

\[_{masKD}^{level}=\|(F_{student}^{level})}{\|Q^{ level}(F_{student}^{level})\|_{2}}-(F_{fusion}^{level})}{\|Q^{level}(F_{fusion}^{level}) \|_{2}}\|_{2}\] (7)

\[_{masKD}=_{level\{low,high\}}_{masKD}^{level}, _{overall}^{student}=_{task}^{student}+ _{masKD}\] (8)Experiments

### Experimental Setup

The versatility of the KD framework is verified by adopting both camera and LiDAR student modalities, and by evaluating on two 3D perception tasks: 3D object detection and BEV map segmentation. Notably, our framework can be easily extended to support additional student modalities such as radar and event-based cameras, and to support other downstream tasks, including 3D object tracking and motion prediction.

Dataset & Evaluation MetricsThe training and evaluation are conducted using the nuScenes dataset, a large-scale dataset under the CC BY-NC-SA 4.0 license comprising 1,000 driving sequences (700/150/150 for train/val/test). This dataset provides diverse annotations and sensor data, featuring six monocular camera images along with a 32-beam LiDAR system, making it ideal for assessing our method across various tasks and modalities. We employ evaluation metrics that are widely adopted by state-of-the-art methods. For 3D object detection, we adopt the mean Average Precision (mAP) along with the nuScenes detection score (NDS), the official evaluation metrics of the nuScenes dataset . For BEV map segmentation, following BEVFusion  we use the class-averaged mean Intersection-over-Union (mIoU) across six background classes (drivable space, pedestrian crossing, walkway, stop line, car-parking area, and lane divider) as the evaluation metrics.

Models & Evaluation ConfigurationWe use MMDetection3D  and MMRazor  under the Apache License 2.0 to implement the fusion module and the knowledge distillation framework. We replace BEVFusion's  fusion module with our Modality-general Fusion Module to serve as the teacher, keeping all other settings identical to BEVFusion. To verify the versatility of our framework, we conduct KD on various representative student models, including the CenterPoint LiDAR student model, BEVDet with a ResNet-50 image backbone, and BEVFormer-S (without temporal fusion) as the camera student models. Additionally, we conduct cross-modal KD experiments on BEVDet4D-Depth student model, which adopts the long-term temporal fusion operation. We maintain the same model architectures and data pipeline as original student models. For the segmentation task, we simply replace the detection head with the BEVFusion  segmentation head for binary segmentation across all classes. The teacher model utilizes a complex DETR detection head , contrasting with the student models' simpler dense detection heads. This helps us assess the KD method's robustness against heterogeneity in model architecture, besides input modality. Detailed experiment settings can be referred to appendix. B.

### Comparison with the State-of-the-Arts

Table 1 presents the comparative results. We choose representative and influential single-modal algorithms as our baseline student models, including CenterPoint  for LiDAR, BEVDet-R50 , and BEVFormer-S  for cameras. In 3D object detection, our framework achieves significant improvements over these baseline models. Our simple LiDAR student performs comparably to those of single-modal state-of-the-art methods, such as TransFusion-L  and BEVFusion-L , which employ a more complex DETR detection head, thereby narrowing the performance gap with more sophisticated multi-modal fusion methods. Additionally, our simple BEVDet-R50 student outperforms FCOS3D  and BEVFusion-C , which use heavier image backbones. Our method also exceeds the performance of state-of-the-art cross-modal KD methods, including UniDistill  and BEVDistill , both of which involve elaborate response KD. This underscores the effectiveness of our feature distillation approach, supported by a superior teacher model and learned spatial masks. For BEV map segmentation, our method also shows significant improvements compared to the baselines, validating the task-independence of our integrated fusion and cross-modal KD framework. Additionally, based on the performance comparison with VCD  in the 3D object detection task, we observe that while our NDS is slightly lower than that of VCD, this may be due to VCDs use of fine-grained trajectory-based distillation, which provides greater advantages in predicting object motion and velocity. Nevertheless, our method achieves comparable results to VCD in terms of mAP, demonstrating that the multi-sweep LiDAR information in the teacher model used by VeXKD significantly enhances the student's localization capabilities.

Furthermore, the comparison of giga floating-point operations (GFLOPs) and inference time clearly illustrates the performance and real-time trade-off introduced by cross-modal KD methods, particularly with our VeXKD on the student model.

### Ablation Studies

In this section, we validate the effectiveness of each module. Given the diversity of modalities and tasks applicable to our method, some ablation studies are conducted on specific modality-task combinations due to computational constraints.

Effectiveness of Each Proposed ModuleIn this paper, we introduce three modules: the Modality-General Fusion Module (MGFM) and Masked Feature Distillation modules for both Low- and High-level BEV features (L-MFD and H-MFD). Configurations without MGFM use the original BEVFusion as teachers. We conduct experiments on CenterPoint detection and BEVDet-R50 segmentation students to evaluate the impact of these modules on different student modalities and tasks. Results in Table 2 indicate each module positively contributes to the effectiveness of the KD, with MGFM showing the most substantial impact. This confirms the previously overlooked critical role of the teacher model in cross-modal KD settings.

Effectiveness of BEV Query Guided Mask Generation NetworkTo validate the effectiveness of the learned spatial mask, comparisons are made with state-of-the-art masking methods on the CenterPoint student model: complete feature mimic (all 1's mask) , key point sampling , Gaussian masks centered on ground truth , and masks derived from teacher's normalized activation map statistics . In map segmentation, key point sampling and Gaussian masks, which rely on ground truth, are inapplicable. Table 3 shows minimal improvements from complete fea

    &  &  &  &  &  \\   & & & & mAP & NDS & mAP & NDS & mIoU \\  TransFusion & L+C & 972 & 1.8 & 68.9 & 71.6 & \(67.5\) & \(71.3\) & – \\ BEVFusion & L+C & 507 & 2.3 & 70.2 & 72.9 & \(68.5\) & \(71.4\) & \(62.9\) \\   TransFusion-L & L & 340 & 5.8 & 65.5 & 70.2 & \(65.1\) & \(70.1\) & – \\ BEVFusion-L & L & 322 & 5.4 & – & – & \(64.7\) & \(69.3\) & \(48.6\) \\  CenterPoint & L & 308 & 11.3 & 60.3 & 67.3 & \(57.4\) & \(65.6\) & \(48.6\) \\ +S2M2-SSD  & L+C \(\) L & 308 & 11.3 & 63.6 & 69.6 & – & – & – \\ +Unidistill  & L+C \(\) L & 308 & 11.3 & 63.9 & 70.1 & \(59.7\) & \(67.5\) & – \\ +VeXKD(Ours) & L+C \(\) L & 308 & 11.3 & **65.1** & **70.5** & **64.2** & **69.6** & **52.1** \\   FCOS3d & C & 2008 & 1.7 & 34.3 & 41.5 & \(29.5\) & \(37.2\) & – \\ BEVDet-Tiny & C & 370 & 6.3 & – & – & \(33.3\) & \(41.0\) & \(56.8^{*}\) \\  BEVDet-R50 & C & 184 & 14.0 & 28.9 & 38.4 & \(28.6\) & \(37.2\) & \(56.4^{*}\) \\ +Unidistill & L+C \(\) C & 184 & 14.0 & 29.6 & 39.3 & – & – & – \\ +VeXKD(Ours) & L+C \(\) C & 184 & 14.0 & **35.8** & **42.6** & **34.7** & **40.6** & **60.7** \\  BEVFormer-S & C & 1152 & 2.6 & 40.9 & 46.2 & \(37.5\) & \(44.8\) & \(61.8^{*}\) \\ +Unidistill & L+C \(\) C & 1152 & 2.6 & – & – & \(37.7^{*}\) & \(45.5^{*}\) & – \\ +BEVDistill & L \(\) C & 1152 & 2.6 & – & – & \(38.6\) & \(45.7\) & – \\ +VeXKD(Ours) & L+C \(\) C & 1152 & 2.6 & **42.5** & **48.3** & **41.2** & **47.7** & **64.2** \\  BEVDet4D-Depth & C & 220 & 12.3 & – & – & \(39.4\) & \(51.5\) & \(61.6^{*}\) \\ +VCD & L+C & 220 & 12.3 & – & – & \(42.6\) & **54.0** & – \\ +VeXKD(Ours) & L+C & 220 & 12.3 & – & – & **42.8** & \(53.5\) & **63.5** \\   

Table 1: **Performance of VeXKD on nuScenes for 3D object detection and BEV map segmentation tasks. “L” and “C” denote the LiDAR and the Camera modality, respectively. “L+C” denotes multi-modal fusion model. “L+C \(\) C”, “L \(\) C”, and “L+C \(\) L” represent the knowledge distillation from the teacher model to the respective single-modal student.“+” indicates the addition of cross-modal KD methods to the above student models. The FPS results are evaluated on GTX 4090 GPU with batch size of one. “*” denotes our re-implementation results. No test-time augmentation is applied during testing.**ture mimic, underscoring the necessity for feature filtering through masking. Gaussian masks and key point sampling focus on limited foreground points for all levels of features, result in underutilized feature distillation. Using normalized activation statistics as masks, assuming that regions with higher teacher activation contain critical information, shows a correlation but not equivalence. Meanwhile, the varying performance across tasks underscores the necessity of learning specialized masks for a versatile KD framework. Additionally, comparisons between Mask Generation with Randomly Initialized Queries (MGTIQ) and Mask Generation with Learned Queries (MGLQ) demonstrate the benefits of using fusion byproducts in mask learning.

Effectiveness of Attention TransferWe use attention transfer (ATTN)  to compute the feature distillation loss, as it helps alleviate the channel dimension heterogeneity from differing architectures. Table 4 presents a comparison of results using attention transfer versus traditional loss L1, L2, and Smooth L1 on CenterPoint student model, each implemented with a simple convolutional adaptive layer. Although employing MGFM and learned masks renders the choice of specific distillation loss less critical, attention transfer consistently outperforms other losses across tasks.

    &  \\ 
**Method** & **LiDAR Det.** & **LiDAR Seg.** \\  No KD & 57.4/65.6 & 48.6 \\ L1 & 62.8/69.0 & 51.3 \\ L2 & 63.7/69.2 & 51.6 \\ Smooth-L1 & 63.5/69.3 & 51.7 \\ ATTN & 64.2/69.6 & 52.1 \\   

Table 4: **Ablation study of different loss functions in feature distillation on nuScenes val.** The choice of specific distillation loss is less critical while attention transfer performs better.

    &  &  \\ 
**Setting** & **MGFM** & **L-MFD** & **H-MFD** & **LiDAR Detection** & **Camera Segmentation** \\ 
1 & & & & 57.4/65.6 & 56.4 \\
2 & & ✓ & & 59.2/66.8 (+1.8/1.2) & 57.6 (+1.2) \\
3 & & & ✓ & 58.0/66.1 (+0.6/0.5) & 57.9 (+1.5) \\
4 & & ✓ & ✓ & 59.8/66.9 (+2.4/1.3) & 58.3 (+1.9) \\
5 & ✓ & ✓ & & 62.8/68.9 (+5.4/3.4) & 59.2 (+2.8) \\
6 & ✓ & & ✓ & 61.9/68.5 (+4.5/2.9) & 59.6 (+3.2) \\
7 & ✓ & ✓ & ✓ & 64.2/69.6 (+6.8/4.0) & 60.7 (+4.3) \\   

Table 2: **Ablation study of three proposed algorithm components on nuScenes val.** MGFM denotes the Modality-General Fusion Module, L-MFD denotes Low-level Masked Feature Distillation, and H-MFD denotes High-level Masked Feature Distillation. Performance metrics include mAP/NDS for detection and mIoU for the segmentation task.

Figure 5: Ablation study of the blocks number in the mask generation network on nuScenes val.

    &  \\ 
**Method** & **LiDAR Detection** & **LiDAR Segementation** \\  No feature distillation & 57.4/65.6 & 48.6 \\ Feature distillation on entire feature maps & 58.7/66.4 & 50.5 \\ Feature distillation on foreground key points & 60.3/67.0 & – \\ Feature distillation masked by Gaussian & 60.8/67.3 & – \\ Feature distillation on activation value & 61.4/67.7 & 50.9 \\  Mask Generation with Randomly Initialized Query & 63.7/69.1 & 51.6 \\ Mask Generation with Learned Query from fusion & 64.2/69.6 & 52.1 \\   

Table 3: **Ablation study of different mask selection methods in feature distillation on nuScenes val.**Influence of Mask Generation Network ArchitectureOur mask generation network consists of stacked transformer blocks. We examine how the number of blocks affects mask learning convergence 1 speed and KD results on CenterPoint student model. Figure 5 shows that increasing the block count beyond two does not significantly enhance KD performance. However, employing three or more blocks accelerates convergence. To balance learning speed and performance, we opt for a block count of three.

### Qualitative Results

Figure 6 shows spatial masks for different levels and tasks created by our BEV Query Guided Mask Generation module. In detection tasks, the mask emphasizes a wider area around foreground objects, especially in low-level BEV features; in segmentation tasks, it also highlights background features. Figure 7 displays BEV features before and after our fusion module, demonstrating that our fusion module effectively enhances crucial areas with camera contextual information. Figure 8 compares the BEV feature maps before and after KD, illustrating how our method improves camera features with more deterministic depth projection accuracy and accentuates important LiDAR features by correlating point distribution with image textural information.

## 5 Conclusion

We propose VeXKD, a simple and versatile framework that integrates cross-modal fusion and knowledge distillation, suitable for various student modalities and 3D perception tasks. Our approach constructs a superior fusion model as the teacher and enhances the effectiveness of cross-modal knowledge distillation. Using a BEV query guided mask generation network, we develop an adaptable feature distillation pipeline that produces spatial masks for features across different tasks and student models, demonstrating its potential for integration with future 3D perception methods. The effectiveness of our framework is confirmed through experiments on the nuScenes dataset, aiming to spur further research into versatile KD frameworks that move beyond model-specific and intricately engineered setups to more universally adaptable approaches.

**Limitations** While our framework is adaptable and has been tested across various modalities and tasks, many scenarios remain untested due to time and computational constraints, such as radar modality or the motion prediction task. The modality-general information utilized in our fusion model has only been shown through experimental outcomes and visualizations. Currently, no studies can theoretically quantify this information. Future research could aim to extend knowledge distillation to more 3D perception tasks and explore the theoretical aspects that influence its efficacy. We utilize the implicit multi-sweep LiDAR information from the teacher model for temporal knowledge transfer to the student model. The comparison with VCD demonstrates the potential of integrating explicit temporal knowledge distillation operations. Developing versatile cross-modal KD frameworks based on explicit temporal knowledge transfer could be a promising future direction.

Figure 8: **Comparison of feature maps – without vs. with distillation. KD improves camera feature, offering more deterministic view projection accuracy and accentuating important LiDAR features.**

Figure 6: **Visualization of learned spatial mask. Our mask generation network can produce spatial masks for features at different levels, tailored for various 3D perception tasks.**

Figure 7: **Feature maps before and after MGFM, showing enhancements in crucial areas with camera textural feature.**