# Modulated Neural ODEs

Ilze Amanda Auzina

University of Amsterdam

i.a.auzina@uva.nl

&Cagatay Yildiz

University of Tubingen

Tubingen AI Center

&Sara Magliacane

University of Amsterdam

MIT-IBM Watson AI Lab

&Matthias Bethge

University of Tubingen

Tubingen AI Center

&Efstratios Gavves

University of Amsterdam

###### Abstract

Neural ordinary differential equations (NODEs) have been proven useful for learning non-linear dynamics of arbitrary trajectories. However, current NODE methods capture variations across trajectories only via the initial state value or by auto-regressive encoder updates. In this work, we introduce Modulated Neural ODEs (MoNODEs), a novel framework that sets apart dynamics states from underlying static factors of variation and improves the existing NODE methods. In particular, we introduce _time-invariant modulator variables_ that are learned from the data. We incorporate our proposed framework into four existing NODE variants. We test MoNODE on oscillating systems, videos and human walking trajectories, where each trajectory has trajectory-specific modulation. Our framework consistently improves the existing model ability to generalize to new dynamic parameterizations and to perform far-horizon forecasting. In addition, we verify that the proposed modulator variables are informative of the true unknown factors of variation as measured by \(R^{2}\) scores.

## 1 Introduction

Differential equations are the _de facto_ standard for learning dynamics of biological (Hirsch et al., 2012) and physical (Tenenbaum and Pollard, 1985) systems. When the observed phenomenon is deterministic, the dynamics are typically expressed in terms of ordinary differential equations (ODEs). Traditionally, ODEs have been built from a mechanistic perspective, in which states of the observed system and the governing differential equation with its parameters are specified by domain experts. However, when the parametric form is unknown and only observations are available, neural network surrogates can be used to model the unknown differential function, called neural ODEs (NODEs) Chen et al. (2018). Since its introduction by Chen et al. (2018), there has been an abundance of research that uses NODE type models for learning differential equation systems and extend it by introducing a recurrent neural networks (RNNs) encoder (Rubanova et al., 2019; Kanaa et al., 2021), a gated recurrent unit (De Brouwer et al., 2019; Park et al., 2021), or a second order system (Yildiz et al., 2019; Norcliffe et al., 2020; Xia et al., 2021).

Despite these recent developments, all of the above methods have a common limitation: any static differences in the observations can only be captured in a time-evolving ODE state. This modelling approach is not suitable when the observations contain underlying factors of variation that are fixed in time, yet can (i) affect the dynamics or (ii) affect the appearance of the observations. As a concrete example of this, consider human walking trajectories. The overall motion is shared across all subjects, e.g. walking. However, every subject might exhibit person-specific factors of variation, which in turn could either affect the motion exhibited, e.g. length of legs, or could be a characteristic of a given subject, e.g. color of a shirt. A modelling approach that is able to (i) distinguish the dynamic(e.g. position, velocity) from the static variables (e.g. height, clothing color), and (ii) use the static variables to modulate the motion or appearance, is advantageous, because it leads to an improved generalization across people. As an empirical confirmation, we show experimentally that the existing NODE models fail to separate the dynamic factors from the static factors. This inevitably leads to overfitting, thus, negatively effecting the model generalization to new dynamics, as well as far-horizon forecasting.

As a remedy, this work introduces a modulated neural ODE (MoNODE) framework, which separates the dynamic variables from the time-invariant variables. We call these time-invariant variables _modulator variables_ and we distinguish between two types: _(i) static modulators_ that modulate the appearance; and _(ii) dynamics modulators_ that modulate the time-evolution of the latent dynamical system (for a schematic overview see Fig. 1). In particular, MoNODE adds a _modulator prediction network_ on top of a NODE, which allows to compute the _modulator variables_ from data. We empirically confirm that our modular framework boosts existing NODE models by achieving improved future predictions and improved generalization to new dynamics. In addition, we verify that our modulator variables are more informative of the true unknown factors of variation by obtaining higher \(R^{2}\) scores than NODE. As a result, the latent ODE state of MoNODE has an equivalent sequence-to-sequence correspondence as the true observations. Our contributions are as follows:

* We extend neural ODE models by introducing _modulator variables_ that allow the model to preserve core dynamics while being adaptive to modulating factors.
* Our modulator framework can easily be integrated into existing NODE models (Chen et al., 2018; Yildiz et al., 2019; Norcliffe et al., 2020; Xia et al., 2021).
* As we show in our experiments on sinusoidal waves, predator-prey dynamics, bouncing ball videos, rotating images, and motion capture data, our modulator framework consistently leads to improved long-term predictions as measured by lower test mean squared error (an average of 55.25% improvement across all experiments).
* Lastly, we verify that our modulator variables are more informative of the true unknown factors of variation than NODE, as we show in terms of \(R^{2}\) scores.

While introduced in the context of neural ODEs, we believe that the presented framework can benefit also stochastic and/or discrete dynamical systems, as the concept of dynamic or mapping modulation is innate for most dynamical systems. We conclude this work by discussing the implications of our method on learning object-centric representations and its potential Bayesian extensions. Our official implementation can be found at https://github.com/IlzeAmandaA/MoNODE.

## 2 Background

We first introduce the basic concepts for Neural ODEs, following the notation by Chen et al. (2018).

Ordinary differential equationsMultivariate ordinary differential equations are defined as

\[}(t):=(t)}{t}=(t,(t)),\] (1)

Figure 1: Schematic illustration of our method. Standard continuous-time latent dynamical systems such as Chen et al. (2018) assume a single ODE state \((t)\) transformed by a surrogate neural differential function. Our method augments the latent space with dynamics and static modulators to account for exogenous variables governing the dynamics and observation mapping (decoding).

where \(t_{+}\) denotes _time_, the vector \((t)^{q_{z}}\) captures the _state_ of the system at time \(t\), and \(}(t)^{q_{z}}\) is the _time derivative_ of the state \((t)\). In this work, we focus on autonomous ODE systems, implying a vector-valued _(time) differential function_\(:^{q_{z}}^{q_{z}}\) that does not explicitly depend on time. The ODE state solution \((t_{2})\) is computed by integrating the differential function starting from an initial value \((t_{1})\):

\[(t_{2})=(t_{1})+_{t_{1}}^{t_{2}}( ()).\] (2)

For non-linear differential functions \(f\), the integral does not have a closed form solution and hence is approximated by numerical solvers (Tenenbaum and Pollard, 1985). Due to the deterministic nature of the differential function, the ODE state solution \((t_{2})\) is completely determined by the corresponding initial value \((t_{1})\) if the function \(\) is known.

### Latent neural ODEs

Chen et al. (2018) proposed neural ODEs for modeling sequential data \(_{1:T}^{T D}\), where \(_{i}(t_{i})\) is the \(D\)-dimensional observation at time \(t_{i}\), and \(T\) is the sequence length. We assume known observation times \(\{t_{1},,t_{T}\}\). Being a latent variable model, NODE infers a _latent trajectory_\(_{1:T}^{T q_{z}}\) for an input trajectory \(_{1:T}\). The generative model relies on random initial values, their continuous-time transformations, and finally an observation mapping from latent to data space:

\[_{1}  p(_{1})\] (3) \[_{i} =_{1}+_{t_{1}}^{t_{i}}_{}( ())\] (4) \[_{i}  p_{}(_{i}_{i})\] (5)

Here, the time differential \(_{}\) is a neural network with parameters \(\) (hence the name "neural ODEs"). Similar to variational auto-encoders (Kingma and Welling, 2013; Rezende et al., 2014), the "decoding" of the observations is performed by another non-linear neural network with a suitable architecture and parameters \(\).

## 3 MoNODE: Modulated Neural ODEs

We begin with a brief description of the dynamical systems of our interest. Without loss of generality, we consider a dataset of \(N\) trajectories with a fixed trajectory length \(T\), where the \(n\)'th trajectory is denoted by \(_{1:T}^{n}\). First, we make the common assumption that the data trajectories are generated by a single dynamical system (_e.g._, a swinging pendulum) while the parameters that _modulate the dynamics_ (_e.g._, pendulum length) vary across the trajectories. Second, we focus on the more general setup in which the observations and the dynamics might lie in different spaces, for example, video recordings of a pendulum. A future video prediction would require a mapping from the dynamics space to the observation space. As the recordings might exhibit different lighting conditions or backgrounds, the mapping typically involves static features that _modulate the mappings_, _e.g._, a parameter specifying the background.

### Our generative model

The generative model of NODE (Chen et al., 2018) involves a latent initial value \(_{1}\) for each observed trajectory as well as a differential function and a decoder with global parameters \(\) and \(\). Hence, by construction, NODE can attribute discrepancies across trajectories only to the initial value as the remaining functions are modeled globally. Subsequent works (Rubanova et al., 2019; Dupont et al., 2019; De Brouwer et al., 2019; Xia et al., 2021; Iakovlev et al., 2023) combine the latent space of NODE with additional variables, however, likewise, they do not account for static, trajectory-specific factors that modulate either the dynamics or the observation mapping. The main claim of this work is that explicitly modeling the above-mentioned _modulating_ variables results in better extrapolation and generalization abilities. To show this, we introduce the following generative model:

\[  p()\] _// dynamics modulator_ (6) \[  p()\] _// static modulator_ (7) \[_{1}  p(_{1})\] _// latent ODE state_ (8) \[_{i} =_{1}+_{t_{1}}^{t_{i}}_{}(();)\;\] (9) \[_{i}  p_{}(_{i}_{i}\;;\;).\] (10)

We broadly refer to \(\) and \(\) as _dynamics_ and _static modulators_, and thus name our framework _Modulated Neural ODE_ (MoNODE). We assume that each observed sequence \(_{1:T}^{n}\) has its own modulators \(^{n}\) and \(^{n}\). As opposed to the ODE state \((t)\), the modulators are _time-invariant_.

We note that for simplicity, we describe our framework in the context of the initial neural ODE model (Chen et al., 2018). However, our framework can be readily adapted to other neural ODE models such as second-order and heavy ball, NODEs, as we demonstrate experimentally. For a schematic overview, please see Fig. 1. Next, we discuss how to learn the _modulator variables_ along with the global dynamics, encoder, and decoder.

### Learning latent modulator variables

A straightforward approach to obtain time-invariant _modulator variables_ is to define them globally and independently of each other and input sequences. While optimizing for such global variables works well in practice (Blei et al., 2017), it does not specify how to compute variables for an unobserved trajectory. As the focus of the present work is on improved generalization to unseen dynamic parametrizations, we estimate the _modulator variables_ via amortized inference based on encoder networks \(_{}\) and \(_{}\), which we detail in the following.

(i) Static modulatorTo learn the unknown static modulator \(^{n}^{q_{z}}\) that captures the time-invariant characteristics of the individual observations \(_{i}^{n}\), we compute the average over the observation embeddings provided by a modulator prediction network \(_{}()\) (_e.g.,_ a convolutional neural network) with parameters \(\):

\[^{n}=_{i=1}^{T}_{i}^{n},_{ i}^{n}=_{}_{i}^{n}.\] (11)

By construction, \(^{n}\) is time-invariant (or more rigorously, invariant to time-dependent effects) as we average over time. In turn, the decoder takes as input the concatenation of the latent ODE state \(_{i}^{n}\) and the static modulator \(^{n}\), and maps the joint latent representation to the observation space (similarly to Franceschi et al. (2020)):

\[_{i}^{n} p_{}_{i}^{n}_{i}^{n},^{n}\;\; i[1,,T].\] (12)

Note that the estimated static modulator \(^{n}\) is fed as input for all time points within a trajectory.

(ii) Dynamics modulatorUnlike the static modulator, the dynamics modulator can only be deduced from multiple time points \(_{i+T_{e}}^{n}\). For example, the dynamics of a pendulum depend on its length. To compute the length of the pendulum one must compute the acceleration for which multiple position and velocity measurements are needed. Thereby, the dynamics modulators, \(_{i}\), are computed from subsequences of length \(T_{e}\) from a given trajectory. To achieve time-invariance we likewise average over time:

\[^{n}=}_{i=1}^{T-T_{e}}_{i}^{n}, _{i}^{n}=_{}_{i:i+T_{e}}^{ n},\] (13)

where \(_{}\) is a modulator prediction network (_e.g.,_ a recurrent neural network) with parameters \(\). The differential function takes as input the concatenation of the latent ODE state \(^{n}()^{q_{z}}\) and the estimated dynamics modulator \(^{n}^{q_{d}}\). Consequently, we redefine the input space of the differential function \(_{}:^{q_{z}+q_{d}}^{q_ {z}}\), implying the following time differential:

\[^{n}(t)}{t}=_{}^{n}(t),^{n}\] (14)The resulting ODE system resembles in a way the augmented neural ODE (ANODE) (Dupont et al., 2019). However, their appended variable dimensions are constant and serve a practical purpose of breaking down the diffeomorphism constraints of NODE, while ours models time-invariant variables. We treat \(T_{e}\) as a hyperparameter and choose it by cross-validation.

Optimization objectiveThe maximization objective of MoNODE is analogous to the evidence-lower bound (elbo) as in (Chen et al., 2018) for NODE, where we place a prior distribution on the unknown latent initial value \(_{1}\) and approximate it by amortized inference. Similar to previous works (Chen et al., 2018; Yildiz et al., 2019), MoNODE encoder for \(_{1}\) takes a sequence of length \(T_{z}<T\) as input, where \(T_{z}\) is a hyper-parameter. We empirically observe that our framework is not sensitive to \(T_{z}\). The optimization of the modulator prediction networks \(_{}\) and \(_{}\) is implicit in that they are trained jointly with other modules while maximizing the elbo objective.

## 4 Related work

Neural ODEsSince the neural ODE breakthrough (Chen et al., 2018), there has been a growing interest in continuous-time dynamic modeling. Such attempts include combining recurrent neural nets with neural ODE dynamics (Rubanova et al., 2019; De Brouwer et al., 2019), where latent trajectories are updated upon observations, as well as upon Hamiltonian (Zhong et al., 2019), Lagrangian (Lutter et al., 2019), second-order (Yildiz et al., 2019), or graph neural network based dynamics (Poli et al., 2019). While our method MoNODE has been introduced in the context of latent neural ODEs, it can be directly utilized within these frameworks as well.

Augmented dynamicsDupont et al. (2019) augment data-space neural ODEs with additional latent variables and test their method on classification problems. Norcliffe et al. (2021) extend neural ODEs to stochastic processes by means of stochastic latent variables, leading to NODE Processes (NODEP). By construction, NODEP embeddings are invariant to the shuffling of the observations. To the best of our knowledge, we are the first to explicitly enforce _time-invariant modulator variables_.

Learning time-invariant variablesThe idea of averaging for invariant function estimation was used in (Kondor, 2008; van der Wilk et al., 2018; Franceschi et al., 2020). Only the latter proposes using such variables in the context of discrete-time stochastic video prediction. Although relevant, their model involves two sets of dynamic latent variables, coupled with an LSTM and is limited to mapping modulation.

## 5 Experiments

To investigate the effect of our proposed _dynamics_ and _static modulators_, we structure the experiments as follows: First, we investigate the effect of the _dynamics modulator_ on classical dynamical systems, namely, sinusoidal wave, predator-prey trajectories and bouncing ball (section 5.1), where the

   Model & Reference & Latent & 
 Time \\ invariant \\ dynamics \\  & Temporal \\  Neural ODE (NODE) & Chen et al. (2018) & ✓ & ✗ & ✓ \\ Augmented NODE (ANODE) & Dupont et al. (2019) & ✗ & ✗ & ✗ \\ Neural Controlled DE (NCDE) & Kidger et al. (2020) & ✓ & ✗ & ✗ \\ Second Order NODE (SONODE) & Norcliffe et al. (2020) & ✗ & ✗ & ✓ \\ Latent SONODE (LSONODE) & Yildiz et al. (2019) & ✓ & ✗ & ✓ \\ NODE Processes (NODEP) & Norcliffe et al. (2021) & ✓ & ✗ & ✓ \\ Heavy Ball NODE (HBNODE) & Xia et al. (2021) & ✗ & ✗ & ✓ \\  Modulated *NODE & this work & ✓ & ✓ & ✓ \\   

Table 1: A taxonomy of the state-of-the-art ODE systems. We empirically demonstrate that our modulating variables idea is straightforward to apply to NODE, SONODE, and HBNODE, and consistently leads to improved prediction accuracy. Where with * we refer to: MoNODE, MoSONODE, MoLSONODE, MoLBNODE.

parameterisation of the dynamics differs across each trajectory. Second, to confirm the utility of the _static modulator_ we implement an experiment of rotating MNIST digits (section 5.2), where the static content is the digit itself. Lastly, we experiment on real data with having both modulator variables present for predicting human walking trajectories (section 5.4). In all experiments, we test whether our framework improves the performance of the base model on generalization to new trajectories and long-horizon forecasting abilities.

Implementation detailsWe implement all models in PyTorch (Paszke et al., 2017). The encoder, decoder, differential function, and modulator prediction networks are all jointly optimized with the Adam optimizer (Kingma and Ba, 2014). For solving the ODE system we use torchdiffeq(Chen, 2018) package. We use the 4th-order Runge-Kutta numerical solver to compute ODE state solutions (see App. D for ablation results for different solvers). For the complete details on data generation and training setup, we refer to App.B. Further, we report the architectures, number of parameters, and details on hyperparameter for each method in App. C Table 7.

Compared methodsWe test our framework on the following models: (i) Latent neural ODE model (Chen et al., 2018) (NODE), (ii) Second-order NODE model (Norcliffe et al., 2020) (SONODE), (iii) Latent second-order NODE model (Yildiz et al., 2019) (LSONODE), (iv) current state-of-the-art, second-order heavy ball NODE (Xia et al., 2021) (HBNODE).

In order for SONODE and HBNODE to have a comparable performance with NODE we adjust the original implementation by the authors by changing the encoder architecture, while keeping the core of the models, the differential function, unchanged. For further details and discussion, see App. A. We do not compare against ANODE (Dupont et al., 2019) as the methodology is presented in the context of density estimation and classification. Furthermore, we performed preliminary tests with NODEP (Norcliffe et al., 2021); however, the model predictions fall back to the prior in datasets with dynamics modulators. Hence, we did not include any results with this model in the paper as the base model did not have sufficiently good performance. Finally, we chose not to compare against Kidger et al. (2020) as their Riemann-Stieltjes integral relies on smooth interpolations between data points while we particularly focus on the extrapolation performance for which the ground truth data is not available. For an overview of the methods discussed see Table 1.

### Dynamics modulator variable

To investigate the benefit of the _dynamics modulator_ variable, we test our framework on three dynamical systems: sine wave, prey-predator (PP) trajectories, and bouncing ball (BB) videos. In contrast to earlier works (Norcliffe et al., 2020, Rubanova et al., 2019), every trajectory has a **different parameterization** of the differential equation. Intuitively, the goal of the modulator prediction network is to learn this parameterisation and pass it as an input to the dynamics function, which is modelled by a neural network.

#### 5.1.1 Sinusoidal data

The training data consists of \(N=300\) oscillating trajectories with length \(T=50\). The amplitude of each trajectory is sampled as \(a^{n}\) and the frequency is sampled as \(^{n}[0.5,1.0]\)

  &  &  \\ Model & \(T=50\) & \(T=150\) & \(T=100\) & \(T=300\) \\  NODE & 0.13 (0.03) & 1.84 (0.70) & 0.85 (0.08) & 23.81 (2.29) \\ MoNODE (ours) & **0.04** (0.01) & **0.29** (0.11) & **0.74** (0.05) & **4.33** (0.19) \\  SONODE & 2.19 ( 0.15) & 3.05 ( 0.07) & 15.80 (0.75) & 40.92 (0.82) \\ MoSONODE (ours) & **0.05** ( 0.01) & **0.35** ( 0.10) & **1.46** (0.28) & **6.70** (0.83) \\  HBNODE & 0.16 ( 0.02) & 3.36 ( 0.33) & 0.88 ( 0.10) & 3346.62 (2119.24) \\ MoHBNODE (ours) & **0.05** ( 0.01) & **0.65** ( 0.30) & 0.94 ( 0.09) & **10.21** ( 1.43) \\  

Table 2: Test MSE and its standard deviation with and without our framework for NODE (Chen et al., 2018), SONODE (Norcliffe et al., 2020), and HBNODE Xia et al. (2021). We test model performance within the training regime and for forecasting accuracy. Each model is run three times with different initial seed, we report the mean and standard deviation across runs. Lower is better.

where \([,]\) denotes a uniform distribution. Validation and test data consist of \(N_{}=N_{}=50\) trajectories with sequence length \(T_{}=50\) and \(T_{}=150\), respectively. We add noise to the data following the implementation of [Rubanova et al., 2019].

The obtained test MSE demonstrates that our framework improves all aforementioned methods, namely, NODE, SONODE, and the state-of-the-art HBNODE (see Fig. 2, Fig. 11, and Fig. 12). In particular, our framework improves generalization to new dynamics and far-horizon forecasting as reported in Table 2 columns 2 and 3. In addition, modulating the motion via the dynamics modulator variable leads to interpretable latent ODE state trajectories \((t)\), see Fig. 2. More specifically, the obtained latent trajectories \((t)\) have qualitatively a comparable radius topology to the observed amplitudes in the data space. By contrast, the latent space of NODE does not have such structure. Similar results are obtained for MoHBNODE (App. D, Fig. 12).

Training and inference timesTo showcase that our proposed framework is easy to train we have plotted the validation MSE versus wall clock time during training for sinusoidal data, please see App. Fig. 10. As it is apparent from the figure, our framework is easier to train than all baseline methods. We further compute the inference time cost for the sin data experiment, where the test data consists of 50 trajectories of length 150. We record the time it takes NODE and MoNODE to predict future states while conditioned on the initial 10 time points. Repeating the experiment ten times, the inference time cost for NODE is \(0.312 0.050\) while for MoNODE is \(0.291 0.040\).

#### 5.1.2 Predator-prey (PP) data

Next, we test our framework on the predator-prey benchmark [Rubanova et al., 2019, Norcliffe et al., 2021], governed by a pair of first-order nonlinear differential equations (also known as Lotka-Volterra

Figure 3: To show our framework applicability to different NODE variants, here we illustrate HBNODE and MoHBNODE performance on PP data. The left panel illustrates PCA embeddings of the latent ODE state inferred by HBNODE and MoHBNODE, where each color matches one trajectory. The figures on right compare HBNODE and MoHBNODE reconstructions on test trajectory sequences. Note that the models take the dashed area as input, and darker area represents the trajectory length seen during training. predator-prey data.

Figure 2: The left panel illustrates PCA embeddings of the latent ODE state inferred by NODE and MoNODE, where each color matches one trajectory. The figures on the right compare NODE and MoNODE reconstructions on test trajectory sequences. Note that the models take the dashed area as input. and the darker area represents the trajectory length seen during training.

system, Eq. 28). The training data consists of \(N=600\) trajectories of length \(T=100\). For every trajectory the four parameters of the differential equation are sampled, therefore each trajectory specifies a different interaction between the two populations. Validation and test data consist of \(N_{}=N_{}=100\) trajectories with sequence length \(T_{}=100\) and \(T_{}=300\). Similarly to sinusoidal data, we add noise to the data following (Rubanova et al., 2019).

The results show that our modulator framework improves the test accuracy of the existing methods for both, generalization to new dynamics as well as for forecasting, see Table 2. Moreover, examining the latent ODE state embeddings reveals that our framework results in more interpretable latent space embeddings also for PP data, see Fig. 3 for HBNODE and App. D Fig. 13 for NODE. In particular, the latent space of MoHBNODE and MoNODE captured the same amplitude relationship across trajectories as in observation space. For visualisation of the SONODE, MoSONODE, NODE, and MoNODE trajectories see App. D, Fig. 14, Fig. 13.

#### 5.1.3 Bouncing ball (BB) with friction

To investigate the performance of our framework on video sequences we test it on a bouncing ball dataset, a benchmark often used in temporal generative modeling (Sutskever et al., 2008; Gan et al., 2015; Yildiz et al., 2019). For data generation, we modify the original implementation of Sutskever et al. (2008) by adding friction to every data trajectory, where friction is sampled from \([0,0.1]\). The friction slows down the ball by a constant factor and is to be inferred by the dynamics modulator. We use \(N=1000\) training sequences of length \(T=20\), and \(N_{}=N_{}=100\) validation and test trajectories with length \(T_{}=20\) and \(T_{}=40\). Our framework improves predictive capability for video sequences as shown in Table 4. As visible in Fig.15, the standard NODE model fails to predict the position of the object at further time points, while MoNODE corrects this error. For the second-order variants, LSONODE and MoLSONODE, we again observe our framework improving MSE from \(0.0181\) to \(0.014\).

#### 5.1.4 Informativeness metric

Next, we quantify how much information latent representations carry about the unknown factors of variation (FoVs) (Eastwood and Williams, 2018), which are the parameters of the sinusoidal, PP, and BB dynamics. As described in (Schott et al., 2021), we compute \(R^{2}\) scores by regressing from latent variables to FoVs. The regression inputs for MoNODE are the dynamics modulators \(\), while for NODE the latent trajectories \(_{1;T}\). Note that \(R^{2}=1\) corresponds to perfect regression and \(R^{2}=0\) indicates random guessing. Our framework obtains better \(R^{2}\) scores on all benchmarks (Table 3), implying better generalization capability of our framework. Therefore, as stated in (Eastwood and Williams, 2018), our MoNODE is better capable of disentangling underlying factors of variations compared to NODE.

### Static modulator variable

To investigate the benefit of the _static modulator_ variable, we test our framework on Rotating MNIST dataset, where the dynamics are fixed and shared across all trajectories, however, the content varies. The goal of the modulator prediction network is to learn the static features of each trajectory and pass it as an input to the decoder network. The data is generated following the implementation by (Casale et al., 2018), where the total number of rotation angles is \(T=16\). We include all ten digits and the initial rotation angle is sampled from all possible angles \(^{n}\{1,,16\}\). The training data consists of \(N=1000\) trajectories with length \(T=16\), which corresponds to one cycle of rotation. Validation and test data consist of \(N_{}=N_{}=100\) trajectories with sequence length \(T_{}=T_{}=45\). At test time, the model receives the first \(T_{z}\) time frames as input and predicts the full horizon (\(T=45\)). We repeat each experiment 3 times and report the mean and standard deviation of the MSEs computed on the forecasting horizon (from \(T=15\) to \(T=45\)). For further training details see App.B, while for a complete overview of hyperparameter see App. C Table 7.

The obtained test MSE confirms that the _static modulator_ variable improves forecasting quality (\(T=45\)), see Table 4 and App. D fig. 16 for qualitative comparison. In addition, the latent ODE

  & NODE & MoNODE \\   Sine & 0.90 & 0.99 \\ PP & -1.35 & 0.39 \\ BB & -0.29 & 0.58 \\ 

Table 3: \(R^{2}\) scores to predict the unknown FoVs from inferred latents. Higher is better.

states of MoNODE form a circular rotation pattern resembling the observed dynamics in data space while for NODE no correspondence is observed, see App. D Fig. 16. Moreover, as shown in Fig. 4, the latent space of MoNODE captured the relative distances between the initial rotation angles, while NODE did not. The TSNE embeddings of the _static modulator_ indicate a clustering per digit shape, see App. D Fig. 17. For additional results and discussion, see App. D.

### Are modulators interchangeable?

To confirm the role of each modulator variable we have performed two additional ablations with the MoNODE framework on: _(a)_ sinusoidal data with static modulator instead of dynamics, and _(b)_ rotating MNIST with dynamics modulator instead of static. We report the test MSE across three different initialization runs with standard deviation. For sinusoidal data with MoNODE + static modulator, the test MSE performance drops to \(2.68 0.38\) from \(0.29 0.11\) (MoNODE + dynamic modulator). For rotating MNIST, MoNODE + dynamics modulator performance drops to \(0.096 0.008\) from \(0.030 0.001\) (MoNODE + static modulator). In addition, we examined the latent embeddings of the dynamics modulator for rotating MNIST. Where previously for the content modulator we observed clusters corresponding to a digit's class, for dynamics modulator such a topology in the latent space is not present (see App. D Fig. 17). Taken together with Table 3, the results confirm that the modulator variables are correlated with the true underlying factors of variation and play their corresponding roles.

### Real world application: modulator variables

Next, we evaluate our framework on a subset of CMU Mocap dataset, which consists of 56 walking sequences from 6 different subjects. We pre-process the data as described in [Wang et al., 2007], resulting in \(50\)-dimensional data sequences. We consider two data splits in which _(i)_ the training and test subjects are the same, and _(ii)_ one subject is reserved for testing. We refer to the datasets as Mocap and Mocap-shift (see App. B for details). In test time, the model receives the first \(75\) observations as input and predicts the full horizon (\(150\) time points). We repeat each experiment five times and report the mean and standard deviation of the MSEs computed on the full horizon, As shown in Table 4 and Fig. 5, our framework improves upon NODE. See Table 12 for ablations with different latent dimensionalities and with only one modulator variable (static or dynamics) present.

  & Bouncing Ball & Rot.MNIST & Mocap & Mocap-shift \\   NODE & \(0.0199(0.001)\) & 0.039 (0.003) & \(72.2(12.4)\) & \(61.6(6.2)\) \\  MoNODE & \(0.0164(0.001)\) & 0.030 (0.001) & \(57.7(9.8)\) & \(58.0(10.7)\) \\ 

Table 4: A Comparison of NODE and MoNODE test MSEs and standard deviations on Bouncing Ball, Rot.MNIST, and Mocap datasets.

Figure 4: PCA embeddings of the latent ODE state for Rotating MNIST dataset as inferred by NODE (top) and MoNODE (bottom). We generate 16 trajectories from a single digit where the initial angle of every trajectory is incremented by \(24^{}\) degrees, starting from \(0^{}\) until \(360^{}\) degrees. Circle: the start of the trajectory (the initial angle), line: ODE trajectory. The color gradient corresponds to the initial angle of the trajectory in observation space.

## 6 Discussion

The present work introduces a novel modulator framework for NODE models that allows to separate time-evolving ODE states from modulator variables. In particular, we introduce two types of modulating variables: _(i) dynamics modulator_ that can modulate the dynamics function and _(ii) static modulator_ that can modulate the observation mapping function. Our empirical results confirm that our framework improves generalization to new dynamics and far-horizon forecasting. Moreover, our modulator variables better capture the true unknown factors of variation as measured by \(R^{2}\) score, and, as a result, the latent ODE states have an equivalent correspondence to the true observations.

Limitations and future workThe dynamical systems explored in the current work are limited to deterministic periodic systems that have different underlying factors of variation. The presented work introduces a framework that builds upon a base NODE model, hence, the performance of Mo*NODE is largely affected by the base model's performance. The current formulation cannot account for epistemic uncertainty and does not generalize to out-of-distribution modulators, because we maintain point estimates for the modulators. A straightforward extension would be to apply our framework to Gaussian process-based ODEs (Hegde et al., 2022) stochastic dynamical systems via an auxiliary variable that models the noise, similarly to Franceschi et al. (2020). Likewise, rather than maintaining point estimates, the time-invariant modulator parameters could be inferred via marginal likelihood as in van der Wilk et al. (2018); Schwobel et al. (2022), leading to a theoretically grounded Bayesian framework. Lastly, the separation of the dynamic factors and modulating factors could be explicitly enforced via an additional self-supervised contrasting loss term (Grill et al., 2020) or by more recent advances in representation learning (Bengio et al., 2013). Lastly, the concept of _dynamics modulator_ could also be extended to object-centric dynamical modeling (Kabra et al., 2021), which would allow accounting for per-object specific dynamics modulations while using a single dynamic model.

AcknowledgementsThe data used in this project was obtained from mocap.cs.cmu.edu. The database was created with funding from NSF EIA-0196217. Cagatay Yildiz funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC-Number 2064/1 - Project number 390727645. This research utilized compute resources at the Tubingen Machine Learning Cloud, DFG FKZ INST 37/1057-1 FUGG. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 950086).