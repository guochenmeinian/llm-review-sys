# Structural Pruning for Diffusion Models

Gongfan Fang  Xinyin Ma  Xinchao Wang

National University of Singapore

gongfan@u.nus.edu, maxinyin@u.nus.edu, xinchao@nus.edu.sg

Corresponding author

###### Abstract

Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present _Diff-Pruning_, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over _pruned timesteps_, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across several datasets highlights two primary benefits of our proposed method: 1) _Efficiency:_ it enables approximately a 50% reduction in FLOPs at a mere 10% to 20% of the original training expenditure; 2) _Consistency_: the pruned diffusion models inherently preserve generative behavior congruent with their pre-trained models. Code is available at https://github.com/VainF/Diff-Pruning.

## 1 Introduction

Generative modeling has undergone significant advancements in the past few years, largely propelled by the advent of Diffusion Probabilistic Models (DPMs) [18; 41; 37]. These models have derived numerous applications ranging from text-to-image generation , image editing , image translation, and even discriminative tasks [2; 1]. The incredible power of DPMs, however, often comes at the expense of considerable computational overhead during both training  and inference . This trade-off between performance and efficiency presents a critical challenge in the broader application of these models, particularly in resource-constrained environments.

In the literature, huge efforts have been made to improve diffusion models, which primarily revolved around three broad themes: improving model architectures [41; 39; 52], optimizing training methods [49; 11] and accelerating sampling [46; 43; 12]. As a result, a multitude of well-trained diffusion models has been created in these valuable works, showcasing their potential for various applications . However, the notable challenge still remains: the absence of a general compression method that enables the efficient reuse and customization of these pre-existing models without heavy re-training. Overcoming this gap is of paramount importance to fully harness the power of pre-trained diffusion models and facilitate their widespread application across different domains and tasks.

In this work, we demonstrate the remarkable effectiveness of structural pruning [23; 8; 26; 4] as a method for compressing diffusion models, which offers a flexible trade-off between efficiency and quality. Structural pruning is a classic technique that effectively reduces model sizes by eliminating redundant parameters and sub-structures from networks. While it has been extensively studied in discriminative tasks such as classification , detection , and segmentation , applying structural pruning techniques to Diffusion Probabilistic Models poses unique challenges that necessitatea rethinking of traditional pruning strategies. For example, the iterative nature of the generative process in DPMs, the models' sensitivity to small perturbations in different timesteps, and the intricate interplay in the diffusion process collectively create a landscape where conventional pruning strategies often fall short.

To this end, we introduce a novel approach called _Diff-Pruning_, explicitly tailored for the compression of diffusion models. Our method is motivated by the observation in previous works [41; 52] that different stages in the diffusion process contribute variably to the generated samples. At the heart of our method lies a Taylor expansion over pruned timesteps, which deftly balances the image content, details, and the negative impact of noisy diffusion steps during pruning. Initially, we show that the objective of diffusion models at late timesteps (\(t T\)) prioritize the high-level content of the generated images during pruning, while the early ones (\(t 0\)) refine the images with finer details. However, it is also observed that, when using Taylor expansion for pruning, the noisy stages with large \(t\) can not provide informative gradients for importance estimation and can even harm the compressed performance. Therefore, we propose to model the trade-off between contents, details, and noises as a pruning problem of the diffusion timesteps, which leads to an efficient and flexible pruning algorithm for diffusion models.

Through extensive empirical evaluations across diverse datasets, we demonstrate that our method achieves substantial compression rates while preserving and in some cases even improving the generative quality of the models. Our experiments also highlight two significant features of Diff-Pruning: efficiency and consistency. For example, when applying our method to an off-the-shelf diffusion model pre-trained on LSUN Church , we achieve an impressive compression rate of 50% FLOPs, with only 10% of the training cost required by the original models, equating to 0.5 million steps compared to the 4.4 million steps of the pre-existing models. Furthermore, we have thoroughly assessed the generative behavior of the compressed models both qualitatively and quantitatively. Our evaluations demonstrate that the compressed model can effectively preserve a similar generation behavior as the pre-trained model, meaning that when provided with the same inputs, both models yield consistent outputs. Such consistency further reveals the practicality and reliability of Diff-Pruning as a compression method for diffusion models.

In summary, this paper introduces Diff-Pruning as an efficient method for compressing Diffusion Probabilistic Models, which is able to achieve compression with only 10% to 20% of the training costs compared to pre-training. This work may serve as an initial baseline and provide a foundation for future research aiming to enhance the quality and consistency of compressed diffusion models.

## 2 Ralted Works

Efficient Diffusion ModelsThe existing methodologies principally address the efficiency issues associated with diffusion models via three primary strategies: the refinement of network architectures [41; 52; 37], the enhancement of training procedures [11; 49], and the acceleration of sampling [18; 27; 12]. Diffusion models typically employ U-Net models as denoisers, of which the efficiency can be improved via the introduction of hierarchical designs  or by executing the training within a novel latent space [41; 19; 25]. Recent studies also suggest integrating more efficient layers or structures into the denoiser to bolster the performance of the U-Net model [52; 39], thereby facilitating superior image quality learning during the training phase. Moreover, a considerable number of studies concentrate on amplifying the training efficiency of diffusion models, with some demonstrating that the diffusion training can be expedited by modulating the weights allocated to distinct timesteps [43; 11]. The training efficiency can also be advanced by learning diffusion models at the patch level . In addition, some approaches underscore the efficiency of sampling, which typically does not necessitate the retraining of diffusion models . In this area, numerous studies aim to diminish the required steps through methods such as early stopping  or distillation .

Network PruningIn recent years, the field of network acceleration [59; 3; 20; 53; 51; 29; 30] has seen notable progress through the deployment of network pruning techniques [31; 16; 33; 23; 14; 5; 15]. The taxonomy of pruning methodologies typically bifurcates into two main categories: structural pruning [23; 6; 56; 26; 56] and unstructured pruning [38; 7; 44; 22]. The distinguishing trait of structural pruning is its ability to physically eliminate parameters and substructures from networks, while unstructured pruning essentially masks parameters by zeroing them out [8; 4]. However, the preponderance of network pruning research is primarily focused on discriminative tasks, particularly classification tasks . A limited number of studies have ventured into examining the effectiveness of pruning in generative tasks, such as GAN compression [24; 47]. Moreover, the application of structural pruning techniques to Diffusion Probabilistic Models introduces unique challenges that demand a reevaluation of conventional pruning strategies. In this work, we introduce the first dedicated method explicitly designed for pruning diffusion models, which may serve as a useful baseline for future works.

## 3 Diffusion Model Objectives

Given a data distribution \(q()\), diffusion models aim to model a generative distribution \(p_{}()\) to approximate \(q()\), taking the form

\[p_{}()= p_{}(_{0:T})d_{1:T},  p_{}(_{0:T}):=p(_{T})_{t=1}^{T}p_{}(_ {t-1}|_{t})\] (1)

And \(_{1},...,_{T}\) refer to the latent variables, which contribute to the joint distribution \(p_{}(_{0:T})\) with learned Gaussian transitions \(p_{}(_{t-1}|_{t})=(_{t-1};_{}( {x}_{t},t),_{}(_{t},t))\). Diffusion Models involve two opposite processes: a forward (diffusion) process \(q(_{t}|_{t-1})=(_{t};}_{ t-1},_{t}I)\) that adds noises to the \(_{t-1}\), based on a pre-defined variance schedule \(_{1:T}\); and a reverse process \(q(_{t-1}|_{t})\) which "denoises" the observation \(_{t}\) to get \(_{t-1}\). Using the notation \(_{t}=1-_{t}\) and \(_{t}=_{s=1}^{t}_{s}\), DDPMs  trains a noise predictor with the objective:

\[():=_{t,_{0} q(),(0,1)}[\|-_{}(_{t}}_{0}+_{t}},t)\|^{2}]\] (2)

where \(\) is a random noise drawn from a fixed Gaussian distribution and \(_{}\) refers to a learned noise predictor, which is usually an U-Net autoencoder  in practice. After training, synthetic images \(_{0}\) can be sampled through an iterative process from a noise \(_{T}(,)\) with the formular:

\[_{t-1}=}}(_{t}-}{ _{t}}}_{}(_{t},t))+ _{t}\] (3)

where \((,)\) for steps \(t>1\) and \(=\) for \(t=1\). In this work, we aim to craft a lightweight \(_{}}\) by removing redundant parameters of \(_{}\), which are expected to produce similar \(_{0}\) while the same \(_{T}\) are presented.

## 4 Structural Pruning for Diffusion Models

Given the parameter \(\) of a pre-trained diffusion model, our goal is to craft a lightweight \(}\) by removing sub-structures from the network following existing paradigms [35; 8]. Without loss of generality, we assume that the parameter \(\) is a simple 2-D matrix, where each sub-structure \(_{i}=[_{i0},_{i1},...,_{iK}]\) is a row vector that contains \(K\) scalar parameters. Structural pruning aims to find a sparse parameter matrix \(}\) that maximally preserves the original performance. Thus, a natural choice is to optimize the loss disruption caused by pruning:

\[_{}}|(})-( {})|,\ \|}\|_{0} s\] (4)

The term \(|}|_{0}\) denotes the L-0 norm of the parameters, which counts the number of non-zero row vectors, and \(s\) represents the sparsity of the pruned model. Nevertheless, due to the iterative nature intrinsic to diffusion models, the training objective, denoted by \(\), can be perceived as a composition of \(T\) interconnected tasks: \(\{_{1},_{2},...,_{T}\}\). Each task affects and depends on the others, thereby posing a new challenge distinct from traditional pruning problems, which primarily concentrate on optimizing a single objective. In light of the pruning objective as defined in Equation 4, we initially delve into the individual contributions of each loss component, \(_{t}\) in pruning, and subsequently propose a tailored method, Diff-Pruning, designed for diffusion models pruning.

Taylor Expansion at \(_{t}\)Initially, we need to model the contribution of \(_{t}\) for structural pruning. This work leverages Taylor expansion  on \(_{t}\) to linearly approximate the loss disruption:

\[_{t}(}) =_{t}()+_{t}()( }-)+O(\|}-\|^{2})\] (5) \[_{t}(})-_{t}( ) =_{t}()(}-)+O(\|}-\|^{2})\]Taylor expansion offers a robust framework for network pruning, as it can estimate the loss disruption using first-order gradients. To evaluate the importance of an individual weight \(_{ik}\), we can simply set \(}_{ik}=0\) in Equation 5, which results in the following importance criterion:

\[_{t}(_{ik},)& =|_{t}(|_{_{ik}=0})-_{t}( )|\\ &=|(_{i0}-_{i0})_{_{ i0}}++(0-_{ik})_{_{ik}}++(_{ iK}-_{iK})_{_{iK}}|\\ &=|_{ik}_{_{ik}}_{t} (,)|\] (6)

where \(_{_{ik}}\) refer to \(_{_{ik}}_{t}(,)\). In structural pruning, we aim to remove the entire vector \(}_{i}\) concurrently. The standard Taylor expansion for multiple variables, as described in the literature , advocates using \(|_{k}_{ik}_{_{ik}}_{t}(,)|\) for importance estimation. This method exclusively takes into account the loss difference between the initial state \(\) and the final states \(}\). However, considering the iterative nature of diffusion models, even minor fluctuations in loss can influence the final generation results. To this end, we propose to aggregate the influence of removing each parameter as the final importance. This modification models cumulative loss disturbance induced by each \(_{ik}\)'s removal and leads to a slightly different score function for structural pruning:

\[_{t}(_{i},)=_{k}|_{t}(| _{_{ik}=})-_{t}()|=_{k}|_{ik}_{_{ik}}_{t}(,)|\] (7)

In the following sections, we utilize Equation 7 as the importance function to identify non-critical parameters in diffusion models.

The Contribution of \(_{t}\).With the Taylor expansion framework, we further explore the contribution of different loss terms \(\{_{1},...,_{T}\}\) during pruning. We consider the functional error \(_{t}=_{}}(,t)-_ {}(,t)\) which represents the prediction error for the same inputs at time step \(t\). The reverse process allows us to exam the effects \(_{t 0}\) on the generated images \(x_{0}\) by iteratively applying the Equation 3 starting from \(_{}}(,t)=_{}( {x},t)+_{t}\). At the \(t-1\) step, it leads to the error \(_{t-1}\) derived as:

\[_{t-1}&=[}}(_{t}-}{}}_{}(_{t},t))+_{t}]-[ }}(_{t}-}{}}(_{}(_{t},t)+_{t}))+_{t} ]\\ &=}}}{}} _{t}\] (8)

This error has a direct impact on the subsequent input, given by \(x^{}_{t-1}=x_{t-1}+_{t-1}\). By checking Equation 3, we can observe that these perturbed inputs can further trigger a chained effect through both \(}}x^{}_{t-1}\) and \(-}}}{}}_{}}(x^{}_{t-1},t-1)\). In the first term, the distortion progressively amplifies by a factor \(}}>1\), which means that this error will be enhanced throughout the generation process. Regarding the second term, pruning affects both the functionality parameterized by \(}\) and the inputs \(^{}_{t-1}\), which contributes to the final results in a nonlinear and more complicated manner, resulting in a more substantial disturbance on the generated images.

Figure 1: Diff-Pruning leverages Taylor expansion at pruned timesteps to estimate the importance of weights, where early steps focus on local details like edges and color and later ones pay more attention to contents such as object and shape. We propose a simple thresholding method to trade off these factors with a binary weight \(_{t}\{0,1\}\), leading to a practical algorithm for diffusion models. The generated images produced by 5%-pruned DDPMs (without post-training) are illustrated.

As a result, prediction errors occurring at larger \(t\) tend to have a larger impact on the images due to the chain effect, which might change the global content of generated images. Conversely, smaller \(t\) values focus on refining the images with relatively small modifications. These findings align with our empirical examination using Taylor expansion as illustrated in Figure 1, as well as the observation in previous works [18; 52], which shows that diffusion models tend to generate object-level information at larger \(t\) values and fine-tune the features at smaller ones. To this end, we model the pruning problem as a weighted trade-off between contents and details by introducing \(_{t}\), which acts as a weighting variable for different timesteps \(t\). Nevertheless, unconstrained reweighting can be highly inefficient, as it entails exploring a large parameter space for \(_{t}\) and requires at least \(T\) forward-backward passes for Taylor expansion. This results in a vast sampling space and can lead to inaccuracies in the linear approximation. To address this issue, we simplify the re-weighting strategy by treating it as a "pruning problem", where \(_{t}\) takes the value of either 0 or 1 for all steps, allowing us to only leverage partial steps for pruning. The general importance metric is modeled as the following.

\[(_{i},)=_{k}|_{ik} _{t}_{t}_{_{ik}}_{t}(,)|,_{t}\{0,1\}\] (9)

Taylor Score over Pruned Timesteps.In Equation 9, we try to remove some "unimportant" timesteps in the diffusion process so as to enable an efficient and stable approximation for partial steps. Our empirical results, as will be discussed in the experiments, indicate two key findings. Firstly, we note that the timesteps responsible for generating content are not exclusively found towards the end of the diffusion process (\(t T\)). Instead, there are numerous noisy and redundant timesteps that contribute minorly to the overall generation, which is similar to the observations in the related work . Secondly, we discovered that employing the full-step objective can sometimes yield suboptimal results compared to using a partial objective. We attribute this negative impact to the presence of converged gradients in the noisy steps (\(t T\)). Taylor approximation in Equation 5 comprises both first-order gradients and higher-order terms. When the loss \(_{t}\) converges, the loss curve is predominantly influenced by the higher-order terms rather than the first-order gradients we utilize. Our experiments on several datasets and diffusion models show that the loss term \(_{t}\) rapidly approaches 0 as \(t T\). For example in Figure 5, the relative loss \(_{t}}{_{max}}\) of a pre-trained diffusion model for CIFAR-10 decreases to 0.05 when \(t=250\). Consequently, a full Taylor expansion can accumulate a considerable amount of noisy gradients from these converged or unimportant steps, resulting in an inaccurate estimation of weight importance.

Considering the significant impact of larger timesteps, it is necessary to incorporate them for importance estimation. To address this problem, Equation 9 naturally provides a simple and practical thresholding strategy for pruning. To achieve this, we introduce a threshold parameter \(\) based on the relative loss \(_{t-}}{_{max}}\). Those timesteps with a relative loss below this threshold, i.e., \(_{t}}{_{max}}<\), are considered uninformative and are disregarded by setting \(_{t}=0\), which yields the finalized importance score:

\[(_{i},)=_{k}|_{ik} _{\{t|_{t}}{_{max}}>\}}_{_{ik}}_{t}(,)|\] (10)

In practice, we need to select an appropriately large value for \(\) to strike a well-balanced preservation of details and content, while also avoiding uninformative gradients from noisy loss terms. The full algorithm is summarized in Alg. 1.

## 5 Experiments

### Settings

Datasets and ModelsThe efficacy of Diff-Pruning is empirically validated across six diverse datasets, including CIFAR-10 (32\(\)32), CelebA-HQ (64\(\)64), LSUN Church (256\(\)256), LSUN Bedroom (256\(\)256)  and ImageNet-1K (256\(\)256). We focus on two popular DPMs in our experiments, i.e., Denoising Diffusion Probability Models (DDPMs)  and Latent Diffusion Models (LDMs) . For the sake of reproducibility, we utilize off-the-shelf DPMs from  and  as pre-trained models and prune these models in a one-shot fashion.

Evaluation MetricsIn this paper, we concentrate primarily on three types of metrics: 1) Efficiency metrics, which include the number of parameters (#Params) and Multiply-Add Accumulation (MACs); 2) Quality metric, namely the Frechet Inception Distance (FID) ; and 3) Consistency metric, represented by Structural Similarity (SSIM) . Unlike previous generative tasks that lacked reference images, we employ the SSIM index to evaluate the similarity between images generated by pre-trained models and pruned models, given identical noise inputs. We deploy a 250-step DDIM sampler  for ImageNet and a 100-step DDIM sampler for other experiments.

### An Simple Benchmark for Diffusion Pruning

Scratch Training v.s. Pruning.Table 1 shows our results on CIFAR-10 and CelebA-HQ. The first baseline method that pluges our interest is scratch training. Numerous studies on network pruning  suggest that training a compact network from scratch can be a formidable competitor. To ensure a fair comparison, we create randomly initialized networks with the same architecture as the pruned ones for scratch training. Our results reveal that scratch training demands relatively more steps to reach convergence. This suggests that training lightweight models from scratch may not be an efficient and economical approach, given its training cost is comparable to that of pre-trained models. Conversely, we observe that all pruning methods are able to converge within approximately 100K steps and outperform scratch training in terms of FID and SSIM scores. Thus, pruning emerges as a potent technique for compressing pre-trained Diffusion Models.

Pruning Criteria.A significant aspect of network pruning is the formulation of pruning criteria, which serve to identify superfluous parameters within networks. Due to the absence of dedicated work on Diffusion model pruning, we adapted three basic pruning methods from discriminative tasks: random pruning, magnitude-based pruning , and Taylor-based pruning , which we refer to as Random, Magnitude, and Taylor respectively in subsequent sections. For a given parameter \(\), Random assigns importance scores derived from a uniform distribution to each \(_{1}\) randomly, denoted as \(()(0,1)\). This results in a straightforward baseline devoid of any prior or bias, and has been shown to be a competitive baseline for pruning . Magnitude subscribes to the "smaller-norm-less-informative" hypothesis [23; 55], modelling the weight importance as \(()=||\). In contrast, Taylor is a data-driven criterion that measures importance as \((,x)=|_{}(x,)|\), which aims to minimize loss change as discussed in our method. As shown in 1, an intriguing phenomenon is that these three baseline methods do not maintain a consistent ranking on these two datasets. For 

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_EMPTY:8]

can be attained at around 250 steps, and adding more steps can slightly deteriorate the quality of the synthetic images. This primarily stems from the inaccuracy of the first-order Taylor expansion at converged points, where the gradient no longer provides useful information and can even distort informative gradients through accumulation. However, we observe that the situation differs slightly with the CelebA dataset, where more steps can be beneficial for importance estimation.

Pruning Ratios.Table 4 presents the #Params, MACs, FID, and SSIM scores of models subjected to various pruning ratios based on MACs. Notably, our findings reveal that, unlike CNNs employed in discriminative models, diffusion models exhibit a significant sensitivity to changes in model size. Even a modest pruning ratio of \(16\%\) leads to a noticeable degradation in FID score (\(4.19 4.62\)). In classification tasks, a perturbation in loss does not necessarily impact the final accuracy; it may only undermine prediction confidence while leaving classification accuracy unaffected. However, in generative models, the FID score is very sensitive, making it more susceptible to domain shift.

Thresholding.In addition, we conducted experiments to investigate the impact of the thresholding parameter \(\). Setting \(=0\) corresponds to a full Taylor expansion at all steps, while \(>0\) denotes pruning of certain timesteps during importance estimation. The quantitative findings presented in Table 5 align with the SSIM results depicted in Figure 5. Notably, Diff-Pruning attains optimal performance when the quality of generated images reaches its peak. For datasets such as CIFAR-10, we observed that a 200-step Taylor expansion is sufficient to achieve satisfactory results. Besides, using a full Taylor expansion, in this case, can be detrimental, as it accumulates noisy gradients over approximately 700 steps, which obscures the correct gradient information from earlier steps.

    &  & \\
**Ratio** & **\#Params** & **MACs** & **FID \(\)** & **SSIM \(\)** \\ 
0\% & 35.7M & 6.1G & 4.19 & 1.000 \\
16\% & 27.5M & 5.1G & 4.62 & 0.942 \\
44\% & 19.8M & 3.4G & 5.29 & 0.932 \\
56\% & 14.3M & 2.7G & 6.36 & 0.922 \\
70\% & 8.6M & 1.5G & 9.33 & 0.909 \\   

Table 4: Pruning with different ratios

  
**Method** & **\#Params \(\)** & **MACs \(\)** & **FID \(\)** & **IS \(\)** & **Train Steps \(\)** \\  Pretrained LDM & 400.92M & 99.80G & 3.60 & 247.67 & 2000K \\  Scratch Training & & & 51.45 & 25.69 & 100K \\ Taylor Pruning & 189.43M & 52.71G & 11.18 & 138.97 & 100K \\ Ours (\(=0.1\)) & & & 9.16 & 201.81 & 100K \\   

Table 3: Compressing conditional Latent Diffusion Models on ImageNet-1K (256 \(\) 256)

Figure 3: Images sampled from the pruned conditional LDM on ImageNet-1K-256

Visualization of Different Importance Criteria.Figure 4 visualizes the images generated by pruned models using different pruning criteria, including the proposed method with \(=0\) (w/o timestep pruning) and \(>0\). The SSIM scores of the generated samples are reported for a quantitative comparison. The Diff-Pruning method with \(>0\) achieves superior visual quality, with an SSIM score of 0.905 after pruning. It is observed that employing more timesteps in our method could have a negative impact, leading to greater distortion in both textures and contents.

## 6 Conclusion

This work introduces Diff-Pruning, a dedicated method for compressing diffusion models. It utilizes Taylor expansion over pruned timesteps to identify and remove non-critical parameters. The proposed approach is capable of crafting lightweight yet consistent models from pre-trained ones, incurring only about 10% to 20% of the cost compared to pre-training. This work may set an initial baseline for future research that aims at improving both the generation quality and the consistency of pruned diffusion models.