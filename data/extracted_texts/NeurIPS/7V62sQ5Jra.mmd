# Prediction-Powered Ranking of

Large Language Models

 Ivi Chatzi

Max Planck Institute for

Software Systems

Kaiserslautern, Germany

ichatzi@mpi-sws.org &Eleni Straitouri

Max Planck Institute for

Software Systems

Kaiserslautern, Germany

estraitouri@mpi-sws.org &Suhas Thejaswi

Max Planck Institute for

Software Systems

Kaiserslautern, Germany

thejaswi@mpi-sws.org &Manuel Gomez Rodriguez

Max Planck Institute for

Software Systems

Kaiserslautern, Germany

manuelgr@mpi-sws.org

###### Abstract

Large language models are often ranked according to their level of alignment with human preferences--a model is better than other models if its outputs are more frequently preferred by humans. One of the popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a common practice to gather pairwise comparisons by a strong large language model--a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a (small) set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provides a rank-set--a set of possible ranking positions--for each of the models under comparison. Moreover, it guarantees that, with a probability greater than or equal to a user-specified value, the rank-sets cover the true ranking consistent with the distribution of human pairwise preferences asymptotically. Using pairwise comparisons made by humans in the LMSYS Chatbot Arena platform and pairwise comparisons made by three strong large language models, we empirically demonstrate the effectivity of our framework and show that the rank-sets constructed using only pairwise comparisons by the strong large language models are often inconsistent with (the distribution of) human pairwise preferences.

## 1 Introduction

During the last years, large language models (LLMs) have shown a remarkable ability to generate and understand general-purpose language . As a result, there has been an increasing excitement in their potential to help humans solve a variety of open-ended, complex tasks across many application domains such as coding , healthcare  and scientific discovery , to name a few. However, evaluating and comparing the performance of different LLMs has become very challenging . The main reason is that, in contrast to traditional machine learning models, LLMs can solve a large number of different tasks and, in many of these tasks, there is not a unique, structured solution. As aconsequence, there has been a paradigm shift towards evaluating their performance according to their level of alignment with human preferences--a model is better than other models if its outputs are more frequently preferred by humans [6; 7; 8; 9; 10].

One of the most popular paradigms to rank a set of LLMs according to their level of alignment with human preferences utilizes pairwise comparisons [10; 11; 12; 13; 14; 15; 16; 17]. Under this paradigm, each pairwise comparison comprises the outputs of two different models picked uniformly at random to an input sampled from a given distribution of inputs. Moreover, the pairwise comparisons are used to rank the models with a variety of methods such as the Elo rating [18; 19; 20; 21; 22], the Bradley-Terry model [10; 17; 23] or the win-rate [12; 17; 23]. While it is widely agreed that, given a sufficiently large set of pairwise comparisons, higher (lower) ranking under this paradigm corresponds to better (worse) human alignment, there have also been increasing concerns that this paradigm is too costly and time-consuming to be practical, especially given the pace at which models are updated and new models are developed.

To lower the cost and increase the efficiency of ranking from pairwise comparisons, it has become a common practice to ask a strong LLM--a model known to strongly align with human preferences--to perform pairwise comparisons [24; 25; 26; 27; 28; 29; 30; 31; 32; 33]. The rationale is that, if a model strongly aligns with human preferences, then, the distributions of pairwise comparisons by the model and by the human should in principle match [24; 27; 34]. Worryingly, there are multiple lines of evidence, including our experimental findings in Figure 3, showing that the rankings constructed using pairwise comparisons made by a strong LLM are sometimes different to those constructed using pairwise comparisons by humans [12; 14; 15; 16; 19; 35; 36], questioning the rationale above. In this work, we introduce a statistical framework to measure the uncertainty in the rankings constructed using pairwise comparisons made by a model, which may be introduced by a mismatch between human and model preferences or by the fact that we use a finite number of pairwise comparisons.

**Our contributions.** Our framework measures uncertainty using rank-sets--sets of possible ranking positions that each model can take. If the rank-set of a model is large (small), it means that there is high (low) uncertainty in the ranking position of the model. To construct the rank-sets, our framework first leverages a (small) set of pairwise comparisons by humans and a large set of pairwise comparisons by a strong LLM to create a confidence ellipsoid. By using prediction-powered inference [37; 38; 39], this confidence ellipsoid is guaranteed to contain the vector of (true) probabilities that each model is preferred over others by humans--the win-rates--with a user-specified coverage probability \(1-\). Then, it uses the distance between this ellipsoid and the hyperplanes under which pairs of models have the same probability values of being preferred over others to efficiently construct the rank-sets. Importantly, we can show that, with probability greater than or equal to \(1-\), the constructed rank-sets are guaranteed to cover the ranking consistent with the (true) probability that each model is preferred over others by humans asymptotically. Moreover, our framework does not make any assumptions on the distribution of human preferences nor about the degree of alignment between pairwise preferences of humans and the strong LLM. Experiments on pairwise comparisons made by humans in the LMSYS Chatbot Arena platform  and pairwise comparisons made by three strong LLMs, namely GPT 3.5, Claude 3 and GPT 4, empirically demonstrate that the rank-sets constructed using our framework are more likely to cover the true ranking consistent with (the distribution of) human pairwise preferences than the rank-sets constructed using only pairwise comparisons made by the strong LLMs. An open-source implementation of our methodology as well as the data on pairwise preferences of strong LLMs used in our experiments are available at https://github.com/Networks-Learning/prediction-powered-ranking.

**Further related work.** Our work builds upon recent work on prediction-powered inference, ranking under uncertainty, and ranking of LLMs.

Prediction-powered inference [37; 38; 39] is a recently introduced statistical framework to obtain valid \(p\)-values and confidence intervals about a population-level quantity such as the mean outcome or a regression coefficient using a small labeled dataset and a large unlabeled dataset, whose labels are imputed using a black-box machine learning model. However, our work is the first to use prediction-powered inference (as a subroutine) to construct rank-sets with coverage guarantees. In this context, it is worth acknowledging that a very recent work by Saad-Falcon et al.  has used prediction-powered inference to construct (single) rankings, rather than rank-sets. However, their rankings do not enjoy coverage guarantees with respect to the true ranking consistent with (the distribution of) the human preferences. Moreover, an independent, concurrent work by Boyeauet al.  has also used prediction-powered inference to construct (single) rankings based on the estimated coefficients of a Bradley-Terry model. However, the estimated coefficients come with large, overlapping confidence intervals, which would have led to uninformative rank-sets, had the authors used them to construct rank-sets.

The vast majority of the literature on ranking under uncertainty has focused on confidence intervals for individual ranking positions . Only recently, a paucity of work has focused on joint measures of uncertainty for rankings . Similarly as in our work, this line of work also seeks to construct rank-sets with coverage guarantees. However, in contrast to our work, it estimates the quality metric (in our work, the probability that an LLM is preferred over others) and the confidence intervals separately for each of the items (in our work, LLMs) using independent samples. As a consequence, it needs to perform multiple comparison correction to create the rank-sets.

In recent years, there has also been a flurry of work on ranking LLMs using benchmark datasets with manually hand-crafted inputs and ground-truth outputs . However, it has become increasingly clear that oftentimes rankings derived from benchmark datasets do not correlate well with rankings derived from human preferences--an improved ranking position in the former does not lead to an improved ranking position in the latter . Within the literature on ranking LLMs from pairwise comparisons, most studies use the Elo rating system , originally introduced for chess tournaments . However, Elo-based rankings are sensitive to the order of pairwise comparisons, as newer comparisons have more weight than older ones, which leads to unstable rankings . To address this limitation, several studies have instead used the Bradley-Terry model , which weighs pairwise comparisons equally regardless of their order. Nevertheless, both the Elo rating system and the Bradley-Terry model have faced criticism, as pairwise comparisons often fail to satisfy the fundamental axiom of transitivity, upon which both approaches rely , Recently, several studies have used the win-rate , which weighs comparisons equally regardless of their order and does not require the transitivity assumption, but requires humans to make pairwise comparisons between every pair of models. In our work, we build upon the win-rate and lift the above requirement by using pairwise comparisons made by a strong LLM.

## 2 LLM Ranking under Uncertainty

Let \(\) be a set of \(k\) large language models (LLMs) and \(P(Q)\) be a distribution of inputs on a discrete set of inputs \(\). Moreover, assume that, for each input \(q P(Q)\),1 each model \(m\) may provide an output \(r P_{m}(R\,|\,Q=q)\) from a discrete set of outputs \(\). Further, given two outputs \(r,r^{}\) from two different models, the (binary) variables \(w,w^{} P(W,W^{}\,|\,Q=q,R=r,R^{}=r^{})\) indicate whether a human prefers \(r\) over \(r^{}\) (\(w=1\), \(w^{}=0\)) or viceversa (\(w=0\), \(w^{}=1\)). In the case of a tie, then \(w=w^{}=0\). In what follows, we use \(m(r)\) and \(m(r^{})\) to denote the models that provide outputs \(r\) and \(r^{}\) respectively, and without loss of generality, we assume that the output \(r\) is shown first. Then, our goal is to rank all models according to the (empirical) probability \(_{m}\) that their outputs are preferred over the outputs of any other model picked uniformly at random.

To this end, we start by writing the probability \(_{m}\) as an expectation over the distribution of inputs, outputs and pairwise preferences:

\[_{m}=_{\{m\}}_{Q}[_{R P_{m},R^{ } P_{}}[_{W}[W\,|\,Q,R,R^{}]]+ .\\ ._{R P_{},R^{} P _{}}[_{W^{}}[W^{}\,|\,Q,R,R^{}] ]],\] (1)

where note that the order of the pairs of outputs is picked at random. Next, following previous work , we formally characterize the ranking position of each model \(m\) in the ranking induced by the probabilities \(_{m}\) using a rank-set \([l(m),u(m)]\), where

\[l(m)=1+_{\{m\}}\{_{m}< _{}\} u(m)=k-_{ \{m\}}\{_{m}>_{}\},\] (2)

are the lower and upper ranking position respectively and smaller ranking position indicates better alignment with human preferences. Here, note that it often holds that \(_{m}_{}\) for all \(\{m\}\) and then the rank-set reduces to a singleton, _i.e._, \(l(m)=u(m)\).

In general, we cannot directly construct the rank-sets as defined above because the probabilities \(_{m}\) are unknown. Consequently, the typical strategy reduces to first gathering pairwise comparisons by humans to compute unbiased estimates of the above probabilities using sample averages and then construct estimates \([(m),(m)]\) of the rank-sets using Eq. 2 with \(_{m}\) rather than \(_{m}\). Under this strategy, if the amount of pairwise comparisons we gather is sufficiently large, the estimates of the rank-sets will closely match the true rank-sets. However, since gathering pairwise comparisons from humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons \(,^{}\) by a strong LLM, rather than pairwise comparisons \(w,w^{}\) by humans [12; 13; 14; 28; 29; 31; 61; 62; 63; 64], and then utilize them to compute unbiased estimates of the probabilities \(_{m}\) that the outputs provided by each model is preferred over others by the strong LLM, which can be written in terms of expectations as follows:

\[_{m}=_{ \{m\}}_{Q}[_{R P _{m},R^{} P_{}}[_{}[\,|\,Q,R,R^ {}]]+.\\ ._{R P_{},R^{} P_{ m}}[_{^{}}[^{}\,|\,Q,R,R^{}] ]].\] (3)

In general, one can only draw valid conclusions about \(\) using (an estimate of) \(}\) if the distribution of the pairwise comparisons by the strong LLM \(P(,^{}\,|\,Q=q,R=r,R^{}=r^{})\) closely matches the distribution of pairwise comparisons by the humans \(P(W,W^{}\,|\,Q=q,R=r,R^{}=r^{})\) for any \(q\) and \(r,r^{}\). However, there are multiple lines of evidence showing that there is a mismatch between the distributions, questioning the validity of the conclusions drawn by a myriad of papers. In what follows, we introduce a statistical framework that, by complementing a (large) set of \(N+n\) pairwise comparisons \(,^{}\) by a strong LLM with a small set of \(n\) pairwise comparisons \(w,w^{}\) by humans, is able to construct estimates \([(m),(m)]\) of the rank-sets with provable coverage guarantees. More formally, given a user-specified value \((0,1)\), the estimates of the rank-sets satisfy that

\[_{n}(_{m}[l(m),u(m)][ (m),(m)]) 1-.\] (4)

To this end, we will first use prediction-powered inference [37; 38] to construct a confidence ellipsoid that, with probability \(1-\), is guaranteed to contain the (column) vector of (true) probabilities \(=(_{m})_{m}\). Then, we will use the distance between this ellipsoid and the hyperplanes under which each pair of models \(m,\) have the same probability values of being preferred over others, to efficiently construct the estimates \([(m),(m)]\) of the rank-sets.

## 3 Constructing Confidence Regions with Prediction-Powered Inference

Let the set \(_{N}=\{(q_{i},r_{i},r^{}_{i},m(r_{i}),m(r^{}_{i}),_{i},^{}_{i})\}_{i=1}^{N}\) comprise pairwise comparisons by a strong LLM to \(N\) inputs and the set \(_{n}=\{(q_{i},r_{i},r^{}_{i},m(r_{i}),m(r^{}_{i}),w_{i},w^{}_{i},_{i},^{}_{i})\}_{i=1}^{n}\) comprise pairwise comparisons by the same strong LLM and by humans to \(n\) inputs, with \(n N\). In what follows, for each pairwise comparison, we will refer to the models \(m(r)\) and \(m(r^{})\) that provided the first and second output using one-hot (column) vectors \(\) and \(^{}\), respectively. Moreover, to summarize the pairwise comparisons2 in \(_{N}\) and \(_{n}\), we will stack the one-hot vectors \(\) and \(^{}\) into four matrices, \(_{N}\) and \(^{}_{N}\) for \(_{N}\) and \(_{n}\) and \(^{}_{n}\) for \(_{n}\), where each column corresponds to a one-hot vector, and the indicators \(w\) and \(\) into six (column) vectors, \(}_{N}\) and \(}^{}_{N}\) for \(_{N}\) and \(}_{n}\), \(}^{}_{n}\), \(_{n}\) and \(^{}_{n}\) for \(_{n}\).

Then, building upon the recent framework of prediction-powered inference , we compute an unbiased estimate \(}\) of the vector of (true) probabilities \(\):

\[}=_{k}((_{ N}+_{N}^{})_{N})^{}_{k} )^{-1}(_{N}}_{N}+_{N}^{} }_{N}^{})}_{}\\ -_{k}((_{n}+_{n}^{ })_{n})^{}_{k})^{-1}( _{n}(}_{n}-_{n})+_{n}^{}( }_{n}^{}-_{n}^{}))}_{},\] (5)

where \(_{d}\) denotes a \(d\)-dimensional column vector where each dimension has value \(1\) and \(_{k}\) denotes a \(k\)-dimensional identity matrix. Here, note that the first term \(\) utilizes the pairwise comparisons by the strong LLM from \(_{N}\) to compute an unbiased estimate of the vector of probabilities \(}\) defined in Eq. 3 using sample averages, and the second term \(\) utilizes the pairwise comparisons by the strong LLM and by humans from \(_{n}\) to compute an unbiased estimate of the difference of probabilities \(-}\) defined in Eqs. 1 and 3, also using sample averages. The parameter \(\) weighs the comparisons \(},}^{}\) differently than the comparisons \(,^{}\). Details on why this can be useful and on the selection of \(\) are in Appendix B.2.

Further, as shown in Angelopoulos et al. , the difference of probabilities \(}-\) converges in distribution to a \(k\)-dimensional normal \(_{k}(0,)\), where \(=[(}-)(}- {})^{}]\), and thus the confidence region

\[_{}=\{^{k}\,|\,(-})^{}(}^{-1}}{_{k,1- }^{2}})(-}) 1\},\] (6)

where \(}\) is an empirical estimate of the covariance matrix \(\) using pairwise comparisons from \(_{N}\) and \(_{n}\), _i.e._,

\[}=}^{}+} ^{},\] (7)

with

\[ =((_{k}(}_{N}-_{N}^{} )^{})_{N}+(_{k}(}_{N}^ {}-_{N}^{})^{})_{N}^{}),\] \[ =(_{k}(}_{n}-_{n}-_{n}^{ })^{})_{n}+(_{k}( }_{n}^{}-_{n}^{}-_{n}^{})^{} )_{n}^{},\]

and \(_{k,1-}^{2}\) is the \(1-\) quantile of the \(^{2}\) distribution with \(k\) degrees of freedom, satisfies that

\[_{n}(_{})=1-.\] (8)

Algorithm 1 summarizes the overall procedure to compute \(}\) and \(}\), which runs in \(O(k^{2}(N+n))\) time.

## 4 Constructing Rank-Sets with Coverage Guarantees

For each pair of models \(m,\) such that \(m\), we first define a hyperplane \(H_{m,}^{k}\) as follows:

\[H_{m,}=\{^{k}\,|\,x_{m}=x_{}\}.\] (9)

Then, for each of these hyperplanes \(H_{m,}\), we calculate the distance \(d(_{},H_{m,})\) between \(H_{m,}\) and the confidence region \(_{}\) defined by Eq. 6, _i.e._,

\[d(_{},H_{m,})=_{m}-_{ }|-_{m,m}+_{,}-2_{m,})_{k,1-}^{2}}}{},\] (10)

where \(}\) is the empirical covariance matrix defined by Eq. 7.

Now, for each pair of models \(m,m^{}\), we can readily conclude that, if the distance \(d(_{},H_{m,})>0\), then, the confidence region \(_{}\) either lies in the half-space of \(^{k}\) where \(x_{m}>x_{}\) if \(_{m}>_{}\) or it lies in the half space of \(^{k}\) where \(x_{m}<x_{}\) if \(_{m}<_{}\). Building upon this observation, for each model \(m\), we construct the following estimates \([(m),(m)]\) of the rank-sets \([l(m),u(m)]\):

\[(m) =1+_{\{m\}}\{d(_{},H_{m,})>0\}\{_{m}<_{ }\}\] (11) \[(m) =k-_{\{m\}}\{d(_{},H_{m,})>0\}\{_{m}>_{ }\}.\]

Importantly, using a similar proof technique as in Lemma 1 in Neuhof and Benjamini , we can show that the above rank-sets estimates enjoy provable coverage guarantees with respect to the rank-sets \([l(m),u(m)]\) induced by the probabilities \(\) that the outputs of each model is preferred over any other model by humans (proven in Appendix A):

**Theorem 4.1**: _The estimates \([(m),(m)]\) of the rank-sets defined by Eq. 11 satisfy that_

\[_{n}(_{m}[l(m),u(m)][(m),(m)]) 1-.\] (12)

Algorithm 2 summarizes the overall procedure to construct the rank-sets \([(m),(m)]\) for all \(m\), which runs in \(O(k^{2}(N+n))\).

Experiments

We apply our framework to construct rank-sets for \(12\) popular LLMs using pairwise comparisons made by humans in the LMSYS Chatbot Arena platform3 and pairwise comparisons made by three strong LLMs. We show that the rank-sets constructed using our framework are significantly more likely to cover the true ranking consistent with (the distribution of) human pairwise preferences than the rank-sets constructed using only pairwise comparisons made by the strong LLMs.

**Experimental setup.** Our starting point is the Chatbot Arena dataset , which comprises \(33{,}481\) pairwise comparisons made by \(13{,}383\) humans about the responses given by \(20\) different LLMs to \(26{,}968\) unique queries. In what follows, we refer to each pair of responses to a query by two different LLMs and the query itself as an instance. As an initial pre-processing, we filter out any instance whose corresponding query is flagged as toxic or multiturn. Then, we gather pairwise comparisons made by three strong LLMs, namely GPT-\(3{.}5\)-turbo-\(0125\) (Gpt3.5), GPT-\(40{125}\)-preview (Gpt4) and Claude-\(3\)-Opus-\(20240229\) (C1.3), about all the (pre-processed) instances from the Chatbot Arena dataset. To this end, we use (almost) the same prompt as in Zheng et al. , which instructs each strong LLM to output option 'A' ('B') if it prefers the response of first (second) LLM, or option 'C' if it declares a tie. Further, we filter out any instances for which at least one strong LLM provides a verbose output instead of 'A', 'B', or 'C', and focus on a set of LLMs with at least \(96\) pairwise comparisons between every pair of LLMs in the set. After these pre-processing steps, we have \(14{,}947\) instances comprising \(13{,}697\) unique queries and \(12\) different LLMs and, for each instance, we have one pairwise comparison made by a human and three pairwise comparisons by the three strong LLMs. Refer to Appendix C for more information regarding the \(12\) LLMs, the number of pairwise comparisons between every pair of LLMs, and the prompt used to gather pairwise comparisons made by the three strong LLMs.

To draw reliable conclusions, in each experiment, we construct rank-sets \(1{,}000\) times and, each time, we use a random set of \(N+n=6{,}336\) instances with an equal number of instances per pair of models, out of the \(14{,}947\) instances. The values of \(N\) and \(n\) vary across experiments and they define two random subsets, also with an equal number of instances per pair of models.

**Methods.** In our experiments, we construct rank-sets using the following methods:

1. Baseline: it constructs (unbiased) rank-sets using the pairwise comparisons made by humans corresponding to the random set of \(N+n\) instances via Algorithms 2 and 3, shown in Appendix B.1. The constructed rank-sets are presumably likely to cover the true rank-sets.
2. Llm Gpt4, Llm Gpt3.5 and Llm Cl3: they construct (possibly biased) rank-sets using the pairwise comparisons made by one of the three strong LLMs corresponding to the random set of \(N+n\) instances via Algorithms 2 and 3.
3. Ppr Gpt4, Ppr Gpt3.5 and Ppr Cl3: they construct (unbiased) rank-sets using pairwise comparisons made by one of the three strong LLMs corresponding to the random set of \(N+n\) instances and pairwise comparisons made by humans corresponding to the random subset of \(n\) instances via Algorithms 1 and 2.
4. Human Only: it constructs (unbiased) rank-sets using the pairwise comparisons made by humans corresponding to the random subset of \(n\) instances via Algorithms 2 and 3.

In the above, note that a), b) and d) use linear regression to construct a confidence region \(_{}\) using only pairwise comparisons by either humans or a strong LLM via Algorithm 3, which runs in \(O(k^{2}(N+n))\) in a)-b) and in \(O(k^{2}n)\) in d), and then use this confidence region to construct the rank-sets via Algorithm 2.

**Quality metrics.** Since the true probabilities \(\) are unknown, we cannot compute the true rank-sets of the \(12\) LLMs under comparison, which presumably may be singletons. As a result, we cannot estimate the (empirical) coverage probability--the probability that the rank-sets constructed using the above methods cover the true rank-sets, which Theorem 4.1 refers to. To overcome this, we assess the quality of the rank-sets using two alternative metrics: rank-set size and baseline intersection probability. Here, smaller (larger) rank-set sizes and larger (smaller) intersection probabilities are better (worse). The baseline intersection probability is just the (empirical) probability that the rank-sets \([(m),(m)]\)constructed using one of the above methods intersect with the rank-sets \([(m),(m)]\) constructed using the Baseline method, _i.e._, \((_{m}\{[(m),(m)][(m),(m)]\})\). Intuitively, we expect that the larger the baseline intersection probability, the larger the coverage probability since the Baseline method uses a large(r) number of pairwise comparisons by humans to construct (unbiased) rank-sets and thus it is expected to approximate well the true rank-sets. Further, note that the baseline intersection probability tells us how frequently there exists at least one single ranking covered by both one of the above methods and the Baseline method. In Appendix D.2, we experiment with an alternative metric, namely baseline coverage probability, which is the (empirical) probability that the rank-sets constructed using one of the above methods covers the rank-sets constructed using the Baseline method. In Appendix E, we additionally evaluate our framework in a synthetic setting where the true rank-sets are known, allowing us to compute the coverage probability and rank-biased overlap (RBO) .

**Quality of the rank-sets.** Figure 1 shows the average rank-set size against the baseline intersection probability for rank-sets constructed using all methods4 except Baseline for different \(\) values5 and \(n=990\). The results show several interesting insights. First, we find that rank-sets constructed using only pairwise comparisons by a strong LLM (Llm Gpt4, Llm Gpt3.5 and Llm Cl3) achieve much lower baseline intersection probability, even orders of magnitude lower, than those constructed using only pairwise comparisons by humans (Human Only) or using both pairwise comparisons by a

Figure 1: Average rank-set size against baseline intersection probability for rank-sets constructed using only pairwise comparisons by a strong LLM (Llm Gpt4, Llm Gpt3.5 and Llm Cl3), only pairwise comparisons by humans (Human Only), and pairwise comparisons by both a strong LLM and humans (Ppr Gpt4, Ppr Gpt3.5 and Ppr Cl3) for different values of \(\) and \(n=990\). Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, \(95\%\) confidence bars for the rank-set size are not shown, as they are always below \(0.02\).

Figure 2: Average rank-set size against baseline intersection probability for rank-sets constructed using pairwise comparisons by both a strong LLM and humans for different values of \(n\) and \(\). Smaller (larger) average rank-set sizes and larger (smaller) intersection probabilities are better (worse). In all panels, \(95\%\) confidence bars for the rank-set size are not shown, as they are always below \(0.04\).

strong LLM and humans (Ppr Gpt4, Ppr Gpt3.5 and Ppr Cl3). This suggests that the distributions of pairwise comparisons by strong LLMs and humans are actually different, questioning the rationale used by an extensive line of work that proposed using only pairwise comparisons by strong LLMs to rank LLMs . Second, we find that rank-sets constructed using both pairwise comparisons by two of the strong LLMs and humans (Ppr Gpt4 and Ppr Cl3) achieve a better trade-off between average rank-set size and baseline intersection probability than those constructed using only pairwise comparisons by humans (Human Only). This suggests that pairwise comparisons by strong LLMs are valuable if they are complemented with (a few) pairwise comparisons by humans. Third, we find that, among the three strong LLMs, GPT 4 stands out as the best performer.

Figure 2 shows the average rank-set size against the baseline intersection probability for rank-sets constructed using Ppr Gpt4, Ppr Gpt3.5 and Ppr Cl3 for different values of \(n\) and \(\) (the same values as in Figure 1).6 The results show that the trade-off between rank-sets and baseline intersection probabilities improves rapidly as the number of pairwise comparisons by humans \(n\) increases but with diminishing returns.

**Structure of the rank-sets.** In this section, we take a closer look to the structure of the rank-sets constructed using Baseline, Llm Gpt4 and Ppr Gpt4. In Appendix D.3, we include additional results for all other methods.

First, we compute the empirical probability that each ranking position is included in the rank-sets constructed by Baseline, Llm Gpt4 and Ppr Gpt4 of each of the LLMs under comparison. Figure 3 summarizes the results for \(n=990\) and \(=0.05\), which reveal several interesting insights. We find that there is lower uncertainty regarding the ranking position of each model for Llm Gpt4 than for Ppr Gpt4. However, for Llm Gpt4, the ranking position with the highest probability mass differs from Baseline in 7 out of 12 LLMs, including the top-2 performers. In contrast, for Ppr Gpt4, it only differs from Baseline in 3 out of 12 LLMs. This questions once more the status quo, which proposed using only pairwise comparisons by strong LLMs to rank LLMs .

Figure 4: Empirical probability of each rank-set constructed by Baseline, Llm Gpt4 and Ppr Gpt4 for GPT 4 (left), Claude 1 (middle left), Vicuna (middle right) and PaLM 2 (right). In all panels, \(n=990\) and \(=0.05\).

Figure 3: Empirical probability that each ranking position is included in the rank-sets constructed by Baseline, Llm Gpt4 and Ppr Gpt4 for each of the LLMs under comparison. In all panels, \(n=990\) and \(=0.05\). Larger (smaller) dots indicate higher (lower) empirical probability.

Next, we compute the empirical probability of each rank-set constructed by Baseline, Llm Gpt4 and PPR Gpt4 for each of the LLMs under comparison. Figure 4 summarizes the results for GPT 4, Claude 1, Vicuna and PaLM 2 for \(n=990\) and \(=0.05\). In agreement with the findings derived from Figure 3, we observe that the distribution of rank-sets constructed by Llm Gpt4 is more concentrated than the distribution of rank sets constructed by PPR Gpt4. However, the rank-sets with the highest probability mass constructed by Llm Gpt4 coincide with those constructed by Baseline much less frequently than those constructed by PPR Gpt4. Refer to Appendix D.3 for qualitatively similar results for other LLMs.

## 6 Discussion and Limitations

In this section, we highlight several limitations of our work, discuss its broader impact, and propose avenues for future work.

**Data.** Our framework assumes that the queries and the pairwise comparisons made by humans and the strong LLMs are drawn i.i.d. from fixed distributions. In future work, it would be very interesting to lift these assumptions and allow for distribution shift. Moreover, our framework assumes that the pairwise comparisons made by humans are truthful. However, an adversary could have an economic incentive to make pairwise comparisons strategically in order to favor a specific model over others. In this context, it would be interesting to extend our framework so that it is robust to strategic behavior.

**Methodology.** Our framework utilizes rank-sets as a measure of uncertainty in rankings. However, in case of limited pairwise comparison data, rank-sets may be large and overlapping, reducing their value. In such situations, it may be worthwhile to explore other measures of uncertainty for rankings beyond rank-sets. Further, to measure the level of alignment with human preferences, our framework utilizes the win-rate--the probability that the outputs of each model are preferred over the outputs of any other model picked uniformly at random. However, if we need to rank \(k\) LLMs and \(k\) is _large_, win-rate may be impractical since, to obtain reliable estimates, we need to gather \(O(k^{2})\) pairwise comparisons made by humans. Finally, our framework constructs rank-sets with asymptotic coverage guarantees, however, it would be interesting to derive PAC-style, finite-sample coverage guarantees.

**Evaluation.** We have showcased our framework using pairwise comparisons made by humans in a single platform, namely LMSYS Chatbot Arena, and pairwise comparisons made by just three strong LLMs. As a result, one may question the generalizability of the conclusion derived from the rank-sets estimated using our framework. In this context, it is also important to acknowledge that, in LMSYS Chatbot Arena, the queries are chosen by the humans who make pairwise comparisons and this may introduce a variety of biases. Therefore, it would be interesting to apply our framework to human data from other platforms.

**Broader Impact.** Our framework rank LLMs according to their level of alignment with human preferences--a LLM is ranked higher than others if its outputs are more frequently preferred by humans. However, in many application domains, especially in high-stakes scenarios, it may be important to account for other important factors such as accuracy, fairness, bias and toxicity .

## 7 Conclusions

We have introduced a statistical framework to construct a ranking of a collection of LLMs consistent with their level of alignment with human preferences using a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a strong LLM. Our framework quantifies uncertainty in the ranking by providing a rank-set--a set of possible ranking positions--for each of the models under comparison. Moreover, it guarantees that, with a probability greater than or equal to a user-specific value, the rank-sets cover the ranking consistent with the (true) probability that each model is preferred over others by humans asymptotically. Finally, we have empirically demonstrated that the rank-sets constructed using our framework are more likely to cover the true ranking consistent with (the distribution of) human pairwise preferences than the rank-sets constructed using only pairwise comparisons made by the strong LLMs.