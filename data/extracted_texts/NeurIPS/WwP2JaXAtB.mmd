# Probabilistic Invariant Learning with

Randomized Linear Classifiers

 Leonardo Cotta

Vector Institute

leonardo.cotta@vectorinstitute.ai &Gal Yehuda

Technion, Haifa, Israel

ygal@cs.technion.ac.il &Assaf Schuster

Technion, Haifa, Israel

assaf@technion.ac.il &Chris J. Maddison

University of Toronto and Vector Institute

cmaddis@cs.toronto.edu

###### Abstract

Designing models that are both expressive and preserve known invariances of tasks is an increasingly hard problem. Existing solutions tradeoff invariance for computational or memory resources. In this work, we show how to leverage randomness and design models that are both expressive and invariant but use less resources. Inspired by randomized algorithms, our key insight is that accepting probabilistic notions of universal approximation and invariance can reduce our resource requirements. More specifically, we propose a class of binary classification models called Randomized Linear Classifiers (RLCs). We give parameter and sample size conditions in which RLCs can, with high probability, approximate any (smooth) function while preserving invariance to compact group transformations. Leveraging this result, we design three RLCs that are provably probabilistic invariant for classification tasks over _sets, graphs, and spherical_ data. We show how these models can achieve probabilistic invariance and universality using less resources than (deterministic) neural networks and their invariant counterparts. Finally, we empirically demonstrate the benefits of this new class of models on invariant tasks where deterministic invariant neural networks are known to struggle.

## 1 Introduction

A modern challenge in machine learning is designing model classes for invariant tasks that are both universal and resource-efficient. Consider designing an architecture for graph problems that is invariant under graph isomorphism. Ensuring that this class is universal is at least as hard as solving the graph isomorphism problem . Similarly, permutation invariance universality comes at the cost of memory--the number of parameters scales with the sequence size .

Using randomness as a computational resource for algorithms is a fundamental idea in computer science. In their essence, randomized algorithms often provide simple solutions that use less memory or run in less time than their deterministic counterparts . Since there is no free lunch, these gains come at the cost of accepting a probabilistic notion of correctness, _i.e._, while deterministic algorithms always output correct answers, their randomized versions will be right only with high probability. Inspired by this, the key insight in our paper is using an external source of randomness to reduce time and space resources when learning an invariant task. To benefit from that, just as in randomized algorithms, we also need to introduce a probabilistic notion of invariance and expressive power. By guaranteeing invariance and universal approximation only with high probability, we show when it is possible to avoid the invariance-resource tradeoff.

We introduce a class of models for binary classification tasks named _Randomized Linear Classifiers_ (RLCs). In an RLC the external source of randomness is input to a neural network that generates a random linear classifier --independently from the input-- which is then used to make a prediction. To guarantee correctness with high probability, we need to repeat this process for a sufficient number of times, _i.e._, take multiple samples, and predict the majority of their output-- this is called amplification. The key features of RLCs are both i) randomness independent from input and ii) linear transformation of the input. Combined, these features provide both theoretical and practical simplicity. On the practical side, RLCs can offload computation and preserve privacy in resource-constrained devices. We highlight the benefits of RLCs in such applications below.

1. _Online computation._ Suppose that our goal is to answer classification queries as quickly as possible, or that the queries arrive in an "online" fashion. By using RLCs, we can sample several linear classifiers "offline" (before observing new inputs), and then in the online phase simply take the majority of a few (different) linear classifiers for each input. This way, the majority of computations can be done offline rather than online --as usual in existing online learning settings.
2. _Private computation._ Suppose that we wish to peform inference with an RLC, but we only have a low-resource computer (_e.g._, a smartwatch). A practical way of doing so is to perform the sampling on a remote server, then retrieve the answer. In our approach, the server can send the randomness, _i.e._, the sampled linear classifier coefficients. There is no communication between the client and the server at all, and the client never sends the input --retaining its privacy. Moreover, since the client only needs to perform a few linear computations, it can be a low-resource computer.

The theoretical advantages of RLCs' features are explored in this work. We consider the required resources of an RLC to be universal and invariant for a given task as the number of parameters used by its neural network and the sufficient number of samples (amplification size). Then, we focus on establishing upper-bounds for these resources in different scenarios. More specifically, our contributions are three-fold:

1. We introduce the general class of Randomized Linear Classifiers (RLCs), presenting a universal approximation theorem coupled with a resource consumption characterization (Theorem 1). In specific, we show that any binary classification problem with a smooth boundary can be approximated with high probability by RLCs with at most the same number of parameters as a deterministic neural network. _This is a result of general purpose and holds for any invariant or non-invariant task._
2. We show how the problem of designing RLCs invariant to compact group transformations can be cast to designing invariant distributions of linear classifiers. As a result, we can leverage representation theorems in probability theory, such as de Finetti's , Aldous-Hoover's and Friedman's  to design invariant RLCs for tasks with set and graph data. Moreover, in Appendix B we also show a simple invariant model for spherical data using these ideas.
3. Finally, we show both theoretically and empirically how invariant RLCs can succeed in tasks where deterministic architectures, such as Deep Sets and GNNs, cannot efficiently approximate. By introducing probabilistic notions of universality and invariance, our work establishes the first alternative to avoid the invariance-resource tradeoff with theoretical guarantees and empirical success.

### Related Work

Here, we briefly review Coin-Flipping Neural Networks (CFNNs) and invariant representation learning --two core concepts that we build upon. Finally, we explain how Bayesian neural networks and RLCs belong to different learning paradigms.

**Coin-flipping neural networks.** CFNNs were presented by , where the authors propose several deep learning architectures using randomness as part of their inference. A few (constructive) examples were shown in which randomness can help by reducing the number of parameters or the depth of the network. In this paper we focus on the case of learning randomized linear classifiers and its use in invariant learning. The invariant models were not explored in ,and to the best of our knowledge this is the first work studying randomized models for invariant learning. One specific CFNN model proposed in  is the Hypernetwork CFNN. In this model, the weights of a model are first randomly generated. Then, the computation of a deterministic neural network using these random weights takes place. In this work, we focus on the "extreme" case of this paradigm: we allow our model to use arbitrarily complex distributions to generate our final weights, but we insist that the deterministic part --using such weights-- would be a _linear_ function. That is, we push the computational complexity to the random part of the model. This gives rise to the class of models we study: Randomized Linear Classifiers (RLCs). The work of  considered a first version of RLCs, but did not provide any empirical results and their theoretical results hold only for single dimensional input (_i.e._, in \(\)). Hence, to the best of our knowledge our work also establishes the first (general purpose) universal approximation theorem of (non-invariant) RLCs (_cf._ Section 2).

Invariant representation learning.Over the last years, there has been a growing interest in designing \(G\)-invariant architectures [4; 6; 18; 35]. In summary, we can often leverage an a priori knowledge that the function to be learned is invariant to the action of a group \(G\) by forcing our model to also be invariant to it. That is, \(G\)-invariant architectures incorporate a known propriety of the target. Such a reduced model space is known to both empirically and theoretically achieve better generalization [4; 6; 11; 19]. Existing works study the \(G\)-invariant function space and how to parameterize it with neural networks. Despite some empirical success, invariance guarantees often come at unreasonable prices. For instance, CNNs are guaranteed to be invariant to translations only with infinite grids (images) . Regarding permutation invariance, Deep Sets is always invariant, but to remain universal it requires a hidden layer as large as the input set . Finally, GNNs, as any other computationally efficient graph isomorphism-invariant architecture, cannot achieve universal approximation. More specifically, achieving graph isomorphism invariance and universal approximation is at least as hard as solving the graph isomorphism problem . In contrast to these results, our work shows that such tradeoffs are usually restricted to the design of deterministic architectures. By accepting probabilistic notions of invariance and universality, we show that under mild data generation conditions we can circumvent these invariance-resource tradeoffs.

Bayesian neural networks.Bayesian Neural Networks (BNNs) incorporate Bayesian inference into both training and inference of neural networks. In short, BNNs explicitly assign a prior to the parameters and perform inference with a posterior. BNNs differ from RLCs in how randomness is used. BNNs explicitly model prior and posterior distributions over the weights. This is usually leveraged in the context of confidence estimation. On the other hand, RLCs do not model a prior or a posterior distribution over the parameters. Rather, RLCs take samples from a model that uses external randomness. In their essence, BNNs and RLCs are trying to achieve different goals. While BNNs are modeling uncertainty over decisions, RLCs are trying to output the correct target with the highest probability they can.

## 2 Randomized Linear Classifiers

We start by introducing the general class of Randomized Linear Classifiers. Due to our universal approximation result, this can be seen as the analogous of a multi-layer perceptron in the context of randomized linear models. In the next sections we will introduce the analogous of \(G\)-invariant models.

For notation simplicity, we denote the random variable of a value with bold letters, _e.g._, a random variable **x** would have a realization \(x\). We consider binary classification tasks where the input data **x** is supported on \(:=()^{d}\) and labeled according to a true labeling function \(y\{-1,1\}\). Therefore, a binary classification task can be summarized by the tuple \((,y)\).

Given a source of randomness **u** supported on \(:=()^{d_{u}}\), and a neural network \(f_{}^{d_{u}}^{d+1}\) parameterized by \(^{d}\), an RLC predicts \(y(x)\) for \(x\) according to

\[(_{},x-_{}):= ( f_{}()_{1:d},x-f_{}( )_{d+1}).\] (1)

From above, one can see the coefficients \((_{},_{})\) of the linear classifier given by the pushforward measure \(f_{}\#\). Finally, unless otherwise stated, we assume \(f_{}\) to be a multi-layer perceptron with ReLU activations.

Analogously to randomized algorithms for decision problems, we can take multiple samples using Equation (1) and make a final prediction using their majority. To simplify notation, let us define the function of our final prediction taking \(m\) samples as

\[_{}^{(m)}(x):=(\{(_{ }^{(j)},x-_{}^{(j)})\}_{j=1}^{m})=(\{ ( f_{}(^{(j)})_{1:d},x-f_{}( ^{(j)})_{d+1})\}_{j=1}^{m}),\] (2)

where each \(^{(j)}_{,,}P()\). Note that as we take more samples, we approach the mode of the distribution of linear predictions for a given \(x\). We define this value as the limiting classification of \(x\), which implies the existence of the limiting classifier \(_{}^{d}\{-1,1\}\) with

\[_{}(x):=_{m}\,(\{( _{}^{(j)},x-_{}^{(j)})\}_{j=1}^ {m})=*{argmax}_{\{-1,1\}}P(( _{},x-_{})=).\] (3)

At this point, it is worth noting that although the random prediction (Equation (1)) is a linear function, its limiting classifier (Equation (3)) can induce an arbitrarily non-linear boundary. In fact, we leverage this notion to define the probabilistic version of universality. **We say that an RLC is universal in a probabilistic sense if its limiting classifier (Equation (3)) is universal.** It is easy to see that models making deterministic predictions collapse the notions of probabilistic and exact universality. On the other hand, for RLCs to be probabilistic universal it suffices to produce predictions that are biased towards the desired answer on every input, _i.e._, \(P(_{}^{(m)}(x)=y(x))>0.5, x\).

In practice, we will make random predictions and therefore we need to capture a sufficient sample size \(m\) such that we are close to the limiting classifier with high probability. An important measure to understand how large \(m\) needs to be is what we call the minimum bias of the RLC. We denote it by \(\) and define it as the infimum of the total variation distances between the random predictions of every input \(x\) and a random variable \(\) with Radamacher distribution. That is,

\[:=\{d_{}(_{}^{(m)}(x),)  x\}.\]

Put into words, \(\) captures a lower-bound on how close to a random prediction the RLC can be. And, naturally, the closer it is to a random prediction the larger \(m\) needs to be such that the majority converges to the limiting prediction. Therefore, our universal approximation result needs to capture how many parameters an RLC needs to converge to correct answers and how fast, _i.e._, with respect to \(m\), it does. Before we proceed with our results on \(m\), \(p\) and the universality property, we need to define the general class of binary classification tasks we consider.

**Assumption 1** (Tasks with smooth separators).: _We say that a binary classification task \((,y)\) admits a smooth separator if there exists a neural network \(s^{d}\) such that \(y(x)=(s(x)), x(X)\)._

Note that due to the universal approximation ability of neural networks, the set of tasks satisfying Assumption 1 is quite large. Moreover, **Assumption 1 is a sufficient but not necessary condition**. We will later prove universality for a wide class of graph problems and we believe many others exist. For now, let us finally state our result on the resource consumption and universal approximation of RLCs in tasks with smooth separators.

**Assumption 2** (Absolutely continuous randomness source).: _We say that the randomness source \(\) is absolutely continuous if at least one of its coordinates \(_{i},1 i d_{}\) has an absolutely continuous marginal distribution._

**Theorem 1** (Resource consumption of universal RLCs).: _Let \((,y)\) be a binary classification task that admits a smooth separator as in Assumption 1. Then, there exists an RLC with neural network \(f_{^{*}}\) and absolutely continuous randomness source \(\) (Assumption 2) that is universal in the limit, i.e.,_

\[_{^{*}}(x)=y(x), x,\]

_and makes random predictions that are correct with probability_

\[P((\{(_{^{*}}^{(j)},x- _{^{*}}^{(j)})\}_{j=1}^{m})=y(x))>1-\{-2^{2}m^{2}\},\]

_where \(\) is the minimum bias of \(_{^{*}}\)._

_Further, if \(p^{}\) is the number of parameters used by a deterministic neural network with one hidden layer to achieve zero-error in the task, \(f_{}\) has at most_

\[p p^{}+(1)\,.\]The complete proof is in Appendix A Note that Theorem 1 does not assume or explore any invariance in the task. It is a general universality result that we will leverage in our study of invariant RLCs, but of independent interest. One way to interpret Theorem 1 is as a probabilistic version of the classical universal approximation theorem of multi-layer perceptrons . The result on the number of parameters is important to make it clear that RLCs do not need to find a solution more complex than one given by a deterministic model in the supervised learning task.

Until now, we have established the expressive power of RLCs, _i.e._, that, under mild assumptions, they can be as expressive and resource-efficient as deterministic neural networks. But, there is still the general question: When are RLCs more resource-efficient than deterministic neural networks? In  it was constructed an specific task where an RLC (although the authors do not name it as such) uses a constant number of parameters, while a deterministic neural network would need a number of parameters that grows with the input dimension. Here, we are interested in more general settings. In the next section we will show a large class of invariant tasks where invariant RLCs are more resource-efficient than their invariant deterministic neural network counterparts.

## 3 \(G\)-invariant Randomized Linear Classifiers

Now, we turn our focus to binary classification tasks that are invariant to compact group transformations. That is, let \(G\) be a compact group with an action of \(g G\) on \(x\) denoted by \(g x\). A task \((,y)\) is \(G\)-invariant if \(y(x)=y(g x), x, g G\). It is easy to see that an RLC that perfectly classifies the task in the probabilistic notion, _i.e._, \(_{^{*}}(x)=y(x), x\), will also be probabilistic \(G\)-invariant, _i.e._, \(_{^{*}}(x)=_{^{*}}(g x), x , g G\). Thus, if we know that a task is \(G\)-invariant, can we restrict our model class to \(G\)-invariant RLCs? At first, the answer is not obvious. We need a design principle that guarantees a search space with only \(G\)-invariant solutions while at least one of them has zero error. In Theorem 2 we present the result that guides our design of invariant RLCs.

**Theorem 2** (\(G\)-invariant RLCs).: _Let \((,y)\) be a \(G\)-invariant task with a smooth separator as in Assumption 1. Then, the set of RLCs with a \(G\)-invariant distribution in the classifier weights, i.e.,_

\[_{}}{{=}}g_{},  g G,\]

_and absolutely continuous randomness source (cf. Assumption 2) is both probabilistic \(G\)-invariant and universal in \((,y)\). That is,_

\[_{}(x)=_{}(g x), x, g G,^{p},\]

_and_

\[^{}^{p}_{^{*}}(x)=y(x),  x, g G.\]

The complete proof is in Appendix A From above, we now know that designing universal RLCs for invariant tasks can be cast to the problem of designing universal \(G\)-invariant distributions --together with a separate (possibly dependent) bias distribution. This design principle differs fundamentally from deterministic invariant representation learning. Here, instead of designing architectures that are \(G\)-invariant, we wish to design architectures that induce a \(G\)-invariant distribution. That is, the RLC neural network \(f_{}\) is not itself \(G\)-invariant.

Next, we will define a very useful assumption about the data generation process. It will allow us to leverage representation results in probability theory to both design universal and efficient \(G\)-invariant distributions and allow for variable-size set and graph data.

**Assumption 3** (Infinitely \(G\)-invariant data).: _Let \(G_{}\) be a homomorphism of \(G\) into the composition of homomorphisms of \(G\) in all finite dimensions, e.g., if \(G\) is the set of permutations of \(\{1,,d\}\), \(G_{}\) is the set of permutations of \(\). We say that a task \((,y)\) has infinitely \(G\)-invariant data if there exists an infinite sequence of random variables \((_{i})_{i=1}^{}}{{=}}g_{} (_{i})_{i=1}^{}, g_{} G_{}\), where \(}{{=}}(_{i})_{i S}\) for \(S}{d}\), with \(}{d}\) being the set of all \(d\)-size subsets of \(\)._

Although the above might seem unintuitive, the representation theorems also help us make sense of them. The data generation process will take the same form as our linear weights distribution. As we will see, for sets it means that items are sampled i.i.d. given a common latent factor of the set . In graphs edges are also generated i.i.d. given latent factors of their endpoints and the graph .

**Proposition 1** (Infinitely \(G\)-invariant RLCs).: _Let \((,y)\) be a \(G\)-invariant task with infinitely \(G\)-invariant data (Assumption 3) with a smooth separator as in Assumption 1. Then, the set of RLCs with an infinitely \(G\)-invariant distribution in the linear classifier weights, i.e., as in Assumption 3 \((_{_{i}})_{i=1}^{}}{{=}}g_{ }(_{_{i}})_{i=1}^{}, g_{} G_{}\), where \(_{}}{{=}}(_{ i})_{i S },\) for \(S}{d}\), and absolutely continuous randomness source (cf. Assumption 2) is probabilistic \(G\)-invariant and universal for \((,y)\) as in Theorem 2._

The complete proof is in Appendix A Next, we design \(G\)-invariant RLCs drawing from results in the representation theory of probability distributions. We consider tasks with set and graph data. In Appendix B we also consider spherical data. For each, we derive conditions for \(G\)-invariance and universality, highlighting their resource gain when compared to their deterministic counterpart.

### RLCs for set data

Here we consider tasks that are invariant to permutations, _i.e._, our input is a set1 of vectors. More formally, we let our input be supported on \(:=(^{k})^{d2}\) and \(G:=([d])\) be the group of permutations of \([d]:=\{1,,d\}\). Then, for an input \(x\) the action \(g x\) is given by permuting the \(d\) vectors of size \(k\) according to \(g\).

The key insight to design RLCs for set data is leveraging the classic de Finetti's theorem . In summary, de Finetti showed how any infinite sequence of exchangeable random variables can be expressed as an infinite sequence of i.i.d. random variables conditioned on a common latent measure. Now, when Assumption 3 is true, de Finetti tells us that we can sample our weights \(_{}\) by first sampling a common noise and use it as input to the same function, which generates the linear weights together with an independent noise in an i.i.d. manner. Note that the bias does not have the exchangeability property. Thus, it needs to receive the same shared noise as the set but use a different function, since it is not necessarily sampled in the same way. Next, we formalize these notions by defining the class of Randomized Set Classifiers (RSetCs) and characterizing its universal approximation power together with resource consumption.

**Definition 1** (Randomized Set Classifiers (RSetCs)).: _A Randomized Set Classifier (RSetC) uses two neural networks \(f_{_{j}}:^{2}^{k}\) and \(g_{_{g}}:^{2}\) together with \(d+2\) sources of randomness: \(\), \((_{i})_{i=1}^{d}\), and \(_{b}\). The random linear classifier coefficients are generated with_

\[_{_{i}}^{(k)}}{{=}}f_{_{f}}( {u},_{i})_{}}{{=}}g_{_{g}}(,_{b}),\]

_where \(_{_{i}}^{(k)}\) is the \(i^{}\) chunk of \(k\) random linear weights, i.e., the weights multiplying the \(i^{}\) vector._

**Proposition 2** (Universality and resource consumption of RSetCs).: _Let \((,y)\) be a permutation-invariant task with a smooth separator as in Assumption 1 and infinitely permutation-invariant data as in Assumption 3. Then, RSetCs as in Definition 1 with absolutely continuous randomness source (cf. Assumption 2) are probabilistic \(G\)-invariant and universal for \((,y)\) (as in Theorem 2). Further, the number of parameters needed by RSetCs in this task will depend only on the smallest finite absolute moments of the weight and bias distributions in the zero-error solution, i.e., the ones given by \(f_{_{g}^{*}}(,_{i})\) and \(g_{_{g}^{*}}(,_{b})\)._

The complete proof is in Appendix A We highlight that Proposition 2 is combining de Finetti's theorem , Kallenberg's noise transfer theorem  and recent results on the capacity of neural networks to generate distributions from noise .

**What is the practical impact of Assumption 3 in set tasks?** Assumption 3 implies that an input set \(\) is generated by first sampling some latent variable \(\) which is then used to sample items in an i.i.d. manner \(_{1},_{2},|_{}P_{}.\). We can make sense of this with an example in e-commerce systems. If the set represents items in a shopping cart, the latent factor \(\) can be thought of as a summary of the user's shopping behavior. Overall, this assumption is not appropriate if there could be mutually exclusive items in the set.

**When are RSetCs better than Deep Sets?** We contrast the results of RSetCs with Deep Sets  due to its universality and computational efficiency . Overall, other deterministic set modelsare either variations of Deep Sets that inherent the same properties [20; 24] or are computationally inefficient . Apart from universality, it is important to note from Proposition 2 how the number of parameters used by the RSetC does not depend on the set size. Instead, its resource consumption is related to how smooth the distribution of linear classifiers weights and bias are. Since all the weights have the same distribution (conditioned on \(\)), the set size is not relevant. This comes in contrast to Deep Sets, which as shown in  only achieves universality when the hidden-layer and the input set have the same size.

Moreover, Deep Sets is known to have poor set size generalization abilities, _i.e._, when the test set contains larger sets than the training set . In RSetCs, although the number of parameters does not depend on the set size, the distribution of linear coefficients can use this information. Overall, the gain in robustness to set size might come from RSetCs performing simple linear transformations of the input. That is, for small changes in the set size, the output is not expected to change abruptly. This opposes to Deep Sets, where the representations of the items are aggregated and input to a neural network. Thus, any small addition to the set can arbitrarily change the output. Although we cannot always guarantee size generalization for RSetCs, we give empirical evidence in Section 4 that they do seem to be a lot more robust than Deep Sets.

### RLCs for graph data

Here we consider tasks that take graphs as input. Given a (possibly weighted) graph \(G=(V,E)\) with \(d\) vertices and \(M\) edges, we take as input a vector \(x\) of size \(d^{2}\) representing the vectorized adjacency matrix of \(G\). Our invariance is then to graph isomorphism. That is, we again have the permutation group \(G:=([d])\), but now with a different action. Here \(g x\) jointly permutes the rows and columns of the adjacency matrix corresponding to \(x\). This way, \(x\) and \(g x\) represent isomorphic graphs. We will then assume that our graph tasks are \(G\)-invariant, _i.e._, two isomorphic graphs \(x,g x\) will always have the same label.

Now, we can leverage Aldous-Hoover's theorem, the analogous of de Finetti's theorem for joint exchangeability in 2-dimensional arrays. It gives us a model, invariance and universality results analogous to the ones for RSetCs.

**Definition 2** (Randomized Graph Classifiers).: _A Randomized Graph Classifier (RGraphC) uses two neural networks \(f_{_{f}}:^{4}\) and \(g_{_{g}}:\) together with \(d^{2}+d+1\) sources of randomness: \(\), \((_{i}^{(n)})_{i=1}^{d}\), \((_{ij}^{(e)})_{i,j[d]}\), and \(_{b}\). The random linear classifier coefficients are generated with_

\[_{ ij}}{{=}}f_{_{f}}(,_{i}^{(n)},_{j}^{(n)},_{ij}^{(e)}),_{}}{{=}}f_{_{f}}(,_{b}),\]

_where \(_{_{ij}}\) is the random linear weight multiplying the entry of \(x\) that corresponds to the entry in row \(i\) and column \(j\) of the graph's adjacency matrix._

Note that we can use the same proof of Proposition 2 and have an equivalent result for RGraphCs. However, since graphs are discrete objects the smooth separator assumption (_cf._ Assumption 1) might not be so easily satisfied. To address this, we define a sufficiently large class of graph models that RGraphCs can approximate.

**Definition 3** (Inner-product decision graph problems).: _Let \(G=(V,E)\) be a graph over \(d\) vertices with vectorized adjacency matrix \(x:=()^{d^{2}}\). We say that a graph property \(y\{-1,1\}\) is an inner product verifiable property if there exists a set of vectors \(S^{d^{2}}\) and a constant \(b\) such that:_

* _For all graphs_ \(x^{+}\) _satisfying the property_ \(y(x^{+})=1\)_, there exists_ \(s S\)_, such that_ \( s,x^{+} b\)_;_
* _For all graphs_ \(x^{-}\) _not satisfying the property_ \(y(x^{-})=-1\)_, we have_ \( x^{-},s^{}<b, s^{} S\)_._

As an example of such property, consider connectivity. The set \(S\) above is defined to be the set of all binary vectors representing adjacencies of spanning trees. The threshold in this case would be \(d-1\). If \(G\) is connected, then there exists a spanning tree for \(G\). When taking the inner product of \(x\) and the vector \(s S\) representing the spanning tree, the result would be the number of edges in the tree, which is \(d-1\). In contrast, if \(G\) is not connected, it does not have a spanning tree. This means that for all vectors \(s^{}\) in \(S\), the inner product \( s^{},x\) is less than \(d-1\). We can use the same logic to show that properties arising from NP-complete problems such as independent set, or simple ones that GNNscannot approximate, such as diameter, girth and connectivity are all encompassed by Definition 3. We are now ready to state the probabilistic invariance and universality result for RGraphCs.

**Theorem 3**.: _Let \((,y)\) be a graph isomorphism-invariant task that either i) has a smooth separator as in Assumption 1 or ii) is an inner-product decision graph problem as in Definition 3. Further, the task has infinitely graph isomorphism-invariant data as in Assumption 3. Then, there exists an RGraphC as in Definition 2 with absolutely continuous randomness source (cf. Assumption 2) that is probabilistic \(G\)-invariant and universal for \((,y)\) (as in Theorem 2). Further, the number of parameters of this RGraphC will depend only on the smallest finite absolute moments of its weight and bias distributions, i.e., the ones given by \(f_{_{f}}(,_{i})\) and \(g_{_{g}}(,_{b})\)._

The complete proof is in Appendix A The main insight behind Theorem 3 is using techniques from randomized algorithms to derive the universality of RLCs in the graph tasks from Definition 3. The other results follow the same line as Proposition 2 while replacing de Finetti's with Aldous-Hoover's theorem.

**What is the practical impact of Assumption 3 in graph tasks?** Janson and Diaconis  showed that Assumption 3 is equivalent to having the input graphs sampled from a graphon model. For the reader unfamiliar with graphons, this class encompasses from simple random graph models like \(G(n,p)\), to modern matrix factorization methods in recommender systems . Moreover, this assumption has been recently used to design representations that are invariant to graph size, see .

**When are RGraphCs better than GNNs?** We contrast the results of RGraphCs with GNNs[17; 22; 26] due its widespread use, empirical success and computational efficiency. A known issue with GNNs is its inability to approximate simple tasks such as graph connectivity . This comes from the fact that GNNs cannot distinguish simple non-isomorphic graphs such as any pair of \(d\)-regular graphs. As such, GNNs cannot universally approximate tasks that assign different labels to any of such pairs. In contrast, RGraphCs can approximate any problem that either has a smooth boundary or can be tested with an inner product as in Definition 3.

Note that we can always define a task following Definition 3 that distinguishes between a specific graph and any other non-isomorphic input. Thus, unlike deterministic models, the set of tasks RGraphCs can solve is not attached to solving the graph isomorphism problem. It is simply attached to whether the decision problem can be tested with an inner product. Note that this does not imply that our model solves the graph isomorphism problem. To do so, we would need to test on an exponential number of tasks. The main implication of this observation is that **the expressive power of probabilistic graph models is not attached to the ability to distinguish non-isomorphic graphs, but to the task's complexity.** Thus, we are not doomed to fail at simple tasks like GNNs. Finally, note that GNNs aggregate messages using Deep Sets (or some variation of it). Thus, its approximation power, even for graphs that it can distinguish, requires a number of parameters that grows with the graphs' number of vertices.

Finally, we highlight that there exist more expressive, but still non-universal, models such as higher-order GNNs  and subgraph GNNs [2; 7]. However, they suffer from a heavy use of computational resources and are restricted to smaller, but existent, set of non-isomorphic graphs it cannot distinguish. As such, they suffer from the same aforementioned problems as GNNs.

## 4 Experiments

Until now, our theory characterized the model space of invariant RLCs. It is natural to wonder whether such guarantees translate into practice. Concretely, we focus on investigating three questions related to our theoretical contributions.

**Q1.** Are RSetCs more parameter-efficient than Deep Sets in practice? (_cf._ Proposition 2)

**Q2.** Are RSetCs more robust than Deep Sets with out-of-distribution data? (_cf._ discussion in Section 3.1)

**Q3.** Can RGraphCs efficiently solve tasks that GNNs struggle in practice? (_cf._ Theorem 3)

**Sorting task.** To address **Q1** and **Q2**, we consider the sorting task proposed in . Our input is given by \(,()=^{d},_{i} (0,1)\) for some odd number of dimensions \(d\) and the labeling function by \(y()=1\) if for a vector \(w,w_{i}:=(-1)^{i+1}\) we have \(w^{T}() 0\) or \(-1\) otherwise. We chose this task since it was shown in  that Deep Sets needs a hidden-layer at least as large as the input set (\(d\)) to approximate it with zero error. Thus, to showcase the parameter efficiency of RSetCs in practice (_cf._ Proposition 2) we test both models in this task, while fixing the hidden layer at \(5\) on both. To make the results comparable, we train RSetCs with a single hidden layer (in both networks) and Deep Sets with a single hidden layer in each of its networks as well. Finally, to answer **Q2** we test both models on sets twice as large as the ones used in training.

**Sign task.** We would also like to investigate **Q1** in a setting where Deep Sets is not supposed to struggle. That is, we would like to know whether RSetCs can be more parameter-efficient in tasks that are (supposedly) easy for Deep Sets. For this, we consider the sign task \(y():=(_{i=1}^{d}(_{i}))\) with \(_{i} N(0,1)\). Note that the networks in Deep Sets would need to simply learn the sign function and thus its parameters should not depend on the set size. To showcase the parameter efficiency of RSetCs, we consider Deep Sets with one and two hidden layers (in both networks) while keeping RSetCs with a single hidden layer. The rest of the experimental setup follows the sorting task verbatim.

**Connectivity task.** We chose the task of deciding whether a graph sampled from a \(G(n,p)\) model with \(p=1.1(n)/n\) is connected or not. Unlike RGraphCs, GNNs provably cannot approximate this simple task . By comparing the performance of GNNs and RGraphCs in this task, we can empirically verify Theorem 3. Moreover, in contrast to GNNs, Theorem 3 tells us that the required number of parameters in RGraphCs is not attached to the graph size. Hence, we also test the performance of both models in this task while increasing the size of the input graphs and keeping hidden layer sizes fixed to \(2\) in both models. To make results comparable, we use a GNN with \(3\) layers and an RGraphC with \(3\) hidden layers of size in each network. Since in each GNN layer there are two network layers, the parameter sizes in the GNN and in the RGraphC are comparable.

**Experimental setup.** We used the Deep Sets architecture as proposed in the original paper . For the GNN, we used the GIN  architecture, which completely captures the GNN properties mentioned in Section 3.2. Finally, to give perspective on how bad a model is performing, we also contrast the results with a constant classifier, _i.e._, how a classifier that always predicts the most common class performs. All models were trained with Pytorch  using Adagrad  for a maximum of 1000 epochs with early stopping. The reported results in Figures 1 and 2 are with respect to five runs. The training sets consisted of 1000 examples, while the the validation and test sets contained 100 examples. We detail the hyperparameters and their search in Appendix C.

**A1 (RSetC parameter efficiency).** In Figure 0(a) we see the (test set) results for RSetCs and Deep Sets when varying the size of the input for a fixed hidden layer. We note that even in the first task, \(d=5\), when Deep Sets is supposed to approximate the task well it fails to generalize. Overall, for all input sizes we can see RSetCs consistently outperforming Deep Sets. On the other hand, in most of the tasks Deep Sets performs similarly or even worse than a constant classifier. Finally, in the sign task from Figure 1(a) we see that the performance of a single hidden-layer RSetC is closer to a two hidden-layer Depe Sets, showing its parameter efficiency even in tasks where Deep Sets is supposed to efficiently succeed. Thus, we can verify Theorem 2 and Proposition 2 in practice.

**A2 (RSetC out-of-distribution robustness).** We can see in Figure 0(b) how RSetCs provide consistently better results than Deep Sets when tested in sets twice the size as the ones used in training. This provides evidence to the size generalization discussion in Section 3.1. We can see that RSetCs perform even similarly to the in-distribution case, while Deep Sets is consistently worse than a constant classifier. Moreover, the high variance in Deep Sets' results confirm our observation in Section 3.1 about its sensitivity.

**A3 (RGraphC universality and parameter efficiency).** In Figure 1(b) we can see the results for RGraphCs vs. GNNs in the connectivity task. We note how the lack of expressive power coupled with the parameter inefficiency of GNNs is reflected in its poor performance, that decreases as the input size increases. In contrast, Theorem 3 is confirmed by RGraphCs' better performance even when the input size is increased (and the number of parameters is fixed). Finally, just as in Deep Sets we note a very high variance in the GNN results. This is often observed in tasks where there is no continuous features in the input, see _e.g._, .

## 5 Conclusions

Our work established the first class of models that leverages randomness to achieve universality and invariance in binary classification tasks. We combined tools from randomized algorithms and probability theory in our results. By leveraging this new principle, we were able to present resource efficient models that, under mild assumptions, are universal and invariant in a probabilistic sense. This work can be extended in many different ways, _e.g._, designing architectures for other group invariances, such as \((n)\) or designing specific optimization procedures for RLCs. It is also interesting to understand the practical benefit of RLCs and their invariant counterparts in real-world tasks.