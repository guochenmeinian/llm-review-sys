# Nearly Optimal Approximation of Matrix Functions

by the Lanczos Method

 Noah Amsel

New York University (noah.amsel@nyu.edu, tyler.chen@nyu.edu, cmusco@nyu.edu)

Tyler Chen1

Anne Greenbaum2

Cameron Musco3

Christopher Musco4

###### Abstract

Approximating the action of a matrix function \(f()\) on a vector \(\) is an increasingly important primitive in machine learning, data science, and statistics, with applications such as sampling high dimensional Gaussians, Gaussian process regression and Bayesian inference, principle component analysis, and approximating Hessian spectral densities. Over the past decade, a number of algorithms enjoy strong theoretical guarantees have been proposed for this task. Many of the most successful belong to a family of algorithms called _Krylov subspace methods_. Remarkably, a classic Krylov subspace method, called the Lanczos method for matrix functions (Lanczos-FA), frequently outperforms newer methods in practice. Our main result is a theoretical justification for this finding: we show that, for a natural class of _rational functions_, Lanczos-FA matches the error of the best possible Krylov subspace method up to a multiplicative approximation factor. The approximation factor depends on the degree of \(f(x)\)'s denominator and the condition number of \(\), but not on the number of iterations \(k\). Our result provides a strong justification for the excellent performance of Lanczos-FA, especially on functions that are well approximated by rationals, such as the matrix square root.

## 1 Introduction

Given a symmetric matrix \(^{d d}\) with eigendecomposition \(=_{i=1}^{d}_{i}_{i}_{i}^{}\), the matrix function \(f()\) corresponding to a scalar function \(f:\) is defined as

\[f():=_{i=1}^{d}f(_{i})_{i}_{i}^{ }.\] (1)

Matrix functions arise throughout machine learning, data science, and statistics. For instance, the matrix square root is used in sampling Gaussians, Bayesian modeling, and Gaussian processes [4; 5; 59], general fractional matrix powers are used in Markov chain modeling and Renyi entropy estimation [68; 42; 43; 19], the matrix logarithm is used for determinantal point processes, kernel learning, and approximating log-determinants for Gaussian process regression and Bayesian inference [17; 39; 31], the matrix sign is used in principal components regression and spectral density estimation [29; 18; 44; 58; 32; 69; 13; 7; 14], and the matrix exponential is used in network science [3; 67; 48]. In many of these applications, we do not need to compute the matrix \(f()\) itself, but rather its action on a vector \(^{d}\); i.e., \(f()\). This task can be performed much more efficiently than computing an eigendecomposition of \(\) and forming \(f()\) using (1).

Perhaps the first general purpose method for approximating \(f()\) is the Lanczos method for matrix function approximation (Lanczos-FA) [22; 30], which is the focus of the present paper. Over the past decade, a number of special purpose algorithms, designed for a single function or class of functions,have been developed [23; 16; 26; 44; 11]. These newer algorithms often satisfy strong theoretical guarantees better than the best-known bounds for Lanczos-FA. The present paper is motivated by the following remarkable observation:

Despite being arguably the simplest and most general algorithm for computing \(f()\), _Lanczos-FA frequently outperforms special purpose algorithms_, sometimes by orders of magnitude, on common test problems.

For instance, in Figures 4, 5, and 8 we compare Lanczos-FA with specialized algorithms [44; 59; 11] that satisfy the best-known theoretical guarantees for computing \(f()\) for the particular functions they were designed for. In these experiments Lanczos-FA drastically outperforms these methods, despite the fact that it was not designed for any particular function. Because of its outstanding performance, Lanczos-FA is perhaps the most commonly used algorithm for computing \(f()\) in practice. The main goal of this paper is to improve our theoretical understanding of _why_ Lanczos-FA performs so well, in order to help close the theory-practice gap.

### Krylov subspace methods

Lanczos-FA falls into a class of algorithms called Krylov Subspace Methods (KSMs). KSMs are among the most powerful and widely used algorithms for a broad range of computational tasks including solving linear systems, computing eigenvalues/vectors, and low-rank approximation [62; 38; 47; 65]. Like other KSMs for computing \(f()\), Lanczos-FA iteratively constructs an approximation from the Krylov subspace

\[_{k}(,):=\{,, ,^{k-1}\}.\] (2)

The Lanczos algorithm  produces an orthonormal basis \(=[_{1},,_{k}]\) for the Krylov subspace \(_{k}(,)\) and a symmetric tridiagonal matrix \(\) satisfying \(=^{}\). The Lanczos-FA algorithm uses this \(\) and \(\) to approximate \(f()\).

**Definition 1**.: _The Lanczos-FA iterate for a problem instance \((f,,,k)\) is defined as_

\[_{k}(f;,):=f()^ {},\]

_where \(\) and \(\) are as above._

In our analysis we assume exact arithmetic. We do not discuss the implementation of Lanczos or Lanczos-FA since there are many resources on this topic; see for instance [52; 8; 9]. Fortunately, if the Lanczos algorithm is implemented properly, its finite-precision behavior closely follows its exact-arithmetic behavior on a nearby problem . See Section 5 for further discussion.

### Optimality guarantees for Krylov subspace methods

For a wide variety of problem instances, Lanczos-FA is observed to converge almost as quickly as the _best_ approximation to \(f()\) that could be returned by _any_ KSM run for the same number of iterations. In particular, all KSMs output approximations that lie in the span of \(_{k}(,)\), that is, approximations of the form \(p()\) for a polynomial \(p\) of degree less than \(k\). Given a problem instance \((f,,,k)\), the best possible approximation returned by a Krylov method is4

\[_{k}(f;,):=*{argmin}_{_{k}(,)}\|f()- \|_{2}.\]

By definition, the error of this Krylov optimal iterate can be characterized as follows:

\[\|f()-_{k}(f;,)\|_{2}= _{(p)<k}\|f()-p()\|_{2}.\] (3)

For general matrix functions, no efficient algorithm for computing \(_{k}(f;,)\) is known, but as shown in Figure 1, the solution returned by Lanczos-FA often nearly matches the error of this best approximation. It is thus natural to ask if, at least for some class of problems, Lanczos-FA satisfies the following strong notion of approximate optimality:

**Definition 2** (Near Instance Optimality).: _For a problem instance \((f,,,k)\), we say that a Krylov method is nearly instance optimal with parameters \(C\) and \(c\) if_

\[\|f()-_{k}(f;,)\|_{2} C _{(p)<ck}\|f()-p()\|_{2}.\]

_Above, \(_{k}(f;,)\) denotes the output of an algorithm (e.g., Lanczos-FA) obtained from the Krylov subspace \(_{k}(,)\), i.e., the output after \(k\) iterations._

In Definition 2, \(C 1\) and \(c 1\) allow some slack in comparing to the Krylov optimal iterate. The right hand side depends on the entire problem instance, \((f;,)\), which is why we call the guarantee "instance optimal".

### Existing near-optimality analyses of Lanczos-FA

The best bound for Lanczos-FA applying to a broad class of functions is the following common bound for Lanczos-FA; see for instance .

**Fact 3**.: _For all problem instances \((f,,,k)\), Lanczos-FA satisfies_

\[\|f()-_{k}(f;,)\|_{2} 2 \|\|_{2}_{(p)<k}(_{x\{_{},_{ }\}}|f(x)-p(x)|).\]

Fact 3 is in some sense an optimality guarantee; it compares the convergence of Lanczos-FA to the best possible _uniform_ polynomial approximation to \(f(x)\). However, it does not take into account properties of \(\) such as isolated or clustered eigenvalues, and as seen in Figure 1, it typically only gives a loose upper bound on the performance of Lanczos-FA.

To date, near-instance-optimality guarantees for Lanczos-FA akin to those of Definition 2 are known only in a few special cases. The most well-known is when \(f(x)=1/x\) and \(\) is positive definite, in which case Lanczos-FA is mathematically equivalent to the celebrated Conjugate Gradient algorithm, and therefore exactly optimal in the \(\)-norm . The instance-optimality guarantee for Lanczos-FA in this setting is used to prove well-known super-exponential convergence in certain settings . In contrast, a bound like Fact 3 only provides exponential convergence and is widely understood by the numerical linear algebra community to not accurately describe the actual behavior of the algorithm in most cases . The only other near-optimality guarantees for Lanczos-FA of which we are aware concern \(^{-1}\) for nonsymmetric \(\) and the matrix exponential \((-t)\). In both cases, Lanczos-FA satisfies guarantees that are reminiscent of (although weaker than) Definition 2. Further discussion is given in Appendix B.1.

Figure 1: Lanczos-FA error \(\|f()-_{k}(f;,)\|_{2}\) at each iteration for several functions/spectra. “Instance Optimal” is the right hand side of Definition 2 with \(C=c=1\), which is a lower bound for all KSMs, including Lanczos-FA. Lanczos-FA performs nearly instance optimally on a wide range of problems, far better than Fact 3 predicts. This is easily seen in the bottom plots, which show the ratio of the error of the Lanczos-FA iterate and the Krylov optimal iterate, \(_{k}(f;,)\).

We also remark that there are a number of works which aim to relate the convergence of of Lanczos-FA for functions with certain integral representations to the convergence of conjugate gradient on linear systems [41; 24; 27; 28; 25; 10]. While these analyses provides spectrum dependent convergence guarantees, they are weaker than Definition2 because it is not clear how the best possible KSM approximation to a given function relates to the convergence of conjugate gradient. These bounds were mostly developed for use as a posteriori stopping criteria rather than as a theoretical explication for the behavior of Lanczos-FA.

### Our contributions

In Section2, we prove near instance optimality for a broad class of rational functions (Theorem4). To the best of our knowledge, this is the first true instance-optimality guarantee for Lanczos-FA to be proven for any function besides \(f(x)=1/x\). In Section2.2, we discuss how results of this kind imply related guarantees for functions that are uniformly well-approximated by rationals, which includes many functions of interest in machine learning. In Section3, we additionally show that Lanczos-FA satisfies a weaker version of near optimality for two crucial non-rational matrix functions--the square root and inverse square root (Theorems6 and 7). AppendixC compares the this version of optimality to Definition2 (near instance optimality) and to that of creftype3. In Section4, we present experimental evidence showing that, for many natural problem instances, our bounds are significantly sharper than the standard bound of creftype3. These experiments also demonstrate that despite its generality, Lanczos-FA often converges significantly faster than other methods in practice. We conclude with a discussion of next steps and open problems in Section5.

## 2 Near optimality for rational functions

In this section, we study the Lanczos-FA approximation to \(r()\), where \(r(x)\) is a rational function with real-valued poles that lie in \(\), where \(:=[_{},_{}]\). Specifically,

\[r(x):=n(x)/m(x),\] (4)

where \(n(x)\) is any degree \(p\) polynomial and

\[m(x)=(x-z_{1})(x-z_{2})(x-z_{q}), z_{i} \,.\]

Since \(z_{j}\), \((-z_{j})\) is either positive definite or negative definite. For convenience, we define

\[_{j}:=+(-z_{j})&z_{j}<_{ }\\ -(-z_{j})&z_{j}>_{}.\] (5)

Our main result on rational functions is the following near-instance-optimality bound, which holds under a mild assumption on the number of iterations \(k\):

**Theorem 4**.: _Let \(r(x)=n(x)/m(x)\) be a degree \((p,q)\)-rational function as in (4) and define \(_{j}\) as in (5). Then, if \(k>\{p,q-1\}\), the Lanczos-FA iterate satisfies the bound_

\[\|r()-an}_{k}(r;, )\|_{2} q(_{1})(_{2}) (_{q})_{(p)<k-q+1}\|r()-p( )\|_{2}.\]

We prove this theorem in AppendixA. Above, \((_{i})\) is the condition number of \(_{i}\), the ratio of the largest to smallest magnitude eigenvalues of \(_{i}\). Theorem4 shows that Lanczos-FA used to approximate \(r()\) satisfies near instance optimality, as in Definition2, with

\[C=q(_{1})(_{2})( _{q}), c=1-(q-1)/k.\]

In particular, when \(\) is positive definite and each of the \(z_{i}\) are negative (as is the case for rational function approximations of many functions including the square root ), then \(C q()^{q}\).

As discussed in Section5, we expect that Theorem4 can be tightened by significantly reducing the prefactor. Nevertheless, as illustrated in Figure2, even in its current form, the bound improves on the standard bound of creftype3 in many natural cases; i.e., it more tightly characterizes the observed convergence of Lanczos-FA. Finally, as discussed further in Section2.2, we note that, beyond rational functions being an interesting function class in their own right, Theorem4 has implications for understanding the convergence of Lanczos-FA for functions like the square root which are much easier to approximate with rational functions than polynomials.

### Proof Sketch

Our proof of Theorem 4 leverages the near optimality of Lanczos-FA in computing \(^{-1}\) to obtain a bound for general rational functions. For illustration, consider the simplest possible rational function for which we are unaware of any previous near-optimality bounds: \(^{-2}\) when \(\) is positive definite. The Lanczos-FA approximation for this function is \(^{-2}^{}=^{-1}^{ }^{-1}^{}\). Using the triangle inequality and submultiplicativity, we can bound the error of the approximation as

\[\|^{-2} -\|_{k}(x^{-2};,)\|_{2}\] \[\|^{-2}-^{-1}^{ }^{-1}\|_{2}+\|^{-1}^{ }^{-1}-^{-2}^{} \|_{2}\] \[\|^{-2}-^{-1}^{ }^{-2}\|_{2}+\|^{-1} ^{}\|_{2}\|^{-1}-^{- 1}^{}^{-1}\|_{2}.\] (6)

Using the normal equations and the fact that \(\) is a basis of the Krylov subspace, it is possible to show that \(^{-1}^{}=(^{ })^{-1}^{}\) is the \(\)-norm5 projector onto Krylov subspace. Hence

\[\|^{-2}-^{-1}^{}^{-2}\|_{} =_{_{K}(,)}\|^{-2} -\|_{}\] \[:=_{(p)<k}\|^{-2}-p() \|_{}.\]

Therefore, using \(\|\|_{2}(1/})\|\|_{}\) and \(\|\|_{}}\|\|_{2}\), the first term on the right hand side of (6) can be bounded as

\[\|^{-2}-^{-1}^{}^{-2}\|_{2})} _{(p)<k}\|^{-2}-p()\|_{2}.\]

Similarly, the second factor of the second term on the right hand side of (6) can be bounded as

\[\|^{-1}-^{-1}^{}^{-1}\|_{2})} _{(p)<k}\|^{-1}-p()\|_{2}.\]

Then, since \(xp(x)\) is polynomial of one degree larger than \(p(x)\),

\[_{(p)<k}\|^{-1}-p()\|_{2} _{(p)<k-1}\|^{-2}-p( )\|_{2}\] \[_{}_{(p)<k-1}\|^{-2}-p ()\|_{2}.\]

Plugging the above bounds into (6) and using the fact that the eigenvalues of \(\) are contained in \([_{},_{}]\) so that \(\|^{-1}^{}\|_{2}\|^{-1}\|_{2}  1/_{}\),

\[\|^{-2}-\|_{k}(x^{-2};,)\|_ {2} 2()^{3/2}_{(p)<k-1}\|^{-2} -p()\|_{2}.\]

Figure 2: Despite its large prefactor, the bound of Theorem 4 qualitatively captures the convergence behavior of Lanczos-FA for rational functions. It can be tighter than the standard bound of Fact 3, even for a moderate number of iterations \(k\). We use rational approximations to \((-x/10)\) and \((x)\) for comparison with Figure 1; see Section 4 for more details.

Bounding \(()^{3/2}\) by \(()^{2}\) gives the bound in Theorem4.

Our proof of Theorem4 generalizes the above approach. We write the Lanczos-FA error for approximating \(r()\) in terms of the error of the optimal approximations to a set of simpler rational functions, and then reduce polynomial approximation of \(r(x)\) to polynomial approximation of each of these particular functions.

### Implications for non-rational functions

Theorem4 can be used to derive guarantees for other (non-rational) functions. In particular, consider any function \(f(x)\) that is uniformly well approximated by a low-degree rational function \(r(x)\) on \(=[_{},_{}]\), the interval containing all of the eigenvalues of \(\); i.e., suppose \(\|r-f\|_{}:=_{x}|r(x)-f(x)|\) is small. A natural way to approximate \(f()\), used in , is to construct \(r(x)\) and output \(r()\), using some iterative linear solver to quickly apply the denominators of the partial fraction decomposition of \(r(x)\). Alternatively, if we have an instance-optimality guarantee for Lanczos-FA on rational functions like Theorem4, we could use Lanczos-FA to compute \(r()\). The following analysis shows that simply using Lanczos-FA on \(f(x)\) itself cannot be much worse.

**Lemma 5**.: _Assume the we have the following instance-optimality guarantee for rational functions:_

\[\|r()-_{k}(r;,)\|_{2}  C_{r}_{(p)<c_{r},k}\|r()-p() \|_{2}.\] (7)

_Here, \(C_{r}\) and \(c_{r}\) depend on the choice of approximant \(r(x)\). Then the error of Lanczos-FA on \(f(x)\) is bounded as follows:_

\[\|f()-_{k}(f;, )\|_{2}\\ _{r}(C_{r}+2)\|\|_{2}\|f-r\|_{ }+C_{r}_{(p)<c_{r},k}\|f()-p( )\|_{2}.\] (8)

We prove this lemma, which follows directly from the triangle inequality, in AppendixA.5. Compare the form of this bound to that of Definitions2 and 13. It is close to a near-instance-optimality guarantee, except for the first term, which requires \(f(x)\) to be uniformly well-approximated by a rational function \(r(x)\) on \([_{},_{}]\). This is still much stronger than Fact3, which requires \(f(x)\) to be uniformly well-approximated by a _polynomial_ to guarantee that Lanczos-FA provides a good approximation. There are many functions with lower degree rational approximations than polynomial approximations, even when we require the rational function \(r(x)\) to have poles only in \(\) (as in our Theorem4). Such rational approximations are obtainable by the Remez algorithm [64, Chapter 24], and for many important functions they are also known explicitly. For example, a uniform polynomial approximation to the square root on a strictly positive interval \([_{},_{}]\) requires degree \((/_{}})\)[64, Chapter 8]. On the other hand, a uniform rational approximation can be obtained with degree only \(O((_{}/_{}))\)[64, Chapter 25]. Likewise, a uniform polynomial approximation to \((-x)\) on the interval \([0,B]\) requires degree \(()\), but uniform rational approximations can be constructed with no dependence on \(B\). For such functions, we expect (8) to be stronger than Fact3.

Notice also that, while in Lemma5, we assume \(f(x)\) is well-approximated by a rational function, we are not required to actually construct the approximation. Indeed, since it holds for any \(r(x)\), instead of fixing a rational approximation of a certain degree, (8) automatically balances \(\|r-f\|_{}\), which decreases as the degree grows, with \(C_{r}\), which may increase as the degree grows (see Figure4).

## 3 Near Spectrum Optimality for \(^{ 1/2}\)

In the previous section, we proved that Lanczos-FA is nearly instance optimal for rational functions in the sense of Definition2. In this section, we prove that Lanczos-FA satisfies a weaker form of near optimality for two important non-rational functions: square root and inverse square root. We term this weaker form of guarantee "near spectrum optimality". In AppendixC, we formally define this notion and compare it to Definition2. We first state our bound for the inverse square root.

**Theorem 6**.: _Let \(\) be the spectrum of \(\). Then for \(k 2\), the Lanczos-FA iterate satisfies the bound_

\[\|^{-1/2}-_{k}(x^{-1/2};, )\|_{2}}()\|\|_{2}_{ (p)<k/2}(_{x}|}-p(x) |).\]That is, Lanczos-FA applied to the inverse square root satisfies Definition12 ("near spectrum optimality") with

\[C=}(), c=.\]

We prove this theorem in AppendixD. The proof relies on comparing the error of the \(k\)th Lanczos iterate for \(x^{-1/2}\) to that of the Lanczos iterate for \(x^{-1}\). First, applying a bound from , we use the Cauchy integral formula to upper bound the error of Lanczos-FA on \(x^{ 1/2}\) by its error on \(x^{-1}\) (Lemma16). Second, as Equation17 shows, Lanczos-FA is nearly instance optimal for the function \(x^{-1}\); that is, it outputs \(p()\) where \(p\) is (nearly) the degree \(k\) polynomial that best approximates \(x^{-1}\). Third, the best degree \(k\) polynomial approximation to \(x^{-1}\) must have lower error than the best degree \(k/2\) approximation to \(x^{-1/2}\). This is because any degree \(k/2\) approximation to \(x^{-1/2}\) can be squared to yield a good degree \(k\) polynomial approximation to \(x^{-1}\). Combining these three steps upper bounds the error of the \(k\)th Lanczos-FA iterate for \(x^{-1/2}\) by the error of the best degree \(k/2\) polynomial approximation of \(x^{-1/2}\).

Nearly the same argument can be used to prove spectrum optimality of Lanczos-FA for the function \(^{-1/n}\) for any \(n\) with \(C=(2^{n}-1)()/\). Furthermore, using Lemma15 of AppendixC, we can convert Theorem6 into a near-instance-optimality guarantee at the price of strong dependence of \(C\) on \(\). We next state our optimality result for the matrix square root.

**Theorem 7**.: _Let \(\) be the spectrum of \(\). Then for \(k 2\), the Lanczos-FA iterate satisfies the bound_

\[\|^{1/2}-_{k}(x^{1/2};,) \|)^{2}}{k^{3/2}}\|\|_{2}_{(p)<k/ 2+1}(_{x\{0\}}|-p(x)|).\]

This bound resembles Definition12 with

\[C=)^{2}}{k^{3/2}}, c=.\]

However, it is slightly weaker in that the maximization is taken over \(\{0\}\) instead of only \(\).

The proof of Theorem7 is nearly the same as that of Theorem6, and it likewise appears in AppendixD. Ideally, if \(p\) is a polynomial approximation to \(x^{1/2}\), we would like to claim that \((p(x)/x)^{2}\) yields a good polynomial approximation to \(x^{-1}\). However, since this function is not necessarily a polynomial, we must instead use \(()^{2}\), which introduces the need to include \(\{0\}\) in the maximization on the right-hand side.

## 4 Experiments

We now present several numerical experiments to assess the quality of our instance-optimality bounds, Theorem4 and Lemma5. Our results show that, despite the large prefactor \(C\), our bounds already supersede the standard uniform approximation bound (Fact3) in many cases. We also compare Lanczos-FA against several recently proposed algorithms for computing matrix functions with strong theoretical guarantees. We find that, in practice, Lanczos-FA performs better than all of them. We implement Lanczos-FA in high precision arithmetic using the flamp library, which is based on mpmath, in order to mitigate any potential impacts of finite precision arithmetic and observe the convergence behavior of the algorithm beyond the standard machine precision.6

In Figure1, we compare the performance of Lanczos to the instance-optimal KSM (which we can compute by direct methods) and against Fact3 for various matrix functions and spectra. We use three test matrices \(^{100 100}\) which all have condition number 100. The first has a uniformly-spaced spectrum, the second has a geometrically-spaced spectrum, and the third has eigenvalues that are all uniformly-spaced in a narrow interval except for ten very small eigenvalues. We compute the bound from Fact3 using the Remez algorithm and compute the instance-optimal approximation using least squares regression onto the Krylov basis \(\). In Figure1, as in almost all cases we tried, Lanczos-FAperforms nearly as well as the instance-optimal approximation. For instance, the error is never more than a small multiple of the optimal error in the experiments we did.

To better understand Theorem4, we compare the bound to the true convergence curve of Lanczos-FA for various rational functions in Figure2. We also plot Fact3 for reference. We use the same matrices and \(\) vectors as in Figure1; results are similar if \(\) is instead chosen as a uniform linear combination of \(\)'s eigenvectors. We choose rational functions to match the functions used for Figure1. We construct a degree 5 rational approximation to \((-x/10)\) following . We construct a degree 10 approximation to \((x)\) using the BRASIL algorithm  and verify that it has real poles outside the interval of the eigenvalues. Despite Theorem4's exponential dependence on the degree of the rational function being applied, Figure2 shows that it matches the shape of the convergence curve well and is tighter than Fact3 when the number of iterations is large. That said, in all cases plotted, Lanczos-FA always returns an approximation much closer to optimal than predicted by Theorem4, suggesting that the leading coefficient in our bound is pessimistic.

### Dependence on the rational function degree

Theorem4 upper bounds the optimality ratio by \(C=q(_{1})(_{2})( _{q})\). We conjecture that this bound is far from tight. However, the following experiment provides evidence that it is not possible to entirely eliminate the dependence on the rational function's denominator degree \(q\). In particular, for parameters \((,q)\), consider approximating \(^{-q}\) where \(\) has spectrum \(_{1}=1\) and \(_{2},,_{100}\) evenly spaced between \(0.999995\) and \(\). In this case, \((_{1})==(_{q})=()=\). We generate a grid of problems by picking different combinations of \((,q)\) and tuning \(\) in a limited way to maximize the maximum ratio between the error of Lanczos-FA and the optimal Krylov error over all iterations. In particular, we took \(\) to be an all ones vector, except we varied its first entry, using grid search to maximize the optimality ratio. The results, plotted in Figure3, suggest that the optimality ratio grows at least as \(()})\). We have yet to find harder problem instances than this.

### Non-rational functions

As noted in Section2.2, an instance-optimality guarantee for rational functions also implies that Lanczos-FA performs well on functions that are well-approximated by rationals. As an example, we consider the function \(f(x)=x^{-0.4}\), for which a rational approximation in any degree can be found using the BRASIL algorithm . Figure4 shows how applying Lanczos-FA to these rational approximations compares to applying Lanczos-FA directly to the \(f(x)\) itself to approximate \(^{-0.4}\). When the number of iterations is small, both methods perform nearly optimally, as the accuracy is limited more by the small size of the Krylov subspace than by the difference between \(f(x)\) and the rational approximant (that is, the second term in (8) dominates the first term). As the number of iterations grows, the error due to approximating \(f(x)\) in the Krylov subspace continues to decrease while the error of uniformly approximating \(f(x)\) by some fixed rational function remains fixed (that is, the first term in (8) dominates the first term); however, increasing the degree of the

Figure 3: The maximum observed ratio between the error of Lanczos-FA and the optimal error over choices of \(\) when approximating \(^{-q}\) for matrices with varying condition number \(\). Each point corresponds to a pair \((,q)\). Points with the same color have the same value of \(\). On the left, the dotted line plots \(\) for the maximum \(\) considered (\(10^{6}\)). On the right, the dotted line plots \(\) for the maximum \(q\) considered (\(2^{6}\)). Overall, the optimality ratio appears to scale at least as \(()\).

rational approximant decreases this source of error. This shows that understanding the convergence of Lanczos-FA for the entire family of rational approximations goes a long way toward explaining its convergence for non-rational functions. In the limit, Lanczos-FA applied to \(f(x)\) itself appears to automatically inherit the instance optimality of a suitably high-degree rational approximation.

This result has an additional implication. A number of papers use explicit rational approximations to compute non-rational matrix functions . These approximations are often applied by applying conjugate gradient (or a related method) to each of the terms in the sum . In the case conjugate gradient is used, the resulting algorithm is mathematically identical to Lanczos-FA used to compute the the rational approximation. However, Figure 4 suggests that simply using Lanczos-FA on the original function is both simpler and converges faster (though memory usage and other factors may need to be considered).

Another line of work uses specialized iterative methods that exploit problem structure to apply the rational approximations . In Figure 5, we compare Lanczos-FA to two such methods from  for computing sign(**A**), for **A** of the form \(=^{}-\). While they achieve better theoretical bounds than are known for Lanczos-FA, Lanczos-FA far outperforms them on the test problems used in .

Figure 4: Applying Lanczos-FA to the function \(^{-0.4}\) and rational approximations of various degrees found using the BRASIL algorithm . In this experiment, the spectrum of **A** contains two clusters: 10 eigenvalues uniformly spaced near 1, and 90 eigenvalues uniformly spaced near 100. As predicted by the bound in Section 2.2, convergence of Lanczos-FA for this function appears to closely track that of a high degree rational approximant.

Figure 5: A comparison of Lanczos-FA with two methods from  (“rational” and “slanczos”) for computing the matrix sign function, which work by using a stochastic iterative method to approximate rational approximations to the step function of various degrees. The “rational” method is the main one studied in , while “slanczos” is included because it is the best performing in their experiments. Each panel corresponds to one of the test problems from . Iterations of these methods are counted in number of inner products with rows of **A** rather than number of matrix-vector products with **A** as a whole. To compare these with Lanczos-FA, we consider \(d\) such inner products to be equivalent to one matrix-vector product.

Additional Experiments.In Section3 we introduce bounds for the matrix square root and inverse square root, and in SectionE.1 we provide numerical tests to study the sharpness of the bounds, verifying that they can improve on creftype3. In SectionE.2, we demonstrate the convergence behavior of Lanczos-FA on rational functions with poles inside the range of eigenvalues (Figure6). This illustrates why a bound like creftype4 is not possible, but suggests a weaker bound, such as the bound in  for \(f(x)=1/x\) and indefinite \(\), may be possible. creftypeE.3 shows that, unlike Lanczos-FA, a related algorithm called Lanczos-OR  (which is exactly optimal for rational functions, though not in the Euclidean norm) can perform poorly on high degree rational functions when the error is measured in the Euclidean norm.

## 5 Outlook

This paper provides instance-optimality guarantees for Lanczos-FA applied to a range of rational functions. We conclude with open questions that we believe are worthy of further study.

Extension to other function classes.Empirically, Lanczos-FA seems to be nearly instance optimal for a wide variety of functions beyond those considered in this paper, such as rational functions with conjugate pairs of _complex_ poles whose real parts lie outside \([_{},_{}]\). As seen in Figure6, the error of Lanczos-FA on functions with real poles in \([_{},_{}]\) is intriguing, oscillating between being very large and nearly optimal. We discuss this more in SectionE.2. It would be valuable to provide bounds explaining these behaviors.

It would be also natural to try to extend our bounds to Stieltjes/Markov functions, which can be viewed as a certain type of infinite degree rational function approximations with poles in \((-,0]\), and includes important functions like the inverse square root and a shifted logarithm. Our bound creftype4 cannot be directly applied to this class due to the dependence on the rational function denominator degree \(q\).

Construction of hard instances / refined upper bounds.Theorem4 has an exponential dependence on the degree of the rational function's denominator \(q\), which limits the practicality of our bounds. It is unclear if and when this dependence can be improved. The experiment in Section4.1 provides strong evidence that some dependence on \(q\) is necessary, but the hardest examples we have depend on \(\), instead of the current bound of \(()^{q}\) guaranteed by creftype4. It is an open question whether creftype4 can be tightened, or whether matching hard instances exist.

Finite precision arithmetic.Our analysis concerns the behavior of the Lanczos algorithm when run in exact arithmetic. In practice, the implementation of the Lanczos algorithm is very important; for instance, practical implementations often output a \(\) which is far from orthogonal . While this instability can be mitigated with more expensive implementations, theoretical work shows that, surprisingly, Lanczos and Lanczos-FA can work well despite it . For example,  show that creftype3 still holds up to a close approximation in finite precision arithmetic for any bounded matrix function. It would be valuable to study whether stronger near-optimality guarantees like those proven in creftype4 are also robust to finite precision. For \(f(x)=1/x\), this problem has been studied in .

Figure 6: Convergence of Lanczos-FA for rational functions with poles in \(\)’s eigenvalue range or that are imaginary. The optimality ratio can be very large for some iterations. Similar behavior is seen for functions like \(()\) that have a discontinuity in the interval of \(\)’s eigenvalues. However, the “overall” convergence of Lanczos-FA still appears to closely track the instance-optimal solution.

Acknowledgments:This research was supported in part by NSF Awards 2045590 (Chen, Ch. Musco), 2046235 (Ca. Musco), 2427363 (Chen, Ca. Musco, Ch. Musco), and 2234660 (Amsel).