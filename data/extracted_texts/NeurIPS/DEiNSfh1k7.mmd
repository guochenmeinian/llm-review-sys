# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Introduction

_"A sense of sameness is the very keel and backbone of our thinking" - William James, 1890_

Our understanding of the visual world hinges crucially on our ability to perceive the similarities between different images. Moreover, humans can reason about many notions of similarity, ranging from low-level perceptual properties such as color and texture, to higher-level concepts such as an object's category or a scene's emotional valence . This capacity to conduct meaningful visual comparisons underlies our ability to effortlessly transfer our knowledge to new environments, e.g., recognizing unseen or unfamiliar objects based on their relatedness to familiar ones [42; 70].

Computer vision has tried to capture this sense of similarity with low-level metrics like PSNR and SSIM , as well as learned perceptual metrics such as LPIPS  and DISTS . Despite their utility, these metrics are limited in that they focus on the pixel or patch level and fail to capture higher-level structures. These limitations have motivated researchers to reach for _image-level_ embeddings from large vision models such as DINO or CLIP to measure image-to-image distances in a large variety of applications [12; 40; 41; 44; 62]. Recent studies have shown that these embeddings do well at capturing certain high-level similarity judgments, in particular, predicting which semantic categories will be considered alike by humans . It remains unclear, however, how well these models align with human perception of richer and more fine-grained visual structure.

In this paper, we introduce a new perceptual metric, which bridges the gap between lower-level patch-based metrics and broad categorical comparisons. We collect a new dataset named NIGHTS - Novel Image Generations with Human-Tested Similarity - containing human similarity judgments over image triplets. Each triplet consists of a reference image and two perturbed versions, along with human judgments as to which version is most similar to the reference (Fig. 1). We use iterative filtering together with recent diffusion models to collect our dataset, which is designed to capture image sets that are cognitively impenetrable (i.e. result in consistent decisions across different individuals) yet showcase rich variations. For example, our data contains images of similar object appearance, viewing angles, camera poses, overall layout, etc. This dataset differs qualitatively from prior low-level datasets , which focused on perturbations like blurring and adding noise, and from previous high-level datasets [34; 35], which showed variation just at the level of categories (e.g. "is an image of a kitchen more like an image of a giraffe or an image of a beach").

On the mid-level similarity task presented by our data, we find that features from recent large pre-trained vision models [10; 39; 66] outperform the current set of standard perceptual metrics [24; 59; 94]. We further show that these large vision models can be tuned on our data to be substantially more human-aligned. Our resulting metric, DreamSim, can be dropped into existing pipelines and demonstrates high agreement with human visual perception in both quantitative assessments and qualitative comparisons using out-of-domain real images (e.g., image retrieval, image synthesis). We also analyze which features our metric is most sensitive to and find that, compared to previous perceptual metrics, it focuses relatively heavily on foreground objects, while compared to modern image embeddings, it does not neglect color and layout.

In summary, our contributions are the following:

* A new image similarity dataset, consisting of 20k synthetic image triplets designed to be cognitively impenetrable with human judgments as labels.
* A tuned metric that captures how humans naturally perceive image similarity, achieving 96.16% accuracy in predicting human judgments on our dataset.
* Analysis of the image properties that affect our model's decisions.
* Demonstration of downstream applications to image retrieval and synthesis.

## 2 Related work

**Perceptual similarity.** Classical metrics such as Manhattan \(_{1}\), Euclidean \(_{2}\), MSE, and PSNR use point-wise difference to measure similarity, thus failing to capture important joint image statistics. Patch-based metrics, including SSIM , FSIM , and HDR-VDP-2  tackle this issue and are widely used in applications involving photometric distortions such as image quality assessment. However, they do not capture the nuances of human vision when more structural ambiguity is present  and are not suited for more complex image generation tasks.

With the deep learning revolution, classical metrics have been replaced by learning-based metrics [26; 29; 43]. These metrics are defined in the space of deep features extracted from pre-trained networks, such as VGG  or AlexNet . Amir and Weiss  demonstrate that even _untrained_ networks can be adapted as perceptual metrics. Zhang _et al._ observe that feature-based metrics outperform classical metrics across different convolutional architectures and learning paradigms, suggesting that perceptual similarity is an emergent property in deep representations. Further tuning on the perceptual data yields improvements, such as in LPIPS , PIE-APP , or DPAM in the audio domain [53; 54]. Further improvements include ensembling for robustness , antialiasing for stability [24; 36; 93], and global descriptors for texture . Muttenthaler _et al._ provide insight into _high-level_ human similarity by training on a subset of the THINGS  dataset, focusing on concept similarity and omitting visual cues for images within a category.

While strong computer vision features make for strong perceptual metrics, counterintuitively, they eventually become _decorrelated_ with perceptual similarity [20; 48]. Today, the predominantly-used perceptual metric is LPIPS, operating on 64\(\)64 patches.

**Recent foundation models as metrics.** Foundation models provide strong pre-trained backbones for a variety of downstream tasks. These models primarily leverage the Vision Transformer (ViT)  architecture and are trained through self-supervised learning [16; 32; 81]. CLIP  learns to map images and text captions into a shared embedding space, proving useful for many (often zero-shot) tasks [41; 67; 90]. CLIP has been employed as a perceptual metric to train models for semantic consistency [12; 87]. Another self-supervised ViT-based model, DINO , extracts disentangled appearance and structure descriptors that can be employed in image generation pipelines. Amir _et al._ show that DINO encodes valuable semantic information about object parts. Our work aims to systematically evaluate such representations for perceptual similarity, also including OpenCLIP (an open-source implementation of CLIP)  and pre-trained masked autoencoders (MAE) .

**Perceptual tests.** The two alternative forced choice (2AFC) test has historically been used by behavioral psychologists to study decision-making [51; 56]. Humans judge the similarity between two images by choosing to consider certain dimensions of similarity more than others . Gathering judgments on ambiguous sets of images can be cognitively penetrable, calling upon a subject's cognitive processes rather than a more automatic, "wired-in" sense that is stable across humans and over time [11; 19; 79]. Previous studies have raised concerns about cognitive penetrability [57; 94]. On the other hand, as a psychophysical measure, just noticeable difference experiments (JND) are thought to be independent of subjective biases . We follow best practices  and collect judgments on both of these complementary perceptual tests.

**Synthetic data.** GANs  have been used widely for dataset generation on tasks such as visual alignment , face manipulation , and adversarial training for image synthesis . In recent years, text-driven generative models (e.g., Stable Diffusion , Imagen , DALLE-2 , MUSE ) have emerged as powerful tools for image synthesis. They have also been used to generate training data for a variety of downstream tasks [6; 9; 33; 75].

## 3 Perceptual dataset collection

While previous datasets focus on _low-level_, patch-based distortions [2; 94] or _high-level_, categorical  changes, we aim to close the gap, capturing distortions including mid-level variations. We aim to produce images with an underlying semantic commonality, but variations in a diversity of factors, such as style, color, pose, and other details, so that a human can assess their visual relationship. In Section 3.1, we describe our data generation pipeline - we prompt Stable Diffusion for related images of a given category, leveraging its natural image prior for variations within the category. We then describe our mechanism for collecting cognitively impenetrable perceptual judgments in Section 3.2.

### Generating images with varied distortions

We leverage Stable Diffusion v1.4 , which generates diverse and high-quality images that adhere to a given text prompt. We sample images with a prompt of the same category, using the structure "An image of a <category>". The <category> is drawn from image labels in popular datasets: ImageNet , CIFAR-10 , CIFAR-100 , Oxford 102 Flower , Food-101 ,

[MISSING_PAGE_FAIL:4]

To collect cognitively impenetrable triplets, we design our 2AFC experiments to ensure that each triplet included in the dataset obtains a unanimous vote for either \(_{0}\) or \(_{1}\). We divide our experiments into 10 rounds, starting with 100,000 triplets, advancing instances that maintain unanimous votes. Triplets retained through all 10 rounds can earn a maximum of 10 votes, but may have less as a result of the aforementioned sentinals. To maintain dataset quality, we only include triplets with \(\)6 unanimous judgments in the final dataset.

Ultimately, our 2AFC dataset \(^{}=\{(x,_{0},_{1}),y\}\) consists of 20,019 triplets with an average of 7 unanimous votes each. We partition our resulting dataset into train, validation, and test components with a random 80/10/10 split. Our dataset is publicly available on our project page.

**Just noticeable differences (JND).** JND aims to characterize the boundary when a distortion becomes _just_ noticeable. Below a threshold, a small perturbation (e.g., a 1-pixel shift) appears identical to a human observer. This perceptual test provides a complementary signal for perceptual similarity and allows us to exploit uniform timing and the presence of a correct answer.

Intuitively, an image pair that falls below the JND threshold should be more likely to be selected as similar in the 2AFC test; thus a high correlation between 2AFC and JND scores indicates that our judgments are reliable under multiple experimental settings. To assess how well our perceptual tests agree, we collect judgments on the triplets from the 2AFC test set, filtering for triplets that "straddle" the JND threshold. Specifically, given triplet \((x,_{0},_{1})^{}\), we independently collect JND judgments on pairs \((x,_{0})\) and \((x,_{1})\), keeping triplets where humans find _one and only one_ to be identical. In total, our JND dataset contains 411 triplets \(^{}=\{(x,_{0},_{1}),s\}\), where users chose one of \(s\{0,1\}\) to be identical.

We show our JND procedure in Figure 2. We interleave two images and their respective distortions into sequence \(x v\), and ask whether the images in each pair were identical. This exploits masking, a perceptual phenomenon that occurs when an image is obscured by another. By masking with a time gap, we standardize viewing time across users and prompt for a decision based on an initial reaction and memory of the image. For each user, we show 48 distorted pairs, along with 24 identical pairs, to balance the expected responses. We collect 3 judgments per pair, with no person seeing a repeat, and take the majority vote as the label. Users mark 20.4\(\%\) of distorted images being identical, indicating that some of our distortions indeed fall below the noticeable threshold.

## 4 Perceptual metric learning

We next evaluate how well pretrained embeddings and learned metrics align with our data, and we investigate if alignment can be improved by fine-tuning.

### Embeddings as a distance metric

We denote a distance between two images as \(D(,;f_{})\), where \(f_{}\) is a feature extractor. We evaluate a variety of state-of-the-art candidate embeddings and metrics. LPIPS  and DISTS  use CNN backbones, whereas DINO , CLIP , OpenCLIP , and MAE  use transformer-based backbones . Following standard practice, distance \(D(x,;f_{})=1-f_{}(x),f_{}()\) is taken as the cosine distance between the \(\) tokens taken from the last layer for DINO and MAE (before and after the layer normalization, respectively), and the embedding vector for CLIP and OpenCLIP. For LPIPS and DISTS, \(D(x,;f_{})\) is simply the distance metric itself. Given a triplet \((x,_{0},_{1})\), and a feature extractor \(f_{}\), the model vote is calculated as:

\[=1,&d_{1}<d_{0}\\ 0,&d_{0}<d_{1},d_{0}=D(x,_{0};f_{}) d_{1}=D(x,_{1};f_{}). \]

Figure 3: **Training overview. Our perceptual metric is an ensemble of backbones, concatenating the representations together and fine-tuning with LoRA. We evaluate how well cosine distance on candidate feature spaces aligns with perceptual judgments \(y\).**

**Evaluating human-metric agreement.** Recall that for a triplet, we collect \(y\), indicating which image a human selects as more similar. We evaluate how often each metric agrees with human judges as \(_{}(f_{})=_{^{}}[_{=y}]\) and \(_{}=_{^{}}[_{=s}]\).

### Learning an improved metric

**Objective.** To better align a perceptual metric with human judgments, given triplet \((x,_{0},_{1})\) we maximize the difference between the perceptual distances \(d_{0},d_{1}\), with smaller distance associated with the distortion \(y\) humans voted to be most similar. To accomplish this, we map \(y\{0,1\}\{-1,1\}\) and use a hinge loss, equivalent to triplet loss  between the embeddings, with a margin of \(m=0.05\):

\[(y,)=(0,m- d), d=\ d_{0}-d_{1}. \]

**Tuning.** Next, we investigate the best method to tune or modify feature extractor \(f_{}\). A challenge is the large number of parameters (billions, for ViT backbones), compared to the relatively small amount of perceptual data. Inspired by , we first try tuning via an MLP head, adding a 1-hidden-layer MLP with a residual connection on top of the pre-trained embeddings. We compare this to fine-tuning through the pre-trained backbones using Low-Rank Adaptation (LoRA) , which we find to achieve better results than full fine-tuning. Using LoRA (\(r=16\), dropout \(p=0.3\), \(=0.5\)) we tune approximately 0.67% of each model's parameters. In all experiments, LoRA significantly outperformed using the MLP head.

**Feature concatenation.** As multiple models may provide complementary features, we try combining them to boost performance. We concatenate features from the best-performing models on our dataset (DINO , CLIP , and OpenCLIP ) into one ensemble metric. By training just a handful of parameters in this ensemble (through LoRA ), we gain the benefits of each large model's embedding space without sacrificing much computational load while fine-tuning.

## 5 Experiments

### How well do existing metrics align with human judgments?

In Figure 5, we show how well metrics align with humans on our dataset, across low-level metrics (PSNR, SSIM), prior learned metrics (LPIPS, DISTS) and recent large vision model embeddings

Figure 4: **Metrics performance on our benchmark.** Large vision models OpenCLIP and DINO outperform prior learned metrics LPIPS and DISTS (orange) (chance = 50%). Further tuning on our perceptual data with an MLP (blue) improves performance over out-of-the-box features (green); we find Low-rank LoRA  tuning boosts performance significantly (pink). Ensembling CLIP, OpenCLIP, and DINO models together improves performance as well. Our final model, _DreamSim_, combines these insights and achieves high agreement with humans (96.16\(\%\)). Error bars represent a 95% confidence interval.

Figure 5: **Correlation between 2AFC & JND.** We plot the agreement between human judgments and existing similarity metrics on two tasks, 2AFC and JND. The strong correlation between the metrics’ agreement with both tasks suggests that our dataset captures a general notion of similarity that can be replicated in two separate, independent human studies.

(MAE, CLIP, DINO). The best-performing configurations of existing base models are DINO B/16, MAE B/16, CLIP B/32, and OpenCLIP B/32 (additional settings are in SM Section B.1). However, existing metrics have significant misalignments with humans; for example, DINO has approximately 10% disagreement. To improve performance, we finetune with LoRA (as described in Section 4.2), denoted as Tuned - LoRA, achieving significant alignment improvements over pre-trained baselines (Figure 4) and training an MLP head (Tuned - MLP). Ensembling models by concatenating OpenCLIP, CLIP, and DINO features achieves the highest score across baselines (90.8%), MLP-tuned models (93.4%), and LoRA-tuned models (96.2%). We refer to the best LoRA-tuned ensemble model as _DreamSim_, and use it for subsequent analysis and applications.

**Do different ways of measuring human perception agree?** We corroborate our results on our 2AFC test set by evaluating the models against JND perceptual judgments. The high correlation between 2AFC and JND scores indicates that our 2AFC judgments capture a general sense of similarity that can replicated across different settings (Figure 5).

**Does improved mid-level perception lead to improved low-level and high-level perception?** We use the same models as in Figure 4 to examine how performance on our dataset corresponds to performance on two others: BAPPS, containing image triplets with low-level augmentations, and THINGS, containing triplets with categorical variations. Training on our dataset improves human alignment on BAPPS but _decreases_ alignment on THINGS, suggesting that humans, when making an automatic judgment, tend to focus on appearance-based visual differences (captured in BAPPS and our dataset) rather than high-level object category. We additionally evaluate on the TID2013  and KADID-10k  image quality assessment (IQA) benchmarks as alternative low-level datasets. Similar to BAPPS, we find that our metric outperforms base models and is competitive with low-level metrics despite not training for low-level similarity. For full results, see SM Section B.1.

### What image attributes affect similarity decisions?

**Sensitivity to image ablations.** We study which image properties affect our model's decisions by ablating properties of the image triplets, such as orientation, color, shading, structure, and foreground or background components. To do this, we modify the triplets to change or discard each attribute and compute the agreement between the model decisions on the modified and unmodified triplets. This operation measures the model's robustness when a given attribute is missing. Note that this analysis aims to provide insight into how our model determines similarity between two images, rather than to compare to human judgments; it is possible that humans may be more sensitive to different image attributes.

As shown in Figure 6, our model is least affected by orientation (tested by flipping the reference image), but ablating color or shading (the L or AB channels in LAB color space), structure , or both color and shading together using contours  has larger impacts. We conclude that color, shading, and structure contain critical information for our model, but our model can tolerate

Figure 6: **Sensitivity of our similarity metric to different image ablations. (Left) We ablate elements of the image, assessing performance dropoff by changing the orientation, color, shading, structure, and foreground vs. background content. The model is fairly robust to orientation, and more impacted by changes in color, shading, or structure. Foreground content is more important for similarity recognition than background content. (Right) For an example triplet (unmodified), we show visualizations of removing elements of the images.**

differences in orientation. To ablate foreground and background components, we segment the image  and replace the corresponding pixels with random uniform noise. This operation removes the color, shading, and texture of the region, even though the same outline remains in both. Removing the background has a smaller impact on model alignment compared to removing the foreground, suggesting that the foreground color and texture is more important for similarity recognition in our model compared to the background.

**Alignment with semantic and low-level metrics.** Given that some image attributes, like semantics, are difficult to ablate, we complement our ablation analysis by examining how well semantic and handcrafted low-level features align with model judgments. We use 10,000 random image triplets from MS-COCO  annotated with individual object instances ("things") and background categories such as grass and sky ("stuff"). We compare our metric to LPIPS, DISTS, DINO, and OpenCLIP (Figure 7). Our model aligns less strongly with RGB-color histogram similarity (calculated using histogram intersection ) compared to LPIPS and DISTS, however is more sensitive to color compared to other large vision models, while none align with depth map similarity . Next, we compute alignment with the things-category and stuff-category histograms, which summarize the area occupied by each semantic category. Our model aligns better with _things_-histogram similarity, whereas other models are comparatively more sensitive to _stuff_. This result suggests that our model is relatively more sensitive to foreground object instances. Finally, we compute how often metrics align with the _per-category_ intersection of categorical histograms. Our model aligns best with the presence of people (additional results in SM Section B.1).

## 6 Applications

**Image retrieval.** We use our metric for image retrieval on two datasets: ImageNet-R , which contains 30K images of various renditions of ImageNet categories (e.g., sketches, cartoons), and a random subset of 30K images from MS-COCO , which contains diverse natural photos. Given a query image, we compute its similarity to the entire dataset and present the top-3 nearest images under each metric in Figure 8. As can be seen, our metric finds meaningful neighbors, demonstrating that it can generalize to diverse image domains. We compare to the best performing ViT embedding methods (OpenCLIP, DINO) and prior learned metrics (LPIPS, DISTS), as evaluated in Section 5.1. Compared to the baselines, which are either dominated by low-level similarity (e.g., background color in LPIPS/DISTS), or higher-level similarity (e.g., semantic class in OpenCLIP), our retrieved images exhibit similarity across a continuum of visual traits. See Section B.1 for more results and analysis on sketch-to-photo and photo-to-sketch domains.

We confirm our retrieval quality with a user study. For each metric, we retrieve the top-10 neighbors for a set of query images. For ImageNet-R, users prefer our metric's retrievals 36.8\(\%\) of the time, followed by OpenCLIP (28.7\(\%\)) and DINO (19.5\(\%\)). We observe similar trends in a study with COCO images (see Figure 9).

**Feature inversion.** To better understand the information captured by our metric, we apply existing feature inversion techniques, where the aim is to optimize for an image that is similar to a target image under some metric. We compare our metric to the DINO and OpenCLIP embeddings, as well as to an ensemble embedding that simply concatenates all features without finetuning.

Figure 7: **Alignment with semantic and low-level metrics.** We select random triplets from the COCO dataset and assess how well low-level and semantic metrics predict model judgments. Compared to LPIPS & DISTS, our model is more sensitive to objects that appear in individual instances (“things”) and the presence of people, and less sensitive to “stuff” categories, such as sky and road. Compared to OpenCLIP & DINO, color explains more of our model’s decisions. We mark our model’s results with a dashed pink line for comparison.

Figure 10 shows the inversion results under a range of image priors. Under vanilla optimization, i.e., not using any image prior , our metric's results exhibit higher fidelity to the original colors, shapes, and semantics. Using Deep Image Prior , our metric reveals better global layout preservation and semantics. This is also evident when using our metric to guide recent generative models , resulting in higher quality image samples that resemble the semantic concepts and appearance of the target image.

Consistent with Section 5.2 and the retrieval results, these visualizations demonstrate our metric's sensitivity to color, layout, and semantics. The increased quality of our fine-tuned embeddings compared to just ensembling shows the effectiveness of tuning on our human-aligned dataset. See SM for more examples, and Section A.4 for details on feature inversion methodology.

Figure 8: **Nearest-neighbor retrieval across different metrics. Nearest neighbor retrieval on ImageNet-R (top) and COCO (bottom), comparing different metrics. Although the datasets include images outside of our training domain, our model consistently retrieves neighbors with similar appearance and class to that of the query image.**

Figure 9: **User preferences for image retrieval results by metric. We conduct a user study that collects preferences for retrieval results from LPIPS, DISTS, DINO, OpenCLIP, and DreamSim. We visualize one standard deviation above and below each bar. On ImageNet-R (top, orange), our metric is preferred by users 36.8% of the time, followed by OpenCLIP (28.7%). Similarly, on COCO (bottom, blue), users prefer our metric 33.6% of the time, with DINO being the second choice (28.1%).**

**k-NN Classification.** We evaluate our metric as a \(k\)-Nearest Neighbors classifier, which requires the retrieval of images that are both visually and semantically relevant to a given query. We find that our metric outperforms all baselines on the ObjectNet  dataset and performs competitively with DINO on the ImageNet100  dataset. For full details and results, refer to SM Section B.1.

## 7 Discussion

We expand the fundamental task of measuring perceptual image similarity to encompass factors beyond mere low-level similarity, spanning multiple notions of similarities (e.g., object poses, colors, shapes, and camera angles). By harnessing a state-of-the-art generative model, we overcome the lack of suitable data and introduce a new dataset consisting of human-judged synthetic image triplets. We evaluate candidate embeddings and propose a new similarity metric that better aligns with human judgments, demonstrating its generalization and effectiveness compared to existing metrics. We note that by using Stable Diffusion (SD) , our benchmark is exposed to potential preexisting biases and sensitive content in the model. Our perceptual model is also finetuned from existing pre-trained backbones, and thus may also inherit prior errors and biases. However, our work is a step in aligning representations to human preferences, and we believe our work can inspire further research in leveraging advances in generative image models for perceptual studies.

**Acknowledgments.** We thank Jim DiCarlo, Liad Mudrik, Nitzan Censor, Michelle Li, and Narek Tumanyan for fruitful discussions throughout the project. This work was supported by a Packard Fellowship to P.I., Israeli Science Foundation (grant 2303/20) to T.D., Meta PhD Fellowship to L.C., and NSF GRFP Fellowship to S.S.

Figure 10: **Feature inversion across different metrics and image priors. Given a target image, we optimize for an image, where the objective is to match the target image embedding of a given backbone. Without any image prior (Optimization), our metric better recovers the color, shape and semantics of the target image. With a weak image prior  (DIP Inversion), our metric is able to reproduce scene structure and semantics. Using a diffusion model  as a strong prior, our metric better captures overall semantics and scene appearance.**