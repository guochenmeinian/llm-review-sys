# Empowering and Assessing the Utility of Large Language Models in Crop Science

Hang Zhang\({}^{1}\)1 Jiawei Sun\({}^{1}\)1 Renqi Chen\({}^{1}\)1 Wei Liu\({}^{1}\) Zhonghang Yuan\({}^{1}\)

**Xinzhe Zheng\({}^{1}\) Zhefan Wang\({}^{1}\) Zhiyuan Yang\({}^{4}\) Hang Yan\({}^{1}\) Hansen Zhong\({}^{1}\)**

**Xiqing Wang\({}^{3}\) Wanli Ouyang\({}^{1}\) Fan Yang\({}^{2}\)\({}^{}\) Nanqing Dong\({}^{1}\)\({}^{}\)**

\({}^{1}\)Shanghai Artificial Intelligence Laboratory \({}^{2}\)Yazhouwan National Laboratory

\({}^{3}\)China Agricultural University \({}^{4}\)Hangzhou Dianzi University

These authors contributed equally to this work.Corresponding authors.The dataset and code are available at website [https://open-sciencelab.github.io/Crop/](https://open-sciencelab.github.io/Crop/).

###### Abstract

Large language models (LLMs) have demonstrated remarkable efficacy across knowledge-intensive tasks. Nevertheless, their untapped potential in crop science presents an opportunity for advancement. To narrow this gap, we introduce CROP2, which includes a novel instruction tuning dataset specifically designed to enhance LLMs' professional capabilities in the crop science sector, along with a benchmark that serves as a comprehensive evaluation of LLMs' understanding of the domain knowledge. The CROP dataset is curated through a task-oriented and LLM-human integrated pipeline, comprising 210,038 single-turn and 1,871 multi-turn dialogues related to crop science scenarios. The CROP benchmark includes 5,045 multiple-choice questions covering three difficulty levels. Our experiments based on the CROP benchmark demonstrate notable enhancements in crop science-related tasks when LLMs are fine-tuned with the CROP dataset. To the best of our knowledge, CROP dataset is the first-ever instruction tuning dataset in the crop science domain. We anticipate that CROP will accelerate the adoption of LLMs in the domain of crop science, ultimately contributing to global food production.

## 1 Introduction

Crop cultivation has posed a consistent and enduring challenge throughout human history. Nevertheless, the ability to guarantee an ample harvest remains uncertain due to various factors, such as weather conditions, regional disparities, and the prevalence of pests and diseases . Without prompt measures, these factors can result in reduced agricultural output, food shortages, and even widespread hunger, particularly in underdeveloped nations . Consequently, it is essential to leverage technological advancements for grain practitioners to enhance production to combat hunger and achieve global food security .

Recently, the field of artificial intelligence and natural language processing (NLP) has witnessed rapid advancements in large language models (LLMs) . With instructions, LLMs can generate contexts with professional knowledge regarding users' enquires , leading to notable achievements and practical applications in various sectors, including legal consulting , clinical management , _etc._ In the realm of crop science, LLMs have been investigated in addressing agricultural-related exam questions , rendering the opportunity to harness LLMs to improve grain production. Despite the overall acceptable outcomes, LLMs struggle to perform well in specific crop science scenarios, such as pest management . In addition, the agriculture-related exam datasetsused for evaluation, including CCA , Embrapa , and Agri Exam , are not comprehensive in terms of both quantity and locality. Consequently, LLMs are not ready to serve as practical crop science assistants yet.

To harness the full potential of LLMs for crop science, we propose a suite called CROP, which encompasses 1) _an extensive instruction tuning dataset, designed to enhance the domain-specific proficiency of LLMs in crop science._ 2) _a meticulously constructed benchmark, aimed at assessing the performance of LLMs across a variety of domain-related tasks._

For the CROP dataset, we chose two primary crops, rice, and corn, as they are vital in maintaining worldwide food safety [43; 29]. We collect 290 professional books and 62K research papers in English (EN) and Chinese (ZH), covering six continents on Earth. These materials result in over 210K single-turn and multi-turn dialogues on various scenarios of crop science, such as crop variety selection, resource management, pest control, _etc._ The CROP benchmark includes more than 5K bilingual multiple choice questions (MCQs) in three difficulty levels, derived from over 2K extra academic papers relevant to crop science research. As depicted in Figure 1, utilizing CROP dataset enables LLMs to produce more professional responses to the queries of crop practitioners.

Our core contributions can be summarized as follows:

**An instruction tuning dataset on crop science.** We construct a pioneering, domain-specific instructional tuning dataset tailored for crop science. CROP dataset provides a rich, context-specific resource for training LLMs, leading to more accurate and relevant responses for crop farming issues.

**A benchmark on crop science.** We develop a new benchmark to evaluate LLMs' performance in crop-related tasks. CROP benchmark exceeds prior benchmarks in both quantity and regional coverage, facilitating a more thorough evaluation of LLMs' proficiency in crop science.

**Comprehensive experiments and analysis.** Through extensive experiments, we observe an average accuracy improvement of 29% in selected LLMs after fine-tuning with the CROP dataset, demonstrating the effectiveness of our proposed dataset.

## 2 Dataset Collection

For the CROP dataset, we intend to propose a highly automated data generation pipeline targeting knowledge-intensive scenarios in crop farming practices. A knowledge-intensive scenario is defined as a situation or process that necessitates a substantial amount of specialized knowledge, expertise, and comprehension . Managing microlimates in protected cultivation or understanding pest life cycles for crop protection exemplify such cases in crop science. Dataset generation is LLM-centric with concentrated human intervention incorporated at crucial junctures. If not otherwise indicated, the LLM utilized in this paper is fixed to be GPT-4 . The whole data generation process, as depicted in Figure 3, is a series of transformations that converts raw digital documents to the final dialogue collection. We create a universal mechanism for dialogue task design and develop separate pipelines for the generation of single-turn and multi-turn dialogues.

Figure 1: Schematic overview of an intended use case. By fine-tuning a base LLM using the proposed CROP dataset, we obtain a new version whose answer becomes more accurate and comprehensive, which is validated by the proposed CROP benchmark objectively.

### Data Source

We strive to ensure that our raw data originates from a wide range of sources, covering most knowledge-intensive application scenarios in the crop science domain. The raw data in the unstructured format, encompasses research papers, professional books, and corporate reports. In addition, we include human-generated structured raw data, which describes various information about different types of cereals. More details are shown in Section 3.2.

### Task Design

We present a framework that synthesizes the expertise of researchers and domain specialists to enhance the task design process. For single-turn dialogues, task types can be either general or domain-specific, with each type compromising specified tasks. For multi-turn dialogues, tasks are defined through a consecutive process that first identifies recommended scenarios and subsequently determines concrete tasks. Tasks in both single-turn and multiple-turn dialogues are designed with an emphasis on practical applicability. Task composition is shown in Figure 2. An in-depth explanation of the task design procedure is shown in Appendix J.1, with examples and mathematical formulations.

Figure 3: Schematic overview of the dialogue collection procedure. Raw data is first converted to TXT or XLS format using text extraction tools. We then prompt an LLM to either directly generate Q&As from unstructured data or design templates that further transform structured data into dialogue format. After additional filtering steps with both human and LLM involved, we get the CROP dataset. Solid lines represent input/output, and dashed lines indicate operation.

Figure 2: Hierarchical view of tasks in CROP dataset. Dialogues can be single-turn or multi-turn (first tier). The second tier specifies task types. The third tier further decomposes these types into finer-grained tasks. Task-specified topics are rendered around the taxonomy.

### Single-turn Q&A

The raw data comes in a variety of structured and unstructured formats. For unstructured raw data, as shown in Figure 3, we choose appropriate text extraction tools for the conversion of document format as the initial step. The information extracted, saved in TXT format, was further refined, ensuring the preservation of specialized knowledge and standardization of the text format. LLM is then utilized to construct single-turn dialogues for a broad spectrum of tasks, using the cleaned texts and task-dependent prompts. For structured raw data, we employ templates generated by the LLM to convert information stored in XLS files into the dialogue format.

For unstructured data, the prompt for the LLM, regardless of tasks, comprises four components: Role Description, Reference Text, Guidelines, and Example Dialogue. This structure adheres to the "Persona-Task-Context-Format" rule as suggested by "Prompting Guide 101" . Role Description employs role-play prompting , a widely used approach for enhancing prompt effectiveness. We further refine the method by making the assigned role context-adaptive. Reference Text contains the context depending upon which Q&A pairs are generated. Guidelines consist of detailed, task-dependent restrictions. In addition, we incorporate XML tags and a one-shot demonstration example, applying techniques from "Anthropic User Guides" . We anticipate all generated pairs to exhibit deterministic bias and be strictly constrained in patterns. Examples of prompt-generation pairs are shown in Appendix J.2.

### Multi-turn Q&A

In CAMEL , a framework for conversation generation for tasks in the problem-solving scenario is proposed. Considering the practice-oriented nature of our intended use cases, we propose a domain-adapted version, as depicted in Figure 4. Domain experts first suggest common application scenarios. For each scenario, an LLM then generates several tasks and assigns a role pair, consisting of an assistant role and a user role, for each task. Dialogues are then generated in a multi-agent setting where the assistant role and the user role are assigned to a user agent and an assistant agent correspondingly to complete a designated task. The prompts for both agents (LLMs) are shown in Appendix J.3. Given the knowledge-intensive nature of covered tasks, we use Retrieval-Augmented Generation (RAG) to enhance the performance of both agents .

Given the role pair and task, an assistant agent \(A\) and a user agent \(U\) engage in a dialogue to collaboratively achieve a goal. The user poses queries, and the assistant is expected to provide responses that address these queries. We denote the query from the user at time \(t\) as \(q_{t}\) and the response from the assistant as \(r_{t}\). The collection of dialogue messages up to time \(t\) can be represented as \(d_{t}=\{(q_{0},r_{0}),,(q_{t},r_{t})\}\).

Specifically, the first query \(q_{0}\) is sampled from a probability distribution over queries \(Q_{0}\), given a context \(c_{}\) that RAG retrieved based on task \(\):

\[Q_{0} P_{U}(q_{0}|c_{}) \]

At the time step \(t+1\), \(U\) takes the historical dialogue message set \(d_{t}\) and an RAG-retrieved context \(c_{d_{t}}\) to provide a new query \(q_{t+1}\), sampled from a probability distribution over queries \(Q_{t+1}\):

\[Q_{t+1} P_{U}(q_{t+1}|d_{t},c_{d_{t}}) \]

The new query \(Q_{t+1}\), along with the dialogue set \(d_{t}\) and RAG-retrieved context \(c_{d_{t}\{q_{t+1}\}}\), is then passed to \(A\), who responds with a message \(r_{t+1}\), sampled from a probability distribution over responses \(R_{t+1}\):

\[R_{t+1} P_{A}(r_{t+1}|d_{t},q_{t+1},c_{d_{t}\{q_{t+1}\}}). \]

After obtaining the response \(r_{t+1}\) to the query \(q_{t+1}\), the dialogue set is updated:

\[d_{t+1}=d_{t}\{(q_{t+1},r_{t+1})\}. \]

The above formulation models a dynamic and interactive dialogue that evolves over time. While this novel framework introduces RAG-retrieved content to refine generations to be context-dependent, this additional content can be highly customized depending on specific use cases. This new mechanism enhances interpretability, providing fresh insights for building a more predictable and controllable generation system.

[MISSING_PAGE_FAIL:5]

Data Source.The data source distribution is diverse for both rice and corn. For the rice part, we use 40,078 research papers from the Web of Science , 186 professional books 4, 645 crop enterprise reports, and human-generated structural data of 5,990 rice species, containing their characteristic information such as rice blast grade and suitable cultivation area. For the corn part, we use 22,056 research papers, 104 professional books, and 1,841 crop enterprise reports.

Geographic Diversity.In our dataset, the sources and cultivation areas of different types of grains originate from various regions of the world. For example, we record Japan if an article introduces a certain grain that was first cultivated in Japan; we record China if an article discusses whether a particular region in China is suitable for grain cultivation. For rice and corn, our data involves 33 and 43 countries across six continents respectively. All geographical regions involved are shown in Appendix Table 6 and 7.

Content Length.We conduct a statistical analysis of the content length measured by token counts (using the official GPT-4 tokenizer) regarding grain species, languages, and tasks in the single-turn dialogue dataset. The results are illustrated in Appendix Table 8. Of all the tasks, closed-book Q&A (CQA) has the shortest average length at 128 tokens, while the summary task has the longest average length at 446 tokens. The average lengths of rice and corn samples show no significant variation. As for language perspective, the Chinese samples are considerably longer, approximately twice the length of the English samples. In the multi-turn dialogue dataset, most samples are of 4 rounds (85%), with a small portion (15%) of 3 or 5 rounds, shown in Appendix Figure 8.

Content Distribution.We focus on the content distribution of the domain Q&A set (including closed-book and open-book Q&A) which mainly consists of professional knowledge and then found that it encompasses a diverse range of topics. For Chinese Q&A samples, topics cover maize (corn) breeding, hybrid rice, growth characteristics, _etc_. For English Q&A samples, topics include selection, resistance, maize (corn), _etc_. More details are provided in Appendix Figure 9 and 10, which illustrate the information cartography of the Chinese and English Q&A samples respectively.

## 4 Benchmark

To quantitatively evaluate the performance of both commercial LLMs and cutting-edge open-source LLMs fine-tuned with the CROP dataset, we present the CROP benchmark. To avoid disputes arising from subjective questions, our benchmark consists of only MCQs, with a label of difficulty level (easy, moderate, difficult) assigned to each.

### Data Source

Additionally, we select 4,018 literature articles in both Chinese and English, on rice and corn for the generation of benchmarks, distinct from the literature used to construct the CROP dataset. To ensure the data source quality, all literature articles are downloaded from the Web of Science .

### Pipeline Design

Q&A pairs, utilized for benchmarking objectives, are not generated in the same manner as CROP dataset that employs a task-oriented pipeline. We abstain from incorporating directives from researchers and domain experts, or from defining specific tasks beforehand, instead allowing the model to independently determine the actual format and content of the Q&A pair. By imposing fewer constraints, we intend to furnish a benchmark that facilitates a comprehensive evaluation of an LLM's capabilities for domain-knowledge tasks, thereby benefiting the broader community.

Using the same pre-processing method as the dataset, we convert the initial PDF files into TXT format and feed them into GPT-4 to generate Q&A pairs. Specifically, our prompts are tailored to closed-book MCQ generation because we prioritize a model's knowledge retrieval capability. The prompt for generating the benchmark dataset is provided in Appendix J.4, The content of Q&A pairs is related to the knowledge of crop science. Specifically, to ensure the fairness and quality of the benchmark, we impose restrictions on their generation, including equal lengths and a certain level of differentiation in content. From a corpus of over 2,000 articles, we extract over 6,000 MCQs. To ensure the quality of MCQs, we conduct manual screening and select 5,045 MCQs as the final benchmark, which are subsequently categorized according to difficulty levels.

### Difficulty Specification

We classify the 5,045 questions in the benchmark into three difficulty levels: easy, moderate, and difficult, using GPT-4 and GPT-3.5. Easy questions are those both models answered correctly, moderate questions are those answered correctly only by GPT-4, and difficult questions are those answered incorrectly by GPT-4. The statistics are shown in Table 1.

### Topic Distribution

We conduct a statistical analysis of the content distribution in the CROP benchmark. For correlated MCQs, the most frequently occurring topics are corn variety, biological characteristics, and genetics of corn. For rice-related MCQs, the most frequently occurring topics are hybrid rice, rice characteristics, and pests and diseases. We represent the main topics of our benchmark in Appendix Figure 11, which almost cover topics that are currently of concern to experts in the crop science domain. This illustrates the practical value of the CROP benchmark.

### Benchmark Comparison

We compare our benchmark with other evaluation benchmarks in the crop science domain. CROP benchmark consists of 5045 Chinese and English MCQs and covers 22 countries across six continents, surpassing existing agriculture-related question databases in terms of language types, size, and geographic coverage. Details are presented in Appendix I.2.

## 5 Experiments

We employ the CROP benchmark to evaluate the performance of currently accessible LLMs and conduct a comprehensive analysis.

### Model Selection

The LLMs we benchmarked can be divided into two groups: 1) _Commercial LLMs_: this group comprises GPT-3.5 , GPT-4 , Claude-3 , and Qwen , all accessible via their API service. 2) _Open-source LLMs_: this group contains LLaMA3-8B , Qwen1.5-7B , and InternLM2-7B , as well as their fine-tuned versions on CROP dataset. We provide training details of open-source LLMs in Appendix K.

   Level & Count & Proportion \\  Easy & 1,613 & 31.97\% \\ Moderate & 2,754 & 53.72\% \\ Difficult & 722 & 14.31\% \\   

Table 1: Proportion of Difficulty.

Figure 5: Benchmark pipeline overview. We prompt an LLM to generate MCQs from TXT files. After additional filtering steps with both human and LLM involved, we get the CROP benchmark, comprising three difficulty levels.

### Evaluation Protocol

We assess model performance on CROP benchmark using accuracy. Owing to our adoption of text generation for eliciting responses from various LLMs, we annotate answer options accompanying each question with Roman numerals (I, II, III, IV) when designing prompts. To match the generated responses with the answer options, regular expressions are utilized to identify and extract the Roman numerals. In cases where this approach fails to accurately decipher the response, we resort to manual inspection by domain experts, thereby ensuring accurate calculation of accuracy.

### Performance Comparison

Table 2 showcases the performance of selected LLMs on the CROP benchmark. It examines whether commercial LLMs possess professional knowledge in crop science and if the capabilities of open-source LLMs can be enhanced when fine-tuned with the CROP dataset. For commercial models, Claude-3 achieves the highest overall accuracy (0.900), while GPT-3.5 lags behind (0.328). Even though GPT-4, Claude-3, and Qwen show acceptable general performance, they struggle with difficult tasks, demonstrating the rationality of difficulty level division and the efficacy of the CROP benchmark. In the open-source category, we choose CQIA  as a general-task instruction tuning dataset to enhance the limited instruction-following capability of the base models . Building on this, we employ the CROP dataset for additional training to confirm that it can enhance an LLM's performance in responding to crop science-related queries. The findings indicate that when further fine-tuned with the CROP dataset, there is an average improvement of 9.2%. Moreover, we can observe that in most cases, the improvements brought by CROP are relatively more compared with CQIA. The comprehensive results underscore the efficacy of CROP dataset in fostering the professional capabilities of LLMs.

### Analysis on Training Epochs

To evaluate the performance of fine-tuned LLMs under different training epochs, we conduct an experiment in terms of training epochs, with the results summarized in Table 3. Different open-source LLMs show distinct convergence tendencies. For instance, LLaMA3 reaches optimal performance

    &  &  &  &  \\   & & & & Easy \(\) & Moderate \(\) & Difficult \(\) \\   \\ GPT-4\({}^{1}\) & API & N/A & 0.856 & 1.000\({}^{2}\) & 1.000\({}^{2}\) & 0.000\({}^{2}\) \\ GPT-3.5\({}^{1}\) & API & N/A & 0.328 & 1.000\({}^{2}\) & 0.000\({}^{2}\) & 0.061 \\ Claude-3\({}^{1}\) & API & N/A & 0.900 & 0.982 & 0.968 & 0.458 \\ Qwen\({}^{1}\) & API & N/A & 0.866 & 0.987 & 0.945 & 0.301 \\   \\ LLaMA3-Base & Weights & SB & 0.348 & 0.443 & 0.341 & 0.161 \\ +CQIA & Weights & SB & 0.643 (+0.295) & 0.791 (+0.348) & 0.651 (+0.310) & 0.281 (+0.120) \\ +CROP & Weights & SB & 0.752 (+0.404) & 0.866 (+0.432) & 0.772 (+0.431) & 0.378 (+0.217) \\ +CQIA+CROP & Weights & SB & 0.754 (+0.406) & 0.918 (+0.475) & 0.779 (+0.438) & 0.295 (+0.134) \\  Qwen 1.5-Base & Weights & TB & 0.646 & 0.709 & 0.646 & 0.302 \\ +CQIA & Weights & TB & 0.688 (+0.042) & 0.880 (+0.081) & 0.689 (+0.043) & 0.258 (-0.044) \\ +CROP & Weights & TB & 0.676 (+0.030) & 0.849 (+0.050) & 0.688 (+0.042) & 0.202 (-0.100) \\ +CQIA+CROP & Weights & TB & 0.709 (+0.063) & 0.910 (+0.111) & 0.704 (+0.058) & 0.227 (-0.075) \\ InternLM2-Base & Weights & TB & 0.368 & 0.445 & 0.381 & 0.148 \\ +CQIA & Weights & TB & 0.723 (+0.355) & 0.861 (+0.416) & 0.750 (+0.369) & 0.317 (+0.169) \\ +CROP & Weights & TB & 0.748 (+0.380) & 0.945 (+0.500) & 0.761 (+0.380) & 0.212 (+0.064) \\ +CQIA+CROP & Weights & TB & 0.768 (+0.400) & 0.939 (+0.494) & 0.794 (+0.413) & 0.285 (+0.137) \\   

* GPT-4 API is "gpt-4-turbo-2024-04-09". GPT-3.5 API is “gpt-3.5-turbo-0125". Claude-3 API is “claude-3-opus-20240229". Qwen API is “qwen-max”.
* The accuracy of 0.000 and 1.000 is explained in Section 4.3.

Table 2: Performance of selected LLMs on the CROP benchmark. We indicate the accuracy changes of the fine-tuned LLMs compared to the base models in blue, where the accuracy has generally improved when the CROP dataset is used in fine-tuning.

following four epochs, whereas InternLM2 attains peak performance after just two epochs of training. Contrarily, the convergence trend of Qwen1.5 appears to be relatively unstable, and its performance enhancement (6.3%) is marginal compared to its base version. More experiments on training epochs when the model is fine-tuned only on the CROP dataset are elaborated in Appendix 14.

### Analysis on Languages

Given that our benchmark includes both Chinese and English, the performance of LLMs on questions in these two languages has been reported in Table 3. After four epochs of training with the CROP dataset, models did not exhibit a remarkable language bias, as variations of accuracy across various languages fall within the range of 0.4% to 3.2%. These results underscore the robustness of the model in multilingual contexts, ensuring its applicability in diverse linguistic scenarios.

## 6 Limitations

The construction of the CROP dataset faces several limitations. For raw data, its collection focuses primarily on a selected number of crop species, specifically corn and rice. Other prevalent crop families, such as wheat, tuber crops, and beans, were rarely included. The format of the source data is text-only. While this choice ensures compatibility with the GPT-4 API, the omission of data in other modalities may limit the richness of the dataset. Crop science is a dynamically evolving field. New findings and research studies should be integrated to keep the dataset up-to-date. In terms of the data construction pipeline, extraction tools used during data collection introduce errors, while efforts to enhance their performance are still ongoing. LLMs, when generating Q&A pairs, may exhibit biases due to their training data. Task design influenced by researchers and domain experts can also introduce bias towards certain question types. In the experiment aspect, rigorous empirical investigations are necessitated to ascertain the efficacy of the suggested dataset in fine-tuning LLMs with other parameter magnitudes (70B, 110B). Detailed discussion is shown in Appendix D.

## 7 Conclusion

In this paper, we propose the CROP dataset to improve the professional capabilities of LLMs in the crop science domain. Additionally, to compensate for the absence of an open-source benchmark for evaluating models' expertise in this domain, we introduce the CROP benchmark, which comprises MCQs for objective assessment. We hope that the proposed dataset and benchmark can foster AI research in crop science, facilitate knowledge transfer for agricultural practitioners, enhance crop yields, and contribute to solving hunger issues.

    &  &  &  &  &  \\   & & & & Easy \(\) & Moderate \(\) & Difficult \(\) & Chinese \(\) & English \(\) & Variation \(\) \\  LLaMa3-Base & N/A & 8B & 0.348 & 0.443 & 0.341 & 0.161 & 0.327 & 0.369 & 4.2\% \\ +CQIA+CROP & 1 & 8B & 0.738 & 0.903 & 0.758 & 0.292 & 0.719 & 0.757 & 3.8\% \\ +CQIA+CROP & 2 & 8B & 0.742 & 0.902 & 0.772 & 0.271 & 0.729 & 0.755 & **2.6\%** \\ +CQIA+CROP & 4 & 8B & **0.754** & **0.918** & **0.779** & **0.295** & **0.738** & **0.770** & 3.2\% \\  Qwen1.5-Base & N/A & 7B & 0.646 & 0.799 & 0.646 & **0.302** & 0.667 & 0.624 & 4.3\% \\ +CQIA+CROP & 1 & 7B & 0.702 & **0.910** & **0.717** & 0.183 & **0.725** & 0.680 & 4.5\% \\ +CQIA+CROP & 2 & 7B & 0.670 & 0.875 & 0.677 & 0.181 & 0.690 & 0.649 & 4.1\% \\ +CQIA+CROP & 4 & 7B & **0.709** & **0.910** & 0.704 & 0.227 & 0.717 & **0.686** & **3.1\%** \\  InternLM2-Base & N/A & 7B & 0.368 & 0.445 & 0.381 & 0.148 & 0.409 & 0.327 & 8.2\% \\ +CQIA+CROP & 1 & 7B & 0.764 & **0.942** & 0.787 & 0.276 & 0.770 & 0.757 & 3.3\% \\ +CQIA+CROP & 2 & 7B & **0.809** & 0.909 & **0.855** & **0.414** & **0.811** & **0.807** & **0.4\%** \\ +CQIA+CROP & 4 & 7B & 0.768 & 0.939 & 0.794 & 0.285 & 0.770 & 0.766 & **0.4\%** \\   

Table 3: Performance of LLMs trained under different epochs. The highest value for each model on different tasks is **bolded**. Variation denotes the accuracy difference between Chinese and English. More experiments on training epochs when the model is fine-tuned only on the CROP dataset are elaborated in Appendix 14.

Acknowledgements

This work was supported by Shanghai Artificial Intelligence Laboratory, the Yazhouwan National Laboratory Project (grant no. 2310CF01), and the Hainan Yazhou Bay Seed Laboratory Project (grant no. B21HJ0001). We thank YZBSTCACC for data storage support.