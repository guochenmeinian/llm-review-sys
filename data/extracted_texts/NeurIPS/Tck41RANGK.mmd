# MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable Convergence

Ionut-Vlad Modoranu\({}^{1}\)

Mher Safaryan\({}^{1}\)

Grigory Malinovsky\({}^{2}\)

Eldar Kurtic\({}^{1}\)

Thomas Robert\({}^{1}\)

Peter Richtarik\({}^{2}\)

Dan Alistarh\({}^{1}\)

\({}^{1}\)Institute of Science and Technology Austria (ISTA)

\({}^{2}\)King Abdullah University of Science and Technology (KAUST)

Correspondence to ionut-vlad.modoranu@ista.ac.at

###### Abstract

We propose a new variant of the Adam optimizer  called MicroAdam that specifically minimizes memory overheads, while maintaining theoretical convergence guarantees. We achieve this by compressing the gradient information before it is fed into the optimizer state, thereby reducing its memory footprint significantly. We control the resulting compression error via a novel instance of the classical _error feedback_ mechanism from distributed optimization  in which _the error correction information is itself compressed_ to allow for practical memory gains. We prove that the resulting approach maintains theoretical convergence guarantees competitive to those of AMSGrad, while providing good practical performance. Specifically, we show that MicroAdam can be implemented efficiently on GPUs: on both million-scale (BERT) and billion-scale (LLaMA) models, MicroAdam provides practical convergence competitive to that of the uncompressed Adam baseline, with lower memory usage and similar running time. Our code is available at https://github.com/IST-DASLab/MicroAdam.

## 1 Introduction

The Adam  adaptive optimizer and its variants  has emerged as a dominant choice for training deep neural networks (DNNs), especially in the case of large language models (LLMs) with billions of parameters. Yet, its versatility comes with the drawback of substantial memory overheads: relative to naive SGD-based optimization, the Adam optimizer states doubles the memory overhead, as it requires storing two additional parameters for each variable. For large-scale models, these memory demands pose a significant challenge. In turn, this has spurred research into memory-efficient adaptive optimizers, such as AdaFactor , 8-bit Adam , or the very recent GaLore  low-rank projection approach. Despite their popularity and practical utility, the above methods lack rigorous convergence guarantees, and often trade off memory reductions with decreased convergence in practice. This raises the question of whether it is possible to design adaptive optimizers that are not only memory-efficient, but also maintain strong theoretical and practical performance metrics.

**Contributions.** In this paper, we address this gap by introducing MicroAdam, an adaptive optimizer which guarantees low memory usage but also ensures provable convergence. _We develop our approach to improve the performance of finetuning LLMs_ and mainly focus on the research question "are all gradient entries important for optimization?" To answer this question, we start from the idea that we can allow the (lossy) sparse projection of gradient information before it enters the optimizer states; crucially, different from prior work, we ensure convergence by correcting for the inherent errordue to compression by employing a novel variant of _error correction_, a mechanism introduced for distributed optimization (Seide et al., 2014). However, simply using error feedback would not lead to memory savings, since the size of the error correction buffer is comparable to the that of the original optimizer state. Instead, our main algorithmic innovation is in showing that the error feedback _can itself be compressed_ in the context of adaptive optimization. This renders the memory overhead of error feedback negligible, while preserving convergence guarantees.

Specifically, on the theoretical side, we provide a new analysis showing that, under reasonable assumptions on the loss function being optimized and on the degree of compression, MicroAdam provably guarantees convergence, at asymptotically the same rate as AMSGrad (Zhou et al., 2024), i.e. a version of Adam with general convergence guarantees, that fixes a fundamental technical issue in the Adam optimizer's proof (Reddi et al., 2019). The key finding is that our approach allows for the overhead of compression to be shifted to the higher-order terms, where it should not impact practical convergence in common cases. This claim holds both for general smooth non-convex functions, and for non-convex functions under the Polyak-Lojasiewicz (PL) assumption, highlighting a trade-off between the degree of compression of the gradients, and that of the error feedback.

We complement our algorithmic and analytic results with an efficient GPU implementation of MicroAdam, which we validate for fine-tuning language models from the BERT (Devlin et al., 2018), OPT (Zhang et al., 2022) and LLaMA (Touvron et al., 2023) families, with hundreds of millions to billions of parameters. We show that, in practice, gradients can be projected to very high sparsity (99%), while the error correction can be stored at 4 bits per component, without loss of convergence. Concretely, our method can significantly improve upon the memory footprint of the extremely popular 8bit Adam (Dettmers et al., 2021) when fine-tuning models such as LLaMA2-7B/13B (Touvron et al., 2023), at similar or better accuracy. At the same time, MicroAdam provides better accuracy relative to high-compression heuristics such as GaLore (Zhao et al., 2024).

In summary, we provide a new theoretically-grounded approach to memory-efficient adaptive optimization, which has the advantage of providing both theoretical guarantees and good practical convergence, while being scalable to billion-parameter models. MicroAdam could therefore serve as a useful new tool for accurate and memory-efficient optimization of large models.

## 2 Related Work

We mainly focus on related work reducing the cost of optimizer states. Dettmers et al. (2021) considers this problem, specifically by performing fine-grained quantization of the optimizer states. Their work does not alter the Adam algorithm; instead, it deals with the challenge of accurately compressing the dynamically-changing meta-data sequence. As the name suggests, the space savings correspond to roughly halving the memory required by the optimizer states, relative to FP16. In the same vein, AdaFactor (Shazeer and Stern, 2018) and CAME (Luo et al., 2023) reduce memory cost by factorizing the second-order statistics, while the recent GaLore (Zhao et al., 2024) factorizes the gradients themselves before they enter the optimizer state (but does not use error correction). Importantly, these methods are _heuristics_: they do not provide theoretical guarantees under standard assumptions,2 and in practice require careful tuning to preserve convergence (Luo et al., 2023). By contrast, our method is theoretically justified, and provides good practical convergence. Earlier work by Anil et al. (2019) provides convergence guarantees for a compressed variant of Adagrad (Duchi et al., 2010) called SM3, improving upon earlier work by Spring et al. (2019). However, it is not clear how to extend their approach to the popular Adam optimizer, and heuristic methods appear to provide superior performance (Luo et al., 2023).

Conceptually, our work is related to error feedback mechanisms studied in distributed optimization, e.g. (Seide et al., 2014; Alistarh et al., 2018; Karimireddy et al., 2019; Richtarik et al., 2021). Specifically, Li et al. (2022) proved convergence of AdaGrad-like algorithms in conjunction with error feedback, in a distributed environment. Our focus is different: minimizing memory costs in the single-node setting: for this, _we show that the error correction buffer can itself be compressed_. We provide an analysis for the resulting new algorithm, and efficient CUDA implementations.

More broadly, scaling adaptive or second-order optimizers to large models is a very active area. Works such as GGT (Agarwal et al., 2019), Shampoo (Gupta et al., 2018) and M-FAC (Frantar et al.,2021) provided quadratic-space algorithms that are still feasible to execute for moderate-sized DNNs, but will not scale for billion-parameter models. Follow-up work such as AdaHessian (Yao et al., 2020), Sophia (Liu et al., 2023), Sketchy (Feinberg et al., 2023) and EFCP (Modoranu et al., 2023), scaled these approaches via additional approximations. Of these, the closest work to ours is EFCP, which uses sparsification plus standard error feedback to compress the gradient window employed in the Fisher approximation of the Hessian. However, EFCP does not compress the error accumulator, assumes a different optimization algorithm (Natural Gradient (Amari, 2016)), lacks convergence guarantees, and does not scale to billion-parameter models.

## 3 The MicroAdam Algorithm

**Notation.** We consider a standard Adam-type algorithm, which we will augment for memory savings. We will use \(f\) for the loss function, \(d\) for the model size, \(k\) for the gradient density (sparsity \(d-k\)), \(_{t}\) and \(g_{t}\) for the model parameters and gradient at step \(t\) respectively, \(_{t}\) for the learning rate, \(\) for the weight decay parameter, \(m_{t}\) and \(v_{t}\) for the first and second moment of the gradient, \(\) for the numerical stability constant, \(_{1}\) and \(_{2}\) for the momentum coefficients for \(m_{t}\) and \(v_{t}\) respectively. Furthermore, we use \(e_{t}\) for the error feedback (EF) vector, \(b\) the number of bits for EF quantization, \(m\) for the sliding window size, \(=(,)\) for the sliding window of size \(m k\) that stores indices \(\) and values \(\) selected by the Top-K operator.

**Algorithm Description.** We provide pseudocode in Algorithm 1 and highlight the parts related to error feedback quantization in blue. The main idea is to compress the gradients via TopK sparsification before they enter the optimizer state, and to correct for the inherent error by applying error feedback \(e_{t}^{d}\). _Instead of storing the optimizer state directly, we maintain a "sliding window" of highly-sparse past gradients and dynamically re-compute the Adam statistics at each step based on this window._ Yet, this alone does not improve space, as the error buffer partially negates the benefits of gradient compression. Instead, we prove that the error feedback accumulator can itself be compressed via quantization.

In detail, at step \(t=1\), the error feedback \(e_{1}\) is completely zero, as initialized in line 2, and thus, at line 5 the accumulator \(a_{1}\) will only contain the stochastic gradient \(g_{1}\). At line 6, we perform the Top-K compression and only keep the top-\(1\%\) of values \(_{1}\) and their corresponding indices \(_{1}\). The compression is equivalent to choosing the top-\(1\%\) values in the left and right tails (outliers) due to the absolute value we apply on top of accumulator \(a\). At line 7, we remove the outliers from the accumulator because they will be transferred to the buffer matrix \(\). This step is equivalent to \(e a-T_{k}(a)\) found in theoretical results. After line 7, what is left in \(a\) is called the error feedback (e.g. the weights which were not chosen by Top-k). At line 8, we compute the statistics \(\) and \(\) needed for quantization, and at line 9, we effectively quantize the accumulator (e.g. error feedback after line 7). At line 10 we update the buffer \(\), in lines 11, 12 and 13 we compute the statistics \(,\) (computed by squaring the entries of \(\) element-wise) and update the model parameters.

For steps \(t 2\), the only change compared to \(t=1\) is that error feedback \(e\) is not zero anymore. Since the error is compressed, we need to decompress it and add it to the gradient. This process happens at line 5 and it is the point where we feed back the error: the accumulator will store the gradient whose direction is corrected by the error (e.g. the cumulative history of weights not chosen by Top-k at the previous steps).

**Properties and Limitations.** We would like to point out that the resulting update \(u_{t}=m_{t}/(+})\) will always be highly sparse when the window size \(m\) is small. For illustration, if we use density \(k=d/100\) (e.g. \(1\%\) density equivalent to \(99\%\) sparsity) with \(m=10\) and suppose that all rows in the indices matrix \(\) are disjoint, then the overall density in the update \(u_{t}\) will be \(90\%\). The sparsity of \(u_{t}\) increases if rows in \(\) have common values. MicroAdam yields good results for LLM finetuning and pre-training computer vision models, as the experimental section shows. However, we noticed the update \(u_{t}\) of MicroAdam is too sparse to be able to provide good enough updates for LLM pre-training. We believe this happens because the attention layers must receive dense updates to be able to learn the correlations between words.

**Dynamic Statistics.** In AdamStats procedure in Algorithm 2 we implement the unrolled recursion of momentum \(z_{t} z_{t-1}+(1-)g_{t}\) for the last \(m\) sparse gradients as \(z_{t}(1-)_{i=t-m}^{t}^{t-i}g_{i}\) and we also perform the bias correction in the end. Because we compute \(_{t}\) and \(_{t}\) using the last \(m\) sparse gradients in the window, in line 4 we dynamically determine the exponent \(r\) for the decay factor \(^{r}\) based on the current optimization step \(t\), \(i^{th}\) row of the circular buffer \(\) and the window size \(m\). The last gradient added to \(\) will have \(r=0\), while the oldest gradient in \(\) will have \(r=m-1\). In the end, we will add the values \(^{r}_{i}\) to the buffer \(z\) at the corresponding indices \(_{i}\), which is a fast operation because we only manipulate \(1\%\) of values at a time. We discuss the efficient CUDA implementation in the Appendix.

**Algorithm Intuition.** To gain intuition, we illustrate the impact of compressing gradients via TopK before they are incorporated into the optimizer state for Adam, both with and without error feedback (EF). Figure 1 shows how EF fixes AdamW with Top-K compression. The plot on the left shows the optimization trajectory of the original Adam optimizer. The center plot illustrates the convergence of Top-K Adam when we only choose the largest coordinate from the accumulator (equivalent to \(50\%\) sparsity since the problem is 2D). In the end, on the right side we show that adding EF to Top-K Adam recovers the same optimization trajectory as the original Adam optimizer. Extrapolating to higher dimensional problems, our MicroAdam approach helps recover the trajectory of the original Adam optimizer, while using less memory. The results clearly show that EF is essential for fast convergence. Besides, TopK with EF, which is a surrogate of MicroAdam, allows for competitive convergence relative to the uncompressed baseline. In Appendix F, we discuss the implications of Error Feedback applied to GaLore.

```
1:Input: \(_{1},_{2},,,T,d,k\)
2:\(m_{0},v_{0} 0_{d},0_{d}\)\(_{1},_{1} 0,0\)\(e_{1} 0_{d}^{4b}\)
3:for\(t=\{1,2,...,T\}\)do
4:\(g_{t}_{}f(_{t})\)
5:\(a_{t} g_{t}+Q^{-1}(e_{t},_{t},_{t})\)
6:\(_{t},_{t} T_{k}(|a_{t}|)\)
7:\(a_{t}[_{t}] 0\)
8:\(_{t+1},_{t+1} min(a_{t}),max(a_{t})\)
9:\(e_{t+1} Q(a_{t},_{t+1},_{t+1})\)
10:\(_{i,}(_{t},_{t})\)
11:\(_{t}(_{1},)\)
12:\(_{t}(_{2},^{2})\)
13:\(_{t+1}_{t}-_{t}_{t}}{+ {v_{t}}}\)
14:\(i(i+1)\%m\)
15:endfor ```

**Algorithm 1** Pseudocode for MicroAdam with quantized EF

### Efficient Implementation

A direct implementation (e.g., in Pytorch) of the previous algorithm would not bring significant benefits, and would in fact might slow down optimization in terms of wall-clock time. To realize the theoretical gains, we detail a GPU-aware implementation below.

Figure 1: Optimization trajectories of Adam, TopK-Adam and TopK-Adam with EF applied on the Rosenbrock function \(f(x,y)=(1-x)^{2}+100(y-x^{2})^{2}\) starting from \((x_{0},y_{0})=(-,1)\). Notice the extremely “jagged” profile of TopK-Adam without EF, and the recovered convergence when EF is added.

**Accumulator \(a_{t}\).** First, we do not use an additional accumulator tensor \(a_{t}\); instead, we use a CUDA kernel to dequantize the error buffer, and store the result in the _grad_ attribute of the model parameters. This allows us to accumulate the error feedback into the gradients, without allocating a full or half precision \(d\)-dimensional array. Each component of the EF has 4 bits and the entire EF is stored in an array of size \(d/2\) of _uint8_ values.

**Top-K.** Since we run on LLMs with billions of parameters, naive storage of the sparse indices would require using an _int64_ type for the indices matrix \(\), assuming that the Top-K operator is applied globally to all the parameters in \(a_{t}\). To avoid this cost, we apply Top-K in blocks of fixed size \(B_{d}<2^{15}-1=32767\) and store block-relative indices in _int16_ format (during the development of MicroAdam, PyTorch did not have support for _uint16_). Applying Top-K per row to \(a_{t}\) reshaped to 2D is not only faster, but provides the block-relative indices directly.

Computing Top-K in blocks also allows us to allocate and efficiently use CUDA shared memory blocks to dynamically compute the statistics \(_{t}\) and \(_{t}\) for Adam, as described in the AdamStats procedure in Algorithm 2. We allocate the maximum possible shared memory for each thread block and store \(_{t}\) (first half) and \(_{t}\) (second half) at consecutive locations in the shared memory. Once these statistics are computed, it is easy to update the model parameters. Note that the block-relative indices returned by Top-K will be directly used as indices in the shared memory array of CUDA thread blocks to retrieve values from \(\) and \(\).

**Quantization metadata.** Our approach also stores two additional vectors \(\) and \(\) used for quantization. Since the quantization block size \(B_{q}\) is set to a very large value (e.g. \(100K\)), the space required for these two arrays becomes negligible in comparison to the buffer \(\) and error feedback \(e\).

**Practical memory usage.** We note that we apply MicroAdam per layer, and that the size of quantization statistics \(\) and \(\) are allocated based on the layer size. Having many such small tensors may result in slightly sub-optimal memory allocation from Pytorch. This is why our reported memory usage can be higher than the theoretical usage for small models, in the 100M parameter range; these effects disappear for billion-parameter models, where the savings are significant.

### Memory footprint analysis for the optimizer states and comparison with other methods

We now compare the theoretical memory footprint of MicroAdam with AdamW (Loshchilov and Hutter, 2019), AdamW-8 bits (Dettmers et al., 2021), and GaLore (Zhao et al., 2024), _focusing on memory usage of the optimizer states \(m_{t}\)_ and \(v_{t}\), each of size \(d\), expressed in bytes (B). For concreteness, we report the practical memory usage for the optimizer state for a Llama-2 7B model for each optimizer.

* **AdamW** stores states in _float32_ format (4 B per component), resulting in a total memory footprint of \(4d+4d=8d\) (B), while using _bfloat16_ would result in \(4d\) (B) memory. We will refer to these memory footprints as \(M_{AW32}=8d\) (B) \(=50.21\) (GB) and \(M_{AW16}=4d\) (B) \(=25.10\) (GB).
* **AdamW-8 bit** stores states in 8-bit format (1 B per component), both with \(d\) components, with memory footprint of \(M_{AW8}=d+d=2d\) (B) \(=12.55\) (GB).
* **MicroAdam** stores the error feedback \(e\) in 4-bit format (0.5 B per component) and the sliding window \(\) that stores the indices \(\) in _int16_ and \(\) in _bfloat16_ format. Both have \(m k\) components, each requiring 2 B per component. In the end, for \(m=10\) and \(k=d/100\), the memory footprint is \(M_{ A}(m)=0.5d+4mk\) (B) \(=0.9d\) (B) \(=5.65\) (GB), that is, half of AdamW-8bit.
* **GaLore.** Given a neural network with \(L\) layers, where each layer has weight matrix \(W_{i}^{A_{i} B_{i}}\) (shaped as a 2D matrix), the model size \(d=_{i=1}^{L}A_{i}B_{i}\). GaLore uses a rank-\(r\) compression via the SVD composition as \(W_{i}=USV^{T}\), where \(U^{A_{i} A_{i}}\) and we choose the first \(r\) columns of \(U\) as \(R_{i}^{A_{i} r}\) to project the gradient of \(W_{i}\) to the \(r\)-dimensional space. As a consequence, the dimension \(d\) shrinks to \(d_{r}=_{i=1}^{L}A_{i}r=r_{i=1}^{L}A_{i}\), which represents the total number of components to be stored in the GPU memory only for the projection matrices \(R_{i}\). If we suppose they are stored in _bfloat16_ (2 B per component), then the entire memory usage for low-rank projection would be \(2d_{r}\). Note that some layers in the model are rank-\(1\) and they do not need compression, but will still have associated states in Adam, which means they must be tan into consideration when computing the theoretical memory (we will use \(_{1}\) for memory of rank-\(1\) layers). In addition, we have to store the states \(m_{t}\) and \(v_{t}\) for AdamW in _bfloat16_ format, which adds another \(4d_{r}\) bytes. In sum, the total memory footprint of GaLore is \(M_{GLAW16}(r)=6d_{r}+2_{1}\) (B), while for the 8-bit version we get \(M_{GLAW8}(r)=4d_{r}+2_{1}\) (B). In the end, the practicalmemory usage for Llama-2 7B is \(M_{GLAW8}(256)=1.36\) (GB), \(M_{GLAW8}(1024)=5.43\) (GB), \(M_{GLAW16}(256)=2.04\) (GB) and \(M_{GLAW8}(1024)=8.15\) (GB).

**Discussion.** Assume our goal is to obtain a lower memory footprint compared to AdamW-8 bit. We fix the gradient density to \(k=d/100\) and we have to determine the number of gradients (window size) \(m\) for MicroAdam in order to be competitive with AdamW-8 bit.

For this, we have to solve the equation \(M_{ A}(m)=M_{AW8}\) for \(m\), resulting in \(m_{}=37.5\). Specifically, if we use a gradient history of \(m<m_{}\) gradients in \(\), MicroAdam will have theoretical memory savings. We will see that, in practice, this history size \(m=10\) is more than enough for good practical results. As entries in the window past this range are dampened extremely significantly, their influence is negligible. In Appendix D, we provide Python code to compute the theoretical memory usage for the three optimizers for Llama-2 7B.

## 4 Convergence Guarantees for Microadam

In this section, we present our theoretical framework. We begin by introducing and discussing the analytical assumptions used in our theory, providing an analytical view of the proposed MicroAdam algorithm, along with two theoretical convergence results.

### Gradient and Error Compression

We now define two classes of compression operators widely used in the literature.

**Assumption 1**.: _The gradient compressor \(:^{d}^{d}\) is \(q\)-contractive with \(0 q<1\), i.e.,_

\[\|(x)-x\| q\|x\|,x^{d}.\]

The compression we use in Algorithm 1 is the TopK compressor \(T_{k}\) selecting top \(k\) coordinates in absolute value. This is known to be contractive with \(q=}{{d}}}\). Another popular contractive compressor is the optimal low-rank projection of gradient shaped as a \(d d\) matrix, in which case \(q=\) where \(R\) is the projection rank.

The second class of compressors, which we use for the error feedback, requires unbiasedness and relaxes the constant in the uniform bound.

**Assumption 2**.: _The error compressor \(:^{d}^{d}\) is unbiased and \(\)-bounded with \( 0\), namely,_

\[[(x)]=x,\|(x)-x\| \|x\|,x^{d}.\]

One example of \(\)-bounded compressor, a version of which is used in Algorithm 2, is the randomized rounding quantizer we employ, whose properties we provide below.

**Lemma 1**.: _Consider Algorithm 2 with randomized rounding, i.e., for a vector \(x^{d}\) with \(=_{i}x_{i}\) and \(=_{i}x_{i}\), let \(_{i}:=-}{u}+ u+\) be the \(i\)-th coordinate of the quantized vector \(\), where \(\) is the uniform random variable and \(u=-1}\) is the quantization level. Then_

\[[]=x,\|-x\|}{2^{b}-1} {-}{+^{2}}}\|x\|,x^{d}.\]

Next, we provide an "analytical" view of our method in Algorithm 3. Essentially, we use the contractive compressor \(\) for compressing the error corrected gradient information \(g_{t}+e_{t}\), and the unbiased compressor \(\) to compress the remaining compression error \(g_{t}+e_{t}-(g_{t}+e_{t})\).

```
1:Input: parameters \(_{1}\), \(_{2}(0,1)\), \(>0\), step-size \(>0\), \(_{1}^{d}\), \(e_{1}=m_{0}=v_{0}=_{0}=0_{d}\)
2:for\(t=\{1,2,...,T\}\)do
3:\(g_{t}=_{}f(_{t})\)\(\) Compute unbiased stochastic gradient
4:\(_{t}=(g_{t}+e_{t})\)\(\) Add accumulated error \(e_{t}\) and compress
5:\(e_{t+1}=(e_{t}+g_{t}-_{t})\)\(\) Update and compress the error
6:\(m_{t}=_{1}m_{t-1}+(1-_{1})_{t}\)\(\) Update first-order gradient moment
7:\(v_{t}=_{2}v_{t-1}+(1-_{2})_{t}^{2}\)\(\) Update second-order gradient moment
8:\(_{t}=(v_{t},_{t-1})\)\(\) Apply AMSGrad normalization
9:\(_{t+1}=_{t}-}{_{t}+}}\)\(\) Update the model parameters
10:endfor ```

**Algorithm 3** Microadam: Analytical ViewIt is clear from this description that our objective with these two compressors, \(\) and \(\), is to approximate the dense gradient information \(g_{t}+e_{t}\) using two compressed vectors: \(_{t}=(g_{t}+e_{t})\) and \((g_{t}+e_{t}-_{t})\). However, in doing so, we inevitably lose some information about \(g_{t}+e_{t}\) depending on the degree of compression applied to each term. Thus, the condition \((1+)q<1\) required by our analysis can be seen as preventing excessive loss of information due to compression.

### Convergence Guarantees for General Smooth Non-convex Functions

Next, we state our algorithm's convergence guarantees under standard assumptions, stated below:

**Assumption 3** (Lower bound and smoothness).: _The loss function \(f^{d}\) is lower bounded by some \(f^{*}\) and \(L\)-smooth, i.e., \(\| f()- f(^{})\| L\|-^{ }\|\), for any \(,^{}^{d}\)._

**Assumption 4** (Unbiased and bounded stochastic gradient).: _For all iterates \(t 1\), the stochastic gradient \(g_{t}\) is unbiased and uniformly bounded by a constant \(G 0\), i.e., \([g_{t}]= f(_{t}),\ \|g_{t}\| G\)._

**Assumption 5** (Bounded variance).: _For all iterates \(t 1\), the variance of the stochastic gradient \(g_{t}\) is uniformly bounded by some constant \(^{2} 0\), i.e., \([\|g_{t}- f(_{t})\|^{2}]^{2}\)._

**Main Result.** The above assumptions are standard in the literature, e.g. (Defossez et al., 2022, Li et al., 2022, Xie et al., 2023, Zhou et al., 2024). Under these conditions, if the two compressors satisfy the basic condition \((1+)q<1\), we show:

**Theorem 1**.: **(Non-convex convergence rate)** _Let Assumptions 1, 2, 3, 4, 5 hold and \(q_{}:=(1+)q<1\). Then, choosing \(=\{},}\}\), MicroAdam (Algorithm 3) satisfies_

\[_{t=1}^{T}[\| f(_{t})\|^{2}] 2C_{0 }()-f^{*}}{}++C_{0}^{2}G^{ 2})}{})+((G+d)}{T})\]

_with constants \(C_{0}:=^{2})^{3}}{(1-q_{}^{2})^{2}}G^{2}+}\) and \(C_{2}:= q(1+}{1-q_{}^{2}})\)._

**Discussion.** First, notice that the leading term \(}\) of the rate is the optimal convergence speed for non-convex stochastic gradient methods (Ghadimi and Lan, 2016). Furthermore, the obtained convergence rate \((}+)\) asymptotically matches the rate of uncompressed AMSGrad in the stochastic non-convex setup (Zhou et al., 2024). Hence, the added compression framework of the MicroAdam together with error feedback mechanism can slow down the convergence speed only up to some constants including the dimension. Evidently, the additional constants \(C_{0}\) and \(C_{2}\) affected by compression and appearing in the leading terms can be easily estimated once the compressors are fixed. Besides, if we store the full error information without applying \(\) compressor (i.e., \(=0,\,q_{}=q\)), then MicroAdam reduces to the single-node Comp-AMS method by Li et al. (2022) recovering the same convergence rate. The full proof is provided in the Appendix.

### Convergence Rate for Non-Convex Functions under the PL Condition

Next, we show that we can obtain even stronger bounds when the objective satisfies the PL condition:

**Assumption 6** (PL-condition).: _For some \(>0\) the loss \(f\) satisfies Polyak-Lojasiewicz (PL) inequality_

\[\| f()\|^{2} 2(f()-f^{*}), ^{d}.\]

In this case, we can show:

**Theorem 2**.: **(PL convergence rate)** _Let Assumptions 1, 2, 3, 4, 5 and 6 hold, and \(q_{}<1\). Then, choosing \(=\{}, T}{ T}\}\), MicroAdam (Algorithm 3) satisfies_

\[[f(_{T+1})]-f^{*}(^{2 }}{}+(C_{1}+C_{2}^{2})G^{2}}{}+(1+C _{1})(1+d)G^{2}}{})+}( (G+d)}{T^{2}})\]

_with constant \(C_{1}:=}{1-_{1}}(1+C_{2})+}{1-q_{} ^{2}}\)._

**Discussion.** In contrast to the general non-convex setup, the study of non-convex analysis under the PL condition for AMSGrad or Adam-type methods is much less extensive. The only work we found analyzing the PL condition, which claims to be the first in this direction, focuses on Adam when \(_{2} 1\), achieving a convergence rate of \(()\)(He et al., 2023). However, our MicroAdam is based on AMSGrad normalization, and no constraint on \(_{2}\) is imposed in the analysis. Therefore, similar to the general non-convex case, we are able to achieve the best-known convergence rate in the leading term, up to a logarithmic factor. The third, higher-order term has higher constant dependencies, but they should be negligible as the term is dampened by \(T^{2}\). Hence, in this case as well, the theory predicts that the convergence rate of the algorithm should be similar to the uncompressed version, modulo a constant that can be controlled using the compression parameters.

## 5 Experiments

We now validate our optimizer experimentally. We focus on comparing MicroAdam with Adam, Adam-8bit, GaLore and CAME in the context of LLM finetuning on different tasks and with SGD, Adam and AdamW-8bit in the context of ResNets on ImageNet. Concretely, we test our optimizer in full finetuning (FFT) scenario on BERT-Base/Large (Devlin et al., 2018) and OPT-1.3B (Zhang et al., 2022) on GLUE/MNLI and Llama2-7B/13B (Touvron et al., 2023) on the GSM8k math reasoning dataset and on the Open-Platypus instruction tuning dataset, as well as pre-training ResNet models on ImageNet. We provide full details regarding training settings hyper-parameters in Appendix B.

**Finetuning results on GLUE/MNLI.** We first test our integration of MicroAdam in HuggingFace Transformers (Wolf et al., 2020) on moderate-sized language models such as BERT-Base/Large (110M and 335M parameters) and OPT-1.3B, comparing with Adam, Adam-8bit, CAME and GaLore. The results are shown in Table 1. Certain optimizers, notably CAME and GaLore, had numerical stability issues across runs; for a fair comparison, we report the numbers for the run with maximum accuracy. We emphasize that all methods were tuned using the same protocol.

The results show that MicroAdam achieves comparable memory usage to the state-of-the-art heuristics Adam-8bit and GaLore, while being surprisingly lower than CAME on all tasks. The memory savings for GaLore are more visible when the model size increases, which follows our analysis of theoretical memory usage. However, we see that these gains come at a significant _accuracy cost_ for GaLore: across all tasks, it drops at least 1% accuracy relative to MicroAdam. For BERT-Base we ran GaLore with a higher SVD re-computation frequency \(T=20\) (\(10\) lower) and the results did not improve, but its running time was much higher. Relative to 8bit Adam, MicroAdam uses essentially the same memory, but achieves slightly better accuracy.

From these results, we conclude that MicroAdam can provide better accuracy relative to other memory-efficient methods on moderate-sized models, at similar space costs. We show training loss curves in Appendix C.

**Finetuning results for LLaMA2 on GSM-8k.** Next, we perform finetuning on Llama-2 7B/13B on GSM-8k, a challenging grade-school-level mathematical reasoning dataset. The baseline model obtains extremely low zero-shot accuracy on this task and therefore fine-tuning is necessary. In this setup, we compare MicroAdam with Adam and Adam-8bit in terms of evaluation accuracy and memory usage. In Table 2 we show our results for 3 training epochs, global batch size \(32\) with micro-batch (per-device) size \(1\), max sequence length \(512\) on a single GPU, which are the standard

    &  & MicroAdam &  &  &  & GaLore \\  & & \((m=10)\) & & & & \(r=256\) \\   & train loss & 0.2651 & 0.4228 & 0.3402 & 0.6431* & 0.3908* \\  & accuracy & 85.10\% & 83.53\% & 84.61\% & 76.13\%* & 83.82\%* \\  & memory & 2.55 GB & 2.70 GB & 2.53 GB & 2.69 GB & 2.53 GB \\   & train loss & 0.2509 & 0.3857 & 0.2876 & 0.6658* & 0.3768* \\  & accuracy & 86.17\% & 84.79\% & 86.18\% & 75.23\%* & 84.90\%* \\   & memory & 5.98 GB & 6.64 GB & 6.04 GB & 6.59 GB & 5.85 GB \\   & train loss & 0.2122 & 0.2066 & 0.2611 & 0.4959 & 0.2831 \\   & accuracy & 88.18\% & 87.90\% & 87.81\% & 83.15\% & 87.70 \\   & memory & 15.28 GB & 17.66 GB & 15.00 GB & 17.13 GB & 13.66 GB \\   

Table 1: Finetuning results on GLUE/MNLI. We report the entire memory usage read from the GPU during training, that includes the optimizer state, activations and gradients. The asterisk flags the runs for which one or two seeds did not converge (we report the run with maximum performance).

parameters for this task. We integrated our optimizer with the llm-foundry repository of MosaicML and tested via lm-evaluation-harness.

For the 7B model, out results show that MicroAdam can allow accurate full fine-tuning of a 7B model on this task using a single 40GB GPU. Moreover, MicroAdam preserves accuracy relative to Adam, with lower memory usage than the well-optimized implementation of 8bit AdamW, and marginally lower running time for the shorter gradient window \(m=10\). Increasing the window size \(m\) to \(20\) gradients leads to slightly better accuracy, at the cost of higher runtime and space, but still in the 40GB limit. Running GaLore in this setup was infeasible since using SVD decomposition for all layers in the model was too slow. Preliminary experiments (with high runtimes) did not yield competitive accuracy. We show training loss curves in Appendix C.

The results show that MicroAdam allows for full accuracy recovery on this task as well relative to Adam, despite using 50% less memory. (The memory usage and runtime are very similar to those in Table 2 and are therefore omitted from Table 3.) Moreover, MicroAdam obtains consistently better accuracy relative to Adam-8b, especially on the more challenging ARC-c task.

**Finetuning results for LLaMA2-7B on Open-Platypus.** Finally, in Table 3 we present FFT results with various optimizers on the popular instruction-tuning Open-Platypus dataset (Lee et al., 2023). To ensure fair comparisons, we perform the same grid search for each optimizer to find the best performing learning-rate, while keeping all other hyperparameters at their default values. We use \(m=10\) gradients for the sliding window and gradient density \(k=1\%\). Evaluations are conducted following the standard few-shot setup of the Open LLM Leaderboard (Beeching et al., 2023) on the following datasets: ARC-c (Clark et al., 2018), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021), and Winogrande (Sakaguchi et al., 2019).

**Pre-training results for ResNets on ImageNet.** In Table 4 we present our results for pre-training (from scratch, randomly initialized weights) for ResNet-18/50 on ImageNet (see Figure 6 and Figure 7 in Appendix C). We compare our MicroAdam with SGD, Adam and AdamW and report the training loss, validation accuracy and only the optimizer state memory because the total memory usage is not an issue for ResNets on ImageNet (here we focus on the results to emphasize the pre-training performance). For ResNet-18, AdamW reaches the lowest training loss, followed by AdamW-8bit, MicroAdam and in the end MicroAdam. However, despite slightly larger loss, MicroAdam yields the best result among all optimizers, with 2% more than the highly tuned SGD, while having the lowest memory footprint for the optimizer states. For ResNet-50, AdamW-8bit reaches the lowest training loss, followed by AdamW, MicroAdam and SGD. The validation accuracy for AdamW and AdamW-8bit is surprisingly small compared to SGD and MicroAdam. As it is widely known

  
**LLaMA-2 size** & **Optimizer** & **Accuracy** & **State** & **Total** & **Runtime** \\   & **Adam** & 34.50\% & 25.1 GB & 55.2 GB & 1h 17m \\  & **Adam-8b** & 34.34\% & 12.55 GB & 42.5 GB & 1h 18m \\  & **MicroAdam (\(m=10\))** & 34.72\% & 5.65 GB & 37.1 GB & 1h 8m \\  & **MicroAdam (\(m=20\))** & 35.10\% & 8.25 GB & 39.7 GB & 1h 37m \\   & **Adam** & 47.08\% & 48.42 GB & 80 GB & 1h 20m \\  & **Adam-8b** & 45.19\% & 24.21 GB & 80 GB & 1h 17m \\   & **MicroAdam (\(m=10\))** & 44.88 \% & 10.9 GB & 70 GB & 1h 38m \\   

Table 2: FFT results for Llama-2 7B/13B on GSM-8k.

   Optimizer & Memory & Average & ARC-c & HellaSwag & MMLU & Winogrande \\  & Accuracy & 25-shot & 10-shot & 5-shot & 5-shot \\  AdamW & 67.17 GB & 62.10 & 52.56 & 77.38 & 45.53 & 72.93 \\ Adam-8b & 53.93 GB & 61.84 & 51.96 & 77.51 & 44.11 & 73.79 \\ MicroAdam & 46.63 GB & 62.36 & 53.07 & 77.46 & 45.04 & 73.87 \\   

Table 3: FFT results on instruction-following Open-Platypus (Lee et al., 2023) dataset. The results show that MicroAdam fully recovers accuracy relative to baseline Adam, and outperforms the 8bit variant, despite using less memory.

in the community, Adam variants have lower performance than SGD for Computer Vision tasks and MicroAdam fixes this issue (see the Discussion section for an intuitive explanation for this phenomenon).

**Pre-training LLMs.** In this section we explain why we do not include LLM pre-training results. Our motivation is twofold. First, MicroAdam is mainly designed for low-memory finetuning, and the experimental section shows that MicroAdam achieved this goal. Surprisingly, updating only 10% of the weights at each step yields to significantly better performance compared to SGD for ResNets on ImageNet. Secondly, our experiments on LLM pre-training showed difficulties in achieving the same performance compared to AdamW-8bit. Our explanation is that projection matrices from the attention layers must receive dense updates to learn the correlations between words. In contrast to the convolutional filters for CV models, the weights in attention are much larger and try to capture global correlations (features) between words, while the convolutional filters are smaller and capture local features.

**Discussion.** In Appendix A we provide information about the optimization set, intuitive explanations for the implicit regularization effect of MicroAdam, as well as an overview of our results.

## 6 Limitations and Broader Impact

The MicroAdam algorithm we propose is designed and tested with fine-tuning workloads in mind, where the user aims to minimize the memory cost of optimizing over a powerful pre-trained model. Additional work is needed to adapt our approach to the case of LLM pre-training, which presents a different set of challenges, both in terms of implementation and optimization trajectory. We plan to undertake this study in future work as the current implementation works for ResNets.

Another limitation we aim to address in future work is that we have only focused on sparsity as a form of gradient projection. However, our theoretical analysis also applies to low-rank projection of gradients. We believe that our practical implementation can be extended to this case as well, although providing a general, accurate, and efficient implementation will require non-trivial efforts.

Our work introduces a new, accurate, and memory-efficient optimizer for fine-tuning LLMs. The major positive impact of our approach is its ability to maintain performance while reducing memory requirements, thereby lowering the cost of running experiments due to the reduced hardware expenses. It is important to note that while our optimizer can enhance performance and reduce costs, we do not have control over the neural network applications trained with it.