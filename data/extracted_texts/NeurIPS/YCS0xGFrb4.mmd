# Regularized Conditional Diffusion Model for Multi-Task Preference Alignment

Xudong Yu

Harbin Institute of Technology

hit20byu@gmail.com

&Chenjia Bai

Institute of Artificial Intelligence (TeleAI), China Telecom

baicj@chinatelecom.cn

Haoran He

Hong Kong University of Science and Technology

haoran.he@connect.ust.hk

&Changhong Wang

Harbin Institute of Technology

cwang@hit.edu.cn

&Xuelong Li

Institute of Artificial Intelligence (TeleAI), China Telecom

xuelong_li@ieee.org

Correspondence to: Chenjia Bai (baicj@chinatelecom.cn).

###### Abstract

Sequential decision-making can be formulated as a conditional generation process, with targets for alignment with human intents and versatility across various tasks. Previous return-conditioned diffusion models manifest comparable performance but rely on well-defined reward functions, which requires amounts of human efforts and faces challenges in multi-task settings. Preferences serve as an alternative but recent work rarely considers preference learning given multiple tasks. To facilitate the alignment and versatility in multi-task preference learning, we adopt multi-task preferences as a unified framework. In this work, we propose to learn preference representations aligned with preference labels, which are then used as conditions to guide the conditional generation process of diffusion models. The traditional classifier-free guidance paradigm suffers from the inconsistency between the conditions and generated trajectories. We thus introduce an auxiliary regularization objective to maximize the mutual information between conditions and corresponding generated trajectories, improving their alignment with preferences. Experiments in D4RL and Meta-World demonstrate the effectiveness and favorable performance of our method in single- and multi-task scenarios.

## 1 Introduction

In sequential decision-making, agents are trained to accomplish fixed or varied human-designed goals by interacting with the environment or learning from offline data. Two key objectives emerge during training: _alignment_, wherein agents are expected to take actions conforming to human intents expressed as crafted rewards or preferences, and _versatility_, denoting the capacity to tackle multiple tasks and generalize to unseen tasks. A promising avenue involves framing sequential decision-making as a sequence modeling problem via transformer  or diffusion models [2; 3]. This paradigm uses expressive models to capture the trajectory distributions and prevents unstable value estimation in conventional offline Reinforcement Learning (RL) methods . Particularly, utilizing return-conditioned or value-guided diffusion models to perform planning or trajectory generation achieves favorable performance in offline RL [5; 6] and the multi-task variant [7; 8].

Despite the great progress, applying diffusion models to sequential decision-making still faces challenges. (i) The condition generation process relies on a pre-defined reward function to provide return conditions. However, developing multiple task-specific reward functions in multi-task settings requires significant efforts  and may cause unintended behaviors . (ii) The condition generation process with classifier-free guidance  often fails to ensure consistency between conditions and generations. As an example in Figure 1, the return-conditioned Decision Diffuser  struggles to achieve effective alignment for generated trajectories. Inspired by recent works [9; 12; 13], we find that preferences offer more versatile supervision across multi-tasks than scalar rewards. Specifically, the trajectories from the \(i\)-th task are preferred over the \(j\)-th task if we set the \(i\)-th task as the target task. Conversely, we can reverse the preference label when setting the target task to the \(j\)-th task. Therefore, we adopt preferences rather than rewards to guide the conditional diffusion models in offline multi-task RL. The generated trajectories are intended to align with preferences, prioritizing higher returns or specific tasks.

To establish aligned and versatile conditional generation, our proposition involves adopting multi-task preferences and constructing a unified preference representation for both single- and multi-task scenarios, instead of learning scalar rewards from preference data [15; 16]. The acquired representations are aligned with the preference labels. The representations subsequently serve as conditions to guide the conditional generation of diffusion models. In this case, two key challenges arise: (i) _aligning the representations with preferences_, and (ii) _aligning the generated trajectories with representation conditions_.

To address the above challenges, we introduce Conditional Alignment via Multi-task Preference representations (CAMP) for multi-task preference learning. Specifically, we define multi-task preferences and extract preference representations from trajectory segments. (i) To align the representation with preferences, we propose a triplet loss and a Kullback-Leibler (KL) divergence loss to enable preferred and undesirable trajectories mapped into distinct zones in the representation space. The learned representations not only differentiate trajectories yielding higher returns but also discern trajectories across various tasks. Additionally, we learn an 'optimal' representation for each task to represent the trajectory with the highest preference. (ii) To align the generated trajectories with representation conditions in diffusion models, we introduce a Mutual Information (MI) regularization method. It augments representation-conditioned diffusion models with an auxiliary MI optimization objective, maximizing the correlation between the conditions and the generated outputs. During the inference for a specific task, we provide the 'optimal' representation for that task as the condition of the diffusion model, allowing the generated trajectories to adhere to desired preferences. Experiments on D4RL  and Meta-World  demonstrate the superiority of our method and the aligned performance in both multi-task and single-task scenarios.

## 2 Preliminaries

### Diffusion Models for MDP

Diffusion models have emerged as effective generative models adept at learning high-dimensional data distribution \(p(x)\)[2; 19; 20]. A diffusion model comprises a pre-defined forward process \(q(x_{k}|x_{k-1})=(x_{k};}x_{k-1},(1-_{k})I)\) and a trainable reverse process \(p_{}(x_{k-1}|x_{k})\), where \(k[1,K]\) denotes the timestep index, and \(_{k}\) decides the variance schedule. By sampling Gaussian noise from \(p(x_{K})\) and iteratively employing the denoising step \(p_{}(x_{k-1}|x_{k})\) for \(K\) steps, diffusion models can generate \(x_{0} p(x)\). Moreover, if additional conditions \(c\) are introduced into

Figure 1: Illustration of the return-conditioned generation of Decision Diffuser  in _hopper-medium-expert_ task. Existing return-conditional diffusion models fail to align generated trajectories with the return condition, while the red line indicates the desired relationship between the return conditions and true returns of generated trajectories .

the denoising process such that \(x_{k-1} p_{}(x_{k-1}|x_{k},c)\), diffusion models can also estimate the conditional distribution \(p(x|c)\).

The Markov Decision Process (MDP) is defined by a tuple \((,,P,r,)\), where \(\) and \(\) are the state and action spaces, \(P\) is the transition function, \(r\) is the reward function, and \(\) is a discount factor. We consider an offline setting where the policy is learned from a given dataset \(\). For multi-task learning, we introduce an additional task space \(\) that contains \(m\) discrete tasks. Diffusion models have been used for offline RL to overcome the distribution shift caused by Temporal-Difference (TD) learning . Specifically, a diffusion model can formulate sequential decision-making as a conditional generative modeling problem by considering \(x_{0}\) as a state sequence \((s_{t},,s_{t+h})\) for planning . The condition \(c\) typically encompasses the return along the trajectory and is designed to generate trajectories with higher returns.

Optimizing the conditional diffusion model involves maximizing the log likelihood \( p(x)= p(x|c)p(c)dc\), where \(p(x|c)\) is the conditional likelihood for a specific \(c\). Building on prior research [19; 21], the optimization is achieved by maximizing the Evidence Lower Bound (ELBO):

\[ p(x_{0}) _{}(x_{0},c)_{q( x_{1}|x_{0})}[_{q_{}}[ p_{}(x_{0}|x_{1},c)]]-D_{}(q_{}\|p(c))\] (1) \[-D_{}(q(x_{K}|x_{0})\|p(x_{K}))-_{k=2}^{K}_{q(x_{k}|x_{0})}[_{q_{}}[D_{}(q_{x_{k-1}}\|p_{ }(x_{k-1}|x_{k},c))]],\]

where \(q_{} q_{}(c|x_{0})\) represents the approximate variational posterior mapping \(x_{0}\) to the condition \(c\), and \(q_{x_{k-1}}=q(x_{k-1}|x_{k},x_{0})\). The complete derivation is provided in SSA.1. In practice, this optimization problem can be addressed via a score-matching objective [2; 11] as,

\[_{}=[\|-_{}(x_{k},(1-)c+ ,k)\|^{2}],\] (2)

where \(_{}\) is parameterized by a neural network to predict the noise \((0,I)\), the expectation is calculated w.r.t. \(k[1,K],\), and \((p)\).

### Preference Learning for Conditional Generation

In decision-making, human preferences are often applied on trajectory segments \(=[s_{i},a_{i}]_{i[1,h]}\). For a trajectory pair \((_{1},_{2})\), human feedback yields a preference label \(y\{0,1,0.5\}\) that indicates which segment a human prefers. Here, \(y=1\) signifies \(_{1}_{2}\), \(y=0\) signifies \(_{1}_{2}\), and \(y=0.5\) means that two trajectories have the same preference. Previous studies commonly employ the Bradley-Terry (BT) model  to derive a reward function \(\) from such preferences. Considering learning with offline preference data , and given a dataset \(_{}=\{(_{1},_{2},y)\}\), \(\) is learned by maximizing the following objective:

\[_{}=_{_{}}[y P[_{1} _{2}]+(1-y) P[_{2}_{1}]].\]

In what follows, we simplify the notation by omitting the label \(y\) and denote the preference data as \(\{(^{+},^{-})\}\), where we have \(^{+}^{-}\). Previous methods [24; 15] based on the BT model follow a two-phase learning process, first deriving a reward function from preference data and then training a policy with RL. Nevertheless, this process hinges on the assumption that pairwise preferences can be substituted with a reward model that can generalize to out-of-distribution data . Moreover, these methods often require complex reward models  when preferences are intricate [26; 27].

As a result, we bypass the reward learning process and learn a preference-relevant representation that aligns well with trajectories in both single- and multi-task settings. Then the diffusion models can use these representations as conditions to generate trajectories that align with human preferences. In this setup, we regard \(x_{0}=\) as the trajectory segments, and the condition \(c\) as the preference-related representation of trajectories, denoted as \(w=f()\). Therefore, the learning objective of diffusion model becomes \( p(_{0})\), and the loss function becomes \(_{}=[\|-_{}(_{k},(1- )w+,k)\|^{2}]\).

## 3 Method

In this section, we give the definition of multi-task preferences and introduce how to extract preference representation from pairwise trajectories. Then, we present the conditional generation process and an auxiliary optimization objective to align the generated trajectories with preferences.

### Versatile Representation for Multi-Task Preferences

Multi-task preferences.In the context of multi-task settings, tasks exhibit distinct reward functions, making reward-relevant information insufficient for providing versatile preferences across tasks. For example, moving fast is preferred and obtains high rewards in a 'running' task, while being unfavorable in a 'yoga' task. To address this challenge, we extend single-task preferences that only contain reward-relevant information to _multi-task_ settings. Specifically, we consider two kinds of preferences given trajectories from \(m\) tasks. For a specific task \(i[m]\), trajectories are assessed based on (i) the **return** of trajectories when they belong to the same task, i.e., \(^{i+}^{i-}\) if \((^{i+})(^{i-})\), where \(()\) calculates the cumulative reward, and (ii) the **task-relevance** of trajectories, i.e., \(^{i}^{j}\) with \(j i\). This means that trajectories from the target task \(i\) are more preferred than any trajectories \(^{j}\) from a different task \(j[m]\).

Preference representations.Based on the multi-task preferences, we propose to learn trajectory representations aligning with the preference data, as shown in Figure 2. The learned representations integrate both the trajectory and preference information, serving as the condition for subsequent trajectory generation. During learning representations, we also need to find the 'optimal' representations \(\{w^{*}_{i}\}_{i[m]}\) that represent the optimal trajectories \(\{^{*}_{i}\}_{i[m]}\) for each task, where \(^{*}_{i}\) is preferred over any offline trajectories in task \(i\). The \(\{w^{*}_{i}\}_{i[m]}\) will be used for inference in diffusion models to generate desired trajectories for each task. Thus, we need to learn a trajectory encoder \(w=f_{}()\) that extracts preference-relevant information and the optimal representation \(\{w^{*}_{i}\}_{i[m]}\).

Furthermore, we model the representations from a distributional perspective to cover their uncertainty. In practice, the distribution of the optimal representation \(p(w^{*}_{i})=(^{*}_{i},^{*}_{i})\) and the distribution of representations given a trajectory \(p(w|)=(_{}(),_{}())\) are both modeled as multivariate Gaussian distributions with a diagonal covariance matrix, where \(\) is parameterized by a transformer-based encoder. To summarize, the learnable parameters include vectors \(\{^{*}_{i},^{*}_{i}\}_{i[m]}\) for \(\{w^{*}_{i}\}_{i[m]}\) and a trajectory encoder \(f_{}\).

Loss functions for \(f_{}\) and \(w^{*}_{i}\).For each task \(i\) in training, we build the multi-task dataset \(=\{^{i+},^{i-},^{j}\}\) to learn the representation space. The preference data are constructed by using the intra-task preference (i.e., \(^{i+}^{i-}\)) and the inter-task preference (i.e., \(^{i+}^{j}\)). We denote the representation distributions as

\[p(w^{+}_{i})=f_{}(^{i+}), p(w^{-}_{i})=f_{}(^{i-})\;\; \;\;f_{}(^{j}).\]

For simplicity, we denote \(p(w^{+}_{i})=(^{+}_{i},^{+}_{i})\) and \(p(w^{-}_{i})=(^{-}_{i},^{-}_{i})\) by omitting the parameter \(\). These distributions are optimized using the following KL loss,

\[(,^{*}_{i},^{*}_{i})=D_{}((^ {+}_{i},^{+}_{i})\|(^{*}_{i},^{*}_{i}))+1/D_{}((^{-}_{i},^{-}_{i})\|(^{*}_{i}, ^{*}_{i})).\] (3)

This loss function encourages the encoder to map trajectories with similar preferences to closer embeddings while distancing dissimilar trajectories. In practice, we find that optimizing two sets of parameters (i.e., \(\{^{*}_{i},^{*}_{i}\}\) and \(\)) simultaneously is unstable and leads to a trivial solution. Thus, we adopt an iterative optimizing process by using a stop-gradient (SG) operator. Specifically, we use the loss \(((),^{*}_{i},^{*}_{i})\) to optimize \(\{^{*}_{i},^{*}_{i}\}\), and \((,(^{*}_{i},^{*}_{i}))\) to optimize the encoder \(f_{}\).

Furthermore, simply minimizing the KL loss cannot prevent the divergence of the unbounded term \(D_{}((^{-}_{i},^{-}_{i})\|(^{*} _{i},^{*}_{i}))\), which may result in deviated representation distributions and unstable training. Hence, we add a triplet loss to learn representations, as

\[(,^{*}_{i})=_{}[(d(^{+}_{i},^ {*}_{i})-d(^{-}_{i},^{*}_{i})+,0)],\] (4)

Figure 2: Illustration of the representation space of trajectories in multi-task preference data. For each task \(i\), the positive samples \(^{+}\) consist of preferred trajectories \(^{i+}\) from task \(i\), while negative samples \(^{-}\) include less preferred \(^{i-}\) from the same task, as well as \(^{j}\) from other tasks. Trajectories from diverse tasks are expected to be differentiated in the representation space, and \(\{w^{*}_{i}\}_{i[m]}\) attempts to characterize the best trajectories for each task.

where \(d\) is the Euclidean distance in the embedding space. The triplet loss calculates the similarity between \(w_{i}^{*}\) and preferred representations \(w_{i}^{+}\), and the similarity between \(w_{i}^{*}\) and unfavorable ones \(w_{i}^{-}\), respectively. By regulating the distances between \(\{_{i}^{+},_{i}^{-}\}\) and \(_{i}^{*}\), the training process is stabilized. Meanwhile, minimizing the triplet loss also guarantees that the optimal embedding is more similar to \(w_{i}^{+}\) and less similar to \(w_{i}^{-}\), while \(w_{i}^{+}\) and \(w_{i}^{-}\) stay away from each other. We set \(\) as a margin between the two similarities and adopt the same iterative optimizing process for \((,_{i}^{*})\). The illustration in Figure 3 captures our learning process. For brevity, we have omitted details related to the distributed form and multi-task learning components while retaining the core methodology.

Prompting with \(w_{i}^{*}\).By optimizing the triplet and KL loss, \(w_{i}^{*}\) gradually aligns with more preferred representations, converging to the optimal representations. After training, the diffusion model can be prompted by \((_{i}^{*},_{i}^{*})\) for a specific task \(i\). The model will then generate the optimal trajectory for task \(i\) with the guidance of conditions. The task \(i\) can be a novel task not present in the training set, and the diffusion model will try to generalize to the new task in the representation space.

### Representation Aligned Trajectory Generation

Given preference-based versatile representations, we train a diffusion model to generate trajectories that align with the representations. Prior work utilizes classifier-free guidance and aligns the generated samples with low-dimensional conditions, such as returns. However, we find this is insufficient to capture the complex relationship between conditions and trajectories. Using the representative method Decision Diffuser  as an example, Figure 1 reveals the relationship between the return conditions and true returns of generated trajectories in the _Hopper-medium-expert_ task. While it is desirable to generate trajectories with higher returns as we increase the return condition, in practice, the attributes of generated samples do not exhibit a smooth transition with changes in the condition. Empirically, Decision Diffuser uses a target return of 0.8-0.9 for the generation process. This difficulty becomes more severe in our method because we cannot exclusively search the high-dimensional representation space and find an empirically effective representation. Therefore, it is critical to enhance the alignment between generated trajectories and previously learned preference representations.

MI regularization.Inspired by recent works on generative models [28; 29; 21], we introduce an auxiliary optimization objective aimed at strengthening the association between representations \(w q_{}(w|)\) and the generated trajectories \(_{0}\). Specifically, we augment the learning objective from Eq. (2) with a regularization term based on mutual information between \(_{0}\) and \(w\). Formally, we train the conditional diffusion model using the following combined objectives:

\[_{q(_{0})}[ p(_{0})]+ I(_{0},w),\] (5)

Figure 3: Overview of our method. (1) We learn preference representations \(w=f_{}()\) and the optimal one \(w_{i}^{*}\) from trajectory segments \(\), which comprise positive samples \(^{+}\) and negative samples \(^{-}\). (2) We augment the diffusion model with an auxiliary mutual information term \(I(_{0};w)\) to ensure the alignment between \(_{0}\) and \(w\). (3) During the inference process, the diffusion model conditioned on \(w_{i}^{*}\) can generate desired trajectories aligned with preferences.

where \(q(_{0})\) indicates the trajectory distribution of the offline dataset \(\), and \(\) is a hyper-parameter controlling the strength of regularization. Eq. (5) encompasses processes such as sampling trajectories \( q(_{0})\), obtaining corresponding representations \(w q_{}(w|)\) via \(f_{}\), and the denoising process \(p_{}(_{k-1}|_{k},w))\) to derive \(_{0}\).

Tractable objective.In practice, sampling \(_{0}\) from \(p(_{0})\) to maximize the MI objective requires the diffusion model to denoise \(K\) steps from \(_{K}\). This process imposes huge computational burden and may lead to potential memory overflow due to the gradient propagation across multi-steps. Consequently, there arises a need for an approximate objective, and an alternative approach is to replace the optimization on \(I(_{0};w)\) with \(I(_{k};w)\). This substitution is motivated by considering the relationship between \(_{0}\) and \(_{k}\), as described by the diffusion process \(_{k}=}_{t}_{0}+_{t}}_{0} :=f_{}(_{0})\). According to the data processing inequality , we have

\[I(_{0};w) I(f_{}(_{0});w)=I(_{k};w),\] (6)

As a result, \(I(_{k};w)\) serves as a lower bound of \(I(_{0};w)\), thus maximizing \(I(_{k};w)\) also maximizes \(I(_{0};w)\). In this case, Eq. (5) can be rewritten as:

\[_{q(_{0})}[ p(_{0})]+ I(_{k},w).\] (7)

We note that the objective in Eq. (7) can be rewritten into an equivalent form that can be optimized efficiently (see SSA.2 for a detailed derivation).

**Proposition 3.1**.: _The optimization objective in Equation (7) can be transformed to_

\[_{1}(,)=_{q(_{0})}[_{}(_{0},w)]-_{p(_{k})}[D_{}(p_{ }(w))\|q_{}(w|_{k})].\] (8)

The first term resembles the ELBO term in Eq. (1) denoted as \(_{}(x_{0}=_{0},c=w)\), which is the same as standard conditional diffusion models in Decision Diffuser . The ELBO term aims to estimate the trajectory distribution via a conditional diffusion process, thus we adopt a similar conditional score-matching objective to optimize it. The second term contributes to enhancing the alignment between \(_{0}\) and \(w\), where \(p_{}(w)\) is empirically averaged on samples \(\) via \(f_{}\), and \(p(_{k}) f_{}(_{0})\) with \(_{0}\) sampling from \(q(_{0})\). We can minimize the KL divergence to optimize \(q_{}(w|_{k})\), a variational predictor to predict the condition \(w\) from the denoised sample \(_{k}\). In practice, we instantiate it with a neural network represented by \(\), taking predicted noises \(_{}(_{k})\) as inputs.

### Algorithmic Description

The entire procedure is shown in Algorithm 1. During training, we iteratively update the representation encoder \(f_{}\) and the optimal representation \(w_{i}^{*}\) to learn versatile preference representations. Then we update the parameters of the conditional diffusion model via the loss function in Eq. (8). To decode the actions from a predicted trajectory, an inverse dynamic model is learned by using a supervised loss from the dataset, as

\[_{}_{s,a,s^{}}\|a-g_{}(s, s^{})\|_{2}^{2}.\] (9)For planning in a specific task \(i\), we use the optimal representation \(w_{i}^{*}\) and the current state \(s_{t}\) as a condition to generate an optimal trajectory \([s_{t},,s_{t+h}]\). Then the inverse dynamics model \(a_{t}=g_{}(s_{t},s_{t+1})\) is used to decode the action from two consecutive states.

## 4 Related Work

Diffusion Models in RL.Diffusion models exhibit significant advantages in generative modeling and characterizing complex distributions. On the one hand, they can be applied for modeling policies, with many research works [31; 32; 33; 34; 35; 36] suggesting that diffusion models are more effective at capturing multi-modal action distributions than other policy types like energy-based policies. On the other hand, diffusion models are adept at directly predicting trajectories via conditional generation, adopting conditions such as value gradients [3; 37; 7], returns , or prompts . Our work also trains diffusion models to generate trajectories, but focuses on guidance from preference representations. Regarding alignment with human preferences, recent works include the integration with designed attributes for various behaviors , fine-tuned policy with preferences [38; 39], and preference augmentation techniques to improve trajectory generation . While these works focus on single-task settings, our work seeks a versatile solution by considering multi-task preferences.

Learning From Human Preferences.Current methods of learning from preferences can be categorized into two groups: _direct learning_ and _indirect learning_. _Indirect learning_ methods involve learning a reward model and incorporating existing RL algorithms. They employ techniques like off-policy learning , pseudo-labeling and data augmentation , iterative updates of reward models and policies , or leveraging Transformer architectures to learn reward models . On the other hand, _direct learning_ methods bypass reward model learning and directly incorporate human preferences into the learning process. This approach involves various strategies, such as combining decision transformer styles with preference embeddings , mapping reward functions to optimal policies [44; 45], or aligning models using extended Bradley-Terry comparisons . This work falls under the category of offline _direct learning_ approaches, framing it as a sequence modeling problem.

Our work considers challenging multi-task settings. While much prior research attempts to find Pareto optimal solutions considering the trade-offs between different preferences, these methods often require heuristic vector selection for unknown Pareto fronts [47; 48], millions of pre-collected preference labels and further online queries , or combination with Gaussian processes to learn preference relations . In contrast, our approach does not require any heuristic methods and learns trajectory representations aligned with preferences from offline data.

## 5 Experiments

In this section, we will validate our approaches through extensive experiments. Our focus revolves around addressing the following key questions: **(i)** Can our approach demonstrate superior performance compared to existing approaches? **(ii)** Does the trajectory encoder discern different trajectories corresponding to multi-task preferences? And does \(\{w_{i}^{*}\}_{i[m]}\) align with the optimal trajectories? **(iii)** Can the diffusion model capture the trajectory distribution and generate trajectories aligned with preferences? **(iv)** How about the generalization ability of our method on unseen tasks? **(v)** To what extent are multi-dimensional representations and regularization crucial to our approach?

### Setups and Baselines

We conduct experiments on two benchmark datasets, **Meta-World** for multi-task scenarios and **D4RL** for single-task scenarios. Within evaluations on MetaWorld, we consider two distinct datasets: _near-optimal_ dataset, which comprises the entire experiences obtained during the training of a SAC  agent for each task, and _sub-optimal_ dataset, encompassing only the initial \(50\%\) of the replay buffer. More details about datasets and baselines are provided in SSB.

Categorically, our baselines encompass three types of approaches: **offline preference-based methods**, **offline reward-based RL methods**, and **behavior cloning** (BC). Within the realm of offline preference-based methods, our selections include: 1) **PT**, using a transformer network to model reward functions, integrated with RL algorithms; 2) **OPRL**, which employs ensemble-based reward functions; 3) **OPPO**, adopting a one-step process to model offline trajectories and preferences, avoiding reward learning. For methods enjoying access to true reward functions, our selection includes: 1) **IQL**, which performs in-distribution Q-learning and achieves significant performance; 2) **MTDiff**, a method leveraging diffusion models in multi-task scenarios. Since many baselines do not consider multiple tasks, we make modifications and mark them with '**MT\(-\)**'.

### Performance Comparison

Multi-task Performance on Meta-WorldWe assess the multi-task performance using MT-10 tasks and present the results in Figure 4. Our observations are as follows: **1)** Given near-optimal datasets, CAMP outperforms MT-BC and MT-IQL trained with ground-truth rewards, with only a small performance gap compared to MTDiff. **2)** Given sub-optimal datasets, CAMP demonstrates comparable performance to MT-IQL and surpasses other baselines, highlighting its robustness to dataset quality. **3)** Two variations of OPPO fail to learn effective policies for multiple tasks, exposing the limitations of existing preference-based RL methods in handling multi-task scenarios. In comparison, CAMP achieves a performance improvement of nearly four times.

Single-task Performance on D4RL.As shown in Table 1, CAMP demonstrates superior performance across all D4RL Mujoco tasks compared to BC. Additionally, it exhibits comparable performance with IQL in medium-expert tasks. When compared to offline preference-based methods such as PT or OPRL, CAMP showcases significant improvements, particularly in hopper tasks. While OPPO is an effective preference-based method, we observe its sensitivity to random seeds. In our re-implementation using default hyperparameters, its performance degrades in walker2d and hopper tasks. In contrast, CAMP provides aligned trajectory generation and favorable performance.

### Visualization

While showing superior performance, does CAMP learn meaningful representations or perform desired conditional generation? In this part, we map trajectory segments to two-dimensional space via T-SNE  visualization and analyze properties of the trajectory encoder and the diffusion model.

Do representations \(w\) discern different trajectories?To assess the capabilities of discerning different trajectories and aligning with the best trajectories, we sample several trajectories from \(\) and project them to the latent space via \(f_{}\), with subsequent T-SNE visualization. The results at different stages during training are illustrated in the left panel of Figure 5. With increasing training steps, we find a gradually clear classification among trajectories with different returns. Meanwhile, the optimal representations \(w_{i}^{*}\), represented as red dots, gradually approach trajectories with higher returns.

 
**Environment** & **BC** & **IQL\({}_{}\)** & **PT** & **OPRL** & **OPPO\({}_{}\)** & **OPPO\({}_{}\)** & **CAMP (Ours)** \\  halfcheeta-medium & \(42.4 0.2\) & \(48.3 0.2\) & \(47.6 0.1\) & \(42.0 2.8\) & \(43.4 0.2\) & \(42.5 0.2\) & \(45.0 0.3\) \\ halfcheeta-medium-replay & \(35.7 2.3\) & \(44.5 0.2\) & \(42.3 2.0\) & \(41.5 2.6\) & \(39.8 0.2\) & \(33.6 2.4\) & \(40.5 2.0\) \\ halfcheeta-medium-expert & \(56.0 7.4\) & \(94.7 0.5\) & \(86.8 6.6\) & \(90.5 4.0\) & \(88.9 2.3\) & \(89.6 0.9\) & \(95.0 2.1\) \\  walker2d-medium & \(63.3 16.2\) & \(80.9 3.2\) & \(76.8 6.5\) & \(60.3 11.1\) & \(85.0 2.9\) & \(71.5 5.8\) & \(73.9 0.8\) \\ walker2d-medium-replay & \(21.8 10.2\) & \(82.2 3.0\) & \(75.7 3.9\) & \(53.3 6.2\) & \(71.7 4.4\) & \(19.7 10.3\) & \(60.5 1.1\) \\ walker2d-medium-expert & \(99.0 16.0\) & \(111.7 0.9\) & \(110.4 0.5\) & \(105.4 5.6\) & \(105.0 2.4\) & \(97.8 19.1\) & \(104.8 3.0\) \\  hopper-medium & \(53.5 1.8\) & \(67.5 3.8\) & \(25.7 1.2\) & \(45.6 3.5\) & \(86.3 3.2\) & \(48.7 4.2\) & \(59.3 0.8\) \\ hopper-medium-replay & \(29.8 2.1\) & \(97.4 6.4\) & \(82.0 7.9\) & \(45.6 10.9\) & \(88.9 2.3\) & \(29.7 17.1\) & \(56.2 0.7\) \\ hopper-medium-expert & \(52.3 4.0\) & \(107.4 7.8\) & \(44.0 8.8\) & \(68.8 18.1\) & \(108.0 5.1\) & \(99.5 23.5\) & \(108.9 1.0\) \\ 
**Average** & 50.4 & 81.6 & 65.7 & 61.4 & 79.7 & 89.2 & 71.6 \\  

Table 1: Performance comparison in D4RL benchmarks with scripted preferences. The subscript \(\) indicates the baseline with access to true reward functions, while \(\) and \(\) indicate the reported scores and our re-implementation with default parameters, respectively.

Figure 4: Average success rates in MT-10 benchmarks trained with different datasets. Orange bars are reward-based methods, while green bars represent preference-based methods. Detailed comparisons for each task can be found in SSD.

**Does the diffusion model generate trajectories aligned with preferences?** While \(w_{i}^{*}\) provides useful guidance, we want to validate the generative ability of the conditional diffusion model. We map the generated trajectories to latent space and compare them with offline trajectories. As demonstrated in Figure 5(c), the green points denote the generated trajectories, while the red dots represent \(w_{i}^{*}\). We observe that these generated trajectories for 6 tasks align closely with \(w_{i}^{*}\), which represent the more favorable trajectories within the dataset. This phenomenon showcases that generated trajectories guided by \(w_{i}^{*}\) align with preferred trajectories, validating our approach's effectiveness.

### Analysis on Generalization Ability

To validate the generalization ability, we evaluate generated trajectories guided by \(_{k}^{*}\{w_{i}^{*}\}_{i[m]}\) that are learned from trajectories of unseen tasks. \(_{k}^{*}\) from those new tasks are learned using the same method as described in Section 3.1. We hold the diffusion model trained on MT-10 tasks fixed and assess its performance when faced with unseen representation conditions. We compare its performance with that of MT-BC, MT-IQL, and MTDiff, all of which are trained using the same settings. As depicted in Table 2, our approach exhibits favorable performance on unseen tasks and outperforms baseline methods by a considerable margin. Further analyses are presented in SSH.

### Ablation Study

This part delves into the impact of multi-dimensional representations and the MI regularization term. Due to space limitations, detailed implementations refer to SSC. Here, we highlight key conclusions: **1)** Learning a multi-dimensional representation and choosing a suitable dimension are crucial. When the dimension is too low, such as \(|w|=1\), the representations are insufficient to capture preferences in trajectories and provide effective guidance. Conversely, when the dimension is too high, as in \(|w|=64\), the representation space becomes challenging to learn, resulting in inferior performance. **2)** The auxiliary loss of mutual information plays a pivotal role in our framework. Without this regularization term, our method's performance on all MetaWorld and D4RL tasks degrades.

  Unseen Tasks & MT-BC & MT-IQL & MTDiff & CAMP \\  button-press-wal-v2 & \(0.0 0.0\) & \(21.0 21.0\) & \(16.0 17.3\) & \(\) \\ button-press-topdown-wall-v2 & \(21.0 19.0\) & \(33.0 19.0\) & \(72.0 8.6\) & \(66.8 5.0\) \\ handle-press-v2 & \(43.0 13.0\) & \(60.0 4.0\) & \(36.7 12.3\) & \(\) \\ handle-press-side-v2 & \(0.0 0.0\) & \(1.0 1.0\) & \(2.0 2.8\) & \(\) \\ pg-unplug-side-v2 & \(0.0 0.0\) & \(0.0 0.0\) & \(2.7 1.9\) & \(1.6 1.5\) \\  Average & 12.4 & 19.9 & 17.8 & \(\) \\  

Table 2: Generalization performance on five unseen tasks. CAMP exhibits superior performance.

Figure 5: **Left:** Brighter dots indicate trajectories with higher returns. Red dots represent each dimension of \(w_{i}^{*}\). Black triangles in (b) mark trajectories with the highest return. \(f_{}\) can separate trajectories from different tasks and with different returns. \(w_{i}^{*}\) aligns with the optimal trajectories for each task. **Right:** Guided by \(w_{i}^{*}\), diffusion models can generate trajectories \(_{0}^{*}\) that mainly lie around \(w_{i}^{*}\) (shown as black circles), which represents better trajectories in offline data \(_{0}\).

Conclusion

This paper introduces a regularized conditional diffusion model for alignment with preferences in multi-task scenarios. Based on the versatile multi-task preferences, our method acquires preference representations that differentiate trajectories across tasks and with different returns, as well as an optimal representation aligning with the best trajectory for each task. By regularizing exiting diffusion models in RL with mutual information maximization between conditions and generated trajectories, our method can generate desired trajectories by conditioning on the optimal representation for each task, ensuring alignment with preferences. Experimental validation demonstrates the favorable performance and generalization ability of our method. Future work may involve using faster sampling methods to enhance algorithm efficiency or extending to fine-tuning foundation models.