# Articulate your NeRF: Unsupervised articulated object modeling via conditional view synthesis

Jianning Deng  Kartic Subr  Hakan Bilen

University of Edinburgh

https://github.com/VICO-UoE/ArticulateYourNerf/

###### Abstract

We propose a novel unsupervised method to learn pose and part-segmentation of articulated objects with rigid parts. Given two observations of an object in different articulation states, our method learns the geometry and appearance of object parts by fitting an implicit model on the first observation and renders the latter observation by distilling the part segmentation and articulation. Additionally, to tackle the challenging joint optimization of part segmentation and articulation, we propose a voxel grid based initialization strategy and a decoupled optimization procedure. Compared to the prior unsupervised work, our model obtains significantly better performance, generalizes to objects with arbitrary number of parts while it can be efficiently learned from few views only for the latter observation.

## 1 Introduction

Articulated objects, composed of multiple rigid parts connected by joints allowing rotational or translational motion, such as doors, cupboards and spectacles are ubiquitous in our daily lives. Automatically understanding the shape, structure and motion of these objects is crucial for numerous applications in robotic manipulation  and character animation . Many works  that focused on this problem use groundtruth 3D shape, articulation information, and/or part segmentation to learn articulated object models but acquiring accurate 3D observations and manual annotations is typically complex and too expensive for building real large-scale datasets.

In this paper, we introduce a novel unsupervised technique that learns part segmentation and articulation (_i.e._, axis of movement, and translation/rotation of each movable part) from two sets of observations _without_ requiring groundtruth shape, part segmentation or articulation. Two sets contain images of the same object from multiple viewpoints in two different articulation states. Our key idea is that articulation changes only the poses of the object parts but not their geometry or texture. Hence, once the geometry and appearance are learned, one can transform to another articulation state given the part locations and target articulation parameters.

Building on this idea, we frame the learning problem as a conditional novel articulation (and view) synthesis task (see Fig. 1(a)). Given a source observation, multiple views of an object in one articulation state, we first learn the object's shape and appearance by using an implicit model  and then freeze its weights. Next we pass the target observation, multi-view images of the same object in a different articulation state, to a tight bottleneck that distills part locations and articulations. We constrain our model to assign each 3D coordinate that is occupied by the object to a part and to apply a valid geometric transformation to the 3D coordinates of each part through ray geometry. The predictions of part segmentation and articulation, along with the target camera viewpoint, are passed to the implicit function and its differential renderer to reproduce the target observations (see Fig. 1(b)). Minimizing the photometric error between the rendered and target view provides supervision for learning part segmentation and articulation. However, joint optimization of these intertwined tasks is challenging and very sensitive to their initialization.

To address the optimization challenge, we propose an initialization strategy using an auxiliary voxel grid, which provides an initial estimate for moving parts by computing the errors in the foreground masks when rendering target views for the source articulation. Additionally, we introduce a decoupled optimization procedure that alternates between optimizing the part segmentation on the photometric error and the articulation parameters on the foreground prediction error.

The key advantage of our method, compared to other unsupervised articulation prediction methods [11; 17], is its stable performance across different object and articulation types, its ability to learn from few target views and to model multiple moving parts. Thanks to the stage-wise training, we achieve high-quality object geometry and appearance by using the well-optimized implicit models . Since the part segmentation and articulation parameters form a small portion of the total weights, along with the initialization and decoupled optimization strategies, our method efficiently learns them from few target views, unlike the most relevant work  that jointly optimizes multiple part-specific implicit functions from scratch.

## 2 Related work

Analysis of articulated objectsThe analysis of articulated objects typically involves segmentation of movable parts, and estimating their attributes such as position and direction of joint axes, rotation angles, and translation distances. Prior works study articulated objects using 3D input such as meshes [31; 22], point clouds [45; 44; 37; 14] or RGBD images [18; 6; 40; 13] that are error-prone and labor-intensive to collect in real-world scenarios. Recent works [10; 30; 34] that use RGB images to segment 3D planar object parts and estimate their articulation parameters simultaneously require ground-truth labels for segmentation and 3D articulation.

Articulated object modeling via novel view synthesisNeural implicit models [20; 26; 21] that are originally designed to model static objects in a 3D consistent way have been extended to articulated objects by multiple recent works [24; 36; 11; 16; 38; 17; 40]. A-SDF  learns a disentangled latent space for shape and articulation to synthesize novel shapes and unseen articulated poses for a given object category. CARTO  extends A-SDF to multiple object categories and uses a stereo image pairs as input. Similarly, CLA-NeRF  learns to perform joint view synthesis, part segmentation, and articulated pose estimation for an object category from multiple views. NARF22  learns a separate neural radiance fields (NeRF) for each part and composes to render the complete object. Unlike our method, both A-SDF and CARTO require 3D shape and articulation pose supervision, CLA-NeRF requires part segmentation labels and is limited to modeling only 1D revolutions for each joint, NARF22 relies on groundtruth part labels and articulation pose.

Most related to our work, DITTO , PARIS  and Weng et al.  aim to estimate part segmentation and articulation pose without labels using from an observation pair of an object in two different articulations. DITTO uses a pair of point cloud as input, can only model shape, whereas both PARIS and our method uses pairs of multi-view images, and to model both geometry and

Figure 1: (a) Our method learns the geometry and appearance of an articulated object by first fitting a NeRF from (source) images of an object in a fixed articulation. Then, from another set of (target) images of the object in another articulation, we distill the relative articulation and part labels. Green lines show the gradient path during this distillation. (b) Using the part geometry and appearance from NeRF, we render the target images by composing the parts after applying the predicted articulations to the segmented parts. The photometric error provides the required supervision for learning the parts and their articulation without groundtruth labels.

appearance. PARIS adopts the dynamic/static modelling of [42; 46], learns a separate NeRF for the dynamic and static parts. They are composited using the estimated relative articulation to render the observations with the different articulation. Similarly,  first reconstructs the object in two articulation states with two sets of RGBD images. Later, the part-level segmentation is performed based on image registration results. While our method is also based on the same analysis-by-synthesis principle to supervise the training, it differs from previous work in several key ways. Compared to DITTO  and Weng et al. , our method relies completely on 2D input data. And compared to PARIS , our method involves only a single NeRF that is learned on multiple views of an object instance in a fixed articulation pose. Once the NeRF is learned, we freeze its parameters, and learn a segmentation head and articulation to selectively deform the rays while rendering views of different articulations. This means our model's size remain nearly constant when the number of parts increases, which, combined with our two step optimization strategy, leads to more stable and data-efficient learning, yielding significantly better performance. Furthermore, we show that our model goes beyond modeling a single moving object part as in PARIS, and successfully learn multiple moving parts.

Deformable NeRFThere exists several techniques [27; 29; 35; 28; 5] that model differences between subsequent video frames of a dynamic scene through a deformation field in 3D. While these techniques are general could be used in modeling articulated objects in principle, the deformation field does not provide explicit predictions for part segmentation and articulation. Prior methods that focus on modeling specific articulated and deformable objects such as human body [39; 33; 8; 9; 25] and four-legged animals  leverages specialized 3D priors  that are not applicable to arbitrary object categories.

## 3 Review: NeRF

Given a set of images \(\) of an object, a NeRF function  maps a tuple \((,)\), where \(^{3}\) is a 3d position and \(^{2}\) is a direction, to an RGB color \(\), and a positive scalar volume density \(\). The model outputs volume density \(\) at the location \(\) along with a latent feature vector \(\) which is then concatenated with the viewing direction \(\) and fed into an MLP to estimate the view-dependent RGB color \(\). With a slight abuse of notation, we also use \(\) and \(z\) as functions \(()\) and \(z()\) respectively and color as a function with \(c(,)\). That is, we use boldface to denote vectors.

NeRFs are trained by expressing the color at each pixel of images in the training subset. The color at a pixel \(C\) is estimated as \(C_{N}\) via the volume rendering equation as a sum of contributions from points along the ray through the pixel, say \((t)=o+t\) where \(\) is the origin of the ray (usually the point of projection of a camera view) and \(t\) is a scalar. Points \(_{i},i=1,,n\) are sampled along the ray as \(_{i}=+t_{i}\). The estimated color is given by

\[C_{N}()=_{i=1}^{n}\ T_{i}^{r}\ (1-(-_{i}^{r}_{i}^{r}) _{i}^{r}) T_{i}^{r}=_{j=1}^{i-1}(-_{ j}^{r}_{j}^{r})\] (1)

where we use the shorthand notation \(_{i}^{r}\) and \(_{i}^{r}\) to denote \(c(_{i},)\) and \((_{i})\) respectively, and \(_{i}=t_{i+1}-t_{i}\) is the distance between samples. And the opacity value is calculated as \(O_{N}(r)=_{i=1}^{n}\ T_{i}^{r}\ (1-(-_{i}^{r}_{i}^{r})\). The parameters of \(c\) and \(\) are obtained by minimizing the photometric loss between predicted color \(C_{N}()\) and the ground truth color \(C(r)\) as

\[_{}=_{r}||C_{N}()-C()||_{ 2}^{2}.\] (2)

## 4 Method

Let \(\) and \(^{}\) be two observations of an object with \(k\) (known a priori) rigid parts in articulation poses \(\) and \(^{}\) respectively. Each observation contains multiple views of the object along with the foreground masks. We call \(\) the source observation and \(^{}\) the target observation. \(\) (resp. \(^{}\)) is a pose tensor composed of \(P_{} SE(3),\ =1,,k\) (resp. \(P_{}^{}\)) transformations as \(4 4\) matrices corresponding to the local pose of each of the \(k\) parts. Our primary goal is to estimate a pose-change tensor \(\) composed of \(M_{} SE(3),\ =1,,k\) so that \(P_{}^{}=M_{}P_{}\). We solve this by starting with the construction of a NeRF from \(\), as described in Sec. 3 along with the efficient coarse-to-finevolume sampling strategy in , followed by a novel modification to build awareness at the part-level. We use this modified parameterization to optimize an auxiliary voxel grid corresponding to the parts via pose estimation and part segmentation pipelines. Once we have identified the parts and their relative transformations, we are able to render novel views corresponding to articulation pose \(^{}\) by transforming view rays suitably by \(M_{}^{-1}\).

### Part-aware rendering

Once a NeRF function, which we call'static NeRF', is learned over \(\), we freeze its parameters and append a segmentation head \(s\) to it towards obtaining part-level information. \(s\) is instantiated as 2-layer MLP, and maps the latent feature vector \(z()\) to a probability distribution over the \(k\) object parts. We denote the probability of a 3D point \(\) to belong object part \(=1,,k\) as \(s_{}(z())\).

If the segmentation class-probabilities \(s_{}\) and pose-change transformations \(M_{}\) are known, then the object in an unseen articulation pose can be synthesised by suitably transforming the statically trained NeRF without modifying its parameters and then compositing the individual part contributions. To model pose change in each part, we use a different ray, _virtual ray_ associated with each part

\[_{}=M_{}^{-1}\] (3)

and the final rendered color is:

\[C_{P}()=_{i=1}^{n}\ _{i}^{r}\ \ _{=1}^{k}(1- (-(s_{}^{r_{}}(_{i})\ ^{r_{}}(_{i})\ _{j}^{r_{}})))_{i}^{r_{}}\] (4)

where \(_{i}^{r}=_{j=1}^{i-1}(-_{=1}^{k}(s_{}^{r_ {}}(_{j})\ ^{r_{}}(_{j})\ _{j}^{r_{}})))\) is the transmittance sum. Compared to the original formulation in Eq. (1), it can seen as the density replaced by a part-aware density which is the product of \(s\) and \(\). Note that we made a similar modification to the efficient volume sampling strategy of  and refer to the Appendix A.1 for more details.

Since the groundtruth part segmentation and articulation are unknown, one could optimize the parameters of \(s\) and \(\) on the target observation such that the photometric error in between the part-aware rendered views and \(^{}\). Note that \(C_{P}()\) is function of \(\) (through \(r_{}\) in Eq. (3)) and \(s\), they can be jointly optimized through backpropagation. However, jointly solving these two intertwined problems through rendering is challenging and highly sensitive to their initial values, as their solutions depend on each other. Hence, we introduce an alternating optimization strategy next.

### Decoupled optimization of \(s\) and \(\)

To learn the part segmentation and pose-change, we first introduce an auxiliary voxel grid that assigns each 3D coordinate either to a background or a part. Once the voxel is initialized, our training iterates multiple times over a three-step procedure that includes optimization of \(\), \(s\), and refinement of the voxel grid entries.

Initialization of voxel gridHere our key idea is to find the pixel differences between the source and target observations by rendering a target view \(v^{}\) by using the static NeRF, and label the 3D locations of the different pixels as the movable parts to provide a good initialization for estimating the articulation (see Fig. 2). To this end, we first build a 3D voxel of the static NeRF by discretizing its 3D space into 128 bins along each dimension. To compute the voxel entries, we query the density value from the static NeRF at each voxel coordinate and binarize it into occupancy values with criteria \((-())>0.1\). Next, we use the static NeRF to render opacity and depth images for the viewpoints \(v^{}\). The pixels rendered as foreground in rendered opacity but not in the corresponding foreground mask of \(^{}\) will be collected. For those pixels, we further compute their depth by accumulating their density values along the corresponding ray, and use their estimated depth values to tag the occupied voxel entries either with static or dynamic parts. In the case of multiple moving parts, which is assumed to be known, we perform a clustering step  to identify \(k-1\) clusters corresponding to moving parts, as we assume that 1 part of the object is static without loss of generality. Finally, we gather the voxel coordinates that is assigned to part \(\) to form a matrix of 3D coordinates \(X_{}\).

Step 1: Optimization of \(M_{}\)As depicted in the right side of Fig. 3, we project the 3D coordinates \(X_{}\) for part \(\) onto the image plane for each camera viewpoint \(v^{}\) included in \(^{}\) by using \(3 4\) dimensional intrinsic camera matrix \(K\) as \(U_{}=KM_{}^{-1}}X_{}\), and denote the projected 2D point matrix on the image plane as \(U_{}\). On the left side of Fig. 3, we collect 2D coordinates from the overlap region between rendered opacity and target image \(^{}\) to obtain \(U^{}\). Then we concatenate \(U^{}\) and part-specific matrices \(U_{}\) to obtain \(U\). We want to optimize \(U\) to be the same as the target coordinates \(F\), which is obtained by gathering the 2D coordinates from the foreground mask of \(^{}\). Note that both \(U\) and \(F\) are matrices where each row corresponds to a 2D image coordinate where \(F\) has significantly more rows, as \(U\) is obtained from a coarse voxel grid. Then we minimize the 2D Chamfer distance between \(U\) and \(F\):

\[^{*}=_{}\ d_{}(U,F).\] (5)

In practice, we only project \(X_{}\) for the moved parts, and stack them with 2D image coordinates of the foreground pixels in both the rendered image and the groundtruth image, as illustrated in Fig. 3.

Step 2: Optimization of \(s_{}\)Once we obtain the solution of the pose-change \(^{*}\) from Step 1, we plug in it to the ray deformation in Eq. (3) and render each view in \(^{}\) in Eq. (4). Then we compute photometric loss between the rendered views and \(^{}\), and minimize it with respect to the parameters of \(s\) only. In the case of multiple moving parts, we initialize the parameters in \(s\) using \(X_{}\) for supervision. Please refer to Appendix A.4 for more details.

Step 3: Voxel grid refinementThe initial part segmentation estimates in the voxel grid are often sparse and noisy due to inaccurate density estimation in NeRF and misassignment of pixels around the foreground and part boundaries. To obtain a more accurate voxel grid, we follow the same steps in the initialization stage except that the 3D coordinates are assigned to the parts based on the predicted label from segmentation head \(s\). We denote the new 3D coordinates as \(X_{}^{*}\). For the consistency, we accept the voxels to \(X_{}^{*}\) only when the predicted part labels agree within a 3D local neighborhood of \(X_{}\). The neighborhood is defined as being within the distance of one grid cell at a resolution of 128. Note that we only perform the refinement step, after approximately 2k iterations to ensure that the segmentation head produces confident results. Then we also increase the voxel resolution to 256 for each dimension.

## 5 Experiment

### Dataset

We evaluate our method on the synthetic 3D PartNet-Mobility dataset [43; 23; 3]. While the dataset contains more than 2000 articulated objects from 46 different categories, we use a subset of the dataset with 6 shapes that was used in . For a fair comparison, we downloaded the processed dataset from  which contains 2 sets of 100 views along with their foreground masks, each with a

Figure 3: Illustration for optimization of \(M_{}\). The green dotted line shows the gradient flow.

Figure 2: Voxel initialization: identify the voxels belonging to moved parts based on pixel opacity difference.

different articulation, and also groundtruth part segmentation labels, for each shape. In addition, we select 4 additional shapes, each with two moving parts, and apply the same data generation process to them. Additionally we collected images of a toy car with a handheld device, with camera viewpoint estimated from kiri engine application.

We train the static NeRF on 100 views from the first observation, train the part segmentation and articulation on 100 views from the second observation. We report the performance of our method for varying number of views from the second observation in Tab. 4.

Following , we report performance in different metrics for pose estimation, novel-view/articulation synthesis, and part segmentation. **Pose estimation:** To report articulation pose performance, we report results in i) direction error \(e_{d}\) that measures the angular discrepancy in degrees between the predicted and actual axis directions across all object categories, ii) position error \(e_{p}\) and geodesic distance \(e_{d}\) for only revolute objects to evaluate the error between the predicted and true axis positions and the error in the predicted rotation angles of parts in degrees, respectively, iii) translation error \(e_{t}\) to measure the disparity between the predicted and actual translation movements for prismatic objects. **Novel-view and -articulation synthesis:** We evaluate the quality of novel view synthesis generated by the models using the Peak Signal-to-Noise Ratio (PSNR) where higher values indicate better reconstruction fidelity. **Part segmentation:** We use mean Intersection over Union (mIoU) on the rendered semantic images to evaluate the accuracy of part segmentation, which is tightly related to the rendered image quality of objects in different articulation states. The ground truth segmentation is generated within the Sapien framework in . Finally, due to the lack of groundtruth segmentation label and articulation in the real data, we only report PSNR and provide qualitative evaluation.

### Results

BaselineWe compare our approach with the state-of-the-art unsupervised technique, PARIS  which constructs separate NeRF models for each part of an articulated object and optimizes motion parameters in an end-to-end manner. However, it is limited to two-part objects with only one movable part. As the authors of PARIS do not report their results over multiple runs in their paper, we use their official public code with the default hyperparameters, train both their and our model over 5 random initializations and report the average performance and the standard deviation. We use 2 sets of 100 views for each object to train the models. More implementation details can be found in the supplementary material. We would like to note that the performance of PARIS in our experiments significantly differ from the original results despite all the care taken to reproduce them in the original way1.

Part-level pose estimationFor part-level pose estimation, we provide quantitative results in average performance over 5 runs and their standard deviations in Tab. 1, and a qualitative analysis in Fig. 6. The results indicate that our method consistently achieves lower errors across most evaluation metrics

   &  &  &  \\   & & laptop & oven & stapler & fridge & blade & storage \\  \)} & PARIS & 0.68 \(\) 0.40 & 1.04 \(\) 0.68 & 2.42 \(\) 0.91 & 0.81 \(\) 0.60 & 48.58 \(\) 25.43 & **0.34 \(\) 0.09** \\  & Ours & **0.33 \(\) 0.04** & **0.34 \(\) 0.03** & **0.33 \(\) 0.04** & **0.54 \(\) 0.08** & **1.54 \(\) 0.07** & 1.11 \(\) 0.14 \\  \)(\(10^{-2}\))} & PARIS & **0.18 \(\) 0.15** & **0.49 \(\) 0.53** & 55.54 \(\) 39.88 & **0.33 \(\) 0.14** & - & - \\  & Ours & 0.48 \(\) 0.06 & 1.29 \(\) 0.04 & **0.17 \(\) 0.05** & 0.44 \(\) 0.03 & - & - \\  \)} & PARIS & 0.60 \(\) 0.32 & 0.68 \(\) 0.39 & 44.62 \(\) 6.17 & 0.87 \(\) 0.55 & - & - \\  & Ours & **0.25 \(\) 0.03** & **0.35\(\) 0.06** & **0.290\(\)0.03** & **0.60 \(\) 0.05** & - & - \\  \)} & PARIS & - & - & - & - & 1.13 \(\) 0.52 & 0.30\(\) 0.01 \\  & Ours & - & - & - & - & **0.01 \(\) 0.01** & **0.02\(\) 0.03** \\  

Table 1: **Part-level pose estimation results. Our method outperforms PARIS in majority of object categories while having lower variation over multiple runs in the performance.**compared to PARIS, except the joint position error \(e_{p}\) where the differences are negligible, within the \(10^{-3}\) range. Notably, the performance of PARIS on the stapler and blade exhibits significantly higher errors. We observe that PARIS fails to converge to a good solution in all 5 runs for these objects. As shown in Fig. 6, PARIS fails to accurately segment the parts in the stapler and blade, which can also be easily identified in the novel articulation synthesis experiments in Fig. 6. Poor part reconstruction in PARIS results in inaccurate part-level pose estimations. Additionally, the lower standard deviation across all reconstructed objects for our method indicates its stable learning and ability to work for different object instances. We attribute the stability of our method to the decoupled optimization along with the stage-wise training, which contrasts with the joint optimization approach used by PARIS.

Segmentation and composite renderingPart segmentation and novel view synthesis in pose \(^{}\) are reported in Tab. 2, complemented by a qualitative analysis of part segmentation in Fig. 4. Our method outperforms PARIS across most evaluated objects in part segmentation and image synthesis quality, with the only exception being a minor difference in the PSNR for the laptop.

As our model builds on a static NeRF that achieves high quality novel view synthesis for one observation, the rendering quality is largely preserved after the second stage. We provide more detailed analysis in Appendix A.3. Additionally, benefiting from accurate pose estimation via a decoupled approach, our method achieves more robust and precise part segmentation, as depicted in Fig. 4. Here, our method consistently delivers accurate segmentation results for challenging objects such as the blade, stapler, and scissors, where PARIS struggles with accurate part reconstruction. In the other instances including the laptop, storage, and oven, our method achieves visibly better results.

Evaluation on objects with multiple movable partsA key advantage of our model is its ability to model objects with multiple moving parts. For such objects, we report pose estimation results in Tab. 3, qualitative for part segmentation in Fig. 5 and novel articulation synthesis in Fig. 6. As in the single part moving object experiments, our method delivers similar performance for the multi-part objects. Notably, we observed a marginally higher joint direction error for glasses, which we attribute to the thin structures such as temples and failure to segment them accurately which can be possibly improved by using higher resolution images.

Figure 4: **Qualitative 2D part segmentation results.** Pixels in green denotes the movable parts. Our method demonstrates consistent performance across all tested objects while PARIS failed for Blade, Laptop and Scissor.

Figure 5: **Qualitative results for 2D multi-part segmentation.** The pink color denotes the static part, while other colors denote the moving parts.

[MISSING_PAGE_FAIL:8]

### Real-world example

The qualitative evaluation can be found in Fig. 7. Novel articulation synthesis in different views are showed in two sides of Fig. 7. The GT is shown in pose \(^{}\) while the segmentation image is the toy car in pose \(\). The average PSNR we obtain from the static NeRF is, and the PSNR for 28.19, while the PSNR for pose \(^{}\) is 24.32. More detail results for segmentaion and articulation interpolation are also presented in Fig. 7.

### Limitations

Our method has few limitations too. As we use rendering to supervise our segmentation and pose, our method may fail to segment very thin parts or small movable parts when the rendering error is small. Our method is less stable for multipart objects compared to two-part objects. When the object parts are nearly symmetry, our method may fail to find the correct articulation and choose another articulation that produces similar rendering. We provide examples of such failure cases in Appendix A.3. Additionally, it also inherits the limitations of the implicit models that are built on such as failure to model transparent objects, and inaccurate 3D modeling when the viewpoint information is noisy. Finally, our method is limited to articulated objects with rigid parts and cannot be used to model soft tissue deformations.

## 6 Conclusion

In this study, we tackled the challenges of self-supervised part segmentation, articulated pose estimation, and novel articulation rendering for objects with multiple movable parts. Our approach is the first known attempt to model multipart articulated objects using only multi-view images acquired from two arbitrary articulation states of the same object. We evaluated our method with both synthetic and real-world datasets. The results suggest that our method competently estimates part segmentation and articulation pose parameters, and effectively renders images of unseen articulations, showing promising improvements over existing state-of-the-art techniques. Further, our real-world data experiments underscore the method's robust generalization capabilities. At last, the code and the data used in this project will be released upon acceptance. However, the reliance on geometric information from moving parts for articulation pose estimation poses challenges in modeling highly symmetrical objects. Future work could improve our method by incorporating both appearance and geometric data into the pose estimation process, potentially enhancing accuracy and applicability.

Broader ImpactsThe proposed technique could be potentially used to understand articulated objects for their robotic manipulation in future. The authors are not aware of any potential harm that may arise when the technology is used

   &  \\   & 2 & 4 & 8 & 16 & 32 & 100 & 100\({}_{P}\) \\  \(e_{d}\) & 46.05 & 8.89 & 0.59 & 0.58 & 0.50 & 0.54 & 0.81 \\  \(e_{g}\) & 44.74 & 20.79 & 0.70 & 0.60 & 0.56 & 0.49 & 0.87 \\  PSNR \(\) & 22.65 & 29.95 & 34.00 & 34.28 & 34.88 & 35.10 & 32.74 \\  

Table 4: **Ablation studies with different number of target images.**

   &  \\  DP & IR & \(e_{d}\) & \(e_{g}\) & PSNR \\  - & - & 2.57 & 26.95 & 18.09 \\  ✓ & - & 2.38 & 14.96 & 25.16 \\  ✓ & \(\) & 0.54 & 0.49 & 35.10 \\  

Table 5: **Ablation study over different initialization strategies.**

Figure 7: Results on real world examples, the red line indicates the estimated joint axis direction. Green and purple color denotes the moving car door, while pink denotes the body of the toy car. Please refer to Fig. 12 for more qualitative evaluation.

AcknowledgementsKartic Subr was supported by a Royal Society University Research Fellowship. Hakan Bilen was supported by the EPSRC Visual AI grant EP/T028572/1.