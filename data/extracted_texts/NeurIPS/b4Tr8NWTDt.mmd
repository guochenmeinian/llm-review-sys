# Co-Learning Empirical Games and World Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Game-based decision-making involves reasoning over both world dynamics and strategic interactions among the agents. Typically, empirical models capturing these respective aspects are learned and used separately. We investigate the potential gain from co-learning these elements: a world model for dynamics and an empirical game for strategic interactions. Empirical games drive world models toward a broader consideration of possible game dynamics induced by a diversity of strategy profiles. Conversely, world models guide empirical games to efficiently discover new strategies through planning. We demonstrate these benefits first independently, then in combination as realized by a new algorithm, Dyna-PSRO, that co-learns an empirical game and a world model. When compared to PSRO--a baseline empirical-game building algorithm, Dyna-PSRO is found to compute lower regret solutions on partially observable general-sum games. In our experiments, Dyna-PSRO also requires substantially fewer experiences than PSRO, a key algorithmic advantage for settings where collecting player-game interaction data is a cost-limiting factor.

## 1 Introduction

Even seemingly simple games can actually embody a level of complexity rendering them intractable to direct reasoning. This complexity stems from the interplay of two sources: dynamics of the game environment, and strategic interactions among the game's players. As an alternative to direct reasoning, models have been developed to facilitate reasoning over these distinct aspects of the game. _Empirical games_ capture strategic interactions in the form of payoff estimates for joint policies . _World models_ represent a game's transition dynamics and reward signal directly [69; 19]. Whereas each of these forms of model have been found useful for game reasoning, typical use in prior work has focused on one or the other, learned and employed in isolation from its natural counterpart.

Co-learning both models presents an opportunity to leverage their complementary strengths as a means to improve each other. World models predict successor states and rewards given a game's current state and action(s). However, their performance depends on coverage of their training data, which is limited by the range of strategies considered during learning. Empirical games can inform training of world models by suggesting a diverse set of salient strategies, based on game-theoretic reasoning . These strategies can expose the world model to a broader range of relevant dynamics. Moreover, as empirical games are estimated through simulation of strategy profiles, this same simulation data can be reused as training data for the world model.

Strategic diversity through empirical games, however, comes at a cost. In the popular framework of Policy-Space Response Oracles (PSRO) , empirical normal-form game models are built iteratively, at each step expanding a restricted strategy set by computing best-response policies to the current game's solution. As computing an exact best-response is generally intractable, PSRO uses Deep Reinforcement Learning (DRL) to compute approximate response policies. However, each application of DRL can be considerably resource-intensive, necessitating the generation ofa vast amount of gameplays for learning. Whether gameplays, or experiences, are generated via simulation  or from real-world interactions , their collection poses a major limiting factor in DRL and by extension PSRO. World models present one avenue to reduce this cost by transferring previously learned game dynamics across response computations.

We investigate the mutual benefits of co-learning a world model and an empirical game by first verifying the potential contributions of each component independently. We then show how to realize the combined effects in a new algorithm, _Dyna-PSRO_, that co-learns a world model and an empirical game (illustrated in Figure 1). Dyna-PSRO extends PSRO to learn a world model concurrently with empirical game expansion, and applies this world model to reduce the computational cost of computing new policies. This is implemented by a Dyna-based reinforcement learner [67; 68] that integrates planning, acting, and learning in parallel. Dyna-PSRO is evaluated against PSRO on a collection of partially observable general-sum games. In our experiments, Dyna-PSRO found lower-regret solutions while requiring substantially fewer cumulative experiences.

The main points of novelty of this paper are as follows: (1) empirically demonstrate that world models benefit from the strategic diversity induced by an empirical game; (2) empirically demonstrate that a world model can be effectively transferred and used in planning with new other-players. The major contribution of this work is a new algorithm, Dyna-PSRO, that co-learns an empirical game and world model finding a stronger solution at less cost than the baseline, PSRO.

## 2 Related Work

Empirical Game Theoretic Analysis (EGTA).The core idea of EGTA  is to reason over approximate game models (_empirical games_) estimated by simulation over a restricted strategy set. This basic approach was first demonstrated by Walsh et al. , in a study of pricing and bidding games. Phelps et al.  introduced the idea of extending a strategy set automatically through optimization, employing genetic search over a policy space. Schwartzman & Wellman  proposed using RL to derive new strategies that are approximate best responses (BRs) to the current empirical game's Nash equilibrium. The general question of which strategies to add to an empirical game has been termed the _strategy exploration problem_. PSRO  generalized the target for BR beyond NE, and introduced DRL for BR computation in empirical games. Many further variants and extensions of EGTA have been proposed, for example those using structured game representations such as extensive-form [43; 34]. Some prior work has considered transfer learning across BR computations in EGTA, specifically by reusing elements of policies and value functions [64; 65].

Model-Based Reinforcement Learning (MBRL)._Model-Based_ RL algorithms construct or use a model of the environment (henceforth, _world model_) in the process of learning a policy or value function . World models may either predict successor observations directly (e.g., at pixel level [76; 79]), or in a learned latent space [18; 17]. The world models can be either used for _background planning_ by rolling out model-predicted trajectories to train a policy, or by _decision-time planning_ where the world model is used to evaluate the current state by planning into the future. Talvitie  demonstrated that even in small Markov decision processes (MDP) , model-prediction errors tend to compound--rendering long-term planning at the abstraction of observations ineffective. A follow-up study demonstrated that for imperfect models, short-term planning was no better than repeatedly training on previously collected real experiences; however, medium-term planning offered advantages even with an imperfect model . Parallel studies hypothesized that these errors are a result of insufficient data for that transition to be learned [36; 8]. To remedy the data insufficiency, ensembles of world models were proposed to account for world model

Figure 1: Dyna-PSRO co-learns a world model and empirical game. Empirical games offer world models strategically diverse game dynamics. World models offer empirical games more efficient strategy discovery through planning.

uncertainty [8; 36; 84], and another line of inquiry used world model uncertainty to guide exploration in state-action space [3; 59]. This study extends this problem into the multiagent setting, where now other-agents may preclude transitions from occurring. The proposed remedy is to leverage the strategy exploration process of building an empirical game to guide data generation.

Multiagent Reinforcement Learning (MARL).Previous research intersecting MARL and MBRL has primarily focused on modeling the opponent, particularly in scenarios where the opponent is fixed and well-defined. Within specific game sub-classes, like cooperative games and two-player zero-sum games, it has been theoretically shown that opponent modeling reduces the sample complexity of RL [73; 85]. Opponent models can either explicitly [46; 15] or implicitly [4; 29] model the behavior of the opponent. Additionally, these models can either construct a single model of opponent behavior, or learn a set of models [12; 21]. While opponent modeling details are beyond the scope of this study, readers can refer to Albrecht & Stone's survey  for a comprehensive review on this subject. Instead, we consider the case where the learner has explicit access to the opponent's policy during training, as is the case in empirical-game building. A natural example is that of Self-Play, where all agents play the same policy; therefore, a world model can be learned used to evaluate the quality of actions with Monte-Carlo Tree Search [60; 62; 72; 56]. Li et al.  expands on this by building a population of candidate opponent policies through PSRO to augment the search procedure. Krupnik et al.  demonstrated that a generative world model could be useful in multi-step opponent-action prediction. Sun et al.  examined modeling stateful game dynamics from observations when the agents' policies are stationary. Chockalingam et al.  explored learning world models for homogeneous agents with a centralized controller in a cooperative game. World models may also be shared by independent reinforcement learners in cooperative games [81; 86].

## 3 Co-Learning Benefits

We begin by specifying exactly what we mean by world model and empirical game. This requires defining some primitive elements. Let \(t\) denote time in the real game, with \(s^{t}\) the _information state_ and \(h^{t}\) the _game state_ at time \(t\). The information state \(s^{t}(m^{,t},o^{t})\) is composed of the _agent's memory_\(m^{}^{}\), or recurrent state, and the current _observation_\(o\). Subscripts denote a player-specific component \(s_{i}\), negative subscripts denote all but the player \(s_{-i}\), and boldface denote the joint of all players \(s\). The _transition dynamics_\(p:() ()\) define the game state update and reward signal. The agent experiences _transitions_, or _experiences_, \((s^{t},\,a^{t},r^{t+1},s^{t+1})\) of the game; where, sequences of transitions are called _trajectories_\(\) and trajectories ending in a terminal game state are _episodes_.

At the start of an episode, all players sample their current _policy_\(\) from their _strategy_\(:\), where \(\) is the _policy space_ and \(\) is the corresponding _strategy space_. A _utility function_\(U:^{n}\) defines the payoffs/returns (i.e., cumulative reward) for each of \(n\) players. The tuple \((,U,n)\) defines a _normal-form game_ (NFG) based on these elements. We represent empirical games in normal form. An _empirical normal-form game_ (ENFG) \((},,n)\) models a game with a _restricted strategy set_\(}\) and an estimated payoff function \(\). An empirical game is typically built by alternating between game reasoning and strategy exploration. During the game reasoning phase, the empirical game is solved based on a solution concept predefined by the modeler. The strategy exploration step uses this solution to generate new policies to add to the empirical game. One common heuristic is to generate new policies that best-respond to the current solution [45; 57]. As exact best-responses typically cannot be computed, RL or DRL are employed to derive approximate best-responses .

An _agent world model_\(w\) represents dynamics in terms of information available to the agent. Specifically, \(w\) maps information states and actions to observations and rewards, \(w:}^{w} }}\), where \(m^{w}^{w}\) is the _world model's memory_, or recurrent state. For simplicity, in this work, we assume the agent learns and uses a deterministic world model, irrespective of stochasticity that may be present in the true game. Specific implementation details for this work are provided in Appendix C.2.

Until now, we have implicitly assumed the need for distinct models. However, if a single model could serve both functions, co-learning two separate models would not be needed. Empirical games, in general, cannot replace a world model as they entirely abstract away any concept of game dynamics. Conversely, world models have the potential to substitute for the payoff estimations in empirical games by estimating payoffs as rollouts with the world model. We explore this possibility in an 

[MISSING_PAGE_FAIL:4]

observation and reward for both players. The world models are optimized with a weighted-average cross-entropy objective. Additional details are in Appendix C.2.

**Results.** Figure 2 presents each world model's per-profile accuracy, as well as its average over all profiles. Inclusion of the random policy corresponds to decreases in observation prediction accuracy: \(}}}\) and \(0.83 0.02\). Figure 13 (Appendix E.1) contains the world model's per-profile recall. Inclusion of the random policy corresponds to increases in reward \(1\) recall: \(0.25 0.07}}}}\) and \(0.26 0.07\)

**Discussion.** The PSRO policies offer the most strategically salient view of the game's dynamics. Consequently, the world model \(\) trained with these policies yields the highest observation accuracy. However, this world model performs poorly on reward accuracy, scoring only \(0.50 0.10\). In comparison, the model trained on the random policy \(\) scores \(0.73 0.08\). This seemingly counterintuitive result can be attributed to a significant class imbalance in rewards. \(\) predicts only the most common class, no reward, which gives the illusion of higher performance. In contrast, the remaining world models attempt to predict rewarding states, which reduces their overall accuracy. Therefore, we should compare the world models based on their ability to recall rewards. When we examine \(\) again, we find that it also struggles to recall rewards, scoring only \(0.26 0.07\). However, when the random policy is included in the training data (\(\)), the recall improves to \(0.37 0.11\). This improvement is also due to the same class imbalance. The PSRO policies are highly competitive, tending to over-harvest. This limits the proportion of rewarding experiences. Including the random policy enhances the diversity of rewards in this instance, as its coplayer can demonstrate successful harvesting. Given the importance of accurately predicting both observations and rewards for effective planning, \(\) appears to be the most promising option. However, the strong performance of \(\) suggests future work on algorithms that can benefit solely from observation predictions. Overall, these results support the claim that strategic diversity enhances the training of world models.

### Response Calculations

Empirical games are built by iteratively calculating and incorporating responses to the current solution. However, direct computation of these responses is often infeasible, so RL or DRL is used to approximate the response. This process of approximating a single response policy using RL is computationally intensive, posing a significant constraint in empirical game modeling when executed repeatedly. World models present an opportunity to address this issue. A world model can serve as a medium for transferring previously learned knowledge about the game's dynamics. Therefore, the dynamics need not be relearned, reducing the computational cost associated with response calculation.

Figure 2: World model accuracy across strategy profiles. Each heatmap portrays a world model’s accuracy over 16 strategy profiles. The meta x-axis corresponds to the profiles used to train the world model (as black cells). Above each heatmap is the model’s average accuracy.

Exercising a world model for transfer is achieved through a process called _planning_. Planning is any procedure that takes a world model and produces or improves a policy. In the context of games, planning can optionally take into account the existence of coplayers. This consideration can reduce experiential variance caused by unobserved confounders (i.e., the coplayers). However, coplayer modeling errors may introduce further errors in the planning procedure .

Planning alongside empirical-game construction allows us to side-step this issue as we have direct access to the policies of all players during training. This allows us to circumvent the challenge of building accurate agent models. Instead, the policies of coplayers can be directly queried and used alongside a world model, leading to more accurate planning. In this section, we empirically demonstrate the effectiveness of two methods that decrease the cost of response calculation by integrating planning with a world model and other agent policies.

#### 3.2.1 Background Planning

The first type of planning that is investigated is _background planning_, popularized by the Dyna architecture . In background planning, agents interact with the world model to produce _planned experiences1_. The planned experiences are then used by a model-free reinforcement learning algorithm as if they were _real experiences_ (experiences generated from the real game). Background planning enables learners to generate experiences of states they are not currently in.

Experiment.To assess whether planned experiences are effective for training a policy in the actual game, we compute two response policies. The first response policy, serving as our baseline, learns exclusively from real experiences. The second response policy, referred to as the planner, is trained using a two-step procedure. Initially, the planner is exclusively trained on planned experiences. After \(10\,000\) updates, it then transitions to learning solely from real experiences. Policies are trained using IMPALA , with further details available in Appendix C.1. The planner employs the \(}\) model from Section 3.1, and the opponent plays the previously held-out policy. In this and subsequent experiments, the cost of methods is measured by the number of experiences they require with the actual game. This is because, experience collection is often the bottleneck when applying RL-based methods . Throughout the remainder of this work, each experience represents a trajectory of \(20\) transitions, facilitating the training of recurrent policies.

Results.Figure 3 presents the results of the background planning experiment. The methods are compared based on their final return, utilizing an equivalent amount of real experiences. The baseline yields a return of \(23.00 4.01\), whereas the planner yields a return of \(31.17 0.25\).

Discussion.In this experiment, the planner converges to a stronger policy, and makes earlier gains in performance than the baseline. Despite this, there is a significant gap in the planner's learning

Figure 3: Effects of background planning on response learning. Left: Return curves measured by the number of real experiences used. Right: Return curves measured by usage of both real and planned experiences. The planner’s return is measured against the real game and the world model. (\(5\) seeds, with \(95\,\%\) bootstrapped CI).

curves, which are reported with respect to both the world model and real game. This gap arises due to accumulated model-prediction errors, causing the trajectories to deviate from the true state space. Nevertheless, the planner effectively learns to interact with the world model during planning, and this behavior shows positive transfer into the real game, as evidenced by the planner's rapid learning. The exact magnitude of benefit will vary across replayers' policies, games, and world models. In Figure 14 (Appendix E.2), we repeat the same experiment with the poorly performing \(\) world model, and observe a marginal benefit (\(26.05 1.32\)). The key take-away is that background planning tends to lead towards learning benefits, and not generally hamper learning.

#### 3.2.2 Decision-Time Planning

The second main way that a world model is used is to inform action selection at _decision time [planning] (DT)_. In this case, the agent evaluates the quality of actions by comparing the value of the model's predicted successor state for all candidate actions. Action evaluation can also occur recursively, allowing the agent to consider successor states further into the future. Overall, this process should enable the learner to select better actions earlier in training, thereby reducing the amount of experiences needed to compute a response. A potential flaw with decision-time planning is that the agent's learned value function may not be well-defined on model-predicted successor states . To remedy this issue, the value function should also be trained on model-predicted states.

Experiment.To evaluate the impact the decision-time planning, we perform an experiment similar to the background planning experiment (Section 3.2.1). However, in this experiment, we evaluate the quality of four types of decision-time planners that perform one-step three-action search. The planners differ in the their ablations of background planning types: (1) _warm-start background planning (BG: W)_ learning from planned experiences before any real experiences, and (2) _concurrent background planning (BG: C)_ where after BG: W, learning proceeds simultaneously on both planned and real experiences. The intuition behind BG: C is that the agent can complement its learning process by incorporating planned experiences that align with its current behavior, offsetting the reliance on costly real experiences. Extended experimental details are provided in Appendix C.

Results.The results for this experiment are shown in Figure 4. The baseline policy receives a final return of \(23.00 4.01\). The planners that do not include BG: W, perform worse, with final returns of \(9.98 7.60\) (DT) and \(12.42 3.97\) (DT & BG: C). The planners that perform BG: W outperform the baseline, with final returns of \(44.11 2.81\) (DT & BG: W) and \(44.31 2.56\) (DT, BG: W, & BG: C).

Discussion.Our results suggest that the addition of BG: W provides sizable benefits: \(9.98 7.60\) (DT) \( 44.11 2.81\) (DT & BG:W) and \(12.42 3.97\) (DT & BG: C) \( 44.31 2.56\) (DT, BG: W, & BG: C). We postulate that this is because it informs the policy's value function on model-predictive states early into training. This allows that the learner is able to more effectively search earlier into training. BG: C appears to offer minor stability and variance improvements throughout the training

Figure 4: Effects of decision-time planning on response learning. Four planners using decision-time planning (DT) are shown in combinations with warm-start background planning (BG: W) and concurrent background planning (BG: C). (5 seeds, with \(95\,\%\) bootstrapped CI).

[MISSING_PAGE_FAIL:8]

solution with \(0.89 0.74\) regret at \(5.126\) experiences. At the same time, PSRO had found a solution with \(6.42 4.73\) regret, and at the end of its run had \(2.50 2.24\) regret. In the final game, RWS, Dyna-PSRO has \(2-3 5-4\) regret at \(1.067\) experiences, and at a similar point (\(9.66\) experiences), PSRO has \(6.68-3 2.51-3\). At the end of the run, PSRO achieves a regret \(3.50-3 7.36-4\).

Discussion.The results indicate that across all games, Dyna-PSRO consistently outperforms PSRO by achieving a superior solution. Furthermore, this improved performance is realized while consuming fewer real-game experiences. For instance, in the case of Harvest: Categorical, the application of the world model for decision-time planning enables the computation of an effective policy after only a few iterations. On the other hand, we observe a trend of accruing marginal gains in other games, suggesting that the benefits are likely attributed to the transfer of knowledge about the game dynamics. In Harvest: Categorical and Running With Scissors, Dyna-PSRO also had lower variance than PSRO.

## 5 Limitations

Although our experiments demonstrate benefits for co-learning world models and empirical games, there are several areas for potential improvement. The world models used in this study necessitated observational data from all players for training, and assumed a simultaneous-action game. Future research could consider relaxing these assumptions to accommodate different interaction protocols, a larger number of players, and incomplete data perspectives. Furthermore, our world models functioned directly on agent observations, which made them computationally costly to query. If the generation of experiences is the major limiting factor, as assumed in this study, this approach is acceptable. Nevertheless, reducing computational demands through methods like latent world models presents a promising avenue for future research. Lastly, the evaluation of solution concepts could also be improved. While combined-game regret employs all available estimates in approximating regret, its inherent inaccuracies may lead to misinterpretations of relative performance.

## 6 Conclusion

This study showed the mutual benefit of co-learning a world model and empirical game. First, we demonstrated that empirical games provide strategically diverse training data that could inform a more robust world model. We then showed that world models can reduce the computational cost, measured in experiences, of response calculations through planning. These two benefits were combined and realized in a new algorithm, Dyna-PSRO. In our experiments, Dyna-PSRO computed lower-regret solutions than PSRO on several partially observable general-sum games. Dyna-PSRO also required substantially fewer experiences than PSRO, a key algorithmic advantage for settings where collecting experiences is a cost-limiting factor.

Figure 5: PSRO compared against Dyna-PSRO. (\(5\) seeds, with \(95\,\%\) bootstrapped CI).